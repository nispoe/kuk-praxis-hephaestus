<!DOCTYPE html>

<html lang="en">
<head><meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>praxis-Llama-2-7b-hf-small-finetune-v12</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<style type="text/css">
    pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: var(--jp-cell-editor-active-background) }
.highlight { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) }
.highlight .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */
.highlight .err { color: var(--jp-mirror-editor-error-color) } /* Error */
.highlight .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */
.highlight .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */
.highlight .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */
.highlight .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */
.highlight .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */
.highlight .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */
.highlight .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */
.highlight .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */
.highlight .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */
.highlight .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */
.highlight .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */
.highlight .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */
.highlight .pm { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation.Marker */
.highlight .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */
.highlight .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */
.highlight .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */
.highlight .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */
.highlight .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */
.highlight .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */
.highlight .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */
.highlight .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */
.highlight .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */
.highlight .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */
.highlight .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */
.highlight .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */
.highlight .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */
.highlight .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */
.highlight .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */
.highlight .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */
.highlight .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */
.highlight .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */
.highlight .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */
.highlight .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */
  </style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
 * Mozilla scrollbar styling
 */

/* use standard opaque scrollbars for most nodes */
[data-jp-theme-scrollbars='true'] {
  scrollbar-color: rgb(var(--jp-scrollbar-thumb-color))
    var(--jp-scrollbar-background-color);
}

/* for code nodes, use a transparent style of scrollbar. These selectors
 * will match lower in the tree, and so will override the above */
[data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar,
[data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
}

/* tiny scrollbar */

.jp-scrollbar-tiny {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
  scrollbar-width: thin;
}

/* tiny scrollbar */

.jp-scrollbar-tiny::-webkit-scrollbar,
.jp-scrollbar-tiny::-webkit-scrollbar-corner {
  background-color: transparent;
  height: 4px;
  width: 4px;
}

.jp-scrollbar-tiny::-webkit-scrollbar-thumb {
  background: rgba(var(--jp-scrollbar-thumb-color), 0.5);
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:horizontal {
  border-left: 0 solid transparent;
  border-right: 0 solid transparent;
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:vertical {
  border-top: 0 solid transparent;
  border-bottom: 0 solid transparent;
}

/*
 * Lumino
 */

.lm-ScrollBar[data-orientation='horizontal'] {
  min-height: 16px;
  max-height: 16px;
  min-width: 45px;
  border-top: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] {
  min-width: 16px;
  max-width: 16px;
  min-height: 45px;
  border-left: 1px solid #a0a0a0;
}

.lm-ScrollBar-button {
  background-color: #f0f0f0;
  background-position: center center;
  min-height: 15px;
  max-height: 15px;
  min-width: 15px;
  max-width: 15px;
}

.lm-ScrollBar-button:hover {
  background-color: #dadada;
}

.lm-ScrollBar-button.lm-mod-active {
  background-color: #cdcdcd;
}

.lm-ScrollBar-track {
  background: #f0f0f0;
}

.lm-ScrollBar-thumb {
  background: #cdcdcd;
}

.lm-ScrollBar-thumb:hover {
  background: #bababa;
}

.lm-ScrollBar-thumb.lm-mod-active {
  background: #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal'] .lm-ScrollBar-thumb {
  height: 100%;
  min-width: 15px;
  border-left: 1px solid #a0a0a0;
  border-right: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] .lm-ScrollBar-thumb {
  width: 100%;
  min-height: 15px;
  border-top: 1px solid #a0a0a0;
  border-bottom: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-left);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-right);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-up);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-down);
  background-size: 17px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Widget {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
}

.lm-Widget.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.lm-AccordionPanel[data-orientation='horizontal'] > .lm-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  display: flex;
  flex-direction: column;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-CommandPalette-search {
  flex: 0 0 auto;
}

.lm-CommandPalette-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  min-height: 0;
  overflow: auto;
  list-style-type: none;
}

.lm-CommandPalette-header {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-CommandPalette-item {
  display: flex;
  flex-direction: row;
}

.lm-CommandPalette-itemIcon {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemContent {
  flex: 1 1 auto;
  overflow: hidden;
}

.lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemLabel {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-close-icon {
  border: 1px solid transparent;
  background-color: transparent;
  position: absolute;
  z-index: 1;
  right: 3%;
  top: 0;
  bottom: 0;
  margin: auto;
  padding: 7px 0;
  display: none;
  vertical-align: middle;
  outline: 0;
  cursor: pointer;
}
.lm-close-icon:after {
  content: 'X';
  display: block;
  width: 15px;
  height: 15px;
  text-align: center;
  color: #000;
  font-weight: normal;
  font-size: 12px;
  cursor: pointer;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-DockPanel {
  z-index: 0;
}

.lm-DockPanel-widget {
  z-index: 0;
}

.lm-DockPanel-tabBar {
  z-index: 1;
}

.lm-DockPanel-handle {
  z-index: 2;
}

.lm-DockPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-DockPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-DockPanel-handle[data-orientation='horizontal'] {
  cursor: ew-resize;
}

.lm-DockPanel-handle[data-orientation='vertical'] {
  cursor: ns-resize;
}

.lm-DockPanel-handle[data-orientation='horizontal']:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-DockPanel-handle[data-orientation='vertical']:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

.lm-DockPanel-overlay {
  z-index: 3;
  box-sizing: border-box;
  pointer-events: none;
}

.lm-DockPanel-overlay.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Menu {
  z-index: 10000;
  position: absolute;
  white-space: nowrap;
  overflow-x: hidden;
  overflow-y: auto;
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-Menu-content {
  margin: 0;
  padding: 0;
  display: table;
  list-style-type: none;
}

.lm-Menu-item {
  display: table-row;
}

.lm-Menu-item.lm-mod-hidden,
.lm-Menu-item.lm-mod-collapsed {
  display: none !important;
}

.lm-Menu-itemIcon,
.lm-Menu-itemSubmenuIcon {
  display: table-cell;
  text-align: center;
}

.lm-Menu-itemLabel {
  display: table-cell;
  text-align: left;
}

.lm-Menu-itemShortcut {
  display: table-cell;
  text-align: right;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-MenuBar {
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-MenuBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex-direction: row;
  list-style-type: none;
}

.lm-MenuBar-item {
  box-sizing: border-box;
}

.lm-MenuBar-itemIcon,
.lm-MenuBar-itemLabel {
  display: inline-block;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-ScrollBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-ScrollBar[data-orientation='horizontal'] {
  flex-direction: row;
}

.lm-ScrollBar[data-orientation='vertical'] {
  flex-direction: column;
}

.lm-ScrollBar-button {
  box-sizing: border-box;
  flex: 0 0 auto;
}

.lm-ScrollBar-track {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
  flex: 1 1 auto;
}

.lm-ScrollBar-thumb {
  box-sizing: border-box;
  position: absolute;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-SplitPanel-child {
  z-index: 0;
}

.lm-SplitPanel-handle {
  z-index: 1;
}

.lm-SplitPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-SplitPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle {
  cursor: ew-resize;
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle {
  cursor: ns-resize;
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-TabBar[data-orientation='horizontal'] {
  flex-direction: row;
  align-items: flex-end;
}

.lm-TabBar[data-orientation='vertical'] {
  flex-direction: column;
  align-items: flex-end;
}

.lm-TabBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex: 1 1 auto;
  list-style-type: none;
}

.lm-TabBar[data-orientation='horizontal'] > .lm-TabBar-content {
  flex-direction: row;
}

.lm-TabBar[data-orientation='vertical'] > .lm-TabBar-content {
  flex-direction: column;
}

.lm-TabBar-tab {
  display: flex;
  flex-direction: row;
  box-sizing: border-box;
  overflow: hidden;
  touch-action: none; /* Disable native Drag/Drop */
}

.lm-TabBar-tabIcon,
.lm-TabBar-tabCloseIcon {
  flex: 0 0 auto;
}

.lm-TabBar-tabLabel {
  flex: 1 1 auto;
  overflow: hidden;
  white-space: nowrap;
}

.lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
}

.lm-TabBar-tab.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar-addButton.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab {
  position: relative;
}

.lm-TabBar.lm-mod-dragging[data-orientation='horizontal'] .lm-TabBar-tab {
  left: 0;
  transition: left 150ms ease;
}

.lm-TabBar.lm-mod-dragging[data-orientation='vertical'] .lm-TabBar-tab {
  top: 0;
  transition: top 150ms ease;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging {
  transition: none;
}

.lm-TabBar-tabLabel .lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
  background: inherit;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabPanel-tabBar {
  z-index: 1;
}

.lm-TabPanel-stackedPanel {
  z-index: 0;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapse {
  display: flex;
  flex-direction: column;
  align-items: stretch;
}

.jp-Collapse-header {
  padding: 1px 12px;
  background-color: var(--jp-layout-color1);
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  align-items: center;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  text-transform: uppercase;
  user-select: none;
}

.jp-Collapser-icon {
  height: 16px;
}

.jp-Collapse-header-collapsed .jp-Collapser-icon {
  transform: rotate(-90deg);
  margin: auto 0;
}

.jp-Collapser-title {
  line-height: 25px;
}

.jp-Collapse-contents {
  padding: 0 12px;
  background-color: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensureUiComponents() in @jupyterlab/buildutils */

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

/* Icons urls */

:root {
  --jp-icon-add-above: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5MikiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik00Ljc1IDQuOTMwNjZINi42MjVWNi44MDU2NkM2LjYyNSA3LjAxMTkxIDYuNzkzNzUgNy4xODA2NiA3IDcuMTgwNjZDNy4yMDYyNSA3LjE4MDY2IDcuMzc1IDcuMDExOTEgNy4zNzUgNi44MDU2NlY0LjkzMDY2SDkuMjVDOS40NTYyNSA0LjkzMDY2IDkuNjI1IDQuNzYxOTEgOS42MjUgNC41NTU2NkM5LjYyNSA0LjM0OTQxIDkuNDU2MjUgNC4xODA2NiA5LjI1IDQuMTgwNjZINy4zNzVWMi4zMDU2NkM3LjM3NSAyLjA5OTQxIDcuMjA2MjUgMS45MzA2NiA3IDEuOTMwNjZDNi43OTM3NSAxLjkzMDY2IDYuNjI1IDIuMDk5NDEgNi42MjUgMi4zMDU2NlY0LjE4MDY2SDQuNzVDNC41NDM3NSA0LjE4MDY2IDQuMzc1IDQuMzQ5NDEgNC4zNzUgNC41NTU2NkM0LjM3NSA0Ljc2MTkxIDQuNTQzNzUgNC45MzA2NiA0Ljc1IDQuOTMwNjZaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC43Ii8+CjwvZz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTExLjUgOS41VjExLjVMMi41IDExLjVWOS41TDExLjUgOS41Wk0xMiA4QzEyLjU1MjMgOCAxMyA4LjQ0NzcyIDEzIDlWMTJDMTMgMTIuNTUyMyAxMi41NTIzIDEzIDEyIDEzTDIgMTNDMS40NDc3MiAxMyAxIDEyLjU1MjMgMSAxMlY5QzEgOC40NDc3MiAxLjQ0NzcxIDggMiA4TDEyIDhaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5MiI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KC0xIDAgMCAxIDEwIDEuNTU1NjYpIi8+CjwvY2xpcFBhdGg+CjwvZGVmcz4KPC9zdmc+Cg==);
  --jp-icon-add-below: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5OCkiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik05LjI1IDEwLjA2OTNMNy4zNzUgMTAuMDY5M0w3LjM3NSA4LjE5NDM0QzcuMzc1IDcuOTg4MDkgNy4yMDYyNSA3LjgxOTM0IDcgNy44MTkzNEM2Ljc5Mzc1IDcuODE5MzQgNi42MjUgNy45ODgwOSA2LjYyNSA4LjE5NDM0TDYuNjI1IDEwLjA2OTNMNC43NSAxMC4wNjkzQzQuNTQzNzUgMTAuMDY5MyA0LjM3NSAxMC4yMzgxIDQuMzc1IDEwLjQ0NDNDNC4zNzUgMTAuNjUwNiA0LjU0Mzc1IDEwLjgxOTMgNC43NSAxMC44MTkzTDYuNjI1IDEwLjgxOTNMNi42MjUgMTIuNjk0M0M2LjYyNSAxMi45MDA2IDYuNzkzNzUgMTMuMDY5MyA3IDEzLjA2OTNDNy4yMDYyNSAxMy4wNjkzIDcuMzc1IDEyLjkwMDYgNy4zNzUgMTIuNjk0M0w3LjM3NSAxMC44MTkzTDkuMjUgMTAuODE5M0M5LjQ1NjI1IDEwLjgxOTMgOS42MjUgMTAuNjUwNiA5LjYyNSAxMC40NDQzQzkuNjI1IDEwLjIzODEgOS40NTYyNSAxMC4wNjkzIDkuMjUgMTAuMDY5M1oiIGZpbGw9IiM2MTYxNjEiIHN0cm9rZT0iIzYxNjE2MSIgc3Ryb2tlLXdpZHRoPSIwLjciLz4KPC9nPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMi41IDUuNUwyLjUgMy41TDExLjUgMy41TDExLjUgNS41TDIuNSA1LjVaTTIgN0MxLjQ0NzcyIDcgMSA2LjU1MjI4IDEgNkwxIDNDMSAyLjQ0NzcyIDEuNDQ3NzIgMiAyIDJMMTIgMkMxMi41NTIzIDIgMTMgMi40NDc3MiAxMyAzTDEzIDZDMTMgNi41NTIyOSAxMi41NTIzIDcgMTIgN0wyIDdaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5OCI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KDEgMS43NDg0NmUtMDcgMS43NDg0NmUtMDcgLTEgNCAxMy40NDQzKSIvPgo8L2NsaXBQYXRoPgo8L2RlZnM+Cjwvc3ZnPgo=);
  --jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bell: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE2IDE2IiB2ZXJzaW9uPSIxLjEiPgogICA8cGF0aCBjbGFzcz0ianAtaWNvbjIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMzMzMzMzIgogICAgICBkPSJtOCAwLjI5Yy0xLjQgMC0yLjcgMC43My0zLjYgMS44LTEuMiAxLjUtMS40IDMuNC0xLjUgNS4yLTAuMTggMi4yLTAuNDQgNC0yLjMgNS4zbDAuMjggMS4zaDVjMC4wMjYgMC42NiAwLjMyIDEuMSAwLjcxIDEuNSAwLjg0IDAuNjEgMiAwLjYxIDIuOCAwIDAuNTItMC40IDAuNi0xIDAuNzEtMS41aDVsMC4yOC0xLjNjLTEuOS0wLjk3LTIuMi0zLjMtMi4zLTUuMy0wLjEzLTEuOC0wLjI2LTMuNy0xLjUtNS4yLTAuODUtMS0yLjItMS44LTMuNi0xLjh6bTAgMS40YzAuODggMCAxLjkgMC41NSAyLjUgMS4zIDAuODggMS4xIDEuMSAyLjcgMS4yIDQuNCAwLjEzIDEuNyAwLjIzIDMuNiAxLjMgNS4yaC0xMGMxLjEtMS42IDEuMi0zLjQgMS4zLTUuMiAwLjEzLTEuNyAwLjMtMy4zIDEuMi00LjQgMC41OS0wLjcyIDEuNi0xLjMgMi41LTEuM3ptLTAuNzQgMTJoMS41Yy0wLjAwMTUgMC4yOCAwLjAxNSAwLjc5LTAuNzQgMC43OS0wLjczIDAuMDAxNi0wLjcyLTAuNTMtMC43NC0wLjc5eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-bug-dot: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiPgogICAgICAgIDxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMTcuMTkgOEgyMFYxMEgxNy45MUMxNy45NiAxMC4zMyAxOCAxMC42NiAxOCAxMVYxMkgyMFYxNEgxOC41SDE4VjE0LjAyNzVDMTUuNzUgMTQuMjc2MiAxNCAxNi4xODM3IDE0IDE4LjVDMTQgMTkuMjA4IDE0LjE2MzUgMTkuODc3OSAxNC40NTQ5IDIwLjQ3MzlDMTMuNzA2MyAyMC44MTE3IDEyLjg3NTcgMjEgMTIgMjFDOS43OCAyMSA3Ljg1IDE5Ljc5IDYuODEgMThINFYxNkg2LjA5QzYuMDQgMTUuNjcgNiAxNS4zNCA2IDE1VjE0SDRWMTJINlYxMUM2IDEwLjY2IDYuMDQgMTAuMzMgNi4wOSAxMEg0VjhINi44MUM3LjI2IDcuMjIgNy44OCA2LjU1IDguNjIgNi4wNEw3IDQuNDFMOC40MSAzTDEwLjU5IDUuMTdDMTEuMDQgNS4wNiAxMS41MSA1IDEyIDVDMTIuNDkgNSAxMi45NiA1LjA2IDEzLjQyIDUuMTdMMTUuNTkgM0wxNyA0LjQxTDE1LjM3IDYuMDRDMTYuMTIgNi41NSAxNi43NCA3LjIyIDE3LjE5IDhaTTEwIDE2SDE0VjE0SDEwVjE2Wk0xMCAxMkgxNFYxMEgxMFYxMloiIGZpbGw9IiM2MTYxNjEiLz4KICAgICAgICA8cGF0aCBkPSJNMjIgMTguNUMyMiAyMC40MzMgMjAuNDMzIDIyIDE4LjUgMjJDMTYuNTY3IDIyIDE1IDIwLjQzMyAxNSAxOC41QzE1IDE2LjU2NyAxNi41NjcgMTUgMTguNSAxNUMyMC40MzMgMTUgMjIgMTYuNTY3IDIyIDE4LjVaIiBmaWxsPSIjNjE2MTYxIi8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yMCA4aC0yLjgxYy0uNDUtLjc4LTEuMDctMS40NS0xLjgyLTEuOTZMMTcgNC40MSAxNS41OSAzbC0yLjE3IDIuMTdDMTIuOTYgNS4wNiAxMi40OSA1IDEyIDVjLS40OSAwLS45Ni4wNi0xLjQxLjE3TDguNDEgMyA3IDQuNDFsMS42MiAxLjYzQzcuODggNi41NSA3LjI2IDcuMjIgNi44MSA4SDR2MmgyLjA5Yy0uMDUuMzMtLjA5LjY2LS4wOSAxdjFINHYyaDJ2MWMwIC4zNC4wNC42Ny4wOSAxSDR2MmgyLjgxYzEuMDQgMS43OSAyLjk3IDMgNS4xOSAzczQuMTUtMS4yMSA1LjE5LTNIMjB2LTJoLTIuMDljLjA1LS4zMy4wOS0uNjYuMDktMXYtMWgydi0yaC0ydi0xYzAtLjM0LS4wNC0uNjctLjA5LTFIMjBWOHptLTYgOGgtNHYtMmg0djJ6bTAtNGgtNHYtMmg0djJ6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-build: url(data:image/svg+xml;base64,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);
  --jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik05IDE2LjE3TDQuODMgMTJsLTEuNDIgMS40MUw5IDE5IDIxIDdsLTEuNDEtMS40MXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBzaGFwZS1yZW5kZXJpbmc9Imdlb21ldHJpY1ByZWNpc2lvbiI+CiAgICA8cGF0aCBkPSJNNi41OSwzLjQxTDIsOEw2LjU5LDEyLjZMOCwxMS4xOEw0LjgyLDhMOCw0LjgyTDYuNTksMy40MU0xMi40MSwzLjQxTDExLDQuODJMMTQuMTgsOEwxMSwxMS4xOEwxMi40MSwxMi42TDE3LDhMMTIuNDEsMy40MU0yMS41OSwxMS41OUwxMy41LDE5LjY4TDkuODMsMTZMOC40MiwxNy40MUwxMy41LDIyLjVMMjMsMTNMMjEuNTksMTEuNTlaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTExLjQgMTguNkw2LjggMTRMMTEuNCA5LjRMMTAgOEw0IDE0TDEwIDIwTDExLjQgMTguNlpNMTYuNiAxOC42TDIxLjIgMTRMMTYuNiA5LjRMMTggOEwyNCAxNEwxOCAyMEwxNi42IDE4LjZWMTguNloiLz4KCTwvZz4KPC9zdmc+Cg==);
  --jp-icon-collapse-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNNiAxM3YyaDh2LTJ6IiAvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1jb25zb2xlLWljb24tYmFja2dyb3VuZC1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtY29uc29sZS1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIj4KICAgIDxwYXRoIGQ9Ik0xMDUgMTI3LjNoNDB2MTIuOGgtNDB6TTUxLjEgNzdMNzQgOTkuOWwtMjMuMyAyMy4zIDEwLjUgMTAuNSAyMy4zLTIzLjNMOTUgOTkuOSA4NC41IDg5LjQgNjEuNiA2Ni41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copyright: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGVuYWJsZS1iYWNrZ3JvdW5kPSJuZXcgMCAwIDI0IDI0IiBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCI+CiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0xMS44OCw5LjE0YzEuMjgsMC4wNiwxLjYxLDEuMTUsMS42MywxLjY2aDEuNzljLTAuMDgtMS45OC0xLjQ5LTMuMTktMy40NS0zLjE5QzkuNjQsNy42MSw4LDksOCwxMi4xNCBjMCwxLjk0LDAuOTMsNC4yNCwzLjg0LDQuMjRjMi4yMiwwLDMuNDEtMS42NSwzLjQ0LTIuOTVoLTEuNzljLTAuMDMsMC41OS0wLjQ1LDEuMzgtMS42MywxLjQ0QzEwLjU1LDE0LjgzLDEwLDEzLjgxLDEwLDEyLjE0IEMxMCw5LjI1LDExLjI4LDkuMTYsMTEuODgsOS4xNHogTTEyLDJDNi40OCwyLDIsNi40OCwyLDEyczQuNDgsMTAsMTAsMTBzMTAtNC40OCwxMC0xMFMxNy41MiwyLDEyLDJ6IE0xMiwyMGMtNC40MSwwLTgtMy41OS04LTggczMuNTktOCw4LThzOCwzLjU5LDgsOFMxNi40MSwyMCwxMiwyMHoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-delete: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2cHgiIGhlaWdodD0iMTZweCI+CiAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIiAvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjI2MjYyIiBkPSJNNiAxOWMwIDEuMS45IDIgMiAyaDhjMS4xIDAgMi0uOSAyLTJWN0g2djEyek0xOSA0aC0zLjVsLTEtMWgtNWwtMSAxSDV2MmgxNFY0eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-duplicate: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTIuNzk5OTggMC44NzVIOC44OTU4MkM5LjIwMDYxIDAuODc1IDkuNDQ5OTggMS4xMzkxNCA5LjQ0OTk4IDEuNDYxOThDOS40NDk5OCAxLjc4NDgyIDkuMjAwNjEgMi4wNDg5NiA4Ljg5NTgyIDIuMDQ4OTZIMy4zNTQxNUMzLjA0OTM2IDIuMDQ4OTYgMi43OTk5OCAyLjMxMzEgMi43OTk5OCAyLjYzNTk0VjkuNjc5NjlDMi43OTk5OCAxMC4wMDI1IDIuNTUwNjEgMTAuMjY2NyAyLjI0NTgyIDEwLjI2NjdDMS45NDEwMyAxMC4yNjY3IDEuNjkxNjUgMTAuMDAyNSAxLjY5MTY1IDkuNjc5NjlWMi4wNDg5NkMxLjY5MTY1IDEuNDAzMjggMi4xOTA0IDAuODc1IDIuNzk5OTggMC44NzVaTTUuMzY2NjUgMTEuOVY0LjU1SDExLjA4MzNWMTEuOUg1LjM2NjY1Wk00LjE0MTY1IDQuMTQxNjdDNC4xNDE2NSAzLjY5MDYzIDQuNTA3MjggMy4zMjUgNC45NTgzMiAzLjMyNUgxMS40OTE3QzExLjk0MjcgMy4zMjUgMTIuMzA4MyAzLjY5MDYzIDEyLjMwODMgNC4xNDE2N1YxMi4zMDgzQzEyLjMwODMgMTIuNzU5NCAxMS45NDI3IDEzLjEyNSAxMS40OTE3IDEzLjEyNUg0Ljk1ODMyQzQuNTA3MjggMTMuMTI1IDQuMTQxNjUgMTIuNzU5NCA0LjE0MTY1IDEyLjMwODNWNC4xNDE2N1oiIGZpbGw9IiM2MTYxNjEiLz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNOS40MzU3NCA4LjI2NTA3SDguMzY0MzFWOS4zMzY1QzguMzY0MzEgOS40NTQzNSA4LjI2Nzg4IDkuNTUwNzggOC4xNTAwMiA5LjU1MDc4QzguMDMyMTcgOS41NTA3OCA3LjkzNTc0IDkuNDU0MzUgNy45MzU3NCA5LjMzNjVWOC4yNjUwN0g2Ljg2NDMxQzYuNzQ2NDUgOC4yNjUwNyA2LjY1MDAyIDguMTY4NjQgNi42NTAwMiA4LjA1MDc4QzYuNjUwMDIgNy45MzI5MiA2Ljc0NjQ1IDcuODM2NSA2Ljg2NDMxIDcuODM2NUg3LjkzNTc0VjYuNzY1MDdDNy45MzU3NCA2LjY0NzIxIDguMDMyMTcgNi41NTA3OCA4LjE1MDAyIDYuNTUwNzhDOC4yNjc4OCA2LjU1MDc4IDguMzY0MzEgNi42NDcyMSA4LjM2NDMxIDYuNzY1MDdWNy44MzY1SDkuNDM1NzRDOS41NTM2IDcuODM2NSA5LjY1MDAyIDcuOTMyOTIgOS42NTAwMiA4LjA1MDc4QzkuNjUwMDIgOC4xNjg2NCA5LjU1MzYgOC4yNjUwNyA5LjQzNTc0IDguMjY1MDdaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC41Ii8+Cjwvc3ZnPgo=);
  --jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-error: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj48Y2lyY2xlIGN4PSIxMiIgY3k9IjE5IiByPSIyIi8+PHBhdGggZD0iTTEwIDNoNHYxMmgtNHoiLz48L2c+CjxwYXRoIGZpbGw9Im5vbmUiIGQ9Ik0wIDBoMjR2MjRIMHoiLz4KPC9zdmc+Cg==);
  --jp-icon-expand-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNMTEgMTBIOXYzSDZ2MmgzdjNoMnYtM2gzdi0yaC0zeiIgLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-filter-dot: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWRvdCIgZmlsbD0iI0ZGRiI+CiAgICA8Y2lyY2xlIGN4PSIxOCIgY3k9IjE3IiByPSIzIj48L2NpcmNsZT4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-filter: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-folder-favorite: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgwVjB6IiBmaWxsPSJub25lIi8+PHBhdGggY2xhc3M9ImpwLWljb24zIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzYxNjE2MSIgZD0iTTIwIDZoLThsLTItMkg0Yy0xLjEgMC0yIC45LTIgMnYxMmMwIDEuMS45IDIgMiAyaDE2YzEuMSAwIDItLjkgMi0yVjhjMC0xLjEtLjktMi0yLTJ6bS0yLjA2IDExTDE1IDE1LjI4IDEyLjA2IDE3bC43OC0zLjMzLTIuNTktMi4yNCAzLjQxLS4yOUwxNSA4bDEuMzQgMy4xNCAzLjQxLjI5LTIuNTkgMi4yNC43OCAzLjMzeiIvPgo8L3N2Zz4K);
  --jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-home: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPjxwYXRoIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xMCAyMHYtNmg0djZoNXYtOGgzTDEyIDMgMiAxMmgzdjh6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==);
  --jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-info: url(data:image/svg+xml;base64,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);
  --jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaW5zcGVjdG9yLWljb24tY29sb3IganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K);
  --jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtanNvbi1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0Y5QTgyNSI+CiAgICA8cGF0aCBkPSJNMjAuMiAxMS44Yy0xLjYgMC0xLjcuNS0xLjcgMSAwIC40LjEuOS4xIDEuMy4xLjUuMS45LjEgMS4zIDAgMS43LTEuNCAyLjMtMy41IDIuM2gtLjl2LTEuOWguNWMxLjEgMCAxLjQgMCAxLjQtLjggMC0uMyAwLS42LS4xLTEgMC0uNC0uMS0uOC0uMS0xLjIgMC0xLjMgMC0xLjggMS4zLTItMS4zLS4yLTEuMy0uNy0xLjMtMiAwLS40LjEtLjguMS0xLjIuMS0uNC4xLS43LjEtMSAwLS44LS40LS43LTEuNC0uOGgtLjVWNC4xaC45YzIuMiAwIDMuNS43IDMuNSAyLjMgMCAuNC0uMS45LS4xIDEuMy0uMS41LS4xLjktLjEgMS4zIDAgLjUuMiAxIDEuNyAxdjEuOHpNMS44IDEwLjFjMS42IDAgMS43LS41IDEuNy0xIDAtLjQtLjEtLjktLjEtMS4zLS4xLS41LS4xLS45LS4xLTEuMyAwLTEuNiAxLjQtMi4zIDMuNS0yLjNoLjl2MS45aC0uNWMtMSAwLTEuNCAwLTEuNC44IDAgLjMgMCAuNi4xIDEgMCAuMi4xLjYuMSAxIDAgMS4zIDAgMS44LTEuMyAyQzYgMTEuMiA2IDExLjcgNiAxM2MwIC40LS4xLjgtLjEgMS4yLS4xLjMtLjEuNy0uMSAxIDAgLjguMy44IDEuNC44aC41djEuOWgtLjljLTIuMSAwLTMuNS0uNi0zLjUtMi4zIDAtLjQuMS0uOS4xLTEuMy4xLS41LjEtLjkuMS0xLjMgMC0uNS0uMi0xLTEuNy0xdi0xLjl6Ii8+CiAgICA8Y2lyY2xlIGN4PSIxMSIgY3k9IjEzLjgiIHI9IjIuMSIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSI4LjIiIHI9IjIuMSIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-julia: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDMyNSAzMDAiPgogIDxnIGNsYXNzPSJqcC1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjY2IzYzMzIj4KICAgIDxwYXRoIGQ9Ik0gMTUwLjg5ODQzOCAyMjUgQyAxNTAuODk4NDM4IDI2Ni40MjE4NzUgMTE3LjMyMDMxMiAzMDAgNzUuODk4NDM4IDMwMCBDIDM0LjQ3NjU2MiAzMDAgMC44OTg0MzggMjY2LjQyMTg3NSAwLjg5ODQzOCAyMjUgQyAwLjg5ODQzOCAxODMuNTc4MTI1IDM0LjQ3NjU2MiAxNTAgNzUuODk4NDM4IDE1MCBDIDExNy4zMjAzMTIgMTUwIDE1MC44OTg0MzggMTgzLjU3ODEyNSAxNTAuODk4NDM4IDIyNSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzM4OTgyNiI+CiAgICA8cGF0aCBkPSJNIDIzNy41IDc1IEMgMjM3LjUgMTE2LjQyMTg3NSAyMDMuOTIxODc1IDE1MCAxNjIuNSAxNTAgQyAxMjEuMDc4MTI1IDE1MCA4Ny41IDExNi40MjE4NzUgODcuNSA3NSBDIDg3LjUgMzMuNTc4MTI1IDEyMS4wNzgxMjUgMCAxNjIuNSAwIEMgMjAzLjkyMTg3NSAwIDIzNy41IDMzLjU3ODEyNSAyMzcuNSA3NSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzk1NThiMiI+CiAgICA8cGF0aCBkPSJNIDMyNC4xMDE1NjIgMjI1IEMgMzI0LjEwMTU2MiAyNjYuNDIxODc1IDI5MC41MjM0MzggMzAwIDI0OS4xMDE1NjIgMzAwIEMgMjA3LjY3OTY4OCAzMDAgMTc0LjEwMTU2MiAyNjYuNDIxODc1IDE3NC4xMDE1NjIgMjI1IEMgMTc0LjEwMTU2MiAxODMuNTc4MTI1IDIwNy42Nzk2ODggMTUwIDI0OS4xMDE1NjIgMTUwIEMgMjkwLjUyMzQzOCAxNTAgMzI0LjEwMTU2MiAxODMuNTc4MTI1IDMyNC4xMDE1NjIgMjI1Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgPGcgY2xhc3M9ImpwLWp1cHl0ZXItaWNvbi1jb2xvciIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-jupyter: url(data:image/svg+xml;base64,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);
  --jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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);
  --jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K);
  --jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==);
  --jp-icon-launch: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMzIgMzIiIHdpZHRoPSIzMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yNiwyOEg2YTIuMDAyNywyLjAwMjcsMCwwLDEtMi0yVjZBMi4wMDI3LDIuMDAyNywwLDAsMSw2LDRIMTZWNkg2VjI2SDI2VjE2aDJWMjZBMi4wMDI3LDIuMDAyNywwLDAsMSwyNiwyOFoiLz4KICAgIDxwb2x5Z29uIHBvaW50cz0iMjAgMiAyMCA0IDI2LjU4NiA0IDE4IDEyLjU4NiAxOS40MTQgMTQgMjggNS40MTQgMjggMTIgMzAgMTIgMzAgMiAyMCAyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==);
  --jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4K);
  --jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-move-down: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMTIuNDcxIDcuNTI4OTlDMTIuNzYzMiA3LjIzNjg0IDEyLjc2MzIgNi43NjMxNiAxMi40NzEgNi40NzEwMVY2LjQ3MTAxQzEyLjE3OSA2LjE3OTA1IDExLjcwNTcgNi4xNzg4NCAxMS40MTM1IDYuNDcwNTRMNy43NSAxMC4xMjc1VjEuNzVDNy43NSAxLjMzNTc5IDcuNDE0MjEgMSA3IDFWMUM2LjU4NTc5IDEgNi4yNSAxLjMzNTc5IDYuMjUgMS43NVYxMC4xMjc1TDIuNTk3MjYgNi40NjgyMkMyLjMwMzM4IDYuMTczODEgMS44MjY0MSA2LjE3MzU5IDEuNTMyMjYgNi40Njc3NFY2LjQ2Nzc0QzEuMjM4MyA2Ljc2MTcgMS4yMzgzIDcuMjM4MyAxLjUzMjI2IDcuNTMyMjZMNi4yOTI4OSAxMi4yOTI5QzYuNjgzNDIgMTIuNjgzNCA3LjMxNjU4IDEyLjY4MzQgNy43MDcxMSAxMi4yOTI5TDEyLjQ3MSA3LjUyODk5WiIgZmlsbD0iIzYxNjE2MSIvPgo8L3N2Zz4K);
  --jp-icon-move-up: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMS41Mjg5OSA2LjQ3MTAxQzEuMjM2ODQgNi43NjMxNiAxLjIzNjg0IDcuMjM2ODQgMS41Mjg5OSA3LjUyODk5VjcuNTI4OTlDMS44MjA5NSA3LjgyMDk1IDIuMjk0MjYgNy44MjExNiAyLjU4NjQ5IDcuNTI5NDZMNi4yNSAzLjg3MjVWMTIuMjVDNi4yNSAxMi42NjQyIDYuNTg1NzkgMTMgNyAxM1YxM0M3LjQxNDIxIDEzIDcuNzUgMTIuNjY0MiA3Ljc1IDEyLjI1VjMuODcyNUwxMS40MDI3IDcuNTMxNzhDMTEuNjk2NiA3LjgyNjE5IDEyLjE3MzYgNy44MjY0MSAxMi40Njc3IDcuNTMyMjZWNy41MzIyNkMxMi43NjE3IDcuMjM4MyAxMi43NjE3IDYuNzYxNyAxMi40Njc3IDYuNDY3NzRMNy43MDcxMSAxLjcwNzExQzcuMzE2NTggMS4zMTY1OCA2LjY4MzQyIDEuMzE2NTggNi4yOTI4OSAxLjcwNzExTDEuNTI4OTkgNi40NzEwMVoiIGZpbGw9IiM2MTYxNjEiLz4KPC9zdmc+Cg==);
  --jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=);
  --jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtbm90ZWJvb2staWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-numbering: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTQgMTlINlYxOS41SDVWMjAuNUg2VjIxSDRWMjJIN1YxOEg0VjE5Wk01IDEwSDZWNkg0VjdINVYxMFpNNCAxM0g1LjhMNCAxNS4xVjE2SDdWMTVINS4yTDcgMTIuOVYxMkg0VjEzWk05IDdWOUgyM1Y3SDlaTTkgMjFIMjNWMTlIOVYyMVpNOSAxNUgyM1YxM0g5VjE1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-offline-bolt: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDIuMDJjLTUuNTEgMC05Ljk4IDQuNDctOS45OCA5Ljk4czQuNDcgOS45OCA5Ljk4IDkuOTggOS45OC00LjQ3IDkuOTgtOS45OFMxNy41MSAyLjAyIDEyIDIuMDJ6TTExLjQ4IDIwdi02LjI2SDhMMTMgNHY2LjI2aDMuMzVMMTEuNDggMjB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-pdf: url(data:image/svg+xml;base64,PHN2ZwogICB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyMiAyMiIgd2lkdGg9IjE2Ij4KICAgIDxwYXRoIHRyYW5zZm9ybT0icm90YXRlKDQ1KSIgY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0ZGMkEyQSIKICAgICAgIGQ9Im0gMjIuMzQ0MzY5LC0zLjAxNjM2NDIgaCA1LjYzODYwNCB2IDEuNTc5MjQzMyBoIC0zLjU0OTIyNyB2IDEuNTA4NjkyOTkgaCAzLjMzNzU3NiBWIDEuNjUwODE1NCBoIC0zLjMzNzU3NiB2IDMuNDM1MjYxMyBoIC0yLjA4OTM3NyB6IG0gLTcuMTM2NDQ0LDEuNTc5MjQzMyB2IDQuOTQzOTU0MyBoIDAuNzQ4OTIgcSAxLjI4MDc2MSwwIDEuOTUzNzAzLC0wLjYzNDk1MzUgMC42NzgzNjksLTAuNjM0OTUzNSAwLjY3ODM2OSwtMS44NDUxNjQxIDAsLTEuMjA0NzgzNTUgLTAuNjcyOTQyLC0xLjgzNDMxMDExIC0wLjY3Mjk0MiwtMC42Mjk1MjY1OSAtMS45NTkxMywtMC42Mjk1MjY1OSB6IG0gLTIuMDg5Mzc3LC0xLjU3OTI0MzMgaCAyLjIwMzM0MyBxIDEuODQ1MTY0LDAgMi43NDYwMzksMC4yNjU5MjA3IDAuOTA2MzAxLDAuMjYwNDkzNyAxLjU1MjEwOCwwLjg5MDAyMDMgMC41Njk4MywwLjU0ODEyMjMgMC44NDY2MDUsMS4yNjQ0ODAwNiAwLjI3Njc3NCwwLjcxNjM1NzgxIDAuMjc2Nzc0LDEuNjIyNjU4OTQgMCwwLjkxNzE1NTEgLTAuMjc2Nzc0LDEuNjM4OTM5OSAtMC4yNzY3NzUsMC43MTYzNTc4IC0wLjg0NjYwNSwxLjI2NDQ4IC0wLjY1MTIzNCwwLjYyOTUyNjYgLTEuNTYyOTYyLDAuODk1NDQ3MyAtMC45MTE3MjgsMC4yNjA0OTM3IC0yLjczNTE4NSwwLjI2MDQ5MzcgaCAtMi4yMDMzNDMgeiBtIC04LjE0NTg1NjUsMCBoIDMuNDY3ODIzIHEgMS41NDY2ODE2LDAgMi4zNzE1Nzg1LDAuNjg5MjIzIDAuODMwMzI0LDAuNjgzNzk2MSAwLjgzMDMyNCwxLjk1MzcwMzE0IDAsMS4yNzUzMzM5NyAtMC44MzAzMjQsMS45NjQ1NTcwNiBRIDkuOTg3MTk2MSwyLjI3NDkxNSA4LjQ0MDUxNDUsMi4yNzQ5MTUgSCA3LjA2MjA2ODQgViA1LjA4NjA3NjcgSCA0Ljk3MjY5MTUgWiBtIDIuMDg5Mzc2OSwxLjUxNDExOTkgdiAyLjI2MzAzOTQzIGggMS4xNTU5NDEgcSAwLjYwNzgxODgsMCAwLjkzODg2MjksLTAuMjkzMDU1NDcgMC4zMzEwNDQxLC0wLjI5ODQ4MjQxIDAuMzMxMDQ0MSwtMC44NDExNzc3MiAwLC0wLjU0MjY5NTMxIC0wLjMzMTA0NDEsLTAuODM1NzUwNzQgLTAuMzMxMDQ0MSwtMC4yOTMwNTU1IC0wLjkzODg2MjksLTAuMjkzMDU1NSB6IgovPgo8L3N2Zz4K);
  --jp-icon-python: url(data:image/svg+xml;base64,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);
  --jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==);
  --jp-icon-react: url(data:image/svg+xml;base64,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);
  --jp-icon-redo: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZD0iTTE4LjQgMTAuNkMxNi41NSA4Ljk5IDE0LjE1IDggMTEuNSA4Yy00LjY1IDAtOC41OCAzLjAzLTkuOTYgNy4yMkwzLjkgMTZjMS4wNS0zLjE5IDQuMDUtNS41IDcuNi01LjUgMS45NSAwIDMuNzMuNzIgNS4xMiAxLjg4TDEzIDE2aDlWN2wtMy42IDMuNnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==);
  --jp-icon-share: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTSAxOCAyIEMgMTYuMzU0OTkgMiAxNSAzLjM1NDk5MDQgMTUgNSBDIDE1IDUuMTkwOTUyOSAxNS4wMjE3OTEgNS4zNzcxMjI0IDE1LjA1NjY0MSA1LjU1ODU5MzggTCA3LjkyMTg3NSA5LjcyMDcwMzEgQyA3LjM5ODUzOTkgOS4yNzc4NTM5IDYuNzMyMDc3MSA5IDYgOSBDIDQuMzU0OTkwNCA5IDMgMTAuMzU0OTkgMyAxMiBDIDMgMTMuNjQ1MDEgNC4zNTQ5OTA0IDE1IDYgMTUgQyA2LjczMjA3NzEgMTUgNy4zOTg1Mzk5IDE0LjcyMjE0NiA3LjkyMTg3NSAxNC4yNzkyOTcgTCAxNS4wNTY2NDEgMTguNDM5NDUzIEMgMTUuMDIxNTU1IDE4LjYyMTUxNCAxNSAxOC44MDgzODYgMTUgMTkgQyAxNSAyMC42NDUwMSAxNi4zNTQ5OSAyMiAxOCAyMiBDIDE5LjY0NTAxIDIyIDIxIDIwLjY0NTAxIDIxIDE5IEMgMjEgMTcuMzU0OTkgMTkuNjQ1MDEgMTYgMTggMTYgQyAxNy4yNjc0OCAxNiAxNi42MDE1OTMgMTYuMjc5MzI4IDE2LjA3ODEyNSAxNi43MjI2NTYgTCA4Ljk0MzM1OTQgMTIuNTU4NTk0IEMgOC45NzgyMDk1IDEyLjM3NzEyMiA5IDEyLjE5MDk1MyA5IDEyIEMgOSAxMS44MDkwNDcgOC45NzgyMDk1IDExLjYyMjg3OCA4Ljk0MzM1OTQgMTEuNDQxNDA2IEwgMTYuMDc4MTI1IDcuMjc5Mjk2OSBDIDE2LjYwMTQ2IDcuNzIyMTQ2MSAxNy4yNjc5MjMgOCAxOCA4IEMgMTkuNjQ1MDEgOCAyMSA2LjY0NTAwOTYgMjEgNSBDIDIxIDMuMzU0OTkwNCAxOS42NDUwMSAyIDE4IDIgeiBNIDE4IDQgQyAxOC41NjQxMjkgNCAxOSA0LjQzNTg3MDYgMTkgNSBDIDE5IDUuNTY0MTI5NCAxOC41NjQxMjkgNiAxOCA2IEMgMTcuNDM1ODcxIDYgMTcgNS41NjQxMjk0IDE3IDUgQyAxNyA0LjQzNTg3MDYgMTcuNDM1ODcxIDQgMTggNCB6IE0gNiAxMSBDIDYuNTY0MTI5NCAxMSA3IDExLjQzNTg3MSA3IDEyIEMgNyAxMi41NjQxMjkgNi41NjQxMjk0IDEzIDYgMTMgQyA1LjQzNTg3MDYgMTMgNSAxMi41NjQxMjkgNSAxMiBDIDUgMTEuNDM1ODcxIDUuNDM1ODcwNiAxMSA2IDExIHogTSAxOCAxOCBDIDE4LjU2NDEyOSAxOCAxOSAxOC40MzU4NzEgMTkgMTkgQyAxOSAxOS41NjQxMjkgMTguNTY0MTI5IDIwIDE4IDIwIEMgMTcuNDM1ODcxIDIwIDE3IDE5LjU2NDEyOSAxNyAxOSBDIDE3IDE4LjQzNTg3MSAxNy40MzU4NzEgMTggMTggMTggeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-table-rows: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMSw4SDNWNGgxOFY4eiBNMjEsMTBIM3Y0aDE4VjEweiBNMjEsMTZIM3Y0aDE4VjE2eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-tag: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjgiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCA0MyAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTI4LjgzMzIgMTIuMzM0TDMyLjk5OTggMTYuNTAwN0wzNy4xNjY1IDEyLjMzNEgyOC44MzMyWiIvPgoJCTxwYXRoIGQ9Ik0xNi4yMDk1IDIxLjYxMDRDMTUuNjg3MyAyMi4xMjk5IDE0Ljg0NDMgMjIuMTI5OSAxNC4zMjQ4IDIxLjYxMDRMNi45ODI5IDE0LjcyNDVDNi41NzI0IDE0LjMzOTQgNi4wODMxMyAxMy42MDk4IDYuMDQ3ODYgMTMuMDQ4MkM1Ljk1MzQ3IDExLjUyODggNi4wMjAwMiA4LjYxOTQ0IDYuMDY2MjEgNy4wNzY5NUM2LjA4MjgxIDYuNTE0NzcgNi41NTU0OCA2LjA0MzQ3IDcuMTE4MDQgNi4wMzA1NUM5LjA4ODYzIDUuOTg0NzMgMTMuMjYzOCA1LjkzNTc5IDEzLjY1MTggNi4zMjQyNUwyMS43MzY5IDEzLjYzOUMyMi4yNTYgMTQuMTU4NSAyMS43ODUxIDE1LjQ3MjQgMjEuMjYyIDE1Ljk5NDZMMTYuMjA5NSAyMS42MTA0Wk05Ljc3NTg1IDguMjY1QzkuMzM1NTEgNy44MjU2NiA4LjYyMzUxIDcuODI1NjYgOC4xODI4IDguMjY1QzcuNzQzNDYgOC43MDU3MSA3Ljc0MzQ2IDkuNDE3MzMgOC4xODI4IDkuODU2NjdDOC42MjM4MiAxMC4yOTY0IDkuMzM1ODIgMTAuMjk2NCA5Ljc3NTg1IDkuODU2NjdDMTAuMjE1NiA5LjQxNzMzIDEwLjIxNTYgOC43MDUzMyA5Ljc3NTg1IDguMjY1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-terminal: url(data:image/svg+xml;base64,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);
  --jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtdGV4dC1lZGl0b3ItaWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xNSAxNUgzdjJoMTJ2LTJ6bTAtOEgzdjJoMTJWN3pNMyAxM2gxOHYtMkgzdjJ6bTAgOGgxOHYtMkgzdjJ6TTMgM3YyaDE4VjNIM3oiLz4KPC9zdmc+Cg==);
  --jp-icon-toc: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik03LDVIMjFWN0g3VjVNNywxM1YxMUgyMVYxM0g3TTQsNC41QTEuNSwxLjUgMCAwLDEgNS41LDZBMS41LDEuNSAwIDAsMSA0LDcuNUExLjUsMS41IDAgMCwxIDIuNSw2QTEuNSwxLjUgMCAwLDEgNCw0LjVNNCwxMC41QTEuNSwxLjUgMCAwLDEgNS41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMy41QTEuNSwxLjUgMCAwLDEgMi41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMC41TTcsMTlWMTdIMjFWMTlIN000LDE2LjVBMS41LDEuNSAwIDAsMSA1LjUsMThBMS41LDEuNSAwIDAsMSA0LDE5LjVBMS41LDEuNSAwIDAsMSAyLjUsMThBMS41LDEuNSAwIDAsMSA0LDE2LjVaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tree-view: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMiAxMVYzaC03djNIOVYzSDJ2OGg3VjhoMnYxMGg0djNoN3YtOGgtN3YzaC0yVjhoMnYzeiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K);
  --jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-user: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE2IDdhNCA0IDAgMTEtOCAwIDQgNCAwIDAxOCAwek0xMiAxNGE3IDcgMCAwMC03IDdoMTRhNyA3IDAgMDAtNy03eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-users: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZlcnNpb249IjEuMSIgdmlld0JveD0iMCAwIDM2IDI0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogPGcgY2xhc3M9ImpwLWljb24zIiB0cmFuc2Zvcm09Im1hdHJpeCgxLjczMjcgMCAwIDEuNzMyNyAtMy42MjgyIC4wOTk1NzcpIiBmaWxsPSIjNjE2MTYxIj4KICA8cGF0aCB0cmFuc2Zvcm09Im1hdHJpeCgxLjUsMCwwLDEuNSwwLC02KSIgZD0ibTEyLjE4NiA3LjUwOThjLTEuMDUzNSAwLTEuOTc1NyAwLjU2NjUtMi40Nzg1IDEuNDEwMiAwLjc1MDYxIDAuMzEyNzcgMS4zOTc0IDAuODI2NDggMS44NzMgMS40NzI3aDMuNDg2M2MwLTEuNTkyLTEuMjg4OS0yLjg4MjgtMi44ODA5LTIuODgyOHoiLz4KICA8cGF0aCBkPSJtMjAuNDY1IDIuMzg5NWEyLjE4ODUgMi4xODg1IDAgMCAxLTIuMTg4NCAyLjE4ODUgMi4xODg1IDIuMTg4NSAwIDAgMS0yLjE4ODUtMi4xODg1IDIuMTg4NSAyLjE4ODUgMCAwIDEgMi4xODg1LTIuMTg4NSAyLjE4ODUgMi4xODg1IDAgMCAxIDIuMTg4NCAyLjE4ODV6Ii8+CiAgPHBhdGggdHJhbnNmb3JtPSJtYXRyaXgoMS41LDAsMCwxLjUsMCwtNikiIGQ9Im0zLjU4OTggOC40MjE5Yy0xLjExMjYgMC0yLjAxMzcgMC45MDExMS0yLjAxMzcgMi4wMTM3aDIuODE0NWMwLjI2Nzk3LTAuMzczMDkgMC41OTA3LTAuNzA0MzUgMC45NTg5OC0wLjk3ODUyLTAuMzQ0MzMtMC42MTY4OC0xLjAwMzEtMS4wMzUyLTEuNzU5OC0xLjAzNTJ6Ii8+CiAgPHBhdGggZD0ibTYuOTE1NCA0LjYyM2ExLjUyOTQgMS41Mjk0IDAgMCAxLTEuNTI5NCAxLjUyOTQgMS41Mjk0IDEuNTI5NCAwIDAgMS0xLjUyOTQtMS41Mjk0IDEuNTI5NCAxLjUyOTQgMCAwIDEgMS41Mjk0LTEuNTI5NCAxLjUyOTQgMS41Mjk0IDAgMCAxIDEuNTI5NCAxLjUyOTR6Ii8+CiAgPHBhdGggZD0ibTYuMTM1IDEzLjUzNWMwLTMuMjM5MiAyLjYyNTktNS44NjUgNS44NjUtNS44NjUgMy4yMzkyIDAgNS44NjUgMi42MjU5IDUuODY1IDUuODY1eiIvPgogIDxjaXJjbGUgY3g9IjEyIiBjeT0iMy43Njg1IiByPSIyLjk2ODUiLz4KIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-word: url(data:image/svg+xml;base64,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);
  --jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K);
}

/* Icon CSS class declarations */

.jp-AddAboveIcon {
  background-image: var(--jp-icon-add-above);
}

.jp-AddBelowIcon {
  background-image: var(--jp-icon-add-below);
}

.jp-AddIcon {
  background-image: var(--jp-icon-add);
}

.jp-BellIcon {
  background-image: var(--jp-icon-bell);
}

.jp-BugDotIcon {
  background-image: var(--jp-icon-bug-dot);
}

.jp-BugIcon {
  background-image: var(--jp-icon-bug);
}

.jp-BuildIcon {
  background-image: var(--jp-icon-build);
}

.jp-CaretDownEmptyIcon {
  background-image: var(--jp-icon-caret-down-empty);
}

.jp-CaretDownEmptyThinIcon {
  background-image: var(--jp-icon-caret-down-empty-thin);
}

.jp-CaretDownIcon {
  background-image: var(--jp-icon-caret-down);
}

.jp-CaretLeftIcon {
  background-image: var(--jp-icon-caret-left);
}

.jp-CaretRightIcon {
  background-image: var(--jp-icon-caret-right);
}

.jp-CaretUpEmptyThinIcon {
  background-image: var(--jp-icon-caret-up-empty-thin);
}

.jp-CaretUpIcon {
  background-image: var(--jp-icon-caret-up);
}

.jp-CaseSensitiveIcon {
  background-image: var(--jp-icon-case-sensitive);
}

.jp-CheckIcon {
  background-image: var(--jp-icon-check);
}

.jp-CircleEmptyIcon {
  background-image: var(--jp-icon-circle-empty);
}

.jp-CircleIcon {
  background-image: var(--jp-icon-circle);
}

.jp-ClearIcon {
  background-image: var(--jp-icon-clear);
}

.jp-CloseIcon {
  background-image: var(--jp-icon-close);
}

.jp-CodeCheckIcon {
  background-image: var(--jp-icon-code-check);
}

.jp-CodeIcon {
  background-image: var(--jp-icon-code);
}

.jp-CollapseAllIcon {
  background-image: var(--jp-icon-collapse-all);
}

.jp-ConsoleIcon {
  background-image: var(--jp-icon-console);
}

.jp-CopyIcon {
  background-image: var(--jp-icon-copy);
}

.jp-CopyrightIcon {
  background-image: var(--jp-icon-copyright);
}

.jp-CutIcon {
  background-image: var(--jp-icon-cut);
}

.jp-DeleteIcon {
  background-image: var(--jp-icon-delete);
}

.jp-DownloadIcon {
  background-image: var(--jp-icon-download);
}

.jp-DuplicateIcon {
  background-image: var(--jp-icon-duplicate);
}

.jp-EditIcon {
  background-image: var(--jp-icon-edit);
}

.jp-EllipsesIcon {
  background-image: var(--jp-icon-ellipses);
}

.jp-ErrorIcon {
  background-image: var(--jp-icon-error);
}

.jp-ExpandAllIcon {
  background-image: var(--jp-icon-expand-all);
}

.jp-ExtensionIcon {
  background-image: var(--jp-icon-extension);
}

.jp-FastForwardIcon {
  background-image: var(--jp-icon-fast-forward);
}

.jp-FileIcon {
  background-image: var(--jp-icon-file);
}

.jp-FileUploadIcon {
  background-image: var(--jp-icon-file-upload);
}

.jp-FilterDotIcon {
  background-image: var(--jp-icon-filter-dot);
}

.jp-FilterIcon {
  background-image: var(--jp-icon-filter);
}

.jp-FilterListIcon {
  background-image: var(--jp-icon-filter-list);
}

.jp-FolderFavoriteIcon {
  background-image: var(--jp-icon-folder-favorite);
}

.jp-FolderIcon {
  background-image: var(--jp-icon-folder);
}

.jp-HomeIcon {
  background-image: var(--jp-icon-home);
}

.jp-Html5Icon {
  background-image: var(--jp-icon-html5);
}

.jp-ImageIcon {
  background-image: var(--jp-icon-image);
}

.jp-InfoIcon {
  background-image: var(--jp-icon-info);
}

.jp-InspectorIcon {
  background-image: var(--jp-icon-inspector);
}

.jp-JsonIcon {
  background-image: var(--jp-icon-json);
}

.jp-JuliaIcon {
  background-image: var(--jp-icon-julia);
}

.jp-JupyterFaviconIcon {
  background-image: var(--jp-icon-jupyter-favicon);
}

.jp-JupyterIcon {
  background-image: var(--jp-icon-jupyter);
}

.jp-JupyterlabWordmarkIcon {
  background-image: var(--jp-icon-jupyterlab-wordmark);
}

.jp-KernelIcon {
  background-image: var(--jp-icon-kernel);
}

.jp-KeyboardIcon {
  background-image: var(--jp-icon-keyboard);
}

.jp-LaunchIcon {
  background-image: var(--jp-icon-launch);
}

.jp-LauncherIcon {
  background-image: var(--jp-icon-launcher);
}

.jp-LineFormIcon {
  background-image: var(--jp-icon-line-form);
}

.jp-LinkIcon {
  background-image: var(--jp-icon-link);
}

.jp-ListIcon {
  background-image: var(--jp-icon-list);
}

.jp-MarkdownIcon {
  background-image: var(--jp-icon-markdown);
}

.jp-MoveDownIcon {
  background-image: var(--jp-icon-move-down);
}

.jp-MoveUpIcon {
  background-image: var(--jp-icon-move-up);
}

.jp-NewFolderIcon {
  background-image: var(--jp-icon-new-folder);
}

.jp-NotTrustedIcon {
  background-image: var(--jp-icon-not-trusted);
}

.jp-NotebookIcon {
  background-image: var(--jp-icon-notebook);
}

.jp-NumberingIcon {
  background-image: var(--jp-icon-numbering);
}

.jp-OfflineBoltIcon {
  background-image: var(--jp-icon-offline-bolt);
}

.jp-PaletteIcon {
  background-image: var(--jp-icon-palette);
}

.jp-PasteIcon {
  background-image: var(--jp-icon-paste);
}

.jp-PdfIcon {
  background-image: var(--jp-icon-pdf);
}

.jp-PythonIcon {
  background-image: var(--jp-icon-python);
}

.jp-RKernelIcon {
  background-image: var(--jp-icon-r-kernel);
}

.jp-ReactIcon {
  background-image: var(--jp-icon-react);
}

.jp-RedoIcon {
  background-image: var(--jp-icon-redo);
}

.jp-RefreshIcon {
  background-image: var(--jp-icon-refresh);
}

.jp-RegexIcon {
  background-image: var(--jp-icon-regex);
}

.jp-RunIcon {
  background-image: var(--jp-icon-run);
}

.jp-RunningIcon {
  background-image: var(--jp-icon-running);
}

.jp-SaveIcon {
  background-image: var(--jp-icon-save);
}

.jp-SearchIcon {
  background-image: var(--jp-icon-search);
}

.jp-SettingsIcon {
  background-image: var(--jp-icon-settings);
}

.jp-ShareIcon {
  background-image: var(--jp-icon-share);
}

.jp-SpreadsheetIcon {
  background-image: var(--jp-icon-spreadsheet);
}

.jp-StopIcon {
  background-image: var(--jp-icon-stop);
}

.jp-TabIcon {
  background-image: var(--jp-icon-tab);
}

.jp-TableRowsIcon {
  background-image: var(--jp-icon-table-rows);
}

.jp-TagIcon {
  background-image: var(--jp-icon-tag);
}

.jp-TerminalIcon {
  background-image: var(--jp-icon-terminal);
}

.jp-TextEditorIcon {
  background-image: var(--jp-icon-text-editor);
}

.jp-TocIcon {
  background-image: var(--jp-icon-toc);
}

.jp-TreeViewIcon {
  background-image: var(--jp-icon-tree-view);
}

.jp-TrustedIcon {
  background-image: var(--jp-icon-trusted);
}

.jp-UndoIcon {
  background-image: var(--jp-icon-undo);
}

.jp-UserIcon {
  background-image: var(--jp-icon-user);
}

.jp-UsersIcon {
  background-image: var(--jp-icon-users);
}

.jp-VegaIcon {
  background-image: var(--jp-icon-vega);
}

.jp-WordIcon {
  background-image: var(--jp-icon-word);
}

.jp-YamlIcon {
  background-image: var(--jp-icon-yaml);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

.jp-Icon,
.jp-MaterialIcon {
  background-position: center;
  background-repeat: no-repeat;
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-cover {
  background-position: center;
  background-repeat: no-repeat;
  background-size: cover;
}

/**
 * (DEPRECATED) Support for specific CSS icon sizes
 */

.jp-Icon-16 {
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-18 {
  background-size: 18px;
  min-width: 18px;
  min-height: 18px;
}

.jp-Icon-20 {
  background-size: 20px;
  min-width: 20px;
  min-height: 20px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.lm-TabBar .lm-TabBar-addButton {
  align-items: center;
  display: flex;
  padding: 4px;
  padding-bottom: 5px;
  margin-right: 1px;
  background-color: var(--jp-layout-color2);
}

.lm-TabBar .lm-TabBar-addButton:hover {
  background-color: var(--jp-layout-color1);
}

.lm-DockPanel-tabBar .lm-TabBar-tab {
  width: var(--jp-private-horizontal-tab-width);
}

.lm-DockPanel-tabBar .lm-TabBar-content {
  flex: unset;
}

.lm-DockPanel-tabBar[data-orientation='horizontal'] {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for icons as inline SVG HTMLElements
 */

/* recolor the primary elements of an icon */
.jp-icon0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-accent0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-accent1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-accent2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-accent3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-accent4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-accent0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-accent1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-accent2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-accent3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-accent4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-none[fill] {
  fill: none;
}

.jp-icon-none[stroke] {
  stroke: none;
}

/* brand icon colors. Same for light and dark */
.jp-icon-brand0[fill] {
  fill: var(--jp-brand-color0);
}

.jp-icon-brand1[fill] {
  fill: var(--jp-brand-color1);
}

.jp-icon-brand2[fill] {
  fill: var(--jp-brand-color2);
}

.jp-icon-brand3[fill] {
  fill: var(--jp-brand-color3);
}

.jp-icon-brand4[fill] {
  fill: var(--jp-brand-color4);
}

.jp-icon-brand0[stroke] {
  stroke: var(--jp-brand-color0);
}

.jp-icon-brand1[stroke] {
  stroke: var(--jp-brand-color1);
}

.jp-icon-brand2[stroke] {
  stroke: var(--jp-brand-color2);
}

.jp-icon-brand3[stroke] {
  stroke: var(--jp-brand-color3);
}

.jp-icon-brand4[stroke] {
  stroke: var(--jp-brand-color4);
}

/* warn icon colors. Same for light and dark */
.jp-icon-warn0[fill] {
  fill: var(--jp-warn-color0);
}

.jp-icon-warn1[fill] {
  fill: var(--jp-warn-color1);
}

.jp-icon-warn2[fill] {
  fill: var(--jp-warn-color2);
}

.jp-icon-warn3[fill] {
  fill: var(--jp-warn-color3);
}

.jp-icon-warn0[stroke] {
  stroke: var(--jp-warn-color0);
}

.jp-icon-warn1[stroke] {
  stroke: var(--jp-warn-color1);
}

.jp-icon-warn2[stroke] {
  stroke: var(--jp-warn-color2);
}

.jp-icon-warn3[stroke] {
  stroke: var(--jp-warn-color3);
}

/* icon colors that contrast well with each other and most backgrounds */
.jp-icon-contrast0[fill] {
  fill: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[fill] {
  fill: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[fill] {
  fill: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[fill] {
  fill: var(--jp-icon-contrast-color3);
}

.jp-icon-contrast0[stroke] {
  stroke: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[stroke] {
  stroke: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[stroke] {
  stroke: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[stroke] {
  stroke: var(--jp-icon-contrast-color3);
}

.jp-icon-dot[fill] {
  fill: var(--jp-warn-color0);
}

.jp-jupyter-icon-color[fill] {
  fill: var(--jp-jupyter-icon-color, var(--jp-warn-color0));
}

.jp-notebook-icon-color[fill] {
  fill: var(--jp-notebook-icon-color, var(--jp-warn-color0));
}

.jp-json-icon-color[fill] {
  fill: var(--jp-json-icon-color, var(--jp-warn-color1));
}

.jp-console-icon-color[fill] {
  fill: var(--jp-console-icon-color, white);
}

.jp-console-icon-background-color[fill] {
  fill: var(--jp-console-icon-background-color, var(--jp-brand-color1));
}

.jp-terminal-icon-color[fill] {
  fill: var(--jp-terminal-icon-color, var(--jp-layout-color2));
}

.jp-terminal-icon-background-color[fill] {
  fill: var(
    --jp-terminal-icon-background-color,
    var(--jp-inverse-layout-color2)
  );
}

.jp-text-editor-icon-color[fill] {
  fill: var(--jp-text-editor-icon-color, var(--jp-inverse-layout-color3));
}

.jp-inspector-icon-color[fill] {
  fill: var(--jp-inspector-icon-color, var(--jp-inverse-layout-color3));
}

/* CSS for icons in selected filebrowser listing items */
.jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

.jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* stylelint-disable selector-max-class, selector-max-compound-selectors */

/**
* TODO: come up with non css-hack solution for showing the busy icon on top
*  of the close icon
* CSS for complex behavior of close icon of tabs in the main area tabbar
*/
.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon3[fill] {
  fill: none;
}

.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: var(--jp-inverse-layout-color3);
}

/* stylelint-enable selector-max-class, selector-max-compound-selectors */

/* CSS for icons in status bar */
#jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

#jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* special handling for splash icon CSS. While the theme CSS reloads during
   splash, the splash icon can loose theming. To prevent that, we set a
   default for its color variable */
:root {
  --jp-warn-color0: var(--md-orange-700);
}

/* not sure what to do with this one, used in filebrowser listing */
.jp-DragIcon {
  margin-right: 4px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for alt colors for icons as inline SVG HTMLElements
 */

/* alt recolor the primary elements of an icon */
.jp-icon-alt .jp-icon0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-alt .jp-icon0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* alt recolor the accent elements of an icon */
.jp-icon-alt .jp-icon-accent0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-alt .jp-icon-accent0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-icon-hoverShow:not(:hover) .jp-icon-hoverShow-content {
  display: none !important;
}

/**
 * Support for hover colors for icons as inline SVG HTMLElements
 */

/**
 * regular colors
 */

/* recolor the primary elements of an icon */
.jp-icon-hover :hover .jp-icon0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-hover :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-hover :hover .jp-icon-none-hover[fill] {
  fill: none;
}

.jp-icon-hover :hover .jp-icon-none-hover[stroke] {
  stroke: none;
}

/**
 * inverse colors
 */

/* inverse recolor the primary elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* inverse recolor the accent elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-IFrame {
  width: 100%;
  height: 100%;
}

.jp-IFrame > iframe {
  border: none;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-IFrame {
  position: relative;
}

body.lm-mod-override-cursor .jp-IFrame::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-HoverBox {
  position: fixed;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FormGroup-content fieldset {
  border: none;
  padding: 0;
  min-width: 0;
  width: 100%;
}

/* stylelint-disable selector-max-type */

.jp-FormGroup-content fieldset .jp-inputFieldWrapper input,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper select,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper textarea {
  font-size: var(--jp-content-font-size2);
  border-color: var(--jp-input-border-color);
  border-style: solid;
  border-radius: var(--jp-border-radius);
  border-width: 1px;
  padding: 6px 8px;
  background: none;
  color: var(--jp-ui-font-color0);
  height: inherit;
}

.jp-FormGroup-content fieldset input[type='checkbox'] {
  position: relative;
  top: 2px;
  margin-left: 0;
}

.jp-FormGroup-content button.jp-mod-styled {
  cursor: pointer;
}

.jp-FormGroup-content .checkbox label {
  cursor: pointer;
  font-size: var(--jp-content-font-size1);
}

.jp-FormGroup-content .jp-root > fieldset > legend {
  display: none;
}

.jp-FormGroup-content .jp-root > fieldset > p {
  display: none;
}

/** copy of `input.jp-mod-styled:focus` style */
.jp-FormGroup-content fieldset input:focus,
.jp-FormGroup-content fieldset select:focus {
  -moz-outline-radius: unset;
  outline: var(--jp-border-width) solid var(--md-blue-500);
  outline-offset: -1px;
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-FormGroup-content fieldset input:hover:not(:focus),
.jp-FormGroup-content fieldset select:hover:not(:focus) {
  background-color: var(--jp-border-color2);
}

/* stylelint-enable selector-max-type */

.jp-FormGroup-content .checkbox .field-description {
  /* Disable default description field for checkbox:
   because other widgets do not have description fields,
   we add descriptions to each widget on the field level.
  */
  display: none;
}

.jp-FormGroup-content #root__description {
  display: none;
}

.jp-FormGroup-content .jp-modifiedIndicator {
  width: 5px;
  background-color: var(--jp-brand-color2);
  margin-top: 0;
  margin-left: calc(var(--jp-private-settingeditor-modifier-indent) * -1);
  flex-shrink: 0;
}

.jp-FormGroup-content .jp-modifiedIndicator.jp-errorIndicator {
  background-color: var(--jp-error-color0);
  margin-right: 0.5em;
}

/* RJSF ARRAY style */

.jp-arrayFieldWrapper legend {
  font-size: var(--jp-content-font-size2);
  color: var(--jp-ui-font-color0);
  flex-basis: 100%;
  padding: 4px 0;
  font-weight: var(--jp-content-heading-font-weight);
  border-bottom: 1px solid var(--jp-border-color2);
}

.jp-arrayFieldWrapper .field-description {
  padding: 4px 0;
  white-space: pre-wrap;
}

.jp-arrayFieldWrapper .array-item {
  width: 100%;
  border: 1px solid var(--jp-border-color2);
  border-radius: 4px;
  margin: 4px;
}

.jp-ArrayOperations {
  display: flex;
  margin-left: 8px;
}

.jp-ArrayOperationsButton {
  margin: 2px;
}

.jp-ArrayOperationsButton .jp-icon3[fill] {
  fill: var(--jp-ui-font-color0);
}

button.jp-ArrayOperationsButton.jp-mod-styled:disabled {
  cursor: not-allowed;
  opacity: 0.5;
}

/* RJSF form validation error */

.jp-FormGroup-content .validationErrors {
  color: var(--jp-error-color0);
}

/* Hide panel level error as duplicated the field level error */
.jp-FormGroup-content .panel.errors {
  display: none;
}

/* RJSF normal content (settings-editor) */

.jp-FormGroup-contentNormal {
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-FormGroup-contentItem {
  margin-left: 7px;
  color: var(--jp-ui-font-color0);
}

.jp-FormGroup-contentNormal .jp-FormGroup-description {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-default {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-fieldLabel {
  font-size: var(--jp-content-font-size1);
  font-weight: normal;
  min-width: 120px;
}

.jp-FormGroup-contentNormal fieldset:not(:first-child) {
  margin-left: 7px;
}

.jp-FormGroup-contentNormal .field-array-of-string .array-item {
  /* Display `jp-ArrayOperations` buttons side-by-side with content except
    for small screens where flex-wrap will place them one below the other.
  */
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-objectFieldWrapper .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

/* RJSF compact content (metadata-form) */

.jp-FormGroup-content.jp-FormGroup-contentCompact {
  width: 100%;
}

.jp-FormGroup-contentCompact .form-group {
  display: flex;
  padding: 0.5em 0.2em 0.5em 0;
}

.jp-FormGroup-contentCompact
  .jp-FormGroup-compactTitle
  .jp-FormGroup-description {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color2);
}

.jp-FormGroup-contentCompact .jp-FormGroup-fieldLabel {
  padding-bottom: 0.3em;
}

.jp-FormGroup-contentCompact .jp-inputFieldWrapper .form-control {
  width: 100%;
  box-sizing: border-box;
}

.jp-FormGroup-contentCompact .jp-arrayFieldWrapper .jp-FormGroup-compactTitle {
  padding-bottom: 7px;
}

.jp-FormGroup-contentCompact
  .jp-objectFieldWrapper
  .jp-objectFieldWrapper
  .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

.jp-FormGroup-contentCompact ul.error-detail {
  margin-block-start: 0.5em;
  margin-block-end: 0.5em;
  padding-inline-start: 1em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-SidePanel {
  display: flex;
  flex-direction: column;
  min-width: var(--jp-sidebar-min-width);
  overflow-y: auto;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  font-size: var(--jp-ui-font-size1);
}

.jp-SidePanel-header {
  flex: 0 0 auto;
  display: flex;
  border-bottom: var(--jp-border-width) solid var(--jp-border-color2);
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin: 0;
  padding: 2px;
  text-transform: uppercase;
}

.jp-SidePanel-toolbar {
  flex: 0 0 auto;
}

.jp-SidePanel-content {
  flex: 1 1 auto;
}

.jp-SidePanel-toolbar,
.jp-AccordionPanel-toolbar {
  height: var(--jp-private-toolbar-height);
}

.jp-SidePanel-toolbar.jp-Toolbar-micro {
  display: none;
}

.lm-AccordionPanel .jp-AccordionPanel-title {
  box-sizing: border-box;
  line-height: 25px;
  margin: 0;
  display: flex;
  align-items: center;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  font-size: var(--jp-ui-font-size0);
}

.jp-AccordionPanel-title {
  cursor: pointer;
  user-select: none;
  -moz-user-select: none;
  -webkit-user-select: none;
  text-transform: uppercase;
}

.lm-AccordionPanel[data-orientation='horizontal'] > .jp-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleLabel {
  user-select: none;
  text-overflow: ellipsis;
  white-space: nowrap;
  overflow: hidden;
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleCollapser {
  transform: rotate(-90deg);
  margin: auto 0;
  height: 16px;
}

.jp-AccordionPanel-title.lm-mod-expanded .lm-AccordionPanel-titleCollapser {
  transform: rotate(0deg);
}

.lm-AccordionPanel .jp-AccordionPanel-toolbar {
  background: none;
  box-shadow: none;
  border: none;
  margin-left: auto;
}

.lm-AccordionPanel .lm-SplitPanel-handle:hover {
  background: var(--jp-layout-color3);
}

.jp-text-truncated {
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Spinner {
  position: absolute;
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 10;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-layout-color0);
  outline: none;
}

.jp-SpinnerContent {
  font-size: 10px;
  margin: 50px auto;
  text-indent: -9999em;
  width: 3em;
  height: 3em;
  border-radius: 50%;
  background: var(--jp-brand-color3);
  background: linear-gradient(
    to right,
    #f37626 10%,
    rgba(255, 255, 255, 0) 42%
  );
  position: relative;
  animation: load3 1s infinite linear, fadeIn 1s;
}

.jp-SpinnerContent::before {
  width: 50%;
  height: 50%;
  background: #f37626;
  border-radius: 100% 0 0;
  position: absolute;
  top: 0;
  left: 0;
  content: '';
}

.jp-SpinnerContent::after {
  background: var(--jp-layout-color0);
  width: 75%;
  height: 75%;
  border-radius: 50%;
  content: '';
  margin: auto;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
}

@keyframes fadeIn {
  0% {
    opacity: 0;
  }

  100% {
    opacity: 1;
  }
}

@keyframes load3 {
  0% {
    transform: rotate(0deg);
  }

  100% {
    transform: rotate(360deg);
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

button.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: none;
  box-sizing: border-box;
  text-align: center;
  line-height: 32px;
  height: 32px;
  padding: 0 12px;
  letter-spacing: 0.8px;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input.jp-mod-styled {
  background: var(--jp-input-background);
  height: 28px;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color1);
  padding-left: 7px;
  padding-right: 7px;
  font-size: var(--jp-ui-font-size2);
  color: var(--jp-ui-font-color0);
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input[type='checkbox'].jp-mod-styled {
  appearance: checkbox;
  -webkit-appearance: checkbox;
  -moz-appearance: checkbox;
  height: auto;
}

input.jp-mod-styled:focus {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-select-wrapper {
  display: flex;
  position: relative;
  flex-direction: column;
  padding: 1px;
  background-color: var(--jp-layout-color1);
  box-sizing: border-box;
  margin-bottom: 12px;
}

.jp-select-wrapper:not(.multiple) {
  height: 28px;
}

.jp-select-wrapper.jp-mod-focused select.jp-mod-styled {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-input-active-background);
}

select.jp-mod-styled:hover {
  cursor: pointer;
  color: var(--jp-ui-font-color0);
  background-color: var(--jp-input-hover-background);
  box-shadow: inset 0 0 1px rgba(0, 0, 0, 0.5);
}

select.jp-mod-styled {
  flex: 1 1 auto;
  width: 100%;
  font-size: var(--jp-ui-font-size2);
  background: var(--jp-input-background);
  color: var(--jp-ui-font-color0);
  padding: 0 25px 0 8px;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

select.jp-mod-styled:not([multiple]) {
  height: 32px;
}

select.jp-mod-styled[multiple] {
  max-height: 200px;
  overflow-y: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-switch {
  display: flex;
  align-items: center;
  padding-left: 4px;
  padding-right: 4px;
  font-size: var(--jp-ui-font-size1);
  background-color: transparent;
  color: var(--jp-ui-font-color1);
  border: none;
  height: 20px;
}

.jp-switch:hover {
  background-color: var(--jp-layout-color2);
}

.jp-switch-label {
  margin-right: 5px;
  font-family: var(--jp-ui-font-family);
}

.jp-switch-track {
  cursor: pointer;
  background-color: var(--jp-switch-color, var(--jp-border-color1));
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 34px;
  height: 16px;
  width: 35px;
  position: relative;
}

.jp-switch-track::before {
  content: '';
  position: absolute;
  height: 10px;
  width: 10px;
  margin: 3px;
  left: 0;
  background-color: var(--jp-ui-inverse-font-color1);
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 50%;
}

.jp-switch[aria-checked='true'] .jp-switch-track {
  background-color: var(--jp-switch-true-position-color, var(--jp-warn-color0));
}

.jp-switch[aria-checked='true'] .jp-switch-track::before {
  /* track width (35) - margins (3 + 3) - thumb width (10) */
  left: 19px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toolbar-height: calc(
    28px + var(--jp-border-width)
  ); /* leave 28px for content */
}

.jp-Toolbar {
  color: var(--jp-ui-font-color1);
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: 2px;
  z-index: 8;
  overflow-x: hidden;
}

/* Toolbar items */

.jp-Toolbar > .jp-Toolbar-item.jp-Toolbar-spacer {
  flex-grow: 1;
  flex-shrink: 1;
}

.jp-Toolbar-item.jp-Toolbar-kernelStatus {
  display: inline-block;
  width: 32px;
  background-repeat: no-repeat;
  background-position: center;
  background-size: 16px;
}

.jp-Toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  display: flex;
  padding-left: 1px;
  padding-right: 1px;
  font-size: var(--jp-ui-font-size1);
  line-height: var(--jp-private-toolbar-height);
  height: 100%;
}

/* Toolbar buttons */

/* This is the div we use to wrap the react component into a Widget */
div.jp-ToolbarButton {
  color: transparent;
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0;
  margin: 0;
}

button.jp-ToolbarButtonComponent {
  background: var(--jp-layout-color1);
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0 6px;
  margin: 0;
  height: 24px;
  border-radius: var(--jp-border-radius);
  display: flex;
  align-items: center;
  text-align: center;
  font-size: 14px;
  min-width: unset;
  min-height: unset;
}

button.jp-ToolbarButtonComponent:disabled {
  opacity: 0.4;
}

button.jp-ToolbarButtonComponent > span {
  padding: 0;
  flex: 0 0 auto;
}

button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label {
  font-size: var(--jp-ui-font-size1);
  line-height: 100%;
  padding-left: 2px;
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar.jp-Toolbar-micro {
  padding: 0;
  min-height: 0;
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar {
  border: none;
  box-shadow: none;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-WindowedPanel-outer {
  position: relative;
  overflow-y: auto;
}

.jp-WindowedPanel-inner {
  position: relative;
}

.jp-WindowedPanel-window {
  position: absolute;
  left: 0;
  right: 0;
  overflow: visible;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* Sibling imports */

body {
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
}

/* Disable native link decoration styles everywhere outside of dialog boxes */
a {
  text-decoration: unset;
  color: unset;
}

a:hover {
  text-decoration: unset;
  color: unset;
}

/* Accessibility for links inside dialog box text */
.jp-Dialog-content a {
  text-decoration: revert;
  color: var(--jp-content-link-color);
}

.jp-Dialog-content a:hover {
  text-decoration: revert;
}

/* Styles for ui-components */
.jp-Button {
  color: var(--jp-ui-font-color2);
  border-radius: var(--jp-border-radius);
  padding: 0 12px;
  font-size: var(--jp-ui-font-size1);

  /* Copy from blueprint 3 */
  display: inline-flex;
  flex-direction: row;
  border: none;
  cursor: pointer;
  align-items: center;
  justify-content: center;
  text-align: left;
  vertical-align: middle;
  min-height: 30px;
  min-width: 30px;
}

.jp-Button:disabled {
  cursor: not-allowed;
}

.jp-Button:empty {
  padding: 0 !important;
}

.jp-Button.jp-mod-small {
  min-height: 24px;
  min-width: 24px;
  font-size: 12px;
  padding: 0 7px;
}

/* Use our own theme for hover styles */
.jp-Button.jp-mod-minimal:hover {
  background-color: var(--jp-layout-color2);
}

.jp-Button.jp-mod-minimal {
  background: none;
}

.jp-InputGroup {
  display: block;
  position: relative;
}

.jp-InputGroup input {
  box-sizing: border-box;
  border: none;
  border-radius: 0;
  background-color: transparent;
  color: var(--jp-ui-font-color0);
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
  padding-bottom: 0;
  padding-top: 0;
  padding-left: 10px;
  padding-right: 28px;
  position: relative;
  width: 100%;
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  font-size: 14px;
  font-weight: 400;
  height: 30px;
  line-height: 30px;
  outline: none;
  vertical-align: middle;
}

.jp-InputGroup input:focus {
  box-shadow: inset 0 0 0 var(--jp-border-width)
      var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-InputGroup input:disabled {
  cursor: not-allowed;
  resize: block;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input:disabled ~ span {
  cursor: not-allowed;
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input::placeholder,
input::placeholder {
  color: var(--jp-ui-font-color2);
}

.jp-InputGroupAction {
  position: absolute;
  bottom: 1px;
  right: 0;
  padding: 6px;
}

.jp-HTMLSelect.jp-DefaultStyle select {
  background-color: initial;
  border: none;
  border-radius: 0;
  box-shadow: none;
  color: var(--jp-ui-font-color0);
  display: block;
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  height: 24px;
  line-height: 14px;
  padding: 0 25px 0 10px;
  text-align: left;
  -moz-appearance: none;
  -webkit-appearance: none;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
  cursor: not-allowed;
  resize: block;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled ~ span {
  cursor: not-allowed;
}

/* Use our own theme for hover and option styles */
/* stylelint-disable-next-line selector-max-type */
.jp-HTMLSelect.jp-DefaultStyle select:hover,
.jp-HTMLSelect.jp-DefaultStyle select > option {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color0);
}

select {
  box-sizing: border-box;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-StatusBar-Widget {
  display: flex;
  align-items: center;
  background: var(--jp-layout-color2);
  min-height: var(--jp-statusbar-height);
  justify-content: space-between;
  padding: 0 10px;
}

.jp-StatusBar-Left {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-StatusBar-Middle {
  display: flex;
  align-items: center;
}

.jp-StatusBar-Right {
  display: flex;
  align-items: center;
  flex-direction: row-reverse;
}

.jp-StatusBar-Item {
  max-height: var(--jp-statusbar-height);
  margin: 0 2px;
  height: var(--jp-statusbar-height);
  white-space: nowrap;
  text-overflow: ellipsis;
  color: var(--jp-ui-font-color1);
  padding: 0 6px;
}

.jp-mod-highlighted:hover {
  background-color: var(--jp-layout-color3);
}

.jp-mod-clicked {
  background-color: var(--jp-brand-color1);
}

.jp-mod-clicked:hover {
  background-color: var(--jp-brand-color0);
}

.jp-mod-clicked .jp-StatusBar-TextItem {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-StatusBar-HoverItem {
  box-shadow: '0px 4px 4px rgba(0, 0, 0, 0.25)';
}

.jp-StatusBar-TextItem {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  line-height: 24px;
  color: var(--jp-ui-font-color1);
}

.jp-StatusBar-GroupItem {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-Statusbar-ProgressCircle svg {
  display: block;
  margin: 0 auto;
  width: 16px;
  height: 24px;
  align-self: normal;
}

.jp-Statusbar-ProgressCircle path {
  fill: var(--jp-inverse-layout-color3);
}

.jp-Statusbar-ProgressBar-progress-bar {
  height: 10px;
  width: 100px;
  border: solid 0.25px var(--jp-brand-color2);
  border-radius: 3px;
  overflow: hidden;
  align-self: center;
}

.jp-Statusbar-ProgressBar-progress-bar > div {
  background-color: var(--jp-brand-color2);
  background-image: linear-gradient(
    -45deg,
    rgba(255, 255, 255, 0.2) 25%,
    transparent 25%,
    transparent 50%,
    rgba(255, 255, 255, 0.2) 50%,
    rgba(255, 255, 255, 0.2) 75%,
    transparent 75%,
    transparent
  );
  background-size: 40px 40px;
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 14px;
  color: #fff;
  text-align: center;
  animation: jp-Statusbar-ExecutionTime-progress-bar 2s linear infinite;
}

.jp-Statusbar-ProgressBar-progress-bar p {
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
  font-size: var(--jp-ui-font-size1);
  line-height: 10px;
  width: 100px;
}

@keyframes jp-Statusbar-ExecutionTime-progress-bar {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 40px 40px;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-commandpalette-search-height: 28px;
}

/*-----------------------------------------------------------------------------
| Overall styles
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  padding-bottom: 0;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Modal variant
|----------------------------------------------------------------------------*/

.jp-ModalCommandPalette {
  position: absolute;
  z-index: 10000;
  top: 38px;
  left: 30%;
  margin: 0;
  padding: 4px;
  width: 40%;
  box-shadow: var(--jp-elevation-z4);
  border-radius: 4px;
  background: var(--jp-layout-color0);
}

.jp-ModalCommandPalette .lm-CommandPalette {
  max-height: 40vh;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-close-icon::after {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-header {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-item {
  margin-left: 4px;
  margin-right: 4px;
}

.jp-ModalCommandPalette
  .lm-CommandPalette
  .lm-CommandPalette-item.lm-mod-disabled {
  display: none;
}

/*-----------------------------------------------------------------------------
| Search
|----------------------------------------------------------------------------*/

.lm-CommandPalette-search {
  padding: 4px;
  background-color: var(--jp-layout-color1);
  z-index: 2;
}

.lm-CommandPalette-wrapper {
  overflow: overlay;
  padding: 0 9px;
  background-color: var(--jp-input-active-background);
  height: 30px;
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
}

.lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper {
  box-shadow: inset 0 0 0 1px var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-SearchIconGroup {
  color: white;
  background-color: var(--jp-brand-color1);
  position: absolute;
  top: 4px;
  right: 4px;
  padding: 5px 5px 1px;
}

.jp-SearchIconGroup svg {
  height: 20px;
  width: 20px;
}

.jp-SearchIconGroup .jp-icon3[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-input {
  background: transparent;
  width: calc(100% - 18px);
  float: left;
  border: none;
  outline: none;
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  line-height: var(--jp-private-commandpalette-search-height);
}

.lm-CommandPalette-input::-webkit-input-placeholder,
.lm-CommandPalette-input::-moz-placeholder,
.lm-CommandPalette-input:-ms-input-placeholder {
  color: var(--jp-ui-font-color2);
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Results
|----------------------------------------------------------------------------*/

.lm-CommandPalette-header:first-child {
  margin-top: 0;
}

.lm-CommandPalette-header {
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin-top: 8px;
  padding: 8px 0 8px 12px;
  text-transform: uppercase;
}

.lm-CommandPalette-header.lm-mod-active {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-header > mark {
  background-color: transparent;
  font-weight: bold;
  color: var(--jp-ui-font-color1);
}

.lm-CommandPalette-item {
  padding: 4px 12px 4px 4px;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  font-weight: 400;
  display: flex;
}

.lm-CommandPalette-item.lm-mod-disabled {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item.lm-mod-active {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item.lm-mod-active .lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-inverse-font-color0);
}

.lm-CommandPalette-item.lm-mod-active .jp-icon-selectable[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled) {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled) {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-itemContent {
  overflow: hidden;
}

.lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.lm-CommandPalette-item.lm-mod-disabled mark {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item .lm-CommandPalette-itemIcon {
  margin: 0 4px 0 0;
  position: relative;
  width: 16px;
  top: 2px;
  flex: 0 0 auto;
}

.lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon {
  opacity: 0.6;
}

.lm-CommandPalette-item .lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemCaption {
  display: none;
}

.lm-CommandPalette-content {
  background-color: var(--jp-layout-color1);
}

.lm-CommandPalette-content:empty::after {
  content: 'No results';
  margin: auto;
  margin-top: 20px;
  width: 100px;
  display: block;
  font-size: var(--jp-ui-font-size2);
  font-family: var(--jp-ui-font-family);
  font-weight: lighter;
}

.lm-CommandPalette-emptyMessage {
  text-align: center;
  margin-top: 24px;
  line-height: 1.32;
  padding: 0 8px;
  color: var(--jp-content-font-color3);
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Dialog {
  position: absolute;
  z-index: 10000;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  top: 0;
  left: 0;
  margin: 0;
  padding: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-dialog-background);
}

.jp-Dialog-content {
  display: flex;
  flex-direction: column;
  margin-left: auto;
  margin-right: auto;
  background: var(--jp-layout-color1);
  padding: 24px 24px 12px;
  min-width: 300px;
  min-height: 150px;
  max-width: 1000px;
  max-height: 500px;
  box-sizing: border-box;
  box-shadow: var(--jp-elevation-z20);
  word-wrap: break-word;
  border-radius: var(--jp-border-radius);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color1);
  resize: both;
}

.jp-Dialog-content.jp-Dialog-content-small {
  max-width: 500px;
}

.jp-Dialog-button {
  overflow: visible;
}

button.jp-Dialog-button:focus {
  outline: 1px solid var(--jp-brand-color1);
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button:focus::-moz-focus-inner {
  border: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus {
  outline: 1px solid var(--jp-accept-color-normal, var(--jp-brand-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus {
  outline: 1px solid var(--jp-warn-color-normal, var(--jp-error-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline: 1px solid var(--jp-reject-color-normal, var(--md-grey-600));
}

button.jp-Dialog-close-button {
  padding: 0;
  height: 100%;
  min-width: unset;
  min-height: unset;
}

.jp-Dialog-header {
  display: flex;
  justify-content: space-between;
  flex: 0 0 auto;
  padding-bottom: 12px;
  font-size: var(--jp-ui-font-size3);
  font-weight: 400;
  color: var(--jp-ui-font-color1);
}

.jp-Dialog-body {
  display: flex;
  flex-direction: column;
  flex: 1 1 auto;
  font-size: var(--jp-ui-font-size1);
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

.jp-Dialog-footer {
  display: flex;
  flex-direction: row;
  justify-content: flex-end;
  align-items: center;
  flex: 0 0 auto;
  margin-left: -12px;
  margin-right: -12px;
  padding: 12px;
}

.jp-Dialog-checkbox {
  padding-right: 5px;
}

.jp-Dialog-checkbox > input:focus-visible {
  outline: 1px solid var(--jp-input-active-border-color);
  outline-offset: 1px;
}

.jp-Dialog-spacer {
  flex: 1 1 auto;
}

.jp-Dialog-title {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.jp-Dialog-body > .jp-select-wrapper {
  width: 100%;
}

.jp-Dialog-body > button {
  padding: 0 16px;
}

.jp-Dialog-body > label {
  line-height: 1.4;
  color: var(--jp-ui-font-color0);
}

.jp-Dialog-button.jp-mod-styled:not(:last-child) {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Input-Boolean-Dialog {
  flex-direction: row-reverse;
  align-items: end;
  width: 100%;
}

.jp-Input-Boolean-Dialog > label {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MainAreaWidget > :focus {
  outline: none;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error {
  padding: 6px;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error > pre {
  width: auto;
  padding: 10px;
  background: var(--jp-error-color3);
  border: var(--jp-border-width) solid var(--jp-error-color1);
  border-radius: var(--jp-border-radius);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  white-space: pre-wrap;
  word-wrap: break-word;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/**
 * google-material-color v1.2.6
 * https://github.com/danlevan/google-material-color
 */
:root {
  --md-red-50: #ffebee;
  --md-red-100: #ffcdd2;
  --md-red-200: #ef9a9a;
  --md-red-300: #e57373;
  --md-red-400: #ef5350;
  --md-red-500: #f44336;
  --md-red-600: #e53935;
  --md-red-700: #d32f2f;
  --md-red-800: #c62828;
  --md-red-900: #b71c1c;
  --md-red-A100: #ff8a80;
  --md-red-A200: #ff5252;
  --md-red-A400: #ff1744;
  --md-red-A700: #d50000;
  --md-pink-50: #fce4ec;
  --md-pink-100: #f8bbd0;
  --md-pink-200: #f48fb1;
  --md-pink-300: #f06292;
  --md-pink-400: #ec407a;
  --md-pink-500: #e91e63;
  --md-pink-600: #d81b60;
  --md-pink-700: #c2185b;
  --md-pink-800: #ad1457;
  --md-pink-900: #880e4f;
  --md-pink-A100: #ff80ab;
  --md-pink-A200: #ff4081;
  --md-pink-A400: #f50057;
  --md-pink-A700: #c51162;
  --md-purple-50: #f3e5f5;
  --md-purple-100: #e1bee7;
  --md-purple-200: #ce93d8;
  --md-purple-300: #ba68c8;
  --md-purple-400: #ab47bc;
  --md-purple-500: #9c27b0;
  --md-purple-600: #8e24aa;
  --md-purple-700: #7b1fa2;
  --md-purple-800: #6a1b9a;
  --md-purple-900: #4a148c;
  --md-purple-A100: #ea80fc;
  --md-purple-A200: #e040fb;
  --md-purple-A400: #d500f9;
  --md-purple-A700: #a0f;
  --md-deep-purple-50: #ede7f6;
  --md-deep-purple-100: #d1c4e9;
  --md-deep-purple-200: #b39ddb;
  --md-deep-purple-300: #9575cd;
  --md-deep-purple-400: #7e57c2;
  --md-deep-purple-500: #673ab7;
  --md-deep-purple-600: #5e35b1;
  --md-deep-purple-700: #512da8;
  --md-deep-purple-800: #4527a0;
  --md-deep-purple-900: #311b92;
  --md-deep-purple-A100: #b388ff;
  --md-deep-purple-A200: #7c4dff;
  --md-deep-purple-A400: #651fff;
  --md-deep-purple-A700: #6200ea;
  --md-indigo-50: #e8eaf6;
  --md-indigo-100: #c5cae9;
  --md-indigo-200: #9fa8da;
  --md-indigo-300: #7986cb;
  --md-indigo-400: #5c6bc0;
  --md-indigo-500: #3f51b5;
  --md-indigo-600: #3949ab;
  --md-indigo-700: #303f9f;
  --md-indigo-800: #283593;
  --md-indigo-900: #1a237e;
  --md-indigo-A100: #8c9eff;
  --md-indigo-A200: #536dfe;
  --md-indigo-A400: #3d5afe;
  --md-indigo-A700: #304ffe;
  --md-blue-50: #e3f2fd;
  --md-blue-100: #bbdefb;
  --md-blue-200: #90caf9;
  --md-blue-300: #64b5f6;
  --md-blue-400: #42a5f5;
  --md-blue-500: #2196f3;
  --md-blue-600: #1e88e5;
  --md-blue-700: #1976d2;
  --md-blue-800: #1565c0;
  --md-blue-900: #0d47a1;
  --md-blue-A100: #82b1ff;
  --md-blue-A200: #448aff;
  --md-blue-A400: #2979ff;
  --md-blue-A700: #2962ff;
  --md-light-blue-50: #e1f5fe;
  --md-light-blue-100: #b3e5fc;
  --md-light-blue-200: #81d4fa;
  --md-light-blue-300: #4fc3f7;
  --md-light-blue-400: #29b6f6;
  --md-light-blue-500: #03a9f4;
  --md-light-blue-600: #039be5;
  --md-light-blue-700: #0288d1;
  --md-light-blue-800: #0277bd;
  --md-light-blue-900: #01579b;
  --md-light-blue-A100: #80d8ff;
  --md-light-blue-A200: #40c4ff;
  --md-light-blue-A400: #00b0ff;
  --md-light-blue-A700: #0091ea;
  --md-cyan-50: #e0f7fa;
  --md-cyan-100: #b2ebf2;
  --md-cyan-200: #80deea;
  --md-cyan-300: #4dd0e1;
  --md-cyan-400: #26c6da;
  --md-cyan-500: #00bcd4;
  --md-cyan-600: #00acc1;
  --md-cyan-700: #0097a7;
  --md-cyan-800: #00838f;
  --md-cyan-900: #006064;
  --md-cyan-A100: #84ffff;
  --md-cyan-A200: #18ffff;
  --md-cyan-A400: #00e5ff;
  --md-cyan-A700: #00b8d4;
  --md-teal-50: #e0f2f1;
  --md-teal-100: #b2dfdb;
  --md-teal-200: #80cbc4;
  --md-teal-300: #4db6ac;
  --md-teal-400: #26a69a;
  --md-teal-500: #009688;
  --md-teal-600: #00897b;
  --md-teal-700: #00796b;
  --md-teal-800: #00695c;
  --md-teal-900: #004d40;
  --md-teal-A100: #a7ffeb;
  --md-teal-A200: #64ffda;
  --md-teal-A400: #1de9b6;
  --md-teal-A700: #00bfa5;
  --md-green-50: #e8f5e9;
  --md-green-100: #c8e6c9;
  --md-green-200: #a5d6a7;
  --md-green-300: #81c784;
  --md-green-400: #66bb6a;
  --md-green-500: #4caf50;
  --md-green-600: #43a047;
  --md-green-700: #388e3c;
  --md-green-800: #2e7d32;
  --md-green-900: #1b5e20;
  --md-green-A100: #b9f6ca;
  --md-green-A200: #69f0ae;
  --md-green-A400: #00e676;
  --md-green-A700: #00c853;
  --md-light-green-50: #f1f8e9;
  --md-light-green-100: #dcedc8;
  --md-light-green-200: #c5e1a5;
  --md-light-green-300: #aed581;
  --md-light-green-400: #9ccc65;
  --md-light-green-500: #8bc34a;
  --md-light-green-600: #7cb342;
  --md-light-green-700: #689f38;
  --md-light-green-800: #558b2f;
  --md-light-green-900: #33691e;
  --md-light-green-A100: #ccff90;
  --md-light-green-A200: #b2ff59;
  --md-light-green-A400: #76ff03;
  --md-light-green-A700: #64dd17;
  --md-lime-50: #f9fbe7;
  --md-lime-100: #f0f4c3;
  --md-lime-200: #e6ee9c;
  --md-lime-300: #dce775;
  --md-lime-400: #d4e157;
  --md-lime-500: #cddc39;
  --md-lime-600: #c0ca33;
  --md-lime-700: #afb42b;
  --md-lime-800: #9e9d24;
  --md-lime-900: #827717;
  --md-lime-A100: #f4ff81;
  --md-lime-A200: #eeff41;
  --md-lime-A400: #c6ff00;
  --md-lime-A700: #aeea00;
  --md-yellow-50: #fffde7;
  --md-yellow-100: #fff9c4;
  --md-yellow-200: #fff59d;
  --md-yellow-300: #fff176;
  --md-yellow-400: #ffee58;
  --md-yellow-500: #ffeb3b;
  --md-yellow-600: #fdd835;
  --md-yellow-700: #fbc02d;
  --md-yellow-800: #f9a825;
  --md-yellow-900: #f57f17;
  --md-yellow-A100: #ffff8d;
  --md-yellow-A200: #ff0;
  --md-yellow-A400: #ffea00;
  --md-yellow-A700: #ffd600;
  --md-amber-50: #fff8e1;
  --md-amber-100: #ffecb3;
  --md-amber-200: #ffe082;
  --md-amber-300: #ffd54f;
  --md-amber-400: #ffca28;
  --md-amber-500: #ffc107;
  --md-amber-600: #ffb300;
  --md-amber-700: #ffa000;
  --md-amber-800: #ff8f00;
  --md-amber-900: #ff6f00;
  --md-amber-A100: #ffe57f;
  --md-amber-A200: #ffd740;
  --md-amber-A400: #ffc400;
  --md-amber-A700: #ffab00;
  --md-orange-50: #fff3e0;
  --md-orange-100: #ffe0b2;
  --md-orange-200: #ffcc80;
  --md-orange-300: #ffb74d;
  --md-orange-400: #ffa726;
  --md-orange-500: #ff9800;
  --md-orange-600: #fb8c00;
  --md-orange-700: #f57c00;
  --md-orange-800: #ef6c00;
  --md-orange-900: #e65100;
  --md-orange-A100: #ffd180;
  --md-orange-A200: #ffab40;
  --md-orange-A400: #ff9100;
  --md-orange-A700: #ff6d00;
  --md-deep-orange-50: #fbe9e7;
  --md-deep-orange-100: #ffccbc;
  --md-deep-orange-200: #ffab91;
  --md-deep-orange-300: #ff8a65;
  --md-deep-orange-400: #ff7043;
  --md-deep-orange-500: #ff5722;
  --md-deep-orange-600: #f4511e;
  --md-deep-orange-700: #e64a19;
  --md-deep-orange-800: #d84315;
  --md-deep-orange-900: #bf360c;
  --md-deep-orange-A100: #ff9e80;
  --md-deep-orange-A200: #ff6e40;
  --md-deep-orange-A400: #ff3d00;
  --md-deep-orange-A700: #dd2c00;
  --md-brown-50: #efebe9;
  --md-brown-100: #d7ccc8;
  --md-brown-200: #bcaaa4;
  --md-brown-300: #a1887f;
  --md-brown-400: #8d6e63;
  --md-brown-500: #795548;
  --md-brown-600: #6d4c41;
  --md-brown-700: #5d4037;
  --md-brown-800: #4e342e;
  --md-brown-900: #3e2723;
  --md-grey-50: #fafafa;
  --md-grey-100: #f5f5f5;
  --md-grey-200: #eee;
  --md-grey-300: #e0e0e0;
  --md-grey-400: #bdbdbd;
  --md-grey-500: #9e9e9e;
  --md-grey-600: #757575;
  --md-grey-700: #616161;
  --md-grey-800: #424242;
  --md-grey-900: #212121;
  --md-blue-grey-50: #eceff1;
  --md-blue-grey-100: #cfd8dc;
  --md-blue-grey-200: #b0bec5;
  --md-blue-grey-300: #90a4ae;
  --md-blue-grey-400: #78909c;
  --md-blue-grey-500: #607d8b;
  --md-blue-grey-600: #546e7a;
  --md-blue-grey-700: #455a64;
  --md-blue-grey-800: #37474f;
  --md-blue-grey-900: #263238;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| RenderedText
|----------------------------------------------------------------------------*/

:root {
  /* This is the padding value to fill the gaps between lines containing spans with background color. */
  --jp-private-code-span-padding: calc(
    (var(--jp-code-line-height) - 1) * var(--jp-code-font-size) / 2
  );
}

.jp-RenderedText {
  text-align: left;
  padding-left: var(--jp-code-padding);
  line-height: var(--jp-code-line-height);
  font-family: var(--jp-code-font-family);
}

.jp-RenderedText pre,
.jp-RenderedJavaScript pre,
.jp-RenderedHTMLCommon pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
  border: none;
  margin: 0;
  padding: 0;
}

.jp-RenderedText pre a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* console foregrounds and backgrounds */
.jp-RenderedText pre .ansi-black-fg {
  color: #3e424d;
}

.jp-RenderedText pre .ansi-red-fg {
  color: #e75c58;
}

.jp-RenderedText pre .ansi-green-fg {
  color: #00a250;
}

.jp-RenderedText pre .ansi-yellow-fg {
  color: #ddb62b;
}

.jp-RenderedText pre .ansi-blue-fg {
  color: #208ffb;
}

.jp-RenderedText pre .ansi-magenta-fg {
  color: #d160c4;
}

.jp-RenderedText pre .ansi-cyan-fg {
  color: #60c6c8;
}

.jp-RenderedText pre .ansi-white-fg {
  color: #c5c1b4;
}

.jp-RenderedText pre .ansi-black-bg {
  background-color: #3e424d;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-bg {
  background-color: #e75c58;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-bg {
  background-color: #00a250;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-bg {
  background-color: #ddb62b;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-bg {
  background-color: #208ffb;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-bg {
  background-color: #d160c4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-bg {
  background-color: #60c6c8;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-bg {
  background-color: #c5c1b4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-black-intense-fg {
  color: #282c36;
}

.jp-RenderedText pre .ansi-red-intense-fg {
  color: #b22b31;
}

.jp-RenderedText pre .ansi-green-intense-fg {
  color: #007427;
}

.jp-RenderedText pre .ansi-yellow-intense-fg {
  color: #b27d12;
}

.jp-RenderedText pre .ansi-blue-intense-fg {
  color: #0065ca;
}

.jp-RenderedText pre .ansi-magenta-intense-fg {
  color: #a03196;
}

.jp-RenderedText pre .ansi-cyan-intense-fg {
  color: #258f8f;
}

.jp-RenderedText pre .ansi-white-intense-fg {
  color: #a1a6b2;
}

.jp-RenderedText pre .ansi-black-intense-bg {
  background-color: #282c36;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-intense-bg {
  background-color: #b22b31;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-intense-bg {
  background-color: #007427;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-intense-bg {
  background-color: #b27d12;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-intense-bg {
  background-color: #0065ca;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-intense-bg {
  background-color: #a03196;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-intense-bg {
  background-color: #258f8f;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-intense-bg {
  background-color: #a1a6b2;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-default-inverse-fg {
  color: var(--jp-ui-inverse-font-color0);
}

.jp-RenderedText pre .ansi-default-inverse-bg {
  background-color: var(--jp-inverse-layout-color0);
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-bold {
  font-weight: bold;
}

.jp-RenderedText pre .ansi-underline {
  text-decoration: underline;
}

.jp-RenderedText[data-mime-type='application/vnd.jupyter.stderr'] {
  background: var(--jp-rendermime-error-background);
  padding-top: var(--jp-code-padding);
}

/*-----------------------------------------------------------------------------
| RenderedLatex
|----------------------------------------------------------------------------*/

.jp-RenderedLatex {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);
}

/* Left-justify outputs.*/
.jp-OutputArea-output.jp-RenderedLatex {
  padding: var(--jp-code-padding);
  text-align: left;
}

/*-----------------------------------------------------------------------------
| RenderedHTML
|----------------------------------------------------------------------------*/

.jp-RenderedHTMLCommon {
  color: var(--jp-content-font-color1);
  font-family: var(--jp-content-font-family);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);

  /* Give a bit more R padding on Markdown text to keep line lengths reasonable */
  padding-right: 20px;
}

.jp-RenderedHTMLCommon em {
  font-style: italic;
}

.jp-RenderedHTMLCommon strong {
  font-weight: bold;
}

.jp-RenderedHTMLCommon u {
  text-decoration: underline;
}

.jp-RenderedHTMLCommon a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* Headings */

.jp-RenderedHTMLCommon h1,
.jp-RenderedHTMLCommon h2,
.jp-RenderedHTMLCommon h3,
.jp-RenderedHTMLCommon h4,
.jp-RenderedHTMLCommon h5,
.jp-RenderedHTMLCommon h6 {
  line-height: var(--jp-content-heading-line-height);
  font-weight: var(--jp-content-heading-font-weight);
  font-style: normal;
  margin: var(--jp-content-heading-margin-top) 0
    var(--jp-content-heading-margin-bottom) 0;
}

.jp-RenderedHTMLCommon h1:first-child,
.jp-RenderedHTMLCommon h2:first-child,
.jp-RenderedHTMLCommon h3:first-child,
.jp-RenderedHTMLCommon h4:first-child,
.jp-RenderedHTMLCommon h5:first-child,
.jp-RenderedHTMLCommon h6:first-child {
  margin-top: calc(0.5 * var(--jp-content-heading-margin-top));
}

.jp-RenderedHTMLCommon h1:last-child,
.jp-RenderedHTMLCommon h2:last-child,
.jp-RenderedHTMLCommon h3:last-child,
.jp-RenderedHTMLCommon h4:last-child,
.jp-RenderedHTMLCommon h5:last-child,
.jp-RenderedHTMLCommon h6:last-child {
  margin-bottom: calc(0.5 * var(--jp-content-heading-margin-bottom));
}

.jp-RenderedHTMLCommon h1 {
  font-size: var(--jp-content-font-size5);
}

.jp-RenderedHTMLCommon h2 {
  font-size: var(--jp-content-font-size4);
}

.jp-RenderedHTMLCommon h3 {
  font-size: var(--jp-content-font-size3);
}

.jp-RenderedHTMLCommon h4 {
  font-size: var(--jp-content-font-size2);
}

.jp-RenderedHTMLCommon h5 {
  font-size: var(--jp-content-font-size1);
}

.jp-RenderedHTMLCommon h6 {
  font-size: var(--jp-content-font-size0);
}

/* Lists */

/* stylelint-disable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon ul:not(.list-inline),
.jp-RenderedHTMLCommon ol:not(.list-inline) {
  padding-left: 2em;
}

.jp-RenderedHTMLCommon ul {
  list-style: disc;
}

.jp-RenderedHTMLCommon ul ul {
  list-style: square;
}

.jp-RenderedHTMLCommon ul ul ul {
  list-style: circle;
}

.jp-RenderedHTMLCommon ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol ol {
  list-style: upper-alpha;
}

.jp-RenderedHTMLCommon ol ol ol {
  list-style: lower-alpha;
}

.jp-RenderedHTMLCommon ol ol ol ol {
  list-style: lower-roman;
}

.jp-RenderedHTMLCommon ol ol ol ol ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol,
.jp-RenderedHTMLCommon ul {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon ul ul,
.jp-RenderedHTMLCommon ul ol,
.jp-RenderedHTMLCommon ol ul,
.jp-RenderedHTMLCommon ol ol {
  margin-bottom: 0;
}

/* stylelint-enable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon hr {
  color: var(--jp-border-color2);
  background-color: var(--jp-border-color1);
  margin-top: 1em;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon > pre {
  margin: 1.5em 2em;
}

.jp-RenderedHTMLCommon pre,
.jp-RenderedHTMLCommon code {
  border: 0;
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  line-height: var(--jp-code-line-height);
  padding: 0;
  white-space: pre-wrap;
}

.jp-RenderedHTMLCommon :not(pre) > code {
  background-color: var(--jp-layout-color2);
  padding: 1px 5px;
}

/* Tables */

.jp-RenderedHTMLCommon table {
  border-collapse: collapse;
  border-spacing: 0;
  border: none;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  table-layout: fixed;
  margin-left: auto;
  margin-bottom: 1em;
  margin-right: auto;
}

.jp-RenderedHTMLCommon thead {
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  vertical-align: bottom;
}

.jp-RenderedHTMLCommon td,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon tr {
  vertical-align: middle;
  padding: 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}

.jp-RenderedMarkdown.jp-RenderedHTMLCommon td,
.jp-RenderedMarkdown.jp-RenderedHTMLCommon th {
  max-width: none;
}

:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon td,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon th,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon tr {
  text-align: right;
}

.jp-RenderedHTMLCommon th {
  font-weight: bold;
}

.jp-RenderedHTMLCommon tbody tr:nth-child(odd) {
  background: var(--jp-layout-color0);
}

.jp-RenderedHTMLCommon tbody tr:nth-child(even) {
  background: var(--jp-rendermime-table-row-background);
}

.jp-RenderedHTMLCommon tbody tr:hover {
  background: var(--jp-rendermime-table-row-hover-background);
}

.jp-RenderedHTMLCommon p {
  text-align: left;
  margin: 0;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon img {
  -moz-force-broken-image-icon: 1;
}

/* Restrict to direct children as other images could be nested in other content. */
.jp-RenderedHTMLCommon > img {
  display: block;
  margin-left: 0;
  margin-right: 0;
  margin-bottom: 1em;
}

/* Change color behind transparent images if they need it... */
[data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-light-background {
  background-color: var(--jp-inverse-layout-color1);
}

[data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-dark-background {
  background-color: var(--jp-inverse-layout-color1);
}

.jp-RenderedHTMLCommon img,
.jp-RenderedImage img,
.jp-RenderedHTMLCommon svg,
.jp-RenderedSVG svg {
  max-width: 100%;
  height: auto;
}

.jp-RenderedHTMLCommon img.jp-mod-unconfined,
.jp-RenderedImage img.jp-mod-unconfined,
.jp-RenderedHTMLCommon svg.jp-mod-unconfined,
.jp-RenderedSVG svg.jp-mod-unconfined {
  max-width: none;
}

.jp-RenderedHTMLCommon .alert {
  padding: var(--jp-notebook-padding);
  border: var(--jp-border-width) solid transparent;
  border-radius: var(--jp-border-radius);
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon .alert-info {
  color: var(--jp-info-color0);
  background-color: var(--jp-info-color3);
  border-color: var(--jp-info-color2);
}

.jp-RenderedHTMLCommon .alert-info hr {
  border-color: var(--jp-info-color3);
}

.jp-RenderedHTMLCommon .alert-info > p:last-child,
.jp-RenderedHTMLCommon .alert-info > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-warning {
  color: var(--jp-warn-color0);
  background-color: var(--jp-warn-color3);
  border-color: var(--jp-warn-color2);
}

.jp-RenderedHTMLCommon .alert-warning hr {
  border-color: var(--jp-warn-color3);
}

.jp-RenderedHTMLCommon .alert-warning > p:last-child,
.jp-RenderedHTMLCommon .alert-warning > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-success {
  color: var(--jp-success-color0);
  background-color: var(--jp-success-color3);
  border-color: var(--jp-success-color2);
}

.jp-RenderedHTMLCommon .alert-success hr {
  border-color: var(--jp-success-color3);
}

.jp-RenderedHTMLCommon .alert-success > p:last-child,
.jp-RenderedHTMLCommon .alert-success > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-danger {
  color: var(--jp-error-color0);
  background-color: var(--jp-error-color3);
  border-color: var(--jp-error-color2);
}

.jp-RenderedHTMLCommon .alert-danger hr {
  border-color: var(--jp-error-color3);
}

.jp-RenderedHTMLCommon .alert-danger > p:last-child,
.jp-RenderedHTMLCommon .alert-danger > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon blockquote {
  margin: 1em 2em;
  padding: 0 1em;
  border-left: 5px solid var(--jp-border-color2);
}

a.jp-InternalAnchorLink {
  visibility: hidden;
  margin-left: 8px;
  color: var(--md-blue-800);
}

h1:hover .jp-InternalAnchorLink,
h2:hover .jp-InternalAnchorLink,
h3:hover .jp-InternalAnchorLink,
h4:hover .jp-InternalAnchorLink,
h5:hover .jp-InternalAnchorLink,
h6:hover .jp-InternalAnchorLink {
  visibility: visible;
}

.jp-RenderedHTMLCommon kbd {
  background-color: var(--jp-rendermime-table-row-background);
  border: 1px solid var(--jp-border-color0);
  border-bottom-color: var(--jp-border-color2);
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
  display: inline-block;
  font-size: var(--jp-ui-font-size0);
  line-height: 1em;
  padding: 0.2em 0.5em;
}

/* Most direct children of .jp-RenderedHTMLCommon have a margin-bottom of 1.0.
 * At the bottom of cells this is a bit too much as there is also spacing
 * between cells. Going all the way to 0 gets too tight between markdown and
 * code cells.
 */
.jp-RenderedHTMLCommon > *:last-child {
  margin-bottom: 0.5em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-cursor-backdrop {
  position: fixed;
  width: 200px;
  height: 200px;
  margin-top: -100px;
  margin-left: -100px;
  will-change: transform;
  z-index: 100;
}

.lm-mod-drag-image {
  will-change: transform;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-lineFormSearch {
  padding: 4px 12px;
  background-color: var(--jp-layout-color2);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
  font-size: var(--jp-ui-font-size1);
}

.jp-lineFormCaption {
  font-size: var(--jp-ui-font-size0);
  line-height: var(--jp-ui-font-size1);
  margin-top: 4px;
  color: var(--jp-ui-font-color0);
}

.jp-baseLineForm {
  border: none;
  border-radius: 0;
  position: absolute;
  background-size: 16px;
  background-repeat: no-repeat;
  background-position: center;
  outline: none;
}

.jp-lineFormButtonContainer {
  top: 4px;
  right: 8px;
  height: 24px;
  padding: 0 12px;
  width: 12px;
}

.jp-lineFormButtonIcon {
  top: 0;
  right: 0;
  background-color: var(--jp-brand-color1);
  height: 100%;
  width: 100%;
  box-sizing: border-box;
  padding: 4px 6px;
}

.jp-lineFormButton {
  top: 0;
  right: 0;
  background-color: transparent;
  height: 100%;
  width: 100%;
  box-sizing: border-box;
}

.jp-lineFormWrapper {
  overflow: hidden;
  padding: 0 8px;
  border: 1px solid var(--jp-border-color0);
  background-color: var(--jp-input-active-background);
  height: 22px;
}

.jp-lineFormWrapperFocusWithin {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-lineFormInput {
  background: transparent;
  width: 200px;
  height: 100%;
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  line-height: 28px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-JSONEditor {
  display: flex;
  flex-direction: column;
  width: 100%;
}

.jp-JSONEditor-host {
  flex: 1 1 auto;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  background: var(--jp-layout-color0);
  min-height: 50px;
  padding: 1px;
}

.jp-JSONEditor.jp-mod-error .jp-JSONEditor-host {
  border-color: red;
  outline-color: red;
}

.jp-JSONEditor-header {
  display: flex;
  flex: 1 0 auto;
  padding: 0 0 0 12px;
}

.jp-JSONEditor-header label {
  flex: 0 0 auto;
}

.jp-JSONEditor-commitButton {
  height: 16px;
  width: 16px;
  background-size: 18px;
  background-repeat: no-repeat;
  background-position: center;
}

.jp-JSONEditor-host.jp-mod-focused {
  background-color: var(--jp-input-active-background);
  border: 1px solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

.jp-Editor.jp-mod-dropTarget {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/
.jp-DocumentSearch-input {
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  font-size: var(--jp-ui-font-size1);
  background-color: var(--jp-layout-color0);
  font-family: var(--jp-ui-font-family);
  padding: 2px 1px;
  resize: none;
}

.jp-DocumentSearch-overlay {
  position: absolute;
  background-color: var(--jp-toolbar-background);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  border-left: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  top: 0;
  right: 0;
  z-index: 7;
  min-width: 405px;
  padding: 2px;
  font-size: var(--jp-ui-font-size1);

  --jp-private-document-search-button-height: 20px;
}

.jp-DocumentSearch-overlay button {
  background-color: var(--jp-toolbar-background);
  outline: 0;
}

.jp-DocumentSearch-overlay button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-overlay button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-overlay-row {
  display: flex;
  align-items: center;
  margin-bottom: 2px;
}

.jp-DocumentSearch-button-content {
  display: inline-block;
  cursor: pointer;
  box-sizing: border-box;
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-button-content svg {
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-input-wrapper {
  border: var(--jp-border-width) solid var(--jp-border-color0);
  display: flex;
  background-color: var(--jp-layout-color0);
  margin: 2px;
}

.jp-DocumentSearch-input-wrapper:focus-within {
  border-color: var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper {
  all: initial;
  overflow: hidden;
  display: inline-block;
  border: none;
  box-sizing: border-box;
}

.jp-DocumentSearch-toggle-wrapper {
  width: 14px;
  height: 14px;
}

.jp-DocumentSearch-button-wrapper {
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
}

.jp-DocumentSearch-toggle-wrapper:focus,
.jp-DocumentSearch-button-wrapper:focus {
  outline: var(--jp-border-width) solid
    var(--jp-cell-editor-active-border-color);
  outline-offset: -1px;
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper,
.jp-DocumentSearch-button-content:focus {
  outline: none;
}

.jp-DocumentSearch-toggle-placeholder {
  width: 5px;
}

.jp-DocumentSearch-input-button::before {
  display: block;
  padding-top: 100%;
}

.jp-DocumentSearch-input-button-off {
  opacity: var(--jp-search-toggle-off-opacity);
}

.jp-DocumentSearch-input-button-off:hover {
  opacity: var(--jp-search-toggle-hover-opacity);
}

.jp-DocumentSearch-input-button-on {
  opacity: var(--jp-search-toggle-on-opacity);
}

.jp-DocumentSearch-index-counter {
  padding-left: 10px;
  padding-right: 10px;
  user-select: none;
  min-width: 35px;
  display: inline-block;
}

.jp-DocumentSearch-up-down-wrapper {
  display: inline-block;
  padding-right: 2px;
  margin-left: auto;
  white-space: nowrap;
}

.jp-DocumentSearch-spacer {
  margin-left: auto;
}

.jp-DocumentSearch-up-down-wrapper button {
  outline: 0;
  border: none;
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
  vertical-align: middle;
  margin: 1px 5px 2px;
}

.jp-DocumentSearch-up-down-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-up-down-button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-filter-button {
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-filter-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled:hover {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-search-options {
  padding: 0 8px;
  margin-left: 3px;
  width: 100%;
  display: grid;
  justify-content: start;
  grid-template-columns: 1fr 1fr;
  align-items: center;
  justify-items: stretch;
}

.jp-DocumentSearch-search-filter-disabled {
  color: var(--jp-ui-font-color2);
}

.jp-DocumentSearch-search-filter {
  display: flex;
  align-items: center;
  user-select: none;
}

.jp-DocumentSearch-regex-error {
  color: var(--jp-error-color0);
}

.jp-DocumentSearch-replace-button-wrapper {
  overflow: hidden;
  display: inline-block;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color0);
  margin: auto 2px;
  padding: 1px 4px;
  height: calc(var(--jp-private-document-search-button-height) + 2px);
}

.jp-DocumentSearch-replace-button-wrapper:focus {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-replace-button {
  display: inline-block;
  text-align: center;
  cursor: pointer;
  box-sizing: border-box;
  color: var(--jp-ui-font-color1);

  /* height - 2 * (padding of wrapper) */
  line-height: calc(var(--jp-private-document-search-button-height) - 2px);
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-replace-button:focus {
  outline: none;
}

.jp-DocumentSearch-replace-wrapper-class {
  margin-left: 14px;
  display: flex;
}

.jp-DocumentSearch-replace-toggle {
  border: none;
  background-color: var(--jp-toolbar-background);
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-replace-toggle:hover {
  background-color: var(--jp-layout-color2);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.cm-editor {
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  border: 0;
  border-radius: 0;
  height: auto;

  /* Changed to auto to autogrow */
}

.cm-editor pre {
  padding: 0 var(--jp-code-padding);
}

.jp-CodeMirrorEditor[data-type='inline'] .cm-dialog {
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
}

.jp-CodeMirrorEditor {
  cursor: text;
}

/* When zoomed out 67% and 33% on a screen of 1440 width x 900 height */
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width1) solid
      var(--jp-editor-cursor-color);
  }
}

/* When zoomed out less than 33% */
@media screen and (min-width: 4320px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width2) solid
      var(--jp-editor-cursor-color);
  }
}

.cm-editor.jp-mod-readOnly .cm-cursor {
  display: none;
}

.jp-CollaboratorCursor {
  border-left: 5px solid transparent;
  border-right: 5px solid transparent;
  border-top: none;
  border-bottom: 3px solid;
  background-clip: content-box;
  margin-left: -5px;
  margin-right: -5px;
}

.cm-searching,
.cm-searching span {
  /* `.cm-searching span`: we need to override syntax highlighting */
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.cm-searching::selection,
.cm-searching span::selection {
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.jp-current-match > .cm-searching,
.jp-current-match > .cm-searching span,
.cm-searching > .jp-current-match,
.cm-searching > .jp-current-match span {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.jp-current-match > .cm-searching::selection,
.cm-searching > .jp-current-match::selection,
.jp-current-match > .cm-searching span::selection {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.cm-trailingspace {
  background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAYAAAB4ka1VAAAAsElEQVQIHQGlAFr/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7+r3zKmT0/+pk9P/7+r3zAAAAAAAAAAABAAAAAAAAAAA6OPzM+/q9wAAAAAA6OPzMwAAAAAAAAAAAgAAAAAAAAAAGR8NiRQaCgAZIA0AGR8NiQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQyoYJ/SY80UAAAAASUVORK5CYII=);
  background-position: center left;
  background-repeat: repeat-x;
}

.jp-CollaboratorCursor-hover {
  position: absolute;
  z-index: 1;
  transform: translateX(-50%);
  color: white;
  border-radius: 3px;
  padding-left: 4px;
  padding-right: 4px;
  padding-top: 1px;
  padding-bottom: 1px;
  text-align: center;
  font-size: var(--jp-ui-font-size1);
  white-space: nowrap;
}

.jp-CodeMirror-ruler {
  border-left: 1px dashed var(--jp-border-color2);
}

/* Styles for shared cursors (remote cursor locations and selected ranges) */
.jp-CodeMirrorEditor .cm-ySelectionCaret {
  position: relative;
  border-left: 1px solid black;
  margin-left: -1px;
  margin-right: -1px;
  box-sizing: border-box;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret > .cm-ySelectionInfo {
  white-space: nowrap;
  position: absolute;
  top: -1.15em;
  padding-bottom: 0.05em;
  left: -1px;
  font-size: 0.95em;
  font-family: var(--jp-ui-font-family);
  font-weight: bold;
  line-height: normal;
  user-select: none;
  color: white;
  padding-left: 2px;
  padding-right: 2px;
  z-index: 101;
  transition: opacity 0.3s ease-in-out;
}

.jp-CodeMirrorEditor .cm-ySelectionInfo {
  transition-delay: 0.7s;
  opacity: 0;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret:hover > .cm-ySelectionInfo {
  opacity: 1;
  transition-delay: 0s;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MimeDocument {
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-filebrowser-button-height: 28px;
  --jp-private-filebrowser-button-width: 48px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FileBrowser .jp-SidePanel-content {
  display: flex;
  flex-direction: column;
}

.jp-FileBrowser-toolbar.jp-Toolbar {
  flex-wrap: wrap;
  row-gap: 12px;
  border-bottom: none;
  height: auto;
  margin: 8px 12px 0;
  box-shadow: none;
  padding: 0;
  justify-content: flex-start;
}

.jp-FileBrowser-Panel {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
}

.jp-BreadCrumbs {
  flex: 0 0 auto;
  margin: 8px 12px;
}

.jp-BreadCrumbs-item {
  margin: 0 2px;
  padding: 0 2px;
  border-radius: var(--jp-border-radius);
  cursor: pointer;
}

.jp-BreadCrumbs-item:hover {
  background-color: var(--jp-layout-color2);
}

.jp-BreadCrumbs-item:first-child {
  margin-left: 0;
}

.jp-BreadCrumbs-item.jp-mod-dropTarget {
  background-color: var(--jp-brand-color2);
  opacity: 0.7;
}

/*-----------------------------------------------------------------------------
| Buttons
|----------------------------------------------------------------------------*/

.jp-FileBrowser-toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  padding-left: 0;
  padding-right: 2px;
  align-items: center;
  height: unset;
}

.jp-FileBrowser-toolbar > .jp-Toolbar-item .jp-ToolbarButtonComponent {
  width: 40px;
}

/*-----------------------------------------------------------------------------
| Other styles
|----------------------------------------------------------------------------*/

.jp-FileDialog.jp-mod-conflict input {
  color: var(--jp-error-color1);
}

.jp-FileDialog .jp-new-name-title {
  margin-top: 12px;
}

.jp-LastModified-hidden {
  display: none;
}

.jp-FileSize-hidden {
  display: none;
}

.jp-FileBrowser .lm-AccordionPanel > h3:first-child {
  display: none;
}

/*-----------------------------------------------------------------------------
| DirListing
|----------------------------------------------------------------------------*/

.jp-DirListing {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
  outline: 0;
}

.jp-DirListing-header {
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  align-items: center;
  overflow: hidden;
  border-top: var(--jp-border-width) solid var(--jp-border-color2);
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
}

.jp-DirListing-headerItem {
  padding: 4px 12px 2px;
  font-weight: 500;
}

.jp-DirListing-headerItem:hover {
  background: var(--jp-layout-color2);
}

.jp-DirListing-headerItem.jp-id-name {
  flex: 1 0 84px;
}

.jp-DirListing-headerItem.jp-id-modified {
  flex: 0 0 112px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-DirListing-headerItem.jp-id-filesize {
  flex: 0 0 75px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-id-narrow {
  display: none;
  flex: 0 0 5px;
  padding: 4px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
  color: var(--jp-border-color2);
}

.jp-DirListing-narrow .jp-id-narrow {
  display: block;
}

.jp-DirListing-narrow .jp-id-modified,
.jp-DirListing-narrow .jp-DirListing-itemModified {
  display: none;
}

.jp-DirListing-headerItem.jp-mod-selected {
  font-weight: 600;
}

/* increase specificity to override bundled default */
.jp-DirListing-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  list-style-type: none;
  overflow: auto;
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-content mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.jp-DirListing-content .jp-DirListing-item.jp-mod-selected mark {
  color: var(--jp-ui-inverse-font-color0);
}

/* Style the directory listing content when a user drops a file to upload */
.jp-DirListing.jp-mod-native-drop .jp-DirListing-content {
  outline: 5px dashed rgba(128, 128, 128, 0.5);
  outline-offset: -10px;
  cursor: copy;
}

.jp-DirListing-item {
  display: flex;
  flex-direction: row;
  align-items: center;
  padding: 4px 12px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-DirListing-checkboxWrapper {
  /* Increases hit area of checkbox. */
  padding: 4px;
}

.jp-DirListing-header
  .jp-DirListing-checkboxWrapper
  + .jp-DirListing-headerItem {
  padding-left: 4px;
}

.jp-DirListing-content .jp-DirListing-checkboxWrapper {
  position: relative;
  left: -4px;
  margin: -4px 0 -4px -8px;
}

.jp-DirListing-checkboxWrapper.jp-mod-visible {
  visibility: visible;
}

/* For devices that support hovering, hide checkboxes until hovered, selected...
*/
@media (hover: hover) {
  .jp-DirListing-checkboxWrapper {
    visibility: hidden;
  }

  .jp-DirListing-item:hover .jp-DirListing-checkboxWrapper,
  .jp-DirListing-item.jp-mod-selected .jp-DirListing-checkboxWrapper {
    visibility: visible;
  }
}

.jp-DirListing-item[data-is-dot] {
  opacity: 75%;
}

.jp-DirListing-item.jp-mod-selected {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.jp-DirListing-item.jp-mod-dropTarget {
  background: var(--jp-brand-color3);
}

.jp-DirListing-item:hover:not(.jp-mod-selected) {
  background: var(--jp-layout-color2);
}

.jp-DirListing-itemIcon {
  flex: 0 0 20px;
  margin-right: 4px;
}

.jp-DirListing-itemText {
  flex: 1 0 64px;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  user-select: none;
}

.jp-DirListing-itemText:focus {
  outline-width: 2px;
  outline-color: var(--jp-inverse-layout-color1);
  outline-style: solid;
  outline-offset: 1px;
}

.jp-DirListing-item.jp-mod-selected .jp-DirListing-itemText:focus {
  outline-color: var(--jp-layout-color1);
}

.jp-DirListing-itemModified {
  flex: 0 0 125px;
  text-align: right;
}

.jp-DirListing-itemFileSize {
  flex: 0 0 90px;
  text-align: right;
}

.jp-DirListing-editor {
  flex: 1 0 64px;
  outline: none;
  border: none;
  color: var(--jp-ui-font-color1);
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon::before {
  color: var(--jp-success-color1);
  content: '\25CF';
  font-size: 8px;
  position: absolute;
  left: -8px;
}

.jp-DirListing-item.jp-mod-running.jp-mod-selected
  .jp-DirListing-itemIcon::before {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-DirListing-item.lm-mod-drag-image,
.jp-DirListing-item.jp-mod-selected.lm-mod-drag-image {
  font-size: var(--jp-ui-font-size1);
  padding-left: 4px;
  margin-left: 4px;
  width: 160px;
  background-color: var(--jp-ui-inverse-font-color2);
  box-shadow: var(--jp-elevation-z2);
  border-radius: 0;
  color: var(--jp-ui-font-color1);
  transform: translateX(-40%) translateY(-58%);
}

.jp-Document {
  min-width: 120px;
  min-height: 120px;
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Main OutputArea
| OutputArea has a list of Outputs
|----------------------------------------------------------------------------*/

.jp-OutputArea {
  overflow-y: auto;
}

.jp-OutputArea-child {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-OutputPrompt {
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-outprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
  opacity: var(--jp-cell-prompt-opacity);

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-OutputArea-prompt {
  display: table-cell;
  vertical-align: top;
}

.jp-OutputArea-output {
  display: table-cell;
  width: 100%;
  height: auto;
  overflow: auto;
  user-select: text;
  -moz-user-select: text;
  -webkit-user-select: text;
  -ms-user-select: text;
}

.jp-OutputArea .jp-RenderedText {
  padding-left: 1ch;
}

/**
 * Prompt overlay.
 */

.jp-OutputArea-promptOverlay {
  position: absolute;
  top: 0;
  width: var(--jp-cell-prompt-width);
  height: 100%;
  opacity: 0.5;
}

.jp-OutputArea-promptOverlay:hover {
  background: var(--jp-layout-color2);
  box-shadow: inset 0 0 1px var(--jp-inverse-layout-color0);
  cursor: zoom-out;
}

.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay:hover {
  cursor: zoom-in;
}

/**
 * Isolated output.
 */
.jp-OutputArea-output.jp-mod-isolated {
  width: 100%;
  display: block;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated {
  position: relative;
}

body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/* pre */

.jp-OutputArea-output pre {
  border: none;
  margin: 0;
  padding: 0;
  overflow-x: auto;
  overflow-y: auto;
  word-break: break-all;
  word-wrap: break-word;
  white-space: pre-wrap;
}

/* tables */

.jp-OutputArea-output.jp-RenderedHTMLCommon table {
  margin-left: 0;
  margin-right: 0;
}

/* description lists */

.jp-OutputArea-output dl,
.jp-OutputArea-output dt,
.jp-OutputArea-output dd {
  display: block;
}

.jp-OutputArea-output dl {
  width: 100%;
  overflow: hidden;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dt {
  font-weight: bold;
  float: left;
  width: 20%;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dd {
  float: left;
  width: 80%;
  padding: 0;
  margin: 0;
}

.jp-TrimmedOutputs pre {
  background: var(--jp-layout-color3);
  font-size: calc(var(--jp-code-font-size) * 1.4);
  text-align: center;
  text-transform: uppercase;
}

/* Hide the gutter in case of
 *  - nested output areas (e.g. in the case of output widgets)
 *  - mirrored output areas
 */
.jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt {
  display: none;
}

/* Hide empty lines in the output area, for instance due to cleared widgets */
.jp-OutputArea-prompt:empty {
  padding: 0;
  border: 0;
}

/*-----------------------------------------------------------------------------
| executeResult is added to any Output-result for the display of the object
| returned by a cell
|----------------------------------------------------------------------------*/

.jp-OutputArea-output.jp-OutputArea-executeResult {
  margin-left: 0;
  width: 100%;
}

/* Text output with the Out[] prompt needs a top padding to match the
 * alignment of the Out[] prompt itself.
 */
.jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output {
  padding-top: var(--jp-code-padding);
  border-top: var(--jp-border-width) solid transparent;
}

/*-----------------------------------------------------------------------------
| The Stdin output
|----------------------------------------------------------------------------*/

.jp-Stdin-prompt {
  color: var(--jp-content-font-color0);
  padding-right: var(--jp-code-padding);
  vertical-align: baseline;
  flex: 0 0 auto;
}

.jp-Stdin-input {
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  color: inherit;
  background-color: inherit;
  width: 42%;
  min-width: 200px;

  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;

  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0 0.25em;
  margin: 0 0.25em;
  flex: 0 0 70%;
}

.jp-Stdin-input::placeholder {
  opacity: 0;
}

.jp-Stdin-input:focus {
  box-shadow: none;
}

.jp-Stdin-input:focus::placeholder {
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Output Area View
|----------------------------------------------------------------------------*/

.jp-LinkedOutputView .jp-OutputArea {
  height: 100%;
  display: block;
}

.jp-LinkedOutputView .jp-OutputArea-output:only-child {
  height: 100%;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

@media print {
  .jp-OutputArea-child {
    break-inside: avoid-page;
  }
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-OutputPrompt {
    display: table-row;
    text-align: left;
  }

  .jp-OutputArea-child .jp-OutputArea-output {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }
}

/* Trimmed outputs warning */
.jp-TrimmedOutputs > a {
  margin: 10px;
  text-decoration: none;
  cursor: pointer;
}

.jp-TrimmedOutputs > a:hover {
  text-decoration: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Table of Contents
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toc-active-width: 4px;
}

.jp-TableOfContents {
  display: flex;
  flex-direction: column;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  height: 100%;
}

.jp-TableOfContents-placeholder {
  text-align: center;
}

.jp-TableOfContents-placeholderContent {
  color: var(--jp-content-font-color2);
  padding: 8px;
}

.jp-TableOfContents-placeholderContent > h3 {
  margin-bottom: var(--jp-content-heading-margin-bottom);
}

.jp-TableOfContents .jp-SidePanel-content {
  overflow-y: auto;
}

.jp-TableOfContents-tree {
  margin: 4px;
}

.jp-TableOfContents ol {
  list-style-type: none;
}

/* stylelint-disable-next-line selector-max-type */
.jp-TableOfContents li > ol {
  /* Align left border with triangle icon center */
  padding-left: 11px;
}

.jp-TableOfContents-content {
  /* left margin for the active heading indicator */
  margin: 0 0 0 var(--jp-private-toc-active-width);
  padding: 0;
  background-color: var(--jp-layout-color1);
}

.jp-tocItem {
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-tocItem-heading {
  display: flex;
  cursor: pointer;
}

.jp-tocItem-heading:hover {
  background-color: var(--jp-layout-color2);
}

.jp-tocItem-content {
  display: block;
  padding: 4px 0;
  white-space: nowrap;
  text-overflow: ellipsis;
  overflow-x: hidden;
}

.jp-tocItem-collapser {
  height: 20px;
  margin: 2px 2px 0;
  padding: 0;
  background: none;
  border: none;
  cursor: pointer;
}

.jp-tocItem-collapser:hover {
  background-color: var(--jp-layout-color3);
}

/* Active heading indicator */

.jp-tocItem-heading::before {
  content: ' ';
  background: transparent;
  width: var(--jp-private-toc-active-width);
  height: 24px;
  position: absolute;
  left: 0;
  border-radius: var(--jp-border-radius);
}

.jp-tocItem-heading.jp-tocItem-active::before {
  background-color: var(--jp-brand-color1);
}

.jp-tocItem-heading:hover.jp-tocItem-active::before {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapser {
  flex: 0 0 var(--jp-cell-collapser-width);
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
  border-radius: var(--jp-border-radius);
  opacity: 1;
}

.jp-Collapser-child {
  display: block;
  width: 100%;
  box-sizing: border-box;

  /* height: 100% doesn't work because the height of its parent is computed from content */
  position: absolute;
  top: 0;
  bottom: 0;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Hiding collapsers in print mode.

Note: input and output wrappers have "display: block" propery in print mode.
*/

@media print {
  .jp-Collapser {
    display: none;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Header/Footer
|----------------------------------------------------------------------------*/

/* Hidden by zero height by default */
.jp-CellHeader,
.jp-CellFooter {
  height: 0;
  width: 100%;
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Input
|----------------------------------------------------------------------------*/

/* All input areas */
.jp-InputArea {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-InputArea-editor {
  display: table-cell;
  overflow: hidden;
  vertical-align: top;

  /* This is the non-active, default styling */
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  background: var(--jp-cell-editor-background);
}

.jp-InputPrompt {
  display: table-cell;
  vertical-align: top;
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-inprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  opacity: var(--jp-cell-prompt-opacity);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-InputArea-editor {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }

  .jp-InputPrompt {
    display: table-row;
    text-align: left;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Placeholder {
  display: table;
  table-layout: fixed;
  width: 100%;
}

.jp-Placeholder-prompt {
  display: table-cell;
  box-sizing: border-box;
}

.jp-Placeholder-content {
  display: table-cell;
  padding: 4px 6px;
  border: 1px solid transparent;
  border-radius: 0;
  background: none;
  box-sizing: border-box;
  cursor: pointer;
}

.jp-Placeholder-contentContainer {
  display: flex;
}

.jp-Placeholder-content:hover,
.jp-InputPlaceholder > .jp-Placeholder-content:hover {
  border-color: var(--jp-layout-color3);
}

.jp-Placeholder-content .jp-MoreHorizIcon {
  width: 32px;
  height: 16px;
  border: 1px solid transparent;
  border-radius: var(--jp-border-radius);
}

.jp-Placeholder-content .jp-MoreHorizIcon:hover {
  border: 1px solid var(--jp-border-color1);
  box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.25);
  background-color: var(--jp-layout-color0);
}

.jp-PlaceholderText {
  white-space: nowrap;
  overflow-x: hidden;
  color: var(--jp-inverse-layout-color3);
  font-family: var(--jp-code-font-family);
}

.jp-InputPlaceholder > .jp-Placeholder-content {
  border-color: var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-cell-scrolling-output-offset: 5px;
}

/*-----------------------------------------------------------------------------
| Cell
|----------------------------------------------------------------------------*/

.jp-Cell {
  padding: var(--jp-cell-padding);
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Common input/output
|----------------------------------------------------------------------------*/

.jp-Cell-inputWrapper,
.jp-Cell-outputWrapper {
  display: flex;
  flex-direction: row;
  padding: 0;
  margin: 0;

  /* Added to reveal the box-shadow on the input and output collapsers. */
  overflow: visible;
}

/* Only input/output areas inside cells */
.jp-Cell-inputArea,
.jp-Cell-outputArea {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Collapser
|----------------------------------------------------------------------------*/

/* Make the output collapser disappear when there is not output, but do so
 * in a manner that leaves it in the layout and preserves its width.
 */
.jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser {
  border: none !important;
  background: transparent !important;
}

.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser {
  min-height: var(--jp-cell-collapser-min-height);
}

/*-----------------------------------------------------------------------------
| Output
|----------------------------------------------------------------------------*/

/* Put a space between input and output when there IS output */
.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper {
  margin-top: 5px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea {
  overflow-y: auto;
  max-height: 24em;
  margin-left: var(--jp-private-cell-scrolling-output-offset);
  resize: vertical;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea[style*='height'] {
  max-height: unset;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea::after {
  content: ' ';
  box-shadow: inset 0 0 6px 2px rgb(0 0 0 / 30%);
  width: 100%;
  height: 100%;
  position: sticky;
  bottom: 0;
  top: 0;
  margin-top: -50%;
  float: left;
  display: block;
  pointer-events: none;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-child {
  padding-top: 6px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt {
  width: calc(
    var(--jp-cell-prompt-width) - var(--jp-private-cell-scrolling-output-offset)
  );
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay {
  left: calc(-1 * var(--jp-private-cell-scrolling-output-offset));
}

/*-----------------------------------------------------------------------------
| CodeCell
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| MarkdownCell
|----------------------------------------------------------------------------*/

.jp-MarkdownOutput {
  display: table-cell;
  width: 100%;
  margin-top: 0;
  margin-bottom: 0;
  padding-left: var(--jp-code-padding);
}

.jp-MarkdownOutput.jp-RenderedHTMLCommon {
  overflow: auto;
}

/* collapseHeadingButton (show always if hiddenCellsButton is _not_ shown) */
.jp-collapseHeadingButton {
  display: flex;
  min-height: var(--jp-cell-collapser-min-height);
  font-size: var(--jp-code-font-size);
  position: absolute;
  background-color: transparent;
  background-size: 25px;
  background-repeat: no-repeat;
  background-position-x: center;
  background-position-y: top;
  background-image: var(--jp-icon-caret-down);
  right: 0;
  top: 0;
  bottom: 0;
}

.jp-collapseHeadingButton.jp-mod-collapsed {
  background-image: var(--jp-icon-caret-right);
}

/*
 set the container font size to match that of content
 so that the nested collapse buttons have the right size
*/
.jp-MarkdownCell .jp-InputPrompt {
  font-size: var(--jp-content-font-size1);
}

/*
  Align collapseHeadingButton with cell top header
  The font sizes are identical to the ones in packages/rendermime/style/base.css
*/
.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='1'] {
  font-size: var(--jp-content-font-size5);
  background-position-y: calc(0.3 * var(--jp-content-font-size5));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='2'] {
  font-size: var(--jp-content-font-size4);
  background-position-y: calc(0.3 * var(--jp-content-font-size4));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='3'] {
  font-size: var(--jp-content-font-size3);
  background-position-y: calc(0.3 * var(--jp-content-font-size3));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='4'] {
  font-size: var(--jp-content-font-size2);
  background-position-y: calc(0.3 * var(--jp-content-font-size2));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='5'] {
  font-size: var(--jp-content-font-size1);
  background-position-y: top;
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='6'] {
  font-size: var(--jp-content-font-size0);
  background-position-y: top;
}

/* collapseHeadingButton (show only on (hover,active) if hiddenCellsButton is shown) */
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-collapseHeadingButton {
  display: none;
}

.jp-Notebook.jp-mod-showHiddenCellsButton
  :is(.jp-MarkdownCell:hover, .jp-mod-active)
  .jp-collapseHeadingButton {
  display: flex;
}

/* showHiddenCellsButton (only show if jp-mod-showHiddenCellsButton is set, which
is a consequence of the showHiddenCellsButton option in Notebook Settings)*/
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton {
  margin-left: calc(var(--jp-cell-prompt-width) + 2 * var(--jp-code-padding));
  margin-top: var(--jp-code-padding);
  border: 1px solid var(--jp-border-color2);
  background-color: var(--jp-border-color3) !important;
  color: var(--jp-content-font-color0) !important;
  display: flex;
}

.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton:hover {
  background-color: var(--jp-border-color2) !important;
}

.jp-showHiddenCellsButton {
  display: none;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Using block instead of flex to allow the use of the break-inside CSS property for
cell outputs.
*/

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-notebook-toolbar-padding: 2px 5px 2px 2px;
}

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-NotebookPanel-toolbar {
  padding: var(--jp-notebook-toolbar-padding);

  /* disable paint containment from lumino 2.0 default strict CSS containment */
  contain: style size !important;
}

.jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused {
  border: none;
  box-shadow: none;
}

.jp-Notebook-toolbarCellTypeDropdown select {
  height: 24px;
  font-size: var(--jp-ui-font-size1);
  line-height: 14px;
  border-radius: 0;
  display: block;
}

.jp-Notebook-toolbarCellTypeDropdown span {
  top: 5px !important;
}

.jp-Toolbar-responsive-popup {
  position: absolute;
  height: fit-content;
  display: flex;
  flex-direction: row;
  flex-wrap: wrap;
  justify-content: flex-end;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: var(--jp-notebook-toolbar-padding);
  z-index: 1;
  right: 0;
  top: 0;
}

.jp-Toolbar > .jp-Toolbar-responsive-opener {
  margin-left: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-Notebook-ExecutionIndicator {
  position: relative;
  display: inline-block;
  height: 100%;
  z-index: 9997;
}

.jp-Notebook-ExecutionIndicator-tooltip {
  visibility: hidden;
  height: auto;
  width: max-content;
  width: -moz-max-content;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color1);
  text-align: justify;
  border-radius: 6px;
  padding: 0 5px;
  position: fixed;
  display: table;
}

.jp-Notebook-ExecutionIndicator-tooltip.up {
  transform: translateX(-50%) translateY(-100%) translateY(-32px);
}

.jp-Notebook-ExecutionIndicator-tooltip.down {
  transform: translateX(calc(-100% + 16px)) translateY(5px);
}

.jp-Notebook-ExecutionIndicator-tooltip.hidden {
  display: none;
}

.jp-Notebook-ExecutionIndicator:hover .jp-Notebook-ExecutionIndicator-tooltip {
  visibility: visible;
}

.jp-Notebook-ExecutionIndicator span {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  color: var(--jp-ui-font-color1);
  line-height: 24px;
  display: block;
}

.jp-Notebook-ExecutionIndicator-progress-bar {
  display: flex;
  justify-content: center;
  height: 100%;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*
 * Execution indicator
 */
.jp-tocItem-content::after {
  content: '';

  /* Must be identical to form a circle */
  width: 12px;
  height: 12px;
  background: none;
  border: none;
  position: absolute;
  right: 0;
}

.jp-tocItem-content[data-running='0']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background: none;
}

.jp-tocItem-content[data-running='1']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background-color: var(--jp-inverse-layout-color3);
}

.jp-tocItem-content[data-running='0'],
.jp-tocItem-content[data-running='1'] {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Notebook-footer {
  height: 27px;
  margin-left: calc(
    var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
      var(--jp-cell-padding)
  );
  width: calc(
    100% -
      (
        var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
          var(--jp-cell-padding) + var(--jp-cell-padding)
      )
  );
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  color: var(--jp-ui-font-color3);
  margin-top: 6px;
  background: none;
  cursor: pointer;
}

.jp-Notebook-footer:focus {
  border-color: var(--jp-cell-editor-active-border-color);
}

/* For devices that support hovering, hide footer until hover */
@media (hover: hover) {
  .jp-Notebook-footer {
    opacity: 0;
  }

  .jp-Notebook-footer:focus,
  .jp-Notebook-footer:hover {
    opacity: 1;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Imports
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-side-by-side-output-size: 1fr;
  --jp-side-by-side-resized-cell: var(--jp-side-by-side-output-size);
  --jp-private-notebook-dragImage-width: 304px;
  --jp-private-notebook-dragImage-height: 36px;
  --jp-private-notebook-selected-color: var(--md-blue-400);
  --jp-private-notebook-active-color: var(--md-green-400);
}

/*-----------------------------------------------------------------------------
| Notebook
|----------------------------------------------------------------------------*/

/* stylelint-disable selector-max-class */

.jp-NotebookPanel {
  display: block;
  height: 100%;
}

.jp-NotebookPanel.jp-Document {
  min-width: 240px;
  min-height: 120px;
}

.jp-Notebook {
  padding: var(--jp-notebook-padding);
  outline: none;
  overflow: auto;
  background: var(--jp-layout-color0);
}

.jp-Notebook.jp-mod-scrollPastEnd::after {
  display: block;
  content: '';
  min-height: var(--jp-notebook-scroll-padding);
}

.jp-MainAreaWidget-ContainStrict .jp-Notebook * {
  contain: strict;
}

.jp-Notebook .jp-Cell {
  overflow: visible;
}

.jp-Notebook .jp-Cell .jp-InputPrompt {
  cursor: move;
}

/*-----------------------------------------------------------------------------
| Notebook state related styling
|
| The notebook and cells each have states, here are the possibilities:
|
| - Notebook
|   - Command
|   - Edit
| - Cell
|   - None
|   - Active (only one can be active)
|   - Selected (the cells actions are applied to)
|   - Multiselected (when multiple selected, the cursor)
|   - No outputs
|----------------------------------------------------------------------------*/

/* Command or edit modes */

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

/* cell is active */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser {
  background: var(--jp-brand-color1);
}

/* cell is dirty */
.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt {
  color: var(--jp-warn-color1);
}

.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt::before {
  color: var(--jp-warn-color1);
  content: '•';
}

.jp-Notebook .jp-Cell.jp-mod-active.jp-mod-dirty .jp-Collapser {
  background: var(--jp-warn-color1);
}

/* collapser is hovered */
.jp-Notebook .jp-Cell .jp-Collapser:hover {
  box-shadow: var(--jp-elevation-z2);
  background: var(--jp-brand-color1);
  opacity: var(--jp-cell-collapser-not-active-hover-opacity);
}

/* cell is active and collapser is hovered */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/* Command mode */

.jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected {
  background: var(--jp-notebook-multiselected-color);
}

.jp-Notebook.jp-mod-commandMode
  .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected) {
  background: transparent;
}

/* Edit mode */

.jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-cell-editor-active-background);
}

/*-----------------------------------------------------------------------------
| Notebook drag and drop
|----------------------------------------------------------------------------*/

.jp-Notebook-cell.jp-mod-dropSource {
  opacity: 0.5;
}

.jp-Notebook-cell.jp-mod-dropTarget,
.jp-Notebook.jp-mod-commandMode
  .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget {
  border-top-color: var(--jp-private-notebook-selected-color);
  border-top-style: solid;
  border-top-width: 2px;
}

.jp-dragImage {
  display: block;
  flex-direction: row;
  width: var(--jp-private-notebook-dragImage-width);
  height: var(--jp-private-notebook-dragImage-height);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
  overflow: visible;
}

.jp-dragImage-singlePrompt {
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

.jp-dragImage .jp-dragImage-content {
  flex: 1 1 auto;
  z-index: 2;
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  line-height: var(--jp-code-line-height);
  padding: var(--jp-code-padding);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background-color);
  color: var(--jp-content-font-color3);
  text-align: left;
  margin: 4px 4px 4px 0;
}

.jp-dragImage .jp-dragImage-prompt {
  flex: 0 0 auto;
  min-width: 36px;
  color: var(--jp-cell-inprompt-font-color);
  padding: var(--jp-code-padding);
  padding-left: 12px;
  font-family: var(--jp-cell-prompt-font-family);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: 1.9;
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
}

.jp-dragImage-multipleBack {
  z-index: -1;
  position: absolute;
  height: 32px;
  width: 300px;
  top: 8px;
  left: 8px;
  background: var(--jp-layout-color2);
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

/*-----------------------------------------------------------------------------
| Cell toolbar
|----------------------------------------------------------------------------*/

.jp-NotebookTools {
  display: block;
  min-width: var(--jp-sidebar-min-width);
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
    * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  overflow: auto;
}

.jp-ActiveCellTool {
  padding: 12px 0;
  display: flex;
}

.jp-ActiveCellTool-Content {
  flex: 1 1 auto;
}

.jp-ActiveCellTool .jp-ActiveCellTool-CellContent {
  background: var(--jp-cell-editor-background);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  min-height: 29px;
}

.jp-ActiveCellTool .jp-InputPrompt {
  min-width: calc(var(--jp-cell-prompt-width) * 0.75);
}

.jp-ActiveCellTool-CellContent > pre {
  padding: 5px 4px;
  margin: 0;
  white-space: normal;
}

.jp-MetadataEditorTool {
  flex-direction: column;
  padding: 12px 0;
}

.jp-RankedPanel > :not(:first-child) {
  margin-top: 12px;
}

.jp-KeySelector select.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: var(--jp-border-width) solid var(--jp-border-color1);
}

.jp-KeySelector label,
.jp-MetadataEditorTool label,
.jp-NumberSetter label {
  line-height: 1.4;
}

.jp-NotebookTools .jp-select-wrapper {
  margin-top: 4px;
  margin-bottom: 0;
}

.jp-NumberSetter input {
  width: 100%;
  margin-top: 4px;
}

.jp-NotebookTools .jp-Collapse {
  margin-top: 16px;
}

/*-----------------------------------------------------------------------------
| Presentation Mode (.jp-mod-presentationMode)
|----------------------------------------------------------------------------*/

.jp-mod-presentationMode .jp-Notebook {
  --jp-content-font-size1: var(--jp-content-presentation-font-size1);
  --jp-code-font-size: var(--jp-code-presentation-font-size);
}

.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt,
.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt {
  flex: 0 0 110px;
}

/*-----------------------------------------------------------------------------
| Side-by-side Mode (.jp-mod-sideBySide)
|----------------------------------------------------------------------------*/
.jp-mod-sideBySide.jp-Notebook .jp-Notebook-cell {
  margin-top: 3em;
  margin-bottom: 3em;
  margin-left: 5%;
  margin-right: 5%;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell {
  display: grid;
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-output-size)
    );
  grid-template-rows: auto minmax(0, 1fr) auto;
  grid-template-areas:
    'header header header'
    'input handle output'
    'footer footer footer';
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell.jp-mod-resizedCell {
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-resized-cell)
    );
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellHeader {
  grid-area: header;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-inputWrapper {
  grid-area: input;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-outputWrapper {
  /* overwrite the default margin (no vertical separation needed in side by side move */
  margin-top: 0;
  grid-area: output;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellFooter {
  grid-area: footer;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle {
  grid-area: handle;
  user-select: none;
  display: block;
  height: 100%;
  cursor: ew-resize;
  padding: 0 var(--jp-cell-padding);
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle::after {
  content: '';
  display: block;
  background: var(--jp-border-color2);
  height: 100%;
  width: 5px;
}

.jp-mod-sideBySide.jp-Notebook
  .jp-CodeCell.jp-mod-resizedCell
  .jp-CellResizeHandle::after {
  background: var(--jp-border-color0);
}

.jp-CellResizeHandle {
  display: none;
}

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Cell-Placeholder {
  padding-left: 55px;
}

.jp-Cell-Placeholder-wrapper {
  background: #fff;
  border: 1px solid;
  border-color: #e5e6e9 #dfe0e4 #d0d1d5;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  margin: 10px 15px;
}

.jp-Cell-Placeholder-wrapper-inner {
  padding: 15px;
  position: relative;
}

.jp-Cell-Placeholder-wrapper-body {
  background-repeat: repeat;
  background-size: 50% auto;
}

.jp-Cell-Placeholder-wrapper-body div {
  background: #f6f7f8;
  background-image: -webkit-linear-gradient(
    left,
    #f6f7f8 0%,
    #edeef1 20%,
    #f6f7f8 40%,
    #f6f7f8 100%
  );
  background-repeat: no-repeat;
  background-size: 800px 104px;
  height: 104px;
  position: absolute;
  right: 15px;
  left: 15px;
  top: 15px;
}

div.jp-Cell-Placeholder-h1 {
  top: 20px;
  height: 20px;
  left: 15px;
  width: 150px;
}

div.jp-Cell-Placeholder-h2 {
  left: 15px;
  top: 50px;
  height: 10px;
  width: 100px;
}

div.jp-Cell-Placeholder-content-1,
div.jp-Cell-Placeholder-content-2,
div.jp-Cell-Placeholder-content-3 {
  left: 15px;
  right: 15px;
  height: 10px;
}

div.jp-Cell-Placeholder-content-1 {
  top: 100px;
}

div.jp-Cell-Placeholder-content-2 {
  top: 120px;
}

div.jp-Cell-Placeholder-content-3 {
  top: 140px;
}

</style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
The following CSS variables define the main, public API for styling JupyterLab.
These variables should be used by all plugins wherever possible. In other
words, plugins should not define custom colors, sizes, etc unless absolutely
necessary. This enables users to change the visual theme of JupyterLab
by changing these variables.

Many variables appear in an ordered sequence (0,1,2,3). These sequences
are designed to work well together, so for example, `--jp-border-color1` should
be used with `--jp-layout-color1`. The numbers have the following meanings:

* 0: super-primary, reserved for special emphasis
* 1: primary, most important under normal situations
* 2: secondary, next most important under normal situations
* 3: tertiary, next most important under normal situations

Throughout JupyterLab, we are mostly following principles from Google's
Material Design when selecting colors. We are not, however, following
all of MD as it is not optimized for dense, information rich UIs.
*/

:root {
  /* Elevation
   *
   * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here:
   *
   * https://github.com/material-components/material-components-web
   * https://material-components-web.appspot.com/elevation.html
   */

  --jp-shadow-base-lightness: 0;
  --jp-shadow-umbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.2
  );
  --jp-shadow-penumbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.14
  );
  --jp-shadow-ambient-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.12
  );
  --jp-elevation-z0: none;
  --jp-elevation-z1: 0 2px 1px -1px var(--jp-shadow-umbra-color),
    0 1px 1px 0 var(--jp-shadow-penumbra-color),
    0 1px 3px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z2: 0 3px 1px -2px var(--jp-shadow-umbra-color),
    0 2px 2px 0 var(--jp-shadow-penumbra-color),
    0 1px 5px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z4: 0 2px 4px -1px var(--jp-shadow-umbra-color),
    0 4px 5px 0 var(--jp-shadow-penumbra-color),
    0 1px 10px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z6: 0 3px 5px -1px var(--jp-shadow-umbra-color),
    0 6px 10px 0 var(--jp-shadow-penumbra-color),
    0 1px 18px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z8: 0 5px 5px -3px var(--jp-shadow-umbra-color),
    0 8px 10px 1px var(--jp-shadow-penumbra-color),
    0 3px 14px 2px var(--jp-shadow-ambient-color);
  --jp-elevation-z12: 0 7px 8px -4px var(--jp-shadow-umbra-color),
    0 12px 17px 2px var(--jp-shadow-penumbra-color),
    0 5px 22px 4px var(--jp-shadow-ambient-color);
  --jp-elevation-z16: 0 8px 10px -5px var(--jp-shadow-umbra-color),
    0 16px 24px 2px var(--jp-shadow-penumbra-color),
    0 6px 30px 5px var(--jp-shadow-ambient-color);
  --jp-elevation-z20: 0 10px 13px -6px var(--jp-shadow-umbra-color),
    0 20px 31px 3px var(--jp-shadow-penumbra-color),
    0 8px 38px 7px var(--jp-shadow-ambient-color);
  --jp-elevation-z24: 0 11px 15px -7px var(--jp-shadow-umbra-color),
    0 24px 38px 3px var(--jp-shadow-penumbra-color),
    0 9px 46px 8px var(--jp-shadow-ambient-color);

  /* Borders
   *
   * The following variables, specify the visual styling of borders in JupyterLab.
   */

  --jp-border-width: 1px;
  --jp-border-color0: var(--md-grey-400);
  --jp-border-color1: var(--md-grey-400);
  --jp-border-color2: var(--md-grey-300);
  --jp-border-color3: var(--md-grey-200);
  --jp-inverse-border-color: var(--md-grey-600);
  --jp-border-radius: 2px;

  /* UI Fonts
   *
   * The UI font CSS variables are used for the typography all of the JupyterLab
   * user interface elements that are not directly user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-ui-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-ui-font-scale-factor: 1.2;
  --jp-ui-font-size0: 0.83333em;
  --jp-ui-font-size1: 13px; /* Base font size */
  --jp-ui-font-size2: 1.2em;
  --jp-ui-font-size3: 1.44em;
  --jp-ui-font-family: system-ui, -apple-system, blinkmacsystemfont, 'Segoe UI',
    helvetica, arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji',
    'Segoe UI Symbol';

  /*
   * Use these font colors against the corresponding main layout colors.
   * In a light theme, these go from dark to light.
   */

  /* Defaults use Material Design specification */
  --jp-ui-font-color0: rgba(0, 0, 0, 1);
  --jp-ui-font-color1: rgba(0, 0, 0, 0.87);
  --jp-ui-font-color2: rgba(0, 0, 0, 0.54);
  --jp-ui-font-color3: rgba(0, 0, 0, 0.38);

  /*
   * Use these against the brand/accent/warn/error colors.
   * These will typically go from light to darker, in both a dark and light theme.
   */

  --jp-ui-inverse-font-color0: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color1: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color2: rgba(255, 255, 255, 0.7);
  --jp-ui-inverse-font-color3: rgba(255, 255, 255, 0.5);

  /* Content Fonts
   *
   * Content font variables are used for typography of user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-content-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-content-line-height: 1.6;
  --jp-content-font-scale-factor: 1.2;
  --jp-content-font-size0: 0.83333em;
  --jp-content-font-size1: 14px; /* Base font size */
  --jp-content-font-size2: 1.2em;
  --jp-content-font-size3: 1.44em;
  --jp-content-font-size4: 1.728em;
  --jp-content-font-size5: 2.0736em;

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-content-presentation-font-size1: 17px;
  --jp-content-heading-line-height: 1;
  --jp-content-heading-margin-top: 1.2em;
  --jp-content-heading-margin-bottom: 0.8em;
  --jp-content-heading-font-weight: 500;

  /* Defaults use Material Design specification */
  --jp-content-font-color0: rgba(0, 0, 0, 1);
  --jp-content-font-color1: rgba(0, 0, 0, 0.87);
  --jp-content-font-color2: rgba(0, 0, 0, 0.54);
  --jp-content-font-color3: rgba(0, 0, 0, 0.38);
  --jp-content-link-color: var(--md-blue-900);
  --jp-content-font-family: system-ui, -apple-system, blinkmacsystemfont,
    'Segoe UI', helvetica, arial, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol';

  /*
   * Code Fonts
   *
   * Code font variables are used for typography of code and other monospaces content.
   */

  --jp-code-font-size: 13px;
  --jp-code-line-height: 1.3077; /* 17px for 13px base */
  --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */
  --jp-code-font-family-default: menlo, consolas, 'DejaVu Sans Mono', monospace;
  --jp-code-font-family: var(--jp-code-font-family-default);

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-code-presentation-font-size: 16px;

  /* may need to tweak cursor width if you change font size */
  --jp-code-cursor-width0: 1.4px;
  --jp-code-cursor-width1: 2px;
  --jp-code-cursor-width2: 4px;

  /* Layout
   *
   * The following are the main layout colors use in JupyterLab. In a light
   * theme these would go from light to dark.
   */

  --jp-layout-color0: white;
  --jp-layout-color1: white;
  --jp-layout-color2: var(--md-grey-200);
  --jp-layout-color3: var(--md-grey-400);
  --jp-layout-color4: var(--md-grey-600);

  /* Inverse Layout
   *
   * The following are the inverse layout colors use in JupyterLab. In a light
   * theme these would go from dark to light.
   */

  --jp-inverse-layout-color0: #111;
  --jp-inverse-layout-color1: var(--md-grey-900);
  --jp-inverse-layout-color2: var(--md-grey-800);
  --jp-inverse-layout-color3: var(--md-grey-700);
  --jp-inverse-layout-color4: var(--md-grey-600);

  /* Brand/accent */

  --jp-brand-color0: var(--md-blue-900);
  --jp-brand-color1: var(--md-blue-700);
  --jp-brand-color2: var(--md-blue-300);
  --jp-brand-color3: var(--md-blue-100);
  --jp-brand-color4: var(--md-blue-50);
  --jp-accent-color0: var(--md-green-900);
  --jp-accent-color1: var(--md-green-700);
  --jp-accent-color2: var(--md-green-300);
  --jp-accent-color3: var(--md-green-100);

  /* State colors (warn, error, success, info) */

  --jp-warn-color0: var(--md-orange-900);
  --jp-warn-color1: var(--md-orange-700);
  --jp-warn-color2: var(--md-orange-300);
  --jp-warn-color3: var(--md-orange-100);
  --jp-error-color0: var(--md-red-900);
  --jp-error-color1: var(--md-red-700);
  --jp-error-color2: var(--md-red-300);
  --jp-error-color3: var(--md-red-100);
  --jp-success-color0: var(--md-green-900);
  --jp-success-color1: var(--md-green-700);
  --jp-success-color2: var(--md-green-300);
  --jp-success-color3: var(--md-green-100);
  --jp-info-color0: var(--md-cyan-900);
  --jp-info-color1: var(--md-cyan-700);
  --jp-info-color2: var(--md-cyan-300);
  --jp-info-color3: var(--md-cyan-100);

  /* Cell specific styles */

  --jp-cell-padding: 5px;
  --jp-cell-collapser-width: 8px;
  --jp-cell-collapser-min-height: 20px;
  --jp-cell-collapser-not-active-hover-opacity: 0.6;
  --jp-cell-editor-background: var(--md-grey-100);
  --jp-cell-editor-border-color: var(--md-grey-300);
  --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-cell-editor-active-background: var(--jp-layout-color0);
  --jp-cell-editor-active-border-color: var(--jp-brand-color1);
  --jp-cell-prompt-width: 64px;
  --jp-cell-prompt-font-family: var(--jp-code-font-family-default);
  --jp-cell-prompt-letter-spacing: 0;
  --jp-cell-prompt-opacity: 1;
  --jp-cell-prompt-not-active-opacity: 0.5;
  --jp-cell-prompt-not-active-font-color: var(--md-grey-700);

  /* A custom blend of MD grey and blue 600
   * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */
  --jp-cell-inprompt-font-color: #307fc1;

  /* A custom blend of MD grey and orange 600
   * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */
  --jp-cell-outprompt-font-color: #bf5b3d;

  /* Notebook specific styles */

  --jp-notebook-padding: 10px;
  --jp-notebook-select-background: var(--jp-layout-color1);
  --jp-notebook-multiselected-color: var(--md-blue-50);

  /* The scroll padding is calculated to fill enough space at the bottom of the
  notebook to show one single-line cell (with appropriate padding) at the top
  when the notebook is scrolled all the way to the bottom. We also subtract one
  pixel so that no scrollbar appears if we have just one single-line cell in the
  notebook. This padding is to enable a 'scroll past end' feature in a notebook.
  */
  --jp-notebook-scroll-padding: calc(
    100% - var(--jp-code-font-size) * var(--jp-code-line-height) -
      var(--jp-code-padding) - var(--jp-cell-padding) - 1px
  );

  /* Rendermime styles */

  --jp-rendermime-error-background: #fdd;
  --jp-rendermime-table-row-background: var(--md-grey-100);
  --jp-rendermime-table-row-hover-background: var(--md-light-blue-50);

  /* Dialog specific styles */

  --jp-dialog-background: rgba(0, 0, 0, 0.25);

  /* Console specific styles */

  --jp-console-padding: 10px;

  /* Toolbar specific styles */

  --jp-toolbar-border-color: var(--jp-border-color1);
  --jp-toolbar-micro-height: 8px;
  --jp-toolbar-background: var(--jp-layout-color1);
  --jp-toolbar-box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.24);
  --jp-toolbar-header-margin: 4px 4px 0 4px;
  --jp-toolbar-active-background: var(--md-grey-300);

  /* Statusbar specific styles */

  --jp-statusbar-height: 24px;

  /* Input field styles */

  --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-input-active-background: var(--jp-layout-color1);
  --jp-input-hover-background: var(--jp-layout-color1);
  --jp-input-background: var(--md-grey-100);
  --jp-input-border-color: var(--jp-inverse-border-color);
  --jp-input-active-border-color: var(--jp-brand-color1);
  --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3);

  /* General editor styles */

  --jp-editor-selected-background: #d9d9d9;
  --jp-editor-selected-focused-background: #d7d4f0;
  --jp-editor-cursor-color: var(--jp-ui-font-color0);

  /* Code mirror specific styles */

  --jp-mirror-editor-keyword-color: #008000;
  --jp-mirror-editor-atom-color: #88f;
  --jp-mirror-editor-number-color: #080;
  --jp-mirror-editor-def-color: #00f;
  --jp-mirror-editor-variable-color: var(--md-grey-900);
  --jp-mirror-editor-variable-2-color: rgb(0, 54, 109);
  --jp-mirror-editor-variable-3-color: #085;
  --jp-mirror-editor-punctuation-color: #05a;
  --jp-mirror-editor-property-color: #05a;
  --jp-mirror-editor-operator-color: #a2f;
  --jp-mirror-editor-comment-color: #408080;
  --jp-mirror-editor-string-color: #ba2121;
  --jp-mirror-editor-string-2-color: #708;
  --jp-mirror-editor-meta-color: #a2f;
  --jp-mirror-editor-qualifier-color: #555;
  --jp-mirror-editor-builtin-color: #008000;
  --jp-mirror-editor-bracket-color: #997;
  --jp-mirror-editor-tag-color: #170;
  --jp-mirror-editor-attribute-color: #00c;
  --jp-mirror-editor-header-color: blue;
  --jp-mirror-editor-quote-color: #090;
  --jp-mirror-editor-link-color: #00c;
  --jp-mirror-editor-error-color: #f00;
  --jp-mirror-editor-hr-color: #999;

  /*
    RTC user specific colors.
    These colors are used for the cursor, username in the editor,
    and the icon of the user.
  */

  --jp-collaborator-color1: #ffad8e;
  --jp-collaborator-color2: #dac83d;
  --jp-collaborator-color3: #72dd76;
  --jp-collaborator-color4: #00e4d0;
  --jp-collaborator-color5: #45d4ff;
  --jp-collaborator-color6: #e2b1ff;
  --jp-collaborator-color7: #ff9de6;

  /* Vega extension styles */

  --jp-vega-background: white;

  /* Sidebar-related styles */

  --jp-sidebar-min-width: 250px;

  /* Search-related styles */

  --jp-search-toggle-off-opacity: 0.5;
  --jp-search-toggle-hover-opacity: 0.8;
  --jp-search-toggle-on-opacity: 1;
  --jp-search-selected-match-background-color: rgb(245, 200, 0);
  --jp-search-selected-match-color: black;
  --jp-search-unselected-match-background-color: var(
    --jp-inverse-layout-color0
  );
  --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0);

  /* Icon colors that work well with light or dark backgrounds */
  --jp-icon-contrast-color0: var(--md-purple-600);
  --jp-icon-contrast-color1: var(--md-green-600);
  --jp-icon-contrast-color2: var(--md-pink-600);
  --jp-icon-contrast-color3: var(--md-blue-600);

  /* Button colors */
  --jp-accept-color-normal: var(--md-blue-700);
  --jp-accept-color-hover: var(--md-blue-800);
  --jp-accept-color-active: var(--md-blue-900);
  --jp-warn-color-normal: var(--md-red-700);
  --jp-warn-color-hover: var(--md-red-800);
  --jp-warn-color-active: var(--md-red-900);
  --jp-reject-color-normal: var(--md-grey-600);
  --jp-reject-color-hover: var(--md-grey-700);
  --jp-reject-color-active: var(--md-grey-800);

  /* File or activity icons and switch semantic variables */
  --jp-jupyter-icon-color: #f37626;
  --jp-notebook-icon-color: #f37626;
  --jp-json-icon-color: var(--md-orange-700);
  --jp-console-icon-background-color: var(--md-blue-700);
  --jp-console-icon-color: white;
  --jp-terminal-icon-background-color: var(--md-grey-800);
  --jp-terminal-icon-color: var(--md-grey-200);
  --jp-text-editor-icon-color: var(--md-grey-700);
  --jp-inspector-icon-color: var(--md-grey-700);
  --jp-switch-color: var(--md-grey-400);
  --jp-switch-true-position-color: var(--md-orange-900);
}
</style>
<style type="text/css">
/* Force rendering true colors when outputing to pdf */
* {
  -webkit-print-color-adjust: exact;
}

/* Misc */
a.anchor-link {
  display: none;
}

/* Input area styling */
.jp-InputArea {
  overflow: hidden;
}

.jp-InputArea-editor {
  overflow: hidden;
}

.cm-editor.cm-s-jupyter .highlight pre {
/* weird, but --jp-code-padding defined to be 5px but 4px horizontal padding is hardcoded for pre.cm-line */
  padding: var(--jp-code-padding) 4px;
  margin: 0;

  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
  color: inherit;

}

.jp-OutputArea-output pre {
  line-height: inherit;
  font-family: inherit;
}

.jp-RenderedText pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
}

/* Hiding the collapser by default */
.jp-Collapser {
  display: none;
}

@page {
    margin: 0.5in; /* Margin for each printed piece of paper */
}

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}
</style>
<!-- Load mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML-full,Safe"> </script>
<!-- MathJax configuration -->
<script type="text/x-mathjax-config">
    init_mathjax = function() {
        if (window.MathJax) {
        // MathJax loaded
            MathJax.Hub.Config({
                TeX: {
                    equationNumbers: {
                    autoNumber: "AMS",
                    useLabelIds: true
                    }
                },
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true,
                    processEnvironments: true
                },
                displayAlign: 'center',
                CommonHTML: {
                    linebreaks: {
                    automatic: true
                    }
                }
            });

            MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
        }
    }
    init_mathjax();
    </script>
<!-- End of mathjax configuration --><script type="module">
  document.addEventListener("DOMContentLoaded", async () => {
    const diagrams = document.querySelectorAll(".jp-Mermaid > pre.mermaid");
    // do not load mermaidjs if not needed
    if (!diagrams.length) {
      return;
    }
    const mermaid = (await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs")).default;
    const parser = new DOMParser();

    mermaid.initialize({
      maxTextSize: 100000,
      maxEdges: 100000,
      startOnLoad: false,
      fontFamily: window
        .getComputedStyle(document.body)
        .getPropertyValue("--jp-ui-font-family"),
      theme: document.querySelector("body[data-jp-theme-light='true']")
        ? "default"
        : "dark",
    });

    let _nextMermaidId = 0;

    function makeMermaidImage(svg) {
      const img = document.createElement("img");
      const doc = parser.parseFromString(svg, "image/svg+xml");
      const svgEl = doc.querySelector("svg");
      const { maxWidth } = svgEl?.style || {};
      const firstTitle = doc.querySelector("title");
      const firstDesc = doc.querySelector("desc");

      img.setAttribute("src", `data:image/svg+xml,${encodeURIComponent(svg)}`);
      if (maxWidth) {
        img.width = parseInt(maxWidth);
      }
      if (firstTitle) {
        img.setAttribute("alt", firstTitle.textContent);
      }
      if (firstDesc) {
        const caption = document.createElement("figcaption");
        caption.className = "sr-only";
        caption.textContent = firstDesc.textContent;
        return [img, caption];
      }
      return [img];
    }

    async function makeMermaidError(text) {
      let errorMessage = "";
      try {
        await mermaid.parse(text);
      } catch (err) {
        errorMessage = `${err}`;
      }

      const result = document.createElement("details");
      result.className = 'jp-RenderedMermaid-Details';
      const summary = document.createElement("summary");
      summary.className = 'jp-RenderedMermaid-Summary';
      const pre = document.createElement("pre");
      const code = document.createElement("code");
      code.innerText = text;
      pre.appendChild(code);
      summary.appendChild(pre);
      result.appendChild(summary);

      const warning = document.createElement("pre");
      warning.innerText = errorMessage;
      result.appendChild(warning);
      return [result];
    }

    async function renderOneMarmaid(src) {
      const id = `jp-mermaid-${_nextMermaidId++}`;
      const parent = src.parentNode;
      let raw = src.textContent.trim();
      const el = document.createElement("div");
      el.style.visibility = "hidden";
      document.body.appendChild(el);
      let results = null;
      let output = null;
      try {
        let { svg } = await mermaid.render(id, raw, el);
        svg = cleanMermaidSvg(svg);
        results = makeMermaidImage(svg);
        output = document.createElement("figure");
        results.map(output.appendChild, output);
      } catch (err) {
        parent.classList.add("jp-mod-warning");
        results = await makeMermaidError(raw);
        output = results[0];
      } finally {
        el.remove();
      }
      parent.classList.add("jp-RenderedMermaid");
      parent.appendChild(output);
    }


    /**
     * Post-process to ensure mermaid diagrams contain only valid SVG and XHTML.
     */
    function cleanMermaidSvg(svg) {
      return svg.replace(RE_VOID_ELEMENT, replaceVoidElement);
    }


    /**
     * A regular expression for all void elements, which may include attributes and
     * a slash.
     *
     * @see https://developer.mozilla.org/en-US/docs/Glossary/Void_element
     *
     * Of these, only `<br>` is generated by Mermaid in place of `\n`,
     * but _any_ "malformed" tag will break the SVG rendering entirely.
     */
    const RE_VOID_ELEMENT =
      /<\s*(area|base|br|col|embed|hr|img|input|link|meta|param|source|track|wbr)\s*([^>]*?)\s*>/gi;

    /**
     * Ensure a void element is closed with a slash, preserving any attributes.
     */
    function replaceVoidElement(match, tag, rest) {
      rest = rest.trim();
      if (!rest.endsWith('/')) {
        rest = `${rest} /`;
      }
      return `<${tag} ${rest}>`;
    }

    void Promise.all([...diagrams].map(renderOneMarmaid));
  });
</script>
<style>
  .jp-Mermaid:not(.jp-RenderedMermaid) {
    display: none;
  }

  .jp-RenderedMermaid {
    overflow: auto;
    display: flex;
  }

  .jp-RenderedMermaid.jp-mod-warning {
    width: auto;
    padding: 0.5em;
    margin-top: 0.5em;
    border: var(--jp-border-width) solid var(--jp-warn-color2);
    border-radius: var(--jp-border-radius);
    color: var(--jp-ui-font-color1);
    font-size: var(--jp-ui-font-size1);
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  .jp-RenderedMermaid figure {
    margin: 0;
    overflow: auto;
    max-width: 100%;
  }

  .jp-RenderedMermaid img {
    max-width: 100%;
  }

  .jp-RenderedMermaid-Details > pre {
    margin-top: 1em;
  }

  .jp-RenderedMermaid-Summary {
    color: var(--jp-warn-color2);
  }

  .jp-RenderedMermaid:not(.jp-mod-warning) pre {
    display: none;
  }

  .jp-RenderedMermaid-Summary > pre {
    display: inline-block;
    white-space: normal;
  }
</style>
<!-- End of mermaid configuration --></head>
<body class="jp-Notebook" data-jp-theme-light="true" data-jp-theme-name="JupyterLab Light">
<main>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=73c2d2c9-4a4b-48ca-90a3-1ccd120ca08b">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h1 id="Fine-tuning-using-Llama-2-7b">Fine tuning using Llama 2 7b<a class="anchor-link" href="#Fine-tuning-using-Llama-2-7b">¶</a></h1>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=5b5bef3d-89b9-42ed-b158-a39bd61f6a31">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Base-Model-and-Quantization">Base Model and Quantization<a class="anchor-link" href="#Base-Model-and-Quantization">¶</a></h3>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=938123ed-15b0-45ee-b622-d9bbe5fe3a48">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>4-bit quantization alongside specific computational optimizations</p>
<p>load_in_4bit: This option likely enables loading and storing tensors in 4-bit precision. This can significantly reduce memory usage at the cost of precision. Enabling this suggests that you're optimizing for memory efficiency, potentially to fit larger models or datasets in memory.</p>
<p>bnb_4bit_use_double_quant: This indicates the use of a double quantization process for 4-bit representation. Double quantization might be used to improve the precision of the 4-bit quantized values, potentially mitigating some of the precision loss associated with low-bit quantization.</p>
<p>bnb_4bit_quant_type="nf4": This specifies the quantization type or algorithm used for converting tensors to 4-bit representations. "nf4" might refer to a specific quantization scheme optimized for neural network weights and activations. The exact nature of "nf4" would depend on the documentation of the BitsAndBytes library, but it suggests an approach tailored to maintain as much information as possible within the 4-bit limitation.</p>
<p>bnb_4bit_compute_dtype=torch.bfloat16: This sets the data type for computations using 4-bit quantized tensors to torch.bfloat16, which is a 16-bit floating-point representation that offers a good balance between precision and memory usage. By performing computations in bfloat16, the configuration aims to maintain computational accuracy and efficiency, particularly on hardware that supports bfloat16 operations natively.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=dfe5f014-527c-4a83-863b-4b6330c72ed5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [1]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># GPU 0: NVIDIA GeForce RTX 4090</span>
<span class="c1"># GPU 1: NVIDIA GeForce RTX 4090</span>
<span class="c1"># GPU 2: NVIDIA GeForce RTX 4090</span>
<span class="c1"># GPU 3: NVIDIA GeForce RTX 3090 Ti</span>
<span class="c1"># GPU 4: NVIDIA GeForce RTX 3090 Ti</span>
<span class="c1"># GPU 5: NVIDIA GeForce RTX 3090</span>
<span class="c1"># GPU 6: NVIDIA GeForce RTX 3090</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"CUDA_VISIBLE_DEVICES"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"0"</span>  <span class="c1"># ""makes all visible, "0" GPU 0 visible</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=53ffaa11-3936-40a0-8f2a-3d083ff2afef">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Supress-warnings">Supress warnings<a class="anchor-link" href="#Supress-warnings">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=e77f7e3d-3af3-4dc8-9571-d13398c29ee9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [2]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">UserWarning</span><span class="p">)</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">FutureWarning</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=ef535c7a-848e-4c6e-a692-208f98610d82">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Inspect-the-base-model">Inspect the base model<a class="anchor-link" href="#Inspect-the-base-model">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=15ec782b-0776-4354-ad08-66f1e9e50187">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [3]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s2">"Llama-2-7b-hf"</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"meta-llama/"</span><span class="o">+</span><span class="n">model_name</span>

<span class="n">bnb_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span>
    <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="s2">"nf4"</span><span class="p">,</span>
    <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
<span class="p">)</span>

<span class="n">access_token</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HF_TOKEN"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">bnb_config</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">token</span><span class="o">=</span><span class="n">access_token</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>config.json:   0%|          | 0.00/609 [00:00&lt;?, ?B/s]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00&lt;?, ?B/s]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Downloading shards:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00&lt;?, ?B/s]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00&lt;?, ?B/s]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Loading checkpoint shards:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=83f74939-3ab3-4011-90cb-8e90c22ea162">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [4]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torchinfo</span> <span class="kn">import</span> <span class="n">summary</span>
<span class="n">summary</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[4]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>================================================================================
Layer (type:depth-idx)                                  Param #
================================================================================
LlamaForSequenceClassification                          --
├─LlamaModel: 1-1                                       --
│    └─Embedding: 2-1                                   131,072,000
│    └─ModuleList: 2-2                                  --
│    │    └─LlamaDecoderLayer: 3-1                      101,195,776
│    │    └─LlamaDecoderLayer: 3-2                      101,195,776
│    │    └─LlamaDecoderLayer: 3-3                      101,195,776
│    │    └─LlamaDecoderLayer: 3-4                      101,195,776
│    │    └─LlamaDecoderLayer: 3-5                      101,195,776
│    │    └─LlamaDecoderLayer: 3-6                      101,195,776
│    │    └─LlamaDecoderLayer: 3-7                      101,195,776
│    │    └─LlamaDecoderLayer: 3-8                      101,195,776
│    │    └─LlamaDecoderLayer: 3-9                      101,195,776
│    │    └─LlamaDecoderLayer: 3-10                     101,195,776
│    │    └─LlamaDecoderLayer: 3-11                     101,195,776
│    │    └─LlamaDecoderLayer: 3-12                     101,195,776
│    │    └─LlamaDecoderLayer: 3-13                     101,195,776
│    │    └─LlamaDecoderLayer: 3-14                     101,195,776
│    │    └─LlamaDecoderLayer: 3-15                     101,195,776
│    │    └─LlamaDecoderLayer: 3-16                     101,195,776
│    │    └─LlamaDecoderLayer: 3-17                     101,195,776
│    │    └─LlamaDecoderLayer: 3-18                     101,195,776
│    │    └─LlamaDecoderLayer: 3-19                     101,195,776
│    │    └─LlamaDecoderLayer: 3-20                     101,195,776
│    │    └─LlamaDecoderLayer: 3-21                     101,195,776
│    │    └─LlamaDecoderLayer: 3-22                     101,195,776
│    │    └─LlamaDecoderLayer: 3-23                     101,195,776
│    │    └─LlamaDecoderLayer: 3-24                     101,195,776
│    │    └─LlamaDecoderLayer: 3-25                     101,195,776
│    │    └─LlamaDecoderLayer: 3-26                     101,195,776
│    │    └─LlamaDecoderLayer: 3-27                     101,195,776
│    │    └─LlamaDecoderLayer: 3-28                     101,195,776
│    │    └─LlamaDecoderLayer: 3-29                     101,195,776
│    │    └─LlamaDecoderLayer: 3-30                     101,195,776
│    │    └─LlamaDecoderLayer: 3-31                     101,195,776
│    │    └─LlamaDecoderLayer: 3-32                     101,195,776
│    └─LlamaRMSNorm: 2-3                                4,096
├─Linear: 1-2                                           8,192
================================================================================
Total params: 3,369,349,120
Trainable params: 131,346,432
Non-trainable params: 3,238,002,688
================================================================================</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=49bbe47c-4430-4f4a-90d8-1113bd320a18">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [5]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[5]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>LlamaForSequenceClassification(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (score): Linear(in_features=4096, out_features=2, bias=False)
)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=6627b1da-57d6-4a17-8ea3-746b5cb1f3d9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Load-the-news-dataset-from-pickle-file">Load the news dataset from pickle file<a class="anchor-link" href="#Load-the-news-dataset-from-pickle-file">¶</a></h3><p>If any of the check_files don't exist then load the pickle file</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=1cc372cc-0cae-4b49-a2d7-8e57f58244a7">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [4]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pickle</span>

<span class="n">base_path</span> <span class="o">=</span> <span class="s1">'./data/'</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">base_path</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">file_name</span> <span class="o">=</span> <span class="s1">'news_small_dataset.pkl'</span>
<span class="n">file_path</span> <span class="o">=</span> <span class="n">base_path</span><span class="o">+</span><span class="n">file_name</span>

<span class="k">def</span> <span class="nf">pickle_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">file_path</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="s1">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
        <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">file</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Dataset has been pickled to: </span><span class="si">{</span><span class="n">file_path</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">load_pickle_dataset</span><span class="p">(</span><span class="n">file_path</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="s1">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Dataset has been loaded from: </span><span class="si">{</span><span class="n">file_path</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dataset</span>

<span class="k">def</span> <span class="nf">check_files_exists</span><span class="p">(</span><span class="n">file_names</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">file_names</span><span class="p">:</span>
        <span class="n">file_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">base_path</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">file_path</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="kc">False</span>

<span class="c1"># if these files exist we do not want to load the news_dataset.pkl to tokenize and make these files</span>
<span class="n">check_files</span> <span class="o">=</span> <span class="p">[</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_train_ds.pkl'</span><span class="p">,</span> <span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_eval_ds.pkl'</span><span class="p">,</span> <span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_test_ds.pkl'</span><span class="p">]</span>

<span class="k">if</span> <span class="n">check_files_exists</span><span class="p">(</span><span class="n">check_files</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"At least one of the specified files already exists. Not loading new dataset."</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">news_split_ds</span> <span class="o">=</span> <span class="n">load_pickle_dataset</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">news_split_ds</span><span class="p">)</span>
    <span class="n">total_rows</span> <span class="o">=</span> <span class="p">(</span><span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'train'</span><span class="p">]</span><span class="o">.</span><span class="n">num_rows</span> <span class="o">+</span>
              <span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'eval'</span><span class="p">]</span><span class="o">.</span><span class="n">num_rows</span> <span class="o">+</span>
              <span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'test'</span><span class="p">]</span><span class="o">.</span><span class="n">num_rows</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Total number of rows:"</span><span class="p">,</span> <span class="n">total_rows</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Dataset loaded successfully."</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>At least one of the specified files already exists. Not loading new dataset.
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=8ad450a2-6cef-432e-9e63-7ff985b4726e">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Tokenization-of-data">Tokenization of data<a class="anchor-link" href="#Tokenization-of-data">¶</a></h3>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=186f093c-7487-44ae-ad95-9ee8d24f04c7">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>
return_tensors="pt": This argument configures the tokenizer to output PyTorch ("pt") tensors. If you're working with TensorFlow, you'd use "tf" instead, and for NumPy arrays, you could omit this argument or set return_tensors to None.
</p>
<p>
Direct Model Input: By converting the tokenized input into tensors, the output can be directly used as input to a PyTorch model, fitting seamlessly into the data processing pipeline for model training or inference.

<p>Handling of Batch Inputs: This approach also supports batch inputs. If you pass a list of texts to the tokenizer with return_tensors="pt", it will automatically pad the sequences to the maximum length in the batch, returning a tensor where the first dimension is the batch size.</p>
<p>Padding and Truncation: The padding=True and truncation=True arguments ensure that all sequences are padded to the same length (up to max_length) and are truncated if they exceed this length, which is important for processing sequences in batches.</p>
</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=88c3a38c-1d24-4a1e-8d96-1b7c2ce162c7">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [5]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">token</span><span class="o">=</span><span class="n">access_token</span><span class="p">)</span>

<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">tokenize_fn</span><span class="p">(</span><span class="n">news</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">news</span><span class="p">[</span><span class="s1">'article'</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>tokenizer_config.json:   0%|          | 0.00/776 [00:00&lt;?, ?B/s]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>tokenizer.model:   0%|          | 0.00/500k [00:00&lt;?, ?B/s]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>tokenizer.json:   0%|          | 0.00/1.84M [00:00&lt;?, ?B/s]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>special_tokens_map.json:   0%|          | 0.00/414 [00:00&lt;?, ?B/s]</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=02efc4f0-742a-4838-887d-5556a26ae15f">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Tokenize-train,-evaluation,-and-test-datasets">Tokenize train, evaluation, and test datasets<a class="anchor-link" href="#Tokenize-train,-evaluation,-and-test-datasets">¶</a></h3><p>If any of the check files exist then don't run tokenization and save some time.
Else load the pickle files that already exist.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=0f3e3101-df30-4af2-9976-49ba0cf44d62">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [6]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">check_files_exists</span><span class="p">(</span><span class="n">check_files</span><span class="p">):</span>
    <span class="n">tokenized_train_ds</span> <span class="o">=</span> <span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'train'</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_fn</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">tokenized_eval_ds</span> <span class="o">=</span> <span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'eval'</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_fn</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">tokenized_test_ds</span> <span class="o">=</span> <span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'test'</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_fn</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">tokenized_train_ds</span><span class="o">.</span><span class="n">features</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">tokenized_eval_ds</span><span class="o">.</span><span class="n">features</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">tokenized_test_ds</span><span class="o">.</span><span class="n">features</span><span class="p">)</span>
    
    <span class="n">pickle_dataset</span><span class="p">(</span><span class="n">tokenized_train_ds</span><span class="p">,</span> <span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_train_ds.pkl'</span><span class="p">)</span>
    <span class="n">pickle_dataset</span><span class="p">(</span><span class="n">tokenized_eval_ds</span><span class="p">,</span> <span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_eval_ds.pkl'</span><span class="p">)</span>
    <span class="n">pickle_dataset</span><span class="p">(</span><span class="n">tokenized_test_ds</span><span class="p">,</span> <span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_test_ds.pkl'</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Files already exist, so load datasets"</span><span class="p">)</span>
    <span class="n">tokenized_train_ds</span> <span class="o">=</span> <span class="n">load_pickle_dataset</span><span class="p">(</span><span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_train_ds.pkl'</span><span class="p">)</span>
    <span class="n">tokenized_eval_ds</span> <span class="o">=</span> <span class="n">load_pickle_dataset</span><span class="p">(</span><span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_eval_ds.pkl'</span><span class="p">)</span>
    <span class="n">tokenized_test_ds</span> <span class="o">=</span> <span class="n">load_pickle_dataset</span><span class="p">(</span><span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_test_ds.pkl'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Files already exist, so load datasets
Dataset has been loaded from: ./data/Llama-2-7b-hf-small_tokenized_train_ds.pkl
Dataset has been loaded from: ./data/Llama-2-7b-hf-small_tokenized_eval_ds.pkl
Dataset has been loaded from: ./data/Llama-2-7b-hf-small_tokenized_test_ds.pkl
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=307cc4fb-9766-4b0f-943b-4a1c90053e09">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Look-at-the-tokenized-data">Look at the tokenized data<a class="anchor-link" href="#Look-at-the-tokenized-data">¶</a></h3><p>Notice what the actual data looks like, and then the tokenized data which is a bunch of numbers, and then the attention mask at the end.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=44321182-e1b9-4570-bd24-7a5cf42ba504">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [9]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">count_train_records</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenized_train_ds</span><span class="p">)</span>
<span class="n">count_eval_records</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenized_eval_ds</span><span class="p">)</span>
<span class="n">count_test_records</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenized_test_ds</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of records in training dataset: </span><span class="si">{</span><span class="n">count_train_records</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of records in evaluation dataset: </span><span class="si">{</span><span class="n">count_eval_records</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of records in test dataset: </span><span class="si">{</span><span class="n">count_test_records</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">count_total_records</span> <span class="o">=</span> <span class="n">count_train_records</span> <span class="o">+</span> <span class="n">count_eval_records</span> <span class="o">+</span> <span class="n">count_test_records</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Total number of records: </span><span class="si">{</span><span class="n">count_total_records</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Number of records in training dataset: 33611
Number of records in evaluation dataset: 7203
Number of records in test dataset: 7203
Total number of records: 48017
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=64be678b-d2a8-403e-bcc8-ef9e7eabb0cf">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [10]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">first_record</span> <span class="o">=</span> <span class="n">tokenized_train_ds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">first_record</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>{'article': "In a year where homicides, rapes and robberies increased slightly, New York City still saw serious crime drop 1.7 percent in 2015, continuing an overall decline that began in the 1990s, NYPD Commissioner William Bratton said Monday.\nAt a news conference with Mayor Bill de Blasio, Bratton touted last year’s crime statistics, which he said, when combined with an even larger decline in 2014, put to rest the fear that substantial decreases couldn’t continue under the new administration at City Hall.\n“While we have had some fluctuation, some increases in certain categories, the overall trend in all our crime categories continues to go down,” Bratton told reporters. “It was a very good year for us, 2015.\nHomicides increased by 4.5 percent in 2015, rising to 350 from 333 in the prior year, which was the lowest since 1994, said Deputy Commissioner Dermot Shea. Rapes increased 6 percent and robberies rose 2 percent, said Shea, who is in charge of data collection and operations for the NYPD.\nThe lower overall crime statistics came about due to what Shea called “targeted enforcement,” where cops make quality arrests even though the overall number of apprehensions was the lowest in the city since 2003.\nTwo boroughs — Manhattan and the Bronx — actually saw serious crimes increase by 3 percent and 4 percent, respectively, Shea said. Manhattan’s increase was driven by more robberies, while the Bronx, although seeing an overall crime increase, had what he said was a “phenomenal” reduction in shootings. Citywide, shootings were down in 2015 about 3 percent, to 1,103 from 1,172 in 2014.\nShea largely attributed the 2015 increase in rapes to victims coming forward with complaints about attacks from years past.\nSign up to get the latest updates Get Newsday's Breaking News alerts in your inbox. By clicking Sign up, you agree to our privacy policy.\n“Twenty percent of these rapes didn’t happen in 2015,” he said.\nThe NYPD has seen an increase in rapes involving single women who, after a night of drinking, get into cabs of all kinds and are attacked, Shea said.\n“They get driven, and passing out and waking up in a desolate area, and they get sexually attacked. This is something, really, that people need to be exceptionally aware of, and like any case in New York City, the buddy system works,” said Shea, referring to the need for people to travel in pairs when taking a cab at night.\nBratton and police brass hope to build upon the continuing drop in overall crime by using technology such as ShotSpotter and a newly minted GPS system for police cars.\nJessica Tisch, NYPD deputy commissioner for technology, said ShotSpotter, an acoustical system that detects gunfire, identified gunshots in 1,672 cases, mostly in Brooklyn. Of those alerts, 74 percent didn’t have any 911 calls from the public associated with them.\nTisch said ShotSpotter helped police recover ballistic evidence in 19 percent of the gunfire alerts. In 22 percent of those cases, Tisch said, cops were able to make positive matches of bullets with those from guns used in earlier shootings.\nTisch also highlighted a special GPS system being tried in about 5,000 patrol cars that allows the NYPD to see where its vehicles are and to track their movements over a 24-hour period, as well as gather information about the officers’ driving.\n", 'label': 0, 'input_ids': [1, 512, 263, 1629, 988, 3632, 293, 2247, 29892, 1153, 5547, 322, 10832, 495, 583, 11664, 10029, 29892, 1570, 3088, 4412, 1603, 4446, 10676, 17268, 5768, 29871, 29896, 29889, 29955, 10151, 297, 29871, 29906, 29900, 29896, 29945, 29892, 3133, 292, 385, 12463, 4845, 457, 393, 4689, 297, 278, 29871, 29896, 29929, 29929, 29900, 29879, 29892, 23526, 25014, 11444, 261, 4667, 1771, 271, 880, 1497, 27822, 29889, 13, 4178, 263, 9763, 21362, 411, 22186, 6682, 316, 3164, 294, 601, 29892, 1771, 271, 880, 5646, 287, 1833, 1629, 30010, 29879, 17268, 13964, 29892, 607, 540, 1497, 29892, 746, 12420, 411, 385, 1584, 7200, 4845, 457, 297, 29871, 29906, 29900, 29896, 29946, 29892, 1925, 304, 1791, 278, 8866, 393, 23228, 9263, 2129, 8496, 30010, 29873, 6773, 1090, 278, 716, 17517, 472, 4412, 6573, 29889, 13, 30015, 8809, 488, 591, 505, 750, 777, 1652, 5313, 29884, 362, 29892, 777, 16415, 297, 3058, 13997, 29892, 278, 12463, 534, 355, 297, 599, 1749, 17268, 13997, 18172, 304, 748, 1623, 3995, 1771, 271, 880, 5429, 1634, 272, 2153, 29889, 1346, 3112, 471, 263, 1407, 1781, 1629, 363, 502, 29892, 29871, 29906, 29900, 29896, 29945, 29889, 13, 24259, 293, 2247, 11664, 491, 29871, 29946, 29889, 29945, 10151, 297, 29871, 29906, 29900, 29896, 29945, 29892, 20493, 304, 29871, 29941, 29945, 29900, 515, 29871, 29941, 29941, 29941, 297, 278, 7536, 1629, 29892, 607, 471, 278, 19604, 1951, 29871, 29896, 29929, 29929, 29946, 29892, 1497, 26721, 29891, 11444, 261, 360, 837, 327, 2296, 29874, 29889, 390, 11603, 11664, 29871, 29953, 10151, 322, 10832, 495, 583, 11492, 29871, 29906, 10151, 29892, 1497, 2296, 29874, 29892, 1058, 338, 297, 8323, 310, 848, 4333, 322, 6931, 363, 278, 23526, 25014, 29889, 13, 1576, 5224, 12463, 17268, 13964, 2996, 1048, 2861, 304, 825, 2296, 29874, 2000, 1346, 5182, 287, 24555, 13561, 3995, 988, 274, 3554, 1207, 11029, 3948, 9197, 1584, 2466, 278, 12463, 1353, 310, 623, 9003, 5580, 471, 278, 19604, 297, 278, 4272, 1951, 29871, 29906, 29900, 29900, 29941, 29889, 13, 13985, 9820, 820, 29879, 813, 29093, 23586, 322, 278, 14165, 29916, 813, 2869, 4446, 10676, 2181, 1355, 7910, 491, 29871, 29941, 10151, 322, 29871, 29946, 10151, 29892, 8307, 29892, 2296, 29874, 1497, 29889, 29093, 23586, 30010, 29879, 7910, 471, 18225, 491, 901, 10832, 495, 583, 29892, 1550, 278, 14165, 29916, 29892, 5998, 8790, 385, 12463, 17268, 7910, 29892, 750, 825, 540, 1497, 471, 263, 1346, 9789, 2770, 284, 30024, 20376, 297, 15049, 886, 29889, 4412, 8157, 29892, 15049, 886, 892, 1623, 297, 29871, 29906, 29900, 29896, 29945, 1048, 29871, 29941, 10151, 29892, 304, 29871, 29896, 29892, 29896, 29900, 29941, 515, 29871, 29896, 29892, 29896, 29955, 29906, 297, 29871, 29906, 29900, 29896, 29946, 29889, 13, 13468, 29874, 18425, 29393, 278, 29871, 29906, 29900, 29896, 29945, 7910, 297, 1153, 5547, 304, 6879, 9893, 6421, 6375, 411, 15313, 9466, 1048, 16661, 515, 2440, 4940, 29889, 13, 10140, 701, 304, 679, 278, 9281, 11217, 3617, 10130, 3250, 29915, 29879, 5826, 5086, 10130, 6655, 29879, 297, 596, 297, 1884, 29889, 2648, 14855, 9954, 701, 29892, 366, 8661, 304, 1749, 5999, 4135, 8898, 29889, 13, 30015, 27418, 6478, 10151, 310, 1438], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=b2d7f4f8-9eb3-4feb-86e7-fb0dad88753f">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Turn-on-accelerate">Turn on accelerate<a class="anchor-link" href="#Turn-on-accelerate">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=f3eab29e-fa47-4a13-abc4-d6425ae741b6">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [7]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">accelerate</span> <span class="kn">import</span> <span class="n">FullyShardedDataParallelPlugin</span><span class="p">,</span> <span class="n">Accelerator</span>
<span class="kn">from</span> <span class="nn">torch.distributed.fsdp.fully_sharded_data_parallel</span> <span class="kn">import</span> <span class="n">FullOptimStateDictConfig</span><span class="p">,</span> <span class="n">FullStateDictConfig</span>

<span class="n">fsdp_plugin</span> <span class="o">=</span> <span class="n">FullyShardedDataParallelPlugin</span><span class="p">(</span>
    <span class="n">state_dict_config</span><span class="o">=</span><span class="n">FullStateDictConfig</span><span class="p">(</span><span class="n">offload_to_cpu</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rank0_only</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="n">optim_state_dict_config</span><span class="o">=</span><span class="n">FullOptimStateDictConfig</span><span class="p">(</span><span class="n">offload_to_cpu</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rank0_only</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
<span class="p">)</span>

<span class="n">accelerator</span> <span class="o">=</span> <span class="n">Accelerator</span><span class="p">(</span><span class="n">fsdp_plugin</span><span class="o">=</span><span class="n">fsdp_plugin</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=22e040f1-e596-476d-9f26-ffa6f8a4a548">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="LoRA---Low-Rank-Adaptation">LoRA - Low-Rank Adaptation<a class="anchor-link" href="#LoRA---Low-Rank-Adaptation">¶</a></h3>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=bc8b8091-6179-4df0-b5bd-b29ec2dde4d5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>LoRA, short for Low-Rank Adaptation, is a technique designed to efficiently fine-tune large pre-trained models without the need to update all the model parameters, significantly reducing the computational and memory overhead typically associated with training. LoRA targets the challenge of adapting massive models, particularly in natural language processing (NLP) and computer vision, to specialized tasks while keeping the resource requirements manageable.</p>
<p>
Gradient checkpointing for the model. Gradient checkpointing is a technique used to reduce memory usage during the training of deep neural networks by trading compute for memory. It works by storing a minimal set of intermediate activations during the forward pass and then recomputing the others during the backward pass. This is particularly useful for training large models or using larger batch sizes.
</p>
<p>
The forward pass is the process where the input data is passed through the network from the input layer to the output layer. During this pass, the network performs a series of computations at each layer, applying weights to the inputs, adding biases (if applicable), and passing the result through an activation function. The final output of the forward pass is the prediction made by the network. The main goal of the forward pass is to compute the output given the current state of the model's parameters (weights and biases). This output is then used to calculate the loss, which quantifies how well the model's predictions match the actual labels.
</p>
<p>
The backward pass, or backpropagation, is the process of computing the gradient of the loss function with respect to each parameter in the network. This involves applying the chain rule of calculus to take derivatives step-by-step from the output layer back to the input layer. Essentially, it calculates how much each parameter contributed to the error in the prediction. The purpose of the backward pass is to update the model's parameters in a way that minimally reduces the loss, improving the model's predictions. The gradients calculated during this pass indicate the direction in which each parameter should be adjusted to decrease the error. Using an optimization algorithm (e.g., Stochastic Gradient Descent), these gradients are then used to update the weights to minimize the loss.
</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=41956597-0441-4bc6-aefe-fc4b0d5349c5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>
r: This parameter specifies the rank of the low-rank matrices that are introduced by LoRA. A smaller rank means fewer parameters to train, leading to a more memory-efficient fine-tuning process.
</p>
<p>
lora_alpha: This multiplier adjusts the scale of the LoRA parameters. A higher value increases the capacity of the LoRA adjustments to the original model weights.
</p>
<p>
target_modules: Lists the specific parts of the model to which LoRA will be applied. These typically correspond to components within transformer blocks, such as the query, key, value, and output projections in attention mechanisms, as well as any additional modules relevant to the model architecture.
</p>
<p>
bias: Specifies how biases are treated in the adaptation process. In this case, biases are not adjusted ("none").
</p>
<p>
lora_dropout: Sets the dropout rate for the LoRA parameters, helping to prevent overfitting during fine-tuning. The dropout rate is a hyperparameter used in the training of neural networks, representing the probability that a given neuron (or unit) is temporarily "dropped" from the network during a specific iteration of training. This means that the neuron will not participate in the forward pass and its contribution to the backward pass (gradient computation) is also ignored during that iteration. Dropout is applied randomly to a subset of neurons in the network at each training step.
</p>
<p>
task_type: Indicates the type of task for which the model is being fine-tuned. The example uses TaskType.SEQ_CLS, 
suggesting a sequence classification task, such as sentiment analysis or document classification. In my case a binary classification of machine versus human generated text.
</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=0532bdc6-4317-474b-859c-4e5d2fe6f1bd">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [12]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">prepare_model_for_kbit_training</span><span class="p">,</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">get_peft_model</span><span class="p">,</span> <span class="n">TaskType</span>

<span class="n">model</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">prepare_model_for_kbit_training</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span>
    <span class="n">r</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span>
        <span class="s2">"q_proj"</span><span class="p">,</span>
        <span class="s2">"k_proj"</span><span class="p">,</span>
        <span class="s2">"v_proj"</span><span class="p">,</span>
        <span class="s2">"o_proj"</span><span class="p">,</span>
        <span class="s2">"gate_proj"</span><span class="p">,</span>
        <span class="s2">"up_proj"</span><span class="p">,</span>
        <span class="s2">"down_proj"</span><span class="p">,</span>
        <span class="s2">"lm_head"</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="n">bias</span><span class="o">=</span><span class="s2">"none"</span><span class="p">,</span>
    <span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">task_type</span><span class="o">=</span><span class="n">TaskType</span><span class="o">.</span><span class="n">SEQ_CLS</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">accelerator</span><span class="o">.</span><span class="n">prepare_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=1cd29494-4453-4c01-b878-ef2f4533d34d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Inspect-the-model">Inspect the model<a class="anchor-link" href="#Inspect-the-model">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=d5058f6d-185d-45af-83b9-bb7f61d4bee3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [13]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">print_trainable_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">trainable_params</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">all_param</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
        <span class="n">all_param</span> <span class="o">+=</span> <span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="n">trainable_params</span> <span class="o">+=</span> <span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"trainable params: </span><span class="si">{</span><span class="n">trainable_params</span><span class="si">}</span><span class="s2"> || all params: </span><span class="si">{</span><span class="n">all_param</span><span class="si">}</span><span class="s2"> || trainable%: </span><span class="si">{</span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">trainable_params</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">all_param</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=b14fe8f2-eccd-4cfe-9d20-ef48bc178842">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [14]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">print_trainable_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">model</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>trainable params: 19996672 || all params: 3389345792 || trainable%: 0.5899861869272499
</pre>
</div>
</div>
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[14]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>PeftModelForSequenceClassification(
  (base_model): LoraModel(
    (model): LlamaForSequenceClassification(
      (model): LlamaModel(
        (embed_tokens): Embedding(32000, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaSdpaAttention(
              (q_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (v_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=11008, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (up_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=11008, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (down_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=11008, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=11008, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm()
            (post_attention_layernorm): LlamaRMSNorm()
          )
        )
        (norm): LlamaRMSNorm()
      )
      (score): ModulesToSaveWrapper(
        (original_module): Linear(in_features=4096, out_features=2, bias=False)
        (modules_to_save): ModuleDict(
          (default): Linear(in_features=4096, out_features=2, bias=False)
        )
      )
    )
  )
)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=e17b4782-03f8-4335-bfda-2a65794bff2c">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Look-at-hardware">Look at hardware<a class="anchor-link" href="#Look-at-hardware">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=c27c3328-1926-4adc-8696-b3b343ec4afd">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [15]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Available GPUs: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"GPU </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_name</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Available GPUs: 1
GPU 0: NVIDIA GeForce RTX 4090
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=bdf68d0e-8786-489d-a4b8-b7478513efea">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [16]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">!</span>nvidia-smi
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Thu May 23 03:37:34 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 3090        Off |   00000000:01:00.0 Off |                  N/A |
| 31%   51C    P2            370W /  420W |    2430MiB /  24576MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA GeForce RTX 4090        Off |   00000000:02:00.0 Off |                  Off |
|  0%   41C    P2             37W /  450W |    4689MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA GeForce RTX 3090 Ti     Off |   00000000:2A:00.0 Off |                  Off |
| 50%   58C    P2            359W /  450W |    1698MiB /  24564MiB |     93%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA GeForce RTX 3090        Off |   00000000:41:00.0 Off |                  N/A |
| 33%   41C    P2            115W /  420W |    1166MiB /  24576MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA GeForce RTX 4090        Off |   00000000:42:00.0 Off |                  Off |
|  0%   49C    P8             15W /  450W |      10MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA GeForce RTX 4090        Off |   00000000:61:00.0 Off |                  Off |
|  0%   40C    P8             16W /  450W |      10MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA GeForce RTX 3090 Ti     Off |   00000000:62:00.0 Off |                  Off |
| 34%   50C    P2            373W /  450W |    4788MiB /  24564MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A      2561      G   /usr/bin/gnome-shell                            4MiB |
|    0   N/A  N/A     12702      C   /usr/bin/python3                             2416MiB |
|    1   N/A  N/A      2561      G   /usr/bin/gnome-shell                           95MiB |
|    1   N/A  N/A     13002      C   /usr/bin/python3                             4584MiB |
|    2   N/A  N/A      2561      G   /usr/bin/gnome-shell                            4MiB |
|    2   N/A  N/A     12510      C   /usr/bin/python3                             1684MiB |
|    3   N/A  N/A      2561      G   /usr/bin/gnome-shell                            4MiB |
|    3   N/A  N/A     12816      C   /usr/bin/python3                             1152MiB |
|    4   N/A  N/A      2561      G   /usr/bin/gnome-shell                            6MiB |
|    5   N/A  N/A      2561      G   /usr/bin/gnome-shell                            6MiB |
|    6   N/A  N/A      2561      G   /usr/bin/gnome-shell                            4MiB |
|    6   N/A  N/A     12625      C   /usr/bin/python3                             4774MiB |
+-----------------------------------------------------------------------------------------+
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=2340f2d1-1cc7-454e-bad6-bf4ac2c46fc9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [8]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">model</span><span class="o">.</span><span class="n">is_parallelizable</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">model</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=1199841a-6519-4422-8259-2fdbb114e466">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [9]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span><span class="p">,</span> <span class="n">accuracy_score</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">logits_and_labels</span><span class="p">):</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">logits_and_labels</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
    <span class="n">f1</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">'macro'</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">'accuracy'</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span> <span class="s1">'f1'</span><span class="p">:</span> <span class="n">f1</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=b8c7f3ee-6997-4ed7-b28e-5d574831fb08">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [10]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">project_name</span> <span class="o">=</span> <span class="s2">"praxis-"</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s2">"-small-finetune"</span>
<span class="n">output_dir_path</span> <span class="o">=</span> <span class="s2">"./"</span> <span class="o">+</span> <span class="n">project_name</span>
<span class="n">output_dir_path</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[10]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>'./praxis-Llama-2-7b-hf-small-finetune'</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=e7a69bde-1d50-46a0-9838-00812afbdb16">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [20]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=ca48a7aa-41d7-4c57-bfec-093d22bf4b54">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p><strong>output_dir</strong> (<code>output_dir_path</code>): This specifies the directory where outputs such as model checkpoints and logs will be saved. It's important for organizing the outputs of your training sessions.</p>
<p><strong>warmup_steps</strong> (<code>5</code>): This parameter sets the number of steps during which the learning rate will gradually increase from zero to the initially set learning rate. This warmup phase helps stabilize the model's training early on, preventing the model from diverging due to high gradient values at the start.</p>
<p><strong>per_device_train_batch_size</strong> (<code>32</code>): This sets the number of training examples to process on each device (like a GPU) during training. A higher batch size can speed up training but may require more memory.</p>
<p><strong>per_device_eval_batch_size</strong> (<code>32</code>): Similar to the training batch size, this is the number of examples to process on each device during evaluation. It determines how quickly the model can process the evaluation data.</p>
<p><strong>num_train_epochs</strong> (<code>10</code>): This defines the total number of times the training process should iterate over the entire dataset. More epochs can lead to better learning but also risk overfitting if too high.</p>
<p><strong>gradient_checkpointing</strong> (<code>False</code>): If set to <code>True</code>, this would enable gradient checkpointing to reduce memory usage at the cost of longer training time. It's useful for very deep models that otherwise would not fit into GPU memory.</p>
<p><strong>gradient_accumulation_steps</strong> (<code>2</code>): This setting allows you to accumulate gradients over multiple steps before performing an update on the model's weights. It's a way to effectively increase the batch size without increasing the memory load, which can be helpful when dealing with hardware constraints.</p>
<p><strong>max_steps</strong> (<code>500</code>): This is the maximum number of training steps to execute, regardless of how many epochs are set. Training will stop when this number of steps is reached.</p>
<p><strong>learning_rate</strong> (<code>2.5e-5</code>): This is the step size at which the optimizer updates the model’s weights. A smaller learning rate might lead to better fine-tuning but slower convergence, and vice versa.</p>
<p><strong>logging_steps</strong> (<code>200</code>): Specifies how often to log training information. The setting determines after how many steps new logs should be created, which might include loss and other metrics. More frequent logging provides finer-grained visibility into the training progress but can add computational overhead.</p>
<p><strong>bf16</strong> (<code>True</code>): This would enable training using bfloat16 precision, which is a mixed precision format with fewer bits than the standard single-precision floating point (fp32). Like fp16, it can reduce memory usage and potentially speed up training if supported by the hardware. It's particularly useful on TPUs and newer GPUs that support this format. (4090)</p>
<p><strong>fp16</strong> (<code>True</code>): This enables half-precision floating point (16-bit) training. It reduces memory usage and can speed up training, provided the hardware (like modern GPUs) supports it. (3090 and 4090)</p>
<p><strong>optim</strong> (<code>"paged_adamw_8bit"</code>): Specifies the optimizer to use. "paged_adamw_8bit" might refer to a variation of the AdamW optimizer that is optimized for lower precision and memory bandwidth, enhancing training speed and efficiency.</p>
<p><strong>logging_dir</strong> (<code>"./logs"</code>): This specifies the directory where training logs should be saved. It's used to store logs if you are using a logging framework or callback that writes out logs to files. Organizing logs in a specific directory is helpful for post-training analysis and for monitoring the training process through tools like TensorBoard.</p>
<p><strong>save_strategy</strong> (<code>"epoch"</code>): This determines how often to save model checkpoints. Setting it to <code>"epoch"</code> means the model will save checkpoints at the end of each epoch.</p>
<p><strong>save_steps</strong> (<code>50</code>): This is closely related to the <code>save_strategy</code> when set to "steps". It defines how often to save the model, specifically after how many training steps. A lower number means more frequent saves, which increases disk I/O but provides more restore points for training.</p>
<p><strong>evaluation_strategy</strong> (<code>"epoch"</code>): This configures when the model should be evaluated against the evaluation dataset. Like <code>save_strategy</code>, setting this to <code>"epoch"</code> triggers evaluations at the end of each epoch, providing feedback on model performance after it has seen the entire training dataset.</p>
<p><strong>eval_steps</strong> (<code>50</code>): This parameter determines how often to evaluate the model if the <code>evaluation_strategy</code> is set to "steps". Similar to <code>logging_steps</code>, setting this affects how frequently the model's performance is assessed on the evaluation dataset during the training process. More frequent evaluations provide a closer look at the model's performance but at the cost of increased computational overhead.</p>
<p><strong>do_eval</strong> (<code>True</code>): This flag enables the evaluation of the model on the evaluation dataset. If <code>True</code>, it will use the evaluation dataset to assess model performance based on metrics</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=d8f3cd41-fd17-4fd8-83b8-54f464df79ff">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Training">Training<a class="anchor-link" href="#Training">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=2df4e2a5-2a08-434b-983a-f6cd42f33da3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [21]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">transformers</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="n">transformers</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">777</span><span class="p">)</span>

<span class="k">if</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_train_ds</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">tokenized_eval_ds</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">transformers</span><span class="o">.</span><span class="n">TrainingArguments</span><span class="p">(</span>
        <span class="n">output_dir</span><span class="o">=</span><span class="n">output_dir_path</span><span class="p">,</span>
        <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">logging_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">logging_dir</span><span class="o">=</span><span class="n">output_dir_path</span><span class="o">+</span><span class="s2">"/logs"</span><span class="p">,</span>
        <span class="n">evaluation_strategy</span><span class="o">=</span><span class="s1">'epoch'</span><span class="p">,</span>
        <span class="n">save_strategy</span><span class="o">=</span><span class="s1">'epoch'</span><span class="p">,</span>
        <span class="n">bf16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">optim</span><span class="o">=</span><span class="s2">"paged_adamw_8bit"</span><span class="p">,</span>
        <span class="n">do_eval</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>
    <span class="n">data_collator</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">DataCollatorWithPadding</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">),</span>
<span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">resume_from_checkpoint</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># Turn to True if power goes out...</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>2024-05-23 03:37:35.318749: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-23 03:37:36.064341: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
<span class="ansi-blue-intense-fg ansi-bold">wandb</span>: Currently logged in as: <span class="ansi-yellow-fg">nispoe</span>. Use <span class="ansi-bold">`wandb login --relogin`</span> to force relogin
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
Tracking run with wandb version 0.17.0
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
Run data is saved locally in <code>/home/nispoe/kuk/Praxis/wandb/run-20240523_033737-08xatxcg</code>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
Syncing run <strong><a href="https://wandb.ai/nispoe/huggingface/runs/08xatxcg" target="_blank">laced-grass-331</a></strong> to <a href="https://wandb.ai/nispoe/huggingface" target="_blank">Weights &amp; Biases</a> (<a href="https://wandb.me/run" target="_blank">docs</a>)<br/>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
 View project at <a href="https://wandb.ai/nispoe/huggingface" target="_blank">https://wandb.ai/nispoe/huggingface</a>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
 View run at <a href="https://wandb.ai/nispoe/huggingface/runs/08xatxcg" target="_blank">https://wandb.ai/nispoe/huggingface/runs/08xatxcg</a>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<div>
<progress max="42020" style="width:300px; height:20px; vertical-align: middle;" value="42020"></progress>
      [42020/42020 33:00:26, Epoch 10/10]
    </div>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: left;">
<th>Epoch</th>
<th>Training Loss</th>
<th>Validation Loss</th>
<th>Accuracy</th>
<th>F1</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0.000100</td>
<td>0.037788</td>
<td>0.992781</td>
<td>0.992374</td>
</tr>
<tr>
<td>2</td>
<td>0.000000</td>
<td>0.044097</td>
<td>0.993891</td>
<td>0.993547</td>
</tr>
<tr>
<td>3</td>
<td>0.000000</td>
<td>0.053543</td>
<td>0.994308</td>
<td>0.993995</td>
</tr>
<tr>
<td>4</td>
<td>0.000000</td>
<td>0.041921</td>
<td>0.994308</td>
<td>0.993979</td>
</tr>
<tr>
<td>5</td>
<td>0.000000</td>
<td>0.063095</td>
<td>0.993336</td>
<td>0.992945</td>
</tr>
<tr>
<td>6</td>
<td>0.000700</td>
<td>0.038035</td>
<td>0.994586</td>
<td>0.994275</td>
</tr>
<tr>
<td>7</td>
<td>0.000000</td>
<td>0.049486</td>
<td>0.995557</td>
<td>0.995302</td>
</tr>
<tr>
<td>8</td>
<td>0.000000</td>
<td>0.046723</td>
<td>0.995696</td>
<td>0.995451</td>
</tr>
<tr>
<td>9</td>
<td>0.000000</td>
<td>0.069512</td>
<td>0.995002</td>
<td>0.994713</td>
</tr>
<tr>
<td>10</td>
<td>0.000000</td>
<td>0.045171</td>
<td>0.996252</td>
<td>0.996039</td>
</tr>
</tbody>
</table><p>
</p></div>
</div>
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[21]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>TrainOutput(global_step=42020, training_loss=0.013706571551333263, metrics={'train_runtime': 118830.9483, 'train_samples_per_second': 2.828, 'train_steps_per_second': 0.354, 'total_flos': 6.707599834192282e+18, 'train_loss': 0.013706571551333263, 'epoch': 10.0})</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=0d13982d-53cb-4c88-bd32-a98d4a5a9874">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Determine-best-checkpoint">Determine best checkpoint<a class="anchor-link" href="#Determine-best-checkpoint">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=8bd570c7-8feb-4b69-bda7-9c678bfd92c3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [22]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">!</span>ls<span class="w"> </span>-ltr<span class="w"> </span><span class="o">{</span>output_dir_path<span class="o">}</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>total 44
drwxr-xr-x 2 nispoe nispoe 4096 May 23 03:37 logs
drwxrwxr-x 2 nispoe nispoe 4096 May 23 06:55 checkpoint-4202
drwxrwxr-x 2 nispoe nispoe 4096 May 23 10:13 checkpoint-8404
drwxrwxr-x 2 nispoe nispoe 4096 May 23 13:31 checkpoint-12606
drwxrwxr-x 2 nispoe nispoe 4096 May 23 16:50 checkpoint-16808
drwxrwxr-x 2 nispoe nispoe 4096 May 23 20:08 checkpoint-21010
drwxrwxr-x 2 nispoe nispoe 4096 May 23 23:26 checkpoint-25212
drwxrwxr-x 2 nispoe nispoe 4096 May 24 02:44 checkpoint-29414
drwxrwxr-x 2 nispoe nispoe 4096 May 24 06:02 checkpoint-33616
drwxrwxr-x 2 nispoe nispoe 4096 May 24 09:20 checkpoint-37818
drwxrwxr-x 2 nispoe nispoe 4096 May 24 12:38 checkpoint-42020
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=97771269-d77c-4c1b-b264-34e082db6cbc">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [13]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.python.summary.summary_iterator</span> <span class="kn">import</span> <span class="n">summary_iterator</span>
<span class="kn">import</span> <span class="nn">glob</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># Construct the logs directory path</span>
<span class="n">logs_directory</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">'./'</span><span class="p">,</span> <span class="n">project_name</span><span class="p">,</span> <span class="s1">'logs'</span><span class="p">)</span>
<span class="n">file_pattern</span> <span class="o">=</span> <span class="s1">'events.out.tfevents.*'</span>

<span class="c1"># Retrieve all event files matching the pattern</span>
<span class="n">event_files</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">logs_directory</span><span class="p">,</span> <span class="n">file_pattern</span><span class="p">))</span>

<span class="c1"># Function to print out TensorBoard event logs</span>
<span class="k">def</span> <span class="nf">print_events_from_file</span><span class="p">(</span><span class="n">event_files</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">event_file</span> <span class="ow">in</span> <span class="n">event_files</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Reading events from file: </span><span class="si">{</span><span class="n">event_file</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">summary_iterator</span><span class="p">(</span><span class="n">event_file</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">e</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">v</span><span class="o">.</span><span class="n">HasField</span><span class="p">(</span><span class="s1">'simple_value'</span><span class="p">):</span>
                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Step: </span><span class="si">{</span><span class="n">e</span><span class="o">.</span><span class="n">step</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">v</span><span class="o">.</span><span class="n">simple_value</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>  <span class="c1"># Just in case the event file is not readable</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Failed to read </span><span class="si">{</span><span class="n">event_file</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">print_events_from_file</span><span class="p">(</span><span class="n">event_files</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Reading events from file: ./praxis-Llama-2-7b-hf-small-finetune/logs/events.out.tfevents.1716453456.hephaestus.13002.0
Step: 10, train/loss: 1.9485000371932983
Step: 10, train/grad_norm: 33.20042419433594
Step: 10, train/learning_rate: 4.998810254619457e-05
Step: 10, train/epoch: 0.0023798190522938967
Step: 20, train/loss: 1.3353999853134155
Step: 20, train/grad_norm: 34.95604705810547
Step: 20, train/learning_rate: 4.997620271751657e-05
Step: 20, train/epoch: 0.004759638104587793
Step: 30, train/loss: 1.0156999826431274
Step: 30, train/grad_norm: 44.40596008300781
Step: 30, train/learning_rate: 4.9964302888838574e-05
Step: 30, train/epoch: 0.007139457389712334
Step: 40, train/loss: 1.2467999458312988
Step: 40, train/grad_norm: 32.90671920776367
Step: 40, train/learning_rate: 4.995240306016058e-05
Step: 40, train/epoch: 0.009519276209175587
Step: 50, train/loss: 0.8202999830245972
Step: 50, train/grad_norm: 19.70381736755371
Step: 50, train/learning_rate: 4.994050323148258e-05
Step: 50, train/epoch: 0.011899095959961414
Step: 60, train/loss: 0.6478000283241272
Step: 60, train/grad_norm: 27.6690616607666
Step: 60, train/learning_rate: 4.992860704078339e-05
Step: 60, train/epoch: 0.014278914779424667
Step: 70, train/loss: 0.41940000653266907
Step: 70, train/grad_norm: 22.997888565063477
Step: 70, train/learning_rate: 4.9916707212105393e-05
Step: 70, train/epoch: 0.016658734530210495
Step: 80, train/loss: 0.16189999878406525
Step: 80, train/grad_norm: 0.06402857601642609
Step: 80, train/learning_rate: 4.9904807383427396e-05
Step: 80, train/epoch: 0.019038552418351173
Step: 90, train/loss: 0.18330000340938568
Step: 90, train/grad_norm: 0.04449604079127312
Step: 90, train/learning_rate: 4.98929075547494e-05
Step: 90, train/epoch: 0.021418372169137
Step: 100, train/loss: 0.29330000281333923
Step: 100, train/grad_norm: 34.72064971923828
Step: 100, train/learning_rate: 4.98810077260714e-05
Step: 100, train/epoch: 0.02379819191992283
Step: 110, train/loss: 0.001500000013038516
Step: 110, train/grad_norm: 0.4604649543762207
Step: 110, train/learning_rate: 4.986911153537221e-05
Step: 110, train/epoch: 0.026178009808063507
Step: 120, train/loss: 0.28360000252723694
Step: 120, train/grad_norm: 0.4367543160915375
Step: 120, train/learning_rate: 4.9857211706694216e-05
Step: 120, train/epoch: 0.028557829558849335
Step: 130, train/loss: 0.3785000145435333
Step: 130, train/grad_norm: 0.016301017254590988
Step: 130, train/learning_rate: 4.984531187801622e-05
Step: 130, train/epoch: 0.030937649309635162
Step: 140, train/loss: 0.28380000591278076
Step: 140, train/grad_norm: 22.72854995727539
Step: 140, train/learning_rate: 4.983341204933822e-05
Step: 140, train/epoch: 0.03331746906042099
Step: 150, train/loss: 0.16359999775886536
Step: 150, train/grad_norm: 0.18421466648578644
Step: 150, train/learning_rate: 4.9821512220660225e-05
Step: 150, train/epoch: 0.03569728881120682
Step: 160, train/loss: 0.2386000007390976
Step: 160, train/grad_norm: 1.2104498147964478
Step: 160, train/learning_rate: 4.9809616029961035e-05
Step: 160, train/epoch: 0.03807710483670235
Step: 170, train/loss: 0.2806999981403351
Step: 170, train/grad_norm: 0.005726468749344349
Step: 170, train/learning_rate: 4.979771620128304e-05
Step: 170, train/epoch: 0.040456924587488174
Step: 180, train/loss: 0.25459998846054077
Step: 180, train/grad_norm: 39.81209182739258
Step: 180, train/learning_rate: 4.978581637260504e-05
Step: 180, train/epoch: 0.042836744338274
Step: 190, train/loss: 0.02879999950528145
Step: 190, train/grad_norm: 0.025070296600461006
Step: 190, train/learning_rate: 4.9773916543927044e-05
Step: 190, train/epoch: 0.04521656408905983
Step: 200, train/loss: 0.08980000019073486
Step: 200, train/grad_norm: 0.0301913283765316
Step: 200, train/learning_rate: 4.976201671524905e-05
Step: 200, train/epoch: 0.04759638383984566
Step: 210, train/loss: 0.01489999983459711
Step: 210, train/grad_norm: 0.17305761575698853
Step: 210, train/learning_rate: 4.975012052454986e-05
Step: 210, train/epoch: 0.049976203590631485
Step: 220, train/loss: 0.10949999839067459
Step: 220, train/grad_norm: 0.00979558378458023
Step: 220, train/learning_rate: 4.973822069587186e-05
Step: 220, train/epoch: 0.052356019616127014
Step: 230, train/loss: 0.03460000082850456
Step: 230, train/grad_norm: 0.009552889503538609
Step: 230, train/learning_rate: 4.972632086719386e-05
Step: 230, train/epoch: 0.05473583936691284
Step: 240, train/loss: 0.18479999899864197
Step: 240, train/grad_norm: 0.8881869316101074
Step: 240, train/learning_rate: 4.9714421038515866e-05
Step: 240, train/epoch: 0.05711565911769867
Step: 250, train/loss: 0.053700000047683716
Step: 250, train/grad_norm: 41.458709716796875
Step: 250, train/learning_rate: 4.970252120983787e-05
Step: 250, train/epoch: 0.0594954788684845
Step: 260, train/loss: 0.10220000147819519
Step: 260, train/grad_norm: 0.531162679195404
Step: 260, train/learning_rate: 4.969062501913868e-05
Step: 260, train/epoch: 0.061875298619270325
Step: 270, train/loss: 0.031300000846385956
Step: 270, train/grad_norm: 0.05119340866804123
Step: 270, train/learning_rate: 4.967872519046068e-05
Step: 270, train/epoch: 0.06425511837005615
Step: 280, train/loss: 0.00570000009611249
Step: 280, train/grad_norm: 0.003850637935101986
Step: 280, train/learning_rate: 4.9666825361782685e-05
Step: 280, train/epoch: 0.06663493812084198
Step: 290, train/loss: 0.4406000077724457
Step: 290, train/grad_norm: 22.80765724182129
Step: 290, train/learning_rate: 4.965492553310469e-05
Step: 290, train/epoch: 0.06901475787162781
Step: 300, train/loss: 0.249099999666214
Step: 300, train/grad_norm: 39.11052703857422
Step: 300, train/learning_rate: 4.964302570442669e-05
Step: 300, train/epoch: 0.07139457762241364
Step: 310, train/loss: 0.08869999647140503
Step: 310, train/grad_norm: 0.0208139531314373
Step: 310, train/learning_rate: 4.96311295137275e-05
Step: 310, train/epoch: 0.07377438992261887
Step: 320, train/loss: 0.16529999673366547
Step: 320, train/grad_norm: 0.003860825439915061
Step: 320, train/learning_rate: 4.9619229685049504e-05
Step: 320, train/epoch: 0.0761542096734047
Step: 330, train/loss: 0.17170000076293945
Step: 330, train/grad_norm: 0.069742850959301
Step: 330, train/learning_rate: 4.960732985637151e-05
Step: 330, train/epoch: 0.07853402942419052
Step: 340, train/loss: 0.156700000166893
Step: 340, train/grad_norm: 73.62384033203125
Step: 340, train/learning_rate: 4.959543002769351e-05
Step: 340, train/epoch: 0.08091384917497635
Step: 350, train/loss: 0.016599999740719795
Step: 350, train/grad_norm: 0.0007949470309540629
Step: 350, train/learning_rate: 4.958353019901551e-05
Step: 350, train/epoch: 0.08329366892576218
Step: 360, train/loss: 0.1914999932050705
Step: 360, train/grad_norm: 0.0017511493060737848
Step: 360, train/learning_rate: 4.957163400831632e-05
Step: 360, train/epoch: 0.085673488676548
Step: 370, train/loss: 0.020099999383091927
Step: 370, train/grad_norm: 0.0017725250218063593
Step: 370, train/learning_rate: 4.9559734179638326e-05
Step: 370, train/epoch: 0.08805330842733383
Step: 380, train/loss: 0.11209999769926071
Step: 380, train/grad_norm: 0.43777191638946533
Step: 380, train/learning_rate: 4.954783435096033e-05
Step: 380, train/epoch: 0.09043312817811966
Step: 390, train/loss: 0.524399995803833
Step: 390, train/grad_norm: 42.71581268310547
Step: 390, train/learning_rate: 4.953593452228233e-05
Step: 390, train/epoch: 0.09281294792890549
Step: 400, train/loss: 0.09749999642372131
Step: 400, train/grad_norm: 0.004299738444387913
Step: 400, train/learning_rate: 4.9524034693604335e-05
Step: 400, train/epoch: 0.09519276767969131
Step: 410, train/loss: 0.2770000100135803
Step: 410, train/grad_norm: 0.0017971573397517204
Step: 410, train/learning_rate: 4.9512138502905145e-05
Step: 410, train/epoch: 0.09757258743047714
Step: 420, train/loss: 0.05790000036358833
Step: 420, train/grad_norm: 3.432204246520996
Step: 420, train/learning_rate: 4.950023867422715e-05
Step: 420, train/epoch: 0.09995240718126297
Step: 430, train/loss: 0.46630001068115234
Step: 430, train/grad_norm: 1.0953569412231445
Step: 430, train/learning_rate: 4.948833884554915e-05
Step: 430, train/epoch: 0.1023322194814682
Step: 440, train/loss: 0.005400000140070915
Step: 440, train/grad_norm: 0.002426045946776867
Step: 440, train/learning_rate: 4.9476439016871154e-05
Step: 440, train/epoch: 0.10471203923225403
Step: 450, train/loss: 0.04830000177025795
Step: 450, train/grad_norm: 0.0014936471125110984
Step: 450, train/learning_rate: 4.946453918819316e-05
Step: 450, train/epoch: 0.10709185898303986
Step: 460, train/loss: 0.010499999858438969
Step: 460, train/grad_norm: 0.0004712555673904717
Step: 460, train/learning_rate: 4.945264299749397e-05
Step: 460, train/epoch: 0.10947167873382568
Step: 470, train/loss: 0.041999999433755875
Step: 470, train/grad_norm: 0.010984026826918125
Step: 470, train/learning_rate: 4.944074316881597e-05
Step: 470, train/epoch: 0.11185149848461151
Step: 480, train/loss: 0.01119999960064888
Step: 480, train/grad_norm: 0.0002631806710269302
Step: 480, train/learning_rate: 4.9428843340137973e-05
Step: 480, train/epoch: 0.11423131823539734
Step: 490, train/loss: 0.03920000046491623
Step: 490, train/grad_norm: 0.034397006034851074
Step: 490, train/learning_rate: 4.9416943511459976e-05
Step: 490, train/epoch: 0.11661113798618317
Step: 500, train/loss: 0.07249999791383743
Step: 500, train/grad_norm: 0.002248314907774329
Step: 500, train/learning_rate: 4.940504368278198e-05
Step: 500, train/epoch: 0.118990957736969
Step: 510, train/loss: 0.15379999577999115
Step: 510, train/grad_norm: 0.006789153907448053
Step: 510, train/learning_rate: 4.939314749208279e-05
Step: 510, train/epoch: 0.12137077748775482
Step: 520, train/loss: 0.03319999948143959
Step: 520, train/grad_norm: 0.0006657738704234362
Step: 520, train/learning_rate: 4.938124766340479e-05
Step: 520, train/epoch: 0.12375059723854065
Step: 530, train/loss: 0.042399998754262924
Step: 530, train/grad_norm: 0.004427735228091478
Step: 530, train/learning_rate: 4.9369347834726796e-05
Step: 530, train/epoch: 0.12613041698932648
Step: 540, train/loss: 0.011500000022351742
Step: 540, train/grad_norm: 0.0024364956188946962
Step: 540, train/learning_rate: 4.93574480060488e-05
Step: 540, train/epoch: 0.1285102367401123
Step: 550, train/loss: 0.15469999611377716
Step: 550, train/grad_norm: 0.0011201396118849516
Step: 550, train/learning_rate: 4.93455481773708e-05
Step: 550, train/epoch: 0.13089005649089813
Step: 560, train/loss: 9.999999747378752e-05
Step: 560, train/grad_norm: 0.0010517529444769025
Step: 560, train/learning_rate: 4.933365198667161e-05
Step: 560, train/epoch: 0.13326987624168396
Step: 570, train/loss: 0.2946999967098236
Step: 570, train/grad_norm: 0.003823569044470787
Step: 570, train/learning_rate: 4.9321752157993615e-05
Step: 570, train/epoch: 0.1356496959924698
Step: 580, train/loss: 0.008500000461935997
Step: 580, train/grad_norm: 0.0037669844459742308
Step: 580, train/learning_rate: 4.930985232931562e-05
Step: 580, train/epoch: 0.13802951574325562
Step: 590, train/loss: 0.07980000227689743
Step: 590, train/grad_norm: 57.595741271972656
Step: 590, train/learning_rate: 4.929795250063762e-05
Step: 590, train/epoch: 0.14040933549404144
Step: 600, train/loss: 0.30660000443458557
Step: 600, train/grad_norm: 0.04702979698777199
Step: 600, train/learning_rate: 4.9286052671959624e-05
Step: 600, train/epoch: 0.14278915524482727
Step: 610, train/loss: 0.028599999845027924
Step: 610, train/grad_norm: 0.17623017728328705
Step: 610, train/learning_rate: 4.9274156481260434e-05
Step: 610, train/epoch: 0.1451689600944519
Step: 620, train/loss: 0.3142000138759613
Step: 620, train/grad_norm: 66.5857162475586
Step: 620, train/learning_rate: 4.926225665258244e-05
Step: 620, train/epoch: 0.14754877984523773
Step: 630, train/loss: 0.03099999949336052
Step: 630, train/grad_norm: 0.005678697023540735
Step: 630, train/learning_rate: 4.925035682390444e-05
Step: 630, train/epoch: 0.14992859959602356
Step: 640, train/loss: 0.0019000000320374966
Step: 640, train/grad_norm: 0.00025861221365630627
Step: 640, train/learning_rate: 4.923845699522644e-05
Step: 640, train/epoch: 0.1523084193468094
Step: 650, train/loss: 0.18039999902248383
Step: 650, train/grad_norm: 0.0017580888234078884
Step: 650, train/learning_rate: 4.9226557166548446e-05
Step: 650, train/epoch: 0.15468823909759521
Step: 660, train/loss: 0.18709999322891235
Step: 660, train/grad_norm: 0.0048030889593064785
Step: 660, train/learning_rate: 4.9214660975849256e-05
Step: 660, train/epoch: 0.15706805884838104
Step: 670, train/loss: 0.08709999918937683
Step: 670, train/grad_norm: 0.0028527809772640467
Step: 670, train/learning_rate: 4.920276114717126e-05
Step: 670, train/epoch: 0.15944787859916687
Step: 680, train/loss: 0.07609999924898148
Step: 680, train/grad_norm: 0.0014952652854844928
Step: 680, train/learning_rate: 4.919086131849326e-05
Step: 680, train/epoch: 0.1618276983499527
Step: 690, train/loss: 0.0828000009059906
Step: 690, train/grad_norm: 0.001226625288836658
Step: 690, train/learning_rate: 4.9178961489815265e-05
Step: 690, train/epoch: 0.16420751810073853
Step: 700, train/loss: 0.013899999670684338
Step: 700, train/grad_norm: 0.03222474083304405
Step: 700, train/learning_rate: 4.916706166113727e-05
Step: 700, train/epoch: 0.16658733785152435
Step: 710, train/loss: 0.2387000024318695
Step: 710, train/grad_norm: 0.0042311991564929485
Step: 710, train/learning_rate: 4.915516547043808e-05
Step: 710, train/epoch: 0.16896715760231018
Step: 720, train/loss: 0.09470000118017197
Step: 720, train/grad_norm: 0.03443637862801552
Step: 720, train/learning_rate: 4.914326564176008e-05
Step: 720, train/epoch: 0.171346977353096
Step: 730, train/loss: 0.00559999980032444
Step: 730, train/grad_norm: 0.0055114878341555595
Step: 730, train/learning_rate: 4.9131365813082084e-05
Step: 730, train/epoch: 0.17372679710388184
Step: 740, train/loss: 0.15399999916553497
Step: 740, train/grad_norm: 27.4868106842041
Step: 740, train/learning_rate: 4.911946598440409e-05
Step: 740, train/epoch: 0.17610661685466766
Step: 750, train/loss: 0.12319999933242798
Step: 750, train/grad_norm: 0.915812075138092
Step: 750, train/learning_rate: 4.910756615572609e-05
Step: 750, train/epoch: 0.1784864366054535
Step: 760, train/loss: 0.0003000000142492354
Step: 760, train/grad_norm: 0.021922748535871506
Step: 760, train/learning_rate: 4.90956699650269e-05
Step: 760, train/epoch: 0.18086625635623932
Step: 770, train/loss: 0.08609999716281891
Step: 770, train/grad_norm: 0.0012441134313121438
Step: 770, train/learning_rate: 4.90837701363489e-05
Step: 770, train/epoch: 0.18324607610702515
Step: 780, train/loss: 0.012500000186264515
Step: 780, train/grad_norm: 0.004286745097488165
Step: 780, train/learning_rate: 4.9071870307670906e-05
Step: 780, train/epoch: 0.18562589585781097
Step: 790, train/loss: 0.17389999330043793
Step: 790, train/grad_norm: 28.16681480407715
Step: 790, train/learning_rate: 4.905997047899291e-05
Step: 790, train/epoch: 0.1880057156085968
Step: 800, train/loss: 0.044599998742341995
Step: 800, train/grad_norm: 0.0009622987126931548
Step: 800, train/learning_rate: 4.904807065031491e-05
Step: 800, train/epoch: 0.19038553535938263
Step: 810, train/loss: 0.5256999731063843
Step: 810, train/grad_norm: 0.016850518062710762
Step: 810, train/learning_rate: 4.903617445961572e-05
Step: 810, train/epoch: 0.19276535511016846
Step: 820, train/loss: 0.20839999616146088
Step: 820, train/grad_norm: 1.631163239479065
Step: 820, train/learning_rate: 4.9024274630937725e-05
Step: 820, train/epoch: 0.19514517486095428
Step: 830, train/loss: 0.001500000013038516
Step: 830, train/grad_norm: 0.018879741430282593
Step: 830, train/learning_rate: 4.901237480225973e-05
Step: 830, train/epoch: 0.1975249946117401
Step: 840, train/loss: 0.39100000262260437
Step: 840, train/grad_norm: 24.83039093017578
Step: 840, train/learning_rate: 4.900047497358173e-05
Step: 840, train/epoch: 0.19990481436252594
Step: 850, train/loss: 0.00019999999494757503
Step: 850, train/grad_norm: 0.01216732244938612
Step: 850, train/learning_rate: 4.8988575144903734e-05
Step: 850, train/epoch: 0.20228461921215057
Step: 860, train/loss: 0.003100000089034438
Step: 860, train/grad_norm: 0.041617538779973984
Step: 860, train/learning_rate: 4.8976678954204544e-05
Step: 860, train/epoch: 0.2046644389629364
Step: 870, train/loss: 0.21170000731945038
Step: 870, train/grad_norm: 0.005133892875164747
Step: 870, train/learning_rate: 4.896477912552655e-05
Step: 870, train/epoch: 0.20704425871372223
Step: 880, train/loss: 0.0957999974489212
Step: 880, train/grad_norm: 0.04512975737452507
Step: 880, train/learning_rate: 4.895287929684855e-05
Step: 880, train/epoch: 0.20942407846450806
Step: 890, train/loss: 0.002899999963119626
Step: 890, train/grad_norm: 0.007356643211096525
Step: 890, train/learning_rate: 4.8940979468170553e-05
Step: 890, train/epoch: 0.21180389821529388
Step: 900, train/loss: 0.06809999793767929
Step: 900, train/grad_norm: 6.14752197265625
Step: 900, train/learning_rate: 4.8929079639492556e-05
Step: 900, train/epoch: 0.2141837179660797
Step: 910, train/loss: 0.0008999999845400453
Step: 910, train/grad_norm: 0.005037209484726191
Step: 910, train/learning_rate: 4.8917183448793367e-05
Step: 910, train/epoch: 0.21656353771686554
Step: 920, train/loss: 0.0005000000237487257
Step: 920, train/grad_norm: 0.0026469402946531773
Step: 920, train/learning_rate: 4.890528362011537e-05
Step: 920, train/epoch: 0.21894335746765137
Step: 930, train/loss: 0.1436000019311905
Step: 930, train/grad_norm: 78.74695587158203
Step: 930, train/learning_rate: 4.889338379143737e-05
Step: 930, train/epoch: 0.2213231772184372
Step: 940, train/loss: 0.11389999836683273
Step: 940, train/grad_norm: 0.08578406274318695
Step: 940, train/learning_rate: 4.8881483962759376e-05
Step: 940, train/epoch: 0.22370299696922302
Step: 950, train/loss: 0.0007999999797903001
Step: 950, train/grad_norm: 2.4575600624084473
Step: 950, train/learning_rate: 4.886958413408138e-05
Step: 950, train/epoch: 0.22608281672000885
Step: 960, train/loss: 0.11169999837875366
Step: 960, train/grad_norm: 0.0021779255475848913
Step: 960, train/learning_rate: 4.885768794338219e-05
Step: 960, train/epoch: 0.22846263647079468
Step: 970, train/loss: 0.00039999998989515007
Step: 970, train/grad_norm: 0.008461114019155502
Step: 970, train/learning_rate: 4.884578811470419e-05
Step: 970, train/epoch: 0.2308424562215805
Step: 980, train/loss: 0.29280000925064087
Step: 980, train/grad_norm: 0.04983271658420563
Step: 980, train/learning_rate: 4.8833888286026195e-05
Step: 980, train/epoch: 0.23322227597236633
Step: 990, train/loss: 0.09290000051259995
Step: 990, train/grad_norm: 0.0051415120251476765
Step: 990, train/learning_rate: 4.88219884573482e-05
Step: 990, train/epoch: 0.23560209572315216
Step: 1000, train/loss: 0.25540000200271606
Step: 1000, train/grad_norm: 0.009394019842147827
Step: 1000, train/learning_rate: 4.88100886286702e-05
Step: 1000, train/epoch: 0.237981915473938
Step: 1010, train/loss: 0.1371999979019165
Step: 1010, train/grad_norm: 0.06652523577213287
Step: 1010, train/learning_rate: 4.879819243797101e-05
Step: 1010, train/epoch: 0.24036173522472382
Step: 1020, train/loss: 0.06549999862909317
Step: 1020, train/grad_norm: 0.0030951027292758226
Step: 1020, train/learning_rate: 4.8786292609293014e-05
Step: 1020, train/epoch: 0.24274155497550964
Step: 1030, train/loss: 0.03689999878406525
Step: 1030, train/grad_norm: 0.002690200926735997
Step: 1030, train/learning_rate: 4.877439278061502e-05
Step: 1030, train/epoch: 0.24512137472629547
Step: 1040, train/loss: 0.031300000846385956
Step: 1040, train/grad_norm: 0.0002003464032895863
Step: 1040, train/learning_rate: 4.876249295193702e-05
Step: 1040, train/epoch: 0.2475011944770813
Step: 1050, train/loss: 0.02250000089406967
Step: 1050, train/grad_norm: 0.0001662430149735883
Step: 1050, train/learning_rate: 4.875059676123783e-05
Step: 1050, train/epoch: 0.24988101422786713
Step: 1060, train/loss: 0.1932000070810318
Step: 1060, train/grad_norm: 0.00021779380040243268
Step: 1060, train/learning_rate: 4.873869693255983e-05
Step: 1060, train/epoch: 0.25226083397865295
Step: 1070, train/loss: 0.1234000027179718
Step: 1070, train/grad_norm: 0.16400480270385742
Step: 1070, train/learning_rate: 4.8726797103881836e-05
Step: 1070, train/epoch: 0.2546406388282776
Step: 1080, train/loss: 0.006200000178068876
Step: 1080, train/grad_norm: 0.006293386220932007
Step: 1080, train/learning_rate: 4.871489727520384e-05
Step: 1080, train/epoch: 0.2570204734802246
Step: 1090, train/loss: 0.053700000047683716
Step: 1090, train/grad_norm: 0.08434765040874481
Step: 1090, train/learning_rate: 4.870299744652584e-05
Step: 1090, train/epoch: 0.25940027832984924
Step: 1100, train/loss: 0.017500000074505806
Step: 1100, train/grad_norm: 0.005702447611838579
Step: 1100, train/learning_rate: 4.869110125582665e-05
Step: 1100, train/epoch: 0.26178011298179626
Step: 1110, train/loss: 0.1454000025987625
Step: 1110, train/grad_norm: 0.00559551827609539
Step: 1110, train/learning_rate: 4.8679201427148655e-05
Step: 1110, train/epoch: 0.2641599178314209
Step: 1120, train/loss: 0.1671999990940094
Step: 1120, train/grad_norm: 0.0040247440338134766
Step: 1120, train/learning_rate: 4.866730159847066e-05
Step: 1120, train/epoch: 0.2665397524833679
Step: 1130, train/loss: 0.011099999770522118
Step: 1130, train/grad_norm: 0.00016337491979356855
Step: 1130, train/learning_rate: 4.865540176979266e-05
Step: 1130, train/epoch: 0.26891955733299255
Step: 1140, train/loss: 0.10890000313520432
Step: 1140, train/grad_norm: 0.04739287123084068
Step: 1140, train/learning_rate: 4.8643501941114664e-05
Step: 1140, train/epoch: 0.2712993919849396
Step: 1150, train/loss: 0.1477999985218048
Step: 1150, train/grad_norm: 0.000604675617069006
Step: 1150, train/learning_rate: 4.8631605750415474e-05
Step: 1150, train/epoch: 0.2736791968345642
Step: 1160, train/loss: 0.02500000037252903
Step: 1160, train/grad_norm: 1.55447518825531
Step: 1160, train/learning_rate: 4.861970592173748e-05
Step: 1160, train/epoch: 0.27605903148651123
Step: 1170, train/loss: 0.30959999561309814
Step: 1170, train/grad_norm: 34.090675354003906
Step: 1170, train/learning_rate: 4.860780609305948e-05
Step: 1170, train/epoch: 0.27843883633613586
Step: 1180, train/loss: 0.00039999998989515007
Step: 1180, train/grad_norm: 0.003172490978613496
Step: 1180, train/learning_rate: 4.859590626438148e-05
Step: 1180, train/epoch: 0.2808186709880829
Step: 1190, train/loss: 0.00019999999494757503
Step: 1190, train/grad_norm: 0.0002683116472326219
Step: 1190, train/learning_rate: 4.8584006435703486e-05
Step: 1190, train/epoch: 0.2831984758377075
Step: 1200, train/loss: 0.1915999948978424
Step: 1200, train/grad_norm: 36.67869567871094
Step: 1200, train/learning_rate: 4.8572110245004296e-05
Step: 1200, train/epoch: 0.28557831048965454
Step: 1210, train/loss: 0.08919999748468399
Step: 1210, train/grad_norm: 0.007443081121891737
Step: 1210, train/learning_rate: 4.85602104163263e-05
Step: 1210, train/epoch: 0.2879581153392792
Step: 1220, train/loss: 0.00019999999494757503
Step: 1220, train/grad_norm: 0.0011447753058746457
Step: 1220, train/learning_rate: 4.85483105876483e-05
Step: 1220, train/epoch: 0.2903379201889038
Step: 1230, train/loss: 9.999999747378752e-05
Step: 1230, train/grad_norm: 0.010432112962007523
Step: 1230, train/learning_rate: 4.8536410758970305e-05
Step: 1230, train/epoch: 0.29271775484085083
Step: 1240, train/loss: 0.02239999920129776
Step: 1240, train/grad_norm: 0.12051920592784882
Step: 1240, train/learning_rate: 4.852451093029231e-05
Step: 1240, train/epoch: 0.29509755969047546
Step: 1250, train/loss: 0.0010999999940395355
Step: 1250, train/grad_norm: 0.0025614791084080935
Step: 1250, train/learning_rate: 4.851261473959312e-05
Step: 1250, train/epoch: 0.2974773943424225
Step: 1260, train/loss: 0.04179999977350235
Step: 1260, train/grad_norm: 92.20906829833984
Step: 1260, train/learning_rate: 4.850071491091512e-05
Step: 1260, train/epoch: 0.2998571991920471
Step: 1270, train/loss: 0.00019999999494757503
Step: 1270, train/grad_norm: 0.00046277313958853483
Step: 1270, train/learning_rate: 4.8488815082237124e-05
Step: 1270, train/epoch: 0.30223703384399414
Step: 1280, train/loss: 0.0003000000142492354
Step: 1280, train/grad_norm: 1.4359024135046639e-05
Step: 1280, train/learning_rate: 4.847691525355913e-05
Step: 1280, train/epoch: 0.3046168386936188
Step: 1290, train/loss: 9.999999747378752e-05
Step: 1290, train/grad_norm: 0.00017406439292244613
Step: 1290, train/learning_rate: 4.846501542488113e-05
Step: 1290, train/epoch: 0.3069966733455658
Step: 1300, train/loss: 0.0
Step: 1300, train/grad_norm: 0.006810705177485943
Step: 1300, train/learning_rate: 4.845311923418194e-05
Step: 1300, train/epoch: 0.30937647819519043
Step: 1310, train/loss: 0.0
Step: 1310, train/grad_norm: 0.0021888352930545807
Step: 1310, train/learning_rate: 4.8441219405503944e-05
Step: 1310, train/epoch: 0.31175631284713745
Step: 1320, train/loss: 0.17159999907016754
Step: 1320, train/grad_norm: 5.2248407882871106e-05
Step: 1320, train/learning_rate: 4.8429319576825947e-05
Step: 1320, train/epoch: 0.3141361176967621
Step: 1330, train/loss: 0.0
Step: 1330, train/grad_norm: 0.0009101265459321439
Step: 1330, train/learning_rate: 4.841741974814795e-05
Step: 1330, train/epoch: 0.3165159523487091
Step: 1340, train/loss: 0.11460000276565552
Step: 1340, train/grad_norm: 0.0010829770471900702
Step: 1340, train/learning_rate: 4.840551991946995e-05
Step: 1340, train/epoch: 0.31889575719833374
Step: 1350, train/loss: 0.0771000012755394
Step: 1350, train/grad_norm: 0.03548157215118408
Step: 1350, train/learning_rate: 4.839362372877076e-05
Step: 1350, train/epoch: 0.32127559185028076
Step: 1360, train/loss: 0.00039999998989515007
Step: 1360, train/grad_norm: 0.005562250968068838
Step: 1360, train/learning_rate: 4.8381723900092766e-05
Step: 1360, train/epoch: 0.3236553966999054
Step: 1370, train/loss: 0.07349999994039536
Step: 1370, train/grad_norm: 67.54328918457031
Step: 1370, train/learning_rate: 4.836982407141477e-05
Step: 1370, train/epoch: 0.3260352313518524
Step: 1380, train/loss: 0.20069999992847443
Step: 1380, train/grad_norm: 0.020528659224510193
Step: 1380, train/learning_rate: 4.835792424273677e-05
Step: 1380, train/epoch: 0.32841503620147705
Step: 1390, train/loss: 0.13619999587535858
Step: 1390, train/grad_norm: 0.43994858860969543
Step: 1390, train/learning_rate: 4.8346024414058775e-05
Step: 1390, train/epoch: 0.3307948708534241
Step: 1400, train/loss: 9.999999747378752e-05
Step: 1400, train/grad_norm: 0.00022415147395804524
Step: 1400, train/learning_rate: 4.8334128223359585e-05
Step: 1400, train/epoch: 0.3331746757030487
Step: 1410, train/loss: 0.02800000086426735
Step: 1410, train/grad_norm: 0.04955103620886803
Step: 1410, train/learning_rate: 4.832222839468159e-05
Step: 1410, train/epoch: 0.3355545103549957
Step: 1420, train/loss: 0.0008999999845400453
Step: 1420, train/grad_norm: 0.0032417052425444126
Step: 1420, train/learning_rate: 4.831032856600359e-05
Step: 1420, train/epoch: 0.33793431520462036
Step: 1430, train/loss: 0.21770000457763672
Step: 1430, train/grad_norm: 0.005658262874931097
Step: 1430, train/learning_rate: 4.8298428737325594e-05
Step: 1430, train/epoch: 0.3403141498565674
Step: 1440, train/loss: 0.10779999941587448
Step: 1440, train/grad_norm: 0.08336164802312851
Step: 1440, train/learning_rate: 4.82865289086476e-05
Step: 1440, train/epoch: 0.342693954706192
Step: 1450, train/loss: 0.14159999787807465
Step: 1450, train/grad_norm: 0.016955556347966194
Step: 1450, train/learning_rate: 4.827463271794841e-05
Step: 1450, train/epoch: 0.34507375955581665
Step: 1460, train/loss: 0.15760000050067902
Step: 1460, train/grad_norm: 3.5595016479492188
Step: 1460, train/learning_rate: 4.826273288927041e-05
Step: 1460, train/epoch: 0.34745359420776367
Step: 1470, train/loss: 0.1281999945640564
Step: 1470, train/grad_norm: 0.0010891512501984835
Step: 1470, train/learning_rate: 4.825083306059241e-05
Step: 1470, train/epoch: 0.3498333990573883
Step: 1480, train/loss: 9.999999747378752e-05
Step: 1480, train/grad_norm: 0.000625037238933146
Step: 1480, train/learning_rate: 4.8238933231914416e-05
Step: 1480, train/epoch: 0.3522132337093353
Step: 1490, train/loss: 0.08919999748468399
Step: 1490, train/grad_norm: 0.010615197941660881
Step: 1490, train/learning_rate: 4.822703340323642e-05
Step: 1490, train/epoch: 0.35459303855895996
Step: 1500, train/loss: 9.999999747378752e-05
Step: 1500, train/grad_norm: 0.010334267280995846
Step: 1500, train/learning_rate: 4.821513721253723e-05
Step: 1500, train/epoch: 0.356972873210907
Step: 1510, train/loss: 0.16339999437332153
Step: 1510, train/grad_norm: 0.10213285684585571
Step: 1510, train/learning_rate: 4.820323738385923e-05
Step: 1510, train/epoch: 0.3593526780605316
Step: 1520, train/loss: 0.08780000358819962
Step: 1520, train/grad_norm: 0.008172215893864632
Step: 1520, train/learning_rate: 4.8191337555181235e-05
Step: 1520, train/epoch: 0.36173251271247864
Step: 1530, train/loss: 0.00019999999494757503
Step: 1530, train/grad_norm: 0.005548374727368355
Step: 1530, train/learning_rate: 4.817943772650324e-05
Step: 1530, train/epoch: 0.36411231756210327
Step: 1540, train/loss: 0.1445000022649765
Step: 1540, train/grad_norm: 0.00042443061829544604
Step: 1540, train/learning_rate: 4.816753789782524e-05
Step: 1540, train/epoch: 0.3664921522140503
Step: 1550, train/loss: 0.002099999925121665
Step: 1550, train/grad_norm: 0.01301534939557314
Step: 1550, train/learning_rate: 4.815564170712605e-05
Step: 1550, train/epoch: 0.3688719570636749
Step: 1560, train/loss: 0.03739999979734421
Step: 1560, train/grad_norm: 0.002533874474465847
Step: 1560, train/learning_rate: 4.8143741878448054e-05
Step: 1560, train/epoch: 0.37125179171562195
Step: 1570, train/loss: 0.09290000051259995
Step: 1570, train/grad_norm: 0.2398378700017929
Step: 1570, train/learning_rate: 4.813184204977006e-05
Step: 1570, train/epoch: 0.3736315965652466
Step: 1580, train/loss: 0.04769999906420708
Step: 1580, train/grad_norm: 0.08948178589344025
Step: 1580, train/learning_rate: 4.811994222109206e-05
Step: 1580, train/epoch: 0.3760114312171936
Step: 1590, train/loss: 0.0738999992609024
Step: 1590, train/grad_norm: 0.0030400576069951057
Step: 1590, train/learning_rate: 4.810804239241406e-05
Step: 1590, train/epoch: 0.37839123606681824
Step: 1600, train/loss: 0.0008999999845400453
Step: 1600, train/grad_norm: 0.0031581190414726734
Step: 1600, train/learning_rate: 4.809614620171487e-05
Step: 1600, train/epoch: 0.38077107071876526
Step: 1610, train/loss: 9.999999747378752e-05
Step: 1610, train/grad_norm: 0.15144892036914825
Step: 1610, train/learning_rate: 4.8084246373036876e-05
Step: 1610, train/epoch: 0.3831508755683899
Step: 1620, train/loss: 0.13019999861717224
Step: 1620, train/grad_norm: 0.00931631587445736
Step: 1620, train/learning_rate: 4.807234654435888e-05
Step: 1620, train/epoch: 0.3855307102203369
Step: 1630, train/loss: 0.05220000073313713
Step: 1630, train/grad_norm: 0.001619502785615623
Step: 1630, train/learning_rate: 4.806044671568088e-05
Step: 1630, train/epoch: 0.38791051506996155
Step: 1640, train/loss: 0.25380000472068787
Step: 1640, train/grad_norm: 0.02704104594886303
Step: 1640, train/learning_rate: 4.8048546887002885e-05
Step: 1640, train/epoch: 0.39029034972190857
Step: 1650, train/loss: 0.0012000000569969416
Step: 1650, train/grad_norm: 0.05177605152130127
Step: 1650, train/learning_rate: 4.8036650696303695e-05
Step: 1650, train/epoch: 0.3926701545715332
Step: 1660, train/loss: 0.0803999975323677
Step: 1660, train/grad_norm: 68.58154296875
Step: 1660, train/learning_rate: 4.80247508676257e-05
Step: 1660, train/epoch: 0.3950499892234802
Step: 1670, train/loss: 0.0005000000237487257
Step: 1670, train/grad_norm: 1.9629909729701467e-05
Step: 1670, train/learning_rate: 4.80128510389477e-05
Step: 1670, train/epoch: 0.39742979407310486
Step: 1680, train/loss: 0.0005000000237487257
Step: 1680, train/grad_norm: 0.00241999514400959
Step: 1680, train/learning_rate: 4.8000951210269704e-05
Step: 1680, train/epoch: 0.3998096287250519
Step: 1690, train/loss: 0.08110000193119049
Step: 1690, train/grad_norm: 0.002817933913320303
Step: 1690, train/learning_rate: 4.798905138159171e-05
Step: 1690, train/epoch: 0.4021894335746765
Step: 1700, train/loss: 0.0
Step: 1700, train/grad_norm: 0.002126503735780716
Step: 1700, train/learning_rate: 4.797715519089252e-05
Step: 1700, train/epoch: 0.40456923842430115
Step: 1710, train/loss: 0.0421999990940094
Step: 1710, train/grad_norm: 0.007547788321971893
Step: 1710, train/learning_rate: 4.796525536221452e-05
Step: 1710, train/epoch: 0.40694907307624817
Step: 1720, train/loss: 0.002099999925121665
Step: 1720, train/grad_norm: 0.003785515669733286
Step: 1720, train/learning_rate: 4.7953355533536524e-05
Step: 1720, train/epoch: 0.4093288779258728
Step: 1730, train/loss: 0.0
Step: 1730, train/grad_norm: 0.009543935768306255
Step: 1730, train/learning_rate: 4.7941455704858527e-05
Step: 1730, train/epoch: 0.4117087125778198
Step: 1740, train/loss: 0.17599999904632568
Step: 1740, train/grad_norm: 3.4339711419306695e-05
Step: 1740, train/learning_rate: 4.792955587618053e-05
Step: 1740, train/epoch: 0.41408851742744446
Step: 1750, train/loss: 0.04729999974370003
Step: 1750, train/grad_norm: 0.0008742103236727417
Step: 1750, train/learning_rate: 4.791765968548134e-05
Step: 1750, train/epoch: 0.4164683520793915
Step: 1760, train/loss: 0.0010000000474974513
Step: 1760, train/grad_norm: 0.000675315735861659
Step: 1760, train/learning_rate: 4.790575985680334e-05
Step: 1760, train/epoch: 0.4188481569290161
Step: 1770, train/loss: 0.0
Step: 1770, train/grad_norm: 0.006322978995740414
Step: 1770, train/learning_rate: 4.7893860028125346e-05
Step: 1770, train/epoch: 0.42122799158096313
Step: 1780, train/loss: 0.002300000051036477
Step: 1780, train/grad_norm: 0.0005661268369294703
Step: 1780, train/learning_rate: 4.788196019944735e-05
Step: 1780, train/epoch: 0.42360779643058777
Step: 1790, train/loss: 0.04399999976158142
Step: 1790, train/grad_norm: 9.155613952316344e-05
Step: 1790, train/learning_rate: 4.787006037076935e-05
Step: 1790, train/epoch: 0.4259876310825348
Step: 1800, train/loss: 0.00139999995008111
Step: 1800, train/grad_norm: 3.110180139541626
Step: 1800, train/learning_rate: 4.785816418007016e-05
Step: 1800, train/epoch: 0.4283674359321594
Step: 1810, train/loss: 0.0003000000142492354
Step: 1810, train/grad_norm: 0.0033422335982322693
Step: 1810, train/learning_rate: 4.7846264351392165e-05
Step: 1810, train/epoch: 0.43074727058410645
Step: 1820, train/loss: 0.07000000029802322
Step: 1820, train/grad_norm: 0.25727346539497375
Step: 1820, train/learning_rate: 4.783436452271417e-05
Step: 1820, train/epoch: 0.4331270754337311
Step: 1830, train/loss: 0.2079000025987625
Step: 1830, train/grad_norm: 0.0029829181730747223
Step: 1830, train/learning_rate: 4.782246469403617e-05
Step: 1830, train/epoch: 0.4355069100856781
Step: 1840, train/loss: 0.09769999980926514
Step: 1840, train/grad_norm: 25.822811126708984
Step: 1840, train/learning_rate: 4.7810564865358174e-05
Step: 1840, train/epoch: 0.43788671493530273
Step: 1850, train/loss: 0.2524999976158142
Step: 1850, train/grad_norm: 47.871238708496094
Step: 1850, train/learning_rate: 4.7798668674658984e-05
Step: 1850, train/epoch: 0.44026654958724976
Step: 1860, train/loss: 0.0017999999690800905
Step: 1860, train/grad_norm: 0.08905427157878876
Step: 1860, train/learning_rate: 4.778676884598099e-05
Step: 1860, train/epoch: 0.4426463544368744
Step: 1870, train/loss: 0.029400000348687172
Step: 1870, train/grad_norm: 0.005527744069695473
Step: 1870, train/learning_rate: 4.777486901730299e-05
Step: 1870, train/epoch: 0.4450261890888214
Step: 1880, train/loss: 0.0008999999845400453
Step: 1880, train/grad_norm: 0.008219484239816666
Step: 1880, train/learning_rate: 4.776296918862499e-05
Step: 1880, train/epoch: 0.44740599393844604
Step: 1890, train/loss: 0.012799999676644802
Step: 1890, train/grad_norm: 0.0008455866482108831
Step: 1890, train/learning_rate: 4.7751069359946996e-05
Step: 1890, train/epoch: 0.44978582859039307
Step: 1900, train/loss: 0.040800001472234726
Step: 1900, train/grad_norm: 4.524832183960825e-05
Step: 1900, train/learning_rate: 4.7739173169247806e-05
Step: 1900, train/epoch: 0.4521656334400177
Step: 1910, train/loss: 0.13169999420642853
Step: 1910, train/grad_norm: 0.0054563917219638824
Step: 1910, train/learning_rate: 4.772727334056981e-05
Step: 1910, train/epoch: 0.4545454680919647
Step: 1920, train/loss: 0.12630000710487366
Step: 1920, train/grad_norm: 0.08962974697351456
Step: 1920, train/learning_rate: 4.771537351189181e-05
Step: 1920, train/epoch: 0.45692527294158936
Step: 1930, train/loss: 0.018400000408291817
Step: 1930, train/grad_norm: 0.008869718760251999
Step: 1930, train/learning_rate: 4.7703473683213815e-05
Step: 1930, train/epoch: 0.4593051075935364
Step: 1940, train/loss: 0.00039999998989515007
Step: 1940, train/grad_norm: 0.06973404437303543
Step: 1940, train/learning_rate: 4.769157385453582e-05
Step: 1940, train/epoch: 0.461684912443161
Step: 1950, train/loss: 0.0006000000284984708
Step: 1950, train/grad_norm: 0.00025567205739207566
Step: 1950, train/learning_rate: 4.767967766383663e-05
Step: 1950, train/epoch: 0.46406471729278564
Step: 1960, train/loss: 0.00019999999494757503
Step: 1960, train/grad_norm: 0.0007471389253623784
Step: 1960, train/learning_rate: 4.766777783515863e-05
Step: 1960, train/epoch: 0.46644455194473267
Step: 1970, train/loss: 0.11490000039339066
Step: 1970, train/grad_norm: 0.0013369786320254207
Step: 1970, train/learning_rate: 4.7655878006480634e-05
Step: 1970, train/epoch: 0.4688243567943573
Step: 1980, train/loss: 0.0560000017285347
Step: 1980, train/grad_norm: 0.027194464579224586
Step: 1980, train/learning_rate: 4.764397817780264e-05
Step: 1980, train/epoch: 0.4712041914463043
Step: 1990, train/loss: 0.0786999985575676
Step: 1990, train/grad_norm: 50.099971771240234
Step: 1990, train/learning_rate: 4.763207834912464e-05
Step: 1990, train/epoch: 0.47358399629592896
Step: 2000, train/loss: 0.0
Step: 2000, train/grad_norm: 7.102096424205229e-05
Step: 2000, train/learning_rate: 4.762018215842545e-05
Step: 2000, train/epoch: 0.475963830947876
Step: 2010, train/loss: 0.15559999644756317
Step: 2010, train/grad_norm: 0.8305935263633728
Step: 2010, train/learning_rate: 4.760828232974745e-05
Step: 2010, train/epoch: 0.4783436357975006
Step: 2020, train/loss: 0.008299999870359898
Step: 2020, train/grad_norm: 25.08686637878418
Step: 2020, train/learning_rate: 4.7596382501069456e-05
Step: 2020, train/epoch: 0.48072347044944763
Step: 2030, train/loss: 0.21449999511241913
Step: 2030, train/grad_norm: 0.000778433692175895
Step: 2030, train/learning_rate: 4.758448267239146e-05
Step: 2030, train/epoch: 0.48310327529907227
Step: 2040, train/loss: 0.00019999999494757503
Step: 2040, train/grad_norm: 0.0034223301336169243
Step: 2040, train/learning_rate: 4.757258284371346e-05
Step: 2040, train/epoch: 0.4854831099510193
Step: 2050, train/loss: 0.22540000081062317
Step: 2050, train/grad_norm: 0.002149067586287856
Step: 2050, train/learning_rate: 4.756068665301427e-05
Step: 2050, train/epoch: 0.4878629148006439
Step: 2060, train/loss: 0.19189999997615814
Step: 2060, train/grad_norm: 0.019468966871500015
Step: 2060, train/learning_rate: 4.7548786824336275e-05
Step: 2060, train/epoch: 0.49024274945259094
Step: 2070, train/loss: 0.02889999933540821
Step: 2070, train/grad_norm: 1.1026148796081543
Step: 2070, train/learning_rate: 4.753688699565828e-05
Step: 2070, train/epoch: 0.4926225543022156
Step: 2080, train/loss: 0.17159999907016754
Step: 2080, train/grad_norm: 0.003981144167482853
Step: 2080, train/learning_rate: 4.752498716698028e-05
Step: 2080, train/epoch: 0.4950023889541626
Step: 2090, train/loss: 0.009100000374019146
Step: 2090, train/grad_norm: 0.4340382516384125
Step: 2090, train/learning_rate: 4.7513087338302284e-05
Step: 2090, train/epoch: 0.49738219380378723
Step: 2100, train/loss: 0.002300000051036477
Step: 2100, train/grad_norm: 0.16417282819747925
Step: 2100, train/learning_rate: 4.7501191147603095e-05
Step: 2100, train/epoch: 0.49976202845573425
Step: 2110, train/loss: 0.005799999926239252
Step: 2110, train/grad_norm: 0.019642263650894165
Step: 2110, train/learning_rate: 4.74892913189251e-05
Step: 2110, train/epoch: 0.5021418333053589
Step: 2120, train/loss: 0.0723000019788742
Step: 2120, train/grad_norm: 0.06639494001865387
Step: 2120, train/learning_rate: 4.74773914902471e-05
Step: 2120, train/epoch: 0.5045216679573059
Step: 2130, train/loss: 0.10209999978542328
Step: 2130, train/grad_norm: 0.0007248307229019701
Step: 2130, train/learning_rate: 4.7465491661569104e-05
Step: 2130, train/epoch: 0.5069015026092529
Step: 2140, train/loss: 0.1339000016450882
Step: 2140, train/grad_norm: 0.010140791535377502
Step: 2140, train/learning_rate: 4.7453591832891107e-05
Step: 2140, train/epoch: 0.5092812776565552
Step: 2150, train/loss: 0.0
Step: 2150, train/grad_norm: 0.0003175127203576267
Step: 2150, train/learning_rate: 4.744169564219192e-05
Step: 2150, train/epoch: 0.5116611123085022
Step: 2160, train/loss: 0.00019999999494757503
Step: 2160, train/grad_norm: 0.0012437297264114022
Step: 2160, train/learning_rate: 4.742979581351392e-05
Step: 2160, train/epoch: 0.5140409469604492
Step: 2170, train/loss: 0.05689999833703041
Step: 2170, train/grad_norm: 0.0023079756647348404
Step: 2170, train/learning_rate: 4.741789598483592e-05
Step: 2170, train/epoch: 0.5164207816123962
Step: 2180, train/loss: 0.0005000000237487257
Step: 2180, train/grad_norm: 0.011909635737538338
Step: 2180, train/learning_rate: 4.7405996156157926e-05
Step: 2180, train/epoch: 0.5188005566596985
Step: 2190, train/loss: 0.008899999782443047
Step: 2190, train/grad_norm: 0.0004445106314960867
Step: 2190, train/learning_rate: 4.739409632747993e-05
Step: 2190, train/epoch: 0.5211803913116455
Step: 2200, train/loss: 0.0
Step: 2200, train/grad_norm: 0.0027985461056232452
Step: 2200, train/learning_rate: 4.738220013678074e-05
Step: 2200, train/epoch: 0.5235602259635925
Step: 2210, train/loss: 0.0
Step: 2210, train/grad_norm: 0.00027735508047044277
Step: 2210, train/learning_rate: 4.737030030810274e-05
Step: 2210, train/epoch: 0.5259400010108948
Step: 2220, train/loss: 9.999999747378752e-05
Step: 2220, train/grad_norm: 1.5550580428680405e-05
Step: 2220, train/learning_rate: 4.7358400479424745e-05
Step: 2220, train/epoch: 0.5283198356628418
Step: 2230, train/loss: 0.11819999665021896
Step: 2230, train/grad_norm: 3.242624370614067e-05
Step: 2230, train/learning_rate: 4.734650065074675e-05
Step: 2230, train/epoch: 0.5306996703147888
Step: 2240, train/loss: 0.0005000000237487257
Step: 2240, train/grad_norm: 0.0662321001291275
Step: 2240, train/learning_rate: 4.733460082206875e-05
Step: 2240, train/epoch: 0.5330795049667358
Step: 2250, train/loss: 0.0
Step: 2250, train/grad_norm: 0.017264030873775482
Step: 2250, train/learning_rate: 4.732270463136956e-05
Step: 2250, train/epoch: 0.5354592800140381
Step: 2260, train/loss: 0.0005000000237487257
Step: 2260, train/grad_norm: 0.0009616566239856184
Step: 2260, train/learning_rate: 4.7310804802691564e-05
Step: 2260, train/epoch: 0.5378391146659851
Step: 2270, train/loss: 0.00019999999494757503
Step: 2270, train/grad_norm: 0.0003117624728474766
Step: 2270, train/learning_rate: 4.729890497401357e-05
Step: 2270, train/epoch: 0.5402189493179321
Step: 2280, train/loss: 0.00559999980032444
Step: 2280, train/grad_norm: 0.0001689272903604433
Step: 2280, train/learning_rate: 4.728700514533557e-05
Step: 2280, train/epoch: 0.5425987839698792
Step: 2290, train/loss: 0.3133000135421753
Step: 2290, train/grad_norm: 0.003047543577849865
Step: 2290, train/learning_rate: 4.727510531665757e-05
Step: 2290, train/epoch: 0.5449785590171814
Step: 2300, train/loss: 0.2847000062465668
Step: 2300, train/grad_norm: 0.0019318652339279652
Step: 2300, train/learning_rate: 4.726320912595838e-05
Step: 2300, train/epoch: 0.5473583936691284
Step: 2310, train/loss: 0.0003000000142492354
Step: 2310, train/grad_norm: 0.004472697619348764
Step: 2310, train/learning_rate: 4.7251309297280386e-05
Step: 2310, train/epoch: 0.5497382283210754
Step: 2320, train/loss: 0.030799999833106995
Step: 2320, train/grad_norm: 28.97142219543457
Step: 2320, train/learning_rate: 4.723940946860239e-05
Step: 2320, train/epoch: 0.5521180629730225
Step: 2330, train/loss: 0.019700000062584877
Step: 2330, train/grad_norm: 0.00043766709859482944
Step: 2330, train/learning_rate: 4.722750963992439e-05
Step: 2330, train/epoch: 0.5544978380203247
Step: 2340, train/loss: 0.1388999968767166
Step: 2340, train/grad_norm: 0.1862184852361679
Step: 2340, train/learning_rate: 4.7215609811246395e-05
Step: 2340, train/epoch: 0.5568776726722717
Step: 2350, train/loss: 0.07249999791383743
Step: 2350, train/grad_norm: 39.3236198425293
Step: 2350, train/learning_rate: 4.7203713620547205e-05
Step: 2350, train/epoch: 0.5592575073242188
Step: 2360, train/loss: 0.02979999966919422
Step: 2360, train/grad_norm: 0.0009575156145729125
Step: 2360, train/learning_rate: 4.719181379186921e-05
Step: 2360, train/epoch: 0.5616373419761658
Step: 2370, train/loss: 0.00019999999494757503
Step: 2370, train/grad_norm: 0.000686301093082875
Step: 2370, train/learning_rate: 4.717991396319121e-05
Step: 2370, train/epoch: 0.564017117023468
Step: 2380, train/loss: 0.016200000420212746
Step: 2380, train/grad_norm: 0.0026691616512835026
Step: 2380, train/learning_rate: 4.7168014134513214e-05
Step: 2380, train/epoch: 0.566396951675415
Step: 2390, train/loss: 9.999999747378752e-05
Step: 2390, train/grad_norm: 0.04127946496009827
Step: 2390, train/learning_rate: 4.7156117943814024e-05
Step: 2390, train/epoch: 0.5687767863273621
Step: 2400, train/loss: 0.014700000174343586
Step: 2400, train/grad_norm: 0.000953165115788579
Step: 2400, train/learning_rate: 4.714421811513603e-05
Step: 2400, train/epoch: 0.5711566209793091
Step: 2410, train/loss: 0.00039999998989515007
Step: 2410, train/grad_norm: 0.6309418082237244
Step: 2410, train/learning_rate: 4.713231828645803e-05
Step: 2410, train/epoch: 0.5735363960266113
Step: 2420, train/loss: 0.00989999994635582
Step: 2420, train/grad_norm: 0.014362478628754616
Step: 2420, train/learning_rate: 4.712041845778003e-05
Step: 2420, train/epoch: 0.5759162306785583
Step: 2430, train/loss: 0.000699999975040555
Step: 2430, train/grad_norm: 7.659643597435206e-05
Step: 2430, train/learning_rate: 4.7108518629102036e-05
Step: 2430, train/epoch: 0.5782960653305054
Step: 2440, train/loss: 0.14810000360012054
Step: 2440, train/grad_norm: 36.26411437988281
Step: 2440, train/learning_rate: 4.7096622438402846e-05
Step: 2440, train/epoch: 0.5806758403778076
Step: 2450, train/loss: 0.0
Step: 2450, train/grad_norm: 0.0011877559591084719
Step: 2450, train/learning_rate: 4.708472260972485e-05
Step: 2450, train/epoch: 0.5830556750297546
Step: 2460, train/loss: 9.999999747378752e-05
Step: 2460, train/grad_norm: 0.0006933950935490429
Step: 2460, train/learning_rate: 4.707282278104685e-05
Step: 2460, train/epoch: 0.5854355096817017
Step: 2470, train/loss: 0.0010999999940395355
Step: 2470, train/grad_norm: 0.3498836159706116
Step: 2470, train/learning_rate: 4.7060922952368855e-05
Step: 2470, train/epoch: 0.5878153443336487
Step: 2480, train/loss: 0.0017000000225380063
Step: 2480, train/grad_norm: 0.00032834347803145647
Step: 2480, train/learning_rate: 4.704902312369086e-05
Step: 2480, train/epoch: 0.5901951193809509
Step: 2490, train/loss: 0.24789999425411224
Step: 2490, train/grad_norm: 0.0023052701726555824
Step: 2490, train/learning_rate: 4.703712693299167e-05
Step: 2490, train/epoch: 0.592574954032898
Step: 2500, train/loss: 9.999999747378752e-05
Step: 2500, train/grad_norm: 0.0012646226678043604
Step: 2500, train/learning_rate: 4.702522710431367e-05
Step: 2500, train/epoch: 0.594954788684845
Step: 2510, train/loss: 0.0
Step: 2510, train/grad_norm: 0.0018241438083350658
Step: 2510, train/learning_rate: 4.7013327275635675e-05
Step: 2510, train/epoch: 0.597334623336792
Step: 2520, train/loss: 0.0
Step: 2520, train/grad_norm: 0.0017022492829710245
Step: 2520, train/learning_rate: 4.700142744695768e-05
Step: 2520, train/epoch: 0.5997143983840942
Step: 2530, train/loss: 0.0
Step: 2530, train/grad_norm: 0.0006943829939700663
Step: 2530, train/learning_rate: 4.698952761827968e-05
Step: 2530, train/epoch: 0.6020942330360413
Step: 2540, train/loss: 0.0005000000237487257
Step: 2540, train/grad_norm: 0.000556660583242774
Step: 2540, train/learning_rate: 4.697763142758049e-05
Step: 2540, train/epoch: 0.6044740676879883
Step: 2550, train/loss: 0.0003000000142492354
Step: 2550, train/grad_norm: 4.3445434130262583e-05
Step: 2550, train/learning_rate: 4.6965731598902494e-05
Step: 2550, train/epoch: 0.6068539023399353
Step: 2560, train/loss: 0.13089999556541443
Step: 2560, train/grad_norm: 1.5428546667099
Step: 2560, train/learning_rate: 4.69538317702245e-05
Step: 2560, train/epoch: 0.6092336773872375
Step: 2570, train/loss: 0.13779999315738678
Step: 2570, train/grad_norm: 0.0035727994982153177
Step: 2570, train/learning_rate: 4.69419319415465e-05
Step: 2570, train/epoch: 0.6116135120391846
Step: 2580, train/loss: 0.28949999809265137
Step: 2580, train/grad_norm: 67.76110076904297
Step: 2580, train/learning_rate: 4.69300321128685e-05
Step: 2580, train/epoch: 0.6139933466911316
Step: 2590, train/loss: 0.0
Step: 2590, train/grad_norm: 0.001694410340860486
Step: 2590, train/learning_rate: 4.691813592216931e-05
Step: 2590, train/epoch: 0.6163731813430786
Step: 2600, train/loss: 0.04500000178813934
Step: 2600, train/grad_norm: 0.01604864001274109
Step: 2600, train/learning_rate: 4.6906236093491316e-05
Step: 2600, train/epoch: 0.6187529563903809
Step: 2610, train/loss: 0.10000000149011612
Step: 2610, train/grad_norm: 0.0007630145410075784
Step: 2610, train/learning_rate: 4.689433626481332e-05
Step: 2610, train/epoch: 0.6211327910423279
Step: 2620, train/loss: 0.0869000032544136
Step: 2620, train/grad_norm: 0.0008772665751166642
Step: 2620, train/learning_rate: 4.688243643613532e-05
Step: 2620, train/epoch: 0.6235126256942749
Step: 2630, train/loss: 0.11069999635219574
Step: 2630, train/grad_norm: 0.004243535455316305
Step: 2630, train/learning_rate: 4.6870536607457325e-05
Step: 2630, train/epoch: 0.6258924603462219
Step: 2640, train/loss: 0.16410000622272491
Step: 2640, train/grad_norm: 41.89394760131836
Step: 2640, train/learning_rate: 4.6858640416758135e-05
Step: 2640, train/epoch: 0.6282722353935242
Step: 2650, train/loss: 0.09260000288486481
Step: 2650, train/grad_norm: 0.2774639427661896
Step: 2650, train/learning_rate: 4.684674058808014e-05
Step: 2650, train/epoch: 0.6306520700454712
Step: 2660, train/loss: 0.00019999999494757503
Step: 2660, train/grad_norm: 0.014765260741114616
Step: 2660, train/learning_rate: 4.683484075940214e-05
Step: 2660, train/epoch: 0.6330319046974182
Step: 2670, train/loss: 0.0017000000225380063
Step: 2670, train/grad_norm: 0.655686616897583
Step: 2670, train/learning_rate: 4.6822940930724144e-05
Step: 2670, train/epoch: 0.6354116797447205
Step: 2680, train/loss: 0.013399999588727951
Step: 2680, train/grad_norm: 0.01847943477332592
Step: 2680, train/learning_rate: 4.681104110204615e-05
Step: 2680, train/epoch: 0.6377915143966675
Step: 2690, train/loss: 0.07460000365972519
Step: 2690, train/grad_norm: 0.008076263591647148
Step: 2690, train/learning_rate: 4.679914491134696e-05
Step: 2690, train/epoch: 0.6401713490486145
Step: 2700, train/loss: 0.0
Step: 2700, train/grad_norm: 0.002494464861229062
Step: 2700, train/learning_rate: 4.678724508266896e-05
Step: 2700, train/epoch: 0.6425511837005615
Step: 2710, train/loss: 0.00039999998989515007
Step: 2710, train/grad_norm: 0.007465738337486982
Step: 2710, train/learning_rate: 4.677534525399096e-05
Step: 2710, train/epoch: 0.6449309587478638
Step: 2720, train/loss: 0.0
Step: 2720, train/grad_norm: 0.000731525185983628
Step: 2720, train/learning_rate: 4.6763445425312966e-05
Step: 2720, train/epoch: 0.6473107933998108
Step: 2730, train/loss: 0.0
Step: 2730, train/grad_norm: 0.00029961791005916893
Step: 2730, train/learning_rate: 4.675154559663497e-05
Step: 2730, train/epoch: 0.6496906280517578
Step: 2740, train/loss: 9.999999747378752e-05
Step: 2740, train/grad_norm: 0.00240521808154881
Step: 2740, train/learning_rate: 4.673964940593578e-05
Step: 2740, train/epoch: 0.6520704627037048
Step: 2750, train/loss: 0.03750000149011612
Step: 2750, train/grad_norm: 0.0007281790021806955
Step: 2750, train/learning_rate: 4.672774957725778e-05
Step: 2750, train/epoch: 0.6544502377510071
Step: 2760, train/loss: 9.999999747378752e-05
Step: 2760, train/grad_norm: 0.019332531839609146
Step: 2760, train/learning_rate: 4.6715849748579785e-05
Step: 2760, train/epoch: 0.6568300724029541
Step: 2770, train/loss: 0.07000000029802322
Step: 2770, train/grad_norm: 0.0025985054671764374
Step: 2770, train/learning_rate: 4.670394991990179e-05
Step: 2770, train/epoch: 0.6592099070549011
Step: 2780, train/loss: 0.2215999960899353
Step: 2780, train/grad_norm: 0.003569128457456827
Step: 2780, train/learning_rate: 4.669205009122379e-05
Step: 2780, train/epoch: 0.6615897417068481
Step: 2790, train/loss: 0.0035000001080334187
Step: 2790, train/grad_norm: 0.009345254860818386
Step: 2790, train/learning_rate: 4.66801539005246e-05
Step: 2790, train/epoch: 0.6639695167541504
Step: 2800, train/loss: 0.0
Step: 2800, train/grad_norm: 0.008947032503783703
Step: 2800, train/learning_rate: 4.6668254071846604e-05
Step: 2800, train/epoch: 0.6663493514060974
Step: 2810, train/loss: 0.0003000000142492354
Step: 2810, train/grad_norm: 0.003030215622857213
Step: 2810, train/learning_rate: 4.665635424316861e-05
Step: 2810, train/epoch: 0.6687291860580444
Step: 2820, train/loss: 9.999999747378752e-05
Step: 2820, train/grad_norm: 0.005710328929126263
Step: 2820, train/learning_rate: 4.664445441449061e-05
Step: 2820, train/epoch: 0.6711090207099915
Step: 2830, train/loss: 0.0003000000142492354
Step: 2830, train/grad_norm: 0.00039956014370545745
Step: 2830, train/learning_rate: 4.663255458581261e-05
Step: 2830, train/epoch: 0.6734887957572937
Step: 2840, train/loss: 0.07959999889135361
Step: 2840, train/grad_norm: 0.0008623735629953444
Step: 2840, train/learning_rate: 4.6620658395113423e-05
Step: 2840, train/epoch: 0.6758686304092407
Step: 2850, train/loss: 0.028999999165534973
Step: 2850, train/grad_norm: 1.8507229469832964e-05
Step: 2850, train/learning_rate: 4.6608758566435426e-05
Step: 2850, train/epoch: 0.6782484650611877
Step: 2860, train/loss: 0.004999999888241291
Step: 2860, train/grad_norm: 7.482161521911621
Step: 2860, train/learning_rate: 4.659685873775743e-05
Step: 2860, train/epoch: 0.6806282997131348
Step: 2870, train/loss: 0.11050000041723251
Step: 2870, train/grad_norm: 0.000513603154104203
Step: 2870, train/learning_rate: 4.658495890907943e-05
Step: 2870, train/epoch: 0.683008074760437
Step: 2880, train/loss: 0.09109999984502792
Step: 2880, train/grad_norm: 0.00036982924211770296
Step: 2880, train/learning_rate: 4.6573059080401435e-05
Step: 2880, train/epoch: 0.685387909412384
Step: 2890, train/loss: 0.0
Step: 2890, train/grad_norm: 0.00016966911789495498
Step: 2890, train/learning_rate: 4.6561162889702246e-05
Step: 2890, train/epoch: 0.687767744064331
Step: 2900, train/loss: 0.28850001096725464
Step: 2900, train/grad_norm: 0.0005642318283207715
Step: 2900, train/learning_rate: 4.654926306102425e-05
Step: 2900, train/epoch: 0.6901475191116333
Step: 2910, train/loss: 0.0032999999821186066
Step: 2910, train/grad_norm: 0.00028876980650238693
Step: 2910, train/learning_rate: 4.653736323234625e-05
Step: 2910, train/epoch: 0.6925273537635803
Step: 2920, train/loss: 0.11819999665021896
Step: 2920, train/grad_norm: 0.001480606384575367
Step: 2920, train/learning_rate: 4.6525463403668255e-05
Step: 2920, train/epoch: 0.6949071884155273
Step: 2930, train/loss: 0.14630000293254852
Step: 2930, train/grad_norm: 84.96566772460938
Step: 2930, train/learning_rate: 4.651356357499026e-05
Step: 2930, train/epoch: 0.6972870230674744
Step: 2940, train/loss: 0.0
Step: 2940, train/grad_norm: 0.0012454045936465263
Step: 2940, train/learning_rate: 4.650166738429107e-05
Step: 2940, train/epoch: 0.6996667981147766
Step: 2950, train/loss: 0.16179999709129333
Step: 2950, train/grad_norm: 0.015512675046920776
Step: 2950, train/learning_rate: 4.648976755561307e-05
Step: 2950, train/epoch: 0.7020466327667236
Step: 2960, train/loss: 0.08950000256299973
Step: 2960, train/grad_norm: 2.980531098728534e-05
Step: 2960, train/learning_rate: 4.6477867726935074e-05
Step: 2960, train/epoch: 0.7044264674186707
Step: 2970, train/loss: 0.22169999778270721
Step: 2970, train/grad_norm: 0.0019445145735517144
Step: 2970, train/learning_rate: 4.646596789825708e-05
Step: 2970, train/epoch: 0.7068063020706177
Step: 2980, train/loss: 0.009600000455975533
Step: 2980, train/grad_norm: 0.08837819844484329
Step: 2980, train/learning_rate: 4.645406806957908e-05
Step: 2980, train/epoch: 0.7091860771179199
Step: 2990, train/loss: 0.007799999788403511
Step: 2990, train/grad_norm: 0.009377836249768734
Step: 2990, train/learning_rate: 4.644217187887989e-05
Step: 2990, train/epoch: 0.7115659117698669
Step: 3000, train/loss: 0.002099999925121665
Step: 3000, train/grad_norm: 0.00468176044523716
Step: 3000, train/learning_rate: 4.643027205020189e-05
Step: 3000, train/epoch: 0.713945746421814
Step: 3010, train/loss: 0.0
Step: 3010, train/grad_norm: 0.0005340761272236705
Step: 3010, train/learning_rate: 4.6418372221523896e-05
Step: 3010, train/epoch: 0.716325581073761
Step: 3020, train/loss: 9.999999747378752e-05
Step: 3020, train/grad_norm: 5.556810720008798e-05
Step: 3020, train/learning_rate: 4.64064723928459e-05
Step: 3020, train/epoch: 0.7187053561210632
Step: 3030, train/loss: 0.0
Step: 3030, train/grad_norm: 0.0014218050055205822
Step: 3030, train/learning_rate: 4.63945725641679e-05
Step: 3030, train/epoch: 0.7210851907730103
Step: 3040, train/loss: 0.07580000162124634
Step: 3040, train/grad_norm: 5.278717799228616e-05
Step: 3040, train/learning_rate: 4.638267637346871e-05
Step: 3040, train/epoch: 0.7234650254249573
Step: 3050, train/loss: 0.0003000000142492354
Step: 3050, train/grad_norm: 0.0016290948260575533
Step: 3050, train/learning_rate: 4.6370776544790715e-05
Step: 3050, train/epoch: 0.7258448600769043
Step: 3060, train/loss: 0.00019999999494757503
Step: 3060, train/grad_norm: 8.435376366833225e-05
Step: 3060, train/learning_rate: 4.635887671611272e-05
Step: 3060, train/epoch: 0.7282246351242065
Step: 3070, train/loss: 0.0
Step: 3070, train/grad_norm: 0.004478749819099903
Step: 3070, train/learning_rate: 4.634697688743472e-05
Step: 3070, train/epoch: 0.7306044697761536
Step: 3080, train/loss: 0.0006000000284984708
Step: 3080, train/grad_norm: 1.119761347770691
Step: 3080, train/learning_rate: 4.6335077058756724e-05
Step: 3080, train/epoch: 0.7329843044281006
Step: 3090, train/loss: 0.1039000004529953
Step: 3090, train/grad_norm: 0.00014871997700538486
Step: 3090, train/learning_rate: 4.6323180868057534e-05
Step: 3090, train/epoch: 0.7353641390800476
Step: 3100, train/loss: 0.0
Step: 3100, train/grad_norm: 8.115297532640398e-06
Step: 3100, train/learning_rate: 4.631128103937954e-05
Step: 3100, train/epoch: 0.7377439141273499
Step: 3110, train/loss: 0.005900000222027302
Step: 3110, train/grad_norm: 0.0004108751018065959
Step: 3110, train/learning_rate: 4.629938121070154e-05
Step: 3110, train/epoch: 0.7401237487792969
Step: 3120, train/loss: 0.0
Step: 3120, train/grad_norm: 0.0005331826978363097
Step: 3120, train/learning_rate: 4.628748138202354e-05
Step: 3120, train/epoch: 0.7425035834312439
Step: 3130, train/loss: 0.1551000028848648
Step: 3130, train/grad_norm: 0.18960003554821014
Step: 3130, train/learning_rate: 4.6275581553345546e-05
Step: 3130, train/epoch: 0.7448834180831909
Step: 3140, train/loss: 0.00039999998989515007
Step: 3140, train/grad_norm: 0.00018307844584342092
Step: 3140, train/learning_rate: 4.6263685362646356e-05
Step: 3140, train/epoch: 0.7472631931304932
Step: 3150, train/loss: 0.0
Step: 3150, train/grad_norm: 4.3062016629846767e-05
Step: 3150, train/learning_rate: 4.625178553396836e-05
Step: 3150, train/epoch: 0.7496430277824402
Step: 3160, train/loss: 0.19040000438690186
Step: 3160, train/grad_norm: 0.00034797945409081876
Step: 3160, train/learning_rate: 4.623988570529036e-05
Step: 3160, train/epoch: 0.7520228624343872
Step: 3170, train/loss: 0.004399999976158142
Step: 3170, train/grad_norm: 11.275696754455566
Step: 3170, train/learning_rate: 4.6227985876612365e-05
Step: 3170, train/epoch: 0.7544026374816895
Step: 3180, train/loss: 0.028200000524520874
Step: 3180, train/grad_norm: 1.0546246812737081e-05
Step: 3180, train/learning_rate: 4.621608604793437e-05
Step: 3180, train/epoch: 0.7567824721336365
Step: 3190, train/loss: 0.042399998754262924
Step: 3190, train/grad_norm: 0.11647593975067139
Step: 3190, train/learning_rate: 4.620418985723518e-05
Step: 3190, train/epoch: 0.7591623067855835
Step: 3200, train/loss: 0.0
Step: 3200, train/grad_norm: 3.0511117074638605e-05
Step: 3200, train/learning_rate: 4.619229002855718e-05
Step: 3200, train/epoch: 0.7615421414375305
Step: 3210, train/loss: 0.09300000220537186
Step: 3210, train/grad_norm: 2.9631914003402926e-05
Step: 3210, train/learning_rate: 4.6180390199879184e-05
Step: 3210, train/epoch: 0.7639219164848328
Step: 3220, train/loss: 0.00019999999494757503
Step: 3220, train/grad_norm: 0.0017651703674346209
Step: 3220, train/learning_rate: 4.616849037120119e-05
Step: 3220, train/epoch: 0.7663017511367798
Step: 3230, train/loss: 0.006899999920278788
Step: 3230, train/grad_norm: 3.960660251323134e-05
Step: 3230, train/learning_rate: 4.615659054252319e-05
Step: 3230, train/epoch: 0.7686815857887268
Step: 3240, train/loss: 0.0
Step: 3240, train/grad_norm: 1.3475490050041117e-05
Step: 3240, train/learning_rate: 4.6144694351824e-05
Step: 3240, train/epoch: 0.7710614204406738
Step: 3250, train/loss: 0.0
Step: 3250, train/grad_norm: 9.389440674567595e-05
Step: 3250, train/learning_rate: 4.6132794523146003e-05
Step: 3250, train/epoch: 0.7734411954879761
Step: 3260, train/loss: 0.0
Step: 3260, train/grad_norm: 0.03412250801920891
Step: 3260, train/learning_rate: 4.6120894694468006e-05
Step: 3260, train/epoch: 0.7758210301399231
Step: 3270, train/loss: 0.41119998693466187
Step: 3270, train/grad_norm: 0.0003562481433618814
Step: 3270, train/learning_rate: 4.610899486579001e-05
Step: 3270, train/epoch: 0.7782008647918701
Step: 3280, train/loss: 0.012299999594688416
Step: 3280, train/grad_norm: 3.4680912494659424
Step: 3280, train/learning_rate: 4.609709503711201e-05
Step: 3280, train/epoch: 0.7805806994438171
Step: 3290, train/loss: 0.027400000020861626
Step: 3290, train/grad_norm: 0.0417281836271286
Step: 3290, train/learning_rate: 4.608519884641282e-05
Step: 3290, train/epoch: 0.7829604744911194
Step: 3300, train/loss: 0.27250000834465027
Step: 3300, train/grad_norm: 0.0007137450738810003
Step: 3300, train/learning_rate: 4.6073299017734826e-05
Step: 3300, train/epoch: 0.7853403091430664
Step: 3310, train/loss: 0.0003000000142492354
Step: 3310, train/grad_norm: 0.023126931861042976
Step: 3310, train/learning_rate: 4.606139918905683e-05
Step: 3310, train/epoch: 0.7877201437950134
Step: 3320, train/loss: 0.022199999541044235
Step: 3320, train/grad_norm: 0.005710931029170752
Step: 3320, train/learning_rate: 4.604949936037883e-05
Step: 3320, train/epoch: 0.7900999784469604
Step: 3330, train/loss: 0.19439999759197235
Step: 3330, train/grad_norm: 3.75300669670105
Step: 3330, train/learning_rate: 4.6037599531700835e-05
Step: 3330, train/epoch: 0.7924797534942627
Step: 3340, train/loss: 0.00019999999494757503
Step: 3340, train/grad_norm: 0.6206409335136414
Step: 3340, train/learning_rate: 4.6025703341001645e-05
Step: 3340, train/epoch: 0.7948595881462097
Step: 3350, train/loss: 0.0019000000320374966
Step: 3350, train/grad_norm: 0.8784012198448181
Step: 3350, train/learning_rate: 4.601380351232365e-05
Step: 3350, train/epoch: 0.7972394227981567
Step: 3360, train/loss: 0.00019999999494757503
Step: 3360, train/grad_norm: 9.757566294865683e-05
Step: 3360, train/learning_rate: 4.600190368364565e-05
Step: 3360, train/epoch: 0.7996192574501038
Step: 3370, train/loss: 0.04690000042319298
Step: 3370, train/grad_norm: 0.003918647766113281
Step: 3370, train/learning_rate: 4.5990003854967654e-05
Step: 3370, train/epoch: 0.801999032497406
Step: 3380, train/loss: 0.0
Step: 3380, train/grad_norm: 0.0007707193144597113
Step: 3380, train/learning_rate: 4.597810402628966e-05
Step: 3380, train/epoch: 0.804378867149353
Step: 3390, train/loss: 0.0
Step: 3390, train/grad_norm: 0.0030124143231660128
Step: 3390, train/learning_rate: 4.596620783559047e-05
Step: 3390, train/epoch: 0.8067587018013
Step: 3400, train/loss: 0.16899999976158142
Step: 3400, train/grad_norm: 0.002002586144953966
Step: 3400, train/learning_rate: 4.595430800691247e-05
Step: 3400, train/epoch: 0.8091384768486023
Step: 3410, train/loss: 0.12890000641345978
Step: 3410, train/grad_norm: 0.0041662235744297504
Step: 3410, train/learning_rate: 4.594240817823447e-05
Step: 3410, train/epoch: 0.8115183115005493
Step: 3420, train/loss: 0.00039999998989515007
Step: 3420, train/grad_norm: 0.12720279395580292
Step: 3420, train/learning_rate: 4.5930508349556476e-05
Step: 3420, train/epoch: 0.8138981461524963
Step: 3430, train/loss: 0.042399998754262924
Step: 3430, train/grad_norm: 23.2457275390625
Step: 3430, train/learning_rate: 4.591860852087848e-05
Step: 3430, train/epoch: 0.8162779808044434
Step: 3440, train/loss: 0.1656000018119812
Step: 3440, train/grad_norm: 0.0017917590448632836
Step: 3440, train/learning_rate: 4.590671233017929e-05
Step: 3440, train/epoch: 0.8186577558517456
Step: 3450, train/loss: 0.03779999911785126
Step: 3450, train/grad_norm: 88.93167114257812
Step: 3450, train/learning_rate: 4.589481250150129e-05
Step: 3450, train/epoch: 0.8210375905036926
Step: 3460, train/loss: 0.00279999990016222
Step: 3460, train/grad_norm: 0.03265632316470146
Step: 3460, train/learning_rate: 4.5882912672823295e-05
Step: 3460, train/epoch: 0.8234174251556396
Step: 3470, train/loss: 0.2085999995470047
Step: 3470, train/grad_norm: 0.0007832931005395949
Step: 3470, train/learning_rate: 4.58710128441453e-05
Step: 3470, train/epoch: 0.8257972598075867
Step: 3480, train/loss: 0.022199999541044235
Step: 3480, train/grad_norm: 0.0006880068103782833
Step: 3480, train/learning_rate: 4.58591130154673e-05
Step: 3480, train/epoch: 0.8281770348548889
Step: 3490, train/loss: 0.0
Step: 3490, train/grad_norm: 0.1095939576625824
Step: 3490, train/learning_rate: 4.584721682476811e-05
Step: 3490, train/epoch: 0.8305568695068359
Step: 3500, train/loss: 0.0
Step: 3500, train/grad_norm: 0.0005021776887588203
Step: 3500, train/learning_rate: 4.5835316996090114e-05
Step: 3500, train/epoch: 0.832936704158783
Step: 3510, train/loss: 0.0
Step: 3510, train/grad_norm: 0.0006506064091809094
Step: 3510, train/learning_rate: 4.582341716741212e-05
Step: 3510, train/epoch: 0.83531653881073
Step: 3520, train/loss: 0.017400000244379044
Step: 3520, train/grad_norm: 0.0020140300039201975
Step: 3520, train/learning_rate: 4.581151733873412e-05
Step: 3520, train/epoch: 0.8376963138580322
Step: 3530, train/loss: 0.00039999998989515007
Step: 3530, train/grad_norm: 1.2311498721828684e-05
Step: 3530, train/learning_rate: 4.579961751005612e-05
Step: 3530, train/epoch: 0.8400761485099792
Step: 3540, train/loss: 9.999999747378752e-05
Step: 3540, train/grad_norm: 0.013100098818540573
Step: 3540, train/learning_rate: 4.578772131935693e-05
Step: 3540, train/epoch: 0.8424559831619263
Step: 3550, train/loss: 0.11959999799728394
Step: 3550, train/grad_norm: 0.04206646978855133
Step: 3550, train/learning_rate: 4.5775821490678936e-05
Step: 3550, train/epoch: 0.8448358178138733
Step: 3560, train/loss: 0.08630000054836273
Step: 3560, train/grad_norm: 52.58268356323242
Step: 3560, train/learning_rate: 4.576392166200094e-05
Step: 3560, train/epoch: 0.8472155928611755
Step: 3570, train/loss: 0.2596000134944916
Step: 3570, train/grad_norm: 0.046053770929574966
Step: 3570, train/learning_rate: 4.575202183332294e-05
Step: 3570, train/epoch: 0.8495954275131226
Step: 3580, train/loss: 0.0005000000237487257
Step: 3580, train/grad_norm: 0.08636303246021271
Step: 3580, train/learning_rate: 4.5740122004644945e-05
Step: 3580, train/epoch: 0.8519752621650696
Step: 3590, train/loss: 0.00039999998989515007
Step: 3590, train/grad_norm: 0.00036581349559128284
Step: 3590, train/learning_rate: 4.5728225813945755e-05
Step: 3590, train/epoch: 0.8543550968170166
Step: 3600, train/loss: 0.00039999998989515007
Step: 3600, train/grad_norm: 0.0064240386709570885
Step: 3600, train/learning_rate: 4.571632598526776e-05
Step: 3600, train/epoch: 0.8567348718643188
Step: 3610, train/loss: 0.041600000113248825
Step: 3610, train/grad_norm: 0.0005687985103577375
Step: 3610, train/learning_rate: 4.570442615658976e-05
Step: 3610, train/epoch: 0.8591147065162659
Step: 3620, train/loss: 9.999999747378752e-05
Step: 3620, train/grad_norm: 0.00017270843090955168
Step: 3620, train/learning_rate: 4.5692526327911764e-05
Step: 3620, train/epoch: 0.8614945411682129
Step: 3630, train/loss: 0.18320000171661377
Step: 3630, train/grad_norm: 0.02368193492293358
Step: 3630, train/learning_rate: 4.568062649923377e-05
Step: 3630, train/epoch: 0.8638743162155151
Step: 3640, train/loss: 0.0008999999845400453
Step: 3640, train/grad_norm: 0.002177588641643524
Step: 3640, train/learning_rate: 4.566873030853458e-05
Step: 3640, train/epoch: 0.8662541508674622
Step: 3650, train/loss: 0.00019999999494757503
Step: 3650, train/grad_norm: 0.02466755174100399
Step: 3650, train/learning_rate: 4.565683047985658e-05
Step: 3650, train/epoch: 0.8686339855194092
Step: 3660, train/loss: 0.00019999999494757503
Step: 3660, train/grad_norm: 0.0005505611188709736
Step: 3660, train/learning_rate: 4.5644930651178584e-05
Step: 3660, train/epoch: 0.8710138201713562
Step: 3670, train/loss: 0.14470000565052032
Step: 3670, train/grad_norm: 55.4046630859375
Step: 3670, train/learning_rate: 4.5633030822500587e-05
Step: 3670, train/epoch: 0.8733935952186584
Step: 3680, train/loss: 0.013299999758601189
Step: 3680, train/grad_norm: 28.987403869628906
Step: 3680, train/learning_rate: 4.562113099382259e-05
Step: 3680, train/epoch: 0.8757734298706055
Step: 3690, train/loss: 0.0003000000142492354
Step: 3690, train/grad_norm: 0.0018837795360013843
Step: 3690, train/learning_rate: 4.56092348031234e-05
Step: 3690, train/epoch: 0.8781532645225525
Step: 3700, train/loss: 0.09399999678134918
Step: 3700, train/grad_norm: 0.0006836060201749206
Step: 3700, train/learning_rate: 4.55973349744454e-05
Step: 3700, train/epoch: 0.8805330991744995
Step: 3710, train/loss: 0.026599999517202377
Step: 3710, train/grad_norm: 0.000590432551689446
Step: 3710, train/learning_rate: 4.5585435145767406e-05
Step: 3710, train/epoch: 0.8829128742218018
Step: 3720, train/loss: 0.23330000042915344
Step: 3720, train/grad_norm: 0.7758964896202087
Step: 3720, train/learning_rate: 4.557353531708941e-05
Step: 3720, train/epoch: 0.8852927088737488
Step: 3730, train/loss: 0.00139999995008111
Step: 3730, train/grad_norm: 2.1323012333596125e-05
Step: 3730, train/learning_rate: 4.556163912639022e-05
Step: 3730, train/epoch: 0.8876725435256958
Step: 3740, train/loss: 0.020400000736117363
Step: 3740, train/grad_norm: 0.00042128757922910154
Step: 3740, train/learning_rate: 4.554973929771222e-05
Step: 3740, train/epoch: 0.8900523781776428
Step: 3750, train/loss: 0.049800001084804535
Step: 3750, train/grad_norm: 0.0005411540623754263
Step: 3750, train/learning_rate: 4.5537839469034225e-05
Step: 3750, train/epoch: 0.8924321532249451
Step: 3760, train/loss: 0.0013000000035390258
Step: 3760, train/grad_norm: 0.00011864090629387647
Step: 3760, train/learning_rate: 4.552593964035623e-05
Step: 3760, train/epoch: 0.8948119878768921
Step: 3770, train/loss: 0.00019999999494757503
Step: 3770, train/grad_norm: 3.335428118589334e-06
Step: 3770, train/learning_rate: 4.551403981167823e-05
Step: 3770, train/epoch: 0.8971918225288391
Step: 3780, train/loss: 0.006500000134110451
Step: 3780, train/grad_norm: 0.00343657867051661
Step: 3780, train/learning_rate: 4.550214362097904e-05
Step: 3780, train/epoch: 0.8995716571807861
Step: 3790, train/loss: 0.0
Step: 3790, train/grad_norm: 0.00037595906178466976
Step: 3790, train/learning_rate: 4.5490243792301044e-05
Step: 3790, train/epoch: 0.9019514322280884
Step: 3800, train/loss: 9.999999747378752e-05
Step: 3800, train/grad_norm: 0.001801152597181499
Step: 3800, train/learning_rate: 4.547834396362305e-05
Step: 3800, train/epoch: 0.9043312668800354
Step: 3810, train/loss: 0.0
Step: 3810, train/grad_norm: 6.697772187180817e-05
Step: 3810, train/learning_rate: 4.546644413494505e-05
Step: 3810, train/epoch: 0.9067111015319824
Step: 3820, train/loss: 0.1257999986410141
Step: 3820, train/grad_norm: 3.3239182812394574e-05
Step: 3820, train/learning_rate: 4.545454430626705e-05
Step: 3820, train/epoch: 0.9090909361839294
Step: 3830, train/loss: 9.999999747378752e-05
Step: 3830, train/grad_norm: 0.010496825911104679
Step: 3830, train/learning_rate: 4.544264811556786e-05
Step: 3830, train/epoch: 0.9114707112312317
Step: 3840, train/loss: 0.0
Step: 3840, train/grad_norm: 0.0001623507123440504
Step: 3840, train/learning_rate: 4.5430748286889866e-05
Step: 3840, train/epoch: 0.9138505458831787
Step: 3850, train/loss: 0.06960000097751617
Step: 3850, train/grad_norm: 0.004099830985069275
Step: 3850, train/learning_rate: 4.541884845821187e-05
Step: 3850, train/epoch: 0.9162303805351257
Step: 3860, train/loss: 0.017999999225139618
Step: 3860, train/grad_norm: 0.0194459967315197
Step: 3860, train/learning_rate: 4.540694862953387e-05
Step: 3860, train/epoch: 0.9186102151870728
Step: 3870, train/loss: 0.16429999470710754
Step: 3870, train/grad_norm: 0.0029934931080788374
Step: 3870, train/learning_rate: 4.5395048800855875e-05
Step: 3870, train/epoch: 0.920989990234375
Step: 3880, train/loss: 0.16019999980926514
Step: 3880, train/grad_norm: 0.0016793052200227976
Step: 3880, train/learning_rate: 4.5383152610156685e-05
Step: 3880, train/epoch: 0.923369824886322
Step: 3890, train/loss: 0.04830000177025795
Step: 3890, train/grad_norm: 1.911756992340088
Step: 3890, train/learning_rate: 4.537125278147869e-05
Step: 3890, train/epoch: 0.925749659538269
Step: 3900, train/loss: 0.00019999999494757503
Step: 3900, train/grad_norm: 0.002993748988956213
Step: 3900, train/learning_rate: 4.535935295280069e-05
Step: 3900, train/epoch: 0.9281294345855713
Step: 3910, train/loss: 9.999999747378752e-05
Step: 3910, train/grad_norm: 0.006089353933930397
Step: 3910, train/learning_rate: 4.5347453124122694e-05
Step: 3910, train/epoch: 0.9305092692375183
Step: 3920, train/loss: 0.0010000000474974513
Step: 3920, train/grad_norm: 0.05408589914441109
Step: 3920, train/learning_rate: 4.53355532954447e-05
Step: 3920, train/epoch: 0.9328891038894653
Step: 3930, train/loss: 0.0005000000237487257
Step: 3930, train/grad_norm: 0.05301038548350334
Step: 3930, train/learning_rate: 4.532365710474551e-05
Step: 3930, train/epoch: 0.9352689385414124
Step: 3940, train/loss: 0.0007999999797903001
Step: 3940, train/grad_norm: 0.00023029469593893737
Step: 3940, train/learning_rate: 4.531175727606751e-05
Step: 3940, train/epoch: 0.9376487135887146
Step: 3950, train/loss: 0.13289999961853027
Step: 3950, train/grad_norm: 0.0037396345287561417
Step: 3950, train/learning_rate: 4.529985744738951e-05
Step: 3950, train/epoch: 0.9400285482406616
Step: 3960, train/loss: 0.00039999998989515007
Step: 3960, train/grad_norm: 0.00010140729136765003
Step: 3960, train/learning_rate: 4.5287957618711516e-05
Step: 3960, train/epoch: 0.9424083828926086
Step: 3970, train/loss: 0.08479999750852585
Step: 3970, train/grad_norm: 0.0009501214371994138
Step: 3970, train/learning_rate: 4.527605779003352e-05
Step: 3970, train/epoch: 0.9447882175445557
Step: 3980, train/loss: 0.016899999231100082
Step: 3980, train/grad_norm: 0.00010410697723273188
Step: 3980, train/learning_rate: 4.526416159933433e-05
Step: 3980, train/epoch: 0.9471679925918579
Step: 3990, train/loss: 0.004100000020116568
Step: 3990, train/grad_norm: 0.003499733516946435
Step: 3990, train/learning_rate: 4.525226177065633e-05
Step: 3990, train/epoch: 0.9495478272438049
Step: 4000, train/loss: 9.999999747378752e-05
Step: 4000, train/grad_norm: 9.416198008693755e-05
Step: 4000, train/learning_rate: 4.5240361941978335e-05
Step: 4000, train/epoch: 0.951927661895752
Step: 4010, train/loss: 0.0
Step: 4010, train/grad_norm: 7.064911187626421e-05
Step: 4010, train/learning_rate: 4.522846211330034e-05
Step: 4010, train/epoch: 0.954307496547699
Step: 4020, train/loss: 0.3021000027656555
Step: 4020, train/grad_norm: 0.0002794907195493579
Step: 4020, train/learning_rate: 4.521656228462234e-05
Step: 4020, train/epoch: 0.9566872715950012
Step: 4030, train/loss: 0.0007999999797903001
Step: 4030, train/grad_norm: 0.6014711856842041
Step: 4030, train/learning_rate: 4.520466609392315e-05
Step: 4030, train/epoch: 0.9590671062469482
Step: 4040, train/loss: 0.0012000000569969416
Step: 4040, train/grad_norm: 0.2651989161968231
Step: 4040, train/learning_rate: 4.5192766265245155e-05
Step: 4040, train/epoch: 0.9614469408988953
Step: 4050, train/loss: 0.006200000178068876
Step: 4050, train/grad_norm: 0.00015069442451931536
Step: 4050, train/learning_rate: 4.518086643656716e-05
Step: 4050, train/epoch: 0.9638267755508423
Step: 4060, train/loss: 0.02800000086426735
Step: 4060, train/grad_norm: 0.004562533926218748
Step: 4060, train/learning_rate: 4.516896660788916e-05
Step: 4060, train/epoch: 0.9662065505981445
Step: 4070, train/loss: 0.0010000000474974513
Step: 4070, train/grad_norm: 2.357204539293889e-05
Step: 4070, train/learning_rate: 4.5157066779211164e-05
Step: 4070, train/epoch: 0.9685863852500916
Step: 4080, train/loss: 0.05920000001788139
Step: 4080, train/grad_norm: 4.264938979758881e-05
Step: 4080, train/learning_rate: 4.5145170588511974e-05
Step: 4080, train/epoch: 0.9709662199020386
Step: 4090, train/loss: 0.09610000252723694
Step: 4090, train/grad_norm: 5.616268754238263e-05
Step: 4090, train/learning_rate: 4.513327075983398e-05
Step: 4090, train/epoch: 0.9733460545539856
Step: 4100, train/loss: 0.013100000098347664
Step: 4100, train/grad_norm: 0.014025826007127762
Step: 4100, train/learning_rate: 4.512137093115598e-05
Step: 4100, train/epoch: 0.9757258296012878
Step: 4110, train/loss: 9.999999747378752e-05
Step: 4110, train/grad_norm: 5.818167483084835e-05
Step: 4110, train/learning_rate: 4.510947110247798e-05
Step: 4110, train/epoch: 0.9781056642532349
Step: 4120, train/loss: 9.999999747378752e-05
Step: 4120, train/grad_norm: 9.70804103417322e-06
Step: 4120, train/learning_rate: 4.5097571273799986e-05
Step: 4120, train/epoch: 0.9804854989051819
Step: 4130, train/loss: 0.027799999341368675
Step: 4130, train/grad_norm: 0.0016159327933564782
Step: 4130, train/learning_rate: 4.5085675083100796e-05
Step: 4130, train/epoch: 0.9828652739524841
Step: 4140, train/loss: 0.0003000000142492354
Step: 4140, train/grad_norm: 1.656297899899073e-05
Step: 4140, train/learning_rate: 4.50737752544228e-05
Step: 4140, train/epoch: 0.9852451086044312
Step: 4150, train/loss: 0.10509999841451645
Step: 4150, train/grad_norm: 0.05470697581768036
Step: 4150, train/learning_rate: 4.50618754257448e-05
Step: 4150, train/epoch: 0.9876249432563782
Step: 4160, train/loss: 0.0
Step: 4160, train/grad_norm: 0.004687114618718624
Step: 4160, train/learning_rate: 4.5049975597066805e-05
Step: 4160, train/epoch: 0.9900047779083252
Step: 4170, train/loss: 9.999999747378752e-05
Step: 4170, train/grad_norm: 3.801305297201907e-07
Step: 4170, train/learning_rate: 4.503807576838881e-05
Step: 4170, train/epoch: 0.9923845529556274
Step: 4180, train/loss: 0.27970001101493835
Step: 4180, train/grad_norm: 9.907615094562061e-06
Step: 4180, train/learning_rate: 4.502617957768962e-05
Step: 4180, train/epoch: 0.9947643876075745
Step: 4190, train/loss: 0.24789999425411224
Step: 4190, train/grad_norm: 16.650554656982422
Step: 4190, train/learning_rate: 4.501427974901162e-05
Step: 4190, train/epoch: 0.9971442222595215
Step: 4200, train/loss: 9.999999747378752e-05
Step: 4200, train/grad_norm: 0.005932045169174671
Step: 4200, train/learning_rate: 4.5002379920333624e-05
Step: 4200, train/epoch: 0.9995240569114685
Step: 4202, eval/loss: 0.03778782859444618
Step: 4202, eval/accuracy: 0.9927808046340942
Step: 4202, eval/f1: 0.9923743009567261
Step: 4202, eval/runtime: 708.1102294921875
Step: 4202, eval/samples_per_second: 10.17199993133545
Step: 4202, eval/steps_per_second: 1.2719999551773071
Step: 4202, train/epoch: 1.0
Step: 4210, train/loss: 0.00039999998989515007
Step: 4210, train/grad_norm: 0.001552984700538218
Step: 4210, train/learning_rate: 4.499048009165563e-05
Step: 4210, train/epoch: 1.0019038915634155
Step: 4220, train/loss: 0.004699999932199717
Step: 4220, train/grad_norm: 0.0020283290650695562
Step: 4220, train/learning_rate: 4.497858026297763e-05
Step: 4220, train/epoch: 1.0042836666107178
Step: 4230, train/loss: 0.007300000172108412
Step: 4230, train/grad_norm: 0.001483957632444799
Step: 4230, train/learning_rate: 4.496668407227844e-05
Step: 4230, train/epoch: 1.00666344165802
Step: 4240, train/loss: 0.14800000190734863
Step: 4240, train/grad_norm: 1.043738603591919
Step: 4240, train/learning_rate: 4.495478424360044e-05
Step: 4240, train/epoch: 1.0090433359146118
Step: 4250, train/loss: 0.00039999998989515007
Step: 4250, train/grad_norm: 0.00019926585082430393
Step: 4250, train/learning_rate: 4.4942884414922446e-05
Step: 4250, train/epoch: 1.011423110961914
Step: 4260, train/loss: 0.052000001072883606
Step: 4260, train/grad_norm: 67.7226791381836
Step: 4260, train/learning_rate: 4.493098458624445e-05
Step: 4260, train/epoch: 1.0138030052185059
Step: 4270, train/loss: 0.10859999805688858
Step: 4270, train/grad_norm: 0.020872607827186584
Step: 4270, train/learning_rate: 4.491908475756645e-05
Step: 4270, train/epoch: 1.016182780265808
Step: 4280, train/loss: 0.0006000000284984708
Step: 4280, train/grad_norm: 0.0010518550407141447
Step: 4280, train/learning_rate: 4.490718856686726e-05
Step: 4280, train/epoch: 1.0185625553131104
Step: 4290, train/loss: 0.00019999999494757503
Step: 4290, train/grad_norm: 0.001586591126397252
Step: 4290, train/learning_rate: 4.4895288738189265e-05
Step: 4290, train/epoch: 1.0209424495697021
Step: 4300, train/loss: 0.001500000013038516
Step: 4300, train/grad_norm: 0.5375654101371765
Step: 4300, train/learning_rate: 4.488338890951127e-05
Step: 4300, train/epoch: 1.0233222246170044
Step: 4310, train/loss: 0.00019999999494757503
Step: 4310, train/grad_norm: 1.63328368216753e-05
Step: 4310, train/learning_rate: 4.487148908083327e-05
Step: 4310, train/epoch: 1.0257019996643066
Step: 4320, train/loss: 0.0
Step: 4320, train/grad_norm: 5.274985596770421e-06
Step: 4320, train/learning_rate: 4.4859589252155274e-05
Step: 4320, train/epoch: 1.0280818939208984
Step: 4330, train/loss: 0.008700000122189522
Step: 4330, train/grad_norm: 7.417532469844446e-05
Step: 4330, train/learning_rate: 4.4847693061456084e-05
Step: 4330, train/epoch: 1.0304616689682007
Step: 4340, train/loss: 0.16349999606609344
Step: 4340, train/grad_norm: 0.40978649258613586
Step: 4340, train/learning_rate: 4.483579323277809e-05
Step: 4340, train/epoch: 1.0328415632247925
Step: 4350, train/loss: 0.0
Step: 4350, train/grad_norm: 8.704381616553292e-05
Step: 4350, train/learning_rate: 4.482389340410009e-05
Step: 4350, train/epoch: 1.0352213382720947
Step: 4360, train/loss: 0.0
Step: 4360, train/grad_norm: 3.7494892239919864e-06
Step: 4360, train/learning_rate: 4.481199357542209e-05
Step: 4360, train/epoch: 1.037601113319397
Step: 4370, train/loss: 0.0
Step: 4370, train/grad_norm: 2.6743902708403766e-05
Step: 4370, train/learning_rate: 4.4800093746744096e-05
Step: 4370, train/epoch: 1.0399810075759888
Step: 4380, train/loss: 9.999999747378752e-05
Step: 4380, train/grad_norm: 7.558021752629429e-05
Step: 4380, train/learning_rate: 4.4788197556044906e-05
Step: 4380, train/epoch: 1.042360782623291
Step: 4390, train/loss: 0.09380000084638596
Step: 4390, train/grad_norm: 0.0019197305664420128
Step: 4390, train/learning_rate: 4.477629772736691e-05
Step: 4390, train/epoch: 1.0447405576705933
Step: 4400, train/loss: 9.999999747378752e-05
Step: 4400, train/grad_norm: 0.06348323822021484
Step: 4400, train/learning_rate: 4.476439789868891e-05
Step: 4400, train/epoch: 1.047120451927185
Step: 4410, train/loss: 0.014600000344216824
Step: 4410, train/grad_norm: 39.8078727722168
Step: 4410, train/learning_rate: 4.4752498070010915e-05
Step: 4410, train/epoch: 1.0495002269744873
Step: 4420, train/loss: 0.0003000000142492354
Step: 4420, train/grad_norm: 0.0012859818525612354
Step: 4420, train/learning_rate: 4.474059824133292e-05
Step: 4420, train/epoch: 1.0518800020217896
Step: 4430, train/loss: 0.0003000000142492354
Step: 4430, train/grad_norm: 1.597496884642169e-05
Step: 4430, train/learning_rate: 4.472870205063373e-05
Step: 4430, train/epoch: 1.0542598962783813
Step: 4440, train/loss: 0.0
Step: 4440, train/grad_norm: 0.0004444563528522849
Step: 4440, train/learning_rate: 4.471680222195573e-05
Step: 4440, train/epoch: 1.0566396713256836
Step: 4450, train/loss: 9.999999747378752e-05
Step: 4450, train/grad_norm: 5.040278665546793e-07
Step: 4450, train/learning_rate: 4.4704902393277735e-05
Step: 4450, train/epoch: 1.0590195655822754
Step: 4460, train/loss: 0.0
Step: 4460, train/grad_norm: 0.009837071411311626
Step: 4460, train/learning_rate: 4.469300256459974e-05
Step: 4460, train/epoch: 1.0613993406295776
Step: 4470, train/loss: 0.0551999993622303
Step: 4470, train/grad_norm: 0.0014108774485066533
Step: 4470, train/learning_rate: 4.468110273592174e-05
Step: 4470, train/epoch: 1.0637791156768799
Step: 4480, train/loss: 0.04149999842047691
Step: 4480, train/grad_norm: 8.529345905117225e-06
Step: 4480, train/learning_rate: 4.466920654522255e-05
Step: 4480, train/epoch: 1.0661590099334717
Step: 4490, train/loss: 0.07069999724626541
Step: 4490, train/grad_norm: 0.0002120355929946527
Step: 4490, train/learning_rate: 4.4657306716544554e-05
Step: 4490, train/epoch: 1.068538784980774
Step: 4500, train/loss: 0.0
Step: 4500, train/grad_norm: 5.288525699143065e-06
Step: 4500, train/learning_rate: 4.464540688786656e-05
Step: 4500, train/epoch: 1.0709185600280762
Step: 4510, train/loss: 0.0
Step: 4510, train/grad_norm: 0.0007733017555437982
Step: 4510, train/learning_rate: 4.463350705918856e-05
Step: 4510, train/epoch: 1.073298454284668
Step: 4520, train/loss: 0.0
Step: 4520, train/grad_norm: 3.18877755489666e-05
Step: 4520, train/learning_rate: 4.462160723051056e-05
Step: 4520, train/epoch: 1.0756782293319702
Step: 4530, train/loss: 0.0
Step: 4530, train/grad_norm: 9.011955626192503e-06
Step: 4530, train/learning_rate: 4.460971103981137e-05
Step: 4530, train/epoch: 1.078058123588562
Step: 4540, train/loss: 0.0
Step: 4540, train/grad_norm: 8.651844837004319e-05
Step: 4540, train/learning_rate: 4.4597811211133376e-05
Step: 4540, train/epoch: 1.0804378986358643
Step: 4550, train/loss: 0.0
Step: 4550, train/grad_norm: 0.00010842249321285635
Step: 4550, train/learning_rate: 4.458591138245538e-05
Step: 4550, train/epoch: 1.0828176736831665
Step: 4560, train/loss: 0.03240000084042549
Step: 4560, train/grad_norm: 0.0015081489691510797
Step: 4560, train/learning_rate: 4.457401155377738e-05
Step: 4560, train/epoch: 1.0851975679397583
Step: 4570, train/loss: 0.0
Step: 4570, train/grad_norm: 0.00037251313915476203
Step: 4570, train/learning_rate: 4.4562111725099385e-05
Step: 4570, train/epoch: 1.0875773429870605
Step: 4580, train/loss: 0.0
Step: 4580, train/grad_norm: 4.9231137381866574e-05
Step: 4580, train/learning_rate: 4.4550215534400195e-05
Step: 4580, train/epoch: 1.0899571180343628
Step: 4590, train/loss: 0.00019999999494757503
Step: 4590, train/grad_norm: 2.7679010599968024e-05
Step: 4590, train/learning_rate: 4.45383157057222e-05
Step: 4590, train/epoch: 1.0923370122909546
Step: 4600, train/loss: 0.052000001072883606
Step: 4600, train/grad_norm: 6.290410965448245e-05
Step: 4600, train/learning_rate: 4.45264158770442e-05
Step: 4600, train/epoch: 1.0947167873382568
Step: 4610, train/loss: 0.0015999999595806003
Step: 4610, train/grad_norm: 5.2711806347360834e-05
Step: 4610, train/learning_rate: 4.4514516048366204e-05
Step: 4610, train/epoch: 1.097096562385559
Step: 4620, train/loss: 0.003700000001117587
Step: 4620, train/grad_norm: 0.0002195528068114072
Step: 4620, train/learning_rate: 4.450261621968821e-05
Step: 4620, train/epoch: 1.0994764566421509
Step: 4630, train/loss: 0.00039999998989515007
Step: 4630, train/grad_norm: 7.437060503434623e-06
Step: 4630, train/learning_rate: 4.449072002898902e-05
Step: 4630, train/epoch: 1.1018562316894531
Step: 4640, train/loss: 0.00039999998989515007
Step: 4640, train/grad_norm: 0.11250780522823334
Step: 4640, train/learning_rate: 4.447882020031102e-05
Step: 4640, train/epoch: 1.104236125946045
Step: 4650, train/loss: 0.0
Step: 4650, train/grad_norm: 2.26905285671819e-05
Step: 4650, train/learning_rate: 4.446692037163302e-05
Step: 4650, train/epoch: 1.1066159009933472
Step: 4660, train/loss: 0.0
Step: 4660, train/grad_norm: 0.00018424617883283645
Step: 4660, train/learning_rate: 4.4455020542955026e-05
Step: 4660, train/epoch: 1.1089956760406494
Step: 4670, train/loss: 0.0005000000237487257
Step: 4670, train/grad_norm: 2.7965188564849086e-05
Step: 4670, train/learning_rate: 4.444312071427703e-05
Step: 4670, train/epoch: 1.1113755702972412
Step: 4680, train/loss: 0.0
Step: 4680, train/grad_norm: 3.3788487030506076e-07
Step: 4680, train/learning_rate: 4.443122452357784e-05
Step: 4680, train/epoch: 1.1137553453445435
Step: 4690, train/loss: 0.27730000019073486
Step: 4690, train/grad_norm: 0.0004744503239635378
Step: 4690, train/learning_rate: 4.441932469489984e-05
Step: 4690, train/epoch: 1.1161351203918457
Step: 4700, train/loss: 0.0
Step: 4700, train/grad_norm: 7.348172221099958e-05
Step: 4700, train/learning_rate: 4.4407424866221845e-05
Step: 4700, train/epoch: 1.1185150146484375
Step: 4710, train/loss: 0.0
Step: 4710, train/grad_norm: 0.00040761116542853415
Step: 4710, train/learning_rate: 4.439552503754385e-05
Step: 4710, train/epoch: 1.1208947896957397
Step: 4720, train/loss: 0.0008999999845400453
Step: 4720, train/grad_norm: 0.002850171411409974
Step: 4720, train/learning_rate: 4.438362520886585e-05
Step: 4720, train/epoch: 1.1232746839523315
Step: 4730, train/loss: 0.19220000505447388
Step: 4730, train/grad_norm: 0.00019445300858933479
Step: 4730, train/learning_rate: 4.437172901816666e-05
Step: 4730, train/epoch: 1.1256544589996338
Step: 4740, train/loss: 0.0
Step: 4740, train/grad_norm: 0.0004579300875775516
Step: 4740, train/learning_rate: 4.4359829189488664e-05
Step: 4740, train/epoch: 1.128034234046936
Step: 4750, train/loss: 0.0010999999940395355
Step: 4750, train/grad_norm: 0.00037499330937862396
Step: 4750, train/learning_rate: 4.434792936081067e-05
Step: 4750, train/epoch: 1.1304141283035278
Step: 4760, train/loss: 0.0
Step: 4760, train/grad_norm: 4.115344199817628e-05
Step: 4760, train/learning_rate: 4.433602953213267e-05
Step: 4760, train/epoch: 1.13279390335083
Step: 4770, train/loss: 0.0
Step: 4770, train/grad_norm: 0.00036225287476554513
Step: 4770, train/learning_rate: 4.432412970345467e-05
Step: 4770, train/epoch: 1.1351736783981323
Step: 4780, train/loss: 0.0
Step: 4780, train/grad_norm: 0.004533871077001095
Step: 4780, train/learning_rate: 4.4312233512755483e-05
Step: 4780, train/epoch: 1.1375535726547241
Step: 4790, train/loss: 0.013700000010430813
Step: 4790, train/grad_norm: 1.4015082342666574e-05
Step: 4790, train/learning_rate: 4.4300333684077486e-05
Step: 4790, train/epoch: 1.1399333477020264
Step: 4800, train/loss: 0.0
Step: 4800, train/grad_norm: 5.379636604629923e-06
Step: 4800, train/learning_rate: 4.428843385539949e-05
Step: 4800, train/epoch: 1.1423132419586182
Step: 4810, train/loss: 0.0
Step: 4810, train/grad_norm: 0.00028803490567952394
Step: 4810, train/learning_rate: 4.427653402672149e-05
Step: 4810, train/epoch: 1.1446930170059204
Step: 4820, train/loss: 0.0027000000700354576
Step: 4820, train/grad_norm: 1.0975958502967842e-05
Step: 4820, train/learning_rate: 4.4264634198043495e-05
Step: 4820, train/epoch: 1.1470727920532227
Step: 4830, train/loss: 0.003800000064074993
Step: 4830, train/grad_norm: 5.2549588872352615e-05
Step: 4830, train/learning_rate: 4.4252738007344306e-05
Step: 4830, train/epoch: 1.1494526863098145
Step: 4840, train/loss: 0.0
Step: 4840, train/grad_norm: 3.2053430913947523e-06
Step: 4840, train/learning_rate: 4.424083817866631e-05
Step: 4840, train/epoch: 1.1518324613571167
Step: 4850, train/loss: 0.0
Step: 4850, train/grad_norm: 0.00014427118003368378
Step: 4850, train/learning_rate: 4.422893834998831e-05
Step: 4850, train/epoch: 1.154212236404419
Step: 4860, train/loss: 9.999999747378752e-05
Step: 4860, train/grad_norm: 2.049478098342661e-06
Step: 4860, train/learning_rate: 4.4217038521310315e-05
Step: 4860, train/epoch: 1.1565921306610107
Step: 4870, train/loss: 0.0
Step: 4870, train/grad_norm: 0.00024080178991425782
Step: 4870, train/learning_rate: 4.420513869263232e-05
Step: 4870, train/epoch: 1.158971905708313
Step: 4880, train/loss: 0.0
Step: 4880, train/grad_norm: 0.0002286253438796848
Step: 4880, train/learning_rate: 4.419324250193313e-05
Step: 4880, train/epoch: 1.1613516807556152
Step: 4890, train/loss: 0.12110000103712082
Step: 4890, train/grad_norm: 7.824687781976536e-05
Step: 4890, train/learning_rate: 4.418134267325513e-05
Step: 4890, train/epoch: 1.163731575012207
Step: 4900, train/loss: 0.0007999999797903001
Step: 4900, train/grad_norm: 0.007805750705301762
Step: 4900, train/learning_rate: 4.4169442844577134e-05
Step: 4900, train/epoch: 1.1661113500595093
Step: 4910, train/loss: 0.0
Step: 4910, train/grad_norm: 8.633906691102311e-05
Step: 4910, train/learning_rate: 4.415754301589914e-05
Step: 4910, train/epoch: 1.168491244316101
Step: 4920, train/loss: 0.0
Step: 4920, train/grad_norm: 2.7985328415525146e-05
Step: 4920, train/learning_rate: 4.414564318722114e-05
Step: 4920, train/epoch: 1.1708710193634033
Step: 4930, train/loss: 0.05550000071525574
Step: 4930, train/grad_norm: 0.009632177650928497
Step: 4930, train/learning_rate: 4.413374699652195e-05
Step: 4930, train/epoch: 1.1732507944107056
Step: 4940, train/loss: 0.04969999939203262
Step: 4940, train/grad_norm: 109.68990325927734
Step: 4940, train/learning_rate: 4.412184716784395e-05
Step: 4940, train/epoch: 1.1756306886672974
Step: 4950, train/loss: 0.0
Step: 4950, train/grad_norm: 4.351795723778196e-06
Step: 4950, train/learning_rate: 4.4109947339165956e-05
Step: 4950, train/epoch: 1.1780104637145996
Step: 4960, train/loss: 0.0
Step: 4960, train/grad_norm: 8.294464919345046e-07
Step: 4960, train/learning_rate: 4.409804751048796e-05
Step: 4960, train/epoch: 1.1803902387619019
Step: 4970, train/loss: 0.026599999517202377
Step: 4970, train/grad_norm: 2.9540674972849956e-07
Step: 4970, train/learning_rate: 4.408614768180996e-05
Step: 4970, train/epoch: 1.1827701330184937
Step: 4980, train/loss: 0.0
Step: 4980, train/grad_norm: 3.078234556141979e-07
Step: 4980, train/learning_rate: 4.407425149111077e-05
Step: 4980, train/epoch: 1.185149908065796
Step: 4990, train/loss: 0.18050000071525574
Step: 4990, train/grad_norm: 4.0057526007331035e-07
Step: 4990, train/learning_rate: 4.4062351662432775e-05
Step: 4990, train/epoch: 1.1875298023223877
Step: 5000, train/loss: 0.0
Step: 5000, train/grad_norm: 0.0011076470836997032
Step: 5000, train/learning_rate: 4.405045183375478e-05
Step: 5000, train/epoch: 1.18990957736969
Step: 5010, train/loss: 0.010300000198185444
Step: 5010, train/grad_norm: 16.755611419677734
Step: 5010, train/learning_rate: 4.403855200507678e-05
Step: 5010, train/epoch: 1.1922893524169922
Step: 5020, train/loss: 0.0
Step: 5020, train/grad_norm: 1.2618030268640723e-05
Step: 5020, train/learning_rate: 4.4026652176398784e-05
Step: 5020, train/epoch: 1.194669246673584
Step: 5030, train/loss: 0.030300000682473183
Step: 5030, train/grad_norm: 0.0007187035516835749
Step: 5030, train/learning_rate: 4.4014755985699594e-05
Step: 5030, train/epoch: 1.1970490217208862
Step: 5040, train/loss: 0.05829999968409538
Step: 5040, train/grad_norm: 5.125390089233406e-06
Step: 5040, train/learning_rate: 4.40028561570216e-05
Step: 5040, train/epoch: 1.1994287967681885
Step: 5050, train/loss: 0.00039999998989515007
Step: 5050, train/grad_norm: 3.9662580775257084e-07
Step: 5050, train/learning_rate: 4.39909563283436e-05
Step: 5050, train/epoch: 1.2018086910247803
Step: 5060, train/loss: 0.0
Step: 5060, train/grad_norm: 2.9972441097925184e-06
Step: 5060, train/learning_rate: 4.39790564996656e-05
Step: 5060, train/epoch: 1.2041884660720825
Step: 5070, train/loss: 0.0
Step: 5070, train/grad_norm: 2.8668380764429457e-05
Step: 5070, train/learning_rate: 4.396716030896641e-05
Step: 5070, train/epoch: 1.2065683603286743
Step: 5080, train/loss: 0.0
Step: 5080, train/grad_norm: 2.743647200986743e-05
Step: 5080, train/learning_rate: 4.3955260480288416e-05
Step: 5080, train/epoch: 1.2089481353759766
Step: 5090, train/loss: 9.999999747378752e-05
Step: 5090, train/grad_norm: 0.7631564736366272
Step: 5090, train/learning_rate: 4.394336065161042e-05
Step: 5090, train/epoch: 1.2113279104232788
Step: 5100, train/loss: 0.0763000026345253
Step: 5100, train/grad_norm: 0.0001833518617786467
Step: 5100, train/learning_rate: 4.393146082293242e-05
Step: 5100, train/epoch: 1.2137078046798706
Step: 5110, train/loss: 0.0
Step: 5110, train/grad_norm: 5.49746400793083e-05
Step: 5110, train/learning_rate: 4.3919560994254425e-05
Step: 5110, train/epoch: 1.2160875797271729
Step: 5120, train/loss: 0.0
Step: 5120, train/grad_norm: 0.0002890972245950252
Step: 5120, train/learning_rate: 4.3907664803555235e-05
Step: 5120, train/epoch: 1.218467354774475
Step: 5130, train/loss: 9.999999747378752e-05
Step: 5130, train/grad_norm: 0.49700069427490234
Step: 5130, train/learning_rate: 4.389576497487724e-05
Step: 5130, train/epoch: 1.220847249031067
Step: 5140, train/loss: 0.0
Step: 5140, train/grad_norm: 5.049478932050988e-05
Step: 5140, train/learning_rate: 4.388386514619924e-05
Step: 5140, train/epoch: 1.2232270240783691
Step: 5150, train/loss: 0.05979999899864197
Step: 5150, train/grad_norm: 5.2939917623007204e-06
Step: 5150, train/learning_rate: 4.3871965317521244e-05
Step: 5150, train/epoch: 1.2256067991256714
Step: 5160, train/loss: 0.00019999999494757503
Step: 5160, train/grad_norm: 0.01433536782860756
Step: 5160, train/learning_rate: 4.386006548884325e-05
Step: 5160, train/epoch: 1.2279866933822632
Step: 5170, train/loss: 0.0
Step: 5170, train/grad_norm: 0.00020050464081577957
Step: 5170, train/learning_rate: 4.384816929814406e-05
Step: 5170, train/epoch: 1.2303664684295654
Step: 5180, train/loss: 0.0
Step: 5180, train/grad_norm: 0.0007958060596138239
Step: 5180, train/learning_rate: 4.383626946946606e-05
Step: 5180, train/epoch: 1.2327463626861572
Step: 5190, train/loss: 0.0
Step: 5190, train/grad_norm: 1.2847240213886835e-05
Step: 5190, train/learning_rate: 4.3824369640788063e-05
Step: 5190, train/epoch: 1.2351261377334595
Step: 5200, train/loss: 0.0
Step: 5200, train/grad_norm: 3.63644867320545e-05
Step: 5200, train/learning_rate: 4.3812469812110066e-05
Step: 5200, train/epoch: 1.2375059127807617
Step: 5210, train/loss: 0.005799999926239252
Step: 5210, train/grad_norm: 6.329302095764433e-07
Step: 5210, train/learning_rate: 4.380056998343207e-05
Step: 5210, train/epoch: 1.2398858070373535
Step: 5220, train/loss: 0.0
Step: 5220, train/grad_norm: 6.766805017832667e-05
Step: 5220, train/learning_rate: 4.378867379273288e-05
Step: 5220, train/epoch: 1.2422655820846558
Step: 5230, train/loss: 0.007799999788403511
Step: 5230, train/grad_norm: 9.545310319936107e-08
Step: 5230, train/learning_rate: 4.377677396405488e-05
Step: 5230, train/epoch: 1.244645357131958
Step: 5240, train/loss: 0.021299999207258224
Step: 5240, train/grad_norm: 0.004895782098174095
Step: 5240, train/learning_rate: 4.3764874135376886e-05
Step: 5240, train/epoch: 1.2470252513885498
Step: 5250, train/loss: 0.0
Step: 5250, train/grad_norm: 3.3202647955477005e-06
Step: 5250, train/learning_rate: 4.375297430669889e-05
Step: 5250, train/epoch: 1.249405026435852
Step: 5260, train/loss: 0.08900000154972076
Step: 5260, train/grad_norm: 15.405407905578613
Step: 5260, train/learning_rate: 4.374107447802089e-05
Step: 5260, train/epoch: 1.2517849206924438
Step: 5270, train/loss: 0.0
Step: 5270, train/grad_norm: 0.0004468466795515269
Step: 5270, train/learning_rate: 4.37291782873217e-05
Step: 5270, train/epoch: 1.254164695739746
Step: 5280, train/loss: 0.0
Step: 5280, train/grad_norm: 2.346652308915509e-06
Step: 5280, train/learning_rate: 4.3717278458643705e-05
Step: 5280, train/epoch: 1.2565444707870483
Step: 5290, train/loss: 0.20739999413490295
Step: 5290, train/grad_norm: 83.86331939697266
Step: 5290, train/learning_rate: 4.370537862996571e-05
Step: 5290, train/epoch: 1.2589243650436401
Step: 5300, train/loss: 0.024900000542402267
Step: 5300, train/grad_norm: 0.0032862366642802954
Step: 5300, train/learning_rate: 4.369347880128771e-05
Step: 5300, train/epoch: 1.2613041400909424
Step: 5310, train/loss: 0.15279999375343323
Step: 5310, train/grad_norm: 7.433442078763619e-05
Step: 5310, train/learning_rate: 4.3681578972609714e-05
Step: 5310, train/epoch: 1.2636839151382446
Step: 5320, train/loss: 0.0
Step: 5320, train/grad_norm: 0.00016374124970752746
Step: 5320, train/learning_rate: 4.3669682781910524e-05
Step: 5320, train/epoch: 1.2660638093948364
Step: 5330, train/loss: 0.02969999983906746
Step: 5330, train/grad_norm: 0.012808069586753845
Step: 5330, train/learning_rate: 4.365778295323253e-05
Step: 5330, train/epoch: 1.2684435844421387
Step: 5340, train/loss: 0.10790000110864639
Step: 5340, train/grad_norm: 14.489277839660645
Step: 5340, train/learning_rate: 4.364588312455453e-05
Step: 5340, train/epoch: 1.270823359489441
Step: 5350, train/loss: 0.0
Step: 5350, train/grad_norm: 0.0005638955044560134
Step: 5350, train/learning_rate: 4.363398329587653e-05
Step: 5350, train/epoch: 1.2732032537460327
Step: 5360, train/loss: 0.0
Step: 5360, train/grad_norm: 6.00656057940796e-05
Step: 5360, train/learning_rate: 4.3622083467198536e-05
Step: 5360, train/epoch: 1.275583028793335
Step: 5370, train/loss: 0.0
Step: 5370, train/grad_norm: 3.0399140086956322e-05
Step: 5370, train/learning_rate: 4.3610187276499346e-05
Step: 5370, train/epoch: 1.2779629230499268
Step: 5380, train/loss: 0.0
Step: 5380, train/grad_norm: 0.00987222045660019
Step: 5380, train/learning_rate: 4.359828744782135e-05
Step: 5380, train/epoch: 1.280342698097229
Step: 5390, train/loss: 0.08179999887943268
Step: 5390, train/grad_norm: 54.40876388549805
Step: 5390, train/learning_rate: 4.358638761914335e-05
Step: 5390, train/epoch: 1.2827224731445312
Step: 5400, train/loss: 0.056699998676776886
Step: 5400, train/grad_norm: 0.0005392834427766502
Step: 5400, train/learning_rate: 4.3574487790465355e-05
Step: 5400, train/epoch: 1.285102367401123
Step: 5410, train/loss: 0.0
Step: 5410, train/grad_norm: 0.0003039234143216163
Step: 5410, train/learning_rate: 4.356258796178736e-05
Step: 5410, train/epoch: 1.2874821424484253
Step: 5420, train/loss: 0.00019999999494757503
Step: 5420, train/grad_norm: 1.3288772106170654
Step: 5420, train/learning_rate: 4.355069177108817e-05
Step: 5420, train/epoch: 1.2898619174957275
Step: 5430, train/loss: 0.0
Step: 5430, train/grad_norm: 0.005609726533293724
Step: 5430, train/learning_rate: 4.353879194241017e-05
Step: 5430, train/epoch: 1.2922418117523193
Step: 5440, train/loss: 0.13199999928474426
Step: 5440, train/grad_norm: 0.0014379931380972266
Step: 5440, train/learning_rate: 4.3526892113732174e-05
Step: 5440, train/epoch: 1.2946215867996216
Step: 5450, train/loss: 0.0
Step: 5450, train/grad_norm: 0.0005099300760775805
Step: 5450, train/learning_rate: 4.351499228505418e-05
Step: 5450, train/epoch: 1.2970014810562134
Step: 5460, train/loss: 0.0
Step: 5460, train/grad_norm: 6.68833454255946e-05
Step: 5460, train/learning_rate: 4.350309245637618e-05
Step: 5460, train/epoch: 1.2993812561035156
Step: 5470, train/loss: 0.0
Step: 5470, train/grad_norm: 0.00021689229470212013
Step: 5470, train/learning_rate: 4.349119626567699e-05
Step: 5470, train/epoch: 1.3017610311508179
Step: 5480, train/loss: 0.011099999770522118
Step: 5480, train/grad_norm: 3.822605503955856e-05
Step: 5480, train/learning_rate: 4.347929643699899e-05
Step: 5480, train/epoch: 1.3041409254074097
Step: 5490, train/loss: 0.0
Step: 5490, train/grad_norm: 2.567301635281183e-06
Step: 5490, train/learning_rate: 4.3467396608320996e-05
Step: 5490, train/epoch: 1.306520700454712
Step: 5500, train/loss: 9.999999747378752e-05
Step: 5500, train/grad_norm: 0.02478010207414627
Step: 5500, train/learning_rate: 4.3455496779643e-05
Step: 5500, train/epoch: 1.3089004755020142
Step: 5510, train/loss: 0.0
Step: 5510, train/grad_norm: 7.105281110852957e-05
Step: 5510, train/learning_rate: 4.3443596950965e-05
Step: 5510, train/epoch: 1.311280369758606
Step: 5520, train/loss: 0.0
Step: 5520, train/grad_norm: 0.011986669152975082
Step: 5520, train/learning_rate: 4.343170076026581e-05
Step: 5520, train/epoch: 1.3136601448059082
Step: 5530, train/loss: 0.0
Step: 5530, train/grad_norm: 0.00010591372847557068
Step: 5530, train/learning_rate: 4.3419800931587815e-05
Step: 5530, train/epoch: 1.3160400390625
Step: 5540, train/loss: 0.08630000054836273
Step: 5540, train/grad_norm: 6.049552757758647e-05
Step: 5540, train/learning_rate: 4.340790110290982e-05
Step: 5540, train/epoch: 1.3184198141098022
Step: 5550, train/loss: 0.012299999594688416
Step: 5550, train/grad_norm: 12.497969627380371
Step: 5550, train/learning_rate: 4.339600127423182e-05
Step: 5550, train/epoch: 1.3207995891571045
Step: 5560, train/loss: 9.999999747378752e-05
Step: 5560, train/grad_norm: 0.005224193911999464
Step: 5560, train/learning_rate: 4.3384101445553824e-05
Step: 5560, train/epoch: 1.3231794834136963
Step: 5570, train/loss: 0.11680000275373459
Step: 5570, train/grad_norm: 0.0009697180357761681
Step: 5570, train/learning_rate: 4.3372205254854634e-05
Step: 5570, train/epoch: 1.3255592584609985
Step: 5580, train/loss: 0.0010000000474974513
Step: 5580, train/grad_norm: 0.0012881177244707942
Step: 5580, train/learning_rate: 4.336030542617664e-05
Step: 5580, train/epoch: 1.3279390335083008
Step: 5590, train/loss: 9.999999747378752e-05
Step: 5590, train/grad_norm: 0.04143490269780159
Step: 5590, train/learning_rate: 4.334840559749864e-05
Step: 5590, train/epoch: 1.3303189277648926
Step: 5600, train/loss: 0.001500000013038516
Step: 5600, train/grad_norm: 0.004401715472340584
Step: 5600, train/learning_rate: 4.3336505768820643e-05
Step: 5600, train/epoch: 1.3326987028121948
Step: 5610, train/loss: 9.999999747378752e-05
Step: 5610, train/grad_norm: 0.0002862866676878184
Step: 5610, train/learning_rate: 4.3324605940142646e-05
Step: 5610, train/epoch: 1.335078477859497
Step: 5620, train/loss: 0.0
Step: 5620, train/grad_norm: 0.018040038645267487
Step: 5620, train/learning_rate: 4.3312709749443457e-05
Step: 5620, train/epoch: 1.3374583721160889
Step: 5630, train/loss: 0.0
Step: 5630, train/grad_norm: 0.0008745904779061675
Step: 5630, train/learning_rate: 4.330080992076546e-05
Step: 5630, train/epoch: 1.3398381471633911
Step: 5640, train/loss: 0.0
Step: 5640, train/grad_norm: 0.0005197833525016904
Step: 5640, train/learning_rate: 4.328891009208746e-05
Step: 5640, train/epoch: 1.342218041419983
Step: 5650, train/loss: 0.0
Step: 5650, train/grad_norm: 0.0006000424036756158
Step: 5650, train/learning_rate: 4.3277010263409466e-05
Step: 5650, train/epoch: 1.3445978164672852
Step: 5660, train/loss: 0.0
Step: 5660, train/grad_norm: 0.0005245847860351205
Step: 5660, train/learning_rate: 4.326511043473147e-05
Step: 5660, train/epoch: 1.3469775915145874
Step: 5670, train/loss: 0.0
Step: 5670, train/grad_norm: 5.087085082777776e-05
Step: 5670, train/learning_rate: 4.325321424403228e-05
Step: 5670, train/epoch: 1.3493574857711792
Step: 5680, train/loss: 0.0
Step: 5680, train/grad_norm: 0.00035866082180291414
Step: 5680, train/learning_rate: 4.324131441535428e-05
Step: 5680, train/epoch: 1.3517372608184814
Step: 5690, train/loss: 0.0
Step: 5690, train/grad_norm: 5.169379437575117e-05
Step: 5690, train/learning_rate: 4.3229414586676285e-05
Step: 5690, train/epoch: 1.3541170358657837
Step: 5700, train/loss: 0.05510000139474869
Step: 5700, train/grad_norm: 7.533469761256129e-05
Step: 5700, train/learning_rate: 4.321751475799829e-05
Step: 5700, train/epoch: 1.3564969301223755
Step: 5710, train/loss: 0.0
Step: 5710, train/grad_norm: 0.0009403146686963737
Step: 5710, train/learning_rate: 4.320561492932029e-05
Step: 5710, train/epoch: 1.3588767051696777
Step: 5720, train/loss: 9.999999747378752e-05
Step: 5720, train/grad_norm: 0.00044840024202130735
Step: 5720, train/learning_rate: 4.31937187386211e-05
Step: 5720, train/epoch: 1.3612565994262695
Step: 5730, train/loss: 9.999999747378752e-05
Step: 5730, train/grad_norm: 3.952699989895336e-05
Step: 5730, train/learning_rate: 4.3181818909943104e-05
Step: 5730, train/epoch: 1.3636363744735718
Step: 5740, train/loss: 0.006000000052154064
Step: 5740, train/grad_norm: 6.931278039701283e-05
Step: 5740, train/learning_rate: 4.316991908126511e-05
Step: 5740, train/epoch: 1.366016149520874
Step: 5750, train/loss: 0.0
Step: 5750, train/grad_norm: 2.5825316697591916e-05
Step: 5750, train/learning_rate: 4.315801925258711e-05
Step: 5750, train/epoch: 1.3683960437774658
Step: 5760, train/loss: 0.0
Step: 5760, train/grad_norm: 0.00019648560555651784
Step: 5760, train/learning_rate: 4.314611942390911e-05
Step: 5760, train/epoch: 1.370775818824768
Step: 5770, train/loss: 0.0013000000035390258
Step: 5770, train/grad_norm: 1.741271375976794e-07
Step: 5770, train/learning_rate: 4.313422323320992e-05
Step: 5770, train/epoch: 1.3731555938720703
Step: 5780, train/loss: 0.0
Step: 5780, train/grad_norm: 4.106842243345454e-05
Step: 5780, train/learning_rate: 4.3122323404531926e-05
Step: 5780, train/epoch: 1.375535488128662
Step: 5790, train/loss: 0.2085999995470047
Step: 5790, train/grad_norm: 92.71823120117188
Step: 5790, train/learning_rate: 4.311042357585393e-05
Step: 5790, train/epoch: 1.3779152631759644
Step: 5800, train/loss: 0.1054999977350235
Step: 5800, train/grad_norm: 9.17632642085664e-05
Step: 5800, train/learning_rate: 4.309852374717593e-05
Step: 5800, train/epoch: 1.3802950382232666
Step: 5810, train/loss: 0.0
Step: 5810, train/grad_norm: 4.1752387915039435e-05
Step: 5810, train/learning_rate: 4.3086623918497935e-05
Step: 5810, train/epoch: 1.3826749324798584
Step: 5820, train/loss: 0.04179999977350235
Step: 5820, train/grad_norm: 0.0003983082715421915
Step: 5820, train/learning_rate: 4.3074727727798745e-05
Step: 5820, train/epoch: 1.3850547075271606
Step: 5830, train/loss: 0.005100000184029341
Step: 5830, train/grad_norm: 0.5929059982299805
Step: 5830, train/learning_rate: 4.306282789912075e-05
Step: 5830, train/epoch: 1.3874346017837524
Step: 5840, train/loss: 0.0013000000035390258
Step: 5840, train/grad_norm: 10.295860290527344
Step: 5840, train/learning_rate: 4.305092807044275e-05
Step: 5840, train/epoch: 1.3898143768310547
Step: 5850, train/loss: 0.003599999938160181
Step: 5850, train/grad_norm: 26.438457489013672
Step: 5850, train/learning_rate: 4.3039028241764754e-05
Step: 5850, train/epoch: 1.392194151878357
Step: 5860, train/loss: 0.0
Step: 5860, train/grad_norm: 0.004436613526195288
Step: 5860, train/learning_rate: 4.302712841308676e-05
Step: 5860, train/epoch: 1.3945740461349487
Step: 5870, train/loss: 0.15489999949932098
Step: 5870, train/grad_norm: 0.0006172299035824835
Step: 5870, train/learning_rate: 4.301523222238757e-05
Step: 5870, train/epoch: 1.396953821182251
Step: 5880, train/loss: 0.00019999999494757503
Step: 5880, train/grad_norm: 0.0054014637134969234
Step: 5880, train/learning_rate: 4.300333239370957e-05
Step: 5880, train/epoch: 1.3993335962295532
Step: 5890, train/loss: 0.0
Step: 5890, train/grad_norm: 0.0026967423036694527
Step: 5890, train/learning_rate: 4.299143256503157e-05
Step: 5890, train/epoch: 1.401713490486145
Step: 5900, train/loss: 0.0333000011742115
Step: 5900, train/grad_norm: 6.828145706094801e-05
Step: 5900, train/learning_rate: 4.2979532736353576e-05
Step: 5900, train/epoch: 1.4040932655334473
Step: 5910, train/loss: 0.0012000000569969416
Step: 5910, train/grad_norm: 0.00017195130931213498
Step: 5910, train/learning_rate: 4.296763290767558e-05
Step: 5910, train/epoch: 1.406473159790039
Step: 5920, train/loss: 0.11020000278949738
Step: 5920, train/grad_norm: 0.0003515393764246255
Step: 5920, train/learning_rate: 4.295573671697639e-05
Step: 5920, train/epoch: 1.4088529348373413
Step: 5930, train/loss: 0.0
Step: 5930, train/grad_norm: 5.619807780021802e-06
Step: 5930, train/learning_rate: 4.294383688829839e-05
Step: 5930, train/epoch: 1.4112327098846436
Step: 5940, train/loss: 0.0632999986410141
Step: 5940, train/grad_norm: 0.0013197084190323949
Step: 5940, train/learning_rate: 4.2931937059620395e-05
Step: 5940, train/epoch: 1.4136126041412354
Step: 5950, train/loss: 0.04749999940395355
Step: 5950, train/grad_norm: 1.5836379528045654
Step: 5950, train/learning_rate: 4.29200372309424e-05
Step: 5950, train/epoch: 1.4159923791885376
Step: 5960, train/loss: 0.00279999990016222
Step: 5960, train/grad_norm: 0.00016536812472622842
Step: 5960, train/learning_rate: 4.29081374022644e-05
Step: 5960, train/epoch: 1.4183721542358398
Step: 5970, train/loss: 0.0
Step: 5970, train/grad_norm: 2.8449097953853197e-05
Step: 5970, train/learning_rate: 4.289624121156521e-05
Step: 5970, train/epoch: 1.4207520484924316
Step: 5980, train/loss: 0.0
Step: 5980, train/grad_norm: 0.0014349124394357204
Step: 5980, train/learning_rate: 4.2884341382887214e-05
Step: 5980, train/epoch: 1.4231318235397339
Step: 5990, train/loss: 0.11029999703168869
Step: 5990, train/grad_norm: 2.5887622996378923e-06
Step: 5990, train/learning_rate: 4.287244155420922e-05
Step: 5990, train/epoch: 1.4255117177963257
Step: 6000, train/loss: 9.999999747378752e-05
Step: 6000, train/grad_norm: 5.457268798636505e-06
Step: 6000, train/learning_rate: 4.286054172553122e-05
Step: 6000, train/epoch: 1.427891492843628
Step: 6010, train/loss: 0.0026000000070780516
Step: 6010, train/grad_norm: 0.00016549330030102283
Step: 6010, train/learning_rate: 4.2848641896853223e-05
Step: 6010, train/epoch: 1.4302712678909302
Step: 6020, train/loss: 0.020500000566244125
Step: 6020, train/grad_norm: 0.00036284688394516706
Step: 6020, train/learning_rate: 4.2836745706154034e-05
Step: 6020, train/epoch: 1.432651162147522
Step: 6030, train/loss: 0.031599998474121094
Step: 6030, train/grad_norm: 4.343912223703228e-05
Step: 6030, train/learning_rate: 4.2824845877476037e-05
Step: 6030, train/epoch: 1.4350309371948242
Step: 6040, train/loss: 0.0
Step: 6040, train/grad_norm: 5.446890099847224e-06
Step: 6040, train/learning_rate: 4.281294604879804e-05
Step: 6040, train/epoch: 1.4374107122421265
Step: 6050, train/loss: 0.031300000846385956
Step: 6050, train/grad_norm: 8.457908734271768e-06
Step: 6050, train/learning_rate: 4.280104622012004e-05
Step: 6050, train/epoch: 1.4397906064987183
Step: 6060, train/loss: 0.00019999999494757503
Step: 6060, train/grad_norm: 2.456245510984445e-06
Step: 6060, train/learning_rate: 4.2789146391442046e-05
Step: 6060, train/epoch: 1.4421703815460205
Step: 6070, train/loss: 0.0
Step: 6070, train/grad_norm: 1.1434702173573896e-05
Step: 6070, train/learning_rate: 4.2777250200742856e-05
Step: 6070, train/epoch: 1.4445501565933228
Step: 6080, train/loss: 0.0
Step: 6080, train/grad_norm: 0.0013977237977087498
Step: 6080, train/learning_rate: 4.276535037206486e-05
Step: 6080, train/epoch: 1.4469300508499146
Step: 6090, train/loss: 0.2558000087738037
Step: 6090, train/grad_norm: 5.957207577012014e-06
Step: 6090, train/learning_rate: 4.275345054338686e-05
Step: 6090, train/epoch: 1.4493098258972168
Step: 6100, train/loss: 0.0
Step: 6100, train/grad_norm: 0.0004943725652992725
Step: 6100, train/learning_rate: 4.2741550714708865e-05
Step: 6100, train/epoch: 1.4516897201538086
Step: 6110, train/loss: 0.0
Step: 6110, train/grad_norm: 1.4443669897445943e-05
Step: 6110, train/learning_rate: 4.272965088603087e-05
Step: 6110, train/epoch: 1.4540694952011108
Step: 6120, train/loss: 0.0
Step: 6120, train/grad_norm: 0.039788778871297836
Step: 6120, train/learning_rate: 4.271775469533168e-05
Step: 6120, train/epoch: 1.456449270248413
Step: 6130, train/loss: 0.0
Step: 6130, train/grad_norm: 2.6476316179468995e-06
Step: 6130, train/learning_rate: 4.270585486665368e-05
Step: 6130, train/epoch: 1.4588291645050049
Step: 6140, train/loss: 0.0
Step: 6140, train/grad_norm: 5.116829129292455e-07
Step: 6140, train/learning_rate: 4.2693955037975684e-05
Step: 6140, train/epoch: 1.4612089395523071
Step: 6150, train/loss: 0.0778999999165535
Step: 6150, train/grad_norm: 0.00020540592959150672
Step: 6150, train/learning_rate: 4.268205520929769e-05
Step: 6150, train/epoch: 1.4635887145996094
Step: 6160, train/loss: 0.0044999998062849045
Step: 6160, train/grad_norm: 2.1835185179952532e-05
Step: 6160, train/learning_rate: 4.267015538061969e-05
Step: 6160, train/epoch: 1.4659686088562012
Step: 6170, train/loss: 0.08590000122785568
Step: 6170, train/grad_norm: 2.3141612473409623e-05
Step: 6170, train/learning_rate: 4.26582591899205e-05
Step: 6170, train/epoch: 1.4683483839035034
Step: 6180, train/loss: 9.999999747378752e-05
Step: 6180, train/grad_norm: 0.21981099247932434
Step: 6180, train/learning_rate: 4.26463593612425e-05
Step: 6180, train/epoch: 1.4707282781600952
Step: 6190, train/loss: 0.0
Step: 6190, train/grad_norm: 8.375219294975977e-06
Step: 6190, train/learning_rate: 4.2634459532564506e-05
Step: 6190, train/epoch: 1.4731080532073975
Step: 6200, train/loss: 0.0
Step: 6200, train/grad_norm: 1.2182863429188728e-05
Step: 6200, train/learning_rate: 4.262255970388651e-05
Step: 6200, train/epoch: 1.4754878282546997
Step: 6210, train/loss: 0.0
Step: 6210, train/grad_norm: 1.0750340152299032e-05
Step: 6210, train/learning_rate: 4.261065987520851e-05
Step: 6210, train/epoch: 1.4778677225112915
Step: 6220, train/loss: 0.0
Step: 6220, train/grad_norm: 5.024684014642844e-06
Step: 6220, train/learning_rate: 4.259876368450932e-05
Step: 6220, train/epoch: 1.4802474975585938
Step: 6230, train/loss: 0.0
Step: 6230, train/grad_norm: 4.9099892152071334e-08
Step: 6230, train/learning_rate: 4.2586863855831325e-05
Step: 6230, train/epoch: 1.482627272605896
Step: 6240, train/loss: 9.999999747378752e-05
Step: 6240, train/grad_norm: 5.739143489336129e-06
Step: 6240, train/learning_rate: 4.257496402715333e-05
Step: 6240, train/epoch: 1.4850071668624878
Step: 6250, train/loss: 0.15629999339580536
Step: 6250, train/grad_norm: 4.0203588014264824e-07
Step: 6250, train/learning_rate: 4.256306419847533e-05
Step: 6250, train/epoch: 1.48738694190979
Step: 6260, train/loss: 0.0006000000284984708
Step: 6260, train/grad_norm: 1.498057246208191
Step: 6260, train/learning_rate: 4.2551164369797334e-05
Step: 6260, train/epoch: 1.4897668361663818
Step: 6270, train/loss: 0.00039999998989515007
Step: 6270, train/grad_norm: 9.485722694080323e-05
Step: 6270, train/learning_rate: 4.2539268179098144e-05
Step: 6270, train/epoch: 1.492146611213684
Step: 6280, train/loss: 0.14730000495910645
Step: 6280, train/grad_norm: 1.4123381333774887e-05
Step: 6280, train/learning_rate: 4.252736835042015e-05
Step: 6280, train/epoch: 1.4945263862609863
Step: 6290, train/loss: 0.0
Step: 6290, train/grad_norm: 5.0076247134711593e-05
Step: 6290, train/learning_rate: 4.251546852174215e-05
Step: 6290, train/epoch: 1.4969062805175781
Step: 6300, train/loss: 9.999999747378752e-05
Step: 6300, train/grad_norm: 0.0004921596264466643
Step: 6300, train/learning_rate: 4.250356869306415e-05
Step: 6300, train/epoch: 1.4992860555648804
Step: 6310, train/loss: 0.003800000064074993
Step: 6310, train/grad_norm: 0.001368759199976921
Step: 6310, train/learning_rate: 4.2491668864386156e-05
Step: 6310, train/epoch: 1.5016658306121826
Step: 6320, train/loss: 0.13259999454021454
Step: 6320, train/grad_norm: 0.0001293076784349978
Step: 6320, train/learning_rate: 4.2479772673686966e-05
Step: 6320, train/epoch: 1.5040457248687744
Step: 6330, train/loss: 0.0006000000284984708
Step: 6330, train/grad_norm: 0.0020297097507864237
Step: 6330, train/learning_rate: 4.246787284500897e-05
Step: 6330, train/epoch: 1.5064254999160767
Step: 6340, train/loss: 0.0
Step: 6340, train/grad_norm: 3.280352700585354e-07
Step: 6340, train/learning_rate: 4.245597301633097e-05
Step: 6340, train/epoch: 1.508805274963379
Step: 6350, train/loss: 0.0071000000461936
Step: 6350, train/grad_norm: 1.7976921640183718e-07
Step: 6350, train/learning_rate: 4.2444073187652975e-05
Step: 6350, train/epoch: 1.5111851692199707
Step: 6360, train/loss: 0.0
Step: 6360, train/grad_norm: 9.700521275135543e-08
Step: 6360, train/learning_rate: 4.243217335897498e-05
Step: 6360, train/epoch: 1.513564944267273
Step: 6370, train/loss: 0.0
Step: 6370, train/grad_norm: 2.499078163964441e-06
Step: 6370, train/learning_rate: 4.242027716827579e-05
Step: 6370, train/epoch: 1.5159448385238647
Step: 6380, train/loss: 0.0
Step: 6380, train/grad_norm: 2.2563185666513164e-06
Step: 6380, train/learning_rate: 4.240837733959779e-05
Step: 6380, train/epoch: 1.518324613571167
Step: 6390, train/loss: 0.0
Step: 6390, train/grad_norm: 0.01548240426927805
Step: 6390, train/learning_rate: 4.2396477510919794e-05
Step: 6390, train/epoch: 1.5207043886184692
Step: 6400, train/loss: 0.0
Step: 6400, train/grad_norm: 5.9869757933483925e-06
Step: 6400, train/learning_rate: 4.23845776822418e-05
Step: 6400, train/epoch: 1.523084282875061
Step: 6410, train/loss: 0.06759999692440033
Step: 6410, train/grad_norm: 5.096355266687169e-07
Step: 6410, train/learning_rate: 4.237268149154261e-05
Step: 6410, train/epoch: 1.5254640579223633
Step: 6420, train/loss: 9.999999747378752e-05
Step: 6420, train/grad_norm: 2.3655338736716658e-06
Step: 6420, train/learning_rate: 4.236078166286461e-05
Step: 6420, train/epoch: 1.5278438329696655
Step: 6430, train/loss: 0.0
Step: 6430, train/grad_norm: 1.113469170377357e-05
Step: 6430, train/learning_rate: 4.2348881834186614e-05
Step: 6430, train/epoch: 1.5302237272262573
Step: 6440, train/loss: 0.0
Step: 6440, train/grad_norm: 1.466911925263048e-07
Step: 6440, train/learning_rate: 4.2336982005508617e-05
Step: 6440, train/epoch: 1.5326035022735596
Step: 6450, train/loss: 0.02290000021457672
Step: 6450, train/grad_norm: 50.56777572631836
Step: 6450, train/learning_rate: 4.232508217683062e-05
Step: 6450, train/epoch: 1.5349833965301514
Step: 6460, train/loss: 0.0
Step: 6460, train/grad_norm: 8.205433914554305e-06
Step: 6460, train/learning_rate: 4.231318598613143e-05
Step: 6460, train/epoch: 1.5373631715774536
Step: 6470, train/loss: 0.0
Step: 6470, train/grad_norm: 7.7027318184264e-05
Step: 6470, train/learning_rate: 4.230128615745343e-05
Step: 6470, train/epoch: 1.5397429466247559
Step: 6480, train/loss: 0.0
Step: 6480, train/grad_norm: 1.3635485629492905e-05
Step: 6480, train/learning_rate: 4.2289386328775436e-05
Step: 6480, train/epoch: 1.5421228408813477
Step: 6490, train/loss: 0.05000000074505806
Step: 6490, train/grad_norm: 0.0007877537282183766
Step: 6490, train/learning_rate: 4.227748650009744e-05
Step: 6490, train/epoch: 1.54450261592865
Step: 6500, train/loss: 0.0031999999191612005
Step: 6500, train/grad_norm: 7.140478714973142e-07
Step: 6500, train/learning_rate: 4.226558667141944e-05
Step: 6500, train/epoch: 1.5468823909759521
Step: 6510, train/loss: 0.0
Step: 6510, train/grad_norm: 1.4120919900051376e-07
Step: 6510, train/learning_rate: 4.225369048072025e-05
Step: 6510, train/epoch: 1.549262285232544
Step: 6520, train/loss: 0.0
Step: 6520, train/grad_norm: 1.1821889529528562e-05
Step: 6520, train/learning_rate: 4.2241790652042255e-05
Step: 6520, train/epoch: 1.5516420602798462
Step: 6530, train/loss: 0.0
Step: 6530, train/grad_norm: 0.0002261717600049451
Step: 6530, train/learning_rate: 4.222989082336426e-05
Step: 6530, train/epoch: 1.5540218353271484
Step: 6540, train/loss: 0.0
Step: 6540, train/grad_norm: 8.048034487728728e-07
Step: 6540, train/learning_rate: 4.221799099468626e-05
Step: 6540, train/epoch: 1.5564017295837402
Step: 6550, train/loss: 0.0
Step: 6550, train/grad_norm: 2.7270818918623263e-06
Step: 6550, train/learning_rate: 4.2206091166008264e-05
Step: 6550, train/epoch: 1.5587815046310425
Step: 6560, train/loss: 0.0
Step: 6560, train/grad_norm: 5.637900812871521e-07
Step: 6560, train/learning_rate: 4.2194194975309074e-05
Step: 6560, train/epoch: 1.5611613988876343
Step: 6570, train/loss: 0.0
Step: 6570, train/grad_norm: 2.620883606141433e-06
Step: 6570, train/learning_rate: 4.218229514663108e-05
Step: 6570, train/epoch: 1.5635411739349365
Step: 6580, train/loss: 0.0
Step: 6580, train/grad_norm: 3.5253372061561095e-06
Step: 6580, train/learning_rate: 4.217039531795308e-05
Step: 6580, train/epoch: 1.5659209489822388
Step: 6590, train/loss: 0.0003000000142492354
Step: 6590, train/grad_norm: 1.6590945961070247e-05
Step: 6590, train/learning_rate: 4.215849548927508e-05
Step: 6590, train/epoch: 1.5683008432388306
Step: 6600, train/loss: 0.13670000433921814
Step: 6600, train/grad_norm: 4.332517346483655e-05
Step: 6600, train/learning_rate: 4.2146595660597086e-05
Step: 6600, train/epoch: 1.5706806182861328
Step: 6610, train/loss: 0.2281000018119812
Step: 6610, train/grad_norm: 46.91257858276367
Step: 6610, train/learning_rate: 4.2134699469897896e-05
Step: 6610, train/epoch: 1.573060393333435
Step: 6620, train/loss: 0.0
Step: 6620, train/grad_norm: 0.0024225928355008364
Step: 6620, train/learning_rate: 4.21227996412199e-05
Step: 6620, train/epoch: 1.5754402875900269
Step: 6630, train/loss: 0.00019999999494757503
Step: 6630, train/grad_norm: 1.8745149077403767e-07
Step: 6630, train/learning_rate: 4.21108998125419e-05
Step: 6630, train/epoch: 1.577820062637329
Step: 6640, train/loss: 0.0020000000949949026
Step: 6640, train/grad_norm: 0.00011691827967297286
Step: 6640, train/learning_rate: 4.2098999983863905e-05
Step: 6640, train/epoch: 1.580199956893921
Step: 6650, train/loss: 0.0003000000142492354
Step: 6650, train/grad_norm: 2.9293714121081393e-08
Step: 6650, train/learning_rate: 4.208710015518591e-05
Step: 6650, train/epoch: 1.5825797319412231
Step: 6660, train/loss: 9.999999747378752e-05
Step: 6660, train/grad_norm: 0.32541513442993164
Step: 6660, train/learning_rate: 4.207520396448672e-05
Step: 6660, train/epoch: 1.5849595069885254
Step: 6670, train/loss: 0.0003000000142492354
Step: 6670, train/grad_norm: 9.52737627812894e-06
Step: 6670, train/learning_rate: 4.206330413580872e-05
Step: 6670, train/epoch: 1.5873394012451172
Step: 6680, train/loss: 0.0
Step: 6680, train/grad_norm: 2.65378253061499e-06
Step: 6680, train/learning_rate: 4.2051404307130724e-05
Step: 6680, train/epoch: 1.5897191762924194
Step: 6690, train/loss: 0.12460000067949295
Step: 6690, train/grad_norm: 5.703808710677549e-06
Step: 6690, train/learning_rate: 4.203950447845273e-05
Step: 6690, train/epoch: 1.5920989513397217
Step: 6700, train/loss: 0.1890999972820282
Step: 6700, train/grad_norm: 2.5404408461326966e-06
Step: 6700, train/learning_rate: 4.202760464977473e-05
Step: 6700, train/epoch: 1.5944788455963135
Step: 6710, train/loss: 0.0
Step: 6710, train/grad_norm: 0.0757448822259903
Step: 6710, train/learning_rate: 4.201570845907554e-05
Step: 6710, train/epoch: 1.5968586206436157
Step: 6720, train/loss: 0.009800000116229057
Step: 6720, train/grad_norm: 13.411046028137207
Step: 6720, train/learning_rate: 4.200380863039754e-05
Step: 6720, train/epoch: 1.5992385149002075
Step: 6730, train/loss: 0.0
Step: 6730, train/grad_norm: 7.278897101059556e-05
Step: 6730, train/learning_rate: 4.1991908801719546e-05
Step: 6730, train/epoch: 1.6016182899475098
Step: 6740, train/loss: 0.10939999669790268
Step: 6740, train/grad_norm: 150.33494567871094
Step: 6740, train/learning_rate: 4.198000897304155e-05
Step: 6740, train/epoch: 1.603998064994812
Step: 6750, train/loss: 0.18930000066757202
Step: 6750, train/grad_norm: 4.336225538281724e-05
Step: 6750, train/learning_rate: 4.196810914436355e-05
Step: 6750, train/epoch: 1.6063779592514038
Step: 6760, train/loss: 0.03189999982714653
Step: 6760, train/grad_norm: 8.155749128491152e-06
Step: 6760, train/learning_rate: 4.195621295366436e-05
Step: 6760, train/epoch: 1.608757734298706
Step: 6770, train/loss: 0.02070000022649765
Step: 6770, train/grad_norm: 0.16347672045230865
Step: 6770, train/learning_rate: 4.1944313124986365e-05
Step: 6770, train/epoch: 1.6111375093460083
Step: 6780, train/loss: 0.0
Step: 6780, train/grad_norm: 6.850797035440337e-06
Step: 6780, train/learning_rate: 4.193241329630837e-05
Step: 6780, train/epoch: 1.6135174036026
Step: 6790, train/loss: 0.0
Step: 6790, train/grad_norm: 2.8434476917027496e-05
Step: 6790, train/learning_rate: 4.192051346763037e-05
Step: 6790, train/epoch: 1.6158971786499023
Step: 6800, train/loss: 0.0
Step: 6800, train/grad_norm: 3.5069122077402426e-06
Step: 6800, train/learning_rate: 4.1908613638952374e-05
Step: 6800, train/epoch: 1.6182769536972046
Step: 6810, train/loss: 0.13050000369548798
Step: 6810, train/grad_norm: 1.3388419574766885e-05
Step: 6810, train/learning_rate: 4.1896717448253185e-05
Step: 6810, train/epoch: 1.6206568479537964
Step: 6820, train/loss: 0.0
Step: 6820, train/grad_norm: 0.0003562411875464022
Step: 6820, train/learning_rate: 4.188481761957519e-05
Step: 6820, train/epoch: 1.6230366230010986
Step: 6830, train/loss: 9.999999747378752e-05
Step: 6830, train/grad_norm: 0.0002286387316416949
Step: 6830, train/learning_rate: 4.187291779089719e-05
Step: 6830, train/epoch: 1.6254165172576904
Step: 6840, train/loss: 0.0
Step: 6840, train/grad_norm: 0.0007002182537689805
Step: 6840, train/learning_rate: 4.1861017962219194e-05
Step: 6840, train/epoch: 1.6277962923049927
Step: 6850, train/loss: 0.0
Step: 6850, train/grad_norm: 0.0001597895607119426
Step: 6850, train/learning_rate: 4.1849118133541197e-05
Step: 6850, train/epoch: 1.630176067352295
Step: 6860, train/loss: 9.999999747378752e-05
Step: 6860, train/grad_norm: 0.00023348060494754463
Step: 6860, train/learning_rate: 4.183722194284201e-05
Step: 6860, train/epoch: 1.6325559616088867
Step: 6870, train/loss: 0.0
Step: 6870, train/grad_norm: 0.000971857865806669
Step: 6870, train/learning_rate: 4.182532211416401e-05
Step: 6870, train/epoch: 1.634935736656189
Step: 6880, train/loss: 0.05660000070929527
Step: 6880, train/grad_norm: 19.287029266357422
Step: 6880, train/learning_rate: 4.181342228548601e-05
Step: 6880, train/epoch: 1.6373155117034912
Step: 6890, train/loss: 0.002199999988079071
Step: 6890, train/grad_norm: 0.00031700331601314247
Step: 6890, train/learning_rate: 4.1801522456808016e-05
Step: 6890, train/epoch: 1.639695405960083
Step: 6900, train/loss: 0.0006000000284984708
Step: 6900, train/grad_norm: 9.69887878454756e-07
Step: 6900, train/learning_rate: 4.178962262813002e-05
Step: 6900, train/epoch: 1.6420751810073853
Step: 6910, train/loss: 0.0
Step: 6910, train/grad_norm: 0.0049485936760902405
Step: 6910, train/learning_rate: 4.177772643743083e-05
Step: 6910, train/epoch: 1.644455075263977
Step: 6920, train/loss: 9.999999747378752e-05
Step: 6920, train/grad_norm: 0.02305072546005249
Step: 6920, train/learning_rate: 4.176582660875283e-05
Step: 6920, train/epoch: 1.6468348503112793
Step: 6930, train/loss: 0.02879999950528145
Step: 6930, train/grad_norm: 7.315690163522959e-05
Step: 6930, train/learning_rate: 4.1753926780074835e-05
Step: 6930, train/epoch: 1.6492146253585815
Step: 6940, train/loss: 0.0
Step: 6940, train/grad_norm: 0.01940363645553589
Step: 6940, train/learning_rate: 4.174202695139684e-05
Step: 6940, train/epoch: 1.6515945196151733
Step: 6950, train/loss: 9.999999747378752e-05
Step: 6950, train/grad_norm: 1.3876334378437605e-05
Step: 6950, train/learning_rate: 4.173012712271884e-05
Step: 6950, train/epoch: 1.6539742946624756
Step: 6960, train/loss: 9.999999747378752e-05
Step: 6960, train/grad_norm: 0.0012119674356654286
Step: 6960, train/learning_rate: 4.171823093201965e-05
Step: 6960, train/epoch: 1.6563540697097778
Step: 6970, train/loss: 0.15530000627040863
Step: 6970, train/grad_norm: 5.0046222895616665e-05
Step: 6970, train/learning_rate: 4.1706331103341654e-05
Step: 6970, train/epoch: 1.6587339639663696
Step: 6980, train/loss: 0.0
Step: 6980, train/grad_norm: 4.436762083059875e-06
Step: 6980, train/learning_rate: 4.169443127466366e-05
Step: 6980, train/epoch: 1.6611137390136719
Step: 6990, train/loss: 0.0
Step: 6990, train/grad_norm: 0.00021244454546831548
Step: 6990, train/learning_rate: 4.168253144598566e-05
Step: 6990, train/epoch: 1.6634936332702637
Step: 7000, train/loss: 0.0
Step: 7000, train/grad_norm: 1.2039336979796644e-05
Step: 7000, train/learning_rate: 4.167063161730766e-05
Step: 7000, train/epoch: 1.665873408317566
Step: 7010, train/loss: 0.0
Step: 7010, train/grad_norm: 1.0105195542564616e-05
Step: 7010, train/learning_rate: 4.165873542660847e-05
Step: 7010, train/epoch: 1.6682531833648682
Step: 7020, train/loss: 0.1754000037908554
Step: 7020, train/grad_norm: 3.661989831016399e-05
Step: 7020, train/learning_rate: 4.1646835597930476e-05
Step: 7020, train/epoch: 1.67063307762146
Step: 7030, train/loss: 0.0
Step: 7030, train/grad_norm: 5.3265302994987e-06
Step: 7030, train/learning_rate: 4.163493576925248e-05
Step: 7030, train/epoch: 1.6730128526687622
Step: 7040, train/loss: 0.02280000038444996
Step: 7040, train/grad_norm: 8.597889973316342e-06
Step: 7040, train/learning_rate: 4.162303594057448e-05
Step: 7040, train/epoch: 1.6753926277160645
Step: 7050, train/loss: 0.0
Step: 7050, train/grad_norm: 0.00010104077955475077
Step: 7050, train/learning_rate: 4.1611136111896485e-05
Step: 7050, train/epoch: 1.6777725219726562
Step: 7060, train/loss: 0.0
Step: 7060, train/grad_norm: 1.9691033230628818e-05
Step: 7060, train/learning_rate: 4.1599239921197295e-05
Step: 7060, train/epoch: 1.6801522970199585
Step: 7070, train/loss: 9.999999747378752e-05
Step: 7070, train/grad_norm: 0.09204041212797165
Step: 7070, train/learning_rate: 4.15873400925193e-05
Step: 7070, train/epoch: 1.6825320720672607
Step: 7080, train/loss: 0.0
Step: 7080, train/grad_norm: 0.0002245972864329815
Step: 7080, train/learning_rate: 4.15754402638413e-05
Step: 7080, train/epoch: 1.6849119663238525
Step: 7090, train/loss: 9.999999747378752e-05
Step: 7090, train/grad_norm: 1.191776118503185e-05
Step: 7090, train/learning_rate: 4.1563540435163304e-05
Step: 7090, train/epoch: 1.6872917413711548
Step: 7100, train/loss: 9.999999747378752e-05
Step: 7100, train/grad_norm: 1.375346073473338e-05
Step: 7100, train/learning_rate: 4.155164060648531e-05
Step: 7100, train/epoch: 1.6896716356277466
Step: 7110, train/loss: 0.0
Step: 7110, train/grad_norm: 0.0002403531107120216
Step: 7110, train/learning_rate: 4.153974441578612e-05
Step: 7110, train/epoch: 1.6920514106750488
Step: 7120, train/loss: 0.0026000000070780516
Step: 7120, train/grad_norm: 0.006214158609509468
Step: 7120, train/learning_rate: 4.152784458710812e-05
Step: 7120, train/epoch: 1.694431185722351
Step: 7130, train/loss: 0.03840000182390213
Step: 7130, train/grad_norm: 0.005537417251616716
Step: 7130, train/learning_rate: 4.151594475843012e-05
Step: 7130, train/epoch: 1.6968110799789429
Step: 7140, train/loss: 0.0
Step: 7140, train/grad_norm: 0.0001558718940941617
Step: 7140, train/learning_rate: 4.1504044929752126e-05
Step: 7140, train/epoch: 1.6991908550262451
Step: 7150, train/loss: 0.0
Step: 7150, train/grad_norm: 0.002344234613701701
Step: 7150, train/learning_rate: 4.149214510107413e-05
Step: 7150, train/epoch: 1.7015706300735474
Step: 7160, train/loss: 0.03310000151395798
Step: 7160, train/grad_norm: 44.754722595214844
Step: 7160, train/learning_rate: 4.148024891037494e-05
Step: 7160, train/epoch: 1.7039505243301392
Step: 7170, train/loss: 0.0031999999191612005
Step: 7170, train/grad_norm: 1.5835967133170925e-05
Step: 7170, train/learning_rate: 4.146834908169694e-05
Step: 7170, train/epoch: 1.7063302993774414
Step: 7180, train/loss: 0.0
Step: 7180, train/grad_norm: 1.1187496056663804e-05
Step: 7180, train/learning_rate: 4.1456449253018945e-05
Step: 7180, train/epoch: 1.7087101936340332
Step: 7190, train/loss: 0.0005000000237487257
Step: 7190, train/grad_norm: 0.0034633700270205736
Step: 7190, train/learning_rate: 4.144454942434095e-05
Step: 7190, train/epoch: 1.7110899686813354
Step: 7200, train/loss: 0.03370000049471855
Step: 7200, train/grad_norm: 0.00041902021621353924
Step: 7200, train/learning_rate: 4.143264959566295e-05
Step: 7200, train/epoch: 1.7134697437286377
Step: 7210, train/loss: 0.06669999659061432
Step: 7210, train/grad_norm: 25.283191680908203
Step: 7210, train/learning_rate: 4.142075340496376e-05
Step: 7210, train/epoch: 1.7158496379852295
Step: 7220, train/loss: 9.999999747378752e-05
Step: 7220, train/grad_norm: 0.08735894411802292
Step: 7220, train/learning_rate: 4.1408853576285765e-05
Step: 7220, train/epoch: 1.7182294130325317
Step: 7230, train/loss: 0.0
Step: 7230, train/grad_norm: 0.0002566108014434576
Step: 7230, train/learning_rate: 4.139695374760777e-05
Step: 7230, train/epoch: 1.720609188079834
Step: 7240, train/loss: 0.0003000000142492354
Step: 7240, train/grad_norm: 1.6071920754257008e-06
Step: 7240, train/learning_rate: 4.138505391892977e-05
Step: 7240, train/epoch: 1.7229890823364258
Step: 7250, train/loss: 0.000699999975040555
Step: 7250, train/grad_norm: 8.348599294549786e-06
Step: 7250, train/learning_rate: 4.1373154090251774e-05
Step: 7250, train/epoch: 1.725368857383728
Step: 7260, train/loss: 0.0
Step: 7260, train/grad_norm: 2.7771528039011173e-05
Step: 7260, train/learning_rate: 4.1361257899552584e-05
Step: 7260, train/epoch: 1.7277486324310303
Step: 7270, train/loss: 0.0005000000237487257
Step: 7270, train/grad_norm: 0.0006509074592031538
Step: 7270, train/learning_rate: 4.134935807087459e-05
Step: 7270, train/epoch: 1.730128526687622
Step: 7280, train/loss: 0.1023000031709671
Step: 7280, train/grad_norm: 24.947078704833984
Step: 7280, train/learning_rate: 4.133745824219659e-05
Step: 7280, train/epoch: 1.7325083017349243
Step: 7290, train/loss: 0.0
Step: 7290, train/grad_norm: 0.00017197678971569985
Step: 7290, train/learning_rate: 4.132555841351859e-05
Step: 7290, train/epoch: 1.7348881959915161
Step: 7300, train/loss: 0.0
Step: 7300, train/grad_norm: 6.811544153606519e-05
Step: 7300, train/learning_rate: 4.1313658584840596e-05
Step: 7300, train/epoch: 1.7372679710388184
Step: 7310, train/loss: 0.05979999899864197
Step: 7310, train/grad_norm: 1.2054858871124452e-06
Step: 7310, train/learning_rate: 4.1301762394141406e-05
Step: 7310, train/epoch: 1.7396477460861206
Step: 7320, train/loss: 0.17110000550746918
Step: 7320, train/grad_norm: 3.890676816808991e-05
Step: 7320, train/learning_rate: 4.128986256546341e-05
Step: 7320, train/epoch: 1.7420276403427124
Step: 7330, train/loss: 0.02930000051856041
Step: 7330, train/grad_norm: 0.0003442732850089669
Step: 7330, train/learning_rate: 4.127796273678541e-05
Step: 7330, train/epoch: 1.7444074153900146
Step: 7340, train/loss: 9.999999747378752e-05
Step: 7340, train/grad_norm: 0.000756988360080868
Step: 7340, train/learning_rate: 4.1266062908107415e-05
Step: 7340, train/epoch: 1.746787190437317
Step: 7350, train/loss: 0.00019999999494757503
Step: 7350, train/grad_norm: 0.0014210755471140146
Step: 7350, train/learning_rate: 4.125416307942942e-05
Step: 7350, train/epoch: 1.7491670846939087
Step: 7360, train/loss: 9.999999747378752e-05
Step: 7360, train/grad_norm: 0.0005802216473966837
Step: 7360, train/learning_rate: 4.124226688873023e-05
Step: 7360, train/epoch: 1.751546859741211
Step: 7370, train/loss: 0.0
Step: 7370, train/grad_norm: 6.813624349888414e-05
Step: 7370, train/learning_rate: 4.123036706005223e-05
Step: 7370, train/epoch: 1.7539267539978027
Step: 7380, train/loss: 0.0
Step: 7380, train/grad_norm: 0.0004932607989758253
Step: 7380, train/learning_rate: 4.1218467231374234e-05
Step: 7380, train/epoch: 1.756306529045105
Step: 7390, train/loss: 0.05270000174641609
Step: 7390, train/grad_norm: 0.00025786037440411747
Step: 7390, train/learning_rate: 4.120656740269624e-05
Step: 7390, train/epoch: 1.7586863040924072
Step: 7400, train/loss: 9.999999747378752e-05
Step: 7400, train/grad_norm: 0.06269090622663498
Step: 7400, train/learning_rate: 4.119466757401824e-05
Step: 7400, train/epoch: 1.761066198348999
Step: 7410, train/loss: 0.0
Step: 7410, train/grad_norm: 0.00011965086014242843
Step: 7410, train/learning_rate: 4.118277138331905e-05
Step: 7410, train/epoch: 1.7634459733963013
Step: 7420, train/loss: 0.0
Step: 7420, train/grad_norm: 6.6002621679217555e-06
Step: 7420, train/learning_rate: 4.117087155464105e-05
Step: 7420, train/epoch: 1.7658257484436035
Step: 7430, train/loss: 0.000699999975040555
Step: 7430, train/grad_norm: 8.443604747299105e-05
Step: 7430, train/learning_rate: 4.1158971725963056e-05
Step: 7430, train/epoch: 1.7682056427001953
Step: 7440, train/loss: 0.0
Step: 7440, train/grad_norm: 2.579628926469013e-05
Step: 7440, train/learning_rate: 4.114707189728506e-05
Step: 7440, train/epoch: 1.7705854177474976
Step: 7450, train/loss: 0.018400000408291817
Step: 7450, train/grad_norm: 1.4533192370436154e-05
Step: 7450, train/learning_rate: 4.113517206860706e-05
Step: 7450, train/epoch: 1.7729653120040894
Step: 7460, train/loss: 0.0
Step: 7460, train/grad_norm: 3.667656346806325e-05
Step: 7460, train/learning_rate: 4.112327587790787e-05
Step: 7460, train/epoch: 1.7753450870513916
Step: 7470, train/loss: 0.0
Step: 7470, train/grad_norm: 8.960258128354326e-06
Step: 7470, train/learning_rate: 4.1111376049229875e-05
Step: 7470, train/epoch: 1.7777248620986938
Step: 7480, train/loss: 0.07460000365972519
Step: 7480, train/grad_norm: 0.0002837172069121152
Step: 7480, train/learning_rate: 4.109947622055188e-05
Step: 7480, train/epoch: 1.7801047563552856
Step: 7490, train/loss: 9.999999747378752e-05
Step: 7490, train/grad_norm: 0.002024137880653143
Step: 7490, train/learning_rate: 4.108757639187388e-05
Step: 7490, train/epoch: 1.782484531402588
Step: 7500, train/loss: 0.0
Step: 7500, train/grad_norm: 0.0003448904608376324
Step: 7500, train/learning_rate: 4.1075676563195884e-05
Step: 7500, train/epoch: 1.7848643064498901
Step: 7510, train/loss: 0.06599999964237213
Step: 7510, train/grad_norm: 0.0020351209677755833
Step: 7510, train/learning_rate: 4.1063780372496694e-05
Step: 7510, train/epoch: 1.787244200706482
Step: 7520, train/loss: 0.045499999076128006
Step: 7520, train/grad_norm: 0.017918575555086136
Step: 7520, train/learning_rate: 4.10518805438187e-05
Step: 7520, train/epoch: 1.7896239757537842
Step: 7530, train/loss: 0.0
Step: 7530, train/grad_norm: 0.0018501650774851441
Step: 7530, train/learning_rate: 4.10399807151407e-05
Step: 7530, train/epoch: 1.7920037508010864
Step: 7540, train/loss: 0.016100000590085983
Step: 7540, train/grad_norm: 8.171884360308468e-07
Step: 7540, train/learning_rate: 4.10280808864627e-05
Step: 7540, train/epoch: 1.7943836450576782
Step: 7550, train/loss: 0.03099999949336052
Step: 7550, train/grad_norm: 4.860809326171875
Step: 7550, train/learning_rate: 4.1016181057784706e-05
Step: 7550, train/epoch: 1.7967634201049805
Step: 7560, train/loss: 0.11569999903440475
Step: 7560, train/grad_norm: 6.346122063405346e-06
Step: 7560, train/learning_rate: 4.1004284867085516e-05
Step: 7560, train/epoch: 1.7991433143615723
Step: 7570, train/loss: 0.02810000069439411
Step: 7570, train/grad_norm: 0.0008395621553063393
Step: 7570, train/learning_rate: 4.099238503840752e-05
Step: 7570, train/epoch: 1.8015230894088745
Step: 7580, train/loss: 0.0008999999845400453
Step: 7580, train/grad_norm: 3.2004780223360285e-05
Step: 7580, train/learning_rate: 4.098048520972952e-05
Step: 7580, train/epoch: 1.8039028644561768
Step: 7590, train/loss: 0.00019999999494757503
Step: 7590, train/grad_norm: 3.743115030374611e-06
Step: 7590, train/learning_rate: 4.0968585381051525e-05
Step: 7590, train/epoch: 1.8062827587127686
Step: 7600, train/loss: 0.016699999570846558
Step: 7600, train/grad_norm: 0.006553466431796551
Step: 7600, train/learning_rate: 4.095668555237353e-05
Step: 7600, train/epoch: 1.8086625337600708
Step: 7610, train/loss: 0.0
Step: 7610, train/grad_norm: 6.833345196355367e-07
Step: 7610, train/learning_rate: 4.094478936167434e-05
Step: 7610, train/epoch: 1.811042308807373
Step: 7620, train/loss: 0.0
Step: 7620, train/grad_norm: 0.002725438680499792
Step: 7620, train/learning_rate: 4.093288953299634e-05
Step: 7620, train/epoch: 1.8134222030639648
Step: 7630, train/loss: 9.999999747378752e-05
Step: 7630, train/grad_norm: 7.772342542011756e-06
Step: 7630, train/learning_rate: 4.0920989704318345e-05
Step: 7630, train/epoch: 1.815801978111267
Step: 7640, train/loss: 0.0
Step: 7640, train/grad_norm: 0.0003311981854494661
Step: 7640, train/learning_rate: 4.090908987564035e-05
Step: 7640, train/epoch: 1.8181818723678589
Step: 7650, train/loss: 0.00019999999494757503
Step: 7650, train/grad_norm: 0.8453859090805054
Step: 7650, train/learning_rate: 4.089719004696235e-05
Step: 7650, train/epoch: 1.8205616474151611
Step: 7660, train/loss: 0.0005000000237487257
Step: 7660, train/grad_norm: 2.6999660462934116e-07
Step: 7660, train/learning_rate: 4.088529385626316e-05
Step: 7660, train/epoch: 1.8229414224624634
Step: 7670, train/loss: 0.0
Step: 7670, train/grad_norm: 8.133677511068527e-06
Step: 7670, train/learning_rate: 4.0873394027585164e-05
Step: 7670, train/epoch: 1.8253213167190552
Step: 7680, train/loss: 0.0
Step: 7680, train/grad_norm: 7.992631481101853e-07
Step: 7680, train/learning_rate: 4.086149419890717e-05
Step: 7680, train/epoch: 1.8277010917663574
Step: 7690, train/loss: 9.999999747378752e-05
Step: 7690, train/grad_norm: 1.5279645595001057e-05
Step: 7690, train/learning_rate: 4.084959437022917e-05
Step: 7690, train/epoch: 1.8300808668136597
Step: 7700, train/loss: 0.05429999902844429
Step: 7700, train/grad_norm: 7.103987445589155e-05
Step: 7700, train/learning_rate: 4.083769454155117e-05
Step: 7700, train/epoch: 1.8324607610702515
Step: 7710, train/loss: 0.0
Step: 7710, train/grad_norm: 0.0006619410123676062
Step: 7710, train/learning_rate: 4.082579835085198e-05
Step: 7710, train/epoch: 1.8348405361175537
Step: 7720, train/loss: 0.14149999618530273
Step: 7720, train/grad_norm: 0.0019759428687393665
Step: 7720, train/learning_rate: 4.0813898522173986e-05
Step: 7720, train/epoch: 1.8372204303741455
Step: 7730, train/loss: 0.0
Step: 7730, train/grad_norm: 0.00024530565133318305
Step: 7730, train/learning_rate: 4.080199869349599e-05
Step: 7730, train/epoch: 1.8396002054214478
Step: 7740, train/loss: 0.0
Step: 7740, train/grad_norm: 1.4803445083089173e-05
Step: 7740, train/learning_rate: 4.079009886481799e-05
Step: 7740, train/epoch: 1.84197998046875
Step: 7750, train/loss: 0.07069999724626541
Step: 7750, train/grad_norm: 138.08480834960938
Step: 7750, train/learning_rate: 4.07782026741188e-05
Step: 7750, train/epoch: 1.8443598747253418
Step: 7760, train/loss: 0.00039999998989515007
Step: 7760, train/grad_norm: 0.00017497639055363834
Step: 7760, train/learning_rate: 4.0766302845440805e-05
Step: 7760, train/epoch: 1.846739649772644
Step: 7770, train/loss: 0.0
Step: 7770, train/grad_norm: 7.555496267741546e-06
Step: 7770, train/learning_rate: 4.075440301676281e-05
Step: 7770, train/epoch: 1.8491194248199463
Step: 7780, train/loss: 0.0
Step: 7780, train/grad_norm: 1.615758810658008e-05
Step: 7780, train/learning_rate: 4.074250318808481e-05
Step: 7780, train/epoch: 1.851499319076538
Step: 7790, train/loss: 0.003100000089034438
Step: 7790, train/grad_norm: 4.854510916629806e-05
Step: 7790, train/learning_rate: 4.0730603359406814e-05
Step: 7790, train/epoch: 1.8538790941238403
Step: 7800, train/loss: 0.00019999999494757503
Step: 7800, train/grad_norm: 1.0556236505508423
Step: 7800, train/learning_rate: 4.0718707168707624e-05
Step: 7800, train/epoch: 1.8562588691711426
Step: 7810, train/loss: 0.02810000069439411
Step: 7810, train/grad_norm: 66.70140838623047
Step: 7810, train/learning_rate: 4.070680734002963e-05
Step: 7810, train/epoch: 1.8586387634277344
Step: 7820, train/loss: 0.020500000566244125
Step: 7820, train/grad_norm: 2.5123674731730716e-06
Step: 7820, train/learning_rate: 4.069490751135163e-05
Step: 7820, train/epoch: 1.8610185384750366
Step: 7830, train/loss: 0.0
Step: 7830, train/grad_norm: 1.2085715184184664e-07
Step: 7830, train/learning_rate: 4.068300768267363e-05
Step: 7830, train/epoch: 1.8633984327316284
Step: 7840, train/loss: 0.0
Step: 7840, train/grad_norm: 7.004003350630228e-07
Step: 7840, train/learning_rate: 4.0671107853995636e-05
Step: 7840, train/epoch: 1.8657782077789307
Step: 7850, train/loss: 0.0
Step: 7850, train/grad_norm: 1.8507223842334497e-07
Step: 7850, train/learning_rate: 4.0659211663296446e-05
Step: 7850, train/epoch: 1.868157982826233
Step: 7860, train/loss: 0.0
Step: 7860, train/grad_norm: 6.832130594602859e-08
Step: 7860, train/learning_rate: 4.064731183461845e-05
Step: 7860, train/epoch: 1.8705378770828247
Step: 7870, train/loss: 0.0005000000237487257
Step: 7870, train/grad_norm: 6.395320269803051e-06
Step: 7870, train/learning_rate: 4.063541200594045e-05
Step: 7870, train/epoch: 1.872917652130127
Step: 7880, train/loss: 9.999999747378752e-05
Step: 7880, train/grad_norm: 6.657526796516322e-08
Step: 7880, train/learning_rate: 4.0623512177262455e-05
Step: 7880, train/epoch: 1.8752974271774292
Step: 7890, train/loss: 0.09769999980926514
Step: 7890, train/grad_norm: 9.095596760744229e-05
Step: 7890, train/learning_rate: 4.061161234858446e-05
Step: 7890, train/epoch: 1.877677321434021
Step: 7900, train/loss: 0.0
Step: 7900, train/grad_norm: 3.1843619581195526e-06
Step: 7900, train/learning_rate: 4.059971615788527e-05
Step: 7900, train/epoch: 1.8800570964813232
Step: 7910, train/loss: 0.0
Step: 7910, train/grad_norm: 4.569188968162052e-05
Step: 7910, train/learning_rate: 4.058781632920727e-05
Step: 7910, train/epoch: 1.882436990737915
Step: 7920, train/loss: 0.0
Step: 7920, train/grad_norm: 7.297431636743568e-08
Step: 7920, train/learning_rate: 4.0575916500529274e-05
Step: 7920, train/epoch: 1.8848167657852173
Step: 7930, train/loss: 0.0
Step: 7930, train/grad_norm: 5.644913107971661e-05
Step: 7930, train/learning_rate: 4.056401667185128e-05
Step: 7930, train/epoch: 1.8871965408325195
Step: 7940, train/loss: 0.0
Step: 7940, train/grad_norm: 0.007274020928889513
Step: 7940, train/learning_rate: 4.055211684317328e-05
Step: 7940, train/epoch: 1.8895764350891113
Step: 7950, train/loss: 0.0
Step: 7950, train/grad_norm: 1.0663609828043263e-05
Step: 7950, train/learning_rate: 4.054022065247409e-05
Step: 7950, train/epoch: 1.8919562101364136
Step: 7960, train/loss: 0.0
Step: 7960, train/grad_norm: 0.0005671844119206071
Step: 7960, train/learning_rate: 4.0528320823796093e-05
Step: 7960, train/epoch: 1.8943359851837158
Step: 7970, train/loss: 0.0
Step: 7970, train/grad_norm: 6.600274105039716e-08
Step: 7970, train/learning_rate: 4.0516420995118096e-05
Step: 7970, train/epoch: 1.8967158794403076
Step: 7980, train/loss: 0.0
Step: 7980, train/grad_norm: 1.7281463371432437e-08
Step: 7980, train/learning_rate: 4.05045211664401e-05
Step: 7980, train/epoch: 1.8990956544876099
Step: 7990, train/loss: 0.0
Step: 7990, train/grad_norm: 0.01836218684911728
Step: 7990, train/learning_rate: 4.04926213377621e-05
Step: 7990, train/epoch: 1.901475429534912
Step: 8000, train/loss: 0.0
Step: 8000, train/grad_norm: 6.490446846640907e-09
Step: 8000, train/learning_rate: 4.048072514706291e-05
Step: 8000, train/epoch: 1.903855323791504
Step: 8010, train/loss: 0.03629999980330467
Step: 8010, train/grad_norm: 0.00023259723093360662
Step: 8010, train/learning_rate: 4.0468825318384916e-05
Step: 8010, train/epoch: 1.9062350988388062
Step: 8020, train/loss: 0.14059999585151672
Step: 8020, train/grad_norm: 0.00034568627597764134
Step: 8020, train/learning_rate: 4.045692548970692e-05
Step: 8020, train/epoch: 1.908614993095398
Step: 8030, train/loss: 0.014700000174343586
Step: 8030, train/grad_norm: 9.859427336778026e-06
Step: 8030, train/learning_rate: 4.044502566102892e-05
Step: 8030, train/epoch: 1.9109947681427002
Step: 8040, train/loss: 0.0
Step: 8040, train/grad_norm: 0.11123297363519669
Step: 8040, train/learning_rate: 4.0433125832350925e-05
Step: 8040, train/epoch: 1.9133745431900024
Step: 8050, train/loss: 0.07699999958276749
Step: 8050, train/grad_norm: 0.0004074277530889958
Step: 8050, train/learning_rate: 4.0421229641651735e-05
Step: 8050, train/epoch: 1.9157544374465942
Step: 8060, train/loss: 0.007899999618530273
Step: 8060, train/grad_norm: 0.00931596104055643
Step: 8060, train/learning_rate: 4.040932981297374e-05
Step: 8060, train/epoch: 1.9181342124938965
Step: 8070, train/loss: 0.1177000030875206
Step: 8070, train/grad_norm: 0.003976206760853529
Step: 8070, train/learning_rate: 4.039742998429574e-05
Step: 8070, train/epoch: 1.9205139875411987
Step: 8080, train/loss: 0.00019999999494757503
Step: 8080, train/grad_norm: 2.174217843275983e-05
Step: 8080, train/learning_rate: 4.0385530155617744e-05
Step: 8080, train/epoch: 1.9228938817977905
Step: 8090, train/loss: 0.0
Step: 8090, train/grad_norm: 0.000359017780283466
Step: 8090, train/learning_rate: 4.037363032693975e-05
Step: 8090, train/epoch: 1.9252736568450928
Step: 8100, train/loss: 0.0007999999797903001
Step: 8100, train/grad_norm: 8.40860195694404e-07
Step: 8100, train/learning_rate: 4.036173413624056e-05
Step: 8100, train/epoch: 1.9276535511016846
Step: 8110, train/loss: 0.0
Step: 8110, train/grad_norm: 2.991021347042988e-06
Step: 8110, train/learning_rate: 4.034983430756256e-05
Step: 8110, train/epoch: 1.9300333261489868
Step: 8120, train/loss: 0.002199999988079071
Step: 8120, train/grad_norm: 3.1181359645415796e-06
Step: 8120, train/learning_rate: 4.033793447888456e-05
Step: 8120, train/epoch: 1.932413101196289
Step: 8130, train/loss: 9.999999747378752e-05
Step: 8130, train/grad_norm: 6.305431725195376e-06
Step: 8130, train/learning_rate: 4.0326034650206566e-05
Step: 8130, train/epoch: 1.9347929954528809
Step: 8140, train/loss: 0.00139999995008111
Step: 8140, train/grad_norm: 0.001881830277852714
Step: 8140, train/learning_rate: 4.031413482152857e-05
Step: 8140, train/epoch: 1.937172770500183
Step: 8150, train/loss: 0.10000000149011612
Step: 8150, train/grad_norm: 6.479757530541974e-07
Step: 8150, train/learning_rate: 4.030223863082938e-05
Step: 8150, train/epoch: 1.9395525455474854
Step: 8160, train/loss: 9.999999747378752e-05
Step: 8160, train/grad_norm: 1.6304113614751259e-09
Step: 8160, train/learning_rate: 4.029033880215138e-05
Step: 8160, train/epoch: 1.9419324398040771
Step: 8170, train/loss: 0.0
Step: 8170, train/grad_norm: 1.9843692911081234e-08
Step: 8170, train/learning_rate: 4.0278438973473385e-05
Step: 8170, train/epoch: 1.9443122148513794
Step: 8180, train/loss: 0.0003000000142492354
Step: 8180, train/grad_norm: 1.1173358871019445e-05
Step: 8180, train/learning_rate: 4.026653914479539e-05
Step: 8180, train/epoch: 1.9466921091079712
Step: 8190, train/loss: 0.00019999999494757503
Step: 8190, train/grad_norm: 1.1598175660765264e-05
Step: 8190, train/learning_rate: 4.025463931611739e-05
Step: 8190, train/epoch: 1.9490718841552734
Step: 8200, train/loss: 0.0
Step: 8200, train/grad_norm: 6.33448166809103e-07
Step: 8200, train/learning_rate: 4.02427431254182e-05
Step: 8200, train/epoch: 1.9514516592025757
Step: 8210, train/loss: 0.05310000106692314
Step: 8210, train/grad_norm: 0.0001199312464450486
Step: 8210, train/learning_rate: 4.0230843296740204e-05
Step: 8210, train/epoch: 1.9538315534591675
Step: 8220, train/loss: 0.018200000748038292
Step: 8220, train/grad_norm: 0.0005598420975729823
Step: 8220, train/learning_rate: 4.021894346806221e-05
Step: 8220, train/epoch: 1.9562113285064697
Step: 8230, train/loss: 0.27399998903274536
Step: 8230, train/grad_norm: 159.70382690429688
Step: 8230, train/learning_rate: 4.020704363938421e-05
Step: 8230, train/epoch: 1.958591103553772
Step: 8240, train/loss: 0.0
Step: 8240, train/grad_norm: 5.334663910616655e-07
Step: 8240, train/learning_rate: 4.019514381070621e-05
Step: 8240, train/epoch: 1.9609709978103638
Step: 8250, train/loss: 0.0
Step: 8250, train/grad_norm: 1.7148446204373613e-05
Step: 8250, train/learning_rate: 4.018324762000702e-05
Step: 8250, train/epoch: 1.963350772857666
Step: 8260, train/loss: 0.21719999611377716
Step: 8260, train/grad_norm: 66.83351135253906
Step: 8260, train/learning_rate: 4.0171347791329026e-05
Step: 8260, train/epoch: 1.9657305479049683
Step: 8270, train/loss: 0.1242000013589859
Step: 8270, train/grad_norm: 0.00036533083766698837
Step: 8270, train/learning_rate: 4.015944796265103e-05
Step: 8270, train/epoch: 1.96811044216156
Step: 8280, train/loss: 0.0
Step: 8280, train/grad_norm: 1.1555785931705032e-05
Step: 8280, train/learning_rate: 4.014754813397303e-05
Step: 8280, train/epoch: 1.9704902172088623
Step: 8290, train/loss: 0.00039999998989515007
Step: 8290, train/grad_norm: 0.00014709599781781435
Step: 8290, train/learning_rate: 4.0135648305295035e-05
Step: 8290, train/epoch: 1.972870111465454
Step: 8300, train/loss: 0.0
Step: 8300, train/grad_norm: 6.3599627537769265e-06
Step: 8300, train/learning_rate: 4.0123752114595845e-05
Step: 8300, train/epoch: 1.9752498865127563
Step: 8310, train/loss: 0.0017000000225380063
Step: 8310, train/grad_norm: 7.0432220127258915e-06
Step: 8310, train/learning_rate: 4.011185228591785e-05
Step: 8310, train/epoch: 1.9776296615600586
Step: 8320, train/loss: 0.0
Step: 8320, train/grad_norm: 3.966282383771613e-05
Step: 8320, train/learning_rate: 4.009995245723985e-05
Step: 8320, train/epoch: 1.9800095558166504
Step: 8330, train/loss: 0.0
Step: 8330, train/grad_norm: 1.0653342314981273e-06
Step: 8330, train/learning_rate: 4.0088052628561854e-05
Step: 8330, train/epoch: 1.9823893308639526
Step: 8340, train/loss: 0.0
Step: 8340, train/grad_norm: 1.0182617415921413e-06
Step: 8340, train/learning_rate: 4.007615279988386e-05
Step: 8340, train/epoch: 1.9847691059112549
Step: 8350, train/loss: 0.0
Step: 8350, train/grad_norm: 0.0006826739991083741
Step: 8350, train/learning_rate: 4.006425660918467e-05
Step: 8350, train/epoch: 1.9871490001678467
Step: 8360, train/loss: 0.0
Step: 8360, train/grad_norm: 1.0860832844628021e-05
Step: 8360, train/learning_rate: 4.005235678050667e-05
Step: 8360, train/epoch: 1.989528775215149
Step: 8370, train/loss: 0.0
Step: 8370, train/grad_norm: 1.6658403183100745e-05
Step: 8370, train/learning_rate: 4.0040456951828673e-05
Step: 8370, train/epoch: 1.9919086694717407
Step: 8380, train/loss: 0.0
Step: 8380, train/grad_norm: 0.0001680513087194413
Step: 8380, train/learning_rate: 4.0028557123150676e-05
Step: 8380, train/epoch: 1.994288444519043
Step: 8390, train/loss: 0.0
Step: 8390, train/grad_norm: 3.0781350801589724e-07
Step: 8390, train/learning_rate: 4.001665729447268e-05
Step: 8390, train/epoch: 1.9966682195663452
Step: 8400, train/loss: 0.0
Step: 8400, train/grad_norm: 1.8102558897226118e-05
Step: 8400, train/learning_rate: 4.000476110377349e-05
Step: 8400, train/epoch: 1.999048113822937
Step: 8404, eval/loss: 0.04409746825695038
Step: 8404, eval/accuracy: 0.9938914179801941
Step: 8404, eval/f1: 0.9935474991798401
Step: 8404, eval/runtime: 707.8258056640625
Step: 8404, eval/samples_per_second: 10.175999641418457
Step: 8404, eval/steps_per_second: 1.2730000019073486
Step: 8404, train/epoch: 2.0
Step: 8410, train/loss: 0.0
Step: 8410, train/grad_norm: 0.0002426881983410567
Step: 8410, train/learning_rate: 3.999286127509549e-05
Step: 8410, train/epoch: 2.0014278888702393
Step: 8420, train/loss: 0.0
Step: 8420, train/grad_norm: 0.006602549459785223
Step: 8420, train/learning_rate: 3.9980961446417496e-05
Step: 8420, train/epoch: 2.003807783126831
Step: 8430, train/loss: 0.0
Step: 8430, train/grad_norm: 0.0002904575376305729
Step: 8430, train/learning_rate: 3.99690616177395e-05
Step: 8430, train/epoch: 2.0061874389648438
Step: 8440, train/loss: 0.0
Step: 8440, train/grad_norm: 7.3669281846378e-05
Step: 8440, train/learning_rate: 3.99571617890615e-05
Step: 8440, train/epoch: 2.0085673332214355
Step: 8450, train/loss: 0.0
Step: 8450, train/grad_norm: 1.9664437786559574e-05
Step: 8450, train/learning_rate: 3.994526559836231e-05
Step: 8450, train/epoch: 2.0109472274780273
Step: 8460, train/loss: 0.125
Step: 8460, train/grad_norm: 0.0008195063564926386
Step: 8460, train/learning_rate: 3.9933365769684315e-05
Step: 8460, train/epoch: 2.01332688331604
Step: 8470, train/loss: 0.015200000256299973
Step: 8470, train/grad_norm: 1.1577113582461607e-05
Step: 8470, train/learning_rate: 3.992146594100632e-05
Step: 8470, train/epoch: 2.015706777572632
Step: 8480, train/loss: 0.0
Step: 8480, train/grad_norm: 0.01244703121483326
Step: 8480, train/learning_rate: 3.990956611232832e-05
Step: 8480, train/epoch: 2.0180866718292236
Step: 8490, train/loss: 9.999999747378752e-05
Step: 8490, train/grad_norm: 0.03637487441301346
Step: 8490, train/learning_rate: 3.9897666283650324e-05
Step: 8490, train/epoch: 2.0204663276672363
Step: 8500, train/loss: 0.0
Step: 8500, train/grad_norm: 1.5680141984830698e-07
Step: 8500, train/learning_rate: 3.9885770092951134e-05
Step: 8500, train/epoch: 2.022846221923828
Step: 8510, train/loss: 0.0
Step: 8510, train/grad_norm: 0.0004399858880788088
Step: 8510, train/learning_rate: 3.987387026427314e-05
Step: 8510, train/epoch: 2.02522611618042
Step: 8520, train/loss: 0.0
Step: 8520, train/grad_norm: 2.8699903964479745e-07
Step: 8520, train/learning_rate: 3.986197043559514e-05
Step: 8520, train/epoch: 2.0276060104370117
Step: 8530, train/loss: 0.0
Step: 8530, train/grad_norm: 6.98949588695541e-05
Step: 8530, train/learning_rate: 3.985007060691714e-05
Step: 8530, train/epoch: 2.0299856662750244
Step: 8540, train/loss: 0.0
Step: 8540, train/grad_norm: 5.808848072774708e-05
Step: 8540, train/learning_rate: 3.9838170778239146e-05
Step: 8540, train/epoch: 2.032365560531616
Step: 8550, train/loss: 0.0
Step: 8550, train/grad_norm: 0.0011294473661109805
Step: 8550, train/learning_rate: 3.9826274587539956e-05
Step: 8550, train/epoch: 2.034745454788208
Step: 8560, train/loss: 0.0
Step: 8560, train/grad_norm: 7.440816261805594e-05
Step: 8560, train/learning_rate: 3.981437475886196e-05
Step: 8560, train/epoch: 2.0371251106262207
Step: 8570, train/loss: 0.0
Step: 8570, train/grad_norm: 0.000530513992998749
Step: 8570, train/learning_rate: 3.980247493018396e-05
Step: 8570, train/epoch: 2.0395050048828125
Step: 8580, train/loss: 0.0020000000949949026
Step: 8580, train/grad_norm: 0.001277900650165975
Step: 8580, train/learning_rate: 3.9790575101505965e-05
Step: 8580, train/epoch: 2.0418848991394043
Step: 8590, train/loss: 0.0
Step: 8590, train/grad_norm: 0.0010044401278719306
Step: 8590, train/learning_rate: 3.977867527282797e-05
Step: 8590, train/epoch: 2.044264554977417
Step: 8600, train/loss: 0.0
Step: 8600, train/grad_norm: 6.2686417550139595e-06
Step: 8600, train/learning_rate: 3.976677908212878e-05
Step: 8600, train/epoch: 2.046644449234009
Step: 8610, train/loss: 0.16019999980926514
Step: 8610, train/grad_norm: 0.0018093204125761986
Step: 8610, train/learning_rate: 3.975487925345078e-05
Step: 8610, train/epoch: 2.0490243434906006
Step: 8620, train/loss: 0.0
Step: 8620, train/grad_norm: 1.3294261407281738e-05
Step: 8620, train/learning_rate: 3.9742979424772784e-05
Step: 8620, train/epoch: 2.0514039993286133
Step: 8630, train/loss: 0.0
Step: 8630, train/grad_norm: 0.0017125892918556929
Step: 8630, train/learning_rate: 3.973107959609479e-05
Step: 8630, train/epoch: 2.053783893585205
Step: 8640, train/loss: 0.0
Step: 8640, train/grad_norm: 5.3088194817974e-07
Step: 8640, train/learning_rate: 3.971917976741679e-05
Step: 8640, train/epoch: 2.056163787841797
Step: 8650, train/loss: 0.0
Step: 8650, train/grad_norm: 0.04756217822432518
Step: 8650, train/learning_rate: 3.97072835767176e-05
Step: 8650, train/epoch: 2.0585434436798096
Step: 8660, train/loss: 0.0
Step: 8660, train/grad_norm: 5.065193363407161e-06
Step: 8660, train/learning_rate: 3.96953837480396e-05
Step: 8660, train/epoch: 2.0609233379364014
Step: 8670, train/loss: 9.999999747378752e-05
Step: 8670, train/grad_norm: 2.854757804016117e-05
Step: 8670, train/learning_rate: 3.9683483919361606e-05
Step: 8670, train/epoch: 2.063303232192993
Step: 8680, train/loss: 0.0
Step: 8680, train/grad_norm: 0.00031798877171240747
Step: 8680, train/learning_rate: 3.967158409068361e-05
Step: 8680, train/epoch: 2.065683126449585
Step: 8690, train/loss: 0.0
Step: 8690, train/grad_norm: 0.008329208008944988
Step: 8690, train/learning_rate: 3.965968426200561e-05
Step: 8690, train/epoch: 2.0680627822875977
Step: 8700, train/loss: 0.0
Step: 8700, train/grad_norm: 1.824277023843024e-05
Step: 8700, train/learning_rate: 3.964778807130642e-05
Step: 8700, train/epoch: 2.0704426765441895
Step: 8710, train/loss: 9.999999747378752e-05
Step: 8710, train/grad_norm: 6.076749741623644e-06
Step: 8710, train/learning_rate: 3.9635888242628425e-05
Step: 8710, train/epoch: 2.0728225708007812
Step: 8720, train/loss: 0.0
Step: 8720, train/grad_norm: 5.131301350047579e-06
Step: 8720, train/learning_rate: 3.962398841395043e-05
Step: 8720, train/epoch: 2.075202226638794
Step: 8730, train/loss: 0.0
Step: 8730, train/grad_norm: 4.242834165779641e-06
Step: 8730, train/learning_rate: 3.961208858527243e-05
Step: 8730, train/epoch: 2.0775821208953857
Step: 8740, train/loss: 0.0
Step: 8740, train/grad_norm: 1.7302774722338654e-05
Step: 8740, train/learning_rate: 3.9600188756594434e-05
Step: 8740, train/epoch: 2.0799620151519775
Step: 8750, train/loss: 9.999999747378752e-05
Step: 8750, train/grad_norm: 3.376859604031779e-05
Step: 8750, train/learning_rate: 3.9588292565895244e-05
Step: 8750, train/epoch: 2.0823416709899902
Step: 8760, train/loss: 0.00039999998989515007
Step: 8760, train/grad_norm: 3.9245266914367676
Step: 8760, train/learning_rate: 3.957639273721725e-05
Step: 8760, train/epoch: 2.084721565246582
Step: 8770, train/loss: 0.0
Step: 8770, train/grad_norm: 3.7756431083835196e-06
Step: 8770, train/learning_rate: 3.956449290853925e-05
Step: 8770, train/epoch: 2.087101459503174
Step: 8780, train/loss: 0.0
Step: 8780, train/grad_norm: 0.0002744566008914262
Step: 8780, train/learning_rate: 3.9552593079861253e-05
Step: 8780, train/epoch: 2.0894811153411865
Step: 8790, train/loss: 0.0
Step: 8790, train/grad_norm: 1.4734512205905048e-06
Step: 8790, train/learning_rate: 3.9540693251183257e-05
Step: 8790, train/epoch: 2.0918610095977783
Step: 8800, train/loss: 0.0
Step: 8800, train/grad_norm: 4.501636112763663e-07
Step: 8800, train/learning_rate: 3.9528797060484067e-05
Step: 8800, train/epoch: 2.09424090385437
Step: 8810, train/loss: 0.0
Step: 8810, train/grad_norm: 2.313358208994032e-06
Step: 8810, train/learning_rate: 3.951689723180607e-05
Step: 8810, train/epoch: 2.096620559692383
Step: 8820, train/loss: 0.0
Step: 8820, train/grad_norm: 3.803944991886965e-06
Step: 8820, train/learning_rate: 3.950499740312807e-05
Step: 8820, train/epoch: 2.0990004539489746
Step: 8830, train/loss: 0.0
Step: 8830, train/grad_norm: 2.929938546003541e-06
Step: 8830, train/learning_rate: 3.9493097574450076e-05
Step: 8830, train/epoch: 2.1013803482055664
Step: 8840, train/loss: 0.0
Step: 8840, train/grad_norm: 1.5602617509102856e-07
Step: 8840, train/learning_rate: 3.948119774577208e-05
Step: 8840, train/epoch: 2.103760004043579
Step: 8850, train/loss: 0.0
Step: 8850, train/grad_norm: 3.0191761197784217e-06
Step: 8850, train/learning_rate: 3.946930155507289e-05
Step: 8850, train/epoch: 2.106139898300171
Step: 8860, train/loss: 0.0
Step: 8860, train/grad_norm: 6.012447192915715e-05
Step: 8860, train/learning_rate: 3.945740172639489e-05
Step: 8860, train/epoch: 2.1085197925567627
Step: 8870, train/loss: 0.0006000000284984708
Step: 8870, train/grad_norm: 0.00016253680223599076
Step: 8870, train/learning_rate: 3.9445501897716895e-05
Step: 8870, train/epoch: 2.1108996868133545
Step: 8880, train/loss: 0.0
Step: 8880, train/grad_norm: 1.8404325174969927e-08
Step: 8880, train/learning_rate: 3.94336020690389e-05
Step: 8880, train/epoch: 2.113279342651367
Step: 8890, train/loss: 0.0
Step: 8890, train/grad_norm: 1.2610611221930412e-08
Step: 8890, train/learning_rate: 3.94217022403609e-05
Step: 8890, train/epoch: 2.115659236907959
Step: 8900, train/loss: 0.0
Step: 8900, train/grad_norm: 4.8755930038169026e-05
Step: 8900, train/learning_rate: 3.940980604966171e-05
Step: 8900, train/epoch: 2.118039131164551
Step: 8910, train/loss: 0.0005000000237487257
Step: 8910, train/grad_norm: 8.122779036057182e-07
Step: 8910, train/learning_rate: 3.9397906220983714e-05
Step: 8910, train/epoch: 2.1204187870025635
Step: 8920, train/loss: 0.0
Step: 8920, train/grad_norm: 0.00015995523426681757
Step: 8920, train/learning_rate: 3.938600639230572e-05
Step: 8920, train/epoch: 2.1227986812591553
Step: 8930, train/loss: 0.0
Step: 8930, train/grad_norm: 4.605799404089339e-07
Step: 8930, train/learning_rate: 3.937410656362772e-05
Step: 8930, train/epoch: 2.125178575515747
Step: 8940, train/loss: 9.999999747378752e-05
Step: 8940, train/grad_norm: 6.26654752977629e-07
Step: 8940, train/learning_rate: 3.936220673494972e-05
Step: 8940, train/epoch: 2.1275582313537598
Step: 8950, train/loss: 0.0
Step: 8950, train/grad_norm: 0.0003713366750162095
Step: 8950, train/learning_rate: 3.935031054425053e-05
Step: 8950, train/epoch: 2.1299381256103516
Step: 8960, train/loss: 0.0
Step: 8960, train/grad_norm: 3.6422148696146905e-05
Step: 8960, train/learning_rate: 3.9338410715572536e-05
Step: 8960, train/epoch: 2.1323180198669434
Step: 8970, train/loss: 0.0
Step: 8970, train/grad_norm: 2.2188187642768753e-07
Step: 8970, train/learning_rate: 3.932651088689454e-05
Step: 8970, train/epoch: 2.134697675704956
Step: 8980, train/loss: 0.00419999985024333
Step: 8980, train/grad_norm: 1.9223212177621463e-07
Step: 8980, train/learning_rate: 3.931461105821654e-05
Step: 8980, train/epoch: 2.137077569961548
Step: 8990, train/loss: 0.0
Step: 8990, train/grad_norm: 9.256095978571466e-08
Step: 8990, train/learning_rate: 3.9302711229538545e-05
Step: 8990, train/epoch: 2.1394574642181396
Step: 9000, train/loss: 0.0
Step: 9000, train/grad_norm: 2.853078740372439e-06
Step: 9000, train/learning_rate: 3.9290815038839355e-05
Step: 9000, train/epoch: 2.1418371200561523
Step: 9010, train/loss: 0.015399999916553497
Step: 9010, train/grad_norm: 8.230115327023668e-07
Step: 9010, train/learning_rate: 3.927891521016136e-05
Step: 9010, train/epoch: 2.144217014312744
Step: 9020, train/loss: 0.0
Step: 9020, train/grad_norm: 1.3257976050340403e-08
Step: 9020, train/learning_rate: 3.926701538148336e-05
Step: 9020, train/epoch: 2.146596908569336
Step: 9030, train/loss: 0.0
Step: 9030, train/grad_norm: 2.6359273306297837e-06
Step: 9030, train/learning_rate: 3.9255115552805364e-05
Step: 9030, train/epoch: 2.1489765644073486
Step: 9040, train/loss: 0.0
Step: 9040, train/grad_norm: 0.0006249112775549293
Step: 9040, train/learning_rate: 3.924321572412737e-05
Step: 9040, train/epoch: 2.1513564586639404
Step: 9050, train/loss: 0.0
Step: 9050, train/grad_norm: 3.3777027965697926e-06
Step: 9050, train/learning_rate: 3.923131953342818e-05
Step: 9050, train/epoch: 2.1537363529205322
Step: 9060, train/loss: 0.0
Step: 9060, train/grad_norm: 9.015706382342614e-06
Step: 9060, train/learning_rate: 3.921941970475018e-05
Step: 9060, train/epoch: 2.156116247177124
Step: 9070, train/loss: 0.0
Step: 9070, train/grad_norm: 7.733611084859149e-08
Step: 9070, train/learning_rate: 3.920751987607218e-05
Step: 9070, train/epoch: 2.1584959030151367
Step: 9080, train/loss: 0.0
Step: 9080, train/grad_norm: 2.9748321139777545e-08
Step: 9080, train/learning_rate: 3.9195620047394186e-05
Step: 9080, train/epoch: 2.1608757972717285
Step: 9090, train/loss: 0.0
Step: 9090, train/grad_norm: 1.951561171154026e-05
Step: 9090, train/learning_rate: 3.918372021871619e-05
Step: 9090, train/epoch: 2.1632556915283203
Step: 9100, train/loss: 0.0
Step: 9100, train/grad_norm: 5.5990079999901354e-05
Step: 9100, train/learning_rate: 3.9171824028017e-05
Step: 9100, train/epoch: 2.165635347366333
Step: 9110, train/loss: 0.0
Step: 9110, train/grad_norm: 6.490910891443491e-06
Step: 9110, train/learning_rate: 3.9159924199339e-05
Step: 9110, train/epoch: 2.168015241622925
Step: 9120, train/loss: 0.0
Step: 9120, train/grad_norm: 1.982368848985061e-05
Step: 9120, train/learning_rate: 3.9148024370661005e-05
Step: 9120, train/epoch: 2.1703951358795166
Step: 9130, train/loss: 0.0
Step: 9130, train/grad_norm: 2.385978177699144e-06
Step: 9130, train/learning_rate: 3.913612454198301e-05
Step: 9130, train/epoch: 2.1727747917175293
Step: 9140, train/loss: 0.0
Step: 9140, train/grad_norm: 5.470155883813277e-07
Step: 9140, train/learning_rate: 3.912422835128382e-05
Step: 9140, train/epoch: 2.175154685974121
Step: 9150, train/loss: 0.0
Step: 9150, train/grad_norm: 3.133374093522434e-06
Step: 9150, train/learning_rate: 3.911232852260582e-05
Step: 9150, train/epoch: 2.177534580230713
Step: 9160, train/loss: 0.0003000000142492354
Step: 9160, train/grad_norm: 4.718276613857597e-05
Step: 9160, train/learning_rate: 3.9100428693927824e-05
Step: 9160, train/epoch: 2.1799142360687256
Step: 9170, train/loss: 9.999999747378752e-05
Step: 9170, train/grad_norm: 5.5592358876310755e-06
Step: 9170, train/learning_rate: 3.908852886524983e-05
Step: 9170, train/epoch: 2.1822941303253174
Step: 9180, train/loss: 0.0
Step: 9180, train/grad_norm: 1.1179328794241883e-05
Step: 9180, train/learning_rate: 3.907662903657183e-05
Step: 9180, train/epoch: 2.184674024581909
Step: 9190, train/loss: 0.002099999925121665
Step: 9190, train/grad_norm: 4.3136372566223145
Step: 9190, train/learning_rate: 3.906473284587264e-05
Step: 9190, train/epoch: 2.187053680419922
Step: 9200, train/loss: 0.0
Step: 9200, train/grad_norm: 5.941587222224598e-09
Step: 9200, train/learning_rate: 3.9052833017194644e-05
Step: 9200, train/epoch: 2.1894335746765137
Step: 9210, train/loss: 0.0
Step: 9210, train/grad_norm: 2.4830134748299315e-07
Step: 9210, train/learning_rate: 3.904093318851665e-05
Step: 9210, train/epoch: 2.1918134689331055
Step: 9220, train/loss: 0.0
Step: 9220, train/grad_norm: 9.407383316784035e-08
Step: 9220, train/learning_rate: 3.902903335983865e-05
Step: 9220, train/epoch: 2.194193124771118
Step: 9230, train/loss: 0.0
Step: 9230, train/grad_norm: 2.1822684459493757e-07
Step: 9230, train/learning_rate: 3.901713353116065e-05
Step: 9230, train/epoch: 2.19657301902771
Step: 9240, train/loss: 0.0
Step: 9240, train/grad_norm: 2.8673689485003706e-06
Step: 9240, train/learning_rate: 3.900523734046146e-05
Step: 9240, train/epoch: 2.1989529132843018
Step: 9250, train/loss: 0.0
Step: 9250, train/grad_norm: 1.1777856023797995e-08
Step: 9250, train/learning_rate: 3.8993337511783466e-05
Step: 9250, train/epoch: 2.2013328075408936
Step: 9260, train/loss: 0.045899998396635056
Step: 9260, train/grad_norm: 1.990765241544068e-07
Step: 9260, train/learning_rate: 3.898143768310547e-05
Step: 9260, train/epoch: 2.2037124633789062
Step: 9270, train/loss: 0.0
Step: 9270, train/grad_norm: 4.4416827854609764e-11
Step: 9270, train/learning_rate: 3.896953785442747e-05
Step: 9270, train/epoch: 2.206092357635498
Step: 9280, train/loss: 0.0
Step: 9280, train/grad_norm: 1.560999218774839e-09
Step: 9280, train/learning_rate: 3.8957638025749475e-05
Step: 9280, train/epoch: 2.20847225189209
Step: 9290, train/loss: 0.0
Step: 9290, train/grad_norm: 1.3881870097876003e-09
Step: 9290, train/learning_rate: 3.8945741835050285e-05
Step: 9290, train/epoch: 2.2108519077301025
Step: 9300, train/loss: 0.0
Step: 9300, train/grad_norm: 9.256932997914191e-09
Step: 9300, train/learning_rate: 3.893384200637229e-05
Step: 9300, train/epoch: 2.2132318019866943
Step: 9310, train/loss: 0.008299999870359898
Step: 9310, train/grad_norm: 7.738610463547957e-08
Step: 9310, train/learning_rate: 3.892194217769429e-05
Step: 9310, train/epoch: 2.215611696243286
Step: 9320, train/loss: 0.0
Step: 9320, train/grad_norm: 4.0561806691208346e-10
Step: 9320, train/learning_rate: 3.8910042349016294e-05
Step: 9320, train/epoch: 2.217991352081299
Step: 9330, train/loss: 0.0
Step: 9330, train/grad_norm: 1.6558827919954666e-10
Step: 9330, train/learning_rate: 3.88981425203383e-05
Step: 9330, train/epoch: 2.2203712463378906
Step: 9340, train/loss: 0.0
Step: 9340, train/grad_norm: 5.334443358151475e-07
Step: 9340, train/learning_rate: 3.888624632963911e-05
Step: 9340, train/epoch: 2.2227511405944824
Step: 9350, train/loss: 0.0
Step: 9350, train/grad_norm: 2.203554032575994e-07
Step: 9350, train/learning_rate: 3.887434650096111e-05
Step: 9350, train/epoch: 2.225130796432495
Step: 9360, train/loss: 0.0
Step: 9360, train/grad_norm: 1.4454973884525657e-09
Step: 9360, train/learning_rate: 3.886244667228311e-05
Step: 9360, train/epoch: 2.227510690689087
Step: 9370, train/loss: 0.0
Step: 9370, train/grad_norm: 3.409612503446624e-08
Step: 9370, train/learning_rate: 3.8850546843605116e-05
Step: 9370, train/epoch: 2.2298905849456787
Step: 9380, train/loss: 0.0
Step: 9380, train/grad_norm: 2.068995854642708e-06
Step: 9380, train/learning_rate: 3.883864701492712e-05
Step: 9380, train/epoch: 2.2322702407836914
Step: 9390, train/loss: 0.0
Step: 9390, train/grad_norm: 1.753223699552109e-08
Step: 9390, train/learning_rate: 3.882675082422793e-05
Step: 9390, train/epoch: 2.234650135040283
Step: 9400, train/loss: 0.0
Step: 9400, train/grad_norm: 2.0413438051036792e-07
Step: 9400, train/learning_rate: 3.881485099554993e-05
Step: 9400, train/epoch: 2.237030029296875
Step: 9410, train/loss: 0.0
Step: 9410, train/grad_norm: 9.23104881422887e-09
Step: 9410, train/learning_rate: 3.8802951166871935e-05
Step: 9410, train/epoch: 2.239409923553467
Step: 9420, train/loss: 0.0
Step: 9420, train/grad_norm: 2.1444433784267858e-08
Step: 9420, train/learning_rate: 3.879105133819394e-05
Step: 9420, train/epoch: 2.2417895793914795
Step: 9430, train/loss: 0.0
Step: 9430, train/grad_norm: 4.218839055880608e-09
Step: 9430, train/learning_rate: 3.877915150951594e-05
Step: 9430, train/epoch: 2.2441694736480713
Step: 9440, train/loss: 0.0
Step: 9440, train/grad_norm: 0.016498159617185593
Step: 9440, train/learning_rate: 3.876725531881675e-05
Step: 9440, train/epoch: 2.246549367904663
Step: 9450, train/loss: 0.16949999332427979
Step: 9450, train/grad_norm: 3.1342151487478986e-05
Step: 9450, train/learning_rate: 3.8755355490138754e-05
Step: 9450, train/epoch: 2.248929023742676
Step: 9460, train/loss: 0.0
Step: 9460, train/grad_norm: 2.085472461388349e-09
Step: 9460, train/learning_rate: 3.874345566146076e-05
Step: 9460, train/epoch: 2.2513089179992676
Step: 9470, train/loss: 0.0
Step: 9470, train/grad_norm: 5.717190560972085e-07
Step: 9470, train/learning_rate: 3.873155583278276e-05
Step: 9470, train/epoch: 2.2536888122558594
Step: 9480, train/loss: 0.0
Step: 9480, train/grad_norm: 2.5491553969914094e-05
Step: 9480, train/learning_rate: 3.871965600410476e-05
Step: 9480, train/epoch: 2.256068468093872
Step: 9490, train/loss: 0.0
Step: 9490, train/grad_norm: 1.1288149437405082e-07
Step: 9490, train/learning_rate: 3.870775981340557e-05
Step: 9490, train/epoch: 2.258448362350464
Step: 9500, train/loss: 0.0
Step: 9500, train/grad_norm: 4.511735429701957e-09
Step: 9500, train/learning_rate: 3.8695859984727576e-05
Step: 9500, train/epoch: 2.2608282566070557
Step: 9510, train/loss: 0.0
Step: 9510, train/grad_norm: 1.5995002655699864e-08
Step: 9510, train/learning_rate: 3.868396015604958e-05
Step: 9510, train/epoch: 2.2632079124450684
Step: 9520, train/loss: 0.0
Step: 9520, train/grad_norm: 8.757423586303048e-08
Step: 9520, train/learning_rate: 3.867206032737158e-05
Step: 9520, train/epoch: 2.26558780670166
Step: 9530, train/loss: 0.0
Step: 9530, train/grad_norm: 4.8932456309103145e-08
Step: 9530, train/learning_rate: 3.8660160498693585e-05
Step: 9530, train/epoch: 2.267967700958252
Step: 9540, train/loss: 0.0
Step: 9540, train/grad_norm: 8.710823173885274e-09
Step: 9540, train/learning_rate: 3.8648264307994395e-05
Step: 9540, train/epoch: 2.2703473567962646
Step: 9550, train/loss: 0.0
Step: 9550, train/grad_norm: 2.3461508291688915e-08
Step: 9550, train/learning_rate: 3.86363644793164e-05
Step: 9550, train/epoch: 2.2727272510528564
Step: 9560, train/loss: 0.0
Step: 9560, train/grad_norm: 0.0004172374028712511
Step: 9560, train/learning_rate: 3.86244646506384e-05
Step: 9560, train/epoch: 2.2751071453094482
Step: 9570, train/loss: 0.0
Step: 9570, train/grad_norm: 5.939237439633871e-07
Step: 9570, train/learning_rate: 3.8612564821960405e-05
Step: 9570, train/epoch: 2.277486801147461
Step: 9580, train/loss: 0.0
Step: 9580, train/grad_norm: 1.637568129808642e-05
Step: 9580, train/learning_rate: 3.860066499328241e-05
Step: 9580, train/epoch: 2.2798666954040527
Step: 9590, train/loss: 0.0
Step: 9590, train/grad_norm: 3.447449614668585e-07
Step: 9590, train/learning_rate: 3.858876880258322e-05
Step: 9590, train/epoch: 2.2822465896606445
Step: 9600, train/loss: 0.0
Step: 9600, train/grad_norm: 3.053335433378379e-07
Step: 9600, train/learning_rate: 3.857686897390522e-05
Step: 9600, train/epoch: 2.2846264839172363
Step: 9610, train/loss: 0.0
Step: 9610, train/grad_norm: 7.478012520323318e-08
Step: 9610, train/learning_rate: 3.8564969145227224e-05
Step: 9610, train/epoch: 2.287006139755249
Step: 9620, train/loss: 0.06719999760389328
Step: 9620, train/grad_norm: 119.37503051757812
Step: 9620, train/learning_rate: 3.855306931654923e-05
Step: 9620, train/epoch: 2.289386034011841
Step: 9630, train/loss: 0.0
Step: 9630, train/grad_norm: 1.2837801932619186e-07
Step: 9630, train/learning_rate: 3.854116948787123e-05
Step: 9630, train/epoch: 2.2917659282684326
Step: 9640, train/loss: 0.01720000058412552
Step: 9640, train/grad_norm: 9.502057764620986e-06
Step: 9640, train/learning_rate: 3.852927329717204e-05
Step: 9640, train/epoch: 2.2941455841064453
Step: 9650, train/loss: 0.07270000129938126
Step: 9650, train/grad_norm: 6.516504669207279e-09
Step: 9650, train/learning_rate: 3.851737346849404e-05
Step: 9650, train/epoch: 2.296525478363037
Step: 9660, train/loss: 0.0
Step: 9660, train/grad_norm: 0.13380900025367737
Step: 9660, train/learning_rate: 3.8505473639816046e-05
Step: 9660, train/epoch: 2.298905372619629
Step: 9670, train/loss: 0.0019000000320374966
Step: 9670, train/grad_norm: 2.5901738531786123e-09
Step: 9670, train/learning_rate: 3.849357381113805e-05
Step: 9670, train/epoch: 2.3012850284576416
Step: 9680, train/loss: 0.0
Step: 9680, train/grad_norm: 3.136473125664452e-08
Step: 9680, train/learning_rate: 3.848167398246005e-05
Step: 9680, train/epoch: 2.3036649227142334
Step: 9690, train/loss: 0.0
Step: 9690, train/grad_norm: 0.2274821549654007
Step: 9690, train/learning_rate: 3.846977779176086e-05
Step: 9690, train/epoch: 2.306044816970825
Step: 9700, train/loss: 0.0
Step: 9700, train/grad_norm: 2.398835749772843e-05
Step: 9700, train/learning_rate: 3.8457877963082865e-05
Step: 9700, train/epoch: 2.308424472808838
Step: 9710, train/loss: 9.999999747378752e-05
Step: 9710, train/grad_norm: 6.553608766779462e-09
Step: 9710, train/learning_rate: 3.844597813440487e-05
Step: 9710, train/epoch: 2.3108043670654297
Step: 9720, train/loss: 0.0
Step: 9720, train/grad_norm: 1.9306831688936654e-08
Step: 9720, train/learning_rate: 3.843407830572687e-05
Step: 9720, train/epoch: 2.3131842613220215
Step: 9730, train/loss: 0.0
Step: 9730, train/grad_norm: 8.548140613129362e-07
Step: 9730, train/learning_rate: 3.8422178477048874e-05
Step: 9730, train/epoch: 2.315563917160034
Step: 9740, train/loss: 0.21879999339580536
Step: 9740, train/grad_norm: 1.497228741645813
Step: 9740, train/learning_rate: 3.8410282286349684e-05
Step: 9740, train/epoch: 2.317943811416626
Step: 9750, train/loss: 0.0
Step: 9750, train/grad_norm: 1.4054548103104025e-07
Step: 9750, train/learning_rate: 3.839838245767169e-05
Step: 9750, train/epoch: 2.3203237056732178
Step: 9760, train/loss: 0.05079999938607216
Step: 9760, train/grad_norm: 1.2024956586031976e-09
Step: 9760, train/learning_rate: 3.838648262899369e-05
Step: 9760, train/epoch: 2.3227033615112305
Step: 9770, train/loss: 0.0
Step: 9770, train/grad_norm: 1.289451123653862e-08
Step: 9770, train/learning_rate: 3.837458280031569e-05
Step: 9770, train/epoch: 2.3250832557678223
Step: 9780, train/loss: 0.0
Step: 9780, train/grad_norm: 2.2176109837346303e-07
Step: 9780, train/learning_rate: 3.8362682971637696e-05
Step: 9780, train/epoch: 2.327463150024414
Step: 9790, train/loss: 0.0
Step: 9790, train/grad_norm: 0.0005541409482248127
Step: 9790, train/learning_rate: 3.8350786780938506e-05
Step: 9790, train/epoch: 2.329843044281006
Step: 9800, train/loss: 0.0
Step: 9800, train/grad_norm: 2.848543068090237e-10
Step: 9800, train/learning_rate: 3.833888695226051e-05
Step: 9800, train/epoch: 2.3322227001190186
Step: 9810, train/loss: 0.0
Step: 9810, train/grad_norm: 5.098397437564017e-08
Step: 9810, train/learning_rate: 3.832698712358251e-05
Step: 9810, train/epoch: 2.3346025943756104
Step: 9820, train/loss: 0.0
Step: 9820, train/grad_norm: 2.499616857676301e-05
Step: 9820, train/learning_rate: 3.8315087294904515e-05
Step: 9820, train/epoch: 2.336982488632202
Step: 9830, train/loss: 0.164900004863739
Step: 9830, train/grad_norm: 1.1298737945253379e-08
Step: 9830, train/learning_rate: 3.830318746622652e-05
Step: 9830, train/epoch: 2.339362144470215
Step: 9840, train/loss: 0.0
Step: 9840, train/grad_norm: 1.680440254858695e-05
Step: 9840, train/learning_rate: 3.829129127552733e-05
Step: 9840, train/epoch: 2.3417420387268066
Step: 9850, train/loss: 0.16500000655651093
Step: 9850, train/grad_norm: 1.0138727702724282e-05
Step: 9850, train/learning_rate: 3.827939144684933e-05
Step: 9850, train/epoch: 2.3441219329833984
Step: 9860, train/loss: 9.999999747378752e-05
Step: 9860, train/grad_norm: 0.3152519762516022
Step: 9860, train/learning_rate: 3.8267491618171334e-05
Step: 9860, train/epoch: 2.346501588821411
Step: 9870, train/loss: 0.0005000000237487257
Step: 9870, train/grad_norm: 1.0308732986450195
Step: 9870, train/learning_rate: 3.825559178949334e-05
Step: 9870, train/epoch: 2.348881483078003
Step: 9880, train/loss: 0.0
Step: 9880, train/grad_norm: 0.0012442318256944418
Step: 9880, train/learning_rate: 3.824369196081534e-05
Step: 9880, train/epoch: 2.3512613773345947
Step: 9890, train/loss: 0.00019999999494757503
Step: 9890, train/grad_norm: 0.4463268518447876
Step: 9890, train/learning_rate: 3.823179577011615e-05
Step: 9890, train/epoch: 2.3536410331726074
Step: 9900, train/loss: 0.0
Step: 9900, train/grad_norm: 3.1970964755601017e-06
Step: 9900, train/learning_rate: 3.821989594143815e-05
Step: 9900, train/epoch: 2.356020927429199
Step: 9910, train/loss: 0.0
Step: 9910, train/grad_norm: 9.438211634460458e-08
Step: 9910, train/learning_rate: 3.8207996112760156e-05
Step: 9910, train/epoch: 2.358400821685791
Step: 9920, train/loss: 0.0007999999797903001
Step: 9920, train/grad_norm: 1.3582254609900701e-07
Step: 9920, train/learning_rate: 3.819609628408216e-05
Step: 9920, train/epoch: 2.3607804775238037
Step: 9930, train/loss: 0.0
Step: 9930, train/grad_norm: 0.0009885051986202598
Step: 9930, train/learning_rate: 3.818419645540416e-05
Step: 9930, train/epoch: 2.3631603717803955
Step: 9940, train/loss: 9.999999747378752e-05
Step: 9940, train/grad_norm: 2.235250394733157e-05
Step: 9940, train/learning_rate: 3.817230026470497e-05
Step: 9940, train/epoch: 2.3655402660369873
Step: 9950, train/loss: 0.0
Step: 9950, train/grad_norm: 1.2958695450038249e-08
Step: 9950, train/learning_rate: 3.8160400436026976e-05
Step: 9950, train/epoch: 2.367919921875
Step: 9960, train/loss: 0.0
Step: 9960, train/grad_norm: 4.835094991051392e-09
Step: 9960, train/learning_rate: 3.814850060734898e-05
Step: 9960, train/epoch: 2.370299816131592
Step: 9970, train/loss: 0.0
Step: 9970, train/grad_norm: 4.743899495451842e-08
Step: 9970, train/learning_rate: 3.813660077867098e-05
Step: 9970, train/epoch: 2.3726797103881836
Step: 9980, train/loss: 0.0
Step: 9980, train/grad_norm: 4.880368109638766e-09
Step: 9980, train/learning_rate: 3.8124700949992985e-05
Step: 9980, train/epoch: 2.3750596046447754
Step: 9990, train/loss: 0.0
Step: 9990, train/grad_norm: 0.00011998226545983925
Step: 9990, train/learning_rate: 3.8112804759293795e-05
Step: 9990, train/epoch: 2.377439260482788
Step: 10000, train/loss: 0.0013000000035390258
Step: 10000, train/grad_norm: 4.126695785089396e-08
Step: 10000, train/learning_rate: 3.81009049306158e-05
Step: 10000, train/epoch: 2.37981915473938
Step: 10010, train/loss: 0.0
Step: 10010, train/grad_norm: 5.5930045164132025e-06
Step: 10010, train/learning_rate: 3.80890051019378e-05
Step: 10010, train/epoch: 2.3821990489959717
Step: 10020, train/loss: 0.0
Step: 10020, train/grad_norm: 1.175838271194607e-08
Step: 10020, train/learning_rate: 3.8077105273259804e-05
Step: 10020, train/epoch: 2.3845787048339844
Step: 10030, train/loss: 0.0
Step: 10030, train/grad_norm: 0.0006595965824089944
Step: 10030, train/learning_rate: 3.806520544458181e-05
Step: 10030, train/epoch: 2.386958599090576
Step: 10040, train/loss: 0.023000000044703484
Step: 10040, train/grad_norm: 84.8461685180664
Step: 10040, train/learning_rate: 3.805330925388262e-05
Step: 10040, train/epoch: 2.389338493347168
Step: 10050, train/loss: 0.0
Step: 10050, train/grad_norm: 1.3333004744708887e-07
Step: 10050, train/learning_rate: 3.804140942520462e-05
Step: 10050, train/epoch: 2.3917181491851807
Step: 10060, train/loss: 0.008500000461935997
Step: 10060, train/grad_norm: 5.1790666475426406e-05
Step: 10060, train/learning_rate: 3.802950959652662e-05
Step: 10060, train/epoch: 2.3940980434417725
Step: 10070, train/loss: 0.029500000178813934
Step: 10070, train/grad_norm: 2.633123585837893e-06
Step: 10070, train/learning_rate: 3.8017609767848626e-05
Step: 10070, train/epoch: 2.3964779376983643
Step: 10080, train/loss: 0.0
Step: 10080, train/grad_norm: 2.4382837082725928e-08
Step: 10080, train/learning_rate: 3.800570993917063e-05
Step: 10080, train/epoch: 2.398857593536377
Step: 10090, train/loss: 0.0
Step: 10090, train/grad_norm: 0.0008025963907130063
Step: 10090, train/learning_rate: 3.799381374847144e-05
Step: 10090, train/epoch: 2.4012374877929688
Step: 10100, train/loss: 0.0
Step: 10100, train/grad_norm: 8.446467290923465e-06
Step: 10100, train/learning_rate: 3.798191391979344e-05
Step: 10100, train/epoch: 2.4036173820495605
Step: 10110, train/loss: 0.0
Step: 10110, train/grad_norm: 1.94679500964412e-07
Step: 10110, train/learning_rate: 3.7970014091115445e-05
Step: 10110, train/epoch: 2.4059970378875732
Step: 10120, train/loss: 0.08129999786615372
Step: 10120, train/grad_norm: 2.012511046700638e-08
Step: 10120, train/learning_rate: 3.795811426243745e-05
Step: 10120, train/epoch: 2.408376932144165
Step: 10130, train/loss: 0.0
Step: 10130, train/grad_norm: 3.4082200727425516e-05
Step: 10130, train/learning_rate: 3.794621443375945e-05
Step: 10130, train/epoch: 2.410756826400757
Step: 10140, train/loss: 0.0
Step: 10140, train/grad_norm: 1.3655764519171498e-07
Step: 10140, train/learning_rate: 3.793431824306026e-05
Step: 10140, train/epoch: 2.4131367206573486
Step: 10150, train/loss: 0.0
Step: 10150, train/grad_norm: 9.937573850038461e-06
Step: 10150, train/learning_rate: 3.7922418414382264e-05
Step: 10150, train/epoch: 2.4155163764953613
Step: 10160, train/loss: 0.0007999999797903001
Step: 10160, train/grad_norm: 1.6087453502677818e-07
Step: 10160, train/learning_rate: 3.791051858570427e-05
Step: 10160, train/epoch: 2.417896270751953
Step: 10170, train/loss: 0.0
Step: 10170, train/grad_norm: 6.740322646692221e-07
Step: 10170, train/learning_rate: 3.789861875702627e-05
Step: 10170, train/epoch: 2.420276165008545
Step: 10180, train/loss: 0.0
Step: 10180, train/grad_norm: 1.452378217692285e-09
Step: 10180, train/learning_rate: 3.788671892834827e-05
Step: 10180, train/epoch: 2.4226558208465576
Step: 10190, train/loss: 0.08049999922513962
Step: 10190, train/grad_norm: 184.7988739013672
Step: 10190, train/learning_rate: 3.787482273764908e-05
Step: 10190, train/epoch: 2.4250357151031494
Step: 10200, train/loss: 0.0
Step: 10200, train/grad_norm: 3.0851942938170396e-06
Step: 10200, train/learning_rate: 3.7862922908971086e-05
Step: 10200, train/epoch: 2.427415609359741
Step: 10210, train/loss: 0.0
Step: 10210, train/grad_norm: 5.681720722350292e-06
Step: 10210, train/learning_rate: 3.785102308029309e-05
Step: 10210, train/epoch: 2.429795265197754
Step: 10220, train/loss: 0.0
Step: 10220, train/grad_norm: 2.0255210841924054e-08
Step: 10220, train/learning_rate: 3.783912325161509e-05
Step: 10220, train/epoch: 2.4321751594543457
Step: 10230, train/loss: 0.0
Step: 10230, train/grad_norm: 5.5834860290815413e-08
Step: 10230, train/learning_rate: 3.7827223422937095e-05
Step: 10230, train/epoch: 2.4345550537109375
Step: 10240, train/loss: 0.0
Step: 10240, train/grad_norm: 6.028131247148849e-05
Step: 10240, train/learning_rate: 3.7815327232237905e-05
Step: 10240, train/epoch: 2.43693470954895
Step: 10250, train/loss: 0.0
Step: 10250, train/grad_norm: 1.3555114492191933e-05
Step: 10250, train/learning_rate: 3.780342740355991e-05
Step: 10250, train/epoch: 2.439314603805542
Step: 10260, train/loss: 0.0
Step: 10260, train/grad_norm: 7.222683052532375e-06
Step: 10260, train/learning_rate: 3.779152757488191e-05
Step: 10260, train/epoch: 2.441694498062134
Step: 10270, train/loss: 0.0
Step: 10270, train/grad_norm: 2.758377149802982e-06
Step: 10270, train/learning_rate: 3.7779627746203914e-05
Step: 10270, train/epoch: 2.4440741539001465
Step: 10280, train/loss: 0.0
Step: 10280, train/grad_norm: 1.9953536423145124e-07
Step: 10280, train/learning_rate: 3.776772791752592e-05
Step: 10280, train/epoch: 2.4464540481567383
Step: 10290, train/loss: 0.0
Step: 10290, train/grad_norm: 2.6844896083844105e-08
Step: 10290, train/learning_rate: 3.775583172682673e-05
Step: 10290, train/epoch: 2.44883394241333
Step: 10300, train/loss: 0.0
Step: 10300, train/grad_norm: 0.0007179545355029404
Step: 10300, train/learning_rate: 3.774393189814873e-05
Step: 10300, train/epoch: 2.4512135982513428
Step: 10310, train/loss: 0.0
Step: 10310, train/grad_norm: 0.00023099729150999337
Step: 10310, train/learning_rate: 3.7732032069470733e-05
Step: 10310, train/epoch: 2.4535934925079346
Step: 10320, train/loss: 0.0
Step: 10320, train/grad_norm: 3.142298965030932e-06
Step: 10320, train/learning_rate: 3.7720132240792736e-05
Step: 10320, train/epoch: 2.4559733867645264
Step: 10330, train/loss: 0.0
Step: 10330, train/grad_norm: 4.493109372560866e-06
Step: 10330, train/learning_rate: 3.770823241211474e-05
Step: 10330, train/epoch: 2.458353281021118
Step: 10340, train/loss: 0.0
Step: 10340, train/grad_norm: 1.1865446936099033e-07
Step: 10340, train/learning_rate: 3.769633622141555e-05
Step: 10340, train/epoch: 2.460732936859131
Step: 10350, train/loss: 0.0
Step: 10350, train/grad_norm: 3.3784588595153764e-05
Step: 10350, train/learning_rate: 3.768443639273755e-05
Step: 10350, train/epoch: 2.4631128311157227
Step: 10360, train/loss: 0.0
Step: 10360, train/grad_norm: 3.2118403936465256e-08
Step: 10360, train/learning_rate: 3.7672536564059556e-05
Step: 10360, train/epoch: 2.4654927253723145
Step: 10370, train/loss: 0.0
Step: 10370, train/grad_norm: 1.3983352801005822e-05
Step: 10370, train/learning_rate: 3.766063673538156e-05
Step: 10370, train/epoch: 2.467872381210327
Step: 10380, train/loss: 0.0
Step: 10380, train/grad_norm: 5.485850351760746e-07
Step: 10380, train/learning_rate: 3.764873690670356e-05
Step: 10380, train/epoch: 2.470252275466919
Step: 10390, train/loss: 0.0003000000142492354
Step: 10390, train/grad_norm: 1.055915355682373
Step: 10390, train/learning_rate: 3.763684071600437e-05
Step: 10390, train/epoch: 2.4726321697235107
Step: 10400, train/loss: 0.0
Step: 10400, train/grad_norm: 7.562110113212839e-05
Step: 10400, train/learning_rate: 3.7624940887326375e-05
Step: 10400, train/epoch: 2.4750118255615234
Step: 10410, train/loss: 0.001500000013038516
Step: 10410, train/grad_norm: 2.7666852474794723e-05
Step: 10410, train/learning_rate: 3.761304105864838e-05
Step: 10410, train/epoch: 2.4773917198181152
Step: 10420, train/loss: 0.0
Step: 10420, train/grad_norm: 1.7209424640896032e-07
Step: 10420, train/learning_rate: 3.760114122997038e-05
Step: 10420, train/epoch: 2.479771614074707
Step: 10430, train/loss: 0.0
Step: 10430, train/grad_norm: 2.7827920803247252e-06
Step: 10430, train/learning_rate: 3.7589241401292384e-05
Step: 10430, train/epoch: 2.4821512699127197
Step: 10440, train/loss: 0.0
Step: 10440, train/grad_norm: 0.008644680492579937
Step: 10440, train/learning_rate: 3.7577345210593194e-05
Step: 10440, train/epoch: 2.4845311641693115
Step: 10450, train/loss: 9.999999747378752e-05
Step: 10450, train/grad_norm: 1.3734191952607944e-06
Step: 10450, train/learning_rate: 3.75654453819152e-05
Step: 10450, train/epoch: 2.4869110584259033
Step: 10460, train/loss: 0.0
Step: 10460, train/grad_norm: 3.4983759178430773e-07
Step: 10460, train/learning_rate: 3.75535455532372e-05
Step: 10460, train/epoch: 2.489290714263916
Step: 10470, train/loss: 0.0
Step: 10470, train/grad_norm: 3.1335278549704526e-07
Step: 10470, train/learning_rate: 3.75416457245592e-05
Step: 10470, train/epoch: 2.491670608520508
Step: 10480, train/loss: 9.999999747378752e-05
Step: 10480, train/grad_norm: 6.683599167445209e-07
Step: 10480, train/learning_rate: 3.752974953386001e-05
Step: 10480, train/epoch: 2.4940505027770996
Step: 10490, train/loss: 0.012500000186264515
Step: 10490, train/grad_norm: 2.8888441860885905e-08
Step: 10490, train/learning_rate: 3.7517849705182016e-05
Step: 10490, train/epoch: 2.4964301586151123
Step: 10500, train/loss: 0.0
Step: 10500, train/grad_norm: 5.476467777043581e-07
Step: 10500, train/learning_rate: 3.750594987650402e-05
Step: 10500, train/epoch: 2.498810052871704
Step: 10510, train/loss: 0.0
Step: 10510, train/grad_norm: 2.8747467695211526e-07
Step: 10510, train/learning_rate: 3.749405004782602e-05
Step: 10510, train/epoch: 2.501189947128296
Step: 10520, train/loss: 0.0
Step: 10520, train/grad_norm: 2.40856820710178e-07
Step: 10520, train/learning_rate: 3.7482150219148025e-05
Step: 10520, train/epoch: 2.5035698413848877
Step: 10530, train/loss: 0.0
Step: 10530, train/grad_norm: 6.1489138225567785e-09
Step: 10530, train/learning_rate: 3.7470254028448835e-05
Step: 10530, train/epoch: 2.5059494972229004
Step: 10540, train/loss: 0.0
Step: 10540, train/grad_norm: 3.5247749110567383e-06
Step: 10540, train/learning_rate: 3.745835419977084e-05
Step: 10540, train/epoch: 2.508329391479492
Step: 10550, train/loss: 0.0
Step: 10550, train/grad_norm: 8.018329822334636e-08
Step: 10550, train/learning_rate: 3.744645437109284e-05
Step: 10550, train/epoch: 2.510709285736084
Step: 10560, train/loss: 0.0
Step: 10560, train/grad_norm: 0.07133390754461288
Step: 10560, train/learning_rate: 3.7434554542414844e-05
Step: 10560, train/epoch: 2.5130889415740967
Step: 10570, train/loss: 0.15780000388622284
Step: 10570, train/grad_norm: 3.781010207148938e-07
Step: 10570, train/learning_rate: 3.742265471373685e-05
Step: 10570, train/epoch: 2.5154688358306885
Step: 10580, train/loss: 0.0
Step: 10580, train/grad_norm: 2.6090372102771653e-06
Step: 10580, train/learning_rate: 3.741075852303766e-05
Step: 10580, train/epoch: 2.5178487300872803
Step: 10590, train/loss: 0.00039999998989515007
Step: 10590, train/grad_norm: 2.9492691737686982e-06
Step: 10590, train/learning_rate: 3.739885869435966e-05
Step: 10590, train/epoch: 2.520228385925293
Step: 10600, train/loss: 0.0
Step: 10600, train/grad_norm: 3.0870851333020255e-05
Step: 10600, train/learning_rate: 3.738695886568166e-05
Step: 10600, train/epoch: 2.5226082801818848
Step: 10610, train/loss: 0.0
Step: 10610, train/grad_norm: 3.0033424991415814e-06
Step: 10610, train/learning_rate: 3.7375059037003666e-05
Step: 10610, train/epoch: 2.5249881744384766
Step: 10620, train/loss: 9.999999747378752e-05
Step: 10620, train/grad_norm: 1.447952229227667e-07
Step: 10620, train/learning_rate: 3.736315920832567e-05
Step: 10620, train/epoch: 2.5273678302764893
Step: 10630, train/loss: 0.0
Step: 10630, train/grad_norm: 2.4150310196091596e-07
Step: 10630, train/learning_rate: 3.735126301762648e-05
Step: 10630, train/epoch: 2.529747724533081
Step: 10640, train/loss: 0.0
Step: 10640, train/grad_norm: 1.1892925755319084e-07
Step: 10640, train/learning_rate: 3.733936318894848e-05
Step: 10640, train/epoch: 2.532127618789673
Step: 10650, train/loss: 0.0
Step: 10650, train/grad_norm: 1.3189848857564357e-07
Step: 10650, train/learning_rate: 3.7327463360270485e-05
Step: 10650, train/epoch: 2.5345072746276855
Step: 10660, train/loss: 0.0
Step: 10660, train/grad_norm: 2.3800313897481828e-07
Step: 10660, train/learning_rate: 3.731556353159249e-05
Step: 10660, train/epoch: 2.5368871688842773
Step: 10670, train/loss: 0.0
Step: 10670, train/grad_norm: 1.0861795018968223e-08
Step: 10670, train/learning_rate: 3.730366370291449e-05
Step: 10670, train/epoch: 2.539267063140869
Step: 10680, train/loss: 0.0
Step: 10680, train/grad_norm: 3.1648175990994787e-07
Step: 10680, train/learning_rate: 3.72917675122153e-05
Step: 10680, train/epoch: 2.541646718978882
Step: 10690, train/loss: 0.0
Step: 10690, train/grad_norm: 7.932477456051856e-05
Step: 10690, train/learning_rate: 3.7279867683537304e-05
Step: 10690, train/epoch: 2.5440266132354736
Step: 10700, train/loss: 0.0
Step: 10700, train/grad_norm: 4.5834458006766e-08
Step: 10700, train/learning_rate: 3.726796785485931e-05
Step: 10700, train/epoch: 2.5464065074920654
Step: 10710, train/loss: 0.0
Step: 10710, train/grad_norm: 0.0002082716819131747
Step: 10710, train/learning_rate: 3.725606802618131e-05
Step: 10710, train/epoch: 2.5487864017486572
Step: 10720, train/loss: 0.0
Step: 10720, train/grad_norm: 9.304561388034926e-08
Step: 10720, train/learning_rate: 3.7244168197503313e-05
Step: 10720, train/epoch: 2.55116605758667
Step: 10730, train/loss: 0.02290000021457672
Step: 10730, train/grad_norm: 1.907304181258951e-08
Step: 10730, train/learning_rate: 3.7232272006804124e-05
Step: 10730, train/epoch: 2.5535459518432617
Step: 10740, train/loss: 0.010200000368058681
Step: 10740, train/grad_norm: 7.967920545581819e-09
Step: 10740, train/learning_rate: 3.7220372178126127e-05
Step: 10740, train/epoch: 2.5559258460998535
Step: 10750, train/loss: 0.0
Step: 10750, train/grad_norm: 1.1043520942166651e-07
Step: 10750, train/learning_rate: 3.720847234944813e-05
Step: 10750, train/epoch: 2.558305501937866
Step: 10760, train/loss: 0.0
Step: 10760, train/grad_norm: 3.3453638934588525e-06
Step: 10760, train/learning_rate: 3.719657252077013e-05
Step: 10760, train/epoch: 2.560685396194458
Step: 10770, train/loss: 0.0
Step: 10770, train/grad_norm: 1.732803525555937e-06
Step: 10770, train/learning_rate: 3.7184672692092136e-05
Step: 10770, train/epoch: 2.56306529045105
Step: 10780, train/loss: 0.030500000342726707
Step: 10780, train/grad_norm: 3.995436054538004e-05
Step: 10780, train/learning_rate: 3.7172776501392946e-05
Step: 10780, train/epoch: 2.5654449462890625
Step: 10790, train/loss: 0.0
Step: 10790, train/grad_norm: 0.0034987973049283028
Step: 10790, train/learning_rate: 3.716087667271495e-05
Step: 10790, train/epoch: 2.5678248405456543
Step: 10800, train/loss: 0.0
Step: 10800, train/grad_norm: 2.6452364604301337e-11
Step: 10800, train/learning_rate: 3.714897684403695e-05
Step: 10800, train/epoch: 2.570204734802246
Step: 10810, train/loss: 0.0
Step: 10810, train/grad_norm: 5.741353561461437e-07
Step: 10810, train/learning_rate: 3.7137077015358955e-05
Step: 10810, train/epoch: 2.572584390640259
Step: 10820, train/loss: 0.0
Step: 10820, train/grad_norm: 0.0002458304225001484
Step: 10820, train/learning_rate: 3.712517718668096e-05
Step: 10820, train/epoch: 2.5749642848968506
Step: 10830, train/loss: 0.0
Step: 10830, train/grad_norm: 0.0007893356378190219
Step: 10830, train/learning_rate: 3.711328099598177e-05
Step: 10830, train/epoch: 2.5773441791534424
Step: 10840, train/loss: 0.0
Step: 10840, train/grad_norm: 6.452954949054401e-06
Step: 10840, train/learning_rate: 3.710138116730377e-05
Step: 10840, train/epoch: 2.579723834991455
Step: 10850, train/loss: 0.0
Step: 10850, train/grad_norm: 0.0002891257172450423
Step: 10850, train/learning_rate: 3.7089481338625774e-05
Step: 10850, train/epoch: 2.582103729248047
Step: 10860, train/loss: 0.0
Step: 10860, train/grad_norm: 2.513462220576912e-07
Step: 10860, train/learning_rate: 3.707758150994778e-05
Step: 10860, train/epoch: 2.5844836235046387
Step: 10870, train/loss: 0.0
Step: 10870, train/grad_norm: 1.1503567520776414e-06
Step: 10870, train/learning_rate: 3.706568168126978e-05
Step: 10870, train/epoch: 2.5868632793426514
Step: 10880, train/loss: 9.999999747378752e-05
Step: 10880, train/grad_norm: 1.0165972064157813e-08
Step: 10880, train/learning_rate: 3.705378549057059e-05
Step: 10880, train/epoch: 2.589243173599243
Step: 10890, train/loss: 0.0
Step: 10890, train/grad_norm: 2.826801846822491e-06
Step: 10890, train/learning_rate: 3.704188566189259e-05
Step: 10890, train/epoch: 2.591623067855835
Step: 10900, train/loss: 0.0
Step: 10900, train/grad_norm: 3.245516495553602e-07
Step: 10900, train/learning_rate: 3.7029985833214596e-05
Step: 10900, train/epoch: 2.5940029621124268
Step: 10910, train/loss: 0.09220000356435776
Step: 10910, train/grad_norm: 3.2150248443940654e-05
Step: 10910, train/learning_rate: 3.70180860045366e-05
Step: 10910, train/epoch: 2.5963826179504395
Step: 10920, train/loss: 0.0
Step: 10920, train/grad_norm: 0.013372216373682022
Step: 10920, train/learning_rate: 3.70061861758586e-05
Step: 10920, train/epoch: 2.5987625122070312
Step: 10930, train/loss: 0.0
Step: 10930, train/grad_norm: 6.337540980894119e-05
Step: 10930, train/learning_rate: 3.699428998515941e-05
Step: 10930, train/epoch: 2.601142406463623
Step: 10940, train/loss: 0.004999999888241291
Step: 10940, train/grad_norm: 1.399857723072273e-07
Step: 10940, train/learning_rate: 3.6982390156481415e-05
Step: 10940, train/epoch: 2.6035220623016357
Step: 10950, train/loss: 0.0
Step: 10950, train/grad_norm: 8.152740338118747e-05
Step: 10950, train/learning_rate: 3.697049032780342e-05
Step: 10950, train/epoch: 2.6059019565582275
Step: 10960, train/loss: 0.0
Step: 10960, train/grad_norm: 0.009640367701649666
Step: 10960, train/learning_rate: 3.695859049912542e-05
Step: 10960, train/epoch: 2.6082818508148193
Step: 10970, train/loss: 0.0
Step: 10970, train/grad_norm: 0.0014324220828711987
Step: 10970, train/learning_rate: 3.6946690670447424e-05
Step: 10970, train/epoch: 2.610661506652832
Step: 10980, train/loss: 0.007300000172108412
Step: 10980, train/grad_norm: 60.76276779174805
Step: 10980, train/learning_rate: 3.6934794479748234e-05
Step: 10980, train/epoch: 2.613041400909424
Step: 10990, train/loss: 0.0
Step: 10990, train/grad_norm: 9.198513595265467e-08
Step: 10990, train/learning_rate: 3.692289465107024e-05
Step: 10990, train/epoch: 2.6154212951660156
Step: 11000, train/loss: 0.0
Step: 11000, train/grad_norm: 1.532783500124424e-07
Step: 11000, train/learning_rate: 3.691099482239224e-05
Step: 11000, train/epoch: 2.6178009510040283
Step: 11010, train/loss: 0.0
Step: 11010, train/grad_norm: 0.0002125390019500628
Step: 11010, train/learning_rate: 3.689909499371424e-05
Step: 11010, train/epoch: 2.62018084526062
Step: 11020, train/loss: 0.0
Step: 11020, train/grad_norm: 3.1531858439848293e-06
Step: 11020, train/learning_rate: 3.6887195165036246e-05
Step: 11020, train/epoch: 2.622560739517212
Step: 11030, train/loss: 0.0
Step: 11030, train/grad_norm: 5.056154384064371e-10
Step: 11030, train/learning_rate: 3.6875298974337056e-05
Step: 11030, train/epoch: 2.6249403953552246
Step: 11040, train/loss: 0.0
Step: 11040, train/grad_norm: 2.8522597617097745e-08
Step: 11040, train/learning_rate: 3.686339914565906e-05
Step: 11040, train/epoch: 2.6273202896118164
Step: 11050, train/loss: 0.0
Step: 11050, train/grad_norm: 1.626060353032699e-08
Step: 11050, train/learning_rate: 3.685149931698106e-05
Step: 11050, train/epoch: 2.629700183868408
Step: 11060, train/loss: 0.0
Step: 11060, train/grad_norm: 2.482837491157852e-09
Step: 11060, train/learning_rate: 3.6839599488303065e-05
Step: 11060, train/epoch: 2.632080078125
Step: 11070, train/loss: 0.0
Step: 11070, train/grad_norm: 2.3389038688037544e-05
Step: 11070, train/learning_rate: 3.682769965962507e-05
Step: 11070, train/epoch: 2.6344597339630127
Step: 11080, train/loss: 0.0
Step: 11080, train/grad_norm: 7.2901821113191545e-06
Step: 11080, train/learning_rate: 3.681580346892588e-05
Step: 11080, train/epoch: 2.6368396282196045
Step: 11090, train/loss: 0.0
Step: 11090, train/grad_norm: 8.272426157418522e-07
Step: 11090, train/learning_rate: 3.680390364024788e-05
Step: 11090, train/epoch: 2.6392195224761963
Step: 11100, train/loss: 0.0
Step: 11100, train/grad_norm: 3.980600737918394e-09
Step: 11100, train/learning_rate: 3.6792003811569884e-05
Step: 11100, train/epoch: 2.641599178314209
Step: 11110, train/loss: 0.0
Step: 11110, train/grad_norm: 4.1300145525724474e-09
Step: 11110, train/learning_rate: 3.678010398289189e-05
Step: 11110, train/epoch: 2.643979072570801
Step: 11120, train/loss: 0.0
Step: 11120, train/grad_norm: 2.417246207642165e-07
Step: 11120, train/learning_rate: 3.676820415421389e-05
Step: 11120, train/epoch: 2.6463589668273926
Step: 11130, train/loss: 0.02329999953508377
Step: 11130, train/grad_norm: 1.3392146058777143e-08
Step: 11130, train/learning_rate: 3.67563079635147e-05
Step: 11130, train/epoch: 2.6487386226654053
Step: 11140, train/loss: 0.0008999999845400453
Step: 11140, train/grad_norm: 3.3065693969547283e-06
Step: 11140, train/learning_rate: 3.6744408134836704e-05
Step: 11140, train/epoch: 2.651118516921997
Step: 11150, train/loss: 0.0
Step: 11150, train/grad_norm: 2.9039276796538616e-06
Step: 11150, train/learning_rate: 3.6732508306158707e-05
Step: 11150, train/epoch: 2.653498411178589
Step: 11160, train/loss: 0.0
Step: 11160, train/grad_norm: 4.280409029888688e-06
Step: 11160, train/learning_rate: 3.672060847748071e-05
Step: 11160, train/epoch: 2.6558780670166016
Step: 11170, train/loss: 0.0
Step: 11170, train/grad_norm: 0.0004160396638326347
Step: 11170, train/learning_rate: 3.670870864880271e-05
Step: 11170, train/epoch: 2.6582579612731934
Step: 11180, train/loss: 0.04839999973773956
Step: 11180, train/grad_norm: 0.0005291054840199649
Step: 11180, train/learning_rate: 3.669681245810352e-05
Step: 11180, train/epoch: 2.660637855529785
Step: 11190, train/loss: 0.0
Step: 11190, train/grad_norm: 2.931602102762554e-05
Step: 11190, train/learning_rate: 3.6684912629425526e-05
Step: 11190, train/epoch: 2.663017511367798
Step: 11200, train/loss: 0.11249999701976776
Step: 11200, train/grad_norm: 1.4859134353173431e-05
Step: 11200, train/learning_rate: 3.667301280074753e-05
Step: 11200, train/epoch: 2.6653974056243896
Step: 11210, train/loss: 0.0
Step: 11210, train/grad_norm: 2.378905492150807e-06
Step: 11210, train/learning_rate: 3.666111297206953e-05
Step: 11210, train/epoch: 2.6677772998809814
Step: 11220, train/loss: 0.0
Step: 11220, train/grad_norm: 4.5058359887661936e-07
Step: 11220, train/learning_rate: 3.6649213143391535e-05
Step: 11220, train/epoch: 2.670156955718994
Step: 11230, train/loss: 0.0
Step: 11230, train/grad_norm: 5.382658855523914e-06
Step: 11230, train/learning_rate: 3.6637316952692345e-05
Step: 11230, train/epoch: 2.672536849975586
Step: 11240, train/loss: 0.0
Step: 11240, train/grad_norm: 1.8364923448643822e-07
Step: 11240, train/learning_rate: 3.662541712401435e-05
Step: 11240, train/epoch: 2.6749167442321777
Step: 11250, train/loss: 0.0
Step: 11250, train/grad_norm: 3.5812522583000828e-06
Step: 11250, train/learning_rate: 3.661351729533635e-05
Step: 11250, train/epoch: 2.6772966384887695
Step: 11260, train/loss: 0.0
Step: 11260, train/grad_norm: 5.342685653886292e-07
Step: 11260, train/learning_rate: 3.6601617466658354e-05
Step: 11260, train/epoch: 2.6796762943267822
Step: 11270, train/loss: 0.0
Step: 11270, train/grad_norm: 9.509077614211492e-08
Step: 11270, train/learning_rate: 3.658971763798036e-05
Step: 11270, train/epoch: 2.682056188583374
Step: 11280, train/loss: 0.0
Step: 11280, train/grad_norm: 3.205994516974897e-06
Step: 11280, train/learning_rate: 3.657782144728117e-05
Step: 11280, train/epoch: 2.684436082839966
Step: 11290, train/loss: 0.0
Step: 11290, train/grad_norm: 8.43801728933613e-07
Step: 11290, train/learning_rate: 3.656592161860317e-05
Step: 11290, train/epoch: 2.6868157386779785
Step: 11300, train/loss: 0.0
Step: 11300, train/grad_norm: 2.875958360348818e-09
Step: 11300, train/learning_rate: 3.655402178992517e-05
Step: 11300, train/epoch: 2.6891956329345703
Step: 11310, train/loss: 0.0
Step: 11310, train/grad_norm: 6.47853739792481e-05
Step: 11310, train/learning_rate: 3.6542121961247176e-05
Step: 11310, train/epoch: 2.691575527191162
Step: 11320, train/loss: 0.0
Step: 11320, train/grad_norm: 6.309409172899905e-08
Step: 11320, train/learning_rate: 3.653022213256918e-05
Step: 11320, train/epoch: 2.693955183029175
Step: 11330, train/loss: 0.0
Step: 11330, train/grad_norm: 9.913381290971301e-06
Step: 11330, train/learning_rate: 3.651832594186999e-05
Step: 11330, train/epoch: 2.6963350772857666
Step: 11340, train/loss: 0.0
Step: 11340, train/grad_norm: 2.4308608317369362e-06
Step: 11340, train/learning_rate: 3.650642611319199e-05
Step: 11340, train/epoch: 2.6987149715423584
Step: 11350, train/loss: 0.0
Step: 11350, train/grad_norm: 6.495317173005333e-09
Step: 11350, train/learning_rate: 3.6494526284513995e-05
Step: 11350, train/epoch: 2.701094627380371
Step: 11360, train/loss: 0.00039999998989515007
Step: 11360, train/grad_norm: 8.245282856478298e-07
Step: 11360, train/learning_rate: 3.6482626455836e-05
Step: 11360, train/epoch: 2.703474521636963
Step: 11370, train/loss: 0.0
Step: 11370, train/grad_norm: 2.7628821044345386e-06
Step: 11370, train/learning_rate: 3.6470726627158e-05
Step: 11370, train/epoch: 2.7058544158935547
Step: 11380, train/loss: 0.00039999998989515007
Step: 11380, train/grad_norm: 0.0855201706290245
Step: 11380, train/learning_rate: 3.645883043645881e-05
Step: 11380, train/epoch: 2.7082340717315674
Step: 11390, train/loss: 0.0
Step: 11390, train/grad_norm: 5.0775843192241155e-06
Step: 11390, train/learning_rate: 3.6446930607780814e-05
Step: 11390, train/epoch: 2.710613965988159
Step: 11400, train/loss: 0.0
Step: 11400, train/grad_norm: 6.529819740475062e-11
Step: 11400, train/learning_rate: 3.643503077910282e-05
Step: 11400, train/epoch: 2.712993860244751
Step: 11410, train/loss: 0.0
Step: 11410, train/grad_norm: 3.3856053960334975e-07
Step: 11410, train/learning_rate: 3.642313095042482e-05
Step: 11410, train/epoch: 2.7153735160827637
Step: 11420, train/loss: 0.1859000027179718
Step: 11420, train/grad_norm: 8.464078149472698e-08
Step: 11420, train/learning_rate: 3.641123112174682e-05
Step: 11420, train/epoch: 2.7177534103393555
Step: 11430, train/loss: 0.0
Step: 11430, train/grad_norm: 1.9045250155613758e-05
Step: 11430, train/learning_rate: 3.639933493104763e-05
Step: 11430, train/epoch: 2.7201333045959473
Step: 11440, train/loss: 0.005200000014156103
Step: 11440, train/grad_norm: 3.525260439118938e-08
Step: 11440, train/learning_rate: 3.6387435102369636e-05
Step: 11440, train/epoch: 2.722513198852539
Step: 11450, train/loss: 0.0
Step: 11450, train/grad_norm: 4.022349003207637e-06
Step: 11450, train/learning_rate: 3.637553527369164e-05
Step: 11450, train/epoch: 2.7248928546905518
Step: 11460, train/loss: 0.08669999986886978
Step: 11460, train/grad_norm: 2.4360062411687977e-07
Step: 11460, train/learning_rate: 3.636363544501364e-05
Step: 11460, train/epoch: 2.7272727489471436
Step: 11470, train/loss: 0.0
Step: 11470, train/grad_norm: 3.1339578754341346e-07
Step: 11470, train/learning_rate: 3.6351735616335645e-05
Step: 11470, train/epoch: 2.7296526432037354
Step: 11480, train/loss: 0.2078000009059906
Step: 11480, train/grad_norm: 1.445736506866524e-05
Step: 11480, train/learning_rate: 3.6339839425636455e-05
Step: 11480, train/epoch: 2.732032299041748
Step: 11490, train/loss: 0.0
Step: 11490, train/grad_norm: 1.0622671453575094e-07
Step: 11490, train/learning_rate: 3.632793959695846e-05
Step: 11490, train/epoch: 2.73441219329834
Step: 11500, train/loss: 9.999999747378752e-05
Step: 11500, train/grad_norm: 2.2551676011062227e-05
Step: 11500, train/learning_rate: 3.631603976828046e-05
Step: 11500, train/epoch: 2.7367920875549316
Step: 11510, train/loss: 0.0
Step: 11510, train/grad_norm: 0.0002577881095930934
Step: 11510, train/learning_rate: 3.6304139939602464e-05
Step: 11510, train/epoch: 2.7391717433929443
Step: 11520, train/loss: 0.0
Step: 11520, train/grad_norm: 1.3973092791275121e-05
Step: 11520, train/learning_rate: 3.629224011092447e-05
Step: 11520, train/epoch: 2.741551637649536
Step: 11530, train/loss: 0.001500000013038516
Step: 11530, train/grad_norm: 9.567820598022081e-06
Step: 11530, train/learning_rate: 3.628034392022528e-05
Step: 11530, train/epoch: 2.743931531906128
Step: 11540, train/loss: 0.0
Step: 11540, train/grad_norm: 0.00011952772183576599
Step: 11540, train/learning_rate: 3.626844409154728e-05
Step: 11540, train/epoch: 2.7463111877441406
Step: 11550, train/loss: 0.010700000450015068
Step: 11550, train/grad_norm: 0.0012780616525560617
Step: 11550, train/learning_rate: 3.6256544262869284e-05
Step: 11550, train/epoch: 2.7486910820007324
Step: 11560, train/loss: 9.999999747378752e-05
Step: 11560, train/grad_norm: 7.298214768525213e-06
Step: 11560, train/learning_rate: 3.6244644434191287e-05
Step: 11560, train/epoch: 2.751070976257324
Step: 11570, train/loss: 0.2840000092983246
Step: 11570, train/grad_norm: 3.8163816498126835e-06
Step: 11570, train/learning_rate: 3.623274460551329e-05
Step: 11570, train/epoch: 2.753450632095337
Step: 11580, train/loss: 0.0
Step: 11580, train/grad_norm: 1.154722303908784e-05
Step: 11580, train/learning_rate: 3.62208484148141e-05
Step: 11580, train/epoch: 2.7558305263519287
Step: 11590, train/loss: 9.999999747378752e-05
Step: 11590, train/grad_norm: 2.2233114123082487e-06
Step: 11590, train/learning_rate: 3.62089485861361e-05
Step: 11590, train/epoch: 2.7582104206085205
Step: 11600, train/loss: 0.0
Step: 11600, train/grad_norm: 6.883987680339487e-06
Step: 11600, train/learning_rate: 3.6197048757458106e-05
Step: 11600, train/epoch: 2.760590076446533
Step: 11610, train/loss: 0.0
Step: 11610, train/grad_norm: 1.847389285103418e-05
Step: 11610, train/learning_rate: 3.618514892878011e-05
Step: 11610, train/epoch: 2.762969970703125
Step: 11620, train/loss: 0.0
Step: 11620, train/grad_norm: 2.2483062878109195e-07
Step: 11620, train/learning_rate: 3.617324910010211e-05
Step: 11620, train/epoch: 2.765349864959717
Step: 11630, train/loss: 0.0
Step: 11630, train/grad_norm: 7.651419196008646e-07
Step: 11630, train/learning_rate: 3.616135290940292e-05
Step: 11630, train/epoch: 2.7677297592163086
Step: 11640, train/loss: 0.0
Step: 11640, train/grad_norm: 1.761019666446373e-05
Step: 11640, train/learning_rate: 3.6149453080724925e-05
Step: 11640, train/epoch: 2.7701094150543213
Step: 11650, train/loss: 0.0
Step: 11650, train/grad_norm: 9.140579493305268e-08
Step: 11650, train/learning_rate: 3.613755325204693e-05
Step: 11650, train/epoch: 2.772489309310913
Step: 11660, train/loss: 0.0
Step: 11660, train/grad_norm: 2.2600347620027605e-06
Step: 11660, train/learning_rate: 3.612565342336893e-05
Step: 11660, train/epoch: 2.774869203567505
Step: 11670, train/loss: 0.0
Step: 11670, train/grad_norm: 1.0278264198859688e-05
Step: 11670, train/learning_rate: 3.6113753594690934e-05
Step: 11670, train/epoch: 2.7772488594055176
Step: 11680, train/loss: 0.0
Step: 11680, train/grad_norm: 0.010481446050107479
Step: 11680, train/learning_rate: 3.6101857403991744e-05
Step: 11680, train/epoch: 2.7796287536621094
Step: 11690, train/loss: 0.0
Step: 11690, train/grad_norm: 6.939039394637803e-06
Step: 11690, train/learning_rate: 3.608995757531375e-05
Step: 11690, train/epoch: 2.782008647918701
Step: 11700, train/loss: 0.0
Step: 11700, train/grad_norm: 1.6830131244205404e-07
Step: 11700, train/learning_rate: 3.607805774663575e-05
Step: 11700, train/epoch: 2.784388303756714
Step: 11710, train/loss: 0.0
Step: 11710, train/grad_norm: 2.951236865555984e-06
Step: 11710, train/learning_rate: 3.606615791795775e-05
Step: 11710, train/epoch: 2.7867681980133057
Step: 11720, train/loss: 0.0
Step: 11720, train/grad_norm: 5.811268056277186e-05
Step: 11720, train/learning_rate: 3.6054258089279756e-05
Step: 11720, train/epoch: 2.7891480922698975
Step: 11730, train/loss: 0.0
Step: 11730, train/grad_norm: 2.156841946998611e-05
Step: 11730, train/learning_rate: 3.6042361898580566e-05
Step: 11730, train/epoch: 2.79152774810791
Step: 11740, train/loss: 0.0
Step: 11740, train/grad_norm: 2.4929682695074007e-05
Step: 11740, train/learning_rate: 3.603046206990257e-05
Step: 11740, train/epoch: 2.793907642364502
Step: 11750, train/loss: 0.0
Step: 11750, train/grad_norm: 0.27347224950790405
Step: 11750, train/learning_rate: 3.601856224122457e-05
Step: 11750, train/epoch: 2.7962875366210938
Step: 11760, train/loss: 0.0
Step: 11760, train/grad_norm: 6.3032116486283485e-06
Step: 11760, train/learning_rate: 3.6006662412546575e-05
Step: 11760, train/epoch: 2.7986671924591064
Step: 11770, train/loss: 0.0
Step: 11770, train/grad_norm: 0.0002525870513636619
Step: 11770, train/learning_rate: 3.599476258386858e-05
Step: 11770, train/epoch: 2.8010470867156982
Step: 11780, train/loss: 0.0
Step: 11780, train/grad_norm: 0.00021983195620123297
Step: 11780, train/learning_rate: 3.598286639316939e-05
Step: 11780, train/epoch: 2.80342698097229
Step: 11790, train/loss: 0.0
Step: 11790, train/grad_norm: 8.17702675703913e-06
Step: 11790, train/learning_rate: 3.597096656449139e-05
Step: 11790, train/epoch: 2.805806875228882
Step: 11800, train/loss: 0.0
Step: 11800, train/grad_norm: 0.0005002441466785967
Step: 11800, train/learning_rate: 3.5959066735813394e-05
Step: 11800, train/epoch: 2.8081865310668945
Step: 11810, train/loss: 0.0
Step: 11810, train/grad_norm: 2.3757255007694766e-07
Step: 11810, train/learning_rate: 3.59471669071354e-05
Step: 11810, train/epoch: 2.8105664253234863
Step: 11820, train/loss: 0.0
Step: 11820, train/grad_norm: 8.773682930041105e-05
Step: 11820, train/learning_rate: 3.593527071643621e-05
Step: 11820, train/epoch: 2.812946319580078
Step: 11830, train/loss: 0.06989999860525131
Step: 11830, train/grad_norm: 1.1899942364834715e-05
Step: 11830, train/learning_rate: 3.592337088775821e-05
Step: 11830, train/epoch: 2.815325975418091
Step: 11840, train/loss: 0.0
Step: 11840, train/grad_norm: 8.1705993579817e-06
Step: 11840, train/learning_rate: 3.591147105908021e-05
Step: 11840, train/epoch: 2.8177058696746826
Step: 11850, train/loss: 0.00019999999494757503
Step: 11850, train/grad_norm: 0.06608892232179642
Step: 11850, train/learning_rate: 3.5899571230402216e-05
Step: 11850, train/epoch: 2.8200857639312744
Step: 11860, train/loss: 0.0
Step: 11860, train/grad_norm: 0.00039226163062267005
Step: 11860, train/learning_rate: 3.588767140172422e-05
Step: 11860, train/epoch: 2.822465419769287
Step: 11870, train/loss: 0.0
Step: 11870, train/grad_norm: 9.091819492823561e-07
Step: 11870, train/learning_rate: 3.587577521102503e-05
Step: 11870, train/epoch: 2.824845314025879
Step: 11880, train/loss: 0.00019999999494757503
Step: 11880, train/grad_norm: 3.180735438945703e-05
Step: 11880, train/learning_rate: 3.586387538234703e-05
Step: 11880, train/epoch: 2.8272252082824707
Step: 11890, train/loss: 0.0
Step: 11890, train/grad_norm: 2.486367293386138e-06
Step: 11890, train/learning_rate: 3.5851975553669035e-05
Step: 11890, train/epoch: 2.8296048641204834
Step: 11900, train/loss: 0.0
Step: 11900, train/grad_norm: 8.259837045443419e-07
Step: 11900, train/learning_rate: 3.584007572499104e-05
Step: 11900, train/epoch: 2.831984758377075
Step: 11910, train/loss: 0.0
Step: 11910, train/grad_norm: 2.1101985936411438e-08
Step: 11910, train/learning_rate: 3.582817589631304e-05
Step: 11910, train/epoch: 2.834364652633667
Step: 11920, train/loss: 0.0038999998942017555
Step: 11920, train/grad_norm: 3.251845015483923e-09
Step: 11920, train/learning_rate: 3.581627970561385e-05
Step: 11920, train/epoch: 2.8367443084716797
Step: 11930, train/loss: 0.0
Step: 11930, train/grad_norm: 1.6082134735029285e-08
Step: 11930, train/learning_rate: 3.5804379876935855e-05
Step: 11930, train/epoch: 2.8391242027282715
Step: 11940, train/loss: 0.05779999867081642
Step: 11940, train/grad_norm: 2.8921483590238495e-06
Step: 11940, train/learning_rate: 3.579248004825786e-05
Step: 11940, train/epoch: 2.8415040969848633
Step: 11950, train/loss: 0.1648000031709671
Step: 11950, train/grad_norm: 1.3121905340085505e-07
Step: 11950, train/learning_rate: 3.578058021957986e-05
Step: 11950, train/epoch: 2.843883752822876
Step: 11960, train/loss: 0.0
Step: 11960, train/grad_norm: 1.4434806239194131e-08
Step: 11960, train/learning_rate: 3.5768680390901864e-05
Step: 11960, train/epoch: 2.8462636470794678
Step: 11970, train/loss: 0.0
Step: 11970, train/grad_norm: 1.5255760388299677e-07
Step: 11970, train/learning_rate: 3.5756784200202674e-05
Step: 11970, train/epoch: 2.8486435413360596
Step: 11980, train/loss: 0.13979999721050262
Step: 11980, train/grad_norm: 0.00011140710557810962
Step: 11980, train/learning_rate: 3.574488437152468e-05
Step: 11980, train/epoch: 2.8510234355926514
Step: 11990, train/loss: 0.0
Step: 11990, train/grad_norm: 0.0005342064541764557
Step: 11990, train/learning_rate: 3.573298454284668e-05
Step: 11990, train/epoch: 2.853403091430664
Step: 12000, train/loss: 0.0
Step: 12000, train/grad_norm: 0.004316791892051697
Step: 12000, train/learning_rate: 3.572108471416868e-05
Step: 12000, train/epoch: 2.855782985687256
Step: 12010, train/loss: 0.0
Step: 12010, train/grad_norm: 0.005087314173579216
Step: 12010, train/learning_rate: 3.5709184885490686e-05
Step: 12010, train/epoch: 2.8581628799438477
Step: 12020, train/loss: 0.0
Step: 12020, train/grad_norm: 4.1866882384056225e-05
Step: 12020, train/learning_rate: 3.5697288694791496e-05
Step: 12020, train/epoch: 2.8605425357818604
Step: 12030, train/loss: 0.0
Step: 12030, train/grad_norm: 1.5301468181405653e-07
Step: 12030, train/learning_rate: 3.56853888661135e-05
Step: 12030, train/epoch: 2.862922430038452
Step: 12040, train/loss: 0.0
Step: 12040, train/grad_norm: 0.0028916026931256056
Step: 12040, train/learning_rate: 3.56734890374355e-05
Step: 12040, train/epoch: 2.865302324295044
Step: 12050, train/loss: 0.09109999984502792
Step: 12050, train/grad_norm: 1.406043139695612e-07
Step: 12050, train/learning_rate: 3.5661589208757505e-05
Step: 12050, train/epoch: 2.8676819801330566
Step: 12060, train/loss: 0.0
Step: 12060, train/grad_norm: 4.081029601366026e-06
Step: 12060, train/learning_rate: 3.564968938007951e-05
Step: 12060, train/epoch: 2.8700618743896484
Step: 12070, train/loss: 0.021199999377131462
Step: 12070, train/grad_norm: 3.0906310257705627e-06
Step: 12070, train/learning_rate: 3.563779318938032e-05
Step: 12070, train/epoch: 2.8724417686462402
Step: 12080, train/loss: 0.0
Step: 12080, train/grad_norm: 0.0001818724995246157
Step: 12080, train/learning_rate: 3.562589336070232e-05
Step: 12080, train/epoch: 2.874821424484253
Step: 12090, train/loss: 0.0
Step: 12090, train/grad_norm: 3.2149833714356646e-05
Step: 12090, train/learning_rate: 3.5613993532024324e-05
Step: 12090, train/epoch: 2.8772013187408447
Step: 12100, train/loss: 0.16019999980926514
Step: 12100, train/grad_norm: 0.0006490833475254476
Step: 12100, train/learning_rate: 3.560209370334633e-05
Step: 12100, train/epoch: 2.8795812129974365
Step: 12110, train/loss: 9.999999747378752e-05
Step: 12110, train/grad_norm: 0.0052977814339101315
Step: 12110, train/learning_rate: 3.559019387466833e-05
Step: 12110, train/epoch: 2.881960868835449
Step: 12120, train/loss: 9.999999747378752e-05
Step: 12120, train/grad_norm: 0.00262677320279181
Step: 12120, train/learning_rate: 3.557829768396914e-05
Step: 12120, train/epoch: 2.884340763092041
Step: 12130, train/loss: 9.999999747378752e-05
Step: 12130, train/grad_norm: 1.7002003005472943e-05
Step: 12130, train/learning_rate: 3.556639785529114e-05
Step: 12130, train/epoch: 2.886720657348633
Step: 12140, train/loss: 9.999999747378752e-05
Step: 12140, train/grad_norm: 0.13935299217700958
Step: 12140, train/learning_rate: 3.5554498026613146e-05
Step: 12140, train/epoch: 2.8891003131866455
Step: 12150, train/loss: 0.0
Step: 12150, train/grad_norm: 0.00014280318282544613
Step: 12150, train/learning_rate: 3.554259819793515e-05
Step: 12150, train/epoch: 2.8914802074432373
Step: 12160, train/loss: 0.010700000450015068
Step: 12160, train/grad_norm: 2.603683810775692e-07
Step: 12160, train/learning_rate: 3.553069836925715e-05
Step: 12160, train/epoch: 2.893860101699829
Step: 12170, train/loss: 0.0
Step: 12170, train/grad_norm: 1.0716888027673122e-05
Step: 12170, train/learning_rate: 3.551880217855796e-05
Step: 12170, train/epoch: 2.896239995956421
Step: 12180, train/loss: 0.001500000013038516
Step: 12180, train/grad_norm: 0.0003415858082007617
Step: 12180, train/learning_rate: 3.5506902349879965e-05
Step: 12180, train/epoch: 2.8986196517944336
Step: 12190, train/loss: 0.004100000020116568
Step: 12190, train/grad_norm: 3.4432305255904794e-05
Step: 12190, train/learning_rate: 3.549500252120197e-05
Step: 12190, train/epoch: 2.9009995460510254
Step: 12200, train/loss: 0.0
Step: 12200, train/grad_norm: 1.860309021139983e-05
Step: 12200, train/learning_rate: 3.548310269252397e-05
Step: 12200, train/epoch: 2.903379440307617
Step: 12210, train/loss: 0.018300000578165054
Step: 12210, train/grad_norm: 5.209770733927144e-06
Step: 12210, train/learning_rate: 3.5471202863845974e-05
Step: 12210, train/epoch: 2.90575909614563
Step: 12220, train/loss: 0.0
Step: 12220, train/grad_norm: 3.146897506667301e-05
Step: 12220, train/learning_rate: 3.5459306673146784e-05
Step: 12220, train/epoch: 2.9081389904022217
Step: 12230, train/loss: 0.2759999930858612
Step: 12230, train/grad_norm: 112.43473052978516
Step: 12230, train/learning_rate: 3.544740684446879e-05
Step: 12230, train/epoch: 2.9105188846588135
Step: 12240, train/loss: 0.00019999999494757503
Step: 12240, train/grad_norm: 3.479073939161026e-06
Step: 12240, train/learning_rate: 3.543550701579079e-05
Step: 12240, train/epoch: 2.912898540496826
Step: 12250, train/loss: 0.0
Step: 12250, train/grad_norm: 1.2900359251943883e-05
Step: 12250, train/learning_rate: 3.542360718711279e-05
Step: 12250, train/epoch: 2.915278434753418
Step: 12260, train/loss: 0.0
Step: 12260, train/grad_norm: 6.4422329160152e-07
Step: 12260, train/learning_rate: 3.5411707358434796e-05
Step: 12260, train/epoch: 2.9176583290100098
Step: 12270, train/loss: 0.09179999679327011
Step: 12270, train/grad_norm: 0.005927924066781998
Step: 12270, train/learning_rate: 3.5399811167735606e-05
Step: 12270, train/epoch: 2.9200379848480225
Step: 12280, train/loss: 0.007499999832361937
Step: 12280, train/grad_norm: 0.0003240907099097967
Step: 12280, train/learning_rate: 3.538791133905761e-05
Step: 12280, train/epoch: 2.9224178791046143
Step: 12290, train/loss: 0.0
Step: 12290, train/grad_norm: 5.897002120036632e-05
Step: 12290, train/learning_rate: 3.537601151037961e-05
Step: 12290, train/epoch: 2.924797773361206
Step: 12300, train/loss: 0.0003000000142492354
Step: 12300, train/grad_norm: 1.0897750826188712e-06
Step: 12300, train/learning_rate: 3.5364111681701615e-05
Step: 12300, train/epoch: 2.9271774291992188
Step: 12310, train/loss: 0.09139999747276306
Step: 12310, train/grad_norm: 4.296295230687974e-08
Step: 12310, train/learning_rate: 3.535221185302362e-05
Step: 12310, train/epoch: 2.9295573234558105
Step: 12320, train/loss: 0.0
Step: 12320, train/grad_norm: 0.00017213991668540984
Step: 12320, train/learning_rate: 3.534031566232443e-05
Step: 12320, train/epoch: 2.9319372177124023
Step: 12330, train/loss: 0.0
Step: 12330, train/grad_norm: 6.590322954025396e-08
Step: 12330, train/learning_rate: 3.532841583364643e-05
Step: 12330, train/epoch: 2.934316873550415
Step: 12340, train/loss: 0.0
Step: 12340, train/grad_norm: 1.4285697602645087e-07
Step: 12340, train/learning_rate: 3.5316516004968435e-05
Step: 12340, train/epoch: 2.936696767807007
Step: 12350, train/loss: 0.0
Step: 12350, train/grad_norm: 1.6048537077040237e-07
Step: 12350, train/learning_rate: 3.530461617629044e-05
Step: 12350, train/epoch: 2.9390766620635986
Step: 12360, train/loss: 0.0
Step: 12360, train/grad_norm: 4.930262065272473e-08
Step: 12360, train/learning_rate: 3.529271634761244e-05
Step: 12360, train/epoch: 2.9414565563201904
Step: 12370, train/loss: 0.0
Step: 12370, train/grad_norm: 1.3935062952441513e-06
Step: 12370, train/learning_rate: 3.528082015691325e-05
Step: 12370, train/epoch: 2.943836212158203
Step: 12380, train/loss: 0.0
Step: 12380, train/grad_norm: 1.2148329631145316e-07
Step: 12380, train/learning_rate: 3.5268920328235254e-05
Step: 12380, train/epoch: 2.946216106414795
Step: 12390, train/loss: 0.0
Step: 12390, train/grad_norm: 2.451678255965817e-06
Step: 12390, train/learning_rate: 3.525702049955726e-05
Step: 12390, train/epoch: 2.9485960006713867
Step: 12400, train/loss: 0.0
Step: 12400, train/grad_norm: 3.2347063552151667e-06
Step: 12400, train/learning_rate: 3.524512067087926e-05
Step: 12400, train/epoch: 2.9509756565093994
Step: 12410, train/loss: 0.0
Step: 12410, train/grad_norm: 6.213633128027141e-07
Step: 12410, train/learning_rate: 3.523322084220126e-05
Step: 12410, train/epoch: 2.953355550765991
Step: 12420, train/loss: 0.0
Step: 12420, train/grad_norm: 0.00015923363389447331
Step: 12420, train/learning_rate: 3.522132465150207e-05
Step: 12420, train/epoch: 2.955735445022583
Step: 12430, train/loss: 0.0
Step: 12430, train/grad_norm: 4.5936038077343255e-05
Step: 12430, train/learning_rate: 3.5209424822824076e-05
Step: 12430, train/epoch: 2.9581151008605957
Step: 12440, train/loss: 0.0
Step: 12440, train/grad_norm: 8.650953304822906e-07
Step: 12440, train/learning_rate: 3.519752499414608e-05
Step: 12440, train/epoch: 2.9604949951171875
Step: 12450, train/loss: 0.020999999716877937
Step: 12450, train/grad_norm: 8.43284487928031e-06
Step: 12450, train/learning_rate: 3.518562516546808e-05
Step: 12450, train/epoch: 2.9628748893737793
Step: 12460, train/loss: 0.017400000244379044
Step: 12460, train/grad_norm: 0.003590776352211833
Step: 12460, train/learning_rate: 3.5173725336790085e-05
Step: 12460, train/epoch: 2.965254545211792
Step: 12470, train/loss: 0.004900000058114529
Step: 12470, train/grad_norm: 5.862296893610619e-05
Step: 12470, train/learning_rate: 3.5161829146090895e-05
Step: 12470, train/epoch: 2.967634439468384
Step: 12480, train/loss: 0.0
Step: 12480, train/grad_norm: 0.0010900358902290463
Step: 12480, train/learning_rate: 3.51499293174129e-05
Step: 12480, train/epoch: 2.9700143337249756
Step: 12490, train/loss: 0.0
Step: 12490, train/grad_norm: 8.238055215770146e-07
Step: 12490, train/learning_rate: 3.51380294887349e-05
Step: 12490, train/epoch: 2.9723939895629883
Step: 12500, train/loss: 0.0010999999940395355
Step: 12500, train/grad_norm: 1.8353772190948803e-07
Step: 12500, train/learning_rate: 3.5126129660056904e-05
Step: 12500, train/epoch: 2.97477388381958
Step: 12510, train/loss: 0.0
Step: 12510, train/grad_norm: 6.846334343890703e-08
Step: 12510, train/learning_rate: 3.511422983137891e-05
Step: 12510, train/epoch: 2.977153778076172
Step: 12520, train/loss: 0.011800000444054604
Step: 12520, train/grad_norm: 5.604166290140711e-06
Step: 12520, train/learning_rate: 3.510233364067972e-05
Step: 12520, train/epoch: 2.9795336723327637
Step: 12530, train/loss: 0.0012000000569969416
Step: 12530, train/grad_norm: 2.8070706321159378e-05
Step: 12530, train/learning_rate: 3.509043381200172e-05
Step: 12530, train/epoch: 2.9819133281707764
Step: 12540, train/loss: 0.0
Step: 12540, train/grad_norm: 7.391902272502193e-07
Step: 12540, train/learning_rate: 3.507853398332372e-05
Step: 12540, train/epoch: 2.984293222427368
Step: 12550, train/loss: 0.0
Step: 12550, train/grad_norm: 1.7788828188258776e-08
Step: 12550, train/learning_rate: 3.5066634154645726e-05
Step: 12550, train/epoch: 2.98667311668396
Step: 12560, train/loss: 0.0
Step: 12560, train/grad_norm: 4.40904768765904e-05
Step: 12560, train/learning_rate: 3.505473432596773e-05
Step: 12560, train/epoch: 2.9890527725219727
Step: 12570, train/loss: 0.00559999980032444
Step: 12570, train/grad_norm: 3.285002549091587e-06
Step: 12570, train/learning_rate: 3.504283813526854e-05
Step: 12570, train/epoch: 2.9914326667785645
Step: 12580, train/loss: 0.0
Step: 12580, train/grad_norm: 5.271425251862638e-08
Step: 12580, train/learning_rate: 3.503093830659054e-05
Step: 12580, train/epoch: 2.9938125610351562
Step: 12590, train/loss: 0.0
Step: 12590, train/grad_norm: 1.8037566107409475e-08
Step: 12590, train/learning_rate: 3.5019038477912545e-05
Step: 12590, train/epoch: 2.996192216873169
Step: 12600, train/loss: 0.0
Step: 12600, train/grad_norm: 9.587414417921991e-09
Step: 12600, train/learning_rate: 3.500713864923455e-05
Step: 12600, train/epoch: 2.9985721111297607
Step: 12606, eval/loss: 0.053543247282505035
Step: 12606, eval/accuracy: 0.9943079352378845
Step: 12606, eval/f1: 0.9939951300621033
Step: 12606, eval/runtime: 708.0709228515625
Step: 12606, eval/samples_per_second: 10.17300033569336
Step: 12606, eval/steps_per_second: 1.2719999551773071
Step: 12606, train/epoch: 3.0
Step: 12610, train/loss: 0.0
Step: 12610, train/grad_norm: 1.2105393132344489e-08
Step: 12610, train/learning_rate: 3.499523882055655e-05
Step: 12610, train/epoch: 3.0009520053863525
Step: 12620, train/loss: 0.0
Step: 12620, train/grad_norm: 2.273270816033346e-08
Step: 12620, train/learning_rate: 3.498334262985736e-05
Step: 12620, train/epoch: 3.0033316612243652
Step: 12630, train/loss: 0.0
Step: 12630, train/grad_norm: 4.905153971890286e-08
Step: 12630, train/learning_rate: 3.4971442801179364e-05
Step: 12630, train/epoch: 3.005711555480957
Step: 12640, train/loss: 0.0
Step: 12640, train/grad_norm: 1.6287657889080265e-09
Step: 12640, train/learning_rate: 3.495954297250137e-05
Step: 12640, train/epoch: 3.008091449737549
Step: 12650, train/loss: 0.0003000000142492354
Step: 12650, train/grad_norm: 3.471529453236144e-06
Step: 12650, train/learning_rate: 3.494764314382337e-05
Step: 12650, train/epoch: 3.0104711055755615
Step: 12660, train/loss: 0.0
Step: 12660, train/grad_norm: 1.0417847917665313e-08
Step: 12660, train/learning_rate: 3.493574331514537e-05
Step: 12660, train/epoch: 3.0128509998321533
Step: 12670, train/loss: 0.0
Step: 12670, train/grad_norm: 1.0519007886955478e-08
Step: 12670, train/learning_rate: 3.4923847124446183e-05
Step: 12670, train/epoch: 3.015230894088745
Step: 12680, train/loss: 0.0010999999940395355
Step: 12680, train/grad_norm: 2.5322010515083093e-06
Step: 12680, train/learning_rate: 3.4911947295768186e-05
Step: 12680, train/epoch: 3.017610549926758
Step: 12690, train/loss: 0.06800000369548798
Step: 12690, train/grad_norm: 1.532418139049696e-07
Step: 12690, train/learning_rate: 3.490004746709019e-05
Step: 12690, train/epoch: 3.0199904441833496
Step: 12700, train/loss: 0.08980000019073486
Step: 12700, train/grad_norm: 2.7437901735538617e-05
Step: 12700, train/learning_rate: 3.488814763841219e-05
Step: 12700, train/epoch: 3.0223703384399414
Step: 12710, train/loss: 0.0
Step: 12710, train/grad_norm: 0.08387386053800583
Step: 12710, train/learning_rate: 3.4876247809734195e-05
Step: 12710, train/epoch: 3.024750232696533
Step: 12720, train/loss: 0.00019999999494757503
Step: 12720, train/grad_norm: 0.00026203534798696637
Step: 12720, train/learning_rate: 3.4864351619035006e-05
Step: 12720, train/epoch: 3.027129888534546
Step: 12730, train/loss: 0.0
Step: 12730, train/grad_norm: 5.131817033543484e-06
Step: 12730, train/learning_rate: 3.485245179035701e-05
Step: 12730, train/epoch: 3.0295097827911377
Step: 12740, train/loss: 0.0
Step: 12740, train/grad_norm: 0.2647784352302551
Step: 12740, train/learning_rate: 3.484055196167901e-05
Step: 12740, train/epoch: 3.0318896770477295
Step: 12750, train/loss: 0.0
Step: 12750, train/grad_norm: 2.2737588096788386e-06
Step: 12750, train/learning_rate: 3.4828652133001015e-05
Step: 12750, train/epoch: 3.034269332885742
Step: 12760, train/loss: 0.0
Step: 12760, train/grad_norm: 3.624786915068512e-09
Step: 12760, train/learning_rate: 3.481675230432302e-05
Step: 12760, train/epoch: 3.036649227142334
Step: 12770, train/loss: 0.0
Step: 12770, train/grad_norm: 0.00010171665053348988
Step: 12770, train/learning_rate: 3.480485611362383e-05
Step: 12770, train/epoch: 3.039029121398926
Step: 12780, train/loss: 0.0
Step: 12780, train/grad_norm: 3.324101271573454e-05
Step: 12780, train/learning_rate: 3.479295628494583e-05
Step: 12780, train/epoch: 3.0414087772369385
Step: 12790, train/loss: 0.0
Step: 12790, train/grad_norm: 2.027390699765874e-09
Step: 12790, train/learning_rate: 3.4781056456267834e-05
Step: 12790, train/epoch: 3.0437886714935303
Step: 12800, train/loss: 0.0
Step: 12800, train/grad_norm: 2.9266957426443696e-06
Step: 12800, train/learning_rate: 3.476915662758984e-05
Step: 12800, train/epoch: 3.046168565750122
Step: 12810, train/loss: 0.0
Step: 12810, train/grad_norm: 2.496550223440863e-06
Step: 12810, train/learning_rate: 3.475725679891184e-05
Step: 12810, train/epoch: 3.0485482215881348
Step: 12820, train/loss: 0.0
Step: 12820, train/grad_norm: 6.783511253161123e-06
Step: 12820, train/learning_rate: 3.474536060821265e-05
Step: 12820, train/epoch: 3.0509281158447266
Step: 12830, train/loss: 0.0
Step: 12830, train/grad_norm: 7.267312321346253e-05
Step: 12830, train/learning_rate: 3.473346077953465e-05
Step: 12830, train/epoch: 3.0533080101013184
Step: 12840, train/loss: 0.0
Step: 12840, train/grad_norm: 2.9390942017926136e-06
Step: 12840, train/learning_rate: 3.4721560950856656e-05
Step: 12840, train/epoch: 3.055687665939331
Step: 12850, train/loss: 0.0
Step: 12850, train/grad_norm: 1.5128099661865235e-08
Step: 12850, train/learning_rate: 3.470966112217866e-05
Step: 12850, train/epoch: 3.058067560195923
Step: 12860, train/loss: 0.0
Step: 12860, train/grad_norm: 2.1253713100577443e-07
Step: 12860, train/learning_rate: 3.469776129350066e-05
Step: 12860, train/epoch: 3.0604474544525146
Step: 12870, train/loss: 0.0
Step: 12870, train/grad_norm: 8.552317012799904e-05
Step: 12870, train/learning_rate: 3.468586510280147e-05
Step: 12870, train/epoch: 3.0628271102905273
Step: 12880, train/loss: 0.0
Step: 12880, train/grad_norm: 2.575261760284775e-06
Step: 12880, train/learning_rate: 3.4673965274123475e-05
Step: 12880, train/epoch: 3.065207004547119
Step: 12890, train/loss: 0.0
Step: 12890, train/grad_norm: 7.144824962779239e-07
Step: 12890, train/learning_rate: 3.466206544544548e-05
Step: 12890, train/epoch: 3.067586898803711
Step: 12900, train/loss: 0.0
Step: 12900, train/grad_norm: 5.2755257939907096e-08
Step: 12900, train/learning_rate: 3.465016561676748e-05
Step: 12900, train/epoch: 3.0699667930603027
Step: 12910, train/loss: 0.0
Step: 12910, train/grad_norm: 7.254930096678436e-07
Step: 12910, train/learning_rate: 3.4638265788089484e-05
Step: 12910, train/epoch: 3.0723464488983154
Step: 12920, train/loss: 0.0
Step: 12920, train/grad_norm: 1.4827274696926906e-07
Step: 12920, train/learning_rate: 3.4626369597390294e-05
Step: 12920, train/epoch: 3.0747263431549072
Step: 12930, train/loss: 0.0
Step: 12930, train/grad_norm: 0.00026882815291173756
Step: 12930, train/learning_rate: 3.46144697687123e-05
Step: 12930, train/epoch: 3.077106237411499
Step: 12940, train/loss: 0.0
Step: 12940, train/grad_norm: 6.123462299001403e-06
Step: 12940, train/learning_rate: 3.46025699400343e-05
Step: 12940, train/epoch: 3.0794858932495117
Step: 12950, train/loss: 0.0
Step: 12950, train/grad_norm: 2.1609137945688417e-07
Step: 12950, train/learning_rate: 3.45906701113563e-05
Step: 12950, train/epoch: 3.0818657875061035
Step: 12960, train/loss: 0.0
Step: 12960, train/grad_norm: 0.00020844525715801865
Step: 12960, train/learning_rate: 3.4578770282678306e-05
Step: 12960, train/epoch: 3.0842456817626953
Step: 12970, train/loss: 0.0
Step: 12970, train/grad_norm: 1.01916266430635e-05
Step: 12970, train/learning_rate: 3.4566874091979116e-05
Step: 12970, train/epoch: 3.086625337600708
Step: 12980, train/loss: 0.0
Step: 12980, train/grad_norm: 0.03346643224358559
Step: 12980, train/learning_rate: 3.455497426330112e-05
Step: 12980, train/epoch: 3.0890052318573
Step: 12990, train/loss: 9.999999747378752e-05
Step: 12990, train/grad_norm: 8.097035606624559e-05
Step: 12990, train/learning_rate: 3.454307443462312e-05
Step: 12990, train/epoch: 3.0913851261138916
Step: 13000, train/loss: 0.0
Step: 13000, train/grad_norm: 0.0006721802055835724
Step: 13000, train/learning_rate: 3.4531174605945125e-05
Step: 13000, train/epoch: 3.0937647819519043
Step: 13010, train/loss: 0.002300000051036477
Step: 13010, train/grad_norm: 0.00010883676441153511
Step: 13010, train/learning_rate: 3.451927477726713e-05
Step: 13010, train/epoch: 3.096144676208496
Step: 13020, train/loss: 0.0
Step: 13020, train/grad_norm: 2.628497597356727e-08
Step: 13020, train/learning_rate: 3.450737858656794e-05
Step: 13020, train/epoch: 3.098524570465088
Step: 13030, train/loss: 0.1023000031709671
Step: 13030, train/grad_norm: 51.71421432495117
Step: 13030, train/learning_rate: 3.449547875788994e-05
Step: 13030, train/epoch: 3.1009042263031006
Step: 13040, train/loss: 0.02410000003874302
Step: 13040, train/grad_norm: 1.2063739632139914e-05
Step: 13040, train/learning_rate: 3.4483578929211944e-05
Step: 13040, train/epoch: 3.1032841205596924
Step: 13050, train/loss: 0.0
Step: 13050, train/grad_norm: 6.593719490410876e-07
Step: 13050, train/learning_rate: 3.447167910053395e-05
Step: 13050, train/epoch: 3.105664014816284
Step: 13060, train/loss: 0.0
Step: 13060, train/grad_norm: 8.394443284487352e-06
Step: 13060, train/learning_rate: 3.445977927185595e-05
Step: 13060, train/epoch: 3.108043670654297
Step: 13070, train/loss: 0.0
Step: 13070, train/grad_norm: 4.014123078377452e-06
Step: 13070, train/learning_rate: 3.444788308115676e-05
Step: 13070, train/epoch: 3.1104235649108887
Step: 13080, train/loss: 0.0
Step: 13080, train/grad_norm: 4.200998660053301e-07
Step: 13080, train/learning_rate: 3.4435983252478763e-05
Step: 13080, train/epoch: 3.1128034591674805
Step: 13090, train/loss: 0.00019999999494757503
Step: 13090, train/grad_norm: 0.39352044463157654
Step: 13090, train/learning_rate: 3.4424083423800766e-05
Step: 13090, train/epoch: 3.1151833534240723
Step: 13100, train/loss: 0.0
Step: 13100, train/grad_norm: 1.6860483009750737e-09
Step: 13100, train/learning_rate: 3.441218359512277e-05
Step: 13100, train/epoch: 3.117563009262085
Step: 13110, train/loss: 0.0
Step: 13110, train/grad_norm: 2.772109382931376e-06
Step: 13110, train/learning_rate: 3.440028376644477e-05
Step: 13110, train/epoch: 3.1199429035186768
Step: 13120, train/loss: 0.0
Step: 13120, train/grad_norm: 3.89819160773186e-06
Step: 13120, train/learning_rate: 3.438838757574558e-05
Step: 13120, train/epoch: 3.1223227977752686
Step: 13130, train/loss: 0.0
Step: 13130, train/grad_norm: 1.6400983327002905e-07
Step: 13130, train/learning_rate: 3.4376487747067586e-05
Step: 13130, train/epoch: 3.1247024536132812
Step: 13140, train/loss: 0.0
Step: 13140, train/grad_norm: 7.494846698818947e-08
Step: 13140, train/learning_rate: 3.436458791838959e-05
Step: 13140, train/epoch: 3.127082347869873
Step: 13150, train/loss: 0.0003000000142492354
Step: 13150, train/grad_norm: 1.3563320067078166e-07
Step: 13150, train/learning_rate: 3.435268808971159e-05
Step: 13150, train/epoch: 3.129462242126465
Step: 13160, train/loss: 0.0
Step: 13160, train/grad_norm: 5.877272087673191e-06
Step: 13160, train/learning_rate: 3.43407918990124e-05
Step: 13160, train/epoch: 3.1318418979644775
Step: 13170, train/loss: 9.999999747378752e-05
Step: 13170, train/grad_norm: 5.3498959459830076e-05
Step: 13170, train/learning_rate: 3.4328892070334405e-05
Step: 13170, train/epoch: 3.1342217922210693
Step: 13180, train/loss: 0.0
Step: 13180, train/grad_norm: 1.3051558198640123e-06
Step: 13180, train/learning_rate: 3.431699224165641e-05
Step: 13180, train/epoch: 3.136601686477661
Step: 13190, train/loss: 0.06759999692440033
Step: 13190, train/grad_norm: 2.4312768687195785e-07
Step: 13190, train/learning_rate: 3.430509241297841e-05
Step: 13190, train/epoch: 3.138981342315674
Step: 13200, train/loss: 0.0
Step: 13200, train/grad_norm: 0.002439035102725029
Step: 13200, train/learning_rate: 3.4293192584300414e-05
Step: 13200, train/epoch: 3.1413612365722656
Step: 13210, train/loss: 0.011099999770522118
Step: 13210, train/grad_norm: 3.578768836831614e-09
Step: 13210, train/learning_rate: 3.4281296393601224e-05
Step: 13210, train/epoch: 3.1437411308288574
Step: 13220, train/loss: 0.0
Step: 13220, train/grad_norm: 2.559261815804348e-07
Step: 13220, train/learning_rate: 3.426939656492323e-05
Step: 13220, train/epoch: 3.14612078666687
Step: 13230, train/loss: 0.0020000000949949026
Step: 13230, train/grad_norm: 6.186166956467787e-08
Step: 13230, train/learning_rate: 3.425749673624523e-05
Step: 13230, train/epoch: 3.148500680923462
Step: 13240, train/loss: 0.02500000037252903
Step: 13240, train/grad_norm: 2.227687400591094e-05
Step: 13240, train/learning_rate: 3.424559690756723e-05
Step: 13240, train/epoch: 3.1508805751800537
Step: 13250, train/loss: 0.0
Step: 13250, train/grad_norm: 1.6561359927891317e-07
Step: 13250, train/learning_rate: 3.4233697078889236e-05
Step: 13250, train/epoch: 3.1532604694366455
Step: 13260, train/loss: 0.0
Step: 13260, train/grad_norm: 6.463830573011364e-07
Step: 13260, train/learning_rate: 3.4221800888190046e-05
Step: 13260, train/epoch: 3.155640125274658
Step: 13270, train/loss: 0.0
Step: 13270, train/grad_norm: 5.113732299832918e-07
Step: 13270, train/learning_rate: 3.420990105951205e-05
Step: 13270, train/epoch: 3.15802001953125
Step: 13280, train/loss: 0.00039999998989515007
Step: 13280, train/grad_norm: 8.496922276890473e-09
Step: 13280, train/learning_rate: 3.419800123083405e-05
Step: 13280, train/epoch: 3.160399913787842
Step: 13290, train/loss: 0.0
Step: 13290, train/grad_norm: 6.898851097503211e-06
Step: 13290, train/learning_rate: 3.4186101402156055e-05
Step: 13290, train/epoch: 3.1627795696258545
Step: 13300, train/loss: 0.0
Step: 13300, train/grad_norm: 0.008142462931573391
Step: 13300, train/learning_rate: 3.417420157347806e-05
Step: 13300, train/epoch: 3.1651594638824463
Step: 13310, train/loss: 0.0
Step: 13310, train/grad_norm: 1.2148379937571008e-05
Step: 13310, train/learning_rate: 3.416230538277887e-05
Step: 13310, train/epoch: 3.167539358139038
Step: 13320, train/loss: 0.0
Step: 13320, train/grad_norm: 2.0805218525765667e-07
Step: 13320, train/learning_rate: 3.415040555410087e-05
Step: 13320, train/epoch: 3.169919013977051
Step: 13330, train/loss: 0.0
Step: 13330, train/grad_norm: 7.249587952173897e-07
Step: 13330, train/learning_rate: 3.4138505725422874e-05
Step: 13330, train/epoch: 3.1722989082336426
Step: 13340, train/loss: 0.00019999999494757503
Step: 13340, train/grad_norm: 3.654552926946053e-07
Step: 13340, train/learning_rate: 3.412660589674488e-05
Step: 13340, train/epoch: 3.1746788024902344
Step: 13350, train/loss: 0.0
Step: 13350, train/grad_norm: 5.0571800080945195e-09
Step: 13350, train/learning_rate: 3.411470606806688e-05
Step: 13350, train/epoch: 3.177058458328247
Step: 13360, train/loss: 0.0
Step: 13360, train/grad_norm: 2.094485251902256e-09
Step: 13360, train/learning_rate: 3.410280987736769e-05
Step: 13360, train/epoch: 3.179438352584839
Step: 13370, train/loss: 0.0
Step: 13370, train/grad_norm: 0.0013129315339028835
Step: 13370, train/learning_rate: 3.409091004868969e-05
Step: 13370, train/epoch: 3.1818182468414307
Step: 13380, train/loss: 0.28439998626708984
Step: 13380, train/grad_norm: 51.7967529296875
Step: 13380, train/learning_rate: 3.4079010220011696e-05
Step: 13380, train/epoch: 3.1841979026794434
Step: 13390, train/loss: 0.0
Step: 13390, train/grad_norm: 1.6066653074631176e-07
Step: 13390, train/learning_rate: 3.40671103913337e-05
Step: 13390, train/epoch: 3.186577796936035
Step: 13400, train/loss: 0.0
Step: 13400, train/grad_norm: 2.709270665945951e-05
Step: 13400, train/learning_rate: 3.40552105626557e-05
Step: 13400, train/epoch: 3.188957691192627
Step: 13410, train/loss: 0.0
Step: 13410, train/grad_norm: 2.779337364700041e-07
Step: 13410, train/learning_rate: 3.404331437195651e-05
Step: 13410, train/epoch: 3.1913373470306396
Step: 13420, train/loss: 0.0
Step: 13420, train/grad_norm: 0.00011506367445690557
Step: 13420, train/learning_rate: 3.4031414543278515e-05
Step: 13420, train/epoch: 3.1937172412872314
Step: 13430, train/loss: 0.0
Step: 13430, train/grad_norm: 9.102597687160596e-06
Step: 13430, train/learning_rate: 3.401951471460052e-05
Step: 13430, train/epoch: 3.1960971355438232
Step: 13440, train/loss: 0.0
Step: 13440, train/grad_norm: 1.093196351575898e-05
Step: 13440, train/learning_rate: 3.400761488592252e-05
Step: 13440, train/epoch: 3.198477029800415
Step: 13450, train/loss: 0.026599999517202377
Step: 13450, train/grad_norm: 2.47971229327959e-07
Step: 13450, train/learning_rate: 3.3995715057244524e-05
Step: 13450, train/epoch: 3.2008566856384277
Step: 13460, train/loss: 0.0
Step: 13460, train/grad_norm: 1.1076956980105024e-05
Step: 13460, train/learning_rate: 3.3983818866545334e-05
Step: 13460, train/epoch: 3.2032365798950195
Step: 13470, train/loss: 0.020999999716877937
Step: 13470, train/grad_norm: 5.6479806517018005e-05
Step: 13470, train/learning_rate: 3.397191903786734e-05
Step: 13470, train/epoch: 3.2056164741516113
Step: 13480, train/loss: 0.0
Step: 13480, train/grad_norm: 2.2745462047168985e-07
Step: 13480, train/learning_rate: 3.396001920918934e-05
Step: 13480, train/epoch: 3.207996129989624
Step: 13490, train/loss: 9.999999747378752e-05
Step: 13490, train/grad_norm: 0.35300302505493164
Step: 13490, train/learning_rate: 3.3948119380511343e-05
Step: 13490, train/epoch: 3.210376024246216
Step: 13500, train/loss: 0.0
Step: 13500, train/grad_norm: 2.266709543619072e-06
Step: 13500, train/learning_rate: 3.3936219551833346e-05
Step: 13500, train/epoch: 3.2127559185028076
Step: 13510, train/loss: 0.0
Step: 13510, train/grad_norm: 3.308104226107389e-07
Step: 13510, train/learning_rate: 3.3924323361134157e-05
Step: 13510, train/epoch: 3.2151355743408203
Step: 13520, train/loss: 0.0
Step: 13520, train/grad_norm: 1.1497147170302924e-05
Step: 13520, train/learning_rate: 3.391242353245616e-05
Step: 13520, train/epoch: 3.217515468597412
Step: 13530, train/loss: 0.0
Step: 13530, train/grad_norm: 4.156373165642435e-07
Step: 13530, train/learning_rate: 3.390052370377816e-05
Step: 13530, train/epoch: 3.219895362854004
Step: 13540, train/loss: 0.0
Step: 13540, train/grad_norm: 3.8627266718549436e-08
Step: 13540, train/learning_rate: 3.3888623875100166e-05
Step: 13540, train/epoch: 3.2222750186920166
Step: 13550, train/loss: 0.0
Step: 13550, train/grad_norm: 5.873947884538211e-05
Step: 13550, train/learning_rate: 3.387672404642217e-05
Step: 13550, train/epoch: 3.2246549129486084
Step: 13560, train/loss: 0.00019999999494757503
Step: 13560, train/grad_norm: 7.155002812453404e-09
Step: 13560, train/learning_rate: 3.386482785572298e-05
Step: 13560, train/epoch: 3.2270348072052
Step: 13570, train/loss: 0.0
Step: 13570, train/grad_norm: 2.7979725928162225e-05
Step: 13570, train/learning_rate: 3.385292802704498e-05
Step: 13570, train/epoch: 3.229414463043213
Step: 13580, train/loss: 0.0
Step: 13580, train/grad_norm: 6.986599743186162e-08
Step: 13580, train/learning_rate: 3.3841028198366985e-05
Step: 13580, train/epoch: 3.2317943572998047
Step: 13590, train/loss: 0.0
Step: 13590, train/grad_norm: 2.571969446307776e-07
Step: 13590, train/learning_rate: 3.382912836968899e-05
Step: 13590, train/epoch: 3.2341742515563965
Step: 13600, train/loss: 0.009700000286102295
Step: 13600, train/grad_norm: 3.242959678573243e-08
Step: 13600, train/learning_rate: 3.381722854101099e-05
Step: 13600, train/epoch: 3.236553907394409
Step: 13610, train/loss: 0.0
Step: 13610, train/grad_norm: 9.485568597256133e-08
Step: 13610, train/learning_rate: 3.38053323503118e-05
Step: 13610, train/epoch: 3.238933801651001
Step: 13620, train/loss: 0.06639999896287918
Step: 13620, train/grad_norm: 1.0311813625207833e-08
Step: 13620, train/learning_rate: 3.3793432521633804e-05
Step: 13620, train/epoch: 3.2413136959075928
Step: 13630, train/loss: 0.0
Step: 13630, train/grad_norm: 0.036578401923179626
Step: 13630, train/learning_rate: 3.378153269295581e-05
Step: 13630, train/epoch: 3.2436935901641846
Step: 13640, train/loss: 0.0
Step: 13640, train/grad_norm: 3.332074527406803e-07
Step: 13640, train/learning_rate: 3.376963286427781e-05
Step: 13640, train/epoch: 3.2460732460021973
Step: 13650, train/loss: 0.08049999922513962
Step: 13650, train/grad_norm: 1.1542721040314063e-05
Step: 13650, train/learning_rate: 3.375773303559981e-05
Step: 13650, train/epoch: 3.248453140258789
Step: 13660, train/loss: 0.0
Step: 13660, train/grad_norm: 7.49001642930125e-08
Step: 13660, train/learning_rate: 3.374583684490062e-05
Step: 13660, train/epoch: 3.250833034515381
Step: 13670, train/loss: 0.0
Step: 13670, train/grad_norm: 9.784794485767634e-08
Step: 13670, train/learning_rate: 3.3733937016222626e-05
Step: 13670, train/epoch: 3.2532126903533936
Step: 13680, train/loss: 0.0
Step: 13680, train/grad_norm: 2.4360506944987037e-09
Step: 13680, train/learning_rate: 3.372203718754463e-05
Step: 13680, train/epoch: 3.2555925846099854
Step: 13690, train/loss: 0.0
Step: 13690, train/grad_norm: 2.3869290544098476e-07
Step: 13690, train/learning_rate: 3.371013735886663e-05
Step: 13690, train/epoch: 3.257972478866577
Step: 13700, train/loss: 0.0
Step: 13700, train/grad_norm: 1.3499539264216764e-08
Step: 13700, train/learning_rate: 3.3698237530188635e-05
Step: 13700, train/epoch: 3.26035213470459
Step: 13710, train/loss: 0.0
Step: 13710, train/grad_norm: 5.338355322237476e-07
Step: 13710, train/learning_rate: 3.3686341339489445e-05
Step: 13710, train/epoch: 3.2627320289611816
Step: 13720, train/loss: 0.00019999999494757503
Step: 13720, train/grad_norm: 2.4511745522204365e-08
Step: 13720, train/learning_rate: 3.367444151081145e-05
Step: 13720, train/epoch: 3.2651119232177734
Step: 13730, train/loss: 0.0
Step: 13730, train/grad_norm: 9.312896622759581e-07
Step: 13730, train/learning_rate: 3.366254168213345e-05
Step: 13730, train/epoch: 3.267491579055786
Step: 13740, train/loss: 0.0
Step: 13740, train/grad_norm: 1.2773655555520236e-07
Step: 13740, train/learning_rate: 3.3650641853455454e-05
Step: 13740, train/epoch: 3.269871473312378
Step: 13750, train/loss: 0.0
Step: 13750, train/grad_norm: 3.078716170890061e-10
Step: 13750, train/learning_rate: 3.363874202477746e-05
Step: 13750, train/epoch: 3.2722513675689697
Step: 13760, train/loss: 0.0
Step: 13760, train/grad_norm: 1.9860701527818492e-08
Step: 13760, train/learning_rate: 3.362684583407827e-05
Step: 13760, train/epoch: 3.2746310234069824
Step: 13770, train/loss: 0.0
Step: 13770, train/grad_norm: 6.718199756505783e-07
Step: 13770, train/learning_rate: 3.361494600540027e-05
Step: 13770, train/epoch: 3.277010917663574
Step: 13780, train/loss: 0.0
Step: 13780, train/grad_norm: 6.69482517423603e-07
Step: 13780, train/learning_rate: 3.360304617672227e-05
Step: 13780, train/epoch: 3.279390811920166
Step: 13790, train/loss: 0.0
Step: 13790, train/grad_norm: 1.1074571375502273e-05
Step: 13790, train/learning_rate: 3.3591146348044276e-05
Step: 13790, train/epoch: 3.2817704677581787
Step: 13800, train/loss: 0.0
Step: 13800, train/grad_norm: 5.2818971418844285e-09
Step: 13800, train/learning_rate: 3.357924651936628e-05
Step: 13800, train/epoch: 3.2841503620147705
Step: 13810, train/loss: 0.0
Step: 13810, train/grad_norm: 6.555135314556537e-07
Step: 13810, train/learning_rate: 3.356735032866709e-05
Step: 13810, train/epoch: 3.2865302562713623
Step: 13820, train/loss: 0.0
Step: 13820, train/grad_norm: 2.793501652220698e-09
Step: 13820, train/learning_rate: 3.355545049998909e-05
Step: 13820, train/epoch: 3.288910150527954
Step: 13830, train/loss: 0.0
Step: 13830, train/grad_norm: 9.860179943643743e-07
Step: 13830, train/learning_rate: 3.3543550671311095e-05
Step: 13830, train/epoch: 3.291289806365967
Step: 13840, train/loss: 0.0
Step: 13840, train/grad_norm: 2.4413006372014934e-07
Step: 13840, train/learning_rate: 3.35316508426331e-05
Step: 13840, train/epoch: 3.2936697006225586
Step: 13850, train/loss: 0.0
Step: 13850, train/grad_norm: 9.444846682526986e-08
Step: 13850, train/learning_rate: 3.35197510139551e-05
Step: 13850, train/epoch: 3.2960495948791504
Step: 13860, train/loss: 0.0
Step: 13860, train/grad_norm: 0.0010407710215076804
Step: 13860, train/learning_rate: 3.350785482325591e-05
Step: 13860, train/epoch: 3.298429250717163
Step: 13870, train/loss: 0.0
Step: 13870, train/grad_norm: 1.120720072300685e-09
Step: 13870, train/learning_rate: 3.3495954994577914e-05
Step: 13870, train/epoch: 3.300809144973755
Step: 13880, train/loss: 0.0
Step: 13880, train/grad_norm: 8.614379209959111e-10
Step: 13880, train/learning_rate: 3.348405516589992e-05
Step: 13880, train/epoch: 3.3031890392303467
Step: 13890, train/loss: 0.0
Step: 13890, train/grad_norm: 5.504061846295372e-06
Step: 13890, train/learning_rate: 3.347215533722192e-05
Step: 13890, train/epoch: 3.3055686950683594
Step: 13900, train/loss: 0.0
Step: 13900, train/grad_norm: 1.1234050845132515e-07
Step: 13900, train/learning_rate: 3.3460255508543923e-05
Step: 13900, train/epoch: 3.307948589324951
Step: 13910, train/loss: 0.0
Step: 13910, train/grad_norm: 5.469040220695831e-10
Step: 13910, train/learning_rate: 3.3448359317844734e-05
Step: 13910, train/epoch: 3.310328483581543
Step: 13920, train/loss: 0.0
Step: 13920, train/grad_norm: 3.108898170012253e-07
Step: 13920, train/learning_rate: 3.3436459489166737e-05
Step: 13920, train/epoch: 3.3127081394195557
Step: 13930, train/loss: 0.0
Step: 13930, train/grad_norm: 2.661884764165734e-06
Step: 13930, train/learning_rate: 3.342455966048874e-05
Step: 13930, train/epoch: 3.3150880336761475
Step: 13940, train/loss: 0.0
Step: 13940, train/grad_norm: 5.082293910163571e-07
Step: 13940, train/learning_rate: 3.341265983181074e-05
Step: 13940, train/epoch: 3.3174679279327393
Step: 13950, train/loss: 0.0
Step: 13950, train/grad_norm: 6.871248956485942e-07
Step: 13950, train/learning_rate: 3.3400760003132746e-05
Step: 13950, train/epoch: 3.319847583770752
Step: 13960, train/loss: 0.0
Step: 13960, train/grad_norm: 1.6776535716189755e-10
Step: 13960, train/learning_rate: 3.3388863812433556e-05
Step: 13960, train/epoch: 3.3222274780273438
Step: 13970, train/loss: 0.0
Step: 13970, train/grad_norm: 6.603426694340442e-10
Step: 13970, train/learning_rate: 3.337696398375556e-05
Step: 13970, train/epoch: 3.3246073722839355
Step: 13980, train/loss: 0.0
Step: 13980, train/grad_norm: 4.151746452407679e-06
Step: 13980, train/learning_rate: 3.336506415507756e-05
Step: 13980, train/epoch: 3.3269872665405273
Step: 13990, train/loss: 0.0
Step: 13990, train/grad_norm: 2.1691937490686541e-07
Step: 13990, train/learning_rate: 3.3353164326399565e-05
Step: 13990, train/epoch: 3.32936692237854
Step: 14000, train/loss: 0.0
Step: 14000, train/grad_norm: 1.1299776669915218e-09
Step: 14000, train/learning_rate: 3.334126449772157e-05
Step: 14000, train/epoch: 3.331746816635132
Step: 14010, train/loss: 0.0
Step: 14010, train/grad_norm: 2.310082569678684e-09
Step: 14010, train/learning_rate: 3.332936830702238e-05
Step: 14010, train/epoch: 3.3341267108917236
Step: 14020, train/loss: 0.0
Step: 14020, train/grad_norm: 9.424665137203192e-08
Step: 14020, train/learning_rate: 3.331746847834438e-05
Step: 14020, train/epoch: 3.3365063667297363
Step: 14030, train/loss: 0.0
Step: 14030, train/grad_norm: 2.3052088238273427e-07
Step: 14030, train/learning_rate: 3.3305568649666384e-05
Step: 14030, train/epoch: 3.338886260986328
Step: 14040, train/loss: 0.0
Step: 14040, train/grad_norm: 9.410322521219427e-10
Step: 14040, train/learning_rate: 3.329366882098839e-05
Step: 14040, train/epoch: 3.34126615524292
Step: 14050, train/loss: 9.999999747378752e-05
Step: 14050, train/grad_norm: 8.681181884462319e-10
Step: 14050, train/learning_rate: 3.328176899231039e-05
Step: 14050, train/epoch: 3.3436458110809326
Step: 14060, train/loss: 0.0
Step: 14060, train/grad_norm: 1.7108925476350123e-07
Step: 14060, train/learning_rate: 3.32698728016112e-05
Step: 14060, train/epoch: 3.3460257053375244
Step: 14070, train/loss: 0.0
Step: 14070, train/grad_norm: 2.0069874153705314e-05
Step: 14070, train/learning_rate: 3.32579729729332e-05
Step: 14070, train/epoch: 3.348405599594116
Step: 14080, train/loss: 0.0012000000569969416
Step: 14080, train/grad_norm: 7.498454124288401e-07
Step: 14080, train/learning_rate: 3.3246073144255206e-05
Step: 14080, train/epoch: 3.350785255432129
Step: 14090, train/loss: 0.0
Step: 14090, train/grad_norm: 4.652582674680161e-08
Step: 14090, train/learning_rate: 3.323417331557721e-05
Step: 14090, train/epoch: 3.3531651496887207
Step: 14100, train/loss: 0.0
Step: 14100, train/grad_norm: 4.1005652207104504e-08
Step: 14100, train/learning_rate: 3.322227348689921e-05
Step: 14100, train/epoch: 3.3555450439453125
Step: 14110, train/loss: 0.0
Step: 14110, train/grad_norm: 8.040819011512212e-06
Step: 14110, train/learning_rate: 3.321037729620002e-05
Step: 14110, train/epoch: 3.357924699783325
Step: 14120, train/loss: 0.0
Step: 14120, train/grad_norm: 7.533215717892006e-10
Step: 14120, train/learning_rate: 3.3198477467522025e-05
Step: 14120, train/epoch: 3.360304594039917
Step: 14130, train/loss: 0.0
Step: 14130, train/grad_norm: 1.2124354498155299e-06
Step: 14130, train/learning_rate: 3.318657763884403e-05
Step: 14130, train/epoch: 3.362684488296509
Step: 14140, train/loss: 0.0
Step: 14140, train/grad_norm: 1.2918152606289368e-05
Step: 14140, train/learning_rate: 3.317467781016603e-05
Step: 14140, train/epoch: 3.3650641441345215
Step: 14150, train/loss: 0.0
Step: 14150, train/grad_norm: 4.9599879758943644e-08
Step: 14150, train/learning_rate: 3.3162777981488034e-05
Step: 14150, train/epoch: 3.3674440383911133
Step: 14160, train/loss: 0.0
Step: 14160, train/grad_norm: 9.211044371681965e-09
Step: 14160, train/learning_rate: 3.3150881790788844e-05
Step: 14160, train/epoch: 3.369823932647705
Step: 14170, train/loss: 0.0
Step: 14170, train/grad_norm: 4.0528647105020355e-08
Step: 14170, train/learning_rate: 3.313898196211085e-05
Step: 14170, train/epoch: 3.372203826904297
Step: 14180, train/loss: 0.0
Step: 14180, train/grad_norm: 5.8944689129702965e-09
Step: 14180, train/learning_rate: 3.312708213343285e-05
Step: 14180, train/epoch: 3.3745834827423096
Step: 14190, train/loss: 0.0
Step: 14190, train/grad_norm: 5.289778073347406e-07
Step: 14190, train/learning_rate: 3.311518230475485e-05
Step: 14190, train/epoch: 3.3769633769989014
Step: 14200, train/loss: 0.0
Step: 14200, train/grad_norm: 3.3781541333155474e-06
Step: 14200, train/learning_rate: 3.3103282476076856e-05
Step: 14200, train/epoch: 3.379343271255493
Step: 14210, train/loss: 0.0
Step: 14210, train/grad_norm: 3.011631832805506e-08
Step: 14210, train/learning_rate: 3.3091386285377666e-05
Step: 14210, train/epoch: 3.381722927093506
Step: 14220, train/loss: 0.0
Step: 14220, train/grad_norm: 1.206564181899239e-08
Step: 14220, train/learning_rate: 3.307948645669967e-05
Step: 14220, train/epoch: 3.3841028213500977
Step: 14230, train/loss: 0.0
Step: 14230, train/grad_norm: 8.010302821048754e-08
Step: 14230, train/learning_rate: 3.306758662802167e-05
Step: 14230, train/epoch: 3.3864827156066895
Step: 14240, train/loss: 0.0
Step: 14240, train/grad_norm: 7.065147400453498e-08
Step: 14240, train/learning_rate: 3.3055686799343675e-05
Step: 14240, train/epoch: 3.388862371444702
Step: 14250, train/loss: 0.0
Step: 14250, train/grad_norm: 3.8000043112162984e-09
Step: 14250, train/learning_rate: 3.304378697066568e-05
Step: 14250, train/epoch: 3.391242265701294
Step: 14260, train/loss: 0.0
Step: 14260, train/grad_norm: 1.0515077519812621e-06
Step: 14260, train/learning_rate: 3.303189077996649e-05
Step: 14260, train/epoch: 3.3936221599578857
Step: 14270, train/loss: 0.0
Step: 14270, train/grad_norm: 1.0279730539020093e-10
Step: 14270, train/learning_rate: 3.301999095128849e-05
Step: 14270, train/epoch: 3.3960018157958984
Step: 14280, train/loss: 0.0
Step: 14280, train/grad_norm: 2.481955263533564e-08
Step: 14280, train/learning_rate: 3.3008091122610494e-05
Step: 14280, train/epoch: 3.3983817100524902
Step: 14290, train/loss: 0.0
Step: 14290, train/grad_norm: 1.9312812682414915e-08
Step: 14290, train/learning_rate: 3.29961912939325e-05
Step: 14290, train/epoch: 3.400761604309082
Step: 14300, train/loss: 0.0
Step: 14300, train/grad_norm: 5.049778337706812e-07
Step: 14300, train/learning_rate: 3.29842914652545e-05
Step: 14300, train/epoch: 3.4031412601470947
Step: 14310, train/loss: 0.0
Step: 14310, train/grad_norm: 2.756316098384559e-05
Step: 14310, train/learning_rate: 3.297239527455531e-05
Step: 14310, train/epoch: 3.4055211544036865
Step: 14320, train/loss: 0.0
Step: 14320, train/grad_norm: 8.14749334665521e-09
Step: 14320, train/learning_rate: 3.2960495445877314e-05
Step: 14320, train/epoch: 3.4079010486602783
Step: 14330, train/loss: 0.0
Step: 14330, train/grad_norm: 5.388871127109951e-09
Step: 14330, train/learning_rate: 3.294859561719932e-05
Step: 14330, train/epoch: 3.410280704498291
Step: 14340, train/loss: 0.0
Step: 14340, train/grad_norm: 4.986542023743823e-08
Step: 14340, train/learning_rate: 3.293669578852132e-05
Step: 14340, train/epoch: 3.412660598754883
Step: 14350, train/loss: 0.0
Step: 14350, train/grad_norm: 1.3321807657007412e-08
Step: 14350, train/learning_rate: 3.292479595984332e-05
Step: 14350, train/epoch: 3.4150404930114746
Step: 14360, train/loss: 0.0
Step: 14360, train/grad_norm: 4.896759109307425e-10
Step: 14360, train/learning_rate: 3.291289976914413e-05
Step: 14360, train/epoch: 3.4174203872680664
Step: 14370, train/loss: 0.0
Step: 14370, train/grad_norm: 1.8834876414075552e-07
Step: 14370, train/learning_rate: 3.2900999940466136e-05
Step: 14370, train/epoch: 3.419800043106079
Step: 14380, train/loss: 0.0
Step: 14380, train/grad_norm: 1.2281357264498638e-08
Step: 14380, train/learning_rate: 3.288910011178814e-05
Step: 14380, train/epoch: 3.422179937362671
Step: 14390, train/loss: 0.0
Step: 14390, train/grad_norm: 5.01254326934486e-09
Step: 14390, train/learning_rate: 3.287720028311014e-05
Step: 14390, train/epoch: 3.4245598316192627
Step: 14400, train/loss: 0.0
Step: 14400, train/grad_norm: 8.211045496864244e-07
Step: 14400, train/learning_rate: 3.2865300454432145e-05
Step: 14400, train/epoch: 3.4269394874572754
Step: 14410, train/loss: 0.0
Step: 14410, train/grad_norm: 4.238265560729815e-08
Step: 14410, train/learning_rate: 3.2853404263732955e-05
Step: 14410, train/epoch: 3.429319381713867
Step: 14420, train/loss: 0.0
Step: 14420, train/grad_norm: 2.0154446167541806e-10
Step: 14420, train/learning_rate: 3.284150443505496e-05
Step: 14420, train/epoch: 3.431699275970459
Step: 14430, train/loss: 0.0
Step: 14430, train/grad_norm: 1.939981730814111e-09
Step: 14430, train/learning_rate: 3.282960460637696e-05
Step: 14430, train/epoch: 3.4340789318084717
Step: 14440, train/loss: 9.999999747378752e-05
Step: 14440, train/grad_norm: 6.7470086833054665e-06
Step: 14440, train/learning_rate: 3.2817704777698964e-05
Step: 14440, train/epoch: 3.4364588260650635
Step: 14450, train/loss: 0.0
Step: 14450, train/grad_norm: 6.843933806521818e-05
Step: 14450, train/learning_rate: 3.280580494902097e-05
Step: 14450, train/epoch: 3.4388387203216553
Step: 14460, train/loss: 0.04470000043511391
Step: 14460, train/grad_norm: 2.724246939145303e-10
Step: 14460, train/learning_rate: 3.279390875832178e-05
Step: 14460, train/epoch: 3.441218376159668
Step: 14470, train/loss: 0.0
Step: 14470, train/grad_norm: 3.787125724130647e-09
Step: 14470, train/learning_rate: 3.278200892964378e-05
Step: 14470, train/epoch: 3.4435982704162598
Step: 14480, train/loss: 0.0
Step: 14480, train/grad_norm: 7.35873072699178e-07
Step: 14480, train/learning_rate: 3.277010910096578e-05
Step: 14480, train/epoch: 3.4459781646728516
Step: 14490, train/loss: 0.0
Step: 14490, train/grad_norm: 0.00014274424756877124
Step: 14490, train/learning_rate: 3.2758209272287786e-05
Step: 14490, train/epoch: 3.4483578205108643
Step: 14500, train/loss: 0.0
Step: 14500, train/grad_norm: 4.5385070701620123e-10
Step: 14500, train/learning_rate: 3.2746313081588596e-05
Step: 14500, train/epoch: 3.450737714767456
Step: 14510, train/loss: 0.0
Step: 14510, train/grad_norm: 4.352482374514466e-09
Step: 14510, train/learning_rate: 3.27344132529106e-05
Step: 14510, train/epoch: 3.453117609024048
Step: 14520, train/loss: 0.0
Step: 14520, train/grad_norm: 2.780656416234706e-07
Step: 14520, train/learning_rate: 3.27225134242326e-05
Step: 14520, train/epoch: 3.4554972648620605
Step: 14530, train/loss: 0.0
Step: 14530, train/grad_norm: 3.774363506181544e-07
Step: 14530, train/learning_rate: 3.2710613595554605e-05
Step: 14530, train/epoch: 3.4578771591186523
Step: 14540, train/loss: 0.0
Step: 14540, train/grad_norm: 8.294965141431021e-07
Step: 14540, train/learning_rate: 3.269871376687661e-05
Step: 14540, train/epoch: 3.460257053375244
Step: 14550, train/loss: 0.0
Step: 14550, train/grad_norm: 5.21336858128052e-07
Step: 14550, train/learning_rate: 3.268681757617742e-05
Step: 14550, train/epoch: 3.462636947631836
Step: 14560, train/loss: 0.0
Step: 14560, train/grad_norm: 9.070443525160954e-07
Step: 14560, train/learning_rate: 3.267491774749942e-05
Step: 14560, train/epoch: 3.4650166034698486
Step: 14570, train/loss: 0.0
Step: 14570, train/grad_norm: 4.6804884412665615e-09
Step: 14570, train/learning_rate: 3.2663017918821424e-05
Step: 14570, train/epoch: 3.4673964977264404
Step: 14580, train/loss: 0.0
Step: 14580, train/grad_norm: 2.1789439141883804e-09
Step: 14580, train/learning_rate: 3.265111809014343e-05
Step: 14580, train/epoch: 3.4697763919830322
Step: 14590, train/loss: 0.0
Step: 14590, train/grad_norm: 3.5630492334348673e-08
Step: 14590, train/learning_rate: 3.263921826146543e-05
Step: 14590, train/epoch: 3.472156047821045
Step: 14600, train/loss: 0.0
Step: 14600, train/grad_norm: 2.0214366713844356e-07
Step: 14600, train/learning_rate: 3.262732207076624e-05
Step: 14600, train/epoch: 3.4745359420776367
Step: 14610, train/loss: 0.0
Step: 14610, train/grad_norm: 0.004470601212233305
Step: 14610, train/learning_rate: 3.261542224208824e-05
Step: 14610, train/epoch: 3.4769158363342285
Step: 14620, train/loss: 0.13519999384880066
Step: 14620, train/grad_norm: 2.924158252426423e-06
Step: 14620, train/learning_rate: 3.2603522413410246e-05
Step: 14620, train/epoch: 3.479295492172241
Step: 14630, train/loss: 0.17499999701976776
Step: 14630, train/grad_norm: 4.1175988485520065e-07
Step: 14630, train/learning_rate: 3.259162258473225e-05
Step: 14630, train/epoch: 3.481675386428833
Step: 14640, train/loss: 0.0
Step: 14640, train/grad_norm: 1.2761112166970179e-09
Step: 14640, train/learning_rate: 3.257972275605425e-05
Step: 14640, train/epoch: 3.484055280685425
Step: 14650, train/loss: 0.0
Step: 14650, train/grad_norm: 1.204960824452428e-07
Step: 14650, train/learning_rate: 3.256782656535506e-05
Step: 14650, train/epoch: 3.4864349365234375
Step: 14660, train/loss: 0.0
Step: 14660, train/grad_norm: 2.4841215235937852e-06
Step: 14660, train/learning_rate: 3.2555926736677065e-05
Step: 14660, train/epoch: 3.4888148307800293
Step: 14670, train/loss: 9.999999747378752e-05
Step: 14670, train/grad_norm: 3.2303018038248865e-09
Step: 14670, train/learning_rate: 3.254402690799907e-05
Step: 14670, train/epoch: 3.491194725036621
Step: 14680, train/loss: 0.013399999588727951
Step: 14680, train/grad_norm: 6.289085376920411e-06
Step: 14680, train/learning_rate: 3.253212707932107e-05
Step: 14680, train/epoch: 3.493574380874634
Step: 14690, train/loss: 0.0
Step: 14690, train/grad_norm: 1.8730808903910656e-07
Step: 14690, train/learning_rate: 3.2520227250643075e-05
Step: 14690, train/epoch: 3.4959542751312256
Step: 14700, train/loss: 0.0
Step: 14700, train/grad_norm: 1.0020837635238422e-06
Step: 14700, train/learning_rate: 3.2508331059943885e-05
Step: 14700, train/epoch: 3.4983341693878174
Step: 14710, train/loss: 0.0
Step: 14710, train/grad_norm: 2.427170286978253e-08
Step: 14710, train/learning_rate: 3.249643123126589e-05
Step: 14710, train/epoch: 3.500714063644409
Step: 14720, train/loss: 0.0
Step: 14720, train/grad_norm: 5.049105022969513e-10
Step: 14720, train/learning_rate: 3.248453140258789e-05
Step: 14720, train/epoch: 3.503093719482422
Step: 14730, train/loss: 0.125
Step: 14730, train/grad_norm: 5.315463567967527e-06
Step: 14730, train/learning_rate: 3.2472631573909894e-05
Step: 14730, train/epoch: 3.5054736137390137
Step: 14740, train/loss: 9.999999747378752e-05
Step: 14740, train/grad_norm: 6.115890016644698e-10
Step: 14740, train/learning_rate: 3.24607317452319e-05
Step: 14740, train/epoch: 3.5078535079956055
Step: 14750, train/loss: 0.0
Step: 14750, train/grad_norm: 9.532584044791292e-06
Step: 14750, train/learning_rate: 3.244883555453271e-05
Step: 14750, train/epoch: 3.510233163833618
Step: 14760, train/loss: 0.0
Step: 14760, train/grad_norm: 4.878766834970349e-10
Step: 14760, train/learning_rate: 3.243693572585471e-05
Step: 14760, train/epoch: 3.51261305809021
Step: 14770, train/loss: 0.0
Step: 14770, train/grad_norm: 7.018880143760953e-09
Step: 14770, train/learning_rate: 3.242503589717671e-05
Step: 14770, train/epoch: 3.5149929523468018
Step: 14780, train/loss: 0.0
Step: 14780, train/grad_norm: 6.056927759345854e-07
Step: 14780, train/learning_rate: 3.2413136068498716e-05
Step: 14780, train/epoch: 3.5173726081848145
Step: 14790, train/loss: 0.0
Step: 14790, train/grad_norm: 4.615393663698342e-06
Step: 14790, train/learning_rate: 3.240123623982072e-05
Step: 14790, train/epoch: 3.5197525024414062
Step: 14800, train/loss: 0.18979999423027039
Step: 14800, train/grad_norm: 152.6623992919922
Step: 14800, train/learning_rate: 3.238934004912153e-05
Step: 14800, train/epoch: 3.522132396697998
Step: 14810, train/loss: 0.0
Step: 14810, train/grad_norm: 4.920925130136311e-05
Step: 14810, train/learning_rate: 3.237744022044353e-05
Step: 14810, train/epoch: 3.5245120525360107
Step: 14820, train/loss: 0.0
Step: 14820, train/grad_norm: 0.00012490167864598334
Step: 14820, train/learning_rate: 3.2365540391765535e-05
Step: 14820, train/epoch: 3.5268919467926025
Step: 14830, train/loss: 0.000699999975040555
Step: 14830, train/grad_norm: 1.7121861617397371e-07
Step: 14830, train/learning_rate: 3.235364056308754e-05
Step: 14830, train/epoch: 3.5292718410491943
Step: 14840, train/loss: 0.0
Step: 14840, train/grad_norm: 7.213481201251426e-11
Step: 14840, train/learning_rate: 3.234174073440954e-05
Step: 14840, train/epoch: 3.531651496887207
Step: 14850, train/loss: 0.0
Step: 14850, train/grad_norm: 2.7355481435620277e-08
Step: 14850, train/learning_rate: 3.232984454371035e-05
Step: 14850, train/epoch: 3.534031391143799
Step: 14860, train/loss: 0.0
Step: 14860, train/grad_norm: 7.741009255823883e-08
Step: 14860, train/learning_rate: 3.2317944715032354e-05
Step: 14860, train/epoch: 3.5364112854003906
Step: 14870, train/loss: 0.0
Step: 14870, train/grad_norm: 0.0002953269868157804
Step: 14870, train/learning_rate: 3.230604488635436e-05
Step: 14870, train/epoch: 3.5387909412384033
Step: 14880, train/loss: 0.0
Step: 14880, train/grad_norm: 0.0486765131354332
Step: 14880, train/learning_rate: 3.229414505767636e-05
Step: 14880, train/epoch: 3.541170835494995
Step: 14890, train/loss: 0.0
Step: 14890, train/grad_norm: 9.678309709215682e-08
Step: 14890, train/learning_rate: 3.228224522899836e-05
Step: 14890, train/epoch: 3.543550729751587
Step: 14900, train/loss: 0.0
Step: 14900, train/grad_norm: 0.001296049915254116
Step: 14900, train/learning_rate: 3.227034903829917e-05
Step: 14900, train/epoch: 3.5459306240081787
Step: 14910, train/loss: 0.0
Step: 14910, train/grad_norm: 3.693051198183639e-08
Step: 14910, train/learning_rate: 3.2258449209621176e-05
Step: 14910, train/epoch: 3.5483102798461914
Step: 14920, train/loss: 0.0
Step: 14920, train/grad_norm: 8.466959116049111e-06
Step: 14920, train/learning_rate: 3.224654938094318e-05
Step: 14920, train/epoch: 3.550690174102783
Step: 14930, train/loss: 0.0
Step: 14930, train/grad_norm: 5.822072921546351e-07
Step: 14930, train/learning_rate: 3.223464955226518e-05
Step: 14930, train/epoch: 3.553070068359375
Step: 14940, train/loss: 0.0
Step: 14940, train/grad_norm: 4.455785518331368e-09
Step: 14940, train/learning_rate: 3.2222749723587185e-05
Step: 14940, train/epoch: 3.5554497241973877
Step: 14950, train/loss: 0.0
Step: 14950, train/grad_norm: 3.8322497175613535e-08
Step: 14950, train/learning_rate: 3.2210853532887995e-05
Step: 14950, train/epoch: 3.5578296184539795
Step: 14960, train/loss: 0.0
Step: 14960, train/grad_norm: 2.975573124786024e-06
Step: 14960, train/learning_rate: 3.219895370421e-05
Step: 14960, train/epoch: 3.5602095127105713
Step: 14970, train/loss: 0.0
Step: 14970, train/grad_norm: 5.845963073625171e-07
Step: 14970, train/learning_rate: 3.2187053875532e-05
Step: 14970, train/epoch: 3.562589168548584
Step: 14980, train/loss: 0.0
Step: 14980, train/grad_norm: 2.130892440277421e-08
Step: 14980, train/learning_rate: 3.2175154046854004e-05
Step: 14980, train/epoch: 3.564969062805176
Step: 14990, train/loss: 0.0
Step: 14990, train/grad_norm: 4.319535307217848e-08
Step: 14990, train/learning_rate: 3.216325421817601e-05
Step: 14990, train/epoch: 3.5673489570617676
Step: 15000, train/loss: 0.0
Step: 15000, train/grad_norm: 1.0626380664291446e-08
Step: 15000, train/learning_rate: 3.215135802747682e-05
Step: 15000, train/epoch: 3.5697286128997803
Step: 15010, train/loss: 0.0
Step: 15010, train/grad_norm: 2.585310854286149e-09
Step: 15010, train/learning_rate: 3.213945819879882e-05
Step: 15010, train/epoch: 3.572108507156372
Step: 15020, train/loss: 0.0
Step: 15020, train/grad_norm: 5.447889179777121e-07
Step: 15020, train/learning_rate: 3.212755837012082e-05
Step: 15020, train/epoch: 3.574488401412964
Step: 15030, train/loss: 0.0
Step: 15030, train/grad_norm: 2.417870632598351e-07
Step: 15030, train/learning_rate: 3.2115658541442826e-05
Step: 15030, train/epoch: 3.5768680572509766
Step: 15040, train/loss: 0.0
Step: 15040, train/grad_norm: 1.5679995613027131e-06
Step: 15040, train/learning_rate: 3.210375871276483e-05
Step: 15040, train/epoch: 3.5792479515075684
Step: 15050, train/loss: 0.0
Step: 15050, train/grad_norm: 8.1423850133433e-06
Step: 15050, train/learning_rate: 3.209186252206564e-05
Step: 15050, train/epoch: 3.58162784576416
Step: 15060, train/loss: 0.0
Step: 15060, train/grad_norm: 6.291721774687176e-07
Step: 15060, train/learning_rate: 3.207996269338764e-05
Step: 15060, train/epoch: 3.584007501602173
Step: 15070, train/loss: 0.0
Step: 15070, train/grad_norm: 1.6722920825884557e-09
Step: 15070, train/learning_rate: 3.2068062864709646e-05
Step: 15070, train/epoch: 3.5863873958587646
Step: 15080, train/loss: 0.0
Step: 15080, train/grad_norm: 6.94471036233324e-10
Step: 15080, train/learning_rate: 3.205616303603165e-05
Step: 15080, train/epoch: 3.5887672901153564
Step: 15090, train/loss: 0.0
Step: 15090, train/grad_norm: 6.40689334918676e-10
Step: 15090, train/learning_rate: 3.204426320735365e-05
Step: 15090, train/epoch: 3.5911471843719482
Step: 15100, train/loss: 0.0
Step: 15100, train/grad_norm: 5.6915184371009175e-12
Step: 15100, train/learning_rate: 3.203236701665446e-05
Step: 15100, train/epoch: 3.593526840209961
Step: 15110, train/loss: 0.0
Step: 15110, train/grad_norm: 1.2889806384919211e-05
Step: 15110, train/learning_rate: 3.2020467187976465e-05
Step: 15110, train/epoch: 3.5959067344665527
Step: 15120, train/loss: 0.0
Step: 15120, train/grad_norm: 7.230388291645795e-06
Step: 15120, train/learning_rate: 3.200856735929847e-05
Step: 15120, train/epoch: 3.5982866287231445
Step: 15130, train/loss: 0.0
Step: 15130, train/grad_norm: 3.2480254041900025e-09
Step: 15130, train/learning_rate: 3.199666753062047e-05
Step: 15130, train/epoch: 3.6006662845611572
Step: 15140, train/loss: 0.0
Step: 15140, train/grad_norm: 7.74349882703973e-06
Step: 15140, train/learning_rate: 3.1984767701942474e-05
Step: 15140, train/epoch: 3.603046178817749
Step: 15150, train/loss: 0.0
Step: 15150, train/grad_norm: 1.522322179425828e-07
Step: 15150, train/learning_rate: 3.1972871511243284e-05
Step: 15150, train/epoch: 3.605426073074341
Step: 15160, train/loss: 0.0
Step: 15160, train/grad_norm: 1.6390018231504655e-07
Step: 15160, train/learning_rate: 3.196097168256529e-05
Step: 15160, train/epoch: 3.6078057289123535
Step: 15170, train/loss: 0.0
Step: 15170, train/grad_norm: 1.0772899372568645e-07
Step: 15170, train/learning_rate: 3.194907185388729e-05
Step: 15170, train/epoch: 3.6101856231689453
Step: 15180, train/loss: 9.999999747378752e-05
Step: 15180, train/grad_norm: 7.062203053465055e-07
Step: 15180, train/learning_rate: 3.193717202520929e-05
Step: 15180, train/epoch: 3.612565517425537
Step: 15190, train/loss: 0.0
Step: 15190, train/grad_norm: 2.935089469247032e-05
Step: 15190, train/learning_rate: 3.1925272196531296e-05
Step: 15190, train/epoch: 3.61494517326355
Step: 15200, train/loss: 0.0010000000474974513
Step: 15200, train/grad_norm: 7.255029021990822e-09
Step: 15200, train/learning_rate: 3.1913376005832106e-05
Step: 15200, train/epoch: 3.6173250675201416
Step: 15210, train/loss: 0.0
Step: 15210, train/grad_norm: 6.154499132549063e-11
Step: 15210, train/learning_rate: 3.190147617715411e-05
Step: 15210, train/epoch: 3.6197049617767334
Step: 15220, train/loss: 0.0003000000142492354
Step: 15220, train/grad_norm: 7.248671352044767e-08
Step: 15220, train/learning_rate: 3.188957634847611e-05
Step: 15220, train/epoch: 3.622084617614746
Step: 15230, train/loss: 0.0
Step: 15230, train/grad_norm: 2.6434657129925654e-08
Step: 15230, train/learning_rate: 3.1877676519798115e-05
Step: 15230, train/epoch: 3.624464511871338
Step: 15240, train/loss: 0.0003000000142492354
Step: 15240, train/grad_norm: 1.7002546925937168e-08
Step: 15240, train/learning_rate: 3.186577669112012e-05
Step: 15240, train/epoch: 3.6268444061279297
Step: 15250, train/loss: 0.0
Step: 15250, train/grad_norm: 7.826086800832854e-08
Step: 15250, train/learning_rate: 3.185388050042093e-05
Step: 15250, train/epoch: 3.6292240619659424
Step: 15260, train/loss: 0.05040000006556511
Step: 15260, train/grad_norm: 2.164554091521609e-09
Step: 15260, train/learning_rate: 3.184198067174293e-05
Step: 15260, train/epoch: 3.631603956222534
Step: 15270, train/loss: 0.0
Step: 15270, train/grad_norm: 7.710793425985685e-08
Step: 15270, train/learning_rate: 3.1830080843064934e-05
Step: 15270, train/epoch: 3.633983850479126
Step: 15280, train/loss: 0.0
Step: 15280, train/grad_norm: 7.797979151291656e-07
Step: 15280, train/learning_rate: 3.181818101438694e-05
Step: 15280, train/epoch: 3.6363637447357178
Step: 15290, train/loss: 0.007699999958276749
Step: 15290, train/grad_norm: 3.008330168086104e-06
Step: 15290, train/learning_rate: 3.180628118570894e-05
Step: 15290, train/epoch: 3.6387434005737305
Step: 15300, train/loss: 0.0
Step: 15300, train/grad_norm: 7.288438297337052e-09
Step: 15300, train/learning_rate: 3.179438499500975e-05
Step: 15300, train/epoch: 3.6411232948303223
Step: 15310, train/loss: 0.0
Step: 15310, train/grad_norm: 8.403118556543632e-08
Step: 15310, train/learning_rate: 3.178248516633175e-05
Step: 15310, train/epoch: 3.643503189086914
Step: 15320, train/loss: 0.0
Step: 15320, train/grad_norm: 9.89014736774152e-08
Step: 15320, train/learning_rate: 3.1770585337653756e-05
Step: 15320, train/epoch: 3.6458828449249268
Step: 15330, train/loss: 0.0
Step: 15330, train/grad_norm: 2.4973664380922855e-07
Step: 15330, train/learning_rate: 3.175868550897576e-05
Step: 15330, train/epoch: 3.6482627391815186
Step: 15340, train/loss: 0.0024999999441206455
Step: 15340, train/grad_norm: 2.0402563194465984e-08
Step: 15340, train/learning_rate: 3.174678568029776e-05
Step: 15340, train/epoch: 3.6506426334381104
Step: 15350, train/loss: 0.0
Step: 15350, train/grad_norm: 8.729916771699209e-07
Step: 15350, train/learning_rate: 3.173488948959857e-05
Step: 15350, train/epoch: 3.653022289276123
Step: 15360, train/loss: 0.0
Step: 15360, train/grad_norm: 2.098890128365838e-08
Step: 15360, train/learning_rate: 3.1722989660920575e-05
Step: 15360, train/epoch: 3.655402183532715
Step: 15370, train/loss: 0.0
Step: 15370, train/grad_norm: 4.555373379844241e-05
Step: 15370, train/learning_rate: 3.171108983224258e-05
Step: 15370, train/epoch: 3.6577820777893066
Step: 15380, train/loss: 0.0
Step: 15380, train/grad_norm: 1.67733187339536e-08
Step: 15380, train/learning_rate: 3.169919000356458e-05
Step: 15380, train/epoch: 3.6601617336273193
Step: 15390, train/loss: 0.0003000000142492354
Step: 15390, train/grad_norm: 5.7138269227152705e-08
Step: 15390, train/learning_rate: 3.1687290174886584e-05
Step: 15390, train/epoch: 3.662541627883911
Step: 15400, train/loss: 0.19140000641345978
Step: 15400, train/grad_norm: 1.000409000084801e-08
Step: 15400, train/learning_rate: 3.1675393984187394e-05
Step: 15400, train/epoch: 3.664921522140503
Step: 15410, train/loss: 0.0
Step: 15410, train/grad_norm: 2.2781394193316373e-07
Step: 15410, train/learning_rate: 3.16634941555094e-05
Step: 15410, train/epoch: 3.6673011779785156
Step: 15420, train/loss: 0.12269999831914902
Step: 15420, train/grad_norm: 7.369385657263194e-10
Step: 15420, train/learning_rate: 3.16515943268314e-05
Step: 15420, train/epoch: 3.6696810722351074
Step: 15430, train/loss: 0.0
Step: 15430, train/grad_norm: 6.605735052289674e-06
Step: 15430, train/learning_rate: 3.1639694498153403e-05
Step: 15430, train/epoch: 3.672060966491699
Step: 15440, train/loss: 0.0
Step: 15440, train/grad_norm: 2.0180870308195153e-07
Step: 15440, train/learning_rate: 3.1627794669475406e-05
Step: 15440, train/epoch: 3.674440860748291
Step: 15450, train/loss: 0.0
Step: 15450, train/grad_norm: 4.2359095397959123e-11
Step: 15450, train/learning_rate: 3.1615898478776217e-05
Step: 15450, train/epoch: 3.6768205165863037
Step: 15460, train/loss: 0.0
Step: 15460, train/grad_norm: 4.096816288989835e-11
Step: 15460, train/learning_rate: 3.160399865009822e-05
Step: 15460, train/epoch: 3.6792004108428955
Step: 15470, train/loss: 0.0
Step: 15470, train/grad_norm: 7.821749932190869e-07
Step: 15470, train/learning_rate: 3.159209882142022e-05
Step: 15470, train/epoch: 3.6815803050994873
Step: 15480, train/loss: 0.0
Step: 15480, train/grad_norm: 2.5528164471211312e-08
Step: 15480, train/learning_rate: 3.1580198992742226e-05
Step: 15480, train/epoch: 3.6839599609375
Step: 15490, train/loss: 0.0
Step: 15490, train/grad_norm: 0.0006711410824209452
Step: 15490, train/learning_rate: 3.156829916406423e-05
Step: 15490, train/epoch: 3.686339855194092
Step: 15500, train/loss: 0.0
Step: 15500, train/grad_norm: 0.00010646639566402882
Step: 15500, train/learning_rate: 3.155640297336504e-05
Step: 15500, train/epoch: 3.6887197494506836
Step: 15510, train/loss: 0.0
Step: 15510, train/grad_norm: 3.03891289910041e-09
Step: 15510, train/learning_rate: 3.154450314468704e-05
Step: 15510, train/epoch: 3.6910994052886963
Step: 15520, train/loss: 0.0
Step: 15520, train/grad_norm: 7.139619242479966e-07
Step: 15520, train/learning_rate: 3.1532603316009045e-05
Step: 15520, train/epoch: 3.693479299545288
Step: 15530, train/loss: 0.0
Step: 15530, train/grad_norm: 1.597493479721379e-07
Step: 15530, train/learning_rate: 3.152070348733105e-05
Step: 15530, train/epoch: 3.69585919380188
Step: 15540, train/loss: 0.0
Step: 15540, train/grad_norm: 2.8078158288008526e-12
Step: 15540, train/learning_rate: 3.150880365865305e-05
Step: 15540, train/epoch: 3.6982388496398926
Step: 15550, train/loss: 0.0
Step: 15550, train/grad_norm: 3.601154574539578e-08
Step: 15550, train/learning_rate: 3.149690746795386e-05
Step: 15550, train/epoch: 3.7006187438964844
Step: 15560, train/loss: 0.0
Step: 15560, train/grad_norm: 4.483444726588459e-10
Step: 15560, train/learning_rate: 3.1485007639275864e-05
Step: 15560, train/epoch: 3.702998638153076
Step: 15570, train/loss: 0.0
Step: 15570, train/grad_norm: 4.948035581264776e-08
Step: 15570, train/learning_rate: 3.147310781059787e-05
Step: 15570, train/epoch: 3.705378293991089
Step: 15580, train/loss: 0.0
Step: 15580, train/grad_norm: 9.180941423769795e-11
Step: 15580, train/learning_rate: 3.146120798191987e-05
Step: 15580, train/epoch: 3.7077581882476807
Step: 15590, train/loss: 0.0
Step: 15590, train/grad_norm: 1.4765832112573207e-08
Step: 15590, train/learning_rate: 3.144930815324187e-05
Step: 15590, train/epoch: 3.7101380825042725
Step: 15600, train/loss: 0.0
Step: 15600, train/grad_norm: 7.745630803412951e-10
Step: 15600, train/learning_rate: 3.143741196254268e-05
Step: 15600, train/epoch: 3.712517738342285
Step: 15610, train/loss: 0.0
Step: 15610, train/grad_norm: 7.118792777482952e-10
Step: 15610, train/learning_rate: 3.1425512133864686e-05
Step: 15610, train/epoch: 3.714897632598877
Step: 15620, train/loss: 0.0
Step: 15620, train/grad_norm: 0.024689679965376854
Step: 15620, train/learning_rate: 3.141361230518669e-05
Step: 15620, train/epoch: 3.7172775268554688
Step: 15630, train/loss: 0.0
Step: 15630, train/grad_norm: 2.367486207077718e-09
Step: 15630, train/learning_rate: 3.140171247650869e-05
Step: 15630, train/epoch: 3.7196574211120605
Step: 15640, train/loss: 0.0
Step: 15640, train/grad_norm: 5.168607941641312e-08
Step: 15640, train/learning_rate: 3.1389812647830695e-05
Step: 15640, train/epoch: 3.7220370769500732
Step: 15650, train/loss: 0.0
Step: 15650, train/grad_norm: 9.116190540225944e-07
Step: 15650, train/learning_rate: 3.1377916457131505e-05
Step: 15650, train/epoch: 3.724416971206665
Step: 15660, train/loss: 0.0
Step: 15660, train/grad_norm: 3.7449446321780044e-11
Step: 15660, train/learning_rate: 3.136601662845351e-05
Step: 15660, train/epoch: 3.726796865463257
Step: 15670, train/loss: 0.0
Step: 15670, train/grad_norm: 2.7644249736558102e-11
Step: 15670, train/learning_rate: 3.135411679977551e-05
Step: 15670, train/epoch: 3.7291765213012695
Step: 15680, train/loss: 0.0
Step: 15680, train/grad_norm: 8.25663715176006e-09
Step: 15680, train/learning_rate: 3.1342216971097514e-05
Step: 15680, train/epoch: 3.7315564155578613
Step: 15690, train/loss: 0.0
Step: 15690, train/grad_norm: 3.2767327046157035e-11
Step: 15690, train/learning_rate: 3.133031714241952e-05
Step: 15690, train/epoch: 3.733936309814453
Step: 15700, train/loss: 0.0
Step: 15700, train/grad_norm: 1.584414022204328e-08
Step: 15700, train/learning_rate: 3.131842095172033e-05
Step: 15700, train/epoch: 3.736315965652466
Step: 15710, train/loss: 0.0
Step: 15710, train/grad_norm: 1.420425087417243e-05
Step: 15710, train/learning_rate: 3.130652112304233e-05
Step: 15710, train/epoch: 3.7386958599090576
Step: 15720, train/loss: 0.002199999988079071
Step: 15720, train/grad_norm: 6.061637236598472e-08
Step: 15720, train/learning_rate: 3.129462129436433e-05
Step: 15720, train/epoch: 3.7410757541656494
Step: 15730, train/loss: 0.0
Step: 15730, train/grad_norm: 2.6099609584662176e-08
Step: 15730, train/learning_rate: 3.1282721465686336e-05
Step: 15730, train/epoch: 3.743455410003662
Step: 15740, train/loss: 0.0
Step: 15740, train/grad_norm: 2.3703767510596663e-05
Step: 15740, train/learning_rate: 3.127082163700834e-05
Step: 15740, train/epoch: 3.745835304260254
Step: 15750, train/loss: 0.0
Step: 15750, train/grad_norm: 4.4917367603147795e-08
Step: 15750, train/learning_rate: 3.125892544630915e-05
Step: 15750, train/epoch: 3.7482151985168457
Step: 15760, train/loss: 0.15309999883174896
Step: 15760, train/grad_norm: 9.744263707034406e-08
Step: 15760, train/learning_rate: 3.124702561763115e-05
Step: 15760, train/epoch: 3.7505948543548584
Step: 15770, train/loss: 0.0
Step: 15770, train/grad_norm: 3.8978650991339236e-05
Step: 15770, train/learning_rate: 3.1235125788953155e-05
Step: 15770, train/epoch: 3.75297474861145
Step: 15780, train/loss: 0.0
Step: 15780, train/grad_norm: 8.509797225997318e-06
Step: 15780, train/learning_rate: 3.122322596027516e-05
Step: 15780, train/epoch: 3.755354642868042
Step: 15790, train/loss: 0.0
Step: 15790, train/grad_norm: 0.002578075509518385
Step: 15790, train/learning_rate: 3.121132613159716e-05
Step: 15790, train/epoch: 3.7577342987060547
Step: 15800, train/loss: 0.0
Step: 15800, train/grad_norm: 1.226787702535148e-07
Step: 15800, train/learning_rate: 3.119942994089797e-05
Step: 15800, train/epoch: 3.7601141929626465
Step: 15810, train/loss: 0.0
Step: 15810, train/grad_norm: 3.072265144510311e-06
Step: 15810, train/learning_rate: 3.1187530112219974e-05
Step: 15810, train/epoch: 3.7624940872192383
Step: 15820, train/loss: 0.0
Step: 15820, train/grad_norm: 4.954579344484955e-05
Step: 15820, train/learning_rate: 3.117563028354198e-05
Step: 15820, train/epoch: 3.76487398147583
Step: 15830, train/loss: 0.0
Step: 15830, train/grad_norm: 8.942591307459224e-07
Step: 15830, train/learning_rate: 3.116373045486398e-05
Step: 15830, train/epoch: 3.7672536373138428
Step: 15840, train/loss: 0.003599999938160181
Step: 15840, train/grad_norm: 1.1578747944440693e-05
Step: 15840, train/learning_rate: 3.115183426416479e-05
Step: 15840, train/epoch: 3.7696335315704346
Step: 15850, train/loss: 0.0
Step: 15850, train/grad_norm: 5.365837409954111e-07
Step: 15850, train/learning_rate: 3.1139934435486794e-05
Step: 15850, train/epoch: 3.7720134258270264
Step: 15860, train/loss: 9.999999747378752e-05
Step: 15860, train/grad_norm: 1.8383065025773249e-07
Step: 15860, train/learning_rate: 3.1128034606808797e-05
Step: 15860, train/epoch: 3.774393081665039
Step: 15870, train/loss: 0.19920000433921814
Step: 15870, train/grad_norm: 2.537473449137906e-07
Step: 15870, train/learning_rate: 3.11161347781308e-05
Step: 15870, train/epoch: 3.776772975921631
Step: 15880, train/loss: 0.0
Step: 15880, train/grad_norm: 8.086401770412976e-09
Step: 15880, train/learning_rate: 3.11042349494528e-05
Step: 15880, train/epoch: 3.7791528701782227
Step: 15890, train/loss: 0.0
Step: 15890, train/grad_norm: 0.10625232756137848
Step: 15890, train/learning_rate: 3.109233875875361e-05
Step: 15890, train/epoch: 3.7815325260162354
Step: 15900, train/loss: 0.0
Step: 15900, train/grad_norm: 2.691562258405611e-06
Step: 15900, train/learning_rate: 3.1080438930075616e-05
Step: 15900, train/epoch: 3.783912420272827
Step: 15910, train/loss: 0.0
Step: 15910, train/grad_norm: 0.001286280807107687
Step: 15910, train/learning_rate: 3.106853910139762e-05
Step: 15910, train/epoch: 3.786292314529419
Step: 15920, train/loss: 0.0
Step: 15920, train/grad_norm: 3.5047607525484636e-05
Step: 15920, train/learning_rate: 3.105663927271962e-05
Step: 15920, train/epoch: 3.7886719703674316
Step: 15930, train/loss: 0.07150000333786011
Step: 15930, train/grad_norm: 9.566672787286734e-08
Step: 15930, train/learning_rate: 3.1044739444041625e-05
Step: 15930, train/epoch: 3.7910518646240234
Step: 15940, train/loss: 0.0
Step: 15940, train/grad_norm: 8.093587530311197e-05
Step: 15940, train/learning_rate: 3.1032843253342435e-05
Step: 15940, train/epoch: 3.7934317588806152
Step: 15950, train/loss: 0.0
Step: 15950, train/grad_norm: 0.028445066884160042
Step: 15950, train/learning_rate: 3.102094342466444e-05
Step: 15950, train/epoch: 3.795811414718628
Step: 15960, train/loss: 0.0
Step: 15960, train/grad_norm: 0.004572759382426739
Step: 15960, train/learning_rate: 3.100904359598644e-05
Step: 15960, train/epoch: 3.7981913089752197
Step: 15970, train/loss: 0.0
Step: 15970, train/grad_norm: 5.108987011226418e-07
Step: 15970, train/learning_rate: 3.0997143767308444e-05
Step: 15970, train/epoch: 3.8005712032318115
Step: 15980, train/loss: 0.0
Step: 15980, train/grad_norm: 3.2040026098911767e-07
Step: 15980, train/learning_rate: 3.098524393863045e-05
Step: 15980, train/epoch: 3.802950859069824
Step: 15990, train/loss: 0.0
Step: 15990, train/grad_norm: 1.6807909560156986e-05
Step: 15990, train/learning_rate: 3.097334774793126e-05
Step: 15990, train/epoch: 3.805330753326416
Step: 16000, train/loss: 0.0
Step: 16000, train/grad_norm: 3.6796154745388776e-06
Step: 16000, train/learning_rate: 3.096144791925326e-05
Step: 16000, train/epoch: 3.807710647583008
Step: 16010, train/loss: 0.0
Step: 16010, train/grad_norm: 2.385378650160419e-07
Step: 16010, train/learning_rate: 3.094954809057526e-05
Step: 16010, train/epoch: 3.8100905418395996
Step: 16020, train/loss: 0.17730000615119934
Step: 16020, train/grad_norm: 3.769992690649815e-05
Step: 16020, train/learning_rate: 3.0937648261897266e-05
Step: 16020, train/epoch: 3.8124701976776123
Step: 16030, train/loss: 0.0
Step: 16030, train/grad_norm: 1.6114829122670926e-05
Step: 16030, train/learning_rate: 3.092574843321927e-05
Step: 16030, train/epoch: 3.814850091934204
Step: 16040, train/loss: 0.0
Step: 16040, train/grad_norm: 4.681550436202997e-08
Step: 16040, train/learning_rate: 3.091385224252008e-05
Step: 16040, train/epoch: 3.817229986190796
Step: 16050, train/loss: 0.0026000000070780516
Step: 16050, train/grad_norm: 2.331305722691468e-06
Step: 16050, train/learning_rate: 3.090195241384208e-05
Step: 16050, train/epoch: 3.8196096420288086
Step: 16060, train/loss: 0.0
Step: 16060, train/grad_norm: 7.864177575811482e-08
Step: 16060, train/learning_rate: 3.0890052585164085e-05
Step: 16060, train/epoch: 3.8219895362854004
Step: 16070, train/loss: 0.0
Step: 16070, train/grad_norm: 4.068388292921554e-08
Step: 16070, train/learning_rate: 3.087815275648609e-05
Step: 16070, train/epoch: 3.824369430541992
Step: 16080, train/loss: 0.0
Step: 16080, train/grad_norm: 1.1326823567969768e-07
Step: 16080, train/learning_rate: 3.086625292780809e-05
Step: 16080, train/epoch: 3.826749086380005
Step: 16090, train/loss: 0.0020000000949949026
Step: 16090, train/grad_norm: 4.483581506065093e-08
Step: 16090, train/learning_rate: 3.08543567371089e-05
Step: 16090, train/epoch: 3.8291289806365967
Step: 16100, train/loss: 0.0
Step: 16100, train/grad_norm: 2.4510666207788745e-06
Step: 16100, train/learning_rate: 3.0842456908430904e-05
Step: 16100, train/epoch: 3.8315088748931885
Step: 16110, train/loss: 0.0
Step: 16110, train/grad_norm: 2.1654050996744445e-08
Step: 16110, train/learning_rate: 3.083055707975291e-05
Step: 16110, train/epoch: 3.833888530731201
Step: 16120, train/loss: 0.0
Step: 16120, train/grad_norm: 1.0365779417043086e-05
Step: 16120, train/learning_rate: 3.081865725107491e-05
Step: 16120, train/epoch: 3.836268424987793
Step: 16130, train/loss: 0.0
Step: 16130, train/grad_norm: 1.0151068181585288e-06
Step: 16130, train/learning_rate: 3.080675742239691e-05
Step: 16130, train/epoch: 3.8386483192443848
Step: 16140, train/loss: 0.001500000013038516
Step: 16140, train/grad_norm: 12.769558906555176
Step: 16140, train/learning_rate: 3.079486123169772e-05
Step: 16140, train/epoch: 3.8410279750823975
Step: 16150, train/loss: 0.0
Step: 16150, train/grad_norm: 4.5761133549149235e-08
Step: 16150, train/learning_rate: 3.0782961403019726e-05
Step: 16150, train/epoch: 3.8434078693389893
Step: 16160, train/loss: 0.0
Step: 16160, train/grad_norm: 3.1460725136156498e-09
Step: 16160, train/learning_rate: 3.077106157434173e-05
Step: 16160, train/epoch: 3.845787763595581
Step: 16170, train/loss: 0.21879999339580536
Step: 16170, train/grad_norm: 3.7678915987271466e-08
Step: 16170, train/learning_rate: 3.075916174566373e-05
Step: 16170, train/epoch: 3.848167657852173
Step: 16180, train/loss: 0.007499999832361937
Step: 16180, train/grad_norm: 44.83838653564453
Step: 16180, train/learning_rate: 3.0747261916985735e-05
Step: 16180, train/epoch: 3.8505473136901855
Step: 16190, train/loss: 0.0
Step: 16190, train/grad_norm: 2.843890570147778e-06
Step: 16190, train/learning_rate: 3.0735365726286545e-05
Step: 16190, train/epoch: 3.8529272079467773
Step: 16200, train/loss: 0.0
Step: 16200, train/grad_norm: 0.0013446116354316473
Step: 16200, train/learning_rate: 3.072346589760855e-05
Step: 16200, train/epoch: 3.855307102203369
Step: 16210, train/loss: 0.0
Step: 16210, train/grad_norm: 4.109229848836549e-06
Step: 16210, train/learning_rate: 3.071156606893055e-05
Step: 16210, train/epoch: 3.857686758041382
Step: 16220, train/loss: 0.0
Step: 16220, train/grad_norm: 0.00040033887489698827
Step: 16220, train/learning_rate: 3.0699666240252554e-05
Step: 16220, train/epoch: 3.8600666522979736
Step: 16230, train/loss: 0.0
Step: 16230, train/grad_norm: 2.3167007384472527e-05
Step: 16230, train/learning_rate: 3.068776641157456e-05
Step: 16230, train/epoch: 3.8624465465545654
Step: 16240, train/loss: 0.0
Step: 16240, train/grad_norm: 3.6850323681392183e-07
Step: 16240, train/learning_rate: 3.067587022087537e-05
Step: 16240, train/epoch: 3.864826202392578
Step: 16250, train/loss: 0.000699999975040555
Step: 16250, train/grad_norm: 3.89057868233067e-06
Step: 16250, train/learning_rate: 3.066397039219737e-05
Step: 16250, train/epoch: 3.86720609664917
Step: 16260, train/loss: 0.0
Step: 16260, train/grad_norm: 3.814366777987743e-07
Step: 16260, train/learning_rate: 3.0652070563519374e-05
Step: 16260, train/epoch: 3.8695859909057617
Step: 16270, train/loss: 0.0
Step: 16270, train/grad_norm: 6.627148650295567e-06
Step: 16270, train/learning_rate: 3.0640170734841377e-05
Step: 16270, train/epoch: 3.8719656467437744
Step: 16280, train/loss: 0.0
Step: 16280, train/grad_norm: 7.755695641264992e-09
Step: 16280, train/learning_rate: 3.062827090616338e-05
Step: 16280, train/epoch: 3.874345541000366
Step: 16290, train/loss: 0.0
Step: 16290, train/grad_norm: 7.670914214941149e-08
Step: 16290, train/learning_rate: 3.061637471546419e-05
Step: 16290, train/epoch: 3.876725435256958
Step: 16300, train/loss: 0.0
Step: 16300, train/grad_norm: 7.038671014925058e-07
Step: 16300, train/learning_rate: 3.060447488678619e-05
Step: 16300, train/epoch: 3.8791050910949707
Step: 16310, train/loss: 0.0
Step: 16310, train/grad_norm: 6.176645115374413e-07
Step: 16310, train/learning_rate: 3.0592575058108196e-05
Step: 16310, train/epoch: 3.8814849853515625
Step: 16320, train/loss: 0.0
Step: 16320, train/grad_norm: 0.0004005925147794187
Step: 16320, train/learning_rate: 3.05806752294302e-05
Step: 16320, train/epoch: 3.8838648796081543
Step: 16330, train/loss: 0.0
Step: 16330, train/grad_norm: 5.828098004201365e-10
Step: 16330, train/learning_rate: 3.05687754007522e-05
Step: 16330, train/epoch: 3.886244535446167
Step: 16340, train/loss: 0.0
Step: 16340, train/grad_norm: 1.1566506145754829e-05
Step: 16340, train/learning_rate: 3.055687921005301e-05
Step: 16340, train/epoch: 3.888624429702759
Step: 16350, train/loss: 0.0
Step: 16350, train/grad_norm: 2.7515493883356612e-08
Step: 16350, train/learning_rate: 3.0544979381375015e-05
Step: 16350, train/epoch: 3.8910043239593506
Step: 16360, train/loss: 0.0
Step: 16360, train/grad_norm: 4.2833158886423917e-07
Step: 16360, train/learning_rate: 3.053307955269702e-05
Step: 16360, train/epoch: 3.8933842182159424
Step: 16370, train/loss: 0.0
Step: 16370, train/grad_norm: 7.894663411889269e-08
Step: 16370, train/learning_rate: 3.052117972401902e-05
Step: 16370, train/epoch: 3.895763874053955
Step: 16380, train/loss: 0.0
Step: 16380, train/grad_norm: 9.373869147566438e-08
Step: 16380, train/learning_rate: 3.0509281714330427e-05
Step: 16380, train/epoch: 3.898143768310547
Step: 16390, train/loss: 0.05860000103712082
Step: 16390, train/grad_norm: 7.557726348750293e-05
Step: 16390, train/learning_rate: 3.049738188565243e-05
Step: 16390, train/epoch: 3.9005236625671387
Step: 16400, train/loss: 0.13369999825954437
Step: 16400, train/grad_norm: 0.3337899148464203
Step: 16400, train/learning_rate: 3.0485483875963837e-05
Step: 16400, train/epoch: 3.9029033184051514
Step: 16410, train/loss: 0.027400000020861626
Step: 16410, train/grad_norm: 1.0484338730520903e-08
Step: 16410, train/learning_rate: 3.047358404728584e-05
Step: 16410, train/epoch: 3.905283212661743
Step: 16420, train/loss: 0.0
Step: 16420, train/grad_norm: 1.634279556128604e-07
Step: 16420, train/learning_rate: 3.0461684218607843e-05
Step: 16420, train/epoch: 3.907663106918335
Step: 16430, train/loss: 0.0
Step: 16430, train/grad_norm: 4.352957716502459e-10
Step: 16430, train/learning_rate: 3.044978620891925e-05
Step: 16430, train/epoch: 3.9100427627563477
Step: 16440, train/loss: 0.0
Step: 16440, train/grad_norm: 4.791506853507599e-06
Step: 16440, train/learning_rate: 3.0437886380241252e-05
Step: 16440, train/epoch: 3.9124226570129395
Step: 16450, train/loss: 0.0
Step: 16450, train/grad_norm: 1.3619184755953029e-05
Step: 16450, train/learning_rate: 3.042598837055266e-05
Step: 16450, train/epoch: 3.9148025512695312
Step: 16460, train/loss: 0.0
Step: 16460, train/grad_norm: 8.091617019090336e-06
Step: 16460, train/learning_rate: 3.0414088541874662e-05
Step: 16460, train/epoch: 3.917182207107544
Step: 16470, train/loss: 0.0
Step: 16470, train/grad_norm: 4.157375030899857e-07
Step: 16470, train/learning_rate: 3.0402188713196665e-05
Step: 16470, train/epoch: 3.9195621013641357
Step: 16480, train/loss: 0.0
Step: 16480, train/grad_norm: 8.069589966908097e-06
Step: 16480, train/learning_rate: 3.039029070350807e-05
Step: 16480, train/epoch: 3.9219419956207275
Step: 16490, train/loss: 0.0
Step: 16490, train/grad_norm: 3.6079777601116803e-06
Step: 16490, train/learning_rate: 3.0378390874830075e-05
Step: 16490, train/epoch: 3.9243216514587402
Step: 16500, train/loss: 0.0
Step: 16500, train/grad_norm: 1.6414384296226103e-09
Step: 16500, train/learning_rate: 3.036649286514148e-05
Step: 16500, train/epoch: 3.926701545715332
Step: 16510, train/loss: 0.0
Step: 16510, train/grad_norm: 1.3315867875007825e-07
Step: 16510, train/learning_rate: 3.0354593036463484e-05
Step: 16510, train/epoch: 3.929081439971924
Step: 16520, train/loss: 0.00019999999494757503
Step: 16520, train/grad_norm: 5.974696978228167e-05
Step: 16520, train/learning_rate: 3.0342693207785487e-05
Step: 16520, train/epoch: 3.9314610958099365
Step: 16530, train/loss: 0.0
Step: 16530, train/grad_norm: 1.8611433915793896e-05
Step: 16530, train/learning_rate: 3.0330795198096894e-05
Step: 16530, train/epoch: 3.9338409900665283
Step: 16540, train/loss: 0.0
Step: 16540, train/grad_norm: 9.16443310128301e-11
Step: 16540, train/learning_rate: 3.0318895369418897e-05
Step: 16540, train/epoch: 3.93622088432312
Step: 16550, train/loss: 0.17669999599456787
Step: 16550, train/grad_norm: 5.0457860112373965e-08
Step: 16550, train/learning_rate: 3.0306997359730303e-05
Step: 16550, train/epoch: 3.938600778579712
Step: 16560, train/loss: 0.0
Step: 16560, train/grad_norm: 0.00014063013077247888
Step: 16560, train/learning_rate: 3.0295097531052306e-05
Step: 16560, train/epoch: 3.9409804344177246
Step: 16570, train/loss: 0.0
Step: 16570, train/grad_norm: 0.018216794356703758
Step: 16570, train/learning_rate: 3.028319770237431e-05
Step: 16570, train/epoch: 3.9433603286743164
Step: 16580, train/loss: 0.0
Step: 16580, train/grad_norm: 1.2463405255402904e-05
Step: 16580, train/learning_rate: 3.0271299692685716e-05
Step: 16580, train/epoch: 3.945740222930908
Step: 16590, train/loss: 0.0008999999845400453
Step: 16590, train/grad_norm: 8.557874934922438e-06
Step: 16590, train/learning_rate: 3.025939986400772e-05
Step: 16590, train/epoch: 3.948119878768921
Step: 16600, train/loss: 0.006800000090152025
Step: 16600, train/grad_norm: 0.0004959063953720033
Step: 16600, train/learning_rate: 3.0247501854319125e-05
Step: 16600, train/epoch: 3.9504997730255127
Step: 16610, train/loss: 0.0
Step: 16610, train/grad_norm: 2.389736550867383e-07
Step: 16610, train/learning_rate: 3.023560202564113e-05
Step: 16610, train/epoch: 3.9528796672821045
Step: 16620, train/loss: 0.0
Step: 16620, train/grad_norm: 5.218113230398558e-08
Step: 16620, train/learning_rate: 3.022370219696313e-05
Step: 16620, train/epoch: 3.955259323120117
Step: 16630, train/loss: 0.0
Step: 16630, train/grad_norm: 2.430077802273445e-05
Step: 16630, train/learning_rate: 3.0211804187274538e-05
Step: 16630, train/epoch: 3.957639217376709
Step: 16640, train/loss: 0.0
Step: 16640, train/grad_norm: 6.575002771569416e-06
Step: 16640, train/learning_rate: 3.019990435859654e-05
Step: 16640, train/epoch: 3.960019111633301
Step: 16650, train/loss: 0.0
Step: 16650, train/grad_norm: 6.871404184494168e-05
Step: 16650, train/learning_rate: 3.0188006348907948e-05
Step: 16650, train/epoch: 3.9623987674713135
Step: 16660, train/loss: 0.0
Step: 16660, train/grad_norm: 6.104304475229583e-07
Step: 16660, train/learning_rate: 3.017610652022995e-05
Step: 16660, train/epoch: 3.9647786617279053
Step: 16670, train/loss: 9.999999747378752e-05
Step: 16670, train/grad_norm: 6.456183473346755e-05
Step: 16670, train/learning_rate: 3.0164206691551954e-05
Step: 16670, train/epoch: 3.967158555984497
Step: 16680, train/loss: 0.0
Step: 16680, train/grad_norm: 1.8044271143935475e-07
Step: 16680, train/learning_rate: 3.015230868186336e-05
Step: 16680, train/epoch: 3.9695382118225098
Step: 16690, train/loss: 0.0
Step: 16690, train/grad_norm: 4.5763533762510633e-07
Step: 16690, train/learning_rate: 3.0140408853185363e-05
Step: 16690, train/epoch: 3.9719181060791016
Step: 16700, train/loss: 0.0
Step: 16700, train/grad_norm: 9.236565529135987e-05
Step: 16700, train/learning_rate: 3.012851084349677e-05
Step: 16700, train/epoch: 3.9742980003356934
Step: 16710, train/loss: 0.0
Step: 16710, train/grad_norm: 3.176799737047986e-06
Step: 16710, train/learning_rate: 3.0116611014818773e-05
Step: 16710, train/epoch: 3.976677656173706
Step: 16720, train/loss: 0.0
Step: 16720, train/grad_norm: 5.63300886824436e-07
Step: 16720, train/learning_rate: 3.0104711186140776e-05
Step: 16720, train/epoch: 3.979057550430298
Step: 16730, train/loss: 0.0
Step: 16730, train/grad_norm: 0.0001738749269861728
Step: 16730, train/learning_rate: 3.0092813176452182e-05
Step: 16730, train/epoch: 3.9814374446868896
Step: 16740, train/loss: 0.0
Step: 16740, train/grad_norm: 0.00011073447967646644
Step: 16740, train/learning_rate: 3.0080913347774185e-05
Step: 16740, train/epoch: 3.9838173389434814
Step: 16750, train/loss: 0.0
Step: 16750, train/grad_norm: 0.0007809619419276714
Step: 16750, train/learning_rate: 3.0069015338085592e-05
Step: 16750, train/epoch: 3.986196994781494
Step: 16760, train/loss: 0.0
Step: 16760, train/grad_norm: 4.379581586277936e-09
Step: 16760, train/learning_rate: 3.0057115509407595e-05
Step: 16760, train/epoch: 3.988576889038086
Step: 16770, train/loss: 0.0
Step: 16770, train/grad_norm: 2.3282104848476592e-06
Step: 16770, train/learning_rate: 3.0045215680729598e-05
Step: 16770, train/epoch: 3.9909567832946777
Step: 16780, train/loss: 0.0
Step: 16780, train/grad_norm: 7.671088496863376e-06
Step: 16780, train/learning_rate: 3.0033317671041004e-05
Step: 16780, train/epoch: 3.9933364391326904
Step: 16790, train/loss: 0.0
Step: 16790, train/grad_norm: 6.447060627579049e-07
Step: 16790, train/learning_rate: 3.0021417842363007e-05
Step: 16790, train/epoch: 3.9957163333892822
Step: 16800, train/loss: 0.0
Step: 16800, train/grad_norm: 4.5059096009936184e-07
Step: 16800, train/learning_rate: 3.0009519832674414e-05
Step: 16800, train/epoch: 3.998096227645874
Step: 16808, eval/loss: 0.04192066937685013
Step: 16808, eval/accuracy: 0.9943079352378845
Step: 16808, eval/f1: 0.9939787983894348
Step: 16808, eval/runtime: 708.1148071289062
Step: 16808, eval/samples_per_second: 10.17199993133545
Step: 16808, eval/steps_per_second: 1.2719999551773071
Step: 16808, train/epoch: 4.0
Step: 16810, train/loss: 0.0
Step: 16810, train/grad_norm: 5.078831236460246e-06
Step: 16810, train/learning_rate: 2.9997620003996417e-05
Step: 16810, train/epoch: 4.000475883483887
Step: 16820, train/loss: 0.0
Step: 16820, train/grad_norm: 1.9707486842435173e-07
Step: 16820, train/learning_rate: 2.9985721994307823e-05
Step: 16820, train/epoch: 4.0028557777404785
Step: 16830, train/loss: 0.0
Step: 16830, train/grad_norm: 3.647885023383424e-05
Step: 16830, train/learning_rate: 2.9973822165629826e-05
Step: 16830, train/epoch: 4.00523567199707
Step: 16840, train/loss: 0.0
Step: 16840, train/grad_norm: 0.00096107431454584
Step: 16840, train/learning_rate: 2.996192233695183e-05
Step: 16840, train/epoch: 4.007615566253662
Step: 16850, train/loss: 0.0
Step: 16850, train/grad_norm: 1.4333871149574406e-05
Step: 16850, train/learning_rate: 2.9950024327263236e-05
Step: 16850, train/epoch: 4.009995460510254
Step: 16860, train/loss: 0.0
Step: 16860, train/grad_norm: 3.360009941388853e-05
Step: 16860, train/learning_rate: 2.993812449858524e-05
Step: 16860, train/epoch: 4.0123748779296875
Step: 16870, train/loss: 0.0
Step: 16870, train/grad_norm: 1.3946830222266726e-05
Step: 16870, train/learning_rate: 2.9926226488896646e-05
Step: 16870, train/epoch: 4.014754772186279
Step: 16880, train/loss: 0.0
Step: 16880, train/grad_norm: 0.047316353768110275
Step: 16880, train/learning_rate: 2.991432666021865e-05
Step: 16880, train/epoch: 4.017134666442871
Step: 16890, train/loss: 0.0
Step: 16890, train/grad_norm: 5.443342274702445e-07
Step: 16890, train/learning_rate: 2.990242683154065e-05
Step: 16890, train/epoch: 4.019514560699463
Step: 16900, train/loss: 0.0
Step: 16900, train/grad_norm: 9.541002509649843e-05
Step: 16900, train/learning_rate: 2.9890528821852058e-05
Step: 16900, train/epoch: 4.021894454956055
Step: 16910, train/loss: 0.0
Step: 16910, train/grad_norm: 2.9945883852633415e-06
Step: 16910, train/learning_rate: 2.987862899317406e-05
Step: 16910, train/epoch: 4.0242743492126465
Step: 16920, train/loss: 0.0
Step: 16920, train/grad_norm: 1.7085518777548714e-08
Step: 16920, train/learning_rate: 2.9866730983485468e-05
Step: 16920, train/epoch: 4.02665376663208
Step: 16930, train/loss: 0.0
Step: 16930, train/grad_norm: 2.3345695865373273e-07
Step: 16930, train/learning_rate: 2.985483115480747e-05
Step: 16930, train/epoch: 4.029033660888672
Step: 16940, train/loss: 0.0
Step: 16940, train/grad_norm: 7.917761308817717e-07
Step: 16940, train/learning_rate: 2.9842931326129474e-05
Step: 16940, train/epoch: 4.031413555145264
Step: 16950, train/loss: 0.0
Step: 16950, train/grad_norm: 9.566775588609744e-06
Step: 16950, train/learning_rate: 2.983103331644088e-05
Step: 16950, train/epoch: 4.0337934494018555
Step: 16960, train/loss: 0.0
Step: 16960, train/grad_norm: 7.898442078158041e-08
Step: 16960, train/learning_rate: 2.9819133487762883e-05
Step: 16960, train/epoch: 4.036173343658447
Step: 16970, train/loss: 0.0
Step: 16970, train/grad_norm: 1.3403428056335542e-06
Step: 16970, train/learning_rate: 2.980723547807429e-05
Step: 16970, train/epoch: 4.038553237915039
Step: 16980, train/loss: 0.0
Step: 16980, train/grad_norm: 1.8834020011126995e-05
Step: 16980, train/learning_rate: 2.9795335649396293e-05
Step: 16980, train/epoch: 4.040932655334473
Step: 16990, train/loss: 0.0
Step: 16990, train/grad_norm: 0.000271862605586648
Step: 16990, train/learning_rate: 2.9783435820718296e-05
Step: 16990, train/epoch: 4.0433125495910645
Step: 17000, train/loss: 0.0
Step: 17000, train/grad_norm: 0.005681311711668968
Step: 17000, train/learning_rate: 2.9771537811029702e-05
Step: 17000, train/epoch: 4.045692443847656
Step: 17010, train/loss: 0.0
Step: 17010, train/grad_norm: 2.7363560661797237e-07
Step: 17010, train/learning_rate: 2.9759637982351705e-05
Step: 17010, train/epoch: 4.048072338104248
Step: 17020, train/loss: 0.0
Step: 17020, train/grad_norm: 5.200180567044299e-06
Step: 17020, train/learning_rate: 2.9747739972663112e-05
Step: 17020, train/epoch: 4.05045223236084
Step: 17030, train/loss: 0.0
Step: 17030, train/grad_norm: 4.73312411486404e-06
Step: 17030, train/learning_rate: 2.9735840143985115e-05
Step: 17030, train/epoch: 4.052832126617432
Step: 17040, train/loss: 0.0
Step: 17040, train/grad_norm: 5.386182806432771e-07
Step: 17040, train/learning_rate: 2.9723940315307118e-05
Step: 17040, train/epoch: 4.055212020874023
Step: 17050, train/loss: 0.0
Step: 17050, train/grad_norm: 1.5397137076433864e-07
Step: 17050, train/learning_rate: 2.9712042305618525e-05
Step: 17050, train/epoch: 4.057591438293457
Step: 17060, train/loss: 0.0
Step: 17060, train/grad_norm: 1.3663950085174292e-05
Step: 17060, train/learning_rate: 2.9700142476940528e-05
Step: 17060, train/epoch: 4.059971332550049
Step: 17070, train/loss: 0.0
Step: 17070, train/grad_norm: 2.8000727070320863e-08
Step: 17070, train/learning_rate: 2.9688244467251934e-05
Step: 17070, train/epoch: 4.062351226806641
Step: 17080, train/loss: 0.0
Step: 17080, train/grad_norm: 1.8763590503567684e-07
Step: 17080, train/learning_rate: 2.9676344638573937e-05
Step: 17080, train/epoch: 4.064731121063232
Step: 17090, train/loss: 0.0
Step: 17090, train/grad_norm: 1.1370250831532758e-05
Step: 17090, train/learning_rate: 2.966444480989594e-05
Step: 17090, train/epoch: 4.067111015319824
Step: 17100, train/loss: 0.0
Step: 17100, train/grad_norm: 3.750915766431717e-06
Step: 17100, train/learning_rate: 2.9652546800207347e-05
Step: 17100, train/epoch: 4.069490909576416
Step: 17110, train/loss: 0.0
Step: 17110, train/grad_norm: 0.0004968493594788015
Step: 17110, train/learning_rate: 2.964064697152935e-05
Step: 17110, train/epoch: 4.07187032699585
Step: 17120, train/loss: 0.0
Step: 17120, train/grad_norm: 3.278809799667215e-06
Step: 17120, train/learning_rate: 2.9628748961840756e-05
Step: 17120, train/epoch: 4.074250221252441
Step: 17130, train/loss: 0.0
Step: 17130, train/grad_norm: 6.786390258639585e-06
Step: 17130, train/learning_rate: 2.961684913316276e-05
Step: 17130, train/epoch: 4.076630115509033
Step: 17140, train/loss: 0.0
Step: 17140, train/grad_norm: 0.0007759454310871661
Step: 17140, train/learning_rate: 2.9604949304484762e-05
Step: 17140, train/epoch: 4.079010009765625
Step: 17150, train/loss: 0.0
Step: 17150, train/grad_norm: 8.844068588587106e-07
Step: 17150, train/learning_rate: 2.959305129479617e-05
Step: 17150, train/epoch: 4.081389904022217
Step: 17160, train/loss: 0.0
Step: 17160, train/grad_norm: 1.1600867537708837e-06
Step: 17160, train/learning_rate: 2.9581151466118172e-05
Step: 17160, train/epoch: 4.083769798278809
Step: 17170, train/loss: 0.0
Step: 17170, train/grad_norm: 4.748601440951461e-06
Step: 17170, train/learning_rate: 2.956925345642958e-05
Step: 17170, train/epoch: 4.086149215698242
Step: 17180, train/loss: 0.0
Step: 17180, train/grad_norm: 7.531655228376621e-06
Step: 17180, train/learning_rate: 2.955735362775158e-05
Step: 17180, train/epoch: 4.088529109954834
Step: 17190, train/loss: 0.0
Step: 17190, train/grad_norm: 1.0605827327481165e-07
Step: 17190, train/learning_rate: 2.9545453799073584e-05
Step: 17190, train/epoch: 4.090909004211426
Step: 17200, train/loss: 0.0
Step: 17200, train/grad_norm: 1.420636275284437e-09
Step: 17200, train/learning_rate: 2.953355578938499e-05
Step: 17200, train/epoch: 4.093288898468018
Step: 17210, train/loss: 0.0
Step: 17210, train/grad_norm: 2.6881609755946556e-06
Step: 17210, train/learning_rate: 2.9521655960706994e-05
Step: 17210, train/epoch: 4.095668792724609
Step: 17220, train/loss: 0.0
Step: 17220, train/grad_norm: 2.007947870197313e-07
Step: 17220, train/learning_rate: 2.95097579510184e-05
Step: 17220, train/epoch: 4.098048686981201
Step: 17230, train/loss: 0.0
Step: 17230, train/grad_norm: 8.66634218255058e-06
Step: 17230, train/learning_rate: 2.9497858122340403e-05
Step: 17230, train/epoch: 4.100428581237793
Step: 17240, train/loss: 0.0
Step: 17240, train/grad_norm: 1.0796308913540997e-07
Step: 17240, train/learning_rate: 2.9485958293662407e-05
Step: 17240, train/epoch: 4.102807998657227
Step: 17250, train/loss: 0.0
Step: 17250, train/grad_norm: 3.2928244309005095e-06
Step: 17250, train/learning_rate: 2.9474060283973813e-05
Step: 17250, train/epoch: 4.105187892913818
Step: 17260, train/loss: 0.0
Step: 17260, train/grad_norm: 4.281031351638376e-07
Step: 17260, train/learning_rate: 2.9462160455295816e-05
Step: 17260, train/epoch: 4.10756778717041
Step: 17270, train/loss: 0.0
Step: 17270, train/grad_norm: 2.3232705643749796e-06
Step: 17270, train/learning_rate: 2.9450262445607223e-05
Step: 17270, train/epoch: 4.109947681427002
Step: 17280, train/loss: 0.0
Step: 17280, train/grad_norm: 2.3118611807149136e-06
Step: 17280, train/learning_rate: 2.9438362616929226e-05
Step: 17280, train/epoch: 4.112327575683594
Step: 17290, train/loss: 0.0
Step: 17290, train/grad_norm: 7.033814597434684e-08
Step: 17290, train/learning_rate: 2.942646278825123e-05
Step: 17290, train/epoch: 4.1147074699401855
Step: 17300, train/loss: 0.0
Step: 17300, train/grad_norm: 0.0008921558619476855
Step: 17300, train/learning_rate: 2.9414564778562635e-05
Step: 17300, train/epoch: 4.117086887359619
Step: 17310, train/loss: 0.0
Step: 17310, train/grad_norm: 0.0029195158276706934
Step: 17310, train/learning_rate: 2.9402664949884638e-05
Step: 17310, train/epoch: 4.119466781616211
Step: 17320, train/loss: 0.0
Step: 17320, train/grad_norm: 5.553221399168251e-06
Step: 17320, train/learning_rate: 2.9390766940196045e-05
Step: 17320, train/epoch: 4.121846675872803
Step: 17330, train/loss: 0.0
Step: 17330, train/grad_norm: 5.803019007544208e-07
Step: 17330, train/learning_rate: 2.9378867111518048e-05
Step: 17330, train/epoch: 4.1242265701293945
Step: 17340, train/loss: 0.0
Step: 17340, train/grad_norm: 1.1251569986825416e-07
Step: 17340, train/learning_rate: 2.936696728284005e-05
Step: 17340, train/epoch: 4.126606464385986
Step: 17350, train/loss: 0.0
Step: 17350, train/grad_norm: 5.1146717083838666e-08
Step: 17350, train/learning_rate: 2.9355069273151457e-05
Step: 17350, train/epoch: 4.128986358642578
Step: 17360, train/loss: 0.0
Step: 17360, train/grad_norm: 3.0775220238865586e-07
Step: 17360, train/learning_rate: 2.934316944447346e-05
Step: 17360, train/epoch: 4.13136625289917
Step: 17370, train/loss: 0.0
Step: 17370, train/grad_norm: 3.8593356066485285e-07
Step: 17370, train/learning_rate: 2.9331271434784867e-05
Step: 17370, train/epoch: 4.1337456703186035
Step: 17380, train/loss: 0.0
Step: 17380, train/grad_norm: 0.0002814193139784038
Step: 17380, train/learning_rate: 2.931937160610687e-05
Step: 17380, train/epoch: 4.136125564575195
Step: 17390, train/loss: 0.0
Step: 17390, train/grad_norm: 4.20876034468165e-07
Step: 17390, train/learning_rate: 2.9307471777428873e-05
Step: 17390, train/epoch: 4.138505458831787
Step: 17400, train/loss: 0.0
Step: 17400, train/grad_norm: 9.120288524400166e-08
Step: 17400, train/learning_rate: 2.929557376774028e-05
Step: 17400, train/epoch: 4.140885353088379
Step: 17410, train/loss: 0.0
Step: 17410, train/grad_norm: 4.382862698548706e-06
Step: 17410, train/learning_rate: 2.9283673939062282e-05
Step: 17410, train/epoch: 4.143265247344971
Step: 17420, train/loss: 0.0
Step: 17420, train/grad_norm: 9.670347026258241e-06
Step: 17420, train/learning_rate: 2.927177592937369e-05
Step: 17420, train/epoch: 4.1456451416015625
Step: 17430, train/loss: 0.0
Step: 17430, train/grad_norm: 1.4406293935564918e-08
Step: 17430, train/learning_rate: 2.9259876100695692e-05
Step: 17430, train/epoch: 4.148024559020996
Step: 17440, train/loss: 0.0
Step: 17440, train/grad_norm: 3.253020622651093e-05
Step: 17440, train/learning_rate: 2.9247976272017695e-05
Step: 17440, train/epoch: 4.150404453277588
Step: 17450, train/loss: 0.0
Step: 17450, train/grad_norm: 3.413207707581023e-07
Step: 17450, train/learning_rate: 2.92360782623291e-05
Step: 17450, train/epoch: 4.15278434753418
Step: 17460, train/loss: 0.0
Step: 17460, train/grad_norm: 3.76303699312075e-09
Step: 17460, train/learning_rate: 2.9224178433651105e-05
Step: 17460, train/epoch: 4.1551642417907715
Step: 17470, train/loss: 0.0
Step: 17470, train/grad_norm: 2.801537220875616e-06
Step: 17470, train/learning_rate: 2.921228042396251e-05
Step: 17470, train/epoch: 4.157544136047363
Step: 17480, train/loss: 0.0
Step: 17480, train/grad_norm: 1.8652580990874412e-07
Step: 17480, train/learning_rate: 2.9200380595284514e-05
Step: 17480, train/epoch: 4.159924030303955
Step: 17490, train/loss: 0.0
Step: 17490, train/grad_norm: 9.147393029707018e-06
Step: 17490, train/learning_rate: 2.9188480766606517e-05
Step: 17490, train/epoch: 4.162303447723389
Step: 17500, train/loss: 9.999999747378752e-05
Step: 17500, train/grad_norm: 2.715944162900996e-07
Step: 17500, train/learning_rate: 2.9176582756917924e-05
Step: 17500, train/epoch: 4.1646833419799805
Step: 17510, train/loss: 0.0
Step: 17510, train/grad_norm: 5.422400590759935e-07
Step: 17510, train/learning_rate: 2.9164682928239927e-05
Step: 17510, train/epoch: 4.167063236236572
Step: 17520, train/loss: 0.0
Step: 17520, train/grad_norm: 2.7293251605442492e-06
Step: 17520, train/learning_rate: 2.9152784918551333e-05
Step: 17520, train/epoch: 4.169443130493164
Step: 17530, train/loss: 0.0
Step: 17530, train/grad_norm: 5.72114970509574e-07
Step: 17530, train/learning_rate: 2.9140885089873336e-05
Step: 17530, train/epoch: 4.171823024749756
Step: 17540, train/loss: 0.0
Step: 17540, train/grad_norm: 4.969980427205201e-09
Step: 17540, train/learning_rate: 2.9128987080184743e-05
Step: 17540, train/epoch: 4.174202919006348
Step: 17550, train/loss: 0.0
Step: 17550, train/grad_norm: 1.7220274628471088e-07
Step: 17550, train/learning_rate: 2.9117087251506746e-05
Step: 17550, train/epoch: 4.1765828132629395
Step: 17560, train/loss: 0.0
Step: 17560, train/grad_norm: 1.1371269437177034e-07
Step: 17560, train/learning_rate: 2.910518742282875e-05
Step: 17560, train/epoch: 4.178962230682373
Step: 17570, train/loss: 0.0
Step: 17570, train/grad_norm: 2.626351943035843e-06
Step: 17570, train/learning_rate: 2.9093289413140155e-05
Step: 17570, train/epoch: 4.181342124938965
Step: 17580, train/loss: 0.0
Step: 17580, train/grad_norm: 5.78630088288179e-10
Step: 17580, train/learning_rate: 2.908138958446216e-05
Step: 17580, train/epoch: 4.183722019195557
Step: 17590, train/loss: 0.0
Step: 17590, train/grad_norm: 6.570567165908869e-06
Step: 17590, train/learning_rate: 2.9069491574773565e-05
Step: 17590, train/epoch: 4.186101913452148
Step: 17600, train/loss: 0.0
Step: 17600, train/grad_norm: 1.616152722760944e-08
Step: 17600, train/learning_rate: 2.9057591746095568e-05
Step: 17600, train/epoch: 4.18848180770874
Step: 17610, train/loss: 0.0
Step: 17610, train/grad_norm: 4.297083222581932e-08
Step: 17610, train/learning_rate: 2.904569191741757e-05
Step: 17610, train/epoch: 4.190861701965332
Step: 17620, train/loss: 0.0
Step: 17620, train/grad_norm: 3.000252490892308e-08
Step: 17620, train/learning_rate: 2.9033793907728978e-05
Step: 17620, train/epoch: 4.193241119384766
Step: 17630, train/loss: 0.0
Step: 17630, train/grad_norm: 1.918237728659733e-07
Step: 17630, train/learning_rate: 2.902189407905098e-05
Step: 17630, train/epoch: 4.195621013641357
Step: 17640, train/loss: 0.0
Step: 17640, train/grad_norm: 1.0027688546188074e-07
Step: 17640, train/learning_rate: 2.9009996069362387e-05
Step: 17640, train/epoch: 4.198000907897949
Step: 17650, train/loss: 0.0
Step: 17650, train/grad_norm: 8.991131039692846e-08
Step: 17650, train/learning_rate: 2.899809624068439e-05
Step: 17650, train/epoch: 4.200380802154541
Step: 17660, train/loss: 0.0
Step: 17660, train/grad_norm: 2.3500078896177e-06
Step: 17660, train/learning_rate: 2.8986196412006393e-05
Step: 17660, train/epoch: 4.202760696411133
Step: 17670, train/loss: 0.0005000000237487257
Step: 17670, train/grad_norm: 2.8184258482610858e-08
Step: 17670, train/learning_rate: 2.89742984023178e-05
Step: 17670, train/epoch: 4.205140590667725
Step: 17680, train/loss: 0.03400000184774399
Step: 17680, train/grad_norm: 6.676098252000884e-08
Step: 17680, train/learning_rate: 2.8962398573639803e-05
Step: 17680, train/epoch: 4.207520008087158
Step: 17690, train/loss: 0.0
Step: 17690, train/grad_norm: 3.1407598726218566e-06
Step: 17690, train/learning_rate: 2.895050056395121e-05
Step: 17690, train/epoch: 4.20989990234375
Step: 17700, train/loss: 0.0
Step: 17700, train/grad_norm: 3.043868446184206e-06
Step: 17700, train/learning_rate: 2.8938600735273212e-05
Step: 17700, train/epoch: 4.212279796600342
Step: 17710, train/loss: 0.0
Step: 17710, train/grad_norm: 5.971496214174365e-11
Step: 17710, train/learning_rate: 2.8926700906595215e-05
Step: 17710, train/epoch: 4.214659690856934
Step: 17720, train/loss: 0.0
Step: 17720, train/grad_norm: 0.00016331681399606168
Step: 17720, train/learning_rate: 2.8914802896906622e-05
Step: 17720, train/epoch: 4.217039585113525
Step: 17730, train/loss: 0.0
Step: 17730, train/grad_norm: 2.4722499603058168e-09
Step: 17730, train/learning_rate: 2.8902903068228625e-05
Step: 17730, train/epoch: 4.219419479370117
Step: 17740, train/loss: 0.014299999922513962
Step: 17740, train/grad_norm: 1.408285754678218e-08
Step: 17740, train/learning_rate: 2.889100505854003e-05
Step: 17740, train/epoch: 4.221799373626709
Step: 17750, train/loss: 0.0
Step: 17750, train/grad_norm: 5.580784723235865e-09
Step: 17750, train/learning_rate: 2.8879105229862034e-05
Step: 17750, train/epoch: 4.224178791046143
Step: 17760, train/loss: 0.0
Step: 17760, train/grad_norm: 1.1801015631363043e-07
Step: 17760, train/learning_rate: 2.8867205401184037e-05
Step: 17760, train/epoch: 4.226558685302734
Step: 17770, train/loss: 0.0
Step: 17770, train/grad_norm: 2.3118441189184935e-11
Step: 17770, train/learning_rate: 2.8855307391495444e-05
Step: 17770, train/epoch: 4.228938579559326
Step: 17780, train/loss: 0.01269999984651804
Step: 17780, train/grad_norm: 3.324101882640207e-08
Step: 17780, train/learning_rate: 2.8843407562817447e-05
Step: 17780, train/epoch: 4.231318473815918
Step: 17790, train/loss: 0.0
Step: 17790, train/grad_norm: 3.807679860301505e-08
Step: 17790, train/learning_rate: 2.8831509553128853e-05
Step: 17790, train/epoch: 4.23369836807251
Step: 17800, train/loss: 0.0
Step: 17800, train/grad_norm: 1.9572397036426992e-07
Step: 17800, train/learning_rate: 2.8819609724450856e-05
Step: 17800, train/epoch: 4.236078262329102
Step: 17810, train/loss: 0.0
Step: 17810, train/grad_norm: 0.007173174526542425
Step: 17810, train/learning_rate: 2.880770989577286e-05
Step: 17810, train/epoch: 4.238457679748535
Step: 17820, train/loss: 0.0
Step: 17820, train/grad_norm: 1.2123758885707048e-08
Step: 17820, train/learning_rate: 2.8795811886084266e-05
Step: 17820, train/epoch: 4.240837574005127
Step: 17830, train/loss: 0.0
Step: 17830, train/grad_norm: 2.004019705736937e-08
Step: 17830, train/learning_rate: 2.878391205740627e-05
Step: 17830, train/epoch: 4.243217468261719
Step: 17840, train/loss: 0.00019999999494757503
Step: 17840, train/grad_norm: 2.4304654289153405e-06
Step: 17840, train/learning_rate: 2.8772014047717676e-05
Step: 17840, train/epoch: 4.2455973625183105
Step: 17850, train/loss: 0.0
Step: 17850, train/grad_norm: 1.232545177032307e-10
Step: 17850, train/learning_rate: 2.876011421903968e-05
Step: 17850, train/epoch: 4.247977256774902
Step: 17860, train/loss: 0.0
Step: 17860, train/grad_norm: 5.189427110252609e-09
Step: 17860, train/learning_rate: 2.874821439036168e-05
Step: 17860, train/epoch: 4.250357151031494
Step: 17870, train/loss: 0.0
Step: 17870, train/grad_norm: 1.9842333642827725e-07
Step: 17870, train/learning_rate: 2.8736316380673088e-05
Step: 17870, train/epoch: 4.252736568450928
Step: 17880, train/loss: 0.0
Step: 17880, train/grad_norm: 1.8935797641006502e-07
Step: 17880, train/learning_rate: 2.872441655199509e-05
Step: 17880, train/epoch: 4.2551164627075195
Step: 17890, train/loss: 0.0
Step: 17890, train/grad_norm: 4.992394275760148e-10
Step: 17890, train/learning_rate: 2.8712518542306498e-05
Step: 17890, train/epoch: 4.257496356964111
Step: 17900, train/loss: 0.0
Step: 17900, train/grad_norm: 3.648110924459047e-09
Step: 17900, train/learning_rate: 2.87006187136285e-05
Step: 17900, train/epoch: 4.259876251220703
Step: 17910, train/loss: 0.0
Step: 17910, train/grad_norm: 5.6022575023462196e-08
Step: 17910, train/learning_rate: 2.8688718884950504e-05
Step: 17910, train/epoch: 4.262256145477295
Step: 17920, train/loss: 0.0
Step: 17920, train/grad_norm: 8.698098352688532e-10
Step: 17920, train/learning_rate: 2.867682087526191e-05
Step: 17920, train/epoch: 4.264636039733887
Step: 17930, train/loss: 0.0
Step: 17930, train/grad_norm: 9.321911242921033e-09
Step: 17930, train/learning_rate: 2.8664921046583913e-05
Step: 17930, train/epoch: 4.2670159339904785
Step: 17940, train/loss: 0.0
Step: 17940, train/grad_norm: 6.982738276484213e-10
Step: 17940, train/learning_rate: 2.865302303689532e-05
Step: 17940, train/epoch: 4.269395351409912
Step: 17950, train/loss: 0.0
Step: 17950, train/grad_norm: 1.429000917596568e-07
Step: 17950, train/learning_rate: 2.8641123208217323e-05
Step: 17950, train/epoch: 4.271775245666504
Step: 17960, train/loss: 0.0
Step: 17960, train/grad_norm: 8.634430948006866e-10
Step: 17960, train/learning_rate: 2.8629223379539326e-05
Step: 17960, train/epoch: 4.274155139923096
Step: 17970, train/loss: 0.0
Step: 17970, train/grad_norm: 1.79662194454977e-07
Step: 17970, train/learning_rate: 2.8617325369850732e-05
Step: 17970, train/epoch: 4.2765350341796875
Step: 17980, train/loss: 0.0
Step: 17980, train/grad_norm: 5.118621082544905e-10
Step: 17980, train/learning_rate: 2.8605425541172735e-05
Step: 17980, train/epoch: 4.278914928436279
Step: 17990, train/loss: 0.0
Step: 17990, train/grad_norm: 6.719698575352595e-08
Step: 17990, train/learning_rate: 2.8593527531484142e-05
Step: 17990, train/epoch: 4.281294822692871
Step: 18000, train/loss: 0.15629999339580536
Step: 18000, train/grad_norm: 0.0005709314136765897
Step: 18000, train/learning_rate: 2.8581627702806145e-05
Step: 18000, train/epoch: 4.283674240112305
Step: 18010, train/loss: 0.0
Step: 18010, train/grad_norm: 1.7118038158514537e-05
Step: 18010, train/learning_rate: 2.8569727874128148e-05
Step: 18010, train/epoch: 4.2860541343688965
Step: 18020, train/loss: 0.0008999999845400453
Step: 18020, train/grad_norm: 2.4950395527412184e-07
Step: 18020, train/learning_rate: 2.8557829864439555e-05
Step: 18020, train/epoch: 4.288434028625488
Step: 18030, train/loss: 0.0
Step: 18030, train/grad_norm: 3.643733634817181e-06
Step: 18030, train/learning_rate: 2.8545930035761558e-05
Step: 18030, train/epoch: 4.29081392288208
Step: 18040, train/loss: 0.0
Step: 18040, train/grad_norm: 5.587206032942049e-05
Step: 18040, train/learning_rate: 2.8534032026072964e-05
Step: 18040, train/epoch: 4.293193817138672
Step: 18050, train/loss: 0.0
Step: 18050, train/grad_norm: 3.0940906725618333e-08
Step: 18050, train/learning_rate: 2.8522132197394967e-05
Step: 18050, train/epoch: 4.295573711395264
Step: 18060, train/loss: 0.0
Step: 18060, train/grad_norm: 2.8116412309486805e-08
Step: 18060, train/learning_rate: 2.851023236871697e-05
Step: 18060, train/epoch: 4.297953128814697
Step: 18070, train/loss: 0.0
Step: 18070, train/grad_norm: 3.2188482990136436e-09
Step: 18070, train/learning_rate: 2.8498334359028377e-05
Step: 18070, train/epoch: 4.300333023071289
Step: 18080, train/loss: 0.0
Step: 18080, train/grad_norm: 4.255985430745568e-08
Step: 18080, train/learning_rate: 2.848643453035038e-05
Step: 18080, train/epoch: 4.302712917327881
Step: 18090, train/loss: 0.0
Step: 18090, train/grad_norm: 0.0008645671186968684
Step: 18090, train/learning_rate: 2.8474536520661786e-05
Step: 18090, train/epoch: 4.305092811584473
Step: 18100, train/loss: 0.0
Step: 18100, train/grad_norm: 1.5242672901649712e-08
Step: 18100, train/learning_rate: 2.846263669198379e-05
Step: 18100, train/epoch: 4.3074727058410645
Step: 18110, train/loss: 0.0
Step: 18110, train/grad_norm: 2.9019731300650164e-05
Step: 18110, train/learning_rate: 2.8450736863305792e-05
Step: 18110, train/epoch: 4.309852600097656
Step: 18120, train/loss: 0.0
Step: 18120, train/grad_norm: 2.888442596216123e-09
Step: 18120, train/learning_rate: 2.84388388536172e-05
Step: 18120, train/epoch: 4.312232494354248
Step: 18130, train/loss: 0.0
Step: 18130, train/grad_norm: 6.272775632254479e-09
Step: 18130, train/learning_rate: 2.8426939024939202e-05
Step: 18130, train/epoch: 4.314611911773682
Step: 18140, train/loss: 0.0
Step: 18140, train/grad_norm: 3.442875151637992e-10
Step: 18140, train/learning_rate: 2.841504101525061e-05
Step: 18140, train/epoch: 4.316991806030273
Step: 18150, train/loss: 0.0
Step: 18150, train/grad_norm: 4.981547263582797e-08
Step: 18150, train/learning_rate: 2.840314118657261e-05
Step: 18150, train/epoch: 4.319371700286865
Step: 18160, train/loss: 0.0
Step: 18160, train/grad_norm: 9.427645295545517e-07
Step: 18160, train/learning_rate: 2.8391241357894614e-05
Step: 18160, train/epoch: 4.321751594543457
Step: 18170, train/loss: 0.0
Step: 18170, train/grad_norm: 4.271108622333486e-08
Step: 18170, train/learning_rate: 2.837934334820602e-05
Step: 18170, train/epoch: 4.324131488800049
Step: 18180, train/loss: 0.0
Step: 18180, train/grad_norm: 3.4688263546300835e-12
Step: 18180, train/learning_rate: 2.8367443519528024e-05
Step: 18180, train/epoch: 4.326511383056641
Step: 18190, train/loss: 0.0
Step: 18190, train/grad_norm: 1.4362462330552717e-09
Step: 18190, train/learning_rate: 2.835554550983943e-05
Step: 18190, train/epoch: 4.328890800476074
Step: 18200, train/loss: 0.0
Step: 18200, train/grad_norm: 3.7870373503778865e-09
Step: 18200, train/learning_rate: 2.8343645681161433e-05
Step: 18200, train/epoch: 4.331270694732666
Step: 18210, train/loss: 0.0
Step: 18210, train/grad_norm: 2.008438215739261e-08
Step: 18210, train/learning_rate: 2.833174767147284e-05
Step: 18210, train/epoch: 4.333650588989258
Step: 18220, train/loss: 0.0
Step: 18220, train/grad_norm: 1.373036297991348e-07
Step: 18220, train/learning_rate: 2.8319847842794843e-05
Step: 18220, train/epoch: 4.33603048324585
Step: 18230, train/loss: 0.0
Step: 18230, train/grad_norm: 1.7757976138454978e-06
Step: 18230, train/learning_rate: 2.8307948014116846e-05
Step: 18230, train/epoch: 4.338410377502441
Step: 18240, train/loss: 0.0
Step: 18240, train/grad_norm: 3.1108195344131673e-06
Step: 18240, train/learning_rate: 2.8296050004428253e-05
Step: 18240, train/epoch: 4.340790271759033
Step: 18250, train/loss: 0.0
Step: 18250, train/grad_norm: 3.740902343452035e-08
Step: 18250, train/learning_rate: 2.8284150175750256e-05
Step: 18250, train/epoch: 4.343169689178467
Step: 18260, train/loss: 0.0
Step: 18260, train/grad_norm: 4.2141869854850356e-11
Step: 18260, train/learning_rate: 2.8272252166061662e-05
Step: 18260, train/epoch: 4.345549583435059
Step: 18270, train/loss: 0.0
Step: 18270, train/grad_norm: 2.8469459678603926e-08
Step: 18270, train/learning_rate: 2.8260352337383665e-05
Step: 18270, train/epoch: 4.34792947769165
Step: 18280, train/loss: 0.0
Step: 18280, train/grad_norm: 6.596862789365332e-08
Step: 18280, train/learning_rate: 2.8248452508705668e-05
Step: 18280, train/epoch: 4.350309371948242
Step: 18290, train/loss: 0.0
Step: 18290, train/grad_norm: 4.439668632727489e-09
Step: 18290, train/learning_rate: 2.8236554499017075e-05
Step: 18290, train/epoch: 4.352689266204834
Step: 18300, train/loss: 0.0
Step: 18300, train/grad_norm: 2.409579646922566e-09
Step: 18300, train/learning_rate: 2.8224654670339078e-05
Step: 18300, train/epoch: 4.355069160461426
Step: 18310, train/loss: 0.0
Step: 18310, train/grad_norm: 6.320739043275125e-09
Step: 18310, train/learning_rate: 2.8212756660650484e-05
Step: 18310, train/epoch: 4.357449054718018
Step: 18320, train/loss: 0.002099999925121665
Step: 18320, train/grad_norm: 1.6823375403873797e-07
Step: 18320, train/learning_rate: 2.8200856831972487e-05
Step: 18320, train/epoch: 4.359828472137451
Step: 18330, train/loss: 0.0
Step: 18330, train/grad_norm: 3.006563531471329e-08
Step: 18330, train/learning_rate: 2.818895700329449e-05
Step: 18330, train/epoch: 4.362208366394043
Step: 18340, train/loss: 0.0
Step: 18340, train/grad_norm: 1.8298885606782278e-07
Step: 18340, train/learning_rate: 2.8177058993605897e-05
Step: 18340, train/epoch: 4.364588260650635
Step: 18350, train/loss: 0.0
Step: 18350, train/grad_norm: 2.8170346055844675e-09
Step: 18350, train/learning_rate: 2.81651591649279e-05
Step: 18350, train/epoch: 4.366968154907227
Step: 18360, train/loss: 0.0
Step: 18360, train/grad_norm: 5.970746119743353e-09
Step: 18360, train/learning_rate: 2.8153261155239306e-05
Step: 18360, train/epoch: 4.369348049163818
Step: 18370, train/loss: 0.10159999877214432
Step: 18370, train/grad_norm: 4.272454134479631e-06
Step: 18370, train/learning_rate: 2.814136132656131e-05
Step: 18370, train/epoch: 4.37172794342041
Step: 18380, train/loss: 0.0
Step: 18380, train/grad_norm: 2.419545580778504e-06
Step: 18380, train/learning_rate: 2.8129461497883312e-05
Step: 18380, train/epoch: 4.374107360839844
Step: 18390, train/loss: 9.999999747378752e-05
Step: 18390, train/grad_norm: 0.17897062003612518
Step: 18390, train/learning_rate: 2.811756348819472e-05
Step: 18390, train/epoch: 4.3764872550964355
Step: 18400, train/loss: 0.0
Step: 18400, train/grad_norm: 4.06424660468474e-05
Step: 18400, train/learning_rate: 2.8105663659516722e-05
Step: 18400, train/epoch: 4.378867149353027
Step: 18410, train/loss: 0.0
Step: 18410, train/grad_norm: 1.430251904821489e-05
Step: 18410, train/learning_rate: 2.809376564982813e-05
Step: 18410, train/epoch: 4.381247043609619
Step: 18420, train/loss: 0.0
Step: 18420, train/grad_norm: 2.36990285173988e-08
Step: 18420, train/learning_rate: 2.808186582115013e-05
Step: 18420, train/epoch: 4.383626937866211
Step: 18430, train/loss: 9.999999747378752e-05
Step: 18430, train/grad_norm: 1.708488284180021e-08
Step: 18430, train/learning_rate: 2.8069965992472135e-05
Step: 18430, train/epoch: 4.386006832122803
Step: 18440, train/loss: 0.0
Step: 18440, train/grad_norm: 2.600599913193946e-08
Step: 18440, train/learning_rate: 2.805806798278354e-05
Step: 18440, train/epoch: 4.388386249542236
Step: 18450, train/loss: 0.0
Step: 18450, train/grad_norm: 1.6676794700742903e-07
Step: 18450, train/learning_rate: 2.8046168154105544e-05
Step: 18450, train/epoch: 4.390766143798828
Step: 18460, train/loss: 0.0
Step: 18460, train/grad_norm: 1.880865312386959e-07
Step: 18460, train/learning_rate: 2.803427014441695e-05
Step: 18460, train/epoch: 4.39314603805542
Step: 18470, train/loss: 0.0
Step: 18470, train/grad_norm: 2.533552560635144e-06
Step: 18470, train/learning_rate: 2.8022370315738954e-05
Step: 18470, train/epoch: 4.395525932312012
Step: 18480, train/loss: 0.0
Step: 18480, train/grad_norm: 4.646263960239594e-07
Step: 18480, train/learning_rate: 2.8010470487060957e-05
Step: 18480, train/epoch: 4.3979058265686035
Step: 18490, train/loss: 0.0
Step: 18490, train/grad_norm: 7.738157670189594e-09
Step: 18490, train/learning_rate: 2.7998572477372363e-05
Step: 18490, train/epoch: 4.400285720825195
Step: 18500, train/loss: 0.019700000062584877
Step: 18500, train/grad_norm: 1.9446426691160923e-08
Step: 18500, train/learning_rate: 2.7986672648694366e-05
Step: 18500, train/epoch: 4.402665615081787
Step: 18510, train/loss: 0.0
Step: 18510, train/grad_norm: 1.0397037897291739e-07
Step: 18510, train/learning_rate: 2.7974774639005773e-05
Step: 18510, train/epoch: 4.405045032501221
Step: 18520, train/loss: 0.0
Step: 18520, train/grad_norm: 8.962305031445794e-08
Step: 18520, train/learning_rate: 2.7962874810327776e-05
Step: 18520, train/epoch: 4.4074249267578125
Step: 18530, train/loss: 0.021800000220537186
Step: 18530, train/grad_norm: 5.334871655549023e-09
Step: 18530, train/learning_rate: 2.795097498164978e-05
Step: 18530, train/epoch: 4.409804821014404
Step: 18540, train/loss: 0.0
Step: 18540, train/grad_norm: 1.0964634582222299e-11
Step: 18540, train/learning_rate: 2.7939076971961185e-05
Step: 18540, train/epoch: 4.412184715270996
Step: 18550, train/loss: 0.0
Step: 18550, train/grad_norm: 1.0847934390767477e-05
Step: 18550, train/learning_rate: 2.792717714328319e-05
Step: 18550, train/epoch: 4.414564609527588
Step: 18560, train/loss: 0.0
Step: 18560, train/grad_norm: 1.287576560571324e-05
Step: 18560, train/learning_rate: 2.7915279133594595e-05
Step: 18560, train/epoch: 4.41694450378418
Step: 18570, train/loss: 9.999999747378752e-05
Step: 18570, train/grad_norm: 0.38409239053726196
Step: 18570, train/learning_rate: 2.7903379304916598e-05
Step: 18570, train/epoch: 4.419323921203613
Step: 18580, train/loss: 0.0
Step: 18580, train/grad_norm: 5.644985776598332e-07
Step: 18580, train/learning_rate: 2.78914794762386e-05
Step: 18580, train/epoch: 4.421703815460205
Step: 18590, train/loss: 0.0
Step: 18590, train/grad_norm: 6.923375935052878e-12
Step: 18590, train/learning_rate: 2.7879581466550007e-05
Step: 18590, train/epoch: 4.424083709716797
Step: 18600, train/loss: 0.0
Step: 18600, train/grad_norm: 4.361494221338802e-10
Step: 18600, train/learning_rate: 2.786768163787201e-05
Step: 18600, train/epoch: 4.426463603973389
Step: 18610, train/loss: 0.0
Step: 18610, train/grad_norm: 3.704443952301517e-05
Step: 18610, train/learning_rate: 2.7855783628183417e-05
Step: 18610, train/epoch: 4.4288434982299805
Step: 18620, train/loss: 0.0
Step: 18620, train/grad_norm: 2.542444974551472e-07
Step: 18620, train/learning_rate: 2.784388379950542e-05
Step: 18620, train/epoch: 4.431223392486572
Step: 18630, train/loss: 0.0
Step: 18630, train/grad_norm: 1.864327714429237e-05
Step: 18630, train/learning_rate: 2.7831983970827423e-05
Step: 18630, train/epoch: 4.433602809906006
Step: 18640, train/loss: 0.0
Step: 18640, train/grad_norm: 2.134932628905517e-06
Step: 18640, train/learning_rate: 2.782008596113883e-05
Step: 18640, train/epoch: 4.435982704162598
Step: 18650, train/loss: 0.0
Step: 18650, train/grad_norm: 1.1705467528599911e-07
Step: 18650, train/learning_rate: 2.7808186132460833e-05
Step: 18650, train/epoch: 4.4383625984191895
Step: 18660, train/loss: 0.0
Step: 18660, train/grad_norm: 1.3571384904764727e-08
Step: 18660, train/learning_rate: 2.779628812277224e-05
Step: 18660, train/epoch: 4.440742492675781
Step: 18670, train/loss: 0.0
Step: 18670, train/grad_norm: 8.096663339784982e-10
Step: 18670, train/learning_rate: 2.7784388294094242e-05
Step: 18670, train/epoch: 4.443122386932373
Step: 18680, train/loss: 0.0
Step: 18680, train/grad_norm: 1.1474984296100388e-09
Step: 18680, train/learning_rate: 2.7772488465416245e-05
Step: 18680, train/epoch: 4.445502281188965
Step: 18690, train/loss: 0.0
Step: 18690, train/grad_norm: 0.0003957064764108509
Step: 18690, train/learning_rate: 2.7760590455727652e-05
Step: 18690, train/epoch: 4.447882175445557
Step: 18700, train/loss: 0.0
Step: 18700, train/grad_norm: 6.324532009216455e-10
Step: 18700, train/learning_rate: 2.7748690627049655e-05
Step: 18700, train/epoch: 4.45026159286499
Step: 18710, train/loss: 0.0
Step: 18710, train/grad_norm: 1.0717879916910533e-07
Step: 18710, train/learning_rate: 2.773679261736106e-05
Step: 18710, train/epoch: 4.452641487121582
Step: 18720, train/loss: 0.0
Step: 18720, train/grad_norm: 1.2383323255704681e-09
Step: 18720, train/learning_rate: 2.7724892788683064e-05
Step: 18720, train/epoch: 4.455021381378174
Step: 18730, train/loss: 0.0
Step: 18730, train/grad_norm: 5.6648834928507696e-11
Step: 18730, train/learning_rate: 2.7712992960005067e-05
Step: 18730, train/epoch: 4.457401275634766
Step: 18740, train/loss: 0.0
Step: 18740, train/grad_norm: 1.339614086326435e-09
Step: 18740, train/learning_rate: 2.7701094950316474e-05
Step: 18740, train/epoch: 4.459781169891357
Step: 18750, train/loss: 0.0005000000237487257
Step: 18750, train/grad_norm: 1.5253363017109223e-07
Step: 18750, train/learning_rate: 2.7689195121638477e-05
Step: 18750, train/epoch: 4.462161064147949
Step: 18760, train/loss: 0.0
Step: 18760, train/grad_norm: 5.965897287296684e-08
Step: 18760, train/learning_rate: 2.7677297111949883e-05
Step: 18760, train/epoch: 4.464540481567383
Step: 18770, train/loss: 0.0
Step: 18770, train/grad_norm: 2.8448296236077697e-12
Step: 18770, train/learning_rate: 2.7665397283271886e-05
Step: 18770, train/epoch: 4.466920375823975
Step: 18780, train/loss: 0.0
Step: 18780, train/grad_norm: 9.055798955159844e-08
Step: 18780, train/learning_rate: 2.765349745459389e-05
Step: 18780, train/epoch: 4.469300270080566
Step: 18790, train/loss: 0.0
Step: 18790, train/grad_norm: 2.3827128643461037e-06
Step: 18790, train/learning_rate: 2.7641599444905296e-05
Step: 18790, train/epoch: 4.471680164337158
Step: 18800, train/loss: 0.0
Step: 18800, train/grad_norm: 0.02832186594605446
Step: 18800, train/learning_rate: 2.76296996162273e-05
Step: 18800, train/epoch: 4.47406005859375
Step: 18810, train/loss: 0.0
Step: 18810, train/grad_norm: 1.1499964536199059e-08
Step: 18810, train/learning_rate: 2.7617801606538706e-05
Step: 18810, train/epoch: 4.476439952850342
Step: 18820, train/loss: 0.0
Step: 18820, train/grad_norm: 2.276906614639973e-11
Step: 18820, train/learning_rate: 2.760590177786071e-05
Step: 18820, train/epoch: 4.478819847106934
Step: 18830, train/loss: 0.0
Step: 18830, train/grad_norm: 5.991760190227069e-06
Step: 18830, train/learning_rate: 2.759400194918271e-05
Step: 18830, train/epoch: 4.481199264526367
Step: 18840, train/loss: 0.0
Step: 18840, train/grad_norm: 1.1718415571237983e-08
Step: 18840, train/learning_rate: 2.7582103939494118e-05
Step: 18840, train/epoch: 4.483579158782959
Step: 18850, train/loss: 9.999999747378752e-05
Step: 18850, train/grad_norm: 2.4129576683044434
Step: 18850, train/learning_rate: 2.757020411081612e-05
Step: 18850, train/epoch: 4.485959053039551
Step: 18860, train/loss: 9.999999747378752e-05
Step: 18860, train/grad_norm: 2.1793058284087612e-14
Step: 18860, train/learning_rate: 2.7558306101127528e-05
Step: 18860, train/epoch: 4.488338947296143
Step: 18870, train/loss: 0.0
Step: 18870, train/grad_norm: 2.999067660880428e-10
Step: 18870, train/learning_rate: 2.754640627244953e-05
Step: 18870, train/epoch: 4.490718841552734
Step: 18880, train/loss: 0.10090000182390213
Step: 18880, train/grad_norm: 6.522360342792188e-12
Step: 18880, train/learning_rate: 2.7534508262760937e-05
Step: 18880, train/epoch: 4.493098735809326
Step: 18890, train/loss: 0.0
Step: 18890, train/grad_norm: 0.10682331770658493
Step: 18890, train/learning_rate: 2.752260843408294e-05
Step: 18890, train/epoch: 4.49547815322876
Step: 18900, train/loss: 0.0
Step: 18900, train/grad_norm: 6.050096999388188e-05
Step: 18900, train/learning_rate: 2.7510708605404943e-05
Step: 18900, train/epoch: 4.497858047485352
Step: 18910, train/loss: 0.0
Step: 18910, train/grad_norm: 1.0437016806363886e-09
Step: 18910, train/learning_rate: 2.749881059571635e-05
Step: 18910, train/epoch: 4.500237941741943
Step: 18920, train/loss: 0.0
Step: 18920, train/grad_norm: 2.0182182197459042e-05
Step: 18920, train/learning_rate: 2.7486910767038353e-05
Step: 18920, train/epoch: 4.502617835998535
Step: 18930, train/loss: 0.0
Step: 18930, train/grad_norm: 2.3149308248093803e-08
Step: 18930, train/learning_rate: 2.747501275734976e-05
Step: 18930, train/epoch: 4.504997730255127
Step: 18940, train/loss: 0.0
Step: 18940, train/grad_norm: 7.156210557468512e-08
Step: 18940, train/learning_rate: 2.7463112928671762e-05
Step: 18940, train/epoch: 4.507377624511719
Step: 18950, train/loss: 0.0
Step: 18950, train/grad_norm: 2.565470058613073e-11
Step: 18950, train/learning_rate: 2.7451213099993765e-05
Step: 18950, train/epoch: 4.509757041931152
Step: 18960, train/loss: 0.0
Step: 18960, train/grad_norm: 1.3428564216155792e-07
Step: 18960, train/learning_rate: 2.7439315090305172e-05
Step: 18960, train/epoch: 4.512136936187744
Step: 18970, train/loss: 0.0024999999441206455
Step: 18970, train/grad_norm: 1.1652446119114757e-07
Step: 18970, train/learning_rate: 2.7427415261627175e-05
Step: 18970, train/epoch: 4.514516830444336
Step: 18980, train/loss: 0.0
Step: 18980, train/grad_norm: 1.2041047625643841e-07
Step: 18980, train/learning_rate: 2.741551725193858e-05
Step: 18980, train/epoch: 4.516896724700928
Step: 18990, train/loss: 0.0
Step: 18990, train/grad_norm: 1.4995229946634936e-07
Step: 18990, train/learning_rate: 2.7403617423260584e-05
Step: 18990, train/epoch: 4.5192766189575195
Step: 19000, train/loss: 0.0
Step: 19000, train/grad_norm: 3.6419007809485038e-09
Step: 19000, train/learning_rate: 2.7391717594582587e-05
Step: 19000, train/epoch: 4.521656513214111
Step: 19010, train/loss: 9.999999747378752e-05
Step: 19010, train/grad_norm: 7.84900819894574e-08
Step: 19010, train/learning_rate: 2.7379819584893994e-05
Step: 19010, train/epoch: 4.524036407470703
Step: 19020, train/loss: 0.0
Step: 19020, train/grad_norm: 9.839035142178432e-10
Step: 19020, train/learning_rate: 2.7367919756215997e-05
Step: 19020, train/epoch: 4.526415824890137
Step: 19030, train/loss: 0.0
Step: 19030, train/grad_norm: 6.719763079310326e-10
Step: 19030, train/learning_rate: 2.7356021746527404e-05
Step: 19030, train/epoch: 4.5287957191467285
Step: 19040, train/loss: 0.0
Step: 19040, train/grad_norm: 1.1388527898503753e-09
Step: 19040, train/learning_rate: 2.7344121917849407e-05
Step: 19040, train/epoch: 4.53117561340332
Step: 19050, train/loss: 0.0
Step: 19050, train/grad_norm: 2.7948410252776057e-09
Step: 19050, train/learning_rate: 2.733222208917141e-05
Step: 19050, train/epoch: 4.533555507659912
Step: 19060, train/loss: 0.0
Step: 19060, train/grad_norm: 4.1771239245314007e-10
Step: 19060, train/learning_rate: 2.7320324079482816e-05
Step: 19060, train/epoch: 4.535935401916504
Step: 19070, train/loss: 0.0
Step: 19070, train/grad_norm: 7.373749326688994e-07
Step: 19070, train/learning_rate: 2.730842425080482e-05
Step: 19070, train/epoch: 4.538315296173096
Step: 19080, train/loss: 0.0
Step: 19080, train/grad_norm: 8.728257006040963e-10
Step: 19080, train/learning_rate: 2.7296526241116226e-05
Step: 19080, train/epoch: 4.540694713592529
Step: 19090, train/loss: 0.0015999999595806003
Step: 19090, train/grad_norm: 4.424345438625021e-12
Step: 19090, train/learning_rate: 2.728462641243823e-05
Step: 19090, train/epoch: 4.543074607849121
Step: 19100, train/loss: 0.0
Step: 19100, train/grad_norm: 2.4660393727060637e-09
Step: 19100, train/learning_rate: 2.7272726583760232e-05
Step: 19100, train/epoch: 4.545454502105713
Step: 19110, train/loss: 0.049400001764297485
Step: 19110, train/grad_norm: 7.873741196817718e-07
Step: 19110, train/learning_rate: 2.7260828574071638e-05
Step: 19110, train/epoch: 4.547834396362305
Step: 19120, train/loss: 0.0
Step: 19120, train/grad_norm: 1.2380107355935976e-11
Step: 19120, train/learning_rate: 2.724892874539364e-05
Step: 19120, train/epoch: 4.5502142906188965
Step: 19130, train/loss: 0.0
Step: 19130, train/grad_norm: 6.000549035434233e-08
Step: 19130, train/learning_rate: 2.7237030735705048e-05
Step: 19130, train/epoch: 4.552594184875488
Step: 19140, train/loss: 0.0
Step: 19140, train/grad_norm: 8.902990771275654e-07
Step: 19140, train/learning_rate: 2.722513090702705e-05
Step: 19140, train/epoch: 4.554973602294922
Step: 19150, train/loss: 0.0
Step: 19150, train/grad_norm: 6.22775095382444e-11
Step: 19150, train/learning_rate: 2.7213231078349054e-05
Step: 19150, train/epoch: 4.557353496551514
Step: 19160, train/loss: 0.0
Step: 19160, train/grad_norm: 0.0005144575843587518
Step: 19160, train/learning_rate: 2.720133306866046e-05
Step: 19160, train/epoch: 4.5597333908081055
Step: 19170, train/loss: 0.00039999998989515007
Step: 19170, train/grad_norm: 2.920658603855486e-09
Step: 19170, train/learning_rate: 2.7189433239982463e-05
Step: 19170, train/epoch: 4.562113285064697
Step: 19180, train/loss: 0.0
Step: 19180, train/grad_norm: 1.1379141379164182e-10
Step: 19180, train/learning_rate: 2.717753523029387e-05
Step: 19180, train/epoch: 4.564493179321289
Step: 19190, train/loss: 0.0
Step: 19190, train/grad_norm: 2.3191837339453514e-09
Step: 19190, train/learning_rate: 2.7165635401615873e-05
Step: 19190, train/epoch: 4.566873073577881
Step: 19200, train/loss: 0.1437000036239624
Step: 19200, train/grad_norm: 2.7823483517153136e-09
Step: 19200, train/learning_rate: 2.7153735572937876e-05
Step: 19200, train/epoch: 4.569252967834473
Step: 19210, train/loss: 0.0
Step: 19210, train/grad_norm: 3.413755944592367e-10
Step: 19210, train/learning_rate: 2.7141837563249283e-05
Step: 19210, train/epoch: 4.571632385253906
Step: 19220, train/loss: 0.0
Step: 19220, train/grad_norm: 1.7241779115551026e-09
Step: 19220, train/learning_rate: 2.7129937734571286e-05
Step: 19220, train/epoch: 4.574012279510498
Step: 19230, train/loss: 0.0
Step: 19230, train/grad_norm: 7.911622468270707e-09
Step: 19230, train/learning_rate: 2.7118039724882692e-05
Step: 19230, train/epoch: 4.57639217376709
Step: 19240, train/loss: 0.0
Step: 19240, train/grad_norm: 1.4144778681668413e-09
Step: 19240, train/learning_rate: 2.7106139896204695e-05
Step: 19240, train/epoch: 4.578772068023682
Step: 19250, train/loss: 0.0
Step: 19250, train/grad_norm: 6.524968284793431e-06
Step: 19250, train/learning_rate: 2.7094240067526698e-05
Step: 19250, train/epoch: 4.581151962280273
Step: 19260, train/loss: 0.0
Step: 19260, train/grad_norm: 5.611453524068111e-10
Step: 19260, train/learning_rate: 2.7082342057838105e-05
Step: 19260, train/epoch: 4.583531856536865
Step: 19270, train/loss: 0.0
Step: 19270, train/grad_norm: 8.602713563732323e-08
Step: 19270, train/learning_rate: 2.7070442229160108e-05
Step: 19270, train/epoch: 4.585911273956299
Step: 19280, train/loss: 0.0
Step: 19280, train/grad_norm: 1.0705324484661105e-06
Step: 19280, train/learning_rate: 2.7058544219471514e-05
Step: 19280, train/epoch: 4.588291168212891
Step: 19290, train/loss: 0.0
Step: 19290, train/grad_norm: 7.12352843379449e-09
Step: 19290, train/learning_rate: 2.7046644390793517e-05
Step: 19290, train/epoch: 4.590671062469482
Step: 19300, train/loss: 0.0
Step: 19300, train/grad_norm: 2.310457164131363e-12
Step: 19300, train/learning_rate: 2.703474456211552e-05
Step: 19300, train/epoch: 4.593050956726074
Step: 19310, train/loss: 0.0
Step: 19310, train/grad_norm: 3.122815783740407e-08
Step: 19310, train/learning_rate: 2.7022846552426927e-05
Step: 19310, train/epoch: 4.595430850982666
Step: 19320, train/loss: 0.0
Step: 19320, train/grad_norm: 1.92479032712356e-09
Step: 19320, train/learning_rate: 2.701094672374893e-05
Step: 19320, train/epoch: 4.597810745239258
Step: 19330, train/loss: 0.0
Step: 19330, train/grad_norm: 2.3773973900631518e-09
Step: 19330, train/learning_rate: 2.6999048714060336e-05
Step: 19330, train/epoch: 4.600190162658691
Step: 19340, train/loss: 0.0
Step: 19340, train/grad_norm: 2.5750883647646106e-09
Step: 19340, train/learning_rate: 2.698714888538234e-05
Step: 19340, train/epoch: 4.602570056915283
Step: 19350, train/loss: 0.0
Step: 19350, train/grad_norm: 2.5949097093302953e-08
Step: 19350, train/learning_rate: 2.6975249056704342e-05
Step: 19350, train/epoch: 4.604949951171875
Step: 19360, train/loss: 0.0
Step: 19360, train/grad_norm: 2.3240423274728528e-07
Step: 19360, train/learning_rate: 2.696335104701575e-05
Step: 19360, train/epoch: 4.607329845428467
Step: 19370, train/loss: 0.0
Step: 19370, train/grad_norm: 7.156135128916219e-10
Step: 19370, train/learning_rate: 2.6951451218337752e-05
Step: 19370, train/epoch: 4.609709739685059
Step: 19380, train/loss: 0.0
Step: 19380, train/grad_norm: 2.91255014417402e-06
Step: 19380, train/learning_rate: 2.693955320864916e-05
Step: 19380, train/epoch: 4.61208963394165
Step: 19390, train/loss: 0.0
Step: 19390, train/grad_norm: 3.2670879335228165e-08
Step: 19390, train/learning_rate: 2.692765337997116e-05
Step: 19390, train/epoch: 4.614469528198242
Step: 19400, train/loss: 0.0
Step: 19400, train/grad_norm: 1.929854676063769e-08
Step: 19400, train/learning_rate: 2.6915753551293164e-05
Step: 19400, train/epoch: 4.616848945617676
Step: 19410, train/loss: 0.0
Step: 19410, train/grad_norm: 3.6286867954871127e-10
Step: 19410, train/learning_rate: 2.690385554160457e-05
Step: 19410, train/epoch: 4.619228839874268
Step: 19420, train/loss: 0.0
Step: 19420, train/grad_norm: 5.89011912160986e-13
Step: 19420, train/learning_rate: 2.6891955712926574e-05
Step: 19420, train/epoch: 4.621608734130859
Step: 19430, train/loss: 0.0
Step: 19430, train/grad_norm: 5.3024113100264e-08
Step: 19430, train/learning_rate: 2.688005770323798e-05
Step: 19430, train/epoch: 4.623988628387451
Step: 19440, train/loss: 0.007300000172108412
Step: 19440, train/grad_norm: 7.5375387496023905e-06
Step: 19440, train/learning_rate: 2.6868157874559984e-05
Step: 19440, train/epoch: 4.626368522644043
Step: 19450, train/loss: 0.0
Step: 19450, train/grad_norm: 4.088068425445179e-12
Step: 19450, train/learning_rate: 2.6856258045881987e-05
Step: 19450, train/epoch: 4.628748416900635
Step: 19460, train/loss: 0.0
Step: 19460, train/grad_norm: 2.704249446594531e-08
Step: 19460, train/learning_rate: 2.6844360036193393e-05
Step: 19460, train/epoch: 4.631127834320068
Step: 19470, train/loss: 0.0
Step: 19470, train/grad_norm: 3.8306205851768027e-07
Step: 19470, train/learning_rate: 2.6832460207515396e-05
Step: 19470, train/epoch: 4.63350772857666
Step: 19480, train/loss: 0.0
Step: 19480, train/grad_norm: 1.4689677918511279e-08
Step: 19480, train/learning_rate: 2.6820562197826803e-05
Step: 19480, train/epoch: 4.635887622833252
Step: 19490, train/loss: 0.13979999721050262
Step: 19490, train/grad_norm: 5.534023443942715e-07
Step: 19490, train/learning_rate: 2.6808662369148806e-05
Step: 19490, train/epoch: 4.638267517089844
Step: 19500, train/loss: 0.0
Step: 19500, train/grad_norm: 2.4499602346850224e-09
Step: 19500, train/learning_rate: 2.679676254047081e-05
Step: 19500, train/epoch: 4.6406474113464355
Step: 19510, train/loss: 0.0
Step: 19510, train/grad_norm: 0.0009041102021001279
Step: 19510, train/learning_rate: 2.6784864530782215e-05
Step: 19510, train/epoch: 4.643027305603027
Step: 19520, train/loss: 0.00039999998989515007
Step: 19520, train/grad_norm: 1.9536157136457177e-09
Step: 19520, train/learning_rate: 2.6772964702104218e-05
Step: 19520, train/epoch: 4.645406723022461
Step: 19530, train/loss: 0.0
Step: 19530, train/grad_norm: 5.062535279876101e-09
Step: 19530, train/learning_rate: 2.6761066692415625e-05
Step: 19530, train/epoch: 4.647786617279053
Step: 19540, train/loss: 0.0
Step: 19540, train/grad_norm: 1.8088337494504003e-09
Step: 19540, train/learning_rate: 2.6749166863737628e-05
Step: 19540, train/epoch: 4.6501665115356445
Step: 19550, train/loss: 0.0
Step: 19550, train/grad_norm: 1.4517953506043568e-09
Step: 19550, train/learning_rate: 2.6737268854049034e-05
Step: 19550, train/epoch: 4.652546405792236
Step: 19560, train/loss: 0.0
Step: 19560, train/grad_norm: 1.410545635849303e-08
Step: 19560, train/learning_rate: 2.6725369025371037e-05
Step: 19560, train/epoch: 4.654926300048828
Step: 19570, train/loss: 0.0
Step: 19570, train/grad_norm: 3.2080965439718057e-08
Step: 19570, train/learning_rate: 2.671346919669304e-05
Step: 19570, train/epoch: 4.65730619430542
Step: 19580, train/loss: 0.0
Step: 19580, train/grad_norm: 6.220262971368129e-09
Step: 19580, train/learning_rate: 2.6701571187004447e-05
Step: 19580, train/epoch: 4.659686088562012
Step: 19590, train/loss: 0.0
Step: 19590, train/grad_norm: 2.8595518415386323e-06
Step: 19590, train/learning_rate: 2.668967135832645e-05
Step: 19590, train/epoch: 4.662065505981445
Step: 19600, train/loss: 0.0
Step: 19600, train/grad_norm: 4.856887514392838e-08
Step: 19600, train/learning_rate: 2.6677773348637857e-05
Step: 19600, train/epoch: 4.664445400238037
Step: 19610, train/loss: 0.0
Step: 19610, train/grad_norm: 1.651271617220118e-07
Step: 19610, train/learning_rate: 2.666587351995986e-05
Step: 19610, train/epoch: 4.666825294494629
Step: 19620, train/loss: 0.0
Step: 19620, train/grad_norm: 5.372359446198516e-09
Step: 19620, train/learning_rate: 2.6653973691281863e-05
Step: 19620, train/epoch: 4.669205188751221
Step: 19630, train/loss: 0.0
Step: 19630, train/grad_norm: 1.880493094574831e-08
Step: 19630, train/learning_rate: 2.664207568159327e-05
Step: 19630, train/epoch: 4.6715850830078125
Step: 19640, train/loss: 0.0
Step: 19640, train/grad_norm: 8.777871207676924e-10
Step: 19640, train/learning_rate: 2.6630175852915272e-05
Step: 19640, train/epoch: 4.673964977264404
Step: 19650, train/loss: 0.0
Step: 19650, train/grad_norm: 1.0251528514970687e-09
Step: 19650, train/learning_rate: 2.661827784322668e-05
Step: 19650, train/epoch: 4.676344394683838
Step: 19660, train/loss: 9.999999747378752e-05
Step: 19660, train/grad_norm: 1.424167095365192e-08
Step: 19660, train/learning_rate: 2.660637801454868e-05
Step: 19660, train/epoch: 4.67872428894043
Step: 19670, train/loss: 0.0
Step: 19670, train/grad_norm: 1.2018720951800788e-07
Step: 19670, train/learning_rate: 2.6594478185870685e-05
Step: 19670, train/epoch: 4.6811041831970215
Step: 19680, train/loss: 0.00019999999494757503
Step: 19680, train/grad_norm: 1.4713088386519857e-11
Step: 19680, train/learning_rate: 2.658258017618209e-05
Step: 19680, train/epoch: 4.683484077453613
Step: 19690, train/loss: 0.0
Step: 19690, train/grad_norm: 3.2038916658594374e-11
Step: 19690, train/learning_rate: 2.6570680347504094e-05
Step: 19690, train/epoch: 4.685863971710205
Step: 19700, train/loss: 0.0
Step: 19700, train/grad_norm: 7.933956269745579e-11
Step: 19700, train/learning_rate: 2.65587823378155e-05
Step: 19700, train/epoch: 4.688243865966797
Step: 19710, train/loss: 0.0
Step: 19710, train/grad_norm: 2.486688686076377e-07
Step: 19710, train/learning_rate: 2.6546882509137504e-05
Step: 19710, train/epoch: 4.6906232833862305
Step: 19720, train/loss: 0.0
Step: 19720, train/grad_norm: 8.60038706917976e-09
Step: 19720, train/learning_rate: 2.6534982680459507e-05
Step: 19720, train/epoch: 4.693003177642822
Step: 19730, train/loss: 0.0
Step: 19730, train/grad_norm: 5.293142368856252e-09
Step: 19730, train/learning_rate: 2.6523084670770913e-05
Step: 19730, train/epoch: 4.695383071899414
Step: 19740, train/loss: 0.0
Step: 19740, train/grad_norm: 1.4729883979214264e-09
Step: 19740, train/learning_rate: 2.6511184842092916e-05
Step: 19740, train/epoch: 4.697762966156006
Step: 19750, train/loss: 0.0
Step: 19750, train/grad_norm: 2.1818756579250476e-07
Step: 19750, train/learning_rate: 2.6499286832404323e-05
Step: 19750, train/epoch: 4.700142860412598
Step: 19760, train/loss: 0.0
Step: 19760, train/grad_norm: 5.508836409262585e-08
Step: 19760, train/learning_rate: 2.6487387003726326e-05
Step: 19760, train/epoch: 4.7025227546691895
Step: 19770, train/loss: 0.0
Step: 19770, train/grad_norm: 9.226141378659847e-11
Step: 19770, train/learning_rate: 2.647548717504833e-05
Step: 19770, train/epoch: 4.704902648925781
Step: 19780, train/loss: 0.0
Step: 19780, train/grad_norm: 3.3259399678797763e-09
Step: 19780, train/learning_rate: 2.6463589165359735e-05
Step: 19780, train/epoch: 4.707282066345215
Step: 19790, train/loss: 0.0
Step: 19790, train/grad_norm: 1.1386688814063461e-10
Step: 19790, train/learning_rate: 2.645168933668174e-05
Step: 19790, train/epoch: 4.709661960601807
Step: 19800, train/loss: 0.0
Step: 19800, train/grad_norm: 6.926378318894422e-06
Step: 19800, train/learning_rate: 2.6439791326993145e-05
Step: 19800, train/epoch: 4.712041854858398
Step: 19810, train/loss: 0.0
Step: 19810, train/grad_norm: 1.0578809828132307e-08
Step: 19810, train/learning_rate: 2.6427891498315148e-05
Step: 19810, train/epoch: 4.71442174911499
Step: 19820, train/loss: 0.0
Step: 19820, train/grad_norm: 3.263037617884379e-10
Step: 19820, train/learning_rate: 2.641599166963715e-05
Step: 19820, train/epoch: 4.716801643371582
Step: 19830, train/loss: 0.0
Step: 19830, train/grad_norm: 3.1883417889351506e-14
Step: 19830, train/learning_rate: 2.6404093659948558e-05
Step: 19830, train/epoch: 4.719181537628174
Step: 19840, train/loss: 0.0
Step: 19840, train/grad_norm: 3.9833819576173823e-10
Step: 19840, train/learning_rate: 2.639219383127056e-05
Step: 19840, train/epoch: 4.721560955047607
Step: 19850, train/loss: 0.0
Step: 19850, train/grad_norm: 1.6388717938298214e-08
Step: 19850, train/learning_rate: 2.6380295821581967e-05
Step: 19850, train/epoch: 4.723940849304199
Step: 19860, train/loss: 0.0
Step: 19860, train/grad_norm: 2.720939664868638e-05
Step: 19860, train/learning_rate: 2.636839599290397e-05
Step: 19860, train/epoch: 4.726320743560791
Step: 19870, train/loss: 0.0
Step: 19870, train/grad_norm: 2.840642387980097e-09
Step: 19870, train/learning_rate: 2.6356496164225973e-05
Step: 19870, train/epoch: 4.728700637817383
Step: 19880, train/loss: 0.0
Step: 19880, train/grad_norm: 5.276293291167633e-11
Step: 19880, train/learning_rate: 2.634459815453738e-05
Step: 19880, train/epoch: 4.731080532073975
Step: 19890, train/loss: 0.0
Step: 19890, train/grad_norm: 1.2819144856734965e-08
Step: 19890, train/learning_rate: 2.6332698325859383e-05
Step: 19890, train/epoch: 4.733460426330566
Step: 19900, train/loss: 0.0
Step: 19900, train/grad_norm: 3.211537702441092e-10
Step: 19900, train/learning_rate: 2.632080031617079e-05
Step: 19900, train/epoch: 4.73583984375
Step: 19910, train/loss: 0.0
Step: 19910, train/grad_norm: 1.4240449097702168e-10
Step: 19910, train/learning_rate: 2.6308900487492792e-05
Step: 19910, train/epoch: 4.738219738006592
Step: 19920, train/loss: 0.0
Step: 19920, train/grad_norm: 1.0095183355929294e-08
Step: 19920, train/learning_rate: 2.6297000658814795e-05
Step: 19920, train/epoch: 4.740599632263184
Step: 19930, train/loss: 0.0
Step: 19930, train/grad_norm: 7.1326744510713525e-09
Step: 19930, train/learning_rate: 2.6285102649126202e-05
Step: 19930, train/epoch: 4.742979526519775
Step: 19940, train/loss: 0.0
Step: 19940, train/grad_norm: 1.5151222498843708e-08
Step: 19940, train/learning_rate: 2.6273202820448205e-05
Step: 19940, train/epoch: 4.745359420776367
Step: 19950, train/loss: 0.0
Step: 19950, train/grad_norm: 9.85445591794587e-09
Step: 19950, train/learning_rate: 2.626130481075961e-05
Step: 19950, train/epoch: 4.747739315032959
Step: 19960, train/loss: 0.0
Step: 19960, train/grad_norm: 4.330572012101186e-11
Step: 19960, train/learning_rate: 2.6249404982081614e-05
Step: 19960, train/epoch: 4.750119209289551
Step: 19970, train/loss: 0.0
Step: 19970, train/grad_norm: 2.993401526651951e-09
Step: 19970, train/learning_rate: 2.6237505153403617e-05
Step: 19970, train/epoch: 4.752498626708984
Step: 19980, train/loss: 0.0
Step: 19980, train/grad_norm: 3.4923000602482546e-10
Step: 19980, train/learning_rate: 2.6225607143715024e-05
Step: 19980, train/epoch: 4.754878520965576
Step: 19990, train/loss: 0.0
Step: 19990, train/grad_norm: 3.5824876398748984e-10
Step: 19990, train/learning_rate: 2.6213707315037027e-05
Step: 19990, train/epoch: 4.757258415222168
Step: 20000, train/loss: 0.0
Step: 20000, train/grad_norm: 2.6263291985628712e-08
Step: 20000, train/learning_rate: 2.6201809305348434e-05
Step: 20000, train/epoch: 4.75963830947876
Step: 20010, train/loss: 0.0
Step: 20010, train/grad_norm: 2.2302306206256617e-06
Step: 20010, train/learning_rate: 2.6189909476670437e-05
Step: 20010, train/epoch: 4.762018203735352
Step: 20020, train/loss: 0.0
Step: 20020, train/grad_norm: 1.9962909103554694e-09
Step: 20020, train/learning_rate: 2.617800964799244e-05
Step: 20020, train/epoch: 4.764398097991943
Step: 20030, train/loss: 0.0
Step: 20030, train/grad_norm: 1.520226132478708e-11
Step: 20030, train/learning_rate: 2.6166111638303846e-05
Step: 20030, train/epoch: 4.766777515411377
Step: 20040, train/loss: 0.0
Step: 20040, train/grad_norm: 3.1775186926097376e-09
Step: 20040, train/learning_rate: 2.615421180962585e-05
Step: 20040, train/epoch: 4.769157409667969
Step: 20050, train/loss: 0.0
Step: 20050, train/grad_norm: 1.0748749446065631e-07
Step: 20050, train/learning_rate: 2.6142313799937256e-05
Step: 20050, train/epoch: 4.7715373039245605
Step: 20060, train/loss: 0.0
Step: 20060, train/grad_norm: 1.2915683800240885e-10
Step: 20060, train/learning_rate: 2.613041397125926e-05
Step: 20060, train/epoch: 4.773917198181152
Step: 20070, train/loss: 0.0
Step: 20070, train/grad_norm: 2.9058722095243184e-09
Step: 20070, train/learning_rate: 2.6118514142581262e-05
Step: 20070, train/epoch: 4.776297092437744
Step: 20080, train/loss: 0.0
Step: 20080, train/grad_norm: 1.4849577123499103e-09
Step: 20080, train/learning_rate: 2.6106616132892668e-05
Step: 20080, train/epoch: 4.778676986694336
Step: 20090, train/loss: 0.0
Step: 20090, train/grad_norm: 4.2994306032051455e-11
Step: 20090, train/learning_rate: 2.609471630421467e-05
Step: 20090, train/epoch: 4.7810564041137695
Step: 20100, train/loss: 0.0
Step: 20100, train/grad_norm: 2.6121416141222653e-10
Step: 20100, train/learning_rate: 2.6082818294526078e-05
Step: 20100, train/epoch: 4.783436298370361
Step: 20110, train/loss: 0.0
Step: 20110, train/grad_norm: 8.564872699956538e-10
Step: 20110, train/learning_rate: 2.607091846584808e-05
Step: 20110, train/epoch: 4.785816192626953
Step: 20120, train/loss: 0.0
Step: 20120, train/grad_norm: 2.5199261699526687e-07
Step: 20120, train/learning_rate: 2.6059018637170084e-05
Step: 20120, train/epoch: 4.788196086883545
Step: 20130, train/loss: 0.0
Step: 20130, train/grad_norm: 2.3108430446949768e-10
Step: 20130, train/learning_rate: 2.604712062748149e-05
Step: 20130, train/epoch: 4.790575981140137
Step: 20140, train/loss: 0.0
Step: 20140, train/grad_norm: 2.325717287021689e-05
Step: 20140, train/learning_rate: 2.6035220798803493e-05
Step: 20140, train/epoch: 4.7929558753967285
Step: 20150, train/loss: 0.0
Step: 20150, train/grad_norm: 1.3454225233910933e-10
Step: 20150, train/learning_rate: 2.60233227891149e-05
Step: 20150, train/epoch: 4.79533576965332
Step: 20160, train/loss: 0.0
Step: 20160, train/grad_norm: 2.1488540724590166e-08
Step: 20160, train/learning_rate: 2.6011422960436903e-05
Step: 20160, train/epoch: 4.797715187072754
Step: 20170, train/loss: 0.0
Step: 20170, train/grad_norm: 7.977297933337013e-09
Step: 20170, train/learning_rate: 2.5999523131758906e-05
Step: 20170, train/epoch: 4.800095081329346
Step: 20180, train/loss: 0.0
Step: 20180, train/grad_norm: 5.763504784539464e-09
Step: 20180, train/learning_rate: 2.5987625122070312e-05
Step: 20180, train/epoch: 4.8024749755859375
Step: 20190, train/loss: 0.0
Step: 20190, train/grad_norm: 1.3569199097673845e-07
Step: 20190, train/learning_rate: 2.5975725293392316e-05
Step: 20190, train/epoch: 4.804854869842529
Step: 20200, train/loss: 0.0
Step: 20200, train/grad_norm: 6.011059011656883e-11
Step: 20200, train/learning_rate: 2.5963827283703722e-05
Step: 20200, train/epoch: 4.807234764099121
Step: 20210, train/loss: 0.0
Step: 20210, train/grad_norm: 2.211507066363083e-08
Step: 20210, train/learning_rate: 2.5951927455025725e-05
Step: 20210, train/epoch: 4.809614658355713
Step: 20220, train/loss: 0.0
Step: 20220, train/grad_norm: 6.051866119349825e-09
Step: 20220, train/learning_rate: 2.594002944533713e-05
Step: 20220, train/epoch: 4.8119940757751465
Step: 20230, train/loss: 0.0
Step: 20230, train/grad_norm: 3.834637851696243e-08
Step: 20230, train/learning_rate: 2.5928129616659135e-05
Step: 20230, train/epoch: 4.814373970031738
Step: 20240, train/loss: 0.0
Step: 20240, train/grad_norm: 2.7414235811984433e-11
Step: 20240, train/learning_rate: 2.5916229787981138e-05
Step: 20240, train/epoch: 4.81675386428833
Step: 20250, train/loss: 0.0
Step: 20250, train/grad_norm: 4.33764917318058e-08
Step: 20250, train/learning_rate: 2.5904331778292544e-05
Step: 20250, train/epoch: 4.819133758544922
Step: 20260, train/loss: 0.0
Step: 20260, train/grad_norm: 6.1441185472688176e-09
Step: 20260, train/learning_rate: 2.5892431949614547e-05
Step: 20260, train/epoch: 4.821513652801514
Step: 20270, train/loss: 0.0
Step: 20270, train/grad_norm: 1.4282255066111471e-11
Step: 20270, train/learning_rate: 2.5880533939925954e-05
Step: 20270, train/epoch: 4.8238935470581055
Step: 20280, train/loss: 0.0
Step: 20280, train/grad_norm: 4.939458335684321e-07
Step: 20280, train/learning_rate: 2.5868634111247957e-05
Step: 20280, train/epoch: 4.826273441314697
Step: 20290, train/loss: 0.0
Step: 20290, train/grad_norm: 5.436356076415905e-08
Step: 20290, train/learning_rate: 2.585673428256996e-05
Step: 20290, train/epoch: 4.828652858734131
Step: 20300, train/loss: 0.0
Step: 20300, train/grad_norm: 8.962608433193964e-10
Step: 20300, train/learning_rate: 2.5844836272881366e-05
Step: 20300, train/epoch: 4.831032752990723
Step: 20310, train/loss: 0.0
Step: 20310, train/grad_norm: 0.0008649657829664648
Step: 20310, train/learning_rate: 2.583293644420337e-05
Step: 20310, train/epoch: 4.8334126472473145
Step: 20320, train/loss: 0.0
Step: 20320, train/grad_norm: 1.0054515442448064e-08
Step: 20320, train/learning_rate: 2.5821038434514776e-05
Step: 20320, train/epoch: 4.835792541503906
Step: 20330, train/loss: 0.0
Step: 20330, train/grad_norm: 1.005447014534866e-08
Step: 20330, train/learning_rate: 2.580913860583678e-05
Step: 20330, train/epoch: 4.838172435760498
Step: 20340, train/loss: 0.0
Step: 20340, train/grad_norm: 1.0260094857050817e-10
Step: 20340, train/learning_rate: 2.5797238777158782e-05
Step: 20340, train/epoch: 4.84055233001709
Step: 20350, train/loss: 0.0
Step: 20350, train/grad_norm: 1.2696479978480113e-10
Step: 20350, train/learning_rate: 2.578534076747019e-05
Step: 20350, train/epoch: 4.842931747436523
Step: 20360, train/loss: 0.0
Step: 20360, train/grad_norm: 3.9058673451108916e-07
Step: 20360, train/learning_rate: 2.577344093879219e-05
Step: 20360, train/epoch: 4.845311641693115
Step: 20370, train/loss: 0.0
Step: 20370, train/grad_norm: 8.160051123695666e-08
Step: 20370, train/learning_rate: 2.5761542929103598e-05
Step: 20370, train/epoch: 4.847691535949707
Step: 20380, train/loss: 0.0
Step: 20380, train/grad_norm: 5.370495728129754e-06
Step: 20380, train/learning_rate: 2.57496431004256e-05
Step: 20380, train/epoch: 4.850071430206299
Step: 20390, train/loss: 0.0
Step: 20390, train/grad_norm: 4.268075615154743e-10
Step: 20390, train/learning_rate: 2.5737743271747604e-05
Step: 20390, train/epoch: 4.852451324462891
Step: 20400, train/loss: 0.0
Step: 20400, train/grad_norm: 9.338747553044868e-10
Step: 20400, train/learning_rate: 2.572584526205901e-05
Step: 20400, train/epoch: 4.854831218719482
Step: 20410, train/loss: 0.0
Step: 20410, train/grad_norm: 7.73941444265347e-09
Step: 20410, train/learning_rate: 2.5713945433381014e-05
Step: 20410, train/epoch: 4.857210636138916
Step: 20420, train/loss: 0.0
Step: 20420, train/grad_norm: 1.2731471876747946e-08
Step: 20420, train/learning_rate: 2.570204742369242e-05
Step: 20420, train/epoch: 4.859590530395508
Step: 20430, train/loss: 0.0
Step: 20430, train/grad_norm: 8.486261293683128e-08
Step: 20430, train/learning_rate: 2.5690147595014423e-05
Step: 20430, train/epoch: 4.8619704246521
Step: 20440, train/loss: 0.0
Step: 20440, train/grad_norm: 5.75181768880384e-09
Step: 20440, train/learning_rate: 2.5678247766336426e-05
Step: 20440, train/epoch: 4.864350318908691
Step: 20450, train/loss: 0.0
Step: 20450, train/grad_norm: 3.910817850116821e-12
Step: 20450, train/learning_rate: 2.5666349756647833e-05
Step: 20450, train/epoch: 4.866730213165283
Step: 20460, train/loss: 0.0
Step: 20460, train/grad_norm: 2.395680320788074e-10
Step: 20460, train/learning_rate: 2.5654449927969836e-05
Step: 20460, train/epoch: 4.869110107421875
Step: 20470, train/loss: 0.0
Step: 20470, train/grad_norm: 1.34475264257361e-09
Step: 20470, train/learning_rate: 2.5642551918281242e-05
Step: 20470, train/epoch: 4.871490001678467
Step: 20480, train/loss: 0.0
Step: 20480, train/grad_norm: 4.864149881489421e-12
Step: 20480, train/learning_rate: 2.5630652089603245e-05
Step: 20480, train/epoch: 4.8738694190979
Step: 20490, train/loss: 0.0
Step: 20490, train/grad_norm: 2.8111563077359847e-10
Step: 20490, train/learning_rate: 2.5618752260925248e-05
Step: 20490, train/epoch: 4.876249313354492
Step: 20500, train/loss: 0.0005000000237487257
Step: 20500, train/grad_norm: 5.012236314883012e-08
Step: 20500, train/learning_rate: 2.5606854251236655e-05
Step: 20500, train/epoch: 4.878629207611084
Step: 20510, train/loss: 0.0
Step: 20510, train/grad_norm: 1.7013880538804815e-12
Step: 20510, train/learning_rate: 2.5594954422558658e-05
Step: 20510, train/epoch: 4.881009101867676
Step: 20520, train/loss: 0.0
Step: 20520, train/grad_norm: 1.6962511395490765e-09
Step: 20520, train/learning_rate: 2.5583056412870064e-05
Step: 20520, train/epoch: 4.883388996124268
Step: 20530, train/loss: 0.2687999904155731
Step: 20530, train/grad_norm: 4.248375429227735e-09
Step: 20530, train/learning_rate: 2.5571156584192067e-05
Step: 20530, train/epoch: 4.885768890380859
Step: 20540, train/loss: 0.0
Step: 20540, train/grad_norm: 5.6896613642720695e-08
Step: 20540, train/learning_rate: 2.555925675551407e-05
Step: 20540, train/epoch: 4.888148307800293
Step: 20550, train/loss: 0.0
Step: 20550, train/grad_norm: 1.0665532901299457e-09
Step: 20550, train/learning_rate: 2.5547358745825477e-05
Step: 20550, train/epoch: 4.890528202056885
Step: 20560, train/loss: 0.0
Step: 20560, train/grad_norm: 3.5126588304734696e-09
Step: 20560, train/learning_rate: 2.553545891714748e-05
Step: 20560, train/epoch: 4.892908096313477
Step: 20570, train/loss: 0.0
Step: 20570, train/grad_norm: 1.8660939460346526e-08
Step: 20570, train/learning_rate: 2.5523560907458887e-05
Step: 20570, train/epoch: 4.895287990570068
Step: 20580, train/loss: 0.0
Step: 20580, train/grad_norm: 1.8422496861347781e-10
Step: 20580, train/learning_rate: 2.551166107878089e-05
Step: 20580, train/epoch: 4.89766788482666
Step: 20590, train/loss: 0.0
Step: 20590, train/grad_norm: 2.6783306128486117e-10
Step: 20590, train/learning_rate: 2.5499761250102893e-05
Step: 20590, train/epoch: 4.900047779083252
Step: 20600, train/loss: 0.0
Step: 20600, train/grad_norm: 5.192998742131749e-07
Step: 20600, train/learning_rate: 2.54878632404143e-05
Step: 20600, train/epoch: 4.9024271965026855
Step: 20610, train/loss: 0.0
Step: 20610, train/grad_norm: 2.375645351548883e-07
Step: 20610, train/learning_rate: 2.5475963411736302e-05
Step: 20610, train/epoch: 4.904807090759277
Step: 20620, train/loss: 0.0
Step: 20620, train/grad_norm: 1.3025787393150523e-10
Step: 20620, train/learning_rate: 2.546406540204771e-05
Step: 20620, train/epoch: 4.907186985015869
Step: 20630, train/loss: 0.0
Step: 20630, train/grad_norm: 7.068155696288159e-07
Step: 20630, train/learning_rate: 2.545216557336971e-05
Step: 20630, train/epoch: 4.909566879272461
Step: 20640, train/loss: 0.0
Step: 20640, train/grad_norm: 3.92775518776034e-07
Step: 20640, train/learning_rate: 2.5440265744691715e-05
Step: 20640, train/epoch: 4.911946773529053
Step: 20650, train/loss: 0.0
Step: 20650, train/grad_norm: 6.062209623181047e-10
Step: 20650, train/learning_rate: 2.542836773500312e-05
Step: 20650, train/epoch: 4.9143266677856445
Step: 20660, train/loss: 0.0
Step: 20660, train/grad_norm: 1.206327482350389e-07
Step: 20660, train/learning_rate: 2.5416467906325124e-05
Step: 20660, train/epoch: 4.916706562042236
Step: 20670, train/loss: 0.0
Step: 20670, train/grad_norm: 4.992621427390986e-09
Step: 20670, train/learning_rate: 2.540456989663653e-05
Step: 20670, train/epoch: 4.91908597946167
Step: 20680, train/loss: 0.0
Step: 20680, train/grad_norm: 2.0673716072394654e-09
Step: 20680, train/learning_rate: 2.5392670067958534e-05
Step: 20680, train/epoch: 4.921465873718262
Step: 20690, train/loss: 0.0
Step: 20690, train/grad_norm: 4.987335614714539e-06
Step: 20690, train/learning_rate: 2.5380770239280537e-05
Step: 20690, train/epoch: 4.9238457679748535
Step: 20700, train/loss: 0.0
Step: 20700, train/grad_norm: 5.383900436584099e-09
Step: 20700, train/learning_rate: 2.5368872229591943e-05
Step: 20700, train/epoch: 4.926225662231445
Step: 20710, train/loss: 0.0
Step: 20710, train/grad_norm: 2.643626828557899e-07
Step: 20710, train/learning_rate: 2.5356972400913946e-05
Step: 20710, train/epoch: 4.928605556488037
Step: 20720, train/loss: 0.0
Step: 20720, train/grad_norm: 1.9117224780984543e-07
Step: 20720, train/learning_rate: 2.5345074391225353e-05
Step: 20720, train/epoch: 4.930985450744629
Step: 20730, train/loss: 0.0
Step: 20730, train/grad_norm: 1.095895285629922e-08
Step: 20730, train/learning_rate: 2.5333174562547356e-05
Step: 20730, train/epoch: 4.9333648681640625
Step: 20740, train/loss: 0.0
Step: 20740, train/grad_norm: 1.3558221212406352e-08
Step: 20740, train/learning_rate: 2.532127473386936e-05
Step: 20740, train/epoch: 4.935744762420654
Step: 20750, train/loss: 0.0
Step: 20750, train/grad_norm: 5.131748359588073e-09
Step: 20750, train/learning_rate: 2.5309376724180765e-05
Step: 20750, train/epoch: 4.938124656677246
Step: 20760, train/loss: 0.0
Step: 20760, train/grad_norm: 7.905715193601281e-09
Step: 20760, train/learning_rate: 2.529747689550277e-05
Step: 20760, train/epoch: 4.940504550933838
Step: 20770, train/loss: 0.0
Step: 20770, train/grad_norm: 5.335441983334022e-06
Step: 20770, train/learning_rate: 2.5285578885814175e-05
Step: 20770, train/epoch: 4.94288444519043
Step: 20780, train/loss: 0.0
Step: 20780, train/grad_norm: 2.1547627682139137e-07
Step: 20780, train/learning_rate: 2.5273679057136178e-05
Step: 20780, train/epoch: 4.9452643394470215
Step: 20790, train/loss: 0.0
Step: 20790, train/grad_norm: 9.730144665809348e-05
Step: 20790, train/learning_rate: 2.526177922845818e-05
Step: 20790, train/epoch: 4.947643756866455
Step: 20800, train/loss: 0.0
Step: 20800, train/grad_norm: 2.3705047169642057e-06
Step: 20800, train/learning_rate: 2.5249881218769588e-05
Step: 20800, train/epoch: 4.950023651123047
Step: 20810, train/loss: 9.999999747378752e-05
Step: 20810, train/grad_norm: 1.163756646604952e-08
Step: 20810, train/learning_rate: 2.523798139009159e-05
Step: 20810, train/epoch: 4.952403545379639
Step: 20820, train/loss: 0.000699999975040555
Step: 20820, train/grad_norm: 1.9662909078732582e-09
Step: 20820, train/learning_rate: 2.5226083380402997e-05
Step: 20820, train/epoch: 4.9547834396362305
Step: 20830, train/loss: 0.0
Step: 20830, train/grad_norm: 2.0666368527599843e-07
Step: 20830, train/learning_rate: 2.5214183551725e-05
Step: 20830, train/epoch: 4.957163333892822
Step: 20840, train/loss: 0.06449999660253525
Step: 20840, train/grad_norm: 6.226919504115358e-05
Step: 20840, train/learning_rate: 2.5202283723047003e-05
Step: 20840, train/epoch: 4.959543228149414
Step: 20850, train/loss: 0.0
Step: 20850, train/grad_norm: 5.802286429101855e-10
Step: 20850, train/learning_rate: 2.519038571335841e-05
Step: 20850, train/epoch: 4.961923122406006
Step: 20860, train/loss: 0.01209999993443489
Step: 20860, train/grad_norm: 2.5051738461456807e-10
Step: 20860, train/learning_rate: 2.5178485884680413e-05
Step: 20860, train/epoch: 4.9643025398254395
Step: 20870, train/loss: 0.0
Step: 20870, train/grad_norm: 2.4615865186206065e-06
Step: 20870, train/learning_rate: 2.516658787499182e-05
Step: 20870, train/epoch: 4.966682434082031
Step: 20880, train/loss: 0.0
Step: 20880, train/grad_norm: 4.4846923397123817e-10
Step: 20880, train/learning_rate: 2.5154688046313822e-05
Step: 20880, train/epoch: 4.969062328338623
Step: 20890, train/loss: 0.0
Step: 20890, train/grad_norm: 1.1820788614613775e-07
Step: 20890, train/learning_rate: 2.514279003662523e-05
Step: 20890, train/epoch: 4.971442222595215
Step: 20900, train/loss: 0.0007999999797903001
Step: 20900, train/grad_norm: 0.00014126567111816257
Step: 20900, train/learning_rate: 2.5130890207947232e-05
Step: 20900, train/epoch: 4.973822116851807
Step: 20910, train/loss: 0.0
Step: 20910, train/grad_norm: 1.9033810638724447e-10
Step: 20910, train/learning_rate: 2.5118990379269235e-05
Step: 20910, train/epoch: 4.976202011108398
Step: 20920, train/loss: 0.0
Step: 20920, train/grad_norm: 2.9156843606159555e-10
Step: 20920, train/learning_rate: 2.510709236958064e-05
Step: 20920, train/epoch: 4.978581428527832
Step: 20930, train/loss: 0.0
Step: 20930, train/grad_norm: 0.0002834809711202979
Step: 20930, train/learning_rate: 2.5095192540902644e-05
Step: 20930, train/epoch: 4.980961322784424
Step: 20940, train/loss: 0.10859999805688858
Step: 20940, train/grad_norm: 0.00025052836281247437
Step: 20940, train/learning_rate: 2.508329453121405e-05
Step: 20940, train/epoch: 4.983341217041016
Step: 20950, train/loss: 0.0
Step: 20950, train/grad_norm: 7.78234243625775e-06
Step: 20950, train/learning_rate: 2.5071394702536054e-05
Step: 20950, train/epoch: 4.985721111297607
Step: 20960, train/loss: 0.06909999996423721
Step: 20960, train/grad_norm: 1.8608392565511167e-05
Step: 20960, train/learning_rate: 2.5059494873858057e-05
Step: 20960, train/epoch: 4.988101005554199
Step: 20970, train/loss: 0.0
Step: 20970, train/grad_norm: 6.208138643160055e-07
Step: 20970, train/learning_rate: 2.5047596864169464e-05
Step: 20970, train/epoch: 4.990480899810791
Step: 20980, train/loss: 0.0
Step: 20980, train/grad_norm: 3.1627476193563098e-09
Step: 20980, train/learning_rate: 2.5035697035491467e-05
Step: 20980, train/epoch: 4.992860317230225
Step: 20990, train/loss: 0.0
Step: 20990, train/grad_norm: 7.176525400609535e-07
Step: 20990, train/learning_rate: 2.5023799025802873e-05
Step: 20990, train/epoch: 4.995240211486816
Step: 21000, train/loss: 0.0
Step: 21000, train/grad_norm: 0.019129421561956406
Step: 21000, train/learning_rate: 2.5011899197124876e-05
Step: 21000, train/epoch: 4.997620105743408
Step: 21010, train/loss: 0.0
Step: 21010, train/grad_norm: 6.354847746116443e-13
Step: 21010, train/learning_rate: 2.499999936844688e-05
Step: 21010, train/epoch: 5.0
Step: 21010, eval/loss: 0.06309477984905243
Step: 21010, eval/accuracy: 0.9933360815048218
Step: 21010, eval/f1: 0.9929454922676086
Step: 21010, eval/runtime: 707.851806640625
Step: 21010, eval/samples_per_second: 10.175999641418457
Step: 21010, eval/steps_per_second: 1.2730000019073486
Step: 21010, train/epoch: 5.0
Step: 21020, train/loss: 0.0
Step: 21020, train/grad_norm: 3.410302042539115e-06
Step: 21020, train/learning_rate: 2.4988101358758286e-05
Step: 21020, train/epoch: 5.002379894256592
Step: 21030, train/loss: 0.0
Step: 21030, train/grad_norm: 3.863299369299966e-08
Step: 21030, train/learning_rate: 2.497620153008029e-05
Step: 21030, train/epoch: 5.004759788513184
Step: 21040, train/loss: 0.0
Step: 21040, train/grad_norm: 5.837926586593767e-09
Step: 21040, train/learning_rate: 2.4964303520391695e-05
Step: 21040, train/epoch: 5.007139682769775
Step: 21050, train/loss: 0.0
Step: 21050, train/grad_norm: 6.260461759666214e-06
Step: 21050, train/learning_rate: 2.4952403691713698e-05
Step: 21050, train/epoch: 5.009519100189209
Step: 21060, train/loss: 0.009100000374019146
Step: 21060, train/grad_norm: 6.077768510692749e-09
Step: 21060, train/learning_rate: 2.49405038630357e-05
Step: 21060, train/epoch: 5.011898994445801
Step: 21070, train/loss: 0.0
Step: 21070, train/grad_norm: 5.211204552324489e-06
Step: 21070, train/learning_rate: 2.4928605853347108e-05
Step: 21070, train/epoch: 5.014278888702393
Step: 21080, train/loss: 0.0
Step: 21080, train/grad_norm: 3.635631401266437e-06
Step: 21080, train/learning_rate: 2.491670602466911e-05
Step: 21080, train/epoch: 5.016658782958984
Step: 21090, train/loss: 0.0
Step: 21090, train/grad_norm: 2.3049285147180854e-08
Step: 21090, train/learning_rate: 2.4904808014980517e-05
Step: 21090, train/epoch: 5.019038677215576
Step: 21100, train/loss: 0.0
Step: 21100, train/grad_norm: 0.07794781774282455
Step: 21100, train/learning_rate: 2.489290818630252e-05
Step: 21100, train/epoch: 5.021418571472168
Step: 21110, train/loss: 0.0
Step: 21110, train/grad_norm: 2.6296447686036117e-06
Step: 21110, train/learning_rate: 2.4881008357624523e-05
Step: 21110, train/epoch: 5.023797988891602
Step: 21120, train/loss: 0.0
Step: 21120, train/grad_norm: 1.4511479093926027e-05
Step: 21120, train/learning_rate: 2.486911034793593e-05
Step: 21120, train/epoch: 5.026177883148193
Step: 21130, train/loss: 0.0
Step: 21130, train/grad_norm: 0.052928294986486435
Step: 21130, train/learning_rate: 2.4857210519257933e-05
Step: 21130, train/epoch: 5.028557777404785
Step: 21140, train/loss: 0.0
Step: 21140, train/grad_norm: 7.567754801129922e-05
Step: 21140, train/learning_rate: 2.484531250956934e-05
Step: 21140, train/epoch: 5.030937671661377
Step: 21150, train/loss: 0.0
Step: 21150, train/grad_norm: 0.005928820930421352
Step: 21150, train/learning_rate: 2.4833412680891342e-05
Step: 21150, train/epoch: 5.033317565917969
Step: 21160, train/loss: 0.0
Step: 21160, train/grad_norm: 1.1961733434873167e-06
Step: 21160, train/learning_rate: 2.4821512852213345e-05
Step: 21160, train/epoch: 5.0356974601745605
Step: 21170, train/loss: 0.0
Step: 21170, train/grad_norm: 1.532477611476679e-08
Step: 21170, train/learning_rate: 2.4809614842524752e-05
Step: 21170, train/epoch: 5.038076877593994
Step: 21180, train/loss: 0.0
Step: 21180, train/grad_norm: 0.042453765869140625
Step: 21180, train/learning_rate: 2.4797715013846755e-05
Step: 21180, train/epoch: 5.040456771850586
Step: 21190, train/loss: 0.0
Step: 21190, train/grad_norm: 2.824935108947102e-06
Step: 21190, train/learning_rate: 2.478581700415816e-05
Step: 21190, train/epoch: 5.042836666107178
Step: 21200, train/loss: 0.0
Step: 21200, train/grad_norm: 1.0177024556412562e-08
Step: 21200, train/learning_rate: 2.4773917175480165e-05
Step: 21200, train/epoch: 5.0452165603637695
Step: 21210, train/loss: 0.0
Step: 21210, train/grad_norm: 1.2862220444276318e-07
Step: 21210, train/learning_rate: 2.4762017346802168e-05
Step: 21210, train/epoch: 5.047596454620361
Step: 21220, train/loss: 0.0
Step: 21220, train/grad_norm: 3.009993179148296e-07
Step: 21220, train/learning_rate: 2.4750119337113574e-05
Step: 21220, train/epoch: 5.049976348876953
Step: 21230, train/loss: 0.00019999999494757503
Step: 21230, train/grad_norm: 0.01694701425731182
Step: 21230, train/learning_rate: 2.4738219508435577e-05
Step: 21230, train/epoch: 5.052356243133545
Step: 21240, train/loss: 0.0
Step: 21240, train/grad_norm: 1.0631657776372094e-09
Step: 21240, train/learning_rate: 2.4726321498746984e-05
Step: 21240, train/epoch: 5.0547356605529785
Step: 21250, train/loss: 0.0
Step: 21250, train/grad_norm: 5.618953946395777e-05
Step: 21250, train/learning_rate: 2.4714421670068987e-05
Step: 21250, train/epoch: 5.05711555480957
Step: 21260, train/loss: 0.0
Step: 21260, train/grad_norm: 3.701710738823749e-06
Step: 21260, train/learning_rate: 2.470252184139099e-05
Step: 21260, train/epoch: 5.059495449066162
Step: 21270, train/loss: 0.05079999938607216
Step: 21270, train/grad_norm: 2.1128512828205714e-10
Step: 21270, train/learning_rate: 2.4690623831702396e-05
Step: 21270, train/epoch: 5.061875343322754
Step: 21280, train/loss: 0.0
Step: 21280, train/grad_norm: 4.470859593652676e-08
Step: 21280, train/learning_rate: 2.46787240030244e-05
Step: 21280, train/epoch: 5.064255237579346
Step: 21290, train/loss: 0.0
Step: 21290, train/grad_norm: 6.048539002989628e-09
Step: 21290, train/learning_rate: 2.4666825993335806e-05
Step: 21290, train/epoch: 5.0666351318359375
Step: 21300, train/loss: 0.0
Step: 21300, train/grad_norm: 1.7725717782468564e-08
Step: 21300, train/learning_rate: 2.465492616465781e-05
Step: 21300, train/epoch: 5.069014549255371
Step: 21310, train/loss: 0.0
Step: 21310, train/grad_norm: 1.3123269582138164e-07
Step: 21310, train/learning_rate: 2.4643026335979812e-05
Step: 21310, train/epoch: 5.071394443511963
Step: 21320, train/loss: 0.0
Step: 21320, train/grad_norm: 2.623101735821365e-10
Step: 21320, train/learning_rate: 2.463112832629122e-05
Step: 21320, train/epoch: 5.073774337768555
Step: 21330, train/loss: 0.18199999630451202
Step: 21330, train/grad_norm: 8.354355668416247e-05
Step: 21330, train/learning_rate: 2.461922849761322e-05
Step: 21330, train/epoch: 5.0761542320251465
Step: 21340, train/loss: 0.15389999747276306
Step: 21340, train/grad_norm: 4.078646398397723e-08
Step: 21340, train/learning_rate: 2.4607330487924628e-05
Step: 21340, train/epoch: 5.078534126281738
Step: 21350, train/loss: 0.0
Step: 21350, train/grad_norm: 4.6094223193904327e-07
Step: 21350, train/learning_rate: 2.459543065924663e-05
Step: 21350, train/epoch: 5.08091402053833
Step: 21360, train/loss: 0.0
Step: 21360, train/grad_norm: 3.6676806303148624e-07
Step: 21360, train/learning_rate: 2.4583530830568634e-05
Step: 21360, train/epoch: 5.083293437957764
Step: 21370, train/loss: 0.0
Step: 21370, train/grad_norm: 1.1138369700347539e-05
Step: 21370, train/learning_rate: 2.457163282088004e-05
Step: 21370, train/epoch: 5.0856733322143555
Step: 21380, train/loss: 0.0
Step: 21380, train/grad_norm: 3.4516160667408258e-06
Step: 21380, train/learning_rate: 2.4559732992202044e-05
Step: 21380, train/epoch: 5.088053226470947
Step: 21390, train/loss: 0.0
Step: 21390, train/grad_norm: 1.0014904546551406e-05
Step: 21390, train/learning_rate: 2.454783498251345e-05
Step: 21390, train/epoch: 5.090433120727539
Step: 21400, train/loss: 0.0
Step: 21400, train/grad_norm: 2.3929711545633836e-08
Step: 21400, train/learning_rate: 2.4535935153835453e-05
Step: 21400, train/epoch: 5.092813014984131
Step: 21410, train/loss: 0.0
Step: 21410, train/grad_norm: 8.981468999991193e-05
Step: 21410, train/learning_rate: 2.4524035325157456e-05
Step: 21410, train/epoch: 5.095192909240723
Step: 21420, train/loss: 0.0
Step: 21420, train/grad_norm: 0.00020872008462902158
Step: 21420, train/learning_rate: 2.4512137315468863e-05
Step: 21420, train/epoch: 5.0975728034973145
Step: 21430, train/loss: 9.999999747378752e-05
Step: 21430, train/grad_norm: 2.977521398861427e-05
Step: 21430, train/learning_rate: 2.4500237486790866e-05
Step: 21430, train/epoch: 5.099952220916748
Step: 21440, train/loss: 0.00019999999494757503
Step: 21440, train/grad_norm: 4.256497376786683e-09
Step: 21440, train/learning_rate: 2.4488339477102272e-05
Step: 21440, train/epoch: 5.10233211517334
Step: 21450, train/loss: 0.0
Step: 21450, train/grad_norm: 5.5396238707317025e-08
Step: 21450, train/learning_rate: 2.4476439648424275e-05
Step: 21450, train/epoch: 5.104712009429932
Step: 21460, train/loss: 0.0
Step: 21460, train/grad_norm: 2.7575106642530045e-08
Step: 21460, train/learning_rate: 2.4464539819746278e-05
Step: 21460, train/epoch: 5.107091903686523
Step: 21470, train/loss: 0.0
Step: 21470, train/grad_norm: 5.127031954543781e-07
Step: 21470, train/learning_rate: 2.4452641810057685e-05
Step: 21470, train/epoch: 5.109471797943115
Step: 21480, train/loss: 0.0
Step: 21480, train/grad_norm: 3.588631079765037e-05
Step: 21480, train/learning_rate: 2.4440741981379688e-05
Step: 21480, train/epoch: 5.111851692199707
Step: 21490, train/loss: 0.0
Step: 21490, train/grad_norm: 8.981276139463645e-11
Step: 21490, train/learning_rate: 2.4428843971691094e-05
Step: 21490, train/epoch: 5.114231109619141
Step: 21500, train/loss: 0.0
Step: 21500, train/grad_norm: 4.3393686866011194e-08
Step: 21500, train/learning_rate: 2.4416944143013097e-05
Step: 21500, train/epoch: 5.116611003875732
Step: 21510, train/loss: 0.0
Step: 21510, train/grad_norm: 9.606962521502282e-06
Step: 21510, train/learning_rate: 2.44050443143351e-05
Step: 21510, train/epoch: 5.118990898132324
Step: 21520, train/loss: 0.0
Step: 21520, train/grad_norm: 6.375839234351588e-08
Step: 21520, train/learning_rate: 2.4393146304646507e-05
Step: 21520, train/epoch: 5.121370792388916
Step: 21530, train/loss: 9.999999747378752e-05
Step: 21530, train/grad_norm: 2.2268658028679056e-09
Step: 21530, train/learning_rate: 2.438124647596851e-05
Step: 21530, train/epoch: 5.123750686645508
Step: 21540, train/loss: 0.0
Step: 21540, train/grad_norm: 7.464039802895783e-12
Step: 21540, train/learning_rate: 2.4369348466279916e-05
Step: 21540, train/epoch: 5.1261305809021
Step: 21550, train/loss: 0.012900000438094139
Step: 21550, train/grad_norm: 2.622736872126552e-07
Step: 21550, train/learning_rate: 2.435744863760192e-05
Step: 21550, train/epoch: 5.128509998321533
Step: 21560, train/loss: 0.0
Step: 21560, train/grad_norm: 4.54367691418156e-05
Step: 21560, train/learning_rate: 2.4345550627913326e-05
Step: 21560, train/epoch: 5.130889892578125
Step: 21570, train/loss: 0.0
Step: 21570, train/grad_norm: 1.2876676391826436e-09
Step: 21570, train/learning_rate: 2.433365079923533e-05
Step: 21570, train/epoch: 5.133269786834717
Step: 21580, train/loss: 0.0
Step: 21580, train/grad_norm: 2.0032965153404803e-07
Step: 21580, train/learning_rate: 2.4321750970557332e-05
Step: 21580, train/epoch: 5.135649681091309
Step: 21590, train/loss: 0.0
Step: 21590, train/grad_norm: 0.0019523047376424074
Step: 21590, train/learning_rate: 2.430985296086874e-05
Step: 21590, train/epoch: 5.1380295753479
Step: 21600, train/loss: 0.0
Step: 21600, train/grad_norm: 8.949914627010003e-05
Step: 21600, train/learning_rate: 2.429795313219074e-05
Step: 21600, train/epoch: 5.140409469604492
Step: 21610, train/loss: 0.0
Step: 21610, train/grad_norm: 4.031583245023285e-08
Step: 21610, train/learning_rate: 2.4286055122502148e-05
Step: 21610, train/epoch: 5.142789363861084
Step: 21620, train/loss: 0.0
Step: 21620, train/grad_norm: 2.4094424588838592e-05
Step: 21620, train/learning_rate: 2.427415529382415e-05
Step: 21620, train/epoch: 5.145168781280518
Step: 21630, train/loss: 0.0
Step: 21630, train/grad_norm: 2.2786021680598623e-10
Step: 21630, train/learning_rate: 2.4262255465146154e-05
Step: 21630, train/epoch: 5.147548675537109
Step: 21640, train/loss: 0.0
Step: 21640, train/grad_norm: 3.1817601331418643e-10
Step: 21640, train/learning_rate: 2.425035745545756e-05
Step: 21640, train/epoch: 5.149928569793701
Step: 21650, train/loss: 0.17499999701976776
Step: 21650, train/grad_norm: 0.002990187145769596
Step: 21650, train/learning_rate: 2.4238457626779564e-05
Step: 21650, train/epoch: 5.152308464050293
Step: 21660, train/loss: 0.0
Step: 21660, train/grad_norm: 6.960030773939252e-09
Step: 21660, train/learning_rate: 2.422655961709097e-05
Step: 21660, train/epoch: 5.154688358306885
Step: 21670, train/loss: 0.0
Step: 21670, train/grad_norm: 3.7187021462159464e-08
Step: 21670, train/learning_rate: 2.4214659788412973e-05
Step: 21670, train/epoch: 5.157068252563477
Step: 21680, train/loss: 0.0
Step: 21680, train/grad_norm: 2.1367263514093793e-09
Step: 21680, train/learning_rate: 2.4202759959734976e-05
Step: 21680, train/epoch: 5.15944766998291
Step: 21690, train/loss: 0.0
Step: 21690, train/grad_norm: 1.4553497962310757e-08
Step: 21690, train/learning_rate: 2.4190861950046383e-05
Step: 21690, train/epoch: 5.161827564239502
Step: 21700, train/loss: 0.0
Step: 21700, train/grad_norm: 3.9102494575615765e-09
Step: 21700, train/learning_rate: 2.4178962121368386e-05
Step: 21700, train/epoch: 5.164207458496094
Step: 21710, train/loss: 9.999999747378752e-05
Step: 21710, train/grad_norm: 3.164139116051956e-06
Step: 21710, train/learning_rate: 2.4167064111679792e-05
Step: 21710, train/epoch: 5.1665873527526855
Step: 21720, train/loss: 0.00019999999494757503
Step: 21720, train/grad_norm: 9.363206043833117e-11
Step: 21720, train/learning_rate: 2.4155164283001795e-05
Step: 21720, train/epoch: 5.168967247009277
Step: 21730, train/loss: 0.0
Step: 21730, train/grad_norm: 1.7474520441762564e-10
Step: 21730, train/learning_rate: 2.41432644543238e-05
Step: 21730, train/epoch: 5.171347141265869
Step: 21740, train/loss: 0.0
Step: 21740, train/grad_norm: 5.7592819402429996e-11
Step: 21740, train/learning_rate: 2.4131366444635205e-05
Step: 21740, train/epoch: 5.173726558685303
Step: 21750, train/loss: 0.0
Step: 21750, train/grad_norm: 2.641488416887938e-10
Step: 21750, train/learning_rate: 2.4119466615957208e-05
Step: 21750, train/epoch: 5.1761064529418945
Step: 21760, train/loss: 0.0
Step: 21760, train/grad_norm: 1.3899768835923254e-10
Step: 21760, train/learning_rate: 2.4107568606268615e-05
Step: 21760, train/epoch: 5.178486347198486
Step: 21770, train/loss: 0.0
Step: 21770, train/grad_norm: 1.0620454515830602e-09
Step: 21770, train/learning_rate: 2.4095668777590618e-05
Step: 21770, train/epoch: 5.180866241455078
Step: 21780, train/loss: 0.0
Step: 21780, train/grad_norm: 2.5623710371291963e-06
Step: 21780, train/learning_rate: 2.408376894891262e-05
Step: 21780, train/epoch: 5.18324613571167
Step: 21790, train/loss: 0.0
Step: 21790, train/grad_norm: 5.448779347716481e-07
Step: 21790, train/learning_rate: 2.4071870939224027e-05
Step: 21790, train/epoch: 5.185626029968262
Step: 21800, train/loss: 0.0
Step: 21800, train/grad_norm: 3.280776127212448e-06
Step: 21800, train/learning_rate: 2.405997111054603e-05
Step: 21800, train/epoch: 5.1880059242248535
Step: 21810, train/loss: 0.0
Step: 21810, train/grad_norm: 5.771871869342249e-11
Step: 21810, train/learning_rate: 2.4048073100857437e-05
Step: 21810, train/epoch: 5.190385341644287
Step: 21820, train/loss: 0.0
Step: 21820, train/grad_norm: 1.233318425164498e-08
Step: 21820, train/learning_rate: 2.403617327217944e-05
Step: 21820, train/epoch: 5.192765235900879
Step: 21830, train/loss: 0.0
Step: 21830, train/grad_norm: 3.846036520371854e-07
Step: 21830, train/learning_rate: 2.4024273443501443e-05
Step: 21830, train/epoch: 5.195145130157471
Step: 21840, train/loss: 0.0
Step: 21840, train/grad_norm: 2.4017115407559686e-08
Step: 21840, train/learning_rate: 2.401237543381285e-05
Step: 21840, train/epoch: 5.1975250244140625
Step: 21850, train/loss: 0.0
Step: 21850, train/grad_norm: 2.1939390304481776e-09
Step: 21850, train/learning_rate: 2.4000475605134852e-05
Step: 21850, train/epoch: 5.199904918670654
Step: 21860, train/loss: 0.0
Step: 21860, train/grad_norm: 3.552994287669975e-10
Step: 21860, train/learning_rate: 2.398857759544626e-05
Step: 21860, train/epoch: 5.202284812927246
Step: 21870, train/loss: 0.0
Step: 21870, train/grad_norm: 1.879752353772801e-09
Step: 21870, train/learning_rate: 2.3976677766768262e-05
Step: 21870, train/epoch: 5.20466423034668
Step: 21880, train/loss: 0.0
Step: 21880, train/grad_norm: 1.247333208942436e-10
Step: 21880, train/learning_rate: 2.3964777938090265e-05
Step: 21880, train/epoch: 5.2070441246032715
Step: 21890, train/loss: 0.0
Step: 21890, train/grad_norm: 2.0502903819163976e-09
Step: 21890, train/learning_rate: 2.395287992840167e-05
Step: 21890, train/epoch: 5.209424018859863
Step: 21900, train/loss: 0.0
Step: 21900, train/grad_norm: 1.8555910974438206e-10
Step: 21900, train/learning_rate: 2.3940980099723674e-05
Step: 21900, train/epoch: 5.211803913116455
Step: 21910, train/loss: 0.0
Step: 21910, train/grad_norm: 3.7329769830840576e-10
Step: 21910, train/learning_rate: 2.392908209003508e-05
Step: 21910, train/epoch: 5.214183807373047
Step: 21920, train/loss: 0.0
Step: 21920, train/grad_norm: 4.9050694062025e-11
Step: 21920, train/learning_rate: 2.3917182261357084e-05
Step: 21920, train/epoch: 5.216563701629639
Step: 21930, train/loss: 0.0
Step: 21930, train/grad_norm: 7.866584184057501e-08
Step: 21930, train/learning_rate: 2.3905282432679087e-05
Step: 21930, train/epoch: 5.2189435958862305
Step: 21940, train/loss: 0.0
Step: 21940, train/grad_norm: 1.5272886288997256e-09
Step: 21940, train/learning_rate: 2.3893384422990493e-05
Step: 21940, train/epoch: 5.221323013305664
Step: 21950, train/loss: 0.0
Step: 21950, train/grad_norm: 1.0362830380472587e-06
Step: 21950, train/learning_rate: 2.3881484594312496e-05
Step: 21950, train/epoch: 5.223702907562256
Step: 21960, train/loss: 0.0
Step: 21960, train/grad_norm: 1.1074086092577318e-09
Step: 21960, train/learning_rate: 2.3869586584623903e-05
Step: 21960, train/epoch: 5.226082801818848
Step: 21970, train/loss: 0.0
Step: 21970, train/grad_norm: 2.5696630601146353e-08
Step: 21970, train/learning_rate: 2.3857686755945906e-05
Step: 21970, train/epoch: 5.2284626960754395
Step: 21980, train/loss: 0.0
Step: 21980, train/grad_norm: 4.551548045128584e-06
Step: 21980, train/learning_rate: 2.384578692726791e-05
Step: 21980, train/epoch: 5.230842590332031
Step: 21990, train/loss: 0.0
Step: 21990, train/grad_norm: 3.580680474346565e-10
Step: 21990, train/learning_rate: 2.3833888917579316e-05
Step: 21990, train/epoch: 5.233222484588623
Step: 22000, train/loss: 0.0
Step: 22000, train/grad_norm: 6.221484993851334e-10
Step: 22000, train/learning_rate: 2.382198908890132e-05
Step: 22000, train/epoch: 5.235601902008057
Step: 22010, train/loss: 0.0
Step: 22010, train/grad_norm: 1.385085646177231e-08
Step: 22010, train/learning_rate: 2.3810091079212725e-05
Step: 22010, train/epoch: 5.237981796264648
Step: 22020, train/loss: 0.0
Step: 22020, train/grad_norm: 5.074002329408245e-10
Step: 22020, train/learning_rate: 2.3798191250534728e-05
Step: 22020, train/epoch: 5.24036169052124
Step: 22030, train/loss: 0.0
Step: 22030, train/grad_norm: 1.0800181061976222e-11
Step: 22030, train/learning_rate: 2.378629142185673e-05
Step: 22030, train/epoch: 5.242741584777832
Step: 22040, train/loss: 0.0
Step: 22040, train/grad_norm: 6.03073146976385e-10
Step: 22040, train/learning_rate: 2.3774393412168138e-05
Step: 22040, train/epoch: 5.245121479034424
Step: 22050, train/loss: 0.0
Step: 22050, train/grad_norm: 2.127723552902694e-10
Step: 22050, train/learning_rate: 2.376249358349014e-05
Step: 22050, train/epoch: 5.247501373291016
Step: 22060, train/loss: 0.0
Step: 22060, train/grad_norm: 4.026714961469224e-09
Step: 22060, train/learning_rate: 2.3750595573801547e-05
Step: 22060, train/epoch: 5.249880790710449
Step: 22070, train/loss: 0.0
Step: 22070, train/grad_norm: 1.871267141240196e-09
Step: 22070, train/learning_rate: 2.373869574512355e-05
Step: 22070, train/epoch: 5.252260684967041
Step: 22080, train/loss: 0.0
Step: 22080, train/grad_norm: 2.4307968971015725e-08
Step: 22080, train/learning_rate: 2.3726795916445553e-05
Step: 22080, train/epoch: 5.254640579223633
Step: 22090, train/loss: 0.0
Step: 22090, train/grad_norm: 9.443966719757668e-10
Step: 22090, train/learning_rate: 2.371489790675696e-05
Step: 22090, train/epoch: 5.257020473480225
Step: 22100, train/loss: 0.0
Step: 22100, train/grad_norm: 6.565486869447579e-12
Step: 22100, train/learning_rate: 2.3702998078078963e-05
Step: 22100, train/epoch: 5.259400367736816
Step: 22110, train/loss: 0.0
Step: 22110, train/grad_norm: 1.4385248547910123e-09
Step: 22110, train/learning_rate: 2.369110006839037e-05
Step: 22110, train/epoch: 5.261780261993408
Step: 22120, train/loss: 0.0
Step: 22120, train/grad_norm: 7.09642501012242e-12
Step: 22120, train/learning_rate: 2.3679200239712372e-05
Step: 22120, train/epoch: 5.26416015625
Step: 22130, train/loss: 0.0
Step: 22130, train/grad_norm: 1.871888707682956e-05
Step: 22130, train/learning_rate: 2.3667300411034375e-05
Step: 22130, train/epoch: 5.266539573669434
Step: 22140, train/loss: 0.0
Step: 22140, train/grad_norm: 8.199933245432722e-13
Step: 22140, train/learning_rate: 2.3655402401345782e-05
Step: 22140, train/epoch: 5.268919467926025
Step: 22150, train/loss: 0.0
Step: 22150, train/grad_norm: 1.654105741266676e-10
Step: 22150, train/learning_rate: 2.3643502572667785e-05
Step: 22150, train/epoch: 5.271299362182617
Step: 22160, train/loss: 0.0
Step: 22160, train/grad_norm: 6.058805013253732e-08
Step: 22160, train/learning_rate: 2.363160456297919e-05
Step: 22160, train/epoch: 5.273679256439209
Step: 22170, train/loss: 0.0
Step: 22170, train/grad_norm: 1.5488743088099e-07
Step: 22170, train/learning_rate: 2.3619704734301195e-05
Step: 22170, train/epoch: 5.276059150695801
Step: 22180, train/loss: 0.0
Step: 22180, train/grad_norm: 9.099311304439084e-10
Step: 22180, train/learning_rate: 2.3607804905623198e-05
Step: 22180, train/epoch: 5.278439044952393
Step: 22190, train/loss: 0.0
Step: 22190, train/grad_norm: 1.8398652912310354e-07
Step: 22190, train/learning_rate: 2.3595906895934604e-05
Step: 22190, train/epoch: 5.280818462371826
Step: 22200, train/loss: 0.0
Step: 22200, train/grad_norm: 1.1764252905166472e-09
Step: 22200, train/learning_rate: 2.3584007067256607e-05
Step: 22200, train/epoch: 5.283198356628418
Step: 22210, train/loss: 0.0
Step: 22210, train/grad_norm: 2.6181232737343407e-09
Step: 22210, train/learning_rate: 2.3572109057568014e-05
Step: 22210, train/epoch: 5.28557825088501
Step: 22220, train/loss: 0.0
Step: 22220, train/grad_norm: 7.070310857670847e-06
Step: 22220, train/learning_rate: 2.3560209228890017e-05
Step: 22220, train/epoch: 5.287958145141602
Step: 22230, train/loss: 0.0
Step: 22230, train/grad_norm: 1.6990734366117977e-06
Step: 22230, train/learning_rate: 2.3548311219201423e-05
Step: 22230, train/epoch: 5.290338039398193
Step: 22240, train/loss: 0.0
Step: 22240, train/grad_norm: 7.710976746011511e-09
Step: 22240, train/learning_rate: 2.3536411390523426e-05
Step: 22240, train/epoch: 5.292717933654785
Step: 22250, train/loss: 0.0
Step: 22250, train/grad_norm: 1.0497768698769505e-06
Step: 22250, train/learning_rate: 2.352451156184543e-05
Step: 22250, train/epoch: 5.295097351074219
Step: 22260, train/loss: 0.0
Step: 22260, train/grad_norm: 2.691157741097072e-09
Step: 22260, train/learning_rate: 2.3512613552156836e-05
Step: 22260, train/epoch: 5.2974772453308105
Step: 22270, train/loss: 0.0
Step: 22270, train/grad_norm: 3.355479748279322e-06
Step: 22270, train/learning_rate: 2.350071372347884e-05
Step: 22270, train/epoch: 5.299857139587402
Step: 22280, train/loss: 0.0
Step: 22280, train/grad_norm: 4.780670859183545e-10
Step: 22280, train/learning_rate: 2.3488815713790245e-05
Step: 22280, train/epoch: 5.302237033843994
Step: 22290, train/loss: 0.0
Step: 22290, train/grad_norm: 8.563863218569168e-08
Step: 22290, train/learning_rate: 2.347691588511225e-05
Step: 22290, train/epoch: 5.304616928100586
Step: 22300, train/loss: 0.0
Step: 22300, train/grad_norm: 1.7108868632931262e-07
Step: 22300, train/learning_rate: 2.346501605643425e-05
Step: 22300, train/epoch: 5.306996822357178
Step: 22310, train/loss: 0.0
Step: 22310, train/grad_norm: 3.999067743620799e-09
Step: 22310, train/learning_rate: 2.3453118046745658e-05
Step: 22310, train/epoch: 5.3093767166137695
Step: 22320, train/loss: 0.0
Step: 22320, train/grad_norm: 9.472539419519421e-11
Step: 22320, train/learning_rate: 2.344121821806766e-05
Step: 22320, train/epoch: 5.311756134033203
Step: 22330, train/loss: 0.0
Step: 22330, train/grad_norm: 3.400166606604138e-11
Step: 22330, train/learning_rate: 2.3429320208379067e-05
Step: 22330, train/epoch: 5.314136028289795
Step: 22340, train/loss: 0.0
Step: 22340, train/grad_norm: 3.2241533887145124e-09
Step: 22340, train/learning_rate: 2.341742037970107e-05
Step: 22340, train/epoch: 5.316515922546387
Step: 22350, train/loss: 0.0
Step: 22350, train/grad_norm: 3.3780912644942873e-07
Step: 22350, train/learning_rate: 2.3405520551023073e-05
Step: 22350, train/epoch: 5.3188958168029785
Step: 22360, train/loss: 0.0
Step: 22360, train/grad_norm: 7.681918794653908e-13
Step: 22360, train/learning_rate: 2.339362254133448e-05
Step: 22360, train/epoch: 5.32127571105957
Step: 22370, train/loss: 0.0
Step: 22370, train/grad_norm: 3.7067376945643105e-11
Step: 22370, train/learning_rate: 2.3381722712656483e-05
Step: 22370, train/epoch: 5.323655605316162
Step: 22380, train/loss: 0.0
Step: 22380, train/grad_norm: 4.4745496197151624e-10
Step: 22380, train/learning_rate: 2.336982470296789e-05
Step: 22380, train/epoch: 5.326035022735596
Step: 22390, train/loss: 0.0
Step: 22390, train/grad_norm: 7.034141731310228e-07
Step: 22390, train/learning_rate: 2.3357924874289893e-05
Step: 22390, train/epoch: 5.3284149169921875
Step: 22400, train/loss: 0.0
Step: 22400, train/grad_norm: 1.7105211469470305e-08
Step: 22400, train/learning_rate: 2.3346025045611896e-05
Step: 22400, train/epoch: 5.330794811248779
Step: 22410, train/loss: 0.0
Step: 22410, train/grad_norm: 3.461363667156547e-05
Step: 22410, train/learning_rate: 2.3334127035923302e-05
Step: 22410, train/epoch: 5.333174705505371
Step: 22420, train/loss: 0.0
Step: 22420, train/grad_norm: 1.2125667657458816e-11
Step: 22420, train/learning_rate: 2.3322227207245305e-05
Step: 22420, train/epoch: 5.335554599761963
Step: 22430, train/loss: 0.0
Step: 22430, train/grad_norm: 5.587047317845428e-12
Step: 22430, train/learning_rate: 2.3310329197556712e-05
Step: 22430, train/epoch: 5.337934494018555
Step: 22440, train/loss: 0.0
Step: 22440, train/grad_norm: 1.5875455616765066e-08
Step: 22440, train/learning_rate: 2.3298429368878715e-05
Step: 22440, train/epoch: 5.340313911437988
Step: 22450, train/loss: 0.0
Step: 22450, train/grad_norm: 1.8679280344713334e-09
Step: 22450, train/learning_rate: 2.3286529540200718e-05
Step: 22450, train/epoch: 5.34269380569458
Step: 22460, train/loss: 0.0
Step: 22460, train/grad_norm: 8.834587816863859e-08
Step: 22460, train/learning_rate: 2.3274631530512124e-05
Step: 22460, train/epoch: 5.345073699951172
Step: 22470, train/loss: 0.0
Step: 22470, train/grad_norm: 6.816935638198629e-08
Step: 22470, train/learning_rate: 2.3262731701834127e-05
Step: 22470, train/epoch: 5.347453594207764
Step: 22480, train/loss: 0.0
Step: 22480, train/grad_norm: 9.50650047570889e-08
Step: 22480, train/learning_rate: 2.3250833692145534e-05
Step: 22480, train/epoch: 5.3498334884643555
Step: 22490, train/loss: 0.0
Step: 22490, train/grad_norm: 9.131535705364513e-08
Step: 22490, train/learning_rate: 2.3238933863467537e-05
Step: 22490, train/epoch: 5.352213382720947
Step: 22500, train/loss: 0.0
Step: 22500, train/grad_norm: 1.2548166949954975e-08
Step: 22500, train/learning_rate: 2.322703403478954e-05
Step: 22500, train/epoch: 5.354593276977539
Step: 22510, train/loss: 0.0
Step: 22510, train/grad_norm: 2.589614723547129e-06
Step: 22510, train/learning_rate: 2.3215136025100946e-05
Step: 22510, train/epoch: 5.356972694396973
Step: 22520, train/loss: 0.0
Step: 22520, train/grad_norm: 1.0178245746228498e-10
Step: 22520, train/learning_rate: 2.320323619642295e-05
Step: 22520, train/epoch: 5.3593525886535645
Step: 22530, train/loss: 0.0
Step: 22530, train/grad_norm: 3.981154517163077e-09
Step: 22530, train/learning_rate: 2.3191338186734356e-05
Step: 22530, train/epoch: 5.361732482910156
Step: 22540, train/loss: 0.0
Step: 22540, train/grad_norm: 2.1154170326553867e-07
Step: 22540, train/learning_rate: 2.317943835805636e-05
Step: 22540, train/epoch: 5.364112377166748
Step: 22550, train/loss: 0.0
Step: 22550, train/grad_norm: 7.332909279789135e-10
Step: 22550, train/learning_rate: 2.3167538529378362e-05
Step: 22550, train/epoch: 5.36649227142334
Step: 22560, train/loss: 0.0
Step: 22560, train/grad_norm: 3.0500384440301787e-08
Step: 22560, train/learning_rate: 2.315564051968977e-05
Step: 22560, train/epoch: 5.368872165679932
Step: 22570, train/loss: 0.0
Step: 22570, train/grad_norm: 2.6050724954984616e-07
Step: 22570, train/learning_rate: 2.314374069101177e-05
Step: 22570, train/epoch: 5.371251583099365
Step: 22580, train/loss: 0.0
Step: 22580, train/grad_norm: 2.49045628564204e-09
Step: 22580, train/learning_rate: 2.3131842681323178e-05
Step: 22580, train/epoch: 5.373631477355957
Step: 22590, train/loss: 0.0
Step: 22590, train/grad_norm: 9.10794217823252e-11
Step: 22590, train/learning_rate: 2.311994285264518e-05
Step: 22590, train/epoch: 5.376011371612549
Step: 22600, train/loss: 0.0
Step: 22600, train/grad_norm: 2.5563846151044345e-08
Step: 22600, train/learning_rate: 2.3108043023967184e-05
Step: 22600, train/epoch: 5.378391265869141
Step: 22610, train/loss: 0.0
Step: 22610, train/grad_norm: 1.5630904215413466e-07
Step: 22610, train/learning_rate: 2.309614501427859e-05
Step: 22610, train/epoch: 5.380771160125732
Step: 22620, train/loss: 0.0
Step: 22620, train/grad_norm: 3.147215821286409e-10
Step: 22620, train/learning_rate: 2.3084245185600594e-05
Step: 22620, train/epoch: 5.383151054382324
Step: 22630, train/loss: 0.0
Step: 22630, train/grad_norm: 6.638778971002068e-10
Step: 22630, train/learning_rate: 2.3072347175912e-05
Step: 22630, train/epoch: 5.385530471801758
Step: 22640, train/loss: 0.0035000001080334187
Step: 22640, train/grad_norm: 23.625825881958008
Step: 22640, train/learning_rate: 2.3060447347234003e-05
Step: 22640, train/epoch: 5.38791036605835
Step: 22650, train/loss: 0.0
Step: 22650, train/grad_norm: 5.42875411291277e-10
Step: 22650, train/learning_rate: 2.3048547518556006e-05
Step: 22650, train/epoch: 5.390290260314941
Step: 22660, train/loss: 0.0
Step: 22660, train/grad_norm: 6.224919885111646e-11
Step: 22660, train/learning_rate: 2.3036649508867413e-05
Step: 22660, train/epoch: 5.392670154571533
Step: 22670, train/loss: 0.0
Step: 22670, train/grad_norm: 9.457102878585033e-10
Step: 22670, train/learning_rate: 2.3024749680189416e-05
Step: 22670, train/epoch: 5.395050048828125
Step: 22680, train/loss: 0.0
Step: 22680, train/grad_norm: 1.3283136368613668e-08
Step: 22680, train/learning_rate: 2.3012851670500822e-05
Step: 22680, train/epoch: 5.397429943084717
Step: 22690, train/loss: 0.0
Step: 22690, train/grad_norm: 2.4371652784793696e-07
Step: 22690, train/learning_rate: 2.3000951841822825e-05
Step: 22690, train/epoch: 5.399809837341309
Step: 22700, train/loss: 0.0
Step: 22700, train/grad_norm: 1.5078942261492756e-10
Step: 22700, train/learning_rate: 2.298905201314483e-05
Step: 22700, train/epoch: 5.402189254760742
Step: 22710, train/loss: 0.0
Step: 22710, train/grad_norm: 3.0405974484892795e-06
Step: 22710, train/learning_rate: 2.2977154003456235e-05
Step: 22710, train/epoch: 5.404569149017334
Step: 22720, train/loss: 0.0
Step: 22720, train/grad_norm: 3.8099948085346114e-08
Step: 22720, train/learning_rate: 2.2965254174778238e-05
Step: 22720, train/epoch: 5.406949043273926
Step: 22730, train/loss: 0.0
Step: 22730, train/grad_norm: 2.8855211553491245e-07
Step: 22730, train/learning_rate: 2.2953356165089644e-05
Step: 22730, train/epoch: 5.409328937530518
Step: 22740, train/loss: 0.0
Step: 22740, train/grad_norm: 0.00012284507101867348
Step: 22740, train/learning_rate: 2.2941456336411647e-05
Step: 22740, train/epoch: 5.411708831787109
Step: 22750, train/loss: 0.0
Step: 22750, train/grad_norm: 7.148577285676083e-09
Step: 22750, train/learning_rate: 2.292955650773365e-05
Step: 22750, train/epoch: 5.414088726043701
Step: 22760, train/loss: 0.0
Step: 22760, train/grad_norm: 1.7465186630261087e-08
Step: 22760, train/learning_rate: 2.2917658498045057e-05
Step: 22760, train/epoch: 5.416468143463135
Step: 22770, train/loss: 0.010499999858438969
Step: 22770, train/grad_norm: 3.6687721749473212e-09
Step: 22770, train/learning_rate: 2.290575866936706e-05
Step: 22770, train/epoch: 5.418848037719727
Step: 22780, train/loss: 0.0
Step: 22780, train/grad_norm: 1.9389259087176924e-09
Step: 22780, train/learning_rate: 2.2893860659678467e-05
Step: 22780, train/epoch: 5.421227931976318
Step: 22790, train/loss: 0.0
Step: 22790, train/grad_norm: 1.1199564609043478e-09
Step: 22790, train/learning_rate: 2.288196083100047e-05
Step: 22790, train/epoch: 5.42360782623291
Step: 22800, train/loss: 9.999999747378752e-05
Step: 22800, train/grad_norm: 1.6074675812660644e-08
Step: 22800, train/learning_rate: 2.2870061002322473e-05
Step: 22800, train/epoch: 5.425987720489502
Step: 22810, train/loss: 0.0
Step: 22810, train/grad_norm: 2.8158206877293424e-09
Step: 22810, train/learning_rate: 2.285816299263388e-05
Step: 22810, train/epoch: 5.428367614746094
Step: 22820, train/loss: 0.0
Step: 22820, train/grad_norm: 5.920635093303872e-10
Step: 22820, train/learning_rate: 2.2846263163955882e-05
Step: 22820, train/epoch: 5.430747032165527
Step: 22830, train/loss: 0.0
Step: 22830, train/grad_norm: 1.3916543139202986e-05
Step: 22830, train/learning_rate: 2.283436515426729e-05
Step: 22830, train/epoch: 5.433126926422119
Step: 22840, train/loss: 0.0
Step: 22840, train/grad_norm: 5.2168875441793716e-08
Step: 22840, train/learning_rate: 2.2822465325589292e-05
Step: 22840, train/epoch: 5.435506820678711
Step: 22850, train/loss: 0.0
Step: 22850, train/grad_norm: 8.223776326587995e-09
Step: 22850, train/learning_rate: 2.2810565496911295e-05
Step: 22850, train/epoch: 5.437886714935303
Step: 22860, train/loss: 0.0
Step: 22860, train/grad_norm: 1.6177884390344843e-05
Step: 22860, train/learning_rate: 2.27986674872227e-05
Step: 22860, train/epoch: 5.4402666091918945
Step: 22870, train/loss: 0.0
Step: 22870, train/grad_norm: 2.859368124219497e-12
Step: 22870, train/learning_rate: 2.2786767658544704e-05
Step: 22870, train/epoch: 5.442646503448486
Step: 22880, train/loss: 0.0
Step: 22880, train/grad_norm: 4.70155114840054e-08
Step: 22880, train/learning_rate: 2.277486964885611e-05
Step: 22880, train/epoch: 5.445026397705078
Step: 22890, train/loss: 0.0
Step: 22890, train/grad_norm: 6.637842497880797e-10
Step: 22890, train/learning_rate: 2.2762969820178114e-05
Step: 22890, train/epoch: 5.447405815124512
Step: 22900, train/loss: 0.0
Step: 22900, train/grad_norm: 3.982275302405469e-06
Step: 22900, train/learning_rate: 2.275107181048952e-05
Step: 22900, train/epoch: 5.4497857093811035
Step: 22910, train/loss: 0.0
Step: 22910, train/grad_norm: 1.9765185321762857e-10
Step: 22910, train/learning_rate: 2.2739171981811523e-05
Step: 22910, train/epoch: 5.452165603637695
Step: 22920, train/loss: 0.0
Step: 22920, train/grad_norm: 1.208823476872567e-07
Step: 22920, train/learning_rate: 2.2727272153133526e-05
Step: 22920, train/epoch: 5.454545497894287
Step: 22930, train/loss: 0.0
Step: 22930, train/grad_norm: 5.565580774025136e-10
Step: 22930, train/learning_rate: 2.2715374143444933e-05
Step: 22930, train/epoch: 5.456925392150879
Step: 22940, train/loss: 0.0
Step: 22940, train/grad_norm: 3.497372745187022e-05
Step: 22940, train/learning_rate: 2.2703474314766936e-05
Step: 22940, train/epoch: 5.459305286407471
Step: 22950, train/loss: 0.0
Step: 22950, train/grad_norm: 2.207334093284885e-09
Step: 22950, train/learning_rate: 2.2691576305078343e-05
Step: 22950, train/epoch: 5.461684703826904
Step: 22960, train/loss: 0.0
Step: 22960, train/grad_norm: 1.3112983587859617e-08
Step: 22960, train/learning_rate: 2.2679676476400346e-05
Step: 22960, train/epoch: 5.464064598083496
Step: 22970, train/loss: 0.0
Step: 22970, train/grad_norm: 1.7569443122589234e-10
Step: 22970, train/learning_rate: 2.266777664772235e-05
Step: 22970, train/epoch: 5.466444492340088
Step: 22980, train/loss: 0.0
Step: 22980, train/grad_norm: 4.413376331058316e-08
Step: 22980, train/learning_rate: 2.2655878638033755e-05
Step: 22980, train/epoch: 5.46882438659668
Step: 22990, train/loss: 0.0
Step: 22990, train/grad_norm: 3.8513803257700374e-10
Step: 22990, train/learning_rate: 2.2643978809355758e-05
Step: 22990, train/epoch: 5.4712042808532715
Step: 23000, train/loss: 0.0
Step: 23000, train/grad_norm: 3.8425898574168116e-10
Step: 23000, train/learning_rate: 2.2632080799667165e-05
Step: 23000, train/epoch: 5.473584175109863
Step: 23010, train/loss: 0.0
Step: 23010, train/grad_norm: 5.8125050372837e-07
Step: 23010, train/learning_rate: 2.2620180970989168e-05
Step: 23010, train/epoch: 5.475963592529297
Step: 23020, train/loss: 0.0
Step: 23020, train/grad_norm: 3.1351821139224967e-09
Step: 23020, train/learning_rate: 2.260828114231117e-05
Step: 23020, train/epoch: 5.478343486785889
Step: 23030, train/loss: 0.0
Step: 23030, train/grad_norm: 2.9617768859679927e-07
Step: 23030, train/learning_rate: 2.2596383132622577e-05
Step: 23030, train/epoch: 5.4807233810424805
Step: 23040, train/loss: 0.0
Step: 23040, train/grad_norm: 1.1321406923059385e-08
Step: 23040, train/learning_rate: 2.258448330394458e-05
Step: 23040, train/epoch: 5.483103275299072
Step: 23050, train/loss: 0.0
Step: 23050, train/grad_norm: 9.808779850573046e-07
Step: 23050, train/learning_rate: 2.2572585294255987e-05
Step: 23050, train/epoch: 5.485483169555664
Step: 23060, train/loss: 0.0
Step: 23060, train/grad_norm: 6.359277904266492e-08
Step: 23060, train/learning_rate: 2.256068546557799e-05
Step: 23060, train/epoch: 5.487863063812256
Step: 23070, train/loss: 0.0
Step: 23070, train/grad_norm: 9.488927421585913e-10
Step: 23070, train/learning_rate: 2.2548785636899993e-05
Step: 23070, train/epoch: 5.490242958068848
Step: 23080, train/loss: 0.0
Step: 23080, train/grad_norm: 2.3751447475461873e-09
Step: 23080, train/learning_rate: 2.25368876272114e-05
Step: 23080, train/epoch: 5.492622375488281
Step: 23090, train/loss: 0.0
Step: 23090, train/grad_norm: 8.916428484440075e-09
Step: 23090, train/learning_rate: 2.2524987798533402e-05
Step: 23090, train/epoch: 5.495002269744873
Step: 23100, train/loss: 0.0
Step: 23100, train/grad_norm: 1.4328459307932917e-08
Step: 23100, train/learning_rate: 2.251308978884481e-05
Step: 23100, train/epoch: 5.497382164001465
Step: 23110, train/loss: 0.0
Step: 23110, train/grad_norm: 2.904599227804283e-09
Step: 23110, train/learning_rate: 2.2501189960166812e-05
Step: 23110, train/epoch: 5.499762058258057
Step: 23120, train/loss: 0.0
Step: 23120, train/grad_norm: 1.0454847057528305e-11
Step: 23120, train/learning_rate: 2.2489290131488815e-05
Step: 23120, train/epoch: 5.502141952514648
Step: 23130, train/loss: 0.0
Step: 23130, train/grad_norm: 0.0017946319421753287
Step: 23130, train/learning_rate: 2.247739212180022e-05
Step: 23130, train/epoch: 5.50452184677124
Step: 23140, train/loss: 0.0
Step: 23140, train/grad_norm: 9.784745236274262e-10
Step: 23140, train/learning_rate: 2.2465492293122225e-05
Step: 23140, train/epoch: 5.506901264190674
Step: 23150, train/loss: 0.0
Step: 23150, train/grad_norm: 8.026579401132494e-09
Step: 23150, train/learning_rate: 2.245359428343363e-05
Step: 23150, train/epoch: 5.509281158447266
Step: 23160, train/loss: 0.0
Step: 23160, train/grad_norm: 6.322556367344134e-10
Step: 23160, train/learning_rate: 2.2441694454755634e-05
Step: 23160, train/epoch: 5.511661052703857
Step: 23170, train/loss: 0.0
Step: 23170, train/grad_norm: 1.933415205712663e-09
Step: 23170, train/learning_rate: 2.2429794626077637e-05
Step: 23170, train/epoch: 5.514040946960449
Step: 23180, train/loss: 0.0
Step: 23180, train/grad_norm: 2.4516873864399713e-08
Step: 23180, train/learning_rate: 2.2417896616389044e-05
Step: 23180, train/epoch: 5.516420841217041
Step: 23190, train/loss: 0.0
Step: 23190, train/grad_norm: 2.3051225070958026e-06
Step: 23190, train/learning_rate: 2.2405996787711047e-05
Step: 23190, train/epoch: 5.518800735473633
Step: 23200, train/loss: 0.0
Step: 23200, train/grad_norm: 1.59108939357111e-08
Step: 23200, train/learning_rate: 2.2394098778022453e-05
Step: 23200, train/epoch: 5.521180152893066
Step: 23210, train/loss: 0.0
Step: 23210, train/grad_norm: 1.0229670444061867e-08
Step: 23210, train/learning_rate: 2.2382198949344456e-05
Step: 23210, train/epoch: 5.523560047149658
Step: 23220, train/loss: 0.0
Step: 23220, train/grad_norm: 4.934148023494345e-08
Step: 23220, train/learning_rate: 2.237029912066646e-05
Step: 23220, train/epoch: 5.52593994140625
Step: 23230, train/loss: 0.0
Step: 23230, train/grad_norm: 4.139912804929091e-12
Step: 23230, train/learning_rate: 2.2358401110977866e-05
Step: 23230, train/epoch: 5.528319835662842
Step: 23240, train/loss: 0.0
Step: 23240, train/grad_norm: 8.456951405833024e-08
Step: 23240, train/learning_rate: 2.234650128229987e-05
Step: 23240, train/epoch: 5.530699729919434
Step: 23250, train/loss: 0.0
Step: 23250, train/grad_norm: 1.3703705947776257e-09
Step: 23250, train/learning_rate: 2.2334603272611275e-05
Step: 23250, train/epoch: 5.533079624176025
Step: 23260, train/loss: 0.0
Step: 23260, train/grad_norm: 1.3264051190731152e-08
Step: 23260, train/learning_rate: 2.232270344393328e-05
Step: 23260, train/epoch: 5.535459518432617
Step: 23270, train/loss: 0.0
Step: 23270, train/grad_norm: 2.0014633150822192e-07
Step: 23270, train/learning_rate: 2.231080361525528e-05
Step: 23270, train/epoch: 5.537838935852051
Step: 23280, train/loss: 0.0
Step: 23280, train/grad_norm: 7.285256842237686e-10
Step: 23280, train/learning_rate: 2.2298905605566688e-05
Step: 23280, train/epoch: 5.540218830108643
Step: 23290, train/loss: 0.0
Step: 23290, train/grad_norm: 4.4283072497819376e-07
Step: 23290, train/learning_rate: 2.228700577688869e-05
Step: 23290, train/epoch: 5.542598724365234
Step: 23300, train/loss: 0.0
Step: 23300, train/grad_norm: 4.6666865927136314e-08
Step: 23300, train/learning_rate: 2.2275107767200097e-05
Step: 23300, train/epoch: 5.544978618621826
Step: 23310, train/loss: 0.0
Step: 23310, train/grad_norm: 1.2070925592411186e-08
Step: 23310, train/learning_rate: 2.22632079385221e-05
Step: 23310, train/epoch: 5.547358512878418
Step: 23320, train/loss: 0.0
Step: 23320, train/grad_norm: 2.665118259415067e-08
Step: 23320, train/learning_rate: 2.2251308109844103e-05
Step: 23320, train/epoch: 5.54973840713501
Step: 23330, train/loss: 0.0
Step: 23330, train/grad_norm: 1.1584051773372295e-10
Step: 23330, train/learning_rate: 2.223941010015551e-05
Step: 23330, train/epoch: 5.552117824554443
Step: 23340, train/loss: 0.0
Step: 23340, train/grad_norm: 1.2423599926592033e-09
Step: 23340, train/learning_rate: 2.2227510271477513e-05
Step: 23340, train/epoch: 5.554497718811035
Step: 23350, train/loss: 0.0
Step: 23350, train/grad_norm: 1.087299805746511e-09
Step: 23350, train/learning_rate: 2.221561226178892e-05
Step: 23350, train/epoch: 5.556877613067627
Step: 23360, train/loss: 0.0
Step: 23360, train/grad_norm: 4.9105279842365235e-09
Step: 23360, train/learning_rate: 2.2203712433110923e-05
Step: 23360, train/epoch: 5.559257507324219
Step: 23370, train/loss: 0.0
Step: 23370, train/grad_norm: 1.0886099799378712e-09
Step: 23370, train/learning_rate: 2.2191812604432926e-05
Step: 23370, train/epoch: 5.5616374015808105
Step: 23380, train/loss: 0.0
Step: 23380, train/grad_norm: 3.970415551890483e-09
Step: 23380, train/learning_rate: 2.2179914594744332e-05
Step: 23380, train/epoch: 5.564017295837402
Step: 23390, train/loss: 0.0
Step: 23390, train/grad_norm: 7.145744412850874e-11
Step: 23390, train/learning_rate: 2.2168014766066335e-05
Step: 23390, train/epoch: 5.566397190093994
Step: 23400, train/loss: 0.0
Step: 23400, train/grad_norm: 1.4988678076477413e-09
Step: 23400, train/learning_rate: 2.2156116756377742e-05
Step: 23400, train/epoch: 5.568776607513428
Step: 23410, train/loss: 0.0
Step: 23410, train/grad_norm: 2.645016081714857e-07
Step: 23410, train/learning_rate: 2.2144216927699745e-05
Step: 23410, train/epoch: 5.5711565017700195
Step: 23420, train/loss: 0.0
Step: 23420, train/grad_norm: 1.2199297572124124e-08
Step: 23420, train/learning_rate: 2.2132317099021748e-05
Step: 23420, train/epoch: 5.573536396026611
Step: 23430, train/loss: 0.0
Step: 23430, train/grad_norm: 2.89279624560157e-11
Step: 23430, train/learning_rate: 2.2120419089333154e-05
Step: 23430, train/epoch: 5.575916290283203
Step: 23440, train/loss: 0.0
Step: 23440, train/grad_norm: 1.740635857672146e-09
Step: 23440, train/learning_rate: 2.2108519260655157e-05
Step: 23440, train/epoch: 5.578296184539795
Step: 23450, train/loss: 0.0
Step: 23450, train/grad_norm: 2.3889430364465447e-11
Step: 23450, train/learning_rate: 2.2096621250966564e-05
Step: 23450, train/epoch: 5.580676078796387
Step: 23460, train/loss: 0.0
Step: 23460, train/grad_norm: 2.4201533506129635e-06
Step: 23460, train/learning_rate: 2.2084721422288567e-05
Step: 23460, train/epoch: 5.58305549621582
Step: 23470, train/loss: 0.0
Step: 23470, train/grad_norm: 4.5956762839693965e-09
Step: 23470, train/learning_rate: 2.207282159361057e-05
Step: 23470, train/epoch: 5.585435390472412
Step: 23480, train/loss: 0.0
Step: 23480, train/grad_norm: 1.216546388604911e-05
Step: 23480, train/learning_rate: 2.2060923583921976e-05
Step: 23480, train/epoch: 5.587815284729004
Step: 23490, train/loss: 0.0
Step: 23490, train/grad_norm: 5.556240125770273e-07
Step: 23490, train/learning_rate: 2.204902375524398e-05
Step: 23490, train/epoch: 5.590195178985596
Step: 23500, train/loss: 0.0
Step: 23500, train/grad_norm: 7.769222765130479e-11
Step: 23500, train/learning_rate: 2.2037125745555386e-05
Step: 23500, train/epoch: 5.5925750732421875
Step: 23510, train/loss: 0.0
Step: 23510, train/grad_norm: 6.530109231128733e-10
Step: 23510, train/learning_rate: 2.202522591687739e-05
Step: 23510, train/epoch: 5.594954967498779
Step: 23520, train/loss: 0.0
Step: 23520, train/grad_norm: 1.5551318566231487e-10
Step: 23520, train/learning_rate: 2.2013326088199392e-05
Step: 23520, train/epoch: 5.597334384918213
Step: 23530, train/loss: 0.0
Step: 23530, train/grad_norm: 9.433710701500786e-09
Step: 23530, train/learning_rate: 2.20014280785108e-05
Step: 23530, train/epoch: 5.599714279174805
Step: 23540, train/loss: 0.0
Step: 23540, train/grad_norm: 9.991123306463123e-07
Step: 23540, train/learning_rate: 2.19895282498328e-05
Step: 23540, train/epoch: 5.6020941734313965
Step: 23550, train/loss: 0.0
Step: 23550, train/grad_norm: 3.887797639379187e-09
Step: 23550, train/learning_rate: 2.1977630240144208e-05
Step: 23550, train/epoch: 5.604474067687988
Step: 23560, train/loss: 0.00019999999494757503
Step: 23560, train/grad_norm: 4.714929141558244e-11
Step: 23560, train/learning_rate: 2.196573041146621e-05
Step: 23560, train/epoch: 5.60685396194458
Step: 23570, train/loss: 0.0
Step: 23570, train/grad_norm: 7.047455952147175e-10
Step: 23570, train/learning_rate: 2.1953832401777618e-05
Step: 23570, train/epoch: 5.609233856201172
Step: 23580, train/loss: 0.0
Step: 23580, train/grad_norm: 4.542959786135725e-09
Step: 23580, train/learning_rate: 2.194193257309962e-05
Step: 23580, train/epoch: 5.611613750457764
Step: 23590, train/loss: 0.0
Step: 23590, train/grad_norm: 6.041257446555426e-13
Step: 23590, train/learning_rate: 2.1930032744421624e-05
Step: 23590, train/epoch: 5.613993167877197
Step: 23600, train/loss: 0.0
Step: 23600, train/grad_norm: 8.939796680706991e-10
Step: 23600, train/learning_rate: 2.191813473473303e-05
Step: 23600, train/epoch: 5.616373062133789
Step: 23610, train/loss: 0.0
Step: 23610, train/grad_norm: 1.6011252382524405e-13
Step: 23610, train/learning_rate: 2.1906234906055033e-05
Step: 23610, train/epoch: 5.618752956390381
Step: 23620, train/loss: 0.2062000036239624
Step: 23620, train/grad_norm: 2.0368164044270998e-08
Step: 23620, train/learning_rate: 2.189433689636644e-05
Step: 23620, train/epoch: 5.621132850646973
Step: 23630, train/loss: 0.0
Step: 23630, train/grad_norm: 1.1639341579439133e-07
Step: 23630, train/learning_rate: 2.1882437067688443e-05
Step: 23630, train/epoch: 5.6235127449035645
Step: 23640, train/loss: 0.0
Step: 23640, train/grad_norm: 2.4601336008345243e-06
Step: 23640, train/learning_rate: 2.1870537239010446e-05
Step: 23640, train/epoch: 5.625892639160156
Step: 23650, train/loss: 0.0
Step: 23650, train/grad_norm: 6.156499615661559e-11
Step: 23650, train/learning_rate: 2.1858639229321852e-05
Step: 23650, train/epoch: 5.62827205657959
Step: 23660, train/loss: 0.0
Step: 23660, train/grad_norm: 8.291896946843735e-09
Step: 23660, train/learning_rate: 2.1846739400643855e-05
Step: 23660, train/epoch: 5.630651950836182
Step: 23670, train/loss: 0.0
Step: 23670, train/grad_norm: 1.9797918512920454e-11
Step: 23670, train/learning_rate: 2.1834841390955262e-05
Step: 23670, train/epoch: 5.633031845092773
Step: 23680, train/loss: 0.010200000368058681
Step: 23680, train/grad_norm: 9.341895257364285e-09
Step: 23680, train/learning_rate: 2.1822941562277265e-05
Step: 23680, train/epoch: 5.635411739349365
Step: 23690, train/loss: 0.0
Step: 23690, train/grad_norm: 4.0246619370520875e-09
Step: 23690, train/learning_rate: 2.1811041733599268e-05
Step: 23690, train/epoch: 5.637791633605957
Step: 23700, train/loss: 0.0
Step: 23700, train/grad_norm: 2.8888078595912248e-09
Step: 23700, train/learning_rate: 2.1799143723910674e-05
Step: 23700, train/epoch: 5.640171527862549
Step: 23710, train/loss: 0.0
Step: 23710, train/grad_norm: 9.141031265258448e-10
Step: 23710, train/learning_rate: 2.1787243895232677e-05
Step: 23710, train/epoch: 5.642550945281982
Step: 23720, train/loss: 0.0
Step: 23720, train/grad_norm: 3.8261113721738127e-10
Step: 23720, train/learning_rate: 2.1775345885544084e-05
Step: 23720, train/epoch: 5.644930839538574
Step: 23730, train/loss: 0.0
Step: 23730, train/grad_norm: 1.413877548372966e-08
Step: 23730, train/learning_rate: 2.1763446056866087e-05
Step: 23730, train/epoch: 5.647310733795166
Step: 23740, train/loss: 0.0
Step: 23740, train/grad_norm: 1.0888967678923667e-12
Step: 23740, train/learning_rate: 2.175154622818809e-05
Step: 23740, train/epoch: 5.649690628051758
Step: 23750, train/loss: 0.0
Step: 23750, train/grad_norm: 7.825575210063107e-08
Step: 23750, train/learning_rate: 2.1739648218499497e-05
Step: 23750, train/epoch: 5.65207052230835
Step: 23760, train/loss: 0.0
Step: 23760, train/grad_norm: 1.260908322198162e-10
Step: 23760, train/learning_rate: 2.17277483898215e-05
Step: 23760, train/epoch: 5.654450416564941
Step: 23770, train/loss: 0.0
Step: 23770, train/grad_norm: 6.311663969249537e-10
Step: 23770, train/learning_rate: 2.1715850380132906e-05
Step: 23770, train/epoch: 5.656830310821533
Step: 23780, train/loss: 0.0
Step: 23780, train/grad_norm: 3.869131681710769e-10
Step: 23780, train/learning_rate: 2.170395055145491e-05
Step: 23780, train/epoch: 5.659209728240967
Step: 23790, train/loss: 0.0
Step: 23790, train/grad_norm: 4.709240329248132e-06
Step: 23790, train/learning_rate: 2.1692050722776912e-05
Step: 23790, train/epoch: 5.661589622497559
Step: 23800, train/loss: 0.0
Step: 23800, train/grad_norm: 2.9607785667629116e-10
Step: 23800, train/learning_rate: 2.168015271308832e-05
Step: 23800, train/epoch: 5.66396951675415
Step: 23810, train/loss: 0.0
Step: 23810, train/grad_norm: 0.003732764394953847
Step: 23810, train/learning_rate: 2.1668252884410322e-05
Step: 23810, train/epoch: 5.666349411010742
Step: 23820, train/loss: 0.0
Step: 23820, train/grad_norm: 2.8726518386257283e-10
Step: 23820, train/learning_rate: 2.1656354874721728e-05
Step: 23820, train/epoch: 5.668729305267334
Step: 23830, train/loss: 0.0
Step: 23830, train/grad_norm: 3.3742498573019475e-08
Step: 23830, train/learning_rate: 2.164445504604373e-05
Step: 23830, train/epoch: 5.671109199523926
Step: 23840, train/loss: 0.0
Step: 23840, train/grad_norm: 9.530353069220343e-12
Step: 23840, train/learning_rate: 2.1632555217365734e-05
Step: 23840, train/epoch: 5.673488616943359
Step: 23850, train/loss: 0.0
Step: 23850, train/grad_norm: 1.2529398629723687e-09
Step: 23850, train/learning_rate: 2.162065720767714e-05
Step: 23850, train/epoch: 5.675868511199951
Step: 23860, train/loss: 0.0
Step: 23860, train/grad_norm: 2.8603025503670665e-10
Step: 23860, train/learning_rate: 2.1608757378999144e-05
Step: 23860, train/epoch: 5.678248405456543
Step: 23870, train/loss: 0.0
Step: 23870, train/grad_norm: 8.498297066061866e-10
Step: 23870, train/learning_rate: 2.159685936931055e-05
Step: 23870, train/epoch: 5.680628299713135
Step: 23880, train/loss: 0.0
Step: 23880, train/grad_norm: 3.8173204180980136e-11
Step: 23880, train/learning_rate: 2.1584959540632553e-05
Step: 23880, train/epoch: 5.683008193969727
Step: 23890, train/loss: 0.0
Step: 23890, train/grad_norm: 1.4752372934356828e-10
Step: 23890, train/learning_rate: 2.1573059711954556e-05
Step: 23890, train/epoch: 5.685388088226318
Step: 23900, train/loss: 0.0
Step: 23900, train/grad_norm: 4.5994585917696895e-08
Step: 23900, train/learning_rate: 2.1561161702265963e-05
Step: 23900, train/epoch: 5.687767505645752
Step: 23910, train/loss: 0.0
Step: 23910, train/grad_norm: 1.534026949912004e-07
Step: 23910, train/learning_rate: 2.1549261873587966e-05
Step: 23910, train/epoch: 5.690147399902344
Step: 23920, train/loss: 0.0
Step: 23920, train/grad_norm: 1.8930838840969244e-10
Step: 23920, train/learning_rate: 2.1537363863899373e-05
Step: 23920, train/epoch: 5.6925272941589355
Step: 23930, train/loss: 0.0
Step: 23930, train/grad_norm: 3.8525987955395635e-11
Step: 23930, train/learning_rate: 2.1525464035221376e-05
Step: 23930, train/epoch: 5.694907188415527
Step: 23940, train/loss: 0.0
Step: 23940, train/grad_norm: 3.3844011682049313e-07
Step: 23940, train/learning_rate: 2.151356420654338e-05
Step: 23940, train/epoch: 5.697287082672119
Step: 23950, train/loss: 0.0
Step: 23950, train/grad_norm: 1.2075523159182922e-07
Step: 23950, train/learning_rate: 2.1501666196854785e-05
Step: 23950, train/epoch: 5.699666976928711
Step: 23960, train/loss: 0.0
Step: 23960, train/grad_norm: 1.657499240081961e-07
Step: 23960, train/learning_rate: 2.1489766368176788e-05
Step: 23960, train/epoch: 5.702046871185303
Step: 23970, train/loss: 0.0
Step: 23970, train/grad_norm: 5.456237683887366e-10
Step: 23970, train/learning_rate: 2.1477868358488195e-05
Step: 23970, train/epoch: 5.704426288604736
Step: 23980, train/loss: 0.0
Step: 23980, train/grad_norm: 1.8212261920780293e-08
Step: 23980, train/learning_rate: 2.1465968529810198e-05
Step: 23980, train/epoch: 5.706806182861328
Step: 23990, train/loss: 0.0
Step: 23990, train/grad_norm: 1.5262806574156684e-09
Step: 23990, train/learning_rate: 2.14540687011322e-05
Step: 23990, train/epoch: 5.70918607711792
Step: 24000, train/loss: 0.0
Step: 24000, train/grad_norm: 7.908880093054904e-07
Step: 24000, train/learning_rate: 2.1442170691443607e-05
Step: 24000, train/epoch: 5.711565971374512
Step: 24010, train/loss: 0.0
Step: 24010, train/grad_norm: 2.372018137464238e-08
Step: 24010, train/learning_rate: 2.143027086276561e-05
Step: 24010, train/epoch: 5.7139458656311035
Step: 24020, train/loss: 0.0
Step: 24020, train/grad_norm: 1.2940544991124625e-07
Step: 24020, train/learning_rate: 2.1418372853077017e-05
Step: 24020, train/epoch: 5.716325759887695
Step: 24030, train/loss: 0.0
Step: 24030, train/grad_norm: 1.6579532191585145e-10
Step: 24030, train/learning_rate: 2.140647302439902e-05
Step: 24030, train/epoch: 5.718705177307129
Step: 24040, train/loss: 0.0
Step: 24040, train/grad_norm: 6.826472542798001e-08
Step: 24040, train/learning_rate: 2.1394573195721023e-05
Step: 24040, train/epoch: 5.721085071563721
Step: 24050, train/loss: 0.0
Step: 24050, train/grad_norm: 5.328478991373231e-09
Step: 24050, train/learning_rate: 2.138267518603243e-05
Step: 24050, train/epoch: 5.7234649658203125
Step: 24060, train/loss: 0.0
Step: 24060, train/grad_norm: 6.2902167918821306e-09
Step: 24060, train/learning_rate: 2.1370775357354432e-05
Step: 24060, train/epoch: 5.725844860076904
Step: 24070, train/loss: 0.0
Step: 24070, train/grad_norm: 4.711544931979006e-09
Step: 24070, train/learning_rate: 2.135887734766584e-05
Step: 24070, train/epoch: 5.728224754333496
Step: 24080, train/loss: 0.0
Step: 24080, train/grad_norm: 2.097622520125242e-09
Step: 24080, train/learning_rate: 2.1346977518987842e-05
Step: 24080, train/epoch: 5.730604648590088
Step: 24090, train/loss: 0.0
Step: 24090, train/grad_norm: 3.1302418435075197e-09
Step: 24090, train/learning_rate: 2.1335077690309845e-05
Step: 24090, train/epoch: 5.7329840660095215
Step: 24100, train/loss: 0.0
Step: 24100, train/grad_norm: 2.119645292619765e-10
Step: 24100, train/learning_rate: 2.132317968062125e-05
Step: 24100, train/epoch: 5.735363960266113
Step: 24110, train/loss: 0.0
Step: 24110, train/grad_norm: 1.0294513505537672e-11
Step: 24110, train/learning_rate: 2.1311279851943254e-05
Step: 24110, train/epoch: 5.737743854522705
Step: 24120, train/loss: 0.0
Step: 24120, train/grad_norm: 1.2565547213849726e-10
Step: 24120, train/learning_rate: 2.129938184225466e-05
Step: 24120, train/epoch: 5.740123748779297
Step: 24130, train/loss: 0.0
Step: 24130, train/grad_norm: 6.209315728256115e-09
Step: 24130, train/learning_rate: 2.1287482013576664e-05
Step: 24130, train/epoch: 5.742503643035889
Step: 24140, train/loss: 0.0
Step: 24140, train/grad_norm: 3.761002065338914e-10
Step: 24140, train/learning_rate: 2.1275582184898667e-05
Step: 24140, train/epoch: 5.7448835372924805
Step: 24150, train/loss: 0.0
Step: 24150, train/grad_norm: 2.859043668479444e-09
Step: 24150, train/learning_rate: 2.1263684175210074e-05
Step: 24150, train/epoch: 5.747263431549072
Step: 24160, train/loss: 0.0
Step: 24160, train/grad_norm: 1.9641589743546461e-10
Step: 24160, train/learning_rate: 2.1251784346532077e-05
Step: 24160, train/epoch: 5.749642848968506
Step: 24170, train/loss: 0.0
Step: 24170, train/grad_norm: 1.3181802871464754e-10
Step: 24170, train/learning_rate: 2.1239886336843483e-05
Step: 24170, train/epoch: 5.752022743225098
Step: 24180, train/loss: 0.0
Step: 24180, train/grad_norm: 9.552187166250725e-12
Step: 24180, train/learning_rate: 2.1227986508165486e-05
Step: 24180, train/epoch: 5.7544026374816895
Step: 24190, train/loss: 0.0
Step: 24190, train/grad_norm: 1.648012629140716e-11
Step: 24190, train/learning_rate: 2.121608667948749e-05
Step: 24190, train/epoch: 5.756782531738281
Step: 24200, train/loss: 0.0
Step: 24200, train/grad_norm: 5.06773659494919e-13
Step: 24200, train/learning_rate: 2.1204188669798896e-05
Step: 24200, train/epoch: 5.759162425994873
Step: 24210, train/loss: 0.0
Step: 24210, train/grad_norm: 7.163650922459297e-11
Step: 24210, train/learning_rate: 2.11922888411209e-05
Step: 24210, train/epoch: 5.761542320251465
Step: 24220, train/loss: 0.0
Step: 24220, train/grad_norm: 4.9067456764362305e-09
Step: 24220, train/learning_rate: 2.1180390831432305e-05
Step: 24220, train/epoch: 5.763921737670898
Step: 24230, train/loss: 0.0
Step: 24230, train/grad_norm: 7.03067515317457e-09
Step: 24230, train/learning_rate: 2.1168491002754308e-05
Step: 24230, train/epoch: 5.76630163192749
Step: 24240, train/loss: 0.0
Step: 24240, train/grad_norm: 1.828757700828021e-10
Step: 24240, train/learning_rate: 2.1156592993065715e-05
Step: 24240, train/epoch: 5.768681526184082
Step: 24250, train/loss: 0.0
Step: 24250, train/grad_norm: 5.2148177331901024e-08
Step: 24250, train/learning_rate: 2.1144693164387718e-05
Step: 24250, train/epoch: 5.771061420440674
Step: 24260, train/loss: 0.0
Step: 24260, train/grad_norm: 3.702946803352258e-12
Step: 24260, train/learning_rate: 2.113279333570972e-05
Step: 24260, train/epoch: 5.773441314697266
Step: 24270, train/loss: 0.0
Step: 24270, train/grad_norm: 4.675508868956513e-10
Step: 24270, train/learning_rate: 2.1120895326021127e-05
Step: 24270, train/epoch: 5.775821208953857
Step: 24280, train/loss: 0.0
Step: 24280, train/grad_norm: 6.573607769233547e-10
Step: 24280, train/learning_rate: 2.110899549734313e-05
Step: 24280, train/epoch: 5.778200626373291
Step: 24290, train/loss: 0.0
Step: 24290, train/grad_norm: 2.6103839445568155e-06
Step: 24290, train/learning_rate: 2.1097097487654537e-05
Step: 24290, train/epoch: 5.780580520629883
Step: 24300, train/loss: 0.0
Step: 24300, train/grad_norm: 1.1075007230743061e-11
Step: 24300, train/learning_rate: 2.108519765897654e-05
Step: 24300, train/epoch: 5.782960414886475
Step: 24310, train/loss: 0.0
Step: 24310, train/grad_norm: 8.375648091751842e-11
Step: 24310, train/learning_rate: 2.1073297830298543e-05
Step: 24310, train/epoch: 5.785340309143066
Step: 24320, train/loss: 0.0
Step: 24320, train/grad_norm: 1.0834273478366185e-09
Step: 24320, train/learning_rate: 2.106139982060995e-05
Step: 24320, train/epoch: 5.787720203399658
Step: 24330, train/loss: 0.0
Step: 24330, train/grad_norm: 2.1819349058205262e-05
Step: 24330, train/learning_rate: 2.1049499991931953e-05
Step: 24330, train/epoch: 5.79010009765625
Step: 24340, train/loss: 0.0
Step: 24340, train/grad_norm: 2.1794571125610673e-07
Step: 24340, train/learning_rate: 2.103760198224336e-05
Step: 24340, train/epoch: 5.792479991912842
Step: 24350, train/loss: 0.0
Step: 24350, train/grad_norm: 5.792563304019005e-11
Step: 24350, train/learning_rate: 2.1025702153565362e-05
Step: 24350, train/epoch: 5.794859409332275
Step: 24360, train/loss: 0.0
Step: 24360, train/grad_norm: 4.1756784141533387e-10
Step: 24360, train/learning_rate: 2.1013802324887365e-05
Step: 24360, train/epoch: 5.797239303588867
Step: 24370, train/loss: 0.0
Step: 24370, train/grad_norm: 3.27098206298615e-08
Step: 24370, train/learning_rate: 2.100190431519877e-05
Step: 24370, train/epoch: 5.799619197845459
Step: 24380, train/loss: 0.0
Step: 24380, train/grad_norm: 1.355342860165365e-08
Step: 24380, train/learning_rate: 2.0990004486520775e-05
Step: 24380, train/epoch: 5.801999092102051
Step: 24390, train/loss: 0.0
Step: 24390, train/grad_norm: 2.855891521533205e-12
Step: 24390, train/learning_rate: 2.097810647683218e-05
Step: 24390, train/epoch: 5.804378986358643
Step: 24400, train/loss: 0.0
Step: 24400, train/grad_norm: 1.888427725305064e-08
Step: 24400, train/learning_rate: 2.0966206648154184e-05
Step: 24400, train/epoch: 5.806758880615234
Step: 24410, train/loss: 0.0
Step: 24410, train/grad_norm: 4.318924124585566e-12
Step: 24410, train/learning_rate: 2.0954306819476187e-05
Step: 24410, train/epoch: 5.809138298034668
Step: 24420, train/loss: 0.0
Step: 24420, train/grad_norm: 4.607952406687592e-15
Step: 24420, train/learning_rate: 2.0942408809787594e-05
Step: 24420, train/epoch: 5.81151819229126
Step: 24430, train/loss: 0.0
Step: 24430, train/grad_norm: 8.674509444084322e-10
Step: 24430, train/learning_rate: 2.0930508981109597e-05
Step: 24430, train/epoch: 5.813898086547852
Step: 24440, train/loss: 0.0
Step: 24440, train/grad_norm: 9.223941055402918e-10
Step: 24440, train/learning_rate: 2.0918610971421003e-05
Step: 24440, train/epoch: 5.816277980804443
Step: 24450, train/loss: 0.0
Step: 24450, train/grad_norm: 6.647222355882221e-11
Step: 24450, train/learning_rate: 2.0906711142743006e-05
Step: 24450, train/epoch: 5.818657875061035
Step: 24460, train/loss: 0.0
Step: 24460, train/grad_norm: 1.7452228107117662e-08
Step: 24460, train/learning_rate: 2.089481131406501e-05
Step: 24460, train/epoch: 5.821037769317627
Step: 24470, train/loss: 0.0
Step: 24470, train/grad_norm: 2.594809783706964e-12
Step: 24470, train/learning_rate: 2.0882913304376416e-05
Step: 24470, train/epoch: 5.8234171867370605
Step: 24480, train/loss: 0.0
Step: 24480, train/grad_norm: 2.774754037648819e-11
Step: 24480, train/learning_rate: 2.087101347569842e-05
Step: 24480, train/epoch: 5.825797080993652
Step: 24490, train/loss: 0.0
Step: 24490, train/grad_norm: 2.839049557223916e-05
Step: 24490, train/learning_rate: 2.0859115466009825e-05
Step: 24490, train/epoch: 5.828176975250244
Step: 24500, train/loss: 0.0
Step: 24500, train/grad_norm: 9.922835886300163e-09
Step: 24500, train/learning_rate: 2.084721563733183e-05
Step: 24500, train/epoch: 5.830556869506836
Step: 24510, train/loss: 0.0
Step: 24510, train/grad_norm: 2.4995380765169273e-10
Step: 24510, train/learning_rate: 2.083531580865383e-05
Step: 24510, train/epoch: 5.832936763763428
Step: 24520, train/loss: 0.0
Step: 24520, train/grad_norm: 9.99813654090076e-09
Step: 24520, train/learning_rate: 2.0823417798965238e-05
Step: 24520, train/epoch: 5.8353166580200195
Step: 24530, train/loss: 0.0
Step: 24530, train/grad_norm: 1.4165761445805725e-11
Step: 24530, train/learning_rate: 2.081151797028724e-05
Step: 24530, train/epoch: 5.837696552276611
Step: 24540, train/loss: 0.11720000207424164
Step: 24540, train/grad_norm: 435.0924377441406
Step: 24540, train/learning_rate: 2.0799619960598648e-05
Step: 24540, train/epoch: 5.840075969696045
Step: 24550, train/loss: 0.0
Step: 24550, train/grad_norm: 2.6363132163420566e-11
Step: 24550, train/learning_rate: 2.078772013192065e-05
Step: 24550, train/epoch: 5.842455863952637
Step: 24560, train/loss: 0.0
Step: 24560, train/grad_norm: 1.3815761690239015e-07
Step: 24560, train/learning_rate: 2.0775820303242654e-05
Step: 24560, train/epoch: 5.8448357582092285
Step: 24570, train/loss: 0.24379999935626984
Step: 24570, train/grad_norm: 1.523823733862173e-09
Step: 24570, train/learning_rate: 2.076392229355406e-05
Step: 24570, train/epoch: 5.84721565246582
Step: 24580, train/loss: 0.0
Step: 24580, train/grad_norm: 4.396118583827047e-06
Step: 24580, train/learning_rate: 2.0752022464876063e-05
Step: 24580, train/epoch: 5.849595546722412
Step: 24590, train/loss: 0.0
Step: 24590, train/grad_norm: 0.014132329262793064
Step: 24590, train/learning_rate: 2.074012445518747e-05
Step: 24590, train/epoch: 5.851975440979004
Step: 24600, train/loss: 0.0
Step: 24600, train/grad_norm: 2.691199142645928e-06
Step: 24600, train/learning_rate: 2.0728224626509473e-05
Step: 24600, train/epoch: 5.8543548583984375
Step: 24610, train/loss: 0.0
Step: 24610, train/grad_norm: 1.9986759070889093e-05
Step: 24610, train/learning_rate: 2.0716324797831476e-05
Step: 24610, train/epoch: 5.856734752655029
Step: 24620, train/loss: 0.0026000000070780516
Step: 24620, train/grad_norm: 3.4125716865673894e-06
Step: 24620, train/learning_rate: 2.0704426788142882e-05
Step: 24620, train/epoch: 5.859114646911621
Step: 24630, train/loss: 0.0
Step: 24630, train/grad_norm: 0.00014761458442080766
Step: 24630, train/learning_rate: 2.0692526959464885e-05
Step: 24630, train/epoch: 5.861494541168213
Step: 24640, train/loss: 0.18129999935626984
Step: 24640, train/grad_norm: 2.0863002646365203e-05
Step: 24640, train/learning_rate: 2.0680628949776292e-05
Step: 24640, train/epoch: 5.863874435424805
Step: 24650, train/loss: 0.0
Step: 24650, train/grad_norm: 2.1390746951510664e-08
Step: 24650, train/learning_rate: 2.0668729121098295e-05
Step: 24650, train/epoch: 5.8662543296813965
Step: 24660, train/loss: 0.0
Step: 24660, train/grad_norm: 0.021102214232087135
Step: 24660, train/learning_rate: 2.0656829292420298e-05
Step: 24660, train/epoch: 5.86863374710083
Step: 24670, train/loss: 0.0
Step: 24670, train/grad_norm: 0.00014208315405994654
Step: 24670, train/learning_rate: 2.0644931282731704e-05
Step: 24670, train/epoch: 5.871013641357422
Step: 24680, train/loss: 0.0
Step: 24680, train/grad_norm: 0.00010301285510649905
Step: 24680, train/learning_rate: 2.0633031454053707e-05
Step: 24680, train/epoch: 5.873393535614014
Step: 24690, train/loss: 0.0
Step: 24690, train/grad_norm: 8.437618816969916e-06
Step: 24690, train/learning_rate: 2.0621133444365114e-05
Step: 24690, train/epoch: 5.8757734298706055
Step: 24700, train/loss: 0.0
Step: 24700, train/grad_norm: 0.0005287223611958325
Step: 24700, train/learning_rate: 2.0609233615687117e-05
Step: 24700, train/epoch: 5.878153324127197
Step: 24710, train/loss: 0.0
Step: 24710, train/grad_norm: 2.806760903695249e-07
Step: 24710, train/learning_rate: 2.059733378700912e-05
Step: 24710, train/epoch: 5.880533218383789
Step: 24720, train/loss: 0.0
Step: 24720, train/grad_norm: 3.41526714464635e-07
Step: 24720, train/learning_rate: 2.0585435777320527e-05
Step: 24720, train/epoch: 5.882913112640381
Step: 24730, train/loss: 0.0
Step: 24730, train/grad_norm: 5.021922220294073e-07
Step: 24730, train/learning_rate: 2.057353594864253e-05
Step: 24730, train/epoch: 5.8852925300598145
Step: 24740, train/loss: 0.0
Step: 24740, train/grad_norm: 3.670593741844641e-06
Step: 24740, train/learning_rate: 2.0561637938953936e-05
Step: 24740, train/epoch: 5.887672424316406
Step: 24750, train/loss: 0.0
Step: 24750, train/grad_norm: 2.356230834266171e-05
Step: 24750, train/learning_rate: 2.054973811027594e-05
Step: 24750, train/epoch: 5.890052318572998
Step: 24760, train/loss: 0.0
Step: 24760, train/grad_norm: 5.682259870809503e-05
Step: 24760, train/learning_rate: 2.0537838281597942e-05
Step: 24760, train/epoch: 5.89243221282959
Step: 24770, train/loss: 0.0
Step: 24770, train/grad_norm: 8.781908036326058e-07
Step: 24770, train/learning_rate: 2.052594027190935e-05
Step: 24770, train/epoch: 5.894812107086182
Step: 24780, train/loss: 0.0
Step: 24780, train/grad_norm: 1.1719579333657748e-06
Step: 24780, train/learning_rate: 2.051404044323135e-05
Step: 24780, train/epoch: 5.897192001342773
Step: 24790, train/loss: 0.0
Step: 24790, train/grad_norm: 1.8098518239639816e-07
Step: 24790, train/learning_rate: 2.0502142433542758e-05
Step: 24790, train/epoch: 5.899571418762207
Step: 24800, train/loss: 0.0
Step: 24800, train/grad_norm: 5.032978515373543e-05
Step: 24800, train/learning_rate: 2.049024260486476e-05
Step: 24800, train/epoch: 5.901951313018799
Step: 24810, train/loss: 0.0
Step: 24810, train/grad_norm: 9.935730486176908e-05
Step: 24810, train/learning_rate: 2.0478342776186764e-05
Step: 24810, train/epoch: 5.904331207275391
Step: 24820, train/loss: 0.0
Step: 24820, train/grad_norm: 3.5597044188762084e-05
Step: 24820, train/learning_rate: 2.046644476649817e-05
Step: 24820, train/epoch: 5.906711101531982
Step: 24830, train/loss: 0.0
Step: 24830, train/grad_norm: 3.0010633054189384e-06
Step: 24830, train/learning_rate: 2.0454544937820174e-05
Step: 24830, train/epoch: 5.909090995788574
Step: 24840, train/loss: 0.0
Step: 24840, train/grad_norm: 3.176108748448314e-06
Step: 24840, train/learning_rate: 2.044264692813158e-05
Step: 24840, train/epoch: 5.911470890045166
Step: 24850, train/loss: 0.0
Step: 24850, train/grad_norm: 9.278130164602771e-05
Step: 24850, train/learning_rate: 2.0430747099453583e-05
Step: 24850, train/epoch: 5.913850784301758
Step: 24860, train/loss: 0.0
Step: 24860, train/grad_norm: 3.491327879601158e-05
Step: 24860, train/learning_rate: 2.0418847270775586e-05
Step: 24860, train/epoch: 5.916230201721191
Step: 24870, train/loss: 0.0
Step: 24870, train/grad_norm: 7.997756483746343e-07
Step: 24870, train/learning_rate: 2.0406949261086993e-05
Step: 24870, train/epoch: 5.918610095977783
Step: 24880, train/loss: 0.0
Step: 24880, train/grad_norm: 2.229670599263045e-07
Step: 24880, train/learning_rate: 2.0395049432408996e-05
Step: 24880, train/epoch: 5.920989990234375
Step: 24890, train/loss: 0.0
Step: 24890, train/grad_norm: 5.251662059890805e-06
Step: 24890, train/learning_rate: 2.0383151422720402e-05
Step: 24890, train/epoch: 5.923369884490967
Step: 24900, train/loss: 0.0
Step: 24900, train/grad_norm: 0.0003877867420669645
Step: 24900, train/learning_rate: 2.0371251594042405e-05
Step: 24900, train/epoch: 5.925749778747559
Step: 24910, train/loss: 0.0
Step: 24910, train/grad_norm: 1.5693931345595047e-05
Step: 24910, train/learning_rate: 2.0359353584353812e-05
Step: 24910, train/epoch: 5.92812967300415
Step: 24920, train/loss: 0.0
Step: 24920, train/grad_norm: 1.0850006226803544e-08
Step: 24920, train/learning_rate: 2.0347453755675815e-05
Step: 24920, train/epoch: 5.930509090423584
Step: 24930, train/loss: 0.0
Step: 24930, train/grad_norm: 0.0012128842063248158
Step: 24930, train/learning_rate: 2.0335553926997818e-05
Step: 24930, train/epoch: 5.932888984680176
Step: 24940, train/loss: 0.0
Step: 24940, train/grad_norm: 5.2116753579412034e-08
Step: 24940, train/learning_rate: 2.0323655917309225e-05
Step: 24940, train/epoch: 5.935268878936768
Step: 24950, train/loss: 0.0
Step: 24950, train/grad_norm: 1.2977487131138332e-05
Step: 24950, train/learning_rate: 2.0311756088631228e-05
Step: 24950, train/epoch: 5.937648773193359
Step: 24960, train/loss: 0.0
Step: 24960, train/grad_norm: 0.00030471754143945873
Step: 24960, train/learning_rate: 2.0299858078942634e-05
Step: 24960, train/epoch: 5.940028667449951
Step: 24970, train/loss: 0.0
Step: 24970, train/grad_norm: 3.705303299739171e-07
Step: 24970, train/learning_rate: 2.0287958250264637e-05
Step: 24970, train/epoch: 5.942408561706543
Step: 24980, train/loss: 0.0
Step: 24980, train/grad_norm: 2.8064846446795855e-07
Step: 24980, train/learning_rate: 2.027605842158664e-05
Step: 24980, train/epoch: 5.944787979125977
Step: 24990, train/loss: 0.0
Step: 24990, train/grad_norm: 3.944801392208319e-07
Step: 24990, train/learning_rate: 2.0264160411898047e-05
Step: 24990, train/epoch: 5.947167873382568
Step: 25000, train/loss: 0.0
Step: 25000, train/grad_norm: 1.2826399142795708e-07
Step: 25000, train/learning_rate: 2.025226058322005e-05
Step: 25000, train/epoch: 5.94954776763916
Step: 25010, train/loss: 0.0
Step: 25010, train/grad_norm: 3.0434882773988647e-06
Step: 25010, train/learning_rate: 2.0240362573531456e-05
Step: 25010, train/epoch: 5.951927661895752
Step: 25020, train/loss: 0.0
Step: 25020, train/grad_norm: 3.996455610888461e-09
Step: 25020, train/learning_rate: 2.022846274485346e-05
Step: 25020, train/epoch: 5.954307556152344
Step: 25030, train/loss: 0.0
Step: 25030, train/grad_norm: 2.3978986973816063e-07
Step: 25030, train/learning_rate: 2.0216562916175462e-05
Step: 25030, train/epoch: 5.9566874504089355
Step: 25040, train/loss: 0.0
Step: 25040, train/grad_norm: 6.988671543695091e-07
Step: 25040, train/learning_rate: 2.020466490648687e-05
Step: 25040, train/epoch: 5.959067344665527
Step: 25050, train/loss: 0.0
Step: 25050, train/grad_norm: 1.4283136806625407e-06
Step: 25050, train/learning_rate: 2.0192765077808872e-05
Step: 25050, train/epoch: 5.961446762084961
Step: 25060, train/loss: 0.0
Step: 25060, train/grad_norm: 8.652428368804976e-05
Step: 25060, train/learning_rate: 2.018086706812028e-05
Step: 25060, train/epoch: 5.963826656341553
Step: 25070, train/loss: 0.0
Step: 25070, train/grad_norm: 4.539232065781107e-07
Step: 25070, train/learning_rate: 2.016896723944228e-05
Step: 25070, train/epoch: 5.9662065505981445
Step: 25080, train/loss: 0.0
Step: 25080, train/grad_norm: 1.2772523405146785e-05
Step: 25080, train/learning_rate: 2.0157067410764284e-05
Step: 25080, train/epoch: 5.968586444854736
Step: 25090, train/loss: 0.0
Step: 25090, train/grad_norm: 7.556790251328493e-07
Step: 25090, train/learning_rate: 2.014516940107569e-05
Step: 25090, train/epoch: 5.970966339111328
Step: 25100, train/loss: 0.0
Step: 25100, train/grad_norm: 1.0198233212577179e-05
Step: 25100, train/learning_rate: 2.0133269572397694e-05
Step: 25100, train/epoch: 5.97334623336792
Step: 25110, train/loss: 0.0
Step: 25110, train/grad_norm: 5.169658834347501e-07
Step: 25110, train/learning_rate: 2.01213715627091e-05
Step: 25110, train/epoch: 5.9757256507873535
Step: 25120, train/loss: 0.02539999969303608
Step: 25120, train/grad_norm: 1.160908709607611e-06
Step: 25120, train/learning_rate: 2.0109471734031104e-05
Step: 25120, train/epoch: 5.978105545043945
Step: 25130, train/loss: 0.210999995470047
Step: 25130, train/grad_norm: 1.67097823577933e-05
Step: 25130, train/learning_rate: 2.0097571905353107e-05
Step: 25130, train/epoch: 5.980485439300537
Step: 25140, train/loss: 0.0
Step: 25140, train/grad_norm: 1.8869493942474946e-05
Step: 25140, train/learning_rate: 2.0085673895664513e-05
Step: 25140, train/epoch: 5.982865333557129
Step: 25150, train/loss: 0.05510000139474869
Step: 25150, train/grad_norm: 0.0010225264122709632
Step: 25150, train/learning_rate: 2.0073774066986516e-05
Step: 25150, train/epoch: 5.985245227813721
Step: 25160, train/loss: 0.0
Step: 25160, train/grad_norm: 1.0645991096680518e-05
Step: 25160, train/learning_rate: 2.0061876057297923e-05
Step: 25160, train/epoch: 5.9876251220703125
Step: 25170, train/loss: 0.0
Step: 25170, train/grad_norm: 8.553441148251295e-05
Step: 25170, train/learning_rate: 2.0049976228619926e-05
Step: 25170, train/epoch: 5.990004539489746
Step: 25180, train/loss: 0.0
Step: 25180, train/grad_norm: 0.0013735429383814335
Step: 25180, train/learning_rate: 2.003807639994193e-05
Step: 25180, train/epoch: 5.992384433746338
Step: 25190, train/loss: 0.0
Step: 25190, train/grad_norm: 0.008677367120981216
Step: 25190, train/learning_rate: 2.0026178390253335e-05
Step: 25190, train/epoch: 5.99476432800293
Step: 25200, train/loss: 0.0
Step: 25200, train/grad_norm: 1.5453031664947048e-05
Step: 25200, train/learning_rate: 2.0014278561575338e-05
Step: 25200, train/epoch: 5.9971442222595215
Step: 25210, train/loss: 0.000699999975040555
Step: 25210, train/grad_norm: 0.000522339076269418
Step: 25210, train/learning_rate: 2.0002380551886745e-05
Step: 25210, train/epoch: 5.999524116516113
Step: 25212, eval/loss: 0.03803480789065361
Step: 25212, eval/accuracy: 0.9945855736732483
Step: 25212, eval/f1: 0.994274914264679
Step: 25212, eval/runtime: 707.9306030273438
Step: 25212, eval/samples_per_second: 10.175000190734863
Step: 25212, eval/steps_per_second: 1.2730000019073486
Step: 25212, train/epoch: 6.0
Step: 25220, train/loss: 0.0
Step: 25220, train/grad_norm: 5.779216735390946e-05
Step: 25220, train/learning_rate: 1.9990480723208748e-05
Step: 25220, train/epoch: 6.001904010772705
Step: 25230, train/loss: 0.0
Step: 25230, train/grad_norm: 0.00183955323882401
Step: 25230, train/learning_rate: 1.997858089453075e-05
Step: 25230, train/epoch: 6.004283905029297
Step: 25240, train/loss: 0.0
Step: 25240, train/grad_norm: 2.7683813641488086e-06
Step: 25240, train/learning_rate: 1.9966682884842157e-05
Step: 25240, train/epoch: 6.0066633224487305
Step: 25250, train/loss: 0.0
Step: 25250, train/grad_norm: 9.040567965712398e-05
Step: 25250, train/learning_rate: 1.995478305616416e-05
Step: 25250, train/epoch: 6.009043216705322
Step: 25260, train/loss: 0.0
Step: 25260, train/grad_norm: 3.604291123338044e-05
Step: 25260, train/learning_rate: 1.9942885046475567e-05
Step: 25260, train/epoch: 6.011423110961914
Step: 25270, train/loss: 0.031599998474121094
Step: 25270, train/grad_norm: 3.548900906480412e-07
Step: 25270, train/learning_rate: 1.993098521779757e-05
Step: 25270, train/epoch: 6.013803005218506
Step: 25280, train/loss: 0.0
Step: 25280, train/grad_norm: 9.267953828384634e-06
Step: 25280, train/learning_rate: 1.9919085389119573e-05
Step: 25280, train/epoch: 6.016182899475098
Step: 25290, train/loss: 0.0008999999845400453
Step: 25290, train/grad_norm: 2.2038742031327274e-07
Step: 25290, train/learning_rate: 1.990718737943098e-05
Step: 25290, train/epoch: 6.0185627937316895
Step: 25300, train/loss: 0.0
Step: 25300, train/grad_norm: 0.00034013675758615136
Step: 25300, train/learning_rate: 1.9895287550752982e-05
Step: 25300, train/epoch: 6.020942211151123
Step: 25310, train/loss: 0.0
Step: 25310, train/grad_norm: 2.032295931542194e-08
Step: 25310, train/learning_rate: 1.988338954106439e-05
Step: 25310, train/epoch: 6.023322105407715
Step: 25320, train/loss: 9.999999747378752e-05
Step: 25320, train/grad_norm: 0.28173384070396423
Step: 25320, train/learning_rate: 1.9871489712386392e-05
Step: 25320, train/epoch: 6.025701999664307
Step: 25330, train/loss: 0.0
Step: 25330, train/grad_norm: 2.3020835214992985e-05
Step: 25330, train/learning_rate: 1.9859589883708395e-05
Step: 25330, train/epoch: 6.028081893920898
Step: 25340, train/loss: 0.0
Step: 25340, train/grad_norm: 3.1383342502522282e-06
Step: 25340, train/learning_rate: 1.98476918740198e-05
Step: 25340, train/epoch: 6.03046178817749
Step: 25350, train/loss: 0.0
Step: 25350, train/grad_norm: 2.8269580525375204e-06
Step: 25350, train/learning_rate: 1.9835792045341805e-05
Step: 25350, train/epoch: 6.032841682434082
Step: 25360, train/loss: 0.0
Step: 25360, train/grad_norm: 7.375567747658351e-07
Step: 25360, train/learning_rate: 1.982389403565321e-05
Step: 25360, train/epoch: 6.035221099853516
Step: 25370, train/loss: 0.0
Step: 25370, train/grad_norm: 1.0443393421155633e-06
Step: 25370, train/learning_rate: 1.9811994206975214e-05
Step: 25370, train/epoch: 6.037600994110107
Step: 25380, train/loss: 0.05270000174641609
Step: 25380, train/grad_norm: 5.408514311966428e-07
Step: 25380, train/learning_rate: 1.9800094378297217e-05
Step: 25380, train/epoch: 6.039980888366699
Step: 25390, train/loss: 0.0
Step: 25390, train/grad_norm: 4.393225935928058e-06
Step: 25390, train/learning_rate: 1.9788196368608624e-05
Step: 25390, train/epoch: 6.042360782623291
Step: 25400, train/loss: 0.0
Step: 25400, train/grad_norm: 6.5008457568183076e-06
Step: 25400, train/learning_rate: 1.9776296539930627e-05
Step: 25400, train/epoch: 6.044740676879883
Step: 25410, train/loss: 0.0
Step: 25410, train/grad_norm: 9.918127119590281e-08
Step: 25410, train/learning_rate: 1.9764398530242033e-05
Step: 25410, train/epoch: 6.047120571136475
Step: 25420, train/loss: 0.0
Step: 25420, train/grad_norm: 8.771141892793821e-07
Step: 25420, train/learning_rate: 1.9752498701564036e-05
Step: 25420, train/epoch: 6.049500465393066
Step: 25430, train/loss: 0.0
Step: 25430, train/grad_norm: 2.6531238290772308e-06
Step: 25430, train/learning_rate: 1.974059887288604e-05
Step: 25430, train/epoch: 6.0518798828125
Step: 25440, train/loss: 0.0
Step: 25440, train/grad_norm: 1.1900532115305396e-07
Step: 25440, train/learning_rate: 1.9728700863197446e-05
Step: 25440, train/epoch: 6.054259777069092
Step: 25450, train/loss: 0.0
Step: 25450, train/grad_norm: 9.041930866260373e-07
Step: 25450, train/learning_rate: 1.971680103451945e-05
Step: 25450, train/epoch: 6.056639671325684
Step: 25460, train/loss: 0.0
Step: 25460, train/grad_norm: 0.0020658019930124283
Step: 25460, train/learning_rate: 1.9704903024830855e-05
Step: 25460, train/epoch: 6.059019565582275
Step: 25470, train/loss: 0.0
Step: 25470, train/grad_norm: 6.730784374298082e-09
Step: 25470, train/learning_rate: 1.969300319615286e-05
Step: 25470, train/epoch: 6.061399459838867
Step: 25480, train/loss: 0.0
Step: 25480, train/grad_norm: 6.709315493935719e-05
Step: 25480, train/learning_rate: 1.968110336747486e-05
Step: 25480, train/epoch: 6.063779354095459
Step: 25490, train/loss: 0.0
Step: 25490, train/grad_norm: 7.069314165164542e-07
Step: 25490, train/learning_rate: 1.9669205357786268e-05
Step: 25490, train/epoch: 6.066158771514893
Step: 25500, train/loss: 0.0
Step: 25500, train/grad_norm: 7.1919434958545025e-06
Step: 25500, train/learning_rate: 1.965730552910827e-05
Step: 25500, train/epoch: 6.068538665771484
Step: 25510, train/loss: 0.0
Step: 25510, train/grad_norm: 5.8466646635224606e-08
Step: 25510, train/learning_rate: 1.9645407519419678e-05
Step: 25510, train/epoch: 6.070918560028076
Step: 25520, train/loss: 0.0
Step: 25520, train/grad_norm: 8.825736586004496e-06
Step: 25520, train/learning_rate: 1.963350769074168e-05
Step: 25520, train/epoch: 6.073298454284668
Step: 25530, train/loss: 0.0
Step: 25530, train/grad_norm: 1.4175667217841692e-07
Step: 25530, train/learning_rate: 1.9621607862063684e-05
Step: 25530, train/epoch: 6.07567834854126
Step: 25540, train/loss: 0.0
Step: 25540, train/grad_norm: 7.188048039097339e-05
Step: 25540, train/learning_rate: 1.960970985237509e-05
Step: 25540, train/epoch: 6.078058242797852
Step: 25550, train/loss: 0.0
Step: 25550, train/grad_norm: 9.634729991603308e-08
Step: 25550, train/learning_rate: 1.9597810023697093e-05
Step: 25550, train/epoch: 6.080437660217285
Step: 25560, train/loss: 0.0
Step: 25560, train/grad_norm: 3.5665718314703554e-05
Step: 25560, train/learning_rate: 1.95859120140085e-05
Step: 25560, train/epoch: 6.082817554473877
Step: 25570, train/loss: 0.0
Step: 25570, train/grad_norm: 1.4923608432582114e-05
Step: 25570, train/learning_rate: 1.9574012185330503e-05
Step: 25570, train/epoch: 6.085197448730469
Step: 25580, train/loss: 0.0
Step: 25580, train/grad_norm: 5.740703556966764e-08
Step: 25580, train/learning_rate: 1.956211417564191e-05
Step: 25580, train/epoch: 6.0875773429870605
Step: 25590, train/loss: 0.0
Step: 25590, train/grad_norm: 3.2754383028077427e-07
Step: 25590, train/learning_rate: 1.9550214346963912e-05
Step: 25590, train/epoch: 6.089957237243652
Step: 25600, train/loss: 0.0
Step: 25600, train/grad_norm: 3.057730054933927e-06
Step: 25600, train/learning_rate: 1.9538314518285915e-05
Step: 25600, train/epoch: 6.092337131500244
Step: 25610, train/loss: 0.0
Step: 25610, train/grad_norm: 0.00010828998347278684
Step: 25610, train/learning_rate: 1.9526416508597322e-05
Step: 25610, train/epoch: 6.094717025756836
Step: 25620, train/loss: 0.0
Step: 25620, train/grad_norm: 1.6431869198640925e-07
Step: 25620, train/learning_rate: 1.9514516679919325e-05
Step: 25620, train/epoch: 6.0970964431762695
Step: 25630, train/loss: 0.0
Step: 25630, train/grad_norm: 2.968625167909522e-08
Step: 25630, train/learning_rate: 1.950261867023073e-05
Step: 25630, train/epoch: 6.099476337432861
Step: 25640, train/loss: 0.0
Step: 25640, train/grad_norm: 7.903550880428156e-08
Step: 25640, train/learning_rate: 1.9490718841552734e-05
Step: 25640, train/epoch: 6.101856231689453
Step: 25650, train/loss: 0.0
Step: 25650, train/grad_norm: 7.250926955748582e-06
Step: 25650, train/learning_rate: 1.9478819012874737e-05
Step: 25650, train/epoch: 6.104236125946045
Step: 25660, train/loss: 0.0
Step: 25660, train/grad_norm: 7.923681977217711e-08
Step: 25660, train/learning_rate: 1.9466921003186144e-05
Step: 25660, train/epoch: 6.106616020202637
Step: 25670, train/loss: 0.0
Step: 25670, train/grad_norm: 2.9919979169790167e-06
Step: 25670, train/learning_rate: 1.9455021174508147e-05
Step: 25670, train/epoch: 6.1089959144592285
Step: 25680, train/loss: 0.0
Step: 25680, train/grad_norm: 2.4951170871645445e-06
Step: 25680, train/learning_rate: 1.9443123164819553e-05
Step: 25680, train/epoch: 6.111375331878662
Step: 25690, train/loss: 0.0
Step: 25690, train/grad_norm: 2.4126866264850833e-05
Step: 25690, train/learning_rate: 1.9431223336141557e-05
Step: 25690, train/epoch: 6.113755226135254
Step: 25700, train/loss: 0.0
Step: 25700, train/grad_norm: 1.0841479706868995e-05
Step: 25700, train/learning_rate: 1.941932350746356e-05
Step: 25700, train/epoch: 6.116135120391846
Step: 25710, train/loss: 0.0
Step: 25710, train/grad_norm: 2.6037611178253428e-08
Step: 25710, train/learning_rate: 1.9407425497774966e-05
Step: 25710, train/epoch: 6.1185150146484375
Step: 25720, train/loss: 0.0
Step: 25720, train/grad_norm: 1.1762367746470659e-09
Step: 25720, train/learning_rate: 1.939552566909697e-05
Step: 25720, train/epoch: 6.120894908905029
Step: 25730, train/loss: 0.0
Step: 25730, train/grad_norm: 0.0014518002280965447
Step: 25730, train/learning_rate: 1.9383627659408376e-05
Step: 25730, train/epoch: 6.123274803161621
Step: 25740, train/loss: 0.0
Step: 25740, train/grad_norm: 2.78325664737622e-08
Step: 25740, train/learning_rate: 1.937172783073038e-05
Step: 25740, train/epoch: 6.125654220581055
Step: 25750, train/loss: 0.0
Step: 25750, train/grad_norm: 1.1213198376935907e-05
Step: 25750, train/learning_rate: 1.935982800205238e-05
Step: 25750, train/epoch: 6.1280341148376465
Step: 25760, train/loss: 0.0
Step: 25760, train/grad_norm: 4.6184887025901844e-08
Step: 25760, train/learning_rate: 1.9347929992363788e-05
Step: 25760, train/epoch: 6.130414009094238
Step: 25770, train/loss: 0.0
Step: 25770, train/grad_norm: 8.326382157974876e-06
Step: 25770, train/learning_rate: 1.933603016368579e-05
Step: 25770, train/epoch: 6.13279390335083
Step: 25780, train/loss: 0.0
Step: 25780, train/grad_norm: 6.8240428845456336e-06
Step: 25780, train/learning_rate: 1.9324132153997198e-05
Step: 25780, train/epoch: 6.135173797607422
Step: 25790, train/loss: 0.0
Step: 25790, train/grad_norm: 4.360522609658801e-08
Step: 25790, train/learning_rate: 1.93122323253192e-05
Step: 25790, train/epoch: 6.137553691864014
Step: 25800, train/loss: 0.0
Step: 25800, train/grad_norm: 2.2990167053649202e-05
Step: 25800, train/learning_rate: 1.9300332496641204e-05
Step: 25800, train/epoch: 6.1399335861206055
Step: 25810, train/loss: 0.0
Step: 25810, train/grad_norm: 2.91664775886602e-07
Step: 25810, train/learning_rate: 1.928843448695261e-05
Step: 25810, train/epoch: 6.142313003540039
Step: 25820, train/loss: 0.0
Step: 25820, train/grad_norm: 2.323731074227453e-08
Step: 25820, train/learning_rate: 1.9276534658274613e-05
Step: 25820, train/epoch: 6.144692897796631
Step: 25830, train/loss: 0.0
Step: 25830, train/grad_norm: 1.0122017556568608e-05
Step: 25830, train/learning_rate: 1.926463664858602e-05
Step: 25830, train/epoch: 6.147072792053223
Step: 25840, train/loss: 0.0
Step: 25840, train/grad_norm: 2.872523339192412e-07
Step: 25840, train/learning_rate: 1.9252736819908023e-05
Step: 25840, train/epoch: 6.1494526863098145
Step: 25850, train/loss: 0.0
Step: 25850, train/grad_norm: 0.0009662541560828686
Step: 25850, train/learning_rate: 1.9240836991230026e-05
Step: 25850, train/epoch: 6.151832580566406
Step: 25860, train/loss: 0.0
Step: 25860, train/grad_norm: 3.2684897632861976e-06
Step: 25860, train/learning_rate: 1.9228938981541432e-05
Step: 25860, train/epoch: 6.154212474822998
Step: 25870, train/loss: 0.0
Step: 25870, train/grad_norm: 5.549958927986154e-07
Step: 25870, train/learning_rate: 1.9217039152863435e-05
Step: 25870, train/epoch: 6.156591892242432
Step: 25880, train/loss: 0.0
Step: 25880, train/grad_norm: 6.704524224687702e-08
Step: 25880, train/learning_rate: 1.9205141143174842e-05
Step: 25880, train/epoch: 6.158971786499023
Step: 25890, train/loss: 0.0
Step: 25890, train/grad_norm: 1.454256270960741e-09
Step: 25890, train/learning_rate: 1.9193241314496845e-05
Step: 25890, train/epoch: 6.161351680755615
Step: 25900, train/loss: 0.0
Step: 25900, train/grad_norm: 5.033888101024786e-06
Step: 25900, train/learning_rate: 1.9181341485818848e-05
Step: 25900, train/epoch: 6.163731575012207
Step: 25910, train/loss: 0.0
Step: 25910, train/grad_norm: 2.882597698317113e-07
Step: 25910, train/learning_rate: 1.9169443476130255e-05
Step: 25910, train/epoch: 6.166111469268799
Step: 25920, train/loss: 0.0
Step: 25920, train/grad_norm: 2.3591137505718507e-06
Step: 25920, train/learning_rate: 1.9157543647452258e-05
Step: 25920, train/epoch: 6.168491363525391
Step: 25930, train/loss: 0.0
Step: 25930, train/grad_norm: 0.002480284543707967
Step: 25930, train/learning_rate: 1.9145645637763664e-05
Step: 25930, train/epoch: 6.170870780944824
Step: 25940, train/loss: 0.0
Step: 25940, train/grad_norm: 9.675889486970846e-06
Step: 25940, train/learning_rate: 1.9133745809085667e-05
Step: 25940, train/epoch: 6.173250675201416
Step: 25950, train/loss: 0.0
Step: 25950, train/grad_norm: 6.7488272179616615e-06
Step: 25950, train/learning_rate: 1.912184598040767e-05
Step: 25950, train/epoch: 6.175630569458008
Step: 25960, train/loss: 0.0
Step: 25960, train/grad_norm: 3.7455666301866586e-07
Step: 25960, train/learning_rate: 1.9109947970719077e-05
Step: 25960, train/epoch: 6.1780104637146
Step: 25970, train/loss: 0.0
Step: 25970, train/grad_norm: 3.2651644232828403e-07
Step: 25970, train/learning_rate: 1.909804814204108e-05
Step: 25970, train/epoch: 6.180390357971191
Step: 25980, train/loss: 0.0
Step: 25980, train/grad_norm: 9.783943255570193e-08
Step: 25980, train/learning_rate: 1.9086150132352486e-05
Step: 25980, train/epoch: 6.182770252227783
Step: 25990, train/loss: 0.0
Step: 25990, train/grad_norm: 7.27435917724506e-07
Step: 25990, train/learning_rate: 1.907425030367449e-05
Step: 25990, train/epoch: 6.185150146484375
Step: 26000, train/loss: 0.0
Step: 26000, train/grad_norm: 0.0008248524973168969
Step: 26000, train/learning_rate: 1.9062350474996492e-05
Step: 26000, train/epoch: 6.187529563903809
Step: 26010, train/loss: 0.0
Step: 26010, train/grad_norm: 0.00010510847641853616
Step: 26010, train/learning_rate: 1.90504524653079e-05
Step: 26010, train/epoch: 6.1899094581604
Step: 26020, train/loss: 0.0
Step: 26020, train/grad_norm: 3.9522856241092086e-05
Step: 26020, train/learning_rate: 1.9038552636629902e-05
Step: 26020, train/epoch: 6.192289352416992
Step: 26030, train/loss: 0.0
Step: 26030, train/grad_norm: 0.000758581212721765
Step: 26030, train/learning_rate: 1.902665462694131e-05
Step: 26030, train/epoch: 6.194669246673584
Step: 26040, train/loss: 0.0
Step: 26040, train/grad_norm: 1.5738086389660566e-08
Step: 26040, train/learning_rate: 1.901475479826331e-05
Step: 26040, train/epoch: 6.197049140930176
Step: 26050, train/loss: 0.0
Step: 26050, train/grad_norm: 7.518786151194945e-05
Step: 26050, train/learning_rate: 1.9002854969585314e-05
Step: 26050, train/epoch: 6.199429035186768
Step: 26060, train/loss: 0.0
Step: 26060, train/grad_norm: 2.8425626297234885e-08
Step: 26060, train/learning_rate: 1.899095695989672e-05
Step: 26060, train/epoch: 6.201808452606201
Step: 26070, train/loss: 0.0
Step: 26070, train/grad_norm: 5.323060463524598e-07
Step: 26070, train/learning_rate: 1.8979057131218724e-05
Step: 26070, train/epoch: 6.204188346862793
Step: 26080, train/loss: 0.0
Step: 26080, train/grad_norm: 8.766123755776789e-07
Step: 26080, train/learning_rate: 1.896715912153013e-05
Step: 26080, train/epoch: 6.206568241119385
Step: 26090, train/loss: 0.0
Step: 26090, train/grad_norm: 7.681628488853676e-08
Step: 26090, train/learning_rate: 1.8955259292852134e-05
Step: 26090, train/epoch: 6.208948135375977
Step: 26100, train/loss: 0.0
Step: 26100, train/grad_norm: 2.9285799882927677e-06
Step: 26100, train/learning_rate: 1.8943359464174137e-05
Step: 26100, train/epoch: 6.211328029632568
Step: 26110, train/loss: 0.0
Step: 26110, train/grad_norm: 2.6017532945843413e-05
Step: 26110, train/learning_rate: 1.8931461454485543e-05
Step: 26110, train/epoch: 6.21370792388916
Step: 26120, train/loss: 0.0
Step: 26120, train/grad_norm: 0.0045342822559177876
Step: 26120, train/learning_rate: 1.8919561625807546e-05
Step: 26120, train/epoch: 6.216087341308594
Step: 26130, train/loss: 0.0
Step: 26130, train/grad_norm: 3.6599028589989757e-06
Step: 26130, train/learning_rate: 1.8907663616118953e-05
Step: 26130, train/epoch: 6.2184672355651855
Step: 26140, train/loss: 0.0
Step: 26140, train/grad_norm: 3.7450777199410368e-06
Step: 26140, train/learning_rate: 1.8895763787440956e-05
Step: 26140, train/epoch: 6.220847129821777
Step: 26150, train/loss: 0.0
Step: 26150, train/grad_norm: 0.0011248067021369934
Step: 26150, train/learning_rate: 1.888386395876296e-05
Step: 26150, train/epoch: 6.223227024078369
Step: 26160, train/loss: 0.0
Step: 26160, train/grad_norm: 7.73184910940472e-06
Step: 26160, train/learning_rate: 1.8871965949074365e-05
Step: 26160, train/epoch: 6.225606918334961
Step: 26170, train/loss: 0.0
Step: 26170, train/grad_norm: 1.805069729243769e-07
Step: 26170, train/learning_rate: 1.8860066120396368e-05
Step: 26170, train/epoch: 6.227986812591553
Step: 26180, train/loss: 0.0
Step: 26180, train/grad_norm: 1.8772920995502318e-08
Step: 26180, train/learning_rate: 1.8848168110707775e-05
Step: 26180, train/epoch: 6.2303667068481445
Step: 26190, train/loss: 0.0
Step: 26190, train/grad_norm: 2.66229562839726e-07
Step: 26190, train/learning_rate: 1.8836268282029778e-05
Step: 26190, train/epoch: 6.232746124267578
Step: 26200, train/loss: 0.0
Step: 26200, train/grad_norm: 0.002457290655001998
Step: 26200, train/learning_rate: 1.882436845335178e-05
Step: 26200, train/epoch: 6.23512601852417
Step: 26210, train/loss: 0.0
Step: 26210, train/grad_norm: 3.382831891940441e-06
Step: 26210, train/learning_rate: 1.8812470443663187e-05
Step: 26210, train/epoch: 6.237505912780762
Step: 26220, train/loss: 0.0
Step: 26220, train/grad_norm: 3.382407157914713e-06
Step: 26220, train/learning_rate: 1.880057061498519e-05
Step: 26220, train/epoch: 6.2398858070373535
Step: 26230, train/loss: 0.0
Step: 26230, train/grad_norm: 1.039584276441019e-07
Step: 26230, train/learning_rate: 1.8788672605296597e-05
Step: 26230, train/epoch: 6.242265701293945
Step: 26240, train/loss: 0.0
Step: 26240, train/grad_norm: 1.0074288780970164e-07
Step: 26240, train/learning_rate: 1.87767727766186e-05
Step: 26240, train/epoch: 6.244645595550537
Step: 26250, train/loss: 0.0
Step: 26250, train/grad_norm: 0.000755661225412041
Step: 26250, train/learning_rate: 1.8764874766930006e-05
Step: 26250, train/epoch: 6.247025012969971
Step: 26260, train/loss: 0.0
Step: 26260, train/grad_norm: 5.0308081256389414e-08
Step: 26260, train/learning_rate: 1.875297493825201e-05
Step: 26260, train/epoch: 6.2494049072265625
Step: 26270, train/loss: 0.0
Step: 26270, train/grad_norm: 1.640808659431059e-05
Step: 26270, train/learning_rate: 1.8741075109574012e-05
Step: 26270, train/epoch: 6.251784801483154
Step: 26280, train/loss: 0.0
Step: 26280, train/grad_norm: 2.1422792997327633e-06
Step: 26280, train/learning_rate: 1.872917709988542e-05
Step: 26280, train/epoch: 6.254164695739746
Step: 26290, train/loss: 0.0
Step: 26290, train/grad_norm: 8.163923759241243e-09
Step: 26290, train/learning_rate: 1.8717277271207422e-05
Step: 26290, train/epoch: 6.256544589996338
Step: 26300, train/loss: 0.0
Step: 26300, train/grad_norm: 3.935813879252237e-07
Step: 26300, train/learning_rate: 1.870537926151883e-05
Step: 26300, train/epoch: 6.25892448425293
Step: 26310, train/loss: 0.0
Step: 26310, train/grad_norm: 3.482381316644023e-06
Step: 26310, train/learning_rate: 1.869347943284083e-05
Step: 26310, train/epoch: 6.2613043785095215
Step: 26320, train/loss: 0.0015999999595806003
Step: 26320, train/grad_norm: 1.843904251508377e-09
Step: 26320, train/learning_rate: 1.8681579604162835e-05
Step: 26320, train/epoch: 6.263683795928955
Step: 26330, train/loss: 0.0
Step: 26330, train/grad_norm: 0.003555234055966139
Step: 26330, train/learning_rate: 1.866968159447424e-05
Step: 26330, train/epoch: 6.266063690185547
Step: 26340, train/loss: 0.0
Step: 26340, train/grad_norm: 9.027140634998432e-08
Step: 26340, train/learning_rate: 1.8657781765796244e-05
Step: 26340, train/epoch: 6.268443584442139
Step: 26350, train/loss: 0.0
Step: 26350, train/grad_norm: 5.4330233979271725e-05
Step: 26350, train/learning_rate: 1.864588375610765e-05
Step: 26350, train/epoch: 6.2708234786987305
Step: 26360, train/loss: 0.0
Step: 26360, train/grad_norm: 5.339609856491734e-07
Step: 26360, train/learning_rate: 1.8633983927429654e-05
Step: 26360, train/epoch: 6.273203372955322
Step: 26370, train/loss: 0.0
Step: 26370, train/grad_norm: 8.520091654418138e-08
Step: 26370, train/learning_rate: 1.8622084098751657e-05
Step: 26370, train/epoch: 6.275583267211914
Step: 26380, train/loss: 9.999999747378752e-05
Step: 26380, train/grad_norm: 2.8211928349008986e-09
Step: 26380, train/learning_rate: 1.8610186089063063e-05
Step: 26380, train/epoch: 6.277962684631348
Step: 26390, train/loss: 0.0
Step: 26390, train/grad_norm: 2.3663952575248004e-08
Step: 26390, train/learning_rate: 1.8598286260385066e-05
Step: 26390, train/epoch: 6.2803425788879395
Step: 26400, train/loss: 0.0
Step: 26400, train/grad_norm: 3.279963038949063e-06
Step: 26400, train/learning_rate: 1.8586388250696473e-05
Step: 26400, train/epoch: 6.282722473144531
Step: 26410, train/loss: 0.0
Step: 26410, train/grad_norm: 2.2058128656965437e-09
Step: 26410, train/learning_rate: 1.8574488422018476e-05
Step: 26410, train/epoch: 6.285102367401123
Step: 26420, train/loss: 0.0
Step: 26420, train/grad_norm: 1.90086448128568e-05
Step: 26420, train/learning_rate: 1.856258859334048e-05
Step: 26420, train/epoch: 6.287482261657715
Step: 26430, train/loss: 0.0
Step: 26430, train/grad_norm: 1.7213382363934215e-07
Step: 26430, train/learning_rate: 1.8550690583651885e-05
Step: 26430, train/epoch: 6.289862155914307
Step: 26440, train/loss: 0.0
Step: 26440, train/grad_norm: 7.421730515488889e-06
Step: 26440, train/learning_rate: 1.853879075497389e-05
Step: 26440, train/epoch: 6.29224157333374
Step: 26450, train/loss: 0.0
Step: 26450, train/grad_norm: 5.590885088224695e-09
Step: 26450, train/learning_rate: 1.8526892745285295e-05
Step: 26450, train/epoch: 6.294621467590332
Step: 26460, train/loss: 9.999999747378752e-05
Step: 26460, train/grad_norm: 1.930821280637929e-09
Step: 26460, train/learning_rate: 1.8514992916607298e-05
Step: 26460, train/epoch: 6.297001361846924
Step: 26470, train/loss: 0.0
Step: 26470, train/grad_norm: 8.597980660773885e-10
Step: 26470, train/learning_rate: 1.85030930879293e-05
Step: 26470, train/epoch: 6.299381256103516
Step: 26480, train/loss: 0.0
Step: 26480, train/grad_norm: 5.0550863051057604e-08
Step: 26480, train/learning_rate: 1.8491195078240708e-05
Step: 26480, train/epoch: 6.301761150360107
Step: 26490, train/loss: 9.999999747378752e-05
Step: 26490, train/grad_norm: 1.884375988581155e-09
Step: 26490, train/learning_rate: 1.847929524956271e-05
Step: 26490, train/epoch: 6.304141044616699
Step: 26500, train/loss: 0.0
Step: 26500, train/grad_norm: 3.363172934589542e-11
Step: 26500, train/learning_rate: 1.8467397239874117e-05
Step: 26500, train/epoch: 6.306520938873291
Step: 26510, train/loss: 0.0
Step: 26510, train/grad_norm: 7.027375348300779e-10
Step: 26510, train/learning_rate: 1.845549741119612e-05
Step: 26510, train/epoch: 6.308900356292725
Step: 26520, train/loss: 0.0
Step: 26520, train/grad_norm: 1.4582488461201137e-07
Step: 26520, train/learning_rate: 1.8443597582518123e-05
Step: 26520, train/epoch: 6.311280250549316
Step: 26530, train/loss: 0.0
Step: 26530, train/grad_norm: 3.7464969864231534e-06
Step: 26530, train/learning_rate: 1.843169957282953e-05
Step: 26530, train/epoch: 6.313660144805908
Step: 26540, train/loss: 0.0
Step: 26540, train/grad_norm: 9.937816685123835e-06
Step: 26540, train/learning_rate: 1.8419799744151533e-05
Step: 26540, train/epoch: 6.3160400390625
Step: 26550, train/loss: 0.0
Step: 26550, train/grad_norm: 2.317351390956901e-05
Step: 26550, train/learning_rate: 1.840790173446294e-05
Step: 26550, train/epoch: 6.318419933319092
Step: 26560, train/loss: 0.0
Step: 26560, train/grad_norm: 1.0271644867998475e-08
Step: 26560, train/learning_rate: 1.8396001905784942e-05
Step: 26560, train/epoch: 6.320799827575684
Step: 26570, train/loss: 0.0
Step: 26570, train/grad_norm: 4.379846359370276e-06
Step: 26570, train/learning_rate: 1.8384102077106945e-05
Step: 26570, train/epoch: 6.323179244995117
Step: 26580, train/loss: 0.0
Step: 26580, train/grad_norm: 1.3128779130511248e-07
Step: 26580, train/learning_rate: 1.8372204067418352e-05
Step: 26580, train/epoch: 6.325559139251709
Step: 26590, train/loss: 0.0
Step: 26590, train/grad_norm: 1.0369170411195228e-08
Step: 26590, train/learning_rate: 1.8360304238740355e-05
Step: 26590, train/epoch: 6.327939033508301
Step: 26600, train/loss: 0.0
Step: 26600, train/grad_norm: 3.278061910805263e-08
Step: 26600, train/learning_rate: 1.834840622905176e-05
Step: 26600, train/epoch: 6.330318927764893
Step: 26610, train/loss: 0.0
Step: 26610, train/grad_norm: 1.9830710584756162e-07
Step: 26610, train/learning_rate: 1.8336506400373764e-05
Step: 26610, train/epoch: 6.332698822021484
Step: 26620, train/loss: 0.0
Step: 26620, train/grad_norm: 1.0842904352159621e-09
Step: 26620, train/learning_rate: 1.8324606571695767e-05
Step: 26620, train/epoch: 6.335078716278076
Step: 26630, train/loss: 0.0
Step: 26630, train/grad_norm: 0.0003004599711857736
Step: 26630, train/learning_rate: 1.8312708562007174e-05
Step: 26630, train/epoch: 6.33745813369751
Step: 26640, train/loss: 0.0
Step: 26640, train/grad_norm: 1.9323183053643334e-08
Step: 26640, train/learning_rate: 1.8300808733329177e-05
Step: 26640, train/epoch: 6.339838027954102
Step: 26650, train/loss: 0.0
Step: 26650, train/grad_norm: 7.985826444567579e-10
Step: 26650, train/learning_rate: 1.8288910723640583e-05
Step: 26650, train/epoch: 6.342217922210693
Step: 26660, train/loss: 0.0
Step: 26660, train/grad_norm: 2.0076971907201369e-07
Step: 26660, train/learning_rate: 1.8277010894962586e-05
Step: 26660, train/epoch: 6.344597816467285
Step: 26670, train/loss: 0.0
Step: 26670, train/grad_norm: 2.0066423522813182e-10
Step: 26670, train/learning_rate: 1.826511106628459e-05
Step: 26670, train/epoch: 6.346977710723877
Step: 26680, train/loss: 0.0
Step: 26680, train/grad_norm: 6.06763705945923e-06
Step: 26680, train/learning_rate: 1.8253213056595996e-05
Step: 26680, train/epoch: 6.349357604980469
Step: 26690, train/loss: 0.0
Step: 26690, train/grad_norm: 3.889715979865649e-11
Step: 26690, train/learning_rate: 1.8241313227918e-05
Step: 26690, train/epoch: 6.3517374992370605
Step: 26700, train/loss: 0.0
Step: 26700, train/grad_norm: 1.1304843727799607e-09
Step: 26700, train/learning_rate: 1.8229415218229406e-05
Step: 26700, train/epoch: 6.354116916656494
Step: 26710, train/loss: 0.0
Step: 26710, train/grad_norm: 6.535461949397359e-08
Step: 26710, train/learning_rate: 1.821751538955141e-05
Step: 26710, train/epoch: 6.356496810913086
Step: 26720, train/loss: 0.0
Step: 26720, train/grad_norm: 4.162312272626423e-09
Step: 26720, train/learning_rate: 1.820561556087341e-05
Step: 26720, train/epoch: 6.358876705169678
Step: 26730, train/loss: 0.0
Step: 26730, train/grad_norm: 5.67021807285073e-09
Step: 26730, train/learning_rate: 1.8193717551184818e-05
Step: 26730, train/epoch: 6.3612565994262695
Step: 26740, train/loss: 0.0
Step: 26740, train/grad_norm: 2.097284124147336e-08
Step: 26740, train/learning_rate: 1.818181772250682e-05
Step: 26740, train/epoch: 6.363636493682861
Step: 26750, train/loss: 0.0
Step: 26750, train/grad_norm: 6.695507437370907e-08
Step: 26750, train/learning_rate: 1.8169919712818228e-05
Step: 26750, train/epoch: 6.366016387939453
Step: 26760, train/loss: 0.0
Step: 26760, train/grad_norm: 1.3310239577180027e-09
Step: 26760, train/learning_rate: 1.815801988414023e-05
Step: 26760, train/epoch: 6.368395805358887
Step: 26770, train/loss: 0.0
Step: 26770, train/grad_norm: 1.9356964642280872e-10
Step: 26770, train/learning_rate: 1.8146120055462234e-05
Step: 26770, train/epoch: 6.3707756996154785
Step: 26780, train/loss: 0.0
Step: 26780, train/grad_norm: 5.05560204810962e-10
Step: 26780, train/learning_rate: 1.813422204577364e-05
Step: 26780, train/epoch: 6.37315559387207
Step: 26790, train/loss: 0.0
Step: 26790, train/grad_norm: 3.5230247608097898e-09
Step: 26790, train/learning_rate: 1.8122322217095643e-05
Step: 26790, train/epoch: 6.375535488128662
Step: 26800, train/loss: 0.0
Step: 26800, train/grad_norm: 9.116365440320351e-09
Step: 26800, train/learning_rate: 1.811042420740705e-05
Step: 26800, train/epoch: 6.377915382385254
Step: 26810, train/loss: 0.0
Step: 26810, train/grad_norm: 3.9443473487388303e-10
Step: 26810, train/learning_rate: 1.8098524378729053e-05
Step: 26810, train/epoch: 6.380295276641846
Step: 26820, train/loss: 0.0
Step: 26820, train/grad_norm: 1.3132276516225616e-11
Step: 26820, train/learning_rate: 1.8086624550051056e-05
Step: 26820, train/epoch: 6.382674694061279
Step: 26830, train/loss: 0.0
Step: 26830, train/grad_norm: 3.088129751915858e-11
Step: 26830, train/learning_rate: 1.8074726540362462e-05
Step: 26830, train/epoch: 6.385054588317871
Step: 26840, train/loss: 0.0
Step: 26840, train/grad_norm: 5.991886951051129e-07
Step: 26840, train/learning_rate: 1.8062826711684465e-05
Step: 26840, train/epoch: 6.387434482574463
Step: 26850, train/loss: 0.0
Step: 26850, train/grad_norm: 1.2526284365321771e-07
Step: 26850, train/learning_rate: 1.8050928701995872e-05
Step: 26850, train/epoch: 6.389814376831055
Step: 26860, train/loss: 0.0
Step: 26860, train/grad_norm: 1.780147917962438e-09
Step: 26860, train/learning_rate: 1.8039028873317875e-05
Step: 26860, train/epoch: 6.3921942710876465
Step: 26870, train/loss: 0.0
Step: 26870, train/grad_norm: 2.6966204269562866e-10
Step: 26870, train/learning_rate: 1.8027129044639878e-05
Step: 26870, train/epoch: 6.394574165344238
Step: 26880, train/loss: 0.0
Step: 26880, train/grad_norm: 6.489221604510931e-09
Step: 26880, train/learning_rate: 1.8015231034951285e-05
Step: 26880, train/epoch: 6.39695405960083
Step: 26890, train/loss: 0.0
Step: 26890, train/grad_norm: 1.0782165382750009e-08
Step: 26890, train/learning_rate: 1.8003331206273288e-05
Step: 26890, train/epoch: 6.399333477020264
Step: 26900, train/loss: 0.0
Step: 26900, train/grad_norm: 7.2903327819062724e-09
Step: 26900, train/learning_rate: 1.7991433196584694e-05
Step: 26900, train/epoch: 6.4017133712768555
Step: 26910, train/loss: 0.0
Step: 26910, train/grad_norm: 1.8639842447321087e-10
Step: 26910, train/learning_rate: 1.7979533367906697e-05
Step: 26910, train/epoch: 6.404093265533447
Step: 26920, train/loss: 0.0
Step: 26920, train/grad_norm: 4.0214789831516384e-10
Step: 26920, train/learning_rate: 1.7967635358218104e-05
Step: 26920, train/epoch: 6.406473159790039
Step: 26930, train/loss: 0.0
Step: 26930, train/grad_norm: 1.5749107262763573e-07
Step: 26930, train/learning_rate: 1.7955735529540107e-05
Step: 26930, train/epoch: 6.408853054046631
Step: 26940, train/loss: 0.0
Step: 26940, train/grad_norm: 6.790072948348325e-10
Step: 26940, train/learning_rate: 1.794383570086211e-05
Step: 26940, train/epoch: 6.411232948303223
Step: 26950, train/loss: 0.0
Step: 26950, train/grad_norm: 1.2930263437738176e-07
Step: 26950, train/learning_rate: 1.7931937691173516e-05
Step: 26950, train/epoch: 6.413612365722656
Step: 26960, train/loss: 0.0
Step: 26960, train/grad_norm: 2.017345318572339e-10
Step: 26960, train/learning_rate: 1.792003786249552e-05
Step: 26960, train/epoch: 6.415992259979248
Step: 26970, train/loss: 0.0
Step: 26970, train/grad_norm: 7.736642970712637e-08
Step: 26970, train/learning_rate: 1.7908139852806926e-05
Step: 26970, train/epoch: 6.41837215423584
Step: 26980, train/loss: 0.0
Step: 26980, train/grad_norm: 2.310358432344728e-10
Step: 26980, train/learning_rate: 1.789624002412893e-05
Step: 26980, train/epoch: 6.420752048492432
Step: 26990, train/loss: 0.0
Step: 26990, train/grad_norm: 1.3105059259999052e-08
Step: 26990, train/learning_rate: 1.7884340195450932e-05
Step: 26990, train/epoch: 6.423131942749023
Step: 27000, train/loss: 0.0
Step: 27000, train/grad_norm: 6.050766138132602e-11
Step: 27000, train/learning_rate: 1.787244218576234e-05
Step: 27000, train/epoch: 6.425511837005615
Step: 27010, train/loss: 0.0
Step: 27010, train/grad_norm: 2.5608910618757363e-06
Step: 27010, train/learning_rate: 1.786054235708434e-05
Step: 27010, train/epoch: 6.427891254425049
Step: 27020, train/loss: 0.0
Step: 27020, train/grad_norm: 4.86296503066086e-10
Step: 27020, train/learning_rate: 1.7848644347395748e-05
Step: 27020, train/epoch: 6.430271148681641
Step: 27030, train/loss: 0.0
Step: 27030, train/grad_norm: 9.061003680699287e-08
Step: 27030, train/learning_rate: 1.783674451871775e-05
Step: 27030, train/epoch: 6.432651042938232
Step: 27040, train/loss: 0.0
Step: 27040, train/grad_norm: 8.316117572348958e-09
Step: 27040, train/learning_rate: 1.7824844690039754e-05
Step: 27040, train/epoch: 6.435030937194824
Step: 27050, train/loss: 0.0
Step: 27050, train/grad_norm: 1.646444869329855e-09
Step: 27050, train/learning_rate: 1.781294668035116e-05
Step: 27050, train/epoch: 6.437410831451416
Step: 27060, train/loss: 0.0
Step: 27060, train/grad_norm: 1.033251123772061e-06
Step: 27060, train/learning_rate: 1.7801046851673163e-05
Step: 27060, train/epoch: 6.439790725708008
Step: 27070, train/loss: 0.0
Step: 27070, train/grad_norm: 8.32921216442628e-08
Step: 27070, train/learning_rate: 1.778914884198457e-05
Step: 27070, train/epoch: 6.4421706199646
Step: 27080, train/loss: 0.02290000021457672
Step: 27080, train/grad_norm: 4.825059249924379e-07
Step: 27080, train/learning_rate: 1.7777249013306573e-05
Step: 27080, train/epoch: 6.444550037384033
Step: 27090, train/loss: 0.0
Step: 27090, train/grad_norm: 1.725224763049482e-08
Step: 27090, train/learning_rate: 1.7765349184628576e-05
Step: 27090, train/epoch: 6.446929931640625
Step: 27100, train/loss: 0.0
Step: 27100, train/grad_norm: 9.998058885685168e-06
Step: 27100, train/learning_rate: 1.7753451174939983e-05
Step: 27100, train/epoch: 6.449309825897217
Step: 27110, train/loss: 0.0
Step: 27110, train/grad_norm: 5.026422968512634e-06
Step: 27110, train/learning_rate: 1.7741551346261986e-05
Step: 27110, train/epoch: 6.451689720153809
Step: 27120, train/loss: 0.0
Step: 27120, train/grad_norm: 6.774854455215973e-08
Step: 27120, train/learning_rate: 1.7729653336573392e-05
Step: 27120, train/epoch: 6.4540696144104
Step: 27130, train/loss: 0.0
Step: 27130, train/grad_norm: 2.00413282414047e-07
Step: 27130, train/learning_rate: 1.7717753507895395e-05
Step: 27130, train/epoch: 6.456449508666992
Step: 27140, train/loss: 0.15780000388622284
Step: 27140, train/grad_norm: 0.00019362509192433208
Step: 27140, train/learning_rate: 1.7705853679217398e-05
Step: 27140, train/epoch: 6.458828926086426
Step: 27150, train/loss: 0.0013000000035390258
Step: 27150, train/grad_norm: 3.971216210629791e-05
Step: 27150, train/learning_rate: 1.7693955669528805e-05
Step: 27150, train/epoch: 6.461208820343018
Step: 27160, train/loss: 0.0
Step: 27160, train/grad_norm: 6.83407506585354e-06
Step: 27160, train/learning_rate: 1.7682055840850808e-05
Step: 27160, train/epoch: 6.463588714599609
Step: 27170, train/loss: 0.0
Step: 27170, train/grad_norm: 1.7830329568369052e-07
Step: 27170, train/learning_rate: 1.7670157831162214e-05
Step: 27170, train/epoch: 6.465968608856201
Step: 27180, train/loss: 0.004100000020116568
Step: 27180, train/grad_norm: 3.8801377399977355e-07
Step: 27180, train/learning_rate: 1.7658258002484217e-05
Step: 27180, train/epoch: 6.468348503112793
Step: 27190, train/loss: 0.0
Step: 27190, train/grad_norm: 0.00013129657600075006
Step: 27190, train/learning_rate: 1.764635817380622e-05
Step: 27190, train/epoch: 6.470728397369385
Step: 27200, train/loss: 0.0
Step: 27200, train/grad_norm: 4.3353412593205576e-07
Step: 27200, train/learning_rate: 1.7634460164117627e-05
Step: 27200, train/epoch: 6.473107814788818
Step: 27210, train/loss: 0.0
Step: 27210, train/grad_norm: 2.832970676536206e-05
Step: 27210, train/learning_rate: 1.762256033543963e-05
Step: 27210, train/epoch: 6.47548770904541
Step: 27220, train/loss: 0.0
Step: 27220, train/grad_norm: 6.548880719492445e-06
Step: 27220, train/learning_rate: 1.7610662325751036e-05
Step: 27220, train/epoch: 6.477867603302002
Step: 27230, train/loss: 0.0
Step: 27230, train/grad_norm: 0.0019097328186035156
Step: 27230, train/learning_rate: 1.759876249707304e-05
Step: 27230, train/epoch: 6.480247497558594
Step: 27240, train/loss: 0.0
Step: 27240, train/grad_norm: 0.0005648788064718246
Step: 27240, train/learning_rate: 1.7586862668395042e-05
Step: 27240, train/epoch: 6.4826273918151855
Step: 27250, train/loss: 0.0
Step: 27250, train/grad_norm: 4.751259621116333e-05
Step: 27250, train/learning_rate: 1.757496465870645e-05
Step: 27250, train/epoch: 6.485007286071777
Step: 27260, train/loss: 0.0
Step: 27260, train/grad_norm: 4.460243144421838e-05
Step: 27260, train/learning_rate: 1.7563064830028452e-05
Step: 27260, train/epoch: 6.487387180328369
Step: 27270, train/loss: 0.0
Step: 27270, train/grad_norm: 0.0020419906359165907
Step: 27270, train/learning_rate: 1.755116682033986e-05
Step: 27270, train/epoch: 6.489766597747803
Step: 27280, train/loss: 0.0
Step: 27280, train/grad_norm: 7.184370360846515e-07
Step: 27280, train/learning_rate: 1.753926699166186e-05
Step: 27280, train/epoch: 6.4921464920043945
Step: 27290, train/loss: 0.0
Step: 27290, train/grad_norm: 8.927445946937951e-07
Step: 27290, train/learning_rate: 1.7527367162983865e-05
Step: 27290, train/epoch: 6.494526386260986
Step: 27300, train/loss: 0.0
Step: 27300, train/grad_norm: 0.00016013724962249398
Step: 27300, train/learning_rate: 1.751546915329527e-05
Step: 27300, train/epoch: 6.496906280517578
Step: 27310, train/loss: 0.0
Step: 27310, train/grad_norm: 1.4865106095385272e-05
Step: 27310, train/learning_rate: 1.7503569324617274e-05
Step: 27310, train/epoch: 6.49928617477417
Step: 27320, train/loss: 0.0
Step: 27320, train/grad_norm: 0.0019432430854067206
Step: 27320, train/learning_rate: 1.749167131492868e-05
Step: 27320, train/epoch: 6.501666069030762
Step: 27330, train/loss: 0.05270000174641609
Step: 27330, train/grad_norm: 1.689627424639184e-05
Step: 27330, train/learning_rate: 1.7479771486250684e-05
Step: 27330, train/epoch: 6.504045486450195
Step: 27340, train/loss: 0.0
Step: 27340, train/grad_norm: 0.0011683222837746143
Step: 27340, train/learning_rate: 1.7467871657572687e-05
Step: 27340, train/epoch: 6.506425380706787
Step: 27350, train/loss: 0.0
Step: 27350, train/grad_norm: 9.505456546321511e-06
Step: 27350, train/learning_rate: 1.7455973647884093e-05
Step: 27350, train/epoch: 6.508805274963379
Step: 27360, train/loss: 0.0
Step: 27360, train/grad_norm: 1.2139415048295632e-05
Step: 27360, train/learning_rate: 1.7444073819206096e-05
Step: 27360, train/epoch: 6.511185169219971
Step: 27370, train/loss: 0.0
Step: 27370, train/grad_norm: 2.59928510786267e-05
Step: 27370, train/learning_rate: 1.7432175809517503e-05
Step: 27370, train/epoch: 6.5135650634765625
Step: 27380, train/loss: 0.0
Step: 27380, train/grad_norm: 2.283951107528992e-05
Step: 27380, train/learning_rate: 1.7420275980839506e-05
Step: 27380, train/epoch: 6.515944957733154
Step: 27390, train/loss: 0.0
Step: 27390, train/grad_norm: 3.2193659080803627e-06
Step: 27390, train/learning_rate: 1.740837615216151e-05
Step: 27390, train/epoch: 6.518324375152588
Step: 27400, train/loss: 0.0
Step: 27400, train/grad_norm: 6.752510671503842e-06
Step: 27400, train/learning_rate: 1.7396478142472915e-05
Step: 27400, train/epoch: 6.52070426940918
Step: 27410, train/loss: 0.0
Step: 27410, train/grad_norm: 1.241066627244436e-07
Step: 27410, train/learning_rate: 1.738457831379492e-05
Step: 27410, train/epoch: 6.5230841636657715
Step: 27420, train/loss: 0.0
Step: 27420, train/grad_norm: 3.2075976719170285e-07
Step: 27420, train/learning_rate: 1.7372680304106325e-05
Step: 27420, train/epoch: 6.525464057922363
Step: 27430, train/loss: 0.0
Step: 27430, train/grad_norm: 2.0746217899159092e-07
Step: 27430, train/learning_rate: 1.7360780475428328e-05
Step: 27430, train/epoch: 6.527843952178955
Step: 27440, train/loss: 0.0
Step: 27440, train/grad_norm: 1.4810772881901357e-05
Step: 27440, train/learning_rate: 1.734888064675033e-05
Step: 27440, train/epoch: 6.530223846435547
Step: 27450, train/loss: 0.0
Step: 27450, train/grad_norm: 0.0028724893927574158
Step: 27450, train/learning_rate: 1.7336982637061737e-05
Step: 27450, train/epoch: 6.532603740692139
Step: 27460, train/loss: 0.0
Step: 27460, train/grad_norm: 3.12787904022116e-07
Step: 27460, train/learning_rate: 1.732508280838374e-05
Step: 27460, train/epoch: 6.534983158111572
Step: 27470, train/loss: 0.0
Step: 27470, train/grad_norm: 4.229411842970876e-07
Step: 27470, train/learning_rate: 1.7313184798695147e-05
Step: 27470, train/epoch: 6.537363052368164
Step: 27480, train/loss: 0.0
Step: 27480, train/grad_norm: 0.0029622248839586973
Step: 27480, train/learning_rate: 1.730128497001715e-05
Step: 27480, train/epoch: 6.539742946624756
Step: 27490, train/loss: 0.0
Step: 27490, train/grad_norm: 9.485457042046619e-08
Step: 27490, train/learning_rate: 1.7289385141339153e-05
Step: 27490, train/epoch: 6.542122840881348
Step: 27500, train/loss: 0.0
Step: 27500, train/grad_norm: 1.8385568978374067e-07
Step: 27500, train/learning_rate: 1.727748713165056e-05
Step: 27500, train/epoch: 6.5445027351379395
Step: 27510, train/loss: 0.004399999976158142
Step: 27510, train/grad_norm: 6.300032140416079e-08
Step: 27510, train/learning_rate: 1.7265587302972563e-05
Step: 27510, train/epoch: 6.546882629394531
Step: 27520, train/loss: 0.0
Step: 27520, train/grad_norm: 5.741491548860722e-08
Step: 27520, train/learning_rate: 1.725368929328397e-05
Step: 27520, train/epoch: 6.549262046813965
Step: 27530, train/loss: 0.0
Step: 27530, train/grad_norm: 5.374338911678933e-07
Step: 27530, train/learning_rate: 1.7241789464605972e-05
Step: 27530, train/epoch: 6.551641941070557
Step: 27540, train/loss: 0.0
Step: 27540, train/grad_norm: 8.148651977535337e-05
Step: 27540, train/learning_rate: 1.7229889635927975e-05
Step: 27540, train/epoch: 6.554021835327148
Step: 27550, train/loss: 0.0
Step: 27550, train/grad_norm: 1.855675968442938e-08
Step: 27550, train/learning_rate: 1.7217991626239382e-05
Step: 27550, train/epoch: 6.55640172958374
Step: 27560, train/loss: 0.0
Step: 27560, train/grad_norm: 0.0028618185315281153
Step: 27560, train/learning_rate: 1.7206091797561385e-05
Step: 27560, train/epoch: 6.558781623840332
Step: 27570, train/loss: 0.0
Step: 27570, train/grad_norm: 8.371461262868252e-06
Step: 27570, train/learning_rate: 1.719419378787279e-05
Step: 27570, train/epoch: 6.561161518096924
Step: 27580, train/loss: 0.0
Step: 27580, train/grad_norm: 2.299568535590879e-09
Step: 27580, train/learning_rate: 1.7182293959194794e-05
Step: 27580, train/epoch: 6.563540935516357
Step: 27590, train/loss: 0.0
Step: 27590, train/grad_norm: 1.6607218356057274e-07
Step: 27590, train/learning_rate: 1.71703959495062e-05
Step: 27590, train/epoch: 6.565920829772949
Step: 27600, train/loss: 0.0
Step: 27600, train/grad_norm: 3.0463514377743195e-08
Step: 27600, train/learning_rate: 1.7158496120828204e-05
Step: 27600, train/epoch: 6.568300724029541
Step: 27610, train/loss: 0.0
Step: 27610, train/grad_norm: 0.0001367964141536504
Step: 27610, train/learning_rate: 1.7146596292150207e-05
Step: 27610, train/epoch: 6.570680618286133
Step: 27620, train/loss: 0.0
Step: 27620, train/grad_norm: 4.5131648221286014e-05
Step: 27620, train/learning_rate: 1.7134698282461613e-05
Step: 27620, train/epoch: 6.573060512542725
Step: 27630, train/loss: 0.0
Step: 27630, train/grad_norm: 1.389333590395836e-07
Step: 27630, train/learning_rate: 1.7122798453783616e-05
Step: 27630, train/epoch: 6.575440406799316
Step: 27640, train/loss: 0.0
Step: 27640, train/grad_norm: 6.072591673955685e-08
Step: 27640, train/learning_rate: 1.7110900444095023e-05
Step: 27640, train/epoch: 6.577820301055908
Step: 27650, train/loss: 0.0
Step: 27650, train/grad_norm: 8.151214814233754e-08
Step: 27650, train/learning_rate: 1.7099000615417026e-05
Step: 27650, train/epoch: 6.580199718475342
Step: 27660, train/loss: 0.0
Step: 27660, train/grad_norm: 2.005979149544146e-05
Step: 27660, train/learning_rate: 1.708710078673903e-05
Step: 27660, train/epoch: 6.582579612731934
Step: 27670, train/loss: 0.0
Step: 27670, train/grad_norm: 0.00014199121505953372
Step: 27670, train/learning_rate: 1.7075202777050436e-05
Step: 27670, train/epoch: 6.584959506988525
Step: 27680, train/loss: 0.0
Step: 27680, train/grad_norm: 5.226601818009158e-09
Step: 27680, train/learning_rate: 1.706330294837244e-05
Step: 27680, train/epoch: 6.587339401245117
Step: 27690, train/loss: 0.0
Step: 27690, train/grad_norm: 5.2866059974121526e-08
Step: 27690, train/learning_rate: 1.7051404938683845e-05
Step: 27690, train/epoch: 6.589719295501709
Step: 27700, train/loss: 0.0
Step: 27700, train/grad_norm: 4.097507371625397e-06
Step: 27700, train/learning_rate: 1.7039505110005848e-05
Step: 27700, train/epoch: 6.592099189758301
Step: 27710, train/loss: 0.0
Step: 27710, train/grad_norm: 6.5305991725495005e-09
Step: 27710, train/learning_rate: 1.702760528132785e-05
Step: 27710, train/epoch: 6.594478607177734
Step: 27720, train/loss: 0.0
Step: 27720, train/grad_norm: 2.7492941256923586e-08
Step: 27720, train/learning_rate: 1.7015707271639258e-05
Step: 27720, train/epoch: 6.596858501434326
Step: 27730, train/loss: 0.0
Step: 27730, train/grad_norm: 0.00014977199316490442
Step: 27730, train/learning_rate: 1.700380744296126e-05
Step: 27730, train/epoch: 6.599238395690918
Step: 27740, train/loss: 0.0
Step: 27740, train/grad_norm: 1.9818544672034477e-07
Step: 27740, train/learning_rate: 1.6991909433272667e-05
Step: 27740, train/epoch: 6.60161828994751
Step: 27750, train/loss: 0.0
Step: 27750, train/grad_norm: 2.569754315118189e-06
Step: 27750, train/learning_rate: 1.698000960459467e-05
Step: 27750, train/epoch: 6.603998184204102
Step: 27760, train/loss: 0.0
Step: 27760, train/grad_norm: 7.142152753658593e-07
Step: 27760, train/learning_rate: 1.6968109775916673e-05
Step: 27760, train/epoch: 6.606378078460693
Step: 27770, train/loss: 0.0
Step: 27770, train/grad_norm: 2.021385512307461e-07
Step: 27770, train/learning_rate: 1.695621176622808e-05
Step: 27770, train/epoch: 6.608757972717285
Step: 27780, train/loss: 0.0
Step: 27780, train/grad_norm: 6.49790763418423e-08
Step: 27780, train/learning_rate: 1.6944311937550083e-05
Step: 27780, train/epoch: 6.611137390136719
Step: 27790, train/loss: 0.0
Step: 27790, train/grad_norm: 7.065640073022905e-09
Step: 27790, train/learning_rate: 1.693241392786149e-05
Step: 27790, train/epoch: 6.6135172843933105
Step: 27800, train/loss: 0.0
Step: 27800, train/grad_norm: 7.783860382915009e-07
Step: 27800, train/learning_rate: 1.6920514099183492e-05
Step: 27800, train/epoch: 6.615897178649902
Step: 27810, train/loss: 0.0
Step: 27810, train/grad_norm: 3.995490089891973e-07
Step: 27810, train/learning_rate: 1.6908614270505495e-05
Step: 27810, train/epoch: 6.618277072906494
Step: 27820, train/loss: 0.0
Step: 27820, train/grad_norm: 2.5780975132505546e-09
Step: 27820, train/learning_rate: 1.6896716260816902e-05
Step: 27820, train/epoch: 6.620656967163086
Step: 27830, train/loss: 0.0
Step: 27830, train/grad_norm: 3.2175441901927115e-06
Step: 27830, train/learning_rate: 1.6884816432138905e-05
Step: 27830, train/epoch: 6.623036861419678
Step: 27840, train/loss: 0.0
Step: 27840, train/grad_norm: 3.872781206837317e-08
Step: 27840, train/learning_rate: 1.687291842245031e-05
Step: 27840, train/epoch: 6.625416278839111
Step: 27850, train/loss: 0.0
Step: 27850, train/grad_norm: 6.422824299079366e-07
Step: 27850, train/learning_rate: 1.6861018593772314e-05
Step: 27850, train/epoch: 6.627796173095703
Step: 27860, train/loss: 0.0
Step: 27860, train/grad_norm: 2.770241735561285e-06
Step: 27860, train/learning_rate: 1.6849118765094317e-05
Step: 27860, train/epoch: 6.630176067352295
Step: 27870, train/loss: 0.0
Step: 27870, train/grad_norm: 1.0904019354107675e-10
Step: 27870, train/learning_rate: 1.6837220755405724e-05
Step: 27870, train/epoch: 6.632555961608887
Step: 27880, train/loss: 0.0
Step: 27880, train/grad_norm: 5.064974550883505e-10
Step: 27880, train/learning_rate: 1.6825320926727727e-05
Step: 27880, train/epoch: 6.6349358558654785
Step: 27890, train/loss: 0.0
Step: 27890, train/grad_norm: 1.0344591828470584e-05
Step: 27890, train/learning_rate: 1.6813422917039134e-05
Step: 27890, train/epoch: 6.63731575012207
Step: 27900, train/loss: 0.0
Step: 27900, train/grad_norm: 7.048092243167048e-07
Step: 27900, train/learning_rate: 1.6801523088361137e-05
Step: 27900, train/epoch: 6.639695167541504
Step: 27910, train/loss: 0.0
Step: 27910, train/grad_norm: 1.063155341540778e-09
Step: 27910, train/learning_rate: 1.678962325968314e-05
Step: 27910, train/epoch: 6.642075061798096
Step: 27920, train/loss: 0.0
Step: 27920, train/grad_norm: 1.1894964700331911e-05
Step: 27920, train/learning_rate: 1.6777725249994546e-05
Step: 27920, train/epoch: 6.6444549560546875
Step: 27930, train/loss: 0.0
Step: 27930, train/grad_norm: 3.922608357243007e-06
Step: 27930, train/learning_rate: 1.676582542131655e-05
Step: 27930, train/epoch: 6.646834850311279
Step: 27940, train/loss: 0.0
Step: 27940, train/grad_norm: 6.857248990854714e-09
Step: 27940, train/learning_rate: 1.6753927411627956e-05
Step: 27940, train/epoch: 6.649214744567871
Step: 27950, train/loss: 0.0
Step: 27950, train/grad_norm: 4.2177515524599585e-07
Step: 27950, train/learning_rate: 1.674202758294996e-05
Step: 27950, train/epoch: 6.651594638824463
Step: 27960, train/loss: 0.0
Step: 27960, train/grad_norm: 0.0006119270692579448
Step: 27960, train/learning_rate: 1.6730127754271962e-05
Step: 27960, train/epoch: 6.653974533081055
Step: 27970, train/loss: 0.0
Step: 27970, train/grad_norm: 6.616323844355065e-06
Step: 27970, train/learning_rate: 1.6718229744583368e-05
Step: 27970, train/epoch: 6.656353950500488
Step: 27980, train/loss: 0.0
Step: 27980, train/grad_norm: 1.0777473846701469e-07
Step: 27980, train/learning_rate: 1.670632991590537e-05
Step: 27980, train/epoch: 6.65873384475708
Step: 27990, train/loss: 0.0
Step: 27990, train/grad_norm: 3.4197042975137038e-09
Step: 27990, train/learning_rate: 1.6694431906216778e-05
Step: 27990, train/epoch: 6.661113739013672
Step: 28000, train/loss: 0.0
Step: 28000, train/grad_norm: 1.2752231492996202e-09
Step: 28000, train/learning_rate: 1.668253207753878e-05
Step: 28000, train/epoch: 6.663493633270264
Step: 28010, train/loss: 0.0
Step: 28010, train/grad_norm: 8.65601123223314e-06
Step: 28010, train/learning_rate: 1.6670632248860784e-05
Step: 28010, train/epoch: 6.6658735275268555
Step: 28020, train/loss: 0.0
Step: 28020, train/grad_norm: 2.6271964159718664e-08
Step: 28020, train/learning_rate: 1.665873423917219e-05
Step: 28020, train/epoch: 6.668253421783447
Step: 28030, train/loss: 0.0
Step: 28030, train/grad_norm: 2.8704101318055564e-09
Step: 28030, train/learning_rate: 1.6646834410494193e-05
Step: 28030, train/epoch: 6.670632839202881
Step: 28040, train/loss: 0.0
Step: 28040, train/grad_norm: 6.96068847005904e-09
Step: 28040, train/learning_rate: 1.66349364008056e-05
Step: 28040, train/epoch: 6.673012733459473
Step: 28050, train/loss: 0.0
Step: 28050, train/grad_norm: 7.072735570545774e-07
Step: 28050, train/learning_rate: 1.6623036572127603e-05
Step: 28050, train/epoch: 6.6753926277160645
Step: 28060, train/loss: 0.0
Step: 28060, train/grad_norm: 7.563282338196586e-07
Step: 28060, train/learning_rate: 1.6611136743449606e-05
Step: 28060, train/epoch: 6.677772521972656
Step: 28070, train/loss: 0.0
Step: 28070, train/grad_norm: 8.164737330673688e-09
Step: 28070, train/learning_rate: 1.6599238733761013e-05
Step: 28070, train/epoch: 6.680152416229248
Step: 28080, train/loss: 0.0
Step: 28080, train/grad_norm: 4.455860680430135e-10
Step: 28080, train/learning_rate: 1.6587338905083016e-05
Step: 28080, train/epoch: 6.68253231048584
Step: 28090, train/loss: 0.0
Step: 28090, train/grad_norm: 8.934350148592785e-07
Step: 28090, train/learning_rate: 1.6575440895394422e-05
Step: 28090, train/epoch: 6.684911727905273
Step: 28100, train/loss: 0.0
Step: 28100, train/grad_norm: 1.1041672021150362e-09
Step: 28100, train/learning_rate: 1.6563541066716425e-05
Step: 28100, train/epoch: 6.687291622161865
Step: 28110, train/loss: 9.999999747378752e-05
Step: 28110, train/grad_norm: 2.1645873857778497e-06
Step: 28110, train/learning_rate: 1.6551641238038428e-05
Step: 28110, train/epoch: 6.689671516418457
Step: 28120, train/loss: 0.0
Step: 28120, train/grad_norm: 2.7091330068174102e-08
Step: 28120, train/learning_rate: 1.6539743228349835e-05
Step: 28120, train/epoch: 6.692051410675049
Step: 28130, train/loss: 0.0
Step: 28130, train/grad_norm: 1.759378420729263e-08
Step: 28130, train/learning_rate: 1.6527843399671838e-05
Step: 28130, train/epoch: 6.694431304931641
Step: 28140, train/loss: 0.0
Step: 28140, train/grad_norm: 5.299749318510294e-05
Step: 28140, train/learning_rate: 1.6515945389983244e-05
Step: 28140, train/epoch: 6.696811199188232
Step: 28150, train/loss: 0.0007999999797903001
Step: 28150, train/grad_norm: 2.2109055905494834e-11
Step: 28150, train/learning_rate: 1.6504045561305247e-05
Step: 28150, train/epoch: 6.699191093444824
Step: 28160, train/loss: 0.0
Step: 28160, train/grad_norm: 1.6227518528921792e-07
Step: 28160, train/learning_rate: 1.649214573262725e-05
Step: 28160, train/epoch: 6.701570510864258
Step: 28170, train/loss: 0.0
Step: 28170, train/grad_norm: 1.176599040420001e-09
Step: 28170, train/learning_rate: 1.6480247722938657e-05
Step: 28170, train/epoch: 6.70395040512085
Step: 28180, train/loss: 0.0
Step: 28180, train/grad_norm: 7.307540386136679e-07
Step: 28180, train/learning_rate: 1.646834789426066e-05
Step: 28180, train/epoch: 6.706330299377441
Step: 28190, train/loss: 0.0
Step: 28190, train/grad_norm: 9.672247031078385e-11
Step: 28190, train/learning_rate: 1.6456449884572066e-05
Step: 28190, train/epoch: 6.708710193634033
Step: 28200, train/loss: 0.0
Step: 28200, train/grad_norm: 1.7682075803548969e-09
Step: 28200, train/learning_rate: 1.644455005589407e-05
Step: 28200, train/epoch: 6.711090087890625
Step: 28210, train/loss: 0.0
Step: 28210, train/grad_norm: 2.2277344413623723e-09
Step: 28210, train/learning_rate: 1.6432650227216072e-05
Step: 28210, train/epoch: 6.713469982147217
Step: 28220, train/loss: 0.0
Step: 28220, train/grad_norm: 4.726998237281066e-10
Step: 28220, train/learning_rate: 1.642075221752748e-05
Step: 28220, train/epoch: 6.71584939956665
Step: 28230, train/loss: 0.0
Step: 28230, train/grad_norm: 3.3132639032373845e-07
Step: 28230, train/learning_rate: 1.6408852388849482e-05
Step: 28230, train/epoch: 6.718229293823242
Step: 28240, train/loss: 9.999999747378752e-05
Step: 28240, train/grad_norm: 6.860755092930049e-06
Step: 28240, train/learning_rate: 1.639695437916089e-05
Step: 28240, train/epoch: 6.720609188079834
Step: 28250, train/loss: 0.0
Step: 28250, train/grad_norm: 4.786658625022255e-09
Step: 28250, train/learning_rate: 1.638505455048289e-05
Step: 28250, train/epoch: 6.722989082336426
Step: 28260, train/loss: 0.0
Step: 28260, train/grad_norm: 9.74465024228266e-07
Step: 28260, train/learning_rate: 1.6373156540794298e-05
Step: 28260, train/epoch: 6.725368976593018
Step: 28270, train/loss: 0.0
Step: 28270, train/grad_norm: 7.29210358763055e-10
Step: 28270, train/learning_rate: 1.63612567121163e-05
Step: 28270, train/epoch: 6.727748870849609
Step: 28280, train/loss: 0.0
Step: 28280, train/grad_norm: 3.4350553651529836e-11
Step: 28280, train/learning_rate: 1.6349356883438304e-05
Step: 28280, train/epoch: 6.730128288269043
Step: 28290, train/loss: 0.0
Step: 28290, train/grad_norm: 2.312396052417398e-09
Step: 28290, train/learning_rate: 1.633745887374971e-05
Step: 28290, train/epoch: 6.732508182525635
Step: 28300, train/loss: 0.0
Step: 28300, train/grad_norm: 1.3159095341164218e-11
Step: 28300, train/learning_rate: 1.6325559045071714e-05
Step: 28300, train/epoch: 6.734888076782227
Step: 28310, train/loss: 0.0
Step: 28310, train/grad_norm: 1.082733014357018e-09
Step: 28310, train/learning_rate: 1.631366103538312e-05
Step: 28310, train/epoch: 6.737267971038818
Step: 28320, train/loss: 0.0
Step: 28320, train/grad_norm: 6.57120802216582e-10
Step: 28320, train/learning_rate: 1.6301761206705123e-05
Step: 28320, train/epoch: 6.73964786529541
Step: 28330, train/loss: 0.0
Step: 28330, train/grad_norm: 4.798915043124907e-08
Step: 28330, train/learning_rate: 1.6289861378027126e-05
Step: 28330, train/epoch: 6.742027759552002
Step: 28340, train/loss: 0.0
Step: 28340, train/grad_norm: 0.0001612167397979647
Step: 28340, train/learning_rate: 1.6277963368338533e-05
Step: 28340, train/epoch: 6.744407653808594
Step: 28350, train/loss: 0.0
Step: 28350, train/grad_norm: 2.364652085251606e-10
Step: 28350, train/learning_rate: 1.6266063539660536e-05
Step: 28350, train/epoch: 6.746787071228027
Step: 28360, train/loss: 0.0
Step: 28360, train/grad_norm: 9.243079218679284e-11
Step: 28360, train/learning_rate: 1.6254165529971942e-05
Step: 28360, train/epoch: 6.749166965484619
Step: 28370, train/loss: 0.0
Step: 28370, train/grad_norm: 2.0265098488181366e-09
Step: 28370, train/learning_rate: 1.6242265701293945e-05
Step: 28370, train/epoch: 6.751546859741211
Step: 28380, train/loss: 0.0
Step: 28380, train/grad_norm: 6.080588810242205e-10
Step: 28380, train/learning_rate: 1.623036587261595e-05
Step: 28380, train/epoch: 6.753926753997803
Step: 28390, train/loss: 0.0
Step: 28390, train/grad_norm: 1.853504727478139e-08
Step: 28390, train/learning_rate: 1.6218467862927355e-05
Step: 28390, train/epoch: 6.7563066482543945
Step: 28400, train/loss: 0.0
Step: 28400, train/grad_norm: 2.254122755118715e-08
Step: 28400, train/learning_rate: 1.6206568034249358e-05
Step: 28400, train/epoch: 6.758686542510986
Step: 28410, train/loss: 0.0
Step: 28410, train/grad_norm: 2.5287359761705375e-08
Step: 28410, train/learning_rate: 1.6194670024560764e-05
Step: 28410, train/epoch: 6.76106595993042
Step: 28420, train/loss: 0.0
Step: 28420, train/grad_norm: 8.542819074364161e-08
Step: 28420, train/learning_rate: 1.6182770195882767e-05
Step: 28420, train/epoch: 6.763445854187012
Step: 28430, train/loss: 0.0
Step: 28430, train/grad_norm: 3.167168957651789e-11
Step: 28430, train/learning_rate: 1.617087036720477e-05
Step: 28430, train/epoch: 6.7658257484436035
Step: 28440, train/loss: 0.0
Step: 28440, train/grad_norm: 5.448883122483039e-09
Step: 28440, train/learning_rate: 1.6158972357516177e-05
Step: 28440, train/epoch: 6.768205642700195
Step: 28450, train/loss: 0.0
Step: 28450, train/grad_norm: 4.537949038763145e-08
Step: 28450, train/learning_rate: 1.614707252883818e-05
Step: 28450, train/epoch: 6.770585536956787
Step: 28460, train/loss: 0.0
Step: 28460, train/grad_norm: 5.037437489363583e-08
Step: 28460, train/learning_rate: 1.6135174519149587e-05
Step: 28460, train/epoch: 6.772965431213379
Step: 28470, train/loss: 0.0
Step: 28470, train/grad_norm: 1.3702770047530066e-05
Step: 28470, train/learning_rate: 1.612327469047159e-05
Step: 28470, train/epoch: 6.7753448486328125
Step: 28480, train/loss: 0.0
Step: 28480, train/grad_norm: 4.8444820377469e-10
Step: 28480, train/learning_rate: 1.6111374861793593e-05
Step: 28480, train/epoch: 6.777724742889404
Step: 28490, train/loss: 0.0
Step: 28490, train/grad_norm: 4.5534491732723836e-07
Step: 28490, train/learning_rate: 1.6099476852105e-05
Step: 28490, train/epoch: 6.780104637145996
Step: 28500, train/loss: 0.0
Step: 28500, train/grad_norm: 3.9907542548345276e-11
Step: 28500, train/learning_rate: 1.6087577023427002e-05
Step: 28500, train/epoch: 6.782484531402588
Step: 28510, train/loss: 0.0
Step: 28510, train/grad_norm: 1.0362228408666851e-07
Step: 28510, train/learning_rate: 1.607567901373841e-05
Step: 28510, train/epoch: 6.78486442565918
Step: 28520, train/loss: 0.0
Step: 28520, train/grad_norm: 3.9701905651945424e-10
Step: 28520, train/learning_rate: 1.606377918506041e-05
Step: 28520, train/epoch: 6.7872443199157715
Step: 28530, train/loss: 0.0
Step: 28530, train/grad_norm: 5.283122206378721e-08
Step: 28530, train/learning_rate: 1.6051879356382415e-05
Step: 28530, train/epoch: 6.789624214172363
Step: 28540, train/loss: 0.0
Step: 28540, train/grad_norm: 2.8062902002190526e-10
Step: 28540, train/learning_rate: 1.603998134669382e-05
Step: 28540, train/epoch: 6.792003631591797
Step: 28550, train/loss: 0.0
Step: 28550, train/grad_norm: 2.689607980776998e-10
Step: 28550, train/learning_rate: 1.6028081518015824e-05
Step: 28550, train/epoch: 6.794383525848389
Step: 28560, train/loss: 0.0
Step: 28560, train/grad_norm: 5.558240090408617e-09
Step: 28560, train/learning_rate: 1.601618350832723e-05
Step: 28560, train/epoch: 6.7967634201049805
Step: 28570, train/loss: 0.0
Step: 28570, train/grad_norm: 9.652697485762474e-08
Step: 28570, train/learning_rate: 1.6004283679649234e-05
Step: 28570, train/epoch: 6.799143314361572
Step: 28580, train/loss: 0.0
Step: 28580, train/grad_norm: 7.936410417741513e-10
Step: 28580, train/learning_rate: 1.5992383850971237e-05
Step: 28580, train/epoch: 6.801523208618164
Step: 28590, train/loss: 0.0
Step: 28590, train/grad_norm: 1.794209048000539e-08
Step: 28590, train/learning_rate: 1.5980485841282643e-05
Step: 28590, train/epoch: 6.803903102874756
Step: 28600, train/loss: 0.0
Step: 28600, train/grad_norm: 1.0153006968494083e-07
Step: 28600, train/learning_rate: 1.5968586012604646e-05
Step: 28600, train/epoch: 6.8062825202941895
Step: 28610, train/loss: 0.0
Step: 28610, train/grad_norm: 3.3683836591080762e-06
Step: 28610, train/learning_rate: 1.5956688002916053e-05
Step: 28610, train/epoch: 6.808662414550781
Step: 28620, train/loss: 0.0
Step: 28620, train/grad_norm: 3.7708385747237116e-08
Step: 28620, train/learning_rate: 1.5944788174238056e-05
Step: 28620, train/epoch: 6.811042308807373
Step: 28630, train/loss: 0.0
Step: 28630, train/grad_norm: 7.443591130140703e-07
Step: 28630, train/learning_rate: 1.593288834556006e-05
Step: 28630, train/epoch: 6.813422203063965
Step: 28640, train/loss: 0.0
Step: 28640, train/grad_norm: 4.960225652439476e-08
Step: 28640, train/learning_rate: 1.5920990335871466e-05
Step: 28640, train/epoch: 6.815802097320557
Step: 28650, train/loss: 0.0
Step: 28650, train/grad_norm: 1.0407531086942257e-10
Step: 28650, train/learning_rate: 1.590909050719347e-05
Step: 28650, train/epoch: 6.818181991577148
Step: 28660, train/loss: 0.0
Step: 28660, train/grad_norm: 3.5697543553959987e-13
Step: 28660, train/learning_rate: 1.5897192497504875e-05
Step: 28660, train/epoch: 6.820561408996582
Step: 28670, train/loss: 0.0
Step: 28670, train/grad_norm: 9.064879923081914e-12
Step: 28670, train/learning_rate: 1.5885292668826878e-05
Step: 28670, train/epoch: 6.822941303253174
Step: 28680, train/loss: 0.0
Step: 28680, train/grad_norm: 6.092759630149658e-10
Step: 28680, train/learning_rate: 1.587339284014888e-05
Step: 28680, train/epoch: 6.825321197509766
Step: 28690, train/loss: 0.0
Step: 28690, train/grad_norm: 4.45524639403061e-09
Step: 28690, train/learning_rate: 1.5861494830460288e-05
Step: 28690, train/epoch: 6.827701091766357
Step: 28700, train/loss: 0.0
Step: 28700, train/grad_norm: 3.4845553525952866e-11
Step: 28700, train/learning_rate: 1.584959500178229e-05
Step: 28700, train/epoch: 6.830080986022949
Step: 28710, train/loss: 0.0
Step: 28710, train/grad_norm: 5.899480903792664e-11
Step: 28710, train/learning_rate: 1.5837696992093697e-05
Step: 28710, train/epoch: 6.832460880279541
Step: 28720, train/loss: 0.0
Step: 28720, train/grad_norm: 3.358485400895006e-07
Step: 28720, train/learning_rate: 1.58257971634157e-05
Step: 28720, train/epoch: 6.834840774536133
Step: 28730, train/loss: 0.0
Step: 28730, train/grad_norm: 4.3630304702446665e-08
Step: 28730, train/learning_rate: 1.5813897334737703e-05
Step: 28730, train/epoch: 6.837220191955566
Step: 28740, train/loss: 0.0
Step: 28740, train/grad_norm: 9.324673255761695e-10
Step: 28740, train/learning_rate: 1.580199932504911e-05
Step: 28740, train/epoch: 6.839600086212158
Step: 28750, train/loss: 0.0
Step: 28750, train/grad_norm: 5.6336359188779994e-12
Step: 28750, train/learning_rate: 1.5790099496371113e-05
Step: 28750, train/epoch: 6.84197998046875
Step: 28760, train/loss: 0.0
Step: 28760, train/grad_norm: 4.077324866624821e-10
Step: 28760, train/learning_rate: 1.577820148668252e-05
Step: 28760, train/epoch: 6.844359874725342
Step: 28770, train/loss: 0.0
Step: 28770, train/grad_norm: 5.924992496630921e-09
Step: 28770, train/learning_rate: 1.5766301658004522e-05
Step: 28770, train/epoch: 6.846739768981934
Step: 28780, train/loss: 0.0
Step: 28780, train/grad_norm: 5.548186798876031e-09
Step: 28780, train/learning_rate: 1.5754401829326525e-05
Step: 28780, train/epoch: 6.849119663238525
Step: 28790, train/loss: 0.0
Step: 28790, train/grad_norm: 2.561416465596267e-07
Step: 28790, train/learning_rate: 1.5742503819637932e-05
Step: 28790, train/epoch: 6.851499080657959
Step: 28800, train/loss: 0.0
Step: 28800, train/grad_norm: 2.2283082046214986e-08
Step: 28800, train/learning_rate: 1.5730603990959935e-05
Step: 28800, train/epoch: 6.853878974914551
Step: 28810, train/loss: 0.0
Step: 28810, train/grad_norm: 2.9328051098787e-10
Step: 28810, train/learning_rate: 1.571870598127134e-05
Step: 28810, train/epoch: 6.856258869171143
Step: 28820, train/loss: 0.0
Step: 28820, train/grad_norm: 3.903346534883667e-09
Step: 28820, train/learning_rate: 1.5706806152593344e-05
Step: 28820, train/epoch: 6.858638763427734
Step: 28830, train/loss: 0.0
Step: 28830, train/grad_norm: 4.036706247045885e-10
Step: 28830, train/learning_rate: 1.5694906323915347e-05
Step: 28830, train/epoch: 6.861018657684326
Step: 28840, train/loss: 0.0
Step: 28840, train/grad_norm: 2.301990403852372e-10
Step: 28840, train/learning_rate: 1.5683008314226754e-05
Step: 28840, train/epoch: 6.863398551940918
Step: 28850, train/loss: 0.0
Step: 28850, train/grad_norm: 1.7529899754009648e-10
Step: 28850, train/learning_rate: 1.5671108485548757e-05
Step: 28850, train/epoch: 6.865777969360352
Step: 28860, train/loss: 0.0
Step: 28860, train/grad_norm: 3.759053583962668e-07
Step: 28860, train/learning_rate: 1.5659210475860164e-05
Step: 28860, train/epoch: 6.868157863616943
Step: 28870, train/loss: 0.0
Step: 28870, train/grad_norm: 6.630870852397663e-10
Step: 28870, train/learning_rate: 1.5647310647182167e-05
Step: 28870, train/epoch: 6.870537757873535
Step: 28880, train/loss: 0.0
Step: 28880, train/grad_norm: 3.006217150214319e-11
Step: 28880, train/learning_rate: 1.563541081850417e-05
Step: 28880, train/epoch: 6.872917652130127
Step: 28890, train/loss: 0.0
Step: 28890, train/grad_norm: 6.932261698011644e-08
Step: 28890, train/learning_rate: 1.5623512808815576e-05
Step: 28890, train/epoch: 6.875297546386719
Step: 28900, train/loss: 0.0
Step: 28900, train/grad_norm: 1.2576502506078668e-08
Step: 28900, train/learning_rate: 1.561161298013758e-05
Step: 28900, train/epoch: 6.8776774406433105
Step: 28910, train/loss: 0.0
Step: 28910, train/grad_norm: 1.1252777598613761e-09
Step: 28910, train/learning_rate: 1.5599714970448986e-05
Step: 28910, train/epoch: 6.880057334899902
Step: 28920, train/loss: 0.0
Step: 28920, train/grad_norm: 1.1340078032162637e-07
Step: 28920, train/learning_rate: 1.558781514177099e-05
Step: 28920, train/epoch: 6.882436752319336
Step: 28930, train/loss: 0.0
Step: 28930, train/grad_norm: 6.075818737016903e-10
Step: 28930, train/learning_rate: 1.5575917132082395e-05
Step: 28930, train/epoch: 6.884816646575928
Step: 28940, train/loss: 0.0
Step: 28940, train/grad_norm: 5.510786138529511e-08
Step: 28940, train/learning_rate: 1.5564017303404398e-05
Step: 28940, train/epoch: 6.8871965408325195
Step: 28950, train/loss: 0.0
Step: 28950, train/grad_norm: 5.246238443668005e-10
Step: 28950, train/learning_rate: 1.55521174747264e-05
Step: 28950, train/epoch: 6.889576435089111
Step: 28960, train/loss: 0.0
Step: 28960, train/grad_norm: 1.1593476179072582e-09
Step: 28960, train/learning_rate: 1.5540219465037808e-05
Step: 28960, train/epoch: 6.891956329345703
Step: 28970, train/loss: 0.0
Step: 28970, train/grad_norm: 3.2650613324136657e-09
Step: 28970, train/learning_rate: 1.552831963635981e-05
Step: 28970, train/epoch: 6.894336223602295
Step: 28980, train/loss: 0.0
Step: 28980, train/grad_norm: 9.816291009212552e-12
Step: 28980, train/learning_rate: 1.5516421626671217e-05
Step: 28980, train/epoch: 6.8967156410217285
Step: 28990, train/loss: 0.0
Step: 28990, train/grad_norm: 5.085387783765327e-06
Step: 28990, train/learning_rate: 1.550452179799322e-05
Step: 28990, train/epoch: 6.89909553527832
Step: 29000, train/loss: 0.0
Step: 29000, train/grad_norm: 3.693904210289034e-11
Step: 29000, train/learning_rate: 1.5492621969315223e-05
Step: 29000, train/epoch: 6.901475429534912
Step: 29010, train/loss: 0.0
Step: 29010, train/grad_norm: 8.015386097903754e-12
Step: 29010, train/learning_rate: 1.548072395962663e-05
Step: 29010, train/epoch: 6.903855323791504
Step: 29020, train/loss: 0.0
Step: 29020, train/grad_norm: 6.833736354572295e-10
Step: 29020, train/learning_rate: 1.5468824130948633e-05
Step: 29020, train/epoch: 6.906235218048096
Step: 29030, train/loss: 0.0
Step: 29030, train/grad_norm: 3.78750053542376e-09
Step: 29030, train/learning_rate: 1.545692612126004e-05
Step: 29030, train/epoch: 6.9086151123046875
Step: 29040, train/loss: 0.0
Step: 29040, train/grad_norm: 1.0455081245197562e-09
Step: 29040, train/learning_rate: 1.5445026292582043e-05
Step: 29040, train/epoch: 6.910994529724121
Step: 29050, train/loss: 0.0
Step: 29050, train/grad_norm: 2.037052526659977e-09
Step: 29050, train/learning_rate: 1.5433126463904046e-05
Step: 29050, train/epoch: 6.913374423980713
Step: 29060, train/loss: 0.0
Step: 29060, train/grad_norm: 4.90218317328317e-11
Step: 29060, train/learning_rate: 1.5421228454215452e-05
Step: 29060, train/epoch: 6.915754318237305
Step: 29070, train/loss: 0.0
Step: 29070, train/grad_norm: 7.637324017650826e-08
Step: 29070, train/learning_rate: 1.5409328625537455e-05
Step: 29070, train/epoch: 6.9181342124938965
Step: 29080, train/loss: 0.0
Step: 29080, train/grad_norm: 4.3008067223127e-06
Step: 29080, train/learning_rate: 1.539743061584886e-05
Step: 29080, train/epoch: 6.920514106750488
Step: 29090, train/loss: 0.0
Step: 29090, train/grad_norm: 1.1680582723272437e-08
Step: 29090, train/learning_rate: 1.5385530787170865e-05
Step: 29090, train/epoch: 6.92289400100708
Step: 29100, train/loss: 0.0
Step: 29100, train/grad_norm: 9.061375955132789e-11
Step: 29100, train/learning_rate: 1.5373630958492868e-05
Step: 29100, train/epoch: 6.925273895263672
Step: 29110, train/loss: 0.0
Step: 29110, train/grad_norm: 5.830974370013564e-09
Step: 29110, train/learning_rate: 1.5361732948804274e-05
Step: 29110, train/epoch: 6.9276533126831055
Step: 29120, train/loss: 0.0
Step: 29120, train/grad_norm: 1.1449830666920846e-11
Step: 29120, train/learning_rate: 1.5349833120126277e-05
Step: 29120, train/epoch: 6.930033206939697
Step: 29130, train/loss: 0.0
Step: 29130, train/grad_norm: 4.5769370515813534e-09
Step: 29130, train/learning_rate: 1.5337935110437684e-05
Step: 29130, train/epoch: 6.932413101196289
Step: 29140, train/loss: 0.0
Step: 29140, train/grad_norm: 5.01476415948332e-09
Step: 29140, train/learning_rate: 1.5326035281759687e-05
Step: 29140, train/epoch: 6.934792995452881
Step: 29150, train/loss: 0.0
Step: 29150, train/grad_norm: 1.1441208016549353e-06
Step: 29150, train/learning_rate: 1.531413545308169e-05
Step: 29150, train/epoch: 6.937172889709473
Step: 29160, train/loss: 0.0
Step: 29160, train/grad_norm: 1.5750203604444207e-13
Step: 29160, train/learning_rate: 1.5302237443393096e-05
Step: 29160, train/epoch: 6.9395527839660645
Step: 29170, train/loss: 0.0
Step: 29170, train/grad_norm: 2.1452061105264875e-07
Step: 29170, train/learning_rate: 1.52903376147151e-05
Step: 29170, train/epoch: 6.941932201385498
Step: 29180, train/loss: 0.0
Step: 29180, train/grad_norm: 1.2478379440850063e-10
Step: 29180, train/learning_rate: 1.5278439605026506e-05
Step: 29180, train/epoch: 6.94431209564209
Step: 29190, train/loss: 0.0
Step: 29190, train/grad_norm: 3.0521715797121596e-12
Step: 29190, train/learning_rate: 1.526653977634851e-05
Step: 29190, train/epoch: 6.946691989898682
Step: 29200, train/loss: 0.0
Step: 29200, train/grad_norm: 8.097345571833614e-10
Step: 29200, train/learning_rate: 1.5254640857165214e-05
Step: 29200, train/epoch: 6.949071884155273
Step: 29210, train/loss: 0.0
Step: 29210, train/grad_norm: 2.263994686169113e-10
Step: 29210, train/learning_rate: 1.5242741937981918e-05
Step: 29210, train/epoch: 6.951451778411865
Step: 29220, train/loss: 0.0
Step: 29220, train/grad_norm: 0.000619735976215452
Step: 29220, train/learning_rate: 1.5230842109303921e-05
Step: 29220, train/epoch: 6.953831672668457
Step: 29230, train/loss: 0.0
Step: 29230, train/grad_norm: 1.2785410063997915e-08
Step: 29230, train/learning_rate: 1.5218943190120626e-05
Step: 29230, train/epoch: 6.956211090087891
Step: 29240, train/loss: 0.0
Step: 29240, train/grad_norm: 3.655387814660571e-08
Step: 29240, train/learning_rate: 1.5207044270937331e-05
Step: 29240, train/epoch: 6.958590984344482
Step: 29250, train/loss: 0.0
Step: 29250, train/grad_norm: 7.782076863804832e-06
Step: 29250, train/learning_rate: 1.5195145351754036e-05
Step: 29250, train/epoch: 6.960970878601074
Step: 29260, train/loss: 0.0
Step: 29260, train/grad_norm: 1.9590483402165404e-10
Step: 29260, train/learning_rate: 1.518324643257074e-05
Step: 29260, train/epoch: 6.963350772857666
Step: 29270, train/loss: 9.999999747378752e-05
Step: 29270, train/grad_norm: 6.134488330644672e-07
Step: 29270, train/learning_rate: 1.5171346603892744e-05
Step: 29270, train/epoch: 6.965730667114258
Step: 29280, train/loss: 0.0
Step: 29280, train/grad_norm: 1.059183816209952e-07
Step: 29280, train/learning_rate: 1.5159447684709448e-05
Step: 29280, train/epoch: 6.96811056137085
Step: 29290, train/loss: 0.016699999570846558
Step: 29290, train/grad_norm: 1.3024293643582041e-08
Step: 29290, train/learning_rate: 1.5147548765526153e-05
Step: 29290, train/epoch: 6.970490455627441
Step: 29300, train/loss: 0.0
Step: 29300, train/grad_norm: 6.364086480026288e-10
Step: 29300, train/learning_rate: 1.5135649846342858e-05
Step: 29300, train/epoch: 6.972869873046875
Step: 29310, train/loss: 0.0
Step: 29310, train/grad_norm: 1.2774959259331808e-06
Step: 29310, train/learning_rate: 1.5123750927159563e-05
Step: 29310, train/epoch: 6.975249767303467
Step: 29320, train/loss: 0.0
Step: 29320, train/grad_norm: 2.488191057636868e-07
Step: 29320, train/learning_rate: 1.5111851098481566e-05
Step: 29320, train/epoch: 6.977629661560059
Step: 29330, train/loss: 0.0
Step: 29330, train/grad_norm: 0.00012885520118288696
Step: 29330, train/learning_rate: 1.509995217929827e-05
Step: 29330, train/epoch: 6.98000955581665
Step: 29340, train/loss: 0.0
Step: 29340, train/grad_norm: 3.540285220537953e-08
Step: 29340, train/learning_rate: 1.5088053260114975e-05
Step: 29340, train/epoch: 6.982389450073242
Step: 29350, train/loss: 0.0
Step: 29350, train/grad_norm: 3.7800690355638267e-10
Step: 29350, train/learning_rate: 1.507615434093168e-05
Step: 29350, train/epoch: 6.984769344329834
Step: 29360, train/loss: 0.0
Step: 29360, train/grad_norm: 3.2788549653162136e-09
Step: 29360, train/learning_rate: 1.5064255421748385e-05
Step: 29360, train/epoch: 6.987148761749268
Step: 29370, train/loss: 0.0005000000237487257
Step: 29370, train/grad_norm: 1.3442232882354688e-09
Step: 29370, train/learning_rate: 1.5052355593070388e-05
Step: 29370, train/epoch: 6.989528656005859
Step: 29380, train/loss: 0.0
Step: 29380, train/grad_norm: 8.786490798229352e-05
Step: 29380, train/learning_rate: 1.5040456673887093e-05
Step: 29380, train/epoch: 6.991908550262451
Step: 29390, train/loss: 0.0
Step: 29390, train/grad_norm: 2.5459295116547764e-08
Step: 29390, train/learning_rate: 1.5028557754703797e-05
Step: 29390, train/epoch: 6.994288444519043
Step: 29400, train/loss: 0.0
Step: 29400, train/grad_norm: 1.0782747958404393e-11
Step: 29400, train/learning_rate: 1.5016658835520502e-05
Step: 29400, train/epoch: 6.996668338775635
Step: 29410, train/loss: 0.0
Step: 29410, train/grad_norm: 3.9717426147944934e-07
Step: 29410, train/learning_rate: 1.5004759916337207e-05
Step: 29410, train/epoch: 6.999048233032227
Step: 29414, eval/loss: 0.04948590323328972
Step: 29414, eval/accuracy: 0.995557427406311
Step: 29414, eval/f1: 0.995302140712738
Step: 29414, eval/runtime: 707.91748046875
Step: 29414, eval/samples_per_second: 10.175000190734863
Step: 29414, eval/steps_per_second: 1.2730000019073486
Step: 29414, train/epoch: 7.0
Step: 29420, train/loss: 0.0
Step: 29420, train/grad_norm: 4.49711450301038e-08
Step: 29420, train/learning_rate: 1.4992860997153912e-05
Step: 29420, train/epoch: 7.001428127288818
Step: 29430, train/loss: 0.0
Step: 29430, train/grad_norm: 6.305085620272877e-11
Step: 29430, train/learning_rate: 1.4980961168475915e-05
Step: 29430, train/epoch: 7.003807544708252
Step: 29440, train/loss: 0.0
Step: 29440, train/grad_norm: 3.4254872272043713e-09
Step: 29440, train/learning_rate: 1.496906224929262e-05
Step: 29440, train/epoch: 7.006187438964844
Step: 29450, train/loss: 0.0
Step: 29450, train/grad_norm: 3.6170515471667386e-08
Step: 29450, train/learning_rate: 1.4957163330109324e-05
Step: 29450, train/epoch: 7.0085673332214355
Step: 29460, train/loss: 0.0
Step: 29460, train/grad_norm: 1.2268713156515787e-08
Step: 29460, train/learning_rate: 1.4945264410926029e-05
Step: 29460, train/epoch: 7.010947227478027
Step: 29470, train/loss: 0.0
Step: 29470, train/grad_norm: 1.2692594753005437e-09
Step: 29470, train/learning_rate: 1.4933365491742734e-05
Step: 29470, train/epoch: 7.013327121734619
Step: 29480, train/loss: 0.0
Step: 29480, train/grad_norm: 2.047451808095957e-08
Step: 29480, train/learning_rate: 1.4921465663064737e-05
Step: 29480, train/epoch: 7.015707015991211
Step: 29490, train/loss: 0.0
Step: 29490, train/grad_norm: 7.365433596362436e-09
Step: 29490, train/learning_rate: 1.4909566743881442e-05
Step: 29490, train/epoch: 7.0180864334106445
Step: 29500, train/loss: 0.0
Step: 29500, train/grad_norm: 5.73243390533662e-08
Step: 29500, train/learning_rate: 1.4897667824698146e-05
Step: 29500, train/epoch: 7.020466327667236
Step: 29510, train/loss: 0.0
Step: 29510, train/grad_norm: 7.797691796440631e-05
Step: 29510, train/learning_rate: 1.4885768905514851e-05
Step: 29510, train/epoch: 7.022846221923828
Step: 29520, train/loss: 0.0
Step: 29520, train/grad_norm: 7.204097318691538e-10
Step: 29520, train/learning_rate: 1.4873869986331556e-05
Step: 29520, train/epoch: 7.02522611618042
Step: 29530, train/loss: 0.0
Step: 29530, train/grad_norm: 1.5010109564173035e-06
Step: 29530, train/learning_rate: 1.4861970157653559e-05
Step: 29530, train/epoch: 7.027606010437012
Step: 29540, train/loss: 0.0
Step: 29540, train/grad_norm: 1.0376409731449598e-09
Step: 29540, train/learning_rate: 1.4850071238470264e-05
Step: 29540, train/epoch: 7.0299859046936035
Step: 29550, train/loss: 0.0
Step: 29550, train/grad_norm: 1.6265510112134507e-06
Step: 29550, train/learning_rate: 1.4838172319286969e-05
Step: 29550, train/epoch: 7.032365322113037
Step: 29560, train/loss: 0.0
Step: 29560, train/grad_norm: 5.541317960044978e-10
Step: 29560, train/learning_rate: 1.4826273400103673e-05
Step: 29560, train/epoch: 7.034745216369629
Step: 29570, train/loss: 0.0
Step: 29570, train/grad_norm: 3.0596979172514693e-08
Step: 29570, train/learning_rate: 1.4814374480920378e-05
Step: 29570, train/epoch: 7.037125110626221
Step: 29580, train/loss: 0.0
Step: 29580, train/grad_norm: 6.169483257600916e-10
Step: 29580, train/learning_rate: 1.4802474652242381e-05
Step: 29580, train/epoch: 7.0395050048828125
Step: 29590, train/loss: 0.0
Step: 29590, train/grad_norm: 1.0940940375903097e-09
Step: 29590, train/learning_rate: 1.4790575733059086e-05
Step: 29590, train/epoch: 7.041884899139404
Step: 29600, train/loss: 0.0
Step: 29600, train/grad_norm: 2.484282527956405e-12
Step: 29600, train/learning_rate: 1.477867681387579e-05
Step: 29600, train/epoch: 7.044264793395996
Step: 29610, train/loss: 0.0
Step: 29610, train/grad_norm: 1.1652044884513657e-09
Step: 29610, train/learning_rate: 1.4766777894692495e-05
Step: 29610, train/epoch: 7.046644687652588
Step: 29620, train/loss: 0.0
Step: 29620, train/grad_norm: 2.535662124714122e-09
Step: 29620, train/learning_rate: 1.47548789755092e-05
Step: 29620, train/epoch: 7.0490241050720215
Step: 29630, train/loss: 0.0
Step: 29630, train/grad_norm: 4.0497365461078516e-08
Step: 29630, train/learning_rate: 1.4742979146831203e-05
Step: 29630, train/epoch: 7.051403999328613
Step: 29640, train/loss: 0.0
Step: 29640, train/grad_norm: 1.5001532460701128e-07
Step: 29640, train/learning_rate: 1.4731080227647908e-05
Step: 29640, train/epoch: 7.053783893585205
Step: 29650, train/loss: 0.0
Step: 29650, train/grad_norm: 6.233460969617965e-11
Step: 29650, train/learning_rate: 1.4719181308464613e-05
Step: 29650, train/epoch: 7.056163787841797
Step: 29660, train/loss: 0.0
Step: 29660, train/grad_norm: 1.3743478355365824e-08
Step: 29660, train/learning_rate: 1.4707282389281318e-05
Step: 29660, train/epoch: 7.058543682098389
Step: 29670, train/loss: 0.0
Step: 29670, train/grad_norm: 3.066612935809232e-11
Step: 29670, train/learning_rate: 1.4695383470098022e-05
Step: 29670, train/epoch: 7.0609235763549805
Step: 29680, train/loss: 0.0
Step: 29680, train/grad_norm: 8.020066273421722e-13
Step: 29680, train/learning_rate: 1.4683483641420025e-05
Step: 29680, train/epoch: 7.063302993774414
Step: 29690, train/loss: 0.0
Step: 29690, train/grad_norm: 2.2512375297267795e-10
Step: 29690, train/learning_rate: 1.467158472223673e-05
Step: 29690, train/epoch: 7.065682888031006
Step: 29700, train/loss: 0.0
Step: 29700, train/grad_norm: 8.255589705186139e-07
Step: 29700, train/learning_rate: 1.4659685803053435e-05
Step: 29700, train/epoch: 7.068062782287598
Step: 29710, train/loss: 0.0
Step: 29710, train/grad_norm: 4.4688150069305266e-08
Step: 29710, train/learning_rate: 1.464778688387014e-05
Step: 29710, train/epoch: 7.0704426765441895
Step: 29720, train/loss: 0.0
Step: 29720, train/grad_norm: 1.3919606800527617e-08
Step: 29720, train/learning_rate: 1.4635887964686844e-05
Step: 29720, train/epoch: 7.072822570800781
Step: 29730, train/loss: 0.0
Step: 29730, train/grad_norm: 3.173818763357872e-09
Step: 29730, train/learning_rate: 1.4623988136008848e-05
Step: 29730, train/epoch: 7.075202465057373
Step: 29740, train/loss: 0.0
Step: 29740, train/grad_norm: 3.086604181135044e-07
Step: 29740, train/learning_rate: 1.4612089216825552e-05
Step: 29740, train/epoch: 7.077581882476807
Step: 29750, train/loss: 0.0
Step: 29750, train/grad_norm: 6.808308139483188e-09
Step: 29750, train/learning_rate: 1.4600190297642257e-05
Step: 29750, train/epoch: 7.079961776733398
Step: 29760, train/loss: 0.0
Step: 29760, train/grad_norm: 2.9899049991399806e-07
Step: 29760, train/learning_rate: 1.4588291378458962e-05
Step: 29760, train/epoch: 7.08234167098999
Step: 29770, train/loss: 0.0
Step: 29770, train/grad_norm: 2.1454308551938084e-08
Step: 29770, train/learning_rate: 1.4576392459275667e-05
Step: 29770, train/epoch: 7.084721565246582
Step: 29780, train/loss: 0.0
Step: 29780, train/grad_norm: 9.538966061484189e-09
Step: 29780, train/learning_rate: 1.4564493540092371e-05
Step: 29780, train/epoch: 7.087101459503174
Step: 29790, train/loss: 0.0
Step: 29790, train/grad_norm: 9.050970106727618e-08
Step: 29790, train/learning_rate: 1.4552593711414374e-05
Step: 29790, train/epoch: 7.089481353759766
Step: 29800, train/loss: 0.0
Step: 29800, train/grad_norm: 9.43603950531724e-09
Step: 29800, train/learning_rate: 1.454069479223108e-05
Step: 29800, train/epoch: 7.091861248016357
Step: 29810, train/loss: 0.0
Step: 29810, train/grad_norm: 5.489752055720665e-11
Step: 29810, train/learning_rate: 1.4528795873047784e-05
Step: 29810, train/epoch: 7.094240665435791
Step: 29820, train/loss: 0.0
Step: 29820, train/grad_norm: 3.676315363243532e-10
Step: 29820, train/learning_rate: 1.4516896953864489e-05
Step: 29820, train/epoch: 7.096620559692383
Step: 29830, train/loss: 0.0
Step: 29830, train/grad_norm: 4.1357754894388865e-12
Step: 29830, train/learning_rate: 1.4504998034681194e-05
Step: 29830, train/epoch: 7.099000453948975
Step: 29840, train/loss: 0.0
Step: 29840, train/grad_norm: 4.1670206174515556e-10
Step: 29840, train/learning_rate: 1.4493098206003197e-05
Step: 29840, train/epoch: 7.101380348205566
Step: 29850, train/loss: 0.0
Step: 29850, train/grad_norm: 2.798467946263372e-08
Step: 29850, train/learning_rate: 1.4481199286819901e-05
Step: 29850, train/epoch: 7.103760242462158
Step: 29860, train/loss: 0.0
Step: 29860, train/grad_norm: 9.135858647368877e-08
Step: 29860, train/learning_rate: 1.4469300367636606e-05
Step: 29860, train/epoch: 7.10614013671875
Step: 29870, train/loss: 0.0
Step: 29870, train/grad_norm: 1.2700320795033804e-09
Step: 29870, train/learning_rate: 1.4457401448453311e-05
Step: 29870, train/epoch: 7.108519554138184
Step: 29880, train/loss: 0.0
Step: 29880, train/grad_norm: 5.128218405481277e-10
Step: 29880, train/learning_rate: 1.4445502529270016e-05
Step: 29880, train/epoch: 7.110899448394775
Step: 29890, train/loss: 0.0
Step: 29890, train/grad_norm: 9.80052504928608e-07
Step: 29890, train/learning_rate: 1.4433602700592019e-05
Step: 29890, train/epoch: 7.113279342651367
Step: 29900, train/loss: 0.0
Step: 29900, train/grad_norm: 6.415160069828119e-11
Step: 29900, train/learning_rate: 1.4421703781408723e-05
Step: 29900, train/epoch: 7.115659236907959
Step: 29910, train/loss: 0.0
Step: 29910, train/grad_norm: 6.85380570075722e-08
Step: 29910, train/learning_rate: 1.4409804862225428e-05
Step: 29910, train/epoch: 7.118039131164551
Step: 29920, train/loss: 0.0
Step: 29920, train/grad_norm: 3.727928898911159e-08
Step: 29920, train/learning_rate: 1.4397905943042133e-05
Step: 29920, train/epoch: 7.120419025421143
Step: 29930, train/loss: 0.0
Step: 29930, train/grad_norm: 9.661330069299368e-11
Step: 29930, train/learning_rate: 1.4386007023858838e-05
Step: 29930, train/epoch: 7.122798442840576
Step: 29940, train/loss: 0.0
Step: 29940, train/grad_norm: 3.3651159636605144e-10
Step: 29940, train/learning_rate: 1.437410719518084e-05
Step: 29940, train/epoch: 7.125178337097168
Step: 29950, train/loss: 0.0
Step: 29950, train/grad_norm: 3.3045073450921336e-06
Step: 29950, train/learning_rate: 1.4362208275997546e-05
Step: 29950, train/epoch: 7.12755823135376
Step: 29960, train/loss: 0.0
Step: 29960, train/grad_norm: 2.0623288321530708e-07
Step: 29960, train/learning_rate: 1.435030935681425e-05
Step: 29960, train/epoch: 7.129938125610352
Step: 29970, train/loss: 0.0
Step: 29970, train/grad_norm: 3.584259644640042e-08
Step: 29970, train/learning_rate: 1.4338410437630955e-05
Step: 29970, train/epoch: 7.132318019866943
Step: 29980, train/loss: 0.0
Step: 29980, train/grad_norm: 2.666116494243198e-10
Step: 29980, train/learning_rate: 1.432651151844766e-05
Step: 29980, train/epoch: 7.134697914123535
Step: 29990, train/loss: 0.0
Step: 29990, train/grad_norm: 5.112790746331086e-11
Step: 29990, train/learning_rate: 1.4314611689769663e-05
Step: 29990, train/epoch: 7.137077808380127
Step: 30000, train/loss: 0.0
Step: 30000, train/grad_norm: 1.464880217127984e-09
Step: 30000, train/learning_rate: 1.4302712770586368e-05
Step: 30000, train/epoch: 7.1394572257995605
Step: 30010, train/loss: 0.0
Step: 30010, train/grad_norm: 9.455560778803829e-10
Step: 30010, train/learning_rate: 1.4290813851403072e-05
Step: 30010, train/epoch: 7.141837120056152
Step: 30020, train/loss: 0.0
Step: 30020, train/grad_norm: 2.1557547370321117e-05
Step: 30020, train/learning_rate: 1.4278914932219777e-05
Step: 30020, train/epoch: 7.144217014312744
Step: 30030, train/loss: 0.0
Step: 30030, train/grad_norm: 8.964759490304175e-12
Step: 30030, train/learning_rate: 1.4267016013036482e-05
Step: 30030, train/epoch: 7.146596908569336
Step: 30040, train/loss: 0.0
Step: 30040, train/grad_norm: 1.2130631568685324e-11
Step: 30040, train/learning_rate: 1.4255116184358485e-05
Step: 30040, train/epoch: 7.148976802825928
Step: 30050, train/loss: 0.0
Step: 30050, train/grad_norm: 2.449443314844757e-09
Step: 30050, train/learning_rate: 1.424321726517519e-05
Step: 30050, train/epoch: 7.1513566970825195
Step: 30060, train/loss: 0.0
Step: 30060, train/grad_norm: 1.1728409909927962e-10
Step: 30060, train/learning_rate: 1.4231318345991895e-05
Step: 30060, train/epoch: 7.153736114501953
Step: 30070, train/loss: 0.0
Step: 30070, train/grad_norm: 4.1634012210023386e-11
Step: 30070, train/learning_rate: 1.42194194268086e-05
Step: 30070, train/epoch: 7.156116008758545
Step: 30080, train/loss: 0.0
Step: 30080, train/grad_norm: 6.637066451986584e-09
Step: 30080, train/learning_rate: 1.4207520507625304e-05
Step: 30080, train/epoch: 7.158495903015137
Step: 30090, train/loss: 0.0
Step: 30090, train/grad_norm: 3.8537531499294175e-10
Step: 30090, train/learning_rate: 1.4195620678947307e-05
Step: 30090, train/epoch: 7.1608757972717285
Step: 30100, train/loss: 0.0
Step: 30100, train/grad_norm: 2.963373296749339e-11
Step: 30100, train/learning_rate: 1.4183721759764012e-05
Step: 30100, train/epoch: 7.16325569152832
Step: 30110, train/loss: 0.0
Step: 30110, train/grad_norm: 0.0010815885616466403
Step: 30110, train/learning_rate: 1.4171822840580717e-05
Step: 30110, train/epoch: 7.165635585784912
Step: 30120, train/loss: 0.0
Step: 30120, train/grad_norm: 9.537155787331386e-11
Step: 30120, train/learning_rate: 1.4159923921397422e-05
Step: 30120, train/epoch: 7.168015003204346
Step: 30130, train/loss: 0.0
Step: 30130, train/grad_norm: 6.518031253621714e-11
Step: 30130, train/learning_rate: 1.4148025002214126e-05
Step: 30130, train/epoch: 7.1703948974609375
Step: 30140, train/loss: 0.0
Step: 30140, train/grad_norm: 2.064936416301677e-11
Step: 30140, train/learning_rate: 1.4136126083030831e-05
Step: 30140, train/epoch: 7.172774791717529
Step: 30150, train/loss: 0.0
Step: 30150, train/grad_norm: 2.666955651875469e-06
Step: 30150, train/learning_rate: 1.4124226254352834e-05
Step: 30150, train/epoch: 7.175154685974121
Step: 30160, train/loss: 0.0
Step: 30160, train/grad_norm: 1.6522667565729843e-12
Step: 30160, train/learning_rate: 1.4112327335169539e-05
Step: 30160, train/epoch: 7.177534580230713
Step: 30170, train/loss: 0.0
Step: 30170, train/grad_norm: 2.03611083549049e-09
Step: 30170, train/learning_rate: 1.4100428415986244e-05
Step: 30170, train/epoch: 7.179914474487305
Step: 30180, train/loss: 0.0
Step: 30180, train/grad_norm: 6.136651603538823e-11
Step: 30180, train/learning_rate: 1.4088529496802948e-05
Step: 30180, train/epoch: 7.1822943687438965
Step: 30190, train/loss: 0.0
Step: 30190, train/grad_norm: 1.3203205639911175e-07
Step: 30190, train/learning_rate: 1.4076630577619653e-05
Step: 30190, train/epoch: 7.18467378616333
Step: 30200, train/loss: 0.0
Step: 30200, train/grad_norm: 8.09059292805614e-06
Step: 30200, train/learning_rate: 1.4064730748941656e-05
Step: 30200, train/epoch: 7.187053680419922
Step: 30210, train/loss: 0.0
Step: 30210, train/grad_norm: 8.961410780106149e-11
Step: 30210, train/learning_rate: 1.4052831829758361e-05
Step: 30210, train/epoch: 7.189433574676514
Step: 30220, train/loss: 0.0
Step: 30220, train/grad_norm: 2.138024424169771e-09
Step: 30220, train/learning_rate: 1.4040932910575066e-05
Step: 30220, train/epoch: 7.1918134689331055
Step: 30230, train/loss: 0.0
Step: 30230, train/grad_norm: 8.386882655031513e-08
Step: 30230, train/learning_rate: 1.402903399139177e-05
Step: 30230, train/epoch: 7.194193363189697
Step: 30240, train/loss: 0.0
Step: 30240, train/grad_norm: 3.028616646361115e-08
Step: 30240, train/learning_rate: 1.4017135072208475e-05
Step: 30240, train/epoch: 7.196573257446289
Step: 30250, train/loss: 0.0
Step: 30250, train/grad_norm: 3.517186195067801e-11
Step: 30250, train/learning_rate: 1.4005235243530478e-05
Step: 30250, train/epoch: 7.198952674865723
Step: 30260, train/loss: 0.0
Step: 30260, train/grad_norm: 8.95239958553784e-12
Step: 30260, train/learning_rate: 1.3993336324347183e-05
Step: 30260, train/epoch: 7.2013325691223145
Step: 30270, train/loss: 0.0
Step: 30270, train/grad_norm: 1.000509541881911e-06
Step: 30270, train/learning_rate: 1.3981437405163888e-05
Step: 30270, train/epoch: 7.203712463378906
Step: 30280, train/loss: 0.0
Step: 30280, train/grad_norm: 2.6023408281616867e-06
Step: 30280, train/learning_rate: 1.3969538485980593e-05
Step: 30280, train/epoch: 7.206092357635498
Step: 30290, train/loss: 0.0
Step: 30290, train/grad_norm: 1.7564849130735638e-08
Step: 30290, train/learning_rate: 1.3957639566797297e-05
Step: 30290, train/epoch: 7.20847225189209
Step: 30300, train/loss: 0.0
Step: 30300, train/grad_norm: 1.3089074002436973e-07
Step: 30300, train/learning_rate: 1.39457397381193e-05
Step: 30300, train/epoch: 7.210852146148682
Step: 30310, train/loss: 0.0
Step: 30310, train/grad_norm: 1.9446559917923878e-08
Step: 30310, train/learning_rate: 1.3933840818936005e-05
Step: 30310, train/epoch: 7.213231563568115
Step: 30320, train/loss: 0.0
Step: 30320, train/grad_norm: 9.913003057304692e-11
Step: 30320, train/learning_rate: 1.392194189975271e-05
Step: 30320, train/epoch: 7.215611457824707
Step: 30330, train/loss: 0.0
Step: 30330, train/grad_norm: 5.449168893889578e-10
Step: 30330, train/learning_rate: 1.3910042980569415e-05
Step: 30330, train/epoch: 7.217991352081299
Step: 30340, train/loss: 0.0
Step: 30340, train/grad_norm: 4.4693343470569857e-10
Step: 30340, train/learning_rate: 1.389814406138612e-05
Step: 30340, train/epoch: 7.220371246337891
Step: 30350, train/loss: 0.0
Step: 30350, train/grad_norm: 1.9885856850088146e-10
Step: 30350, train/learning_rate: 1.3886244232708123e-05
Step: 30350, train/epoch: 7.222751140594482
Step: 30360, train/loss: 0.0
Step: 30360, train/grad_norm: 5.4573023183790426e-11
Step: 30360, train/learning_rate: 1.3874345313524827e-05
Step: 30360, train/epoch: 7.225131034851074
Step: 30370, train/loss: 0.0
Step: 30370, train/grad_norm: 3.71755670691698e-09
Step: 30370, train/learning_rate: 1.3862446394341532e-05
Step: 30370, train/epoch: 7.227510929107666
Step: 30380, train/loss: 0.0
Step: 30380, train/grad_norm: 5.661044724547537e-06
Step: 30380, train/learning_rate: 1.3850547475158237e-05
Step: 30380, train/epoch: 7.2298903465271
Step: 30390, train/loss: 0.0
Step: 30390, train/grad_norm: 2.2088077200610456e-10
Step: 30390, train/learning_rate: 1.3838648555974942e-05
Step: 30390, train/epoch: 7.232270240783691
Step: 30400, train/loss: 0.0
Step: 30400, train/grad_norm: 1.4881616772211004e-10
Step: 30400, train/learning_rate: 1.3826748727296945e-05
Step: 30400, train/epoch: 7.234650135040283
Step: 30410, train/loss: 0.0
Step: 30410, train/grad_norm: 4.7456603091688976e-09
Step: 30410, train/learning_rate: 1.381484980811365e-05
Step: 30410, train/epoch: 7.237030029296875
Step: 30420, train/loss: 0.0
Step: 30420, train/grad_norm: 1.1336459548871858e-11
Step: 30420, train/learning_rate: 1.3802950888930354e-05
Step: 30420, train/epoch: 7.239409923553467
Step: 30430, train/loss: 0.0
Step: 30430, train/grad_norm: 2.628529838233362e-09
Step: 30430, train/learning_rate: 1.3791051969747059e-05
Step: 30430, train/epoch: 7.241789817810059
Step: 30440, train/loss: 0.0
Step: 30440, train/grad_norm: 4.953984333866401e-09
Step: 30440, train/learning_rate: 1.3779153050563764e-05
Step: 30440, train/epoch: 7.244169235229492
Step: 30450, train/loss: 0.0
Step: 30450, train/grad_norm: 2.239917001878311e-10
Step: 30450, train/learning_rate: 1.3767254131380469e-05
Step: 30450, train/epoch: 7.246549129486084
Step: 30460, train/loss: 0.0
Step: 30460, train/grad_norm: 1.4336623554989103e-10
Step: 30460, train/learning_rate: 1.3755354302702472e-05
Step: 30460, train/epoch: 7.248929023742676
Step: 30470, train/loss: 0.0
Step: 30470, train/grad_norm: 8.05119739766269e-08
Step: 30470, train/learning_rate: 1.3743455383519176e-05
Step: 30470, train/epoch: 7.251308917999268
Step: 30480, train/loss: 0.0
Step: 30480, train/grad_norm: 1.3638883078481712e-10
Step: 30480, train/learning_rate: 1.3731556464335881e-05
Step: 30480, train/epoch: 7.253688812255859
Step: 30490, train/loss: 0.0
Step: 30490, train/grad_norm: 2.5736554221111874e-08
Step: 30490, train/learning_rate: 1.3719657545152586e-05
Step: 30490, train/epoch: 7.256068706512451
Step: 30500, train/loss: 0.0
Step: 30500, train/grad_norm: 2.630804907255424e-09
Step: 30500, train/learning_rate: 1.370775862596929e-05
Step: 30500, train/epoch: 7.258448123931885
Step: 30510, train/loss: 0.0
Step: 30510, train/grad_norm: 8.200581902428894e-08
Step: 30510, train/learning_rate: 1.3695858797291294e-05
Step: 30510, train/epoch: 7.260828018188477
Step: 30520, train/loss: 0.0
Step: 30520, train/grad_norm: 1.3147443667094194e-07
Step: 30520, train/learning_rate: 1.3683959878107999e-05
Step: 30520, train/epoch: 7.263207912445068
Step: 30530, train/loss: 0.0
Step: 30530, train/grad_norm: 3.7936531498417025e-06
Step: 30530, train/learning_rate: 1.3672060958924703e-05
Step: 30530, train/epoch: 7.26558780670166
Step: 30540, train/loss: 0.0
Step: 30540, train/grad_norm: 5.323632160103564e-11
Step: 30540, train/learning_rate: 1.3660162039741408e-05
Step: 30540, train/epoch: 7.267967700958252
Step: 30550, train/loss: 0.0
Step: 30550, train/grad_norm: 4.5175787022344593e-07
Step: 30550, train/learning_rate: 1.3648263120558113e-05
Step: 30550, train/epoch: 7.270347595214844
Step: 30560, train/loss: 0.0
Step: 30560, train/grad_norm: 1.2906838098292184e-10
Step: 30560, train/learning_rate: 1.3636363291880116e-05
Step: 30560, train/epoch: 7.2727274894714355
Step: 30570, train/loss: 0.0
Step: 30570, train/grad_norm: 2.2504876184825662e-08
Step: 30570, train/learning_rate: 1.362446437269682e-05
Step: 30570, train/epoch: 7.275106906890869
Step: 30580, train/loss: 0.0
Step: 30580, train/grad_norm: 2.0894159735718176e-09
Step: 30580, train/learning_rate: 1.3612565453513525e-05
Step: 30580, train/epoch: 7.277486801147461
Step: 30590, train/loss: 0.0
Step: 30590, train/grad_norm: 1.953797817977332e-10
Step: 30590, train/learning_rate: 1.360066653433023e-05
Step: 30590, train/epoch: 7.279866695404053
Step: 30600, train/loss: 0.0
Step: 30600, train/grad_norm: 4.1461625799321666e-11
Step: 30600, train/learning_rate: 1.3588767615146935e-05
Step: 30600, train/epoch: 7.2822465896606445
Step: 30610, train/loss: 0.0
Step: 30610, train/grad_norm: 5.312100204157844e-10
Step: 30610, train/learning_rate: 1.3576867786468938e-05
Step: 30610, train/epoch: 7.284626483917236
Step: 30620, train/loss: 0.0
Step: 30620, train/grad_norm: 9.693561509038773e-09
Step: 30620, train/learning_rate: 1.3564968867285643e-05
Step: 30620, train/epoch: 7.287006378173828
Step: 30630, train/loss: 0.0
Step: 30630, train/grad_norm: 7.837880566796684e-09
Step: 30630, train/learning_rate: 1.3553069948102348e-05
Step: 30630, train/epoch: 7.289385795593262
Step: 30640, train/loss: 0.0
Step: 30640, train/grad_norm: 2.449595803977189e-10
Step: 30640, train/learning_rate: 1.3541171028919052e-05
Step: 30640, train/epoch: 7.2917656898498535
Step: 30650, train/loss: 0.0
Step: 30650, train/grad_norm: 1.0018118778987173e-08
Step: 30650, train/learning_rate: 1.3529272109735757e-05
Step: 30650, train/epoch: 7.294145584106445
Step: 30660, train/loss: 0.0
Step: 30660, train/grad_norm: 1.0879344092273868e-09
Step: 30660, train/learning_rate: 1.351737228105776e-05
Step: 30660, train/epoch: 7.296525478363037
Step: 30670, train/loss: 0.0
Step: 30670, train/grad_norm: 1.9786494220852546e-08
Step: 30670, train/learning_rate: 1.3505473361874465e-05
Step: 30670, train/epoch: 7.298905372619629
Step: 30680, train/loss: 0.0
Step: 30680, train/grad_norm: 5.573153938343012e-09
Step: 30680, train/learning_rate: 1.349357444269117e-05
Step: 30680, train/epoch: 7.301285266876221
Step: 30690, train/loss: 0.0
Step: 30690, train/grad_norm: 2.6896767479911432e-08
Step: 30690, train/learning_rate: 1.3481675523507874e-05
Step: 30690, train/epoch: 7.303664684295654
Step: 30700, train/loss: 0.0
Step: 30700, train/grad_norm: 3.4164402222813806e-06
Step: 30700, train/learning_rate: 1.346977660432458e-05
Step: 30700, train/epoch: 7.306044578552246
Step: 30710, train/loss: 0.0
Step: 30710, train/grad_norm: 7.529390999572172e-10
Step: 30710, train/learning_rate: 1.3457876775646582e-05
Step: 30710, train/epoch: 7.308424472808838
Step: 30720, train/loss: 0.0
Step: 30720, train/grad_norm: 6.601707180919902e-09
Step: 30720, train/learning_rate: 1.3445977856463287e-05
Step: 30720, train/epoch: 7.31080436706543
Step: 30730, train/loss: 0.0
Step: 30730, train/grad_norm: 3.7170646560724663e-09
Step: 30730, train/learning_rate: 1.3434078937279992e-05
Step: 30730, train/epoch: 7.3131842613220215
Step: 30740, train/loss: 0.0
Step: 30740, train/grad_norm: 3.763012124124998e-09
Step: 30740, train/learning_rate: 1.3422180018096697e-05
Step: 30740, train/epoch: 7.315564155578613
Step: 30750, train/loss: 0.0
Step: 30750, train/grad_norm: 4.0184477967386556e-08
Step: 30750, train/learning_rate: 1.3410281098913401e-05
Step: 30750, train/epoch: 7.317944049835205
Step: 30760, train/loss: 0.0
Step: 30760, train/grad_norm: 1.5686907328671396e-09
Step: 30760, train/learning_rate: 1.3398381270235404e-05
Step: 30760, train/epoch: 7.320323467254639
Step: 30770, train/loss: 0.0
Step: 30770, train/grad_norm: 1.5587744428557926e-08
Step: 30770, train/learning_rate: 1.3386482351052109e-05
Step: 30770, train/epoch: 7.3227033615112305
Step: 30780, train/loss: 0.0
Step: 30780, train/grad_norm: 2.74438805014654e-09
Step: 30780, train/learning_rate: 1.3374583431868814e-05
Step: 30780, train/epoch: 7.325083255767822
Step: 30790, train/loss: 0.0
Step: 30790, train/grad_norm: 1.3212566152276395e-08
Step: 30790, train/learning_rate: 1.3362684512685519e-05
Step: 30790, train/epoch: 7.327463150024414
Step: 30800, train/loss: 0.0
Step: 30800, train/grad_norm: 1.12883558056609e-09
Step: 30800, train/learning_rate: 1.3350785593502223e-05
Step: 30800, train/epoch: 7.329843044281006
Step: 30810, train/loss: 0.0
Step: 30810, train/grad_norm: 1.2584186492858862e-07
Step: 30810, train/learning_rate: 1.3338886674318928e-05
Step: 30810, train/epoch: 7.332222938537598
Step: 30820, train/loss: 0.0
Step: 30820, train/grad_norm: 4.206132109274563e-10
Step: 30820, train/learning_rate: 1.3326986845640931e-05
Step: 30820, train/epoch: 7.334602355957031
Step: 30830, train/loss: 0.0
Step: 30830, train/grad_norm: 3.5282068933106814e-10
Step: 30830, train/learning_rate: 1.3315087926457636e-05
Step: 30830, train/epoch: 7.336982250213623
Step: 30840, train/loss: 0.0
Step: 30840, train/grad_norm: 1.6628911581051398e-09
Step: 30840, train/learning_rate: 1.330318900727434e-05
Step: 30840, train/epoch: 7.339362144470215
Step: 30850, train/loss: 0.0
Step: 30850, train/grad_norm: 2.6716353573874585e-09
Step: 30850, train/learning_rate: 1.3291290088091046e-05
Step: 30850, train/epoch: 7.341742038726807
Step: 30860, train/loss: 0.0
Step: 30860, train/grad_norm: 4.886590545538638e-07
Step: 30860, train/learning_rate: 1.327939116890775e-05
Step: 30860, train/epoch: 7.344121932983398
Step: 30870, train/loss: 0.0
Step: 30870, train/grad_norm: 4.1326452446810436e-06
Step: 30870, train/learning_rate: 1.3267491340229753e-05
Step: 30870, train/epoch: 7.34650182723999
Step: 30880, train/loss: 0.0
Step: 30880, train/grad_norm: 5.9310117581090616e-12
Step: 30880, train/learning_rate: 1.3255592421046458e-05
Step: 30880, train/epoch: 7.348881721496582
Step: 30890, train/loss: 0.0
Step: 30890, train/grad_norm: 3.9843460086785853e-08
Step: 30890, train/learning_rate: 1.3243693501863163e-05
Step: 30890, train/epoch: 7.351261138916016
Step: 30900, train/loss: 0.0
Step: 30900, train/grad_norm: 2.83722734195635e-09
Step: 30900, train/learning_rate: 1.3231794582679868e-05
Step: 30900, train/epoch: 7.353641033172607
Step: 30910, train/loss: 0.0
Step: 30910, train/grad_norm: 1.5949954690341883e-09
Step: 30910, train/learning_rate: 1.3219895663496573e-05
Step: 30910, train/epoch: 7.356020927429199
Step: 30920, train/loss: 0.0
Step: 30920, train/grad_norm: 1.821120321210401e-08
Step: 30920, train/learning_rate: 1.3207995834818576e-05
Step: 30920, train/epoch: 7.358400821685791
Step: 30930, train/loss: 0.0
Step: 30930, train/grad_norm: 7.44510286754263e-11
Step: 30930, train/learning_rate: 1.319609691563528e-05
Step: 30930, train/epoch: 7.360780715942383
Step: 30940, train/loss: 0.0
Step: 30940, train/grad_norm: 4.927870111970378e-09
Step: 30940, train/learning_rate: 1.3184197996451985e-05
Step: 30940, train/epoch: 7.363160610198975
Step: 30950, train/loss: 0.0
Step: 30950, train/grad_norm: 1.265168525499405e-09
Step: 30950, train/learning_rate: 1.317229907726869e-05
Step: 30950, train/epoch: 7.365540027618408
Step: 30960, train/loss: 0.0
Step: 30960, train/grad_norm: 2.3159054535204504e-11
Step: 30960, train/learning_rate: 1.3160400158085395e-05
Step: 30960, train/epoch: 7.367919921875
Step: 30970, train/loss: 0.0
Step: 30970, train/grad_norm: 7.241419703457597e-12
Step: 30970, train/learning_rate: 1.3148500329407398e-05
Step: 30970, train/epoch: 7.370299816131592
Step: 30980, train/loss: 0.0
Step: 30980, train/grad_norm: 8.876960798376299e-12
Step: 30980, train/learning_rate: 1.3136601410224102e-05
Step: 30980, train/epoch: 7.372679710388184
Step: 30990, train/loss: 0.0
Step: 30990, train/grad_norm: 1.013488182266542e-10
Step: 30990, train/learning_rate: 1.3124702491040807e-05
Step: 30990, train/epoch: 7.375059604644775
Step: 31000, train/loss: 0.0
Step: 31000, train/grad_norm: 1.3403361753816512e-10
Step: 31000, train/learning_rate: 1.3112803571857512e-05
Step: 31000, train/epoch: 7.377439498901367
Step: 31010, train/loss: 0.0
Step: 31010, train/grad_norm: 3.817299898400961e-08
Step: 31010, train/learning_rate: 1.3100904652674217e-05
Step: 31010, train/epoch: 7.379818916320801
Step: 31020, train/loss: 0.0
Step: 31020, train/grad_norm: 1.2198207777203152e-08
Step: 31020, train/learning_rate: 1.308900482399622e-05
Step: 31020, train/epoch: 7.382198810577393
Step: 31030, train/loss: 0.0
Step: 31030, train/grad_norm: 3.59993322263108e-07
Step: 31030, train/learning_rate: 1.3077105904812925e-05
Step: 31030, train/epoch: 7.384578704833984
Step: 31040, train/loss: 0.0
Step: 31040, train/grad_norm: 1.0435761144123035e-09
Step: 31040, train/learning_rate: 1.306520698562963e-05
Step: 31040, train/epoch: 7.386958599090576
Step: 31050, train/loss: 0.0
Step: 31050, train/grad_norm: 3.0438168652224817e-10
Step: 31050, train/learning_rate: 1.3053308066446334e-05
Step: 31050, train/epoch: 7.389338493347168
Step: 31060, train/loss: 0.0
Step: 31060, train/grad_norm: 7.498899634583722e-08
Step: 31060, train/learning_rate: 1.3041409147263039e-05
Step: 31060, train/epoch: 7.39171838760376
Step: 31070, train/loss: 0.0
Step: 31070, train/grad_norm: 2.5919252855111097e-10
Step: 31070, train/learning_rate: 1.3029509318585042e-05
Step: 31070, train/epoch: 7.394098281860352
Step: 31080, train/loss: 0.0
Step: 31080, train/grad_norm: 3.4352176051660743e-12
Step: 31080, train/learning_rate: 1.3017610399401747e-05
Step: 31080, train/epoch: 7.396477699279785
Step: 31090, train/loss: 0.0
Step: 31090, train/grad_norm: 2.4420401256719515e-09
Step: 31090, train/learning_rate: 1.3005711480218451e-05
Step: 31090, train/epoch: 7.398857593536377
Step: 31100, train/loss: 0.0
Step: 31100, train/grad_norm: 2.921406689893047e-07
Step: 31100, train/learning_rate: 1.2993812561035156e-05
Step: 31100, train/epoch: 7.401237487792969
Step: 31110, train/loss: 0.0
Step: 31110, train/grad_norm: 7.090046949542739e-08
Step: 31110, train/learning_rate: 1.2981913641851861e-05
Step: 31110, train/epoch: 7.4036173820495605
Step: 31120, train/loss: 0.0
Step: 31120, train/grad_norm: 2.0649186804888586e-09
Step: 31120, train/learning_rate: 1.2970014722668566e-05
Step: 31120, train/epoch: 7.405997276306152
Step: 31130, train/loss: 0.0
Step: 31130, train/grad_norm: 1.7846688753556106e-12
Step: 31130, train/learning_rate: 1.2958114893990569e-05
Step: 31130, train/epoch: 7.408377170562744
Step: 31140, train/loss: 0.0
Step: 31140, train/grad_norm: 8.355380831659431e-10
Step: 31140, train/learning_rate: 1.2946215974807274e-05
Step: 31140, train/epoch: 7.410756587982178
Step: 31150, train/loss: 0.0
Step: 31150, train/grad_norm: 1.0501510587346274e-05
Step: 31150, train/learning_rate: 1.2934317055623978e-05
Step: 31150, train/epoch: 7.4131364822387695
Step: 31160, train/loss: 0.0
Step: 31160, train/grad_norm: 4.5490458622232666e-10
Step: 31160, train/learning_rate: 1.2922418136440683e-05
Step: 31160, train/epoch: 7.415516376495361
Step: 31170, train/loss: 0.0
Step: 31170, train/grad_norm: 4.3641473768119e-10
Step: 31170, train/learning_rate: 1.2910519217257388e-05
Step: 31170, train/epoch: 7.417896270751953
Step: 31180, train/loss: 0.0
Step: 31180, train/grad_norm: 1.9927771238714698e-11
Step: 31180, train/learning_rate: 1.2898619388579391e-05
Step: 31180, train/epoch: 7.420276165008545
Step: 31190, train/loss: 0.0
Step: 31190, train/grad_norm: 3.5000424780662343e-09
Step: 31190, train/learning_rate: 1.2886720469396096e-05
Step: 31190, train/epoch: 7.422656059265137
Step: 31200, train/loss: 0.0
Step: 31200, train/grad_norm: 6.958016030012004e-08
Step: 31200, train/learning_rate: 1.28748215502128e-05
Step: 31200, train/epoch: 7.42503547668457
Step: 31210, train/loss: 0.0
Step: 31210, train/grad_norm: 1.8045305694158742e-08
Step: 31210, train/learning_rate: 1.2862922631029505e-05
Step: 31210, train/epoch: 7.427415370941162
Step: 31220, train/loss: 0.0
Step: 31220, train/grad_norm: 5.344770112603037e-11
Step: 31220, train/learning_rate: 1.285102371184621e-05
Step: 31220, train/epoch: 7.429795265197754
Step: 31230, train/loss: 0.0
Step: 31230, train/grad_norm: 2.9176826288335178e-08
Step: 31230, train/learning_rate: 1.2839123883168213e-05
Step: 31230, train/epoch: 7.432175159454346
Step: 31240, train/loss: 0.0
Step: 31240, train/grad_norm: 7.85878668807527e-08
Step: 31240, train/learning_rate: 1.2827224963984918e-05
Step: 31240, train/epoch: 7.4345550537109375
Step: 31250, train/loss: 0.0
Step: 31250, train/grad_norm: 1.7323725787221633e-10
Step: 31250, train/learning_rate: 1.2815326044801623e-05
Step: 31250, train/epoch: 7.436934947967529
Step: 31260, train/loss: 0.0
Step: 31260, train/grad_norm: 8.573372012321556e-10
Step: 31260, train/learning_rate: 1.2803427125618327e-05
Step: 31260, train/epoch: 7.439314842224121
Step: 31270, train/loss: 0.0
Step: 31270, train/grad_norm: 1.060114868778328e-06
Step: 31270, train/learning_rate: 1.2791528206435032e-05
Step: 31270, train/epoch: 7.441694259643555
Step: 31280, train/loss: 0.0
Step: 31280, train/grad_norm: 1.1330267852827092e-06
Step: 31280, train/learning_rate: 1.2779628377757035e-05
Step: 31280, train/epoch: 7.4440741539001465
Step: 31290, train/loss: 0.0
Step: 31290, train/grad_norm: 8.73788792432606e-08
Step: 31290, train/learning_rate: 1.276772945857374e-05
Step: 31290, train/epoch: 7.446454048156738
Step: 31300, train/loss: 0.0
Step: 31300, train/grad_norm: 9.9072408943357e-08
Step: 31300, train/learning_rate: 1.2755830539390445e-05
Step: 31300, train/epoch: 7.44883394241333
Step: 31310, train/loss: 0.0
Step: 31310, train/grad_norm: 1.2396300519412762e-07
Step: 31310, train/learning_rate: 1.274393162020715e-05
Step: 31310, train/epoch: 7.451213836669922
Step: 31320, train/loss: 0.0
Step: 31320, train/grad_norm: 7.627193099324359e-08
Step: 31320, train/learning_rate: 1.2732032701023854e-05
Step: 31320, train/epoch: 7.453593730926514
Step: 31330, train/loss: 0.0
Step: 31330, train/grad_norm: 7.805085466827677e-10
Step: 31330, train/learning_rate: 1.2720132872345857e-05
Step: 31330, train/epoch: 7.455973148345947
Step: 31340, train/loss: 0.0
Step: 31340, train/grad_norm: 4.839951994739522e-09
Step: 31340, train/learning_rate: 1.2708233953162562e-05
Step: 31340, train/epoch: 7.458353042602539
Step: 31350, train/loss: 0.0
Step: 31350, train/grad_norm: 8.412585827954899e-08
Step: 31350, train/learning_rate: 1.2696335033979267e-05
Step: 31350, train/epoch: 7.460732936859131
Step: 31360, train/loss: 0.0
Step: 31360, train/grad_norm: 2.3963223072520634e-10
Step: 31360, train/learning_rate: 1.2684436114795972e-05
Step: 31360, train/epoch: 7.463112831115723
Step: 31370, train/loss: 0.0
Step: 31370, train/grad_norm: 2.7057345947101297e-11
Step: 31370, train/learning_rate: 1.2672537195612676e-05
Step: 31370, train/epoch: 7.4654927253723145
Step: 31380, train/loss: 0.0
Step: 31380, train/grad_norm: 1.1161410337634514e-12
Step: 31380, train/learning_rate: 1.266063736693468e-05
Step: 31380, train/epoch: 7.467872619628906
Step: 31390, train/loss: 0.0
Step: 31390, train/grad_norm: 8.832526710023103e-09
Step: 31390, train/learning_rate: 1.2648738447751384e-05
Step: 31390, train/epoch: 7.47025203704834
Step: 31400, train/loss: 0.0
Step: 31400, train/grad_norm: 5.649128276274951e-09
Step: 31400, train/learning_rate: 1.2636839528568089e-05
Step: 31400, train/epoch: 7.472631931304932
Step: 31410, train/loss: 0.0
Step: 31410, train/grad_norm: 7.346278252384764e-09
Step: 31410, train/learning_rate: 1.2624940609384794e-05
Step: 31410, train/epoch: 7.475011825561523
Step: 31420, train/loss: 0.0
Step: 31420, train/grad_norm: 6.184917161755621e-11
Step: 31420, train/learning_rate: 1.2613041690201499e-05
Step: 31420, train/epoch: 7.477391719818115
Step: 31430, train/loss: 0.0
Step: 31430, train/grad_norm: 1.7962773313229263e-07
Step: 31430, train/learning_rate: 1.2601141861523502e-05
Step: 31430, train/epoch: 7.479771614074707
Step: 31440, train/loss: 0.0
Step: 31440, train/grad_norm: 3.1769475938858704e-09
Step: 31440, train/learning_rate: 1.2589242942340206e-05
Step: 31440, train/epoch: 7.482151508331299
Step: 31450, train/loss: 0.0
Step: 31450, train/grad_norm: 2.54749998873649e-10
Step: 31450, train/learning_rate: 1.2577344023156911e-05
Step: 31450, train/epoch: 7.484531402587891
Step: 31460, train/loss: 0.0
Step: 31460, train/grad_norm: 1.0150232654382307e-08
Step: 31460, train/learning_rate: 1.2565445103973616e-05
Step: 31460, train/epoch: 7.486910820007324
Step: 31470, train/loss: 0.0
Step: 31470, train/grad_norm: 5.614749554183618e-08
Step: 31470, train/learning_rate: 1.255354618479032e-05
Step: 31470, train/epoch: 7.489290714263916
Step: 31480, train/loss: 0.0
Step: 31480, train/grad_norm: 1.6896775645314221e-10
Step: 31480, train/learning_rate: 1.2541647265607025e-05
Step: 31480, train/epoch: 7.491670608520508
Step: 31490, train/loss: 0.0
Step: 31490, train/grad_norm: 1.922742520754639e-10
Step: 31490, train/learning_rate: 1.2529747436929028e-05
Step: 31490, train/epoch: 7.4940505027771
Step: 31500, train/loss: 0.0
Step: 31500, train/grad_norm: 2.5733969621910546e-08
Step: 31500, train/learning_rate: 1.2517848517745733e-05
Step: 31500, train/epoch: 7.496430397033691
Step: 31510, train/loss: 0.0
Step: 31510, train/grad_norm: 8.8924849706018e-08
Step: 31510, train/learning_rate: 1.2505949598562438e-05
Step: 31510, train/epoch: 7.498810291290283
Step: 31520, train/loss: 0.0
Step: 31520, train/grad_norm: 2.1535211114365183e-11
Step: 31520, train/learning_rate: 1.2494050679379143e-05
Step: 31520, train/epoch: 7.501189708709717
Step: 31530, train/loss: 0.0
Step: 31530, train/grad_norm: 3.0758300795241666e-08
Step: 31530, train/learning_rate: 1.2482151760195848e-05
Step: 31530, train/epoch: 7.503569602966309
Step: 31540, train/loss: 0.0
Step: 31540, train/grad_norm: 1.471234800654031e-10
Step: 31540, train/learning_rate: 1.247025193151785e-05
Step: 31540, train/epoch: 7.5059494972229
Step: 31550, train/loss: 0.0
Step: 31550, train/grad_norm: 1.4065152376119272e-09
Step: 31550, train/learning_rate: 1.2458353012334555e-05
Step: 31550, train/epoch: 7.508329391479492
Step: 31560, train/loss: 0.0
Step: 31560, train/grad_norm: 3.573191520445107e-08
Step: 31560, train/learning_rate: 1.244645409315126e-05
Step: 31560, train/epoch: 7.510709285736084
Step: 31570, train/loss: 0.0
Step: 31570, train/grad_norm: 4.07050393391728e-10
Step: 31570, train/learning_rate: 1.2434555173967965e-05
Step: 31570, train/epoch: 7.513089179992676
Step: 31580, train/loss: 0.0
Step: 31580, train/grad_norm: 8.881291364559729e-07
Step: 31580, train/learning_rate: 1.242265625478467e-05
Step: 31580, train/epoch: 7.515468597412109
Step: 31590, train/loss: 0.0
Step: 31590, train/grad_norm: 5.803219210065436e-06
Step: 31590, train/learning_rate: 1.2410756426106673e-05
Step: 31590, train/epoch: 7.517848491668701
Step: 31600, train/loss: 0.0
Step: 31600, train/grad_norm: 5.011309422986443e-11
Step: 31600, train/learning_rate: 1.2398857506923378e-05
Step: 31600, train/epoch: 7.520228385925293
Step: 31610, train/loss: 0.0
Step: 31610, train/grad_norm: 2.3636377477487258e-07
Step: 31610, train/learning_rate: 1.2386958587740082e-05
Step: 31610, train/epoch: 7.522608280181885
Step: 31620, train/loss: 0.0
Step: 31620, train/grad_norm: 8.867992562500149e-09
Step: 31620, train/learning_rate: 1.2375059668556787e-05
Step: 31620, train/epoch: 7.524988174438477
Step: 31630, train/loss: 0.0
Step: 31630, train/grad_norm: 4.6170811618395646e-10
Step: 31630, train/learning_rate: 1.2363160749373492e-05
Step: 31630, train/epoch: 7.527368068695068
Step: 31640, train/loss: 0.0
Step: 31640, train/grad_norm: 1.1583152215166592e-09
Step: 31640, train/learning_rate: 1.2351260920695495e-05
Step: 31640, train/epoch: 7.52974796295166
Step: 31650, train/loss: 0.0
Step: 31650, train/grad_norm: 6.001978647418582e-08
Step: 31650, train/learning_rate: 1.23393620015122e-05
Step: 31650, train/epoch: 7.532127380371094
Step: 31660, train/loss: 0.0
Step: 31660, train/grad_norm: 1.4280190918647406e-10
Step: 31660, train/learning_rate: 1.2327463082328904e-05
Step: 31660, train/epoch: 7.5345072746276855
Step: 31670, train/loss: 0.0
Step: 31670, train/grad_norm: 3.2177935316290984e-10
Step: 31670, train/learning_rate: 1.231556416314561e-05
Step: 31670, train/epoch: 7.536887168884277
Step: 31680, train/loss: 0.0
Step: 31680, train/grad_norm: 8.252016431287146e-11
Step: 31680, train/learning_rate: 1.2303665243962314e-05
Step: 31680, train/epoch: 7.539267063140869
Step: 31690, train/loss: 0.0
Step: 31690, train/grad_norm: 1.3672269913445056e-11
Step: 31690, train/learning_rate: 1.2291765415284317e-05
Step: 31690, train/epoch: 7.541646957397461
Step: 31700, train/loss: 0.0
Step: 31700, train/grad_norm: 1.221703016529574e-10
Step: 31700, train/learning_rate: 1.2279866496101022e-05
Step: 31700, train/epoch: 7.544026851654053
Step: 31710, train/loss: 0.0
Step: 31710, train/grad_norm: 2.458358849821707e-09
Step: 31710, train/learning_rate: 1.2267967576917727e-05
Step: 31710, train/epoch: 7.546406269073486
Step: 31720, train/loss: 0.0
Step: 31720, train/grad_norm: 1.4333697562207703e-09
Step: 31720, train/learning_rate: 1.2256068657734431e-05
Step: 31720, train/epoch: 7.548786163330078
Step: 31730, train/loss: 0.0
Step: 31730, train/grad_norm: 1.2120680015215157e-09
Step: 31730, train/learning_rate: 1.2244169738551136e-05
Step: 31730, train/epoch: 7.55116605758667
Step: 31740, train/loss: 0.0
Step: 31740, train/grad_norm: 4.9000401958210205e-08
Step: 31740, train/learning_rate: 1.2232269909873139e-05
Step: 31740, train/epoch: 7.553545951843262
Step: 31750, train/loss: 0.0
Step: 31750, train/grad_norm: 3.507534529489931e-08
Step: 31750, train/learning_rate: 1.2220370990689844e-05
Step: 31750, train/epoch: 7.5559258460998535
Step: 31760, train/loss: 0.0
Step: 31760, train/grad_norm: 1.7246708228224605e-10
Step: 31760, train/learning_rate: 1.2208472071506549e-05
Step: 31760, train/epoch: 7.558305740356445
Step: 31770, train/loss: 0.0
Step: 31770, train/grad_norm: 5.550127912812286e-09
Step: 31770, train/learning_rate: 1.2196573152323253e-05
Step: 31770, train/epoch: 7.560685157775879
Step: 31780, train/loss: 0.0
Step: 31780, train/grad_norm: 2.3022671009204076e-12
Step: 31780, train/learning_rate: 1.2184674233139958e-05
Step: 31780, train/epoch: 7.563065052032471
Step: 31790, train/loss: 0.0
Step: 31790, train/grad_norm: 1.9936929884778465e-10
Step: 31790, train/learning_rate: 1.2172775313956663e-05
Step: 31790, train/epoch: 7.5654449462890625
Step: 31800, train/loss: 0.0
Step: 31800, train/grad_norm: 5.170437428897445e-12
Step: 31800, train/learning_rate: 1.2160875485278666e-05
Step: 31800, train/epoch: 7.567824840545654
Step: 31810, train/loss: 0.0
Step: 31810, train/grad_norm: 5.527506230507129e-10
Step: 31810, train/learning_rate: 1.214897656609537e-05
Step: 31810, train/epoch: 7.570204734802246
Step: 31820, train/loss: 0.0
Step: 31820, train/grad_norm: 2.561880286904017e-13
Step: 31820, train/learning_rate: 1.2137077646912076e-05
Step: 31820, train/epoch: 7.572584629058838
Step: 31830, train/loss: 0.0
Step: 31830, train/grad_norm: 1.755204692699408e-08
Step: 31830, train/learning_rate: 1.212517872772878e-05
Step: 31830, train/epoch: 7.57496452331543
Step: 31840, train/loss: 0.0
Step: 31840, train/grad_norm: 1.855172548914652e-08
Step: 31840, train/learning_rate: 1.2113279808545485e-05
Step: 31840, train/epoch: 7.577343940734863
Step: 31850, train/loss: 0.0
Step: 31850, train/grad_norm: 1.643038205489944e-10
Step: 31850, train/learning_rate: 1.2101379979867488e-05
Step: 31850, train/epoch: 7.579723834991455
Step: 31860, train/loss: 0.0
Step: 31860, train/grad_norm: 8.029167020140449e-08
Step: 31860, train/learning_rate: 1.2089481060684193e-05
Step: 31860, train/epoch: 7.582103729248047
Step: 31870, train/loss: 0.0
Step: 31870, train/grad_norm: 1.301594387825844e-08
Step: 31870, train/learning_rate: 1.2077582141500898e-05
Step: 31870, train/epoch: 7.584483623504639
Step: 31880, train/loss: 0.0
Step: 31880, train/grad_norm: 7.344758828908837e-11
Step: 31880, train/learning_rate: 1.2065683222317602e-05
Step: 31880, train/epoch: 7.5868635177612305
Step: 31890, train/loss: 0.0
Step: 31890, train/grad_norm: 5.582477911048045e-07
Step: 31890, train/learning_rate: 1.2053784303134307e-05
Step: 31890, train/epoch: 7.589243412017822
Step: 31900, train/loss: 0.0
Step: 31900, train/grad_norm: 3.3582980840662913e-10
Step: 31900, train/learning_rate: 1.204188447445631e-05
Step: 31900, train/epoch: 7.591622829437256
Step: 31910, train/loss: 0.0
Step: 31910, train/grad_norm: 2.0847163995085793e-09
Step: 31910, train/learning_rate: 1.2029985555273015e-05
Step: 31910, train/epoch: 7.594002723693848
Step: 31920, train/loss: 0.0
Step: 31920, train/grad_norm: 1.9987629606976753e-10
Step: 31920, train/learning_rate: 1.201808663608972e-05
Step: 31920, train/epoch: 7.5963826179504395
Step: 31930, train/loss: 0.0
Step: 31930, train/grad_norm: 8.919954185004908e-12
Step: 31930, train/learning_rate: 1.2006187716906425e-05
Step: 31930, train/epoch: 7.598762512207031
Step: 31940, train/loss: 0.0
Step: 31940, train/grad_norm: 7.938281976205275e-11
Step: 31940, train/learning_rate: 1.199428879772313e-05
Step: 31940, train/epoch: 7.601142406463623
Step: 31950, train/loss: 0.0
Step: 31950, train/grad_norm: 1.0389181737124886e-09
Step: 31950, train/learning_rate: 1.1982388969045132e-05
Step: 31950, train/epoch: 7.603522300720215
Step: 31960, train/loss: 0.0
Step: 31960, train/grad_norm: 4.498761363436188e-09
Step: 31960, train/learning_rate: 1.1970490049861837e-05
Step: 31960, train/epoch: 7.605901718139648
Step: 31970, train/loss: 0.0
Step: 31970, train/grad_norm: 2.9220884698588634e-06
Step: 31970, train/learning_rate: 1.1958591130678542e-05
Step: 31970, train/epoch: 7.60828161239624
Step: 31980, train/loss: 0.0
Step: 31980, train/grad_norm: 1.4366624556672036e-09
Step: 31980, train/learning_rate: 1.1946692211495247e-05
Step: 31980, train/epoch: 7.610661506652832
Step: 31990, train/loss: 0.0
Step: 31990, train/grad_norm: 2.553957711981525e-09
Step: 31990, train/learning_rate: 1.1934793292311952e-05
Step: 31990, train/epoch: 7.613041400909424
Step: 32000, train/loss: 0.0
Step: 32000, train/grad_norm: 3.1006884559925396e-11
Step: 32000, train/learning_rate: 1.1922893463633955e-05
Step: 32000, train/epoch: 7.615421295166016
Step: 32010, train/loss: 0.0
Step: 32010, train/grad_norm: 5.968850302906503e-09
Step: 32010, train/learning_rate: 1.191099454445066e-05
Step: 32010, train/epoch: 7.617801189422607
Step: 32020, train/loss: 0.0
Step: 32020, train/grad_norm: 2.0008505963176049e-10
Step: 32020, train/learning_rate: 1.1899095625267364e-05
Step: 32020, train/epoch: 7.620181083679199
Step: 32030, train/loss: 0.0
Step: 32030, train/grad_norm: 5.114711432163688e-10
Step: 32030, train/learning_rate: 1.1887196706084069e-05
Step: 32030, train/epoch: 7.622560501098633
Step: 32040, train/loss: 0.0
Step: 32040, train/grad_norm: 3.0601208234060095e-09
Step: 32040, train/learning_rate: 1.1875297786900774e-05
Step: 32040, train/epoch: 7.624940395355225
Step: 32050, train/loss: 0.0
Step: 32050, train/grad_norm: 3.10093756228369e-10
Step: 32050, train/learning_rate: 1.1863397958222777e-05
Step: 32050, train/epoch: 7.627320289611816
Step: 32060, train/loss: 0.0
Step: 32060, train/grad_norm: 4.98510954738407e-11
Step: 32060, train/learning_rate: 1.1851499039039481e-05
Step: 32060, train/epoch: 7.629700183868408
Step: 32070, train/loss: 0.0
Step: 32070, train/grad_norm: 2.1881939316070742e-10
Step: 32070, train/learning_rate: 1.1839600119856186e-05
Step: 32070, train/epoch: 7.632080078125
Step: 32080, train/loss: 0.0
Step: 32080, train/grad_norm: 2.5417139504213537e-09
Step: 32080, train/learning_rate: 1.1827701200672891e-05
Step: 32080, train/epoch: 7.634459972381592
Step: 32090, train/loss: 0.0
Step: 32090, train/grad_norm: 6.4331153251662165e-12
Step: 32090, train/learning_rate: 1.1815802281489596e-05
Step: 32090, train/epoch: 7.636839389801025
Step: 32100, train/loss: 0.0
Step: 32100, train/grad_norm: 1.3054805847456663e-10
Step: 32100, train/learning_rate: 1.1803902452811599e-05
Step: 32100, train/epoch: 7.639219284057617
Step: 32110, train/loss: 0.0
Step: 32110, train/grad_norm: 4.70726702062052e-09
Step: 32110, train/learning_rate: 1.1792003533628304e-05
Step: 32110, train/epoch: 7.641599178314209
Step: 32120, train/loss: 0.0
Step: 32120, train/grad_norm: 3.104846935109151e-10
Step: 32120, train/learning_rate: 1.1780104614445008e-05
Step: 32120, train/epoch: 7.643979072570801
Step: 32130, train/loss: 0.0
Step: 32130, train/grad_norm: 6.615829412082164e-12
Step: 32130, train/learning_rate: 1.1768205695261713e-05
Step: 32130, train/epoch: 7.646358966827393
Step: 32140, train/loss: 0.0
Step: 32140, train/grad_norm: 8.2230781117687e-07
Step: 32140, train/learning_rate: 1.1756306776078418e-05
Step: 32140, train/epoch: 7.648738861083984
Step: 32150, train/loss: 0.0
Step: 32150, train/grad_norm: 2.532470544033827e-12
Step: 32150, train/learning_rate: 1.1744407856895123e-05
Step: 32150, train/epoch: 7.651118278503418
Step: 32160, train/loss: 0.0
Step: 32160, train/grad_norm: 3.2879081401926413e-11
Step: 32160, train/learning_rate: 1.1732508028217126e-05
Step: 32160, train/epoch: 7.65349817276001
Step: 32170, train/loss: 0.0
Step: 32170, train/grad_norm: 2.4461113690144032e-11
Step: 32170, train/learning_rate: 1.172060910903383e-05
Step: 32170, train/epoch: 7.655878067016602
Step: 32180, train/loss: 0.0
Step: 32180, train/grad_norm: 4.5188838782017626e-10
Step: 32180, train/learning_rate: 1.1708710189850535e-05
Step: 32180, train/epoch: 7.658257961273193
Step: 32190, train/loss: 0.0
Step: 32190, train/grad_norm: 3.636052525735067e-08
Step: 32190, train/learning_rate: 1.169681127066724e-05
Step: 32190, train/epoch: 7.660637855529785
Step: 32200, train/loss: 0.0
Step: 32200, train/grad_norm: 3.122462288729366e-08
Step: 32200, train/learning_rate: 1.1684912351483945e-05
Step: 32200, train/epoch: 7.663017749786377
Step: 32210, train/loss: 0.0
Step: 32210, train/grad_norm: 9.725791016990115e-08
Step: 32210, train/learning_rate: 1.1673012522805948e-05
Step: 32210, train/epoch: 7.665397644042969
Step: 32220, train/loss: 0.0
Step: 32220, train/grad_norm: 5.266350688870602e-10
Step: 32220, train/learning_rate: 1.1661113603622653e-05
Step: 32220, train/epoch: 7.667777061462402
Step: 32230, train/loss: 0.0
Step: 32230, train/grad_norm: 9.018247815184566e-10
Step: 32230, train/learning_rate: 1.1649214684439357e-05
Step: 32230, train/epoch: 7.670156955718994
Step: 32240, train/loss: 0.0
Step: 32240, train/grad_norm: 8.498217407559849e-11
Step: 32240, train/learning_rate: 1.1637315765256062e-05
Step: 32240, train/epoch: 7.672536849975586
Step: 32250, train/loss: 0.0
Step: 32250, train/grad_norm: 1.1604559091438205e-08
Step: 32250, train/learning_rate: 1.1625416846072767e-05
Step: 32250, train/epoch: 7.674916744232178
Step: 32260, train/loss: 0.0
Step: 32260, train/grad_norm: 6.514613232866395e-06
Step: 32260, train/learning_rate: 1.161351701739477e-05
Step: 32260, train/epoch: 7.6772966384887695
Step: 32270, train/loss: 0.0
Step: 32270, train/grad_norm: 1.2860718566487517e-11
Step: 32270, train/learning_rate: 1.1601618098211475e-05
Step: 32270, train/epoch: 7.679676532745361
Step: 32280, train/loss: 0.0
Step: 32280, train/grad_norm: 1.6705439115360932e-11
Step: 32280, train/learning_rate: 1.158971917902818e-05
Step: 32280, train/epoch: 7.682055950164795
Step: 32290, train/loss: 0.0
Step: 32290, train/grad_norm: 1.0411473283511441e-07
Step: 32290, train/learning_rate: 1.1577820259844884e-05
Step: 32290, train/epoch: 7.684435844421387
Step: 32300, train/loss: 0.0
Step: 32300, train/grad_norm: 2.1946274131323662e-08
Step: 32300, train/learning_rate: 1.1565921340661589e-05
Step: 32300, train/epoch: 7.6868157386779785
Step: 32310, train/loss: 0.0
Step: 32310, train/grad_norm: 2.2656987397340345e-08
Step: 32310, train/learning_rate: 1.1554021511983592e-05
Step: 32310, train/epoch: 7.68919563293457
Step: 32320, train/loss: 0.0
Step: 32320, train/grad_norm: 4.833077227317517e-08
Step: 32320, train/learning_rate: 1.1542122592800297e-05
Step: 32320, train/epoch: 7.691575527191162
Step: 32330, train/loss: 0.0
Step: 32330, train/grad_norm: 3.894722543605622e-13
Step: 32330, train/learning_rate: 1.1530223673617002e-05
Step: 32330, train/epoch: 7.693955421447754
Step: 32340, train/loss: 0.0
Step: 32340, train/grad_norm: 2.971036611176814e-11
Step: 32340, train/learning_rate: 1.1518324754433706e-05
Step: 32340, train/epoch: 7.696335315704346
Step: 32350, train/loss: 0.0
Step: 32350, train/grad_norm: 4.77148596189636e-08
Step: 32350, train/learning_rate: 1.1506425835250411e-05
Step: 32350, train/epoch: 7.698714733123779
Step: 32360, train/loss: 0.0
Step: 32360, train/grad_norm: 6.6164540513113934e-09
Step: 32360, train/learning_rate: 1.1494526006572414e-05
Step: 32360, train/epoch: 7.701094627380371
Step: 32370, train/loss: 0.0
Step: 32370, train/grad_norm: 2.3359168332265368e-12
Step: 32370, train/learning_rate: 1.1482627087389119e-05
Step: 32370, train/epoch: 7.703474521636963
Step: 32380, train/loss: 0.0
Step: 32380, train/grad_norm: 5.410158010477062e-08
Step: 32380, train/learning_rate: 1.1470728168205824e-05
Step: 32380, train/epoch: 7.705854415893555
Step: 32390, train/loss: 0.0
Step: 32390, train/grad_norm: 1.9533643591529426e-09
Step: 32390, train/learning_rate: 1.1458829249022529e-05
Step: 32390, train/epoch: 7.7082343101501465
Step: 32400, train/loss: 0.0
Step: 32400, train/grad_norm: 1.2969807450247117e-09
Step: 32400, train/learning_rate: 1.1446930329839233e-05
Step: 32400, train/epoch: 7.710614204406738
Step: 32410, train/loss: 0.0
Step: 32410, train/grad_norm: 1.0063529032322549e-07
Step: 32410, train/learning_rate: 1.1435030501161236e-05
Step: 32410, train/epoch: 7.712993621826172
Step: 32420, train/loss: 0.0
Step: 32420, train/grad_norm: 9.600987782576453e-10
Step: 32420, train/learning_rate: 1.1423131581977941e-05
Step: 32420, train/epoch: 7.715373516082764
Step: 32430, train/loss: 0.0
Step: 32430, train/grad_norm: 2.1779511882868974e-07
Step: 32430, train/learning_rate: 1.1411232662794646e-05
Step: 32430, train/epoch: 7.7177534103393555
Step: 32440, train/loss: 0.0
Step: 32440, train/grad_norm: 1.0358400942545032e-10
Step: 32440, train/learning_rate: 1.139933374361135e-05
Step: 32440, train/epoch: 7.720133304595947
Step: 32450, train/loss: 0.0
Step: 32450, train/grad_norm: 2.356575871942379e-12
Step: 32450, train/learning_rate: 1.1387434824428055e-05
Step: 32450, train/epoch: 7.722513198852539
Step: 32460, train/loss: 0.0
Step: 32460, train/grad_norm: 1.89801327432626e-10
Step: 32460, train/learning_rate: 1.137553590524476e-05
Step: 32460, train/epoch: 7.724893093109131
Step: 32470, train/loss: 0.0
Step: 32470, train/grad_norm: 1.7401058372001899e-09
Step: 32470, train/learning_rate: 1.1363636076566763e-05
Step: 32470, train/epoch: 7.7272725105285645
Step: 32480, train/loss: 0.0
Step: 32480, train/grad_norm: 1.3756229488848248e-09
Step: 32480, train/learning_rate: 1.1351737157383468e-05
Step: 32480, train/epoch: 7.729652404785156
Step: 32490, train/loss: 0.0
Step: 32490, train/grad_norm: 3.164069700689609e-12
Step: 32490, train/learning_rate: 1.1339838238200173e-05
Step: 32490, train/epoch: 7.732032299041748
Step: 32500, train/loss: 0.0
Step: 32500, train/grad_norm: 9.801942724152468e-07
Step: 32500, train/learning_rate: 1.1327939319016878e-05
Step: 32500, train/epoch: 7.73441219329834
Step: 32510, train/loss: 0.0
Step: 32510, train/grad_norm: 5.483551834828404e-09
Step: 32510, train/learning_rate: 1.1316040399833582e-05
Step: 32510, train/epoch: 7.736792087554932
Step: 32520, train/loss: 0.0
Step: 32520, train/grad_norm: 7.39880878786181e-10
Step: 32520, train/learning_rate: 1.1304140571155585e-05
Step: 32520, train/epoch: 7.739171981811523
Step: 32530, train/loss: 0.0
Step: 32530, train/grad_norm: 2.6615702974908118e-09
Step: 32530, train/learning_rate: 1.129224165197229e-05
Step: 32530, train/epoch: 7.741551876068115
Step: 32540, train/loss: 0.0
Step: 32540, train/grad_norm: 1.3138030940940126e-10
Step: 32540, train/learning_rate: 1.1280342732788995e-05
Step: 32540, train/epoch: 7.743931293487549
Step: 32550, train/loss: 0.0
Step: 32550, train/grad_norm: 7.358531117773737e-09
Step: 32550, train/learning_rate: 1.12684438136057e-05
Step: 32550, train/epoch: 7.746311187744141
Step: 32560, train/loss: 0.0
Step: 32560, train/grad_norm: 4.05777597937626e-13
Step: 32560, train/learning_rate: 1.1256544894422404e-05
Step: 32560, train/epoch: 7.748691082000732
Step: 32570, train/loss: 0.0
Step: 32570, train/grad_norm: 7.295783976957182e-10
Step: 32570, train/learning_rate: 1.1244645065744407e-05
Step: 32570, train/epoch: 7.751070976257324
Step: 32580, train/loss: 0.0
Step: 32580, train/grad_norm: 5.431818328460736e-10
Step: 32580, train/learning_rate: 1.1232746146561112e-05
Step: 32580, train/epoch: 7.753450870513916
Step: 32590, train/loss: 0.0
Step: 32590, train/grad_norm: 3.85117893131337e-09
Step: 32590, train/learning_rate: 1.1220847227377817e-05
Step: 32590, train/epoch: 7.755830764770508
Step: 32600, train/loss: 0.0
Step: 32600, train/grad_norm: 6.286904152830175e-08
Step: 32600, train/learning_rate: 1.1208948308194522e-05
Step: 32600, train/epoch: 7.758210182189941
Step: 32610, train/loss: 0.0
Step: 32610, train/grad_norm: 5.0598002350465965e-12
Step: 32610, train/learning_rate: 1.1197049389011227e-05
Step: 32610, train/epoch: 7.760590076446533
Step: 32620, train/loss: 0.0
Step: 32620, train/grad_norm: 1.0102049752269693e-10
Step: 32620, train/learning_rate: 1.118514956033323e-05
Step: 32620, train/epoch: 7.762969970703125
Step: 32630, train/loss: 0.0
Step: 32630, train/grad_norm: 4.119415919490166e-11
Step: 32630, train/learning_rate: 1.1173250641149934e-05
Step: 32630, train/epoch: 7.765349864959717
Step: 32640, train/loss: 0.0
Step: 32640, train/grad_norm: 3.959965688693501e-09
Step: 32640, train/learning_rate: 1.116135172196664e-05
Step: 32640, train/epoch: 7.767729759216309
Step: 32650, train/loss: 0.0
Step: 32650, train/grad_norm: 3.7136171915364e-10
Step: 32650, train/learning_rate: 1.1149452802783344e-05
Step: 32650, train/epoch: 7.7701096534729
Step: 32660, train/loss: 0.0
Step: 32660, train/grad_norm: 3.3740812455107516e-07
Step: 32660, train/learning_rate: 1.1137553883600049e-05
Step: 32660, train/epoch: 7.772489070892334
Step: 32670, train/loss: 0.0
Step: 32670, train/grad_norm: 3.687098890092777e-11
Step: 32670, train/learning_rate: 1.1125654054922052e-05
Step: 32670, train/epoch: 7.774868965148926
Step: 32680, train/loss: 0.0
Step: 32680, train/grad_norm: 3.221471342840232e-05
Step: 32680, train/learning_rate: 1.1113755135738757e-05
Step: 32680, train/epoch: 7.777248859405518
Step: 32690, train/loss: 0.0
Step: 32690, train/grad_norm: 1.8647570466934216e-12
Step: 32690, train/learning_rate: 1.1101856216555461e-05
Step: 32690, train/epoch: 7.779628753662109
Step: 32700, train/loss: 0.0
Step: 32700, train/grad_norm: 1.833195227562978e-11
Step: 32700, train/learning_rate: 1.1089957297372166e-05
Step: 32700, train/epoch: 7.782008647918701
Step: 32710, train/loss: 0.0
Step: 32710, train/grad_norm: 2.711177273795329e-07
Step: 32710, train/learning_rate: 1.1078058378188871e-05
Step: 32710, train/epoch: 7.784388542175293
Step: 32720, train/loss: 0.0
Step: 32720, train/grad_norm: 2.9183384597786244e-09
Step: 32720, train/learning_rate: 1.1066158549510874e-05
Step: 32720, train/epoch: 7.786768436431885
Step: 32730, train/loss: 0.0
Step: 32730, train/grad_norm: 8.684934726943538e-08
Step: 32730, train/learning_rate: 1.1054259630327579e-05
Step: 32730, train/epoch: 7.789147853851318
Step: 32740, train/loss: 0.0
Step: 32740, train/grad_norm: 7.557556247483888e-10
Step: 32740, train/learning_rate: 1.1042360711144283e-05
Step: 32740, train/epoch: 7.79152774810791
Step: 32750, train/loss: 0.0
Step: 32750, train/grad_norm: 7.544707913975657e-11
Step: 32750, train/learning_rate: 1.1030461791960988e-05
Step: 32750, train/epoch: 7.793907642364502
Step: 32760, train/loss: 0.0
Step: 32760, train/grad_norm: 6.438716226853103e-09
Step: 32760, train/learning_rate: 1.1018562872777693e-05
Step: 32760, train/epoch: 7.796287536621094
Step: 32770, train/loss: 0.0
Step: 32770, train/grad_norm: 1.9414754248714416e-08
Step: 32770, train/learning_rate: 1.1006663044099696e-05
Step: 32770, train/epoch: 7.7986674308776855
Step: 32780, train/loss: 0.0
Step: 32780, train/grad_norm: 9.689534635981545e-06
Step: 32780, train/learning_rate: 1.09947641249164e-05
Step: 32780, train/epoch: 7.801047325134277
Step: 32790, train/loss: 0.0
Step: 32790, train/grad_norm: 5.73369129952539e-09
Step: 32790, train/learning_rate: 1.0982865205733106e-05
Step: 32790, train/epoch: 7.803426742553711
Step: 32800, train/loss: 0.0
Step: 32800, train/grad_norm: 7.003224222756899e-09
Step: 32800, train/learning_rate: 1.097096628654981e-05
Step: 32800, train/epoch: 7.805806636810303
Step: 32810, train/loss: 0.0
Step: 32810, train/grad_norm: 5.738919894859862e-10
Step: 32810, train/learning_rate: 1.0959067367366515e-05
Step: 32810, train/epoch: 7.8081865310668945
Step: 32820, train/loss: 0.0
Step: 32820, train/grad_norm: 5.47938761030764e-09
Step: 32820, train/learning_rate: 1.094716844818322e-05
Step: 32820, train/epoch: 7.810566425323486
Step: 32830, train/loss: 0.0
Step: 32830, train/grad_norm: 6.399057672634711e-11
Step: 32830, train/learning_rate: 1.0935268619505223e-05
Step: 32830, train/epoch: 7.812946319580078
Step: 32840, train/loss: 0.0
Step: 32840, train/grad_norm: 2.2241442021453395e-09
Step: 32840, train/learning_rate: 1.0923369700321928e-05
Step: 32840, train/epoch: 7.81532621383667
Step: 32850, train/loss: 0.0
Step: 32850, train/grad_norm: 3.0763510228820623e-12
Step: 32850, train/learning_rate: 1.0911470781138632e-05
Step: 32850, train/epoch: 7.8177056312561035
Step: 32860, train/loss: 0.0
Step: 32860, train/grad_norm: 9.742667117507153e-09
Step: 32860, train/learning_rate: 1.0899571861955337e-05
Step: 32860, train/epoch: 7.820085525512695
Step: 32870, train/loss: 0.0
Step: 32870, train/grad_norm: 1.897087820168508e-09
Step: 32870, train/learning_rate: 1.0887672942772042e-05
Step: 32870, train/epoch: 7.822465419769287
Step: 32880, train/loss: 0.0
Step: 32880, train/grad_norm: 8.450378174984507e-10
Step: 32880, train/learning_rate: 1.0875773114094045e-05
Step: 32880, train/epoch: 7.824845314025879
Step: 32890, train/loss: 0.0
Step: 32890, train/grad_norm: 5.0801347128981433e-08
Step: 32890, train/learning_rate: 1.086387419491075e-05
Step: 32890, train/epoch: 7.827225208282471
Step: 32900, train/loss: 0.0
Step: 32900, train/grad_norm: 1.4932711067672244e-08
Step: 32900, train/learning_rate: 1.0851975275727455e-05
Step: 32900, train/epoch: 7.8296051025390625
Step: 32910, train/loss: 0.0
Step: 32910, train/grad_norm: 7.957429488669732e-07
Step: 32910, train/learning_rate: 1.084007635654416e-05
Step: 32910, train/epoch: 7.831984996795654
Step: 32920, train/loss: 0.0
Step: 32920, train/grad_norm: 2.9175907911849208e-05
Step: 32920, train/learning_rate: 1.0828177437360864e-05
Step: 32920, train/epoch: 7.834364414215088
Step: 32930, train/loss: 0.0
Step: 32930, train/grad_norm: 2.2330630400357876e-11
Step: 32930, train/learning_rate: 1.0816277608682867e-05
Step: 32930, train/epoch: 7.83674430847168
Step: 32940, train/loss: 0.0
Step: 32940, train/grad_norm: 2.612554617087426e-09
Step: 32940, train/learning_rate: 1.0804378689499572e-05
Step: 32940, train/epoch: 7.8391242027282715
Step: 32950, train/loss: 0.0
Step: 32950, train/grad_norm: 4.992060098629736e-10
Step: 32950, train/learning_rate: 1.0792479770316277e-05
Step: 32950, train/epoch: 7.841504096984863
Step: 32960, train/loss: 0.0
Step: 32960, train/grad_norm: 9.871533848127e-07
Step: 32960, train/learning_rate: 1.0780580851132981e-05
Step: 32960, train/epoch: 7.843883991241455
Step: 32970, train/loss: 0.0
Step: 32970, train/grad_norm: 3.077157972875e-10
Step: 32970, train/learning_rate: 1.0768681931949686e-05
Step: 32970, train/epoch: 7.846263885498047
Step: 32980, train/loss: 0.0
Step: 32980, train/grad_norm: 1.347967781839543e-08
Step: 32980, train/learning_rate: 1.075678210327169e-05
Step: 32980, train/epoch: 7.8486433029174805
Step: 32990, train/loss: 0.0
Step: 32990, train/grad_norm: 7.730884021839302e-08
Step: 32990, train/learning_rate: 1.0744883184088394e-05
Step: 32990, train/epoch: 7.851023197174072
Step: 33000, train/loss: 0.0
Step: 33000, train/grad_norm: 2.8034678507538047e-06
Step: 33000, train/learning_rate: 1.0732984264905099e-05
Step: 33000, train/epoch: 7.853403091430664
Step: 33010, train/loss: 0.0
Step: 33010, train/grad_norm: 1.994390208537311e-09
Step: 33010, train/learning_rate: 1.0721085345721804e-05
Step: 33010, train/epoch: 7.855782985687256
Step: 33020, train/loss: 0.0
Step: 33020, train/grad_norm: 1.4276947979396937e-07
Step: 33020, train/learning_rate: 1.0709186426538508e-05
Step: 33020, train/epoch: 7.858162879943848
Step: 33030, train/loss: 0.0
Step: 33030, train/grad_norm: 3.161315376143392e-10
Step: 33030, train/learning_rate: 1.0697286597860511e-05
Step: 33030, train/epoch: 7.8605427742004395
Step: 33040, train/loss: 0.0
Step: 33040, train/grad_norm: 1.168432128828556e-09
Step: 33040, train/learning_rate: 1.0685387678677216e-05
Step: 33040, train/epoch: 7.862922191619873
Step: 33050, train/loss: 0.0
Step: 33050, train/grad_norm: 2.0832353619937294e-09
Step: 33050, train/learning_rate: 1.0673488759493921e-05
Step: 33050, train/epoch: 7.865302085876465
Step: 33060, train/loss: 0.0
Step: 33060, train/grad_norm: 4.340833470450889e-09
Step: 33060, train/learning_rate: 1.0661589840310626e-05
Step: 33060, train/epoch: 7.867681980133057
Step: 33070, train/loss: 0.0
Step: 33070, train/grad_norm: 4.226389478390047e-07
Step: 33070, train/learning_rate: 1.064969092112733e-05
Step: 33070, train/epoch: 7.870061874389648
Step: 33080, train/loss: 0.0
Step: 33080, train/grad_norm: 1.2337995902722554e-10
Step: 33080, train/learning_rate: 1.0637791092449334e-05
Step: 33080, train/epoch: 7.87244176864624
Step: 33090, train/loss: 0.0
Step: 33090, train/grad_norm: 7.556121062179955e-08
Step: 33090, train/learning_rate: 1.0625892173266038e-05
Step: 33090, train/epoch: 7.874821662902832
Step: 33100, train/loss: 0.0
Step: 33100, train/grad_norm: 1.8430380485656706e-11
Step: 33100, train/learning_rate: 1.0613993254082743e-05
Step: 33100, train/epoch: 7.877201557159424
Step: 33110, train/loss: 0.0
Step: 33110, train/grad_norm: 3.379661828173397e-10
Step: 33110, train/learning_rate: 1.0602094334899448e-05
Step: 33110, train/epoch: 7.879580974578857
Step: 33120, train/loss: 0.0
Step: 33120, train/grad_norm: 3.617487975837719e-10
Step: 33120, train/learning_rate: 1.0590195415716153e-05
Step: 33120, train/epoch: 7.881960868835449
Step: 33130, train/loss: 0.0
Step: 33130, train/grad_norm: 3.7416409526258576e-08
Step: 33130, train/learning_rate: 1.0578296496532857e-05
Step: 33130, train/epoch: 7.884340763092041
Step: 33140, train/loss: 0.0
Step: 33140, train/grad_norm: 3.0934341893607e-11
Step: 33140, train/learning_rate: 1.056639666785486e-05
Step: 33140, train/epoch: 7.886720657348633
Step: 33150, train/loss: 0.0
Step: 33150, train/grad_norm: 1.250705511379735e-10
Step: 33150, train/learning_rate: 1.0554497748671565e-05
Step: 33150, train/epoch: 7.889100551605225
Step: 33160, train/loss: 0.0
Step: 33160, train/grad_norm: 2.987537683907249e-08
Step: 33160, train/learning_rate: 1.054259882948827e-05
Step: 33160, train/epoch: 7.891480445861816
Step: 33170, train/loss: 0.0
Step: 33170, train/grad_norm: 3.7097036553745966e-09
Step: 33170, train/learning_rate: 1.0530699910304975e-05
Step: 33170, train/epoch: 7.89385986328125
Step: 33180, train/loss: 0.0
Step: 33180, train/grad_norm: 1.699560847612247e-08
Step: 33180, train/learning_rate: 1.051880099112168e-05
Step: 33180, train/epoch: 7.896239757537842
Step: 33190, train/loss: 0.0
Step: 33190, train/grad_norm: 3.292864245163507e-11
Step: 33190, train/learning_rate: 1.0506901162443683e-05
Step: 33190, train/epoch: 7.898619651794434
Step: 33200, train/loss: 0.0
Step: 33200, train/grad_norm: 1.4942513004712055e-08
Step: 33200, train/learning_rate: 1.0495002243260387e-05
Step: 33200, train/epoch: 7.900999546051025
Step: 33210, train/loss: 0.0
Step: 33210, train/grad_norm: 2.5395990865817453e-10
Step: 33210, train/learning_rate: 1.0483103324077092e-05
Step: 33210, train/epoch: 7.903379440307617
Step: 33220, train/loss: 0.0
Step: 33220, train/grad_norm: 4.4907765006652056e-11
Step: 33220, train/learning_rate: 1.0471204404893797e-05
Step: 33220, train/epoch: 7.905759334564209
Step: 33230, train/loss: 0.0
Step: 33230, train/grad_norm: 3.261579308855289e-07
Step: 33230, train/learning_rate: 1.0459305485710502e-05
Step: 33230, train/epoch: 7.908138751983643
Step: 33240, train/loss: 0.0
Step: 33240, train/grad_norm: 2.1668007388342403e-09
Step: 33240, train/learning_rate: 1.0447405657032505e-05
Step: 33240, train/epoch: 7.910518646240234
Step: 33250, train/loss: 0.0
Step: 33250, train/grad_norm: 3.488680988539272e-08
Step: 33250, train/learning_rate: 1.043550673784921e-05
Step: 33250, train/epoch: 7.912898540496826
Step: 33260, train/loss: 0.0
Step: 33260, train/grad_norm: 3.934010692319134e-06
Step: 33260, train/learning_rate: 1.0423607818665914e-05
Step: 33260, train/epoch: 7.915278434753418
Step: 33270, train/loss: 0.0
Step: 33270, train/grad_norm: 1.806211997745777e-07
Step: 33270, train/learning_rate: 1.0411708899482619e-05
Step: 33270, train/epoch: 7.91765832901001
Step: 33280, train/loss: 0.0
Step: 33280, train/grad_norm: 5.48785905607474e-10
Step: 33280, train/learning_rate: 1.0399809980299324e-05
Step: 33280, train/epoch: 7.920038223266602
Step: 33290, train/loss: 0.0
Step: 33290, train/grad_norm: 1.3227176509644778e-07
Step: 33290, train/learning_rate: 1.0387910151621327e-05
Step: 33290, train/epoch: 7.922418117523193
Step: 33300, train/loss: 0.0
Step: 33300, train/grad_norm: 3.800884607052524e-11
Step: 33300, train/learning_rate: 1.0376011232438032e-05
Step: 33300, train/epoch: 7.924797534942627
Step: 33310, train/loss: 0.0
Step: 33310, train/grad_norm: 1.1287919932101431e-08
Step: 33310, train/learning_rate: 1.0364112313254736e-05
Step: 33310, train/epoch: 7.927177429199219
Step: 33320, train/loss: 0.0
Step: 33320, train/grad_norm: 1.1349644141178672e-10
Step: 33320, train/learning_rate: 1.0352213394071441e-05
Step: 33320, train/epoch: 7.9295573234558105
Step: 33330, train/loss: 0.0
Step: 33330, train/grad_norm: 1.891576495438585e-08
Step: 33330, train/learning_rate: 1.0340314474888146e-05
Step: 33330, train/epoch: 7.931937217712402
Step: 33340, train/loss: 0.0
Step: 33340, train/grad_norm: 6.913507149874931e-06
Step: 33340, train/learning_rate: 1.0328414646210149e-05
Step: 33340, train/epoch: 7.934317111968994
Step: 33350, train/loss: 0.0
Step: 33350, train/grad_norm: 5.847170747586006e-09
Step: 33350, train/learning_rate: 1.0316515727026854e-05
Step: 33350, train/epoch: 7.936697006225586
Step: 33360, train/loss: 0.0
Step: 33360, train/grad_norm: 3.674491821925585e-09
Step: 33360, train/learning_rate: 1.0304616807843558e-05
Step: 33360, train/epoch: 7.9390764236450195
Step: 33370, train/loss: 0.0
Step: 33370, train/grad_norm: 2.75943108363208e-08
Step: 33370, train/learning_rate: 1.0292717888660263e-05
Step: 33370, train/epoch: 7.941456317901611
Step: 33380, train/loss: 0.0
Step: 33380, train/grad_norm: 8.075780755234518e-08
Step: 33380, train/learning_rate: 1.0280818969476968e-05
Step: 33380, train/epoch: 7.943836212158203
Step: 33390, train/loss: 0.0
Step: 33390, train/grad_norm: 7.305759330833439e-10
Step: 33390, train/learning_rate: 1.0268919140798971e-05
Step: 33390, train/epoch: 7.946216106414795
Step: 33400, train/loss: 0.0
Step: 33400, train/grad_norm: 3.424334948931573e-08
Step: 33400, train/learning_rate: 1.0257020221615676e-05
Step: 33400, train/epoch: 7.948596000671387
Step: 33410, train/loss: 0.0
Step: 33410, train/grad_norm: 2.0928219157667627e-09
Step: 33410, train/learning_rate: 1.024512130243238e-05
Step: 33410, train/epoch: 7.9509758949279785
Step: 33420, train/loss: 0.0
Step: 33420, train/grad_norm: 1.3157125389184898e-10
Step: 33420, train/learning_rate: 1.0233222383249085e-05
Step: 33420, train/epoch: 7.953355312347412
Step: 33430, train/loss: 0.0
Step: 33430, train/grad_norm: 4.6682501420036715e-08
Step: 33430, train/learning_rate: 1.022132346406579e-05
Step: 33430, train/epoch: 7.955735206604004
Step: 33440, train/loss: 0.0
Step: 33440, train/grad_norm: 5.106734479731756e-10
Step: 33440, train/learning_rate: 1.0209423635387793e-05
Step: 33440, train/epoch: 7.958115100860596
Step: 33450, train/loss: 0.0
Step: 33450, train/grad_norm: 1.0830555652319163e-07
Step: 33450, train/learning_rate: 1.0197524716204498e-05
Step: 33450, train/epoch: 7.9604949951171875
Step: 33460, train/loss: 0.0
Step: 33460, train/grad_norm: 9.570850188767022e-12
Step: 33460, train/learning_rate: 1.0185625797021203e-05
Step: 33460, train/epoch: 7.962874889373779
Step: 33470, train/loss: 0.0
Step: 33470, train/grad_norm: 1.0270456485272916e-08
Step: 33470, train/learning_rate: 1.0173726877837908e-05
Step: 33470, train/epoch: 7.965254783630371
Step: 33480, train/loss: 0.0
Step: 33480, train/grad_norm: 2.235196749911239e-10
Step: 33480, train/learning_rate: 1.0161827958654612e-05
Step: 33480, train/epoch: 7.967634677886963
Step: 33490, train/loss: 0.0
Step: 33490, train/grad_norm: 6.10562378433599e-10
Step: 33490, train/learning_rate: 1.0149929039471317e-05
Step: 33490, train/epoch: 7.9700140953063965
Step: 33500, train/loss: 0.0
Step: 33500, train/grad_norm: 1.6102111310978273e-10
Step: 33500, train/learning_rate: 1.013802921079332e-05
Step: 33500, train/epoch: 7.972393989562988
Step: 33510, train/loss: 0.0
Step: 33510, train/grad_norm: 2.3506233404191335e-08
Step: 33510, train/learning_rate: 1.0126130291610025e-05
Step: 33510, train/epoch: 7.97477388381958
Step: 33520, train/loss: 0.0
Step: 33520, train/grad_norm: 3.105822656834789e-07
Step: 33520, train/learning_rate: 1.011423137242673e-05
Step: 33520, train/epoch: 7.977153778076172
Step: 33530, train/loss: 0.0
Step: 33530, train/grad_norm: 3.684464955355793e-10
Step: 33530, train/learning_rate: 1.0102332453243434e-05
Step: 33530, train/epoch: 7.979533672332764
Step: 33540, train/loss: 0.0
Step: 33540, train/grad_norm: 1.5398083849649247e-11
Step: 33540, train/learning_rate: 1.009043353406014e-05
Step: 33540, train/epoch: 7.9819135665893555
Step: 33550, train/loss: 0.0
Step: 33550, train/grad_norm: 2.0622097363087732e-09
Step: 33550, train/learning_rate: 1.0078533705382142e-05
Step: 33550, train/epoch: 7.984292984008789
Step: 33560, train/loss: 0.0
Step: 33560, train/grad_norm: 3.187474533206114e-07
Step: 33560, train/learning_rate: 1.0066634786198847e-05
Step: 33560, train/epoch: 7.986672878265381
Step: 33570, train/loss: 0.0
Step: 33570, train/grad_norm: 1.402986384846372e-07
Step: 33570, train/learning_rate: 1.0054735867015552e-05
Step: 33570, train/epoch: 7.989052772521973
Step: 33580, train/loss: 0.0
Step: 33580, train/grad_norm: 5.186501006448907e-09
Step: 33580, train/learning_rate: 1.0042836947832257e-05
Step: 33580, train/epoch: 7.9914326667785645
Step: 33590, train/loss: 0.0
Step: 33590, train/grad_norm: 8.100073389805118e-10
Step: 33590, train/learning_rate: 1.0030938028648961e-05
Step: 33590, train/epoch: 7.993812561035156
Step: 33600, train/loss: 0.0
Step: 33600, train/grad_norm: 1.7607222624338448e-10
Step: 33600, train/learning_rate: 1.0019038199970964e-05
Step: 33600, train/epoch: 7.996192455291748
Step: 33610, train/loss: 0.0
Step: 33610, train/grad_norm: 2.032767704163163e-11
Step: 33610, train/learning_rate: 1.0007139280787669e-05
Step: 33610, train/epoch: 7.998571872711182
Step: 33616, eval/loss: 0.04672269523143768
Step: 33616, eval/accuracy: 0.9956962466239929
Step: 33616, eval/f1: 0.995450496673584
Step: 33616, eval/runtime: 707.955810546875
Step: 33616, eval/samples_per_second: 10.173999786376953
Step: 33616, eval/steps_per_second: 1.2730000019073486
Step: 33616, train/epoch: 8.0
Step: 33620, train/loss: 0.0
Step: 33620, train/grad_norm: 2.435555579438642e-08
Step: 33620, train/learning_rate: 9.995240361604374e-06
Step: 33620, train/epoch: 8.000951766967773
Step: 33630, train/loss: 0.0
Step: 33630, train/grad_norm: 6.595852336532815e-11
Step: 33630, train/learning_rate: 9.983341442421079e-06
Step: 33630, train/epoch: 8.003332138061523
Step: 33640, train/loss: 0.0
Step: 33640, train/grad_norm: 2.495902963473018e-11
Step: 33640, train/learning_rate: 9.971442523237783e-06
Step: 33640, train/epoch: 8.005711555480957
Step: 33650, train/loss: 0.0
Step: 33650, train/grad_norm: 5.032816261518747e-07
Step: 33650, train/learning_rate: 9.959542694559786e-06
Step: 33650, train/epoch: 8.00809097290039
Step: 33660, train/loss: 0.0
Step: 33660, train/grad_norm: 9.388159138978835e-10
Step: 33660, train/learning_rate: 9.947643775376491e-06
Step: 33660, train/epoch: 8.01047134399414
Step: 33670, train/loss: 0.0
Step: 33670, train/grad_norm: 2.1414341654091285e-12
Step: 33670, train/learning_rate: 9.935744856193196e-06
Step: 33670, train/epoch: 8.012850761413574
Step: 33680, train/loss: 0.0
Step: 33680, train/grad_norm: 7.177996530494113e-10
Step: 33680, train/learning_rate: 9.9238459370099e-06
Step: 33680, train/epoch: 8.015231132507324
Step: 33690, train/loss: 0.0
Step: 33690, train/grad_norm: 1.3722893044132434e-08
Step: 33690, train/learning_rate: 9.911947017826606e-06
Step: 33690, train/epoch: 8.017610549926758
Step: 33700, train/loss: 0.0
Step: 33700, train/grad_norm: 2.60316198819055e-07
Step: 33700, train/learning_rate: 9.900047189148609e-06
Step: 33700, train/epoch: 8.019990921020508
Step: 33710, train/loss: 0.0
Step: 33710, train/grad_norm: 3.5170664158812315e-09
Step: 33710, train/learning_rate: 9.888148269965313e-06
Step: 33710, train/epoch: 8.022370338439941
Step: 33720, train/loss: 0.0
Step: 33720, train/grad_norm: 1.4089628130875553e-08
Step: 33720, train/learning_rate: 9.876249350782018e-06
Step: 33720, train/epoch: 8.024749755859375
Step: 33730, train/loss: 0.0
Step: 33730, train/grad_norm: 9.038786386028619e-11
Step: 33730, train/learning_rate: 9.864350431598723e-06
Step: 33730, train/epoch: 8.027130126953125
Step: 33740, train/loss: 0.0
Step: 33740, train/grad_norm: 1.2514324299051083e-10
Step: 33740, train/learning_rate: 9.852451512415428e-06
Step: 33740, train/epoch: 8.029509544372559
Step: 33750, train/loss: 0.0
Step: 33750, train/grad_norm: 7.061470519431623e-09
Step: 33750, train/learning_rate: 9.84055168373743e-06
Step: 33750, train/epoch: 8.031889915466309
Step: 33760, train/loss: 0.0
Step: 33760, train/grad_norm: 2.71248551397818e-10
Step: 33760, train/learning_rate: 9.828652764554136e-06
Step: 33760, train/epoch: 8.034269332885742
Step: 33770, train/loss: 0.0
Step: 33770, train/grad_norm: 1.2366203616664961e-09
Step: 33770, train/learning_rate: 9.81675384537084e-06
Step: 33770, train/epoch: 8.036648750305176
Step: 33780, train/loss: 0.0
Step: 33780, train/grad_norm: 5.679907499195336e-12
Step: 33780, train/learning_rate: 9.804854926187545e-06
Step: 33780, train/epoch: 8.039029121398926
Step: 33790, train/loss: 0.0
Step: 33790, train/grad_norm: 5.072914088799507e-09
Step: 33790, train/learning_rate: 9.79295600700425e-06
Step: 33790, train/epoch: 8.04140853881836
Step: 33800, train/loss: 0.0
Step: 33800, train/grad_norm: 8.905635257017153e-11
Step: 33800, train/learning_rate: 9.781057087820955e-06
Step: 33800, train/epoch: 8.04378890991211
Step: 33810, train/loss: 0.0
Step: 33810, train/grad_norm: 4.64606964012404e-11
Step: 33810, train/learning_rate: 9.769157259142958e-06
Step: 33810, train/epoch: 8.046168327331543
Step: 33820, train/loss: 0.0
Step: 33820, train/grad_norm: 1.725930271097298e-12
Step: 33820, train/learning_rate: 9.757258339959662e-06
Step: 33820, train/epoch: 8.048548698425293
Step: 33830, train/loss: 0.0
Step: 33830, train/grad_norm: 2.3147829608660686e-07
Step: 33830, train/learning_rate: 9.745359420776367e-06
Step: 33830, train/epoch: 8.050928115844727
Step: 33840, train/loss: 0.0
Step: 33840, train/grad_norm: 2.86191043086248e-10
Step: 33840, train/learning_rate: 9.733460501593072e-06
Step: 33840, train/epoch: 8.05330753326416
Step: 33850, train/loss: 0.0
Step: 33850, train/grad_norm: 1.5125355412592967e-09
Step: 33850, train/learning_rate: 9.721561582409777e-06
Step: 33850, train/epoch: 8.05568790435791
Step: 33860, train/loss: 0.0
Step: 33860, train/grad_norm: 6.20358022729306e-08
Step: 33860, train/learning_rate: 9.70966175373178e-06
Step: 33860, train/epoch: 8.058067321777344
Step: 33870, train/loss: 0.0
Step: 33870, train/grad_norm: 1.946037081479446e-12
Step: 33870, train/learning_rate: 9.697762834548485e-06
Step: 33870, train/epoch: 8.060447692871094
Step: 33880, train/loss: 0.0
Step: 33880, train/grad_norm: 1.057159637607441e-10
Step: 33880, train/learning_rate: 9.68586391536519e-06
Step: 33880, train/epoch: 8.062827110290527
Step: 33890, train/loss: 0.0
Step: 33890, train/grad_norm: 3.630937044363236e-07
Step: 33890, train/learning_rate: 9.673964996181894e-06
Step: 33890, train/epoch: 8.065207481384277
Step: 33900, train/loss: 0.0
Step: 33900, train/grad_norm: 6.138382857567848e-11
Step: 33900, train/learning_rate: 9.662066076998599e-06
Step: 33900, train/epoch: 8.067586898803711
Step: 33910, train/loss: 0.0
Step: 33910, train/grad_norm: 4.052412183597198e-09
Step: 33910, train/learning_rate: 9.650166248320602e-06
Step: 33910, train/epoch: 8.069966316223145
Step: 33920, train/loss: 0.0
Step: 33920, train/grad_norm: 7.52058539887912e-08
Step: 33920, train/learning_rate: 9.638267329137307e-06
Step: 33920, train/epoch: 8.072346687316895
Step: 33930, train/loss: 0.0
Step: 33930, train/grad_norm: 2.89168438030174e-05
Step: 33930, train/learning_rate: 9.626368409954011e-06
Step: 33930, train/epoch: 8.074726104736328
Step: 33940, train/loss: 0.0
Step: 33940, train/grad_norm: 2.87674675747418e-11
Step: 33940, train/learning_rate: 9.614469490770716e-06
Step: 33940, train/epoch: 8.077106475830078
Step: 33950, train/loss: 0.0
Step: 33950, train/grad_norm: 2.713540538934467e-07
Step: 33950, train/learning_rate: 9.602570571587421e-06
Step: 33950, train/epoch: 8.079485893249512
Step: 33960, train/loss: 0.0
Step: 33960, train/grad_norm: 8.770548731718009e-09
Step: 33960, train/learning_rate: 9.590670742909424e-06
Step: 33960, train/epoch: 8.081865310668945
Step: 33970, train/loss: 0.0
Step: 33970, train/grad_norm: 3.691883909695548e-09
Step: 33970, train/learning_rate: 9.578771823726129e-06
Step: 33970, train/epoch: 8.084245681762695
Step: 33980, train/loss: 0.0
Step: 33980, train/grad_norm: 4.2136844013995756e-10
Step: 33980, train/learning_rate: 9.566872904542834e-06
Step: 33980, train/epoch: 8.086625099182129
Step: 33990, train/loss: 0.0
Step: 33990, train/grad_norm: 6.985820255600572e-10
Step: 33990, train/learning_rate: 9.554973985359538e-06
Step: 33990, train/epoch: 8.089005470275879
Step: 34000, train/loss: 0.0
Step: 34000, train/grad_norm: 1.128004356587553e-09
Step: 34000, train/learning_rate: 9.543075066176243e-06
Step: 34000, train/epoch: 8.091384887695312
Step: 34010, train/loss: 0.0
Step: 34010, train/grad_norm: 3.903380049741223e-11
Step: 34010, train/learning_rate: 9.531175237498246e-06
Step: 34010, train/epoch: 8.093765258789062
Step: 34020, train/loss: 0.0
Step: 34020, train/grad_norm: 1.603085308943264e-08
Step: 34020, train/learning_rate: 9.519276318314951e-06
Step: 34020, train/epoch: 8.096144676208496
Step: 34030, train/loss: 0.0
Step: 34030, train/grad_norm: 5.7126037233956595e-09
Step: 34030, train/learning_rate: 9.507377399131656e-06
Step: 34030, train/epoch: 8.09852409362793
Step: 34040, train/loss: 0.0
Step: 34040, train/grad_norm: 2.8199493087854854e-11
Step: 34040, train/learning_rate: 9.49547847994836e-06
Step: 34040, train/epoch: 8.10090446472168
Step: 34050, train/loss: 0.0
Step: 34050, train/grad_norm: 1.7313761091486413e-08
Step: 34050, train/learning_rate: 9.483579560765065e-06
Step: 34050, train/epoch: 8.103283882141113
Step: 34060, train/loss: 0.0
Step: 34060, train/grad_norm: 3.258388281413005e-10
Step: 34060, train/learning_rate: 9.471679732087068e-06
Step: 34060, train/epoch: 8.105664253234863
Step: 34070, train/loss: 0.0
Step: 34070, train/grad_norm: 8.355960542905886e-15
Step: 34070, train/learning_rate: 9.459780812903773e-06
Step: 34070, train/epoch: 8.108043670654297
Step: 34080, train/loss: 0.0
Step: 34080, train/grad_norm: 4.564222777503346e-09
Step: 34080, train/learning_rate: 9.447881893720478e-06
Step: 34080, train/epoch: 8.110424041748047
Step: 34090, train/loss: 0.0
Step: 34090, train/grad_norm: 1.8136461299791962e-12
Step: 34090, train/learning_rate: 9.435982974537183e-06
Step: 34090, train/epoch: 8.11280345916748
Step: 34100, train/loss: 0.0
Step: 34100, train/grad_norm: 5.841496557301706e-12
Step: 34100, train/learning_rate: 9.424084055353887e-06
Step: 34100, train/epoch: 8.115182876586914
Step: 34110, train/loss: 0.0
Step: 34110, train/grad_norm: 4.0871774714679177e-11
Step: 34110, train/learning_rate: 9.41218422667589e-06
Step: 34110, train/epoch: 8.117563247680664
Step: 34120, train/loss: 0.0
Step: 34120, train/grad_norm: 2.823321264278089e-11
Step: 34120, train/learning_rate: 9.400285307492595e-06
Step: 34120, train/epoch: 8.119942665100098
Step: 34130, train/loss: 0.0
Step: 34130, train/grad_norm: 1.8959001124585484e-07
Step: 34130, train/learning_rate: 9.3883863883093e-06
Step: 34130, train/epoch: 8.122323036193848
Step: 34140, train/loss: 0.0
Step: 34140, train/grad_norm: 8.432358145071817e-10
Step: 34140, train/learning_rate: 9.376487469126005e-06
Step: 34140, train/epoch: 8.124702453613281
Step: 34150, train/loss: 0.0
Step: 34150, train/grad_norm: 1.3309088206514552e-11
Step: 34150, train/learning_rate: 9.36458854994271e-06
Step: 34150, train/epoch: 8.127081871032715
Step: 34160, train/loss: 0.0
Step: 34160, train/grad_norm: 2.1425283769360703e-09
Step: 34160, train/learning_rate: 9.352689630759414e-06
Step: 34160, train/epoch: 8.129462242126465
Step: 34170, train/loss: 0.0
Step: 34170, train/grad_norm: 5.3798597521304004e-11
Step: 34170, train/learning_rate: 9.340789802081417e-06
Step: 34170, train/epoch: 8.131841659545898
Step: 34180, train/loss: 0.0
Step: 34180, train/grad_norm: 1.3936250591040089e-11
Step: 34180, train/learning_rate: 9.328890882898122e-06
Step: 34180, train/epoch: 8.134222030639648
Step: 34190, train/loss: 0.0
Step: 34190, train/grad_norm: 2.8364316104201315e-11
Step: 34190, train/learning_rate: 9.316991963714827e-06
Step: 34190, train/epoch: 8.136601448059082
Step: 34200, train/loss: 0.0
Step: 34200, train/grad_norm: 2.157171663519364e-11
Step: 34200, train/learning_rate: 9.305093044531532e-06
Step: 34200, train/epoch: 8.138981819152832
Step: 34210, train/loss: 0.0
Step: 34210, train/grad_norm: 2.5632423700933105e-12
Step: 34210, train/learning_rate: 9.293194125348236e-06
Step: 34210, train/epoch: 8.141361236572266
Step: 34220, train/loss: 0.0
Step: 34220, train/grad_norm: 2.1056248411532863e-10
Step: 34220, train/learning_rate: 9.28129429667024e-06
Step: 34220, train/epoch: 8.1437406539917
Step: 34230, train/loss: 0.0
Step: 34230, train/grad_norm: 3.017876504252115e-10
Step: 34230, train/learning_rate: 9.269395377486944e-06
Step: 34230, train/epoch: 8.14612102508545
Step: 34240, train/loss: 0.0
Step: 34240, train/grad_norm: 3.96371852007249e-10
Step: 34240, train/learning_rate: 9.257496458303649e-06
Step: 34240, train/epoch: 8.148500442504883
Step: 34250, train/loss: 0.0
Step: 34250, train/grad_norm: 2.069216549216435e-07
Step: 34250, train/learning_rate: 9.245597539120354e-06
Step: 34250, train/epoch: 8.150880813598633
Step: 34260, train/loss: 0.0
Step: 34260, train/grad_norm: 3.0967475250776033e-09
Step: 34260, train/learning_rate: 9.233698619937059e-06
Step: 34260, train/epoch: 8.153260231018066
Step: 34270, train/loss: 0.0
Step: 34270, train/grad_norm: 8.668448714388433e-08
Step: 34270, train/learning_rate: 9.221798791259062e-06
Step: 34270, train/epoch: 8.155640602111816
Step: 34280, train/loss: 0.0
Step: 34280, train/grad_norm: 1.1170450120445707e-09
Step: 34280, train/learning_rate: 9.209899872075766e-06
Step: 34280, train/epoch: 8.15802001953125
Step: 34290, train/loss: 0.0
Step: 34290, train/grad_norm: 3.715226171152608e-08
Step: 34290, train/learning_rate: 9.198000952892471e-06
Step: 34290, train/epoch: 8.160399436950684
Step: 34300, train/loss: 0.0
Step: 34300, train/grad_norm: 2.0126956101762516e-08
Step: 34300, train/learning_rate: 9.186102033709176e-06
Step: 34300, train/epoch: 8.162779808044434
Step: 34310, train/loss: 0.0
Step: 34310, train/grad_norm: 5.614450224178391e-11
Step: 34310, train/learning_rate: 9.17420311452588e-06
Step: 34310, train/epoch: 8.165159225463867
Step: 34320, train/loss: 0.0
Step: 34320, train/grad_norm: 1.0857655219354001e-08
Step: 34320, train/learning_rate: 9.162303285847884e-06
Step: 34320, train/epoch: 8.167539596557617
Step: 34330, train/loss: 0.0
Step: 34330, train/grad_norm: 2.8629734694085585e-10
Step: 34330, train/learning_rate: 9.150404366664588e-06
Step: 34330, train/epoch: 8.16991901397705
Step: 34340, train/loss: 0.0
Step: 34340, train/grad_norm: 1.1242301534153398e-10
Step: 34340, train/learning_rate: 9.138505447481293e-06
Step: 34340, train/epoch: 8.172298431396484
Step: 34350, train/loss: 0.0
Step: 34350, train/grad_norm: 1.3663037368161213e-10
Step: 34350, train/learning_rate: 9.126606528297998e-06
Step: 34350, train/epoch: 8.174678802490234
Step: 34360, train/loss: 0.0
Step: 34360, train/grad_norm: 1.2686644790260715e-10
Step: 34360, train/learning_rate: 9.114707609114703e-06
Step: 34360, train/epoch: 8.177058219909668
Step: 34370, train/loss: 0.0
Step: 34370, train/grad_norm: 4.340295050447862e-12
Step: 34370, train/learning_rate: 9.102807780436706e-06
Step: 34370, train/epoch: 8.179438591003418
Step: 34380, train/loss: 0.0
Step: 34380, train/grad_norm: 1.096391311072864e-09
Step: 34380, train/learning_rate: 9.09090886125341e-06
Step: 34380, train/epoch: 8.181818008422852
Step: 34390, train/loss: 0.0
Step: 34390, train/grad_norm: 5.979131856292952e-09
Step: 34390, train/learning_rate: 9.079009942070115e-06
Step: 34390, train/epoch: 8.184198379516602
Step: 34400, train/loss: 0.0
Step: 34400, train/grad_norm: 3.243694202126335e-09
Step: 34400, train/learning_rate: 9.06711102288682e-06
Step: 34400, train/epoch: 8.186577796936035
Step: 34410, train/loss: 0.0
Step: 34410, train/grad_norm: 1.4087174135535285e-11
Step: 34410, train/learning_rate: 9.055212103703525e-06
Step: 34410, train/epoch: 8.188957214355469
Step: 34420, train/loss: 0.0
Step: 34420, train/grad_norm: 6.60116517003928e-10
Step: 34420, train/learning_rate: 9.043312275025528e-06
Step: 34420, train/epoch: 8.191337585449219
Step: 34430, train/loss: 0.0
Step: 34430, train/grad_norm: 2.101891230510411e-11
Step: 34430, train/learning_rate: 9.031413355842233e-06
Step: 34430, train/epoch: 8.193717002868652
Step: 34440, train/loss: 0.0
Step: 34440, train/grad_norm: 4.402596343044962e-12
Step: 34440, train/learning_rate: 9.019514436658937e-06
Step: 34440, train/epoch: 8.196097373962402
Step: 34450, train/loss: 0.0
Step: 34450, train/grad_norm: 5.062790187082555e-10
Step: 34450, train/learning_rate: 9.007615517475642e-06
Step: 34450, train/epoch: 8.198476791381836
Step: 34460, train/loss: 0.0
Step: 34460, train/grad_norm: 3.637999013150761e-09
Step: 34460, train/learning_rate: 8.995716598292347e-06
Step: 34460, train/epoch: 8.200857162475586
Step: 34470, train/loss: 0.0
Step: 34470, train/grad_norm: 4.2200987149243474e-10
Step: 34470, train/learning_rate: 8.983817679109052e-06
Step: 34470, train/epoch: 8.20323657989502
Step: 34480, train/loss: 0.0
Step: 34480, train/grad_norm: 1.8933674628129893e-09
Step: 34480, train/learning_rate: 8.971917850431055e-06
Step: 34480, train/epoch: 8.205615997314453
Step: 34490, train/loss: 0.0
Step: 34490, train/grad_norm: 8.705952070364731e-10
Step: 34490, train/learning_rate: 8.96001893124776e-06
Step: 34490, train/epoch: 8.207996368408203
Step: 34500, train/loss: 0.0
Step: 34500, train/grad_norm: 1.2421751405256032e-09
Step: 34500, train/learning_rate: 8.948120012064464e-06
Step: 34500, train/epoch: 8.210375785827637
Step: 34510, train/loss: 0.0
Step: 34510, train/grad_norm: 1.976627972410938e-09
Step: 34510, train/learning_rate: 8.93622109288117e-06
Step: 34510, train/epoch: 8.212756156921387
Step: 34520, train/loss: 0.0
Step: 34520, train/grad_norm: 2.0859330106759444e-05
Step: 34520, train/learning_rate: 8.924322173697874e-06
Step: 34520, train/epoch: 8.21513557434082
Step: 34530, train/loss: 0.0
Step: 34530, train/grad_norm: 7.208182939422159e-10
Step: 34530, train/learning_rate: 8.912422345019877e-06
Step: 34530, train/epoch: 8.21751594543457
Step: 34540, train/loss: 0.0
Step: 34540, train/grad_norm: 8.968359907157719e-06
Step: 34540, train/learning_rate: 8.900523425836582e-06
Step: 34540, train/epoch: 8.219895362854004
Step: 34550, train/loss: 0.0
Step: 34550, train/grad_norm: 4.889206661057299e-12
Step: 34550, train/learning_rate: 8.888624506653287e-06
Step: 34550, train/epoch: 8.222274780273438
Step: 34560, train/loss: 0.0
Step: 34560, train/grad_norm: 6.035585897734563e-08
Step: 34560, train/learning_rate: 8.876725587469991e-06
Step: 34560, train/epoch: 8.224655151367188
Step: 34570, train/loss: 0.0
Step: 34570, train/grad_norm: 1.1509589809000076e-10
Step: 34570, train/learning_rate: 8.864826668286696e-06
Step: 34570, train/epoch: 8.227034568786621
Step: 34580, train/loss: 0.0
Step: 34580, train/grad_norm: 6.89873268129304e-05
Step: 34580, train/learning_rate: 8.852926839608699e-06
Step: 34580, train/epoch: 8.229414939880371
Step: 34590, train/loss: 0.0
Step: 34590, train/grad_norm: 1.0593391408519892e-07
Step: 34590, train/learning_rate: 8.841027920425404e-06
Step: 34590, train/epoch: 8.231794357299805
Step: 34600, train/loss: 0.0
Step: 34600, train/grad_norm: 3.282682428107364e-07
Step: 34600, train/learning_rate: 8.829129001242109e-06
Step: 34600, train/epoch: 8.234173774719238
Step: 34610, train/loss: 0.0
Step: 34610, train/grad_norm: 2.634189932848585e-08
Step: 34610, train/learning_rate: 8.817230082058813e-06
Step: 34610, train/epoch: 8.236554145812988
Step: 34620, train/loss: 0.0
Step: 34620, train/grad_norm: 4.109020346199088e-10
Step: 34620, train/learning_rate: 8.805331162875518e-06
Step: 34620, train/epoch: 8.238933563232422
Step: 34630, train/loss: 0.0
Step: 34630, train/grad_norm: 5.37048905346893e-10
Step: 34630, train/learning_rate: 8.793431334197521e-06
Step: 34630, train/epoch: 8.241313934326172
Step: 34640, train/loss: 0.0
Step: 34640, train/grad_norm: 4.5062281683883043e-10
Step: 34640, train/learning_rate: 8.781532415014226e-06
Step: 34640, train/epoch: 8.243693351745605
Step: 34650, train/loss: 0.0
Step: 34650, train/grad_norm: 5.961218324523898e-11
Step: 34650, train/learning_rate: 8.76963349583093e-06
Step: 34650, train/epoch: 8.246073722839355
Step: 34660, train/loss: 0.0
Step: 34660, train/grad_norm: 6.664548080070887e-11
Step: 34660, train/learning_rate: 8.757734576647636e-06
Step: 34660, train/epoch: 8.248453140258789
Step: 34670, train/loss: 0.0
Step: 34670, train/grad_norm: 2.0195964900437957e-09
Step: 34670, train/learning_rate: 8.74583565746434e-06
Step: 34670, train/epoch: 8.250832557678223
Step: 34680, train/loss: 0.0
Step: 34680, train/grad_norm: 2.1537177374852945e-08
Step: 34680, train/learning_rate: 8.733935828786343e-06
Step: 34680, train/epoch: 8.253212928771973
Step: 34690, train/loss: 0.0
Step: 34690, train/grad_norm: 2.912643992658559e-08
Step: 34690, train/learning_rate: 8.722036909603048e-06
Step: 34690, train/epoch: 8.255592346191406
Step: 34700, train/loss: 0.0
Step: 34700, train/grad_norm: 8.220340532716364e-07
Step: 34700, train/learning_rate: 8.710137990419753e-06
Step: 34700, train/epoch: 8.257972717285156
Step: 34710, train/loss: 0.0
Step: 34710, train/grad_norm: 1.3827849087988398e-08
Step: 34710, train/learning_rate: 8.698239071236458e-06
Step: 34710, train/epoch: 8.26035213470459
Step: 34720, train/loss: 0.0
Step: 34720, train/grad_norm: 7.723038875084853e-10
Step: 34720, train/learning_rate: 8.686340152053162e-06
Step: 34720, train/epoch: 8.26273250579834
Step: 34730, train/loss: 0.0
Step: 34730, train/grad_norm: 6.77324973885618e-10
Step: 34730, train/learning_rate: 8.674440323375165e-06
Step: 34730, train/epoch: 8.265111923217773
Step: 34740, train/loss: 0.0
Step: 34740, train/grad_norm: 3.3447735692249125e-09
Step: 34740, train/learning_rate: 8.66254140419187e-06
Step: 34740, train/epoch: 8.267491340637207
Step: 34750, train/loss: 0.0
Step: 34750, train/grad_norm: 2.2293411561236098e-10
Step: 34750, train/learning_rate: 8.650642485008575e-06
Step: 34750, train/epoch: 8.269871711730957
Step: 34760, train/loss: 0.0
Step: 34760, train/grad_norm: 6.267030272510965e-08
Step: 34760, train/learning_rate: 8.63874356582528e-06
Step: 34760, train/epoch: 8.27225112915039
Step: 34770, train/loss: 0.0
Step: 34770, train/grad_norm: 2.813019372993608e-09
Step: 34770, train/learning_rate: 8.626844646641985e-06
Step: 34770, train/epoch: 8.27463150024414
Step: 34780, train/loss: 0.0
Step: 34780, train/grad_norm: 2.4891498584533878e-11
Step: 34780, train/learning_rate: 8.614944817963988e-06
Step: 34780, train/epoch: 8.277010917663574
Step: 34790, train/loss: 0.0
Step: 34790, train/grad_norm: 1.0901609476254848e-10
Step: 34790, train/learning_rate: 8.603045898780692e-06
Step: 34790, train/epoch: 8.279390335083008
Step: 34800, train/loss: 0.0
Step: 34800, train/grad_norm: 2.018067135622914e-08
Step: 34800, train/learning_rate: 8.591146979597397e-06
Step: 34800, train/epoch: 8.281770706176758
Step: 34810, train/loss: 0.0
Step: 34810, train/grad_norm: 9.743943429896262e-10
Step: 34810, train/learning_rate: 8.579248060414102e-06
Step: 34810, train/epoch: 8.284150123596191
Step: 34820, train/loss: 0.0
Step: 34820, train/grad_norm: 1.6507806233079236e-10
Step: 34820, train/learning_rate: 8.567349141230807e-06
Step: 34820, train/epoch: 8.286530494689941
Step: 34830, train/loss: 0.0
Step: 34830, train/grad_norm: 1.7215467096320936e-07
Step: 34830, train/learning_rate: 8.555450222047511e-06
Step: 34830, train/epoch: 8.288909912109375
Step: 34840, train/loss: 0.0
Step: 34840, train/grad_norm: 5.273524672499974e-12
Step: 34840, train/learning_rate: 8.543550393369514e-06
Step: 34840, train/epoch: 8.291290283203125
Step: 34850, train/loss: 0.0
Step: 34850, train/grad_norm: 7.963707915337181e-09
Step: 34850, train/learning_rate: 8.53165147418622e-06
Step: 34850, train/epoch: 8.293669700622559
Step: 34860, train/loss: 0.0
Step: 34860, train/grad_norm: 0.00021108226792421192
Step: 34860, train/learning_rate: 8.519752555002924e-06
Step: 34860, train/epoch: 8.296049118041992
Step: 34870, train/loss: 0.0
Step: 34870, train/grad_norm: 5.16643972048314e-09
Step: 34870, train/learning_rate: 8.507853635819629e-06
Step: 34870, train/epoch: 8.298429489135742
Step: 34880, train/loss: 0.0
Step: 34880, train/grad_norm: 3.920846211258322e-06
Step: 34880, train/learning_rate: 8.495954716636334e-06
Step: 34880, train/epoch: 8.300808906555176
Step: 34890, train/loss: 0.0
Step: 34890, train/grad_norm: 5.604433361838801e-09
Step: 34890, train/learning_rate: 8.484054887958337e-06
Step: 34890, train/epoch: 8.303189277648926
Step: 34900, train/loss: 0.0
Step: 34900, train/grad_norm: 2.110250996167906e-08
Step: 34900, train/learning_rate: 8.472155968775041e-06
Step: 34900, train/epoch: 8.30556869506836
Step: 34910, train/loss: 0.0
Step: 34910, train/grad_norm: 3.611728516261792e-08
Step: 34910, train/learning_rate: 8.460257049591746e-06
Step: 34910, train/epoch: 8.30794906616211
Step: 34920, train/loss: 0.0
Step: 34920, train/grad_norm: 7.005206953181187e-06
Step: 34920, train/learning_rate: 8.448358130408451e-06
Step: 34920, train/epoch: 8.310328483581543
Step: 34930, train/loss: 0.0
Step: 34930, train/grad_norm: 2.1031253127912208e-10
Step: 34930, train/learning_rate: 8.436459211225156e-06
Step: 34930, train/epoch: 8.312707901000977
Step: 34940, train/loss: 0.0
Step: 34940, train/grad_norm: 4.0797948352988556e-11
Step: 34940, train/learning_rate: 8.424559382547159e-06
Step: 34940, train/epoch: 8.315088272094727
Step: 34950, train/loss: 0.0
Step: 34950, train/grad_norm: 2.005195121057568e-09
Step: 34950, train/learning_rate: 8.412660463363864e-06
Step: 34950, train/epoch: 8.31746768951416
Step: 34960, train/loss: 0.0
Step: 34960, train/grad_norm: 3.079946253592425e-08
Step: 34960, train/learning_rate: 8.400761544180568e-06
Step: 34960, train/epoch: 8.31984806060791
Step: 34970, train/loss: 0.0
Step: 34970, train/grad_norm: 1.6564988269962555e-10
Step: 34970, train/learning_rate: 8.388862624997273e-06
Step: 34970, train/epoch: 8.322227478027344
Step: 34980, train/loss: 0.0
Step: 34980, train/grad_norm: 1.0684706591712612e-11
Step: 34980, train/learning_rate: 8.376963705813978e-06
Step: 34980, train/epoch: 8.324606895446777
Step: 34990, train/loss: 0.0
Step: 34990, train/grad_norm: 3.1155640289881603e-09
Step: 34990, train/learning_rate: 8.365063877135981e-06
Step: 34990, train/epoch: 8.326987266540527
Step: 35000, train/loss: 0.0
Step: 35000, train/grad_norm: 3.1715260973008697e-10
Step: 35000, train/learning_rate: 8.353164957952686e-06
Step: 35000, train/epoch: 8.329366683959961
Step: 35010, train/loss: 0.0
Step: 35010, train/grad_norm: 2.4494314909695447e-10
Step: 35010, train/learning_rate: 8.34126603876939e-06
Step: 35010, train/epoch: 8.331747055053711
Step: 35020, train/loss: 0.0
Step: 35020, train/grad_norm: 3.7479050973843187e-08
Step: 35020, train/learning_rate: 8.329367119586095e-06
Step: 35020, train/epoch: 8.334126472473145
Step: 35030, train/loss: 0.0
Step: 35030, train/grad_norm: 1.7495099813302772e-09
Step: 35030, train/learning_rate: 8.3174682004028e-06
Step: 35030, train/epoch: 8.336506843566895
Step: 35040, train/loss: 0.0
Step: 35040, train/grad_norm: 8.408933271919494e-12
Step: 35040, train/learning_rate: 8.305568371724803e-06
Step: 35040, train/epoch: 8.338886260986328
Step: 35050, train/loss: 0.0
Step: 35050, train/grad_norm: 1.7480984715323444e-10
Step: 35050, train/learning_rate: 8.293669452541508e-06
Step: 35050, train/epoch: 8.341265678405762
Step: 35060, train/loss: 0.0
Step: 35060, train/grad_norm: 3.288310162358199e-12
Step: 35060, train/learning_rate: 8.281770533358213e-06
Step: 35060, train/epoch: 8.343646049499512
Step: 35070, train/loss: 0.0
Step: 35070, train/grad_norm: 1.3791863484602374e-10
Step: 35070, train/learning_rate: 8.269871614174917e-06
Step: 35070, train/epoch: 8.346025466918945
Step: 35080, train/loss: 0.0
Step: 35080, train/grad_norm: 5.098867040942423e-05
Step: 35080, train/learning_rate: 8.257972694991622e-06
Step: 35080, train/epoch: 8.348405838012695
Step: 35090, train/loss: 0.0
Step: 35090, train/grad_norm: 1.2940304383590728e-10
Step: 35090, train/learning_rate: 8.246072866313625e-06
Step: 35090, train/epoch: 8.350785255432129
Step: 35100, train/loss: 0.0
Step: 35100, train/grad_norm: 3.5250079235993326e-05
Step: 35100, train/learning_rate: 8.23417394713033e-06
Step: 35100, train/epoch: 8.353165626525879
Step: 35110, train/loss: 0.0
Step: 35110, train/grad_norm: 7.876433727460608e-08
Step: 35110, train/learning_rate: 8.222275027947035e-06
Step: 35110, train/epoch: 8.355545043945312
Step: 35120, train/loss: 0.0
Step: 35120, train/grad_norm: 6.225241988566665e-10
Step: 35120, train/learning_rate: 8.21037610876374e-06
Step: 35120, train/epoch: 8.357924461364746
Step: 35130, train/loss: 0.0
Step: 35130, train/grad_norm: 4.576021159219401e-11
Step: 35130, train/learning_rate: 8.198477189580444e-06
Step: 35130, train/epoch: 8.360304832458496
Step: 35140, train/loss: 0.0
Step: 35140, train/grad_norm: 1.0190360708040025e-11
Step: 35140, train/learning_rate: 8.186578270397149e-06
Step: 35140, train/epoch: 8.36268424987793
Step: 35150, train/loss: 0.0
Step: 35150, train/grad_norm: 1.561454715526267e-10
Step: 35150, train/learning_rate: 8.174678441719152e-06
Step: 35150, train/epoch: 8.36506462097168
Step: 35160, train/loss: 0.0
Step: 35160, train/grad_norm: 1.1672941502283152e-09
Step: 35160, train/learning_rate: 8.162779522535857e-06
Step: 35160, train/epoch: 8.367444038391113
Step: 35170, train/loss: 0.0
Step: 35170, train/grad_norm: 1.0021147245353745e-09
Step: 35170, train/learning_rate: 8.150880603352562e-06
Step: 35170, train/epoch: 8.369823455810547
Step: 35180, train/loss: 0.0
Step: 35180, train/grad_norm: 1.4192667874279863e-10
Step: 35180, train/learning_rate: 8.138981684169266e-06
Step: 35180, train/epoch: 8.372203826904297
Step: 35190, train/loss: 0.0
Step: 35190, train/grad_norm: 2.1319947543507745e-11
Step: 35190, train/learning_rate: 8.127082764985971e-06
Step: 35190, train/epoch: 8.37458324432373
Step: 35200, train/loss: 0.0
Step: 35200, train/grad_norm: 1.0212472645321213e-08
Step: 35200, train/learning_rate: 8.115182936307974e-06
Step: 35200, train/epoch: 8.37696361541748
Step: 35210, train/loss: 0.0
Step: 35210, train/grad_norm: 5.608534436873924e-08
Step: 35210, train/learning_rate: 8.103284017124679e-06
Step: 35210, train/epoch: 8.379343032836914
Step: 35220, train/loss: 0.0
Step: 35220, train/grad_norm: 3.701639827990988e-10
Step: 35220, train/learning_rate: 8.091385097941384e-06
Step: 35220, train/epoch: 8.381723403930664
Step: 35230, train/loss: 0.0
Step: 35230, train/grad_norm: 2.996518162490247e-07
Step: 35230, train/learning_rate: 8.079486178758088e-06
Step: 35230, train/epoch: 8.384102821350098
Step: 35240, train/loss: 0.0
Step: 35240, train/grad_norm: 2.523723274805434e-08
Step: 35240, train/learning_rate: 8.067587259574793e-06
Step: 35240, train/epoch: 8.386482238769531
Step: 35250, train/loss: 0.0
Step: 35250, train/grad_norm: 2.66284883032597e-10
Step: 35250, train/learning_rate: 8.055687430896796e-06
Step: 35250, train/epoch: 8.388862609863281
Step: 35260, train/loss: 0.0
Step: 35260, train/grad_norm: 3.292290606804471e-10
Step: 35260, train/learning_rate: 8.043788511713501e-06
Step: 35260, train/epoch: 8.391242027282715
Step: 35270, train/loss: 0.0
Step: 35270, train/grad_norm: 3.1511111497906086e-09
Step: 35270, train/learning_rate: 8.031889592530206e-06
Step: 35270, train/epoch: 8.393622398376465
Step: 35280, train/loss: 0.0
Step: 35280, train/grad_norm: 7.880363896151099e-12
Step: 35280, train/learning_rate: 8.01999067334691e-06
Step: 35280, train/epoch: 8.396001815795898
Step: 35290, train/loss: 0.0
Step: 35290, train/grad_norm: 2.1399666483290503e-09
Step: 35290, train/learning_rate: 8.008091754163615e-06
Step: 35290, train/epoch: 8.398382186889648
Step: 35300, train/loss: 0.0
Step: 35300, train/grad_norm: 6.831943899499038e-09
Step: 35300, train/learning_rate: 7.996191925485618e-06
Step: 35300, train/epoch: 8.400761604309082
Step: 35310, train/loss: 0.0
Step: 35310, train/grad_norm: 1.6817829073811907e-10
Step: 35310, train/learning_rate: 7.984293006302323e-06
Step: 35310, train/epoch: 8.403141021728516
Step: 35320, train/loss: 0.0
Step: 35320, train/grad_norm: 3.113354712924732e-11
Step: 35320, train/learning_rate: 7.972394087119028e-06
Step: 35320, train/epoch: 8.405521392822266
Step: 35330, train/loss: 0.0
Step: 35330, train/grad_norm: 6.373030575490546e-12
Step: 35330, train/learning_rate: 7.960495167935733e-06
Step: 35330, train/epoch: 8.4079008102417
Step: 35340, train/loss: 0.0
Step: 35340, train/grad_norm: 4.792243268880725e-10
Step: 35340, train/learning_rate: 7.948596248752438e-06
Step: 35340, train/epoch: 8.41028118133545
Step: 35350, train/loss: 0.0
Step: 35350, train/grad_norm: 1.65081544878376e-07
Step: 35350, train/learning_rate: 7.93669642007444e-06
Step: 35350, train/epoch: 8.412660598754883
Step: 35360, train/loss: 0.0
Step: 35360, train/grad_norm: 4.451659318949197e-10
Step: 35360, train/learning_rate: 7.924797500891145e-06
Step: 35360, train/epoch: 8.415040016174316
Step: 35370, train/loss: 0.0
Step: 35370, train/grad_norm: 6.31202479173254e-10
Step: 35370, train/learning_rate: 7.91289858170785e-06
Step: 35370, train/epoch: 8.417420387268066
Step: 35380, train/loss: 0.0
Step: 35380, train/grad_norm: 2.25948241450169e-07
Step: 35380, train/learning_rate: 7.900999662524555e-06
Step: 35380, train/epoch: 8.4197998046875
Step: 35390, train/loss: 0.0
Step: 35390, train/grad_norm: 2.6876623593352633e-08
Step: 35390, train/learning_rate: 7.88910074334126e-06
Step: 35390, train/epoch: 8.42218017578125
Step: 35400, train/loss: 0.0
Step: 35400, train/grad_norm: 6.636869276377411e-09
Step: 35400, train/learning_rate: 7.877200914663263e-06
Step: 35400, train/epoch: 8.424559593200684
Step: 35410, train/loss: 0.0
Step: 35410, train/grad_norm: 3.75124965812379e-11
Step: 35410, train/learning_rate: 7.865301995479967e-06
Step: 35410, train/epoch: 8.426939964294434
Step: 35420, train/loss: 0.0
Step: 35420, train/grad_norm: 1.1967171698046286e-10
Step: 35420, train/learning_rate: 7.853403076296672e-06
Step: 35420, train/epoch: 8.429319381713867
Step: 35430, train/loss: 0.0
Step: 35430, train/grad_norm: 2.8369329108102193e-09
Step: 35430, train/learning_rate: 7.841504157113377e-06
Step: 35430, train/epoch: 8.4316987991333
Step: 35440, train/loss: 0.0
Step: 35440, train/grad_norm: 5.414459436359209e-10
Step: 35440, train/learning_rate: 7.829605237930082e-06
Step: 35440, train/epoch: 8.43407917022705
Step: 35450, train/loss: 0.0
Step: 35450, train/grad_norm: 8.225193681710152e-08
Step: 35450, train/learning_rate: 7.817705409252085e-06
Step: 35450, train/epoch: 8.436458587646484
Step: 35460, train/loss: 0.0
Step: 35460, train/grad_norm: 7.323870676589905e-11
Step: 35460, train/learning_rate: 7.80580649006879e-06
Step: 35460, train/epoch: 8.438838958740234
Step: 35470, train/loss: 0.0
Step: 35470, train/grad_norm: 1.607439355233442e-11
Step: 35470, train/learning_rate: 7.793907570885494e-06
Step: 35470, train/epoch: 8.441218376159668
Step: 35480, train/loss: 0.0
Step: 35480, train/grad_norm: 1.4394233005532442e-07
Step: 35480, train/learning_rate: 7.782008651702199e-06
Step: 35480, train/epoch: 8.443598747253418
Step: 35490, train/loss: 0.0
Step: 35490, train/grad_norm: 5.088765780286719e-12
Step: 35490, train/learning_rate: 7.770109732518904e-06
Step: 35490, train/epoch: 8.445978164672852
Step: 35500, train/loss: 0.0
Step: 35500, train/grad_norm: 4.301866241007701e-08
Step: 35500, train/learning_rate: 7.758210813335609e-06
Step: 35500, train/epoch: 8.448357582092285
Step: 35510, train/loss: 0.0
Step: 35510, train/grad_norm: 2.68252614565867e-10
Step: 35510, train/learning_rate: 7.746310984657612e-06
Step: 35510, train/epoch: 8.450737953186035
Step: 35520, train/loss: 0.0
Step: 35520, train/grad_norm: 1.1581755643419456e-07
Step: 35520, train/learning_rate: 7.734412065474316e-06
Step: 35520, train/epoch: 8.453117370605469
Step: 35530, train/loss: 0.0
Step: 35530, train/grad_norm: 3.438607454331333e-10
Step: 35530, train/learning_rate: 7.722513146291021e-06
Step: 35530, train/epoch: 8.455497741699219
Step: 35540, train/loss: 0.0
Step: 35540, train/grad_norm: 5.183406770470356e-08
Step: 35540, train/learning_rate: 7.710614227107726e-06
Step: 35540, train/epoch: 8.457877159118652
Step: 35550, train/loss: 0.0
Step: 35550, train/grad_norm: 3.984239674292844e-11
Step: 35550, train/learning_rate: 7.69871530792443e-06
Step: 35550, train/epoch: 8.460256576538086
Step: 35560, train/loss: 0.0
Step: 35560, train/grad_norm: 3.383720248439914e-10
Step: 35560, train/learning_rate: 7.686815479246434e-06
Step: 35560, train/epoch: 8.462636947631836
Step: 35570, train/loss: 0.0
Step: 35570, train/grad_norm: 1.6848498773924803e-07
Step: 35570, train/learning_rate: 7.674916560063139e-06
Step: 35570, train/epoch: 8.46501636505127
Step: 35580, train/loss: 0.0
Step: 35580, train/grad_norm: 2.2383179043572454e-07
Step: 35580, train/learning_rate: 7.663017640879843e-06
Step: 35580, train/epoch: 8.46739673614502
Step: 35590, train/loss: 0.0
Step: 35590, train/grad_norm: 8.627451819620546e-08
Step: 35590, train/learning_rate: 7.651118721696548e-06
Step: 35590, train/epoch: 8.469776153564453
Step: 35600, train/loss: 0.0
Step: 35600, train/grad_norm: 8.992260928930573e-12
Step: 35600, train/learning_rate: 7.639219802513253e-06
Step: 35600, train/epoch: 8.472156524658203
Step: 35610, train/loss: 0.0
Step: 35610, train/grad_norm: 7.793307665870941e-11
Step: 35610, train/learning_rate: 7.627320428582607e-06
Step: 35610, train/epoch: 8.474535942077637
Step: 35620, train/loss: 0.0
Step: 35620, train/grad_norm: 1.761724099935691e-11
Step: 35620, train/learning_rate: 7.615421054651961e-06
Step: 35620, train/epoch: 8.47691535949707
Step: 35630, train/loss: 0.0
Step: 35630, train/grad_norm: 1.6075547143445945e-11
Step: 35630, train/learning_rate: 7.6035221354686655e-06
Step: 35630, train/epoch: 8.47929573059082
Step: 35640, train/loss: 0.0
Step: 35640, train/grad_norm: 4.663854857867022e-10
Step: 35640, train/learning_rate: 7.59162321628537e-06
Step: 35640, train/epoch: 8.481675148010254
Step: 35650, train/loss: 0.0
Step: 35650, train/grad_norm: 1.45193593259485e-10
Step: 35650, train/learning_rate: 7.579723842354724e-06
Step: 35650, train/epoch: 8.484055519104004
Step: 35660, train/loss: 0.0
Step: 35660, train/grad_norm: 1.5402982708745405e-11
Step: 35660, train/learning_rate: 7.567824923171429e-06
Step: 35660, train/epoch: 8.486434936523438
Step: 35670, train/loss: 0.0
Step: 35670, train/grad_norm: 1.1410992151184018e-08
Step: 35670, train/learning_rate: 7.555925549240783e-06
Step: 35670, train/epoch: 8.488815307617188
Step: 35680, train/loss: 0.0
Step: 35680, train/grad_norm: 2.4484042571160103e-10
Step: 35680, train/learning_rate: 7.544026630057488e-06
Step: 35680, train/epoch: 8.491194725036621
Step: 35690, train/loss: 0.0
Step: 35690, train/grad_norm: 2.5422774996286535e-10
Step: 35690, train/learning_rate: 7.532127710874192e-06
Step: 35690, train/epoch: 8.493574142456055
Step: 35700, train/loss: 0.0
Step: 35700, train/grad_norm: 4.923841956383512e-08
Step: 35700, train/learning_rate: 7.520228336943546e-06
Step: 35700, train/epoch: 8.495954513549805
Step: 35710, train/loss: 0.0
Step: 35710, train/grad_norm: 2.2159689194722887e-07
Step: 35710, train/learning_rate: 7.508329417760251e-06
Step: 35710, train/epoch: 8.498333930969238
Step: 35720, train/loss: 0.0
Step: 35720, train/grad_norm: 7.028203019565638e-11
Step: 35720, train/learning_rate: 7.496430498576956e-06
Step: 35720, train/epoch: 8.500714302062988
Step: 35730, train/loss: 0.0
Step: 35730, train/grad_norm: 4.182956203635513e-08
Step: 35730, train/learning_rate: 7.48453112464631e-06
Step: 35730, train/epoch: 8.503093719482422
Step: 35740, train/loss: 0.0
Step: 35740, train/grad_norm: 8.840545717703208e-08
Step: 35740, train/learning_rate: 7.4726322054630145e-06
Step: 35740, train/epoch: 8.505473136901855
Step: 35750, train/loss: 0.0
Step: 35750, train/grad_norm: 2.7793632284556224e-07
Step: 35750, train/learning_rate: 7.4607328315323684e-06
Step: 35750, train/epoch: 8.507853507995605
Step: 35760, train/loss: 0.0
Step: 35760, train/grad_norm: 2.5446983409338486e-10
Step: 35760, train/learning_rate: 7.448833912349073e-06
Step: 35760, train/epoch: 8.510232925415039
Step: 35770, train/loss: 0.0
Step: 35770, train/grad_norm: 3.333114229064904e-09
Step: 35770, train/learning_rate: 7.436934993165778e-06
Step: 35770, train/epoch: 8.512613296508789
Step: 35780, train/loss: 0.0
Step: 35780, train/grad_norm: 3.8619716313803565e-09
Step: 35780, train/learning_rate: 7.425035619235132e-06
Step: 35780, train/epoch: 8.514992713928223
Step: 35790, train/loss: 0.0
Step: 35790, train/grad_norm: 3.684049545427115e-07
Step: 35790, train/learning_rate: 7.413136700051837e-06
Step: 35790, train/epoch: 8.517373085021973
Step: 35800, train/loss: 0.0
Step: 35800, train/grad_norm: 9.834234815375709e-11
Step: 35800, train/learning_rate: 7.4012373261211906e-06
Step: 35800, train/epoch: 8.519752502441406
Step: 35810, train/loss: 0.0
Step: 35810, train/grad_norm: 7.212735297912332e-09
Step: 35810, train/learning_rate: 7.389338406937895e-06
Step: 35810, train/epoch: 8.52213191986084
Step: 35820, train/loss: 0.0
Step: 35820, train/grad_norm: 8.525364442180106e-12
Step: 35820, train/learning_rate: 7.3774394877546e-06
Step: 35820, train/epoch: 8.52451229095459
Step: 35830, train/loss: 0.0
Step: 35830, train/grad_norm: 5.676771552831639e-12
Step: 35830, train/learning_rate: 7.365540113823954e-06
Step: 35830, train/epoch: 8.526891708374023
Step: 35840, train/loss: 0.0
Step: 35840, train/grad_norm: 3.934558456819559e-09
Step: 35840, train/learning_rate: 7.353641194640659e-06
Step: 35840, train/epoch: 8.529272079467773
Step: 35850, train/loss: 0.0
Step: 35850, train/grad_norm: 8.14360460576341e-13
Step: 35850, train/learning_rate: 7.341741820710013e-06
Step: 35850, train/epoch: 8.531651496887207
Step: 35860, train/loss: 0.0
Step: 35860, train/grad_norm: 2.083782972561732e-11
Step: 35860, train/learning_rate: 7.3298429015267175e-06
Step: 35860, train/epoch: 8.534031867980957
Step: 35870, train/loss: 0.0
Step: 35870, train/grad_norm: 1.0455024401778701e-08
Step: 35870, train/learning_rate: 7.317943982343422e-06
Step: 35870, train/epoch: 8.53641128540039
Step: 35880, train/loss: 0.0
Step: 35880, train/grad_norm: 1.6055518026192317e-10
Step: 35880, train/learning_rate: 7.306044608412776e-06
Step: 35880, train/epoch: 8.538790702819824
Step: 35890, train/loss: 0.0
Step: 35890, train/grad_norm: 1.1938824151513927e-08
Step: 35890, train/learning_rate: 7.294145689229481e-06
Step: 35890, train/epoch: 8.541171073913574
Step: 35900, train/loss: 0.0
Step: 35900, train/grad_norm: 6.300410149151503e-08
Step: 35900, train/learning_rate: 7.282246770046186e-06
Step: 35900, train/epoch: 8.543550491333008
Step: 35910, train/loss: 0.0
Step: 35910, train/grad_norm: 7.000454438355064e-09
Step: 35910, train/learning_rate: 7.27034739611554e-06
Step: 35910, train/epoch: 8.545930862426758
Step: 35920, train/loss: 0.0
Step: 35920, train/grad_norm: 4.039250271065109e-12
Step: 35920, train/learning_rate: 7.258448476932244e-06
Step: 35920, train/epoch: 8.548310279846191
Step: 35930, train/loss: 0.0
Step: 35930, train/grad_norm: 3.86740361957294e-10
Step: 35930, train/learning_rate: 7.246549103001598e-06
Step: 35930, train/epoch: 8.550689697265625
Step: 35940, train/loss: 0.0
Step: 35940, train/grad_norm: 9.734026085173042e-11
Step: 35940, train/learning_rate: 7.234650183818303e-06
Step: 35940, train/epoch: 8.553070068359375
Step: 35950, train/loss: 0.0
Step: 35950, train/grad_norm: 3.189878228226917e-08
Step: 35950, train/learning_rate: 7.222751264635008e-06
Step: 35950, train/epoch: 8.555449485778809
Step: 35960, train/loss: 0.0
Step: 35960, train/grad_norm: 3.709448082034328e-09
Step: 35960, train/learning_rate: 7.210851890704362e-06
Step: 35960, train/epoch: 8.557829856872559
Step: 35970, train/loss: 0.0
Step: 35970, train/grad_norm: 1.076709721381519e-09
Step: 35970, train/learning_rate: 7.1989529715210665e-06
Step: 35970, train/epoch: 8.560209274291992
Step: 35980, train/loss: 0.0
Step: 35980, train/grad_norm: 3.773484635871682e-09
Step: 35980, train/learning_rate: 7.18705359759042e-06
Step: 35980, train/epoch: 8.562589645385742
Step: 35990, train/loss: 0.0
Step: 35990, train/grad_norm: 2.911217816294054e-13
Step: 35990, train/learning_rate: 7.175154678407125e-06
Step: 35990, train/epoch: 8.564969062805176
Step: 36000, train/loss: 0.0
Step: 36000, train/grad_norm: 3.7276974042266264e-13
Step: 36000, train/learning_rate: 7.16325575922383e-06
Step: 36000, train/epoch: 8.56734848022461
Step: 36010, train/loss: 0.0
Step: 36010, train/grad_norm: 1.4047695628960355e-07
Step: 36010, train/learning_rate: 7.151356385293184e-06
Step: 36010, train/epoch: 8.56972885131836
Step: 36020, train/loss: 0.0
Step: 36020, train/grad_norm: 2.5076443144200766e-09
Step: 36020, train/learning_rate: 7.139457466109889e-06
Step: 36020, train/epoch: 8.572108268737793
Step: 36030, train/loss: 0.0
Step: 36030, train/grad_norm: 5.496977451002749e-08
Step: 36030, train/learning_rate: 7.1275580921792425e-06
Step: 36030, train/epoch: 8.574488639831543
Step: 36040, train/loss: 0.0
Step: 36040, train/grad_norm: 6.286134807131916e-11
Step: 36040, train/learning_rate: 7.115659172995947e-06
Step: 36040, train/epoch: 8.576868057250977
Step: 36050, train/loss: 0.0
Step: 36050, train/grad_norm: 6.130815299876247e-11
Step: 36050, train/learning_rate: 7.103760253812652e-06
Step: 36050, train/epoch: 8.579248428344727
Step: 36060, train/loss: 0.0
Step: 36060, train/grad_norm: 3.330188746986096e-08
Step: 36060, train/learning_rate: 7.091860879882006e-06
Step: 36060, train/epoch: 8.58162784576416
Step: 36070, train/loss: 0.0
Step: 36070, train/grad_norm: 1.5139629283567047e-07
Step: 36070, train/learning_rate: 7.079961960698711e-06
Step: 36070, train/epoch: 8.584007263183594
Step: 36080, train/loss: 0.0
Step: 36080, train/grad_norm: 7.745495911315459e-11
Step: 36080, train/learning_rate: 7.0680630415154155e-06
Step: 36080, train/epoch: 8.586387634277344
Step: 36090, train/loss: 0.0
Step: 36090, train/grad_norm: 2.37337843822516e-10
Step: 36090, train/learning_rate: 7.0561636675847694e-06
Step: 36090, train/epoch: 8.588767051696777
Step: 36100, train/loss: 0.0
Step: 36100, train/grad_norm: 2.1403873465275502e-11
Step: 36100, train/learning_rate: 7.044264748401474e-06
Step: 36100, train/epoch: 8.591147422790527
Step: 36110, train/loss: 0.0
Step: 36110, train/grad_norm: 1.8556881586917484e-09
Step: 36110, train/learning_rate: 7.032365374470828e-06
Step: 36110, train/epoch: 8.593526840209961
Step: 36120, train/loss: 0.0
Step: 36120, train/grad_norm: 3.2619018597301874e-10
Step: 36120, train/learning_rate: 7.020466455287533e-06
Step: 36120, train/epoch: 8.595906257629395
Step: 36130, train/loss: 0.0
Step: 36130, train/grad_norm: 2.4062124515111805e-10
Step: 36130, train/learning_rate: 7.008567536104238e-06
Step: 36130, train/epoch: 8.598286628723145
Step: 36140, train/loss: 0.0
Step: 36140, train/grad_norm: 1.257427356682328e-10
Step: 36140, train/learning_rate: 6.9966681621735916e-06
Step: 36140, train/epoch: 8.600666046142578
Step: 36150, train/loss: 0.0
Step: 36150, train/grad_norm: 4.418092114377714e-09
Step: 36150, train/learning_rate: 6.984769242990296e-06
Step: 36150, train/epoch: 8.603046417236328
Step: 36160, train/loss: 0.0
Step: 36160, train/grad_norm: 1.2512153979571394e-08
Step: 36160, train/learning_rate: 6.97286986905965e-06
Step: 36160, train/epoch: 8.605425834655762
Step: 36170, train/loss: 0.0
Step: 36170, train/grad_norm: 8.176580940100209e-11
Step: 36170, train/learning_rate: 6.960970949876355e-06
Step: 36170, train/epoch: 8.607806205749512
Step: 36180, train/loss: 0.0
Step: 36180, train/grad_norm: 8.888096925119271e-10
Step: 36180, train/learning_rate: 6.94907203069306e-06
Step: 36180, train/epoch: 8.610185623168945
Step: 36190, train/loss: 0.0
Step: 36190, train/grad_norm: 6.727268742068304e-10
Step: 36190, train/learning_rate: 6.937172656762414e-06
Step: 36190, train/epoch: 8.612565040588379
Step: 36200, train/loss: 0.0
Step: 36200, train/grad_norm: 3.187412573463222e-12
Step: 36200, train/learning_rate: 6.9252737375791185e-06
Step: 36200, train/epoch: 8.614945411682129
Step: 36210, train/loss: 0.0
Step: 36210, train/grad_norm: 3.140929877298859e-11
Step: 36210, train/learning_rate: 6.913374363648472e-06
Step: 36210, train/epoch: 8.617324829101562
Step: 36220, train/loss: 0.0
Step: 36220, train/grad_norm: 6.446505551593873e-08
Step: 36220, train/learning_rate: 6.901475444465177e-06
Step: 36220, train/epoch: 8.619705200195312
Step: 36230, train/loss: 0.0
Step: 36230, train/grad_norm: 9.17033549791002e-12
Step: 36230, train/learning_rate: 6.889576525281882e-06
Step: 36230, train/epoch: 8.622084617614746
Step: 36240, train/loss: 0.0
Step: 36240, train/grad_norm: 1.4810749959792702e-08
Step: 36240, train/learning_rate: 6.877677151351236e-06
Step: 36240, train/epoch: 8.624464988708496
Step: 36250, train/loss: 0.0
Step: 36250, train/grad_norm: 3.972911954974734e-08
Step: 36250, train/learning_rate: 6.865778232167941e-06
Step: 36250, train/epoch: 8.62684440612793
Step: 36260, train/loss: 0.0
Step: 36260, train/grad_norm: 6.187150347614079e-08
Step: 36260, train/learning_rate: 6.853879312984645e-06
Step: 36260, train/epoch: 8.629223823547363
Step: 36270, train/loss: 0.0
Step: 36270, train/grad_norm: 3.0032942959223874e-05
Step: 36270, train/learning_rate: 6.841979939053999e-06
Step: 36270, train/epoch: 8.631604194641113
Step: 36280, train/loss: 0.0
Step: 36280, train/grad_norm: 5.903760553344073e-12
Step: 36280, train/learning_rate: 6.830081019870704e-06
Step: 36280, train/epoch: 8.633983612060547
Step: 36290, train/loss: 0.0
Step: 36290, train/grad_norm: 2.287578571014137e-09
Step: 36290, train/learning_rate: 6.818181645940058e-06
Step: 36290, train/epoch: 8.636363983154297
Step: 36300, train/loss: 0.0
Step: 36300, train/grad_norm: 1.3418910427276387e-10
Step: 36300, train/learning_rate: 6.806282726756763e-06
Step: 36300, train/epoch: 8.63874340057373
Step: 36310, train/loss: 0.0
Step: 36310, train/grad_norm: 1.9591672728580534e-09
Step: 36310, train/learning_rate: 6.7943838075734675e-06
Step: 36310, train/epoch: 8.641122817993164
Step: 36320, train/loss: 0.0
Step: 36320, train/grad_norm: 4.3982356645599907e-10
Step: 36320, train/learning_rate: 6.782484433642821e-06
Step: 36320, train/epoch: 8.643503189086914
Step: 36330, train/loss: 0.0
Step: 36330, train/grad_norm: 6.125755902530727e-08
Step: 36330, train/learning_rate: 6.770585514459526e-06
Step: 36330, train/epoch: 8.645882606506348
Step: 36340, train/loss: 0.0
Step: 36340, train/grad_norm: 1.831040041810894e-10
Step: 36340, train/learning_rate: 6.75868614052888e-06
Step: 36340, train/epoch: 8.648262977600098
Step: 36350, train/loss: 0.0
Step: 36350, train/grad_norm: 3.291085112766545e-11
Step: 36350, train/learning_rate: 6.746787221345585e-06
Step: 36350, train/epoch: 8.650642395019531
Step: 36360, train/loss: 0.0
Step: 36360, train/grad_norm: 4.203292575111206e-11
Step: 36360, train/learning_rate: 6.73488830216229e-06
Step: 36360, train/epoch: 8.653022766113281
Step: 36370, train/loss: 0.0
Step: 36370, train/grad_norm: 2.4807480514255076e-09
Step: 36370, train/learning_rate: 6.7229889282316435e-06
Step: 36370, train/epoch: 8.655402183532715
Step: 36380, train/loss: 0.0
Step: 36380, train/grad_norm: 9.182432592069745e-11
Step: 36380, train/learning_rate: 6.711090009048348e-06
Step: 36380, train/epoch: 8.657781600952148
Step: 36390, train/loss: 0.0
Step: 36390, train/grad_norm: 1.0875063516380123e-08
Step: 36390, train/learning_rate: 6.699190635117702e-06
Step: 36390, train/epoch: 8.660161972045898
Step: 36400, train/loss: 0.0
Step: 36400, train/grad_norm: 9.408302262259305e-12
Step: 36400, train/learning_rate: 6.687291715934407e-06
Step: 36400, train/epoch: 8.662541389465332
Step: 36410, train/loss: 0.0
Step: 36410, train/grad_norm: 7.939915391830255e-10
Step: 36410, train/learning_rate: 6.675392796751112e-06
Step: 36410, train/epoch: 8.664921760559082
Step: 36420, train/loss: 0.0
Step: 36420, train/grad_norm: 4.971616174298532e-11
Step: 36420, train/learning_rate: 6.663493422820466e-06
Step: 36420, train/epoch: 8.667301177978516
Step: 36430, train/loss: 0.0
Step: 36430, train/grad_norm: 4.509492335103005e-09
Step: 36430, train/learning_rate: 6.65159450363717e-06
Step: 36430, train/epoch: 8.669681549072266
Step: 36440, train/loss: 0.0
Step: 36440, train/grad_norm: 2.88440915596766e-09
Step: 36440, train/learning_rate: 6.639695584453875e-06
Step: 36440, train/epoch: 8.6720609664917
Step: 36450, train/loss: 0.0
Step: 36450, train/grad_norm: 4.620761018259145e-08
Step: 36450, train/learning_rate: 6.627796210523229e-06
Step: 36450, train/epoch: 8.674440383911133
Step: 36460, train/loss: 0.0
Step: 36460, train/grad_norm: 4.9205279607777985e-12
Step: 36460, train/learning_rate: 6.615897291339934e-06
Step: 36460, train/epoch: 8.676820755004883
Step: 36470, train/loss: 0.0
Step: 36470, train/grad_norm: 2.271246330565191e-06
Step: 36470, train/learning_rate: 6.603997917409288e-06
Step: 36470, train/epoch: 8.679200172424316
Step: 36480, train/loss: 0.0
Step: 36480, train/grad_norm: 1.4049319485565093e-09
Step: 36480, train/learning_rate: 6.5920989982259925e-06
Step: 36480, train/epoch: 8.681580543518066
Step: 36490, train/loss: 0.0
Step: 36490, train/grad_norm: 3.341475530760363e-05
Step: 36490, train/learning_rate: 6.580200079042697e-06
Step: 36490, train/epoch: 8.6839599609375
Step: 36500, train/loss: 0.0
Step: 36500, train/grad_norm: 2.613087524139246e-10
Step: 36500, train/learning_rate: 6.568300705112051e-06
Step: 36500, train/epoch: 8.686339378356934
Step: 36510, train/loss: 0.0
Step: 36510, train/grad_norm: 3.920316571370819e-10
Step: 36510, train/learning_rate: 6.556401785928756e-06
Step: 36510, train/epoch: 8.688719749450684
Step: 36520, train/loss: 0.0
Step: 36520, train/grad_norm: 4.556298094371414e-08
Step: 36520, train/learning_rate: 6.54450241199811e-06
Step: 36520, train/epoch: 8.691099166870117
Step: 36530, train/loss: 0.0
Step: 36530, train/grad_norm: 1.2220259122841526e-05
Step: 36530, train/learning_rate: 6.532603492814815e-06
Step: 36530, train/epoch: 8.693479537963867
Step: 36540, train/loss: 0.0
Step: 36540, train/grad_norm: 2.517343000718597e-10
Step: 36540, train/learning_rate: 6.5207045736315195e-06
Step: 36540, train/epoch: 8.6958589553833
Step: 36550, train/loss: 0.0
Step: 36550, train/grad_norm: 7.488966669022545e-10
Step: 36550, train/learning_rate: 6.508805199700873e-06
Step: 36550, train/epoch: 8.69823932647705
Step: 36560, train/loss: 0.0
Step: 36560, train/grad_norm: 1.0257210192321509e-08
Step: 36560, train/learning_rate: 6.496906280517578e-06
Step: 36560, train/epoch: 8.700618743896484
Step: 36570, train/loss: 0.0
Step: 36570, train/grad_norm: 1.262874249619017e-09
Step: 36570, train/learning_rate: 6.485007361334283e-06
Step: 36570, train/epoch: 8.702998161315918
Step: 36580, train/loss: 0.0
Step: 36580, train/grad_norm: 3.9119690820044184e-10
Step: 36580, train/learning_rate: 6.473107987403637e-06
Step: 36580, train/epoch: 8.705378532409668
Step: 36590, train/loss: 0.0
Step: 36590, train/grad_norm: 1.416647021912354e-09
Step: 36590, train/learning_rate: 6.461209068220342e-06
Step: 36590, train/epoch: 8.707757949829102
Step: 36600, train/loss: 0.0
Step: 36600, train/grad_norm: 1.0201193223480232e-09
Step: 36600, train/learning_rate: 6.4493096942896955e-06
Step: 36600, train/epoch: 8.710138320922852
Step: 36610, train/loss: 0.0
Step: 36610, train/grad_norm: 1.3194763337498472e-10
Step: 36610, train/learning_rate: 6.4374107751064e-06
Step: 36610, train/epoch: 8.712517738342285
Step: 36620, train/loss: 0.0
Step: 36620, train/grad_norm: 1.0241467673921534e-09
Step: 36620, train/learning_rate: 6.425511855923105e-06
Step: 36620, train/epoch: 8.714898109436035
Step: 36630, train/loss: 0.0
Step: 36630, train/grad_norm: 1.1706740021821815e-09
Step: 36630, train/learning_rate: 6.413612481992459e-06
Step: 36630, train/epoch: 8.717277526855469
Step: 36640, train/loss: 0.0
Step: 36640, train/grad_norm: 7.952998676286072e-11
Step: 36640, train/learning_rate: 6.401713562809164e-06
Step: 36640, train/epoch: 8.719656944274902
Step: 36650, train/loss: 0.0
Step: 36650, train/grad_norm: 2.578691774002273e-11
Step: 36650, train/learning_rate: 6.389814188878518e-06
Step: 36650, train/epoch: 8.722037315368652
Step: 36660, train/loss: 0.0
Step: 36660, train/grad_norm: 7.23496569321469e-08
Step: 36660, train/learning_rate: 6.377915269695222e-06
Step: 36660, train/epoch: 8.724416732788086
Step: 36670, train/loss: 0.0
Step: 36670, train/grad_norm: 8.688839092663159e-10
Step: 36670, train/learning_rate: 6.366016350511927e-06
Step: 36670, train/epoch: 8.726797103881836
Step: 36680, train/loss: 0.0
Step: 36680, train/grad_norm: 5.3792201804014894e-09
Step: 36680, train/learning_rate: 6.354116976581281e-06
Step: 36680, train/epoch: 8.72917652130127
Step: 36690, train/loss: 0.0
Step: 36690, train/grad_norm: 0.015759170055389404
Step: 36690, train/learning_rate: 6.342218057397986e-06
Step: 36690, train/epoch: 8.731555938720703
Step: 36700, train/loss: 0.0
Step: 36700, train/grad_norm: 3.832116703961219e-07
Step: 36700, train/learning_rate: 6.33031868346734e-06
Step: 36700, train/epoch: 8.733936309814453
Step: 36710, train/loss: 0.0
Step: 36710, train/grad_norm: 4.0088380531821954e-11
Step: 36710, train/learning_rate: 6.3184197642840445e-06
Step: 36710, train/epoch: 8.736315727233887
Step: 36720, train/loss: 0.0
Step: 36720, train/grad_norm: 1.5098416961123462e-09
Step: 36720, train/learning_rate: 6.306520845100749e-06
Step: 36720, train/epoch: 8.738696098327637
Step: 36730, train/loss: 0.0
Step: 36730, train/grad_norm: 5.740073749649355e-09
Step: 36730, train/learning_rate: 6.294621471170103e-06
Step: 36730, train/epoch: 8.74107551574707
Step: 36740, train/loss: 0.0
Step: 36740, train/grad_norm: 1.4598020015021973e-10
Step: 36740, train/learning_rate: 6.282722551986808e-06
Step: 36740, train/epoch: 8.74345588684082
Step: 36750, train/loss: 0.0
Step: 36750, train/grad_norm: 1.101516744483888e-08
Step: 36750, train/learning_rate: 6.270823632803513e-06
Step: 36750, train/epoch: 8.745835304260254
Step: 36760, train/loss: 0.0
Step: 36760, train/grad_norm: 1.4255014946229494e-09
Step: 36760, train/learning_rate: 6.258924258872867e-06
Step: 36760, train/epoch: 8.748214721679688
Step: 36770, train/loss: 0.0
Step: 36770, train/grad_norm: 6.713872791053177e-10
Step: 36770, train/learning_rate: 6.247025339689571e-06
Step: 36770, train/epoch: 8.750595092773438
Step: 36780, train/loss: 0.0
Step: 36780, train/grad_norm: 7.986630454204224e-12
Step: 36780, train/learning_rate: 6.235125965758925e-06
Step: 36780, train/epoch: 8.752974510192871
Step: 36790, train/loss: 0.0
Step: 36790, train/grad_norm: 2.917613720065937e-11
Step: 36790, train/learning_rate: 6.22322704657563e-06
Step: 36790, train/epoch: 8.755354881286621
Step: 36800, train/loss: 0.0
Step: 36800, train/grad_norm: 2.819724176372773e-10
Step: 36800, train/learning_rate: 6.211328127392335e-06
Step: 36800, train/epoch: 8.757734298706055
Step: 36810, train/loss: 0.0
Step: 36810, train/grad_norm: 4.121053223116178e-07
Step: 36810, train/learning_rate: 6.199428753461689e-06
Step: 36810, train/epoch: 8.760114669799805
Step: 36820, train/loss: 0.0
Step: 36820, train/grad_norm: 1.4372284473651575e-09
Step: 36820, train/learning_rate: 6.1875298342783935e-06
Step: 36820, train/epoch: 8.762494087219238
Step: 36830, train/loss: 0.0
Step: 36830, train/grad_norm: 4.544263632055845e-08
Step: 36830, train/learning_rate: 6.1756304603477474e-06
Step: 36830, train/epoch: 8.764873504638672
Step: 36840, train/loss: 0.0
Step: 36840, train/grad_norm: 2.0566963063184396e-11
Step: 36840, train/learning_rate: 6.163731541164452e-06
Step: 36840, train/epoch: 8.767253875732422
Step: 36850, train/loss: 0.0
Step: 36850, train/grad_norm: 1.6664928118026445e-11
Step: 36850, train/learning_rate: 6.151832621981157e-06
Step: 36850, train/epoch: 8.769633293151855
Step: 36860, train/loss: 0.0
Step: 36860, train/grad_norm: 3.729112130201884e-09
Step: 36860, train/learning_rate: 6.139933248050511e-06
Step: 36860, train/epoch: 8.772013664245605
Step: 36870, train/loss: 0.0
Step: 36870, train/grad_norm: 2.8150118954600734e-12
Step: 36870, train/learning_rate: 6.128034328867216e-06
Step: 36870, train/epoch: 8.774393081665039
Step: 36880, train/loss: 0.0
Step: 36880, train/grad_norm: 6.664041540815901e-11
Step: 36880, train/learning_rate: 6.1161349549365696e-06
Step: 36880, train/epoch: 8.776772499084473
Step: 36890, train/loss: 0.0
Step: 36890, train/grad_norm: 2.1808353733998587e-10
Step: 36890, train/learning_rate: 6.104236035753274e-06
Step: 36890, train/epoch: 8.779152870178223
Step: 36900, train/loss: 0.0
Step: 36900, train/grad_norm: 8.06792521679256e-10
Step: 36900, train/learning_rate: 6.092337116569979e-06
Step: 36900, train/epoch: 8.781532287597656
Step: 36910, train/loss: 0.0
Step: 36910, train/grad_norm: 1.0651662191163425e-10
Step: 36910, train/learning_rate: 6.080437742639333e-06
Step: 36910, train/epoch: 8.783912658691406
Step: 36920, train/loss: 0.0
Step: 36920, train/grad_norm: 3.090070950853563e-12
Step: 36920, train/learning_rate: 6.068538823456038e-06
Step: 36920, train/epoch: 8.78629207611084
Step: 36930, train/loss: 0.0
Step: 36930, train/grad_norm: 1.4560962158238766e-10
Step: 36930, train/learning_rate: 6.0566399042727426e-06
Step: 36930, train/epoch: 8.78867244720459
Step: 36940, train/loss: 0.0
Step: 36940, train/grad_norm: 1.1573770370906789e-12
Step: 36940, train/learning_rate: 6.0447405303420965e-06
Step: 36940, train/epoch: 8.791051864624023
Step: 36950, train/loss: 0.0
Step: 36950, train/grad_norm: 1.996474383383906e-12
Step: 36950, train/learning_rate: 6.032841611158801e-06
Step: 36950, train/epoch: 8.793431282043457
Step: 36960, train/loss: 0.0
Step: 36960, train/grad_norm: 1.6128177352664474e-12
Step: 36960, train/learning_rate: 6.020942237228155e-06
Step: 36960, train/epoch: 8.795811653137207
Step: 36970, train/loss: 0.0
Step: 36970, train/grad_norm: 6.7648326940172865e-09
Step: 36970, train/learning_rate: 6.00904331804486e-06
Step: 36970, train/epoch: 8.79819107055664
Step: 36980, train/loss: 0.0
Step: 36980, train/grad_norm: 6.603359758950919e-14
Step: 36980, train/learning_rate: 5.997144398861565e-06
Step: 36980, train/epoch: 8.80057144165039
Step: 36990, train/loss: 0.0
Step: 36990, train/grad_norm: 6.52487064378704e-10
Step: 36990, train/learning_rate: 5.985245024930919e-06
Step: 36990, train/epoch: 8.802950859069824
Step: 37000, train/loss: 0.0
Step: 37000, train/grad_norm: 9.112005594502648e-11
Step: 37000, train/learning_rate: 5.973346105747623e-06
Step: 37000, train/epoch: 8.805331230163574
Step: 37010, train/loss: 0.0
Step: 37010, train/grad_norm: 1.2028124052321232e-09
Step: 37010, train/learning_rate: 5.961446731816977e-06
Step: 37010, train/epoch: 8.807710647583008
Step: 37020, train/loss: 0.0
Step: 37020, train/grad_norm: 2.0227508903958302e-12
Step: 37020, train/learning_rate: 5.949547812633682e-06
Step: 37020, train/epoch: 8.810090065002441
Step: 37030, train/loss: 0.0
Step: 37030, train/grad_norm: 1.1126024407337454e-10
Step: 37030, train/learning_rate: 5.937648893450387e-06
Step: 37030, train/epoch: 8.812470436096191
Step: 37040, train/loss: 0.0
Step: 37040, train/grad_norm: 8.760694426845905e-12
Step: 37040, train/learning_rate: 5.925749519519741e-06
Step: 37040, train/epoch: 8.814849853515625
Step: 37050, train/loss: 0.0
Step: 37050, train/grad_norm: 5.785486312248622e-09
Step: 37050, train/learning_rate: 5.9138506003364455e-06
Step: 37050, train/epoch: 8.817230224609375
Step: 37060, train/loss: 0.0
Step: 37060, train/grad_norm: 5.684058779209522e-10
Step: 37060, train/learning_rate: 5.901951226405799e-06
Step: 37060, train/epoch: 8.819609642028809
Step: 37070, train/loss: 0.0
Step: 37070, train/grad_norm: 1.2536522930872707e-09
Step: 37070, train/learning_rate: 5.890052307222504e-06
Step: 37070, train/epoch: 8.821989059448242
Step: 37080, train/loss: 0.0
Step: 37080, train/grad_norm: 7.541139479365355e-13
Step: 37080, train/learning_rate: 5.878153388039209e-06
Step: 37080, train/epoch: 8.824369430541992
Step: 37090, train/loss: 0.0
Step: 37090, train/grad_norm: 3.061801479020687e-09
Step: 37090, train/learning_rate: 5.866254014108563e-06
Step: 37090, train/epoch: 8.826748847961426
Step: 37100, train/loss: 0.0
Step: 37100, train/grad_norm: 5.619053399658069e-12
Step: 37100, train/learning_rate: 5.854355094925268e-06
Step: 37100, train/epoch: 8.829129219055176
Step: 37110, train/loss: 0.0
Step: 37110, train/grad_norm: 4.7695034588457474e-09
Step: 37110, train/learning_rate: 5.842456175741972e-06
Step: 37110, train/epoch: 8.83150863647461
Step: 37120, train/loss: 0.0
Step: 37120, train/grad_norm: 1.2251293729503843e-12
Step: 37120, train/learning_rate: 5.830556801811326e-06
Step: 37120, train/epoch: 8.83388900756836
Step: 37130, train/loss: 0.0
Step: 37130, train/grad_norm: 5.506488060663672e-12
Step: 37130, train/learning_rate: 5.818657882628031e-06
Step: 37130, train/epoch: 8.836268424987793
Step: 37140, train/loss: 0.0
Step: 37140, train/grad_norm: 8.685436050925865e-12
Step: 37140, train/learning_rate: 5.806758508697385e-06
Step: 37140, train/epoch: 8.838647842407227
Step: 37150, train/loss: 0.0
Step: 37150, train/grad_norm: 6.595671057929575e-12
Step: 37150, train/learning_rate: 5.79485958951409e-06
Step: 37150, train/epoch: 8.841028213500977
Step: 37160, train/loss: 0.0
Step: 37160, train/grad_norm: 4.1187261934361175e-10
Step: 37160, train/learning_rate: 5.7829606703307945e-06
Step: 37160, train/epoch: 8.84340763092041
Step: 37170, train/loss: 0.0
Step: 37170, train/grad_norm: 2.3641111113192892e-07
Step: 37170, train/learning_rate: 5.771061296400148e-06
Step: 37170, train/epoch: 8.84578800201416
Step: 37180, train/loss: 0.0
Step: 37180, train/grad_norm: 6.371417607918539e-13
Step: 37180, train/learning_rate: 5.759162377216853e-06
Step: 37180, train/epoch: 8.848167419433594
Step: 37190, train/loss: 0.0
Step: 37190, train/grad_norm: 4.2684014260885306e-06
Step: 37190, train/learning_rate: 5.747263003286207e-06
Step: 37190, train/epoch: 8.850547790527344
Step: 37200, train/loss: 0.0
Step: 37200, train/grad_norm: 1.152732664877476e-08
Step: 37200, train/learning_rate: 5.735364084102912e-06
Step: 37200, train/epoch: 8.852927207946777
Step: 37210, train/loss: 0.0
Step: 37210, train/grad_norm: 4.0298684528261575e-12
Step: 37210, train/learning_rate: 5.723465164919617e-06
Step: 37210, train/epoch: 8.855306625366211
Step: 37220, train/loss: 0.0
Step: 37220, train/grad_norm: 2.003248678050795e-09
Step: 37220, train/learning_rate: 5.7115657909889705e-06
Step: 37220, train/epoch: 8.857686996459961
Step: 37230, train/loss: 0.0
Step: 37230, train/grad_norm: 1.4070677067934412e-08
Step: 37230, train/learning_rate: 5.699666871805675e-06
Step: 37230, train/epoch: 8.860066413879395
Step: 37240, train/loss: 0.0
Step: 37240, train/grad_norm: 1.6019412185652726e-10
Step: 37240, train/learning_rate: 5.68776795262238e-06
Step: 37240, train/epoch: 8.862446784973145
Step: 37250, train/loss: 0.0
Step: 37250, train/grad_norm: 9.328643955086591e-07
Step: 37250, train/learning_rate: 5.675868578691734e-06
Step: 37250, train/epoch: 8.864826202392578
Step: 37260, train/loss: 0.0
Step: 37260, train/grad_norm: 1.411799814832193e-07
Step: 37260, train/learning_rate: 5.663969659508439e-06
Step: 37260, train/epoch: 8.867205619812012
Step: 37270, train/loss: 0.0
Step: 37270, train/grad_norm: 6.986990430668527e-10
Step: 37270, train/learning_rate: 5.652070285577793e-06
Step: 37270, train/epoch: 8.869585990905762
Step: 37280, train/loss: 0.0
Step: 37280, train/grad_norm: 1.0852622134294165e-09
Step: 37280, train/learning_rate: 5.6401713663944975e-06
Step: 37280, train/epoch: 8.871965408325195
Step: 37290, train/loss: 0.0
Step: 37290, train/grad_norm: 1.9810897811112227e-07
Step: 37290, train/learning_rate: 5.628272447211202e-06
Step: 37290, train/epoch: 8.874345779418945
Step: 37300, train/loss: 0.0
Step: 37300, train/grad_norm: 2.704155475929948e-11
Step: 37300, train/learning_rate: 5.616373073280556e-06
Step: 37300, train/epoch: 8.876725196838379
Step: 37310, train/loss: 0.0
Step: 37310, train/grad_norm: 2.9777740603170733e-12
Step: 37310, train/learning_rate: 5.604474154097261e-06
Step: 37310, train/epoch: 8.879105567932129
Step: 37320, train/loss: 0.0
Step: 37320, train/grad_norm: 4.822557353456602e-10
Step: 37320, train/learning_rate: 5.592574780166615e-06
Step: 37320, train/epoch: 8.881484985351562
Step: 37330, train/loss: 0.0
Step: 37330, train/grad_norm: 1.6750441094934287e-12
Step: 37330, train/learning_rate: 5.58067586098332e-06
Step: 37330, train/epoch: 8.883864402770996
Step: 37340, train/loss: 0.0
Step: 37340, train/grad_norm: 1.1080629747084458e-07
Step: 37340, train/learning_rate: 5.568776941800024e-06
Step: 37340, train/epoch: 8.886244773864746
Step: 37350, train/loss: 0.0
Step: 37350, train/grad_norm: 2.263896625720463e-08
Step: 37350, train/learning_rate: 5.556877567869378e-06
Step: 37350, train/epoch: 8.88862419128418
Step: 37360, train/loss: 0.0
Step: 37360, train/grad_norm: 8.022294828435861e-09
Step: 37360, train/learning_rate: 5.544978648686083e-06
Step: 37360, train/epoch: 8.89100456237793
Step: 37370, train/loss: 0.0
Step: 37370, train/grad_norm: 1.9876860032040644e-13
Step: 37370, train/learning_rate: 5.533079274755437e-06
Step: 37370, train/epoch: 8.893383979797363
Step: 37380, train/loss: 0.0
Step: 37380, train/grad_norm: 7.292850351392488e-12
Step: 37380, train/learning_rate: 5.521180355572142e-06
Step: 37380, train/epoch: 8.895764350891113
Step: 37390, train/loss: 0.0
Step: 37390, train/grad_norm: 1.6486642051560807e-09
Step: 37390, train/learning_rate: 5.5092814363888465e-06
Step: 37390, train/epoch: 8.898143768310547
Step: 37400, train/loss: 0.0
Step: 37400, train/grad_norm: 5.987904616588935e-12
Step: 37400, train/learning_rate: 5.4973820624582e-06
Step: 37400, train/epoch: 8.90052318572998
Step: 37410, train/loss: 0.0
Step: 37410, train/grad_norm: 4.254575358686452e-08
Step: 37410, train/learning_rate: 5.485483143274905e-06
Step: 37410, train/epoch: 8.90290355682373
Step: 37420, train/loss: 0.0
Step: 37420, train/grad_norm: 1.215475376598052e-11
Step: 37420, train/learning_rate: 5.47358422409161e-06
Step: 37420, train/epoch: 8.905282974243164
Step: 37430, train/loss: 0.0
Step: 37430, train/grad_norm: 2.2113554043468042e-10
Step: 37430, train/learning_rate: 5.461684850160964e-06
Step: 37430, train/epoch: 8.907663345336914
Step: 37440, train/loss: 0.0
Step: 37440, train/grad_norm: 4.752255880480227e-12
Step: 37440, train/learning_rate: 5.449785930977669e-06
Step: 37440, train/epoch: 8.910042762756348
Step: 37450, train/loss: 0.0
Step: 37450, train/grad_norm: 1.4399371972562136e-10
Step: 37450, train/learning_rate: 5.4378865570470225e-06
Step: 37450, train/epoch: 8.912422180175781
Step: 37460, train/loss: 0.0
Step: 37460, train/grad_norm: 1.3853156469179062e-11
Step: 37460, train/learning_rate: 5.425987637863727e-06
Step: 37460, train/epoch: 8.914802551269531
Step: 37470, train/loss: 0.0
Step: 37470, train/grad_norm: 3.8377537259215444e-10
Step: 37470, train/learning_rate: 5.414088718680432e-06
Step: 37470, train/epoch: 8.917181968688965
Step: 37480, train/loss: 0.0
Step: 37480, train/grad_norm: 5.042949391409479e-11
Step: 37480, train/learning_rate: 5.402189344749786e-06
Step: 37480, train/epoch: 8.919562339782715
Step: 37490, train/loss: 0.0
Step: 37490, train/grad_norm: 1.0066080781412357e-11
Step: 37490, train/learning_rate: 5.390290425566491e-06
Step: 37490, train/epoch: 8.921941757202148
Step: 37500, train/loss: 0.0
Step: 37500, train/grad_norm: 2.4896168460131207e-10
Step: 37500, train/learning_rate: 5.378391051635845e-06
Step: 37500, train/epoch: 8.924322128295898
Step: 37510, train/loss: 0.0
Step: 37510, train/grad_norm: 2.2887626445781614e-14
Step: 37510, train/learning_rate: 5.366492132452549e-06
Step: 37510, train/epoch: 8.926701545715332
Step: 37520, train/loss: 0.0
Step: 37520, train/grad_norm: 1.6537081704015577e-09
Step: 37520, train/learning_rate: 5.354593213269254e-06
Step: 37520, train/epoch: 8.929080963134766
Step: 37530, train/loss: 0.0
Step: 37530, train/grad_norm: 3.2524771764741445e-10
Step: 37530, train/learning_rate: 5.342693839338608e-06
Step: 37530, train/epoch: 8.931461334228516
Step: 37540, train/loss: 0.0
Step: 37540, train/grad_norm: 1.2438690077942738e-09
Step: 37540, train/learning_rate: 5.330794920155313e-06
Step: 37540, train/epoch: 8.93384075164795
Step: 37550, train/loss: 0.0
Step: 37550, train/grad_norm: 6.254149975681855e-12
Step: 37550, train/learning_rate: 5.318895546224667e-06
Step: 37550, train/epoch: 8.9362211227417
Step: 37560, train/loss: 0.0
Step: 37560, train/grad_norm: 7.817595041367115e-14
Step: 37560, train/learning_rate: 5.3069966270413715e-06
Step: 37560, train/epoch: 8.938600540161133
Step: 37570, train/loss: 0.0
Step: 37570, train/grad_norm: 2.351734605665623e-11
Step: 37570, train/learning_rate: 5.295097707858076e-06
Step: 37570, train/epoch: 8.940980911254883
Step: 37580, train/loss: 0.0
Step: 37580, train/grad_norm: 8.16738413011997e-11
Step: 37580, train/learning_rate: 5.28319833392743e-06
Step: 37580, train/epoch: 8.943360328674316
Step: 37590, train/loss: 0.0
Step: 37590, train/grad_norm: 4.318398069691476e-11
Step: 37590, train/learning_rate: 5.271299414744135e-06
Step: 37590, train/epoch: 8.94573974609375
Step: 37600, train/loss: 0.0
Step: 37600, train/grad_norm: 4.713589518701156e-10
Step: 37600, train/learning_rate: 5.25940049556084e-06
Step: 37600, train/epoch: 8.9481201171875
Step: 37610, train/loss: 0.0
Step: 37610, train/grad_norm: 1.3071420335020179e-10
Step: 37610, train/learning_rate: 5.247501121630194e-06
Step: 37610, train/epoch: 8.950499534606934
Step: 37620, train/loss: 0.0
Step: 37620, train/grad_norm: 6.999655965955753e-09
Step: 37620, train/learning_rate: 5.2356022024468984e-06
Step: 37620, train/epoch: 8.952879905700684
Step: 37630, train/loss: 0.0
Step: 37630, train/grad_norm: 6.222994125212877e-13
Step: 37630, train/learning_rate: 5.223702828516252e-06
Step: 37630, train/epoch: 8.955259323120117
Step: 37640, train/loss: 0.0010000000474974513
Step: 37640, train/grad_norm: 1.7960570530828335e-10
Step: 37640, train/learning_rate: 5.211803909332957e-06
Step: 37640, train/epoch: 8.957639694213867
Step: 37650, train/loss: 0.0
Step: 37650, train/grad_norm: 7.270029023231928e-11
Step: 37650, train/learning_rate: 5.199904990149662e-06
Step: 37650, train/epoch: 8.9600191116333
Step: 37660, train/loss: 0.0
Step: 37660, train/grad_norm: 3.43664208202199e-09
Step: 37660, train/learning_rate: 5.188005616219016e-06
Step: 37660, train/epoch: 8.962398529052734
Step: 37670, train/loss: 0.0
Step: 37670, train/grad_norm: 8.502138229560252e-12
Step: 37670, train/learning_rate: 5.1761066970357206e-06
Step: 37670, train/epoch: 8.964778900146484
Step: 37680, train/loss: 0.0
Step: 37680, train/grad_norm: 2.6621363533735343e-12
Step: 37680, train/learning_rate: 5.1642073231050745e-06
Step: 37680, train/epoch: 8.967158317565918
Step: 37690, train/loss: 0.0
Step: 37690, train/grad_norm: 3.754405640543634e-12
Step: 37690, train/learning_rate: 5.152308403921779e-06
Step: 37690, train/epoch: 8.969538688659668
Step: 37700, train/loss: 0.07540000230073929
Step: 37700, train/grad_norm: 4.135535913718513e-09
Step: 37700, train/learning_rate: 5.140409484738484e-06
Step: 37700, train/epoch: 8.971918106079102
Step: 37710, train/loss: 0.0
Step: 37710, train/grad_norm: 2.730215775681444e-10
Step: 37710, train/learning_rate: 5.128510110807838e-06
Step: 37710, train/epoch: 8.974297523498535
Step: 37720, train/loss: 0.0
Step: 37720, train/grad_norm: 3.7573014144420824e-11
Step: 37720, train/learning_rate: 5.116611191624543e-06
Step: 37720, train/epoch: 8.976677894592285
Step: 37730, train/loss: 0.0
Step: 37730, train/grad_norm: 1.5817611977164692e-12
Step: 37730, train/learning_rate: 5.104711817693897e-06
Step: 37730, train/epoch: 8.979057312011719
Step: 37740, train/loss: 0.0
Step: 37740, train/grad_norm: 7.899934179045331e-12
Step: 37740, train/learning_rate: 5.092812898510601e-06
Step: 37740, train/epoch: 8.981437683105469
Step: 37750, train/loss: 0.0
Step: 37750, train/grad_norm: 2.096867035561445e-08
Step: 37750, train/learning_rate: 5.080913979327306e-06
Step: 37750, train/epoch: 8.983817100524902
Step: 37760, train/loss: 0.0
Step: 37760, train/grad_norm: 6.968683036222625e-14
Step: 37760, train/learning_rate: 5.06901460539666e-06
Step: 37760, train/epoch: 8.986197471618652
Step: 37770, train/loss: 0.0
Step: 37770, train/grad_norm: 1.6197782698457175e-11
Step: 37770, train/learning_rate: 5.057115686213365e-06
Step: 37770, train/epoch: 8.988576889038086
Step: 37780, train/loss: 0.0
Step: 37780, train/grad_norm: 2.013361179165063e-11
Step: 37780, train/learning_rate: 5.04521676703007e-06
Step: 37780, train/epoch: 8.99095630645752
Step: 37790, train/loss: 0.0
Step: 37790, train/grad_norm: 8.905676751602698e-10
Step: 37790, train/learning_rate: 5.0333173930994235e-06
Step: 37790, train/epoch: 8.99333667755127
Step: 37800, train/loss: 0.0
Step: 37800, train/grad_norm: 2.0693451119324635e-10
Step: 37800, train/learning_rate: 5.021418473916128e-06
Step: 37800, train/epoch: 8.995716094970703
Step: 37810, train/loss: 0.0
Step: 37810, train/grad_norm: 1.712341379800364e-08
Step: 37810, train/learning_rate: 5.009519099985482e-06
Step: 37810, train/epoch: 8.998096466064453
Step: 37818, eval/loss: 0.06951162964105606
Step: 37818, eval/accuracy: 0.9950020909309387
Step: 37818, eval/f1: 0.9947134852409363
Step: 37818, eval/runtime: 707.82470703125
Step: 37818, eval/samples_per_second: 10.175999641418457
Step: 37818, eval/steps_per_second: 1.2730000019073486
Step: 37818, train/epoch: 9.0
Step: 37820, train/loss: 0.0
Step: 37820, train/grad_norm: 3.023165504600911e-07
Step: 37820, train/learning_rate: 4.997620180802187e-06
Step: 37820, train/epoch: 9.000475883483887
Step: 37830, train/loss: 0.0
Step: 37830, train/grad_norm: 1.1548094924762609e-08
Step: 37830, train/learning_rate: 4.985721261618892e-06
Step: 37830, train/epoch: 9.002856254577637
Step: 37840, train/loss: 0.0
Step: 37840, train/grad_norm: 2.2360208618010802e-07
Step: 37840, train/learning_rate: 4.973821887688246e-06
Step: 37840, train/epoch: 9.00523567199707
Step: 37850, train/loss: 0.0
Step: 37850, train/grad_norm: 4.987512181031661e-09
Step: 37850, train/learning_rate: 4.96192296850495e-06
Step: 37850, train/epoch: 9.007615089416504
Step: 37860, train/loss: 0.0
Step: 37860, train/grad_norm: 3.676001018339259e-13
Step: 37860, train/learning_rate: 4.950023594574304e-06
Step: 37860, train/epoch: 9.009995460510254
Step: 37870, train/loss: 0.0
Step: 37870, train/grad_norm: 2.420136591130273e-10
Step: 37870, train/learning_rate: 4.938124675391009e-06
Step: 37870, train/epoch: 9.012374877929688
Step: 37880, train/loss: 0.0
Step: 37880, train/grad_norm: 2.43553205381275e-10
Step: 37880, train/learning_rate: 4.926225756207714e-06
Step: 37880, train/epoch: 9.014755249023438
Step: 37890, train/loss: 0.0
Step: 37890, train/grad_norm: 6.767195137591386e-11
Step: 37890, train/learning_rate: 4.914326382277068e-06
Step: 37890, train/epoch: 9.017134666442871
Step: 37900, train/loss: 0.0
Step: 37900, train/grad_norm: 6.885811365542338e-13
Step: 37900, train/learning_rate: 4.9024274630937725e-06
Step: 37900, train/epoch: 9.019514083862305
Step: 37910, train/loss: 0.0
Step: 37910, train/grad_norm: 5.205498575591716e-13
Step: 37910, train/learning_rate: 4.890528543910477e-06
Step: 37910, train/epoch: 9.021894454956055
Step: 37920, train/loss: 0.0
Step: 37920, train/grad_norm: 1.228512513939961e-09
Step: 37920, train/learning_rate: 4.878629169979831e-06
Step: 37920, train/epoch: 9.024273872375488
Step: 37930, train/loss: 0.0
Step: 37930, train/grad_norm: 1.871065968828134e-09
Step: 37930, train/learning_rate: 4.866730250796536e-06
Step: 37930, train/epoch: 9.026654243469238
Step: 37940, train/loss: 0.0
Step: 37940, train/grad_norm: 6.604182478664455e-11
Step: 37940, train/learning_rate: 4.85483087686589e-06
Step: 37940, train/epoch: 9.029033660888672
Step: 37950, train/loss: 0.0
Step: 37950, train/grad_norm: 9.575886611434825e-11
Step: 37950, train/learning_rate: 4.842931957682595e-06
Step: 37950, train/epoch: 9.031414031982422
Step: 37960, train/loss: 0.0
Step: 37960, train/grad_norm: 1.6217350812947062e-12
Step: 37960, train/learning_rate: 4.8310330384992994e-06
Step: 37960, train/epoch: 9.033793449401855
Step: 37970, train/loss: 0.0
Step: 37970, train/grad_norm: 1.7506308624959388e-09
Step: 37970, train/learning_rate: 4.819133664568653e-06
Step: 37970, train/epoch: 9.036172866821289
Step: 37980, train/loss: 0.0
Step: 37980, train/grad_norm: 2.2423710110963668e-10
Step: 37980, train/learning_rate: 4.807234745385358e-06
Step: 37980, train/epoch: 9.038553237915039
Step: 37990, train/loss: 0.0
Step: 37990, train/grad_norm: 2.8463070442241722e-11
Step: 37990, train/learning_rate: 4.795335371454712e-06
Step: 37990, train/epoch: 9.040932655334473
Step: 38000, train/loss: 0.0
Step: 38000, train/grad_norm: 6.203258046122428e-11
Step: 38000, train/learning_rate: 4.783436452271417e-06
Step: 38000, train/epoch: 9.043313026428223
Step: 38010, train/loss: 0.0
Step: 38010, train/grad_norm: 2.4075105797827234e-10
Step: 38010, train/learning_rate: 4.7715375330881216e-06
Step: 38010, train/epoch: 9.045692443847656
Step: 38020, train/loss: 0.0
Step: 38020, train/grad_norm: 8.083334890329752e-09
Step: 38020, train/learning_rate: 4.7596381591574755e-06
Step: 38020, train/epoch: 9.048072814941406
Step: 38030, train/loss: 0.0
Step: 38030, train/grad_norm: 2.9156503600358263e-12
Step: 38030, train/learning_rate: 4.74773923997418e-06
Step: 38030, train/epoch: 9.05045223236084
Step: 38040, train/loss: 0.0
Step: 38040, train/grad_norm: 2.0345551909883852e-09
Step: 38040, train/learning_rate: 4.735839866043534e-06
Step: 38040, train/epoch: 9.052831649780273
Step: 38050, train/loss: 0.0
Step: 38050, train/grad_norm: 5.6975389683966426e-11
Step: 38050, train/learning_rate: 4.723940946860239e-06
Step: 38050, train/epoch: 9.055212020874023
Step: 38060, train/loss: 0.0
Step: 38060, train/grad_norm: 2.4647625607165935e-10
Step: 38060, train/learning_rate: 4.712042027676944e-06
Step: 38060, train/epoch: 9.057591438293457
Step: 38070, train/loss: 0.0
Step: 38070, train/grad_norm: 2.608328275588434e-10
Step: 38070, train/learning_rate: 4.700142653746298e-06
Step: 38070, train/epoch: 9.059971809387207
Step: 38080, train/loss: 0.0
Step: 38080, train/grad_norm: 1.3061637993416753e-08
Step: 38080, train/learning_rate: 4.688243734563002e-06
Step: 38080, train/epoch: 9.06235122680664
Step: 38090, train/loss: 0.0
Step: 38090, train/grad_norm: 7.633956045083323e-08
Step: 38090, train/learning_rate: 4.676344815379707e-06
Step: 38090, train/epoch: 9.064730644226074
Step: 38100, train/loss: 0.0
Step: 38100, train/grad_norm: 5.341657116630927e-10
Step: 38100, train/learning_rate: 4.664445441449061e-06
Step: 38100, train/epoch: 9.067111015319824
Step: 38110, train/loss: 0.0
Step: 38110, train/grad_norm: 5.928870133557751e-14
Step: 38110, train/learning_rate: 4.652546522265766e-06
Step: 38110, train/epoch: 9.069490432739258
Step: 38120, train/loss: 0.0
Step: 38120, train/grad_norm: 4.150902910282639e-08
Step: 38120, train/learning_rate: 4.64064714833512e-06
Step: 38120, train/epoch: 9.071870803833008
Step: 38130, train/loss: 0.0
Step: 38130, train/grad_norm: 5.365971881242382e-13
Step: 38130, train/learning_rate: 4.6287482291518245e-06
Step: 38130, train/epoch: 9.074250221252441
Step: 38140, train/loss: 0.0
Step: 38140, train/grad_norm: 8.520345318174805e-09
Step: 38140, train/learning_rate: 4.616849309968529e-06
Step: 38140, train/epoch: 9.076630592346191
Step: 38150, train/loss: 0.0
Step: 38150, train/grad_norm: 3.633697943233871e-12
Step: 38150, train/learning_rate: 4.604949936037883e-06
Step: 38150, train/epoch: 9.079010009765625
Step: 38160, train/loss: 0.0
Step: 38160, train/grad_norm: 1.4071947163074583e-08
Step: 38160, train/learning_rate: 4.593051016854588e-06
Step: 38160, train/epoch: 9.081389427185059
Step: 38170, train/loss: 0.0
Step: 38170, train/grad_norm: 1.477236821756378e-07
Step: 38170, train/learning_rate: 4.581151642923942e-06
Step: 38170, train/epoch: 9.083769798278809
Step: 38180, train/loss: 0.0
Step: 38180, train/grad_norm: 1.0119275657416438e-06
Step: 38180, train/learning_rate: 4.569252723740647e-06
Step: 38180, train/epoch: 9.086149215698242
Step: 38190, train/loss: 0.0
Step: 38190, train/grad_norm: 2.483695595856261e-07
Step: 38190, train/learning_rate: 4.557353804557351e-06
Step: 38190, train/epoch: 9.088529586791992
Step: 38200, train/loss: 0.0
Step: 38200, train/grad_norm: 1.0741645203524164e-12
Step: 38200, train/learning_rate: 4.545454430626705e-06
Step: 38200, train/epoch: 9.090909004211426
Step: 38210, train/loss: 0.0
Step: 38210, train/grad_norm: 3.041851437401988e-09
Step: 38210, train/learning_rate: 4.53355551144341e-06
Step: 38210, train/epoch: 9.093289375305176
Step: 38220, train/loss: 0.0
Step: 38220, train/grad_norm: 1.9791986105577308e-10
Step: 38220, train/learning_rate: 4.521656137512764e-06
Step: 38220, train/epoch: 9.09566879272461
Step: 38230, train/loss: 0.0
Step: 38230, train/grad_norm: 3.333875842059797e-09
Step: 38230, train/learning_rate: 4.509757218329469e-06
Step: 38230, train/epoch: 9.098048210144043
Step: 38240, train/loss: 0.0
Step: 38240, train/grad_norm: 4.127721775493143e-11
Step: 38240, train/learning_rate: 4.4978582991461735e-06
Step: 38240, train/epoch: 9.100428581237793
Step: 38250, train/loss: 0.0
Step: 38250, train/grad_norm: 1.1390102194752671e-09
Step: 38250, train/learning_rate: 4.485958925215527e-06
Step: 38250, train/epoch: 9.102807998657227
Step: 38260, train/loss: 0.0
Step: 38260, train/grad_norm: 3.5991262054002604e-11
Step: 38260, train/learning_rate: 4.474060006032232e-06
Step: 38260, train/epoch: 9.105188369750977
Step: 38270, train/loss: 0.0
Step: 38270, train/grad_norm: 0.004182728473097086
Step: 38270, train/learning_rate: 4.462161086848937e-06
Step: 38270, train/epoch: 9.10756778717041
Step: 38280, train/loss: 0.0
Step: 38280, train/grad_norm: 9.947298096335544e-09
Step: 38280, train/learning_rate: 4.450261712918291e-06
Step: 38280, train/epoch: 9.109947204589844
Step: 38290, train/loss: 0.0
Step: 38290, train/grad_norm: 6.739911146552702e-12
Step: 38290, train/learning_rate: 4.438362793734996e-06
Step: 38290, train/epoch: 9.112327575683594
Step: 38300, train/loss: 0.0
Step: 38300, train/grad_norm: 1.2454076658841018e-09
Step: 38300, train/learning_rate: 4.4264634198043495e-06
Step: 38300, train/epoch: 9.114706993103027
Step: 38310, train/loss: 0.0
Step: 38310, train/grad_norm: 1.8303277116160643e-08
Step: 38310, train/learning_rate: 4.414564500621054e-06
Step: 38310, train/epoch: 9.117087364196777
Step: 38320, train/loss: 0.0
Step: 38320, train/grad_norm: 9.330474171065362e-10
Step: 38320, train/learning_rate: 4.402665581437759e-06
Step: 38320, train/epoch: 9.119466781616211
Step: 38330, train/loss: 0.0
Step: 38330, train/grad_norm: 4.442516493563531e-11
Step: 38330, train/learning_rate: 4.390766207507113e-06
Step: 38330, train/epoch: 9.121847152709961
Step: 38340, train/loss: 0.0
Step: 38340, train/grad_norm: 1.258292172678921e-06
Step: 38340, train/learning_rate: 4.378867288323818e-06
Step: 38340, train/epoch: 9.124226570129395
Step: 38350, train/loss: 0.0
Step: 38350, train/grad_norm: 5.284437470942649e-11
Step: 38350, train/learning_rate: 4.366967914393172e-06
Step: 38350, train/epoch: 9.126605987548828
Step: 38360, train/loss: 0.0
Step: 38360, train/grad_norm: 1.3127415549995386e-13
Step: 38360, train/learning_rate: 4.3550689952098764e-06
Step: 38360, train/epoch: 9.128986358642578
Step: 38370, train/loss: 0.01769999973475933
Step: 38370, train/grad_norm: 5.955737014673446e-10
Step: 38370, train/learning_rate: 4.343170076026581e-06
Step: 38370, train/epoch: 9.131365776062012
Step: 38380, train/loss: 0.0
Step: 38380, train/grad_norm: 1.6600604340055725e-06
Step: 38380, train/learning_rate: 4.331270702095935e-06
Step: 38380, train/epoch: 9.133746147155762
Step: 38390, train/loss: 0.0
Step: 38390, train/grad_norm: 1.7393003270277374e-12
Step: 38390, train/learning_rate: 4.31937178291264e-06
Step: 38390, train/epoch: 9.136125564575195
Step: 38400, train/loss: 0.0
Step: 38400, train/grad_norm: 4.765937089423744e-10
Step: 38400, train/learning_rate: 4.307472408981994e-06
Step: 38400, train/epoch: 9.138505935668945
Step: 38410, train/loss: 0.0
Step: 38410, train/grad_norm: 9.603958517345745e-09
Step: 38410, train/learning_rate: 4.2955734897986986e-06
Step: 38410, train/epoch: 9.140885353088379
Step: 38420, train/loss: 0.0
Step: 38420, train/grad_norm: 3.091221500994834e-08
Step: 38420, train/learning_rate: 4.283674570615403e-06
Step: 38420, train/epoch: 9.143264770507812
Step: 38430, train/loss: 0.0
Step: 38430, train/grad_norm: 1.0741530581670489e-10
Step: 38430, train/learning_rate: 4.271775196684757e-06
Step: 38430, train/epoch: 9.145645141601562
Step: 38440, train/loss: 0.0
Step: 38440, train/grad_norm: 1.7590084944174578e-09
Step: 38440, train/learning_rate: 4.259876277501462e-06
Step: 38440, train/epoch: 9.148024559020996
Step: 38450, train/loss: 0.0
Step: 38450, train/grad_norm: 2.153375433522342e-08
Step: 38450, train/learning_rate: 4.247977358318167e-06
Step: 38450, train/epoch: 9.150404930114746
Step: 38460, train/loss: 0.0
Step: 38460, train/grad_norm: 4.702999589767387e-08
Step: 38460, train/learning_rate: 4.236077984387521e-06
Step: 38460, train/epoch: 9.15278434753418
Step: 38470, train/loss: 0.0
Step: 38470, train/grad_norm: 1.2731270204735523e-10
Step: 38470, train/learning_rate: 4.2241790652042255e-06
Step: 38470, train/epoch: 9.155163764953613
Step: 38480, train/loss: 0.0
Step: 38480, train/grad_norm: 1.6438109469163464e-06
Step: 38480, train/learning_rate: 4.212279691273579e-06
Step: 38480, train/epoch: 9.157544136047363
Step: 38490, train/loss: 0.0
Step: 38490, train/grad_norm: 2.2572820349009426e-11
Step: 38490, train/learning_rate: 4.200380772090284e-06
Step: 38490, train/epoch: 9.159923553466797
Step: 38500, train/loss: 0.0
Step: 38500, train/grad_norm: 1.1604881985927129e-11
Step: 38500, train/learning_rate: 4.188481852906989e-06
Step: 38500, train/epoch: 9.162303924560547
Step: 38510, train/loss: 0.0
Step: 38510, train/grad_norm: 9.682568635760447e-11
Step: 38510, train/learning_rate: 4.176582478976343e-06
Step: 38510, train/epoch: 9.16468334197998
Step: 38520, train/loss: 0.0
Step: 38520, train/grad_norm: 1.892746404053014e-09
Step: 38520, train/learning_rate: 4.164683559793048e-06
Step: 38520, train/epoch: 9.16706371307373
Step: 38530, train/loss: 0.0
Step: 38530, train/grad_norm: 2.9016258146219442e-11
Step: 38530, train/learning_rate: 4.1527841858624015e-06
Step: 38530, train/epoch: 9.169443130493164
Step: 38540, train/loss: 0.0
Step: 38540, train/grad_norm: 8.990556210619616e-08
Step: 38540, train/learning_rate: 4.140885266679106e-06
Step: 38540, train/epoch: 9.171822547912598
Step: 38550, train/loss: 0.0
Step: 38550, train/grad_norm: 4.217048044097282e-09
Step: 38550, train/learning_rate: 4.128986347495811e-06
Step: 38550, train/epoch: 9.174202919006348
Step: 38560, train/loss: 0.0
Step: 38560, train/grad_norm: 7.394621093226306e-08
Step: 38560, train/learning_rate: 4.117086973565165e-06
Step: 38560, train/epoch: 9.176582336425781
Step: 38570, train/loss: 0.0
Step: 38570, train/grad_norm: 7.544530972181107e-12
Step: 38570, train/learning_rate: 4.10518805438187e-06
Step: 38570, train/epoch: 9.178962707519531
Step: 38580, train/loss: 0.0
Step: 38580, train/grad_norm: 1.1274990718845856e-08
Step: 38580, train/learning_rate: 4.0932891351985745e-06
Step: 38580, train/epoch: 9.181342124938965
Step: 38590, train/loss: 0.0
Step: 38590, train/grad_norm: 5.649682111030785e-11
Step: 38590, train/learning_rate: 4.081389761267928e-06
Step: 38590, train/epoch: 9.183722496032715
Step: 38600, train/loss: 0.0
Step: 38600, train/grad_norm: 2.6440995660320654e-12
Step: 38600, train/learning_rate: 4.069490842084633e-06
Step: 38600, train/epoch: 9.186101913452148
Step: 38610, train/loss: 0.0
Step: 38610, train/grad_norm: 3.4673598627715796e-14
Step: 38610, train/learning_rate: 4.057591468153987e-06
Step: 38610, train/epoch: 9.188481330871582
Step: 38620, train/loss: 0.0
Step: 38620, train/grad_norm: 9.301266701289279e-11
Step: 38620, train/learning_rate: 4.045692548970692e-06
Step: 38620, train/epoch: 9.190861701965332
Step: 38630, train/loss: 0.0
Step: 38630, train/grad_norm: 4.187623581231037e-10
Step: 38630, train/learning_rate: 4.033793629787397e-06
Step: 38630, train/epoch: 9.193241119384766
Step: 38640, train/loss: 0.0
Step: 38640, train/grad_norm: 1.726139982902511e-14
Step: 38640, train/learning_rate: 4.0218942558567505e-06
Step: 38640, train/epoch: 9.195621490478516
Step: 38650, train/loss: 0.0
Step: 38650, train/grad_norm: 2.3024086109879605e-11
Step: 38650, train/learning_rate: 4.009995336673455e-06
Step: 38650, train/epoch: 9.19800090789795
Step: 38660, train/loss: 0.0
Step: 38660, train/grad_norm: 2.639735541265509e-09
Step: 38660, train/learning_rate: 3.998095962742809e-06
Step: 38660, train/epoch: 9.200380325317383
Step: 38670, train/loss: 0.0
Step: 38670, train/grad_norm: 1.6641080874402192e-11
Step: 38670, train/learning_rate: 3.986197043559514e-06
Step: 38670, train/epoch: 9.202760696411133
Step: 38680, train/loss: 0.0
Step: 38680, train/grad_norm: 4.5058092673633254e-11
Step: 38680, train/learning_rate: 3.974298124376219e-06
Step: 38680, train/epoch: 9.205140113830566
Step: 38690, train/loss: 0.0
Step: 38690, train/grad_norm: 1.0492182643540016e-11
Step: 38690, train/learning_rate: 3.962398750445573e-06
Step: 38690, train/epoch: 9.207520484924316
Step: 38700, train/loss: 0.0
Step: 38700, train/grad_norm: 4.1448109300290525e-07
Step: 38700, train/learning_rate: 3.9504998312622774e-06
Step: 38700, train/epoch: 9.20989990234375
Step: 38710, train/loss: 0.0
Step: 38710, train/grad_norm: 5.085857082698375e-11
Step: 38710, train/learning_rate: 3.938600457331631e-06
Step: 38710, train/epoch: 9.2122802734375
Step: 38720, train/loss: 0.0
Step: 38720, train/grad_norm: 3.9521773982816033e-13
Step: 38720, train/learning_rate: 3.926701538148336e-06
Step: 38720, train/epoch: 9.214659690856934
Step: 38730, train/loss: 0.0
Step: 38730, train/grad_norm: 2.2195795548740627e-11
Step: 38730, train/learning_rate: 3.914802618965041e-06
Step: 38730, train/epoch: 9.217039108276367
Step: 38740, train/loss: 0.0
Step: 38740, train/grad_norm: 3.629516825975898e-11
Step: 38740, train/learning_rate: 3.902903245034395e-06
Step: 38740, train/epoch: 9.219419479370117
Step: 38750, train/loss: 0.0
Step: 38750, train/grad_norm: 3.279642182829612e-11
Step: 38750, train/learning_rate: 3.8910043258510996e-06
Step: 38750, train/epoch: 9.22179889678955
Step: 38760, train/loss: 0.0
Step: 38760, train/grad_norm: 4.274123069358815e-11
Step: 38760, train/learning_rate: 3.879105406667804e-06
Step: 38760, train/epoch: 9.2241792678833
Step: 38770, train/loss: 0.0
Step: 38770, train/grad_norm: 4.329501479549691e-10
Step: 38770, train/learning_rate: 3.867206032737158e-06
Step: 38770, train/epoch: 9.226558685302734
Step: 38780, train/loss: 0.0
Step: 38780, train/grad_norm: 2.2753417205922943e-11
Step: 38780, train/learning_rate: 3.855307113553863e-06
Step: 38780, train/epoch: 9.228939056396484
Step: 38790, train/loss: 0.0
Step: 38790, train/grad_norm: 2.0059337524358511e-10
Step: 38790, train/learning_rate: 3.843407739623217e-06
Step: 38790, train/epoch: 9.231318473815918
Step: 38800, train/loss: 0.0
Step: 38800, train/grad_norm: 1.765895901728598e-10
Step: 38800, train/learning_rate: 3.831508820439922e-06
Step: 38800, train/epoch: 9.233697891235352
Step: 38810, train/loss: 0.0
Step: 38810, train/grad_norm: 6.12712300600976e-13
Step: 38810, train/learning_rate: 3.8196099012566265e-06
Step: 38810, train/epoch: 9.236078262329102
Step: 38820, train/loss: 0.0
Step: 38820, train/grad_norm: 8.46573422474961e-10
Step: 38820, train/learning_rate: 3.8077105273259804e-06
Step: 38820, train/epoch: 9.238457679748535
Step: 38830, train/loss: 0.0
Step: 38830, train/grad_norm: 1.8149334257699934e-09
Step: 38830, train/learning_rate: 3.795811608142685e-06
Step: 38830, train/epoch: 9.240838050842285
Step: 38840, train/loss: 0.0
Step: 38840, train/grad_norm: 6.688985476621667e-11
Step: 38840, train/learning_rate: 3.7839124615857145e-06
Step: 38840, train/epoch: 9.243217468261719
Step: 38850, train/loss: 0.0
Step: 38850, train/grad_norm: 8.07265507773991e-08
Step: 38850, train/learning_rate: 3.772013315028744e-06
Step: 38850, train/epoch: 9.245596885681152
Step: 38860, train/loss: 0.0
Step: 38860, train/grad_norm: 8.662893180577669e-10
Step: 38860, train/learning_rate: 3.760114168471773e-06
Step: 38860, train/epoch: 9.247977256774902
Step: 38870, train/loss: 0.0
Step: 38870, train/grad_norm: 5.7235611805595e-09
Step: 38870, train/learning_rate: 3.748215249288478e-06
Step: 38870, train/epoch: 9.250356674194336
Step: 38880, train/loss: 0.0
Step: 38880, train/grad_norm: 2.174920465947139e-09
Step: 38880, train/learning_rate: 3.7363161027315073e-06
Step: 38880, train/epoch: 9.252737045288086
Step: 38890, train/loss: 0.0
Step: 38890, train/grad_norm: 1.3074926341269388e-12
Step: 38890, train/learning_rate: 3.7244169561745366e-06
Step: 38890, train/epoch: 9.25511646270752
Step: 38900, train/loss: 0.0
Step: 38900, train/grad_norm: 3.782954394182525e-08
Step: 38900, train/learning_rate: 3.712517809617566e-06
Step: 38900, train/epoch: 9.25749683380127
Step: 38910, train/loss: 0.0
Step: 38910, train/grad_norm: 7.163857007608243e-11
Step: 38910, train/learning_rate: 3.7006186630605953e-06
Step: 38910, train/epoch: 9.259876251220703
Step: 38920, train/loss: 0.0
Step: 38920, train/grad_norm: 2.5561153194075814e-10
Step: 38920, train/learning_rate: 3.6887197438773e-06
Step: 38920, train/epoch: 9.262255668640137
Step: 38930, train/loss: 0.0
Step: 38930, train/grad_norm: 4.694786781556104e-10
Step: 38930, train/learning_rate: 3.6768205973203294e-06
Step: 38930, train/epoch: 9.264636039733887
Step: 38940, train/loss: 0.0
Step: 38940, train/grad_norm: 7.039652281243258e-12
Step: 38940, train/learning_rate: 3.6649214507633587e-06
Step: 38940, train/epoch: 9.26701545715332
Step: 38950, train/loss: 0.0
Step: 38950, train/grad_norm: 1.7952571651491667e-09
Step: 38950, train/learning_rate: 3.653022304206388e-06
Step: 38950, train/epoch: 9.26939582824707
Step: 38960, train/loss: 0.0
Step: 38960, train/grad_norm: 3.872867998522267e-11
Step: 38960, train/learning_rate: 3.641123385023093e-06
Step: 38960, train/epoch: 9.271775245666504
Step: 38970, train/loss: 0.0
Step: 38970, train/grad_norm: 2.481777672258545e-09
Step: 38970, train/learning_rate: 3.629224238466122e-06
Step: 38970, train/epoch: 9.274155616760254
Step: 38980, train/loss: 0.0
Step: 38980, train/grad_norm: 1.277932204502008e-09
Step: 38980, train/learning_rate: 3.6173250919091515e-06
Step: 38980, train/epoch: 9.276535034179688
Step: 38990, train/loss: 0.0
Step: 38990, train/grad_norm: 1.6092671639711398e-09
Step: 38990, train/learning_rate: 3.605425945352181e-06
Step: 38990, train/epoch: 9.278914451599121
Step: 39000, train/loss: 0.0
Step: 39000, train/grad_norm: 1.679175722080206e-11
Step: 39000, train/learning_rate: 3.59352679879521e-06
Step: 39000, train/epoch: 9.281294822692871
Step: 39010, train/loss: 0.0
Step: 39010, train/grad_norm: 2.1225362445420615e-07
Step: 39010, train/learning_rate: 3.581627879611915e-06
Step: 39010, train/epoch: 9.283674240112305
Step: 39020, train/loss: 0.0
Step: 39020, train/grad_norm: 4.641829143281484e-09
Step: 39020, train/learning_rate: 3.5697287330549443e-06
Step: 39020, train/epoch: 9.286054611206055
Step: 39030, train/loss: 0.0
Step: 39030, train/grad_norm: 4.7054538043767025e-09
Step: 39030, train/learning_rate: 3.5578295864979737e-06
Step: 39030, train/epoch: 9.288434028625488
Step: 39040, train/loss: 0.0
Step: 39040, train/grad_norm: 1.1534628113762935e-10
Step: 39040, train/learning_rate: 3.545930439941003e-06
Step: 39040, train/epoch: 9.290813446044922
Step: 39050, train/loss: 0.0
Step: 39050, train/grad_norm: 2.632522498602352e-11
Step: 39050, train/learning_rate: 3.5340315207577078e-06
Step: 39050, train/epoch: 9.293193817138672
Step: 39060, train/loss: 0.0
Step: 39060, train/grad_norm: 4.16970652450388e-10
Step: 39060, train/learning_rate: 3.522132374200737e-06
Step: 39060, train/epoch: 9.295573234558105
Step: 39070, train/loss: 0.0
Step: 39070, train/grad_norm: 2.9837596837722913e-09
Step: 39070, train/learning_rate: 3.5102332276437664e-06
Step: 39070, train/epoch: 9.297953605651855
Step: 39080, train/loss: 0.0
Step: 39080, train/grad_norm: 1.6101693589565258e-10
Step: 39080, train/learning_rate: 3.4983340810867958e-06
Step: 39080, train/epoch: 9.300333023071289
Step: 39090, train/loss: 0.0
Step: 39090, train/grad_norm: 1.4453493690780306e-07
Step: 39090, train/learning_rate: 3.486434934529825e-06
Step: 39090, train/epoch: 9.302713394165039
Step: 39100, train/loss: 0.0
Step: 39100, train/grad_norm: 6.397137593955327e-13
Step: 39100, train/learning_rate: 3.47453601534653e-06
Step: 39100, train/epoch: 9.305092811584473
Step: 39110, train/loss: 0.0
Step: 39110, train/grad_norm: 2.341712246334282e-08
Step: 39110, train/learning_rate: 3.4626368687895592e-06
Step: 39110, train/epoch: 9.307472229003906
Step: 39120, train/loss: 0.0
Step: 39120, train/grad_norm: 3.036118187083048e-06
Step: 39120, train/learning_rate: 3.4507377222325886e-06
Step: 39120, train/epoch: 9.309852600097656
Step: 39130, train/loss: 0.0
Step: 39130, train/grad_norm: 3.3309777158763154e-10
Step: 39130, train/learning_rate: 3.438838575675618e-06
Step: 39130, train/epoch: 9.31223201751709
Step: 39140, train/loss: 0.0
Step: 39140, train/grad_norm: 1.4601834408267678e-08
Step: 39140, train/learning_rate: 3.4269396564923227e-06
Step: 39140, train/epoch: 9.31461238861084
Step: 39150, train/loss: 0.0
Step: 39150, train/grad_norm: 1.9208269463888428e-07
Step: 39150, train/learning_rate: 3.415040509935352e-06
Step: 39150, train/epoch: 9.316991806030273
Step: 39160, train/loss: 0.0
Step: 39160, train/grad_norm: 1.2243750013851695e-08
Step: 39160, train/learning_rate: 3.4031413633783814e-06
Step: 39160, train/epoch: 9.319372177124023
Step: 39170, train/loss: 0.0
Step: 39170, train/grad_norm: 9.056739386226198e-11
Step: 39170, train/learning_rate: 3.3912422168214107e-06
Step: 39170, train/epoch: 9.321751594543457
Step: 39180, train/loss: 0.0
Step: 39180, train/grad_norm: 7.801568280285665e-10
Step: 39180, train/learning_rate: 3.37934307026444e-06
Step: 39180, train/epoch: 9.32413101196289
Step: 39190, train/loss: 0.0
Step: 39190, train/grad_norm: 5.621232923580521e-10
Step: 39190, train/learning_rate: 3.367444151081145e-06
Step: 39190, train/epoch: 9.32651138305664
Step: 39200, train/loss: 0.0
Step: 39200, train/grad_norm: 1.21710240819084e-06
Step: 39200, train/learning_rate: 3.355545004524174e-06
Step: 39200, train/epoch: 9.328890800476074
Step: 39210, train/loss: 0.0
Step: 39210, train/grad_norm: 4.7534726155262774e-11
Step: 39210, train/learning_rate: 3.3436458579672035e-06
Step: 39210, train/epoch: 9.331271171569824
Step: 39220, train/loss: 0.0
Step: 39220, train/grad_norm: 4.8675724909075235e-12
Step: 39220, train/learning_rate: 3.331746711410233e-06
Step: 39220, train/epoch: 9.333650588989258
Step: 39230, train/loss: 0.0
Step: 39230, train/grad_norm: 7.134591140101065e-09
Step: 39230, train/learning_rate: 3.3198477922269376e-06
Step: 39230, train/epoch: 9.336030006408691
Step: 39240, train/loss: 0.0
Step: 39240, train/grad_norm: 9.980353349314797e-11
Step: 39240, train/learning_rate: 3.307948645669967e-06
Step: 39240, train/epoch: 9.338410377502441
Step: 39250, train/loss: 0.0
Step: 39250, train/grad_norm: 2.7612991004843934e-08
Step: 39250, train/learning_rate: 3.2960494991129963e-06
Step: 39250, train/epoch: 9.340789794921875
Step: 39260, train/loss: 0.0
Step: 39260, train/grad_norm: 6.558689008562268e-11
Step: 39260, train/learning_rate: 3.2841503525560256e-06
Step: 39260, train/epoch: 9.343170166015625
Step: 39270, train/loss: 0.0
Step: 39270, train/grad_norm: 3.497792055995319e-10
Step: 39270, train/learning_rate: 3.272251205999055e-06
Step: 39270, train/epoch: 9.345549583435059
Step: 39280, train/loss: 0.0
Step: 39280, train/grad_norm: 1.4249469131755177e-05
Step: 39280, train/learning_rate: 3.2603522868157597e-06
Step: 39280, train/epoch: 9.347929954528809
Step: 39290, train/loss: 0.0
Step: 39290, train/grad_norm: 5.505546951667384e-08
Step: 39290, train/learning_rate: 3.248453140258789e-06
Step: 39290, train/epoch: 9.350309371948242
Step: 39300, train/loss: 0.0
Step: 39300, train/grad_norm: 2.8164414800357918e-08
Step: 39300, train/learning_rate: 3.2365539937018184e-06
Step: 39300, train/epoch: 9.352688789367676
Step: 39310, train/loss: 0.0
Step: 39310, train/grad_norm: 3.178465712849743e-09
Step: 39310, train/learning_rate: 3.2246548471448477e-06
Step: 39310, train/epoch: 9.355069160461426
Step: 39320, train/loss: 0.0
Step: 39320, train/grad_norm: 1.2415571681856363e-08
Step: 39320, train/learning_rate: 3.2127559279615525e-06
Step: 39320, train/epoch: 9.35744857788086
Step: 39330, train/loss: 0.0
Step: 39330, train/grad_norm: 2.5480953458334454e-10
Step: 39330, train/learning_rate: 3.200856781404582e-06
Step: 39330, train/epoch: 9.35982894897461
Step: 39340, train/loss: 0.0
Step: 39340, train/grad_norm: 9.588693034023876e-11
Step: 39340, train/learning_rate: 3.188957634847611e-06
Step: 39340, train/epoch: 9.362208366394043
Step: 39350, train/loss: 0.0
Step: 39350, train/grad_norm: 9.311174054005278e-10
Step: 39350, train/learning_rate: 3.1770584882906405e-06
Step: 39350, train/epoch: 9.364588737487793
Step: 39360, train/loss: 0.0
Step: 39360, train/grad_norm: 2.0211540885384238e-08
Step: 39360, train/learning_rate: 3.16515934173367e-06
Step: 39360, train/epoch: 9.366968154907227
Step: 39370, train/loss: 0.0
Step: 39370, train/grad_norm: 5.202647344049183e-07
Step: 39370, train/learning_rate: 3.1532604225503746e-06
Step: 39370, train/epoch: 9.36934757232666
Step: 39380, train/loss: 0.0
Step: 39380, train/grad_norm: 1.8818424596389605e-10
Step: 39380, train/learning_rate: 3.141361275993404e-06
Step: 39380, train/epoch: 9.37172794342041
Step: 39390, train/loss: 0.0
Step: 39390, train/grad_norm: 4.5752134525400834e-08
Step: 39390, train/learning_rate: 3.1294621294364333e-06
Step: 39390, train/epoch: 9.374107360839844
Step: 39400, train/loss: 0.0
Step: 39400, train/grad_norm: 2.512680952193591e-09
Step: 39400, train/learning_rate: 3.1175629828794627e-06
Step: 39400, train/epoch: 9.376487731933594
Step: 39410, train/loss: 0.0
Step: 39410, train/grad_norm: 6.925134243829234e-09
Step: 39410, train/learning_rate: 3.1056640636961674e-06
Step: 39410, train/epoch: 9.378867149353027
Step: 39420, train/loss: 0.0
Step: 39420, train/grad_norm: 2.1504905495239868e-11
Step: 39420, train/learning_rate: 3.0937649171391968e-06
Step: 39420, train/epoch: 9.381246566772461
Step: 39430, train/loss: 0.0
Step: 39430, train/grad_norm: 2.0758575414170366e-10
Step: 39430, train/learning_rate: 3.081865770582226e-06
Step: 39430, train/epoch: 9.383626937866211
Step: 39440, train/loss: 0.0
Step: 39440, train/grad_norm: 1.491214085547199e-08
Step: 39440, train/learning_rate: 3.0699666240252554e-06
Step: 39440, train/epoch: 9.386006355285645
Step: 39450, train/loss: 0.0
Step: 39450, train/grad_norm: 5.371447731050694e-10
Step: 39450, train/learning_rate: 3.0580674774682848e-06
Step: 39450, train/epoch: 9.388386726379395
Step: 39460, train/loss: 0.0
Step: 39460, train/grad_norm: 6.234592841991571e-10
Step: 39460, train/learning_rate: 3.0461685582849896e-06
Step: 39460, train/epoch: 9.390766143798828
Step: 39470, train/loss: 0.0
Step: 39470, train/grad_norm: 3.273824988880847e-09
Step: 39470, train/learning_rate: 3.034269411728019e-06
Step: 39470, train/epoch: 9.393146514892578
Step: 39480, train/loss: 0.0
Step: 39480, train/grad_norm: 1.172183688004716e-13
Step: 39480, train/learning_rate: 3.0223702651710482e-06
Step: 39480, train/epoch: 9.395525932312012
Step: 39490, train/loss: 0.0
Step: 39490, train/grad_norm: 1.660172346817923e-11
Step: 39490, train/learning_rate: 3.0104711186140776e-06
Step: 39490, train/epoch: 9.397905349731445
Step: 39500, train/loss: 0.0
Step: 39500, train/grad_norm: 1.2261843984617826e-08
Step: 39500, train/learning_rate: 2.9985721994307823e-06
Step: 39500, train/epoch: 9.400285720825195
Step: 39510, train/loss: 0.0
Step: 39510, train/grad_norm: 3.1723673687977794e-10
Step: 39510, train/learning_rate: 2.9866730528738117e-06
Step: 39510, train/epoch: 9.402665138244629
Step: 39520, train/loss: 0.0
Step: 39520, train/grad_norm: 1.158451482297096e-06
Step: 39520, train/learning_rate: 2.974773906316841e-06
Step: 39520, train/epoch: 9.405045509338379
Step: 39530, train/loss: 0.0
Step: 39530, train/grad_norm: 5.737986474851908e-11
Step: 39530, train/learning_rate: 2.9628747597598704e-06
Step: 39530, train/epoch: 9.407424926757812
Step: 39540, train/loss: 0.0
Step: 39540, train/grad_norm: 3.968587611313801e-12
Step: 39540, train/learning_rate: 2.9509756132028997e-06
Step: 39540, train/epoch: 9.409805297851562
Step: 39550, train/loss: 0.0
Step: 39550, train/grad_norm: 1.4212708787653128e-10
Step: 39550, train/learning_rate: 2.9390766940196045e-06
Step: 39550, train/epoch: 9.412184715270996
Step: 39560, train/loss: 0.0
Step: 39560, train/grad_norm: 3.811710769241472e-07
Step: 39560, train/learning_rate: 2.927177547462634e-06
Step: 39560, train/epoch: 9.41456413269043
Step: 39570, train/loss: 0.0
Step: 39570, train/grad_norm: 1.80624473711255e-11
Step: 39570, train/learning_rate: 2.915278400905663e-06
Step: 39570, train/epoch: 9.41694450378418
Step: 39580, train/loss: 0.0
Step: 39580, train/grad_norm: 6.636608418375545e-08
Step: 39580, train/learning_rate: 2.9033792543486925e-06
Step: 39580, train/epoch: 9.419323921203613
Step: 39590, train/loss: 0.0
Step: 39590, train/grad_norm: 3.3741687829186295e-12
Step: 39590, train/learning_rate: 2.8914803351653973e-06
Step: 39590, train/epoch: 9.421704292297363
Step: 39600, train/loss: 0.0
Step: 39600, train/grad_norm: 5.6668243014756925e-11
Step: 39600, train/learning_rate: 2.8795811886084266e-06
Step: 39600, train/epoch: 9.424083709716797
Step: 39610, train/loss: 0.0
Step: 39610, train/grad_norm: 2.8114605754581135e-08
Step: 39610, train/learning_rate: 2.867682042051456e-06
Step: 39610, train/epoch: 9.42646312713623
Step: 39620, train/loss: 0.0
Step: 39620, train/grad_norm: 8.7734142653062e-06
Step: 39620, train/learning_rate: 2.8557828954944853e-06
Step: 39620, train/epoch: 9.42884349822998
Step: 39630, train/loss: 0.0
Step: 39630, train/grad_norm: 3.4723016129056816e-10
Step: 39630, train/learning_rate: 2.84388397631119e-06
Step: 39630, train/epoch: 9.431222915649414
Step: 39640, train/loss: 0.0
Step: 39640, train/grad_norm: 9.425005664809305e-09
Step: 39640, train/learning_rate: 2.8319848297542194e-06
Step: 39640, train/epoch: 9.433603286743164
Step: 39650, train/loss: 0.0
Step: 39650, train/grad_norm: 3.730400432999659e-09
Step: 39650, train/learning_rate: 2.8200856831972487e-06
Step: 39650, train/epoch: 9.435982704162598
Step: 39660, train/loss: 0.0
Step: 39660, train/grad_norm: 2.4258062580884143e-07
Step: 39660, train/learning_rate: 2.808186536640278e-06
Step: 39660, train/epoch: 9.438363075256348
Step: 39670, train/loss: 0.0
Step: 39670, train/grad_norm: 3.04989783428411e-10
Step: 39670, train/learning_rate: 2.7962873900833074e-06
Step: 39670, train/epoch: 9.440742492675781
Step: 39680, train/loss: 0.0
Step: 39680, train/grad_norm: 9.30439210920786e-13
Step: 39680, train/learning_rate: 2.784388470900012e-06
Step: 39680, train/epoch: 9.443121910095215
Step: 39690, train/loss: 0.0
Step: 39690, train/grad_norm: 1.5168814815336162e-10
Step: 39690, train/learning_rate: 2.7724893243430415e-06
Step: 39690, train/epoch: 9.445502281188965
Step: 39700, train/loss: 0.0
Step: 39700, train/grad_norm: 7.351121664514459e-13
Step: 39700, train/learning_rate: 2.760590177786071e-06
Step: 39700, train/epoch: 9.447881698608398
Step: 39710, train/loss: 0.0
Step: 39710, train/grad_norm: 5.394940605363274e-10
Step: 39710, train/learning_rate: 2.7486910312291e-06
Step: 39710, train/epoch: 9.450262069702148
Step: 39720, train/loss: 0.0
Step: 39720, train/grad_norm: 2.8658528675173933e-11
Step: 39720, train/learning_rate: 2.736792112045805e-06
Step: 39720, train/epoch: 9.452641487121582
Step: 39730, train/loss: 0.0
Step: 39730, train/grad_norm: 8.793519157279661e-08
Step: 39730, train/learning_rate: 2.7248929654888343e-06
Step: 39730, train/epoch: 9.455021858215332
Step: 39740, train/loss: 0.0
Step: 39740, train/grad_norm: 2.6950872089592792e-11
Step: 39740, train/learning_rate: 2.7129938189318636e-06
Step: 39740, train/epoch: 9.457401275634766
Step: 39750, train/loss: 0.0
Step: 39750, train/grad_norm: 5.579829376323175e-10
Step: 39750, train/learning_rate: 2.701094672374893e-06
Step: 39750, train/epoch: 9.4597806930542
Step: 39760, train/loss: 0.0
Step: 39760, train/grad_norm: 8.21997225841642e-09
Step: 39760, train/learning_rate: 2.6891955258179223e-06
Step: 39760, train/epoch: 9.46216106414795
Step: 39770, train/loss: 0.0
Step: 39770, train/grad_norm: 2.9915370181043954e-09
Step: 39770, train/learning_rate: 2.677296606634627e-06
Step: 39770, train/epoch: 9.464540481567383
Step: 39780, train/loss: 0.0
Step: 39780, train/grad_norm: 2.9183627736628637e-11
Step: 39780, train/learning_rate: 2.6653974600776564e-06
Step: 39780, train/epoch: 9.466920852661133
Step: 39790, train/loss: 0.0
Step: 39790, train/grad_norm: 3.905289850081317e-05
Step: 39790, train/learning_rate: 2.6534983135206858e-06
Step: 39790, train/epoch: 9.469300270080566
Step: 39800, train/loss: 0.0
Step: 39800, train/grad_norm: 2.251245414044978e-12
Step: 39800, train/learning_rate: 2.641599166963715e-06
Step: 39800, train/epoch: 9.4716796875
Step: 39810, train/loss: 0.0
Step: 39810, train/grad_norm: 9.005578401055647e-13
Step: 39810, train/learning_rate: 2.62970024778042e-06
Step: 39810, train/epoch: 9.47406005859375
Step: 39820, train/loss: 0.0
Step: 39820, train/grad_norm: 2.180269076390573e-09
Step: 39820, train/learning_rate: 2.6178011012234492e-06
Step: 39820, train/epoch: 9.476439476013184
Step: 39830, train/loss: 0.0
Step: 39830, train/grad_norm: 7.116144895569221e-10
Step: 39830, train/learning_rate: 2.6059019546664786e-06
Step: 39830, train/epoch: 9.478819847106934
Step: 39840, train/loss: 0.0
Step: 39840, train/grad_norm: 8.403058249228934e-10
Step: 39840, train/learning_rate: 2.594002808109508e-06
Step: 39840, train/epoch: 9.481199264526367
Step: 39850, train/loss: 0.0
Step: 39850, train/grad_norm: 1.5255256780033477e-10
Step: 39850, train/learning_rate: 2.5821036615525372e-06
Step: 39850, train/epoch: 9.483579635620117
Step: 39860, train/loss: 0.0
Step: 39860, train/grad_norm: 1.4784012236646049e-08
Step: 39860, train/learning_rate: 2.570204742369242e-06
Step: 39860, train/epoch: 9.48595905303955
Step: 39870, train/loss: 0.0
Step: 39870, train/grad_norm: 2.41206585954723e-11
Step: 39870, train/learning_rate: 2.5583055958122713e-06
Step: 39870, train/epoch: 9.488338470458984
Step: 39880, train/loss: 0.0
Step: 39880, train/grad_norm: 2.087658268479231e-09
Step: 39880, train/learning_rate: 2.5464064492553007e-06
Step: 39880, train/epoch: 9.490718841552734
Step: 39890, train/loss: 0.0
Step: 39890, train/grad_norm: 1.898152295065625e-11
Step: 39890, train/learning_rate: 2.53450730269833e-06
Step: 39890, train/epoch: 9.493098258972168
Step: 39900, train/loss: 0.0
Step: 39900, train/grad_norm: 9.309994442041614e-10
Step: 39900, train/learning_rate: 2.522608383515035e-06
Step: 39900, train/epoch: 9.495478630065918
Step: 39910, train/loss: 0.0
Step: 39910, train/grad_norm: 2.2438013669301426e-09
Step: 39910, train/learning_rate: 2.510709236958064e-06
Step: 39910, train/epoch: 9.497858047485352
Step: 39920, train/loss: 0.0
Step: 39920, train/grad_norm: 7.628948139881686e-08
Step: 39920, train/learning_rate: 2.4988100904010935e-06
Step: 39920, train/epoch: 9.500238418579102
Step: 39930, train/loss: 0.0
Step: 39930, train/grad_norm: 7.53381038109957e-11
Step: 39930, train/learning_rate: 2.486910943844123e-06
Step: 39930, train/epoch: 9.502617835998535
Step: 39940, train/loss: 0.0
Step: 39940, train/grad_norm: 6.3580638531846034e-09
Step: 39940, train/learning_rate: 2.475011797287152e-06
Step: 39940, train/epoch: 9.504997253417969
Step: 39950, train/loss: 0.0
Step: 39950, train/grad_norm: 2.802929000012e-10
Step: 39950, train/learning_rate: 2.463112878103857e-06
Step: 39950, train/epoch: 9.507377624511719
Step: 39960, train/loss: 0.0
Step: 39960, train/grad_norm: 2.4274268359270046e-11
Step: 39960, train/learning_rate: 2.4512137315468863e-06
Step: 39960, train/epoch: 9.509757041931152
Step: 39970, train/loss: 0.0
Step: 39970, train/grad_norm: 2.4285931488066126e-08
Step: 39970, train/learning_rate: 2.4393145849899156e-06
Step: 39970, train/epoch: 9.512137413024902
Step: 39980, train/loss: 0.0
Step: 39980, train/grad_norm: 7.1963923709006394e-09
Step: 39980, train/learning_rate: 2.427415438432945e-06
Step: 39980, train/epoch: 9.514516830444336
Step: 39990, train/loss: 0.0
Step: 39990, train/grad_norm: 5.322279506003724e-09
Step: 39990, train/learning_rate: 2.4155165192496497e-06
Step: 39990, train/epoch: 9.51689624786377
Step: 40000, train/loss: 0.0
Step: 40000, train/grad_norm: 1.7845171174091234e-10
Step: 40000, train/learning_rate: 2.403617372692679e-06
Step: 40000, train/epoch: 9.51927661895752
Step: 40010, train/loss: 0.0
Step: 40010, train/grad_norm: 1.9563540121225742e-08
Step: 40010, train/learning_rate: 2.3917182261357084e-06
Step: 40010, train/epoch: 9.521656036376953
Step: 40020, train/loss: 0.0
Step: 40020, train/grad_norm: 3.1646825959796843e-07
Step: 40020, train/learning_rate: 2.3798190795787377e-06
Step: 40020, train/epoch: 9.524036407470703
Step: 40030, train/loss: 0.0
Step: 40030, train/grad_norm: 1.0134288430663219e-07
Step: 40030, train/learning_rate: 2.367919933021767e-06
Step: 40030, train/epoch: 9.526415824890137
Step: 40040, train/loss: 0.0
Step: 40040, train/grad_norm: 2.70646260958074e-09
Step: 40040, train/learning_rate: 2.356021013838472e-06
Step: 40040, train/epoch: 9.528796195983887
Step: 40050, train/loss: 0.0
Step: 40050, train/grad_norm: 4.3137990957120564e-09
Step: 40050, train/learning_rate: 2.344121867281501e-06
Step: 40050, train/epoch: 9.53117561340332
Step: 40060, train/loss: 0.0
Step: 40060, train/grad_norm: 7.692810277681872e-11
Step: 40060, train/learning_rate: 2.3322227207245305e-06
Step: 40060, train/epoch: 9.533555030822754
Step: 40070, train/loss: 0.0
Step: 40070, train/grad_norm: 1.179193868201464e-13
Step: 40070, train/learning_rate: 2.32032357416756e-06
Step: 40070, train/epoch: 9.535935401916504
Step: 40080, train/loss: 0.0
Step: 40080, train/grad_norm: 1.2641379913344575e-12
Step: 40080, train/learning_rate: 2.3084246549842646e-06
Step: 40080, train/epoch: 9.538314819335938
Step: 40090, train/loss: 0.0
Step: 40090, train/grad_norm: 1.7667978354212605e-13
Step: 40090, train/learning_rate: 2.296525508427294e-06
Step: 40090, train/epoch: 9.540695190429688
Step: 40100, train/loss: 0.0
Step: 40100, train/grad_norm: 7.54693391114003e-11
Step: 40100, train/learning_rate: 2.2846263618703233e-06
Step: 40100, train/epoch: 9.543074607849121
Step: 40110, train/loss: 0.0
Step: 40110, train/grad_norm: 6.231273914636404e-07
Step: 40110, train/learning_rate: 2.2727272153133526e-06
Step: 40110, train/epoch: 9.545454978942871
Step: 40120, train/loss: 0.0
Step: 40120, train/grad_norm: 6.2196157946115e-11
Step: 40120, train/learning_rate: 2.260828068756382e-06
Step: 40120, train/epoch: 9.547834396362305
Step: 40130, train/loss: 0.0
Step: 40130, train/grad_norm: 3.7679181996708166e-10
Step: 40130, train/learning_rate: 2.2489291495730868e-06
Step: 40130, train/epoch: 9.550213813781738
Step: 40140, train/loss: 0.0
Step: 40140, train/grad_norm: 9.055129090995706e-09
Step: 40140, train/learning_rate: 2.237030003016116e-06
Step: 40140, train/epoch: 9.552594184875488
Step: 40150, train/loss: 0.0
Step: 40150, train/grad_norm: 5.296820204669928e-10
Step: 40150, train/learning_rate: 2.2251308564591454e-06
Step: 40150, train/epoch: 9.554973602294922
Step: 40160, train/loss: 0.0
Step: 40160, train/grad_norm: 8.442223586868636e-10
Step: 40160, train/learning_rate: 2.2132317099021748e-06
Step: 40160, train/epoch: 9.557353973388672
Step: 40170, train/loss: 0.0
Step: 40170, train/grad_norm: 1.7496578408326968e-08
Step: 40170, train/learning_rate: 2.2013327907188796e-06
Step: 40170, train/epoch: 9.559733390808105
Step: 40180, train/loss: 0.0
Step: 40180, train/grad_norm: 1.854015940772058e-09
Step: 40180, train/learning_rate: 2.189433644161909e-06
Step: 40180, train/epoch: 9.562112808227539
Step: 40190, train/loss: 0.0
Step: 40190, train/grad_norm: 1.3621362718319041e-11
Step: 40190, train/learning_rate: 2.1775344976049382e-06
Step: 40190, train/epoch: 9.564493179321289
Step: 40200, train/loss: 0.0
Step: 40200, train/grad_norm: 4.329433079403033e-12
Step: 40200, train/learning_rate: 2.1656353510479676e-06
Step: 40200, train/epoch: 9.566872596740723
Step: 40210, train/loss: 0.0
Step: 40210, train/grad_norm: 2.9892369135531283e-10
Step: 40210, train/learning_rate: 2.153736204490997e-06
Step: 40210, train/epoch: 9.569252967834473
Step: 40220, train/loss: 0.0
Step: 40220, train/grad_norm: 1.1896969631663978e-08
Step: 40220, train/learning_rate: 2.1418372853077017e-06
Step: 40220, train/epoch: 9.571632385253906
Step: 40230, train/loss: 0.0
Step: 40230, train/grad_norm: 3.7722741597079334e-10
Step: 40230, train/learning_rate: 2.129938138750731e-06
Step: 40230, train/epoch: 9.574012756347656
Step: 40240, train/loss: 0.0
Step: 40240, train/grad_norm: 1.0133449634963654e-09
Step: 40240, train/learning_rate: 2.1180389921937604e-06
Step: 40240, train/epoch: 9.57639217376709
Step: 40250, train/loss: 0.0
Step: 40250, train/grad_norm: 1.3501547969918537e-11
Step: 40250, train/learning_rate: 2.1061398456367897e-06
Step: 40250, train/epoch: 9.578771591186523
Step: 40260, train/loss: 0.0
Step: 40260, train/grad_norm: 2.440643444723972e-13
Step: 40260, train/learning_rate: 2.0942409264534945e-06
Step: 40260, train/epoch: 9.581151962280273
Step: 40270, train/loss: 0.0
Step: 40270, train/grad_norm: 9.075097118227404e-09
Step: 40270, train/learning_rate: 2.082341779896524e-06
Step: 40270, train/epoch: 9.583531379699707
Step: 40280, train/loss: 0.0
Step: 40280, train/grad_norm: 4.380152351934896e-10
Step: 40280, train/learning_rate: 2.070442633339553e-06
Step: 40280, train/epoch: 9.585911750793457
Step: 40290, train/loss: 0.0
Step: 40290, train/grad_norm: 3.504518275576629e-08
Step: 40290, train/learning_rate: 2.0585434867825825e-06
Step: 40290, train/epoch: 9.58829116821289
Step: 40300, train/loss: 0.0
Step: 40300, train/grad_norm: 9.87083303982672e-09
Step: 40300, train/learning_rate: 2.0466445675992873e-06
Step: 40300, train/epoch: 9.59067153930664
Step: 40310, train/loss: 0.0
Step: 40310, train/grad_norm: 2.6181858903129296e-09
Step: 40310, train/learning_rate: 2.0347454210423166e-06
Step: 40310, train/epoch: 9.593050956726074
Step: 40320, train/loss: 0.0
Step: 40320, train/grad_norm: 6.658476880971875e-09
Step: 40320, train/learning_rate: 2.022846274485346e-06
Step: 40320, train/epoch: 9.595430374145508
Step: 40330, train/loss: 0.0
Step: 40330, train/grad_norm: 8.85223272462099e-08
Step: 40330, train/learning_rate: 2.0109471279283753e-06
Step: 40330, train/epoch: 9.597810745239258
Step: 40340, train/loss: 0.0
Step: 40340, train/grad_norm: 5.625612822801607e-13
Step: 40340, train/learning_rate: 1.9990479813714046e-06
Step: 40340, train/epoch: 9.600190162658691
Step: 40350, train/loss: 0.0
Step: 40350, train/grad_norm: 2.1567275743095138e-10
Step: 40350, train/learning_rate: 1.9871490621881094e-06
Step: 40350, train/epoch: 9.602570533752441
Step: 40360, train/loss: 0.0
Step: 40360, train/grad_norm: 1.2948125904799213e-10
Step: 40360, train/learning_rate: 1.9752499156311387e-06
Step: 40360, train/epoch: 9.604949951171875
Step: 40370, train/loss: 0.0
Step: 40370, train/grad_norm: 1.6059689134095834e-09
Step: 40370, train/learning_rate: 1.963350769074168e-06
Step: 40370, train/epoch: 9.607329368591309
Step: 40380, train/loss: 0.0
Step: 40380, train/grad_norm: 4.034314982775955e-12
Step: 40380, train/learning_rate: 1.9514516225171974e-06
Step: 40380, train/epoch: 9.609709739685059
Step: 40390, train/loss: 0.0
Step: 40390, train/grad_norm: 4.076470716540825e-09
Step: 40390, train/learning_rate: 1.939552703333902e-06
Step: 40390, train/epoch: 9.612089157104492
Step: 40400, train/loss: 0.0
Step: 40400, train/grad_norm: 1.748794109523999e-09
Step: 40400, train/learning_rate: 1.9276535567769315e-06
Step: 40400, train/epoch: 9.614469528198242
Step: 40410, train/loss: 0.0
Step: 40410, train/grad_norm: 6.9264698143722825e-12
Step: 40410, train/learning_rate: 1.915754410219961e-06
Step: 40410, train/epoch: 9.616848945617676
Step: 40420, train/loss: 0.0
Step: 40420, train/grad_norm: 4.159985290269619e-12
Step: 40420, train/learning_rate: 1.9038552636629902e-06
Step: 40420, train/epoch: 9.619229316711426
Step: 40430, train/loss: 0.0
Step: 40430, train/grad_norm: 6.395227229727407e-13
Step: 40430, train/learning_rate: 1.8919562307928572e-06
Step: 40430, train/epoch: 9.62160873413086
Step: 40440, train/loss: 0.0
Step: 40440, train/grad_norm: 1.4178028751032912e-09
Step: 40440, train/learning_rate: 1.8800570842358866e-06
Step: 40440, train/epoch: 9.623988151550293
Step: 40450, train/loss: 0.0
Step: 40450, train/grad_norm: 1.0293655034257498e-12
Step: 40450, train/learning_rate: 1.8681580513657536e-06
Step: 40450, train/epoch: 9.626368522644043
Step: 40460, train/loss: 0.0
Step: 40460, train/grad_norm: 1.6627512084213536e-12
Step: 40460, train/learning_rate: 1.856258904808783e-06
Step: 40460, train/epoch: 9.628747940063477
Step: 40470, train/loss: 0.0
Step: 40470, train/grad_norm: 3.9134109841576503e-10
Step: 40470, train/learning_rate: 1.84435987193865e-06
Step: 40470, train/epoch: 9.631128311157227
Step: 40480, train/loss: 0.0
Step: 40480, train/grad_norm: 2.7968905403491506e-12
Step: 40480, train/learning_rate: 1.8324607253816794e-06
Step: 40480, train/epoch: 9.63350772857666
Step: 40490, train/loss: 0.0
Step: 40490, train/grad_norm: 4.0114306321115123e-10
Step: 40490, train/learning_rate: 1.8205616925115464e-06
Step: 40490, train/epoch: 9.63588809967041
Step: 40500, train/loss: 0.0
Step: 40500, train/grad_norm: 1.2274173899484708e-09
Step: 40500, train/learning_rate: 1.8086625459545758e-06
Step: 40500, train/epoch: 9.638267517089844
Step: 40510, train/loss: 0.0
Step: 40510, train/grad_norm: 1.4900665235018096e-07
Step: 40510, train/learning_rate: 1.796763399397605e-06
Step: 40510, train/epoch: 9.640646934509277
Step: 40520, train/loss: 0.0
Step: 40520, train/grad_norm: 1.1186734047777258e-12
Step: 40520, train/learning_rate: 1.7848643665274722e-06
Step: 40520, train/epoch: 9.643027305603027
Step: 40530, train/loss: 0.0
Step: 40530, train/grad_norm: 1.5208601045202386e-10
Step: 40530, train/learning_rate: 1.7729652199705015e-06
Step: 40530, train/epoch: 9.645406723022461
Step: 40540, train/loss: 0.0
Step: 40540, train/grad_norm: 1.712379571472411e-09
Step: 40540, train/learning_rate: 1.7610661871003686e-06
Step: 40540, train/epoch: 9.647787094116211
Step: 40550, train/loss: 0.0
Step: 40550, train/grad_norm: 6.880934710906672e-11
Step: 40550, train/learning_rate: 1.7491670405433979e-06
Step: 40550, train/epoch: 9.650166511535645
Step: 40560, train/loss: 0.0
Step: 40560, train/grad_norm: 1.80928545887582e-05
Step: 40560, train/learning_rate: 1.737268007673265e-06
Step: 40560, train/epoch: 9.652546882629395
Step: 40570, train/loss: 0.0
Step: 40570, train/grad_norm: 8.020399622221674e-11
Step: 40570, train/learning_rate: 1.7253688611162943e-06
Step: 40570, train/epoch: 9.654926300048828
Step: 40580, train/loss: 0.0
Step: 40580, train/grad_norm: 2.5466956543596098e-08
Step: 40580, train/learning_rate: 1.7134698282461613e-06
Step: 40580, train/epoch: 9.657305717468262
Step: 40590, train/loss: 0.0
Step: 40590, train/grad_norm: 1.1628016258202756e-11
Step: 40590, train/learning_rate: 1.7015706816891907e-06
Step: 40590, train/epoch: 9.659686088562012
Step: 40600, train/loss: 0.0
Step: 40600, train/grad_norm: 1.0023942564885147e-08
Step: 40600, train/learning_rate: 1.68967153513222e-06
Step: 40600, train/epoch: 9.662065505981445
Step: 40610, train/loss: 0.0
Step: 40610, train/grad_norm: 3.4286760930424265e-11
Step: 40610, train/learning_rate: 1.677772502262087e-06
Step: 40610, train/epoch: 9.664445877075195
Step: 40620, train/loss: 0.0
Step: 40620, train/grad_norm: 1.0307396713926664e-08
Step: 40620, train/learning_rate: 1.6658733557051164e-06
Step: 40620, train/epoch: 9.666825294494629
Step: 40630, train/loss: 0.0
Step: 40630, train/grad_norm: 1.2064128585009826e-09
Step: 40630, train/learning_rate: 1.6539743228349835e-06
Step: 40630, train/epoch: 9.669204711914062
Step: 40640, train/loss: 0.0
Step: 40640, train/grad_norm: 7.295995474443373e-10
Step: 40640, train/learning_rate: 1.6420751762780128e-06
Step: 40640, train/epoch: 9.671585083007812
Step: 40650, train/loss: 0.0
Step: 40650, train/grad_norm: 1.1938267215072518e-12
Step: 40650, train/learning_rate: 1.6301761434078799e-06
Step: 40650, train/epoch: 9.673964500427246
Step: 40660, train/loss: 0.0
Step: 40660, train/grad_norm: 1.4267600989548157e-10
Step: 40660, train/learning_rate: 1.6182769968509092e-06
Step: 40660, train/epoch: 9.676344871520996
Step: 40670, train/loss: 0.0
Step: 40670, train/grad_norm: 2.987994851544329e-10
Step: 40670, train/learning_rate: 1.6063779639807763e-06
Step: 40670, train/epoch: 9.67872428894043
Step: 40680, train/loss: 0.0
Step: 40680, train/grad_norm: 4.0878824769663424e-09
Step: 40680, train/learning_rate: 1.5944788174238056e-06
Step: 40680, train/epoch: 9.68110466003418
Step: 40690, train/loss: 0.0
Step: 40690, train/grad_norm: 5.389140800282632e-10
Step: 40690, train/learning_rate: 1.582579670866835e-06
Step: 40690, train/epoch: 9.683484077453613
Step: 40700, train/loss: 0.0
Step: 40700, train/grad_norm: 6.572012378747161e-11
Step: 40700, train/learning_rate: 1.570680637996702e-06
Step: 40700, train/epoch: 9.685863494873047
Step: 40710, train/loss: 0.0
Step: 40710, train/grad_norm: 5.933096680621475e-09
Step: 40710, train/learning_rate: 1.5587814914397313e-06
Step: 40710, train/epoch: 9.688243865966797
Step: 40720, train/loss: 0.0
Step: 40720, train/grad_norm: 1.4290234794100964e-11
Step: 40720, train/learning_rate: 1.5468824585695984e-06
Step: 40720, train/epoch: 9.69062328338623
Step: 40730, train/loss: 0.0
Step: 40730, train/grad_norm: 6.431370991322183e-08
Step: 40730, train/learning_rate: 1.5349833120126277e-06
Step: 40730, train/epoch: 9.69300365447998
Step: 40740, train/loss: 0.0
Step: 40740, train/grad_norm: 8.963151332253005e-10
Step: 40740, train/learning_rate: 1.5230842791424948e-06
Step: 40740, train/epoch: 9.695383071899414
Step: 40750, train/loss: 0.0
Step: 40750, train/grad_norm: 1.551381662023843e-10
Step: 40750, train/learning_rate: 1.5111851325855241e-06
Step: 40750, train/epoch: 9.697763442993164
Step: 40760, train/loss: 0.0
Step: 40760, train/grad_norm: 1.8022264791639486e-09
Step: 40760, train/learning_rate: 1.4992860997153912e-06
Step: 40760, train/epoch: 9.700142860412598
Step: 40770, train/loss: 0.0
Step: 40770, train/grad_norm: 1.490353738857475e-12
Step: 40770, train/learning_rate: 1.4873869531584205e-06
Step: 40770, train/epoch: 9.702522277832031
Step: 40780, train/loss: 0.0
Step: 40780, train/grad_norm: 1.879160826945281e-08
Step: 40780, train/learning_rate: 1.4754878066014498e-06
Step: 40780, train/epoch: 9.704902648925781
Step: 40790, train/loss: 0.0
Step: 40790, train/grad_norm: 1.6003174480028193e-11
Step: 40790, train/learning_rate: 1.463588773731317e-06
Step: 40790, train/epoch: 9.707282066345215
Step: 40800, train/loss: 0.0
Step: 40800, train/grad_norm: 1.060588838974752e-09
Step: 40800, train/learning_rate: 1.4516896271743462e-06
Step: 40800, train/epoch: 9.709662437438965
Step: 40810, train/loss: 0.0
Step: 40810, train/grad_norm: 1.7710997946007723e-10
Step: 40810, train/learning_rate: 1.4397905943042133e-06
Step: 40810, train/epoch: 9.712041854858398
Step: 40820, train/loss: 0.0
Step: 40820, train/grad_norm: 1.3430729861596546e-09
Step: 40820, train/learning_rate: 1.4278914477472426e-06
Step: 40820, train/epoch: 9.714421272277832
Step: 40830, train/loss: 0.0
Step: 40830, train/grad_norm: 9.945707590830466e-10
Step: 40830, train/learning_rate: 1.4159924148771097e-06
Step: 40830, train/epoch: 9.716801643371582
Step: 40840, train/loss: 0.0
Step: 40840, train/grad_norm: 6.915383266026254e-10
Step: 40840, train/learning_rate: 1.404093268320139e-06
Step: 40840, train/epoch: 9.719181060791016
Step: 40850, train/loss: 0.0
Step: 40850, train/grad_norm: 5.37907773878743e-11
Step: 40850, train/learning_rate: 1.392194235450006e-06
Step: 40850, train/epoch: 9.721561431884766
Step: 40860, train/loss: 0.0
Step: 40860, train/grad_norm: 4.5893319526590926e-12
Step: 40860, train/learning_rate: 1.3802950888930354e-06
Step: 40860, train/epoch: 9.7239408493042
Step: 40870, train/loss: 0.0
Step: 40870, train/grad_norm: 1.385770664885655e-10
Step: 40870, train/learning_rate: 1.3683960560229025e-06
Step: 40870, train/epoch: 9.72632122039795
Step: 40880, train/loss: 0.0
Step: 40880, train/grad_norm: 2.4680515964270455e-10
Step: 40880, train/learning_rate: 1.3564969094659318e-06
Step: 40880, train/epoch: 9.728700637817383
Step: 40890, train/loss: 0.0
Step: 40890, train/grad_norm: 5.705792727184189e-09
Step: 40890, train/learning_rate: 1.3445977629089612e-06
Step: 40890, train/epoch: 9.731080055236816
Step: 40900, train/loss: 0.0
Step: 40900, train/grad_norm: 1.3667746101564404e-10
Step: 40900, train/learning_rate: 1.3326987300388282e-06
Step: 40900, train/epoch: 9.733460426330566
Step: 40910, train/loss: 0.0
Step: 40910, train/grad_norm: 3.620504286996895e-12
Step: 40910, train/learning_rate: 1.3207995834818576e-06
Step: 40910, train/epoch: 9.73583984375
Step: 40920, train/loss: 0.0
Step: 40920, train/grad_norm: 1.8599010331854515e-08
Step: 40920, train/learning_rate: 1.3089005506117246e-06
Step: 40920, train/epoch: 9.73822021484375
Step: 40930, train/loss: 0.0
Step: 40930, train/grad_norm: 4.996532076972926e-09
Step: 40930, train/learning_rate: 1.297001404054754e-06
Step: 40930, train/epoch: 9.740599632263184
Step: 40940, train/loss: 0.0
Step: 40940, train/grad_norm: 7.359217590874323e-08
Step: 40940, train/learning_rate: 1.285102371184621e-06
Step: 40940, train/epoch: 9.742980003356934
Step: 40950, train/loss: 0.0
Step: 40950, train/grad_norm: 2.066095038111282e-11
Step: 40950, train/learning_rate: 1.2732032246276503e-06
Step: 40950, train/epoch: 9.745359420776367
Step: 40960, train/loss: 0.0
Step: 40960, train/grad_norm: 8.064492650061705e-12
Step: 40960, train/learning_rate: 1.2613041917575174e-06
Step: 40960, train/epoch: 9.7477388381958
Step: 40970, train/loss: 0.0
Step: 40970, train/grad_norm: 5.306312722552775e-09
Step: 40970, train/learning_rate: 1.2494050452005467e-06
Step: 40970, train/epoch: 9.75011920928955
Step: 40980, train/loss: 0.0
Step: 40980, train/grad_norm: 2.2229952989505364e-08
Step: 40980, train/learning_rate: 1.237505898643576e-06
Step: 40980, train/epoch: 9.752498626708984
Step: 40990, train/loss: 0.0
Step: 40990, train/grad_norm: 7.100030074980168e-08
Step: 40990, train/learning_rate: 1.2256068657734431e-06
Step: 40990, train/epoch: 9.754878997802734
Step: 41000, train/loss: 0.0
Step: 41000, train/grad_norm: 6.377298862603187e-13
Step: 41000, train/learning_rate: 1.2137077192164725e-06
Step: 41000, train/epoch: 9.757258415222168
Step: 41010, train/loss: 0.0
Step: 41010, train/grad_norm: 1.4515815632831774e-11
Step: 41010, train/learning_rate: 1.2018086863463395e-06
Step: 41010, train/epoch: 9.759637832641602
Step: 41020, train/loss: 0.0
Step: 41020, train/grad_norm: 1.3344582612262457e-06
Step: 41020, train/learning_rate: 1.1899095397893689e-06
Step: 41020, train/epoch: 9.762018203735352
Step: 41030, train/loss: 0.0
Step: 41030, train/grad_norm: 1.66654331223981e-07
Step: 41030, train/learning_rate: 1.178010506919236e-06
Step: 41030, train/epoch: 9.764397621154785
Step: 41040, train/loss: 0.0
Step: 41040, train/grad_norm: 3.5794174291226e-09
Step: 41040, train/learning_rate: 1.1661113603622653e-06
Step: 41040, train/epoch: 9.766777992248535
Step: 41050, train/loss: 0.0
Step: 41050, train/grad_norm: 7.650014510751646e-10
Step: 41050, train/learning_rate: 1.1542123274921323e-06
Step: 41050, train/epoch: 9.769157409667969
Step: 41060, train/loss: 0.0
Step: 41060, train/grad_norm: 1.5352848992122858e-09
Step: 41060, train/learning_rate: 1.1423131809351617e-06
Step: 41060, train/epoch: 9.771537780761719
Step: 41070, train/loss: 0.0
Step: 41070, train/grad_norm: 8.263605077996061e-12
Step: 41070, train/learning_rate: 1.130414034378191e-06
Step: 41070, train/epoch: 9.773917198181152
Step: 41080, train/loss: 0.0
Step: 41080, train/grad_norm: 1.542838990245543e-12
Step: 41080, train/learning_rate: 1.118515001508058e-06
Step: 41080, train/epoch: 9.776296615600586
Step: 41090, train/loss: 0.0
Step: 41090, train/grad_norm: 1.6391611679100748e-10
Step: 41090, train/learning_rate: 1.1066158549510874e-06
Step: 41090, train/epoch: 9.778676986694336
Step: 41100, train/loss: 0.0
Step: 41100, train/grad_norm: 3.5151704463887157e-11
Step: 41100, train/learning_rate: 1.0947168220809544e-06
Step: 41100, train/epoch: 9.78105640411377
Step: 41110, train/loss: 0.0
Step: 41110, train/grad_norm: 5.514505687642668e-07
Step: 41110, train/learning_rate: 1.0828176755239838e-06
Step: 41110, train/epoch: 9.78343677520752
Step: 41120, train/loss: 0.0
Step: 41120, train/grad_norm: 1.0938681210825862e-10
Step: 41120, train/learning_rate: 1.0709186426538508e-06
Step: 41120, train/epoch: 9.785816192626953
Step: 41130, train/loss: 0.0
Step: 41130, train/grad_norm: 1.6140684437875724e-13
Step: 41130, train/learning_rate: 1.0590194960968802e-06
Step: 41130, train/epoch: 9.788196563720703
Step: 41140, train/loss: 0.0
Step: 41140, train/grad_norm: 2.032153556541516e-09
Step: 41140, train/learning_rate: 1.0471204632267472e-06
Step: 41140, train/epoch: 9.790575981140137
Step: 41150, train/loss: 0.0
Step: 41150, train/grad_norm: 7.573394356086283e-08
Step: 41150, train/learning_rate: 1.0352213166697766e-06
Step: 41150, train/epoch: 9.79295539855957
Step: 41160, train/loss: 0.0
Step: 41160, train/grad_norm: 9.127225017446783e-12
Step: 41160, train/learning_rate: 1.0233222837996436e-06
Step: 41160, train/epoch: 9.79533576965332
Step: 41170, train/loss: 0.0
Step: 41170, train/grad_norm: 2.5117996571566437e-10
Step: 41170, train/learning_rate: 1.011423137242673e-06
Step: 41170, train/epoch: 9.797715187072754
Step: 41180, train/loss: 0.0
Step: 41180, train/grad_norm: 2.8538240835155465e-14
Step: 41180, train/learning_rate: 9.995239906857023e-07
Step: 41180, train/epoch: 9.800095558166504
Step: 41190, train/loss: 0.0
Step: 41190, train/grad_norm: 7.544099893397327e-12
Step: 41190, train/learning_rate: 9.876249578155694e-07
Step: 41190, train/epoch: 9.802474975585938
Step: 41200, train/loss: 0.0
Step: 41200, train/grad_norm: 1.7613312197628517e-10
Step: 41200, train/learning_rate: 9.757258112585987e-07
Step: 41200, train/epoch: 9.804854393005371
Step: 41210, train/loss: 0.0
Step: 41210, train/grad_norm: 1.0812470030430177e-08
Step: 41210, train/learning_rate: 9.638267783884658e-07
Step: 41210, train/epoch: 9.807234764099121
Step: 41220, train/loss: 0.0
Step: 41220, train/grad_norm: 1.572382675452122e-11
Step: 41220, train/learning_rate: 9.519276318314951e-07
Step: 41220, train/epoch: 9.809614181518555
Step: 41230, train/loss: 0.0
Step: 41230, train/grad_norm: 1.2921415326605512e-10
Step: 41230, train/learning_rate: 9.400285421179433e-07
Step: 41230, train/epoch: 9.811994552612305
Step: 41240, train/loss: 0.0
Step: 41240, train/grad_norm: 2.0348644644285763e-12
Step: 41240, train/learning_rate: 9.281294524043915e-07
Step: 41240, train/epoch: 9.814373970031738
Step: 41250, train/loss: 0.0
Step: 41250, train/grad_norm: 5.790534274296988e-09
Step: 41250, train/learning_rate: 9.162303626908397e-07
Step: 41250, train/epoch: 9.816754341125488
Step: 41260, train/loss: 0.0
Step: 41260, train/grad_norm: 5.334617838616273e-13
Step: 41260, train/learning_rate: 9.043312729772879e-07
Step: 41260, train/epoch: 9.819133758544922
Step: 41270, train/loss: 0.0
Step: 41270, train/grad_norm: 3.869633644626447e-07
Step: 41270, train/learning_rate: 8.924321832637361e-07
Step: 41270, train/epoch: 9.821513175964355
Step: 41280, train/loss: 0.0
Step: 41280, train/grad_norm: 1.7632012516699547e-10
Step: 41280, train/learning_rate: 8.805330935501843e-07
Step: 41280, train/epoch: 9.823893547058105
Step: 41290, train/loss: 0.0
Step: 41290, train/grad_norm: 4.0387571759170626e-11
Step: 41290, train/learning_rate: 8.686340038366325e-07
Step: 41290, train/epoch: 9.826272964477539
Step: 41300, train/loss: 0.0
Step: 41300, train/grad_norm: 9.366166731084036e-10
Step: 41300, train/learning_rate: 8.567349141230807e-07
Step: 41300, train/epoch: 9.828653335571289
Step: 41310, train/loss: 0.0
Step: 41310, train/grad_norm: 1.5806495001768894e-11
Step: 41310, train/learning_rate: 8.4483576756611e-07
Step: 41310, train/epoch: 9.831032752990723
Step: 41320, train/loss: 0.0
Step: 41320, train/grad_norm: 5.533704694471453e-08
Step: 41320, train/learning_rate: 8.329366778525582e-07
Step: 41320, train/epoch: 9.833413124084473
Step: 41330, train/loss: 0.0
Step: 41330, train/grad_norm: 9.580242155138308e-10
Step: 41330, train/learning_rate: 8.210375881390064e-07
Step: 41330, train/epoch: 9.835792541503906
Step: 41340, train/loss: 0.0
Step: 41340, train/grad_norm: 3.5583005431050196e-09
Step: 41340, train/learning_rate: 8.091384984254546e-07
Step: 41340, train/epoch: 9.83817195892334
Step: 41350, train/loss: 0.0
Step: 41350, train/grad_norm: 1.4981771032096525e-11
Step: 41350, train/learning_rate: 7.972394087119028e-07
Step: 41350, train/epoch: 9.84055233001709
Step: 41360, train/loss: 0.0
Step: 41360, train/grad_norm: 2.0012542734093586e-09
Step: 41360, train/learning_rate: 7.85340318998351e-07
Step: 41360, train/epoch: 9.842931747436523
Step: 41370, train/loss: 0.0
Step: 41370, train/grad_norm: 2.89599733083179e-11
Step: 41370, train/learning_rate: 7.734412292847992e-07
Step: 41370, train/epoch: 9.845312118530273
Step: 41380, train/loss: 0.0
Step: 41380, train/grad_norm: 1.3306241525290474e-11
Step: 41380, train/learning_rate: 7.615421395712474e-07
Step: 41380, train/epoch: 9.847691535949707
Step: 41390, train/loss: 0.0
Step: 41390, train/grad_norm: 1.5686114807067497e-07
Step: 41390, train/learning_rate: 7.496430498576956e-07
Step: 41390, train/epoch: 9.85007095336914
Step: 41400, train/loss: 0.0
Step: 41400, train/grad_norm: 9.501474607098714e-11
Step: 41400, train/learning_rate: 7.377439033007249e-07
Step: 41400, train/epoch: 9.85245132446289
Step: 41410, train/loss: 0.0
Step: 41410, train/grad_norm: 7.552013875367081e-11
Step: 41410, train/learning_rate: 7.258448135871731e-07
Step: 41410, train/epoch: 9.854830741882324
Step: 41420, train/loss: 0.0
Step: 41420, train/grad_norm: 7.340833496627397e-10
Step: 41420, train/learning_rate: 7.139457238736213e-07
Step: 41420, train/epoch: 9.857211112976074
Step: 41430, train/loss: 0.0
Step: 41430, train/grad_norm: 9.323160299334887e-12
Step: 41430, train/learning_rate: 7.020466341600695e-07
Step: 41430, train/epoch: 9.859590530395508
Step: 41440, train/loss: 0.0
Step: 41440, train/grad_norm: 1.2061117729555981e-11
Step: 41440, train/learning_rate: 6.901475444465177e-07
Step: 41440, train/epoch: 9.861970901489258
Step: 41450, train/loss: 0.0
Step: 41450, train/grad_norm: 1.1421003899558801e-11
Step: 41450, train/learning_rate: 6.782484547329659e-07
Step: 41450, train/epoch: 9.864350318908691
Step: 41460, train/loss: 0.0
Step: 41460, train/grad_norm: 2.719278358043198e-09
Step: 41460, train/learning_rate: 6.663493650194141e-07
Step: 41460, train/epoch: 9.866729736328125
Step: 41470, train/loss: 0.0
Step: 41470, train/grad_norm: 1.6978508876608345e-11
Step: 41470, train/learning_rate: 6.544502753058623e-07
Step: 41470, train/epoch: 9.869110107421875
Step: 41480, train/loss: 0.0
Step: 41480, train/grad_norm: 3.64528164598088e-11
Step: 41480, train/learning_rate: 6.425511855923105e-07
Step: 41480, train/epoch: 9.871489524841309
Step: 41490, train/loss: 0.0
Step: 41490, train/grad_norm: 3.129884962316254e-10
Step: 41490, train/learning_rate: 6.306520958787587e-07
Step: 41490, train/epoch: 9.873869895935059
Step: 41500, train/loss: 0.0
Step: 41500, train/grad_norm: 3.1844988451767975e-11
Step: 41500, train/learning_rate: 6.18752949321788e-07
Step: 41500, train/epoch: 9.876249313354492
Step: 41510, train/loss: 0.0
Step: 41510, train/grad_norm: 2.9580970919290683e-13
Step: 41510, train/learning_rate: 6.068538596082362e-07
Step: 41510, train/epoch: 9.878629684448242
Step: 41520, train/loss: 0.0
Step: 41520, train/grad_norm: 2.209911142969645e-09
Step: 41520, train/learning_rate: 5.949547698946844e-07
Step: 41520, train/epoch: 9.881009101867676
Step: 41530, train/loss: 0.0
Step: 41530, train/grad_norm: 4.557312394126711e-08
Step: 41530, train/learning_rate: 5.830556801811326e-07
Step: 41530, train/epoch: 9.88338851928711
Step: 41540, train/loss: 0.0
Step: 41540, train/grad_norm: 2.7457875383107844e-13
Step: 41540, train/learning_rate: 5.711565904675808e-07
Step: 41540, train/epoch: 9.88576889038086
Step: 41550, train/loss: 0.0
Step: 41550, train/grad_norm: 6.753721713825822e-12
Step: 41550, train/learning_rate: 5.59257500754029e-07
Step: 41550, train/epoch: 9.888148307800293
Step: 41560, train/loss: 0.0
Step: 41560, train/grad_norm: 3.737797626968131e-08
Step: 41560, train/learning_rate: 5.473584110404772e-07
Step: 41560, train/epoch: 9.890528678894043
Step: 41570, train/loss: 0.0
Step: 41570, train/grad_norm: 3.126929160046643e-09
Step: 41570, train/learning_rate: 5.354593213269254e-07
Step: 41570, train/epoch: 9.892908096313477
Step: 41580, train/loss: 0.0
Step: 41580, train/grad_norm: 2.3334983723088953e-08
Step: 41580, train/learning_rate: 5.235602316133736e-07
Step: 41580, train/epoch: 9.89528751373291
Step: 41590, train/loss: 0.0
Step: 41590, train/grad_norm: 7.61040175323302e-10
Step: 41590, train/learning_rate: 5.116611418998218e-07
Step: 41590, train/epoch: 9.89766788482666
Step: 41600, train/loss: 0.0
Step: 41600, train/grad_norm: 2.259485709643627e-09
Step: 41600, train/learning_rate: 4.997619953428512e-07
Step: 41600, train/epoch: 9.900047302246094
Step: 41610, train/loss: 0.0
Step: 41610, train/grad_norm: 9.408302262259305e-12
Step: 41610, train/learning_rate: 4.878629056292993e-07
Step: 41610, train/epoch: 9.902427673339844
Step: 41620, train/loss: 0.0
Step: 41620, train/grad_norm: 3.581093821480863e-08
Step: 41620, train/learning_rate: 4.7596381591574755e-07
Step: 41620, train/epoch: 9.904807090759277
Step: 41630, train/loss: 0.0
Step: 41630, train/grad_norm: 1.0547529960547308e-08
Step: 41630, train/learning_rate: 4.6406472620219574e-07
Step: 41630, train/epoch: 9.907187461853027
Step: 41640, train/loss: 0.0
Step: 41640, train/grad_norm: 5.991512286307454e-10
Step: 41640, train/learning_rate: 4.5216563648864394e-07
Step: 41640, train/epoch: 9.909566879272461
Step: 41650, train/loss: 0.0
Step: 41650, train/grad_norm: 9.360161257188082e-11
Step: 41650, train/learning_rate: 4.4026654677509214e-07
Step: 41650, train/epoch: 9.911946296691895
Step: 41660, train/loss: 0.0
Step: 41660, train/grad_norm: 3.9531988793584105e-11
Step: 41660, train/learning_rate: 4.2836745706154034e-07
Step: 41660, train/epoch: 9.914326667785645
Step: 41670, train/loss: 0.0
Step: 41670, train/grad_norm: 8.125799512426202e-12
Step: 41670, train/learning_rate: 4.164683389262791e-07
Step: 41670, train/epoch: 9.916706085205078
Step: 41680, train/loss: 0.0
Step: 41680, train/grad_norm: 1.2222989731469625e-08
Step: 41680, train/learning_rate: 4.045692492127273e-07
Step: 41680, train/epoch: 9.919086456298828
Step: 41690, train/loss: 0.0
Step: 41690, train/grad_norm: 3.387419511557965e-11
Step: 41690, train/learning_rate: 3.926701594991755e-07
Step: 41690, train/epoch: 9.921465873718262
Step: 41700, train/loss: 0.0
Step: 41700, train/grad_norm: 4.645166196137751e-10
Step: 41700, train/learning_rate: 3.807710697856237e-07
Step: 41700, train/epoch: 9.923846244812012
Step: 41710, train/loss: 0.0
Step: 41710, train/grad_norm: 2.7170277139276777e-10
Step: 41710, train/learning_rate: 3.6887195165036246e-07
Step: 41710, train/epoch: 9.926225662231445
Step: 41720, train/loss: 0.0
Step: 41720, train/grad_norm: 1.513917768924955e-09
Step: 41720, train/learning_rate: 3.5697286193681066e-07
Step: 41720, train/epoch: 9.928605079650879
Step: 41730, train/loss: 0.0
Step: 41730, train/grad_norm: 1.794235870988814e-08
Step: 41730, train/learning_rate: 3.4507377222325886e-07
Step: 41730, train/epoch: 9.930985450744629
Step: 41740, train/loss: 0.0
Step: 41740, train/grad_norm: 1.1719790693476284e-09
Step: 41740, train/learning_rate: 3.3317468250970705e-07
Step: 41740, train/epoch: 9.933364868164062
Step: 41750, train/loss: 0.0
Step: 41750, train/grad_norm: 3.2773052934670366e-12
Step: 41750, train/learning_rate: 3.2127559279615525e-07
Step: 41750, train/epoch: 9.935745239257812
Step: 41760, train/loss: 0.0
Step: 41760, train/grad_norm: 6.558198872852472e-09
Step: 41760, train/learning_rate: 3.09376474660894e-07
Step: 41760, train/epoch: 9.938124656677246
Step: 41770, train/loss: 0.0
Step: 41770, train/grad_norm: 7.229786214146827e-11
Step: 41770, train/learning_rate: 2.974773849473422e-07
Step: 41770, train/epoch: 9.94050407409668
Step: 41780, train/loss: 0.0
Step: 41780, train/grad_norm: 2.2126685900181187e-11
Step: 41780, train/learning_rate: 2.855782952337904e-07
Step: 41780, train/epoch: 9.94288444519043
Step: 41790, train/loss: 0.0
Step: 41790, train/grad_norm: 1.3082445349255067e-08
Step: 41790, train/learning_rate: 2.736792055202386e-07
Step: 41790, train/epoch: 9.945263862609863
Step: 41800, train/loss: 0.0
Step: 41800, train/grad_norm: 9.308907067406835e-08
Step: 41800, train/learning_rate: 2.617801158066868e-07
Step: 41800, train/epoch: 9.947644233703613
Step: 41810, train/loss: 0.0
Step: 41810, train/grad_norm: 6.771116445314362e-10
Step: 41810, train/learning_rate: 2.498809976714256e-07
Step: 41810, train/epoch: 9.950023651123047
Step: 41820, train/loss: 0.0
Step: 41820, train/grad_norm: 6.708067434857412e-11
Step: 41820, train/learning_rate: 2.3798190795787377e-07
Step: 41820, train/epoch: 9.952404022216797
Step: 41830, train/loss: 0.0
Step: 41830, train/grad_norm: 6.810562780401597e-09
Step: 41830, train/learning_rate: 2.2608281824432197e-07
Step: 41830, train/epoch: 9.95478343963623
Step: 41840, train/loss: 0.0
Step: 41840, train/grad_norm: 3.7214561987575223e-10
Step: 41840, train/learning_rate: 2.1418372853077017e-07
Step: 41840, train/epoch: 9.957162857055664
Step: 41850, train/loss: 0.0
Step: 41850, train/grad_norm: 3.7873282288103383e-10
Step: 41850, train/learning_rate: 2.0228462460636365e-07
Step: 41850, train/epoch: 9.959543228149414
Step: 41860, train/loss: 0.0
Step: 41860, train/grad_norm: 1.298991136877703e-08
Step: 41860, train/learning_rate: 1.9038553489281185e-07
Step: 41860, train/epoch: 9.961922645568848
Step: 41870, train/loss: 0.0
Step: 41870, train/grad_norm: 5.875631231555545e-11
Step: 41870, train/learning_rate: 1.7848643096840533e-07
Step: 41870, train/epoch: 9.964303016662598
Step: 41880, train/loss: 0.0
Step: 41880, train/grad_norm: 6.430020960124239e-10
Step: 41880, train/learning_rate: 1.6658734125485353e-07
Step: 41880, train/epoch: 9.966682434082031
Step: 41890, train/loss: 0.0
Step: 41890, train/grad_norm: 1.610673226737358e-11
Step: 41890, train/learning_rate: 1.54688237330447e-07
Step: 41890, train/epoch: 9.969062805175781
Step: 41900, train/loss: 0.0
Step: 41900, train/grad_norm: 4.740828823263099e-12
Step: 41900, train/learning_rate: 1.427891476168952e-07
Step: 41900, train/epoch: 9.971442222595215
Step: 41910, train/loss: 0.0
Step: 41910, train/grad_norm: 9.457991681505185e-12
Step: 41910, train/learning_rate: 1.308900579033434e-07
Step: 41910, train/epoch: 9.973821640014648
Step: 41920, train/loss: 0.0
Step: 41920, train/grad_norm: 2.631324249203343e-13
Step: 41920, train/learning_rate: 1.1899095397893689e-07
Step: 41920, train/epoch: 9.976202011108398
Step: 41930, train/loss: 0.0
Step: 41930, train/grad_norm: 5.321718621331684e-09
Step: 41930, train/learning_rate: 1.0709186426538508e-07
Step: 41930, train/epoch: 9.978581428527832
Step: 41940, train/loss: 0.0
Step: 41940, train/grad_norm: 3.923417146722841e-12
Step: 41940, train/learning_rate: 9.519276744640592e-08
Step: 41940, train/epoch: 9.980961799621582
Step: 41950, train/loss: 0.0
Step: 41950, train/grad_norm: 2.5563484662427527e-10
Step: 41950, train/learning_rate: 8.329367062742676e-08
Step: 41950, train/epoch: 9.983341217041016
Step: 41960, train/loss: 0.0
Step: 41960, train/grad_norm: 7.424627579410981e-10
Step: 41960, train/learning_rate: 7.13945738084476e-08
Step: 41960, train/epoch: 9.98572063446045
Step: 41970, train/loss: 0.0
Step: 41970, train/grad_norm: 1.056787279907212e-08
Step: 41970, train/learning_rate: 5.949547698946844e-08
Step: 41970, train/epoch: 9.9881010055542
Step: 41980, train/loss: 0.0
Step: 41980, train/grad_norm: 9.546637258495139e-09
Step: 41980, train/learning_rate: 4.759638372320296e-08
Step: 41980, train/epoch: 9.990480422973633
Step: 41990, train/loss: 0.0
Step: 41990, train/grad_norm: 2.7140986347262697e-08
Step: 41990, train/learning_rate: 3.56972869042238e-08
Step: 41990, train/epoch: 9.992860794067383
Step: 42000, train/loss: 0.0
Step: 42000, train/grad_norm: 1.1596137383662608e-09
Step: 42000, train/learning_rate: 2.379819186160148e-08
Step: 42000, train/epoch: 9.995240211486816
Step: 42010, train/loss: 0.0
Step: 42010, train/grad_norm: 2.2969945390194368e-10
Step: 42010, train/learning_rate: 1.189909593080074e-08
Step: 42010, train/epoch: 9.997620582580566
Step: 42020, train/loss: 0.0
Step: 42020, train/grad_norm: 3.3595370409200687e-12
Step: 42020, train/learning_rate: 0.0
Step: 42020, train/epoch: 10.0
Step: 42020, eval/loss: 0.04517054930329323
Step: 42020, eval/accuracy: 0.9962515830993652
Step: 42020, eval/f1: 0.9960391521453857
Step: 42020, eval/runtime: 708.1640014648438
Step: 42020, eval/samples_per_second: 10.170999526977539
Step: 42020, eval/steps_per_second: 1.2719999551773071
Step: 42020, train/epoch: 10.0
Step: 42020, train/train_runtime: 118830.9453125
Step: 42020, train/train_samples_per_second: 2.828000068664551
Step: 42020, train/train_steps_per_second: 0.3540000021457672
Step: 42020, train/total_flos: 6.707599822305624e+18
Step: 42020, train/train_loss: 0.0137065714225173
Step: 42020, train/epoch: 10.0
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=6ffd0164-0f41-4bbe-b905-7dda371dfd20">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [14]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.summary.summary_iterator</span> <span class="kn">import</span> <span class="n">summary_iterator</span>

<span class="n">logs_directory</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">'./'</span><span class="p">,</span> <span class="n">project_name</span><span class="p">,</span> <span class="s1">'logs'</span><span class="p">)</span>
<span class="n">file_pattern</span> <span class="o">=</span> <span class="s1">'events.out.tfevents.*'</span>

<span class="n">event_files</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">logs_directory</span><span class="p">,</span> <span class="n">file_pattern</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">extract_metrics</span><span class="p">(</span><span class="n">event_files</span><span class="p">):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">last_train_loss</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">for</span> <span class="n">event_file</span> <span class="ow">in</span> <span class="n">event_files</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">summary_iterator</span><span class="p">(</span><span class="n">event_file</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">e</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">v</span><span class="o">.</span><span class="n">HasField</span><span class="p">(</span><span class="s1">'simple_value'</span><span class="p">):</span>
                    <span class="n">step</span> <span class="o">=</span> <span class="n">e</span><span class="o">.</span><span class="n">step</span>
                    <span class="n">metric_name</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">'/'</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                    <span class="n">metric_value</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">simple_value</span>

                    <span class="n">formatted_value</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">metric_value</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">"</span>

                    <span class="k">if</span> <span class="s1">'train/loss'</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="p">:</span>
                        <span class="n">last_train_loss</span> <span class="o">=</span> <span class="n">formatted_value</span>

                    <span class="k">if</span> <span class="s1">'eval'</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="p">:</span>
                        <span class="n">entry</span> <span class="o">=</span> <span class="nb">next</span><span class="p">((</span><span class="n">item</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">data</span> <span class="k">if</span> <span class="n">item</span><span class="p">[</span><span class="s1">'Step'</span><span class="p">]</span> <span class="o">==</span> <span class="n">step</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="n">entry</span><span class="p">:</span>
                            <span class="n">entry</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'Step'</span><span class="p">:</span> <span class="n">step</span><span class="p">,</span> <span class="s1">'Train Loss'</span><span class="p">:</span> <span class="n">last_train_loss</span><span class="p">,</span> <span class="s1">'Eval Loss'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">'Accuracy'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">'F1'</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>
                            <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">entry</span><span class="p">)</span>
                        <span class="k">if</span> <span class="s1">'loss'</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="p">:</span>
                            <span class="n">entry</span><span class="p">[</span><span class="s1">'Eval Loss'</span><span class="p">]</span> <span class="o">=</span> <span class="n">formatted_value</span>
                        <span class="k">elif</span> <span class="s1">'accuracy'</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="p">:</span>
                            <span class="n">entry</span><span class="p">[</span><span class="s1">'Accuracy'</span><span class="p">]</span> <span class="o">=</span> <span class="n">formatted_value</span>
                        <span class="k">elif</span> <span class="s1">'f1'</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="p">:</span>
                            <span class="n">entry</span><span class="p">[</span><span class="s1">'F1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">formatted_value</span>

    <span class="k">return</span> <span class="n">data</span>

<span class="n">metrics_data</span> <span class="o">=</span> <span class="n">extract_metrics</span><span class="p">(</span><span class="n">event_files</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">metrics_data</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">'Step'</span><span class="p">)</span>

<span class="n">file_path</span> <span class="o">=</span> <span class="s2">"./images/"</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s2">"_Checkpoint_Data.csv"</span>
<span class="n">df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>    Step Train Loss Eval Loss  Accuracy        F1
0   4202   0.000100  0.037788  0.992781  0.992374
1   8404   0.000000  0.044097  0.993891  0.993547
2  12606   0.000000  0.053543  0.994308  0.993995
3  16808   0.000000  0.041921  0.994308  0.993979
4  21010   0.000000  0.063095  0.993336  0.992945
5  25212   0.000700  0.038035  0.994586  0.994275
6  29414   0.000000  0.049486  0.995557  0.995302
7  33616   0.000000  0.046723  0.995696  0.995450
8  37818   0.000000  0.069512  0.995002  0.994713
9  42020   0.000000  0.045171  0.996252  0.996039
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=37cc5f92-d47f-4356-8fad-d9b64b6f5361">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [15]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">fillna</span><span class="p">({</span>
    <span class="s1">'Eval Loss'</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="s1">'inf'</span><span class="p">),</span>
    <span class="s1">'Accuracy'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">'F1'</span><span class="p">:</span> <span class="mi">0</span>
<span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">df</span><span class="p">[</span><span class="s1">'Eval Loss'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Eval Loss'</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'Accuracy'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Accuracy'</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'F1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'F1'</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>

<span class="n">df</span><span class="p">[</span><span class="s1">'Eval Loss Rank'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Eval Loss'</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'Accuracy Rank'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Accuracy'</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'F1 Rank'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'F1'</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">df</span><span class="p">[</span><span class="s1">'Rank Sum'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Eval Loss Rank'</span><span class="p">]</span> <span class="o">+</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Accuracy Rank'</span><span class="p">]</span> <span class="o">+</span> <span class="n">df</span><span class="p">[</span><span class="s1">'F1 Rank'</span><span class="p">]</span>

<span class="n">best_checkpoint</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">'Rank Sum'</span><span class="p">]</span><span class="o">.</span><span class="n">idxmin</span><span class="p">()]</span>

<span class="n">checkpoint_folder_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"checkpoint-</span><span class="si">{</span><span class="n">best_checkpoint</span><span class="p">[</span><span class="s1">'Step'</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Best Checkpoint Step: </span><span class="si">{</span><span class="n">checkpoint_folder_name</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">best_checkpoint</span><span class="p">[[</span><span class="s1">'Step'</span><span class="p">,</span> <span class="s1">'Train Loss'</span><span class="p">,</span> <span class="s1">'Eval Loss'</span><span class="p">,</span> <span class="s1">'Accuracy'</span><span class="p">,</span> <span class="s1">'F1'</span><span class="p">,</span> <span class="s1">'Rank Sum'</span><span class="p">]])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Best Checkpoint Step: checkpoint-42020
Step             42020
Train Loss    0.000000
Eval Loss     0.045171
Accuracy      0.996252
F1            0.996039
Rank Sum           7.0
Name: 9, dtype: object
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=5c3c3999-8c32-4a25-aaca-2ec9036b69ed">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Run-TensorBoard">Run TensorBoard<a class="anchor-link" href="#Run-TensorBoard">¶</a></h3><p>tensorboard --logdir=~/kuk/Praxis/praxis-Llama-2-7b-hf-small-finetune/logs --host=0.0.0.0</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=f8f36022-dc73-492f-998c-43e5ed1a6f46">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="PAUSE-SCRIPT">PAUSE SCRIPT<a class="anchor-link" href="#PAUSE-SCRIPT">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=444ae65b-241c-47ef-bbc2-81f3a2ce50e9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [26]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># My flag to pause the script, set to True to pause</span>
<span class="n">pause_script</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=c8be3672-68e0-44f8-b8b1-31290665658d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [27]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">StopExecution</span><span class="p">(</span><span class="ne">Exception</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">_render_traceback_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Script Paused"</span><span class="p">)</span>
        <span class="k">pass</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=eb3b5ed2-5320-49c0-9e61-90d6f0942f7a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [28]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="n">pause_script</span><span class="p">:</span>
    <span class="k">raise</span> <span class="n">StopExecution</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=19be6e26-d888-4da0-b3f8-836d68ac2051">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Testing">Testing<a class="anchor-link" href="#Testing">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=b5111194-5eeb-4104-8df0-f977a6b716e0">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [29]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>

<span class="n">bnb_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span>
    <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="s2">"nf4"</span><span class="p">,</span>
    <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
<span class="p">)</span>

<span class="n">base_model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">checkpoint</span><span class="p">,</span>
    <span class="n">quantization_config</span><span class="o">=</span><span class="n">bnb_config</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span>
    <span class="n">num_labels</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">token</span><span class="o">=</span><span class="n">access_token</span><span class="p">,</span>
    <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">eval_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">checkpoint</span><span class="p">,</span>
    <span class="n">token</span><span class="o">=</span><span class="n">access_token</span><span class="p">,</span>
    <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Loading checkpoint shards:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=530fa55f-c8c4-485f-a093-09bf2175d586">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [30]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">PeftModel</span>
<span class="n">test_checkpoint_name</span> <span class="o">=</span> <span class="n">checkpoint_folder_name</span>
<span class="n">ft_model</span> <span class="o">=</span> <span class="n">PeftModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model</span><span class="p">,</span> <span class="n">project_name</span><span class="o">+</span><span class="s1">'/'</span><span class="o">+</span><span class="n">test_checkpoint_name</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=27354c4f-95cc-4d1f-ac64-c5e6b4a842ae">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [31]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Using"</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">(),</span> <span class="s2">"GPUs!"</span><span class="p">)</span>
    <span class="n">ft_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">ft_model</span><span class="p">)</span>

<span class="n">ft_model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[31]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>PeftModelForSequenceClassification(
  (base_model): LoraModel(
    (model): LlamaForSequenceClassification(
      (model): LlamaModel(
        (embed_tokens): Embedding(32000, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaSdpaAttention(
              (q_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (v_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=11008, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (up_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=11008, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (down_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=11008, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=11008, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm()
            (post_attention_layernorm): LlamaRMSNorm()
          )
        )
        (norm): LlamaRMSNorm()
      )
      (score): ModulesToSaveWrapper(
        (original_module): Linear(in_features=4096, out_features=2, bias=False)
        (modules_to_save): ModuleDict(
          (default): Linear(in_features=4096, out_features=2, bias=False)
        )
      )
    )
  )
)</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=eee9ab6f-a7d5-4dd5-9436-2f6f6e2bffaf">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [32]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tqdm.auto</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="n">processed_predictions</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">tokenized_test_ds</span><span class="p">):</span>

        <span class="n">eval_prompt</span> <span class="o">=</span> <span class="n">record</span><span class="p">[</span><span class="s1">'article'</span><span class="p">]</span>
        <span class="n">model_input</span> <span class="o">=</span> <span class="n">tokenize_fn</span><span class="p">({</span><span class="s1">'article'</span><span class="p">:</span> <span class="n">eval_prompt</span><span class="p">})</span>

        <span class="c1"># model_input = {k: v.to('cuda') for k, v in model_input.items()}</span>
        
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">ft_model</span><span class="p">(</span><span class="o">**</span><span class="n">model_input</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>
        
        <span class="n">prediction</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>  <span class="c1"># Use .item() to get a Python number</span>
        <span class="n">processed_predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">prediction</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>  0%|          | 0/7203 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=80f06a2c-5ded-4da0-8232-145383967c9d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Accuracy-and-F1">Accuracy and F1<a class="anchor-link" href="#Accuracy-and-F1">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=6e891bc5-55e0-4c43-a035-0edc37dcb6e3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [33]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">true_articles</span> <span class="o">=</span> <span class="n">tokenized_test_ds</span><span class="p">[</span><span class="s1">'label'</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=6bf33824-d7cc-4d22-af4d-7d44e569c2b9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [34]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span><span class="p">,</span> <span class="n">accuracy_score</span>

<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">true_articles</span><span class="p">,</span> <span class="n">processed_predictions</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"accuracy:"</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>accuracy: 0.9966680549770929
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=9b37dc24-3bc0-48a0-ace9-a360bae91169">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [35]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">true_articles</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=a29ebf68-632f-49d1-b903-407ff05b5569">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [36]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">processed_predictions</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=fb4d8350-a8d1-4853-a9f8-74ae9ea36bde">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [37]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">f1_score</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">true_articles</span><span class="p">,</span> <span class="n">processed_predictions</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">'macro'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"f1_score:"</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>f1_score: 0.9964828242877024
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=2dcaac14-2b83-4c24-b83a-f6d0e73a2d2a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Confusion-Matrix">Confusion Matrix<a class="anchor-link" href="#Confusion-Matrix">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=57e64afb-e3ee-4c79-8228-b2d79806229e">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [38]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">ConfusionMatrixDisplay</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">tokenized_test_ds</span><span class="p">[</span><span class="s1">'label'</span><span class="p">],</span> <span class="n">processed_predictions</span><span class="p">)</span>

<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'human'</span><span class="p">,</span> <span class="s1">'machine'</span><span class="p">]</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="o">=</span><span class="n">cm</span><span class="p">,</span> <span class="n">display_labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Blues</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=af7d6f55-fb4f-4195-ac3f-93852ecd4b65">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [41]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">file_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">project_name</span><span class="si">}</span><span class="s2">-v12.ipynb"</span>
<span class="n">html_file_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">file_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">'.ipynb'</span><span class="p">,</span><span class="w"> </span><span class="s1">'.html'</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span>

<span class="n">command</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"jupyter nbconvert '</span><span class="si">{</span><span class="n">file_name</span><span class="si">}</span><span class="s2">' --to html --output-dir './html' --output '</span><span class="si">{</span><span class="n">html_file_name</span><span class="si">}</span><span class="s2">'"</span>
<span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">system</span><span class="p">(</span><span class="n">command</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[NbConvertApp] Converting notebook praxis-Llama-2-7b-hf-small-finetune-v10.ipynb to html
[NbConvertApp] WARNING | Alternative text is missing on 1 image(s).
[NbConvertApp] Writing 1274323 bytes to html/praxis-Llama-2-7b-hf-small-finetune-v10.html
</pre>
</div>
</div>
</div>
</div>
</div>
</main>
</body>
</html>
