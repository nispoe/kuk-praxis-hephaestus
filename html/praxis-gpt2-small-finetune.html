<!DOCTYPE html>

<html lang="en">
<head><meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>praxis-gpt2-small-finetune-v12</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<style type="text/css">
    pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: var(--jp-cell-editor-active-background) }
.highlight { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) }
.highlight .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */
.highlight .err { color: var(--jp-mirror-editor-error-color) } /* Error */
.highlight .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */
.highlight .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */
.highlight .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */
.highlight .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */
.highlight .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */
.highlight .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */
.highlight .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */
.highlight .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */
.highlight .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */
.highlight .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */
.highlight .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */
.highlight .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */
.highlight .pm { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation.Marker */
.highlight .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */
.highlight .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */
.highlight .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */
.highlight .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */
.highlight .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */
.highlight .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */
.highlight .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */
.highlight .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */
.highlight .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */
.highlight .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */
.highlight .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */
.highlight .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */
.highlight .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */
.highlight .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */
.highlight .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */
.highlight .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */
.highlight .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */
.highlight .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */
.highlight .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */
.highlight .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */
  </style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
 * Mozilla scrollbar styling
 */

/* use standard opaque scrollbars for most nodes */
[data-jp-theme-scrollbars='true'] {
  scrollbar-color: rgb(var(--jp-scrollbar-thumb-color))
    var(--jp-scrollbar-background-color);
}

/* for code nodes, use a transparent style of scrollbar. These selectors
 * will match lower in the tree, and so will override the above */
[data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar,
[data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
}

/* tiny scrollbar */

.jp-scrollbar-tiny {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
  scrollbar-width: thin;
}

/* tiny scrollbar */

.jp-scrollbar-tiny::-webkit-scrollbar,
.jp-scrollbar-tiny::-webkit-scrollbar-corner {
  background-color: transparent;
  height: 4px;
  width: 4px;
}

.jp-scrollbar-tiny::-webkit-scrollbar-thumb {
  background: rgba(var(--jp-scrollbar-thumb-color), 0.5);
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:horizontal {
  border-left: 0 solid transparent;
  border-right: 0 solid transparent;
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:vertical {
  border-top: 0 solid transparent;
  border-bottom: 0 solid transparent;
}

/*
 * Lumino
 */

.lm-ScrollBar[data-orientation='horizontal'] {
  min-height: 16px;
  max-height: 16px;
  min-width: 45px;
  border-top: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] {
  min-width: 16px;
  max-width: 16px;
  min-height: 45px;
  border-left: 1px solid #a0a0a0;
}

.lm-ScrollBar-button {
  background-color: #f0f0f0;
  background-position: center center;
  min-height: 15px;
  max-height: 15px;
  min-width: 15px;
  max-width: 15px;
}

.lm-ScrollBar-button:hover {
  background-color: #dadada;
}

.lm-ScrollBar-button.lm-mod-active {
  background-color: #cdcdcd;
}

.lm-ScrollBar-track {
  background: #f0f0f0;
}

.lm-ScrollBar-thumb {
  background: #cdcdcd;
}

.lm-ScrollBar-thumb:hover {
  background: #bababa;
}

.lm-ScrollBar-thumb.lm-mod-active {
  background: #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal'] .lm-ScrollBar-thumb {
  height: 100%;
  min-width: 15px;
  border-left: 1px solid #a0a0a0;
  border-right: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] .lm-ScrollBar-thumb {
  width: 100%;
  min-height: 15px;
  border-top: 1px solid #a0a0a0;
  border-bottom: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-left);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-right);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-up);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-down);
  background-size: 17px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Widget {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
}

.lm-Widget.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.lm-AccordionPanel[data-orientation='horizontal'] > .lm-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  display: flex;
  flex-direction: column;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-CommandPalette-search {
  flex: 0 0 auto;
}

.lm-CommandPalette-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  min-height: 0;
  overflow: auto;
  list-style-type: none;
}

.lm-CommandPalette-header {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-CommandPalette-item {
  display: flex;
  flex-direction: row;
}

.lm-CommandPalette-itemIcon {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemContent {
  flex: 1 1 auto;
  overflow: hidden;
}

.lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemLabel {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-close-icon {
  border: 1px solid transparent;
  background-color: transparent;
  position: absolute;
  z-index: 1;
  right: 3%;
  top: 0;
  bottom: 0;
  margin: auto;
  padding: 7px 0;
  display: none;
  vertical-align: middle;
  outline: 0;
  cursor: pointer;
}
.lm-close-icon:after {
  content: 'X';
  display: block;
  width: 15px;
  height: 15px;
  text-align: center;
  color: #000;
  font-weight: normal;
  font-size: 12px;
  cursor: pointer;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-DockPanel {
  z-index: 0;
}

.lm-DockPanel-widget {
  z-index: 0;
}

.lm-DockPanel-tabBar {
  z-index: 1;
}

.lm-DockPanel-handle {
  z-index: 2;
}

.lm-DockPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-DockPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-DockPanel-handle[data-orientation='horizontal'] {
  cursor: ew-resize;
}

.lm-DockPanel-handle[data-orientation='vertical'] {
  cursor: ns-resize;
}

.lm-DockPanel-handle[data-orientation='horizontal']:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-DockPanel-handle[data-orientation='vertical']:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

.lm-DockPanel-overlay {
  z-index: 3;
  box-sizing: border-box;
  pointer-events: none;
}

.lm-DockPanel-overlay.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Menu {
  z-index: 10000;
  position: absolute;
  white-space: nowrap;
  overflow-x: hidden;
  overflow-y: auto;
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-Menu-content {
  margin: 0;
  padding: 0;
  display: table;
  list-style-type: none;
}

.lm-Menu-item {
  display: table-row;
}

.lm-Menu-item.lm-mod-hidden,
.lm-Menu-item.lm-mod-collapsed {
  display: none !important;
}

.lm-Menu-itemIcon,
.lm-Menu-itemSubmenuIcon {
  display: table-cell;
  text-align: center;
}

.lm-Menu-itemLabel {
  display: table-cell;
  text-align: left;
}

.lm-Menu-itemShortcut {
  display: table-cell;
  text-align: right;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-MenuBar {
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-MenuBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex-direction: row;
  list-style-type: none;
}

.lm-MenuBar-item {
  box-sizing: border-box;
}

.lm-MenuBar-itemIcon,
.lm-MenuBar-itemLabel {
  display: inline-block;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-ScrollBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-ScrollBar[data-orientation='horizontal'] {
  flex-direction: row;
}

.lm-ScrollBar[data-orientation='vertical'] {
  flex-direction: column;
}

.lm-ScrollBar-button {
  box-sizing: border-box;
  flex: 0 0 auto;
}

.lm-ScrollBar-track {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
  flex: 1 1 auto;
}

.lm-ScrollBar-thumb {
  box-sizing: border-box;
  position: absolute;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-SplitPanel-child {
  z-index: 0;
}

.lm-SplitPanel-handle {
  z-index: 1;
}

.lm-SplitPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-SplitPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle {
  cursor: ew-resize;
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle {
  cursor: ns-resize;
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-TabBar[data-orientation='horizontal'] {
  flex-direction: row;
  align-items: flex-end;
}

.lm-TabBar[data-orientation='vertical'] {
  flex-direction: column;
  align-items: flex-end;
}

.lm-TabBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex: 1 1 auto;
  list-style-type: none;
}

.lm-TabBar[data-orientation='horizontal'] > .lm-TabBar-content {
  flex-direction: row;
}

.lm-TabBar[data-orientation='vertical'] > .lm-TabBar-content {
  flex-direction: column;
}

.lm-TabBar-tab {
  display: flex;
  flex-direction: row;
  box-sizing: border-box;
  overflow: hidden;
  touch-action: none; /* Disable native Drag/Drop */
}

.lm-TabBar-tabIcon,
.lm-TabBar-tabCloseIcon {
  flex: 0 0 auto;
}

.lm-TabBar-tabLabel {
  flex: 1 1 auto;
  overflow: hidden;
  white-space: nowrap;
}

.lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
}

.lm-TabBar-tab.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar-addButton.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab {
  position: relative;
}

.lm-TabBar.lm-mod-dragging[data-orientation='horizontal'] .lm-TabBar-tab {
  left: 0;
  transition: left 150ms ease;
}

.lm-TabBar.lm-mod-dragging[data-orientation='vertical'] .lm-TabBar-tab {
  top: 0;
  transition: top 150ms ease;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging {
  transition: none;
}

.lm-TabBar-tabLabel .lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
  background: inherit;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabPanel-tabBar {
  z-index: 1;
}

.lm-TabPanel-stackedPanel {
  z-index: 0;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapse {
  display: flex;
  flex-direction: column;
  align-items: stretch;
}

.jp-Collapse-header {
  padding: 1px 12px;
  background-color: var(--jp-layout-color1);
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  align-items: center;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  text-transform: uppercase;
  user-select: none;
}

.jp-Collapser-icon {
  height: 16px;
}

.jp-Collapse-header-collapsed .jp-Collapser-icon {
  transform: rotate(-90deg);
  margin: auto 0;
}

.jp-Collapser-title {
  line-height: 25px;
}

.jp-Collapse-contents {
  padding: 0 12px;
  background-color: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensureUiComponents() in @jupyterlab/buildutils */

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

/* Icons urls */

:root {
  --jp-icon-add-above: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5MikiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik00Ljc1IDQuOTMwNjZINi42MjVWNi44MDU2NkM2LjYyNSA3LjAxMTkxIDYuNzkzNzUgNy4xODA2NiA3IDcuMTgwNjZDNy4yMDYyNSA3LjE4MDY2IDcuMzc1IDcuMDExOTEgNy4zNzUgNi44MDU2NlY0LjkzMDY2SDkuMjVDOS40NTYyNSA0LjkzMDY2IDkuNjI1IDQuNzYxOTEgOS42MjUgNC41NTU2NkM5LjYyNSA0LjM0OTQxIDkuNDU2MjUgNC4xODA2NiA5LjI1IDQuMTgwNjZINy4zNzVWMi4zMDU2NkM3LjM3NSAyLjA5OTQxIDcuMjA2MjUgMS45MzA2NiA3IDEuOTMwNjZDNi43OTM3NSAxLjkzMDY2IDYuNjI1IDIuMDk5NDEgNi42MjUgMi4zMDU2NlY0LjE4MDY2SDQuNzVDNC41NDM3NSA0LjE4MDY2IDQuMzc1IDQuMzQ5NDEgNC4zNzUgNC41NTU2NkM0LjM3NSA0Ljc2MTkxIDQuNTQzNzUgNC45MzA2NiA0Ljc1IDQuOTMwNjZaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC43Ii8+CjwvZz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTExLjUgOS41VjExLjVMMi41IDExLjVWOS41TDExLjUgOS41Wk0xMiA4QzEyLjU1MjMgOCAxMyA4LjQ0NzcyIDEzIDlWMTJDMTMgMTIuNTUyMyAxMi41NTIzIDEzIDEyIDEzTDIgMTNDMS40NDc3MiAxMyAxIDEyLjU1MjMgMSAxMlY5QzEgOC40NDc3MiAxLjQ0NzcxIDggMiA4TDEyIDhaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5MiI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KC0xIDAgMCAxIDEwIDEuNTU1NjYpIi8+CjwvY2xpcFBhdGg+CjwvZGVmcz4KPC9zdmc+Cg==);
  --jp-icon-add-below: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5OCkiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik05LjI1IDEwLjA2OTNMNy4zNzUgMTAuMDY5M0w3LjM3NSA4LjE5NDM0QzcuMzc1IDcuOTg4MDkgNy4yMDYyNSA3LjgxOTM0IDcgNy44MTkzNEM2Ljc5Mzc1IDcuODE5MzQgNi42MjUgNy45ODgwOSA2LjYyNSA4LjE5NDM0TDYuNjI1IDEwLjA2OTNMNC43NSAxMC4wNjkzQzQuNTQzNzUgMTAuMDY5MyA0LjM3NSAxMC4yMzgxIDQuMzc1IDEwLjQ0NDNDNC4zNzUgMTAuNjUwNiA0LjU0Mzc1IDEwLjgxOTMgNC43NSAxMC44MTkzTDYuNjI1IDEwLjgxOTNMNi42MjUgMTIuNjk0M0M2LjYyNSAxMi45MDA2IDYuNzkzNzUgMTMuMDY5MyA3IDEzLjA2OTNDNy4yMDYyNSAxMy4wNjkzIDcuMzc1IDEyLjkwMDYgNy4zNzUgMTIuNjk0M0w3LjM3NSAxMC44MTkzTDkuMjUgMTAuODE5M0M5LjQ1NjI1IDEwLjgxOTMgOS42MjUgMTAuNjUwNiA5LjYyNSAxMC40NDQzQzkuNjI1IDEwLjIzODEgOS40NTYyNSAxMC4wNjkzIDkuMjUgMTAuMDY5M1oiIGZpbGw9IiM2MTYxNjEiIHN0cm9rZT0iIzYxNjE2MSIgc3Ryb2tlLXdpZHRoPSIwLjciLz4KPC9nPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMi41IDUuNUwyLjUgMy41TDExLjUgMy41TDExLjUgNS41TDIuNSA1LjVaTTIgN0MxLjQ0NzcyIDcgMSA2LjU1MjI4IDEgNkwxIDNDMSAyLjQ0NzcyIDEuNDQ3NzIgMiAyIDJMMTIgMkMxMi41NTIzIDIgMTMgMi40NDc3MiAxMyAzTDEzIDZDMTMgNi41NTIyOSAxMi41NTIzIDcgMTIgN0wyIDdaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5OCI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KDEgMS43NDg0NmUtMDcgMS43NDg0NmUtMDcgLTEgNCAxMy40NDQzKSIvPgo8L2NsaXBQYXRoPgo8L2RlZnM+Cjwvc3ZnPgo=);
  --jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bell: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE2IDE2IiB2ZXJzaW9uPSIxLjEiPgogICA8cGF0aCBjbGFzcz0ianAtaWNvbjIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMzMzMzMzIgogICAgICBkPSJtOCAwLjI5Yy0xLjQgMC0yLjcgMC43My0zLjYgMS44LTEuMiAxLjUtMS40IDMuNC0xLjUgNS4yLTAuMTggMi4yLTAuNDQgNC0yLjMgNS4zbDAuMjggMS4zaDVjMC4wMjYgMC42NiAwLjMyIDEuMSAwLjcxIDEuNSAwLjg0IDAuNjEgMiAwLjYxIDIuOCAwIDAuNTItMC40IDAuNi0xIDAuNzEtMS41aDVsMC4yOC0xLjNjLTEuOS0wLjk3LTIuMi0zLjMtMi4zLTUuMy0wLjEzLTEuOC0wLjI2LTMuNy0xLjUtNS4yLTAuODUtMS0yLjItMS44LTMuNi0xLjh6bTAgMS40YzAuODggMCAxLjkgMC41NSAyLjUgMS4zIDAuODggMS4xIDEuMSAyLjcgMS4yIDQuNCAwLjEzIDEuNyAwLjIzIDMuNiAxLjMgNS4yaC0xMGMxLjEtMS42IDEuMi0zLjQgMS4zLTUuMiAwLjEzLTEuNyAwLjMtMy4zIDEuMi00LjQgMC41OS0wLjcyIDEuNi0xLjMgMi41LTEuM3ptLTAuNzQgMTJoMS41Yy0wLjAwMTUgMC4yOCAwLjAxNSAwLjc5LTAuNzQgMC43OS0wLjczIDAuMDAxNi0wLjcyLTAuNTMtMC43NC0wLjc5eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-bug-dot: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiPgogICAgICAgIDxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMTcuMTkgOEgyMFYxMEgxNy45MUMxNy45NiAxMC4zMyAxOCAxMC42NiAxOCAxMVYxMkgyMFYxNEgxOC41SDE4VjE0LjAyNzVDMTUuNzUgMTQuMjc2MiAxNCAxNi4xODM3IDE0IDE4LjVDMTQgMTkuMjA4IDE0LjE2MzUgMTkuODc3OSAxNC40NTQ5IDIwLjQ3MzlDMTMuNzA2MyAyMC44MTE3IDEyLjg3NTcgMjEgMTIgMjFDOS43OCAyMSA3Ljg1IDE5Ljc5IDYuODEgMThINFYxNkg2LjA5QzYuMDQgMTUuNjcgNiAxNS4zNCA2IDE1VjE0SDRWMTJINlYxMUM2IDEwLjY2IDYuMDQgMTAuMzMgNi4wOSAxMEg0VjhINi44MUM3LjI2IDcuMjIgNy44OCA2LjU1IDguNjIgNi4wNEw3IDQuNDFMOC40MSAzTDEwLjU5IDUuMTdDMTEuMDQgNS4wNiAxMS41MSA1IDEyIDVDMTIuNDkgNSAxMi45NiA1LjA2IDEzLjQyIDUuMTdMMTUuNTkgM0wxNyA0LjQxTDE1LjM3IDYuMDRDMTYuMTIgNi41NSAxNi43NCA3LjIyIDE3LjE5IDhaTTEwIDE2SDE0VjE0SDEwVjE2Wk0xMCAxMkgxNFYxMEgxMFYxMloiIGZpbGw9IiM2MTYxNjEiLz4KICAgICAgICA8cGF0aCBkPSJNMjIgMTguNUMyMiAyMC40MzMgMjAuNDMzIDIyIDE4LjUgMjJDMTYuNTY3IDIyIDE1IDIwLjQzMyAxNSAxOC41QzE1IDE2LjU2NyAxNi41NjcgMTUgMTguNSAxNUMyMC40MzMgMTUgMjIgMTYuNTY3IDIyIDE4LjVaIiBmaWxsPSIjNjE2MTYxIi8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yMCA4aC0yLjgxYy0uNDUtLjc4LTEuMDctMS40NS0xLjgyLTEuOTZMMTcgNC40MSAxNS41OSAzbC0yLjE3IDIuMTdDMTIuOTYgNS4wNiAxMi40OSA1IDEyIDVjLS40OSAwLS45Ni4wNi0xLjQxLjE3TDguNDEgMyA3IDQuNDFsMS42MiAxLjYzQzcuODggNi41NSA3LjI2IDcuMjIgNi44MSA4SDR2MmgyLjA5Yy0uMDUuMzMtLjA5LjY2LS4wOSAxdjFINHYyaDJ2MWMwIC4zNC4wNC42Ny4wOSAxSDR2MmgyLjgxYzEuMDQgMS43OSAyLjk3IDMgNS4xOSAzczQuMTUtMS4yMSA1LjE5LTNIMjB2LTJoLTIuMDljLjA1LS4zMy4wOS0uNjYuMDktMXYtMWgydi0yaC0ydi0xYzAtLjM0LS4wNC0uNjctLjA5LTFIMjBWOHptLTYgOGgtNHYtMmg0djJ6bTAtNGgtNHYtMmg0djJ6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-build: url(data:image/svg+xml;base64,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);
  --jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik05IDE2LjE3TDQuODMgMTJsLTEuNDIgMS40MUw5IDE5IDIxIDdsLTEuNDEtMS40MXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBzaGFwZS1yZW5kZXJpbmc9Imdlb21ldHJpY1ByZWNpc2lvbiI+CiAgICA8cGF0aCBkPSJNNi41OSwzLjQxTDIsOEw2LjU5LDEyLjZMOCwxMS4xOEw0LjgyLDhMOCw0LjgyTDYuNTksMy40MU0xMi40MSwzLjQxTDExLDQuODJMMTQuMTgsOEwxMSwxMS4xOEwxMi40MSwxMi42TDE3LDhMMTIuNDEsMy40MU0yMS41OSwxMS41OUwxMy41LDE5LjY4TDkuODMsMTZMOC40MiwxNy40MUwxMy41LDIyLjVMMjMsMTNMMjEuNTksMTEuNTlaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTExLjQgMTguNkw2LjggMTRMMTEuNCA5LjRMMTAgOEw0IDE0TDEwIDIwTDExLjQgMTguNlpNMTYuNiAxOC42TDIxLjIgMTRMMTYuNiA5LjRMMTggOEwyNCAxNEwxOCAyMEwxNi42IDE4LjZWMTguNloiLz4KCTwvZz4KPC9zdmc+Cg==);
  --jp-icon-collapse-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNNiAxM3YyaDh2LTJ6IiAvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1jb25zb2xlLWljb24tYmFja2dyb3VuZC1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtY29uc29sZS1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIj4KICAgIDxwYXRoIGQ9Ik0xMDUgMTI3LjNoNDB2MTIuOGgtNDB6TTUxLjEgNzdMNzQgOTkuOWwtMjMuMyAyMy4zIDEwLjUgMTAuNSAyMy4zLTIzLjNMOTUgOTkuOSA4NC41IDg5LjQgNjEuNiA2Ni41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copyright: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGVuYWJsZS1iYWNrZ3JvdW5kPSJuZXcgMCAwIDI0IDI0IiBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCI+CiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0xMS44OCw5LjE0YzEuMjgsMC4wNiwxLjYxLDEuMTUsMS42MywxLjY2aDEuNzljLTAuMDgtMS45OC0xLjQ5LTMuMTktMy40NS0zLjE5QzkuNjQsNy42MSw4LDksOCwxMi4xNCBjMCwxLjk0LDAuOTMsNC4yNCwzLjg0LDQuMjRjMi4yMiwwLDMuNDEtMS42NSwzLjQ0LTIuOTVoLTEuNzljLTAuMDMsMC41OS0wLjQ1LDEuMzgtMS42MywxLjQ0QzEwLjU1LDE0LjgzLDEwLDEzLjgxLDEwLDEyLjE0IEMxMCw5LjI1LDExLjI4LDkuMTYsMTEuODgsOS4xNHogTTEyLDJDNi40OCwyLDIsNi40OCwyLDEyczQuNDgsMTAsMTAsMTBzMTAtNC40OCwxMC0xMFMxNy41MiwyLDEyLDJ6IE0xMiwyMGMtNC40MSwwLTgtMy41OS04LTggczMuNTktOCw4LThzOCwzLjU5LDgsOFMxNi40MSwyMCwxMiwyMHoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-delete: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2cHgiIGhlaWdodD0iMTZweCI+CiAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIiAvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjI2MjYyIiBkPSJNNiAxOWMwIDEuMS45IDIgMiAyaDhjMS4xIDAgMi0uOSAyLTJWN0g2djEyek0xOSA0aC0zLjVsLTEtMWgtNWwtMSAxSDV2MmgxNFY0eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-duplicate: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTIuNzk5OTggMC44NzVIOC44OTU4MkM5LjIwMDYxIDAuODc1IDkuNDQ5OTggMS4xMzkxNCA5LjQ0OTk4IDEuNDYxOThDOS40NDk5OCAxLjc4NDgyIDkuMjAwNjEgMi4wNDg5NiA4Ljg5NTgyIDIuMDQ4OTZIMy4zNTQxNUMzLjA0OTM2IDIuMDQ4OTYgMi43OTk5OCAyLjMxMzEgMi43OTk5OCAyLjYzNTk0VjkuNjc5NjlDMi43OTk5OCAxMC4wMDI1IDIuNTUwNjEgMTAuMjY2NyAyLjI0NTgyIDEwLjI2NjdDMS45NDEwMyAxMC4yNjY3IDEuNjkxNjUgMTAuMDAyNSAxLjY5MTY1IDkuNjc5NjlWMi4wNDg5NkMxLjY5MTY1IDEuNDAzMjggMi4xOTA0IDAuODc1IDIuNzk5OTggMC44NzVaTTUuMzY2NjUgMTEuOVY0LjU1SDExLjA4MzNWMTEuOUg1LjM2NjY1Wk00LjE0MTY1IDQuMTQxNjdDNC4xNDE2NSAzLjY5MDYzIDQuNTA3MjggMy4zMjUgNC45NTgzMiAzLjMyNUgxMS40OTE3QzExLjk0MjcgMy4zMjUgMTIuMzA4MyAzLjY5MDYzIDEyLjMwODMgNC4xNDE2N1YxMi4zMDgzQzEyLjMwODMgMTIuNzU5NCAxMS45NDI3IDEzLjEyNSAxMS40OTE3IDEzLjEyNUg0Ljk1ODMyQzQuNTA3MjggMTMuMTI1IDQuMTQxNjUgMTIuNzU5NCA0LjE0MTY1IDEyLjMwODNWNC4xNDE2N1oiIGZpbGw9IiM2MTYxNjEiLz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNOS40MzU3NCA4LjI2NTA3SDguMzY0MzFWOS4zMzY1QzguMzY0MzEgOS40NTQzNSA4LjI2Nzg4IDkuNTUwNzggOC4xNTAwMiA5LjU1MDc4QzguMDMyMTcgOS41NTA3OCA3LjkzNTc0IDkuNDU0MzUgNy45MzU3NCA5LjMzNjVWOC4yNjUwN0g2Ljg2NDMxQzYuNzQ2NDUgOC4yNjUwNyA2LjY1MDAyIDguMTY4NjQgNi42NTAwMiA4LjA1MDc4QzYuNjUwMDIgNy45MzI5MiA2Ljc0NjQ1IDcuODM2NSA2Ljg2NDMxIDcuODM2NUg3LjkzNTc0VjYuNzY1MDdDNy45MzU3NCA2LjY0NzIxIDguMDMyMTcgNi41NTA3OCA4LjE1MDAyIDYuNTUwNzhDOC4yNjc4OCA2LjU1MDc4IDguMzY0MzEgNi42NDcyMSA4LjM2NDMxIDYuNzY1MDdWNy44MzY1SDkuNDM1NzRDOS41NTM2IDcuODM2NSA5LjY1MDAyIDcuOTMyOTIgOS42NTAwMiA4LjA1MDc4QzkuNjUwMDIgOC4xNjg2NCA5LjU1MzYgOC4yNjUwNyA5LjQzNTc0IDguMjY1MDdaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC41Ii8+Cjwvc3ZnPgo=);
  --jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-error: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj48Y2lyY2xlIGN4PSIxMiIgY3k9IjE5IiByPSIyIi8+PHBhdGggZD0iTTEwIDNoNHYxMmgtNHoiLz48L2c+CjxwYXRoIGZpbGw9Im5vbmUiIGQ9Ik0wIDBoMjR2MjRIMHoiLz4KPC9zdmc+Cg==);
  --jp-icon-expand-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNMTEgMTBIOXYzSDZ2MmgzdjNoMnYtM2gzdi0yaC0zeiIgLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-filter-dot: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWRvdCIgZmlsbD0iI0ZGRiI+CiAgICA8Y2lyY2xlIGN4PSIxOCIgY3k9IjE3IiByPSIzIj48L2NpcmNsZT4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-filter: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-folder-favorite: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgwVjB6IiBmaWxsPSJub25lIi8+PHBhdGggY2xhc3M9ImpwLWljb24zIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzYxNjE2MSIgZD0iTTIwIDZoLThsLTItMkg0Yy0xLjEgMC0yIC45LTIgMnYxMmMwIDEuMS45IDIgMiAyaDE2YzEuMSAwIDItLjkgMi0yVjhjMC0xLjEtLjktMi0yLTJ6bS0yLjA2IDExTDE1IDE1LjI4IDEyLjA2IDE3bC43OC0zLjMzLTIuNTktMi4yNCAzLjQxLS4yOUwxNSA4bDEuMzQgMy4xNCAzLjQxLjI5LTIuNTkgMi4yNC43OCAzLjMzeiIvPgo8L3N2Zz4K);
  --jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-home: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPjxwYXRoIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xMCAyMHYtNmg0djZoNXYtOGgzTDEyIDMgMiAxMmgzdjh6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==);
  --jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-info: url(data:image/svg+xml;base64,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);
  --jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaW5zcGVjdG9yLWljb24tY29sb3IganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K);
  --jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtanNvbi1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0Y5QTgyNSI+CiAgICA8cGF0aCBkPSJNMjAuMiAxMS44Yy0xLjYgMC0xLjcuNS0xLjcgMSAwIC40LjEuOS4xIDEuMy4xLjUuMS45LjEgMS4zIDAgMS43LTEuNCAyLjMtMy41IDIuM2gtLjl2LTEuOWguNWMxLjEgMCAxLjQgMCAxLjQtLjggMC0uMyAwLS42LS4xLTEgMC0uNC0uMS0uOC0uMS0xLjIgMC0xLjMgMC0xLjggMS4zLTItMS4zLS4yLTEuMy0uNy0xLjMtMiAwLS40LjEtLjguMS0xLjIuMS0uNC4xLS43LjEtMSAwLS44LS40LS43LTEuNC0uOGgtLjVWNC4xaC45YzIuMiAwIDMuNS43IDMuNSAyLjMgMCAuNC0uMS45LS4xIDEuMy0uMS41LS4xLjktLjEgMS4zIDAgLjUuMiAxIDEuNyAxdjEuOHpNMS44IDEwLjFjMS42IDAgMS43LS41IDEuNy0xIDAtLjQtLjEtLjktLjEtMS4zLS4xLS41LS4xLS45LS4xLTEuMyAwLTEuNiAxLjQtMi4zIDMuNS0yLjNoLjl2MS45aC0uNWMtMSAwLTEuNCAwLTEuNC44IDAgLjMgMCAuNi4xIDEgMCAuMi4xLjYuMSAxIDAgMS4zIDAgMS44LTEuMyAyQzYgMTEuMiA2IDExLjcgNiAxM2MwIC40LS4xLjgtLjEgMS4yLS4xLjMtLjEuNy0uMSAxIDAgLjguMy44IDEuNC44aC41djEuOWgtLjljLTIuMSAwLTMuNS0uNi0zLjUtMi4zIDAtLjQuMS0uOS4xLTEuMy4xLS41LjEtLjkuMS0xLjMgMC0uNS0uMi0xLTEuNy0xdi0xLjl6Ii8+CiAgICA8Y2lyY2xlIGN4PSIxMSIgY3k9IjEzLjgiIHI9IjIuMSIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSI4LjIiIHI9IjIuMSIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-julia: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDMyNSAzMDAiPgogIDxnIGNsYXNzPSJqcC1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjY2IzYzMzIj4KICAgIDxwYXRoIGQ9Ik0gMTUwLjg5ODQzOCAyMjUgQyAxNTAuODk4NDM4IDI2Ni40MjE4NzUgMTE3LjMyMDMxMiAzMDAgNzUuODk4NDM4IDMwMCBDIDM0LjQ3NjU2MiAzMDAgMC44OTg0MzggMjY2LjQyMTg3NSAwLjg5ODQzOCAyMjUgQyAwLjg5ODQzOCAxODMuNTc4MTI1IDM0LjQ3NjU2MiAxNTAgNzUuODk4NDM4IDE1MCBDIDExNy4zMjAzMTIgMTUwIDE1MC44OTg0MzggMTgzLjU3ODEyNSAxNTAuODk4NDM4IDIyNSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzM4OTgyNiI+CiAgICA8cGF0aCBkPSJNIDIzNy41IDc1IEMgMjM3LjUgMTE2LjQyMTg3NSAyMDMuOTIxODc1IDE1MCAxNjIuNSAxNTAgQyAxMjEuMDc4MTI1IDE1MCA4Ny41IDExNi40MjE4NzUgODcuNSA3NSBDIDg3LjUgMzMuNTc4MTI1IDEyMS4wNzgxMjUgMCAxNjIuNSAwIEMgMjAzLjkyMTg3NSAwIDIzNy41IDMzLjU3ODEyNSAyMzcuNSA3NSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzk1NThiMiI+CiAgICA8cGF0aCBkPSJNIDMyNC4xMDE1NjIgMjI1IEMgMzI0LjEwMTU2MiAyNjYuNDIxODc1IDI5MC41MjM0MzggMzAwIDI0OS4xMDE1NjIgMzAwIEMgMjA3LjY3OTY4OCAzMDAgMTc0LjEwMTU2MiAyNjYuNDIxODc1IDE3NC4xMDE1NjIgMjI1IEMgMTc0LjEwMTU2MiAxODMuNTc4MTI1IDIwNy42Nzk2ODggMTUwIDI0OS4xMDE1NjIgMTUwIEMgMjkwLjUyMzQzOCAxNTAgMzI0LjEwMTU2MiAxODMuNTc4MTI1IDMyNC4xMDE1NjIgMjI1Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgPGcgY2xhc3M9ImpwLWp1cHl0ZXItaWNvbi1jb2xvciIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-jupyter: url(data:image/svg+xml;base64,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);
  --jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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);
  --jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K);
  --jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==);
  --jp-icon-launch: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMzIgMzIiIHdpZHRoPSIzMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yNiwyOEg2YTIuMDAyNywyLjAwMjcsMCwwLDEtMi0yVjZBMi4wMDI3LDIuMDAyNywwLDAsMSw2LDRIMTZWNkg2VjI2SDI2VjE2aDJWMjZBMi4wMDI3LDIuMDAyNywwLDAsMSwyNiwyOFoiLz4KICAgIDxwb2x5Z29uIHBvaW50cz0iMjAgMiAyMCA0IDI2LjU4NiA0IDE4IDEyLjU4NiAxOS40MTQgMTQgMjggNS40MTQgMjggMTIgMzAgMTIgMzAgMiAyMCAyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==);
  --jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4K);
  --jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-move-down: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMTIuNDcxIDcuNTI4OTlDMTIuNzYzMiA3LjIzNjg0IDEyLjc2MzIgNi43NjMxNiAxMi40NzEgNi40NzEwMVY2LjQ3MTAxQzEyLjE3OSA2LjE3OTA1IDExLjcwNTcgNi4xNzg4NCAxMS40MTM1IDYuNDcwNTRMNy43NSAxMC4xMjc1VjEuNzVDNy43NSAxLjMzNTc5IDcuNDE0MjEgMSA3IDFWMUM2LjU4NTc5IDEgNi4yNSAxLjMzNTc5IDYuMjUgMS43NVYxMC4xMjc1TDIuNTk3MjYgNi40NjgyMkMyLjMwMzM4IDYuMTczODEgMS44MjY0MSA2LjE3MzU5IDEuNTMyMjYgNi40Njc3NFY2LjQ2Nzc0QzEuMjM4MyA2Ljc2MTcgMS4yMzgzIDcuMjM4MyAxLjUzMjI2IDcuNTMyMjZMNi4yOTI4OSAxMi4yOTI5QzYuNjgzNDIgMTIuNjgzNCA3LjMxNjU4IDEyLjY4MzQgNy43MDcxMSAxMi4yOTI5TDEyLjQ3MSA3LjUyODk5WiIgZmlsbD0iIzYxNjE2MSIvPgo8L3N2Zz4K);
  --jp-icon-move-up: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMS41Mjg5OSA2LjQ3MTAxQzEuMjM2ODQgNi43NjMxNiAxLjIzNjg0IDcuMjM2ODQgMS41Mjg5OSA3LjUyODk5VjcuNTI4OTlDMS44MjA5NSA3LjgyMDk1IDIuMjk0MjYgNy44MjExNiAyLjU4NjQ5IDcuNTI5NDZMNi4yNSAzLjg3MjVWMTIuMjVDNi4yNSAxMi42NjQyIDYuNTg1NzkgMTMgNyAxM1YxM0M3LjQxNDIxIDEzIDcuNzUgMTIuNjY0MiA3Ljc1IDEyLjI1VjMuODcyNUwxMS40MDI3IDcuNTMxNzhDMTEuNjk2NiA3LjgyNjE5IDEyLjE3MzYgNy44MjY0MSAxMi40Njc3IDcuNTMyMjZWNy41MzIyNkMxMi43NjE3IDcuMjM4MyAxMi43NjE3IDYuNzYxNyAxMi40Njc3IDYuNDY3NzRMNy43MDcxMSAxLjcwNzExQzcuMzE2NTggMS4zMTY1OCA2LjY4MzQyIDEuMzE2NTggNi4yOTI4OSAxLjcwNzExTDEuNTI4OTkgNi40NzEwMVoiIGZpbGw9IiM2MTYxNjEiLz4KPC9zdmc+Cg==);
  --jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=);
  --jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtbm90ZWJvb2staWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-numbering: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTQgMTlINlYxOS41SDVWMjAuNUg2VjIxSDRWMjJIN1YxOEg0VjE5Wk01IDEwSDZWNkg0VjdINVYxMFpNNCAxM0g1LjhMNCAxNS4xVjE2SDdWMTVINS4yTDcgMTIuOVYxMkg0VjEzWk05IDdWOUgyM1Y3SDlaTTkgMjFIMjNWMTlIOVYyMVpNOSAxNUgyM1YxM0g5VjE1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-offline-bolt: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDIuMDJjLTUuNTEgMC05Ljk4IDQuNDctOS45OCA5Ljk4czQuNDcgOS45OCA5Ljk4IDkuOTggOS45OC00LjQ3IDkuOTgtOS45OFMxNy41MSAyLjAyIDEyIDIuMDJ6TTExLjQ4IDIwdi02LjI2SDhMMTMgNHY2LjI2aDMuMzVMMTEuNDggMjB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-pdf: url(data:image/svg+xml;base64,PHN2ZwogICB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyMiAyMiIgd2lkdGg9IjE2Ij4KICAgIDxwYXRoIHRyYW5zZm9ybT0icm90YXRlKDQ1KSIgY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0ZGMkEyQSIKICAgICAgIGQ9Im0gMjIuMzQ0MzY5LC0zLjAxNjM2NDIgaCA1LjYzODYwNCB2IDEuNTc5MjQzMyBoIC0zLjU0OTIyNyB2IDEuNTA4NjkyOTkgaCAzLjMzNzU3NiBWIDEuNjUwODE1NCBoIC0zLjMzNzU3NiB2IDMuNDM1MjYxMyBoIC0yLjA4OTM3NyB6IG0gLTcuMTM2NDQ0LDEuNTc5MjQzMyB2IDQuOTQzOTU0MyBoIDAuNzQ4OTIgcSAxLjI4MDc2MSwwIDEuOTUzNzAzLC0wLjYzNDk1MzUgMC42NzgzNjksLTAuNjM0OTUzNSAwLjY3ODM2OSwtMS44NDUxNjQxIDAsLTEuMjA0NzgzNTUgLTAuNjcyOTQyLC0xLjgzNDMxMDExIC0wLjY3Mjk0MiwtMC42Mjk1MjY1OSAtMS45NTkxMywtMC42Mjk1MjY1OSB6IG0gLTIuMDg5Mzc3LC0xLjU3OTI0MzMgaCAyLjIwMzM0MyBxIDEuODQ1MTY0LDAgMi43NDYwMzksMC4yNjU5MjA3IDAuOTA2MzAxLDAuMjYwNDkzNyAxLjU1MjEwOCwwLjg5MDAyMDMgMC41Njk4MywwLjU0ODEyMjMgMC44NDY2MDUsMS4yNjQ0ODAwNiAwLjI3Njc3NCwwLjcxNjM1NzgxIDAuMjc2Nzc0LDEuNjIyNjU4OTQgMCwwLjkxNzE1NTEgLTAuMjc2Nzc0LDEuNjM4OTM5OSAtMC4yNzY3NzUsMC43MTYzNTc4IC0wLjg0NjYwNSwxLjI2NDQ4IC0wLjY1MTIzNCwwLjYyOTUyNjYgLTEuNTYyOTYyLDAuODk1NDQ3MyAtMC45MTE3MjgsMC4yNjA0OTM3IC0yLjczNTE4NSwwLjI2MDQ5MzcgaCAtMi4yMDMzNDMgeiBtIC04LjE0NTg1NjUsMCBoIDMuNDY3ODIzIHEgMS41NDY2ODE2LDAgMi4zNzE1Nzg1LDAuNjg5MjIzIDAuODMwMzI0LDAuNjgzNzk2MSAwLjgzMDMyNCwxLjk1MzcwMzE0IDAsMS4yNzUzMzM5NyAtMC44MzAzMjQsMS45NjQ1NTcwNiBRIDkuOTg3MTk2MSwyLjI3NDkxNSA4LjQ0MDUxNDUsMi4yNzQ5MTUgSCA3LjA2MjA2ODQgViA1LjA4NjA3NjcgSCA0Ljk3MjY5MTUgWiBtIDIuMDg5Mzc2OSwxLjUxNDExOTkgdiAyLjI2MzAzOTQzIGggMS4xNTU5NDEgcSAwLjYwNzgxODgsMCAwLjkzODg2MjksLTAuMjkzMDU1NDcgMC4zMzEwNDQxLC0wLjI5ODQ4MjQxIDAuMzMxMDQ0MSwtMC44NDExNzc3MiAwLC0wLjU0MjY5NTMxIC0wLjMzMTA0NDEsLTAuODM1NzUwNzQgLTAuMzMxMDQ0MSwtMC4yOTMwNTU1IC0wLjkzODg2MjksLTAuMjkzMDU1NSB6IgovPgo8L3N2Zz4K);
  --jp-icon-python: url(data:image/svg+xml;base64,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);
  --jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==);
  --jp-icon-react: url(data:image/svg+xml;base64,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);
  --jp-icon-redo: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZD0iTTE4LjQgMTAuNkMxNi41NSA4Ljk5IDE0LjE1IDggMTEuNSA4Yy00LjY1IDAtOC41OCAzLjAzLTkuOTYgNy4yMkwzLjkgMTZjMS4wNS0zLjE5IDQuMDUtNS41IDcuNi01LjUgMS45NSAwIDMuNzMuNzIgNS4xMiAxLjg4TDEzIDE2aDlWN2wtMy42IDMuNnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==);
  --jp-icon-share: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTSAxOCAyIEMgMTYuMzU0OTkgMiAxNSAzLjM1NDk5MDQgMTUgNSBDIDE1IDUuMTkwOTUyOSAxNS4wMjE3OTEgNS4zNzcxMjI0IDE1LjA1NjY0MSA1LjU1ODU5MzggTCA3LjkyMTg3NSA5LjcyMDcwMzEgQyA3LjM5ODUzOTkgOS4yNzc4NTM5IDYuNzMyMDc3MSA5IDYgOSBDIDQuMzU0OTkwNCA5IDMgMTAuMzU0OTkgMyAxMiBDIDMgMTMuNjQ1MDEgNC4zNTQ5OTA0IDE1IDYgMTUgQyA2LjczMjA3NzEgMTUgNy4zOTg1Mzk5IDE0LjcyMjE0NiA3LjkyMTg3NSAxNC4yNzkyOTcgTCAxNS4wNTY2NDEgMTguNDM5NDUzIEMgMTUuMDIxNTU1IDE4LjYyMTUxNCAxNSAxOC44MDgzODYgMTUgMTkgQyAxNSAyMC42NDUwMSAxNi4zNTQ5OSAyMiAxOCAyMiBDIDE5LjY0NTAxIDIyIDIxIDIwLjY0NTAxIDIxIDE5IEMgMjEgMTcuMzU0OTkgMTkuNjQ1MDEgMTYgMTggMTYgQyAxNy4yNjc0OCAxNiAxNi42MDE1OTMgMTYuMjc5MzI4IDE2LjA3ODEyNSAxNi43MjI2NTYgTCA4Ljk0MzM1OTQgMTIuNTU4NTk0IEMgOC45NzgyMDk1IDEyLjM3NzEyMiA5IDEyLjE5MDk1MyA5IDEyIEMgOSAxMS44MDkwNDcgOC45NzgyMDk1IDExLjYyMjg3OCA4Ljk0MzM1OTQgMTEuNDQxNDA2IEwgMTYuMDc4MTI1IDcuMjc5Mjk2OSBDIDE2LjYwMTQ2IDcuNzIyMTQ2MSAxNy4yNjc5MjMgOCAxOCA4IEMgMTkuNjQ1MDEgOCAyMSA2LjY0NTAwOTYgMjEgNSBDIDIxIDMuMzU0OTkwNCAxOS42NDUwMSAyIDE4IDIgeiBNIDE4IDQgQyAxOC41NjQxMjkgNCAxOSA0LjQzNTg3MDYgMTkgNSBDIDE5IDUuNTY0MTI5NCAxOC41NjQxMjkgNiAxOCA2IEMgMTcuNDM1ODcxIDYgMTcgNS41NjQxMjk0IDE3IDUgQyAxNyA0LjQzNTg3MDYgMTcuNDM1ODcxIDQgMTggNCB6IE0gNiAxMSBDIDYuNTY0MTI5NCAxMSA3IDExLjQzNTg3MSA3IDEyIEMgNyAxMi41NjQxMjkgNi41NjQxMjk0IDEzIDYgMTMgQyA1LjQzNTg3MDYgMTMgNSAxMi41NjQxMjkgNSAxMiBDIDUgMTEuNDM1ODcxIDUuNDM1ODcwNiAxMSA2IDExIHogTSAxOCAxOCBDIDE4LjU2NDEyOSAxOCAxOSAxOC40MzU4NzEgMTkgMTkgQyAxOSAxOS41NjQxMjkgMTguNTY0MTI5IDIwIDE4IDIwIEMgMTcuNDM1ODcxIDIwIDE3IDE5LjU2NDEyOSAxNyAxOSBDIDE3IDE4LjQzNTg3MSAxNy40MzU4NzEgMTggMTggMTggeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-table-rows: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMSw4SDNWNGgxOFY4eiBNMjEsMTBIM3Y0aDE4VjEweiBNMjEsMTZIM3Y0aDE4VjE2eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-tag: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjgiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCA0MyAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTI4LjgzMzIgMTIuMzM0TDMyLjk5OTggMTYuNTAwN0wzNy4xNjY1IDEyLjMzNEgyOC44MzMyWiIvPgoJCTxwYXRoIGQ9Ik0xNi4yMDk1IDIxLjYxMDRDMTUuNjg3MyAyMi4xMjk5IDE0Ljg0NDMgMjIuMTI5OSAxNC4zMjQ4IDIxLjYxMDRMNi45ODI5IDE0LjcyNDVDNi41NzI0IDE0LjMzOTQgNi4wODMxMyAxMy42MDk4IDYuMDQ3ODYgMTMuMDQ4MkM1Ljk1MzQ3IDExLjUyODggNi4wMjAwMiA4LjYxOTQ0IDYuMDY2MjEgNy4wNzY5NUM2LjA4MjgxIDYuNTE0NzcgNi41NTU0OCA2LjA0MzQ3IDcuMTE4MDQgNi4wMzA1NUM5LjA4ODYzIDUuOTg0NzMgMTMuMjYzOCA1LjkzNTc5IDEzLjY1MTggNi4zMjQyNUwyMS43MzY5IDEzLjYzOUMyMi4yNTYgMTQuMTU4NSAyMS43ODUxIDE1LjQ3MjQgMjEuMjYyIDE1Ljk5NDZMMTYuMjA5NSAyMS42MTA0Wk05Ljc3NTg1IDguMjY1QzkuMzM1NTEgNy44MjU2NiA4LjYyMzUxIDcuODI1NjYgOC4xODI4IDguMjY1QzcuNzQzNDYgOC43MDU3MSA3Ljc0MzQ2IDkuNDE3MzMgOC4xODI4IDkuODU2NjdDOC42MjM4MiAxMC4yOTY0IDkuMzM1ODIgMTAuMjk2NCA5Ljc3NTg1IDkuODU2NjdDMTAuMjE1NiA5LjQxNzMzIDEwLjIxNTYgOC43MDUzMyA5Ljc3NTg1IDguMjY1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-terminal: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0IiA+CiAgICA8cmVjdCBjbGFzcz0ianAtdGVybWluYWwtaWNvbi1iYWNrZ3JvdW5kLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZSIgd2lkdGg9IjIwIiBoZWlnaHQ9IjIwIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgyIDIpIiBmaWxsPSIjMzMzMzMzIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtdGVybWluYWwtaWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUtaW52ZXJzZSIgZD0iTTUuMDU2NjQgOC43NjE3MkM1LjA1NjY0IDguNTk3NjYgNS4wMzEyNSA4LjQ1MzEyIDQuOTgwNDcgOC4zMjgxMkM0LjkzMzU5IDguMTk5MjIgNC44NTU0NyA4LjA4MjAzIDQuNzQ2MDkgNy45NzY1NkM0LjY0MDYyIDcuODcxMDkgNC41IDcuNzc1MzkgNC4zMjQyMiA3LjY4OTQ1QzQuMTUyMzQgNy41OTk2MSAzLjk0MzM2IDcuNTExNzIgMy42OTcyNyA3LjQyNTc4QzMuMzAyNzMgNy4yODUxNiAyLjk0MzM2IDcuMTM2NzIgMi42MTkxNCA2Ljk4MDQ3QzIuMjk0OTIgNi44MjQyMiAyLjAxNzU4IDYuNjQyNTggMS43ODcxMSA2LjQzNTU1QzEuNTYwNTUgNi4yMjg1MiAxLjM4NDc3IDUuOTg4MjggMS4yNTk3NyA1LjcxNDg0QzEuMTM0NzcgNS40Mzc1IDEuMDcyMjcgNS4xMDkzOCAxLjA3MjI3IDQuNzMwNDdDMS4wNzIyNyA0LjM5ODQ0IDEuMTI4OTEgNC4wOTU3IDEuMjQyMTkgMy44MjIyN0MxLjM1NTQ3IDMuNTQ0OTIgMS41MTU2MiAzLjMwNDY5IDEuNzIyNjYgMy4xMDE1NkMxLjkyOTY5IDIuODk4NDQgMi4xNzk2OSAyLjczNDM3IDIuNDcyNjYgMi42MDkzOEMyLjc2NTYyIDIuNDg0MzggMy4wOTE4IDIuNDA0MyAzLjQ1MTE3IDIuMzY5MTRWMS4xMDkzOEg0LjM4ODY3VjIuMzgwODZDNC43NDAyMyAyLjQyNzczIDUuMDU2NjQgMi41MjM0NCA1LjMzNzg5IDIuNjY3OTdDNS42MTkxNCAyLjgxMjUgNS44NTc0MiAzLjAwMTk1IDYuMDUyNzMgMy4yMzYzM0M2LjI1MTk1IDMuNDY2OCA2LjQwNDMgMy43NDAyMyA2LjUwOTc3IDQuMDU2NjRDNi42MTkxNCA0LjM2OTE0IDYuNjczODMgNC43MjA3IDYuNjczODMgNS4xMTEzM0g1LjA0NDkyQzUuMDQ0OTIgNC42Mzg2NyA0LjkzNzUgNC4yODEyNSA0LjcyMjY2IDQuMDM5MDZDNC41MDc4MSAzLjc5Mjk3IDQuMjE2OCAzLjY2OTkyIDMuODQ5NjEgMy42Njk5MkMzLjY1MDM5IDMuNjY5OTIgMy40NzY1NiAzLjY5NzI3IDMuMzI4MTIgMy43NTE5NUMzLjE4MzU5IDMuODAyNzMgMy4wNjQ0NSAzLjg3Njk1IDIuOTcwNyAzLjk3NDYxQzIuODc2OTUgNC4wNjgzNiAyLjgwNjY0IDQuMTc5NjkgMi43NTk3NyA0LjMwODU5QzIuNzE2OCA0LjQzNzUgMi42OTUzMSA0LjU3ODEyIDIuNjk1MzEgNC43MzA0N0MyLjY5NTMxIDQuODgyODEgMi43MTY4IDUuMDE5NTMgMi43NTk3NyA1LjE0MDYyQzIuODA2NjQgNS4yNTc4MSAyLjg4MjgxIDUuMzY3MTkgMi45ODgyOCA1LjQ2ODc1QzMuMDk3NjYgNS41NzAzMSAzLjI0MDIzIDUuNjY3OTcgMy40MTYwMiA1Ljc2MTcyQzMuNTkxOCA1Ljg1MTU2IDMuODEwNTUgNS45NDMzNiA0LjA3MjI3IDYuMDM3MTFDNC40NjY4IDYuMTg1NTUgNC44MjQyMiA2LjMzOTg0IDUuMTQ0NTMgNi41QzUuNDY0ODQgNi42NTYyNSA1LjczODI4IDYuODM5ODQgNS45NjQ4NCA3LjA1MDc4QzYuMTk1MzEgNy4yNTc4MSA2LjM3MTA5IDcuNSA2LjQ5MjE5IDcuNzc3MzRDNi42MTcxOSA4LjA1MDc4IDYuNjc5NjkgOC4zNzUgNi42Nzk2OSA4Ljc1QzYuNjc5NjkgOS4wOTM3NSA2LjYyMzA1IDkuNDA0MyA2LjUwOTc3IDkuNjgxNjRDNi4zOTY0OCA5Ljk1NTA4IDYuMjM0MzggMTAuMTkxNCA2LjAyMzQ0IDEwLjM5MDZDNS44MTI1IDEwLjU4OTggNS41NTg1OSAxMC43NSA1LjI2MTcyIDEwLjg3MTFDNC45NjQ4NCAxMC45ODgzIDQuNjMyODEgMTEuMDY0NSA0LjI2NTYyIDExLjA5OTZWMTIuMjQ4SDMuMzMzOThWMTEuMDk5NkMzLjAwMTk1IDExLjA2ODQgMi42Nzk2OSAxMC45OTYxIDIuMzY3MTkgMTAuODgyOEMyLjA1NDY5IDEwLjc2NTYgMS43NzczNCAxMC41OTc3IDEuNTM1MTYgMTAuMzc4OUMxLjI5Njg4IDEwLjE2MDIgMS4xMDU0NyA5Ljg4NDc3IDAuOTYwOTM4IDkuNTUyNzNDMC44MTY0MDYgOS4yMTY4IDAuNzQ0MTQxIDguODE0NDUgMC43NDQxNDEgOC4zNDU3SDIuMzc4OTFDMi4zNzg5MSA4LjYyNjk1IDIuNDE5OTIgOC44NjMyOCAyLjUwMTk1IDkuMDU0NjlDMi41ODM5OCA5LjI0MjE5IDIuNjg5NDUgOS4zOTI1OCAyLjgxODM2IDkuNTA1ODZDMi45NTExNyA5LjYxNTIzIDMuMTAxNTYgOS42OTMzNiAzLjI2OTUzIDkuNzQwMjNDMy40Mzc1IDkuNzg3MTEgMy42MDkzOCA5LjgxMDU1IDMuNzg1MTYgOS44MTA1NUM0LjIwMzEyIDkuODEwNTUgNC41MTk1MyA5LjcxMjg5IDQuNzM0MzggOS41MTc1OEM0Ljk0OTIyIDkuMzIyMjcgNS4wNTY2NCA5LjA3MDMxIDUuMDU2NjQgOC43NjE3MlpNMTMuNDE4IDEyLjI3MTVIOC4wNzQyMlYxMUgxMy40MThWMTIuMjcxNVoiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMuOTUyNjQgNikiIGZpbGw9IndoaXRlIi8+Cjwvc3ZnPgo=);
  --jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtdGV4dC1lZGl0b3ItaWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xNSAxNUgzdjJoMTJ2LTJ6bTAtOEgzdjJoMTJWN3pNMyAxM2gxOHYtMkgzdjJ6bTAgOGgxOHYtMkgzdjJ6TTMgM3YyaDE4VjNIM3oiLz4KPC9zdmc+Cg==);
  --jp-icon-toc: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik03LDVIMjFWN0g3VjVNNywxM1YxMUgyMVYxM0g3TTQsNC41QTEuNSwxLjUgMCAwLDEgNS41LDZBMS41LDEuNSAwIDAsMSA0LDcuNUExLjUsMS41IDAgMCwxIDIuNSw2QTEuNSwxLjUgMCAwLDEgNCw0LjVNNCwxMC41QTEuNSwxLjUgMCAwLDEgNS41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMy41QTEuNSwxLjUgMCAwLDEgMi41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMC41TTcsMTlWMTdIMjFWMTlIN000LDE2LjVBMS41LDEuNSAwIDAsMSA1LjUsMThBMS41LDEuNSAwIDAsMSA0LDE5LjVBMS41LDEuNSAwIDAsMSAyLjUsMThBMS41LDEuNSAwIDAsMSA0LDE2LjVaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tree-view: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMiAxMVYzaC03djNIOVYzSDJ2OGg3VjhoMnYxMGg0djNoN3YtOGgtN3YzaC0yVjhoMnYzeiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K);
  --jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-user: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE2IDdhNCA0IDAgMTEtOCAwIDQgNCAwIDAxOCAwek0xMiAxNGE3IDcgMCAwMC03IDdoMTRhNyA3IDAgMDAtNy03eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-users: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZlcnNpb249IjEuMSIgdmlld0JveD0iMCAwIDM2IDI0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogPGcgY2xhc3M9ImpwLWljb24zIiB0cmFuc2Zvcm09Im1hdHJpeCgxLjczMjcgMCAwIDEuNzMyNyAtMy42MjgyIC4wOTk1NzcpIiBmaWxsPSIjNjE2MTYxIj4KICA8cGF0aCB0cmFuc2Zvcm09Im1hdHJpeCgxLjUsMCwwLDEuNSwwLC02KSIgZD0ibTEyLjE4NiA3LjUwOThjLTEuMDUzNSAwLTEuOTc1NyAwLjU2NjUtMi40Nzg1IDEuNDEwMiAwLjc1MDYxIDAuMzEyNzcgMS4zOTc0IDAuODI2NDggMS44NzMgMS40NzI3aDMuNDg2M2MwLTEuNTkyLTEuMjg4OS0yLjg4MjgtMi44ODA5LTIuODgyOHoiLz4KICA8cGF0aCBkPSJtMjAuNDY1IDIuMzg5NWEyLjE4ODUgMi4xODg1IDAgMCAxLTIuMTg4NCAyLjE4ODUgMi4xODg1IDIuMTg4NSAwIDAgMS0yLjE4ODUtMi4xODg1IDIuMTg4NSAyLjE4ODUgMCAwIDEgMi4xODg1LTIuMTg4NSAyLjE4ODUgMi4xODg1IDAgMCAxIDIuMTg4NCAyLjE4ODV6Ii8+CiAgPHBhdGggdHJhbnNmb3JtPSJtYXRyaXgoMS41LDAsMCwxLjUsMCwtNikiIGQ9Im0zLjU4OTggOC40MjE5Yy0xLjExMjYgMC0yLjAxMzcgMC45MDExMS0yLjAxMzcgMi4wMTM3aDIuODE0NWMwLjI2Nzk3LTAuMzczMDkgMC41OTA3LTAuNzA0MzUgMC45NTg5OC0wLjk3ODUyLTAuMzQ0MzMtMC42MTY4OC0xLjAwMzEtMS4wMzUyLTEuNzU5OC0xLjAzNTJ6Ii8+CiAgPHBhdGggZD0ibTYuOTE1NCA0LjYyM2ExLjUyOTQgMS41Mjk0IDAgMCAxLTEuNTI5NCAxLjUyOTQgMS41Mjk0IDEuNTI5NCAwIDAgMS0xLjUyOTQtMS41Mjk0IDEuNTI5NCAxLjUyOTQgMCAwIDEgMS41Mjk0LTEuNTI5NCAxLjUyOTQgMS41Mjk0IDAgMCAxIDEuNTI5NCAxLjUyOTR6Ii8+CiAgPHBhdGggZD0ibTYuMTM1IDEzLjUzNWMwLTMuMjM5MiAyLjYyNTktNS44NjUgNS44NjUtNS44NjUgMy4yMzkyIDAgNS44NjUgMi42MjU5IDUuODY1IDUuODY1eiIvPgogIDxjaXJjbGUgY3g9IjEyIiBjeT0iMy43Njg1IiByPSIyLjk2ODUiLz4KIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-word: url(data:image/svg+xml;base64,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);
  --jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K);
}

/* Icon CSS class declarations */

.jp-AddAboveIcon {
  background-image: var(--jp-icon-add-above);
}

.jp-AddBelowIcon {
  background-image: var(--jp-icon-add-below);
}

.jp-AddIcon {
  background-image: var(--jp-icon-add);
}

.jp-BellIcon {
  background-image: var(--jp-icon-bell);
}

.jp-BugDotIcon {
  background-image: var(--jp-icon-bug-dot);
}

.jp-BugIcon {
  background-image: var(--jp-icon-bug);
}

.jp-BuildIcon {
  background-image: var(--jp-icon-build);
}

.jp-CaretDownEmptyIcon {
  background-image: var(--jp-icon-caret-down-empty);
}

.jp-CaretDownEmptyThinIcon {
  background-image: var(--jp-icon-caret-down-empty-thin);
}

.jp-CaretDownIcon {
  background-image: var(--jp-icon-caret-down);
}

.jp-CaretLeftIcon {
  background-image: var(--jp-icon-caret-left);
}

.jp-CaretRightIcon {
  background-image: var(--jp-icon-caret-right);
}

.jp-CaretUpEmptyThinIcon {
  background-image: var(--jp-icon-caret-up-empty-thin);
}

.jp-CaretUpIcon {
  background-image: var(--jp-icon-caret-up);
}

.jp-CaseSensitiveIcon {
  background-image: var(--jp-icon-case-sensitive);
}

.jp-CheckIcon {
  background-image: var(--jp-icon-check);
}

.jp-CircleEmptyIcon {
  background-image: var(--jp-icon-circle-empty);
}

.jp-CircleIcon {
  background-image: var(--jp-icon-circle);
}

.jp-ClearIcon {
  background-image: var(--jp-icon-clear);
}

.jp-CloseIcon {
  background-image: var(--jp-icon-close);
}

.jp-CodeCheckIcon {
  background-image: var(--jp-icon-code-check);
}

.jp-CodeIcon {
  background-image: var(--jp-icon-code);
}

.jp-CollapseAllIcon {
  background-image: var(--jp-icon-collapse-all);
}

.jp-ConsoleIcon {
  background-image: var(--jp-icon-console);
}

.jp-CopyIcon {
  background-image: var(--jp-icon-copy);
}

.jp-CopyrightIcon {
  background-image: var(--jp-icon-copyright);
}

.jp-CutIcon {
  background-image: var(--jp-icon-cut);
}

.jp-DeleteIcon {
  background-image: var(--jp-icon-delete);
}

.jp-DownloadIcon {
  background-image: var(--jp-icon-download);
}

.jp-DuplicateIcon {
  background-image: var(--jp-icon-duplicate);
}

.jp-EditIcon {
  background-image: var(--jp-icon-edit);
}

.jp-EllipsesIcon {
  background-image: var(--jp-icon-ellipses);
}

.jp-ErrorIcon {
  background-image: var(--jp-icon-error);
}

.jp-ExpandAllIcon {
  background-image: var(--jp-icon-expand-all);
}

.jp-ExtensionIcon {
  background-image: var(--jp-icon-extension);
}

.jp-FastForwardIcon {
  background-image: var(--jp-icon-fast-forward);
}

.jp-FileIcon {
  background-image: var(--jp-icon-file);
}

.jp-FileUploadIcon {
  background-image: var(--jp-icon-file-upload);
}

.jp-FilterDotIcon {
  background-image: var(--jp-icon-filter-dot);
}

.jp-FilterIcon {
  background-image: var(--jp-icon-filter);
}

.jp-FilterListIcon {
  background-image: var(--jp-icon-filter-list);
}

.jp-FolderFavoriteIcon {
  background-image: var(--jp-icon-folder-favorite);
}

.jp-FolderIcon {
  background-image: var(--jp-icon-folder);
}

.jp-HomeIcon {
  background-image: var(--jp-icon-home);
}

.jp-Html5Icon {
  background-image: var(--jp-icon-html5);
}

.jp-ImageIcon {
  background-image: var(--jp-icon-image);
}

.jp-InfoIcon {
  background-image: var(--jp-icon-info);
}

.jp-InspectorIcon {
  background-image: var(--jp-icon-inspector);
}

.jp-JsonIcon {
  background-image: var(--jp-icon-json);
}

.jp-JuliaIcon {
  background-image: var(--jp-icon-julia);
}

.jp-JupyterFaviconIcon {
  background-image: var(--jp-icon-jupyter-favicon);
}

.jp-JupyterIcon {
  background-image: var(--jp-icon-jupyter);
}

.jp-JupyterlabWordmarkIcon {
  background-image: var(--jp-icon-jupyterlab-wordmark);
}

.jp-KernelIcon {
  background-image: var(--jp-icon-kernel);
}

.jp-KeyboardIcon {
  background-image: var(--jp-icon-keyboard);
}

.jp-LaunchIcon {
  background-image: var(--jp-icon-launch);
}

.jp-LauncherIcon {
  background-image: var(--jp-icon-launcher);
}

.jp-LineFormIcon {
  background-image: var(--jp-icon-line-form);
}

.jp-LinkIcon {
  background-image: var(--jp-icon-link);
}

.jp-ListIcon {
  background-image: var(--jp-icon-list);
}

.jp-MarkdownIcon {
  background-image: var(--jp-icon-markdown);
}

.jp-MoveDownIcon {
  background-image: var(--jp-icon-move-down);
}

.jp-MoveUpIcon {
  background-image: var(--jp-icon-move-up);
}

.jp-NewFolderIcon {
  background-image: var(--jp-icon-new-folder);
}

.jp-NotTrustedIcon {
  background-image: var(--jp-icon-not-trusted);
}

.jp-NotebookIcon {
  background-image: var(--jp-icon-notebook);
}

.jp-NumberingIcon {
  background-image: var(--jp-icon-numbering);
}

.jp-OfflineBoltIcon {
  background-image: var(--jp-icon-offline-bolt);
}

.jp-PaletteIcon {
  background-image: var(--jp-icon-palette);
}

.jp-PasteIcon {
  background-image: var(--jp-icon-paste);
}

.jp-PdfIcon {
  background-image: var(--jp-icon-pdf);
}

.jp-PythonIcon {
  background-image: var(--jp-icon-python);
}

.jp-RKernelIcon {
  background-image: var(--jp-icon-r-kernel);
}

.jp-ReactIcon {
  background-image: var(--jp-icon-react);
}

.jp-RedoIcon {
  background-image: var(--jp-icon-redo);
}

.jp-RefreshIcon {
  background-image: var(--jp-icon-refresh);
}

.jp-RegexIcon {
  background-image: var(--jp-icon-regex);
}

.jp-RunIcon {
  background-image: var(--jp-icon-run);
}

.jp-RunningIcon {
  background-image: var(--jp-icon-running);
}

.jp-SaveIcon {
  background-image: var(--jp-icon-save);
}

.jp-SearchIcon {
  background-image: var(--jp-icon-search);
}

.jp-SettingsIcon {
  background-image: var(--jp-icon-settings);
}

.jp-ShareIcon {
  background-image: var(--jp-icon-share);
}

.jp-SpreadsheetIcon {
  background-image: var(--jp-icon-spreadsheet);
}

.jp-StopIcon {
  background-image: var(--jp-icon-stop);
}

.jp-TabIcon {
  background-image: var(--jp-icon-tab);
}

.jp-TableRowsIcon {
  background-image: var(--jp-icon-table-rows);
}

.jp-TagIcon {
  background-image: var(--jp-icon-tag);
}

.jp-TerminalIcon {
  background-image: var(--jp-icon-terminal);
}

.jp-TextEditorIcon {
  background-image: var(--jp-icon-text-editor);
}

.jp-TocIcon {
  background-image: var(--jp-icon-toc);
}

.jp-TreeViewIcon {
  background-image: var(--jp-icon-tree-view);
}

.jp-TrustedIcon {
  background-image: var(--jp-icon-trusted);
}

.jp-UndoIcon {
  background-image: var(--jp-icon-undo);
}

.jp-UserIcon {
  background-image: var(--jp-icon-user);
}

.jp-UsersIcon {
  background-image: var(--jp-icon-users);
}

.jp-VegaIcon {
  background-image: var(--jp-icon-vega);
}

.jp-WordIcon {
  background-image: var(--jp-icon-word);
}

.jp-YamlIcon {
  background-image: var(--jp-icon-yaml);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

.jp-Icon,
.jp-MaterialIcon {
  background-position: center;
  background-repeat: no-repeat;
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-cover {
  background-position: center;
  background-repeat: no-repeat;
  background-size: cover;
}

/**
 * (DEPRECATED) Support for specific CSS icon sizes
 */

.jp-Icon-16 {
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-18 {
  background-size: 18px;
  min-width: 18px;
  min-height: 18px;
}

.jp-Icon-20 {
  background-size: 20px;
  min-width: 20px;
  min-height: 20px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.lm-TabBar .lm-TabBar-addButton {
  align-items: center;
  display: flex;
  padding: 4px;
  padding-bottom: 5px;
  margin-right: 1px;
  background-color: var(--jp-layout-color2);
}

.lm-TabBar .lm-TabBar-addButton:hover {
  background-color: var(--jp-layout-color1);
}

.lm-DockPanel-tabBar .lm-TabBar-tab {
  width: var(--jp-private-horizontal-tab-width);
}

.lm-DockPanel-tabBar .lm-TabBar-content {
  flex: unset;
}

.lm-DockPanel-tabBar[data-orientation='horizontal'] {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for icons as inline SVG HTMLElements
 */

/* recolor the primary elements of an icon */
.jp-icon0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-accent0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-accent1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-accent2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-accent3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-accent4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-accent0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-accent1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-accent2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-accent3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-accent4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-none[fill] {
  fill: none;
}

.jp-icon-none[stroke] {
  stroke: none;
}

/* brand icon colors. Same for light and dark */
.jp-icon-brand0[fill] {
  fill: var(--jp-brand-color0);
}

.jp-icon-brand1[fill] {
  fill: var(--jp-brand-color1);
}

.jp-icon-brand2[fill] {
  fill: var(--jp-brand-color2);
}

.jp-icon-brand3[fill] {
  fill: var(--jp-brand-color3);
}

.jp-icon-brand4[fill] {
  fill: var(--jp-brand-color4);
}

.jp-icon-brand0[stroke] {
  stroke: var(--jp-brand-color0);
}

.jp-icon-brand1[stroke] {
  stroke: var(--jp-brand-color1);
}

.jp-icon-brand2[stroke] {
  stroke: var(--jp-brand-color2);
}

.jp-icon-brand3[stroke] {
  stroke: var(--jp-brand-color3);
}

.jp-icon-brand4[stroke] {
  stroke: var(--jp-brand-color4);
}

/* warn icon colors. Same for light and dark */
.jp-icon-warn0[fill] {
  fill: var(--jp-warn-color0);
}

.jp-icon-warn1[fill] {
  fill: var(--jp-warn-color1);
}

.jp-icon-warn2[fill] {
  fill: var(--jp-warn-color2);
}

.jp-icon-warn3[fill] {
  fill: var(--jp-warn-color3);
}

.jp-icon-warn0[stroke] {
  stroke: var(--jp-warn-color0);
}

.jp-icon-warn1[stroke] {
  stroke: var(--jp-warn-color1);
}

.jp-icon-warn2[stroke] {
  stroke: var(--jp-warn-color2);
}

.jp-icon-warn3[stroke] {
  stroke: var(--jp-warn-color3);
}

/* icon colors that contrast well with each other and most backgrounds */
.jp-icon-contrast0[fill] {
  fill: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[fill] {
  fill: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[fill] {
  fill: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[fill] {
  fill: var(--jp-icon-contrast-color3);
}

.jp-icon-contrast0[stroke] {
  stroke: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[stroke] {
  stroke: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[stroke] {
  stroke: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[stroke] {
  stroke: var(--jp-icon-contrast-color3);
}

.jp-icon-dot[fill] {
  fill: var(--jp-warn-color0);
}

.jp-jupyter-icon-color[fill] {
  fill: var(--jp-jupyter-icon-color, var(--jp-warn-color0));
}

.jp-notebook-icon-color[fill] {
  fill: var(--jp-notebook-icon-color, var(--jp-warn-color0));
}

.jp-json-icon-color[fill] {
  fill: var(--jp-json-icon-color, var(--jp-warn-color1));
}

.jp-console-icon-color[fill] {
  fill: var(--jp-console-icon-color, white);
}

.jp-console-icon-background-color[fill] {
  fill: var(--jp-console-icon-background-color, var(--jp-brand-color1));
}

.jp-terminal-icon-color[fill] {
  fill: var(--jp-terminal-icon-color, var(--jp-layout-color2));
}

.jp-terminal-icon-background-color[fill] {
  fill: var(
    --jp-terminal-icon-background-color,
    var(--jp-inverse-layout-color2)
  );
}

.jp-text-editor-icon-color[fill] {
  fill: var(--jp-text-editor-icon-color, var(--jp-inverse-layout-color3));
}

.jp-inspector-icon-color[fill] {
  fill: var(--jp-inspector-icon-color, var(--jp-inverse-layout-color3));
}

/* CSS for icons in selected filebrowser listing items */
.jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

.jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* stylelint-disable selector-max-class, selector-max-compound-selectors */

/**
* TODO: come up with non css-hack solution for showing the busy icon on top
*  of the close icon
* CSS for complex behavior of close icon of tabs in the main area tabbar
*/
.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon3[fill] {
  fill: none;
}

.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: var(--jp-inverse-layout-color3);
}

/* stylelint-enable selector-max-class, selector-max-compound-selectors */

/* CSS for icons in status bar */
#jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

#jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* special handling for splash icon CSS. While the theme CSS reloads during
   splash, the splash icon can loose theming. To prevent that, we set a
   default for its color variable */
:root {
  --jp-warn-color0: var(--md-orange-700);
}

/* not sure what to do with this one, used in filebrowser listing */
.jp-DragIcon {
  margin-right: 4px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for alt colors for icons as inline SVG HTMLElements
 */

/* alt recolor the primary elements of an icon */
.jp-icon-alt .jp-icon0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-alt .jp-icon0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* alt recolor the accent elements of an icon */
.jp-icon-alt .jp-icon-accent0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-alt .jp-icon-accent0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-icon-hoverShow:not(:hover) .jp-icon-hoverShow-content {
  display: none !important;
}

/**
 * Support for hover colors for icons as inline SVG HTMLElements
 */

/**
 * regular colors
 */

/* recolor the primary elements of an icon */
.jp-icon-hover :hover .jp-icon0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-hover :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-hover :hover .jp-icon-none-hover[fill] {
  fill: none;
}

.jp-icon-hover :hover .jp-icon-none-hover[stroke] {
  stroke: none;
}

/**
 * inverse colors
 */

/* inverse recolor the primary elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* inverse recolor the accent elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-IFrame {
  width: 100%;
  height: 100%;
}

.jp-IFrame > iframe {
  border: none;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-IFrame {
  position: relative;
}

body.lm-mod-override-cursor .jp-IFrame::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-HoverBox {
  position: fixed;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FormGroup-content fieldset {
  border: none;
  padding: 0;
  min-width: 0;
  width: 100%;
}

/* stylelint-disable selector-max-type */

.jp-FormGroup-content fieldset .jp-inputFieldWrapper input,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper select,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper textarea {
  font-size: var(--jp-content-font-size2);
  border-color: var(--jp-input-border-color);
  border-style: solid;
  border-radius: var(--jp-border-radius);
  border-width: 1px;
  padding: 6px 8px;
  background: none;
  color: var(--jp-ui-font-color0);
  height: inherit;
}

.jp-FormGroup-content fieldset input[type='checkbox'] {
  position: relative;
  top: 2px;
  margin-left: 0;
}

.jp-FormGroup-content button.jp-mod-styled {
  cursor: pointer;
}

.jp-FormGroup-content .checkbox label {
  cursor: pointer;
  font-size: var(--jp-content-font-size1);
}

.jp-FormGroup-content .jp-root > fieldset > legend {
  display: none;
}

.jp-FormGroup-content .jp-root > fieldset > p {
  display: none;
}

/** copy of `input.jp-mod-styled:focus` style */
.jp-FormGroup-content fieldset input:focus,
.jp-FormGroup-content fieldset select:focus {
  -moz-outline-radius: unset;
  outline: var(--jp-border-width) solid var(--md-blue-500);
  outline-offset: -1px;
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-FormGroup-content fieldset input:hover:not(:focus),
.jp-FormGroup-content fieldset select:hover:not(:focus) {
  background-color: var(--jp-border-color2);
}

/* stylelint-enable selector-max-type */

.jp-FormGroup-content .checkbox .field-description {
  /* Disable default description field for checkbox:
   because other widgets do not have description fields,
   we add descriptions to each widget on the field level.
  */
  display: none;
}

.jp-FormGroup-content #root__description {
  display: none;
}

.jp-FormGroup-content .jp-modifiedIndicator {
  width: 5px;
  background-color: var(--jp-brand-color2);
  margin-top: 0;
  margin-left: calc(var(--jp-private-settingeditor-modifier-indent) * -1);
  flex-shrink: 0;
}

.jp-FormGroup-content .jp-modifiedIndicator.jp-errorIndicator {
  background-color: var(--jp-error-color0);
  margin-right: 0.5em;
}

/* RJSF ARRAY style */

.jp-arrayFieldWrapper legend {
  font-size: var(--jp-content-font-size2);
  color: var(--jp-ui-font-color0);
  flex-basis: 100%;
  padding: 4px 0;
  font-weight: var(--jp-content-heading-font-weight);
  border-bottom: 1px solid var(--jp-border-color2);
}

.jp-arrayFieldWrapper .field-description {
  padding: 4px 0;
  white-space: pre-wrap;
}

.jp-arrayFieldWrapper .array-item {
  width: 100%;
  border: 1px solid var(--jp-border-color2);
  border-radius: 4px;
  margin: 4px;
}

.jp-ArrayOperations {
  display: flex;
  margin-left: 8px;
}

.jp-ArrayOperationsButton {
  margin: 2px;
}

.jp-ArrayOperationsButton .jp-icon3[fill] {
  fill: var(--jp-ui-font-color0);
}

button.jp-ArrayOperationsButton.jp-mod-styled:disabled {
  cursor: not-allowed;
  opacity: 0.5;
}

/* RJSF form validation error */

.jp-FormGroup-content .validationErrors {
  color: var(--jp-error-color0);
}

/* Hide panel level error as duplicated the field level error */
.jp-FormGroup-content .panel.errors {
  display: none;
}

/* RJSF normal content (settings-editor) */

.jp-FormGroup-contentNormal {
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-FormGroup-contentItem {
  margin-left: 7px;
  color: var(--jp-ui-font-color0);
}

.jp-FormGroup-contentNormal .jp-FormGroup-description {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-default {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-fieldLabel {
  font-size: var(--jp-content-font-size1);
  font-weight: normal;
  min-width: 120px;
}

.jp-FormGroup-contentNormal fieldset:not(:first-child) {
  margin-left: 7px;
}

.jp-FormGroup-contentNormal .field-array-of-string .array-item {
  /* Display `jp-ArrayOperations` buttons side-by-side with content except
    for small screens where flex-wrap will place them one below the other.
  */
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-objectFieldWrapper .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

/* RJSF compact content (metadata-form) */

.jp-FormGroup-content.jp-FormGroup-contentCompact {
  width: 100%;
}

.jp-FormGroup-contentCompact .form-group {
  display: flex;
  padding: 0.5em 0.2em 0.5em 0;
}

.jp-FormGroup-contentCompact
  .jp-FormGroup-compactTitle
  .jp-FormGroup-description {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color2);
}

.jp-FormGroup-contentCompact .jp-FormGroup-fieldLabel {
  padding-bottom: 0.3em;
}

.jp-FormGroup-contentCompact .jp-inputFieldWrapper .form-control {
  width: 100%;
  box-sizing: border-box;
}

.jp-FormGroup-contentCompact .jp-arrayFieldWrapper .jp-FormGroup-compactTitle {
  padding-bottom: 7px;
}

.jp-FormGroup-contentCompact
  .jp-objectFieldWrapper
  .jp-objectFieldWrapper
  .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

.jp-FormGroup-contentCompact ul.error-detail {
  margin-block-start: 0.5em;
  margin-block-end: 0.5em;
  padding-inline-start: 1em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-SidePanel {
  display: flex;
  flex-direction: column;
  min-width: var(--jp-sidebar-min-width);
  overflow-y: auto;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  font-size: var(--jp-ui-font-size1);
}

.jp-SidePanel-header {
  flex: 0 0 auto;
  display: flex;
  border-bottom: var(--jp-border-width) solid var(--jp-border-color2);
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin: 0;
  padding: 2px;
  text-transform: uppercase;
}

.jp-SidePanel-toolbar {
  flex: 0 0 auto;
}

.jp-SidePanel-content {
  flex: 1 1 auto;
}

.jp-SidePanel-toolbar,
.jp-AccordionPanel-toolbar {
  height: var(--jp-private-toolbar-height);
}

.jp-SidePanel-toolbar.jp-Toolbar-micro {
  display: none;
}

.lm-AccordionPanel .jp-AccordionPanel-title {
  box-sizing: border-box;
  line-height: 25px;
  margin: 0;
  display: flex;
  align-items: center;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  font-size: var(--jp-ui-font-size0);
}

.jp-AccordionPanel-title {
  cursor: pointer;
  user-select: none;
  -moz-user-select: none;
  -webkit-user-select: none;
  text-transform: uppercase;
}

.lm-AccordionPanel[data-orientation='horizontal'] > .jp-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleLabel {
  user-select: none;
  text-overflow: ellipsis;
  white-space: nowrap;
  overflow: hidden;
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleCollapser {
  transform: rotate(-90deg);
  margin: auto 0;
  height: 16px;
}

.jp-AccordionPanel-title.lm-mod-expanded .lm-AccordionPanel-titleCollapser {
  transform: rotate(0deg);
}

.lm-AccordionPanel .jp-AccordionPanel-toolbar {
  background: none;
  box-shadow: none;
  border: none;
  margin-left: auto;
}

.lm-AccordionPanel .lm-SplitPanel-handle:hover {
  background: var(--jp-layout-color3);
}

.jp-text-truncated {
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Spinner {
  position: absolute;
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 10;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-layout-color0);
  outline: none;
}

.jp-SpinnerContent {
  font-size: 10px;
  margin: 50px auto;
  text-indent: -9999em;
  width: 3em;
  height: 3em;
  border-radius: 50%;
  background: var(--jp-brand-color3);
  background: linear-gradient(
    to right,
    #f37626 10%,
    rgba(255, 255, 255, 0) 42%
  );
  position: relative;
  animation: load3 1s infinite linear, fadeIn 1s;
}

.jp-SpinnerContent::before {
  width: 50%;
  height: 50%;
  background: #f37626;
  border-radius: 100% 0 0;
  position: absolute;
  top: 0;
  left: 0;
  content: '';
}

.jp-SpinnerContent::after {
  background: var(--jp-layout-color0);
  width: 75%;
  height: 75%;
  border-radius: 50%;
  content: '';
  margin: auto;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
}

@keyframes fadeIn {
  0% {
    opacity: 0;
  }

  100% {
    opacity: 1;
  }
}

@keyframes load3 {
  0% {
    transform: rotate(0deg);
  }

  100% {
    transform: rotate(360deg);
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

button.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: none;
  box-sizing: border-box;
  text-align: center;
  line-height: 32px;
  height: 32px;
  padding: 0 12px;
  letter-spacing: 0.8px;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input.jp-mod-styled {
  background: var(--jp-input-background);
  height: 28px;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color1);
  padding-left: 7px;
  padding-right: 7px;
  font-size: var(--jp-ui-font-size2);
  color: var(--jp-ui-font-color0);
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input[type='checkbox'].jp-mod-styled {
  appearance: checkbox;
  -webkit-appearance: checkbox;
  -moz-appearance: checkbox;
  height: auto;
}

input.jp-mod-styled:focus {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-select-wrapper {
  display: flex;
  position: relative;
  flex-direction: column;
  padding: 1px;
  background-color: var(--jp-layout-color1);
  box-sizing: border-box;
  margin-bottom: 12px;
}

.jp-select-wrapper:not(.multiple) {
  height: 28px;
}

.jp-select-wrapper.jp-mod-focused select.jp-mod-styled {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-input-active-background);
}

select.jp-mod-styled:hover {
  cursor: pointer;
  color: var(--jp-ui-font-color0);
  background-color: var(--jp-input-hover-background);
  box-shadow: inset 0 0 1px rgba(0, 0, 0, 0.5);
}

select.jp-mod-styled {
  flex: 1 1 auto;
  width: 100%;
  font-size: var(--jp-ui-font-size2);
  background: var(--jp-input-background);
  color: var(--jp-ui-font-color0);
  padding: 0 25px 0 8px;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

select.jp-mod-styled:not([multiple]) {
  height: 32px;
}

select.jp-mod-styled[multiple] {
  max-height: 200px;
  overflow-y: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-switch {
  display: flex;
  align-items: center;
  padding-left: 4px;
  padding-right: 4px;
  font-size: var(--jp-ui-font-size1);
  background-color: transparent;
  color: var(--jp-ui-font-color1);
  border: none;
  height: 20px;
}

.jp-switch:hover {
  background-color: var(--jp-layout-color2);
}

.jp-switch-label {
  margin-right: 5px;
  font-family: var(--jp-ui-font-family);
}

.jp-switch-track {
  cursor: pointer;
  background-color: var(--jp-switch-color, var(--jp-border-color1));
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 34px;
  height: 16px;
  width: 35px;
  position: relative;
}

.jp-switch-track::before {
  content: '';
  position: absolute;
  height: 10px;
  width: 10px;
  margin: 3px;
  left: 0;
  background-color: var(--jp-ui-inverse-font-color1);
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 50%;
}

.jp-switch[aria-checked='true'] .jp-switch-track {
  background-color: var(--jp-switch-true-position-color, var(--jp-warn-color0));
}

.jp-switch[aria-checked='true'] .jp-switch-track::before {
  /* track width (35) - margins (3 + 3) - thumb width (10) */
  left: 19px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toolbar-height: calc(
    28px + var(--jp-border-width)
  ); /* leave 28px for content */
}

.jp-Toolbar {
  color: var(--jp-ui-font-color1);
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: 2px;
  z-index: 8;
  overflow-x: hidden;
}

/* Toolbar items */

.jp-Toolbar > .jp-Toolbar-item.jp-Toolbar-spacer {
  flex-grow: 1;
  flex-shrink: 1;
}

.jp-Toolbar-item.jp-Toolbar-kernelStatus {
  display: inline-block;
  width: 32px;
  background-repeat: no-repeat;
  background-position: center;
  background-size: 16px;
}

.jp-Toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  display: flex;
  padding-left: 1px;
  padding-right: 1px;
  font-size: var(--jp-ui-font-size1);
  line-height: var(--jp-private-toolbar-height);
  height: 100%;
}

/* Toolbar buttons */

/* This is the div we use to wrap the react component into a Widget */
div.jp-ToolbarButton {
  color: transparent;
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0;
  margin: 0;
}

button.jp-ToolbarButtonComponent {
  background: var(--jp-layout-color1);
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0 6px;
  margin: 0;
  height: 24px;
  border-radius: var(--jp-border-radius);
  display: flex;
  align-items: center;
  text-align: center;
  font-size: 14px;
  min-width: unset;
  min-height: unset;
}

button.jp-ToolbarButtonComponent:disabled {
  opacity: 0.4;
}

button.jp-ToolbarButtonComponent > span {
  padding: 0;
  flex: 0 0 auto;
}

button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label {
  font-size: var(--jp-ui-font-size1);
  line-height: 100%;
  padding-left: 2px;
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar.jp-Toolbar-micro {
  padding: 0;
  min-height: 0;
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar {
  border: none;
  box-shadow: none;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-WindowedPanel-outer {
  position: relative;
  overflow-y: auto;
}

.jp-WindowedPanel-inner {
  position: relative;
}

.jp-WindowedPanel-window {
  position: absolute;
  left: 0;
  right: 0;
  overflow: visible;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* Sibling imports */

body {
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
}

/* Disable native link decoration styles everywhere outside of dialog boxes */
a {
  text-decoration: unset;
  color: unset;
}

a:hover {
  text-decoration: unset;
  color: unset;
}

/* Accessibility for links inside dialog box text */
.jp-Dialog-content a {
  text-decoration: revert;
  color: var(--jp-content-link-color);
}

.jp-Dialog-content a:hover {
  text-decoration: revert;
}

/* Styles for ui-components */
.jp-Button {
  color: var(--jp-ui-font-color2);
  border-radius: var(--jp-border-radius);
  padding: 0 12px;
  font-size: var(--jp-ui-font-size1);

  /* Copy from blueprint 3 */
  display: inline-flex;
  flex-direction: row;
  border: none;
  cursor: pointer;
  align-items: center;
  justify-content: center;
  text-align: left;
  vertical-align: middle;
  min-height: 30px;
  min-width: 30px;
}

.jp-Button:disabled {
  cursor: not-allowed;
}

.jp-Button:empty {
  padding: 0 !important;
}

.jp-Button.jp-mod-small {
  min-height: 24px;
  min-width: 24px;
  font-size: 12px;
  padding: 0 7px;
}

/* Use our own theme for hover styles */
.jp-Button.jp-mod-minimal:hover {
  background-color: var(--jp-layout-color2);
}

.jp-Button.jp-mod-minimal {
  background: none;
}

.jp-InputGroup {
  display: block;
  position: relative;
}

.jp-InputGroup input {
  box-sizing: border-box;
  border: none;
  border-radius: 0;
  background-color: transparent;
  color: var(--jp-ui-font-color0);
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
  padding-bottom: 0;
  padding-top: 0;
  padding-left: 10px;
  padding-right: 28px;
  position: relative;
  width: 100%;
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  font-size: 14px;
  font-weight: 400;
  height: 30px;
  line-height: 30px;
  outline: none;
  vertical-align: middle;
}

.jp-InputGroup input:focus {
  box-shadow: inset 0 0 0 var(--jp-border-width)
      var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-InputGroup input:disabled {
  cursor: not-allowed;
  resize: block;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input:disabled ~ span {
  cursor: not-allowed;
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input::placeholder,
input::placeholder {
  color: var(--jp-ui-font-color2);
}

.jp-InputGroupAction {
  position: absolute;
  bottom: 1px;
  right: 0;
  padding: 6px;
}

.jp-HTMLSelect.jp-DefaultStyle select {
  background-color: initial;
  border: none;
  border-radius: 0;
  box-shadow: none;
  color: var(--jp-ui-font-color0);
  display: block;
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  height: 24px;
  line-height: 14px;
  padding: 0 25px 0 10px;
  text-align: left;
  -moz-appearance: none;
  -webkit-appearance: none;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
  cursor: not-allowed;
  resize: block;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled ~ span {
  cursor: not-allowed;
}

/* Use our own theme for hover and option styles */
/* stylelint-disable-next-line selector-max-type */
.jp-HTMLSelect.jp-DefaultStyle select:hover,
.jp-HTMLSelect.jp-DefaultStyle select > option {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color0);
}

select {
  box-sizing: border-box;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-StatusBar-Widget {
  display: flex;
  align-items: center;
  background: var(--jp-layout-color2);
  min-height: var(--jp-statusbar-height);
  justify-content: space-between;
  padding: 0 10px;
}

.jp-StatusBar-Left {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-StatusBar-Middle {
  display: flex;
  align-items: center;
}

.jp-StatusBar-Right {
  display: flex;
  align-items: center;
  flex-direction: row-reverse;
}

.jp-StatusBar-Item {
  max-height: var(--jp-statusbar-height);
  margin: 0 2px;
  height: var(--jp-statusbar-height);
  white-space: nowrap;
  text-overflow: ellipsis;
  color: var(--jp-ui-font-color1);
  padding: 0 6px;
}

.jp-mod-highlighted:hover {
  background-color: var(--jp-layout-color3);
}

.jp-mod-clicked {
  background-color: var(--jp-brand-color1);
}

.jp-mod-clicked:hover {
  background-color: var(--jp-brand-color0);
}

.jp-mod-clicked .jp-StatusBar-TextItem {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-StatusBar-HoverItem {
  box-shadow: '0px 4px 4px rgba(0, 0, 0, 0.25)';
}

.jp-StatusBar-TextItem {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  line-height: 24px;
  color: var(--jp-ui-font-color1);
}

.jp-StatusBar-GroupItem {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-Statusbar-ProgressCircle svg {
  display: block;
  margin: 0 auto;
  width: 16px;
  height: 24px;
  align-self: normal;
}

.jp-Statusbar-ProgressCircle path {
  fill: var(--jp-inverse-layout-color3);
}

.jp-Statusbar-ProgressBar-progress-bar {
  height: 10px;
  width: 100px;
  border: solid 0.25px var(--jp-brand-color2);
  border-radius: 3px;
  overflow: hidden;
  align-self: center;
}

.jp-Statusbar-ProgressBar-progress-bar > div {
  background-color: var(--jp-brand-color2);
  background-image: linear-gradient(
    -45deg,
    rgba(255, 255, 255, 0.2) 25%,
    transparent 25%,
    transparent 50%,
    rgba(255, 255, 255, 0.2) 50%,
    rgba(255, 255, 255, 0.2) 75%,
    transparent 75%,
    transparent
  );
  background-size: 40px 40px;
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 14px;
  color: #fff;
  text-align: center;
  animation: jp-Statusbar-ExecutionTime-progress-bar 2s linear infinite;
}

.jp-Statusbar-ProgressBar-progress-bar p {
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
  font-size: var(--jp-ui-font-size1);
  line-height: 10px;
  width: 100px;
}

@keyframes jp-Statusbar-ExecutionTime-progress-bar {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 40px 40px;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-commandpalette-search-height: 28px;
}

/*-----------------------------------------------------------------------------
| Overall styles
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  padding-bottom: 0;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Modal variant
|----------------------------------------------------------------------------*/

.jp-ModalCommandPalette {
  position: absolute;
  z-index: 10000;
  top: 38px;
  left: 30%;
  margin: 0;
  padding: 4px;
  width: 40%;
  box-shadow: var(--jp-elevation-z4);
  border-radius: 4px;
  background: var(--jp-layout-color0);
}

.jp-ModalCommandPalette .lm-CommandPalette {
  max-height: 40vh;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-close-icon::after {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-header {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-item {
  margin-left: 4px;
  margin-right: 4px;
}

.jp-ModalCommandPalette
  .lm-CommandPalette
  .lm-CommandPalette-item.lm-mod-disabled {
  display: none;
}

/*-----------------------------------------------------------------------------
| Search
|----------------------------------------------------------------------------*/

.lm-CommandPalette-search {
  padding: 4px;
  background-color: var(--jp-layout-color1);
  z-index: 2;
}

.lm-CommandPalette-wrapper {
  overflow: overlay;
  padding: 0 9px;
  background-color: var(--jp-input-active-background);
  height: 30px;
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
}

.lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper {
  box-shadow: inset 0 0 0 1px var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-SearchIconGroup {
  color: white;
  background-color: var(--jp-brand-color1);
  position: absolute;
  top: 4px;
  right: 4px;
  padding: 5px 5px 1px;
}

.jp-SearchIconGroup svg {
  height: 20px;
  width: 20px;
}

.jp-SearchIconGroup .jp-icon3[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-input {
  background: transparent;
  width: calc(100% - 18px);
  float: left;
  border: none;
  outline: none;
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  line-height: var(--jp-private-commandpalette-search-height);
}

.lm-CommandPalette-input::-webkit-input-placeholder,
.lm-CommandPalette-input::-moz-placeholder,
.lm-CommandPalette-input:-ms-input-placeholder {
  color: var(--jp-ui-font-color2);
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Results
|----------------------------------------------------------------------------*/

.lm-CommandPalette-header:first-child {
  margin-top: 0;
}

.lm-CommandPalette-header {
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin-top: 8px;
  padding: 8px 0 8px 12px;
  text-transform: uppercase;
}

.lm-CommandPalette-header.lm-mod-active {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-header > mark {
  background-color: transparent;
  font-weight: bold;
  color: var(--jp-ui-font-color1);
}

.lm-CommandPalette-item {
  padding: 4px 12px 4px 4px;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  font-weight: 400;
  display: flex;
}

.lm-CommandPalette-item.lm-mod-disabled {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item.lm-mod-active {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item.lm-mod-active .lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-inverse-font-color0);
}

.lm-CommandPalette-item.lm-mod-active .jp-icon-selectable[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled) {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled) {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-itemContent {
  overflow: hidden;
}

.lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.lm-CommandPalette-item.lm-mod-disabled mark {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item .lm-CommandPalette-itemIcon {
  margin: 0 4px 0 0;
  position: relative;
  width: 16px;
  top: 2px;
  flex: 0 0 auto;
}

.lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon {
  opacity: 0.6;
}

.lm-CommandPalette-item .lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemCaption {
  display: none;
}

.lm-CommandPalette-content {
  background-color: var(--jp-layout-color1);
}

.lm-CommandPalette-content:empty::after {
  content: 'No results';
  margin: auto;
  margin-top: 20px;
  width: 100px;
  display: block;
  font-size: var(--jp-ui-font-size2);
  font-family: var(--jp-ui-font-family);
  font-weight: lighter;
}

.lm-CommandPalette-emptyMessage {
  text-align: center;
  margin-top: 24px;
  line-height: 1.32;
  padding: 0 8px;
  color: var(--jp-content-font-color3);
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Dialog {
  position: absolute;
  z-index: 10000;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  top: 0;
  left: 0;
  margin: 0;
  padding: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-dialog-background);
}

.jp-Dialog-content {
  display: flex;
  flex-direction: column;
  margin-left: auto;
  margin-right: auto;
  background: var(--jp-layout-color1);
  padding: 24px 24px 12px;
  min-width: 300px;
  min-height: 150px;
  max-width: 1000px;
  max-height: 500px;
  box-sizing: border-box;
  box-shadow: var(--jp-elevation-z20);
  word-wrap: break-word;
  border-radius: var(--jp-border-radius);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color1);
  resize: both;
}

.jp-Dialog-content.jp-Dialog-content-small {
  max-width: 500px;
}

.jp-Dialog-button {
  overflow: visible;
}

button.jp-Dialog-button:focus {
  outline: 1px solid var(--jp-brand-color1);
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button:focus::-moz-focus-inner {
  border: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus {
  outline: 1px solid var(--jp-accept-color-normal, var(--jp-brand-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus {
  outline: 1px solid var(--jp-warn-color-normal, var(--jp-error-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline: 1px solid var(--jp-reject-color-normal, var(--md-grey-600));
}

button.jp-Dialog-close-button {
  padding: 0;
  height: 100%;
  min-width: unset;
  min-height: unset;
}

.jp-Dialog-header {
  display: flex;
  justify-content: space-between;
  flex: 0 0 auto;
  padding-bottom: 12px;
  font-size: var(--jp-ui-font-size3);
  font-weight: 400;
  color: var(--jp-ui-font-color1);
}

.jp-Dialog-body {
  display: flex;
  flex-direction: column;
  flex: 1 1 auto;
  font-size: var(--jp-ui-font-size1);
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

.jp-Dialog-footer {
  display: flex;
  flex-direction: row;
  justify-content: flex-end;
  align-items: center;
  flex: 0 0 auto;
  margin-left: -12px;
  margin-right: -12px;
  padding: 12px;
}

.jp-Dialog-checkbox {
  padding-right: 5px;
}

.jp-Dialog-checkbox > input:focus-visible {
  outline: 1px solid var(--jp-input-active-border-color);
  outline-offset: 1px;
}

.jp-Dialog-spacer {
  flex: 1 1 auto;
}

.jp-Dialog-title {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.jp-Dialog-body > .jp-select-wrapper {
  width: 100%;
}

.jp-Dialog-body > button {
  padding: 0 16px;
}

.jp-Dialog-body > label {
  line-height: 1.4;
  color: var(--jp-ui-font-color0);
}

.jp-Dialog-button.jp-mod-styled:not(:last-child) {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Input-Boolean-Dialog {
  flex-direction: row-reverse;
  align-items: end;
  width: 100%;
}

.jp-Input-Boolean-Dialog > label {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MainAreaWidget > :focus {
  outline: none;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error {
  padding: 6px;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error > pre {
  width: auto;
  padding: 10px;
  background: var(--jp-error-color3);
  border: var(--jp-border-width) solid var(--jp-error-color1);
  border-radius: var(--jp-border-radius);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  white-space: pre-wrap;
  word-wrap: break-word;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/**
 * google-material-color v1.2.6
 * https://github.com/danlevan/google-material-color
 */
:root {
  --md-red-50: #ffebee;
  --md-red-100: #ffcdd2;
  --md-red-200: #ef9a9a;
  --md-red-300: #e57373;
  --md-red-400: #ef5350;
  --md-red-500: #f44336;
  --md-red-600: #e53935;
  --md-red-700: #d32f2f;
  --md-red-800: #c62828;
  --md-red-900: #b71c1c;
  --md-red-A100: #ff8a80;
  --md-red-A200: #ff5252;
  --md-red-A400: #ff1744;
  --md-red-A700: #d50000;
  --md-pink-50: #fce4ec;
  --md-pink-100: #f8bbd0;
  --md-pink-200: #f48fb1;
  --md-pink-300: #f06292;
  --md-pink-400: #ec407a;
  --md-pink-500: #e91e63;
  --md-pink-600: #d81b60;
  --md-pink-700: #c2185b;
  --md-pink-800: #ad1457;
  --md-pink-900: #880e4f;
  --md-pink-A100: #ff80ab;
  --md-pink-A200: #ff4081;
  --md-pink-A400: #f50057;
  --md-pink-A700: #c51162;
  --md-purple-50: #f3e5f5;
  --md-purple-100: #e1bee7;
  --md-purple-200: #ce93d8;
  --md-purple-300: #ba68c8;
  --md-purple-400: #ab47bc;
  --md-purple-500: #9c27b0;
  --md-purple-600: #8e24aa;
  --md-purple-700: #7b1fa2;
  --md-purple-800: #6a1b9a;
  --md-purple-900: #4a148c;
  --md-purple-A100: #ea80fc;
  --md-purple-A200: #e040fb;
  --md-purple-A400: #d500f9;
  --md-purple-A700: #a0f;
  --md-deep-purple-50: #ede7f6;
  --md-deep-purple-100: #d1c4e9;
  --md-deep-purple-200: #b39ddb;
  --md-deep-purple-300: #9575cd;
  --md-deep-purple-400: #7e57c2;
  --md-deep-purple-500: #673ab7;
  --md-deep-purple-600: #5e35b1;
  --md-deep-purple-700: #512da8;
  --md-deep-purple-800: #4527a0;
  --md-deep-purple-900: #311b92;
  --md-deep-purple-A100: #b388ff;
  --md-deep-purple-A200: #7c4dff;
  --md-deep-purple-A400: #651fff;
  --md-deep-purple-A700: #6200ea;
  --md-indigo-50: #e8eaf6;
  --md-indigo-100: #c5cae9;
  --md-indigo-200: #9fa8da;
  --md-indigo-300: #7986cb;
  --md-indigo-400: #5c6bc0;
  --md-indigo-500: #3f51b5;
  --md-indigo-600: #3949ab;
  --md-indigo-700: #303f9f;
  --md-indigo-800: #283593;
  --md-indigo-900: #1a237e;
  --md-indigo-A100: #8c9eff;
  --md-indigo-A200: #536dfe;
  --md-indigo-A400: #3d5afe;
  --md-indigo-A700: #304ffe;
  --md-blue-50: #e3f2fd;
  --md-blue-100: #bbdefb;
  --md-blue-200: #90caf9;
  --md-blue-300: #64b5f6;
  --md-blue-400: #42a5f5;
  --md-blue-500: #2196f3;
  --md-blue-600: #1e88e5;
  --md-blue-700: #1976d2;
  --md-blue-800: #1565c0;
  --md-blue-900: #0d47a1;
  --md-blue-A100: #82b1ff;
  --md-blue-A200: #448aff;
  --md-blue-A400: #2979ff;
  --md-blue-A700: #2962ff;
  --md-light-blue-50: #e1f5fe;
  --md-light-blue-100: #b3e5fc;
  --md-light-blue-200: #81d4fa;
  --md-light-blue-300: #4fc3f7;
  --md-light-blue-400: #29b6f6;
  --md-light-blue-500: #03a9f4;
  --md-light-blue-600: #039be5;
  --md-light-blue-700: #0288d1;
  --md-light-blue-800: #0277bd;
  --md-light-blue-900: #01579b;
  --md-light-blue-A100: #80d8ff;
  --md-light-blue-A200: #40c4ff;
  --md-light-blue-A400: #00b0ff;
  --md-light-blue-A700: #0091ea;
  --md-cyan-50: #e0f7fa;
  --md-cyan-100: #b2ebf2;
  --md-cyan-200: #80deea;
  --md-cyan-300: #4dd0e1;
  --md-cyan-400: #26c6da;
  --md-cyan-500: #00bcd4;
  --md-cyan-600: #00acc1;
  --md-cyan-700: #0097a7;
  --md-cyan-800: #00838f;
  --md-cyan-900: #006064;
  --md-cyan-A100: #84ffff;
  --md-cyan-A200: #18ffff;
  --md-cyan-A400: #00e5ff;
  --md-cyan-A700: #00b8d4;
  --md-teal-50: #e0f2f1;
  --md-teal-100: #b2dfdb;
  --md-teal-200: #80cbc4;
  --md-teal-300: #4db6ac;
  --md-teal-400: #26a69a;
  --md-teal-500: #009688;
  --md-teal-600: #00897b;
  --md-teal-700: #00796b;
  --md-teal-800: #00695c;
  --md-teal-900: #004d40;
  --md-teal-A100: #a7ffeb;
  --md-teal-A200: #64ffda;
  --md-teal-A400: #1de9b6;
  --md-teal-A700: #00bfa5;
  --md-green-50: #e8f5e9;
  --md-green-100: #c8e6c9;
  --md-green-200: #a5d6a7;
  --md-green-300: #81c784;
  --md-green-400: #66bb6a;
  --md-green-500: #4caf50;
  --md-green-600: #43a047;
  --md-green-700: #388e3c;
  --md-green-800: #2e7d32;
  --md-green-900: #1b5e20;
  --md-green-A100: #b9f6ca;
  --md-green-A200: #69f0ae;
  --md-green-A400: #00e676;
  --md-green-A700: #00c853;
  --md-light-green-50: #f1f8e9;
  --md-light-green-100: #dcedc8;
  --md-light-green-200: #c5e1a5;
  --md-light-green-300: #aed581;
  --md-light-green-400: #9ccc65;
  --md-light-green-500: #8bc34a;
  --md-light-green-600: #7cb342;
  --md-light-green-700: #689f38;
  --md-light-green-800: #558b2f;
  --md-light-green-900: #33691e;
  --md-light-green-A100: #ccff90;
  --md-light-green-A200: #b2ff59;
  --md-light-green-A400: #76ff03;
  --md-light-green-A700: #64dd17;
  --md-lime-50: #f9fbe7;
  --md-lime-100: #f0f4c3;
  --md-lime-200: #e6ee9c;
  --md-lime-300: #dce775;
  --md-lime-400: #d4e157;
  --md-lime-500: #cddc39;
  --md-lime-600: #c0ca33;
  --md-lime-700: #afb42b;
  --md-lime-800: #9e9d24;
  --md-lime-900: #827717;
  --md-lime-A100: #f4ff81;
  --md-lime-A200: #eeff41;
  --md-lime-A400: #c6ff00;
  --md-lime-A700: #aeea00;
  --md-yellow-50: #fffde7;
  --md-yellow-100: #fff9c4;
  --md-yellow-200: #fff59d;
  --md-yellow-300: #fff176;
  --md-yellow-400: #ffee58;
  --md-yellow-500: #ffeb3b;
  --md-yellow-600: #fdd835;
  --md-yellow-700: #fbc02d;
  --md-yellow-800: #f9a825;
  --md-yellow-900: #f57f17;
  --md-yellow-A100: #ffff8d;
  --md-yellow-A200: #ff0;
  --md-yellow-A400: #ffea00;
  --md-yellow-A700: #ffd600;
  --md-amber-50: #fff8e1;
  --md-amber-100: #ffecb3;
  --md-amber-200: #ffe082;
  --md-amber-300: #ffd54f;
  --md-amber-400: #ffca28;
  --md-amber-500: #ffc107;
  --md-amber-600: #ffb300;
  --md-amber-700: #ffa000;
  --md-amber-800: #ff8f00;
  --md-amber-900: #ff6f00;
  --md-amber-A100: #ffe57f;
  --md-amber-A200: #ffd740;
  --md-amber-A400: #ffc400;
  --md-amber-A700: #ffab00;
  --md-orange-50: #fff3e0;
  --md-orange-100: #ffe0b2;
  --md-orange-200: #ffcc80;
  --md-orange-300: #ffb74d;
  --md-orange-400: #ffa726;
  --md-orange-500: #ff9800;
  --md-orange-600: #fb8c00;
  --md-orange-700: #f57c00;
  --md-orange-800: #ef6c00;
  --md-orange-900: #e65100;
  --md-orange-A100: #ffd180;
  --md-orange-A200: #ffab40;
  --md-orange-A400: #ff9100;
  --md-orange-A700: #ff6d00;
  --md-deep-orange-50: #fbe9e7;
  --md-deep-orange-100: #ffccbc;
  --md-deep-orange-200: #ffab91;
  --md-deep-orange-300: #ff8a65;
  --md-deep-orange-400: #ff7043;
  --md-deep-orange-500: #ff5722;
  --md-deep-orange-600: #f4511e;
  --md-deep-orange-700: #e64a19;
  --md-deep-orange-800: #d84315;
  --md-deep-orange-900: #bf360c;
  --md-deep-orange-A100: #ff9e80;
  --md-deep-orange-A200: #ff6e40;
  --md-deep-orange-A400: #ff3d00;
  --md-deep-orange-A700: #dd2c00;
  --md-brown-50: #efebe9;
  --md-brown-100: #d7ccc8;
  --md-brown-200: #bcaaa4;
  --md-brown-300: #a1887f;
  --md-brown-400: #8d6e63;
  --md-brown-500: #795548;
  --md-brown-600: #6d4c41;
  --md-brown-700: #5d4037;
  --md-brown-800: #4e342e;
  --md-brown-900: #3e2723;
  --md-grey-50: #fafafa;
  --md-grey-100: #f5f5f5;
  --md-grey-200: #eee;
  --md-grey-300: #e0e0e0;
  --md-grey-400: #bdbdbd;
  --md-grey-500: #9e9e9e;
  --md-grey-600: #757575;
  --md-grey-700: #616161;
  --md-grey-800: #424242;
  --md-grey-900: #212121;
  --md-blue-grey-50: #eceff1;
  --md-blue-grey-100: #cfd8dc;
  --md-blue-grey-200: #b0bec5;
  --md-blue-grey-300: #90a4ae;
  --md-blue-grey-400: #78909c;
  --md-blue-grey-500: #607d8b;
  --md-blue-grey-600: #546e7a;
  --md-blue-grey-700: #455a64;
  --md-blue-grey-800: #37474f;
  --md-blue-grey-900: #263238;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| RenderedText
|----------------------------------------------------------------------------*/

:root {
  /* This is the padding value to fill the gaps between lines containing spans with background color. */
  --jp-private-code-span-padding: calc(
    (var(--jp-code-line-height) - 1) * var(--jp-code-font-size) / 2
  );
}

.jp-RenderedText {
  text-align: left;
  padding-left: var(--jp-code-padding);
  line-height: var(--jp-code-line-height);
  font-family: var(--jp-code-font-family);
}

.jp-RenderedText pre,
.jp-RenderedJavaScript pre,
.jp-RenderedHTMLCommon pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
  border: none;
  margin: 0;
  padding: 0;
}

.jp-RenderedText pre a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* console foregrounds and backgrounds */
.jp-RenderedText pre .ansi-black-fg {
  color: #3e424d;
}

.jp-RenderedText pre .ansi-red-fg {
  color: #e75c58;
}

.jp-RenderedText pre .ansi-green-fg {
  color: #00a250;
}

.jp-RenderedText pre .ansi-yellow-fg {
  color: #ddb62b;
}

.jp-RenderedText pre .ansi-blue-fg {
  color: #208ffb;
}

.jp-RenderedText pre .ansi-magenta-fg {
  color: #d160c4;
}

.jp-RenderedText pre .ansi-cyan-fg {
  color: #60c6c8;
}

.jp-RenderedText pre .ansi-white-fg {
  color: #c5c1b4;
}

.jp-RenderedText pre .ansi-black-bg {
  background-color: #3e424d;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-bg {
  background-color: #e75c58;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-bg {
  background-color: #00a250;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-bg {
  background-color: #ddb62b;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-bg {
  background-color: #208ffb;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-bg {
  background-color: #d160c4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-bg {
  background-color: #60c6c8;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-bg {
  background-color: #c5c1b4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-black-intense-fg {
  color: #282c36;
}

.jp-RenderedText pre .ansi-red-intense-fg {
  color: #b22b31;
}

.jp-RenderedText pre .ansi-green-intense-fg {
  color: #007427;
}

.jp-RenderedText pre .ansi-yellow-intense-fg {
  color: #b27d12;
}

.jp-RenderedText pre .ansi-blue-intense-fg {
  color: #0065ca;
}

.jp-RenderedText pre .ansi-magenta-intense-fg {
  color: #a03196;
}

.jp-RenderedText pre .ansi-cyan-intense-fg {
  color: #258f8f;
}

.jp-RenderedText pre .ansi-white-intense-fg {
  color: #a1a6b2;
}

.jp-RenderedText pre .ansi-black-intense-bg {
  background-color: #282c36;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-intense-bg {
  background-color: #b22b31;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-intense-bg {
  background-color: #007427;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-intense-bg {
  background-color: #b27d12;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-intense-bg {
  background-color: #0065ca;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-intense-bg {
  background-color: #a03196;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-intense-bg {
  background-color: #258f8f;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-intense-bg {
  background-color: #a1a6b2;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-default-inverse-fg {
  color: var(--jp-ui-inverse-font-color0);
}

.jp-RenderedText pre .ansi-default-inverse-bg {
  background-color: var(--jp-inverse-layout-color0);
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-bold {
  font-weight: bold;
}

.jp-RenderedText pre .ansi-underline {
  text-decoration: underline;
}

.jp-RenderedText[data-mime-type='application/vnd.jupyter.stderr'] {
  background: var(--jp-rendermime-error-background);
  padding-top: var(--jp-code-padding);
}

/*-----------------------------------------------------------------------------
| RenderedLatex
|----------------------------------------------------------------------------*/

.jp-RenderedLatex {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);
}

/* Left-justify outputs.*/
.jp-OutputArea-output.jp-RenderedLatex {
  padding: var(--jp-code-padding);
  text-align: left;
}

/*-----------------------------------------------------------------------------
| RenderedHTML
|----------------------------------------------------------------------------*/

.jp-RenderedHTMLCommon {
  color: var(--jp-content-font-color1);
  font-family: var(--jp-content-font-family);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);

  /* Give a bit more R padding on Markdown text to keep line lengths reasonable */
  padding-right: 20px;
}

.jp-RenderedHTMLCommon em {
  font-style: italic;
}

.jp-RenderedHTMLCommon strong {
  font-weight: bold;
}

.jp-RenderedHTMLCommon u {
  text-decoration: underline;
}

.jp-RenderedHTMLCommon a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* Headings */

.jp-RenderedHTMLCommon h1,
.jp-RenderedHTMLCommon h2,
.jp-RenderedHTMLCommon h3,
.jp-RenderedHTMLCommon h4,
.jp-RenderedHTMLCommon h5,
.jp-RenderedHTMLCommon h6 {
  line-height: var(--jp-content-heading-line-height);
  font-weight: var(--jp-content-heading-font-weight);
  font-style: normal;
  margin: var(--jp-content-heading-margin-top) 0
    var(--jp-content-heading-margin-bottom) 0;
}

.jp-RenderedHTMLCommon h1:first-child,
.jp-RenderedHTMLCommon h2:first-child,
.jp-RenderedHTMLCommon h3:first-child,
.jp-RenderedHTMLCommon h4:first-child,
.jp-RenderedHTMLCommon h5:first-child,
.jp-RenderedHTMLCommon h6:first-child {
  margin-top: calc(0.5 * var(--jp-content-heading-margin-top));
}

.jp-RenderedHTMLCommon h1:last-child,
.jp-RenderedHTMLCommon h2:last-child,
.jp-RenderedHTMLCommon h3:last-child,
.jp-RenderedHTMLCommon h4:last-child,
.jp-RenderedHTMLCommon h5:last-child,
.jp-RenderedHTMLCommon h6:last-child {
  margin-bottom: calc(0.5 * var(--jp-content-heading-margin-bottom));
}

.jp-RenderedHTMLCommon h1 {
  font-size: var(--jp-content-font-size5);
}

.jp-RenderedHTMLCommon h2 {
  font-size: var(--jp-content-font-size4);
}

.jp-RenderedHTMLCommon h3 {
  font-size: var(--jp-content-font-size3);
}

.jp-RenderedHTMLCommon h4 {
  font-size: var(--jp-content-font-size2);
}

.jp-RenderedHTMLCommon h5 {
  font-size: var(--jp-content-font-size1);
}

.jp-RenderedHTMLCommon h6 {
  font-size: var(--jp-content-font-size0);
}

/* Lists */

/* stylelint-disable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon ul:not(.list-inline),
.jp-RenderedHTMLCommon ol:not(.list-inline) {
  padding-left: 2em;
}

.jp-RenderedHTMLCommon ul {
  list-style: disc;
}

.jp-RenderedHTMLCommon ul ul {
  list-style: square;
}

.jp-RenderedHTMLCommon ul ul ul {
  list-style: circle;
}

.jp-RenderedHTMLCommon ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol ol {
  list-style: upper-alpha;
}

.jp-RenderedHTMLCommon ol ol ol {
  list-style: lower-alpha;
}

.jp-RenderedHTMLCommon ol ol ol ol {
  list-style: lower-roman;
}

.jp-RenderedHTMLCommon ol ol ol ol ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol,
.jp-RenderedHTMLCommon ul {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon ul ul,
.jp-RenderedHTMLCommon ul ol,
.jp-RenderedHTMLCommon ol ul,
.jp-RenderedHTMLCommon ol ol {
  margin-bottom: 0;
}

/* stylelint-enable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon hr {
  color: var(--jp-border-color2);
  background-color: var(--jp-border-color1);
  margin-top: 1em;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon > pre {
  margin: 1.5em 2em;
}

.jp-RenderedHTMLCommon pre,
.jp-RenderedHTMLCommon code {
  border: 0;
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  line-height: var(--jp-code-line-height);
  padding: 0;
  white-space: pre-wrap;
}

.jp-RenderedHTMLCommon :not(pre) > code {
  background-color: var(--jp-layout-color2);
  padding: 1px 5px;
}

/* Tables */

.jp-RenderedHTMLCommon table {
  border-collapse: collapse;
  border-spacing: 0;
  border: none;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  table-layout: fixed;
  margin-left: auto;
  margin-bottom: 1em;
  margin-right: auto;
}

.jp-RenderedHTMLCommon thead {
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  vertical-align: bottom;
}

.jp-RenderedHTMLCommon td,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon tr {
  vertical-align: middle;
  padding: 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}

.jp-RenderedMarkdown.jp-RenderedHTMLCommon td,
.jp-RenderedMarkdown.jp-RenderedHTMLCommon th {
  max-width: none;
}

:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon td,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon th,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon tr {
  text-align: right;
}

.jp-RenderedHTMLCommon th {
  font-weight: bold;
}

.jp-RenderedHTMLCommon tbody tr:nth-child(odd) {
  background: var(--jp-layout-color0);
}

.jp-RenderedHTMLCommon tbody tr:nth-child(even) {
  background: var(--jp-rendermime-table-row-background);
}

.jp-RenderedHTMLCommon tbody tr:hover {
  background: var(--jp-rendermime-table-row-hover-background);
}

.jp-RenderedHTMLCommon p {
  text-align: left;
  margin: 0;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon img {
  -moz-force-broken-image-icon: 1;
}

/* Restrict to direct children as other images could be nested in other content. */
.jp-RenderedHTMLCommon > img {
  display: block;
  margin-left: 0;
  margin-right: 0;
  margin-bottom: 1em;
}

/* Change color behind transparent images if they need it... */
[data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-light-background {
  background-color: var(--jp-inverse-layout-color1);
}

[data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-dark-background {
  background-color: var(--jp-inverse-layout-color1);
}

.jp-RenderedHTMLCommon img,
.jp-RenderedImage img,
.jp-RenderedHTMLCommon svg,
.jp-RenderedSVG svg {
  max-width: 100%;
  height: auto;
}

.jp-RenderedHTMLCommon img.jp-mod-unconfined,
.jp-RenderedImage img.jp-mod-unconfined,
.jp-RenderedHTMLCommon svg.jp-mod-unconfined,
.jp-RenderedSVG svg.jp-mod-unconfined {
  max-width: none;
}

.jp-RenderedHTMLCommon .alert {
  padding: var(--jp-notebook-padding);
  border: var(--jp-border-width) solid transparent;
  border-radius: var(--jp-border-radius);
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon .alert-info {
  color: var(--jp-info-color0);
  background-color: var(--jp-info-color3);
  border-color: var(--jp-info-color2);
}

.jp-RenderedHTMLCommon .alert-info hr {
  border-color: var(--jp-info-color3);
}

.jp-RenderedHTMLCommon .alert-info > p:last-child,
.jp-RenderedHTMLCommon .alert-info > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-warning {
  color: var(--jp-warn-color0);
  background-color: var(--jp-warn-color3);
  border-color: var(--jp-warn-color2);
}

.jp-RenderedHTMLCommon .alert-warning hr {
  border-color: var(--jp-warn-color3);
}

.jp-RenderedHTMLCommon .alert-warning > p:last-child,
.jp-RenderedHTMLCommon .alert-warning > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-success {
  color: var(--jp-success-color0);
  background-color: var(--jp-success-color3);
  border-color: var(--jp-success-color2);
}

.jp-RenderedHTMLCommon .alert-success hr {
  border-color: var(--jp-success-color3);
}

.jp-RenderedHTMLCommon .alert-success > p:last-child,
.jp-RenderedHTMLCommon .alert-success > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-danger {
  color: var(--jp-error-color0);
  background-color: var(--jp-error-color3);
  border-color: var(--jp-error-color2);
}

.jp-RenderedHTMLCommon .alert-danger hr {
  border-color: var(--jp-error-color3);
}

.jp-RenderedHTMLCommon .alert-danger > p:last-child,
.jp-RenderedHTMLCommon .alert-danger > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon blockquote {
  margin: 1em 2em;
  padding: 0 1em;
  border-left: 5px solid var(--jp-border-color2);
}

a.jp-InternalAnchorLink {
  visibility: hidden;
  margin-left: 8px;
  color: var(--md-blue-800);
}

h1:hover .jp-InternalAnchorLink,
h2:hover .jp-InternalAnchorLink,
h3:hover .jp-InternalAnchorLink,
h4:hover .jp-InternalAnchorLink,
h5:hover .jp-InternalAnchorLink,
h6:hover .jp-InternalAnchorLink {
  visibility: visible;
}

.jp-RenderedHTMLCommon kbd {
  background-color: var(--jp-rendermime-table-row-background);
  border: 1px solid var(--jp-border-color0);
  border-bottom-color: var(--jp-border-color2);
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
  display: inline-block;
  font-size: var(--jp-ui-font-size0);
  line-height: 1em;
  padding: 0.2em 0.5em;
}

/* Most direct children of .jp-RenderedHTMLCommon have a margin-bottom of 1.0.
 * At the bottom of cells this is a bit too much as there is also spacing
 * between cells. Going all the way to 0 gets too tight between markdown and
 * code cells.
 */
.jp-RenderedHTMLCommon > *:last-child {
  margin-bottom: 0.5em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-cursor-backdrop {
  position: fixed;
  width: 200px;
  height: 200px;
  margin-top: -100px;
  margin-left: -100px;
  will-change: transform;
  z-index: 100;
}

.lm-mod-drag-image {
  will-change: transform;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-lineFormSearch {
  padding: 4px 12px;
  background-color: var(--jp-layout-color2);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
  font-size: var(--jp-ui-font-size1);
}

.jp-lineFormCaption {
  font-size: var(--jp-ui-font-size0);
  line-height: var(--jp-ui-font-size1);
  margin-top: 4px;
  color: var(--jp-ui-font-color0);
}

.jp-baseLineForm {
  border: none;
  border-radius: 0;
  position: absolute;
  background-size: 16px;
  background-repeat: no-repeat;
  background-position: center;
  outline: none;
}

.jp-lineFormButtonContainer {
  top: 4px;
  right: 8px;
  height: 24px;
  padding: 0 12px;
  width: 12px;
}

.jp-lineFormButtonIcon {
  top: 0;
  right: 0;
  background-color: var(--jp-brand-color1);
  height: 100%;
  width: 100%;
  box-sizing: border-box;
  padding: 4px 6px;
}

.jp-lineFormButton {
  top: 0;
  right: 0;
  background-color: transparent;
  height: 100%;
  width: 100%;
  box-sizing: border-box;
}

.jp-lineFormWrapper {
  overflow: hidden;
  padding: 0 8px;
  border: 1px solid var(--jp-border-color0);
  background-color: var(--jp-input-active-background);
  height: 22px;
}

.jp-lineFormWrapperFocusWithin {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-lineFormInput {
  background: transparent;
  width: 200px;
  height: 100%;
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  line-height: 28px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-JSONEditor {
  display: flex;
  flex-direction: column;
  width: 100%;
}

.jp-JSONEditor-host {
  flex: 1 1 auto;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  background: var(--jp-layout-color0);
  min-height: 50px;
  padding: 1px;
}

.jp-JSONEditor.jp-mod-error .jp-JSONEditor-host {
  border-color: red;
  outline-color: red;
}

.jp-JSONEditor-header {
  display: flex;
  flex: 1 0 auto;
  padding: 0 0 0 12px;
}

.jp-JSONEditor-header label {
  flex: 0 0 auto;
}

.jp-JSONEditor-commitButton {
  height: 16px;
  width: 16px;
  background-size: 18px;
  background-repeat: no-repeat;
  background-position: center;
}

.jp-JSONEditor-host.jp-mod-focused {
  background-color: var(--jp-input-active-background);
  border: 1px solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

.jp-Editor.jp-mod-dropTarget {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/
.jp-DocumentSearch-input {
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  font-size: var(--jp-ui-font-size1);
  background-color: var(--jp-layout-color0);
  font-family: var(--jp-ui-font-family);
  padding: 2px 1px;
  resize: none;
}

.jp-DocumentSearch-overlay {
  position: absolute;
  background-color: var(--jp-toolbar-background);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  border-left: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  top: 0;
  right: 0;
  z-index: 7;
  min-width: 405px;
  padding: 2px;
  font-size: var(--jp-ui-font-size1);

  --jp-private-document-search-button-height: 20px;
}

.jp-DocumentSearch-overlay button {
  background-color: var(--jp-toolbar-background);
  outline: 0;
}

.jp-DocumentSearch-overlay button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-overlay button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-overlay-row {
  display: flex;
  align-items: center;
  margin-bottom: 2px;
}

.jp-DocumentSearch-button-content {
  display: inline-block;
  cursor: pointer;
  box-sizing: border-box;
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-button-content svg {
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-input-wrapper {
  border: var(--jp-border-width) solid var(--jp-border-color0);
  display: flex;
  background-color: var(--jp-layout-color0);
  margin: 2px;
}

.jp-DocumentSearch-input-wrapper:focus-within {
  border-color: var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper {
  all: initial;
  overflow: hidden;
  display: inline-block;
  border: none;
  box-sizing: border-box;
}

.jp-DocumentSearch-toggle-wrapper {
  width: 14px;
  height: 14px;
}

.jp-DocumentSearch-button-wrapper {
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
}

.jp-DocumentSearch-toggle-wrapper:focus,
.jp-DocumentSearch-button-wrapper:focus {
  outline: var(--jp-border-width) solid
    var(--jp-cell-editor-active-border-color);
  outline-offset: -1px;
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper,
.jp-DocumentSearch-button-content:focus {
  outline: none;
}

.jp-DocumentSearch-toggle-placeholder {
  width: 5px;
}

.jp-DocumentSearch-input-button::before {
  display: block;
  padding-top: 100%;
}

.jp-DocumentSearch-input-button-off {
  opacity: var(--jp-search-toggle-off-opacity);
}

.jp-DocumentSearch-input-button-off:hover {
  opacity: var(--jp-search-toggle-hover-opacity);
}

.jp-DocumentSearch-input-button-on {
  opacity: var(--jp-search-toggle-on-opacity);
}

.jp-DocumentSearch-index-counter {
  padding-left: 10px;
  padding-right: 10px;
  user-select: none;
  min-width: 35px;
  display: inline-block;
}

.jp-DocumentSearch-up-down-wrapper {
  display: inline-block;
  padding-right: 2px;
  margin-left: auto;
  white-space: nowrap;
}

.jp-DocumentSearch-spacer {
  margin-left: auto;
}

.jp-DocumentSearch-up-down-wrapper button {
  outline: 0;
  border: none;
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
  vertical-align: middle;
  margin: 1px 5px 2px;
}

.jp-DocumentSearch-up-down-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-up-down-button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-filter-button {
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-filter-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled:hover {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-search-options {
  padding: 0 8px;
  margin-left: 3px;
  width: 100%;
  display: grid;
  justify-content: start;
  grid-template-columns: 1fr 1fr;
  align-items: center;
  justify-items: stretch;
}

.jp-DocumentSearch-search-filter-disabled {
  color: var(--jp-ui-font-color2);
}

.jp-DocumentSearch-search-filter {
  display: flex;
  align-items: center;
  user-select: none;
}

.jp-DocumentSearch-regex-error {
  color: var(--jp-error-color0);
}

.jp-DocumentSearch-replace-button-wrapper {
  overflow: hidden;
  display: inline-block;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color0);
  margin: auto 2px;
  padding: 1px 4px;
  height: calc(var(--jp-private-document-search-button-height) + 2px);
}

.jp-DocumentSearch-replace-button-wrapper:focus {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-replace-button {
  display: inline-block;
  text-align: center;
  cursor: pointer;
  box-sizing: border-box;
  color: var(--jp-ui-font-color1);

  /* height - 2 * (padding of wrapper) */
  line-height: calc(var(--jp-private-document-search-button-height) - 2px);
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-replace-button:focus {
  outline: none;
}

.jp-DocumentSearch-replace-wrapper-class {
  margin-left: 14px;
  display: flex;
}

.jp-DocumentSearch-replace-toggle {
  border: none;
  background-color: var(--jp-toolbar-background);
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-replace-toggle:hover {
  background-color: var(--jp-layout-color2);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.cm-editor {
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  border: 0;
  border-radius: 0;
  height: auto;

  /* Changed to auto to autogrow */
}

.cm-editor pre {
  padding: 0 var(--jp-code-padding);
}

.jp-CodeMirrorEditor[data-type='inline'] .cm-dialog {
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
}

.jp-CodeMirrorEditor {
  cursor: text;
}

/* When zoomed out 67% and 33% on a screen of 1440 width x 900 height */
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width1) solid
      var(--jp-editor-cursor-color);
  }
}

/* When zoomed out less than 33% */
@media screen and (min-width: 4320px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width2) solid
      var(--jp-editor-cursor-color);
  }
}

.cm-editor.jp-mod-readOnly .cm-cursor {
  display: none;
}

.jp-CollaboratorCursor {
  border-left: 5px solid transparent;
  border-right: 5px solid transparent;
  border-top: none;
  border-bottom: 3px solid;
  background-clip: content-box;
  margin-left: -5px;
  margin-right: -5px;
}

.cm-searching,
.cm-searching span {
  /* `.cm-searching span`: we need to override syntax highlighting */
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.cm-searching::selection,
.cm-searching span::selection {
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.jp-current-match > .cm-searching,
.jp-current-match > .cm-searching span,
.cm-searching > .jp-current-match,
.cm-searching > .jp-current-match span {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.jp-current-match > .cm-searching::selection,
.cm-searching > .jp-current-match::selection,
.jp-current-match > .cm-searching span::selection {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.cm-trailingspace {
  background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAYAAAB4ka1VAAAAsElEQVQIHQGlAFr/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7+r3zKmT0/+pk9P/7+r3zAAAAAAAAAAABAAAAAAAAAAA6OPzM+/q9wAAAAAA6OPzMwAAAAAAAAAAAgAAAAAAAAAAGR8NiRQaCgAZIA0AGR8NiQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQyoYJ/SY80UAAAAASUVORK5CYII=);
  background-position: center left;
  background-repeat: repeat-x;
}

.jp-CollaboratorCursor-hover {
  position: absolute;
  z-index: 1;
  transform: translateX(-50%);
  color: white;
  border-radius: 3px;
  padding-left: 4px;
  padding-right: 4px;
  padding-top: 1px;
  padding-bottom: 1px;
  text-align: center;
  font-size: var(--jp-ui-font-size1);
  white-space: nowrap;
}

.jp-CodeMirror-ruler {
  border-left: 1px dashed var(--jp-border-color2);
}

/* Styles for shared cursors (remote cursor locations and selected ranges) */
.jp-CodeMirrorEditor .cm-ySelectionCaret {
  position: relative;
  border-left: 1px solid black;
  margin-left: -1px;
  margin-right: -1px;
  box-sizing: border-box;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret > .cm-ySelectionInfo {
  white-space: nowrap;
  position: absolute;
  top: -1.15em;
  padding-bottom: 0.05em;
  left: -1px;
  font-size: 0.95em;
  font-family: var(--jp-ui-font-family);
  font-weight: bold;
  line-height: normal;
  user-select: none;
  color: white;
  padding-left: 2px;
  padding-right: 2px;
  z-index: 101;
  transition: opacity 0.3s ease-in-out;
}

.jp-CodeMirrorEditor .cm-ySelectionInfo {
  transition-delay: 0.7s;
  opacity: 0;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret:hover > .cm-ySelectionInfo {
  opacity: 1;
  transition-delay: 0s;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MimeDocument {
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-filebrowser-button-height: 28px;
  --jp-private-filebrowser-button-width: 48px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FileBrowser .jp-SidePanel-content {
  display: flex;
  flex-direction: column;
}

.jp-FileBrowser-toolbar.jp-Toolbar {
  flex-wrap: wrap;
  row-gap: 12px;
  border-bottom: none;
  height: auto;
  margin: 8px 12px 0;
  box-shadow: none;
  padding: 0;
  justify-content: flex-start;
}

.jp-FileBrowser-Panel {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
}

.jp-BreadCrumbs {
  flex: 0 0 auto;
  margin: 8px 12px;
}

.jp-BreadCrumbs-item {
  margin: 0 2px;
  padding: 0 2px;
  border-radius: var(--jp-border-radius);
  cursor: pointer;
}

.jp-BreadCrumbs-item:hover {
  background-color: var(--jp-layout-color2);
}

.jp-BreadCrumbs-item:first-child {
  margin-left: 0;
}

.jp-BreadCrumbs-item.jp-mod-dropTarget {
  background-color: var(--jp-brand-color2);
  opacity: 0.7;
}

/*-----------------------------------------------------------------------------
| Buttons
|----------------------------------------------------------------------------*/

.jp-FileBrowser-toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  padding-left: 0;
  padding-right: 2px;
  align-items: center;
  height: unset;
}

.jp-FileBrowser-toolbar > .jp-Toolbar-item .jp-ToolbarButtonComponent {
  width: 40px;
}

/*-----------------------------------------------------------------------------
| Other styles
|----------------------------------------------------------------------------*/

.jp-FileDialog.jp-mod-conflict input {
  color: var(--jp-error-color1);
}

.jp-FileDialog .jp-new-name-title {
  margin-top: 12px;
}

.jp-LastModified-hidden {
  display: none;
}

.jp-FileSize-hidden {
  display: none;
}

.jp-FileBrowser .lm-AccordionPanel > h3:first-child {
  display: none;
}

/*-----------------------------------------------------------------------------
| DirListing
|----------------------------------------------------------------------------*/

.jp-DirListing {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
  outline: 0;
}

.jp-DirListing-header {
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  align-items: center;
  overflow: hidden;
  border-top: var(--jp-border-width) solid var(--jp-border-color2);
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
}

.jp-DirListing-headerItem {
  padding: 4px 12px 2px;
  font-weight: 500;
}

.jp-DirListing-headerItem:hover {
  background: var(--jp-layout-color2);
}

.jp-DirListing-headerItem.jp-id-name {
  flex: 1 0 84px;
}

.jp-DirListing-headerItem.jp-id-modified {
  flex: 0 0 112px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-DirListing-headerItem.jp-id-filesize {
  flex: 0 0 75px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-id-narrow {
  display: none;
  flex: 0 0 5px;
  padding: 4px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
  color: var(--jp-border-color2);
}

.jp-DirListing-narrow .jp-id-narrow {
  display: block;
}

.jp-DirListing-narrow .jp-id-modified,
.jp-DirListing-narrow .jp-DirListing-itemModified {
  display: none;
}

.jp-DirListing-headerItem.jp-mod-selected {
  font-weight: 600;
}

/* increase specificity to override bundled default */
.jp-DirListing-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  list-style-type: none;
  overflow: auto;
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-content mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.jp-DirListing-content .jp-DirListing-item.jp-mod-selected mark {
  color: var(--jp-ui-inverse-font-color0);
}

/* Style the directory listing content when a user drops a file to upload */
.jp-DirListing.jp-mod-native-drop .jp-DirListing-content {
  outline: 5px dashed rgba(128, 128, 128, 0.5);
  outline-offset: -10px;
  cursor: copy;
}

.jp-DirListing-item {
  display: flex;
  flex-direction: row;
  align-items: center;
  padding: 4px 12px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-DirListing-checkboxWrapper {
  /* Increases hit area of checkbox. */
  padding: 4px;
}

.jp-DirListing-header
  .jp-DirListing-checkboxWrapper
  + .jp-DirListing-headerItem {
  padding-left: 4px;
}

.jp-DirListing-content .jp-DirListing-checkboxWrapper {
  position: relative;
  left: -4px;
  margin: -4px 0 -4px -8px;
}

.jp-DirListing-checkboxWrapper.jp-mod-visible {
  visibility: visible;
}

/* For devices that support hovering, hide checkboxes until hovered, selected...
*/
@media (hover: hover) {
  .jp-DirListing-checkboxWrapper {
    visibility: hidden;
  }

  .jp-DirListing-item:hover .jp-DirListing-checkboxWrapper,
  .jp-DirListing-item.jp-mod-selected .jp-DirListing-checkboxWrapper {
    visibility: visible;
  }
}

.jp-DirListing-item[data-is-dot] {
  opacity: 75%;
}

.jp-DirListing-item.jp-mod-selected {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.jp-DirListing-item.jp-mod-dropTarget {
  background: var(--jp-brand-color3);
}

.jp-DirListing-item:hover:not(.jp-mod-selected) {
  background: var(--jp-layout-color2);
}

.jp-DirListing-itemIcon {
  flex: 0 0 20px;
  margin-right: 4px;
}

.jp-DirListing-itemText {
  flex: 1 0 64px;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  user-select: none;
}

.jp-DirListing-itemText:focus {
  outline-width: 2px;
  outline-color: var(--jp-inverse-layout-color1);
  outline-style: solid;
  outline-offset: 1px;
}

.jp-DirListing-item.jp-mod-selected .jp-DirListing-itemText:focus {
  outline-color: var(--jp-layout-color1);
}

.jp-DirListing-itemModified {
  flex: 0 0 125px;
  text-align: right;
}

.jp-DirListing-itemFileSize {
  flex: 0 0 90px;
  text-align: right;
}

.jp-DirListing-editor {
  flex: 1 0 64px;
  outline: none;
  border: none;
  color: var(--jp-ui-font-color1);
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon::before {
  color: var(--jp-success-color1);
  content: '\25CF';
  font-size: 8px;
  position: absolute;
  left: -8px;
}

.jp-DirListing-item.jp-mod-running.jp-mod-selected
  .jp-DirListing-itemIcon::before {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-DirListing-item.lm-mod-drag-image,
.jp-DirListing-item.jp-mod-selected.lm-mod-drag-image {
  font-size: var(--jp-ui-font-size1);
  padding-left: 4px;
  margin-left: 4px;
  width: 160px;
  background-color: var(--jp-ui-inverse-font-color2);
  box-shadow: var(--jp-elevation-z2);
  border-radius: 0;
  color: var(--jp-ui-font-color1);
  transform: translateX(-40%) translateY(-58%);
}

.jp-Document {
  min-width: 120px;
  min-height: 120px;
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Main OutputArea
| OutputArea has a list of Outputs
|----------------------------------------------------------------------------*/

.jp-OutputArea {
  overflow-y: auto;
}

.jp-OutputArea-child {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-OutputPrompt {
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-outprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
  opacity: var(--jp-cell-prompt-opacity);

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-OutputArea-prompt {
  display: table-cell;
  vertical-align: top;
}

.jp-OutputArea-output {
  display: table-cell;
  width: 100%;
  height: auto;
  overflow: auto;
  user-select: text;
  -moz-user-select: text;
  -webkit-user-select: text;
  -ms-user-select: text;
}

.jp-OutputArea .jp-RenderedText {
  padding-left: 1ch;
}

/**
 * Prompt overlay.
 */

.jp-OutputArea-promptOverlay {
  position: absolute;
  top: 0;
  width: var(--jp-cell-prompt-width);
  height: 100%;
  opacity: 0.5;
}

.jp-OutputArea-promptOverlay:hover {
  background: var(--jp-layout-color2);
  box-shadow: inset 0 0 1px var(--jp-inverse-layout-color0);
  cursor: zoom-out;
}

.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay:hover {
  cursor: zoom-in;
}

/**
 * Isolated output.
 */
.jp-OutputArea-output.jp-mod-isolated {
  width: 100%;
  display: block;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated {
  position: relative;
}

body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/* pre */

.jp-OutputArea-output pre {
  border: none;
  margin: 0;
  padding: 0;
  overflow-x: auto;
  overflow-y: auto;
  word-break: break-all;
  word-wrap: break-word;
  white-space: pre-wrap;
}

/* tables */

.jp-OutputArea-output.jp-RenderedHTMLCommon table {
  margin-left: 0;
  margin-right: 0;
}

/* description lists */

.jp-OutputArea-output dl,
.jp-OutputArea-output dt,
.jp-OutputArea-output dd {
  display: block;
}

.jp-OutputArea-output dl {
  width: 100%;
  overflow: hidden;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dt {
  font-weight: bold;
  float: left;
  width: 20%;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dd {
  float: left;
  width: 80%;
  padding: 0;
  margin: 0;
}

.jp-TrimmedOutputs pre {
  background: var(--jp-layout-color3);
  font-size: calc(var(--jp-code-font-size) * 1.4);
  text-align: center;
  text-transform: uppercase;
}

/* Hide the gutter in case of
 *  - nested output areas (e.g. in the case of output widgets)
 *  - mirrored output areas
 */
.jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt {
  display: none;
}

/* Hide empty lines in the output area, for instance due to cleared widgets */
.jp-OutputArea-prompt:empty {
  padding: 0;
  border: 0;
}

/*-----------------------------------------------------------------------------
| executeResult is added to any Output-result for the display of the object
| returned by a cell
|----------------------------------------------------------------------------*/

.jp-OutputArea-output.jp-OutputArea-executeResult {
  margin-left: 0;
  width: 100%;
}

/* Text output with the Out[] prompt needs a top padding to match the
 * alignment of the Out[] prompt itself.
 */
.jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output {
  padding-top: var(--jp-code-padding);
  border-top: var(--jp-border-width) solid transparent;
}

/*-----------------------------------------------------------------------------
| The Stdin output
|----------------------------------------------------------------------------*/

.jp-Stdin-prompt {
  color: var(--jp-content-font-color0);
  padding-right: var(--jp-code-padding);
  vertical-align: baseline;
  flex: 0 0 auto;
}

.jp-Stdin-input {
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  color: inherit;
  background-color: inherit;
  width: 42%;
  min-width: 200px;

  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;

  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0 0.25em;
  margin: 0 0.25em;
  flex: 0 0 70%;
}

.jp-Stdin-input::placeholder {
  opacity: 0;
}

.jp-Stdin-input:focus {
  box-shadow: none;
}

.jp-Stdin-input:focus::placeholder {
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Output Area View
|----------------------------------------------------------------------------*/

.jp-LinkedOutputView .jp-OutputArea {
  height: 100%;
  display: block;
}

.jp-LinkedOutputView .jp-OutputArea-output:only-child {
  height: 100%;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

@media print {
  .jp-OutputArea-child {
    break-inside: avoid-page;
  }
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-OutputPrompt {
    display: table-row;
    text-align: left;
  }

  .jp-OutputArea-child .jp-OutputArea-output {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }
}

/* Trimmed outputs warning */
.jp-TrimmedOutputs > a {
  margin: 10px;
  text-decoration: none;
  cursor: pointer;
}

.jp-TrimmedOutputs > a:hover {
  text-decoration: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Table of Contents
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toc-active-width: 4px;
}

.jp-TableOfContents {
  display: flex;
  flex-direction: column;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  height: 100%;
}

.jp-TableOfContents-placeholder {
  text-align: center;
}

.jp-TableOfContents-placeholderContent {
  color: var(--jp-content-font-color2);
  padding: 8px;
}

.jp-TableOfContents-placeholderContent > h3 {
  margin-bottom: var(--jp-content-heading-margin-bottom);
}

.jp-TableOfContents .jp-SidePanel-content {
  overflow-y: auto;
}

.jp-TableOfContents-tree {
  margin: 4px;
}

.jp-TableOfContents ol {
  list-style-type: none;
}

/* stylelint-disable-next-line selector-max-type */
.jp-TableOfContents li > ol {
  /* Align left border with triangle icon center */
  padding-left: 11px;
}

.jp-TableOfContents-content {
  /* left margin for the active heading indicator */
  margin: 0 0 0 var(--jp-private-toc-active-width);
  padding: 0;
  background-color: var(--jp-layout-color1);
}

.jp-tocItem {
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-tocItem-heading {
  display: flex;
  cursor: pointer;
}

.jp-tocItem-heading:hover {
  background-color: var(--jp-layout-color2);
}

.jp-tocItem-content {
  display: block;
  padding: 4px 0;
  white-space: nowrap;
  text-overflow: ellipsis;
  overflow-x: hidden;
}

.jp-tocItem-collapser {
  height: 20px;
  margin: 2px 2px 0;
  padding: 0;
  background: none;
  border: none;
  cursor: pointer;
}

.jp-tocItem-collapser:hover {
  background-color: var(--jp-layout-color3);
}

/* Active heading indicator */

.jp-tocItem-heading::before {
  content: ' ';
  background: transparent;
  width: var(--jp-private-toc-active-width);
  height: 24px;
  position: absolute;
  left: 0;
  border-radius: var(--jp-border-radius);
}

.jp-tocItem-heading.jp-tocItem-active::before {
  background-color: var(--jp-brand-color1);
}

.jp-tocItem-heading:hover.jp-tocItem-active::before {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapser {
  flex: 0 0 var(--jp-cell-collapser-width);
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
  border-radius: var(--jp-border-radius);
  opacity: 1;
}

.jp-Collapser-child {
  display: block;
  width: 100%;
  box-sizing: border-box;

  /* height: 100% doesn't work because the height of its parent is computed from content */
  position: absolute;
  top: 0;
  bottom: 0;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Hiding collapsers in print mode.

Note: input and output wrappers have "display: block" propery in print mode.
*/

@media print {
  .jp-Collapser {
    display: none;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Header/Footer
|----------------------------------------------------------------------------*/

/* Hidden by zero height by default */
.jp-CellHeader,
.jp-CellFooter {
  height: 0;
  width: 100%;
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Input
|----------------------------------------------------------------------------*/

/* All input areas */
.jp-InputArea {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-InputArea-editor {
  display: table-cell;
  overflow: hidden;
  vertical-align: top;

  /* This is the non-active, default styling */
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  background: var(--jp-cell-editor-background);
}

.jp-InputPrompt {
  display: table-cell;
  vertical-align: top;
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-inprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  opacity: var(--jp-cell-prompt-opacity);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-InputArea-editor {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }

  .jp-InputPrompt {
    display: table-row;
    text-align: left;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Placeholder {
  display: table;
  table-layout: fixed;
  width: 100%;
}

.jp-Placeholder-prompt {
  display: table-cell;
  box-sizing: border-box;
}

.jp-Placeholder-content {
  display: table-cell;
  padding: 4px 6px;
  border: 1px solid transparent;
  border-radius: 0;
  background: none;
  box-sizing: border-box;
  cursor: pointer;
}

.jp-Placeholder-contentContainer {
  display: flex;
}

.jp-Placeholder-content:hover,
.jp-InputPlaceholder > .jp-Placeholder-content:hover {
  border-color: var(--jp-layout-color3);
}

.jp-Placeholder-content .jp-MoreHorizIcon {
  width: 32px;
  height: 16px;
  border: 1px solid transparent;
  border-radius: var(--jp-border-radius);
}

.jp-Placeholder-content .jp-MoreHorizIcon:hover {
  border: 1px solid var(--jp-border-color1);
  box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.25);
  background-color: var(--jp-layout-color0);
}

.jp-PlaceholderText {
  white-space: nowrap;
  overflow-x: hidden;
  color: var(--jp-inverse-layout-color3);
  font-family: var(--jp-code-font-family);
}

.jp-InputPlaceholder > .jp-Placeholder-content {
  border-color: var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-cell-scrolling-output-offset: 5px;
}

/*-----------------------------------------------------------------------------
| Cell
|----------------------------------------------------------------------------*/

.jp-Cell {
  padding: var(--jp-cell-padding);
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Common input/output
|----------------------------------------------------------------------------*/

.jp-Cell-inputWrapper,
.jp-Cell-outputWrapper {
  display: flex;
  flex-direction: row;
  padding: 0;
  margin: 0;

  /* Added to reveal the box-shadow on the input and output collapsers. */
  overflow: visible;
}

/* Only input/output areas inside cells */
.jp-Cell-inputArea,
.jp-Cell-outputArea {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Collapser
|----------------------------------------------------------------------------*/

/* Make the output collapser disappear when there is not output, but do so
 * in a manner that leaves it in the layout and preserves its width.
 */
.jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser {
  border: none !important;
  background: transparent !important;
}

.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser {
  min-height: var(--jp-cell-collapser-min-height);
}

/*-----------------------------------------------------------------------------
| Output
|----------------------------------------------------------------------------*/

/* Put a space between input and output when there IS output */
.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper {
  margin-top: 5px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea {
  overflow-y: auto;
  max-height: 24em;
  margin-left: var(--jp-private-cell-scrolling-output-offset);
  resize: vertical;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea[style*='height'] {
  max-height: unset;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea::after {
  content: ' ';
  box-shadow: inset 0 0 6px 2px rgb(0 0 0 / 30%);
  width: 100%;
  height: 100%;
  position: sticky;
  bottom: 0;
  top: 0;
  margin-top: -50%;
  float: left;
  display: block;
  pointer-events: none;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-child {
  padding-top: 6px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt {
  width: calc(
    var(--jp-cell-prompt-width) - var(--jp-private-cell-scrolling-output-offset)
  );
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay {
  left: calc(-1 * var(--jp-private-cell-scrolling-output-offset));
}

/*-----------------------------------------------------------------------------
| CodeCell
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| MarkdownCell
|----------------------------------------------------------------------------*/

.jp-MarkdownOutput {
  display: table-cell;
  width: 100%;
  margin-top: 0;
  margin-bottom: 0;
  padding-left: var(--jp-code-padding);
}

.jp-MarkdownOutput.jp-RenderedHTMLCommon {
  overflow: auto;
}

/* collapseHeadingButton (show always if hiddenCellsButton is _not_ shown) */
.jp-collapseHeadingButton {
  display: flex;
  min-height: var(--jp-cell-collapser-min-height);
  font-size: var(--jp-code-font-size);
  position: absolute;
  background-color: transparent;
  background-size: 25px;
  background-repeat: no-repeat;
  background-position-x: center;
  background-position-y: top;
  background-image: var(--jp-icon-caret-down);
  right: 0;
  top: 0;
  bottom: 0;
}

.jp-collapseHeadingButton.jp-mod-collapsed {
  background-image: var(--jp-icon-caret-right);
}

/*
 set the container font size to match that of content
 so that the nested collapse buttons have the right size
*/
.jp-MarkdownCell .jp-InputPrompt {
  font-size: var(--jp-content-font-size1);
}

/*
  Align collapseHeadingButton with cell top header
  The font sizes are identical to the ones in packages/rendermime/style/base.css
*/
.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='1'] {
  font-size: var(--jp-content-font-size5);
  background-position-y: calc(0.3 * var(--jp-content-font-size5));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='2'] {
  font-size: var(--jp-content-font-size4);
  background-position-y: calc(0.3 * var(--jp-content-font-size4));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='3'] {
  font-size: var(--jp-content-font-size3);
  background-position-y: calc(0.3 * var(--jp-content-font-size3));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='4'] {
  font-size: var(--jp-content-font-size2);
  background-position-y: calc(0.3 * var(--jp-content-font-size2));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='5'] {
  font-size: var(--jp-content-font-size1);
  background-position-y: top;
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='6'] {
  font-size: var(--jp-content-font-size0);
  background-position-y: top;
}

/* collapseHeadingButton (show only on (hover,active) if hiddenCellsButton is shown) */
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-collapseHeadingButton {
  display: none;
}

.jp-Notebook.jp-mod-showHiddenCellsButton
  :is(.jp-MarkdownCell:hover, .jp-mod-active)
  .jp-collapseHeadingButton {
  display: flex;
}

/* showHiddenCellsButton (only show if jp-mod-showHiddenCellsButton is set, which
is a consequence of the showHiddenCellsButton option in Notebook Settings)*/
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton {
  margin-left: calc(var(--jp-cell-prompt-width) + 2 * var(--jp-code-padding));
  margin-top: var(--jp-code-padding);
  border: 1px solid var(--jp-border-color2);
  background-color: var(--jp-border-color3) !important;
  color: var(--jp-content-font-color0) !important;
  display: flex;
}

.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton:hover {
  background-color: var(--jp-border-color2) !important;
}

.jp-showHiddenCellsButton {
  display: none;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Using block instead of flex to allow the use of the break-inside CSS property for
cell outputs.
*/

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-notebook-toolbar-padding: 2px 5px 2px 2px;
}

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-NotebookPanel-toolbar {
  padding: var(--jp-notebook-toolbar-padding);

  /* disable paint containment from lumino 2.0 default strict CSS containment */
  contain: style size !important;
}

.jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused {
  border: none;
  box-shadow: none;
}

.jp-Notebook-toolbarCellTypeDropdown select {
  height: 24px;
  font-size: var(--jp-ui-font-size1);
  line-height: 14px;
  border-radius: 0;
  display: block;
}

.jp-Notebook-toolbarCellTypeDropdown span {
  top: 5px !important;
}

.jp-Toolbar-responsive-popup {
  position: absolute;
  height: fit-content;
  display: flex;
  flex-direction: row;
  flex-wrap: wrap;
  justify-content: flex-end;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: var(--jp-notebook-toolbar-padding);
  z-index: 1;
  right: 0;
  top: 0;
}

.jp-Toolbar > .jp-Toolbar-responsive-opener {
  margin-left: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-Notebook-ExecutionIndicator {
  position: relative;
  display: inline-block;
  height: 100%;
  z-index: 9997;
}

.jp-Notebook-ExecutionIndicator-tooltip {
  visibility: hidden;
  height: auto;
  width: max-content;
  width: -moz-max-content;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color1);
  text-align: justify;
  border-radius: 6px;
  padding: 0 5px;
  position: fixed;
  display: table;
}

.jp-Notebook-ExecutionIndicator-tooltip.up {
  transform: translateX(-50%) translateY(-100%) translateY(-32px);
}

.jp-Notebook-ExecutionIndicator-tooltip.down {
  transform: translateX(calc(-100% + 16px)) translateY(5px);
}

.jp-Notebook-ExecutionIndicator-tooltip.hidden {
  display: none;
}

.jp-Notebook-ExecutionIndicator:hover .jp-Notebook-ExecutionIndicator-tooltip {
  visibility: visible;
}

.jp-Notebook-ExecutionIndicator span {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  color: var(--jp-ui-font-color1);
  line-height: 24px;
  display: block;
}

.jp-Notebook-ExecutionIndicator-progress-bar {
  display: flex;
  justify-content: center;
  height: 100%;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*
 * Execution indicator
 */
.jp-tocItem-content::after {
  content: '';

  /* Must be identical to form a circle */
  width: 12px;
  height: 12px;
  background: none;
  border: none;
  position: absolute;
  right: 0;
}

.jp-tocItem-content[data-running='0']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background: none;
}

.jp-tocItem-content[data-running='1']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background-color: var(--jp-inverse-layout-color3);
}

.jp-tocItem-content[data-running='0'],
.jp-tocItem-content[data-running='1'] {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Notebook-footer {
  height: 27px;
  margin-left: calc(
    var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
      var(--jp-cell-padding)
  );
  width: calc(
    100% -
      (
        var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
          var(--jp-cell-padding) + var(--jp-cell-padding)
      )
  );
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  color: var(--jp-ui-font-color3);
  margin-top: 6px;
  background: none;
  cursor: pointer;
}

.jp-Notebook-footer:focus {
  border-color: var(--jp-cell-editor-active-border-color);
}

/* For devices that support hovering, hide footer until hover */
@media (hover: hover) {
  .jp-Notebook-footer {
    opacity: 0;
  }

  .jp-Notebook-footer:focus,
  .jp-Notebook-footer:hover {
    opacity: 1;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Imports
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-side-by-side-output-size: 1fr;
  --jp-side-by-side-resized-cell: var(--jp-side-by-side-output-size);
  --jp-private-notebook-dragImage-width: 304px;
  --jp-private-notebook-dragImage-height: 36px;
  --jp-private-notebook-selected-color: var(--md-blue-400);
  --jp-private-notebook-active-color: var(--md-green-400);
}

/*-----------------------------------------------------------------------------
| Notebook
|----------------------------------------------------------------------------*/

/* stylelint-disable selector-max-class */

.jp-NotebookPanel {
  display: block;
  height: 100%;
}

.jp-NotebookPanel.jp-Document {
  min-width: 240px;
  min-height: 120px;
}

.jp-Notebook {
  padding: var(--jp-notebook-padding);
  outline: none;
  overflow: auto;
  background: var(--jp-layout-color0);
}

.jp-Notebook.jp-mod-scrollPastEnd::after {
  display: block;
  content: '';
  min-height: var(--jp-notebook-scroll-padding);
}

.jp-MainAreaWidget-ContainStrict .jp-Notebook * {
  contain: strict;
}

.jp-Notebook .jp-Cell {
  overflow: visible;
}

.jp-Notebook .jp-Cell .jp-InputPrompt {
  cursor: move;
}

/*-----------------------------------------------------------------------------
| Notebook state related styling
|
| The notebook and cells each have states, here are the possibilities:
|
| - Notebook
|   - Command
|   - Edit
| - Cell
|   - None
|   - Active (only one can be active)
|   - Selected (the cells actions are applied to)
|   - Multiselected (when multiple selected, the cursor)
|   - No outputs
|----------------------------------------------------------------------------*/

/* Command or edit modes */

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

/* cell is active */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser {
  background: var(--jp-brand-color1);
}

/* cell is dirty */
.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt {
  color: var(--jp-warn-color1);
}

.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt::before {
  color: var(--jp-warn-color1);
  content: '•';
}

.jp-Notebook .jp-Cell.jp-mod-active.jp-mod-dirty .jp-Collapser {
  background: var(--jp-warn-color1);
}

/* collapser is hovered */
.jp-Notebook .jp-Cell .jp-Collapser:hover {
  box-shadow: var(--jp-elevation-z2);
  background: var(--jp-brand-color1);
  opacity: var(--jp-cell-collapser-not-active-hover-opacity);
}

/* cell is active and collapser is hovered */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/* Command mode */

.jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected {
  background: var(--jp-notebook-multiselected-color);
}

.jp-Notebook.jp-mod-commandMode
  .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected) {
  background: transparent;
}

/* Edit mode */

.jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-cell-editor-active-background);
}

/*-----------------------------------------------------------------------------
| Notebook drag and drop
|----------------------------------------------------------------------------*/

.jp-Notebook-cell.jp-mod-dropSource {
  opacity: 0.5;
}

.jp-Notebook-cell.jp-mod-dropTarget,
.jp-Notebook.jp-mod-commandMode
  .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget {
  border-top-color: var(--jp-private-notebook-selected-color);
  border-top-style: solid;
  border-top-width: 2px;
}

.jp-dragImage {
  display: block;
  flex-direction: row;
  width: var(--jp-private-notebook-dragImage-width);
  height: var(--jp-private-notebook-dragImage-height);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
  overflow: visible;
}

.jp-dragImage-singlePrompt {
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

.jp-dragImage .jp-dragImage-content {
  flex: 1 1 auto;
  z-index: 2;
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  line-height: var(--jp-code-line-height);
  padding: var(--jp-code-padding);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background-color);
  color: var(--jp-content-font-color3);
  text-align: left;
  margin: 4px 4px 4px 0;
}

.jp-dragImage .jp-dragImage-prompt {
  flex: 0 0 auto;
  min-width: 36px;
  color: var(--jp-cell-inprompt-font-color);
  padding: var(--jp-code-padding);
  padding-left: 12px;
  font-family: var(--jp-cell-prompt-font-family);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: 1.9;
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
}

.jp-dragImage-multipleBack {
  z-index: -1;
  position: absolute;
  height: 32px;
  width: 300px;
  top: 8px;
  left: 8px;
  background: var(--jp-layout-color2);
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

/*-----------------------------------------------------------------------------
| Cell toolbar
|----------------------------------------------------------------------------*/

.jp-NotebookTools {
  display: block;
  min-width: var(--jp-sidebar-min-width);
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
    * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  overflow: auto;
}

.jp-ActiveCellTool {
  padding: 12px 0;
  display: flex;
}

.jp-ActiveCellTool-Content {
  flex: 1 1 auto;
}

.jp-ActiveCellTool .jp-ActiveCellTool-CellContent {
  background: var(--jp-cell-editor-background);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  min-height: 29px;
}

.jp-ActiveCellTool .jp-InputPrompt {
  min-width: calc(var(--jp-cell-prompt-width) * 0.75);
}

.jp-ActiveCellTool-CellContent > pre {
  padding: 5px 4px;
  margin: 0;
  white-space: normal;
}

.jp-MetadataEditorTool {
  flex-direction: column;
  padding: 12px 0;
}

.jp-RankedPanel > :not(:first-child) {
  margin-top: 12px;
}

.jp-KeySelector select.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: var(--jp-border-width) solid var(--jp-border-color1);
}

.jp-KeySelector label,
.jp-MetadataEditorTool label,
.jp-NumberSetter label {
  line-height: 1.4;
}

.jp-NotebookTools .jp-select-wrapper {
  margin-top: 4px;
  margin-bottom: 0;
}

.jp-NumberSetter input {
  width: 100%;
  margin-top: 4px;
}

.jp-NotebookTools .jp-Collapse {
  margin-top: 16px;
}

/*-----------------------------------------------------------------------------
| Presentation Mode (.jp-mod-presentationMode)
|----------------------------------------------------------------------------*/

.jp-mod-presentationMode .jp-Notebook {
  --jp-content-font-size1: var(--jp-content-presentation-font-size1);
  --jp-code-font-size: var(--jp-code-presentation-font-size);
}

.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt,
.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt {
  flex: 0 0 110px;
}

/*-----------------------------------------------------------------------------
| Side-by-side Mode (.jp-mod-sideBySide)
|----------------------------------------------------------------------------*/
.jp-mod-sideBySide.jp-Notebook .jp-Notebook-cell {
  margin-top: 3em;
  margin-bottom: 3em;
  margin-left: 5%;
  margin-right: 5%;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell {
  display: grid;
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-output-size)
    );
  grid-template-rows: auto minmax(0, 1fr) auto;
  grid-template-areas:
    'header header header'
    'input handle output'
    'footer footer footer';
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell.jp-mod-resizedCell {
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-resized-cell)
    );
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellHeader {
  grid-area: header;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-inputWrapper {
  grid-area: input;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-outputWrapper {
  /* overwrite the default margin (no vertical separation needed in side by side move */
  margin-top: 0;
  grid-area: output;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellFooter {
  grid-area: footer;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle {
  grid-area: handle;
  user-select: none;
  display: block;
  height: 100%;
  cursor: ew-resize;
  padding: 0 var(--jp-cell-padding);
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle::after {
  content: '';
  display: block;
  background: var(--jp-border-color2);
  height: 100%;
  width: 5px;
}

.jp-mod-sideBySide.jp-Notebook
  .jp-CodeCell.jp-mod-resizedCell
  .jp-CellResizeHandle::after {
  background: var(--jp-border-color0);
}

.jp-CellResizeHandle {
  display: none;
}

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Cell-Placeholder {
  padding-left: 55px;
}

.jp-Cell-Placeholder-wrapper {
  background: #fff;
  border: 1px solid;
  border-color: #e5e6e9 #dfe0e4 #d0d1d5;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  margin: 10px 15px;
}

.jp-Cell-Placeholder-wrapper-inner {
  padding: 15px;
  position: relative;
}

.jp-Cell-Placeholder-wrapper-body {
  background-repeat: repeat;
  background-size: 50% auto;
}

.jp-Cell-Placeholder-wrapper-body div {
  background: #f6f7f8;
  background-image: -webkit-linear-gradient(
    left,
    #f6f7f8 0%,
    #edeef1 20%,
    #f6f7f8 40%,
    #f6f7f8 100%
  );
  background-repeat: no-repeat;
  background-size: 800px 104px;
  height: 104px;
  position: absolute;
  right: 15px;
  left: 15px;
  top: 15px;
}

div.jp-Cell-Placeholder-h1 {
  top: 20px;
  height: 20px;
  left: 15px;
  width: 150px;
}

div.jp-Cell-Placeholder-h2 {
  left: 15px;
  top: 50px;
  height: 10px;
  width: 100px;
}

div.jp-Cell-Placeholder-content-1,
div.jp-Cell-Placeholder-content-2,
div.jp-Cell-Placeholder-content-3 {
  left: 15px;
  right: 15px;
  height: 10px;
}

div.jp-Cell-Placeholder-content-1 {
  top: 100px;
}

div.jp-Cell-Placeholder-content-2 {
  top: 120px;
}

div.jp-Cell-Placeholder-content-3 {
  top: 140px;
}

</style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
The following CSS variables define the main, public API for styling JupyterLab.
These variables should be used by all plugins wherever possible. In other
words, plugins should not define custom colors, sizes, etc unless absolutely
necessary. This enables users to change the visual theme of JupyterLab
by changing these variables.

Many variables appear in an ordered sequence (0,1,2,3). These sequences
are designed to work well together, so for example, `--jp-border-color1` should
be used with `--jp-layout-color1`. The numbers have the following meanings:

* 0: super-primary, reserved for special emphasis
* 1: primary, most important under normal situations
* 2: secondary, next most important under normal situations
* 3: tertiary, next most important under normal situations

Throughout JupyterLab, we are mostly following principles from Google's
Material Design when selecting colors. We are not, however, following
all of MD as it is not optimized for dense, information rich UIs.
*/

:root {
  /* Elevation
   *
   * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here:
   *
   * https://github.com/material-components/material-components-web
   * https://material-components-web.appspot.com/elevation.html
   */

  --jp-shadow-base-lightness: 0;
  --jp-shadow-umbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.2
  );
  --jp-shadow-penumbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.14
  );
  --jp-shadow-ambient-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.12
  );
  --jp-elevation-z0: none;
  --jp-elevation-z1: 0 2px 1px -1px var(--jp-shadow-umbra-color),
    0 1px 1px 0 var(--jp-shadow-penumbra-color),
    0 1px 3px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z2: 0 3px 1px -2px var(--jp-shadow-umbra-color),
    0 2px 2px 0 var(--jp-shadow-penumbra-color),
    0 1px 5px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z4: 0 2px 4px -1px var(--jp-shadow-umbra-color),
    0 4px 5px 0 var(--jp-shadow-penumbra-color),
    0 1px 10px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z6: 0 3px 5px -1px var(--jp-shadow-umbra-color),
    0 6px 10px 0 var(--jp-shadow-penumbra-color),
    0 1px 18px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z8: 0 5px 5px -3px var(--jp-shadow-umbra-color),
    0 8px 10px 1px var(--jp-shadow-penumbra-color),
    0 3px 14px 2px var(--jp-shadow-ambient-color);
  --jp-elevation-z12: 0 7px 8px -4px var(--jp-shadow-umbra-color),
    0 12px 17px 2px var(--jp-shadow-penumbra-color),
    0 5px 22px 4px var(--jp-shadow-ambient-color);
  --jp-elevation-z16: 0 8px 10px -5px var(--jp-shadow-umbra-color),
    0 16px 24px 2px var(--jp-shadow-penumbra-color),
    0 6px 30px 5px var(--jp-shadow-ambient-color);
  --jp-elevation-z20: 0 10px 13px -6px var(--jp-shadow-umbra-color),
    0 20px 31px 3px var(--jp-shadow-penumbra-color),
    0 8px 38px 7px var(--jp-shadow-ambient-color);
  --jp-elevation-z24: 0 11px 15px -7px var(--jp-shadow-umbra-color),
    0 24px 38px 3px var(--jp-shadow-penumbra-color),
    0 9px 46px 8px var(--jp-shadow-ambient-color);

  /* Borders
   *
   * The following variables, specify the visual styling of borders in JupyterLab.
   */

  --jp-border-width: 1px;
  --jp-border-color0: var(--md-grey-400);
  --jp-border-color1: var(--md-grey-400);
  --jp-border-color2: var(--md-grey-300);
  --jp-border-color3: var(--md-grey-200);
  --jp-inverse-border-color: var(--md-grey-600);
  --jp-border-radius: 2px;

  /* UI Fonts
   *
   * The UI font CSS variables are used for the typography all of the JupyterLab
   * user interface elements that are not directly user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-ui-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-ui-font-scale-factor: 1.2;
  --jp-ui-font-size0: 0.83333em;
  --jp-ui-font-size1: 13px; /* Base font size */
  --jp-ui-font-size2: 1.2em;
  --jp-ui-font-size3: 1.44em;
  --jp-ui-font-family: system-ui, -apple-system, blinkmacsystemfont, 'Segoe UI',
    helvetica, arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji',
    'Segoe UI Symbol';

  /*
   * Use these font colors against the corresponding main layout colors.
   * In a light theme, these go from dark to light.
   */

  /* Defaults use Material Design specification */
  --jp-ui-font-color0: rgba(0, 0, 0, 1);
  --jp-ui-font-color1: rgba(0, 0, 0, 0.87);
  --jp-ui-font-color2: rgba(0, 0, 0, 0.54);
  --jp-ui-font-color3: rgba(0, 0, 0, 0.38);

  /*
   * Use these against the brand/accent/warn/error colors.
   * These will typically go from light to darker, in both a dark and light theme.
   */

  --jp-ui-inverse-font-color0: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color1: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color2: rgba(255, 255, 255, 0.7);
  --jp-ui-inverse-font-color3: rgba(255, 255, 255, 0.5);

  /* Content Fonts
   *
   * Content font variables are used for typography of user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-content-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-content-line-height: 1.6;
  --jp-content-font-scale-factor: 1.2;
  --jp-content-font-size0: 0.83333em;
  --jp-content-font-size1: 14px; /* Base font size */
  --jp-content-font-size2: 1.2em;
  --jp-content-font-size3: 1.44em;
  --jp-content-font-size4: 1.728em;
  --jp-content-font-size5: 2.0736em;

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-content-presentation-font-size1: 17px;
  --jp-content-heading-line-height: 1;
  --jp-content-heading-margin-top: 1.2em;
  --jp-content-heading-margin-bottom: 0.8em;
  --jp-content-heading-font-weight: 500;

  /* Defaults use Material Design specification */
  --jp-content-font-color0: rgba(0, 0, 0, 1);
  --jp-content-font-color1: rgba(0, 0, 0, 0.87);
  --jp-content-font-color2: rgba(0, 0, 0, 0.54);
  --jp-content-font-color3: rgba(0, 0, 0, 0.38);
  --jp-content-link-color: var(--md-blue-900);
  --jp-content-font-family: system-ui, -apple-system, blinkmacsystemfont,
    'Segoe UI', helvetica, arial, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol';

  /*
   * Code Fonts
   *
   * Code font variables are used for typography of code and other monospaces content.
   */

  --jp-code-font-size: 13px;
  --jp-code-line-height: 1.3077; /* 17px for 13px base */
  --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */
  --jp-code-font-family-default: menlo, consolas, 'DejaVu Sans Mono', monospace;
  --jp-code-font-family: var(--jp-code-font-family-default);

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-code-presentation-font-size: 16px;

  /* may need to tweak cursor width if you change font size */
  --jp-code-cursor-width0: 1.4px;
  --jp-code-cursor-width1: 2px;
  --jp-code-cursor-width2: 4px;

  /* Layout
   *
   * The following are the main layout colors use in JupyterLab. In a light
   * theme these would go from light to dark.
   */

  --jp-layout-color0: white;
  --jp-layout-color1: white;
  --jp-layout-color2: var(--md-grey-200);
  --jp-layout-color3: var(--md-grey-400);
  --jp-layout-color4: var(--md-grey-600);

  /* Inverse Layout
   *
   * The following are the inverse layout colors use in JupyterLab. In a light
   * theme these would go from dark to light.
   */

  --jp-inverse-layout-color0: #111;
  --jp-inverse-layout-color1: var(--md-grey-900);
  --jp-inverse-layout-color2: var(--md-grey-800);
  --jp-inverse-layout-color3: var(--md-grey-700);
  --jp-inverse-layout-color4: var(--md-grey-600);

  /* Brand/accent */

  --jp-brand-color0: var(--md-blue-900);
  --jp-brand-color1: var(--md-blue-700);
  --jp-brand-color2: var(--md-blue-300);
  --jp-brand-color3: var(--md-blue-100);
  --jp-brand-color4: var(--md-blue-50);
  --jp-accent-color0: var(--md-green-900);
  --jp-accent-color1: var(--md-green-700);
  --jp-accent-color2: var(--md-green-300);
  --jp-accent-color3: var(--md-green-100);

  /* State colors (warn, error, success, info) */

  --jp-warn-color0: var(--md-orange-900);
  --jp-warn-color1: var(--md-orange-700);
  --jp-warn-color2: var(--md-orange-300);
  --jp-warn-color3: var(--md-orange-100);
  --jp-error-color0: var(--md-red-900);
  --jp-error-color1: var(--md-red-700);
  --jp-error-color2: var(--md-red-300);
  --jp-error-color3: var(--md-red-100);
  --jp-success-color0: var(--md-green-900);
  --jp-success-color1: var(--md-green-700);
  --jp-success-color2: var(--md-green-300);
  --jp-success-color3: var(--md-green-100);
  --jp-info-color0: var(--md-cyan-900);
  --jp-info-color1: var(--md-cyan-700);
  --jp-info-color2: var(--md-cyan-300);
  --jp-info-color3: var(--md-cyan-100);

  /* Cell specific styles */

  --jp-cell-padding: 5px;
  --jp-cell-collapser-width: 8px;
  --jp-cell-collapser-min-height: 20px;
  --jp-cell-collapser-not-active-hover-opacity: 0.6;
  --jp-cell-editor-background: var(--md-grey-100);
  --jp-cell-editor-border-color: var(--md-grey-300);
  --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-cell-editor-active-background: var(--jp-layout-color0);
  --jp-cell-editor-active-border-color: var(--jp-brand-color1);
  --jp-cell-prompt-width: 64px;
  --jp-cell-prompt-font-family: var(--jp-code-font-family-default);
  --jp-cell-prompt-letter-spacing: 0;
  --jp-cell-prompt-opacity: 1;
  --jp-cell-prompt-not-active-opacity: 0.5;
  --jp-cell-prompt-not-active-font-color: var(--md-grey-700);

  /* A custom blend of MD grey and blue 600
   * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */
  --jp-cell-inprompt-font-color: #307fc1;

  /* A custom blend of MD grey and orange 600
   * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */
  --jp-cell-outprompt-font-color: #bf5b3d;

  /* Notebook specific styles */

  --jp-notebook-padding: 10px;
  --jp-notebook-select-background: var(--jp-layout-color1);
  --jp-notebook-multiselected-color: var(--md-blue-50);

  /* The scroll padding is calculated to fill enough space at the bottom of the
  notebook to show one single-line cell (with appropriate padding) at the top
  when the notebook is scrolled all the way to the bottom. We also subtract one
  pixel so that no scrollbar appears if we have just one single-line cell in the
  notebook. This padding is to enable a 'scroll past end' feature in a notebook.
  */
  --jp-notebook-scroll-padding: calc(
    100% - var(--jp-code-font-size) * var(--jp-code-line-height) -
      var(--jp-code-padding) - var(--jp-cell-padding) - 1px
  );

  /* Rendermime styles */

  --jp-rendermime-error-background: #fdd;
  --jp-rendermime-table-row-background: var(--md-grey-100);
  --jp-rendermime-table-row-hover-background: var(--md-light-blue-50);

  /* Dialog specific styles */

  --jp-dialog-background: rgba(0, 0, 0, 0.25);

  /* Console specific styles */

  --jp-console-padding: 10px;

  /* Toolbar specific styles */

  --jp-toolbar-border-color: var(--jp-border-color1);
  --jp-toolbar-micro-height: 8px;
  --jp-toolbar-background: var(--jp-layout-color1);
  --jp-toolbar-box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.24);
  --jp-toolbar-header-margin: 4px 4px 0 4px;
  --jp-toolbar-active-background: var(--md-grey-300);

  /* Statusbar specific styles */

  --jp-statusbar-height: 24px;

  /* Input field styles */

  --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-input-active-background: var(--jp-layout-color1);
  --jp-input-hover-background: var(--jp-layout-color1);
  --jp-input-background: var(--md-grey-100);
  --jp-input-border-color: var(--jp-inverse-border-color);
  --jp-input-active-border-color: var(--jp-brand-color1);
  --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3);

  /* General editor styles */

  --jp-editor-selected-background: #d9d9d9;
  --jp-editor-selected-focused-background: #d7d4f0;
  --jp-editor-cursor-color: var(--jp-ui-font-color0);

  /* Code mirror specific styles */

  --jp-mirror-editor-keyword-color: #008000;
  --jp-mirror-editor-atom-color: #88f;
  --jp-mirror-editor-number-color: #080;
  --jp-mirror-editor-def-color: #00f;
  --jp-mirror-editor-variable-color: var(--md-grey-900);
  --jp-mirror-editor-variable-2-color: rgb(0, 54, 109);
  --jp-mirror-editor-variable-3-color: #085;
  --jp-mirror-editor-punctuation-color: #05a;
  --jp-mirror-editor-property-color: #05a;
  --jp-mirror-editor-operator-color: #a2f;
  --jp-mirror-editor-comment-color: #408080;
  --jp-mirror-editor-string-color: #ba2121;
  --jp-mirror-editor-string-2-color: #708;
  --jp-mirror-editor-meta-color: #a2f;
  --jp-mirror-editor-qualifier-color: #555;
  --jp-mirror-editor-builtin-color: #008000;
  --jp-mirror-editor-bracket-color: #997;
  --jp-mirror-editor-tag-color: #170;
  --jp-mirror-editor-attribute-color: #00c;
  --jp-mirror-editor-header-color: blue;
  --jp-mirror-editor-quote-color: #090;
  --jp-mirror-editor-link-color: #00c;
  --jp-mirror-editor-error-color: #f00;
  --jp-mirror-editor-hr-color: #999;

  /*
    RTC user specific colors.
    These colors are used for the cursor, username in the editor,
    and the icon of the user.
  */

  --jp-collaborator-color1: #ffad8e;
  --jp-collaborator-color2: #dac83d;
  --jp-collaborator-color3: #72dd76;
  --jp-collaborator-color4: #00e4d0;
  --jp-collaborator-color5: #45d4ff;
  --jp-collaborator-color6: #e2b1ff;
  --jp-collaborator-color7: #ff9de6;

  /* Vega extension styles */

  --jp-vega-background: white;

  /* Sidebar-related styles */

  --jp-sidebar-min-width: 250px;

  /* Search-related styles */

  --jp-search-toggle-off-opacity: 0.5;
  --jp-search-toggle-hover-opacity: 0.8;
  --jp-search-toggle-on-opacity: 1;
  --jp-search-selected-match-background-color: rgb(245, 200, 0);
  --jp-search-selected-match-color: black;
  --jp-search-unselected-match-background-color: var(
    --jp-inverse-layout-color0
  );
  --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0);

  /* Icon colors that work well with light or dark backgrounds */
  --jp-icon-contrast-color0: var(--md-purple-600);
  --jp-icon-contrast-color1: var(--md-green-600);
  --jp-icon-contrast-color2: var(--md-pink-600);
  --jp-icon-contrast-color3: var(--md-blue-600);

  /* Button colors */
  --jp-accept-color-normal: var(--md-blue-700);
  --jp-accept-color-hover: var(--md-blue-800);
  --jp-accept-color-active: var(--md-blue-900);
  --jp-warn-color-normal: var(--md-red-700);
  --jp-warn-color-hover: var(--md-red-800);
  --jp-warn-color-active: var(--md-red-900);
  --jp-reject-color-normal: var(--md-grey-600);
  --jp-reject-color-hover: var(--md-grey-700);
  --jp-reject-color-active: var(--md-grey-800);

  /* File or activity icons and switch semantic variables */
  --jp-jupyter-icon-color: #f37626;
  --jp-notebook-icon-color: #f37626;
  --jp-json-icon-color: var(--md-orange-700);
  --jp-console-icon-background-color: var(--md-blue-700);
  --jp-console-icon-color: white;
  --jp-terminal-icon-background-color: var(--md-grey-800);
  --jp-terminal-icon-color: var(--md-grey-200);
  --jp-text-editor-icon-color: var(--md-grey-700);
  --jp-inspector-icon-color: var(--md-grey-700);
  --jp-switch-color: var(--md-grey-400);
  --jp-switch-true-position-color: var(--md-orange-900);
}
</style>
<style type="text/css">
/* Force rendering true colors when outputing to pdf */
* {
  -webkit-print-color-adjust: exact;
}

/* Misc */
a.anchor-link {
  display: none;
}

/* Input area styling */
.jp-InputArea {
  overflow: hidden;
}

.jp-InputArea-editor {
  overflow: hidden;
}

.cm-editor.cm-s-jupyter .highlight pre {
/* weird, but --jp-code-padding defined to be 5px but 4px horizontal padding is hardcoded for pre.cm-line */
  padding: var(--jp-code-padding) 4px;
  margin: 0;

  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
  color: inherit;

}

.jp-OutputArea-output pre {
  line-height: inherit;
  font-family: inherit;
}

.jp-RenderedText pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
}

/* Hiding the collapser by default */
.jp-Collapser {
  display: none;
}

@page {
    margin: 0.5in; /* Margin for each printed piece of paper */
}

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}
</style>
<!-- Load mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML-full,Safe"> </script>
<!-- MathJax configuration -->
<script type="text/x-mathjax-config">
    init_mathjax = function() {
        if (window.MathJax) {
        // MathJax loaded
            MathJax.Hub.Config({
                TeX: {
                    equationNumbers: {
                    autoNumber: "AMS",
                    useLabelIds: true
                    }
                },
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true,
                    processEnvironments: true
                },
                displayAlign: 'center',
                CommonHTML: {
                    linebreaks: {
                    automatic: true
                    }
                }
            });

            MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
        }
    }
    init_mathjax();
    </script>
<!-- End of mathjax configuration --><script type="module">
  document.addEventListener("DOMContentLoaded", async () => {
    const diagrams = document.querySelectorAll(".jp-Mermaid > pre.mermaid");
    // do not load mermaidjs if not needed
    if (!diagrams.length) {
      return;
    }
    const mermaid = (await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs")).default;
    const parser = new DOMParser();

    mermaid.initialize({
      maxTextSize: 100000,
      maxEdges: 100000,
      startOnLoad: false,
      fontFamily: window
        .getComputedStyle(document.body)
        .getPropertyValue("--jp-ui-font-family"),
      theme: document.querySelector("body[data-jp-theme-light='true']")
        ? "default"
        : "dark",
    });

    let _nextMermaidId = 0;

    function makeMermaidImage(svg) {
      const img = document.createElement("img");
      const doc = parser.parseFromString(svg, "image/svg+xml");
      const svgEl = doc.querySelector("svg");
      const { maxWidth } = svgEl?.style || {};
      const firstTitle = doc.querySelector("title");
      const firstDesc = doc.querySelector("desc");

      img.setAttribute("src", `data:image/svg+xml,${encodeURIComponent(svg)}`);
      if (maxWidth) {
        img.width = parseInt(maxWidth);
      }
      if (firstTitle) {
        img.setAttribute("alt", firstTitle.textContent);
      }
      if (firstDesc) {
        const caption = document.createElement("figcaption");
        caption.className = "sr-only";
        caption.textContent = firstDesc.textContent;
        return [img, caption];
      }
      return [img];
    }

    async function makeMermaidError(text) {
      let errorMessage = "";
      try {
        await mermaid.parse(text);
      } catch (err) {
        errorMessage = `${err}`;
      }

      const result = document.createElement("details");
      result.className = 'jp-RenderedMermaid-Details';
      const summary = document.createElement("summary");
      summary.className = 'jp-RenderedMermaid-Summary';
      const pre = document.createElement("pre");
      const code = document.createElement("code");
      code.innerText = text;
      pre.appendChild(code);
      summary.appendChild(pre);
      result.appendChild(summary);

      const warning = document.createElement("pre");
      warning.innerText = errorMessage;
      result.appendChild(warning);
      return [result];
    }

    async function renderOneMarmaid(src) {
      const id = `jp-mermaid-${_nextMermaidId++}`;
      const parent = src.parentNode;
      let raw = src.textContent.trim();
      const el = document.createElement("div");
      el.style.visibility = "hidden";
      document.body.appendChild(el);
      let results = null;
      let output = null;
      try {
        let { svg } = await mermaid.render(id, raw, el);
        svg = cleanMermaidSvg(svg);
        results = makeMermaidImage(svg);
        output = document.createElement("figure");
        results.map(output.appendChild, output);
      } catch (err) {
        parent.classList.add("jp-mod-warning");
        results = await makeMermaidError(raw);
        output = results[0];
      } finally {
        el.remove();
      }
      parent.classList.add("jp-RenderedMermaid");
      parent.appendChild(output);
    }


    /**
     * Post-process to ensure mermaid diagrams contain only valid SVG and XHTML.
     */
    function cleanMermaidSvg(svg) {
      return svg.replace(RE_VOID_ELEMENT, replaceVoidElement);
    }


    /**
     * A regular expression for all void elements, which may include attributes and
     * a slash.
     *
     * @see https://developer.mozilla.org/en-US/docs/Glossary/Void_element
     *
     * Of these, only `<br>` is generated by Mermaid in place of `\n`,
     * but _any_ "malformed" tag will break the SVG rendering entirely.
     */
    const RE_VOID_ELEMENT =
      /<\s*(area|base|br|col|embed|hr|img|input|link|meta|param|source|track|wbr)\s*([^>]*?)\s*>/gi;

    /**
     * Ensure a void element is closed with a slash, preserving any attributes.
     */
    function replaceVoidElement(match, tag, rest) {
      rest = rest.trim();
      if (!rest.endsWith('/')) {
        rest = `${rest} /`;
      }
      return `<${tag} ${rest}>`;
    }

    void Promise.all([...diagrams].map(renderOneMarmaid));
  });
</script>
<style>
  .jp-Mermaid:not(.jp-RenderedMermaid) {
    display: none;
  }

  .jp-RenderedMermaid {
    overflow: auto;
    display: flex;
  }

  .jp-RenderedMermaid.jp-mod-warning {
    width: auto;
    padding: 0.5em;
    margin-top: 0.5em;
    border: var(--jp-border-width) solid var(--jp-warn-color2);
    border-radius: var(--jp-border-radius);
    color: var(--jp-ui-font-color1);
    font-size: var(--jp-ui-font-size1);
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  .jp-RenderedMermaid figure {
    margin: 0;
    overflow: auto;
    max-width: 100%;
  }

  .jp-RenderedMermaid img {
    max-width: 100%;
  }

  .jp-RenderedMermaid-Details > pre {
    margin-top: 1em;
  }

  .jp-RenderedMermaid-Summary {
    color: var(--jp-warn-color2);
  }

  .jp-RenderedMermaid:not(.jp-mod-warning) pre {
    display: none;
  }

  .jp-RenderedMermaid-Summary > pre {
    display: inline-block;
    white-space: normal;
  }
</style>
<!-- End of mermaid configuration --></head>
<body class="jp-Notebook" data-jp-theme-light="true" data-jp-theme-name="JupyterLab Light">
<main>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=73c2d2c9-4a4b-48ca-90a3-1ccd120ca08b">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h1 id="Fine-tuning-using-GPT2">Fine tuning using GPT2<a class="anchor-link" href="#Fine-tuning-using-GPT2">¶</a></h1>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=5b5bef3d-89b9-42ed-b158-a39bd61f6a31">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Base-Model-and-Quantization">Base Model and Quantization<a class="anchor-link" href="#Base-Model-and-Quantization">¶</a></h3>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=938123ed-15b0-45ee-b622-d9bbe5fe3a48">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>4-bit quantization alongside specific computational optimizations</p>
<p>load_in_4bit: This option likely enables loading and storing tensors in 4-bit precision. This can significantly reduce memory usage at the cost of precision. Enabling this suggests that you're optimizing for memory efficiency, potentially to fit larger models or datasets in memory.</p>
<p>bnb_4bit_use_double_quant: This indicates the use of a double quantization process for 4-bit representation. Double quantization might be used to improve the precision of the 4-bit quantized values, potentially mitigating some of the precision loss associated with low-bit quantization.</p>
<p>bnb_4bit_quant_type="nf4": This specifies the quantization type or algorithm used for converting tensors to 4-bit representations. "nf4" might refer to a specific quantization scheme optimized for neural network weights and activations. The exact nature of "nf4" would depend on the documentation of the BitsAndBytes library, but it suggests an approach tailored to maintain as much information as possible within the 4-bit limitation.</p>
<p>bnb_4bit_compute_dtype=torch.bfloat16: This sets the data type for computations using 4-bit quantized tensors to torch.bfloat16, which is a 16-bit floating-point representation that offers a good balance between precision and memory usage. By performing computations in bfloat16, the configuration aims to maintain computational accuracy and efficiency, particularly on hardware that supports bfloat16 operations natively.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=dfe5f014-527c-4a83-863b-4b6330c72ed5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [1]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># GPU 0: NVIDIA GeForce RTX 4090</span>
<span class="c1"># GPU 1: NVIDIA GeForce RTX 4090</span>
<span class="c1"># GPU 2: NVIDIA GeForce RTX 4090</span>
<span class="c1"># GPU 3: NVIDIA GeForce RTX 3090 Ti</span>
<span class="c1"># GPU 4: NVIDIA GeForce RTX 3090 Ti</span>
<span class="c1"># GPU 5: NVIDIA GeForce RTX 3090</span>
<span class="c1"># GPU 6: NVIDIA GeForce RTX 3090</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"CUDA_VISIBLE_DEVICES"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"3"</span>  <span class="c1"># ""makes all visible, "0" GPU 0 visible</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=53ffaa11-3936-40a0-8f2a-3d083ff2afef">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Supress-warnings">Supress warnings<a class="anchor-link" href="#Supress-warnings">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=e77f7e3d-3af3-4dc8-9571-d13398c29ee9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [2]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">UserWarning</span><span class="p">)</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">FutureWarning</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=ef535c7a-848e-4c6e-a692-208f98610d82">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Inspect-the-base-model">Inspect the base model<a class="anchor-link" href="#Inspect-the-base-model">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=15ec782b-0776-4354-ad08-66f1e9e50187">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [3]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s2">"gpt2"</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"openai-community/"</span><span class="o">+</span><span class="n">model_name</span>

<span class="n">bnb_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span>
    <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="s2">"nf4"</span><span class="p">,</span>
    <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
<span class="p">)</span>

<span class="n">access_token</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HF_TOKEN"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">bnb_config</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">token</span><span class="o">=</span><span class="n">access_token</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>config.json:   0%|          | 0.00/665 [00:00&lt;?, ?B/s]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>model.safetensors:   0%|          | 0.00/548M [00:00&lt;?, ?B/s]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=83f74939-3ab3-4011-90cb-8e90c22ea162">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [4]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torchinfo</span> <span class="kn">import</span> <span class="n">summary</span>
<span class="n">summary</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[4]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
GPT2ForSequenceClassification                      --
├─GPT2Model: 1-1                                   --
│    └─Embedding: 2-1                              38,597,376
│    └─Embedding: 2-2                              786,432
│    └─Dropout: 2-3                                --
│    └─ModuleList: 2-4                             --
│    │    └─GPT2Block: 3-1                         3,548,928
│    │    └─GPT2Block: 3-2                         3,548,928
│    │    └─GPT2Block: 3-3                         3,548,928
│    │    └─GPT2Block: 3-4                         3,548,928
│    │    └─GPT2Block: 3-5                         3,548,928
│    │    └─GPT2Block: 3-6                         3,548,928
│    │    └─GPT2Block: 3-7                         3,548,928
│    │    └─GPT2Block: 3-8                         3,548,928
│    │    └─GPT2Block: 3-9                         3,548,928
│    │    └─GPT2Block: 3-10                        3,548,928
│    │    └─GPT2Block: 3-11                        3,548,928
│    │    └─GPT2Block: 3-12                        3,548,928
│    └─LayerNorm: 2-5                              1,536
├─Linear: 1-2                                      1,536
===========================================================================
Total params: 81,974,016
Trainable params: 39,423,744
Non-trainable params: 42,550,272
===========================================================================</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=49bbe47c-4430-4f4a-90d8-1113bd320a18">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [5]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[5]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>GPT2ForSequenceClassification(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Linear4bit(in_features=768, out_features=2304, bias=True)
          (c_proj): Linear4bit(in_features=768, out_features=768, bias=True)
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Linear4bit(in_features=768, out_features=3072, bias=True)
          (c_proj): Linear4bit(in_features=3072, out_features=768, bias=True)
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (score): Linear(in_features=768, out_features=2, bias=False)
)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=6627b1da-57d6-4a17-8ea3-746b5cb1f3d9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Load-the-news-dataset-from-pickle-file">Load the news dataset from pickle file<a class="anchor-link" href="#Load-the-news-dataset-from-pickle-file">¶</a></h3><p>If any of the check_files don't exist then load the pickle file</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=1cc372cc-0cae-4b49-a2d7-8e57f58244a7">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [4]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pickle</span>

<span class="n">base_path</span> <span class="o">=</span> <span class="s1">'./data/'</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">base_path</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">file_name</span> <span class="o">=</span> <span class="s1">'news_small_dataset.pkl'</span>
<span class="n">file_path</span> <span class="o">=</span> <span class="n">base_path</span><span class="o">+</span><span class="n">file_name</span>

<span class="k">def</span> <span class="nf">pickle_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">file_path</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="s1">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
        <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">file</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Dataset has been pickled to: </span><span class="si">{</span><span class="n">file_path</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">load_pickle_dataset</span><span class="p">(</span><span class="n">file_path</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="s1">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Dataset has been loaded from: </span><span class="si">{</span><span class="n">file_path</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dataset</span>

<span class="k">def</span> <span class="nf">check_files_exists</span><span class="p">(</span><span class="n">file_names</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">file_names</span><span class="p">:</span>
        <span class="n">file_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">base_path</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">file_path</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="kc">False</span>

<span class="c1"># if these files exist we do not want to load the news_dataset.pkl to tokenize and make these files</span>
<span class="n">check_files</span> <span class="o">=</span> <span class="p">[</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_train_ds.pkl'</span><span class="p">,</span> <span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_eval_ds.pkl'</span><span class="p">,</span> <span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_test_ds.pkl'</span><span class="p">]</span>

<span class="k">if</span> <span class="n">check_files_exists</span><span class="p">(</span><span class="n">check_files</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"At least one of the specified files already exists. Not loading new dataset."</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">news_split_ds</span> <span class="o">=</span> <span class="n">load_pickle_dataset</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">news_split_ds</span><span class="p">)</span>
    <span class="n">total_rows</span> <span class="o">=</span> <span class="p">(</span><span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'train'</span><span class="p">]</span><span class="o">.</span><span class="n">num_rows</span> <span class="o">+</span>
              <span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'eval'</span><span class="p">]</span><span class="o">.</span><span class="n">num_rows</span> <span class="o">+</span>
              <span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'test'</span><span class="p">]</span><span class="o">.</span><span class="n">num_rows</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Total number of rows:"</span><span class="p">,</span> <span class="n">total_rows</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Dataset loaded successfully."</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>At least one of the specified files already exists. Not loading new dataset.
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=8ad450a2-6cef-432e-9e63-7ff985b4726e">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Tokenization-of-data">Tokenization of data<a class="anchor-link" href="#Tokenization-of-data">¶</a></h3>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=186f093c-7487-44ae-ad95-9ee8d24f04c7">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>
return_tensors="pt": This argument configures the tokenizer to output PyTorch ("pt") tensors. If you're working with TensorFlow, you'd use "tf" instead, and for NumPy arrays, you could omit this argument or set return_tensors to None.
</p>
<p>
Direct Model Input: By converting the tokenized input into tensors, the output can be directly used as input to a PyTorch model, fitting seamlessly into the data processing pipeline for model training or inference.

<p>Handling of Batch Inputs: This approach also supports batch inputs. If you pass a list of texts to the tokenizer with return_tensors="pt", it will automatically pad the sequences to the maximum length in the batch, returning a tensor where the first dimension is the batch size.</p>
<p>Padding and Truncation: The padding=True and truncation=True arguments ensure that all sequences are padded to the same length (up to max_length) and are truncated if they exceed this length, which is important for processing sequences in batches.</p>
</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=88c3a38c-1d24-4a1e-8d96-1b7c2ce162c7">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [5]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">token</span><span class="o">=</span><span class="n">access_token</span><span class="p">)</span>

<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">tokenize_fn</span><span class="p">(</span><span class="n">news</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">news</span><span class="p">[</span><span class="s1">'article'</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>tokenizer_config.json:   0%|          | 0.00/26.0 [00:00&lt;?, ?B/s]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>vocab.json:   0%|          | 0.00/1.04M [00:00&lt;?, ?B/s]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>merges.txt:   0%|          | 0.00/456k [00:00&lt;?, ?B/s]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>tokenizer.json:   0%|          | 0.00/1.36M [00:00&lt;?, ?B/s]</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=02efc4f0-742a-4838-887d-5556a26ae15f">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Tokenize-train,-evaluation,-and-test-datasets">Tokenize train, evaluation, and test datasets<a class="anchor-link" href="#Tokenize-train,-evaluation,-and-test-datasets">¶</a></h3><p>If any of the check files exist then don't run tokenization and save some time.
Else load the pickle files that already exist.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=0f3e3101-df30-4af2-9976-49ba0cf44d62">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [6]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">check_files_exists</span><span class="p">(</span><span class="n">check_files</span><span class="p">):</span>
    <span class="n">tokenized_train_ds</span> <span class="o">=</span> <span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'train'</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_fn</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">tokenized_eval_ds</span> <span class="o">=</span> <span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'eval'</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_fn</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">tokenized_test_ds</span> <span class="o">=</span> <span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'test'</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_fn</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">tokenized_train_ds</span><span class="o">.</span><span class="n">features</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">tokenized_eval_ds</span><span class="o">.</span><span class="n">features</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">tokenized_test_ds</span><span class="o">.</span><span class="n">features</span><span class="p">)</span>
    
    <span class="n">pickle_dataset</span><span class="p">(</span><span class="n">tokenized_train_ds</span><span class="p">,</span> <span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_train_ds.pkl'</span><span class="p">)</span>
    <span class="n">pickle_dataset</span><span class="p">(</span><span class="n">tokenized_eval_ds</span><span class="p">,</span> <span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_eval_ds.pkl'</span><span class="p">)</span>
    <span class="n">pickle_dataset</span><span class="p">(</span><span class="n">tokenized_test_ds</span><span class="p">,</span> <span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_test_ds.pkl'</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Files already exist, so load datasets"</span><span class="p">)</span>
    <span class="n">tokenized_train_ds</span> <span class="o">=</span> <span class="n">load_pickle_dataset</span><span class="p">(</span><span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_train_ds.pkl'</span><span class="p">)</span>
    <span class="n">tokenized_eval_ds</span> <span class="o">=</span> <span class="n">load_pickle_dataset</span><span class="p">(</span><span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_eval_ds.pkl'</span><span class="p">)</span>
    <span class="n">tokenized_test_ds</span> <span class="o">=</span> <span class="n">load_pickle_dataset</span><span class="p">(</span><span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_test_ds.pkl'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Files already exist, so load datasets
Dataset has been loaded from: ./data/gpt2-small_tokenized_train_ds.pkl
Dataset has been loaded from: ./data/gpt2-small_tokenized_eval_ds.pkl
Dataset has been loaded from: ./data/gpt2-small_tokenized_test_ds.pkl
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=307cc4fb-9766-4b0f-943b-4a1c90053e09">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Look-at-the-tokenized-data">Look at the tokenized data<a class="anchor-link" href="#Look-at-the-tokenized-data">¶</a></h3><p>Notice what the actual data looks like, and then the tokenized data which is a bunch of numbers, and then the attention mask at the end.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=44321182-e1b9-4570-bd24-7a5cf42ba504">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [9]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">count_train_records</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenized_train_ds</span><span class="p">)</span>
<span class="n">count_eval_records</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenized_eval_ds</span><span class="p">)</span>
<span class="n">count_test_records</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenized_test_ds</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of records in training dataset: </span><span class="si">{</span><span class="n">count_train_records</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of records in evaluation dataset: </span><span class="si">{</span><span class="n">count_eval_records</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of records in test dataset: </span><span class="si">{</span><span class="n">count_test_records</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">count_total_records</span> <span class="o">=</span> <span class="n">count_train_records</span> <span class="o">+</span> <span class="n">count_eval_records</span> <span class="o">+</span> <span class="n">count_test_records</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Total number of records: </span><span class="si">{</span><span class="n">count_total_records</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Number of records in training dataset: 33611
Number of records in evaluation dataset: 7203
Number of records in test dataset: 7203
Total number of records: 48017
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=64be678b-d2a8-403e-bcc8-ef9e7eabb0cf">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [10]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">first_record</span> <span class="o">=</span> <span class="n">tokenized_train_ds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">first_record</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>{'article': "In a year where homicides, rapes and robberies increased slightly, New York City still saw serious crime drop 1.7 percent in 2015, continuing an overall decline that began in the 1990s, NYPD Commissioner William Bratton said Monday.\nAt a news conference with Mayor Bill de Blasio, Bratton touted last year’s crime statistics, which he said, when combined with an even larger decline in 2014, put to rest the fear that substantial decreases couldn’t continue under the new administration at City Hall.\n“While we have had some fluctuation, some increases in certain categories, the overall trend in all our crime categories continues to go down,” Bratton told reporters. “It was a very good year for us, 2015.\nHomicides increased by 4.5 percent in 2015, rising to 350 from 333 in the prior year, which was the lowest since 1994, said Deputy Commissioner Dermot Shea. Rapes increased 6 percent and robberies rose 2 percent, said Shea, who is in charge of data collection and operations for the NYPD.\nThe lower overall crime statistics came about due to what Shea called “targeted enforcement,” where cops make quality arrests even though the overall number of apprehensions was the lowest in the city since 2003.\nTwo boroughs — Manhattan and the Bronx — actually saw serious crimes increase by 3 percent and 4 percent, respectively, Shea said. Manhattan’s increase was driven by more robberies, while the Bronx, although seeing an overall crime increase, had what he said was a “phenomenal” reduction in shootings. Citywide, shootings were down in 2015 about 3 percent, to 1,103 from 1,172 in 2014.\nShea largely attributed the 2015 increase in rapes to victims coming forward with complaints about attacks from years past.\nSign up to get the latest updates Get Newsday's Breaking News alerts in your inbox. By clicking Sign up, you agree to our privacy policy.\n“Twenty percent of these rapes didn’t happen in 2015,” he said.\nThe NYPD has seen an increase in rapes involving single women who, after a night of drinking, get into cabs of all kinds and are attacked, Shea said.\n“They get driven, and passing out and waking up in a desolate area, and they get sexually attacked. This is something, really, that people need to be exceptionally aware of, and like any case in New York City, the buddy system works,” said Shea, referring to the need for people to travel in pairs when taking a cab at night.\nBratton and police brass hope to build upon the continuing drop in overall crime by using technology such as ShotSpotter and a newly minted GPS system for police cars.\nJessica Tisch, NYPD deputy commissioner for technology, said ShotSpotter, an acoustical system that detects gunfire, identified gunshots in 1,672 cases, mostly in Brooklyn. Of those alerts, 74 percent didn’t have any 911 calls from the public associated with them.\nTisch said ShotSpotter helped police recover ballistic evidence in 19 percent of the gunfire alerts. In 22 percent of those cases, Tisch said, cops were able to make positive matches of bullets with those from guns used in earlier shootings.\nTisch also highlighted a special GPS system being tried in about 5,000 patrol cars that allows the NYPD to see where its vehicles are and to track their movements over a 24-hour period, as well as gather information about the officers’ driving.\n", 'label': 0, 'input_ids': [818, 257, 614, 810, 33025, 11, 37459, 290, 43774, 3220, 4622, 11, 968, 1971, 2254, 991, 2497, 2726, 4065, 4268, 352, 13, 22, 1411, 287, 1853, 11, 8282, 281, 4045, 7794, 326, 2540, 287, 262, 6303, 82, 11, 27615, 13270, 3977, 1709, 38680, 531, 3321, 13, 198, 2953, 257, 1705, 4495, 351, 10106, 3941, 390, 36200, 11, 1709, 38680, 28275, 938, 614, 447, 247, 82, 4065, 7869, 11, 543, 339, 531, 11, 618, 5929, 351, 281, 772, 4025, 7794, 287, 1946, 11, 1234, 284, 1334, 262, 3252, 326, 8904, 20638, 3521, 447, 247, 83, 2555, 739, 262, 649, 3662, 379, 2254, 4789, 13, 198, 447, 250, 3633, 356, 423, 550, 617, 19180, 2288, 11, 617, 5732, 287, 1728, 9376, 11, 262, 4045, 5182, 287, 477, 674, 4065, 9376, 4477, 284, 467, 866, 11, 447, 251, 1709, 38680, 1297, 7638, 13, 564, 250, 1026, 373, 257, 845, 922, 614, 329, 514, 11, 1853, 13, 198, 39, 10179, 1460, 3220, 416, 604, 13, 20, 1411, 287, 1853, 11, 7396, 284, 13803, 422, 23460, 287, 262, 3161, 614, 11, 543, 373, 262, 9016, 1201, 9162, 11, 531, 15110, 13270, 360, 7780, 313, 42368, 13, 371, 7916, 3220, 718, 1411, 290, 43774, 8278, 362, 1411, 11, 531, 42368, 11, 508, 318, 287, 3877, 286, 1366, 4947, 290, 4560, 329, 262, 27615, 13, 198, 464, 2793, 4045, 4065, 7869, 1625, 546, 2233, 284, 644, 42368, 1444, 564, 250, 16793, 276, 5394, 11, 447, 251, 810, 14073, 787, 3081, 14794, 772, 996, 262, 4045, 1271, 286, 31887, 507, 373, 262, 9016, 287, 262, 1748, 1201, 5816, 13, 198, 7571, 33534, 82, 851, 13458, 290, 262, 32486, 851, 1682, 2497, 2726, 6741, 2620, 416, 513, 1411, 290, 604, 1411, 11, 8148, 11, 42368, 531, 13, 13458, 447, 247, 82, 2620, 373, 7986, 416, 517, 43774, 11, 981, 262, 32486, 11, 3584, 4379, 281, 4045, 4065, 2620, 11, 550, 644, 339, 531, 373, 257, 564, 250, 31024, 3674, 282, 447, 251, 7741, 287, 17690, 13, 2254, 4421, 11, 17690, 547, 866, 287, 1853, 546, 513, 1411, 11, 284, 352, 11, 15197, 422, 352, 11, 23628, 287, 1946, 13, 198, 3347, 64, 5688, 14183, 262, 1853, 2620, 287, 37459, 284, 4970, 2406, 2651, 351, 9687, 546, 3434, 422, 812, 1613, 13, 198, 11712, 510, 284, 651, 262, 3452, 5992, 3497, 3000, 820, 338, 24942, 3000, 21675, 287, 534, 13734, 13, 2750, 12264, 5865, 510, 11, 345, 4236, 284, 674, 6782, 2450, 13, 198, 447, 250, 34096, 1411, 286, 777, 37459, 1422, 447, 247, 83, 1645, 287, 1853, 11, 447, 251, 339, 531, 13, 198, 464, 27615, 468, 1775, 281, 2620, 287, 37459, 7411, 2060, 1466, 508, 11, 706, 257, 1755, 286, 7722, 11, 651, 656, 269, 8937, 286, 477, 6982, 290, 389, 7384, 11, 42368, 531, 13, 198, 447, 250, 2990, 651, 7986, 11, 290, 6427, 503, 290, 23137, 510, 287, 257, 50244, 1989, 11, 290, 484, 651, 11363, 7384, 13, 770, 318, 1223, 11, 1107, 11, 326, 661, 761, 284, 307, 24822, 3910, 286, 11, 290, 588, 597, 1339, 287, 968, 1971, 2254, 11, 262, 24407, 1080, 2499, 11, 447], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=b2d7f4f8-9eb3-4feb-86e7-fb0dad88753f">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Turn-on-accelerate">Turn on accelerate<a class="anchor-link" href="#Turn-on-accelerate">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=f3eab29e-fa47-4a13-abc4-d6425ae741b6">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [7]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">accelerate</span> <span class="kn">import</span> <span class="n">FullyShardedDataParallelPlugin</span><span class="p">,</span> <span class="n">Accelerator</span>
<span class="kn">from</span> <span class="nn">torch.distributed.fsdp.fully_sharded_data_parallel</span> <span class="kn">import</span> <span class="n">FullOptimStateDictConfig</span><span class="p">,</span> <span class="n">FullStateDictConfig</span>

<span class="n">fsdp_plugin</span> <span class="o">=</span> <span class="n">FullyShardedDataParallelPlugin</span><span class="p">(</span>
    <span class="n">state_dict_config</span><span class="o">=</span><span class="n">FullStateDictConfig</span><span class="p">(</span><span class="n">offload_to_cpu</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rank0_only</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="n">optim_state_dict_config</span><span class="o">=</span><span class="n">FullOptimStateDictConfig</span><span class="p">(</span><span class="n">offload_to_cpu</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rank0_only</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
<span class="p">)</span>

<span class="n">accelerator</span> <span class="o">=</span> <span class="n">Accelerator</span><span class="p">(</span><span class="n">fsdp_plugin</span><span class="o">=</span><span class="n">fsdp_plugin</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=22e040f1-e596-476d-9f26-ffa6f8a4a548">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="LoRA---Low-Rank-Adaptation">LoRA - Low-Rank Adaptation<a class="anchor-link" href="#LoRA---Low-Rank-Adaptation">¶</a></h3>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=bc8b8091-6179-4df0-b5bd-b29ec2dde4d5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>LoRA, short for Low-Rank Adaptation, is a technique designed to efficiently fine-tune large pre-trained models without the need to update all the model parameters, significantly reducing the computational and memory overhead typically associated with training. LoRA targets the challenge of adapting massive models, particularly in natural language processing (NLP) and computer vision, to specialized tasks while keeping the resource requirements manageable.</p>
<p>
Gradient checkpointing for the model. Gradient checkpointing is a technique used to reduce memory usage during the training of deep neural networks by trading compute for memory. It works by storing a minimal set of intermediate activations during the forward pass and then recomputing the others during the backward pass. This is particularly useful for training large models or using larger batch sizes.
</p>
<p>
The forward pass is the process where the input data is passed through the network from the input layer to the output layer. During this pass, the network performs a series of computations at each layer, applying weights to the inputs, adding biases (if applicable), and passing the result through an activation function. The final output of the forward pass is the prediction made by the network. The main goal of the forward pass is to compute the output given the current state of the model's parameters (weights and biases). This output is then used to calculate the loss, which quantifies how well the model's predictions match the actual labels.
</p>
<p>
The backward pass, or backpropagation, is the process of computing the gradient of the loss function with respect to each parameter in the network. This involves applying the chain rule of calculus to take derivatives step-by-step from the output layer back to the input layer. Essentially, it calculates how much each parameter contributed to the error in the prediction. The purpose of the backward pass is to update the model's parameters in a way that minimally reduces the loss, improving the model's predictions. The gradients calculated during this pass indicate the direction in which each parameter should be adjusted to decrease the error. Using an optimization algorithm (e.g., Stochastic Gradient Descent), these gradients are then used to update the weights to minimize the loss.
</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=41956597-0441-4bc6-aefe-fc4b0d5349c5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>
r: This parameter specifies the rank of the low-rank matrices that are introduced by LoRA. A smaller rank means fewer parameters to train, leading to a more memory-efficient fine-tuning process.
</p>
<p>
lora_alpha: This multiplier adjusts the scale of the LoRA parameters. A higher value increases the capacity of the LoRA adjustments to the original model weights.
</p>
<p>
target_modules: Lists the specific parts of the model to which LoRA will be applied. These typically correspond to components within transformer blocks, such as the query, key, value, and output projections in attention mechanisms, as well as any additional modules relevant to the model architecture.
</p>
<p>
bias: Specifies how biases are treated in the adaptation process. In this case, biases are not adjusted ("none").
</p>
<p>
lora_dropout: Sets the dropout rate for the LoRA parameters, helping to prevent overfitting during fine-tuning. The dropout rate is a hyperparameter used in the training of neural networks, representing the probability that a given neuron (or unit) is temporarily "dropped" from the network during a specific iteration of training. This means that the neuron will not participate in the forward pass and its contribution to the backward pass (gradient computation) is also ignored during that iteration. Dropout is applied randomly to a subset of neurons in the network at each training step.
</p>
<p>
task_type: Indicates the type of task for which the model is being fine-tuned. The example uses TaskType.SEQ_CLS, 
suggesting a sequence classification task, such as sentiment analysis or document classification. In my case a binary classification of machine versus human generated text.
</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=0532bdc6-4317-474b-859c-4e5d2fe6f1bd">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [12]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">prepare_model_for_kbit_training</span><span class="p">,</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">get_peft_model</span><span class="p">,</span> <span class="n">TaskType</span>

<span class="n">model</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">prepare_model_for_kbit_training</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span>
    <span class="n">r</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span>
        <span class="s2">"c_attn"</span><span class="p">,</span>
        <span class="s2">"c_proj"</span><span class="p">,</span>
        <span class="s2">"c_fc"</span><span class="p">,</span>
        <span class="s2">"c_proj"</span><span class="p">,</span>
        <span class="s2">"lm_head"</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="n">bias</span><span class="o">=</span><span class="s2">"none"</span><span class="p">,</span>
    <span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">task_type</span><span class="o">=</span><span class="n">TaskType</span><span class="o">.</span><span class="n">SEQ_CLS</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">accelerator</span><span class="o">.</span><span class="n">prepare_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=1cd29494-4453-4c01-b878-ef2f4533d34d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Inspect-the-model">Inspect the model<a class="anchor-link" href="#Inspect-the-model">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=d5058f6d-185d-45af-83b9-bb7f61d4bee3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [13]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">print_trainable_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">trainable_params</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">all_param</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
        <span class="n">all_param</span> <span class="o">+=</span> <span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="n">trainable_params</span> <span class="o">+=</span> <span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"trainable params: </span><span class="si">{</span><span class="n">trainable_params</span><span class="si">}</span><span class="s2"> || all params: </span><span class="si">{</span><span class="n">all_param</span><span class="si">}</span><span class="s2"> || trainable%: </span><span class="si">{</span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">trainable_params</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">all_param</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=b14fe8f2-eccd-4cfe-9d20-ef48bc178842">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [14]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">print_trainable_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">model</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>trainable params: 1181184 || all params: 83155200 || trainable%: 1.4204571692449781
</pre>
</div>
</div>
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[14]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>PeftModelForSequenceClassification(
  (base_model): LoraModel(
    (model): GPT2ForSequenceClassification(
      (transformer): GPT2Model(
        (wte): Embedding(50257, 768)
        (wpe): Embedding(1024, 768)
        (drop): Dropout(p=0.1, inplace=False)
        (h): ModuleList(
          (0-11): 12 x GPT2Block(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=768, out_features=2304, bias=True)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=768, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=2304, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (c_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=768, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=768, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=768, out_features=3072, bias=True)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=768, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=3072, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (c_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=3072, out_features=768, bias=True)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=768, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (score): ModulesToSaveWrapper(
        (original_module): Linear(in_features=768, out_features=2, bias=False)
        (modules_to_save): ModuleDict(
          (default): Linear(in_features=768, out_features=2, bias=False)
        )
      )
    )
  )
)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=e17b4782-03f8-4335-bfda-2a65794bff2c">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Look-at-hardware">Look at hardware<a class="anchor-link" href="#Look-at-hardware">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=c27c3328-1926-4adc-8696-b3b343ec4afd">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [8]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Available GPUs: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"GPU </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_name</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Available GPUs: 1
GPU 0: NVIDIA GeForce RTX 3090 Ti
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=bdf68d0e-8786-489d-a4b8-b7478513efea">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [16]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">!</span>nvidia-smi
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Thu May 23 03:37:15 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 3090        Off |   00000000:01:00.0 Off |                  N/A |
| 31%   40C    P2            125W /  420W |     612MiB /  24576MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA GeForce RTX 4090        Off |   00000000:02:00.0 Off |                  Off |
|  0%   41C    P2             66W /  450W |    4361MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA GeForce RTX 3090 Ti     Off |   00000000:2A:00.0 Off |                  Off |
| 38%   45C    P2             51W /  450W |     538MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA GeForce RTX 3090        Off |   00000000:41:00.0 Off |                  N/A |
| 33%   41C    P2            115W /  420W |     896MiB /  24576MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA GeForce RTX 4090        Off |   00000000:42:00.0 Off |                  Off |
|  0%   49C    P8             15W /  450W |      10MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA GeForce RTX 4090        Off |   00000000:61:00.0 Off |                  Off |
|  0%   40C    P8             16W /  450W |      10MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA GeForce RTX 3090 Ti     Off |   00000000:62:00.0 Off |                  Off |
| 33%   40C    P2             97W /  450W |    1282MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A      2561      G   /usr/bin/gnome-shell                            4MiB |
|    0   N/A  N/A     12702      C   /usr/bin/python3                              598MiB |
|    1   N/A  N/A      2561      G   /usr/bin/gnome-shell                           95MiB |
|    1   N/A  N/A     13002      C   /usr/bin/python3                             4256MiB |
|    2   N/A  N/A      2561      G   /usr/bin/gnome-shell                            4MiB |
|    2   N/A  N/A     12510      C   /usr/bin/python3                              524MiB |
|    3   N/A  N/A      2561      G   /usr/bin/gnome-shell                            4MiB |
|    3   N/A  N/A     12816      C   /usr/bin/python3                              882MiB |
|    4   N/A  N/A      2561      G   /usr/bin/gnome-shell                            6MiB |
|    5   N/A  N/A      2561      G   /usr/bin/gnome-shell                            6MiB |
|    6   N/A  N/A      2561      G   /usr/bin/gnome-shell                            4MiB |
|    6   N/A  N/A     12625      C   /usr/bin/python3                             1268MiB |
+-----------------------------------------------------------------------------------------+
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=2340f2d1-1cc7-454e-bad6-bf4ac2c46fc9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [9]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">model</span><span class="o">.</span><span class="n">is_parallelizable</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">model</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=1199841a-6519-4422-8259-2fdbb114e466">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [10]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span><span class="p">,</span> <span class="n">accuracy_score</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">logits_and_labels</span><span class="p">):</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">logits_and_labels</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
    <span class="n">f1</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">'macro'</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">'accuracy'</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span> <span class="s1">'f1'</span><span class="p">:</span> <span class="n">f1</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=b8c7f3ee-6997-4ed7-b28e-5d574831fb08">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [11]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">project_name</span> <span class="o">=</span> <span class="s2">"praxis-"</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s2">"-small-finetune"</span>
<span class="n">output_dir_path</span> <span class="o">=</span> <span class="s2">"./"</span> <span class="o">+</span> <span class="n">project_name</span>
<span class="n">output_dir_path</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[11]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>'./praxis-gpt2-small-finetune'</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=e7a69bde-1d50-46a0-9838-00812afbdb16">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [20]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=ca48a7aa-41d7-4c57-bfec-093d22bf4b54">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p><strong>output_dir</strong> (<code>output_dir_path</code>): This specifies the directory where outputs such as model checkpoints and logs will be saved. It's important for organizing the outputs of your training sessions.</p>
<p><strong>warmup_steps</strong> (<code>5</code>): This parameter sets the number of steps during which the learning rate will gradually increase from zero to the initially set learning rate. This warmup phase helps stabilize the model's training early on, preventing the model from diverging due to high gradient values at the start.</p>
<p><strong>per_device_train_batch_size</strong> (<code>32</code>): This sets the number of training examples to process on each device (like a GPU) during training. A higher batch size can speed up training but may require more memory.</p>
<p><strong>per_device_eval_batch_size</strong> (<code>32</code>): Similar to the training batch size, this is the number of examples to process on each device during evaluation. It determines how quickly the model can process the evaluation data.</p>
<p><strong>num_train_epochs</strong> (<code>10</code>): This defines the total number of times the training process should iterate over the entire dataset. More epochs can lead to better learning but also risk overfitting if too high.</p>
<p><strong>gradient_checkpointing</strong> (<code>False</code>): If set to <code>True</code>, this would enable gradient checkpointing to reduce memory usage at the cost of longer training time. It's useful for very deep models that otherwise would not fit into GPU memory.</p>
<p><strong>gradient_accumulation_steps</strong> (<code>2</code>): This setting allows you to accumulate gradients over multiple steps before performing an update on the model's weights. It's a way to effectively increase the batch size without increasing the memory load, which can be helpful when dealing with hardware constraints.</p>
<p><strong>max_steps</strong> (<code>500</code>): This is the maximum number of training steps to execute, regardless of how many epochs are set. Training will stop when this number of steps is reached.</p>
<p><strong>learning_rate</strong> (<code>2.5e-5</code>): This is the step size at which the optimizer updates the model’s weights. A smaller learning rate might lead to better fine-tuning but slower convergence, and vice versa.</p>
<p><strong>logging_steps</strong> (<code>200</code>): Specifies how often to log training information. The setting determines after how many steps new logs should be created, which might include loss and other metrics. More frequent logging provides finer-grained visibility into the training progress but can add computational overhead.</p>
<p><strong>bf16</strong> (<code>True</code>): This would enable training using bfloat16 precision, which is a mixed precision format with fewer bits than the standard single-precision floating point (fp32). Like fp16, it can reduce memory usage and potentially speed up training if supported by the hardware. It's particularly useful on TPUs and newer GPUs that support this format. (4090)</p>
<p><strong>fp16</strong> (<code>True</code>): This enables half-precision floating point (16-bit) training. It reduces memory usage and can speed up training, provided the hardware (like modern GPUs) supports it. (3090 and 4090)</p>
<p><strong>optim</strong> (<code>"paged_adamw_8bit"</code>): Specifies the optimizer to use. "paged_adamw_8bit" might refer to a variation of the AdamW optimizer that is optimized for lower precision and memory bandwidth, enhancing training speed and efficiency.</p>
<p><strong>logging_dir</strong> (<code>"./logs"</code>): This specifies the directory where training logs should be saved. It's used to store logs if you are using a logging framework or callback that writes out logs to files. Organizing logs in a specific directory is helpful for post-training analysis and for monitoring the training process through tools like TensorBoard.</p>
<p><strong>save_strategy</strong> (<code>"epoch"</code>): This determines how often to save model checkpoints. Setting it to <code>"epoch"</code> means the model will save checkpoints at the end of each epoch.</p>
<p><strong>save_steps</strong> (<code>50</code>): This is closely related to the <code>save_strategy</code> when set to "steps". It defines how often to save the model, specifically after how many training steps. A lower number means more frequent saves, which increases disk I/O but provides more restore points for training.</p>
<p><strong>evaluation_strategy</strong> (<code>"epoch"</code>): This configures when the model should be evaluated against the evaluation dataset. Like <code>save_strategy</code>, setting this to <code>"epoch"</code> triggers evaluations at the end of each epoch, providing feedback on model performance after it has seen the entire training dataset.</p>
<p><strong>eval_steps</strong> (<code>50</code>): This parameter determines how often to evaluate the model if the <code>evaluation_strategy</code> is set to "steps". Similar to <code>logging_steps</code>, setting this affects how frequently the model's performance is assessed on the evaluation dataset during the training process. More frequent evaluations provide a closer look at the model's performance but at the cost of increased computational overhead.</p>
<p><strong>do_eval</strong> (<code>True</code>): This flag enables the evaluation of the model on the evaluation dataset. If <code>True</code>, it will use the evaluation dataset to assess model performance based on metrics</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=d8f3cd41-fd17-4fd8-83b8-54f464df79ff">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Training">Training<a class="anchor-link" href="#Training">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=2df4e2a5-2a08-434b-983a-f6cd42f33da3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [21]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">transformers</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="n">transformers</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">777</span><span class="p">)</span>

<span class="k">if</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_train_ds</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">tokenized_eval_ds</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">transformers</span><span class="o">.</span><span class="n">TrainingArguments</span><span class="p">(</span>
        <span class="n">output_dir</span><span class="o">=</span><span class="n">output_dir_path</span><span class="p">,</span>
        <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">logging_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">logging_dir</span><span class="o">=</span><span class="n">output_dir_path</span><span class="o">+</span><span class="s2">"/logs"</span><span class="p">,</span>
        <span class="n">evaluation_strategy</span><span class="o">=</span><span class="s1">'epoch'</span><span class="p">,</span>
        <span class="n">save_strategy</span><span class="o">=</span><span class="s1">'epoch'</span><span class="p">,</span>
        <span class="n">bf16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">optim</span><span class="o">=</span><span class="s2">"paged_adamw_8bit"</span><span class="p">,</span>
        <span class="n">do_eval</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>
    <span class="n">data_collator</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">DataCollatorWithPadding</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">),</span>
<span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">resume_from_checkpoint</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># Turn to True if power goes out...</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>2024-05-23 03:37:15.970731: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-23 03:37:16.682079: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
<span class="ansi-blue-intense-fg ansi-bold">wandb</span>: Currently logged in as: <span class="ansi-yellow-fg">nispoe</span>. Use <span class="ansi-bold">`wandb login --relogin`</span> to force relogin
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
Tracking run with wandb version 0.17.0
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
Run data is saved locally in <code>/home/nispoe/kuk/Praxis/wandb/run-20240523_033718-x1nplgxr</code>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
Syncing run <strong><a href="https://wandb.ai/nispoe/huggingface/runs/x1nplgxr" target="_blank">noble-butterfly-327</a></strong> to <a href="https://wandb.ai/nispoe/huggingface" target="_blank">Weights &amp; Biases</a> (<a href="https://wandb.me/run" target="_blank">docs</a>)<br/>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
 View project at <a href="https://wandb.ai/nispoe/huggingface" target="_blank">https://wandb.ai/nispoe/huggingface</a>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
 View run at <a href="https://wandb.ai/nispoe/huggingface/runs/x1nplgxr" target="_blank">https://wandb.ai/nispoe/huggingface/runs/x1nplgxr</a>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<div>
<progress max="42020" style="width:300px; height:20px; vertical-align: middle;" value="42020"></progress>
      [42020/42020 2:44:12, Epoch 10/10]
    </div>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: left;">
<th>Epoch</th>
<th>Training Loss</th>
<th>Validation Loss</th>
<th>Accuracy</th>
<th>F1</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0.608400</td>
<td>0.823679</td>
<td>0.532278</td>
<td>0.512181</td>
</tr>
<tr>
<td>2</td>
<td>0.386500</td>
<td>1.012033</td>
<td>0.581424</td>
<td>0.569434</td>
</tr>
<tr>
<td>3</td>
<td>0.490900</td>
<td>1.214639</td>
<td>0.525337</td>
<td>0.497754</td>
</tr>
<tr>
<td>4</td>
<td>0.445200</td>
<td>0.797666</td>
<td>0.654172</td>
<td>0.651540</td>
</tr>
<tr>
<td>5</td>
<td>0.600400</td>
<td>1.012137</td>
<td>0.635707</td>
<td>0.630936</td>
</tr>
<tr>
<td>6</td>
<td>0.428100</td>
<td>0.871624</td>
<td>0.681938</td>
<td>0.680768</td>
</tr>
<tr>
<td>7</td>
<td>0.401900</td>
<td>0.629089</td>
<td>0.758712</td>
<td>0.758301</td>
</tr>
<tr>
<td>8</td>
<td>0.287600</td>
<td>1.018618</td>
<td>0.664723</td>
<td>0.662412</td>
</tr>
<tr>
<td>9</td>
<td>0.239300</td>
<td>0.857376</td>
<td>0.704429</td>
<td>0.704078</td>
</tr>
<tr>
<td>10</td>
<td>0.302700</td>
<td>0.925887</td>
<td>0.709288</td>
<td>0.708994</td>
</tr>
</tbody>
</table><p>
</p></div>
</div>
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[21]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>TrainOutput(global_step=42020, training_loss=0.41203572257594345, metrics={'train_runtime': 9854.2688, 'train_samples_per_second': 34.108, 'train_steps_per_second': 4.264, 'total_flos': 8.90440586625024e+16, 'train_loss': 0.41203572257594345, 'epoch': 10.0})</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=0d13982d-53cb-4c88-bd32-a98d4a5a9874">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Determine-best-checkpoint">Determine best checkpoint<a class="anchor-link" href="#Determine-best-checkpoint">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=8bd570c7-8feb-4b69-bda7-9c678bfd92c3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [22]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">!</span>ls<span class="w"> </span>-ltr<span class="w"> </span><span class="o">{</span>output_dir_path<span class="o">}</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>total 44
drwxr-xr-x 2 nispoe nispoe 4096 May 23 03:37 logs
drwxrwxr-x 2 nispoe nispoe 4096 May 23 03:53 checkpoint-4202
drwxrwxr-x 2 nispoe nispoe 4096 May 23 04:09 checkpoint-8404
drwxrwxr-x 2 nispoe nispoe 4096 May 23 04:26 checkpoint-12606
drwxrwxr-x 2 nispoe nispoe 4096 May 23 04:42 checkpoint-16808
drwxrwxr-x 2 nispoe nispoe 4096 May 23 04:58 checkpoint-21010
drwxrwxr-x 2 nispoe nispoe 4096 May 23 05:15 checkpoint-25212
drwxrwxr-x 2 nispoe nispoe 4096 May 23 05:31 checkpoint-29414
drwxrwxr-x 2 nispoe nispoe 4096 May 23 05:48 checkpoint-33616
drwxrwxr-x 2 nispoe nispoe 4096 May 23 06:05 checkpoint-37818
drwxrwxr-x 2 nispoe nispoe 4096 May 23 06:21 checkpoint-42020
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=97771269-d77c-4c1b-b264-34e082db6cbc">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [12]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.python.summary.summary_iterator</span> <span class="kn">import</span> <span class="n">summary_iterator</span>
<span class="kn">import</span> <span class="nn">glob</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># Construct the logs directory path</span>
<span class="n">logs_directory</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">'./'</span><span class="p">,</span> <span class="n">project_name</span><span class="p">,</span> <span class="s1">'logs'</span><span class="p">)</span>
<span class="n">file_pattern</span> <span class="o">=</span> <span class="s1">'events.out.tfevents.*'</span>

<span class="c1"># Retrieve all event files matching the pattern</span>
<span class="n">event_files</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">logs_directory</span><span class="p">,</span> <span class="n">file_pattern</span><span class="p">))</span>

<span class="c1"># Function to print out TensorBoard event logs</span>
<span class="k">def</span> <span class="nf">print_events_from_file</span><span class="p">(</span><span class="n">event_files</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">event_file</span> <span class="ow">in</span> <span class="n">event_files</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Reading events from file: </span><span class="si">{</span><span class="n">event_file</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">summary_iterator</span><span class="p">(</span><span class="n">event_file</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">e</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">v</span><span class="o">.</span><span class="n">HasField</span><span class="p">(</span><span class="s1">'simple_value'</span><span class="p">):</span>
                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Step: </span><span class="si">{</span><span class="n">e</span><span class="o">.</span><span class="n">step</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">v</span><span class="o">.</span><span class="n">simple_value</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>  <span class="c1"># Just in case the event file is not readable</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Failed to read </span><span class="si">{</span><span class="n">event_file</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">print_events_from_file</span><span class="p">(</span><span class="n">event_files</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>2024-06-05 00:24:05.352979: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-05 00:24:05.928715: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Reading events from file: ./praxis-gpt2-small-finetune/logs/events.out.tfevents.1716453437.hephaestus.12510.0
WARNING:tensorflow:From /home/nispoe/.local/lib/python3.10/site-packages/tensorflow/python/summary/summary_iterator.py:27: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.
Instructions for updating:
Use eager execution and: 
`tf.data.TFRecordDataset(path)`
Step: 10, train/loss: 1.8467999696731567
Step: 10, train/grad_norm: 141.04576110839844
Step: 10, train/learning_rate: 4.998810254619457e-05
Step: 10, train/epoch: 0.0023798190522938967
Step: 20, train/loss: 1.954300045967102
Step: 20, train/grad_norm: 37.688209533691406
Step: 20, train/learning_rate: 4.997620271751657e-05
Step: 20, train/epoch: 0.004759638104587793
Step: 30, train/loss: 1.3849999904632568
Step: 30, train/grad_norm: 109.02017211914062
Step: 30, train/learning_rate: 4.9964302888838574e-05
Step: 30, train/epoch: 0.007139457389712334
Step: 40, train/loss: 1.2128000259399414
Step: 40, train/grad_norm: 75.34876251220703
Step: 40, train/learning_rate: 4.995240306016058e-05
Step: 40, train/epoch: 0.009519276209175587
Step: 50, train/loss: 1.1584999561309814
Step: 50, train/grad_norm: 63.098751068115234
Step: 50, train/learning_rate: 4.994050323148258e-05
Step: 50, train/epoch: 0.011899095959961414
Step: 60, train/loss: 0.9139999747276306
Step: 60, train/grad_norm: 68.55852508544922
Step: 60, train/learning_rate: 4.992860704078339e-05
Step: 60, train/epoch: 0.014278914779424667
Step: 70, train/loss: 0.696399986743927
Step: 70, train/grad_norm: 54.27604293823242
Step: 70, train/learning_rate: 4.9916707212105393e-05
Step: 70, train/epoch: 0.016658734530210495
Step: 80, train/loss: 0.7924000024795532
Step: 80, train/grad_norm: 45.31159591674805
Step: 80, train/learning_rate: 4.9904807383427396e-05
Step: 80, train/epoch: 0.019038552418351173
Step: 90, train/loss: 0.6844000220298767
Step: 90, train/grad_norm: 94.93656158447266
Step: 90, train/learning_rate: 4.98929075547494e-05
Step: 90, train/epoch: 0.021418372169137
Step: 100, train/loss: 0.9681000113487244
Step: 100, train/grad_norm: 123.96014404296875
Step: 100, train/learning_rate: 4.98810077260714e-05
Step: 100, train/epoch: 0.02379819191992283
Step: 110, train/loss: 0.7993999719619751
Step: 110, train/grad_norm: 44.00965118408203
Step: 110, train/learning_rate: 4.986911153537221e-05
Step: 110, train/epoch: 0.026178009808063507
Step: 120, train/loss: 0.9010999798774719
Step: 120, train/grad_norm: 115.30155944824219
Step: 120, train/learning_rate: 4.9857211706694216e-05
Step: 120, train/epoch: 0.028557829558849335
Step: 130, train/loss: 0.8001999855041504
Step: 130, train/grad_norm: 41.60950469970703
Step: 130, train/learning_rate: 4.984531187801622e-05
Step: 130, train/epoch: 0.030937649309635162
Step: 140, train/loss: 0.833899974822998
Step: 140, train/grad_norm: 32.86480712890625
Step: 140, train/learning_rate: 4.983341204933822e-05
Step: 140, train/epoch: 0.03331746906042099
Step: 150, train/loss: 0.8050000071525574
Step: 150, train/grad_norm: 115.15097045898438
Step: 150, train/learning_rate: 4.9821512220660225e-05
Step: 150, train/epoch: 0.03569728881120682
Step: 160, train/loss: 0.7894999980926514
Step: 160, train/grad_norm: 38.06871032714844
Step: 160, train/learning_rate: 4.9809616029961035e-05
Step: 160, train/epoch: 0.03807710483670235
Step: 170, train/loss: 0.7671999931335449
Step: 170, train/grad_norm: 105.39321899414062
Step: 170, train/learning_rate: 4.979771620128304e-05
Step: 170, train/epoch: 0.040456924587488174
Step: 180, train/loss: 0.754800021648407
Step: 180, train/grad_norm: 41.90463638305664
Step: 180, train/learning_rate: 4.978581637260504e-05
Step: 180, train/epoch: 0.042836744338274
Step: 190, train/loss: 0.7348999977111816
Step: 190, train/grad_norm: 38.676231384277344
Step: 190, train/learning_rate: 4.9773916543927044e-05
Step: 190, train/epoch: 0.04521656408905983
Step: 200, train/loss: 0.7269999980926514
Step: 200, train/grad_norm: 16.514019012451172
Step: 200, train/learning_rate: 4.976201671524905e-05
Step: 200, train/epoch: 0.04759638383984566
Step: 210, train/loss: 0.6730999946594238
Step: 210, train/grad_norm: 34.18742370605469
Step: 210, train/learning_rate: 4.975012052454986e-05
Step: 210, train/epoch: 0.049976203590631485
Step: 220, train/loss: 0.7885000109672546
Step: 220, train/grad_norm: 11.143024444580078
Step: 220, train/learning_rate: 4.973822069587186e-05
Step: 220, train/epoch: 0.052356019616127014
Step: 230, train/loss: 0.7396000027656555
Step: 230, train/grad_norm: 38.58441925048828
Step: 230, train/learning_rate: 4.972632086719386e-05
Step: 230, train/epoch: 0.05473583936691284
Step: 240, train/loss: 0.760699987411499
Step: 240, train/grad_norm: 17.081501007080078
Step: 240, train/learning_rate: 4.9714421038515866e-05
Step: 240, train/epoch: 0.05711565911769867
Step: 250, train/loss: 0.6326000094413757
Step: 250, train/grad_norm: 58.94453048706055
Step: 250, train/learning_rate: 4.970252120983787e-05
Step: 250, train/epoch: 0.0594954788684845
Step: 260, train/loss: 0.7224000096321106
Step: 260, train/grad_norm: 46.3563117980957
Step: 260, train/learning_rate: 4.969062501913868e-05
Step: 260, train/epoch: 0.061875298619270325
Step: 270, train/loss: 0.8349000215530396
Step: 270, train/grad_norm: 12.752452850341797
Step: 270, train/learning_rate: 4.967872519046068e-05
Step: 270, train/epoch: 0.06425511837005615
Step: 280, train/loss: 0.6122000217437744
Step: 280, train/grad_norm: 26.998605728149414
Step: 280, train/learning_rate: 4.9666825361782685e-05
Step: 280, train/epoch: 0.06663493812084198
Step: 290, train/loss: 0.8036999702453613
Step: 290, train/grad_norm: 113.42318725585938
Step: 290, train/learning_rate: 4.965492553310469e-05
Step: 290, train/epoch: 0.06901475787162781
Step: 300, train/loss: 0.7939000129699707
Step: 300, train/grad_norm: 10.524587631225586
Step: 300, train/learning_rate: 4.964302570442669e-05
Step: 300, train/epoch: 0.07139457762241364
Step: 310, train/loss: 0.7099999785423279
Step: 310, train/grad_norm: 49.658267974853516
Step: 310, train/learning_rate: 4.96311295137275e-05
Step: 310, train/epoch: 0.07377438992261887
Step: 320, train/loss: 0.6858999729156494
Step: 320, train/grad_norm: 99.15940856933594
Step: 320, train/learning_rate: 4.9619229685049504e-05
Step: 320, train/epoch: 0.0761542096734047
Step: 330, train/loss: 0.6786999702453613
Step: 330, train/grad_norm: 15.564597129821777
Step: 330, train/learning_rate: 4.960732985637151e-05
Step: 330, train/epoch: 0.07853402942419052
Step: 340, train/loss: 0.7527999877929688
Step: 340, train/grad_norm: 36.95386505126953
Step: 340, train/learning_rate: 4.959543002769351e-05
Step: 340, train/epoch: 0.08091384917497635
Step: 350, train/loss: 0.7555000185966492
Step: 350, train/grad_norm: 45.25962829589844
Step: 350, train/learning_rate: 4.958353019901551e-05
Step: 350, train/epoch: 0.08329366892576218
Step: 360, train/loss: 0.6919000148773193
Step: 360, train/grad_norm: 29.352005004882812
Step: 360, train/learning_rate: 4.957163400831632e-05
Step: 360, train/epoch: 0.085673488676548
Step: 370, train/loss: 0.7473999857902527
Step: 370, train/grad_norm: 24.876379013061523
Step: 370, train/learning_rate: 4.9559734179638326e-05
Step: 370, train/epoch: 0.08805330842733383
Step: 380, train/loss: 0.6517999768257141
Step: 380, train/grad_norm: 16.27165412902832
Step: 380, train/learning_rate: 4.954783435096033e-05
Step: 380, train/epoch: 0.09043312817811966
Step: 390, train/loss: 0.7106999754905701
Step: 390, train/grad_norm: 10.14643669128418
Step: 390, train/learning_rate: 4.953593452228233e-05
Step: 390, train/epoch: 0.09281294792890549
Step: 400, train/loss: 0.6948000192642212
Step: 400, train/grad_norm: 25.847938537597656
Step: 400, train/learning_rate: 4.9524034693604335e-05
Step: 400, train/epoch: 0.09519276767969131
Step: 410, train/loss: 0.6714000105857849
Step: 410, train/grad_norm: 19.548099517822266
Step: 410, train/learning_rate: 4.9512138502905145e-05
Step: 410, train/epoch: 0.09757258743047714
Step: 420, train/loss: 0.671500027179718
Step: 420, train/grad_norm: 9.775819778442383
Step: 420, train/learning_rate: 4.950023867422715e-05
Step: 420, train/epoch: 0.09995240718126297
Step: 430, train/loss: 0.6736999750137329
Step: 430, train/grad_norm: 53.4937858581543
Step: 430, train/learning_rate: 4.948833884554915e-05
Step: 430, train/epoch: 0.1023322194814682
Step: 440, train/loss: 0.6597999930381775
Step: 440, train/grad_norm: 18.402536392211914
Step: 440, train/learning_rate: 4.9476439016871154e-05
Step: 440, train/epoch: 0.10471203923225403
Step: 450, train/loss: 0.6312000155448914
Step: 450, train/grad_norm: 29.31719207763672
Step: 450, train/learning_rate: 4.946453918819316e-05
Step: 450, train/epoch: 0.10709185898303986
Step: 460, train/loss: 0.6891999840736389
Step: 460, train/grad_norm: 54.22526931762695
Step: 460, train/learning_rate: 4.945264299749397e-05
Step: 460, train/epoch: 0.10947167873382568
Step: 470, train/loss: 0.6890000104904175
Step: 470, train/grad_norm: 28.571359634399414
Step: 470, train/learning_rate: 4.944074316881597e-05
Step: 470, train/epoch: 0.11185149848461151
Step: 480, train/loss: 0.652400016784668
Step: 480, train/grad_norm: 61.603641510009766
Step: 480, train/learning_rate: 4.9428843340137973e-05
Step: 480, train/epoch: 0.11423131823539734
Step: 490, train/loss: 0.6829000115394592
Step: 490, train/grad_norm: 22.59014320373535
Step: 490, train/learning_rate: 4.9416943511459976e-05
Step: 490, train/epoch: 0.11661113798618317
Step: 500, train/loss: 0.6384999752044678
Step: 500, train/grad_norm: 45.296268463134766
Step: 500, train/learning_rate: 4.940504368278198e-05
Step: 500, train/epoch: 0.118990957736969
Step: 510, train/loss: 0.6036999821662903
Step: 510, train/grad_norm: 19.166101455688477
Step: 510, train/learning_rate: 4.939314749208279e-05
Step: 510, train/epoch: 0.12137077748775482
Step: 520, train/loss: 0.7246999740600586
Step: 520, train/grad_norm: 65.55514526367188
Step: 520, train/learning_rate: 4.938124766340479e-05
Step: 520, train/epoch: 0.12375059723854065
Step: 530, train/loss: 0.7203999757766724
Step: 530, train/grad_norm: 41.429683685302734
Step: 530, train/learning_rate: 4.9369347834726796e-05
Step: 530, train/epoch: 0.12613041698932648
Step: 540, train/loss: 0.6901999711990356
Step: 540, train/grad_norm: 25.43857192993164
Step: 540, train/learning_rate: 4.93574480060488e-05
Step: 540, train/epoch: 0.1285102367401123
Step: 550, train/loss: 0.6244999766349792
Step: 550, train/grad_norm: 2.834409475326538
Step: 550, train/learning_rate: 4.93455481773708e-05
Step: 550, train/epoch: 0.13089005649089813
Step: 560, train/loss: 0.6931999921798706
Step: 560, train/grad_norm: 6.59805154800415
Step: 560, train/learning_rate: 4.933365198667161e-05
Step: 560, train/epoch: 0.13326987624168396
Step: 570, train/loss: 0.7595000267028809
Step: 570, train/grad_norm: 5.4414286613464355
Step: 570, train/learning_rate: 4.9321752157993615e-05
Step: 570, train/epoch: 0.1356496959924698
Step: 580, train/loss: 0.6851999759674072
Step: 580, train/grad_norm: 9.939441680908203
Step: 580, train/learning_rate: 4.930985232931562e-05
Step: 580, train/epoch: 0.13802951574325562
Step: 590, train/loss: 0.6850000023841858
Step: 590, train/grad_norm: 9.954719543457031
Step: 590, train/learning_rate: 4.929795250063762e-05
Step: 590, train/epoch: 0.14040933549404144
Step: 600, train/loss: 0.6574000120162964
Step: 600, train/grad_norm: 28.167722702026367
Step: 600, train/learning_rate: 4.9286052671959624e-05
Step: 600, train/epoch: 0.14278915524482727
Step: 610, train/loss: 0.6707000136375427
Step: 610, train/grad_norm: 9.021578788757324
Step: 610, train/learning_rate: 4.9274156481260434e-05
Step: 610, train/epoch: 0.1451689600944519
Step: 620, train/loss: 0.6661999821662903
Step: 620, train/grad_norm: 6.093663692474365
Step: 620, train/learning_rate: 4.926225665258244e-05
Step: 620, train/epoch: 0.14754877984523773
Step: 630, train/loss: 0.6883999705314636
Step: 630, train/grad_norm: 8.242692947387695
Step: 630, train/learning_rate: 4.925035682390444e-05
Step: 630, train/epoch: 0.14992859959602356
Step: 640, train/loss: 0.6980999708175659
Step: 640, train/grad_norm: 37.09843444824219
Step: 640, train/learning_rate: 4.923845699522644e-05
Step: 640, train/epoch: 0.1523084193468094
Step: 650, train/loss: 0.6935999989509583
Step: 650, train/grad_norm: 16.435274124145508
Step: 650, train/learning_rate: 4.9226557166548446e-05
Step: 650, train/epoch: 0.15468823909759521
Step: 660, train/loss: 0.6638000011444092
Step: 660, train/grad_norm: 18.87287139892578
Step: 660, train/learning_rate: 4.9214660975849256e-05
Step: 660, train/epoch: 0.15706805884838104
Step: 670, train/loss: 0.6518999934196472
Step: 670, train/grad_norm: 4.298393726348877
Step: 670, train/learning_rate: 4.920276114717126e-05
Step: 670, train/epoch: 0.15944787859916687
Step: 680, train/loss: 0.6664999723434448
Step: 680, train/grad_norm: 3.809173583984375
Step: 680, train/learning_rate: 4.919086131849326e-05
Step: 680, train/epoch: 0.1618276983499527
Step: 690, train/loss: 0.6765000224113464
Step: 690, train/grad_norm: 31.945409774780273
Step: 690, train/learning_rate: 4.9178961489815265e-05
Step: 690, train/epoch: 0.16420751810073853
Step: 700, train/loss: 0.7258999943733215
Step: 700, train/grad_norm: 35.29347229003906
Step: 700, train/learning_rate: 4.916706166113727e-05
Step: 700, train/epoch: 0.16658733785152435
Step: 710, train/loss: 0.6741999983787537
Step: 710, train/grad_norm: 12.382884979248047
Step: 710, train/learning_rate: 4.915516547043808e-05
Step: 710, train/epoch: 0.16896715760231018
Step: 720, train/loss: 0.6553000211715698
Step: 720, train/grad_norm: 58.583187103271484
Step: 720, train/learning_rate: 4.914326564176008e-05
Step: 720, train/epoch: 0.171346977353096
Step: 730, train/loss: 0.6363999843597412
Step: 730, train/grad_norm: 37.70943832397461
Step: 730, train/learning_rate: 4.9131365813082084e-05
Step: 730, train/epoch: 0.17372679710388184
Step: 740, train/loss: 0.6797999739646912
Step: 740, train/grad_norm: 43.057861328125
Step: 740, train/learning_rate: 4.911946598440409e-05
Step: 740, train/epoch: 0.17610661685466766
Step: 750, train/loss: 0.6952999830245972
Step: 750, train/grad_norm: 12.27648639678955
Step: 750, train/learning_rate: 4.910756615572609e-05
Step: 750, train/epoch: 0.1784864366054535
Step: 760, train/loss: 0.6657999753952026
Step: 760, train/grad_norm: 36.2241325378418
Step: 760, train/learning_rate: 4.90956699650269e-05
Step: 760, train/epoch: 0.18086625635623932
Step: 770, train/loss: 0.7157999873161316
Step: 770, train/grad_norm: 28.120441436767578
Step: 770, train/learning_rate: 4.90837701363489e-05
Step: 770, train/epoch: 0.18324607610702515
Step: 780, train/loss: 0.6694999933242798
Step: 780, train/grad_norm: 16.089670181274414
Step: 780, train/learning_rate: 4.9071870307670906e-05
Step: 780, train/epoch: 0.18562589585781097
Step: 790, train/loss: 0.670799970626831
Step: 790, train/grad_norm: 46.91265106201172
Step: 790, train/learning_rate: 4.905997047899291e-05
Step: 790, train/epoch: 0.1880057156085968
Step: 800, train/loss: 0.6514000296592712
Step: 800, train/grad_norm: 13.747284889221191
Step: 800, train/learning_rate: 4.904807065031491e-05
Step: 800, train/epoch: 0.19038553535938263
Step: 810, train/loss: 0.66839998960495
Step: 810, train/grad_norm: 8.623621940612793
Step: 810, train/learning_rate: 4.903617445961572e-05
Step: 810, train/epoch: 0.19276535511016846
Step: 820, train/loss: 0.6872000098228455
Step: 820, train/grad_norm: 9.31850814819336
Step: 820, train/learning_rate: 4.9024274630937725e-05
Step: 820, train/epoch: 0.19514517486095428
Step: 830, train/loss: 0.5748000144958496
Step: 830, train/grad_norm: 8.96530818939209
Step: 830, train/learning_rate: 4.901237480225973e-05
Step: 830, train/epoch: 0.1975249946117401
Step: 840, train/loss: 0.8162000179290771
Step: 840, train/grad_norm: 31.194902420043945
Step: 840, train/learning_rate: 4.900047497358173e-05
Step: 840, train/epoch: 0.19990481436252594
Step: 850, train/loss: 0.6071000099182129
Step: 850, train/grad_norm: 8.534759521484375
Step: 850, train/learning_rate: 4.8988575144903734e-05
Step: 850, train/epoch: 0.20228461921215057
Step: 860, train/loss: 0.6725999712944031
Step: 860, train/grad_norm: 24.15912437438965
Step: 860, train/learning_rate: 4.8976678954204544e-05
Step: 860, train/epoch: 0.2046644389629364
Step: 870, train/loss: 0.7046999931335449
Step: 870, train/grad_norm: 11.135865211486816
Step: 870, train/learning_rate: 4.896477912552655e-05
Step: 870, train/epoch: 0.20704425871372223
Step: 880, train/loss: 0.6309999823570251
Step: 880, train/grad_norm: 4.838676929473877
Step: 880, train/learning_rate: 4.895287929684855e-05
Step: 880, train/epoch: 0.20942407846450806
Step: 890, train/loss: 0.6883999705314636
Step: 890, train/grad_norm: 14.718552589416504
Step: 890, train/learning_rate: 4.8940979468170553e-05
Step: 890, train/epoch: 0.21180389821529388
Step: 900, train/loss: 0.6305000185966492
Step: 900, train/grad_norm: 6.363997459411621
Step: 900, train/learning_rate: 4.8929079639492556e-05
Step: 900, train/epoch: 0.2141837179660797
Step: 910, train/loss: 0.6216999888420105
Step: 910, train/grad_norm: 20.299381256103516
Step: 910, train/learning_rate: 4.8917183448793367e-05
Step: 910, train/epoch: 0.21656353771686554
Step: 920, train/loss: 0.6438999772071838
Step: 920, train/grad_norm: 17.402631759643555
Step: 920, train/learning_rate: 4.890528362011537e-05
Step: 920, train/epoch: 0.21894335746765137
Step: 930, train/loss: 0.6335999965667725
Step: 930, train/grad_norm: 3.995885133743286
Step: 930, train/learning_rate: 4.889338379143737e-05
Step: 930, train/epoch: 0.2213231772184372
Step: 940, train/loss: 0.6610000133514404
Step: 940, train/grad_norm: 11.09764289855957
Step: 940, train/learning_rate: 4.8881483962759376e-05
Step: 940, train/epoch: 0.22370299696922302
Step: 950, train/loss: 0.6358000040054321
Step: 950, train/grad_norm: 9.810426712036133
Step: 950, train/learning_rate: 4.886958413408138e-05
Step: 950, train/epoch: 0.22608281672000885
Step: 960, train/loss: 0.6462000012397766
Step: 960, train/grad_norm: 14.547017097473145
Step: 960, train/learning_rate: 4.885768794338219e-05
Step: 960, train/epoch: 0.22846263647079468
Step: 970, train/loss: 0.6152999997138977
Step: 970, train/grad_norm: 8.766724586486816
Step: 970, train/learning_rate: 4.884578811470419e-05
Step: 970, train/epoch: 0.2308424562215805
Step: 980, train/loss: 0.6489999890327454
Step: 980, train/grad_norm: 36.66849899291992
Step: 980, train/learning_rate: 4.8833888286026195e-05
Step: 980, train/epoch: 0.23322227597236633
Step: 990, train/loss: 0.5881999731063843
Step: 990, train/grad_norm: 14.051604270935059
Step: 990, train/learning_rate: 4.88219884573482e-05
Step: 990, train/epoch: 0.23560209572315216
Step: 1000, train/loss: 0.6521000266075134
Step: 1000, train/grad_norm: 13.516498565673828
Step: 1000, train/learning_rate: 4.88100886286702e-05
Step: 1000, train/epoch: 0.237981915473938
Step: 1010, train/loss: 0.6309000253677368
Step: 1010, train/grad_norm: 6.878478050231934
Step: 1010, train/learning_rate: 4.879819243797101e-05
Step: 1010, train/epoch: 0.24036173522472382
Step: 1020, train/loss: 0.6187999844551086
Step: 1020, train/grad_norm: 6.928576946258545
Step: 1020, train/learning_rate: 4.8786292609293014e-05
Step: 1020, train/epoch: 0.24274155497550964
Step: 1030, train/loss: 0.6164000034332275
Step: 1030, train/grad_norm: 15.057636260986328
Step: 1030, train/learning_rate: 4.877439278061502e-05
Step: 1030, train/epoch: 0.24512137472629547
Step: 1040, train/loss: 0.6473000049591064
Step: 1040, train/grad_norm: 39.01806640625
Step: 1040, train/learning_rate: 4.876249295193702e-05
Step: 1040, train/epoch: 0.2475011944770813
Step: 1050, train/loss: 0.6373000144958496
Step: 1050, train/grad_norm: 4.531142711639404
Step: 1050, train/learning_rate: 4.875059676123783e-05
Step: 1050, train/epoch: 0.24988101422786713
Step: 1060, train/loss: 0.694599986076355
Step: 1060, train/grad_norm: 4.747936725616455
Step: 1060, train/learning_rate: 4.873869693255983e-05
Step: 1060, train/epoch: 0.25226083397865295
Step: 1070, train/loss: 0.6432999968528748
Step: 1070, train/grad_norm: 4.233264446258545
Step: 1070, train/learning_rate: 4.8726797103881836e-05
Step: 1070, train/epoch: 0.2546406388282776
Step: 1080, train/loss: 0.6862999796867371
Step: 1080, train/grad_norm: 15.886224746704102
Step: 1080, train/learning_rate: 4.871489727520384e-05
Step: 1080, train/epoch: 0.2570204734802246
Step: 1090, train/loss: 0.5947999954223633
Step: 1090, train/grad_norm: 23.950542449951172
Step: 1090, train/learning_rate: 4.870299744652584e-05
Step: 1090, train/epoch: 0.25940027832984924
Step: 1100, train/loss: 0.5687999725341797
Step: 1100, train/grad_norm: 24.454809188842773
Step: 1100, train/learning_rate: 4.869110125582665e-05
Step: 1100, train/epoch: 0.26178011298179626
Step: 1110, train/loss: 0.6237999796867371
Step: 1110, train/grad_norm: 7.1031975746154785
Step: 1110, train/learning_rate: 4.8679201427148655e-05
Step: 1110, train/epoch: 0.2641599178314209
Step: 1120, train/loss: 0.6148999929428101
Step: 1120, train/grad_norm: 3.1709213256835938
Step: 1120, train/learning_rate: 4.866730159847066e-05
Step: 1120, train/epoch: 0.2665397524833679
Step: 1130, train/loss: 0.7106999754905701
Step: 1130, train/grad_norm: 25.99291229248047
Step: 1130, train/learning_rate: 4.865540176979266e-05
Step: 1130, train/epoch: 0.26891955733299255
Step: 1140, train/loss: 0.6753000020980835
Step: 1140, train/grad_norm: 7.580532073974609
Step: 1140, train/learning_rate: 4.8643501941114664e-05
Step: 1140, train/epoch: 0.2712993919849396
Step: 1150, train/loss: 0.6251000165939331
Step: 1150, train/grad_norm: 15.559925079345703
Step: 1150, train/learning_rate: 4.8631605750415474e-05
Step: 1150, train/epoch: 0.2736791968345642
Step: 1160, train/loss: 0.6746000051498413
Step: 1160, train/grad_norm: 4.546782970428467
Step: 1160, train/learning_rate: 4.861970592173748e-05
Step: 1160, train/epoch: 0.27605903148651123
Step: 1170, train/loss: 0.6322000026702881
Step: 1170, train/grad_norm: 13.961463928222656
Step: 1170, train/learning_rate: 4.860780609305948e-05
Step: 1170, train/epoch: 0.27843883633613586
Step: 1180, train/loss: 0.6622999906539917
Step: 1180, train/grad_norm: 38.1390380859375
Step: 1180, train/learning_rate: 4.859590626438148e-05
Step: 1180, train/epoch: 0.2808186709880829
Step: 1190, train/loss: 0.6057999730110168
Step: 1190, train/grad_norm: 7.426792621612549
Step: 1190, train/learning_rate: 4.8584006435703486e-05
Step: 1190, train/epoch: 0.2831984758377075
Step: 1200, train/loss: 0.6432999968528748
Step: 1200, train/grad_norm: 4.02902364730835
Step: 1200, train/learning_rate: 4.8572110245004296e-05
Step: 1200, train/epoch: 0.28557831048965454
Step: 1210, train/loss: 0.5942000150680542
Step: 1210, train/grad_norm: 6.179030418395996
Step: 1210, train/learning_rate: 4.85602104163263e-05
Step: 1210, train/epoch: 0.2879581153392792
Step: 1220, train/loss: 0.6787999868392944
Step: 1220, train/grad_norm: 8.029450416564941
Step: 1220, train/learning_rate: 4.85483105876483e-05
Step: 1220, train/epoch: 0.2903379201889038
Step: 1230, train/loss: 0.5950000286102295
Step: 1230, train/grad_norm: 42.730525970458984
Step: 1230, train/learning_rate: 4.8536410758970305e-05
Step: 1230, train/epoch: 0.29271775484085083
Step: 1240, train/loss: 0.6692000031471252
Step: 1240, train/grad_norm: 36.386844635009766
Step: 1240, train/learning_rate: 4.852451093029231e-05
Step: 1240, train/epoch: 0.29509755969047546
Step: 1250, train/loss: 0.6474000215530396
Step: 1250, train/grad_norm: 3.62510347366333
Step: 1250, train/learning_rate: 4.851261473959312e-05
Step: 1250, train/epoch: 0.2974773943424225
Step: 1260, train/loss: 0.6438000202178955
Step: 1260, train/grad_norm: 13.11107063293457
Step: 1260, train/learning_rate: 4.850071491091512e-05
Step: 1260, train/epoch: 0.2998571991920471
Step: 1270, train/loss: 0.6011999845504761
Step: 1270, train/grad_norm: 31.708681106567383
Step: 1270, train/learning_rate: 4.8488815082237124e-05
Step: 1270, train/epoch: 0.30223703384399414
Step: 1280, train/loss: 0.642300009727478
Step: 1280, train/grad_norm: 33.55057907104492
Step: 1280, train/learning_rate: 4.847691525355913e-05
Step: 1280, train/epoch: 0.3046168386936188
Step: 1290, train/loss: 0.595300018787384
Step: 1290, train/grad_norm: 41.00901794433594
Step: 1290, train/learning_rate: 4.846501542488113e-05
Step: 1290, train/epoch: 0.3069966733455658
Step: 1300, train/loss: 0.6571000218391418
Step: 1300, train/grad_norm: 5.294342041015625
Step: 1300, train/learning_rate: 4.845311923418194e-05
Step: 1300, train/epoch: 0.30937647819519043
Step: 1310, train/loss: 0.6322000026702881
Step: 1310, train/grad_norm: 35.69755935668945
Step: 1310, train/learning_rate: 4.8441219405503944e-05
Step: 1310, train/epoch: 0.31175631284713745
Step: 1320, train/loss: 0.6707000136375427
Step: 1320, train/grad_norm: 80.75843048095703
Step: 1320, train/learning_rate: 4.8429319576825947e-05
Step: 1320, train/epoch: 0.3141361176967621
Step: 1330, train/loss: 0.5769000053405762
Step: 1330, train/grad_norm: 32.58389663696289
Step: 1330, train/learning_rate: 4.841741974814795e-05
Step: 1330, train/epoch: 0.3165159523487091
Step: 1340, train/loss: 0.5436999797821045
Step: 1340, train/grad_norm: 13.965149879455566
Step: 1340, train/learning_rate: 4.840551991946995e-05
Step: 1340, train/epoch: 0.31889575719833374
Step: 1350, train/loss: 0.5819000005722046
Step: 1350, train/grad_norm: 4.398637294769287
Step: 1350, train/learning_rate: 4.839362372877076e-05
Step: 1350, train/epoch: 0.32127559185028076
Step: 1360, train/loss: 0.599399983882904
Step: 1360, train/grad_norm: 20.70256233215332
Step: 1360, train/learning_rate: 4.8381723900092766e-05
Step: 1360, train/epoch: 0.3236553966999054
Step: 1370, train/loss: 0.6047999858856201
Step: 1370, train/grad_norm: 27.676883697509766
Step: 1370, train/learning_rate: 4.836982407141477e-05
Step: 1370, train/epoch: 0.3260352313518524
Step: 1380, train/loss: 0.507099986076355
Step: 1380, train/grad_norm: 35.0332145690918
Step: 1380, train/learning_rate: 4.835792424273677e-05
Step: 1380, train/epoch: 0.32841503620147705
Step: 1390, train/loss: 0.6384999752044678
Step: 1390, train/grad_norm: 9.034119606018066
Step: 1390, train/learning_rate: 4.8346024414058775e-05
Step: 1390, train/epoch: 0.3307948708534241
Step: 1400, train/loss: 0.6276000142097473
Step: 1400, train/grad_norm: 15.07584285736084
Step: 1400, train/learning_rate: 4.8334128223359585e-05
Step: 1400, train/epoch: 0.3331746757030487
Step: 1410, train/loss: 0.6378999948501587
Step: 1410, train/grad_norm: 4.581620693206787
Step: 1410, train/learning_rate: 4.832222839468159e-05
Step: 1410, train/epoch: 0.3355545103549957
Step: 1420, train/loss: 0.6051999926567078
Step: 1420, train/grad_norm: 13.439318656921387
Step: 1420, train/learning_rate: 4.831032856600359e-05
Step: 1420, train/epoch: 0.33793431520462036
Step: 1430, train/loss: 0.5210000276565552
Step: 1430, train/grad_norm: 4.866482734680176
Step: 1430, train/learning_rate: 4.8298428737325594e-05
Step: 1430, train/epoch: 0.3403141498565674
Step: 1440, train/loss: 0.6636000275611877
Step: 1440, train/grad_norm: 12.256526947021484
Step: 1440, train/learning_rate: 4.82865289086476e-05
Step: 1440, train/epoch: 0.342693954706192
Step: 1450, train/loss: 0.573199987411499
Step: 1450, train/grad_norm: 20.09432601928711
Step: 1450, train/learning_rate: 4.827463271794841e-05
Step: 1450, train/epoch: 0.34507375955581665
Step: 1460, train/loss: 0.6151000261306763
Step: 1460, train/grad_norm: 3.1365485191345215
Step: 1460, train/learning_rate: 4.826273288927041e-05
Step: 1460, train/epoch: 0.34745359420776367
Step: 1470, train/loss: 0.6722000241279602
Step: 1470, train/grad_norm: 41.329593658447266
Step: 1470, train/learning_rate: 4.825083306059241e-05
Step: 1470, train/epoch: 0.3498333990573883
Step: 1480, train/loss: 0.6320000290870667
Step: 1480, train/grad_norm: 43.63624954223633
Step: 1480, train/learning_rate: 4.8238933231914416e-05
Step: 1480, train/epoch: 0.3522132337093353
Step: 1490, train/loss: 0.7215999960899353
Step: 1490, train/grad_norm: 14.947699546813965
Step: 1490, train/learning_rate: 4.822703340323642e-05
Step: 1490, train/epoch: 0.35459303855895996
Step: 1500, train/loss: 0.5777999758720398
Step: 1500, train/grad_norm: 3.9357357025146484
Step: 1500, train/learning_rate: 4.821513721253723e-05
Step: 1500, train/epoch: 0.356972873210907
Step: 1510, train/loss: 0.6650000214576721
Step: 1510, train/grad_norm: 28.686540603637695
Step: 1510, train/learning_rate: 4.820323738385923e-05
Step: 1510, train/epoch: 0.3593526780605316
Step: 1520, train/loss: 0.6450999975204468
Step: 1520, train/grad_norm: 50.07347869873047
Step: 1520, train/learning_rate: 4.8191337555181235e-05
Step: 1520, train/epoch: 0.36173251271247864
Step: 1530, train/loss: 0.5928000211715698
Step: 1530, train/grad_norm: 6.106337070465088
Step: 1530, train/learning_rate: 4.817943772650324e-05
Step: 1530, train/epoch: 0.36411231756210327
Step: 1540, train/loss: 0.6775000095367432
Step: 1540, train/grad_norm: 14.022204399108887
Step: 1540, train/learning_rate: 4.816753789782524e-05
Step: 1540, train/epoch: 0.3664921522140503
Step: 1550, train/loss: 0.6953999996185303
Step: 1550, train/grad_norm: 5.557634353637695
Step: 1550, train/learning_rate: 4.815564170712605e-05
Step: 1550, train/epoch: 0.3688719570636749
Step: 1560, train/loss: 0.59579998254776
Step: 1560, train/grad_norm: 30.981901168823242
Step: 1560, train/learning_rate: 4.8143741878448054e-05
Step: 1560, train/epoch: 0.37125179171562195
Step: 1570, train/loss: 0.621399998664856
Step: 1570, train/grad_norm: 4.166040897369385
Step: 1570, train/learning_rate: 4.813184204977006e-05
Step: 1570, train/epoch: 0.3736315965652466
Step: 1580, train/loss: 0.6758000254631042
Step: 1580, train/grad_norm: 27.087209701538086
Step: 1580, train/learning_rate: 4.811994222109206e-05
Step: 1580, train/epoch: 0.3760114312171936
Step: 1590, train/loss: 0.5849000215530396
Step: 1590, train/grad_norm: 8.331868171691895
Step: 1590, train/learning_rate: 4.810804239241406e-05
Step: 1590, train/epoch: 0.37839123606681824
Step: 1600, train/loss: 0.6169000267982483
Step: 1600, train/grad_norm: 48.061431884765625
Step: 1600, train/learning_rate: 4.809614620171487e-05
Step: 1600, train/epoch: 0.38077107071876526
Step: 1610, train/loss: 0.6718999743461609
Step: 1610, train/grad_norm: 9.21252155303955
Step: 1610, train/learning_rate: 4.8084246373036876e-05
Step: 1610, train/epoch: 0.3831508755683899
Step: 1620, train/loss: 0.6568999886512756
Step: 1620, train/grad_norm: 8.783195495605469
Step: 1620, train/learning_rate: 4.807234654435888e-05
Step: 1620, train/epoch: 0.3855307102203369
Step: 1630, train/loss: 0.5990999937057495
Step: 1630, train/grad_norm: 25.184574127197266
Step: 1630, train/learning_rate: 4.806044671568088e-05
Step: 1630, train/epoch: 0.38791051506996155
Step: 1640, train/loss: 0.6916000247001648
Step: 1640, train/grad_norm: 3.549467086791992
Step: 1640, train/learning_rate: 4.8048546887002885e-05
Step: 1640, train/epoch: 0.39029034972190857
Step: 1650, train/loss: 0.6535000205039978
Step: 1650, train/grad_norm: 9.891103744506836
Step: 1650, train/learning_rate: 4.8036650696303695e-05
Step: 1650, train/epoch: 0.3926701545715332
Step: 1660, train/loss: 0.628600001335144
Step: 1660, train/grad_norm: 7.510908603668213
Step: 1660, train/learning_rate: 4.80247508676257e-05
Step: 1660, train/epoch: 0.3950499892234802
Step: 1670, train/loss: 0.6273999810218811
Step: 1670, train/grad_norm: 31.444421768188477
Step: 1670, train/learning_rate: 4.80128510389477e-05
Step: 1670, train/epoch: 0.39742979407310486
Step: 1680, train/loss: 0.5697000026702881
Step: 1680, train/grad_norm: 21.03191375732422
Step: 1680, train/learning_rate: 4.8000951210269704e-05
Step: 1680, train/epoch: 0.3998096287250519
Step: 1690, train/loss: 0.6656000018119812
Step: 1690, train/grad_norm: 21.704002380371094
Step: 1690, train/learning_rate: 4.798905138159171e-05
Step: 1690, train/epoch: 0.4021894335746765
Step: 1700, train/loss: 0.5799000263214111
Step: 1700, train/grad_norm: 30.3299560546875
Step: 1700, train/learning_rate: 4.797715519089252e-05
Step: 1700, train/epoch: 0.40456923842430115
Step: 1710, train/loss: 0.652400016784668
Step: 1710, train/grad_norm: 29.26286506652832
Step: 1710, train/learning_rate: 4.796525536221452e-05
Step: 1710, train/epoch: 0.40694907307624817
Step: 1720, train/loss: 0.550599992275238
Step: 1720, train/grad_norm: 6.086697101593018
Step: 1720, train/learning_rate: 4.7953355533536524e-05
Step: 1720, train/epoch: 0.4093288779258728
Step: 1730, train/loss: 0.5939000248908997
Step: 1730, train/grad_norm: 11.097164154052734
Step: 1730, train/learning_rate: 4.7941455704858527e-05
Step: 1730, train/epoch: 0.4117087125778198
Step: 1740, train/loss: 0.6029999852180481
Step: 1740, train/grad_norm: 18.552227020263672
Step: 1740, train/learning_rate: 4.792955587618053e-05
Step: 1740, train/epoch: 0.41408851742744446
Step: 1750, train/loss: 0.6004999876022339
Step: 1750, train/grad_norm: 30.48309898376465
Step: 1750, train/learning_rate: 4.791765968548134e-05
Step: 1750, train/epoch: 0.4164683520793915
Step: 1760, train/loss: 0.5612000226974487
Step: 1760, train/grad_norm: 6.090909957885742
Step: 1760, train/learning_rate: 4.790575985680334e-05
Step: 1760, train/epoch: 0.4188481569290161
Step: 1770, train/loss: 0.6151000261306763
Step: 1770, train/grad_norm: 62.52983093261719
Step: 1770, train/learning_rate: 4.7893860028125346e-05
Step: 1770, train/epoch: 0.42122799158096313
Step: 1780, train/loss: 0.5857999920845032
Step: 1780, train/grad_norm: 11.493189811706543
Step: 1780, train/learning_rate: 4.788196019944735e-05
Step: 1780, train/epoch: 0.42360779643058777
Step: 1790, train/loss: 0.5843999981880188
Step: 1790, train/grad_norm: 27.971233367919922
Step: 1790, train/learning_rate: 4.787006037076935e-05
Step: 1790, train/epoch: 0.4259876310825348
Step: 1800, train/loss: 0.573199987411499
Step: 1800, train/grad_norm: 7.198032855987549
Step: 1800, train/learning_rate: 4.785816418007016e-05
Step: 1800, train/epoch: 0.4283674359321594
Step: 1810, train/loss: 0.6389999985694885
Step: 1810, train/grad_norm: 7.421957492828369
Step: 1810, train/learning_rate: 4.7846264351392165e-05
Step: 1810, train/epoch: 0.43074727058410645
Step: 1820, train/loss: 0.5849999785423279
Step: 1820, train/grad_norm: 39.30133819580078
Step: 1820, train/learning_rate: 4.783436452271417e-05
Step: 1820, train/epoch: 0.4331270754337311
Step: 1830, train/loss: 0.6784999966621399
Step: 1830, train/grad_norm: 7.349477767944336
Step: 1830, train/learning_rate: 4.782246469403617e-05
Step: 1830, train/epoch: 0.4355069100856781
Step: 1840, train/loss: 0.6103000044822693
Step: 1840, train/grad_norm: 10.071364402770996
Step: 1840, train/learning_rate: 4.7810564865358174e-05
Step: 1840, train/epoch: 0.43788671493530273
Step: 1850, train/loss: 0.5371999740600586
Step: 1850, train/grad_norm: 25.888492584228516
Step: 1850, train/learning_rate: 4.7798668674658984e-05
Step: 1850, train/epoch: 0.44026654958724976
Step: 1860, train/loss: 0.6549000144004822
Step: 1860, train/grad_norm: 21.283220291137695
Step: 1860, train/learning_rate: 4.778676884598099e-05
Step: 1860, train/epoch: 0.4426463544368744
Step: 1870, train/loss: 0.694100022315979
Step: 1870, train/grad_norm: 23.439353942871094
Step: 1870, train/learning_rate: 4.777486901730299e-05
Step: 1870, train/epoch: 0.4450261890888214
Step: 1880, train/loss: 0.6194000244140625
Step: 1880, train/grad_norm: 12.485786437988281
Step: 1880, train/learning_rate: 4.776296918862499e-05
Step: 1880, train/epoch: 0.44740599393844604
Step: 1890, train/loss: 0.6919999718666077
Step: 1890, train/grad_norm: 48.86758041381836
Step: 1890, train/learning_rate: 4.7751069359946996e-05
Step: 1890, train/epoch: 0.44978582859039307
Step: 1900, train/loss: 0.6704000234603882
Step: 1900, train/grad_norm: 15.766642570495605
Step: 1900, train/learning_rate: 4.7739173169247806e-05
Step: 1900, train/epoch: 0.4521656334400177
Step: 1910, train/loss: 0.5440999865531921
Step: 1910, train/grad_norm: 9.195197105407715
Step: 1910, train/learning_rate: 4.772727334056981e-05
Step: 1910, train/epoch: 0.4545454680919647
Step: 1920, train/loss: 0.5756000280380249
Step: 1920, train/grad_norm: 7.130548477172852
Step: 1920, train/learning_rate: 4.771537351189181e-05
Step: 1920, train/epoch: 0.45692527294158936
Step: 1930, train/loss: 0.5990999937057495
Step: 1930, train/grad_norm: 21.977497100830078
Step: 1930, train/learning_rate: 4.7703473683213815e-05
Step: 1930, train/epoch: 0.4593051075935364
Step: 1940, train/loss: 0.6725000143051147
Step: 1940, train/grad_norm: 4.177605152130127
Step: 1940, train/learning_rate: 4.769157385453582e-05
Step: 1940, train/epoch: 0.461684912443161
Step: 1950, train/loss: 0.5663999915122986
Step: 1950, train/grad_norm: 22.98894500732422
Step: 1950, train/learning_rate: 4.767967766383663e-05
Step: 1950, train/epoch: 0.46406471729278564
Step: 1960, train/loss: 0.5809000134468079
Step: 1960, train/grad_norm: 28.98054313659668
Step: 1960, train/learning_rate: 4.766777783515863e-05
Step: 1960, train/epoch: 0.46644455194473267
Step: 1970, train/loss: 0.5612000226974487
Step: 1970, train/grad_norm: 5.730383396148682
Step: 1970, train/learning_rate: 4.7655878006480634e-05
Step: 1970, train/epoch: 0.4688243567943573
Step: 1980, train/loss: 0.5752000212669373
Step: 1980, train/grad_norm: 37.71666717529297
Step: 1980, train/learning_rate: 4.764397817780264e-05
Step: 1980, train/epoch: 0.4712041914463043
Step: 1990, train/loss: 0.5831999778747559
Step: 1990, train/grad_norm: 17.020336151123047
Step: 1990, train/learning_rate: 4.763207834912464e-05
Step: 1990, train/epoch: 0.47358399629592896
Step: 2000, train/loss: 0.6345000267028809
Step: 2000, train/grad_norm: 14.121773719787598
Step: 2000, train/learning_rate: 4.762018215842545e-05
Step: 2000, train/epoch: 0.475963830947876
Step: 2010, train/loss: 0.59579998254776
Step: 2010, train/grad_norm: 7.7553863525390625
Step: 2010, train/learning_rate: 4.760828232974745e-05
Step: 2010, train/epoch: 0.4783436357975006
Step: 2020, train/loss: 0.5986999869346619
Step: 2020, train/grad_norm: 8.302970886230469
Step: 2020, train/learning_rate: 4.7596382501069456e-05
Step: 2020, train/epoch: 0.48072347044944763
Step: 2030, train/loss: 0.6830000281333923
Step: 2030, train/grad_norm: 5.287156581878662
Step: 2030, train/learning_rate: 4.758448267239146e-05
Step: 2030, train/epoch: 0.48310327529907227
Step: 2040, train/loss: 0.5688999891281128
Step: 2040, train/grad_norm: 7.973756790161133
Step: 2040, train/learning_rate: 4.757258284371346e-05
Step: 2040, train/epoch: 0.4854831099510193
Step: 2050, train/loss: 0.6664000153541565
Step: 2050, train/grad_norm: 10.290534019470215
Step: 2050, train/learning_rate: 4.756068665301427e-05
Step: 2050, train/epoch: 0.4878629148006439
Step: 2060, train/loss: 0.5864999890327454
Step: 2060, train/grad_norm: 4.910629749298096
Step: 2060, train/learning_rate: 4.7548786824336275e-05
Step: 2060, train/epoch: 0.49024274945259094
Step: 2070, train/loss: 0.6209999918937683
Step: 2070, train/grad_norm: 8.094223976135254
Step: 2070, train/learning_rate: 4.753688699565828e-05
Step: 2070, train/epoch: 0.4926225543022156
Step: 2080, train/loss: 0.5447999835014343
Step: 2080, train/grad_norm: 3.760122299194336
Step: 2080, train/learning_rate: 4.752498716698028e-05
Step: 2080, train/epoch: 0.4950023889541626
Step: 2090, train/loss: 0.5307999849319458
Step: 2090, train/grad_norm: 17.303930282592773
Step: 2090, train/learning_rate: 4.7513087338302284e-05
Step: 2090, train/epoch: 0.49738219380378723
Step: 2100, train/loss: 0.6315000057220459
Step: 2100, train/grad_norm: 13.989327430725098
Step: 2100, train/learning_rate: 4.7501191147603095e-05
Step: 2100, train/epoch: 0.49976202845573425
Step: 2110, train/loss: 0.565500020980835
Step: 2110, train/grad_norm: 13.577457427978516
Step: 2110, train/learning_rate: 4.74892913189251e-05
Step: 2110, train/epoch: 0.5021418333053589
Step: 2120, train/loss: 0.5978999733924866
Step: 2120, train/grad_norm: 6.706502437591553
Step: 2120, train/learning_rate: 4.74773914902471e-05
Step: 2120, train/epoch: 0.5045216679573059
Step: 2130, train/loss: 0.7570000290870667
Step: 2130, train/grad_norm: 3.970949649810791
Step: 2130, train/learning_rate: 4.7465491661569104e-05
Step: 2130, train/epoch: 0.5069015026092529
Step: 2140, train/loss: 0.6193000078201294
Step: 2140, train/grad_norm: 13.18130874633789
Step: 2140, train/learning_rate: 4.7453591832891107e-05
Step: 2140, train/epoch: 0.5092812776565552
Step: 2150, train/loss: 0.6514999866485596
Step: 2150, train/grad_norm: 15.469217300415039
Step: 2150, train/learning_rate: 4.744169564219192e-05
Step: 2150, train/epoch: 0.5116611123085022
Step: 2160, train/loss: 0.570900022983551
Step: 2160, train/grad_norm: 18.16610336303711
Step: 2160, train/learning_rate: 4.742979581351392e-05
Step: 2160, train/epoch: 0.5140409469604492
Step: 2170, train/loss: 0.6514000296592712
Step: 2170, train/grad_norm: 3.919477939605713
Step: 2170, train/learning_rate: 4.741789598483592e-05
Step: 2170, train/epoch: 0.5164207816123962
Step: 2180, train/loss: 0.5997999906539917
Step: 2180, train/grad_norm: 4.607109546661377
Step: 2180, train/learning_rate: 4.7405996156157926e-05
Step: 2180, train/epoch: 0.5188005566596985
Step: 2190, train/loss: 0.6455000042915344
Step: 2190, train/grad_norm: 29.49576187133789
Step: 2190, train/learning_rate: 4.739409632747993e-05
Step: 2190, train/epoch: 0.5211803913116455
Step: 2200, train/loss: 0.593999981880188
Step: 2200, train/grad_norm: 3.746166229248047
Step: 2200, train/learning_rate: 4.738220013678074e-05
Step: 2200, train/epoch: 0.5235602259635925
Step: 2210, train/loss: 0.6435999870300293
Step: 2210, train/grad_norm: 15.071317672729492
Step: 2210, train/learning_rate: 4.737030030810274e-05
Step: 2210, train/epoch: 0.5259400010108948
Step: 2220, train/loss: 0.6189000010490417
Step: 2220, train/grad_norm: 3.352477550506592
Step: 2220, train/learning_rate: 4.7358400479424745e-05
Step: 2220, train/epoch: 0.5283198356628418
Step: 2230, train/loss: 0.544700026512146
Step: 2230, train/grad_norm: 17.797222137451172
Step: 2230, train/learning_rate: 4.734650065074675e-05
Step: 2230, train/epoch: 0.5306996703147888
Step: 2240, train/loss: 0.6378999948501587
Step: 2240, train/grad_norm: 8.83908462524414
Step: 2240, train/learning_rate: 4.733460082206875e-05
Step: 2240, train/epoch: 0.5330795049667358
Step: 2250, train/loss: 0.6040999889373779
Step: 2250, train/grad_norm: 8.522724151611328
Step: 2250, train/learning_rate: 4.732270463136956e-05
Step: 2250, train/epoch: 0.5354592800140381
Step: 2260, train/loss: 0.5468000173568726
Step: 2260, train/grad_norm: 20.5648250579834
Step: 2260, train/learning_rate: 4.7310804802691564e-05
Step: 2260, train/epoch: 0.5378391146659851
Step: 2270, train/loss: 0.5436000227928162
Step: 2270, train/grad_norm: 5.304749011993408
Step: 2270, train/learning_rate: 4.729890497401357e-05
Step: 2270, train/epoch: 0.5402189493179321
Step: 2280, train/loss: 0.6269000172615051
Step: 2280, train/grad_norm: 8.44590950012207
Step: 2280, train/learning_rate: 4.728700514533557e-05
Step: 2280, train/epoch: 0.5425987839698792
Step: 2290, train/loss: 0.5123999714851379
Step: 2290, train/grad_norm: 22.375764846801758
Step: 2290, train/learning_rate: 4.727510531665757e-05
Step: 2290, train/epoch: 0.5449785590171814
Step: 2300, train/loss: 0.6176000237464905
Step: 2300, train/grad_norm: 8.35225772857666
Step: 2300, train/learning_rate: 4.726320912595838e-05
Step: 2300, train/epoch: 0.5473583936691284
Step: 2310, train/loss: 0.5845999717712402
Step: 2310, train/grad_norm: 22.935791015625
Step: 2310, train/learning_rate: 4.7251309297280386e-05
Step: 2310, train/epoch: 0.5497382283210754
Step: 2320, train/loss: 0.5562000274658203
Step: 2320, train/grad_norm: 10.523962020874023
Step: 2320, train/learning_rate: 4.723940946860239e-05
Step: 2320, train/epoch: 0.5521180629730225
Step: 2330, train/loss: 0.5641999840736389
Step: 2330, train/grad_norm: 26.115352630615234
Step: 2330, train/learning_rate: 4.722750963992439e-05
Step: 2330, train/epoch: 0.5544978380203247
Step: 2340, train/loss: 0.5662000179290771
Step: 2340, train/grad_norm: 17.333660125732422
Step: 2340, train/learning_rate: 4.7215609811246395e-05
Step: 2340, train/epoch: 0.5568776726722717
Step: 2350, train/loss: 0.7315000295639038
Step: 2350, train/grad_norm: 25.47972297668457
Step: 2350, train/learning_rate: 4.7203713620547205e-05
Step: 2350, train/epoch: 0.5592575073242188
Step: 2360, train/loss: 0.5485000014305115
Step: 2360, train/grad_norm: 15.61485767364502
Step: 2360, train/learning_rate: 4.719181379186921e-05
Step: 2360, train/epoch: 0.5616373419761658
Step: 2370, train/loss: 0.5213000178337097
Step: 2370, train/grad_norm: 16.1425724029541
Step: 2370, train/learning_rate: 4.717991396319121e-05
Step: 2370, train/epoch: 0.564017117023468
Step: 2380, train/loss: 0.597100019454956
Step: 2380, train/grad_norm: 4.841531753540039
Step: 2380, train/learning_rate: 4.7168014134513214e-05
Step: 2380, train/epoch: 0.566396951675415
Step: 2390, train/loss: 0.5616000294685364
Step: 2390, train/grad_norm: 18.509458541870117
Step: 2390, train/learning_rate: 4.7156117943814024e-05
Step: 2390, train/epoch: 0.5687767863273621
Step: 2400, train/loss: 0.5708000063896179
Step: 2400, train/grad_norm: 19.29082679748535
Step: 2400, train/learning_rate: 4.714421811513603e-05
Step: 2400, train/epoch: 0.5711566209793091
Step: 2410, train/loss: 0.5942999720573425
Step: 2410, train/grad_norm: 14.257741928100586
Step: 2410, train/learning_rate: 4.713231828645803e-05
Step: 2410, train/epoch: 0.5735363960266113
Step: 2420, train/loss: 0.5467000007629395
Step: 2420, train/grad_norm: 11.267068862915039
Step: 2420, train/learning_rate: 4.712041845778003e-05
Step: 2420, train/epoch: 0.5759162306785583
Step: 2430, train/loss: 0.531000018119812
Step: 2430, train/grad_norm: 12.75492000579834
Step: 2430, train/learning_rate: 4.7108518629102036e-05
Step: 2430, train/epoch: 0.5782960653305054
Step: 2440, train/loss: 0.5024999976158142
Step: 2440, train/grad_norm: 3.2234954833984375
Step: 2440, train/learning_rate: 4.7096622438402846e-05
Step: 2440, train/epoch: 0.5806758403778076
Step: 2450, train/loss: 0.6819999814033508
Step: 2450, train/grad_norm: 18.26082992553711
Step: 2450, train/learning_rate: 4.708472260972485e-05
Step: 2450, train/epoch: 0.5830556750297546
Step: 2460, train/loss: 0.5095000267028809
Step: 2460, train/grad_norm: 18.81722068786621
Step: 2460, train/learning_rate: 4.707282278104685e-05
Step: 2460, train/epoch: 0.5854355096817017
Step: 2470, train/loss: 0.6829000115394592
Step: 2470, train/grad_norm: 12.683061599731445
Step: 2470, train/learning_rate: 4.7060922952368855e-05
Step: 2470, train/epoch: 0.5878153443336487
Step: 2480, train/loss: 0.645799994468689
Step: 2480, train/grad_norm: 36.35129928588867
Step: 2480, train/learning_rate: 4.704902312369086e-05
Step: 2480, train/epoch: 0.5901951193809509
Step: 2490, train/loss: 0.7064999938011169
Step: 2490, train/grad_norm: 19.1744327545166
Step: 2490, train/learning_rate: 4.703712693299167e-05
Step: 2490, train/epoch: 0.592574954032898
Step: 2500, train/loss: 0.635699987411499
Step: 2500, train/grad_norm: 4.998068332672119
Step: 2500, train/learning_rate: 4.702522710431367e-05
Step: 2500, train/epoch: 0.594954788684845
Step: 2510, train/loss: 0.5891000032424927
Step: 2510, train/grad_norm: 6.238563060760498
Step: 2510, train/learning_rate: 4.7013327275635675e-05
Step: 2510, train/epoch: 0.597334623336792
Step: 2520, train/loss: 0.6092000007629395
Step: 2520, train/grad_norm: 16.156034469604492
Step: 2520, train/learning_rate: 4.700142744695768e-05
Step: 2520, train/epoch: 0.5997143983840942
Step: 2530, train/loss: 0.5375000238418579
Step: 2530, train/grad_norm: 8.722685813903809
Step: 2530, train/learning_rate: 4.698952761827968e-05
Step: 2530, train/epoch: 0.6020942330360413
Step: 2540, train/loss: 0.6378999948501587
Step: 2540, train/grad_norm: 15.101847648620605
Step: 2540, train/learning_rate: 4.697763142758049e-05
Step: 2540, train/epoch: 0.6044740676879883
Step: 2550, train/loss: 0.5622000098228455
Step: 2550, train/grad_norm: 18.638988494873047
Step: 2550, train/learning_rate: 4.6965731598902494e-05
Step: 2550, train/epoch: 0.6068539023399353
Step: 2560, train/loss: 0.6057000160217285
Step: 2560, train/grad_norm: 4.580227375030518
Step: 2560, train/learning_rate: 4.69538317702245e-05
Step: 2560, train/epoch: 0.6092336773872375
Step: 2570, train/loss: 0.6277999877929688
Step: 2570, train/grad_norm: 9.046487808227539
Step: 2570, train/learning_rate: 4.69419319415465e-05
Step: 2570, train/epoch: 0.6116135120391846
Step: 2580, train/loss: 0.5358999967575073
Step: 2580, train/grad_norm: 28.01892852783203
Step: 2580, train/learning_rate: 4.69300321128685e-05
Step: 2580, train/epoch: 0.6139933466911316
Step: 2590, train/loss: 0.47609999775886536
Step: 2590, train/grad_norm: 12.484630584716797
Step: 2590, train/learning_rate: 4.691813592216931e-05
Step: 2590, train/epoch: 0.6163731813430786
Step: 2600, train/loss: 0.5422000288963318
Step: 2600, train/grad_norm: 25.659751892089844
Step: 2600, train/learning_rate: 4.6906236093491316e-05
Step: 2600, train/epoch: 0.6187529563903809
Step: 2610, train/loss: 0.5115000009536743
Step: 2610, train/grad_norm: 21.708131790161133
Step: 2610, train/learning_rate: 4.689433626481332e-05
Step: 2610, train/epoch: 0.6211327910423279
Step: 2620, train/loss: 0.531000018119812
Step: 2620, train/grad_norm: 27.032873153686523
Step: 2620, train/learning_rate: 4.688243643613532e-05
Step: 2620, train/epoch: 0.6235126256942749
Step: 2630, train/loss: 0.5023999810218811
Step: 2630, train/grad_norm: 8.33800220489502
Step: 2630, train/learning_rate: 4.6870536607457325e-05
Step: 2630, train/epoch: 0.6258924603462219
Step: 2640, train/loss: 0.6498000025749207
Step: 2640, train/grad_norm: 13.804969787597656
Step: 2640, train/learning_rate: 4.6858640416758135e-05
Step: 2640, train/epoch: 0.6282722353935242
Step: 2650, train/loss: 0.5853000283241272
Step: 2650, train/grad_norm: 17.06917381286621
Step: 2650, train/learning_rate: 4.684674058808014e-05
Step: 2650, train/epoch: 0.6306520700454712
Step: 2660, train/loss: 0.6075000166893005
Step: 2660, train/grad_norm: 6.014502048492432
Step: 2660, train/learning_rate: 4.683484075940214e-05
Step: 2660, train/epoch: 0.6330319046974182
Step: 2670, train/loss: 0.5085999965667725
Step: 2670, train/grad_norm: 4.943373680114746
Step: 2670, train/learning_rate: 4.6822940930724144e-05
Step: 2670, train/epoch: 0.6354116797447205
Step: 2680, train/loss: 0.6351000070571899
Step: 2680, train/grad_norm: 3.7172555923461914
Step: 2680, train/learning_rate: 4.681104110204615e-05
Step: 2680, train/epoch: 0.6377915143966675
Step: 2690, train/loss: 0.671500027179718
Step: 2690, train/grad_norm: 15.539655685424805
Step: 2690, train/learning_rate: 4.679914491134696e-05
Step: 2690, train/epoch: 0.6401713490486145
Step: 2700, train/loss: 0.5297999978065491
Step: 2700, train/grad_norm: 6.894989490509033
Step: 2700, train/learning_rate: 4.678724508266896e-05
Step: 2700, train/epoch: 0.6425511837005615
Step: 2710, train/loss: 0.5321999788284302
Step: 2710, train/grad_norm: 6.3351569175720215
Step: 2710, train/learning_rate: 4.677534525399096e-05
Step: 2710, train/epoch: 0.6449309587478638
Step: 2720, train/loss: 0.5960000157356262
Step: 2720, train/grad_norm: 4.7092976570129395
Step: 2720, train/learning_rate: 4.6763445425312966e-05
Step: 2720, train/epoch: 0.6473107933998108
Step: 2730, train/loss: 0.5983999967575073
Step: 2730, train/grad_norm: 12.450798988342285
Step: 2730, train/learning_rate: 4.675154559663497e-05
Step: 2730, train/epoch: 0.6496906280517578
Step: 2740, train/loss: 0.5164999961853027
Step: 2740, train/grad_norm: 10.064329147338867
Step: 2740, train/learning_rate: 4.673964940593578e-05
Step: 2740, train/epoch: 0.6520704627037048
Step: 2750, train/loss: 0.5489000082015991
Step: 2750, train/grad_norm: 6.027235507965088
Step: 2750, train/learning_rate: 4.672774957725778e-05
Step: 2750, train/epoch: 0.6544502377510071
Step: 2760, train/loss: 0.6482999920845032
Step: 2760, train/grad_norm: 4.0845465660095215
Step: 2760, train/learning_rate: 4.6715849748579785e-05
Step: 2760, train/epoch: 0.6568300724029541
Step: 2770, train/loss: 0.5929999947547913
Step: 2770, train/grad_norm: 12.57055377960205
Step: 2770, train/learning_rate: 4.670394991990179e-05
Step: 2770, train/epoch: 0.6592099070549011
Step: 2780, train/loss: 0.5900999903678894
Step: 2780, train/grad_norm: 13.853398323059082
Step: 2780, train/learning_rate: 4.669205009122379e-05
Step: 2780, train/epoch: 0.6615897417068481
Step: 2790, train/loss: 0.6539999842643738
Step: 2790, train/grad_norm: 22.28554344177246
Step: 2790, train/learning_rate: 4.66801539005246e-05
Step: 2790, train/epoch: 0.6639695167541504
Step: 2800, train/loss: 0.6704999804496765
Step: 2800, train/grad_norm: 30.671573638916016
Step: 2800, train/learning_rate: 4.6668254071846604e-05
Step: 2800, train/epoch: 0.6663493514060974
Step: 2810, train/loss: 0.5626000165939331
Step: 2810, train/grad_norm: 3.216313362121582
Step: 2810, train/learning_rate: 4.665635424316861e-05
Step: 2810, train/epoch: 0.6687291860580444
Step: 2820, train/loss: 0.5703999996185303
Step: 2820, train/grad_norm: 17.325063705444336
Step: 2820, train/learning_rate: 4.664445441449061e-05
Step: 2820, train/epoch: 0.6711090207099915
Step: 2830, train/loss: 0.6035000085830688
Step: 2830, train/grad_norm: 27.930742263793945
Step: 2830, train/learning_rate: 4.663255458581261e-05
Step: 2830, train/epoch: 0.6734887957572937
Step: 2840, train/loss: 0.5745999813079834
Step: 2840, train/grad_norm: 16.92591094970703
Step: 2840, train/learning_rate: 4.6620658395113423e-05
Step: 2840, train/epoch: 0.6758686304092407
Step: 2850, train/loss: 0.6449000239372253
Step: 2850, train/grad_norm: 42.80832290649414
Step: 2850, train/learning_rate: 4.6608758566435426e-05
Step: 2850, train/epoch: 0.6782484650611877
Step: 2860, train/loss: 0.5658000111579895
Step: 2860, train/grad_norm: 7.827005386352539
Step: 2860, train/learning_rate: 4.659685873775743e-05
Step: 2860, train/epoch: 0.6806282997131348
Step: 2870, train/loss: 0.5572999715805054
Step: 2870, train/grad_norm: 7.79276180267334
Step: 2870, train/learning_rate: 4.658495890907943e-05
Step: 2870, train/epoch: 0.683008074760437
Step: 2880, train/loss: 0.5906999707221985
Step: 2880, train/grad_norm: 7.02424955368042
Step: 2880, train/learning_rate: 4.6573059080401435e-05
Step: 2880, train/epoch: 0.685387909412384
Step: 2890, train/loss: 0.5723000168800354
Step: 2890, train/grad_norm: 8.695150375366211
Step: 2890, train/learning_rate: 4.6561162889702246e-05
Step: 2890, train/epoch: 0.687767744064331
Step: 2900, train/loss: 0.5264999866485596
Step: 2900, train/grad_norm: 6.090688228607178
Step: 2900, train/learning_rate: 4.654926306102425e-05
Step: 2900, train/epoch: 0.6901475191116333
Step: 2910, train/loss: 0.5317000150680542
Step: 2910, train/grad_norm: 9.186497688293457
Step: 2910, train/learning_rate: 4.653736323234625e-05
Step: 2910, train/epoch: 0.6925273537635803
Step: 2920, train/loss: 0.5618000030517578
Step: 2920, train/grad_norm: 11.218897819519043
Step: 2920, train/learning_rate: 4.6525463403668255e-05
Step: 2920, train/epoch: 0.6949071884155273
Step: 2930, train/loss: 0.5605999827384949
Step: 2930, train/grad_norm: 22.64783477783203
Step: 2930, train/learning_rate: 4.651356357499026e-05
Step: 2930, train/epoch: 0.6972870230674744
Step: 2940, train/loss: 0.512499988079071
Step: 2940, train/grad_norm: 20.20337677001953
Step: 2940, train/learning_rate: 4.650166738429107e-05
Step: 2940, train/epoch: 0.6996667981147766
Step: 2950, train/loss: 0.5728999972343445
Step: 2950, train/grad_norm: 20.097352981567383
Step: 2950, train/learning_rate: 4.648976755561307e-05
Step: 2950, train/epoch: 0.7020466327667236
Step: 2960, train/loss: 0.6966000199317932
Step: 2960, train/grad_norm: 33.529502868652344
Step: 2960, train/learning_rate: 4.6477867726935074e-05
Step: 2960, train/epoch: 0.7044264674186707
Step: 2970, train/loss: 0.5895000100135803
Step: 2970, train/grad_norm: 5.517000198364258
Step: 2970, train/learning_rate: 4.646596789825708e-05
Step: 2970, train/epoch: 0.7068063020706177
Step: 2980, train/loss: 0.6132000088691711
Step: 2980, train/grad_norm: 11.717309951782227
Step: 2980, train/learning_rate: 4.645406806957908e-05
Step: 2980, train/epoch: 0.7091860771179199
Step: 2990, train/loss: 0.576200008392334
Step: 2990, train/grad_norm: 6.033024311065674
Step: 2990, train/learning_rate: 4.644217187887989e-05
Step: 2990, train/epoch: 0.7115659117698669
Step: 3000, train/loss: 0.5853999853134155
Step: 3000, train/grad_norm: 4.7834553718566895
Step: 3000, train/learning_rate: 4.643027205020189e-05
Step: 3000, train/epoch: 0.713945746421814
Step: 3010, train/loss: 0.5683000087738037
Step: 3010, train/grad_norm: 45.163333892822266
Step: 3010, train/learning_rate: 4.6418372221523896e-05
Step: 3010, train/epoch: 0.716325581073761
Step: 3020, train/loss: 0.6068000197410583
Step: 3020, train/grad_norm: 4.315175533294678
Step: 3020, train/learning_rate: 4.64064723928459e-05
Step: 3020, train/epoch: 0.7187053561210632
Step: 3030, train/loss: 0.5210999846458435
Step: 3030, train/grad_norm: 15.032960891723633
Step: 3030, train/learning_rate: 4.63945725641679e-05
Step: 3030, train/epoch: 0.7210851907730103
Step: 3040, train/loss: 0.5120999813079834
Step: 3040, train/grad_norm: 13.987217903137207
Step: 3040, train/learning_rate: 4.638267637346871e-05
Step: 3040, train/epoch: 0.7234650254249573
Step: 3050, train/loss: 0.46389999985694885
Step: 3050, train/grad_norm: 9.78702449798584
Step: 3050, train/learning_rate: 4.6370776544790715e-05
Step: 3050, train/epoch: 0.7258448600769043
Step: 3060, train/loss: 0.616599977016449
Step: 3060, train/grad_norm: 32.5034294128418
Step: 3060, train/learning_rate: 4.635887671611272e-05
Step: 3060, train/epoch: 0.7282246351242065
Step: 3070, train/loss: 0.6215000152587891
Step: 3070, train/grad_norm: 9.530379295349121
Step: 3070, train/learning_rate: 4.634697688743472e-05
Step: 3070, train/epoch: 0.7306044697761536
Step: 3080, train/loss: 0.5570999979972839
Step: 3080, train/grad_norm: 7.034937858581543
Step: 3080, train/learning_rate: 4.6335077058756724e-05
Step: 3080, train/epoch: 0.7329843044281006
Step: 3090, train/loss: 0.49399998784065247
Step: 3090, train/grad_norm: 23.224943161010742
Step: 3090, train/learning_rate: 4.6323180868057534e-05
Step: 3090, train/epoch: 0.7353641390800476
Step: 3100, train/loss: 0.5056999921798706
Step: 3100, train/grad_norm: 8.506211280822754
Step: 3100, train/learning_rate: 4.631128103937954e-05
Step: 3100, train/epoch: 0.7377439141273499
Step: 3110, train/loss: 0.48539999127388
Step: 3110, train/grad_norm: 23.002378463745117
Step: 3110, train/learning_rate: 4.629938121070154e-05
Step: 3110, train/epoch: 0.7401237487792969
Step: 3120, train/loss: 0.6603999733924866
Step: 3120, train/grad_norm: 27.12864112854004
Step: 3120, train/learning_rate: 4.628748138202354e-05
Step: 3120, train/epoch: 0.7425035834312439
Step: 3130, train/loss: 0.7394999861717224
Step: 3130, train/grad_norm: 20.450267791748047
Step: 3130, train/learning_rate: 4.6275581553345546e-05
Step: 3130, train/epoch: 0.7448834180831909
Step: 3140, train/loss: 0.6014000177383423
Step: 3140, train/grad_norm: 8.187220573425293
Step: 3140, train/learning_rate: 4.6263685362646356e-05
Step: 3140, train/epoch: 0.7472631931304932
Step: 3150, train/loss: 0.5623000264167786
Step: 3150, train/grad_norm: 11.715060234069824
Step: 3150, train/learning_rate: 4.625178553396836e-05
Step: 3150, train/epoch: 0.7496430277824402
Step: 3160, train/loss: 0.5289999842643738
Step: 3160, train/grad_norm: 4.245377540588379
Step: 3160, train/learning_rate: 4.623988570529036e-05
Step: 3160, train/epoch: 0.7520228624343872
Step: 3170, train/loss: 0.6723999977111816
Step: 3170, train/grad_norm: 10.404101371765137
Step: 3170, train/learning_rate: 4.6227985876612365e-05
Step: 3170, train/epoch: 0.7544026374816895
Step: 3180, train/loss: 0.6237000226974487
Step: 3180, train/grad_norm: 4.333482265472412
Step: 3180, train/learning_rate: 4.621608604793437e-05
Step: 3180, train/epoch: 0.7567824721336365
Step: 3190, train/loss: 0.5473999977111816
Step: 3190, train/grad_norm: 19.763805389404297
Step: 3190, train/learning_rate: 4.620418985723518e-05
Step: 3190, train/epoch: 0.7591623067855835
Step: 3200, train/loss: 0.6872000098228455
Step: 3200, train/grad_norm: 21.39568328857422
Step: 3200, train/learning_rate: 4.619229002855718e-05
Step: 3200, train/epoch: 0.7615421414375305
Step: 3210, train/loss: 0.6194999814033508
Step: 3210, train/grad_norm: 8.367984771728516
Step: 3210, train/learning_rate: 4.6180390199879184e-05
Step: 3210, train/epoch: 0.7639219164848328
Step: 3220, train/loss: 0.6312999725341797
Step: 3220, train/grad_norm: 5.401419639587402
Step: 3220, train/learning_rate: 4.616849037120119e-05
Step: 3220, train/epoch: 0.7663017511367798
Step: 3230, train/loss: 0.5673999786376953
Step: 3230, train/grad_norm: 4.418122291564941
Step: 3230, train/learning_rate: 4.615659054252319e-05
Step: 3230, train/epoch: 0.7686815857887268
Step: 3240, train/loss: 0.560699999332428
Step: 3240, train/grad_norm: 8.171492576599121
Step: 3240, train/learning_rate: 4.6144694351824e-05
Step: 3240, train/epoch: 0.7710614204406738
Step: 3250, train/loss: 0.6528000235557556
Step: 3250, train/grad_norm: 8.70118236541748
Step: 3250, train/learning_rate: 4.6132794523146003e-05
Step: 3250, train/epoch: 0.7734411954879761
Step: 3260, train/loss: 0.6035000085830688
Step: 3260, train/grad_norm: 3.0953760147094727
Step: 3260, train/learning_rate: 4.6120894694468006e-05
Step: 3260, train/epoch: 0.7758210301399231
Step: 3270, train/loss: 0.5242999792098999
Step: 3270, train/grad_norm: 7.319055080413818
Step: 3270, train/learning_rate: 4.610899486579001e-05
Step: 3270, train/epoch: 0.7782008647918701
Step: 3280, train/loss: 0.461899995803833
Step: 3280, train/grad_norm: 4.566797733306885
Step: 3280, train/learning_rate: 4.609709503711201e-05
Step: 3280, train/epoch: 0.7805806994438171
Step: 3290, train/loss: 0.5175999999046326
Step: 3290, train/grad_norm: 4.525296688079834
Step: 3290, train/learning_rate: 4.608519884641282e-05
Step: 3290, train/epoch: 0.7829604744911194
Step: 3300, train/loss: 0.4429999887943268
Step: 3300, train/grad_norm: 14.619417190551758
Step: 3300, train/learning_rate: 4.6073299017734826e-05
Step: 3300, train/epoch: 0.7853403091430664
Step: 3310, train/loss: 0.5067999958992004
Step: 3310, train/grad_norm: 3.9215643405914307
Step: 3310, train/learning_rate: 4.606139918905683e-05
Step: 3310, train/epoch: 0.7877201437950134
Step: 3320, train/loss: 0.49900001287460327
Step: 3320, train/grad_norm: 12.50268840789795
Step: 3320, train/learning_rate: 4.604949936037883e-05
Step: 3320, train/epoch: 0.7900999784469604
Step: 3330, train/loss: 0.6251999735832214
Step: 3330, train/grad_norm: 6.198301792144775
Step: 3330, train/learning_rate: 4.6037599531700835e-05
Step: 3330, train/epoch: 0.7924797534942627
Step: 3340, train/loss: 0.5913000106811523
Step: 3340, train/grad_norm: 12.309486389160156
Step: 3340, train/learning_rate: 4.6025703341001645e-05
Step: 3340, train/epoch: 0.7948595881462097
Step: 3350, train/loss: 0.5357000231742859
Step: 3350, train/grad_norm: 17.91438102722168
Step: 3350, train/learning_rate: 4.601380351232365e-05
Step: 3350, train/epoch: 0.7972394227981567
Step: 3360, train/loss: 0.6248999834060669
Step: 3360, train/grad_norm: 11.027688026428223
Step: 3360, train/learning_rate: 4.600190368364565e-05
Step: 3360, train/epoch: 0.7996192574501038
Step: 3370, train/loss: 0.616599977016449
Step: 3370, train/grad_norm: 6.1711554527282715
Step: 3370, train/learning_rate: 4.5990003854967654e-05
Step: 3370, train/epoch: 0.801999032497406
Step: 3380, train/loss: 0.6388000249862671
Step: 3380, train/grad_norm: 24.844356536865234
Step: 3380, train/learning_rate: 4.597810402628966e-05
Step: 3380, train/epoch: 0.804378867149353
Step: 3390, train/loss: 0.48820000886917114
Step: 3390, train/grad_norm: 25.837961196899414
Step: 3390, train/learning_rate: 4.596620783559047e-05
Step: 3390, train/epoch: 0.8067587018013
Step: 3400, train/loss: 0.5857999920845032
Step: 3400, train/grad_norm: 38.48484420776367
Step: 3400, train/learning_rate: 4.595430800691247e-05
Step: 3400, train/epoch: 0.8091384768486023
Step: 3410, train/loss: 0.5843999981880188
Step: 3410, train/grad_norm: 7.446201801300049
Step: 3410, train/learning_rate: 4.594240817823447e-05
Step: 3410, train/epoch: 0.8115183115005493
Step: 3420, train/loss: 0.5291000008583069
Step: 3420, train/grad_norm: 6.884984016418457
Step: 3420, train/learning_rate: 4.5930508349556476e-05
Step: 3420, train/epoch: 0.8138981461524963
Step: 3430, train/loss: 0.598800003528595
Step: 3430, train/grad_norm: 16.11137580871582
Step: 3430, train/learning_rate: 4.591860852087848e-05
Step: 3430, train/epoch: 0.8162779808044434
Step: 3440, train/loss: 0.5598000288009644
Step: 3440, train/grad_norm: 24.503700256347656
Step: 3440, train/learning_rate: 4.590671233017929e-05
Step: 3440, train/epoch: 0.8186577558517456
Step: 3450, train/loss: 0.5644000172615051
Step: 3450, train/grad_norm: 9.276968955993652
Step: 3450, train/learning_rate: 4.589481250150129e-05
Step: 3450, train/epoch: 0.8210375905036926
Step: 3460, train/loss: 0.5357000231742859
Step: 3460, train/grad_norm: 11.644370079040527
Step: 3460, train/learning_rate: 4.5882912672823295e-05
Step: 3460, train/epoch: 0.8234174251556396
Step: 3470, train/loss: 0.5275999903678894
Step: 3470, train/grad_norm: 3.7297208309173584
Step: 3470, train/learning_rate: 4.58710128441453e-05
Step: 3470, train/epoch: 0.8257972598075867
Step: 3480, train/loss: 0.48260000348091125
Step: 3480, train/grad_norm: 9.900921821594238
Step: 3480, train/learning_rate: 4.58591130154673e-05
Step: 3480, train/epoch: 0.8281770348548889
Step: 3490, train/loss: 0.6086000204086304
Step: 3490, train/grad_norm: 7.676603317260742
Step: 3490, train/learning_rate: 4.584721682476811e-05
Step: 3490, train/epoch: 0.8305568695068359
Step: 3500, train/loss: 0.5956000089645386
Step: 3500, train/grad_norm: 5.00106954574585
Step: 3500, train/learning_rate: 4.5835316996090114e-05
Step: 3500, train/epoch: 0.832936704158783
Step: 3510, train/loss: 0.5321999788284302
Step: 3510, train/grad_norm: 8.494751930236816
Step: 3510, train/learning_rate: 4.582341716741212e-05
Step: 3510, train/epoch: 0.83531653881073
Step: 3520, train/loss: 0.5030999779701233
Step: 3520, train/grad_norm: 8.933160781860352
Step: 3520, train/learning_rate: 4.581151733873412e-05
Step: 3520, train/epoch: 0.8376963138580322
Step: 3530, train/loss: 0.48750001192092896
Step: 3530, train/grad_norm: 17.42972183227539
Step: 3530, train/learning_rate: 4.579961751005612e-05
Step: 3530, train/epoch: 0.8400761485099792
Step: 3540, train/loss: 0.5776000022888184
Step: 3540, train/grad_norm: 17.989749908447266
Step: 3540, train/learning_rate: 4.578772131935693e-05
Step: 3540, train/epoch: 0.8424559831619263
Step: 3550, train/loss: 0.6651999950408936
Step: 3550, train/grad_norm: 9.200667381286621
Step: 3550, train/learning_rate: 4.5775821490678936e-05
Step: 3550, train/epoch: 0.8448358178138733
Step: 3560, train/loss: 0.5626000165939331
Step: 3560, train/grad_norm: 20.377283096313477
Step: 3560, train/learning_rate: 4.576392166200094e-05
Step: 3560, train/epoch: 0.8472155928611755
Step: 3570, train/loss: 0.5637000203132629
Step: 3570, train/grad_norm: 11.833477973937988
Step: 3570, train/learning_rate: 4.575202183332294e-05
Step: 3570, train/epoch: 0.8495954275131226
Step: 3580, train/loss: 0.45969998836517334
Step: 3580, train/grad_norm: 8.992483139038086
Step: 3580, train/learning_rate: 4.5740122004644945e-05
Step: 3580, train/epoch: 0.8519752621650696
Step: 3590, train/loss: 0.48100000619888306
Step: 3590, train/grad_norm: 6.598354339599609
Step: 3590, train/learning_rate: 4.5728225813945755e-05
Step: 3590, train/epoch: 0.8543550968170166
Step: 3600, train/loss: 0.47999998927116394
Step: 3600, train/grad_norm: 7.803426265716553
Step: 3600, train/learning_rate: 4.571632598526776e-05
Step: 3600, train/epoch: 0.8567348718643188
Step: 3610, train/loss: 0.5485000014305115
Step: 3610, train/grad_norm: 8.82700252532959
Step: 3610, train/learning_rate: 4.570442615658976e-05
Step: 3610, train/epoch: 0.8591147065162659
Step: 3620, train/loss: 0.5275999903678894
Step: 3620, train/grad_norm: 5.341456413269043
Step: 3620, train/learning_rate: 4.5692526327911764e-05
Step: 3620, train/epoch: 0.8614945411682129
Step: 3630, train/loss: 0.6158000230789185
Step: 3630, train/grad_norm: 6.337094783782959
Step: 3630, train/learning_rate: 4.568062649923377e-05
Step: 3630, train/epoch: 0.8638743162155151
Step: 3640, train/loss: 0.4514999985694885
Step: 3640, train/grad_norm: 23.787073135375977
Step: 3640, train/learning_rate: 4.566873030853458e-05
Step: 3640, train/epoch: 0.8662541508674622
Step: 3650, train/loss: 0.5120999813079834
Step: 3650, train/grad_norm: 10.465405464172363
Step: 3650, train/learning_rate: 4.565683047985658e-05
Step: 3650, train/epoch: 0.8686339855194092
Step: 3660, train/loss: 0.5054000020027161
Step: 3660, train/grad_norm: 15.655832290649414
Step: 3660, train/learning_rate: 4.5644930651178584e-05
Step: 3660, train/epoch: 0.8710138201713562
Step: 3670, train/loss: 0.5679000020027161
Step: 3670, train/grad_norm: 26.056201934814453
Step: 3670, train/learning_rate: 4.5633030822500587e-05
Step: 3670, train/epoch: 0.8733935952186584
Step: 3680, train/loss: 0.49149999022483826
Step: 3680, train/grad_norm: 17.469226837158203
Step: 3680, train/learning_rate: 4.562113099382259e-05
Step: 3680, train/epoch: 0.8757734298706055
Step: 3690, train/loss: 0.5756000280380249
Step: 3690, train/grad_norm: 18.080474853515625
Step: 3690, train/learning_rate: 4.56092348031234e-05
Step: 3690, train/epoch: 0.8781532645225525
Step: 3700, train/loss: 0.49639999866485596
Step: 3700, train/grad_norm: 26.916780471801758
Step: 3700, train/learning_rate: 4.55973349744454e-05
Step: 3700, train/epoch: 0.8805330991744995
Step: 3710, train/loss: 0.5960000157356262
Step: 3710, train/grad_norm: 10.268431663513184
Step: 3710, train/learning_rate: 4.5585435145767406e-05
Step: 3710, train/epoch: 0.8829128742218018
Step: 3720, train/loss: 0.5611000061035156
Step: 3720, train/grad_norm: 26.71316146850586
Step: 3720, train/learning_rate: 4.557353531708941e-05
Step: 3720, train/epoch: 0.8852927088737488
Step: 3730, train/loss: 0.6823999881744385
Step: 3730, train/grad_norm: 5.189594745635986
Step: 3730, train/learning_rate: 4.556163912639022e-05
Step: 3730, train/epoch: 0.8876725435256958
Step: 3740, train/loss: 0.5296000242233276
Step: 3740, train/grad_norm: 4.922247886657715
Step: 3740, train/learning_rate: 4.554973929771222e-05
Step: 3740, train/epoch: 0.8900523781776428
Step: 3750, train/loss: 0.5702999830245972
Step: 3750, train/grad_norm: 9.045674324035645
Step: 3750, train/learning_rate: 4.5537839469034225e-05
Step: 3750, train/epoch: 0.8924321532249451
Step: 3760, train/loss: 0.5656999945640564
Step: 3760, train/grad_norm: 15.14492416381836
Step: 3760, train/learning_rate: 4.552593964035623e-05
Step: 3760, train/epoch: 0.8948119878768921
Step: 3770, train/loss: 0.5787000060081482
Step: 3770, train/grad_norm: 5.765282154083252
Step: 3770, train/learning_rate: 4.551403981167823e-05
Step: 3770, train/epoch: 0.8971918225288391
Step: 3780, train/loss: 0.5490999817848206
Step: 3780, train/grad_norm: 11.863601684570312
Step: 3780, train/learning_rate: 4.550214362097904e-05
Step: 3780, train/epoch: 0.8995716571807861
Step: 3790, train/loss: 0.5595999956130981
Step: 3790, train/grad_norm: 15.437949180603027
Step: 3790, train/learning_rate: 4.5490243792301044e-05
Step: 3790, train/epoch: 0.9019514322280884
Step: 3800, train/loss: 0.5638999938964844
Step: 3800, train/grad_norm: 11.73785400390625
Step: 3800, train/learning_rate: 4.547834396362305e-05
Step: 3800, train/epoch: 0.9043312668800354
Step: 3810, train/loss: 0.5266000032424927
Step: 3810, train/grad_norm: 18.399234771728516
Step: 3810, train/learning_rate: 4.546644413494505e-05
Step: 3810, train/epoch: 0.9067111015319824
Step: 3820, train/loss: 0.48510000109672546
Step: 3820, train/grad_norm: 5.01513671875
Step: 3820, train/learning_rate: 4.545454430626705e-05
Step: 3820, train/epoch: 0.9090909361839294
Step: 3830, train/loss: 0.49570000171661377
Step: 3830, train/grad_norm: 13.604833602905273
Step: 3830, train/learning_rate: 4.544264811556786e-05
Step: 3830, train/epoch: 0.9114707112312317
Step: 3840, train/loss: 0.5300999879837036
Step: 3840, train/grad_norm: 10.981117248535156
Step: 3840, train/learning_rate: 4.5430748286889866e-05
Step: 3840, train/epoch: 0.9138505458831787
Step: 3850, train/loss: 0.507099986076355
Step: 3850, train/grad_norm: 13.65113353729248
Step: 3850, train/learning_rate: 4.541884845821187e-05
Step: 3850, train/epoch: 0.9162303805351257
Step: 3860, train/loss: 0.5594000220298767
Step: 3860, train/grad_norm: 8.531611442565918
Step: 3860, train/learning_rate: 4.540694862953387e-05
Step: 3860, train/epoch: 0.9186102151870728
Step: 3870, train/loss: 0.6402000188827515
Step: 3870, train/grad_norm: 37.170658111572266
Step: 3870, train/learning_rate: 4.5395048800855875e-05
Step: 3870, train/epoch: 0.920989990234375
Step: 3880, train/loss: 0.5615000128746033
Step: 3880, train/grad_norm: 11.743011474609375
Step: 3880, train/learning_rate: 4.5383152610156685e-05
Step: 3880, train/epoch: 0.923369824886322
Step: 3890, train/loss: 0.5496000051498413
Step: 3890, train/grad_norm: 15.176253318786621
Step: 3890, train/learning_rate: 4.537125278147869e-05
Step: 3890, train/epoch: 0.925749659538269
Step: 3900, train/loss: 0.5034999847412109
Step: 3900, train/grad_norm: 30.64926528930664
Step: 3900, train/learning_rate: 4.535935295280069e-05
Step: 3900, train/epoch: 0.9281294345855713
Step: 3910, train/loss: 0.6312999725341797
Step: 3910, train/grad_norm: 25.46512794494629
Step: 3910, train/learning_rate: 4.5347453124122694e-05
Step: 3910, train/epoch: 0.9305092692375183
Step: 3920, train/loss: 0.5383999943733215
Step: 3920, train/grad_norm: 14.095008850097656
Step: 3920, train/learning_rate: 4.53355532954447e-05
Step: 3920, train/epoch: 0.9328891038894653
Step: 3930, train/loss: 0.503000020980835
Step: 3930, train/grad_norm: 3.537402391433716
Step: 3930, train/learning_rate: 4.532365710474551e-05
Step: 3930, train/epoch: 0.9352689385414124
Step: 3940, train/loss: 0.5631999969482422
Step: 3940, train/grad_norm: 3.6829264163970947
Step: 3940, train/learning_rate: 4.531175727606751e-05
Step: 3940, train/epoch: 0.9376487135887146
Step: 3950, train/loss: 0.4007999897003174
Step: 3950, train/grad_norm: 12.418829917907715
Step: 3950, train/learning_rate: 4.529985744738951e-05
Step: 3950, train/epoch: 0.9400285482406616
Step: 3960, train/loss: 0.44620001316070557
Step: 3960, train/grad_norm: 7.376079559326172
Step: 3960, train/learning_rate: 4.5287957618711516e-05
Step: 3960, train/epoch: 0.9424083828926086
Step: 3970, train/loss: 0.5407000184059143
Step: 3970, train/grad_norm: 7.2083306312561035
Step: 3970, train/learning_rate: 4.527605779003352e-05
Step: 3970, train/epoch: 0.9447882175445557
Step: 3980, train/loss: 0.5307999849319458
Step: 3980, train/grad_norm: 5.60573148727417
Step: 3980, train/learning_rate: 4.526416159933433e-05
Step: 3980, train/epoch: 0.9471679925918579
Step: 3990, train/loss: 0.5630999803543091
Step: 3990, train/grad_norm: 18.357284545898438
Step: 3990, train/learning_rate: 4.525226177065633e-05
Step: 3990, train/epoch: 0.9495478272438049
Step: 4000, train/loss: 0.4634000062942505
Step: 4000, train/grad_norm: 22.435115814208984
Step: 4000, train/learning_rate: 4.5240361941978335e-05
Step: 4000, train/epoch: 0.951927661895752
Step: 4010, train/loss: 0.4862000048160553
Step: 4010, train/grad_norm: 15.393794059753418
Step: 4010, train/learning_rate: 4.522846211330034e-05
Step: 4010, train/epoch: 0.954307496547699
Step: 4020, train/loss: 0.46860000491142273
Step: 4020, train/grad_norm: 4.173796653747559
Step: 4020, train/learning_rate: 4.521656228462234e-05
Step: 4020, train/epoch: 0.9566872715950012
Step: 4030, train/loss: 0.5163999795913696
Step: 4030, train/grad_norm: 14.201318740844727
Step: 4030, train/learning_rate: 4.520466609392315e-05
Step: 4030, train/epoch: 0.9590671062469482
Step: 4040, train/loss: 0.5443999767303467
Step: 4040, train/grad_norm: 18.311681747436523
Step: 4040, train/learning_rate: 4.5192766265245155e-05
Step: 4040, train/epoch: 0.9614469408988953
Step: 4050, train/loss: 0.7070000171661377
Step: 4050, train/grad_norm: 12.979385375976562
Step: 4050, train/learning_rate: 4.518086643656716e-05
Step: 4050, train/epoch: 0.9638267755508423
Step: 4060, train/loss: 0.4180000126361847
Step: 4060, train/grad_norm: 11.438366889953613
Step: 4060, train/learning_rate: 4.516896660788916e-05
Step: 4060, train/epoch: 0.9662065505981445
Step: 4070, train/loss: 0.534500002861023
Step: 4070, train/grad_norm: 16.064041137695312
Step: 4070, train/learning_rate: 4.5157066779211164e-05
Step: 4070, train/epoch: 0.9685863852500916
Step: 4080, train/loss: 0.5095000267028809
Step: 4080, train/grad_norm: 15.145556449890137
Step: 4080, train/learning_rate: 4.5145170588511974e-05
Step: 4080, train/epoch: 0.9709662199020386
Step: 4090, train/loss: 0.5472000241279602
Step: 4090, train/grad_norm: 11.451428413391113
Step: 4090, train/learning_rate: 4.513327075983398e-05
Step: 4090, train/epoch: 0.9733460545539856
Step: 4100, train/loss: 0.5809000134468079
Step: 4100, train/grad_norm: 12.418059349060059
Step: 4100, train/learning_rate: 4.512137093115598e-05
Step: 4100, train/epoch: 0.9757258296012878
Step: 4110, train/loss: 0.535099983215332
Step: 4110, train/grad_norm: 29.665096282958984
Step: 4110, train/learning_rate: 4.510947110247798e-05
Step: 4110, train/epoch: 0.9781056642532349
Step: 4120, train/loss: 0.6320000290870667
Step: 4120, train/grad_norm: 17.11904525756836
Step: 4120, train/learning_rate: 4.5097571273799986e-05
Step: 4120, train/epoch: 0.9804854989051819
Step: 4130, train/loss: 0.5267999768257141
Step: 4130, train/grad_norm: 8.278660774230957
Step: 4130, train/learning_rate: 4.5085675083100796e-05
Step: 4130, train/epoch: 0.9828652739524841
Step: 4140, train/loss: 0.5591999888420105
Step: 4140, train/grad_norm: 7.577389717102051
Step: 4140, train/learning_rate: 4.50737752544228e-05
Step: 4140, train/epoch: 0.9852451086044312
Step: 4150, train/loss: 0.6190999746322632
Step: 4150, train/grad_norm: 8.626518249511719
Step: 4150, train/learning_rate: 4.50618754257448e-05
Step: 4150, train/epoch: 0.9876249432563782
Step: 4160, train/loss: 0.4909999966621399
Step: 4160, train/grad_norm: 17.479312896728516
Step: 4160, train/learning_rate: 4.5049975597066805e-05
Step: 4160, train/epoch: 0.9900047779083252
Step: 4170, train/loss: 0.4715000092983246
Step: 4170, train/grad_norm: 6.851343154907227
Step: 4170, train/learning_rate: 4.503807576838881e-05
Step: 4170, train/epoch: 0.9923845529556274
Step: 4180, train/loss: 0.6046000123023987
Step: 4180, train/grad_norm: 18.636615753173828
Step: 4180, train/learning_rate: 4.502617957768962e-05
Step: 4180, train/epoch: 0.9947643876075745
Step: 4190, train/loss: 0.6557999849319458
Step: 4190, train/grad_norm: 18.075031280517578
Step: 4190, train/learning_rate: 4.501427974901162e-05
Step: 4190, train/epoch: 0.9971442222595215
Step: 4200, train/loss: 0.6083999872207642
Step: 4200, train/grad_norm: 11.337906837463379
Step: 4200, train/learning_rate: 4.5002379920333624e-05
Step: 4200, train/epoch: 0.9995240569114685
Step: 4202, eval/loss: 0.8236786127090454
Step: 4202, eval/accuracy: 0.5322782397270203
Step: 4202, eval/f1: 0.5121814608573914
Step: 4202, eval/runtime: 54.6859016418457
Step: 4202, eval/samples_per_second: 131.71600341796875
Step: 4202, eval/steps_per_second: 16.47599983215332
Step: 4202, train/epoch: 1.0
Step: 4210, train/loss: 0.49380001425743103
Step: 4210, train/grad_norm: 5.8084940910339355
Step: 4210, train/learning_rate: 4.499048009165563e-05
Step: 4210, train/epoch: 1.0019038915634155
Step: 4220, train/loss: 0.4666000008583069
Step: 4220, train/grad_norm: 10.49310302734375
Step: 4220, train/learning_rate: 4.497858026297763e-05
Step: 4220, train/epoch: 1.0042836666107178
Step: 4230, train/loss: 0.6172000169754028
Step: 4230, train/grad_norm: 22.96649742126465
Step: 4230, train/learning_rate: 4.496668407227844e-05
Step: 4230, train/epoch: 1.00666344165802
Step: 4240, train/loss: 0.5343999862670898
Step: 4240, train/grad_norm: 10.468408584594727
Step: 4240, train/learning_rate: 4.495478424360044e-05
Step: 4240, train/epoch: 1.0090433359146118
Step: 4250, train/loss: 0.4779999852180481
Step: 4250, train/grad_norm: 4.825917720794678
Step: 4250, train/learning_rate: 4.4942884414922446e-05
Step: 4250, train/epoch: 1.011423110961914
Step: 4260, train/loss: 0.41190001368522644
Step: 4260, train/grad_norm: 17.100107192993164
Step: 4260, train/learning_rate: 4.493098458624445e-05
Step: 4260, train/epoch: 1.0138030052185059
Step: 4270, train/loss: 0.5473999977111816
Step: 4270, train/grad_norm: 18.083314895629883
Step: 4270, train/learning_rate: 4.491908475756645e-05
Step: 4270, train/epoch: 1.016182780265808
Step: 4280, train/loss: 0.5421000123023987
Step: 4280, train/grad_norm: 9.180100440979004
Step: 4280, train/learning_rate: 4.490718856686726e-05
Step: 4280, train/epoch: 1.0185625553131104
Step: 4290, train/loss: 0.6496999859809875
Step: 4290, train/grad_norm: 25.737354278564453
Step: 4290, train/learning_rate: 4.4895288738189265e-05
Step: 4290, train/epoch: 1.0209424495697021
Step: 4300, train/loss: 0.4864000082015991
Step: 4300, train/grad_norm: 5.476737976074219
Step: 4300, train/learning_rate: 4.488338890951127e-05
Step: 4300, train/epoch: 1.0233222246170044
Step: 4310, train/loss: 0.5135999917984009
Step: 4310, train/grad_norm: 20.085372924804688
Step: 4310, train/learning_rate: 4.487148908083327e-05
Step: 4310, train/epoch: 1.0257019996643066
Step: 4320, train/loss: 0.504800021648407
Step: 4320, train/grad_norm: 6.342214107513428
Step: 4320, train/learning_rate: 4.4859589252155274e-05
Step: 4320, train/epoch: 1.0280818939208984
Step: 4330, train/loss: 0.57669997215271
Step: 4330, train/grad_norm: 24.429733276367188
Step: 4330, train/learning_rate: 4.4847693061456084e-05
Step: 4330, train/epoch: 1.0304616689682007
Step: 4340, train/loss: 0.42489999532699585
Step: 4340, train/grad_norm: 12.682880401611328
Step: 4340, train/learning_rate: 4.483579323277809e-05
Step: 4340, train/epoch: 1.0328415632247925
Step: 4350, train/loss: 0.47189998626708984
Step: 4350, train/grad_norm: 17.371763229370117
Step: 4350, train/learning_rate: 4.482389340410009e-05
Step: 4350, train/epoch: 1.0352213382720947
Step: 4360, train/loss: 0.5612000226974487
Step: 4360, train/grad_norm: 11.17258358001709
Step: 4360, train/learning_rate: 4.481199357542209e-05
Step: 4360, train/epoch: 1.037601113319397
Step: 4370, train/loss: 0.42080000042915344
Step: 4370, train/grad_norm: 10.824541091918945
Step: 4370, train/learning_rate: 4.4800093746744096e-05
Step: 4370, train/epoch: 1.0399810075759888
Step: 4380, train/loss: 0.5077999830245972
Step: 4380, train/grad_norm: 4.509608268737793
Step: 4380, train/learning_rate: 4.4788197556044906e-05
Step: 4380, train/epoch: 1.042360782623291
Step: 4390, train/loss: 0.45989999175071716
Step: 4390, train/grad_norm: 6.2397050857543945
Step: 4390, train/learning_rate: 4.477629772736691e-05
Step: 4390, train/epoch: 1.0447405576705933
Step: 4400, train/loss: 0.5135999917984009
Step: 4400, train/grad_norm: 6.787052154541016
Step: 4400, train/learning_rate: 4.476439789868891e-05
Step: 4400, train/epoch: 1.047120451927185
Step: 4410, train/loss: 0.4636000096797943
Step: 4410, train/grad_norm: 22.320919036865234
Step: 4410, train/learning_rate: 4.4752498070010915e-05
Step: 4410, train/epoch: 1.0495002269744873
Step: 4420, train/loss: 0.5439000129699707
Step: 4420, train/grad_norm: 21.444860458374023
Step: 4420, train/learning_rate: 4.474059824133292e-05
Step: 4420, train/epoch: 1.0518800020217896
Step: 4430, train/loss: 0.33500000834465027
Step: 4430, train/grad_norm: 22.66848373413086
Step: 4430, train/learning_rate: 4.472870205063373e-05
Step: 4430, train/epoch: 1.0542598962783813
Step: 4440, train/loss: 0.44339999556541443
Step: 4440, train/grad_norm: 7.545251369476318
Step: 4440, train/learning_rate: 4.471680222195573e-05
Step: 4440, train/epoch: 1.0566396713256836
Step: 4450, train/loss: 0.5432999730110168
Step: 4450, train/grad_norm: 7.2331223487854
Step: 4450, train/learning_rate: 4.4704902393277735e-05
Step: 4450, train/epoch: 1.0590195655822754
Step: 4460, train/loss: 0.5031999945640564
Step: 4460, train/grad_norm: 15.105358123779297
Step: 4460, train/learning_rate: 4.469300256459974e-05
Step: 4460, train/epoch: 1.0613993406295776
Step: 4470, train/loss: 0.4650000035762787
Step: 4470, train/grad_norm: 5.157001972198486
Step: 4470, train/learning_rate: 4.468110273592174e-05
Step: 4470, train/epoch: 1.0637791156768799
Step: 4480, train/loss: 0.541700005531311
Step: 4480, train/grad_norm: 16.999549865722656
Step: 4480, train/learning_rate: 4.466920654522255e-05
Step: 4480, train/epoch: 1.0661590099334717
Step: 4490, train/loss: 0.5498999953269958
Step: 4490, train/grad_norm: 5.808089733123779
Step: 4490, train/learning_rate: 4.4657306716544554e-05
Step: 4490, train/epoch: 1.068538784980774
Step: 4500, train/loss: 0.6718999743461609
Step: 4500, train/grad_norm: 24.474538803100586
Step: 4500, train/learning_rate: 4.464540688786656e-05
Step: 4500, train/epoch: 1.0709185600280762
Step: 4510, train/loss: 0.5835999846458435
Step: 4510, train/grad_norm: 11.193191528320312
Step: 4510, train/learning_rate: 4.463350705918856e-05
Step: 4510, train/epoch: 1.073298454284668
Step: 4520, train/loss: 0.445499986410141
Step: 4520, train/grad_norm: 7.641384601593018
Step: 4520, train/learning_rate: 4.462160723051056e-05
Step: 4520, train/epoch: 1.0756782293319702
Step: 4530, train/loss: 0.6197999715805054
Step: 4530, train/grad_norm: 7.612380504608154
Step: 4530, train/learning_rate: 4.460971103981137e-05
Step: 4530, train/epoch: 1.078058123588562
Step: 4540, train/loss: 0.595300018787384
Step: 4540, train/grad_norm: 9.864559173583984
Step: 4540, train/learning_rate: 4.4597811211133376e-05
Step: 4540, train/epoch: 1.0804378986358643
Step: 4550, train/loss: 0.5666000247001648
Step: 4550, train/grad_norm: 13.722670555114746
Step: 4550, train/learning_rate: 4.458591138245538e-05
Step: 4550, train/epoch: 1.0828176736831665
Step: 4560, train/loss: 0.5299000144004822
Step: 4560, train/grad_norm: 13.853313446044922
Step: 4560, train/learning_rate: 4.457401155377738e-05
Step: 4560, train/epoch: 1.0851975679397583
Step: 4570, train/loss: 0.6669999957084656
Step: 4570, train/grad_norm: 4.93814754486084
Step: 4570, train/learning_rate: 4.4562111725099385e-05
Step: 4570, train/epoch: 1.0875773429870605
Step: 4580, train/loss: 0.43230000138282776
Step: 4580, train/grad_norm: 7.266514301300049
Step: 4580, train/learning_rate: 4.4550215534400195e-05
Step: 4580, train/epoch: 1.0899571180343628
Step: 4590, train/loss: 0.5497999787330627
Step: 4590, train/grad_norm: 18.915925979614258
Step: 4590, train/learning_rate: 4.45383157057222e-05
Step: 4590, train/epoch: 1.0923370122909546
Step: 4600, train/loss: 0.4893999993801117
Step: 4600, train/grad_norm: 11.095498085021973
Step: 4600, train/learning_rate: 4.45264158770442e-05
Step: 4600, train/epoch: 1.0947167873382568
Step: 4610, train/loss: 0.7389000058174133
Step: 4610, train/grad_norm: 20.135208129882812
Step: 4610, train/learning_rate: 4.4514516048366204e-05
Step: 4610, train/epoch: 1.097096562385559
Step: 4620, train/loss: 0.4293000102043152
Step: 4620, train/grad_norm: 5.983705520629883
Step: 4620, train/learning_rate: 4.450261621968821e-05
Step: 4620, train/epoch: 1.0994764566421509
Step: 4630, train/loss: 0.7364000082015991
Step: 4630, train/grad_norm: 6.4000630378723145
Step: 4630, train/learning_rate: 4.449072002898902e-05
Step: 4630, train/epoch: 1.1018562316894531
Step: 4640, train/loss: 0.6434000134468079
Step: 4640, train/grad_norm: 16.28225326538086
Step: 4640, train/learning_rate: 4.447882020031102e-05
Step: 4640, train/epoch: 1.104236125946045
Step: 4650, train/loss: 0.4438999891281128
Step: 4650, train/grad_norm: 7.189256191253662
Step: 4650, train/learning_rate: 4.446692037163302e-05
Step: 4650, train/epoch: 1.1066159009933472
Step: 4660, train/loss: 0.554099977016449
Step: 4660, train/grad_norm: 4.858027458190918
Step: 4660, train/learning_rate: 4.4455020542955026e-05
Step: 4660, train/epoch: 1.1089956760406494
Step: 4670, train/loss: 0.5313000082969666
Step: 4670, train/grad_norm: 13.536725044250488
Step: 4670, train/learning_rate: 4.444312071427703e-05
Step: 4670, train/epoch: 1.1113755702972412
Step: 4680, train/loss: 0.48820000886917114
Step: 4680, train/grad_norm: 11.183284759521484
Step: 4680, train/learning_rate: 4.443122452357784e-05
Step: 4680, train/epoch: 1.1137553453445435
Step: 4690, train/loss: 0.461899995803833
Step: 4690, train/grad_norm: 17.55727195739746
Step: 4690, train/learning_rate: 4.441932469489984e-05
Step: 4690, train/epoch: 1.1161351203918457
Step: 4700, train/loss: 0.604200005531311
Step: 4700, train/grad_norm: 6.026260852813721
Step: 4700, train/learning_rate: 4.4407424866221845e-05
Step: 4700, train/epoch: 1.1185150146484375
Step: 4710, train/loss: 0.5953999757766724
Step: 4710, train/grad_norm: 6.116644382476807
Step: 4710, train/learning_rate: 4.439552503754385e-05
Step: 4710, train/epoch: 1.1208947896957397
Step: 4720, train/loss: 0.558899998664856
Step: 4720, train/grad_norm: 10.55243968963623
Step: 4720, train/learning_rate: 4.438362520886585e-05
Step: 4720, train/epoch: 1.1232746839523315
Step: 4730, train/loss: 0.5641000270843506
Step: 4730, train/grad_norm: 5.462919235229492
Step: 4730, train/learning_rate: 4.437172901816666e-05
Step: 4730, train/epoch: 1.1256544589996338
Step: 4740, train/loss: 0.5457000136375427
Step: 4740, train/grad_norm: 19.17960548400879
Step: 4740, train/learning_rate: 4.4359829189488664e-05
Step: 4740, train/epoch: 1.128034234046936
Step: 4750, train/loss: 0.3889000117778778
Step: 4750, train/grad_norm: 5.9645771980285645
Step: 4750, train/learning_rate: 4.434792936081067e-05
Step: 4750, train/epoch: 1.1304141283035278
Step: 4760, train/loss: 0.6114000082015991
Step: 4760, train/grad_norm: 30.44278335571289
Step: 4760, train/learning_rate: 4.433602953213267e-05
Step: 4760, train/epoch: 1.13279390335083
Step: 4770, train/loss: 0.4185999929904938
Step: 4770, train/grad_norm: 4.825801372528076
Step: 4770, train/learning_rate: 4.432412970345467e-05
Step: 4770, train/epoch: 1.1351736783981323
Step: 4780, train/loss: 0.5092999935150146
Step: 4780, train/grad_norm: 10.894355773925781
Step: 4780, train/learning_rate: 4.4312233512755483e-05
Step: 4780, train/epoch: 1.1375535726547241
Step: 4790, train/loss: 0.5175999999046326
Step: 4790, train/grad_norm: 6.382715225219727
Step: 4790, train/learning_rate: 4.4300333684077486e-05
Step: 4790, train/epoch: 1.1399333477020264
Step: 4800, train/loss: 0.551800012588501
Step: 4800, train/grad_norm: 7.8564629554748535
Step: 4800, train/learning_rate: 4.428843385539949e-05
Step: 4800, train/epoch: 1.1423132419586182
Step: 4810, train/loss: 0.38119998574256897
Step: 4810, train/grad_norm: 9.08163833618164
Step: 4810, train/learning_rate: 4.427653402672149e-05
Step: 4810, train/epoch: 1.1446930170059204
Step: 4820, train/loss: 0.5716000199317932
Step: 4820, train/grad_norm: 8.114486694335938
Step: 4820, train/learning_rate: 4.4264634198043495e-05
Step: 4820, train/epoch: 1.1470727920532227
Step: 4830, train/loss: 0.4697999954223633
Step: 4830, train/grad_norm: 5.260573387145996
Step: 4830, train/learning_rate: 4.4252738007344306e-05
Step: 4830, train/epoch: 1.1494526863098145
Step: 4840, train/loss: 0.438400000333786
Step: 4840, train/grad_norm: 6.99287748336792
Step: 4840, train/learning_rate: 4.424083817866631e-05
Step: 4840, train/epoch: 1.1518324613571167
Step: 4850, train/loss: 0.43070000410079956
Step: 4850, train/grad_norm: 13.54545783996582
Step: 4850, train/learning_rate: 4.422893834998831e-05
Step: 4850, train/epoch: 1.154212236404419
Step: 4860, train/loss: 0.43650001287460327
Step: 4860, train/grad_norm: 3.3117716312408447
Step: 4860, train/learning_rate: 4.4217038521310315e-05
Step: 4860, train/epoch: 1.1565921306610107
Step: 4870, train/loss: 0.5929999947547913
Step: 4870, train/grad_norm: 12.70671558380127
Step: 4870, train/learning_rate: 4.420513869263232e-05
Step: 4870, train/epoch: 1.158971905708313
Step: 4880, train/loss: 0.5412999987602234
Step: 4880, train/grad_norm: 15.563721656799316
Step: 4880, train/learning_rate: 4.419324250193313e-05
Step: 4880, train/epoch: 1.1613516807556152
Step: 4890, train/loss: 0.8004999756813049
Step: 4890, train/grad_norm: 8.163644790649414
Step: 4890, train/learning_rate: 4.418134267325513e-05
Step: 4890, train/epoch: 1.163731575012207
Step: 4900, train/loss: 0.4945000112056732
Step: 4900, train/grad_norm: 6.566259384155273
Step: 4900, train/learning_rate: 4.4169442844577134e-05
Step: 4900, train/epoch: 1.1661113500595093
Step: 4910, train/loss: 0.5242000222206116
Step: 4910, train/grad_norm: 13.98803997039795
Step: 4910, train/learning_rate: 4.415754301589914e-05
Step: 4910, train/epoch: 1.168491244316101
Step: 4920, train/loss: 0.5688999891281128
Step: 4920, train/grad_norm: 12.548935890197754
Step: 4920, train/learning_rate: 4.414564318722114e-05
Step: 4920, train/epoch: 1.1708710193634033
Step: 4930, train/loss: 0.4796999990940094
Step: 4930, train/grad_norm: 5.891964435577393
Step: 4930, train/learning_rate: 4.413374699652195e-05
Step: 4930, train/epoch: 1.1732507944107056
Step: 4940, train/loss: 0.5074999928474426
Step: 4940, train/grad_norm: 11.095626831054688
Step: 4940, train/learning_rate: 4.412184716784395e-05
Step: 4940, train/epoch: 1.1756306886672974
Step: 4950, train/loss: 0.5006999969482422
Step: 4950, train/grad_norm: 7.253875255584717
Step: 4950, train/learning_rate: 4.4109947339165956e-05
Step: 4950, train/epoch: 1.1780104637145996
Step: 4960, train/loss: 0.5364000201225281
Step: 4960, train/grad_norm: 9.585832595825195
Step: 4960, train/learning_rate: 4.409804751048796e-05
Step: 4960, train/epoch: 1.1803902387619019
Step: 4970, train/loss: 0.5389000177383423
Step: 4970, train/grad_norm: 18.21906852722168
Step: 4970, train/learning_rate: 4.408614768180996e-05
Step: 4970, train/epoch: 1.1827701330184937
Step: 4980, train/loss: 0.516700029373169
Step: 4980, train/grad_norm: 7.163687705993652
Step: 4980, train/learning_rate: 4.407425149111077e-05
Step: 4980, train/epoch: 1.185149908065796
Step: 4990, train/loss: 0.4717999994754791
Step: 4990, train/grad_norm: 8.098367691040039
Step: 4990, train/learning_rate: 4.4062351662432775e-05
Step: 4990, train/epoch: 1.1875298023223877
Step: 5000, train/loss: 0.46070000529289246
Step: 5000, train/grad_norm: 8.545344352722168
Step: 5000, train/learning_rate: 4.405045183375478e-05
Step: 5000, train/epoch: 1.18990957736969
Step: 5010, train/loss: 0.6001999974250793
Step: 5010, train/grad_norm: 9.650908470153809
Step: 5010, train/learning_rate: 4.403855200507678e-05
Step: 5010, train/epoch: 1.1922893524169922
Step: 5020, train/loss: 0.5162000060081482
Step: 5020, train/grad_norm: 14.787177085876465
Step: 5020, train/learning_rate: 4.4026652176398784e-05
Step: 5020, train/epoch: 1.194669246673584
Step: 5030, train/loss: 0.5192999839782715
Step: 5030, train/grad_norm: 4.697476387023926
Step: 5030, train/learning_rate: 4.4014755985699594e-05
Step: 5030, train/epoch: 1.1970490217208862
Step: 5040, train/loss: 0.478300005197525
Step: 5040, train/grad_norm: 6.447154521942139
Step: 5040, train/learning_rate: 4.40028561570216e-05
Step: 5040, train/epoch: 1.1994287967681885
Step: 5050, train/loss: 0.6116999983787537
Step: 5050, train/grad_norm: 20.31116485595703
Step: 5050, train/learning_rate: 4.39909563283436e-05
Step: 5050, train/epoch: 1.2018086910247803
Step: 5060, train/loss: 0.4869999885559082
Step: 5060, train/grad_norm: 13.78125286102295
Step: 5060, train/learning_rate: 4.39790564996656e-05
Step: 5060, train/epoch: 1.2041884660720825
Step: 5070, train/loss: 0.5878999829292297
Step: 5070, train/grad_norm: 21.62720489501953
Step: 5070, train/learning_rate: 4.396716030896641e-05
Step: 5070, train/epoch: 1.2065683603286743
Step: 5080, train/loss: 0.5365999937057495
Step: 5080, train/grad_norm: 7.066341400146484
Step: 5080, train/learning_rate: 4.3955260480288416e-05
Step: 5080, train/epoch: 1.2089481353759766
Step: 5090, train/loss: 0.534500002861023
Step: 5090, train/grad_norm: 20.922876358032227
Step: 5090, train/learning_rate: 4.394336065161042e-05
Step: 5090, train/epoch: 1.2113279104232788
Step: 5100, train/loss: 0.4839000105857849
Step: 5100, train/grad_norm: 11.976510047912598
Step: 5100, train/learning_rate: 4.393146082293242e-05
Step: 5100, train/epoch: 1.2137078046798706
Step: 5110, train/loss: 0.44760000705718994
Step: 5110, train/grad_norm: 10.501851081848145
Step: 5110, train/learning_rate: 4.3919560994254425e-05
Step: 5110, train/epoch: 1.2160875797271729
Step: 5120, train/loss: 0.534600019454956
Step: 5120, train/grad_norm: 9.787814140319824
Step: 5120, train/learning_rate: 4.3907664803555235e-05
Step: 5120, train/epoch: 1.218467354774475
Step: 5130, train/loss: 0.4837999939918518
Step: 5130, train/grad_norm: 27.065305709838867
Step: 5130, train/learning_rate: 4.389576497487724e-05
Step: 5130, train/epoch: 1.220847249031067
Step: 5140, train/loss: 0.4643000066280365
Step: 5140, train/grad_norm: 8.08708667755127
Step: 5140, train/learning_rate: 4.388386514619924e-05
Step: 5140, train/epoch: 1.2232270240783691
Step: 5150, train/loss: 0.5597000122070312
Step: 5150, train/grad_norm: 9.927491188049316
Step: 5150, train/learning_rate: 4.3871965317521244e-05
Step: 5150, train/epoch: 1.2256067991256714
Step: 5160, train/loss: 0.5034000277519226
Step: 5160, train/grad_norm: 8.592327117919922
Step: 5160, train/learning_rate: 4.386006548884325e-05
Step: 5160, train/epoch: 1.2279866933822632
Step: 5170, train/loss: 0.47540000081062317
Step: 5170, train/grad_norm: 31.191024780273438
Step: 5170, train/learning_rate: 4.384816929814406e-05
Step: 5170, train/epoch: 1.2303664684295654
Step: 5180, train/loss: 0.4341000020503998
Step: 5180, train/grad_norm: 27.442827224731445
Step: 5180, train/learning_rate: 4.383626946946606e-05
Step: 5180, train/epoch: 1.2327463626861572
Step: 5190, train/loss: 0.807699978351593
Step: 5190, train/grad_norm: 12.135199546813965
Step: 5190, train/learning_rate: 4.3824369640788063e-05
Step: 5190, train/epoch: 1.2351261377334595
Step: 5200, train/loss: 0.7479000091552734
Step: 5200, train/grad_norm: 17.144607543945312
Step: 5200, train/learning_rate: 4.3812469812110066e-05
Step: 5200, train/epoch: 1.2375059127807617
Step: 5210, train/loss: 0.5819000005722046
Step: 5210, train/grad_norm: 17.10074234008789
Step: 5210, train/learning_rate: 4.380056998343207e-05
Step: 5210, train/epoch: 1.2398858070373535
Step: 5220, train/loss: 0.5583999752998352
Step: 5220, train/grad_norm: 6.119487285614014
Step: 5220, train/learning_rate: 4.378867379273288e-05
Step: 5220, train/epoch: 1.2422655820846558
Step: 5230, train/loss: 0.5113000273704529
Step: 5230, train/grad_norm: 16.070903778076172
Step: 5230, train/learning_rate: 4.377677396405488e-05
Step: 5230, train/epoch: 1.244645357131958
Step: 5240, train/loss: 0.5461000204086304
Step: 5240, train/grad_norm: 13.25240421295166
Step: 5240, train/learning_rate: 4.3764874135376886e-05
Step: 5240, train/epoch: 1.2470252513885498
Step: 5250, train/loss: 0.5534999966621399
Step: 5250, train/grad_norm: 6.242483139038086
Step: 5250, train/learning_rate: 4.375297430669889e-05
Step: 5250, train/epoch: 1.249405026435852
Step: 5260, train/loss: 0.46480000019073486
Step: 5260, train/grad_norm: 15.587615013122559
Step: 5260, train/learning_rate: 4.374107447802089e-05
Step: 5260, train/epoch: 1.2517849206924438
Step: 5270, train/loss: 0.4916999936103821
Step: 5270, train/grad_norm: 7.016241073608398
Step: 5270, train/learning_rate: 4.37291782873217e-05
Step: 5270, train/epoch: 1.254164695739746
Step: 5280, train/loss: 0.46050000190734863
Step: 5280, train/grad_norm: 5.670767784118652
Step: 5280, train/learning_rate: 4.3717278458643705e-05
Step: 5280, train/epoch: 1.2565444707870483
Step: 5290, train/loss: 0.6531999707221985
Step: 5290, train/grad_norm: 14.808948516845703
Step: 5290, train/learning_rate: 4.370537862996571e-05
Step: 5290, train/epoch: 1.2589243650436401
Step: 5300, train/loss: 0.6015999913215637
Step: 5300, train/grad_norm: 12.323293685913086
Step: 5300, train/learning_rate: 4.369347880128771e-05
Step: 5300, train/epoch: 1.2613041400909424
Step: 5310, train/loss: 0.6223999857902527
Step: 5310, train/grad_norm: 5.482438564300537
Step: 5310, train/learning_rate: 4.3681578972609714e-05
Step: 5310, train/epoch: 1.2636839151382446
Step: 5320, train/loss: 0.6467000246047974
Step: 5320, train/grad_norm: 29.354507446289062
Step: 5320, train/learning_rate: 4.3669682781910524e-05
Step: 5320, train/epoch: 1.2660638093948364
Step: 5330, train/loss: 0.5309000015258789
Step: 5330, train/grad_norm: 31.947267532348633
Step: 5330, train/learning_rate: 4.365778295323253e-05
Step: 5330, train/epoch: 1.2684435844421387
Step: 5340, train/loss: 0.544700026512146
Step: 5340, train/grad_norm: 8.914163589477539
Step: 5340, train/learning_rate: 4.364588312455453e-05
Step: 5340, train/epoch: 1.270823359489441
Step: 5350, train/loss: 0.592199981212616
Step: 5350, train/grad_norm: 7.733737945556641
Step: 5350, train/learning_rate: 4.363398329587653e-05
Step: 5350, train/epoch: 1.2732032537460327
Step: 5360, train/loss: 0.44359999895095825
Step: 5360, train/grad_norm: 23.408767700195312
Step: 5360, train/learning_rate: 4.3622083467198536e-05
Step: 5360, train/epoch: 1.275583028793335
Step: 5370, train/loss: 0.37059998512268066
Step: 5370, train/grad_norm: 6.934601783752441
Step: 5370, train/learning_rate: 4.3610187276499346e-05
Step: 5370, train/epoch: 1.2779629230499268
Step: 5380, train/loss: 0.6528000235557556
Step: 5380, train/grad_norm: 16.34528350830078
Step: 5380, train/learning_rate: 4.359828744782135e-05
Step: 5380, train/epoch: 1.280342698097229
Step: 5390, train/loss: 0.4415000081062317
Step: 5390, train/grad_norm: 4.790487289428711
Step: 5390, train/learning_rate: 4.358638761914335e-05
Step: 5390, train/epoch: 1.2827224731445312
Step: 5400, train/loss: 0.5788999795913696
Step: 5400, train/grad_norm: 12.926816940307617
Step: 5400, train/learning_rate: 4.3574487790465355e-05
Step: 5400, train/epoch: 1.285102367401123
Step: 5410, train/loss: 0.4406000077724457
Step: 5410, train/grad_norm: 8.302652359008789
Step: 5410, train/learning_rate: 4.356258796178736e-05
Step: 5410, train/epoch: 1.2874821424484253
Step: 5420, train/loss: 0.517300009727478
Step: 5420, train/grad_norm: 5.0756001472473145
Step: 5420, train/learning_rate: 4.355069177108817e-05
Step: 5420, train/epoch: 1.2898619174957275
Step: 5430, train/loss: 0.5439000129699707
Step: 5430, train/grad_norm: 22.520145416259766
Step: 5430, train/learning_rate: 4.353879194241017e-05
Step: 5430, train/epoch: 1.2922418117523193
Step: 5440, train/loss: 0.4758000075817108
Step: 5440, train/grad_norm: 14.056506156921387
Step: 5440, train/learning_rate: 4.3526892113732174e-05
Step: 5440, train/epoch: 1.2946215867996216
Step: 5450, train/loss: 0.5296000242233276
Step: 5450, train/grad_norm: 8.54230785369873
Step: 5450, train/learning_rate: 4.351499228505418e-05
Step: 5450, train/epoch: 1.2970014810562134
Step: 5460, train/loss: 0.43650001287460327
Step: 5460, train/grad_norm: 9.63573932647705
Step: 5460, train/learning_rate: 4.350309245637618e-05
Step: 5460, train/epoch: 1.2993812561035156
Step: 5470, train/loss: 0.4221000075340271
Step: 5470, train/grad_norm: 25.21671485900879
Step: 5470, train/learning_rate: 4.349119626567699e-05
Step: 5470, train/epoch: 1.3017610311508179
Step: 5480, train/loss: 0.4851999878883362
Step: 5480, train/grad_norm: 14.289146423339844
Step: 5480, train/learning_rate: 4.347929643699899e-05
Step: 5480, train/epoch: 1.3041409254074097
Step: 5490, train/loss: 0.4553000032901764
Step: 5490, train/grad_norm: 33.117313385009766
Step: 5490, train/learning_rate: 4.3467396608320996e-05
Step: 5490, train/epoch: 1.306520700454712
Step: 5500, train/loss: 0.6295999884605408
Step: 5500, train/grad_norm: 19.57406234741211
Step: 5500, train/learning_rate: 4.3455496779643e-05
Step: 5500, train/epoch: 1.3089004755020142
Step: 5510, train/loss: 0.5726000070571899
Step: 5510, train/grad_norm: 5.912868022918701
Step: 5510, train/learning_rate: 4.3443596950965e-05
Step: 5510, train/epoch: 1.311280369758606
Step: 5520, train/loss: 0.517799973487854
Step: 5520, train/grad_norm: 8.761521339416504
Step: 5520, train/learning_rate: 4.343170076026581e-05
Step: 5520, train/epoch: 1.3136601448059082
Step: 5530, train/loss: 0.4092999994754791
Step: 5530, train/grad_norm: 6.968538761138916
Step: 5530, train/learning_rate: 4.3419800931587815e-05
Step: 5530, train/epoch: 1.3160400390625
Step: 5540, train/loss: 0.6908000111579895
Step: 5540, train/grad_norm: 4.506034851074219
Step: 5540, train/learning_rate: 4.340790110290982e-05
Step: 5540, train/epoch: 1.3184198141098022
Step: 5550, train/loss: 0.7721999883651733
Step: 5550, train/grad_norm: 39.930442810058594
Step: 5550, train/learning_rate: 4.339600127423182e-05
Step: 5550, train/epoch: 1.3207995891571045
Step: 5560, train/loss: 0.5453000068664551
Step: 5560, train/grad_norm: 11.91049861907959
Step: 5560, train/learning_rate: 4.3384101445553824e-05
Step: 5560, train/epoch: 1.3231794834136963
Step: 5570, train/loss: 0.5562000274658203
Step: 5570, train/grad_norm: 11.032390594482422
Step: 5570, train/learning_rate: 4.3372205254854634e-05
Step: 5570, train/epoch: 1.3255592584609985
Step: 5580, train/loss: 0.6682000160217285
Step: 5580, train/grad_norm: 13.117818832397461
Step: 5580, train/learning_rate: 4.336030542617664e-05
Step: 5580, train/epoch: 1.3279390335083008
Step: 5590, train/loss: 0.39100000262260437
Step: 5590, train/grad_norm: 8.415450096130371
Step: 5590, train/learning_rate: 4.334840559749864e-05
Step: 5590, train/epoch: 1.3303189277648926
Step: 5600, train/loss: 0.5472999811172485
Step: 5600, train/grad_norm: 5.809806823730469
Step: 5600, train/learning_rate: 4.3336505768820643e-05
Step: 5600, train/epoch: 1.3326987028121948
Step: 5610, train/loss: 0.5202999711036682
Step: 5610, train/grad_norm: 7.231844902038574
Step: 5610, train/learning_rate: 4.3324605940142646e-05
Step: 5610, train/epoch: 1.335078477859497
Step: 5620, train/loss: 0.4203999936580658
Step: 5620, train/grad_norm: 7.554439067840576
Step: 5620, train/learning_rate: 4.3312709749443457e-05
Step: 5620, train/epoch: 1.3374583721160889
Step: 5630, train/loss: 0.5551000237464905
Step: 5630, train/grad_norm: 5.349603176116943
Step: 5630, train/learning_rate: 4.330080992076546e-05
Step: 5630, train/epoch: 1.3398381471633911
Step: 5640, train/loss: 0.5866000056266785
Step: 5640, train/grad_norm: 21.093645095825195
Step: 5640, train/learning_rate: 4.328891009208746e-05
Step: 5640, train/epoch: 1.342218041419983
Step: 5650, train/loss: 0.49939998984336853
Step: 5650, train/grad_norm: 11.232854843139648
Step: 5650, train/learning_rate: 4.3277010263409466e-05
Step: 5650, train/epoch: 1.3445978164672852
Step: 5660, train/loss: 0.43290001153945923
Step: 5660, train/grad_norm: 11.241400718688965
Step: 5660, train/learning_rate: 4.326511043473147e-05
Step: 5660, train/epoch: 1.3469775915145874
Step: 5670, train/loss: 0.4401000142097473
Step: 5670, train/grad_norm: 6.857240200042725
Step: 5670, train/learning_rate: 4.325321424403228e-05
Step: 5670, train/epoch: 1.3493574857711792
Step: 5680, train/loss: 0.47040000557899475
Step: 5680, train/grad_norm: 15.760992050170898
Step: 5680, train/learning_rate: 4.324131441535428e-05
Step: 5680, train/epoch: 1.3517372608184814
Step: 5690, train/loss: 0.5238000154495239
Step: 5690, train/grad_norm: 13.606407165527344
Step: 5690, train/learning_rate: 4.3229414586676285e-05
Step: 5690, train/epoch: 1.3541170358657837
Step: 5700, train/loss: 0.5120000243186951
Step: 5700, train/grad_norm: 8.042272567749023
Step: 5700, train/learning_rate: 4.321751475799829e-05
Step: 5700, train/epoch: 1.3564969301223755
Step: 5710, train/loss: 0.48399999737739563
Step: 5710, train/grad_norm: 3.445427179336548
Step: 5710, train/learning_rate: 4.320561492932029e-05
Step: 5710, train/epoch: 1.3588767051696777
Step: 5720, train/loss: 0.428600013256073
Step: 5720, train/grad_norm: 17.548696517944336
Step: 5720, train/learning_rate: 4.31937187386211e-05
Step: 5720, train/epoch: 1.3612565994262695
Step: 5730, train/loss: 0.421099990606308
Step: 5730, train/grad_norm: 8.715154647827148
Step: 5730, train/learning_rate: 4.3181818909943104e-05
Step: 5730, train/epoch: 1.3636363744735718
Step: 5740, train/loss: 0.4629000127315521
Step: 5740, train/grad_norm: 6.807788372039795
Step: 5740, train/learning_rate: 4.316991908126511e-05
Step: 5740, train/epoch: 1.366016149520874
Step: 5750, train/loss: 0.5060999989509583
Step: 5750, train/grad_norm: 13.668508529663086
Step: 5750, train/learning_rate: 4.315801925258711e-05
Step: 5750, train/epoch: 1.3683960437774658
Step: 5760, train/loss: 0.5105000138282776
Step: 5760, train/grad_norm: 12.685853004455566
Step: 5760, train/learning_rate: 4.314611942390911e-05
Step: 5760, train/epoch: 1.370775818824768
Step: 5770, train/loss: 0.5333999991416931
Step: 5770, train/grad_norm: 17.562633514404297
Step: 5770, train/learning_rate: 4.313422323320992e-05
Step: 5770, train/epoch: 1.3731555938720703
Step: 5780, train/loss: 0.492900013923645
Step: 5780, train/grad_norm: 11.885964393615723
Step: 5780, train/learning_rate: 4.3122323404531926e-05
Step: 5780, train/epoch: 1.375535488128662
Step: 5790, train/loss: 0.6031000018119812
Step: 5790, train/grad_norm: 21.325281143188477
Step: 5790, train/learning_rate: 4.311042357585393e-05
Step: 5790, train/epoch: 1.3779152631759644
Step: 5800, train/loss: 0.6298999786376953
Step: 5800, train/grad_norm: 17.14255142211914
Step: 5800, train/learning_rate: 4.309852374717593e-05
Step: 5800, train/epoch: 1.3802950382232666
Step: 5810, train/loss: 0.5468999743461609
Step: 5810, train/grad_norm: 11.206016540527344
Step: 5810, train/learning_rate: 4.3086623918497935e-05
Step: 5810, train/epoch: 1.3826749324798584
Step: 5820, train/loss: 0.3869999945163727
Step: 5820, train/grad_norm: 8.405929565429688
Step: 5820, train/learning_rate: 4.3074727727798745e-05
Step: 5820, train/epoch: 1.3850547075271606
Step: 5830, train/loss: 0.5752000212669373
Step: 5830, train/grad_norm: 8.816596984863281
Step: 5830, train/learning_rate: 4.306282789912075e-05
Step: 5830, train/epoch: 1.3874346017837524
Step: 5840, train/loss: 0.5192999839782715
Step: 5840, train/grad_norm: 7.783285617828369
Step: 5840, train/learning_rate: 4.305092807044275e-05
Step: 5840, train/epoch: 1.3898143768310547
Step: 5850, train/loss: 0.4009000062942505
Step: 5850, train/grad_norm: 5.419125556945801
Step: 5850, train/learning_rate: 4.3039028241764754e-05
Step: 5850, train/epoch: 1.392194151878357
Step: 5860, train/loss: 0.5548999905586243
Step: 5860, train/grad_norm: 14.984972953796387
Step: 5860, train/learning_rate: 4.302712841308676e-05
Step: 5860, train/epoch: 1.3945740461349487
Step: 5870, train/loss: 0.47699999809265137
Step: 5870, train/grad_norm: 18.49392318725586
Step: 5870, train/learning_rate: 4.301523222238757e-05
Step: 5870, train/epoch: 1.396953821182251
Step: 5880, train/loss: 0.4047999978065491
Step: 5880, train/grad_norm: 28.62926483154297
Step: 5880, train/learning_rate: 4.300333239370957e-05
Step: 5880, train/epoch: 1.3993335962295532
Step: 5890, train/loss: 0.5593000054359436
Step: 5890, train/grad_norm: 4.9657063484191895
Step: 5890, train/learning_rate: 4.299143256503157e-05
Step: 5890, train/epoch: 1.401713490486145
Step: 5900, train/loss: 0.47350001335144043
Step: 5900, train/grad_norm: 15.927948951721191
Step: 5900, train/learning_rate: 4.2979532736353576e-05
Step: 5900, train/epoch: 1.4040932655334473
Step: 5910, train/loss: 0.45080000162124634
Step: 5910, train/grad_norm: 11.835566520690918
Step: 5910, train/learning_rate: 4.296763290767558e-05
Step: 5910, train/epoch: 1.406473159790039
Step: 5920, train/loss: 0.5419999957084656
Step: 5920, train/grad_norm: 14.874709129333496
Step: 5920, train/learning_rate: 4.295573671697639e-05
Step: 5920, train/epoch: 1.4088529348373413
Step: 5930, train/loss: 0.35429999232292175
Step: 5930, train/grad_norm: 5.505517959594727
Step: 5930, train/learning_rate: 4.294383688829839e-05
Step: 5930, train/epoch: 1.4112327098846436
Step: 5940, train/loss: 0.43230000138282776
Step: 5940, train/grad_norm: 10.370256423950195
Step: 5940, train/learning_rate: 4.2931937059620395e-05
Step: 5940, train/epoch: 1.4136126041412354
Step: 5950, train/loss: 0.5426999926567078
Step: 5950, train/grad_norm: 9.469439506530762
Step: 5950, train/learning_rate: 4.29200372309424e-05
Step: 5950, train/epoch: 1.4159923791885376
Step: 5960, train/loss: 0.5346999764442444
Step: 5960, train/grad_norm: 13.27415657043457
Step: 5960, train/learning_rate: 4.29081374022644e-05
Step: 5960, train/epoch: 1.4183721542358398
Step: 5970, train/loss: 0.49230000376701355
Step: 5970, train/grad_norm: 7.142212867736816
Step: 5970, train/learning_rate: 4.289624121156521e-05
Step: 5970, train/epoch: 1.4207520484924316
Step: 5980, train/loss: 0.4311999976634979
Step: 5980, train/grad_norm: 11.040824890136719
Step: 5980, train/learning_rate: 4.2884341382887214e-05
Step: 5980, train/epoch: 1.4231318235397339
Step: 5990, train/loss: 0.4708999991416931
Step: 5990, train/grad_norm: 4.835688591003418
Step: 5990, train/learning_rate: 4.287244155420922e-05
Step: 5990, train/epoch: 1.4255117177963257
Step: 6000, train/loss: 0.44020000100135803
Step: 6000, train/grad_norm: 7.247220516204834
Step: 6000, train/learning_rate: 4.286054172553122e-05
Step: 6000, train/epoch: 1.427891492843628
Step: 6010, train/loss: 0.52920001745224
Step: 6010, train/grad_norm: 4.946185111999512
Step: 6010, train/learning_rate: 4.2848641896853223e-05
Step: 6010, train/epoch: 1.4302712678909302
Step: 6020, train/loss: 0.5254999995231628
Step: 6020, train/grad_norm: 5.449464321136475
Step: 6020, train/learning_rate: 4.2836745706154034e-05
Step: 6020, train/epoch: 1.432651162147522
Step: 6030, train/loss: 0.6003999710083008
Step: 6030, train/grad_norm: 16.370515823364258
Step: 6030, train/learning_rate: 4.2824845877476037e-05
Step: 6030, train/epoch: 1.4350309371948242
Step: 6040, train/loss: 0.5166000127792358
Step: 6040, train/grad_norm: 6.076763153076172
Step: 6040, train/learning_rate: 4.281294604879804e-05
Step: 6040, train/epoch: 1.4374107122421265
Step: 6050, train/loss: 0.6025999784469604
Step: 6050, train/grad_norm: 11.877986907958984
Step: 6050, train/learning_rate: 4.280104622012004e-05
Step: 6050, train/epoch: 1.4397906064987183
Step: 6060, train/loss: 0.6486999988555908
Step: 6060, train/grad_norm: 13.322336196899414
Step: 6060, train/learning_rate: 4.2789146391442046e-05
Step: 6060, train/epoch: 1.4421703815460205
Step: 6070, train/loss: 0.5234000086784363
Step: 6070, train/grad_norm: 32.25016403198242
Step: 6070, train/learning_rate: 4.2777250200742856e-05
Step: 6070, train/epoch: 1.4445501565933228
Step: 6080, train/loss: 0.46070000529289246
Step: 6080, train/grad_norm: 5.698705196380615
Step: 6080, train/learning_rate: 4.276535037206486e-05
Step: 6080, train/epoch: 1.4469300508499146
Step: 6090, train/loss: 0.49869999289512634
Step: 6090, train/grad_norm: 5.865386962890625
Step: 6090, train/learning_rate: 4.275345054338686e-05
Step: 6090, train/epoch: 1.4493098258972168
Step: 6100, train/loss: 0.47929999232292175
Step: 6100, train/grad_norm: 5.212833404541016
Step: 6100, train/learning_rate: 4.2741550714708865e-05
Step: 6100, train/epoch: 1.4516897201538086
Step: 6110, train/loss: 0.5088000297546387
Step: 6110, train/grad_norm: 6.1890668869018555
Step: 6110, train/learning_rate: 4.272965088603087e-05
Step: 6110, train/epoch: 1.4540694952011108
Step: 6120, train/loss: 0.5180000066757202
Step: 6120, train/grad_norm: 5.83028507232666
Step: 6120, train/learning_rate: 4.271775469533168e-05
Step: 6120, train/epoch: 1.456449270248413
Step: 6130, train/loss: 0.5145000219345093
Step: 6130, train/grad_norm: 6.5634307861328125
Step: 6130, train/learning_rate: 4.270585486665368e-05
Step: 6130, train/epoch: 1.4588291645050049
Step: 6140, train/loss: 0.47699999809265137
Step: 6140, train/grad_norm: 7.518431186676025
Step: 6140, train/learning_rate: 4.2693955037975684e-05
Step: 6140, train/epoch: 1.4612089395523071
Step: 6150, train/loss: 0.6538000106811523
Step: 6150, train/grad_norm: 18.03234100341797
Step: 6150, train/learning_rate: 4.268205520929769e-05
Step: 6150, train/epoch: 1.4635887145996094
Step: 6160, train/loss: 0.5740000009536743
Step: 6160, train/grad_norm: 5.825285911560059
Step: 6160, train/learning_rate: 4.267015538061969e-05
Step: 6160, train/epoch: 1.4659686088562012
Step: 6170, train/loss: 0.5311999917030334
Step: 6170, train/grad_norm: 14.117940902709961
Step: 6170, train/learning_rate: 4.26582591899205e-05
Step: 6170, train/epoch: 1.4683483839035034
Step: 6180, train/loss: 0.5067999958992004
Step: 6180, train/grad_norm: 11.28633975982666
Step: 6180, train/learning_rate: 4.26463593612425e-05
Step: 6180, train/epoch: 1.4707282781600952
Step: 6190, train/loss: 0.3968999981880188
Step: 6190, train/grad_norm: 4.771166801452637
Step: 6190, train/learning_rate: 4.2634459532564506e-05
Step: 6190, train/epoch: 1.4731080532073975
Step: 6200, train/loss: 0.4952000081539154
Step: 6200, train/grad_norm: 15.950432777404785
Step: 6200, train/learning_rate: 4.262255970388651e-05
Step: 6200, train/epoch: 1.4754878282546997
Step: 6210, train/loss: 0.5723000168800354
Step: 6210, train/grad_norm: 5.501928806304932
Step: 6210, train/learning_rate: 4.261065987520851e-05
Step: 6210, train/epoch: 1.4778677225112915
Step: 6220, train/loss: 0.37299999594688416
Step: 6220, train/grad_norm: 5.47977352142334
Step: 6220, train/learning_rate: 4.259876368450932e-05
Step: 6220, train/epoch: 1.4802474975585938
Step: 6230, train/loss: 0.6169999837875366
Step: 6230, train/grad_norm: 5.497557640075684
Step: 6230, train/learning_rate: 4.2586863855831325e-05
Step: 6230, train/epoch: 1.482627272605896
Step: 6240, train/loss: 0.5472999811172485
Step: 6240, train/grad_norm: 5.415633678436279
Step: 6240, train/learning_rate: 4.257496402715333e-05
Step: 6240, train/epoch: 1.4850071668624878
Step: 6250, train/loss: 0.5217000246047974
Step: 6250, train/grad_norm: 16.102272033691406
Step: 6250, train/learning_rate: 4.256306419847533e-05
Step: 6250, train/epoch: 1.48738694190979
Step: 6260, train/loss: 0.5160999894142151
Step: 6260, train/grad_norm: 9.715635299682617
Step: 6260, train/learning_rate: 4.2551164369797334e-05
Step: 6260, train/epoch: 1.4897668361663818
Step: 6270, train/loss: 0.5034000277519226
Step: 6270, train/grad_norm: 8.581480026245117
Step: 6270, train/learning_rate: 4.2539268179098144e-05
Step: 6270, train/epoch: 1.492146611213684
Step: 6280, train/loss: 0.4731999933719635
Step: 6280, train/grad_norm: 4.741957187652588
Step: 6280, train/learning_rate: 4.252736835042015e-05
Step: 6280, train/epoch: 1.4945263862609863
Step: 6290, train/loss: 0.4203999936580658
Step: 6290, train/grad_norm: 8.95834732055664
Step: 6290, train/learning_rate: 4.251546852174215e-05
Step: 6290, train/epoch: 1.4969062805175781
Step: 6300, train/loss: 0.5001999735832214
Step: 6300, train/grad_norm: 13.884801864624023
Step: 6300, train/learning_rate: 4.250356869306415e-05
Step: 6300, train/epoch: 1.4992860555648804
Step: 6310, train/loss: 0.45840001106262207
Step: 6310, train/grad_norm: 9.961722373962402
Step: 6310, train/learning_rate: 4.2491668864386156e-05
Step: 6310, train/epoch: 1.5016658306121826
Step: 6320, train/loss: 0.5231000185012817
Step: 6320, train/grad_norm: 7.796911716461182
Step: 6320, train/learning_rate: 4.2479772673686966e-05
Step: 6320, train/epoch: 1.5040457248687744
Step: 6330, train/loss: 0.40689998865127563
Step: 6330, train/grad_norm: 12.500922203063965
Step: 6330, train/learning_rate: 4.246787284500897e-05
Step: 6330, train/epoch: 1.5064254999160767
Step: 6340, train/loss: 0.4560000002384186
Step: 6340, train/grad_norm: 9.139902114868164
Step: 6340, train/learning_rate: 4.245597301633097e-05
Step: 6340, train/epoch: 1.508805274963379
Step: 6350, train/loss: 0.546500027179718
Step: 6350, train/grad_norm: 11.932430267333984
Step: 6350, train/learning_rate: 4.2444073187652975e-05
Step: 6350, train/epoch: 1.5111851692199707
Step: 6360, train/loss: 0.5864999890327454
Step: 6360, train/grad_norm: 18.49344253540039
Step: 6360, train/learning_rate: 4.243217335897498e-05
Step: 6360, train/epoch: 1.513564944267273
Step: 6370, train/loss: 0.482699990272522
Step: 6370, train/grad_norm: 8.636636734008789
Step: 6370, train/learning_rate: 4.242027716827579e-05
Step: 6370, train/epoch: 1.5159448385238647
Step: 6380, train/loss: 0.4715000092983246
Step: 6380, train/grad_norm: 32.70145034790039
Step: 6380, train/learning_rate: 4.240837733959779e-05
Step: 6380, train/epoch: 1.518324613571167
Step: 6390, train/loss: 0.631600022315979
Step: 6390, train/grad_norm: 11.576889038085938
Step: 6390, train/learning_rate: 4.2396477510919794e-05
Step: 6390, train/epoch: 1.5207043886184692
Step: 6400, train/loss: 0.5666000247001648
Step: 6400, train/grad_norm: 8.188350677490234
Step: 6400, train/learning_rate: 4.23845776822418e-05
Step: 6400, train/epoch: 1.523084282875061
Step: 6410, train/loss: 0.4869999885559082
Step: 6410, train/grad_norm: 11.120279312133789
Step: 6410, train/learning_rate: 4.237268149154261e-05
Step: 6410, train/epoch: 1.5254640579223633
Step: 6420, train/loss: 0.5339999794960022
Step: 6420, train/grad_norm: 4.791274547576904
Step: 6420, train/learning_rate: 4.236078166286461e-05
Step: 6420, train/epoch: 1.5278438329696655
Step: 6430, train/loss: 0.5163000226020813
Step: 6430, train/grad_norm: 4.851163864135742
Step: 6430, train/learning_rate: 4.2348881834186614e-05
Step: 6430, train/epoch: 1.5302237272262573
Step: 6440, train/loss: 0.5288000106811523
Step: 6440, train/grad_norm: 10.43052864074707
Step: 6440, train/learning_rate: 4.2336982005508617e-05
Step: 6440, train/epoch: 1.5326035022735596
Step: 6450, train/loss: 0.48910000920295715
Step: 6450, train/grad_norm: 6.837069511413574
Step: 6450, train/learning_rate: 4.232508217683062e-05
Step: 6450, train/epoch: 1.5349833965301514
Step: 6460, train/loss: 0.41190001368522644
Step: 6460, train/grad_norm: 7.773378372192383
Step: 6460, train/learning_rate: 4.231318598613143e-05
Step: 6460, train/epoch: 1.5373631715774536
Step: 6470, train/loss: 0.5546000003814697
Step: 6470, train/grad_norm: 20.375024795532227
Step: 6470, train/learning_rate: 4.230128615745343e-05
Step: 6470, train/epoch: 1.5397429466247559
Step: 6480, train/loss: 0.3937999904155731
Step: 6480, train/grad_norm: 17.19491195678711
Step: 6480, train/learning_rate: 4.2289386328775436e-05
Step: 6480, train/epoch: 1.5421228408813477
Step: 6490, train/loss: 0.5954999923706055
Step: 6490, train/grad_norm: 10.457630157470703
Step: 6490, train/learning_rate: 4.227748650009744e-05
Step: 6490, train/epoch: 1.54450261592865
Step: 6500, train/loss: 0.4860000014305115
Step: 6500, train/grad_norm: 11.871182441711426
Step: 6500, train/learning_rate: 4.226558667141944e-05
Step: 6500, train/epoch: 1.5468823909759521
Step: 6510, train/loss: 0.44190001487731934
Step: 6510, train/grad_norm: 5.0096282958984375
Step: 6510, train/learning_rate: 4.225369048072025e-05
Step: 6510, train/epoch: 1.549262285232544
Step: 6520, train/loss: 0.3603000044822693
Step: 6520, train/grad_norm: 3.106739044189453
Step: 6520, train/learning_rate: 4.2241790652042255e-05
Step: 6520, train/epoch: 1.5516420602798462
Step: 6530, train/loss: 0.5235000252723694
Step: 6530, train/grad_norm: 5.6106719970703125
Step: 6530, train/learning_rate: 4.222989082336426e-05
Step: 6530, train/epoch: 1.5540218353271484
Step: 6540, train/loss: 0.5038999915122986
Step: 6540, train/grad_norm: 14.424704551696777
Step: 6540, train/learning_rate: 4.221799099468626e-05
Step: 6540, train/epoch: 1.5564017295837402
Step: 6550, train/loss: 0.4431999921798706
Step: 6550, train/grad_norm: 2.934316873550415
Step: 6550, train/learning_rate: 4.2206091166008264e-05
Step: 6550, train/epoch: 1.5587815046310425
Step: 6560, train/loss: 0.6032999753952026
Step: 6560, train/grad_norm: 21.01546287536621
Step: 6560, train/learning_rate: 4.2194194975309074e-05
Step: 6560, train/epoch: 1.5611613988876343
Step: 6570, train/loss: 0.42800000309944153
Step: 6570, train/grad_norm: 5.453845977783203
Step: 6570, train/learning_rate: 4.218229514663108e-05
Step: 6570, train/epoch: 1.5635411739349365
Step: 6580, train/loss: 0.521399974822998
Step: 6580, train/grad_norm: 19.775157928466797
Step: 6580, train/learning_rate: 4.217039531795308e-05
Step: 6580, train/epoch: 1.5659209489822388
Step: 6590, train/loss: 0.47609999775886536
Step: 6590, train/grad_norm: 12.536258697509766
Step: 6590, train/learning_rate: 4.215849548927508e-05
Step: 6590, train/epoch: 1.5683008432388306
Step: 6600, train/loss: 0.521399974822998
Step: 6600, train/grad_norm: 10.220430374145508
Step: 6600, train/learning_rate: 4.2146595660597086e-05
Step: 6600, train/epoch: 1.5706806182861328
Step: 6610, train/loss: 0.5863000154495239
Step: 6610, train/grad_norm: 9.033316612243652
Step: 6610, train/learning_rate: 4.2134699469897896e-05
Step: 6610, train/epoch: 1.573060393333435
Step: 6620, train/loss: 0.5512999892234802
Step: 6620, train/grad_norm: 28.98450469970703
Step: 6620, train/learning_rate: 4.21227996412199e-05
Step: 6620, train/epoch: 1.5754402875900269
Step: 6630, train/loss: 0.4447999894618988
Step: 6630, train/grad_norm: 9.234040260314941
Step: 6630, train/learning_rate: 4.21108998125419e-05
Step: 6630, train/epoch: 1.577820062637329
Step: 6640, train/loss: 0.5622000098228455
Step: 6640, train/grad_norm: 7.548791885375977
Step: 6640, train/learning_rate: 4.2098999983863905e-05
Step: 6640, train/epoch: 1.580199956893921
Step: 6650, train/loss: 0.5162000060081482
Step: 6650, train/grad_norm: 6.692270755767822
Step: 6650, train/learning_rate: 4.208710015518591e-05
Step: 6650, train/epoch: 1.5825797319412231
Step: 6660, train/loss: 0.4821000099182129
Step: 6660, train/grad_norm: 8.649584770202637
Step: 6660, train/learning_rate: 4.207520396448672e-05
Step: 6660, train/epoch: 1.5849595069885254
Step: 6670, train/loss: 0.46810001134872437
Step: 6670, train/grad_norm: 15.217891693115234
Step: 6670, train/learning_rate: 4.206330413580872e-05
Step: 6670, train/epoch: 1.5873394012451172
Step: 6680, train/loss: 0.49869999289512634
Step: 6680, train/grad_norm: 4.885136127471924
Step: 6680, train/learning_rate: 4.2051404307130724e-05
Step: 6680, train/epoch: 1.5897191762924194
Step: 6690, train/loss: 0.46309998631477356
Step: 6690, train/grad_norm: 4.117282390594482
Step: 6690, train/learning_rate: 4.203950447845273e-05
Step: 6690, train/epoch: 1.5920989513397217
Step: 6700, train/loss: 0.6636999845504761
Step: 6700, train/grad_norm: 15.066947937011719
Step: 6700, train/learning_rate: 4.202760464977473e-05
Step: 6700, train/epoch: 1.5944788455963135
Step: 6710, train/loss: 0.44369998574256897
Step: 6710, train/grad_norm: 5.125385284423828
Step: 6710, train/learning_rate: 4.201570845907554e-05
Step: 6710, train/epoch: 1.5968586206436157
Step: 6720, train/loss: 0.4675000011920929
Step: 6720, train/grad_norm: 4.045605182647705
Step: 6720, train/learning_rate: 4.200380863039754e-05
Step: 6720, train/epoch: 1.5992385149002075
Step: 6730, train/loss: 0.4643999934196472
Step: 6730, train/grad_norm: 7.741118431091309
Step: 6730, train/learning_rate: 4.1991908801719546e-05
Step: 6730, train/epoch: 1.6016182899475098
Step: 6740, train/loss: 0.5077999830245972
Step: 6740, train/grad_norm: 12.314655303955078
Step: 6740, train/learning_rate: 4.198000897304155e-05
Step: 6740, train/epoch: 1.603998064994812
Step: 6750, train/loss: 0.5458999872207642
Step: 6750, train/grad_norm: 11.87824535369873
Step: 6750, train/learning_rate: 4.196810914436355e-05
Step: 6750, train/epoch: 1.6063779592514038
Step: 6760, train/loss: 0.47600001096725464
Step: 6760, train/grad_norm: 25.963096618652344
Step: 6760, train/learning_rate: 4.195621295366436e-05
Step: 6760, train/epoch: 1.608757734298706
Step: 6770, train/loss: 0.477400004863739
Step: 6770, train/grad_norm: 20.064077377319336
Step: 6770, train/learning_rate: 4.1944313124986365e-05
Step: 6770, train/epoch: 1.6111375093460083
Step: 6780, train/loss: 0.5437999963760376
Step: 6780, train/grad_norm: 5.4753217697143555
Step: 6780, train/learning_rate: 4.193241329630837e-05
Step: 6780, train/epoch: 1.6135174036026
Step: 6790, train/loss: 0.5436000227928162
Step: 6790, train/grad_norm: 16.555889129638672
Step: 6790, train/learning_rate: 4.192051346763037e-05
Step: 6790, train/epoch: 1.6158971786499023
Step: 6800, train/loss: 0.4830000102519989
Step: 6800, train/grad_norm: 7.623557090759277
Step: 6800, train/learning_rate: 4.1908613638952374e-05
Step: 6800, train/epoch: 1.6182769536972046
Step: 6810, train/loss: 0.5666999816894531
Step: 6810, train/grad_norm: 21.507783889770508
Step: 6810, train/learning_rate: 4.1896717448253185e-05
Step: 6810, train/epoch: 1.6206568479537964
Step: 6820, train/loss: 0.567300021648407
Step: 6820, train/grad_norm: 10.14877700805664
Step: 6820, train/learning_rate: 4.188481761957519e-05
Step: 6820, train/epoch: 1.6230366230010986
Step: 6830, train/loss: 0.43560001254081726
Step: 6830, train/grad_norm: 9.57592487335205
Step: 6830, train/learning_rate: 4.187291779089719e-05
Step: 6830, train/epoch: 1.6254165172576904
Step: 6840, train/loss: 0.47780001163482666
Step: 6840, train/grad_norm: 11.141347885131836
Step: 6840, train/learning_rate: 4.1861017962219194e-05
Step: 6840, train/epoch: 1.6277962923049927
Step: 6850, train/loss: 0.5723000168800354
Step: 6850, train/grad_norm: 7.254897117614746
Step: 6850, train/learning_rate: 4.1849118133541197e-05
Step: 6850, train/epoch: 1.630176067352295
Step: 6860, train/loss: 0.43619999289512634
Step: 6860, train/grad_norm: 18.657054901123047
Step: 6860, train/learning_rate: 4.183722194284201e-05
Step: 6860, train/epoch: 1.6325559616088867
Step: 6870, train/loss: 0.46070000529289246
Step: 6870, train/grad_norm: 6.262752056121826
Step: 6870, train/learning_rate: 4.182532211416401e-05
Step: 6870, train/epoch: 1.634935736656189
Step: 6880, train/loss: 0.5636000037193298
Step: 6880, train/grad_norm: 13.831039428710938
Step: 6880, train/learning_rate: 4.181342228548601e-05
Step: 6880, train/epoch: 1.6373155117034912
Step: 6890, train/loss: 0.5037999749183655
Step: 6890, train/grad_norm: 6.140892505645752
Step: 6890, train/learning_rate: 4.1801522456808016e-05
Step: 6890, train/epoch: 1.639695405960083
Step: 6900, train/loss: 0.47049999237060547
Step: 6900, train/grad_norm: 22.090444564819336
Step: 6900, train/learning_rate: 4.178962262813002e-05
Step: 6900, train/epoch: 1.6420751810073853
Step: 6910, train/loss: 0.35199999809265137
Step: 6910, train/grad_norm: 14.545428276062012
Step: 6910, train/learning_rate: 4.177772643743083e-05
Step: 6910, train/epoch: 1.644455075263977
Step: 6920, train/loss: 0.4578000009059906
Step: 6920, train/grad_norm: 2.8121635913848877
Step: 6920, train/learning_rate: 4.176582660875283e-05
Step: 6920, train/epoch: 1.6468348503112793
Step: 6930, train/loss: 0.4846999943256378
Step: 6930, train/grad_norm: 14.561344146728516
Step: 6930, train/learning_rate: 4.1753926780074835e-05
Step: 6930, train/epoch: 1.6492146253585815
Step: 6940, train/loss: 0.39579999446868896
Step: 6940, train/grad_norm: 8.625487327575684
Step: 6940, train/learning_rate: 4.174202695139684e-05
Step: 6940, train/epoch: 1.6515945196151733
Step: 6950, train/loss: 0.5825999975204468
Step: 6950, train/grad_norm: 16.62216567993164
Step: 6950, train/learning_rate: 4.173012712271884e-05
Step: 6950, train/epoch: 1.6539742946624756
Step: 6960, train/loss: 0.5371999740600586
Step: 6960, train/grad_norm: 13.303703308105469
Step: 6960, train/learning_rate: 4.171823093201965e-05
Step: 6960, train/epoch: 1.6563540697097778
Step: 6970, train/loss: 0.6092000007629395
Step: 6970, train/grad_norm: 9.339698791503906
Step: 6970, train/learning_rate: 4.1706331103341654e-05
Step: 6970, train/epoch: 1.6587339639663696
Step: 6980, train/loss: 0.5083000063896179
Step: 6980, train/grad_norm: 16.130117416381836
Step: 6980, train/learning_rate: 4.169443127466366e-05
Step: 6980, train/epoch: 1.6611137390136719
Step: 6990, train/loss: 0.5303000211715698
Step: 6990, train/grad_norm: 7.787676811218262
Step: 6990, train/learning_rate: 4.168253144598566e-05
Step: 6990, train/epoch: 1.6634936332702637
Step: 7000, train/loss: 0.5230000019073486
Step: 7000, train/grad_norm: 9.376378059387207
Step: 7000, train/learning_rate: 4.167063161730766e-05
Step: 7000, train/epoch: 1.665873408317566
Step: 7010, train/loss: 0.44209998846054077
Step: 7010, train/grad_norm: 6.477059364318848
Step: 7010, train/learning_rate: 4.165873542660847e-05
Step: 7010, train/epoch: 1.6682531833648682
Step: 7020, train/loss: 0.48019999265670776
Step: 7020, train/grad_norm: 20.550168991088867
Step: 7020, train/learning_rate: 4.1646835597930476e-05
Step: 7020, train/epoch: 1.67063307762146
Step: 7030, train/loss: 0.4537999927997589
Step: 7030, train/grad_norm: 6.452119827270508
Step: 7030, train/learning_rate: 4.163493576925248e-05
Step: 7030, train/epoch: 1.6730128526687622
Step: 7040, train/loss: 0.5263000130653381
Step: 7040, train/grad_norm: 14.116006851196289
Step: 7040, train/learning_rate: 4.162303594057448e-05
Step: 7040, train/epoch: 1.6753926277160645
Step: 7050, train/loss: 0.4560000002384186
Step: 7050, train/grad_norm: 18.91937255859375
Step: 7050, train/learning_rate: 4.1611136111896485e-05
Step: 7050, train/epoch: 1.6777725219726562
Step: 7060, train/loss: 0.46399998664855957
Step: 7060, train/grad_norm: 12.590103149414062
Step: 7060, train/learning_rate: 4.1599239921197295e-05
Step: 7060, train/epoch: 1.6801522970199585
Step: 7070, train/loss: 0.5209000110626221
Step: 7070, train/grad_norm: 22.481464385986328
Step: 7070, train/learning_rate: 4.15873400925193e-05
Step: 7070, train/epoch: 1.6825320720672607
Step: 7080, train/loss: 0.36419999599456787
Step: 7080, train/grad_norm: 6.373562335968018
Step: 7080, train/learning_rate: 4.15754402638413e-05
Step: 7080, train/epoch: 1.6849119663238525
Step: 7090, train/loss: 0.6050000190734863
Step: 7090, train/grad_norm: 19.16358184814453
Step: 7090, train/learning_rate: 4.1563540435163304e-05
Step: 7090, train/epoch: 1.6872917413711548
Step: 7100, train/loss: 0.4982999861240387
Step: 7100, train/grad_norm: 5.172571182250977
Step: 7100, train/learning_rate: 4.155164060648531e-05
Step: 7100, train/epoch: 1.6896716356277466
Step: 7110, train/loss: 0.47760000824928284
Step: 7110, train/grad_norm: 19.861919403076172
Step: 7110, train/learning_rate: 4.153974441578612e-05
Step: 7110, train/epoch: 1.6920514106750488
Step: 7120, train/loss: 0.5540000200271606
Step: 7120, train/grad_norm: 17.36885643005371
Step: 7120, train/learning_rate: 4.152784458710812e-05
Step: 7120, train/epoch: 1.694431185722351
Step: 7130, train/loss: 0.5091000199317932
Step: 7130, train/grad_norm: 3.989544153213501
Step: 7130, train/learning_rate: 4.151594475843012e-05
Step: 7130, train/epoch: 1.6968110799789429
Step: 7140, train/loss: 0.48890000581741333
Step: 7140, train/grad_norm: 14.584100723266602
Step: 7140, train/learning_rate: 4.1504044929752126e-05
Step: 7140, train/epoch: 1.6991908550262451
Step: 7150, train/loss: 0.391400009393692
Step: 7150, train/grad_norm: 2.71120023727417
Step: 7150, train/learning_rate: 4.149214510107413e-05
Step: 7150, train/epoch: 1.7015706300735474
Step: 7160, train/loss: 0.4650000035762787
Step: 7160, train/grad_norm: 4.8144145011901855
Step: 7160, train/learning_rate: 4.148024891037494e-05
Step: 7160, train/epoch: 1.7039505243301392
Step: 7170, train/loss: 0.5724999904632568
Step: 7170, train/grad_norm: 7.624841690063477
Step: 7170, train/learning_rate: 4.146834908169694e-05
Step: 7170, train/epoch: 1.7063302993774414
Step: 7180, train/loss: 0.4325999915599823
Step: 7180, train/grad_norm: 9.980666160583496
Step: 7180, train/learning_rate: 4.1456449253018945e-05
Step: 7180, train/epoch: 1.7087101936340332
Step: 7190, train/loss: 0.46140000224113464
Step: 7190, train/grad_norm: 8.293818473815918
Step: 7190, train/learning_rate: 4.144454942434095e-05
Step: 7190, train/epoch: 1.7110899686813354
Step: 7200, train/loss: 0.383899986743927
Step: 7200, train/grad_norm: 6.762136936187744
Step: 7200, train/learning_rate: 4.143264959566295e-05
Step: 7200, train/epoch: 1.7134697437286377
Step: 7210, train/loss: 0.5343999862670898
Step: 7210, train/grad_norm: 25.507434844970703
Step: 7210, train/learning_rate: 4.142075340496376e-05
Step: 7210, train/epoch: 1.7158496379852295
Step: 7220, train/loss: 0.6535000205039978
Step: 7220, train/grad_norm: 21.923398971557617
Step: 7220, train/learning_rate: 4.1408853576285765e-05
Step: 7220, train/epoch: 1.7182294130325317
Step: 7230, train/loss: 0.6262999773025513
Step: 7230, train/grad_norm: 8.73950481414795
Step: 7230, train/learning_rate: 4.139695374760777e-05
Step: 7230, train/epoch: 1.720609188079834
Step: 7240, train/loss: 0.49140000343322754
Step: 7240, train/grad_norm: 6.837615013122559
Step: 7240, train/learning_rate: 4.138505391892977e-05
Step: 7240, train/epoch: 1.7229890823364258
Step: 7250, train/loss: 0.35040000081062317
Step: 7250, train/grad_norm: 5.74575662612915
Step: 7250, train/learning_rate: 4.1373154090251774e-05
Step: 7250, train/epoch: 1.725368857383728
Step: 7260, train/loss: 0.3970000147819519
Step: 7260, train/grad_norm: 8.717089653015137
Step: 7260, train/learning_rate: 4.1361257899552584e-05
Step: 7260, train/epoch: 1.7277486324310303
Step: 7270, train/loss: 0.621999979019165
Step: 7270, train/grad_norm: 8.850602149963379
Step: 7270, train/learning_rate: 4.134935807087459e-05
Step: 7270, train/epoch: 1.730128526687622
Step: 7280, train/loss: 0.652999997138977
Step: 7280, train/grad_norm: 10.552488327026367
Step: 7280, train/learning_rate: 4.133745824219659e-05
Step: 7280, train/epoch: 1.7325083017349243
Step: 7290, train/loss: 0.6273999810218811
Step: 7290, train/grad_norm: 7.016906261444092
Step: 7290, train/learning_rate: 4.132555841351859e-05
Step: 7290, train/epoch: 1.7348881959915161
Step: 7300, train/loss: 0.454800009727478
Step: 7300, train/grad_norm: 2.9642608165740967
Step: 7300, train/learning_rate: 4.1313658584840596e-05
Step: 7300, train/epoch: 1.7372679710388184
Step: 7310, train/loss: 0.44699999690055847
Step: 7310, train/grad_norm: 4.290500640869141
Step: 7310, train/learning_rate: 4.1301762394141406e-05
Step: 7310, train/epoch: 1.7396477460861206
Step: 7320, train/loss: 0.4941999912261963
Step: 7320, train/grad_norm: 8.228334426879883
Step: 7320, train/learning_rate: 4.128986256546341e-05
Step: 7320, train/epoch: 1.7420276403427124
Step: 7330, train/loss: 0.5547000169754028
Step: 7330, train/grad_norm: 16.501523971557617
Step: 7330, train/learning_rate: 4.127796273678541e-05
Step: 7330, train/epoch: 1.7444074153900146
Step: 7340, train/loss: 0.4975000023841858
Step: 7340, train/grad_norm: 12.8383150100708
Step: 7340, train/learning_rate: 4.1266062908107415e-05
Step: 7340, train/epoch: 1.746787190437317
Step: 7350, train/loss: 0.48840001225471497
Step: 7350, train/grad_norm: 4.821094512939453
Step: 7350, train/learning_rate: 4.125416307942942e-05
Step: 7350, train/epoch: 1.7491670846939087
Step: 7360, train/loss: 0.593500018119812
Step: 7360, train/grad_norm: 10.05728530883789
Step: 7360, train/learning_rate: 4.124226688873023e-05
Step: 7360, train/epoch: 1.751546859741211
Step: 7370, train/loss: 0.421999990940094
Step: 7370, train/grad_norm: 5.793964385986328
Step: 7370, train/learning_rate: 4.123036706005223e-05
Step: 7370, train/epoch: 1.7539267539978027
Step: 7380, train/loss: 0.5666000247001648
Step: 7380, train/grad_norm: 18.960613250732422
Step: 7380, train/learning_rate: 4.1218467231374234e-05
Step: 7380, train/epoch: 1.756306529045105
Step: 7390, train/loss: 0.5250999927520752
Step: 7390, train/grad_norm: 5.074316024780273
Step: 7390, train/learning_rate: 4.120656740269624e-05
Step: 7390, train/epoch: 1.7586863040924072
Step: 7400, train/loss: 0.4341999888420105
Step: 7400, train/grad_norm: 15.254727363586426
Step: 7400, train/learning_rate: 4.119466757401824e-05
Step: 7400, train/epoch: 1.761066198348999
Step: 7410, train/loss: 0.47859999537467957
Step: 7410, train/grad_norm: 6.129082679748535
Step: 7410, train/learning_rate: 4.118277138331905e-05
Step: 7410, train/epoch: 1.7634459733963013
Step: 7420, train/loss: 0.6162999868392944
Step: 7420, train/grad_norm: 12.800924301147461
Step: 7420, train/learning_rate: 4.117087155464105e-05
Step: 7420, train/epoch: 1.7658257484436035
Step: 7430, train/loss: 0.5879999995231628
Step: 7430, train/grad_norm: 13.575071334838867
Step: 7430, train/learning_rate: 4.1158971725963056e-05
Step: 7430, train/epoch: 1.7682056427001953
Step: 7440, train/loss: 0.3903000056743622
Step: 7440, train/grad_norm: 9.41386890411377
Step: 7440, train/learning_rate: 4.114707189728506e-05
Step: 7440, train/epoch: 1.7705854177474976
Step: 7450, train/loss: 0.46059998869895935
Step: 7450, train/grad_norm: 21.27474021911621
Step: 7450, train/learning_rate: 4.113517206860706e-05
Step: 7450, train/epoch: 1.7729653120040894
Step: 7460, train/loss: 0.5224000215530396
Step: 7460, train/grad_norm: 7.510146141052246
Step: 7460, train/learning_rate: 4.112327587790787e-05
Step: 7460, train/epoch: 1.7753450870513916
Step: 7470, train/loss: 0.4975000023841858
Step: 7470, train/grad_norm: 15.75515365600586
Step: 7470, train/learning_rate: 4.1111376049229875e-05
Step: 7470, train/epoch: 1.7777248620986938
Step: 7480, train/loss: 0.44130000472068787
Step: 7480, train/grad_norm: 10.172505378723145
Step: 7480, train/learning_rate: 4.109947622055188e-05
Step: 7480, train/epoch: 1.7801047563552856
Step: 7490, train/loss: 0.46540001034736633
Step: 7490, train/grad_norm: 8.40979290008545
Step: 7490, train/learning_rate: 4.108757639187388e-05
Step: 7490, train/epoch: 1.782484531402588
Step: 7500, train/loss: 0.5282999873161316
Step: 7500, train/grad_norm: 11.027756690979004
Step: 7500, train/learning_rate: 4.1075676563195884e-05
Step: 7500, train/epoch: 1.7848643064498901
Step: 7510, train/loss: 0.5361999869346619
Step: 7510, train/grad_norm: 8.06960391998291
Step: 7510, train/learning_rate: 4.1063780372496694e-05
Step: 7510, train/epoch: 1.787244200706482
Step: 7520, train/loss: 0.5160999894142151
Step: 7520, train/grad_norm: 12.031879425048828
Step: 7520, train/learning_rate: 4.10518805438187e-05
Step: 7520, train/epoch: 1.7896239757537842
Step: 7530, train/loss: 0.4934000074863434
Step: 7530, train/grad_norm: 10.741403579711914
Step: 7530, train/learning_rate: 4.10399807151407e-05
Step: 7530, train/epoch: 1.7920037508010864
Step: 7540, train/loss: 0.5324000120162964
Step: 7540, train/grad_norm: 37.92715835571289
Step: 7540, train/learning_rate: 4.10280808864627e-05
Step: 7540, train/epoch: 1.7943836450576782
Step: 7550, train/loss: 0.38839998841285706
Step: 7550, train/grad_norm: 10.843130111694336
Step: 7550, train/learning_rate: 4.1016181057784706e-05
Step: 7550, train/epoch: 1.7967634201049805
Step: 7560, train/loss: 0.4090000092983246
Step: 7560, train/grad_norm: 4.092146873474121
Step: 7560, train/learning_rate: 4.1004284867085516e-05
Step: 7560, train/epoch: 1.7991433143615723
Step: 7570, train/loss: 0.5361999869346619
Step: 7570, train/grad_norm: 8.504585266113281
Step: 7570, train/learning_rate: 4.099238503840752e-05
Step: 7570, train/epoch: 1.8015230894088745
Step: 7580, train/loss: 0.5205000042915344
Step: 7580, train/grad_norm: 14.387639999389648
Step: 7580, train/learning_rate: 4.098048520972952e-05
Step: 7580, train/epoch: 1.8039028644561768
Step: 7590, train/loss: 0.5126000046730042
Step: 7590, train/grad_norm: 6.839485168457031
Step: 7590, train/learning_rate: 4.0968585381051525e-05
Step: 7590, train/epoch: 1.8062827587127686
Step: 7600, train/loss: 0.5027999877929688
Step: 7600, train/grad_norm: 3.5724194049835205
Step: 7600, train/learning_rate: 4.095668555237353e-05
Step: 7600, train/epoch: 1.8086625337600708
Step: 7610, train/loss: 0.5339000225067139
Step: 7610, train/grad_norm: 10.580342292785645
Step: 7610, train/learning_rate: 4.094478936167434e-05
Step: 7610, train/epoch: 1.811042308807373
Step: 7620, train/loss: 0.554099977016449
Step: 7620, train/grad_norm: 11.779121398925781
Step: 7620, train/learning_rate: 4.093288953299634e-05
Step: 7620, train/epoch: 1.8134222030639648
Step: 7630, train/loss: 0.5246999859809875
Step: 7630, train/grad_norm: 6.587399959564209
Step: 7630, train/learning_rate: 4.0920989704318345e-05
Step: 7630, train/epoch: 1.815801978111267
Step: 7640, train/loss: 0.4440000057220459
Step: 7640, train/grad_norm: 5.1762309074401855
Step: 7640, train/learning_rate: 4.090908987564035e-05
Step: 7640, train/epoch: 1.8181818723678589
Step: 7650, train/loss: 0.4309999942779541
Step: 7650, train/grad_norm: 17.2299747467041
Step: 7650, train/learning_rate: 4.089719004696235e-05
Step: 7650, train/epoch: 1.8205616474151611
Step: 7660, train/loss: 0.4449999928474426
Step: 7660, train/grad_norm: 5.3456525802612305
Step: 7660, train/learning_rate: 4.088529385626316e-05
Step: 7660, train/epoch: 1.8229414224624634
Step: 7670, train/loss: 0.6258999705314636
Step: 7670, train/grad_norm: 32.91401672363281
Step: 7670, train/learning_rate: 4.0873394027585164e-05
Step: 7670, train/epoch: 1.8253213167190552
Step: 7680, train/loss: 0.4887999892234802
Step: 7680, train/grad_norm: 11.901104927062988
Step: 7680, train/learning_rate: 4.086149419890717e-05
Step: 7680, train/epoch: 1.8277010917663574
Step: 7690, train/loss: 0.5206999778747559
Step: 7690, train/grad_norm: 4.045323371887207
Step: 7690, train/learning_rate: 4.084959437022917e-05
Step: 7690, train/epoch: 1.8300808668136597
Step: 7700, train/loss: 0.375900000333786
Step: 7700, train/grad_norm: 13.772132873535156
Step: 7700, train/learning_rate: 4.083769454155117e-05
Step: 7700, train/epoch: 1.8324607610702515
Step: 7710, train/loss: 0.41830000281333923
Step: 7710, train/grad_norm: 6.361138820648193
Step: 7710, train/learning_rate: 4.082579835085198e-05
Step: 7710, train/epoch: 1.8348405361175537
Step: 7720, train/loss: 0.5665000081062317
Step: 7720, train/grad_norm: 16.382408142089844
Step: 7720, train/learning_rate: 4.0813898522173986e-05
Step: 7720, train/epoch: 1.8372204303741455
Step: 7730, train/loss: 0.38019999861717224
Step: 7730, train/grad_norm: 12.385780334472656
Step: 7730, train/learning_rate: 4.080199869349599e-05
Step: 7730, train/epoch: 1.8396002054214478
Step: 7740, train/loss: 0.4537000060081482
Step: 7740, train/grad_norm: 11.726688385009766
Step: 7740, train/learning_rate: 4.079009886481799e-05
Step: 7740, train/epoch: 1.84197998046875
Step: 7750, train/loss: 0.49470001459121704
Step: 7750, train/grad_norm: 7.164038181304932
Step: 7750, train/learning_rate: 4.07782026741188e-05
Step: 7750, train/epoch: 1.8443598747253418
Step: 7760, train/loss: 0.48339998722076416
Step: 7760, train/grad_norm: 4.264466762542725
Step: 7760, train/learning_rate: 4.0766302845440805e-05
Step: 7760, train/epoch: 1.846739649772644
Step: 7770, train/loss: 0.36629998683929443
Step: 7770, train/grad_norm: 21.750089645385742
Step: 7770, train/learning_rate: 4.075440301676281e-05
Step: 7770, train/epoch: 1.8491194248199463
Step: 7780, train/loss: 0.4970000088214874
Step: 7780, train/grad_norm: 23.722354888916016
Step: 7780, train/learning_rate: 4.074250318808481e-05
Step: 7780, train/epoch: 1.851499319076538
Step: 7790, train/loss: 0.5465999841690063
Step: 7790, train/grad_norm: 20.9567813873291
Step: 7790, train/learning_rate: 4.0730603359406814e-05
Step: 7790, train/epoch: 1.8538790941238403
Step: 7800, train/loss: 0.40860000252723694
Step: 7800, train/grad_norm: 25.69881820678711
Step: 7800, train/learning_rate: 4.0718707168707624e-05
Step: 7800, train/epoch: 1.8562588691711426
Step: 7810, train/loss: 0.4765999913215637
Step: 7810, train/grad_norm: 21.56110954284668
Step: 7810, train/learning_rate: 4.070680734002963e-05
Step: 7810, train/epoch: 1.8586387634277344
Step: 7820, train/loss: 0.621999979019165
Step: 7820, train/grad_norm: 20.068071365356445
Step: 7820, train/learning_rate: 4.069490751135163e-05
Step: 7820, train/epoch: 1.8610185384750366
Step: 7830, train/loss: 0.4975999891757965
Step: 7830, train/grad_norm: 11.212260246276855
Step: 7830, train/learning_rate: 4.068300768267363e-05
Step: 7830, train/epoch: 1.8633984327316284
Step: 7840, train/loss: 0.4961000084877014
Step: 7840, train/grad_norm: 24.007213592529297
Step: 7840, train/learning_rate: 4.0671107853995636e-05
Step: 7840, train/epoch: 1.8657782077789307
Step: 7850, train/loss: 0.3880000114440918
Step: 7850, train/grad_norm: 5.437498569488525
Step: 7850, train/learning_rate: 4.0659211663296446e-05
Step: 7850, train/epoch: 1.868157982826233
Step: 7860, train/loss: 0.5802000164985657
Step: 7860, train/grad_norm: 12.145709991455078
Step: 7860, train/learning_rate: 4.064731183461845e-05
Step: 7860, train/epoch: 1.8705378770828247
Step: 7870, train/loss: 0.4625999927520752
Step: 7870, train/grad_norm: 14.7637357711792
Step: 7870, train/learning_rate: 4.063541200594045e-05
Step: 7870, train/epoch: 1.872917652130127
Step: 7880, train/loss: 0.4368000030517578
Step: 7880, train/grad_norm: 6.849346160888672
Step: 7880, train/learning_rate: 4.0623512177262455e-05
Step: 7880, train/epoch: 1.8752974271774292
Step: 7890, train/loss: 0.3564999997615814
Step: 7890, train/grad_norm: 12.850221633911133
Step: 7890, train/learning_rate: 4.061161234858446e-05
Step: 7890, train/epoch: 1.877677321434021
Step: 7900, train/loss: 0.46549999713897705
Step: 7900, train/grad_norm: 2.3050801753997803
Step: 7900, train/learning_rate: 4.059971615788527e-05
Step: 7900, train/epoch: 1.8800570964813232
Step: 7910, train/loss: 0.45239999890327454
Step: 7910, train/grad_norm: 4.8267822265625
Step: 7910, train/learning_rate: 4.058781632920727e-05
Step: 7910, train/epoch: 1.882436990737915
Step: 7920, train/loss: 0.49549999833106995
Step: 7920, train/grad_norm: 20.460126876831055
Step: 7920, train/learning_rate: 4.0575916500529274e-05
Step: 7920, train/epoch: 1.8848167657852173
Step: 7930, train/loss: 0.5322999954223633
Step: 7930, train/grad_norm: 4.2706146240234375
Step: 7930, train/learning_rate: 4.056401667185128e-05
Step: 7930, train/epoch: 1.8871965408325195
Step: 7940, train/loss: 0.4625000059604645
Step: 7940, train/grad_norm: 29.74614906311035
Step: 7940, train/learning_rate: 4.055211684317328e-05
Step: 7940, train/epoch: 1.8895764350891113
Step: 7950, train/loss: 0.5454000234603882
Step: 7950, train/grad_norm: 17.756805419921875
Step: 7950, train/learning_rate: 4.054022065247409e-05
Step: 7950, train/epoch: 1.8919562101364136
Step: 7960, train/loss: 0.5440000295639038
Step: 7960, train/grad_norm: 23.08535385131836
Step: 7960, train/learning_rate: 4.0528320823796093e-05
Step: 7960, train/epoch: 1.8943359851837158
Step: 7970, train/loss: 0.44119998812675476
Step: 7970, train/grad_norm: 7.151772499084473
Step: 7970, train/learning_rate: 4.0516420995118096e-05
Step: 7970, train/epoch: 1.8967158794403076
Step: 7980, train/loss: 0.6007000207901001
Step: 7980, train/grad_norm: 12.152965545654297
Step: 7980, train/learning_rate: 4.05045211664401e-05
Step: 7980, train/epoch: 1.8990956544876099
Step: 7990, train/loss: 0.48429998755455017
Step: 7990, train/grad_norm: 5.107416152954102
Step: 7990, train/learning_rate: 4.04926213377621e-05
Step: 7990, train/epoch: 1.901475429534912
Step: 8000, train/loss: 0.49639999866485596
Step: 8000, train/grad_norm: 10.451900482177734
Step: 8000, train/learning_rate: 4.048072514706291e-05
Step: 8000, train/epoch: 1.903855323791504
Step: 8010, train/loss: 0.44279998540878296
Step: 8010, train/grad_norm: 6.482759952545166
Step: 8010, train/learning_rate: 4.0468825318384916e-05
Step: 8010, train/epoch: 1.9062350988388062
Step: 8020, train/loss: 0.4578999876976013
Step: 8020, train/grad_norm: 11.427966117858887
Step: 8020, train/learning_rate: 4.045692548970692e-05
Step: 8020, train/epoch: 1.908614993095398
Step: 8030, train/loss: 0.5440000295639038
Step: 8030, train/grad_norm: 7.432872772216797
Step: 8030, train/learning_rate: 4.044502566102892e-05
Step: 8030, train/epoch: 1.9109947681427002
Step: 8040, train/loss: 0.4745999872684479
Step: 8040, train/grad_norm: 15.190982818603516
Step: 8040, train/learning_rate: 4.0433125832350925e-05
Step: 8040, train/epoch: 1.9133745431900024
Step: 8050, train/loss: 0.5012000203132629
Step: 8050, train/grad_norm: 11.855901718139648
Step: 8050, train/learning_rate: 4.0421229641651735e-05
Step: 8050, train/epoch: 1.9157544374465942
Step: 8060, train/loss: 0.4426000118255615
Step: 8060, train/grad_norm: 19.44216537475586
Step: 8060, train/learning_rate: 4.040932981297374e-05
Step: 8060, train/epoch: 1.9181342124938965
Step: 8070, train/loss: 0.366100013256073
Step: 8070, train/grad_norm: 7.906790256500244
Step: 8070, train/learning_rate: 4.039742998429574e-05
Step: 8070, train/epoch: 1.9205139875411987
Step: 8080, train/loss: 0.44440001249313354
Step: 8080, train/grad_norm: 6.326840877532959
Step: 8080, train/learning_rate: 4.0385530155617744e-05
Step: 8080, train/epoch: 1.9228938817977905
Step: 8090, train/loss: 0.4278999865055084
Step: 8090, train/grad_norm: 5.79863977432251
Step: 8090, train/learning_rate: 4.037363032693975e-05
Step: 8090, train/epoch: 1.9252736568450928
Step: 8100, train/loss: 0.35420000553131104
Step: 8100, train/grad_norm: 16.626373291015625
Step: 8100, train/learning_rate: 4.036173413624056e-05
Step: 8100, train/epoch: 1.9276535511016846
Step: 8110, train/loss: 0.5831999778747559
Step: 8110, train/grad_norm: 7.937691688537598
Step: 8110, train/learning_rate: 4.034983430756256e-05
Step: 8110, train/epoch: 1.9300333261489868
Step: 8120, train/loss: 0.5443000197410583
Step: 8120, train/grad_norm: 2.898524045944214
Step: 8120, train/learning_rate: 4.033793447888456e-05
Step: 8120, train/epoch: 1.932413101196289
Step: 8130, train/loss: 0.4691999852657318
Step: 8130, train/grad_norm: 10.001075744628906
Step: 8130, train/learning_rate: 4.0326034650206566e-05
Step: 8130, train/epoch: 1.9347929954528809
Step: 8140, train/loss: 0.5759999752044678
Step: 8140, train/grad_norm: 13.499695777893066
Step: 8140, train/learning_rate: 4.031413482152857e-05
Step: 8140, train/epoch: 1.937172770500183
Step: 8150, train/loss: 0.38260000944137573
Step: 8150, train/grad_norm: 11.087289810180664
Step: 8150, train/learning_rate: 4.030223863082938e-05
Step: 8150, train/epoch: 1.9395525455474854
Step: 8160, train/loss: 0.45579999685287476
Step: 8160, train/grad_norm: 18.39265251159668
Step: 8160, train/learning_rate: 4.029033880215138e-05
Step: 8160, train/epoch: 1.9419324398040771
Step: 8170, train/loss: 0.3801000118255615
Step: 8170, train/grad_norm: 7.13955545425415
Step: 8170, train/learning_rate: 4.0278438973473385e-05
Step: 8170, train/epoch: 1.9443122148513794
Step: 8180, train/loss: 0.37130001187324524
Step: 8180, train/grad_norm: 13.865954399108887
Step: 8180, train/learning_rate: 4.026653914479539e-05
Step: 8180, train/epoch: 1.9466921091079712
Step: 8190, train/loss: 0.5709999799728394
Step: 8190, train/grad_norm: 4.470008373260498
Step: 8190, train/learning_rate: 4.025463931611739e-05
Step: 8190, train/epoch: 1.9490718841552734
Step: 8200, train/loss: 0.321399986743927
Step: 8200, train/grad_norm: 12.576382637023926
Step: 8200, train/learning_rate: 4.02427431254182e-05
Step: 8200, train/epoch: 1.9514516592025757
Step: 8210, train/loss: 0.3736000061035156
Step: 8210, train/grad_norm: 8.480409622192383
Step: 8210, train/learning_rate: 4.0230843296740204e-05
Step: 8210, train/epoch: 1.9538315534591675
Step: 8220, train/loss: 0.4620000123977661
Step: 8220, train/grad_norm: 8.794411659240723
Step: 8220, train/learning_rate: 4.021894346806221e-05
Step: 8220, train/epoch: 1.9562113285064697
Step: 8230, train/loss: 0.46389999985694885
Step: 8230, train/grad_norm: 8.05074405670166
Step: 8230, train/learning_rate: 4.020704363938421e-05
Step: 8230, train/epoch: 1.958591103553772
Step: 8240, train/loss: 0.3580999970436096
Step: 8240, train/grad_norm: 6.868666172027588
Step: 8240, train/learning_rate: 4.019514381070621e-05
Step: 8240, train/epoch: 1.9609709978103638
Step: 8250, train/loss: 0.44179999828338623
Step: 8250, train/grad_norm: 8.728235244750977
Step: 8250, train/learning_rate: 4.018324762000702e-05
Step: 8250, train/epoch: 1.963350772857666
Step: 8260, train/loss: 0.4787999987602234
Step: 8260, train/grad_norm: 20.209362030029297
Step: 8260, train/learning_rate: 4.0171347791329026e-05
Step: 8260, train/epoch: 1.9657305479049683
Step: 8270, train/loss: 0.5184999704360962
Step: 8270, train/grad_norm: 17.408967971801758
Step: 8270, train/learning_rate: 4.015944796265103e-05
Step: 8270, train/epoch: 1.96811044216156
Step: 8280, train/loss: 0.4717000126838684
Step: 8280, train/grad_norm: 10.467815399169922
Step: 8280, train/learning_rate: 4.014754813397303e-05
Step: 8280, train/epoch: 1.9704902172088623
Step: 8290, train/loss: 0.4440999925136566
Step: 8290, train/grad_norm: 6.368185997009277
Step: 8290, train/learning_rate: 4.0135648305295035e-05
Step: 8290, train/epoch: 1.972870111465454
Step: 8300, train/loss: 0.3312999904155731
Step: 8300, train/grad_norm: 8.00365161895752
Step: 8300, train/learning_rate: 4.0123752114595845e-05
Step: 8300, train/epoch: 1.9752498865127563
Step: 8310, train/loss: 0.392300009727478
Step: 8310, train/grad_norm: 10.403964042663574
Step: 8310, train/learning_rate: 4.011185228591785e-05
Step: 8310, train/epoch: 1.9776296615600586
Step: 8320, train/loss: 0.3815999925136566
Step: 8320, train/grad_norm: 9.021967887878418
Step: 8320, train/learning_rate: 4.009995245723985e-05
Step: 8320, train/epoch: 1.9800095558166504
Step: 8330, train/loss: 0.4018000066280365
Step: 8330, train/grad_norm: 7.228675365447998
Step: 8330, train/learning_rate: 4.0088052628561854e-05
Step: 8330, train/epoch: 1.9823893308639526
Step: 8340, train/loss: 0.553600013256073
Step: 8340, train/grad_norm: 6.447570323944092
Step: 8340, train/learning_rate: 4.007615279988386e-05
Step: 8340, train/epoch: 1.9847691059112549
Step: 8350, train/loss: 0.3817000091075897
Step: 8350, train/grad_norm: 7.869462013244629
Step: 8350, train/learning_rate: 4.006425660918467e-05
Step: 8350, train/epoch: 1.9871490001678467
Step: 8360, train/loss: 0.42809998989105225
Step: 8360, train/grad_norm: 14.119474411010742
Step: 8360, train/learning_rate: 4.005235678050667e-05
Step: 8360, train/epoch: 1.989528775215149
Step: 8370, train/loss: 0.4424000084400177
Step: 8370, train/grad_norm: 11.025729179382324
Step: 8370, train/learning_rate: 4.0040456951828673e-05
Step: 8370, train/epoch: 1.9919086694717407
Step: 8380, train/loss: 0.48069998621940613
Step: 8380, train/grad_norm: 8.567426681518555
Step: 8380, train/learning_rate: 4.0028557123150676e-05
Step: 8380, train/epoch: 1.994288444519043
Step: 8390, train/loss: 0.3935000002384186
Step: 8390, train/grad_norm: 15.995219230651855
Step: 8390, train/learning_rate: 4.001665729447268e-05
Step: 8390, train/epoch: 1.9966682195663452
Step: 8400, train/loss: 0.3865000009536743
Step: 8400, train/grad_norm: 2.568471908569336
Step: 8400, train/learning_rate: 4.000476110377349e-05
Step: 8400, train/epoch: 1.999048113822937
Step: 8404, eval/loss: 1.0120329856872559
Step: 8404, eval/accuracy: 0.5814244151115417
Step: 8404, eval/f1: 0.5694343447685242
Step: 8404, eval/runtime: 54.971900939941406
Step: 8404, eval/samples_per_second: 131.031005859375
Step: 8404, eval/steps_per_second: 16.389999389648438
Step: 8404, train/epoch: 2.0
Step: 8410, train/loss: 0.4043000042438507
Step: 8410, train/grad_norm: 3.286233901977539
Step: 8410, train/learning_rate: 3.999286127509549e-05
Step: 8410, train/epoch: 2.0014278888702393
Step: 8420, train/loss: 0.5331000089645386
Step: 8420, train/grad_norm: 11.504105567932129
Step: 8420, train/learning_rate: 3.9980961446417496e-05
Step: 8420, train/epoch: 2.003807783126831
Step: 8430, train/loss: 0.5048999786376953
Step: 8430, train/grad_norm: 5.588561534881592
Step: 8430, train/learning_rate: 3.99690616177395e-05
Step: 8430, train/epoch: 2.0061874389648438
Step: 8440, train/loss: 0.5418000221252441
Step: 8440, train/grad_norm: 11.38550090789795
Step: 8440, train/learning_rate: 3.99571617890615e-05
Step: 8440, train/epoch: 2.0085673332214355
Step: 8450, train/loss: 0.40799999237060547
Step: 8450, train/grad_norm: 11.490602493286133
Step: 8450, train/learning_rate: 3.994526559836231e-05
Step: 8450, train/epoch: 2.0109472274780273
Step: 8460, train/loss: 0.5601000189781189
Step: 8460, train/grad_norm: 11.35186767578125
Step: 8460, train/learning_rate: 3.9933365769684315e-05
Step: 8460, train/epoch: 2.01332688331604
Step: 8470, train/loss: 0.5906999707221985
Step: 8470, train/grad_norm: 9.513687133789062
Step: 8470, train/learning_rate: 3.992146594100632e-05
Step: 8470, train/epoch: 2.015706777572632
Step: 8480, train/loss: 0.5737000107765198
Step: 8480, train/grad_norm: 10.600383758544922
Step: 8480, train/learning_rate: 3.990956611232832e-05
Step: 8480, train/epoch: 2.0180866718292236
Step: 8490, train/loss: 0.3131999969482422
Step: 8490, train/grad_norm: 14.355680465698242
Step: 8490, train/learning_rate: 3.9897666283650324e-05
Step: 8490, train/epoch: 2.0204663276672363
Step: 8500, train/loss: 0.40790000557899475
Step: 8500, train/grad_norm: 4.621978282928467
Step: 8500, train/learning_rate: 3.9885770092951134e-05
Step: 8500, train/epoch: 2.022846221923828
Step: 8510, train/loss: 0.46700000762939453
Step: 8510, train/grad_norm: 23.010597229003906
Step: 8510, train/learning_rate: 3.987387026427314e-05
Step: 8510, train/epoch: 2.02522611618042
Step: 8520, train/loss: 0.5152000188827515
Step: 8520, train/grad_norm: 16.021989822387695
Step: 8520, train/learning_rate: 3.986197043559514e-05
Step: 8520, train/epoch: 2.0276060104370117
Step: 8530, train/loss: 0.5289999842643738
Step: 8530, train/grad_norm: 24.340097427368164
Step: 8530, train/learning_rate: 3.985007060691714e-05
Step: 8530, train/epoch: 2.0299856662750244
Step: 8540, train/loss: 0.507099986076355
Step: 8540, train/grad_norm: 8.05810832977295
Step: 8540, train/learning_rate: 3.9838170778239146e-05
Step: 8540, train/epoch: 2.032365560531616
Step: 8550, train/loss: 0.42480000853538513
Step: 8550, train/grad_norm: 14.37328815460205
Step: 8550, train/learning_rate: 3.9826274587539956e-05
Step: 8550, train/epoch: 2.034745454788208
Step: 8560, train/loss: 0.421099990606308
Step: 8560, train/grad_norm: 17.999982833862305
Step: 8560, train/learning_rate: 3.981437475886196e-05
Step: 8560, train/epoch: 2.0371251106262207
Step: 8570, train/loss: 0.515999972820282
Step: 8570, train/grad_norm: 8.562570571899414
Step: 8570, train/learning_rate: 3.980247493018396e-05
Step: 8570, train/epoch: 2.0395050048828125
Step: 8580, train/loss: 0.5494999885559082
Step: 8580, train/grad_norm: 18.174455642700195
Step: 8580, train/learning_rate: 3.9790575101505965e-05
Step: 8580, train/epoch: 2.0418848991394043
Step: 8590, train/loss: 0.382099986076355
Step: 8590, train/grad_norm: 8.17499828338623
Step: 8590, train/learning_rate: 3.977867527282797e-05
Step: 8590, train/epoch: 2.044264554977417
Step: 8600, train/loss: 0.4023999869823456
Step: 8600, train/grad_norm: 18.79163360595703
Step: 8600, train/learning_rate: 3.976677908212878e-05
Step: 8600, train/epoch: 2.046644449234009
Step: 8610, train/loss: 0.382999986410141
Step: 8610, train/grad_norm: 10.475028038024902
Step: 8610, train/learning_rate: 3.975487925345078e-05
Step: 8610, train/epoch: 2.0490243434906006
Step: 8620, train/loss: 0.5264999866485596
Step: 8620, train/grad_norm: 4.730206489562988
Step: 8620, train/learning_rate: 3.9742979424772784e-05
Step: 8620, train/epoch: 2.0514039993286133
Step: 8630, train/loss: 0.5569999814033508
Step: 8630, train/grad_norm: 12.221084594726562
Step: 8630, train/learning_rate: 3.973107959609479e-05
Step: 8630, train/epoch: 2.053783893585205
Step: 8640, train/loss: 0.43689998984336853
Step: 8640, train/grad_norm: 4.422430515289307
Step: 8640, train/learning_rate: 3.971917976741679e-05
Step: 8640, train/epoch: 2.056163787841797
Step: 8650, train/loss: 0.42890000343322754
Step: 8650, train/grad_norm: 7.31531286239624
Step: 8650, train/learning_rate: 3.97072835767176e-05
Step: 8650, train/epoch: 2.0585434436798096
Step: 8660, train/loss: 0.6008999943733215
Step: 8660, train/grad_norm: 9.06195068359375
Step: 8660, train/learning_rate: 3.96953837480396e-05
Step: 8660, train/epoch: 2.0609233379364014
Step: 8670, train/loss: 0.47620001435279846
Step: 8670, train/grad_norm: 15.85300350189209
Step: 8670, train/learning_rate: 3.9683483919361606e-05
Step: 8670, train/epoch: 2.063303232192993
Step: 8680, train/loss: 0.40149998664855957
Step: 8680, train/grad_norm: 6.918015480041504
Step: 8680, train/learning_rate: 3.967158409068361e-05
Step: 8680, train/epoch: 2.065683126449585
Step: 8690, train/loss: 0.4465000033378601
Step: 8690, train/grad_norm: 6.032649517059326
Step: 8690, train/learning_rate: 3.965968426200561e-05
Step: 8690, train/epoch: 2.0680627822875977
Step: 8700, train/loss: 0.4878000020980835
Step: 8700, train/grad_norm: 16.721662521362305
Step: 8700, train/learning_rate: 3.964778807130642e-05
Step: 8700, train/epoch: 2.0704426765441895
Step: 8710, train/loss: 0.5554999709129333
Step: 8710, train/grad_norm: 7.9697089195251465
Step: 8710, train/learning_rate: 3.9635888242628425e-05
Step: 8710, train/epoch: 2.0728225708007812
Step: 8720, train/loss: 0.5478000044822693
Step: 8720, train/grad_norm: 14.383668899536133
Step: 8720, train/learning_rate: 3.962398841395043e-05
Step: 8720, train/epoch: 2.075202226638794
Step: 8730, train/loss: 0.4332999885082245
Step: 8730, train/grad_norm: 19.53064727783203
Step: 8730, train/learning_rate: 3.961208858527243e-05
Step: 8730, train/epoch: 2.0775821208953857
Step: 8740, train/loss: 0.40059998631477356
Step: 8740, train/grad_norm: 9.075660705566406
Step: 8740, train/learning_rate: 3.9600188756594434e-05
Step: 8740, train/epoch: 2.0799620151519775
Step: 8750, train/loss: 0.4690999984741211
Step: 8750, train/grad_norm: 8.871659278869629
Step: 8750, train/learning_rate: 3.9588292565895244e-05
Step: 8750, train/epoch: 2.0823416709899902
Step: 8760, train/loss: 0.3368000090122223
Step: 8760, train/grad_norm: 18.831409454345703
Step: 8760, train/learning_rate: 3.957639273721725e-05
Step: 8760, train/epoch: 2.084721565246582
Step: 8770, train/loss: 0.45260000228881836
Step: 8770, train/grad_norm: 15.990185737609863
Step: 8770, train/learning_rate: 3.956449290853925e-05
Step: 8770, train/epoch: 2.087101459503174
Step: 8780, train/loss: 0.3968000113964081
Step: 8780, train/grad_norm: 16.14429473876953
Step: 8780, train/learning_rate: 3.9552593079861253e-05
Step: 8780, train/epoch: 2.0894811153411865
Step: 8790, train/loss: 0.4300000071525574
Step: 8790, train/grad_norm: 14.626550674438477
Step: 8790, train/learning_rate: 3.9540693251183257e-05
Step: 8790, train/epoch: 2.0918610095977783
Step: 8800, train/loss: 0.44940000772476196
Step: 8800, train/grad_norm: 7.176468372344971
Step: 8800, train/learning_rate: 3.9528797060484067e-05
Step: 8800, train/epoch: 2.09424090385437
Step: 8810, train/loss: 0.4814999997615814
Step: 8810, train/grad_norm: 11.895482063293457
Step: 8810, train/learning_rate: 3.951689723180607e-05
Step: 8810, train/epoch: 2.096620559692383
Step: 8820, train/loss: 0.47850000858306885
Step: 8820, train/grad_norm: 16.139232635498047
Step: 8820, train/learning_rate: 3.950499740312807e-05
Step: 8820, train/epoch: 2.0990004539489746
Step: 8830, train/loss: 0.400299996137619
Step: 8830, train/grad_norm: 16.516321182250977
Step: 8830, train/learning_rate: 3.9493097574450076e-05
Step: 8830, train/epoch: 2.1013803482055664
Step: 8840, train/loss: 0.5008999705314636
Step: 8840, train/grad_norm: 19.664886474609375
Step: 8840, train/learning_rate: 3.948119774577208e-05
Step: 8840, train/epoch: 2.103760004043579
Step: 8850, train/loss: 0.5026999711990356
Step: 8850, train/grad_norm: 10.044586181640625
Step: 8850, train/learning_rate: 3.946930155507289e-05
Step: 8850, train/epoch: 2.106139898300171
Step: 8860, train/loss: 0.44369998574256897
Step: 8860, train/grad_norm: 5.277080059051514
Step: 8860, train/learning_rate: 3.945740172639489e-05
Step: 8860, train/epoch: 2.1085197925567627
Step: 8870, train/loss: 0.3767000138759613
Step: 8870, train/grad_norm: 13.091169357299805
Step: 8870, train/learning_rate: 3.9445501897716895e-05
Step: 8870, train/epoch: 2.1108996868133545
Step: 8880, train/loss: 0.4456999897956848
Step: 8880, train/grad_norm: 11.894980430603027
Step: 8880, train/learning_rate: 3.94336020690389e-05
Step: 8880, train/epoch: 2.113279342651367
Step: 8890, train/loss: 0.41029998660087585
Step: 8890, train/grad_norm: 10.825162887573242
Step: 8890, train/learning_rate: 3.94217022403609e-05
Step: 8890, train/epoch: 2.115659236907959
Step: 8900, train/loss: 0.48019999265670776
Step: 8900, train/grad_norm: 8.157217979431152
Step: 8900, train/learning_rate: 3.940980604966171e-05
Step: 8900, train/epoch: 2.118039131164551
Step: 8910, train/loss: 0.4918999969959259
Step: 8910, train/grad_norm: 6.424995422363281
Step: 8910, train/learning_rate: 3.9397906220983714e-05
Step: 8910, train/epoch: 2.1204187870025635
Step: 8920, train/loss: 0.42399999499320984
Step: 8920, train/grad_norm: 17.934770584106445
Step: 8920, train/learning_rate: 3.938600639230572e-05
Step: 8920, train/epoch: 2.1227986812591553
Step: 8930, train/loss: 0.6800000071525574
Step: 8930, train/grad_norm: 19.010223388671875
Step: 8930, train/learning_rate: 3.937410656362772e-05
Step: 8930, train/epoch: 2.125178575515747
Step: 8940, train/loss: 0.42809998989105225
Step: 8940, train/grad_norm: 8.046311378479004
Step: 8940, train/learning_rate: 3.936220673494972e-05
Step: 8940, train/epoch: 2.1275582313537598
Step: 8950, train/loss: 0.520799994468689
Step: 8950, train/grad_norm: 4.70548152923584
Step: 8950, train/learning_rate: 3.935031054425053e-05
Step: 8950, train/epoch: 2.1299381256103516
Step: 8960, train/loss: 0.4205000102519989
Step: 8960, train/grad_norm: 6.038954257965088
Step: 8960, train/learning_rate: 3.9338410715572536e-05
Step: 8960, train/epoch: 2.1323180198669434
Step: 8970, train/loss: 0.3865000009536743
Step: 8970, train/grad_norm: 4.916721343994141
Step: 8970, train/learning_rate: 3.932651088689454e-05
Step: 8970, train/epoch: 2.134697675704956
Step: 8980, train/loss: 0.4221999943256378
Step: 8980, train/grad_norm: 14.502168655395508
Step: 8980, train/learning_rate: 3.931461105821654e-05
Step: 8980, train/epoch: 2.137077569961548
Step: 8990, train/loss: 0.43290001153945923
Step: 8990, train/grad_norm: 3.539905548095703
Step: 8990, train/learning_rate: 3.9302711229538545e-05
Step: 8990, train/epoch: 2.1394574642181396
Step: 9000, train/loss: 0.5741999745368958
Step: 9000, train/grad_norm: 17.20447540283203
Step: 9000, train/learning_rate: 3.9290815038839355e-05
Step: 9000, train/epoch: 2.1418371200561523
Step: 9010, train/loss: 0.5227000117301941
Step: 9010, train/grad_norm: 31.462690353393555
Step: 9010, train/learning_rate: 3.927891521016136e-05
Step: 9010, train/epoch: 2.144217014312744
Step: 9020, train/loss: 0.43709999322891235
Step: 9020, train/grad_norm: 4.055132865905762
Step: 9020, train/learning_rate: 3.926701538148336e-05
Step: 9020, train/epoch: 2.146596908569336
Step: 9030, train/loss: 0.5062999725341797
Step: 9030, train/grad_norm: 16.016698837280273
Step: 9030, train/learning_rate: 3.9255115552805364e-05
Step: 9030, train/epoch: 2.1489765644073486
Step: 9040, train/loss: 0.4472000002861023
Step: 9040, train/grad_norm: 8.254528999328613
Step: 9040, train/learning_rate: 3.924321572412737e-05
Step: 9040, train/epoch: 2.1513564586639404
Step: 9050, train/loss: 0.3869999945163727
Step: 9050, train/grad_norm: 12.735346794128418
Step: 9050, train/learning_rate: 3.923131953342818e-05
Step: 9050, train/epoch: 2.1537363529205322
Step: 9060, train/loss: 0.40389999747276306
Step: 9060, train/grad_norm: 20.128095626831055
Step: 9060, train/learning_rate: 3.921941970475018e-05
Step: 9060, train/epoch: 2.156116247177124
Step: 9070, train/loss: 0.45989999175071716
Step: 9070, train/grad_norm: 5.014438152313232
Step: 9070, train/learning_rate: 3.920751987607218e-05
Step: 9070, train/epoch: 2.1584959030151367
Step: 9080, train/loss: 0.47850000858306885
Step: 9080, train/grad_norm: 24.01268768310547
Step: 9080, train/learning_rate: 3.9195620047394186e-05
Step: 9080, train/epoch: 2.1608757972717285
Step: 9090, train/loss: 0.4011000096797943
Step: 9090, train/grad_norm: 8.906902313232422
Step: 9090, train/learning_rate: 3.918372021871619e-05
Step: 9090, train/epoch: 2.1632556915283203
Step: 9100, train/loss: 0.5486999750137329
Step: 9100, train/grad_norm: 11.664105415344238
Step: 9100, train/learning_rate: 3.9171824028017e-05
Step: 9100, train/epoch: 2.165635347366333
Step: 9110, train/loss: 0.505299985408783
Step: 9110, train/grad_norm: 20.024234771728516
Step: 9110, train/learning_rate: 3.9159924199339e-05
Step: 9110, train/epoch: 2.168015241622925
Step: 9120, train/loss: 0.3986000120639801
Step: 9120, train/grad_norm: 11.003334045410156
Step: 9120, train/learning_rate: 3.9148024370661005e-05
Step: 9120, train/epoch: 2.1703951358795166
Step: 9130, train/loss: 0.4154999852180481
Step: 9130, train/grad_norm: 13.061056137084961
Step: 9130, train/learning_rate: 3.913612454198301e-05
Step: 9130, train/epoch: 2.1727747917175293
Step: 9140, train/loss: 0.4122999906539917
Step: 9140, train/grad_norm: 12.606476783752441
Step: 9140, train/learning_rate: 3.912422835128382e-05
Step: 9140, train/epoch: 2.175154685974121
Step: 9150, train/loss: 0.559499979019165
Step: 9150, train/grad_norm: 29.50031280517578
Step: 9150, train/learning_rate: 3.911232852260582e-05
Step: 9150, train/epoch: 2.177534580230713
Step: 9160, train/loss: 0.4309999942779541
Step: 9160, train/grad_norm: 6.3039655685424805
Step: 9160, train/learning_rate: 3.9100428693927824e-05
Step: 9160, train/epoch: 2.1799142360687256
Step: 9170, train/loss: 0.4113999903202057
Step: 9170, train/grad_norm: 4.508700847625732
Step: 9170, train/learning_rate: 3.908852886524983e-05
Step: 9170, train/epoch: 2.1822941303253174
Step: 9180, train/loss: 0.5957000255584717
Step: 9180, train/grad_norm: 9.335227012634277
Step: 9180, train/learning_rate: 3.907662903657183e-05
Step: 9180, train/epoch: 2.184674024581909
Step: 9190, train/loss: 0.48510000109672546
Step: 9190, train/grad_norm: 8.535639762878418
Step: 9190, train/learning_rate: 3.906473284587264e-05
Step: 9190, train/epoch: 2.187053680419922
Step: 9200, train/loss: 0.48100000619888306
Step: 9200, train/grad_norm: 28.095605850219727
Step: 9200, train/learning_rate: 3.9052833017194644e-05
Step: 9200, train/epoch: 2.1894335746765137
Step: 9210, train/loss: 0.3962000012397766
Step: 9210, train/grad_norm: 5.875717639923096
Step: 9210, train/learning_rate: 3.904093318851665e-05
Step: 9210, train/epoch: 2.1918134689331055
Step: 9220, train/loss: 0.5328999757766724
Step: 9220, train/grad_norm: 6.6188249588012695
Step: 9220, train/learning_rate: 3.902903335983865e-05
Step: 9220, train/epoch: 2.194193124771118
Step: 9230, train/loss: 0.4390000104904175
Step: 9230, train/grad_norm: 9.018272399902344
Step: 9230, train/learning_rate: 3.901713353116065e-05
Step: 9230, train/epoch: 2.19657301902771
Step: 9240, train/loss: 0.3856000006198883
Step: 9240, train/grad_norm: 12.892558097839355
Step: 9240, train/learning_rate: 3.900523734046146e-05
Step: 9240, train/epoch: 2.1989529132843018
Step: 9250, train/loss: 0.42100000381469727
Step: 9250, train/grad_norm: 15.054482460021973
Step: 9250, train/learning_rate: 3.8993337511783466e-05
Step: 9250, train/epoch: 2.2013328075408936
Step: 9260, train/loss: 0.4081999957561493
Step: 9260, train/grad_norm: 11.615965843200684
Step: 9260, train/learning_rate: 3.898143768310547e-05
Step: 9260, train/epoch: 2.2037124633789062
Step: 9270, train/loss: 0.3840999901294708
Step: 9270, train/grad_norm: 6.0144500732421875
Step: 9270, train/learning_rate: 3.896953785442747e-05
Step: 9270, train/epoch: 2.206092357635498
Step: 9280, train/loss: 0.4099999964237213
Step: 9280, train/grad_norm: 10.351497650146484
Step: 9280, train/learning_rate: 3.8957638025749475e-05
Step: 9280, train/epoch: 2.20847225189209
Step: 9290, train/loss: 0.5324000120162964
Step: 9290, train/grad_norm: 6.9650750160217285
Step: 9290, train/learning_rate: 3.8945741835050285e-05
Step: 9290, train/epoch: 2.2108519077301025
Step: 9300, train/loss: 0.4377000033855438
Step: 9300, train/grad_norm: 6.658005237579346
Step: 9300, train/learning_rate: 3.893384200637229e-05
Step: 9300, train/epoch: 2.2132318019866943
Step: 9310, train/loss: 0.5681999921798706
Step: 9310, train/grad_norm: 22.776363372802734
Step: 9310, train/learning_rate: 3.892194217769429e-05
Step: 9310, train/epoch: 2.215611696243286
Step: 9320, train/loss: 0.3571999967098236
Step: 9320, train/grad_norm: 18.95738983154297
Step: 9320, train/learning_rate: 3.8910042349016294e-05
Step: 9320, train/epoch: 2.217991352081299
Step: 9330, train/loss: 0.4945000112056732
Step: 9330, train/grad_norm: 11.371160507202148
Step: 9330, train/learning_rate: 3.88981425203383e-05
Step: 9330, train/epoch: 2.2203712463378906
Step: 9340, train/loss: 0.483599990606308
Step: 9340, train/grad_norm: 6.121960639953613
Step: 9340, train/learning_rate: 3.888624632963911e-05
Step: 9340, train/epoch: 2.2227511405944824
Step: 9350, train/loss: 0.5304999947547913
Step: 9350, train/grad_norm: 13.142502784729004
Step: 9350, train/learning_rate: 3.887434650096111e-05
Step: 9350, train/epoch: 2.225130796432495
Step: 9360, train/loss: 0.492900013923645
Step: 9360, train/grad_norm: 9.496553421020508
Step: 9360, train/learning_rate: 3.886244667228311e-05
Step: 9360, train/epoch: 2.227510690689087
Step: 9370, train/loss: 0.4140999913215637
Step: 9370, train/grad_norm: 8.482497215270996
Step: 9370, train/learning_rate: 3.8850546843605116e-05
Step: 9370, train/epoch: 2.2298905849456787
Step: 9380, train/loss: 0.4047999978065491
Step: 9380, train/grad_norm: 4.676026821136475
Step: 9380, train/learning_rate: 3.883864701492712e-05
Step: 9380, train/epoch: 2.2322702407836914
Step: 9390, train/loss: 0.38029998540878296
Step: 9390, train/grad_norm: 9.04890251159668
Step: 9390, train/learning_rate: 3.882675082422793e-05
Step: 9390, train/epoch: 2.234650135040283
Step: 9400, train/loss: 0.4717000126838684
Step: 9400, train/grad_norm: 14.888447761535645
Step: 9400, train/learning_rate: 3.881485099554993e-05
Step: 9400, train/epoch: 2.237030029296875
Step: 9410, train/loss: 0.41339999437332153
Step: 9410, train/grad_norm: 13.147228240966797
Step: 9410, train/learning_rate: 3.8802951166871935e-05
Step: 9410, train/epoch: 2.239409923553467
Step: 9420, train/loss: 0.6266999840736389
Step: 9420, train/grad_norm: 4.7044267654418945
Step: 9420, train/learning_rate: 3.879105133819394e-05
Step: 9420, train/epoch: 2.2417895793914795
Step: 9430, train/loss: 0.43860000371932983
Step: 9430, train/grad_norm: 7.089905261993408
Step: 9430, train/learning_rate: 3.877915150951594e-05
Step: 9430, train/epoch: 2.2441694736480713
Step: 9440, train/loss: 0.4056999981403351
Step: 9440, train/grad_norm: 3.5144712924957275
Step: 9440, train/learning_rate: 3.876725531881675e-05
Step: 9440, train/epoch: 2.246549367904663
Step: 9450, train/loss: 0.39259999990463257
Step: 9450, train/grad_norm: 4.575978755950928
Step: 9450, train/learning_rate: 3.8755355490138754e-05
Step: 9450, train/epoch: 2.248929023742676
Step: 9460, train/loss: 0.5376999974250793
Step: 9460, train/grad_norm: 17.279253005981445
Step: 9460, train/learning_rate: 3.874345566146076e-05
Step: 9460, train/epoch: 2.2513089179992676
Step: 9470, train/loss: 0.28060001134872437
Step: 9470, train/grad_norm: 4.556375026702881
Step: 9470, train/learning_rate: 3.873155583278276e-05
Step: 9470, train/epoch: 2.2536888122558594
Step: 9480, train/loss: 0.4650000035762787
Step: 9480, train/grad_norm: 5.495358467102051
Step: 9480, train/learning_rate: 3.871965600410476e-05
Step: 9480, train/epoch: 2.256068468093872
Step: 9490, train/loss: 0.5148000121116638
Step: 9490, train/grad_norm: 24.541305541992188
Step: 9490, train/learning_rate: 3.870775981340557e-05
Step: 9490, train/epoch: 2.258448362350464
Step: 9500, train/loss: 0.43549999594688416
Step: 9500, train/grad_norm: 5.634788513183594
Step: 9500, train/learning_rate: 3.8695859984727576e-05
Step: 9500, train/epoch: 2.2608282566070557
Step: 9510, train/loss: 0.32420000433921814
Step: 9510, train/grad_norm: 7.357082843780518
Step: 9510, train/learning_rate: 3.868396015604958e-05
Step: 9510, train/epoch: 2.2632079124450684
Step: 9520, train/loss: 0.5551999807357788
Step: 9520, train/grad_norm: 19.766252517700195
Step: 9520, train/learning_rate: 3.867206032737158e-05
Step: 9520, train/epoch: 2.26558780670166
Step: 9530, train/loss: 0.3903999924659729
Step: 9530, train/grad_norm: 6.942618370056152
Step: 9530, train/learning_rate: 3.8660160498693585e-05
Step: 9530, train/epoch: 2.267967700958252
Step: 9540, train/loss: 0.4575999975204468
Step: 9540, train/grad_norm: 16.639944076538086
Step: 9540, train/learning_rate: 3.8648264307994395e-05
Step: 9540, train/epoch: 2.2703473567962646
Step: 9550, train/loss: 0.39899998903274536
Step: 9550, train/grad_norm: 13.908890724182129
Step: 9550, train/learning_rate: 3.86363644793164e-05
Step: 9550, train/epoch: 2.2727272510528564
Step: 9560, train/loss: 0.6100999712944031
Step: 9560, train/grad_norm: 7.398029804229736
Step: 9560, train/learning_rate: 3.86244646506384e-05
Step: 9560, train/epoch: 2.2751071453094482
Step: 9570, train/loss: 0.39649999141693115
Step: 9570, train/grad_norm: 3.8538966178894043
Step: 9570, train/learning_rate: 3.8612564821960405e-05
Step: 9570, train/epoch: 2.277486801147461
Step: 9580, train/loss: 0.4742000102996826
Step: 9580, train/grad_norm: 8.877811431884766
Step: 9580, train/learning_rate: 3.860066499328241e-05
Step: 9580, train/epoch: 2.2798666954040527
Step: 9590, train/loss: 0.38280001282691956
Step: 9590, train/grad_norm: 18.16695213317871
Step: 9590, train/learning_rate: 3.858876880258322e-05
Step: 9590, train/epoch: 2.2822465896606445
Step: 9600, train/loss: 0.52920001745224
Step: 9600, train/grad_norm: 14.260059356689453
Step: 9600, train/learning_rate: 3.857686897390522e-05
Step: 9600, train/epoch: 2.2846264839172363
Step: 9610, train/loss: 0.33660000562667847
Step: 9610, train/grad_norm: 3.6012682914733887
Step: 9610, train/learning_rate: 3.8564969145227224e-05
Step: 9610, train/epoch: 2.287006139755249
Step: 9620, train/loss: 0.424699991941452
Step: 9620, train/grad_norm: 19.593904495239258
Step: 9620, train/learning_rate: 3.855306931654923e-05
Step: 9620, train/epoch: 2.289386034011841
Step: 9630, train/loss: 0.6401000022888184
Step: 9630, train/grad_norm: 6.738536357879639
Step: 9630, train/learning_rate: 3.854116948787123e-05
Step: 9630, train/epoch: 2.2917659282684326
Step: 9640, train/loss: 0.38429999351501465
Step: 9640, train/grad_norm: 9.286755561828613
Step: 9640, train/learning_rate: 3.852927329717204e-05
Step: 9640, train/epoch: 2.2941455841064453
Step: 9650, train/loss: 0.5098000168800354
Step: 9650, train/grad_norm: 7.057628631591797
Step: 9650, train/learning_rate: 3.851737346849404e-05
Step: 9650, train/epoch: 2.296525478363037
Step: 9660, train/loss: 0.5112000107765198
Step: 9660, train/grad_norm: 10.239301681518555
Step: 9660, train/learning_rate: 3.8505473639816046e-05
Step: 9660, train/epoch: 2.298905372619629
Step: 9670, train/loss: 0.39910000562667847
Step: 9670, train/grad_norm: 9.011164665222168
Step: 9670, train/learning_rate: 3.849357381113805e-05
Step: 9670, train/epoch: 2.3012850284576416
Step: 9680, train/loss: 0.5236999988555908
Step: 9680, train/grad_norm: 6.05641508102417
Step: 9680, train/learning_rate: 3.848167398246005e-05
Step: 9680, train/epoch: 2.3036649227142334
Step: 9690, train/loss: 0.5062000155448914
Step: 9690, train/grad_norm: 6.354147911071777
Step: 9690, train/learning_rate: 3.846977779176086e-05
Step: 9690, train/epoch: 2.306044816970825
Step: 9700, train/loss: 0.45210000872612
Step: 9700, train/grad_norm: 5.790156841278076
Step: 9700, train/learning_rate: 3.8457877963082865e-05
Step: 9700, train/epoch: 2.308424472808838
Step: 9710, train/loss: 0.3781999945640564
Step: 9710, train/grad_norm: 12.200369834899902
Step: 9710, train/learning_rate: 3.844597813440487e-05
Step: 9710, train/epoch: 2.3108043670654297
Step: 9720, train/loss: 0.41609999537467957
Step: 9720, train/grad_norm: 23.149221420288086
Step: 9720, train/learning_rate: 3.843407830572687e-05
Step: 9720, train/epoch: 2.3131842613220215
Step: 9730, train/loss: 0.4731999933719635
Step: 9730, train/grad_norm: 10.26479434967041
Step: 9730, train/learning_rate: 3.8422178477048874e-05
Step: 9730, train/epoch: 2.315563917160034
Step: 9740, train/loss: 0.30790001153945923
Step: 9740, train/grad_norm: 4.51664400100708
Step: 9740, train/learning_rate: 3.8410282286349684e-05
Step: 9740, train/epoch: 2.317943811416626
Step: 9750, train/loss: 0.39329999685287476
Step: 9750, train/grad_norm: 7.432501316070557
Step: 9750, train/learning_rate: 3.839838245767169e-05
Step: 9750, train/epoch: 2.3203237056732178
Step: 9760, train/loss: 0.5042999982833862
Step: 9760, train/grad_norm: 10.342339515686035
Step: 9760, train/learning_rate: 3.838648262899369e-05
Step: 9760, train/epoch: 2.3227033615112305
Step: 9770, train/loss: 0.4088999927043915
Step: 9770, train/grad_norm: 4.586640357971191
Step: 9770, train/learning_rate: 3.837458280031569e-05
Step: 9770, train/epoch: 2.3250832557678223
Step: 9780, train/loss: 0.4223000109195709
Step: 9780, train/grad_norm: 5.858067989349365
Step: 9780, train/learning_rate: 3.8362682971637696e-05
Step: 9780, train/epoch: 2.327463150024414
Step: 9790, train/loss: 0.38609999418258667
Step: 9790, train/grad_norm: 13.901582717895508
Step: 9790, train/learning_rate: 3.8350786780938506e-05
Step: 9790, train/epoch: 2.329843044281006
Step: 9800, train/loss: 0.5375000238418579
Step: 9800, train/grad_norm: 16.41439437866211
Step: 9800, train/learning_rate: 3.833888695226051e-05
Step: 9800, train/epoch: 2.3322227001190186
Step: 9810, train/loss: 0.5069000124931335
Step: 9810, train/grad_norm: 23.010570526123047
Step: 9810, train/learning_rate: 3.832698712358251e-05
Step: 9810, train/epoch: 2.3346025943756104
Step: 9820, train/loss: 0.3946000039577484
Step: 9820, train/grad_norm: 10.72074031829834
Step: 9820, train/learning_rate: 3.8315087294904515e-05
Step: 9820, train/epoch: 2.336982488632202
Step: 9830, train/loss: 0.5856999754905701
Step: 9830, train/grad_norm: 14.733089447021484
Step: 9830, train/learning_rate: 3.830318746622652e-05
Step: 9830, train/epoch: 2.339362144470215
Step: 9840, train/loss: 0.37450000643730164
Step: 9840, train/grad_norm: 13.195320129394531
Step: 9840, train/learning_rate: 3.829129127552733e-05
Step: 9840, train/epoch: 2.3417420387268066
Step: 9850, train/loss: 0.5199000239372253
Step: 9850, train/grad_norm: 20.51673698425293
Step: 9850, train/learning_rate: 3.827939144684933e-05
Step: 9850, train/epoch: 2.3441219329833984
Step: 9860, train/loss: 0.3847000002861023
Step: 9860, train/grad_norm: 18.47986602783203
Step: 9860, train/learning_rate: 3.8267491618171334e-05
Step: 9860, train/epoch: 2.346501588821411
Step: 9870, train/loss: 0.46480000019073486
Step: 9870, train/grad_norm: 14.799920082092285
Step: 9870, train/learning_rate: 3.825559178949334e-05
Step: 9870, train/epoch: 2.348881483078003
Step: 9880, train/loss: 0.46219998598098755
Step: 9880, train/grad_norm: 9.728706359863281
Step: 9880, train/learning_rate: 3.824369196081534e-05
Step: 9880, train/epoch: 2.3512613773345947
Step: 9890, train/loss: 0.5044999718666077
Step: 9890, train/grad_norm: 5.894068717956543
Step: 9890, train/learning_rate: 3.823179577011615e-05
Step: 9890, train/epoch: 2.3536410331726074
Step: 9900, train/loss: 0.3001999855041504
Step: 9900, train/grad_norm: 16.650402069091797
Step: 9900, train/learning_rate: 3.821989594143815e-05
Step: 9900, train/epoch: 2.356020927429199
Step: 9910, train/loss: 0.4043000042438507
Step: 9910, train/grad_norm: 7.714852809906006
Step: 9910, train/learning_rate: 3.8207996112760156e-05
Step: 9910, train/epoch: 2.358400821685791
Step: 9920, train/loss: 0.28600001335144043
Step: 9920, train/grad_norm: 12.016396522521973
Step: 9920, train/learning_rate: 3.819609628408216e-05
Step: 9920, train/epoch: 2.3607804775238037
Step: 9930, train/loss: 0.36579999327659607
Step: 9930, train/grad_norm: 3.852240800857544
Step: 9930, train/learning_rate: 3.818419645540416e-05
Step: 9930, train/epoch: 2.3631603717803955
Step: 9940, train/loss: 0.4853000044822693
Step: 9940, train/grad_norm: 1.2423231601715088
Step: 9940, train/learning_rate: 3.817230026470497e-05
Step: 9940, train/epoch: 2.3655402660369873
Step: 9950, train/loss: 0.43160000443458557
Step: 9950, train/grad_norm: 10.306351661682129
Step: 9950, train/learning_rate: 3.8160400436026976e-05
Step: 9950, train/epoch: 2.367919921875
Step: 9960, train/loss: 0.421099990606308
Step: 9960, train/grad_norm: 7.110914707183838
Step: 9960, train/learning_rate: 3.814850060734898e-05
Step: 9960, train/epoch: 2.370299816131592
Step: 9970, train/loss: 0.38999998569488525
Step: 9970, train/grad_norm: 17.988601684570312
Step: 9970, train/learning_rate: 3.813660077867098e-05
Step: 9970, train/epoch: 2.3726797103881836
Step: 9980, train/loss: 0.5210999846458435
Step: 9980, train/grad_norm: 5.350986480712891
Step: 9980, train/learning_rate: 3.8124700949992985e-05
Step: 9980, train/epoch: 2.3750596046447754
Step: 9990, train/loss: 0.4634000062942505
Step: 9990, train/grad_norm: 10.437053680419922
Step: 9990, train/learning_rate: 3.8112804759293795e-05
Step: 9990, train/epoch: 2.377439260482788
Step: 10000, train/loss: 0.3752000033855438
Step: 10000, train/grad_norm: 6.470355987548828
Step: 10000, train/learning_rate: 3.81009049306158e-05
Step: 10000, train/epoch: 2.37981915473938
Step: 10010, train/loss: 0.454800009727478
Step: 10010, train/grad_norm: 14.621485710144043
Step: 10010, train/learning_rate: 3.80890051019378e-05
Step: 10010, train/epoch: 2.3821990489959717
Step: 10020, train/loss: 0.32409998774528503
Step: 10020, train/grad_norm: 8.970248222351074
Step: 10020, train/learning_rate: 3.8077105273259804e-05
Step: 10020, train/epoch: 2.3845787048339844
Step: 10030, train/loss: 0.40070000290870667
Step: 10030, train/grad_norm: 11.034394264221191
Step: 10030, train/learning_rate: 3.806520544458181e-05
Step: 10030, train/epoch: 2.386958599090576
Step: 10040, train/loss: 0.42080000042915344
Step: 10040, train/grad_norm: 16.289644241333008
Step: 10040, train/learning_rate: 3.805330925388262e-05
Step: 10040, train/epoch: 2.389338493347168
Step: 10050, train/loss: 0.45719999074935913
Step: 10050, train/grad_norm: 10.364313125610352
Step: 10050, train/learning_rate: 3.804140942520462e-05
Step: 10050, train/epoch: 2.3917181491851807
Step: 10060, train/loss: 0.48249998688697815
Step: 10060, train/grad_norm: 24.142253875732422
Step: 10060, train/learning_rate: 3.802950959652662e-05
Step: 10060, train/epoch: 2.3940980434417725
Step: 10070, train/loss: 0.3529999852180481
Step: 10070, train/grad_norm: 23.044511795043945
Step: 10070, train/learning_rate: 3.8017609767848626e-05
Step: 10070, train/epoch: 2.3964779376983643
Step: 10080, train/loss: 0.6444000005722046
Step: 10080, train/grad_norm: 16.820886611938477
Step: 10080, train/learning_rate: 3.800570993917063e-05
Step: 10080, train/epoch: 2.398857593536377
Step: 10090, train/loss: 0.37470000982284546
Step: 10090, train/grad_norm: 13.352428436279297
Step: 10090, train/learning_rate: 3.799381374847144e-05
Step: 10090, train/epoch: 2.4012374877929688
Step: 10100, train/loss: 0.5465999841690063
Step: 10100, train/grad_norm: 20.535274505615234
Step: 10100, train/learning_rate: 3.798191391979344e-05
Step: 10100, train/epoch: 2.4036173820495605
Step: 10110, train/loss: 0.5953999757766724
Step: 10110, train/grad_norm: 2.3321616649627686
Step: 10110, train/learning_rate: 3.7970014091115445e-05
Step: 10110, train/epoch: 2.4059970378875732
Step: 10120, train/loss: 0.4997999966144562
Step: 10120, train/grad_norm: 7.449954509735107
Step: 10120, train/learning_rate: 3.795811426243745e-05
Step: 10120, train/epoch: 2.408376932144165
Step: 10130, train/loss: 0.38679999113082886
Step: 10130, train/grad_norm: 15.367618560791016
Step: 10130, train/learning_rate: 3.794621443375945e-05
Step: 10130, train/epoch: 2.410756826400757
Step: 10140, train/loss: 0.424699991941452
Step: 10140, train/grad_norm: 10.914999008178711
Step: 10140, train/learning_rate: 3.793431824306026e-05
Step: 10140, train/epoch: 2.4131367206573486
Step: 10150, train/loss: 0.4449000060558319
Step: 10150, train/grad_norm: 22.162158966064453
Step: 10150, train/learning_rate: 3.7922418414382264e-05
Step: 10150, train/epoch: 2.4155163764953613
Step: 10160, train/loss: 0.5590999722480774
Step: 10160, train/grad_norm: 6.200451374053955
Step: 10160, train/learning_rate: 3.791051858570427e-05
Step: 10160, train/epoch: 2.417896270751953
Step: 10170, train/loss: 0.47429999709129333
Step: 10170, train/grad_norm: 9.568693161010742
Step: 10170, train/learning_rate: 3.789861875702627e-05
Step: 10170, train/epoch: 2.420276165008545
Step: 10180, train/loss: 0.4007999897003174
Step: 10180, train/grad_norm: 6.427308082580566
Step: 10180, train/learning_rate: 3.788671892834827e-05
Step: 10180, train/epoch: 2.4226558208465576
Step: 10190, train/loss: 0.5361999869346619
Step: 10190, train/grad_norm: 7.664926052093506
Step: 10190, train/learning_rate: 3.787482273764908e-05
Step: 10190, train/epoch: 2.4250357151031494
Step: 10200, train/loss: 0.3871999979019165
Step: 10200, train/grad_norm: 9.868528366088867
Step: 10200, train/learning_rate: 3.7862922908971086e-05
Step: 10200, train/epoch: 2.427415609359741
Step: 10210, train/loss: 0.4514999985694885
Step: 10210, train/grad_norm: 6.364630699157715
Step: 10210, train/learning_rate: 3.785102308029309e-05
Step: 10210, train/epoch: 2.429795265197754
Step: 10220, train/loss: 0.41100001335144043
Step: 10220, train/grad_norm: 8.575582504272461
Step: 10220, train/learning_rate: 3.783912325161509e-05
Step: 10220, train/epoch: 2.4321751594543457
Step: 10230, train/loss: 0.328900009393692
Step: 10230, train/grad_norm: 6.4577226638793945
Step: 10230, train/learning_rate: 3.7827223422937095e-05
Step: 10230, train/epoch: 2.4345550537109375
Step: 10240, train/loss: 0.44209998846054077
Step: 10240, train/grad_norm: 10.556066513061523
Step: 10240, train/learning_rate: 3.7815327232237905e-05
Step: 10240, train/epoch: 2.43693470954895
Step: 10250, train/loss: 0.5419999957084656
Step: 10250, train/grad_norm: 20.81633186340332
Step: 10250, train/learning_rate: 3.780342740355991e-05
Step: 10250, train/epoch: 2.439314603805542
Step: 10260, train/loss: 0.4519999921321869
Step: 10260, train/grad_norm: 5.345772743225098
Step: 10260, train/learning_rate: 3.779152757488191e-05
Step: 10260, train/epoch: 2.441694498062134
Step: 10270, train/loss: 0.43939998745918274
Step: 10270, train/grad_norm: 11.779709815979004
Step: 10270, train/learning_rate: 3.7779627746203914e-05
Step: 10270, train/epoch: 2.4440741539001465
Step: 10280, train/loss: 0.41290000081062317
Step: 10280, train/grad_norm: 7.4312896728515625
Step: 10280, train/learning_rate: 3.776772791752592e-05
Step: 10280, train/epoch: 2.4464540481567383
Step: 10290, train/loss: 0.39649999141693115
Step: 10290, train/grad_norm: 4.677394866943359
Step: 10290, train/learning_rate: 3.775583172682673e-05
Step: 10290, train/epoch: 2.44883394241333
Step: 10300, train/loss: 0.3944999873638153
Step: 10300, train/grad_norm: 3.1408400535583496
Step: 10300, train/learning_rate: 3.774393189814873e-05
Step: 10300, train/epoch: 2.4512135982513428
Step: 10310, train/loss: 0.33640000224113464
Step: 10310, train/grad_norm: 7.665161609649658
Step: 10310, train/learning_rate: 3.7732032069470733e-05
Step: 10310, train/epoch: 2.4535934925079346
Step: 10320, train/loss: 0.37630000710487366
Step: 10320, train/grad_norm: 12.963258743286133
Step: 10320, train/learning_rate: 3.7720132240792736e-05
Step: 10320, train/epoch: 2.4559733867645264
Step: 10330, train/loss: 0.44909998774528503
Step: 10330, train/grad_norm: 27.7549991607666
Step: 10330, train/learning_rate: 3.770823241211474e-05
Step: 10330, train/epoch: 2.458353281021118
Step: 10340, train/loss: 0.534600019454956
Step: 10340, train/grad_norm: 4.179365634918213
Step: 10340, train/learning_rate: 3.769633622141555e-05
Step: 10340, train/epoch: 2.460732936859131
Step: 10350, train/loss: 0.42559999227523804
Step: 10350, train/grad_norm: 10.937006950378418
Step: 10350, train/learning_rate: 3.768443639273755e-05
Step: 10350, train/epoch: 2.4631128311157227
Step: 10360, train/loss: 0.3652999997138977
Step: 10360, train/grad_norm: 10.524133682250977
Step: 10360, train/learning_rate: 3.7672536564059556e-05
Step: 10360, train/epoch: 2.4654927253723145
Step: 10370, train/loss: 0.33500000834465027
Step: 10370, train/grad_norm: 12.310136795043945
Step: 10370, train/learning_rate: 3.766063673538156e-05
Step: 10370, train/epoch: 2.467872381210327
Step: 10380, train/loss: 0.6147000193595886
Step: 10380, train/grad_norm: 22.60565948486328
Step: 10380, train/learning_rate: 3.764873690670356e-05
Step: 10380, train/epoch: 2.470252275466919
Step: 10390, train/loss: 0.48410001397132874
Step: 10390, train/grad_norm: 10.922094345092773
Step: 10390, train/learning_rate: 3.763684071600437e-05
Step: 10390, train/epoch: 2.4726321697235107
Step: 10400, train/loss: 0.4593999981880188
Step: 10400, train/grad_norm: 9.674351692199707
Step: 10400, train/learning_rate: 3.7624940887326375e-05
Step: 10400, train/epoch: 2.4750118255615234
Step: 10410, train/loss: 0.47209998965263367
Step: 10410, train/grad_norm: 13.267245292663574
Step: 10410, train/learning_rate: 3.761304105864838e-05
Step: 10410, train/epoch: 2.4773917198181152
Step: 10420, train/loss: 0.4027999937534332
Step: 10420, train/grad_norm: 14.82143783569336
Step: 10420, train/learning_rate: 3.760114122997038e-05
Step: 10420, train/epoch: 2.479771614074707
Step: 10430, train/loss: 0.4722000062465668
Step: 10430, train/grad_norm: 10.804125785827637
Step: 10430, train/learning_rate: 3.7589241401292384e-05
Step: 10430, train/epoch: 2.4821512699127197
Step: 10440, train/loss: 0.4327999949455261
Step: 10440, train/grad_norm: 11.505107879638672
Step: 10440, train/learning_rate: 3.7577345210593194e-05
Step: 10440, train/epoch: 2.4845311641693115
Step: 10450, train/loss: 0.6175000071525574
Step: 10450, train/grad_norm: 8.964143753051758
Step: 10450, train/learning_rate: 3.75654453819152e-05
Step: 10450, train/epoch: 2.4869110584259033
Step: 10460, train/loss: 0.46959999203681946
Step: 10460, train/grad_norm: 4.790012359619141
Step: 10460, train/learning_rate: 3.75535455532372e-05
Step: 10460, train/epoch: 2.489290714263916
Step: 10470, train/loss: 0.36980000138282776
Step: 10470, train/grad_norm: 5.6494011878967285
Step: 10470, train/learning_rate: 3.75416457245592e-05
Step: 10470, train/epoch: 2.491670608520508
Step: 10480, train/loss: 0.4636000096797943
Step: 10480, train/grad_norm: 7.737411022186279
Step: 10480, train/learning_rate: 3.752974953386001e-05
Step: 10480, train/epoch: 2.4940505027770996
Step: 10490, train/loss: 0.25949999690055847
Step: 10490, train/grad_norm: 6.07444429397583
Step: 10490, train/learning_rate: 3.7517849705182016e-05
Step: 10490, train/epoch: 2.4964301586151123
Step: 10500, train/loss: 0.3968999981880188
Step: 10500, train/grad_norm: 4.683706760406494
Step: 10500, train/learning_rate: 3.750594987650402e-05
Step: 10500, train/epoch: 2.498810052871704
Step: 10510, train/loss: 0.4296000003814697
Step: 10510, train/grad_norm: 8.970614433288574
Step: 10510, train/learning_rate: 3.749405004782602e-05
Step: 10510, train/epoch: 2.501189947128296
Step: 10520, train/loss: 0.5307999849319458
Step: 10520, train/grad_norm: 13.581039428710938
Step: 10520, train/learning_rate: 3.7482150219148025e-05
Step: 10520, train/epoch: 2.5035698413848877
Step: 10530, train/loss: 0.6007000207901001
Step: 10530, train/grad_norm: 17.914865493774414
Step: 10530, train/learning_rate: 3.7470254028448835e-05
Step: 10530, train/epoch: 2.5059494972229004
Step: 10540, train/loss: 0.400299996137619
Step: 10540, train/grad_norm: 19.648221969604492
Step: 10540, train/learning_rate: 3.745835419977084e-05
Step: 10540, train/epoch: 2.508329391479492
Step: 10550, train/loss: 0.4207000136375427
Step: 10550, train/grad_norm: 15.538373947143555
Step: 10550, train/learning_rate: 3.744645437109284e-05
Step: 10550, train/epoch: 2.510709285736084
Step: 10560, train/loss: 0.5697000026702881
Step: 10560, train/grad_norm: 13.184660911560059
Step: 10560, train/learning_rate: 3.7434554542414844e-05
Step: 10560, train/epoch: 2.5130889415740967
Step: 10570, train/loss: 0.42809998989105225
Step: 10570, train/grad_norm: 11.583000183105469
Step: 10570, train/learning_rate: 3.742265471373685e-05
Step: 10570, train/epoch: 2.5154688358306885
Step: 10580, train/loss: 0.4424000084400177
Step: 10580, train/grad_norm: 21.595577239990234
Step: 10580, train/learning_rate: 3.741075852303766e-05
Step: 10580, train/epoch: 2.5178487300872803
Step: 10590, train/loss: 0.5123000144958496
Step: 10590, train/grad_norm: 3.649456739425659
Step: 10590, train/learning_rate: 3.739885869435966e-05
Step: 10590, train/epoch: 2.520228385925293
Step: 10600, train/loss: 0.4828000068664551
Step: 10600, train/grad_norm: 8.382277488708496
Step: 10600, train/learning_rate: 3.738695886568166e-05
Step: 10600, train/epoch: 2.5226082801818848
Step: 10610, train/loss: 0.4765999913215637
Step: 10610, train/grad_norm: 22.524152755737305
Step: 10610, train/learning_rate: 3.7375059037003666e-05
Step: 10610, train/epoch: 2.5249881744384766
Step: 10620, train/loss: 0.4700999855995178
Step: 10620, train/grad_norm: 4.330859661102295
Step: 10620, train/learning_rate: 3.736315920832567e-05
Step: 10620, train/epoch: 2.5273678302764893
Step: 10630, train/loss: 0.3181000053882599
Step: 10630, train/grad_norm: 6.143679618835449
Step: 10630, train/learning_rate: 3.735126301762648e-05
Step: 10630, train/epoch: 2.529747724533081
Step: 10640, train/loss: 0.34860000014305115
Step: 10640, train/grad_norm: 6.7633585929870605
Step: 10640, train/learning_rate: 3.733936318894848e-05
Step: 10640, train/epoch: 2.532127618789673
Step: 10650, train/loss: 0.44589999318122864
Step: 10650, train/grad_norm: 8.478968620300293
Step: 10650, train/learning_rate: 3.7327463360270485e-05
Step: 10650, train/epoch: 2.5345072746276855
Step: 10660, train/loss: 0.38659998774528503
Step: 10660, train/grad_norm: 9.876628875732422
Step: 10660, train/learning_rate: 3.731556353159249e-05
Step: 10660, train/epoch: 2.5368871688842773
Step: 10670, train/loss: 0.37139999866485596
Step: 10670, train/grad_norm: 10.005295753479004
Step: 10670, train/learning_rate: 3.730366370291449e-05
Step: 10670, train/epoch: 2.539267063140869
Step: 10680, train/loss: 0.4875999987125397
Step: 10680, train/grad_norm: 14.56765365600586
Step: 10680, train/learning_rate: 3.72917675122153e-05
Step: 10680, train/epoch: 2.541646718978882
Step: 10690, train/loss: 0.5428000092506409
Step: 10690, train/grad_norm: 5.865333080291748
Step: 10690, train/learning_rate: 3.7279867683537304e-05
Step: 10690, train/epoch: 2.5440266132354736
Step: 10700, train/loss: 0.4490000009536743
Step: 10700, train/grad_norm: 4.527318477630615
Step: 10700, train/learning_rate: 3.726796785485931e-05
Step: 10700, train/epoch: 2.5464065074920654
Step: 10710, train/loss: 0.5141000151634216
Step: 10710, train/grad_norm: 5.751458168029785
Step: 10710, train/learning_rate: 3.725606802618131e-05
Step: 10710, train/epoch: 2.5487864017486572
Step: 10720, train/loss: 0.5954999923706055
Step: 10720, train/grad_norm: 7.269959926605225
Step: 10720, train/learning_rate: 3.7244168197503313e-05
Step: 10720, train/epoch: 2.55116605758667
Step: 10730, train/loss: 0.567300021648407
Step: 10730, train/grad_norm: 11.633042335510254
Step: 10730, train/learning_rate: 3.7232272006804124e-05
Step: 10730, train/epoch: 2.5535459518432617
Step: 10740, train/loss: 0.40869998931884766
Step: 10740, train/grad_norm: 10.527508735656738
Step: 10740, train/learning_rate: 3.7220372178126127e-05
Step: 10740, train/epoch: 2.5559258460998535
Step: 10750, train/loss: 0.5289999842643738
Step: 10750, train/grad_norm: 14.066758155822754
Step: 10750, train/learning_rate: 3.720847234944813e-05
Step: 10750, train/epoch: 2.558305501937866
Step: 10760, train/loss: 0.40139999985694885
Step: 10760, train/grad_norm: 8.848276138305664
Step: 10760, train/learning_rate: 3.719657252077013e-05
Step: 10760, train/epoch: 2.560685396194458
Step: 10770, train/loss: 0.43639999628067017
Step: 10770, train/grad_norm: 12.025459289550781
Step: 10770, train/learning_rate: 3.7184672692092136e-05
Step: 10770, train/epoch: 2.56306529045105
Step: 10780, train/loss: 0.3711000084877014
Step: 10780, train/grad_norm: 13.905916213989258
Step: 10780, train/learning_rate: 3.7172776501392946e-05
Step: 10780, train/epoch: 2.5654449462890625
Step: 10790, train/loss: 0.3546999990940094
Step: 10790, train/grad_norm: 11.97909927368164
Step: 10790, train/learning_rate: 3.716087667271495e-05
Step: 10790, train/epoch: 2.5678248405456543
Step: 10800, train/loss: 0.4805000126361847
Step: 10800, train/grad_norm: 7.0932464599609375
Step: 10800, train/learning_rate: 3.714897684403695e-05
Step: 10800, train/epoch: 2.570204734802246
Step: 10810, train/loss: 0.46810001134872437
Step: 10810, train/grad_norm: 3.7265853881835938
Step: 10810, train/learning_rate: 3.7137077015358955e-05
Step: 10810, train/epoch: 2.572584390640259
Step: 10820, train/loss: 0.45820000767707825
Step: 10820, train/grad_norm: 33.198333740234375
Step: 10820, train/learning_rate: 3.712517718668096e-05
Step: 10820, train/epoch: 2.5749642848968506
Step: 10830, train/loss: 0.5468000173568726
Step: 10830, train/grad_norm: 5.451982498168945
Step: 10830, train/learning_rate: 3.711328099598177e-05
Step: 10830, train/epoch: 2.5773441791534424
Step: 10840, train/loss: 0.39980000257492065
Step: 10840, train/grad_norm: 4.553766250610352
Step: 10840, train/learning_rate: 3.710138116730377e-05
Step: 10840, train/epoch: 2.579723834991455
Step: 10850, train/loss: 0.5940999984741211
Step: 10850, train/grad_norm: 31.38707160949707
Step: 10850, train/learning_rate: 3.7089481338625774e-05
Step: 10850, train/epoch: 2.582103729248047
Step: 10860, train/loss: 0.43479999899864197
Step: 10860, train/grad_norm: 6.119320869445801
Step: 10860, train/learning_rate: 3.707758150994778e-05
Step: 10860, train/epoch: 2.5844836235046387
Step: 10870, train/loss: 0.36649999022483826
Step: 10870, train/grad_norm: 13.9572172164917
Step: 10870, train/learning_rate: 3.706568168126978e-05
Step: 10870, train/epoch: 2.5868632793426514
Step: 10880, train/loss: 0.4968000054359436
Step: 10880, train/grad_norm: 19.453641891479492
Step: 10880, train/learning_rate: 3.705378549057059e-05
Step: 10880, train/epoch: 2.589243173599243
Step: 10890, train/loss: 0.5145000219345093
Step: 10890, train/grad_norm: 11.028823852539062
Step: 10890, train/learning_rate: 3.704188566189259e-05
Step: 10890, train/epoch: 2.591623067855835
Step: 10900, train/loss: 0.5357999801635742
Step: 10900, train/grad_norm: 30.726016998291016
Step: 10900, train/learning_rate: 3.7029985833214596e-05
Step: 10900, train/epoch: 2.5940029621124268
Step: 10910, train/loss: 0.5027999877929688
Step: 10910, train/grad_norm: 4.444388389587402
Step: 10910, train/learning_rate: 3.70180860045366e-05
Step: 10910, train/epoch: 2.5963826179504395
Step: 10920, train/loss: 0.32330000400543213
Step: 10920, train/grad_norm: 3.4859554767608643
Step: 10920, train/learning_rate: 3.70061861758586e-05
Step: 10920, train/epoch: 2.5987625122070312
Step: 10930, train/loss: 0.41370001435279846
Step: 10930, train/grad_norm: 5.118203163146973
Step: 10930, train/learning_rate: 3.699428998515941e-05
Step: 10930, train/epoch: 2.601142406463623
Step: 10940, train/loss: 0.4350999891757965
Step: 10940, train/grad_norm: 14.74162483215332
Step: 10940, train/learning_rate: 3.6982390156481415e-05
Step: 10940, train/epoch: 2.6035220623016357
Step: 10950, train/loss: 0.4368000030517578
Step: 10950, train/grad_norm: 15.324158668518066
Step: 10950, train/learning_rate: 3.697049032780342e-05
Step: 10950, train/epoch: 2.6059019565582275
Step: 10960, train/loss: 0.296099990606308
Step: 10960, train/grad_norm: 12.616004943847656
Step: 10960, train/learning_rate: 3.695859049912542e-05
Step: 10960, train/epoch: 2.6082818508148193
Step: 10970, train/loss: 0.3919999897480011
Step: 10970, train/grad_norm: 6.46140193939209
Step: 10970, train/learning_rate: 3.6946690670447424e-05
Step: 10970, train/epoch: 2.610661506652832
Step: 10980, train/loss: 0.4648999869823456
Step: 10980, train/grad_norm: 8.364935874938965
Step: 10980, train/learning_rate: 3.6934794479748234e-05
Step: 10980, train/epoch: 2.613041400909424
Step: 10990, train/loss: 0.5565999746322632
Step: 10990, train/grad_norm: 10.79583740234375
Step: 10990, train/learning_rate: 3.692289465107024e-05
Step: 10990, train/epoch: 2.6154212951660156
Step: 11000, train/loss: 0.4487999975681305
Step: 11000, train/grad_norm: 8.14448070526123
Step: 11000, train/learning_rate: 3.691099482239224e-05
Step: 11000, train/epoch: 2.6178009510040283
Step: 11010, train/loss: 0.4699000120162964
Step: 11010, train/grad_norm: 8.303683280944824
Step: 11010, train/learning_rate: 3.689909499371424e-05
Step: 11010, train/epoch: 2.62018084526062
Step: 11020, train/loss: 0.5533999800682068
Step: 11020, train/grad_norm: 9.254880905151367
Step: 11020, train/learning_rate: 3.6887195165036246e-05
Step: 11020, train/epoch: 2.622560739517212
Step: 11030, train/loss: 0.5231999754905701
Step: 11030, train/grad_norm: 19.21105194091797
Step: 11030, train/learning_rate: 3.6875298974337056e-05
Step: 11030, train/epoch: 2.6249403953552246
Step: 11040, train/loss: 0.45419999957084656
Step: 11040, train/grad_norm: 7.9615020751953125
Step: 11040, train/learning_rate: 3.686339914565906e-05
Step: 11040, train/epoch: 2.6273202896118164
Step: 11050, train/loss: 0.4683000147342682
Step: 11050, train/grad_norm: 7.531891822814941
Step: 11050, train/learning_rate: 3.685149931698106e-05
Step: 11050, train/epoch: 2.629700183868408
Step: 11060, train/loss: 0.33000001311302185
Step: 11060, train/grad_norm: 28.19344139099121
Step: 11060, train/learning_rate: 3.6839599488303065e-05
Step: 11060, train/epoch: 2.632080078125
Step: 11070, train/loss: 0.3790999948978424
Step: 11070, train/grad_norm: 15.426934242248535
Step: 11070, train/learning_rate: 3.682769965962507e-05
Step: 11070, train/epoch: 2.6344597339630127
Step: 11080, train/loss: 0.4246000051498413
Step: 11080, train/grad_norm: 11.268986701965332
Step: 11080, train/learning_rate: 3.681580346892588e-05
Step: 11080, train/epoch: 2.6368396282196045
Step: 11090, train/loss: 0.4496999979019165
Step: 11090, train/grad_norm: 7.055749893188477
Step: 11090, train/learning_rate: 3.680390364024788e-05
Step: 11090, train/epoch: 2.6392195224761963
Step: 11100, train/loss: 0.5128999948501587
Step: 11100, train/grad_norm: 9.4816255569458
Step: 11100, train/learning_rate: 3.6792003811569884e-05
Step: 11100, train/epoch: 2.641599178314209
Step: 11110, train/loss: 0.4970000088214874
Step: 11110, train/grad_norm: 5.558829307556152
Step: 11110, train/learning_rate: 3.678010398289189e-05
Step: 11110, train/epoch: 2.643979072570801
Step: 11120, train/loss: 0.45820000767707825
Step: 11120, train/grad_norm: 5.639481067657471
Step: 11120, train/learning_rate: 3.676820415421389e-05
Step: 11120, train/epoch: 2.6463589668273926
Step: 11130, train/loss: 0.35370001196861267
Step: 11130, train/grad_norm: 6.478368759155273
Step: 11130, train/learning_rate: 3.67563079635147e-05
Step: 11130, train/epoch: 2.6487386226654053
Step: 11140, train/loss: 0.3594000041484833
Step: 11140, train/grad_norm: 8.932796478271484
Step: 11140, train/learning_rate: 3.6744408134836704e-05
Step: 11140, train/epoch: 2.651118516921997
Step: 11150, train/loss: 0.24639999866485596
Step: 11150, train/grad_norm: 3.538458824157715
Step: 11150, train/learning_rate: 3.6732508306158707e-05
Step: 11150, train/epoch: 2.653498411178589
Step: 11160, train/loss: 0.38089999556541443
Step: 11160, train/grad_norm: 20.53131103515625
Step: 11160, train/learning_rate: 3.672060847748071e-05
Step: 11160, train/epoch: 2.6558780670166016
Step: 11170, train/loss: 0.4758000075817108
Step: 11170, train/grad_norm: 5.451107978820801
Step: 11170, train/learning_rate: 3.670870864880271e-05
Step: 11170, train/epoch: 2.6582579612731934
Step: 11180, train/loss: 0.4690000116825104
Step: 11180, train/grad_norm: 10.478926658630371
Step: 11180, train/learning_rate: 3.669681245810352e-05
Step: 11180, train/epoch: 2.660637855529785
Step: 11190, train/loss: 0.4625000059604645
Step: 11190, train/grad_norm: 10.63729190826416
Step: 11190, train/learning_rate: 3.6684912629425526e-05
Step: 11190, train/epoch: 2.663017511367798
Step: 11200, train/loss: 0.43639999628067017
Step: 11200, train/grad_norm: 13.678716659545898
Step: 11200, train/learning_rate: 3.667301280074753e-05
Step: 11200, train/epoch: 2.6653974056243896
Step: 11210, train/loss: 0.2793999910354614
Step: 11210, train/grad_norm: 10.076264381408691
Step: 11210, train/learning_rate: 3.666111297206953e-05
Step: 11210, train/epoch: 2.6677772998809814
Step: 11220, train/loss: 0.44929999113082886
Step: 11220, train/grad_norm: 20.51833152770996
Step: 11220, train/learning_rate: 3.6649213143391535e-05
Step: 11220, train/epoch: 2.670156955718994
Step: 11230, train/loss: 0.35850000381469727
Step: 11230, train/grad_norm: 10.939634323120117
Step: 11230, train/learning_rate: 3.6637316952692345e-05
Step: 11230, train/epoch: 2.672536849975586
Step: 11240, train/loss: 0.3490000069141388
Step: 11240, train/grad_norm: 9.656505584716797
Step: 11240, train/learning_rate: 3.662541712401435e-05
Step: 11240, train/epoch: 2.6749167442321777
Step: 11250, train/loss: 0.3662000000476837
Step: 11250, train/grad_norm: 18.466585159301758
Step: 11250, train/learning_rate: 3.661351729533635e-05
Step: 11250, train/epoch: 2.6772966384887695
Step: 11260, train/loss: 0.42890000343322754
Step: 11260, train/grad_norm: 11.701854705810547
Step: 11260, train/learning_rate: 3.6601617466658354e-05
Step: 11260, train/epoch: 2.6796762943267822
Step: 11270, train/loss: 0.3587000072002411
Step: 11270, train/grad_norm: 11.280447959899902
Step: 11270, train/learning_rate: 3.658971763798036e-05
Step: 11270, train/epoch: 2.682056188583374
Step: 11280, train/loss: 0.6399000287055969
Step: 11280, train/grad_norm: 25.775558471679688
Step: 11280, train/learning_rate: 3.657782144728117e-05
Step: 11280, train/epoch: 2.684436082839966
Step: 11290, train/loss: 0.4706000089645386
Step: 11290, train/grad_norm: 11.341824531555176
Step: 11290, train/learning_rate: 3.656592161860317e-05
Step: 11290, train/epoch: 2.6868157386779785
Step: 11300, train/loss: 0.5260999798774719
Step: 11300, train/grad_norm: 3.854290008544922
Step: 11300, train/learning_rate: 3.655402178992517e-05
Step: 11300, train/epoch: 2.6891956329345703
Step: 11310, train/loss: 0.39910000562667847
Step: 11310, train/grad_norm: 13.054642677307129
Step: 11310, train/learning_rate: 3.6542121961247176e-05
Step: 11310, train/epoch: 2.691575527191162
Step: 11320, train/loss: 0.5611000061035156
Step: 11320, train/grad_norm: 13.20527172088623
Step: 11320, train/learning_rate: 3.653022213256918e-05
Step: 11320, train/epoch: 2.693955183029175
Step: 11330, train/loss: 0.4699000120162964
Step: 11330, train/grad_norm: 3.8820273876190186
Step: 11330, train/learning_rate: 3.651832594186999e-05
Step: 11330, train/epoch: 2.6963350772857666
Step: 11340, train/loss: 0.3871999979019165
Step: 11340, train/grad_norm: 19.26304054260254
Step: 11340, train/learning_rate: 3.650642611319199e-05
Step: 11340, train/epoch: 2.6987149715423584
Step: 11350, train/loss: 0.35409998893737793
Step: 11350, train/grad_norm: 4.361825942993164
Step: 11350, train/learning_rate: 3.6494526284513995e-05
Step: 11350, train/epoch: 2.701094627380371
Step: 11360, train/loss: 0.3490000069141388
Step: 11360, train/grad_norm: 10.878955841064453
Step: 11360, train/learning_rate: 3.6482626455836e-05
Step: 11360, train/epoch: 2.703474521636963
Step: 11370, train/loss: 0.6165000200271606
Step: 11370, train/grad_norm: 7.405054092407227
Step: 11370, train/learning_rate: 3.6470726627158e-05
Step: 11370, train/epoch: 2.7058544158935547
Step: 11380, train/loss: 0.5097000002861023
Step: 11380, train/grad_norm: 20.63054656982422
Step: 11380, train/learning_rate: 3.645883043645881e-05
Step: 11380, train/epoch: 2.7082340717315674
Step: 11390, train/loss: 0.35409998893737793
Step: 11390, train/grad_norm: 20.927183151245117
Step: 11390, train/learning_rate: 3.6446930607780814e-05
Step: 11390, train/epoch: 2.710613965988159
Step: 11400, train/loss: 0.45660001039505005
Step: 11400, train/grad_norm: 26.053844451904297
Step: 11400, train/learning_rate: 3.643503077910282e-05
Step: 11400, train/epoch: 2.712993860244751
Step: 11410, train/loss: 0.373199999332428
Step: 11410, train/grad_norm: 9.246960639953613
Step: 11410, train/learning_rate: 3.642313095042482e-05
Step: 11410, train/epoch: 2.7153735160827637
Step: 11420, train/loss: 0.5113999843597412
Step: 11420, train/grad_norm: 9.280631065368652
Step: 11420, train/learning_rate: 3.641123112174682e-05
Step: 11420, train/epoch: 2.7177534103393555
Step: 11430, train/loss: 0.37459999322891235
Step: 11430, train/grad_norm: 4.4178080558776855
Step: 11430, train/learning_rate: 3.639933493104763e-05
Step: 11430, train/epoch: 2.7201333045959473
Step: 11440, train/loss: 0.3531000018119812
Step: 11440, train/grad_norm: 6.086481094360352
Step: 11440, train/learning_rate: 3.6387435102369636e-05
Step: 11440, train/epoch: 2.722513198852539
Step: 11450, train/loss: 0.4465999901294708
Step: 11450, train/grad_norm: 7.635176658630371
Step: 11450, train/learning_rate: 3.637553527369164e-05
Step: 11450, train/epoch: 2.7248928546905518
Step: 11460, train/loss: 0.4018999934196472
Step: 11460, train/grad_norm: 6.432783603668213
Step: 11460, train/learning_rate: 3.636363544501364e-05
Step: 11460, train/epoch: 2.7272727489471436
Step: 11470, train/loss: 0.5170999765396118
Step: 11470, train/grad_norm: 6.202821254730225
Step: 11470, train/learning_rate: 3.6351735616335645e-05
Step: 11470, train/epoch: 2.7296526432037354
Step: 11480, train/loss: 0.5335999727249146
Step: 11480, train/grad_norm: 12.115056037902832
Step: 11480, train/learning_rate: 3.6339839425636455e-05
Step: 11480, train/epoch: 2.732032299041748
Step: 11490, train/loss: 0.4650999903678894
Step: 11490, train/grad_norm: 16.091665267944336
Step: 11490, train/learning_rate: 3.632793959695846e-05
Step: 11490, train/epoch: 2.73441219329834
Step: 11500, train/loss: 0.3269999921321869
Step: 11500, train/grad_norm: 14.133403778076172
Step: 11500, train/learning_rate: 3.631603976828046e-05
Step: 11500, train/epoch: 2.7367920875549316
Step: 11510, train/loss: 0.3294999897480011
Step: 11510, train/grad_norm: 8.20322322845459
Step: 11510, train/learning_rate: 3.6304139939602464e-05
Step: 11510, train/epoch: 2.7391717433929443
Step: 11520, train/loss: 0.49140000343322754
Step: 11520, train/grad_norm: 10.438481330871582
Step: 11520, train/learning_rate: 3.629224011092447e-05
Step: 11520, train/epoch: 2.741551637649536
Step: 11530, train/loss: 0.3727000057697296
Step: 11530, train/grad_norm: 14.533324241638184
Step: 11530, train/learning_rate: 3.628034392022528e-05
Step: 11530, train/epoch: 2.743931531906128
Step: 11540, train/loss: 0.3926999866962433
Step: 11540, train/grad_norm: 10.800372123718262
Step: 11540, train/learning_rate: 3.626844409154728e-05
Step: 11540, train/epoch: 2.7463111877441406
Step: 11550, train/loss: 0.383899986743927
Step: 11550, train/grad_norm: 16.8052978515625
Step: 11550, train/learning_rate: 3.6256544262869284e-05
Step: 11550, train/epoch: 2.7486910820007324
Step: 11560, train/loss: 0.430400013923645
Step: 11560, train/grad_norm: 5.808787822723389
Step: 11560, train/learning_rate: 3.6244644434191287e-05
Step: 11560, train/epoch: 2.751070976257324
Step: 11570, train/loss: 0.4839000105857849
Step: 11570, train/grad_norm: 5.408308029174805
Step: 11570, train/learning_rate: 3.623274460551329e-05
Step: 11570, train/epoch: 2.753450632095337
Step: 11580, train/loss: 0.4000000059604645
Step: 11580, train/grad_norm: 4.643997669219971
Step: 11580, train/learning_rate: 3.62208484148141e-05
Step: 11580, train/epoch: 2.7558305263519287
Step: 11590, train/loss: 0.5055999755859375
Step: 11590, train/grad_norm: 22.012813568115234
Step: 11590, train/learning_rate: 3.62089485861361e-05
Step: 11590, train/epoch: 2.7582104206085205
Step: 11600, train/loss: 0.41670000553131104
Step: 11600, train/grad_norm: 7.2448649406433105
Step: 11600, train/learning_rate: 3.6197048757458106e-05
Step: 11600, train/epoch: 2.760590076446533
Step: 11610, train/loss: 0.4004000127315521
Step: 11610, train/grad_norm: 11.424383163452148
Step: 11610, train/learning_rate: 3.618514892878011e-05
Step: 11610, train/epoch: 2.762969970703125
Step: 11620, train/loss: 0.4415999948978424
Step: 11620, train/grad_norm: 5.217672824859619
Step: 11620, train/learning_rate: 3.617324910010211e-05
Step: 11620, train/epoch: 2.765349864959717
Step: 11630, train/loss: 0.4697999954223633
Step: 11630, train/grad_norm: 7.82846736907959
Step: 11630, train/learning_rate: 3.616135290940292e-05
Step: 11630, train/epoch: 2.7677297592163086
Step: 11640, train/loss: 0.32899999618530273
Step: 11640, train/grad_norm: 7.676889419555664
Step: 11640, train/learning_rate: 3.6149453080724925e-05
Step: 11640, train/epoch: 2.7701094150543213
Step: 11650, train/loss: 0.385699987411499
Step: 11650, train/grad_norm: 8.138206481933594
Step: 11650, train/learning_rate: 3.613755325204693e-05
Step: 11650, train/epoch: 2.772489309310913
Step: 11660, train/loss: 0.454800009727478
Step: 11660, train/grad_norm: 3.7373151779174805
Step: 11660, train/learning_rate: 3.612565342336893e-05
Step: 11660, train/epoch: 2.774869203567505
Step: 11670, train/loss: 0.3750999867916107
Step: 11670, train/grad_norm: 10.772899627685547
Step: 11670, train/learning_rate: 3.6113753594690934e-05
Step: 11670, train/epoch: 2.7772488594055176
Step: 11680, train/loss: 0.33000001311302185
Step: 11680, train/grad_norm: 12.433269500732422
Step: 11680, train/learning_rate: 3.6101857403991744e-05
Step: 11680, train/epoch: 2.7796287536621094
Step: 11690, train/loss: 0.36959999799728394
Step: 11690, train/grad_norm: 5.074491024017334
Step: 11690, train/learning_rate: 3.608995757531375e-05
Step: 11690, train/epoch: 2.782008647918701
Step: 11700, train/loss: 0.3336000144481659
Step: 11700, train/grad_norm: 23.798643112182617
Step: 11700, train/learning_rate: 3.607805774663575e-05
Step: 11700, train/epoch: 2.784388303756714
Step: 11710, train/loss: 0.3749000132083893
Step: 11710, train/grad_norm: 25.845779418945312
Step: 11710, train/learning_rate: 3.606615791795775e-05
Step: 11710, train/epoch: 2.7867681980133057
Step: 11720, train/loss: 0.445499986410141
Step: 11720, train/grad_norm: 20.390140533447266
Step: 11720, train/learning_rate: 3.6054258089279756e-05
Step: 11720, train/epoch: 2.7891480922698975
Step: 11730, train/loss: 0.46380001306533813
Step: 11730, train/grad_norm: 11.017531394958496
Step: 11730, train/learning_rate: 3.6042361898580566e-05
Step: 11730, train/epoch: 2.79152774810791
Step: 11740, train/loss: 0.5001999735832214
Step: 11740, train/grad_norm: 15.720043182373047
Step: 11740, train/learning_rate: 3.603046206990257e-05
Step: 11740, train/epoch: 2.793907642364502
Step: 11750, train/loss: 0.30219998955726624
Step: 11750, train/grad_norm: 9.765067100524902
Step: 11750, train/learning_rate: 3.601856224122457e-05
Step: 11750, train/epoch: 2.7962875366210938
Step: 11760, train/loss: 0.3862999975681305
Step: 11760, train/grad_norm: 7.856141090393066
Step: 11760, train/learning_rate: 3.6006662412546575e-05
Step: 11760, train/epoch: 2.7986671924591064
Step: 11770, train/loss: 0.36980000138282776
Step: 11770, train/grad_norm: 13.084636688232422
Step: 11770, train/learning_rate: 3.599476258386858e-05
Step: 11770, train/epoch: 2.8010470867156982
Step: 11780, train/loss: 0.33340001106262207
Step: 11780, train/grad_norm: 12.79532527923584
Step: 11780, train/learning_rate: 3.598286639316939e-05
Step: 11780, train/epoch: 2.80342698097229
Step: 11790, train/loss: 0.4927999973297119
Step: 11790, train/grad_norm: 7.579706192016602
Step: 11790, train/learning_rate: 3.597096656449139e-05
Step: 11790, train/epoch: 2.805806875228882
Step: 11800, train/loss: 0.5357999801635742
Step: 11800, train/grad_norm: 9.071405410766602
Step: 11800, train/learning_rate: 3.5959066735813394e-05
Step: 11800, train/epoch: 2.8081865310668945
Step: 11810, train/loss: 0.2540000081062317
Step: 11810, train/grad_norm: 5.740520477294922
Step: 11810, train/learning_rate: 3.59471669071354e-05
Step: 11810, train/epoch: 2.8105664253234863
Step: 11820, train/loss: 0.4007999897003174
Step: 11820, train/grad_norm: 10.445079803466797
Step: 11820, train/learning_rate: 3.593527071643621e-05
Step: 11820, train/epoch: 2.812946319580078
Step: 11830, train/loss: 0.436599999666214
Step: 11830, train/grad_norm: 5.095530033111572
Step: 11830, train/learning_rate: 3.592337088775821e-05
Step: 11830, train/epoch: 2.815325975418091
Step: 11840, train/loss: 0.49619999527931213
Step: 11840, train/grad_norm: 5.472165584564209
Step: 11840, train/learning_rate: 3.591147105908021e-05
Step: 11840, train/epoch: 2.8177058696746826
Step: 11850, train/loss: 0.5368000268936157
Step: 11850, train/grad_norm: 2.3992063999176025
Step: 11850, train/learning_rate: 3.5899571230402216e-05
Step: 11850, train/epoch: 2.8200857639312744
Step: 11860, train/loss: 0.34779998660087585
Step: 11860, train/grad_norm: 8.707478523254395
Step: 11860, train/learning_rate: 3.588767140172422e-05
Step: 11860, train/epoch: 2.822465419769287
Step: 11870, train/loss: 0.33090001344680786
Step: 11870, train/grad_norm: 3.60795521736145
Step: 11870, train/learning_rate: 3.587577521102503e-05
Step: 11870, train/epoch: 2.824845314025879
Step: 11880, train/loss: 0.3725000023841858
Step: 11880, train/grad_norm: 18.770809173583984
Step: 11880, train/learning_rate: 3.586387538234703e-05
Step: 11880, train/epoch: 2.8272252082824707
Step: 11890, train/loss: 0.3253999948501587
Step: 11890, train/grad_norm: 10.579923629760742
Step: 11890, train/learning_rate: 3.5851975553669035e-05
Step: 11890, train/epoch: 2.8296048641204834
Step: 11900, train/loss: 0.41029998660087585
Step: 11900, train/grad_norm: 11.924986839294434
Step: 11900, train/learning_rate: 3.584007572499104e-05
Step: 11900, train/epoch: 2.831984758377075
Step: 11910, train/loss: 0.3474000096321106
Step: 11910, train/grad_norm: 5.99025821685791
Step: 11910, train/learning_rate: 3.582817589631304e-05
Step: 11910, train/epoch: 2.834364652633667
Step: 11920, train/loss: 0.46149998903274536
Step: 11920, train/grad_norm: 8.55666446685791
Step: 11920, train/learning_rate: 3.581627970561385e-05
Step: 11920, train/epoch: 2.8367443084716797
Step: 11930, train/loss: 0.3727000057697296
Step: 11930, train/grad_norm: 3.689223051071167
Step: 11930, train/learning_rate: 3.5804379876935855e-05
Step: 11930, train/epoch: 2.8391242027282715
Step: 11940, train/loss: 0.4447000026702881
Step: 11940, train/grad_norm: 16.64926528930664
Step: 11940, train/learning_rate: 3.579248004825786e-05
Step: 11940, train/epoch: 2.8415040969848633
Step: 11950, train/loss: 0.40139999985694885
Step: 11950, train/grad_norm: 14.974006652832031
Step: 11950, train/learning_rate: 3.578058021957986e-05
Step: 11950, train/epoch: 2.843883752822876
Step: 11960, train/loss: 0.37929999828338623
Step: 11960, train/grad_norm: 10.64100170135498
Step: 11960, train/learning_rate: 3.5768680390901864e-05
Step: 11960, train/epoch: 2.8462636470794678
Step: 11970, train/loss: 0.3797999918460846
Step: 11970, train/grad_norm: 12.507591247558594
Step: 11970, train/learning_rate: 3.5756784200202674e-05
Step: 11970, train/epoch: 2.8486435413360596
Step: 11980, train/loss: 0.4551999866962433
Step: 11980, train/grad_norm: 7.757272720336914
Step: 11980, train/learning_rate: 3.574488437152468e-05
Step: 11980, train/epoch: 2.8510234355926514
Step: 11990, train/loss: 0.5060999989509583
Step: 11990, train/grad_norm: 8.89653205871582
Step: 11990, train/learning_rate: 3.573298454284668e-05
Step: 11990, train/epoch: 2.853403091430664
Step: 12000, train/loss: 0.40709999203681946
Step: 12000, train/grad_norm: 2.213711977005005
Step: 12000, train/learning_rate: 3.572108471416868e-05
Step: 12000, train/epoch: 2.855782985687256
Step: 12010, train/loss: 0.5149000287055969
Step: 12010, train/grad_norm: 8.477401733398438
Step: 12010, train/learning_rate: 3.5709184885490686e-05
Step: 12010, train/epoch: 2.8581628799438477
Step: 12020, train/loss: 0.3400999903678894
Step: 12020, train/grad_norm: 17.334211349487305
Step: 12020, train/learning_rate: 3.5697288694791496e-05
Step: 12020, train/epoch: 2.8605425357818604
Step: 12030, train/loss: 0.47999998927116394
Step: 12030, train/grad_norm: 17.113576889038086
Step: 12030, train/learning_rate: 3.56853888661135e-05
Step: 12030, train/epoch: 2.862922430038452
Step: 12040, train/loss: 0.3955000042915344
Step: 12040, train/grad_norm: 9.780762672424316
Step: 12040, train/learning_rate: 3.56734890374355e-05
Step: 12040, train/epoch: 2.865302324295044
Step: 12050, train/loss: 0.366100013256073
Step: 12050, train/grad_norm: 13.865922927856445
Step: 12050, train/learning_rate: 3.5661589208757505e-05
Step: 12050, train/epoch: 2.8676819801330566
Step: 12060, train/loss: 0.4537000060081482
Step: 12060, train/grad_norm: 16.346595764160156
Step: 12060, train/learning_rate: 3.564968938007951e-05
Step: 12060, train/epoch: 2.8700618743896484
Step: 12070, train/loss: 0.35339999198913574
Step: 12070, train/grad_norm: 3.8927114009857178
Step: 12070, train/learning_rate: 3.563779318938032e-05
Step: 12070, train/epoch: 2.8724417686462402
Step: 12080, train/loss: 0.4724999964237213
Step: 12080, train/grad_norm: 11.325270652770996
Step: 12080, train/learning_rate: 3.562589336070232e-05
Step: 12080, train/epoch: 2.874821424484253
Step: 12090, train/loss: 0.3497999906539917
Step: 12090, train/grad_norm: 2.909942626953125
Step: 12090, train/learning_rate: 3.5613993532024324e-05
Step: 12090, train/epoch: 2.8772013187408447
Step: 12100, train/loss: 0.5030999779701233
Step: 12100, train/grad_norm: 9.051186561584473
Step: 12100, train/learning_rate: 3.560209370334633e-05
Step: 12100, train/epoch: 2.8795812129974365
Step: 12110, train/loss: 0.35179999470710754
Step: 12110, train/grad_norm: 3.771584987640381
Step: 12110, train/learning_rate: 3.559019387466833e-05
Step: 12110, train/epoch: 2.881960868835449
Step: 12120, train/loss: 0.4472000002861023
Step: 12120, train/grad_norm: 10.664796829223633
Step: 12120, train/learning_rate: 3.557829768396914e-05
Step: 12120, train/epoch: 2.884340763092041
Step: 12130, train/loss: 0.375
Step: 12130, train/grad_norm: 10.312522888183594
Step: 12130, train/learning_rate: 3.556639785529114e-05
Step: 12130, train/epoch: 2.886720657348633
Step: 12140, train/loss: 0.34220001101493835
Step: 12140, train/grad_norm: 7.977972507476807
Step: 12140, train/learning_rate: 3.5554498026613146e-05
Step: 12140, train/epoch: 2.8891003131866455
Step: 12150, train/loss: 0.5169000029563904
Step: 12150, train/grad_norm: 5.210363864898682
Step: 12150, train/learning_rate: 3.554259819793515e-05
Step: 12150, train/epoch: 2.8914802074432373
Step: 12160, train/loss: 0.3926999866962433
Step: 12160, train/grad_norm: 7.636013031005859
Step: 12160, train/learning_rate: 3.553069836925715e-05
Step: 12160, train/epoch: 2.893860101699829
Step: 12170, train/loss: 0.4374000132083893
Step: 12170, train/grad_norm: 16.603702545166016
Step: 12170, train/learning_rate: 3.551880217855796e-05
Step: 12170, train/epoch: 2.896239995956421
Step: 12180, train/loss: 0.3253999948501587
Step: 12180, train/grad_norm: 6.924532890319824
Step: 12180, train/learning_rate: 3.5506902349879965e-05
Step: 12180, train/epoch: 2.8986196517944336
Step: 12190, train/loss: 0.3723999857902527
Step: 12190, train/grad_norm: 7.590040683746338
Step: 12190, train/learning_rate: 3.549500252120197e-05
Step: 12190, train/epoch: 2.9009995460510254
Step: 12200, train/loss: 0.42989999055862427
Step: 12200, train/grad_norm: 12.920071601867676
Step: 12200, train/learning_rate: 3.548310269252397e-05
Step: 12200, train/epoch: 2.903379440307617
Step: 12210, train/loss: 0.4343000054359436
Step: 12210, train/grad_norm: 16.968795776367188
Step: 12210, train/learning_rate: 3.5471202863845974e-05
Step: 12210, train/epoch: 2.90575909614563
Step: 12220, train/loss: 0.4323999881744385
Step: 12220, train/grad_norm: 3.3983852863311768
Step: 12220, train/learning_rate: 3.5459306673146784e-05
Step: 12220, train/epoch: 2.9081389904022217
Step: 12230, train/loss: 0.44530001282691956
Step: 12230, train/grad_norm: 7.368040084838867
Step: 12230, train/learning_rate: 3.544740684446879e-05
Step: 12230, train/epoch: 2.9105188846588135
Step: 12240, train/loss: 0.620199978351593
Step: 12240, train/grad_norm: 13.43265151977539
Step: 12240, train/learning_rate: 3.543550701579079e-05
Step: 12240, train/epoch: 2.912898540496826
Step: 12250, train/loss: 0.37299999594688416
Step: 12250, train/grad_norm: 7.108134746551514
Step: 12250, train/learning_rate: 3.542360718711279e-05
Step: 12250, train/epoch: 2.915278434753418
Step: 12260, train/loss: 0.4803999960422516
Step: 12260, train/grad_norm: 13.225805282592773
Step: 12260, train/learning_rate: 3.5411707358434796e-05
Step: 12260, train/epoch: 2.9176583290100098
Step: 12270, train/loss: 0.3772999942302704
Step: 12270, train/grad_norm: 7.046013832092285
Step: 12270, train/learning_rate: 3.5399811167735606e-05
Step: 12270, train/epoch: 2.9200379848480225
Step: 12280, train/loss: 0.44769999384880066
Step: 12280, train/grad_norm: 5.248068332672119
Step: 12280, train/learning_rate: 3.538791133905761e-05
Step: 12280, train/epoch: 2.9224178791046143
Step: 12290, train/loss: 0.45100000500679016
Step: 12290, train/grad_norm: 8.678526878356934
Step: 12290, train/learning_rate: 3.537601151037961e-05
Step: 12290, train/epoch: 2.924797773361206
Step: 12300, train/loss: 0.40130001306533813
Step: 12300, train/grad_norm: 14.74290657043457
Step: 12300, train/learning_rate: 3.5364111681701615e-05
Step: 12300, train/epoch: 2.9271774291992188
Step: 12310, train/loss: 0.43059998750686646
Step: 12310, train/grad_norm: 6.8999176025390625
Step: 12310, train/learning_rate: 3.535221185302362e-05
Step: 12310, train/epoch: 2.9295573234558105
Step: 12320, train/loss: 0.35010001063346863
Step: 12320, train/grad_norm: 9.550768852233887
Step: 12320, train/learning_rate: 3.534031566232443e-05
Step: 12320, train/epoch: 2.9319372177124023
Step: 12330, train/loss: 0.5397999882698059
Step: 12330, train/grad_norm: 5.959518909454346
Step: 12330, train/learning_rate: 3.532841583364643e-05
Step: 12330, train/epoch: 2.934316873550415
Step: 12340, train/loss: 0.4544000029563904
Step: 12340, train/grad_norm: 2.9395785331726074
Step: 12340, train/learning_rate: 3.5316516004968435e-05
Step: 12340, train/epoch: 2.936696767807007
Step: 12350, train/loss: 0.3833000063896179
Step: 12350, train/grad_norm: 20.03180694580078
Step: 12350, train/learning_rate: 3.530461617629044e-05
Step: 12350, train/epoch: 2.9390766620635986
Step: 12360, train/loss: 0.42640000581741333
Step: 12360, train/grad_norm: 11.608954429626465
Step: 12360, train/learning_rate: 3.529271634761244e-05
Step: 12360, train/epoch: 2.9414565563201904
Step: 12370, train/loss: 0.36399999260902405
Step: 12370, train/grad_norm: 16.695423126220703
Step: 12370, train/learning_rate: 3.528082015691325e-05
Step: 12370, train/epoch: 2.943836212158203
Step: 12380, train/loss: 0.46459999680519104
Step: 12380, train/grad_norm: 12.267987251281738
Step: 12380, train/learning_rate: 3.5268920328235254e-05
Step: 12380, train/epoch: 2.946216106414795
Step: 12390, train/loss: 0.3179999887943268
Step: 12390, train/grad_norm: 8.64165210723877
Step: 12390, train/learning_rate: 3.525702049955726e-05
Step: 12390, train/epoch: 2.9485960006713867
Step: 12400, train/loss: 0.499099999666214
Step: 12400, train/grad_norm: 24.154096603393555
Step: 12400, train/learning_rate: 3.524512067087926e-05
Step: 12400, train/epoch: 2.9509756565093994
Step: 12410, train/loss: 0.5094000101089478
Step: 12410, train/grad_norm: 8.83832836151123
Step: 12410, train/learning_rate: 3.523322084220126e-05
Step: 12410, train/epoch: 2.953355550765991
Step: 12420, train/loss: 0.2768999934196472
Step: 12420, train/grad_norm: 5.8037428855896
Step: 12420, train/learning_rate: 3.522132465150207e-05
Step: 12420, train/epoch: 2.955735445022583
Step: 12430, train/loss: 0.4154999852180481
Step: 12430, train/grad_norm: 3.9911842346191406
Step: 12430, train/learning_rate: 3.5209424822824076e-05
Step: 12430, train/epoch: 2.9581151008605957
Step: 12440, train/loss: 0.4433000087738037
Step: 12440, train/grad_norm: 10.102088928222656
Step: 12440, train/learning_rate: 3.519752499414608e-05
Step: 12440, train/epoch: 2.9604949951171875
Step: 12450, train/loss: 0.5698000192642212
Step: 12450, train/grad_norm: 33.2943229675293
Step: 12450, train/learning_rate: 3.518562516546808e-05
Step: 12450, train/epoch: 2.9628748893737793
Step: 12460, train/loss: 0.4171000123023987
Step: 12460, train/grad_norm: 17.233905792236328
Step: 12460, train/learning_rate: 3.5173725336790085e-05
Step: 12460, train/epoch: 2.965254545211792
Step: 12470, train/loss: 0.4683000147342682
Step: 12470, train/grad_norm: 9.796843528747559
Step: 12470, train/learning_rate: 3.5161829146090895e-05
Step: 12470, train/epoch: 2.967634439468384
Step: 12480, train/loss: 0.22040000557899475
Step: 12480, train/grad_norm: 5.305729389190674
Step: 12480, train/learning_rate: 3.51499293174129e-05
Step: 12480, train/epoch: 2.9700143337249756
Step: 12490, train/loss: 0.4708999991416931
Step: 12490, train/grad_norm: 8.864507675170898
Step: 12490, train/learning_rate: 3.51380294887349e-05
Step: 12490, train/epoch: 2.9723939895629883
Step: 12500, train/loss: 0.42100000381469727
Step: 12500, train/grad_norm: 19.62387466430664
Step: 12500, train/learning_rate: 3.5126129660056904e-05
Step: 12500, train/epoch: 2.97477388381958
Step: 12510, train/loss: 0.5439000129699707
Step: 12510, train/grad_norm: 12.009684562683105
Step: 12510, train/learning_rate: 3.511422983137891e-05
Step: 12510, train/epoch: 2.977153778076172
Step: 12520, train/loss: 0.3646000027656555
Step: 12520, train/grad_norm: 5.038902282714844
Step: 12520, train/learning_rate: 3.510233364067972e-05
Step: 12520, train/epoch: 2.9795336723327637
Step: 12530, train/loss: 0.4756999909877777
Step: 12530, train/grad_norm: 7.488260746002197
Step: 12530, train/learning_rate: 3.509043381200172e-05
Step: 12530, train/epoch: 2.9819133281707764
Step: 12540, train/loss: 0.3718999922275543
Step: 12540, train/grad_norm: 16.136768341064453
Step: 12540, train/learning_rate: 3.507853398332372e-05
Step: 12540, train/epoch: 2.984293222427368
Step: 12550, train/loss: 0.44040000438690186
Step: 12550, train/grad_norm: 9.183077812194824
Step: 12550, train/learning_rate: 3.5066634154645726e-05
Step: 12550, train/epoch: 2.98667311668396
Step: 12560, train/loss: 0.5860000252723694
Step: 12560, train/grad_norm: 19.397497177124023
Step: 12560, train/learning_rate: 3.505473432596773e-05
Step: 12560, train/epoch: 2.9890527725219727
Step: 12570, train/loss: 0.5523999929428101
Step: 12570, train/grad_norm: 18.861665725708008
Step: 12570, train/learning_rate: 3.504283813526854e-05
Step: 12570, train/epoch: 2.9914326667785645
Step: 12580, train/loss: 0.5037999749183655
Step: 12580, train/grad_norm: 9.112037658691406
Step: 12580, train/learning_rate: 3.503093830659054e-05
Step: 12580, train/epoch: 2.9938125610351562
Step: 12590, train/loss: 0.4465000033378601
Step: 12590, train/grad_norm: 5.375755310058594
Step: 12590, train/learning_rate: 3.5019038477912545e-05
Step: 12590, train/epoch: 2.996192216873169
Step: 12600, train/loss: 0.4909000098705292
Step: 12600, train/grad_norm: 12.169236183166504
Step: 12600, train/learning_rate: 3.500713864923455e-05
Step: 12600, train/epoch: 2.9985721111297607
Step: 12606, eval/loss: 1.2146390676498413
Step: 12606, eval/accuracy: 0.5253366827964783
Step: 12606, eval/f1: 0.49775412678718567
Step: 12606, eval/runtime: 54.8307991027832
Step: 12606, eval/samples_per_second: 131.3679962158203
Step: 12606, eval/steps_per_second: 16.43199920654297
Step: 12606, train/epoch: 3.0
Step: 12610, train/loss: 0.5394999980926514
Step: 12610, train/grad_norm: 6.911758899688721
Step: 12610, train/learning_rate: 3.499523882055655e-05
Step: 12610, train/epoch: 3.0009520053863525
Step: 12620, train/loss: 0.4796999990940094
Step: 12620, train/grad_norm: 21.241899490356445
Step: 12620, train/learning_rate: 3.498334262985736e-05
Step: 12620, train/epoch: 3.0033316612243652
Step: 12630, train/loss: 0.45579999685287476
Step: 12630, train/grad_norm: 16.730541229248047
Step: 12630, train/learning_rate: 3.4971442801179364e-05
Step: 12630, train/epoch: 3.005711555480957
Step: 12640, train/loss: 0.5871000289916992
Step: 12640, train/grad_norm: 9.055696487426758
Step: 12640, train/learning_rate: 3.495954297250137e-05
Step: 12640, train/epoch: 3.008091449737549
Step: 12650, train/loss: 0.4900999963283539
Step: 12650, train/grad_norm: 15.999635696411133
Step: 12650, train/learning_rate: 3.494764314382337e-05
Step: 12650, train/epoch: 3.0104711055755615
Step: 12660, train/loss: 0.4767000079154968
Step: 12660, train/grad_norm: 19.00660514831543
Step: 12660, train/learning_rate: 3.493574331514537e-05
Step: 12660, train/epoch: 3.0128509998321533
Step: 12670, train/loss: 0.27959999442100525
Step: 12670, train/grad_norm: 8.525086402893066
Step: 12670, train/learning_rate: 3.4923847124446183e-05
Step: 12670, train/epoch: 3.015230894088745
Step: 12680, train/loss: 0.3285999894142151
Step: 12680, train/grad_norm: 11.046156883239746
Step: 12680, train/learning_rate: 3.4911947295768186e-05
Step: 12680, train/epoch: 3.017610549926758
Step: 12690, train/loss: 0.46059998869895935
Step: 12690, train/grad_norm: 4.3037614822387695
Step: 12690, train/learning_rate: 3.490004746709019e-05
Step: 12690, train/epoch: 3.0199904441833496
Step: 12700, train/loss: 0.46560001373291016
Step: 12700, train/grad_norm: 7.242622375488281
Step: 12700, train/learning_rate: 3.488814763841219e-05
Step: 12700, train/epoch: 3.0223703384399414
Step: 12710, train/loss: 0.4399000108242035
Step: 12710, train/grad_norm: 4.071704864501953
Step: 12710, train/learning_rate: 3.4876247809734195e-05
Step: 12710, train/epoch: 3.024750232696533
Step: 12720, train/loss: 0.49219998717308044
Step: 12720, train/grad_norm: 11.447729110717773
Step: 12720, train/learning_rate: 3.4864351619035006e-05
Step: 12720, train/epoch: 3.027129888534546
Step: 12730, train/loss: 0.4408000111579895
Step: 12730, train/grad_norm: 8.181187629699707
Step: 12730, train/learning_rate: 3.485245179035701e-05
Step: 12730, train/epoch: 3.0295097827911377
Step: 12740, train/loss: 0.3140000104904175
Step: 12740, train/grad_norm: 13.020021438598633
Step: 12740, train/learning_rate: 3.484055196167901e-05
Step: 12740, train/epoch: 3.0318896770477295
Step: 12750, train/loss: 0.5611000061035156
Step: 12750, train/grad_norm: 14.000051498413086
Step: 12750, train/learning_rate: 3.4828652133001015e-05
Step: 12750, train/epoch: 3.034269332885742
Step: 12760, train/loss: 0.3905999958515167
Step: 12760, train/grad_norm: 10.379448890686035
Step: 12760, train/learning_rate: 3.481675230432302e-05
Step: 12760, train/epoch: 3.036649227142334
Step: 12770, train/loss: 0.3504999876022339
Step: 12770, train/grad_norm: 10.276694297790527
Step: 12770, train/learning_rate: 3.480485611362383e-05
Step: 12770, train/epoch: 3.039029121398926
Step: 12780, train/loss: 0.41519999504089355
Step: 12780, train/grad_norm: 10.123516082763672
Step: 12780, train/learning_rate: 3.479295628494583e-05
Step: 12780, train/epoch: 3.0414087772369385
Step: 12790, train/loss: 0.5383999943733215
Step: 12790, train/grad_norm: 10.438426971435547
Step: 12790, train/learning_rate: 3.4781056456267834e-05
Step: 12790, train/epoch: 3.0437886714935303
Step: 12800, train/loss: 0.3248000144958496
Step: 12800, train/grad_norm: 5.28727388381958
Step: 12800, train/learning_rate: 3.476915662758984e-05
Step: 12800, train/epoch: 3.046168565750122
Step: 12810, train/loss: 0.35260000824928284
Step: 12810, train/grad_norm: 11.721096992492676
Step: 12810, train/learning_rate: 3.475725679891184e-05
Step: 12810, train/epoch: 3.0485482215881348
Step: 12820, train/loss: 0.290800005197525
Step: 12820, train/grad_norm: 11.085836410522461
Step: 12820, train/learning_rate: 3.474536060821265e-05
Step: 12820, train/epoch: 3.0509281158447266
Step: 12830, train/loss: 0.39169999957084656
Step: 12830, train/grad_norm: 34.95515060424805
Step: 12830, train/learning_rate: 3.473346077953465e-05
Step: 12830, train/epoch: 3.0533080101013184
Step: 12840, train/loss: 0.45570001006126404
Step: 12840, train/grad_norm: 6.459038734436035
Step: 12840, train/learning_rate: 3.4721560950856656e-05
Step: 12840, train/epoch: 3.055687665939331
Step: 12850, train/loss: 0.33340001106262207
Step: 12850, train/grad_norm: 6.543985366821289
Step: 12850, train/learning_rate: 3.470966112217866e-05
Step: 12850, train/epoch: 3.058067560195923
Step: 12860, train/loss: 0.2937000095844269
Step: 12860, train/grad_norm: 15.20750904083252
Step: 12860, train/learning_rate: 3.469776129350066e-05
Step: 12860, train/epoch: 3.0604474544525146
Step: 12870, train/loss: 0.6399999856948853
Step: 12870, train/grad_norm: 2.9573802947998047
Step: 12870, train/learning_rate: 3.468586510280147e-05
Step: 12870, train/epoch: 3.0628271102905273
Step: 12880, train/loss: 0.5449000000953674
Step: 12880, train/grad_norm: 12.863567352294922
Step: 12880, train/learning_rate: 3.4673965274123475e-05
Step: 12880, train/epoch: 3.065207004547119
Step: 12890, train/loss: 0.4131999909877777
Step: 12890, train/grad_norm: 6.182780742645264
Step: 12890, train/learning_rate: 3.466206544544548e-05
Step: 12890, train/epoch: 3.067586898803711
Step: 12900, train/loss: 0.54830002784729
Step: 12900, train/grad_norm: 8.530960083007812
Step: 12900, train/learning_rate: 3.465016561676748e-05
Step: 12900, train/epoch: 3.0699667930603027
Step: 12910, train/loss: 0.41499999165534973
Step: 12910, train/grad_norm: 6.916631698608398
Step: 12910, train/learning_rate: 3.4638265788089484e-05
Step: 12910, train/epoch: 3.0723464488983154
Step: 12920, train/loss: 0.3521000146865845
Step: 12920, train/grad_norm: 3.3548223972320557
Step: 12920, train/learning_rate: 3.4626369597390294e-05
Step: 12920, train/epoch: 3.0747263431549072
Step: 12930, train/loss: 0.5321000218391418
Step: 12930, train/grad_norm: 4.82321834564209
Step: 12930, train/learning_rate: 3.46144697687123e-05
Step: 12930, train/epoch: 3.077106237411499
Step: 12940, train/loss: 0.4530999958515167
Step: 12940, train/grad_norm: 7.120521068572998
Step: 12940, train/learning_rate: 3.46025699400343e-05
Step: 12940, train/epoch: 3.0794858932495117
Step: 12950, train/loss: 0.3601999878883362
Step: 12950, train/grad_norm: 5.282685279846191
Step: 12950, train/learning_rate: 3.45906701113563e-05
Step: 12950, train/epoch: 3.0818657875061035
Step: 12960, train/loss: 0.34940001368522644
Step: 12960, train/grad_norm: 7.401315212249756
Step: 12960, train/learning_rate: 3.4578770282678306e-05
Step: 12960, train/epoch: 3.0842456817626953
Step: 12970, train/loss: 0.48840001225471497
Step: 12970, train/grad_norm: 7.2805328369140625
Step: 12970, train/learning_rate: 3.4566874091979116e-05
Step: 12970, train/epoch: 3.086625337600708
Step: 12980, train/loss: 0.35670000314712524
Step: 12980, train/grad_norm: 6.150058746337891
Step: 12980, train/learning_rate: 3.455497426330112e-05
Step: 12980, train/epoch: 3.0890052318573
Step: 12990, train/loss: 0.34139999747276306
Step: 12990, train/grad_norm: 22.24629783630371
Step: 12990, train/learning_rate: 3.454307443462312e-05
Step: 12990, train/epoch: 3.0913851261138916
Step: 13000, train/loss: 0.3961000144481659
Step: 13000, train/grad_norm: 20.612125396728516
Step: 13000, train/learning_rate: 3.4531174605945125e-05
Step: 13000, train/epoch: 3.0937647819519043
Step: 13010, train/loss: 0.296099990606308
Step: 13010, train/grad_norm: 5.818615913391113
Step: 13010, train/learning_rate: 3.451927477726713e-05
Step: 13010, train/epoch: 3.096144676208496
Step: 13020, train/loss: 0.5033000111579895
Step: 13020, train/grad_norm: 6.89642858505249
Step: 13020, train/learning_rate: 3.450737858656794e-05
Step: 13020, train/epoch: 3.098524570465088
Step: 13030, train/loss: 0.3790000081062317
Step: 13030, train/grad_norm: 12.42566967010498
Step: 13030, train/learning_rate: 3.449547875788994e-05
Step: 13030, train/epoch: 3.1009042263031006
Step: 13040, train/loss: 0.44609999656677246
Step: 13040, train/grad_norm: 7.146290302276611
Step: 13040, train/learning_rate: 3.4483578929211944e-05
Step: 13040, train/epoch: 3.1032841205596924
Step: 13050, train/loss: 0.48750001192092896
Step: 13050, train/grad_norm: 6.880812644958496
Step: 13050, train/learning_rate: 3.447167910053395e-05
Step: 13050, train/epoch: 3.105664014816284
Step: 13060, train/loss: 0.5313000082969666
Step: 13060, train/grad_norm: 15.94626522064209
Step: 13060, train/learning_rate: 3.445977927185595e-05
Step: 13060, train/epoch: 3.108043670654297
Step: 13070, train/loss: 0.36500000953674316
Step: 13070, train/grad_norm: 21.60504150390625
Step: 13070, train/learning_rate: 3.444788308115676e-05
Step: 13070, train/epoch: 3.1104235649108887
Step: 13080, train/loss: 0.3215999901294708
Step: 13080, train/grad_norm: 14.829103469848633
Step: 13080, train/learning_rate: 3.4435983252478763e-05
Step: 13080, train/epoch: 3.1128034591674805
Step: 13090, train/loss: 0.3046000003814697
Step: 13090, train/grad_norm: 12.165255546569824
Step: 13090, train/learning_rate: 3.4424083423800766e-05
Step: 13090, train/epoch: 3.1151833534240723
Step: 13100, train/loss: 0.4034000039100647
Step: 13100, train/grad_norm: 11.3916654586792
Step: 13100, train/learning_rate: 3.441218359512277e-05
Step: 13100, train/epoch: 3.117563009262085
Step: 13110, train/loss: 0.4325999915599823
Step: 13110, train/grad_norm: 17.242107391357422
Step: 13110, train/learning_rate: 3.440028376644477e-05
Step: 13110, train/epoch: 3.1199429035186768
Step: 13120, train/loss: 0.365200012922287
Step: 13120, train/grad_norm: 14.930532455444336
Step: 13120, train/learning_rate: 3.438838757574558e-05
Step: 13120, train/epoch: 3.1223227977752686
Step: 13130, train/loss: 0.5095999836921692
Step: 13130, train/grad_norm: 9.216848373413086
Step: 13130, train/learning_rate: 3.4376487747067586e-05
Step: 13130, train/epoch: 3.1247024536132812
Step: 13140, train/loss: 0.3776000142097473
Step: 13140, train/grad_norm: 17.642568588256836
Step: 13140, train/learning_rate: 3.436458791838959e-05
Step: 13140, train/epoch: 3.127082347869873
Step: 13150, train/loss: 0.4796000123023987
Step: 13150, train/grad_norm: 15.795750617980957
Step: 13150, train/learning_rate: 3.435268808971159e-05
Step: 13150, train/epoch: 3.129462242126465
Step: 13160, train/loss: 0.4162999987602234
Step: 13160, train/grad_norm: 5.143819808959961
Step: 13160, train/learning_rate: 3.43407918990124e-05
Step: 13160, train/epoch: 3.1318418979644775
Step: 13170, train/loss: 0.4291999936103821
Step: 13170, train/grad_norm: 10.231619834899902
Step: 13170, train/learning_rate: 3.4328892070334405e-05
Step: 13170, train/epoch: 3.1342217922210693
Step: 13180, train/loss: 0.3427000045776367
Step: 13180, train/grad_norm: 3.584360122680664
Step: 13180, train/learning_rate: 3.431699224165641e-05
Step: 13180, train/epoch: 3.136601686477661
Step: 13190, train/loss: 0.46000000834465027
Step: 13190, train/grad_norm: 4.938447952270508
Step: 13190, train/learning_rate: 3.430509241297841e-05
Step: 13190, train/epoch: 3.138981342315674
Step: 13200, train/loss: 0.40619999170303345
Step: 13200, train/grad_norm: 13.75136947631836
Step: 13200, train/learning_rate: 3.4293192584300414e-05
Step: 13200, train/epoch: 3.1413612365722656
Step: 13210, train/loss: 0.4722999930381775
Step: 13210, train/grad_norm: 7.625729560852051
Step: 13210, train/learning_rate: 3.4281296393601224e-05
Step: 13210, train/epoch: 3.1437411308288574
Step: 13220, train/loss: 0.37369999289512634
Step: 13220, train/grad_norm: 14.173792839050293
Step: 13220, train/learning_rate: 3.426939656492323e-05
Step: 13220, train/epoch: 3.14612078666687
Step: 13230, train/loss: 0.5224000215530396
Step: 13230, train/grad_norm: 21.64673614501953
Step: 13230, train/learning_rate: 3.425749673624523e-05
Step: 13230, train/epoch: 3.148500680923462
Step: 13240, train/loss: 0.40230000019073486
Step: 13240, train/grad_norm: 19.646299362182617
Step: 13240, train/learning_rate: 3.424559690756723e-05
Step: 13240, train/epoch: 3.1508805751800537
Step: 13250, train/loss: 0.33640000224113464
Step: 13250, train/grad_norm: 11.150506019592285
Step: 13250, train/learning_rate: 3.4233697078889236e-05
Step: 13250, train/epoch: 3.1532604694366455
Step: 13260, train/loss: 0.37209999561309814
Step: 13260, train/grad_norm: 4.916648864746094
Step: 13260, train/learning_rate: 3.4221800888190046e-05
Step: 13260, train/epoch: 3.155640125274658
Step: 13270, train/loss: 0.3418999910354614
Step: 13270, train/grad_norm: 4.170744895935059
Step: 13270, train/learning_rate: 3.420990105951205e-05
Step: 13270, train/epoch: 3.15802001953125
Step: 13280, train/loss: 0.32499998807907104
Step: 13280, train/grad_norm: 1.8337842226028442
Step: 13280, train/learning_rate: 3.419800123083405e-05
Step: 13280, train/epoch: 3.160399913787842
Step: 13290, train/loss: 0.4056999981403351
Step: 13290, train/grad_norm: 7.347733497619629
Step: 13290, train/learning_rate: 3.4186101402156055e-05
Step: 13290, train/epoch: 3.1627795696258545
Step: 13300, train/loss: 0.4641999900341034
Step: 13300, train/grad_norm: 10.616205215454102
Step: 13300, train/learning_rate: 3.417420157347806e-05
Step: 13300, train/epoch: 3.1651594638824463
Step: 13310, train/loss: 0.4320000112056732
Step: 13310, train/grad_norm: 11.598608016967773
Step: 13310, train/learning_rate: 3.416230538277887e-05
Step: 13310, train/epoch: 3.167539358139038
Step: 13320, train/loss: 0.4745999872684479
Step: 13320, train/grad_norm: 8.059739112854004
Step: 13320, train/learning_rate: 3.415040555410087e-05
Step: 13320, train/epoch: 3.169919013977051
Step: 13330, train/loss: 0.4357999861240387
Step: 13330, train/grad_norm: 7.694551467895508
Step: 13330, train/learning_rate: 3.4138505725422874e-05
Step: 13330, train/epoch: 3.1722989082336426
Step: 13340, train/loss: 0.4074999988079071
Step: 13340, train/grad_norm: 8.426621437072754
Step: 13340, train/learning_rate: 3.412660589674488e-05
Step: 13340, train/epoch: 3.1746788024902344
Step: 13350, train/loss: 0.5206999778747559
Step: 13350, train/grad_norm: 10.259998321533203
Step: 13350, train/learning_rate: 3.411470606806688e-05
Step: 13350, train/epoch: 3.177058458328247
Step: 13360, train/loss: 0.3395000100135803
Step: 13360, train/grad_norm: 15.047810554504395
Step: 13360, train/learning_rate: 3.410280987736769e-05
Step: 13360, train/epoch: 3.179438352584839
Step: 13370, train/loss: 0.41670000553131104
Step: 13370, train/grad_norm: 4.138742446899414
Step: 13370, train/learning_rate: 3.409091004868969e-05
Step: 13370, train/epoch: 3.1818182468414307
Step: 13380, train/loss: 0.2538999915122986
Step: 13380, train/grad_norm: 11.208248138427734
Step: 13380, train/learning_rate: 3.4079010220011696e-05
Step: 13380, train/epoch: 3.1841979026794434
Step: 13390, train/loss: 0.4203000068664551
Step: 13390, train/grad_norm: 15.117900848388672
Step: 13390, train/learning_rate: 3.40671103913337e-05
Step: 13390, train/epoch: 3.186577796936035
Step: 13400, train/loss: 0.35510000586509705
Step: 13400, train/grad_norm: 9.789693832397461
Step: 13400, train/learning_rate: 3.40552105626557e-05
Step: 13400, train/epoch: 3.188957691192627
Step: 13410, train/loss: 0.4489000141620636
Step: 13410, train/grad_norm: 5.105688095092773
Step: 13410, train/learning_rate: 3.404331437195651e-05
Step: 13410, train/epoch: 3.1913373470306396
Step: 13420, train/loss: 0.3174999952316284
Step: 13420, train/grad_norm: 12.549854278564453
Step: 13420, train/learning_rate: 3.4031414543278515e-05
Step: 13420, train/epoch: 3.1937172412872314
Step: 13430, train/loss: 0.4041999876499176
Step: 13430, train/grad_norm: 11.563802719116211
Step: 13430, train/learning_rate: 3.401951471460052e-05
Step: 13430, train/epoch: 3.1960971355438232
Step: 13440, train/loss: 0.548799991607666
Step: 13440, train/grad_norm: 21.896127700805664
Step: 13440, train/learning_rate: 3.400761488592252e-05
Step: 13440, train/epoch: 3.198477029800415
Step: 13450, train/loss: 0.47369998693466187
Step: 13450, train/grad_norm: 13.369842529296875
Step: 13450, train/learning_rate: 3.3995715057244524e-05
Step: 13450, train/epoch: 3.2008566856384277
Step: 13460, train/loss: 0.3564000129699707
Step: 13460, train/grad_norm: 11.320286750793457
Step: 13460, train/learning_rate: 3.3983818866545334e-05
Step: 13460, train/epoch: 3.2032365798950195
Step: 13470, train/loss: 0.43630000948905945
Step: 13470, train/grad_norm: 10.854331016540527
Step: 13470, train/learning_rate: 3.397191903786734e-05
Step: 13470, train/epoch: 3.2056164741516113
Step: 13480, train/loss: 0.42179998755455017
Step: 13480, train/grad_norm: 8.973139762878418
Step: 13480, train/learning_rate: 3.396001920918934e-05
Step: 13480, train/epoch: 3.207996129989624
Step: 13490, train/loss: 0.5430999994277954
Step: 13490, train/grad_norm: 11.709797859191895
Step: 13490, train/learning_rate: 3.3948119380511343e-05
Step: 13490, train/epoch: 3.210376024246216
Step: 13500, train/loss: 0.4203000068664551
Step: 13500, train/grad_norm: 15.201913833618164
Step: 13500, train/learning_rate: 3.3936219551833346e-05
Step: 13500, train/epoch: 3.2127559185028076
Step: 13510, train/loss: 0.54830002784729
Step: 13510, train/grad_norm: 7.003166198730469
Step: 13510, train/learning_rate: 3.3924323361134157e-05
Step: 13510, train/epoch: 3.2151355743408203
Step: 13520, train/loss: 0.351500004529953
Step: 13520, train/grad_norm: 7.97547721862793
Step: 13520, train/learning_rate: 3.391242353245616e-05
Step: 13520, train/epoch: 3.217515468597412
Step: 13530, train/loss: 0.36320000886917114
Step: 13530, train/grad_norm: 15.886270523071289
Step: 13530, train/learning_rate: 3.390052370377816e-05
Step: 13530, train/epoch: 3.219895362854004
Step: 13540, train/loss: 0.45680001378059387
Step: 13540, train/grad_norm: 12.270752906799316
Step: 13540, train/learning_rate: 3.3888623875100166e-05
Step: 13540, train/epoch: 3.2222750186920166
Step: 13550, train/loss: 0.3303000032901764
Step: 13550, train/grad_norm: 7.793539524078369
Step: 13550, train/learning_rate: 3.387672404642217e-05
Step: 13550, train/epoch: 3.2246549129486084
Step: 13560, train/loss: 0.5220999717712402
Step: 13560, train/grad_norm: 8.51756477355957
Step: 13560, train/learning_rate: 3.386482785572298e-05
Step: 13560, train/epoch: 3.2270348072052
Step: 13570, train/loss: 0.3833000063896179
Step: 13570, train/grad_norm: 8.3198823928833
Step: 13570, train/learning_rate: 3.385292802704498e-05
Step: 13570, train/epoch: 3.229414463043213
Step: 13580, train/loss: 0.37299999594688416
Step: 13580, train/grad_norm: 3.8871819972991943
Step: 13580, train/learning_rate: 3.3841028198366985e-05
Step: 13580, train/epoch: 3.2317943572998047
Step: 13590, train/loss: 0.4616999924182892
Step: 13590, train/grad_norm: 9.432726860046387
Step: 13590, train/learning_rate: 3.382912836968899e-05
Step: 13590, train/epoch: 3.2341742515563965
Step: 13600, train/loss: 0.47929999232292175
Step: 13600, train/grad_norm: 10.015506744384766
Step: 13600, train/learning_rate: 3.381722854101099e-05
Step: 13600, train/epoch: 3.236553907394409
Step: 13610, train/loss: 0.3723999857902527
Step: 13610, train/grad_norm: 3.118252992630005
Step: 13610, train/learning_rate: 3.38053323503118e-05
Step: 13610, train/epoch: 3.238933801651001
Step: 13620, train/loss: 0.45399999618530273
Step: 13620, train/grad_norm: 8.20930004119873
Step: 13620, train/learning_rate: 3.3793432521633804e-05
Step: 13620, train/epoch: 3.2413136959075928
Step: 13630, train/loss: 0.4187999963760376
Step: 13630, train/grad_norm: 11.21889877319336
Step: 13630, train/learning_rate: 3.378153269295581e-05
Step: 13630, train/epoch: 3.2436935901641846
Step: 13640, train/loss: 0.460999995470047
Step: 13640, train/grad_norm: 5.619154930114746
Step: 13640, train/learning_rate: 3.376963286427781e-05
Step: 13640, train/epoch: 3.2460732460021973
Step: 13650, train/loss: 0.2858999967575073
Step: 13650, train/grad_norm: 13.441669464111328
Step: 13650, train/learning_rate: 3.375773303559981e-05
Step: 13650, train/epoch: 3.248453140258789
Step: 13660, train/loss: 0.3538999855518341
Step: 13660, train/grad_norm: 7.042154788970947
Step: 13660, train/learning_rate: 3.374583684490062e-05
Step: 13660, train/epoch: 3.250833034515381
Step: 13670, train/loss: 0.47999998927116394
Step: 13670, train/grad_norm: 15.827354431152344
Step: 13670, train/learning_rate: 3.3733937016222626e-05
Step: 13670, train/epoch: 3.2532126903533936
Step: 13680, train/loss: 0.43160000443458557
Step: 13680, train/grad_norm: 17.909549713134766
Step: 13680, train/learning_rate: 3.372203718754463e-05
Step: 13680, train/epoch: 3.2555925846099854
Step: 13690, train/loss: 0.583899974822998
Step: 13690, train/grad_norm: 49.16973876953125
Step: 13690, train/learning_rate: 3.371013735886663e-05
Step: 13690, train/epoch: 3.257972478866577
Step: 13700, train/loss: 0.4943999946117401
Step: 13700, train/grad_norm: 7.331001281738281
Step: 13700, train/learning_rate: 3.3698237530188635e-05
Step: 13700, train/epoch: 3.26035213470459
Step: 13710, train/loss: 0.6628000140190125
Step: 13710, train/grad_norm: 1.6350337266921997
Step: 13710, train/learning_rate: 3.3686341339489445e-05
Step: 13710, train/epoch: 3.2627320289611816
Step: 13720, train/loss: 0.5687999725341797
Step: 13720, train/grad_norm: 18.860797882080078
Step: 13720, train/learning_rate: 3.367444151081145e-05
Step: 13720, train/epoch: 3.2651119232177734
Step: 13730, train/loss: 0.4246000051498413
Step: 13730, train/grad_norm: 7.123854160308838
Step: 13730, train/learning_rate: 3.366254168213345e-05
Step: 13730, train/epoch: 3.267491579055786
Step: 13740, train/loss: 0.4032000005245209
Step: 13740, train/grad_norm: 3.4637739658355713
Step: 13740, train/learning_rate: 3.3650641853455454e-05
Step: 13740, train/epoch: 3.269871473312378
Step: 13750, train/loss: 0.33880001306533813
Step: 13750, train/grad_norm: 7.44813346862793
Step: 13750, train/learning_rate: 3.363874202477746e-05
Step: 13750, train/epoch: 3.2722513675689697
Step: 13760, train/loss: 0.3391000032424927
Step: 13760, train/grad_norm: 9.404081344604492
Step: 13760, train/learning_rate: 3.362684583407827e-05
Step: 13760, train/epoch: 3.2746310234069824
Step: 13770, train/loss: 0.4196000099182129
Step: 13770, train/grad_norm: 9.712249755859375
Step: 13770, train/learning_rate: 3.361494600540027e-05
Step: 13770, train/epoch: 3.277010917663574
Step: 13780, train/loss: 0.3091999888420105
Step: 13780, train/grad_norm: 28.02574348449707
Step: 13780, train/learning_rate: 3.360304617672227e-05
Step: 13780, train/epoch: 3.279390811920166
Step: 13790, train/loss: 0.531000018119812
Step: 13790, train/grad_norm: 4.9429216384887695
Step: 13790, train/learning_rate: 3.3591146348044276e-05
Step: 13790, train/epoch: 3.2817704677581787
Step: 13800, train/loss: 0.365200012922287
Step: 13800, train/grad_norm: 8.532087326049805
Step: 13800, train/learning_rate: 3.357924651936628e-05
Step: 13800, train/epoch: 3.2841503620147705
Step: 13810, train/loss: 0.5321999788284302
Step: 13810, train/grad_norm: 18.493993759155273
Step: 13810, train/learning_rate: 3.356735032866709e-05
Step: 13810, train/epoch: 3.2865302562713623
Step: 13820, train/loss: 0.5058000087738037
Step: 13820, train/grad_norm: 16.163175582885742
Step: 13820, train/learning_rate: 3.355545049998909e-05
Step: 13820, train/epoch: 3.288910150527954
Step: 13830, train/loss: 0.47999998927116394
Step: 13830, train/grad_norm: 7.623333930969238
Step: 13830, train/learning_rate: 3.3543550671311095e-05
Step: 13830, train/epoch: 3.291289806365967
Step: 13840, train/loss: 0.5684000253677368
Step: 13840, train/grad_norm: 8.859472274780273
Step: 13840, train/learning_rate: 3.35316508426331e-05
Step: 13840, train/epoch: 3.2936697006225586
Step: 13850, train/loss: 0.41130000352859497
Step: 13850, train/grad_norm: 5.135087490081787
Step: 13850, train/learning_rate: 3.35197510139551e-05
Step: 13850, train/epoch: 3.2960495948791504
Step: 13860, train/loss: 0.3619000017642975
Step: 13860, train/grad_norm: 7.881652355194092
Step: 13860, train/learning_rate: 3.350785482325591e-05
Step: 13860, train/epoch: 3.298429250717163
Step: 13870, train/loss: 0.4442000091075897
Step: 13870, train/grad_norm: 9.583702087402344
Step: 13870, train/learning_rate: 3.3495954994577914e-05
Step: 13870, train/epoch: 3.300809144973755
Step: 13880, train/loss: 0.32589998841285706
Step: 13880, train/grad_norm: 9.732749938964844
Step: 13880, train/learning_rate: 3.348405516589992e-05
Step: 13880, train/epoch: 3.3031890392303467
Step: 13890, train/loss: 0.47769999504089355
Step: 13890, train/grad_norm: 32.23969268798828
Step: 13890, train/learning_rate: 3.347215533722192e-05
Step: 13890, train/epoch: 3.3055686950683594
Step: 13900, train/loss: 0.34610000252723694
Step: 13900, train/grad_norm: 9.065679550170898
Step: 13900, train/learning_rate: 3.3460255508543923e-05
Step: 13900, train/epoch: 3.307948589324951
Step: 13910, train/loss: 0.41179999709129333
Step: 13910, train/grad_norm: 5.056944847106934
Step: 13910, train/learning_rate: 3.3448359317844734e-05
Step: 13910, train/epoch: 3.310328483581543
Step: 13920, train/loss: 0.39989998936653137
Step: 13920, train/grad_norm: 4.815384387969971
Step: 13920, train/learning_rate: 3.3436459489166737e-05
Step: 13920, train/epoch: 3.3127081394195557
Step: 13930, train/loss: 0.4000999927520752
Step: 13930, train/grad_norm: 14.109380722045898
Step: 13930, train/learning_rate: 3.342455966048874e-05
Step: 13930, train/epoch: 3.3150880336761475
Step: 13940, train/loss: 0.4611000120639801
Step: 13940, train/grad_norm: 25.709901809692383
Step: 13940, train/learning_rate: 3.341265983181074e-05
Step: 13940, train/epoch: 3.3174679279327393
Step: 13950, train/loss: 0.43799999356269836
Step: 13950, train/grad_norm: 20.213701248168945
Step: 13950, train/learning_rate: 3.3400760003132746e-05
Step: 13950, train/epoch: 3.319847583770752
Step: 13960, train/loss: 0.3386000096797943
Step: 13960, train/grad_norm: 31.15626335144043
Step: 13960, train/learning_rate: 3.3388863812433556e-05
Step: 13960, train/epoch: 3.3222274780273438
Step: 13970, train/loss: 0.3984000086784363
Step: 13970, train/grad_norm: 5.697124481201172
Step: 13970, train/learning_rate: 3.337696398375556e-05
Step: 13970, train/epoch: 3.3246073722839355
Step: 13980, train/loss: 0.5101000070571899
Step: 13980, train/grad_norm: 11.350194931030273
Step: 13980, train/learning_rate: 3.336506415507756e-05
Step: 13980, train/epoch: 3.3269872665405273
Step: 13990, train/loss: 0.38019999861717224
Step: 13990, train/grad_norm: 9.507613182067871
Step: 13990, train/learning_rate: 3.3353164326399565e-05
Step: 13990, train/epoch: 3.32936692237854
Step: 14000, train/loss: 0.31929999589920044
Step: 14000, train/grad_norm: 25.10064125061035
Step: 14000, train/learning_rate: 3.334126449772157e-05
Step: 14000, train/epoch: 3.331746816635132
Step: 14010, train/loss: 0.5052000284194946
Step: 14010, train/grad_norm: 5.779898643493652
Step: 14010, train/learning_rate: 3.332936830702238e-05
Step: 14010, train/epoch: 3.3341267108917236
Step: 14020, train/loss: 0.34139999747276306
Step: 14020, train/grad_norm: 12.037697792053223
Step: 14020, train/learning_rate: 3.331746847834438e-05
Step: 14020, train/epoch: 3.3365063667297363
Step: 14030, train/loss: 0.4675000011920929
Step: 14030, train/grad_norm: 6.030729293823242
Step: 14030, train/learning_rate: 3.3305568649666384e-05
Step: 14030, train/epoch: 3.338886260986328
Step: 14040, train/loss: 0.5468000173568726
Step: 14040, train/grad_norm: 10.482866287231445
Step: 14040, train/learning_rate: 3.329366882098839e-05
Step: 14040, train/epoch: 3.34126615524292
Step: 14050, train/loss: 0.3849000036716461
Step: 14050, train/grad_norm: 9.915733337402344
Step: 14050, train/learning_rate: 3.328176899231039e-05
Step: 14050, train/epoch: 3.3436458110809326
Step: 14060, train/loss: 0.27649998664855957
Step: 14060, train/grad_norm: 6.749001502990723
Step: 14060, train/learning_rate: 3.32698728016112e-05
Step: 14060, train/epoch: 3.3460257053375244
Step: 14070, train/loss: 0.3750999867916107
Step: 14070, train/grad_norm: 13.607810974121094
Step: 14070, train/learning_rate: 3.32579729729332e-05
Step: 14070, train/epoch: 3.348405599594116
Step: 14080, train/loss: 0.41190001368522644
Step: 14080, train/grad_norm: 5.960258960723877
Step: 14080, train/learning_rate: 3.3246073144255206e-05
Step: 14080, train/epoch: 3.350785255432129
Step: 14090, train/loss: 0.4458000063896179
Step: 14090, train/grad_norm: 10.115422248840332
Step: 14090, train/learning_rate: 3.323417331557721e-05
Step: 14090, train/epoch: 3.3531651496887207
Step: 14100, train/loss: 0.3928000032901764
Step: 14100, train/grad_norm: 9.020936965942383
Step: 14100, train/learning_rate: 3.322227348689921e-05
Step: 14100, train/epoch: 3.3555450439453125
Step: 14110, train/loss: 0.5281999707221985
Step: 14110, train/grad_norm: 18.693437576293945
Step: 14110, train/learning_rate: 3.321037729620002e-05
Step: 14110, train/epoch: 3.357924699783325
Step: 14120, train/loss: 0.3792000114917755
Step: 14120, train/grad_norm: 16.389678955078125
Step: 14120, train/learning_rate: 3.3198477467522025e-05
Step: 14120, train/epoch: 3.360304594039917
Step: 14130, train/loss: 0.3578999936580658
Step: 14130, train/grad_norm: 9.16793155670166
Step: 14130, train/learning_rate: 3.318657763884403e-05
Step: 14130, train/epoch: 3.362684488296509
Step: 14140, train/loss: 0.429500013589859
Step: 14140, train/grad_norm: 5.998227596282959
Step: 14140, train/learning_rate: 3.317467781016603e-05
Step: 14140, train/epoch: 3.3650641441345215
Step: 14150, train/loss: 0.3702999949455261
Step: 14150, train/grad_norm: 18.576120376586914
Step: 14150, train/learning_rate: 3.3162777981488034e-05
Step: 14150, train/epoch: 3.3674440383911133
Step: 14160, train/loss: 0.38370001316070557
Step: 14160, train/grad_norm: 12.664413452148438
Step: 14160, train/learning_rate: 3.3150881790788844e-05
Step: 14160, train/epoch: 3.369823932647705
Step: 14170, train/loss: 0.3847000002861023
Step: 14170, train/grad_norm: 17.18018341064453
Step: 14170, train/learning_rate: 3.313898196211085e-05
Step: 14170, train/epoch: 3.372203826904297
Step: 14180, train/loss: 0.39480000734329224
Step: 14180, train/grad_norm: 6.667784690856934
Step: 14180, train/learning_rate: 3.312708213343285e-05
Step: 14180, train/epoch: 3.3745834827423096
Step: 14190, train/loss: 0.42010000348091125
Step: 14190, train/grad_norm: 25.206199645996094
Step: 14190, train/learning_rate: 3.311518230475485e-05
Step: 14190, train/epoch: 3.3769633769989014
Step: 14200, train/loss: 0.4553000032901764
Step: 14200, train/grad_norm: 12.059718132019043
Step: 14200, train/learning_rate: 3.3103282476076856e-05
Step: 14200, train/epoch: 3.379343271255493
Step: 14210, train/loss: 0.4205999970436096
Step: 14210, train/grad_norm: 14.170072555541992
Step: 14210, train/learning_rate: 3.3091386285377666e-05
Step: 14210, train/epoch: 3.381722927093506
Step: 14220, train/loss: 0.5601000189781189
Step: 14220, train/grad_norm: 26.461435317993164
Step: 14220, train/learning_rate: 3.307948645669967e-05
Step: 14220, train/epoch: 3.3841028213500977
Step: 14230, train/loss: 0.5385000109672546
Step: 14230, train/grad_norm: 9.808816909790039
Step: 14230, train/learning_rate: 3.306758662802167e-05
Step: 14230, train/epoch: 3.3864827156066895
Step: 14240, train/loss: 0.4253999888896942
Step: 14240, train/grad_norm: 9.149669647216797
Step: 14240, train/learning_rate: 3.3055686799343675e-05
Step: 14240, train/epoch: 3.388862371444702
Step: 14250, train/loss: 0.44670000672340393
Step: 14250, train/grad_norm: 4.265839576721191
Step: 14250, train/learning_rate: 3.304378697066568e-05
Step: 14250, train/epoch: 3.391242265701294
Step: 14260, train/loss: 0.3481000065803528
Step: 14260, train/grad_norm: 17.99892807006836
Step: 14260, train/learning_rate: 3.303189077996649e-05
Step: 14260, train/epoch: 3.3936221599578857
Step: 14270, train/loss: 0.5159000158309937
Step: 14270, train/grad_norm: 10.839080810546875
Step: 14270, train/learning_rate: 3.301999095128849e-05
Step: 14270, train/epoch: 3.3960018157958984
Step: 14280, train/loss: 0.3544999957084656
Step: 14280, train/grad_norm: 17.644014358520508
Step: 14280, train/learning_rate: 3.3008091122610494e-05
Step: 14280, train/epoch: 3.3983817100524902
Step: 14290, train/loss: 0.34040001034736633
Step: 14290, train/grad_norm: 9.60570240020752
Step: 14290, train/learning_rate: 3.29961912939325e-05
Step: 14290, train/epoch: 3.400761604309082
Step: 14300, train/loss: 0.36390000581741333
Step: 14300, train/grad_norm: 8.527358055114746
Step: 14300, train/learning_rate: 3.29842914652545e-05
Step: 14300, train/epoch: 3.4031412601470947
Step: 14310, train/loss: 0.3779999911785126
Step: 14310, train/grad_norm: 4.4555511474609375
Step: 14310, train/learning_rate: 3.297239527455531e-05
Step: 14310, train/epoch: 3.4055211544036865
Step: 14320, train/loss: 0.3873000144958496
Step: 14320, train/grad_norm: 9.2371826171875
Step: 14320, train/learning_rate: 3.2960495445877314e-05
Step: 14320, train/epoch: 3.4079010486602783
Step: 14330, train/loss: 0.42820000648498535
Step: 14330, train/grad_norm: 12.929601669311523
Step: 14330, train/learning_rate: 3.294859561719932e-05
Step: 14330, train/epoch: 3.410280704498291
Step: 14340, train/loss: 0.5169000029563904
Step: 14340, train/grad_norm: 31.344816207885742
Step: 14340, train/learning_rate: 3.293669578852132e-05
Step: 14340, train/epoch: 3.412660598754883
Step: 14350, train/loss: 0.39410001039505005
Step: 14350, train/grad_norm: 3.1917078495025635
Step: 14350, train/learning_rate: 3.292479595984332e-05
Step: 14350, train/epoch: 3.4150404930114746
Step: 14360, train/loss: 0.46540001034736633
Step: 14360, train/grad_norm: 24.892086029052734
Step: 14360, train/learning_rate: 3.291289976914413e-05
Step: 14360, train/epoch: 3.4174203872680664
Step: 14370, train/loss: 0.37279999256134033
Step: 14370, train/grad_norm: 8.681148529052734
Step: 14370, train/learning_rate: 3.2900999940466136e-05
Step: 14370, train/epoch: 3.419800043106079
Step: 14380, train/loss: 0.6107000112533569
Step: 14380, train/grad_norm: 12.107945442199707
Step: 14380, train/learning_rate: 3.288910011178814e-05
Step: 14380, train/epoch: 3.422179937362671
Step: 14390, train/loss: 0.43070000410079956
Step: 14390, train/grad_norm: 4.893783092498779
Step: 14390, train/learning_rate: 3.287720028311014e-05
Step: 14390, train/epoch: 3.4245598316192627
Step: 14400, train/loss: 0.3968999981880188
Step: 14400, train/grad_norm: 13.1876220703125
Step: 14400, train/learning_rate: 3.2865300454432145e-05
Step: 14400, train/epoch: 3.4269394874572754
Step: 14410, train/loss: 0.3937999904155731
Step: 14410, train/grad_norm: 11.027619361877441
Step: 14410, train/learning_rate: 3.2853404263732955e-05
Step: 14410, train/epoch: 3.429319381713867
Step: 14420, train/loss: 0.35850000381469727
Step: 14420, train/grad_norm: 6.293802738189697
Step: 14420, train/learning_rate: 3.284150443505496e-05
Step: 14420, train/epoch: 3.431699275970459
Step: 14430, train/loss: 0.48669999837875366
Step: 14430, train/grad_norm: 24.037853240966797
Step: 14430, train/learning_rate: 3.282960460637696e-05
Step: 14430, train/epoch: 3.4340789318084717
Step: 14440, train/loss: 0.5026000142097473
Step: 14440, train/grad_norm: 7.059695720672607
Step: 14440, train/learning_rate: 3.2817704777698964e-05
Step: 14440, train/epoch: 3.4364588260650635
Step: 14450, train/loss: 0.4049000144004822
Step: 14450, train/grad_norm: 22.488924026489258
Step: 14450, train/learning_rate: 3.280580494902097e-05
Step: 14450, train/epoch: 3.4388387203216553
Step: 14460, train/loss: 0.42730000615119934
Step: 14460, train/grad_norm: 42.302734375
Step: 14460, train/learning_rate: 3.279390875832178e-05
Step: 14460, train/epoch: 3.441218376159668
Step: 14470, train/loss: 0.31529998779296875
Step: 14470, train/grad_norm: 4.4811530113220215
Step: 14470, train/learning_rate: 3.278200892964378e-05
Step: 14470, train/epoch: 3.4435982704162598
Step: 14480, train/loss: 0.4075999855995178
Step: 14480, train/grad_norm: 20.331201553344727
Step: 14480, train/learning_rate: 3.277010910096578e-05
Step: 14480, train/epoch: 3.4459781646728516
Step: 14490, train/loss: 0.35740000009536743
Step: 14490, train/grad_norm: 19.815279006958008
Step: 14490, train/learning_rate: 3.2758209272287786e-05
Step: 14490, train/epoch: 3.4483578205108643
Step: 14500, train/loss: 0.35740000009536743
Step: 14500, train/grad_norm: 26.097503662109375
Step: 14500, train/learning_rate: 3.2746313081588596e-05
Step: 14500, train/epoch: 3.450737714767456
Step: 14510, train/loss: 0.3596999943256378
Step: 14510, train/grad_norm: 12.210070610046387
Step: 14510, train/learning_rate: 3.27344132529106e-05
Step: 14510, train/epoch: 3.453117609024048
Step: 14520, train/loss: 0.45890000462532043
Step: 14520, train/grad_norm: 4.246099948883057
Step: 14520, train/learning_rate: 3.27225134242326e-05
Step: 14520, train/epoch: 3.4554972648620605
Step: 14530, train/loss: 0.39329999685287476
Step: 14530, train/grad_norm: 3.230422019958496
Step: 14530, train/learning_rate: 3.2710613595554605e-05
Step: 14530, train/epoch: 3.4578771591186523
Step: 14540, train/loss: 0.26460000872612
Step: 14540, train/grad_norm: 14.993144035339355
Step: 14540, train/learning_rate: 3.269871376687661e-05
Step: 14540, train/epoch: 3.460257053375244
Step: 14550, train/loss: 0.319599986076355
Step: 14550, train/grad_norm: 6.551547527313232
Step: 14550, train/learning_rate: 3.268681757617742e-05
Step: 14550, train/epoch: 3.462636947631836
Step: 14560, train/loss: 0.3970000147819519
Step: 14560, train/grad_norm: 18.296554565429688
Step: 14560, train/learning_rate: 3.267491774749942e-05
Step: 14560, train/epoch: 3.4650166034698486
Step: 14570, train/loss: 0.32120001316070557
Step: 14570, train/grad_norm: 8.195830345153809
Step: 14570, train/learning_rate: 3.2663017918821424e-05
Step: 14570, train/epoch: 3.4673964977264404
Step: 14580, train/loss: 0.3962000012397766
Step: 14580, train/grad_norm: 6.919699668884277
Step: 14580, train/learning_rate: 3.265111809014343e-05
Step: 14580, train/epoch: 3.4697763919830322
Step: 14590, train/loss: 0.3544999957084656
Step: 14590, train/grad_norm: 5.356687545776367
Step: 14590, train/learning_rate: 3.263921826146543e-05
Step: 14590, train/epoch: 3.472156047821045
Step: 14600, train/loss: 0.44749999046325684
Step: 14600, train/grad_norm: 7.898418426513672
Step: 14600, train/learning_rate: 3.262732207076624e-05
Step: 14600, train/epoch: 3.4745359420776367
Step: 14610, train/loss: 0.2718999981880188
Step: 14610, train/grad_norm: 6.756104946136475
Step: 14610, train/learning_rate: 3.261542224208824e-05
Step: 14610, train/epoch: 3.4769158363342285
Step: 14620, train/loss: 0.41200000047683716
Step: 14620, train/grad_norm: 23.566282272338867
Step: 14620, train/learning_rate: 3.2603522413410246e-05
Step: 14620, train/epoch: 3.479295492172241
Step: 14630, train/loss: 0.362199991941452
Step: 14630, train/grad_norm: 12.9146146774292
Step: 14630, train/learning_rate: 3.259162258473225e-05
Step: 14630, train/epoch: 3.481675386428833
Step: 14640, train/loss: 0.32350000739097595
Step: 14640, train/grad_norm: 9.336949348449707
Step: 14640, train/learning_rate: 3.257972275605425e-05
Step: 14640, train/epoch: 3.484055280685425
Step: 14650, train/loss: 0.36660000681877136
Step: 14650, train/grad_norm: 7.105133533477783
Step: 14650, train/learning_rate: 3.256782656535506e-05
Step: 14650, train/epoch: 3.4864349365234375
Step: 14660, train/loss: 0.33469998836517334
Step: 14660, train/grad_norm: 22.99635887145996
Step: 14660, train/learning_rate: 3.2555926736677065e-05
Step: 14660, train/epoch: 3.4888148307800293
Step: 14670, train/loss: 0.3709999918937683
Step: 14670, train/grad_norm: 23.834178924560547
Step: 14670, train/learning_rate: 3.254402690799907e-05
Step: 14670, train/epoch: 3.491194725036621
Step: 14680, train/loss: 0.45719999074935913
Step: 14680, train/grad_norm: 13.286617279052734
Step: 14680, train/learning_rate: 3.253212707932107e-05
Step: 14680, train/epoch: 3.493574380874634
Step: 14690, train/loss: 0.30550000071525574
Step: 14690, train/grad_norm: 3.7634646892547607
Step: 14690, train/learning_rate: 3.2520227250643075e-05
Step: 14690, train/epoch: 3.4959542751312256
Step: 14700, train/loss: 0.49959999322891235
Step: 14700, train/grad_norm: 14.077500343322754
Step: 14700, train/learning_rate: 3.2508331059943885e-05
Step: 14700, train/epoch: 3.4983341693878174
Step: 14710, train/loss: 0.39989998936653137
Step: 14710, train/grad_norm: 22.3974609375
Step: 14710, train/learning_rate: 3.249643123126589e-05
Step: 14710, train/epoch: 3.500714063644409
Step: 14720, train/loss: 0.23479999601840973
Step: 14720, train/grad_norm: 4.869670867919922
Step: 14720, train/learning_rate: 3.248453140258789e-05
Step: 14720, train/epoch: 3.503093719482422
Step: 14730, train/loss: 0.3061999976634979
Step: 14730, train/grad_norm: 13.22705364227295
Step: 14730, train/learning_rate: 3.2472631573909894e-05
Step: 14730, train/epoch: 3.5054736137390137
Step: 14740, train/loss: 0.5990999937057495
Step: 14740, train/grad_norm: 10.911943435668945
Step: 14740, train/learning_rate: 3.24607317452319e-05
Step: 14740, train/epoch: 3.5078535079956055
Step: 14750, train/loss: 0.4699000120162964
Step: 14750, train/grad_norm: 3.019629716873169
Step: 14750, train/learning_rate: 3.244883555453271e-05
Step: 14750, train/epoch: 3.510233163833618
Step: 14760, train/loss: 0.5388000011444092
Step: 14760, train/grad_norm: 21.69762420654297
Step: 14760, train/learning_rate: 3.243693572585471e-05
Step: 14760, train/epoch: 3.51261305809021
Step: 14770, train/loss: 0.2867000102996826
Step: 14770, train/grad_norm: 6.028989791870117
Step: 14770, train/learning_rate: 3.242503589717671e-05
Step: 14770, train/epoch: 3.5149929523468018
Step: 14780, train/loss: 0.5562000274658203
Step: 14780, train/grad_norm: 16.18204689025879
Step: 14780, train/learning_rate: 3.2413136068498716e-05
Step: 14780, train/epoch: 3.5173726081848145
Step: 14790, train/loss: 0.39500001072883606
Step: 14790, train/grad_norm: 17.6405086517334
Step: 14790, train/learning_rate: 3.240123623982072e-05
Step: 14790, train/epoch: 3.5197525024414062
Step: 14800, train/loss: 0.3192000091075897
Step: 14800, train/grad_norm: 9.080337524414062
Step: 14800, train/learning_rate: 3.238934004912153e-05
Step: 14800, train/epoch: 3.522132396697998
Step: 14810, train/loss: 0.28439998626708984
Step: 14810, train/grad_norm: 12.561859130859375
Step: 14810, train/learning_rate: 3.237744022044353e-05
Step: 14810, train/epoch: 3.5245120525360107
Step: 14820, train/loss: 0.47360000014305115
Step: 14820, train/grad_norm: 18.28477668762207
Step: 14820, train/learning_rate: 3.2365540391765535e-05
Step: 14820, train/epoch: 3.5268919467926025
Step: 14830, train/loss: 0.4514000117778778
Step: 14830, train/grad_norm: 11.95842170715332
Step: 14830, train/learning_rate: 3.235364056308754e-05
Step: 14830, train/epoch: 3.5292718410491943
Step: 14840, train/loss: 0.38659998774528503
Step: 14840, train/grad_norm: 13.690420150756836
Step: 14840, train/learning_rate: 3.234174073440954e-05
Step: 14840, train/epoch: 3.531651496887207
Step: 14850, train/loss: 0.2727999985218048
Step: 14850, train/grad_norm: 6.568620204925537
Step: 14850, train/learning_rate: 3.232984454371035e-05
Step: 14850, train/epoch: 3.534031391143799
Step: 14860, train/loss: 0.6233999729156494
Step: 14860, train/grad_norm: 22.83454704284668
Step: 14860, train/learning_rate: 3.2317944715032354e-05
Step: 14860, train/epoch: 3.5364112854003906
Step: 14870, train/loss: 0.5407999753952026
Step: 14870, train/grad_norm: 18.615121841430664
Step: 14870, train/learning_rate: 3.230604488635436e-05
Step: 14870, train/epoch: 3.5387909412384033
Step: 14880, train/loss: 0.4415999948978424
Step: 14880, train/grad_norm: 12.887167930603027
Step: 14880, train/learning_rate: 3.229414505767636e-05
Step: 14880, train/epoch: 3.541170835494995
Step: 14890, train/loss: 0.48019999265670776
Step: 14890, train/grad_norm: 23.165027618408203
Step: 14890, train/learning_rate: 3.228224522899836e-05
Step: 14890, train/epoch: 3.543550729751587
Step: 14900, train/loss: 0.337799996137619
Step: 14900, train/grad_norm: 12.42422103881836
Step: 14900, train/learning_rate: 3.227034903829917e-05
Step: 14900, train/epoch: 3.5459306240081787
Step: 14910, train/loss: 0.413100004196167
Step: 14910, train/grad_norm: 4.0724639892578125
Step: 14910, train/learning_rate: 3.2258449209621176e-05
Step: 14910, train/epoch: 3.5483102798461914
Step: 14920, train/loss: 0.31380000710487366
Step: 14920, train/grad_norm: 18.554529190063477
Step: 14920, train/learning_rate: 3.224654938094318e-05
Step: 14920, train/epoch: 3.550690174102783
Step: 14930, train/loss: 0.5080999732017517
Step: 14930, train/grad_norm: 18.72197723388672
Step: 14930, train/learning_rate: 3.223464955226518e-05
Step: 14930, train/epoch: 3.553070068359375
Step: 14940, train/loss: 0.4864000082015991
Step: 14940, train/grad_norm: 8.539894104003906
Step: 14940, train/learning_rate: 3.2222749723587185e-05
Step: 14940, train/epoch: 3.5554497241973877
Step: 14950, train/loss: 0.33500000834465027
Step: 14950, train/grad_norm: 9.738215446472168
Step: 14950, train/learning_rate: 3.2210853532887995e-05
Step: 14950, train/epoch: 3.5578296184539795
Step: 14960, train/loss: 0.3864000141620636
Step: 14960, train/grad_norm: 5.527485370635986
Step: 14960, train/learning_rate: 3.219895370421e-05
Step: 14960, train/epoch: 3.5602095127105713
Step: 14970, train/loss: 0.32109999656677246
Step: 14970, train/grad_norm: 11.904623031616211
Step: 14970, train/learning_rate: 3.2187053875532e-05
Step: 14970, train/epoch: 3.562589168548584
Step: 14980, train/loss: 0.24390000104904175
Step: 14980, train/grad_norm: 8.267656326293945
Step: 14980, train/learning_rate: 3.2175154046854004e-05
Step: 14980, train/epoch: 3.564969062805176
Step: 14990, train/loss: 0.399399995803833
Step: 14990, train/grad_norm: 6.139820575714111
Step: 14990, train/learning_rate: 3.216325421817601e-05
Step: 14990, train/epoch: 3.5673489570617676
Step: 15000, train/loss: 0.2800999879837036
Step: 15000, train/grad_norm: 19.619447708129883
Step: 15000, train/learning_rate: 3.215135802747682e-05
Step: 15000, train/epoch: 3.5697286128997803
Step: 15010, train/loss: 0.5223000049591064
Step: 15010, train/grad_norm: 5.624574661254883
Step: 15010, train/learning_rate: 3.213945819879882e-05
Step: 15010, train/epoch: 3.572108507156372
Step: 15020, train/loss: 0.3488999903202057
Step: 15020, train/grad_norm: 9.184552192687988
Step: 15020, train/learning_rate: 3.212755837012082e-05
Step: 15020, train/epoch: 3.574488401412964
Step: 15030, train/loss: 0.5059999823570251
Step: 15030, train/grad_norm: 12.67759895324707
Step: 15030, train/learning_rate: 3.2115658541442826e-05
Step: 15030, train/epoch: 3.5768680572509766
Step: 15040, train/loss: 0.4120999872684479
Step: 15040, train/grad_norm: 2.689462900161743
Step: 15040, train/learning_rate: 3.210375871276483e-05
Step: 15040, train/epoch: 3.5792479515075684
Step: 15050, train/loss: 0.3677999973297119
Step: 15050, train/grad_norm: 11.849050521850586
Step: 15050, train/learning_rate: 3.209186252206564e-05
Step: 15050, train/epoch: 3.58162784576416
Step: 15060, train/loss: 0.3752000033855438
Step: 15060, train/grad_norm: 7.942997932434082
Step: 15060, train/learning_rate: 3.207996269338764e-05
Step: 15060, train/epoch: 3.584007501602173
Step: 15070, train/loss: 0.2791999876499176
Step: 15070, train/grad_norm: 6.7914299964904785
Step: 15070, train/learning_rate: 3.2068062864709646e-05
Step: 15070, train/epoch: 3.5863873958587646
Step: 15080, train/loss: 0.4341000020503998
Step: 15080, train/grad_norm: 1.6242071390151978
Step: 15080, train/learning_rate: 3.205616303603165e-05
Step: 15080, train/epoch: 3.5887672901153564
Step: 15090, train/loss: 0.3799000084400177
Step: 15090, train/grad_norm: 14.12230110168457
Step: 15090, train/learning_rate: 3.204426320735365e-05
Step: 15090, train/epoch: 3.5911471843719482
Step: 15100, train/loss: 0.41830000281333923
Step: 15100, train/grad_norm: 9.013440132141113
Step: 15100, train/learning_rate: 3.203236701665446e-05
Step: 15100, train/epoch: 3.593526840209961
Step: 15110, train/loss: 0.33059999346733093
Step: 15110, train/grad_norm: 17.61200523376465
Step: 15110, train/learning_rate: 3.2020467187976465e-05
Step: 15110, train/epoch: 3.5959067344665527
Step: 15120, train/loss: 0.43230000138282776
Step: 15120, train/grad_norm: 6.862748146057129
Step: 15120, train/learning_rate: 3.200856735929847e-05
Step: 15120, train/epoch: 3.5982866287231445
Step: 15130, train/loss: 0.505299985408783
Step: 15130, train/grad_norm: 31.368408203125
Step: 15130, train/learning_rate: 3.199666753062047e-05
Step: 15130, train/epoch: 3.6006662845611572
Step: 15140, train/loss: 0.3691999912261963
Step: 15140, train/grad_norm: 6.8612284660339355
Step: 15140, train/learning_rate: 3.1984767701942474e-05
Step: 15140, train/epoch: 3.603046178817749
Step: 15150, train/loss: 0.38940000534057617
Step: 15150, train/grad_norm: 8.095114707946777
Step: 15150, train/learning_rate: 3.1972871511243284e-05
Step: 15150, train/epoch: 3.605426073074341
Step: 15160, train/loss: 0.32499998807907104
Step: 15160, train/grad_norm: 15.796086311340332
Step: 15160, train/learning_rate: 3.196097168256529e-05
Step: 15160, train/epoch: 3.6078057289123535
Step: 15170, train/loss: 0.4578999876976013
Step: 15170, train/grad_norm: 12.339757919311523
Step: 15170, train/learning_rate: 3.194907185388729e-05
Step: 15170, train/epoch: 3.6101856231689453
Step: 15180, train/loss: 0.5324000120162964
Step: 15180, train/grad_norm: 22.19744300842285
Step: 15180, train/learning_rate: 3.193717202520929e-05
Step: 15180, train/epoch: 3.612565517425537
Step: 15190, train/loss: 0.49559998512268066
Step: 15190, train/grad_norm: 10.5717134475708
Step: 15190, train/learning_rate: 3.1925272196531296e-05
Step: 15190, train/epoch: 3.61494517326355
Step: 15200, train/loss: 0.30809998512268066
Step: 15200, train/grad_norm: 9.751651763916016
Step: 15200, train/learning_rate: 3.1913376005832106e-05
Step: 15200, train/epoch: 3.6173250675201416
Step: 15210, train/loss: 0.41690000891685486
Step: 15210, train/grad_norm: 11.155959129333496
Step: 15210, train/learning_rate: 3.190147617715411e-05
Step: 15210, train/epoch: 3.6197049617767334
Step: 15220, train/loss: 0.3889000117778778
Step: 15220, train/grad_norm: 17.86675262451172
Step: 15220, train/learning_rate: 3.188957634847611e-05
Step: 15220, train/epoch: 3.622084617614746
Step: 15230, train/loss: 0.30649998784065247
Step: 15230, train/grad_norm: 17.67975425720215
Step: 15230, train/learning_rate: 3.1877676519798115e-05
Step: 15230, train/epoch: 3.624464511871338
Step: 15240, train/loss: 0.5101000070571899
Step: 15240, train/grad_norm: 11.704751968383789
Step: 15240, train/learning_rate: 3.186577669112012e-05
Step: 15240, train/epoch: 3.6268444061279297
Step: 15250, train/loss: 0.47440001368522644
Step: 15250, train/grad_norm: 22.905845642089844
Step: 15250, train/learning_rate: 3.185388050042093e-05
Step: 15250, train/epoch: 3.6292240619659424
Step: 15260, train/loss: 0.41119998693466187
Step: 15260, train/grad_norm: 20.42013931274414
Step: 15260, train/learning_rate: 3.184198067174293e-05
Step: 15260, train/epoch: 3.631603956222534
Step: 15270, train/loss: 0.5216000080108643
Step: 15270, train/grad_norm: 13.542085647583008
Step: 15270, train/learning_rate: 3.1830080843064934e-05
Step: 15270, train/epoch: 3.633983850479126
Step: 15280, train/loss: 0.3978999853134155
Step: 15280, train/grad_norm: 3.790093183517456
Step: 15280, train/learning_rate: 3.181818101438694e-05
Step: 15280, train/epoch: 3.6363637447357178
Step: 15290, train/loss: 0.3718000054359436
Step: 15290, train/grad_norm: 1.8659204244613647
Step: 15290, train/learning_rate: 3.180628118570894e-05
Step: 15290, train/epoch: 3.6387434005737305
Step: 15300, train/loss: 0.47699999809265137
Step: 15300, train/grad_norm: 11.821940422058105
Step: 15300, train/learning_rate: 3.179438499500975e-05
Step: 15300, train/epoch: 3.6411232948303223
Step: 15310, train/loss: 0.3337000012397766
Step: 15310, train/grad_norm: 4.627030849456787
Step: 15310, train/learning_rate: 3.178248516633175e-05
Step: 15310, train/epoch: 3.643503189086914
Step: 15320, train/loss: 0.3156000077724457
Step: 15320, train/grad_norm: 14.235869407653809
Step: 15320, train/learning_rate: 3.1770585337653756e-05
Step: 15320, train/epoch: 3.6458828449249268
Step: 15330, train/loss: 0.45509999990463257
Step: 15330, train/grad_norm: 15.007904052734375
Step: 15330, train/learning_rate: 3.175868550897576e-05
Step: 15330, train/epoch: 3.6482627391815186
Step: 15340, train/loss: 0.4271000027656555
Step: 15340, train/grad_norm: 12.65830135345459
Step: 15340, train/learning_rate: 3.174678568029776e-05
Step: 15340, train/epoch: 3.6506426334381104
Step: 15350, train/loss: 0.4142000079154968
Step: 15350, train/grad_norm: 12.341277122497559
Step: 15350, train/learning_rate: 3.173488948959857e-05
Step: 15350, train/epoch: 3.653022289276123
Step: 15360, train/loss: 0.41670000553131104
Step: 15360, train/grad_norm: 4.353326320648193
Step: 15360, train/learning_rate: 3.1722989660920575e-05
Step: 15360, train/epoch: 3.655402183532715
Step: 15370, train/loss: 0.41350001096725464
Step: 15370, train/grad_norm: 6.134470462799072
Step: 15370, train/learning_rate: 3.171108983224258e-05
Step: 15370, train/epoch: 3.6577820777893066
Step: 15380, train/loss: 0.4580000042915344
Step: 15380, train/grad_norm: 6.207895755767822
Step: 15380, train/learning_rate: 3.169919000356458e-05
Step: 15380, train/epoch: 3.6601617336273193
Step: 15390, train/loss: 0.3621000051498413
Step: 15390, train/grad_norm: 10.817646026611328
Step: 15390, train/learning_rate: 3.1687290174886584e-05
Step: 15390, train/epoch: 3.662541627883911
Step: 15400, train/loss: 0.36329999566078186
Step: 15400, train/grad_norm: 13.776336669921875
Step: 15400, train/learning_rate: 3.1675393984187394e-05
Step: 15400, train/epoch: 3.664921522140503
Step: 15410, train/loss: 0.25189998745918274
Step: 15410, train/grad_norm: 4.298873424530029
Step: 15410, train/learning_rate: 3.16634941555094e-05
Step: 15410, train/epoch: 3.6673011779785156
Step: 15420, train/loss: 0.508400022983551
Step: 15420, train/grad_norm: 12.035832405090332
Step: 15420, train/learning_rate: 3.16515943268314e-05
Step: 15420, train/epoch: 3.6696810722351074
Step: 15430, train/loss: 0.3330000042915344
Step: 15430, train/grad_norm: 19.626689910888672
Step: 15430, train/learning_rate: 3.1639694498153403e-05
Step: 15430, train/epoch: 3.672060966491699
Step: 15440, train/loss: 0.3628999888896942
Step: 15440, train/grad_norm: 4.04499626159668
Step: 15440, train/learning_rate: 3.1627794669475406e-05
Step: 15440, train/epoch: 3.674440860748291
Step: 15450, train/loss: 0.24240000545978546
Step: 15450, train/grad_norm: 20.763694763183594
Step: 15450, train/learning_rate: 3.1615898478776217e-05
Step: 15450, train/epoch: 3.6768205165863037
Step: 15460, train/loss: 0.44179999828338623
Step: 15460, train/grad_norm: 10.411246299743652
Step: 15460, train/learning_rate: 3.160399865009822e-05
Step: 15460, train/epoch: 3.6792004108428955
Step: 15470, train/loss: 0.5706999897956848
Step: 15470, train/grad_norm: 27.19959259033203
Step: 15470, train/learning_rate: 3.159209882142022e-05
Step: 15470, train/epoch: 3.6815803050994873
Step: 15480, train/loss: 0.5523999929428101
Step: 15480, train/grad_norm: 22.244203567504883
Step: 15480, train/learning_rate: 3.1580198992742226e-05
Step: 15480, train/epoch: 3.6839599609375
Step: 15490, train/loss: 0.5896000266075134
Step: 15490, train/grad_norm: 3.494702100753784
Step: 15490, train/learning_rate: 3.156829916406423e-05
Step: 15490, train/epoch: 3.686339855194092
Step: 15500, train/loss: 0.42640000581741333
Step: 15500, train/grad_norm: 13.433262825012207
Step: 15500, train/learning_rate: 3.155640297336504e-05
Step: 15500, train/epoch: 3.6887197494506836
Step: 15510, train/loss: 0.4221999943256378
Step: 15510, train/grad_norm: 3.7766964435577393
Step: 15510, train/learning_rate: 3.154450314468704e-05
Step: 15510, train/epoch: 3.6910994052886963
Step: 15520, train/loss: 0.3984000086784363
Step: 15520, train/grad_norm: 11.309064865112305
Step: 15520, train/learning_rate: 3.1532603316009045e-05
Step: 15520, train/epoch: 3.693479299545288
Step: 15530, train/loss: 0.4357999861240387
Step: 15530, train/grad_norm: 10.524184226989746
Step: 15530, train/learning_rate: 3.152070348733105e-05
Step: 15530, train/epoch: 3.69585919380188
Step: 15540, train/loss: 0.32280001044273376
Step: 15540, train/grad_norm: 4.768145561218262
Step: 15540, train/learning_rate: 3.150880365865305e-05
Step: 15540, train/epoch: 3.6982388496398926
Step: 15550, train/loss: 0.45239999890327454
Step: 15550, train/grad_norm: 19.485334396362305
Step: 15550, train/learning_rate: 3.149690746795386e-05
Step: 15550, train/epoch: 3.7006187438964844
Step: 15560, train/loss: 0.4023999869823456
Step: 15560, train/grad_norm: 3.0667965412139893
Step: 15560, train/learning_rate: 3.1485007639275864e-05
Step: 15560, train/epoch: 3.702998638153076
Step: 15570, train/loss: 0.4059999883174896
Step: 15570, train/grad_norm: 8.124207496643066
Step: 15570, train/learning_rate: 3.147310781059787e-05
Step: 15570, train/epoch: 3.705378293991089
Step: 15580, train/loss: 0.42489999532699585
Step: 15580, train/grad_norm: 4.453885078430176
Step: 15580, train/learning_rate: 3.146120798191987e-05
Step: 15580, train/epoch: 3.7077581882476807
Step: 15590, train/loss: 0.31310001015663147
Step: 15590, train/grad_norm: 11.6274995803833
Step: 15590, train/learning_rate: 3.144930815324187e-05
Step: 15590, train/epoch: 3.7101380825042725
Step: 15600, train/loss: 0.3433000147342682
Step: 15600, train/grad_norm: 9.85632038116455
Step: 15600, train/learning_rate: 3.143741196254268e-05
Step: 15600, train/epoch: 3.712517738342285
Step: 15610, train/loss: 0.5910999774932861
Step: 15610, train/grad_norm: 32.8420524597168
Step: 15610, train/learning_rate: 3.1425512133864686e-05
Step: 15610, train/epoch: 3.714897632598877
Step: 15620, train/loss: 0.43540000915527344
Step: 15620, train/grad_norm: 15.947699546813965
Step: 15620, train/learning_rate: 3.141361230518669e-05
Step: 15620, train/epoch: 3.7172775268554688
Step: 15630, train/loss: 0.251800000667572
Step: 15630, train/grad_norm: 7.259709358215332
Step: 15630, train/learning_rate: 3.140171247650869e-05
Step: 15630, train/epoch: 3.7196574211120605
Step: 15640, train/loss: 0.4348999857902527
Step: 15640, train/grad_norm: 19.274141311645508
Step: 15640, train/learning_rate: 3.1389812647830695e-05
Step: 15640, train/epoch: 3.7220370769500732
Step: 15650, train/loss: 0.5099999904632568
Step: 15650, train/grad_norm: 5.8136796951293945
Step: 15650, train/learning_rate: 3.1377916457131505e-05
Step: 15650, train/epoch: 3.724416971206665
Step: 15660, train/loss: 0.5957000255584717
Step: 15660, train/grad_norm: 11.8709135055542
Step: 15660, train/learning_rate: 3.136601662845351e-05
Step: 15660, train/epoch: 3.726796865463257
Step: 15670, train/loss: 0.4668999910354614
Step: 15670, train/grad_norm: 23.2699031829834
Step: 15670, train/learning_rate: 3.135411679977551e-05
Step: 15670, train/epoch: 3.7291765213012695
Step: 15680, train/loss: 0.3190999925136566
Step: 15680, train/grad_norm: 6.118269920349121
Step: 15680, train/learning_rate: 3.1342216971097514e-05
Step: 15680, train/epoch: 3.7315564155578613
Step: 15690, train/loss: 0.3312000036239624
Step: 15690, train/grad_norm: 8.910399436950684
Step: 15690, train/learning_rate: 3.133031714241952e-05
Step: 15690, train/epoch: 3.733936309814453
Step: 15700, train/loss: 0.4494999945163727
Step: 15700, train/grad_norm: 15.80810260772705
Step: 15700, train/learning_rate: 3.131842095172033e-05
Step: 15700, train/epoch: 3.736315965652466
Step: 15710, train/loss: 0.48739999532699585
Step: 15710, train/grad_norm: 22.923938751220703
Step: 15710, train/learning_rate: 3.130652112304233e-05
Step: 15710, train/epoch: 3.7386958599090576
Step: 15720, train/loss: 0.2750000059604645
Step: 15720, train/grad_norm: 7.557828426361084
Step: 15720, train/learning_rate: 3.129462129436433e-05
Step: 15720, train/epoch: 3.7410757541656494
Step: 15730, train/loss: 0.28760001063346863
Step: 15730, train/grad_norm: 6.011358737945557
Step: 15730, train/learning_rate: 3.1282721465686336e-05
Step: 15730, train/epoch: 3.743455410003662
Step: 15740, train/loss: 0.4717000126838684
Step: 15740, train/grad_norm: 10.219329833984375
Step: 15740, train/learning_rate: 3.127082163700834e-05
Step: 15740, train/epoch: 3.745835304260254
Step: 15750, train/loss: 0.41999998688697815
Step: 15750, train/grad_norm: 17.33951759338379
Step: 15750, train/learning_rate: 3.125892544630915e-05
Step: 15750, train/epoch: 3.7482151985168457
Step: 15760, train/loss: 0.4544999897480011
Step: 15760, train/grad_norm: 7.388480186462402
Step: 15760, train/learning_rate: 3.124702561763115e-05
Step: 15760, train/epoch: 3.7505948543548584
Step: 15770, train/loss: 0.4440000057220459
Step: 15770, train/grad_norm: 5.004148483276367
Step: 15770, train/learning_rate: 3.1235125788953155e-05
Step: 15770, train/epoch: 3.75297474861145
Step: 15780, train/loss: 0.31610000133514404
Step: 15780, train/grad_norm: 9.820906639099121
Step: 15780, train/learning_rate: 3.122322596027516e-05
Step: 15780, train/epoch: 3.755354642868042
Step: 15790, train/loss: 0.33340001106262207
Step: 15790, train/grad_norm: 13.359243392944336
Step: 15790, train/learning_rate: 3.121132613159716e-05
Step: 15790, train/epoch: 3.7577342987060547
Step: 15800, train/loss: 0.6067000031471252
Step: 15800, train/grad_norm: 14.834563255310059
Step: 15800, train/learning_rate: 3.119942994089797e-05
Step: 15800, train/epoch: 3.7601141929626465
Step: 15810, train/loss: 0.35120001435279846
Step: 15810, train/grad_norm: 6.800012588500977
Step: 15810, train/learning_rate: 3.1187530112219974e-05
Step: 15810, train/epoch: 3.7624940872192383
Step: 15820, train/loss: 0.2799000144004822
Step: 15820, train/grad_norm: 12.412747383117676
Step: 15820, train/learning_rate: 3.117563028354198e-05
Step: 15820, train/epoch: 3.76487398147583
Step: 15830, train/loss: 0.30079999566078186
Step: 15830, train/grad_norm: 9.396724700927734
Step: 15830, train/learning_rate: 3.116373045486398e-05
Step: 15830, train/epoch: 3.7672536373138428
Step: 15840, train/loss: 0.3695000112056732
Step: 15840, train/grad_norm: 16.9509334564209
Step: 15840, train/learning_rate: 3.115183426416479e-05
Step: 15840, train/epoch: 3.7696335315704346
Step: 15850, train/loss: 0.3546000123023987
Step: 15850, train/grad_norm: 5.20098876953125
Step: 15850, train/learning_rate: 3.1139934435486794e-05
Step: 15850, train/epoch: 3.7720134258270264
Step: 15860, train/loss: 0.3492000102996826
Step: 15860, train/grad_norm: 11.870085716247559
Step: 15860, train/learning_rate: 3.1128034606808797e-05
Step: 15860, train/epoch: 3.774393081665039
Step: 15870, train/loss: 0.4246000051498413
Step: 15870, train/grad_norm: 6.399178504943848
Step: 15870, train/learning_rate: 3.11161347781308e-05
Step: 15870, train/epoch: 3.776772975921631
Step: 15880, train/loss: 0.4034999907016754
Step: 15880, train/grad_norm: 9.461557388305664
Step: 15880, train/learning_rate: 3.11042349494528e-05
Step: 15880, train/epoch: 3.7791528701782227
Step: 15890, train/loss: 0.42160001397132874
Step: 15890, train/grad_norm: 15.894623756408691
Step: 15890, train/learning_rate: 3.109233875875361e-05
Step: 15890, train/epoch: 3.7815325260162354
Step: 15900, train/loss: 0.36719998717308044
Step: 15900, train/grad_norm: 8.580178260803223
Step: 15900, train/learning_rate: 3.1080438930075616e-05
Step: 15900, train/epoch: 3.783912420272827
Step: 15910, train/loss: 0.5680000185966492
Step: 15910, train/grad_norm: 24.45233917236328
Step: 15910, train/learning_rate: 3.106853910139762e-05
Step: 15910, train/epoch: 3.786292314529419
Step: 15920, train/loss: 0.35989999771118164
Step: 15920, train/grad_norm: 11.314051628112793
Step: 15920, train/learning_rate: 3.105663927271962e-05
Step: 15920, train/epoch: 3.7886719703674316
Step: 15930, train/loss: 0.44749999046325684
Step: 15930, train/grad_norm: 8.117173194885254
Step: 15930, train/learning_rate: 3.1044739444041625e-05
Step: 15930, train/epoch: 3.7910518646240234
Step: 15940, train/loss: 0.4083999991416931
Step: 15940, train/grad_norm: 10.451972007751465
Step: 15940, train/learning_rate: 3.1032843253342435e-05
Step: 15940, train/epoch: 3.7934317588806152
Step: 15950, train/loss: 0.5353000164031982
Step: 15950, train/grad_norm: 3.6931025981903076
Step: 15950, train/learning_rate: 3.102094342466444e-05
Step: 15950, train/epoch: 3.795811414718628
Step: 15960, train/loss: 0.4625999927520752
Step: 15960, train/grad_norm: 27.916610717773438
Step: 15960, train/learning_rate: 3.100904359598644e-05
Step: 15960, train/epoch: 3.7981913089752197
Step: 15970, train/loss: 0.47780001163482666
Step: 15970, train/grad_norm: 8.51179027557373
Step: 15970, train/learning_rate: 3.0997143767308444e-05
Step: 15970, train/epoch: 3.8005712032318115
Step: 15980, train/loss: 0.2840999960899353
Step: 15980, train/grad_norm: 9.413952827453613
Step: 15980, train/learning_rate: 3.098524393863045e-05
Step: 15980, train/epoch: 3.802950859069824
Step: 15990, train/loss: 0.3864000141620636
Step: 15990, train/grad_norm: 16.908321380615234
Step: 15990, train/learning_rate: 3.097334774793126e-05
Step: 15990, train/epoch: 3.805330753326416
Step: 16000, train/loss: 0.4049000144004822
Step: 16000, train/grad_norm: 6.175294876098633
Step: 16000, train/learning_rate: 3.096144791925326e-05
Step: 16000, train/epoch: 3.807710647583008
Step: 16010, train/loss: 0.3801000118255615
Step: 16010, train/grad_norm: 7.83622407913208
Step: 16010, train/learning_rate: 3.094954809057526e-05
Step: 16010, train/epoch: 3.8100905418395996
Step: 16020, train/loss: 0.34779998660087585
Step: 16020, train/grad_norm: 15.724289894104004
Step: 16020, train/learning_rate: 3.0937648261897266e-05
Step: 16020, train/epoch: 3.8124701976776123
Step: 16030, train/loss: 0.48249998688697815
Step: 16030, train/grad_norm: 5.247960567474365
Step: 16030, train/learning_rate: 3.092574843321927e-05
Step: 16030, train/epoch: 3.814850091934204
Step: 16040, train/loss: 0.4124999940395355
Step: 16040, train/grad_norm: 11.210809707641602
Step: 16040, train/learning_rate: 3.091385224252008e-05
Step: 16040, train/epoch: 3.817229986190796
Step: 16050, train/loss: 0.44429999589920044
Step: 16050, train/grad_norm: 11.961180686950684
Step: 16050, train/learning_rate: 3.090195241384208e-05
Step: 16050, train/epoch: 3.8196096420288086
Step: 16060, train/loss: 0.5595999956130981
Step: 16060, train/grad_norm: 20.510671615600586
Step: 16060, train/learning_rate: 3.0890052585164085e-05
Step: 16060, train/epoch: 3.8219895362854004
Step: 16070, train/loss: 0.40209999680519104
Step: 16070, train/grad_norm: 3.4712371826171875
Step: 16070, train/learning_rate: 3.087815275648609e-05
Step: 16070, train/epoch: 3.824369430541992
Step: 16080, train/loss: 0.5522000193595886
Step: 16080, train/grad_norm: 22.795608520507812
Step: 16080, train/learning_rate: 3.086625292780809e-05
Step: 16080, train/epoch: 3.826749086380005
Step: 16090, train/loss: 0.4115000069141388
Step: 16090, train/grad_norm: 12.32815933227539
Step: 16090, train/learning_rate: 3.08543567371089e-05
Step: 16090, train/epoch: 3.8291289806365967
Step: 16100, train/loss: 0.4153999984264374
Step: 16100, train/grad_norm: 4.633702754974365
Step: 16100, train/learning_rate: 3.0842456908430904e-05
Step: 16100, train/epoch: 3.8315088748931885
Step: 16110, train/loss: 0.38940000534057617
Step: 16110, train/grad_norm: 14.649045944213867
Step: 16110, train/learning_rate: 3.083055707975291e-05
Step: 16110, train/epoch: 3.833888530731201
Step: 16120, train/loss: 0.4059999883174896
Step: 16120, train/grad_norm: 39.868003845214844
Step: 16120, train/learning_rate: 3.081865725107491e-05
Step: 16120, train/epoch: 3.836268424987793
Step: 16130, train/loss: 0.2973000109195709
Step: 16130, train/grad_norm: 3.6401047706604004
Step: 16130, train/learning_rate: 3.080675742239691e-05
Step: 16130, train/epoch: 3.8386483192443848
Step: 16140, train/loss: 0.4259999990463257
Step: 16140, train/grad_norm: 5.6883625984191895
Step: 16140, train/learning_rate: 3.079486123169772e-05
Step: 16140, train/epoch: 3.8410279750823975
Step: 16150, train/loss: 0.33250001072883606
Step: 16150, train/grad_norm: 2.79498291015625
Step: 16150, train/learning_rate: 3.0782961403019726e-05
Step: 16150, train/epoch: 3.8434078693389893
Step: 16160, train/loss: 0.4242999851703644
Step: 16160, train/grad_norm: 12.380053520202637
Step: 16160, train/learning_rate: 3.077106157434173e-05
Step: 16160, train/epoch: 3.845787763595581
Step: 16170, train/loss: 0.39010000228881836
Step: 16170, train/grad_norm: 7.220593452453613
Step: 16170, train/learning_rate: 3.075916174566373e-05
Step: 16170, train/epoch: 3.848167657852173
Step: 16180, train/loss: 0.4302000105381012
Step: 16180, train/grad_norm: 10.6942777633667
Step: 16180, train/learning_rate: 3.0747261916985735e-05
Step: 16180, train/epoch: 3.8505473136901855
Step: 16190, train/loss: 0.44530001282691956
Step: 16190, train/grad_norm: 8.071351051330566
Step: 16190, train/learning_rate: 3.0735365726286545e-05
Step: 16190, train/epoch: 3.8529272079467773
Step: 16200, train/loss: 0.3939000070095062
Step: 16200, train/grad_norm: 10.189793586730957
Step: 16200, train/learning_rate: 3.072346589760855e-05
Step: 16200, train/epoch: 3.855307102203369
Step: 16210, train/loss: 0.38580000400543213
Step: 16210, train/grad_norm: 9.933981895446777
Step: 16210, train/learning_rate: 3.071156606893055e-05
Step: 16210, train/epoch: 3.857686758041382
Step: 16220, train/loss: 0.3531999886035919
Step: 16220, train/grad_norm: 9.48613452911377
Step: 16220, train/learning_rate: 3.0699666240252554e-05
Step: 16220, train/epoch: 3.8600666522979736
Step: 16230, train/loss: 0.4738999903202057
Step: 16230, train/grad_norm: 21.871009826660156
Step: 16230, train/learning_rate: 3.068776641157456e-05
Step: 16230, train/epoch: 3.8624465465545654
Step: 16240, train/loss: 0.335999995470047
Step: 16240, train/grad_norm: 5.767021656036377
Step: 16240, train/learning_rate: 3.067587022087537e-05
Step: 16240, train/epoch: 3.864826202392578
Step: 16250, train/loss: 0.38260000944137573
Step: 16250, train/grad_norm: 5.704387187957764
Step: 16250, train/learning_rate: 3.066397039219737e-05
Step: 16250, train/epoch: 3.86720609664917
Step: 16260, train/loss: 0.2549999952316284
Step: 16260, train/grad_norm: 5.6357645988464355
Step: 16260, train/learning_rate: 3.0652070563519374e-05
Step: 16260, train/epoch: 3.8695859909057617
Step: 16270, train/loss: 0.32019999623298645
Step: 16270, train/grad_norm: 4.234262466430664
Step: 16270, train/learning_rate: 3.0640170734841377e-05
Step: 16270, train/epoch: 3.8719656467437744
Step: 16280, train/loss: 0.3686000108718872
Step: 16280, train/grad_norm: 11.289624214172363
Step: 16280, train/learning_rate: 3.062827090616338e-05
Step: 16280, train/epoch: 3.874345541000366
Step: 16290, train/loss: 0.4212000072002411
Step: 16290, train/grad_norm: 12.044913291931152
Step: 16290, train/learning_rate: 3.061637471546419e-05
Step: 16290, train/epoch: 3.876725435256958
Step: 16300, train/loss: 0.41179999709129333
Step: 16300, train/grad_norm: 6.835602283477783
Step: 16300, train/learning_rate: 3.060447488678619e-05
Step: 16300, train/epoch: 3.8791050910949707
Step: 16310, train/loss: 0.39410001039505005
Step: 16310, train/grad_norm: 9.376538276672363
Step: 16310, train/learning_rate: 3.0592575058108196e-05
Step: 16310, train/epoch: 3.8814849853515625
Step: 16320, train/loss: 0.2831000089645386
Step: 16320, train/grad_norm: 9.60155963897705
Step: 16320, train/learning_rate: 3.05806752294302e-05
Step: 16320, train/epoch: 3.8838648796081543
Step: 16330, train/loss: 0.6129000186920166
Step: 16330, train/grad_norm: 19.76569175720215
Step: 16330, train/learning_rate: 3.05687754007522e-05
Step: 16330, train/epoch: 3.886244535446167
Step: 16340, train/loss: 0.32350000739097595
Step: 16340, train/grad_norm: 7.275688171386719
Step: 16340, train/learning_rate: 3.055687921005301e-05
Step: 16340, train/epoch: 3.888624429702759
Step: 16350, train/loss: 0.450300008058548
Step: 16350, train/grad_norm: 10.170105934143066
Step: 16350, train/learning_rate: 3.0544979381375015e-05
Step: 16350, train/epoch: 3.8910043239593506
Step: 16360, train/loss: 0.29809999465942383
Step: 16360, train/grad_norm: 13.5230131149292
Step: 16360, train/learning_rate: 3.053307955269702e-05
Step: 16360, train/epoch: 3.8933842182159424
Step: 16370, train/loss: 0.3370000123977661
Step: 16370, train/grad_norm: 6.455235004425049
Step: 16370, train/learning_rate: 3.052117972401902e-05
Step: 16370, train/epoch: 3.895763874053955
Step: 16380, train/loss: 0.4740000069141388
Step: 16380, train/grad_norm: 8.901602745056152
Step: 16380, train/learning_rate: 3.0509281714330427e-05
Step: 16380, train/epoch: 3.898143768310547
Step: 16390, train/loss: 0.4625999927520752
Step: 16390, train/grad_norm: 35.88979721069336
Step: 16390, train/learning_rate: 3.049738188565243e-05
Step: 16390, train/epoch: 3.9005236625671387
Step: 16400, train/loss: 0.36500000953674316
Step: 16400, train/grad_norm: 7.924720287322998
Step: 16400, train/learning_rate: 3.0485483875963837e-05
Step: 16400, train/epoch: 3.9029033184051514
Step: 16410, train/loss: 0.5461000204086304
Step: 16410, train/grad_norm: 11.965620994567871
Step: 16410, train/learning_rate: 3.047358404728584e-05
Step: 16410, train/epoch: 3.905283212661743
Step: 16420, train/loss: 0.5047000050544739
Step: 16420, train/grad_norm: 14.052420616149902
Step: 16420, train/learning_rate: 3.0461684218607843e-05
Step: 16420, train/epoch: 3.907663106918335
Step: 16430, train/loss: 0.38760000467300415
Step: 16430, train/grad_norm: 28.421010971069336
Step: 16430, train/learning_rate: 3.044978620891925e-05
Step: 16430, train/epoch: 3.9100427627563477
Step: 16440, train/loss: 0.3939000070095062
Step: 16440, train/grad_norm: 14.970637321472168
Step: 16440, train/learning_rate: 3.0437886380241252e-05
Step: 16440, train/epoch: 3.9124226570129395
Step: 16450, train/loss: 0.2935999929904938
Step: 16450, train/grad_norm: 2.5614840984344482
Step: 16450, train/learning_rate: 3.042598837055266e-05
Step: 16450, train/epoch: 3.9148025512695312
Step: 16460, train/loss: 0.45190000534057617
Step: 16460, train/grad_norm: 17.058595657348633
Step: 16460, train/learning_rate: 3.0414088541874662e-05
Step: 16460, train/epoch: 3.917182207107544
Step: 16470, train/loss: 0.4156999886035919
Step: 16470, train/grad_norm: 7.736289024353027
Step: 16470, train/learning_rate: 3.0402188713196665e-05
Step: 16470, train/epoch: 3.9195621013641357
Step: 16480, train/loss: 0.4251999855041504
Step: 16480, train/grad_norm: 9.084675788879395
Step: 16480, train/learning_rate: 3.039029070350807e-05
Step: 16480, train/epoch: 3.9219419956207275
Step: 16490, train/loss: 0.5584999918937683
Step: 16490, train/grad_norm: 10.846577644348145
Step: 16490, train/learning_rate: 3.0378390874830075e-05
Step: 16490, train/epoch: 3.9243216514587402
Step: 16500, train/loss: 0.3328999876976013
Step: 16500, train/grad_norm: 4.649928569793701
Step: 16500, train/learning_rate: 3.036649286514148e-05
Step: 16500, train/epoch: 3.926701545715332
Step: 16510, train/loss: 0.40290001034736633
Step: 16510, train/grad_norm: 9.252696990966797
Step: 16510, train/learning_rate: 3.0354593036463484e-05
Step: 16510, train/epoch: 3.929081439971924
Step: 16520, train/loss: 0.42989999055862427
Step: 16520, train/grad_norm: 16.412574768066406
Step: 16520, train/learning_rate: 3.0342693207785487e-05
Step: 16520, train/epoch: 3.9314610958099365
Step: 16530, train/loss: 0.5317999720573425
Step: 16530, train/grad_norm: 30.76396942138672
Step: 16530, train/learning_rate: 3.0330795198096894e-05
Step: 16530, train/epoch: 3.9338409900665283
Step: 16540, train/loss: 0.34220001101493835
Step: 16540, train/grad_norm: 6.501945972442627
Step: 16540, train/learning_rate: 3.0318895369418897e-05
Step: 16540, train/epoch: 3.93622088432312
Step: 16550, train/loss: 0.30390000343322754
Step: 16550, train/grad_norm: 24.132699966430664
Step: 16550, train/learning_rate: 3.0306997359730303e-05
Step: 16550, train/epoch: 3.938600778579712
Step: 16560, train/loss: 0.3666999936103821
Step: 16560, train/grad_norm: 9.887857437133789
Step: 16560, train/learning_rate: 3.0295097531052306e-05
Step: 16560, train/epoch: 3.9409804344177246
Step: 16570, train/loss: 0.5121999979019165
Step: 16570, train/grad_norm: 6.802670001983643
Step: 16570, train/learning_rate: 3.028319770237431e-05
Step: 16570, train/epoch: 3.9433603286743164
Step: 16580, train/loss: 0.3864000141620636
Step: 16580, train/grad_norm: 8.685566902160645
Step: 16580, train/learning_rate: 3.0271299692685716e-05
Step: 16580, train/epoch: 3.945740222930908
Step: 16590, train/loss: 0.2874999940395355
Step: 16590, train/grad_norm: 9.430889129638672
Step: 16590, train/learning_rate: 3.025939986400772e-05
Step: 16590, train/epoch: 3.948119878768921
Step: 16600, train/loss: 0.45899999141693115
Step: 16600, train/grad_norm: 9.451533317565918
Step: 16600, train/learning_rate: 3.0247501854319125e-05
Step: 16600, train/epoch: 3.9504997730255127
Step: 16610, train/loss: 0.5486999750137329
Step: 16610, train/grad_norm: 9.369220733642578
Step: 16610, train/learning_rate: 3.023560202564113e-05
Step: 16610, train/epoch: 3.9528796672821045
Step: 16620, train/loss: 0.3792000114917755
Step: 16620, train/grad_norm: 3.57116961479187
Step: 16620, train/learning_rate: 3.022370219696313e-05
Step: 16620, train/epoch: 3.955259323120117
Step: 16630, train/loss: 0.47040000557899475
Step: 16630, train/grad_norm: 16.55527687072754
Step: 16630, train/learning_rate: 3.0211804187274538e-05
Step: 16630, train/epoch: 3.957639217376709
Step: 16640, train/loss: 0.3901999890804291
Step: 16640, train/grad_norm: 16.878192901611328
Step: 16640, train/learning_rate: 3.019990435859654e-05
Step: 16640, train/epoch: 3.960019111633301
Step: 16650, train/loss: 0.3702999949455261
Step: 16650, train/grad_norm: 6.334396839141846
Step: 16650, train/learning_rate: 3.0188006348907948e-05
Step: 16650, train/epoch: 3.9623987674713135
Step: 16660, train/loss: 0.31769999861717224
Step: 16660, train/grad_norm: 13.787055969238281
Step: 16660, train/learning_rate: 3.017610652022995e-05
Step: 16660, train/epoch: 3.9647786617279053
Step: 16670, train/loss: 0.2921000123023987
Step: 16670, train/grad_norm: 32.23256301879883
Step: 16670, train/learning_rate: 3.0164206691551954e-05
Step: 16670, train/epoch: 3.967158555984497
Step: 16680, train/loss: 0.4641999900341034
Step: 16680, train/grad_norm: 7.315036773681641
Step: 16680, train/learning_rate: 3.015230868186336e-05
Step: 16680, train/epoch: 3.9695382118225098
Step: 16690, train/loss: 0.4447999894618988
Step: 16690, train/grad_norm: 12.085972785949707
Step: 16690, train/learning_rate: 3.0140408853185363e-05
Step: 16690, train/epoch: 3.9719181060791016
Step: 16700, train/loss: 0.5393000245094299
Step: 16700, train/grad_norm: 9.852591514587402
Step: 16700, train/learning_rate: 3.012851084349677e-05
Step: 16700, train/epoch: 3.9742980003356934
Step: 16710, train/loss: 0.6638000011444092
Step: 16710, train/grad_norm: 13.815528869628906
Step: 16710, train/learning_rate: 3.0116611014818773e-05
Step: 16710, train/epoch: 3.976677656173706
Step: 16720, train/loss: 0.4410000145435333
Step: 16720, train/grad_norm: 7.029505252838135
Step: 16720, train/learning_rate: 3.0104711186140776e-05
Step: 16720, train/epoch: 3.979057550430298
Step: 16730, train/loss: 0.46239998936653137
Step: 16730, train/grad_norm: 7.362574100494385
Step: 16730, train/learning_rate: 3.0092813176452182e-05
Step: 16730, train/epoch: 3.9814374446868896
Step: 16740, train/loss: 0.4733999967575073
Step: 16740, train/grad_norm: 10.794297218322754
Step: 16740, train/learning_rate: 3.0080913347774185e-05
Step: 16740, train/epoch: 3.9838173389434814
Step: 16750, train/loss: 0.25369998812675476
Step: 16750, train/grad_norm: 6.510781288146973
Step: 16750, train/learning_rate: 3.0069015338085592e-05
Step: 16750, train/epoch: 3.986196994781494
Step: 16760, train/loss: 0.383899986743927
Step: 16760, train/grad_norm: 20.96904182434082
Step: 16760, train/learning_rate: 3.0057115509407595e-05
Step: 16760, train/epoch: 3.988576889038086
Step: 16770, train/loss: 0.47380000352859497
Step: 16770, train/grad_norm: 20.380077362060547
Step: 16770, train/learning_rate: 3.0045215680729598e-05
Step: 16770, train/epoch: 3.9909567832946777
Step: 16780, train/loss: 0.3093000054359436
Step: 16780, train/grad_norm: 12.320615768432617
Step: 16780, train/learning_rate: 3.0033317671041004e-05
Step: 16780, train/epoch: 3.9933364391326904
Step: 16790, train/loss: 0.3806000053882599
Step: 16790, train/grad_norm: 1.894405722618103
Step: 16790, train/learning_rate: 3.0021417842363007e-05
Step: 16790, train/epoch: 3.9957163333892822
Step: 16800, train/loss: 0.44519999623298645
Step: 16800, train/grad_norm: 6.114330291748047
Step: 16800, train/learning_rate: 3.0009519832674414e-05
Step: 16800, train/epoch: 3.998096227645874
Step: 16808, eval/loss: 0.7976657152175903
Step: 16808, eval/accuracy: 0.654171884059906
Step: 16808, eval/f1: 0.6515399217605591
Step: 16808, eval/runtime: 54.73210144042969
Step: 16808, eval/samples_per_second: 131.60499572753906
Step: 16808, eval/steps_per_second: 16.461999893188477
Step: 16808, train/epoch: 4.0
Step: 16810, train/loss: 0.3537999987602234
Step: 16810, train/grad_norm: 17.932435989379883
Step: 16810, train/learning_rate: 2.9997620003996417e-05
Step: 16810, train/epoch: 4.000475883483887
Step: 16820, train/loss: 0.30630001425743103
Step: 16820, train/grad_norm: 7.35239315032959
Step: 16820, train/learning_rate: 2.9985721994307823e-05
Step: 16820, train/epoch: 4.0028557777404785
Step: 16830, train/loss: 0.45010000467300415
Step: 16830, train/grad_norm: 7.954296588897705
Step: 16830, train/learning_rate: 2.9973822165629826e-05
Step: 16830, train/epoch: 4.00523567199707
Step: 16840, train/loss: 0.4426000118255615
Step: 16840, train/grad_norm: 13.535835266113281
Step: 16840, train/learning_rate: 2.996192233695183e-05
Step: 16840, train/epoch: 4.007615566253662
Step: 16850, train/loss: 0.3723999857902527
Step: 16850, train/grad_norm: 8.822309494018555
Step: 16850, train/learning_rate: 2.9950024327263236e-05
Step: 16850, train/epoch: 4.009995460510254
Step: 16860, train/loss: 0.42289999127388
Step: 16860, train/grad_norm: 18.959257125854492
Step: 16860, train/learning_rate: 2.993812449858524e-05
Step: 16860, train/epoch: 4.0123748779296875
Step: 16870, train/loss: 0.44690001010894775
Step: 16870, train/grad_norm: 13.39001178741455
Step: 16870, train/learning_rate: 2.9926226488896646e-05
Step: 16870, train/epoch: 4.014754772186279
Step: 16880, train/loss: 0.3440000116825104
Step: 16880, train/grad_norm: 22.529579162597656
Step: 16880, train/learning_rate: 2.991432666021865e-05
Step: 16880, train/epoch: 4.017134666442871
Step: 16890, train/loss: 0.38589999079704285
Step: 16890, train/grad_norm: 3.386960029602051
Step: 16890, train/learning_rate: 2.990242683154065e-05
Step: 16890, train/epoch: 4.019514560699463
Step: 16900, train/loss: 0.36800000071525574
Step: 16900, train/grad_norm: 8.47513198852539
Step: 16900, train/learning_rate: 2.9890528821852058e-05
Step: 16900, train/epoch: 4.021894454956055
Step: 16910, train/loss: 0.3822999894618988
Step: 16910, train/grad_norm: 8.511920928955078
Step: 16910, train/learning_rate: 2.987862899317406e-05
Step: 16910, train/epoch: 4.0242743492126465
Step: 16920, train/loss: 0.45879998803138733
Step: 16920, train/grad_norm: 48.249393463134766
Step: 16920, train/learning_rate: 2.9866730983485468e-05
Step: 16920, train/epoch: 4.02665376663208
Step: 16930, train/loss: 0.35499998927116394
Step: 16930, train/grad_norm: 5.774297714233398
Step: 16930, train/learning_rate: 2.985483115480747e-05
Step: 16930, train/epoch: 4.029033660888672
Step: 16940, train/loss: 0.38449999690055847
Step: 16940, train/grad_norm: 6.774257659912109
Step: 16940, train/learning_rate: 2.9842931326129474e-05
Step: 16940, train/epoch: 4.031413555145264
Step: 16950, train/loss: 0.4383000135421753
Step: 16950, train/grad_norm: 13.673851013183594
Step: 16950, train/learning_rate: 2.983103331644088e-05
Step: 16950, train/epoch: 4.0337934494018555
Step: 16960, train/loss: 0.3057999908924103
Step: 16960, train/grad_norm: 9.518285751342773
Step: 16960, train/learning_rate: 2.9819133487762883e-05
Step: 16960, train/epoch: 4.036173343658447
Step: 16970, train/loss: 0.4260999858379364
Step: 16970, train/grad_norm: 2.6864728927612305
Step: 16970, train/learning_rate: 2.980723547807429e-05
Step: 16970, train/epoch: 4.038553237915039
Step: 16980, train/loss: 0.4902999997138977
Step: 16980, train/grad_norm: 5.309053897857666
Step: 16980, train/learning_rate: 2.9795335649396293e-05
Step: 16980, train/epoch: 4.040932655334473
Step: 16990, train/loss: 0.3939000070095062
Step: 16990, train/grad_norm: 5.615418910980225
Step: 16990, train/learning_rate: 2.9783435820718296e-05
Step: 16990, train/epoch: 4.0433125495910645
Step: 17000, train/loss: 0.45669999718666077
Step: 17000, train/grad_norm: 16.770355224609375
Step: 17000, train/learning_rate: 2.9771537811029702e-05
Step: 17000, train/epoch: 4.045692443847656
Step: 17010, train/loss: 0.4113999903202057
Step: 17010, train/grad_norm: 13.012767791748047
Step: 17010, train/learning_rate: 2.9759637982351705e-05
Step: 17010, train/epoch: 4.048072338104248
Step: 17020, train/loss: 0.3393000066280365
Step: 17020, train/grad_norm: 7.210464954376221
Step: 17020, train/learning_rate: 2.9747739972663112e-05
Step: 17020, train/epoch: 4.05045223236084
Step: 17030, train/loss: 0.2953000068664551
Step: 17030, train/grad_norm: 11.502084732055664
Step: 17030, train/learning_rate: 2.9735840143985115e-05
Step: 17030, train/epoch: 4.052832126617432
Step: 17040, train/loss: 0.5582000017166138
Step: 17040, train/grad_norm: 8.226859092712402
Step: 17040, train/learning_rate: 2.9723940315307118e-05
Step: 17040, train/epoch: 4.055212020874023
Step: 17050, train/loss: 0.3625999987125397
Step: 17050, train/grad_norm: 7.392198085784912
Step: 17050, train/learning_rate: 2.9712042305618525e-05
Step: 17050, train/epoch: 4.057591438293457
Step: 17060, train/loss: 0.3142000138759613
Step: 17060, train/grad_norm: 4.792056560516357
Step: 17060, train/learning_rate: 2.9700142476940528e-05
Step: 17060, train/epoch: 4.059971332550049
Step: 17070, train/loss: 0.46889999508857727
Step: 17070, train/grad_norm: 15.062255859375
Step: 17070, train/learning_rate: 2.9688244467251934e-05
Step: 17070, train/epoch: 4.062351226806641
Step: 17080, train/loss: 0.48420000076293945
Step: 17080, train/grad_norm: 13.909991264343262
Step: 17080, train/learning_rate: 2.9676344638573937e-05
Step: 17080, train/epoch: 4.064731121063232
Step: 17090, train/loss: 0.3538999855518341
Step: 17090, train/grad_norm: 9.426217079162598
Step: 17090, train/learning_rate: 2.966444480989594e-05
Step: 17090, train/epoch: 4.067111015319824
Step: 17100, train/loss: 0.3529999852180481
Step: 17100, train/grad_norm: 13.07882022857666
Step: 17100, train/learning_rate: 2.9652546800207347e-05
Step: 17100, train/epoch: 4.069490909576416
Step: 17110, train/loss: 0.4323999881744385
Step: 17110, train/grad_norm: 10.3895845413208
Step: 17110, train/learning_rate: 2.964064697152935e-05
Step: 17110, train/epoch: 4.07187032699585
Step: 17120, train/loss: 0.45669999718666077
Step: 17120, train/grad_norm: 22.82911491394043
Step: 17120, train/learning_rate: 2.9628748961840756e-05
Step: 17120, train/epoch: 4.074250221252441
Step: 17130, train/loss: 0.510699987411499
Step: 17130, train/grad_norm: 11.8899564743042
Step: 17130, train/learning_rate: 2.961684913316276e-05
Step: 17130, train/epoch: 4.076630115509033
Step: 17140, train/loss: 0.3165000081062317
Step: 17140, train/grad_norm: 6.187920093536377
Step: 17140, train/learning_rate: 2.9604949304484762e-05
Step: 17140, train/epoch: 4.079010009765625
Step: 17150, train/loss: 0.3328000009059906
Step: 17150, train/grad_norm: 13.614948272705078
Step: 17150, train/learning_rate: 2.959305129479617e-05
Step: 17150, train/epoch: 4.081389904022217
Step: 17160, train/loss: 0.5284000039100647
Step: 17160, train/grad_norm: 18.36189842224121
Step: 17160, train/learning_rate: 2.9581151466118172e-05
Step: 17160, train/epoch: 4.083769798278809
Step: 17170, train/loss: 0.4952999949455261
Step: 17170, train/grad_norm: 6.896998405456543
Step: 17170, train/learning_rate: 2.956925345642958e-05
Step: 17170, train/epoch: 4.086149215698242
Step: 17180, train/loss: 0.48539999127388
Step: 17180, train/grad_norm: 3.671780586242676
Step: 17180, train/learning_rate: 2.955735362775158e-05
Step: 17180, train/epoch: 4.088529109954834
Step: 17190, train/loss: 0.42419999837875366
Step: 17190, train/grad_norm: 8.202156066894531
Step: 17190, train/learning_rate: 2.9545453799073584e-05
Step: 17190, train/epoch: 4.090909004211426
Step: 17200, train/loss: 0.5077999830245972
Step: 17200, train/grad_norm: 13.99325942993164
Step: 17200, train/learning_rate: 2.953355578938499e-05
Step: 17200, train/epoch: 4.093288898468018
Step: 17210, train/loss: 0.3050999939441681
Step: 17210, train/grad_norm: 2.1763694286346436
Step: 17210, train/learning_rate: 2.9521655960706994e-05
Step: 17210, train/epoch: 4.095668792724609
Step: 17220, train/loss: 0.33640000224113464
Step: 17220, train/grad_norm: 4.716011047363281
Step: 17220, train/learning_rate: 2.95097579510184e-05
Step: 17220, train/epoch: 4.098048686981201
Step: 17230, train/loss: 0.3813999891281128
Step: 17230, train/grad_norm: 8.11916732788086
Step: 17230, train/learning_rate: 2.9497858122340403e-05
Step: 17230, train/epoch: 4.100428581237793
Step: 17240, train/loss: 0.322299987077713
Step: 17240, train/grad_norm: 8.069822311401367
Step: 17240, train/learning_rate: 2.9485958293662407e-05
Step: 17240, train/epoch: 4.102807998657227
Step: 17250, train/loss: 0.36070001125335693
Step: 17250, train/grad_norm: 12.809311866760254
Step: 17250, train/learning_rate: 2.9474060283973813e-05
Step: 17250, train/epoch: 4.105187892913818
Step: 17260, train/loss: 0.27900001406669617
Step: 17260, train/grad_norm: 6.867569923400879
Step: 17260, train/learning_rate: 2.9462160455295816e-05
Step: 17260, train/epoch: 4.10756778717041
Step: 17270, train/loss: 0.29789999127388
Step: 17270, train/grad_norm: 10.904630661010742
Step: 17270, train/learning_rate: 2.9450262445607223e-05
Step: 17270, train/epoch: 4.109947681427002
Step: 17280, train/loss: 0.499099999666214
Step: 17280, train/grad_norm: 6.80726957321167
Step: 17280, train/learning_rate: 2.9438362616929226e-05
Step: 17280, train/epoch: 4.112327575683594
Step: 17290, train/loss: 0.45019999146461487
Step: 17290, train/grad_norm: 14.084492683410645
Step: 17290, train/learning_rate: 2.942646278825123e-05
Step: 17290, train/epoch: 4.1147074699401855
Step: 17300, train/loss: 0.5156000256538391
Step: 17300, train/grad_norm: 13.132572174072266
Step: 17300, train/learning_rate: 2.9414564778562635e-05
Step: 17300, train/epoch: 4.117086887359619
Step: 17310, train/loss: 0.5843999981880188
Step: 17310, train/grad_norm: 17.78082275390625
Step: 17310, train/learning_rate: 2.9402664949884638e-05
Step: 17310, train/epoch: 4.119466781616211
Step: 17320, train/loss: 0.41280001401901245
Step: 17320, train/grad_norm: 6.5388031005859375
Step: 17320, train/learning_rate: 2.9390766940196045e-05
Step: 17320, train/epoch: 4.121846675872803
Step: 17330, train/loss: 0.5200999975204468
Step: 17330, train/grad_norm: 7.604279518127441
Step: 17330, train/learning_rate: 2.9378867111518048e-05
Step: 17330, train/epoch: 4.1242265701293945
Step: 17340, train/loss: 0.2935999929904938
Step: 17340, train/grad_norm: 8.727806091308594
Step: 17340, train/learning_rate: 2.936696728284005e-05
Step: 17340, train/epoch: 4.126606464385986
Step: 17350, train/loss: 0.48820000886917114
Step: 17350, train/grad_norm: 8.645838737487793
Step: 17350, train/learning_rate: 2.9355069273151457e-05
Step: 17350, train/epoch: 4.128986358642578
Step: 17360, train/loss: 0.3993000090122223
Step: 17360, train/grad_norm: 18.87746810913086
Step: 17360, train/learning_rate: 2.934316944447346e-05
Step: 17360, train/epoch: 4.13136625289917
Step: 17370, train/loss: 0.28519999980926514
Step: 17370, train/grad_norm: 21.925228118896484
Step: 17370, train/learning_rate: 2.9331271434784867e-05
Step: 17370, train/epoch: 4.1337456703186035
Step: 17380, train/loss: 0.43970000743865967
Step: 17380, train/grad_norm: 30.379358291625977
Step: 17380, train/learning_rate: 2.931937160610687e-05
Step: 17380, train/epoch: 4.136125564575195
Step: 17390, train/loss: 0.4242999851703644
Step: 17390, train/grad_norm: 14.322586059570312
Step: 17390, train/learning_rate: 2.9307471777428873e-05
Step: 17390, train/epoch: 4.138505458831787
Step: 17400, train/loss: 0.4595000147819519
Step: 17400, train/grad_norm: 24.108749389648438
Step: 17400, train/learning_rate: 2.929557376774028e-05
Step: 17400, train/epoch: 4.140885353088379
Step: 17410, train/loss: 0.42179998755455017
Step: 17410, train/grad_norm: 13.132607460021973
Step: 17410, train/learning_rate: 2.9283673939062282e-05
Step: 17410, train/epoch: 4.143265247344971
Step: 17420, train/loss: 0.33340001106262207
Step: 17420, train/grad_norm: 11.91373062133789
Step: 17420, train/learning_rate: 2.927177592937369e-05
Step: 17420, train/epoch: 4.1456451416015625
Step: 17430, train/loss: 0.2793999910354614
Step: 17430, train/grad_norm: 10.018505096435547
Step: 17430, train/learning_rate: 2.9259876100695692e-05
Step: 17430, train/epoch: 4.148024559020996
Step: 17440, train/loss: 0.35409998893737793
Step: 17440, train/grad_norm: 11.959465026855469
Step: 17440, train/learning_rate: 2.9247976272017695e-05
Step: 17440, train/epoch: 4.150404453277588
Step: 17450, train/loss: 0.4032000005245209
Step: 17450, train/grad_norm: 13.97216796875
Step: 17450, train/learning_rate: 2.92360782623291e-05
Step: 17450, train/epoch: 4.15278434753418
Step: 17460, train/loss: 0.4805000126361847
Step: 17460, train/grad_norm: 24.528392791748047
Step: 17460, train/learning_rate: 2.9224178433651105e-05
Step: 17460, train/epoch: 4.1551642417907715
Step: 17470, train/loss: 0.28360000252723694
Step: 17470, train/grad_norm: 4.890997409820557
Step: 17470, train/learning_rate: 2.921228042396251e-05
Step: 17470, train/epoch: 4.157544136047363
Step: 17480, train/loss: 0.36959999799728394
Step: 17480, train/grad_norm: 10.171634674072266
Step: 17480, train/learning_rate: 2.9200380595284514e-05
Step: 17480, train/epoch: 4.159924030303955
Step: 17490, train/loss: 0.3928000032901764
Step: 17490, train/grad_norm: 6.391442775726318
Step: 17490, train/learning_rate: 2.9188480766606517e-05
Step: 17490, train/epoch: 4.162303447723389
Step: 17500, train/loss: 0.2816999852657318
Step: 17500, train/grad_norm: 7.194786071777344
Step: 17500, train/learning_rate: 2.9176582756917924e-05
Step: 17500, train/epoch: 4.1646833419799805
Step: 17510, train/loss: 0.39329999685287476
Step: 17510, train/grad_norm: 1.3875994682312012
Step: 17510, train/learning_rate: 2.9164682928239927e-05
Step: 17510, train/epoch: 4.167063236236572
Step: 17520, train/loss: 0.5501999855041504
Step: 17520, train/grad_norm: 13.453516960144043
Step: 17520, train/learning_rate: 2.9152784918551333e-05
Step: 17520, train/epoch: 4.169443130493164
Step: 17530, train/loss: 0.3589000105857849
Step: 17530, train/grad_norm: 9.413215637207031
Step: 17530, train/learning_rate: 2.9140885089873336e-05
Step: 17530, train/epoch: 4.171823024749756
Step: 17540, train/loss: 0.33180001378059387
Step: 17540, train/grad_norm: 13.21388053894043
Step: 17540, train/learning_rate: 2.9128987080184743e-05
Step: 17540, train/epoch: 4.174202919006348
Step: 17550, train/loss: 0.35670000314712524
Step: 17550, train/grad_norm: 8.130812644958496
Step: 17550, train/learning_rate: 2.9117087251506746e-05
Step: 17550, train/epoch: 4.1765828132629395
Step: 17560, train/loss: 0.49939998984336853
Step: 17560, train/grad_norm: 16.386816024780273
Step: 17560, train/learning_rate: 2.910518742282875e-05
Step: 17560, train/epoch: 4.178962230682373
Step: 17570, train/loss: 0.4602000117301941
Step: 17570, train/grad_norm: 8.350927352905273
Step: 17570, train/learning_rate: 2.9093289413140155e-05
Step: 17570, train/epoch: 4.181342124938965
Step: 17580, train/loss: 0.4284000098705292
Step: 17580, train/grad_norm: 12.443584442138672
Step: 17580, train/learning_rate: 2.908138958446216e-05
Step: 17580, train/epoch: 4.183722019195557
Step: 17590, train/loss: 0.3605000078678131
Step: 17590, train/grad_norm: 8.116901397705078
Step: 17590, train/learning_rate: 2.9069491574773565e-05
Step: 17590, train/epoch: 4.186101913452148
Step: 17600, train/loss: 0.35010001063346863
Step: 17600, train/grad_norm: 3.834815263748169
Step: 17600, train/learning_rate: 2.9057591746095568e-05
Step: 17600, train/epoch: 4.18848180770874
Step: 17610, train/loss: 0.42969998717308044
Step: 17610, train/grad_norm: 10.903146743774414
Step: 17610, train/learning_rate: 2.904569191741757e-05
Step: 17610, train/epoch: 4.190861701965332
Step: 17620, train/loss: 0.33730000257492065
Step: 17620, train/grad_norm: 11.34500789642334
Step: 17620, train/learning_rate: 2.9033793907728978e-05
Step: 17620, train/epoch: 4.193241119384766
Step: 17630, train/loss: 0.5580000281333923
Step: 17630, train/grad_norm: 6.7943267822265625
Step: 17630, train/learning_rate: 2.902189407905098e-05
Step: 17630, train/epoch: 4.195621013641357
Step: 17640, train/loss: 0.4652999937534332
Step: 17640, train/grad_norm: 6.161806583404541
Step: 17640, train/learning_rate: 2.9009996069362387e-05
Step: 17640, train/epoch: 4.198000907897949
Step: 17650, train/loss: 0.3343999981880188
Step: 17650, train/grad_norm: 6.954392433166504
Step: 17650, train/learning_rate: 2.899809624068439e-05
Step: 17650, train/epoch: 4.200380802154541
Step: 17660, train/loss: 0.4081999957561493
Step: 17660, train/grad_norm: 3.7211685180664062
Step: 17660, train/learning_rate: 2.8986196412006393e-05
Step: 17660, train/epoch: 4.202760696411133
Step: 17670, train/loss: 0.45329999923706055
Step: 17670, train/grad_norm: 9.773550987243652
Step: 17670, train/learning_rate: 2.89742984023178e-05
Step: 17670, train/epoch: 4.205140590667725
Step: 17680, train/loss: 0.3125999867916107
Step: 17680, train/grad_norm: 9.530461311340332
Step: 17680, train/learning_rate: 2.8962398573639803e-05
Step: 17680, train/epoch: 4.207520008087158
Step: 17690, train/loss: 0.3772999942302704
Step: 17690, train/grad_norm: 9.617058753967285
Step: 17690, train/learning_rate: 2.895050056395121e-05
Step: 17690, train/epoch: 4.20989990234375
Step: 17700, train/loss: 0.47859999537467957
Step: 17700, train/grad_norm: 20.911149978637695
Step: 17700, train/learning_rate: 2.8938600735273212e-05
Step: 17700, train/epoch: 4.212279796600342
Step: 17710, train/loss: 0.453900009393692
Step: 17710, train/grad_norm: 2.998220920562744
Step: 17710, train/learning_rate: 2.8926700906595215e-05
Step: 17710, train/epoch: 4.214659690856934
Step: 17720, train/loss: 0.3409999907016754
Step: 17720, train/grad_norm: 21.69706153869629
Step: 17720, train/learning_rate: 2.8914802896906622e-05
Step: 17720, train/epoch: 4.217039585113525
Step: 17730, train/loss: 0.41290000081062317
Step: 17730, train/grad_norm: 19.509769439697266
Step: 17730, train/learning_rate: 2.8902903068228625e-05
Step: 17730, train/epoch: 4.219419479370117
Step: 17740, train/loss: 0.46239998936653137
Step: 17740, train/grad_norm: 5.418638229370117
Step: 17740, train/learning_rate: 2.889100505854003e-05
Step: 17740, train/epoch: 4.221799373626709
Step: 17750, train/loss: 0.36419999599456787
Step: 17750, train/grad_norm: 16.843433380126953
Step: 17750, train/learning_rate: 2.8879105229862034e-05
Step: 17750, train/epoch: 4.224178791046143
Step: 17760, train/loss: 0.3506999909877777
Step: 17760, train/grad_norm: 8.523303985595703
Step: 17760, train/learning_rate: 2.8867205401184037e-05
Step: 17760, train/epoch: 4.226558685302734
Step: 17770, train/loss: 0.3930000066757202
Step: 17770, train/grad_norm: 22.18199348449707
Step: 17770, train/learning_rate: 2.8855307391495444e-05
Step: 17770, train/epoch: 4.228938579559326
Step: 17780, train/loss: 0.35440000891685486
Step: 17780, train/grad_norm: 6.215269088745117
Step: 17780, train/learning_rate: 2.8843407562817447e-05
Step: 17780, train/epoch: 4.231318473815918
Step: 17790, train/loss: 0.4408999979496002
Step: 17790, train/grad_norm: 7.736409664154053
Step: 17790, train/learning_rate: 2.8831509553128853e-05
Step: 17790, train/epoch: 4.23369836807251
Step: 17800, train/loss: 0.39640000462532043
Step: 17800, train/grad_norm: 18.224225997924805
Step: 17800, train/learning_rate: 2.8819609724450856e-05
Step: 17800, train/epoch: 4.236078262329102
Step: 17810, train/loss: 0.4530999958515167
Step: 17810, train/grad_norm: 25.47166633605957
Step: 17810, train/learning_rate: 2.880770989577286e-05
Step: 17810, train/epoch: 4.238457679748535
Step: 17820, train/loss: 0.3409999907016754
Step: 17820, train/grad_norm: 2.333906888961792
Step: 17820, train/learning_rate: 2.8795811886084266e-05
Step: 17820, train/epoch: 4.240837574005127
Step: 17830, train/loss: 0.305400013923645
Step: 17830, train/grad_norm: 7.479517936706543
Step: 17830, train/learning_rate: 2.878391205740627e-05
Step: 17830, train/epoch: 4.243217468261719
Step: 17840, train/loss: 0.4327000081539154
Step: 17840, train/grad_norm: 26.685365676879883
Step: 17840, train/learning_rate: 2.8772014047717676e-05
Step: 17840, train/epoch: 4.2455973625183105
Step: 17850, train/loss: 0.3089999854564667
Step: 17850, train/grad_norm: 12.017104148864746
Step: 17850, train/learning_rate: 2.876011421903968e-05
Step: 17850, train/epoch: 4.247977256774902
Step: 17860, train/loss: 0.39489999413490295
Step: 17860, train/grad_norm: 10.405759811401367
Step: 17860, train/learning_rate: 2.874821439036168e-05
Step: 17860, train/epoch: 4.250357151031494
Step: 17870, train/loss: 0.4381999969482422
Step: 17870, train/grad_norm: 13.863694190979004
Step: 17870, train/learning_rate: 2.8736316380673088e-05
Step: 17870, train/epoch: 4.252736568450928
Step: 17880, train/loss: 0.3725999891757965
Step: 17880, train/grad_norm: 8.224857330322266
Step: 17880, train/learning_rate: 2.872441655199509e-05
Step: 17880, train/epoch: 4.2551164627075195
Step: 17890, train/loss: 0.2572999894618988
Step: 17890, train/grad_norm: 11.400446891784668
Step: 17890, train/learning_rate: 2.8712518542306498e-05
Step: 17890, train/epoch: 4.257496356964111
Step: 17900, train/loss: 0.3434000015258789
Step: 17900, train/grad_norm: 2.6783106327056885
Step: 17900, train/learning_rate: 2.87006187136285e-05
Step: 17900, train/epoch: 4.259876251220703
Step: 17910, train/loss: 0.5828999876976013
Step: 17910, train/grad_norm: 7.990983963012695
Step: 17910, train/learning_rate: 2.8688718884950504e-05
Step: 17910, train/epoch: 4.262256145477295
Step: 17920, train/loss: 0.45559999346733093
Step: 17920, train/grad_norm: 14.605630874633789
Step: 17920, train/learning_rate: 2.867682087526191e-05
Step: 17920, train/epoch: 4.264636039733887
Step: 17930, train/loss: 0.44269999861717224
Step: 17930, train/grad_norm: 9.714376449584961
Step: 17930, train/learning_rate: 2.8664921046583913e-05
Step: 17930, train/epoch: 4.2670159339904785
Step: 17940, train/loss: 0.3370000123977661
Step: 17940, train/grad_norm: 19.410757064819336
Step: 17940, train/learning_rate: 2.865302303689532e-05
Step: 17940, train/epoch: 4.269395351409912
Step: 17950, train/loss: 0.38589999079704285
Step: 17950, train/grad_norm: 6.525824546813965
Step: 17950, train/learning_rate: 2.8641123208217323e-05
Step: 17950, train/epoch: 4.271775245666504
Step: 17960, train/loss: 0.26190000772476196
Step: 17960, train/grad_norm: 7.248272895812988
Step: 17960, train/learning_rate: 2.8629223379539326e-05
Step: 17960, train/epoch: 4.274155139923096
Step: 17970, train/loss: 0.5673999786376953
Step: 17970, train/grad_norm: 11.273699760437012
Step: 17970, train/learning_rate: 2.8617325369850732e-05
Step: 17970, train/epoch: 4.2765350341796875
Step: 17980, train/loss: 0.6050999760627747
Step: 17980, train/grad_norm: 31.255569458007812
Step: 17980, train/learning_rate: 2.8605425541172735e-05
Step: 17980, train/epoch: 4.278914928436279
Step: 17990, train/loss: 0.44769999384880066
Step: 17990, train/grad_norm: 9.739011764526367
Step: 17990, train/learning_rate: 2.8593527531484142e-05
Step: 17990, train/epoch: 4.281294822692871
Step: 18000, train/loss: 0.3984000086784363
Step: 18000, train/grad_norm: 21.820667266845703
Step: 18000, train/learning_rate: 2.8581627702806145e-05
Step: 18000, train/epoch: 4.283674240112305
Step: 18010, train/loss: 0.4262000024318695
Step: 18010, train/grad_norm: 19.505884170532227
Step: 18010, train/learning_rate: 2.8569727874128148e-05
Step: 18010, train/epoch: 4.2860541343688965
Step: 18020, train/loss: 0.35920000076293945
Step: 18020, train/grad_norm: 4.9570136070251465
Step: 18020, train/learning_rate: 2.8557829864439555e-05
Step: 18020, train/epoch: 4.288434028625488
Step: 18030, train/loss: 0.427700012922287
Step: 18030, train/grad_norm: 8.967705726623535
Step: 18030, train/learning_rate: 2.8545930035761558e-05
Step: 18030, train/epoch: 4.29081392288208
Step: 18040, train/loss: 0.31929999589920044
Step: 18040, train/grad_norm: 10.618271827697754
Step: 18040, train/learning_rate: 2.8534032026072964e-05
Step: 18040, train/epoch: 4.293193817138672
Step: 18050, train/loss: 0.4487999975681305
Step: 18050, train/grad_norm: 10.23866081237793
Step: 18050, train/learning_rate: 2.8522132197394967e-05
Step: 18050, train/epoch: 4.295573711395264
Step: 18060, train/loss: 0.336899995803833
Step: 18060, train/grad_norm: 4.712980270385742
Step: 18060, train/learning_rate: 2.851023236871697e-05
Step: 18060, train/epoch: 4.297953128814697
Step: 18070, train/loss: 0.44850000739097595
Step: 18070, train/grad_norm: 11.668025970458984
Step: 18070, train/learning_rate: 2.8498334359028377e-05
Step: 18070, train/epoch: 4.300333023071289
Step: 18080, train/loss: 0.2623000144958496
Step: 18080, train/grad_norm: 1.845137596130371
Step: 18080, train/learning_rate: 2.848643453035038e-05
Step: 18080, train/epoch: 4.302712917327881
Step: 18090, train/loss: 0.4113999903202057
Step: 18090, train/grad_norm: 9.99659252166748
Step: 18090, train/learning_rate: 2.8474536520661786e-05
Step: 18090, train/epoch: 4.305092811584473
Step: 18100, train/loss: 0.3416000008583069
Step: 18100, train/grad_norm: 3.121469020843506
Step: 18100, train/learning_rate: 2.846263669198379e-05
Step: 18100, train/epoch: 4.3074727058410645
Step: 18110, train/loss: 0.296999990940094
Step: 18110, train/grad_norm: 1.8957098722457886
Step: 18110, train/learning_rate: 2.8450736863305792e-05
Step: 18110, train/epoch: 4.309852600097656
Step: 18120, train/loss: 0.4262999892234802
Step: 18120, train/grad_norm: 10.211150169372559
Step: 18120, train/learning_rate: 2.84388388536172e-05
Step: 18120, train/epoch: 4.312232494354248
Step: 18130, train/loss: 0.3230000138282776
Step: 18130, train/grad_norm: 0.9734126925468445
Step: 18130, train/learning_rate: 2.8426939024939202e-05
Step: 18130, train/epoch: 4.314611911773682
Step: 18140, train/loss: 0.30390000343322754
Step: 18140, train/grad_norm: 4.2080583572387695
Step: 18140, train/learning_rate: 2.841504101525061e-05
Step: 18140, train/epoch: 4.316991806030273
Step: 18150, train/loss: 0.4456999897956848
Step: 18150, train/grad_norm: 15.499822616577148
Step: 18150, train/learning_rate: 2.840314118657261e-05
Step: 18150, train/epoch: 4.319371700286865
Step: 18160, train/loss: 0.3441999852657318
Step: 18160, train/grad_norm: 19.890790939331055
Step: 18160, train/learning_rate: 2.8391241357894614e-05
Step: 18160, train/epoch: 4.321751594543457
Step: 18170, train/loss: 0.28459998965263367
Step: 18170, train/grad_norm: 5.023922443389893
Step: 18170, train/learning_rate: 2.837934334820602e-05
Step: 18170, train/epoch: 4.324131488800049
Step: 18180, train/loss: 0.4449000060558319
Step: 18180, train/grad_norm: 21.202869415283203
Step: 18180, train/learning_rate: 2.8367443519528024e-05
Step: 18180, train/epoch: 4.326511383056641
Step: 18190, train/loss: 0.3537999987602234
Step: 18190, train/grad_norm: 2.6340298652648926
Step: 18190, train/learning_rate: 2.835554550983943e-05
Step: 18190, train/epoch: 4.328890800476074
Step: 18200, train/loss: 0.5170000195503235
Step: 18200, train/grad_norm: 18.03422737121582
Step: 18200, train/learning_rate: 2.8343645681161433e-05
Step: 18200, train/epoch: 4.331270694732666
Step: 18210, train/loss: 0.48170000314712524
Step: 18210, train/grad_norm: 8.111187934875488
Step: 18210, train/learning_rate: 2.833174767147284e-05
Step: 18210, train/epoch: 4.333650588989258
Step: 18220, train/loss: 0.2644999921321869
Step: 18220, train/grad_norm: 1.5771236419677734
Step: 18220, train/learning_rate: 2.8319847842794843e-05
Step: 18220, train/epoch: 4.33603048324585
Step: 18230, train/loss: 0.4645000100135803
Step: 18230, train/grad_norm: 6.898699760437012
Step: 18230, train/learning_rate: 2.8307948014116846e-05
Step: 18230, train/epoch: 4.338410377502441
Step: 18240, train/loss: 0.531499981880188
Step: 18240, train/grad_norm: 18.81825065612793
Step: 18240, train/learning_rate: 2.8296050004428253e-05
Step: 18240, train/epoch: 4.340790271759033
Step: 18250, train/loss: 0.3637000024318695
Step: 18250, train/grad_norm: 10.190471649169922
Step: 18250, train/learning_rate: 2.8284150175750256e-05
Step: 18250, train/epoch: 4.343169689178467
Step: 18260, train/loss: 0.36800000071525574
Step: 18260, train/grad_norm: 18.404884338378906
Step: 18260, train/learning_rate: 2.8272252166061662e-05
Step: 18260, train/epoch: 4.345549583435059
Step: 18270, train/loss: 0.2667999863624573
Step: 18270, train/grad_norm: 9.089971542358398
Step: 18270, train/learning_rate: 2.8260352337383665e-05
Step: 18270, train/epoch: 4.34792947769165
Step: 18280, train/loss: 0.36980000138282776
Step: 18280, train/grad_norm: 12.407340049743652
Step: 18280, train/learning_rate: 2.8248452508705668e-05
Step: 18280, train/epoch: 4.350309371948242
Step: 18290, train/loss: 0.33410000801086426
Step: 18290, train/grad_norm: 26.792062759399414
Step: 18290, train/learning_rate: 2.8236554499017075e-05
Step: 18290, train/epoch: 4.352689266204834
Step: 18300, train/loss: 0.47519999742507935
Step: 18300, train/grad_norm: 8.702232360839844
Step: 18300, train/learning_rate: 2.8224654670339078e-05
Step: 18300, train/epoch: 4.355069160461426
Step: 18310, train/loss: 0.3578000068664551
Step: 18310, train/grad_norm: 11.211339950561523
Step: 18310, train/learning_rate: 2.8212756660650484e-05
Step: 18310, train/epoch: 4.357449054718018
Step: 18320, train/loss: 0.4092000126838684
Step: 18320, train/grad_norm: 10.508088111877441
Step: 18320, train/learning_rate: 2.8200856831972487e-05
Step: 18320, train/epoch: 4.359828472137451
Step: 18330, train/loss: 0.5284000039100647
Step: 18330, train/grad_norm: 11.048650741577148
Step: 18330, train/learning_rate: 2.818895700329449e-05
Step: 18330, train/epoch: 4.362208366394043
Step: 18340, train/loss: 0.3440000116825104
Step: 18340, train/grad_norm: 16.559694290161133
Step: 18340, train/learning_rate: 2.8177058993605897e-05
Step: 18340, train/epoch: 4.364588260650635
Step: 18350, train/loss: 0.3192000091075897
Step: 18350, train/grad_norm: 19.42989158630371
Step: 18350, train/learning_rate: 2.81651591649279e-05
Step: 18350, train/epoch: 4.366968154907227
Step: 18360, train/loss: 0.3720000088214874
Step: 18360, train/grad_norm: 7.0433549880981445
Step: 18360, train/learning_rate: 2.8153261155239306e-05
Step: 18360, train/epoch: 4.369348049163818
Step: 18370, train/loss: 0.40610000491142273
Step: 18370, train/grad_norm: 5.521788120269775
Step: 18370, train/learning_rate: 2.814136132656131e-05
Step: 18370, train/epoch: 4.37172794342041
Step: 18380, train/loss: 0.49549999833106995
Step: 18380, train/grad_norm: 12.385665893554688
Step: 18380, train/learning_rate: 2.8129461497883312e-05
Step: 18380, train/epoch: 4.374107360839844
Step: 18390, train/loss: 0.3546999990940094
Step: 18390, train/grad_norm: 7.853784084320068
Step: 18390, train/learning_rate: 2.811756348819472e-05
Step: 18390, train/epoch: 4.3764872550964355
Step: 18400, train/loss: 0.41510000824928284
Step: 18400, train/grad_norm: 2.6222047805786133
Step: 18400, train/learning_rate: 2.8105663659516722e-05
Step: 18400, train/epoch: 4.378867149353027
Step: 18410, train/loss: 0.5110999941825867
Step: 18410, train/grad_norm: 9.532963752746582
Step: 18410, train/learning_rate: 2.809376564982813e-05
Step: 18410, train/epoch: 4.381247043609619
Step: 18420, train/loss: 0.46950000524520874
Step: 18420, train/grad_norm: 28.134519577026367
Step: 18420, train/learning_rate: 2.808186582115013e-05
Step: 18420, train/epoch: 4.383626937866211
Step: 18430, train/loss: 0.4339999854564667
Step: 18430, train/grad_norm: 12.78414249420166
Step: 18430, train/learning_rate: 2.8069965992472135e-05
Step: 18430, train/epoch: 4.386006832122803
Step: 18440, train/loss: 0.4302999973297119
Step: 18440, train/grad_norm: 11.461554527282715
Step: 18440, train/learning_rate: 2.805806798278354e-05
Step: 18440, train/epoch: 4.388386249542236
Step: 18450, train/loss: 0.28679999709129333
Step: 18450, train/grad_norm: 6.622982025146484
Step: 18450, train/learning_rate: 2.8046168154105544e-05
Step: 18450, train/epoch: 4.390766143798828
Step: 18460, train/loss: 0.3928000032901764
Step: 18460, train/grad_norm: 26.37763023376465
Step: 18460, train/learning_rate: 2.803427014441695e-05
Step: 18460, train/epoch: 4.39314603805542
Step: 18470, train/loss: 0.44760000705718994
Step: 18470, train/grad_norm: 13.153715133666992
Step: 18470, train/learning_rate: 2.8022370315738954e-05
Step: 18470, train/epoch: 4.395525932312012
Step: 18480, train/loss: 0.33899998664855957
Step: 18480, train/grad_norm: 8.551711082458496
Step: 18480, train/learning_rate: 2.8010470487060957e-05
Step: 18480, train/epoch: 4.3979058265686035
Step: 18490, train/loss: 0.31220000982284546
Step: 18490, train/grad_norm: 19.612316131591797
Step: 18490, train/learning_rate: 2.7998572477372363e-05
Step: 18490, train/epoch: 4.400285720825195
Step: 18500, train/loss: 0.3630000054836273
Step: 18500, train/grad_norm: 7.442731857299805
Step: 18500, train/learning_rate: 2.7986672648694366e-05
Step: 18500, train/epoch: 4.402665615081787
Step: 18510, train/loss: 0.45969998836517334
Step: 18510, train/grad_norm: 18.11928367614746
Step: 18510, train/learning_rate: 2.7974774639005773e-05
Step: 18510, train/epoch: 4.405045032501221
Step: 18520, train/loss: 0.3440999984741211
Step: 18520, train/grad_norm: 20.07868194580078
Step: 18520, train/learning_rate: 2.7962874810327776e-05
Step: 18520, train/epoch: 4.4074249267578125
Step: 18530, train/loss: 0.33500000834465027
Step: 18530, train/grad_norm: 23.533720016479492
Step: 18530, train/learning_rate: 2.795097498164978e-05
Step: 18530, train/epoch: 4.409804821014404
Step: 18540, train/loss: 0.27320000529289246
Step: 18540, train/grad_norm: 6.282749176025391
Step: 18540, train/learning_rate: 2.7939076971961185e-05
Step: 18540, train/epoch: 4.412184715270996
Step: 18550, train/loss: 0.3199000060558319
Step: 18550, train/grad_norm: 27.164411544799805
Step: 18550, train/learning_rate: 2.792717714328319e-05
Step: 18550, train/epoch: 4.414564609527588
Step: 18560, train/loss: 0.36500000953674316
Step: 18560, train/grad_norm: 11.59330940246582
Step: 18560, train/learning_rate: 2.7915279133594595e-05
Step: 18560, train/epoch: 4.41694450378418
Step: 18570, train/loss: 0.36640000343322754
Step: 18570, train/grad_norm: 9.335699081420898
Step: 18570, train/learning_rate: 2.7903379304916598e-05
Step: 18570, train/epoch: 4.419323921203613
Step: 18580, train/loss: 0.28600001335144043
Step: 18580, train/grad_norm: 4.236179351806641
Step: 18580, train/learning_rate: 2.78914794762386e-05
Step: 18580, train/epoch: 4.421703815460205
Step: 18590, train/loss: 0.3767000138759613
Step: 18590, train/grad_norm: 26.273794174194336
Step: 18590, train/learning_rate: 2.7879581466550007e-05
Step: 18590, train/epoch: 4.424083709716797
Step: 18600, train/loss: 0.3806999921798706
Step: 18600, train/grad_norm: 12.214543342590332
Step: 18600, train/learning_rate: 2.786768163787201e-05
Step: 18600, train/epoch: 4.426463603973389
Step: 18610, train/loss: 0.38929998874664307
Step: 18610, train/grad_norm: 10.550541877746582
Step: 18610, train/learning_rate: 2.7855783628183417e-05
Step: 18610, train/epoch: 4.4288434982299805
Step: 18620, train/loss: 0.3808000087738037
Step: 18620, train/grad_norm: 13.346067428588867
Step: 18620, train/learning_rate: 2.784388379950542e-05
Step: 18620, train/epoch: 4.431223392486572
Step: 18630, train/loss: 0.3714999854564667
Step: 18630, train/grad_norm: 19.018381118774414
Step: 18630, train/learning_rate: 2.7831983970827423e-05
Step: 18630, train/epoch: 4.433602809906006
Step: 18640, train/loss: 0.579800009727478
Step: 18640, train/grad_norm: 19.491973876953125
Step: 18640, train/learning_rate: 2.782008596113883e-05
Step: 18640, train/epoch: 4.435982704162598
Step: 18650, train/loss: 0.3571000099182129
Step: 18650, train/grad_norm: 18.054759979248047
Step: 18650, train/learning_rate: 2.7808186132460833e-05
Step: 18650, train/epoch: 4.4383625984191895
Step: 18660, train/loss: 0.424699991941452
Step: 18660, train/grad_norm: 15.805869102478027
Step: 18660, train/learning_rate: 2.779628812277224e-05
Step: 18660, train/epoch: 4.440742492675781
Step: 18670, train/loss: 0.40610000491142273
Step: 18670, train/grad_norm: 6.650102615356445
Step: 18670, train/learning_rate: 2.7784388294094242e-05
Step: 18670, train/epoch: 4.443122386932373
Step: 18680, train/loss: 0.5534999966621399
Step: 18680, train/grad_norm: 7.336965560913086
Step: 18680, train/learning_rate: 2.7772488465416245e-05
Step: 18680, train/epoch: 4.445502281188965
Step: 18690, train/loss: 0.3837999999523163
Step: 18690, train/grad_norm: 7.814332485198975
Step: 18690, train/learning_rate: 2.7760590455727652e-05
Step: 18690, train/epoch: 4.447882175445557
Step: 18700, train/loss: 0.3163999915122986
Step: 18700, train/grad_norm: 16.260009765625
Step: 18700, train/learning_rate: 2.7748690627049655e-05
Step: 18700, train/epoch: 4.45026159286499
Step: 18710, train/loss: 0.3343999981880188
Step: 18710, train/grad_norm: 9.56332015991211
Step: 18710, train/learning_rate: 2.773679261736106e-05
Step: 18710, train/epoch: 4.452641487121582
Step: 18720, train/loss: 0.49720001220703125
Step: 18720, train/grad_norm: 9.757513999938965
Step: 18720, train/learning_rate: 2.7724892788683064e-05
Step: 18720, train/epoch: 4.455021381378174
Step: 18730, train/loss: 0.3772999942302704
Step: 18730, train/grad_norm: 5.1887407302856445
Step: 18730, train/learning_rate: 2.7712992960005067e-05
Step: 18730, train/epoch: 4.457401275634766
Step: 18740, train/loss: 0.2946999967098236
Step: 18740, train/grad_norm: 4.775925159454346
Step: 18740, train/learning_rate: 2.7701094950316474e-05
Step: 18740, train/epoch: 4.459781169891357
Step: 18750, train/loss: 0.4803999960422516
Step: 18750, train/grad_norm: 14.68192195892334
Step: 18750, train/learning_rate: 2.7689195121638477e-05
Step: 18750, train/epoch: 4.462161064147949
Step: 18760, train/loss: 0.33070001006126404
Step: 18760, train/grad_norm: 5.369907379150391
Step: 18760, train/learning_rate: 2.7677297111949883e-05
Step: 18760, train/epoch: 4.464540481567383
Step: 18770, train/loss: 0.4230000078678131
Step: 18770, train/grad_norm: 10.078015327453613
Step: 18770, train/learning_rate: 2.7665397283271886e-05
Step: 18770, train/epoch: 4.466920375823975
Step: 18780, train/loss: 0.4717000126838684
Step: 18780, train/grad_norm: 10.376296043395996
Step: 18780, train/learning_rate: 2.765349745459389e-05
Step: 18780, train/epoch: 4.469300270080566
Step: 18790, train/loss: 0.35920000076293945
Step: 18790, train/grad_norm: 14.553756713867188
Step: 18790, train/learning_rate: 2.7641599444905296e-05
Step: 18790, train/epoch: 4.471680164337158
Step: 18800, train/loss: 0.31290000677108765
Step: 18800, train/grad_norm: 10.445337295532227
Step: 18800, train/learning_rate: 2.76296996162273e-05
Step: 18800, train/epoch: 4.47406005859375
Step: 18810, train/loss: 0.3100000023841858
Step: 18810, train/grad_norm: 6.471155643463135
Step: 18810, train/learning_rate: 2.7617801606538706e-05
Step: 18810, train/epoch: 4.476439952850342
Step: 18820, train/loss: 0.38420000672340393
Step: 18820, train/grad_norm: 18.250524520874023
Step: 18820, train/learning_rate: 2.760590177786071e-05
Step: 18820, train/epoch: 4.478819847106934
Step: 18830, train/loss: 0.40860000252723694
Step: 18830, train/grad_norm: 9.44156551361084
Step: 18830, train/learning_rate: 2.759400194918271e-05
Step: 18830, train/epoch: 4.481199264526367
Step: 18840, train/loss: 0.5457000136375427
Step: 18840, train/grad_norm: 31.34169578552246
Step: 18840, train/learning_rate: 2.7582103939494118e-05
Step: 18840, train/epoch: 4.483579158782959
Step: 18850, train/loss: 0.36340001225471497
Step: 18850, train/grad_norm: 12.59176254272461
Step: 18850, train/learning_rate: 2.757020411081612e-05
Step: 18850, train/epoch: 4.485959053039551
Step: 18860, train/loss: 0.41100001335144043
Step: 18860, train/grad_norm: 4.130402565002441
Step: 18860, train/learning_rate: 2.7558306101127528e-05
Step: 18860, train/epoch: 4.488338947296143
Step: 18870, train/loss: 0.43130001425743103
Step: 18870, train/grad_norm: 2.7272887229919434
Step: 18870, train/learning_rate: 2.754640627244953e-05
Step: 18870, train/epoch: 4.490718841552734
Step: 18880, train/loss: 0.42179998755455017
Step: 18880, train/grad_norm: 14.382872581481934
Step: 18880, train/learning_rate: 2.7534508262760937e-05
Step: 18880, train/epoch: 4.493098735809326
Step: 18890, train/loss: 0.42739999294281006
Step: 18890, train/grad_norm: 7.274141788482666
Step: 18890, train/learning_rate: 2.752260843408294e-05
Step: 18890, train/epoch: 4.49547815322876
Step: 18900, train/loss: 0.3889999985694885
Step: 18900, train/grad_norm: 9.674725532531738
Step: 18900, train/learning_rate: 2.7510708605404943e-05
Step: 18900, train/epoch: 4.497858047485352
Step: 18910, train/loss: 0.6646999716758728
Step: 18910, train/grad_norm: 6.650008678436279
Step: 18910, train/learning_rate: 2.749881059571635e-05
Step: 18910, train/epoch: 4.500237941741943
Step: 18920, train/loss: 0.3343000113964081
Step: 18920, train/grad_norm: 15.603764533996582
Step: 18920, train/learning_rate: 2.7486910767038353e-05
Step: 18920, train/epoch: 4.502617835998535
Step: 18930, train/loss: 0.2549999952316284
Step: 18930, train/grad_norm: 4.346810340881348
Step: 18930, train/learning_rate: 2.747501275734976e-05
Step: 18930, train/epoch: 4.504997730255127
Step: 18940, train/loss: 0.4722999930381775
Step: 18940, train/grad_norm: 8.840043067932129
Step: 18940, train/learning_rate: 2.7463112928671762e-05
Step: 18940, train/epoch: 4.507377624511719
Step: 18950, train/loss: 0.28439998626708984
Step: 18950, train/grad_norm: 19.097463607788086
Step: 18950, train/learning_rate: 2.7451213099993765e-05
Step: 18950, train/epoch: 4.509757041931152
Step: 18960, train/loss: 0.5037000179290771
Step: 18960, train/grad_norm: 2.758913993835449
Step: 18960, train/learning_rate: 2.7439315090305172e-05
Step: 18960, train/epoch: 4.512136936187744
Step: 18970, train/loss: 0.3707999885082245
Step: 18970, train/grad_norm: 12.417680740356445
Step: 18970, train/learning_rate: 2.7427415261627175e-05
Step: 18970, train/epoch: 4.514516830444336
Step: 18980, train/loss: 0.40380001068115234
Step: 18980, train/grad_norm: 12.688131332397461
Step: 18980, train/learning_rate: 2.741551725193858e-05
Step: 18980, train/epoch: 4.516896724700928
Step: 18990, train/loss: 0.29510000348091125
Step: 18990, train/grad_norm: 13.95466136932373
Step: 18990, train/learning_rate: 2.7403617423260584e-05
Step: 18990, train/epoch: 4.5192766189575195
Step: 19000, train/loss: 0.39809998869895935
Step: 19000, train/grad_norm: 12.573534965515137
Step: 19000, train/learning_rate: 2.7391717594582587e-05
Step: 19000, train/epoch: 4.521656513214111
Step: 19010, train/loss: 0.2676999866962433
Step: 19010, train/grad_norm: 15.064836502075195
Step: 19010, train/learning_rate: 2.7379819584893994e-05
Step: 19010, train/epoch: 4.524036407470703
Step: 19020, train/loss: 0.4925999939441681
Step: 19020, train/grad_norm: 6.766750812530518
Step: 19020, train/learning_rate: 2.7367919756215997e-05
Step: 19020, train/epoch: 4.526415824890137
Step: 19030, train/loss: 0.5026000142097473
Step: 19030, train/grad_norm: 8.896183013916016
Step: 19030, train/learning_rate: 2.7356021746527404e-05
Step: 19030, train/epoch: 4.5287957191467285
Step: 19040, train/loss: 0.37709999084472656
Step: 19040, train/grad_norm: 14.077718734741211
Step: 19040, train/learning_rate: 2.7344121917849407e-05
Step: 19040, train/epoch: 4.53117561340332
Step: 19050, train/loss: 0.31439998745918274
Step: 19050, train/grad_norm: 3.635103225708008
Step: 19050, train/learning_rate: 2.733222208917141e-05
Step: 19050, train/epoch: 4.533555507659912
Step: 19060, train/loss: 0.4316999912261963
Step: 19060, train/grad_norm: 8.305500984191895
Step: 19060, train/learning_rate: 2.7320324079482816e-05
Step: 19060, train/epoch: 4.535935401916504
Step: 19070, train/loss: 0.4196000099182129
Step: 19070, train/grad_norm: 26.444007873535156
Step: 19070, train/learning_rate: 2.730842425080482e-05
Step: 19070, train/epoch: 4.538315296173096
Step: 19080, train/loss: 0.361299991607666
Step: 19080, train/grad_norm: 1.7124968767166138
Step: 19080, train/learning_rate: 2.7296526241116226e-05
Step: 19080, train/epoch: 4.540694713592529
Step: 19090, train/loss: 0.436599999666214
Step: 19090, train/grad_norm: 3.456582546234131
Step: 19090, train/learning_rate: 2.728462641243823e-05
Step: 19090, train/epoch: 4.543074607849121
Step: 19100, train/loss: 0.3700999915599823
Step: 19100, train/grad_norm: 10.336929321289062
Step: 19100, train/learning_rate: 2.7272726583760232e-05
Step: 19100, train/epoch: 4.545454502105713
Step: 19110, train/loss: 0.5942000150680542
Step: 19110, train/grad_norm: 9.676332473754883
Step: 19110, train/learning_rate: 2.7260828574071638e-05
Step: 19110, train/epoch: 4.547834396362305
Step: 19120, train/loss: 0.5752000212669373
Step: 19120, train/grad_norm: 9.05057430267334
Step: 19120, train/learning_rate: 2.724892874539364e-05
Step: 19120, train/epoch: 4.5502142906188965
Step: 19130, train/loss: 0.27489998936653137
Step: 19130, train/grad_norm: 2.684483289718628
Step: 19130, train/learning_rate: 2.7237030735705048e-05
Step: 19130, train/epoch: 4.552594184875488
Step: 19140, train/loss: 0.39640000462532043
Step: 19140, train/grad_norm: 18.22479248046875
Step: 19140, train/learning_rate: 2.722513090702705e-05
Step: 19140, train/epoch: 4.554973602294922
Step: 19150, train/loss: 0.3538999855518341
Step: 19150, train/grad_norm: 12.913336753845215
Step: 19150, train/learning_rate: 2.7213231078349054e-05
Step: 19150, train/epoch: 4.557353496551514
Step: 19160, train/loss: 0.3855000138282776
Step: 19160, train/grad_norm: 18.324934005737305
Step: 19160, train/learning_rate: 2.720133306866046e-05
Step: 19160, train/epoch: 4.5597333908081055
Step: 19170, train/loss: 0.33550000190734863
Step: 19170, train/grad_norm: 11.539725303649902
Step: 19170, train/learning_rate: 2.7189433239982463e-05
Step: 19170, train/epoch: 4.562113285064697
Step: 19180, train/loss: 0.47130000591278076
Step: 19180, train/grad_norm: 11.866941452026367
Step: 19180, train/learning_rate: 2.717753523029387e-05
Step: 19180, train/epoch: 4.564493179321289
Step: 19190, train/loss: 0.3012000024318695
Step: 19190, train/grad_norm: 21.178232192993164
Step: 19190, train/learning_rate: 2.7165635401615873e-05
Step: 19190, train/epoch: 4.566873073577881
Step: 19200, train/loss: 0.32440000772476196
Step: 19200, train/grad_norm: 10.233260154724121
Step: 19200, train/learning_rate: 2.7153735572937876e-05
Step: 19200, train/epoch: 4.569252967834473
Step: 19210, train/loss: 0.3084000051021576
Step: 19210, train/grad_norm: 21.65205192565918
Step: 19210, train/learning_rate: 2.7141837563249283e-05
Step: 19210, train/epoch: 4.571632385253906
Step: 19220, train/loss: 0.3427000045776367
Step: 19220, train/grad_norm: 3.065056562423706
Step: 19220, train/learning_rate: 2.7129937734571286e-05
Step: 19220, train/epoch: 4.574012279510498
Step: 19230, train/loss: 0.38040000200271606
Step: 19230, train/grad_norm: 9.154790878295898
Step: 19230, train/learning_rate: 2.7118039724882692e-05
Step: 19230, train/epoch: 4.57639217376709
Step: 19240, train/loss: 0.4016999900341034
Step: 19240, train/grad_norm: 3.7090065479278564
Step: 19240, train/learning_rate: 2.7106139896204695e-05
Step: 19240, train/epoch: 4.578772068023682
Step: 19250, train/loss: 0.36250001192092896
Step: 19250, train/grad_norm: 19.072534561157227
Step: 19250, train/learning_rate: 2.7094240067526698e-05
Step: 19250, train/epoch: 4.581151962280273
Step: 19260, train/loss: 0.48339998722076416
Step: 19260, train/grad_norm: 10.483407974243164
Step: 19260, train/learning_rate: 2.7082342057838105e-05
Step: 19260, train/epoch: 4.583531856536865
Step: 19270, train/loss: 0.24310000240802765
Step: 19270, train/grad_norm: 2.621387004852295
Step: 19270, train/learning_rate: 2.7070442229160108e-05
Step: 19270, train/epoch: 4.585911273956299
Step: 19280, train/loss: 0.42800000309944153
Step: 19280, train/grad_norm: 19.49472999572754
Step: 19280, train/learning_rate: 2.7058544219471514e-05
Step: 19280, train/epoch: 4.588291168212891
Step: 19290, train/loss: 0.3668999969959259
Step: 19290, train/grad_norm: 13.885309219360352
Step: 19290, train/learning_rate: 2.7046644390793517e-05
Step: 19290, train/epoch: 4.590671062469482
Step: 19300, train/loss: 0.45500001311302185
Step: 19300, train/grad_norm: 4.505095481872559
Step: 19300, train/learning_rate: 2.703474456211552e-05
Step: 19300, train/epoch: 4.593050956726074
Step: 19310, train/loss: 0.4415999948978424
Step: 19310, train/grad_norm: 0.9608638882637024
Step: 19310, train/learning_rate: 2.7022846552426927e-05
Step: 19310, train/epoch: 4.595430850982666
Step: 19320, train/loss: 0.39570000767707825
Step: 19320, train/grad_norm: 13.409760475158691
Step: 19320, train/learning_rate: 2.701094672374893e-05
Step: 19320, train/epoch: 4.597810745239258
Step: 19330, train/loss: 0.3249000012874603
Step: 19330, train/grad_norm: 12.572388648986816
Step: 19330, train/learning_rate: 2.6999048714060336e-05
Step: 19330, train/epoch: 4.600190162658691
Step: 19340, train/loss: 0.6478000283241272
Step: 19340, train/grad_norm: 22.373279571533203
Step: 19340, train/learning_rate: 2.698714888538234e-05
Step: 19340, train/epoch: 4.602570056915283
Step: 19350, train/loss: 0.29840001463890076
Step: 19350, train/grad_norm: 22.641931533813477
Step: 19350, train/learning_rate: 2.6975249056704342e-05
Step: 19350, train/epoch: 4.604949951171875
Step: 19360, train/loss: 0.4104999899864197
Step: 19360, train/grad_norm: 7.785679340362549
Step: 19360, train/learning_rate: 2.696335104701575e-05
Step: 19360, train/epoch: 4.607329845428467
Step: 19370, train/loss: 0.39410001039505005
Step: 19370, train/grad_norm: 7.371375560760498
Step: 19370, train/learning_rate: 2.6951451218337752e-05
Step: 19370, train/epoch: 4.609709739685059
Step: 19380, train/loss: 0.4311000108718872
Step: 19380, train/grad_norm: 23.781978607177734
Step: 19380, train/learning_rate: 2.693955320864916e-05
Step: 19380, train/epoch: 4.61208963394165
Step: 19390, train/loss: 0.3560999929904938
Step: 19390, train/grad_norm: 19.11516761779785
Step: 19390, train/learning_rate: 2.692765337997116e-05
Step: 19390, train/epoch: 4.614469528198242
Step: 19400, train/loss: 0.26080000400543213
Step: 19400, train/grad_norm: 6.549153804779053
Step: 19400, train/learning_rate: 2.6915753551293164e-05
Step: 19400, train/epoch: 4.616848945617676
Step: 19410, train/loss: 0.34950000047683716
Step: 19410, train/grad_norm: 4.70102071762085
Step: 19410, train/learning_rate: 2.690385554160457e-05
Step: 19410, train/epoch: 4.619228839874268
Step: 19420, train/loss: 0.3382999897003174
Step: 19420, train/grad_norm: 22.262847900390625
Step: 19420, train/learning_rate: 2.6891955712926574e-05
Step: 19420, train/epoch: 4.621608734130859
Step: 19430, train/loss: 0.3889999985694885
Step: 19430, train/grad_norm: 9.017297744750977
Step: 19430, train/learning_rate: 2.688005770323798e-05
Step: 19430, train/epoch: 4.623988628387451
Step: 19440, train/loss: 0.22220000624656677
Step: 19440, train/grad_norm: 14.412960052490234
Step: 19440, train/learning_rate: 2.6868157874559984e-05
Step: 19440, train/epoch: 4.626368522644043
Step: 19450, train/loss: 0.48190000653266907
Step: 19450, train/grad_norm: 10.021135330200195
Step: 19450, train/learning_rate: 2.6856258045881987e-05
Step: 19450, train/epoch: 4.628748416900635
Step: 19460, train/loss: 0.4293000102043152
Step: 19460, train/grad_norm: 23.089771270751953
Step: 19460, train/learning_rate: 2.6844360036193393e-05
Step: 19460, train/epoch: 4.631127834320068
Step: 19470, train/loss: 0.3483000099658966
Step: 19470, train/grad_norm: 4.957877159118652
Step: 19470, train/learning_rate: 2.6832460207515396e-05
Step: 19470, train/epoch: 4.63350772857666
Step: 19480, train/loss: 0.3522999882698059
Step: 19480, train/grad_norm: 3.558398485183716
Step: 19480, train/learning_rate: 2.6820562197826803e-05
Step: 19480, train/epoch: 4.635887622833252
Step: 19490, train/loss: 0.25850000977516174
Step: 19490, train/grad_norm: 11.98234748840332
Step: 19490, train/learning_rate: 2.6808662369148806e-05
Step: 19490, train/epoch: 4.638267517089844
Step: 19500, train/loss: 0.2964000105857849
Step: 19500, train/grad_norm: 17.751869201660156
Step: 19500, train/learning_rate: 2.679676254047081e-05
Step: 19500, train/epoch: 4.6406474113464355
Step: 19510, train/loss: 0.4104999899864197
Step: 19510, train/grad_norm: 6.662226676940918
Step: 19510, train/learning_rate: 2.6784864530782215e-05
Step: 19510, train/epoch: 4.643027305603027
Step: 19520, train/loss: 0.3257000148296356
Step: 19520, train/grad_norm: 5.92710542678833
Step: 19520, train/learning_rate: 2.6772964702104218e-05
Step: 19520, train/epoch: 4.645406723022461
Step: 19530, train/loss: 0.4772999882698059
Step: 19530, train/grad_norm: 10.33857250213623
Step: 19530, train/learning_rate: 2.6761066692415625e-05
Step: 19530, train/epoch: 4.647786617279053
Step: 19540, train/loss: 0.51419997215271
Step: 19540, train/grad_norm: 8.917473793029785
Step: 19540, train/learning_rate: 2.6749166863737628e-05
Step: 19540, train/epoch: 4.6501665115356445
Step: 19550, train/loss: 0.5245000123977661
Step: 19550, train/grad_norm: 15.361961364746094
Step: 19550, train/learning_rate: 2.6737268854049034e-05
Step: 19550, train/epoch: 4.652546405792236
Step: 19560, train/loss: 0.2597000002861023
Step: 19560, train/grad_norm: 4.993771076202393
Step: 19560, train/learning_rate: 2.6725369025371037e-05
Step: 19560, train/epoch: 4.654926300048828
Step: 19570, train/loss: 0.3059999942779541
Step: 19570, train/grad_norm: 10.493450164794922
Step: 19570, train/learning_rate: 2.671346919669304e-05
Step: 19570, train/epoch: 4.65730619430542
Step: 19580, train/loss: 0.4020000100135803
Step: 19580, train/grad_norm: 9.727457046508789
Step: 19580, train/learning_rate: 2.6701571187004447e-05
Step: 19580, train/epoch: 4.659686088562012
Step: 19590, train/loss: 0.3248000144958496
Step: 19590, train/grad_norm: 5.665841579437256
Step: 19590, train/learning_rate: 2.668967135832645e-05
Step: 19590, train/epoch: 4.662065505981445
Step: 19600, train/loss: 0.26570001244544983
Step: 19600, train/grad_norm: 4.708524703979492
Step: 19600, train/learning_rate: 2.6677773348637857e-05
Step: 19600, train/epoch: 4.664445400238037
Step: 19610, train/loss: 0.39010000228881836
Step: 19610, train/grad_norm: 12.92685317993164
Step: 19610, train/learning_rate: 2.666587351995986e-05
Step: 19610, train/epoch: 4.666825294494629
Step: 19620, train/loss: 0.1843000054359436
Step: 19620, train/grad_norm: 7.247447490692139
Step: 19620, train/learning_rate: 2.6653973691281863e-05
Step: 19620, train/epoch: 4.669205188751221
Step: 19630, train/loss: 0.491100013256073
Step: 19630, train/grad_norm: 13.574947357177734
Step: 19630, train/learning_rate: 2.664207568159327e-05
Step: 19630, train/epoch: 4.6715850830078125
Step: 19640, train/loss: 0.3508000075817108
Step: 19640, train/grad_norm: 21.565303802490234
Step: 19640, train/learning_rate: 2.6630175852915272e-05
Step: 19640, train/epoch: 4.673964977264404
Step: 19650, train/loss: 0.34540000557899475
Step: 19650, train/grad_norm: 5.240893840789795
Step: 19650, train/learning_rate: 2.661827784322668e-05
Step: 19650, train/epoch: 4.676344394683838
Step: 19660, train/loss: 0.43479999899864197
Step: 19660, train/grad_norm: 11.399788856506348
Step: 19660, train/learning_rate: 2.660637801454868e-05
Step: 19660, train/epoch: 4.67872428894043
Step: 19670, train/loss: 0.28450000286102295
Step: 19670, train/grad_norm: 6.75495719909668
Step: 19670, train/learning_rate: 2.6594478185870685e-05
Step: 19670, train/epoch: 4.6811041831970215
Step: 19680, train/loss: 0.2752000093460083
Step: 19680, train/grad_norm: 34.54804611206055
Step: 19680, train/learning_rate: 2.658258017618209e-05
Step: 19680, train/epoch: 4.683484077453613
Step: 19690, train/loss: 0.39809998869895935
Step: 19690, train/grad_norm: 6.284617900848389
Step: 19690, train/learning_rate: 2.6570680347504094e-05
Step: 19690, train/epoch: 4.685863971710205
Step: 19700, train/loss: 0.49230000376701355
Step: 19700, train/grad_norm: 12.898982048034668
Step: 19700, train/learning_rate: 2.65587823378155e-05
Step: 19700, train/epoch: 4.688243865966797
Step: 19710, train/loss: 0.4287000000476837
Step: 19710, train/grad_norm: 14.47354507446289
Step: 19710, train/learning_rate: 2.6546882509137504e-05
Step: 19710, train/epoch: 4.6906232833862305
Step: 19720, train/loss: 0.35370001196861267
Step: 19720, train/grad_norm: 12.082867622375488
Step: 19720, train/learning_rate: 2.6534982680459507e-05
Step: 19720, train/epoch: 4.693003177642822
Step: 19730, train/loss: 0.4016000032424927
Step: 19730, train/grad_norm: 10.234231948852539
Step: 19730, train/learning_rate: 2.6523084670770913e-05
Step: 19730, train/epoch: 4.695383071899414
Step: 19740, train/loss: 0.4950999915599823
Step: 19740, train/grad_norm: 8.276816368103027
Step: 19740, train/learning_rate: 2.6511184842092916e-05
Step: 19740, train/epoch: 4.697762966156006
Step: 19750, train/loss: 0.3066999912261963
Step: 19750, train/grad_norm: 22.351394653320312
Step: 19750, train/learning_rate: 2.6499286832404323e-05
Step: 19750, train/epoch: 4.700142860412598
Step: 19760, train/loss: 0.40790000557899475
Step: 19760, train/grad_norm: 14.434061050415039
Step: 19760, train/learning_rate: 2.6487387003726326e-05
Step: 19760, train/epoch: 4.7025227546691895
Step: 19770, train/loss: 0.42239999771118164
Step: 19770, train/grad_norm: 11.14340877532959
Step: 19770, train/learning_rate: 2.647548717504833e-05
Step: 19770, train/epoch: 4.704902648925781
Step: 19780, train/loss: 0.33820000290870667
Step: 19780, train/grad_norm: 6.952674865722656
Step: 19780, train/learning_rate: 2.6463589165359735e-05
Step: 19780, train/epoch: 4.707282066345215
Step: 19790, train/loss: 0.4544000029563904
Step: 19790, train/grad_norm: 12.526028633117676
Step: 19790, train/learning_rate: 2.645168933668174e-05
Step: 19790, train/epoch: 4.709661960601807
Step: 19800, train/loss: 0.4383000135421753
Step: 19800, train/grad_norm: 26.080278396606445
Step: 19800, train/learning_rate: 2.6439791326993145e-05
Step: 19800, train/epoch: 4.712041854858398
Step: 19810, train/loss: 0.47620001435279846
Step: 19810, train/grad_norm: 31.22856330871582
Step: 19810, train/learning_rate: 2.6427891498315148e-05
Step: 19810, train/epoch: 4.71442174911499
Step: 19820, train/loss: 0.43479999899864197
Step: 19820, train/grad_norm: 6.851768970489502
Step: 19820, train/learning_rate: 2.641599166963715e-05
Step: 19820, train/epoch: 4.716801643371582
Step: 19830, train/loss: 0.29409998655319214
Step: 19830, train/grad_norm: 9.025897026062012
Step: 19830, train/learning_rate: 2.6404093659948558e-05
Step: 19830, train/epoch: 4.719181537628174
Step: 19840, train/loss: 0.2696000039577484
Step: 19840, train/grad_norm: 15.503308296203613
Step: 19840, train/learning_rate: 2.639219383127056e-05
Step: 19840, train/epoch: 4.721560955047607
Step: 19850, train/loss: 0.3061999976634979
Step: 19850, train/grad_norm: 13.822710037231445
Step: 19850, train/learning_rate: 2.6380295821581967e-05
Step: 19850, train/epoch: 4.723940849304199
Step: 19860, train/loss: 0.3903999924659729
Step: 19860, train/grad_norm: 9.791786193847656
Step: 19860, train/learning_rate: 2.636839599290397e-05
Step: 19860, train/epoch: 4.726320743560791
Step: 19870, train/loss: 0.28439998626708984
Step: 19870, train/grad_norm: 18.43581199645996
Step: 19870, train/learning_rate: 2.6356496164225973e-05
Step: 19870, train/epoch: 4.728700637817383
Step: 19880, train/loss: 0.427700012922287
Step: 19880, train/grad_norm: 15.808402061462402
Step: 19880, train/learning_rate: 2.634459815453738e-05
Step: 19880, train/epoch: 4.731080532073975
Step: 19890, train/loss: 0.45320001244544983
Step: 19890, train/grad_norm: 34.72927474975586
Step: 19890, train/learning_rate: 2.6332698325859383e-05
Step: 19890, train/epoch: 4.733460426330566
Step: 19900, train/loss: 0.4641000032424927
Step: 19900, train/grad_norm: 17.289499282836914
Step: 19900, train/learning_rate: 2.632080031617079e-05
Step: 19900, train/epoch: 4.73583984375
Step: 19910, train/loss: 0.43779999017715454
Step: 19910, train/grad_norm: 2.662294864654541
Step: 19910, train/learning_rate: 2.6308900487492792e-05
Step: 19910, train/epoch: 4.738219738006592
Step: 19920, train/loss: 0.4259999990463257
Step: 19920, train/grad_norm: 21.66343116760254
Step: 19920, train/learning_rate: 2.6297000658814795e-05
Step: 19920, train/epoch: 4.740599632263184
Step: 19930, train/loss: 0.3806999921798706
Step: 19930, train/grad_norm: 10.930264472961426
Step: 19930, train/learning_rate: 2.6285102649126202e-05
Step: 19930, train/epoch: 4.742979526519775
Step: 19940, train/loss: 0.3968000113964081
Step: 19940, train/grad_norm: 10.027242660522461
Step: 19940, train/learning_rate: 2.6273202820448205e-05
Step: 19940, train/epoch: 4.745359420776367
Step: 19950, train/loss: 0.4706999957561493
Step: 19950, train/grad_norm: 4.502095699310303
Step: 19950, train/learning_rate: 2.626130481075961e-05
Step: 19950, train/epoch: 4.747739315032959
Step: 19960, train/loss: 0.40369999408721924
Step: 19960, train/grad_norm: 24.784931182861328
Step: 19960, train/learning_rate: 2.6249404982081614e-05
Step: 19960, train/epoch: 4.750119209289551
Step: 19970, train/loss: 0.4207000136375427
Step: 19970, train/grad_norm: 4.995666980743408
Step: 19970, train/learning_rate: 2.6237505153403617e-05
Step: 19970, train/epoch: 4.752498626708984
Step: 19980, train/loss: 0.3456999957561493
Step: 19980, train/grad_norm: 2.308197498321533
Step: 19980, train/learning_rate: 2.6225607143715024e-05
Step: 19980, train/epoch: 4.754878520965576
Step: 19990, train/loss: 0.3659999966621399
Step: 19990, train/grad_norm: 17.880403518676758
Step: 19990, train/learning_rate: 2.6213707315037027e-05
Step: 19990, train/epoch: 4.757258415222168
Step: 20000, train/loss: 0.39419999718666077
Step: 20000, train/grad_norm: 10.014946937561035
Step: 20000, train/learning_rate: 2.6201809305348434e-05
Step: 20000, train/epoch: 4.75963830947876
Step: 20010, train/loss: 0.26260000467300415
Step: 20010, train/grad_norm: 17.052345275878906
Step: 20010, train/learning_rate: 2.6189909476670437e-05
Step: 20010, train/epoch: 4.762018203735352
Step: 20020, train/loss: 0.2694000005722046
Step: 20020, train/grad_norm: 2.5925400257110596
Step: 20020, train/learning_rate: 2.617800964799244e-05
Step: 20020, train/epoch: 4.764398097991943
Step: 20030, train/loss: 0.4253999888896942
Step: 20030, train/grad_norm: 7.86525297164917
Step: 20030, train/learning_rate: 2.6166111638303846e-05
Step: 20030, train/epoch: 4.766777515411377
Step: 20040, train/loss: 0.38019999861717224
Step: 20040, train/grad_norm: 26.88617515563965
Step: 20040, train/learning_rate: 2.615421180962585e-05
Step: 20040, train/epoch: 4.769157409667969
Step: 20050, train/loss: 0.46219998598098755
Step: 20050, train/grad_norm: 15.919673919677734
Step: 20050, train/learning_rate: 2.6142313799937256e-05
Step: 20050, train/epoch: 4.7715373039245605
Step: 20060, train/loss: 0.3407000005245209
Step: 20060, train/grad_norm: 5.601970195770264
Step: 20060, train/learning_rate: 2.613041397125926e-05
Step: 20060, train/epoch: 4.773917198181152
Step: 20070, train/loss: 0.3564000129699707
Step: 20070, train/grad_norm: 3.173928737640381
Step: 20070, train/learning_rate: 2.6118514142581262e-05
Step: 20070, train/epoch: 4.776297092437744
Step: 20080, train/loss: 0.5985000133514404
Step: 20080, train/grad_norm: 21.11140251159668
Step: 20080, train/learning_rate: 2.6106616132892668e-05
Step: 20080, train/epoch: 4.778676986694336
Step: 20090, train/loss: 0.392300009727478
Step: 20090, train/grad_norm: 11.45730972290039
Step: 20090, train/learning_rate: 2.609471630421467e-05
Step: 20090, train/epoch: 4.7810564041137695
Step: 20100, train/loss: 0.27320000529289246
Step: 20100, train/grad_norm: 5.98568058013916
Step: 20100, train/learning_rate: 2.6082818294526078e-05
Step: 20100, train/epoch: 4.783436298370361
Step: 20110, train/loss: 0.2687999904155731
Step: 20110, train/grad_norm: 7.147535800933838
Step: 20110, train/learning_rate: 2.607091846584808e-05
Step: 20110, train/epoch: 4.785816192626953
Step: 20120, train/loss: 0.3495999872684479
Step: 20120, train/grad_norm: 16.382789611816406
Step: 20120, train/learning_rate: 2.6059018637170084e-05
Step: 20120, train/epoch: 4.788196086883545
Step: 20130, train/loss: 0.3774999976158142
Step: 20130, train/grad_norm: 10.132730484008789
Step: 20130, train/learning_rate: 2.604712062748149e-05
Step: 20130, train/epoch: 4.790575981140137
Step: 20140, train/loss: 0.22540000081062317
Step: 20140, train/grad_norm: 15.417433738708496
Step: 20140, train/learning_rate: 2.6035220798803493e-05
Step: 20140, train/epoch: 4.7929558753967285
Step: 20150, train/loss: 0.32170000672340393
Step: 20150, train/grad_norm: 12.711396217346191
Step: 20150, train/learning_rate: 2.60233227891149e-05
Step: 20150, train/epoch: 4.79533576965332
Step: 20160, train/loss: 0.35440000891685486
Step: 20160, train/grad_norm: 11.692917823791504
Step: 20160, train/learning_rate: 2.6011422960436903e-05
Step: 20160, train/epoch: 4.797715187072754
Step: 20170, train/loss: 0.505299985408783
Step: 20170, train/grad_norm: 28.78604507446289
Step: 20170, train/learning_rate: 2.5999523131758906e-05
Step: 20170, train/epoch: 4.800095081329346
Step: 20180, train/loss: 0.3594000041484833
Step: 20180, train/grad_norm: 24.916641235351562
Step: 20180, train/learning_rate: 2.5987625122070312e-05
Step: 20180, train/epoch: 4.8024749755859375
Step: 20190, train/loss: 0.5110999941825867
Step: 20190, train/grad_norm: 15.634844779968262
Step: 20190, train/learning_rate: 2.5975725293392316e-05
Step: 20190, train/epoch: 4.804854869842529
Step: 20200, train/loss: 0.5383999943733215
Step: 20200, train/grad_norm: 20.33997917175293
Step: 20200, train/learning_rate: 2.5963827283703722e-05
Step: 20200, train/epoch: 4.807234764099121
Step: 20210, train/loss: 0.33079999685287476
Step: 20210, train/grad_norm: 17.413040161132812
Step: 20210, train/learning_rate: 2.5951927455025725e-05
Step: 20210, train/epoch: 4.809614658355713
Step: 20220, train/loss: 0.4401000142097473
Step: 20220, train/grad_norm: 10.925068855285645
Step: 20220, train/learning_rate: 2.594002944533713e-05
Step: 20220, train/epoch: 4.8119940757751465
Step: 20230, train/loss: 0.3052000105381012
Step: 20230, train/grad_norm: 9.721884727478027
Step: 20230, train/learning_rate: 2.5928129616659135e-05
Step: 20230, train/epoch: 4.814373970031738
Step: 20240, train/loss: 0.3479999899864197
Step: 20240, train/grad_norm: 11.575169563293457
Step: 20240, train/learning_rate: 2.5916229787981138e-05
Step: 20240, train/epoch: 4.81675386428833
Step: 20250, train/loss: 0.42329999804496765
Step: 20250, train/grad_norm: 35.57631301879883
Step: 20250, train/learning_rate: 2.5904331778292544e-05
Step: 20250, train/epoch: 4.819133758544922
Step: 20260, train/loss: 0.4729999899864197
Step: 20260, train/grad_norm: 20.268356323242188
Step: 20260, train/learning_rate: 2.5892431949614547e-05
Step: 20260, train/epoch: 4.821513652801514
Step: 20270, train/loss: 0.38100001215934753
Step: 20270, train/grad_norm: 5.124151229858398
Step: 20270, train/learning_rate: 2.5880533939925954e-05
Step: 20270, train/epoch: 4.8238935470581055
Step: 20280, train/loss: 0.42250001430511475
Step: 20280, train/grad_norm: 16.97348403930664
Step: 20280, train/learning_rate: 2.5868634111247957e-05
Step: 20280, train/epoch: 4.826273441314697
Step: 20290, train/loss: 0.29420000314712524
Step: 20290, train/grad_norm: 2.835761308670044
Step: 20290, train/learning_rate: 2.585673428256996e-05
Step: 20290, train/epoch: 4.828652858734131
Step: 20300, train/loss: 0.29280000925064087
Step: 20300, train/grad_norm: 4.202729225158691
Step: 20300, train/learning_rate: 2.5844836272881366e-05
Step: 20300, train/epoch: 4.831032752990723
Step: 20310, train/loss: 0.289000004529953
Step: 20310, train/grad_norm: 18.318531036376953
Step: 20310, train/learning_rate: 2.583293644420337e-05
Step: 20310, train/epoch: 4.8334126472473145
Step: 20320, train/loss: 0.6308000087738037
Step: 20320, train/grad_norm: 7.02753210067749
Step: 20320, train/learning_rate: 2.5821038434514776e-05
Step: 20320, train/epoch: 4.835792541503906
Step: 20330, train/loss: 0.33980000019073486
Step: 20330, train/grad_norm: 26.553760528564453
Step: 20330, train/learning_rate: 2.580913860583678e-05
Step: 20330, train/epoch: 4.838172435760498
Step: 20340, train/loss: 0.36169999837875366
Step: 20340, train/grad_norm: 5.767375469207764
Step: 20340, train/learning_rate: 2.5797238777158782e-05
Step: 20340, train/epoch: 4.84055233001709
Step: 20350, train/loss: 0.31929999589920044
Step: 20350, train/grad_norm: 3.5145647525787354
Step: 20350, train/learning_rate: 2.578534076747019e-05
Step: 20350, train/epoch: 4.842931747436523
Step: 20360, train/loss: 0.38690000772476196
Step: 20360, train/grad_norm: 5.592727184295654
Step: 20360, train/learning_rate: 2.577344093879219e-05
Step: 20360, train/epoch: 4.845311641693115
Step: 20370, train/loss: 0.3319000005722046
Step: 20370, train/grad_norm: 8.03331470489502
Step: 20370, train/learning_rate: 2.5761542929103598e-05
Step: 20370, train/epoch: 4.847691535949707
Step: 20380, train/loss: 0.28110000491142273
Step: 20380, train/grad_norm: 8.49179744720459
Step: 20380, train/learning_rate: 2.57496431004256e-05
Step: 20380, train/epoch: 4.850071430206299
Step: 20390, train/loss: 0.42890000343322754
Step: 20390, train/grad_norm: 21.409523010253906
Step: 20390, train/learning_rate: 2.5737743271747604e-05
Step: 20390, train/epoch: 4.852451324462891
Step: 20400, train/loss: 0.39969998598098755
Step: 20400, train/grad_norm: 10.804366111755371
Step: 20400, train/learning_rate: 2.572584526205901e-05
Step: 20400, train/epoch: 4.854831218719482
Step: 20410, train/loss: 0.36390000581741333
Step: 20410, train/grad_norm: 13.372847557067871
Step: 20410, train/learning_rate: 2.5713945433381014e-05
Step: 20410, train/epoch: 4.857210636138916
Step: 20420, train/loss: 0.4429999887943268
Step: 20420, train/grad_norm: 5.123904228210449
Step: 20420, train/learning_rate: 2.570204742369242e-05
Step: 20420, train/epoch: 4.859590530395508
Step: 20430, train/loss: 0.39559999108314514
Step: 20430, train/grad_norm: 7.6122145652771
Step: 20430, train/learning_rate: 2.5690147595014423e-05
Step: 20430, train/epoch: 4.8619704246521
Step: 20440, train/loss: 0.3718000054359436
Step: 20440, train/grad_norm: 10.38492202758789
Step: 20440, train/learning_rate: 2.5678247766336426e-05
Step: 20440, train/epoch: 4.864350318908691
Step: 20450, train/loss: 0.43059998750686646
Step: 20450, train/grad_norm: 14.18979263305664
Step: 20450, train/learning_rate: 2.5666349756647833e-05
Step: 20450, train/epoch: 4.866730213165283
Step: 20460, train/loss: 0.376800000667572
Step: 20460, train/grad_norm: 7.144253730773926
Step: 20460, train/learning_rate: 2.5654449927969836e-05
Step: 20460, train/epoch: 4.869110107421875
Step: 20470, train/loss: 0.3188999891281128
Step: 20470, train/grad_norm: 3.811260223388672
Step: 20470, train/learning_rate: 2.5642551918281242e-05
Step: 20470, train/epoch: 4.871490001678467
Step: 20480, train/loss: 0.24230000376701355
Step: 20480, train/grad_norm: 1.1684858798980713
Step: 20480, train/learning_rate: 2.5630652089603245e-05
Step: 20480, train/epoch: 4.8738694190979
Step: 20490, train/loss: 0.2676999866962433
Step: 20490, train/grad_norm: 9.927759170532227
Step: 20490, train/learning_rate: 2.5618752260925248e-05
Step: 20490, train/epoch: 4.876249313354492
Step: 20500, train/loss: 0.31520000100135803
Step: 20500, train/grad_norm: 9.979856491088867
Step: 20500, train/learning_rate: 2.5606854251236655e-05
Step: 20500, train/epoch: 4.878629207611084
Step: 20510, train/loss: 0.4523000121116638
Step: 20510, train/grad_norm: 10.155457496643066
Step: 20510, train/learning_rate: 2.5594954422558658e-05
Step: 20510, train/epoch: 4.881009101867676
Step: 20520, train/loss: 0.33469998836517334
Step: 20520, train/grad_norm: 3.320207118988037
Step: 20520, train/learning_rate: 2.5583056412870064e-05
Step: 20520, train/epoch: 4.883388996124268
Step: 20530, train/loss: 0.40799999237060547
Step: 20530, train/grad_norm: 7.342934608459473
Step: 20530, train/learning_rate: 2.5571156584192067e-05
Step: 20530, train/epoch: 4.885768890380859
Step: 20540, train/loss: 0.45829999446868896
Step: 20540, train/grad_norm: 8.131258964538574
Step: 20540, train/learning_rate: 2.555925675551407e-05
Step: 20540, train/epoch: 4.888148307800293
Step: 20550, train/loss: 0.5120999813079834
Step: 20550, train/grad_norm: 10.51902961730957
Step: 20550, train/learning_rate: 2.5547358745825477e-05
Step: 20550, train/epoch: 4.890528202056885
Step: 20560, train/loss: 0.4717999994754791
Step: 20560, train/grad_norm: 13.163813591003418
Step: 20560, train/learning_rate: 2.553545891714748e-05
Step: 20560, train/epoch: 4.892908096313477
Step: 20570, train/loss: 0.41190001368522644
Step: 20570, train/grad_norm: 15.973637580871582
Step: 20570, train/learning_rate: 2.5523560907458887e-05
Step: 20570, train/epoch: 4.895287990570068
Step: 20580, train/loss: 0.35929998755455017
Step: 20580, train/grad_norm: 16.382911682128906
Step: 20580, train/learning_rate: 2.551166107878089e-05
Step: 20580, train/epoch: 4.89766788482666
Step: 20590, train/loss: 0.34850001335144043
Step: 20590, train/grad_norm: 2.1489808559417725
Step: 20590, train/learning_rate: 2.5499761250102893e-05
Step: 20590, train/epoch: 4.900047779083252
Step: 20600, train/loss: 0.5333999991416931
Step: 20600, train/grad_norm: 9.717408180236816
Step: 20600, train/learning_rate: 2.54878632404143e-05
Step: 20600, train/epoch: 4.9024271965026855
Step: 20610, train/loss: 0.2766000032424927
Step: 20610, train/grad_norm: 3.686501979827881
Step: 20610, train/learning_rate: 2.5475963411736302e-05
Step: 20610, train/epoch: 4.904807090759277
Step: 20620, train/loss: 0.37380000948905945
Step: 20620, train/grad_norm: 13.282247543334961
Step: 20620, train/learning_rate: 2.546406540204771e-05
Step: 20620, train/epoch: 4.907186985015869
Step: 20630, train/loss: 0.3702000081539154
Step: 20630, train/grad_norm: 4.219906330108643
Step: 20630, train/learning_rate: 2.545216557336971e-05
Step: 20630, train/epoch: 4.909566879272461
Step: 20640, train/loss: 0.515999972820282
Step: 20640, train/grad_norm: 27.196632385253906
Step: 20640, train/learning_rate: 2.5440265744691715e-05
Step: 20640, train/epoch: 4.911946773529053
Step: 20650, train/loss: 0.3393000066280365
Step: 20650, train/grad_norm: 10.25859260559082
Step: 20650, train/learning_rate: 2.542836773500312e-05
Step: 20650, train/epoch: 4.9143266677856445
Step: 20660, train/loss: 0.35749998688697815
Step: 20660, train/grad_norm: 10.802789688110352
Step: 20660, train/learning_rate: 2.5416467906325124e-05
Step: 20660, train/epoch: 4.916706562042236
Step: 20670, train/loss: 0.3465999960899353
Step: 20670, train/grad_norm: 13.850580215454102
Step: 20670, train/learning_rate: 2.540456989663653e-05
Step: 20670, train/epoch: 4.91908597946167
Step: 20680, train/loss: 0.4120999872684479
Step: 20680, train/grad_norm: 9.371221542358398
Step: 20680, train/learning_rate: 2.5392670067958534e-05
Step: 20680, train/epoch: 4.921465873718262
Step: 20690, train/loss: 0.34700000286102295
Step: 20690, train/grad_norm: 9.61571216583252
Step: 20690, train/learning_rate: 2.5380770239280537e-05
Step: 20690, train/epoch: 4.9238457679748535
Step: 20700, train/loss: 0.37599998712539673
Step: 20700, train/grad_norm: 10.812723159790039
Step: 20700, train/learning_rate: 2.5368872229591943e-05
Step: 20700, train/epoch: 4.926225662231445
Step: 20710, train/loss: 0.2888999879360199
Step: 20710, train/grad_norm: 8.10635757446289
Step: 20710, train/learning_rate: 2.5356972400913946e-05
Step: 20710, train/epoch: 4.928605556488037
Step: 20720, train/loss: 0.5097000002861023
Step: 20720, train/grad_norm: 27.429536819458008
Step: 20720, train/learning_rate: 2.5345074391225353e-05
Step: 20720, train/epoch: 4.930985450744629
Step: 20730, train/loss: 0.3425000011920929
Step: 20730, train/grad_norm: 6.7283196449279785
Step: 20730, train/learning_rate: 2.5333174562547356e-05
Step: 20730, train/epoch: 4.9333648681640625
Step: 20740, train/loss: 0.45500001311302185
Step: 20740, train/grad_norm: 8.068273544311523
Step: 20740, train/learning_rate: 2.532127473386936e-05
Step: 20740, train/epoch: 4.935744762420654
Step: 20750, train/loss: 0.33799999952316284
Step: 20750, train/grad_norm: 21.196182250976562
Step: 20750, train/learning_rate: 2.5309376724180765e-05
Step: 20750, train/epoch: 4.938124656677246
Step: 20760, train/loss: 0.5317999720573425
Step: 20760, train/grad_norm: 29.824878692626953
Step: 20760, train/learning_rate: 2.529747689550277e-05
Step: 20760, train/epoch: 4.940504550933838
Step: 20770, train/loss: 0.3750999867916107
Step: 20770, train/grad_norm: 13.555344581604004
Step: 20770, train/learning_rate: 2.5285578885814175e-05
Step: 20770, train/epoch: 4.94288444519043
Step: 20780, train/loss: 0.3449000120162964
Step: 20780, train/grad_norm: 5.411223411560059
Step: 20780, train/learning_rate: 2.5273679057136178e-05
Step: 20780, train/epoch: 4.9452643394470215
Step: 20790, train/loss: 0.3230000138282776
Step: 20790, train/grad_norm: 20.605989456176758
Step: 20790, train/learning_rate: 2.526177922845818e-05
Step: 20790, train/epoch: 4.947643756866455
Step: 20800, train/loss: 0.4065999984741211
Step: 20800, train/grad_norm: 26.69485092163086
Step: 20800, train/learning_rate: 2.5249881218769588e-05
Step: 20800, train/epoch: 4.950023651123047
Step: 20810, train/loss: 0.3321000039577484
Step: 20810, train/grad_norm: 3.3607916831970215
Step: 20810, train/learning_rate: 2.523798139009159e-05
Step: 20810, train/epoch: 4.952403545379639
Step: 20820, train/loss: 0.3716000020503998
Step: 20820, train/grad_norm: 8.244998931884766
Step: 20820, train/learning_rate: 2.5226083380402997e-05
Step: 20820, train/epoch: 4.9547834396362305
Step: 20830, train/loss: 0.27300000190734863
Step: 20830, train/grad_norm: 9.771201133728027
Step: 20830, train/learning_rate: 2.5214183551725e-05
Step: 20830, train/epoch: 4.957163333892822
Step: 20840, train/loss: 0.3043000102043152
Step: 20840, train/grad_norm: 11.63683795928955
Step: 20840, train/learning_rate: 2.5202283723047003e-05
Step: 20840, train/epoch: 4.959543228149414
Step: 20850, train/loss: 0.2529999911785126
Step: 20850, train/grad_norm: 7.560133457183838
Step: 20850, train/learning_rate: 2.519038571335841e-05
Step: 20850, train/epoch: 4.961923122406006
Step: 20860, train/loss: 0.3885999917984009
Step: 20860, train/grad_norm: 16.97087287902832
Step: 20860, train/learning_rate: 2.5178485884680413e-05
Step: 20860, train/epoch: 4.9643025398254395
Step: 20870, train/loss: 0.3151000142097473
Step: 20870, train/grad_norm: 11.639037132263184
Step: 20870, train/learning_rate: 2.516658787499182e-05
Step: 20870, train/epoch: 4.966682434082031
Step: 20880, train/loss: 0.5042999982833862
Step: 20880, train/grad_norm: 11.330830574035645
Step: 20880, train/learning_rate: 2.5154688046313822e-05
Step: 20880, train/epoch: 4.969062328338623
Step: 20890, train/loss: 0.3050999939441681
Step: 20890, train/grad_norm: 10.034781455993652
Step: 20890, train/learning_rate: 2.514279003662523e-05
Step: 20890, train/epoch: 4.971442222595215
Step: 20900, train/loss: 0.30730000138282776
Step: 20900, train/grad_norm: 22.988025665283203
Step: 20900, train/learning_rate: 2.5130890207947232e-05
Step: 20900, train/epoch: 4.973822116851807
Step: 20910, train/loss: 0.4729999899864197
Step: 20910, train/grad_norm: 5.69512939453125
Step: 20910, train/learning_rate: 2.5118990379269235e-05
Step: 20910, train/epoch: 4.976202011108398
Step: 20920, train/loss: 0.36550000309944153
Step: 20920, train/grad_norm: 8.348017692565918
Step: 20920, train/learning_rate: 2.510709236958064e-05
Step: 20920, train/epoch: 4.978581428527832
Step: 20930, train/loss: 0.439300000667572
Step: 20930, train/grad_norm: 9.862863540649414
Step: 20930, train/learning_rate: 2.5095192540902644e-05
Step: 20930, train/epoch: 4.980961322784424
Step: 20940, train/loss: 0.35370001196861267
Step: 20940, train/grad_norm: 3.666815996170044
Step: 20940, train/learning_rate: 2.508329453121405e-05
Step: 20940, train/epoch: 4.983341217041016
Step: 20950, train/loss: 0.23659999668598175
Step: 20950, train/grad_norm: 7.652586936950684
Step: 20950, train/learning_rate: 2.5071394702536054e-05
Step: 20950, train/epoch: 4.985721111297607
Step: 20960, train/loss: 0.45399999618530273
Step: 20960, train/grad_norm: 21.810623168945312
Step: 20960, train/learning_rate: 2.5059494873858057e-05
Step: 20960, train/epoch: 4.988101005554199
Step: 20970, train/loss: 0.39980000257492065
Step: 20970, train/grad_norm: 8.825471878051758
Step: 20970, train/learning_rate: 2.5047596864169464e-05
Step: 20970, train/epoch: 4.990480899810791
Step: 20980, train/loss: 0.4194999933242798
Step: 20980, train/grad_norm: 11.285861015319824
Step: 20980, train/learning_rate: 2.5035697035491467e-05
Step: 20980, train/epoch: 4.992860317230225
Step: 20990, train/loss: 0.3779999911785126
Step: 20990, train/grad_norm: 12.51041316986084
Step: 20990, train/learning_rate: 2.5023799025802873e-05
Step: 20990, train/epoch: 4.995240211486816
Step: 21000, train/loss: 0.40619999170303345
Step: 21000, train/grad_norm: 14.25123119354248
Step: 21000, train/learning_rate: 2.5011899197124876e-05
Step: 21000, train/epoch: 4.997620105743408
Step: 21010, train/loss: 0.6003999710083008
Step: 21010, train/grad_norm: 42.73500061035156
Step: 21010, train/learning_rate: 2.499999936844688e-05
Step: 21010, train/epoch: 5.0
Step: 21010, eval/loss: 1.0121369361877441
Step: 21010, eval/accuracy: 0.6357073187828064
Step: 21010, eval/f1: 0.6309359669685364
Step: 21010, eval/runtime: 55.66999816894531
Step: 21010, eval/samples_per_second: 129.38699340820312
Step: 21010, eval/steps_per_second: 16.184999465942383
Step: 21010, train/epoch: 5.0
Step: 21020, train/loss: 0.303600013256073
Step: 21020, train/grad_norm: 12.115626335144043
Step: 21020, train/learning_rate: 2.4988101358758286e-05
Step: 21020, train/epoch: 5.002379894256592
Step: 21030, train/loss: 0.320499986410141
Step: 21030, train/grad_norm: 14.914413452148438
Step: 21030, train/learning_rate: 2.497620153008029e-05
Step: 21030, train/epoch: 5.004759788513184
Step: 21040, train/loss: 0.37059998512268066
Step: 21040, train/grad_norm: 3.7923104763031006
Step: 21040, train/learning_rate: 2.4964303520391695e-05
Step: 21040, train/epoch: 5.007139682769775
Step: 21050, train/loss: 0.34369999170303345
Step: 21050, train/grad_norm: 16.526704788208008
Step: 21050, train/learning_rate: 2.4952403691713698e-05
Step: 21050, train/epoch: 5.009519100189209
Step: 21060, train/loss: 0.36000001430511475
Step: 21060, train/grad_norm: 17.141407012939453
Step: 21060, train/learning_rate: 2.49405038630357e-05
Step: 21060, train/epoch: 5.011898994445801
Step: 21070, train/loss: 0.438400000333786
Step: 21070, train/grad_norm: 27.309490203857422
Step: 21070, train/learning_rate: 2.4928605853347108e-05
Step: 21070, train/epoch: 5.014278888702393
Step: 21080, train/loss: 0.5213000178337097
Step: 21080, train/grad_norm: 22.6267032623291
Step: 21080, train/learning_rate: 2.491670602466911e-05
Step: 21080, train/epoch: 5.016658782958984
Step: 21090, train/loss: 0.3903999924659729
Step: 21090, train/grad_norm: 15.971628189086914
Step: 21090, train/learning_rate: 2.4904808014980517e-05
Step: 21090, train/epoch: 5.019038677215576
Step: 21100, train/loss: 0.42750000953674316
Step: 21100, train/grad_norm: 6.981729507446289
Step: 21100, train/learning_rate: 2.489290818630252e-05
Step: 21100, train/epoch: 5.021418571472168
Step: 21110, train/loss: 0.48080000281333923
Step: 21110, train/grad_norm: 4.605839729309082
Step: 21110, train/learning_rate: 2.4881008357624523e-05
Step: 21110, train/epoch: 5.023797988891602
Step: 21120, train/loss: 0.31439998745918274
Step: 21120, train/grad_norm: 9.353792190551758
Step: 21120, train/learning_rate: 2.486911034793593e-05
Step: 21120, train/epoch: 5.026177883148193
Step: 21130, train/loss: 0.3528999984264374
Step: 21130, train/grad_norm: 4.84904670715332
Step: 21130, train/learning_rate: 2.4857210519257933e-05
Step: 21130, train/epoch: 5.028557777404785
Step: 21140, train/loss: 0.4043999910354614
Step: 21140, train/grad_norm: 15.309388160705566
Step: 21140, train/learning_rate: 2.484531250956934e-05
Step: 21140, train/epoch: 5.030937671661377
Step: 21150, train/loss: 0.4609000086784363
Step: 21150, train/grad_norm: 6.293034076690674
Step: 21150, train/learning_rate: 2.4833412680891342e-05
Step: 21150, train/epoch: 5.033317565917969
Step: 21160, train/loss: 0.4535999894142151
Step: 21160, train/grad_norm: 22.68886947631836
Step: 21160, train/learning_rate: 2.4821512852213345e-05
Step: 21160, train/epoch: 5.0356974601745605
Step: 21170, train/loss: 0.367900013923645
Step: 21170, train/grad_norm: 10.05197525024414
Step: 21170, train/learning_rate: 2.4809614842524752e-05
Step: 21170, train/epoch: 5.038076877593994
Step: 21180, train/loss: 0.4916999936103821
Step: 21180, train/grad_norm: 43.139957427978516
Step: 21180, train/learning_rate: 2.4797715013846755e-05
Step: 21180, train/epoch: 5.040456771850586
Step: 21190, train/loss: 0.2296999990940094
Step: 21190, train/grad_norm: 4.247610569000244
Step: 21190, train/learning_rate: 2.478581700415816e-05
Step: 21190, train/epoch: 5.042836666107178
Step: 21200, train/loss: 0.3095000088214874
Step: 21200, train/grad_norm: 6.326985836029053
Step: 21200, train/learning_rate: 2.4773917175480165e-05
Step: 21200, train/epoch: 5.0452165603637695
Step: 21210, train/loss: 0.29159998893737793
Step: 21210, train/grad_norm: 9.904621124267578
Step: 21210, train/learning_rate: 2.4762017346802168e-05
Step: 21210, train/epoch: 5.047596454620361
Step: 21220, train/loss: 0.2838999927043915
Step: 21220, train/grad_norm: 16.79206657409668
Step: 21220, train/learning_rate: 2.4750119337113574e-05
Step: 21220, train/epoch: 5.049976348876953
Step: 21230, train/loss: 0.4271000027656555
Step: 21230, train/grad_norm: 10.439803123474121
Step: 21230, train/learning_rate: 2.4738219508435577e-05
Step: 21230, train/epoch: 5.052356243133545
Step: 21240, train/loss: 0.4239000082015991
Step: 21240, train/grad_norm: 14.634584426879883
Step: 21240, train/learning_rate: 2.4726321498746984e-05
Step: 21240, train/epoch: 5.0547356605529785
Step: 21250, train/loss: 0.36410000920295715
Step: 21250, train/grad_norm: 2.0136020183563232
Step: 21250, train/learning_rate: 2.4714421670068987e-05
Step: 21250, train/epoch: 5.05711555480957
Step: 21260, train/loss: 0.3499999940395355
Step: 21260, train/grad_norm: 11.755805969238281
Step: 21260, train/learning_rate: 2.470252184139099e-05
Step: 21260, train/epoch: 5.059495449066162
Step: 21270, train/loss: 0.38769999146461487
Step: 21270, train/grad_norm: 20.613901138305664
Step: 21270, train/learning_rate: 2.4690623831702396e-05
Step: 21270, train/epoch: 5.061875343322754
Step: 21280, train/loss: 0.34929999709129333
Step: 21280, train/grad_norm: 4.798241138458252
Step: 21280, train/learning_rate: 2.46787240030244e-05
Step: 21280, train/epoch: 5.064255237579346
Step: 21290, train/loss: 0.5042999982833862
Step: 21290, train/grad_norm: 6.570465564727783
Step: 21290, train/learning_rate: 2.4666825993335806e-05
Step: 21290, train/epoch: 5.0666351318359375
Step: 21300, train/loss: 0.3767000138759613
Step: 21300, train/grad_norm: 3.46585750579834
Step: 21300, train/learning_rate: 2.465492616465781e-05
Step: 21300, train/epoch: 5.069014549255371
Step: 21310, train/loss: 0.5692999958992004
Step: 21310, train/grad_norm: 23.298486709594727
Step: 21310, train/learning_rate: 2.4643026335979812e-05
Step: 21310, train/epoch: 5.071394443511963
Step: 21320, train/loss: 0.49970000982284546
Step: 21320, train/grad_norm: 7.683411121368408
Step: 21320, train/learning_rate: 2.463112832629122e-05
Step: 21320, train/epoch: 5.073774337768555
Step: 21330, train/loss: 0.41609999537467957
Step: 21330, train/grad_norm: 12.797162055969238
Step: 21330, train/learning_rate: 2.461922849761322e-05
Step: 21330, train/epoch: 5.0761542320251465
Step: 21340, train/loss: 0.3961000144481659
Step: 21340, train/grad_norm: 12.868407249450684
Step: 21340, train/learning_rate: 2.4607330487924628e-05
Step: 21340, train/epoch: 5.078534126281738
Step: 21350, train/loss: 0.35249999165534973
Step: 21350, train/grad_norm: 14.247455596923828
Step: 21350, train/learning_rate: 2.459543065924663e-05
Step: 21350, train/epoch: 5.08091402053833
Step: 21360, train/loss: 0.3517000079154968
Step: 21360, train/grad_norm: 6.744151592254639
Step: 21360, train/learning_rate: 2.4583530830568634e-05
Step: 21360, train/epoch: 5.083293437957764
Step: 21370, train/loss: 0.40959998965263367
Step: 21370, train/grad_norm: 13.731123924255371
Step: 21370, train/learning_rate: 2.457163282088004e-05
Step: 21370, train/epoch: 5.0856733322143555
Step: 21380, train/loss: 0.30320000648498535
Step: 21380, train/grad_norm: 8.676465034484863
Step: 21380, train/learning_rate: 2.4559732992202044e-05
Step: 21380, train/epoch: 5.088053226470947
Step: 21390, train/loss: 0.24889999628067017
Step: 21390, train/grad_norm: 4.312616348266602
Step: 21390, train/learning_rate: 2.454783498251345e-05
Step: 21390, train/epoch: 5.090433120727539
Step: 21400, train/loss: 0.3569999933242798
Step: 21400, train/grad_norm: 15.12013053894043
Step: 21400, train/learning_rate: 2.4535935153835453e-05
Step: 21400, train/epoch: 5.092813014984131
Step: 21410, train/loss: 0.3682999908924103
Step: 21410, train/grad_norm: 11.693758964538574
Step: 21410, train/learning_rate: 2.4524035325157456e-05
Step: 21410, train/epoch: 5.095192909240723
Step: 21420, train/loss: 0.33070001006126404
Step: 21420, train/grad_norm: 7.087333679199219
Step: 21420, train/learning_rate: 2.4512137315468863e-05
Step: 21420, train/epoch: 5.0975728034973145
Step: 21430, train/loss: 0.39489999413490295
Step: 21430, train/grad_norm: 9.59726619720459
Step: 21430, train/learning_rate: 2.4500237486790866e-05
Step: 21430, train/epoch: 5.099952220916748
Step: 21440, train/loss: 0.40299999713897705
Step: 21440, train/grad_norm: 5.293861389160156
Step: 21440, train/learning_rate: 2.4488339477102272e-05
Step: 21440, train/epoch: 5.10233211517334
Step: 21450, train/loss: 0.33869999647140503
Step: 21450, train/grad_norm: 11.413126945495605
Step: 21450, train/learning_rate: 2.4476439648424275e-05
Step: 21450, train/epoch: 5.104712009429932
Step: 21460, train/loss: 0.36719998717308044
Step: 21460, train/grad_norm: 12.981148719787598
Step: 21460, train/learning_rate: 2.4464539819746278e-05
Step: 21460, train/epoch: 5.107091903686523
Step: 21470, train/loss: 0.29679998755455017
Step: 21470, train/grad_norm: 17.936588287353516
Step: 21470, train/learning_rate: 2.4452641810057685e-05
Step: 21470, train/epoch: 5.109471797943115
Step: 21480, train/loss: 0.4961000084877014
Step: 21480, train/grad_norm: 30.518632888793945
Step: 21480, train/learning_rate: 2.4440741981379688e-05
Step: 21480, train/epoch: 5.111851692199707
Step: 21490, train/loss: 0.36480000615119934
Step: 21490, train/grad_norm: 10.198531150817871
Step: 21490, train/learning_rate: 2.4428843971691094e-05
Step: 21490, train/epoch: 5.114231109619141
Step: 21500, train/loss: 0.4569000005722046
Step: 21500, train/grad_norm: 11.359326362609863
Step: 21500, train/learning_rate: 2.4416944143013097e-05
Step: 21500, train/epoch: 5.116611003875732
Step: 21510, train/loss: 0.4620000123977661
Step: 21510, train/grad_norm: 20.54928970336914
Step: 21510, train/learning_rate: 2.44050443143351e-05
Step: 21510, train/epoch: 5.118990898132324
Step: 21520, train/loss: 0.414900004863739
Step: 21520, train/grad_norm: 11.646403312683105
Step: 21520, train/learning_rate: 2.4393146304646507e-05
Step: 21520, train/epoch: 5.121370792388916
Step: 21530, train/loss: 0.37389999628067017
Step: 21530, train/grad_norm: 9.696663856506348
Step: 21530, train/learning_rate: 2.438124647596851e-05
Step: 21530, train/epoch: 5.123750686645508
Step: 21540, train/loss: 0.3490999937057495
Step: 21540, train/grad_norm: 22.66902732849121
Step: 21540, train/learning_rate: 2.4369348466279916e-05
Step: 21540, train/epoch: 5.1261305809021
Step: 21550, train/loss: 0.5109000205993652
Step: 21550, train/grad_norm: 15.246936798095703
Step: 21550, train/learning_rate: 2.435744863760192e-05
Step: 21550, train/epoch: 5.128509998321533
Step: 21560, train/loss: 0.5356000065803528
Step: 21560, train/grad_norm: 43.685951232910156
Step: 21560, train/learning_rate: 2.4345550627913326e-05
Step: 21560, train/epoch: 5.130889892578125
Step: 21570, train/loss: 0.5005999803543091
Step: 21570, train/grad_norm: 18.056690216064453
Step: 21570, train/learning_rate: 2.433365079923533e-05
Step: 21570, train/epoch: 5.133269786834717
Step: 21580, train/loss: 0.34369999170303345
Step: 21580, train/grad_norm: 11.687897682189941
Step: 21580, train/learning_rate: 2.4321750970557332e-05
Step: 21580, train/epoch: 5.135649681091309
Step: 21590, train/loss: 0.4715999960899353
Step: 21590, train/grad_norm: 5.3205037117004395
Step: 21590, train/learning_rate: 2.430985296086874e-05
Step: 21590, train/epoch: 5.1380295753479
Step: 21600, train/loss: 0.33399999141693115
Step: 21600, train/grad_norm: 35.28358840942383
Step: 21600, train/learning_rate: 2.429795313219074e-05
Step: 21600, train/epoch: 5.140409469604492
Step: 21610, train/loss: 0.4154999852180481
Step: 21610, train/grad_norm: 7.049825668334961
Step: 21610, train/learning_rate: 2.4286055122502148e-05
Step: 21610, train/epoch: 5.142789363861084
Step: 21620, train/loss: 0.3952000141143799
Step: 21620, train/grad_norm: 11.83264446258545
Step: 21620, train/learning_rate: 2.427415529382415e-05
Step: 21620, train/epoch: 5.145168781280518
Step: 21630, train/loss: 0.43149998784065247
Step: 21630, train/grad_norm: 8.632682800292969
Step: 21630, train/learning_rate: 2.4262255465146154e-05
Step: 21630, train/epoch: 5.147548675537109
Step: 21640, train/loss: 0.2802000045776367
Step: 21640, train/grad_norm: 14.207310676574707
Step: 21640, train/learning_rate: 2.425035745545756e-05
Step: 21640, train/epoch: 5.149928569793701
Step: 21650, train/loss: 0.3292999863624573
Step: 21650, train/grad_norm: 6.396743297576904
Step: 21650, train/learning_rate: 2.4238457626779564e-05
Step: 21650, train/epoch: 5.152308464050293
Step: 21660, train/loss: 0.29919999837875366
Step: 21660, train/grad_norm: 12.598333358764648
Step: 21660, train/learning_rate: 2.422655961709097e-05
Step: 21660, train/epoch: 5.154688358306885
Step: 21670, train/loss: 0.3937999904155731
Step: 21670, train/grad_norm: 11.55443000793457
Step: 21670, train/learning_rate: 2.4214659788412973e-05
Step: 21670, train/epoch: 5.157068252563477
Step: 21680, train/loss: 0.4293999969959259
Step: 21680, train/grad_norm: 6.5973968505859375
Step: 21680, train/learning_rate: 2.4202759959734976e-05
Step: 21680, train/epoch: 5.15944766998291
Step: 21690, train/loss: 0.34929999709129333
Step: 21690, train/grad_norm: 8.508795738220215
Step: 21690, train/learning_rate: 2.4190861950046383e-05
Step: 21690, train/epoch: 5.161827564239502
Step: 21700, train/loss: 0.5468000173568726
Step: 21700, train/grad_norm: 8.995278358459473
Step: 21700, train/learning_rate: 2.4178962121368386e-05
Step: 21700, train/epoch: 5.164207458496094
Step: 21710, train/loss: 0.3474999964237213
Step: 21710, train/grad_norm: 11.616135597229004
Step: 21710, train/learning_rate: 2.4167064111679792e-05
Step: 21710, train/epoch: 5.1665873527526855
Step: 21720, train/loss: 0.28290000557899475
Step: 21720, train/grad_norm: 13.024706840515137
Step: 21720, train/learning_rate: 2.4155164283001795e-05
Step: 21720, train/epoch: 5.168967247009277
Step: 21730, train/loss: 0.31380000710487366
Step: 21730, train/grad_norm: 6.130321979522705
Step: 21730, train/learning_rate: 2.41432644543238e-05
Step: 21730, train/epoch: 5.171347141265869
Step: 21740, train/loss: 0.3709999918937683
Step: 21740, train/grad_norm: 5.569228649139404
Step: 21740, train/learning_rate: 2.4131366444635205e-05
Step: 21740, train/epoch: 5.173726558685303
Step: 21750, train/loss: 0.3573000133037567
Step: 21750, train/grad_norm: 4.6095170974731445
Step: 21750, train/learning_rate: 2.4119466615957208e-05
Step: 21750, train/epoch: 5.1761064529418945
Step: 21760, train/loss: 0.5321999788284302
Step: 21760, train/grad_norm: 12.731171607971191
Step: 21760, train/learning_rate: 2.4107568606268615e-05
Step: 21760, train/epoch: 5.178486347198486
Step: 21770, train/loss: 0.304500013589859
Step: 21770, train/grad_norm: 6.284757137298584
Step: 21770, train/learning_rate: 2.4095668777590618e-05
Step: 21770, train/epoch: 5.180866241455078
Step: 21780, train/loss: 0.2994000017642975
Step: 21780, train/grad_norm: 26.65309715270996
Step: 21780, train/learning_rate: 2.408376894891262e-05
Step: 21780, train/epoch: 5.18324613571167
Step: 21790, train/loss: 0.28380000591278076
Step: 21790, train/grad_norm: 15.074869155883789
Step: 21790, train/learning_rate: 2.4071870939224027e-05
Step: 21790, train/epoch: 5.185626029968262
Step: 21800, train/loss: 0.33649998903274536
Step: 21800, train/grad_norm: 14.377318382263184
Step: 21800, train/learning_rate: 2.405997111054603e-05
Step: 21800, train/epoch: 5.1880059242248535
Step: 21810, train/loss: 0.578000009059906
Step: 21810, train/grad_norm: 6.055577278137207
Step: 21810, train/learning_rate: 2.4048073100857437e-05
Step: 21810, train/epoch: 5.190385341644287
Step: 21820, train/loss: 0.3978999853134155
Step: 21820, train/grad_norm: 28.152877807617188
Step: 21820, train/learning_rate: 2.403617327217944e-05
Step: 21820, train/epoch: 5.192765235900879
Step: 21830, train/loss: 0.37940001487731934
Step: 21830, train/grad_norm: 12.582534790039062
Step: 21830, train/learning_rate: 2.4024273443501443e-05
Step: 21830, train/epoch: 5.195145130157471
Step: 21840, train/loss: 0.3605000078678131
Step: 21840, train/grad_norm: 15.761637687683105
Step: 21840, train/learning_rate: 2.401237543381285e-05
Step: 21840, train/epoch: 5.1975250244140625
Step: 21850, train/loss: 0.3328999876976013
Step: 21850, train/grad_norm: 26.142160415649414
Step: 21850, train/learning_rate: 2.4000475605134852e-05
Step: 21850, train/epoch: 5.199904918670654
Step: 21860, train/loss: 0.375900000333786
Step: 21860, train/grad_norm: 24.825349807739258
Step: 21860, train/learning_rate: 2.398857759544626e-05
Step: 21860, train/epoch: 5.202284812927246
Step: 21870, train/loss: 0.4855000078678131
Step: 21870, train/grad_norm: 10.110977172851562
Step: 21870, train/learning_rate: 2.3976677766768262e-05
Step: 21870, train/epoch: 5.20466423034668
Step: 21880, train/loss: 0.29280000925064087
Step: 21880, train/grad_norm: 37.29581069946289
Step: 21880, train/learning_rate: 2.3964777938090265e-05
Step: 21880, train/epoch: 5.2070441246032715
Step: 21890, train/loss: 0.46810001134872437
Step: 21890, train/grad_norm: 15.0772066116333
Step: 21890, train/learning_rate: 2.395287992840167e-05
Step: 21890, train/epoch: 5.209424018859863
Step: 21900, train/loss: 0.3312999904155731
Step: 21900, train/grad_norm: 17.751251220703125
Step: 21900, train/learning_rate: 2.3940980099723674e-05
Step: 21900, train/epoch: 5.211803913116455
Step: 21910, train/loss: 0.3894999921321869
Step: 21910, train/grad_norm: 8.38137149810791
Step: 21910, train/learning_rate: 2.392908209003508e-05
Step: 21910, train/epoch: 5.214183807373047
Step: 21920, train/loss: 0.18649999797344208
Step: 21920, train/grad_norm: 6.753075122833252
Step: 21920, train/learning_rate: 2.3917182261357084e-05
Step: 21920, train/epoch: 5.216563701629639
Step: 21930, train/loss: 0.2540999948978424
Step: 21930, train/grad_norm: 6.360324859619141
Step: 21930, train/learning_rate: 2.3905282432679087e-05
Step: 21930, train/epoch: 5.2189435958862305
Step: 21940, train/loss: 0.2865000069141388
Step: 21940, train/grad_norm: 4.969506740570068
Step: 21940, train/learning_rate: 2.3893384422990493e-05
Step: 21940, train/epoch: 5.221323013305664
Step: 21950, train/loss: 0.25290000438690186
Step: 21950, train/grad_norm: 8.23725414276123
Step: 21950, train/learning_rate: 2.3881484594312496e-05
Step: 21950, train/epoch: 5.223702907562256
Step: 21960, train/loss: 0.210099995136261
Step: 21960, train/grad_norm: 3.2840912342071533
Step: 21960, train/learning_rate: 2.3869586584623903e-05
Step: 21960, train/epoch: 5.226082801818848
Step: 21970, train/loss: 0.48750001192092896
Step: 21970, train/grad_norm: 18.291034698486328
Step: 21970, train/learning_rate: 2.3857686755945906e-05
Step: 21970, train/epoch: 5.2284626960754395
Step: 21980, train/loss: 0.5633000135421753
Step: 21980, train/grad_norm: 20.608518600463867
Step: 21980, train/learning_rate: 2.384578692726791e-05
Step: 21980, train/epoch: 5.230842590332031
Step: 21990, train/loss: 0.43209999799728394
Step: 21990, train/grad_norm: 3.057103395462036
Step: 21990, train/learning_rate: 2.3833888917579316e-05
Step: 21990, train/epoch: 5.233222484588623
Step: 22000, train/loss: 0.4433000087738037
Step: 22000, train/grad_norm: 17.362300872802734
Step: 22000, train/learning_rate: 2.382198908890132e-05
Step: 22000, train/epoch: 5.235601902008057
Step: 22010, train/loss: 0.3822000026702881
Step: 22010, train/grad_norm: 9.90169906616211
Step: 22010, train/learning_rate: 2.3810091079212725e-05
Step: 22010, train/epoch: 5.237981796264648
Step: 22020, train/loss: 0.48890000581741333
Step: 22020, train/grad_norm: 14.024994850158691
Step: 22020, train/learning_rate: 2.3798191250534728e-05
Step: 22020, train/epoch: 5.24036169052124
Step: 22030, train/loss: 0.30329999327659607
Step: 22030, train/grad_norm: 1.5309768915176392
Step: 22030, train/learning_rate: 2.378629142185673e-05
Step: 22030, train/epoch: 5.242741584777832
Step: 22040, train/loss: 0.40950000286102295
Step: 22040, train/grad_norm: 18.776044845581055
Step: 22040, train/learning_rate: 2.3774393412168138e-05
Step: 22040, train/epoch: 5.245121479034424
Step: 22050, train/loss: 0.44179999828338623
Step: 22050, train/grad_norm: 10.200053215026855
Step: 22050, train/learning_rate: 2.376249358349014e-05
Step: 22050, train/epoch: 5.247501373291016
Step: 22060, train/loss: 0.3255000114440918
Step: 22060, train/grad_norm: 13.429118156433105
Step: 22060, train/learning_rate: 2.3750595573801547e-05
Step: 22060, train/epoch: 5.249880790710449
Step: 22070, train/loss: 0.44029998779296875
Step: 22070, train/grad_norm: 24.8980770111084
Step: 22070, train/learning_rate: 2.373869574512355e-05
Step: 22070, train/epoch: 5.252260684967041
Step: 22080, train/loss: 0.5174999833106995
Step: 22080, train/grad_norm: 20.929901123046875
Step: 22080, train/learning_rate: 2.3726795916445553e-05
Step: 22080, train/epoch: 5.254640579223633
Step: 22090, train/loss: 0.31700000166893005
Step: 22090, train/grad_norm: 10.362975120544434
Step: 22090, train/learning_rate: 2.371489790675696e-05
Step: 22090, train/epoch: 5.257020473480225
Step: 22100, train/loss: 0.1859000027179718
Step: 22100, train/grad_norm: 11.890222549438477
Step: 22100, train/learning_rate: 2.3702998078078963e-05
Step: 22100, train/epoch: 5.259400367736816
Step: 22110, train/loss: 0.35040000081062317
Step: 22110, train/grad_norm: 8.032411575317383
Step: 22110, train/learning_rate: 2.369110006839037e-05
Step: 22110, train/epoch: 5.261780261993408
Step: 22120, train/loss: 0.310699999332428
Step: 22120, train/grad_norm: 3.488088607788086
Step: 22120, train/learning_rate: 2.3679200239712372e-05
Step: 22120, train/epoch: 5.26416015625
Step: 22130, train/loss: 0.2092999964952469
Step: 22130, train/grad_norm: 3.499417304992676
Step: 22130, train/learning_rate: 2.3667300411034375e-05
Step: 22130, train/epoch: 5.266539573669434
Step: 22140, train/loss: 0.3894999921321869
Step: 22140, train/grad_norm: 5.577682971954346
Step: 22140, train/learning_rate: 2.3655402401345782e-05
Step: 22140, train/epoch: 5.268919467926025
Step: 22150, train/loss: 0.46939998865127563
Step: 22150, train/grad_norm: 11.324394226074219
Step: 22150, train/learning_rate: 2.3643502572667785e-05
Step: 22150, train/epoch: 5.271299362182617
Step: 22160, train/loss: 0.5633999705314636
Step: 22160, train/grad_norm: 5.926312446594238
Step: 22160, train/learning_rate: 2.363160456297919e-05
Step: 22160, train/epoch: 5.273679256439209
Step: 22170, train/loss: 0.4212000072002411
Step: 22170, train/grad_norm: 15.953215599060059
Step: 22170, train/learning_rate: 2.3619704734301195e-05
Step: 22170, train/epoch: 5.276059150695801
Step: 22180, train/loss: 0.36149999499320984
Step: 22180, train/grad_norm: 10.720988273620605
Step: 22180, train/learning_rate: 2.3607804905623198e-05
Step: 22180, train/epoch: 5.278439044952393
Step: 22190, train/loss: 0.29190000891685486
Step: 22190, train/grad_norm: 10.817879676818848
Step: 22190, train/learning_rate: 2.3595906895934604e-05
Step: 22190, train/epoch: 5.280818462371826
Step: 22200, train/loss: 0.4284999966621399
Step: 22200, train/grad_norm: 17.007761001586914
Step: 22200, train/learning_rate: 2.3584007067256607e-05
Step: 22200, train/epoch: 5.283198356628418
Step: 22210, train/loss: 0.28850001096725464
Step: 22210, train/grad_norm: 11.453276634216309
Step: 22210, train/learning_rate: 2.3572109057568014e-05
Step: 22210, train/epoch: 5.28557825088501
Step: 22220, train/loss: 0.41290000081062317
Step: 22220, train/grad_norm: 2.309800624847412
Step: 22220, train/learning_rate: 2.3560209228890017e-05
Step: 22220, train/epoch: 5.287958145141602
Step: 22230, train/loss: 0.3569999933242798
Step: 22230, train/grad_norm: 14.815714836120605
Step: 22230, train/learning_rate: 2.3548311219201423e-05
Step: 22230, train/epoch: 5.290338039398193
Step: 22240, train/loss: 0.32690000534057617
Step: 22240, train/grad_norm: 23.2101993560791
Step: 22240, train/learning_rate: 2.3536411390523426e-05
Step: 22240, train/epoch: 5.292717933654785
Step: 22250, train/loss: 0.4722999930381775
Step: 22250, train/grad_norm: 18.383474349975586
Step: 22250, train/learning_rate: 2.352451156184543e-05
Step: 22250, train/epoch: 5.295097351074219
Step: 22260, train/loss: 0.4487999975681305
Step: 22260, train/grad_norm: 13.764920234680176
Step: 22260, train/learning_rate: 2.3512613552156836e-05
Step: 22260, train/epoch: 5.2974772453308105
Step: 22270, train/loss: 0.38519999384880066
Step: 22270, train/grad_norm: 14.157360076904297
Step: 22270, train/learning_rate: 2.350071372347884e-05
Step: 22270, train/epoch: 5.299857139587402
Step: 22280, train/loss: 0.22280000150203705
Step: 22280, train/grad_norm: 3.589075803756714
Step: 22280, train/learning_rate: 2.3488815713790245e-05
Step: 22280, train/epoch: 5.302237033843994
Step: 22290, train/loss: 0.4544999897480011
Step: 22290, train/grad_norm: 1.6937832832336426
Step: 22290, train/learning_rate: 2.347691588511225e-05
Step: 22290, train/epoch: 5.304616928100586
Step: 22300, train/loss: 0.3806999921798706
Step: 22300, train/grad_norm: 24.1680850982666
Step: 22300, train/learning_rate: 2.346501605643425e-05
Step: 22300, train/epoch: 5.306996822357178
Step: 22310, train/loss: 0.421099990606308
Step: 22310, train/grad_norm: 7.326450824737549
Step: 22310, train/learning_rate: 2.3453118046745658e-05
Step: 22310, train/epoch: 5.3093767166137695
Step: 22320, train/loss: 0.374099999666214
Step: 22320, train/grad_norm: 17.134347915649414
Step: 22320, train/learning_rate: 2.344121821806766e-05
Step: 22320, train/epoch: 5.311756134033203
Step: 22330, train/loss: 0.4991999864578247
Step: 22330, train/grad_norm: 4.089166641235352
Step: 22330, train/learning_rate: 2.3429320208379067e-05
Step: 22330, train/epoch: 5.314136028289795
Step: 22340, train/loss: 0.3580000102519989
Step: 22340, train/grad_norm: 9.622716903686523
Step: 22340, train/learning_rate: 2.341742037970107e-05
Step: 22340, train/epoch: 5.316515922546387
Step: 22350, train/loss: 0.29429998993873596
Step: 22350, train/grad_norm: 5.302725791931152
Step: 22350, train/learning_rate: 2.3405520551023073e-05
Step: 22350, train/epoch: 5.3188958168029785
Step: 22360, train/loss: 0.41850000619888306
Step: 22360, train/grad_norm: 8.927762985229492
Step: 22360, train/learning_rate: 2.339362254133448e-05
Step: 22360, train/epoch: 5.32127571105957
Step: 22370, train/loss: 0.45010000467300415
Step: 22370, train/grad_norm: 28.073810577392578
Step: 22370, train/learning_rate: 2.3381722712656483e-05
Step: 22370, train/epoch: 5.323655605316162
Step: 22380, train/loss: 0.3450999855995178
Step: 22380, train/grad_norm: 17.804340362548828
Step: 22380, train/learning_rate: 2.336982470296789e-05
Step: 22380, train/epoch: 5.326035022735596
Step: 22390, train/loss: 0.3813000023365021
Step: 22390, train/grad_norm: 4.095473766326904
Step: 22390, train/learning_rate: 2.3357924874289893e-05
Step: 22390, train/epoch: 5.3284149169921875
Step: 22400, train/loss: 0.4235999882221222
Step: 22400, train/grad_norm: 7.92977237701416
Step: 22400, train/learning_rate: 2.3346025045611896e-05
Step: 22400, train/epoch: 5.330794811248779
Step: 22410, train/loss: 0.3578000068664551
Step: 22410, train/grad_norm: 15.906482696533203
Step: 22410, train/learning_rate: 2.3334127035923302e-05
Step: 22410, train/epoch: 5.333174705505371
Step: 22420, train/loss: 0.26829999685287476
Step: 22420, train/grad_norm: 2.5728073120117188
Step: 22420, train/learning_rate: 2.3322227207245305e-05
Step: 22420, train/epoch: 5.335554599761963
Step: 22430, train/loss: 0.23919999599456787
Step: 22430, train/grad_norm: 10.0866117477417
Step: 22430, train/learning_rate: 2.3310329197556712e-05
Step: 22430, train/epoch: 5.337934494018555
Step: 22440, train/loss: 0.30230000615119934
Step: 22440, train/grad_norm: 7.414291858673096
Step: 22440, train/learning_rate: 2.3298429368878715e-05
Step: 22440, train/epoch: 5.340313911437988
Step: 22450, train/loss: 0.541100025177002
Step: 22450, train/grad_norm: 15.268440246582031
Step: 22450, train/learning_rate: 2.3286529540200718e-05
Step: 22450, train/epoch: 5.34269380569458
Step: 22460, train/loss: 0.43369999527931213
Step: 22460, train/grad_norm: 18.477203369140625
Step: 22460, train/learning_rate: 2.3274631530512124e-05
Step: 22460, train/epoch: 5.345073699951172
Step: 22470, train/loss: 0.37369999289512634
Step: 22470, train/grad_norm: 24.01927375793457
Step: 22470, train/learning_rate: 2.3262731701834127e-05
Step: 22470, train/epoch: 5.347453594207764
Step: 22480, train/loss: 0.2953000068664551
Step: 22480, train/grad_norm: 6.151113986968994
Step: 22480, train/learning_rate: 2.3250833692145534e-05
Step: 22480, train/epoch: 5.3498334884643555
Step: 22490, train/loss: 0.31679999828338623
Step: 22490, train/grad_norm: 23.476184844970703
Step: 22490, train/learning_rate: 2.3238933863467537e-05
Step: 22490, train/epoch: 5.352213382720947
Step: 22500, train/loss: 0.3837999999523163
Step: 22500, train/grad_norm: 11.596253395080566
Step: 22500, train/learning_rate: 2.322703403478954e-05
Step: 22500, train/epoch: 5.354593276977539
Step: 22510, train/loss: 0.4832000136375427
Step: 22510, train/grad_norm: 23.062389373779297
Step: 22510, train/learning_rate: 2.3215136025100946e-05
Step: 22510, train/epoch: 5.356972694396973
Step: 22520, train/loss: 0.3986000120639801
Step: 22520, train/grad_norm: 16.16175079345703
Step: 22520, train/learning_rate: 2.320323619642295e-05
Step: 22520, train/epoch: 5.3593525886535645
Step: 22530, train/loss: 0.34940001368522644
Step: 22530, train/grad_norm: 7.492874622344971
Step: 22530, train/learning_rate: 2.3191338186734356e-05
Step: 22530, train/epoch: 5.361732482910156
Step: 22540, train/loss: 0.36500000953674316
Step: 22540, train/grad_norm: 3.148165225982666
Step: 22540, train/learning_rate: 2.317943835805636e-05
Step: 22540, train/epoch: 5.364112377166748
Step: 22550, train/loss: 0.34769999980926514
Step: 22550, train/grad_norm: 20.567821502685547
Step: 22550, train/learning_rate: 2.3167538529378362e-05
Step: 22550, train/epoch: 5.36649227142334
Step: 22560, train/loss: 0.45829999446868896
Step: 22560, train/grad_norm: 14.568107604980469
Step: 22560, train/learning_rate: 2.315564051968977e-05
Step: 22560, train/epoch: 5.368872165679932
Step: 22570, train/loss: 0.3490000069141388
Step: 22570, train/grad_norm: 7.032751083374023
Step: 22570, train/learning_rate: 2.314374069101177e-05
Step: 22570, train/epoch: 5.371251583099365
Step: 22580, train/loss: 0.49219998717308044
Step: 22580, train/grad_norm: 4.367899417877197
Step: 22580, train/learning_rate: 2.3131842681323178e-05
Step: 22580, train/epoch: 5.373631477355957
Step: 22590, train/loss: 0.24050000309944153
Step: 22590, train/grad_norm: 12.709339141845703
Step: 22590, train/learning_rate: 2.311994285264518e-05
Step: 22590, train/epoch: 5.376011371612549
Step: 22600, train/loss: 0.3091000020503998
Step: 22600, train/grad_norm: 16.78360939025879
Step: 22600, train/learning_rate: 2.3108043023967184e-05
Step: 22600, train/epoch: 5.378391265869141
Step: 22610, train/loss: 0.17820000648498535
Step: 22610, train/grad_norm: 9.560057640075684
Step: 22610, train/learning_rate: 2.309614501427859e-05
Step: 22610, train/epoch: 5.380771160125732
Step: 22620, train/loss: 0.3425000011920929
Step: 22620, train/grad_norm: 9.458661079406738
Step: 22620, train/learning_rate: 2.3084245185600594e-05
Step: 22620, train/epoch: 5.383151054382324
Step: 22630, train/loss: 0.420199990272522
Step: 22630, train/grad_norm: 17.522172927856445
Step: 22630, train/learning_rate: 2.3072347175912e-05
Step: 22630, train/epoch: 5.385530471801758
Step: 22640, train/loss: 0.4284999966621399
Step: 22640, train/grad_norm: 29.07332420349121
Step: 22640, train/learning_rate: 2.3060447347234003e-05
Step: 22640, train/epoch: 5.38791036605835
Step: 22650, train/loss: 0.29739999771118164
Step: 22650, train/grad_norm: 14.973225593566895
Step: 22650, train/learning_rate: 2.3048547518556006e-05
Step: 22650, train/epoch: 5.390290260314941
Step: 22660, train/loss: 0.42669999599456787
Step: 22660, train/grad_norm: 14.43461799621582
Step: 22660, train/learning_rate: 2.3036649508867413e-05
Step: 22660, train/epoch: 5.392670154571533
Step: 22670, train/loss: 0.35830000042915344
Step: 22670, train/grad_norm: 15.529775619506836
Step: 22670, train/learning_rate: 2.3024749680189416e-05
Step: 22670, train/epoch: 5.395050048828125
Step: 22680, train/loss: 0.336899995803833
Step: 22680, train/grad_norm: 4.254449844360352
Step: 22680, train/learning_rate: 2.3012851670500822e-05
Step: 22680, train/epoch: 5.397429943084717
Step: 22690, train/loss: 0.35199999809265137
Step: 22690, train/grad_norm: 4.021468639373779
Step: 22690, train/learning_rate: 2.3000951841822825e-05
Step: 22690, train/epoch: 5.399809837341309
Step: 22700, train/loss: 0.5160999894142151
Step: 22700, train/grad_norm: 4.433574199676514
Step: 22700, train/learning_rate: 2.298905201314483e-05
Step: 22700, train/epoch: 5.402189254760742
Step: 22710, train/loss: 0.4909000098705292
Step: 22710, train/grad_norm: 20.01517105102539
Step: 22710, train/learning_rate: 2.2977154003456235e-05
Step: 22710, train/epoch: 5.404569149017334
Step: 22720, train/loss: 0.35910001397132874
Step: 22720, train/grad_norm: 30.133142471313477
Step: 22720, train/learning_rate: 2.2965254174778238e-05
Step: 22720, train/epoch: 5.406949043273926
Step: 22730, train/loss: 0.4611999988555908
Step: 22730, train/grad_norm: 23.434492111206055
Step: 22730, train/learning_rate: 2.2953356165089644e-05
Step: 22730, train/epoch: 5.409328937530518
Step: 22740, train/loss: 0.5160999894142151
Step: 22740, train/grad_norm: 13.480323791503906
Step: 22740, train/learning_rate: 2.2941456336411647e-05
Step: 22740, train/epoch: 5.411708831787109
Step: 22750, train/loss: 0.2973000109195709
Step: 22750, train/grad_norm: 9.552547454833984
Step: 22750, train/learning_rate: 2.292955650773365e-05
Step: 22750, train/epoch: 5.414088726043701
Step: 22760, train/loss: 0.4185999929904938
Step: 22760, train/grad_norm: 4.742530822753906
Step: 22760, train/learning_rate: 2.2917658498045057e-05
Step: 22760, train/epoch: 5.416468143463135
Step: 22770, train/loss: 0.20800000429153442
Step: 22770, train/grad_norm: 4.937786102294922
Step: 22770, train/learning_rate: 2.290575866936706e-05
Step: 22770, train/epoch: 5.418848037719727
Step: 22780, train/loss: 0.2994999885559082
Step: 22780, train/grad_norm: 0.7029823660850525
Step: 22780, train/learning_rate: 2.2893860659678467e-05
Step: 22780, train/epoch: 5.421227931976318
Step: 22790, train/loss: 0.3605000078678131
Step: 22790, train/grad_norm: 4.992747783660889
Step: 22790, train/learning_rate: 2.288196083100047e-05
Step: 22790, train/epoch: 5.42360782623291
Step: 22800, train/loss: 0.265500009059906
Step: 22800, train/grad_norm: 9.850214958190918
Step: 22800, train/learning_rate: 2.2870061002322473e-05
Step: 22800, train/epoch: 5.425987720489502
Step: 22810, train/loss: 0.4934000074863434
Step: 22810, train/grad_norm: 7.7978715896606445
Step: 22810, train/learning_rate: 2.285816299263388e-05
Step: 22810, train/epoch: 5.428367614746094
Step: 22820, train/loss: 0.19529999792575836
Step: 22820, train/grad_norm: 4.646660327911377
Step: 22820, train/learning_rate: 2.2846263163955882e-05
Step: 22820, train/epoch: 5.430747032165527
Step: 22830, train/loss: 0.2786000072956085
Step: 22830, train/grad_norm: 13.673969268798828
Step: 22830, train/learning_rate: 2.283436515426729e-05
Step: 22830, train/epoch: 5.433126926422119
Step: 22840, train/loss: 0.28690001368522644
Step: 22840, train/grad_norm: 12.930567741394043
Step: 22840, train/learning_rate: 2.2822465325589292e-05
Step: 22840, train/epoch: 5.435506820678711
Step: 22850, train/loss: 0.4047999978065491
Step: 22850, train/grad_norm: 13.348814964294434
Step: 22850, train/learning_rate: 2.2810565496911295e-05
Step: 22850, train/epoch: 5.437886714935303
Step: 22860, train/loss: 0.3425000011920929
Step: 22860, train/grad_norm: 9.277159690856934
Step: 22860, train/learning_rate: 2.27986674872227e-05
Step: 22860, train/epoch: 5.4402666091918945
Step: 22870, train/loss: 0.42500001192092896
Step: 22870, train/grad_norm: 9.149843215942383
Step: 22870, train/learning_rate: 2.2786767658544704e-05
Step: 22870, train/epoch: 5.442646503448486
Step: 22880, train/loss: 0.25040000677108765
Step: 22880, train/grad_norm: 9.358078002929688
Step: 22880, train/learning_rate: 2.277486964885611e-05
Step: 22880, train/epoch: 5.445026397705078
Step: 22890, train/loss: 0.3573000133037567
Step: 22890, train/grad_norm: 3.8580567836761475
Step: 22890, train/learning_rate: 2.2762969820178114e-05
Step: 22890, train/epoch: 5.447405815124512
Step: 22900, train/loss: 0.42899999022483826
Step: 22900, train/grad_norm: 12.330981254577637
Step: 22900, train/learning_rate: 2.275107181048952e-05
Step: 22900, train/epoch: 5.4497857093811035
Step: 22910, train/loss: 0.2565999925136566
Step: 22910, train/grad_norm: 9.482864379882812
Step: 22910, train/learning_rate: 2.2739171981811523e-05
Step: 22910, train/epoch: 5.452165603637695
Step: 22920, train/loss: 0.2590999901294708
Step: 22920, train/grad_norm: 6.138774871826172
Step: 22920, train/learning_rate: 2.2727272153133526e-05
Step: 22920, train/epoch: 5.454545497894287
Step: 22930, train/loss: 0.34619998931884766
Step: 22930, train/grad_norm: 21.8125057220459
Step: 22930, train/learning_rate: 2.2715374143444933e-05
Step: 22930, train/epoch: 5.456925392150879
Step: 22940, train/loss: 0.2287999987602234
Step: 22940, train/grad_norm: 3.7302603721618652
Step: 22940, train/learning_rate: 2.2703474314766936e-05
Step: 22940, train/epoch: 5.459305286407471
Step: 22950, train/loss: 0.47749999165534973
Step: 22950, train/grad_norm: 8.699134826660156
Step: 22950, train/learning_rate: 2.2691576305078343e-05
Step: 22950, train/epoch: 5.461684703826904
Step: 22960, train/loss: 0.39629998803138733
Step: 22960, train/grad_norm: 18.411108016967773
Step: 22960, train/learning_rate: 2.2679676476400346e-05
Step: 22960, train/epoch: 5.464064598083496
Step: 22970, train/loss: 0.34769999980926514
Step: 22970, train/grad_norm: 4.62905740737915
Step: 22970, train/learning_rate: 2.266777664772235e-05
Step: 22970, train/epoch: 5.466444492340088
Step: 22980, train/loss: 0.3310000002384186
Step: 22980, train/grad_norm: 17.009449005126953
Step: 22980, train/learning_rate: 2.2655878638033755e-05
Step: 22980, train/epoch: 5.46882438659668
Step: 22990, train/loss: 0.32409998774528503
Step: 22990, train/grad_norm: 2.671051263809204
Step: 22990, train/learning_rate: 2.2643978809355758e-05
Step: 22990, train/epoch: 5.4712042808532715
Step: 23000, train/loss: 0.41019999980926514
Step: 23000, train/grad_norm: 10.617697715759277
Step: 23000, train/learning_rate: 2.2632080799667165e-05
Step: 23000, train/epoch: 5.473584175109863
Step: 23010, train/loss: 0.5081999897956848
Step: 23010, train/grad_norm: 15.900641441345215
Step: 23010, train/learning_rate: 2.2620180970989168e-05
Step: 23010, train/epoch: 5.475963592529297
Step: 23020, train/loss: 0.3968999981880188
Step: 23020, train/grad_norm: 17.203569412231445
Step: 23020, train/learning_rate: 2.260828114231117e-05
Step: 23020, train/epoch: 5.478343486785889
Step: 23030, train/loss: 0.24459999799728394
Step: 23030, train/grad_norm: 16.76827621459961
Step: 23030, train/learning_rate: 2.2596383132622577e-05
Step: 23030, train/epoch: 5.4807233810424805
Step: 23040, train/loss: 0.3919999897480011
Step: 23040, train/grad_norm: 5.699917316436768
Step: 23040, train/learning_rate: 2.258448330394458e-05
Step: 23040, train/epoch: 5.483103275299072
Step: 23050, train/loss: 0.31470000743865967
Step: 23050, train/grad_norm: 11.045321464538574
Step: 23050, train/learning_rate: 2.2572585294255987e-05
Step: 23050, train/epoch: 5.485483169555664
Step: 23060, train/loss: 0.3433000147342682
Step: 23060, train/grad_norm: 11.151694297790527
Step: 23060, train/learning_rate: 2.256068546557799e-05
Step: 23060, train/epoch: 5.487863063812256
Step: 23070, train/loss: 0.44519999623298645
Step: 23070, train/grad_norm: 8.551215171813965
Step: 23070, train/learning_rate: 2.2548785636899993e-05
Step: 23070, train/epoch: 5.490242958068848
Step: 23080, train/loss: 0.28439998626708984
Step: 23080, train/grad_norm: 5.362665176391602
Step: 23080, train/learning_rate: 2.25368876272114e-05
Step: 23080, train/epoch: 5.492622375488281
Step: 23090, train/loss: 0.41609999537467957
Step: 23090, train/grad_norm: 12.520825386047363
Step: 23090, train/learning_rate: 2.2524987798533402e-05
Step: 23090, train/epoch: 5.495002269744873
Step: 23100, train/loss: 0.31769999861717224
Step: 23100, train/grad_norm: 8.000408172607422
Step: 23100, train/learning_rate: 2.251308978884481e-05
Step: 23100, train/epoch: 5.497382164001465
Step: 23110, train/loss: 0.3490999937057495
Step: 23110, train/grad_norm: 6.801946640014648
Step: 23110, train/learning_rate: 2.2501189960166812e-05
Step: 23110, train/epoch: 5.499762058258057
Step: 23120, train/loss: 0.4300000071525574
Step: 23120, train/grad_norm: 24.333232879638672
Step: 23120, train/learning_rate: 2.2489290131488815e-05
Step: 23120, train/epoch: 5.502141952514648
Step: 23130, train/loss: 0.329800009727478
Step: 23130, train/grad_norm: 14.403003692626953
Step: 23130, train/learning_rate: 2.247739212180022e-05
Step: 23130, train/epoch: 5.50452184677124
Step: 23140, train/loss: 0.5684000253677368
Step: 23140, train/grad_norm: 11.417646408081055
Step: 23140, train/learning_rate: 2.2465492293122225e-05
Step: 23140, train/epoch: 5.506901264190674
Step: 23150, train/loss: 0.4778999984264374
Step: 23150, train/grad_norm: 8.720006942749023
Step: 23150, train/learning_rate: 2.245359428343363e-05
Step: 23150, train/epoch: 5.509281158447266
Step: 23160, train/loss: 0.43059998750686646
Step: 23160, train/grad_norm: 15.491050720214844
Step: 23160, train/learning_rate: 2.2441694454755634e-05
Step: 23160, train/epoch: 5.511661052703857
Step: 23170, train/loss: 0.3061999976634979
Step: 23170, train/grad_norm: 23.80930519104004
Step: 23170, train/learning_rate: 2.2429794626077637e-05
Step: 23170, train/epoch: 5.514040946960449
Step: 23180, train/loss: 0.3659000098705292
Step: 23180, train/grad_norm: 14.798243522644043
Step: 23180, train/learning_rate: 2.2417896616389044e-05
Step: 23180, train/epoch: 5.516420841217041
Step: 23190, train/loss: 0.3580999970436096
Step: 23190, train/grad_norm: 26.712446212768555
Step: 23190, train/learning_rate: 2.2405996787711047e-05
Step: 23190, train/epoch: 5.518800735473633
Step: 23200, train/loss: 0.3935999870300293
Step: 23200, train/grad_norm: 7.256772041320801
Step: 23200, train/learning_rate: 2.2394098778022453e-05
Step: 23200, train/epoch: 5.521180152893066
Step: 23210, train/loss: 0.3799000084400177
Step: 23210, train/grad_norm: 7.774134159088135
Step: 23210, train/learning_rate: 2.2382198949344456e-05
Step: 23210, train/epoch: 5.523560047149658
Step: 23220, train/loss: 0.4154999852180481
Step: 23220, train/grad_norm: 17.205448150634766
Step: 23220, train/learning_rate: 2.237029912066646e-05
Step: 23220, train/epoch: 5.52593994140625
Step: 23230, train/loss: 0.3937999904155731
Step: 23230, train/grad_norm: 16.57495880126953
Step: 23230, train/learning_rate: 2.2358401110977866e-05
Step: 23230, train/epoch: 5.528319835662842
Step: 23240, train/loss: 0.34150001406669617
Step: 23240, train/grad_norm: 12.542167663574219
Step: 23240, train/learning_rate: 2.234650128229987e-05
Step: 23240, train/epoch: 5.530699729919434
Step: 23250, train/loss: 0.5454000234603882
Step: 23250, train/grad_norm: 6.012016773223877
Step: 23250, train/learning_rate: 2.2334603272611275e-05
Step: 23250, train/epoch: 5.533079624176025
Step: 23260, train/loss: 0.42329999804496765
Step: 23260, train/grad_norm: 5.130329132080078
Step: 23260, train/learning_rate: 2.232270344393328e-05
Step: 23260, train/epoch: 5.535459518432617
Step: 23270, train/loss: 0.39570000767707825
Step: 23270, train/grad_norm: 10.788156509399414
Step: 23270, train/learning_rate: 2.231080361525528e-05
Step: 23270, train/epoch: 5.537838935852051
Step: 23280, train/loss: 0.4034999907016754
Step: 23280, train/grad_norm: 15.884284019470215
Step: 23280, train/learning_rate: 2.2298905605566688e-05
Step: 23280, train/epoch: 5.540218830108643
Step: 23290, train/loss: 0.2824000120162964
Step: 23290, train/grad_norm: 8.679025650024414
Step: 23290, train/learning_rate: 2.228700577688869e-05
Step: 23290, train/epoch: 5.542598724365234
Step: 23300, train/loss: 0.3100999891757965
Step: 23300, train/grad_norm: 3.03155255317688
Step: 23300, train/learning_rate: 2.2275107767200097e-05
Step: 23300, train/epoch: 5.544978618621826
Step: 23310, train/loss: 0.33329999446868896
Step: 23310, train/grad_norm: 12.97613525390625
Step: 23310, train/learning_rate: 2.22632079385221e-05
Step: 23310, train/epoch: 5.547358512878418
Step: 23320, train/loss: 0.4440000057220459
Step: 23320, train/grad_norm: 8.031438827514648
Step: 23320, train/learning_rate: 2.2251308109844103e-05
Step: 23320, train/epoch: 5.54973840713501
Step: 23330, train/loss: 0.41749998927116394
Step: 23330, train/grad_norm: 12.915976524353027
Step: 23330, train/learning_rate: 2.223941010015551e-05
Step: 23330, train/epoch: 5.552117824554443
Step: 23340, train/loss: 0.352400004863739
Step: 23340, train/grad_norm: 8.12319564819336
Step: 23340, train/learning_rate: 2.2227510271477513e-05
Step: 23340, train/epoch: 5.554497718811035
Step: 23350, train/loss: 0.3910999894142151
Step: 23350, train/grad_norm: 13.628936767578125
Step: 23350, train/learning_rate: 2.221561226178892e-05
Step: 23350, train/epoch: 5.556877613067627
Step: 23360, train/loss: 0.28859999775886536
Step: 23360, train/grad_norm: 7.38846492767334
Step: 23360, train/learning_rate: 2.2203712433110923e-05
Step: 23360, train/epoch: 5.559257507324219
Step: 23370, train/loss: 0.3305000066757202
Step: 23370, train/grad_norm: 7.605622291564941
Step: 23370, train/learning_rate: 2.2191812604432926e-05
Step: 23370, train/epoch: 5.5616374015808105
Step: 23380, train/loss: 0.4043000042438507
Step: 23380, train/grad_norm: 6.164279937744141
Step: 23380, train/learning_rate: 2.2179914594744332e-05
Step: 23380, train/epoch: 5.564017295837402
Step: 23390, train/loss: 0.33709999918937683
Step: 23390, train/grad_norm: 16.886878967285156
Step: 23390, train/learning_rate: 2.2168014766066335e-05
Step: 23390, train/epoch: 5.566397190093994
Step: 23400, train/loss: 0.28049999475479126
Step: 23400, train/grad_norm: 5.651278018951416
Step: 23400, train/learning_rate: 2.2156116756377742e-05
Step: 23400, train/epoch: 5.568776607513428
Step: 23410, train/loss: 0.25209999084472656
Step: 23410, train/grad_norm: 9.033987045288086
Step: 23410, train/learning_rate: 2.2144216927699745e-05
Step: 23410, train/epoch: 5.5711565017700195
Step: 23420, train/loss: 0.29190000891685486
Step: 23420, train/grad_norm: 16.29264259338379
Step: 23420, train/learning_rate: 2.2132317099021748e-05
Step: 23420, train/epoch: 5.573536396026611
Step: 23430, train/loss: 0.38830000162124634
Step: 23430, train/grad_norm: 8.453557968139648
Step: 23430, train/learning_rate: 2.2120419089333154e-05
Step: 23430, train/epoch: 5.575916290283203
Step: 23440, train/loss: 0.5199999809265137
Step: 23440, train/grad_norm: 19.567481994628906
Step: 23440, train/learning_rate: 2.2108519260655157e-05
Step: 23440, train/epoch: 5.578296184539795
Step: 23450, train/loss: 0.28540000319480896
Step: 23450, train/grad_norm: 8.96112060546875
Step: 23450, train/learning_rate: 2.2096621250966564e-05
Step: 23450, train/epoch: 5.580676078796387
Step: 23460, train/loss: 0.446399986743927
Step: 23460, train/grad_norm: 16.402116775512695
Step: 23460, train/learning_rate: 2.2084721422288567e-05
Step: 23460, train/epoch: 5.58305549621582
Step: 23470, train/loss: 0.41990000009536743
Step: 23470, train/grad_norm: 1.4142529964447021
Step: 23470, train/learning_rate: 2.207282159361057e-05
Step: 23470, train/epoch: 5.585435390472412
Step: 23480, train/loss: 0.4781999886035919
Step: 23480, train/grad_norm: 3.6980645656585693
Step: 23480, train/learning_rate: 2.2060923583921976e-05
Step: 23480, train/epoch: 5.587815284729004
Step: 23490, train/loss: 0.3361000120639801
Step: 23490, train/grad_norm: 7.121159553527832
Step: 23490, train/learning_rate: 2.204902375524398e-05
Step: 23490, train/epoch: 5.590195178985596
Step: 23500, train/loss: 0.38609999418258667
Step: 23500, train/grad_norm: 17.52381706237793
Step: 23500, train/learning_rate: 2.2037125745555386e-05
Step: 23500, train/epoch: 5.5925750732421875
Step: 23510, train/loss: 0.36419999599456787
Step: 23510, train/grad_norm: 3.9558868408203125
Step: 23510, train/learning_rate: 2.202522591687739e-05
Step: 23510, train/epoch: 5.594954967498779
Step: 23520, train/loss: 0.36340001225471497
Step: 23520, train/grad_norm: 19.19896125793457
Step: 23520, train/learning_rate: 2.2013326088199392e-05
Step: 23520, train/epoch: 5.597334384918213
Step: 23530, train/loss: 0.31049999594688416
Step: 23530, train/grad_norm: 17.775672912597656
Step: 23530, train/learning_rate: 2.20014280785108e-05
Step: 23530, train/epoch: 5.599714279174805
Step: 23540, train/loss: 0.3294000029563904
Step: 23540, train/grad_norm: 11.355712890625
Step: 23540, train/learning_rate: 2.19895282498328e-05
Step: 23540, train/epoch: 5.6020941734313965
Step: 23550, train/loss: 0.3497999906539917
Step: 23550, train/grad_norm: 6.384243011474609
Step: 23550, train/learning_rate: 2.1977630240144208e-05
Step: 23550, train/epoch: 5.604474067687988
Step: 23560, train/loss: 0.46650001406669617
Step: 23560, train/grad_norm: 35.453529357910156
Step: 23560, train/learning_rate: 2.196573041146621e-05
Step: 23560, train/epoch: 5.60685396194458
Step: 23570, train/loss: 0.398499995470047
Step: 23570, train/grad_norm: 20.722532272338867
Step: 23570, train/learning_rate: 2.1953832401777618e-05
Step: 23570, train/epoch: 5.609233856201172
Step: 23580, train/loss: 0.4900999963283539
Step: 23580, train/grad_norm: 14.743911743164062
Step: 23580, train/learning_rate: 2.194193257309962e-05
Step: 23580, train/epoch: 5.611613750457764
Step: 23590, train/loss: 0.2777999937534332
Step: 23590, train/grad_norm: 31.778682708740234
Step: 23590, train/learning_rate: 2.1930032744421624e-05
Step: 23590, train/epoch: 5.613993167877197
Step: 23600, train/loss: 0.3393000066280365
Step: 23600, train/grad_norm: 21.76776885986328
Step: 23600, train/learning_rate: 2.191813473473303e-05
Step: 23600, train/epoch: 5.616373062133789
Step: 23610, train/loss: 0.3952000141143799
Step: 23610, train/grad_norm: 9.420431137084961
Step: 23610, train/learning_rate: 2.1906234906055033e-05
Step: 23610, train/epoch: 5.618752956390381
Step: 23620, train/loss: 0.31189998984336853
Step: 23620, train/grad_norm: 10.020371437072754
Step: 23620, train/learning_rate: 2.189433689636644e-05
Step: 23620, train/epoch: 5.621132850646973
Step: 23630, train/loss: 0.4092999994754791
Step: 23630, train/grad_norm: 10.371771812438965
Step: 23630, train/learning_rate: 2.1882437067688443e-05
Step: 23630, train/epoch: 5.6235127449035645
Step: 23640, train/loss: 0.44589999318122864
Step: 23640, train/grad_norm: 7.072196006774902
Step: 23640, train/learning_rate: 2.1870537239010446e-05
Step: 23640, train/epoch: 5.625892639160156
Step: 23650, train/loss: 0.4465999901294708
Step: 23650, train/grad_norm: 27.538537979125977
Step: 23650, train/learning_rate: 2.1858639229321852e-05
Step: 23650, train/epoch: 5.62827205657959
Step: 23660, train/loss: 0.2903999984264374
Step: 23660, train/grad_norm: 2.156909942626953
Step: 23660, train/learning_rate: 2.1846739400643855e-05
Step: 23660, train/epoch: 5.630651950836182
Step: 23670, train/loss: 0.5180000066757202
Step: 23670, train/grad_norm: 18.722614288330078
Step: 23670, train/learning_rate: 2.1834841390955262e-05
Step: 23670, train/epoch: 5.633031845092773
Step: 23680, train/loss: 0.4316999912261963
Step: 23680, train/grad_norm: 9.437749862670898
Step: 23680, train/learning_rate: 2.1822941562277265e-05
Step: 23680, train/epoch: 5.635411739349365
Step: 23690, train/loss: 0.2653000056743622
Step: 23690, train/grad_norm: 8.308579444885254
Step: 23690, train/learning_rate: 2.1811041733599268e-05
Step: 23690, train/epoch: 5.637791633605957
Step: 23700, train/loss: 0.37049999833106995
Step: 23700, train/grad_norm: 3.29648494720459
Step: 23700, train/learning_rate: 2.1799143723910674e-05
Step: 23700, train/epoch: 5.640171527862549
Step: 23710, train/loss: 0.3878999948501587
Step: 23710, train/grad_norm: 7.465036869049072
Step: 23710, train/learning_rate: 2.1787243895232677e-05
Step: 23710, train/epoch: 5.642550945281982
Step: 23720, train/loss: 0.5827000141143799
Step: 23720, train/grad_norm: 18.87206268310547
Step: 23720, train/learning_rate: 2.1775345885544084e-05
Step: 23720, train/epoch: 5.644930839538574
Step: 23730, train/loss: 0.2736000120639801
Step: 23730, train/grad_norm: 11.940796852111816
Step: 23730, train/learning_rate: 2.1763446056866087e-05
Step: 23730, train/epoch: 5.647310733795166
Step: 23740, train/loss: 0.30239999294281006
Step: 23740, train/grad_norm: 8.542685508728027
Step: 23740, train/learning_rate: 2.175154622818809e-05
Step: 23740, train/epoch: 5.649690628051758
Step: 23750, train/loss: 0.32199999690055847
Step: 23750, train/grad_norm: 7.158176422119141
Step: 23750, train/learning_rate: 2.1739648218499497e-05
Step: 23750, train/epoch: 5.65207052230835
Step: 23760, train/loss: 0.2492000013589859
Step: 23760, train/grad_norm: 8.170204162597656
Step: 23760, train/learning_rate: 2.17277483898215e-05
Step: 23760, train/epoch: 5.654450416564941
Step: 23770, train/loss: 0.43720000982284546
Step: 23770, train/grad_norm: 11.329581260681152
Step: 23770, train/learning_rate: 2.1715850380132906e-05
Step: 23770, train/epoch: 5.656830310821533
Step: 23780, train/loss: 0.5827999711036682
Step: 23780, train/grad_norm: 13.08937931060791
Step: 23780, train/learning_rate: 2.170395055145491e-05
Step: 23780, train/epoch: 5.659209728240967
Step: 23790, train/loss: 0.4616999924182892
Step: 23790, train/grad_norm: 11.516551971435547
Step: 23790, train/learning_rate: 2.1692050722776912e-05
Step: 23790, train/epoch: 5.661589622497559
Step: 23800, train/loss: 0.37950000166893005
Step: 23800, train/grad_norm: 23.13909339904785
Step: 23800, train/learning_rate: 2.168015271308832e-05
Step: 23800, train/epoch: 5.66396951675415
Step: 23810, train/loss: 0.3955000042915344
Step: 23810, train/grad_norm: 32.43389892578125
Step: 23810, train/learning_rate: 2.1668252884410322e-05
Step: 23810, train/epoch: 5.666349411010742
Step: 23820, train/loss: 0.2092999964952469
Step: 23820, train/grad_norm: 5.560641288757324
Step: 23820, train/learning_rate: 2.1656354874721728e-05
Step: 23820, train/epoch: 5.668729305267334
Step: 23830, train/loss: 0.36419999599456787
Step: 23830, train/grad_norm: 14.484463691711426
Step: 23830, train/learning_rate: 2.164445504604373e-05
Step: 23830, train/epoch: 5.671109199523926
Step: 23840, train/loss: 0.43070000410079956
Step: 23840, train/grad_norm: 8.869649887084961
Step: 23840, train/learning_rate: 2.1632555217365734e-05
Step: 23840, train/epoch: 5.673488616943359
Step: 23850, train/loss: 0.3677999973297119
Step: 23850, train/grad_norm: 13.885773658752441
Step: 23850, train/learning_rate: 2.162065720767714e-05
Step: 23850, train/epoch: 5.675868511199951
Step: 23860, train/loss: 0.36500000953674316
Step: 23860, train/grad_norm: 9.907241821289062
Step: 23860, train/learning_rate: 2.1608757378999144e-05
Step: 23860, train/epoch: 5.678248405456543
Step: 23870, train/loss: 0.3192000091075897
Step: 23870, train/grad_norm: 8.888704299926758
Step: 23870, train/learning_rate: 2.159685936931055e-05
Step: 23870, train/epoch: 5.680628299713135
Step: 23880, train/loss: 0.296999990940094
Step: 23880, train/grad_norm: 6.561824321746826
Step: 23880, train/learning_rate: 2.1584959540632553e-05
Step: 23880, train/epoch: 5.683008193969727
Step: 23890, train/loss: 0.3668999969959259
Step: 23890, train/grad_norm: 11.42730712890625
Step: 23890, train/learning_rate: 2.1573059711954556e-05
Step: 23890, train/epoch: 5.685388088226318
Step: 23900, train/loss: 0.6075000166893005
Step: 23900, train/grad_norm: 14.72205638885498
Step: 23900, train/learning_rate: 2.1561161702265963e-05
Step: 23900, train/epoch: 5.687767505645752
Step: 23910, train/loss: 0.36629998683929443
Step: 23910, train/grad_norm: 23.95551872253418
Step: 23910, train/learning_rate: 2.1549261873587966e-05
Step: 23910, train/epoch: 5.690147399902344
Step: 23920, train/loss: 0.2939999997615814
Step: 23920, train/grad_norm: 7.380814552307129
Step: 23920, train/learning_rate: 2.1537363863899373e-05
Step: 23920, train/epoch: 5.6925272941589355
Step: 23930, train/loss: 0.38530001044273376
Step: 23930, train/grad_norm: 21.709957122802734
Step: 23930, train/learning_rate: 2.1525464035221376e-05
Step: 23930, train/epoch: 5.694907188415527
Step: 23940, train/loss: 0.38679999113082886
Step: 23940, train/grad_norm: 17.958791732788086
Step: 23940, train/learning_rate: 2.151356420654338e-05
Step: 23940, train/epoch: 5.697287082672119
Step: 23950, train/loss: 0.3310000002384186
Step: 23950, train/grad_norm: 10.45820426940918
Step: 23950, train/learning_rate: 2.1501666196854785e-05
Step: 23950, train/epoch: 5.699666976928711
Step: 23960, train/loss: 0.2587999999523163
Step: 23960, train/grad_norm: 12.744362831115723
Step: 23960, train/learning_rate: 2.1489766368176788e-05
Step: 23960, train/epoch: 5.702046871185303
Step: 23970, train/loss: 0.4690000116825104
Step: 23970, train/grad_norm: 20.30101776123047
Step: 23970, train/learning_rate: 2.1477868358488195e-05
Step: 23970, train/epoch: 5.704426288604736
Step: 23980, train/loss: 0.39259999990463257
Step: 23980, train/grad_norm: 15.13286304473877
Step: 23980, train/learning_rate: 2.1465968529810198e-05
Step: 23980, train/epoch: 5.706806182861328
Step: 23990, train/loss: 0.2222999930381775
Step: 23990, train/grad_norm: 2.353224754333496
Step: 23990, train/learning_rate: 2.14540687011322e-05
Step: 23990, train/epoch: 5.70918607711792
Step: 24000, train/loss: 0.42080000042915344
Step: 24000, train/grad_norm: 6.784195423126221
Step: 24000, train/learning_rate: 2.1442170691443607e-05
Step: 24000, train/epoch: 5.711565971374512
Step: 24010, train/loss: 0.4255000054836273
Step: 24010, train/grad_norm: 23.49869728088379
Step: 24010, train/learning_rate: 2.143027086276561e-05
Step: 24010, train/epoch: 5.7139458656311035
Step: 24020, train/loss: 0.5444999933242798
Step: 24020, train/grad_norm: 5.744472503662109
Step: 24020, train/learning_rate: 2.1418372853077017e-05
Step: 24020, train/epoch: 5.716325759887695
Step: 24030, train/loss: 0.26570001244544983
Step: 24030, train/grad_norm: 6.817192077636719
Step: 24030, train/learning_rate: 2.140647302439902e-05
Step: 24030, train/epoch: 5.718705177307129
Step: 24040, train/loss: 0.22920000553131104
Step: 24040, train/grad_norm: 2.517878532409668
Step: 24040, train/learning_rate: 2.1394573195721023e-05
Step: 24040, train/epoch: 5.721085071563721
Step: 24050, train/loss: 0.2775000035762787
Step: 24050, train/grad_norm: 17.366168975830078
Step: 24050, train/learning_rate: 2.138267518603243e-05
Step: 24050, train/epoch: 5.7234649658203125
Step: 24060, train/loss: 0.3806000053882599
Step: 24060, train/grad_norm: 23.871746063232422
Step: 24060, train/learning_rate: 2.1370775357354432e-05
Step: 24060, train/epoch: 5.725844860076904
Step: 24070, train/loss: 0.3287000060081482
Step: 24070, train/grad_norm: 12.872418403625488
Step: 24070, train/learning_rate: 2.135887734766584e-05
Step: 24070, train/epoch: 5.728224754333496
Step: 24080, train/loss: 0.3093000054359436
Step: 24080, train/grad_norm: 9.835268020629883
Step: 24080, train/learning_rate: 2.1346977518987842e-05
Step: 24080, train/epoch: 5.730604648590088
Step: 24090, train/loss: 0.3483000099658966
Step: 24090, train/grad_norm: 16.3419246673584
Step: 24090, train/learning_rate: 2.1335077690309845e-05
Step: 24090, train/epoch: 5.7329840660095215
Step: 24100, train/loss: 0.2671999931335449
Step: 24100, train/grad_norm: 12.346839904785156
Step: 24100, train/learning_rate: 2.132317968062125e-05
Step: 24100, train/epoch: 5.735363960266113
Step: 24110, train/loss: 0.3249000012874603
Step: 24110, train/grad_norm: 3.6340670585632324
Step: 24110, train/learning_rate: 2.1311279851943254e-05
Step: 24110, train/epoch: 5.737743854522705
Step: 24120, train/loss: 0.5159000158309937
Step: 24120, train/grad_norm: 13.10315227508545
Step: 24120, train/learning_rate: 2.129938184225466e-05
Step: 24120, train/epoch: 5.740123748779297
Step: 24130, train/loss: 0.30390000343322754
Step: 24130, train/grad_norm: 4.0576019287109375
Step: 24130, train/learning_rate: 2.1287482013576664e-05
Step: 24130, train/epoch: 5.742503643035889
Step: 24140, train/loss: 0.3528999984264374
Step: 24140, train/grad_norm: 4.714513301849365
Step: 24140, train/learning_rate: 2.1275582184898667e-05
Step: 24140, train/epoch: 5.7448835372924805
Step: 24150, train/loss: 0.29179999232292175
Step: 24150, train/grad_norm: 16.89061164855957
Step: 24150, train/learning_rate: 2.1263684175210074e-05
Step: 24150, train/epoch: 5.747263431549072
Step: 24160, train/loss: 0.3540000021457672
Step: 24160, train/grad_norm: 24.15378189086914
Step: 24160, train/learning_rate: 2.1251784346532077e-05
Step: 24160, train/epoch: 5.749642848968506
Step: 24170, train/loss: 0.3140999972820282
Step: 24170, train/grad_norm: 9.804264068603516
Step: 24170, train/learning_rate: 2.1239886336843483e-05
Step: 24170, train/epoch: 5.752022743225098
Step: 24180, train/loss: 0.22179999947547913
Step: 24180, train/grad_norm: 7.6609110832214355
Step: 24180, train/learning_rate: 2.1227986508165486e-05
Step: 24180, train/epoch: 5.7544026374816895
Step: 24190, train/loss: 0.42149999737739563
Step: 24190, train/grad_norm: 24.445011138916016
Step: 24190, train/learning_rate: 2.121608667948749e-05
Step: 24190, train/epoch: 5.756782531738281
Step: 24200, train/loss: 0.357699990272522
Step: 24200, train/grad_norm: 11.974267959594727
Step: 24200, train/learning_rate: 2.1204188669798896e-05
Step: 24200, train/epoch: 5.759162425994873
Step: 24210, train/loss: 0.40230000019073486
Step: 24210, train/grad_norm: 21.395648956298828
Step: 24210, train/learning_rate: 2.11922888411209e-05
Step: 24210, train/epoch: 5.761542320251465
Step: 24220, train/loss: 0.5077999830245972
Step: 24220, train/grad_norm: 18.157573699951172
Step: 24220, train/learning_rate: 2.1180390831432305e-05
Step: 24220, train/epoch: 5.763921737670898
Step: 24230, train/loss: 0.4372999966144562
Step: 24230, train/grad_norm: 18.753564834594727
Step: 24230, train/learning_rate: 2.1168491002754308e-05
Step: 24230, train/epoch: 5.76630163192749
Step: 24240, train/loss: 0.36239999532699585
Step: 24240, train/grad_norm: 2.568126916885376
Step: 24240, train/learning_rate: 2.1156592993065715e-05
Step: 24240, train/epoch: 5.768681526184082
Step: 24250, train/loss: 0.23199999332427979
Step: 24250, train/grad_norm: 7.7814836502075195
Step: 24250, train/learning_rate: 2.1144693164387718e-05
Step: 24250, train/epoch: 5.771061420440674
Step: 24260, train/loss: 0.45339998602867126
Step: 24260, train/grad_norm: 8.73447322845459
Step: 24260, train/learning_rate: 2.113279333570972e-05
Step: 24260, train/epoch: 5.773441314697266
Step: 24270, train/loss: 0.414000004529953
Step: 24270, train/grad_norm: 9.890849113464355
Step: 24270, train/learning_rate: 2.1120895326021127e-05
Step: 24270, train/epoch: 5.775821208953857
Step: 24280, train/loss: 0.3919999897480011
Step: 24280, train/grad_norm: 12.808009147644043
Step: 24280, train/learning_rate: 2.110899549734313e-05
Step: 24280, train/epoch: 5.778200626373291
Step: 24290, train/loss: 0.3199999928474426
Step: 24290, train/grad_norm: 9.034500122070312
Step: 24290, train/learning_rate: 2.1097097487654537e-05
Step: 24290, train/epoch: 5.780580520629883
Step: 24300, train/loss: 0.3068999946117401
Step: 24300, train/grad_norm: 10.462735176086426
Step: 24300, train/learning_rate: 2.108519765897654e-05
Step: 24300, train/epoch: 5.782960414886475
Step: 24310, train/loss: 0.3154999911785126
Step: 24310, train/grad_norm: 15.999003410339355
Step: 24310, train/learning_rate: 2.1073297830298543e-05
Step: 24310, train/epoch: 5.785340309143066
Step: 24320, train/loss: 0.36660000681877136
Step: 24320, train/grad_norm: 25.300647735595703
Step: 24320, train/learning_rate: 2.106139982060995e-05
Step: 24320, train/epoch: 5.787720203399658
Step: 24330, train/loss: 0.5110999941825867
Step: 24330, train/grad_norm: 8.103891372680664
Step: 24330, train/learning_rate: 2.1049499991931953e-05
Step: 24330, train/epoch: 5.79010009765625
Step: 24340, train/loss: 0.41999998688697815
Step: 24340, train/grad_norm: 3.199903726577759
Step: 24340, train/learning_rate: 2.103760198224336e-05
Step: 24340, train/epoch: 5.792479991912842
Step: 24350, train/loss: 0.3935999870300293
Step: 24350, train/grad_norm: 10.951529502868652
Step: 24350, train/learning_rate: 2.1025702153565362e-05
Step: 24350, train/epoch: 5.794859409332275
Step: 24360, train/loss: 0.4415999948978424
Step: 24360, train/grad_norm: 26.691728591918945
Step: 24360, train/learning_rate: 2.1013802324887365e-05
Step: 24360, train/epoch: 5.797239303588867
Step: 24370, train/loss: 0.3774000108242035
Step: 24370, train/grad_norm: 5.884993553161621
Step: 24370, train/learning_rate: 2.100190431519877e-05
Step: 24370, train/epoch: 5.799619197845459
Step: 24380, train/loss: 0.4318999946117401
Step: 24380, train/grad_norm: 13.405896186828613
Step: 24380, train/learning_rate: 2.0990004486520775e-05
Step: 24380, train/epoch: 5.801999092102051
Step: 24390, train/loss: 0.29109999537467957
Step: 24390, train/grad_norm: 12.90942668914795
Step: 24390, train/learning_rate: 2.097810647683218e-05
Step: 24390, train/epoch: 5.804378986358643
Step: 24400, train/loss: 0.36649999022483826
Step: 24400, train/grad_norm: 22.155635833740234
Step: 24400, train/learning_rate: 2.0966206648154184e-05
Step: 24400, train/epoch: 5.806758880615234
Step: 24410, train/loss: 0.4097999930381775
Step: 24410, train/grad_norm: 13.961050987243652
Step: 24410, train/learning_rate: 2.0954306819476187e-05
Step: 24410, train/epoch: 5.809138298034668
Step: 24420, train/loss: 0.4000999927520752
Step: 24420, train/grad_norm: 14.048291206359863
Step: 24420, train/learning_rate: 2.0942408809787594e-05
Step: 24420, train/epoch: 5.81151819229126
Step: 24430, train/loss: 0.32519999146461487
Step: 24430, train/grad_norm: 13.138319969177246
Step: 24430, train/learning_rate: 2.0930508981109597e-05
Step: 24430, train/epoch: 5.813898086547852
Step: 24440, train/loss: 0.3785000145435333
Step: 24440, train/grad_norm: 6.986085414886475
Step: 24440, train/learning_rate: 2.0918610971421003e-05
Step: 24440, train/epoch: 5.816277980804443
Step: 24450, train/loss: 0.4196000099182129
Step: 24450, train/grad_norm: 16.1069393157959
Step: 24450, train/learning_rate: 2.0906711142743006e-05
Step: 24450, train/epoch: 5.818657875061035
Step: 24460, train/loss: 0.36880001425743103
Step: 24460, train/grad_norm: 13.545016288757324
Step: 24460, train/learning_rate: 2.089481131406501e-05
Step: 24460, train/epoch: 5.821037769317627
Step: 24470, train/loss: 0.3612000048160553
Step: 24470, train/grad_norm: 14.287642478942871
Step: 24470, train/learning_rate: 2.0882913304376416e-05
Step: 24470, train/epoch: 5.8234171867370605
Step: 24480, train/loss: 0.30160000920295715
Step: 24480, train/grad_norm: 3.1120474338531494
Step: 24480, train/learning_rate: 2.087101347569842e-05
Step: 24480, train/epoch: 5.825797080993652
Step: 24490, train/loss: 0.43939998745918274
Step: 24490, train/grad_norm: 14.251606941223145
Step: 24490, train/learning_rate: 2.0859115466009825e-05
Step: 24490, train/epoch: 5.828176975250244
Step: 24500, train/loss: 0.4357999861240387
Step: 24500, train/grad_norm: 15.439493179321289
Step: 24500, train/learning_rate: 2.084721563733183e-05
Step: 24500, train/epoch: 5.830556869506836
Step: 24510, train/loss: 0.24709999561309814
Step: 24510, train/grad_norm: 5.721830368041992
Step: 24510, train/learning_rate: 2.083531580865383e-05
Step: 24510, train/epoch: 5.832936763763428
Step: 24520, train/loss: 0.39070001244544983
Step: 24520, train/grad_norm: 5.40585994720459
Step: 24520, train/learning_rate: 2.0823417798965238e-05
Step: 24520, train/epoch: 5.8353166580200195
Step: 24530, train/loss: 0.3596000075340271
Step: 24530, train/grad_norm: 12.004755020141602
Step: 24530, train/learning_rate: 2.081151797028724e-05
Step: 24530, train/epoch: 5.837696552276611
Step: 24540, train/loss: 0.46050000190734863
Step: 24540, train/grad_norm: 9.343853950500488
Step: 24540, train/learning_rate: 2.0799619960598648e-05
Step: 24540, train/epoch: 5.840075969696045
Step: 24550, train/loss: 0.5288000106811523
Step: 24550, train/grad_norm: 15.848530769348145
Step: 24550, train/learning_rate: 2.078772013192065e-05
Step: 24550, train/epoch: 5.842455863952637
Step: 24560, train/loss: 0.36649999022483826
Step: 24560, train/grad_norm: 3.6404879093170166
Step: 24560, train/learning_rate: 2.0775820303242654e-05
Step: 24560, train/epoch: 5.8448357582092285
Step: 24570, train/loss: 0.4032999873161316
Step: 24570, train/grad_norm: 13.745207786560059
Step: 24570, train/learning_rate: 2.076392229355406e-05
Step: 24570, train/epoch: 5.84721565246582
Step: 24580, train/loss: 0.31130000948905945
Step: 24580, train/grad_norm: 4.683660984039307
Step: 24580, train/learning_rate: 2.0752022464876063e-05
Step: 24580, train/epoch: 5.849595546722412
Step: 24590, train/loss: 0.3492000102996826
Step: 24590, train/grad_norm: 23.53306007385254
Step: 24590, train/learning_rate: 2.074012445518747e-05
Step: 24590, train/epoch: 5.851975440979004
Step: 24600, train/loss: 0.26919999718666077
Step: 24600, train/grad_norm: 8.738932609558105
Step: 24600, train/learning_rate: 2.0728224626509473e-05
Step: 24600, train/epoch: 5.8543548583984375
Step: 24610, train/loss: 0.36730000376701355
Step: 24610, train/grad_norm: 20.579442977905273
Step: 24610, train/learning_rate: 2.0716324797831476e-05
Step: 24610, train/epoch: 5.856734752655029
Step: 24620, train/loss: 0.2791999876499176
Step: 24620, train/grad_norm: 4.716657638549805
Step: 24620, train/learning_rate: 2.0704426788142882e-05
Step: 24620, train/epoch: 5.859114646911621
Step: 24630, train/loss: 0.42640000581741333
Step: 24630, train/grad_norm: 11.653453826904297
Step: 24630, train/learning_rate: 2.0692526959464885e-05
Step: 24630, train/epoch: 5.861494541168213
Step: 24640, train/loss: 0.33090001344680786
Step: 24640, train/grad_norm: 13.692072868347168
Step: 24640, train/learning_rate: 2.0680628949776292e-05
Step: 24640, train/epoch: 5.863874435424805
Step: 24650, train/loss: 0.32170000672340393
Step: 24650, train/grad_norm: 4.115798473358154
Step: 24650, train/learning_rate: 2.0668729121098295e-05
Step: 24650, train/epoch: 5.8662543296813965
Step: 24660, train/loss: 0.35249999165534973
Step: 24660, train/grad_norm: 8.472602844238281
Step: 24660, train/learning_rate: 2.0656829292420298e-05
Step: 24660, train/epoch: 5.86863374710083
Step: 24670, train/loss: 0.506600022315979
Step: 24670, train/grad_norm: 9.208327293395996
Step: 24670, train/learning_rate: 2.0644931282731704e-05
Step: 24670, train/epoch: 5.871013641357422
Step: 24680, train/loss: 0.3000999987125397
Step: 24680, train/grad_norm: 14.569673538208008
Step: 24680, train/learning_rate: 2.0633031454053707e-05
Step: 24680, train/epoch: 5.873393535614014
Step: 24690, train/loss: 0.3817000091075897
Step: 24690, train/grad_norm: 21.274700164794922
Step: 24690, train/learning_rate: 2.0621133444365114e-05
Step: 24690, train/epoch: 5.8757734298706055
Step: 24700, train/loss: 0.3919000029563904
Step: 24700, train/grad_norm: 13.554174423217773
Step: 24700, train/learning_rate: 2.0609233615687117e-05
Step: 24700, train/epoch: 5.878153324127197
Step: 24710, train/loss: 0.250900000333786
Step: 24710, train/grad_norm: 2.425766706466675
Step: 24710, train/learning_rate: 2.059733378700912e-05
Step: 24710, train/epoch: 5.880533218383789
Step: 24720, train/loss: 0.3937999904155731
Step: 24720, train/grad_norm: 7.978933334350586
Step: 24720, train/learning_rate: 2.0585435777320527e-05
Step: 24720, train/epoch: 5.882913112640381
Step: 24730, train/loss: 0.22020000219345093
Step: 24730, train/grad_norm: 4.899154186248779
Step: 24730, train/learning_rate: 2.057353594864253e-05
Step: 24730, train/epoch: 5.8852925300598145
Step: 24740, train/loss: 0.305400013923645
Step: 24740, train/grad_norm: 0.9185803532600403
Step: 24740, train/learning_rate: 2.0561637938953936e-05
Step: 24740, train/epoch: 5.887672424316406
Step: 24750, train/loss: 0.39809998869895935
Step: 24750, train/grad_norm: 19.888586044311523
Step: 24750, train/learning_rate: 2.054973811027594e-05
Step: 24750, train/epoch: 5.890052318572998
Step: 24760, train/loss: 0.3601999878883362
Step: 24760, train/grad_norm: 9.448267936706543
Step: 24760, train/learning_rate: 2.0537838281597942e-05
Step: 24760, train/epoch: 5.89243221282959
Step: 24770, train/loss: 0.31540000438690186
Step: 24770, train/grad_norm: 5.96492862701416
Step: 24770, train/learning_rate: 2.052594027190935e-05
Step: 24770, train/epoch: 5.894812107086182
Step: 24780, train/loss: 0.2093999981880188
Step: 24780, train/grad_norm: 4.51200008392334
Step: 24780, train/learning_rate: 2.051404044323135e-05
Step: 24780, train/epoch: 5.897192001342773
Step: 24790, train/loss: 0.3440999984741211
Step: 24790, train/grad_norm: 4.5095343589782715
Step: 24790, train/learning_rate: 2.0502142433542758e-05
Step: 24790, train/epoch: 5.899571418762207
Step: 24800, train/loss: 0.3400999903678894
Step: 24800, train/grad_norm: 11.686137199401855
Step: 24800, train/learning_rate: 2.049024260486476e-05
Step: 24800, train/epoch: 5.901951313018799
Step: 24810, train/loss: 0.4593000113964081
Step: 24810, train/grad_norm: 29.064741134643555
Step: 24810, train/learning_rate: 2.0478342776186764e-05
Step: 24810, train/epoch: 5.904331207275391
Step: 24820, train/loss: 0.41620001196861267
Step: 24820, train/grad_norm: 18.371288299560547
Step: 24820, train/learning_rate: 2.046644476649817e-05
Step: 24820, train/epoch: 5.906711101531982
Step: 24830, train/loss: 0.24400000274181366
Step: 24830, train/grad_norm: 18.741168975830078
Step: 24830, train/learning_rate: 2.0454544937820174e-05
Step: 24830, train/epoch: 5.909090995788574
Step: 24840, train/loss: 0.26669999957084656
Step: 24840, train/grad_norm: 4.446238040924072
Step: 24840, train/learning_rate: 2.044264692813158e-05
Step: 24840, train/epoch: 5.911470890045166
Step: 24850, train/loss: 0.23929999768733978
Step: 24850, train/grad_norm: 8.756739616394043
Step: 24850, train/learning_rate: 2.0430747099453583e-05
Step: 24850, train/epoch: 5.913850784301758
Step: 24860, train/loss: 0.27559998631477356
Step: 24860, train/grad_norm: 12.695124626159668
Step: 24860, train/learning_rate: 2.0418847270775586e-05
Step: 24860, train/epoch: 5.916230201721191
Step: 24870, train/loss: 0.4456999897956848
Step: 24870, train/grad_norm: 10.176348686218262
Step: 24870, train/learning_rate: 2.0406949261086993e-05
Step: 24870, train/epoch: 5.918610095977783
Step: 24880, train/loss: 0.39419999718666077
Step: 24880, train/grad_norm: 12.116289138793945
Step: 24880, train/learning_rate: 2.0395049432408996e-05
Step: 24880, train/epoch: 5.920989990234375
Step: 24890, train/loss: 0.36079999804496765
Step: 24890, train/grad_norm: 8.195281982421875
Step: 24890, train/learning_rate: 2.0383151422720402e-05
Step: 24890, train/epoch: 5.923369884490967
Step: 24900, train/loss: 0.18320000171661377
Step: 24900, train/grad_norm: 3.044858694076538
Step: 24900, train/learning_rate: 2.0371251594042405e-05
Step: 24900, train/epoch: 5.925749778747559
Step: 24910, train/loss: 0.5084999799728394
Step: 24910, train/grad_norm: 11.073332786560059
Step: 24910, train/learning_rate: 2.0359353584353812e-05
Step: 24910, train/epoch: 5.92812967300415
Step: 24920, train/loss: 0.4090999960899353
Step: 24920, train/grad_norm: 10.364436149597168
Step: 24920, train/learning_rate: 2.0347453755675815e-05
Step: 24920, train/epoch: 5.930509090423584
Step: 24930, train/loss: 0.4754999876022339
Step: 24930, train/grad_norm: 21.73893165588379
Step: 24930, train/learning_rate: 2.0335553926997818e-05
Step: 24930, train/epoch: 5.932888984680176
Step: 24940, train/loss: 0.450300008058548
Step: 24940, train/grad_norm: 22.44891929626465
Step: 24940, train/learning_rate: 2.0323655917309225e-05
Step: 24940, train/epoch: 5.935268878936768
Step: 24950, train/loss: 0.4584999978542328
Step: 24950, train/grad_norm: 11.236410140991211
Step: 24950, train/learning_rate: 2.0311756088631228e-05
Step: 24950, train/epoch: 5.937648773193359
Step: 24960, train/loss: 0.22509999573230743
Step: 24960, train/grad_norm: 15.104364395141602
Step: 24960, train/learning_rate: 2.0299858078942634e-05
Step: 24960, train/epoch: 5.940028667449951
Step: 24970, train/loss: 0.4345000088214874
Step: 24970, train/grad_norm: 12.545928001403809
Step: 24970, train/learning_rate: 2.0287958250264637e-05
Step: 24970, train/epoch: 5.942408561706543
Step: 24980, train/loss: 0.4530999958515167
Step: 24980, train/grad_norm: 1.9797002077102661
Step: 24980, train/learning_rate: 2.027605842158664e-05
Step: 24980, train/epoch: 5.944787979125977
Step: 24990, train/loss: 0.28450000286102295
Step: 24990, train/grad_norm: 13.856745719909668
Step: 24990, train/learning_rate: 2.0264160411898047e-05
Step: 24990, train/epoch: 5.947167873382568
Step: 25000, train/loss: 0.37529999017715454
Step: 25000, train/grad_norm: 5.175990581512451
Step: 25000, train/learning_rate: 2.025226058322005e-05
Step: 25000, train/epoch: 5.94954776763916
Step: 25010, train/loss: 0.25679999589920044
Step: 25010, train/grad_norm: 5.915589332580566
Step: 25010, train/learning_rate: 2.0240362573531456e-05
Step: 25010, train/epoch: 5.951927661895752
Step: 25020, train/loss: 0.47279998660087585
Step: 25020, train/grad_norm: 18.834890365600586
Step: 25020, train/learning_rate: 2.022846274485346e-05
Step: 25020, train/epoch: 5.954307556152344
Step: 25030, train/loss: 0.3837999999523163
Step: 25030, train/grad_norm: 1.9468116760253906
Step: 25030, train/learning_rate: 2.0216562916175462e-05
Step: 25030, train/epoch: 5.9566874504089355
Step: 25040, train/loss: 0.43779999017715454
Step: 25040, train/grad_norm: 7.476627826690674
Step: 25040, train/learning_rate: 2.020466490648687e-05
Step: 25040, train/epoch: 5.959067344665527
Step: 25050, train/loss: 0.3140000104904175
Step: 25050, train/grad_norm: 9.663472175598145
Step: 25050, train/learning_rate: 2.0192765077808872e-05
Step: 25050, train/epoch: 5.961446762084961
Step: 25060, train/loss: 0.558899998664856
Step: 25060, train/grad_norm: 34.52378463745117
Step: 25060, train/learning_rate: 2.018086706812028e-05
Step: 25060, train/epoch: 5.963826656341553
Step: 25070, train/loss: 0.4318999946117401
Step: 25070, train/grad_norm: 14.575055122375488
Step: 25070, train/learning_rate: 2.016896723944228e-05
Step: 25070, train/epoch: 5.9662065505981445
Step: 25080, train/loss: 0.3059000074863434
Step: 25080, train/grad_norm: 15.96499252319336
Step: 25080, train/learning_rate: 2.0157067410764284e-05
Step: 25080, train/epoch: 5.968586444854736
Step: 25090, train/loss: 0.42089998722076416
Step: 25090, train/grad_norm: 34.16120910644531
Step: 25090, train/learning_rate: 2.014516940107569e-05
Step: 25090, train/epoch: 5.970966339111328
Step: 25100, train/loss: 0.4244000017642975
Step: 25100, train/grad_norm: 17.669939041137695
Step: 25100, train/learning_rate: 2.0133269572397694e-05
Step: 25100, train/epoch: 5.97334623336792
Step: 25110, train/loss: 0.45509999990463257
Step: 25110, train/grad_norm: 14.240488052368164
Step: 25110, train/learning_rate: 2.01213715627091e-05
Step: 25110, train/epoch: 5.9757256507873535
Step: 25120, train/loss: 0.3028999865055084
Step: 25120, train/grad_norm: 11.235427856445312
Step: 25120, train/learning_rate: 2.0109471734031104e-05
Step: 25120, train/epoch: 5.978105545043945
Step: 25130, train/loss: 0.3580000102519989
Step: 25130, train/grad_norm: 19.367382049560547
Step: 25130, train/learning_rate: 2.0097571905353107e-05
Step: 25130, train/epoch: 5.980485439300537
Step: 25140, train/loss: 0.2851000130176544
Step: 25140, train/grad_norm: 6.681746482849121
Step: 25140, train/learning_rate: 2.0085673895664513e-05
Step: 25140, train/epoch: 5.982865333557129
Step: 25150, train/loss: 0.375900000333786
Step: 25150, train/grad_norm: 17.383275985717773
Step: 25150, train/learning_rate: 2.0073774066986516e-05
Step: 25150, train/epoch: 5.985245227813721
Step: 25160, train/loss: 0.5796999931335449
Step: 25160, train/grad_norm: 15.826408386230469
Step: 25160, train/learning_rate: 2.0061876057297923e-05
Step: 25160, train/epoch: 5.9876251220703125
Step: 25170, train/loss: 0.3617999851703644
Step: 25170, train/grad_norm: 9.002854347229004
Step: 25170, train/learning_rate: 2.0049976228619926e-05
Step: 25170, train/epoch: 5.990004539489746
Step: 25180, train/loss: 0.23690000176429749
Step: 25180, train/grad_norm: 22.30559730529785
Step: 25180, train/learning_rate: 2.003807639994193e-05
Step: 25180, train/epoch: 5.992384433746338
Step: 25190, train/loss: 0.3046000003814697
Step: 25190, train/grad_norm: 3.8753976821899414
Step: 25190, train/learning_rate: 2.0026178390253335e-05
Step: 25190, train/epoch: 5.99476432800293
Step: 25200, train/loss: 0.5121999979019165
Step: 25200, train/grad_norm: 15.578079223632812
Step: 25200, train/learning_rate: 2.0014278561575338e-05
Step: 25200, train/epoch: 5.9971442222595215
Step: 25210, train/loss: 0.42809998989105225
Step: 25210, train/grad_norm: 10.381898880004883
Step: 25210, train/learning_rate: 2.0002380551886745e-05
Step: 25210, train/epoch: 5.999524116516113
Step: 25212, eval/loss: 0.8716239929199219
Step: 25212, eval/accuracy: 0.6819380521774292
Step: 25212, eval/f1: 0.6807684302330017
Step: 25212, eval/runtime: 55.619998931884766
Step: 25212, eval/samples_per_second: 129.50399780273438
Step: 25212, eval/steps_per_second: 16.198999404907227
Step: 25212, train/epoch: 6.0
Step: 25220, train/loss: 0.3522000014781952
Step: 25220, train/grad_norm: 13.347633361816406
Step: 25220, train/learning_rate: 1.9990480723208748e-05
Step: 25220, train/epoch: 6.001904010772705
Step: 25230, train/loss: 0.5005999803543091
Step: 25230, train/grad_norm: 14.65914535522461
Step: 25230, train/learning_rate: 1.997858089453075e-05
Step: 25230, train/epoch: 6.004283905029297
Step: 25240, train/loss: 0.4336000084877014
Step: 25240, train/grad_norm: 8.363059997558594
Step: 25240, train/learning_rate: 1.9966682884842157e-05
Step: 25240, train/epoch: 6.0066633224487305
Step: 25250, train/loss: 0.23810000717639923
Step: 25250, train/grad_norm: 21.815017700195312
Step: 25250, train/learning_rate: 1.995478305616416e-05
Step: 25250, train/epoch: 6.009043216705322
Step: 25260, train/loss: 0.18250000476837158
Step: 25260, train/grad_norm: 8.92740249633789
Step: 25260, train/learning_rate: 1.9942885046475567e-05
Step: 25260, train/epoch: 6.011423110961914
Step: 25270, train/loss: 0.329800009727478
Step: 25270, train/grad_norm: 8.191479682922363
Step: 25270, train/learning_rate: 1.993098521779757e-05
Step: 25270, train/epoch: 6.013803005218506
Step: 25280, train/loss: 0.3465999960899353
Step: 25280, train/grad_norm: 7.62093448638916
Step: 25280, train/learning_rate: 1.9919085389119573e-05
Step: 25280, train/epoch: 6.016182899475098
Step: 25290, train/loss: 0.29820001125335693
Step: 25290, train/grad_norm: 7.837622165679932
Step: 25290, train/learning_rate: 1.990718737943098e-05
Step: 25290, train/epoch: 6.0185627937316895
Step: 25300, train/loss: 0.23980000615119934
Step: 25300, train/grad_norm: 16.6389102935791
Step: 25300, train/learning_rate: 1.9895287550752982e-05
Step: 25300, train/epoch: 6.020942211151123
Step: 25310, train/loss: 0.4377000033855438
Step: 25310, train/grad_norm: 9.723730087280273
Step: 25310, train/learning_rate: 1.988338954106439e-05
Step: 25310, train/epoch: 6.023322105407715
Step: 25320, train/loss: 0.42730000615119934
Step: 25320, train/grad_norm: 15.91562557220459
Step: 25320, train/learning_rate: 1.9871489712386392e-05
Step: 25320, train/epoch: 6.025701999664307
Step: 25330, train/loss: 0.44600000977516174
Step: 25330, train/grad_norm: 4.967569828033447
Step: 25330, train/learning_rate: 1.9859589883708395e-05
Step: 25330, train/epoch: 6.028081893920898
Step: 25340, train/loss: 0.4462999999523163
Step: 25340, train/grad_norm: 13.600652694702148
Step: 25340, train/learning_rate: 1.98476918740198e-05
Step: 25340, train/epoch: 6.03046178817749
Step: 25350, train/loss: 0.41589999198913574
Step: 25350, train/grad_norm: 27.171466827392578
Step: 25350, train/learning_rate: 1.9835792045341805e-05
Step: 25350, train/epoch: 6.032841682434082
Step: 25360, train/loss: 0.3237999975681305
Step: 25360, train/grad_norm: 3.2385475635528564
Step: 25360, train/learning_rate: 1.982389403565321e-05
Step: 25360, train/epoch: 6.035221099853516
Step: 25370, train/loss: 0.24060000479221344
Step: 25370, train/grad_norm: 8.541749954223633
Step: 25370, train/learning_rate: 1.9811994206975214e-05
Step: 25370, train/epoch: 6.037600994110107
Step: 25380, train/loss: 0.4032000005245209
Step: 25380, train/grad_norm: 19.63637351989746
Step: 25380, train/learning_rate: 1.9800094378297217e-05
Step: 25380, train/epoch: 6.039980888366699
Step: 25390, train/loss: 0.3009999990463257
Step: 25390, train/grad_norm: 5.473681926727295
Step: 25390, train/learning_rate: 1.9788196368608624e-05
Step: 25390, train/epoch: 6.042360782623291
Step: 25400, train/loss: 0.32749998569488525
Step: 25400, train/grad_norm: 26.368560791015625
Step: 25400, train/learning_rate: 1.9776296539930627e-05
Step: 25400, train/epoch: 6.044740676879883
Step: 25410, train/loss: 0.3472000062465668
Step: 25410, train/grad_norm: 19.279869079589844
Step: 25410, train/learning_rate: 1.9764398530242033e-05
Step: 25410, train/epoch: 6.047120571136475
Step: 25420, train/loss: 0.3043999969959259
Step: 25420, train/grad_norm: 4.908435344696045
Step: 25420, train/learning_rate: 1.9752498701564036e-05
Step: 25420, train/epoch: 6.049500465393066
Step: 25430, train/loss: 0.3465000092983246
Step: 25430, train/grad_norm: 19.078691482543945
Step: 25430, train/learning_rate: 1.974059887288604e-05
Step: 25430, train/epoch: 6.0518798828125
Step: 25440, train/loss: 0.28790000081062317
Step: 25440, train/grad_norm: 6.337368011474609
Step: 25440, train/learning_rate: 1.9728700863197446e-05
Step: 25440, train/epoch: 6.054259777069092
Step: 25450, train/loss: 0.4343999922275543
Step: 25450, train/grad_norm: 15.89859676361084
Step: 25450, train/learning_rate: 1.971680103451945e-05
Step: 25450, train/epoch: 6.056639671325684
Step: 25460, train/loss: 0.2669999897480011
Step: 25460, train/grad_norm: 22.522157669067383
Step: 25460, train/learning_rate: 1.9704903024830855e-05
Step: 25460, train/epoch: 6.059019565582275
Step: 25470, train/loss: 0.2222999930381775
Step: 25470, train/grad_norm: 3.2902281284332275
Step: 25470, train/learning_rate: 1.969300319615286e-05
Step: 25470, train/epoch: 6.061399459838867
Step: 25480, train/loss: 0.26919999718666077
Step: 25480, train/grad_norm: 10.297590255737305
Step: 25480, train/learning_rate: 1.968110336747486e-05
Step: 25480, train/epoch: 6.063779354095459
Step: 25490, train/loss: 0.4334000051021576
Step: 25490, train/grad_norm: 30.08468246459961
Step: 25490, train/learning_rate: 1.9669205357786268e-05
Step: 25490, train/epoch: 6.066158771514893
Step: 25500, train/loss: 0.2667999863624573
Step: 25500, train/grad_norm: 5.786036014556885
Step: 25500, train/learning_rate: 1.965730552910827e-05
Step: 25500, train/epoch: 6.068538665771484
Step: 25510, train/loss: 0.4332999885082245
Step: 25510, train/grad_norm: 8.168318748474121
Step: 25510, train/learning_rate: 1.9645407519419678e-05
Step: 25510, train/epoch: 6.070918560028076
Step: 25520, train/loss: 0.3244999945163727
Step: 25520, train/grad_norm: 13.869287490844727
Step: 25520, train/learning_rate: 1.963350769074168e-05
Step: 25520, train/epoch: 6.073298454284668
Step: 25530, train/loss: 0.2565000057220459
Step: 25530, train/grad_norm: 12.82384967803955
Step: 25530, train/learning_rate: 1.9621607862063684e-05
Step: 25530, train/epoch: 6.07567834854126
Step: 25540, train/loss: 0.40209999680519104
Step: 25540, train/grad_norm: 13.737966537475586
Step: 25540, train/learning_rate: 1.960970985237509e-05
Step: 25540, train/epoch: 6.078058242797852
Step: 25550, train/loss: 0.475600004196167
Step: 25550, train/grad_norm: 8.721328735351562
Step: 25550, train/learning_rate: 1.9597810023697093e-05
Step: 25550, train/epoch: 6.080437660217285
Step: 25560, train/loss: 0.39480000734329224
Step: 25560, train/grad_norm: 17.391820907592773
Step: 25560, train/learning_rate: 1.95859120140085e-05
Step: 25560, train/epoch: 6.082817554473877
Step: 25570, train/loss: 0.48100000619888306
Step: 25570, train/grad_norm: 20.743988037109375
Step: 25570, train/learning_rate: 1.9574012185330503e-05
Step: 25570, train/epoch: 6.085197448730469
Step: 25580, train/loss: 0.34380000829696655
Step: 25580, train/grad_norm: 12.740476608276367
Step: 25580, train/learning_rate: 1.956211417564191e-05
Step: 25580, train/epoch: 6.0875773429870605
Step: 25590, train/loss: 0.30630001425743103
Step: 25590, train/grad_norm: 6.837438106536865
Step: 25590, train/learning_rate: 1.9550214346963912e-05
Step: 25590, train/epoch: 6.089957237243652
Step: 25600, train/loss: 0.4000000059604645
Step: 25600, train/grad_norm: 4.090915203094482
Step: 25600, train/learning_rate: 1.9538314518285915e-05
Step: 25600, train/epoch: 6.092337131500244
Step: 25610, train/loss: 0.4311000108718872
Step: 25610, train/grad_norm: 10.918171882629395
Step: 25610, train/learning_rate: 1.9526416508597322e-05
Step: 25610, train/epoch: 6.094717025756836
Step: 25620, train/loss: 0.3767000138759613
Step: 25620, train/grad_norm: 22.66460609436035
Step: 25620, train/learning_rate: 1.9514516679919325e-05
Step: 25620, train/epoch: 6.0970964431762695
Step: 25630, train/loss: 0.4724000096321106
Step: 25630, train/grad_norm: 9.217913627624512
Step: 25630, train/learning_rate: 1.950261867023073e-05
Step: 25630, train/epoch: 6.099476337432861
Step: 25640, train/loss: 0.3181000053882599
Step: 25640, train/grad_norm: 20.791399002075195
Step: 25640, train/learning_rate: 1.9490718841552734e-05
Step: 25640, train/epoch: 6.101856231689453
Step: 25650, train/loss: 0.4122999906539917
Step: 25650, train/grad_norm: 7.412288665771484
Step: 25650, train/learning_rate: 1.9478819012874737e-05
Step: 25650, train/epoch: 6.104236125946045
Step: 25660, train/loss: 0.39239999651908875
Step: 25660, train/grad_norm: 15.415850639343262
Step: 25660, train/learning_rate: 1.9466921003186144e-05
Step: 25660, train/epoch: 6.106616020202637
Step: 25670, train/loss: 0.3531999886035919
Step: 25670, train/grad_norm: 6.8058180809021
Step: 25670, train/learning_rate: 1.9455021174508147e-05
Step: 25670, train/epoch: 6.1089959144592285
Step: 25680, train/loss: 0.4108999967575073
Step: 25680, train/grad_norm: 10.711791038513184
Step: 25680, train/learning_rate: 1.9443123164819553e-05
Step: 25680, train/epoch: 6.111375331878662
Step: 25690, train/loss: 0.26170000433921814
Step: 25690, train/grad_norm: 12.495027542114258
Step: 25690, train/learning_rate: 1.9431223336141557e-05
Step: 25690, train/epoch: 6.113755226135254
Step: 25700, train/loss: 0.3653999865055084
Step: 25700, train/grad_norm: 12.563724517822266
Step: 25700, train/learning_rate: 1.941932350746356e-05
Step: 25700, train/epoch: 6.116135120391846
Step: 25710, train/loss: 0.39570000767707825
Step: 25710, train/grad_norm: 9.51134204864502
Step: 25710, train/learning_rate: 1.9407425497774966e-05
Step: 25710, train/epoch: 6.1185150146484375
Step: 25720, train/loss: 0.3783000111579895
Step: 25720, train/grad_norm: 16.683208465576172
Step: 25720, train/learning_rate: 1.939552566909697e-05
Step: 25720, train/epoch: 6.120894908905029
Step: 25730, train/loss: 0.4519999921321869
Step: 25730, train/grad_norm: 17.782569885253906
Step: 25730, train/learning_rate: 1.9383627659408376e-05
Step: 25730, train/epoch: 6.123274803161621
Step: 25740, train/loss: 0.265500009059906
Step: 25740, train/grad_norm: 20.38357925415039
Step: 25740, train/learning_rate: 1.937172783073038e-05
Step: 25740, train/epoch: 6.125654220581055
Step: 25750, train/loss: 0.32330000400543213
Step: 25750, train/grad_norm: 17.493053436279297
Step: 25750, train/learning_rate: 1.935982800205238e-05
Step: 25750, train/epoch: 6.1280341148376465
Step: 25760, train/loss: 0.3555999994277954
Step: 25760, train/grad_norm: 5.552109241485596
Step: 25760, train/learning_rate: 1.9347929992363788e-05
Step: 25760, train/epoch: 6.130414009094238
Step: 25770, train/loss: 0.426800012588501
Step: 25770, train/grad_norm: 11.777397155761719
Step: 25770, train/learning_rate: 1.933603016368579e-05
Step: 25770, train/epoch: 6.13279390335083
Step: 25780, train/loss: 0.42170000076293945
Step: 25780, train/grad_norm: 10.516322135925293
Step: 25780, train/learning_rate: 1.9324132153997198e-05
Step: 25780, train/epoch: 6.135173797607422
Step: 25790, train/loss: 0.27469998598098755
Step: 25790, train/grad_norm: 10.7827787399292
Step: 25790, train/learning_rate: 1.93122323253192e-05
Step: 25790, train/epoch: 6.137553691864014
Step: 25800, train/loss: 0.31850001215934753
Step: 25800, train/grad_norm: 7.045419692993164
Step: 25800, train/learning_rate: 1.9300332496641204e-05
Step: 25800, train/epoch: 6.1399335861206055
Step: 25810, train/loss: 0.35989999771118164
Step: 25810, train/grad_norm: 13.212770462036133
Step: 25810, train/learning_rate: 1.928843448695261e-05
Step: 25810, train/epoch: 6.142313003540039
Step: 25820, train/loss: 0.45500001311302185
Step: 25820, train/grad_norm: 9.981598854064941
Step: 25820, train/learning_rate: 1.9276534658274613e-05
Step: 25820, train/epoch: 6.144692897796631
Step: 25830, train/loss: 0.42329999804496765
Step: 25830, train/grad_norm: 24.659343719482422
Step: 25830, train/learning_rate: 1.926463664858602e-05
Step: 25830, train/epoch: 6.147072792053223
Step: 25840, train/loss: 0.3458999991416931
Step: 25840, train/grad_norm: 17.15353012084961
Step: 25840, train/learning_rate: 1.9252736819908023e-05
Step: 25840, train/epoch: 6.1494526863098145
Step: 25850, train/loss: 0.4000000059604645
Step: 25850, train/grad_norm: 28.255603790283203
Step: 25850, train/learning_rate: 1.9240836991230026e-05
Step: 25850, train/epoch: 6.151832580566406
Step: 25860, train/loss: 0.3743000030517578
Step: 25860, train/grad_norm: 8.984490394592285
Step: 25860, train/learning_rate: 1.9228938981541432e-05
Step: 25860, train/epoch: 6.154212474822998
Step: 25870, train/loss: 0.38119998574256897
Step: 25870, train/grad_norm: 20.74433135986328
Step: 25870, train/learning_rate: 1.9217039152863435e-05
Step: 25870, train/epoch: 6.156591892242432
Step: 25880, train/loss: 0.367000013589859
Step: 25880, train/grad_norm: 9.2083740234375
Step: 25880, train/learning_rate: 1.9205141143174842e-05
Step: 25880, train/epoch: 6.158971786499023
Step: 25890, train/loss: 0.287200003862381
Step: 25890, train/grad_norm: 7.180756568908691
Step: 25890, train/learning_rate: 1.9193241314496845e-05
Step: 25890, train/epoch: 6.161351680755615
Step: 25900, train/loss: 0.26809999346733093
Step: 25900, train/grad_norm: 8.614127159118652
Step: 25900, train/learning_rate: 1.9181341485818848e-05
Step: 25900, train/epoch: 6.163731575012207
Step: 25910, train/loss: 0.53329998254776
Step: 25910, train/grad_norm: 12.047015190124512
Step: 25910, train/learning_rate: 1.9169443476130255e-05
Step: 25910, train/epoch: 6.166111469268799
Step: 25920, train/loss: 0.4397999942302704
Step: 25920, train/grad_norm: 16.32302474975586
Step: 25920, train/learning_rate: 1.9157543647452258e-05
Step: 25920, train/epoch: 6.168491363525391
Step: 25930, train/loss: 0.574400007724762
Step: 25930, train/grad_norm: 13.31112289428711
Step: 25930, train/learning_rate: 1.9145645637763664e-05
Step: 25930, train/epoch: 6.170870780944824
Step: 25940, train/loss: 0.37929999828338623
Step: 25940, train/grad_norm: 8.814040184020996
Step: 25940, train/learning_rate: 1.9133745809085667e-05
Step: 25940, train/epoch: 6.173250675201416
Step: 25950, train/loss: 0.438400000333786
Step: 25950, train/grad_norm: 16.924081802368164
Step: 25950, train/learning_rate: 1.912184598040767e-05
Step: 25950, train/epoch: 6.175630569458008
Step: 25960, train/loss: 0.3082999885082245
Step: 25960, train/grad_norm: 16.91835594177246
Step: 25960, train/learning_rate: 1.9109947970719077e-05
Step: 25960, train/epoch: 6.1780104637146
Step: 25970, train/loss: 0.34790000319480896
Step: 25970, train/grad_norm: 5.1037516593933105
Step: 25970, train/learning_rate: 1.909804814204108e-05
Step: 25970, train/epoch: 6.180390357971191
Step: 25980, train/loss: 0.41359999775886536
Step: 25980, train/grad_norm: 12.825032234191895
Step: 25980, train/learning_rate: 1.9086150132352486e-05
Step: 25980, train/epoch: 6.182770252227783
Step: 25990, train/loss: 0.2273000031709671
Step: 25990, train/grad_norm: 10.795171737670898
Step: 25990, train/learning_rate: 1.907425030367449e-05
Step: 25990, train/epoch: 6.185150146484375
Step: 26000, train/loss: 0.38109999895095825
Step: 26000, train/grad_norm: 6.527626991271973
Step: 26000, train/learning_rate: 1.9062350474996492e-05
Step: 26000, train/epoch: 6.187529563903809
Step: 26010, train/loss: 0.4036000072956085
Step: 26010, train/grad_norm: 13.707348823547363
Step: 26010, train/learning_rate: 1.90504524653079e-05
Step: 26010, train/epoch: 6.1899094581604
Step: 26020, train/loss: 0.3400000035762787
Step: 26020, train/grad_norm: 11.8518705368042
Step: 26020, train/learning_rate: 1.9038552636629902e-05
Step: 26020, train/epoch: 6.192289352416992
Step: 26030, train/loss: 0.33799999952316284
Step: 26030, train/grad_norm: 8.243377685546875
Step: 26030, train/learning_rate: 1.902665462694131e-05
Step: 26030, train/epoch: 6.194669246673584
Step: 26040, train/loss: 0.299699991941452
Step: 26040, train/grad_norm: 5.954588890075684
Step: 26040, train/learning_rate: 1.901475479826331e-05
Step: 26040, train/epoch: 6.197049140930176
Step: 26050, train/loss: 0.5483999848365784
Step: 26050, train/grad_norm: 11.769296646118164
Step: 26050, train/learning_rate: 1.9002854969585314e-05
Step: 26050, train/epoch: 6.199429035186768
Step: 26060, train/loss: 0.31200000643730164
Step: 26060, train/grad_norm: 12.779292106628418
Step: 26060, train/learning_rate: 1.899095695989672e-05
Step: 26060, train/epoch: 6.201808452606201
Step: 26070, train/loss: 0.46219998598098755
Step: 26070, train/grad_norm: 11.477007865905762
Step: 26070, train/learning_rate: 1.8979057131218724e-05
Step: 26070, train/epoch: 6.204188346862793
Step: 26080, train/loss: 0.3625999987125397
Step: 26080, train/grad_norm: 28.38394546508789
Step: 26080, train/learning_rate: 1.896715912153013e-05
Step: 26080, train/epoch: 6.206568241119385
Step: 26090, train/loss: 0.3937000036239624
Step: 26090, train/grad_norm: 21.629323959350586
Step: 26090, train/learning_rate: 1.8955259292852134e-05
Step: 26090, train/epoch: 6.208948135375977
Step: 26100, train/loss: 0.2442999929189682
Step: 26100, train/grad_norm: 10.888863563537598
Step: 26100, train/learning_rate: 1.8943359464174137e-05
Step: 26100, train/epoch: 6.211328029632568
Step: 26110, train/loss: 0.4487000107765198
Step: 26110, train/grad_norm: 12.215173721313477
Step: 26110, train/learning_rate: 1.8931461454485543e-05
Step: 26110, train/epoch: 6.21370792388916
Step: 26120, train/loss: 0.30730000138282776
Step: 26120, train/grad_norm: 11.315271377563477
Step: 26120, train/learning_rate: 1.8919561625807546e-05
Step: 26120, train/epoch: 6.216087341308594
Step: 26130, train/loss: 0.24070000648498535
Step: 26130, train/grad_norm: 15.277345657348633
Step: 26130, train/learning_rate: 1.8907663616118953e-05
Step: 26130, train/epoch: 6.2184672355651855
Step: 26140, train/loss: 0.21400000154972076
Step: 26140, train/grad_norm: 1.6633903980255127
Step: 26140, train/learning_rate: 1.8895763787440956e-05
Step: 26140, train/epoch: 6.220847129821777
Step: 26150, train/loss: 0.36880001425743103
Step: 26150, train/grad_norm: 21.524526596069336
Step: 26150, train/learning_rate: 1.888386395876296e-05
Step: 26150, train/epoch: 6.223227024078369
Step: 26160, train/loss: 0.3434999883174896
Step: 26160, train/grad_norm: 5.103159427642822
Step: 26160, train/learning_rate: 1.8871965949074365e-05
Step: 26160, train/epoch: 6.225606918334961
Step: 26170, train/loss: 0.3734999895095825
Step: 26170, train/grad_norm: 14.206535339355469
Step: 26170, train/learning_rate: 1.8860066120396368e-05
Step: 26170, train/epoch: 6.227986812591553
Step: 26180, train/loss: 0.3937000036239624
Step: 26180, train/grad_norm: 10.615795135498047
Step: 26180, train/learning_rate: 1.8848168110707775e-05
Step: 26180, train/epoch: 6.2303667068481445
Step: 26190, train/loss: 0.5156999826431274
Step: 26190, train/grad_norm: 6.116442680358887
Step: 26190, train/learning_rate: 1.8836268282029778e-05
Step: 26190, train/epoch: 6.232746124267578
Step: 26200, train/loss: 0.3086000084877014
Step: 26200, train/grad_norm: 26.85841178894043
Step: 26200, train/learning_rate: 1.882436845335178e-05
Step: 26200, train/epoch: 6.23512601852417
Step: 26210, train/loss: 0.2831000089645386
Step: 26210, train/grad_norm: 7.753950119018555
Step: 26210, train/learning_rate: 1.8812470443663187e-05
Step: 26210, train/epoch: 6.237505912780762
Step: 26220, train/loss: 0.42410001158714294
Step: 26220, train/grad_norm: 23.642377853393555
Step: 26220, train/learning_rate: 1.880057061498519e-05
Step: 26220, train/epoch: 6.2398858070373535
Step: 26230, train/loss: 0.3384000062942505
Step: 26230, train/grad_norm: 19.293699264526367
Step: 26230, train/learning_rate: 1.8788672605296597e-05
Step: 26230, train/epoch: 6.242265701293945
Step: 26240, train/loss: 0.2517000138759613
Step: 26240, train/grad_norm: 8.37233829498291
Step: 26240, train/learning_rate: 1.87767727766186e-05
Step: 26240, train/epoch: 6.244645595550537
Step: 26250, train/loss: 0.4943999946117401
Step: 26250, train/grad_norm: 14.35952377319336
Step: 26250, train/learning_rate: 1.8764874766930006e-05
Step: 26250, train/epoch: 6.247025012969971
Step: 26260, train/loss: 0.5291000008583069
Step: 26260, train/grad_norm: 16.130233764648438
Step: 26260, train/learning_rate: 1.875297493825201e-05
Step: 26260, train/epoch: 6.2494049072265625
Step: 26270, train/loss: 0.4278999865055084
Step: 26270, train/grad_norm: 10.874776840209961
Step: 26270, train/learning_rate: 1.8741075109574012e-05
Step: 26270, train/epoch: 6.251784801483154
Step: 26280, train/loss: 0.4302000105381012
Step: 26280, train/grad_norm: 20.17239761352539
Step: 26280, train/learning_rate: 1.872917709988542e-05
Step: 26280, train/epoch: 6.254164695739746
Step: 26290, train/loss: 0.2930000126361847
Step: 26290, train/grad_norm: 5.207455158233643
Step: 26290, train/learning_rate: 1.8717277271207422e-05
Step: 26290, train/epoch: 6.256544589996338
Step: 26300, train/loss: 0.38530001044273376
Step: 26300, train/grad_norm: 5.208556175231934
Step: 26300, train/learning_rate: 1.870537926151883e-05
Step: 26300, train/epoch: 6.25892448425293
Step: 26310, train/loss: 0.2493000030517578
Step: 26310, train/grad_norm: 12.983841896057129
Step: 26310, train/learning_rate: 1.869347943284083e-05
Step: 26310, train/epoch: 6.2613043785095215
Step: 26320, train/loss: 0.5827000141143799
Step: 26320, train/grad_norm: 7.381613254547119
Step: 26320, train/learning_rate: 1.8681579604162835e-05
Step: 26320, train/epoch: 6.263683795928955
Step: 26330, train/loss: 0.2838999927043915
Step: 26330, train/grad_norm: 9.045263290405273
Step: 26330, train/learning_rate: 1.866968159447424e-05
Step: 26330, train/epoch: 6.266063690185547
Step: 26340, train/loss: 0.2754000127315521
Step: 26340, train/grad_norm: 41.34539031982422
Step: 26340, train/learning_rate: 1.8657781765796244e-05
Step: 26340, train/epoch: 6.268443584442139
Step: 26350, train/loss: 0.37070000171661377
Step: 26350, train/grad_norm: 15.120253562927246
Step: 26350, train/learning_rate: 1.864588375610765e-05
Step: 26350, train/epoch: 6.2708234786987305
Step: 26360, train/loss: 0.4018000066280365
Step: 26360, train/grad_norm: 21.02663803100586
Step: 26360, train/learning_rate: 1.8633983927429654e-05
Step: 26360, train/epoch: 6.273203372955322
Step: 26370, train/loss: 0.21629999577999115
Step: 26370, train/grad_norm: 9.765478134155273
Step: 26370, train/learning_rate: 1.8622084098751657e-05
Step: 26370, train/epoch: 6.275583267211914
Step: 26380, train/loss: 0.421099990606308
Step: 26380, train/grad_norm: 10.200284957885742
Step: 26380, train/learning_rate: 1.8610186089063063e-05
Step: 26380, train/epoch: 6.277962684631348
Step: 26390, train/loss: 0.5151000022888184
Step: 26390, train/grad_norm: 18.853015899658203
Step: 26390, train/learning_rate: 1.8598286260385066e-05
Step: 26390, train/epoch: 6.2803425788879395
Step: 26400, train/loss: 0.4032999873161316
Step: 26400, train/grad_norm: 10.51518726348877
Step: 26400, train/learning_rate: 1.8586388250696473e-05
Step: 26400, train/epoch: 6.282722473144531
Step: 26410, train/loss: 0.32100000977516174
Step: 26410, train/grad_norm: 8.876590728759766
Step: 26410, train/learning_rate: 1.8574488422018476e-05
Step: 26410, train/epoch: 6.285102367401123
Step: 26420, train/loss: 0.35830000042915344
Step: 26420, train/grad_norm: 15.274206161499023
Step: 26420, train/learning_rate: 1.856258859334048e-05
Step: 26420, train/epoch: 6.287482261657715
Step: 26430, train/loss: 0.31029999256134033
Step: 26430, train/grad_norm: 11.354479789733887
Step: 26430, train/learning_rate: 1.8550690583651885e-05
Step: 26430, train/epoch: 6.289862155914307
Step: 26440, train/loss: 0.3073999881744385
Step: 26440, train/grad_norm: 8.765810012817383
Step: 26440, train/learning_rate: 1.853879075497389e-05
Step: 26440, train/epoch: 6.29224157333374
Step: 26450, train/loss: 0.2515000104904175
Step: 26450, train/grad_norm: 8.969430923461914
Step: 26450, train/learning_rate: 1.8526892745285295e-05
Step: 26450, train/epoch: 6.294621467590332
Step: 26460, train/loss: 0.43630000948905945
Step: 26460, train/grad_norm: 15.842391967773438
Step: 26460, train/learning_rate: 1.8514992916607298e-05
Step: 26460, train/epoch: 6.297001361846924
Step: 26470, train/loss: 0.27720001339912415
Step: 26470, train/grad_norm: 7.191136837005615
Step: 26470, train/learning_rate: 1.85030930879293e-05
Step: 26470, train/epoch: 6.299381256103516
Step: 26480, train/loss: 0.4198000133037567
Step: 26480, train/grad_norm: 17.68180274963379
Step: 26480, train/learning_rate: 1.8491195078240708e-05
Step: 26480, train/epoch: 6.301761150360107
Step: 26490, train/loss: 0.32269999384880066
Step: 26490, train/grad_norm: 6.972557067871094
Step: 26490, train/learning_rate: 1.847929524956271e-05
Step: 26490, train/epoch: 6.304141044616699
Step: 26500, train/loss: 0.26989999413490295
Step: 26500, train/grad_norm: 7.972312927246094
Step: 26500, train/learning_rate: 1.8467397239874117e-05
Step: 26500, train/epoch: 6.306520938873291
Step: 26510, train/loss: 0.429500013589859
Step: 26510, train/grad_norm: 10.847813606262207
Step: 26510, train/learning_rate: 1.845549741119612e-05
Step: 26510, train/epoch: 6.308900356292725
Step: 26520, train/loss: 0.45100000500679016
Step: 26520, train/grad_norm: 8.161205291748047
Step: 26520, train/learning_rate: 1.8443597582518123e-05
Step: 26520, train/epoch: 6.311280250549316
Step: 26530, train/loss: 0.35580000281333923
Step: 26530, train/grad_norm: 4.790441989898682
Step: 26530, train/learning_rate: 1.843169957282953e-05
Step: 26530, train/epoch: 6.313660144805908
Step: 26540, train/loss: 0.39500001072883606
Step: 26540, train/grad_norm: 15.09659194946289
Step: 26540, train/learning_rate: 1.8419799744151533e-05
Step: 26540, train/epoch: 6.3160400390625
Step: 26550, train/loss: 0.2662000060081482
Step: 26550, train/grad_norm: 14.957791328430176
Step: 26550, train/learning_rate: 1.840790173446294e-05
Step: 26550, train/epoch: 6.318419933319092
Step: 26560, train/loss: 0.413100004196167
Step: 26560, train/grad_norm: 15.447824478149414
Step: 26560, train/learning_rate: 1.8396001905784942e-05
Step: 26560, train/epoch: 6.320799827575684
Step: 26570, train/loss: 0.44620001316070557
Step: 26570, train/grad_norm: 22.16036033630371
Step: 26570, train/learning_rate: 1.8384102077106945e-05
Step: 26570, train/epoch: 6.323179244995117
Step: 26580, train/loss: 0.37540000677108765
Step: 26580, train/grad_norm: 13.191349983215332
Step: 26580, train/learning_rate: 1.8372204067418352e-05
Step: 26580, train/epoch: 6.325559139251709
Step: 26590, train/loss: 0.304500013589859
Step: 26590, train/grad_norm: 11.55558967590332
Step: 26590, train/learning_rate: 1.8360304238740355e-05
Step: 26590, train/epoch: 6.327939033508301
Step: 26600, train/loss: 0.37529999017715454
Step: 26600, train/grad_norm: 17.950700759887695
Step: 26600, train/learning_rate: 1.834840622905176e-05
Step: 26600, train/epoch: 6.330318927764893
Step: 26610, train/loss: 0.29350000619888306
Step: 26610, train/grad_norm: 13.884440422058105
Step: 26610, train/learning_rate: 1.8336506400373764e-05
Step: 26610, train/epoch: 6.332698822021484
Step: 26620, train/loss: 0.29420000314712524
Step: 26620, train/grad_norm: 5.3758955001831055
Step: 26620, train/learning_rate: 1.8324606571695767e-05
Step: 26620, train/epoch: 6.335078716278076
Step: 26630, train/loss: 0.42410001158714294
Step: 26630, train/grad_norm: 8.478229522705078
Step: 26630, train/learning_rate: 1.8312708562007174e-05
Step: 26630, train/epoch: 6.33745813369751
Step: 26640, train/loss: 0.3831999897956848
Step: 26640, train/grad_norm: 6.50343656539917
Step: 26640, train/learning_rate: 1.8300808733329177e-05
Step: 26640, train/epoch: 6.339838027954102
Step: 26650, train/loss: 0.3188999891281128
Step: 26650, train/grad_norm: 7.208399772644043
Step: 26650, train/learning_rate: 1.8288910723640583e-05
Step: 26650, train/epoch: 6.342217922210693
Step: 26660, train/loss: 0.35920000076293945
Step: 26660, train/grad_norm: 26.606176376342773
Step: 26660, train/learning_rate: 1.8277010894962586e-05
Step: 26660, train/epoch: 6.344597816467285
Step: 26670, train/loss: 0.2110999971628189
Step: 26670, train/grad_norm: 6.877915859222412
Step: 26670, train/learning_rate: 1.826511106628459e-05
Step: 26670, train/epoch: 6.346977710723877
Step: 26680, train/loss: 0.34380000829696655
Step: 26680, train/grad_norm: 11.67466926574707
Step: 26680, train/learning_rate: 1.8253213056595996e-05
Step: 26680, train/epoch: 6.349357604980469
Step: 26690, train/loss: 0.3783000111579895
Step: 26690, train/grad_norm: 1.6675407886505127
Step: 26690, train/learning_rate: 1.8241313227918e-05
Step: 26690, train/epoch: 6.3517374992370605
Step: 26700, train/loss: 0.30149999260902405
Step: 26700, train/grad_norm: 4.395873069763184
Step: 26700, train/learning_rate: 1.8229415218229406e-05
Step: 26700, train/epoch: 6.354116916656494
Step: 26710, train/loss: 0.6467999815940857
Step: 26710, train/grad_norm: 13.805730819702148
Step: 26710, train/learning_rate: 1.821751538955141e-05
Step: 26710, train/epoch: 6.356496810913086
Step: 26720, train/loss: 0.30489999055862427
Step: 26720, train/grad_norm: 18.080284118652344
Step: 26720, train/learning_rate: 1.820561556087341e-05
Step: 26720, train/epoch: 6.358876705169678
Step: 26730, train/loss: 0.14409999549388885
Step: 26730, train/grad_norm: 3.3815431594848633
Step: 26730, train/learning_rate: 1.8193717551184818e-05
Step: 26730, train/epoch: 6.3612565994262695
Step: 26740, train/loss: 0.4115000069141388
Step: 26740, train/grad_norm: 11.375862121582031
Step: 26740, train/learning_rate: 1.818181772250682e-05
Step: 26740, train/epoch: 6.363636493682861
Step: 26750, train/loss: 0.3328999876976013
Step: 26750, train/grad_norm: 24.3397274017334
Step: 26750, train/learning_rate: 1.8169919712818228e-05
Step: 26750, train/epoch: 6.366016387939453
Step: 26760, train/loss: 0.3124000132083893
Step: 26760, train/grad_norm: 35.210975646972656
Step: 26760, train/learning_rate: 1.815801988414023e-05
Step: 26760, train/epoch: 6.368395805358887
Step: 26770, train/loss: 0.4237000048160553
Step: 26770, train/grad_norm: 8.419588088989258
Step: 26770, train/learning_rate: 1.8146120055462234e-05
Step: 26770, train/epoch: 6.3707756996154785
Step: 26780, train/loss: 0.5641999840736389
Step: 26780, train/grad_norm: 11.749271392822266
Step: 26780, train/learning_rate: 1.813422204577364e-05
Step: 26780, train/epoch: 6.37315559387207
Step: 26790, train/loss: 0.3659000098705292
Step: 26790, train/grad_norm: 11.181092262268066
Step: 26790, train/learning_rate: 1.8122322217095643e-05
Step: 26790, train/epoch: 6.375535488128662
Step: 26800, train/loss: 0.2906000018119812
Step: 26800, train/grad_norm: 13.861617088317871
Step: 26800, train/learning_rate: 1.811042420740705e-05
Step: 26800, train/epoch: 6.377915382385254
Step: 26810, train/loss: 0.39010000228881836
Step: 26810, train/grad_norm: 13.700815200805664
Step: 26810, train/learning_rate: 1.8098524378729053e-05
Step: 26810, train/epoch: 6.380295276641846
Step: 26820, train/loss: 0.2685000002384186
Step: 26820, train/grad_norm: 15.385993003845215
Step: 26820, train/learning_rate: 1.8086624550051056e-05
Step: 26820, train/epoch: 6.382674694061279
Step: 26830, train/loss: 0.35839998722076416
Step: 26830, train/grad_norm: 8.753273963928223
Step: 26830, train/learning_rate: 1.8074726540362462e-05
Step: 26830, train/epoch: 6.385054588317871
Step: 26840, train/loss: 0.5774999856948853
Step: 26840, train/grad_norm: 6.510833740234375
Step: 26840, train/learning_rate: 1.8062826711684465e-05
Step: 26840, train/epoch: 6.387434482574463
Step: 26850, train/loss: 0.226500004529953
Step: 26850, train/grad_norm: 16.394145965576172
Step: 26850, train/learning_rate: 1.8050928701995872e-05
Step: 26850, train/epoch: 6.389814376831055
Step: 26860, train/loss: 0.22910000383853912
Step: 26860, train/grad_norm: 3.3085315227508545
Step: 26860, train/learning_rate: 1.8039028873317875e-05
Step: 26860, train/epoch: 6.3921942710876465
Step: 26870, train/loss: 0.34290000796318054
Step: 26870, train/grad_norm: 4.848410606384277
Step: 26870, train/learning_rate: 1.8027129044639878e-05
Step: 26870, train/epoch: 6.394574165344238
Step: 26880, train/loss: 0.32170000672340393
Step: 26880, train/grad_norm: 13.179163932800293
Step: 26880, train/learning_rate: 1.8015231034951285e-05
Step: 26880, train/epoch: 6.39695405960083
Step: 26890, train/loss: 0.38989999890327454
Step: 26890, train/grad_norm: 16.514245986938477
Step: 26890, train/learning_rate: 1.8003331206273288e-05
Step: 26890, train/epoch: 6.399333477020264
Step: 26900, train/loss: 0.4203000068664551
Step: 26900, train/grad_norm: 5.901226997375488
Step: 26900, train/learning_rate: 1.7991433196584694e-05
Step: 26900, train/epoch: 6.4017133712768555
Step: 26910, train/loss: 0.37950000166893005
Step: 26910, train/grad_norm: 5.785221099853516
Step: 26910, train/learning_rate: 1.7979533367906697e-05
Step: 26910, train/epoch: 6.404093265533447
Step: 26920, train/loss: 0.3776000142097473
Step: 26920, train/grad_norm: 3.480191946029663
Step: 26920, train/learning_rate: 1.7967635358218104e-05
Step: 26920, train/epoch: 6.406473159790039
Step: 26930, train/loss: 0.41990000009536743
Step: 26930, train/grad_norm: 19.807579040527344
Step: 26930, train/learning_rate: 1.7955735529540107e-05
Step: 26930, train/epoch: 6.408853054046631
Step: 26940, train/loss: 0.31690001487731934
Step: 26940, train/grad_norm: 17.000713348388672
Step: 26940, train/learning_rate: 1.794383570086211e-05
Step: 26940, train/epoch: 6.411232948303223
Step: 26950, train/loss: 0.5001000165939331
Step: 26950, train/grad_norm: 3.275986909866333
Step: 26950, train/learning_rate: 1.7931937691173516e-05
Step: 26950, train/epoch: 6.413612365722656
Step: 26960, train/loss: 0.391400009393692
Step: 26960, train/grad_norm: 4.007866859436035
Step: 26960, train/learning_rate: 1.792003786249552e-05
Step: 26960, train/epoch: 6.415992259979248
Step: 26970, train/loss: 0.3294999897480011
Step: 26970, train/grad_norm: 4.545302867889404
Step: 26970, train/learning_rate: 1.7908139852806926e-05
Step: 26970, train/epoch: 6.41837215423584
Step: 26980, train/loss: 0.3686999976634979
Step: 26980, train/grad_norm: 14.840861320495605
Step: 26980, train/learning_rate: 1.789624002412893e-05
Step: 26980, train/epoch: 6.420752048492432
Step: 26990, train/loss: 0.3749000132083893
Step: 26990, train/grad_norm: 20.796701431274414
Step: 26990, train/learning_rate: 1.7884340195450932e-05
Step: 26990, train/epoch: 6.423131942749023
Step: 27000, train/loss: 0.2824000120162964
Step: 27000, train/grad_norm: 16.0330810546875
Step: 27000, train/learning_rate: 1.787244218576234e-05
Step: 27000, train/epoch: 6.425511837005615
Step: 27010, train/loss: 0.1598999947309494
Step: 27010, train/grad_norm: 13.761655807495117
Step: 27010, train/learning_rate: 1.786054235708434e-05
Step: 27010, train/epoch: 6.427891254425049
Step: 27020, train/loss: 0.4528000056743622
Step: 27020, train/grad_norm: 28.518600463867188
Step: 27020, train/learning_rate: 1.7848644347395748e-05
Step: 27020, train/epoch: 6.430271148681641
Step: 27030, train/loss: 0.4936999976634979
Step: 27030, train/grad_norm: 14.373231887817383
Step: 27030, train/learning_rate: 1.783674451871775e-05
Step: 27030, train/epoch: 6.432651042938232
Step: 27040, train/loss: 0.4553000032901764
Step: 27040, train/grad_norm: 2.2419087886810303
Step: 27040, train/learning_rate: 1.7824844690039754e-05
Step: 27040, train/epoch: 6.435030937194824
Step: 27050, train/loss: 0.2741999924182892
Step: 27050, train/grad_norm: 28.915019989013672
Step: 27050, train/learning_rate: 1.781294668035116e-05
Step: 27050, train/epoch: 6.437410831451416
Step: 27060, train/loss: 0.33160001039505005
Step: 27060, train/grad_norm: 11.346585273742676
Step: 27060, train/learning_rate: 1.7801046851673163e-05
Step: 27060, train/epoch: 6.439790725708008
Step: 27070, train/loss: 0.43779999017715454
Step: 27070, train/grad_norm: 10.283395767211914
Step: 27070, train/learning_rate: 1.778914884198457e-05
Step: 27070, train/epoch: 6.4421706199646
Step: 27080, train/loss: 0.46619999408721924
Step: 27080, train/grad_norm: 14.795586585998535
Step: 27080, train/learning_rate: 1.7777249013306573e-05
Step: 27080, train/epoch: 6.444550037384033
Step: 27090, train/loss: 0.6814000010490417
Step: 27090, train/grad_norm: 9.301660537719727
Step: 27090, train/learning_rate: 1.7765349184628576e-05
Step: 27090, train/epoch: 6.446929931640625
Step: 27100, train/loss: 0.4991999864578247
Step: 27100, train/grad_norm: 12.406545639038086
Step: 27100, train/learning_rate: 1.7753451174939983e-05
Step: 27100, train/epoch: 6.449309825897217
Step: 27110, train/loss: 0.4041999876499176
Step: 27110, train/grad_norm: 10.726757049560547
Step: 27110, train/learning_rate: 1.7741551346261986e-05
Step: 27110, train/epoch: 6.451689720153809
Step: 27120, train/loss: 0.2535000145435333
Step: 27120, train/grad_norm: 3.4697439670562744
Step: 27120, train/learning_rate: 1.7729653336573392e-05
Step: 27120, train/epoch: 6.4540696144104
Step: 27130, train/loss: 0.4510999917984009
Step: 27130, train/grad_norm: 15.18323040008545
Step: 27130, train/learning_rate: 1.7717753507895395e-05
Step: 27130, train/epoch: 6.456449508666992
Step: 27140, train/loss: 0.26269999146461487
Step: 27140, train/grad_norm: 14.499674797058105
Step: 27140, train/learning_rate: 1.7705853679217398e-05
Step: 27140, train/epoch: 6.458828926086426
Step: 27150, train/loss: 0.3901999890804291
Step: 27150, train/grad_norm: 18.626976013183594
Step: 27150, train/learning_rate: 1.7693955669528805e-05
Step: 27150, train/epoch: 6.461208820343018
Step: 27160, train/loss: 0.33250001072883606
Step: 27160, train/grad_norm: 11.207032203674316
Step: 27160, train/learning_rate: 1.7682055840850808e-05
Step: 27160, train/epoch: 6.463588714599609
Step: 27170, train/loss: 0.2678000032901764
Step: 27170, train/grad_norm: 15.543228149414062
Step: 27170, train/learning_rate: 1.7670157831162214e-05
Step: 27170, train/epoch: 6.465968608856201
Step: 27180, train/loss: 0.4862000048160553
Step: 27180, train/grad_norm: 18.508174896240234
Step: 27180, train/learning_rate: 1.7658258002484217e-05
Step: 27180, train/epoch: 6.468348503112793
Step: 27190, train/loss: 0.3928000032901764
Step: 27190, train/grad_norm: 1.3502812385559082
Step: 27190, train/learning_rate: 1.764635817380622e-05
Step: 27190, train/epoch: 6.470728397369385
Step: 27200, train/loss: 0.29350000619888306
Step: 27200, train/grad_norm: 15.12408447265625
Step: 27200, train/learning_rate: 1.7634460164117627e-05
Step: 27200, train/epoch: 6.473107814788818
Step: 27210, train/loss: 0.413100004196167
Step: 27210, train/grad_norm: 7.689368724822998
Step: 27210, train/learning_rate: 1.762256033543963e-05
Step: 27210, train/epoch: 6.47548770904541
Step: 27220, train/loss: 0.44359999895095825
Step: 27220, train/grad_norm: 8.901875495910645
Step: 27220, train/learning_rate: 1.7610662325751036e-05
Step: 27220, train/epoch: 6.477867603302002
Step: 27230, train/loss: 0.41670000553131104
Step: 27230, train/grad_norm: 14.551424026489258
Step: 27230, train/learning_rate: 1.759876249707304e-05
Step: 27230, train/epoch: 6.480247497558594
Step: 27240, train/loss: 0.3901999890804291
Step: 27240, train/grad_norm: 16.508718490600586
Step: 27240, train/learning_rate: 1.7586862668395042e-05
Step: 27240, train/epoch: 6.4826273918151855
Step: 27250, train/loss: 0.2865000069141388
Step: 27250, train/grad_norm: 8.280059814453125
Step: 27250, train/learning_rate: 1.757496465870645e-05
Step: 27250, train/epoch: 6.485007286071777
Step: 27260, train/loss: 0.2581000030040741
Step: 27260, train/grad_norm: 4.237583637237549
Step: 27260, train/learning_rate: 1.7563064830028452e-05
Step: 27260, train/epoch: 6.487387180328369
Step: 27270, train/loss: 0.38449999690055847
Step: 27270, train/grad_norm: 17.22405242919922
Step: 27270, train/learning_rate: 1.755116682033986e-05
Step: 27270, train/epoch: 6.489766597747803
Step: 27280, train/loss: 0.2955999970436096
Step: 27280, train/grad_norm: 11.720069885253906
Step: 27280, train/learning_rate: 1.753926699166186e-05
Step: 27280, train/epoch: 6.4921464920043945
Step: 27290, train/loss: 0.4729999899864197
Step: 27290, train/grad_norm: 18.566301345825195
Step: 27290, train/learning_rate: 1.7527367162983865e-05
Step: 27290, train/epoch: 6.494526386260986
Step: 27300, train/loss: 0.17149999737739563
Step: 27300, train/grad_norm: 9.389422416687012
Step: 27300, train/learning_rate: 1.751546915329527e-05
Step: 27300, train/epoch: 6.496906280517578
Step: 27310, train/loss: 0.3919999897480011
Step: 27310, train/grad_norm: 3.924452781677246
Step: 27310, train/learning_rate: 1.7503569324617274e-05
Step: 27310, train/epoch: 6.49928617477417
Step: 27320, train/loss: 0.4668000042438507
Step: 27320, train/grad_norm: 27.91718864440918
Step: 27320, train/learning_rate: 1.749167131492868e-05
Step: 27320, train/epoch: 6.501666069030762
Step: 27330, train/loss: 0.39169999957084656
Step: 27330, train/grad_norm: 39.910072326660156
Step: 27330, train/learning_rate: 1.7479771486250684e-05
Step: 27330, train/epoch: 6.504045486450195
Step: 27340, train/loss: 0.39899998903274536
Step: 27340, train/grad_norm: 6.71270751953125
Step: 27340, train/learning_rate: 1.7467871657572687e-05
Step: 27340, train/epoch: 6.506425380706787
Step: 27350, train/loss: 0.3343999981880188
Step: 27350, train/grad_norm: 6.1320481300354
Step: 27350, train/learning_rate: 1.7455973647884093e-05
Step: 27350, train/epoch: 6.508805274963379
Step: 27360, train/loss: 0.3089999854564667
Step: 27360, train/grad_norm: 31.815715789794922
Step: 27360, train/learning_rate: 1.7444073819206096e-05
Step: 27360, train/epoch: 6.511185169219971
Step: 27370, train/loss: 0.39660000801086426
Step: 27370, train/grad_norm: 10.68378734588623
Step: 27370, train/learning_rate: 1.7432175809517503e-05
Step: 27370, train/epoch: 6.5135650634765625
Step: 27380, train/loss: 0.3305000066757202
Step: 27380, train/grad_norm: 10.562128067016602
Step: 27380, train/learning_rate: 1.7420275980839506e-05
Step: 27380, train/epoch: 6.515944957733154
Step: 27390, train/loss: 0.23389999568462372
Step: 27390, train/grad_norm: 13.045166015625
Step: 27390, train/learning_rate: 1.740837615216151e-05
Step: 27390, train/epoch: 6.518324375152588
Step: 27400, train/loss: 0.4065000116825104
Step: 27400, train/grad_norm: 19.018653869628906
Step: 27400, train/learning_rate: 1.7396478142472915e-05
Step: 27400, train/epoch: 6.52070426940918
Step: 27410, train/loss: 0.26969999074935913
Step: 27410, train/grad_norm: 6.916823387145996
Step: 27410, train/learning_rate: 1.738457831379492e-05
Step: 27410, train/epoch: 6.5230841636657715
Step: 27420, train/loss: 0.26010000705718994
Step: 27420, train/grad_norm: 12.034358978271484
Step: 27420, train/learning_rate: 1.7372680304106325e-05
Step: 27420, train/epoch: 6.525464057922363
Step: 27430, train/loss: 0.26759999990463257
Step: 27430, train/grad_norm: 23.32946014404297
Step: 27430, train/learning_rate: 1.7360780475428328e-05
Step: 27430, train/epoch: 6.527843952178955
Step: 27440, train/loss: 0.3589000105857849
Step: 27440, train/grad_norm: 7.798048496246338
Step: 27440, train/learning_rate: 1.734888064675033e-05
Step: 27440, train/epoch: 6.530223846435547
Step: 27450, train/loss: 0.3946000039577484
Step: 27450, train/grad_norm: 7.3150787353515625
Step: 27450, train/learning_rate: 1.7336982637061737e-05
Step: 27450, train/epoch: 6.532603740692139
Step: 27460, train/loss: 0.3237000107765198
Step: 27460, train/grad_norm: 8.586710929870605
Step: 27460, train/learning_rate: 1.732508280838374e-05
Step: 27460, train/epoch: 6.534983158111572
Step: 27470, train/loss: 0.38909998536109924
Step: 27470, train/grad_norm: 21.54045295715332
Step: 27470, train/learning_rate: 1.7313184798695147e-05
Step: 27470, train/epoch: 6.537363052368164
Step: 27480, train/loss: 0.42879998683929443
Step: 27480, train/grad_norm: 18.450481414794922
Step: 27480, train/learning_rate: 1.730128497001715e-05
Step: 27480, train/epoch: 6.539742946624756
Step: 27490, train/loss: 0.27950000762939453
Step: 27490, train/grad_norm: 7.319936275482178
Step: 27490, train/learning_rate: 1.7289385141339153e-05
Step: 27490, train/epoch: 6.542122840881348
Step: 27500, train/loss: 0.4083000123500824
Step: 27500, train/grad_norm: 18.526105880737305
Step: 27500, train/learning_rate: 1.727748713165056e-05
Step: 27500, train/epoch: 6.5445027351379395
Step: 27510, train/loss: 0.39250001311302185
Step: 27510, train/grad_norm: 10.899572372436523
Step: 27510, train/learning_rate: 1.7265587302972563e-05
Step: 27510, train/epoch: 6.546882629394531
Step: 27520, train/loss: 0.5432000160217285
Step: 27520, train/grad_norm: 35.0283088684082
Step: 27520, train/learning_rate: 1.725368929328397e-05
Step: 27520, train/epoch: 6.549262046813965
Step: 27530, train/loss: 0.3050000071525574
Step: 27530, train/grad_norm: 8.88621997833252
Step: 27530, train/learning_rate: 1.7241789464605972e-05
Step: 27530, train/epoch: 6.551641941070557
Step: 27540, train/loss: 0.2628999948501587
Step: 27540, train/grad_norm: 18.516571044921875
Step: 27540, train/learning_rate: 1.7229889635927975e-05
Step: 27540, train/epoch: 6.554021835327148
Step: 27550, train/loss: 0.49320000410079956
Step: 27550, train/grad_norm: 6.098008155822754
Step: 27550, train/learning_rate: 1.7217991626239382e-05
Step: 27550, train/epoch: 6.55640172958374
Step: 27560, train/loss: 0.4034999907016754
Step: 27560, train/grad_norm: 7.687303066253662
Step: 27560, train/learning_rate: 1.7206091797561385e-05
Step: 27560, train/epoch: 6.558781623840332
Step: 27570, train/loss: 0.359499990940094
Step: 27570, train/grad_norm: 9.211030960083008
Step: 27570, train/learning_rate: 1.719419378787279e-05
Step: 27570, train/epoch: 6.561161518096924
Step: 27580, train/loss: 0.4047999978065491
Step: 27580, train/grad_norm: 8.176566123962402
Step: 27580, train/learning_rate: 1.7182293959194794e-05
Step: 27580, train/epoch: 6.563540935516357
Step: 27590, train/loss: 0.2867000102996826
Step: 27590, train/grad_norm: 20.207326889038086
Step: 27590, train/learning_rate: 1.71703959495062e-05
Step: 27590, train/epoch: 6.565920829772949
Step: 27600, train/loss: 0.5073000192642212
Step: 27600, train/grad_norm: 8.968123435974121
Step: 27600, train/learning_rate: 1.7158496120828204e-05
Step: 27600, train/epoch: 6.568300724029541
Step: 27610, train/loss: 0.23510000109672546
Step: 27610, train/grad_norm: 4.84758996963501
Step: 27610, train/learning_rate: 1.7146596292150207e-05
Step: 27610, train/epoch: 6.570680618286133
Step: 27620, train/loss: 0.33730000257492065
Step: 27620, train/grad_norm: 12.927824974060059
Step: 27620, train/learning_rate: 1.7134698282461613e-05
Step: 27620, train/epoch: 6.573060512542725
Step: 27630, train/loss: 0.3068999946117401
Step: 27630, train/grad_norm: 7.936523914337158
Step: 27630, train/learning_rate: 1.7122798453783616e-05
Step: 27630, train/epoch: 6.575440406799316
Step: 27640, train/loss: 0.26420000195503235
Step: 27640, train/grad_norm: 17.5531063079834
Step: 27640, train/learning_rate: 1.7110900444095023e-05
Step: 27640, train/epoch: 6.577820301055908
Step: 27650, train/loss: 0.3278999924659729
Step: 27650, train/grad_norm: 4.541684150695801
Step: 27650, train/learning_rate: 1.7099000615417026e-05
Step: 27650, train/epoch: 6.580199718475342
Step: 27660, train/loss: 0.3012000024318695
Step: 27660, train/grad_norm: 10.52413272857666
Step: 27660, train/learning_rate: 1.708710078673903e-05
Step: 27660, train/epoch: 6.582579612731934
Step: 27670, train/loss: 0.4339999854564667
Step: 27670, train/grad_norm: 8.72059154510498
Step: 27670, train/learning_rate: 1.7075202777050436e-05
Step: 27670, train/epoch: 6.584959506988525
Step: 27680, train/loss: 0.3619999885559082
Step: 27680, train/grad_norm: 15.451351165771484
Step: 27680, train/learning_rate: 1.706330294837244e-05
Step: 27680, train/epoch: 6.587339401245117
Step: 27690, train/loss: 0.32739999890327454
Step: 27690, train/grad_norm: 9.538777351379395
Step: 27690, train/learning_rate: 1.7051404938683845e-05
Step: 27690, train/epoch: 6.589719295501709
Step: 27700, train/loss: 0.26899999380111694
Step: 27700, train/grad_norm: 7.783566474914551
Step: 27700, train/learning_rate: 1.7039505110005848e-05
Step: 27700, train/epoch: 6.592099189758301
Step: 27710, train/loss: 0.2711000144481659
Step: 27710, train/grad_norm: 4.33900260925293
Step: 27710, train/learning_rate: 1.702760528132785e-05
Step: 27710, train/epoch: 6.594478607177734
Step: 27720, train/loss: 0.3628000020980835
Step: 27720, train/grad_norm: 20.591251373291016
Step: 27720, train/learning_rate: 1.7015707271639258e-05
Step: 27720, train/epoch: 6.596858501434326
Step: 27730, train/loss: 0.4453999996185303
Step: 27730, train/grad_norm: 23.616912841796875
Step: 27730, train/learning_rate: 1.700380744296126e-05
Step: 27730, train/epoch: 6.599238395690918
Step: 27740, train/loss: 0.4862000048160553
Step: 27740, train/grad_norm: 20.751026153564453
Step: 27740, train/learning_rate: 1.6991909433272667e-05
Step: 27740, train/epoch: 6.60161828994751
Step: 27750, train/loss: 0.4293000102043152
Step: 27750, train/grad_norm: 9.882608413696289
Step: 27750, train/learning_rate: 1.698000960459467e-05
Step: 27750, train/epoch: 6.603998184204102
Step: 27760, train/loss: 0.28619998693466187
Step: 27760, train/grad_norm: 7.278033256530762
Step: 27760, train/learning_rate: 1.6968109775916673e-05
Step: 27760, train/epoch: 6.606378078460693
Step: 27770, train/loss: 0.23160000145435333
Step: 27770, train/grad_norm: 2.050938844680786
Step: 27770, train/learning_rate: 1.695621176622808e-05
Step: 27770, train/epoch: 6.608757972717285
Step: 27780, train/loss: 0.3774000108242035
Step: 27780, train/grad_norm: 8.204312324523926
Step: 27780, train/learning_rate: 1.6944311937550083e-05
Step: 27780, train/epoch: 6.611137390136719
Step: 27790, train/loss: 0.3727000057697296
Step: 27790, train/grad_norm: 3.4462411403656006
Step: 27790, train/learning_rate: 1.693241392786149e-05
Step: 27790, train/epoch: 6.6135172843933105
Step: 27800, train/loss: 0.46889999508857727
Step: 27800, train/grad_norm: 25.15345573425293
Step: 27800, train/learning_rate: 1.6920514099183492e-05
Step: 27800, train/epoch: 6.615897178649902
Step: 27810, train/loss: 0.376800000667572
Step: 27810, train/grad_norm: 7.636153221130371
Step: 27810, train/learning_rate: 1.6908614270505495e-05
Step: 27810, train/epoch: 6.618277072906494
Step: 27820, train/loss: 0.4278999865055084
Step: 27820, train/grad_norm: 9.266136169433594
Step: 27820, train/learning_rate: 1.6896716260816902e-05
Step: 27820, train/epoch: 6.620656967163086
Step: 27830, train/loss: 0.1525000035762787
Step: 27830, train/grad_norm: 16.27381134033203
Step: 27830, train/learning_rate: 1.6884816432138905e-05
Step: 27830, train/epoch: 6.623036861419678
Step: 27840, train/loss: 0.26600000262260437
Step: 27840, train/grad_norm: 11.819782257080078
Step: 27840, train/learning_rate: 1.687291842245031e-05
Step: 27840, train/epoch: 6.625416278839111
Step: 27850, train/loss: 0.5979999899864197
Step: 27850, train/grad_norm: 31.82272720336914
Step: 27850, train/learning_rate: 1.6861018593772314e-05
Step: 27850, train/epoch: 6.627796173095703
Step: 27860, train/loss: 0.41019999980926514
Step: 27860, train/grad_norm: 6.866835117340088
Step: 27860, train/learning_rate: 1.6849118765094317e-05
Step: 27860, train/epoch: 6.630176067352295
Step: 27870, train/loss: 0.24210000038146973
Step: 27870, train/grad_norm: 2.528944253921509
Step: 27870, train/learning_rate: 1.6837220755405724e-05
Step: 27870, train/epoch: 6.632555961608887
Step: 27880, train/loss: 0.23350000381469727
Step: 27880, train/grad_norm: 21.149200439453125
Step: 27880, train/learning_rate: 1.6825320926727727e-05
Step: 27880, train/epoch: 6.6349358558654785
Step: 27890, train/loss: 0.31630000472068787
Step: 27890, train/grad_norm: 16.16272735595703
Step: 27890, train/learning_rate: 1.6813422917039134e-05
Step: 27890, train/epoch: 6.63731575012207
Step: 27900, train/loss: 0.5134999752044678
Step: 27900, train/grad_norm: 3.7280547618865967
Step: 27900, train/learning_rate: 1.6801523088361137e-05
Step: 27900, train/epoch: 6.639695167541504
Step: 27910, train/loss: 0.29440000653266907
Step: 27910, train/grad_norm: 3.6023004055023193
Step: 27910, train/learning_rate: 1.678962325968314e-05
Step: 27910, train/epoch: 6.642075061798096
Step: 27920, train/loss: 0.2883000075817108
Step: 27920, train/grad_norm: 9.133441925048828
Step: 27920, train/learning_rate: 1.6777725249994546e-05
Step: 27920, train/epoch: 6.6444549560546875
Step: 27930, train/loss: 0.3492000102996826
Step: 27930, train/grad_norm: 19.987993240356445
Step: 27930, train/learning_rate: 1.676582542131655e-05
Step: 27930, train/epoch: 6.646834850311279
Step: 27940, train/loss: 0.5037999749183655
Step: 27940, train/grad_norm: 39.05789566040039
Step: 27940, train/learning_rate: 1.6753927411627956e-05
Step: 27940, train/epoch: 6.649214744567871
Step: 27950, train/loss: 0.2102999985218048
Step: 27950, train/grad_norm: 3.584733009338379
Step: 27950, train/learning_rate: 1.674202758294996e-05
Step: 27950, train/epoch: 6.651594638824463
Step: 27960, train/loss: 0.35740000009536743
Step: 27960, train/grad_norm: 13.744184494018555
Step: 27960, train/learning_rate: 1.6730127754271962e-05
Step: 27960, train/epoch: 6.653974533081055
Step: 27970, train/loss: 0.3573000133037567
Step: 27970, train/grad_norm: 28.46261978149414
Step: 27970, train/learning_rate: 1.6718229744583368e-05
Step: 27970, train/epoch: 6.656353950500488
Step: 27980, train/loss: 0.28540000319480896
Step: 27980, train/grad_norm: 10.911307334899902
Step: 27980, train/learning_rate: 1.670632991590537e-05
Step: 27980, train/epoch: 6.65873384475708
Step: 27990, train/loss: 0.3305000066757202
Step: 27990, train/grad_norm: 13.713109016418457
Step: 27990, train/learning_rate: 1.6694431906216778e-05
Step: 27990, train/epoch: 6.661113739013672
Step: 28000, train/loss: 0.2572999894618988
Step: 28000, train/grad_norm: 8.636378288269043
Step: 28000, train/learning_rate: 1.668253207753878e-05
Step: 28000, train/epoch: 6.663493633270264
Step: 28010, train/loss: 0.23720000684261322
Step: 28010, train/grad_norm: 20.660802841186523
Step: 28010, train/learning_rate: 1.6670632248860784e-05
Step: 28010, train/epoch: 6.6658735275268555
Step: 28020, train/loss: 0.5321000218391418
Step: 28020, train/grad_norm: 16.620040893554688
Step: 28020, train/learning_rate: 1.665873423917219e-05
Step: 28020, train/epoch: 6.668253421783447
Step: 28030, train/loss: 0.37369999289512634
Step: 28030, train/grad_norm: 11.541075706481934
Step: 28030, train/learning_rate: 1.6646834410494193e-05
Step: 28030, train/epoch: 6.670632839202881
Step: 28040, train/loss: 0.3138999938964844
Step: 28040, train/grad_norm: 4.754086017608643
Step: 28040, train/learning_rate: 1.66349364008056e-05
Step: 28040, train/epoch: 6.673012733459473
Step: 28050, train/loss: 0.3248000144958496
Step: 28050, train/grad_norm: 2.4663314819335938
Step: 28050, train/learning_rate: 1.6623036572127603e-05
Step: 28050, train/epoch: 6.6753926277160645
Step: 28060, train/loss: 0.4235000014305115
Step: 28060, train/grad_norm: 6.289052963256836
Step: 28060, train/learning_rate: 1.6611136743449606e-05
Step: 28060, train/epoch: 6.677772521972656
Step: 28070, train/loss: 0.3785000145435333
Step: 28070, train/grad_norm: 5.427392482757568
Step: 28070, train/learning_rate: 1.6599238733761013e-05
Step: 28070, train/epoch: 6.680152416229248
Step: 28080, train/loss: 0.19699999690055847
Step: 28080, train/grad_norm: 14.579001426696777
Step: 28080, train/learning_rate: 1.6587338905083016e-05
Step: 28080, train/epoch: 6.68253231048584
Step: 28090, train/loss: 0.4034999907016754
Step: 28090, train/grad_norm: 16.687665939331055
Step: 28090, train/learning_rate: 1.6575440895394422e-05
Step: 28090, train/epoch: 6.684911727905273
Step: 28100, train/loss: 0.38609999418258667
Step: 28100, train/grad_norm: 5.895840167999268
Step: 28100, train/learning_rate: 1.6563541066716425e-05
Step: 28100, train/epoch: 6.687291622161865
Step: 28110, train/loss: 0.4101000130176544
Step: 28110, train/grad_norm: 12.46450424194336
Step: 28110, train/learning_rate: 1.6551641238038428e-05
Step: 28110, train/epoch: 6.689671516418457
Step: 28120, train/loss: 0.3416999876499176
Step: 28120, train/grad_norm: 1.2594484090805054
Step: 28120, train/learning_rate: 1.6539743228349835e-05
Step: 28120, train/epoch: 6.692051410675049
Step: 28130, train/loss: 0.3098999857902527
Step: 28130, train/grad_norm: 13.023883819580078
Step: 28130, train/learning_rate: 1.6527843399671838e-05
Step: 28130, train/epoch: 6.694431304931641
Step: 28140, train/loss: 0.29339998960494995
Step: 28140, train/grad_norm: 15.925871849060059
Step: 28140, train/learning_rate: 1.6515945389983244e-05
Step: 28140, train/epoch: 6.696811199188232
Step: 28150, train/loss: 0.41920000314712524
Step: 28150, train/grad_norm: 6.635691165924072
Step: 28150, train/learning_rate: 1.6504045561305247e-05
Step: 28150, train/epoch: 6.699191093444824
Step: 28160, train/loss: 0.3034999966621399
Step: 28160, train/grad_norm: 11.030879974365234
Step: 28160, train/learning_rate: 1.649214573262725e-05
Step: 28160, train/epoch: 6.701570510864258
Step: 28170, train/loss: 0.3269999921321869
Step: 28170, train/grad_norm: 5.300278663635254
Step: 28170, train/learning_rate: 1.6480247722938657e-05
Step: 28170, train/epoch: 6.70395040512085
Step: 28180, train/loss: 0.2906000018119812
Step: 28180, train/grad_norm: 4.303573131561279
Step: 28180, train/learning_rate: 1.646834789426066e-05
Step: 28180, train/epoch: 6.706330299377441
Step: 28190, train/loss: 0.40380001068115234
Step: 28190, train/grad_norm: 12.752252578735352
Step: 28190, train/learning_rate: 1.6456449884572066e-05
Step: 28190, train/epoch: 6.708710193634033
Step: 28200, train/loss: 0.25870001316070557
Step: 28200, train/grad_norm: 7.183084964752197
Step: 28200, train/learning_rate: 1.644455005589407e-05
Step: 28200, train/epoch: 6.711090087890625
Step: 28210, train/loss: 0.5230000019073486
Step: 28210, train/grad_norm: 9.347774505615234
Step: 28210, train/learning_rate: 1.6432650227216072e-05
Step: 28210, train/epoch: 6.713469982147217
Step: 28220, train/loss: 0.27639999985694885
Step: 28220, train/grad_norm: 5.001243591308594
Step: 28220, train/learning_rate: 1.642075221752748e-05
Step: 28220, train/epoch: 6.71584939956665
Step: 28230, train/loss: 0.29269999265670776
Step: 28230, train/grad_norm: 12.121581077575684
Step: 28230, train/learning_rate: 1.6408852388849482e-05
Step: 28230, train/epoch: 6.718229293823242
Step: 28240, train/loss: 0.5726000070571899
Step: 28240, train/grad_norm: 11.414132118225098
Step: 28240, train/learning_rate: 1.639695437916089e-05
Step: 28240, train/epoch: 6.720609188079834
Step: 28250, train/loss: 0.3693999946117401
Step: 28250, train/grad_norm: 10.746557235717773
Step: 28250, train/learning_rate: 1.638505455048289e-05
Step: 28250, train/epoch: 6.722989082336426
Step: 28260, train/loss: 0.44999998807907104
Step: 28260, train/grad_norm: 5.970267295837402
Step: 28260, train/learning_rate: 1.6373156540794298e-05
Step: 28260, train/epoch: 6.725368976593018
Step: 28270, train/loss: 0.42719998955726624
Step: 28270, train/grad_norm: 9.962615966796875
Step: 28270, train/learning_rate: 1.63612567121163e-05
Step: 28270, train/epoch: 6.727748870849609
Step: 28280, train/loss: 0.25540000200271606
Step: 28280, train/grad_norm: 5.509791374206543
Step: 28280, train/learning_rate: 1.6349356883438304e-05
Step: 28280, train/epoch: 6.730128288269043
Step: 28290, train/loss: 0.2937000095844269
Step: 28290, train/grad_norm: 3.888378143310547
Step: 28290, train/learning_rate: 1.633745887374971e-05
Step: 28290, train/epoch: 6.732508182525635
Step: 28300, train/loss: 0.3734999895095825
Step: 28300, train/grad_norm: 4.289247512817383
Step: 28300, train/learning_rate: 1.6325559045071714e-05
Step: 28300, train/epoch: 6.734888076782227
Step: 28310, train/loss: 0.314300000667572
Step: 28310, train/grad_norm: 8.08353042602539
Step: 28310, train/learning_rate: 1.631366103538312e-05
Step: 28310, train/epoch: 6.737267971038818
Step: 28320, train/loss: 0.3089999854564667
Step: 28320, train/grad_norm: 6.558446407318115
Step: 28320, train/learning_rate: 1.6301761206705123e-05
Step: 28320, train/epoch: 6.73964786529541
Step: 28330, train/loss: 0.45350000262260437
Step: 28330, train/grad_norm: 20.172019958496094
Step: 28330, train/learning_rate: 1.6289861378027126e-05
Step: 28330, train/epoch: 6.742027759552002
Step: 28340, train/loss: 0.4812999963760376
Step: 28340, train/grad_norm: 14.469351768493652
Step: 28340, train/learning_rate: 1.6277963368338533e-05
Step: 28340, train/epoch: 6.744407653808594
Step: 28350, train/loss: 0.3634999990463257
Step: 28350, train/grad_norm: 9.29428482055664
Step: 28350, train/learning_rate: 1.6266063539660536e-05
Step: 28350, train/epoch: 6.746787071228027
Step: 28360, train/loss: 0.45719999074935913
Step: 28360, train/grad_norm: 34.645660400390625
Step: 28360, train/learning_rate: 1.6254165529971942e-05
Step: 28360, train/epoch: 6.749166965484619
Step: 28370, train/loss: 0.32989999651908875
Step: 28370, train/grad_norm: 13.1022367477417
Step: 28370, train/learning_rate: 1.6242265701293945e-05
Step: 28370, train/epoch: 6.751546859741211
Step: 28380, train/loss: 0.28769999742507935
Step: 28380, train/grad_norm: 14.347569465637207
Step: 28380, train/learning_rate: 1.623036587261595e-05
Step: 28380, train/epoch: 6.753926753997803
Step: 28390, train/loss: 0.4399999976158142
Step: 28390, train/grad_norm: 14.49007511138916
Step: 28390, train/learning_rate: 1.6218467862927355e-05
Step: 28390, train/epoch: 6.7563066482543945
Step: 28400, train/loss: 0.3984000086784363
Step: 28400, train/grad_norm: 13.171870231628418
Step: 28400, train/learning_rate: 1.6206568034249358e-05
Step: 28400, train/epoch: 6.758686542510986
Step: 28410, train/loss: 0.3580000102519989
Step: 28410, train/grad_norm: 32.232967376708984
Step: 28410, train/learning_rate: 1.6194670024560764e-05
Step: 28410, train/epoch: 6.76106595993042
Step: 28420, train/loss: 0.249099999666214
Step: 28420, train/grad_norm: 2.390531539916992
Step: 28420, train/learning_rate: 1.6182770195882767e-05
Step: 28420, train/epoch: 6.763445854187012
Step: 28430, train/loss: 0.325300008058548
Step: 28430, train/grad_norm: 27.113014221191406
Step: 28430, train/learning_rate: 1.617087036720477e-05
Step: 28430, train/epoch: 6.7658257484436035
Step: 28440, train/loss: 0.3395000100135803
Step: 28440, train/grad_norm: 8.655304908752441
Step: 28440, train/learning_rate: 1.6158972357516177e-05
Step: 28440, train/epoch: 6.768205642700195
Step: 28450, train/loss: 0.4142000079154968
Step: 28450, train/grad_norm: 4.360827922821045
Step: 28450, train/learning_rate: 1.614707252883818e-05
Step: 28450, train/epoch: 6.770585536956787
Step: 28460, train/loss: 0.3449999988079071
Step: 28460, train/grad_norm: 15.975532531738281
Step: 28460, train/learning_rate: 1.6135174519149587e-05
Step: 28460, train/epoch: 6.772965431213379
Step: 28470, train/loss: 0.38920000195503235
Step: 28470, train/grad_norm: 15.50323486328125
Step: 28470, train/learning_rate: 1.612327469047159e-05
Step: 28470, train/epoch: 6.7753448486328125
Step: 28480, train/loss: 0.390500009059906
Step: 28480, train/grad_norm: 19.015859603881836
Step: 28480, train/learning_rate: 1.6111374861793593e-05
Step: 28480, train/epoch: 6.777724742889404
Step: 28490, train/loss: 0.41370001435279846
Step: 28490, train/grad_norm: 21.55276870727539
Step: 28490, train/learning_rate: 1.6099476852105e-05
Step: 28490, train/epoch: 6.780104637145996
Step: 28500, train/loss: 0.25609999895095825
Step: 28500, train/grad_norm: 3.720931053161621
Step: 28500, train/learning_rate: 1.6087577023427002e-05
Step: 28500, train/epoch: 6.782484531402588
Step: 28510, train/loss: 0.48809999227523804
Step: 28510, train/grad_norm: 20.1296443939209
Step: 28510, train/learning_rate: 1.607567901373841e-05
Step: 28510, train/epoch: 6.78486442565918
Step: 28520, train/loss: 0.4081999957561493
Step: 28520, train/grad_norm: 6.967043876647949
Step: 28520, train/learning_rate: 1.606377918506041e-05
Step: 28520, train/epoch: 6.7872443199157715
Step: 28530, train/loss: 0.4251999855041504
Step: 28530, train/grad_norm: 33.68281936645508
Step: 28530, train/learning_rate: 1.6051879356382415e-05
Step: 28530, train/epoch: 6.789624214172363
Step: 28540, train/loss: 0.2718999981880188
Step: 28540, train/grad_norm: 17.29050064086914
Step: 28540, train/learning_rate: 1.603998134669382e-05
Step: 28540, train/epoch: 6.792003631591797
Step: 28550, train/loss: 0.42730000615119934
Step: 28550, train/grad_norm: 4.864697456359863
Step: 28550, train/learning_rate: 1.6028081518015824e-05
Step: 28550, train/epoch: 6.794383525848389
Step: 28560, train/loss: 0.30790001153945923
Step: 28560, train/grad_norm: 19.110157012939453
Step: 28560, train/learning_rate: 1.601618350832723e-05
Step: 28560, train/epoch: 6.7967634201049805
Step: 28570, train/loss: 0.41620001196861267
Step: 28570, train/grad_norm: 19.91680908203125
Step: 28570, train/learning_rate: 1.6004283679649234e-05
Step: 28570, train/epoch: 6.799143314361572
Step: 28580, train/loss: 0.45719999074935913
Step: 28580, train/grad_norm: 10.939240455627441
Step: 28580, train/learning_rate: 1.5992383850971237e-05
Step: 28580, train/epoch: 6.801523208618164
Step: 28590, train/loss: 0.3003999888896942
Step: 28590, train/grad_norm: 23.251014709472656
Step: 28590, train/learning_rate: 1.5980485841282643e-05
Step: 28590, train/epoch: 6.803903102874756
Step: 28600, train/loss: 0.29409998655319214
Step: 28600, train/grad_norm: 23.838857650756836
Step: 28600, train/learning_rate: 1.5968586012604646e-05
Step: 28600, train/epoch: 6.8062825202941895
Step: 28610, train/loss: 0.26109999418258667
Step: 28610, train/grad_norm: 5.61285924911499
Step: 28610, train/learning_rate: 1.5956688002916053e-05
Step: 28610, train/epoch: 6.808662414550781
Step: 28620, train/loss: 0.28529998660087585
Step: 28620, train/grad_norm: 18.314531326293945
Step: 28620, train/learning_rate: 1.5944788174238056e-05
Step: 28620, train/epoch: 6.811042308807373
Step: 28630, train/loss: 0.3222000002861023
Step: 28630, train/grad_norm: 8.779162406921387
Step: 28630, train/learning_rate: 1.593288834556006e-05
Step: 28630, train/epoch: 6.813422203063965
Step: 28640, train/loss: 0.4271000027656555
Step: 28640, train/grad_norm: 3.904937505722046
Step: 28640, train/learning_rate: 1.5920990335871466e-05
Step: 28640, train/epoch: 6.815802097320557
Step: 28650, train/loss: 0.41760000586509705
Step: 28650, train/grad_norm: 5.029407024383545
Step: 28650, train/learning_rate: 1.590909050719347e-05
Step: 28650, train/epoch: 6.818181991577148
Step: 28660, train/loss: 0.3529999852180481
Step: 28660, train/grad_norm: 13.107172966003418
Step: 28660, train/learning_rate: 1.5897192497504875e-05
Step: 28660, train/epoch: 6.820561408996582
Step: 28670, train/loss: 0.3779999911785126
Step: 28670, train/grad_norm: 1.961472988128662
Step: 28670, train/learning_rate: 1.5885292668826878e-05
Step: 28670, train/epoch: 6.822941303253174
Step: 28680, train/loss: 0.28610000014305115
Step: 28680, train/grad_norm: 17.931156158447266
Step: 28680, train/learning_rate: 1.587339284014888e-05
Step: 28680, train/epoch: 6.825321197509766
Step: 28690, train/loss: 0.3619999885559082
Step: 28690, train/grad_norm: 7.02264928817749
Step: 28690, train/learning_rate: 1.5861494830460288e-05
Step: 28690, train/epoch: 6.827701091766357
Step: 28700, train/loss: 0.2791999876499176
Step: 28700, train/grad_norm: 11.107622146606445
Step: 28700, train/learning_rate: 1.584959500178229e-05
Step: 28700, train/epoch: 6.830080986022949
Step: 28710, train/loss: 0.2053000032901764
Step: 28710, train/grad_norm: 9.555700302124023
Step: 28710, train/learning_rate: 1.5837696992093697e-05
Step: 28710, train/epoch: 6.832460880279541
Step: 28720, train/loss: 0.4138999879360199
Step: 28720, train/grad_norm: 14.860365867614746
Step: 28720, train/learning_rate: 1.58257971634157e-05
Step: 28720, train/epoch: 6.834840774536133
Step: 28730, train/loss: 0.4343000054359436
Step: 28730, train/grad_norm: 4.0558390617370605
Step: 28730, train/learning_rate: 1.5813897334737703e-05
Step: 28730, train/epoch: 6.837220191955566
Step: 28740, train/loss: 0.5562000274658203
Step: 28740, train/grad_norm: 2.449453353881836
Step: 28740, train/learning_rate: 1.580199932504911e-05
Step: 28740, train/epoch: 6.839600086212158
Step: 28750, train/loss: 0.37940001487731934
Step: 28750, train/grad_norm: 7.970383167266846
Step: 28750, train/learning_rate: 1.5790099496371113e-05
Step: 28750, train/epoch: 6.84197998046875
Step: 28760, train/loss: 0.5911999940872192
Step: 28760, train/grad_norm: 9.22456169128418
Step: 28760, train/learning_rate: 1.577820148668252e-05
Step: 28760, train/epoch: 6.844359874725342
Step: 28770, train/loss: 0.535099983215332
Step: 28770, train/grad_norm: 8.17485237121582
Step: 28770, train/learning_rate: 1.5766301658004522e-05
Step: 28770, train/epoch: 6.846739768981934
Step: 28780, train/loss: 0.3668999969959259
Step: 28780, train/grad_norm: 5.982374668121338
Step: 28780, train/learning_rate: 1.5754401829326525e-05
Step: 28780, train/epoch: 6.849119663238525
Step: 28790, train/loss: 0.3172000050544739
Step: 28790, train/grad_norm: 11.40175724029541
Step: 28790, train/learning_rate: 1.5742503819637932e-05
Step: 28790, train/epoch: 6.851499080657959
Step: 28800, train/loss: 0.4348999857902527
Step: 28800, train/grad_norm: 9.063314437866211
Step: 28800, train/learning_rate: 1.5730603990959935e-05
Step: 28800, train/epoch: 6.853878974914551
Step: 28810, train/loss: 0.27649998664855957
Step: 28810, train/grad_norm: 20.699338912963867
Step: 28810, train/learning_rate: 1.571870598127134e-05
Step: 28810, train/epoch: 6.856258869171143
Step: 28820, train/loss: 0.22419999539852142
Step: 28820, train/grad_norm: 3.040375232696533
Step: 28820, train/learning_rate: 1.5706806152593344e-05
Step: 28820, train/epoch: 6.858638763427734
Step: 28830, train/loss: 0.3052999973297119
Step: 28830, train/grad_norm: 16.015365600585938
Step: 28830, train/learning_rate: 1.5694906323915347e-05
Step: 28830, train/epoch: 6.861018657684326
Step: 28840, train/loss: 0.30809998512268066
Step: 28840, train/grad_norm: 5.334681034088135
Step: 28840, train/learning_rate: 1.5683008314226754e-05
Step: 28840, train/epoch: 6.863398551940918
Step: 28850, train/loss: 0.33309999108314514
Step: 28850, train/grad_norm: 7.150209903717041
Step: 28850, train/learning_rate: 1.5671108485548757e-05
Step: 28850, train/epoch: 6.865777969360352
Step: 28860, train/loss: 0.2851000130176544
Step: 28860, train/grad_norm: 5.325811386108398
Step: 28860, train/learning_rate: 1.5659210475860164e-05
Step: 28860, train/epoch: 6.868157863616943
Step: 28870, train/loss: 0.4142000079154968
Step: 28870, train/grad_norm: 21.724544525146484
Step: 28870, train/learning_rate: 1.5647310647182167e-05
Step: 28870, train/epoch: 6.870537757873535
Step: 28880, train/loss: 0.29760000109672546
Step: 28880, train/grad_norm: 10.365763664245605
Step: 28880, train/learning_rate: 1.563541081850417e-05
Step: 28880, train/epoch: 6.872917652130127
Step: 28890, train/loss: 0.353300005197525
Step: 28890, train/grad_norm: 22.611099243164062
Step: 28890, train/learning_rate: 1.5623512808815576e-05
Step: 28890, train/epoch: 6.875297546386719
Step: 28900, train/loss: 0.398499995470047
Step: 28900, train/grad_norm: 27.410181045532227
Step: 28900, train/learning_rate: 1.561161298013758e-05
Step: 28900, train/epoch: 6.8776774406433105
Step: 28910, train/loss: 0.37400001287460327
Step: 28910, train/grad_norm: 6.623309135437012
Step: 28910, train/learning_rate: 1.5599714970448986e-05
Step: 28910, train/epoch: 6.880057334899902
Step: 28920, train/loss: 0.2793999910354614
Step: 28920, train/grad_norm: 6.950725078582764
Step: 28920, train/learning_rate: 1.558781514177099e-05
Step: 28920, train/epoch: 6.882436752319336
Step: 28930, train/loss: 0.3928999900817871
Step: 28930, train/grad_norm: 16.210346221923828
Step: 28930, train/learning_rate: 1.5575917132082395e-05
Step: 28930, train/epoch: 6.884816646575928
Step: 28940, train/loss: 0.35339999198913574
Step: 28940, train/grad_norm: 8.656197547912598
Step: 28940, train/learning_rate: 1.5564017303404398e-05
Step: 28940, train/epoch: 6.8871965408325195
Step: 28950, train/loss: 0.26080000400543213
Step: 28950, train/grad_norm: 10.6309814453125
Step: 28950, train/learning_rate: 1.55521174747264e-05
Step: 28950, train/epoch: 6.889576435089111
Step: 28960, train/loss: 0.3928999900817871
Step: 28960, train/grad_norm: 19.60114860534668
Step: 28960, train/learning_rate: 1.5540219465037808e-05
Step: 28960, train/epoch: 6.891956329345703
Step: 28970, train/loss: 0.2515000104904175
Step: 28970, train/grad_norm: 7.1080522537231445
Step: 28970, train/learning_rate: 1.552831963635981e-05
Step: 28970, train/epoch: 6.894336223602295
Step: 28980, train/loss: 0.35440000891685486
Step: 28980, train/grad_norm: 8.387262344360352
Step: 28980, train/learning_rate: 1.5516421626671217e-05
Step: 28980, train/epoch: 6.8967156410217285
Step: 28990, train/loss: 0.24410000443458557
Step: 28990, train/grad_norm: 10.244178771972656
Step: 28990, train/learning_rate: 1.550452179799322e-05
Step: 28990, train/epoch: 6.89909553527832
Step: 29000, train/loss: 0.3781999945640564
Step: 29000, train/grad_norm: 5.425586223602295
Step: 29000, train/learning_rate: 1.5492621969315223e-05
Step: 29000, train/epoch: 6.901475429534912
Step: 29010, train/loss: 0.4837999939918518
Step: 29010, train/grad_norm: 12.301305770874023
Step: 29010, train/learning_rate: 1.548072395962663e-05
Step: 29010, train/epoch: 6.903855323791504
Step: 29020, train/loss: 0.46959999203681946
Step: 29020, train/grad_norm: 9.92197322845459
Step: 29020, train/learning_rate: 1.5468824130948633e-05
Step: 29020, train/epoch: 6.906235218048096
Step: 29030, train/loss: 0.35370001196861267
Step: 29030, train/grad_norm: 16.685928344726562
Step: 29030, train/learning_rate: 1.545692612126004e-05
Step: 29030, train/epoch: 6.9086151123046875
Step: 29040, train/loss: 0.32199999690055847
Step: 29040, train/grad_norm: 5.2591938972473145
Step: 29040, train/learning_rate: 1.5445026292582043e-05
Step: 29040, train/epoch: 6.910994529724121
Step: 29050, train/loss: 0.3151000142097473
Step: 29050, train/grad_norm: 2.485016345977783
Step: 29050, train/learning_rate: 1.5433126463904046e-05
Step: 29050, train/epoch: 6.913374423980713
Step: 29060, train/loss: 0.38909998536109924
Step: 29060, train/grad_norm: 26.58767318725586
Step: 29060, train/learning_rate: 1.5421228454215452e-05
Step: 29060, train/epoch: 6.915754318237305
Step: 29070, train/loss: 0.3077000081539154
Step: 29070, train/grad_norm: 6.69753885269165
Step: 29070, train/learning_rate: 1.5409328625537455e-05
Step: 29070, train/epoch: 6.9181342124938965
Step: 29080, train/loss: 0.40720000863075256
Step: 29080, train/grad_norm: 6.387696266174316
Step: 29080, train/learning_rate: 1.539743061584886e-05
Step: 29080, train/epoch: 6.920514106750488
Step: 29090, train/loss: 0.39160001277923584
Step: 29090, train/grad_norm: 8.70938777923584
Step: 29090, train/learning_rate: 1.5385530787170865e-05
Step: 29090, train/epoch: 6.92289400100708
Step: 29100, train/loss: 0.27230000495910645
Step: 29100, train/grad_norm: 8.08260726928711
Step: 29100, train/learning_rate: 1.5373630958492868e-05
Step: 29100, train/epoch: 6.925273895263672
Step: 29110, train/loss: 0.301800012588501
Step: 29110, train/grad_norm: 7.947826385498047
Step: 29110, train/learning_rate: 1.5361732948804274e-05
Step: 29110, train/epoch: 6.9276533126831055
Step: 29120, train/loss: 0.2549000084400177
Step: 29120, train/grad_norm: 2.6666219234466553
Step: 29120, train/learning_rate: 1.5349833120126277e-05
Step: 29120, train/epoch: 6.930033206939697
Step: 29130, train/loss: 0.3005000054836273
Step: 29130, train/grad_norm: 19.647245407104492
Step: 29130, train/learning_rate: 1.5337935110437684e-05
Step: 29130, train/epoch: 6.932413101196289
Step: 29140, train/loss: 0.38850000500679016
Step: 29140, train/grad_norm: 11.832222938537598
Step: 29140, train/learning_rate: 1.5326035281759687e-05
Step: 29140, train/epoch: 6.934792995452881
Step: 29150, train/loss: 0.3296999931335449
Step: 29150, train/grad_norm: 11.83372688293457
Step: 29150, train/learning_rate: 1.531413545308169e-05
Step: 29150, train/epoch: 6.937172889709473
Step: 29160, train/loss: 0.23160000145435333
Step: 29160, train/grad_norm: 7.700159072875977
Step: 29160, train/learning_rate: 1.5302237443393096e-05
Step: 29160, train/epoch: 6.9395527839660645
Step: 29170, train/loss: 0.2727000117301941
Step: 29170, train/grad_norm: 20.95309829711914
Step: 29170, train/learning_rate: 1.52903376147151e-05
Step: 29170, train/epoch: 6.941932201385498
Step: 29180, train/loss: 0.2021999955177307
Step: 29180, train/grad_norm: 7.401318073272705
Step: 29180, train/learning_rate: 1.5278439605026506e-05
Step: 29180, train/epoch: 6.94431209564209
Step: 29190, train/loss: 0.35199999809265137
Step: 29190, train/grad_norm: 6.845581531524658
Step: 29190, train/learning_rate: 1.526653977634851e-05
Step: 29190, train/epoch: 6.946691989898682
Step: 29200, train/loss: 0.3418999910354614
Step: 29200, train/grad_norm: 9.677262306213379
Step: 29200, train/learning_rate: 1.5254640857165214e-05
Step: 29200, train/epoch: 6.949071884155273
Step: 29210, train/loss: 0.35010001063346863
Step: 29210, train/grad_norm: 8.141404151916504
Step: 29210, train/learning_rate: 1.5242741937981918e-05
Step: 29210, train/epoch: 6.951451778411865
Step: 29220, train/loss: 0.3066999912261963
Step: 29220, train/grad_norm: 29.541982650756836
Step: 29220, train/learning_rate: 1.5230842109303921e-05
Step: 29220, train/epoch: 6.953831672668457
Step: 29230, train/loss: 0.33399999141693115
Step: 29230, train/grad_norm: 22.81352996826172
Step: 29230, train/learning_rate: 1.5218943190120626e-05
Step: 29230, train/epoch: 6.956211090087891
Step: 29240, train/loss: 0.3386000096797943
Step: 29240, train/grad_norm: 18.01259994506836
Step: 29240, train/learning_rate: 1.5207044270937331e-05
Step: 29240, train/epoch: 6.958590984344482
Step: 29250, train/loss: 0.43790000677108765
Step: 29250, train/grad_norm: 16.745929718017578
Step: 29250, train/learning_rate: 1.5195145351754036e-05
Step: 29250, train/epoch: 6.960970878601074
Step: 29260, train/loss: 0.3828999996185303
Step: 29260, train/grad_norm: 18.37859344482422
Step: 29260, train/learning_rate: 1.518324643257074e-05
Step: 29260, train/epoch: 6.963350772857666
Step: 29270, train/loss: 0.4348999857902527
Step: 29270, train/grad_norm: 7.2905168533325195
Step: 29270, train/learning_rate: 1.5171346603892744e-05
Step: 29270, train/epoch: 6.965730667114258
Step: 29280, train/loss: 0.3898000121116638
Step: 29280, train/grad_norm: 12.545587539672852
Step: 29280, train/learning_rate: 1.5159447684709448e-05
Step: 29280, train/epoch: 6.96811056137085
Step: 29290, train/loss: 0.3650999963283539
Step: 29290, train/grad_norm: 7.462180137634277
Step: 29290, train/learning_rate: 1.5147548765526153e-05
Step: 29290, train/epoch: 6.970490455627441
Step: 29300, train/loss: 0.4562000036239624
Step: 29300, train/grad_norm: 12.007498741149902
Step: 29300, train/learning_rate: 1.5135649846342858e-05
Step: 29300, train/epoch: 6.972869873046875
Step: 29310, train/loss: 0.45210000872612
Step: 29310, train/grad_norm: 2.9620261192321777
Step: 29310, train/learning_rate: 1.5123750927159563e-05
Step: 29310, train/epoch: 6.975249767303467
Step: 29320, train/loss: 0.24570000171661377
Step: 29320, train/grad_norm: 3.340449571609497
Step: 29320, train/learning_rate: 1.5111851098481566e-05
Step: 29320, train/epoch: 6.977629661560059
Step: 29330, train/loss: 0.49380001425743103
Step: 29330, train/grad_norm: 21.207061767578125
Step: 29330, train/learning_rate: 1.509995217929827e-05
Step: 29330, train/epoch: 6.98000955581665
Step: 29340, train/loss: 0.27959999442100525
Step: 29340, train/grad_norm: 1.835651159286499
Step: 29340, train/learning_rate: 1.5088053260114975e-05
Step: 29340, train/epoch: 6.982389450073242
Step: 29350, train/loss: 0.32679998874664307
Step: 29350, train/grad_norm: 15.449968338012695
Step: 29350, train/learning_rate: 1.507615434093168e-05
Step: 29350, train/epoch: 6.984769344329834
Step: 29360, train/loss: 0.35269999504089355
Step: 29360, train/grad_norm: 9.119769096374512
Step: 29360, train/learning_rate: 1.5064255421748385e-05
Step: 29360, train/epoch: 6.987148761749268
Step: 29370, train/loss: 0.4099000096321106
Step: 29370, train/grad_norm: 12.684892654418945
Step: 29370, train/learning_rate: 1.5052355593070388e-05
Step: 29370, train/epoch: 6.989528656005859
Step: 29380, train/loss: 0.4828000068664551
Step: 29380, train/grad_norm: 15.365800857543945
Step: 29380, train/learning_rate: 1.5040456673887093e-05
Step: 29380, train/epoch: 6.991908550262451
Step: 29390, train/loss: 0.30640000104904175
Step: 29390, train/grad_norm: 10.127120018005371
Step: 29390, train/learning_rate: 1.5028557754703797e-05
Step: 29390, train/epoch: 6.994288444519043
Step: 29400, train/loss: 0.32089999318122864
Step: 29400, train/grad_norm: 3.5938758850097656
Step: 29400, train/learning_rate: 1.5016658835520502e-05
Step: 29400, train/epoch: 6.996668338775635
Step: 29410, train/loss: 0.4018999934196472
Step: 29410, train/grad_norm: 19.601991653442383
Step: 29410, train/learning_rate: 1.5004759916337207e-05
Step: 29410, train/epoch: 6.999048233032227
Step: 29414, eval/loss: 0.6290885806083679
Step: 29414, eval/accuracy: 0.7587116360664368
Step: 29414, eval/f1: 0.7583007216453552
Step: 29414, eval/runtime: 55.43259811401367
Step: 29414, eval/samples_per_second: 129.9409942626953
Step: 29414, eval/steps_per_second: 16.253999710083008
Step: 29414, train/epoch: 7.0
Step: 29420, train/loss: 0.414900004863739
Step: 29420, train/grad_norm: 6.781305313110352
Step: 29420, train/learning_rate: 1.4992860997153912e-05
Step: 29420, train/epoch: 7.001428127288818
Step: 29430, train/loss: 0.33489999175071716
Step: 29430, train/grad_norm: 19.900182723999023
Step: 29430, train/learning_rate: 1.4980961168475915e-05
Step: 29430, train/epoch: 7.003807544708252
Step: 29440, train/loss: 0.3743000030517578
Step: 29440, train/grad_norm: 14.032644271850586
Step: 29440, train/learning_rate: 1.496906224929262e-05
Step: 29440, train/epoch: 7.006187438964844
Step: 29450, train/loss: 0.447299987077713
Step: 29450, train/grad_norm: 11.362641334533691
Step: 29450, train/learning_rate: 1.4957163330109324e-05
Step: 29450, train/epoch: 7.0085673332214355
Step: 29460, train/loss: 0.41440001130104065
Step: 29460, train/grad_norm: 17.80329704284668
Step: 29460, train/learning_rate: 1.4945264410926029e-05
Step: 29460, train/epoch: 7.010947227478027
Step: 29470, train/loss: 0.41440001130104065
Step: 29470, train/grad_norm: 5.743572235107422
Step: 29470, train/learning_rate: 1.4933365491742734e-05
Step: 29470, train/epoch: 7.013327121734619
Step: 29480, train/loss: 0.3212999999523163
Step: 29480, train/grad_norm: 16.660202026367188
Step: 29480, train/learning_rate: 1.4921465663064737e-05
Step: 29480, train/epoch: 7.015707015991211
Step: 29490, train/loss: 0.33379998803138733
Step: 29490, train/grad_norm: 2.9521079063415527
Step: 29490, train/learning_rate: 1.4909566743881442e-05
Step: 29490, train/epoch: 7.0180864334106445
Step: 29500, train/loss: 0.39410001039505005
Step: 29500, train/grad_norm: 15.032608985900879
Step: 29500, train/learning_rate: 1.4897667824698146e-05
Step: 29500, train/epoch: 7.020466327667236
Step: 29510, train/loss: 0.3345000147819519
Step: 29510, train/grad_norm: 12.281839370727539
Step: 29510, train/learning_rate: 1.4885768905514851e-05
Step: 29510, train/epoch: 7.022846221923828
Step: 29520, train/loss: 0.2653999924659729
Step: 29520, train/grad_norm: 5.965819358825684
Step: 29520, train/learning_rate: 1.4873869986331556e-05
Step: 29520, train/epoch: 7.02522611618042
Step: 29530, train/loss: 0.46050000190734863
Step: 29530, train/grad_norm: 13.662993431091309
Step: 29530, train/learning_rate: 1.4861970157653559e-05
Step: 29530, train/epoch: 7.027606010437012
Step: 29540, train/loss: 0.1987999975681305
Step: 29540, train/grad_norm: 8.461808204650879
Step: 29540, train/learning_rate: 1.4850071238470264e-05
Step: 29540, train/epoch: 7.0299859046936035
Step: 29550, train/loss: 0.33709999918937683
Step: 29550, train/grad_norm: 11.104174613952637
Step: 29550, train/learning_rate: 1.4838172319286969e-05
Step: 29550, train/epoch: 7.032365322113037
Step: 29560, train/loss: 0.44130000472068787
Step: 29560, train/grad_norm: 9.854991912841797
Step: 29560, train/learning_rate: 1.4826273400103673e-05
Step: 29560, train/epoch: 7.034745216369629
Step: 29570, train/loss: 0.515500009059906
Step: 29570, train/grad_norm: 24.210683822631836
Step: 29570, train/learning_rate: 1.4814374480920378e-05
Step: 29570, train/epoch: 7.037125110626221
Step: 29580, train/loss: 0.243599995970726
Step: 29580, train/grad_norm: 6.677920341491699
Step: 29580, train/learning_rate: 1.4802474652242381e-05
Step: 29580, train/epoch: 7.0395050048828125
Step: 29590, train/loss: 0.3237999975681305
Step: 29590, train/grad_norm: 20.5549259185791
Step: 29590, train/learning_rate: 1.4790575733059086e-05
Step: 29590, train/epoch: 7.041884899139404
Step: 29600, train/loss: 0.25609999895095825
Step: 29600, train/grad_norm: 6.633983612060547
Step: 29600, train/learning_rate: 1.477867681387579e-05
Step: 29600, train/epoch: 7.044264793395996
Step: 29610, train/loss: 0.3100999891757965
Step: 29610, train/grad_norm: 13.089213371276855
Step: 29610, train/learning_rate: 1.4766777894692495e-05
Step: 29610, train/epoch: 7.046644687652588
Step: 29620, train/loss: 0.36419999599456787
Step: 29620, train/grad_norm: 29.477462768554688
Step: 29620, train/learning_rate: 1.47548789755092e-05
Step: 29620, train/epoch: 7.0490241050720215
Step: 29630, train/loss: 0.42419999837875366
Step: 29630, train/grad_norm: 25.834699630737305
Step: 29630, train/learning_rate: 1.4742979146831203e-05
Step: 29630, train/epoch: 7.051403999328613
Step: 29640, train/loss: 0.37779998779296875
Step: 29640, train/grad_norm: 2.817589521408081
Step: 29640, train/learning_rate: 1.4731080227647908e-05
Step: 29640, train/epoch: 7.053783893585205
Step: 29650, train/loss: 0.4510999917984009
Step: 29650, train/grad_norm: 17.7327880859375
Step: 29650, train/learning_rate: 1.4719181308464613e-05
Step: 29650, train/epoch: 7.056163787841797
Step: 29660, train/loss: 0.3508000075817108
Step: 29660, train/grad_norm: 14.330313682556152
Step: 29660, train/learning_rate: 1.4707282389281318e-05
Step: 29660, train/epoch: 7.058543682098389
Step: 29670, train/loss: 0.30169999599456787
Step: 29670, train/grad_norm: 7.519748210906982
Step: 29670, train/learning_rate: 1.4695383470098022e-05
Step: 29670, train/epoch: 7.0609235763549805
Step: 29680, train/loss: 0.27090001106262207
Step: 29680, train/grad_norm: 18.306324005126953
Step: 29680, train/learning_rate: 1.4683483641420025e-05
Step: 29680, train/epoch: 7.063302993774414
Step: 29690, train/loss: 0.349700003862381
Step: 29690, train/grad_norm: 10.260170936584473
Step: 29690, train/learning_rate: 1.467158472223673e-05
Step: 29690, train/epoch: 7.065682888031006
Step: 29700, train/loss: 0.2822999954223633
Step: 29700, train/grad_norm: 12.579829216003418
Step: 29700, train/learning_rate: 1.4659685803053435e-05
Step: 29700, train/epoch: 7.068062782287598
Step: 29710, train/loss: 0.2808000147342682
Step: 29710, train/grad_norm: 10.181486129760742
Step: 29710, train/learning_rate: 1.464778688387014e-05
Step: 29710, train/epoch: 7.0704426765441895
Step: 29720, train/loss: 0.482699990272522
Step: 29720, train/grad_norm: 10.647926330566406
Step: 29720, train/learning_rate: 1.4635887964686844e-05
Step: 29720, train/epoch: 7.072822570800781
Step: 29730, train/loss: 0.250900000333786
Step: 29730, train/grad_norm: 10.336507797241211
Step: 29730, train/learning_rate: 1.4623988136008848e-05
Step: 29730, train/epoch: 7.075202465057373
Step: 29740, train/loss: 0.2791999876499176
Step: 29740, train/grad_norm: 23.36764907836914
Step: 29740, train/learning_rate: 1.4612089216825552e-05
Step: 29740, train/epoch: 7.077581882476807
Step: 29750, train/loss: 0.28049999475479126
Step: 29750, train/grad_norm: 6.673811912536621
Step: 29750, train/learning_rate: 1.4600190297642257e-05
Step: 29750, train/epoch: 7.079961776733398
Step: 29760, train/loss: 0.44929999113082886
Step: 29760, train/grad_norm: 7.161654472351074
Step: 29760, train/learning_rate: 1.4588291378458962e-05
Step: 29760, train/epoch: 7.08234167098999
Step: 29770, train/loss: 0.3249000012874603
Step: 29770, train/grad_norm: 21.631582260131836
Step: 29770, train/learning_rate: 1.4576392459275667e-05
Step: 29770, train/epoch: 7.084721565246582
Step: 29780, train/loss: 0.3409999907016754
Step: 29780, train/grad_norm: 16.054792404174805
Step: 29780, train/learning_rate: 1.4564493540092371e-05
Step: 29780, train/epoch: 7.087101459503174
Step: 29790, train/loss: 0.30070000886917114
Step: 29790, train/grad_norm: 6.488402843475342
Step: 29790, train/learning_rate: 1.4552593711414374e-05
Step: 29790, train/epoch: 7.089481353759766
Step: 29800, train/loss: 0.1876000016927719
Step: 29800, train/grad_norm: 7.551352024078369
Step: 29800, train/learning_rate: 1.454069479223108e-05
Step: 29800, train/epoch: 7.091861248016357
Step: 29810, train/loss: 0.3224000036716461
Step: 29810, train/grad_norm: 1.5457392930984497
Step: 29810, train/learning_rate: 1.4528795873047784e-05
Step: 29810, train/epoch: 7.094240665435791
Step: 29820, train/loss: 0.30219998955726624
Step: 29820, train/grad_norm: 5.918846130371094
Step: 29820, train/learning_rate: 1.4516896953864489e-05
Step: 29820, train/epoch: 7.096620559692383
Step: 29830, train/loss: 0.28940001130104065
Step: 29830, train/grad_norm: 8.156667709350586
Step: 29830, train/learning_rate: 1.4504998034681194e-05
Step: 29830, train/epoch: 7.099000453948975
Step: 29840, train/loss: 0.31200000643730164
Step: 29840, train/grad_norm: 8.344697952270508
Step: 29840, train/learning_rate: 1.4493098206003197e-05
Step: 29840, train/epoch: 7.101380348205566
Step: 29850, train/loss: 0.23690000176429749
Step: 29850, train/grad_norm: 13.45828628540039
Step: 29850, train/learning_rate: 1.4481199286819901e-05
Step: 29850, train/epoch: 7.103760242462158
Step: 29860, train/loss: 0.28870001435279846
Step: 29860, train/grad_norm: 13.197068214416504
Step: 29860, train/learning_rate: 1.4469300367636606e-05
Step: 29860, train/epoch: 7.10614013671875
Step: 29870, train/loss: 0.31779998540878296
Step: 29870, train/grad_norm: 1.3613407611846924
Step: 29870, train/learning_rate: 1.4457401448453311e-05
Step: 29870, train/epoch: 7.108519554138184
Step: 29880, train/loss: 0.2996000051498413
Step: 29880, train/grad_norm: 18.520793914794922
Step: 29880, train/learning_rate: 1.4445502529270016e-05
Step: 29880, train/epoch: 7.110899448394775
Step: 29890, train/loss: 0.37860000133514404
Step: 29890, train/grad_norm: 19.6367130279541
Step: 29890, train/learning_rate: 1.4433602700592019e-05
Step: 29890, train/epoch: 7.113279342651367
Step: 29900, train/loss: 0.23090000450611115
Step: 29900, train/grad_norm: 10.241454124450684
Step: 29900, train/learning_rate: 1.4421703781408723e-05
Step: 29900, train/epoch: 7.115659236907959
Step: 29910, train/loss: 0.414000004529953
Step: 29910, train/grad_norm: 16.33470916748047
Step: 29910, train/learning_rate: 1.4409804862225428e-05
Step: 29910, train/epoch: 7.118039131164551
Step: 29920, train/loss: 0.4584999978542328
Step: 29920, train/grad_norm: 14.989452362060547
Step: 29920, train/learning_rate: 1.4397905943042133e-05
Step: 29920, train/epoch: 7.120419025421143
Step: 29930, train/loss: 0.3887999951839447
Step: 29930, train/grad_norm: 20.049272537231445
Step: 29930, train/learning_rate: 1.4386007023858838e-05
Step: 29930, train/epoch: 7.122798442840576
Step: 29940, train/loss: 0.31029999256134033
Step: 29940, train/grad_norm: 15.332148551940918
Step: 29940, train/learning_rate: 1.437410719518084e-05
Step: 29940, train/epoch: 7.125178337097168
Step: 29950, train/loss: 0.31029999256134033
Step: 29950, train/grad_norm: 19.484216690063477
Step: 29950, train/learning_rate: 1.4362208275997546e-05
Step: 29950, train/epoch: 7.12755823135376
Step: 29960, train/loss: 0.5072000026702881
Step: 29960, train/grad_norm: 14.980375289916992
Step: 29960, train/learning_rate: 1.435030935681425e-05
Step: 29960, train/epoch: 7.129938125610352
Step: 29970, train/loss: 0.3806000053882599
Step: 29970, train/grad_norm: 7.014464378356934
Step: 29970, train/learning_rate: 1.4338410437630955e-05
Step: 29970, train/epoch: 7.132318019866943
Step: 29980, train/loss: 0.2888999879360199
Step: 29980, train/grad_norm: 23.88919448852539
Step: 29980, train/learning_rate: 1.432651151844766e-05
Step: 29980, train/epoch: 7.134697914123535
Step: 29990, train/loss: 0.29170000553131104
Step: 29990, train/grad_norm: 13.167740821838379
Step: 29990, train/learning_rate: 1.4314611689769663e-05
Step: 29990, train/epoch: 7.137077808380127
Step: 30000, train/loss: 0.18279999494552612
Step: 30000, train/grad_norm: 7.100834369659424
Step: 30000, train/learning_rate: 1.4302712770586368e-05
Step: 30000, train/epoch: 7.1394572257995605
Step: 30010, train/loss: 0.3192000091075897
Step: 30010, train/grad_norm: 36.81454086303711
Step: 30010, train/learning_rate: 1.4290813851403072e-05
Step: 30010, train/epoch: 7.141837120056152
Step: 30020, train/loss: 0.43220001459121704
Step: 30020, train/grad_norm: 12.771413803100586
Step: 30020, train/learning_rate: 1.4278914932219777e-05
Step: 30020, train/epoch: 7.144217014312744
Step: 30030, train/loss: 0.21299999952316284
Step: 30030, train/grad_norm: 12.501060485839844
Step: 30030, train/learning_rate: 1.4267016013036482e-05
Step: 30030, train/epoch: 7.146596908569336
Step: 30040, train/loss: 0.4528000056743622
Step: 30040, train/grad_norm: 7.487006187438965
Step: 30040, train/learning_rate: 1.4255116184358485e-05
Step: 30040, train/epoch: 7.148976802825928
Step: 30050, train/loss: 0.3294000029563904
Step: 30050, train/grad_norm: 1.7201365232467651
Step: 30050, train/learning_rate: 1.424321726517519e-05
Step: 30050, train/epoch: 7.1513566970825195
Step: 30060, train/loss: 0.34630000591278076
Step: 30060, train/grad_norm: 9.271698951721191
Step: 30060, train/learning_rate: 1.4231318345991895e-05
Step: 30060, train/epoch: 7.153736114501953
Step: 30070, train/loss: 0.4309999942779541
Step: 30070, train/grad_norm: 18.114490509033203
Step: 30070, train/learning_rate: 1.42194194268086e-05
Step: 30070, train/epoch: 7.156116008758545
Step: 30080, train/loss: 0.24619999527931213
Step: 30080, train/grad_norm: 6.409325122833252
Step: 30080, train/learning_rate: 1.4207520507625304e-05
Step: 30080, train/epoch: 7.158495903015137
Step: 30090, train/loss: 0.33869999647140503
Step: 30090, train/grad_norm: 5.915788173675537
Step: 30090, train/learning_rate: 1.4195620678947307e-05
Step: 30090, train/epoch: 7.1608757972717285
Step: 30100, train/loss: 0.2531999945640564
Step: 30100, train/grad_norm: 7.678237438201904
Step: 30100, train/learning_rate: 1.4183721759764012e-05
Step: 30100, train/epoch: 7.16325569152832
Step: 30110, train/loss: 0.4456000030040741
Step: 30110, train/grad_norm: 9.541324615478516
Step: 30110, train/learning_rate: 1.4171822840580717e-05
Step: 30110, train/epoch: 7.165635585784912
Step: 30120, train/loss: 0.4560000002384186
Step: 30120, train/grad_norm: 14.086129188537598
Step: 30120, train/learning_rate: 1.4159923921397422e-05
Step: 30120, train/epoch: 7.168015003204346
Step: 30130, train/loss: 0.19110000133514404
Step: 30130, train/grad_norm: 13.559616088867188
Step: 30130, train/learning_rate: 1.4148025002214126e-05
Step: 30130, train/epoch: 7.1703948974609375
Step: 30140, train/loss: 0.48739999532699585
Step: 30140, train/grad_norm: 3.0827791690826416
Step: 30140, train/learning_rate: 1.4136126083030831e-05
Step: 30140, train/epoch: 7.172774791717529
Step: 30150, train/loss: 0.4081999957561493
Step: 30150, train/grad_norm: 43.26905822753906
Step: 30150, train/learning_rate: 1.4124226254352834e-05
Step: 30150, train/epoch: 7.175154685974121
Step: 30160, train/loss: 0.38989999890327454
Step: 30160, train/grad_norm: 12.285079002380371
Step: 30160, train/learning_rate: 1.4112327335169539e-05
Step: 30160, train/epoch: 7.177534580230713
Step: 30170, train/loss: 0.41920000314712524
Step: 30170, train/grad_norm: 10.441534042358398
Step: 30170, train/learning_rate: 1.4100428415986244e-05
Step: 30170, train/epoch: 7.179914474487305
Step: 30180, train/loss: 0.22310000658035278
Step: 30180, train/grad_norm: 10.364140510559082
Step: 30180, train/learning_rate: 1.4088529496802948e-05
Step: 30180, train/epoch: 7.1822943687438965
Step: 30190, train/loss: 0.49619999527931213
Step: 30190, train/grad_norm: 20.429059982299805
Step: 30190, train/learning_rate: 1.4076630577619653e-05
Step: 30190, train/epoch: 7.18467378616333
Step: 30200, train/loss: 0.47679999470710754
Step: 30200, train/grad_norm: 38.65999984741211
Step: 30200, train/learning_rate: 1.4064730748941656e-05
Step: 30200, train/epoch: 7.187053680419922
Step: 30210, train/loss: 0.2094999998807907
Step: 30210, train/grad_norm: 10.243953704833984
Step: 30210, train/learning_rate: 1.4052831829758361e-05
Step: 30210, train/epoch: 7.189433574676514
Step: 30220, train/loss: 0.4343000054359436
Step: 30220, train/grad_norm: 3.07209849357605
Step: 30220, train/learning_rate: 1.4040932910575066e-05
Step: 30220, train/epoch: 7.1918134689331055
Step: 30230, train/loss: 0.323199987411499
Step: 30230, train/grad_norm: 5.159681797027588
Step: 30230, train/learning_rate: 1.402903399139177e-05
Step: 30230, train/epoch: 7.194193363189697
Step: 30240, train/loss: 0.27790001034736633
Step: 30240, train/grad_norm: 14.010331153869629
Step: 30240, train/learning_rate: 1.4017135072208475e-05
Step: 30240, train/epoch: 7.196573257446289
Step: 30250, train/loss: 0.3506999909877777
Step: 30250, train/grad_norm: 9.109283447265625
Step: 30250, train/learning_rate: 1.4005235243530478e-05
Step: 30250, train/epoch: 7.198952674865723
Step: 30260, train/loss: 0.2930999994277954
Step: 30260, train/grad_norm: 19.22576332092285
Step: 30260, train/learning_rate: 1.3993336324347183e-05
Step: 30260, train/epoch: 7.2013325691223145
Step: 30270, train/loss: 0.35350000858306885
Step: 30270, train/grad_norm: 7.382339000701904
Step: 30270, train/learning_rate: 1.3981437405163888e-05
Step: 30270, train/epoch: 7.203712463378906
Step: 30280, train/loss: 0.350600004196167
Step: 30280, train/grad_norm: 10.555427551269531
Step: 30280, train/learning_rate: 1.3969538485980593e-05
Step: 30280, train/epoch: 7.206092357635498
Step: 30290, train/loss: 0.2872999906539917
Step: 30290, train/grad_norm: 19.921001434326172
Step: 30290, train/learning_rate: 1.3957639566797297e-05
Step: 30290, train/epoch: 7.20847225189209
Step: 30300, train/loss: 0.20170000195503235
Step: 30300, train/grad_norm: 18.103992462158203
Step: 30300, train/learning_rate: 1.39457397381193e-05
Step: 30300, train/epoch: 7.210852146148682
Step: 30310, train/loss: 0.43720000982284546
Step: 30310, train/grad_norm: 14.491799354553223
Step: 30310, train/learning_rate: 1.3933840818936005e-05
Step: 30310, train/epoch: 7.213231563568115
Step: 30320, train/loss: 0.3970000147819519
Step: 30320, train/grad_norm: 14.984622955322266
Step: 30320, train/learning_rate: 1.392194189975271e-05
Step: 30320, train/epoch: 7.215611457824707
Step: 30330, train/loss: 0.43970000743865967
Step: 30330, train/grad_norm: 10.031268119812012
Step: 30330, train/learning_rate: 1.3910042980569415e-05
Step: 30330, train/epoch: 7.217991352081299
Step: 30340, train/loss: 0.30320000648498535
Step: 30340, train/grad_norm: 19.311342239379883
Step: 30340, train/learning_rate: 1.389814406138612e-05
Step: 30340, train/epoch: 7.220371246337891
Step: 30350, train/loss: 0.21119999885559082
Step: 30350, train/grad_norm: 3.2158849239349365
Step: 30350, train/learning_rate: 1.3886244232708123e-05
Step: 30350, train/epoch: 7.222751140594482
Step: 30360, train/loss: 0.31949999928474426
Step: 30360, train/grad_norm: 3.0951409339904785
Step: 30360, train/learning_rate: 1.3874345313524827e-05
Step: 30360, train/epoch: 7.225131034851074
Step: 30370, train/loss: 0.28630000352859497
Step: 30370, train/grad_norm: 8.8385648727417
Step: 30370, train/learning_rate: 1.3862446394341532e-05
Step: 30370, train/epoch: 7.227510929107666
Step: 30380, train/loss: 0.3621000051498413
Step: 30380, train/grad_norm: 11.0630464553833
Step: 30380, train/learning_rate: 1.3850547475158237e-05
Step: 30380, train/epoch: 7.2298903465271
Step: 30390, train/loss: 0.41510000824928284
Step: 30390, train/grad_norm: 13.249980926513672
Step: 30390, train/learning_rate: 1.3838648555974942e-05
Step: 30390, train/epoch: 7.232270240783691
Step: 30400, train/loss: 0.3903000056743622
Step: 30400, train/grad_norm: 3.0988612174987793
Step: 30400, train/learning_rate: 1.3826748727296945e-05
Step: 30400, train/epoch: 7.234650135040283
Step: 30410, train/loss: 0.22930000722408295
Step: 30410, train/grad_norm: 8.076836585998535
Step: 30410, train/learning_rate: 1.381484980811365e-05
Step: 30410, train/epoch: 7.237030029296875
Step: 30420, train/loss: 0.3418999910354614
Step: 30420, train/grad_norm: 23.237321853637695
Step: 30420, train/learning_rate: 1.3802950888930354e-05
Step: 30420, train/epoch: 7.239409923553467
Step: 30430, train/loss: 0.4140999913215637
Step: 30430, train/grad_norm: 9.066717147827148
Step: 30430, train/learning_rate: 1.3791051969747059e-05
Step: 30430, train/epoch: 7.241789817810059
Step: 30440, train/loss: 0.33180001378059387
Step: 30440, train/grad_norm: 22.089134216308594
Step: 30440, train/learning_rate: 1.3779153050563764e-05
Step: 30440, train/epoch: 7.244169235229492
Step: 30450, train/loss: 0.4458000063896179
Step: 30450, train/grad_norm: 11.66259479522705
Step: 30450, train/learning_rate: 1.3767254131380469e-05
Step: 30450, train/epoch: 7.246549129486084
Step: 30460, train/loss: 0.2992999851703644
Step: 30460, train/grad_norm: 20.446407318115234
Step: 30460, train/learning_rate: 1.3755354302702472e-05
Step: 30460, train/epoch: 7.248929023742676
Step: 30470, train/loss: 0.25600001215934753
Step: 30470, train/grad_norm: 10.700706481933594
Step: 30470, train/learning_rate: 1.3743455383519176e-05
Step: 30470, train/epoch: 7.251308917999268
Step: 30480, train/loss: 0.42010000348091125
Step: 30480, train/grad_norm: 8.795104026794434
Step: 30480, train/learning_rate: 1.3731556464335881e-05
Step: 30480, train/epoch: 7.253688812255859
Step: 30490, train/loss: 0.3190999925136566
Step: 30490, train/grad_norm: 8.415974617004395
Step: 30490, train/learning_rate: 1.3719657545152586e-05
Step: 30490, train/epoch: 7.256068706512451
Step: 30500, train/loss: 0.31949999928474426
Step: 30500, train/grad_norm: 18.578786849975586
Step: 30500, train/learning_rate: 1.370775862596929e-05
Step: 30500, train/epoch: 7.258448123931885
Step: 30510, train/loss: 0.3619999885559082
Step: 30510, train/grad_norm: 8.185248374938965
Step: 30510, train/learning_rate: 1.3695858797291294e-05
Step: 30510, train/epoch: 7.260828018188477
Step: 30520, train/loss: 0.3391999900341034
Step: 30520, train/grad_norm: 5.375857353210449
Step: 30520, train/learning_rate: 1.3683959878107999e-05
Step: 30520, train/epoch: 7.263207912445068
Step: 30530, train/loss: 0.24230000376701355
Step: 30530, train/grad_norm: 10.29101276397705
Step: 30530, train/learning_rate: 1.3672060958924703e-05
Step: 30530, train/epoch: 7.26558780670166
Step: 30540, train/loss: 0.2513999938964844
Step: 30540, train/grad_norm: 5.249517917633057
Step: 30540, train/learning_rate: 1.3660162039741408e-05
Step: 30540, train/epoch: 7.267967700958252
Step: 30550, train/loss: 0.49810001254081726
Step: 30550, train/grad_norm: 23.257877349853516
Step: 30550, train/learning_rate: 1.3648263120558113e-05
Step: 30550, train/epoch: 7.270347595214844
Step: 30560, train/loss: 0.5206000208854675
Step: 30560, train/grad_norm: 11.23961067199707
Step: 30560, train/learning_rate: 1.3636363291880116e-05
Step: 30560, train/epoch: 7.2727274894714355
Step: 30570, train/loss: 0.3384000062942505
Step: 30570, train/grad_norm: 13.125547409057617
Step: 30570, train/learning_rate: 1.362446437269682e-05
Step: 30570, train/epoch: 7.275106906890869
Step: 30580, train/loss: 0.46650001406669617
Step: 30580, train/grad_norm: 13.592365264892578
Step: 30580, train/learning_rate: 1.3612565453513525e-05
Step: 30580, train/epoch: 7.277486801147461
Step: 30590, train/loss: 0.3463999927043915
Step: 30590, train/grad_norm: 34.443878173828125
Step: 30590, train/learning_rate: 1.360066653433023e-05
Step: 30590, train/epoch: 7.279866695404053
Step: 30600, train/loss: 0.4205000102519989
Step: 30600, train/grad_norm: 11.156367301940918
Step: 30600, train/learning_rate: 1.3588767615146935e-05
Step: 30600, train/epoch: 7.2822465896606445
Step: 30610, train/loss: 0.3384000062942505
Step: 30610, train/grad_norm: 8.791533470153809
Step: 30610, train/learning_rate: 1.3576867786468938e-05
Step: 30610, train/epoch: 7.284626483917236
Step: 30620, train/loss: 0.4260999858379364
Step: 30620, train/grad_norm: 27.95223617553711
Step: 30620, train/learning_rate: 1.3564968867285643e-05
Step: 30620, train/epoch: 7.287006378173828
Step: 30630, train/loss: 0.384799987077713
Step: 30630, train/grad_norm: 17.466867446899414
Step: 30630, train/learning_rate: 1.3553069948102348e-05
Step: 30630, train/epoch: 7.289385795593262
Step: 30640, train/loss: 0.2370000034570694
Step: 30640, train/grad_norm: 4.216612815856934
Step: 30640, train/learning_rate: 1.3541171028919052e-05
Step: 30640, train/epoch: 7.2917656898498535
Step: 30650, train/loss: 0.3619000017642975
Step: 30650, train/grad_norm: 14.885679244995117
Step: 30650, train/learning_rate: 1.3529272109735757e-05
Step: 30650, train/epoch: 7.294145584106445
Step: 30660, train/loss: 0.28349998593330383
Step: 30660, train/grad_norm: 16.850770950317383
Step: 30660, train/learning_rate: 1.351737228105776e-05
Step: 30660, train/epoch: 7.296525478363037
Step: 30670, train/loss: 0.2768999934196472
Step: 30670, train/grad_norm: 10.613822937011719
Step: 30670, train/learning_rate: 1.3505473361874465e-05
Step: 30670, train/epoch: 7.298905372619629
Step: 30680, train/loss: 0.3215000033378601
Step: 30680, train/grad_norm: 2.828763008117676
Step: 30680, train/learning_rate: 1.349357444269117e-05
Step: 30680, train/epoch: 7.301285266876221
Step: 30690, train/loss: 0.30570000410079956
Step: 30690, train/grad_norm: 2.966989040374756
Step: 30690, train/learning_rate: 1.3481675523507874e-05
Step: 30690, train/epoch: 7.303664684295654
Step: 30700, train/loss: 0.2992999851703644
Step: 30700, train/grad_norm: 5.9971022605896
Step: 30700, train/learning_rate: 1.346977660432458e-05
Step: 30700, train/epoch: 7.306044578552246
Step: 30710, train/loss: 0.20229999721050262
Step: 30710, train/grad_norm: 16.111419677734375
Step: 30710, train/learning_rate: 1.3457876775646582e-05
Step: 30710, train/epoch: 7.308424472808838
Step: 30720, train/loss: 0.3779999911785126
Step: 30720, train/grad_norm: 17.439332962036133
Step: 30720, train/learning_rate: 1.3445977856463287e-05
Step: 30720, train/epoch: 7.31080436706543
Step: 30730, train/loss: 0.3197999894618988
Step: 30730, train/grad_norm: 20.28526496887207
Step: 30730, train/learning_rate: 1.3434078937279992e-05
Step: 30730, train/epoch: 7.3131842613220215
Step: 30740, train/loss: 0.5329999923706055
Step: 30740, train/grad_norm: 6.099273681640625
Step: 30740, train/learning_rate: 1.3422180018096697e-05
Step: 30740, train/epoch: 7.315564155578613
Step: 30750, train/loss: 0.4327000081539154
Step: 30750, train/grad_norm: 17.090974807739258
Step: 30750, train/learning_rate: 1.3410281098913401e-05
Step: 30750, train/epoch: 7.317944049835205
Step: 30760, train/loss: 0.42010000348091125
Step: 30760, train/grad_norm: 19.726089477539062
Step: 30760, train/learning_rate: 1.3398381270235404e-05
Step: 30760, train/epoch: 7.320323467254639
Step: 30770, train/loss: 0.3239000141620636
Step: 30770, train/grad_norm: 9.48266315460205
Step: 30770, train/learning_rate: 1.3386482351052109e-05
Step: 30770, train/epoch: 7.3227033615112305
Step: 30780, train/loss: 0.3601999878883362
Step: 30780, train/grad_norm: 37.486122131347656
Step: 30780, train/learning_rate: 1.3374583431868814e-05
Step: 30780, train/epoch: 7.325083255767822
Step: 30790, train/loss: 0.37549999356269836
Step: 30790, train/grad_norm: 17.64773941040039
Step: 30790, train/learning_rate: 1.3362684512685519e-05
Step: 30790, train/epoch: 7.327463150024414
Step: 30800, train/loss: 0.3797000050544739
Step: 30800, train/grad_norm: 27.4337158203125
Step: 30800, train/learning_rate: 1.3350785593502223e-05
Step: 30800, train/epoch: 7.329843044281006
Step: 30810, train/loss: 0.391400009393692
Step: 30810, train/grad_norm: 9.268851280212402
Step: 30810, train/learning_rate: 1.3338886674318928e-05
Step: 30810, train/epoch: 7.332222938537598
Step: 30820, train/loss: 0.4325999915599823
Step: 30820, train/grad_norm: 5.3562140464782715
Step: 30820, train/learning_rate: 1.3326986845640931e-05
Step: 30820, train/epoch: 7.334602355957031
Step: 30830, train/loss: 0.40070000290870667
Step: 30830, train/grad_norm: 23.658300399780273
Step: 30830, train/learning_rate: 1.3315087926457636e-05
Step: 30830, train/epoch: 7.336982250213623
Step: 30840, train/loss: 0.3522999882698059
Step: 30840, train/grad_norm: 13.428231239318848
Step: 30840, train/learning_rate: 1.330318900727434e-05
Step: 30840, train/epoch: 7.339362144470215
Step: 30850, train/loss: 0.28189998865127563
Step: 30850, train/grad_norm: 5.978348255157471
Step: 30850, train/learning_rate: 1.3291290088091046e-05
Step: 30850, train/epoch: 7.341742038726807
Step: 30860, train/loss: 0.29499998688697815
Step: 30860, train/grad_norm: 16.018901824951172
Step: 30860, train/learning_rate: 1.327939116890775e-05
Step: 30860, train/epoch: 7.344121932983398
Step: 30870, train/loss: 0.4092000126838684
Step: 30870, train/grad_norm: 16.030794143676758
Step: 30870, train/learning_rate: 1.3267491340229753e-05
Step: 30870, train/epoch: 7.34650182723999
Step: 30880, train/loss: 0.3935000002384186
Step: 30880, train/grad_norm: 26.966861724853516
Step: 30880, train/learning_rate: 1.3255592421046458e-05
Step: 30880, train/epoch: 7.348881721496582
Step: 30890, train/loss: 0.2831999957561493
Step: 30890, train/grad_norm: 13.992655754089355
Step: 30890, train/learning_rate: 1.3243693501863163e-05
Step: 30890, train/epoch: 7.351261138916016
Step: 30900, train/loss: 0.28040000796318054
Step: 30900, train/grad_norm: 2.3026697635650635
Step: 30900, train/learning_rate: 1.3231794582679868e-05
Step: 30900, train/epoch: 7.353641033172607
Step: 30910, train/loss: 0.32519999146461487
Step: 30910, train/grad_norm: 11.287376403808594
Step: 30910, train/learning_rate: 1.3219895663496573e-05
Step: 30910, train/epoch: 7.356020927429199
Step: 30920, train/loss: 0.3968000113964081
Step: 30920, train/grad_norm: 20.657602310180664
Step: 30920, train/learning_rate: 1.3207995834818576e-05
Step: 30920, train/epoch: 7.358400821685791
Step: 30930, train/loss: 0.47909998893737793
Step: 30930, train/grad_norm: 7.446686744689941
Step: 30930, train/learning_rate: 1.319609691563528e-05
Step: 30930, train/epoch: 7.360780715942383
Step: 30940, train/loss: 0.3544999957084656
Step: 30940, train/grad_norm: 8.673460960388184
Step: 30940, train/learning_rate: 1.3184197996451985e-05
Step: 30940, train/epoch: 7.363160610198975
Step: 30950, train/loss: 0.2912999987602234
Step: 30950, train/grad_norm: 13.76186752319336
Step: 30950, train/learning_rate: 1.317229907726869e-05
Step: 30950, train/epoch: 7.365540027618408
Step: 30960, train/loss: 0.2734000086784363
Step: 30960, train/grad_norm: 5.814863681793213
Step: 30960, train/learning_rate: 1.3160400158085395e-05
Step: 30960, train/epoch: 7.367919921875
Step: 30970, train/loss: 0.259799987077713
Step: 30970, train/grad_norm: 2.9763667583465576
Step: 30970, train/learning_rate: 1.3148500329407398e-05
Step: 30970, train/epoch: 7.370299816131592
Step: 30980, train/loss: 0.26820001006126404
Step: 30980, train/grad_norm: 6.513430118560791
Step: 30980, train/learning_rate: 1.3136601410224102e-05
Step: 30980, train/epoch: 7.372679710388184
Step: 30990, train/loss: 0.29490000009536743
Step: 30990, train/grad_norm: 13.313425064086914
Step: 30990, train/learning_rate: 1.3124702491040807e-05
Step: 30990, train/epoch: 7.375059604644775
Step: 31000, train/loss: 0.35899999737739563
Step: 31000, train/grad_norm: 5.956540584564209
Step: 31000, train/learning_rate: 1.3112803571857512e-05
Step: 31000, train/epoch: 7.377439498901367
Step: 31010, train/loss: 0.2842000126838684
Step: 31010, train/grad_norm: 9.850385665893555
Step: 31010, train/learning_rate: 1.3100904652674217e-05
Step: 31010, train/epoch: 7.379818916320801
Step: 31020, train/loss: 0.42910000681877136
Step: 31020, train/grad_norm: 29.960481643676758
Step: 31020, train/learning_rate: 1.308900482399622e-05
Step: 31020, train/epoch: 7.382198810577393
Step: 31030, train/loss: 0.39649999141693115
Step: 31030, train/grad_norm: 8.537368774414062
Step: 31030, train/learning_rate: 1.3077105904812925e-05
Step: 31030, train/epoch: 7.384578704833984
Step: 31040, train/loss: 0.3540000021457672
Step: 31040, train/grad_norm: 7.782626152038574
Step: 31040, train/learning_rate: 1.306520698562963e-05
Step: 31040, train/epoch: 7.386958599090576
Step: 31050, train/loss: 0.2705000042915344
Step: 31050, train/grad_norm: 17.289880752563477
Step: 31050, train/learning_rate: 1.3053308066446334e-05
Step: 31050, train/epoch: 7.389338493347168
Step: 31060, train/loss: 0.35569998621940613
Step: 31060, train/grad_norm: 14.421513557434082
Step: 31060, train/learning_rate: 1.3041409147263039e-05
Step: 31060, train/epoch: 7.39171838760376
Step: 31070, train/loss: 0.3546000123023987
Step: 31070, train/grad_norm: 12.906527519226074
Step: 31070, train/learning_rate: 1.3029509318585042e-05
Step: 31070, train/epoch: 7.394098281860352
Step: 31080, train/loss: 0.38580000400543213
Step: 31080, train/grad_norm: 5.565446853637695
Step: 31080, train/learning_rate: 1.3017610399401747e-05
Step: 31080, train/epoch: 7.396477699279785
Step: 31090, train/loss: 0.3662000000476837
Step: 31090, train/grad_norm: 4.654013633728027
Step: 31090, train/learning_rate: 1.3005711480218451e-05
Step: 31090, train/epoch: 7.398857593536377
Step: 31100, train/loss: 0.37389999628067017
Step: 31100, train/grad_norm: 5.6369853019714355
Step: 31100, train/learning_rate: 1.2993812561035156e-05
Step: 31100, train/epoch: 7.401237487792969
Step: 31110, train/loss: 0.43779999017715454
Step: 31110, train/grad_norm: 7.388874053955078
Step: 31110, train/learning_rate: 1.2981913641851861e-05
Step: 31110, train/epoch: 7.4036173820495605
Step: 31120, train/loss: 0.23839999735355377
Step: 31120, train/grad_norm: 27.380125045776367
Step: 31120, train/learning_rate: 1.2970014722668566e-05
Step: 31120, train/epoch: 7.405997276306152
Step: 31130, train/loss: 0.3312999904155731
Step: 31130, train/grad_norm: 37.02980422973633
Step: 31130, train/learning_rate: 1.2958114893990569e-05
Step: 31130, train/epoch: 7.408377170562744
Step: 31140, train/loss: 0.24570000171661377
Step: 31140, train/grad_norm: 0.6619207859039307
Step: 31140, train/learning_rate: 1.2946215974807274e-05
Step: 31140, train/epoch: 7.410756587982178
Step: 31150, train/loss: 0.38420000672340393
Step: 31150, train/grad_norm: 9.105524063110352
Step: 31150, train/learning_rate: 1.2934317055623978e-05
Step: 31150, train/epoch: 7.4131364822387695
Step: 31160, train/loss: 0.33469998836517334
Step: 31160, train/grad_norm: 10.899985313415527
Step: 31160, train/learning_rate: 1.2922418136440683e-05
Step: 31160, train/epoch: 7.415516376495361
Step: 31170, train/loss: 0.43389999866485596
Step: 31170, train/grad_norm: 16.89402961730957
Step: 31170, train/learning_rate: 1.2910519217257388e-05
Step: 31170, train/epoch: 7.417896270751953
Step: 31180, train/loss: 0.12460000067949295
Step: 31180, train/grad_norm: 11.215802192687988
Step: 31180, train/learning_rate: 1.2898619388579391e-05
Step: 31180, train/epoch: 7.420276165008545
Step: 31190, train/loss: 0.29789999127388
Step: 31190, train/grad_norm: 8.110529899597168
Step: 31190, train/learning_rate: 1.2886720469396096e-05
Step: 31190, train/epoch: 7.422656059265137
Step: 31200, train/loss: 0.3046000003814697
Step: 31200, train/grad_norm: 1.496600866317749
Step: 31200, train/learning_rate: 1.28748215502128e-05
Step: 31200, train/epoch: 7.42503547668457
Step: 31210, train/loss: 0.27570000290870667
Step: 31210, train/grad_norm: 27.537723541259766
Step: 31210, train/learning_rate: 1.2862922631029505e-05
Step: 31210, train/epoch: 7.427415370941162
Step: 31220, train/loss: 0.257999986410141
Step: 31220, train/grad_norm: 7.996693134307861
Step: 31220, train/learning_rate: 1.285102371184621e-05
Step: 31220, train/epoch: 7.429795265197754
Step: 31230, train/loss: 0.32829999923706055
Step: 31230, train/grad_norm: 17.504812240600586
Step: 31230, train/learning_rate: 1.2839123883168213e-05
Step: 31230, train/epoch: 7.432175159454346
Step: 31240, train/loss: 0.3741999864578247
Step: 31240, train/grad_norm: 13.090788841247559
Step: 31240, train/learning_rate: 1.2827224963984918e-05
Step: 31240, train/epoch: 7.4345550537109375
Step: 31250, train/loss: 0.4778999984264374
Step: 31250, train/grad_norm: 15.679059028625488
Step: 31250, train/learning_rate: 1.2815326044801623e-05
Step: 31250, train/epoch: 7.436934947967529
Step: 31260, train/loss: 0.44690001010894775
Step: 31260, train/grad_norm: 27.15497589111328
Step: 31260, train/learning_rate: 1.2803427125618327e-05
Step: 31260, train/epoch: 7.439314842224121
Step: 31270, train/loss: 0.2565999925136566
Step: 31270, train/grad_norm: 11.654205322265625
Step: 31270, train/learning_rate: 1.2791528206435032e-05
Step: 31270, train/epoch: 7.441694259643555
Step: 31280, train/loss: 0.39399999380111694
Step: 31280, train/grad_norm: 14.141888618469238
Step: 31280, train/learning_rate: 1.2779628377757035e-05
Step: 31280, train/epoch: 7.4440741539001465
Step: 31290, train/loss: 0.5579000115394592
Step: 31290, train/grad_norm: 12.940410614013672
Step: 31290, train/learning_rate: 1.276772945857374e-05
Step: 31290, train/epoch: 7.446454048156738
Step: 31300, train/loss: 0.28290000557899475
Step: 31300, train/grad_norm: 4.1553778648376465
Step: 31300, train/learning_rate: 1.2755830539390445e-05
Step: 31300, train/epoch: 7.44883394241333
Step: 31310, train/loss: 0.4027999937534332
Step: 31310, train/grad_norm: 8.128569602966309
Step: 31310, train/learning_rate: 1.274393162020715e-05
Step: 31310, train/epoch: 7.451213836669922
Step: 31320, train/loss: 0.25110000371932983
Step: 31320, train/grad_norm: 20.990671157836914
Step: 31320, train/learning_rate: 1.2732032701023854e-05
Step: 31320, train/epoch: 7.453593730926514
Step: 31330, train/loss: 0.3377000093460083
Step: 31330, train/grad_norm: 14.232926368713379
Step: 31330, train/learning_rate: 1.2720132872345857e-05
Step: 31330, train/epoch: 7.455973148345947
Step: 31340, train/loss: 0.326200008392334
Step: 31340, train/grad_norm: 11.146095275878906
Step: 31340, train/learning_rate: 1.2708233953162562e-05
Step: 31340, train/epoch: 7.458353042602539
Step: 31350, train/loss: 0.4675999879837036
Step: 31350, train/grad_norm: 10.54304313659668
Step: 31350, train/learning_rate: 1.2696335033979267e-05
Step: 31350, train/epoch: 7.460732936859131
Step: 31360, train/loss: 0.37049999833106995
Step: 31360, train/grad_norm: 14.171472549438477
Step: 31360, train/learning_rate: 1.2684436114795972e-05
Step: 31360, train/epoch: 7.463112831115723
Step: 31370, train/loss: 0.2858000099658966
Step: 31370, train/grad_norm: 29.781835556030273
Step: 31370, train/learning_rate: 1.2672537195612676e-05
Step: 31370, train/epoch: 7.4654927253723145
Step: 31380, train/loss: 0.4503999948501587
Step: 31380, train/grad_norm: 7.351638317108154
Step: 31380, train/learning_rate: 1.266063736693468e-05
Step: 31380, train/epoch: 7.467872619628906
Step: 31390, train/loss: 0.2513999938964844
Step: 31390, train/grad_norm: 22.5280704498291
Step: 31390, train/learning_rate: 1.2648738447751384e-05
Step: 31390, train/epoch: 7.47025203704834
Step: 31400, train/loss: 0.23119999468326569
Step: 31400, train/grad_norm: 6.780226707458496
Step: 31400, train/learning_rate: 1.2636839528568089e-05
Step: 31400, train/epoch: 7.472631931304932
Step: 31410, train/loss: 0.1906999945640564
Step: 31410, train/grad_norm: 23.500507354736328
Step: 31410, train/learning_rate: 1.2624940609384794e-05
Step: 31410, train/epoch: 7.475011825561523
Step: 31420, train/loss: 0.5238000154495239
Step: 31420, train/grad_norm: 9.80027961730957
Step: 31420, train/learning_rate: 1.2613041690201499e-05
Step: 31420, train/epoch: 7.477391719818115
Step: 31430, train/loss: 0.30149999260902405
Step: 31430, train/grad_norm: 34.2678337097168
Step: 31430, train/learning_rate: 1.2601141861523502e-05
Step: 31430, train/epoch: 7.479771614074707
Step: 31440, train/loss: 0.3230000138282776
Step: 31440, train/grad_norm: 8.186891555786133
Step: 31440, train/learning_rate: 1.2589242942340206e-05
Step: 31440, train/epoch: 7.482151508331299
Step: 31450, train/loss: 0.3765999972820282
Step: 31450, train/grad_norm: 17.031057357788086
Step: 31450, train/learning_rate: 1.2577344023156911e-05
Step: 31450, train/epoch: 7.484531402587891
Step: 31460, train/loss: 0.42010000348091125
Step: 31460, train/grad_norm: 28.82626724243164
Step: 31460, train/learning_rate: 1.2565445103973616e-05
Step: 31460, train/epoch: 7.486910820007324
Step: 31470, train/loss: 0.3149000108242035
Step: 31470, train/grad_norm: 19.763782501220703
Step: 31470, train/learning_rate: 1.255354618479032e-05
Step: 31470, train/epoch: 7.489290714263916
Step: 31480, train/loss: 0.29269999265670776
Step: 31480, train/grad_norm: 17.071582794189453
Step: 31480, train/learning_rate: 1.2541647265607025e-05
Step: 31480, train/epoch: 7.491670608520508
Step: 31490, train/loss: 0.30140000581741333
Step: 31490, train/grad_norm: 5.928173065185547
Step: 31490, train/learning_rate: 1.2529747436929028e-05
Step: 31490, train/epoch: 7.4940505027771
Step: 31500, train/loss: 0.30720001459121704
Step: 31500, train/grad_norm: 25.968154907226562
Step: 31500, train/learning_rate: 1.2517848517745733e-05
Step: 31500, train/epoch: 7.496430397033691
Step: 31510, train/loss: 0.3050000071525574
Step: 31510, train/grad_norm: 2.8466787338256836
Step: 31510, train/learning_rate: 1.2505949598562438e-05
Step: 31510, train/epoch: 7.498810291290283
Step: 31520, train/loss: 0.5177000164985657
Step: 31520, train/grad_norm: 18.222545623779297
Step: 31520, train/learning_rate: 1.2494050679379143e-05
Step: 31520, train/epoch: 7.501189708709717
Step: 31530, train/loss: 0.29179999232292175
Step: 31530, train/grad_norm: 17.76852035522461
Step: 31530, train/learning_rate: 1.2482151760195848e-05
Step: 31530, train/epoch: 7.503569602966309
Step: 31540, train/loss: 0.5110999941825867
Step: 31540, train/grad_norm: 17.351099014282227
Step: 31540, train/learning_rate: 1.247025193151785e-05
Step: 31540, train/epoch: 7.5059494972229
Step: 31550, train/loss: 0.2847999930381775
Step: 31550, train/grad_norm: 3.333540201187134
Step: 31550, train/learning_rate: 1.2458353012334555e-05
Step: 31550, train/epoch: 7.508329391479492
Step: 31560, train/loss: 0.3831000030040741
Step: 31560, train/grad_norm: 4.407971382141113
Step: 31560, train/learning_rate: 1.244645409315126e-05
Step: 31560, train/epoch: 7.510709285736084
Step: 31570, train/loss: 0.503600001335144
Step: 31570, train/grad_norm: 19.622224807739258
Step: 31570, train/learning_rate: 1.2434555173967965e-05
Step: 31570, train/epoch: 7.513089179992676
Step: 31580, train/loss: 0.272599995136261
Step: 31580, train/grad_norm: 5.002312660217285
Step: 31580, train/learning_rate: 1.242265625478467e-05
Step: 31580, train/epoch: 7.515468597412109
Step: 31590, train/loss: 0.511900007724762
Step: 31590, train/grad_norm: 14.699066162109375
Step: 31590, train/learning_rate: 1.2410756426106673e-05
Step: 31590, train/epoch: 7.517848491668701
Step: 31600, train/loss: 0.23680000007152557
Step: 31600, train/grad_norm: 8.055032730102539
Step: 31600, train/learning_rate: 1.2398857506923378e-05
Step: 31600, train/epoch: 7.520228385925293
Step: 31610, train/loss: 0.44690001010894775
Step: 31610, train/grad_norm: 17.18058204650879
Step: 31610, train/learning_rate: 1.2386958587740082e-05
Step: 31610, train/epoch: 7.522608280181885
Step: 31620, train/loss: 0.3628999888896942
Step: 31620, train/grad_norm: 6.351325035095215
Step: 31620, train/learning_rate: 1.2375059668556787e-05
Step: 31620, train/epoch: 7.524988174438477
Step: 31630, train/loss: 0.34689998626708984
Step: 31630, train/grad_norm: 17.50476837158203
Step: 31630, train/learning_rate: 1.2363160749373492e-05
Step: 31630, train/epoch: 7.527368068695068
Step: 31640, train/loss: 0.3765000104904175
Step: 31640, train/grad_norm: 6.410261154174805
Step: 31640, train/learning_rate: 1.2351260920695495e-05
Step: 31640, train/epoch: 7.52974796295166
Step: 31650, train/loss: 0.2953000068664551
Step: 31650, train/grad_norm: 22.571529388427734
Step: 31650, train/learning_rate: 1.23393620015122e-05
Step: 31650, train/epoch: 7.532127380371094
Step: 31660, train/loss: 0.39410001039505005
Step: 31660, train/grad_norm: 10.853099822998047
Step: 31660, train/learning_rate: 1.2327463082328904e-05
Step: 31660, train/epoch: 7.5345072746276855
Step: 31670, train/loss: 0.28290000557899475
Step: 31670, train/grad_norm: 5.6760029792785645
Step: 31670, train/learning_rate: 1.231556416314561e-05
Step: 31670, train/epoch: 7.536887168884277
Step: 31680, train/loss: 0.4625000059604645
Step: 31680, train/grad_norm: 20.76551055908203
Step: 31680, train/learning_rate: 1.2303665243962314e-05
Step: 31680, train/epoch: 7.539267063140869
Step: 31690, train/loss: 0.3725999891757965
Step: 31690, train/grad_norm: 6.245303630828857
Step: 31690, train/learning_rate: 1.2291765415284317e-05
Step: 31690, train/epoch: 7.541646957397461
Step: 31700, train/loss: 0.3598000109195709
Step: 31700, train/grad_norm: 19.19312858581543
Step: 31700, train/learning_rate: 1.2279866496101022e-05
Step: 31700, train/epoch: 7.544026851654053
Step: 31710, train/loss: 0.2815999984741211
Step: 31710, train/grad_norm: 2.941673517227173
Step: 31710, train/learning_rate: 1.2267967576917727e-05
Step: 31710, train/epoch: 7.546406269073486
Step: 31720, train/loss: 0.4731999933719635
Step: 31720, train/grad_norm: 2.6986889839172363
Step: 31720, train/learning_rate: 1.2256068657734431e-05
Step: 31720, train/epoch: 7.548786163330078
Step: 31730, train/loss: 0.24799999594688416
Step: 31730, train/grad_norm: 0.7351868748664856
Step: 31730, train/learning_rate: 1.2244169738551136e-05
Step: 31730, train/epoch: 7.55116605758667
Step: 31740, train/loss: 0.3384999930858612
Step: 31740, train/grad_norm: 13.161428451538086
Step: 31740, train/learning_rate: 1.2232269909873139e-05
Step: 31740, train/epoch: 7.553545951843262
Step: 31750, train/loss: 0.259799987077713
Step: 31750, train/grad_norm: 13.885260581970215
Step: 31750, train/learning_rate: 1.2220370990689844e-05
Step: 31750, train/epoch: 7.5559258460998535
Step: 31760, train/loss: 0.20239999890327454
Step: 31760, train/grad_norm: 7.354428291320801
Step: 31760, train/learning_rate: 1.2208472071506549e-05
Step: 31760, train/epoch: 7.558305740356445
Step: 31770, train/loss: 0.2409999966621399
Step: 31770, train/grad_norm: 3.9731359481811523
Step: 31770, train/learning_rate: 1.2196573152323253e-05
Step: 31770, train/epoch: 7.560685157775879
Step: 31780, train/loss: 0.2046000063419342
Step: 31780, train/grad_norm: 7.029717922210693
Step: 31780, train/learning_rate: 1.2184674233139958e-05
Step: 31780, train/epoch: 7.563065052032471
Step: 31790, train/loss: 0.3752000033855438
Step: 31790, train/grad_norm: 2.9183743000030518
Step: 31790, train/learning_rate: 1.2172775313956663e-05
Step: 31790, train/epoch: 7.5654449462890625
Step: 31800, train/loss: 0.19339999556541443
Step: 31800, train/grad_norm: 14.067092895507812
Step: 31800, train/learning_rate: 1.2160875485278666e-05
Step: 31800, train/epoch: 7.567824840545654
Step: 31810, train/loss: 0.5228999853134155
Step: 31810, train/grad_norm: 21.965938568115234
Step: 31810, train/learning_rate: 1.214897656609537e-05
Step: 31810, train/epoch: 7.570204734802246
Step: 31820, train/loss: 0.4108999967575073
Step: 31820, train/grad_norm: 7.656269550323486
Step: 31820, train/learning_rate: 1.2137077646912076e-05
Step: 31820, train/epoch: 7.572584629058838
Step: 31830, train/loss: 0.2750999927520752
Step: 31830, train/grad_norm: 3.981696844100952
Step: 31830, train/learning_rate: 1.212517872772878e-05
Step: 31830, train/epoch: 7.57496452331543
Step: 31840, train/loss: 0.37040001153945923
Step: 31840, train/grad_norm: 11.692469596862793
Step: 31840, train/learning_rate: 1.2113279808545485e-05
Step: 31840, train/epoch: 7.577343940734863
Step: 31850, train/loss: 0.3707999885082245
Step: 31850, train/grad_norm: 1.8423869609832764
Step: 31850, train/learning_rate: 1.2101379979867488e-05
Step: 31850, train/epoch: 7.579723834991455
Step: 31860, train/loss: 0.35109999775886536
Step: 31860, train/grad_norm: 6.183250427246094
Step: 31860, train/learning_rate: 1.2089481060684193e-05
Step: 31860, train/epoch: 7.582103729248047
Step: 31870, train/loss: 0.4242999851703644
Step: 31870, train/grad_norm: 6.984216690063477
Step: 31870, train/learning_rate: 1.2077582141500898e-05
Step: 31870, train/epoch: 7.584483623504639
Step: 31880, train/loss: 0.46380001306533813
Step: 31880, train/grad_norm: 16.269182205200195
Step: 31880, train/learning_rate: 1.2065683222317602e-05
Step: 31880, train/epoch: 7.5868635177612305
Step: 31890, train/loss: 0.27709999680519104
Step: 31890, train/grad_norm: 13.002291679382324
Step: 31890, train/learning_rate: 1.2053784303134307e-05
Step: 31890, train/epoch: 7.589243412017822
Step: 31900, train/loss: 0.2345000058412552
Step: 31900, train/grad_norm: 13.008302688598633
Step: 31900, train/learning_rate: 1.204188447445631e-05
Step: 31900, train/epoch: 7.591622829437256
Step: 31910, train/loss: 0.4120999872684479
Step: 31910, train/grad_norm: 16.077959060668945
Step: 31910, train/learning_rate: 1.2029985555273015e-05
Step: 31910, train/epoch: 7.594002723693848
Step: 31920, train/loss: 0.42329999804496765
Step: 31920, train/grad_norm: 16.162689208984375
Step: 31920, train/learning_rate: 1.201808663608972e-05
Step: 31920, train/epoch: 7.5963826179504395
Step: 31930, train/loss: 0.501800000667572
Step: 31930, train/grad_norm: 9.552738189697266
Step: 31930, train/learning_rate: 1.2006187716906425e-05
Step: 31930, train/epoch: 7.598762512207031
Step: 31940, train/loss: 0.23469999432563782
Step: 31940, train/grad_norm: 15.095407485961914
Step: 31940, train/learning_rate: 1.199428879772313e-05
Step: 31940, train/epoch: 7.601142406463623
Step: 31950, train/loss: 0.2639999985694885
Step: 31950, train/grad_norm: 9.824411392211914
Step: 31950, train/learning_rate: 1.1982388969045132e-05
Step: 31950, train/epoch: 7.603522300720215
Step: 31960, train/loss: 0.40950000286102295
Step: 31960, train/grad_norm: 10.027318954467773
Step: 31960, train/learning_rate: 1.1970490049861837e-05
Step: 31960, train/epoch: 7.605901718139648
Step: 31970, train/loss: 0.42899999022483826
Step: 31970, train/grad_norm: 11.412650108337402
Step: 31970, train/learning_rate: 1.1958591130678542e-05
Step: 31970, train/epoch: 7.60828161239624
Step: 31980, train/loss: 0.37220001220703125
Step: 31980, train/grad_norm: 16.373756408691406
Step: 31980, train/learning_rate: 1.1946692211495247e-05
Step: 31980, train/epoch: 7.610661506652832
Step: 31990, train/loss: 0.31859999895095825
Step: 31990, train/grad_norm: 6.55757999420166
Step: 31990, train/learning_rate: 1.1934793292311952e-05
Step: 31990, train/epoch: 7.613041400909424
Step: 32000, train/loss: 0.4000000059604645
Step: 32000, train/grad_norm: 26.587881088256836
Step: 32000, train/learning_rate: 1.1922893463633955e-05
Step: 32000, train/epoch: 7.615421295166016
Step: 32010, train/loss: 0.2282000035047531
Step: 32010, train/grad_norm: 4.052751541137695
Step: 32010, train/learning_rate: 1.191099454445066e-05
Step: 32010, train/epoch: 7.617801189422607
Step: 32020, train/loss: 0.3107999861240387
Step: 32020, train/grad_norm: 10.28942584991455
Step: 32020, train/learning_rate: 1.1899095625267364e-05
Step: 32020, train/epoch: 7.620181083679199
Step: 32030, train/loss: 0.4749000072479248
Step: 32030, train/grad_norm: 10.909923553466797
Step: 32030, train/learning_rate: 1.1887196706084069e-05
Step: 32030, train/epoch: 7.622560501098633
Step: 32040, train/loss: 0.32600000500679016
Step: 32040, train/grad_norm: 21.44271469116211
Step: 32040, train/learning_rate: 1.1875297786900774e-05
Step: 32040, train/epoch: 7.624940395355225
Step: 32050, train/loss: 0.3174999952316284
Step: 32050, train/grad_norm: 27.852458953857422
Step: 32050, train/learning_rate: 1.1863397958222777e-05
Step: 32050, train/epoch: 7.627320289611816
Step: 32060, train/loss: 0.33980000019073486
Step: 32060, train/grad_norm: 5.766937732696533
Step: 32060, train/learning_rate: 1.1851499039039481e-05
Step: 32060, train/epoch: 7.629700183868408
Step: 32070, train/loss: 0.2614000141620636
Step: 32070, train/grad_norm: 21.394649505615234
Step: 32070, train/learning_rate: 1.1839600119856186e-05
Step: 32070, train/epoch: 7.632080078125
Step: 32080, train/loss: 0.44510000944137573
Step: 32080, train/grad_norm: 6.867847919464111
Step: 32080, train/learning_rate: 1.1827701200672891e-05
Step: 32080, train/epoch: 7.634459972381592
Step: 32090, train/loss: 0.2906999886035919
Step: 32090, train/grad_norm: 17.45758056640625
Step: 32090, train/learning_rate: 1.1815802281489596e-05
Step: 32090, train/epoch: 7.636839389801025
Step: 32100, train/loss: 0.3693999946117401
Step: 32100, train/grad_norm: 4.401742935180664
Step: 32100, train/learning_rate: 1.1803902452811599e-05
Step: 32100, train/epoch: 7.639219284057617
Step: 32110, train/loss: 0.33550000190734863
Step: 32110, train/grad_norm: 20.017114639282227
Step: 32110, train/learning_rate: 1.1792003533628304e-05
Step: 32110, train/epoch: 7.641599178314209
Step: 32120, train/loss: 0.42640000581741333
Step: 32120, train/grad_norm: 18.441219329833984
Step: 32120, train/learning_rate: 1.1780104614445008e-05
Step: 32120, train/epoch: 7.643979072570801
Step: 32130, train/loss: 0.5001999735832214
Step: 32130, train/grad_norm: 23.869884490966797
Step: 32130, train/learning_rate: 1.1768205695261713e-05
Step: 32130, train/epoch: 7.646358966827393
Step: 32140, train/loss: 0.366100013256073
Step: 32140, train/grad_norm: 15.584854125976562
Step: 32140, train/learning_rate: 1.1756306776078418e-05
Step: 32140, train/epoch: 7.648738861083984
Step: 32150, train/loss: 0.40939998626708984
Step: 32150, train/grad_norm: 8.653271675109863
Step: 32150, train/learning_rate: 1.1744407856895123e-05
Step: 32150, train/epoch: 7.651118278503418
Step: 32160, train/loss: 0.6942999958992004
Step: 32160, train/grad_norm: 22.466358184814453
Step: 32160, train/learning_rate: 1.1732508028217126e-05
Step: 32160, train/epoch: 7.65349817276001
Step: 32170, train/loss: 0.275299996137619
Step: 32170, train/grad_norm: 7.900738716125488
Step: 32170, train/learning_rate: 1.172060910903383e-05
Step: 32170, train/epoch: 7.655878067016602
Step: 32180, train/loss: 0.2623000144958496
Step: 32180, train/grad_norm: 8.11296558380127
Step: 32180, train/learning_rate: 1.1708710189850535e-05
Step: 32180, train/epoch: 7.658257961273193
Step: 32190, train/loss: 0.3278000056743622
Step: 32190, train/grad_norm: 28.72538948059082
Step: 32190, train/learning_rate: 1.169681127066724e-05
Step: 32190, train/epoch: 7.660637855529785
Step: 32200, train/loss: 0.3086000084877014
Step: 32200, train/grad_norm: 9.54637622833252
Step: 32200, train/learning_rate: 1.1684912351483945e-05
Step: 32200, train/epoch: 7.663017749786377
Step: 32210, train/loss: 0.5390999913215637
Step: 32210, train/grad_norm: 42.676231384277344
Step: 32210, train/learning_rate: 1.1673012522805948e-05
Step: 32210, train/epoch: 7.665397644042969
Step: 32220, train/loss: 0.38339999318122864
Step: 32220, train/grad_norm: 7.890002727508545
Step: 32220, train/learning_rate: 1.1661113603622653e-05
Step: 32220, train/epoch: 7.667777061462402
Step: 32230, train/loss: 0.483599990606308
Step: 32230, train/grad_norm: 5.474245071411133
Step: 32230, train/learning_rate: 1.1649214684439357e-05
Step: 32230, train/epoch: 7.670156955718994
Step: 32240, train/loss: 0.4083000123500824
Step: 32240, train/grad_norm: 11.684122085571289
Step: 32240, train/learning_rate: 1.1637315765256062e-05
Step: 32240, train/epoch: 7.672536849975586
Step: 32250, train/loss: 0.326200008392334
Step: 32250, train/grad_norm: 7.137272834777832
Step: 32250, train/learning_rate: 1.1625416846072767e-05
Step: 32250, train/epoch: 7.674916744232178
Step: 32260, train/loss: 0.5083000063896179
Step: 32260, train/grad_norm: 20.75348472595215
Step: 32260, train/learning_rate: 1.161351701739477e-05
Step: 32260, train/epoch: 7.6772966384887695
Step: 32270, train/loss: 0.26269999146461487
Step: 32270, train/grad_norm: 6.520843029022217
Step: 32270, train/learning_rate: 1.1601618098211475e-05
Step: 32270, train/epoch: 7.679676532745361
Step: 32280, train/loss: 0.2939999997615814
Step: 32280, train/grad_norm: 7.476353645324707
Step: 32280, train/learning_rate: 1.158971917902818e-05
Step: 32280, train/epoch: 7.682055950164795
Step: 32290, train/loss: 0.3027999997138977
Step: 32290, train/grad_norm: 8.607154846191406
Step: 32290, train/learning_rate: 1.1577820259844884e-05
Step: 32290, train/epoch: 7.684435844421387
Step: 32300, train/loss: 0.45159998536109924
Step: 32300, train/grad_norm: 12.731014251708984
Step: 32300, train/learning_rate: 1.1565921340661589e-05
Step: 32300, train/epoch: 7.6868157386779785
Step: 32310, train/loss: 0.24740000069141388
Step: 32310, train/grad_norm: 7.355417251586914
Step: 32310, train/learning_rate: 1.1554021511983592e-05
Step: 32310, train/epoch: 7.68919563293457
Step: 32320, train/loss: 0.42750000953674316
Step: 32320, train/grad_norm: 21.988544464111328
Step: 32320, train/learning_rate: 1.1542122592800297e-05
Step: 32320, train/epoch: 7.691575527191162
Step: 32330, train/loss: 0.2531999945640564
Step: 32330, train/grad_norm: 5.441545009613037
Step: 32330, train/learning_rate: 1.1530223673617002e-05
Step: 32330, train/epoch: 7.693955421447754
Step: 32340, train/loss: 0.38670000433921814
Step: 32340, train/grad_norm: 2.7122063636779785
Step: 32340, train/learning_rate: 1.1518324754433706e-05
Step: 32340, train/epoch: 7.696335315704346
Step: 32350, train/loss: 0.27950000762939453
Step: 32350, train/grad_norm: 10.634221076965332
Step: 32350, train/learning_rate: 1.1506425835250411e-05
Step: 32350, train/epoch: 7.698714733123779
Step: 32360, train/loss: 0.34040001034736633
Step: 32360, train/grad_norm: 3.124997615814209
Step: 32360, train/learning_rate: 1.1494526006572414e-05
Step: 32360, train/epoch: 7.701094627380371
Step: 32370, train/loss: 0.30559998750686646
Step: 32370, train/grad_norm: 14.694235801696777
Step: 32370, train/learning_rate: 1.1482627087389119e-05
Step: 32370, train/epoch: 7.703474521636963
Step: 32380, train/loss: 0.45879998803138733
Step: 32380, train/grad_norm: 1.0617094039916992
Step: 32380, train/learning_rate: 1.1470728168205824e-05
Step: 32380, train/epoch: 7.705854415893555
Step: 32390, train/loss: 0.3418999910354614
Step: 32390, train/grad_norm: 17.728178024291992
Step: 32390, train/learning_rate: 1.1458829249022529e-05
Step: 32390, train/epoch: 7.7082343101501465
Step: 32400, train/loss: 0.30140000581741333
Step: 32400, train/grad_norm: 2.715726137161255
Step: 32400, train/learning_rate: 1.1446930329839233e-05
Step: 32400, train/epoch: 7.710614204406738
Step: 32410, train/loss: 0.4154999852180481
Step: 32410, train/grad_norm: 10.402632713317871
Step: 32410, train/learning_rate: 1.1435030501161236e-05
Step: 32410, train/epoch: 7.712993621826172
Step: 32420, train/loss: 0.46320000290870667
Step: 32420, train/grad_norm: 24.969757080078125
Step: 32420, train/learning_rate: 1.1423131581977941e-05
Step: 32420, train/epoch: 7.715373516082764
Step: 32430, train/loss: 0.3743000030517578
Step: 32430, train/grad_norm: 22.27840232849121
Step: 32430, train/learning_rate: 1.1411232662794646e-05
Step: 32430, train/epoch: 7.7177534103393555
Step: 32440, train/loss: 0.21709999442100525
Step: 32440, train/grad_norm: 20.63653564453125
Step: 32440, train/learning_rate: 1.139933374361135e-05
Step: 32440, train/epoch: 7.720133304595947
Step: 32450, train/loss: 0.23729999363422394
Step: 32450, train/grad_norm: 7.293599605560303
Step: 32450, train/learning_rate: 1.1387434824428055e-05
Step: 32450, train/epoch: 7.722513198852539
Step: 32460, train/loss: 0.3418000042438507
Step: 32460, train/grad_norm: 17.30763053894043
Step: 32460, train/learning_rate: 1.137553590524476e-05
Step: 32460, train/epoch: 7.724893093109131
Step: 32470, train/loss: 0.47690001130104065
Step: 32470, train/grad_norm: 3.4817018508911133
Step: 32470, train/learning_rate: 1.1363636076566763e-05
Step: 32470, train/epoch: 7.7272725105285645
Step: 32480, train/loss: 0.3637999892234802
Step: 32480, train/grad_norm: 15.970990180969238
Step: 32480, train/learning_rate: 1.1351737157383468e-05
Step: 32480, train/epoch: 7.729652404785156
Step: 32490, train/loss: 0.2567000091075897
Step: 32490, train/grad_norm: 5.1539483070373535
Step: 32490, train/learning_rate: 1.1339838238200173e-05
Step: 32490, train/epoch: 7.732032299041748
Step: 32500, train/loss: 0.4442000091075897
Step: 32500, train/grad_norm: 12.723668098449707
Step: 32500, train/learning_rate: 1.1327939319016878e-05
Step: 32500, train/epoch: 7.73441219329834
Step: 32510, train/loss: 0.3977999985218048
Step: 32510, train/grad_norm: 3.2584402561187744
Step: 32510, train/learning_rate: 1.1316040399833582e-05
Step: 32510, train/epoch: 7.736792087554932
Step: 32520, train/loss: 0.26249998807907104
Step: 32520, train/grad_norm: 14.754129409790039
Step: 32520, train/learning_rate: 1.1304140571155585e-05
Step: 32520, train/epoch: 7.739171981811523
Step: 32530, train/loss: 0.33390000462532043
Step: 32530, train/grad_norm: 13.177728652954102
Step: 32530, train/learning_rate: 1.129224165197229e-05
Step: 32530, train/epoch: 7.741551876068115
Step: 32540, train/loss: 0.3328999876976013
Step: 32540, train/grad_norm: 10.076981544494629
Step: 32540, train/learning_rate: 1.1280342732788995e-05
Step: 32540, train/epoch: 7.743931293487549
Step: 32550, train/loss: 0.2660999894142151
Step: 32550, train/grad_norm: 11.645835876464844
Step: 32550, train/learning_rate: 1.12684438136057e-05
Step: 32550, train/epoch: 7.746311187744141
Step: 32560, train/loss: 0.4025999903678894
Step: 32560, train/grad_norm: 27.061065673828125
Step: 32560, train/learning_rate: 1.1256544894422404e-05
Step: 32560, train/epoch: 7.748691082000732
Step: 32570, train/loss: 0.29899999499320984
Step: 32570, train/grad_norm: 5.334609508514404
Step: 32570, train/learning_rate: 1.1244645065744407e-05
Step: 32570, train/epoch: 7.751070976257324
Step: 32580, train/loss: 0.3409000039100647
Step: 32580, train/grad_norm: 17.22711753845215
Step: 32580, train/learning_rate: 1.1232746146561112e-05
Step: 32580, train/epoch: 7.753450870513916
Step: 32590, train/loss: 0.4293000102043152
Step: 32590, train/grad_norm: 23.25605010986328
Step: 32590, train/learning_rate: 1.1220847227377817e-05
Step: 32590, train/epoch: 7.755830764770508
Step: 32600, train/loss: 0.41679999232292175
Step: 32600, train/grad_norm: 15.053400039672852
Step: 32600, train/learning_rate: 1.1208948308194522e-05
Step: 32600, train/epoch: 7.758210182189941
Step: 32610, train/loss: 0.18129999935626984
Step: 32610, train/grad_norm: 15.238640785217285
Step: 32610, train/learning_rate: 1.1197049389011227e-05
Step: 32610, train/epoch: 7.760590076446533
Step: 32620, train/loss: 0.28780001401901245
Step: 32620, train/grad_norm: 16.385221481323242
Step: 32620, train/learning_rate: 1.118514956033323e-05
Step: 32620, train/epoch: 7.762969970703125
Step: 32630, train/loss: 0.32109999656677246
Step: 32630, train/grad_norm: 12.858417510986328
Step: 32630, train/learning_rate: 1.1173250641149934e-05
Step: 32630, train/epoch: 7.765349864959717
Step: 32640, train/loss: 0.33250001072883606
Step: 32640, train/grad_norm: 26.8133602142334
Step: 32640, train/learning_rate: 1.116135172196664e-05
Step: 32640, train/epoch: 7.767729759216309
Step: 32650, train/loss: 0.3181999921798706
Step: 32650, train/grad_norm: 23.56261444091797
Step: 32650, train/learning_rate: 1.1149452802783344e-05
Step: 32650, train/epoch: 7.7701096534729
Step: 32660, train/loss: 0.4758000075817108
Step: 32660, train/grad_norm: 14.080501556396484
Step: 32660, train/learning_rate: 1.1137553883600049e-05
Step: 32660, train/epoch: 7.772489070892334
Step: 32670, train/loss: 0.3474999964237213
Step: 32670, train/grad_norm: 12.911787986755371
Step: 32670, train/learning_rate: 1.1125654054922052e-05
Step: 32670, train/epoch: 7.774868965148926
Step: 32680, train/loss: 0.4230000078678131
Step: 32680, train/grad_norm: 10.698125839233398
Step: 32680, train/learning_rate: 1.1113755135738757e-05
Step: 32680, train/epoch: 7.777248859405518
Step: 32690, train/loss: 0.31839999556541443
Step: 32690, train/grad_norm: 13.804282188415527
Step: 32690, train/learning_rate: 1.1101856216555461e-05
Step: 32690, train/epoch: 7.779628753662109
Step: 32700, train/loss: 0.3596999943256378
Step: 32700, train/grad_norm: 7.34222412109375
Step: 32700, train/learning_rate: 1.1089957297372166e-05
Step: 32700, train/epoch: 7.782008647918701
Step: 32710, train/loss: 0.3181000053882599
Step: 32710, train/grad_norm: 12.222122192382812
Step: 32710, train/learning_rate: 1.1078058378188871e-05
Step: 32710, train/epoch: 7.784388542175293
Step: 32720, train/loss: 0.5128999948501587
Step: 32720, train/grad_norm: 12.138330459594727
Step: 32720, train/learning_rate: 1.1066158549510874e-05
Step: 32720, train/epoch: 7.786768436431885
Step: 32730, train/loss: 0.5198000073432922
Step: 32730, train/grad_norm: 19.13472557067871
Step: 32730, train/learning_rate: 1.1054259630327579e-05
Step: 32730, train/epoch: 7.789147853851318
Step: 32740, train/loss: 0.27549999952316284
Step: 32740, train/grad_norm: 6.2300872802734375
Step: 32740, train/learning_rate: 1.1042360711144283e-05
Step: 32740, train/epoch: 7.79152774810791
Step: 32750, train/loss: 0.38190001249313354
Step: 32750, train/grad_norm: 12.275614738464355
Step: 32750, train/learning_rate: 1.1030461791960988e-05
Step: 32750, train/epoch: 7.793907642364502
Step: 32760, train/loss: 0.349700003862381
Step: 32760, train/grad_norm: 16.341489791870117
Step: 32760, train/learning_rate: 1.1018562872777693e-05
Step: 32760, train/epoch: 7.796287536621094
Step: 32770, train/loss: 0.35760000348091125
Step: 32770, train/grad_norm: 9.069656372070312
Step: 32770, train/learning_rate: 1.1006663044099696e-05
Step: 32770, train/epoch: 7.7986674308776855
Step: 32780, train/loss: 0.3653999865055084
Step: 32780, train/grad_norm: 26.86699867248535
Step: 32780, train/learning_rate: 1.09947641249164e-05
Step: 32780, train/epoch: 7.801047325134277
Step: 32790, train/loss: 0.4018000066280365
Step: 32790, train/grad_norm: 8.711307525634766
Step: 32790, train/learning_rate: 1.0982865205733106e-05
Step: 32790, train/epoch: 7.803426742553711
Step: 32800, train/loss: 0.3944999873638153
Step: 32800, train/grad_norm: 1.8281912803649902
Step: 32800, train/learning_rate: 1.097096628654981e-05
Step: 32800, train/epoch: 7.805806636810303
Step: 32810, train/loss: 0.26820001006126404
Step: 32810, train/grad_norm: 0.6978350281715393
Step: 32810, train/learning_rate: 1.0959067367366515e-05
Step: 32810, train/epoch: 7.8081865310668945
Step: 32820, train/loss: 0.3684999942779541
Step: 32820, train/grad_norm: 13.128170013427734
Step: 32820, train/learning_rate: 1.094716844818322e-05
Step: 32820, train/epoch: 7.810566425323486
Step: 32830, train/loss: 0.25949999690055847
Step: 32830, train/grad_norm: 14.615898132324219
Step: 32830, train/learning_rate: 1.0935268619505223e-05
Step: 32830, train/epoch: 7.812946319580078
Step: 32840, train/loss: 0.32089999318122864
Step: 32840, train/grad_norm: 10.439459800720215
Step: 32840, train/learning_rate: 1.0923369700321928e-05
Step: 32840, train/epoch: 7.81532621383667
Step: 32850, train/loss: 0.4341999888420105
Step: 32850, train/grad_norm: 20.613956451416016
Step: 32850, train/learning_rate: 1.0911470781138632e-05
Step: 32850, train/epoch: 7.8177056312561035
Step: 32860, train/loss: 0.37860000133514404
Step: 32860, train/grad_norm: 7.517282485961914
Step: 32860, train/learning_rate: 1.0899571861955337e-05
Step: 32860, train/epoch: 7.820085525512695
Step: 32870, train/loss: 0.5073000192642212
Step: 32870, train/grad_norm: 39.615177154541016
Step: 32870, train/learning_rate: 1.0887672942772042e-05
Step: 32870, train/epoch: 7.822465419769287
Step: 32880, train/loss: 0.38100001215934753
Step: 32880, train/grad_norm: 11.554830551147461
Step: 32880, train/learning_rate: 1.0875773114094045e-05
Step: 32880, train/epoch: 7.824845314025879
Step: 32890, train/loss: 0.24469999969005585
Step: 32890, train/grad_norm: 32.4805908203125
Step: 32890, train/learning_rate: 1.086387419491075e-05
Step: 32890, train/epoch: 7.827225208282471
Step: 32900, train/loss: 0.28790000081062317
Step: 32900, train/grad_norm: 18.248722076416016
Step: 32900, train/learning_rate: 1.0851975275727455e-05
Step: 32900, train/epoch: 7.8296051025390625
Step: 32910, train/loss: 0.3479999899864197
Step: 32910, train/grad_norm: 19.646032333374023
Step: 32910, train/learning_rate: 1.084007635654416e-05
Step: 32910, train/epoch: 7.831984996795654
Step: 32920, train/loss: 0.2556999921798706
Step: 32920, train/grad_norm: 12.779609680175781
Step: 32920, train/learning_rate: 1.0828177437360864e-05
Step: 32920, train/epoch: 7.834364414215088
Step: 32930, train/loss: 0.29109999537467957
Step: 32930, train/grad_norm: 2.0685806274414062
Step: 32930, train/learning_rate: 1.0816277608682867e-05
Step: 32930, train/epoch: 7.83674430847168
Step: 32940, train/loss: 0.3407999873161316
Step: 32940, train/grad_norm: 4.094221591949463
Step: 32940, train/learning_rate: 1.0804378689499572e-05
Step: 32940, train/epoch: 7.8391242027282715
Step: 32950, train/loss: 0.336899995803833
Step: 32950, train/grad_norm: 5.949069499969482
Step: 32950, train/learning_rate: 1.0792479770316277e-05
Step: 32950, train/epoch: 7.841504096984863
Step: 32960, train/loss: 0.46140000224113464
Step: 32960, train/grad_norm: 2.1165242195129395
Step: 32960, train/learning_rate: 1.0780580851132981e-05
Step: 32960, train/epoch: 7.843883991241455
Step: 32970, train/loss: 0.4742000102996826
Step: 32970, train/grad_norm: 14.280355453491211
Step: 32970, train/learning_rate: 1.0768681931949686e-05
Step: 32970, train/epoch: 7.846263885498047
Step: 32980, train/loss: 0.3476000130176544
Step: 32980, train/grad_norm: 19.088796615600586
Step: 32980, train/learning_rate: 1.075678210327169e-05
Step: 32980, train/epoch: 7.8486433029174805
Step: 32990, train/loss: 0.23729999363422394
Step: 32990, train/grad_norm: 10.454431533813477
Step: 32990, train/learning_rate: 1.0744883184088394e-05
Step: 32990, train/epoch: 7.851023197174072
Step: 33000, train/loss: 0.3562000095844269
Step: 33000, train/grad_norm: 21.830184936523438
Step: 33000, train/learning_rate: 1.0732984264905099e-05
Step: 33000, train/epoch: 7.853403091430664
Step: 33010, train/loss: 0.40630000829696655
Step: 33010, train/grad_norm: 25.81781005859375
Step: 33010, train/learning_rate: 1.0721085345721804e-05
Step: 33010, train/epoch: 7.855782985687256
Step: 33020, train/loss: 0.4277999997138977
Step: 33020, train/grad_norm: 11.860791206359863
Step: 33020, train/learning_rate: 1.0709186426538508e-05
Step: 33020, train/epoch: 7.858162879943848
Step: 33030, train/loss: 0.506600022315979
Step: 33030, train/grad_norm: 12.577573776245117
Step: 33030, train/learning_rate: 1.0697286597860511e-05
Step: 33030, train/epoch: 7.8605427742004395
Step: 33040, train/loss: 0.33480000495910645
Step: 33040, train/grad_norm: 10.887890815734863
Step: 33040, train/learning_rate: 1.0685387678677216e-05
Step: 33040, train/epoch: 7.862922191619873
Step: 33050, train/loss: 0.29120001196861267
Step: 33050, train/grad_norm: 37.471134185791016
Step: 33050, train/learning_rate: 1.0673488759493921e-05
Step: 33050, train/epoch: 7.865302085876465
Step: 33060, train/loss: 0.3292999863624573
Step: 33060, train/grad_norm: 3.841668128967285
Step: 33060, train/learning_rate: 1.0661589840310626e-05
Step: 33060, train/epoch: 7.867681980133057
Step: 33070, train/loss: 0.40119999647140503
Step: 33070, train/grad_norm: 10.566003799438477
Step: 33070, train/learning_rate: 1.064969092112733e-05
Step: 33070, train/epoch: 7.870061874389648
Step: 33080, train/loss: 0.29580000042915344
Step: 33080, train/grad_norm: 11.508749961853027
Step: 33080, train/learning_rate: 1.0637791092449334e-05
Step: 33080, train/epoch: 7.87244176864624
Step: 33090, train/loss: 0.25859999656677246
Step: 33090, train/grad_norm: 3.64314603805542
Step: 33090, train/learning_rate: 1.0625892173266038e-05
Step: 33090, train/epoch: 7.874821662902832
Step: 33100, train/loss: 0.3425000011920929
Step: 33100, train/grad_norm: 4.196990489959717
Step: 33100, train/learning_rate: 1.0613993254082743e-05
Step: 33100, train/epoch: 7.877201557159424
Step: 33110, train/loss: 0.4867999851703644
Step: 33110, train/grad_norm: 13.367203712463379
Step: 33110, train/learning_rate: 1.0602094334899448e-05
Step: 33110, train/epoch: 7.879580974578857
Step: 33120, train/loss: 0.34060001373291016
Step: 33120, train/grad_norm: 12.462340354919434
Step: 33120, train/learning_rate: 1.0590195415716153e-05
Step: 33120, train/epoch: 7.881960868835449
Step: 33130, train/loss: 0.263700008392334
Step: 33130, train/grad_norm: 7.297584533691406
Step: 33130, train/learning_rate: 1.0578296496532857e-05
Step: 33130, train/epoch: 7.884340763092041
Step: 33140, train/loss: 0.2443999946117401
Step: 33140, train/grad_norm: 16.37712860107422
Step: 33140, train/learning_rate: 1.056639666785486e-05
Step: 33140, train/epoch: 7.886720657348633
Step: 33150, train/loss: 0.4627000093460083
Step: 33150, train/grad_norm: 10.82337760925293
Step: 33150, train/learning_rate: 1.0554497748671565e-05
Step: 33150, train/epoch: 7.889100551605225
Step: 33160, train/loss: 0.34279999136924744
Step: 33160, train/grad_norm: 22.118989944458008
Step: 33160, train/learning_rate: 1.054259882948827e-05
Step: 33160, train/epoch: 7.891480445861816
Step: 33170, train/loss: 0.37400001287460327
Step: 33170, train/grad_norm: 16.091838836669922
Step: 33170, train/learning_rate: 1.0530699910304975e-05
Step: 33170, train/epoch: 7.89385986328125
Step: 33180, train/loss: 0.4269999861717224
Step: 33180, train/grad_norm: 10.510897636413574
Step: 33180, train/learning_rate: 1.051880099112168e-05
Step: 33180, train/epoch: 7.896239757537842
Step: 33190, train/loss: 0.38040000200271606
Step: 33190, train/grad_norm: 33.512939453125
Step: 33190, train/learning_rate: 1.0506901162443683e-05
Step: 33190, train/epoch: 7.898619651794434
Step: 33200, train/loss: 0.5138999819755554
Step: 33200, train/grad_norm: 19.70041847229004
Step: 33200, train/learning_rate: 1.0495002243260387e-05
Step: 33200, train/epoch: 7.900999546051025
Step: 33210, train/loss: 0.11959999799728394
Step: 33210, train/grad_norm: 7.487968921661377
Step: 33210, train/learning_rate: 1.0483103324077092e-05
Step: 33210, train/epoch: 7.903379440307617
Step: 33220, train/loss: 0.23680000007152557
Step: 33220, train/grad_norm: 3.022582530975342
Step: 33220, train/learning_rate: 1.0471204404893797e-05
Step: 33220, train/epoch: 7.905759334564209
Step: 33230, train/loss: 0.20499999821186066
Step: 33230, train/grad_norm: 7.285288333892822
Step: 33230, train/learning_rate: 1.0459305485710502e-05
Step: 33230, train/epoch: 7.908138751983643
Step: 33240, train/loss: 0.30880001187324524
Step: 33240, train/grad_norm: 9.443794250488281
Step: 33240, train/learning_rate: 1.0447405657032505e-05
Step: 33240, train/epoch: 7.910518646240234
Step: 33250, train/loss: 0.2337000072002411
Step: 33250, train/grad_norm: 16.458969116210938
Step: 33250, train/learning_rate: 1.043550673784921e-05
Step: 33250, train/epoch: 7.912898540496826
Step: 33260, train/loss: 0.2856999933719635
Step: 33260, train/grad_norm: 5.629156112670898
Step: 33260, train/learning_rate: 1.0423607818665914e-05
Step: 33260, train/epoch: 7.915278434753418
Step: 33270, train/loss: 0.4172999858856201
Step: 33270, train/grad_norm: 5.967621326446533
Step: 33270, train/learning_rate: 1.0411708899482619e-05
Step: 33270, train/epoch: 7.91765832901001
Step: 33280, train/loss: 0.3059000074863434
Step: 33280, train/grad_norm: 3.7297253608703613
Step: 33280, train/learning_rate: 1.0399809980299324e-05
Step: 33280, train/epoch: 7.920038223266602
Step: 33290, train/loss: 0.3086000084877014
Step: 33290, train/grad_norm: 9.051633834838867
Step: 33290, train/learning_rate: 1.0387910151621327e-05
Step: 33290, train/epoch: 7.922418117523193
Step: 33300, train/loss: 0.46619999408721924
Step: 33300, train/grad_norm: 20.22966194152832
Step: 33300, train/learning_rate: 1.0376011232438032e-05
Step: 33300, train/epoch: 7.924797534942627
Step: 33310, train/loss: 0.3012999892234802
Step: 33310, train/grad_norm: 15.40812873840332
Step: 33310, train/learning_rate: 1.0364112313254736e-05
Step: 33310, train/epoch: 7.927177429199219
Step: 33320, train/loss: 0.23839999735355377
Step: 33320, train/grad_norm: 8.022353172302246
Step: 33320, train/learning_rate: 1.0352213394071441e-05
Step: 33320, train/epoch: 7.9295573234558105
Step: 33330, train/loss: 0.23909999430179596
Step: 33330, train/grad_norm: 19.273738861083984
Step: 33330, train/learning_rate: 1.0340314474888146e-05
Step: 33330, train/epoch: 7.931937217712402
Step: 33340, train/loss: 0.2280000001192093
Step: 33340, train/grad_norm: 4.689350605010986
Step: 33340, train/learning_rate: 1.0328414646210149e-05
Step: 33340, train/epoch: 7.934317111968994
Step: 33350, train/loss: 0.30489999055862427
Step: 33350, train/grad_norm: 15.626700401306152
Step: 33350, train/learning_rate: 1.0316515727026854e-05
Step: 33350, train/epoch: 7.936697006225586
Step: 33360, train/loss: 0.29679998755455017
Step: 33360, train/grad_norm: 13.11127758026123
Step: 33360, train/learning_rate: 1.0304616807843558e-05
Step: 33360, train/epoch: 7.9390764236450195
Step: 33370, train/loss: 0.3280999958515167
Step: 33370, train/grad_norm: 11.351480484008789
Step: 33370, train/learning_rate: 1.0292717888660263e-05
Step: 33370, train/epoch: 7.941456317901611
Step: 33380, train/loss: 0.5149999856948853
Step: 33380, train/grad_norm: 8.39062213897705
Step: 33380, train/learning_rate: 1.0280818969476968e-05
Step: 33380, train/epoch: 7.943836212158203
Step: 33390, train/loss: 0.2628999948501587
Step: 33390, train/grad_norm: 8.631114959716797
Step: 33390, train/learning_rate: 1.0268919140798971e-05
Step: 33390, train/epoch: 7.946216106414795
Step: 33400, train/loss: 0.4129999876022339
Step: 33400, train/grad_norm: 1.3454246520996094
Step: 33400, train/learning_rate: 1.0257020221615676e-05
Step: 33400, train/epoch: 7.948596000671387
Step: 33410, train/loss: 0.5184999704360962
Step: 33410, train/grad_norm: 12.176968574523926
Step: 33410, train/learning_rate: 1.024512130243238e-05
Step: 33410, train/epoch: 7.9509758949279785
Step: 33420, train/loss: 0.35370001196861267
Step: 33420, train/grad_norm: 5.826566219329834
Step: 33420, train/learning_rate: 1.0233222383249085e-05
Step: 33420, train/epoch: 7.953355312347412
Step: 33430, train/loss: 0.2637999951839447
Step: 33430, train/grad_norm: 4.921943187713623
Step: 33430, train/learning_rate: 1.022132346406579e-05
Step: 33430, train/epoch: 7.955735206604004
Step: 33440, train/loss: 0.3319000005722046
Step: 33440, train/grad_norm: 9.941864967346191
Step: 33440, train/learning_rate: 1.0209423635387793e-05
Step: 33440, train/epoch: 7.958115100860596
Step: 33450, train/loss: 0.4011000096797943
Step: 33450, train/grad_norm: 19.661521911621094
Step: 33450, train/learning_rate: 1.0197524716204498e-05
Step: 33450, train/epoch: 7.9604949951171875
Step: 33460, train/loss: 0.25940001010894775
Step: 33460, train/grad_norm: 7.440858840942383
Step: 33460, train/learning_rate: 1.0185625797021203e-05
Step: 33460, train/epoch: 7.962874889373779
Step: 33470, train/loss: 0.474700003862381
Step: 33470, train/grad_norm: 6.978671073913574
Step: 33470, train/learning_rate: 1.0173726877837908e-05
Step: 33470, train/epoch: 7.965254783630371
Step: 33480, train/loss: 0.26989999413490295
Step: 33480, train/grad_norm: 8.578146934509277
Step: 33480, train/learning_rate: 1.0161827958654612e-05
Step: 33480, train/epoch: 7.967634677886963
Step: 33490, train/loss: 0.3790999948978424
Step: 33490, train/grad_norm: 10.197378158569336
Step: 33490, train/learning_rate: 1.0149929039471317e-05
Step: 33490, train/epoch: 7.9700140953063965
Step: 33500, train/loss: 0.2538999915122986
Step: 33500, train/grad_norm: 8.990224838256836
Step: 33500, train/learning_rate: 1.013802921079332e-05
Step: 33500, train/epoch: 7.972393989562988
Step: 33510, train/loss: 0.33880001306533813
Step: 33510, train/grad_norm: 17.633502960205078
Step: 33510, train/learning_rate: 1.0126130291610025e-05
Step: 33510, train/epoch: 7.97477388381958
Step: 33520, train/loss: 0.35019999742507935
Step: 33520, train/grad_norm: 11.498887062072754
Step: 33520, train/learning_rate: 1.011423137242673e-05
Step: 33520, train/epoch: 7.977153778076172
Step: 33530, train/loss: 0.3580999970436096
Step: 33530, train/grad_norm: 29.59280014038086
Step: 33530, train/learning_rate: 1.0102332453243434e-05
Step: 33530, train/epoch: 7.979533672332764
Step: 33540, train/loss: 0.3582000136375427
Step: 33540, train/grad_norm: 1.3876659870147705
Step: 33540, train/learning_rate: 1.009043353406014e-05
Step: 33540, train/epoch: 7.9819135665893555
Step: 33550, train/loss: 0.382999986410141
Step: 33550, train/grad_norm: 7.744937419891357
Step: 33550, train/learning_rate: 1.0078533705382142e-05
Step: 33550, train/epoch: 7.984292984008789
Step: 33560, train/loss: 0.39320001006126404
Step: 33560, train/grad_norm: 5.262413024902344
Step: 33560, train/learning_rate: 1.0066634786198847e-05
Step: 33560, train/epoch: 7.986672878265381
Step: 33570, train/loss: 0.32989999651908875
Step: 33570, train/grad_norm: 9.080583572387695
Step: 33570, train/learning_rate: 1.0054735867015552e-05
Step: 33570, train/epoch: 7.989052772521973
Step: 33580, train/loss: 0.33889999985694885
Step: 33580, train/grad_norm: 22.690650939941406
Step: 33580, train/learning_rate: 1.0042836947832257e-05
Step: 33580, train/epoch: 7.9914326667785645
Step: 33590, train/loss: 0.34529998898506165
Step: 33590, train/grad_norm: 10.785048484802246
Step: 33590, train/learning_rate: 1.0030938028648961e-05
Step: 33590, train/epoch: 7.993812561035156
Step: 33600, train/loss: 0.398499995470047
Step: 33600, train/grad_norm: 4.7703447341918945
Step: 33600, train/learning_rate: 1.0019038199970964e-05
Step: 33600, train/epoch: 7.996192455291748
Step: 33610, train/loss: 0.28760001063346863
Step: 33610, train/grad_norm: 7.915607452392578
Step: 33610, train/learning_rate: 1.0007139280787669e-05
Step: 33610, train/epoch: 7.998571872711182
Step: 33616, eval/loss: 1.0186184644699097
Step: 33616, eval/accuracy: 0.6647230386734009
Step: 33616, eval/f1: 0.6624117493629456
Step: 33616, eval/runtime: 55.623600006103516
Step: 33616, eval/samples_per_second: 129.4949951171875
Step: 33616, eval/steps_per_second: 16.197999954223633
Step: 33616, train/epoch: 8.0
Step: 33620, train/loss: 0.39340001344680786
Step: 33620, train/grad_norm: 16.51701545715332
Step: 33620, train/learning_rate: 9.995240361604374e-06
Step: 33620, train/epoch: 8.000951766967773
Step: 33630, train/loss: 0.2567000091075897
Step: 33630, train/grad_norm: 3.873880386352539
Step: 33630, train/learning_rate: 9.983341442421079e-06
Step: 33630, train/epoch: 8.003332138061523
Step: 33640, train/loss: 0.27709999680519104
Step: 33640, train/grad_norm: 4.188459873199463
Step: 33640, train/learning_rate: 9.971442523237783e-06
Step: 33640, train/epoch: 8.005711555480957
Step: 33650, train/loss: 0.3018999993801117
Step: 33650, train/grad_norm: 8.943229675292969
Step: 33650, train/learning_rate: 9.959542694559786e-06
Step: 33650, train/epoch: 8.00809097290039
Step: 33660, train/loss: 0.21699999272823334
Step: 33660, train/grad_norm: 4.794572353363037
Step: 33660, train/learning_rate: 9.947643775376491e-06
Step: 33660, train/epoch: 8.01047134399414
Step: 33670, train/loss: 0.34150001406669617
Step: 33670, train/grad_norm: 2.1863157749176025
Step: 33670, train/learning_rate: 9.935744856193196e-06
Step: 33670, train/epoch: 8.012850761413574
Step: 33680, train/loss: 0.413100004196167
Step: 33680, train/grad_norm: 9.431108474731445
Step: 33680, train/learning_rate: 9.9238459370099e-06
Step: 33680, train/epoch: 8.015231132507324
Step: 33690, train/loss: 0.34130001068115234
Step: 33690, train/grad_norm: 25.543701171875
Step: 33690, train/learning_rate: 9.911947017826606e-06
Step: 33690, train/epoch: 8.017610549926758
Step: 33700, train/loss: 0.2992999851703644
Step: 33700, train/grad_norm: 14.285888671875
Step: 33700, train/learning_rate: 9.900047189148609e-06
Step: 33700, train/epoch: 8.019990921020508
Step: 33710, train/loss: 0.4453999996185303
Step: 33710, train/grad_norm: 6.778321266174316
Step: 33710, train/learning_rate: 9.888148269965313e-06
Step: 33710, train/epoch: 8.022370338439941
Step: 33720, train/loss: 0.24390000104904175
Step: 33720, train/grad_norm: 4.568315505981445
Step: 33720, train/learning_rate: 9.876249350782018e-06
Step: 33720, train/epoch: 8.024749755859375
Step: 33730, train/loss: 0.2752000093460083
Step: 33730, train/grad_norm: 11.165302276611328
Step: 33730, train/learning_rate: 9.864350431598723e-06
Step: 33730, train/epoch: 8.027130126953125
Step: 33740, train/loss: 0.4596000015735626
Step: 33740, train/grad_norm: 9.606186866760254
Step: 33740, train/learning_rate: 9.852451512415428e-06
Step: 33740, train/epoch: 8.029509544372559
Step: 33750, train/loss: 0.3700000047683716
Step: 33750, train/grad_norm: 9.868247032165527
Step: 33750, train/learning_rate: 9.84055168373743e-06
Step: 33750, train/epoch: 8.031889915466309
Step: 33760, train/loss: 0.25780001282691956
Step: 33760, train/grad_norm: 11.157026290893555
Step: 33760, train/learning_rate: 9.828652764554136e-06
Step: 33760, train/epoch: 8.034269332885742
Step: 33770, train/loss: 0.3725000023841858
Step: 33770, train/grad_norm: 13.794754028320312
Step: 33770, train/learning_rate: 9.81675384537084e-06
Step: 33770, train/epoch: 8.036648750305176
Step: 33780, train/loss: 0.35830000042915344
Step: 33780, train/grad_norm: 1.8210113048553467
Step: 33780, train/learning_rate: 9.804854926187545e-06
Step: 33780, train/epoch: 8.039029121398926
Step: 33790, train/loss: 0.33160001039505005
Step: 33790, train/grad_norm: 5.374399185180664
Step: 33790, train/learning_rate: 9.79295600700425e-06
Step: 33790, train/epoch: 8.04140853881836
Step: 33800, train/loss: 0.2535000145435333
Step: 33800, train/grad_norm: 10.223869323730469
Step: 33800, train/learning_rate: 9.781057087820955e-06
Step: 33800, train/epoch: 8.04378890991211
Step: 33810, train/loss: 0.3231000006198883
Step: 33810, train/grad_norm: 11.406878471374512
Step: 33810, train/learning_rate: 9.769157259142958e-06
Step: 33810, train/epoch: 8.046168327331543
Step: 33820, train/loss: 0.5695000290870667
Step: 33820, train/grad_norm: 14.94876766204834
Step: 33820, train/learning_rate: 9.757258339959662e-06
Step: 33820, train/epoch: 8.048548698425293
Step: 33830, train/loss: 0.4142000079154968
Step: 33830, train/grad_norm: 23.052648544311523
Step: 33830, train/learning_rate: 9.745359420776367e-06
Step: 33830, train/epoch: 8.050928115844727
Step: 33840, train/loss: 0.3601999878883362
Step: 33840, train/grad_norm: 12.67282485961914
Step: 33840, train/learning_rate: 9.733460501593072e-06
Step: 33840, train/epoch: 8.05330753326416
Step: 33850, train/loss: 0.2076999992132187
Step: 33850, train/grad_norm: 11.018610000610352
Step: 33850, train/learning_rate: 9.721561582409777e-06
Step: 33850, train/epoch: 8.05568790435791
Step: 33860, train/loss: 0.3124000132083893
Step: 33860, train/grad_norm: 8.69091796875
Step: 33860, train/learning_rate: 9.70966175373178e-06
Step: 33860, train/epoch: 8.058067321777344
Step: 33870, train/loss: 0.38760000467300415
Step: 33870, train/grad_norm: 5.544277191162109
Step: 33870, train/learning_rate: 9.697762834548485e-06
Step: 33870, train/epoch: 8.060447692871094
Step: 33880, train/loss: 0.34380000829696655
Step: 33880, train/grad_norm: 7.441444396972656
Step: 33880, train/learning_rate: 9.68586391536519e-06
Step: 33880, train/epoch: 8.062827110290527
Step: 33890, train/loss: 0.5166000127792358
Step: 33890, train/grad_norm: 3.8933355808258057
Step: 33890, train/learning_rate: 9.673964996181894e-06
Step: 33890, train/epoch: 8.065207481384277
Step: 33900, train/loss: 0.382099986076355
Step: 33900, train/grad_norm: 5.766541957855225
Step: 33900, train/learning_rate: 9.662066076998599e-06
Step: 33900, train/epoch: 8.067586898803711
Step: 33910, train/loss: 0.35280001163482666
Step: 33910, train/grad_norm: 3.1863021850585938
Step: 33910, train/learning_rate: 9.650166248320602e-06
Step: 33910, train/epoch: 8.069966316223145
Step: 33920, train/loss: 0.41499999165534973
Step: 33920, train/grad_norm: 8.740050315856934
Step: 33920, train/learning_rate: 9.638267329137307e-06
Step: 33920, train/epoch: 8.072346687316895
Step: 33930, train/loss: 0.3555000126361847
Step: 33930, train/grad_norm: 1.601353645324707
Step: 33930, train/learning_rate: 9.626368409954011e-06
Step: 33930, train/epoch: 8.074726104736328
Step: 33940, train/loss: 0.34470000863075256
Step: 33940, train/grad_norm: 9.368976593017578
Step: 33940, train/learning_rate: 9.614469490770716e-06
Step: 33940, train/epoch: 8.077106475830078
Step: 33950, train/loss: 0.4300999939441681
Step: 33950, train/grad_norm: 6.340013027191162
Step: 33950, train/learning_rate: 9.602570571587421e-06
Step: 33950, train/epoch: 8.079485893249512
Step: 33960, train/loss: 0.39070001244544983
Step: 33960, train/grad_norm: 22.42868423461914
Step: 33960, train/learning_rate: 9.590670742909424e-06
Step: 33960, train/epoch: 8.081865310668945
Step: 33970, train/loss: 0.30079999566078186
Step: 33970, train/grad_norm: 26.84612274169922
Step: 33970, train/learning_rate: 9.578771823726129e-06
Step: 33970, train/epoch: 8.084245681762695
Step: 33980, train/loss: 0.28299999237060547
Step: 33980, train/grad_norm: 14.091075897216797
Step: 33980, train/learning_rate: 9.566872904542834e-06
Step: 33980, train/epoch: 8.086625099182129
Step: 33990, train/loss: 0.3073999881744385
Step: 33990, train/grad_norm: 14.00143814086914
Step: 33990, train/learning_rate: 9.554973985359538e-06
Step: 33990, train/epoch: 8.089005470275879
Step: 34000, train/loss: 0.22529999911785126
Step: 34000, train/grad_norm: 2.876784086227417
Step: 34000, train/learning_rate: 9.543075066176243e-06
Step: 34000, train/epoch: 8.091384887695312
Step: 34010, train/loss: 0.3393000066280365
Step: 34010, train/grad_norm: 25.011884689331055
Step: 34010, train/learning_rate: 9.531175237498246e-06
Step: 34010, train/epoch: 8.093765258789062
Step: 34020, train/loss: 0.21250000596046448
Step: 34020, train/grad_norm: 9.148618698120117
Step: 34020, train/learning_rate: 9.519276318314951e-06
Step: 34020, train/epoch: 8.096144676208496
Step: 34030, train/loss: 0.3294999897480011
Step: 34030, train/grad_norm: 12.233536720275879
Step: 34030, train/learning_rate: 9.507377399131656e-06
Step: 34030, train/epoch: 8.09852409362793
Step: 34040, train/loss: 0.3862999975681305
Step: 34040, train/grad_norm: 20.961910247802734
Step: 34040, train/learning_rate: 9.49547847994836e-06
Step: 34040, train/epoch: 8.10090446472168
Step: 34050, train/loss: 0.25
Step: 34050, train/grad_norm: 16.97483253479004
Step: 34050, train/learning_rate: 9.483579560765065e-06
Step: 34050, train/epoch: 8.103283882141113
Step: 34060, train/loss: 0.2782999873161316
Step: 34060, train/grad_norm: 9.265068054199219
Step: 34060, train/learning_rate: 9.471679732087068e-06
Step: 34060, train/epoch: 8.105664253234863
Step: 34070, train/loss: 0.43130001425743103
Step: 34070, train/grad_norm: 4.854048728942871
Step: 34070, train/learning_rate: 9.459780812903773e-06
Step: 34070, train/epoch: 8.108043670654297
Step: 34080, train/loss: 0.5220000147819519
Step: 34080, train/grad_norm: 13.724862098693848
Step: 34080, train/learning_rate: 9.447881893720478e-06
Step: 34080, train/epoch: 8.110424041748047
Step: 34090, train/loss: 0.2662999927997589
Step: 34090, train/grad_norm: 10.043394088745117
Step: 34090, train/learning_rate: 9.435982974537183e-06
Step: 34090, train/epoch: 8.11280345916748
Step: 34100, train/loss: 0.26089999079704285
Step: 34100, train/grad_norm: 5.963009834289551
Step: 34100, train/learning_rate: 9.424084055353887e-06
Step: 34100, train/epoch: 8.115182876586914
Step: 34110, train/loss: 0.23000000417232513
Step: 34110, train/grad_norm: 9.032831192016602
Step: 34110, train/learning_rate: 9.41218422667589e-06
Step: 34110, train/epoch: 8.117563247680664
Step: 34120, train/loss: 0.34610000252723694
Step: 34120, train/grad_norm: 11.026222229003906
Step: 34120, train/learning_rate: 9.400285307492595e-06
Step: 34120, train/epoch: 8.119942665100098
Step: 34130, train/loss: 0.2612000107765198
Step: 34130, train/grad_norm: 17.46434783935547
Step: 34130, train/learning_rate: 9.3883863883093e-06
Step: 34130, train/epoch: 8.122323036193848
Step: 34140, train/loss: 0.3199000060558319
Step: 34140, train/grad_norm: 8.201598167419434
Step: 34140, train/learning_rate: 9.376487469126005e-06
Step: 34140, train/epoch: 8.124702453613281
Step: 34150, train/loss: 0.24740000069141388
Step: 34150, train/grad_norm: 4.911116600036621
Step: 34150, train/learning_rate: 9.36458854994271e-06
Step: 34150, train/epoch: 8.127081871032715
Step: 34160, train/loss: 0.2865000069141388
Step: 34160, train/grad_norm: 19.385717391967773
Step: 34160, train/learning_rate: 9.352689630759414e-06
Step: 34160, train/epoch: 8.129462242126465
Step: 34170, train/loss: 0.26179999113082886
Step: 34170, train/grad_norm: 17.6033992767334
Step: 34170, train/learning_rate: 9.340789802081417e-06
Step: 34170, train/epoch: 8.131841659545898
Step: 34180, train/loss: 0.5275999903678894
Step: 34180, train/grad_norm: 12.732027053833008
Step: 34180, train/learning_rate: 9.328890882898122e-06
Step: 34180, train/epoch: 8.134222030639648
Step: 34190, train/loss: 0.26440000534057617
Step: 34190, train/grad_norm: 2.204425573348999
Step: 34190, train/learning_rate: 9.316991963714827e-06
Step: 34190, train/epoch: 8.136601448059082
Step: 34200, train/loss: 0.41119998693466187
Step: 34200, train/grad_norm: 5.341550350189209
Step: 34200, train/learning_rate: 9.305093044531532e-06
Step: 34200, train/epoch: 8.138981819152832
Step: 34210, train/loss: 0.38519999384880066
Step: 34210, train/grad_norm: 11.08024787902832
Step: 34210, train/learning_rate: 9.293194125348236e-06
Step: 34210, train/epoch: 8.141361236572266
Step: 34220, train/loss: 0.16249999403953552
Step: 34220, train/grad_norm: 4.29180383682251
Step: 34220, train/learning_rate: 9.28129429667024e-06
Step: 34220, train/epoch: 8.1437406539917
Step: 34230, train/loss: 0.43709999322891235
Step: 34230, train/grad_norm: 11.75736141204834
Step: 34230, train/learning_rate: 9.269395377486944e-06
Step: 34230, train/epoch: 8.14612102508545
Step: 34240, train/loss: 0.3425000011920929
Step: 34240, train/grad_norm: 2.5033204555511475
Step: 34240, train/learning_rate: 9.257496458303649e-06
Step: 34240, train/epoch: 8.148500442504883
Step: 34250, train/loss: 0.41200000047683716
Step: 34250, train/grad_norm: 29.200843811035156
Step: 34250, train/learning_rate: 9.245597539120354e-06
Step: 34250, train/epoch: 8.150880813598633
Step: 34260, train/loss: 0.34049999713897705
Step: 34260, train/grad_norm: 8.802433967590332
Step: 34260, train/learning_rate: 9.233698619937059e-06
Step: 34260, train/epoch: 8.153260231018066
Step: 34270, train/loss: 0.35350000858306885
Step: 34270, train/grad_norm: 5.183942794799805
Step: 34270, train/learning_rate: 9.221798791259062e-06
Step: 34270, train/epoch: 8.155640602111816
Step: 34280, train/loss: 0.29190000891685486
Step: 34280, train/grad_norm: 0.7764915227890015
Step: 34280, train/learning_rate: 9.209899872075766e-06
Step: 34280, train/epoch: 8.15802001953125
Step: 34290, train/loss: 0.20730000734329224
Step: 34290, train/grad_norm: 4.761660099029541
Step: 34290, train/learning_rate: 9.198000952892471e-06
Step: 34290, train/epoch: 8.160399436950684
Step: 34300, train/loss: 0.23989999294281006
Step: 34300, train/grad_norm: 23.589675903320312
Step: 34300, train/learning_rate: 9.186102033709176e-06
Step: 34300, train/epoch: 8.162779808044434
Step: 34310, train/loss: 0.28290000557899475
Step: 34310, train/grad_norm: 14.416309356689453
Step: 34310, train/learning_rate: 9.17420311452588e-06
Step: 34310, train/epoch: 8.165159225463867
Step: 34320, train/loss: 0.35740000009536743
Step: 34320, train/grad_norm: 25.92168617248535
Step: 34320, train/learning_rate: 9.162303285847884e-06
Step: 34320, train/epoch: 8.167539596557617
Step: 34330, train/loss: 0.42309999465942383
Step: 34330, train/grad_norm: 17.502952575683594
Step: 34330, train/learning_rate: 9.150404366664588e-06
Step: 34330, train/epoch: 8.16991901397705
Step: 34340, train/loss: 0.2924000024795532
Step: 34340, train/grad_norm: 5.7195658683776855
Step: 34340, train/learning_rate: 9.138505447481293e-06
Step: 34340, train/epoch: 8.172298431396484
Step: 34350, train/loss: 0.4756999909877777
Step: 34350, train/grad_norm: 12.926533699035645
Step: 34350, train/learning_rate: 9.126606528297998e-06
Step: 34350, train/epoch: 8.174678802490234
Step: 34360, train/loss: 0.2590000033378601
Step: 34360, train/grad_norm: 22.305070877075195
Step: 34360, train/learning_rate: 9.114707609114703e-06
Step: 34360, train/epoch: 8.177058219909668
Step: 34370, train/loss: 0.24709999561309814
Step: 34370, train/grad_norm: 10.032055854797363
Step: 34370, train/learning_rate: 9.102807780436706e-06
Step: 34370, train/epoch: 8.179438591003418
Step: 34380, train/loss: 0.3138999938964844
Step: 34380, train/grad_norm: 35.63762664794922
Step: 34380, train/learning_rate: 9.09090886125341e-06
Step: 34380, train/epoch: 8.181818008422852
Step: 34390, train/loss: 0.24379999935626984
Step: 34390, train/grad_norm: 7.786585330963135
Step: 34390, train/learning_rate: 9.079009942070115e-06
Step: 34390, train/epoch: 8.184198379516602
Step: 34400, train/loss: 0.42910000681877136
Step: 34400, train/grad_norm: 39.88801574707031
Step: 34400, train/learning_rate: 9.06711102288682e-06
Step: 34400, train/epoch: 8.186577796936035
Step: 34410, train/loss: 0.3869999945163727
Step: 34410, train/grad_norm: 25.236196517944336
Step: 34410, train/learning_rate: 9.055212103703525e-06
Step: 34410, train/epoch: 8.188957214355469
Step: 34420, train/loss: 0.3052999973297119
Step: 34420, train/grad_norm: 5.823929309844971
Step: 34420, train/learning_rate: 9.043312275025528e-06
Step: 34420, train/epoch: 8.191337585449219
Step: 34430, train/loss: 0.3239000141620636
Step: 34430, train/grad_norm: 6.839338302612305
Step: 34430, train/learning_rate: 9.031413355842233e-06
Step: 34430, train/epoch: 8.193717002868652
Step: 34440, train/loss: 0.4465000033378601
Step: 34440, train/grad_norm: 13.904278755187988
Step: 34440, train/learning_rate: 9.019514436658937e-06
Step: 34440, train/epoch: 8.196097373962402
Step: 34450, train/loss: 0.26260000467300415
Step: 34450, train/grad_norm: 7.104234218597412
Step: 34450, train/learning_rate: 9.007615517475642e-06
Step: 34450, train/epoch: 8.198476791381836
Step: 34460, train/loss: 0.2896000146865845
Step: 34460, train/grad_norm: 10.147236824035645
Step: 34460, train/learning_rate: 8.995716598292347e-06
Step: 34460, train/epoch: 8.200857162475586
Step: 34470, train/loss: 0.26440000534057617
Step: 34470, train/grad_norm: 11.118597030639648
Step: 34470, train/learning_rate: 8.983817679109052e-06
Step: 34470, train/epoch: 8.20323657989502
Step: 34480, train/loss: 0.38589999079704285
Step: 34480, train/grad_norm: 9.207442283630371
Step: 34480, train/learning_rate: 8.971917850431055e-06
Step: 34480, train/epoch: 8.205615997314453
Step: 34490, train/loss: 0.2980000078678131
Step: 34490, train/grad_norm: 15.463451385498047
Step: 34490, train/learning_rate: 8.96001893124776e-06
Step: 34490, train/epoch: 8.207996368408203
Step: 34500, train/loss: 0.2606000006198883
Step: 34500, train/grad_norm: 16.92396354675293
Step: 34500, train/learning_rate: 8.948120012064464e-06
Step: 34500, train/epoch: 8.210375785827637
Step: 34510, train/loss: 0.36340001225471497
Step: 34510, train/grad_norm: 16.479707717895508
Step: 34510, train/learning_rate: 8.93622109288117e-06
Step: 34510, train/epoch: 8.212756156921387
Step: 34520, train/loss: 0.4352000057697296
Step: 34520, train/grad_norm: 31.33428192138672
Step: 34520, train/learning_rate: 8.924322173697874e-06
Step: 34520, train/epoch: 8.21513557434082
Step: 34530, train/loss: 0.25049999356269836
Step: 34530, train/grad_norm: 13.447680473327637
Step: 34530, train/learning_rate: 8.912422345019877e-06
Step: 34530, train/epoch: 8.21751594543457
Step: 34540, train/loss: 0.429500013589859
Step: 34540, train/grad_norm: 41.186988830566406
Step: 34540, train/learning_rate: 8.900523425836582e-06
Step: 34540, train/epoch: 8.219895362854004
Step: 34550, train/loss: 0.31150001287460327
Step: 34550, train/grad_norm: 7.523993492126465
Step: 34550, train/learning_rate: 8.888624506653287e-06
Step: 34550, train/epoch: 8.222274780273438
Step: 34560, train/loss: 0.3625999987125397
Step: 34560, train/grad_norm: 13.675071716308594
Step: 34560, train/learning_rate: 8.876725587469991e-06
Step: 34560, train/epoch: 8.224655151367188
Step: 34570, train/loss: 0.23739999532699585
Step: 34570, train/grad_norm: 13.620939254760742
Step: 34570, train/learning_rate: 8.864826668286696e-06
Step: 34570, train/epoch: 8.227034568786621
Step: 34580, train/loss: 0.4781999886035919
Step: 34580, train/grad_norm: 17.568504333496094
Step: 34580, train/learning_rate: 8.852926839608699e-06
Step: 34580, train/epoch: 8.229414939880371
Step: 34590, train/loss: 0.2551000118255615
Step: 34590, train/grad_norm: 7.319420337677002
Step: 34590, train/learning_rate: 8.841027920425404e-06
Step: 34590, train/epoch: 8.231794357299805
Step: 34600, train/loss: 0.3977999985218048
Step: 34600, train/grad_norm: 26.63100814819336
Step: 34600, train/learning_rate: 8.829129001242109e-06
Step: 34600, train/epoch: 8.234173774719238
Step: 34610, train/loss: 0.2892000079154968
Step: 34610, train/grad_norm: 3.757312774658203
Step: 34610, train/learning_rate: 8.817230082058813e-06
Step: 34610, train/epoch: 8.236554145812988
Step: 34620, train/loss: 0.3472000062465668
Step: 34620, train/grad_norm: 9.041176795959473
Step: 34620, train/learning_rate: 8.805331162875518e-06
Step: 34620, train/epoch: 8.238933563232422
Step: 34630, train/loss: 0.4090000092983246
Step: 34630, train/grad_norm: 17.650104522705078
Step: 34630, train/learning_rate: 8.793431334197521e-06
Step: 34630, train/epoch: 8.241313934326172
Step: 34640, train/loss: 0.29269999265670776
Step: 34640, train/grad_norm: 8.74271011352539
Step: 34640, train/learning_rate: 8.781532415014226e-06
Step: 34640, train/epoch: 8.243693351745605
Step: 34650, train/loss: 0.44190001487731934
Step: 34650, train/grad_norm: 2.6518208980560303
Step: 34650, train/learning_rate: 8.76963349583093e-06
Step: 34650, train/epoch: 8.246073722839355
Step: 34660, train/loss: 0.2702000141143799
Step: 34660, train/grad_norm: 22.17264175415039
Step: 34660, train/learning_rate: 8.757734576647636e-06
Step: 34660, train/epoch: 8.248453140258789
Step: 34670, train/loss: 0.3352999985218048
Step: 34670, train/grad_norm: 9.812565803527832
Step: 34670, train/learning_rate: 8.74583565746434e-06
Step: 34670, train/epoch: 8.250832557678223
Step: 34680, train/loss: 0.2151000052690506
Step: 34680, train/grad_norm: 2.1194825172424316
Step: 34680, train/learning_rate: 8.733935828786343e-06
Step: 34680, train/epoch: 8.253212928771973
Step: 34690, train/loss: 0.38190001249313354
Step: 34690, train/grad_norm: 30.578325271606445
Step: 34690, train/learning_rate: 8.722036909603048e-06
Step: 34690, train/epoch: 8.255592346191406
Step: 34700, train/loss: 0.24950000643730164
Step: 34700, train/grad_norm: 11.664403915405273
Step: 34700, train/learning_rate: 8.710137990419753e-06
Step: 34700, train/epoch: 8.257972717285156
Step: 34710, train/loss: 0.3779999911785126
Step: 34710, train/grad_norm: 6.92256498336792
Step: 34710, train/learning_rate: 8.698239071236458e-06
Step: 34710, train/epoch: 8.26035213470459
Step: 34720, train/loss: 0.3183000087738037
Step: 34720, train/grad_norm: 10.314059257507324
Step: 34720, train/learning_rate: 8.686340152053162e-06
Step: 34720, train/epoch: 8.26273250579834
Step: 34730, train/loss: 0.2736000120639801
Step: 34730, train/grad_norm: 3.888082981109619
Step: 34730, train/learning_rate: 8.674440323375165e-06
Step: 34730, train/epoch: 8.265111923217773
Step: 34740, train/loss: 0.3273000121116638
Step: 34740, train/grad_norm: 7.163527488708496
Step: 34740, train/learning_rate: 8.66254140419187e-06
Step: 34740, train/epoch: 8.267491340637207
Step: 34750, train/loss: 0.3950999975204468
Step: 34750, train/grad_norm: 24.262310028076172
Step: 34750, train/learning_rate: 8.650642485008575e-06
Step: 34750, train/epoch: 8.269871711730957
Step: 34760, train/loss: 0.25060001015663147
Step: 34760, train/grad_norm: 7.686984539031982
Step: 34760, train/learning_rate: 8.63874356582528e-06
Step: 34760, train/epoch: 8.27225112915039
Step: 34770, train/loss: 0.3862000107765198
Step: 34770, train/grad_norm: 12.100606918334961
Step: 34770, train/learning_rate: 8.626844646641985e-06
Step: 34770, train/epoch: 8.27463150024414
Step: 34780, train/loss: 0.4205000102519989
Step: 34780, train/grad_norm: 20.465999603271484
Step: 34780, train/learning_rate: 8.614944817963988e-06
Step: 34780, train/epoch: 8.277010917663574
Step: 34790, train/loss: 0.34940001368522644
Step: 34790, train/grad_norm: 6.576879024505615
Step: 34790, train/learning_rate: 8.603045898780692e-06
Step: 34790, train/epoch: 8.279390335083008
Step: 34800, train/loss: 0.3571999967098236
Step: 34800, train/grad_norm: 27.34998893737793
Step: 34800, train/learning_rate: 8.591146979597397e-06
Step: 34800, train/epoch: 8.281770706176758
Step: 34810, train/loss: 0.3149999976158142
Step: 34810, train/grad_norm: 13.614903450012207
Step: 34810, train/learning_rate: 8.579248060414102e-06
Step: 34810, train/epoch: 8.284150123596191
Step: 34820, train/loss: 0.3513000011444092
Step: 34820, train/grad_norm: 32.77914047241211
Step: 34820, train/learning_rate: 8.567349141230807e-06
Step: 34820, train/epoch: 8.286530494689941
Step: 34830, train/loss: 0.314300000667572
Step: 34830, train/grad_norm: 14.136537551879883
Step: 34830, train/learning_rate: 8.555450222047511e-06
Step: 34830, train/epoch: 8.288909912109375
Step: 34840, train/loss: 0.33309999108314514
Step: 34840, train/grad_norm: 19.594873428344727
Step: 34840, train/learning_rate: 8.543550393369514e-06
Step: 34840, train/epoch: 8.291290283203125
Step: 34850, train/loss: 0.37209999561309814
Step: 34850, train/grad_norm: 26.61538314819336
Step: 34850, train/learning_rate: 8.53165147418622e-06
Step: 34850, train/epoch: 8.293669700622559
Step: 34860, train/loss: 0.414900004863739
Step: 34860, train/grad_norm: 27.04909324645996
Step: 34860, train/learning_rate: 8.519752555002924e-06
Step: 34860, train/epoch: 8.296049118041992
Step: 34870, train/loss: 0.4738999903202057
Step: 34870, train/grad_norm: 19.975269317626953
Step: 34870, train/learning_rate: 8.507853635819629e-06
Step: 34870, train/epoch: 8.298429489135742
Step: 34880, train/loss: 0.30079999566078186
Step: 34880, train/grad_norm: 19.00373649597168
Step: 34880, train/learning_rate: 8.495954716636334e-06
Step: 34880, train/epoch: 8.300808906555176
Step: 34890, train/loss: 0.3154999911785126
Step: 34890, train/grad_norm: 16.60793685913086
Step: 34890, train/learning_rate: 8.484054887958337e-06
Step: 34890, train/epoch: 8.303189277648926
Step: 34900, train/loss: 0.3617999851703644
Step: 34900, train/grad_norm: 11.528724670410156
Step: 34900, train/learning_rate: 8.472155968775041e-06
Step: 34900, train/epoch: 8.30556869506836
Step: 34910, train/loss: 0.29339998960494995
Step: 34910, train/grad_norm: 19.663562774658203
Step: 34910, train/learning_rate: 8.460257049591746e-06
Step: 34910, train/epoch: 8.30794906616211
Step: 34920, train/loss: 0.3871000111103058
Step: 34920, train/grad_norm: 6.350449085235596
Step: 34920, train/learning_rate: 8.448358130408451e-06
Step: 34920, train/epoch: 8.310328483581543
Step: 34930, train/loss: 0.3249000012874603
Step: 34930, train/grad_norm: 11.742652893066406
Step: 34930, train/learning_rate: 8.436459211225156e-06
Step: 34930, train/epoch: 8.312707901000977
Step: 34940, train/loss: 0.35920000076293945
Step: 34940, train/grad_norm: 10.242433547973633
Step: 34940, train/learning_rate: 8.424559382547159e-06
Step: 34940, train/epoch: 8.315088272094727
Step: 34950, train/loss: 0.3100000023841858
Step: 34950, train/grad_norm: 21.6718692779541
Step: 34950, train/learning_rate: 8.412660463363864e-06
Step: 34950, train/epoch: 8.31746768951416
Step: 34960, train/loss: 0.37470000982284546
Step: 34960, train/grad_norm: 5.537693500518799
Step: 34960, train/learning_rate: 8.400761544180568e-06
Step: 34960, train/epoch: 8.31984806060791
Step: 34970, train/loss: 0.22990000247955322
Step: 34970, train/grad_norm: 9.811010360717773
Step: 34970, train/learning_rate: 8.388862624997273e-06
Step: 34970, train/epoch: 8.322227478027344
Step: 34980, train/loss: 0.45899999141693115
Step: 34980, train/grad_norm: 19.138702392578125
Step: 34980, train/learning_rate: 8.376963705813978e-06
Step: 34980, train/epoch: 8.324606895446777
Step: 34990, train/loss: 0.26089999079704285
Step: 34990, train/grad_norm: 4.13660192489624
Step: 34990, train/learning_rate: 8.365063877135981e-06
Step: 34990, train/epoch: 8.326987266540527
Step: 35000, train/loss: 0.578499972820282
Step: 35000, train/grad_norm: 19.16673469543457
Step: 35000, train/learning_rate: 8.353164957952686e-06
Step: 35000, train/epoch: 8.329366683959961
Step: 35010, train/loss: 0.5620999932289124
Step: 35010, train/grad_norm: 10.84257984161377
Step: 35010, train/learning_rate: 8.34126603876939e-06
Step: 35010, train/epoch: 8.331747055053711
Step: 35020, train/loss: 0.3230000138282776
Step: 35020, train/grad_norm: 6.967931747436523
Step: 35020, train/learning_rate: 8.329367119586095e-06
Step: 35020, train/epoch: 8.334126472473145
Step: 35030, train/loss: 0.3720000088214874
Step: 35030, train/grad_norm: 5.323243618011475
Step: 35030, train/learning_rate: 8.3174682004028e-06
Step: 35030, train/epoch: 8.336506843566895
Step: 35040, train/loss: 0.26600000262260437
Step: 35040, train/grad_norm: 20.887197494506836
Step: 35040, train/learning_rate: 8.305568371724803e-06
Step: 35040, train/epoch: 8.338886260986328
Step: 35050, train/loss: 0.3776000142097473
Step: 35050, train/grad_norm: 12.491477012634277
Step: 35050, train/learning_rate: 8.293669452541508e-06
Step: 35050, train/epoch: 8.341265678405762
Step: 35060, train/loss: 0.2799000144004822
Step: 35060, train/grad_norm: 8.799286842346191
Step: 35060, train/learning_rate: 8.281770533358213e-06
Step: 35060, train/epoch: 8.343646049499512
Step: 35070, train/loss: 0.25130000710487366
Step: 35070, train/grad_norm: 24.647186279296875
Step: 35070, train/learning_rate: 8.269871614174917e-06
Step: 35070, train/epoch: 8.346025466918945
Step: 35080, train/loss: 0.4797999858856201
Step: 35080, train/grad_norm: 14.738346099853516
Step: 35080, train/learning_rate: 8.257972694991622e-06
Step: 35080, train/epoch: 8.348405838012695
Step: 35090, train/loss: 0.29670000076293945
Step: 35090, train/grad_norm: 4.414031982421875
Step: 35090, train/learning_rate: 8.246072866313625e-06
Step: 35090, train/epoch: 8.350785255432129
Step: 35100, train/loss: 0.36800000071525574
Step: 35100, train/grad_norm: 8.81984806060791
Step: 35100, train/learning_rate: 8.23417394713033e-06
Step: 35100, train/epoch: 8.353165626525879
Step: 35110, train/loss: 0.30959999561309814
Step: 35110, train/grad_norm: 12.868402481079102
Step: 35110, train/learning_rate: 8.222275027947035e-06
Step: 35110, train/epoch: 8.355545043945312
Step: 35120, train/loss: 0.2808000147342682
Step: 35120, train/grad_norm: 11.149628639221191
Step: 35120, train/learning_rate: 8.21037610876374e-06
Step: 35120, train/epoch: 8.357924461364746
Step: 35130, train/loss: 0.6717000007629395
Step: 35130, train/grad_norm: 12.107503890991211
Step: 35130, train/learning_rate: 8.198477189580444e-06
Step: 35130, train/epoch: 8.360304832458496
Step: 35140, train/loss: 0.3815000057220459
Step: 35140, train/grad_norm: 9.668267250061035
Step: 35140, train/learning_rate: 8.186578270397149e-06
Step: 35140, train/epoch: 8.36268424987793
Step: 35150, train/loss: 0.3522999882698059
Step: 35150, train/grad_norm: 20.3010196685791
Step: 35150, train/learning_rate: 8.174678441719152e-06
Step: 35150, train/epoch: 8.36506462097168
Step: 35160, train/loss: 0.23160000145435333
Step: 35160, train/grad_norm: 13.54405689239502
Step: 35160, train/learning_rate: 8.162779522535857e-06
Step: 35160, train/epoch: 8.367444038391113
Step: 35170, train/loss: 0.26080000400543213
Step: 35170, train/grad_norm: 7.710753440856934
Step: 35170, train/learning_rate: 8.150880603352562e-06
Step: 35170, train/epoch: 8.369823455810547
Step: 35180, train/loss: 0.5935999751091003
Step: 35180, train/grad_norm: 19.653034210205078
Step: 35180, train/learning_rate: 8.138981684169266e-06
Step: 35180, train/epoch: 8.372203826904297
Step: 35190, train/loss: 0.3928999900817871
Step: 35190, train/grad_norm: 28.73762321472168
Step: 35190, train/learning_rate: 8.127082764985971e-06
Step: 35190, train/epoch: 8.37458324432373
Step: 35200, train/loss: 0.3605000078678131
Step: 35200, train/grad_norm: 19.687633514404297
Step: 35200, train/learning_rate: 8.115182936307974e-06
Step: 35200, train/epoch: 8.37696361541748
Step: 35210, train/loss: 0.29420000314712524
Step: 35210, train/grad_norm: 11.062788009643555
Step: 35210, train/learning_rate: 8.103284017124679e-06
Step: 35210, train/epoch: 8.379343032836914
Step: 35220, train/loss: 0.335099995136261
Step: 35220, train/grad_norm: 14.904485702514648
Step: 35220, train/learning_rate: 8.091385097941384e-06
Step: 35220, train/epoch: 8.381723403930664
Step: 35230, train/loss: 0.23989999294281006
Step: 35230, train/grad_norm: 19.095943450927734
Step: 35230, train/learning_rate: 8.079486178758088e-06
Step: 35230, train/epoch: 8.384102821350098
Step: 35240, train/loss: 0.4372999966144562
Step: 35240, train/grad_norm: 23.25986671447754
Step: 35240, train/learning_rate: 8.067587259574793e-06
Step: 35240, train/epoch: 8.386482238769531
Step: 35250, train/loss: 0.33629998564720154
Step: 35250, train/grad_norm: 20.408950805664062
Step: 35250, train/learning_rate: 8.055687430896796e-06
Step: 35250, train/epoch: 8.388862609863281
Step: 35260, train/loss: 0.2822999954223633
Step: 35260, train/grad_norm: 7.390635013580322
Step: 35260, train/learning_rate: 8.043788511713501e-06
Step: 35260, train/epoch: 8.391242027282715
Step: 35270, train/loss: 0.42829999327659607
Step: 35270, train/grad_norm: 15.392232894897461
Step: 35270, train/learning_rate: 8.031889592530206e-06
Step: 35270, train/epoch: 8.393622398376465
Step: 35280, train/loss: 0.2312999963760376
Step: 35280, train/grad_norm: 5.5455474853515625
Step: 35280, train/learning_rate: 8.01999067334691e-06
Step: 35280, train/epoch: 8.396001815795898
Step: 35290, train/loss: 0.3626999855041504
Step: 35290, train/grad_norm: 25.662818908691406
Step: 35290, train/learning_rate: 8.008091754163615e-06
Step: 35290, train/epoch: 8.398382186889648
Step: 35300, train/loss: 0.23970000445842743
Step: 35300, train/grad_norm: 8.007662773132324
Step: 35300, train/learning_rate: 7.996191925485618e-06
Step: 35300, train/epoch: 8.400761604309082
Step: 35310, train/loss: 0.29269999265670776
Step: 35310, train/grad_norm: 4.10872220993042
Step: 35310, train/learning_rate: 7.984293006302323e-06
Step: 35310, train/epoch: 8.403141021728516
Step: 35320, train/loss: 0.3091999888420105
Step: 35320, train/grad_norm: 16.30331802368164
Step: 35320, train/learning_rate: 7.972394087119028e-06
Step: 35320, train/epoch: 8.405521392822266
Step: 35330, train/loss: 0.2689000070095062
Step: 35330, train/grad_norm: 29.250755310058594
Step: 35330, train/learning_rate: 7.960495167935733e-06
Step: 35330, train/epoch: 8.4079008102417
Step: 35340, train/loss: 0.25780001282691956
Step: 35340, train/grad_norm: 3.5585830211639404
Step: 35340, train/learning_rate: 7.948596248752438e-06
Step: 35340, train/epoch: 8.41028118133545
Step: 35350, train/loss: 0.2770000100135803
Step: 35350, train/grad_norm: 9.532358169555664
Step: 35350, train/learning_rate: 7.93669642007444e-06
Step: 35350, train/epoch: 8.412660598754883
Step: 35360, train/loss: 0.27720001339912415
Step: 35360, train/grad_norm: 16.151527404785156
Step: 35360, train/learning_rate: 7.924797500891145e-06
Step: 35360, train/epoch: 8.415040016174316
Step: 35370, train/loss: 0.2540999948978424
Step: 35370, train/grad_norm: 1.4768180847167969
Step: 35370, train/learning_rate: 7.91289858170785e-06
Step: 35370, train/epoch: 8.417420387268066
Step: 35380, train/loss: 0.30169999599456787
Step: 35380, train/grad_norm: 11.836454391479492
Step: 35380, train/learning_rate: 7.900999662524555e-06
Step: 35380, train/epoch: 8.4197998046875
Step: 35390, train/loss: 0.5090000033378601
Step: 35390, train/grad_norm: 12.029956817626953
Step: 35390, train/learning_rate: 7.88910074334126e-06
Step: 35390, train/epoch: 8.42218017578125
Step: 35400, train/loss: 0.2142000049352646
Step: 35400, train/grad_norm: 3.949007749557495
Step: 35400, train/learning_rate: 7.877200914663263e-06
Step: 35400, train/epoch: 8.424559593200684
Step: 35410, train/loss: 0.34689998626708984
Step: 35410, train/grad_norm: 10.671350479125977
Step: 35410, train/learning_rate: 7.865301995479967e-06
Step: 35410, train/epoch: 8.426939964294434
Step: 35420, train/loss: 0.506600022315979
Step: 35420, train/grad_norm: 7.782988548278809
Step: 35420, train/learning_rate: 7.853403076296672e-06
Step: 35420, train/epoch: 8.429319381713867
Step: 35430, train/loss: 0.39010000228881836
Step: 35430, train/grad_norm: 1.559395432472229
Step: 35430, train/learning_rate: 7.841504157113377e-06
Step: 35430, train/epoch: 8.4316987991333
Step: 35440, train/loss: 0.2395000010728836
Step: 35440, train/grad_norm: 3.4220855236053467
Step: 35440, train/learning_rate: 7.829605237930082e-06
Step: 35440, train/epoch: 8.43407917022705
Step: 35450, train/loss: 0.31869998574256897
Step: 35450, train/grad_norm: 2.802809000015259
Step: 35450, train/learning_rate: 7.817705409252085e-06
Step: 35450, train/epoch: 8.436458587646484
Step: 35460, train/loss: 0.3391999900341034
Step: 35460, train/grad_norm: 18.565383911132812
Step: 35460, train/learning_rate: 7.80580649006879e-06
Step: 35460, train/epoch: 8.438838958740234
Step: 35470, train/loss: 0.3140999972820282
Step: 35470, train/grad_norm: 2.474138021469116
Step: 35470, train/learning_rate: 7.793907570885494e-06
Step: 35470, train/epoch: 8.441218376159668
Step: 35480, train/loss: 0.27570000290870667
Step: 35480, train/grad_norm: 19.752534866333008
Step: 35480, train/learning_rate: 7.782008651702199e-06
Step: 35480, train/epoch: 8.443598747253418
Step: 35490, train/loss: 0.26840001344680786
Step: 35490, train/grad_norm: 16.827077865600586
Step: 35490, train/learning_rate: 7.770109732518904e-06
Step: 35490, train/epoch: 8.445978164672852
Step: 35500, train/loss: 0.28139999508857727
Step: 35500, train/grad_norm: 1.3834989070892334
Step: 35500, train/learning_rate: 7.758210813335609e-06
Step: 35500, train/epoch: 8.448357582092285
Step: 35510, train/loss: 0.28380000591278076
Step: 35510, train/grad_norm: 18.975305557250977
Step: 35510, train/learning_rate: 7.746310984657612e-06
Step: 35510, train/epoch: 8.450737953186035
Step: 35520, train/loss: 0.27140000462532043
Step: 35520, train/grad_norm: 10.025613784790039
Step: 35520, train/learning_rate: 7.734412065474316e-06
Step: 35520, train/epoch: 8.453117370605469
Step: 35530, train/loss: 0.4659999907016754
Step: 35530, train/grad_norm: 17.224390029907227
Step: 35530, train/learning_rate: 7.722513146291021e-06
Step: 35530, train/epoch: 8.455497741699219
Step: 35540, train/loss: 0.42899999022483826
Step: 35540, train/grad_norm: 11.86397647857666
Step: 35540, train/learning_rate: 7.710614227107726e-06
Step: 35540, train/epoch: 8.457877159118652
Step: 35550, train/loss: 0.23680000007152557
Step: 35550, train/grad_norm: 16.326398849487305
Step: 35550, train/learning_rate: 7.69871530792443e-06
Step: 35550, train/epoch: 8.460256576538086
Step: 35560, train/loss: 0.4083000123500824
Step: 35560, train/grad_norm: 26.887451171875
Step: 35560, train/learning_rate: 7.686815479246434e-06
Step: 35560, train/epoch: 8.462636947631836
Step: 35570, train/loss: 0.2897000014781952
Step: 35570, train/grad_norm: 10.283501625061035
Step: 35570, train/learning_rate: 7.674916560063139e-06
Step: 35570, train/epoch: 8.46501636505127
Step: 35580, train/loss: 0.2797999978065491
Step: 35580, train/grad_norm: 17.848249435424805
Step: 35580, train/learning_rate: 7.663017640879843e-06
Step: 35580, train/epoch: 8.46739673614502
Step: 35590, train/loss: 0.3774000108242035
Step: 35590, train/grad_norm: 23.46110725402832
Step: 35590, train/learning_rate: 7.651118721696548e-06
Step: 35590, train/epoch: 8.469776153564453
Step: 35600, train/loss: 0.16329999268054962
Step: 35600, train/grad_norm: 7.954137325286865
Step: 35600, train/learning_rate: 7.639219802513253e-06
Step: 35600, train/epoch: 8.472156524658203
Step: 35610, train/loss: 0.2912999987602234
Step: 35610, train/grad_norm: 12.725687026977539
Step: 35610, train/learning_rate: 7.627320428582607e-06
Step: 35610, train/epoch: 8.474535942077637
Step: 35620, train/loss: 0.3303999900817871
Step: 35620, train/grad_norm: 52.278926849365234
Step: 35620, train/learning_rate: 7.615421054651961e-06
Step: 35620, train/epoch: 8.47691535949707
Step: 35630, train/loss: 0.2809999883174896
Step: 35630, train/grad_norm: 5.49157190322876
Step: 35630, train/learning_rate: 7.6035221354686655e-06
Step: 35630, train/epoch: 8.47929573059082
Step: 35640, train/loss: 0.328000009059906
Step: 35640, train/grad_norm: 21.94247817993164
Step: 35640, train/learning_rate: 7.59162321628537e-06
Step: 35640, train/epoch: 8.481675148010254
Step: 35650, train/loss: 0.5479999780654907
Step: 35650, train/grad_norm: 41.20073318481445
Step: 35650, train/learning_rate: 7.579723842354724e-06
Step: 35650, train/epoch: 8.484055519104004
Step: 35660, train/loss: 0.20020000636577606
Step: 35660, train/grad_norm: 2.5039570331573486
Step: 35660, train/learning_rate: 7.567824923171429e-06
Step: 35660, train/epoch: 8.486434936523438
Step: 35670, train/loss: 0.24979999661445618
Step: 35670, train/grad_norm: 6.573700428009033
Step: 35670, train/learning_rate: 7.555925549240783e-06
Step: 35670, train/epoch: 8.488815307617188
Step: 35680, train/loss: 0.30329999327659607
Step: 35680, train/grad_norm: 6.5018839836120605
Step: 35680, train/learning_rate: 7.544026630057488e-06
Step: 35680, train/epoch: 8.491194725036621
Step: 35690, train/loss: 0.30640000104904175
Step: 35690, train/grad_norm: 3.699981451034546
Step: 35690, train/learning_rate: 7.532127710874192e-06
Step: 35690, train/epoch: 8.493574142456055
Step: 35700, train/loss: 0.4016999900341034
Step: 35700, train/grad_norm: 13.115324974060059
Step: 35700, train/learning_rate: 7.520228336943546e-06
Step: 35700, train/epoch: 8.495954513549805
Step: 35710, train/loss: 0.3675000071525574
Step: 35710, train/grad_norm: 7.217461585998535
Step: 35710, train/learning_rate: 7.508329417760251e-06
Step: 35710, train/epoch: 8.498333930969238
Step: 35720, train/loss: 0.22310000658035278
Step: 35720, train/grad_norm: 14.406624794006348
Step: 35720, train/learning_rate: 7.496430498576956e-06
Step: 35720, train/epoch: 8.500714302062988
Step: 35730, train/loss: 0.3603000044822693
Step: 35730, train/grad_norm: 6.348540306091309
Step: 35730, train/learning_rate: 7.48453112464631e-06
Step: 35730, train/epoch: 8.503093719482422
Step: 35740, train/loss: 0.3077999949455261
Step: 35740, train/grad_norm: 4.420295715332031
Step: 35740, train/learning_rate: 7.4726322054630145e-06
Step: 35740, train/epoch: 8.505473136901855
Step: 35750, train/loss: 0.2567000091075897
Step: 35750, train/grad_norm: 14.382288932800293
Step: 35750, train/learning_rate: 7.4607328315323684e-06
Step: 35750, train/epoch: 8.507853507995605
Step: 35760, train/loss: 0.2976999878883362
Step: 35760, train/grad_norm: 4.090219497680664
Step: 35760, train/learning_rate: 7.448833912349073e-06
Step: 35760, train/epoch: 8.510232925415039
Step: 35770, train/loss: 0.18389999866485596
Step: 35770, train/grad_norm: 3.255718469619751
Step: 35770, train/learning_rate: 7.436934993165778e-06
Step: 35770, train/epoch: 8.512613296508789
Step: 35780, train/loss: 0.4560999870300293
Step: 35780, train/grad_norm: 5.139166831970215
Step: 35780, train/learning_rate: 7.425035619235132e-06
Step: 35780, train/epoch: 8.514992713928223
Step: 35790, train/loss: 0.38830000162124634
Step: 35790, train/grad_norm: 15.700389862060547
Step: 35790, train/learning_rate: 7.413136700051837e-06
Step: 35790, train/epoch: 8.517373085021973
Step: 35800, train/loss: 0.21770000457763672
Step: 35800, train/grad_norm: 16.99498748779297
Step: 35800, train/learning_rate: 7.4012373261211906e-06
Step: 35800, train/epoch: 8.519752502441406
Step: 35810, train/loss: 0.3091999888420105
Step: 35810, train/grad_norm: 16.745023727416992
Step: 35810, train/learning_rate: 7.389338406937895e-06
Step: 35810, train/epoch: 8.52213191986084
Step: 35820, train/loss: 0.2328999936580658
Step: 35820, train/grad_norm: 24.29291343688965
Step: 35820, train/learning_rate: 7.3774394877546e-06
Step: 35820, train/epoch: 8.52451229095459
Step: 35830, train/loss: 0.3149000108242035
Step: 35830, train/grad_norm: 25.0675048828125
Step: 35830, train/learning_rate: 7.365540113823954e-06
Step: 35830, train/epoch: 8.526891708374023
Step: 35840, train/loss: 0.2619999945163727
Step: 35840, train/grad_norm: 3.791921615600586
Step: 35840, train/learning_rate: 7.353641194640659e-06
Step: 35840, train/epoch: 8.529272079467773
Step: 35850, train/loss: 0.5554999709129333
Step: 35850, train/grad_norm: 18.87440299987793
Step: 35850, train/learning_rate: 7.341741820710013e-06
Step: 35850, train/epoch: 8.531651496887207
Step: 35860, train/loss: 0.31450000405311584
Step: 35860, train/grad_norm: 15.930889129638672
Step: 35860, train/learning_rate: 7.3298429015267175e-06
Step: 35860, train/epoch: 8.534031867980957
Step: 35870, train/loss: 0.40059998631477356
Step: 35870, train/grad_norm: 25.65523338317871
Step: 35870, train/learning_rate: 7.317943982343422e-06
Step: 35870, train/epoch: 8.53641128540039
Step: 35880, train/loss: 0.33340001106262207
Step: 35880, train/grad_norm: 20.20916748046875
Step: 35880, train/learning_rate: 7.306044608412776e-06
Step: 35880, train/epoch: 8.538790702819824
Step: 35890, train/loss: 0.4334999918937683
Step: 35890, train/grad_norm: 8.383940696716309
Step: 35890, train/learning_rate: 7.294145689229481e-06
Step: 35890, train/epoch: 8.541171073913574
Step: 35900, train/loss: 0.3840999901294708
Step: 35900, train/grad_norm: 21.92089080810547
Step: 35900, train/learning_rate: 7.282246770046186e-06
Step: 35900, train/epoch: 8.543550491333008
Step: 35910, train/loss: 0.2687000036239624
Step: 35910, train/grad_norm: 15.294407844543457
Step: 35910, train/learning_rate: 7.27034739611554e-06
Step: 35910, train/epoch: 8.545930862426758
Step: 35920, train/loss: 0.27149999141693115
Step: 35920, train/grad_norm: 16.037500381469727
Step: 35920, train/learning_rate: 7.258448476932244e-06
Step: 35920, train/epoch: 8.548310279846191
Step: 35930, train/loss: 0.4000000059604645
Step: 35930, train/grad_norm: 10.406932830810547
Step: 35930, train/learning_rate: 7.246549103001598e-06
Step: 35930, train/epoch: 8.550689697265625
Step: 35940, train/loss: 0.3125
Step: 35940, train/grad_norm: 24.282564163208008
Step: 35940, train/learning_rate: 7.234650183818303e-06
Step: 35940, train/epoch: 8.553070068359375
Step: 35950, train/loss: 0.38999998569488525
Step: 35950, train/grad_norm: 16.439237594604492
Step: 35950, train/learning_rate: 7.222751264635008e-06
Step: 35950, train/epoch: 8.555449485778809
Step: 35960, train/loss: 0.1987999975681305
Step: 35960, train/grad_norm: 2.105985641479492
Step: 35960, train/learning_rate: 7.210851890704362e-06
Step: 35960, train/epoch: 8.557829856872559
Step: 35970, train/loss: 0.31299999356269836
Step: 35970, train/grad_norm: 26.316997528076172
Step: 35970, train/learning_rate: 7.1989529715210665e-06
Step: 35970, train/epoch: 8.560209274291992
Step: 35980, train/loss: 0.40950000286102295
Step: 35980, train/grad_norm: 21.990060806274414
Step: 35980, train/learning_rate: 7.18705359759042e-06
Step: 35980, train/epoch: 8.562589645385742
Step: 35990, train/loss: 0.350600004196167
Step: 35990, train/grad_norm: 19.10536766052246
Step: 35990, train/learning_rate: 7.175154678407125e-06
Step: 35990, train/epoch: 8.564969062805176
Step: 36000, train/loss: 0.49480000138282776
Step: 36000, train/grad_norm: 13.792162895202637
Step: 36000, train/learning_rate: 7.16325575922383e-06
Step: 36000, train/epoch: 8.56734848022461
Step: 36010, train/loss: 0.274399995803833
Step: 36010, train/grad_norm: 22.255878448486328
Step: 36010, train/learning_rate: 7.151356385293184e-06
Step: 36010, train/epoch: 8.56972885131836
Step: 36020, train/loss: 0.43160000443458557
Step: 36020, train/grad_norm: 29.99123191833496
Step: 36020, train/learning_rate: 7.139457466109889e-06
Step: 36020, train/epoch: 8.572108268737793
Step: 36030, train/loss: 0.26339998841285706
Step: 36030, train/grad_norm: 18.88750648498535
Step: 36030, train/learning_rate: 7.1275580921792425e-06
Step: 36030, train/epoch: 8.574488639831543
Step: 36040, train/loss: 0.31279999017715454
Step: 36040, train/grad_norm: 3.3790547847747803
Step: 36040, train/learning_rate: 7.115659172995947e-06
Step: 36040, train/epoch: 8.576868057250977
Step: 36050, train/loss: 0.3788999915122986
Step: 36050, train/grad_norm: 16.398395538330078
Step: 36050, train/learning_rate: 7.103760253812652e-06
Step: 36050, train/epoch: 8.579248428344727
Step: 36060, train/loss: 0.3619999885559082
Step: 36060, train/grad_norm: 3.266538619995117
Step: 36060, train/learning_rate: 7.091860879882006e-06
Step: 36060, train/epoch: 8.58162784576416
Step: 36070, train/loss: 0.33959999680519104
Step: 36070, train/grad_norm: 12.202160835266113
Step: 36070, train/learning_rate: 7.079961960698711e-06
Step: 36070, train/epoch: 8.584007263183594
Step: 36080, train/loss: 0.48350000381469727
Step: 36080, train/grad_norm: 1.614803433418274
Step: 36080, train/learning_rate: 7.0680630415154155e-06
Step: 36080, train/epoch: 8.586387634277344
Step: 36090, train/loss: 0.35010001063346863
Step: 36090, train/grad_norm: 17.477237701416016
Step: 36090, train/learning_rate: 7.0561636675847694e-06
Step: 36090, train/epoch: 8.588767051696777
Step: 36100, train/loss: 0.2838999927043915
Step: 36100, train/grad_norm: 10.81320571899414
Step: 36100, train/learning_rate: 7.044264748401474e-06
Step: 36100, train/epoch: 8.591147422790527
Step: 36110, train/loss: 0.4507000148296356
Step: 36110, train/grad_norm: 27.28829002380371
Step: 36110, train/learning_rate: 7.032365374470828e-06
Step: 36110, train/epoch: 8.593526840209961
Step: 36120, train/loss: 0.3472999930381775
Step: 36120, train/grad_norm: 33.226749420166016
Step: 36120, train/learning_rate: 7.020466455287533e-06
Step: 36120, train/epoch: 8.595906257629395
Step: 36130, train/loss: 0.1274999976158142
Step: 36130, train/grad_norm: 7.355053901672363
Step: 36130, train/learning_rate: 7.008567536104238e-06
Step: 36130, train/epoch: 8.598286628723145
Step: 36140, train/loss: 0.2572000026702881
Step: 36140, train/grad_norm: 17.77169418334961
Step: 36140, train/learning_rate: 6.9966681621735916e-06
Step: 36140, train/epoch: 8.600666046142578
Step: 36150, train/loss: 0.27219998836517334
Step: 36150, train/grad_norm: 6.528645038604736
Step: 36150, train/learning_rate: 6.984769242990296e-06
Step: 36150, train/epoch: 8.603046417236328
Step: 36160, train/loss: 0.37540000677108765
Step: 36160, train/grad_norm: 20.383390426635742
Step: 36160, train/learning_rate: 6.97286986905965e-06
Step: 36160, train/epoch: 8.605425834655762
Step: 36170, train/loss: 0.5370000004768372
Step: 36170, train/grad_norm: 23.79497718811035
Step: 36170, train/learning_rate: 6.960970949876355e-06
Step: 36170, train/epoch: 8.607806205749512
Step: 36180, train/loss: 0.3260999917984009
Step: 36180, train/grad_norm: 5.376153469085693
Step: 36180, train/learning_rate: 6.94907203069306e-06
Step: 36180, train/epoch: 8.610185623168945
Step: 36190, train/loss: 0.2635999917984009
Step: 36190, train/grad_norm: 5.099952220916748
Step: 36190, train/learning_rate: 6.937172656762414e-06
Step: 36190, train/epoch: 8.612565040588379
Step: 36200, train/loss: 0.37040001153945923
Step: 36200, train/grad_norm: 7.115636825561523
Step: 36200, train/learning_rate: 6.9252737375791185e-06
Step: 36200, train/epoch: 8.614945411682129
Step: 36210, train/loss: 0.3961000144481659
Step: 36210, train/grad_norm: 23.22378158569336
Step: 36210, train/learning_rate: 6.913374363648472e-06
Step: 36210, train/epoch: 8.617324829101562
Step: 36220, train/loss: 0.3894999921321869
Step: 36220, train/grad_norm: 7.502401828765869
Step: 36220, train/learning_rate: 6.901475444465177e-06
Step: 36220, train/epoch: 8.619705200195312
Step: 36230, train/loss: 0.18279999494552612
Step: 36230, train/grad_norm: 7.5852861404418945
Step: 36230, train/learning_rate: 6.889576525281882e-06
Step: 36230, train/epoch: 8.622084617614746
Step: 36240, train/loss: 0.365200012922287
Step: 36240, train/grad_norm: 5.380248546600342
Step: 36240, train/learning_rate: 6.877677151351236e-06
Step: 36240, train/epoch: 8.624464988708496
Step: 36250, train/loss: 0.4810999929904938
Step: 36250, train/grad_norm: 8.25828742980957
Step: 36250, train/learning_rate: 6.865778232167941e-06
Step: 36250, train/epoch: 8.62684440612793
Step: 36260, train/loss: 0.34689998626708984
Step: 36260, train/grad_norm: 18.68490219116211
Step: 36260, train/learning_rate: 6.853879312984645e-06
Step: 36260, train/epoch: 8.629223823547363
Step: 36270, train/loss: 0.24490000307559967
Step: 36270, train/grad_norm: 8.59316635131836
Step: 36270, train/learning_rate: 6.841979939053999e-06
Step: 36270, train/epoch: 8.631604194641113
Step: 36280, train/loss: 0.33180001378059387
Step: 36280, train/grad_norm: 23.654621124267578
Step: 36280, train/learning_rate: 6.830081019870704e-06
Step: 36280, train/epoch: 8.633983612060547
Step: 36290, train/loss: 0.18230000138282776
Step: 36290, train/grad_norm: 5.821232795715332
Step: 36290, train/learning_rate: 6.818181645940058e-06
Step: 36290, train/epoch: 8.636363983154297
Step: 36300, train/loss: 0.3465000092983246
Step: 36300, train/grad_norm: 13.342934608459473
Step: 36300, train/learning_rate: 6.806282726756763e-06
Step: 36300, train/epoch: 8.63874340057373
Step: 36310, train/loss: 0.4429999887943268
Step: 36310, train/grad_norm: 33.08018112182617
Step: 36310, train/learning_rate: 6.7943838075734675e-06
Step: 36310, train/epoch: 8.641122817993164
Step: 36320, train/loss: 0.2711000144481659
Step: 36320, train/grad_norm: 3.422513246536255
Step: 36320, train/learning_rate: 6.782484433642821e-06
Step: 36320, train/epoch: 8.643503189086914
Step: 36330, train/loss: 0.22110000252723694
Step: 36330, train/grad_norm: 9.994699478149414
Step: 36330, train/learning_rate: 6.770585514459526e-06
Step: 36330, train/epoch: 8.645882606506348
Step: 36340, train/loss: 0.22259999811649323
Step: 36340, train/grad_norm: 0.9480544328689575
Step: 36340, train/learning_rate: 6.75868614052888e-06
Step: 36340, train/epoch: 8.648262977600098
Step: 36350, train/loss: 0.35249999165534973
Step: 36350, train/grad_norm: 20.31863021850586
Step: 36350, train/learning_rate: 6.746787221345585e-06
Step: 36350, train/epoch: 8.650642395019531
Step: 36360, train/loss: 0.4690000116825104
Step: 36360, train/grad_norm: 21.166553497314453
Step: 36360, train/learning_rate: 6.73488830216229e-06
Step: 36360, train/epoch: 8.653022766113281
Step: 36370, train/loss: 0.3483000099658966
Step: 36370, train/grad_norm: 11.505714416503906
Step: 36370, train/learning_rate: 6.7229889282316435e-06
Step: 36370, train/epoch: 8.655402183532715
Step: 36380, train/loss: 0.3257000148296356
Step: 36380, train/grad_norm: 9.163313865661621
Step: 36380, train/learning_rate: 6.711090009048348e-06
Step: 36380, train/epoch: 8.657781600952148
Step: 36390, train/loss: 0.3183000087738037
Step: 36390, train/grad_norm: 27.489200592041016
Step: 36390, train/learning_rate: 6.699190635117702e-06
Step: 36390, train/epoch: 8.660161972045898
Step: 36400, train/loss: 0.2896000146865845
Step: 36400, train/grad_norm: 10.253096580505371
Step: 36400, train/learning_rate: 6.687291715934407e-06
Step: 36400, train/epoch: 8.662541389465332
Step: 36410, train/loss: 0.24250000715255737
Step: 36410, train/grad_norm: 8.510936737060547
Step: 36410, train/learning_rate: 6.675392796751112e-06
Step: 36410, train/epoch: 8.664921760559082
Step: 36420, train/loss: 0.32690000534057617
Step: 36420, train/grad_norm: 13.53259563446045
Step: 36420, train/learning_rate: 6.663493422820466e-06
Step: 36420, train/epoch: 8.667301177978516
Step: 36430, train/loss: 0.3725000023841858
Step: 36430, train/grad_norm: 9.788263320922852
Step: 36430, train/learning_rate: 6.65159450363717e-06
Step: 36430, train/epoch: 8.669681549072266
Step: 36440, train/loss: 0.3677999973297119
Step: 36440, train/grad_norm: 12.887786865234375
Step: 36440, train/learning_rate: 6.639695584453875e-06
Step: 36440, train/epoch: 8.6720609664917
Step: 36450, train/loss: 0.47049999237060547
Step: 36450, train/grad_norm: 16.926715850830078
Step: 36450, train/learning_rate: 6.627796210523229e-06
Step: 36450, train/epoch: 8.674440383911133
Step: 36460, train/loss: 0.373199999332428
Step: 36460, train/grad_norm: 5.9455485343933105
Step: 36460, train/learning_rate: 6.615897291339934e-06
Step: 36460, train/epoch: 8.676820755004883
Step: 36470, train/loss: 0.2304999977350235
Step: 36470, train/grad_norm: 2.5117835998535156
Step: 36470, train/learning_rate: 6.603997917409288e-06
Step: 36470, train/epoch: 8.679200172424316
Step: 36480, train/loss: 0.27390000224113464
Step: 36480, train/grad_norm: 17.31572151184082
Step: 36480, train/learning_rate: 6.5920989982259925e-06
Step: 36480, train/epoch: 8.681580543518066
Step: 36490, train/loss: 0.25949999690055847
Step: 36490, train/grad_norm: 10.809494018554688
Step: 36490, train/learning_rate: 6.580200079042697e-06
Step: 36490, train/epoch: 8.6839599609375
Step: 36500, train/loss: 0.41110000014305115
Step: 36500, train/grad_norm: 5.218407154083252
Step: 36500, train/learning_rate: 6.568300705112051e-06
Step: 36500, train/epoch: 8.686339378356934
Step: 36510, train/loss: 0.3107999861240387
Step: 36510, train/grad_norm: 29.068790435791016
Step: 36510, train/learning_rate: 6.556401785928756e-06
Step: 36510, train/epoch: 8.688719749450684
Step: 36520, train/loss: 0.4447999894618988
Step: 36520, train/grad_norm: 16.016780853271484
Step: 36520, train/learning_rate: 6.54450241199811e-06
Step: 36520, train/epoch: 8.691099166870117
Step: 36530, train/loss: 0.3862999975681305
Step: 36530, train/grad_norm: 15.050882339477539
Step: 36530, train/learning_rate: 6.532603492814815e-06
Step: 36530, train/epoch: 8.693479537963867
Step: 36540, train/loss: 0.41280001401901245
Step: 36540, train/grad_norm: 34.178382873535156
Step: 36540, train/learning_rate: 6.5207045736315195e-06
Step: 36540, train/epoch: 8.6958589553833
Step: 36550, train/loss: 0.3310999870300293
Step: 36550, train/grad_norm: 16.745372772216797
Step: 36550, train/learning_rate: 6.508805199700873e-06
Step: 36550, train/epoch: 8.69823932647705
Step: 36560, train/loss: 0.3504999876022339
Step: 36560, train/grad_norm: 11.969239234924316
Step: 36560, train/learning_rate: 6.496906280517578e-06
Step: 36560, train/epoch: 8.700618743896484
Step: 36570, train/loss: 0.44670000672340393
Step: 36570, train/grad_norm: 12.204974174499512
Step: 36570, train/learning_rate: 6.485007361334283e-06
Step: 36570, train/epoch: 8.702998161315918
Step: 36580, train/loss: 0.3303999900817871
Step: 36580, train/grad_norm: 4.548890590667725
Step: 36580, train/learning_rate: 6.473107987403637e-06
Step: 36580, train/epoch: 8.705378532409668
Step: 36590, train/loss: 0.27880001068115234
Step: 36590, train/grad_norm: 3.4066662788391113
Step: 36590, train/learning_rate: 6.461209068220342e-06
Step: 36590, train/epoch: 8.707757949829102
Step: 36600, train/loss: 0.3188999891281128
Step: 36600, train/grad_norm: 21.18035888671875
Step: 36600, train/learning_rate: 6.4493096942896955e-06
Step: 36600, train/epoch: 8.710138320922852
Step: 36610, train/loss: 0.5390999913215637
Step: 36610, train/grad_norm: 11.193796157836914
Step: 36610, train/learning_rate: 6.4374107751064e-06
Step: 36610, train/epoch: 8.712517738342285
Step: 36620, train/loss: 0.2612999975681305
Step: 36620, train/grad_norm: 12.819114685058594
Step: 36620, train/learning_rate: 6.425511855923105e-06
Step: 36620, train/epoch: 8.714898109436035
Step: 36630, train/loss: 0.2743000090122223
Step: 36630, train/grad_norm: 7.667869567871094
Step: 36630, train/learning_rate: 6.413612481992459e-06
Step: 36630, train/epoch: 8.717277526855469
Step: 36640, train/loss: 0.48489999771118164
Step: 36640, train/grad_norm: 18.041427612304688
Step: 36640, train/learning_rate: 6.401713562809164e-06
Step: 36640, train/epoch: 8.719656944274902
Step: 36650, train/loss: 0.32359999418258667
Step: 36650, train/grad_norm: 7.820143222808838
Step: 36650, train/learning_rate: 6.389814188878518e-06
Step: 36650, train/epoch: 8.722037315368652
Step: 36660, train/loss: 0.5220000147819519
Step: 36660, train/grad_norm: 8.365152359008789
Step: 36660, train/learning_rate: 6.377915269695222e-06
Step: 36660, train/epoch: 8.724416732788086
Step: 36670, train/loss: 0.25380000472068787
Step: 36670, train/grad_norm: 21.359024047851562
Step: 36670, train/learning_rate: 6.366016350511927e-06
Step: 36670, train/epoch: 8.726797103881836
Step: 36680, train/loss: 0.4101000130176544
Step: 36680, train/grad_norm: 5.212311744689941
Step: 36680, train/learning_rate: 6.354116976581281e-06
Step: 36680, train/epoch: 8.72917652130127
Step: 36690, train/loss: 0.4255000054836273
Step: 36690, train/grad_norm: 16.422161102294922
Step: 36690, train/learning_rate: 6.342218057397986e-06
Step: 36690, train/epoch: 8.731555938720703
Step: 36700, train/loss: 0.41179999709129333
Step: 36700, train/grad_norm: 14.752408981323242
Step: 36700, train/learning_rate: 6.33031868346734e-06
Step: 36700, train/epoch: 8.733936309814453
Step: 36710, train/loss: 0.4449999928474426
Step: 36710, train/grad_norm: 11.469453811645508
Step: 36710, train/learning_rate: 6.3184197642840445e-06
Step: 36710, train/epoch: 8.736315727233887
Step: 36720, train/loss: 0.18970000743865967
Step: 36720, train/grad_norm: 12.841318130493164
Step: 36720, train/learning_rate: 6.306520845100749e-06
Step: 36720, train/epoch: 8.738696098327637
Step: 36730, train/loss: 0.32260000705718994
Step: 36730, train/grad_norm: 26.494070053100586
Step: 36730, train/learning_rate: 6.294621471170103e-06
Step: 36730, train/epoch: 8.74107551574707
Step: 36740, train/loss: 0.34360000491142273
Step: 36740, train/grad_norm: 2.937840223312378
Step: 36740, train/learning_rate: 6.282722551986808e-06
Step: 36740, train/epoch: 8.74345588684082
Step: 36750, train/loss: 0.27639999985694885
Step: 36750, train/grad_norm: 16.204084396362305
Step: 36750, train/learning_rate: 6.270823632803513e-06
Step: 36750, train/epoch: 8.745835304260254
Step: 36760, train/loss: 0.46459999680519104
Step: 36760, train/grad_norm: 11.853737831115723
Step: 36760, train/learning_rate: 6.258924258872867e-06
Step: 36760, train/epoch: 8.748214721679688
Step: 36770, train/loss: 0.3587000072002411
Step: 36770, train/grad_norm: 12.68066120147705
Step: 36770, train/learning_rate: 6.247025339689571e-06
Step: 36770, train/epoch: 8.750595092773438
Step: 36780, train/loss: 0.5436999797821045
Step: 36780, train/grad_norm: 13.043403625488281
Step: 36780, train/learning_rate: 6.235125965758925e-06
Step: 36780, train/epoch: 8.752974510192871
Step: 36790, train/loss: 0.1607999950647354
Step: 36790, train/grad_norm: 4.3201398849487305
Step: 36790, train/learning_rate: 6.22322704657563e-06
Step: 36790, train/epoch: 8.755354881286621
Step: 36800, train/loss: 0.23880000412464142
Step: 36800, train/grad_norm: 19.3547306060791
Step: 36800, train/learning_rate: 6.211328127392335e-06
Step: 36800, train/epoch: 8.757734298706055
Step: 36810, train/loss: 0.250900000333786
Step: 36810, train/grad_norm: 15.321900367736816
Step: 36810, train/learning_rate: 6.199428753461689e-06
Step: 36810, train/epoch: 8.760114669799805
Step: 36820, train/loss: 0.41929998993873596
Step: 36820, train/grad_norm: 10.10668659210205
Step: 36820, train/learning_rate: 6.1875298342783935e-06
Step: 36820, train/epoch: 8.762494087219238
Step: 36830, train/loss: 0.43230000138282776
Step: 36830, train/grad_norm: 8.154150009155273
Step: 36830, train/learning_rate: 6.1756304603477474e-06
Step: 36830, train/epoch: 8.764873504638672
Step: 36840, train/loss: 0.2653000056743622
Step: 36840, train/grad_norm: 11.779858589172363
Step: 36840, train/learning_rate: 6.163731541164452e-06
Step: 36840, train/epoch: 8.767253875732422
Step: 36850, train/loss: 0.5144000053405762
Step: 36850, train/grad_norm: 21.251888275146484
Step: 36850, train/learning_rate: 6.151832621981157e-06
Step: 36850, train/epoch: 8.769633293151855
Step: 36860, train/loss: 0.4120999872684479
Step: 36860, train/grad_norm: 13.841355323791504
Step: 36860, train/learning_rate: 6.139933248050511e-06
Step: 36860, train/epoch: 8.772013664245605
Step: 36870, train/loss: 0.3540000021457672
Step: 36870, train/grad_norm: 5.410852909088135
Step: 36870, train/learning_rate: 6.128034328867216e-06
Step: 36870, train/epoch: 8.774393081665039
Step: 36880, train/loss: 0.31040000915527344
Step: 36880, train/grad_norm: 15.71345043182373
Step: 36880, train/learning_rate: 6.1161349549365696e-06
Step: 36880, train/epoch: 8.776772499084473
Step: 36890, train/loss: 0.3061999976634979
Step: 36890, train/grad_norm: 3.1984219551086426
Step: 36890, train/learning_rate: 6.104236035753274e-06
Step: 36890, train/epoch: 8.779152870178223
Step: 36900, train/loss: 0.16609999537467957
Step: 36900, train/grad_norm: 15.300662994384766
Step: 36900, train/learning_rate: 6.092337116569979e-06
Step: 36900, train/epoch: 8.781532287597656
Step: 36910, train/loss: 0.28610000014305115
Step: 36910, train/grad_norm: 7.6769561767578125
Step: 36910, train/learning_rate: 6.080437742639333e-06
Step: 36910, train/epoch: 8.783912658691406
Step: 36920, train/loss: 0.35659998655319214
Step: 36920, train/grad_norm: 6.961372375488281
Step: 36920, train/learning_rate: 6.068538823456038e-06
Step: 36920, train/epoch: 8.78629207611084
Step: 36930, train/loss: 0.3244999945163727
Step: 36930, train/grad_norm: 8.464327812194824
Step: 36930, train/learning_rate: 6.0566399042727426e-06
Step: 36930, train/epoch: 8.78867244720459
Step: 36940, train/loss: 0.23729999363422394
Step: 36940, train/grad_norm: 1.2396589517593384
Step: 36940, train/learning_rate: 6.0447405303420965e-06
Step: 36940, train/epoch: 8.791051864624023
Step: 36950, train/loss: 0.16369999945163727
Step: 36950, train/grad_norm: 0.8474202156066895
Step: 36950, train/learning_rate: 6.032841611158801e-06
Step: 36950, train/epoch: 8.793431282043457
Step: 36960, train/loss: 0.3549000024795532
Step: 36960, train/grad_norm: 7.160940647125244
Step: 36960, train/learning_rate: 6.020942237228155e-06
Step: 36960, train/epoch: 8.795811653137207
Step: 36970, train/loss: 0.2515999972820282
Step: 36970, train/grad_norm: 5.767456531524658
Step: 36970, train/learning_rate: 6.00904331804486e-06
Step: 36970, train/epoch: 8.79819107055664
Step: 36980, train/loss: 0.2818000018596649
Step: 36980, train/grad_norm: 7.7285027503967285
Step: 36980, train/learning_rate: 5.997144398861565e-06
Step: 36980, train/epoch: 8.80057144165039
Step: 36990, train/loss: 0.385699987411499
Step: 36990, train/grad_norm: 6.387509346008301
Step: 36990, train/learning_rate: 5.985245024930919e-06
Step: 36990, train/epoch: 8.802950859069824
Step: 37000, train/loss: 0.2345999926328659
Step: 37000, train/grad_norm: 16.21023941040039
Step: 37000, train/learning_rate: 5.973346105747623e-06
Step: 37000, train/epoch: 8.805331230163574
Step: 37010, train/loss: 0.31610000133514404
Step: 37010, train/grad_norm: 16.172306060791016
Step: 37010, train/learning_rate: 5.961446731816977e-06
Step: 37010, train/epoch: 8.807710647583008
Step: 37020, train/loss: 0.3693999946117401
Step: 37020, train/grad_norm: 9.906648635864258
Step: 37020, train/learning_rate: 5.949547812633682e-06
Step: 37020, train/epoch: 8.810090065002441
Step: 37030, train/loss: 0.33070001006126404
Step: 37030, train/grad_norm: 30.77704429626465
Step: 37030, train/learning_rate: 5.937648893450387e-06
Step: 37030, train/epoch: 8.812470436096191
Step: 37040, train/loss: 0.3179999887943268
Step: 37040, train/grad_norm: 9.991633415222168
Step: 37040, train/learning_rate: 5.925749519519741e-06
Step: 37040, train/epoch: 8.814849853515625
Step: 37050, train/loss: 0.3732999861240387
Step: 37050, train/grad_norm: 0.9117050170898438
Step: 37050, train/learning_rate: 5.9138506003364455e-06
Step: 37050, train/epoch: 8.817230224609375
Step: 37060, train/loss: 0.41449999809265137
Step: 37060, train/grad_norm: 35.38318634033203
Step: 37060, train/learning_rate: 5.901951226405799e-06
Step: 37060, train/epoch: 8.819609642028809
Step: 37070, train/loss: 0.41819998621940613
Step: 37070, train/grad_norm: 33.5003776550293
Step: 37070, train/learning_rate: 5.890052307222504e-06
Step: 37070, train/epoch: 8.821989059448242
Step: 37080, train/loss: 0.2689000070095062
Step: 37080, train/grad_norm: 7.120633125305176
Step: 37080, train/learning_rate: 5.878153388039209e-06
Step: 37080, train/epoch: 8.824369430541992
Step: 37090, train/loss: 0.42809998989105225
Step: 37090, train/grad_norm: 3.7448501586914062
Step: 37090, train/learning_rate: 5.866254014108563e-06
Step: 37090, train/epoch: 8.826748847961426
Step: 37100, train/loss: 0.2791999876499176
Step: 37100, train/grad_norm: 10.878294944763184
Step: 37100, train/learning_rate: 5.854355094925268e-06
Step: 37100, train/epoch: 8.829129219055176
Step: 37110, train/loss: 0.30630001425743103
Step: 37110, train/grad_norm: 10.382367134094238
Step: 37110, train/learning_rate: 5.842456175741972e-06
Step: 37110, train/epoch: 8.83150863647461
Step: 37120, train/loss: 0.5909000039100647
Step: 37120, train/grad_norm: 12.749338150024414
Step: 37120, train/learning_rate: 5.830556801811326e-06
Step: 37120, train/epoch: 8.83388900756836
Step: 37130, train/loss: 0.37709999084472656
Step: 37130, train/grad_norm: 12.97607707977295
Step: 37130, train/learning_rate: 5.818657882628031e-06
Step: 37130, train/epoch: 8.836268424987793
Step: 37140, train/loss: 0.4147999882698059
Step: 37140, train/grad_norm: 7.833888530731201
Step: 37140, train/learning_rate: 5.806758508697385e-06
Step: 37140, train/epoch: 8.838647842407227
Step: 37150, train/loss: 0.3865000009536743
Step: 37150, train/grad_norm: 11.383499145507812
Step: 37150, train/learning_rate: 5.79485958951409e-06
Step: 37150, train/epoch: 8.841028213500977
Step: 37160, train/loss: 0.22130000591278076
Step: 37160, train/grad_norm: 14.405953407287598
Step: 37160, train/learning_rate: 5.7829606703307945e-06
Step: 37160, train/epoch: 8.84340763092041
Step: 37170, train/loss: 0.34389999508857727
Step: 37170, train/grad_norm: 7.893792152404785
Step: 37170, train/learning_rate: 5.771061296400148e-06
Step: 37170, train/epoch: 8.84578800201416
Step: 37180, train/loss: 0.2750999927520752
Step: 37180, train/grad_norm: 7.140020370483398
Step: 37180, train/learning_rate: 5.759162377216853e-06
Step: 37180, train/epoch: 8.848167419433594
Step: 37190, train/loss: 0.42829999327659607
Step: 37190, train/grad_norm: 12.950389862060547
Step: 37190, train/learning_rate: 5.747263003286207e-06
Step: 37190, train/epoch: 8.850547790527344
Step: 37200, train/loss: 0.3822999894618988
Step: 37200, train/grad_norm: 8.264269828796387
Step: 37200, train/learning_rate: 5.735364084102912e-06
Step: 37200, train/epoch: 8.852927207946777
Step: 37210, train/loss: 0.391400009393692
Step: 37210, train/grad_norm: 3.4226491451263428
Step: 37210, train/learning_rate: 5.723465164919617e-06
Step: 37210, train/epoch: 8.855306625366211
Step: 37220, train/loss: 0.266400009393692
Step: 37220, train/grad_norm: 12.03519344329834
Step: 37220, train/learning_rate: 5.7115657909889705e-06
Step: 37220, train/epoch: 8.857686996459961
Step: 37230, train/loss: 0.31940001249313354
Step: 37230, train/grad_norm: 2.035118579864502
Step: 37230, train/learning_rate: 5.699666871805675e-06
Step: 37230, train/epoch: 8.860066413879395
Step: 37240, train/loss: 0.37940001487731934
Step: 37240, train/grad_norm: 8.129542350769043
Step: 37240, train/learning_rate: 5.68776795262238e-06
Step: 37240, train/epoch: 8.862446784973145
Step: 37250, train/loss: 0.33739998936653137
Step: 37250, train/grad_norm: 26.537818908691406
Step: 37250, train/learning_rate: 5.675868578691734e-06
Step: 37250, train/epoch: 8.864826202392578
Step: 37260, train/loss: 0.3483000099658966
Step: 37260, train/grad_norm: 10.641438484191895
Step: 37260, train/learning_rate: 5.663969659508439e-06
Step: 37260, train/epoch: 8.867205619812012
Step: 37270, train/loss: 0.36340001225471497
Step: 37270, train/grad_norm: 13.26058292388916
Step: 37270, train/learning_rate: 5.652070285577793e-06
Step: 37270, train/epoch: 8.869585990905762
Step: 37280, train/loss: 0.24390000104904175
Step: 37280, train/grad_norm: 15.269389152526855
Step: 37280, train/learning_rate: 5.6401713663944975e-06
Step: 37280, train/epoch: 8.871965408325195
Step: 37290, train/loss: 0.3377000093460083
Step: 37290, train/grad_norm: 1.3944681882858276
Step: 37290, train/learning_rate: 5.628272447211202e-06
Step: 37290, train/epoch: 8.874345779418945
Step: 37300, train/loss: 0.26649999618530273
Step: 37300, train/grad_norm: 5.634303092956543
Step: 37300, train/learning_rate: 5.616373073280556e-06
Step: 37300, train/epoch: 8.876725196838379
Step: 37310, train/loss: 0.38499999046325684
Step: 37310, train/grad_norm: 16.06676483154297
Step: 37310, train/learning_rate: 5.604474154097261e-06
Step: 37310, train/epoch: 8.879105567932129
Step: 37320, train/loss: 0.18070000410079956
Step: 37320, train/grad_norm: 6.178321838378906
Step: 37320, train/learning_rate: 5.592574780166615e-06
Step: 37320, train/epoch: 8.881484985351562
Step: 37330, train/loss: 0.26840001344680786
Step: 37330, train/grad_norm: 16.091978073120117
Step: 37330, train/learning_rate: 5.58067586098332e-06
Step: 37330, train/epoch: 8.883864402770996
Step: 37340, train/loss: 0.30160000920295715
Step: 37340, train/grad_norm: 15.75721263885498
Step: 37340, train/learning_rate: 5.568776941800024e-06
Step: 37340, train/epoch: 8.886244773864746
Step: 37350, train/loss: 0.4221999943256378
Step: 37350, train/grad_norm: 8.710906982421875
Step: 37350, train/learning_rate: 5.556877567869378e-06
Step: 37350, train/epoch: 8.88862419128418
Step: 37360, train/loss: 0.2709999978542328
Step: 37360, train/grad_norm: 8.402308464050293
Step: 37360, train/learning_rate: 5.544978648686083e-06
Step: 37360, train/epoch: 8.89100456237793
Step: 37370, train/loss: 0.33899998664855957
Step: 37370, train/grad_norm: 11.039517402648926
Step: 37370, train/learning_rate: 5.533079274755437e-06
Step: 37370, train/epoch: 8.893383979797363
Step: 37380, train/loss: 0.3345000147819519
Step: 37380, train/grad_norm: 6.95902156829834
Step: 37380, train/learning_rate: 5.521180355572142e-06
Step: 37380, train/epoch: 8.895764350891113
Step: 37390, train/loss: 0.423799991607666
Step: 37390, train/grad_norm: 17.79418182373047
Step: 37390, train/learning_rate: 5.5092814363888465e-06
Step: 37390, train/epoch: 8.898143768310547
Step: 37400, train/loss: 0.3249000012874603
Step: 37400, train/grad_norm: 13.563121795654297
Step: 37400, train/learning_rate: 5.4973820624582e-06
Step: 37400, train/epoch: 8.90052318572998
Step: 37410, train/loss: 0.446399986743927
Step: 37410, train/grad_norm: 21.098716735839844
Step: 37410, train/learning_rate: 5.485483143274905e-06
Step: 37410, train/epoch: 8.90290355682373
Step: 37420, train/loss: 0.29499998688697815
Step: 37420, train/grad_norm: 14.213146209716797
Step: 37420, train/learning_rate: 5.47358422409161e-06
Step: 37420, train/epoch: 8.905282974243164
Step: 37430, train/loss: 0.19910000264644623
Step: 37430, train/grad_norm: 11.312643051147461
Step: 37430, train/learning_rate: 5.461684850160964e-06
Step: 37430, train/epoch: 8.907663345336914
Step: 37440, train/loss: 0.2849000096321106
Step: 37440, train/grad_norm: 5.189042091369629
Step: 37440, train/learning_rate: 5.449785930977669e-06
Step: 37440, train/epoch: 8.910042762756348
Step: 37450, train/loss: 0.49619999527931213
Step: 37450, train/grad_norm: 17.729549407958984
Step: 37450, train/learning_rate: 5.4378865570470225e-06
Step: 37450, train/epoch: 8.912422180175781
Step: 37460, train/loss: 0.23319999873638153
Step: 37460, train/grad_norm: 6.2476959228515625
Step: 37460, train/learning_rate: 5.425987637863727e-06
Step: 37460, train/epoch: 8.914802551269531
Step: 37470, train/loss: 0.23749999701976776
Step: 37470, train/grad_norm: 19.758560180664062
Step: 37470, train/learning_rate: 5.414088718680432e-06
Step: 37470, train/epoch: 8.917181968688965
Step: 37480, train/loss: 0.7027999758720398
Step: 37480, train/grad_norm: 14.289077758789062
Step: 37480, train/learning_rate: 5.402189344749786e-06
Step: 37480, train/epoch: 8.919562339782715
Step: 37490, train/loss: 0.41119998693466187
Step: 37490, train/grad_norm: 23.076072692871094
Step: 37490, train/learning_rate: 5.390290425566491e-06
Step: 37490, train/epoch: 8.921941757202148
Step: 37500, train/loss: 0.2939999997615814
Step: 37500, train/grad_norm: 7.275487422943115
Step: 37500, train/learning_rate: 5.378391051635845e-06
Step: 37500, train/epoch: 8.924322128295898
Step: 37510, train/loss: 0.4564000070095062
Step: 37510, train/grad_norm: 21.054386138916016
Step: 37510, train/learning_rate: 5.366492132452549e-06
Step: 37510, train/epoch: 8.926701545715332
Step: 37520, train/loss: 0.36320000886917114
Step: 37520, train/grad_norm: 22.384151458740234
Step: 37520, train/learning_rate: 5.354593213269254e-06
Step: 37520, train/epoch: 8.929080963134766
Step: 37530, train/loss: 0.24199999868869781
Step: 37530, train/grad_norm: 19.540555953979492
Step: 37530, train/learning_rate: 5.342693839338608e-06
Step: 37530, train/epoch: 8.931461334228516
Step: 37540, train/loss: 0.4207000136375427
Step: 37540, train/grad_norm: 9.646879196166992
Step: 37540, train/learning_rate: 5.330794920155313e-06
Step: 37540, train/epoch: 8.93384075164795
Step: 37550, train/loss: 0.31220000982284546
Step: 37550, train/grad_norm: 23.30634117126465
Step: 37550, train/learning_rate: 5.318895546224667e-06
Step: 37550, train/epoch: 8.9362211227417
Step: 37560, train/loss: 0.3149999976158142
Step: 37560, train/grad_norm: 11.809622764587402
Step: 37560, train/learning_rate: 5.3069966270413715e-06
Step: 37560, train/epoch: 8.938600540161133
Step: 37570, train/loss: 0.2800000011920929
Step: 37570, train/grad_norm: 6.325583457946777
Step: 37570, train/learning_rate: 5.295097707858076e-06
Step: 37570, train/epoch: 8.940980911254883
Step: 37580, train/loss: 0.22370000183582306
Step: 37580, train/grad_norm: 14.145890235900879
Step: 37580, train/learning_rate: 5.28319833392743e-06
Step: 37580, train/epoch: 8.943360328674316
Step: 37590, train/loss: 0.4187999963760376
Step: 37590, train/grad_norm: 10.582742691040039
Step: 37590, train/learning_rate: 5.271299414744135e-06
Step: 37590, train/epoch: 8.94573974609375
Step: 37600, train/loss: 0.2806999981403351
Step: 37600, train/grad_norm: 19.719478607177734
Step: 37600, train/learning_rate: 5.25940049556084e-06
Step: 37600, train/epoch: 8.9481201171875
Step: 37610, train/loss: 0.2526000142097473
Step: 37610, train/grad_norm: 11.105840682983398
Step: 37610, train/learning_rate: 5.247501121630194e-06
Step: 37610, train/epoch: 8.950499534606934
Step: 37620, train/loss: 0.373199999332428
Step: 37620, train/grad_norm: 7.886207103729248
Step: 37620, train/learning_rate: 5.2356022024468984e-06
Step: 37620, train/epoch: 8.952879905700684
Step: 37630, train/loss: 0.33959999680519104
Step: 37630, train/grad_norm: 12.743875503540039
Step: 37630, train/learning_rate: 5.223702828516252e-06
Step: 37630, train/epoch: 8.955259323120117
Step: 37640, train/loss: 0.21690000593662262
Step: 37640, train/grad_norm: 12.759687423706055
Step: 37640, train/learning_rate: 5.211803909332957e-06
Step: 37640, train/epoch: 8.957639694213867
Step: 37650, train/loss: 0.39329999685287476
Step: 37650, train/grad_norm: 15.811663627624512
Step: 37650, train/learning_rate: 5.199904990149662e-06
Step: 37650, train/epoch: 8.9600191116333
Step: 37660, train/loss: 0.3402000069618225
Step: 37660, train/grad_norm: 8.300171852111816
Step: 37660, train/learning_rate: 5.188005616219016e-06
Step: 37660, train/epoch: 8.962398529052734
Step: 37670, train/loss: 0.42590001225471497
Step: 37670, train/grad_norm: 32.14164733886719
Step: 37670, train/learning_rate: 5.1761066970357206e-06
Step: 37670, train/epoch: 8.964778900146484
Step: 37680, train/loss: 0.26489999890327454
Step: 37680, train/grad_norm: 10.63010025024414
Step: 37680, train/learning_rate: 5.1642073231050745e-06
Step: 37680, train/epoch: 8.967158317565918
Step: 37690, train/loss: 0.27079999446868896
Step: 37690, train/grad_norm: 14.863090515136719
Step: 37690, train/learning_rate: 5.152308403921779e-06
Step: 37690, train/epoch: 8.969538688659668
Step: 37700, train/loss: 0.3610000014305115
Step: 37700, train/grad_norm: 11.21975326538086
Step: 37700, train/learning_rate: 5.140409484738484e-06
Step: 37700, train/epoch: 8.971918106079102
Step: 37710, train/loss: 0.3255999982357025
Step: 37710, train/grad_norm: 9.636188507080078
Step: 37710, train/learning_rate: 5.128510110807838e-06
Step: 37710, train/epoch: 8.974297523498535
Step: 37720, train/loss: 0.3165999948978424
Step: 37720, train/grad_norm: 1.5158475637435913
Step: 37720, train/learning_rate: 5.116611191624543e-06
Step: 37720, train/epoch: 8.976677894592285
Step: 37730, train/loss: 0.31349998712539673
Step: 37730, train/grad_norm: 20.022207260131836
Step: 37730, train/learning_rate: 5.104711817693897e-06
Step: 37730, train/epoch: 8.979057312011719
Step: 37740, train/loss: 0.43149998784065247
Step: 37740, train/grad_norm: 18.486913681030273
Step: 37740, train/learning_rate: 5.092812898510601e-06
Step: 37740, train/epoch: 8.981437683105469
Step: 37750, train/loss: 0.5411999821662903
Step: 37750, train/grad_norm: 19.036109924316406
Step: 37750, train/learning_rate: 5.080913979327306e-06
Step: 37750, train/epoch: 8.983817100524902
Step: 37760, train/loss: 0.3458000123500824
Step: 37760, train/grad_norm: 2.5192766189575195
Step: 37760, train/learning_rate: 5.06901460539666e-06
Step: 37760, train/epoch: 8.986197471618652
Step: 37770, train/loss: 0.5031999945640564
Step: 37770, train/grad_norm: 9.75893783569336
Step: 37770, train/learning_rate: 5.057115686213365e-06
Step: 37770, train/epoch: 8.988576889038086
Step: 37780, train/loss: 0.3174999952316284
Step: 37780, train/grad_norm: 12.979825019836426
Step: 37780, train/learning_rate: 5.04521676703007e-06
Step: 37780, train/epoch: 8.99095630645752
Step: 37790, train/loss: 0.30480000376701355
Step: 37790, train/grad_norm: 19.475372314453125
Step: 37790, train/learning_rate: 5.0333173930994235e-06
Step: 37790, train/epoch: 8.99333667755127
Step: 37800, train/loss: 0.4147000014781952
Step: 37800, train/grad_norm: 18.943859100341797
Step: 37800, train/learning_rate: 5.021418473916128e-06
Step: 37800, train/epoch: 8.995716094970703
Step: 37810, train/loss: 0.23929999768733978
Step: 37810, train/grad_norm: 11.307326316833496
Step: 37810, train/learning_rate: 5.009519099985482e-06
Step: 37810, train/epoch: 8.998096466064453
Step: 37818, eval/loss: 0.8573763370513916
Step: 37818, eval/accuracy: 0.7044287323951721
Step: 37818, eval/f1: 0.7040778994560242
Step: 37818, eval/runtime: 55.49020004272461
Step: 37818, eval/samples_per_second: 129.8070068359375
Step: 37818, eval/steps_per_second: 16.23699951171875
Step: 37818, train/epoch: 9.0
Step: 37820, train/loss: 0.40540000796318054
Step: 37820, train/grad_norm: 8.79781723022461
Step: 37820, train/learning_rate: 4.997620180802187e-06
Step: 37820, train/epoch: 9.000475883483887
Step: 37830, train/loss: 0.32030001282691956
Step: 37830, train/grad_norm: 23.630966186523438
Step: 37830, train/learning_rate: 4.985721261618892e-06
Step: 37830, train/epoch: 9.002856254577637
Step: 37840, train/loss: 0.4023999869823456
Step: 37840, train/grad_norm: 7.254848957061768
Step: 37840, train/learning_rate: 4.973821887688246e-06
Step: 37840, train/epoch: 9.00523567199707
Step: 37850, train/loss: 0.3434999883174896
Step: 37850, train/grad_norm: 26.71827507019043
Step: 37850, train/learning_rate: 4.96192296850495e-06
Step: 37850, train/epoch: 9.007615089416504
Step: 37860, train/loss: 0.42579999566078186
Step: 37860, train/grad_norm: 3.0414576530456543
Step: 37860, train/learning_rate: 4.950023594574304e-06
Step: 37860, train/epoch: 9.009995460510254
Step: 37870, train/loss: 0.3741999864578247
Step: 37870, train/grad_norm: 22.303817749023438
Step: 37870, train/learning_rate: 4.938124675391009e-06
Step: 37870, train/epoch: 9.012374877929688
Step: 37880, train/loss: 0.2847999930381775
Step: 37880, train/grad_norm: 7.330812931060791
Step: 37880, train/learning_rate: 4.926225756207714e-06
Step: 37880, train/epoch: 9.014755249023438
Step: 37890, train/loss: 0.2973000109195709
Step: 37890, train/grad_norm: 18.78119468688965
Step: 37890, train/learning_rate: 4.914326382277068e-06
Step: 37890, train/epoch: 9.017134666442871
Step: 37900, train/loss: 0.3869999945163727
Step: 37900, train/grad_norm: 2.958247661590576
Step: 37900, train/learning_rate: 4.9024274630937725e-06
Step: 37900, train/epoch: 9.019514083862305
Step: 37910, train/loss: 0.21629999577999115
Step: 37910, train/grad_norm: 10.124601364135742
Step: 37910, train/learning_rate: 4.890528543910477e-06
Step: 37910, train/epoch: 9.021894454956055
Step: 37920, train/loss: 0.3077999949455261
Step: 37920, train/grad_norm: 8.03907299041748
Step: 37920, train/learning_rate: 4.878629169979831e-06
Step: 37920, train/epoch: 9.024273872375488
Step: 37930, train/loss: 0.2046000063419342
Step: 37930, train/grad_norm: 10.075777053833008
Step: 37930, train/learning_rate: 4.866730250796536e-06
Step: 37930, train/epoch: 9.026654243469238
Step: 37940, train/loss: 0.36399999260902405
Step: 37940, train/grad_norm: 2.3432822227478027
Step: 37940, train/learning_rate: 4.85483087686589e-06
Step: 37940, train/epoch: 9.029033660888672
Step: 37950, train/loss: 0.33169999718666077
Step: 37950, train/grad_norm: 20.90285301208496
Step: 37950, train/learning_rate: 4.842931957682595e-06
Step: 37950, train/epoch: 9.031414031982422
Step: 37960, train/loss: 0.19140000641345978
Step: 37960, train/grad_norm: 6.383829593658447
Step: 37960, train/learning_rate: 4.8310330384992994e-06
Step: 37960, train/epoch: 9.033793449401855
Step: 37970, train/loss: 0.27309998869895935
Step: 37970, train/grad_norm: 9.2347993850708
Step: 37970, train/learning_rate: 4.819133664568653e-06
Step: 37970, train/epoch: 9.036172866821289
Step: 37980, train/loss: 0.21969999372959137
Step: 37980, train/grad_norm: 16.14290428161621
Step: 37980, train/learning_rate: 4.807234745385358e-06
Step: 37980, train/epoch: 9.038553237915039
Step: 37990, train/loss: 0.1551000028848648
Step: 37990, train/grad_norm: 2.982724666595459
Step: 37990, train/learning_rate: 4.795335371454712e-06
Step: 37990, train/epoch: 9.040932655334473
Step: 38000, train/loss: 0.3580999970436096
Step: 38000, train/grad_norm: 21.59317398071289
Step: 38000, train/learning_rate: 4.783436452271417e-06
Step: 38000, train/epoch: 9.043313026428223
Step: 38010, train/loss: 0.3034999966621399
Step: 38010, train/grad_norm: 17.871591567993164
Step: 38010, train/learning_rate: 4.7715375330881216e-06
Step: 38010, train/epoch: 9.045692443847656
Step: 38020, train/loss: 0.25540000200271606
Step: 38020, train/grad_norm: 28.86984634399414
Step: 38020, train/learning_rate: 4.7596381591574755e-06
Step: 38020, train/epoch: 9.048072814941406
Step: 38030, train/loss: 0.32510000467300415
Step: 38030, train/grad_norm: 18.431415557861328
Step: 38030, train/learning_rate: 4.74773923997418e-06
Step: 38030, train/epoch: 9.05045223236084
Step: 38040, train/loss: 0.392300009727478
Step: 38040, train/grad_norm: 23.11030387878418
Step: 38040, train/learning_rate: 4.735839866043534e-06
Step: 38040, train/epoch: 9.052831649780273
Step: 38050, train/loss: 0.31279999017715454
Step: 38050, train/grad_norm: 8.8850736618042
Step: 38050, train/learning_rate: 4.723940946860239e-06
Step: 38050, train/epoch: 9.055212020874023
Step: 38060, train/loss: 0.32339999079704285
Step: 38060, train/grad_norm: 12.875631332397461
Step: 38060, train/learning_rate: 4.712042027676944e-06
Step: 38060, train/epoch: 9.057591438293457
Step: 38070, train/loss: 0.47600001096725464
Step: 38070, train/grad_norm: 8.85798168182373
Step: 38070, train/learning_rate: 4.700142653746298e-06
Step: 38070, train/epoch: 9.059971809387207
Step: 38080, train/loss: 0.20170000195503235
Step: 38080, train/grad_norm: 4.846923351287842
Step: 38080, train/learning_rate: 4.688243734563002e-06
Step: 38080, train/epoch: 9.06235122680664
Step: 38090, train/loss: 0.3741999864578247
Step: 38090, train/grad_norm: 24.25759506225586
Step: 38090, train/learning_rate: 4.676344815379707e-06
Step: 38090, train/epoch: 9.064730644226074
Step: 38100, train/loss: 0.39469999074935913
Step: 38100, train/grad_norm: 7.442578315734863
Step: 38100, train/learning_rate: 4.664445441449061e-06
Step: 38100, train/epoch: 9.067111015319824
Step: 38110, train/loss: 0.2816999852657318
Step: 38110, train/grad_norm: 8.358237266540527
Step: 38110, train/learning_rate: 4.652546522265766e-06
Step: 38110, train/epoch: 9.069490432739258
Step: 38120, train/loss: 0.4334999918937683
Step: 38120, train/grad_norm: 22.97074317932129
Step: 38120, train/learning_rate: 4.64064714833512e-06
Step: 38120, train/epoch: 9.071870803833008
Step: 38130, train/loss: 0.25870001316070557
Step: 38130, train/grad_norm: 4.770857810974121
Step: 38130, train/learning_rate: 4.6287482291518245e-06
Step: 38130, train/epoch: 9.074250221252441
Step: 38140, train/loss: 0.4074000120162964
Step: 38140, train/grad_norm: 4.4927778244018555
Step: 38140, train/learning_rate: 4.616849309968529e-06
Step: 38140, train/epoch: 9.076630592346191
Step: 38150, train/loss: 0.34470000863075256
Step: 38150, train/grad_norm: 20.31633186340332
Step: 38150, train/learning_rate: 4.604949936037883e-06
Step: 38150, train/epoch: 9.079010009765625
Step: 38160, train/loss: 0.4986000061035156
Step: 38160, train/grad_norm: 14.77449893951416
Step: 38160, train/learning_rate: 4.593051016854588e-06
Step: 38160, train/epoch: 9.081389427185059
Step: 38170, train/loss: 0.40860000252723694
Step: 38170, train/grad_norm: 8.30871295928955
Step: 38170, train/learning_rate: 4.581151642923942e-06
Step: 38170, train/epoch: 9.083769798278809
Step: 38180, train/loss: 0.37119999527931213
Step: 38180, train/grad_norm: 6.968688488006592
Step: 38180, train/learning_rate: 4.569252723740647e-06
Step: 38180, train/epoch: 9.086149215698242
Step: 38190, train/loss: 0.35670000314712524
Step: 38190, train/grad_norm: 2.226630210876465
Step: 38190, train/learning_rate: 4.557353804557351e-06
Step: 38190, train/epoch: 9.088529586791992
Step: 38200, train/loss: 0.2736999988555908
Step: 38200, train/grad_norm: 15.76483154296875
Step: 38200, train/learning_rate: 4.545454430626705e-06
Step: 38200, train/epoch: 9.090909004211426
Step: 38210, train/loss: 0.27469998598098755
Step: 38210, train/grad_norm: 3.7246201038360596
Step: 38210, train/learning_rate: 4.53355551144341e-06
Step: 38210, train/epoch: 9.093289375305176
Step: 38220, train/loss: 0.23589999973773956
Step: 38220, train/grad_norm: 17.380699157714844
Step: 38220, train/learning_rate: 4.521656137512764e-06
Step: 38220, train/epoch: 9.09566879272461
Step: 38230, train/loss: 0.4781000018119812
Step: 38230, train/grad_norm: 29.1699161529541
Step: 38230, train/learning_rate: 4.509757218329469e-06
Step: 38230, train/epoch: 9.098048210144043
Step: 38240, train/loss: 0.37770000100135803
Step: 38240, train/grad_norm: 13.145411491394043
Step: 38240, train/learning_rate: 4.4978582991461735e-06
Step: 38240, train/epoch: 9.100428581237793
Step: 38250, train/loss: 0.22360000014305115
Step: 38250, train/grad_norm: 9.290193557739258
Step: 38250, train/learning_rate: 4.485958925215527e-06
Step: 38250, train/epoch: 9.102807998657227
Step: 38260, train/loss: 0.34360000491142273
Step: 38260, train/grad_norm: 13.162273406982422
Step: 38260, train/learning_rate: 4.474060006032232e-06
Step: 38260, train/epoch: 9.105188369750977
Step: 38270, train/loss: 0.4814999997615814
Step: 38270, train/grad_norm: 3.7065658569335938
Step: 38270, train/learning_rate: 4.462161086848937e-06
Step: 38270, train/epoch: 9.10756778717041
Step: 38280, train/loss: 0.33309999108314514
Step: 38280, train/grad_norm: 3.032853841781616
Step: 38280, train/learning_rate: 4.450261712918291e-06
Step: 38280, train/epoch: 9.109947204589844
Step: 38290, train/loss: 0.3871000111103058
Step: 38290, train/grad_norm: 12.839585304260254
Step: 38290, train/learning_rate: 4.438362793734996e-06
Step: 38290, train/epoch: 9.112327575683594
Step: 38300, train/loss: 0.31869998574256897
Step: 38300, train/grad_norm: 5.868484020233154
Step: 38300, train/learning_rate: 4.4264634198043495e-06
Step: 38300, train/epoch: 9.114706993103027
Step: 38310, train/loss: 0.3605000078678131
Step: 38310, train/grad_norm: 17.58217430114746
Step: 38310, train/learning_rate: 4.414564500621054e-06
Step: 38310, train/epoch: 9.117087364196777
Step: 38320, train/loss: 0.3167000114917755
Step: 38320, train/grad_norm: 8.855141639709473
Step: 38320, train/learning_rate: 4.402665581437759e-06
Step: 38320, train/epoch: 9.119466781616211
Step: 38330, train/loss: 0.42669999599456787
Step: 38330, train/grad_norm: 24.105501174926758
Step: 38330, train/learning_rate: 4.390766207507113e-06
Step: 38330, train/epoch: 9.121847152709961
Step: 38340, train/loss: 0.491100013256073
Step: 38340, train/grad_norm: 13.072075843811035
Step: 38340, train/learning_rate: 4.378867288323818e-06
Step: 38340, train/epoch: 9.124226570129395
Step: 38350, train/loss: 0.325300008058548
Step: 38350, train/grad_norm: 15.353252410888672
Step: 38350, train/learning_rate: 4.366967914393172e-06
Step: 38350, train/epoch: 9.126605987548828
Step: 38360, train/loss: 0.22120000422000885
Step: 38360, train/grad_norm: 3.854548931121826
Step: 38360, train/learning_rate: 4.3550689952098764e-06
Step: 38360, train/epoch: 9.128986358642578
Step: 38370, train/loss: 0.31859999895095825
Step: 38370, train/grad_norm: 23.862215042114258
Step: 38370, train/learning_rate: 4.343170076026581e-06
Step: 38370, train/epoch: 9.131365776062012
Step: 38380, train/loss: 0.2152000069618225
Step: 38380, train/grad_norm: 9.22498607635498
Step: 38380, train/learning_rate: 4.331270702095935e-06
Step: 38380, train/epoch: 9.133746147155762
Step: 38390, train/loss: 0.4697999954223633
Step: 38390, train/grad_norm: 8.344992637634277
Step: 38390, train/learning_rate: 4.31937178291264e-06
Step: 38390, train/epoch: 9.136125564575195
Step: 38400, train/loss: 0.25600001215934753
Step: 38400, train/grad_norm: 14.628156661987305
Step: 38400, train/learning_rate: 4.307472408981994e-06
Step: 38400, train/epoch: 9.138505935668945
Step: 38410, train/loss: 0.2483000010251999
Step: 38410, train/grad_norm: 15.788743019104004
Step: 38410, train/learning_rate: 4.2955734897986986e-06
Step: 38410, train/epoch: 9.140885353088379
Step: 38420, train/loss: 0.4072999954223633
Step: 38420, train/grad_norm: 10.118630409240723
Step: 38420, train/learning_rate: 4.283674570615403e-06
Step: 38420, train/epoch: 9.143264770507812
Step: 38430, train/loss: 0.4074999988079071
Step: 38430, train/grad_norm: 14.30729866027832
Step: 38430, train/learning_rate: 4.271775196684757e-06
Step: 38430, train/epoch: 9.145645141601562
Step: 38440, train/loss: 0.3564000129699707
Step: 38440, train/grad_norm: 15.495389938354492
Step: 38440, train/learning_rate: 4.259876277501462e-06
Step: 38440, train/epoch: 9.148024559020996
Step: 38450, train/loss: 0.22280000150203705
Step: 38450, train/grad_norm: 7.0657172203063965
Step: 38450, train/learning_rate: 4.247977358318167e-06
Step: 38450, train/epoch: 9.150404930114746
Step: 38460, train/loss: 0.40799999237060547
Step: 38460, train/grad_norm: 7.668190956115723
Step: 38460, train/learning_rate: 4.236077984387521e-06
Step: 38460, train/epoch: 9.15278434753418
Step: 38470, train/loss: 0.2696000039577484
Step: 38470, train/grad_norm: 0.9380965828895569
Step: 38470, train/learning_rate: 4.2241790652042255e-06
Step: 38470, train/epoch: 9.155163764953613
Step: 38480, train/loss: 0.30649998784065247
Step: 38480, train/grad_norm: 1.5982036590576172
Step: 38480, train/learning_rate: 4.212279691273579e-06
Step: 38480, train/epoch: 9.157544136047363
Step: 38490, train/loss: 0.36250001192092896
Step: 38490, train/grad_norm: 11.530635833740234
Step: 38490, train/learning_rate: 4.200380772090284e-06
Step: 38490, train/epoch: 9.159923553466797
Step: 38500, train/loss: 0.37209999561309814
Step: 38500, train/grad_norm: 24.475772857666016
Step: 38500, train/learning_rate: 4.188481852906989e-06
Step: 38500, train/epoch: 9.162303924560547
Step: 38510, train/loss: 0.28999999165534973
Step: 38510, train/grad_norm: 4.031351566314697
Step: 38510, train/learning_rate: 4.176582478976343e-06
Step: 38510, train/epoch: 9.16468334197998
Step: 38520, train/loss: 0.19050000607967377
Step: 38520, train/grad_norm: 21.702037811279297
Step: 38520, train/learning_rate: 4.164683559793048e-06
Step: 38520, train/epoch: 9.16706371307373
Step: 38530, train/loss: 0.24089999496936798
Step: 38530, train/grad_norm: 15.193197250366211
Step: 38530, train/learning_rate: 4.1527841858624015e-06
Step: 38530, train/epoch: 9.169443130493164
Step: 38540, train/loss: 0.3776000142097473
Step: 38540, train/grad_norm: 15.67350959777832
Step: 38540, train/learning_rate: 4.140885266679106e-06
Step: 38540, train/epoch: 9.171822547912598
Step: 38550, train/loss: 0.43720000982284546
Step: 38550, train/grad_norm: 14.273512840270996
Step: 38550, train/learning_rate: 4.128986347495811e-06
Step: 38550, train/epoch: 9.174202919006348
Step: 38560, train/loss: 0.2630000114440918
Step: 38560, train/grad_norm: 12.693761825561523
Step: 38560, train/learning_rate: 4.117086973565165e-06
Step: 38560, train/epoch: 9.176582336425781
Step: 38570, train/loss: 0.3012999892234802
Step: 38570, train/grad_norm: 7.43414831161499
Step: 38570, train/learning_rate: 4.10518805438187e-06
Step: 38570, train/epoch: 9.178962707519531
Step: 38580, train/loss: 0.2881999909877777
Step: 38580, train/grad_norm: 6.486908435821533
Step: 38580, train/learning_rate: 4.0932891351985745e-06
Step: 38580, train/epoch: 9.181342124938965
Step: 38590, train/loss: 0.3215999901294708
Step: 38590, train/grad_norm: 11.01290512084961
Step: 38590, train/learning_rate: 4.081389761267928e-06
Step: 38590, train/epoch: 9.183722496032715
Step: 38600, train/loss: 0.40299999713897705
Step: 38600, train/grad_norm: 4.125890731811523
Step: 38600, train/learning_rate: 4.069490842084633e-06
Step: 38600, train/epoch: 9.186101913452148
Step: 38610, train/loss: 0.18469999730587006
Step: 38610, train/grad_norm: 7.439274787902832
Step: 38610, train/learning_rate: 4.057591468153987e-06
Step: 38610, train/epoch: 9.188481330871582
Step: 38620, train/loss: 0.30979999899864197
Step: 38620, train/grad_norm: 3.3963873386383057
Step: 38620, train/learning_rate: 4.045692548970692e-06
Step: 38620, train/epoch: 9.190861701965332
Step: 38630, train/loss: 0.23849999904632568
Step: 38630, train/grad_norm: 4.293937683105469
Step: 38630, train/learning_rate: 4.033793629787397e-06
Step: 38630, train/epoch: 9.193241119384766
Step: 38640, train/loss: 0.2962000072002411
Step: 38640, train/grad_norm: 8.109115600585938
Step: 38640, train/learning_rate: 4.0218942558567505e-06
Step: 38640, train/epoch: 9.195621490478516
Step: 38650, train/loss: 0.4521999955177307
Step: 38650, train/grad_norm: 12.868338584899902
Step: 38650, train/learning_rate: 4.009995336673455e-06
Step: 38650, train/epoch: 9.19800090789795
Step: 38660, train/loss: 0.37720000743865967
Step: 38660, train/grad_norm: 17.92681121826172
Step: 38660, train/learning_rate: 3.998095962742809e-06
Step: 38660, train/epoch: 9.200380325317383
Step: 38670, train/loss: 0.2345000058412552
Step: 38670, train/grad_norm: 22.10272216796875
Step: 38670, train/learning_rate: 3.986197043559514e-06
Step: 38670, train/epoch: 9.202760696411133
Step: 38680, train/loss: 0.4683000147342682
Step: 38680, train/grad_norm: 20.88241195678711
Step: 38680, train/learning_rate: 3.974298124376219e-06
Step: 38680, train/epoch: 9.205140113830566
Step: 38690, train/loss: 0.21660000085830688
Step: 38690, train/grad_norm: 8.866924285888672
Step: 38690, train/learning_rate: 3.962398750445573e-06
Step: 38690, train/epoch: 9.207520484924316
Step: 38700, train/loss: 0.302700012922287
Step: 38700, train/grad_norm: 12.84426212310791
Step: 38700, train/learning_rate: 3.9504998312622774e-06
Step: 38700, train/epoch: 9.20989990234375
Step: 38710, train/loss: 0.2581000030040741
Step: 38710, train/grad_norm: 4.7120232582092285
Step: 38710, train/learning_rate: 3.938600457331631e-06
Step: 38710, train/epoch: 9.2122802734375
Step: 38720, train/loss: 0.2865999937057495
Step: 38720, train/grad_norm: 0.2762722671031952
Step: 38720, train/learning_rate: 3.926701538148336e-06
Step: 38720, train/epoch: 9.214659690856934
Step: 38730, train/loss: 0.2206999957561493
Step: 38730, train/grad_norm: 10.615836143493652
Step: 38730, train/learning_rate: 3.914802618965041e-06
Step: 38730, train/epoch: 9.217039108276367
Step: 38740, train/loss: 0.2264000028371811
Step: 38740, train/grad_norm: 6.608831882476807
Step: 38740, train/learning_rate: 3.902903245034395e-06
Step: 38740, train/epoch: 9.219419479370117
Step: 38750, train/loss: 0.30230000615119934
Step: 38750, train/grad_norm: 3.893219470977783
Step: 38750, train/learning_rate: 3.8910043258510996e-06
Step: 38750, train/epoch: 9.22179889678955
Step: 38760, train/loss: 0.26809999346733093
Step: 38760, train/grad_norm: 7.314979076385498
Step: 38760, train/learning_rate: 3.879105406667804e-06
Step: 38760, train/epoch: 9.2241792678833
Step: 38770, train/loss: 0.2648000121116638
Step: 38770, train/grad_norm: 9.149115562438965
Step: 38770, train/learning_rate: 3.867206032737158e-06
Step: 38770, train/epoch: 9.226558685302734
Step: 38780, train/loss: 0.2605000138282776
Step: 38780, train/grad_norm: 28.271883010864258
Step: 38780, train/learning_rate: 3.855307113553863e-06
Step: 38780, train/epoch: 9.228939056396484
Step: 38790, train/loss: 0.33709999918937683
Step: 38790, train/grad_norm: 12.758129119873047
Step: 38790, train/learning_rate: 3.843407739623217e-06
Step: 38790, train/epoch: 9.231318473815918
Step: 38800, train/loss: 0.19580000638961792
Step: 38800, train/grad_norm: 8.58956527709961
Step: 38800, train/learning_rate: 3.831508820439922e-06
Step: 38800, train/epoch: 9.233697891235352
Step: 38810, train/loss: 0.4975999891757965
Step: 38810, train/grad_norm: 12.735696792602539
Step: 38810, train/learning_rate: 3.8196099012566265e-06
Step: 38810, train/epoch: 9.236078262329102
Step: 38820, train/loss: 0.21690000593662262
Step: 38820, train/grad_norm: 11.986004829406738
Step: 38820, train/learning_rate: 3.8077105273259804e-06
Step: 38820, train/epoch: 9.238457679748535
Step: 38830, train/loss: 0.2777999937534332
Step: 38830, train/grad_norm: 8.792197227478027
Step: 38830, train/learning_rate: 3.795811608142685e-06
Step: 38830, train/epoch: 9.240838050842285
Step: 38840, train/loss: 0.390500009059906
Step: 38840, train/grad_norm: 13.575523376464844
Step: 38840, train/learning_rate: 3.7839124615857145e-06
Step: 38840, train/epoch: 9.243217468261719
Step: 38850, train/loss: 0.26409998536109924
Step: 38850, train/grad_norm: 10.66008472442627
Step: 38850, train/learning_rate: 3.772013315028744e-06
Step: 38850, train/epoch: 9.245596885681152
Step: 38860, train/loss: 0.3061999976634979
Step: 38860, train/grad_norm: 11.952624320983887
Step: 38860, train/learning_rate: 3.760114168471773e-06
Step: 38860, train/epoch: 9.247977256774902
Step: 38870, train/loss: 0.545799970626831
Step: 38870, train/grad_norm: 16.47139549255371
Step: 38870, train/learning_rate: 3.748215249288478e-06
Step: 38870, train/epoch: 9.250356674194336
Step: 38880, train/loss: 0.41679999232292175
Step: 38880, train/grad_norm: 9.565555572509766
Step: 38880, train/learning_rate: 3.7363161027315073e-06
Step: 38880, train/epoch: 9.252737045288086
Step: 38890, train/loss: 0.27079999446868896
Step: 38890, train/grad_norm: 26.211793899536133
Step: 38890, train/learning_rate: 3.7244169561745366e-06
Step: 38890, train/epoch: 9.25511646270752
Step: 38900, train/loss: 0.4830000102519989
Step: 38900, train/grad_norm: 32.33407974243164
Step: 38900, train/learning_rate: 3.712517809617566e-06
Step: 38900, train/epoch: 9.25749683380127
Step: 38910, train/loss: 0.4724000096321106
Step: 38910, train/grad_norm: 12.042096138000488
Step: 38910, train/learning_rate: 3.7006186630605953e-06
Step: 38910, train/epoch: 9.259876251220703
Step: 38920, train/loss: 0.23880000412464142
Step: 38920, train/grad_norm: 8.576809883117676
Step: 38920, train/learning_rate: 3.6887197438773e-06
Step: 38920, train/epoch: 9.262255668640137
Step: 38930, train/loss: 0.27639999985694885
Step: 38930, train/grad_norm: 11.758169174194336
Step: 38930, train/learning_rate: 3.6768205973203294e-06
Step: 38930, train/epoch: 9.264636039733887
Step: 38940, train/loss: 0.2720000147819519
Step: 38940, train/grad_norm: 8.923698425292969
Step: 38940, train/learning_rate: 3.6649214507633587e-06
Step: 38940, train/epoch: 9.26701545715332
Step: 38950, train/loss: 0.273499995470047
Step: 38950, train/grad_norm: 24.789628982543945
Step: 38950, train/learning_rate: 3.653022304206388e-06
Step: 38950, train/epoch: 9.26939582824707
Step: 38960, train/loss: 0.2953999936580658
Step: 38960, train/grad_norm: 12.531841278076172
Step: 38960, train/learning_rate: 3.641123385023093e-06
Step: 38960, train/epoch: 9.271775245666504
Step: 38970, train/loss: 0.31790000200271606
Step: 38970, train/grad_norm: 3.374134063720703
Step: 38970, train/learning_rate: 3.629224238466122e-06
Step: 38970, train/epoch: 9.274155616760254
Step: 38980, train/loss: 0.36500000953674316
Step: 38980, train/grad_norm: 1.9139913320541382
Step: 38980, train/learning_rate: 3.6173250919091515e-06
Step: 38980, train/epoch: 9.276535034179688
Step: 38990, train/loss: 0.4169999957084656
Step: 38990, train/grad_norm: 19.29283332824707
Step: 38990, train/learning_rate: 3.605425945352181e-06
Step: 38990, train/epoch: 9.278914451599121
Step: 39000, train/loss: 0.2345999926328659
Step: 39000, train/grad_norm: 2.688784599304199
Step: 39000, train/learning_rate: 3.59352679879521e-06
Step: 39000, train/epoch: 9.281294822692871
Step: 39010, train/loss: 0.31709998846054077
Step: 39010, train/grad_norm: 13.501802444458008
Step: 39010, train/learning_rate: 3.581627879611915e-06
Step: 39010, train/epoch: 9.283674240112305
Step: 39020, train/loss: 0.4681999981403351
Step: 39020, train/grad_norm: 26.985490798950195
Step: 39020, train/learning_rate: 3.5697287330549443e-06
Step: 39020, train/epoch: 9.286054611206055
Step: 39030, train/loss: 0.24560000002384186
Step: 39030, train/grad_norm: 4.982621669769287
Step: 39030, train/learning_rate: 3.5578295864979737e-06
Step: 39030, train/epoch: 9.288434028625488
Step: 39040, train/loss: 0.27399998903274536
Step: 39040, train/grad_norm: 18.20958137512207
Step: 39040, train/learning_rate: 3.545930439941003e-06
Step: 39040, train/epoch: 9.290813446044922
Step: 39050, train/loss: 0.36419999599456787
Step: 39050, train/grad_norm: 18.467998504638672
Step: 39050, train/learning_rate: 3.5340315207577078e-06
Step: 39050, train/epoch: 9.293193817138672
Step: 39060, train/loss: 0.3041999936103821
Step: 39060, train/grad_norm: 26.741392135620117
Step: 39060, train/learning_rate: 3.522132374200737e-06
Step: 39060, train/epoch: 9.295573234558105
Step: 39070, train/loss: 0.38199999928474426
Step: 39070, train/grad_norm: 14.73083782196045
Step: 39070, train/learning_rate: 3.5102332276437664e-06
Step: 39070, train/epoch: 9.297953605651855
Step: 39080, train/loss: 0.2906000018119812
Step: 39080, train/grad_norm: 6.94080924987793
Step: 39080, train/learning_rate: 3.4983340810867958e-06
Step: 39080, train/epoch: 9.300333023071289
Step: 39090, train/loss: 0.32350000739097595
Step: 39090, train/grad_norm: 7.042242050170898
Step: 39090, train/learning_rate: 3.486434934529825e-06
Step: 39090, train/epoch: 9.302713394165039
Step: 39100, train/loss: 0.259799987077713
Step: 39100, train/grad_norm: 4.914101600646973
Step: 39100, train/learning_rate: 3.47453601534653e-06
Step: 39100, train/epoch: 9.305092811584473
Step: 39110, train/loss: 0.3815999925136566
Step: 39110, train/grad_norm: 27.664730072021484
Step: 39110, train/learning_rate: 3.4626368687895592e-06
Step: 39110, train/epoch: 9.307472229003906
Step: 39120, train/loss: 0.2547999918460846
Step: 39120, train/grad_norm: 25.999189376831055
Step: 39120, train/learning_rate: 3.4507377222325886e-06
Step: 39120, train/epoch: 9.309852600097656
Step: 39130, train/loss: 0.3587999939918518
Step: 39130, train/grad_norm: 13.276566505432129
Step: 39130, train/learning_rate: 3.438838575675618e-06
Step: 39130, train/epoch: 9.31223201751709
Step: 39140, train/loss: 0.5527999997138977
Step: 39140, train/grad_norm: 18.259428024291992
Step: 39140, train/learning_rate: 3.4269396564923227e-06
Step: 39140, train/epoch: 9.31461238861084
Step: 39150, train/loss: 0.32850000262260437
Step: 39150, train/grad_norm: 10.850153923034668
Step: 39150, train/learning_rate: 3.415040509935352e-06
Step: 39150, train/epoch: 9.316991806030273
Step: 39160, train/loss: 0.37950000166893005
Step: 39160, train/grad_norm: 14.267415046691895
Step: 39160, train/learning_rate: 3.4031413633783814e-06
Step: 39160, train/epoch: 9.319372177124023
Step: 39170, train/loss: 0.35249999165534973
Step: 39170, train/grad_norm: 16.87080955505371
Step: 39170, train/learning_rate: 3.3912422168214107e-06
Step: 39170, train/epoch: 9.321751594543457
Step: 39180, train/loss: 0.26750001311302185
Step: 39180, train/grad_norm: 4.610924243927002
Step: 39180, train/learning_rate: 3.37934307026444e-06
Step: 39180, train/epoch: 9.32413101196289
Step: 39190, train/loss: 0.24889999628067017
Step: 39190, train/grad_norm: 2.9794042110443115
Step: 39190, train/learning_rate: 3.367444151081145e-06
Step: 39190, train/epoch: 9.32651138305664
Step: 39200, train/loss: 0.2223999947309494
Step: 39200, train/grad_norm: 9.689681053161621
Step: 39200, train/learning_rate: 3.355545004524174e-06
Step: 39200, train/epoch: 9.328890800476074
Step: 39210, train/loss: 0.2784999907016754
Step: 39210, train/grad_norm: 11.117844581604004
Step: 39210, train/learning_rate: 3.3436458579672035e-06
Step: 39210, train/epoch: 9.331271171569824
Step: 39220, train/loss: 0.21870000660419464
Step: 39220, train/grad_norm: 18.206682205200195
Step: 39220, train/learning_rate: 3.331746711410233e-06
Step: 39220, train/epoch: 9.333650588989258
Step: 39230, train/loss: 0.21879999339580536
Step: 39230, train/grad_norm: 16.524446487426758
Step: 39230, train/learning_rate: 3.3198477922269376e-06
Step: 39230, train/epoch: 9.336030006408691
Step: 39240, train/loss: 0.34040001034736633
Step: 39240, train/grad_norm: 13.701678276062012
Step: 39240, train/learning_rate: 3.307948645669967e-06
Step: 39240, train/epoch: 9.338410377502441
Step: 39250, train/loss: 0.37540000677108765
Step: 39250, train/grad_norm: 24.698299407958984
Step: 39250, train/learning_rate: 3.2960494991129963e-06
Step: 39250, train/epoch: 9.340789794921875
Step: 39260, train/loss: 0.2946999967098236
Step: 39260, train/grad_norm: 9.955150604248047
Step: 39260, train/learning_rate: 3.2841503525560256e-06
Step: 39260, train/epoch: 9.343170166015625
Step: 39270, train/loss: 0.30809998512268066
Step: 39270, train/grad_norm: 12.23141098022461
Step: 39270, train/learning_rate: 3.272251205999055e-06
Step: 39270, train/epoch: 9.345549583435059
Step: 39280, train/loss: 0.3287000060081482
Step: 39280, train/grad_norm: 8.343235969543457
Step: 39280, train/learning_rate: 3.2603522868157597e-06
Step: 39280, train/epoch: 9.347929954528809
Step: 39290, train/loss: 0.32100000977516174
Step: 39290, train/grad_norm: 7.1032586097717285
Step: 39290, train/learning_rate: 3.248453140258789e-06
Step: 39290, train/epoch: 9.350309371948242
Step: 39300, train/loss: 0.250900000333786
Step: 39300, train/grad_norm: 10.756675720214844
Step: 39300, train/learning_rate: 3.2365539937018184e-06
Step: 39300, train/epoch: 9.352688789367676
Step: 39310, train/loss: 0.4049000144004822
Step: 39310, train/grad_norm: 14.851367950439453
Step: 39310, train/learning_rate: 3.2246548471448477e-06
Step: 39310, train/epoch: 9.355069160461426
Step: 39320, train/loss: 0.30959999561309814
Step: 39320, train/grad_norm: 21.250118255615234
Step: 39320, train/learning_rate: 3.2127559279615525e-06
Step: 39320, train/epoch: 9.35744857788086
Step: 39330, train/loss: 0.23960000276565552
Step: 39330, train/grad_norm: 15.493118286132812
Step: 39330, train/learning_rate: 3.200856781404582e-06
Step: 39330, train/epoch: 9.35982894897461
Step: 39340, train/loss: 0.3303000032901764
Step: 39340, train/grad_norm: 21.1517391204834
Step: 39340, train/learning_rate: 3.188957634847611e-06
Step: 39340, train/epoch: 9.362208366394043
Step: 39350, train/loss: 0.3889999985694885
Step: 39350, train/grad_norm: 25.89599609375
Step: 39350, train/learning_rate: 3.1770584882906405e-06
Step: 39350, train/epoch: 9.364588737487793
Step: 39360, train/loss: 0.26429998874664307
Step: 39360, train/grad_norm: 5.223820686340332
Step: 39360, train/learning_rate: 3.16515934173367e-06
Step: 39360, train/epoch: 9.366968154907227
Step: 39370, train/loss: 0.49810001254081726
Step: 39370, train/grad_norm: 16.14912223815918
Step: 39370, train/learning_rate: 3.1532604225503746e-06
Step: 39370, train/epoch: 9.36934757232666
Step: 39380, train/loss: 0.4578000009059906
Step: 39380, train/grad_norm: 4.091394424438477
Step: 39380, train/learning_rate: 3.141361275993404e-06
Step: 39380, train/epoch: 9.37172794342041
Step: 39390, train/loss: 0.3386000096797943
Step: 39390, train/grad_norm: 17.342918395996094
Step: 39390, train/learning_rate: 3.1294621294364333e-06
Step: 39390, train/epoch: 9.374107360839844
Step: 39400, train/loss: 0.31940001249313354
Step: 39400, train/grad_norm: 2.5319178104400635
Step: 39400, train/learning_rate: 3.1175629828794627e-06
Step: 39400, train/epoch: 9.376487731933594
Step: 39410, train/loss: 0.2337000072002411
Step: 39410, train/grad_norm: 8.621456146240234
Step: 39410, train/learning_rate: 3.1056640636961674e-06
Step: 39410, train/epoch: 9.378867149353027
Step: 39420, train/loss: 0.35040000081062317
Step: 39420, train/grad_norm: 9.666516304016113
Step: 39420, train/learning_rate: 3.0937649171391968e-06
Step: 39420, train/epoch: 9.381246566772461
Step: 39430, train/loss: 0.18709999322891235
Step: 39430, train/grad_norm: 27.28241729736328
Step: 39430, train/learning_rate: 3.081865770582226e-06
Step: 39430, train/epoch: 9.383626937866211
Step: 39440, train/loss: 0.32120001316070557
Step: 39440, train/grad_norm: 19.874452590942383
Step: 39440, train/learning_rate: 3.0699666240252554e-06
Step: 39440, train/epoch: 9.386006355285645
Step: 39450, train/loss: 0.2849000096321106
Step: 39450, train/grad_norm: 4.3493757247924805
Step: 39450, train/learning_rate: 3.0580674774682848e-06
Step: 39450, train/epoch: 9.388386726379395
Step: 39460, train/loss: 0.2554999887943268
Step: 39460, train/grad_norm: 6.634105205535889
Step: 39460, train/learning_rate: 3.0461685582849896e-06
Step: 39460, train/epoch: 9.390766143798828
Step: 39470, train/loss: 0.29750001430511475
Step: 39470, train/grad_norm: 8.516945838928223
Step: 39470, train/learning_rate: 3.034269411728019e-06
Step: 39470, train/epoch: 9.393146514892578
Step: 39480, train/loss: 0.21899999678134918
Step: 39480, train/grad_norm: 0.7609568238258362
Step: 39480, train/learning_rate: 3.0223702651710482e-06
Step: 39480, train/epoch: 9.395525932312012
Step: 39490, train/loss: 0.3174999952316284
Step: 39490, train/grad_norm: 22.3200626373291
Step: 39490, train/learning_rate: 3.0104711186140776e-06
Step: 39490, train/epoch: 9.397905349731445
Step: 39500, train/loss: 0.4528000056743622
Step: 39500, train/grad_norm: 22.089954376220703
Step: 39500, train/learning_rate: 2.9985721994307823e-06
Step: 39500, train/epoch: 9.400285720825195
Step: 39510, train/loss: 0.3433000147342682
Step: 39510, train/grad_norm: 11.995000839233398
Step: 39510, train/learning_rate: 2.9866730528738117e-06
Step: 39510, train/epoch: 9.402665138244629
Step: 39520, train/loss: 0.40380001068115234
Step: 39520, train/grad_norm: 20.21840476989746
Step: 39520, train/learning_rate: 2.974773906316841e-06
Step: 39520, train/epoch: 9.405045509338379
Step: 39530, train/loss: 0.35760000348091125
Step: 39530, train/grad_norm: 22.341876983642578
Step: 39530, train/learning_rate: 2.9628747597598704e-06
Step: 39530, train/epoch: 9.407424926757812
Step: 39540, train/loss: 0.3662000000476837
Step: 39540, train/grad_norm: 18.351043701171875
Step: 39540, train/learning_rate: 2.9509756132028997e-06
Step: 39540, train/epoch: 9.409805297851562
Step: 39550, train/loss: 0.27489998936653137
Step: 39550, train/grad_norm: 7.023471832275391
Step: 39550, train/learning_rate: 2.9390766940196045e-06
Step: 39550, train/epoch: 9.412184715270996
Step: 39560, train/loss: 0.4099000096321106
Step: 39560, train/grad_norm: 18.466188430786133
Step: 39560, train/learning_rate: 2.927177547462634e-06
Step: 39560, train/epoch: 9.41456413269043
Step: 39570, train/loss: 0.3732999861240387
Step: 39570, train/grad_norm: 4.537690162658691
Step: 39570, train/learning_rate: 2.915278400905663e-06
Step: 39570, train/epoch: 9.41694450378418
Step: 39580, train/loss: 0.49709999561309814
Step: 39580, train/grad_norm: 22.787546157836914
Step: 39580, train/learning_rate: 2.9033792543486925e-06
Step: 39580, train/epoch: 9.419323921203613
Step: 39590, train/loss: 0.328900009393692
Step: 39590, train/grad_norm: 5.709521770477295
Step: 39590, train/learning_rate: 2.8914803351653973e-06
Step: 39590, train/epoch: 9.421704292297363
Step: 39600, train/loss: 0.2590999901294708
Step: 39600, train/grad_norm: 9.000123023986816
Step: 39600, train/learning_rate: 2.8795811886084266e-06
Step: 39600, train/epoch: 9.424083709716797
Step: 39610, train/loss: 0.37119999527931213
Step: 39610, train/grad_norm: 13.811100006103516
Step: 39610, train/learning_rate: 2.867682042051456e-06
Step: 39610, train/epoch: 9.42646312713623
Step: 39620, train/loss: 0.38580000400543213
Step: 39620, train/grad_norm: 16.07246208190918
Step: 39620, train/learning_rate: 2.8557828954944853e-06
Step: 39620, train/epoch: 9.42884349822998
Step: 39630, train/loss: 0.26759999990463257
Step: 39630, train/grad_norm: 2.759845018386841
Step: 39630, train/learning_rate: 2.84388397631119e-06
Step: 39630, train/epoch: 9.431222915649414
Step: 39640, train/loss: 0.2935999929904938
Step: 39640, train/grad_norm: 15.487165451049805
Step: 39640, train/learning_rate: 2.8319848297542194e-06
Step: 39640, train/epoch: 9.433603286743164
Step: 39650, train/loss: 0.43810001015663147
Step: 39650, train/grad_norm: 13.597854614257812
Step: 39650, train/learning_rate: 2.8200856831972487e-06
Step: 39650, train/epoch: 9.435982704162598
Step: 39660, train/loss: 0.29649999737739563
Step: 39660, train/grad_norm: 10.49207592010498
Step: 39660, train/learning_rate: 2.808186536640278e-06
Step: 39660, train/epoch: 9.438363075256348
Step: 39670, train/loss: 0.26420000195503235
Step: 39670, train/grad_norm: 14.29064655303955
Step: 39670, train/learning_rate: 2.7962873900833074e-06
Step: 39670, train/epoch: 9.440742492675781
Step: 39680, train/loss: 0.36169999837875366
Step: 39680, train/grad_norm: 24.782270431518555
Step: 39680, train/learning_rate: 2.784388470900012e-06
Step: 39680, train/epoch: 9.443121910095215
Step: 39690, train/loss: 0.27720001339912415
Step: 39690, train/grad_norm: 16.897937774658203
Step: 39690, train/learning_rate: 2.7724893243430415e-06
Step: 39690, train/epoch: 9.445502281188965
Step: 39700, train/loss: 0.2799000144004822
Step: 39700, train/grad_norm: 7.055731773376465
Step: 39700, train/learning_rate: 2.760590177786071e-06
Step: 39700, train/epoch: 9.447881698608398
Step: 39710, train/loss: 0.24060000479221344
Step: 39710, train/grad_norm: 6.039803504943848
Step: 39710, train/learning_rate: 2.7486910312291e-06
Step: 39710, train/epoch: 9.450262069702148
Step: 39720, train/loss: 0.319599986076355
Step: 39720, train/grad_norm: 7.750549793243408
Step: 39720, train/learning_rate: 2.736792112045805e-06
Step: 39720, train/epoch: 9.452641487121582
Step: 39730, train/loss: 0.4675000011920929
Step: 39730, train/grad_norm: 18.879316329956055
Step: 39730, train/learning_rate: 2.7248929654888343e-06
Step: 39730, train/epoch: 9.455021858215332
Step: 39740, train/loss: 0.34769999980926514
Step: 39740, train/grad_norm: 6.679988384246826
Step: 39740, train/learning_rate: 2.7129938189318636e-06
Step: 39740, train/epoch: 9.457401275634766
Step: 39750, train/loss: 0.35370001196861267
Step: 39750, train/grad_norm: 20.206968307495117
Step: 39750, train/learning_rate: 2.701094672374893e-06
Step: 39750, train/epoch: 9.4597806930542
Step: 39760, train/loss: 0.18520000576972961
Step: 39760, train/grad_norm: 5.78718900680542
Step: 39760, train/learning_rate: 2.6891955258179223e-06
Step: 39760, train/epoch: 9.46216106414795
Step: 39770, train/loss: 0.24459999799728394
Step: 39770, train/grad_norm: 8.771675109863281
Step: 39770, train/learning_rate: 2.677296606634627e-06
Step: 39770, train/epoch: 9.464540481567383
Step: 39780, train/loss: 0.49559998512268066
Step: 39780, train/grad_norm: 2.091432571411133
Step: 39780, train/learning_rate: 2.6653974600776564e-06
Step: 39780, train/epoch: 9.466920852661133
Step: 39790, train/loss: 0.31459999084472656
Step: 39790, train/grad_norm: 17.22274398803711
Step: 39790, train/learning_rate: 2.6534983135206858e-06
Step: 39790, train/epoch: 9.469300270080566
Step: 39800, train/loss: 0.3142000138759613
Step: 39800, train/grad_norm: 18.550025939941406
Step: 39800, train/learning_rate: 2.641599166963715e-06
Step: 39800, train/epoch: 9.4716796875
Step: 39810, train/loss: 0.32330000400543213
Step: 39810, train/grad_norm: 7.2716827392578125
Step: 39810, train/learning_rate: 2.62970024778042e-06
Step: 39810, train/epoch: 9.47406005859375
Step: 39820, train/loss: 0.27970001101493835
Step: 39820, train/grad_norm: 11.036612510681152
Step: 39820, train/learning_rate: 2.6178011012234492e-06
Step: 39820, train/epoch: 9.476439476013184
Step: 39830, train/loss: 0.5824999809265137
Step: 39830, train/grad_norm: 24.216594696044922
Step: 39830, train/learning_rate: 2.6059019546664786e-06
Step: 39830, train/epoch: 9.478819847106934
Step: 39840, train/loss: 0.289000004529953
Step: 39840, train/grad_norm: 11.741107940673828
Step: 39840, train/learning_rate: 2.594002808109508e-06
Step: 39840, train/epoch: 9.481199264526367
Step: 39850, train/loss: 0.24650000035762787
Step: 39850, train/grad_norm: 13.635049819946289
Step: 39850, train/learning_rate: 2.5821036615525372e-06
Step: 39850, train/epoch: 9.483579635620117
Step: 39860, train/loss: 0.33959999680519104
Step: 39860, train/grad_norm: 27.157358169555664
Step: 39860, train/learning_rate: 2.570204742369242e-06
Step: 39860, train/epoch: 9.48595905303955
Step: 39870, train/loss: 0.3370000123977661
Step: 39870, train/grad_norm: 4.756092548370361
Step: 39870, train/learning_rate: 2.5583055958122713e-06
Step: 39870, train/epoch: 9.488338470458984
Step: 39880, train/loss: 0.2833999991416931
Step: 39880, train/grad_norm: 6.979506969451904
Step: 39880, train/learning_rate: 2.5464064492553007e-06
Step: 39880, train/epoch: 9.490718841552734
Step: 39890, train/loss: 0.2782000005245209
Step: 39890, train/grad_norm: 3.317176580429077
Step: 39890, train/learning_rate: 2.53450730269833e-06
Step: 39890, train/epoch: 9.493098258972168
Step: 39900, train/loss: 0.4018000066280365
Step: 39900, train/grad_norm: 13.163200378417969
Step: 39900, train/learning_rate: 2.522608383515035e-06
Step: 39900, train/epoch: 9.495478630065918
Step: 39910, train/loss: 0.3334999978542328
Step: 39910, train/grad_norm: 26.39348602294922
Step: 39910, train/learning_rate: 2.510709236958064e-06
Step: 39910, train/epoch: 9.497858047485352
Step: 39920, train/loss: 0.4018999934196472
Step: 39920, train/grad_norm: 3.739108085632324
Step: 39920, train/learning_rate: 2.4988100904010935e-06
Step: 39920, train/epoch: 9.500238418579102
Step: 39930, train/loss: 0.4327000081539154
Step: 39930, train/grad_norm: 15.601764678955078
Step: 39930, train/learning_rate: 2.486910943844123e-06
Step: 39930, train/epoch: 9.502617835998535
Step: 39940, train/loss: 0.2766000032424927
Step: 39940, train/grad_norm: 33.741485595703125
Step: 39940, train/learning_rate: 2.475011797287152e-06
Step: 39940, train/epoch: 9.504997253417969
Step: 39950, train/loss: 0.3303000032901764
Step: 39950, train/grad_norm: 14.254663467407227
Step: 39950, train/learning_rate: 2.463112878103857e-06
Step: 39950, train/epoch: 9.507377624511719
Step: 39960, train/loss: 0.4081000089645386
Step: 39960, train/grad_norm: 12.326125144958496
Step: 39960, train/learning_rate: 2.4512137315468863e-06
Step: 39960, train/epoch: 9.509757041931152
Step: 39970, train/loss: 0.40470001101493835
Step: 39970, train/grad_norm: 8.426082611083984
Step: 39970, train/learning_rate: 2.4393145849899156e-06
Step: 39970, train/epoch: 9.512137413024902
Step: 39980, train/loss: 0.3950999975204468
Step: 39980, train/grad_norm: 12.085686683654785
Step: 39980, train/learning_rate: 2.427415438432945e-06
Step: 39980, train/epoch: 9.514516830444336
Step: 39990, train/loss: 0.5494999885559082
Step: 39990, train/grad_norm: 20.225906372070312
Step: 39990, train/learning_rate: 2.4155165192496497e-06
Step: 39990, train/epoch: 9.51689624786377
Step: 40000, train/loss: 0.436599999666214
Step: 40000, train/grad_norm: 1.3913016319274902
Step: 40000, train/learning_rate: 2.403617372692679e-06
Step: 40000, train/epoch: 9.51927661895752
Step: 40010, train/loss: 0.2752000093460083
Step: 40010, train/grad_norm: 5.103468418121338
Step: 40010, train/learning_rate: 2.3917182261357084e-06
Step: 40010, train/epoch: 9.521656036376953
Step: 40020, train/loss: 0.44429999589920044
Step: 40020, train/grad_norm: 13.0073881149292
Step: 40020, train/learning_rate: 2.3798190795787377e-06
Step: 40020, train/epoch: 9.524036407470703
Step: 40030, train/loss: 0.31940001249313354
Step: 40030, train/grad_norm: 13.254725456237793
Step: 40030, train/learning_rate: 2.367919933021767e-06
Step: 40030, train/epoch: 9.526415824890137
Step: 40040, train/loss: 0.3084000051021576
Step: 40040, train/grad_norm: 9.479974746704102
Step: 40040, train/learning_rate: 2.356021013838472e-06
Step: 40040, train/epoch: 9.528796195983887
Step: 40050, train/loss: 0.3174999952316284
Step: 40050, train/grad_norm: 39.97154235839844
Step: 40050, train/learning_rate: 2.344121867281501e-06
Step: 40050, train/epoch: 9.53117561340332
Step: 40060, train/loss: 0.26429998874664307
Step: 40060, train/grad_norm: 5.338494777679443
Step: 40060, train/learning_rate: 2.3322227207245305e-06
Step: 40060, train/epoch: 9.533555030822754
Step: 40070, train/loss: 0.3336000144481659
Step: 40070, train/grad_norm: 8.72675895690918
Step: 40070, train/learning_rate: 2.32032357416756e-06
Step: 40070, train/epoch: 9.535935401916504
Step: 40080, train/loss: 0.2540000081062317
Step: 40080, train/grad_norm: 6.420202255249023
Step: 40080, train/learning_rate: 2.3084246549842646e-06
Step: 40080, train/epoch: 9.538314819335938
Step: 40090, train/loss: 0.2685000002384186
Step: 40090, train/grad_norm: 11.23936939239502
Step: 40090, train/learning_rate: 2.296525508427294e-06
Step: 40090, train/epoch: 9.540695190429688
Step: 40100, train/loss: 0.25279998779296875
Step: 40100, train/grad_norm: 7.3788933753967285
Step: 40100, train/learning_rate: 2.2846263618703233e-06
Step: 40100, train/epoch: 9.543074607849121
Step: 40110, train/loss: 0.2159000039100647
Step: 40110, train/grad_norm: 14.264647483825684
Step: 40110, train/learning_rate: 2.2727272153133526e-06
Step: 40110, train/epoch: 9.545454978942871
Step: 40120, train/loss: 0.27399998903274536
Step: 40120, train/grad_norm: 21.15350914001465
Step: 40120, train/learning_rate: 2.260828068756382e-06
Step: 40120, train/epoch: 9.547834396362305
Step: 40130, train/loss: 0.3513000011444092
Step: 40130, train/grad_norm: 12.001861572265625
Step: 40130, train/learning_rate: 2.2489291495730868e-06
Step: 40130, train/epoch: 9.550213813781738
Step: 40140, train/loss: 0.42340001463890076
Step: 40140, train/grad_norm: 13.978836059570312
Step: 40140, train/learning_rate: 2.237030003016116e-06
Step: 40140, train/epoch: 9.552594184875488
Step: 40150, train/loss: 0.22269999980926514
Step: 40150, train/grad_norm: 6.452093601226807
Step: 40150, train/learning_rate: 2.2251308564591454e-06
Step: 40150, train/epoch: 9.554973602294922
Step: 40160, train/loss: 0.30469998717308044
Step: 40160, train/grad_norm: 15.070601463317871
Step: 40160, train/learning_rate: 2.2132317099021748e-06
Step: 40160, train/epoch: 9.557353973388672
Step: 40170, train/loss: 0.2223999947309494
Step: 40170, train/grad_norm: 13.12247085571289
Step: 40170, train/learning_rate: 2.2013327907188796e-06
Step: 40170, train/epoch: 9.559733390808105
Step: 40180, train/loss: 0.27059999108314514
Step: 40180, train/grad_norm: 7.64410924911499
Step: 40180, train/learning_rate: 2.189433644161909e-06
Step: 40180, train/epoch: 9.562112808227539
Step: 40190, train/loss: 0.20399999618530273
Step: 40190, train/grad_norm: 3.3880560398101807
Step: 40190, train/learning_rate: 2.1775344976049382e-06
Step: 40190, train/epoch: 9.564493179321289
Step: 40200, train/loss: 0.43860000371932983
Step: 40200, train/grad_norm: 13.041412353515625
Step: 40200, train/learning_rate: 2.1656353510479676e-06
Step: 40200, train/epoch: 9.566872596740723
Step: 40210, train/loss: 0.30090001225471497
Step: 40210, train/grad_norm: 3.4679462909698486
Step: 40210, train/learning_rate: 2.153736204490997e-06
Step: 40210, train/epoch: 9.569252967834473
Step: 40220, train/loss: 0.2558000087738037
Step: 40220, train/grad_norm: 16.900651931762695
Step: 40220, train/learning_rate: 2.1418372853077017e-06
Step: 40220, train/epoch: 9.571632385253906
Step: 40230, train/loss: 0.3571000099182129
Step: 40230, train/grad_norm: 15.017895698547363
Step: 40230, train/learning_rate: 2.129938138750731e-06
Step: 40230, train/epoch: 9.574012756347656
Step: 40240, train/loss: 0.2483000010251999
Step: 40240, train/grad_norm: 4.5438642501831055
Step: 40240, train/learning_rate: 2.1180389921937604e-06
Step: 40240, train/epoch: 9.57639217376709
Step: 40250, train/loss: 0.34139999747276306
Step: 40250, train/grad_norm: 19.11103057861328
Step: 40250, train/learning_rate: 2.1061398456367897e-06
Step: 40250, train/epoch: 9.578771591186523
Step: 40260, train/loss: 0.35269999504089355
Step: 40260, train/grad_norm: 15.017653465270996
Step: 40260, train/learning_rate: 2.0942409264534945e-06
Step: 40260, train/epoch: 9.581151962280273
Step: 40270, train/loss: 0.31790000200271606
Step: 40270, train/grad_norm: 4.284476280212402
Step: 40270, train/learning_rate: 2.082341779896524e-06
Step: 40270, train/epoch: 9.583531379699707
Step: 40280, train/loss: 0.1859000027179718
Step: 40280, train/grad_norm: 8.984294891357422
Step: 40280, train/learning_rate: 2.070442633339553e-06
Step: 40280, train/epoch: 9.585911750793457
Step: 40290, train/loss: 0.48179998993873596
Step: 40290, train/grad_norm: 14.214665412902832
Step: 40290, train/learning_rate: 2.0585434867825825e-06
Step: 40290, train/epoch: 9.58829116821289
Step: 40300, train/loss: 0.42160001397132874
Step: 40300, train/grad_norm: 11.900177955627441
Step: 40300, train/learning_rate: 2.0466445675992873e-06
Step: 40300, train/epoch: 9.59067153930664
Step: 40310, train/loss: 0.2648000121116638
Step: 40310, train/grad_norm: 6.317582607269287
Step: 40310, train/learning_rate: 2.0347454210423166e-06
Step: 40310, train/epoch: 9.593050956726074
Step: 40320, train/loss: 0.2969000041484833
Step: 40320, train/grad_norm: 13.563582420349121
Step: 40320, train/learning_rate: 2.022846274485346e-06
Step: 40320, train/epoch: 9.595430374145508
Step: 40330, train/loss: 0.43970000743865967
Step: 40330, train/grad_norm: 15.423653602600098
Step: 40330, train/learning_rate: 2.0109471279283753e-06
Step: 40330, train/epoch: 9.597810745239258
Step: 40340, train/loss: 0.36250001192092896
Step: 40340, train/grad_norm: 10.942336082458496
Step: 40340, train/learning_rate: 1.9990479813714046e-06
Step: 40340, train/epoch: 9.600190162658691
Step: 40350, train/loss: 0.28949999809265137
Step: 40350, train/grad_norm: 11.540489196777344
Step: 40350, train/learning_rate: 1.9871490621881094e-06
Step: 40350, train/epoch: 9.602570533752441
Step: 40360, train/loss: 0.5307999849319458
Step: 40360, train/grad_norm: 13.939949035644531
Step: 40360, train/learning_rate: 1.9752499156311387e-06
Step: 40360, train/epoch: 9.604949951171875
Step: 40370, train/loss: 0.3061999976634979
Step: 40370, train/grad_norm: 24.999073028564453
Step: 40370, train/learning_rate: 1.963350769074168e-06
Step: 40370, train/epoch: 9.607329368591309
Step: 40380, train/loss: 0.462799996137619
Step: 40380, train/grad_norm: 23.380016326904297
Step: 40380, train/learning_rate: 1.9514516225171974e-06
Step: 40380, train/epoch: 9.609709739685059
Step: 40390, train/loss: 0.45559999346733093
Step: 40390, train/grad_norm: 33.48478698730469
Step: 40390, train/learning_rate: 1.939552703333902e-06
Step: 40390, train/epoch: 9.612089157104492
Step: 40400, train/loss: 0.2921000123023987
Step: 40400, train/grad_norm: 9.955299377441406
Step: 40400, train/learning_rate: 1.9276535567769315e-06
Step: 40400, train/epoch: 9.614469528198242
Step: 40410, train/loss: 0.2524000108242035
Step: 40410, train/grad_norm: 4.476401329040527
Step: 40410, train/learning_rate: 1.915754410219961e-06
Step: 40410, train/epoch: 9.616848945617676
Step: 40420, train/loss: 0.37290000915527344
Step: 40420, train/grad_norm: 36.70043182373047
Step: 40420, train/learning_rate: 1.9038552636629902e-06
Step: 40420, train/epoch: 9.619229316711426
Step: 40430, train/loss: 0.2825999855995178
Step: 40430, train/grad_norm: 14.185589790344238
Step: 40430, train/learning_rate: 1.8919562307928572e-06
Step: 40430, train/epoch: 9.62160873413086
Step: 40440, train/loss: 0.34610000252723694
Step: 40440, train/grad_norm: 3.88126540184021
Step: 40440, train/learning_rate: 1.8800570842358866e-06
Step: 40440, train/epoch: 9.623988151550293
Step: 40450, train/loss: 0.35850000381469727
Step: 40450, train/grad_norm: 9.03288459777832
Step: 40450, train/learning_rate: 1.8681580513657536e-06
Step: 40450, train/epoch: 9.626368522644043
Step: 40460, train/loss: 0.18799999356269836
Step: 40460, train/grad_norm: 0.739648699760437
Step: 40460, train/learning_rate: 1.856258904808783e-06
Step: 40460, train/epoch: 9.628747940063477
Step: 40470, train/loss: 0.34380000829696655
Step: 40470, train/grad_norm: 19.520618438720703
Step: 40470, train/learning_rate: 1.84435987193865e-06
Step: 40470, train/epoch: 9.631128311157227
Step: 40480, train/loss: 0.23350000381469727
Step: 40480, train/grad_norm: 0.641382098197937
Step: 40480, train/learning_rate: 1.8324607253816794e-06
Step: 40480, train/epoch: 9.63350772857666
Step: 40490, train/loss: 0.21660000085830688
Step: 40490, train/grad_norm: 28.939666748046875
Step: 40490, train/learning_rate: 1.8205616925115464e-06
Step: 40490, train/epoch: 9.63588809967041
Step: 40500, train/loss: 0.26840001344680786
Step: 40500, train/grad_norm: 25.84392738342285
Step: 40500, train/learning_rate: 1.8086625459545758e-06
Step: 40500, train/epoch: 9.638267517089844
Step: 40510, train/loss: 0.4496999979019165
Step: 40510, train/grad_norm: 23.86865997314453
Step: 40510, train/learning_rate: 1.796763399397605e-06
Step: 40510, train/epoch: 9.640646934509277
Step: 40520, train/loss: 0.2605000138282776
Step: 40520, train/grad_norm: 10.363787651062012
Step: 40520, train/learning_rate: 1.7848643665274722e-06
Step: 40520, train/epoch: 9.643027305603027
Step: 40530, train/loss: 0.22519999742507935
Step: 40530, train/grad_norm: 24.621150970458984
Step: 40530, train/learning_rate: 1.7729652199705015e-06
Step: 40530, train/epoch: 9.645406723022461
Step: 40540, train/loss: 0.2992999851703644
Step: 40540, train/grad_norm: 16.005353927612305
Step: 40540, train/learning_rate: 1.7610661871003686e-06
Step: 40540, train/epoch: 9.647787094116211
Step: 40550, train/loss: 0.4081000089645386
Step: 40550, train/grad_norm: 12.389640808105469
Step: 40550, train/learning_rate: 1.7491670405433979e-06
Step: 40550, train/epoch: 9.650166511535645
Step: 40560, train/loss: 0.3589000105857849
Step: 40560, train/grad_norm: 25.307077407836914
Step: 40560, train/learning_rate: 1.737268007673265e-06
Step: 40560, train/epoch: 9.652546882629395
Step: 40570, train/loss: 0.24390000104904175
Step: 40570, train/grad_norm: 3.9735615253448486
Step: 40570, train/learning_rate: 1.7253688611162943e-06
Step: 40570, train/epoch: 9.654926300048828
Step: 40580, train/loss: 0.2865999937057495
Step: 40580, train/grad_norm: 19.59095001220703
Step: 40580, train/learning_rate: 1.7134698282461613e-06
Step: 40580, train/epoch: 9.657305717468262
Step: 40590, train/loss: 0.42250001430511475
Step: 40590, train/grad_norm: 17.02446174621582
Step: 40590, train/learning_rate: 1.7015706816891907e-06
Step: 40590, train/epoch: 9.659686088562012
Step: 40600, train/loss: 0.2849000096321106
Step: 40600, train/grad_norm: 13.544439315795898
Step: 40600, train/learning_rate: 1.68967153513222e-06
Step: 40600, train/epoch: 9.662065505981445
Step: 40610, train/loss: 0.37770000100135803
Step: 40610, train/grad_norm: 19.0008602142334
Step: 40610, train/learning_rate: 1.677772502262087e-06
Step: 40610, train/epoch: 9.664445877075195
Step: 40620, train/loss: 0.31029999256134033
Step: 40620, train/grad_norm: 1.5113543272018433
Step: 40620, train/learning_rate: 1.6658733557051164e-06
Step: 40620, train/epoch: 9.666825294494629
Step: 40630, train/loss: 0.18170000612735748
Step: 40630, train/grad_norm: 6.721042633056641
Step: 40630, train/learning_rate: 1.6539743228349835e-06
Step: 40630, train/epoch: 9.669204711914062
Step: 40640, train/loss: 0.3741999864578247
Step: 40640, train/grad_norm: 11.384838104248047
Step: 40640, train/learning_rate: 1.6420751762780128e-06
Step: 40640, train/epoch: 9.671585083007812
Step: 40650, train/loss: 0.26660001277923584
Step: 40650, train/grad_norm: 18.89733123779297
Step: 40650, train/learning_rate: 1.6301761434078799e-06
Step: 40650, train/epoch: 9.673964500427246
Step: 40660, train/loss: 0.4645000100135803
Step: 40660, train/grad_norm: 18.906076431274414
Step: 40660, train/learning_rate: 1.6182769968509092e-06
Step: 40660, train/epoch: 9.676344871520996
Step: 40670, train/loss: 0.3961000144481659
Step: 40670, train/grad_norm: 10.420814514160156
Step: 40670, train/learning_rate: 1.6063779639807763e-06
Step: 40670, train/epoch: 9.67872428894043
Step: 40680, train/loss: 0.3010999858379364
Step: 40680, train/grad_norm: 6.814167499542236
Step: 40680, train/learning_rate: 1.5944788174238056e-06
Step: 40680, train/epoch: 9.68110466003418
Step: 40690, train/loss: 0.2671999931335449
Step: 40690, train/grad_norm: 29.658876419067383
Step: 40690, train/learning_rate: 1.582579670866835e-06
Step: 40690, train/epoch: 9.683484077453613
Step: 40700, train/loss: 0.2906000018119812
Step: 40700, train/grad_norm: 15.845698356628418
Step: 40700, train/learning_rate: 1.570680637996702e-06
Step: 40700, train/epoch: 9.685863494873047
Step: 40710, train/loss: 0.28209999203681946
Step: 40710, train/grad_norm: 15.77426528930664
Step: 40710, train/learning_rate: 1.5587814914397313e-06
Step: 40710, train/epoch: 9.688243865966797
Step: 40720, train/loss: 0.31709998846054077
Step: 40720, train/grad_norm: 24.11762237548828
Step: 40720, train/learning_rate: 1.5468824585695984e-06
Step: 40720, train/epoch: 9.69062328338623
Step: 40730, train/loss: 0.46700000762939453
Step: 40730, train/grad_norm: 4.493687152862549
Step: 40730, train/learning_rate: 1.5349833120126277e-06
Step: 40730, train/epoch: 9.69300365447998
Step: 40740, train/loss: 0.41999998688697815
Step: 40740, train/grad_norm: 6.067651748657227
Step: 40740, train/learning_rate: 1.5230842791424948e-06
Step: 40740, train/epoch: 9.695383071899414
Step: 40750, train/loss: 0.3212999999523163
Step: 40750, train/grad_norm: 8.64220142364502
Step: 40750, train/learning_rate: 1.5111851325855241e-06
Step: 40750, train/epoch: 9.697763442993164
Step: 40760, train/loss: 0.6205000281333923
Step: 40760, train/grad_norm: 22.8145751953125
Step: 40760, train/learning_rate: 1.4992860997153912e-06
Step: 40760, train/epoch: 9.700142860412598
Step: 40770, train/loss: 0.44679999351501465
Step: 40770, train/grad_norm: 13.789978981018066
Step: 40770, train/learning_rate: 1.4873869531584205e-06
Step: 40770, train/epoch: 9.702522277832031
Step: 40780, train/loss: 0.23980000615119934
Step: 40780, train/grad_norm: 10.226807594299316
Step: 40780, train/learning_rate: 1.4754878066014498e-06
Step: 40780, train/epoch: 9.704902648925781
Step: 40790, train/loss: 0.36160001158714294
Step: 40790, train/grad_norm: 19.995826721191406
Step: 40790, train/learning_rate: 1.463588773731317e-06
Step: 40790, train/epoch: 9.707282066345215
Step: 40800, train/loss: 0.2295999974012375
Step: 40800, train/grad_norm: 8.713309288024902
Step: 40800, train/learning_rate: 1.4516896271743462e-06
Step: 40800, train/epoch: 9.709662437438965
Step: 40810, train/loss: 0.15620000660419464
Step: 40810, train/grad_norm: 9.538078308105469
Step: 40810, train/learning_rate: 1.4397905943042133e-06
Step: 40810, train/epoch: 9.712041854858398
Step: 40820, train/loss: 0.3725999891757965
Step: 40820, train/grad_norm: 8.12277889251709
Step: 40820, train/learning_rate: 1.4278914477472426e-06
Step: 40820, train/epoch: 9.714421272277832
Step: 40830, train/loss: 0.29820001125335693
Step: 40830, train/grad_norm: 6.5871901512146
Step: 40830, train/learning_rate: 1.4159924148771097e-06
Step: 40830, train/epoch: 9.716801643371582
Step: 40840, train/loss: 0.3160000145435333
Step: 40840, train/grad_norm: 10.696660041809082
Step: 40840, train/learning_rate: 1.404093268320139e-06
Step: 40840, train/epoch: 9.719181060791016
Step: 40850, train/loss: 0.3490000069141388
Step: 40850, train/grad_norm: 6.5233917236328125
Step: 40850, train/learning_rate: 1.392194235450006e-06
Step: 40850, train/epoch: 9.721561431884766
Step: 40860, train/loss: 0.21449999511241913
Step: 40860, train/grad_norm: 22.177654266357422
Step: 40860, train/learning_rate: 1.3802950888930354e-06
Step: 40860, train/epoch: 9.7239408493042
Step: 40870, train/loss: 0.34139999747276306
Step: 40870, train/grad_norm: 32.734275817871094
Step: 40870, train/learning_rate: 1.3683960560229025e-06
Step: 40870, train/epoch: 9.72632122039795
Step: 40880, train/loss: 0.3353999853134155
Step: 40880, train/grad_norm: 1.8286110162734985
Step: 40880, train/learning_rate: 1.3564969094659318e-06
Step: 40880, train/epoch: 9.728700637817383
Step: 40890, train/loss: 0.3578000068664551
Step: 40890, train/grad_norm: 3.195131540298462
Step: 40890, train/learning_rate: 1.3445977629089612e-06
Step: 40890, train/epoch: 9.731080055236816
Step: 40900, train/loss: 0.3443000018596649
Step: 40900, train/grad_norm: 29.031644821166992
Step: 40900, train/learning_rate: 1.3326987300388282e-06
Step: 40900, train/epoch: 9.733460426330566
Step: 40910, train/loss: 0.3573000133037567
Step: 40910, train/grad_norm: 4.43837833404541
Step: 40910, train/learning_rate: 1.3207995834818576e-06
Step: 40910, train/epoch: 9.73583984375
Step: 40920, train/loss: 0.38119998574256897
Step: 40920, train/grad_norm: 26.470340728759766
Step: 40920, train/learning_rate: 1.3089005506117246e-06
Step: 40920, train/epoch: 9.73822021484375
Step: 40930, train/loss: 0.4514000117778778
Step: 40930, train/grad_norm: 15.052114486694336
Step: 40930, train/learning_rate: 1.297001404054754e-06
Step: 40930, train/epoch: 9.740599632263184
Step: 40940, train/loss: 0.28519999980926514
Step: 40940, train/grad_norm: 9.94315242767334
Step: 40940, train/learning_rate: 1.285102371184621e-06
Step: 40940, train/epoch: 9.742980003356934
Step: 40950, train/loss: 0.3582000136375427
Step: 40950, train/grad_norm: 17.45138931274414
Step: 40950, train/learning_rate: 1.2732032246276503e-06
Step: 40950, train/epoch: 9.745359420776367
Step: 40960, train/loss: 0.29159998893737793
Step: 40960, train/grad_norm: 6.315725326538086
Step: 40960, train/learning_rate: 1.2613041917575174e-06
Step: 40960, train/epoch: 9.7477388381958
Step: 40970, train/loss: 0.36970001459121704
Step: 40970, train/grad_norm: 15.840518951416016
Step: 40970, train/learning_rate: 1.2494050452005467e-06
Step: 40970, train/epoch: 9.75011920928955
Step: 40980, train/loss: 0.3562000095844269
Step: 40980, train/grad_norm: 14.781134605407715
Step: 40980, train/learning_rate: 1.237505898643576e-06
Step: 40980, train/epoch: 9.752498626708984
Step: 40990, train/loss: 0.3919000029563904
Step: 40990, train/grad_norm: 23.035451889038086
Step: 40990, train/learning_rate: 1.2256068657734431e-06
Step: 40990, train/epoch: 9.754878997802734
Step: 41000, train/loss: 0.2775999903678894
Step: 41000, train/grad_norm: 11.477593421936035
Step: 41000, train/learning_rate: 1.2137077192164725e-06
Step: 41000, train/epoch: 9.757258415222168
Step: 41010, train/loss: 0.3240000009536743
Step: 41010, train/grad_norm: 10.071710586547852
Step: 41010, train/learning_rate: 1.2018086863463395e-06
Step: 41010, train/epoch: 9.759637832641602
Step: 41020, train/loss: 0.2443999946117401
Step: 41020, train/grad_norm: 6.495020866394043
Step: 41020, train/learning_rate: 1.1899095397893689e-06
Step: 41020, train/epoch: 9.762018203735352
Step: 41030, train/loss: 0.4803999960422516
Step: 41030, train/grad_norm: 1.389263391494751
Step: 41030, train/learning_rate: 1.178010506919236e-06
Step: 41030, train/epoch: 9.764397621154785
Step: 41040, train/loss: 0.3961000144481659
Step: 41040, train/grad_norm: 8.58253002166748
Step: 41040, train/learning_rate: 1.1661113603622653e-06
Step: 41040, train/epoch: 9.766777992248535
Step: 41050, train/loss: 0.2460000067949295
Step: 41050, train/grad_norm: 5.757673263549805
Step: 41050, train/learning_rate: 1.1542123274921323e-06
Step: 41050, train/epoch: 9.769157409667969
Step: 41060, train/loss: 0.47029998898506165
Step: 41060, train/grad_norm: 25.149940490722656
Step: 41060, train/learning_rate: 1.1423131809351617e-06
Step: 41060, train/epoch: 9.771537780761719
Step: 41070, train/loss: 0.20270000398159027
Step: 41070, train/grad_norm: 4.313989162445068
Step: 41070, train/learning_rate: 1.130414034378191e-06
Step: 41070, train/epoch: 9.773917198181152
Step: 41080, train/loss: 0.40310001373291016
Step: 41080, train/grad_norm: 11.645272254943848
Step: 41080, train/learning_rate: 1.118515001508058e-06
Step: 41080, train/epoch: 9.776296615600586
Step: 41090, train/loss: 0.23100000619888306
Step: 41090, train/grad_norm: 8.640857696533203
Step: 41090, train/learning_rate: 1.1066158549510874e-06
Step: 41090, train/epoch: 9.778676986694336
Step: 41100, train/loss: 0.43230000138282776
Step: 41100, train/grad_norm: 10.209599494934082
Step: 41100, train/learning_rate: 1.0947168220809544e-06
Step: 41100, train/epoch: 9.78105640411377
Step: 41110, train/loss: 0.28839999437332153
Step: 41110, train/grad_norm: 5.235450267791748
Step: 41110, train/learning_rate: 1.0828176755239838e-06
Step: 41110, train/epoch: 9.78343677520752
Step: 41120, train/loss: 0.3264000117778778
Step: 41120, train/grad_norm: 18.692928314208984
Step: 41120, train/learning_rate: 1.0709186426538508e-06
Step: 41120, train/epoch: 9.785816192626953
Step: 41130, train/loss: 0.37119999527931213
Step: 41130, train/grad_norm: 22.26751136779785
Step: 41130, train/learning_rate: 1.0590194960968802e-06
Step: 41130, train/epoch: 9.788196563720703
Step: 41140, train/loss: 0.2549000084400177
Step: 41140, train/grad_norm: 14.694380760192871
Step: 41140, train/learning_rate: 1.0471204632267472e-06
Step: 41140, train/epoch: 9.790575981140137
Step: 41150, train/loss: 0.32089999318122864
Step: 41150, train/grad_norm: 16.94745635986328
Step: 41150, train/learning_rate: 1.0352213166697766e-06
Step: 41150, train/epoch: 9.79295539855957
Step: 41160, train/loss: 0.30640000104904175
Step: 41160, train/grad_norm: 5.831609725952148
Step: 41160, train/learning_rate: 1.0233222837996436e-06
Step: 41160, train/epoch: 9.79533576965332
Step: 41170, train/loss: 0.23229999840259552
Step: 41170, train/grad_norm: 17.413118362426758
Step: 41170, train/learning_rate: 1.011423137242673e-06
Step: 41170, train/epoch: 9.797715187072754
Step: 41180, train/loss: 0.29100000858306885
Step: 41180, train/grad_norm: 15.994829177856445
Step: 41180, train/learning_rate: 9.995239906857023e-07
Step: 41180, train/epoch: 9.800095558166504
Step: 41190, train/loss: 0.24120000004768372
Step: 41190, train/grad_norm: 18.66946792602539
Step: 41190, train/learning_rate: 9.876249578155694e-07
Step: 41190, train/epoch: 9.802474975585938
Step: 41200, train/loss: 0.18469999730587006
Step: 41200, train/grad_norm: 5.342141151428223
Step: 41200, train/learning_rate: 9.757258112585987e-07
Step: 41200, train/epoch: 9.804854393005371
Step: 41210, train/loss: 0.2485000044107437
Step: 41210, train/grad_norm: 5.505863189697266
Step: 41210, train/learning_rate: 9.638267783884658e-07
Step: 41210, train/epoch: 9.807234764099121
Step: 41220, train/loss: 0.4975999891757965
Step: 41220, train/grad_norm: 27.550498962402344
Step: 41220, train/learning_rate: 9.519276318314951e-07
Step: 41220, train/epoch: 9.809614181518555
Step: 41230, train/loss: 0.20900000631809235
Step: 41230, train/grad_norm: 7.297072410583496
Step: 41230, train/learning_rate: 9.400285421179433e-07
Step: 41230, train/epoch: 9.811994552612305
Step: 41240, train/loss: 0.24449999630451202
Step: 41240, train/grad_norm: 5.642361640930176
Step: 41240, train/learning_rate: 9.281294524043915e-07
Step: 41240, train/epoch: 9.814373970031738
Step: 41250, train/loss: 0.2896000146865845
Step: 41250, train/grad_norm: 25.36204719543457
Step: 41250, train/learning_rate: 9.162303626908397e-07
Step: 41250, train/epoch: 9.816754341125488
Step: 41260, train/loss: 0.23119999468326569
Step: 41260, train/grad_norm: 1.6017143726348877
Step: 41260, train/learning_rate: 9.043312729772879e-07
Step: 41260, train/epoch: 9.819133758544922
Step: 41270, train/loss: 0.3255999982357025
Step: 41270, train/grad_norm: 6.482863903045654
Step: 41270, train/learning_rate: 8.924321832637361e-07
Step: 41270, train/epoch: 9.821513175964355
Step: 41280, train/loss: 0.2784000039100647
Step: 41280, train/grad_norm: 5.396121025085449
Step: 41280, train/learning_rate: 8.805330935501843e-07
Step: 41280, train/epoch: 9.823893547058105
Step: 41290, train/loss: 0.3379000127315521
Step: 41290, train/grad_norm: 2.515291213989258
Step: 41290, train/learning_rate: 8.686340038366325e-07
Step: 41290, train/epoch: 9.826272964477539
Step: 41300, train/loss: 0.4390000104904175
Step: 41300, train/grad_norm: 14.522703170776367
Step: 41300, train/learning_rate: 8.567349141230807e-07
Step: 41300, train/epoch: 9.828653335571289
Step: 41310, train/loss: 0.490200012922287
Step: 41310, train/grad_norm: 3.6223134994506836
Step: 41310, train/learning_rate: 8.4483576756611e-07
Step: 41310, train/epoch: 9.831032752990723
Step: 41320, train/loss: 0.20180000364780426
Step: 41320, train/grad_norm: 6.804985523223877
Step: 41320, train/learning_rate: 8.329366778525582e-07
Step: 41320, train/epoch: 9.833413124084473
Step: 41330, train/loss: 0.33160001039505005
Step: 41330, train/grad_norm: 14.831116676330566
Step: 41330, train/learning_rate: 8.210375881390064e-07
Step: 41330, train/epoch: 9.835792541503906
Step: 41340, train/loss: 0.24580000340938568
Step: 41340, train/grad_norm: 6.260056495666504
Step: 41340, train/learning_rate: 8.091384984254546e-07
Step: 41340, train/epoch: 9.83817195892334
Step: 41350, train/loss: 0.1412000060081482
Step: 41350, train/grad_norm: 5.531459808349609
Step: 41350, train/learning_rate: 7.972394087119028e-07
Step: 41350, train/epoch: 9.84055233001709
Step: 41360, train/loss: 0.29100000858306885
Step: 41360, train/grad_norm: 8.279391288757324
Step: 41360, train/learning_rate: 7.85340318998351e-07
Step: 41360, train/epoch: 9.842931747436523
Step: 41370, train/loss: 0.2547000050544739
Step: 41370, train/grad_norm: 17.00247573852539
Step: 41370, train/learning_rate: 7.734412292847992e-07
Step: 41370, train/epoch: 9.845312118530273
Step: 41380, train/loss: 0.3785000145435333
Step: 41380, train/grad_norm: 26.474472045898438
Step: 41380, train/learning_rate: 7.615421395712474e-07
Step: 41380, train/epoch: 9.847691535949707
Step: 41390, train/loss: 0.25870001316070557
Step: 41390, train/grad_norm: 3.5482940673828125
Step: 41390, train/learning_rate: 7.496430498576956e-07
Step: 41390, train/epoch: 9.85007095336914
Step: 41400, train/loss: 0.2513999938964844
Step: 41400, train/grad_norm: 6.307486057281494
Step: 41400, train/learning_rate: 7.377439033007249e-07
Step: 41400, train/epoch: 9.85245132446289
Step: 41410, train/loss: 0.23080000281333923
Step: 41410, train/grad_norm: 7.142697334289551
Step: 41410, train/learning_rate: 7.258448135871731e-07
Step: 41410, train/epoch: 9.854830741882324
Step: 41420, train/loss: 0.43149998784065247
Step: 41420, train/grad_norm: 13.127347946166992
Step: 41420, train/learning_rate: 7.139457238736213e-07
Step: 41420, train/epoch: 9.857211112976074
Step: 41430, train/loss: 0.16850000619888306
Step: 41430, train/grad_norm: 0.8318533301353455
Step: 41430, train/learning_rate: 7.020466341600695e-07
Step: 41430, train/epoch: 9.859590530395508
Step: 41440, train/loss: 0.3993000090122223
Step: 41440, train/grad_norm: 26.226299285888672
Step: 41440, train/learning_rate: 6.901475444465177e-07
Step: 41440, train/epoch: 9.861970901489258
Step: 41450, train/loss: 0.25600001215934753
Step: 41450, train/grad_norm: 6.445243835449219
Step: 41450, train/learning_rate: 6.782484547329659e-07
Step: 41450, train/epoch: 9.864350318908691
Step: 41460, train/loss: 0.3790999948978424
Step: 41460, train/grad_norm: 6.887691974639893
Step: 41460, train/learning_rate: 6.663493650194141e-07
Step: 41460, train/epoch: 9.866729736328125
Step: 41470, train/loss: 0.2249000072479248
Step: 41470, train/grad_norm: 8.49881649017334
Step: 41470, train/learning_rate: 6.544502753058623e-07
Step: 41470, train/epoch: 9.869110107421875
Step: 41480, train/loss: 0.30480000376701355
Step: 41480, train/grad_norm: 9.725371360778809
Step: 41480, train/learning_rate: 6.425511855923105e-07
Step: 41480, train/epoch: 9.871489524841309
Step: 41490, train/loss: 0.1257999986410141
Step: 41490, train/grad_norm: 43.28179931640625
Step: 41490, train/learning_rate: 6.306520958787587e-07
Step: 41490, train/epoch: 9.873869895935059
Step: 41500, train/loss: 0.2676999866962433
Step: 41500, train/grad_norm: 2.8785555362701416
Step: 41500, train/learning_rate: 6.18752949321788e-07
Step: 41500, train/epoch: 9.876249313354492
Step: 41510, train/loss: 0.3880999982357025
Step: 41510, train/grad_norm: 13.811243057250977
Step: 41510, train/learning_rate: 6.068538596082362e-07
Step: 41510, train/epoch: 9.878629684448242
Step: 41520, train/loss: 0.4368000030517578
Step: 41520, train/grad_norm: 7.31951379776001
Step: 41520, train/learning_rate: 5.949547698946844e-07
Step: 41520, train/epoch: 9.881009101867676
Step: 41530, train/loss: 0.4505999982357025
Step: 41530, train/grad_norm: 6.451805591583252
Step: 41530, train/learning_rate: 5.830556801811326e-07
Step: 41530, train/epoch: 9.88338851928711
Step: 41540, train/loss: 0.188400000333786
Step: 41540, train/grad_norm: 2.892106771469116
Step: 41540, train/learning_rate: 5.711565904675808e-07
Step: 41540, train/epoch: 9.88576889038086
Step: 41550, train/loss: 0.3837999999523163
Step: 41550, train/grad_norm: 12.989293098449707
Step: 41550, train/learning_rate: 5.59257500754029e-07
Step: 41550, train/epoch: 9.888148307800293
Step: 41560, train/loss: 0.3709000051021576
Step: 41560, train/grad_norm: 16.65904998779297
Step: 41560, train/learning_rate: 5.473584110404772e-07
Step: 41560, train/epoch: 9.890528678894043
Step: 41570, train/loss: 0.31150001287460327
Step: 41570, train/grad_norm: 13.990259170532227
Step: 41570, train/learning_rate: 5.354593213269254e-07
Step: 41570, train/epoch: 9.892908096313477
Step: 41580, train/loss: 0.41290000081062317
Step: 41580, train/grad_norm: 7.748931407928467
Step: 41580, train/learning_rate: 5.235602316133736e-07
Step: 41580, train/epoch: 9.89528751373291
Step: 41590, train/loss: 0.3084999918937683
Step: 41590, train/grad_norm: 23.582984924316406
Step: 41590, train/learning_rate: 5.116611418998218e-07
Step: 41590, train/epoch: 9.89766788482666
Step: 41600, train/loss: 0.3330000042915344
Step: 41600, train/grad_norm: 23.338760375976562
Step: 41600, train/learning_rate: 4.997619953428512e-07
Step: 41600, train/epoch: 9.900047302246094
Step: 41610, train/loss: 0.2296999990940094
Step: 41610, train/grad_norm: 15.076539039611816
Step: 41610, train/learning_rate: 4.878629056292993e-07
Step: 41610, train/epoch: 9.902427673339844
Step: 41620, train/loss: 0.3695000112056732
Step: 41620, train/grad_norm: 13.196534156799316
Step: 41620, train/learning_rate: 4.7596381591574755e-07
Step: 41620, train/epoch: 9.904807090759277
Step: 41630, train/loss: 0.35499998927116394
Step: 41630, train/grad_norm: 25.353525161743164
Step: 41630, train/learning_rate: 4.6406472620219574e-07
Step: 41630, train/epoch: 9.907187461853027
Step: 41640, train/loss: 0.392300009727478
Step: 41640, train/grad_norm: 32.809879302978516
Step: 41640, train/learning_rate: 4.5216563648864394e-07
Step: 41640, train/epoch: 9.909566879272461
Step: 41650, train/loss: 0.39969998598098755
Step: 41650, train/grad_norm: 8.765541076660156
Step: 41650, train/learning_rate: 4.4026654677509214e-07
Step: 41650, train/epoch: 9.911946296691895
Step: 41660, train/loss: 0.35260000824928284
Step: 41660, train/grad_norm: 20.00676155090332
Step: 41660, train/learning_rate: 4.2836745706154034e-07
Step: 41660, train/epoch: 9.914326667785645
Step: 41670, train/loss: 0.41760000586509705
Step: 41670, train/grad_norm: 28.57282829284668
Step: 41670, train/learning_rate: 4.164683389262791e-07
Step: 41670, train/epoch: 9.916706085205078
Step: 41680, train/loss: 0.31949999928474426
Step: 41680, train/grad_norm: 15.267597198486328
Step: 41680, train/learning_rate: 4.045692492127273e-07
Step: 41680, train/epoch: 9.919086456298828
Step: 41690, train/loss: 0.32519999146461487
Step: 41690, train/grad_norm: 20.23931312561035
Step: 41690, train/learning_rate: 3.926701594991755e-07
Step: 41690, train/epoch: 9.921465873718262
Step: 41700, train/loss: 0.4041999876499176
Step: 41700, train/grad_norm: 16.657588958740234
Step: 41700, train/learning_rate: 3.807710697856237e-07
Step: 41700, train/epoch: 9.923846244812012
Step: 41710, train/loss: 0.2809999883174896
Step: 41710, train/grad_norm: 17.453439712524414
Step: 41710, train/learning_rate: 3.6887195165036246e-07
Step: 41710, train/epoch: 9.926225662231445
Step: 41720, train/loss: 0.3792000114917755
Step: 41720, train/grad_norm: 2.8032896518707275
Step: 41720, train/learning_rate: 3.5697286193681066e-07
Step: 41720, train/epoch: 9.928605079650879
Step: 41730, train/loss: 0.33239999413490295
Step: 41730, train/grad_norm: 9.102266311645508
Step: 41730, train/learning_rate: 3.4507377222325886e-07
Step: 41730, train/epoch: 9.930985450744629
Step: 41740, train/loss: 0.3287000060081482
Step: 41740, train/grad_norm: 19.5886287689209
Step: 41740, train/learning_rate: 3.3317468250970705e-07
Step: 41740, train/epoch: 9.933364868164062
Step: 41750, train/loss: 0.29510000348091125
Step: 41750, train/grad_norm: 6.790838241577148
Step: 41750, train/learning_rate: 3.2127559279615525e-07
Step: 41750, train/epoch: 9.935745239257812
Step: 41760, train/loss: 0.37689998745918274
Step: 41760, train/grad_norm: 18.081695556640625
Step: 41760, train/learning_rate: 3.09376474660894e-07
Step: 41760, train/epoch: 9.938124656677246
Step: 41770, train/loss: 0.3718999922275543
Step: 41770, train/grad_norm: 15.5103178024292
Step: 41770, train/learning_rate: 2.974773849473422e-07
Step: 41770, train/epoch: 9.94050407409668
Step: 41780, train/loss: 0.3612000048160553
Step: 41780, train/grad_norm: 5.015814304351807
Step: 41780, train/learning_rate: 2.855782952337904e-07
Step: 41780, train/epoch: 9.94288444519043
Step: 41790, train/loss: 0.3781000077724457
Step: 41790, train/grad_norm: 12.28203296661377
Step: 41790, train/learning_rate: 2.736792055202386e-07
Step: 41790, train/epoch: 9.945263862609863
Step: 41800, train/loss: 0.24979999661445618
Step: 41800, train/grad_norm: 5.726940631866455
Step: 41800, train/learning_rate: 2.617801158066868e-07
Step: 41800, train/epoch: 9.947644233703613
Step: 41810, train/loss: 0.41940000653266907
Step: 41810, train/grad_norm: 11.874967575073242
Step: 41810, train/learning_rate: 2.498809976714256e-07
Step: 41810, train/epoch: 9.950023651123047
Step: 41820, train/loss: 0.21850000321865082
Step: 41820, train/grad_norm: 4.674328327178955
Step: 41820, train/learning_rate: 2.3798190795787377e-07
Step: 41820, train/epoch: 9.952404022216797
Step: 41830, train/loss: 0.31679999828338623
Step: 41830, train/grad_norm: 2.0486674308776855
Step: 41830, train/learning_rate: 2.2608281824432197e-07
Step: 41830, train/epoch: 9.95478343963623
Step: 41840, train/loss: 0.31299999356269836
Step: 41840, train/grad_norm: 5.794093132019043
Step: 41840, train/learning_rate: 2.1418372853077017e-07
Step: 41840, train/epoch: 9.957162857055664
Step: 41850, train/loss: 0.46470001339912415
Step: 41850, train/grad_norm: 9.09522819519043
Step: 41850, train/learning_rate: 2.0228462460636365e-07
Step: 41850, train/epoch: 9.959543228149414
Step: 41860, train/loss: 0.29580000042915344
Step: 41860, train/grad_norm: 13.96679973602295
Step: 41860, train/learning_rate: 1.9038553489281185e-07
Step: 41860, train/epoch: 9.961922645568848
Step: 41870, train/loss: 0.23119999468326569
Step: 41870, train/grad_norm: 3.3463540077209473
Step: 41870, train/learning_rate: 1.7848643096840533e-07
Step: 41870, train/epoch: 9.964303016662598
Step: 41880, train/loss: 0.24779999256134033
Step: 41880, train/grad_norm: 1.5508477687835693
Step: 41880, train/learning_rate: 1.6658734125485353e-07
Step: 41880, train/epoch: 9.966682434082031
Step: 41890, train/loss: 0.2296999990940094
Step: 41890, train/grad_norm: 18.584518432617188
Step: 41890, train/learning_rate: 1.54688237330447e-07
Step: 41890, train/epoch: 9.969062805175781
Step: 41900, train/loss: 0.14030000567436218
Step: 41900, train/grad_norm: 11.956526756286621
Step: 41900, train/learning_rate: 1.427891476168952e-07
Step: 41900, train/epoch: 9.971442222595215
Step: 41910, train/loss: 0.27720001339912415
Step: 41910, train/grad_norm: 14.022648811340332
Step: 41910, train/learning_rate: 1.308900579033434e-07
Step: 41910, train/epoch: 9.973821640014648
Step: 41920, train/loss: 0.358599990606308
Step: 41920, train/grad_norm: 7.469069004058838
Step: 41920, train/learning_rate: 1.1899095397893689e-07
Step: 41920, train/epoch: 9.976202011108398
Step: 41930, train/loss: 0.364300012588501
Step: 41930, train/grad_norm: 14.75278091430664
Step: 41930, train/learning_rate: 1.0709186426538508e-07
Step: 41930, train/epoch: 9.978581428527832
Step: 41940, train/loss: 0.29600000381469727
Step: 41940, train/grad_norm: 8.043790817260742
Step: 41940, train/learning_rate: 9.519276744640592e-08
Step: 41940, train/epoch: 9.980961799621582
Step: 41950, train/loss: 0.301800012588501
Step: 41950, train/grad_norm: 19.27719497680664
Step: 41950, train/learning_rate: 8.329367062742676e-08
Step: 41950, train/epoch: 9.983341217041016
Step: 41960, train/loss: 0.319599986076355
Step: 41960, train/grad_norm: 13.209385871887207
Step: 41960, train/learning_rate: 7.13945738084476e-08
Step: 41960, train/epoch: 9.98572063446045
Step: 41970, train/loss: 0.2831999957561493
Step: 41970, train/grad_norm: 3.0262155532836914
Step: 41970, train/learning_rate: 5.949547698946844e-08
Step: 41970, train/epoch: 9.9881010055542
Step: 41980, train/loss: 0.499099999666214
Step: 41980, train/grad_norm: 7.788137435913086
Step: 41980, train/learning_rate: 4.759638372320296e-08
Step: 41980, train/epoch: 9.990480422973633
Step: 41990, train/loss: 0.35760000348091125
Step: 41990, train/grad_norm: 39.2772331237793
Step: 41990, train/learning_rate: 3.56972869042238e-08
Step: 41990, train/epoch: 9.992860794067383
Step: 42000, train/loss: 0.36390000581741333
Step: 42000, train/grad_norm: 8.377988815307617
Step: 42000, train/learning_rate: 2.379819186160148e-08
Step: 42000, train/epoch: 9.995240211486816
Step: 42010, train/loss: 0.412200003862381
Step: 42010, train/grad_norm: 18.106191635131836
Step: 42010, train/learning_rate: 1.189909593080074e-08
Step: 42010, train/epoch: 9.997620582580566
Step: 42020, train/loss: 0.302700012922287
Step: 42020, train/grad_norm: 9.620919227600098
Step: 42020, train/learning_rate: 0.0
Step: 42020, train/epoch: 10.0
Step: 42020, eval/loss: 0.9258869290351868
Step: 42020, eval/accuracy: 0.7092878222465515
Step: 42020, eval/f1: 0.708993673324585
Step: 42020, eval/runtime: 55.61280059814453
Step: 42020, eval/samples_per_second: 129.52099609375
Step: 42020, eval/steps_per_second: 16.201000213623047
Step: 42020, train/epoch: 10.0
Step: 42020, train/train_runtime: 9854.2685546875
Step: 42020, train/train_samples_per_second: 34.108001708984375
Step: 42020, train/train_steps_per_second: 4.263999938964844
Step: 42020, train/total_flos: 8.904406084458906e+16
Step: 42020, train/train_loss: 0.41203573346138
Step: 42020, train/epoch: 10.0
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=6ffd0164-0f41-4bbe-b905-7dda371dfd20">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [13]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.summary.summary_iterator</span> <span class="kn">import</span> <span class="n">summary_iterator</span>

<span class="n">logs_directory</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">'./'</span><span class="p">,</span> <span class="n">project_name</span><span class="p">,</span> <span class="s1">'logs'</span><span class="p">)</span>
<span class="n">file_pattern</span> <span class="o">=</span> <span class="s1">'events.out.tfevents.*'</span>

<span class="n">event_files</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">logs_directory</span><span class="p">,</span> <span class="n">file_pattern</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">extract_metrics</span><span class="p">(</span><span class="n">event_files</span><span class="p">):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">last_train_loss</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">for</span> <span class="n">event_file</span> <span class="ow">in</span> <span class="n">event_files</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">summary_iterator</span><span class="p">(</span><span class="n">event_file</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">e</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">v</span><span class="o">.</span><span class="n">HasField</span><span class="p">(</span><span class="s1">'simple_value'</span><span class="p">):</span>
                    <span class="n">step</span> <span class="o">=</span> <span class="n">e</span><span class="o">.</span><span class="n">step</span>
                    <span class="n">metric_name</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">'/'</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                    <span class="n">metric_value</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">simple_value</span>

                    <span class="n">formatted_value</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">metric_value</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">"</span>

                    <span class="k">if</span> <span class="s1">'train/loss'</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="p">:</span>
                        <span class="n">last_train_loss</span> <span class="o">=</span> <span class="n">formatted_value</span>

                    <span class="k">if</span> <span class="s1">'eval'</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="p">:</span>
                        <span class="n">entry</span> <span class="o">=</span> <span class="nb">next</span><span class="p">((</span><span class="n">item</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">data</span> <span class="k">if</span> <span class="n">item</span><span class="p">[</span><span class="s1">'Step'</span><span class="p">]</span> <span class="o">==</span> <span class="n">step</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="n">entry</span><span class="p">:</span>
                            <span class="n">entry</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'Step'</span><span class="p">:</span> <span class="n">step</span><span class="p">,</span> <span class="s1">'Train Loss'</span><span class="p">:</span> <span class="n">last_train_loss</span><span class="p">,</span> <span class="s1">'Eval Loss'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">'Accuracy'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">'F1'</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>
                            <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">entry</span><span class="p">)</span>
                        <span class="k">if</span> <span class="s1">'loss'</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="p">:</span>
                            <span class="n">entry</span><span class="p">[</span><span class="s1">'Eval Loss'</span><span class="p">]</span> <span class="o">=</span> <span class="n">formatted_value</span>
                        <span class="k">elif</span> <span class="s1">'accuracy'</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="p">:</span>
                            <span class="n">entry</span><span class="p">[</span><span class="s1">'Accuracy'</span><span class="p">]</span> <span class="o">=</span> <span class="n">formatted_value</span>
                        <span class="k">elif</span> <span class="s1">'f1'</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="p">:</span>
                            <span class="n">entry</span><span class="p">[</span><span class="s1">'F1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">formatted_value</span>

    <span class="k">return</span> <span class="n">data</span>

<span class="n">metrics_data</span> <span class="o">=</span> <span class="n">extract_metrics</span><span class="p">(</span><span class="n">event_files</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">metrics_data</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">'Step'</span><span class="p">)</span>

<span class="n">file_path</span> <span class="o">=</span> <span class="s2">"./images/"</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s2">"_Checkpoint_Data.csv"</span>
<span class="n">df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>    Step Train Loss Eval Loss  Accuracy        F1
0   4202   0.608400  0.823679  0.532278  0.512181
1   8404   0.386500  1.012033  0.581424  0.569434
2  12606   0.490900  1.214639  0.525337  0.497754
3  16808   0.445200  0.797666  0.654172  0.651540
4  21010   0.600400  1.012137  0.635707  0.630936
5  25212   0.428100  0.871624  0.681938  0.680768
6  29414   0.401900  0.629089  0.758712  0.758301
7  33616   0.287600  1.018618  0.664723  0.662412
8  37818   0.239300  0.857376  0.704429  0.704078
9  42020   0.302700  0.925887  0.709288  0.708994
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=37cc5f92-d47f-4356-8fad-d9b64b6f5361">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [14]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">fillna</span><span class="p">({</span>
    <span class="s1">'Eval Loss'</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="s1">'inf'</span><span class="p">),</span>
    <span class="s1">'Accuracy'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">'F1'</span><span class="p">:</span> <span class="mi">0</span>
<span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">df</span><span class="p">[</span><span class="s1">'Eval Loss'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Eval Loss'</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'Accuracy'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Accuracy'</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'F1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'F1'</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>

<span class="n">df</span><span class="p">[</span><span class="s1">'Eval Loss Rank'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Eval Loss'</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'Accuracy Rank'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Accuracy'</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'F1 Rank'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'F1'</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">df</span><span class="p">[</span><span class="s1">'Rank Sum'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Eval Loss Rank'</span><span class="p">]</span> <span class="o">+</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Accuracy Rank'</span><span class="p">]</span> <span class="o">+</span> <span class="n">df</span><span class="p">[</span><span class="s1">'F1 Rank'</span><span class="p">]</span>

<span class="n">best_checkpoint</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">'Rank Sum'</span><span class="p">]</span><span class="o">.</span><span class="n">idxmin</span><span class="p">()]</span>

<span class="n">checkpoint_folder_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"checkpoint-</span><span class="si">{</span><span class="n">best_checkpoint</span><span class="p">[</span><span class="s1">'Step'</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Best Checkpoint Step: </span><span class="si">{</span><span class="n">checkpoint_folder_name</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">best_checkpoint</span><span class="p">[[</span><span class="s1">'Step'</span><span class="p">,</span> <span class="s1">'Train Loss'</span><span class="p">,</span> <span class="s1">'Eval Loss'</span><span class="p">,</span> <span class="s1">'Accuracy'</span><span class="p">,</span> <span class="s1">'F1'</span><span class="p">,</span> <span class="s1">'Rank Sum'</span><span class="p">]])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Best Checkpoint Step: checkpoint-29414
Step             29414
Train Loss    0.401900
Eval Loss     0.629089
Accuracy      0.758712
F1            0.758301
Rank Sum           3.0
Name: 6, dtype: object
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=5c3c3999-8c32-4a25-aaca-2ec9036b69ed">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Run-TensorBoard">Run TensorBoard<a class="anchor-link" href="#Run-TensorBoard">¶</a></h3><p>tensorboard --logdir=~/kuk/Praxis/praxis-Llama-2-7b-hf-small-finetune/logs --host=0.0.0.0</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=f8f36022-dc73-492f-998c-43e5ed1a6f46">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="PAUSE-SCRIPT">PAUSE SCRIPT<a class="anchor-link" href="#PAUSE-SCRIPT">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=444ae65b-241c-47ef-bbc2-81f3a2ce50e9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [26]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># My flag to pause the script, set to True to pause</span>
<span class="n">pause_script</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=c8be3672-68e0-44f8-b8b1-31290665658d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [27]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">StopExecution</span><span class="p">(</span><span class="ne">Exception</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">_render_traceback_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Script Paused"</span><span class="p">)</span>
        <span class="k">pass</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=eb3b5ed2-5320-49c0-9e61-90d6f0942f7a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [28]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="n">pause_script</span><span class="p">:</span>
    <span class="k">raise</span> <span class="n">StopExecution</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=19be6e26-d888-4da0-b3f8-836d68ac2051">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Testing">Testing<a class="anchor-link" href="#Testing">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=b5111194-5eeb-4104-8df0-f977a6b716e0">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [29]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>

<span class="n">bnb_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span>
    <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="s2">"nf4"</span><span class="p">,</span>
    <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
<span class="p">)</span>

<span class="n">base_model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">checkpoint</span><span class="p">,</span>
    <span class="n">quantization_config</span><span class="o">=</span><span class="n">bnb_config</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span>
    <span class="n">num_labels</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">token</span><span class="o">=</span><span class="n">access_token</span><span class="p">,</span>
    <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">eval_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">checkpoint</span><span class="p">,</span>
    <span class="n">token</span><span class="o">=</span><span class="n">access_token</span><span class="p">,</span>
    <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=530fa55f-c8c4-485f-a093-09bf2175d586">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [30]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">PeftModel</span>
<span class="n">test_checkpoint_name</span> <span class="o">=</span> <span class="n">checkpoint_folder_name</span>
<span class="n">ft_model</span> <span class="o">=</span> <span class="n">PeftModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model</span><span class="p">,</span> <span class="n">project_name</span><span class="o">+</span><span class="s1">'/'</span><span class="o">+</span><span class="n">test_checkpoint_name</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=27354c4f-95cc-4d1f-ac64-c5e6b4a842ae">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [31]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># if torch.cuda.device_count() &gt; 1:</span>
<span class="c1">#     print("Using", torch.cuda.device_count(), "GPUs!")</span>
<span class="c1">#     ft_model = torch.nn.DataParallel(ft_model)</span>

<span class="c1"># ft_model.cuda()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[31]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>PeftModelForSequenceClassification(
  (base_model): LoraModel(
    (model): GPT2ForSequenceClassification(
      (transformer): GPT2Model(
        (wte): Embedding(50257, 768)
        (wpe): Embedding(1024, 768)
        (drop): Dropout(p=0.1, inplace=False)
        (h): ModuleList(
          (0-11): 12 x GPT2Block(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=768, out_features=2304, bias=True)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=768, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=2304, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (c_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=768, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=768, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=768, out_features=3072, bias=True)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=768, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=3072, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (c_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=3072, out_features=768, bias=True)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=3072, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=768, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (score): ModulesToSaveWrapper(
        (original_module): Linear(in_features=768, out_features=2, bias=False)
        (modules_to_save): ModuleDict(
          (default): Linear(in_features=768, out_features=2, bias=False)
        )
      )
    )
  )
)</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=eee9ab6f-a7d5-4dd5-9436-2f6f6e2bffaf">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [32]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tqdm.auto</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="n">processed_predictions</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">tokenized_test_ds</span><span class="p">):</span>

        <span class="n">eval_prompt</span> <span class="o">=</span> <span class="n">record</span><span class="p">[</span><span class="s1">'article'</span><span class="p">]</span>
        <span class="n">model_input</span> <span class="o">=</span> <span class="n">tokenize_fn</span><span class="p">({</span><span class="s1">'article'</span><span class="p">:</span> <span class="n">eval_prompt</span><span class="p">})</span>

        <span class="c1"># model_input = {k: v.to('cuda') for k, v in model_input.items()}</span>
        
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">ft_model</span><span class="p">(</span><span class="o">**</span><span class="n">model_input</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>
        
        <span class="n">prediction</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>  <span class="c1"># Use .item() to get a Python number</span>
        <span class="n">processed_predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">prediction</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>  0%|          | 0/7203 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=80f06a2c-5ded-4da0-8232-145383967c9d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Accuracy-and-F1">Accuracy and F1<a class="anchor-link" href="#Accuracy-and-F1">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=6e891bc5-55e0-4c43-a035-0edc37dcb6e3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [33]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">true_articles</span> <span class="o">=</span> <span class="n">tokenized_test_ds</span><span class="p">[</span><span class="s1">'label'</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=6bf33824-d7cc-4d22-af4d-7d44e569c2b9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [34]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span><span class="p">,</span> <span class="n">accuracy_score</span>

<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">true_articles</span><span class="p">,</span> <span class="n">processed_predictions</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"accuracy:"</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>accuracy: 0.7538525614327364
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=9b37dc24-3bc0-48a0-ace9-a360bae91169">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [35]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">true_articles</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=a29ebf68-632f-49d1-b903-407ff05b5569">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [36]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">processed_predictions</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0]
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=fb4d8350-a8d1-4853-a9f8-74ae9ea36bde">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [37]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">f1_score</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">true_articles</span><span class="p">,</span> <span class="n">processed_predictions</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">'macro'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"f1_score:"</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>f1_score: 0.7536391505386995
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=2dcaac14-2b83-4c24-b83a-f6d0e73a2d2a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Confusion-Matrix">Confusion Matrix<a class="anchor-link" href="#Confusion-Matrix">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=57e64afb-e3ee-4c79-8228-b2d79806229e">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [38]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">ConfusionMatrixDisplay</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">tokenized_test_ds</span><span class="p">[</span><span class="s1">'label'</span><span class="p">],</span> <span class="n">processed_predictions</span><span class="p">)</span>

<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'human'</span><span class="p">,</span> <span class="s1">'machine'</span><span class="p">]</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="o">=</span><span class="n">cm</span><span class="p">,</span> <span class="n">display_labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Blues</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAjYAAAGwCAYAAAC6ty9tAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABHMElEQVR4nO3de3zP9f//8ft7Yyfbe3PaKUtjDnOW+mnlWDKSFNUnVCr0KYdCDvmE5lD6kuiE4uNUJJ18HIqWQliKrJAWQyOblbHZ2Pn1+0PeeYd3e9t75vV2u3Z5XS5er9fz9Xw/Xmu2h8fz+Xy9LIZhGAIAAHADHuUdAAAAgKuQ2AAAALdBYgMAANwGiQ0AAHAbJDYAAMBtkNgAAAC3QWIDAADcRoXyDgBnFBcX68iRIwoICJDFYinvcAAATjIMQydPnlR4eLg8PMqmbpCbm6v8/HyX9OXl5SUfHx+X9HUlIbG5Qhw5ckQRERHlHQYAoJQOHTqkGjVquLzf3Nxc+QZUlQpPuaS/0NBQHThwwO2SGxKbK0RAQIAkyatBH1k8vco5GqBs3D24T3mHAJSZgtM5+vjpWNvPc1fLz8+XCk/Ju0EfqbS/J4rylfbTQuXn55PYoGycHX6yeHqR2MBtefn5l3cIQJkr8+kEFXxK/XvCsLjvFFsSGwAAzMQiqbTJkxtP5SSxAQDATCweZ7bS9uGm3PfOAADAVYeKDQAAZmKxuGAoyn3HokhsAAAwE4aiHHLfOwMAAFcdKjYAAJgJQ1EOkdgAAGAqLhiKcuMBG/e9MwAAcNWhYgMAgJkwFOUQiQ0AAGbCqiiH3PfOAADAVYeKDQAAZsJQlEMkNgAAmAlDUQ6R2AAAYCZUbBxy35QNAABcdajYAABgJgxFOURiAwCAmVgsLkhsGIoCAAC44lGxAQDATDwsZ7bS9uGmSGwAADAT5tg45L53BgAArjpUbAAAMBOeY+MQiQ0AAGbCUJRD7ntnAADgqkPFBgAAM2EoyiESGwAAzIShKIdIbAAAMBMqNg65b8oGAACuOlRsAAAwE4aiHCKxAQDATBiKcsh9UzYAAHDVoWIDAICpuGAoyo3rGiQ2AACYCUNRDrlvygYAAK46VGwAADATi8UFq6Lct2JDYgMAgJmw3Nsh970zAABw1SGxAQDATM5OHi7t5oTJkyfrxhtvVEBAgIKDg3X33XcrKSnJrk27du1ksVjstieeeMKuTUpKirp06SI/Pz8FBwdrxIgRKiwstGuzfv16XX/99fL29lZUVJQWLFjgVKwkNgAAmMnZoajSbk7YsGGDBg4cqG+++Ubx8fEqKChQx44dlZOTY9euf//+Sk1NtW1TpkyxnSsqKlKXLl2Un5+vLVu2aOHChVqwYIHGjRtna3PgwAF16dJF7du3V2JiooYMGaJ+/fpp7dq1JY6VOTYAAJhJOSz3XrNmjd3+ggULFBwcrO3bt6tNmza2435+fgoNDb1gH59//rl++uknffHFFwoJCVGzZs00ceJEjRo1SnFxcfLy8tLs2bMVGRmpadOmSZKio6O1adMmTZ8+XbGxsSWKlYoNAABXqaysLLstLy+vRNdlZmZKkqpUqWJ3fPHixapWrZoaNWqk0aNH69SpU7ZzCQkJaty4sUJCQmzHYmNjlZWVpd27d9vadOjQwa7P2NhYJSQklPieqNgAAGAmLlwVFRERYXf4+eefV1xcnMNLi4uLNWTIEN1yyy1q1KiR7XivXr1Us2ZNhYeH68cff9SoUaOUlJSkjz/+WJKUlpZml9RIsu2npaU5bJOVlaXTp0/L19f3H2+NxAYAADNx4VDUoUOHZLVabYe9vb3/8dKBAwdq165d2rRpk93xxx9/3Pbnxo0bKywsTLfddpuSk5NVu3bt0sXrBIaiAAC4SlmtVrvtnxKbQYMGadWqVfrqq69Uo0YNh21btmwpSdq3b58kKTQ0VEePHrVrc3b/7Lyci7WxWq0lqtZIJDYAAJjK35dUX+rmDMMwNGjQIH3yySf68ssvFRkZ+Y/XJCYmSpLCwsIkSTExMdq5c6fS09NtbeLj42W1WtWgQQNbm3Xr1tn1Ex8fr5iYmBLHSmIDAICJlEdiM3DgQL377rtasmSJAgIClJaWprS0NJ0+fVqSlJycrIkTJ2r79u06ePCgVqxYoYcfflht2rRRkyZNJEkdO3ZUgwYN9NBDD+mHH37Q2rVrNWbMGA0cONBWKXriiSe0f/9+jRw5Uj///LNmzpypZcuWaejQoSWOlcQGAAA4NGvWLGVmZqpdu3YKCwuzbe+//74kycvLS1988YU6duyo+vXr65lnnlGPHj20cuVKWx+enp5atWqVPD09FRMTowcffFAPP/ywJkyYYGsTGRmp1atXKz4+Xk2bNtW0adM0d+7cEi/1lpg8DACAuVj+3ErbhxMMw3B4PiIiQhs2bPjHfmrWrKlPP/3UYZt27dppx44dTsV3LhIbAABM5FKGki7QiWuCuQIxFAUAANwGFRsAAEyEio1jJDYAAJgIiY1jJDYAAJgIiY1jzLEBAABug4oNAABmUg7Lvc2ExAYAABNhKMoxhqIAAIDboGIDAICJWCxyQcXGNbFciUhsAAAwEYtcMBTlxpkNQ1EAAMBtULEBAMBEmDzsGIkNAABmwnJvhxiKAgAAboOKDQAAZuKCoSiDoSgAAHAlcMUcm9KvqrpykdgAAGAiJDaOMccGAAC4DSo2AACYCauiHCKxAQDARBiKcoyhKAAA4Dao2AAAYCJUbBwjsQEAwERIbBxjKAoAALgNKjYAAJgIFRvHSGwAADATlns7xFAUAABwG1RsAAAwEYaiHCOxAQDAREhsHCOxAQDAREhsHGOODQAAcBtUbAAAMBNWRTlEYgMAgIkwFOUYQ1EAAMBtULGBaQ19pKPubN9UdWqGKDevQN/+uF9xb/xP+35Nt7UJrhqgCU/do3Yt68vfz1v7fk3XtHlrtfKrRElSRFgVjejbSW1uqKvgqlal/ZGpZZ99p2nz1qqgsEiS5O1VQa+MfkDN6l+ruteFaO2mXXpwxJzyuGVcZepUr6SO9aqrZhU/BflW1MxNB5T4W5Zdm9AAb/VoGqa61f3l4SGlZuVp9uaDyjhVIEmq4GHRfc3CdeO1QargYdFPaSe1ePtvOplXKEmq5OWpvjddqxpBvqrk5amTeYX64bcsffJjqnILiy/7PeOfUbFxzDSJTbt27dSsWTPNmDGjvEPBFeLm66M094ON2vHTr6rg6amxA7rq49cH6ab7J+lUbr4kaVbcwwoM8FWvYW/pWGa27o29QfMnP6b2D0/Rzl8Oq+51IfLw8NDQyUu1//DvalA7XDP+01N+vt4a9+onkiRPDw/l5hborffXq+utzcrxjnG18fb00OETudp8IEMDWkWed756JS+NvC1Km/dnaMWuo8otKFJ4oI8Kigxbm/ubh6tJmFVvbflVpwuK1PP6a/Rkq+s0Zd0+SZJhSD/8lqX/7UzTybxCBft7q1eLa+R3Qw3995uUy3avKDmLXJDYuPEkG9MkNsDf3ffUTLv9AePf1b74l9QsOkJbdiRLkv5fk1oa/tJSff/Tr5KkafPWakDPW9UsOkI7fzmsdQl7tC5hj62PX387pqhrg/XYva1tic2p3Hw983/vS5JaNq2lQH/fy3F7gHalndSutJMXPX93k1DtSs3SRz+m2o79npNv+7NvRQ+1iqyiud+kKCk9W5K08NtDmnBHfUVW9dOBY6d0qqBIG5KP2a7JOFWg9fuOqWP96mVwR0DZY44N3IbV30eSdDzrlO3Ytz/u1z23t1CQ1U8Wi0Xdb28hb+8K2rR9r4N+fHU889RFzwNXAoukxmFWHT2Zp6fb1NLL3RpodIcoNbvGamtzbWU/VfD00J6jfyVHaSfzdCwnX7Wr+l2w30CfCmpeI1C/pOeU9S3gEp0diirt5q5MldgUFxdr5MiRqlKlikJDQxUXFydJOnjwoCwWixITE21tT5w4IYvFovXr10uS1q9fL4vForVr16p58+by9fXVrbfeqvT0dH322WeKjo6W1WpVr169dOrUX7/U1qxZo1atWikoKEhVq1bVnXfeqeTkZNv5s5/98ccfq3379vLz81PTpk2VkJBwOb4k+JPFYtHkYffqm8Rk7Un+61+vj46epwoVPHVg3RQd3TJD0//zgB4aMUcHDv9xwX4ia1TT4/9qqwWfbLpcoQOXJMCngnwqeqpTdLB2p2Vpxob92nE4S0/ccp3qVq8k6UySUlBUrNMF9nNlsnILZfWpaHes303X6o0ejTW1W0PlFhRp0XeHLtu9wEkWF21uylSJzcKFC1WpUiVt3bpVU6ZM0YQJExQfH+9UH3FxcXrjjTe0ZcsWHTp0SPfff79mzJihJUuWaPXq1fr888/1+uuv29rn5ORo2LBh2rZtm9atWycPDw/dc889Ki62/0Hx3HPPafjw4UpMTFTdunXVs2dPFRYWXjSOvLw8ZWVl2W24dC+PvF/RtcPU97n5dsefe+JOBQb4qtuA13Trw1P05uIvNX/yY2pQO/y8PsKqB+rD1wZq+Rc7tGj5lssVOnBJzv5eSvwtS1/88ocOn8jVmp/TtfNIltrUrup0f8sSj2jS57/oza8PqLq/t+5vfv7fEcAMTDXHpkmTJnr++eclSXXq1NEbb7yhdevWqU6dOiXuY9KkSbrlllskSX379tXo0aOVnJysWrVqSZLuvfdeffXVVxo1apQkqUePHnbXz5s3T9WrV9dPP/2kRo0a2Y4PHz5cXbp0kSSNHz9eDRs21L59+1S/fv0LxjF58mSNHz++xHHj4qaMuE+xrRvpjsdn6Ej6Cdvx6645U32J+dck/bw/TZK0a+9vimleW/3ua6NhLy21tQ2tFqgVs57Wtz/u15AX37vctwA4LTu/SEXFhlKzcu2Op2blKerPik1mbqEqenrIt6KHXdXG6lNBWbkFdtdl5RYqK7dQaSfzlJNfpJG3RWn17qPKzL34P9BQPlgV5ZipKjZNmjSx2w8LC1N6evpFWv9zHyEhIfLz87MlNWePndvn3r171bNnT9WqVUtWq1XXXXedJCklxX61wLn9hoWFSZLD2EaPHq3MzEzbdugQZd9LMWXEferSrqnuevI1pRw5ZnfOz8dLklRcbNgdLyoyZPH46y91WPVArZz9tH74OUUDJ7wrw7BvD1yJiooNHcw4pdAAb7vjIQHeOvbnBOKU46dUWFSs6JAAu/NVK3kp+djF55Gd/Z1XwcN9f/mZGXNsHDNVxaZiRfsxYYvFouLiYnl4nMnPzv2FVFBg/6+RC/VhsVgu2udZXbt2Vc2aNTVnzhyFh4eruLhYjRo1Un5+vt11f+9X0nnDVefy9vaWt7f3Rc/jn7086n7dG3uDeg1/W9mnchVc9cwP76zsXOXmFeiXg2lKTknX9NE9NfbVT5SRmaMu7Zqofct6emDobEl/JTWH0jI09tVPVK2yv63/9GN/TbisFxmqihU9VdlaSf5+3mpU9xpJ0q5ffruMd4yrjXcFD1X397LtV6vkpRpBPjqVX6SMUwVa+3O6Ho+pqV9+z1FSerYahQaoSbhV0746Mw/wdEGxNh3I0H3NwpWTX6jTBcXqef01Sv4jRwf+TGwahQXI6lNBBzNOK+/P5eI9moZr3+85Onbqwj9HUb4slr+Sz9L04a5MldhcTPXqZ5Ylpqamqnnz5pJkN5H4Uh07dkxJSUmaM2eOWrduLUnatIlJpVeKvve2kSStfmuI3fEB49/Re6u2qrCoWPcPmaXnB3XTe6/8W5X8vHXg0O8aEPeO4rf8JElq17K+al8brNrXBuunT1+w66fyjYNsf14240ldG/7XvIWvF48+rw3gajUr+2r4rVG2/fubn0motxzI0IJvDynxtywt3v6bOkUH64Hm1+joyTMP59v3x18rmpbtOCLDkJ64+TpV8LRod9pJLdn+V0JeUFSs1rWq6v5mPqrgYdHx0wX6/nCm1uw5evluFHAht0hsfH19ddNNN+mll15SZGSk0tPTNWbMmFL3W7lyZVWtWlVvv/22wsLClJKSomeffdYFEcMVSpJU7D/0u/qMmnvR8++t2qr3Vm39x36adnveqdgAV/jl9xw9/v4PDttsPpChzQcyLnq+sNjQe9//pve+v3B1MSk9R//358P6YA5nKjalnWPjomCuQKaaY+PIvHnzVFhYqBYtWmjIkCGaNGlSqfv08PDQ0qVLtX37djVq1EhDhw7V1KlTXRAtAACXyPLXcNSlbu683NtiMFPyipCVlaXAwEB5N+4vi6fXP18AmNB9w/uXdwhAmck/la33H2+lzMxMWa3Wf77ASWd/T9R66kN5elcqVV9FeTna/9q9ZRZreXKLoSgAAK4WLPd2jMQGAAATYVWUY24zxwYAAICKDQAAJuLhYZFHKR+eaLjxwxdJbAAAMBGGohxjKAoAALgNKjYAAJgIq6IcI7EBAMBEGIpyjMQGAAAToWLjGHNsAACA26BiAwCAiVCxcYyKDQAAJlLaF2BeyhydyZMn68Ybb1RAQICCg4N19913Kykpya5Nbm6uBg4cqKpVq8rf3189evTQ0aNH7dqkpKSoS5cu8vPzU3BwsEaMGKHCwkK7NuvXr9f1118vb29vRUVFacGCBU7FSmIDAAAc2rBhgwYOHKhvvvlG8fHxKigoUMeOHZWTk2NrM3ToUK1cuVIffPCBNmzYoCNHjqh79+6280VFRerSpYvy8/O1ZcsWLVy4UAsWLNC4ceNsbQ4cOKAuXbqoffv2SkxM1JAhQ9SvXz+tXbu2xLHydu8rBG/3xtWAt3vDnV2ut3s3fnaFPH1K+Xbv3BztfOmuS471999/V3BwsDZs2KA2bdooMzNT1atX15IlS3TvvfdKkn7++WdFR0crISFBN910kz777DPdeeedOnLkiEJCQiRJs2fP1qhRo/T777/Ly8tLo0aN0urVq7Vr1y7bZz3wwAM6ceKE1qxZU6LYqNgAAGAirhyKysrKstvy8vJKFENmZqYkqUqVKpKk7du3q6CgQB06dLC1qV+/vq699lolJCRIkhISEtS4cWNbUiNJsbGxysrK0u7du21tzu3jbJuzfZQEiQ0AAFepiIgIBQYG2rbJkyf/4zXFxcUaMmSIbrnlFjVq1EiSlJaWJi8vLwUFBdm1DQkJUVpamq3NuUnN2fNnzzlqk5WVpdOnT5fonlgVBQCAibhyVdShQ4fshqK8vb3/8dqBAwdq165d2rRpU6liKCskNgAAmIgrnzxstVqdmmMzaNAgrVq1Shs3blSNGjVsx0NDQ5Wfn68TJ07YVW2OHj2q0NBQW5tvv/3Wrr+zq6bObfP3lVRHjx6V1WqVr69viWJkKAoAADhkGIYGDRqkTz75RF9++aUiIyPtzrdo0UIVK1bUunXrbMeSkpKUkpKimJgYSVJMTIx27typ9PR0W5v4+HhZrVY1aNDA1ubcPs62OdtHSVCxAQDARMrjAX0DBw7UkiVL9L///U8BAQG2OTGBgYHy9fVVYGCg+vbtq2HDhqlKlSqyWq0aPHiwYmJidNNNN0mSOnbsqAYNGuihhx7SlClTlJaWpjFjxmjgwIG2IbAnnnhCb7zxhkaOHKnHHntMX375pZYtW6bVq1eXOFYSGwAATKQ8XoI5a9YsSVK7du3sjs+fP1+PPPKIJGn69Ony8PBQjx49lJeXp9jYWM2cOdPW1tPTU6tWrdKTTz6pmJgYVapUSX369NGECRNsbSIjI7V69WoNHTpUr776qmrUqKG5c+cqNja2xLGS2AAAYCLlUbEpySPvfHx89Oabb+rNN9+8aJuaNWvq008/ddhPu3bttGPHDqfiOxdzbAAAgNugYgMAgJm4YChK7vsOTBIbAADMhLd7O8ZQFAAAcBtUbAAAMJHyWBVlJiQ2AACYCENRjjEUBQAA3AYVGwAATIShKMdIbAAAMBGGohxjKAoAALgNKjYAAJgIFRvHSGwAADAR5tg4RmIDAICJULFxjDk2AADAbVCxAQDARBiKcozEBgAAE2EoyjGGogAAgNugYgMAgIlY5IKhKJdEcmUisQEAwEQ8LBZ5lDKzKe31VzKGogAAgNugYgMAgImwKsoxEhsAAEyEVVGOkdgAAGAiHpYzW2n7cFfMsQEAAG6Dig0AAGZiccFQkhtXbEhsAAAwESYPO8ZQFAAAcBtUbAAAMBHLn/+Vtg93RWIDAICJsCrKMYaiAACA26BiAwCAifCAPsdIbAAAMBFWRTlWosRmxYoVJe7wrrvuuuRgAAAASqNEic3dd99dos4sFouKiopKEw8AAHDAw2KRRylLLqW9/kpWosSmuLi4rOMAAAAlwFCUY6WaY5ObmysfHx9XxQIAAP4Bk4cdc3q5d1FRkSZOnKhrrrlG/v7+2r9/vyRp7Nix+u9//+vyAAEAAErK6cTmhRde0IIFCzRlyhR5eXnZjjdq1Ehz5851aXAAAMDe2aGo0m7uyunEZtGiRXr77bfVu3dveXp62o43bdpUP//8s0uDAwAA9s5OHi7t5q6cTmx+++03RUVFnXe8uLhYBQUFLgkKAADgUjid2DRo0EBff/31ecc//PBDNW/e3CVBAQCAC7O4aHNXTq+KGjdunPr06aPffvtNxcXF+vjjj5WUlKRFixZp1apVZREjAAD4E6uiHHO6YtOtWzetXLlSX3zxhSpVqqRx48Zpz549WrlypW6//fayiBEAAKBELuk5Nq1bt1Z8fLyrYwEAAP/Aw3JmK20f7uqSH9C3bds27dmzR9KZeTctWrRwWVAAAODCGIpyzOnE5vDhw+rZs6c2b96soKAgSdKJEyd08803a+nSpapRo4arYwQAACgRp+fY9OvXTwUFBdqzZ48yMjKUkZGhPXv2qLi4WP369SuLGAEAwDl4ON/FOV2x2bBhg7Zs2aJ69erZjtWrV0+vv/66Wrdu7dLgAACAPYaiHHM6sYmIiLjgg/iKiooUHh7ukqAAAMCFMXnYMaeHoqZOnarBgwdr27ZttmPbtm3T008/rZdfftmlwQEAADijRBWbypUr25WtcnJy1LJlS1WocObywsJCVahQQY899pjuvvvuMgkUAAAwFPVPSpTYzJgxo4zDAAAAJeGKVyK4b1pTwsSmT58+ZR0HAABAqV3yA/okKTc3V/n5+XbHrFZrqQICAAAX52GxyKOUQ0mlvf5K5vTk4ZycHA0aNEjBwcGqVKmSKleubLcBAICyU9pn2Lj7s2ycTmxGjhypL7/8UrNmzZK3t7fmzp2r8ePHKzw8XIsWLSqLGAEAAErE6aGolStXatGiRWrXrp0effRRtW7dWlFRUapZs6YWL16s3r17l0WcAABArIr6J05XbDIyMlSrVi1JZ+bTZGRkSJJatWqljRs3ujY6AABgh6Eox5xObGrVqqUDBw5IkurXr69ly5ZJOlPJOftSTAAAgPLgdGLz6KOP6ocffpAkPfvss3rzzTfl4+OjoUOHasSIES4PEAAA/OXsqqjSbs7YuHGjunbtqvDwcFksFi1fvtzu/COPPGIbIju7derUya5NRkaGevfuLavVqqCgIPXt21fZ2dl2bX788Ue1bt1aPj4+ioiI0JQpU5z++jg9x2bo0KG2P3fo0EE///yztm/frqioKDVp0sTpAAAAQMm5YijJ2etzcnLUtGlTPfbYY+revfsF23Tq1Enz58+37Xt7e9ud7927t1JTUxUfH6+CggI9+uijevzxx7VkyRJJUlZWljp27KgOHTpo9uzZ2rlzpx577DEFBQXp8ccfL3GspXqOjSTVrFlTNWvWLG03AACgBFw5eTgrK8vuuLe393kJiSR17txZnTt3dtint7e3QkNDL3huz549WrNmjb777jvdcMMNkqTXX39dd9xxh15++WWFh4dr8eLFys/P17x58+Tl5aWGDRsqMTFRr7zyiusTm9dee63EHT711FMlbgsAAMpPRESE3f7zzz+vuLi4S+pr/fr1Cg4OVuXKlXXrrbdq0qRJqlq1qiQpISFBQUFBtqRGOjPq4+Hhoa1bt+qee+5RQkKC2rRpIy8vL1ub2NhY/d///Z+OHz9e4mfllSixmT59eok6s1gsJDallLL+ZZ7eDLdVubPz4+WAWRiFuZflczx0CRNkL9CHJB06dMjud86FqjUl0alTJ3Xv3l2RkZFKTk7Wf/7zH3Xu3FkJCQny9PRUWlqagoOD7a6pUKGCqlSporS0NElSWlqaIiMj7dqEhITYzrk0sTm7CgoAAJQvVw5FWa1Wl/xj+oEHHrD9uXHjxmrSpIlq166t9evX67bbbit1/84obdIHAABgp1atWqpWrZr27dsnSQoNDVV6erpdm8LCQmVkZNjm5YSGhuro0aN2bc7uX2zuzoWQ2AAAYCIWi+RRyq2sH9B3+PBhHTt2TGFhYZKkmJgYnThxQtu3b7e1+fLLL1VcXKyWLVva2mzcuFEFBQW2NvHx8apXr55T76IksQEAwERKm9Sc3ZyRnZ2txMREJSYmSjozRSUxMVEpKSnKzs7WiBEj9M033+jgwYNat26dunXrpqioKMXGxkqSoqOj1alTJ/Xv31/ffvutNm/erEGDBumBBx5QeHi4JKlXr17y8vJS3759tXv3br3//vt69dVXNWzYMOe+Ps7dGgAAuNps27ZNzZs3V/PmzSVJw4YNU/PmzTVu3Dh5enrqxx9/1F133aW6deuqb9++atGihb7++mu7yciLFy9W/fr1ddttt+mOO+5Qq1at9Pbbb9vOBwYG6vPPP9eBAwfUokULPfPMMxo3bpxTS70lFzzHBgAAXD7l8RLMdu3ayTCMi55fu3btP/ZRpUoV28P4LqZJkyb6+uuvnYrt7y6pYvP111/rwQcfVExMjH777TdJ0jvvvKNNmzaVKhgAAOBYeQxFmYnTic1HH32k2NhY+fr6aseOHcrLy5MkZWZm6sUXX3R5gAAAACXldGIzadIkzZ49W3PmzFHFihVtx2+55RZ9//33Lg0OAADYO/uuqNJu7srpOTZJSUlq06bNeccDAwN14sQJV8QEAAAu4lLezn2hPtyV0xWb0NBQ2wN3zrVp0ybVqlXLJUEBAIAL83DR5q6cvrf+/fvr6aef1tatW2WxWHTkyBEtXrxYw4cP15NPPlkWMQIAAJSI00NRzz77rIqLi3Xbbbfp1KlTatOmjby9vTV8+HANHjy4LGIEAAB/csUcGTceiXI+sbFYLHruuec0YsQI7du3T9nZ2WrQoIH8/f3LIj4AAHAOD7lgjo3cN7O55Af0eXl5qUGDBq6MBQAAoFScTmzat2/v8ImFX375ZakCAgAAF8dQlGNOJzbNmjWz2y8oKFBiYqJ27dqlPn36uCouAABwAa54crA7P3nY6cRm+vTpFzweFxen7OzsUgcEAABwqVy2lP3BBx/UvHnzXNUdAAC4AIvlr4f0XerGUFQJJCQkyMfHx1XdAQCAC2COjWNOJzbdu3e32zcMQ6mpqdq2bZvGjh3rssAAAACc5XRiExgYaLfv4eGhevXqacKECerYsaPLAgMAAOdj8rBjTiU2RUVFevTRR9W4cWNVrly5rGICAAAXYfnzv9L24a6cmjzs6empjh078hZvAADKydmKTWk3d+X0qqhGjRpp//79ZRELAABAqTid2EyaNEnDhw/XqlWrlJqaqqysLLsNAACUHSo2jpV4js2ECRP0zDPP6I477pAk3XXXXXavVjAMQxaLRUVFRa6PEgAASDrzMmpHrzYqaR/uqsSJzfjx4/XEE0/oq6++Kst4AAAALlmJExvDMCRJbdu2LbNgAACAYyz3dsyp5d7uXLoCAMAMePKwY04lNnXr1v3H5CYjI6NUAQEAAFwqpxKb8ePHn/fkYQAAcPmcfZFlaftwV04lNg888ICCg4PLKhYAAPAPmGPjWImfY8P8GgAAcKVzelUUAAAoRy6YPOzGr4oqeWJTXFxclnEAAIAS8JBFHqXMTEp7/ZXMqTk2AACgfLHc2zGn3xUFAABwpaJiAwCAibAqyjESGwAATITn2DjGUBQAAHAbVGwAADARJg87RmIDAICJeMgFQ1FuvNyboSgAAOA2qNgAAGAiDEU5RmIDAICJeKj0wy3uPFzjzvcGAACuMlRsAAAwEYvFIkspx5JKe/2VjMQGAAATsaj0L+d237SGxAYAAFPhycOOMccGAAC4DSo2AACYjPvWW0qPxAYAABPhOTaOMRQFAADcBhUbAABMhOXejpHYAABgIjx52DF3vjcAAHCVoWIDAICJMBTlGIkNAAAmwpOHHWMoCgAAuA0qNgAAmAhDUY6R2AAAYCKsinKMxAYAABOhYuOYOydtAADgKkNiAwCAiVhctDlj48aN6tq1q8LDw2WxWLR8+XK784ZhaNy4cQoLC5Ovr686dOigvXv32rXJyMhQ7969ZbVaFRQUpL59+yo7O9uuzY8//qjWrVvLx8dHERERmjJlipORktgAAGAqZ1+CWdrNGTk5OWratKnefPPNC56fMmWKXnvtNc2ePVtbt25VpUqVFBsbq9zcXFub3r17a/fu3YqPj9eqVau0ceNGPf7447bzWVlZ6tixo2rWrKnt27dr6tSpiouL09tvv+1UrMyxAQAADnXu3FmdO3e+4DnDMDRjxgyNGTNG3bp1kyQtWrRIISEhWr58uR544AHt2bNHa9as0XfffacbbrhBkvT666/rjjvu0Msvv6zw8HAtXrxY+fn5mjdvnry8vNSwYUMlJibqlVdesUuA/gkVGwAATMRDFpds0pkqyblbXl6e0/EcOHBAaWlp6tChg+1YYGCgWrZsqYSEBElSQkKCgoKCbEmNJHXo0EEeHh7aunWrrU2bNm3k5eVlaxMbG6ukpCQdP37cia8PAAAwDVcORUVERCgwMNC2TZ482el40tLSJEkhISF2x0NCQmzn0tLSFBwcbHe+QoUKqlKlil2bC/Vx7meUBENRAABcpQ4dOiSr1Wrb9/b2LsdoXIOKDQAAJmJx0X+SZLVa7bZLSWxCQ0MlSUePHrU7fvToUdu50NBQpaen250vLCxURkaGXZsL9XHuZ5QEiQ0AACZSHquiHImMjFRoaKjWrVtnO5aVlaWtW7cqJiZGkhQTE6MTJ05o+/bttjZffvmliouL1bJlS1ubjRs3qqCgwNYmPj5e9erVU+XKlUscD4kNAABwKDs7W4mJiUpMTJR0ZsJwYmKiUlJSZLFYNGTIEE2aNEkrVqzQzp079fDDDys8PFx33323JCk6OlqdOnVS//799e2332rz5s0aNGiQHnjgAYWHh0uSevXqJS8vL/Xt21e7d+/W+++/r1dffVXDhg1zKlbm2AAAYCKWc1Y1laYPZ2zbtk3t27e37Z9NNvr06aMFCxZo5MiRysnJ0eOPP64TJ06oVatWWrNmjXx8fGzXLF68WIMGDdJtt90mDw8P9ejRQ6+99prtfGBgoD7//HMNHDhQLVq0ULVq1TRu3DinlnpLksUwDMOpK1AmsrKyFBgYqKPHMu0mcgHupHJn558iCpiFUZirvPXPKzOzbH6On/098dHWZFXyDyhVXznZJ9WjZe0yi7U8UbEBAMBEXDFHxo3fgckcGwAA4D6o2AAAYCLnLtcuTR/uisQGAAAT8bCc2Urbh7tiKAoAALgNKjYAAJgIQ1GOkdgAAGAirIpyjKEoAADgNqjYAABgIhaVfijJjQs2JDYAAJgJq6IcYygKAAC4DSo2cCubv9+n19/5Qj/8nKK0P7L07tT+6tKuqe185RsHXfC68U/drace6iBJOp6Zo5FTP9DaTbtksVh0163NNPmZe+Xv531Z7gGQpKH3t9Sdt9RVnRpVlZtfoG9/OqK4eRu077cMu3Y31g/XmD6t1aJ+mIqKDe1KTlePMR8oN79QkhTk76MpAzootmVtGcWGVmz+RaNnr1NOboGtj7tb19Owf92k2tdU0bHMU5qzcode/+jby3q/KDlWRTl21VVsLBaLli9fftHz69evl8Vi0YkTJy5bTHCdU6fz1KjuNZo68l8XPP/zZy/abW+M7X0meWnfzNam/9iF+nl/qj5+Y5CWTn9CW3bs05AXl1ymOwDOuLlxhOau3KGOQ99R9/8sU8UKHvr4hfvk513R1ubG+uH6cNJ9+ur7g+rw9Du67al3NGfl9yo+593Gc0beqfrXVlX3/yzTA3Ef6eZGEZrxVKztfIcbIvX2yDs1/9MfdPOT8zT8zXg9ec8N6t+1+WW9X5Tc2VVRpd3cFRWbv7n55puVmpqqwMDA8g4Fl+D2Wxrq9lsaXvR8SDX7t9h+unGnWreoo+tqVJMkJR1I07qEn/TlwhFq3qCmJOn/ht+n+4fM0sSn71FY9aAyix04131jP7TbH/DKp9q3dLCa1QnRll2HJUkv/PtWvfW/7ZrxwVZbu3MrOnUjqqjDjbXU/qlFStybJkkaNesLLZtwr8bOXa+0jGz969aGWp2wV/M/TZQk/ZqWqenLvtHT97XUnJU7yvgucSksKv3kXzfOa66+is0/8fLyUmhoqCzunM5CkpR+LEufb9qlB7vF2I59t/OAAgN8bUmNJLX7f/Xk4WHR9l2/lkeYgCTJ+udQ6PGTuZKkaoF+urF+uH7PPKW103oraclArZrSUzc1vMZ2zY3R1+jEyVxbUiNJ63ccVLFhqEX9MEmSV0VP5eUX2X1Wbl6hrqluVUSw/T8EADMo18SmXbt2Gjx4sIYMGaLKlSsrJCREc+bMUU5Ojh599FEFBAQoKipKn332mSSpqKhIffv2VWRkpHx9fVWvXj29+uqr5/U7b948NWzYUN7e3goLC9OgQfbzKv744w/dc8898vPzU506dbRixQrbub8PRS1YsEBBQUFau3atoqOj5e/vr06dOik1NdWuz7lz5yo6Olo+Pj6qX7++Zs6c6fDe8/LylJWVZbfh8npv9Vb5V/JR13OGoY4ey1L1ygF27SpU8FRlq5+OHuP/EcqHxSJN/vdt+mb3Ye359Q9J0nVhZ6rKz/a+RQvX/KB7x36gH/Yd1fLJ/1Kt8MqSpJDKlfR75im7voqKDR0/eVohlStJkr78/qDuvKWO2jS7VhaLVPuayhrY/UZJUmgV/8t1i3CChyzysJRyc+OaTblXbBYuXKhq1arp22+/1eDBg/Xkk0/qvvvu080336zvv/9eHTt21EMPPaRTp06puLhYNWrU0AcffKCffvpJ48aN03/+8x8tW7bM1t+sWbM0cOBAPf7449q5c6dWrFihqKgou88cP3687r//fv3444+644471Lt3b2VkZPw9NJtTp07p5Zdf1jvvvKONGzcqJSVFw4cPt51fvHixxo0bpxdeeEF79uzRiy++qLFjx2rhwoUX7XPy5MkKDAy0bREREaX4KuJSLF7xje7rdIN8zpmzAFyJXh54u6Kvq6a+L/31jzCPP6vKCz5N1JL4XdqZnK7n3v5S+w5n6MGOjUvc98LPftDclTu0NK6H0lcOV/z0B/Xxhj2SZDdXB1cOi4s2d1XuiU3Tpk01ZswY1alTR6NHj5aPj4+qVaum/v37q06dOho3bpyOHTumH3/8URUrVtT48eN1ww03KDIyUr1799ajjz5ql9hMmjRJzzzzjJ5++mnVrVtXN954o4YMGWL3mY888oh69uypqKgovfjii8rOzta33158BUBBQYFmz56tG264Qddff70GDRqkdevW2c4///zzmjZtmrp3767IyEh1795dQ4cO1VtvvXXRPkePHq3MzEzbdujQoUv/IsJpW3bs095fj+qhbjfbHQ+patXvx0/aHSssLNLxrFMKqUpZHpfflCc7KPb/1VbXUUt15I9s2/G0jBxJUlLKMbv2SSkZqvHnENLR4zmqHuhnd97Tw6LKAb46ejzHdixu3gbV6D5DTfrMVr1eb+r7X85UpA+mnSiLWwLKVLlPHm7SpIntz56enqpataoaN/7rXxshISGSpPT0dEnSm2++qXnz5iklJUWnT59Wfn6+mjVrZmtz5MgR3XbbbSX+zEqVKslqtdr6vxA/Pz/Vrl3bth8WFmZrn5OTo+TkZPXt21f9+/e3tSksLHQ4Adnb21ve3iwfLi/v/i9BzaIj1LhuDbvjNzaOVObJ00rck6Jm0ddKkjZu+0XFxYZaNKp5oa6AMjPlyQ7qcnMddR21VClHM+3OpRzN1JE/TiqqRhW741E1KuuL7/ZLkr7b85uCAnzUNCpEP+w7Kklq06ymPCwWbf/Zfji9uNhQ6rEziVOPttH69qffdCzzdFndGkqD2cMOlXtiU7Gi/TCAxWKxO3Z2Em9xcbGWLl2q4cOHa9q0aYqJiVFAQICmTp2qrVvPrAjw9fW95M8sLi52qr3xZ4k2O/vMD4I5c+aoZcuWdu08PT1LFA9cJ/tUng4c+t22/+uRY9qZdFhBgX6KCD3zCyAr+7T+t26HJg6557zr60WG6raYBnr6hSV6ZfQDKigs0sipy9S94/WsiMJl9fLA23Vvu2j1mvCJsk/nK/jPOTFZOXm2Z9S8/tG3Gv1gK+06kK6dyenq2aGR6tSooj4v/E+S9MuhDH3x3X69+nQnDXt9rSpW8NSUJzvo4w17lJZx5mdXFauvurWqp00/psjbq4J6395I3VrX050j3yufG8c/4jk2jpV7YuOMzZs36+abb9aAAQNsx5KTk21/DggI0HXXXad169apffv2lyWmkJAQhYeHa//+/erdu/dl+UxcXOKeX9X1idds+89N/1iS1LNLS82Me0iS9PHn22UYhnrE3nDBPuZM7KMRU5fp7gGv2x7Q99Lw+8o+eOAcfe888xyZ1VN62h0fMO1TvffFLknS7OXb5VOxgl58/FYFBfho9/7f1f25ZTqYesLWvv+UVZo6oIOWT35AhmFoxeYkPTtrnV2fD3RoqAn92slikb7bc0RdRy3V97+kCTAjUyU2derU0aJFi7R27VpFRkbqnXfe0XfffafIyEhbm7i4OD3xxBMKDg5W586ddfLkSW3evFmDBw8us7jGjx+vp556SoGBgerUqZPy8vK0bds2HT9+XMOGDSuzz8X5WrWoq+PfveGwzSPdW+mR7q0uer5yYCXNnfSoq0MDnFK585QStZvxwVa759j83YnsXPWfsuqi5zOyTit22GKn40M5csUD9ty3YGOuxObf//63duzYoX/961+yWCzq2bOnBgwYYFsOLkl9+vRRbm6upk+fruHDh6tatWq69957yzSufv36yc/PT1OnTtWIESNUqVIlNW7c+LxJywAAlBZTbByzGAbr+a4EWVlZCgwM1NFjmbJaWX0D91TSKgRgRkZhrvLWP6/MzLL5OX7298SXiSnyDyhd/9kns3Rrs2vLLNbyZKqKDQAAVz1KNg6R2AAAYCKsinKMxAYAABNxxdu53fl1iOX+5GEAAABXoWIDAICJMMXGMRIbAADMhMzGIYaiAACA26BiAwCAibAqyjESGwAATIRVUY4xFAUAANwGFRsAAEyEucOOkdgAAGAmZDYOMRQFAADcBhUbAABMhFVRjpHYAABgIqyKcozEBgAAE2GKjWPMsQEAAG6Dig0AAGZCycYhEhsAAEyEycOOMRQFAADcBhUbAABMhFVRjpHYAABgIkyxcYyhKAAA4Dao2AAAYCaUbBwisQEAwERYFeUYQ1EAAMBtULEBAMBEWBXlGIkNAAAmwhQbx0hsAAAwEzIbh5hjAwAA3AYVGwAATIRVUY6R2AAAYCYumDzsxnkNQ1EAAMB9ULEBAMBEmDvsGIkNAABmQmbjEENRAADAobi4OFksFrutfv36tvO5ubkaOHCgqlatKn9/f/Xo0UNHjx616yMlJUVdunSRn5+fgoODNWLECBUWFro8Vio2AACYSHmtimrYsKG++OIL236FCn+lEEOHDtXq1av1wQcfKDAwUIMGDVL37t21efNmSVJRUZG6dOmi0NBQbdmyRampqXr44YdVsWJFvfjii6W6l78jsQEAwETK65UKFSpUUGho6HnHMzMz9d///ldLlizRrbfeKkmaP3++oqOj9c033+imm27S559/rp9++klffPGFQkJC1KxZM02cOFGjRo1SXFycvLy8SndD52AoCgCAq1RWVpbdlpeXd9G2e/fuVXh4uGrVqqXevXsrJSVFkrR9+3YVFBSoQ4cOtrb169fXtddeq4SEBElSQkKCGjdurJCQEFub2NhYZWVlaffu3S69JxIbAABMxOKiTZIiIiIUGBho2yZPnnzBz2zZsqUWLFigNWvWaNasWTpw4IBat26tkydPKi0tTV5eXgoKCrK7JiQkRGlpaZKktLQ0u6Tm7Pmz51yJoSgAAMzEhauiDh06JKvVajvs7e19weadO3e2/blJkyZq2bKlatasqWXLlsnX17eUwbgWFRsAAEzE4qL/JMlqtdptF0ts/i4oKEh169bVvn37FBoaqvz8fJ04ccKuzdGjR21zckJDQ89bJXV2/0LzdkqDxAYAADglOztbycnJCgsLU4sWLVSxYkWtW7fOdj4pKUkpKSmKiYmRJMXExGjnzp1KT0+3tYmPj5fValWDBg1cGhtDUQAAmIhFLlgV5WT74cOHq2vXrqpZs6aOHDmi559/Xp6enurZs6cCAwPVt29fDRs2TFWqVJHVatXgwYMVExOjm266SZLUsWNHNWjQQA899JCmTJmitLQ0jRkzRgMHDixxlaikSGwAADCR8njw8OHDh9WzZ08dO3ZM1atXV6tWrfTNN9+oevXqkqTp06fLw8NDPXr0UF5enmJjYzVz5kzb9Z6enlq1apWefPJJxcTEqFKlSurTp48mTJhQyjs5n8UwDMPlvcJpWVlZCgwM1NFjmXYTuQB3UrnzlPIOASgzRmGu8tY/r8zMsvk5fvb3xO4D6QooZf8ns7LUMDK4zGItT1RsAAAwkfJ6QJ9ZkNgAAGAqvAXTEVZFAQAAt0HFBgAAE2EoyjESGwAATISBKMcYigIAAG6Dig0AACbCUJRjJDYAAJjIue96Kk0f7orEBgAAM2GSjUPMsQEAAG6Dig0AACZCwcYxEhsAAEyEycOOMRQFAADcBhUbAABMhFVRjpHYAABgJkyycYihKAAA4Dao2AAAYCIUbBwjsQEAwERYFeUYQ1EAAMBtULEBAMBUSr8qyp0Ho0hsAAAwEYaiHGMoCgAAuA0SGwAA4DYYigIAwEQYinKMxAYAABPhlQqOMRQFAADcBhUbAABMhKEox0hsAAAwEV6p4BhDUQAAwG1QsQEAwEwo2ThEYgMAgImwKsoxhqIAAIDboGIDAICJsCrKMRIbAABMhCk2jpHYAABgJmQ2DjHHBgAAuA0qNgAAmAirohwjsQEAwESYPOwYic0VwjAMSdLJrKxyjgQoO0ZhbnmHAJSZs9/fZ3+el5UsF/yecEUfVyoSmyvEyZMnJUlRkRHlHAkAoDROnjypwMBAl/fr5eWl0NBQ1XHR74nQ0FB5eXm5pK8ricUo69QSJVJcXKwjR44oICBAFneuEV4hsrKyFBERoUOHDslqtZZ3OIDL8T1++RmGoZMnTyo8PFweHmWzNic3N1f5+fku6cvLy0s+Pj4u6etKQsXmCuHh4aEaNWqUdxhXHavVyg99uDW+xy+vsqjUnMvHx8ctkxFXYrk3AABwGyQ2AADAbZDY4Krk7e2t559/Xt7e3uUdClAm+B7H1YrJwwAAwG1QsQEAAG6DxAYAALgNEhsAAOA2SGxwRWvXrp2GDBlS3mEApmWxWLR8+fKLnl+/fr0sFotOnDhx2WICyhKJDQBcxW6++WalpqaW+YPlgMuFJw8DwFXs7PuHAHdBxQZXvOLiYo0cOVJVqlRRaGio4uLiJEkHDx6UxWJRYmKire2JEydksVi0fv16SX+V2deuXavmzZvL19dXt956q9LT0/XZZ58pOjpaVqtVvXr10qlTp2z9rFmzRq1atVJQUJCqVq2qO++8U8nJybbzZz/7448/Vvv27eXn56emTZsqISHhcnxJYFLt2rXT4MGDNWTIEFWuXFkhISGaM2eOcnJy9OijjyogIEBRUVH67LPPJElFRUXq27evIiMj5evrq3r16unVV189r9958+apYcOG8vb2VlhYmAYNGmR3/o8//tA999wjPz8/1alTRytWrLCd+/tQ1IIFCxQUFKS1a9cqOjpa/v7+6tSpk1JTU+36nDt3rqKjo+Xj46P69etr5syZLv5qAZfIAK5gbdu2NaxWqxEXF2f88ssvxsKFCw2LxWJ8/vnnxoEDBwxJxo4dO2ztjx8/bkgyvvrqK8MwDOOrr74yJBk33XSTsWnTJuP77783oqKijLZt2xodO3Y0vv/+e2Pjxo1G1apVjZdeesnWz4cffmh89NFHxt69e40dO3YYXbt2NRo3bmwUFRUZhmHYPrt+/frGqlWrjKSkJOPee+81atasaRQUFFzOLxFMpG3btkZAQIAxceJE45dffjEmTpxoeHp6Gp07dzbefvtt45dffjGefPJJo2rVqkZOTo6Rn59vjBs3zvjuu++M/fv3G++++67h5+dnvP/++7Y+Z86cafj4+BgzZswwkpKSjG+//daYPn267bwko0aNGsaSJUuMvXv3Gk899ZTh7+9vHDt2zDCMv/6OHD9+3DAMw5g/f75RsWJFo0OHDsZ3331nbN++3YiOjjZ69epl6/Pdd981wsLCjI8++sjYv3+/8dFHHxlVqlQxFixYcFm+joAjJDa4orVt29Zo1aqV3bEbb7zRGDVqlFOJzRdffGFrM3nyZEOSkZycbDv273//24iNjb1oHL///rshydi5c6dhGH8lNnPnzrW12b17tyHJ2LNnT2luGW7s79/PhYWFRqVKlYyHHnrIdiw1NdWQZCQkJFywj4EDBxo9evSw7YeHhxvPPffcRT9TkjFmzBjbfnZ2tiHJ+OyzzwzDuHBiI8nYt2+f7Zo333zTCAkJse3Xrl3bWLJkid3nTJw40YiJiXF0+8BlwVAUrnhNmjSx2w8LC1N6evol9xESEiI/Pz/VqlXL7ti5fe7du1c9e/ZUrVq1ZLVadd1110mSUlJSLtpvWFiYJDkdG64u537PeHp6qmrVqmrcuLHtWEhIiKS/vo/efPNNtWjRQtWrV5e/v7/efvtt2/dhenq6jhw5ottuu63En1mpUiVZrVaH36d+fn6qXbu2bf/cv3M5OTlKTk5W37595e/vb9smTZpkN1wLlBcmD+OKV7FiRbt9i8Wi4uJieXicycuNc94KUlBQ8I99WCyWi/Z5VteuXVWzZk3NmTNH4eHhKi4uVqNGjZSfn++wX0l2/QB/d6HvvYt9Hy1dulTDhw/XtGnTFBMTo4CAAE2dOlVbt26VJPn6+l7yZzr6Pr1Q+7N/z7KzsyVJc+bMUcuWLe3aeXp6ligeoCyR2MC0qlevLklKTU1V8+bNJcluIvGlOnbsmJKSkjRnzhy1bt1akrRp06ZS9ws4a/Pmzbr55ps1YMAA27FzqyIBAQG67rrrtG7dOrVv3/6yxBQSEqLw8HDt379fvXv3viyfCTiDxAam5evrq5tuukkvvfSSIiMjlZ6erjFjxpS638qVK6tq1ap6++23FRYWppSUFD377LMuiBhwTp06dbRo0SKtXbtWkZGReuedd/Tdd98pMjLS1iYuLk5PPPGEgoOD1blzZ508eVKbN2/W4MGDyyyu8ePH66mnnlJgYKA6deqkvLw8bdu2TcePH9ewYcPK7HOBkmCODUxt3rx5KiwsVIsWLTRkyBBNmjSp1H16eHho6dKl2r59uxo1aqShQ4dq6tSpLogWcM6///1vde/eXf/617/UsmVLHTt2zK56I0l9+vTRjBkzNHPmTDVs2FB33nmn9u7dW6Zx9evXT3PnztX8+fPVuHFjtW3bVgsWLLBLuIDyYjHOnaAAAABgYlRsAACA2yCxAQAAboPEBgAAuA0SGwAA4DZIbAAAgNsgsQEAAG6DxAYAALgNEhsAAOA2SGwA2DzyyCO6++67bfvt2rXTkCFDLnsc69evl8Vi0YkTJy7axmKxaPny5SXuMy4uTs2aNStVXAcPHpTFYnHJO8kAlA0SG+AK98gjj8hischiscjLy0tRUVGaMGGCCgsLy/yzP/74Y02cOLFEbUuSjABAWeMlmIAJdOrUSfPnz1deXp4+/fRTDRw4UBUrVtTo0aPPa5ufny8vLy+XfG6VKlVc0g8AXC5UbAAT8Pb2VmhoqGrWrKknn3xSHTp00IoVKyT9NXz0wgsvKDw8XPXq1ZMkHTp0SPfff7+CgoJUpUoVdevWTQcPHrT1WVRUpGHDhikoKEhVq1bVyJEj9fdXx/19KCovL0+jRo1SRESEvL29FRUVpf/+9786ePCg2rdvL+nM29EtFoseeeQRSVJxcbEmT56syMhI+fr6qmnTpvrwww/tPufTTz9V3bp15evrq/bt29vFWVKjRo1S3bp15efnp1q1amns2LEqKCg4r91bb72liIgI+fn56f7771dmZqbd+blz5yo6Olo+Pj6qX7++Zs6c6XQsAMoPiQ1gQr6+vsrPz7ftr1u3TklJSYqPj9eqVatUUFCg2NhYBQQE6Ouvv9bmzZvl7++vTp062a6bNm2aFixYoHnz5mnTpk3KyMjQJ5984vBzH374Yb333nt67bXXtGfPHr311lvy9/dXRESEPvroI0lSUlKSUlNT9eqrr0qSJk+erEWLFmn27NnavXu3hg4dqgcffFAbNmyQdCYB6969u7p27arExET169dPzz77rNNfk4CAAC1YsEA//fSTXn31Vc2ZM0fTp0+3a7Nv3z4tW7ZMK1eu1Jo1a7Rjxw67t2UvXrxY48aN0wsvvKA9e/boxRdf1NixY7Vw4UKn4wFQTgwAV7Q+ffoY3bp1MwzDMIqLi434+HjD29vbGD58uO18SEiIkZeXZ7vmnXfeMerVq2cUFxfbjuXl5Rm+vr7G2rVrDcMwjLCwMGPKlCm28wUFBUaNGjVsn2UYhtG2bVvj6aefNgzDMJKSkgxJRnx8/AXj/OqrrwxJxvHjx23HcnNzDT8/P2PLli12bfv27Wv07NnTMAzDGD16tNGgQQO786NGjTqvr7+TZHzyyScXPT916lSjRYsWtv3nn3/e8PT0NA4fPmw79tlnnxkeHh5GamqqYRiGUbt2bWPJkiV2/UycONGIiYkxDMMwDhw4YEgyduzYcdHPBVC+mGMDmMCqVavk7++vgoICFRcXq1evXoqLi7Odb9y4sd28mh9++EH79u1TQECAXT+5ublKTk5WZmamUlNT1bJlS9u5ChUq6IYbbjhvOOqsxMREeXp6qm3btiWOe9++fTp16pRuv/12u+P5+flq3ry5JGnPnj12cUhSTExMiT/jrPfff1+vvfaakpOTlZ2drcLCQlmtVrs21157ra655hq7zykuLlZSUpICAgKUnJysvn37qn///rY2hYWFCgwMdDoeAOWDxAYwgfbt22vWrFny8vJSeHi4KlSw/6tbqVIlu/3s7Gy1aNFCixcvPq+v6tWrX1IMvr6+Tl+TnZ0tSVq9erVdQiGdmTfkKgkJCerdu7fGjx+v2NhYBQYGaunSpZo2bZrTsc6ZM+e8RMvT09NlsQIoWyQ2gAlUqlRJUVFRJW5//fXX6/3331dwcPB5VYuzwsLCtHXrVrVp00bSmcrE9u3bdf3111+wfePGjVVcXKwNGzaoQ4cO550/WzEqKiqyHWvQoIG8vb2VkpJy0UpPdHS0bSL0Wd98880/3+Q5tmzZopo1a+q5556zHfv111/Pa5eSkqIjR44oPDzc9jkeHh6qV6+eQkJCFB4erv3796t3795OfT6AKweThwE31Lt3b1WrVk3dunXT119/rQMHDmj9+vV66qmndPjwYUnS008/rZdeeknLly/Xzz//rAEDBjh8Bs11112nPn366LHHHtPy5cttfS5btkySVLNmTVksFq1atUq///67srOzFRAQoOHDh2vo0KFauHChkpOT9f333+v111+3Tch94okntHfvXo0YMUJJSUlasmSJFixY4NT91qlTRykpKVq6dKmSk5P12muvXXAitI+Pj/r06aMffvhBX3/9tZ566indf//9Cg0NlSSNHz9ekydP1muvvaZffvlFO3fu1Pz58/XKK684FQ+A8kNiA7ghPz8/bdy4Uddee626d++u6Oho9e3bV7m5ubYKzjPPPKOHHnpIffr0UUxMjAICAnTPPfc47HfWrFm69957NWDAANWvX1/9+/dXTk6OJOmaa67R+PHj9eyzzyokJESDBg2SJE2cOFFjx47V5MmTFR0drU6dOmn16tWKjIyUdGbey0cffaTly5eradOmmj17tl588UWn7veuu+7S0KFDNWjQIDVr1kxbtmzR2LFjz2sXFRWl7t2764477lDHjh3VpEkTu+Xc/fr109y5czV//nw1btxYbdu21YIFC2yxArjyWYyLzRQEAAAwGSo2AADAbZDYAAAAt0FiAwAA3AaJDQAAcBskNgAAwG2Q2AAAALdBYgMAANwGiQ0AAHAbJDYAAMBtkNgAAAC3QWIDAADcxv8HE1c5wIHnKHsAAAAASUVORK5CYII="/>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=af7d6f55-fb4f-4195-ac3f-93852ecd4b65">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [40]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">file_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">project_name</span><span class="si">}</span><span class="s2">-v12.ipynb"</span>
<span class="n">html_file_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">file_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">'.ipynb'</span><span class="p">,</span><span class="w"> </span><span class="s1">'.html'</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span>

<span class="n">command</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"jupyter nbconvert '</span><span class="si">{</span><span class="n">file_name</span><span class="si">}</span><span class="s2">' --to html --output-dir './html' --output '</span><span class="si">{</span><span class="n">html_file_name</span><span class="si">}</span><span class="s2">'"</span>
<span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">system</span><span class="p">(</span><span class="n">command</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[NbConvertApp] Converting notebook praxis-gpt2-small-finetune-v10.ipynb to html
[NbConvertApp] WARNING | Alternative text is missing on 1 image(s).
[NbConvertApp] Writing 1299947 bytes to html/praxis-gpt2-small-finetune-v10.html
</pre>
</div>
</div>
</div>
</div>
</div>
</main>
</body>
</html>
