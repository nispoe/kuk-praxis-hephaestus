<!DOCTYPE html>

<html lang="en">
<head><meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>praxis-Meta-Llama-3-70B-small-finetune-v12</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<style type="text/css">
    pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: var(--jp-cell-editor-active-background) }
.highlight { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) }
.highlight .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */
.highlight .err { color: var(--jp-mirror-editor-error-color) } /* Error */
.highlight .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */
.highlight .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */
.highlight .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */
.highlight .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */
.highlight .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */
.highlight .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */
.highlight .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */
.highlight .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */
.highlight .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */
.highlight .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */
.highlight .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */
.highlight .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */
.highlight .pm { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation.Marker */
.highlight .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */
.highlight .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */
.highlight .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */
.highlight .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */
.highlight .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */
.highlight .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */
.highlight .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */
.highlight .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */
.highlight .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */
.highlight .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */
.highlight .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */
.highlight .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */
.highlight .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */
.highlight .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */
.highlight .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */
.highlight .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */
.highlight .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */
.highlight .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */
.highlight .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */
.highlight .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */
  </style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
 * Mozilla scrollbar styling
 */

/* use standard opaque scrollbars for most nodes */
[data-jp-theme-scrollbars='true'] {
  scrollbar-color: rgb(var(--jp-scrollbar-thumb-color))
    var(--jp-scrollbar-background-color);
}

/* for code nodes, use a transparent style of scrollbar. These selectors
 * will match lower in the tree, and so will override the above */
[data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar,
[data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
}

/* tiny scrollbar */

.jp-scrollbar-tiny {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
  scrollbar-width: thin;
}

/* tiny scrollbar */

.jp-scrollbar-tiny::-webkit-scrollbar,
.jp-scrollbar-tiny::-webkit-scrollbar-corner {
  background-color: transparent;
  height: 4px;
  width: 4px;
}

.jp-scrollbar-tiny::-webkit-scrollbar-thumb {
  background: rgba(var(--jp-scrollbar-thumb-color), 0.5);
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:horizontal {
  border-left: 0 solid transparent;
  border-right: 0 solid transparent;
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:vertical {
  border-top: 0 solid transparent;
  border-bottom: 0 solid transparent;
}

/*
 * Lumino
 */

.lm-ScrollBar[data-orientation='horizontal'] {
  min-height: 16px;
  max-height: 16px;
  min-width: 45px;
  border-top: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] {
  min-width: 16px;
  max-width: 16px;
  min-height: 45px;
  border-left: 1px solid #a0a0a0;
}

.lm-ScrollBar-button {
  background-color: #f0f0f0;
  background-position: center center;
  min-height: 15px;
  max-height: 15px;
  min-width: 15px;
  max-width: 15px;
}

.lm-ScrollBar-button:hover {
  background-color: #dadada;
}

.lm-ScrollBar-button.lm-mod-active {
  background-color: #cdcdcd;
}

.lm-ScrollBar-track {
  background: #f0f0f0;
}

.lm-ScrollBar-thumb {
  background: #cdcdcd;
}

.lm-ScrollBar-thumb:hover {
  background: #bababa;
}

.lm-ScrollBar-thumb.lm-mod-active {
  background: #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal'] .lm-ScrollBar-thumb {
  height: 100%;
  min-width: 15px;
  border-left: 1px solid #a0a0a0;
  border-right: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] .lm-ScrollBar-thumb {
  width: 100%;
  min-height: 15px;
  border-top: 1px solid #a0a0a0;
  border-bottom: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-left);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-right);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-up);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-down);
  background-size: 17px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Widget {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
}

.lm-Widget.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.lm-AccordionPanel[data-orientation='horizontal'] > .lm-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  display: flex;
  flex-direction: column;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-CommandPalette-search {
  flex: 0 0 auto;
}

.lm-CommandPalette-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  min-height: 0;
  overflow: auto;
  list-style-type: none;
}

.lm-CommandPalette-header {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-CommandPalette-item {
  display: flex;
  flex-direction: row;
}

.lm-CommandPalette-itemIcon {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemContent {
  flex: 1 1 auto;
  overflow: hidden;
}

.lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemLabel {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-close-icon {
  border: 1px solid transparent;
  background-color: transparent;
  position: absolute;
  z-index: 1;
  right: 3%;
  top: 0;
  bottom: 0;
  margin: auto;
  padding: 7px 0;
  display: none;
  vertical-align: middle;
  outline: 0;
  cursor: pointer;
}
.lm-close-icon:after {
  content: 'X';
  display: block;
  width: 15px;
  height: 15px;
  text-align: center;
  color: #000;
  font-weight: normal;
  font-size: 12px;
  cursor: pointer;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-DockPanel {
  z-index: 0;
}

.lm-DockPanel-widget {
  z-index: 0;
}

.lm-DockPanel-tabBar {
  z-index: 1;
}

.lm-DockPanel-handle {
  z-index: 2;
}

.lm-DockPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-DockPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-DockPanel-handle[data-orientation='horizontal'] {
  cursor: ew-resize;
}

.lm-DockPanel-handle[data-orientation='vertical'] {
  cursor: ns-resize;
}

.lm-DockPanel-handle[data-orientation='horizontal']:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-DockPanel-handle[data-orientation='vertical']:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

.lm-DockPanel-overlay {
  z-index: 3;
  box-sizing: border-box;
  pointer-events: none;
}

.lm-DockPanel-overlay.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Menu {
  z-index: 10000;
  position: absolute;
  white-space: nowrap;
  overflow-x: hidden;
  overflow-y: auto;
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-Menu-content {
  margin: 0;
  padding: 0;
  display: table;
  list-style-type: none;
}

.lm-Menu-item {
  display: table-row;
}

.lm-Menu-item.lm-mod-hidden,
.lm-Menu-item.lm-mod-collapsed {
  display: none !important;
}

.lm-Menu-itemIcon,
.lm-Menu-itemSubmenuIcon {
  display: table-cell;
  text-align: center;
}

.lm-Menu-itemLabel {
  display: table-cell;
  text-align: left;
}

.lm-Menu-itemShortcut {
  display: table-cell;
  text-align: right;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-MenuBar {
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-MenuBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex-direction: row;
  list-style-type: none;
}

.lm-MenuBar-item {
  box-sizing: border-box;
}

.lm-MenuBar-itemIcon,
.lm-MenuBar-itemLabel {
  display: inline-block;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-ScrollBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-ScrollBar[data-orientation='horizontal'] {
  flex-direction: row;
}

.lm-ScrollBar[data-orientation='vertical'] {
  flex-direction: column;
}

.lm-ScrollBar-button {
  box-sizing: border-box;
  flex: 0 0 auto;
}

.lm-ScrollBar-track {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
  flex: 1 1 auto;
}

.lm-ScrollBar-thumb {
  box-sizing: border-box;
  position: absolute;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-SplitPanel-child {
  z-index: 0;
}

.lm-SplitPanel-handle {
  z-index: 1;
}

.lm-SplitPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-SplitPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle {
  cursor: ew-resize;
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle {
  cursor: ns-resize;
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-TabBar[data-orientation='horizontal'] {
  flex-direction: row;
  align-items: flex-end;
}

.lm-TabBar[data-orientation='vertical'] {
  flex-direction: column;
  align-items: flex-end;
}

.lm-TabBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex: 1 1 auto;
  list-style-type: none;
}

.lm-TabBar[data-orientation='horizontal'] > .lm-TabBar-content {
  flex-direction: row;
}

.lm-TabBar[data-orientation='vertical'] > .lm-TabBar-content {
  flex-direction: column;
}

.lm-TabBar-tab {
  display: flex;
  flex-direction: row;
  box-sizing: border-box;
  overflow: hidden;
  touch-action: none; /* Disable native Drag/Drop */
}

.lm-TabBar-tabIcon,
.lm-TabBar-tabCloseIcon {
  flex: 0 0 auto;
}

.lm-TabBar-tabLabel {
  flex: 1 1 auto;
  overflow: hidden;
  white-space: nowrap;
}

.lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
}

.lm-TabBar-tab.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar-addButton.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab {
  position: relative;
}

.lm-TabBar.lm-mod-dragging[data-orientation='horizontal'] .lm-TabBar-tab {
  left: 0;
  transition: left 150ms ease;
}

.lm-TabBar.lm-mod-dragging[data-orientation='vertical'] .lm-TabBar-tab {
  top: 0;
  transition: top 150ms ease;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging {
  transition: none;
}

.lm-TabBar-tabLabel .lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
  background: inherit;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabPanel-tabBar {
  z-index: 1;
}

.lm-TabPanel-stackedPanel {
  z-index: 0;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapse {
  display: flex;
  flex-direction: column;
  align-items: stretch;
}

.jp-Collapse-header {
  padding: 1px 12px;
  background-color: var(--jp-layout-color1);
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  align-items: center;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  text-transform: uppercase;
  user-select: none;
}

.jp-Collapser-icon {
  height: 16px;
}

.jp-Collapse-header-collapsed .jp-Collapser-icon {
  transform: rotate(-90deg);
  margin: auto 0;
}

.jp-Collapser-title {
  line-height: 25px;
}

.jp-Collapse-contents {
  padding: 0 12px;
  background-color: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensureUiComponents() in @jupyterlab/buildutils */

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

/* Icons urls */

:root {
  --jp-icon-add-above: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5MikiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik00Ljc1IDQuOTMwNjZINi42MjVWNi44MDU2NkM2LjYyNSA3LjAxMTkxIDYuNzkzNzUgNy4xODA2NiA3IDcuMTgwNjZDNy4yMDYyNSA3LjE4MDY2IDcuMzc1IDcuMDExOTEgNy4zNzUgNi44MDU2NlY0LjkzMDY2SDkuMjVDOS40NTYyNSA0LjkzMDY2IDkuNjI1IDQuNzYxOTEgOS42MjUgNC41NTU2NkM5LjYyNSA0LjM0OTQxIDkuNDU2MjUgNC4xODA2NiA5LjI1IDQuMTgwNjZINy4zNzVWMi4zMDU2NkM3LjM3NSAyLjA5OTQxIDcuMjA2MjUgMS45MzA2NiA3IDEuOTMwNjZDNi43OTM3NSAxLjkzMDY2IDYuNjI1IDIuMDk5NDEgNi42MjUgMi4zMDU2NlY0LjE4MDY2SDQuNzVDNC41NDM3NSA0LjE4MDY2IDQuMzc1IDQuMzQ5NDEgNC4zNzUgNC41NTU2NkM0LjM3NSA0Ljc2MTkxIDQuNTQzNzUgNC45MzA2NiA0Ljc1IDQuOTMwNjZaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC43Ii8+CjwvZz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTExLjUgOS41VjExLjVMMi41IDExLjVWOS41TDExLjUgOS41Wk0xMiA4QzEyLjU1MjMgOCAxMyA4LjQ0NzcyIDEzIDlWMTJDMTMgMTIuNTUyMyAxMi41NTIzIDEzIDEyIDEzTDIgMTNDMS40NDc3MiAxMyAxIDEyLjU1MjMgMSAxMlY5QzEgOC40NDc3MiAxLjQ0NzcxIDggMiA4TDEyIDhaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5MiI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KC0xIDAgMCAxIDEwIDEuNTU1NjYpIi8+CjwvY2xpcFBhdGg+CjwvZGVmcz4KPC9zdmc+Cg==);
  --jp-icon-add-below: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5OCkiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik05LjI1IDEwLjA2OTNMNy4zNzUgMTAuMDY5M0w3LjM3NSA4LjE5NDM0QzcuMzc1IDcuOTg4MDkgNy4yMDYyNSA3LjgxOTM0IDcgNy44MTkzNEM2Ljc5Mzc1IDcuODE5MzQgNi42MjUgNy45ODgwOSA2LjYyNSA4LjE5NDM0TDYuNjI1IDEwLjA2OTNMNC43NSAxMC4wNjkzQzQuNTQzNzUgMTAuMDY5MyA0LjM3NSAxMC4yMzgxIDQuMzc1IDEwLjQ0NDNDNC4zNzUgMTAuNjUwNiA0LjU0Mzc1IDEwLjgxOTMgNC43NSAxMC44MTkzTDYuNjI1IDEwLjgxOTNMNi42MjUgMTIuNjk0M0M2LjYyNSAxMi45MDA2IDYuNzkzNzUgMTMuMDY5MyA3IDEzLjA2OTNDNy4yMDYyNSAxMy4wNjkzIDcuMzc1IDEyLjkwMDYgNy4zNzUgMTIuNjk0M0w3LjM3NSAxMC44MTkzTDkuMjUgMTAuODE5M0M5LjQ1NjI1IDEwLjgxOTMgOS42MjUgMTAuNjUwNiA5LjYyNSAxMC40NDQzQzkuNjI1IDEwLjIzODEgOS40NTYyNSAxMC4wNjkzIDkuMjUgMTAuMDY5M1oiIGZpbGw9IiM2MTYxNjEiIHN0cm9rZT0iIzYxNjE2MSIgc3Ryb2tlLXdpZHRoPSIwLjciLz4KPC9nPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMi41IDUuNUwyLjUgMy41TDExLjUgMy41TDExLjUgNS41TDIuNSA1LjVaTTIgN0MxLjQ0NzcyIDcgMSA2LjU1MjI4IDEgNkwxIDNDMSAyLjQ0NzcyIDEuNDQ3NzIgMiAyIDJMMTIgMkMxMi41NTIzIDIgMTMgMi40NDc3MiAxMyAzTDEzIDZDMTMgNi41NTIyOSAxMi41NTIzIDcgMTIgN0wyIDdaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5OCI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KDEgMS43NDg0NmUtMDcgMS43NDg0NmUtMDcgLTEgNCAxMy40NDQzKSIvPgo8L2NsaXBQYXRoPgo8L2RlZnM+Cjwvc3ZnPgo=);
  --jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bell: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE2IDE2IiB2ZXJzaW9uPSIxLjEiPgogICA8cGF0aCBjbGFzcz0ianAtaWNvbjIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMzMzMzMzIgogICAgICBkPSJtOCAwLjI5Yy0xLjQgMC0yLjcgMC43My0zLjYgMS44LTEuMiAxLjUtMS40IDMuNC0xLjUgNS4yLTAuMTggMi4yLTAuNDQgNC0yLjMgNS4zbDAuMjggMS4zaDVjMC4wMjYgMC42NiAwLjMyIDEuMSAwLjcxIDEuNSAwLjg0IDAuNjEgMiAwLjYxIDIuOCAwIDAuNTItMC40IDAuNi0xIDAuNzEtMS41aDVsMC4yOC0xLjNjLTEuOS0wLjk3LTIuMi0zLjMtMi4zLTUuMy0wLjEzLTEuOC0wLjI2LTMuNy0xLjUtNS4yLTAuODUtMS0yLjItMS44LTMuNi0xLjh6bTAgMS40YzAuODggMCAxLjkgMC41NSAyLjUgMS4zIDAuODggMS4xIDEuMSAyLjcgMS4yIDQuNCAwLjEzIDEuNyAwLjIzIDMuNiAxLjMgNS4yaC0xMGMxLjEtMS42IDEuMi0zLjQgMS4zLTUuMiAwLjEzLTEuNyAwLjMtMy4zIDEuMi00LjQgMC41OS0wLjcyIDEuNi0xLjMgMi41LTEuM3ptLTAuNzQgMTJoMS41Yy0wLjAwMTUgMC4yOCAwLjAxNSAwLjc5LTAuNzQgMC43OS0wLjczIDAuMDAxNi0wLjcyLTAuNTMtMC43NC0wLjc5eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-bug-dot: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiPgogICAgICAgIDxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMTcuMTkgOEgyMFYxMEgxNy45MUMxNy45NiAxMC4zMyAxOCAxMC42NiAxOCAxMVYxMkgyMFYxNEgxOC41SDE4VjE0LjAyNzVDMTUuNzUgMTQuMjc2MiAxNCAxNi4xODM3IDE0IDE4LjVDMTQgMTkuMjA4IDE0LjE2MzUgMTkuODc3OSAxNC40NTQ5IDIwLjQ3MzlDMTMuNzA2MyAyMC44MTE3IDEyLjg3NTcgMjEgMTIgMjFDOS43OCAyMSA3Ljg1IDE5Ljc5IDYuODEgMThINFYxNkg2LjA5QzYuMDQgMTUuNjcgNiAxNS4zNCA2IDE1VjE0SDRWMTJINlYxMUM2IDEwLjY2IDYuMDQgMTAuMzMgNi4wOSAxMEg0VjhINi44MUM3LjI2IDcuMjIgNy44OCA2LjU1IDguNjIgNi4wNEw3IDQuNDFMOC40MSAzTDEwLjU5IDUuMTdDMTEuMDQgNS4wNiAxMS41MSA1IDEyIDVDMTIuNDkgNSAxMi45NiA1LjA2IDEzLjQyIDUuMTdMMTUuNTkgM0wxNyA0LjQxTDE1LjM3IDYuMDRDMTYuMTIgNi41NSAxNi43NCA3LjIyIDE3LjE5IDhaTTEwIDE2SDE0VjE0SDEwVjE2Wk0xMCAxMkgxNFYxMEgxMFYxMloiIGZpbGw9IiM2MTYxNjEiLz4KICAgICAgICA8cGF0aCBkPSJNMjIgMTguNUMyMiAyMC40MzMgMjAuNDMzIDIyIDE4LjUgMjJDMTYuNTY3IDIyIDE1IDIwLjQzMyAxNSAxOC41QzE1IDE2LjU2NyAxNi41NjcgMTUgMTguNSAxNUMyMC40MzMgMTUgMjIgMTYuNTY3IDIyIDE4LjVaIiBmaWxsPSIjNjE2MTYxIi8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yMCA4aC0yLjgxYy0uNDUtLjc4LTEuMDctMS40NS0xLjgyLTEuOTZMMTcgNC40MSAxNS41OSAzbC0yLjE3IDIuMTdDMTIuOTYgNS4wNiAxMi40OSA1IDEyIDVjLS40OSAwLS45Ni4wNi0xLjQxLjE3TDguNDEgMyA3IDQuNDFsMS42MiAxLjYzQzcuODggNi41NSA3LjI2IDcuMjIgNi44MSA4SDR2MmgyLjA5Yy0uMDUuMzMtLjA5LjY2LS4wOSAxdjFINHYyaDJ2MWMwIC4zNC4wNC42Ny4wOSAxSDR2MmgyLjgxYzEuMDQgMS43OSAyLjk3IDMgNS4xOSAzczQuMTUtMS4yMSA1LjE5LTNIMjB2LTJoLTIuMDljLjA1LS4zMy4wOS0uNjYuMDktMXYtMWgydi0yaC0ydi0xYzAtLjM0LS4wNC0uNjctLjA5LTFIMjBWOHptLTYgOGgtNHYtMmg0djJ6bTAtNGgtNHYtMmg0djJ6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-build: url(data:image/svg+xml;base64,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);
  --jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik05IDE2LjE3TDQuODMgMTJsLTEuNDIgMS40MUw5IDE5IDIxIDdsLTEuNDEtMS40MXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBzaGFwZS1yZW5kZXJpbmc9Imdlb21ldHJpY1ByZWNpc2lvbiI+CiAgICA8cGF0aCBkPSJNNi41OSwzLjQxTDIsOEw2LjU5LDEyLjZMOCwxMS4xOEw0LjgyLDhMOCw0LjgyTDYuNTksMy40MU0xMi40MSwzLjQxTDExLDQuODJMMTQuMTgsOEwxMSwxMS4xOEwxMi40MSwxMi42TDE3LDhMMTIuNDEsMy40MU0yMS41OSwxMS41OUwxMy41LDE5LjY4TDkuODMsMTZMOC40MiwxNy40MUwxMy41LDIyLjVMMjMsMTNMMjEuNTksMTEuNTlaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTExLjQgMTguNkw2LjggMTRMMTEuNCA5LjRMMTAgOEw0IDE0TDEwIDIwTDExLjQgMTguNlpNMTYuNiAxOC42TDIxLjIgMTRMMTYuNiA5LjRMMTggOEwyNCAxNEwxOCAyMEwxNi42IDE4LjZWMTguNloiLz4KCTwvZz4KPC9zdmc+Cg==);
  --jp-icon-collapse-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNNiAxM3YyaDh2LTJ6IiAvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1jb25zb2xlLWljb24tYmFja2dyb3VuZC1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtY29uc29sZS1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIj4KICAgIDxwYXRoIGQ9Ik0xMDUgMTI3LjNoNDB2MTIuOGgtNDB6TTUxLjEgNzdMNzQgOTkuOWwtMjMuMyAyMy4zIDEwLjUgMTAuNSAyMy4zLTIzLjNMOTUgOTkuOSA4NC41IDg5LjQgNjEuNiA2Ni41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copyright: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGVuYWJsZS1iYWNrZ3JvdW5kPSJuZXcgMCAwIDI0IDI0IiBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCI+CiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0xMS44OCw5LjE0YzEuMjgsMC4wNiwxLjYxLDEuMTUsMS42MywxLjY2aDEuNzljLTAuMDgtMS45OC0xLjQ5LTMuMTktMy40NS0zLjE5QzkuNjQsNy42MSw4LDksOCwxMi4xNCBjMCwxLjk0LDAuOTMsNC4yNCwzLjg0LDQuMjRjMi4yMiwwLDMuNDEtMS42NSwzLjQ0LTIuOTVoLTEuNzljLTAuMDMsMC41OS0wLjQ1LDEuMzgtMS42MywxLjQ0QzEwLjU1LDE0LjgzLDEwLDEzLjgxLDEwLDEyLjE0IEMxMCw5LjI1LDExLjI4LDkuMTYsMTEuODgsOS4xNHogTTEyLDJDNi40OCwyLDIsNi40OCwyLDEyczQuNDgsMTAsMTAsMTBzMTAtNC40OCwxMC0xMFMxNy41MiwyLDEyLDJ6IE0xMiwyMGMtNC40MSwwLTgtMy41OS04LTggczMuNTktOCw4LThzOCwzLjU5LDgsOFMxNi40MSwyMCwxMiwyMHoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-delete: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2cHgiIGhlaWdodD0iMTZweCI+CiAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIiAvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjI2MjYyIiBkPSJNNiAxOWMwIDEuMS45IDIgMiAyaDhjMS4xIDAgMi0uOSAyLTJWN0g2djEyek0xOSA0aC0zLjVsLTEtMWgtNWwtMSAxSDV2MmgxNFY0eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-duplicate: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTIuNzk5OTggMC44NzVIOC44OTU4MkM5LjIwMDYxIDAuODc1IDkuNDQ5OTggMS4xMzkxNCA5LjQ0OTk4IDEuNDYxOThDOS40NDk5OCAxLjc4NDgyIDkuMjAwNjEgMi4wNDg5NiA4Ljg5NTgyIDIuMDQ4OTZIMy4zNTQxNUMzLjA0OTM2IDIuMDQ4OTYgMi43OTk5OCAyLjMxMzEgMi43OTk5OCAyLjYzNTk0VjkuNjc5NjlDMi43OTk5OCAxMC4wMDI1IDIuNTUwNjEgMTAuMjY2NyAyLjI0NTgyIDEwLjI2NjdDMS45NDEwMyAxMC4yNjY3IDEuNjkxNjUgMTAuMDAyNSAxLjY5MTY1IDkuNjc5NjlWMi4wNDg5NkMxLjY5MTY1IDEuNDAzMjggMi4xOTA0IDAuODc1IDIuNzk5OTggMC44NzVaTTUuMzY2NjUgMTEuOVY0LjU1SDExLjA4MzNWMTEuOUg1LjM2NjY1Wk00LjE0MTY1IDQuMTQxNjdDNC4xNDE2NSAzLjY5MDYzIDQuNTA3MjggMy4zMjUgNC45NTgzMiAzLjMyNUgxMS40OTE3QzExLjk0MjcgMy4zMjUgMTIuMzA4MyAzLjY5MDYzIDEyLjMwODMgNC4xNDE2N1YxMi4zMDgzQzEyLjMwODMgMTIuNzU5NCAxMS45NDI3IDEzLjEyNSAxMS40OTE3IDEzLjEyNUg0Ljk1ODMyQzQuNTA3MjggMTMuMTI1IDQuMTQxNjUgMTIuNzU5NCA0LjE0MTY1IDEyLjMwODNWNC4xNDE2N1oiIGZpbGw9IiM2MTYxNjEiLz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNOS40MzU3NCA4LjI2NTA3SDguMzY0MzFWOS4zMzY1QzguMzY0MzEgOS40NTQzNSA4LjI2Nzg4IDkuNTUwNzggOC4xNTAwMiA5LjU1MDc4QzguMDMyMTcgOS41NTA3OCA3LjkzNTc0IDkuNDU0MzUgNy45MzU3NCA5LjMzNjVWOC4yNjUwN0g2Ljg2NDMxQzYuNzQ2NDUgOC4yNjUwNyA2LjY1MDAyIDguMTY4NjQgNi42NTAwMiA4LjA1MDc4QzYuNjUwMDIgNy45MzI5MiA2Ljc0NjQ1IDcuODM2NSA2Ljg2NDMxIDcuODM2NUg3LjkzNTc0VjYuNzY1MDdDNy45MzU3NCA2LjY0NzIxIDguMDMyMTcgNi41NTA3OCA4LjE1MDAyIDYuNTUwNzhDOC4yNjc4OCA2LjU1MDc4IDguMzY0MzEgNi42NDcyMSA4LjM2NDMxIDYuNzY1MDdWNy44MzY1SDkuNDM1NzRDOS41NTM2IDcuODM2NSA5LjY1MDAyIDcuOTMyOTIgOS42NTAwMiA4LjA1MDc4QzkuNjUwMDIgOC4xNjg2NCA5LjU1MzYgOC4yNjUwNyA5LjQzNTc0IDguMjY1MDdaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC41Ii8+Cjwvc3ZnPgo=);
  --jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-error: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj48Y2lyY2xlIGN4PSIxMiIgY3k9IjE5IiByPSIyIi8+PHBhdGggZD0iTTEwIDNoNHYxMmgtNHoiLz48L2c+CjxwYXRoIGZpbGw9Im5vbmUiIGQ9Ik0wIDBoMjR2MjRIMHoiLz4KPC9zdmc+Cg==);
  --jp-icon-expand-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNMTEgMTBIOXYzSDZ2MmgzdjNoMnYtM2gzdi0yaC0zeiIgLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-filter-dot: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWRvdCIgZmlsbD0iI0ZGRiI+CiAgICA8Y2lyY2xlIGN4PSIxOCIgY3k9IjE3IiByPSIzIj48L2NpcmNsZT4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-filter: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-folder-favorite: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgwVjB6IiBmaWxsPSJub25lIi8+PHBhdGggY2xhc3M9ImpwLWljb24zIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzYxNjE2MSIgZD0iTTIwIDZoLThsLTItMkg0Yy0xLjEgMC0yIC45LTIgMnYxMmMwIDEuMS45IDIgMiAyaDE2YzEuMSAwIDItLjkgMi0yVjhjMC0xLjEtLjktMi0yLTJ6bS0yLjA2IDExTDE1IDE1LjI4IDEyLjA2IDE3bC43OC0zLjMzLTIuNTktMi4yNCAzLjQxLS4yOUwxNSA4bDEuMzQgMy4xNCAzLjQxLjI5LTIuNTkgMi4yNC43OCAzLjMzeiIvPgo8L3N2Zz4K);
  --jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-home: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPjxwYXRoIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xMCAyMHYtNmg0djZoNXYtOGgzTDEyIDMgMiAxMmgzdjh6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==);
  --jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-info: url(data:image/svg+xml;base64,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);
  --jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaW5zcGVjdG9yLWljb24tY29sb3IganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K);
  --jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtanNvbi1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0Y5QTgyNSI+CiAgICA8cGF0aCBkPSJNMjAuMiAxMS44Yy0xLjYgMC0xLjcuNS0xLjcgMSAwIC40LjEuOS4xIDEuMy4xLjUuMS45LjEgMS4zIDAgMS43LTEuNCAyLjMtMy41IDIuM2gtLjl2LTEuOWguNWMxLjEgMCAxLjQgMCAxLjQtLjggMC0uMyAwLS42LS4xLTEgMC0uNC0uMS0uOC0uMS0xLjIgMC0xLjMgMC0xLjggMS4zLTItMS4zLS4yLTEuMy0uNy0xLjMtMiAwLS40LjEtLjguMS0xLjIuMS0uNC4xLS43LjEtMSAwLS44LS40LS43LTEuNC0uOGgtLjVWNC4xaC45YzIuMiAwIDMuNS43IDMuNSAyLjMgMCAuNC0uMS45LS4xIDEuMy0uMS41LS4xLjktLjEgMS4zIDAgLjUuMiAxIDEuNyAxdjEuOHpNMS44IDEwLjFjMS42IDAgMS43LS41IDEuNy0xIDAtLjQtLjEtLjktLjEtMS4zLS4xLS41LS4xLS45LS4xLTEuMyAwLTEuNiAxLjQtMi4zIDMuNS0yLjNoLjl2MS45aC0uNWMtMSAwLTEuNCAwLTEuNC44IDAgLjMgMCAuNi4xIDEgMCAuMi4xLjYuMSAxIDAgMS4zIDAgMS44LTEuMyAyQzYgMTEuMiA2IDExLjcgNiAxM2MwIC40LS4xLjgtLjEgMS4yLS4xLjMtLjEuNy0uMSAxIDAgLjguMy44IDEuNC44aC41djEuOWgtLjljLTIuMSAwLTMuNS0uNi0zLjUtMi4zIDAtLjQuMS0uOS4xLTEuMy4xLS41LjEtLjkuMS0xLjMgMC0uNS0uMi0xLTEuNy0xdi0xLjl6Ii8+CiAgICA8Y2lyY2xlIGN4PSIxMSIgY3k9IjEzLjgiIHI9IjIuMSIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSI4LjIiIHI9IjIuMSIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-julia: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDMyNSAzMDAiPgogIDxnIGNsYXNzPSJqcC1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjY2IzYzMzIj4KICAgIDxwYXRoIGQ9Ik0gMTUwLjg5ODQzOCAyMjUgQyAxNTAuODk4NDM4IDI2Ni40MjE4NzUgMTE3LjMyMDMxMiAzMDAgNzUuODk4NDM4IDMwMCBDIDM0LjQ3NjU2MiAzMDAgMC44OTg0MzggMjY2LjQyMTg3NSAwLjg5ODQzOCAyMjUgQyAwLjg5ODQzOCAxODMuNTc4MTI1IDM0LjQ3NjU2MiAxNTAgNzUuODk4NDM4IDE1MCBDIDExNy4zMjAzMTIgMTUwIDE1MC44OTg0MzggMTgzLjU3ODEyNSAxNTAuODk4NDM4IDIyNSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzM4OTgyNiI+CiAgICA8cGF0aCBkPSJNIDIzNy41IDc1IEMgMjM3LjUgMTE2LjQyMTg3NSAyMDMuOTIxODc1IDE1MCAxNjIuNSAxNTAgQyAxMjEuMDc4MTI1IDE1MCA4Ny41IDExNi40MjE4NzUgODcuNSA3NSBDIDg3LjUgMzMuNTc4MTI1IDEyMS4wNzgxMjUgMCAxNjIuNSAwIEMgMjAzLjkyMTg3NSAwIDIzNy41IDMzLjU3ODEyNSAyMzcuNSA3NSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzk1NThiMiI+CiAgICA8cGF0aCBkPSJNIDMyNC4xMDE1NjIgMjI1IEMgMzI0LjEwMTU2MiAyNjYuNDIxODc1IDI5MC41MjM0MzggMzAwIDI0OS4xMDE1NjIgMzAwIEMgMjA3LjY3OTY4OCAzMDAgMTc0LjEwMTU2MiAyNjYuNDIxODc1IDE3NC4xMDE1NjIgMjI1IEMgMTc0LjEwMTU2MiAxODMuNTc4MTI1IDIwNy42Nzk2ODggMTUwIDI0OS4xMDE1NjIgMTUwIEMgMjkwLjUyMzQzOCAxNTAgMzI0LjEwMTU2MiAxODMuNTc4MTI1IDMyNC4xMDE1NjIgMjI1Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgPGcgY2xhc3M9ImpwLWp1cHl0ZXItaWNvbi1jb2xvciIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-jupyter: url(data:image/svg+xml;base64,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);
  --jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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);
  --jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K);
  --jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==);
  --jp-icon-launch: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMzIgMzIiIHdpZHRoPSIzMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yNiwyOEg2YTIuMDAyNywyLjAwMjcsMCwwLDEtMi0yVjZBMi4wMDI3LDIuMDAyNywwLDAsMSw2LDRIMTZWNkg2VjI2SDI2VjE2aDJWMjZBMi4wMDI3LDIuMDAyNywwLDAsMSwyNiwyOFoiLz4KICAgIDxwb2x5Z29uIHBvaW50cz0iMjAgMiAyMCA0IDI2LjU4NiA0IDE4IDEyLjU4NiAxOS40MTQgMTQgMjggNS40MTQgMjggMTIgMzAgMTIgMzAgMiAyMCAyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==);
  --jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4K);
  --jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-move-down: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMTIuNDcxIDcuNTI4OTlDMTIuNzYzMiA3LjIzNjg0IDEyLjc2MzIgNi43NjMxNiAxMi40NzEgNi40NzEwMVY2LjQ3MTAxQzEyLjE3OSA2LjE3OTA1IDExLjcwNTcgNi4xNzg4NCAxMS40MTM1IDYuNDcwNTRMNy43NSAxMC4xMjc1VjEuNzVDNy43NSAxLjMzNTc5IDcuNDE0MjEgMSA3IDFWMUM2LjU4NTc5IDEgNi4yNSAxLjMzNTc5IDYuMjUgMS43NVYxMC4xMjc1TDIuNTk3MjYgNi40NjgyMkMyLjMwMzM4IDYuMTczODEgMS44MjY0MSA2LjE3MzU5IDEuNTMyMjYgNi40Njc3NFY2LjQ2Nzc0QzEuMjM4MyA2Ljc2MTcgMS4yMzgzIDcuMjM4MyAxLjUzMjI2IDcuNTMyMjZMNi4yOTI4OSAxMi4yOTI5QzYuNjgzNDIgMTIuNjgzNCA3LjMxNjU4IDEyLjY4MzQgNy43MDcxMSAxMi4yOTI5TDEyLjQ3MSA3LjUyODk5WiIgZmlsbD0iIzYxNjE2MSIvPgo8L3N2Zz4K);
  --jp-icon-move-up: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMS41Mjg5OSA2LjQ3MTAxQzEuMjM2ODQgNi43NjMxNiAxLjIzNjg0IDcuMjM2ODQgMS41Mjg5OSA3LjUyODk5VjcuNTI4OTlDMS44MjA5NSA3LjgyMDk1IDIuMjk0MjYgNy44MjExNiAyLjU4NjQ5IDcuNTI5NDZMNi4yNSAzLjg3MjVWMTIuMjVDNi4yNSAxMi42NjQyIDYuNTg1NzkgMTMgNyAxM1YxM0M3LjQxNDIxIDEzIDcuNzUgMTIuNjY0MiA3Ljc1IDEyLjI1VjMuODcyNUwxMS40MDI3IDcuNTMxNzhDMTEuNjk2NiA3LjgyNjE5IDEyLjE3MzYgNy44MjY0MSAxMi40Njc3IDcuNTMyMjZWNy41MzIyNkMxMi43NjE3IDcuMjM4MyAxMi43NjE3IDYuNzYxNyAxMi40Njc3IDYuNDY3NzRMNy43MDcxMSAxLjcwNzExQzcuMzE2NTggMS4zMTY1OCA2LjY4MzQyIDEuMzE2NTggNi4yOTI4OSAxLjcwNzExTDEuNTI4OTkgNi40NzEwMVoiIGZpbGw9IiM2MTYxNjEiLz4KPC9zdmc+Cg==);
  --jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=);
  --jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtbm90ZWJvb2staWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-numbering: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTQgMTlINlYxOS41SDVWMjAuNUg2VjIxSDRWMjJIN1YxOEg0VjE5Wk01IDEwSDZWNkg0VjdINVYxMFpNNCAxM0g1LjhMNCAxNS4xVjE2SDdWMTVINS4yTDcgMTIuOVYxMkg0VjEzWk05IDdWOUgyM1Y3SDlaTTkgMjFIMjNWMTlIOVYyMVpNOSAxNUgyM1YxM0g5VjE1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-offline-bolt: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDIuMDJjLTUuNTEgMC05Ljk4IDQuNDctOS45OCA5Ljk4czQuNDcgOS45OCA5Ljk4IDkuOTggOS45OC00LjQ3IDkuOTgtOS45OFMxNy41MSAyLjAyIDEyIDIuMDJ6TTExLjQ4IDIwdi02LjI2SDhMMTMgNHY2LjI2aDMuMzVMMTEuNDggMjB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-pdf: url(data:image/svg+xml;base64,PHN2ZwogICB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyMiAyMiIgd2lkdGg9IjE2Ij4KICAgIDxwYXRoIHRyYW5zZm9ybT0icm90YXRlKDQ1KSIgY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0ZGMkEyQSIKICAgICAgIGQ9Im0gMjIuMzQ0MzY5LC0zLjAxNjM2NDIgaCA1LjYzODYwNCB2IDEuNTc5MjQzMyBoIC0zLjU0OTIyNyB2IDEuNTA4NjkyOTkgaCAzLjMzNzU3NiBWIDEuNjUwODE1NCBoIC0zLjMzNzU3NiB2IDMuNDM1MjYxMyBoIC0yLjA4OTM3NyB6IG0gLTcuMTM2NDQ0LDEuNTc5MjQzMyB2IDQuOTQzOTU0MyBoIDAuNzQ4OTIgcSAxLjI4MDc2MSwwIDEuOTUzNzAzLC0wLjYzNDk1MzUgMC42NzgzNjksLTAuNjM0OTUzNSAwLjY3ODM2OSwtMS44NDUxNjQxIDAsLTEuMjA0NzgzNTUgLTAuNjcyOTQyLC0xLjgzNDMxMDExIC0wLjY3Mjk0MiwtMC42Mjk1MjY1OSAtMS45NTkxMywtMC42Mjk1MjY1OSB6IG0gLTIuMDg5Mzc3LC0xLjU3OTI0MzMgaCAyLjIwMzM0MyBxIDEuODQ1MTY0LDAgMi43NDYwMzksMC4yNjU5MjA3IDAuOTA2MzAxLDAuMjYwNDkzNyAxLjU1MjEwOCwwLjg5MDAyMDMgMC41Njk4MywwLjU0ODEyMjMgMC44NDY2MDUsMS4yNjQ0ODAwNiAwLjI3Njc3NCwwLjcxNjM1NzgxIDAuMjc2Nzc0LDEuNjIyNjU4OTQgMCwwLjkxNzE1NTEgLTAuMjc2Nzc0LDEuNjM4OTM5OSAtMC4yNzY3NzUsMC43MTYzNTc4IC0wLjg0NjYwNSwxLjI2NDQ4IC0wLjY1MTIzNCwwLjYyOTUyNjYgLTEuNTYyOTYyLDAuODk1NDQ3MyAtMC45MTE3MjgsMC4yNjA0OTM3IC0yLjczNTE4NSwwLjI2MDQ5MzcgaCAtMi4yMDMzNDMgeiBtIC04LjE0NTg1NjUsMCBoIDMuNDY3ODIzIHEgMS41NDY2ODE2LDAgMi4zNzE1Nzg1LDAuNjg5MjIzIDAuODMwMzI0LDAuNjgzNzk2MSAwLjgzMDMyNCwxLjk1MzcwMzE0IDAsMS4yNzUzMzM5NyAtMC44MzAzMjQsMS45NjQ1NTcwNiBRIDkuOTg3MTk2MSwyLjI3NDkxNSA4LjQ0MDUxNDUsMi4yNzQ5MTUgSCA3LjA2MjA2ODQgViA1LjA4NjA3NjcgSCA0Ljk3MjY5MTUgWiBtIDIuMDg5Mzc2OSwxLjUxNDExOTkgdiAyLjI2MzAzOTQzIGggMS4xNTU5NDEgcSAwLjYwNzgxODgsMCAwLjkzODg2MjksLTAuMjkzMDU1NDcgMC4zMzEwNDQxLC0wLjI5ODQ4MjQxIDAuMzMxMDQ0MSwtMC44NDExNzc3MiAwLC0wLjU0MjY5NTMxIC0wLjMzMTA0NDEsLTAuODM1NzUwNzQgLTAuMzMxMDQ0MSwtMC4yOTMwNTU1IC0wLjkzODg2MjksLTAuMjkzMDU1NSB6IgovPgo8L3N2Zz4K);
  --jp-icon-python: url(data:image/svg+xml;base64,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);
  --jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==);
  --jp-icon-react: url(data:image/svg+xml;base64,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);
  --jp-icon-redo: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZD0iTTE4LjQgMTAuNkMxNi41NSA4Ljk5IDE0LjE1IDggMTEuNSA4Yy00LjY1IDAtOC41OCAzLjAzLTkuOTYgNy4yMkwzLjkgMTZjMS4wNS0zLjE5IDQuMDUtNS41IDcuNi01LjUgMS45NSAwIDMuNzMuNzIgNS4xMiAxLjg4TDEzIDE2aDlWN2wtMy42IDMuNnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==);
  --jp-icon-share: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTSAxOCAyIEMgMTYuMzU0OTkgMiAxNSAzLjM1NDk5MDQgMTUgNSBDIDE1IDUuMTkwOTUyOSAxNS4wMjE3OTEgNS4zNzcxMjI0IDE1LjA1NjY0MSA1LjU1ODU5MzggTCA3LjkyMTg3NSA5LjcyMDcwMzEgQyA3LjM5ODUzOTkgOS4yNzc4NTM5IDYuNzMyMDc3MSA5IDYgOSBDIDQuMzU0OTkwNCA5IDMgMTAuMzU0OTkgMyAxMiBDIDMgMTMuNjQ1MDEgNC4zNTQ5OTA0IDE1IDYgMTUgQyA2LjczMjA3NzEgMTUgNy4zOTg1Mzk5IDE0LjcyMjE0NiA3LjkyMTg3NSAxNC4yNzkyOTcgTCAxNS4wNTY2NDEgMTguNDM5NDUzIEMgMTUuMDIxNTU1IDE4LjYyMTUxNCAxNSAxOC44MDgzODYgMTUgMTkgQyAxNSAyMC42NDUwMSAxNi4zNTQ5OSAyMiAxOCAyMiBDIDE5LjY0NTAxIDIyIDIxIDIwLjY0NTAxIDIxIDE5IEMgMjEgMTcuMzU0OTkgMTkuNjQ1MDEgMTYgMTggMTYgQyAxNy4yNjc0OCAxNiAxNi42MDE1OTMgMTYuMjc5MzI4IDE2LjA3ODEyNSAxNi43MjI2NTYgTCA4Ljk0MzM1OTQgMTIuNTU4NTk0IEMgOC45NzgyMDk1IDEyLjM3NzEyMiA5IDEyLjE5MDk1MyA5IDEyIEMgOSAxMS44MDkwNDcgOC45NzgyMDk1IDExLjYyMjg3OCA4Ljk0MzM1OTQgMTEuNDQxNDA2IEwgMTYuMDc4MTI1IDcuMjc5Mjk2OSBDIDE2LjYwMTQ2IDcuNzIyMTQ2MSAxNy4yNjc5MjMgOCAxOCA4IEMgMTkuNjQ1MDEgOCAyMSA2LjY0NTAwOTYgMjEgNSBDIDIxIDMuMzU0OTkwNCAxOS42NDUwMSAyIDE4IDIgeiBNIDE4IDQgQyAxOC41NjQxMjkgNCAxOSA0LjQzNTg3MDYgMTkgNSBDIDE5IDUuNTY0MTI5NCAxOC41NjQxMjkgNiAxOCA2IEMgMTcuNDM1ODcxIDYgMTcgNS41NjQxMjk0IDE3IDUgQyAxNyA0LjQzNTg3MDYgMTcuNDM1ODcxIDQgMTggNCB6IE0gNiAxMSBDIDYuNTY0MTI5NCAxMSA3IDExLjQzNTg3MSA3IDEyIEMgNyAxMi41NjQxMjkgNi41NjQxMjk0IDEzIDYgMTMgQyA1LjQzNTg3MDYgMTMgNSAxMi41NjQxMjkgNSAxMiBDIDUgMTEuNDM1ODcxIDUuNDM1ODcwNiAxMSA2IDExIHogTSAxOCAxOCBDIDE4LjU2NDEyOSAxOCAxOSAxOC40MzU4NzEgMTkgMTkgQyAxOSAxOS41NjQxMjkgMTguNTY0MTI5IDIwIDE4IDIwIEMgMTcuNDM1ODcxIDIwIDE3IDE5LjU2NDEyOSAxNyAxOSBDIDE3IDE4LjQzNTg3MSAxNy40MzU4NzEgMTggMTggMTggeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-table-rows: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMSw4SDNWNGgxOFY4eiBNMjEsMTBIM3Y0aDE4VjEweiBNMjEsMTZIM3Y0aDE4VjE2eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-tag: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjgiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCA0MyAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTI4LjgzMzIgMTIuMzM0TDMyLjk5OTggMTYuNTAwN0wzNy4xNjY1IDEyLjMzNEgyOC44MzMyWiIvPgoJCTxwYXRoIGQ9Ik0xNi4yMDk1IDIxLjYxMDRDMTUuNjg3MyAyMi4xMjk5IDE0Ljg0NDMgMjIuMTI5OSAxNC4zMjQ4IDIxLjYxMDRMNi45ODI5IDE0LjcyNDVDNi41NzI0IDE0LjMzOTQgNi4wODMxMyAxMy42MDk4IDYuMDQ3ODYgMTMuMDQ4MkM1Ljk1MzQ3IDExLjUyODggNi4wMjAwMiA4LjYxOTQ0IDYuMDY2MjEgNy4wNzY5NUM2LjA4MjgxIDYuNTE0NzcgNi41NTU0OCA2LjA0MzQ3IDcuMTE4MDQgNi4wMzA1NUM5LjA4ODYzIDUuOTg0NzMgMTMuMjYzOCA1LjkzNTc5IDEzLjY1MTggNi4zMjQyNUwyMS43MzY5IDEzLjYzOUMyMi4yNTYgMTQuMTU4NSAyMS43ODUxIDE1LjQ3MjQgMjEuMjYyIDE1Ljk5NDZMMTYuMjA5NSAyMS42MTA0Wk05Ljc3NTg1IDguMjY1QzkuMzM1NTEgNy44MjU2NiA4LjYyMzUxIDcuODI1NjYgOC4xODI4IDguMjY1QzcuNzQzNDYgOC43MDU3MSA3Ljc0MzQ2IDkuNDE3MzMgOC4xODI4IDkuODU2NjdDOC42MjM4MiAxMC4yOTY0IDkuMzM1ODIgMTAuMjk2NCA5Ljc3NTg1IDkuODU2NjdDMTAuMjE1NiA5LjQxNzMzIDEwLjIxNTYgOC43MDUzMyA5Ljc3NTg1IDguMjY1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-terminal: url(data:image/svg+xml;base64,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);
  --jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtdGV4dC1lZGl0b3ItaWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xNSAxNUgzdjJoMTJ2LTJ6bTAtOEgzdjJoMTJWN3pNMyAxM2gxOHYtMkgzdjJ6bTAgOGgxOHYtMkgzdjJ6TTMgM3YyaDE4VjNIM3oiLz4KPC9zdmc+Cg==);
  --jp-icon-toc: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik03LDVIMjFWN0g3VjVNNywxM1YxMUgyMVYxM0g3TTQsNC41QTEuNSwxLjUgMCAwLDEgNS41LDZBMS41LDEuNSAwIDAsMSA0LDcuNUExLjUsMS41IDAgMCwxIDIuNSw2QTEuNSwxLjUgMCAwLDEgNCw0LjVNNCwxMC41QTEuNSwxLjUgMCAwLDEgNS41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMy41QTEuNSwxLjUgMCAwLDEgMi41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMC41TTcsMTlWMTdIMjFWMTlIN000LDE2LjVBMS41LDEuNSAwIDAsMSA1LjUsMThBMS41LDEuNSAwIDAsMSA0LDE5LjVBMS41LDEuNSAwIDAsMSAyLjUsMThBMS41LDEuNSAwIDAsMSA0LDE2LjVaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tree-view: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMiAxMVYzaC03djNIOVYzSDJ2OGg3VjhoMnYxMGg0djNoN3YtOGgtN3YzaC0yVjhoMnYzeiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K);
  --jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-user: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE2IDdhNCA0IDAgMTEtOCAwIDQgNCAwIDAxOCAwek0xMiAxNGE3IDcgMCAwMC03IDdoMTRhNyA3IDAgMDAtNy03eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-users: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZlcnNpb249IjEuMSIgdmlld0JveD0iMCAwIDM2IDI0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogPGcgY2xhc3M9ImpwLWljb24zIiB0cmFuc2Zvcm09Im1hdHJpeCgxLjczMjcgMCAwIDEuNzMyNyAtMy42MjgyIC4wOTk1NzcpIiBmaWxsPSIjNjE2MTYxIj4KICA8cGF0aCB0cmFuc2Zvcm09Im1hdHJpeCgxLjUsMCwwLDEuNSwwLC02KSIgZD0ibTEyLjE4NiA3LjUwOThjLTEuMDUzNSAwLTEuOTc1NyAwLjU2NjUtMi40Nzg1IDEuNDEwMiAwLjc1MDYxIDAuMzEyNzcgMS4zOTc0IDAuODI2NDggMS44NzMgMS40NzI3aDMuNDg2M2MwLTEuNTkyLTEuMjg4OS0yLjg4MjgtMi44ODA5LTIuODgyOHoiLz4KICA8cGF0aCBkPSJtMjAuNDY1IDIuMzg5NWEyLjE4ODUgMi4xODg1IDAgMCAxLTIuMTg4NCAyLjE4ODUgMi4xODg1IDIuMTg4NSAwIDAgMS0yLjE4ODUtMi4xODg1IDIuMTg4NSAyLjE4ODUgMCAwIDEgMi4xODg1LTIuMTg4NSAyLjE4ODUgMi4xODg1IDAgMCAxIDIuMTg4NCAyLjE4ODV6Ii8+CiAgPHBhdGggdHJhbnNmb3JtPSJtYXRyaXgoMS41LDAsMCwxLjUsMCwtNikiIGQ9Im0zLjU4OTggOC40MjE5Yy0xLjExMjYgMC0yLjAxMzcgMC45MDExMS0yLjAxMzcgMi4wMTM3aDIuODE0NWMwLjI2Nzk3LTAuMzczMDkgMC41OTA3LTAuNzA0MzUgMC45NTg5OC0wLjk3ODUyLTAuMzQ0MzMtMC42MTY4OC0xLjAwMzEtMS4wMzUyLTEuNzU5OC0xLjAzNTJ6Ii8+CiAgPHBhdGggZD0ibTYuOTE1NCA0LjYyM2ExLjUyOTQgMS41Mjk0IDAgMCAxLTEuNTI5NCAxLjUyOTQgMS41Mjk0IDEuNTI5NCAwIDAgMS0xLjUyOTQtMS41Mjk0IDEuNTI5NCAxLjUyOTQgMCAwIDEgMS41Mjk0LTEuNTI5NCAxLjUyOTQgMS41Mjk0IDAgMCAxIDEuNTI5NCAxLjUyOTR6Ii8+CiAgPHBhdGggZD0ibTYuMTM1IDEzLjUzNWMwLTMuMjM5MiAyLjYyNTktNS44NjUgNS44NjUtNS44NjUgMy4yMzkyIDAgNS44NjUgMi42MjU5IDUuODY1IDUuODY1eiIvPgogIDxjaXJjbGUgY3g9IjEyIiBjeT0iMy43Njg1IiByPSIyLjk2ODUiLz4KIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-word: url(data:image/svg+xml;base64,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);
  --jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K);
}

/* Icon CSS class declarations */

.jp-AddAboveIcon {
  background-image: var(--jp-icon-add-above);
}

.jp-AddBelowIcon {
  background-image: var(--jp-icon-add-below);
}

.jp-AddIcon {
  background-image: var(--jp-icon-add);
}

.jp-BellIcon {
  background-image: var(--jp-icon-bell);
}

.jp-BugDotIcon {
  background-image: var(--jp-icon-bug-dot);
}

.jp-BugIcon {
  background-image: var(--jp-icon-bug);
}

.jp-BuildIcon {
  background-image: var(--jp-icon-build);
}

.jp-CaretDownEmptyIcon {
  background-image: var(--jp-icon-caret-down-empty);
}

.jp-CaretDownEmptyThinIcon {
  background-image: var(--jp-icon-caret-down-empty-thin);
}

.jp-CaretDownIcon {
  background-image: var(--jp-icon-caret-down);
}

.jp-CaretLeftIcon {
  background-image: var(--jp-icon-caret-left);
}

.jp-CaretRightIcon {
  background-image: var(--jp-icon-caret-right);
}

.jp-CaretUpEmptyThinIcon {
  background-image: var(--jp-icon-caret-up-empty-thin);
}

.jp-CaretUpIcon {
  background-image: var(--jp-icon-caret-up);
}

.jp-CaseSensitiveIcon {
  background-image: var(--jp-icon-case-sensitive);
}

.jp-CheckIcon {
  background-image: var(--jp-icon-check);
}

.jp-CircleEmptyIcon {
  background-image: var(--jp-icon-circle-empty);
}

.jp-CircleIcon {
  background-image: var(--jp-icon-circle);
}

.jp-ClearIcon {
  background-image: var(--jp-icon-clear);
}

.jp-CloseIcon {
  background-image: var(--jp-icon-close);
}

.jp-CodeCheckIcon {
  background-image: var(--jp-icon-code-check);
}

.jp-CodeIcon {
  background-image: var(--jp-icon-code);
}

.jp-CollapseAllIcon {
  background-image: var(--jp-icon-collapse-all);
}

.jp-ConsoleIcon {
  background-image: var(--jp-icon-console);
}

.jp-CopyIcon {
  background-image: var(--jp-icon-copy);
}

.jp-CopyrightIcon {
  background-image: var(--jp-icon-copyright);
}

.jp-CutIcon {
  background-image: var(--jp-icon-cut);
}

.jp-DeleteIcon {
  background-image: var(--jp-icon-delete);
}

.jp-DownloadIcon {
  background-image: var(--jp-icon-download);
}

.jp-DuplicateIcon {
  background-image: var(--jp-icon-duplicate);
}

.jp-EditIcon {
  background-image: var(--jp-icon-edit);
}

.jp-EllipsesIcon {
  background-image: var(--jp-icon-ellipses);
}

.jp-ErrorIcon {
  background-image: var(--jp-icon-error);
}

.jp-ExpandAllIcon {
  background-image: var(--jp-icon-expand-all);
}

.jp-ExtensionIcon {
  background-image: var(--jp-icon-extension);
}

.jp-FastForwardIcon {
  background-image: var(--jp-icon-fast-forward);
}

.jp-FileIcon {
  background-image: var(--jp-icon-file);
}

.jp-FileUploadIcon {
  background-image: var(--jp-icon-file-upload);
}

.jp-FilterDotIcon {
  background-image: var(--jp-icon-filter-dot);
}

.jp-FilterIcon {
  background-image: var(--jp-icon-filter);
}

.jp-FilterListIcon {
  background-image: var(--jp-icon-filter-list);
}

.jp-FolderFavoriteIcon {
  background-image: var(--jp-icon-folder-favorite);
}

.jp-FolderIcon {
  background-image: var(--jp-icon-folder);
}

.jp-HomeIcon {
  background-image: var(--jp-icon-home);
}

.jp-Html5Icon {
  background-image: var(--jp-icon-html5);
}

.jp-ImageIcon {
  background-image: var(--jp-icon-image);
}

.jp-InfoIcon {
  background-image: var(--jp-icon-info);
}

.jp-InspectorIcon {
  background-image: var(--jp-icon-inspector);
}

.jp-JsonIcon {
  background-image: var(--jp-icon-json);
}

.jp-JuliaIcon {
  background-image: var(--jp-icon-julia);
}

.jp-JupyterFaviconIcon {
  background-image: var(--jp-icon-jupyter-favicon);
}

.jp-JupyterIcon {
  background-image: var(--jp-icon-jupyter);
}

.jp-JupyterlabWordmarkIcon {
  background-image: var(--jp-icon-jupyterlab-wordmark);
}

.jp-KernelIcon {
  background-image: var(--jp-icon-kernel);
}

.jp-KeyboardIcon {
  background-image: var(--jp-icon-keyboard);
}

.jp-LaunchIcon {
  background-image: var(--jp-icon-launch);
}

.jp-LauncherIcon {
  background-image: var(--jp-icon-launcher);
}

.jp-LineFormIcon {
  background-image: var(--jp-icon-line-form);
}

.jp-LinkIcon {
  background-image: var(--jp-icon-link);
}

.jp-ListIcon {
  background-image: var(--jp-icon-list);
}

.jp-MarkdownIcon {
  background-image: var(--jp-icon-markdown);
}

.jp-MoveDownIcon {
  background-image: var(--jp-icon-move-down);
}

.jp-MoveUpIcon {
  background-image: var(--jp-icon-move-up);
}

.jp-NewFolderIcon {
  background-image: var(--jp-icon-new-folder);
}

.jp-NotTrustedIcon {
  background-image: var(--jp-icon-not-trusted);
}

.jp-NotebookIcon {
  background-image: var(--jp-icon-notebook);
}

.jp-NumberingIcon {
  background-image: var(--jp-icon-numbering);
}

.jp-OfflineBoltIcon {
  background-image: var(--jp-icon-offline-bolt);
}

.jp-PaletteIcon {
  background-image: var(--jp-icon-palette);
}

.jp-PasteIcon {
  background-image: var(--jp-icon-paste);
}

.jp-PdfIcon {
  background-image: var(--jp-icon-pdf);
}

.jp-PythonIcon {
  background-image: var(--jp-icon-python);
}

.jp-RKernelIcon {
  background-image: var(--jp-icon-r-kernel);
}

.jp-ReactIcon {
  background-image: var(--jp-icon-react);
}

.jp-RedoIcon {
  background-image: var(--jp-icon-redo);
}

.jp-RefreshIcon {
  background-image: var(--jp-icon-refresh);
}

.jp-RegexIcon {
  background-image: var(--jp-icon-regex);
}

.jp-RunIcon {
  background-image: var(--jp-icon-run);
}

.jp-RunningIcon {
  background-image: var(--jp-icon-running);
}

.jp-SaveIcon {
  background-image: var(--jp-icon-save);
}

.jp-SearchIcon {
  background-image: var(--jp-icon-search);
}

.jp-SettingsIcon {
  background-image: var(--jp-icon-settings);
}

.jp-ShareIcon {
  background-image: var(--jp-icon-share);
}

.jp-SpreadsheetIcon {
  background-image: var(--jp-icon-spreadsheet);
}

.jp-StopIcon {
  background-image: var(--jp-icon-stop);
}

.jp-TabIcon {
  background-image: var(--jp-icon-tab);
}

.jp-TableRowsIcon {
  background-image: var(--jp-icon-table-rows);
}

.jp-TagIcon {
  background-image: var(--jp-icon-tag);
}

.jp-TerminalIcon {
  background-image: var(--jp-icon-terminal);
}

.jp-TextEditorIcon {
  background-image: var(--jp-icon-text-editor);
}

.jp-TocIcon {
  background-image: var(--jp-icon-toc);
}

.jp-TreeViewIcon {
  background-image: var(--jp-icon-tree-view);
}

.jp-TrustedIcon {
  background-image: var(--jp-icon-trusted);
}

.jp-UndoIcon {
  background-image: var(--jp-icon-undo);
}

.jp-UserIcon {
  background-image: var(--jp-icon-user);
}

.jp-UsersIcon {
  background-image: var(--jp-icon-users);
}

.jp-VegaIcon {
  background-image: var(--jp-icon-vega);
}

.jp-WordIcon {
  background-image: var(--jp-icon-word);
}

.jp-YamlIcon {
  background-image: var(--jp-icon-yaml);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

.jp-Icon,
.jp-MaterialIcon {
  background-position: center;
  background-repeat: no-repeat;
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-cover {
  background-position: center;
  background-repeat: no-repeat;
  background-size: cover;
}

/**
 * (DEPRECATED) Support for specific CSS icon sizes
 */

.jp-Icon-16 {
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-18 {
  background-size: 18px;
  min-width: 18px;
  min-height: 18px;
}

.jp-Icon-20 {
  background-size: 20px;
  min-width: 20px;
  min-height: 20px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.lm-TabBar .lm-TabBar-addButton {
  align-items: center;
  display: flex;
  padding: 4px;
  padding-bottom: 5px;
  margin-right: 1px;
  background-color: var(--jp-layout-color2);
}

.lm-TabBar .lm-TabBar-addButton:hover {
  background-color: var(--jp-layout-color1);
}

.lm-DockPanel-tabBar .lm-TabBar-tab {
  width: var(--jp-private-horizontal-tab-width);
}

.lm-DockPanel-tabBar .lm-TabBar-content {
  flex: unset;
}

.lm-DockPanel-tabBar[data-orientation='horizontal'] {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for icons as inline SVG HTMLElements
 */

/* recolor the primary elements of an icon */
.jp-icon0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-accent0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-accent1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-accent2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-accent3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-accent4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-accent0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-accent1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-accent2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-accent3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-accent4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-none[fill] {
  fill: none;
}

.jp-icon-none[stroke] {
  stroke: none;
}

/* brand icon colors. Same for light and dark */
.jp-icon-brand0[fill] {
  fill: var(--jp-brand-color0);
}

.jp-icon-brand1[fill] {
  fill: var(--jp-brand-color1);
}

.jp-icon-brand2[fill] {
  fill: var(--jp-brand-color2);
}

.jp-icon-brand3[fill] {
  fill: var(--jp-brand-color3);
}

.jp-icon-brand4[fill] {
  fill: var(--jp-brand-color4);
}

.jp-icon-brand0[stroke] {
  stroke: var(--jp-brand-color0);
}

.jp-icon-brand1[stroke] {
  stroke: var(--jp-brand-color1);
}

.jp-icon-brand2[stroke] {
  stroke: var(--jp-brand-color2);
}

.jp-icon-brand3[stroke] {
  stroke: var(--jp-brand-color3);
}

.jp-icon-brand4[stroke] {
  stroke: var(--jp-brand-color4);
}

/* warn icon colors. Same for light and dark */
.jp-icon-warn0[fill] {
  fill: var(--jp-warn-color0);
}

.jp-icon-warn1[fill] {
  fill: var(--jp-warn-color1);
}

.jp-icon-warn2[fill] {
  fill: var(--jp-warn-color2);
}

.jp-icon-warn3[fill] {
  fill: var(--jp-warn-color3);
}

.jp-icon-warn0[stroke] {
  stroke: var(--jp-warn-color0);
}

.jp-icon-warn1[stroke] {
  stroke: var(--jp-warn-color1);
}

.jp-icon-warn2[stroke] {
  stroke: var(--jp-warn-color2);
}

.jp-icon-warn3[stroke] {
  stroke: var(--jp-warn-color3);
}

/* icon colors that contrast well with each other and most backgrounds */
.jp-icon-contrast0[fill] {
  fill: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[fill] {
  fill: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[fill] {
  fill: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[fill] {
  fill: var(--jp-icon-contrast-color3);
}

.jp-icon-contrast0[stroke] {
  stroke: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[stroke] {
  stroke: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[stroke] {
  stroke: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[stroke] {
  stroke: var(--jp-icon-contrast-color3);
}

.jp-icon-dot[fill] {
  fill: var(--jp-warn-color0);
}

.jp-jupyter-icon-color[fill] {
  fill: var(--jp-jupyter-icon-color, var(--jp-warn-color0));
}

.jp-notebook-icon-color[fill] {
  fill: var(--jp-notebook-icon-color, var(--jp-warn-color0));
}

.jp-json-icon-color[fill] {
  fill: var(--jp-json-icon-color, var(--jp-warn-color1));
}

.jp-console-icon-color[fill] {
  fill: var(--jp-console-icon-color, white);
}

.jp-console-icon-background-color[fill] {
  fill: var(--jp-console-icon-background-color, var(--jp-brand-color1));
}

.jp-terminal-icon-color[fill] {
  fill: var(--jp-terminal-icon-color, var(--jp-layout-color2));
}

.jp-terminal-icon-background-color[fill] {
  fill: var(
    --jp-terminal-icon-background-color,
    var(--jp-inverse-layout-color2)
  );
}

.jp-text-editor-icon-color[fill] {
  fill: var(--jp-text-editor-icon-color, var(--jp-inverse-layout-color3));
}

.jp-inspector-icon-color[fill] {
  fill: var(--jp-inspector-icon-color, var(--jp-inverse-layout-color3));
}

/* CSS for icons in selected filebrowser listing items */
.jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

.jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* stylelint-disable selector-max-class, selector-max-compound-selectors */

/**
* TODO: come up with non css-hack solution for showing the busy icon on top
*  of the close icon
* CSS for complex behavior of close icon of tabs in the main area tabbar
*/
.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon3[fill] {
  fill: none;
}

.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: var(--jp-inverse-layout-color3);
}

/* stylelint-enable selector-max-class, selector-max-compound-selectors */

/* CSS for icons in status bar */
#jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

#jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* special handling for splash icon CSS. While the theme CSS reloads during
   splash, the splash icon can loose theming. To prevent that, we set a
   default for its color variable */
:root {
  --jp-warn-color0: var(--md-orange-700);
}

/* not sure what to do with this one, used in filebrowser listing */
.jp-DragIcon {
  margin-right: 4px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for alt colors for icons as inline SVG HTMLElements
 */

/* alt recolor the primary elements of an icon */
.jp-icon-alt .jp-icon0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-alt .jp-icon0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* alt recolor the accent elements of an icon */
.jp-icon-alt .jp-icon-accent0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-alt .jp-icon-accent0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-icon-hoverShow:not(:hover) .jp-icon-hoverShow-content {
  display: none !important;
}

/**
 * Support for hover colors for icons as inline SVG HTMLElements
 */

/**
 * regular colors
 */

/* recolor the primary elements of an icon */
.jp-icon-hover :hover .jp-icon0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-hover :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-hover :hover .jp-icon-none-hover[fill] {
  fill: none;
}

.jp-icon-hover :hover .jp-icon-none-hover[stroke] {
  stroke: none;
}

/**
 * inverse colors
 */

/* inverse recolor the primary elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* inverse recolor the accent elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-IFrame {
  width: 100%;
  height: 100%;
}

.jp-IFrame > iframe {
  border: none;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-IFrame {
  position: relative;
}

body.lm-mod-override-cursor .jp-IFrame::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-HoverBox {
  position: fixed;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FormGroup-content fieldset {
  border: none;
  padding: 0;
  min-width: 0;
  width: 100%;
}

/* stylelint-disable selector-max-type */

.jp-FormGroup-content fieldset .jp-inputFieldWrapper input,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper select,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper textarea {
  font-size: var(--jp-content-font-size2);
  border-color: var(--jp-input-border-color);
  border-style: solid;
  border-radius: var(--jp-border-radius);
  border-width: 1px;
  padding: 6px 8px;
  background: none;
  color: var(--jp-ui-font-color0);
  height: inherit;
}

.jp-FormGroup-content fieldset input[type='checkbox'] {
  position: relative;
  top: 2px;
  margin-left: 0;
}

.jp-FormGroup-content button.jp-mod-styled {
  cursor: pointer;
}

.jp-FormGroup-content .checkbox label {
  cursor: pointer;
  font-size: var(--jp-content-font-size1);
}

.jp-FormGroup-content .jp-root > fieldset > legend {
  display: none;
}

.jp-FormGroup-content .jp-root > fieldset > p {
  display: none;
}

/** copy of `input.jp-mod-styled:focus` style */
.jp-FormGroup-content fieldset input:focus,
.jp-FormGroup-content fieldset select:focus {
  -moz-outline-radius: unset;
  outline: var(--jp-border-width) solid var(--md-blue-500);
  outline-offset: -1px;
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-FormGroup-content fieldset input:hover:not(:focus),
.jp-FormGroup-content fieldset select:hover:not(:focus) {
  background-color: var(--jp-border-color2);
}

/* stylelint-enable selector-max-type */

.jp-FormGroup-content .checkbox .field-description {
  /* Disable default description field for checkbox:
   because other widgets do not have description fields,
   we add descriptions to each widget on the field level.
  */
  display: none;
}

.jp-FormGroup-content #root__description {
  display: none;
}

.jp-FormGroup-content .jp-modifiedIndicator {
  width: 5px;
  background-color: var(--jp-brand-color2);
  margin-top: 0;
  margin-left: calc(var(--jp-private-settingeditor-modifier-indent) * -1);
  flex-shrink: 0;
}

.jp-FormGroup-content .jp-modifiedIndicator.jp-errorIndicator {
  background-color: var(--jp-error-color0);
  margin-right: 0.5em;
}

/* RJSF ARRAY style */

.jp-arrayFieldWrapper legend {
  font-size: var(--jp-content-font-size2);
  color: var(--jp-ui-font-color0);
  flex-basis: 100%;
  padding: 4px 0;
  font-weight: var(--jp-content-heading-font-weight);
  border-bottom: 1px solid var(--jp-border-color2);
}

.jp-arrayFieldWrapper .field-description {
  padding: 4px 0;
  white-space: pre-wrap;
}

.jp-arrayFieldWrapper .array-item {
  width: 100%;
  border: 1px solid var(--jp-border-color2);
  border-radius: 4px;
  margin: 4px;
}

.jp-ArrayOperations {
  display: flex;
  margin-left: 8px;
}

.jp-ArrayOperationsButton {
  margin: 2px;
}

.jp-ArrayOperationsButton .jp-icon3[fill] {
  fill: var(--jp-ui-font-color0);
}

button.jp-ArrayOperationsButton.jp-mod-styled:disabled {
  cursor: not-allowed;
  opacity: 0.5;
}

/* RJSF form validation error */

.jp-FormGroup-content .validationErrors {
  color: var(--jp-error-color0);
}

/* Hide panel level error as duplicated the field level error */
.jp-FormGroup-content .panel.errors {
  display: none;
}

/* RJSF normal content (settings-editor) */

.jp-FormGroup-contentNormal {
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-FormGroup-contentItem {
  margin-left: 7px;
  color: var(--jp-ui-font-color0);
}

.jp-FormGroup-contentNormal .jp-FormGroup-description {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-default {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-fieldLabel {
  font-size: var(--jp-content-font-size1);
  font-weight: normal;
  min-width: 120px;
}

.jp-FormGroup-contentNormal fieldset:not(:first-child) {
  margin-left: 7px;
}

.jp-FormGroup-contentNormal .field-array-of-string .array-item {
  /* Display `jp-ArrayOperations` buttons side-by-side with content except
    for small screens where flex-wrap will place them one below the other.
  */
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-objectFieldWrapper .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

/* RJSF compact content (metadata-form) */

.jp-FormGroup-content.jp-FormGroup-contentCompact {
  width: 100%;
}

.jp-FormGroup-contentCompact .form-group {
  display: flex;
  padding: 0.5em 0.2em 0.5em 0;
}

.jp-FormGroup-contentCompact
  .jp-FormGroup-compactTitle
  .jp-FormGroup-description {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color2);
}

.jp-FormGroup-contentCompact .jp-FormGroup-fieldLabel {
  padding-bottom: 0.3em;
}

.jp-FormGroup-contentCompact .jp-inputFieldWrapper .form-control {
  width: 100%;
  box-sizing: border-box;
}

.jp-FormGroup-contentCompact .jp-arrayFieldWrapper .jp-FormGroup-compactTitle {
  padding-bottom: 7px;
}

.jp-FormGroup-contentCompact
  .jp-objectFieldWrapper
  .jp-objectFieldWrapper
  .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

.jp-FormGroup-contentCompact ul.error-detail {
  margin-block-start: 0.5em;
  margin-block-end: 0.5em;
  padding-inline-start: 1em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-SidePanel {
  display: flex;
  flex-direction: column;
  min-width: var(--jp-sidebar-min-width);
  overflow-y: auto;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  font-size: var(--jp-ui-font-size1);
}

.jp-SidePanel-header {
  flex: 0 0 auto;
  display: flex;
  border-bottom: var(--jp-border-width) solid var(--jp-border-color2);
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin: 0;
  padding: 2px;
  text-transform: uppercase;
}

.jp-SidePanel-toolbar {
  flex: 0 0 auto;
}

.jp-SidePanel-content {
  flex: 1 1 auto;
}

.jp-SidePanel-toolbar,
.jp-AccordionPanel-toolbar {
  height: var(--jp-private-toolbar-height);
}

.jp-SidePanel-toolbar.jp-Toolbar-micro {
  display: none;
}

.lm-AccordionPanel .jp-AccordionPanel-title {
  box-sizing: border-box;
  line-height: 25px;
  margin: 0;
  display: flex;
  align-items: center;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  font-size: var(--jp-ui-font-size0);
}

.jp-AccordionPanel-title {
  cursor: pointer;
  user-select: none;
  -moz-user-select: none;
  -webkit-user-select: none;
  text-transform: uppercase;
}

.lm-AccordionPanel[data-orientation='horizontal'] > .jp-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleLabel {
  user-select: none;
  text-overflow: ellipsis;
  white-space: nowrap;
  overflow: hidden;
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleCollapser {
  transform: rotate(-90deg);
  margin: auto 0;
  height: 16px;
}

.jp-AccordionPanel-title.lm-mod-expanded .lm-AccordionPanel-titleCollapser {
  transform: rotate(0deg);
}

.lm-AccordionPanel .jp-AccordionPanel-toolbar {
  background: none;
  box-shadow: none;
  border: none;
  margin-left: auto;
}

.lm-AccordionPanel .lm-SplitPanel-handle:hover {
  background: var(--jp-layout-color3);
}

.jp-text-truncated {
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Spinner {
  position: absolute;
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 10;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-layout-color0);
  outline: none;
}

.jp-SpinnerContent {
  font-size: 10px;
  margin: 50px auto;
  text-indent: -9999em;
  width: 3em;
  height: 3em;
  border-radius: 50%;
  background: var(--jp-brand-color3);
  background: linear-gradient(
    to right,
    #f37626 10%,
    rgba(255, 255, 255, 0) 42%
  );
  position: relative;
  animation: load3 1s infinite linear, fadeIn 1s;
}

.jp-SpinnerContent::before {
  width: 50%;
  height: 50%;
  background: #f37626;
  border-radius: 100% 0 0;
  position: absolute;
  top: 0;
  left: 0;
  content: '';
}

.jp-SpinnerContent::after {
  background: var(--jp-layout-color0);
  width: 75%;
  height: 75%;
  border-radius: 50%;
  content: '';
  margin: auto;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
}

@keyframes fadeIn {
  0% {
    opacity: 0;
  }

  100% {
    opacity: 1;
  }
}

@keyframes load3 {
  0% {
    transform: rotate(0deg);
  }

  100% {
    transform: rotate(360deg);
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

button.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: none;
  box-sizing: border-box;
  text-align: center;
  line-height: 32px;
  height: 32px;
  padding: 0 12px;
  letter-spacing: 0.8px;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input.jp-mod-styled {
  background: var(--jp-input-background);
  height: 28px;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color1);
  padding-left: 7px;
  padding-right: 7px;
  font-size: var(--jp-ui-font-size2);
  color: var(--jp-ui-font-color0);
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input[type='checkbox'].jp-mod-styled {
  appearance: checkbox;
  -webkit-appearance: checkbox;
  -moz-appearance: checkbox;
  height: auto;
}

input.jp-mod-styled:focus {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-select-wrapper {
  display: flex;
  position: relative;
  flex-direction: column;
  padding: 1px;
  background-color: var(--jp-layout-color1);
  box-sizing: border-box;
  margin-bottom: 12px;
}

.jp-select-wrapper:not(.multiple) {
  height: 28px;
}

.jp-select-wrapper.jp-mod-focused select.jp-mod-styled {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-input-active-background);
}

select.jp-mod-styled:hover {
  cursor: pointer;
  color: var(--jp-ui-font-color0);
  background-color: var(--jp-input-hover-background);
  box-shadow: inset 0 0 1px rgba(0, 0, 0, 0.5);
}

select.jp-mod-styled {
  flex: 1 1 auto;
  width: 100%;
  font-size: var(--jp-ui-font-size2);
  background: var(--jp-input-background);
  color: var(--jp-ui-font-color0);
  padding: 0 25px 0 8px;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

select.jp-mod-styled:not([multiple]) {
  height: 32px;
}

select.jp-mod-styled[multiple] {
  max-height: 200px;
  overflow-y: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-switch {
  display: flex;
  align-items: center;
  padding-left: 4px;
  padding-right: 4px;
  font-size: var(--jp-ui-font-size1);
  background-color: transparent;
  color: var(--jp-ui-font-color1);
  border: none;
  height: 20px;
}

.jp-switch:hover {
  background-color: var(--jp-layout-color2);
}

.jp-switch-label {
  margin-right: 5px;
  font-family: var(--jp-ui-font-family);
}

.jp-switch-track {
  cursor: pointer;
  background-color: var(--jp-switch-color, var(--jp-border-color1));
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 34px;
  height: 16px;
  width: 35px;
  position: relative;
}

.jp-switch-track::before {
  content: '';
  position: absolute;
  height: 10px;
  width: 10px;
  margin: 3px;
  left: 0;
  background-color: var(--jp-ui-inverse-font-color1);
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 50%;
}

.jp-switch[aria-checked='true'] .jp-switch-track {
  background-color: var(--jp-switch-true-position-color, var(--jp-warn-color0));
}

.jp-switch[aria-checked='true'] .jp-switch-track::before {
  /* track width (35) - margins (3 + 3) - thumb width (10) */
  left: 19px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toolbar-height: calc(
    28px + var(--jp-border-width)
  ); /* leave 28px for content */
}

.jp-Toolbar {
  color: var(--jp-ui-font-color1);
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: 2px;
  z-index: 8;
  overflow-x: hidden;
}

/* Toolbar items */

.jp-Toolbar > .jp-Toolbar-item.jp-Toolbar-spacer {
  flex-grow: 1;
  flex-shrink: 1;
}

.jp-Toolbar-item.jp-Toolbar-kernelStatus {
  display: inline-block;
  width: 32px;
  background-repeat: no-repeat;
  background-position: center;
  background-size: 16px;
}

.jp-Toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  display: flex;
  padding-left: 1px;
  padding-right: 1px;
  font-size: var(--jp-ui-font-size1);
  line-height: var(--jp-private-toolbar-height);
  height: 100%;
}

/* Toolbar buttons */

/* This is the div we use to wrap the react component into a Widget */
div.jp-ToolbarButton {
  color: transparent;
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0;
  margin: 0;
}

button.jp-ToolbarButtonComponent {
  background: var(--jp-layout-color1);
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0 6px;
  margin: 0;
  height: 24px;
  border-radius: var(--jp-border-radius);
  display: flex;
  align-items: center;
  text-align: center;
  font-size: 14px;
  min-width: unset;
  min-height: unset;
}

button.jp-ToolbarButtonComponent:disabled {
  opacity: 0.4;
}

button.jp-ToolbarButtonComponent > span {
  padding: 0;
  flex: 0 0 auto;
}

button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label {
  font-size: var(--jp-ui-font-size1);
  line-height: 100%;
  padding-left: 2px;
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar.jp-Toolbar-micro {
  padding: 0;
  min-height: 0;
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar {
  border: none;
  box-shadow: none;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-WindowedPanel-outer {
  position: relative;
  overflow-y: auto;
}

.jp-WindowedPanel-inner {
  position: relative;
}

.jp-WindowedPanel-window {
  position: absolute;
  left: 0;
  right: 0;
  overflow: visible;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* Sibling imports */

body {
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
}

/* Disable native link decoration styles everywhere outside of dialog boxes */
a {
  text-decoration: unset;
  color: unset;
}

a:hover {
  text-decoration: unset;
  color: unset;
}

/* Accessibility for links inside dialog box text */
.jp-Dialog-content a {
  text-decoration: revert;
  color: var(--jp-content-link-color);
}

.jp-Dialog-content a:hover {
  text-decoration: revert;
}

/* Styles for ui-components */
.jp-Button {
  color: var(--jp-ui-font-color2);
  border-radius: var(--jp-border-radius);
  padding: 0 12px;
  font-size: var(--jp-ui-font-size1);

  /* Copy from blueprint 3 */
  display: inline-flex;
  flex-direction: row;
  border: none;
  cursor: pointer;
  align-items: center;
  justify-content: center;
  text-align: left;
  vertical-align: middle;
  min-height: 30px;
  min-width: 30px;
}

.jp-Button:disabled {
  cursor: not-allowed;
}

.jp-Button:empty {
  padding: 0 !important;
}

.jp-Button.jp-mod-small {
  min-height: 24px;
  min-width: 24px;
  font-size: 12px;
  padding: 0 7px;
}

/* Use our own theme for hover styles */
.jp-Button.jp-mod-minimal:hover {
  background-color: var(--jp-layout-color2);
}

.jp-Button.jp-mod-minimal {
  background: none;
}

.jp-InputGroup {
  display: block;
  position: relative;
}

.jp-InputGroup input {
  box-sizing: border-box;
  border: none;
  border-radius: 0;
  background-color: transparent;
  color: var(--jp-ui-font-color0);
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
  padding-bottom: 0;
  padding-top: 0;
  padding-left: 10px;
  padding-right: 28px;
  position: relative;
  width: 100%;
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  font-size: 14px;
  font-weight: 400;
  height: 30px;
  line-height: 30px;
  outline: none;
  vertical-align: middle;
}

.jp-InputGroup input:focus {
  box-shadow: inset 0 0 0 var(--jp-border-width)
      var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-InputGroup input:disabled {
  cursor: not-allowed;
  resize: block;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input:disabled ~ span {
  cursor: not-allowed;
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input::placeholder,
input::placeholder {
  color: var(--jp-ui-font-color2);
}

.jp-InputGroupAction {
  position: absolute;
  bottom: 1px;
  right: 0;
  padding: 6px;
}

.jp-HTMLSelect.jp-DefaultStyle select {
  background-color: initial;
  border: none;
  border-radius: 0;
  box-shadow: none;
  color: var(--jp-ui-font-color0);
  display: block;
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  height: 24px;
  line-height: 14px;
  padding: 0 25px 0 10px;
  text-align: left;
  -moz-appearance: none;
  -webkit-appearance: none;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
  cursor: not-allowed;
  resize: block;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled ~ span {
  cursor: not-allowed;
}

/* Use our own theme for hover and option styles */
/* stylelint-disable-next-line selector-max-type */
.jp-HTMLSelect.jp-DefaultStyle select:hover,
.jp-HTMLSelect.jp-DefaultStyle select > option {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color0);
}

select {
  box-sizing: border-box;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-StatusBar-Widget {
  display: flex;
  align-items: center;
  background: var(--jp-layout-color2);
  min-height: var(--jp-statusbar-height);
  justify-content: space-between;
  padding: 0 10px;
}

.jp-StatusBar-Left {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-StatusBar-Middle {
  display: flex;
  align-items: center;
}

.jp-StatusBar-Right {
  display: flex;
  align-items: center;
  flex-direction: row-reverse;
}

.jp-StatusBar-Item {
  max-height: var(--jp-statusbar-height);
  margin: 0 2px;
  height: var(--jp-statusbar-height);
  white-space: nowrap;
  text-overflow: ellipsis;
  color: var(--jp-ui-font-color1);
  padding: 0 6px;
}

.jp-mod-highlighted:hover {
  background-color: var(--jp-layout-color3);
}

.jp-mod-clicked {
  background-color: var(--jp-brand-color1);
}

.jp-mod-clicked:hover {
  background-color: var(--jp-brand-color0);
}

.jp-mod-clicked .jp-StatusBar-TextItem {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-StatusBar-HoverItem {
  box-shadow: '0px 4px 4px rgba(0, 0, 0, 0.25)';
}

.jp-StatusBar-TextItem {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  line-height: 24px;
  color: var(--jp-ui-font-color1);
}

.jp-StatusBar-GroupItem {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-Statusbar-ProgressCircle svg {
  display: block;
  margin: 0 auto;
  width: 16px;
  height: 24px;
  align-self: normal;
}

.jp-Statusbar-ProgressCircle path {
  fill: var(--jp-inverse-layout-color3);
}

.jp-Statusbar-ProgressBar-progress-bar {
  height: 10px;
  width: 100px;
  border: solid 0.25px var(--jp-brand-color2);
  border-radius: 3px;
  overflow: hidden;
  align-self: center;
}

.jp-Statusbar-ProgressBar-progress-bar > div {
  background-color: var(--jp-brand-color2);
  background-image: linear-gradient(
    -45deg,
    rgba(255, 255, 255, 0.2) 25%,
    transparent 25%,
    transparent 50%,
    rgba(255, 255, 255, 0.2) 50%,
    rgba(255, 255, 255, 0.2) 75%,
    transparent 75%,
    transparent
  );
  background-size: 40px 40px;
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 14px;
  color: #fff;
  text-align: center;
  animation: jp-Statusbar-ExecutionTime-progress-bar 2s linear infinite;
}

.jp-Statusbar-ProgressBar-progress-bar p {
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
  font-size: var(--jp-ui-font-size1);
  line-height: 10px;
  width: 100px;
}

@keyframes jp-Statusbar-ExecutionTime-progress-bar {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 40px 40px;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-commandpalette-search-height: 28px;
}

/*-----------------------------------------------------------------------------
| Overall styles
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  padding-bottom: 0;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Modal variant
|----------------------------------------------------------------------------*/

.jp-ModalCommandPalette {
  position: absolute;
  z-index: 10000;
  top: 38px;
  left: 30%;
  margin: 0;
  padding: 4px;
  width: 40%;
  box-shadow: var(--jp-elevation-z4);
  border-radius: 4px;
  background: var(--jp-layout-color0);
}

.jp-ModalCommandPalette .lm-CommandPalette {
  max-height: 40vh;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-close-icon::after {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-header {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-item {
  margin-left: 4px;
  margin-right: 4px;
}

.jp-ModalCommandPalette
  .lm-CommandPalette
  .lm-CommandPalette-item.lm-mod-disabled {
  display: none;
}

/*-----------------------------------------------------------------------------
| Search
|----------------------------------------------------------------------------*/

.lm-CommandPalette-search {
  padding: 4px;
  background-color: var(--jp-layout-color1);
  z-index: 2;
}

.lm-CommandPalette-wrapper {
  overflow: overlay;
  padding: 0 9px;
  background-color: var(--jp-input-active-background);
  height: 30px;
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
}

.lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper {
  box-shadow: inset 0 0 0 1px var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-SearchIconGroup {
  color: white;
  background-color: var(--jp-brand-color1);
  position: absolute;
  top: 4px;
  right: 4px;
  padding: 5px 5px 1px;
}

.jp-SearchIconGroup svg {
  height: 20px;
  width: 20px;
}

.jp-SearchIconGroup .jp-icon3[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-input {
  background: transparent;
  width: calc(100% - 18px);
  float: left;
  border: none;
  outline: none;
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  line-height: var(--jp-private-commandpalette-search-height);
}

.lm-CommandPalette-input::-webkit-input-placeholder,
.lm-CommandPalette-input::-moz-placeholder,
.lm-CommandPalette-input:-ms-input-placeholder {
  color: var(--jp-ui-font-color2);
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Results
|----------------------------------------------------------------------------*/

.lm-CommandPalette-header:first-child {
  margin-top: 0;
}

.lm-CommandPalette-header {
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin-top: 8px;
  padding: 8px 0 8px 12px;
  text-transform: uppercase;
}

.lm-CommandPalette-header.lm-mod-active {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-header > mark {
  background-color: transparent;
  font-weight: bold;
  color: var(--jp-ui-font-color1);
}

.lm-CommandPalette-item {
  padding: 4px 12px 4px 4px;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  font-weight: 400;
  display: flex;
}

.lm-CommandPalette-item.lm-mod-disabled {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item.lm-mod-active {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item.lm-mod-active .lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-inverse-font-color0);
}

.lm-CommandPalette-item.lm-mod-active .jp-icon-selectable[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled) {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled) {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-itemContent {
  overflow: hidden;
}

.lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.lm-CommandPalette-item.lm-mod-disabled mark {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item .lm-CommandPalette-itemIcon {
  margin: 0 4px 0 0;
  position: relative;
  width: 16px;
  top: 2px;
  flex: 0 0 auto;
}

.lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon {
  opacity: 0.6;
}

.lm-CommandPalette-item .lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemCaption {
  display: none;
}

.lm-CommandPalette-content {
  background-color: var(--jp-layout-color1);
}

.lm-CommandPalette-content:empty::after {
  content: 'No results';
  margin: auto;
  margin-top: 20px;
  width: 100px;
  display: block;
  font-size: var(--jp-ui-font-size2);
  font-family: var(--jp-ui-font-family);
  font-weight: lighter;
}

.lm-CommandPalette-emptyMessage {
  text-align: center;
  margin-top: 24px;
  line-height: 1.32;
  padding: 0 8px;
  color: var(--jp-content-font-color3);
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Dialog {
  position: absolute;
  z-index: 10000;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  top: 0;
  left: 0;
  margin: 0;
  padding: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-dialog-background);
}

.jp-Dialog-content {
  display: flex;
  flex-direction: column;
  margin-left: auto;
  margin-right: auto;
  background: var(--jp-layout-color1);
  padding: 24px 24px 12px;
  min-width: 300px;
  min-height: 150px;
  max-width: 1000px;
  max-height: 500px;
  box-sizing: border-box;
  box-shadow: var(--jp-elevation-z20);
  word-wrap: break-word;
  border-radius: var(--jp-border-radius);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color1);
  resize: both;
}

.jp-Dialog-content.jp-Dialog-content-small {
  max-width: 500px;
}

.jp-Dialog-button {
  overflow: visible;
}

button.jp-Dialog-button:focus {
  outline: 1px solid var(--jp-brand-color1);
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button:focus::-moz-focus-inner {
  border: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus {
  outline: 1px solid var(--jp-accept-color-normal, var(--jp-brand-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus {
  outline: 1px solid var(--jp-warn-color-normal, var(--jp-error-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline: 1px solid var(--jp-reject-color-normal, var(--md-grey-600));
}

button.jp-Dialog-close-button {
  padding: 0;
  height: 100%;
  min-width: unset;
  min-height: unset;
}

.jp-Dialog-header {
  display: flex;
  justify-content: space-between;
  flex: 0 0 auto;
  padding-bottom: 12px;
  font-size: var(--jp-ui-font-size3);
  font-weight: 400;
  color: var(--jp-ui-font-color1);
}

.jp-Dialog-body {
  display: flex;
  flex-direction: column;
  flex: 1 1 auto;
  font-size: var(--jp-ui-font-size1);
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

.jp-Dialog-footer {
  display: flex;
  flex-direction: row;
  justify-content: flex-end;
  align-items: center;
  flex: 0 0 auto;
  margin-left: -12px;
  margin-right: -12px;
  padding: 12px;
}

.jp-Dialog-checkbox {
  padding-right: 5px;
}

.jp-Dialog-checkbox > input:focus-visible {
  outline: 1px solid var(--jp-input-active-border-color);
  outline-offset: 1px;
}

.jp-Dialog-spacer {
  flex: 1 1 auto;
}

.jp-Dialog-title {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.jp-Dialog-body > .jp-select-wrapper {
  width: 100%;
}

.jp-Dialog-body > button {
  padding: 0 16px;
}

.jp-Dialog-body > label {
  line-height: 1.4;
  color: var(--jp-ui-font-color0);
}

.jp-Dialog-button.jp-mod-styled:not(:last-child) {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Input-Boolean-Dialog {
  flex-direction: row-reverse;
  align-items: end;
  width: 100%;
}

.jp-Input-Boolean-Dialog > label {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MainAreaWidget > :focus {
  outline: none;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error {
  padding: 6px;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error > pre {
  width: auto;
  padding: 10px;
  background: var(--jp-error-color3);
  border: var(--jp-border-width) solid var(--jp-error-color1);
  border-radius: var(--jp-border-radius);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  white-space: pre-wrap;
  word-wrap: break-word;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/**
 * google-material-color v1.2.6
 * https://github.com/danlevan/google-material-color
 */
:root {
  --md-red-50: #ffebee;
  --md-red-100: #ffcdd2;
  --md-red-200: #ef9a9a;
  --md-red-300: #e57373;
  --md-red-400: #ef5350;
  --md-red-500: #f44336;
  --md-red-600: #e53935;
  --md-red-700: #d32f2f;
  --md-red-800: #c62828;
  --md-red-900: #b71c1c;
  --md-red-A100: #ff8a80;
  --md-red-A200: #ff5252;
  --md-red-A400: #ff1744;
  --md-red-A700: #d50000;
  --md-pink-50: #fce4ec;
  --md-pink-100: #f8bbd0;
  --md-pink-200: #f48fb1;
  --md-pink-300: #f06292;
  --md-pink-400: #ec407a;
  --md-pink-500: #e91e63;
  --md-pink-600: #d81b60;
  --md-pink-700: #c2185b;
  --md-pink-800: #ad1457;
  --md-pink-900: #880e4f;
  --md-pink-A100: #ff80ab;
  --md-pink-A200: #ff4081;
  --md-pink-A400: #f50057;
  --md-pink-A700: #c51162;
  --md-purple-50: #f3e5f5;
  --md-purple-100: #e1bee7;
  --md-purple-200: #ce93d8;
  --md-purple-300: #ba68c8;
  --md-purple-400: #ab47bc;
  --md-purple-500: #9c27b0;
  --md-purple-600: #8e24aa;
  --md-purple-700: #7b1fa2;
  --md-purple-800: #6a1b9a;
  --md-purple-900: #4a148c;
  --md-purple-A100: #ea80fc;
  --md-purple-A200: #e040fb;
  --md-purple-A400: #d500f9;
  --md-purple-A700: #a0f;
  --md-deep-purple-50: #ede7f6;
  --md-deep-purple-100: #d1c4e9;
  --md-deep-purple-200: #b39ddb;
  --md-deep-purple-300: #9575cd;
  --md-deep-purple-400: #7e57c2;
  --md-deep-purple-500: #673ab7;
  --md-deep-purple-600: #5e35b1;
  --md-deep-purple-700: #512da8;
  --md-deep-purple-800: #4527a0;
  --md-deep-purple-900: #311b92;
  --md-deep-purple-A100: #b388ff;
  --md-deep-purple-A200: #7c4dff;
  --md-deep-purple-A400: #651fff;
  --md-deep-purple-A700: #6200ea;
  --md-indigo-50: #e8eaf6;
  --md-indigo-100: #c5cae9;
  --md-indigo-200: #9fa8da;
  --md-indigo-300: #7986cb;
  --md-indigo-400: #5c6bc0;
  --md-indigo-500: #3f51b5;
  --md-indigo-600: #3949ab;
  --md-indigo-700: #303f9f;
  --md-indigo-800: #283593;
  --md-indigo-900: #1a237e;
  --md-indigo-A100: #8c9eff;
  --md-indigo-A200: #536dfe;
  --md-indigo-A400: #3d5afe;
  --md-indigo-A700: #304ffe;
  --md-blue-50: #e3f2fd;
  --md-blue-100: #bbdefb;
  --md-blue-200: #90caf9;
  --md-blue-300: #64b5f6;
  --md-blue-400: #42a5f5;
  --md-blue-500: #2196f3;
  --md-blue-600: #1e88e5;
  --md-blue-700: #1976d2;
  --md-blue-800: #1565c0;
  --md-blue-900: #0d47a1;
  --md-blue-A100: #82b1ff;
  --md-blue-A200: #448aff;
  --md-blue-A400: #2979ff;
  --md-blue-A700: #2962ff;
  --md-light-blue-50: #e1f5fe;
  --md-light-blue-100: #b3e5fc;
  --md-light-blue-200: #81d4fa;
  --md-light-blue-300: #4fc3f7;
  --md-light-blue-400: #29b6f6;
  --md-light-blue-500: #03a9f4;
  --md-light-blue-600: #039be5;
  --md-light-blue-700: #0288d1;
  --md-light-blue-800: #0277bd;
  --md-light-blue-900: #01579b;
  --md-light-blue-A100: #80d8ff;
  --md-light-blue-A200: #40c4ff;
  --md-light-blue-A400: #00b0ff;
  --md-light-blue-A700: #0091ea;
  --md-cyan-50: #e0f7fa;
  --md-cyan-100: #b2ebf2;
  --md-cyan-200: #80deea;
  --md-cyan-300: #4dd0e1;
  --md-cyan-400: #26c6da;
  --md-cyan-500: #00bcd4;
  --md-cyan-600: #00acc1;
  --md-cyan-700: #0097a7;
  --md-cyan-800: #00838f;
  --md-cyan-900: #006064;
  --md-cyan-A100: #84ffff;
  --md-cyan-A200: #18ffff;
  --md-cyan-A400: #00e5ff;
  --md-cyan-A700: #00b8d4;
  --md-teal-50: #e0f2f1;
  --md-teal-100: #b2dfdb;
  --md-teal-200: #80cbc4;
  --md-teal-300: #4db6ac;
  --md-teal-400: #26a69a;
  --md-teal-500: #009688;
  --md-teal-600: #00897b;
  --md-teal-700: #00796b;
  --md-teal-800: #00695c;
  --md-teal-900: #004d40;
  --md-teal-A100: #a7ffeb;
  --md-teal-A200: #64ffda;
  --md-teal-A400: #1de9b6;
  --md-teal-A700: #00bfa5;
  --md-green-50: #e8f5e9;
  --md-green-100: #c8e6c9;
  --md-green-200: #a5d6a7;
  --md-green-300: #81c784;
  --md-green-400: #66bb6a;
  --md-green-500: #4caf50;
  --md-green-600: #43a047;
  --md-green-700: #388e3c;
  --md-green-800: #2e7d32;
  --md-green-900: #1b5e20;
  --md-green-A100: #b9f6ca;
  --md-green-A200: #69f0ae;
  --md-green-A400: #00e676;
  --md-green-A700: #00c853;
  --md-light-green-50: #f1f8e9;
  --md-light-green-100: #dcedc8;
  --md-light-green-200: #c5e1a5;
  --md-light-green-300: #aed581;
  --md-light-green-400: #9ccc65;
  --md-light-green-500: #8bc34a;
  --md-light-green-600: #7cb342;
  --md-light-green-700: #689f38;
  --md-light-green-800: #558b2f;
  --md-light-green-900: #33691e;
  --md-light-green-A100: #ccff90;
  --md-light-green-A200: #b2ff59;
  --md-light-green-A400: #76ff03;
  --md-light-green-A700: #64dd17;
  --md-lime-50: #f9fbe7;
  --md-lime-100: #f0f4c3;
  --md-lime-200: #e6ee9c;
  --md-lime-300: #dce775;
  --md-lime-400: #d4e157;
  --md-lime-500: #cddc39;
  --md-lime-600: #c0ca33;
  --md-lime-700: #afb42b;
  --md-lime-800: #9e9d24;
  --md-lime-900: #827717;
  --md-lime-A100: #f4ff81;
  --md-lime-A200: #eeff41;
  --md-lime-A400: #c6ff00;
  --md-lime-A700: #aeea00;
  --md-yellow-50: #fffde7;
  --md-yellow-100: #fff9c4;
  --md-yellow-200: #fff59d;
  --md-yellow-300: #fff176;
  --md-yellow-400: #ffee58;
  --md-yellow-500: #ffeb3b;
  --md-yellow-600: #fdd835;
  --md-yellow-700: #fbc02d;
  --md-yellow-800: #f9a825;
  --md-yellow-900: #f57f17;
  --md-yellow-A100: #ffff8d;
  --md-yellow-A200: #ff0;
  --md-yellow-A400: #ffea00;
  --md-yellow-A700: #ffd600;
  --md-amber-50: #fff8e1;
  --md-amber-100: #ffecb3;
  --md-amber-200: #ffe082;
  --md-amber-300: #ffd54f;
  --md-amber-400: #ffca28;
  --md-amber-500: #ffc107;
  --md-amber-600: #ffb300;
  --md-amber-700: #ffa000;
  --md-amber-800: #ff8f00;
  --md-amber-900: #ff6f00;
  --md-amber-A100: #ffe57f;
  --md-amber-A200: #ffd740;
  --md-amber-A400: #ffc400;
  --md-amber-A700: #ffab00;
  --md-orange-50: #fff3e0;
  --md-orange-100: #ffe0b2;
  --md-orange-200: #ffcc80;
  --md-orange-300: #ffb74d;
  --md-orange-400: #ffa726;
  --md-orange-500: #ff9800;
  --md-orange-600: #fb8c00;
  --md-orange-700: #f57c00;
  --md-orange-800: #ef6c00;
  --md-orange-900: #e65100;
  --md-orange-A100: #ffd180;
  --md-orange-A200: #ffab40;
  --md-orange-A400: #ff9100;
  --md-orange-A700: #ff6d00;
  --md-deep-orange-50: #fbe9e7;
  --md-deep-orange-100: #ffccbc;
  --md-deep-orange-200: #ffab91;
  --md-deep-orange-300: #ff8a65;
  --md-deep-orange-400: #ff7043;
  --md-deep-orange-500: #ff5722;
  --md-deep-orange-600: #f4511e;
  --md-deep-orange-700: #e64a19;
  --md-deep-orange-800: #d84315;
  --md-deep-orange-900: #bf360c;
  --md-deep-orange-A100: #ff9e80;
  --md-deep-orange-A200: #ff6e40;
  --md-deep-orange-A400: #ff3d00;
  --md-deep-orange-A700: #dd2c00;
  --md-brown-50: #efebe9;
  --md-brown-100: #d7ccc8;
  --md-brown-200: #bcaaa4;
  --md-brown-300: #a1887f;
  --md-brown-400: #8d6e63;
  --md-brown-500: #795548;
  --md-brown-600: #6d4c41;
  --md-brown-700: #5d4037;
  --md-brown-800: #4e342e;
  --md-brown-900: #3e2723;
  --md-grey-50: #fafafa;
  --md-grey-100: #f5f5f5;
  --md-grey-200: #eee;
  --md-grey-300: #e0e0e0;
  --md-grey-400: #bdbdbd;
  --md-grey-500: #9e9e9e;
  --md-grey-600: #757575;
  --md-grey-700: #616161;
  --md-grey-800: #424242;
  --md-grey-900: #212121;
  --md-blue-grey-50: #eceff1;
  --md-blue-grey-100: #cfd8dc;
  --md-blue-grey-200: #b0bec5;
  --md-blue-grey-300: #90a4ae;
  --md-blue-grey-400: #78909c;
  --md-blue-grey-500: #607d8b;
  --md-blue-grey-600: #546e7a;
  --md-blue-grey-700: #455a64;
  --md-blue-grey-800: #37474f;
  --md-blue-grey-900: #263238;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| RenderedText
|----------------------------------------------------------------------------*/

:root {
  /* This is the padding value to fill the gaps between lines containing spans with background color. */
  --jp-private-code-span-padding: calc(
    (var(--jp-code-line-height) - 1) * var(--jp-code-font-size) / 2
  );
}

.jp-RenderedText {
  text-align: left;
  padding-left: var(--jp-code-padding);
  line-height: var(--jp-code-line-height);
  font-family: var(--jp-code-font-family);
}

.jp-RenderedText pre,
.jp-RenderedJavaScript pre,
.jp-RenderedHTMLCommon pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
  border: none;
  margin: 0;
  padding: 0;
}

.jp-RenderedText pre a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* console foregrounds and backgrounds */
.jp-RenderedText pre .ansi-black-fg {
  color: #3e424d;
}

.jp-RenderedText pre .ansi-red-fg {
  color: #e75c58;
}

.jp-RenderedText pre .ansi-green-fg {
  color: #00a250;
}

.jp-RenderedText pre .ansi-yellow-fg {
  color: #ddb62b;
}

.jp-RenderedText pre .ansi-blue-fg {
  color: #208ffb;
}

.jp-RenderedText pre .ansi-magenta-fg {
  color: #d160c4;
}

.jp-RenderedText pre .ansi-cyan-fg {
  color: #60c6c8;
}

.jp-RenderedText pre .ansi-white-fg {
  color: #c5c1b4;
}

.jp-RenderedText pre .ansi-black-bg {
  background-color: #3e424d;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-bg {
  background-color: #e75c58;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-bg {
  background-color: #00a250;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-bg {
  background-color: #ddb62b;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-bg {
  background-color: #208ffb;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-bg {
  background-color: #d160c4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-bg {
  background-color: #60c6c8;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-bg {
  background-color: #c5c1b4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-black-intense-fg {
  color: #282c36;
}

.jp-RenderedText pre .ansi-red-intense-fg {
  color: #b22b31;
}

.jp-RenderedText pre .ansi-green-intense-fg {
  color: #007427;
}

.jp-RenderedText pre .ansi-yellow-intense-fg {
  color: #b27d12;
}

.jp-RenderedText pre .ansi-blue-intense-fg {
  color: #0065ca;
}

.jp-RenderedText pre .ansi-magenta-intense-fg {
  color: #a03196;
}

.jp-RenderedText pre .ansi-cyan-intense-fg {
  color: #258f8f;
}

.jp-RenderedText pre .ansi-white-intense-fg {
  color: #a1a6b2;
}

.jp-RenderedText pre .ansi-black-intense-bg {
  background-color: #282c36;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-intense-bg {
  background-color: #b22b31;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-intense-bg {
  background-color: #007427;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-intense-bg {
  background-color: #b27d12;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-intense-bg {
  background-color: #0065ca;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-intense-bg {
  background-color: #a03196;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-intense-bg {
  background-color: #258f8f;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-intense-bg {
  background-color: #a1a6b2;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-default-inverse-fg {
  color: var(--jp-ui-inverse-font-color0);
}

.jp-RenderedText pre .ansi-default-inverse-bg {
  background-color: var(--jp-inverse-layout-color0);
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-bold {
  font-weight: bold;
}

.jp-RenderedText pre .ansi-underline {
  text-decoration: underline;
}

.jp-RenderedText[data-mime-type='application/vnd.jupyter.stderr'] {
  background: var(--jp-rendermime-error-background);
  padding-top: var(--jp-code-padding);
}

/*-----------------------------------------------------------------------------
| RenderedLatex
|----------------------------------------------------------------------------*/

.jp-RenderedLatex {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);
}

/* Left-justify outputs.*/
.jp-OutputArea-output.jp-RenderedLatex {
  padding: var(--jp-code-padding);
  text-align: left;
}

/*-----------------------------------------------------------------------------
| RenderedHTML
|----------------------------------------------------------------------------*/

.jp-RenderedHTMLCommon {
  color: var(--jp-content-font-color1);
  font-family: var(--jp-content-font-family);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);

  /* Give a bit more R padding on Markdown text to keep line lengths reasonable */
  padding-right: 20px;
}

.jp-RenderedHTMLCommon em {
  font-style: italic;
}

.jp-RenderedHTMLCommon strong {
  font-weight: bold;
}

.jp-RenderedHTMLCommon u {
  text-decoration: underline;
}

.jp-RenderedHTMLCommon a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* Headings */

.jp-RenderedHTMLCommon h1,
.jp-RenderedHTMLCommon h2,
.jp-RenderedHTMLCommon h3,
.jp-RenderedHTMLCommon h4,
.jp-RenderedHTMLCommon h5,
.jp-RenderedHTMLCommon h6 {
  line-height: var(--jp-content-heading-line-height);
  font-weight: var(--jp-content-heading-font-weight);
  font-style: normal;
  margin: var(--jp-content-heading-margin-top) 0
    var(--jp-content-heading-margin-bottom) 0;
}

.jp-RenderedHTMLCommon h1:first-child,
.jp-RenderedHTMLCommon h2:first-child,
.jp-RenderedHTMLCommon h3:first-child,
.jp-RenderedHTMLCommon h4:first-child,
.jp-RenderedHTMLCommon h5:first-child,
.jp-RenderedHTMLCommon h6:first-child {
  margin-top: calc(0.5 * var(--jp-content-heading-margin-top));
}

.jp-RenderedHTMLCommon h1:last-child,
.jp-RenderedHTMLCommon h2:last-child,
.jp-RenderedHTMLCommon h3:last-child,
.jp-RenderedHTMLCommon h4:last-child,
.jp-RenderedHTMLCommon h5:last-child,
.jp-RenderedHTMLCommon h6:last-child {
  margin-bottom: calc(0.5 * var(--jp-content-heading-margin-bottom));
}

.jp-RenderedHTMLCommon h1 {
  font-size: var(--jp-content-font-size5);
}

.jp-RenderedHTMLCommon h2 {
  font-size: var(--jp-content-font-size4);
}

.jp-RenderedHTMLCommon h3 {
  font-size: var(--jp-content-font-size3);
}

.jp-RenderedHTMLCommon h4 {
  font-size: var(--jp-content-font-size2);
}

.jp-RenderedHTMLCommon h5 {
  font-size: var(--jp-content-font-size1);
}

.jp-RenderedHTMLCommon h6 {
  font-size: var(--jp-content-font-size0);
}

/* Lists */

/* stylelint-disable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon ul:not(.list-inline),
.jp-RenderedHTMLCommon ol:not(.list-inline) {
  padding-left: 2em;
}

.jp-RenderedHTMLCommon ul {
  list-style: disc;
}

.jp-RenderedHTMLCommon ul ul {
  list-style: square;
}

.jp-RenderedHTMLCommon ul ul ul {
  list-style: circle;
}

.jp-RenderedHTMLCommon ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol ol {
  list-style: upper-alpha;
}

.jp-RenderedHTMLCommon ol ol ol {
  list-style: lower-alpha;
}

.jp-RenderedHTMLCommon ol ol ol ol {
  list-style: lower-roman;
}

.jp-RenderedHTMLCommon ol ol ol ol ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol,
.jp-RenderedHTMLCommon ul {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon ul ul,
.jp-RenderedHTMLCommon ul ol,
.jp-RenderedHTMLCommon ol ul,
.jp-RenderedHTMLCommon ol ol {
  margin-bottom: 0;
}

/* stylelint-enable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon hr {
  color: var(--jp-border-color2);
  background-color: var(--jp-border-color1);
  margin-top: 1em;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon > pre {
  margin: 1.5em 2em;
}

.jp-RenderedHTMLCommon pre,
.jp-RenderedHTMLCommon code {
  border: 0;
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  line-height: var(--jp-code-line-height);
  padding: 0;
  white-space: pre-wrap;
}

.jp-RenderedHTMLCommon :not(pre) > code {
  background-color: var(--jp-layout-color2);
  padding: 1px 5px;
}

/* Tables */

.jp-RenderedHTMLCommon table {
  border-collapse: collapse;
  border-spacing: 0;
  border: none;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  table-layout: fixed;
  margin-left: auto;
  margin-bottom: 1em;
  margin-right: auto;
}

.jp-RenderedHTMLCommon thead {
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  vertical-align: bottom;
}

.jp-RenderedHTMLCommon td,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon tr {
  vertical-align: middle;
  padding: 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}

.jp-RenderedMarkdown.jp-RenderedHTMLCommon td,
.jp-RenderedMarkdown.jp-RenderedHTMLCommon th {
  max-width: none;
}

:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon td,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon th,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon tr {
  text-align: right;
}

.jp-RenderedHTMLCommon th {
  font-weight: bold;
}

.jp-RenderedHTMLCommon tbody tr:nth-child(odd) {
  background: var(--jp-layout-color0);
}

.jp-RenderedHTMLCommon tbody tr:nth-child(even) {
  background: var(--jp-rendermime-table-row-background);
}

.jp-RenderedHTMLCommon tbody tr:hover {
  background: var(--jp-rendermime-table-row-hover-background);
}

.jp-RenderedHTMLCommon p {
  text-align: left;
  margin: 0;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon img {
  -moz-force-broken-image-icon: 1;
}

/* Restrict to direct children as other images could be nested in other content. */
.jp-RenderedHTMLCommon > img {
  display: block;
  margin-left: 0;
  margin-right: 0;
  margin-bottom: 1em;
}

/* Change color behind transparent images if they need it... */
[data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-light-background {
  background-color: var(--jp-inverse-layout-color1);
}

[data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-dark-background {
  background-color: var(--jp-inverse-layout-color1);
}

.jp-RenderedHTMLCommon img,
.jp-RenderedImage img,
.jp-RenderedHTMLCommon svg,
.jp-RenderedSVG svg {
  max-width: 100%;
  height: auto;
}

.jp-RenderedHTMLCommon img.jp-mod-unconfined,
.jp-RenderedImage img.jp-mod-unconfined,
.jp-RenderedHTMLCommon svg.jp-mod-unconfined,
.jp-RenderedSVG svg.jp-mod-unconfined {
  max-width: none;
}

.jp-RenderedHTMLCommon .alert {
  padding: var(--jp-notebook-padding);
  border: var(--jp-border-width) solid transparent;
  border-radius: var(--jp-border-radius);
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon .alert-info {
  color: var(--jp-info-color0);
  background-color: var(--jp-info-color3);
  border-color: var(--jp-info-color2);
}

.jp-RenderedHTMLCommon .alert-info hr {
  border-color: var(--jp-info-color3);
}

.jp-RenderedHTMLCommon .alert-info > p:last-child,
.jp-RenderedHTMLCommon .alert-info > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-warning {
  color: var(--jp-warn-color0);
  background-color: var(--jp-warn-color3);
  border-color: var(--jp-warn-color2);
}

.jp-RenderedHTMLCommon .alert-warning hr {
  border-color: var(--jp-warn-color3);
}

.jp-RenderedHTMLCommon .alert-warning > p:last-child,
.jp-RenderedHTMLCommon .alert-warning > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-success {
  color: var(--jp-success-color0);
  background-color: var(--jp-success-color3);
  border-color: var(--jp-success-color2);
}

.jp-RenderedHTMLCommon .alert-success hr {
  border-color: var(--jp-success-color3);
}

.jp-RenderedHTMLCommon .alert-success > p:last-child,
.jp-RenderedHTMLCommon .alert-success > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-danger {
  color: var(--jp-error-color0);
  background-color: var(--jp-error-color3);
  border-color: var(--jp-error-color2);
}

.jp-RenderedHTMLCommon .alert-danger hr {
  border-color: var(--jp-error-color3);
}

.jp-RenderedHTMLCommon .alert-danger > p:last-child,
.jp-RenderedHTMLCommon .alert-danger > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon blockquote {
  margin: 1em 2em;
  padding: 0 1em;
  border-left: 5px solid var(--jp-border-color2);
}

a.jp-InternalAnchorLink {
  visibility: hidden;
  margin-left: 8px;
  color: var(--md-blue-800);
}

h1:hover .jp-InternalAnchorLink,
h2:hover .jp-InternalAnchorLink,
h3:hover .jp-InternalAnchorLink,
h4:hover .jp-InternalAnchorLink,
h5:hover .jp-InternalAnchorLink,
h6:hover .jp-InternalAnchorLink {
  visibility: visible;
}

.jp-RenderedHTMLCommon kbd {
  background-color: var(--jp-rendermime-table-row-background);
  border: 1px solid var(--jp-border-color0);
  border-bottom-color: var(--jp-border-color2);
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
  display: inline-block;
  font-size: var(--jp-ui-font-size0);
  line-height: 1em;
  padding: 0.2em 0.5em;
}

/* Most direct children of .jp-RenderedHTMLCommon have a margin-bottom of 1.0.
 * At the bottom of cells this is a bit too much as there is also spacing
 * between cells. Going all the way to 0 gets too tight between markdown and
 * code cells.
 */
.jp-RenderedHTMLCommon > *:last-child {
  margin-bottom: 0.5em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-cursor-backdrop {
  position: fixed;
  width: 200px;
  height: 200px;
  margin-top: -100px;
  margin-left: -100px;
  will-change: transform;
  z-index: 100;
}

.lm-mod-drag-image {
  will-change: transform;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-lineFormSearch {
  padding: 4px 12px;
  background-color: var(--jp-layout-color2);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
  font-size: var(--jp-ui-font-size1);
}

.jp-lineFormCaption {
  font-size: var(--jp-ui-font-size0);
  line-height: var(--jp-ui-font-size1);
  margin-top: 4px;
  color: var(--jp-ui-font-color0);
}

.jp-baseLineForm {
  border: none;
  border-radius: 0;
  position: absolute;
  background-size: 16px;
  background-repeat: no-repeat;
  background-position: center;
  outline: none;
}

.jp-lineFormButtonContainer {
  top: 4px;
  right: 8px;
  height: 24px;
  padding: 0 12px;
  width: 12px;
}

.jp-lineFormButtonIcon {
  top: 0;
  right: 0;
  background-color: var(--jp-brand-color1);
  height: 100%;
  width: 100%;
  box-sizing: border-box;
  padding: 4px 6px;
}

.jp-lineFormButton {
  top: 0;
  right: 0;
  background-color: transparent;
  height: 100%;
  width: 100%;
  box-sizing: border-box;
}

.jp-lineFormWrapper {
  overflow: hidden;
  padding: 0 8px;
  border: 1px solid var(--jp-border-color0);
  background-color: var(--jp-input-active-background);
  height: 22px;
}

.jp-lineFormWrapperFocusWithin {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-lineFormInput {
  background: transparent;
  width: 200px;
  height: 100%;
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  line-height: 28px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-JSONEditor {
  display: flex;
  flex-direction: column;
  width: 100%;
}

.jp-JSONEditor-host {
  flex: 1 1 auto;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  background: var(--jp-layout-color0);
  min-height: 50px;
  padding: 1px;
}

.jp-JSONEditor.jp-mod-error .jp-JSONEditor-host {
  border-color: red;
  outline-color: red;
}

.jp-JSONEditor-header {
  display: flex;
  flex: 1 0 auto;
  padding: 0 0 0 12px;
}

.jp-JSONEditor-header label {
  flex: 0 0 auto;
}

.jp-JSONEditor-commitButton {
  height: 16px;
  width: 16px;
  background-size: 18px;
  background-repeat: no-repeat;
  background-position: center;
}

.jp-JSONEditor-host.jp-mod-focused {
  background-color: var(--jp-input-active-background);
  border: 1px solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

.jp-Editor.jp-mod-dropTarget {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/
.jp-DocumentSearch-input {
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  font-size: var(--jp-ui-font-size1);
  background-color: var(--jp-layout-color0);
  font-family: var(--jp-ui-font-family);
  padding: 2px 1px;
  resize: none;
}

.jp-DocumentSearch-overlay {
  position: absolute;
  background-color: var(--jp-toolbar-background);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  border-left: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  top: 0;
  right: 0;
  z-index: 7;
  min-width: 405px;
  padding: 2px;
  font-size: var(--jp-ui-font-size1);

  --jp-private-document-search-button-height: 20px;
}

.jp-DocumentSearch-overlay button {
  background-color: var(--jp-toolbar-background);
  outline: 0;
}

.jp-DocumentSearch-overlay button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-overlay button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-overlay-row {
  display: flex;
  align-items: center;
  margin-bottom: 2px;
}

.jp-DocumentSearch-button-content {
  display: inline-block;
  cursor: pointer;
  box-sizing: border-box;
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-button-content svg {
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-input-wrapper {
  border: var(--jp-border-width) solid var(--jp-border-color0);
  display: flex;
  background-color: var(--jp-layout-color0);
  margin: 2px;
}

.jp-DocumentSearch-input-wrapper:focus-within {
  border-color: var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper {
  all: initial;
  overflow: hidden;
  display: inline-block;
  border: none;
  box-sizing: border-box;
}

.jp-DocumentSearch-toggle-wrapper {
  width: 14px;
  height: 14px;
}

.jp-DocumentSearch-button-wrapper {
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
}

.jp-DocumentSearch-toggle-wrapper:focus,
.jp-DocumentSearch-button-wrapper:focus {
  outline: var(--jp-border-width) solid
    var(--jp-cell-editor-active-border-color);
  outline-offset: -1px;
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper,
.jp-DocumentSearch-button-content:focus {
  outline: none;
}

.jp-DocumentSearch-toggle-placeholder {
  width: 5px;
}

.jp-DocumentSearch-input-button::before {
  display: block;
  padding-top: 100%;
}

.jp-DocumentSearch-input-button-off {
  opacity: var(--jp-search-toggle-off-opacity);
}

.jp-DocumentSearch-input-button-off:hover {
  opacity: var(--jp-search-toggle-hover-opacity);
}

.jp-DocumentSearch-input-button-on {
  opacity: var(--jp-search-toggle-on-opacity);
}

.jp-DocumentSearch-index-counter {
  padding-left: 10px;
  padding-right: 10px;
  user-select: none;
  min-width: 35px;
  display: inline-block;
}

.jp-DocumentSearch-up-down-wrapper {
  display: inline-block;
  padding-right: 2px;
  margin-left: auto;
  white-space: nowrap;
}

.jp-DocumentSearch-spacer {
  margin-left: auto;
}

.jp-DocumentSearch-up-down-wrapper button {
  outline: 0;
  border: none;
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
  vertical-align: middle;
  margin: 1px 5px 2px;
}

.jp-DocumentSearch-up-down-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-up-down-button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-filter-button {
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-filter-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled:hover {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-search-options {
  padding: 0 8px;
  margin-left: 3px;
  width: 100%;
  display: grid;
  justify-content: start;
  grid-template-columns: 1fr 1fr;
  align-items: center;
  justify-items: stretch;
}

.jp-DocumentSearch-search-filter-disabled {
  color: var(--jp-ui-font-color2);
}

.jp-DocumentSearch-search-filter {
  display: flex;
  align-items: center;
  user-select: none;
}

.jp-DocumentSearch-regex-error {
  color: var(--jp-error-color0);
}

.jp-DocumentSearch-replace-button-wrapper {
  overflow: hidden;
  display: inline-block;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color0);
  margin: auto 2px;
  padding: 1px 4px;
  height: calc(var(--jp-private-document-search-button-height) + 2px);
}

.jp-DocumentSearch-replace-button-wrapper:focus {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-replace-button {
  display: inline-block;
  text-align: center;
  cursor: pointer;
  box-sizing: border-box;
  color: var(--jp-ui-font-color1);

  /* height - 2 * (padding of wrapper) */
  line-height: calc(var(--jp-private-document-search-button-height) - 2px);
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-replace-button:focus {
  outline: none;
}

.jp-DocumentSearch-replace-wrapper-class {
  margin-left: 14px;
  display: flex;
}

.jp-DocumentSearch-replace-toggle {
  border: none;
  background-color: var(--jp-toolbar-background);
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-replace-toggle:hover {
  background-color: var(--jp-layout-color2);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.cm-editor {
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  border: 0;
  border-radius: 0;
  height: auto;

  /* Changed to auto to autogrow */
}

.cm-editor pre {
  padding: 0 var(--jp-code-padding);
}

.jp-CodeMirrorEditor[data-type='inline'] .cm-dialog {
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
}

.jp-CodeMirrorEditor {
  cursor: text;
}

/* When zoomed out 67% and 33% on a screen of 1440 width x 900 height */
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width1) solid
      var(--jp-editor-cursor-color);
  }
}

/* When zoomed out less than 33% */
@media screen and (min-width: 4320px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width2) solid
      var(--jp-editor-cursor-color);
  }
}

.cm-editor.jp-mod-readOnly .cm-cursor {
  display: none;
}

.jp-CollaboratorCursor {
  border-left: 5px solid transparent;
  border-right: 5px solid transparent;
  border-top: none;
  border-bottom: 3px solid;
  background-clip: content-box;
  margin-left: -5px;
  margin-right: -5px;
}

.cm-searching,
.cm-searching span {
  /* `.cm-searching span`: we need to override syntax highlighting */
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.cm-searching::selection,
.cm-searching span::selection {
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.jp-current-match > .cm-searching,
.jp-current-match > .cm-searching span,
.cm-searching > .jp-current-match,
.cm-searching > .jp-current-match span {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.jp-current-match > .cm-searching::selection,
.cm-searching > .jp-current-match::selection,
.jp-current-match > .cm-searching span::selection {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.cm-trailingspace {
  background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAYAAAB4ka1VAAAAsElEQVQIHQGlAFr/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7+r3zKmT0/+pk9P/7+r3zAAAAAAAAAAABAAAAAAAAAAA6OPzM+/q9wAAAAAA6OPzMwAAAAAAAAAAAgAAAAAAAAAAGR8NiRQaCgAZIA0AGR8NiQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQyoYJ/SY80UAAAAASUVORK5CYII=);
  background-position: center left;
  background-repeat: repeat-x;
}

.jp-CollaboratorCursor-hover {
  position: absolute;
  z-index: 1;
  transform: translateX(-50%);
  color: white;
  border-radius: 3px;
  padding-left: 4px;
  padding-right: 4px;
  padding-top: 1px;
  padding-bottom: 1px;
  text-align: center;
  font-size: var(--jp-ui-font-size1);
  white-space: nowrap;
}

.jp-CodeMirror-ruler {
  border-left: 1px dashed var(--jp-border-color2);
}

/* Styles for shared cursors (remote cursor locations and selected ranges) */
.jp-CodeMirrorEditor .cm-ySelectionCaret {
  position: relative;
  border-left: 1px solid black;
  margin-left: -1px;
  margin-right: -1px;
  box-sizing: border-box;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret > .cm-ySelectionInfo {
  white-space: nowrap;
  position: absolute;
  top: -1.15em;
  padding-bottom: 0.05em;
  left: -1px;
  font-size: 0.95em;
  font-family: var(--jp-ui-font-family);
  font-weight: bold;
  line-height: normal;
  user-select: none;
  color: white;
  padding-left: 2px;
  padding-right: 2px;
  z-index: 101;
  transition: opacity 0.3s ease-in-out;
}

.jp-CodeMirrorEditor .cm-ySelectionInfo {
  transition-delay: 0.7s;
  opacity: 0;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret:hover > .cm-ySelectionInfo {
  opacity: 1;
  transition-delay: 0s;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MimeDocument {
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-filebrowser-button-height: 28px;
  --jp-private-filebrowser-button-width: 48px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FileBrowser .jp-SidePanel-content {
  display: flex;
  flex-direction: column;
}

.jp-FileBrowser-toolbar.jp-Toolbar {
  flex-wrap: wrap;
  row-gap: 12px;
  border-bottom: none;
  height: auto;
  margin: 8px 12px 0;
  box-shadow: none;
  padding: 0;
  justify-content: flex-start;
}

.jp-FileBrowser-Panel {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
}

.jp-BreadCrumbs {
  flex: 0 0 auto;
  margin: 8px 12px;
}

.jp-BreadCrumbs-item {
  margin: 0 2px;
  padding: 0 2px;
  border-radius: var(--jp-border-radius);
  cursor: pointer;
}

.jp-BreadCrumbs-item:hover {
  background-color: var(--jp-layout-color2);
}

.jp-BreadCrumbs-item:first-child {
  margin-left: 0;
}

.jp-BreadCrumbs-item.jp-mod-dropTarget {
  background-color: var(--jp-brand-color2);
  opacity: 0.7;
}

/*-----------------------------------------------------------------------------
| Buttons
|----------------------------------------------------------------------------*/

.jp-FileBrowser-toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  padding-left: 0;
  padding-right: 2px;
  align-items: center;
  height: unset;
}

.jp-FileBrowser-toolbar > .jp-Toolbar-item .jp-ToolbarButtonComponent {
  width: 40px;
}

/*-----------------------------------------------------------------------------
| Other styles
|----------------------------------------------------------------------------*/

.jp-FileDialog.jp-mod-conflict input {
  color: var(--jp-error-color1);
}

.jp-FileDialog .jp-new-name-title {
  margin-top: 12px;
}

.jp-LastModified-hidden {
  display: none;
}

.jp-FileSize-hidden {
  display: none;
}

.jp-FileBrowser .lm-AccordionPanel > h3:first-child {
  display: none;
}

/*-----------------------------------------------------------------------------
| DirListing
|----------------------------------------------------------------------------*/

.jp-DirListing {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
  outline: 0;
}

.jp-DirListing-header {
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  align-items: center;
  overflow: hidden;
  border-top: var(--jp-border-width) solid var(--jp-border-color2);
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
}

.jp-DirListing-headerItem {
  padding: 4px 12px 2px;
  font-weight: 500;
}

.jp-DirListing-headerItem:hover {
  background: var(--jp-layout-color2);
}

.jp-DirListing-headerItem.jp-id-name {
  flex: 1 0 84px;
}

.jp-DirListing-headerItem.jp-id-modified {
  flex: 0 0 112px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-DirListing-headerItem.jp-id-filesize {
  flex: 0 0 75px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-id-narrow {
  display: none;
  flex: 0 0 5px;
  padding: 4px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
  color: var(--jp-border-color2);
}

.jp-DirListing-narrow .jp-id-narrow {
  display: block;
}

.jp-DirListing-narrow .jp-id-modified,
.jp-DirListing-narrow .jp-DirListing-itemModified {
  display: none;
}

.jp-DirListing-headerItem.jp-mod-selected {
  font-weight: 600;
}

/* increase specificity to override bundled default */
.jp-DirListing-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  list-style-type: none;
  overflow: auto;
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-content mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.jp-DirListing-content .jp-DirListing-item.jp-mod-selected mark {
  color: var(--jp-ui-inverse-font-color0);
}

/* Style the directory listing content when a user drops a file to upload */
.jp-DirListing.jp-mod-native-drop .jp-DirListing-content {
  outline: 5px dashed rgba(128, 128, 128, 0.5);
  outline-offset: -10px;
  cursor: copy;
}

.jp-DirListing-item {
  display: flex;
  flex-direction: row;
  align-items: center;
  padding: 4px 12px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-DirListing-checkboxWrapper {
  /* Increases hit area of checkbox. */
  padding: 4px;
}

.jp-DirListing-header
  .jp-DirListing-checkboxWrapper
  + .jp-DirListing-headerItem {
  padding-left: 4px;
}

.jp-DirListing-content .jp-DirListing-checkboxWrapper {
  position: relative;
  left: -4px;
  margin: -4px 0 -4px -8px;
}

.jp-DirListing-checkboxWrapper.jp-mod-visible {
  visibility: visible;
}

/* For devices that support hovering, hide checkboxes until hovered, selected...
*/
@media (hover: hover) {
  .jp-DirListing-checkboxWrapper {
    visibility: hidden;
  }

  .jp-DirListing-item:hover .jp-DirListing-checkboxWrapper,
  .jp-DirListing-item.jp-mod-selected .jp-DirListing-checkboxWrapper {
    visibility: visible;
  }
}

.jp-DirListing-item[data-is-dot] {
  opacity: 75%;
}

.jp-DirListing-item.jp-mod-selected {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.jp-DirListing-item.jp-mod-dropTarget {
  background: var(--jp-brand-color3);
}

.jp-DirListing-item:hover:not(.jp-mod-selected) {
  background: var(--jp-layout-color2);
}

.jp-DirListing-itemIcon {
  flex: 0 0 20px;
  margin-right: 4px;
}

.jp-DirListing-itemText {
  flex: 1 0 64px;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  user-select: none;
}

.jp-DirListing-itemText:focus {
  outline-width: 2px;
  outline-color: var(--jp-inverse-layout-color1);
  outline-style: solid;
  outline-offset: 1px;
}

.jp-DirListing-item.jp-mod-selected .jp-DirListing-itemText:focus {
  outline-color: var(--jp-layout-color1);
}

.jp-DirListing-itemModified {
  flex: 0 0 125px;
  text-align: right;
}

.jp-DirListing-itemFileSize {
  flex: 0 0 90px;
  text-align: right;
}

.jp-DirListing-editor {
  flex: 1 0 64px;
  outline: none;
  border: none;
  color: var(--jp-ui-font-color1);
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon::before {
  color: var(--jp-success-color1);
  content: '\25CF';
  font-size: 8px;
  position: absolute;
  left: -8px;
}

.jp-DirListing-item.jp-mod-running.jp-mod-selected
  .jp-DirListing-itemIcon::before {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-DirListing-item.lm-mod-drag-image,
.jp-DirListing-item.jp-mod-selected.lm-mod-drag-image {
  font-size: var(--jp-ui-font-size1);
  padding-left: 4px;
  margin-left: 4px;
  width: 160px;
  background-color: var(--jp-ui-inverse-font-color2);
  box-shadow: var(--jp-elevation-z2);
  border-radius: 0;
  color: var(--jp-ui-font-color1);
  transform: translateX(-40%) translateY(-58%);
}

.jp-Document {
  min-width: 120px;
  min-height: 120px;
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Main OutputArea
| OutputArea has a list of Outputs
|----------------------------------------------------------------------------*/

.jp-OutputArea {
  overflow-y: auto;
}

.jp-OutputArea-child {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-OutputPrompt {
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-outprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
  opacity: var(--jp-cell-prompt-opacity);

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-OutputArea-prompt {
  display: table-cell;
  vertical-align: top;
}

.jp-OutputArea-output {
  display: table-cell;
  width: 100%;
  height: auto;
  overflow: auto;
  user-select: text;
  -moz-user-select: text;
  -webkit-user-select: text;
  -ms-user-select: text;
}

.jp-OutputArea .jp-RenderedText {
  padding-left: 1ch;
}

/**
 * Prompt overlay.
 */

.jp-OutputArea-promptOverlay {
  position: absolute;
  top: 0;
  width: var(--jp-cell-prompt-width);
  height: 100%;
  opacity: 0.5;
}

.jp-OutputArea-promptOverlay:hover {
  background: var(--jp-layout-color2);
  box-shadow: inset 0 0 1px var(--jp-inverse-layout-color0);
  cursor: zoom-out;
}

.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay:hover {
  cursor: zoom-in;
}

/**
 * Isolated output.
 */
.jp-OutputArea-output.jp-mod-isolated {
  width: 100%;
  display: block;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated {
  position: relative;
}

body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/* pre */

.jp-OutputArea-output pre {
  border: none;
  margin: 0;
  padding: 0;
  overflow-x: auto;
  overflow-y: auto;
  word-break: break-all;
  word-wrap: break-word;
  white-space: pre-wrap;
}

/* tables */

.jp-OutputArea-output.jp-RenderedHTMLCommon table {
  margin-left: 0;
  margin-right: 0;
}

/* description lists */

.jp-OutputArea-output dl,
.jp-OutputArea-output dt,
.jp-OutputArea-output dd {
  display: block;
}

.jp-OutputArea-output dl {
  width: 100%;
  overflow: hidden;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dt {
  font-weight: bold;
  float: left;
  width: 20%;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dd {
  float: left;
  width: 80%;
  padding: 0;
  margin: 0;
}

.jp-TrimmedOutputs pre {
  background: var(--jp-layout-color3);
  font-size: calc(var(--jp-code-font-size) * 1.4);
  text-align: center;
  text-transform: uppercase;
}

/* Hide the gutter in case of
 *  - nested output areas (e.g. in the case of output widgets)
 *  - mirrored output areas
 */
.jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt {
  display: none;
}

/* Hide empty lines in the output area, for instance due to cleared widgets */
.jp-OutputArea-prompt:empty {
  padding: 0;
  border: 0;
}

/*-----------------------------------------------------------------------------
| executeResult is added to any Output-result for the display of the object
| returned by a cell
|----------------------------------------------------------------------------*/

.jp-OutputArea-output.jp-OutputArea-executeResult {
  margin-left: 0;
  width: 100%;
}

/* Text output with the Out[] prompt needs a top padding to match the
 * alignment of the Out[] prompt itself.
 */
.jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output {
  padding-top: var(--jp-code-padding);
  border-top: var(--jp-border-width) solid transparent;
}

/*-----------------------------------------------------------------------------
| The Stdin output
|----------------------------------------------------------------------------*/

.jp-Stdin-prompt {
  color: var(--jp-content-font-color0);
  padding-right: var(--jp-code-padding);
  vertical-align: baseline;
  flex: 0 0 auto;
}

.jp-Stdin-input {
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  color: inherit;
  background-color: inherit;
  width: 42%;
  min-width: 200px;

  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;

  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0 0.25em;
  margin: 0 0.25em;
  flex: 0 0 70%;
}

.jp-Stdin-input::placeholder {
  opacity: 0;
}

.jp-Stdin-input:focus {
  box-shadow: none;
}

.jp-Stdin-input:focus::placeholder {
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Output Area View
|----------------------------------------------------------------------------*/

.jp-LinkedOutputView .jp-OutputArea {
  height: 100%;
  display: block;
}

.jp-LinkedOutputView .jp-OutputArea-output:only-child {
  height: 100%;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

@media print {
  .jp-OutputArea-child {
    break-inside: avoid-page;
  }
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-OutputPrompt {
    display: table-row;
    text-align: left;
  }

  .jp-OutputArea-child .jp-OutputArea-output {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }
}

/* Trimmed outputs warning */
.jp-TrimmedOutputs > a {
  margin: 10px;
  text-decoration: none;
  cursor: pointer;
}

.jp-TrimmedOutputs > a:hover {
  text-decoration: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Table of Contents
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toc-active-width: 4px;
}

.jp-TableOfContents {
  display: flex;
  flex-direction: column;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  height: 100%;
}

.jp-TableOfContents-placeholder {
  text-align: center;
}

.jp-TableOfContents-placeholderContent {
  color: var(--jp-content-font-color2);
  padding: 8px;
}

.jp-TableOfContents-placeholderContent > h3 {
  margin-bottom: var(--jp-content-heading-margin-bottom);
}

.jp-TableOfContents .jp-SidePanel-content {
  overflow-y: auto;
}

.jp-TableOfContents-tree {
  margin: 4px;
}

.jp-TableOfContents ol {
  list-style-type: none;
}

/* stylelint-disable-next-line selector-max-type */
.jp-TableOfContents li > ol {
  /* Align left border with triangle icon center */
  padding-left: 11px;
}

.jp-TableOfContents-content {
  /* left margin for the active heading indicator */
  margin: 0 0 0 var(--jp-private-toc-active-width);
  padding: 0;
  background-color: var(--jp-layout-color1);
}

.jp-tocItem {
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-tocItem-heading {
  display: flex;
  cursor: pointer;
}

.jp-tocItem-heading:hover {
  background-color: var(--jp-layout-color2);
}

.jp-tocItem-content {
  display: block;
  padding: 4px 0;
  white-space: nowrap;
  text-overflow: ellipsis;
  overflow-x: hidden;
}

.jp-tocItem-collapser {
  height: 20px;
  margin: 2px 2px 0;
  padding: 0;
  background: none;
  border: none;
  cursor: pointer;
}

.jp-tocItem-collapser:hover {
  background-color: var(--jp-layout-color3);
}

/* Active heading indicator */

.jp-tocItem-heading::before {
  content: ' ';
  background: transparent;
  width: var(--jp-private-toc-active-width);
  height: 24px;
  position: absolute;
  left: 0;
  border-radius: var(--jp-border-radius);
}

.jp-tocItem-heading.jp-tocItem-active::before {
  background-color: var(--jp-brand-color1);
}

.jp-tocItem-heading:hover.jp-tocItem-active::before {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapser {
  flex: 0 0 var(--jp-cell-collapser-width);
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
  border-radius: var(--jp-border-radius);
  opacity: 1;
}

.jp-Collapser-child {
  display: block;
  width: 100%;
  box-sizing: border-box;

  /* height: 100% doesn't work because the height of its parent is computed from content */
  position: absolute;
  top: 0;
  bottom: 0;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Hiding collapsers in print mode.

Note: input and output wrappers have "display: block" propery in print mode.
*/

@media print {
  .jp-Collapser {
    display: none;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Header/Footer
|----------------------------------------------------------------------------*/

/* Hidden by zero height by default */
.jp-CellHeader,
.jp-CellFooter {
  height: 0;
  width: 100%;
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Input
|----------------------------------------------------------------------------*/

/* All input areas */
.jp-InputArea {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-InputArea-editor {
  display: table-cell;
  overflow: hidden;
  vertical-align: top;

  /* This is the non-active, default styling */
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  background: var(--jp-cell-editor-background);
}

.jp-InputPrompt {
  display: table-cell;
  vertical-align: top;
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-inprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  opacity: var(--jp-cell-prompt-opacity);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-InputArea-editor {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }

  .jp-InputPrompt {
    display: table-row;
    text-align: left;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Placeholder {
  display: table;
  table-layout: fixed;
  width: 100%;
}

.jp-Placeholder-prompt {
  display: table-cell;
  box-sizing: border-box;
}

.jp-Placeholder-content {
  display: table-cell;
  padding: 4px 6px;
  border: 1px solid transparent;
  border-radius: 0;
  background: none;
  box-sizing: border-box;
  cursor: pointer;
}

.jp-Placeholder-contentContainer {
  display: flex;
}

.jp-Placeholder-content:hover,
.jp-InputPlaceholder > .jp-Placeholder-content:hover {
  border-color: var(--jp-layout-color3);
}

.jp-Placeholder-content .jp-MoreHorizIcon {
  width: 32px;
  height: 16px;
  border: 1px solid transparent;
  border-radius: var(--jp-border-radius);
}

.jp-Placeholder-content .jp-MoreHorizIcon:hover {
  border: 1px solid var(--jp-border-color1);
  box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.25);
  background-color: var(--jp-layout-color0);
}

.jp-PlaceholderText {
  white-space: nowrap;
  overflow-x: hidden;
  color: var(--jp-inverse-layout-color3);
  font-family: var(--jp-code-font-family);
}

.jp-InputPlaceholder > .jp-Placeholder-content {
  border-color: var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-cell-scrolling-output-offset: 5px;
}

/*-----------------------------------------------------------------------------
| Cell
|----------------------------------------------------------------------------*/

.jp-Cell {
  padding: var(--jp-cell-padding);
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Common input/output
|----------------------------------------------------------------------------*/

.jp-Cell-inputWrapper,
.jp-Cell-outputWrapper {
  display: flex;
  flex-direction: row;
  padding: 0;
  margin: 0;

  /* Added to reveal the box-shadow on the input and output collapsers. */
  overflow: visible;
}

/* Only input/output areas inside cells */
.jp-Cell-inputArea,
.jp-Cell-outputArea {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Collapser
|----------------------------------------------------------------------------*/

/* Make the output collapser disappear when there is not output, but do so
 * in a manner that leaves it in the layout and preserves its width.
 */
.jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser {
  border: none !important;
  background: transparent !important;
}

.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser {
  min-height: var(--jp-cell-collapser-min-height);
}

/*-----------------------------------------------------------------------------
| Output
|----------------------------------------------------------------------------*/

/* Put a space between input and output when there IS output */
.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper {
  margin-top: 5px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea {
  overflow-y: auto;
  max-height: 24em;
  margin-left: var(--jp-private-cell-scrolling-output-offset);
  resize: vertical;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea[style*='height'] {
  max-height: unset;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea::after {
  content: ' ';
  box-shadow: inset 0 0 6px 2px rgb(0 0 0 / 30%);
  width: 100%;
  height: 100%;
  position: sticky;
  bottom: 0;
  top: 0;
  margin-top: -50%;
  float: left;
  display: block;
  pointer-events: none;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-child {
  padding-top: 6px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt {
  width: calc(
    var(--jp-cell-prompt-width) - var(--jp-private-cell-scrolling-output-offset)
  );
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay {
  left: calc(-1 * var(--jp-private-cell-scrolling-output-offset));
}

/*-----------------------------------------------------------------------------
| CodeCell
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| MarkdownCell
|----------------------------------------------------------------------------*/

.jp-MarkdownOutput {
  display: table-cell;
  width: 100%;
  margin-top: 0;
  margin-bottom: 0;
  padding-left: var(--jp-code-padding);
}

.jp-MarkdownOutput.jp-RenderedHTMLCommon {
  overflow: auto;
}

/* collapseHeadingButton (show always if hiddenCellsButton is _not_ shown) */
.jp-collapseHeadingButton {
  display: flex;
  min-height: var(--jp-cell-collapser-min-height);
  font-size: var(--jp-code-font-size);
  position: absolute;
  background-color: transparent;
  background-size: 25px;
  background-repeat: no-repeat;
  background-position-x: center;
  background-position-y: top;
  background-image: var(--jp-icon-caret-down);
  right: 0;
  top: 0;
  bottom: 0;
}

.jp-collapseHeadingButton.jp-mod-collapsed {
  background-image: var(--jp-icon-caret-right);
}

/*
 set the container font size to match that of content
 so that the nested collapse buttons have the right size
*/
.jp-MarkdownCell .jp-InputPrompt {
  font-size: var(--jp-content-font-size1);
}

/*
  Align collapseHeadingButton with cell top header
  The font sizes are identical to the ones in packages/rendermime/style/base.css
*/
.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='1'] {
  font-size: var(--jp-content-font-size5);
  background-position-y: calc(0.3 * var(--jp-content-font-size5));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='2'] {
  font-size: var(--jp-content-font-size4);
  background-position-y: calc(0.3 * var(--jp-content-font-size4));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='3'] {
  font-size: var(--jp-content-font-size3);
  background-position-y: calc(0.3 * var(--jp-content-font-size3));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='4'] {
  font-size: var(--jp-content-font-size2);
  background-position-y: calc(0.3 * var(--jp-content-font-size2));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='5'] {
  font-size: var(--jp-content-font-size1);
  background-position-y: top;
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='6'] {
  font-size: var(--jp-content-font-size0);
  background-position-y: top;
}

/* collapseHeadingButton (show only on (hover,active) if hiddenCellsButton is shown) */
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-collapseHeadingButton {
  display: none;
}

.jp-Notebook.jp-mod-showHiddenCellsButton
  :is(.jp-MarkdownCell:hover, .jp-mod-active)
  .jp-collapseHeadingButton {
  display: flex;
}

/* showHiddenCellsButton (only show if jp-mod-showHiddenCellsButton is set, which
is a consequence of the showHiddenCellsButton option in Notebook Settings)*/
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton {
  margin-left: calc(var(--jp-cell-prompt-width) + 2 * var(--jp-code-padding));
  margin-top: var(--jp-code-padding);
  border: 1px solid var(--jp-border-color2);
  background-color: var(--jp-border-color3) !important;
  color: var(--jp-content-font-color0) !important;
  display: flex;
}

.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton:hover {
  background-color: var(--jp-border-color2) !important;
}

.jp-showHiddenCellsButton {
  display: none;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Using block instead of flex to allow the use of the break-inside CSS property for
cell outputs.
*/

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-notebook-toolbar-padding: 2px 5px 2px 2px;
}

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-NotebookPanel-toolbar {
  padding: var(--jp-notebook-toolbar-padding);

  /* disable paint containment from lumino 2.0 default strict CSS containment */
  contain: style size !important;
}

.jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused {
  border: none;
  box-shadow: none;
}

.jp-Notebook-toolbarCellTypeDropdown select {
  height: 24px;
  font-size: var(--jp-ui-font-size1);
  line-height: 14px;
  border-radius: 0;
  display: block;
}

.jp-Notebook-toolbarCellTypeDropdown span {
  top: 5px !important;
}

.jp-Toolbar-responsive-popup {
  position: absolute;
  height: fit-content;
  display: flex;
  flex-direction: row;
  flex-wrap: wrap;
  justify-content: flex-end;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: var(--jp-notebook-toolbar-padding);
  z-index: 1;
  right: 0;
  top: 0;
}

.jp-Toolbar > .jp-Toolbar-responsive-opener {
  margin-left: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-Notebook-ExecutionIndicator {
  position: relative;
  display: inline-block;
  height: 100%;
  z-index: 9997;
}

.jp-Notebook-ExecutionIndicator-tooltip {
  visibility: hidden;
  height: auto;
  width: max-content;
  width: -moz-max-content;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color1);
  text-align: justify;
  border-radius: 6px;
  padding: 0 5px;
  position: fixed;
  display: table;
}

.jp-Notebook-ExecutionIndicator-tooltip.up {
  transform: translateX(-50%) translateY(-100%) translateY(-32px);
}

.jp-Notebook-ExecutionIndicator-tooltip.down {
  transform: translateX(calc(-100% + 16px)) translateY(5px);
}

.jp-Notebook-ExecutionIndicator-tooltip.hidden {
  display: none;
}

.jp-Notebook-ExecutionIndicator:hover .jp-Notebook-ExecutionIndicator-tooltip {
  visibility: visible;
}

.jp-Notebook-ExecutionIndicator span {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  color: var(--jp-ui-font-color1);
  line-height: 24px;
  display: block;
}

.jp-Notebook-ExecutionIndicator-progress-bar {
  display: flex;
  justify-content: center;
  height: 100%;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*
 * Execution indicator
 */
.jp-tocItem-content::after {
  content: '';

  /* Must be identical to form a circle */
  width: 12px;
  height: 12px;
  background: none;
  border: none;
  position: absolute;
  right: 0;
}

.jp-tocItem-content[data-running='0']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background: none;
}

.jp-tocItem-content[data-running='1']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background-color: var(--jp-inverse-layout-color3);
}

.jp-tocItem-content[data-running='0'],
.jp-tocItem-content[data-running='1'] {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Notebook-footer {
  height: 27px;
  margin-left: calc(
    var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
      var(--jp-cell-padding)
  );
  width: calc(
    100% -
      (
        var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
          var(--jp-cell-padding) + var(--jp-cell-padding)
      )
  );
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  color: var(--jp-ui-font-color3);
  margin-top: 6px;
  background: none;
  cursor: pointer;
}

.jp-Notebook-footer:focus {
  border-color: var(--jp-cell-editor-active-border-color);
}

/* For devices that support hovering, hide footer until hover */
@media (hover: hover) {
  .jp-Notebook-footer {
    opacity: 0;
  }

  .jp-Notebook-footer:focus,
  .jp-Notebook-footer:hover {
    opacity: 1;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Imports
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-side-by-side-output-size: 1fr;
  --jp-side-by-side-resized-cell: var(--jp-side-by-side-output-size);
  --jp-private-notebook-dragImage-width: 304px;
  --jp-private-notebook-dragImage-height: 36px;
  --jp-private-notebook-selected-color: var(--md-blue-400);
  --jp-private-notebook-active-color: var(--md-green-400);
}

/*-----------------------------------------------------------------------------
| Notebook
|----------------------------------------------------------------------------*/

/* stylelint-disable selector-max-class */

.jp-NotebookPanel {
  display: block;
  height: 100%;
}

.jp-NotebookPanel.jp-Document {
  min-width: 240px;
  min-height: 120px;
}

.jp-Notebook {
  padding: var(--jp-notebook-padding);
  outline: none;
  overflow: auto;
  background: var(--jp-layout-color0);
}

.jp-Notebook.jp-mod-scrollPastEnd::after {
  display: block;
  content: '';
  min-height: var(--jp-notebook-scroll-padding);
}

.jp-MainAreaWidget-ContainStrict .jp-Notebook * {
  contain: strict;
}

.jp-Notebook .jp-Cell {
  overflow: visible;
}

.jp-Notebook .jp-Cell .jp-InputPrompt {
  cursor: move;
}

/*-----------------------------------------------------------------------------
| Notebook state related styling
|
| The notebook and cells each have states, here are the possibilities:
|
| - Notebook
|   - Command
|   - Edit
| - Cell
|   - None
|   - Active (only one can be active)
|   - Selected (the cells actions are applied to)
|   - Multiselected (when multiple selected, the cursor)
|   - No outputs
|----------------------------------------------------------------------------*/

/* Command or edit modes */

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

/* cell is active */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser {
  background: var(--jp-brand-color1);
}

/* cell is dirty */
.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt {
  color: var(--jp-warn-color1);
}

.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt::before {
  color: var(--jp-warn-color1);
  content: '';
}

.jp-Notebook .jp-Cell.jp-mod-active.jp-mod-dirty .jp-Collapser {
  background: var(--jp-warn-color1);
}

/* collapser is hovered */
.jp-Notebook .jp-Cell .jp-Collapser:hover {
  box-shadow: var(--jp-elevation-z2);
  background: var(--jp-brand-color1);
  opacity: var(--jp-cell-collapser-not-active-hover-opacity);
}

/* cell is active and collapser is hovered */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/* Command mode */

.jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected {
  background: var(--jp-notebook-multiselected-color);
}

.jp-Notebook.jp-mod-commandMode
  .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected) {
  background: transparent;
}

/* Edit mode */

.jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-cell-editor-active-background);
}

/*-----------------------------------------------------------------------------
| Notebook drag and drop
|----------------------------------------------------------------------------*/

.jp-Notebook-cell.jp-mod-dropSource {
  opacity: 0.5;
}

.jp-Notebook-cell.jp-mod-dropTarget,
.jp-Notebook.jp-mod-commandMode
  .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget {
  border-top-color: var(--jp-private-notebook-selected-color);
  border-top-style: solid;
  border-top-width: 2px;
}

.jp-dragImage {
  display: block;
  flex-direction: row;
  width: var(--jp-private-notebook-dragImage-width);
  height: var(--jp-private-notebook-dragImage-height);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
  overflow: visible;
}

.jp-dragImage-singlePrompt {
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

.jp-dragImage .jp-dragImage-content {
  flex: 1 1 auto;
  z-index: 2;
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  line-height: var(--jp-code-line-height);
  padding: var(--jp-code-padding);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background-color);
  color: var(--jp-content-font-color3);
  text-align: left;
  margin: 4px 4px 4px 0;
}

.jp-dragImage .jp-dragImage-prompt {
  flex: 0 0 auto;
  min-width: 36px;
  color: var(--jp-cell-inprompt-font-color);
  padding: var(--jp-code-padding);
  padding-left: 12px;
  font-family: var(--jp-cell-prompt-font-family);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: 1.9;
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
}

.jp-dragImage-multipleBack {
  z-index: -1;
  position: absolute;
  height: 32px;
  width: 300px;
  top: 8px;
  left: 8px;
  background: var(--jp-layout-color2);
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

/*-----------------------------------------------------------------------------
| Cell toolbar
|----------------------------------------------------------------------------*/

.jp-NotebookTools {
  display: block;
  min-width: var(--jp-sidebar-min-width);
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
    * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  overflow: auto;
}

.jp-ActiveCellTool {
  padding: 12px 0;
  display: flex;
}

.jp-ActiveCellTool-Content {
  flex: 1 1 auto;
}

.jp-ActiveCellTool .jp-ActiveCellTool-CellContent {
  background: var(--jp-cell-editor-background);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  min-height: 29px;
}

.jp-ActiveCellTool .jp-InputPrompt {
  min-width: calc(var(--jp-cell-prompt-width) * 0.75);
}

.jp-ActiveCellTool-CellContent > pre {
  padding: 5px 4px;
  margin: 0;
  white-space: normal;
}

.jp-MetadataEditorTool {
  flex-direction: column;
  padding: 12px 0;
}

.jp-RankedPanel > :not(:first-child) {
  margin-top: 12px;
}

.jp-KeySelector select.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: var(--jp-border-width) solid var(--jp-border-color1);
}

.jp-KeySelector label,
.jp-MetadataEditorTool label,
.jp-NumberSetter label {
  line-height: 1.4;
}

.jp-NotebookTools .jp-select-wrapper {
  margin-top: 4px;
  margin-bottom: 0;
}

.jp-NumberSetter input {
  width: 100%;
  margin-top: 4px;
}

.jp-NotebookTools .jp-Collapse {
  margin-top: 16px;
}

/*-----------------------------------------------------------------------------
| Presentation Mode (.jp-mod-presentationMode)
|----------------------------------------------------------------------------*/

.jp-mod-presentationMode .jp-Notebook {
  --jp-content-font-size1: var(--jp-content-presentation-font-size1);
  --jp-code-font-size: var(--jp-code-presentation-font-size);
}

.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt,
.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt {
  flex: 0 0 110px;
}

/*-----------------------------------------------------------------------------
| Side-by-side Mode (.jp-mod-sideBySide)
|----------------------------------------------------------------------------*/
.jp-mod-sideBySide.jp-Notebook .jp-Notebook-cell {
  margin-top: 3em;
  margin-bottom: 3em;
  margin-left: 5%;
  margin-right: 5%;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell {
  display: grid;
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-output-size)
    );
  grid-template-rows: auto minmax(0, 1fr) auto;
  grid-template-areas:
    'header header header'
    'input handle output'
    'footer footer footer';
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell.jp-mod-resizedCell {
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-resized-cell)
    );
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellHeader {
  grid-area: header;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-inputWrapper {
  grid-area: input;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-outputWrapper {
  /* overwrite the default margin (no vertical separation needed in side by side move */
  margin-top: 0;
  grid-area: output;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellFooter {
  grid-area: footer;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle {
  grid-area: handle;
  user-select: none;
  display: block;
  height: 100%;
  cursor: ew-resize;
  padding: 0 var(--jp-cell-padding);
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle::after {
  content: '';
  display: block;
  background: var(--jp-border-color2);
  height: 100%;
  width: 5px;
}

.jp-mod-sideBySide.jp-Notebook
  .jp-CodeCell.jp-mod-resizedCell
  .jp-CellResizeHandle::after {
  background: var(--jp-border-color0);
}

.jp-CellResizeHandle {
  display: none;
}

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Cell-Placeholder {
  padding-left: 55px;
}

.jp-Cell-Placeholder-wrapper {
  background: #fff;
  border: 1px solid;
  border-color: #e5e6e9 #dfe0e4 #d0d1d5;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  margin: 10px 15px;
}

.jp-Cell-Placeholder-wrapper-inner {
  padding: 15px;
  position: relative;
}

.jp-Cell-Placeholder-wrapper-body {
  background-repeat: repeat;
  background-size: 50% auto;
}

.jp-Cell-Placeholder-wrapper-body div {
  background: #f6f7f8;
  background-image: -webkit-linear-gradient(
    left,
    #f6f7f8 0%,
    #edeef1 20%,
    #f6f7f8 40%,
    #f6f7f8 100%
  );
  background-repeat: no-repeat;
  background-size: 800px 104px;
  height: 104px;
  position: absolute;
  right: 15px;
  left: 15px;
  top: 15px;
}

div.jp-Cell-Placeholder-h1 {
  top: 20px;
  height: 20px;
  left: 15px;
  width: 150px;
}

div.jp-Cell-Placeholder-h2 {
  left: 15px;
  top: 50px;
  height: 10px;
  width: 100px;
}

div.jp-Cell-Placeholder-content-1,
div.jp-Cell-Placeholder-content-2,
div.jp-Cell-Placeholder-content-3 {
  left: 15px;
  right: 15px;
  height: 10px;
}

div.jp-Cell-Placeholder-content-1 {
  top: 100px;
}

div.jp-Cell-Placeholder-content-2 {
  top: 120px;
}

div.jp-Cell-Placeholder-content-3 {
  top: 140px;
}

</style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
The following CSS variables define the main, public API for styling JupyterLab.
These variables should be used by all plugins wherever possible. In other
words, plugins should not define custom colors, sizes, etc unless absolutely
necessary. This enables users to change the visual theme of JupyterLab
by changing these variables.

Many variables appear in an ordered sequence (0,1,2,3). These sequences
are designed to work well together, so for example, `--jp-border-color1` should
be used with `--jp-layout-color1`. The numbers have the following meanings:

* 0: super-primary, reserved for special emphasis
* 1: primary, most important under normal situations
* 2: secondary, next most important under normal situations
* 3: tertiary, next most important under normal situations

Throughout JupyterLab, we are mostly following principles from Google's
Material Design when selecting colors. We are not, however, following
all of MD as it is not optimized for dense, information rich UIs.
*/

:root {
  /* Elevation
   *
   * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here:
   *
   * https://github.com/material-components/material-components-web
   * https://material-components-web.appspot.com/elevation.html
   */

  --jp-shadow-base-lightness: 0;
  --jp-shadow-umbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.2
  );
  --jp-shadow-penumbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.14
  );
  --jp-shadow-ambient-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.12
  );
  --jp-elevation-z0: none;
  --jp-elevation-z1: 0 2px 1px -1px var(--jp-shadow-umbra-color),
    0 1px 1px 0 var(--jp-shadow-penumbra-color),
    0 1px 3px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z2: 0 3px 1px -2px var(--jp-shadow-umbra-color),
    0 2px 2px 0 var(--jp-shadow-penumbra-color),
    0 1px 5px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z4: 0 2px 4px -1px var(--jp-shadow-umbra-color),
    0 4px 5px 0 var(--jp-shadow-penumbra-color),
    0 1px 10px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z6: 0 3px 5px -1px var(--jp-shadow-umbra-color),
    0 6px 10px 0 var(--jp-shadow-penumbra-color),
    0 1px 18px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z8: 0 5px 5px -3px var(--jp-shadow-umbra-color),
    0 8px 10px 1px var(--jp-shadow-penumbra-color),
    0 3px 14px 2px var(--jp-shadow-ambient-color);
  --jp-elevation-z12: 0 7px 8px -4px var(--jp-shadow-umbra-color),
    0 12px 17px 2px var(--jp-shadow-penumbra-color),
    0 5px 22px 4px var(--jp-shadow-ambient-color);
  --jp-elevation-z16: 0 8px 10px -5px var(--jp-shadow-umbra-color),
    0 16px 24px 2px var(--jp-shadow-penumbra-color),
    0 6px 30px 5px var(--jp-shadow-ambient-color);
  --jp-elevation-z20: 0 10px 13px -6px var(--jp-shadow-umbra-color),
    0 20px 31px 3px var(--jp-shadow-penumbra-color),
    0 8px 38px 7px var(--jp-shadow-ambient-color);
  --jp-elevation-z24: 0 11px 15px -7px var(--jp-shadow-umbra-color),
    0 24px 38px 3px var(--jp-shadow-penumbra-color),
    0 9px 46px 8px var(--jp-shadow-ambient-color);

  /* Borders
   *
   * The following variables, specify the visual styling of borders in JupyterLab.
   */

  --jp-border-width: 1px;
  --jp-border-color0: var(--md-grey-400);
  --jp-border-color1: var(--md-grey-400);
  --jp-border-color2: var(--md-grey-300);
  --jp-border-color3: var(--md-grey-200);
  --jp-inverse-border-color: var(--md-grey-600);
  --jp-border-radius: 2px;

  /* UI Fonts
   *
   * The UI font CSS variables are used for the typography all of the JupyterLab
   * user interface elements that are not directly user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-ui-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-ui-font-scale-factor: 1.2;
  --jp-ui-font-size0: 0.83333em;
  --jp-ui-font-size1: 13px; /* Base font size */
  --jp-ui-font-size2: 1.2em;
  --jp-ui-font-size3: 1.44em;
  --jp-ui-font-family: system-ui, -apple-system, blinkmacsystemfont, 'Segoe UI',
    helvetica, arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji',
    'Segoe UI Symbol';

  /*
   * Use these font colors against the corresponding main layout colors.
   * In a light theme, these go from dark to light.
   */

  /* Defaults use Material Design specification */
  --jp-ui-font-color0: rgba(0, 0, 0, 1);
  --jp-ui-font-color1: rgba(0, 0, 0, 0.87);
  --jp-ui-font-color2: rgba(0, 0, 0, 0.54);
  --jp-ui-font-color3: rgba(0, 0, 0, 0.38);

  /*
   * Use these against the brand/accent/warn/error colors.
   * These will typically go from light to darker, in both a dark and light theme.
   */

  --jp-ui-inverse-font-color0: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color1: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color2: rgba(255, 255, 255, 0.7);
  --jp-ui-inverse-font-color3: rgba(255, 255, 255, 0.5);

  /* Content Fonts
   *
   * Content font variables are used for typography of user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-content-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-content-line-height: 1.6;
  --jp-content-font-scale-factor: 1.2;
  --jp-content-font-size0: 0.83333em;
  --jp-content-font-size1: 14px; /* Base font size */
  --jp-content-font-size2: 1.2em;
  --jp-content-font-size3: 1.44em;
  --jp-content-font-size4: 1.728em;
  --jp-content-font-size5: 2.0736em;

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-content-presentation-font-size1: 17px;
  --jp-content-heading-line-height: 1;
  --jp-content-heading-margin-top: 1.2em;
  --jp-content-heading-margin-bottom: 0.8em;
  --jp-content-heading-font-weight: 500;

  /* Defaults use Material Design specification */
  --jp-content-font-color0: rgba(0, 0, 0, 1);
  --jp-content-font-color1: rgba(0, 0, 0, 0.87);
  --jp-content-font-color2: rgba(0, 0, 0, 0.54);
  --jp-content-font-color3: rgba(0, 0, 0, 0.38);
  --jp-content-link-color: var(--md-blue-900);
  --jp-content-font-family: system-ui, -apple-system, blinkmacsystemfont,
    'Segoe UI', helvetica, arial, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol';

  /*
   * Code Fonts
   *
   * Code font variables are used for typography of code and other monospaces content.
   */

  --jp-code-font-size: 13px;
  --jp-code-line-height: 1.3077; /* 17px for 13px base */
  --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */
  --jp-code-font-family-default: menlo, consolas, 'DejaVu Sans Mono', monospace;
  --jp-code-font-family: var(--jp-code-font-family-default);

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-code-presentation-font-size: 16px;

  /* may need to tweak cursor width if you change font size */
  --jp-code-cursor-width0: 1.4px;
  --jp-code-cursor-width1: 2px;
  --jp-code-cursor-width2: 4px;

  /* Layout
   *
   * The following are the main layout colors use in JupyterLab. In a light
   * theme these would go from light to dark.
   */

  --jp-layout-color0: white;
  --jp-layout-color1: white;
  --jp-layout-color2: var(--md-grey-200);
  --jp-layout-color3: var(--md-grey-400);
  --jp-layout-color4: var(--md-grey-600);

  /* Inverse Layout
   *
   * The following are the inverse layout colors use in JupyterLab. In a light
   * theme these would go from dark to light.
   */

  --jp-inverse-layout-color0: #111;
  --jp-inverse-layout-color1: var(--md-grey-900);
  --jp-inverse-layout-color2: var(--md-grey-800);
  --jp-inverse-layout-color3: var(--md-grey-700);
  --jp-inverse-layout-color4: var(--md-grey-600);

  /* Brand/accent */

  --jp-brand-color0: var(--md-blue-900);
  --jp-brand-color1: var(--md-blue-700);
  --jp-brand-color2: var(--md-blue-300);
  --jp-brand-color3: var(--md-blue-100);
  --jp-brand-color4: var(--md-blue-50);
  --jp-accent-color0: var(--md-green-900);
  --jp-accent-color1: var(--md-green-700);
  --jp-accent-color2: var(--md-green-300);
  --jp-accent-color3: var(--md-green-100);

  /* State colors (warn, error, success, info) */

  --jp-warn-color0: var(--md-orange-900);
  --jp-warn-color1: var(--md-orange-700);
  --jp-warn-color2: var(--md-orange-300);
  --jp-warn-color3: var(--md-orange-100);
  --jp-error-color0: var(--md-red-900);
  --jp-error-color1: var(--md-red-700);
  --jp-error-color2: var(--md-red-300);
  --jp-error-color3: var(--md-red-100);
  --jp-success-color0: var(--md-green-900);
  --jp-success-color1: var(--md-green-700);
  --jp-success-color2: var(--md-green-300);
  --jp-success-color3: var(--md-green-100);
  --jp-info-color0: var(--md-cyan-900);
  --jp-info-color1: var(--md-cyan-700);
  --jp-info-color2: var(--md-cyan-300);
  --jp-info-color3: var(--md-cyan-100);

  /* Cell specific styles */

  --jp-cell-padding: 5px;
  --jp-cell-collapser-width: 8px;
  --jp-cell-collapser-min-height: 20px;
  --jp-cell-collapser-not-active-hover-opacity: 0.6;
  --jp-cell-editor-background: var(--md-grey-100);
  --jp-cell-editor-border-color: var(--md-grey-300);
  --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-cell-editor-active-background: var(--jp-layout-color0);
  --jp-cell-editor-active-border-color: var(--jp-brand-color1);
  --jp-cell-prompt-width: 64px;
  --jp-cell-prompt-font-family: var(--jp-code-font-family-default);
  --jp-cell-prompt-letter-spacing: 0;
  --jp-cell-prompt-opacity: 1;
  --jp-cell-prompt-not-active-opacity: 0.5;
  --jp-cell-prompt-not-active-font-color: var(--md-grey-700);

  /* A custom blend of MD grey and blue 600
   * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */
  --jp-cell-inprompt-font-color: #307fc1;

  /* A custom blend of MD grey and orange 600
   * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */
  --jp-cell-outprompt-font-color: #bf5b3d;

  /* Notebook specific styles */

  --jp-notebook-padding: 10px;
  --jp-notebook-select-background: var(--jp-layout-color1);
  --jp-notebook-multiselected-color: var(--md-blue-50);

  /* The scroll padding is calculated to fill enough space at the bottom of the
  notebook to show one single-line cell (with appropriate padding) at the top
  when the notebook is scrolled all the way to the bottom. We also subtract one
  pixel so that no scrollbar appears if we have just one single-line cell in the
  notebook. This padding is to enable a 'scroll past end' feature in a notebook.
  */
  --jp-notebook-scroll-padding: calc(
    100% - var(--jp-code-font-size) * var(--jp-code-line-height) -
      var(--jp-code-padding) - var(--jp-cell-padding) - 1px
  );

  /* Rendermime styles */

  --jp-rendermime-error-background: #fdd;
  --jp-rendermime-table-row-background: var(--md-grey-100);
  --jp-rendermime-table-row-hover-background: var(--md-light-blue-50);

  /* Dialog specific styles */

  --jp-dialog-background: rgba(0, 0, 0, 0.25);

  /* Console specific styles */

  --jp-console-padding: 10px;

  /* Toolbar specific styles */

  --jp-toolbar-border-color: var(--jp-border-color1);
  --jp-toolbar-micro-height: 8px;
  --jp-toolbar-background: var(--jp-layout-color1);
  --jp-toolbar-box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.24);
  --jp-toolbar-header-margin: 4px 4px 0 4px;
  --jp-toolbar-active-background: var(--md-grey-300);

  /* Statusbar specific styles */

  --jp-statusbar-height: 24px;

  /* Input field styles */

  --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-input-active-background: var(--jp-layout-color1);
  --jp-input-hover-background: var(--jp-layout-color1);
  --jp-input-background: var(--md-grey-100);
  --jp-input-border-color: var(--jp-inverse-border-color);
  --jp-input-active-border-color: var(--jp-brand-color1);
  --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3);

  /* General editor styles */

  --jp-editor-selected-background: #d9d9d9;
  --jp-editor-selected-focused-background: #d7d4f0;
  --jp-editor-cursor-color: var(--jp-ui-font-color0);

  /* Code mirror specific styles */

  --jp-mirror-editor-keyword-color: #008000;
  --jp-mirror-editor-atom-color: #88f;
  --jp-mirror-editor-number-color: #080;
  --jp-mirror-editor-def-color: #00f;
  --jp-mirror-editor-variable-color: var(--md-grey-900);
  --jp-mirror-editor-variable-2-color: rgb(0, 54, 109);
  --jp-mirror-editor-variable-3-color: #085;
  --jp-mirror-editor-punctuation-color: #05a;
  --jp-mirror-editor-property-color: #05a;
  --jp-mirror-editor-operator-color: #a2f;
  --jp-mirror-editor-comment-color: #408080;
  --jp-mirror-editor-string-color: #ba2121;
  --jp-mirror-editor-string-2-color: #708;
  --jp-mirror-editor-meta-color: #a2f;
  --jp-mirror-editor-qualifier-color: #555;
  --jp-mirror-editor-builtin-color: #008000;
  --jp-mirror-editor-bracket-color: #997;
  --jp-mirror-editor-tag-color: #170;
  --jp-mirror-editor-attribute-color: #00c;
  --jp-mirror-editor-header-color: blue;
  --jp-mirror-editor-quote-color: #090;
  --jp-mirror-editor-link-color: #00c;
  --jp-mirror-editor-error-color: #f00;
  --jp-mirror-editor-hr-color: #999;

  /*
    RTC user specific colors.
    These colors are used for the cursor, username in the editor,
    and the icon of the user.
  */

  --jp-collaborator-color1: #ffad8e;
  --jp-collaborator-color2: #dac83d;
  --jp-collaborator-color3: #72dd76;
  --jp-collaborator-color4: #00e4d0;
  --jp-collaborator-color5: #45d4ff;
  --jp-collaborator-color6: #e2b1ff;
  --jp-collaborator-color7: #ff9de6;

  /* Vega extension styles */

  --jp-vega-background: white;

  /* Sidebar-related styles */

  --jp-sidebar-min-width: 250px;

  /* Search-related styles */

  --jp-search-toggle-off-opacity: 0.5;
  --jp-search-toggle-hover-opacity: 0.8;
  --jp-search-toggle-on-opacity: 1;
  --jp-search-selected-match-background-color: rgb(245, 200, 0);
  --jp-search-selected-match-color: black;
  --jp-search-unselected-match-background-color: var(
    --jp-inverse-layout-color0
  );
  --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0);

  /* Icon colors that work well with light or dark backgrounds */
  --jp-icon-contrast-color0: var(--md-purple-600);
  --jp-icon-contrast-color1: var(--md-green-600);
  --jp-icon-contrast-color2: var(--md-pink-600);
  --jp-icon-contrast-color3: var(--md-blue-600);

  /* Button colors */
  --jp-accept-color-normal: var(--md-blue-700);
  --jp-accept-color-hover: var(--md-blue-800);
  --jp-accept-color-active: var(--md-blue-900);
  --jp-warn-color-normal: var(--md-red-700);
  --jp-warn-color-hover: var(--md-red-800);
  --jp-warn-color-active: var(--md-red-900);
  --jp-reject-color-normal: var(--md-grey-600);
  --jp-reject-color-hover: var(--md-grey-700);
  --jp-reject-color-active: var(--md-grey-800);

  /* File or activity icons and switch semantic variables */
  --jp-jupyter-icon-color: #f37626;
  --jp-notebook-icon-color: #f37626;
  --jp-json-icon-color: var(--md-orange-700);
  --jp-console-icon-background-color: var(--md-blue-700);
  --jp-console-icon-color: white;
  --jp-terminal-icon-background-color: var(--md-grey-800);
  --jp-terminal-icon-color: var(--md-grey-200);
  --jp-text-editor-icon-color: var(--md-grey-700);
  --jp-inspector-icon-color: var(--md-grey-700);
  --jp-switch-color: var(--md-grey-400);
  --jp-switch-true-position-color: var(--md-orange-900);
}
</style>
<style type="text/css">
/* Force rendering true colors when outputing to pdf */
* {
  -webkit-print-color-adjust: exact;
}

/* Misc */
a.anchor-link {
  display: none;
}

/* Input area styling */
.jp-InputArea {
  overflow: hidden;
}

.jp-InputArea-editor {
  overflow: hidden;
}

.cm-editor.cm-s-jupyter .highlight pre {
/* weird, but --jp-code-padding defined to be 5px but 4px horizontal padding is hardcoded for pre.cm-line */
  padding: var(--jp-code-padding) 4px;
  margin: 0;

  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
  color: inherit;

}

.jp-OutputArea-output pre {
  line-height: inherit;
  font-family: inherit;
}

.jp-RenderedText pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
}

/* Hiding the collapser by default */
.jp-Collapser {
  display: none;
}

@page {
    margin: 0.5in; /* Margin for each printed piece of paper */
}

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}
</style>
<!-- Load mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML-full,Safe"> </script>
<!-- MathJax configuration -->
<script type="text/x-mathjax-config">
    init_mathjax = function() {
        if (window.MathJax) {
        // MathJax loaded
            MathJax.Hub.Config({
                TeX: {
                    equationNumbers: {
                    autoNumber: "AMS",
                    useLabelIds: true
                    }
                },
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true,
                    processEnvironments: true
                },
                displayAlign: 'center',
                CommonHTML: {
                    linebreaks: {
                    automatic: true
                    }
                }
            });

            MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
        }
    }
    init_mathjax();
    </script>
<!-- End of mathjax configuration --><script type="module">
  document.addEventListener("DOMContentLoaded", async () => {
    const diagrams = document.querySelectorAll(".jp-Mermaid > pre.mermaid");
    // do not load mermaidjs if not needed
    if (!diagrams.length) {
      return;
    }
    const mermaid = (await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs")).default;
    const parser = new DOMParser();

    mermaid.initialize({
      maxTextSize: 100000,
      maxEdges: 100000,
      startOnLoad: false,
      fontFamily: window
        .getComputedStyle(document.body)
        .getPropertyValue("--jp-ui-font-family"),
      theme: document.querySelector("body[data-jp-theme-light='true']")
        ? "default"
        : "dark",
    });

    let _nextMermaidId = 0;

    function makeMermaidImage(svg) {
      const img = document.createElement("img");
      const doc = parser.parseFromString(svg, "image/svg+xml");
      const svgEl = doc.querySelector("svg");
      const { maxWidth } = svgEl?.style || {};
      const firstTitle = doc.querySelector("title");
      const firstDesc = doc.querySelector("desc");

      img.setAttribute("src", `data:image/svg+xml,${encodeURIComponent(svg)}`);
      if (maxWidth) {
        img.width = parseInt(maxWidth);
      }
      if (firstTitle) {
        img.setAttribute("alt", firstTitle.textContent);
      }
      if (firstDesc) {
        const caption = document.createElement("figcaption");
        caption.className = "sr-only";
        caption.textContent = firstDesc.textContent;
        return [img, caption];
      }
      return [img];
    }

    async function makeMermaidError(text) {
      let errorMessage = "";
      try {
        await mermaid.parse(text);
      } catch (err) {
        errorMessage = `${err}`;
      }

      const result = document.createElement("details");
      result.className = 'jp-RenderedMermaid-Details';
      const summary = document.createElement("summary");
      summary.className = 'jp-RenderedMermaid-Summary';
      const pre = document.createElement("pre");
      const code = document.createElement("code");
      code.innerText = text;
      pre.appendChild(code);
      summary.appendChild(pre);
      result.appendChild(summary);

      const warning = document.createElement("pre");
      warning.innerText = errorMessage;
      result.appendChild(warning);
      return [result];
    }

    async function renderOneMarmaid(src) {
      const id = `jp-mermaid-${_nextMermaidId++}`;
      const parent = src.parentNode;
      let raw = src.textContent.trim();
      const el = document.createElement("div");
      el.style.visibility = "hidden";
      document.body.appendChild(el);
      let results = null;
      let output = null;
      try {
        let { svg } = await mermaid.render(id, raw, el);
        svg = cleanMermaidSvg(svg);
        results = makeMermaidImage(svg);
        output = document.createElement("figure");
        results.map(output.appendChild, output);
      } catch (err) {
        parent.classList.add("jp-mod-warning");
        results = await makeMermaidError(raw);
        output = results[0];
      } finally {
        el.remove();
      }
      parent.classList.add("jp-RenderedMermaid");
      parent.appendChild(output);
    }


    /**
     * Post-process to ensure mermaid diagrams contain only valid SVG and XHTML.
     */
    function cleanMermaidSvg(svg) {
      return svg.replace(RE_VOID_ELEMENT, replaceVoidElement);
    }


    /**
     * A regular expression for all void elements, which may include attributes and
     * a slash.
     *
     * @see https://developer.mozilla.org/en-US/docs/Glossary/Void_element
     *
     * Of these, only `<br>` is generated by Mermaid in place of `\n`,
     * but _any_ "malformed" tag will break the SVG rendering entirely.
     */
    const RE_VOID_ELEMENT =
      /<\s*(area|base|br|col|embed|hr|img|input|link|meta|param|source|track|wbr)\s*([^>]*?)\s*>/gi;

    /**
     * Ensure a void element is closed with a slash, preserving any attributes.
     */
    function replaceVoidElement(match, tag, rest) {
      rest = rest.trim();
      if (!rest.endsWith('/')) {
        rest = `${rest} /`;
      }
      return `<${tag} ${rest}>`;
    }

    void Promise.all([...diagrams].map(renderOneMarmaid));
  });
</script>
<style>
  .jp-Mermaid:not(.jp-RenderedMermaid) {
    display: none;
  }

  .jp-RenderedMermaid {
    overflow: auto;
    display: flex;
  }

  .jp-RenderedMermaid.jp-mod-warning {
    width: auto;
    padding: 0.5em;
    margin-top: 0.5em;
    border: var(--jp-border-width) solid var(--jp-warn-color2);
    border-radius: var(--jp-border-radius);
    color: var(--jp-ui-font-color1);
    font-size: var(--jp-ui-font-size1);
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  .jp-RenderedMermaid figure {
    margin: 0;
    overflow: auto;
    max-width: 100%;
  }

  .jp-RenderedMermaid img {
    max-width: 100%;
  }

  .jp-RenderedMermaid-Details > pre {
    margin-top: 1em;
  }

  .jp-RenderedMermaid-Summary {
    color: var(--jp-warn-color2);
  }

  .jp-RenderedMermaid:not(.jp-mod-warning) pre {
    display: none;
  }

  .jp-RenderedMermaid-Summary > pre {
    display: inline-block;
    white-space: normal;
  }
</style>
<!-- End of mermaid configuration --></head>
<body class="jp-Notebook" data-jp-theme-light="true" data-jp-theme-name="JupyterLab Light">
<main>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=73c2d2c9-4a4b-48ca-90a3-1ccd120ca08b">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h1 id="Fine-tuning-using-Llama-3-70B">Fine tuning using Llama 3 70B<a class="anchor-link" href="#Fine-tuning-using-Llama-3-70B"></a></h1>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=5b5bef3d-89b9-42ed-b158-a39bd61f6a31">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Base-Model-and-Quantization">Base Model and Quantization<a class="anchor-link" href="#Base-Model-and-Quantization"></a></h3>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=938123ed-15b0-45ee-b622-d9bbe5fe3a48">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>4-bit quantization alongside specific computational optimizations</p>
<p>load_in_4bit: This option likely enables loading and storing tensors in 4-bit precision. This can significantly reduce memory usage at the cost of precision. Enabling this suggests that you're optimizing for memory efficiency, potentially to fit larger models or datasets in memory.</p>
<p>bnb_4bit_use_double_quant: This indicates the use of a double quantization process for 4-bit representation. Double quantization might be used to improve the precision of the 4-bit quantized values, potentially mitigating some of the precision loss associated with low-bit quantization.</p>
<p>bnb_4bit_quant_type="nf4": This specifies the quantization type or algorithm used for converting tensors to 4-bit representations. "nf4" might refer to a specific quantization scheme optimized for neural network weights and activations. The exact nature of "nf4" would depend on the documentation of the BitsAndBytes library, but it suggests an approach tailored to maintain as much information as possible within the 4-bit limitation.</p>
<p>bnb_4bit_compute_dtype=torch.bfloat16: This sets the data type for computations using 4-bit quantized tensors to torch.bfloat16, which is a 16-bit floating-point representation that offers a good balance between precision and memory usage. By performing computations in bfloat16, the configuration aims to maintain computational accuracy and efficiency, particularly on hardware that supports bfloat16 operations natively.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=dfe5f014-527c-4a83-863b-4b6330c72ed5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[1]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># GPU 0: NVIDIA GeForce RTX 4090</span>
<span class="c1"># GPU 1: NVIDIA GeForce RTX 4090</span>
<span class="c1"># GPU 2: NVIDIA GeForce RTX 4090</span>
<span class="c1"># GPU 3: NVIDIA GeForce RTX 3090 Ti</span>
<span class="c1"># GPU 4: NVIDIA GeForce RTX 3090 Ti</span>
<span class="c1"># GPU 5: NVIDIA GeForce RTX 3090</span>
<span class="c1"># GPU 6: NVIDIA GeForce RTX 3090</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="c1"># os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # ""makes all visible, "0" GPU 0 visible</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=53ffaa11-3936-40a0-8f2a-3d083ff2afef">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Supress-warnings">Supress warnings<a class="anchor-link" href="#Supress-warnings"></a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=e77f7e3d-3af3-4dc8-9571-d13398c29ee9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[2]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">UserWarning</span><span class="p">)</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">FutureWarning</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=ef535c7a-848e-4c6e-a692-208f98610d82">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Inspect-the-base-model">Inspect the base model<a class="anchor-link" href="#Inspect-the-base-model"></a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=8bfef1a8-c05a-4e14-af0e-358bfb04352a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[3]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">access_token</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HF_TOKEN"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=89cce100-9b2e-4675-bfda-f029e7c4056e">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[4]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">login</span>
<span class="n">login</span><span class="p">(</span><span class="n">access_token</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>VBox(children=(HTML(value='&lt;center&gt; &lt;img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=15ec782b-0776-4354-ad08-66f1e9e50187">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[5]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s2">"Meta-Llama-3-70B"</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"meta-llama/"</span><span class="o">+</span><span class="n">model_name</span>

<span class="n">bnb_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span>
    <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="s2">"nf4"</span><span class="p">,</span>
    <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=cbf95826-7975-4943-962c-ccfa26f31030">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[6]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">bnb_config</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">token</span><span class="o">=</span><span class="n">access_token</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Loading checkpoint shards:   0%|          | 0/30 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-70B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=83f74939-3ab3-4011-90cb-8e90c22ea162">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[7]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torchinfo</span> <span class="kn">import</span> <span class="n">summary</span>
<span class="n">summary</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[7]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>================================================================================
Layer (type:depth-idx)                                  Param #
================================================================================
LlamaForSequenceClassification                          --
LlamaModel: 1-1                                       --
    Embedding: 2-1                                   1,050,673,152
    ModuleList: 2-2                                  --
        LlamaDecoderLayer: 3-1                      427,835,392
        LlamaDecoderLayer: 3-2                      427,835,392
        LlamaDecoderLayer: 3-3                      427,835,392
        LlamaDecoderLayer: 3-4                      427,835,392
        LlamaDecoderLayer: 3-5                      427,835,392
        LlamaDecoderLayer: 3-6                      427,835,392
        LlamaDecoderLayer: 3-7                      427,835,392
        LlamaDecoderLayer: 3-8                      427,835,392
        LlamaDecoderLayer: 3-9                      427,835,392
        LlamaDecoderLayer: 3-10                     427,835,392
        LlamaDecoderLayer: 3-11                     427,835,392
        LlamaDecoderLayer: 3-12                     427,835,392
        LlamaDecoderLayer: 3-13                     427,835,392
        LlamaDecoderLayer: 3-14                     427,835,392
        LlamaDecoderLayer: 3-15                     427,835,392
        LlamaDecoderLayer: 3-16                     427,835,392
        LlamaDecoderLayer: 3-17                     427,835,392
        LlamaDecoderLayer: 3-18                     427,835,392
        LlamaDecoderLayer: 3-19                     427,835,392
        LlamaDecoderLayer: 3-20                     427,835,392
        LlamaDecoderLayer: 3-21                     427,835,392
        LlamaDecoderLayer: 3-22                     427,835,392
        LlamaDecoderLayer: 3-23                     427,835,392
        LlamaDecoderLayer: 3-24                     427,835,392
        LlamaDecoderLayer: 3-25                     427,835,392
        LlamaDecoderLayer: 3-26                     427,835,392
        LlamaDecoderLayer: 3-27                     427,835,392
        LlamaDecoderLayer: 3-28                     427,835,392
        LlamaDecoderLayer: 3-29                     427,835,392
        LlamaDecoderLayer: 3-30                     427,835,392
        LlamaDecoderLayer: 3-31                     427,835,392
        LlamaDecoderLayer: 3-32                     427,835,392
        LlamaDecoderLayer: 3-33                     427,835,392
        LlamaDecoderLayer: 3-34                     427,835,392
        LlamaDecoderLayer: 3-35                     427,835,392
        LlamaDecoderLayer: 3-36                     427,835,392
        LlamaDecoderLayer: 3-37                     427,835,392
        LlamaDecoderLayer: 3-38                     427,835,392
        LlamaDecoderLayer: 3-39                     427,835,392
        LlamaDecoderLayer: 3-40                     427,835,392
        LlamaDecoderLayer: 3-41                     427,835,392
        LlamaDecoderLayer: 3-42                     427,835,392
        LlamaDecoderLayer: 3-43                     427,835,392
        LlamaDecoderLayer: 3-44                     427,835,392
        LlamaDecoderLayer: 3-45                     427,835,392
        LlamaDecoderLayer: 3-46                     427,835,392
        LlamaDecoderLayer: 3-47                     427,835,392
        LlamaDecoderLayer: 3-48                     427,835,392
        LlamaDecoderLayer: 3-49                     427,835,392
        LlamaDecoderLayer: 3-50                     427,835,392
        LlamaDecoderLayer: 3-51                     427,835,392
        LlamaDecoderLayer: 3-52                     427,835,392
        LlamaDecoderLayer: 3-53                     427,835,392
        LlamaDecoderLayer: 3-54                     427,835,392
        LlamaDecoderLayer: 3-55                     427,835,392
        LlamaDecoderLayer: 3-56                     427,835,392
        LlamaDecoderLayer: 3-57                     427,835,392
        LlamaDecoderLayer: 3-58                     427,835,392
        LlamaDecoderLayer: 3-59                     427,835,392
        LlamaDecoderLayer: 3-60                     427,835,392
        LlamaDecoderLayer: 3-61                     427,835,392
        LlamaDecoderLayer: 3-62                     427,835,392
        LlamaDecoderLayer: 3-63                     427,835,392
        LlamaDecoderLayer: 3-64                     427,835,392
        LlamaDecoderLayer: 3-65                     427,835,392
        LlamaDecoderLayer: 3-66                     427,835,392
        LlamaDecoderLayer: 3-67                     427,835,392
        LlamaDecoderLayer: 3-68                     427,835,392
        LlamaDecoderLayer: 3-69                     427,835,392
        LlamaDecoderLayer: 3-70                     427,835,392
        LlamaDecoderLayer: 3-71                     427,835,392
        LlamaDecoderLayer: 3-72                     427,835,392
        LlamaDecoderLayer: 3-73                     427,835,392
        LlamaDecoderLayer: 3-74                     427,835,392
        LlamaDecoderLayer: 3-75                     427,835,392
        LlamaDecoderLayer: 3-76                     427,835,392
        LlamaDecoderLayer: 3-77                     427,835,392
        LlamaDecoderLayer: 3-78                     427,835,392
        LlamaDecoderLayer: 3-79                     427,835,392
        LlamaDecoderLayer: 3-80                     427,835,392
    LlamaRMSNorm: 2-3                                8,192
Linear: 1-2                                           16,384
================================================================================
Total params: 35,277,529,088
Trainable params: 1,052,008,448
Non-trainable params: 34,225,520,640
================================================================================</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=49bbe47c-4430-4f4a-90d8-1113bd320a18">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[8]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[8]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>LlamaForSequenceClassification(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 8192)
    (layers): ModuleList(
      (0-79): 80 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)
          (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)
          (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)
          (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=8192, out_features=28672, bias=False)
          (up_proj): Linear4bit(in_features=8192, out_features=28672, bias=False)
          (down_proj): Linear4bit(in_features=28672, out_features=8192, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (score): Linear(in_features=8192, out_features=2, bias=False)
)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=6627b1da-57d6-4a17-8ea3-746b5cb1f3d9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Load-the-news-dataset-from-pickle-file">Load the news dataset from pickle file<a class="anchor-link" href="#Load-the-news-dataset-from-pickle-file"></a></h3><p>If any of the check_files don't exist then load the pickle file</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=1cc372cc-0cae-4b49-a2d7-8e57f58244a7">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[9]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pickle</span>

<span class="n">base_path</span> <span class="o">=</span> <span class="s1">'./data/'</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">base_path</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">file_name</span> <span class="o">=</span> <span class="s1">'news_small_dataset.pkl'</span>
<span class="n">file_path</span> <span class="o">=</span> <span class="n">base_path</span><span class="o">+</span><span class="n">file_name</span>

<span class="k">def</span> <span class="nf">pickle_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">file_path</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="s1">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
        <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">file</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Dataset has been pickled to: </span><span class="si">{</span><span class="n">file_path</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">load_pickle_dataset</span><span class="p">(</span><span class="n">file_path</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="s1">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Dataset has been loaded from: </span><span class="si">{</span><span class="n">file_path</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dataset</span>

<span class="k">def</span> <span class="nf">check_files_exists</span><span class="p">(</span><span class="n">file_names</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">file_names</span><span class="p">:</span>
        <span class="n">file_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">base_path</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">file_path</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="kc">False</span>

<span class="c1"># if these files exist we do not want to load the news_dataset.pkl to tokenize and make these files</span>
<span class="n">check_files</span> <span class="o">=</span> <span class="p">[</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_train_ds.pkl'</span><span class="p">,</span> <span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_eval_ds.pkl'</span><span class="p">,</span> <span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_test_ds.pkl'</span><span class="p">]</span>

<span class="k">if</span> <span class="n">check_files_exists</span><span class="p">(</span><span class="n">check_files</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"At least one of the specified files already exists. Not loading new dataset."</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">news_split_ds</span> <span class="o">=</span> <span class="n">load_pickle_dataset</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">news_split_ds</span><span class="p">)</span>
    <span class="n">total_rows</span> <span class="o">=</span> <span class="p">(</span><span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'train'</span><span class="p">]</span><span class="o">.</span><span class="n">num_rows</span> <span class="o">+</span>
              <span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'eval'</span><span class="p">]</span><span class="o">.</span><span class="n">num_rows</span> <span class="o">+</span>
              <span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'test'</span><span class="p">]</span><span class="o">.</span><span class="n">num_rows</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Total number of rows:"</span><span class="p">,</span> <span class="n">total_rows</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Dataset loaded successfully."</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>At least one of the specified files already exists. Not loading new dataset.
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=8ad450a2-6cef-432e-9e63-7ff985b4726e">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Tokenization-of-data">Tokenization of data<a class="anchor-link" href="#Tokenization-of-data"></a></h3>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=186f093c-7487-44ae-ad95-9ee8d24f04c7">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>
return_tensors="pt": This argument configures the tokenizer to output PyTorch ("pt") tensors. If you're working with TensorFlow, you'd use "tf" instead, and for NumPy arrays, you could omit this argument or set return_tensors to None.
</p>
<p>
Direct Model Input: By converting the tokenized input into tensors, the output can be directly used as input to a PyTorch model, fitting seamlessly into the data processing pipeline for model training or inference.

<p>Handling of Batch Inputs: This approach also supports batch inputs. If you pass a list of texts to the tokenizer with return_tensors="pt", it will automatically pad the sequences to the maximum length in the batch, returning a tensor where the first dimension is the batch size.</p>
<p>Padding and Truncation: The padding=True and truncation=True arguments ensure that all sequences are padded to the same length (up to max_length) and are truncated if they exceed this length, which is important for processing sequences in batches.</p>
</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=88c3a38c-1d24-4a1e-8d96-1b7c2ce162c7">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[10]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">token</span><span class="o">=</span><span class="n">access_token</span><span class="p">)</span>

<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">tokenize_fn</span><span class="p">(</span><span class="n">news</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">news</span><span class="p">[</span><span class="s1">'article'</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=02efc4f0-742a-4838-887d-5556a26ae15f">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Tokenize-train,-evaluation,-and-test-datasets">Tokenize train, evaluation, and test datasets<a class="anchor-link" href="#Tokenize-train,-evaluation,-and-test-datasets"></a></h3><p>If any of the check files exist then don't run tokenization and save some time.
Else load the pickle files that already exist.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=0f3e3101-df30-4af2-9976-49ba0cf44d62">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[11]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">check_files_exists</span><span class="p">(</span><span class="n">check_files</span><span class="p">):</span>
    <span class="n">tokenized_train_ds</span> <span class="o">=</span> <span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'train'</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_fn</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">tokenized_eval_ds</span> <span class="o">=</span> <span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'eval'</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_fn</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">tokenized_test_ds</span> <span class="o">=</span> <span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'test'</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_fn</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">tokenized_train_ds</span><span class="o">.</span><span class="n">features</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">tokenized_eval_ds</span><span class="o">.</span><span class="n">features</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">tokenized_test_ds</span><span class="o">.</span><span class="n">features</span><span class="p">)</span>
    
    <span class="n">pickle_dataset</span><span class="p">(</span><span class="n">tokenized_train_ds</span><span class="p">,</span> <span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_train_ds.pkl'</span><span class="p">)</span>
    <span class="n">pickle_dataset</span><span class="p">(</span><span class="n">tokenized_eval_ds</span><span class="p">,</span> <span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_eval_ds.pkl'</span><span class="p">)</span>
    <span class="n">pickle_dataset</span><span class="p">(</span><span class="n">tokenized_test_ds</span><span class="p">,</span> <span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_test_ds.pkl'</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Files already exist, so load datasets"</span><span class="p">)</span>
    <span class="n">tokenized_train_ds</span> <span class="o">=</span> <span class="n">load_pickle_dataset</span><span class="p">(</span><span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_train_ds.pkl'</span><span class="p">)</span>
    <span class="n">tokenized_eval_ds</span> <span class="o">=</span> <span class="n">load_pickle_dataset</span><span class="p">(</span><span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_eval_ds.pkl'</span><span class="p">)</span>
    <span class="n">tokenized_test_ds</span> <span class="o">=</span> <span class="n">load_pickle_dataset</span><span class="p">(</span><span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_test_ds.pkl'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Files already exist, so load datasets
Dataset has been loaded from: ./data/Meta-Llama-3-70B-small_tokenized_train_ds.pkl
Dataset has been loaded from: ./data/Meta-Llama-3-70B-small_tokenized_eval_ds.pkl
Dataset has been loaded from: ./data/Meta-Llama-3-70B-small_tokenized_test_ds.pkl
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=307cc4fb-9766-4b0f-943b-4a1c90053e09">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Look-at-the-tokenized-data">Look at the tokenized data<a class="anchor-link" href="#Look-at-the-tokenized-data"></a></h3><p>Notice what the actual data looks like, and then the tokenized data which is a bunch of numbers, and then the attention mask at the end.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=44321182-e1b9-4570-bd24-7a5cf42ba504">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[12]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">count_train_records</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenized_train_ds</span><span class="p">)</span>
<span class="n">count_eval_records</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenized_eval_ds</span><span class="p">)</span>
<span class="n">count_test_records</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenized_test_ds</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of records in training dataset: </span><span class="si">{</span><span class="n">count_train_records</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of records in evaluation dataset: </span><span class="si">{</span><span class="n">count_eval_records</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of records in test dataset: </span><span class="si">{</span><span class="n">count_test_records</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">count_total_records</span> <span class="o">=</span> <span class="n">count_train_records</span> <span class="o">+</span> <span class="n">count_eval_records</span> <span class="o">+</span> <span class="n">count_test_records</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Total number of records: </span><span class="si">{</span><span class="n">count_total_records</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Number of records in training dataset: 33611
Number of records in evaluation dataset: 7203
Number of records in test dataset: 7203
Total number of records: 48017
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=64be678b-d2a8-403e-bcc8-ef9e7eabb0cf">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[13]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">first_record</span> <span class="o">=</span> <span class="n">tokenized_train_ds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">first_record</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>{'article': "In a year where homicides, rapes and robberies increased slightly, New York City still saw serious crime drop 1.7 percent in 2015, continuing an overall decline that began in the 1990s, NYPD Commissioner William Bratton said Monday.\nAt a news conference with Mayor Bill de Blasio, Bratton touted last years crime statistics, which he said, when combined with an even larger decline in 2014, put to rest the fear that substantial decreases couldnt continue under the new administration at City Hall.\nWhile we have had some fluctuation, some increases in certain categories, the overall trend in all our crime categories continues to go down, Bratton told reporters. It was a very good year for us, 2015.\nHomicides increased by 4.5 percent in 2015, rising to 350 from 333 in the prior year, which was the lowest since 1994, said Deputy Commissioner Dermot Shea. Rapes increased 6 percent and robberies rose 2 percent, said Shea, who is in charge of data collection and operations for the NYPD.\nThe lower overall crime statistics came about due to what Shea called targeted enforcement, where cops make quality arrests even though the overall number of apprehensions was the lowest in the city since 2003.\nTwo boroughs  Manhattan and the Bronx  actually saw serious crimes increase by 3 percent and 4 percent, respectively, Shea said. Manhattans increase was driven by more robberies, while the Bronx, although seeing an overall crime increase, had what he said was a phenomenal reduction in shootings. Citywide, shootings were down in 2015 about 3 percent, to 1,103 from 1,172 in 2014.\nShea largely attributed the 2015 increase in rapes to victims coming forward with complaints about attacks from years past.\nSign up to get the latest updates Get Newsday's Breaking News alerts in your inbox. By clicking Sign up, you agree to our privacy policy.\nTwenty percent of these rapes didnt happen in 2015, he said.\nThe NYPD has seen an increase in rapes involving single women who, after a night of drinking, get into cabs of all kinds and are attacked, Shea said.\nThey get driven, and passing out and waking up in a desolate area, and they get sexually attacked. This is something, really, that people need to be exceptionally aware of, and like any case in New York City, the buddy system works, said Shea, referring to the need for people to travel in pairs when taking a cab at night.\nBratton and police brass hope to build upon the continuing drop in overall crime by using technology such as ShotSpotter and a newly minted GPS system for police cars.\nJessica Tisch, NYPD deputy commissioner for technology, said ShotSpotter, an acoustical system that detects gunfire, identified gunshots in 1,672 cases, mostly in Brooklyn. Of those alerts, 74 percent didnt have any 911 calls from the public associated with them.\nTisch said ShotSpotter helped police recover ballistic evidence in 19 percent of the gunfire alerts. In 22 percent of those cases, Tisch said, cops were able to make positive matches of bullets with those from guns used in earlier shootings.\nTisch also highlighted a special GPS system being tried in about 5,000 patrol cars that allows the NYPD to see where its vehicles are and to track their movements over a 24-hour period, as well as gather information about the officers driving.\n", 'label': 0, 'input_ids': [128000, 644, 264, 1060, 1405, 89495, 11, 96330, 323, 71650, 552, 7319, 10284, 11, 1561, 4356, 4409, 2103, 5602, 6129, 9977, 6068, 220, 16, 13, 22, 3346, 304, 220, 679, 20, 11, 14691, 459, 8244, 18174, 430, 6137, 304, 279, 220, 2550, 15, 82, 11, 74255, 30454, 12656, 3320, 266, 783, 1071, 7159, 627, 1688, 264, 3754, 10017, 449, 22868, 8766, 409, 93493, 11, 3320, 266, 783, 67528, 1566, 1060, 753, 9977, 13443, 11, 902, 568, 1071, 11, 994, 11093, 449, 459, 1524, 8294, 18174, 304, 220, 679, 19, 11, 2231, 311, 2800, 279, 8850, 430, 12190, 43154, 7846, 1431, 3136, 1234, 279, 502, 8735, 520, 4409, 11166, 627, 2118, 8142, 584, 617, 1047, 1063, 39388, 4090, 11, 1063, 12992, 304, 3738, 11306, 11, 279, 8244, 9327, 304, 682, 1057, 9977, 11306, 9731, 311, 733, 1523, 2476, 3320, 266, 783, 3309, 19578, 13, 1054, 2181, 574, 264, 1633, 1695, 1060, 369, 603, 11, 220, 679, 20, 627, 39, 3151, 3422, 7319, 555, 220, 19, 13, 20, 3346, 304, 220, 679, 20, 11, 16448, 311, 220, 8652, 505, 220, 8765, 304, 279, 4972, 1060, 11, 902, 574, 279, 15821, 2533, 220, 2550, 19, 11, 1071, 32724, 30454, 76508, 354, 86068, 13, 432, 9521, 7319, 220, 21, 3346, 323, 71650, 552, 16392, 220, 17, 3346, 11, 1071, 86068, 11, 889, 374, 304, 6900, 315, 828, 4526, 323, 7677, 369, 279, 74255, 627, 791, 4827, 8244, 9977, 13443, 3782, 922, 4245, 311, 1148, 86068, 2663, 1054, 5775, 291, 13627, 2476, 1405, 35317, 1304, 4367, 38811, 1524, 3582, 279, 8244, 1396, 315, 47291, 4769, 574, 279, 15821, 304, 279, 3363, 2533, 220, 1049, 18, 627, 11874, 66841, 82, 2001, 29890, 323, 279, 66236, 2001, 3604, 5602, 6129, 17073, 5376, 555, 220, 18, 3346, 323, 220, 19, 3346, 11, 15947, 11, 86068, 1071, 13, 29890, 753, 5376, 574, 16625, 555, 810, 71650, 552, 11, 1418, 279, 66236, 11, 8051, 9298, 459, 8244, 9977, 5376, 11, 1047, 1148, 568, 1071, 574, 264, 1054, 15112, 6431, 278, 863, 14278, 304, 44861, 13, 4409, 9328, 11, 44861, 1051, 1523, 304, 220, 679, 20, 922, 220, 18, 3346, 11, 311, 220, 16, 11, 6889, 505, 220, 16, 11, 10861, 304, 220, 679, 19, 627, 8100, 64, 14090, 30706, 279, 220, 679, 20, 5376, 304, 96330, 311, 12697, 5108, 4741, 449, 21859, 922, 8951, 505, 1667, 3347, 627, 7412, 709, 311, 636, 279, 5652, 9013, 2175, 5513, 1316, 596, 52624, 5513, 30350, 304, 701, 23732, 13, 3296, 18965, 7220, 709, 11, 499, 7655, 311, 1057, 12625, 4947, 627, 2118, 76896, 3346, 315, 1521, 96330, 3287, 1431, 3621, 304, 220, 679, 20, 2476, 568, 1071, 627, 791, 74255, 706, 3970, 459, 5376, 304, 96330, 16239, 3254, 3278, 889, 11, 1306, 264, 3814, 315, 16558, 11, 636, 1139, 272, 3518, 315, 682, 13124, 323, 527, 18855, 11, 86068, 1071, 627, 46690, 636, 16625, 11, 323, 12579, 704, 323, 48728, 709, 304, 264, 951, 34166, 3158, 11, 323, 814, 636, 27681, 18855, 13, 1115, 374, 2555, 11, 2216, 11, 430, 1274, 1205, 311, 387, 48298, 8010, 315, 11, 323, 1093, 904, 1162, 304, 1561, 4356], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=b2d7f4f8-9eb3-4feb-86e7-fb0dad88753f">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Turn-on-accelerate">Turn on accelerate<a class="anchor-link" href="#Turn-on-accelerate"></a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=f3eab29e-fa47-4a13-abc4-d6425ae741b6">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[14]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">accelerate</span> <span class="kn">import</span> <span class="n">FullyShardedDataParallelPlugin</span><span class="p">,</span> <span class="n">Accelerator</span>
<span class="kn">from</span> <span class="nn">torch.distributed.fsdp.fully_sharded_data_parallel</span> <span class="kn">import</span> <span class="n">FullOptimStateDictConfig</span><span class="p">,</span> <span class="n">FullStateDictConfig</span>

<span class="n">fsdp_plugin</span> <span class="o">=</span> <span class="n">FullyShardedDataParallelPlugin</span><span class="p">(</span>
    <span class="n">state_dict_config</span><span class="o">=</span><span class="n">FullStateDictConfig</span><span class="p">(</span><span class="n">offload_to_cpu</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rank0_only</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="n">optim_state_dict_config</span><span class="o">=</span><span class="n">FullOptimStateDictConfig</span><span class="p">(</span><span class="n">offload_to_cpu</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rank0_only</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
<span class="p">)</span>

<span class="n">accelerator</span> <span class="o">=</span> <span class="n">Accelerator</span><span class="p">(</span><span class="n">fsdp_plugin</span><span class="o">=</span><span class="n">fsdp_plugin</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=22e040f1-e596-476d-9f26-ffa6f8a4a548">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="LoRA---Low-Rank-Adaptation">LoRA - Low-Rank Adaptation<a class="anchor-link" href="#LoRA---Low-Rank-Adaptation"></a></h3>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=bc8b8091-6179-4df0-b5bd-b29ec2dde4d5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>LoRA, short for Low-Rank Adaptation, is a technique designed to efficiently fine-tune large pre-trained models without the need to update all the model parameters, significantly reducing the computational and memory overhead typically associated with training. LoRA targets the challenge of adapting massive models, particularly in natural language processing (NLP) and computer vision, to specialized tasks while keeping the resource requirements manageable.</p>
<p>
Gradient checkpointing for the model. Gradient checkpointing is a technique used to reduce memory usage during the training of deep neural networks by trading compute for memory. It works by storing a minimal set of intermediate activations during the forward pass and then recomputing the others during the backward pass. This is particularly useful for training large models or using larger batch sizes.
</p>
<p>
The forward pass is the process where the input data is passed through the network from the input layer to the output layer. During this pass, the network performs a series of computations at each layer, applying weights to the inputs, adding biases (if applicable), and passing the result through an activation function. The final output of the forward pass is the prediction made by the network. The main goal of the forward pass is to compute the output given the current state of the model's parameters (weights and biases). This output is then used to calculate the loss, which quantifies how well the model's predictions match the actual labels.
</p>
<p>
The backward pass, or backpropagation, is the process of computing the gradient of the loss function with respect to each parameter in the network. This involves applying the chain rule of calculus to take derivatives step-by-step from the output layer back to the input layer. Essentially, it calculates how much each parameter contributed to the error in the prediction. The purpose of the backward pass is to update the model's parameters in a way that minimally reduces the loss, improving the model's predictions. The gradients calculated during this pass indicate the direction in which each parameter should be adjusted to decrease the error. Using an optimization algorithm (e.g., Stochastic Gradient Descent), these gradients are then used to update the weights to minimize the loss.
</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=41956597-0441-4bc6-aefe-fc4b0d5349c5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>
r: This parameter specifies the rank of the low-rank matrices that are introduced by LoRA. A smaller rank means fewer parameters to train, leading to a more memory-efficient fine-tuning process.
</p>
<p>
lora_alpha: This multiplier adjusts the scale of the LoRA parameters. A higher value increases the capacity of the LoRA adjustments to the original model weights.
</p>
<p>
target_modules: Lists the specific parts of the model to which LoRA will be applied. These typically correspond to components within transformer blocks, such as the query, key, value, and output projections in attention mechanisms, as well as any additional modules relevant to the model architecture.
</p>
<p>
bias: Specifies how biases are treated in the adaptation process. In this case, biases are not adjusted ("none").
</p>
<p>
lora_dropout: Sets the dropout rate for the LoRA parameters, helping to prevent overfitting during fine-tuning. The dropout rate is a hyperparameter used in the training of neural networks, representing the probability that a given neuron (or unit) is temporarily "dropped" from the network during a specific iteration of training. This means that the neuron will not participate in the forward pass and its contribution to the backward pass (gradient computation) is also ignored during that iteration. Dropout is applied randomly to a subset of neurons in the network at each training step.
</p>
<p>
task_type: Indicates the type of task for which the model is being fine-tuned. The example uses TaskType.SEQ_CLS, 
suggesting a sequence classification task, such as sentiment analysis or document classification. In my case a binary classification of machine versus human generated text.
</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=0532bdc6-4317-474b-859c-4e5d2fe6f1bd">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[15]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">prepare_model_for_kbit_training</span><span class="p">,</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">get_peft_model</span><span class="p">,</span> <span class="n">TaskType</span>

<span class="n">model</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">prepare_model_for_kbit_training</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span>
    <span class="n">r</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span>
        <span class="s2">"q_proj"</span><span class="p">,</span>
        <span class="s2">"k_proj"</span><span class="p">,</span>
        <span class="s2">"v_proj"</span><span class="p">,</span>
        <span class="s2">"o_proj"</span><span class="p">,</span>
        <span class="s2">"gate_proj"</span><span class="p">,</span>
        <span class="s2">"up_proj"</span><span class="p">,</span>
        <span class="s2">"down_proj"</span><span class="p">,</span>
        <span class="s2">"lm_head"</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="n">bias</span><span class="o">=</span><span class="s2">"none"</span><span class="p">,</span>
    <span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">task_type</span><span class="o">=</span><span class="n">TaskType</span><span class="o">.</span><span class="n">SEQ_CLS</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">accelerator</span><span class="o">.</span><span class="n">prepare_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=1cd29494-4453-4c01-b878-ef2f4533d34d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Inspect-the-model">Inspect the model<a class="anchor-link" href="#Inspect-the-model"></a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=d5058f6d-185d-45af-83b9-bb7f61d4bee3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[16]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">print_trainable_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">trainable_params</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">all_param</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
        <span class="n">all_param</span> <span class="o">+=</span> <span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="n">trainable_params</span> <span class="o">+=</span> <span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"trainable params: </span><span class="si">{</span><span class="n">trainable_params</span><span class="si">}</span><span class="s2"> || all params: </span><span class="si">{</span><span class="n">all_param</span><span class="si">}</span><span class="s2"> || trainable%: </span><span class="si">{</span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">trainable_params</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">all_param</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=b14fe8f2-eccd-4cfe-9d20-ef48bc178842">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[17]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">print_trainable_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">model</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>trainable params: 103563264 || all params: 35381092352 || trainable%: 0.29270793272765033
</pre>
</div>
</div>
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[17]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>PeftModelForSequenceClassification(
  (base_model): LoraModel(
    (model): LlamaForSequenceClassification(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 8192)
        (layers): ModuleList(
          (0-79): 80 x LlamaDecoderLayer(
            (self_attn): LlamaSdpaAttention(
              (q_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=8192, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=8192, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (v_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=8192, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=8192, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=28672, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=28672, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (up_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=8192, out_features=28672, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8192, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=28672, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (down_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=28672, out_features=8192, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=28672, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=8192, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm()
            (post_attention_layernorm): LlamaRMSNorm()
          )
        )
        (norm): LlamaRMSNorm()
      )
      (score): ModulesToSaveWrapper(
        (original_module): Linear(in_features=8192, out_features=2, bias=False)
        (modules_to_save): ModuleDict(
          (default): Linear(in_features=8192, out_features=2, bias=False)
        )
      )
    )
  )
)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=e17b4782-03f8-4335-bfda-2a65794bff2c">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Look-at-hardware">Look at hardware<a class="anchor-link" href="#Look-at-hardware"></a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=c27c3328-1926-4adc-8696-b3b343ec4afd">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[18]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Available GPUs: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"GPU </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_name</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Available GPUs: 7
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090
GPU 2: NVIDIA GeForce RTX 4090
GPU 3: NVIDIA GeForce RTX 3090 Ti
GPU 4: NVIDIA GeForce RTX 3090 Ti
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=bdf68d0e-8786-489d-a4b8-b7478513efea">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[19]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">!</span>nvidia-smi
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Mon Jun 10 04:47:01 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.67                 Driver Version: 550.67         CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 3090        Off |   00000000:01:00.0 Off |                  N/A |
| 31%   38C    P2            124W /  420W |    5069MiB /  24576MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA GeForce RTX 4090        Off |   00000000:02:00.0 Off |                  Off |
|  0%   48C    P2             72W /  450W |    8463MiB /  24564MiB |     18%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA GeForce RTX 3090 Ti     Off |   00000000:2B:00.0 Off |                  Off |
| 32%   43C    P2            101W /  450W |    5081MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA GeForce RTX 3090        Off |   00000000:41:00.0 Off |                  N/A |
| 32%   38C    P2            113W /  420W |    7227MiB /  24576MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA GeForce RTX 4090        Off |   00000000:42:00.0 Off |                  Off |
|  0%   52C    P2             58W /  450W |    5220MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA GeForce RTX 4090        Off |   00000000:61:00.0 Off |                  Off |
|  0%   45C    P2             46W /  450W |    5208MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA GeForce RTX 3090 Ti     Off |   00000000:62:00.0 Off |                  Off |
| 30%   40C    P2             97W /  450W |    5069MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A      2515      G   /usr/lib/xorg/Xorg                              4MiB |
|    0   N/A  N/A      4781      C   /usr/bin/python3                             5052MiB |
|    1   N/A  N/A      2515      G   /usr/lib/xorg/Xorg                             86MiB |
|    1   N/A  N/A      2599      G   /usr/bin/gnome-shell                           13MiB |
|    1   N/A  N/A      4781      C   /usr/bin/python3                             8346MiB |
|    2   N/A  N/A      2515      G   /usr/lib/xorg/Xorg                              4MiB |
|    2   N/A  N/A      4781      C   /usr/bin/python3                             5064MiB |
|    3   N/A  N/A      2515      G   /usr/lib/xorg/Xorg                              4MiB |
|    3   N/A  N/A      4781      C   /usr/bin/python3                             7210MiB |
|    4   N/A  N/A      2515      G   /usr/lib/xorg/Xorg                              4MiB |
|    4   N/A  N/A      4781      C   /usr/bin/python3                             5202MiB |
|    5   N/A  N/A      2515      G   /usr/lib/xorg/Xorg                              4MiB |
|    5   N/A  N/A      4781      C   /usr/bin/python3                             5190MiB |
|    6   N/A  N/A      2515      G   /usr/lib/xorg/Xorg                              4MiB |
|    6   N/A  N/A      4781      C   /usr/bin/python3                             5052MiB |
+-----------------------------------------------------------------------------------------+
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=2340f2d1-1cc7-454e-bad6-bf4ac2c46fc9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[20]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">model</span><span class="o">.</span><span class="n">is_parallelizable</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">model</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=1199841a-6519-4422-8259-2fdbb114e466">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[21]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span><span class="p">,</span> <span class="n">accuracy_score</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">logits_and_labels</span><span class="p">):</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">logits_and_labels</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
    <span class="n">f1</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">'macro'</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">'accuracy'</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span> <span class="s1">'f1'</span><span class="p">:</span> <span class="n">f1</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=b8c7f3ee-6997-4ed7-b28e-5d574831fb08">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[22]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">project_name</span> <span class="o">=</span> <span class="s2">"praxis-"</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s2">"-small-finetune"</span>
<span class="n">output_dir_path</span> <span class="o">=</span> <span class="s2">"./"</span> <span class="o">+</span> <span class="n">project_name</span>
<span class="n">output_dir_path</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[22]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>'./praxis-Meta-Llama-3-70B-small-finetune'</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=e7a69bde-1d50-46a0-9838-00812afbdb16">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[23]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=ca48a7aa-41d7-4c57-bfec-093d22bf4b54">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p><strong>output_dir</strong> (<code>output_dir_path</code>): This specifies the directory where outputs such as model checkpoints and logs will be saved. It's important for organizing the outputs of your training sessions.</p>
<p><strong>warmup_steps</strong> (<code>5</code>): This parameter sets the number of steps during which the learning rate will gradually increase from zero to the initially set learning rate. This warmup phase helps stabilize the model's training early on, preventing the model from diverging due to high gradient values at the start.</p>
<p><strong>per_device_train_batch_size</strong> (<code>32</code>): This sets the number of training examples to process on each device (like a GPU) during training. A higher batch size can speed up training but may require more memory.</p>
<p><strong>per_device_eval_batch_size</strong> (<code>32</code>): Similar to the training batch size, this is the number of examples to process on each device during evaluation. It determines how quickly the model can process the evaluation data.</p>
<p><strong>num_train_epochs</strong> (<code>10</code>): This defines the total number of times the training process should iterate over the entire dataset. More epochs can lead to better learning but also risk overfitting if too high.</p>
<p><strong>gradient_checkpointing</strong> (<code>False</code>): If set to <code>True</code>, this would enable gradient checkpointing to reduce memory usage at the cost of longer training time. It's useful for very deep models that otherwise would not fit into GPU memory.</p>
<p><strong>gradient_accumulation_steps</strong> (<code>2</code>): This setting allows you to accumulate gradients over multiple steps before performing an update on the model's weights. It's a way to effectively increase the batch size without increasing the memory load, which can be helpful when dealing with hardware constraints.</p>
<p><strong>max_steps</strong> (<code>500</code>): This is the maximum number of training steps to execute, regardless of how many epochs are set. Training will stop when this number of steps is reached.</p>
<p><strong>learning_rate</strong> (<code>2.5e-5</code>): This is the step size at which the optimizer updates the models weights. A smaller learning rate might lead to better fine-tuning but slower convergence, and vice versa.</p>
<p><strong>logging_steps</strong> (<code>200</code>): Specifies how often to log training information. The setting determines after how many steps new logs should be created, which might include loss and other metrics. More frequent logging provides finer-grained visibility into the training progress but can add computational overhead.</p>
<p><strong>bf16</strong> (<code>True</code>): This would enable training using bfloat16 precision, which is a mixed precision format with fewer bits than the standard single-precision floating point (fp32). Like fp16, it can reduce memory usage and potentially speed up training if supported by the hardware. It's particularly useful on TPUs and newer GPUs that support this format. (4090)</p>
<p><strong>fp16</strong> (<code>True</code>): This enables half-precision floating point (16-bit) training. It reduces memory usage and can speed up training, provided the hardware (like modern GPUs) supports it. (3090 and 4090)</p>
<p><strong>optim</strong> (<code>"paged_adamw_8bit"</code>): Specifies the optimizer to use. "paged_adamw_8bit" might refer to a variation of the AdamW optimizer that is optimized for lower precision and memory bandwidth, enhancing training speed and efficiency.</p>
<p><strong>logging_dir</strong> (<code>"./logs"</code>): This specifies the directory where training logs should be saved. It's used to store logs if you are using a logging framework or callback that writes out logs to files. Organizing logs in a specific directory is helpful for post-training analysis and for monitoring the training process through tools like TensorBoard.</p>
<p><strong>save_strategy</strong> (<code>"epoch"</code>): This determines how often to save model checkpoints. Setting it to <code>"epoch"</code> means the model will save checkpoints at the end of each epoch.</p>
<p><strong>save_steps</strong> (<code>50</code>): This is closely related to the <code>save_strategy</code> when set to "steps". It defines how often to save the model, specifically after how many training steps. A lower number means more frequent saves, which increases disk I/O but provides more restore points for training.</p>
<p><strong>evaluation_strategy</strong> (<code>"epoch"</code>): This configures when the model should be evaluated against the evaluation dataset. Like <code>save_strategy</code>, setting this to <code>"epoch"</code> triggers evaluations at the end of each epoch, providing feedback on model performance after it has seen the entire training dataset.</p>
<p><strong>eval_steps</strong> (<code>50</code>): This parameter determines how often to evaluate the model if the <code>evaluation_strategy</code> is set to "steps". Similar to <code>logging_steps</code>, setting this affects how frequently the model's performance is assessed on the evaluation dataset during the training process. More frequent evaluations provide a closer look at the model's performance but at the cost of increased computational overhead.</p>
<p><strong>do_eval</strong> (<code>True</code>): This flag enables the evaluation of the model on the evaluation dataset. If <code>True</code>, it will use the evaluation dataset to assess model performance based on metrics</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=d8f3cd41-fd17-4fd8-83b8-54f464df79ff">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Training">Training<a class="anchor-link" href="#Training"></a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=2df4e2a5-2a08-434b-983a-f6cd42f33da3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[24]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">transformers</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="n">transformers</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">777</span><span class="p">)</span>

<span class="k">if</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_train_ds</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">tokenized_eval_ds</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">transformers</span><span class="o">.</span><span class="n">TrainingArguments</span><span class="p">(</span>
        <span class="n">output_dir</span><span class="o">=</span><span class="n">output_dir_path</span><span class="p">,</span>
        <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">logging_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">logging_dir</span><span class="o">=</span><span class="n">output_dir_path</span><span class="o">+</span><span class="s2">"/logs"</span><span class="p">,</span>
        <span class="n">evaluation_strategy</span><span class="o">=</span><span class="s1">'epoch'</span><span class="p">,</span>
        <span class="n">save_strategy</span><span class="o">=</span><span class="s1">'epoch'</span><span class="p">,</span>
        <span class="n">bf16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">optim</span><span class="o">=</span><span class="s2">"paged_adamw_8bit"</span><span class="p">,</span>
        <span class="n">do_eval</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>
    <span class="n">data_collator</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">DataCollatorWithPadding</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">),</span>
<span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">resume_from_checkpoint</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># Turn to True if power goes out...</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>2024-06-10 04:47:02.964765: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-10 04:47:03.617326: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
<span class="ansi-blue-intense-fg ansi-bold">wandb</span>: Currently logged in as: <span class="ansi-yellow-fg">nispoe</span>. Use <span class="ansi-bold">`wandb login --relogin`</span> to force relogin
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
wandb version 0.17.1 is available!  To upgrade, please run:
 $ pip install wandb --upgrade
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
Tracking run with wandb version 0.17.0
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
Run data is saved locally in <code>/home/nispoe/data/kuk/Praxis/wandb/run-20240610_044706-koii8j9m</code>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
Syncing run <strong><a href="https://wandb.ai/nispoe/huggingface/runs/koii8j9m" target="_blank">vocal-plant-378</a></strong> to <a href="https://wandb.ai/nispoe/huggingface" target="_blank">Weights &amp; Biases</a> (<a href="https://wandb.me/run" target="_blank">docs</a>)<br/>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
 View project at <a href="https://wandb.ai/nispoe/huggingface" target="_blank">https://wandb.ai/nispoe/huggingface</a>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
 View run at <a href="https://wandb.ai/nispoe/huggingface/runs/koii8j9m" target="_blank">https://wandb.ai/nispoe/huggingface/runs/koii8j9m</a>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<div>
<progress max="12606" style="width:300px; height:20px; vertical-align: middle;" value="12606"></progress>
      [12606/12606 35:06:58, Epoch 3/3]
    </div>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: left;">
<th>Epoch</th>
<th>Training Loss</th>
<th>Validation Loss</th>
<th>Accuracy</th>
<th>F1</th>
</tr>
</thead>
<tbody>
<tr>
<td>3</td>
<td>0.000000</td>
<td>0.002042</td>
<td>0.999722</td>
<td>0.999707</td>
</tr>
</tbody>
</table><p>
</p></div>
</div>
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[24]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>TrainOutput(global_step=12606, training_loss=0.0004971224649754778, metrics={'train_runtime': 126447.9573, 'train_samples_per_second': 0.797, 'train_steps_per_second': 0.1, 'total_flos': 2.1235817723556004e+19, 'train_loss': 0.0004971224649754778, 'epoch': 3.0})</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=0d13982d-53cb-4c88-bd32-a98d4a5a9874">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Determine-best-checkpoint">Determine best checkpoint<a class="anchor-link" href="#Determine-best-checkpoint"></a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=8bd570c7-8feb-4b69-bda7-9c678bfd92c3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[25]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">!</span>ls<span class="w"> </span>-ltr<span class="w"> </span><span class="o">{</span>output_dir_path<span class="o">}</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>total 16
drwxrwxr-x 2 nispoe nispoe 4096 Jun  7 14:57 checkpoint-4202
drwxrwxr-x 2 nispoe nispoe 4096 Jun  9 02:06 checkpoint-8404
drwxr-xr-x 2 nispoe nispoe 4096 Jun 10 04:47 logs
drwxrwxr-x 2 nispoe nispoe 4096 Jun 11 15:54 checkpoint-12606
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=97771269-d77c-4c1b-b264-34e082db6cbc">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[26]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.python.summary.summary_iterator</span> <span class="kn">import</span> <span class="n">summary_iterator</span>
<span class="kn">import</span> <span class="nn">glob</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># Construct the logs directory path</span>
<span class="n">logs_directory</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">'./'</span><span class="p">,</span> <span class="n">project_name</span><span class="p">,</span> <span class="s1">'logs'</span><span class="p">)</span>
<span class="n">file_pattern</span> <span class="o">=</span> <span class="s1">'events.out.tfevents.*'</span>

<span class="c1"># Retrieve all event files matching the pattern</span>
<span class="n">event_files</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">logs_directory</span><span class="p">,</span> <span class="n">file_pattern</span><span class="p">))</span>

<span class="c1"># Function to print out TensorBoard event logs</span>
<span class="k">def</span> <span class="nf">print_events_from_file</span><span class="p">(</span><span class="n">event_files</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">event_file</span> <span class="ow">in</span> <span class="n">event_files</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Reading events from file: </span><span class="si">{</span><span class="n">event_file</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">summary_iterator</span><span class="p">(</span><span class="n">event_file</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">e</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">v</span><span class="o">.</span><span class="n">HasField</span><span class="p">(</span><span class="s1">'simple_value'</span><span class="p">):</span>
                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Step: </span><span class="si">{</span><span class="n">e</span><span class="o">.</span><span class="n">step</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">v</span><span class="o">.</span><span class="n">simple_value</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>  <span class="c1"># Just in case the event file is not readable</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Failed to read </span><span class="si">{</span><span class="n">event_file</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">print_events_from_file</span><span class="p">(</span><span class="n">event_files</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Reading events from file: ./praxis-Meta-Llama-3-70B-small-finetune/logs/events.out.tfevents.1717663737.hephaestus.17728.0
WARNING:tensorflow:From /home/nispoe/.local/lib/python3.10/site-packages/tensorflow/python/summary/summary_iterator.py:27: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.
Instructions for updating:
Use eager execution and: 
`tf.data.TFRecordDataset(path)`
Step: 10, train/loss: 1.5233999490737915
Step: 10, train/grad_norm: 50.8408203125
Step: 10, train/learning_rate: 4.9960337491938844e-05
Step: 10, train/epoch: 0.0023798190522938967
Step: 20, train/loss: 1.3343000411987305
Step: 20, train/grad_norm: 22.7810001373291
Step: 20, train/learning_rate: 4.992067260900512e-05
Step: 20, train/epoch: 0.004759638104587793
Step: 30, train/loss: 0.8616999983787537
Step: 30, train/grad_norm: 41.443641662597656
Step: 30, train/learning_rate: 4.98810077260714e-05
Step: 30, train/epoch: 0.007139457389712334
Step: 40, train/loss: 0.49410000443458557
Step: 40, train/grad_norm: 36.65349197387695
Step: 40, train/learning_rate: 4.984134648111649e-05
Step: 40, train/epoch: 0.009519276209175587
Step: 50, train/loss: 0.04500000178813934
Step: 50, train/grad_norm: 0.9125515818595886
Step: 50, train/learning_rate: 4.980168159818277e-05
Step: 50, train/epoch: 0.011899095959961414
Step: 60, train/loss: 0.06830000132322311
Step: 60, train/grad_norm: 0.4801281988620758
Step: 60, train/learning_rate: 4.976201671524905e-05
Step: 60, train/epoch: 0.014278914779424667
Step: 70, train/loss: 0.20759999752044678
Step: 70, train/grad_norm: 10.63454532623291
Step: 70, train/learning_rate: 4.972235547029413e-05
Step: 70, train/epoch: 0.016658734530210495
Step: 80, train/loss: 0.04580000042915344
Step: 80, train/grad_norm: 2.984823368024081e-05
Step: 80, train/learning_rate: 4.968269058736041e-05
Step: 80, train/epoch: 0.019038552418351173
Step: 90, train/loss: 0.25
Step: 90, train/grad_norm: 3.789926097397256e-07
Step: 90, train/learning_rate: 4.964302570442669e-05
Step: 90, train/epoch: 0.021418372169137
Step: 100, train/loss: 0.11320000141859055
Step: 100, train/grad_norm: 0.0004901330103166401
Step: 100, train/learning_rate: 4.960336445947178e-05
Step: 100, train/epoch: 0.02379819191992283
Step: 110, train/loss: 0.011599999852478504
Step: 110, train/grad_norm: 7.538951081187406e-07
Step: 110, train/learning_rate: 4.9563699576538056e-05
Step: 110, train/epoch: 0.026178009808063507
Step: 120, train/loss: 0.25940001010894775
Step: 120, train/grad_norm: 2.2117898623719157e-08
Step: 120, train/learning_rate: 4.9524034693604335e-05
Step: 120, train/epoch: 0.028557829558849335
Step: 130, train/loss: 9.999999747378752e-05
Step: 130, train/grad_norm: 1.0396584571026324e-07
Step: 130, train/learning_rate: 4.948437344864942e-05
Step: 130, train/epoch: 0.030937649309635162
Step: 140, train/loss: 0.34060001373291016
Step: 140, train/grad_norm: 36.553958892822266
Step: 140, train/learning_rate: 4.94447085657157e-05
Step: 140, train/epoch: 0.03331746906042099
Step: 150, train/loss: 0.0006000000284984708
Step: 150, train/grad_norm: 0.019226865842938423
Step: 150, train/learning_rate: 4.940504368278198e-05
Step: 150, train/epoch: 0.03569728881120682
Step: 160, train/loss: 0.18209999799728394
Step: 160, train/grad_norm: 2.279935359954834
Step: 160, train/learning_rate: 4.9365382437827066e-05
Step: 160, train/epoch: 0.03807710483670235
Step: 170, train/loss: 0.22370000183582306
Step: 170, train/grad_norm: 0.010026735253632069
Step: 170, train/learning_rate: 4.9325717554893345e-05
Step: 170, train/epoch: 0.040456924587488174
Step: 180, train/loss: 0.2768999934196472
Step: 180, train/grad_norm: 115.44369506835938
Step: 180, train/learning_rate: 4.9286052671959624e-05
Step: 180, train/epoch: 0.042836744338274
Step: 190, train/loss: 0.3562999963760376
Step: 190, train/grad_norm: 211.43601989746094
Step: 190, train/learning_rate: 4.924639142700471e-05
Step: 190, train/epoch: 0.04521656408905983
Step: 200, train/loss: 0.009999999776482582
Step: 200, train/grad_norm: 0.010756638832390308
Step: 200, train/learning_rate: 4.920672654407099e-05
Step: 200, train/epoch: 0.04759638383984566
Step: 210, train/loss: 0.006500000134110451
Step: 210, train/grad_norm: 0.0005524966982193291
Step: 210, train/learning_rate: 4.916706166113727e-05
Step: 210, train/epoch: 0.049976203590631485
Step: 220, train/loss: 0.21170000731945038
Step: 220, train/grad_norm: 5.62481363886036e-06
Step: 220, train/learning_rate: 4.9127400416182354e-05
Step: 220, train/epoch: 0.052356019616127014
Step: 230, train/loss: 0.15629999339580536
Step: 230, train/grad_norm: 5.011291432310827e-05
Step: 230, train/learning_rate: 4.908773553324863e-05
Step: 230, train/epoch: 0.05473583936691284
Step: 240, train/loss: 0.1460999995470047
Step: 240, train/grad_norm: 0.0005144528695382178
Step: 240, train/learning_rate: 4.904807065031491e-05
Step: 240, train/epoch: 0.05711565911769867
Step: 250, train/loss: 0.0
Step: 250, train/grad_norm: 1.691859807806395e-07
Step: 250, train/learning_rate: 4.900840940536e-05
Step: 250, train/epoch: 0.0594954788684845
Step: 260, train/loss: 0.07729999721050262
Step: 260, train/grad_norm: 1.8565729988040403e-05
Step: 260, train/learning_rate: 4.896874452242628e-05
Step: 260, train/epoch: 0.061875298619270325
Step: 270, train/loss: 0.0
Step: 270, train/grad_norm: 2.9555277336612562e-08
Step: 270, train/learning_rate: 4.8929079639492556e-05
Step: 270, train/epoch: 0.06425511837005615
Step: 280, train/loss: 0.06759999692440033
Step: 280, train/grad_norm: 0.0005364647950045764
Step: 280, train/learning_rate: 4.888941839453764e-05
Step: 280, train/epoch: 0.06663493812084198
Step: 290, train/loss: 0.0
Step: 290, train/grad_norm: 1.055896336765727e-05
Step: 290, train/learning_rate: 4.884975351160392e-05
Step: 290, train/epoch: 0.06901475787162781
Step: 300, train/loss: 0.0
Step: 300, train/grad_norm: 4.5715041778748855e-06
Step: 300, train/learning_rate: 4.88100886286702e-05
Step: 300, train/epoch: 0.07139457762241364
Step: 310, train/loss: 0.0005000000237487257
Step: 310, train/grad_norm: 4.02080650019343e-06
Step: 310, train/learning_rate: 4.877042738371529e-05
Step: 310, train/epoch: 0.07377438992261887
Step: 320, train/loss: 0.0
Step: 320, train/grad_norm: 0.0001485404500272125
Step: 320, train/learning_rate: 4.8730762500781566e-05
Step: 320, train/epoch: 0.0761542096734047
Step: 330, train/loss: 0.0
Step: 330, train/grad_norm: 4.527995770331472e-06
Step: 330, train/learning_rate: 4.869110125582665e-05
Step: 330, train/epoch: 0.07853402942419052
Step: 340, train/loss: 0.04830000177025795
Step: 340, train/grad_norm: 218.5382843017578
Step: 340, train/learning_rate: 4.865143637289293e-05
Step: 340, train/epoch: 0.08091384917497635
Step: 350, train/loss: 0.0
Step: 350, train/grad_norm: 4.429004093253752e-06
Step: 350, train/learning_rate: 4.861177148995921e-05
Step: 350, train/epoch: 0.08329366892576218
Step: 360, train/loss: 9.999999747378752e-05
Step: 360, train/grad_norm: 1.0092267075378913e-05
Step: 360, train/learning_rate: 4.8572110245004296e-05
Step: 360, train/epoch: 0.085673488676548
Step: 370, train/loss: 0.0
Step: 370, train/grad_norm: 0.0005187962087802589
Step: 370, train/learning_rate: 4.8532445362070575e-05
Step: 370, train/epoch: 0.08805330842733383
Step: 380, train/loss: 0.0
Step: 380, train/grad_norm: 2.4993574697873555e-05
Step: 380, train/learning_rate: 4.8492780479136854e-05
Step: 380, train/epoch: 0.09043312817811966
Step: 390, train/loss: 0.1281999945640564
Step: 390, train/grad_norm: 0.29792794585227966
Step: 390, train/learning_rate: 4.845311923418194e-05
Step: 390, train/epoch: 0.09281294792890549
Step: 400, train/loss: 0.00039999998989515007
Step: 400, train/grad_norm: 1.5867217371123843e-05
Step: 400, train/learning_rate: 4.841345435124822e-05
Step: 400, train/epoch: 0.09519276767969131
Step: 410, train/loss: 0.0
Step: 410, train/grad_norm: 1.8973753412865335e-07
Step: 410, train/learning_rate: 4.83737894683145e-05
Step: 410, train/epoch: 0.09757258743047714
Step: 420, train/loss: 0.06599999964237213
Step: 420, train/grad_norm: 0.002930575516074896
Step: 420, train/learning_rate: 4.8334128223359585e-05
Step: 420, train/epoch: 0.09995240718126297
Step: 430, train/loss: 0.0
Step: 430, train/grad_norm: 1.3410010069492273e-05
Step: 430, train/learning_rate: 4.8294463340425864e-05
Step: 430, train/epoch: 0.1023322194814682
Step: 440, train/loss: 0.2312999963760376
Step: 440, train/grad_norm: 3.762261621886864e-05
Step: 440, train/learning_rate: 4.825479845749214e-05
Step: 440, train/epoch: 0.10471203923225403
Step: 450, train/loss: 0.07429999858140945
Step: 450, train/grad_norm: 3.572216033935547
Step: 450, train/learning_rate: 4.821513721253723e-05
Step: 450, train/epoch: 0.10709185898303986
Step: 460, train/loss: 0.0
Step: 460, train/grad_norm: 0.014298929832875729
Step: 460, train/learning_rate: 4.817547232960351e-05
Step: 460, train/epoch: 0.10947167873382568
Step: 470, train/loss: 0.23280000686645508
Step: 470, train/grad_norm: 0.000524279079400003
Step: 470, train/learning_rate: 4.813580744666979e-05
Step: 470, train/epoch: 0.11185149848461151
Step: 480, train/loss: 0.00039999998989515007
Step: 480, train/grad_norm: 0.0002864289563149214
Step: 480, train/learning_rate: 4.809614620171487e-05
Step: 480, train/epoch: 0.11423131823539734
Step: 490, train/loss: 0.002300000051036477
Step: 490, train/grad_norm: 0.012695805169641972
Step: 490, train/learning_rate: 4.805648131878115e-05
Step: 490, train/epoch: 0.11661113798618317
Step: 500, train/loss: 0.046300001442432404
Step: 500, train/grad_norm: 0.00010031615238403901
Step: 500, train/learning_rate: 4.801681643584743e-05
Step: 500, train/epoch: 0.118990957736969
Step: 510, train/loss: 9.999999747378752e-05
Step: 510, train/grad_norm: 9.934209629136603e-06
Step: 510, train/learning_rate: 4.797715519089252e-05
Step: 510, train/epoch: 0.12137077748775482
Step: 520, train/loss: 0.0
Step: 520, train/grad_norm: 0.0001492688897997141
Step: 520, train/learning_rate: 4.79374903079588e-05
Step: 520, train/epoch: 0.12375059723854065
Step: 530, train/loss: 0.0940999984741211
Step: 530, train/grad_norm: 0.00011478640954010189
Step: 530, train/learning_rate: 4.7897825425025076e-05
Step: 530, train/epoch: 0.12613041698932648
Step: 540, train/loss: 0.0
Step: 540, train/grad_norm: 2.9707946325174817e-08
Step: 540, train/learning_rate: 4.785816418007016e-05
Step: 540, train/epoch: 0.1285102367401123
Step: 550, train/loss: 0.08060000091791153
Step: 550, train/grad_norm: 78.2262191772461
Step: 550, train/learning_rate: 4.781849929713644e-05
Step: 550, train/epoch: 0.13089005649089813
Step: 560, train/loss: 0.0
Step: 560, train/grad_norm: 2.0955974378011888e-07
Step: 560, train/learning_rate: 4.777883441420272e-05
Step: 560, train/epoch: 0.13326987624168396
Step: 570, train/loss: 0.0
Step: 570, train/grad_norm: 8.823413963909843e-07
Step: 570, train/learning_rate: 4.7739173169247806e-05
Step: 570, train/epoch: 0.1356496959924698
Step: 580, train/loss: 0.0
Step: 580, train/grad_norm: 6.230795406736434e-05
Step: 580, train/learning_rate: 4.7699508286314085e-05
Step: 580, train/epoch: 0.13802951574325562
Step: 590, train/loss: 0.15389999747276306
Step: 590, train/grad_norm: 6.679168291157112e-05
Step: 590, train/learning_rate: 4.7659843403380364e-05
Step: 590, train/epoch: 0.14040933549404144
Step: 600, train/loss: 0.18129999935626984
Step: 600, train/grad_norm: 0.005633645225316286
Step: 600, train/learning_rate: 4.762018215842545e-05
Step: 600, train/epoch: 0.14278915524482727
Step: 610, train/loss: 0.00570000009611249
Step: 610, train/grad_norm: 4.425718307495117
Step: 610, train/learning_rate: 4.758051727549173e-05
Step: 610, train/epoch: 0.1451689600944519
Step: 620, train/loss: 0.0
Step: 620, train/grad_norm: 2.6813746444531716e-05
Step: 620, train/learning_rate: 4.754085239255801e-05
Step: 620, train/epoch: 0.14754877984523773
Step: 630, train/loss: 0.0
Step: 630, train/grad_norm: 7.475990315697345e-08
Step: 630, train/learning_rate: 4.7501191147603095e-05
Step: 630, train/epoch: 0.14992859959602356
Step: 640, train/loss: 0.0
Step: 640, train/grad_norm: 4.81875073177207e-09
Step: 640, train/learning_rate: 4.7461526264669374e-05
Step: 640, train/epoch: 0.1523084193468094
Step: 650, train/loss: 0.0
Step: 650, train/grad_norm: 6.414263109370821e-13
Step: 650, train/learning_rate: 4.742186138173565e-05
Step: 650, train/epoch: 0.15468823909759521
Step: 660, train/loss: 0.0
Step: 660, train/grad_norm: 3.03088825148734e-08
Step: 660, train/learning_rate: 4.738220013678074e-05
Step: 660, train/epoch: 0.15706805884838104
Step: 670, train/loss: 0.0
Step: 670, train/grad_norm: 3.6646254919503463e-09
Step: 670, train/learning_rate: 4.734253525384702e-05
Step: 670, train/epoch: 0.15944787859916687
Step: 680, train/loss: 0.06800000369548798
Step: 680, train/grad_norm: 4.9693919118576346e-11
Step: 680, train/learning_rate: 4.73028703709133e-05
Step: 680, train/epoch: 0.1618276983499527
Step: 690, train/loss: 0.20669999718666077
Step: 690, train/grad_norm: 5.136827894602902e-05
Step: 690, train/learning_rate: 4.726320912595838e-05
Step: 690, train/epoch: 0.16420751810073853
Step: 700, train/loss: 0.04470000043511391
Step: 700, train/grad_norm: 2.2270060640039446e-07
Step: 700, train/learning_rate: 4.722354424302466e-05
Step: 700, train/epoch: 0.16658733785152435
Step: 710, train/loss: 0.0003000000142492354
Step: 710, train/grad_norm: 2.0053408661624417e-05
Step: 710, train/learning_rate: 4.718387936009094e-05
Step: 710, train/epoch: 0.16896715760231018
Step: 720, train/loss: 0.0
Step: 720, train/grad_norm: 0.0004926729016005993
Step: 720, train/learning_rate: 4.714421811513603e-05
Step: 720, train/epoch: 0.171346977353096
Step: 730, train/loss: 0.0
Step: 730, train/grad_norm: 2.679149133655301e-07
Step: 730, train/learning_rate: 4.7104553232202306e-05
Step: 730, train/epoch: 0.17372679710388184
Step: 740, train/loss: 0.5720000267028809
Step: 740, train/grad_norm: 1.6526916724046714e-08
Step: 740, train/learning_rate: 4.7064888349268585e-05
Step: 740, train/epoch: 0.17610661685466766
Step: 750, train/loss: 0.03920000046491623
Step: 750, train/grad_norm: 0.02579658478498459
Step: 750, train/learning_rate: 4.702522710431367e-05
Step: 750, train/epoch: 0.1784864366054535
Step: 760, train/loss: 0.0003000000142492354
Step: 760, train/grad_norm: 1.6827833348997956e-08
Step: 760, train/learning_rate: 4.698556222137995e-05
Step: 760, train/epoch: 0.18086625635623932
Step: 770, train/loss: 0.011500000022351742
Step: 770, train/grad_norm: 2.3843705321269226e-07
Step: 770, train/learning_rate: 4.694589733844623e-05
Step: 770, train/epoch: 0.18324607610702515
Step: 780, train/loss: 0.5062999725341797
Step: 780, train/grad_norm: 9.143536772171501e-08
Step: 780, train/learning_rate: 4.6906236093491316e-05
Step: 780, train/epoch: 0.18562589585781097
Step: 790, train/loss: 0.0
Step: 790, train/grad_norm: 4.263246955815703e-05
Step: 790, train/learning_rate: 4.6866571210557595e-05
Step: 790, train/epoch: 0.1880057156085968
Step: 800, train/loss: 0.0
Step: 800, train/grad_norm: 0.0001524948893347755
Step: 800, train/learning_rate: 4.6826906327623874e-05
Step: 800, train/epoch: 0.19038553535938263
Step: 810, train/loss: 0.14830000698566437
Step: 810, train/grad_norm: 0.0003993749851360917
Step: 810, train/learning_rate: 4.678724508266896e-05
Step: 810, train/epoch: 0.19276535511016846
Step: 820, train/loss: 0.021199999377131462
Step: 820, train/grad_norm: 0.002277626423165202
Step: 820, train/learning_rate: 4.674758019973524e-05
Step: 820, train/epoch: 0.19514517486095428
Step: 830, train/loss: 0.0010000000474974513
Step: 830, train/grad_norm: 3.6222292526533195e-11
Step: 830, train/learning_rate: 4.670791531680152e-05
Step: 830, train/epoch: 0.1975249946117401
Step: 840, train/loss: 0.0017000000225380063
Step: 840, train/grad_norm: 5.936188429700451e-09
Step: 840, train/learning_rate: 4.6668254071846604e-05
Step: 840, train/epoch: 0.19990481436252594
Step: 850, train/loss: 0.0
Step: 850, train/grad_norm: 1.1829839191568325e-11
Step: 850, train/learning_rate: 4.6628589188912883e-05
Step: 850, train/epoch: 0.20228461921215057
Step: 860, train/loss: 0.0
Step: 860, train/grad_norm: 4.0914126253621674e-11
Step: 860, train/learning_rate: 4.658892430597916e-05
Step: 860, train/epoch: 0.2046644389629364
Step: 870, train/loss: 0.03790000081062317
Step: 870, train/grad_norm: 3.073362148543063e-13
Step: 870, train/learning_rate: 4.654926306102425e-05
Step: 870, train/epoch: 0.20704425871372223
Step: 880, train/loss: 0.0
Step: 880, train/grad_norm: 2.016213151989632e-08
Step: 880, train/learning_rate: 4.650959817809053e-05
Step: 880, train/epoch: 0.20942407846450806
Step: 890, train/loss: 0.0
Step: 890, train/grad_norm: 1.8680647306812403e-10
Step: 890, train/learning_rate: 4.646993329515681e-05
Step: 890, train/epoch: 0.21180389821529388
Step: 900, train/loss: 0.014499999582767487
Step: 900, train/grad_norm: 3.8784132155855744e-11
Step: 900, train/learning_rate: 4.643027205020189e-05
Step: 900, train/epoch: 0.2141837179660797
Step: 910, train/loss: 0.0
Step: 910, train/grad_norm: 1.0001135475334877e-09
Step: 910, train/learning_rate: 4.639060716726817e-05
Step: 910, train/epoch: 0.21656353771686554
Step: 920, train/loss: 0.00019999999494757503
Step: 920, train/grad_norm: 6.697387977538938e-09
Step: 920, train/learning_rate: 4.635094228433445e-05
Step: 920, train/epoch: 0.21894335746765137
Step: 930, train/loss: 0.0
Step: 930, train/grad_norm: 3.8294376736303093e-07
Step: 930, train/learning_rate: 4.631128103937954e-05
Step: 930, train/epoch: 0.2213231772184372
Step: 940, train/loss: 0.33180001378059387
Step: 940, train/grad_norm: 95.8254165649414
Step: 940, train/learning_rate: 4.6271616156445816e-05
Step: 940, train/epoch: 0.22370299696922302
Step: 950, train/loss: 0.019700000062584877
Step: 950, train/grad_norm: 1.022566475938902e-09
Step: 950, train/learning_rate: 4.6231951273512095e-05
Step: 950, train/epoch: 0.22608281672000885
Step: 960, train/loss: 0.1746000051498413
Step: 960, train/grad_norm: 4.5976334718533796e-12
Step: 960, train/learning_rate: 4.619229002855718e-05
Step: 960, train/epoch: 0.22846263647079468
Step: 970, train/loss: 0.0
Step: 970, train/grad_norm: 1.0567365871239076e-09
Step: 970, train/learning_rate: 4.615262514562346e-05
Step: 970, train/epoch: 0.2308424562215805
Step: 980, train/loss: 0.012900000438094139
Step: 980, train/grad_norm: 9.045571403021313e-08
Step: 980, train/learning_rate: 4.611296026268974e-05
Step: 980, train/epoch: 0.23322227597236633
Step: 990, train/loss: 0.0
Step: 990, train/grad_norm: 0.0016425902722403407
Step: 990, train/learning_rate: 4.6073299017734826e-05
Step: 990, train/epoch: 0.23560209572315216
Step: 1000, train/loss: 0.0
Step: 1000, train/grad_norm: 3.690364991371098e-08
Step: 1000, train/learning_rate: 4.6033634134801105e-05
Step: 1000, train/epoch: 0.237981915473938
Step: 1010, train/loss: 0.5874999761581421
Step: 1010, train/grad_norm: 4.935565812047571e-06
Step: 1010, train/learning_rate: 4.599397288984619e-05
Step: 1010, train/epoch: 0.24036173522472382
Step: 1020, train/loss: 0.0005000000237487257
Step: 1020, train/grad_norm: 1.8509109800390888e-09
Step: 1020, train/learning_rate: 4.595430800691247e-05
Step: 1020, train/epoch: 0.24274155497550964
Step: 1030, train/loss: 0.05000000074505806
Step: 1030, train/grad_norm: 1.424617090961533e-09
Step: 1030, train/learning_rate: 4.591464312397875e-05
Step: 1030, train/epoch: 0.24512137472629547
Step: 1040, train/loss: 0.0
Step: 1040, train/grad_norm: 9.221125252256712e-11
Step: 1040, train/learning_rate: 4.5874981879023835e-05
Step: 1040, train/epoch: 0.2475011944770813
Step: 1050, train/loss: 0.0
Step: 1050, train/grad_norm: 8.396066064619845e-09
Step: 1050, train/learning_rate: 4.5835316996090114e-05
Step: 1050, train/epoch: 0.24988101422786713
Step: 1060, train/loss: 0.19059999287128448
Step: 1060, train/grad_norm: 0.06466285139322281
Step: 1060, train/learning_rate: 4.579565211315639e-05
Step: 1060, train/epoch: 0.25226083397865295
Step: 1070, train/loss: 0.3174999952316284
Step: 1070, train/grad_norm: 6.43745012851582e-09
Step: 1070, train/learning_rate: 4.575599086820148e-05
Step: 1070, train/epoch: 0.2546406388282776
Step: 1080, train/loss: 0.0010999999940395355
Step: 1080, train/grad_norm: 1.825443823877393e-10
Step: 1080, train/learning_rate: 4.571632598526776e-05
Step: 1080, train/epoch: 0.2570204734802246
Step: 1090, train/loss: 0.005200000014156103
Step: 1090, train/grad_norm: 8.716783406192974e-10
Step: 1090, train/learning_rate: 4.567666110233404e-05
Step: 1090, train/epoch: 0.25940027832984924
Step: 1100, train/loss: 0.0729999989271164
Step: 1100, train/grad_norm: 1.1056540216713984e-07
Step: 1100, train/learning_rate: 4.5636999857379124e-05
Step: 1100, train/epoch: 0.26178011298179626
Step: 1110, train/loss: 0.0
Step: 1110, train/grad_norm: 1.2589788100925148e-09
Step: 1110, train/learning_rate: 4.55973349744454e-05
Step: 1110, train/epoch: 0.2641599178314209
Step: 1120, train/loss: 0.1687999963760376
Step: 1120, train/grad_norm: 3.115429763056454e-07
Step: 1120, train/learning_rate: 4.555767009151168e-05
Step: 1120, train/epoch: 0.2665397524833679
Step: 1130, train/loss: 0.0020000000949949026
Step: 1130, train/grad_norm: 3.911979547410738e-06
Step: 1130, train/learning_rate: 4.551800884655677e-05
Step: 1130, train/epoch: 0.26891955733299255
Step: 1140, train/loss: 0.0
Step: 1140, train/grad_norm: 2.24438689855333e-10
Step: 1140, train/learning_rate: 4.547834396362305e-05
Step: 1140, train/epoch: 0.2712993919849396
Step: 1150, train/loss: 0.0
Step: 1150, train/grad_norm: 8.263723998425121e-07
Step: 1150, train/learning_rate: 4.5438679080689326e-05
Step: 1150, train/epoch: 0.2736791968345642
Step: 1160, train/loss: 0.0
Step: 1160, train/grad_norm: 3.1353373231013393e-09
Step: 1160, train/learning_rate: 4.539901783573441e-05
Step: 1160, train/epoch: 0.27605903148651123
Step: 1170, train/loss: 0.0
Step: 1170, train/grad_norm: 8.704129754733003e-07
Step: 1170, train/learning_rate: 4.535935295280069e-05
Step: 1170, train/epoch: 0.27843883633613586
Step: 1180, train/loss: 0.0
Step: 1180, train/grad_norm: 4.980430512446787e-10
Step: 1180, train/learning_rate: 4.531968806986697e-05
Step: 1180, train/epoch: 0.2808186709880829
Step: 1190, train/loss: 0.0
Step: 1190, train/grad_norm: 2.59844729066927e-10
Step: 1190, train/learning_rate: 4.5280026824912056e-05
Step: 1190, train/epoch: 0.2831984758377075
Step: 1200, train/loss: 0.0010000000474974513
Step: 1200, train/grad_norm: 1.360734040645184e-05
Step: 1200, train/learning_rate: 4.5240361941978335e-05
Step: 1200, train/epoch: 0.28557831048965454
Step: 1210, train/loss: 0.0026000000070780516
Step: 1210, train/grad_norm: 12.662961959838867
Step: 1210, train/learning_rate: 4.5200697059044614e-05
Step: 1210, train/epoch: 0.2879581153392792
Step: 1220, train/loss: 0.0
Step: 1220, train/grad_norm: 1.4303750406341464e-09
Step: 1220, train/learning_rate: 4.51610358140897e-05
Step: 1220, train/epoch: 0.2903379201889038
Step: 1230, train/loss: 0.0
Step: 1230, train/grad_norm: 8.30621615932614e-09
Step: 1230, train/learning_rate: 4.512137093115598e-05
Step: 1230, train/epoch: 0.29271775484085083
Step: 1240, train/loss: 0.0
Step: 1240, train/grad_norm: 3.892339009325951e-06
Step: 1240, train/learning_rate: 4.508170604822226e-05
Step: 1240, train/epoch: 0.29509755969047546
Step: 1250, train/loss: 0.0
Step: 1250, train/grad_norm: 2.499133699984668e-08
Step: 1250, train/learning_rate: 4.5042044803267345e-05
Step: 1250, train/epoch: 0.2974773943424225
Step: 1260, train/loss: 0.0
Step: 1260, train/grad_norm: 9.524157721166326e-13
Step: 1260, train/learning_rate: 4.5002379920333624e-05
Step: 1260, train/epoch: 0.2998571991920471
Step: 1270, train/loss: 0.0
Step: 1270, train/grad_norm: 2.3257316072999856e-09
Step: 1270, train/learning_rate: 4.49627150373999e-05
Step: 1270, train/epoch: 0.30223703384399414
Step: 1280, train/loss: 0.0
Step: 1280, train/grad_norm: 9.739671742725609e-12
Step: 1280, train/learning_rate: 4.492305379244499e-05
Step: 1280, train/epoch: 0.3046168386936188
Step: 1290, train/loss: 0.0
Step: 1290, train/grad_norm: 2.793044684423762e-09
Step: 1290, train/learning_rate: 4.488338890951127e-05
Step: 1290, train/epoch: 0.3069966733455658
Step: 1300, train/loss: 0.0
Step: 1300, train/grad_norm: 4.730994242196918e-13
Step: 1300, train/learning_rate: 4.484372402657755e-05
Step: 1300, train/epoch: 0.30937647819519043
Step: 1310, train/loss: 0.05119999870657921
Step: 1310, train/grad_norm: 1.7065580892872134e-10
Step: 1310, train/learning_rate: 4.480406278162263e-05
Step: 1310, train/epoch: 0.31175631284713745
Step: 1320, train/loss: 0.0
Step: 1320, train/grad_norm: 0.0006334357894957066
Step: 1320, train/learning_rate: 4.476439789868891e-05
Step: 1320, train/epoch: 0.3141361176967621
Step: 1330, train/loss: 0.0017000000225380063
Step: 1330, train/grad_norm: 346.0721130371094
Step: 1330, train/learning_rate: 4.472473301575519e-05
Step: 1330, train/epoch: 0.3165159523487091
Step: 1340, train/loss: 0.0
Step: 1340, train/grad_norm: 2.678081933993326e-08
Step: 1340, train/learning_rate: 4.468507177080028e-05
Step: 1340, train/epoch: 0.31889575719833374
Step: 1350, train/loss: 9.999999747378752e-05
Step: 1350, train/grad_norm: 2.4852188015844767e-11
Step: 1350, train/learning_rate: 4.464540688786656e-05
Step: 1350, train/epoch: 0.32127559185028076
Step: 1360, train/loss: 0.2312999963760376
Step: 1360, train/grad_norm: 1.9689055719140924e-08
Step: 1360, train/learning_rate: 4.4605742004932836e-05
Step: 1360, train/epoch: 0.3236553966999054
Step: 1370, train/loss: 0.0
Step: 1370, train/grad_norm: 9.102628590121342e-13
Step: 1370, train/learning_rate: 4.456608075997792e-05
Step: 1370, train/epoch: 0.3260352313518524
Step: 1380, train/loss: 0.0
Step: 1380, train/grad_norm: 2.436587237752974e-05
Step: 1380, train/learning_rate: 4.45264158770442e-05
Step: 1380, train/epoch: 0.32841503620147705
Step: 1390, train/loss: 0.0
Step: 1390, train/grad_norm: 5.979899952990309e-08
Step: 1390, train/learning_rate: 4.448675099411048e-05
Step: 1390, train/epoch: 0.3307948708534241
Step: 1400, train/loss: 0.0
Step: 1400, train/grad_norm: 6.3094334201707625e-09
Step: 1400, train/learning_rate: 4.4447089749155566e-05
Step: 1400, train/epoch: 0.3331746757030487
Step: 1410, train/loss: 0.0
Step: 1410, train/grad_norm: 3.119501457149454e-07
Step: 1410, train/learning_rate: 4.4407424866221845e-05
Step: 1410, train/epoch: 0.3355545103549957
Step: 1420, train/loss: 0.0
Step: 1420, train/grad_norm: 1.0101390210404126e-11
Step: 1420, train/learning_rate: 4.4367759983288124e-05
Step: 1420, train/epoch: 0.33793431520462036
Step: 1430, train/loss: 0.0
Step: 1430, train/grad_norm: 8.036404874900427e-11
Step: 1430, train/learning_rate: 4.432809873833321e-05
Step: 1430, train/epoch: 0.3403141498565674
Step: 1440, train/loss: 0.0
Step: 1440, train/grad_norm: 2.887012487917673e-05
Step: 1440, train/learning_rate: 4.428843385539949e-05
Step: 1440, train/epoch: 0.342693954706192
Step: 1450, train/loss: 0.0
Step: 1450, train/grad_norm: 9.91395268101769e-08
Step: 1450, train/learning_rate: 4.424876897246577e-05
Step: 1450, train/epoch: 0.34507375955581665
Step: 1460, train/loss: 0.0
Step: 1460, train/grad_norm: 6.466303048568989e-09
Step: 1460, train/learning_rate: 4.4209107727510855e-05
Step: 1460, train/epoch: 0.34745359420776367
Step: 1470, train/loss: 0.0
Step: 1470, train/grad_norm: 1.1694691437469373e-08
Step: 1470, train/learning_rate: 4.4169442844577134e-05
Step: 1470, train/epoch: 0.3498333990573883
Step: 1480, train/loss: 0.0
Step: 1480, train/grad_norm: 5.2888973582632115e-12
Step: 1480, train/learning_rate: 4.412977796164341e-05
Step: 1480, train/epoch: 0.3522132337093353
Step: 1490, train/loss: 0.0
Step: 1490, train/grad_norm: 0.00047379129682667553
Step: 1490, train/learning_rate: 4.40901167166885e-05
Step: 1490, train/epoch: 0.35459303855895996
Step: 1500, train/loss: 0.0
Step: 1500, train/grad_norm: 5.132459790502253e-09
Step: 1500, train/learning_rate: 4.405045183375478e-05
Step: 1500, train/epoch: 0.356972873210907
Step: 1510, train/loss: 0.0
Step: 1510, train/grad_norm: 1.1387646468158374e-14
Step: 1510, train/learning_rate: 4.401078695082106e-05
Step: 1510, train/epoch: 0.3593526780605316
Step: 1520, train/loss: 0.0
Step: 1520, train/grad_norm: 6.3767471303322054e-09
Step: 1520, train/learning_rate: 4.397112570586614e-05
Step: 1520, train/epoch: 0.36173251271247864
Step: 1530, train/loss: 0.0
Step: 1530, train/grad_norm: 1.0803661076863591e-08
Step: 1530, train/learning_rate: 4.393146082293242e-05
Step: 1530, train/epoch: 0.36411231756210327
Step: 1540, train/loss: 9.999999747378752e-05
Step: 1540, train/grad_norm: 2.3947131921886466e-05
Step: 1540, train/learning_rate: 4.38917959399987e-05
Step: 1540, train/epoch: 0.3664921522140503
Step: 1550, train/loss: 0.0
Step: 1550, train/grad_norm: 4.936391739818191e-09
Step: 1550, train/learning_rate: 4.385213469504379e-05
Step: 1550, train/epoch: 0.3688719570636749
Step: 1560, train/loss: 0.0
Step: 1560, train/grad_norm: 5.5313472552276366e-11
Step: 1560, train/learning_rate: 4.3812469812110066e-05
Step: 1560, train/epoch: 0.37125179171562195
Step: 1570, train/loss: 0.0
Step: 1570, train/grad_norm: 0.0031240838579833508
Step: 1570, train/learning_rate: 4.3772804929176345e-05
Step: 1570, train/epoch: 0.3736315965652466
Step: 1580, train/loss: 0.0
Step: 1580, train/grad_norm: 0.00022137649648357183
Step: 1580, train/learning_rate: 4.373314368422143e-05
Step: 1580, train/epoch: 0.3760114312171936
Step: 1590, train/loss: 0.0
Step: 1590, train/grad_norm: 5.540646341728461e-08
Step: 1590, train/learning_rate: 4.369347880128771e-05
Step: 1590, train/epoch: 0.37839123606681824
Step: 1600, train/loss: 0.0
Step: 1600, train/grad_norm: 4.4686598812404554e-06
Step: 1600, train/learning_rate: 4.365381391835399e-05
Step: 1600, train/epoch: 0.38077107071876526
Step: 1610, train/loss: 0.0
Step: 1610, train/grad_norm: 0.006647789850831032
Step: 1610, train/learning_rate: 4.3614152673399076e-05
Step: 1610, train/epoch: 0.3831508755683899
Step: 1620, train/loss: 0.010599999688565731
Step: 1620, train/grad_norm: 8.06702171729512e-09
Step: 1620, train/learning_rate: 4.3574487790465355e-05
Step: 1620, train/epoch: 0.3855307102203369
Step: 1630, train/loss: 0.0
Step: 1630, train/grad_norm: 4.4052574088571106e-11
Step: 1630, train/learning_rate: 4.3534822907531634e-05
Step: 1630, train/epoch: 0.38791051506996155
Step: 1640, train/loss: 0.17659999430179596
Step: 1640, train/grad_norm: 1.536458604789459e-08
Step: 1640, train/learning_rate: 4.349516166257672e-05
Step: 1640, train/epoch: 0.39029034972190857
Step: 1650, train/loss: 0.0
Step: 1650, train/grad_norm: 1.2085265232997244e-08
Step: 1650, train/learning_rate: 4.3455496779643e-05
Step: 1650, train/epoch: 0.3926701545715332
Step: 1660, train/loss: 0.0
Step: 1660, train/grad_norm: 4.2810381017943655e-08
Step: 1660, train/learning_rate: 4.3415835534688085e-05
Step: 1660, train/epoch: 0.3950499892234802
Step: 1670, train/loss: 0.0
Step: 1670, train/grad_norm: 1.0193772502541457e-13
Step: 1670, train/learning_rate: 4.3376170651754364e-05
Step: 1670, train/epoch: 0.39742979407310486
Step: 1680, train/loss: 9.999999747378752e-05
Step: 1680, train/grad_norm: 3.1054012139541953e-10
Step: 1680, train/learning_rate: 4.3336505768820643e-05
Step: 1680, train/epoch: 0.3998096287250519
Step: 1690, train/loss: 0.0
Step: 1690, train/grad_norm: 3.0771003523000218e-09
Step: 1690, train/learning_rate: 4.329684452386573e-05
Step: 1690, train/epoch: 0.4021894335746765
Step: 1700, train/loss: 0.20469999313354492
Step: 1700, train/grad_norm: 1.0350902357458835e-07
Step: 1700, train/learning_rate: 4.325717964093201e-05
Step: 1700, train/epoch: 0.40456923842430115
Step: 1710, train/loss: 0.09220000356435776
Step: 1710, train/grad_norm: 8.288154731417308e-07
Step: 1710, train/learning_rate: 4.321751475799829e-05
Step: 1710, train/epoch: 0.40694907307624817
Step: 1720, train/loss: 0.0031999999191612005
Step: 1720, train/grad_norm: 9.505699836154236e-07
Step: 1720, train/learning_rate: 4.3177853513043374e-05
Step: 1720, train/epoch: 0.4093288779258728
Step: 1730, train/loss: 0.10080000013113022
Step: 1730, train/grad_norm: 3.648577546755405e-07
Step: 1730, train/learning_rate: 4.313818863010965e-05
Step: 1730, train/epoch: 0.4117087125778198
Step: 1740, train/loss: 0.0
Step: 1740, train/grad_norm: 6.050284273584339e-09
Step: 1740, train/learning_rate: 4.309852374717593e-05
Step: 1740, train/epoch: 0.41408851742744446
Step: 1750, train/loss: 0.0
Step: 1750, train/grad_norm: 0.029153674840927124
Step: 1750, train/learning_rate: 4.305886250222102e-05
Step: 1750, train/epoch: 0.4164683520793915
Step: 1760, train/loss: 0.0
Step: 1760, train/grad_norm: 2.4525206518410947e-11
Step: 1760, train/learning_rate: 4.30191976192873e-05
Step: 1760, train/epoch: 0.4188481569290161
Step: 1770, train/loss: 0.0005000000237487257
Step: 1770, train/grad_norm: 1.2690560602379719e-08
Step: 1770, train/learning_rate: 4.2979532736353576e-05
Step: 1770, train/epoch: 0.42122799158096313
Step: 1780, train/loss: 0.0
Step: 1780, train/grad_norm: 9.133740604738705e-06
Step: 1780, train/learning_rate: 4.293987149139866e-05
Step: 1780, train/epoch: 0.42360779643058777
Step: 1790, train/loss: 0.0
Step: 1790, train/grad_norm: 1.778073097557977e-14
Step: 1790, train/learning_rate: 4.290020660846494e-05
Step: 1790, train/epoch: 0.4259876310825348
Step: 1800, train/loss: 0.0
Step: 1800, train/grad_norm: 1.1457150506100078e-10
Step: 1800, train/learning_rate: 4.286054172553122e-05
Step: 1800, train/epoch: 0.4283674359321594
Step: 1810, train/loss: 0.0
Step: 1810, train/grad_norm: 4.3679739825108754e-09
Step: 1810, train/learning_rate: 4.2820880480576307e-05
Step: 1810, train/epoch: 0.43074727058410645
Step: 1820, train/loss: 0.0
Step: 1820, train/grad_norm: 0.0013183319242671132
Step: 1820, train/learning_rate: 4.2781215597642586e-05
Step: 1820, train/epoch: 0.4331270754337311
Step: 1830, train/loss: 9.999999747378752e-05
Step: 1830, train/grad_norm: 2.8070453947370844e-11
Step: 1830, train/learning_rate: 4.2741550714708865e-05
Step: 1830, train/epoch: 0.4355069100856781
Step: 1840, train/loss: 0.0
Step: 1840, train/grad_norm: 1.9839964632110962e-14
Step: 1840, train/learning_rate: 4.270188946975395e-05
Step: 1840, train/epoch: 0.43788671493530273
Step: 1850, train/loss: 0.12120000272989273
Step: 1850, train/grad_norm: 235.7143096923828
Step: 1850, train/learning_rate: 4.266222458682023e-05
Step: 1850, train/epoch: 0.44026654958724976
Step: 1860, train/loss: 0.0
Step: 1860, train/grad_norm: 6.564560521837848e-07
Step: 1860, train/learning_rate: 4.262255970388651e-05
Step: 1860, train/epoch: 0.4426463544368744
Step: 1870, train/loss: 0.0
Step: 1870, train/grad_norm: 3.102418877354296e-10
Step: 1870, train/learning_rate: 4.2582898458931595e-05
Step: 1870, train/epoch: 0.4450261890888214
Step: 1880, train/loss: 0.11089999973773956
Step: 1880, train/grad_norm: 1.8477204832834104e-07
Step: 1880, train/learning_rate: 4.2543233575997874e-05
Step: 1880, train/epoch: 0.44740599393844604
Step: 1890, train/loss: 0.0
Step: 1890, train/grad_norm: 7.67909796195454e-07
Step: 1890, train/learning_rate: 4.250356869306415e-05
Step: 1890, train/epoch: 0.44978582859039307
Step: 1900, train/loss: 0.0
Step: 1900, train/grad_norm: 2.8200872193018256e-10
Step: 1900, train/learning_rate: 4.246390744810924e-05
Step: 1900, train/epoch: 0.4521656334400177
Step: 1910, train/loss: 0.0
Step: 1910, train/grad_norm: 1.6435620864285738e-06
Step: 1910, train/learning_rate: 4.242424256517552e-05
Step: 1910, train/epoch: 0.4545454680919647
Step: 1920, train/loss: 0.05739999935030937
Step: 1920, train/grad_norm: 1.260685280612961e-06
Step: 1920, train/learning_rate: 4.23845776822418e-05
Step: 1920, train/epoch: 0.45692527294158936
Step: 1930, train/loss: 0.0
Step: 1930, train/grad_norm: 5.835721094626933e-05
Step: 1930, train/learning_rate: 4.2344916437286884e-05
Step: 1930, train/epoch: 0.4593051075935364
Step: 1940, train/loss: 0.0
Step: 1940, train/grad_norm: 4.004453019179621e-11
Step: 1940, train/learning_rate: 4.230525155435316e-05
Step: 1940, train/epoch: 0.461684912443161
Step: 1950, train/loss: 0.0007999999797903001
Step: 1950, train/grad_norm: 6.58177397132309e-13
Step: 1950, train/learning_rate: 4.226558667141944e-05
Step: 1950, train/epoch: 0.46406471729278564
Step: 1960, train/loss: 0.0
Step: 1960, train/grad_norm: 9.943308112644877e-13
Step: 1960, train/learning_rate: 4.222592542646453e-05
Step: 1960, train/epoch: 0.46644455194473267
Step: 1970, train/loss: 0.0
Step: 1970, train/grad_norm: 7.119020750678828e-08
Step: 1970, train/learning_rate: 4.218626054353081e-05
Step: 1970, train/epoch: 0.4688243567943573
Step: 1980, train/loss: 0.0
Step: 1980, train/grad_norm: 0.0003421432920731604
Step: 1980, train/learning_rate: 4.2146595660597086e-05
Step: 1980, train/epoch: 0.4712041914463043
Step: 1990, train/loss: 0.0
Step: 1990, train/grad_norm: 6.005745945003582e-08
Step: 1990, train/learning_rate: 4.210693441564217e-05
Step: 1990, train/epoch: 0.47358399629592896
Step: 2000, train/loss: 0.0
Step: 2000, train/grad_norm: 1.2884308897564978e-10
Step: 2000, train/learning_rate: 4.206726953270845e-05
Step: 2000, train/epoch: 0.475963830947876
Step: 2010, train/loss: 0.08590000122785568
Step: 2010, train/grad_norm: 0.00011393507156753913
Step: 2010, train/learning_rate: 4.202760464977473e-05
Step: 2010, train/epoch: 0.4783436357975006
Step: 2020, train/loss: 0.0
Step: 2020, train/grad_norm: 1.3499903843694483e-06
Step: 2020, train/learning_rate: 4.1987943404819816e-05
Step: 2020, train/epoch: 0.48072347044944763
Step: 2030, train/loss: 0.00019999999494757503
Step: 2030, train/grad_norm: 1.389158211205499e-12
Step: 2030, train/learning_rate: 4.1948278521886095e-05
Step: 2030, train/epoch: 0.48310327529907227
Step: 2040, train/loss: 0.0210999995470047
Step: 2040, train/grad_norm: 7.05028979641753e-10
Step: 2040, train/learning_rate: 4.1908613638952374e-05
Step: 2040, train/epoch: 0.4854831099510193
Step: 2050, train/loss: 9.999999747378752e-05
Step: 2050, train/grad_norm: 2.4669077902217396e-05
Step: 2050, train/learning_rate: 4.186895239399746e-05
Step: 2050, train/epoch: 0.4878629148006439
Step: 2060, train/loss: 0.0013000000035390258
Step: 2060, train/grad_norm: 6.480795761154567e-11
Step: 2060, train/learning_rate: 4.182928751106374e-05
Step: 2060, train/epoch: 0.49024274945259094
Step: 2070, train/loss: 0.009200000204145908
Step: 2070, train/grad_norm: 1.890004302107099e-18
Step: 2070, train/learning_rate: 4.178962262813002e-05
Step: 2070, train/epoch: 0.4926225543022156
Step: 2080, train/loss: 0.0
Step: 2080, train/grad_norm: 4.550493013580748e-21
Step: 2080, train/learning_rate: 4.1749961383175105e-05
Step: 2080, train/epoch: 0.4950023889541626
Step: 2090, train/loss: 0.0
Step: 2090, train/grad_norm: 0.0
Step: 2090, train/learning_rate: 4.1710296500241384e-05
Step: 2090, train/epoch: 0.49738219380378723
Step: 2100, train/loss: 0.0
Step: 2100, train/grad_norm: 0.0
Step: 2100, train/learning_rate: 4.167063161730766e-05
Step: 2100, train/epoch: 0.49976202845573425
Step: 2110, train/loss: 0.0
Step: 2110, train/grad_norm: 3.88889411055731e-18
Step: 2110, train/learning_rate: 4.163097037235275e-05
Step: 2110, train/epoch: 0.5021418333053589
Step: 2120, train/loss: 0.8008000254631042
Step: 2120, train/grad_norm: 0.00010262933210469782
Step: 2120, train/learning_rate: 4.159130548941903e-05
Step: 2120, train/epoch: 0.5045216679573059
Step: 2130, train/loss: 0.0
Step: 2130, train/grad_norm: 7.009639721965166e-13
Step: 2130, train/learning_rate: 4.155164060648531e-05
Step: 2130, train/epoch: 0.5069015026092529
Step: 2140, train/loss: 0.0017000000225380063
Step: 2140, train/grad_norm: 34.36040496826172
Step: 2140, train/learning_rate: 4.151197936153039e-05
Step: 2140, train/epoch: 0.5092812776565552
Step: 2150, train/loss: 0.0
Step: 2150, train/grad_norm: 3.6149035154142695e-14
Step: 2150, train/learning_rate: 4.147231447859667e-05
Step: 2150, train/epoch: 0.5116611123085022
Step: 2160, train/loss: 0.0
Step: 2160, train/grad_norm: 1.937257299022832e-10
Step: 2160, train/learning_rate: 4.143264959566295e-05
Step: 2160, train/epoch: 0.5140409469604492
Step: 2170, train/loss: 0.6812999844551086
Step: 2170, train/grad_norm: 4.936901754569638e-18
Step: 2170, train/learning_rate: 4.139298835070804e-05
Step: 2170, train/epoch: 0.5164207816123962
Step: 2180, train/loss: 0.0
Step: 2180, train/grad_norm: 8.460387338621311e-13
Step: 2180, train/learning_rate: 4.135332346777432e-05
Step: 2180, train/epoch: 0.5188005566596985
Step: 2190, train/loss: 0.0
Step: 2190, train/grad_norm: 7.13236090632563e-07
Step: 2190, train/learning_rate: 4.1313658584840596e-05
Step: 2190, train/epoch: 0.5211803913116455
Step: 2200, train/loss: 0.0
Step: 2200, train/grad_norm: 3.314924723607504e-10
Step: 2200, train/learning_rate: 4.127399733988568e-05
Step: 2200, train/epoch: 0.5235602259635925
Step: 2210, train/loss: 0.31459999084472656
Step: 2210, train/grad_norm: 2.1370412384147386e-11
Step: 2210, train/learning_rate: 4.123433245695196e-05
Step: 2210, train/epoch: 0.5259400010108948
Step: 2220, train/loss: 0.0
Step: 2220, train/grad_norm: 4.04928347796929e-11
Step: 2220, train/learning_rate: 4.119466757401824e-05
Step: 2220, train/epoch: 0.5283198356628418
Step: 2230, train/loss: 0.0
Step: 2230, train/grad_norm: 7.522388744893305e-13
Step: 2230, train/learning_rate: 4.1155006329063326e-05
Step: 2230, train/epoch: 0.5306996703147888
Step: 2240, train/loss: 9.999999747378752e-05
Step: 2240, train/grad_norm: 0.0002068059693556279
Step: 2240, train/learning_rate: 4.1115341446129605e-05
Step: 2240, train/epoch: 0.5330795049667358
Step: 2250, train/loss: 9.999999747378752e-05
Step: 2250, train/grad_norm: 8.382648687601385e-14
Step: 2250, train/learning_rate: 4.1075676563195884e-05
Step: 2250, train/epoch: 0.5354592800140381
Step: 2260, train/loss: 0.13359999656677246
Step: 2260, train/grad_norm: 3.438961060364676e-10
Step: 2260, train/learning_rate: 4.103601531824097e-05
Step: 2260, train/epoch: 0.5378391146659851
Step: 2270, train/loss: 0.025599999353289604
Step: 2270, train/grad_norm: 0.0011329250410199165
Step: 2270, train/learning_rate: 4.099635043530725e-05
Step: 2270, train/epoch: 0.5402189493179321
Step: 2280, train/loss: 0.15940000116825104
Step: 2280, train/grad_norm: 4.688875378633384e-06
Step: 2280, train/learning_rate: 4.095668555237353e-05
Step: 2280, train/epoch: 0.5425987839698792
Step: 2290, train/loss: 0.0
Step: 2290, train/grad_norm: 1.0849588534256327e-06
Step: 2290, train/learning_rate: 4.0917024307418615e-05
Step: 2290, train/epoch: 0.5449785590171814
Step: 2300, train/loss: 0.0
Step: 2300, train/grad_norm: 1.2128720675908845e-14
Step: 2300, train/learning_rate: 4.0877359424484894e-05
Step: 2300, train/epoch: 0.5473583936691284
Step: 2310, train/loss: 0.0
Step: 2310, train/grad_norm: 4.3012366148797873e-17
Step: 2310, train/learning_rate: 4.083769454155117e-05
Step: 2310, train/epoch: 0.5497382283210754
Step: 2320, train/loss: 0.0010000000474974513
Step: 2320, train/grad_norm: 2.744756292042591e-18
Step: 2320, train/learning_rate: 4.079803329659626e-05
Step: 2320, train/epoch: 0.5521180629730225
Step: 2330, train/loss: 0.0
Step: 2330, train/grad_norm: 7.980738499799387e-15
Step: 2330, train/learning_rate: 4.075836841366254e-05
Step: 2330, train/epoch: 0.5544978380203247
Step: 2340, train/loss: 0.002899999963119626
Step: 2340, train/grad_norm: 1.50227838262147e-18
Step: 2340, train/learning_rate: 4.0718707168707624e-05
Step: 2340, train/epoch: 0.5568776726722717
Step: 2350, train/loss: 0.0
Step: 2350, train/grad_norm: 1.3939593916003806e-18
Step: 2350, train/learning_rate: 4.06790422857739e-05
Step: 2350, train/epoch: 0.5592575073242188
Step: 2360, train/loss: 0.0
Step: 2360, train/grad_norm: 3.815137095131149e-10
Step: 2360, train/learning_rate: 4.063937740284018e-05
Step: 2360, train/epoch: 0.5616373419761658
Step: 2370, train/loss: 0.0
Step: 2370, train/grad_norm: 2.7804384229029866e-12
Step: 2370, train/learning_rate: 4.059971615788527e-05
Step: 2370, train/epoch: 0.564017117023468
Step: 2380, train/loss: 0.008700000122189522
Step: 2380, train/grad_norm: 1.639957429837996e-13
Step: 2380, train/learning_rate: 4.056005127495155e-05
Step: 2380, train/epoch: 0.566396951675415
Step: 2390, train/loss: 0.0
Step: 2390, train/grad_norm: 1.6105803750809712e-17
Step: 2390, train/learning_rate: 4.0520386392017826e-05
Step: 2390, train/epoch: 0.5687767863273621
Step: 2400, train/loss: 0.1671999990940094
Step: 2400, train/grad_norm: 1.5367325467696702e-12
Step: 2400, train/learning_rate: 4.048072514706291e-05
Step: 2400, train/epoch: 0.5711566209793091
Step: 2410, train/loss: 0.0
Step: 2410, train/grad_norm: 3.1533572516593383e-19
Step: 2410, train/learning_rate: 4.044106026412919e-05
Step: 2410, train/epoch: 0.5735363960266113
Step: 2420, train/loss: 0.00019999999494757503
Step: 2420, train/grad_norm: 4.98148966521228e-10
Step: 2420, train/learning_rate: 4.040139538119547e-05
Step: 2420, train/epoch: 0.5759162306785583
Step: 2430, train/loss: 0.0
Step: 2430, train/grad_norm: 9.735835471147425e-10
Step: 2430, train/learning_rate: 4.036173413624056e-05
Step: 2430, train/epoch: 0.5782960653305054
Step: 2440, train/loss: 0.0
Step: 2440, train/grad_norm: 3.2994611274921037e-13
Step: 2440, train/learning_rate: 4.0322069253306836e-05
Step: 2440, train/epoch: 0.5806758403778076
Step: 2450, train/loss: 0.0
Step: 2450, train/grad_norm: 7.610236997777716e-17
Step: 2450, train/learning_rate: 4.0282404370373115e-05
Step: 2450, train/epoch: 0.5830556750297546
Step: 2460, train/loss: 0.0
Step: 2460, train/grad_norm: 1.706697838610438e-10
Step: 2460, train/learning_rate: 4.02427431254182e-05
Step: 2460, train/epoch: 0.5854355096817017
Step: 2470, train/loss: 0.0
Step: 2470, train/grad_norm: 1.4727382119835528e-20
Step: 2470, train/learning_rate: 4.020307824248448e-05
Step: 2470, train/epoch: 0.5878153443336487
Step: 2480, train/loss: 0.0
Step: 2480, train/grad_norm: 9.904084925088661e-23
Step: 2480, train/learning_rate: 4.016341335955076e-05
Step: 2480, train/epoch: 0.5901951193809509
Step: 2490, train/loss: 0.0
Step: 2490, train/grad_norm: 1.4994276758666927e-15
Step: 2490, train/learning_rate: 4.0123752114595845e-05
Step: 2490, train/epoch: 0.592574954032898
Step: 2500, train/loss: 0.0
Step: 2500, train/grad_norm: 1.8489240571952722e-19
Step: 2500, train/learning_rate: 4.0084087231662124e-05
Step: 2500, train/epoch: 0.594954788684845
Step: 2510, train/loss: 0.0
Step: 2510, train/grad_norm: 0.0
Step: 2510, train/learning_rate: 4.0044422348728403e-05
Step: 2510, train/epoch: 0.597334623336792
Step: 2520, train/loss: 0.0006000000284984708
Step: 2520, train/grad_norm: 4.713702049227283e-21
Step: 2520, train/learning_rate: 4.000476110377349e-05
Step: 2520, train/epoch: 0.5997143983840942
Step: 2530, train/loss: 0.0
Step: 2530, train/grad_norm: 3.622590236752318e-21
Step: 2530, train/learning_rate: 3.996509622083977e-05
Step: 2530, train/epoch: 0.6020942330360413
Step: 2540, train/loss: 0.0
Step: 2540, train/grad_norm: 0.0
Step: 2540, train/learning_rate: 3.992543133790605e-05
Step: 2540, train/epoch: 0.6044740676879883
Step: 2550, train/loss: 0.0
Step: 2550, train/grad_norm: 1.5380559151306917e-20
Step: 2550, train/learning_rate: 3.9885770092951134e-05
Step: 2550, train/epoch: 0.6068539023399353
Step: 2560, train/loss: 0.0
Step: 2560, train/grad_norm: 2.458260063836954e-19
Step: 2560, train/learning_rate: 3.984610521001741e-05
Step: 2560, train/epoch: 0.6092336773872375
Step: 2570, train/loss: 0.1234000027179718
Step: 2570, train/grad_norm: 1.6438181878136076e-18
Step: 2570, train/learning_rate: 3.980644032708369e-05
Step: 2570, train/epoch: 0.6116135120391846
Step: 2580, train/loss: 0.0
Step: 2580, train/grad_norm: 1.2606011247950672e-15
Step: 2580, train/learning_rate: 3.976677908212878e-05
Step: 2580, train/epoch: 0.6139933466911316
Step: 2590, train/loss: 0.0
Step: 2590, train/grad_norm: 1.022158553600427e-20
Step: 2590, train/learning_rate: 3.972711419919506e-05
Step: 2590, train/epoch: 0.6163731813430786
Step: 2600, train/loss: 0.05939999967813492
Step: 2600, train/grad_norm: 0.0
Step: 2600, train/learning_rate: 3.9687449316261336e-05
Step: 2600, train/epoch: 0.6187529563903809
Step: 2610, train/loss: 0.0
Step: 2610, train/grad_norm: 1.6236020177199035e-17
Step: 2610, train/learning_rate: 3.964778807130642e-05
Step: 2610, train/epoch: 0.6211327910423279
Step: 2620, train/loss: 0.0
Step: 2620, train/grad_norm: 0.0
Step: 2620, train/learning_rate: 3.96081231883727e-05
Step: 2620, train/epoch: 0.6235126256942749
Step: 2630, train/loss: 0.002300000051036477
Step: 2630, train/grad_norm: 1.3654079085896175e-19
Step: 2630, train/learning_rate: 3.956845830543898e-05
Step: 2630, train/epoch: 0.6258924603462219
Step: 2640, train/loss: 0.0
Step: 2640, train/grad_norm: 5.142945375347299e-20
Step: 2640, train/learning_rate: 3.9528797060484067e-05
Step: 2640, train/epoch: 0.6282722353935242
Step: 2650, train/loss: 0.0
Step: 2650, train/grad_norm: 5.293955920339377e-23
Step: 2650, train/learning_rate: 3.9489132177550346e-05
Step: 2650, train/epoch: 0.6306520700454712
Step: 2660, train/loss: 0.0
Step: 2660, train/grad_norm: 3.1441619480107293e-15
Step: 2660, train/learning_rate: 3.9449467294616625e-05
Step: 2660, train/epoch: 0.6330319046974182
Step: 2670, train/loss: 0.2515999972820282
Step: 2670, train/grad_norm: 0.0
Step: 2670, train/learning_rate: 3.940980604966171e-05
Step: 2670, train/epoch: 0.6354116797447205
Step: 2680, train/loss: 0.0
Step: 2680, train/grad_norm: 3.3532562000307406e-17
Step: 2680, train/learning_rate: 3.937014116672799e-05
Step: 2680, train/epoch: 0.6377915143966675
Step: 2690, train/loss: 0.0
Step: 2690, train/grad_norm: 6.483745598763743e-23
Step: 2690, train/learning_rate: 3.933047628379427e-05
Step: 2690, train/epoch: 0.6401713490486145
Step: 2700, train/loss: 0.0
Step: 2700, train/grad_norm: 1.0886032903321694e-19
Step: 2700, train/learning_rate: 3.9290815038839355e-05
Step: 2700, train/epoch: 0.6425511837005615
Step: 2710, train/loss: 0.0
Step: 2710, train/grad_norm: 0.0
Step: 2710, train/learning_rate: 3.9251150155905634e-05
Step: 2710, train/epoch: 0.6449309587478638
Step: 2720, train/loss: 0.0
Step: 2720, train/grad_norm: 2.729658539803485e-19
Step: 2720, train/learning_rate: 3.921148527297191e-05
Step: 2720, train/epoch: 0.6473107933998108
Step: 2730, train/loss: 0.0
Step: 2730, train/grad_norm: 6.530485109927306e-14
Step: 2730, train/learning_rate: 3.9171824028017e-05
Step: 2730, train/epoch: 0.6496906280517578
Step: 2740, train/loss: 0.0
Step: 2740, train/grad_norm: 2.2331363977821117e-18
Step: 2740, train/learning_rate: 3.913215914508328e-05
Step: 2740, train/epoch: 0.6520704627037048
Step: 2750, train/loss: 0.0
Step: 2750, train/grad_norm: 3.1493599013491758e-18
Step: 2750, train/learning_rate: 3.909249426214956e-05
Step: 2750, train/epoch: 0.6544502377510071
Step: 2760, train/loss: 0.0
Step: 2760, train/grad_norm: 0.0
Step: 2760, train/learning_rate: 3.9052833017194644e-05
Step: 2760, train/epoch: 0.6568300724029541
Step: 2770, train/loss: 0.0
Step: 2770, train/grad_norm: 0.0
Step: 2770, train/learning_rate: 3.901316813426092e-05
Step: 2770, train/epoch: 0.6592099070549011
Step: 2780, train/loss: 0.0026000000070780516
Step: 2780, train/grad_norm: 0.0
Step: 2780, train/learning_rate: 3.89735032513272e-05
Step: 2780, train/epoch: 0.6615897417068481
Step: 2790, train/loss: 0.0
Step: 2790, train/grad_norm: 3.400984430902171e-20
Step: 2790, train/learning_rate: 3.893384200637229e-05
Step: 2790, train/epoch: 0.6639695167541504
Step: 2800, train/loss: 0.0
Step: 2800, train/grad_norm: 1.6274623174374558e-15
Step: 2800, train/learning_rate: 3.889417712343857e-05
Step: 2800, train/epoch: 0.6663493514060974
Step: 2810, train/loss: 0.0
Step: 2810, train/grad_norm: 5.564833533214242e-13
Step: 2810, train/learning_rate: 3.8854512240504846e-05
Step: 2810, train/epoch: 0.6687291860580444
Step: 2820, train/loss: 0.0
Step: 2820, train/grad_norm: 2.74140030397519e-20
Step: 2820, train/learning_rate: 3.881485099554993e-05
Step: 2820, train/epoch: 0.6711090207099915
Step: 2830, train/loss: 0.22130000591278076
Step: 2830, train/grad_norm: 0.0
Step: 2830, train/learning_rate: 3.877518611261621e-05
Step: 2830, train/epoch: 0.6734887957572937
Step: 2840, train/loss: 0.0
Step: 2840, train/grad_norm: 1.3226975336747535e-16
Step: 2840, train/learning_rate: 3.873552122968249e-05
Step: 2840, train/epoch: 0.6758686304092407
Step: 2850, train/loss: 0.5562999844551086
Step: 2850, train/grad_norm: 1.356992971266794e-16
Step: 2850, train/learning_rate: 3.8695859984727576e-05
Step: 2850, train/epoch: 0.6782484650611877
Step: 2860, train/loss: 0.0
Step: 2860, train/grad_norm: 5.39316769021525e-09
Step: 2860, train/learning_rate: 3.8656195101793855e-05
Step: 2860, train/epoch: 0.6806282997131348
Step: 2870, train/loss: 0.08399999886751175
Step: 2870, train/grad_norm: 0.00030527563649229705
Step: 2870, train/learning_rate: 3.8616530218860134e-05
Step: 2870, train/epoch: 0.683008074760437
Step: 2880, train/loss: 0.4440999925136566
Step: 2880, train/grad_norm: 5.346047909915441e-11
Step: 2880, train/learning_rate: 3.857686897390522e-05
Step: 2880, train/epoch: 0.685387909412384
Step: 2890, train/loss: 0.0
Step: 2890, train/grad_norm: 2.1906775005131607e-11
Step: 2890, train/learning_rate: 3.85372040909715e-05
Step: 2890, train/epoch: 0.687767744064331
Step: 2900, train/loss: 0.05079999938607216
Step: 2900, train/grad_norm: 2.7492955134711394e-10
Step: 2900, train/learning_rate: 3.849753920803778e-05
Step: 2900, train/epoch: 0.6901475191116333
Step: 2910, train/loss: 0.0
Step: 2910, train/grad_norm: 1.151594227963293e-12
Step: 2910, train/learning_rate: 3.8457877963082865e-05
Step: 2910, train/epoch: 0.6925273537635803
Step: 2920, train/loss: 0.20229999721050262
Step: 2920, train/grad_norm: 3.0462229960726006e-11
Step: 2920, train/learning_rate: 3.8418213080149144e-05
Step: 2920, train/epoch: 0.6949071884155273
Step: 2930, train/loss: 0.0
Step: 2930, train/grad_norm: 3.719465291318613e-10
Step: 2930, train/learning_rate: 3.837854819721542e-05
Step: 2930, train/epoch: 0.6972870230674744
Step: 2940, train/loss: 0.0
Step: 2940, train/grad_norm: 6.303182975564425e-11
Step: 2940, train/learning_rate: 3.833888695226051e-05
Step: 2940, train/epoch: 0.6996667981147766
Step: 2950, train/loss: 0.0
Step: 2950, train/grad_norm: 2.3979731977874508e-08
Step: 2950, train/learning_rate: 3.829922206932679e-05
Step: 2950, train/epoch: 0.7020466327667236
Step: 2960, train/loss: 0.529699981212616
Step: 2960, train/grad_norm: 2.1298879326892006e-11
Step: 2960, train/learning_rate: 3.825955718639307e-05
Step: 2960, train/epoch: 0.7044264674186707
Step: 2970, train/loss: 0.00039999998989515007
Step: 2970, train/grad_norm: 0.009656603448092937
Step: 2970, train/learning_rate: 3.821989594143815e-05
Step: 2970, train/epoch: 0.7068063020706177
Step: 2980, train/loss: 9.999999747378752e-05
Step: 2980, train/grad_norm: 0.008048453368246555
Step: 2980, train/learning_rate: 3.818023105850443e-05
Step: 2980, train/epoch: 0.7091860771179199
Step: 2990, train/loss: 0.0
Step: 2990, train/grad_norm: 3.971511887357337e-06
Step: 2990, train/learning_rate: 3.814056617557071e-05
Step: 2990, train/epoch: 0.7115659117698669
Step: 3000, train/loss: 9.999999747378752e-05
Step: 3000, train/grad_norm: 4.588124738802435e-06
Step: 3000, train/learning_rate: 3.81009049306158e-05
Step: 3000, train/epoch: 0.713945746421814
Step: 3010, train/loss: 0.0
Step: 3010, train/grad_norm: 0.0015666419640183449
Step: 3010, train/learning_rate: 3.806124004768208e-05
Step: 3010, train/epoch: 0.716325581073761
Step: 3020, train/loss: 0.0
Step: 3020, train/grad_norm: 2.288998643962259e-07
Step: 3020, train/learning_rate: 3.802157880272716e-05
Step: 3020, train/epoch: 0.7187053561210632
Step: 3030, train/loss: 0.0
Step: 3030, train/grad_norm: 0.012879861518740654
Step: 3030, train/learning_rate: 3.798191391979344e-05
Step: 3030, train/epoch: 0.7210851907730103
Step: 3040, train/loss: 0.0
Step: 3040, train/grad_norm: 6.680140329784479e-10
Step: 3040, train/learning_rate: 3.794224903685972e-05
Step: 3040, train/epoch: 0.7234650254249573
Step: 3050, train/loss: 0.0
Step: 3050, train/grad_norm: 1.6069138553120865e-07
Step: 3050, train/learning_rate: 3.790258779190481e-05
Step: 3050, train/epoch: 0.7258448600769043
Step: 3060, train/loss: 0.15549999475479126
Step: 3060, train/grad_norm: 9.452224730921444e-06
Step: 3060, train/learning_rate: 3.7862922908971086e-05
Step: 3060, train/epoch: 0.7282246351242065
Step: 3070, train/loss: 9.999999747378752e-05
Step: 3070, train/grad_norm: 0.010042889043688774
Step: 3070, train/learning_rate: 3.7823258026037365e-05
Step: 3070, train/epoch: 0.7306044697761536
Step: 3080, train/loss: 0.0
Step: 3080, train/grad_norm: 0.00021075828408356756
Step: 3080, train/learning_rate: 3.778359678108245e-05
Step: 3080, train/epoch: 0.7329843044281006
Step: 3090, train/loss: 0.01600000075995922
Step: 3090, train/grad_norm: 1.7422431497493562e-08
Step: 3090, train/learning_rate: 3.774393189814873e-05
Step: 3090, train/epoch: 0.7353641390800476
Step: 3100, train/loss: 0.0
Step: 3100, train/grad_norm: 1.1468890073751403e-11
Step: 3100, train/learning_rate: 3.770426701521501e-05
Step: 3100, train/epoch: 0.7377439141273499
Step: 3110, train/loss: 0.0
Step: 3110, train/grad_norm: 7.377390155206571e-16
Step: 3110, train/learning_rate: 3.7664605770260096e-05
Step: 3110, train/epoch: 0.7401237487792969
Step: 3120, train/loss: 0.0
Step: 3120, train/grad_norm: 1.0437181785505345e-08
Step: 3120, train/learning_rate: 3.7624940887326375e-05
Step: 3120, train/epoch: 0.7425035834312439
Step: 3130, train/loss: 0.0
Step: 3130, train/grad_norm: 2.5360047395395213e-09
Step: 3130, train/learning_rate: 3.7585276004392654e-05
Step: 3130, train/epoch: 0.7448834180831909
Step: 3140, train/loss: 0.0
Step: 3140, train/grad_norm: 8.264501062671403e-12
Step: 3140, train/learning_rate: 3.754561475943774e-05
Step: 3140, train/epoch: 0.7472631931304932
Step: 3150, train/loss: 0.42149999737739563
Step: 3150, train/grad_norm: 9.375592316396286e-12
Step: 3150, train/learning_rate: 3.750594987650402e-05
Step: 3150, train/epoch: 0.7496430277824402
Step: 3160, train/loss: 0.12110000103712082
Step: 3160, train/grad_norm: 8.005424660950666e-07
Step: 3160, train/learning_rate: 3.74662849935703e-05
Step: 3160, train/epoch: 0.7520228624343872
Step: 3170, train/loss: 0.0
Step: 3170, train/grad_norm: 2.4103956677845595e-11
Step: 3170, train/learning_rate: 3.7426623748615384e-05
Step: 3170, train/epoch: 0.7544026374816895
Step: 3180, train/loss: 0.0
Step: 3180, train/grad_norm: 1.617319966840114e-08
Step: 3180, train/learning_rate: 3.738695886568166e-05
Step: 3180, train/epoch: 0.7567824721336365
Step: 3190, train/loss: 2.293800115585327
Step: 3190, train/grad_norm: 2.651347807969273e-09
Step: 3190, train/learning_rate: 3.734729398274794e-05
Step: 3190, train/epoch: 0.7591623067855835
Step: 3200, train/loss: 0.4269999861717224
Step: 3200, train/grad_norm: 39020.6953125
Step: 3200, train/learning_rate: 3.730763273779303e-05
Step: 3200, train/epoch: 0.7615421414375305
Step: 3210, train/loss: 0.0
Step: 3210, train/grad_norm: 4.424333383212797e-06
Step: 3210, train/learning_rate: 3.726796785485931e-05
Step: 3210, train/epoch: 0.7639219164848328
Step: 3220, train/loss: 0.3093999922275543
Step: 3220, train/grad_norm: 6.434268229327245e-09
Step: 3220, train/learning_rate: 3.7228302971925586e-05
Step: 3220, train/epoch: 0.7663017511367798
Step: 3230, train/loss: 0.0
Step: 3230, train/grad_norm: 3.792591201090545e-07
Step: 3230, train/learning_rate: 3.718864172697067e-05
Step: 3230, train/epoch: 0.7686815857887268
Step: 3240, train/loss: 0.03009999915957451
Step: 3240, train/grad_norm: 5.159790089237504e-05
Step: 3240, train/learning_rate: 3.714897684403695e-05
Step: 3240, train/epoch: 0.7710614204406738
Step: 3250, train/loss: 0.032999999821186066
Step: 3250, train/grad_norm: 1.0221014235867187e-05
Step: 3250, train/learning_rate: 3.710931196110323e-05
Step: 3250, train/epoch: 0.7734411954879761
Step: 3260, train/loss: 0.04010000079870224
Step: 3260, train/grad_norm: 0.00568530336022377
Step: 3260, train/learning_rate: 3.706965071614832e-05
Step: 3260, train/epoch: 0.7758210301399231
Step: 3270, train/loss: 0.21879999339580536
Step: 3270, train/grad_norm: 3.285085767856799e-05
Step: 3270, train/learning_rate: 3.7029985833214596e-05
Step: 3270, train/epoch: 0.7782008647918701
Step: 3280, train/loss: 0.0
Step: 3280, train/grad_norm: 0.0010751362424343824
Step: 3280, train/learning_rate: 3.6990320950280875e-05
Step: 3280, train/epoch: 0.7805806994438171
Step: 3290, train/loss: 0.0
Step: 3290, train/grad_norm: 3.5624198062578216e-05
Step: 3290, train/learning_rate: 3.695065970532596e-05
Step: 3290, train/epoch: 0.7829604744911194
Step: 3300, train/loss: 0.0
Step: 3300, train/grad_norm: 8.004428309504874e-06
Step: 3300, train/learning_rate: 3.691099482239224e-05
Step: 3300, train/epoch: 0.7853403091430664
Step: 3310, train/loss: 0.0
Step: 3310, train/grad_norm: 2.1138653327756884e-10
Step: 3310, train/learning_rate: 3.687132993945852e-05
Step: 3310, train/epoch: 0.7877201437950134
Step: 3320, train/loss: 0.0
Step: 3320, train/grad_norm: 3.996141458628699e-05
Step: 3320, train/learning_rate: 3.6831668694503605e-05
Step: 3320, train/epoch: 0.7900999784469604
Step: 3330, train/loss: 0.0
Step: 3330, train/grad_norm: 0.00261782668530941
Step: 3330, train/learning_rate: 3.6792003811569884e-05
Step: 3330, train/epoch: 0.7924797534942627
Step: 3340, train/loss: 0.0
Step: 3340, train/grad_norm: 0.00034103417419828475
Step: 3340, train/learning_rate: 3.6752338928636163e-05
Step: 3340, train/epoch: 0.7948595881462097
Step: 3350, train/loss: 0.0
Step: 3350, train/grad_norm: 5.633263572235592e-05
Step: 3350, train/learning_rate: 3.671267768368125e-05
Step: 3350, train/epoch: 0.7972394227981567
Step: 3360, train/loss: 0.0
Step: 3360, train/grad_norm: 1.168680773844244e-05
Step: 3360, train/learning_rate: 3.667301280074753e-05
Step: 3360, train/epoch: 0.7996192574501038
Step: 3370, train/loss: 0.0
Step: 3370, train/grad_norm: 1.3070969544060063e-05
Step: 3370, train/learning_rate: 3.663334791781381e-05
Step: 3370, train/epoch: 0.801999032497406
Step: 3380, train/loss: 0.0
Step: 3380, train/grad_norm: 1.6768784917076118e-05
Step: 3380, train/learning_rate: 3.6593686672858894e-05
Step: 3380, train/epoch: 0.804378867149353
Step: 3390, train/loss: 0.0
Step: 3390, train/grad_norm: 2.5690569600556046e-05
Step: 3390, train/learning_rate: 3.655402178992517e-05
Step: 3390, train/epoch: 0.8067587018013
Step: 3400, train/loss: 0.0
Step: 3400, train/grad_norm: 0.00031202170066535473
Step: 3400, train/learning_rate: 3.651435690699145e-05
Step: 3400, train/epoch: 0.8091384768486023
Step: 3410, train/loss: 0.0
Step: 3410, train/grad_norm: 1.7713650777295697e-06
Step: 3410, train/learning_rate: 3.647469566203654e-05
Step: 3410, train/epoch: 0.8115183115005493
Step: 3420, train/loss: 0.0
Step: 3420, train/grad_norm: 2.3670887117077655e-07
Step: 3420, train/learning_rate: 3.643503077910282e-05
Step: 3420, train/epoch: 0.8138981461524963
Step: 3430, train/loss: 0.0
Step: 3430, train/grad_norm: 5.148843229108024e-06
Step: 3430, train/learning_rate: 3.6395365896169096e-05
Step: 3430, train/epoch: 0.8162779808044434
Step: 3440, train/loss: 0.18479999899864197
Step: 3440, train/grad_norm: 7.344429468503222e-05
Step: 3440, train/learning_rate: 3.635570465121418e-05
Step: 3440, train/epoch: 0.8186577558517456
Step: 3450, train/loss: 0.0
Step: 3450, train/grad_norm: 0.00034649300505407155
Step: 3450, train/learning_rate: 3.631603976828046e-05
Step: 3450, train/epoch: 0.8210375905036926
Step: 3460, train/loss: 9.999999747378752e-05
Step: 3460, train/grad_norm: 0.00012622418580576777
Step: 3460, train/learning_rate: 3.627637488534674e-05
Step: 3460, train/epoch: 0.8234174251556396
Step: 3470, train/loss: 0.0
Step: 3470, train/grad_norm: 0.048726506531238556
Step: 3470, train/learning_rate: 3.623671364039183e-05
Step: 3470, train/epoch: 0.8257972598075867
Step: 3480, train/loss: 0.36559998989105225
Step: 3480, train/grad_norm: 1.9301540305605158e-05
Step: 3480, train/learning_rate: 3.6197048757458106e-05
Step: 3480, train/epoch: 0.8281770348548889
Step: 3490, train/loss: 0.0
Step: 3490, train/grad_norm: 0.0015195623273029923
Step: 3490, train/learning_rate: 3.6157383874524385e-05
Step: 3490, train/epoch: 0.8305568695068359
Step: 3500, train/loss: 0.0003000000142492354
Step: 3500, train/grad_norm: 3.451445991231594e-07
Step: 3500, train/learning_rate: 3.611772262956947e-05
Step: 3500, train/epoch: 0.832936704158783
Step: 3510, train/loss: 0.0
Step: 3510, train/grad_norm: 4.142273155594012e-06
Step: 3510, train/learning_rate: 3.607805774663575e-05
Step: 3510, train/epoch: 0.83531653881073
Step: 3520, train/loss: 0.0
Step: 3520, train/grad_norm: 1.0316747648175806e-05
Step: 3520, train/learning_rate: 3.603839286370203e-05
Step: 3520, train/epoch: 0.8376963138580322
Step: 3530, train/loss: 0.0
Step: 3530, train/grad_norm: 2.072530878649559e-05
Step: 3530, train/learning_rate: 3.5998731618747115e-05
Step: 3530, train/epoch: 0.8400761485099792
Step: 3540, train/loss: 9.999999747378752e-05
Step: 3540, train/grad_norm: 2.4832013878040016e-05
Step: 3540, train/learning_rate: 3.5959066735813394e-05
Step: 3540, train/epoch: 0.8424559831619263
Step: 3550, train/loss: 0.0
Step: 3550, train/grad_norm: 5.541634777728177e-07
Step: 3550, train/learning_rate: 3.591940185287967e-05
Step: 3550, train/epoch: 0.8448358178138733
Step: 3560, train/loss: 0.0
Step: 3560, train/grad_norm: 2.035056058957707e-05
Step: 3560, train/learning_rate: 3.587974060792476e-05
Step: 3560, train/epoch: 0.8472155928611755
Step: 3570, train/loss: 0.0
Step: 3570, train/grad_norm: 4.4986558123127907e-07
Step: 3570, train/learning_rate: 3.584007572499104e-05
Step: 3570, train/epoch: 0.8495954275131226
Step: 3580, train/loss: 0.0
Step: 3580, train/grad_norm: 0.00033934172824956477
Step: 3580, train/learning_rate: 3.580041084205732e-05
Step: 3580, train/epoch: 0.8519752621650696
Step: 3590, train/loss: 0.0
Step: 3590, train/grad_norm: 4.376797591021386e-08
Step: 3590, train/learning_rate: 3.5760749597102404e-05
Step: 3590, train/epoch: 0.8543550968170166
Step: 3600, train/loss: 0.0
Step: 3600, train/grad_norm: 4.2461834937057574e-07
Step: 3600, train/learning_rate: 3.572108471416868e-05
Step: 3600, train/epoch: 0.8567348718643188
Step: 3610, train/loss: 0.0
Step: 3610, train/grad_norm: 3.0071078072069213e-07
Step: 3610, train/learning_rate: 3.568141983123496e-05
Step: 3610, train/epoch: 0.8591147065162659
Step: 3620, train/loss: 0.0
Step: 3620, train/grad_norm: 4.4438071199692786e-05
Step: 3620, train/learning_rate: 3.564175858628005e-05
Step: 3620, train/epoch: 0.8614945411682129
Step: 3630, train/loss: 0.0
Step: 3630, train/grad_norm: 0.0001278221607208252
Step: 3630, train/learning_rate: 3.560209370334633e-05
Step: 3630, train/epoch: 0.8638743162155151
Step: 3640, train/loss: 0.0
Step: 3640, train/grad_norm: 5.421488253887219e-07
Step: 3640, train/learning_rate: 3.5562428820412606e-05
Step: 3640, train/epoch: 0.8662541508674622
Step: 3650, train/loss: 0.0
Step: 3650, train/grad_norm: 2.5264305804739706e-05
Step: 3650, train/learning_rate: 3.552276757545769e-05
Step: 3650, train/epoch: 0.8686339855194092
Step: 3660, train/loss: 0.0
Step: 3660, train/grad_norm: 9.30105525185354e-05
Step: 3660, train/learning_rate: 3.548310269252397e-05
Step: 3660, train/epoch: 0.8710138201713562
Step: 3670, train/loss: 0.10729999840259552
Step: 3670, train/grad_norm: 0.0001616941881366074
Step: 3670, train/learning_rate: 3.544344144756906e-05
Step: 3670, train/epoch: 0.8733935952186584
Step: 3680, train/loss: 0.02019999921321869
Step: 3680, train/grad_norm: 0.0005882481927983463
Step: 3680, train/learning_rate: 3.5403776564635336e-05
Step: 3680, train/epoch: 0.8757734298706055
Step: 3690, train/loss: 0.0
Step: 3690, train/grad_norm: 3.623439326361222e-08
Step: 3690, train/learning_rate: 3.5364111681701615e-05
Step: 3690, train/epoch: 0.8781532645225525
Step: 3700, train/loss: 0.007199999876320362
Step: 3700, train/grad_norm: 0.0022612372413277626
Step: 3700, train/learning_rate: 3.53244504367467e-05
Step: 3700, train/epoch: 0.8805330991744995
Step: 3710, train/loss: 0.15860000252723694
Step: 3710, train/grad_norm: 2.9209499263771477e-09
Step: 3710, train/learning_rate: 3.528478555381298e-05
Step: 3710, train/epoch: 0.8829128742218018
Step: 3720, train/loss: 0.0
Step: 3720, train/grad_norm: 3.29061691672905e-07
Step: 3720, train/learning_rate: 3.524512067087926e-05
Step: 3720, train/epoch: 0.8852927088737488
Step: 3730, train/loss: 0.0
Step: 3730, train/grad_norm: 1.6054525531217223e-07
Step: 3730, train/learning_rate: 3.5205459425924346e-05
Step: 3730, train/epoch: 0.8876725435256958
Step: 3740, train/loss: 0.023000000044703484
Step: 3740, train/grad_norm: 2.0092134178639753e-08
Step: 3740, train/learning_rate: 3.5165794542990625e-05
Step: 3740, train/epoch: 0.8900523781776428
Step: 3750, train/loss: 0.0
Step: 3750, train/grad_norm: 2.3700385654024103e-08
Step: 3750, train/learning_rate: 3.5126129660056904e-05
Step: 3750, train/epoch: 0.8924321532249451
Step: 3760, train/loss: 0.0
Step: 3760, train/grad_norm: 2.6608979908360197e-08
Step: 3760, train/learning_rate: 3.508646841510199e-05
Step: 3760, train/epoch: 0.8948119878768921
Step: 3770, train/loss: 9.999999747378752e-05
Step: 3770, train/grad_norm: 1.7189084600133242e-09
Step: 3770, train/learning_rate: 3.504680353216827e-05
Step: 3770, train/epoch: 0.8971918225288391
Step: 3780, train/loss: 0.0
Step: 3780, train/grad_norm: 2.6175845935227926e-09
Step: 3780, train/learning_rate: 3.500713864923455e-05
Step: 3780, train/epoch: 0.8995716571807861
Step: 3790, train/loss: 0.0
Step: 3790, train/grad_norm: 7.017460224023608e-11
Step: 3790, train/learning_rate: 3.4967477404279634e-05
Step: 3790, train/epoch: 0.9019514322280884
Step: 3800, train/loss: 0.0
Step: 3800, train/grad_norm: 7.403833796049142e-11
Step: 3800, train/learning_rate: 3.4927812521345913e-05
Step: 3800, train/epoch: 0.9043312668800354
Step: 3810, train/loss: 0.009100000374019146
Step: 3810, train/grad_norm: 1.905120949885486e-09
Step: 3810, train/learning_rate: 3.488814763841219e-05
Step: 3810, train/epoch: 0.9067111015319824
Step: 3820, train/loss: 0.21879999339580536
Step: 3820, train/grad_norm: 5.33097697930085e-12
Step: 3820, train/learning_rate: 3.484848639345728e-05
Step: 3820, train/epoch: 0.9090909361839294
Step: 3830, train/loss: 0.0
Step: 3830, train/grad_norm: 6.507536731703567e-10
Step: 3830, train/learning_rate: 3.480882151052356e-05
Step: 3830, train/epoch: 0.9114707112312317
Step: 3840, train/loss: 0.0010999999940395355
Step: 3840, train/grad_norm: 1.2098843399144243e-05
Step: 3840, train/learning_rate: 3.476915662758984e-05
Step: 3840, train/epoch: 0.9138505458831787
Step: 3850, train/loss: 0.009600000455975533
Step: 3850, train/grad_norm: 9.97678071144037e-05
Step: 3850, train/learning_rate: 3.472949538263492e-05
Step: 3850, train/epoch: 0.9162303805351257
Step: 3860, train/loss: 0.00930000003427267
Step: 3860, train/grad_norm: 4.654207714338554e-06
Step: 3860, train/learning_rate: 3.46898304997012e-05
Step: 3860, train/epoch: 0.9186102151870728
Step: 3870, train/loss: 0.004699999932199717
Step: 3870, train/grad_norm: 1.1393556237360158e-10
Step: 3870, train/learning_rate: 3.465016561676748e-05
Step: 3870, train/epoch: 0.920989990234375
Step: 3880, train/loss: 0.0
Step: 3880, train/grad_norm: 3.444595009227669e-08
Step: 3880, train/learning_rate: 3.461050437181257e-05
Step: 3880, train/epoch: 0.923369824886322
Step: 3890, train/loss: 0.21559999883174896
Step: 3890, train/grad_norm: 5.756211618468399e-10
Step: 3890, train/learning_rate: 3.4570839488878846e-05
Step: 3890, train/epoch: 0.925749659538269
Step: 3900, train/loss: 0.0
Step: 3900, train/grad_norm: 3.3361067242054787e-09
Step: 3900, train/learning_rate: 3.4531174605945125e-05
Step: 3900, train/epoch: 0.9281294345855713
Step: 3910, train/loss: 0.0
Step: 3910, train/grad_norm: 9.407437573827337e-06
Step: 3910, train/learning_rate: 3.449151336099021e-05
Step: 3910, train/epoch: 0.9305092692375183
Step: 3920, train/loss: 0.0
Step: 3920, train/grad_norm: 8.007816177268978e-06
Step: 3920, train/learning_rate: 3.445184847805649e-05
Step: 3920, train/epoch: 0.9328891038894653
Step: 3930, train/loss: 0.0
Step: 3930, train/grad_norm: 0.006796570029109716
Step: 3930, train/learning_rate: 3.441218359512277e-05
Step: 3930, train/epoch: 0.9352689385414124
Step: 3940, train/loss: 0.0
Step: 3940, train/grad_norm: 9.894490915485221e-08
Step: 3940, train/learning_rate: 3.4372522350167856e-05
Step: 3940, train/epoch: 0.9376487135887146
Step: 3950, train/loss: 0.0003000000142492354
Step: 3950, train/grad_norm: 0.0014665836934000254
Step: 3950, train/learning_rate: 3.4332857467234135e-05
Step: 3950, train/epoch: 0.9400285482406616
Step: 3960, train/loss: 0.0
Step: 3960, train/grad_norm: 6.4731247029214956e-09
Step: 3960, train/learning_rate: 3.4293192584300414e-05
Step: 3960, train/epoch: 0.9424083828926086
Step: 3970, train/loss: 0.0
Step: 3970, train/grad_norm: 0.0004749726504087448
Step: 3970, train/learning_rate: 3.42535313393455e-05
Step: 3970, train/epoch: 0.9447882175445557
Step: 3980, train/loss: 0.0
Step: 3980, train/grad_norm: 1.6738400665516906e-09
Step: 3980, train/learning_rate: 3.421386645641178e-05
Step: 3980, train/epoch: 0.9471679925918579
Step: 3990, train/loss: 0.0
Step: 3990, train/grad_norm: 3.35412010665781e-11
Step: 3990, train/learning_rate: 3.417420157347806e-05
Step: 3990, train/epoch: 0.9495478272438049
Step: 4000, train/loss: 0.0
Step: 4000, train/grad_norm: 4.215165994025938e-10
Step: 4000, train/learning_rate: 3.4134540328523144e-05
Step: 4000, train/epoch: 0.951927661895752
Step: 4010, train/loss: 0.0
Step: 4010, train/grad_norm: 1.3667320608590217e-09
Step: 4010, train/learning_rate: 3.409487544558942e-05
Step: 4010, train/epoch: 0.954307496547699
Step: 4020, train/loss: 0.1460999995470047
Step: 4020, train/grad_norm: 0.0002072437055176124
Step: 4020, train/learning_rate: 3.40552105626557e-05
Step: 4020, train/epoch: 0.9566872715950012
Step: 4030, train/loss: 0.0
Step: 4030, train/grad_norm: 2.503908191897608e-09
Step: 4030, train/learning_rate: 3.401554931770079e-05
Step: 4030, train/epoch: 0.9590671062469482
Step: 4040, train/loss: 9.999999747378752e-05
Step: 4040, train/grad_norm: 1.3174698324291967e-05
Step: 4040, train/learning_rate: 3.397588443476707e-05
Step: 4040, train/epoch: 0.9614469408988953
Step: 4050, train/loss: 0.0
Step: 4050, train/grad_norm: 7.61060459097962e-09
Step: 4050, train/learning_rate: 3.3936219551833346e-05
Step: 4050, train/epoch: 0.9638267755508423
Step: 4060, train/loss: 0.0
Step: 4060, train/grad_norm: 5.297167504636491e-08
Step: 4060, train/learning_rate: 3.389655830687843e-05
Step: 4060, train/epoch: 0.9662065505981445
Step: 4070, train/loss: 0.0
Step: 4070, train/grad_norm: 1.0041036148322746e-05
Step: 4070, train/learning_rate: 3.385689342394471e-05
Step: 4070, train/epoch: 0.9685863852500916
Step: 4080, train/loss: 0.0
Step: 4080, train/grad_norm: 8.913286081435601e-11
Step: 4080, train/learning_rate: 3.381722854101099e-05
Step: 4080, train/epoch: 0.9709662199020386
Step: 4090, train/loss: 0.0
Step: 4090, train/grad_norm: 7.241523491963164e-10
Step: 4090, train/learning_rate: 3.377756729605608e-05
Step: 4090, train/epoch: 0.9733460545539856
Step: 4100, train/loss: 0.0
Step: 4100, train/grad_norm: 9.388697597145779e-10
Step: 4100, train/learning_rate: 3.3737902413122356e-05
Step: 4100, train/epoch: 0.9757258296012878
Step: 4110, train/loss: 0.0
Step: 4110, train/grad_norm: 7.994186423943006e-10
Step: 4110, train/learning_rate: 3.3698237530188635e-05
Step: 4110, train/epoch: 0.9781056642532349
Step: 4120, train/loss: 0.13600000739097595
Step: 4120, train/grad_norm: 5.026940197438989e-09
Step: 4120, train/learning_rate: 3.365857628523372e-05
Step: 4120, train/epoch: 0.9804854989051819
Step: 4130, train/loss: 0.0
Step: 4130, train/grad_norm: 9.959474027709803e-07
Step: 4130, train/learning_rate: 3.36189114023e-05
Step: 4130, train/epoch: 0.9828652739524841
Step: 4140, train/loss: 0.13910000026226044
Step: 4140, train/grad_norm: 4.38845987327241e-10
Step: 4140, train/learning_rate: 3.357924651936628e-05
Step: 4140, train/epoch: 0.9852451086044312
Step: 4150, train/loss: 0.09809999912977219
Step: 4150, train/grad_norm: 6.63751270622015e-05
Step: 4150, train/learning_rate: 3.3539585274411365e-05
Step: 4150, train/epoch: 0.9876249432563782
Step: 4160, train/loss: 0.13130000233650208
Step: 4160, train/grad_norm: 1.059717757989631e-09
Step: 4160, train/learning_rate: 3.3499920391477644e-05
Step: 4160, train/epoch: 0.9900047779083252
Step: 4170, train/loss: 0.30550000071525574
Step: 4170, train/grad_norm: 0.0012815860100090504
Step: 4170, train/learning_rate: 3.3460255508543923e-05
Step: 4170, train/epoch: 0.9923845529556274
Step: 4180, train/loss: 0.7602999806404114
Step: 4180, train/grad_norm: 2.0258219990409998e-08
Step: 4180, train/learning_rate: 3.342059426358901e-05
Step: 4180, train/epoch: 0.9947643876075745
Step: 4190, train/loss: 0.28040000796318054
Step: 4190, train/grad_norm: 0.04654674604535103
Step: 4190, train/learning_rate: 3.338092938065529e-05
Step: 4190, train/epoch: 0.9971442222595215
Step: 4200, train/loss: 0.15119999647140503
Step: 4200, train/grad_norm: 1.1547553539276123
Step: 4200, train/learning_rate: 3.334126449772157e-05
Step: 4200, train/epoch: 0.9995240569114685
Step: 4202, eval/loss: 0.09178511053323746
Step: 4202, eval/accuracy: 0.981674313545227
Step: 4202, eval/f1: 0.9807193875312805
Step: 4202, eval/runtime: 7990.0068359375
Step: 4202, eval/samples_per_second: 0.9020000100135803
Step: 4202, eval/steps_per_second: 0.11299999803304672
Step: 4202, train/epoch: 1.0
Step: 4210, train/loss: 0.6916999816894531
Step: 4210, train/grad_norm: 0.00013312208466231823
Step: 4210, train/learning_rate: 3.3301603252766654e-05
Step: 4210, train/epoch: 1.0019038915634155
Step: 4220, train/loss: 0.0017000000225380063
Step: 4220, train/grad_norm: 1.0717847544583492e-05
Step: 4220, train/learning_rate: 3.326193836983293e-05
Step: 4220, train/epoch: 1.0042836666107178
Step: 4230, train/loss: 0.0
Step: 4230, train/grad_norm: 0.0060021900571882725
Step: 4230, train/learning_rate: 3.322227348689921e-05
Step: 4230, train/epoch: 1.00666344165802
Step: 4240, train/loss: 0.0003000000142492354
Step: 4240, train/grad_norm: 3.7011297990829917e-06
Step: 4240, train/learning_rate: 3.31826122419443e-05
Step: 4240, train/epoch: 1.0090433359146118
Step: 4250, train/loss: 0.0
Step: 4250, train/grad_norm: 7.907885901659029e-07
Step: 4250, train/learning_rate: 3.314294735901058e-05
Step: 4250, train/epoch: 1.011423110961914
Step: 4260, train/loss: 0.0
Step: 4260, train/grad_norm: 6.714225619930403e-09
Step: 4260, train/learning_rate: 3.3103282476076856e-05
Step: 4260, train/epoch: 1.0138030052185059
Step: 4270, train/loss: 0.20630000531673431
Step: 4270, train/grad_norm: 3.5274399579066085e-06
Step: 4270, train/learning_rate: 3.306362123112194e-05
Step: 4270, train/epoch: 1.016182780265808
Step: 4280, train/loss: 0.07270000129938126
Step: 4280, train/grad_norm: 1.091521426133113e-06
Step: 4280, train/learning_rate: 3.302395634818822e-05
Step: 4280, train/epoch: 1.0185625553131104
Step: 4290, train/loss: 0.0
Step: 4290, train/grad_norm: 3.8593892526250784e-08
Step: 4290, train/learning_rate: 3.29842914652545e-05
Step: 4290, train/epoch: 1.0209424495697021
Step: 4300, train/loss: 0.0
Step: 4300, train/grad_norm: 2.0301257563914987e-07
Step: 4300, train/learning_rate: 3.294463022029959e-05
Step: 4300, train/epoch: 1.0233222246170044
Step: 4310, train/loss: 0.0
Step: 4310, train/grad_norm: 5.0301291594223585e-06
Step: 4310, train/learning_rate: 3.2904965337365866e-05
Step: 4310, train/epoch: 1.0257019996643066
Step: 4320, train/loss: 0.07580000162124634
Step: 4320, train/grad_norm: 3.235333451812039e-06
Step: 4320, train/learning_rate: 3.2865300454432145e-05
Step: 4320, train/epoch: 1.0280818939208984
Step: 4330, train/loss: 0.0
Step: 4330, train/grad_norm: 1.4709483366459608e-05
Step: 4330, train/learning_rate: 3.282563920947723e-05
Step: 4330, train/epoch: 1.0304616689682007
Step: 4340, train/loss: 0.28780001401901245
Step: 4340, train/grad_norm: 0.36763954162597656
Step: 4340, train/learning_rate: 3.278597432654351e-05
Step: 4340, train/epoch: 1.0328415632247925
Step: 4350, train/loss: 0.11800000071525574
Step: 4350, train/grad_norm: 5.2062308242284416e-08
Step: 4350, train/learning_rate: 3.2746313081588596e-05
Step: 4350, train/epoch: 1.0352213382720947
Step: 4360, train/loss: 0.0
Step: 4360, train/grad_norm: 8.911271152101108e-07
Step: 4360, train/learning_rate: 3.2706648198654875e-05
Step: 4360, train/epoch: 1.037601113319397
Step: 4370, train/loss: 0.0
Step: 4370, train/grad_norm: 0.012074485421180725
Step: 4370, train/learning_rate: 3.2666983315721154e-05
Step: 4370, train/epoch: 1.0399810075759888
Step: 4380, train/loss: 0.0
Step: 4380, train/grad_norm: 6.665602995781228e-05
Step: 4380, train/learning_rate: 3.262732207076624e-05
Step: 4380, train/epoch: 1.042360782623291
Step: 4390, train/loss: 0.0
Step: 4390, train/grad_norm: 3.451589145697653e-05
Step: 4390, train/learning_rate: 3.258765718783252e-05
Step: 4390, train/epoch: 1.0447405576705933
Step: 4400, train/loss: 0.0
Step: 4400, train/grad_norm: 0.000791738333646208
Step: 4400, train/learning_rate: 3.25479923048988e-05
Step: 4400, train/epoch: 1.047120451927185
Step: 4410, train/loss: 0.0
Step: 4410, train/grad_norm: 4.258257831679657e-06
Step: 4410, train/learning_rate: 3.2508331059943885e-05
Step: 4410, train/epoch: 1.0495002269744873
Step: 4420, train/loss: 0.0
Step: 4420, train/grad_norm: 1.7192417089972878e-07
Step: 4420, train/learning_rate: 3.2468666177010164e-05
Step: 4420, train/epoch: 1.0518800020217896
Step: 4430, train/loss: 0.0
Step: 4430, train/grad_norm: 6.680774049527827e-07
Step: 4430, train/learning_rate: 3.242900129407644e-05
Step: 4430, train/epoch: 1.0542598962783813
Step: 4440, train/loss: 0.0
Step: 4440, train/grad_norm: 5.751868684455985e-06
Step: 4440, train/learning_rate: 3.238934004912153e-05
Step: 4440, train/epoch: 1.0566396713256836
Step: 4450, train/loss: 0.010700000450015068
Step: 4450, train/grad_norm: 1.7507804273009242e-07
Step: 4450, train/learning_rate: 3.234967516618781e-05
Step: 4450, train/epoch: 1.0590195655822754
Step: 4460, train/loss: 0.00019999999494757503
Step: 4460, train/grad_norm: 0.00032582253334112465
Step: 4460, train/learning_rate: 3.231001028325409e-05
Step: 4460, train/epoch: 1.0613993406295776
Step: 4470, train/loss: 9.999999747378752e-05
Step: 4470, train/grad_norm: 0.00013802447938360274
Step: 4470, train/learning_rate: 3.227034903829917e-05
Step: 4470, train/epoch: 1.0637791156768799
Step: 4480, train/loss: 0.0
Step: 4480, train/grad_norm: 8.485993019391458e-10
Step: 4480, train/learning_rate: 3.223068415536545e-05
Step: 4480, train/epoch: 1.0661590099334717
Step: 4490, train/loss: 0.0
Step: 4490, train/grad_norm: 0.0006961869657970965
Step: 4490, train/learning_rate: 3.219101927243173e-05
Step: 4490, train/epoch: 1.068538784980774
Step: 4500, train/loss: 0.00039999998989515007
Step: 4500, train/grad_norm: 7.049311534501612e-08
Step: 4500, train/learning_rate: 3.215135802747682e-05
Step: 4500, train/epoch: 1.0709185600280762
Step: 4510, train/loss: 0.0
Step: 4510, train/grad_norm: 5.7300426625772616e-09
Step: 4510, train/learning_rate: 3.2111693144543096e-05
Step: 4510, train/epoch: 1.073298454284668
Step: 4520, train/loss: 0.0
Step: 4520, train/grad_norm: 2.1298194496921496e-06
Step: 4520, train/learning_rate: 3.2072028261609375e-05
Step: 4520, train/epoch: 1.0756782293319702
Step: 4530, train/loss: 0.19840000569820404
Step: 4530, train/grad_norm: 2.0225174868215845e-09
Step: 4530, train/learning_rate: 3.203236701665446e-05
Step: 4530, train/epoch: 1.078058123588562
Step: 4540, train/loss: 0.0
Step: 4540, train/grad_norm: 8.494424719174276e-09
Step: 4540, train/learning_rate: 3.199270213372074e-05
Step: 4540, train/epoch: 1.0804378986358643
Step: 4550, train/loss: 0.0
Step: 4550, train/grad_norm: 9.193688477182604e-09
Step: 4550, train/learning_rate: 3.195303725078702e-05
Step: 4550, train/epoch: 1.0828176736831665
Step: 4560, train/loss: 0.0
Step: 4560, train/grad_norm: 1.92317628489036e-09
Step: 4560, train/learning_rate: 3.1913376005832106e-05
Step: 4560, train/epoch: 1.0851975679397583
Step: 4570, train/loss: 0.0
Step: 4570, train/grad_norm: 3.688764493858798e-08
Step: 4570, train/learning_rate: 3.1873711122898385e-05
Step: 4570, train/epoch: 1.0875773429870605
Step: 4580, train/loss: 0.0
Step: 4580, train/grad_norm: 1.1165108304567184e-07
Step: 4580, train/learning_rate: 3.1834046239964664e-05
Step: 4580, train/epoch: 1.0899571180343628
Step: 4590, train/loss: 0.0
Step: 4590, train/grad_norm: 5.24319361261405e-08
Step: 4590, train/learning_rate: 3.179438499500975e-05
Step: 4590, train/epoch: 1.0923370122909546
Step: 4600, train/loss: 0.0
Step: 4600, train/grad_norm: 1.1548609180067615e-08
Step: 4600, train/learning_rate: 3.175472011207603e-05
Step: 4600, train/epoch: 1.0947167873382568
Step: 4610, train/loss: 0.0
Step: 4610, train/grad_norm: 6.153587084334333e-10
Step: 4610, train/learning_rate: 3.171505522914231e-05
Step: 4610, train/epoch: 1.097096562385559
Step: 4620, train/loss: 9.999999747378752e-05
Step: 4620, train/grad_norm: 6.889815384880649e-10
Step: 4620, train/learning_rate: 3.1675393984187394e-05
Step: 4620, train/epoch: 1.0994764566421509
Step: 4630, train/loss: 0.0
Step: 4630, train/grad_norm: 7.545494229432848e-10
Step: 4630, train/learning_rate: 3.1635729101253673e-05
Step: 4630, train/epoch: 1.1018562316894531
Step: 4640, train/loss: 0.0
Step: 4640, train/grad_norm: 9.013175761296566e-10
Step: 4640, train/learning_rate: 3.159606421831995e-05
Step: 4640, train/epoch: 1.104236125946045
Step: 4650, train/loss: 9.999999747378752e-05
Step: 4650, train/grad_norm: 2.732052140075325e-09
Step: 4650, train/learning_rate: 3.155640297336504e-05
Step: 4650, train/epoch: 1.1066159009933472
Step: 4660, train/loss: 0.0
Step: 4660, train/grad_norm: 2.860353065514687e-09
Step: 4660, train/learning_rate: 3.151673809043132e-05
Step: 4660, train/epoch: 1.1089956760406494
Step: 4670, train/loss: 0.0
Step: 4670, train/grad_norm: 2.1424129137415093e-09
Step: 4670, train/learning_rate: 3.14770732074976e-05
Step: 4670, train/epoch: 1.1113755702972412
Step: 4680, train/loss: 0.0
Step: 4680, train/grad_norm: 1.400143556651301e-09
Step: 4680, train/learning_rate: 3.143741196254268e-05
Step: 4680, train/epoch: 1.1137553453445435
Step: 4690, train/loss: 0.0
Step: 4690, train/grad_norm: 7.245398023769667e-07
Step: 4690, train/learning_rate: 3.139774707960896e-05
Step: 4690, train/epoch: 1.1161351203918457
Step: 4700, train/loss: 0.05820000171661377
Step: 4700, train/grad_norm: 3.209754018129729e-09
Step: 4700, train/learning_rate: 3.135808219667524e-05
Step: 4700, train/epoch: 1.1185150146484375
Step: 4710, train/loss: 0.0
Step: 4710, train/grad_norm: 5.506830991208744e-09
Step: 4710, train/learning_rate: 3.131842095172033e-05
Step: 4710, train/epoch: 1.1208947896957397
Step: 4720, train/loss: 0.0
Step: 4720, train/grad_norm: 1.1855280313000094e-08
Step: 4720, train/learning_rate: 3.1278756068786606e-05
Step: 4720, train/epoch: 1.1232746839523315
Step: 4730, train/loss: 0.0
Step: 4730, train/grad_norm: 1.1662704224590925e-07
Step: 4730, train/learning_rate: 3.1239091185852885e-05
Step: 4730, train/epoch: 1.1256544589996338
Step: 4740, train/loss: 0.0
Step: 4740, train/grad_norm: 1.7133351093434612e-06
Step: 4740, train/learning_rate: 3.119942994089797e-05
Step: 4740, train/epoch: 1.128034234046936
Step: 4750, train/loss: 0.0
Step: 4750, train/grad_norm: 1.0306334452536703e-08
Step: 4750, train/learning_rate: 3.115976505796425e-05
Step: 4750, train/epoch: 1.1304141283035278
Step: 4760, train/loss: 0.0
Step: 4760, train/grad_norm: 5.6758083476216026e-12
Step: 4760, train/learning_rate: 3.112010017503053e-05
Step: 4760, train/epoch: 1.13279390335083
Step: 4770, train/loss: 0.0
Step: 4770, train/grad_norm: 3.0856843746818186e-09
Step: 4770, train/learning_rate: 3.1080438930075616e-05
Step: 4770, train/epoch: 1.1351736783981323
Step: 4780, train/loss: 0.0
Step: 4780, train/grad_norm: 1.2786044223389581e-08
Step: 4780, train/learning_rate: 3.1040774047141895e-05
Step: 4780, train/epoch: 1.1375535726547241
Step: 4790, train/loss: 0.0
Step: 4790, train/grad_norm: 3.90594401267208e-09
Step: 4790, train/learning_rate: 3.1001109164208174e-05
Step: 4790, train/epoch: 1.1399333477020264
Step: 4800, train/loss: 0.0
Step: 4800, train/grad_norm: 2.2919786069053316e-09
Step: 4800, train/learning_rate: 3.096144791925326e-05
Step: 4800, train/epoch: 1.1423132419586182
Step: 4810, train/loss: 0.0
Step: 4810, train/grad_norm: 3.754863797666985e-08
Step: 4810, train/learning_rate: 3.092178303631954e-05
Step: 4810, train/epoch: 1.1446930170059204
Step: 4820, train/loss: 0.0
Step: 4820, train/grad_norm: 1.2792521708604454e-08
Step: 4820, train/learning_rate: 3.088211815338582e-05
Step: 4820, train/epoch: 1.1470727920532227
Step: 4830, train/loss: 0.0
Step: 4830, train/grad_norm: 4.903812467205171e-09
Step: 4830, train/learning_rate: 3.0842456908430904e-05
Step: 4830, train/epoch: 1.1494526863098145
Step: 4840, train/loss: 0.0
Step: 4840, train/grad_norm: 2.1393699256577747e-08
Step: 4840, train/learning_rate: 3.080279202549718e-05
Step: 4840, train/epoch: 1.1518324613571167
Step: 4850, train/loss: 0.0
Step: 4850, train/grad_norm: 1.1976351022013887e-09
Step: 4850, train/learning_rate: 3.076312714256346e-05
Step: 4850, train/epoch: 1.154212236404419
Step: 4860, train/loss: 0.0
Step: 4860, train/grad_norm: 8.477627488900907e-09
Step: 4860, train/learning_rate: 3.072346589760855e-05
Step: 4860, train/epoch: 1.1565921306610107
Step: 4870, train/loss: 0.0
Step: 4870, train/grad_norm: 4.5150969185669965e-08
Step: 4870, train/learning_rate: 3.068380101467483e-05
Step: 4870, train/epoch: 1.158971905708313
Step: 4880, train/loss: 0.0
Step: 4880, train/grad_norm: 8.798671302656658e-08
Step: 4880, train/learning_rate: 3.0644136131741107e-05
Step: 4880, train/epoch: 1.1613516807556152
Step: 4890, train/loss: 0.025200000032782555
Step: 4890, train/grad_norm: 1.6285317983033565e-08
Step: 4890, train/learning_rate: 3.060447488678619e-05
Step: 4890, train/epoch: 1.163731575012207
Step: 4900, train/loss: 0.0
Step: 4900, train/grad_norm: 8.79338657444606e-10
Step: 4900, train/learning_rate: 3.056481000385247e-05
Step: 4900, train/epoch: 1.1661113500595093
Step: 4910, train/loss: 0.0
Step: 4910, train/grad_norm: 3.920536784107753e-08
Step: 4910, train/learning_rate: 3.052514512091875e-05
Step: 4910, train/epoch: 1.168491244316101
Step: 4920, train/loss: 0.00039999998989515007
Step: 4920, train/grad_norm: 7.831664206037203e-09
Step: 4920, train/learning_rate: 3.0485483875963837e-05
Step: 4920, train/epoch: 1.1708710193634033
Step: 4930, train/loss: 0.0
Step: 4930, train/grad_norm: 6.877955843753725e-11
Step: 4930, train/learning_rate: 3.0445818993030116e-05
Step: 4930, train/epoch: 1.1732507944107056
Step: 4940, train/loss: 0.0
Step: 4940, train/grad_norm: 6.506161720487569e-10
Step: 4940, train/learning_rate: 3.04061559290858e-05
Step: 4940, train/epoch: 1.1756306886672974
Step: 4950, train/loss: 0.0
Step: 4950, train/grad_norm: 1.3491546768662488e-09
Step: 4950, train/learning_rate: 3.036649286514148e-05
Step: 4950, train/epoch: 1.1780104637145996
Step: 4960, train/loss: 0.0
Step: 4960, train/grad_norm: 1.4418968241614039e-09
Step: 4960, train/learning_rate: 3.032682798220776e-05
Step: 4960, train/epoch: 1.1803902387619019
Step: 4970, train/loss: 0.0
Step: 4970, train/grad_norm: 2.782708641291265e-08
Step: 4970, train/learning_rate: 3.0287164918263443e-05
Step: 4970, train/epoch: 1.1827701330184937
Step: 4980, train/loss: 0.0
Step: 4980, train/grad_norm: 1.7492994552892327e-10
Step: 4980, train/learning_rate: 3.0247501854319125e-05
Step: 4980, train/epoch: 1.185149908065796
Step: 4990, train/loss: 0.0
Step: 4990, train/grad_norm: 4.8378510086877213e-08
Step: 4990, train/learning_rate: 3.0207836971385404e-05
Step: 4990, train/epoch: 1.1875298023223877
Step: 5000, train/loss: 0.0
Step: 5000, train/grad_norm: 7.573822458084578e-09
Step: 5000, train/learning_rate: 3.0168173907441087e-05
Step: 5000, train/epoch: 1.18990957736969
Step: 5010, train/loss: 0.0
Step: 5010, train/grad_norm: 0.0001921606162795797
Step: 5010, train/learning_rate: 3.012851084349677e-05
Step: 5010, train/epoch: 1.1922893524169922
Step: 5020, train/loss: 0.0
Step: 5020, train/grad_norm: 1.3338291582343231e-10
Step: 5020, train/learning_rate: 3.008884596056305e-05
Step: 5020, train/epoch: 1.194669246673584
Step: 5030, train/loss: 0.0
Step: 5030, train/grad_norm: 7.710866667398619e-11
Step: 5030, train/learning_rate: 3.004918289661873e-05
Step: 5030, train/epoch: 1.1970490217208862
Step: 5040, train/loss: 0.0
Step: 5040, train/grad_norm: 2.2745576255811528e-10
Step: 5040, train/learning_rate: 3.0009519832674414e-05
Step: 5040, train/epoch: 1.1994287967681885
Step: 5050, train/loss: 0.0
Step: 5050, train/grad_norm: 3.8025542714592575e-09
Step: 5050, train/learning_rate: 2.9969854949740693e-05
Step: 5050, train/epoch: 1.2018086910247803
Step: 5060, train/loss: 0.0
Step: 5060, train/grad_norm: 1.4390791891472077e-09
Step: 5060, train/learning_rate: 2.9930191885796376e-05
Step: 5060, train/epoch: 1.2041884660720825
Step: 5070, train/loss: 0.0
Step: 5070, train/grad_norm: 3.52901791123017e-10
Step: 5070, train/learning_rate: 2.9890528821852058e-05
Step: 5070, train/epoch: 1.2065683603286743
Step: 5080, train/loss: 0.0
Step: 5080, train/grad_norm: 8.187809541482238e-09
Step: 5080, train/learning_rate: 2.9850863938918337e-05
Step: 5080, train/epoch: 1.2089481353759766
Step: 5090, train/loss: 0.0
Step: 5090, train/grad_norm: 4.407106644066516e-06
Step: 5090, train/learning_rate: 2.981120087497402e-05
Step: 5090, train/epoch: 1.2113279104232788
Step: 5100, train/loss: 0.0
Step: 5100, train/grad_norm: 7.205943575172569e-08
Step: 5100, train/learning_rate: 2.9771537811029702e-05
Step: 5100, train/epoch: 1.2137078046798706
Step: 5110, train/loss: 0.0
Step: 5110, train/grad_norm: 2.818679734062357e-10
Step: 5110, train/learning_rate: 2.973187292809598e-05
Step: 5110, train/epoch: 1.2160875797271729
Step: 5120, train/loss: 0.0
Step: 5120, train/grad_norm: 8.743140100797575e-10
Step: 5120, train/learning_rate: 2.9692209864151664e-05
Step: 5120, train/epoch: 1.218467354774475
Step: 5130, train/loss: 0.0
Step: 5130, train/grad_norm: 4.512345608276291e-09
Step: 5130, train/learning_rate: 2.9652546800207347e-05
Step: 5130, train/epoch: 1.220847249031067
Step: 5140, train/loss: 0.0
Step: 5140, train/grad_norm: 1.4441859930158785e-09
Step: 5140, train/learning_rate: 2.9612881917273626e-05
Step: 5140, train/epoch: 1.2232270240783691
Step: 5150, train/loss: 0.0
Step: 5150, train/grad_norm: 3.038399256638513e-07
Step: 5150, train/learning_rate: 2.957321885332931e-05
Step: 5150, train/epoch: 1.2256067991256714
Step: 5160, train/loss: 9.999999747378752e-05
Step: 5160, train/grad_norm: 1.0943774242377913e-07
Step: 5160, train/learning_rate: 2.953355578938499e-05
Step: 5160, train/epoch: 1.2279866933822632
Step: 5170, train/loss: 0.0
Step: 5170, train/grad_norm: 3.034463791351527e-10
Step: 5170, train/learning_rate: 2.949389090645127e-05
Step: 5170, train/epoch: 1.2303664684295654
Step: 5180, train/loss: 0.0
Step: 5180, train/grad_norm: 9.482094970314492e-11
Step: 5180, train/learning_rate: 2.9454227842506953e-05
Step: 5180, train/epoch: 1.2327463626861572
Step: 5190, train/loss: 0.0
Step: 5190, train/grad_norm: 1.6673588065785339e-09
Step: 5190, train/learning_rate: 2.9414564778562635e-05
Step: 5190, train/epoch: 1.2351261377334595
Step: 5200, train/loss: 0.0
Step: 5200, train/grad_norm: 5.387816599977668e-06
Step: 5200, train/learning_rate: 2.9374901714618318e-05
Step: 5200, train/epoch: 1.2375059127807617
Step: 5210, train/loss: 0.0
Step: 5210, train/grad_norm: 2.9584887872857735e-09
Step: 5210, train/learning_rate: 2.9335236831684597e-05
Step: 5210, train/epoch: 1.2398858070373535
Step: 5220, train/loss: 0.0
Step: 5220, train/grad_norm: 7.691957487621082e-11
Step: 5220, train/learning_rate: 2.929557376774028e-05
Step: 5220, train/epoch: 1.2422655820846558
Step: 5230, train/loss: 0.0
Step: 5230, train/grad_norm: 1.6653688428291957e-09
Step: 5230, train/learning_rate: 2.9255910703795962e-05
Step: 5230, train/epoch: 1.244645357131958
Step: 5240, train/loss: 0.01730000041425228
Step: 5240, train/grad_norm: 39.244319915771484
Step: 5240, train/learning_rate: 2.921624582086224e-05
Step: 5240, train/epoch: 1.2470252513885498
Step: 5250, train/loss: 0.0
Step: 5250, train/grad_norm: 3.9665584949943877e-07
Step: 5250, train/learning_rate: 2.9176582756917924e-05
Step: 5250, train/epoch: 1.249405026435852
Step: 5260, train/loss: 0.0
Step: 5260, train/grad_norm: 0.00017782856593839824
Step: 5260, train/learning_rate: 2.9136919692973606e-05
Step: 5260, train/epoch: 1.2517849206924438
Step: 5270, train/loss: 0.025800000876188278
Step: 5270, train/grad_norm: 1.1264112799835857e-05
Step: 5270, train/learning_rate: 2.9097254810039885e-05
Step: 5270, train/epoch: 1.254164695739746
Step: 5280, train/loss: 0.005200000014156103
Step: 5280, train/grad_norm: 1.558488804676017e-08
Step: 5280, train/learning_rate: 2.9057591746095568e-05
Step: 5280, train/epoch: 1.2565444707870483
Step: 5290, train/loss: 0.0
Step: 5290, train/grad_norm: 1.3925500834920113e-13
Step: 5290, train/learning_rate: 2.901792868215125e-05
Step: 5290, train/epoch: 1.2589243650436401
Step: 5300, train/loss: 0.0
Step: 5300, train/grad_norm: 2.8012173743791058e-12
Step: 5300, train/learning_rate: 2.897826379921753e-05
Step: 5300, train/epoch: 1.2613041400909424
Step: 5310, train/loss: 0.0
Step: 5310, train/grad_norm: 7.09449182343877e-14
Step: 5310, train/learning_rate: 2.8938600735273212e-05
Step: 5310, train/epoch: 1.2636839151382446
Step: 5320, train/loss: 0.0
Step: 5320, train/grad_norm: 5.33865750185214e-06
Step: 5320, train/learning_rate: 2.8898937671328895e-05
Step: 5320, train/epoch: 1.2660638093948364
Step: 5330, train/loss: 0.0
Step: 5330, train/grad_norm: 2.318834143796855e-13
Step: 5330, train/learning_rate: 2.8859272788395174e-05
Step: 5330, train/epoch: 1.2684435844421387
Step: 5340, train/loss: 0.0
Step: 5340, train/grad_norm: 2.2967738821932926e-13
Step: 5340, train/learning_rate: 2.8819609724450856e-05
Step: 5340, train/epoch: 1.270823359489441
Step: 5350, train/loss: 0.0
Step: 5350, train/grad_norm: 1.6074221489449647e-13
Step: 5350, train/learning_rate: 2.877994666050654e-05
Step: 5350, train/epoch: 1.2732032537460327
Step: 5360, train/loss: 0.0
Step: 5360, train/grad_norm: 1.1930928467407398e-11
Step: 5360, train/learning_rate: 2.8740281777572818e-05
Step: 5360, train/epoch: 1.275583028793335
Step: 5370, train/loss: 0.0
Step: 5370, train/grad_norm: 2.5069033762932513e-13
Step: 5370, train/learning_rate: 2.87006187136285e-05
Step: 5370, train/epoch: 1.2779629230499268
Step: 5380, train/loss: 0.05000000074505806
Step: 5380, train/grad_norm: 1.6597107802690525e-13
Step: 5380, train/learning_rate: 2.8660955649684183e-05
Step: 5380, train/epoch: 1.280342698097229
Step: 5390, train/loss: 0.22499999403953552
Step: 5390, train/grad_norm: 68.37811279296875
Step: 5390, train/learning_rate: 2.8621290766750462e-05
Step: 5390, train/epoch: 1.2827224731445312
Step: 5400, train/loss: 0.0
Step: 5400, train/grad_norm: 1.911967046908103e-07
Step: 5400, train/learning_rate: 2.8581627702806145e-05
Step: 5400, train/epoch: 1.285102367401123
Step: 5410, train/loss: 0.0003000000142492354
Step: 5410, train/grad_norm: 3.145550264704866e-09
Step: 5410, train/learning_rate: 2.8541964638861828e-05
Step: 5410, train/epoch: 1.2874821424484253
Step: 5420, train/loss: 0.0
Step: 5420, train/grad_norm: 1.6507378797214756e-09
Step: 5420, train/learning_rate: 2.8502299755928107e-05
Step: 5420, train/epoch: 1.2898619174957275
Step: 5430, train/loss: 0.0
Step: 5430, train/grad_norm: 1.5141580211874839e-09
Step: 5430, train/learning_rate: 2.846263669198379e-05
Step: 5430, train/epoch: 1.2922418117523193
Step: 5440, train/loss: 0.22499999403953552
Step: 5440, train/grad_norm: 1.1945266123802867e-05
Step: 5440, train/learning_rate: 2.8422973628039472e-05
Step: 5440, train/epoch: 1.2946215867996216
Step: 5450, train/loss: 0.00019999999494757503
Step: 5450, train/grad_norm: 0.00028172298334538937
Step: 5450, train/learning_rate: 2.838330874510575e-05
Step: 5450, train/epoch: 1.2970014810562134
Step: 5460, train/loss: 0.0
Step: 5460, train/grad_norm: 1.4919373825250659e-05
Step: 5460, train/learning_rate: 2.8343645681161433e-05
Step: 5460, train/epoch: 1.2993812561035156
Step: 5470, train/loss: 0.0
Step: 5470, train/grad_norm: 4.4345816263557936e-07
Step: 5470, train/learning_rate: 2.8303982617217116e-05
Step: 5470, train/epoch: 1.3017610311508179
Step: 5480, train/loss: 0.0
Step: 5480, train/grad_norm: 7.272899892996065e-06
Step: 5480, train/learning_rate: 2.8264317734283395e-05
Step: 5480, train/epoch: 1.3041409254074097
Step: 5490, train/loss: 0.0
Step: 5490, train/grad_norm: 1.12232362425857e-06
Step: 5490, train/learning_rate: 2.8224654670339078e-05
Step: 5490, train/epoch: 1.306520700454712
Step: 5500, train/loss: 0.0
Step: 5500, train/grad_norm: 5.702757022163496e-08
Step: 5500, train/learning_rate: 2.818499160639476e-05
Step: 5500, train/epoch: 1.3089004755020142
Step: 5510, train/loss: 0.0
Step: 5510, train/grad_norm: 4.2145138650084846e-06
Step: 5510, train/learning_rate: 2.814532672346104e-05
Step: 5510, train/epoch: 1.311280369758606
Step: 5520, train/loss: 0.0
Step: 5520, train/grad_norm: 3.338938768138178e-05
Step: 5520, train/learning_rate: 2.8105663659516722e-05
Step: 5520, train/epoch: 1.3136601448059082
Step: 5530, train/loss: 0.0
Step: 5530, train/grad_norm: 2.1917379910973978e-08
Step: 5530, train/learning_rate: 2.8066000595572405e-05
Step: 5530, train/epoch: 1.3160400390625
Step: 5540, train/loss: 0.0
Step: 5540, train/grad_norm: 4.33049353887327e-06
Step: 5540, train/learning_rate: 2.8026337531628087e-05
Step: 5540, train/epoch: 1.3184198141098022
Step: 5550, train/loss: 0.0
Step: 5550, train/grad_norm: 8.231614287979028e-07
Step: 5550, train/learning_rate: 2.7986672648694366e-05
Step: 5550, train/epoch: 1.3207995891571045
Step: 5560, train/loss: 0.0
Step: 5560, train/grad_norm: 3.990463483205531e-06
Step: 5560, train/learning_rate: 2.794700958475005e-05
Step: 5560, train/epoch: 1.3231794834136963
Step: 5570, train/loss: 0.0
Step: 5570, train/grad_norm: 4.0723878669268743e-07
Step: 5570, train/learning_rate: 2.790734652080573e-05
Step: 5570, train/epoch: 1.3255592584609985
Step: 5580, train/loss: 0.0
Step: 5580, train/grad_norm: 3.5924531402997673e-06
Step: 5580, train/learning_rate: 2.786768163787201e-05
Step: 5580, train/epoch: 1.3279390335083008
Step: 5590, train/loss: 0.0
Step: 5590, train/grad_norm: 1.9861452926761558e-08
Step: 5590, train/learning_rate: 2.7828018573927693e-05
Step: 5590, train/epoch: 1.3303189277648926
Step: 5600, train/loss: 0.0
Step: 5600, train/grad_norm: 4.850135155720636e-07
Step: 5600, train/learning_rate: 2.7788355509983376e-05
Step: 5600, train/epoch: 1.3326987028121948
Step: 5610, train/loss: 0.0
Step: 5610, train/grad_norm: 3.494147620131116e-07
Step: 5610, train/learning_rate: 2.7748690627049655e-05
Step: 5610, train/epoch: 1.335078477859497
Step: 5620, train/loss: 0.0
Step: 5620, train/grad_norm: 3.102199457316601e-07
Step: 5620, train/learning_rate: 2.7709027563105337e-05
Step: 5620, train/epoch: 1.3374583721160889
Step: 5630, train/loss: 0.0
Step: 5630, train/grad_norm: 1.0303118642696063e-06
Step: 5630, train/learning_rate: 2.766936449916102e-05
Step: 5630, train/epoch: 1.3398381471633911
Step: 5640, train/loss: 0.0
Step: 5640, train/grad_norm: 2.0936855435138568e-05
Step: 5640, train/learning_rate: 2.76296996162273e-05
Step: 5640, train/epoch: 1.342218041419983
Step: 5650, train/loss: 0.0
Step: 5650, train/grad_norm: 1.678413141803503e-08
Step: 5650, train/learning_rate: 2.759003655228298e-05
Step: 5650, train/epoch: 1.3445978164672852
Step: 5660, train/loss: 0.04859999939799309
Step: 5660, train/grad_norm: 2.7824187327496475e-06
Step: 5660, train/learning_rate: 2.7550373488338664e-05
Step: 5660, train/epoch: 1.3469775915145874
Step: 5670, train/loss: 0.0
Step: 5670, train/grad_norm: 1.4235070011636708e-07
Step: 5670, train/learning_rate: 2.7510708605404943e-05
Step: 5670, train/epoch: 1.3493574857711792
Step: 5680, train/loss: 0.0
Step: 5680, train/grad_norm: 1.634514319448499e-07
Step: 5680, train/learning_rate: 2.7471045541460626e-05
Step: 5680, train/epoch: 1.3517372608184814
Step: 5690, train/loss: 0.0
Step: 5690, train/grad_norm: 2.482573968620727e-08
Step: 5690, train/learning_rate: 2.743138247751631e-05
Step: 5690, train/epoch: 1.3541170358657837
Step: 5700, train/loss: 0.0
Step: 5700, train/grad_norm: 3.5468931969262485e-07
Step: 5700, train/learning_rate: 2.7391717594582587e-05
Step: 5700, train/epoch: 1.3564969301223755
Step: 5710, train/loss: 0.0
Step: 5710, train/grad_norm: 5.060107923782198e-07
Step: 5710, train/learning_rate: 2.735205453063827e-05
Step: 5710, train/epoch: 1.3588767051696777
Step: 5720, train/loss: 0.0
Step: 5720, train/grad_norm: 9.10122980712913e-06
Step: 5720, train/learning_rate: 2.7312391466693953e-05
Step: 5720, train/epoch: 1.3612565994262695
Step: 5730, train/loss: 0.0
Step: 5730, train/grad_norm: 1.0749503331908272e-07
Step: 5730, train/learning_rate: 2.7272726583760232e-05
Step: 5730, train/epoch: 1.3636363744735718
Step: 5740, train/loss: 0.0
Step: 5740, train/grad_norm: 3.7820652778464137e-06
Step: 5740, train/learning_rate: 2.7233063519815914e-05
Step: 5740, train/epoch: 1.366016149520874
Step: 5750, train/loss: 0.0
Step: 5750, train/grad_norm: 1.8909487664586777e-07
Step: 5750, train/learning_rate: 2.7193400455871597e-05
Step: 5750, train/epoch: 1.3683960437774658
Step: 5760, train/loss: 0.0
Step: 5760, train/grad_norm: 1.852089503984189e-08
Step: 5760, train/learning_rate: 2.7153735572937876e-05
Step: 5760, train/epoch: 1.370775818824768
Step: 5770, train/loss: 0.0
Step: 5770, train/grad_norm: 4.824296411243267e-07
Step: 5770, train/learning_rate: 2.711407250899356e-05
Step: 5770, train/epoch: 1.3731555938720703
Step: 5780, train/loss: 0.0
Step: 5780, train/grad_norm: 3.944917352782795e-07
Step: 5780, train/learning_rate: 2.707440944504924e-05
Step: 5780, train/epoch: 1.375535488128662
Step: 5790, train/loss: 0.0
Step: 5790, train/grad_norm: 1.0736293631907756e-07
Step: 5790, train/learning_rate: 2.703474456211552e-05
Step: 5790, train/epoch: 1.3779152631759644
Step: 5800, train/loss: 0.0
Step: 5800, train/grad_norm: 1.358862618872081e-06
Step: 5800, train/learning_rate: 2.6995081498171203e-05
Step: 5800, train/epoch: 1.3802950382232666
Step: 5810, train/loss: 0.0
Step: 5810, train/grad_norm: 1.3308752500051924e-07
Step: 5810, train/learning_rate: 2.6955418434226885e-05
Step: 5810, train/epoch: 1.3826749324798584
Step: 5820, train/loss: 0.0
Step: 5820, train/grad_norm: 1.2032207052925514e-07
Step: 5820, train/learning_rate: 2.6915753551293164e-05
Step: 5820, train/epoch: 1.3850547075271606
Step: 5830, train/loss: 0.11630000174045563
Step: 5830, train/grad_norm: 1.9152428976099145e-08
Step: 5830, train/learning_rate: 2.6876090487348847e-05
Step: 5830, train/epoch: 1.3874346017837524
Step: 5840, train/loss: 0.0
Step: 5840, train/grad_norm: 6.848740952136723e-08
Step: 5840, train/learning_rate: 2.683642742340453e-05
Step: 5840, train/epoch: 1.3898143768310547
Step: 5850, train/loss: 0.0
Step: 5850, train/grad_norm: 3.360124267715037e-08
Step: 5850, train/learning_rate: 2.679676254047081e-05
Step: 5850, train/epoch: 1.392194151878357
Step: 5860, train/loss: 0.0
Step: 5860, train/grad_norm: 0.00018269135034643114
Step: 5860, train/learning_rate: 2.675709947652649e-05
Step: 5860, train/epoch: 1.3945740461349487
Step: 5870, train/loss: 0.04690000042319298
Step: 5870, train/grad_norm: 5.885506197955692e-07
Step: 5870, train/learning_rate: 2.6717436412582174e-05
Step: 5870, train/epoch: 1.396953821182251
Step: 5880, train/loss: 0.11410000175237656
Step: 5880, train/grad_norm: 669.31396484375
Step: 5880, train/learning_rate: 2.6677773348637857e-05
Step: 5880, train/epoch: 1.3993335962295532
Step: 5890, train/loss: 0.0
Step: 5890, train/grad_norm: 1.7324407508567674e-07
Step: 5890, train/learning_rate: 2.6638108465704136e-05
Step: 5890, train/epoch: 1.401713490486145
Step: 5900, train/loss: 0.006099999882280827
Step: 5900, train/grad_norm: 1.2908772362152376e-07
Step: 5900, train/learning_rate: 2.6598445401759818e-05
Step: 5900, train/epoch: 1.4040932655334473
Step: 5910, train/loss: 0.0
Step: 5910, train/grad_norm: 9.438525125915476e-07
Step: 5910, train/learning_rate: 2.65587823378155e-05
Step: 5910, train/epoch: 1.406473159790039
Step: 5920, train/loss: 0.0
Step: 5920, train/grad_norm: 0.0003138610045425594
Step: 5920, train/learning_rate: 2.651911745488178e-05
Step: 5920, train/epoch: 1.4088529348373413
Step: 5930, train/loss: 0.0
Step: 5930, train/grad_norm: 4.465339387138556e-08
Step: 5930, train/learning_rate: 2.6479454390937462e-05
Step: 5930, train/epoch: 1.4112327098846436
Step: 5940, train/loss: 0.04749999940395355
Step: 5940, train/grad_norm: 8.284395391910948e-08
Step: 5940, train/learning_rate: 2.6439791326993145e-05
Step: 5940, train/epoch: 1.4136126041412354
Step: 5950, train/loss: 0.029100000858306885
Step: 5950, train/grad_norm: 1.7353634120809147e-06
Step: 5950, train/learning_rate: 2.6400126444059424e-05
Step: 5950, train/epoch: 1.4159923791885376
Step: 5960, train/loss: 0.00019999999494757503
Step: 5960, train/grad_norm: 0.0023939749225974083
Step: 5960, train/learning_rate: 2.6360463380115107e-05
Step: 5960, train/epoch: 1.4183721542358398
Step: 5970, train/loss: 0.3167000114917755
Step: 5970, train/grad_norm: 4.645646978218565e-09
Step: 5970, train/learning_rate: 2.632080031617079e-05
Step: 5970, train/epoch: 1.4207520484924316
Step: 5980, train/loss: 0.14069999754428864
Step: 5980, train/grad_norm: 0.20524893701076508
Step: 5980, train/learning_rate: 2.628113543323707e-05
Step: 5980, train/epoch: 1.4231318235397339
Step: 5990, train/loss: 0.006399999838322401
Step: 5990, train/grad_norm: 3.940267561119981e-06
Step: 5990, train/learning_rate: 2.624147236929275e-05
Step: 5990, train/epoch: 1.4255117177963257
Step: 6000, train/loss: 0.0
Step: 6000, train/grad_norm: 3.743235943431955e-09
Step: 6000, train/learning_rate: 2.6201809305348434e-05
Step: 6000, train/epoch: 1.427891492843628
Step: 6010, train/loss: 0.0
Step: 6010, train/grad_norm: 2.0526315369640002e-10
Step: 6010, train/learning_rate: 2.6162144422414713e-05
Step: 6010, train/epoch: 1.4302712678909302
Step: 6020, train/loss: 0.0
Step: 6020, train/grad_norm: 2.1240498943031483e-11
Step: 6020, train/learning_rate: 2.6122481358470395e-05
Step: 6020, train/epoch: 1.432651162147522
Step: 6030, train/loss: 0.0
Step: 6030, train/grad_norm: 1.4780053569918294e-10
Step: 6030, train/learning_rate: 2.6082818294526078e-05
Step: 6030, train/epoch: 1.4350309371948242
Step: 6040, train/loss: 0.0
Step: 6040, train/grad_norm: 3.690734176609034e-11
Step: 6040, train/learning_rate: 2.6043153411592357e-05
Step: 6040, train/epoch: 1.4374107122421265
Step: 6050, train/loss: 0.0
Step: 6050, train/grad_norm: 2.350102577819424e-11
Step: 6050, train/learning_rate: 2.600349034764804e-05
Step: 6050, train/epoch: 1.4397906064987183
Step: 6060, train/loss: 0.0
Step: 6060, train/grad_norm: 3.4805980320129493e-10
Step: 6060, train/learning_rate: 2.5963827283703722e-05
Step: 6060, train/epoch: 1.4421703815460205
Step: 6070, train/loss: 0.0
Step: 6070, train/grad_norm: 6.548848130449869e-10
Step: 6070, train/learning_rate: 2.592416240077e-05
Step: 6070, train/epoch: 1.4445501565933228
Step: 6080, train/loss: 0.0
Step: 6080, train/grad_norm: 4.869382119743193e-10
Step: 6080, train/learning_rate: 2.5884499336825684e-05
Step: 6080, train/epoch: 1.4469300508499146
Step: 6090, train/loss: 0.0
Step: 6090, train/grad_norm: 1.8121696365724915e-09
Step: 6090, train/learning_rate: 2.5844836272881366e-05
Step: 6090, train/epoch: 1.4493098258972168
Step: 6100, train/loss: 0.0
Step: 6100, train/grad_norm: 2.2119762377315055e-07
Step: 6100, train/learning_rate: 2.5805171389947645e-05
Step: 6100, train/epoch: 1.4516897201538086
Step: 6110, train/loss: 0.0
Step: 6110, train/grad_norm: 9.684929835884759e-08
Step: 6110, train/learning_rate: 2.5765508326003328e-05
Step: 6110, train/epoch: 1.4540694952011108
Step: 6120, train/loss: 0.0010000000474974513
Step: 6120, train/grad_norm: 4.603568914962608e-10
Step: 6120, train/learning_rate: 2.572584526205901e-05
Step: 6120, train/epoch: 1.456449270248413
Step: 6130, train/loss: 0.0
Step: 6130, train/grad_norm: 1.8804943713313094e-10
Step: 6130, train/learning_rate: 2.568618037912529e-05
Step: 6130, train/epoch: 1.4588291645050049
Step: 6140, train/loss: 0.0
Step: 6140, train/grad_norm: 3.166659295894547e-11
Step: 6140, train/learning_rate: 2.5646517315180972e-05
Step: 6140, train/epoch: 1.4612089395523071
Step: 6150, train/loss: 0.0
Step: 6150, train/grad_norm: 2.1918408776855358e-07
Step: 6150, train/learning_rate: 2.5606854251236655e-05
Step: 6150, train/epoch: 1.4635887145996094
Step: 6160, train/loss: 0.0
Step: 6160, train/grad_norm: 9.606888201618702e-12
Step: 6160, train/learning_rate: 2.5567189368302934e-05
Step: 6160, train/epoch: 1.4659686088562012
Step: 6170, train/loss: 0.0
Step: 6170, train/grad_norm: 6.307872695288097e-07
Step: 6170, train/learning_rate: 2.5527526304358616e-05
Step: 6170, train/epoch: 1.4683483839035034
Step: 6180, train/loss: 0.0
Step: 6180, train/grad_norm: 8.805209006368386e-09
Step: 6180, train/learning_rate: 2.54878632404143e-05
Step: 6180, train/epoch: 1.4707282781600952
Step: 6190, train/loss: 0.0
Step: 6190, train/grad_norm: 7.489393993864724e-08
Step: 6190, train/learning_rate: 2.544820017646998e-05
Step: 6190, train/epoch: 1.4731080532073975
Step: 6200, train/loss: 0.0
Step: 6200, train/grad_norm: 1.9783079341362253e-10
Step: 6200, train/learning_rate: 2.540853529353626e-05
Step: 6200, train/epoch: 1.4754878282546997
Step: 6210, train/loss: 0.0
Step: 6210, train/grad_norm: 9.81361392149438e-09
Step: 6210, train/learning_rate: 2.5368872229591943e-05
Step: 6210, train/epoch: 1.4778677225112915
Step: 6220, train/loss: 0.11559999734163284
Step: 6220, train/grad_norm: 5.450878082235988e-10
Step: 6220, train/learning_rate: 2.5329209165647626e-05
Step: 6220, train/epoch: 1.4802474975585938
Step: 6230, train/loss: 0.0
Step: 6230, train/grad_norm: 6.649523054576278e-13
Step: 6230, train/learning_rate: 2.5289544282713905e-05
Step: 6230, train/epoch: 1.482627272605896
Step: 6240, train/loss: 0.0
Step: 6240, train/grad_norm: 7.550143843459978e-10
Step: 6240, train/learning_rate: 2.5249881218769588e-05
Step: 6240, train/epoch: 1.4850071668624878
Step: 6250, train/loss: 0.0
Step: 6250, train/grad_norm: 4.501347905527808e-11
Step: 6250, train/learning_rate: 2.521021815482527e-05
Step: 6250, train/epoch: 1.48738694190979
Step: 6260, train/loss: 0.0
Step: 6260, train/grad_norm: 7.153750924970836e-10
Step: 6260, train/learning_rate: 2.517055327189155e-05
Step: 6260, train/epoch: 1.4897668361663818
Step: 6270, train/loss: 9.999999747378752e-05
Step: 6270, train/grad_norm: 5.891979681926784e-10
Step: 6270, train/learning_rate: 2.5130890207947232e-05
Step: 6270, train/epoch: 1.492146611213684
Step: 6280, train/loss: 0.0
Step: 6280, train/grad_norm: 3.4539920923393197e-10
Step: 6280, train/learning_rate: 2.5091227144002914e-05
Step: 6280, train/epoch: 1.4945263862609863
Step: 6290, train/loss: 0.0
Step: 6290, train/grad_norm: 1.100387576197237e-11
Step: 6290, train/learning_rate: 2.5051562261069193e-05
Step: 6290, train/epoch: 1.4969062805175781
Step: 6300, train/loss: 0.0
Step: 6300, train/grad_norm: 6.836564647727528e-10
Step: 6300, train/learning_rate: 2.5011899197124876e-05
Step: 6300, train/epoch: 1.4992860555648804
Step: 6310, train/loss: 0.0
Step: 6310, train/grad_norm: 3.222563049742888e-10
Step: 6310, train/learning_rate: 2.497223613318056e-05
Step: 6310, train/epoch: 1.5016658306121826
Step: 6320, train/loss: 0.0
Step: 6320, train/grad_norm: 1.7575765287602962e-09
Step: 6320, train/learning_rate: 2.4932571250246838e-05
Step: 6320, train/epoch: 1.5040457248687744
Step: 6330, train/loss: 0.0
Step: 6330, train/grad_norm: 1.3485063066198677e-09
Step: 6330, train/learning_rate: 2.489290818630252e-05
Step: 6330, train/epoch: 1.5064254999160767
Step: 6340, train/loss: 0.005200000014156103
Step: 6340, train/grad_norm: 7.262701551269402e-10
Step: 6340, train/learning_rate: 2.4853245122358203e-05
Step: 6340, train/epoch: 1.508805274963379
Step: 6350, train/loss: 0.0
Step: 6350, train/grad_norm: 8.238569271235008e-10
Step: 6350, train/learning_rate: 2.4813580239424482e-05
Step: 6350, train/epoch: 1.5111851692199707
Step: 6360, train/loss: 0.27970001101493835
Step: 6360, train/grad_norm: 1.518643699682798e-08
Step: 6360, train/learning_rate: 2.4773917175480165e-05
Step: 6360, train/epoch: 1.513564944267273
Step: 6370, train/loss: 0.24120000004768372
Step: 6370, train/grad_norm: 9.492572949909572e-09
Step: 6370, train/learning_rate: 2.4734254111535847e-05
Step: 6370, train/epoch: 1.5159448385238647
Step: 6380, train/loss: 0.2969000041484833
Step: 6380, train/grad_norm: 9.875122941593872e-10
Step: 6380, train/learning_rate: 2.4694589228602126e-05
Step: 6380, train/epoch: 1.518324613571167
Step: 6390, train/loss: 0.26249998807907104
Step: 6390, train/grad_norm: 4.028849254211764e-09
Step: 6390, train/learning_rate: 2.465492616465781e-05
Step: 6390, train/epoch: 1.5207043886184692
Step: 6400, train/loss: 0.0
Step: 6400, train/grad_norm: 5.962701266071235e-08
Step: 6400, train/learning_rate: 2.461526310071349e-05
Step: 6400, train/epoch: 1.523084282875061
Step: 6410, train/loss: 0.0
Step: 6410, train/grad_norm: 7.794110246095443e-09
Step: 6410, train/learning_rate: 2.457559821777977e-05
Step: 6410, train/epoch: 1.5254640579223633
Step: 6420, train/loss: 0.0
Step: 6420, train/grad_norm: 5.606706765526326e-10
Step: 6420, train/learning_rate: 2.4535935153835453e-05
Step: 6420, train/epoch: 1.5278438329696655
Step: 6430, train/loss: 0.0
Step: 6430, train/grad_norm: 1.0990945931155238e-08
Step: 6430, train/learning_rate: 2.4496272089891136e-05
Step: 6430, train/epoch: 1.5302237272262573
Step: 6440, train/loss: 0.0
Step: 6440, train/grad_norm: 7.967808812736621e-08
Step: 6440, train/learning_rate: 2.4456607206957415e-05
Step: 6440, train/epoch: 1.5326035022735596
Step: 6450, train/loss: 0.0
Step: 6450, train/grad_norm: 1.1855475889888112e-07
Step: 6450, train/learning_rate: 2.4416944143013097e-05
Step: 6450, train/epoch: 1.5349833965301514
Step: 6460, train/loss: 0.0010000000474974513
Step: 6460, train/grad_norm: 6.005286223853545e-08
Step: 6460, train/learning_rate: 2.437728107906878e-05
Step: 6460, train/epoch: 1.5373631715774536
Step: 6470, train/loss: 0.0
Step: 6470, train/grad_norm: 1.9462740965536796e-06
Step: 6470, train/learning_rate: 2.433761619613506e-05
Step: 6470, train/epoch: 1.5397429466247559
Step: 6480, train/loss: 0.0
Step: 6480, train/grad_norm: 1.746255634316185e-06
Step: 6480, train/learning_rate: 2.429795313219074e-05
Step: 6480, train/epoch: 1.5421228408813477
Step: 6490, train/loss: 0.0
Step: 6490, train/grad_norm: 5.101633178128395e-07
Step: 6490, train/learning_rate: 2.4258290068246424e-05
Step: 6490, train/epoch: 1.54450261592865
Step: 6500, train/loss: 0.0
Step: 6500, train/grad_norm: 0.014356807805597782
Step: 6500, train/learning_rate: 2.4218625185312703e-05
Step: 6500, train/epoch: 1.5468823909759521
Step: 6510, train/loss: 0.0
Step: 6510, train/grad_norm: 2.1357786295084225e-07
Step: 6510, train/learning_rate: 2.4178962121368386e-05
Step: 6510, train/epoch: 1.549262285232544
Step: 6520, train/loss: 0.0
Step: 6520, train/grad_norm: 2.615084326862416e-07
Step: 6520, train/learning_rate: 2.413929905742407e-05
Step: 6520, train/epoch: 1.5516420602798462
Step: 6530, train/loss: 0.0
Step: 6530, train/grad_norm: 1.4906868273101281e-07
Step: 6530, train/learning_rate: 2.409963599347975e-05
Step: 6530, train/epoch: 1.5540218353271484
Step: 6540, train/loss: 0.0
Step: 6540, train/grad_norm: 9.211237994577459e-08
Step: 6540, train/learning_rate: 2.405997111054603e-05
Step: 6540, train/epoch: 1.5564017295837402
Step: 6550, train/loss: 0.0
Step: 6550, train/grad_norm: 1.4671795334209037e-08
Step: 6550, train/learning_rate: 2.4020308046601713e-05
Step: 6550, train/epoch: 1.5587815046310425
Step: 6560, train/loss: 0.0
Step: 6560, train/grad_norm: 3.8685858072540213e-08
Step: 6560, train/learning_rate: 2.3980644982657395e-05
Step: 6560, train/epoch: 1.5611613988876343
Step: 6570, train/loss: 0.0
Step: 6570, train/grad_norm: 2.6916842088553494e-08
Step: 6570, train/learning_rate: 2.3940980099723674e-05
Step: 6570, train/epoch: 1.5635411739349365
Step: 6580, train/loss: 0.0
Step: 6580, train/grad_norm: 0.00010208435560343787
Step: 6580, train/learning_rate: 2.3901317035779357e-05
Step: 6580, train/epoch: 1.5659209489822388
Step: 6590, train/loss: 0.0
Step: 6590, train/grad_norm: 3.9363276527204505e-10
Step: 6590, train/learning_rate: 2.386165397183504e-05
Step: 6590, train/epoch: 1.5683008432388306
Step: 6600, train/loss: 0.0006000000284984708
Step: 6600, train/grad_norm: 2.8460272361030547e-08
Step: 6600, train/learning_rate: 2.382198908890132e-05
Step: 6600, train/epoch: 1.5706806182861328
Step: 6610, train/loss: 0.0
Step: 6610, train/grad_norm: 6.104500238635069e-10
Step: 6610, train/learning_rate: 2.3782326024957e-05
Step: 6610, train/epoch: 1.573060393333435
Step: 6620, train/loss: 9.999999747378752e-05
Step: 6620, train/grad_norm: 2.073455572128296
Step: 6620, train/learning_rate: 2.3742662961012684e-05
Step: 6620, train/epoch: 1.5754402875900269
Step: 6630, train/loss: 0.0
Step: 6630, train/grad_norm: 1.0822522966691395e-07
Step: 6630, train/learning_rate: 2.3702998078078963e-05
Step: 6630, train/epoch: 1.577820062637329
Step: 6640, train/loss: 0.0
Step: 6640, train/grad_norm: 4.534673969658343e-09
Step: 6640, train/learning_rate: 2.3663335014134645e-05
Step: 6640, train/epoch: 1.580199956893921
Step: 6650, train/loss: 0.0012000000569969416
Step: 6650, train/grad_norm: 3.2900748792030754e-10
Step: 6650, train/learning_rate: 2.3623671950190328e-05
Step: 6650, train/epoch: 1.5825797319412231
Step: 6660, train/loss: 0.0
Step: 6660, train/grad_norm: 6.9572242864524014e-06
Step: 6660, train/learning_rate: 2.3584007067256607e-05
Step: 6660, train/epoch: 1.5849595069885254
Step: 6670, train/loss: 0.0
Step: 6670, train/grad_norm: 2.846187695249025e-11
Step: 6670, train/learning_rate: 2.354434400331229e-05
Step: 6670, train/epoch: 1.5873394012451172
Step: 6680, train/loss: 0.002899999963119626
Step: 6680, train/grad_norm: 6.972476285227458e-07
Step: 6680, train/learning_rate: 2.3504680939367972e-05
Step: 6680, train/epoch: 1.5897191762924194
Step: 6690, train/loss: 0.0
Step: 6690, train/grad_norm: 8.391038974764342e-10
Step: 6690, train/learning_rate: 2.346501605643425e-05
Step: 6690, train/epoch: 1.5920989513397217
Step: 6700, train/loss: 0.0
Step: 6700, train/grad_norm: 4.778123452453542e-10
Step: 6700, train/learning_rate: 2.3425352992489934e-05
Step: 6700, train/epoch: 1.5944788455963135
Step: 6710, train/loss: 0.0
Step: 6710, train/grad_norm: 6.067455426972401e-09
Step: 6710, train/learning_rate: 2.3385689928545617e-05
Step: 6710, train/epoch: 1.5968586206436157
Step: 6720, train/loss: 0.0
Step: 6720, train/grad_norm: 0.05236198753118515
Step: 6720, train/learning_rate: 2.3346025045611896e-05
Step: 6720, train/epoch: 1.5992385149002075
Step: 6730, train/loss: 0.0
Step: 6730, train/grad_norm: 5.421450510745274e-10
Step: 6730, train/learning_rate: 2.3306361981667578e-05
Step: 6730, train/epoch: 1.6016182899475098
Step: 6740, train/loss: 0.0
Step: 6740, train/grad_norm: 3.0384022409180034e-08
Step: 6740, train/learning_rate: 2.326669891772326e-05
Step: 6740, train/epoch: 1.603998064994812
Step: 6750, train/loss: 0.0
Step: 6750, train/grad_norm: 3.2466749289028485e-09
Step: 6750, train/learning_rate: 2.322703403478954e-05
Step: 6750, train/epoch: 1.6063779592514038
Step: 6760, train/loss: 0.0
Step: 6760, train/grad_norm: 1.039581842832149e-08
Step: 6760, train/learning_rate: 2.3187370970845222e-05
Step: 6760, train/epoch: 1.608757734298706
Step: 6770, train/loss: 0.0
Step: 6770, train/grad_norm: 1.3373133711525043e-09
Step: 6770, train/learning_rate: 2.3147707906900905e-05
Step: 6770, train/epoch: 1.6111375093460083
Step: 6780, train/loss: 0.0
Step: 6780, train/grad_norm: 1.0986740761609326e-07
Step: 6780, train/learning_rate: 2.3108043023967184e-05
Step: 6780, train/epoch: 1.6135174036026
Step: 6790, train/loss: 0.0
Step: 6790, train/grad_norm: 4.0397429126590654e-21
Step: 6790, train/learning_rate: 2.3068379960022867e-05
Step: 6790, train/epoch: 1.6158971786499023
Step: 6800, train/loss: 0.0
Step: 6800, train/grad_norm: 1.0473723000004043e-09
Step: 6800, train/learning_rate: 2.302871689607855e-05
Step: 6800, train/epoch: 1.6182769536972046
Step: 6810, train/loss: 0.0
Step: 6810, train/grad_norm: 2.7433583182912e-10
Step: 6810, train/learning_rate: 2.298905201314483e-05
Step: 6810, train/epoch: 1.6206568479537964
Step: 6820, train/loss: 0.0
Step: 6820, train/grad_norm: 2.4986632207735227e-10
Step: 6820, train/learning_rate: 2.294938894920051e-05
Step: 6820, train/epoch: 1.6230366230010986
Step: 6830, train/loss: 0.00039999998989515007
Step: 6830, train/grad_norm: 2.3646948288380543e-10
Step: 6830, train/learning_rate: 2.2909725885256194e-05
Step: 6830, train/epoch: 1.6254165172576904
Step: 6840, train/loss: 0.0
Step: 6840, train/grad_norm: 1.1080643389504985e-06
Step: 6840, train/learning_rate: 2.2870061002322473e-05
Step: 6840, train/epoch: 1.6277962923049927
Step: 6850, train/loss: 0.0
Step: 6850, train/grad_norm: 8.017923214254097e-09
Step: 6850, train/learning_rate: 2.2830397938378155e-05
Step: 6850, train/epoch: 1.630176067352295
Step: 6860, train/loss: 0.0
Step: 6860, train/grad_norm: 2.9848227001139094e-08
Step: 6860, train/learning_rate: 2.2790734874433838e-05
Step: 6860, train/epoch: 1.6325559616088867
Step: 6870, train/loss: 0.0
Step: 6870, train/grad_norm: 4.132735820228106e-11
Step: 6870, train/learning_rate: 2.275107181048952e-05
Step: 6870, train/epoch: 1.634935736656189
Step: 6880, train/loss: 0.3375000059604645
Step: 6880, train/grad_norm: 8.503621140576456e-10
Step: 6880, train/learning_rate: 2.27114069275558e-05
Step: 6880, train/epoch: 1.6373155117034912
Step: 6890, train/loss: 0.0
Step: 6890, train/grad_norm: 1.405605321025405e-08
Step: 6890, train/learning_rate: 2.2671743863611482e-05
Step: 6890, train/epoch: 1.639695405960083
Step: 6900, train/loss: 0.0
Step: 6900, train/grad_norm: 4.3663952453698585e-08
Step: 6900, train/learning_rate: 2.2632080799667165e-05
Step: 6900, train/epoch: 1.6420751810073853
Step: 6910, train/loss: 0.0
Step: 6910, train/grad_norm: 6.74034994485595e-11
Step: 6910, train/learning_rate: 2.2592415916733444e-05
Step: 6910, train/epoch: 1.644455075263977
Step: 6920, train/loss: 0.0
Step: 6920, train/grad_norm: 6.75327171961726e-08
Step: 6920, train/learning_rate: 2.2552752852789126e-05
Step: 6920, train/epoch: 1.6468348503112793
Step: 6930, train/loss: 0.0
Step: 6930, train/grad_norm: 4.140157670917688e-06
Step: 6930, train/learning_rate: 2.251308978884481e-05
Step: 6930, train/epoch: 1.6492146253585815
Step: 6940, train/loss: 0.0
Step: 6940, train/grad_norm: 7.199441256489081e-07
Step: 6940, train/learning_rate: 2.2473424905911088e-05
Step: 6940, train/epoch: 1.6515945196151733
Step: 6950, train/loss: 0.0
Step: 6950, train/grad_norm: 1.4014820237662207e-07
Step: 6950, train/learning_rate: 2.243376184196677e-05
Step: 6950, train/epoch: 1.6539742946624756
Step: 6960, train/loss: 0.0
Step: 6960, train/grad_norm: 0.002869555726647377
Step: 6960, train/learning_rate: 2.2394098778022453e-05
Step: 6960, train/epoch: 1.6563540697097778
Step: 6970, train/loss: 0.32510000467300415
Step: 6970, train/grad_norm: 3.219844657564863e-08
Step: 6970, train/learning_rate: 2.2354433895088732e-05
Step: 6970, train/epoch: 1.6587339639663696
Step: 6980, train/loss: 0.0
Step: 6980, train/grad_norm: 1.291607265585526e-08
Step: 6980, train/learning_rate: 2.2314770831144415e-05
Step: 6980, train/epoch: 1.6611137390136719
Step: 6990, train/loss: 0.0649000033736229
Step: 6990, train/grad_norm: 3.665135972497069e-09
Step: 6990, train/learning_rate: 2.2275107767200097e-05
Step: 6990, train/epoch: 1.6634936332702637
Step: 7000, train/loss: 0.0
Step: 7000, train/grad_norm: 4.325242830560683e-09
Step: 7000, train/learning_rate: 2.2235442884266376e-05
Step: 7000, train/epoch: 1.665873408317566
Step: 7010, train/loss: 0.0
Step: 7010, train/grad_norm: 2.0174729442601347e-08
Step: 7010, train/learning_rate: 2.219577982032206e-05
Step: 7010, train/epoch: 1.6682531833648682
Step: 7020, train/loss: 0.0
Step: 7020, train/grad_norm: 6.004951558225002e-08
Step: 7020, train/learning_rate: 2.2156116756377742e-05
Step: 7020, train/epoch: 1.67063307762146
Step: 7030, train/loss: 0.0
Step: 7030, train/grad_norm: 4.243079665400273e-09
Step: 7030, train/learning_rate: 2.211645187344402e-05
Step: 7030, train/epoch: 1.6730128526687622
Step: 7040, train/loss: 0.0
Step: 7040, train/grad_norm: 3.0933162520341284e-07
Step: 7040, train/learning_rate: 2.2076788809499703e-05
Step: 7040, train/epoch: 1.6753926277160645
Step: 7050, train/loss: 0.0
Step: 7050, train/grad_norm: 4.733279865831719e-07
Step: 7050, train/learning_rate: 2.2037125745555386e-05
Step: 7050, train/epoch: 1.6777725219726562
Step: 7060, train/loss: 0.0
Step: 7060, train/grad_norm: 1.7546610830976306e-09
Step: 7060, train/learning_rate: 2.1997460862621665e-05
Step: 7060, train/epoch: 1.6801522970199585
Step: 7070, train/loss: 0.0
Step: 7070, train/grad_norm: 1.7905741334089953e-09
Step: 7070, train/learning_rate: 2.1957797798677348e-05
Step: 7070, train/epoch: 1.6825320720672607
Step: 7080, train/loss: 0.0003000000142492354
Step: 7080, train/grad_norm: 1.8445733829253186e-08
Step: 7080, train/learning_rate: 2.191813473473303e-05
Step: 7080, train/epoch: 1.6849119663238525
Step: 7090, train/loss: 0.0
Step: 7090, train/grad_norm: 2.152497984297952e-07
Step: 7090, train/learning_rate: 2.187846985179931e-05
Step: 7090, train/epoch: 1.6872917413711548
Step: 7100, train/loss: 0.0
Step: 7100, train/grad_norm: 1.4768598788350573e-08
Step: 7100, train/learning_rate: 2.1838806787854992e-05
Step: 7100, train/epoch: 1.6896716356277466
Step: 7110, train/loss: 0.0
Step: 7110, train/grad_norm: 1.7945117178896908e-06
Step: 7110, train/learning_rate: 2.1799143723910674e-05
Step: 7110, train/epoch: 1.6920514106750488
Step: 7120, train/loss: 0.0
Step: 7120, train/grad_norm: 7.256356866491842e-07
Step: 7120, train/learning_rate: 2.1759478840976954e-05
Step: 7120, train/epoch: 1.694431185722351
Step: 7130, train/loss: 0.0
Step: 7130, train/grad_norm: 5.625403787234973e-07
Step: 7130, train/learning_rate: 2.1719815777032636e-05
Step: 7130, train/epoch: 1.6968110799789429
Step: 7140, train/loss: 0.0
Step: 7140, train/grad_norm: 1.211247217725031e-05
Step: 7140, train/learning_rate: 2.168015271308832e-05
Step: 7140, train/epoch: 1.6991908550262451
Step: 7150, train/loss: 0.0
Step: 7150, train/grad_norm: 9.83937661658274e-06
Step: 7150, train/learning_rate: 2.1640487830154598e-05
Step: 7150, train/epoch: 1.7015706300735474
Step: 7160, train/loss: 0.04450000077486038
Step: 7160, train/grad_norm: 52.924598693847656
Step: 7160, train/learning_rate: 2.160082476621028e-05
Step: 7160, train/epoch: 1.7039505243301392
Step: 7170, train/loss: 0.00019999999494757503
Step: 7170, train/grad_norm: 8.606611601180703e-08
Step: 7170, train/learning_rate: 2.1561161702265963e-05
Step: 7170, train/epoch: 1.7063302993774414
Step: 7180, train/loss: 0.0
Step: 7180, train/grad_norm: 1.1717335990368838e-09
Step: 7180, train/learning_rate: 2.1521496819332242e-05
Step: 7180, train/epoch: 1.7087101936340332
Step: 7190, train/loss: 0.0
Step: 7190, train/grad_norm: 0.00016992371820379049
Step: 7190, train/learning_rate: 2.1481833755387925e-05
Step: 7190, train/epoch: 1.7110899686813354
Step: 7200, train/loss: 0.0
Step: 7200, train/grad_norm: 1.7394978613083367e-07
Step: 7200, train/learning_rate: 2.1442170691443607e-05
Step: 7200, train/epoch: 1.7134697437286377
Step: 7210, train/loss: 0.0
Step: 7210, train/grad_norm: 1.8433497643854935e-06
Step: 7210, train/learning_rate: 2.140250762749929e-05
Step: 7210, train/epoch: 1.7158496379852295
Step: 7220, train/loss: 0.0
Step: 7220, train/grad_norm: 4.3020378370783874e-08
Step: 7220, train/learning_rate: 2.136284274456557e-05
Step: 7220, train/epoch: 1.7182294130325317
Step: 7230, train/loss: 0.0
Step: 7230, train/grad_norm: 3.389773439721466e-08
Step: 7230, train/learning_rate: 2.132317968062125e-05
Step: 7230, train/epoch: 1.720609188079834
Step: 7240, train/loss: 0.0
Step: 7240, train/grad_norm: 5.842885570928047e-07
Step: 7240, train/learning_rate: 2.1283516616676934e-05
Step: 7240, train/epoch: 1.7229890823364258
Step: 7250, train/loss: 0.30160000920295715
Step: 7250, train/grad_norm: 6.3560849916655116e-09
Step: 7250, train/learning_rate: 2.1243851733743213e-05
Step: 7250, train/epoch: 1.725368857383728
Step: 7260, train/loss: 0.0
Step: 7260, train/grad_norm: 8.171647891686007e-08
Step: 7260, train/learning_rate: 2.1204188669798896e-05
Step: 7260, train/epoch: 1.7277486324310303
Step: 7270, train/loss: 0.0
Step: 7270, train/grad_norm: 4.655681095755426e-06
Step: 7270, train/learning_rate: 2.116452560585458e-05
Step: 7270, train/epoch: 1.730128526687622
Step: 7280, train/loss: 0.07660000026226044
Step: 7280, train/grad_norm: 181.140869140625
Step: 7280, train/learning_rate: 2.1124860722920857e-05
Step: 7280, train/epoch: 1.7325083017349243
Step: 7290, train/loss: 0.0
Step: 7290, train/grad_norm: 1.935525943963512e-07
Step: 7290, train/learning_rate: 2.108519765897654e-05
Step: 7290, train/epoch: 1.7348881959915161
Step: 7300, train/loss: 0.0
Step: 7300, train/grad_norm: 4.072388946951833e-06
Step: 7300, train/learning_rate: 2.1045534595032223e-05
Step: 7300, train/epoch: 1.7372679710388184
Step: 7310, train/loss: 9.999999747378752e-05
Step: 7310, train/grad_norm: 0.00017708977975416929
Step: 7310, train/learning_rate: 2.10058697120985e-05
Step: 7310, train/epoch: 1.7396477460861206
Step: 7320, train/loss: 0.07180000096559525
Step: 7320, train/grad_norm: 1.0391189420033697e-07
Step: 7320, train/learning_rate: 2.0966206648154184e-05
Step: 7320, train/epoch: 1.7420276403427124
Step: 7330, train/loss: 0.0
Step: 7330, train/grad_norm: 3.756614887606702e-06
Step: 7330, train/learning_rate: 2.0926543584209867e-05
Step: 7330, train/epoch: 1.7444074153900146
Step: 7340, train/loss: 0.0
Step: 7340, train/grad_norm: 3.078534973610658e-07
Step: 7340, train/learning_rate: 2.0886878701276146e-05
Step: 7340, train/epoch: 1.746787190437317
Step: 7350, train/loss: 0.0
Step: 7350, train/grad_norm: 8.167371845502203e-08
Step: 7350, train/learning_rate: 2.084721563733183e-05
Step: 7350, train/epoch: 1.7491670846939087
Step: 7360, train/loss: 0.0
Step: 7360, train/grad_norm: 1.7213149476447143e-05
Step: 7360, train/learning_rate: 2.080755257338751e-05
Step: 7360, train/epoch: 1.751546859741211
Step: 7370, train/loss: 0.0
Step: 7370, train/grad_norm: 3.352625981278834e-06
Step: 7370, train/learning_rate: 2.076788769045379e-05
Step: 7370, train/epoch: 1.7539267539978027
Step: 7380, train/loss: 0.0
Step: 7380, train/grad_norm: 7.4959143603337e-06
Step: 7380, train/learning_rate: 2.0728224626509473e-05
Step: 7380, train/epoch: 1.756306529045105
Step: 7390, train/loss: 0.0
Step: 7390, train/grad_norm: 1.5567348299327932e-08
Step: 7390, train/learning_rate: 2.0688561562565155e-05
Step: 7390, train/epoch: 1.7586863040924072
Step: 7400, train/loss: 0.0
Step: 7400, train/grad_norm: 1.2854193300881889e-05
Step: 7400, train/learning_rate: 2.0648896679631434e-05
Step: 7400, train/epoch: 1.761066198348999
Step: 7410, train/loss: 0.0
Step: 7410, train/grad_norm: 1.3471391866914928e-05
Step: 7410, train/learning_rate: 2.0609233615687117e-05
Step: 7410, train/epoch: 1.7634459733963013
Step: 7420, train/loss: 0.00019999999494757503
Step: 7420, train/grad_norm: 6.811960702179931e-07
Step: 7420, train/learning_rate: 2.05695705517428e-05
Step: 7420, train/epoch: 1.7658257484436035
Step: 7430, train/loss: 0.0
Step: 7430, train/grad_norm: 2.725026752159465e-05
Step: 7430, train/learning_rate: 2.052990566880908e-05
Step: 7430, train/epoch: 1.7682056427001953
Step: 7440, train/loss: 0.0
Step: 7440, train/grad_norm: 3.473887772997841e-07
Step: 7440, train/learning_rate: 2.049024260486476e-05
Step: 7440, train/epoch: 1.7705854177474976
Step: 7450, train/loss: 0.0
Step: 7450, train/grad_norm: 0.0015676137991249561
Step: 7450, train/learning_rate: 2.0450579540920444e-05
Step: 7450, train/epoch: 1.7729653120040894
Step: 7460, train/loss: 0.0
Step: 7460, train/grad_norm: 6.008293951254018e-08
Step: 7460, train/learning_rate: 2.0410914657986723e-05
Step: 7460, train/epoch: 1.7753450870513916
Step: 7470, train/loss: 0.0
Step: 7470, train/grad_norm: 5.786895144410664e-06
Step: 7470, train/learning_rate: 2.0371251594042405e-05
Step: 7470, train/epoch: 1.7777248620986938
Step: 7480, train/loss: 0.0
Step: 7480, train/grad_norm: 1.078568402590463e-06
Step: 7480, train/learning_rate: 2.0331588530098088e-05
Step: 7480, train/epoch: 1.7801047563552856
Step: 7490, train/loss: 0.0
Step: 7490, train/grad_norm: 7.384405034827068e-05
Step: 7490, train/learning_rate: 2.0291923647164367e-05
Step: 7490, train/epoch: 1.782484531402588
Step: 7500, train/loss: 0.0
Step: 7500, train/grad_norm: 1.3174483228794998e-06
Step: 7500, train/learning_rate: 2.025226058322005e-05
Step: 7500, train/epoch: 1.7848643064498901
Step: 7510, train/loss: 0.0828000009059906
Step: 7510, train/grad_norm: 41.263816833496094
Step: 7510, train/learning_rate: 2.0212597519275732e-05
Step: 7510, train/epoch: 1.787244200706482
Step: 7520, train/loss: 0.0
Step: 7520, train/grad_norm: 7.148631766540348e-07
Step: 7520, train/learning_rate: 2.017293263634201e-05
Step: 7520, train/epoch: 1.7896239757537842
Step: 7530, train/loss: 0.0
Step: 7530, train/grad_norm: 8.653964869154152e-06
Step: 7530, train/learning_rate: 2.0133269572397694e-05
Step: 7530, train/epoch: 1.7920037508010864
Step: 7540, train/loss: 0.0
Step: 7540, train/grad_norm: 2.0503641007252327e-09
Step: 7540, train/learning_rate: 2.0093606508453377e-05
Step: 7540, train/epoch: 1.7943836450576782
Step: 7550, train/loss: 0.08160000294446945
Step: 7550, train/grad_norm: 5.621073341899319e-06
Step: 7550, train/learning_rate: 2.005394344450906e-05
Step: 7550, train/epoch: 1.7967634201049805
Step: 7560, train/loss: 0.0
Step: 7560, train/grad_norm: 9.653972732337479e-09
Step: 7560, train/learning_rate: 2.0014278561575338e-05
Step: 7560, train/epoch: 1.7991433143615723
Step: 7570, train/loss: 0.0
Step: 7570, train/grad_norm: 4.2455300786059524e-07
Step: 7570, train/learning_rate: 1.997461549763102e-05
Step: 7570, train/epoch: 1.8015230894088745
Step: 7580, train/loss: 9.999999747378752e-05
Step: 7580, train/grad_norm: 0.1402873992919922
Step: 7580, train/learning_rate: 1.9934952433686703e-05
Step: 7580, train/epoch: 1.8039028644561768
Step: 7590, train/loss: 0.0
Step: 7590, train/grad_norm: 5.301940745994216e-06
Step: 7590, train/learning_rate: 1.9895287550752982e-05
Step: 7590, train/epoch: 1.8062827587127686
Step: 7600, train/loss: 0.012500000186264515
Step: 7600, train/grad_norm: 4.1092935276765274e-08
Step: 7600, train/learning_rate: 1.9855624486808665e-05
Step: 7600, train/epoch: 1.8086625337600708
Step: 7610, train/loss: 0.0
Step: 7610, train/grad_norm: 1.6636074917641963e-07
Step: 7610, train/learning_rate: 1.9815961422864348e-05
Step: 7610, train/epoch: 1.811042308807373
Step: 7620, train/loss: 0.0
Step: 7620, train/grad_norm: 3.696059991398215e-08
Step: 7620, train/learning_rate: 1.9776296539930627e-05
Step: 7620, train/epoch: 1.8134222030639648
Step: 7630, train/loss: 0.0005000000237487257
Step: 7630, train/grad_norm: 1.9441602105985112e-08
Step: 7630, train/learning_rate: 1.973663347598631e-05
Step: 7630, train/epoch: 1.815801978111267
Step: 7640, train/loss: 0.12189999967813492
Step: 7640, train/grad_norm: 1.6722637496968673e-08
Step: 7640, train/learning_rate: 1.9696970412041992e-05
Step: 7640, train/epoch: 1.8181818723678589
Step: 7650, train/loss: 0.0010000000474974513
Step: 7650, train/grad_norm: 578.452392578125
Step: 7650, train/learning_rate: 1.965730552910827e-05
Step: 7650, train/epoch: 1.8205616474151611
Step: 7660, train/loss: 0.0
Step: 7660, train/grad_norm: 8.26150608190801e-06
Step: 7660, train/learning_rate: 1.9617642465163954e-05
Step: 7660, train/epoch: 1.8229414224624634
Step: 7670, train/loss: 0.0
Step: 7670, train/grad_norm: 9.411572632345155e-10
Step: 7670, train/learning_rate: 1.9577979401219636e-05
Step: 7670, train/epoch: 1.8253213167190552
Step: 7680, train/loss: 0.3702999949455261
Step: 7680, train/grad_norm: 39.89574432373047
Step: 7680, train/learning_rate: 1.9538314518285915e-05
Step: 7680, train/epoch: 1.8277010917663574
Step: 7690, train/loss: 0.0
Step: 7690, train/grad_norm: 0.0001775791752152145
Step: 7690, train/learning_rate: 1.9498651454341598e-05
Step: 7690, train/epoch: 1.8300808668136597
Step: 7700, train/loss: 0.11169999837875366
Step: 7700, train/grad_norm: 0.018474606797099113
Step: 7700, train/learning_rate: 1.945898839039728e-05
Step: 7700, train/epoch: 1.8324607610702515
Step: 7710, train/loss: 9.999999747378752e-05
Step: 7710, train/grad_norm: 0.11032405495643616
Step: 7710, train/learning_rate: 1.941932350746356e-05
Step: 7710, train/epoch: 1.8348405361175537
Step: 7720, train/loss: 0.0
Step: 7720, train/grad_norm: 2.3689739464316517e-05
Step: 7720, train/learning_rate: 1.9379660443519242e-05
Step: 7720, train/epoch: 1.8372204303741455
Step: 7730, train/loss: 0.0
Step: 7730, train/grad_norm: 4.230709600960836e-05
Step: 7730, train/learning_rate: 1.9339997379574925e-05
Step: 7730, train/epoch: 1.8396002054214478
Step: 7740, train/loss: 0.0
Step: 7740, train/grad_norm: 0.0003731542674358934
Step: 7740, train/learning_rate: 1.9300332496641204e-05
Step: 7740, train/epoch: 1.84197998046875
Step: 7750, train/loss: 0.0
Step: 7750, train/grad_norm: 3.504149935906753e-05
Step: 7750, train/learning_rate: 1.9260669432696886e-05
Step: 7750, train/epoch: 1.8443598747253418
Step: 7760, train/loss: 0.0
Step: 7760, train/grad_norm: 9.024206519825384e-05
Step: 7760, train/learning_rate: 1.922100636875257e-05
Step: 7760, train/epoch: 1.846739649772644
Step: 7770, train/loss: 0.0
Step: 7770, train/grad_norm: 6.74737457302399e-05
Step: 7770, train/learning_rate: 1.9181341485818848e-05
Step: 7770, train/epoch: 1.8491194248199463
Step: 7780, train/loss: 0.0
Step: 7780, train/grad_norm: 8.645517482364085e-06
Step: 7780, train/learning_rate: 1.914167842187453e-05
Step: 7780, train/epoch: 1.851499319076538
Step: 7790, train/loss: 0.0
Step: 7790, train/grad_norm: 3.0122295356704853e-05
Step: 7790, train/learning_rate: 1.9102015357930213e-05
Step: 7790, train/epoch: 1.8538790941238403
Step: 7800, train/loss: 0.0
Step: 7800, train/grad_norm: 7.596353498229291e-06
Step: 7800, train/learning_rate: 1.9062350474996492e-05
Step: 7800, train/epoch: 1.8562588691711426
Step: 7810, train/loss: 0.0
Step: 7810, train/grad_norm: 0.0001566413848195225
Step: 7810, train/learning_rate: 1.9022687411052175e-05
Step: 7810, train/epoch: 1.8586387634277344
Step: 7820, train/loss: 0.014800000004470348
Step: 7820, train/grad_norm: 2.9778235330013558e-05
Step: 7820, train/learning_rate: 1.8983024347107857e-05
Step: 7820, train/epoch: 1.8610185384750366
Step: 7830, train/loss: 0.0
Step: 7830, train/grad_norm: 5.906189812776574e-07
Step: 7830, train/learning_rate: 1.8943359464174137e-05
Step: 7830, train/epoch: 1.8633984327316284
Step: 7840, train/loss: 0.0
Step: 7840, train/grad_norm: 1.2313580555201042e-07
Step: 7840, train/learning_rate: 1.890369640022982e-05
Step: 7840, train/epoch: 1.8657782077789307
Step: 7850, train/loss: 0.0
Step: 7850, train/grad_norm: 1.6795840451777622e-07
Step: 7850, train/learning_rate: 1.8864033336285502e-05
Step: 7850, train/epoch: 1.868157982826233
Step: 7860, train/loss: 0.003700000001117587
Step: 7860, train/grad_norm: 7.086757847218905e-08
Step: 7860, train/learning_rate: 1.882436845335178e-05
Step: 7860, train/epoch: 1.8705378770828247
Step: 7870, train/loss: 0.0
Step: 7870, train/grad_norm: 2.4219548322435003e-06
Step: 7870, train/learning_rate: 1.8784705389407463e-05
Step: 7870, train/epoch: 1.872917652130127
Step: 7880, train/loss: 0.0
Step: 7880, train/grad_norm: 3.3790868201322155e-06
Step: 7880, train/learning_rate: 1.8745042325463146e-05
Step: 7880, train/epoch: 1.8752974271774292
Step: 7890, train/loss: 0.0
Step: 7890, train/grad_norm: 4.62407751911087e-06
Step: 7890, train/learning_rate: 1.870537926151883e-05
Step: 7890, train/epoch: 1.877677321434021
Step: 7900, train/loss: 0.0
Step: 7900, train/grad_norm: 3.752388408884144e-07
Step: 7900, train/learning_rate: 1.8665714378585108e-05
Step: 7900, train/epoch: 1.8800570964813232
Step: 7910, train/loss: 0.0
Step: 7910, train/grad_norm: 1.8199569922217051e-06
Step: 7910, train/learning_rate: 1.862605131464079e-05
Step: 7910, train/epoch: 1.882436990737915
Step: 7920, train/loss: 0.0
Step: 7920, train/grad_norm: 4.4791181608161423e-07
Step: 7920, train/learning_rate: 1.8586388250696473e-05
Step: 7920, train/epoch: 1.8848167657852173
Step: 7930, train/loss: 0.06480000168085098
Step: 7930, train/grad_norm: 3.525098009049543e-06
Step: 7930, train/learning_rate: 1.8546723367762752e-05
Step: 7930, train/epoch: 1.8871965408325195
Step: 7940, train/loss: 0.0
Step: 7940, train/grad_norm: 1.664148356894657e-07
Step: 7940, train/learning_rate: 1.8507060303818434e-05
Step: 7940, train/epoch: 1.8895764350891113
Step: 7950, train/loss: 0.00019999999494757503
Step: 7950, train/grad_norm: 9.393105574417859e-05
Step: 7950, train/learning_rate: 1.8467397239874117e-05
Step: 7950, train/epoch: 1.8919562101364136
Step: 7960, train/loss: 0.0
Step: 7960, train/grad_norm: 4.211549094179645e-05
Step: 7960, train/learning_rate: 1.8427732356940396e-05
Step: 7960, train/epoch: 1.8943359851837158
Step: 7970, train/loss: 0.0
Step: 7970, train/grad_norm: 4.4759177399100736e-05
Step: 7970, train/learning_rate: 1.838806929299608e-05
Step: 7970, train/epoch: 1.8967158794403076
Step: 7980, train/loss: 0.0
Step: 7980, train/grad_norm: 0.0034719433169811964
Step: 7980, train/learning_rate: 1.834840622905176e-05
Step: 7980, train/epoch: 1.8990956544876099
Step: 7990, train/loss: 0.0
Step: 7990, train/grad_norm: 0.00025418042787350714
Step: 7990, train/learning_rate: 1.830874134611804e-05
Step: 7990, train/epoch: 1.901475429534912
Step: 8000, train/loss: 0.0
Step: 8000, train/grad_norm: 0.0014713361160829663
Step: 8000, train/learning_rate: 1.8269078282173723e-05
Step: 8000, train/epoch: 1.903855323791504
Step: 8010, train/loss: 0.0
Step: 8010, train/grad_norm: 5.606191734841559e-06
Step: 8010, train/learning_rate: 1.8229415218229406e-05
Step: 8010, train/epoch: 1.9062350988388062
Step: 8020, train/loss: 0.0
Step: 8020, train/grad_norm: 9.725241625346825e-07
Step: 8020, train/learning_rate: 1.8189750335295685e-05
Step: 8020, train/epoch: 1.908614993095398
Step: 8030, train/loss: 0.0
Step: 8030, train/grad_norm: 1.6053874787758105e-05
Step: 8030, train/learning_rate: 1.8150087271351367e-05
Step: 8030, train/epoch: 1.9109947681427002
Step: 8040, train/loss: 0.0
Step: 8040, train/grad_norm: 0.00038450834108516574
Step: 8040, train/learning_rate: 1.811042420740705e-05
Step: 8040, train/epoch: 1.9133745431900024
Step: 8050, train/loss: 0.0
Step: 8050, train/grad_norm: 0.0001398155145579949
Step: 8050, train/learning_rate: 1.807075932447333e-05
Step: 8050, train/epoch: 1.9157544374465942
Step: 8060, train/loss: 0.0
Step: 8060, train/grad_norm: 3.58516481355764e-05
Step: 8060, train/learning_rate: 1.803109626052901e-05
Step: 8060, train/epoch: 1.9181342124938965
Step: 8070, train/loss: 0.0
Step: 8070, train/grad_norm: 8.500258263666183e-05
Step: 8070, train/learning_rate: 1.7991433196584694e-05
Step: 8070, train/epoch: 1.9205139875411987
Step: 8080, train/loss: 0.0
Step: 8080, train/grad_norm: 2.5859892048174515e-05
Step: 8080, train/learning_rate: 1.7951768313650973e-05
Step: 8080, train/epoch: 1.9228938817977905
Step: 8090, train/loss: 0.0
Step: 8090, train/grad_norm: 2.7038583993999055e-07
Step: 8090, train/learning_rate: 1.7912105249706656e-05
Step: 8090, train/epoch: 1.9252736568450928
Step: 8100, train/loss: 0.0
Step: 8100, train/grad_norm: 3.255211777286604e-05
Step: 8100, train/learning_rate: 1.787244218576234e-05
Step: 8100, train/epoch: 1.9276535511016846
Step: 8110, train/loss: 0.0
Step: 8110, train/grad_norm: 3.2859708881005645e-05
Step: 8110, train/learning_rate: 1.7832777302828617e-05
Step: 8110, train/epoch: 1.9300333261489868
Step: 8120, train/loss: 0.0
Step: 8120, train/grad_norm: 4.6852392188156955e-06
Step: 8120, train/learning_rate: 1.77931142388843e-05
Step: 8120, train/epoch: 1.932413101196289
Step: 8130, train/loss: 0.0
Step: 8130, train/grad_norm: 6.45454420009628e-05
Step: 8130, train/learning_rate: 1.7753451174939983e-05
Step: 8130, train/epoch: 1.9347929954528809
Step: 8140, train/loss: 0.0
Step: 8140, train/grad_norm: 6.495561137853656e-06
Step: 8140, train/learning_rate: 1.771378629200626e-05
Step: 8140, train/epoch: 1.937172770500183
Step: 8150, train/loss: 0.0
Step: 8150, train/grad_norm: 4.565614290186204e-05
Step: 8150, train/learning_rate: 1.7674123228061944e-05
Step: 8150, train/epoch: 1.9395525455474854
Step: 8160, train/loss: 0.0
Step: 8160, train/grad_norm: 1.4191565242072102e-05
Step: 8160, train/learning_rate: 1.7634460164117627e-05
Step: 8160, train/epoch: 1.9419324398040771
Step: 8170, train/loss: 0.00039999998989515007
Step: 8170, train/grad_norm: 9.260209480999038e-06
Step: 8170, train/learning_rate: 1.7594795281183906e-05
Step: 8170, train/epoch: 1.9443122148513794
Step: 8180, train/loss: 0.0
Step: 8180, train/grad_norm: 1.3151659459254006e-06
Step: 8180, train/learning_rate: 1.755513221723959e-05
Step: 8180, train/epoch: 1.9466921091079712
Step: 8190, train/loss: 0.0
Step: 8190, train/grad_norm: 0.0007004595245234668
Step: 8190, train/learning_rate: 1.751546915329527e-05
Step: 8190, train/epoch: 1.9490718841552734
Step: 8200, train/loss: 0.06480000168085098
Step: 8200, train/grad_norm: 3.927802794123636e-08
Step: 8200, train/learning_rate: 1.747580427036155e-05
Step: 8200, train/epoch: 1.9514516592025757
Step: 8210, train/loss: 0.13130000233650208
Step: 8210, train/grad_norm: 5.744373083871324e-06
Step: 8210, train/learning_rate: 1.7436141206417233e-05
Step: 8210, train/epoch: 1.9538315534591675
Step: 8220, train/loss: 0.0
Step: 8220, train/grad_norm: 0.001212593400850892
Step: 8220, train/learning_rate: 1.7396478142472915e-05
Step: 8220, train/epoch: 1.9562113285064697
Step: 8230, train/loss: 0.0
Step: 8230, train/grad_norm: 3.47889399563428e-05
Step: 8230, train/learning_rate: 1.7356815078528598e-05
Step: 8230, train/epoch: 1.958591103553772
Step: 8240, train/loss: 0.0
Step: 8240, train/grad_norm: 1.0397592632216401e-05
Step: 8240, train/learning_rate: 1.7317150195594877e-05
Step: 8240, train/epoch: 1.9609709978103638
Step: 8250, train/loss: 0.0
Step: 8250, train/grad_norm: 2.3188028990261955e-06
Step: 8250, train/learning_rate: 1.727748713165056e-05
Step: 8250, train/epoch: 1.963350772857666
Step: 8260, train/loss: 0.0
Step: 8260, train/grad_norm: 8.965178858488798e-05
Step: 8260, train/learning_rate: 1.7237824067706242e-05
Step: 8260, train/epoch: 1.9657305479049683
Step: 8270, train/loss: 0.0
Step: 8270, train/grad_norm: 4.6052181801314873e-07
Step: 8270, train/learning_rate: 1.719815918477252e-05
Step: 8270, train/epoch: 1.96811044216156
Step: 8280, train/loss: 0.0
Step: 8280, train/grad_norm: 2.2209437702258583e-06
Step: 8280, train/learning_rate: 1.7158496120828204e-05
Step: 8280, train/epoch: 1.9704902172088623
Step: 8290, train/loss: 0.0
Step: 8290, train/grad_norm: 2.3009195501799695e-05
Step: 8290, train/learning_rate: 1.7118833056883886e-05
Step: 8290, train/epoch: 1.972870111465454
Step: 8300, train/loss: 0.0
Step: 8300, train/grad_norm: 1.55673873791784e-07
Step: 8300, train/learning_rate: 1.7079168173950166e-05
Step: 8300, train/epoch: 1.9752498865127563
Step: 8310, train/loss: 0.0
Step: 8310, train/grad_norm: 7.624308636877686e-06
Step: 8310, train/learning_rate: 1.7039505110005848e-05
Step: 8310, train/epoch: 1.9776296615600586
Step: 8320, train/loss: 0.0
Step: 8320, train/grad_norm: 3.6779832157662895e-07
Step: 8320, train/learning_rate: 1.699984204606153e-05
Step: 8320, train/epoch: 1.9800095558166504
Step: 8330, train/loss: 0.0
Step: 8330, train/grad_norm: 0.0006903172470629215
Step: 8330, train/learning_rate: 1.696017716312781e-05
Step: 8330, train/epoch: 1.9823893308639526
Step: 8340, train/loss: 0.0
Step: 8340, train/grad_norm: 2.1568587271758588e-07
Step: 8340, train/learning_rate: 1.6920514099183492e-05
Step: 8340, train/epoch: 1.9847691059112549
Step: 8350, train/loss: 0.0
Step: 8350, train/grad_norm: 9.750766594152083e-07
Step: 8350, train/learning_rate: 1.6880851035239175e-05
Step: 8350, train/epoch: 1.9871490001678467
Step: 8360, train/loss: 0.0
Step: 8360, train/grad_norm: 4.1111320570053067e-07
Step: 8360, train/learning_rate: 1.6841186152305454e-05
Step: 8360, train/epoch: 1.989528775215149
Step: 8370, train/loss: 0.0
Step: 8370, train/grad_norm: 8.584042006987147e-06
Step: 8370, train/learning_rate: 1.6801523088361137e-05
Step: 8370, train/epoch: 1.9919086694717407
Step: 8380, train/loss: 0.0
Step: 8380, train/grad_norm: 4.602038461598568e-05
Step: 8380, train/learning_rate: 1.676186002441682e-05
Step: 8380, train/epoch: 1.994288444519043
Step: 8390, train/loss: 0.0
Step: 8390, train/grad_norm: 8.405744011952265e-08
Step: 8390, train/learning_rate: 1.6722195141483098e-05
Step: 8390, train/epoch: 1.9966682195663452
Step: 8400, train/loss: 0.0
Step: 8400, train/grad_norm: 5.726852325693699e-10
Step: 8400, train/learning_rate: 1.668253207753878e-05
Step: 8400, train/epoch: 1.999048113822937
Step: 8404, eval/loss: 0.0038453158922493458
Step: 8404, eval/accuracy: 0.9995834827423096
Step: 8404, eval/f1: 0.9995601177215576
Step: 8404, eval/runtime: 7990.6611328125
Step: 8404, eval/samples_per_second: 0.9010000228881836
Step: 8404, eval/steps_per_second: 0.11299999803304672
Step: 8404, train/epoch: 2.0
Step: 8410, train/loss: 0.0
Step: 8410, train/grad_norm: 6.6468186510348914e-09
Step: 8410, train/learning_rate: 1.6642869013594463e-05
Step: 8410, train/epoch: 2.0014278888702393
Step: 8420, train/loss: 0.0
Step: 8420, train/grad_norm: 5.904029194425675e-07
Step: 8420, train/learning_rate: 1.6603204130660743e-05
Step: 8420, train/epoch: 2.003807783126831
Step: 8430, train/loss: 0.0
Step: 8430, train/grad_norm: 1.6074307040980784e-06
Step: 8430, train/learning_rate: 1.6563541066716425e-05
Step: 8430, train/epoch: 2.0061874389648438
Step: 8440, train/loss: 0.0
Step: 8440, train/grad_norm: 4.800179340236355e-06
Step: 8440, train/learning_rate: 1.6523878002772108e-05
Step: 8440, train/epoch: 2.0085673332214355
Step: 8450, train/loss: 0.0
Step: 8450, train/grad_norm: 2.5588568064449646e-07
Step: 8450, train/learning_rate: 1.6484213119838387e-05
Step: 8450, train/epoch: 2.0109472274780273
Step: 8460, train/loss: 0.0
Step: 8460, train/grad_norm: 6.344654224221813e-08
Step: 8460, train/learning_rate: 1.644455005589407e-05
Step: 8460, train/epoch: 2.01332688331604
Step: 8470, train/loss: 0.0
Step: 8470, train/grad_norm: 1.3105039897709503e-07
Step: 8470, train/learning_rate: 1.6404886991949752e-05
Step: 8470, train/epoch: 2.015706777572632
Step: 8480, train/loss: 0.0
Step: 8480, train/grad_norm: 8.410524969804101e-07
Step: 8480, train/learning_rate: 1.636522210901603e-05
Step: 8480, train/epoch: 2.0180866718292236
Step: 8490, train/loss: 0.0
Step: 8490, train/grad_norm: 8.857202374201734e-06
Step: 8490, train/learning_rate: 1.6325559045071714e-05
Step: 8490, train/epoch: 2.0204663276672363
Step: 8500, train/loss: 0.0
Step: 8500, train/grad_norm: 4.3803694893540523e-08
Step: 8500, train/learning_rate: 1.6285895981127396e-05
Step: 8500, train/epoch: 2.022846221923828
Step: 8510, train/loss: 0.0
Step: 8510, train/grad_norm: 3.324205977150996e-08
Step: 8510, train/learning_rate: 1.6246231098193675e-05
Step: 8510, train/epoch: 2.02522611618042
Step: 8520, train/loss: 0.0
Step: 8520, train/grad_norm: 8.6637349738794e-08
Step: 8520, train/learning_rate: 1.6206568034249358e-05
Step: 8520, train/epoch: 2.0276060104370117
Step: 8530, train/loss: 0.0
Step: 8530, train/grad_norm: 2.3769706558596226e-07
Step: 8530, train/learning_rate: 1.616690497030504e-05
Step: 8530, train/epoch: 2.0299856662750244
Step: 8540, train/loss: 0.0
Step: 8540, train/grad_norm: 3.9168662624433637e-07
Step: 8540, train/learning_rate: 1.612724008737132e-05
Step: 8540, train/epoch: 2.032365560531616
Step: 8550, train/loss: 0.0
Step: 8550, train/grad_norm: 4.6497181216409444e-08
Step: 8550, train/learning_rate: 1.6087577023427002e-05
Step: 8550, train/epoch: 2.034745454788208
Step: 8560, train/loss: 0.0
Step: 8560, train/grad_norm: 1.0728447463748125e-08
Step: 8560, train/learning_rate: 1.6047913959482685e-05
Step: 8560, train/epoch: 2.0371251106262207
Step: 8570, train/loss: 0.0
Step: 8570, train/grad_norm: 1.0333194495615317e-06
Step: 8570, train/learning_rate: 1.6008250895538367e-05
Step: 8570, train/epoch: 2.0395050048828125
Step: 8580, train/loss: 0.0
Step: 8580, train/grad_norm: 6.747817451469018e-07
Step: 8580, train/learning_rate: 1.5968586012604646e-05
Step: 8580, train/epoch: 2.0418848991394043
Step: 8590, train/loss: 0.0
Step: 8590, train/grad_norm: 1.1532331001262719e-07
Step: 8590, train/learning_rate: 1.592892294866033e-05
Step: 8590, train/epoch: 2.044264554977417
Step: 8600, train/loss: 0.0
Step: 8600, train/grad_norm: 3.825985004368704e-07
Step: 8600, train/learning_rate: 1.588925988471601e-05
Step: 8600, train/epoch: 2.046644449234009
Step: 8610, train/loss: 0.0
Step: 8610, train/grad_norm: 3.3623933859416866e-08
Step: 8610, train/learning_rate: 1.584959500178229e-05
Step: 8610, train/epoch: 2.0490243434906006
Step: 8620, train/loss: 0.0020000000949949026
Step: 8620, train/grad_norm: 7.920720008769422e-07
Step: 8620, train/learning_rate: 1.5809931937837973e-05
Step: 8620, train/epoch: 2.0514039993286133
Step: 8630, train/loss: 0.0
Step: 8630, train/grad_norm: 0.0014508559834212065
Step: 8630, train/learning_rate: 1.5770268873893656e-05
Step: 8630, train/epoch: 2.053783893585205
Step: 8640, train/loss: 0.0
Step: 8640, train/grad_norm: 3.725079977812129e-07
Step: 8640, train/learning_rate: 1.5730603990959935e-05
Step: 8640, train/epoch: 2.056163787841797
Step: 8650, train/loss: 0.0
Step: 8650, train/grad_norm: 5.712992106055026e-07
Step: 8650, train/learning_rate: 1.5690940927015617e-05
Step: 8650, train/epoch: 2.0585434436798096
Step: 8660, train/loss: 0.0
Step: 8660, train/grad_norm: 4.0810920154399355e-07
Step: 8660, train/learning_rate: 1.56512778630713e-05
Step: 8660, train/epoch: 2.0609233379364014
Step: 8670, train/loss: 0.0
Step: 8670, train/grad_norm: 4.4770725793341626e-08
Step: 8670, train/learning_rate: 1.561161298013758e-05
Step: 8670, train/epoch: 2.063303232192993
Step: 8680, train/loss: 0.0
Step: 8680, train/grad_norm: 5.06534661326441e-06
Step: 8680, train/learning_rate: 1.5571949916193262e-05
Step: 8680, train/epoch: 2.065683126449585
Step: 8690, train/loss: 0.0
Step: 8690, train/grad_norm: 5.925532241235487e-05
Step: 8690, train/learning_rate: 1.5532286852248944e-05
Step: 8690, train/epoch: 2.0680627822875977
Step: 8700, train/loss: 0.0
Step: 8700, train/grad_norm: 1.0007122455135686e-06
Step: 8700, train/learning_rate: 1.5492621969315223e-05
Step: 8700, train/epoch: 2.0704426765441895
Step: 8710, train/loss: 0.0
Step: 8710, train/grad_norm: 6.5738788634917e-08
Step: 8710, train/learning_rate: 1.5452958905370906e-05
Step: 8710, train/epoch: 2.0728225708007812
Step: 8720, train/loss: 0.0
Step: 8720, train/grad_norm: 8.60463629237529e-08
Step: 8720, train/learning_rate: 1.541329584142659e-05
Step: 8720, train/epoch: 2.075202226638794
Step: 8730, train/loss: 0.0
Step: 8730, train/grad_norm: 9.380063836772479e-09
Step: 8730, train/learning_rate: 1.5373630958492868e-05
Step: 8730, train/epoch: 2.0775821208953857
Step: 8740, train/loss: 0.0
Step: 8740, train/grad_norm: 6.636986995545158e-07
Step: 8740, train/learning_rate: 1.533396789454855e-05
Step: 8740, train/epoch: 2.0799620151519775
Step: 8750, train/loss: 0.0
Step: 8750, train/grad_norm: 3.0893284019839484e-07
Step: 8750, train/learning_rate: 1.5294304830604233e-05
Step: 8750, train/epoch: 2.0823416709899902
Step: 8760, train/loss: 0.0
Step: 8760, train/grad_norm: 1.5529569282080047e-05
Step: 8760, train/learning_rate: 1.5254640857165214e-05
Step: 8760, train/epoch: 2.084721565246582
Step: 8770, train/loss: 0.0
Step: 8770, train/grad_norm: 1.0586407483970106e-07
Step: 8770, train/learning_rate: 1.5214976883726195e-05
Step: 8770, train/epoch: 2.087101459503174
Step: 8780, train/loss: 0.0
Step: 8780, train/grad_norm: 6.243364225611003e-08
Step: 8780, train/learning_rate: 1.5175312910287175e-05
Step: 8780, train/epoch: 2.0894811153411865
Step: 8790, train/loss: 0.0
Step: 8790, train/grad_norm: 9.871941131223139e-08
Step: 8790, train/learning_rate: 1.5135649846342858e-05
Step: 8790, train/epoch: 2.0918610095977783
Step: 8800, train/loss: 0.0
Step: 8800, train/grad_norm: 9.677643930672275e-08
Step: 8800, train/learning_rate: 1.5095985872903839e-05
Step: 8800, train/epoch: 2.09424090385437
Step: 8810, train/loss: 0.0
Step: 8810, train/grad_norm: 4.5102906369720586e-06
Step: 8810, train/learning_rate: 1.5056322808959521e-05
Step: 8810, train/epoch: 2.096620559692383
Step: 8820, train/loss: 9.999999747378752e-05
Step: 8820, train/grad_norm: 7.835222248786522e-08
Step: 8820, train/learning_rate: 1.5016658835520502e-05
Step: 8820, train/epoch: 2.0990004539489746
Step: 8830, train/loss: 0.0
Step: 8830, train/grad_norm: 1.0427918795130608e-07
Step: 8830, train/learning_rate: 1.4976994862081483e-05
Step: 8830, train/epoch: 2.1013803482055664
Step: 8840, train/loss: 0.0
Step: 8840, train/grad_norm: 9.84856782793031e-08
Step: 8840, train/learning_rate: 1.4937331798137166e-05
Step: 8840, train/epoch: 2.103760004043579
Step: 8850, train/loss: 0.0
Step: 8850, train/grad_norm: 9.914163712210211e-09
Step: 8850, train/learning_rate: 1.4897667824698146e-05
Step: 8850, train/epoch: 2.106139898300171
Step: 8860, train/loss: 0.0
Step: 8860, train/grad_norm: 9.88638504395567e-10
Step: 8860, train/learning_rate: 1.4858003851259127e-05
Step: 8860, train/epoch: 2.1085197925567627
Step: 8870, train/loss: 0.0
Step: 8870, train/grad_norm: 2.027166656759505e-09
Step: 8870, train/learning_rate: 1.481834078731481e-05
Step: 8870, train/epoch: 2.1108996868133545
Step: 8880, train/loss: 0.0
Step: 8880, train/grad_norm: 1.1012805778420898e-08
Step: 8880, train/learning_rate: 1.477867681387579e-05
Step: 8880, train/epoch: 2.113279342651367
Step: 8890, train/loss: 0.0
Step: 8890, train/grad_norm: 1.0455820209642752e-08
Step: 8890, train/learning_rate: 1.4739012840436772e-05
Step: 8890, train/epoch: 2.115659236907959
Step: 8900, train/loss: 0.0
Step: 8900, train/grad_norm: 1.6595886620507372e-07
Step: 8900, train/learning_rate: 1.4699349776492454e-05
Step: 8900, train/epoch: 2.118039131164551
Step: 8910, train/loss: 0.0
Step: 8910, train/grad_norm: 1.1352276452214483e-08
Step: 8910, train/learning_rate: 1.4659685803053435e-05
Step: 8910, train/epoch: 2.1204187870025635
Step: 8920, train/loss: 0.0
Step: 8920, train/grad_norm: 2.937668552860373e-09
Step: 8920, train/learning_rate: 1.4620021829614416e-05
Step: 8920, train/epoch: 2.1227986812591553
Step: 8930, train/loss: 0.0
Step: 8930, train/grad_norm: 1.830717053508124e-07
Step: 8930, train/learning_rate: 1.4580358765670098e-05
Step: 8930, train/epoch: 2.125178575515747
Step: 8940, train/loss: 0.0
Step: 8940, train/grad_norm: 2.2069292526794015e-07
Step: 8940, train/learning_rate: 1.454069479223108e-05
Step: 8940, train/epoch: 2.1275582313537598
Step: 8950, train/loss: 0.0
Step: 8950, train/grad_norm: 7.956032987976869e-08
Step: 8950, train/learning_rate: 1.450103081879206e-05
Step: 8950, train/epoch: 2.1299381256103516
Step: 8960, train/loss: 0.0
Step: 8960, train/grad_norm: 3.969780593138239e-08
Step: 8960, train/learning_rate: 1.4461367754847743e-05
Step: 8960, train/epoch: 2.1323180198669434
Step: 8970, train/loss: 0.0
Step: 8970, train/grad_norm: 1.7184139267101273e-07
Step: 8970, train/learning_rate: 1.4421703781408723e-05
Step: 8970, train/epoch: 2.134697675704956
Step: 8980, train/loss: 0.0
Step: 8980, train/grad_norm: 4.586526358707488e-08
Step: 8980, train/learning_rate: 1.4382040717464406e-05
Step: 8980, train/epoch: 2.137077569961548
Step: 8990, train/loss: 0.0
Step: 8990, train/grad_norm: 1.0863464572352655e-09
Step: 8990, train/learning_rate: 1.4342376744025387e-05
Step: 8990, train/epoch: 2.1394574642181396
Step: 9000, train/loss: 0.0
Step: 9000, train/grad_norm: 1.409399175145154e-07
Step: 9000, train/learning_rate: 1.4302712770586368e-05
Step: 9000, train/epoch: 2.1418371200561523
Step: 9010, train/loss: 0.0
Step: 9010, train/grad_norm: 3.0592950395202934e-09
Step: 9010, train/learning_rate: 1.426304970664205e-05
Step: 9010, train/epoch: 2.144217014312744
Step: 9020, train/loss: 0.0
Step: 9020, train/grad_norm: 3.282381658209488e-05
Step: 9020, train/learning_rate: 1.4223385733203031e-05
Step: 9020, train/epoch: 2.146596908569336
Step: 9030, train/loss: 0.0
Step: 9030, train/grad_norm: 1.0337682931904624e-09
Step: 9030, train/learning_rate: 1.4183721759764012e-05
Step: 9030, train/epoch: 2.1489765644073486
Step: 9040, train/loss: 0.0
Step: 9040, train/grad_norm: 3.4906022960967675e-08
Step: 9040, train/learning_rate: 1.4144058695819695e-05
Step: 9040, train/epoch: 2.1513564586639404
Step: 9050, train/loss: 0.0
Step: 9050, train/grad_norm: 8.507084459097314e-08
Step: 9050, train/learning_rate: 1.4104394722380675e-05
Step: 9050, train/epoch: 2.1537363529205322
Step: 9060, train/loss: 0.0
Step: 9060, train/grad_norm: 1.2056152520756314e-08
Step: 9060, train/learning_rate: 1.4064730748941656e-05
Step: 9060, train/epoch: 2.156116247177124
Step: 9070, train/loss: 0.0
Step: 9070, train/grad_norm: 1.564768226103297e-08
Step: 9070, train/learning_rate: 1.4025067684997339e-05
Step: 9070, train/epoch: 2.1584959030151367
Step: 9080, train/loss: 0.0
Step: 9080, train/grad_norm: 1.2818430761285526e-08
Step: 9080, train/learning_rate: 1.398540371155832e-05
Step: 9080, train/epoch: 2.1608757972717285
Step: 9090, train/loss: 0.0
Step: 9090, train/grad_norm: 9.624717023370977e-08
Step: 9090, train/learning_rate: 1.39457397381193e-05
Step: 9090, train/epoch: 2.1632556915283203
Step: 9100, train/loss: 0.0
Step: 9100, train/grad_norm: 0.00037625670665875077
Step: 9100, train/learning_rate: 1.3906076674174983e-05
Step: 9100, train/epoch: 2.165635347366333
Step: 9110, train/loss: 0.0
Step: 9110, train/grad_norm: 8.06663358332571e-09
Step: 9110, train/learning_rate: 1.3866412700735964e-05
Step: 9110, train/epoch: 2.168015241622925
Step: 9120, train/loss: 0.0
Step: 9120, train/grad_norm: 1.0007666872979826e-07
Step: 9120, train/learning_rate: 1.3826748727296945e-05
Step: 9120, train/epoch: 2.1703951358795166
Step: 9130, train/loss: 0.2312999963760376
Step: 9130, train/grad_norm: 33.754974365234375
Step: 9130, train/learning_rate: 1.3787085663352627e-05
Step: 9130, train/epoch: 2.1727747917175293
Step: 9140, train/loss: 9.999999747378752e-05
Step: 9140, train/grad_norm: 0.11669633537530899
Step: 9140, train/learning_rate: 1.3747421689913608e-05
Step: 9140, train/epoch: 2.175154685974121
Step: 9150, train/loss: 0.0003000000142492354
Step: 9150, train/grad_norm: 0.0009885573526844382
Step: 9150, train/learning_rate: 1.370775862596929e-05
Step: 9150, train/epoch: 2.177534580230713
Step: 9160, train/loss: 0.0
Step: 9160, train/grad_norm: 8.14058439573273e-05
Step: 9160, train/learning_rate: 1.3668094652530272e-05
Step: 9160, train/epoch: 2.1799142360687256
Step: 9170, train/loss: 0.0
Step: 9170, train/grad_norm: 2.3467622668249533e-05
Step: 9170, train/learning_rate: 1.3628430679091252e-05
Step: 9170, train/epoch: 2.1822941303253174
Step: 9180, train/loss: 0.0
Step: 9180, train/grad_norm: 3.0116309062577784e-05
Step: 9180, train/learning_rate: 1.3588767615146935e-05
Step: 9180, train/epoch: 2.184674024581909
Step: 9190, train/loss: 0.0
Step: 9190, train/grad_norm: 3.32399140461348e-05
Step: 9190, train/learning_rate: 1.3549103641707916e-05
Step: 9190, train/epoch: 2.187053680419922
Step: 9200, train/loss: 0.0
Step: 9200, train/grad_norm: 9.49368313740706e-06
Step: 9200, train/learning_rate: 1.3509439668268897e-05
Step: 9200, train/epoch: 2.1894335746765137
Step: 9210, train/loss: 0.0
Step: 9210, train/grad_norm: 1.987975883821491e-05
Step: 9210, train/learning_rate: 1.346977660432458e-05
Step: 9210, train/epoch: 2.1918134689331055
Step: 9220, train/loss: 0.0
Step: 9220, train/grad_norm: 1.4362782167154364e-05
Step: 9220, train/learning_rate: 1.343011263088556e-05
Step: 9220, train/epoch: 2.194193124771118
Step: 9230, train/loss: 0.0
Step: 9230, train/grad_norm: 0.00029297408764250576
Step: 9230, train/learning_rate: 1.3390448657446541e-05
Step: 9230, train/epoch: 2.19657301902771
Step: 9240, train/loss: 0.0
Step: 9240, train/grad_norm: 1.0008462595578749e-05
Step: 9240, train/learning_rate: 1.3350785593502223e-05
Step: 9240, train/epoch: 2.1989529132843018
Step: 9250, train/loss: 0.0
Step: 9250, train/grad_norm: 1.9844948837999254e-05
Step: 9250, train/learning_rate: 1.3311121620063204e-05
Step: 9250, train/epoch: 2.2013328075408936
Step: 9260, train/loss: 0.0
Step: 9260, train/grad_norm: 3.8044829125283286e-05
Step: 9260, train/learning_rate: 1.3271457646624185e-05
Step: 9260, train/epoch: 2.2037124633789062
Step: 9270, train/loss: 0.0
Step: 9270, train/grad_norm: 3.087599452555878e-06
Step: 9270, train/learning_rate: 1.3231794582679868e-05
Step: 9270, train/epoch: 2.206092357635498
Step: 9280, train/loss: 0.0
Step: 9280, train/grad_norm: 1.2116242942283861e-05
Step: 9280, train/learning_rate: 1.3192130609240849e-05
Step: 9280, train/epoch: 2.20847225189209
Step: 9290, train/loss: 0.0
Step: 9290, train/grad_norm: 7.568341516162036e-06
Step: 9290, train/learning_rate: 1.315246663580183e-05
Step: 9290, train/epoch: 2.2108519077301025
Step: 9300, train/loss: 0.0
Step: 9300, train/grad_norm: 6.940009916434065e-05
Step: 9300, train/learning_rate: 1.3112803571857512e-05
Step: 9300, train/epoch: 2.2132318019866943
Step: 9310, train/loss: 0.0
Step: 9310, train/grad_norm: 6.388818292180076e-06
Step: 9310, train/learning_rate: 1.3073139598418493e-05
Step: 9310, train/epoch: 2.215611696243286
Step: 9320, train/loss: 0.0
Step: 9320, train/grad_norm: 6.661860879830783e-06
Step: 9320, train/learning_rate: 1.3033476534474175e-05
Step: 9320, train/epoch: 2.217991352081299
Step: 9330, train/loss: 0.0
Step: 9330, train/grad_norm: 3.275743802078068e-05
Step: 9330, train/learning_rate: 1.2993812561035156e-05
Step: 9330, train/epoch: 2.2203712463378906
Step: 9340, train/loss: 0.0
Step: 9340, train/grad_norm: 2.8048621970810927e-05
Step: 9340, train/learning_rate: 1.2954148587596137e-05
Step: 9340, train/epoch: 2.2227511405944824
Step: 9350, train/loss: 0.0
Step: 9350, train/grad_norm: 3.872327579301782e-05
Step: 9350, train/learning_rate: 1.291448552365182e-05
Step: 9350, train/epoch: 2.225130796432495
Step: 9360, train/loss: 0.0
Step: 9360, train/grad_norm: 4.961110244039446e-05
Step: 9360, train/learning_rate: 1.28748215502128e-05
Step: 9360, train/epoch: 2.227510690689087
Step: 9370, train/loss: 0.0
Step: 9370, train/grad_norm: 1.809140849218238e-05
Step: 9370, train/learning_rate: 1.2835157576773781e-05
Step: 9370, train/epoch: 2.2298905849456787
Step: 9380, train/loss: 0.0
Step: 9380, train/grad_norm: 7.448264568665763e-06
Step: 9380, train/learning_rate: 1.2795494512829464e-05
Step: 9380, train/epoch: 2.2322702407836914
Step: 9390, train/loss: 0.15309999883174896
Step: 9390, train/grad_norm: 3.3462422379670897e-06
Step: 9390, train/learning_rate: 1.2755830539390445e-05
Step: 9390, train/epoch: 2.234650135040283
Step: 9400, train/loss: 0.0
Step: 9400, train/grad_norm: 2.325878767805989e-06
Step: 9400, train/learning_rate: 1.2716166565951426e-05
Step: 9400, train/epoch: 2.237030029296875
Step: 9410, train/loss: 0.0
Step: 9410, train/grad_norm: 4.474156867217971e-06
Step: 9410, train/learning_rate: 1.2676503502007108e-05
Step: 9410, train/epoch: 2.239409923553467
Step: 9420, train/loss: 0.0
Step: 9420, train/grad_norm: 0.00013071420835331082
Step: 9420, train/learning_rate: 1.2636839528568089e-05
Step: 9420, train/epoch: 2.2417895793914795
Step: 9430, train/loss: 0.0
Step: 9430, train/grad_norm: 2.4311775632668287e-05
Step: 9430, train/learning_rate: 1.259717555512907e-05
Step: 9430, train/epoch: 2.2441694736480713
Step: 9440, train/loss: 0.0
Step: 9440, train/grad_norm: 8.459297532681376e-06
Step: 9440, train/learning_rate: 1.2557512491184752e-05
Step: 9440, train/epoch: 2.246549367904663
Step: 9450, train/loss: 0.0
Step: 9450, train/grad_norm: 0.00035012385342270136
Step: 9450, train/learning_rate: 1.2517848517745733e-05
Step: 9450, train/epoch: 2.248929023742676
Step: 9460, train/loss: 0.0
Step: 9460, train/grad_norm: 5.030080137657933e-05
Step: 9460, train/learning_rate: 1.2478184544306714e-05
Step: 9460, train/epoch: 2.2513089179992676
Step: 9470, train/loss: 0.13359999656677246
Step: 9470, train/grad_norm: 3.913531600119313e-06
Step: 9470, train/learning_rate: 1.2438521480362397e-05
Step: 9470, train/epoch: 2.2536888122558594
Step: 9480, train/loss: 0.0
Step: 9480, train/grad_norm: 0.00016879108443390578
Step: 9480, train/learning_rate: 1.2398857506923378e-05
Step: 9480, train/epoch: 2.256068468093872
Step: 9490, train/loss: 0.0
Step: 9490, train/grad_norm: 8.34030652185902e-05
Step: 9490, train/learning_rate: 1.235919444297906e-05
Step: 9490, train/epoch: 2.258448362350464
Step: 9500, train/loss: 0.0
Step: 9500, train/grad_norm: 2.4814980861265212e-05
Step: 9500, train/learning_rate: 1.2319530469540041e-05
Step: 9500, train/epoch: 2.2608282566070557
Step: 9510, train/loss: 0.0
Step: 9510, train/grad_norm: 3.771371120819822e-05
Step: 9510, train/learning_rate: 1.2279866496101022e-05
Step: 9510, train/epoch: 2.2632079124450684
Step: 9520, train/loss: 0.0
Step: 9520, train/grad_norm: 5.704271461581811e-05
Step: 9520, train/learning_rate: 1.2240203432156704e-05
Step: 9520, train/epoch: 2.26558780670166
Step: 9530, train/loss: 0.0
Step: 9530, train/grad_norm: 1.515134863439016e-05
Step: 9530, train/learning_rate: 1.2200539458717685e-05
Step: 9530, train/epoch: 2.267967700958252
Step: 9540, train/loss: 0.0
Step: 9540, train/grad_norm: 2.6819061531568877e-05
Step: 9540, train/learning_rate: 1.2160875485278666e-05
Step: 9540, train/epoch: 2.2703473567962646
Step: 9550, train/loss: 0.0
Step: 9550, train/grad_norm: 3.76286479877308e-05
Step: 9550, train/learning_rate: 1.2121212421334349e-05
Step: 9550, train/epoch: 2.2727272510528564
Step: 9560, train/loss: 0.0
Step: 9560, train/grad_norm: 0.00018650504352990538
Step: 9560, train/learning_rate: 1.208154844789533e-05
Step: 9560, train/epoch: 2.2751071453094482
Step: 9570, train/loss: 0.0
Step: 9570, train/grad_norm: 9.04646294657141e-05
Step: 9570, train/learning_rate: 1.204188447445631e-05
Step: 9570, train/epoch: 2.277486801147461
Step: 9580, train/loss: 0.0
Step: 9580, train/grad_norm: 0.025157740339636803
Step: 9580, train/learning_rate: 1.2002221410511993e-05
Step: 9580, train/epoch: 2.2798666954040527
Step: 9590, train/loss: 0.0
Step: 9590, train/grad_norm: 5.261924889055081e-05
Step: 9590, train/learning_rate: 1.1962557437072974e-05
Step: 9590, train/epoch: 2.2822465896606445
Step: 9600, train/loss: 0.0
Step: 9600, train/grad_norm: 6.381997081916779e-05
Step: 9600, train/learning_rate: 1.1922893463633955e-05
Step: 9600, train/epoch: 2.2846264839172363
Step: 9610, train/loss: 0.0
Step: 9610, train/grad_norm: 2.0185363103220055e-10
Step: 9610, train/learning_rate: 1.1883230399689637e-05
Step: 9610, train/epoch: 2.287006139755249
Step: 9620, train/loss: 0.0
Step: 9620, train/grad_norm: 0.00027646601665765047
Step: 9620, train/learning_rate: 1.1843566426250618e-05
Step: 9620, train/epoch: 2.289386034011841
Step: 9630, train/loss: 0.0
Step: 9630, train/grad_norm: 0.00021161064796615392
Step: 9630, train/learning_rate: 1.1803902452811599e-05
Step: 9630, train/epoch: 2.2917659282684326
Step: 9640, train/loss: 0.0
Step: 9640, train/grad_norm: 0.0002961415739264339
Step: 9640, train/learning_rate: 1.1764239388867281e-05
Step: 9640, train/epoch: 2.2941455841064453
Step: 9650, train/loss: 0.0
Step: 9650, train/grad_norm: 1.1591040674829856e-05
Step: 9650, train/learning_rate: 1.1724575415428262e-05
Step: 9650, train/epoch: 2.296525478363037
Step: 9660, train/loss: 0.0
Step: 9660, train/grad_norm: 7.069417188176885e-05
Step: 9660, train/learning_rate: 1.1684912351483945e-05
Step: 9660, train/epoch: 2.298905372619629
Step: 9670, train/loss: 0.0364999994635582
Step: 9670, train/grad_norm: 8.717826858628541e-05
Step: 9670, train/learning_rate: 1.1645248378044926e-05
Step: 9670, train/epoch: 2.3012850284576416
Step: 9680, train/loss: 0.0
Step: 9680, train/grad_norm: 1.624862852622755e-05
Step: 9680, train/learning_rate: 1.1605584404605906e-05
Step: 9680, train/epoch: 2.3036649227142334
Step: 9690, train/loss: 0.0
Step: 9690, train/grad_norm: 6.165591912576929e-05
Step: 9690, train/learning_rate: 1.1565921340661589e-05
Step: 9690, train/epoch: 2.306044816970825
Step: 9700, train/loss: 0.0
Step: 9700, train/grad_norm: 6.151827165012946e-06
Step: 9700, train/learning_rate: 1.152625736722257e-05
Step: 9700, train/epoch: 2.308424472808838
Step: 9710, train/loss: 0.0
Step: 9710, train/grad_norm: 5.782837433798704e-06
Step: 9710, train/learning_rate: 1.148659339378355e-05
Step: 9710, train/epoch: 2.3108043670654297
Step: 9720, train/loss: 0.0
Step: 9720, train/grad_norm: 3.0373080335266422e-06
Step: 9720, train/learning_rate: 1.1446930329839233e-05
Step: 9720, train/epoch: 2.3131842613220215
Step: 9730, train/loss: 0.0
Step: 9730, train/grad_norm: 7.714255843893625e-06
Step: 9730, train/learning_rate: 1.1407266356400214e-05
Step: 9730, train/epoch: 2.315563917160034
Step: 9740, train/loss: 0.0
Step: 9740, train/grad_norm: 6.813268669247918e-07
Step: 9740, train/learning_rate: 1.1367602382961195e-05
Step: 9740, train/epoch: 2.317943811416626
Step: 9750, train/loss: 0.0
Step: 9750, train/grad_norm: 4.413665010361001e-06
Step: 9750, train/learning_rate: 1.1327939319016878e-05
Step: 9750, train/epoch: 2.3203237056732178
Step: 9760, train/loss: 0.0
Step: 9760, train/grad_norm: 7.793947588652372e-06
Step: 9760, train/learning_rate: 1.1288275345577858e-05
Step: 9760, train/epoch: 2.3227033615112305
Step: 9770, train/loss: 9.999999747378752e-05
Step: 9770, train/grad_norm: 5.267996129987296e-06
Step: 9770, train/learning_rate: 1.124861137213884e-05
Step: 9770, train/epoch: 2.3250832557678223
Step: 9780, train/loss: 0.0
Step: 9780, train/grad_norm: 3.5098994430882158e-06
Step: 9780, train/learning_rate: 1.1208948308194522e-05
Step: 9780, train/epoch: 2.327463150024414
Step: 9790, train/loss: 0.0
Step: 9790, train/grad_norm: 3.801573939199443e-06
Step: 9790, train/learning_rate: 1.1169284334755503e-05
Step: 9790, train/epoch: 2.329843044281006
Step: 9800, train/loss: 0.0
Step: 9800, train/grad_norm: 5.625831818178995e-07
Step: 9800, train/learning_rate: 1.1129620361316483e-05
Step: 9800, train/epoch: 2.3322227001190186
Step: 9810, train/loss: 0.004800000227987766
Step: 9810, train/grad_norm: 2.3252798200701363e-05
Step: 9810, train/learning_rate: 1.1089957297372166e-05
Step: 9810, train/epoch: 2.3346025943756104
Step: 9820, train/loss: 0.0
Step: 9820, train/grad_norm: 9.493999641563278e-06
Step: 9820, train/learning_rate: 1.1050293323933147e-05
Step: 9820, train/epoch: 2.336982488632202
Step: 9830, train/loss: 0.0
Step: 9830, train/grad_norm: 4.702608748630155e-06
Step: 9830, train/learning_rate: 1.101063025998883e-05
Step: 9830, train/epoch: 2.339362144470215
Step: 9840, train/loss: 0.0
Step: 9840, train/grad_norm: 0.001118289539590478
Step: 9840, train/learning_rate: 1.097096628654981e-05
Step: 9840, train/epoch: 2.3417420387268066
Step: 9850, train/loss: 0.0
Step: 9850, train/grad_norm: 1.0909219781751744e-05
Step: 9850, train/learning_rate: 1.0931302313110791e-05
Step: 9850, train/epoch: 2.3441219329833984
Step: 9860, train/loss: 0.0
Step: 9860, train/grad_norm: 7.655491208424792e-05
Step: 9860, train/learning_rate: 1.0891639249166474e-05
Step: 9860, train/epoch: 2.346501588821411
Step: 9870, train/loss: 0.0
Step: 9870, train/grad_norm: 0.00013012406998313963
Step: 9870, train/learning_rate: 1.0851975275727455e-05
Step: 9870, train/epoch: 2.348881483078003
Step: 9880, train/loss: 0.0
Step: 9880, train/grad_norm: 3.8994789065327495e-05
Step: 9880, train/learning_rate: 1.0812311302288435e-05
Step: 9880, train/epoch: 2.3512613773345947
Step: 9890, train/loss: 0.0
Step: 9890, train/grad_norm: 6.411167305486742e-06
Step: 9890, train/learning_rate: 1.0772648238344118e-05
Step: 9890, train/epoch: 2.3536410331726074
Step: 9900, train/loss: 0.0
Step: 9900, train/grad_norm: 1.1187254358446808e-06
Step: 9900, train/learning_rate: 1.0732984264905099e-05
Step: 9900, train/epoch: 2.356020927429199
Step: 9910, train/loss: 0.0
Step: 9910, train/grad_norm: 1.5734394764876924e-05
Step: 9910, train/learning_rate: 1.069332029146608e-05
Step: 9910, train/epoch: 2.358400821685791
Step: 9920, train/loss: 0.0
Step: 9920, train/grad_norm: 1.570803942740895e-05
Step: 9920, train/learning_rate: 1.0653657227521762e-05
Step: 9920, train/epoch: 2.3607804775238037
Step: 9930, train/loss: 0.0
Step: 9930, train/grad_norm: 1.0413625659566605e-06
Step: 9930, train/learning_rate: 1.0613993254082743e-05
Step: 9930, train/epoch: 2.3631603717803955
Step: 9940, train/loss: 0.0
Step: 9940, train/grad_norm: 2.7172885893378407e-05
Step: 9940, train/learning_rate: 1.0574329280643724e-05
Step: 9940, train/epoch: 2.3655402660369873
Step: 9950, train/loss: 0.0
Step: 9950, train/grad_norm: 3.1948165997164324e-05
Step: 9950, train/learning_rate: 1.0534666216699407e-05
Step: 9950, train/epoch: 2.367919921875
Step: 9960, train/loss: 0.0
Step: 9960, train/grad_norm: 2.876349526559352e-06
Step: 9960, train/learning_rate: 1.0495002243260387e-05
Step: 9960, train/epoch: 2.370299816131592
Step: 9970, train/loss: 0.0
Step: 9970, train/grad_norm: 2.1121859390405007e-06
Step: 9970, train/learning_rate: 1.045533917931607e-05
Step: 9970, train/epoch: 2.3726797103881836
Step: 9980, train/loss: 0.0
Step: 9980, train/grad_norm: 3.715870843734592e-05
Step: 9980, train/learning_rate: 1.041567520587705e-05
Step: 9980, train/epoch: 2.3750596046447754
Step: 9990, train/loss: 0.0
Step: 9990, train/grad_norm: 1.3126533531249152e-06
Step: 9990, train/learning_rate: 1.0376011232438032e-05
Step: 9990, train/epoch: 2.377439260482788
Step: 10000, train/loss: 0.0
Step: 10000, train/grad_norm: 1.536435229354538e-05
Step: 10000, train/learning_rate: 1.0336348168493714e-05
Step: 10000, train/epoch: 2.37981915473938
Step: 10010, train/loss: 0.0
Step: 10010, train/grad_norm: 4.123293365410063e-06
Step: 10010, train/learning_rate: 1.0296684195054695e-05
Step: 10010, train/epoch: 2.3821990489959717
Step: 10020, train/loss: 0.0
Step: 10020, train/grad_norm: 4.688048647949472e-06
Step: 10020, train/learning_rate: 1.0257020221615676e-05
Step: 10020, train/epoch: 2.3845787048339844
Step: 10030, train/loss: 0.0
Step: 10030, train/grad_norm: 3.2029034628067166e-05
Step: 10030, train/learning_rate: 1.0217357157671358e-05
Step: 10030, train/epoch: 2.386958599090576
Step: 10040, train/loss: 0.0
Step: 10040, train/grad_norm: 0.005190706811845303
Step: 10040, train/learning_rate: 1.017769318423234e-05
Step: 10040, train/epoch: 2.389338493347168
Step: 10050, train/loss: 0.0
Step: 10050, train/grad_norm: 5.376826720748795e-06
Step: 10050, train/learning_rate: 1.013802921079332e-05
Step: 10050, train/epoch: 2.3917181491851807
Step: 10060, train/loss: 0.0
Step: 10060, train/grad_norm: 1.9691802663146518e-05
Step: 10060, train/learning_rate: 1.0098366146849003e-05
Step: 10060, train/epoch: 2.3940980434417725
Step: 10070, train/loss: 9.999999747378752e-05
Step: 10070, train/grad_norm: 3.6642256873165024e-06
Step: 10070, train/learning_rate: 1.0058702173409984e-05
Step: 10070, train/epoch: 2.3964779376983643
Step: 10080, train/loss: 0.0
Step: 10080, train/grad_norm: 7.63335447118152e-06
Step: 10080, train/learning_rate: 1.0019038199970964e-05
Step: 10080, train/epoch: 2.398857593536377
Step: 10090, train/loss: 0.0
Step: 10090, train/grad_norm: 4.991442438040394e-06
Step: 10090, train/learning_rate: 9.979375136026647e-06
Step: 10090, train/epoch: 2.4012374877929688
Step: 10100, train/loss: 0.0
Step: 10100, train/grad_norm: 3.418341657379642e-05
Step: 10100, train/learning_rate: 9.939711162587628e-06
Step: 10100, train/epoch: 2.4036173820495605
Step: 10110, train/loss: 0.0
Step: 10110, train/grad_norm: 5.028519808547571e-06
Step: 10110, train/learning_rate: 9.900047189148609e-06
Step: 10110, train/epoch: 2.4059970378875732
Step: 10120, train/loss: 0.0
Step: 10120, train/grad_norm: 0.00041356708970852196
Step: 10120, train/learning_rate: 9.860384125204291e-06
Step: 10120, train/epoch: 2.408376932144165
Step: 10130, train/loss: 0.0
Step: 10130, train/grad_norm: 2.832150312315207e-05
Step: 10130, train/learning_rate: 9.820720151765272e-06
Step: 10130, train/epoch: 2.410756826400757
Step: 10140, train/loss: 0.0
Step: 10140, train/grad_norm: 1.1124946468044072e-05
Step: 10140, train/learning_rate: 9.781057087820955e-06
Step: 10140, train/epoch: 2.4131367206573486
Step: 10150, train/loss: 0.0
Step: 10150, train/grad_norm: 0.00021748672588728368
Step: 10150, train/learning_rate: 9.741393114381935e-06
Step: 10150, train/epoch: 2.4155163764953613
Step: 10160, train/loss: 0.0
Step: 10160, train/grad_norm: 9.569732355885208e-05
Step: 10160, train/learning_rate: 9.701729140942916e-06
Step: 10160, train/epoch: 2.417896270751953
Step: 10170, train/loss: 0.0
Step: 10170, train/grad_norm: 1.943496499734465e-05
Step: 10170, train/learning_rate: 9.662066076998599e-06
Step: 10170, train/epoch: 2.420276165008545
Step: 10180, train/loss: 0.0
Step: 10180, train/grad_norm: 4.507417088461807e-06
Step: 10180, train/learning_rate: 9.62240210355958e-06
Step: 10180, train/epoch: 2.4226558208465576
Step: 10190, train/loss: 0.0
Step: 10190, train/grad_norm: 0.011709201149642467
Step: 10190, train/learning_rate: 9.58273813012056e-06
Step: 10190, train/epoch: 2.4250357151031494
Step: 10200, train/loss: 0.0
Step: 10200, train/grad_norm: 2.320446219528094e-05
Step: 10200, train/learning_rate: 9.543075066176243e-06
Step: 10200, train/epoch: 2.427415609359741
Step: 10210, train/loss: 0.0
Step: 10210, train/grad_norm: 4.583751069731079e-05
Step: 10210, train/learning_rate: 9.503411092737224e-06
Step: 10210, train/epoch: 2.429795265197754
Step: 10220, train/loss: 0.0
Step: 10220, train/grad_norm: 0.0001006021848297678
Step: 10220, train/learning_rate: 9.463747119298205e-06
Step: 10220, train/epoch: 2.4321751594543457
Step: 10230, train/loss: 0.0
Step: 10230, train/grad_norm: 4.131736568524502e-05
Step: 10230, train/learning_rate: 9.424084055353887e-06
Step: 10230, train/epoch: 2.4345550537109375
Step: 10240, train/loss: 0.0
Step: 10240, train/grad_norm: 3.403926166356541e-05
Step: 10240, train/learning_rate: 9.384420081914868e-06
Step: 10240, train/epoch: 2.43693470954895
Step: 10250, train/loss: 0.0
Step: 10250, train/grad_norm: 0.0034915092401206493
Step: 10250, train/learning_rate: 9.344756108475849e-06
Step: 10250, train/epoch: 2.439314603805542
Step: 10260, train/loss: 0.0
Step: 10260, train/grad_norm: 3.579641634132713e-05
Step: 10260, train/learning_rate: 9.305093044531532e-06
Step: 10260, train/epoch: 2.441694498062134
Step: 10270, train/loss: 0.0
Step: 10270, train/grad_norm: 8.747079846216366e-05
Step: 10270, train/learning_rate: 9.265429071092512e-06
Step: 10270, train/epoch: 2.4440741539001465
Step: 10280, train/loss: 0.0
Step: 10280, train/grad_norm: 2.022949502133997e-06
Step: 10280, train/learning_rate: 9.225765097653493e-06
Step: 10280, train/epoch: 2.4464540481567383
Step: 10290, train/loss: 0.0
Step: 10290, train/grad_norm: 8.01648639026098e-06
Step: 10290, train/learning_rate: 9.186102033709176e-06
Step: 10290, train/epoch: 2.44883394241333
Step: 10300, train/loss: 0.0
Step: 10300, train/grad_norm: 4.746341801364906e-06
Step: 10300, train/learning_rate: 9.146438060270157e-06
Step: 10300, train/epoch: 2.4512135982513428
Step: 10310, train/loss: 0.0
Step: 10310, train/grad_norm: 4.002145942649804e-06
Step: 10310, train/learning_rate: 9.10677499632584e-06
Step: 10310, train/epoch: 2.4535934925079346
Step: 10320, train/loss: 0.0
Step: 10320, train/grad_norm: 4.41535230493173e-05
Step: 10320, train/learning_rate: 9.06711102288682e-06
Step: 10320, train/epoch: 2.4559733867645264
Step: 10330, train/loss: 0.0
Step: 10330, train/grad_norm: 8.689972310094163e-05
Step: 10330, train/learning_rate: 9.027447049447801e-06
Step: 10330, train/epoch: 2.458353281021118
Step: 10340, train/loss: 0.0
Step: 10340, train/grad_norm: 7.2678362812439445e-06
Step: 10340, train/learning_rate: 8.987783985503484e-06
Step: 10340, train/epoch: 2.460732936859131
Step: 10350, train/loss: 0.0
Step: 10350, train/grad_norm: 1.8518019714974798e-05
Step: 10350, train/learning_rate: 8.948120012064464e-06
Step: 10350, train/epoch: 2.4631128311157227
Step: 10360, train/loss: 0.0
Step: 10360, train/grad_norm: 4.996558345737867e-06
Step: 10360, train/learning_rate: 8.908456038625445e-06
Step: 10360, train/epoch: 2.4654927253723145
Step: 10370, train/loss: 0.0
Step: 10370, train/grad_norm: 1.8601784177008085e-05
Step: 10370, train/learning_rate: 8.868792974681128e-06
Step: 10370, train/epoch: 2.467872381210327
Step: 10380, train/loss: 0.0
Step: 10380, train/grad_norm: 1.0594300874799956e-05
Step: 10380, train/learning_rate: 8.829129001242109e-06
Step: 10380, train/epoch: 2.470252275466919
Step: 10390, train/loss: 0.0
Step: 10390, train/grad_norm: 4.4566713768290356e-05
Step: 10390, train/learning_rate: 8.78946502780309e-06
Step: 10390, train/epoch: 2.4726321697235107
Step: 10400, train/loss: 0.0
Step: 10400, train/grad_norm: 2.342655716347508e-05
Step: 10400, train/learning_rate: 8.749801963858772e-06
Step: 10400, train/epoch: 2.4750118255615234
Step: 10410, train/loss: 0.0
Step: 10410, train/grad_norm: 6.062935426598415e-05
Step: 10410, train/learning_rate: 8.710137990419753e-06
Step: 10410, train/epoch: 2.4773917198181152
Step: 10420, train/loss: 0.0
Step: 10420, train/grad_norm: 5.50401750842866e-07
Step: 10420, train/learning_rate: 8.670474016980734e-06
Step: 10420, train/epoch: 2.479771614074707
Step: 10430, train/loss: 0.0
Step: 10430, train/grad_norm: 8.154725037456956e-06
Step: 10430, train/learning_rate: 8.630810953036416e-06
Step: 10430, train/epoch: 2.4821512699127197
Step: 10440, train/loss: 0.0
Step: 10440, train/grad_norm: 0.0001027652106131427
Step: 10440, train/learning_rate: 8.591146979597397e-06
Step: 10440, train/epoch: 2.4845311641693115
Step: 10450, train/loss: 0.0
Step: 10450, train/grad_norm: 7.6020201049686875e-06
Step: 10450, train/learning_rate: 8.551483006158378e-06
Step: 10450, train/epoch: 2.4869110584259033
Step: 10460, train/loss: 0.0
Step: 10460, train/grad_norm: 1.1784559319494292e-05
Step: 10460, train/learning_rate: 8.51181994221406e-06
Step: 10460, train/epoch: 2.489290714263916
Step: 10470, train/loss: 0.0
Step: 10470, train/grad_norm: 0.00017303491767961532
Step: 10470, train/learning_rate: 8.472155968775041e-06
Step: 10470, train/epoch: 2.491670608520508
Step: 10480, train/loss: 0.0
Step: 10480, train/grad_norm: 4.291114692023257e-06
Step: 10480, train/learning_rate: 8.432492904830724e-06
Step: 10480, train/epoch: 2.4940505027770996
Step: 10490, train/loss: 0.0
Step: 10490, train/grad_norm: 3.193953034497099e-06
Step: 10490, train/learning_rate: 8.392828931391705e-06
Step: 10490, train/epoch: 2.4964301586151123
Step: 10500, train/loss: 0.0
Step: 10500, train/grad_norm: 2.54752958426252e-05
Step: 10500, train/learning_rate: 8.353164957952686e-06
Step: 10500, train/epoch: 2.498810052871704
Step: 10510, train/loss: 0.0
Step: 10510, train/grad_norm: 9.281835446017794e-06
Step: 10510, train/learning_rate: 8.313501894008368e-06
Step: 10510, train/epoch: 2.501189947128296
Step: 10520, train/loss: 0.0
Step: 10520, train/grad_norm: 4.54225073553971e-06
Step: 10520, train/learning_rate: 8.273837920569349e-06
Step: 10520, train/epoch: 2.5035698413848877
Step: 10530, train/loss: 0.0
Step: 10530, train/grad_norm: 1.4657674000773113e-05
Step: 10530, train/learning_rate: 8.23417394713033e-06
Step: 10530, train/epoch: 2.5059494972229004
Step: 10540, train/loss: 0.0
Step: 10540, train/grad_norm: 1.7285809008171782e-05
Step: 10540, train/learning_rate: 8.194510883186013e-06
Step: 10540, train/epoch: 2.508329391479492
Step: 10550, train/loss: 0.0
Step: 10550, train/grad_norm: 4.69275164505234e-06
Step: 10550, train/learning_rate: 8.154846909746993e-06
Step: 10550, train/epoch: 2.510709285736084
Step: 10560, train/loss: 0.0
Step: 10560, train/grad_norm: 1.4500245015369728e-05
Step: 10560, train/learning_rate: 8.115182936307974e-06
Step: 10560, train/epoch: 2.5130889415740967
Step: 10570, train/loss: 0.0
Step: 10570, train/grad_norm: 1.74237611645367e-05
Step: 10570, train/learning_rate: 8.075519872363657e-06
Step: 10570, train/epoch: 2.5154688358306885
Step: 10580, train/loss: 0.0
Step: 10580, train/grad_norm: 3.994915005023358e-06
Step: 10580, train/learning_rate: 8.035855898924638e-06
Step: 10580, train/epoch: 2.5178487300872803
Step: 10590, train/loss: 0.0
Step: 10590, train/grad_norm: 9.133829735219479e-06
Step: 10590, train/learning_rate: 7.996191925485618e-06
Step: 10590, train/epoch: 2.520228385925293
Step: 10600, train/loss: 0.0
Step: 10600, train/grad_norm: 3.2708101116440957e-06
Step: 10600, train/learning_rate: 7.956528861541301e-06
Step: 10600, train/epoch: 2.5226082801818848
Step: 10610, train/loss: 0.0
Step: 10610, train/grad_norm: 1.8364982679486275e-05
Step: 10610, train/learning_rate: 7.916864888102282e-06
Step: 10610, train/epoch: 2.5249881744384766
Step: 10620, train/loss: 0.0
Step: 10620, train/grad_norm: 3.7536481158895185e-06
Step: 10620, train/learning_rate: 7.877200914663263e-06
Step: 10620, train/epoch: 2.5273678302764893
Step: 10630, train/loss: 0.0
Step: 10630, train/grad_norm: 9.046298146131448e-06
Step: 10630, train/learning_rate: 7.837537850718945e-06
Step: 10630, train/epoch: 2.529747724533081
Step: 10640, train/loss: 0.0
Step: 10640, train/grad_norm: 2.9847036785213277e-05
Step: 10640, train/learning_rate: 7.797873877279926e-06
Step: 10640, train/epoch: 2.532127618789673
Step: 10650, train/loss: 0.0
Step: 10650, train/grad_norm: 4.2745341488625854e-06
Step: 10650, train/learning_rate: 7.758210813335609e-06
Step: 10650, train/epoch: 2.5345072746276855
Step: 10660, train/loss: 0.0
Step: 10660, train/grad_norm: 7.677048415644094e-06
Step: 10660, train/learning_rate: 7.71854683989659e-06
Step: 10660, train/epoch: 2.5368871688842773
Step: 10670, train/loss: 0.0
Step: 10670, train/grad_norm: 1.4213008398655802e-05
Step: 10670, train/learning_rate: 7.67888286645757e-06
Step: 10670, train/epoch: 2.539267063140869
Step: 10680, train/loss: 0.0
Step: 10680, train/grad_norm: 1.8256598195875995e-05
Step: 10680, train/learning_rate: 7.639219802513253e-06
Step: 10680, train/epoch: 2.541646718978882
Step: 10690, train/loss: 0.0
Step: 10690, train/grad_norm: 7.473262485291343e-06
Step: 10690, train/learning_rate: 7.599555829074234e-06
Step: 10690, train/epoch: 2.5440266132354736
Step: 10700, train/loss: 0.0
Step: 10700, train/grad_norm: 8.203994184441399e-06
Step: 10700, train/learning_rate: 7.5598923103825655e-06
Step: 10700, train/epoch: 2.5464065074920654
Step: 10710, train/loss: 0.0
Step: 10710, train/grad_norm: 1.20201793833985e-05
Step: 10710, train/learning_rate: 7.520228336943546e-06
Step: 10710, train/epoch: 2.5487864017486572
Step: 10720, train/loss: 0.0
Step: 10720, train/grad_norm: 1.300138592341682e-06
Step: 10720, train/learning_rate: 7.480564818251878e-06
Step: 10720, train/epoch: 2.55116605758667
Step: 10730, train/loss: 0.0
Step: 10730, train/grad_norm: 0.0003960760368499905
Step: 10730, train/learning_rate: 7.44090129956021e-06
Step: 10730, train/epoch: 2.5535459518432617
Step: 10740, train/loss: 0.0
Step: 10740, train/grad_norm: 0.00015679550415370613
Step: 10740, train/learning_rate: 7.4012373261211906e-06
Step: 10740, train/epoch: 2.5559258460998535
Step: 10750, train/loss: 0.0
Step: 10750, train/grad_norm: 4.070103386766277e-06
Step: 10750, train/learning_rate: 7.361573807429522e-06
Step: 10750, train/epoch: 2.558305501937866
Step: 10760, train/loss: 0.0
Step: 10760, train/grad_norm: 6.999061042733956e-06
Step: 10760, train/learning_rate: 7.321910288737854e-06
Step: 10760, train/epoch: 2.560685396194458
Step: 10770, train/loss: 0.0
Step: 10770, train/grad_norm: 2.2093408915679902e-05
Step: 10770, train/learning_rate: 7.282246770046186e-06
Step: 10770, train/epoch: 2.56306529045105
Step: 10780, train/loss: 0.0
Step: 10780, train/grad_norm: 2.8736999446721256e-09
Step: 10780, train/learning_rate: 7.2425827966071665e-06
Step: 10780, train/epoch: 2.5654449462890625
Step: 10790, train/loss: 0.0
Step: 10790, train/grad_norm: 3.1289491744246334e-05
Step: 10790, train/learning_rate: 7.202919277915498e-06
Step: 10790, train/epoch: 2.5678248405456543
Step: 10800, train/loss: 0.0
Step: 10800, train/grad_norm: 1.2310108559177024e-06
Step: 10800, train/learning_rate: 7.16325575922383e-06
Step: 10800, train/epoch: 2.570204734802246
Step: 10810, train/loss: 0.0
Step: 10810, train/grad_norm: 3.572213927327539e-06
Step: 10810, train/learning_rate: 7.123591785784811e-06
Step: 10810, train/epoch: 2.572584390640259
Step: 10820, train/loss: 0.0
Step: 10820, train/grad_norm: 1.3644847058458254e-05
Step: 10820, train/learning_rate: 7.0839282670931425e-06
Step: 10820, train/epoch: 2.5749642848968506
Step: 10830, train/loss: 0.0
Step: 10830, train/grad_norm: 8.845143383950926e-06
Step: 10830, train/learning_rate: 7.044264748401474e-06
Step: 10830, train/epoch: 2.5773441791534424
Step: 10840, train/loss: 0.0
Step: 10840, train/grad_norm: 5.002059424441541e-06
Step: 10840, train/learning_rate: 7.004600774962455e-06
Step: 10840, train/epoch: 2.579723834991455
Step: 10850, train/loss: 0.0
Step: 10850, train/grad_norm: 3.7351230275817215e-05
Step: 10850, train/learning_rate: 6.964937256270787e-06
Step: 10850, train/epoch: 2.582103729248047
Step: 10860, train/loss: 0.0
Step: 10860, train/grad_norm: 7.089122391334968e-06
Step: 10860, train/learning_rate: 6.9252737375791185e-06
Step: 10860, train/epoch: 2.5844836235046387
Step: 10870, train/loss: 0.0
Step: 10870, train/grad_norm: 7.21228752809111e-06
Step: 10870, train/learning_rate: 6.88561021888745e-06
Step: 10870, train/epoch: 2.5868632793426514
Step: 10880, train/loss: 0.0
Step: 10880, train/grad_norm: 1.2508793588494882e-05
Step: 10880, train/learning_rate: 6.845946245448431e-06
Step: 10880, train/epoch: 2.589243173599243
Step: 10890, train/loss: 0.0
Step: 10890, train/grad_norm: 1.0319356988475192e-05
Step: 10890, train/learning_rate: 6.806282726756763e-06
Step: 10890, train/epoch: 2.591623067855835
Step: 10900, train/loss: 0.0
Step: 10900, train/grad_norm: 5.248638444754761e-06
Step: 10900, train/learning_rate: 6.7666192080650944e-06
Step: 10900, train/epoch: 2.5940029621124268
Step: 10910, train/loss: 0.0
Step: 10910, train/grad_norm: 8.629675676274928e-07
Step: 10910, train/learning_rate: 6.726955234626075e-06
Step: 10910, train/epoch: 2.5963826179504395
Step: 10920, train/loss: 0.0
Step: 10920, train/grad_norm: 4.244067531544715e-05
Step: 10920, train/learning_rate: 6.687291715934407e-06
Step: 10920, train/epoch: 2.5987625122070312
Step: 10930, train/loss: 0.0
Step: 10930, train/grad_norm: 4.237012944940943e-06
Step: 10930, train/learning_rate: 6.647628197242739e-06
Step: 10930, train/epoch: 2.601142406463623
Step: 10940, train/loss: 0.0
Step: 10940, train/grad_norm: 4.220479240757413e-06
Step: 10940, train/learning_rate: 6.60796467855107e-06
Step: 10940, train/epoch: 2.6035220623016357
Step: 10950, train/loss: 0.0
Step: 10950, train/grad_norm: 3.889479557983577e-06
Step: 10950, train/learning_rate: 6.568300705112051e-06
Step: 10950, train/epoch: 2.6059019565582275
Step: 10960, train/loss: 0.0
Step: 10960, train/grad_norm: 3.65254368261958e-06
Step: 10960, train/learning_rate: 6.528637186420383e-06
Step: 10960, train/epoch: 2.6082818508148193
Step: 10970, train/loss: 0.0
Step: 10970, train/grad_norm: 6.4475470935576595e-06
Step: 10970, train/learning_rate: 6.488973667728715e-06
Step: 10970, train/epoch: 2.610661506652832
Step: 10980, train/loss: 0.0
Step: 10980, train/grad_norm: 2.2636560970568098e-05
Step: 10980, train/learning_rate: 6.4493096942896955e-06
Step: 10980, train/epoch: 2.613041400909424
Step: 10990, train/loss: 0.0
Step: 10990, train/grad_norm: 3.1673405374021968e-06
Step: 10990, train/learning_rate: 6.409646175598027e-06
Step: 10990, train/epoch: 2.6154212951660156
Step: 11000, train/loss: 0.0
Step: 11000, train/grad_norm: 8.489625543006696e-06
Step: 11000, train/learning_rate: 6.369982656906359e-06
Step: 11000, train/epoch: 2.6178009510040283
Step: 11010, train/loss: 0.0
Step: 11010, train/grad_norm: 7.197841114248149e-06
Step: 11010, train/learning_rate: 6.33031868346734e-06
Step: 11010, train/epoch: 2.62018084526062
Step: 11020, train/loss: 0.0
Step: 11020, train/grad_norm: 4.427724888955709e-06
Step: 11020, train/learning_rate: 6.2906551647756714e-06
Step: 11020, train/epoch: 2.622560739517212
Step: 11030, train/loss: 0.0
Step: 11030, train/grad_norm: 2.0132347344770096e-06
Step: 11030, train/learning_rate: 6.250991646084003e-06
Step: 11030, train/epoch: 2.6249403953552246
Step: 11040, train/loss: 0.0
Step: 11040, train/grad_norm: 3.726392606040463e-05
Step: 11040, train/learning_rate: 6.211328127392335e-06
Step: 11040, train/epoch: 2.6273202896118164
Step: 11050, train/loss: 0.0
Step: 11050, train/grad_norm: 1.4514060922010685e-06
Step: 11050, train/learning_rate: 6.171664153953316e-06
Step: 11050, train/epoch: 2.629700183868408
Step: 11060, train/loss: 0.0
Step: 11060, train/grad_norm: 7.450153134413995e-06
Step: 11060, train/learning_rate: 6.132000635261647e-06
Step: 11060, train/epoch: 2.632080078125
Step: 11070, train/loss: 0.0
Step: 11070, train/grad_norm: 4.480380539462203e-06
Step: 11070, train/learning_rate: 6.092337116569979e-06
Step: 11070, train/epoch: 2.6344597339630127
Step: 11080, train/loss: 0.0
Step: 11080, train/grad_norm: 9.556881559547037e-05
Step: 11080, train/learning_rate: 6.05267314313096e-06
Step: 11080, train/epoch: 2.6368396282196045
Step: 11090, train/loss: 0.0
Step: 11090, train/grad_norm: 8.038309715630021e-06
Step: 11090, train/learning_rate: 6.013009624439292e-06
Step: 11090, train/epoch: 2.6392195224761963
Step: 11100, train/loss: 0.0
Step: 11100, train/grad_norm: 3.2150408060260816e-06
Step: 11100, train/learning_rate: 5.973346105747623e-06
Step: 11100, train/epoch: 2.641599178314209
Step: 11110, train/loss: 0.0
Step: 11110, train/grad_norm: 1.1106371857749764e-06
Step: 11110, train/learning_rate: 5.933682587055955e-06
Step: 11110, train/epoch: 2.643979072570801
Step: 11120, train/loss: 0.0
Step: 11120, train/grad_norm: 2.1042504158685915e-05
Step: 11120, train/learning_rate: 5.894018613616936e-06
Step: 11120, train/epoch: 2.6463589668273926
Step: 11130, train/loss: 0.0
Step: 11130, train/grad_norm: 2.0217682958900696e-06
Step: 11130, train/learning_rate: 5.854355094925268e-06
Step: 11130, train/epoch: 2.6487386226654053
Step: 11140, train/loss: 0.24379999935626984
Step: 11140, train/grad_norm: 2.387494532740675e-05
Step: 11140, train/learning_rate: 5.814691576233599e-06
Step: 11140, train/epoch: 2.651118516921997
Step: 11150, train/loss: 0.0
Step: 11150, train/grad_norm: 0.0003946165961679071
Step: 11150, train/learning_rate: 5.77502760279458e-06
Step: 11150, train/epoch: 2.653498411178589
Step: 11160, train/loss: 0.0
Step: 11160, train/grad_norm: 1.0159767043660395e-05
Step: 11160, train/learning_rate: 5.735364084102912e-06
Step: 11160, train/epoch: 2.6558780670166016
Step: 11170, train/loss: 0.0
Step: 11170, train/grad_norm: 1.3891265552956611e-05
Step: 11170, train/learning_rate: 5.695700565411244e-06
Step: 11170, train/epoch: 2.6582579612731934
Step: 11180, train/loss: 0.0
Step: 11180, train/grad_norm: 1.7065592601284152e-06
Step: 11180, train/learning_rate: 5.656036591972224e-06
Step: 11180, train/epoch: 2.660637855529785
Step: 11190, train/loss: 0.0
Step: 11190, train/grad_norm: 0.04703811928629875
Step: 11190, train/learning_rate: 5.616373073280556e-06
Step: 11190, train/epoch: 2.663017511367798
Step: 11200, train/loss: 0.0
Step: 11200, train/grad_norm: 9.193481673719361e-06
Step: 11200, train/learning_rate: 5.576709554588888e-06
Step: 11200, train/epoch: 2.6653974056243896
Step: 11210, train/loss: 0.0
Step: 11210, train/grad_norm: 4.5113042688171845e-06
Step: 11210, train/learning_rate: 5.5370460358972196e-06
Step: 11210, train/epoch: 2.6677772998809814
Step: 11220, train/loss: 0.0
Step: 11220, train/grad_norm: 2.011056312767323e-05
Step: 11220, train/learning_rate: 5.4973820624582e-06
Step: 11220, train/epoch: 2.670156955718994
Step: 11230, train/loss: 0.0
Step: 11230, train/grad_norm: 0.0001800102909328416
Step: 11230, train/learning_rate: 5.457718543766532e-06
Step: 11230, train/epoch: 2.672536849975586
Step: 11240, train/loss: 0.0
Step: 11240, train/grad_norm: 0.002098712371662259
Step: 11240, train/learning_rate: 5.418055025074864e-06
Step: 11240, train/epoch: 2.6749167442321777
Step: 11250, train/loss: 0.0
Step: 11250, train/grad_norm: 5.702187627321109e-05
Step: 11250, train/learning_rate: 5.378391051635845e-06
Step: 11250, train/epoch: 2.6772966384887695
Step: 11260, train/loss: 0.0
Step: 11260, train/grad_norm: 5.894266325023878e-11
Step: 11260, train/learning_rate: 5.338727532944176e-06
Step: 11260, train/epoch: 2.6796762943267822
Step: 11270, train/loss: 0.0
Step: 11270, train/grad_norm: 8.991481448283878e-10
Step: 11270, train/learning_rate: 5.299064014252508e-06
Step: 11270, train/epoch: 2.682056188583374
Step: 11280, train/loss: 0.0
Step: 11280, train/grad_norm: 1.581915967108216e-05
Step: 11280, train/learning_rate: 5.25940049556084e-06
Step: 11280, train/epoch: 2.684436082839966
Step: 11290, train/loss: 0.0
Step: 11290, train/grad_norm: 9.571546979714185e-05
Step: 11290, train/learning_rate: 5.219736522121821e-06
Step: 11290, train/epoch: 2.6868157386779785
Step: 11300, train/loss: 0.0
Step: 11300, train/grad_norm: 3.713783735292964e-05
Step: 11300, train/learning_rate: 5.180073003430152e-06
Step: 11300, train/epoch: 2.6891956329345703
Step: 11310, train/loss: 0.0
Step: 11310, train/grad_norm: 0.00012125865032430738
Step: 11310, train/learning_rate: 5.140409484738484e-06
Step: 11310, train/epoch: 2.691575527191162
Step: 11320, train/loss: 0.0
Step: 11320, train/grad_norm: 1.9531675206962973e-05
Step: 11320, train/learning_rate: 5.100745511299465e-06
Step: 11320, train/epoch: 2.693955183029175
Step: 11330, train/loss: 0.0
Step: 11330, train/grad_norm: 6.4995001594070345e-06
Step: 11330, train/learning_rate: 5.0610819926077966e-06
Step: 11330, train/epoch: 2.6963350772857666
Step: 11340, train/loss: 0.0
Step: 11340, train/grad_norm: 6.329647294478491e-05
Step: 11340, train/learning_rate: 5.021418473916128e-06
Step: 11340, train/epoch: 2.6987149715423584
Step: 11350, train/loss: 0.0
Step: 11350, train/grad_norm: 9.297646101913415e-06
Step: 11350, train/learning_rate: 4.981754500477109e-06
Step: 11350, train/epoch: 2.701094627380371
Step: 11360, train/loss: 0.0
Step: 11360, train/grad_norm: 1.432850694982335e-06
Step: 11360, train/learning_rate: 4.942090981785441e-06
Step: 11360, train/epoch: 2.703474521636963
Step: 11370, train/loss: 0.0
Step: 11370, train/grad_norm: 0.00015792466001585126
Step: 11370, train/learning_rate: 4.9024274630937725e-06
Step: 11370, train/epoch: 2.7058544158935547
Step: 11380, train/loss: 0.0
Step: 11380, train/grad_norm: 7.223935654110392e-07
Step: 11380, train/learning_rate: 4.862763944402104e-06
Step: 11380, train/epoch: 2.7082340717315674
Step: 11390, train/loss: 0.0
Step: 11390, train/grad_norm: 3.5159988328814507e-05
Step: 11390, train/learning_rate: 4.823099970963085e-06
Step: 11390, train/epoch: 2.710613965988159
Step: 11400, train/loss: 0.0
Step: 11400, train/grad_norm: 2.5171355446218513e-05
Step: 11400, train/learning_rate: 4.783436452271417e-06
Step: 11400, train/epoch: 2.712993860244751
Step: 11410, train/loss: 0.0
Step: 11410, train/grad_norm: 2.798286368488334e-05
Step: 11410, train/learning_rate: 4.7437729335797485e-06
Step: 11410, train/epoch: 2.7153735160827637
Step: 11420, train/loss: 0.0
Step: 11420, train/grad_norm: 1.5711493688286282e-05
Step: 11420, train/learning_rate: 4.704108960140729e-06
Step: 11420, train/epoch: 2.7177534103393555
Step: 11430, train/loss: 0.0
Step: 11430, train/grad_norm: 5.866817900823662e-06
Step: 11430, train/learning_rate: 4.664445441449061e-06
Step: 11430, train/epoch: 2.7201333045959473
Step: 11440, train/loss: 0.0
Step: 11440, train/grad_norm: 4.789803369931178e-06
Step: 11440, train/learning_rate: 4.624781922757393e-06
Step: 11440, train/epoch: 2.722513198852539
Step: 11450, train/loss: 0.0
Step: 11450, train/grad_norm: 0.0001289117499254644
Step: 11450, train/learning_rate: 4.5851184040657245e-06
Step: 11450, train/epoch: 2.7248928546905518
Step: 11460, train/loss: 0.0
Step: 11460, train/grad_norm: 7.312505658774171e-06
Step: 11460, train/learning_rate: 4.545454430626705e-06
Step: 11460, train/epoch: 2.7272727489471436
Step: 11470, train/loss: 0.0
Step: 11470, train/grad_norm: 1.580660682520829e-05
Step: 11470, train/learning_rate: 4.505790911935037e-06
Step: 11470, train/epoch: 2.7296526432037354
Step: 11480, train/loss: 0.0
Step: 11480, train/grad_norm: 1.4748854482604656e-05
Step: 11480, train/learning_rate: 4.466127393243369e-06
Step: 11480, train/epoch: 2.732032299041748
Step: 11490, train/loss: 0.0
Step: 11490, train/grad_norm: 8.528769285476301e-06
Step: 11490, train/learning_rate: 4.4264634198043495e-06
Step: 11490, train/epoch: 2.73441219329834
Step: 11500, train/loss: 0.0
Step: 11500, train/grad_norm: 5.840317317051813e-05
Step: 11500, train/learning_rate: 4.386799901112681e-06
Step: 11500, train/epoch: 2.7367920875549316
Step: 11510, train/loss: 0.0
Step: 11510, train/grad_norm: 4.4365118810674176e-05
Step: 11510, train/learning_rate: 4.347136382421013e-06
Step: 11510, train/epoch: 2.7391717433929443
Step: 11520, train/loss: 0.0
Step: 11520, train/grad_norm: 0.0002820575318764895
Step: 11520, train/learning_rate: 4.307472408981994e-06
Step: 11520, train/epoch: 2.741551637649536
Step: 11530, train/loss: 0.0
Step: 11530, train/grad_norm: 5.690792022505775e-05
Step: 11530, train/learning_rate: 4.2678088902903255e-06
Step: 11530, train/epoch: 2.743931531906128
Step: 11540, train/loss: 0.0
Step: 11540, train/grad_norm: 0.00022590981097891927
Step: 11540, train/learning_rate: 4.228145371598657e-06
Step: 11540, train/epoch: 2.7463111877441406
Step: 11550, train/loss: 0.0
Step: 11550, train/grad_norm: 4.9171312639373355e-06
Step: 11550, train/learning_rate: 4.188481852906989e-06
Step: 11550, train/epoch: 2.7486910820007324
Step: 11560, train/loss: 0.0
Step: 11560, train/grad_norm: 2.5808063583099283e-05
Step: 11560, train/learning_rate: 4.14881787946797e-06
Step: 11560, train/epoch: 2.751070976257324
Reading events from file: ./praxis-Meta-Llama-3-70B-small-finetune/logs/events.out.tfevents.1718012825.hephaestus.4781.0
Step: 8410, train/loss: 0.0
Step: 8410, train/grad_norm: 8.272678542198264e-09
Step: 8410, train/learning_rate: 1.6642869013594463e-05
Step: 8410, train/epoch: 2.0014278888702393
Step: 8420, train/loss: 0.0
Step: 8420, train/grad_norm: 4.0251356381304504e-07
Step: 8420, train/learning_rate: 1.6603204130660743e-05
Step: 8420, train/epoch: 2.003807783126831
Step: 8430, train/loss: 0.0
Step: 8430, train/grad_norm: 6.601164932362735e-05
Step: 8430, train/learning_rate: 1.6563541066716425e-05
Step: 8430, train/epoch: 2.0061874389648438
Step: 8440, train/loss: 0.0
Step: 8440, train/grad_norm: 8.639522093289997e-06
Step: 8440, train/learning_rate: 1.6523878002772108e-05
Step: 8440, train/epoch: 2.0085673332214355
Step: 8450, train/loss: 0.0
Step: 8450, train/grad_norm: 2.6018406629191304e-07
Step: 8450, train/learning_rate: 1.6484213119838387e-05
Step: 8450, train/epoch: 2.0109472274780273
Step: 8460, train/loss: 0.0
Step: 8460, train/grad_norm: 6.224735926707581e-08
Step: 8460, train/learning_rate: 1.644455005589407e-05
Step: 8460, train/epoch: 2.01332688331604
Step: 8470, train/loss: 0.0
Step: 8470, train/grad_norm: 7.380370448117901e-08
Step: 8470, train/learning_rate: 1.6404886991949752e-05
Step: 8470, train/epoch: 2.015706777572632
Step: 8480, train/loss: 0.0
Step: 8480, train/grad_norm: 8.675490903442551e-07
Step: 8480, train/learning_rate: 1.636522210901603e-05
Step: 8480, train/epoch: 2.0180866718292236
Step: 8490, train/loss: 0.0
Step: 8490, train/grad_norm: 8.253558007709216e-06
Step: 8490, train/learning_rate: 1.6325559045071714e-05
Step: 8490, train/epoch: 2.0204663276672363
Step: 8500, train/loss: 0.0
Step: 8500, train/grad_norm: 3.97643695748684e-08
Step: 8500, train/learning_rate: 1.6285895981127396e-05
Step: 8500, train/epoch: 2.022846221923828
Step: 8510, train/loss: 0.0
Step: 8510, train/grad_norm: 3.9898935710880323e-08
Step: 8510, train/learning_rate: 1.6246231098193675e-05
Step: 8510, train/epoch: 2.02522611618042
Step: 8520, train/loss: 0.0
Step: 8520, train/grad_norm: 8.595552003498597e-08
Step: 8520, train/learning_rate: 1.6206568034249358e-05
Step: 8520, train/epoch: 2.0276060104370117
Step: 8530, train/loss: 0.0
Step: 8530, train/grad_norm: 1.411584804600352e-07
Step: 8530, train/learning_rate: 1.616690497030504e-05
Step: 8530, train/epoch: 2.0299856662750244
Step: 8540, train/loss: 0.0
Step: 8540, train/grad_norm: 3.7522698903558194e-07
Step: 8540, train/learning_rate: 1.612724008737132e-05
Step: 8540, train/epoch: 2.032365560531616
Step: 8550, train/loss: 0.0
Step: 8550, train/grad_norm: 5.268335812047553e-08
Step: 8550, train/learning_rate: 1.6087577023427002e-05
Step: 8550, train/epoch: 2.034745454788208
Step: 8560, train/loss: 0.0
Step: 8560, train/grad_norm: 1.2077687294720363e-08
Step: 8560, train/learning_rate: 1.6047913959482685e-05
Step: 8560, train/epoch: 2.0371251106262207
Step: 8570, train/loss: 0.0
Step: 8570, train/grad_norm: 1.066165054908197e-06
Step: 8570, train/learning_rate: 1.6008250895538367e-05
Step: 8570, train/epoch: 2.0395050048828125
Step: 8580, train/loss: 0.0
Step: 8580, train/grad_norm: 5.808769287796167e-07
Step: 8580, train/learning_rate: 1.5968586012604646e-05
Step: 8580, train/epoch: 2.0418848991394043
Step: 8590, train/loss: 0.0
Step: 8590, train/grad_norm: 1.3087377226383978e-07
Step: 8590, train/learning_rate: 1.592892294866033e-05
Step: 8590, train/epoch: 2.044264554977417
Step: 8600, train/loss: 0.0
Step: 8600, train/grad_norm: 4.280392147393286e-07
Step: 8600, train/learning_rate: 1.588925988471601e-05
Step: 8600, train/epoch: 2.046644449234009
Step: 8610, train/loss: 0.0
Step: 8610, train/grad_norm: 3.2230190072368714e-08
Step: 8610, train/learning_rate: 1.584959500178229e-05
Step: 8610, train/epoch: 2.0490243434906006
Step: 8620, train/loss: 0.0032999999821186066
Step: 8620, train/grad_norm: 1.6503318533978018e-07
Step: 8620, train/learning_rate: 1.5809931937837973e-05
Step: 8620, train/epoch: 2.0514039993286133
Step: 8630, train/loss: 0.0
Step: 8630, train/grad_norm: 0.0008447564905509353
Step: 8630, train/learning_rate: 1.5770268873893656e-05
Step: 8630, train/epoch: 2.053783893585205
Step: 8640, train/loss: 0.0
Step: 8640, train/grad_norm: 4.6658149699396745e-07
Step: 8640, train/learning_rate: 1.5730603990959935e-05
Step: 8640, train/epoch: 2.056163787841797
Step: 8650, train/loss: 0.0
Step: 8650, train/grad_norm: 8.483963824801322e-07
Step: 8650, train/learning_rate: 1.5690940927015617e-05
Step: 8650, train/epoch: 2.0585434436798096
Step: 8660, train/loss: 0.0
Step: 8660, train/grad_norm: 5.651050400956592e-07
Step: 8660, train/learning_rate: 1.56512778630713e-05
Step: 8660, train/epoch: 2.0609233379364014
Step: 8670, train/loss: 0.0
Step: 8670, train/grad_norm: 7.985644856489671e-08
Step: 8670, train/learning_rate: 1.561161298013758e-05
Step: 8670, train/epoch: 2.063303232192993
Step: 8680, train/loss: 0.0
Step: 8680, train/grad_norm: 4.062620973854791e-06
Step: 8680, train/learning_rate: 1.5571949916193262e-05
Step: 8680, train/epoch: 2.065683126449585
Step: 8690, train/loss: 0.0
Step: 8690, train/grad_norm: 3.913785985787399e-05
Step: 8690, train/learning_rate: 1.5532286852248944e-05
Step: 8690, train/epoch: 2.0680627822875977
Step: 8700, train/loss: 0.0
Step: 8700, train/grad_norm: 9.570637757860823e-07
Step: 8700, train/learning_rate: 1.5492621969315223e-05
Step: 8700, train/epoch: 2.0704426765441895
Step: 8710, train/loss: 0.0
Step: 8710, train/grad_norm: 1.7715163380671584e-07
Step: 8710, train/learning_rate: 1.5452958905370906e-05
Step: 8710, train/epoch: 2.0728225708007812
Step: 8720, train/loss: 0.0
Step: 8720, train/grad_norm: 2.427842673569103e-07
Step: 8720, train/learning_rate: 1.541329584142659e-05
Step: 8720, train/epoch: 2.075202226638794
Step: 8730, train/loss: 0.0
Step: 8730, train/grad_norm: 5.1993513494608123e-08
Step: 8730, train/learning_rate: 1.5373630958492868e-05
Step: 8730, train/epoch: 2.0775821208953857
Step: 8740, train/loss: 0.0
Step: 8740, train/grad_norm: 9.960992883861763e-07
Step: 8740, train/learning_rate: 1.533396789454855e-05
Step: 8740, train/epoch: 2.0799620151519775
Step: 8750, train/loss: 0.0
Step: 8750, train/grad_norm: 7.616412176503218e-07
Step: 8750, train/learning_rate: 1.5294304830604233e-05
Step: 8750, train/epoch: 2.0823416709899902
Step: 8760, train/loss: 0.0
Step: 8760, train/grad_norm: 2.2154190446599387e-05
Step: 8760, train/learning_rate: 1.5254640857165214e-05
Step: 8760, train/epoch: 2.084721565246582
Step: 8770, train/loss: 0.0
Step: 8770, train/grad_norm: 2.634573377235938e-07
Step: 8770, train/learning_rate: 1.5214976883726195e-05
Step: 8770, train/epoch: 2.087101459503174
Step: 8780, train/loss: 0.0
Step: 8780, train/grad_norm: 1.997704259792954e-07
Step: 8780, train/learning_rate: 1.5175312910287175e-05
Step: 8780, train/epoch: 2.0894811153411865
Step: 8790, train/loss: 0.0
Step: 8790, train/grad_norm: 2.0003457734674157e-07
Step: 8790, train/learning_rate: 1.5135649846342858e-05
Step: 8790, train/epoch: 2.0918610095977783
Step: 8800, train/loss: 0.0
Step: 8800, train/grad_norm: 1.4279081028689689e-07
Step: 8800, train/learning_rate: 1.5095985872903839e-05
Step: 8800, train/epoch: 2.09424090385437
Step: 8810, train/loss: 0.0
Step: 8810, train/grad_norm: 9.088008482649457e-06
Step: 8810, train/learning_rate: 1.5056322808959521e-05
Step: 8810, train/epoch: 2.096620559692383
Step: 8820, train/loss: 0.0
Step: 8820, train/grad_norm: 2.2365857432760095e-07
Step: 8820, train/learning_rate: 1.5016658835520502e-05
Step: 8820, train/epoch: 2.0990004539489746
Step: 8830, train/loss: 0.0
Step: 8830, train/grad_norm: 7.666905048608896e-07
Step: 8830, train/learning_rate: 1.4976994862081483e-05
Step: 8830, train/epoch: 2.1013803482055664
Step: 8840, train/loss: 0.0
Step: 8840, train/grad_norm: 1.0094266826854437e-06
Step: 8840, train/learning_rate: 1.4937331798137166e-05
Step: 8840, train/epoch: 2.103760004043579
Step: 8850, train/loss: 0.0
Step: 8850, train/grad_norm: 9.782515775214051e-08
Step: 8850, train/learning_rate: 1.4897667824698146e-05
Step: 8850, train/epoch: 2.106139898300171
Step: 8860, train/loss: 0.0
Step: 8860, train/grad_norm: 1.3557651890039324e-08
Step: 8860, train/learning_rate: 1.4858003851259127e-05
Step: 8860, train/epoch: 2.1085197925567627
Step: 8870, train/loss: 0.0
Step: 8870, train/grad_norm: 3.201706277877747e-08
Step: 8870, train/learning_rate: 1.481834078731481e-05
Step: 8870, train/epoch: 2.1108996868133545
Step: 8880, train/loss: 0.0
Step: 8880, train/grad_norm: 1.9453734978469583e-07
Step: 8880, train/learning_rate: 1.477867681387579e-05
Step: 8880, train/epoch: 2.113279342651367
Step: 8890, train/loss: 0.0
Step: 8890, train/grad_norm: 1.901451440744495e-07
Step: 8890, train/learning_rate: 1.4739012840436772e-05
Step: 8890, train/epoch: 2.115659236907959
Step: 8900, train/loss: 0.0
Step: 8900, train/grad_norm: 1.432441877113888e-06
Step: 8900, train/learning_rate: 1.4699349776492454e-05
Step: 8900, train/epoch: 2.118039131164551
Step: 8910, train/loss: 0.0
Step: 8910, train/grad_norm: 2.0605713757504418e-07
Step: 8910, train/learning_rate: 1.4659685803053435e-05
Step: 8910, train/epoch: 2.1204187870025635
Step: 8920, train/loss: 0.0
Step: 8920, train/grad_norm: 1.2152783668284428e-08
Step: 8920, train/learning_rate: 1.4620021829614416e-05
Step: 8920, train/epoch: 2.1227986812591553
Step: 8930, train/loss: 0.0
Step: 8930, train/grad_norm: 2.606211410238757e-06
Step: 8930, train/learning_rate: 1.4580358765670098e-05
Step: 8930, train/epoch: 2.125178575515747
Step: 8940, train/loss: 0.0
Step: 8940, train/grad_norm: 2.5745978859958996e-07
Step: 8940, train/learning_rate: 1.454069479223108e-05
Step: 8940, train/epoch: 2.1275582313537598
Step: 8950, train/loss: 0.0
Step: 8950, train/grad_norm: 2.921966597568826e-07
Step: 8950, train/learning_rate: 1.450103081879206e-05
Step: 8950, train/epoch: 2.1299381256103516
Step: 8960, train/loss: 0.0
Step: 8960, train/grad_norm: 6.907640113240632e-07
Step: 8960, train/learning_rate: 1.4461367754847743e-05
Step: 8960, train/epoch: 2.1323180198669434
Step: 8970, train/loss: 0.0
Step: 8970, train/grad_norm: 3.63175468010013e-06
Step: 8970, train/learning_rate: 1.4421703781408723e-05
Step: 8970, train/epoch: 2.134697675704956
Step: 8980, train/loss: 0.0
Step: 8980, train/grad_norm: 9.91834667729563e-07
Step: 8980, train/learning_rate: 1.4382040717464406e-05
Step: 8980, train/epoch: 2.137077569961548
Step: 8990, train/loss: 0.0
Step: 8990, train/grad_norm: 2.9151072666877553e-08
Step: 8990, train/learning_rate: 1.4342376744025387e-05
Step: 8990, train/epoch: 2.1394574642181396
Step: 9000, train/loss: 0.0
Step: 9000, train/grad_norm: 7.382615194728714e-07
Step: 9000, train/learning_rate: 1.4302712770586368e-05
Step: 9000, train/epoch: 2.1418371200561523
Step: 9010, train/loss: 0.0
Step: 9010, train/grad_norm: 5.474712949649074e-08
Step: 9010, train/learning_rate: 1.426304970664205e-05
Step: 9010, train/epoch: 2.144217014312744
Step: 9020, train/loss: 0.0
Step: 9020, train/grad_norm: 2.7399913960834965e-05
Step: 9020, train/learning_rate: 1.4223385733203031e-05
Step: 9020, train/epoch: 2.146596908569336
Step: 9030, train/loss: 0.0
Step: 9030, train/grad_norm: 1.5844027245748293e-07
Step: 9030, train/learning_rate: 1.4183721759764012e-05
Step: 9030, train/epoch: 2.1489765644073486
Step: 9040, train/loss: 0.0
Step: 9040, train/grad_norm: 6.119792317349493e-08
Step: 9040, train/learning_rate: 1.4144058695819695e-05
Step: 9040, train/epoch: 2.1513564586639404
Step: 9050, train/loss: 0.0
Step: 9050, train/grad_norm: 9.547077297611395e-07
Step: 9050, train/learning_rate: 1.4104394722380675e-05
Step: 9050, train/epoch: 2.1537363529205322
Step: 9060, train/loss: 0.0
Step: 9060, train/grad_norm: 5.886004714739101e-07
Step: 9060, train/learning_rate: 1.4064730748941656e-05
Step: 9060, train/epoch: 2.156116247177124
Step: 9070, train/loss: 0.0
Step: 9070, train/grad_norm: 1.4681022264539934e-07
Step: 9070, train/learning_rate: 1.4025067684997339e-05
Step: 9070, train/epoch: 2.1584959030151367
Step: 9080, train/loss: 0.0
Step: 9080, train/grad_norm: 2.699857759580482e-07
Step: 9080, train/learning_rate: 1.398540371155832e-05
Step: 9080, train/epoch: 2.1608757972717285
Step: 9090, train/loss: 0.0
Step: 9090, train/grad_norm: 1.0442103075547493e-06
Step: 9090, train/learning_rate: 1.39457397381193e-05
Step: 9090, train/epoch: 2.1632556915283203
Step: 9100, train/loss: 0.0
Step: 9100, train/grad_norm: 0.0017103459686040878
Step: 9100, train/learning_rate: 1.3906076674174983e-05
Step: 9100, train/epoch: 2.165635347366333
Step: 9110, train/loss: 0.0
Step: 9110, train/grad_norm: 1.7975933985781012e-07
Step: 9110, train/learning_rate: 1.3866412700735964e-05
Step: 9110, train/epoch: 2.168015241622925
Step: 9120, train/loss: 0.0
Step: 9120, train/grad_norm: 1.3053578413746436e-06
Step: 9120, train/learning_rate: 1.3826748727296945e-05
Step: 9120, train/epoch: 2.1703951358795166
Step: 9130, train/loss: 0.19840000569820404
Step: 9130, train/grad_norm: 34.629249572753906
Step: 9130, train/learning_rate: 1.3787085663352627e-05
Step: 9130, train/epoch: 2.1727747917175293
Step: 9140, train/loss: 0.01360000018030405
Step: 9140, train/grad_norm: 0.25029072165489197
Step: 9140, train/learning_rate: 1.3747421689913608e-05
Step: 9140, train/epoch: 2.175154685974121
Step: 9150, train/loss: 0.0003000000142492354
Step: 9150, train/grad_norm: 0.00012378618703223765
Step: 9150, train/learning_rate: 1.370775862596929e-05
Step: 9150, train/epoch: 2.177534580230713
Step: 9160, train/loss: 0.001500000013038516
Step: 9160, train/grad_norm: 0.000333303032675758
Step: 9160, train/learning_rate: 1.3668094652530272e-05
Step: 9160, train/epoch: 2.1799142360687256
Step: 9170, train/loss: 0.0
Step: 9170, train/grad_norm: 1.8379286075287382e-06
Step: 9170, train/learning_rate: 1.3628430679091252e-05
Step: 9170, train/epoch: 2.1822941303253174
Step: 9180, train/loss: 0.0
Step: 9180, train/grad_norm: 6.787193456148088e-07
Step: 9180, train/learning_rate: 1.3588767615146935e-05
Step: 9180, train/epoch: 2.184674024581909
Step: 9190, train/loss: 0.0
Step: 9190, train/grad_norm: 1.233456032423419e-06
Step: 9190, train/learning_rate: 1.3549103641707916e-05
Step: 9190, train/epoch: 2.187053680419922
Step: 9200, train/loss: 0.0
Step: 9200, train/grad_norm: 2.999808543791005e-07
Step: 9200, train/learning_rate: 1.3509439668268897e-05
Step: 9200, train/epoch: 2.1894335746765137
Step: 9210, train/loss: 0.0
Step: 9210, train/grad_norm: 6.467321895797795e-07
Step: 9210, train/learning_rate: 1.346977660432458e-05
Step: 9210, train/epoch: 2.1918134689331055
Step: 9220, train/loss: 0.0
Step: 9220, train/grad_norm: 1.064537627826212e-06
Step: 9220, train/learning_rate: 1.343011263088556e-05
Step: 9220, train/epoch: 2.194193124771118
Step: 9230, train/loss: 0.0
Step: 9230, train/grad_norm: 2.7263997253612615e-05
Step: 9230, train/learning_rate: 1.3390448657446541e-05
Step: 9230, train/epoch: 2.19657301902771
Step: 9240, train/loss: 0.0
Step: 9240, train/grad_norm: 2.084216390585425e-07
Step: 9240, train/learning_rate: 1.3350785593502223e-05
Step: 9240, train/epoch: 2.1989529132843018
Step: 9250, train/loss: 0.0
Step: 9250, train/grad_norm: 3.205661414540373e-06
Step: 9250, train/learning_rate: 1.3311121620063204e-05
Step: 9250, train/epoch: 2.2013328075408936
Step: 9260, train/loss: 0.0
Step: 9260, train/grad_norm: 1.497843004472088e-05
Step: 9260, train/learning_rate: 1.3271457646624185e-05
Step: 9260, train/epoch: 2.2037124633789062
Step: 9270, train/loss: 0.0
Step: 9270, train/grad_norm: 2.184154901385682e-08
Step: 9270, train/learning_rate: 1.3231794582679868e-05
Step: 9270, train/epoch: 2.206092357635498
Step: 9280, train/loss: 0.07270000129938126
Step: 9280, train/grad_norm: 2.307835984538542e-07
Step: 9280, train/learning_rate: 1.3192130609240849e-05
Step: 9280, train/epoch: 2.20847225189209
Step: 9290, train/loss: 0.0
Step: 9290, train/grad_norm: 5.773244993179105e-07
Step: 9290, train/learning_rate: 1.315246663580183e-05
Step: 9290, train/epoch: 2.2108519077301025
Step: 9300, train/loss: 0.0
Step: 9300, train/grad_norm: 5.510816208698088e-06
Step: 9300, train/learning_rate: 1.3112803571857512e-05
Step: 9300, train/epoch: 2.2132318019866943
Step: 9310, train/loss: 0.0
Step: 9310, train/grad_norm: 5.188687168811157e-07
Step: 9310, train/learning_rate: 1.3073139598418493e-05
Step: 9310, train/epoch: 2.215611696243286
Step: 9320, train/loss: 0.0
Step: 9320, train/grad_norm: 2.7317830131323717e-07
Step: 9320, train/learning_rate: 1.3033476534474175e-05
Step: 9320, train/epoch: 2.217991352081299
Step: 9330, train/loss: 0.0
Step: 9330, train/grad_norm: 3.7951454032736365e-06
Step: 9330, train/learning_rate: 1.2993812561035156e-05
Step: 9330, train/epoch: 2.2203712463378906
Step: 9340, train/loss: 0.0
Step: 9340, train/grad_norm: 4.788518253917573e-06
Step: 9340, train/learning_rate: 1.2954148587596137e-05
Step: 9340, train/epoch: 2.2227511405944824
Step: 9350, train/loss: 0.0
Step: 9350, train/grad_norm: 1.5387269741040654e-05
Step: 9350, train/learning_rate: 1.291448552365182e-05
Step: 9350, train/epoch: 2.225130796432495
Step: 9360, train/loss: 0.0
Step: 9360, train/grad_norm: 4.266761607141234e-06
Step: 9360, train/learning_rate: 1.28748215502128e-05
Step: 9360, train/epoch: 2.227510690689087
Step: 9370, train/loss: 0.0
Step: 9370, train/grad_norm: 2.402491418251884e-06
Step: 9370, train/learning_rate: 1.2835157576773781e-05
Step: 9370, train/epoch: 2.2298905849456787
Step: 9380, train/loss: 0.0
Step: 9380, train/grad_norm: 1.1443446510384092e-06
Step: 9380, train/learning_rate: 1.2795494512829464e-05
Step: 9380, train/epoch: 2.2322702407836914
Step: 9390, train/loss: 0.0
Step: 9390, train/grad_norm: 3.4748057942124433e-07
Step: 9390, train/learning_rate: 1.2755830539390445e-05
Step: 9390, train/epoch: 2.234650135040283
Step: 9400, train/loss: 0.0
Step: 9400, train/grad_norm: 2.894188071422832e-07
Step: 9400, train/learning_rate: 1.2716166565951426e-05
Step: 9400, train/epoch: 2.237030029296875
Step: 9410, train/loss: 0.0
Step: 9410, train/grad_norm: 6.491015938081546e-07
Step: 9410, train/learning_rate: 1.2676503502007108e-05
Step: 9410, train/epoch: 2.239409923553467
Step: 9420, train/loss: 0.0
Step: 9420, train/grad_norm: 3.908580038114451e-05
Step: 9420, train/learning_rate: 1.2636839528568089e-05
Step: 9420, train/epoch: 2.2417895793914795
Step: 9430, train/loss: 0.0
Step: 9430, train/grad_norm: 1.6387421055696905e-06
Step: 9430, train/learning_rate: 1.259717555512907e-05
Step: 9430, train/epoch: 2.2441694736480713
Step: 9440, train/loss: 0.0
Step: 9440, train/grad_norm: 4.692028028330242e-07
Step: 9440, train/learning_rate: 1.2557512491184752e-05
Step: 9440, train/epoch: 2.246549367904663
Step: 9450, train/loss: 0.0
Step: 9450, train/grad_norm: 5.6138185755116865e-05
Step: 9450, train/learning_rate: 1.2517848517745733e-05
Step: 9450, train/epoch: 2.248929023742676
Step: 9460, train/loss: 0.0
Step: 9460, train/grad_norm: 4.090995389560703e-06
Step: 9460, train/learning_rate: 1.2478184544306714e-05
Step: 9460, train/epoch: 2.2513089179992676
Step: 9470, train/loss: 0.06639999896287918
Step: 9470, train/grad_norm: 1.826977751306913e-07
Step: 9470, train/learning_rate: 1.2438521480362397e-05
Step: 9470, train/epoch: 2.2536888122558594
Step: 9480, train/loss: 0.0
Step: 9480, train/grad_norm: 7.759087748127058e-05
Step: 9480, train/learning_rate: 1.2398857506923378e-05
Step: 9480, train/epoch: 2.256068468093872
Step: 9490, train/loss: 0.0
Step: 9490, train/grad_norm: 3.3966030059673358e-06
Step: 9490, train/learning_rate: 1.235919444297906e-05
Step: 9490, train/epoch: 2.258448362350464
Step: 9500, train/loss: 0.0
Step: 9500, train/grad_norm: 7.71327108850528e-07
Step: 9500, train/learning_rate: 1.2319530469540041e-05
Step: 9500, train/epoch: 2.2608282566070557
Step: 9510, train/loss: 0.0
Step: 9510, train/grad_norm: 1.258831161976559e-06
Step: 9510, train/learning_rate: 1.2279866496101022e-05
Step: 9510, train/epoch: 2.2632079124450684
Step: 9520, train/loss: 0.0
Step: 9520, train/grad_norm: 1.3104305480737821e-06
Step: 9520, train/learning_rate: 1.2240203432156704e-05
Step: 9520, train/epoch: 2.26558780670166
Step: 9530, train/loss: 0.0
Step: 9530, train/grad_norm: 3.0764581993025786e-07
Step: 9530, train/learning_rate: 1.2200539458717685e-05
Step: 9530, train/epoch: 2.267967700958252
Step: 9540, train/loss: 0.0
Step: 9540, train/grad_norm: 9.660312798587256e-07
Step: 9540, train/learning_rate: 1.2160875485278666e-05
Step: 9540, train/epoch: 2.2703473567962646
Step: 9550, train/loss: 0.0
Step: 9550, train/grad_norm: 4.661369530367665e-06
Step: 9550, train/learning_rate: 1.2121212421334349e-05
Step: 9550, train/epoch: 2.2727272510528564
Step: 9560, train/loss: 0.0
Step: 9560, train/grad_norm: 1.3558003956859466e-05
Step: 9560, train/learning_rate: 1.208154844789533e-05
Step: 9560, train/epoch: 2.2751071453094482
Step: 9570, train/loss: 0.0
Step: 9570, train/grad_norm: 7.716193067608401e-06
Step: 9570, train/learning_rate: 1.204188447445631e-05
Step: 9570, train/epoch: 2.277486801147461
Step: 9580, train/loss: 0.0
Step: 9580, train/grad_norm: 2.2691501726512797e-05
Step: 9580, train/learning_rate: 1.2002221410511993e-05
Step: 9580, train/epoch: 2.2798666954040527
Step: 9590, train/loss: 0.0
Step: 9590, train/grad_norm: 1.0380680578236934e-05
Step: 9590, train/learning_rate: 1.1962557437072974e-05
Step: 9590, train/epoch: 2.2822465896606445
Step: 9600, train/loss: 0.0
Step: 9600, train/grad_norm: 1.391440127918031e-05
Step: 9600, train/learning_rate: 1.1922893463633955e-05
Step: 9600, train/epoch: 2.2846264839172363
Step: 9610, train/loss: 0.0
Step: 9610, train/grad_norm: 8.852948929494175e-11
Step: 9610, train/learning_rate: 1.1883230399689637e-05
Step: 9610, train/epoch: 2.287006139755249
Step: 9620, train/loss: 0.0
Step: 9620, train/grad_norm: 1.678125227044802e-05
Step: 9620, train/learning_rate: 1.1843566426250618e-05
Step: 9620, train/epoch: 2.289386034011841
Step: 9630, train/loss: 0.0
Step: 9630, train/grad_norm: 0.0002509759506210685
Step: 9630, train/learning_rate: 1.1803902452811599e-05
Step: 9630, train/epoch: 2.2917659282684326
Step: 9640, train/loss: 0.0
Step: 9640, train/grad_norm: 7.505287794629112e-05
Step: 9640, train/learning_rate: 1.1764239388867281e-05
Step: 9640, train/epoch: 2.2941455841064453
Step: 9650, train/loss: 0.0
Step: 9650, train/grad_norm: 3.658377920601197e-07
Step: 9650, train/learning_rate: 1.1724575415428262e-05
Step: 9650, train/epoch: 2.296525478363037
Step: 9660, train/loss: 0.0
Step: 9660, train/grad_norm: 4.662669653043849e-06
Step: 9660, train/learning_rate: 1.1684912351483945e-05
Step: 9660, train/epoch: 2.298905372619629
Step: 9670, train/loss: 0.09019999951124191
Step: 9670, train/grad_norm: 5.344973942555953e-06
Step: 9670, train/learning_rate: 1.1645248378044926e-05
Step: 9670, train/epoch: 2.3012850284576416
Step: 9680, train/loss: 0.0
Step: 9680, train/grad_norm: 5.09535027504171e-07
Step: 9680, train/learning_rate: 1.1605584404605906e-05
Step: 9680, train/epoch: 2.3036649227142334
Step: 9690, train/loss: 0.0
Step: 9690, train/grad_norm: 8.29103373689577e-06
Step: 9690, train/learning_rate: 1.1565921340661589e-05
Step: 9690, train/epoch: 2.306044816970825
Step: 9700, train/loss: 0.0
Step: 9700, train/grad_norm: 2.6523573524173116e-06
Step: 9700, train/learning_rate: 1.152625736722257e-05
Step: 9700, train/epoch: 2.308424472808838
Step: 9710, train/loss: 0.0
Step: 9710, train/grad_norm: 2.67865118530608e-07
Step: 9710, train/learning_rate: 1.148659339378355e-05
Step: 9710, train/epoch: 2.3108043670654297
Step: 9720, train/loss: 9.999999747378752e-05
Step: 9720, train/grad_norm: 7.826833581248138e-08
Step: 9720, train/learning_rate: 1.1446930329839233e-05
Step: 9720, train/epoch: 2.3131842613220215
Step: 9730, train/loss: 0.0
Step: 9730, train/grad_norm: 9.634192110752338e-07
Step: 9730, train/learning_rate: 1.1407266356400214e-05
Step: 9730, train/epoch: 2.315563917160034
Step: 9740, train/loss: 0.0
Step: 9740, train/grad_norm: 3.869255849053843e-08
Step: 9740, train/learning_rate: 1.1367602382961195e-05
Step: 9740, train/epoch: 2.317943811416626
Step: 9750, train/loss: 0.0
Step: 9750, train/grad_norm: 1.9895765944966115e-05
Step: 9750, train/learning_rate: 1.1327939319016878e-05
Step: 9750, train/epoch: 2.3203237056732178
Step: 9760, train/loss: 0.0
Step: 9760, train/grad_norm: 2.171273763451609e-06
Step: 9760, train/learning_rate: 1.1288275345577858e-05
Step: 9760, train/epoch: 2.3227033615112305
Step: 9770, train/loss: 0.0
Step: 9770, train/grad_norm: 5.268548193271272e-07
Step: 9770, train/learning_rate: 1.124861137213884e-05
Step: 9770, train/epoch: 2.3250832557678223
Step: 9780, train/loss: 0.0
Step: 9780, train/grad_norm: 7.332826612582721e-08
Step: 9780, train/learning_rate: 1.1208948308194522e-05
Step: 9780, train/epoch: 2.327463150024414
Step: 9790, train/loss: 0.0
Step: 9790, train/grad_norm: 4.6792050056865264e-07
Step: 9790, train/learning_rate: 1.1169284334755503e-05
Step: 9790, train/epoch: 2.329843044281006
Step: 9800, train/loss: 0.0
Step: 9800, train/grad_norm: 5.5815075228338173e-08
Step: 9800, train/learning_rate: 1.1129620361316483e-05
Step: 9800, train/epoch: 2.3322227001190186
Step: 9810, train/loss: 0.0
Step: 9810, train/grad_norm: 4.587133844324853e-06
Step: 9810, train/learning_rate: 1.1089957297372166e-05
Step: 9810, train/epoch: 2.3346025943756104
Step: 9820, train/loss: 0.0
Step: 9820, train/grad_norm: 5.062134960098774e-07
Step: 9820, train/learning_rate: 1.1050293323933147e-05
Step: 9820, train/epoch: 2.336982488632202
Step: 9830, train/loss: 0.0
Step: 9830, train/grad_norm: 2.27811483455298e-07
Step: 9830, train/learning_rate: 1.101063025998883e-05
Step: 9830, train/epoch: 2.339362144470215
Step: 9840, train/loss: 0.0
Step: 9840, train/grad_norm: 2.6520603569224477e-05
Step: 9840, train/learning_rate: 1.097096628654981e-05
Step: 9840, train/epoch: 2.3417420387268066
Step: 9850, train/loss: 0.0
Step: 9850, train/grad_norm: 5.059769137005787e-07
Step: 9850, train/learning_rate: 1.0931302313110791e-05
Step: 9850, train/epoch: 2.3441219329833984
Step: 9860, train/loss: 0.0
Step: 9860, train/grad_norm: 1.5803807400516234e-06
Step: 9860, train/learning_rate: 1.0891639249166474e-05
Step: 9860, train/epoch: 2.346501588821411
Step: 9870, train/loss: 0.0
Step: 9870, train/grad_norm: 3.005767212016508e-05
Step: 9870, train/learning_rate: 1.0851975275727455e-05
Step: 9870, train/epoch: 2.348881483078003
Step: 9880, train/loss: 0.0
Step: 9880, train/grad_norm: 0.0002520062553230673
Step: 9880, train/learning_rate: 1.0812311302288435e-05
Step: 9880, train/epoch: 2.3512613773345947
Step: 9890, train/loss: 0.0
Step: 9890, train/grad_norm: 6.165467993923812e-07
Step: 9890, train/learning_rate: 1.0772648238344118e-05
Step: 9890, train/epoch: 2.3536410331726074
Step: 9900, train/loss: 0.0
Step: 9900, train/grad_norm: 8.35591364989341e-08
Step: 9900, train/learning_rate: 1.0732984264905099e-05
Step: 9900, train/epoch: 2.356020927429199
Step: 9910, train/loss: 0.0
Step: 9910, train/grad_norm: 1.8694089476412046e-06
Step: 9910, train/learning_rate: 1.069332029146608e-05
Step: 9910, train/epoch: 2.358400821685791
Step: 9920, train/loss: 0.0
Step: 9920, train/grad_norm: 5.361815738069708e-07
Step: 9920, train/learning_rate: 1.0653657227521762e-05
Step: 9920, train/epoch: 2.3607804775238037
Step: 9930, train/loss: 0.0
Step: 9930, train/grad_norm: 3.6210478810971836e-06
Step: 9930, train/learning_rate: 1.0613993254082743e-05
Step: 9930, train/epoch: 2.3631603717803955
Step: 9940, train/loss: 0.0
Step: 9940, train/grad_norm: 5.213004624238238e-05
Step: 9940, train/learning_rate: 1.0574329280643724e-05
Step: 9940, train/epoch: 2.3655402660369873
Step: 9950, train/loss: 0.0
Step: 9950, train/grad_norm: 7.039316642476479e-06
Step: 9950, train/learning_rate: 1.0534666216699407e-05
Step: 9950, train/epoch: 2.367919921875
Step: 9960, train/loss: 0.0
Step: 9960, train/grad_norm: 2.991826519860297e-08
Step: 9960, train/learning_rate: 1.0495002243260387e-05
Step: 9960, train/epoch: 2.370299816131592
Step: 9970, train/loss: 0.0
Step: 9970, train/grad_norm: 1.3078745553229965e-07
Step: 9970, train/learning_rate: 1.045533917931607e-05
Step: 9970, train/epoch: 2.3726797103881836
Step: 9980, train/loss: 0.0
Step: 9980, train/grad_norm: 3.6779306356038433e-06
Step: 9980, train/learning_rate: 1.041567520587705e-05
Step: 9980, train/epoch: 2.3750596046447754
Step: 9990, train/loss: 0.0
Step: 9990, train/grad_norm: 2.8324691356829135e-06
Step: 9990, train/learning_rate: 1.0376011232438032e-05
Step: 9990, train/epoch: 2.377439260482788
Step: 10000, train/loss: 0.0
Step: 10000, train/grad_norm: 6.482704293375718e-07
Step: 10000, train/learning_rate: 1.0336348168493714e-05
Step: 10000, train/epoch: 2.37981915473938
Step: 10010, train/loss: 0.0
Step: 10010, train/grad_norm: 4.652980862829281e-07
Step: 10010, train/learning_rate: 1.0296684195054695e-05
Step: 10010, train/epoch: 2.3821990489959717
Step: 10020, train/loss: 0.0
Step: 10020, train/grad_norm: 7.820308383088559e-07
Step: 10020, train/learning_rate: 1.0257020221615676e-05
Step: 10020, train/epoch: 2.3845787048339844
Step: 10030, train/loss: 0.0
Step: 10030, train/grad_norm: 1.096255004995328e-06
Step: 10030, train/learning_rate: 1.0217357157671358e-05
Step: 10030, train/epoch: 2.386958599090576
Step: 10040, train/loss: 0.0
Step: 10040, train/grad_norm: 0.0016773123061284423
Step: 10040, train/learning_rate: 1.017769318423234e-05
Step: 10040, train/epoch: 2.389338493347168
Step: 10050, train/loss: 0.0
Step: 10050, train/grad_norm: 3.384986939636292e-07
Step: 10050, train/learning_rate: 1.013802921079332e-05
Step: 10050, train/epoch: 2.3917181491851807
Step: 10060, train/loss: 0.0
Step: 10060, train/grad_norm: 2.0152497199887875e-06
Step: 10060, train/learning_rate: 1.0098366146849003e-05
Step: 10060, train/epoch: 2.3940980434417725
Step: 10070, train/loss: 0.0
Step: 10070, train/grad_norm: 1.8408522350910062e-07
Step: 10070, train/learning_rate: 1.0058702173409984e-05
Step: 10070, train/epoch: 2.3964779376983643
Step: 10080, train/loss: 0.0
Step: 10080, train/grad_norm: 2.6922333518086816e-07
Step: 10080, train/learning_rate: 1.0019038199970964e-05
Step: 10080, train/epoch: 2.398857593536377
Step: 10090, train/loss: 0.0
Step: 10090, train/grad_norm: 1.8346239016864274e-07
Step: 10090, train/learning_rate: 9.979375136026647e-06
Step: 10090, train/epoch: 2.4012374877929688
Step: 10100, train/loss: 0.0
Step: 10100, train/grad_norm: 1.000929387373617e-06
Step: 10100, train/learning_rate: 9.939711162587628e-06
Step: 10100, train/epoch: 2.4036173820495605
Step: 10110, train/loss: 0.0
Step: 10110, train/grad_norm: 2.557783034262684e-07
Step: 10110, train/learning_rate: 9.900047189148609e-06
Step: 10110, train/epoch: 2.4059970378875732
Step: 10120, train/loss: 0.0
Step: 10120, train/grad_norm: 7.102478775777854e-06
Step: 10120, train/learning_rate: 9.860384125204291e-06
Step: 10120, train/epoch: 2.408376932144165
Step: 10130, train/loss: 0.0
Step: 10130, train/grad_norm: 1.5277493048415636e-06
Step: 10130, train/learning_rate: 9.820720151765272e-06
Step: 10130, train/epoch: 2.410756826400757
Step: 10140, train/loss: 0.0
Step: 10140, train/grad_norm: 4.4590109382625087e-07
Step: 10140, train/learning_rate: 9.781057087820955e-06
Step: 10140, train/epoch: 2.4131367206573486
Step: 10150, train/loss: 0.0
Step: 10150, train/grad_norm: 2.444847950755502e-07
Step: 10150, train/learning_rate: 9.741393114381935e-06
Step: 10150, train/epoch: 2.4155163764953613
Step: 10160, train/loss: 0.0
Step: 10160, train/grad_norm: 1.8501650345115195e-07
Step: 10160, train/learning_rate: 9.701729140942916e-06
Step: 10160, train/epoch: 2.417896270751953
Step: 10170, train/loss: 0.0
Step: 10170, train/grad_norm: 1.0219143860012991e-06
Step: 10170, train/learning_rate: 9.662066076998599e-06
Step: 10170, train/epoch: 2.420276165008545
Step: 10180, train/loss: 0.0
Step: 10180, train/grad_norm: 1.2140411342898005e-07
Step: 10180, train/learning_rate: 9.62240210355958e-06
Step: 10180, train/epoch: 2.4226558208465576
Step: 10190, train/loss: 0.0
Step: 10190, train/grad_norm: 0.000639421574305743
Step: 10190, train/learning_rate: 9.58273813012056e-06
Step: 10190, train/epoch: 2.4250357151031494
Step: 10200, train/loss: 0.0
Step: 10200, train/grad_norm: 9.104043101615389e-07
Step: 10200, train/learning_rate: 9.543075066176243e-06
Step: 10200, train/epoch: 2.427415609359741
Step: 10210, train/loss: 0.0
Step: 10210, train/grad_norm: 1.2311015780142043e-05
Step: 10210, train/learning_rate: 9.503411092737224e-06
Step: 10210, train/epoch: 2.429795265197754
Step: 10220, train/loss: 0.0
Step: 10220, train/grad_norm: 3.8063133160903817e-06
Step: 10220, train/learning_rate: 9.463747119298205e-06
Step: 10220, train/epoch: 2.4321751594543457
Step: 10230, train/loss: 0.0
Step: 10230, train/grad_norm: 5.879660420760047e-06
Step: 10230, train/learning_rate: 9.424084055353887e-06
Step: 10230, train/epoch: 2.4345550537109375
Step: 10240, train/loss: 0.0
Step: 10240, train/grad_norm: 5.565618721448118e-06
Step: 10240, train/learning_rate: 9.384420081914868e-06
Step: 10240, train/epoch: 2.43693470954895
Step: 10250, train/loss: 0.0
Step: 10250, train/grad_norm: 0.006035094149410725
Step: 10250, train/learning_rate: 9.344756108475849e-06
Step: 10250, train/epoch: 2.439314603805542
Step: 10260, train/loss: 0.0
Step: 10260, train/grad_norm: 0.0002137732517439872
Step: 10260, train/learning_rate: 9.305093044531532e-06
Step: 10260, train/epoch: 2.441694498062134
Step: 10270, train/loss: 0.0
Step: 10270, train/grad_norm: 0.00021794912754558027
Step: 10270, train/learning_rate: 9.265429071092512e-06
Step: 10270, train/epoch: 2.4440741539001465
Step: 10280, train/loss: 0.0
Step: 10280, train/grad_norm: 4.948813980831801e-08
Step: 10280, train/learning_rate: 9.225765097653493e-06
Step: 10280, train/epoch: 2.4464540481567383
Step: 10290, train/loss: 0.0
Step: 10290, train/grad_norm: 2.3008040272998187e-07
Step: 10290, train/learning_rate: 9.186102033709176e-06
Step: 10290, train/epoch: 2.44883394241333
Step: 10300, train/loss: 0.0
Step: 10300, train/grad_norm: 1.4365002698468743e-07
Step: 10300, train/learning_rate: 9.146438060270157e-06
Step: 10300, train/epoch: 2.4512135982513428
Step: 10310, train/loss: 0.0
Step: 10310, train/grad_norm: 1.3414451416338125e-07
Step: 10310, train/learning_rate: 9.10677499632584e-06
Step: 10310, train/epoch: 2.4535934925079346
Step: 10320, train/loss: 0.0
Step: 10320, train/grad_norm: 1.3607226492240443e-06
Step: 10320, train/learning_rate: 9.06711102288682e-06
Step: 10320, train/epoch: 2.4559733867645264
Step: 10330, train/loss: 0.0
Step: 10330, train/grad_norm: 5.219321337790461e-06
Step: 10330, train/learning_rate: 9.027447049447801e-06
Step: 10330, train/epoch: 2.458353281021118
Step: 10340, train/loss: 0.0
Step: 10340, train/grad_norm: 3.5169216516806046e-07
Step: 10340, train/learning_rate: 8.987783985503484e-06
Step: 10340, train/epoch: 2.460732936859131
Step: 10350, train/loss: 0.0
Step: 10350, train/grad_norm: 1.0125093012902653e-06
Step: 10350, train/learning_rate: 8.948120012064464e-06
Step: 10350, train/epoch: 2.4631128311157227
Step: 10360, train/loss: 0.0
Step: 10360, train/grad_norm: 2.539302101922658e-07
Step: 10360, train/learning_rate: 8.908456038625445e-06
Step: 10360, train/epoch: 2.4654927253723145
Step: 10370, train/loss: 0.0
Step: 10370, train/grad_norm: 4.936830464430386e-06
Step: 10370, train/learning_rate: 8.868792974681128e-06
Step: 10370, train/epoch: 2.467872381210327
Step: 10380, train/loss: 0.0
Step: 10380, train/grad_norm: 1.9772008386098605e-07
Step: 10380, train/learning_rate: 8.829129001242109e-06
Step: 10380, train/epoch: 2.470252275466919
Step: 10390, train/loss: 0.0
Step: 10390, train/grad_norm: 1.6146250345627777e-06
Step: 10390, train/learning_rate: 8.78946502780309e-06
Step: 10390, train/epoch: 2.4726321697235107
Step: 10400, train/loss: 0.0
Step: 10400, train/grad_norm: 1.5228989695970085e-06
Step: 10400, train/learning_rate: 8.749801963858772e-06
Step: 10400, train/epoch: 2.4750118255615234
Step: 10410, train/loss: 0.0
Step: 10410, train/grad_norm: 8.657108082843479e-06
Step: 10410, train/learning_rate: 8.710137990419753e-06
Step: 10410, train/epoch: 2.4773917198181152
Step: 10420, train/loss: 0.0
Step: 10420, train/grad_norm: 2.844574886751161e-08
Step: 10420, train/learning_rate: 8.670474016980734e-06
Step: 10420, train/epoch: 2.479771614074707
Step: 10430, train/loss: 0.0
Step: 10430, train/grad_norm: 1.3841111012879992e-06
Step: 10430, train/learning_rate: 8.630810953036416e-06
Step: 10430, train/epoch: 2.4821512699127197
Step: 10440, train/loss: 0.0
Step: 10440, train/grad_norm: 3.4282702472410165e-06
Step: 10440, train/learning_rate: 8.591146979597397e-06
Step: 10440, train/epoch: 2.4845311641693115
Step: 10450, train/loss: 0.0
Step: 10450, train/grad_norm: 2.5465351427556016e-07
Step: 10450, train/learning_rate: 8.551483006158378e-06
Step: 10450, train/epoch: 2.4869110584259033
Step: 10460, train/loss: 0.0
Step: 10460, train/grad_norm: 4.1829505903479003e-07
Step: 10460, train/learning_rate: 8.51181994221406e-06
Step: 10460, train/epoch: 2.489290714263916
Step: 10470, train/loss: 0.0
Step: 10470, train/grad_norm: 2.0382291040732525e-05
Step: 10470, train/learning_rate: 8.472155968775041e-06
Step: 10470, train/epoch: 2.491670608520508
Step: 10480, train/loss: 0.0
Step: 10480, train/grad_norm: 1.4772244583127758e-07
Step: 10480, train/learning_rate: 8.432492904830724e-06
Step: 10480, train/epoch: 2.4940505027770996
Step: 10490, train/loss: 0.0
Step: 10490, train/grad_norm: 7.469992624464794e-08
Step: 10490, train/learning_rate: 8.392828931391705e-06
Step: 10490, train/epoch: 2.4964301586151123
Step: 10500, train/loss: 0.0
Step: 10500, train/grad_norm: 1.5229629752866458e-06
Step: 10500, train/learning_rate: 8.353164957952686e-06
Step: 10500, train/epoch: 2.498810052871704
Step: 10510, train/loss: 0.0
Step: 10510, train/grad_norm: 5.297832217365794e-07
Step: 10510, train/learning_rate: 8.313501894008368e-06
Step: 10510, train/epoch: 2.501189947128296
Step: 10520, train/loss: 0.0
Step: 10520, train/grad_norm: 2.8621462888622773e-07
Step: 10520, train/learning_rate: 8.273837920569349e-06
Step: 10520, train/epoch: 2.5035698413848877
Step: 10530, train/loss: 0.0
Step: 10530, train/grad_norm: 7.353266369136691e-07
Step: 10530, train/learning_rate: 8.23417394713033e-06
Step: 10530, train/epoch: 2.5059494972229004
Step: 10540, train/loss: 0.0
Step: 10540, train/grad_norm: 8.502826176481904e-07
Step: 10540, train/learning_rate: 8.194510883186013e-06
Step: 10540, train/epoch: 2.508329391479492
Step: 10550, train/loss: 0.0
Step: 10550, train/grad_norm: 2.918567645338044e-07
Step: 10550, train/learning_rate: 8.154846909746993e-06
Step: 10550, train/epoch: 2.510709285736084
Step: 10560, train/loss: 0.0
Step: 10560, train/grad_norm: 1.2936991424794542e-06
Step: 10560, train/learning_rate: 8.115182936307974e-06
Step: 10560, train/epoch: 2.5130889415740967
Step: 10570, train/loss: 0.0
Step: 10570, train/grad_norm: 1.3533165201806696e-06
Step: 10570, train/learning_rate: 8.075519872363657e-06
Step: 10570, train/epoch: 2.5154688358306885
Step: 10580, train/loss: 0.0
Step: 10580, train/grad_norm: 1.9227012160172308e-07
Step: 10580, train/learning_rate: 8.035855898924638e-06
Step: 10580, train/epoch: 2.5178487300872803
Step: 10590, train/loss: 0.0
Step: 10590, train/grad_norm: 5.580853894571192e-07
Step: 10590, train/learning_rate: 7.996191925485618e-06
Step: 10590, train/epoch: 2.520228385925293
Step: 10600, train/loss: 0.0
Step: 10600, train/grad_norm: 1.5223930915908568e-07
Step: 10600, train/learning_rate: 7.956528861541301e-06
Step: 10600, train/epoch: 2.5226082801818848
Step: 10610, train/loss: 0.0
Step: 10610, train/grad_norm: 3.6252820336812874e-06
Step: 10610, train/learning_rate: 7.916864888102282e-06
Step: 10610, train/epoch: 2.5249881744384766
Step: 10620, train/loss: 0.0
Step: 10620, train/grad_norm: 2.769223215182137e-07
Step: 10620, train/learning_rate: 7.877200914663263e-06
Step: 10620, train/epoch: 2.5273678302764893
Step: 10630, train/loss: 0.0
Step: 10630, train/grad_norm: 5.479060405377822e-07
Step: 10630, train/learning_rate: 7.837537850718945e-06
Step: 10630, train/epoch: 2.529747724533081
Step: 10640, train/loss: 0.004800000227987766
Step: 10640, train/grad_norm: 3.6123840345680946e-06
Step: 10640, train/learning_rate: 7.797873877279926e-06
Step: 10640, train/epoch: 2.532127618789673
Step: 10650, train/loss: 0.0
Step: 10650, train/grad_norm: 7.888422146606899e-07
Step: 10650, train/learning_rate: 7.758210813335609e-06
Step: 10650, train/epoch: 2.5345072746276855
Step: 10660, train/loss: 0.0
Step: 10660, train/grad_norm: 5.239828055891849e-07
Step: 10660, train/learning_rate: 7.71854683989659e-06
Step: 10660, train/epoch: 2.5368871688842773
Step: 10670, train/loss: 0.0
Step: 10670, train/grad_norm: 1.0192672306175155e-07
Step: 10670, train/learning_rate: 7.67888286645757e-06
Step: 10670, train/epoch: 2.539267063140869
Step: 10680, train/loss: 0.0
Step: 10680, train/grad_norm: 1.3229681599113974e-06
Step: 10680, train/learning_rate: 7.639219802513253e-06
Step: 10680, train/epoch: 2.541646718978882
Step: 10690, train/loss: 0.0
Step: 10690, train/grad_norm: 4.755667646350048e-07
Step: 10690, train/learning_rate: 7.599555829074234e-06
Step: 10690, train/epoch: 2.5440266132354736
Step: 10700, train/loss: 0.0
Step: 10700, train/grad_norm: 7.881952797106351e-07
Step: 10700, train/learning_rate: 7.5598923103825655e-06
Step: 10700, train/epoch: 2.5464065074920654
Step: 10710, train/loss: 0.0
Step: 10710, train/grad_norm: 1.2380218095131568e-06
Step: 10710, train/learning_rate: 7.520228336943546e-06
Step: 10710, train/epoch: 2.5487864017486572
Step: 10720, train/loss: 0.0
Step: 10720, train/grad_norm: 1.6336062458321976e-07
Step: 10720, train/learning_rate: 7.480564818251878e-06
Step: 10720, train/epoch: 2.55116605758667
Step: 10730, train/loss: 0.0
Step: 10730, train/grad_norm: 0.0008340034983120859
Step: 10730, train/learning_rate: 7.44090129956021e-06
Step: 10730, train/epoch: 2.5535459518432617
Step: 10740, train/loss: 0.0
Step: 10740, train/grad_norm: 0.0001039263152051717
Step: 10740, train/learning_rate: 7.4012373261211906e-06
Step: 10740, train/epoch: 2.5559258460998535
Step: 10750, train/loss: 0.0
Step: 10750, train/grad_norm: 3.7158989130148257e-07
Step: 10750, train/learning_rate: 7.361573807429522e-06
Step: 10750, train/epoch: 2.558305501937866
Step: 10760, train/loss: 0.0
Step: 10760, train/grad_norm: 5.509511424861557e-07
Step: 10760, train/learning_rate: 7.321910288737854e-06
Step: 10760, train/epoch: 2.560685396194458
Step: 10770, train/loss: 0.0
Step: 10770, train/grad_norm: 0.00012075529957655817
Step: 10770, train/learning_rate: 7.282246770046186e-06
Step: 10770, train/epoch: 2.56306529045105
Step: 10780, train/loss: 0.0
Step: 10780, train/grad_norm: 1.809351668491388e-09
Step: 10780, train/learning_rate: 7.2425827966071665e-06
Step: 10780, train/epoch: 2.5654449462890625
Step: 10790, train/loss: 0.0
Step: 10790, train/grad_norm: 5.031588898418704e-06
Step: 10790, train/learning_rate: 7.202919277915498e-06
Step: 10790, train/epoch: 2.5678248405456543
Step: 10800, train/loss: 0.0
Step: 10800, train/grad_norm: 3.506064061298275e-08
Step: 10800, train/learning_rate: 7.16325575922383e-06
Step: 10800, train/epoch: 2.570204734802246
Step: 10810, train/loss: 0.0
Step: 10810, train/grad_norm: 2.0599939887233631e-07
Step: 10810, train/learning_rate: 7.123591785784811e-06
Step: 10810, train/epoch: 2.572584390640259
Step: 10820, train/loss: 0.0
Step: 10820, train/grad_norm: 1.3292786206875462e-05
Step: 10820, train/learning_rate: 7.0839282670931425e-06
Step: 10820, train/epoch: 2.5749642848968506
Step: 10830, train/loss: 0.0
Step: 10830, train/grad_norm: 6.507860348392569e-07
Step: 10830, train/learning_rate: 7.044264748401474e-06
Step: 10830, train/epoch: 2.5773441791534424
Step: 10840, train/loss: 0.0
Step: 10840, train/grad_norm: 3.336818394927832e-07
Step: 10840, train/learning_rate: 7.004600774962455e-06
Step: 10840, train/epoch: 2.579723834991455
Step: 10850, train/loss: 0.0
Step: 10850, train/grad_norm: 1.045780663844198e-06
Step: 10850, train/learning_rate: 6.964937256270787e-06
Step: 10850, train/epoch: 2.582103729248047
Step: 10860, train/loss: 0.0
Step: 10860, train/grad_norm: 6.00029920860834e-07
Step: 10860, train/learning_rate: 6.9252737375791185e-06
Step: 10860, train/epoch: 2.5844836235046387
Step: 10870, train/loss: 0.0
Step: 10870, train/grad_norm: 3.1646384286432294e-06
Step: 10870, train/learning_rate: 6.88561021888745e-06
Step: 10870, train/epoch: 2.5868632793426514
Step: 10880, train/loss: 0.0
Step: 10880, train/grad_norm: 5.332294676918536e-07
Step: 10880, train/learning_rate: 6.845946245448431e-06
Step: 10880, train/epoch: 2.589243173599243
Step: 10890, train/loss: 9.999999747378752e-05
Step: 10890, train/grad_norm: 4.04497910722057e-07
Step: 10890, train/learning_rate: 6.806282726756763e-06
Step: 10890, train/epoch: 2.591623067855835
Step: 10900, train/loss: 0.0
Step: 10900, train/grad_norm: 2.7579406491895497e-07
Step: 10900, train/learning_rate: 6.7666192080650944e-06
Step: 10900, train/epoch: 2.5940029621124268
Step: 10910, train/loss: 0.0
Step: 10910, train/grad_norm: 4.600281044986332e-08
Step: 10910, train/learning_rate: 6.726955234626075e-06
Step: 10910, train/epoch: 2.5963826179504395
Step: 10920, train/loss: 0.0
Step: 10920, train/grad_norm: 6.151910270091321e-07
Step: 10920, train/learning_rate: 6.687291715934407e-06
Step: 10920, train/epoch: 2.5987625122070312
Step: 10930, train/loss: 0.0
Step: 10930, train/grad_norm: 4.397153361423989e-07
Step: 10930, train/learning_rate: 6.647628197242739e-06
Step: 10930, train/epoch: 2.601142406463623
Step: 10940, train/loss: 0.0
Step: 10940, train/grad_norm: 3.622941733283369e-07
Step: 10940, train/learning_rate: 6.60796467855107e-06
Step: 10940, train/epoch: 2.6035220623016357
Step: 10950, train/loss: 0.0
Step: 10950, train/grad_norm: 1.6635868860248593e-06
Step: 10950, train/learning_rate: 6.568300705112051e-06
Step: 10950, train/epoch: 2.6059019565582275
Step: 10960, train/loss: 0.0
Step: 10960, train/grad_norm: 3.857084323044546e-07
Step: 10960, train/learning_rate: 6.528637186420383e-06
Step: 10960, train/epoch: 2.6082818508148193
Step: 10970, train/loss: 0.0
Step: 10970, train/grad_norm: 3.0392220651265234e-07
Step: 10970, train/learning_rate: 6.488973667728715e-06
Step: 10970, train/epoch: 2.610661506652832
Step: 10980, train/loss: 0.0
Step: 10980, train/grad_norm: 1.0759842552943155e-05
Step: 10980, train/learning_rate: 6.4493096942896955e-06
Step: 10980, train/epoch: 2.613041400909424
Step: 10990, train/loss: 0.0
Step: 10990, train/grad_norm: 1.4510881385376706e-07
Step: 10990, train/learning_rate: 6.409646175598027e-06
Step: 10990, train/epoch: 2.6154212951660156
Step: 11000, train/loss: 0.0
Step: 11000, train/grad_norm: 1.111689653043868e-05
Step: 11000, train/learning_rate: 6.369982656906359e-06
Step: 11000, train/epoch: 2.6178009510040283
Step: 11010, train/loss: 0.0
Step: 11010, train/grad_norm: 5.385870167629037e-07
Step: 11010, train/learning_rate: 6.33031868346734e-06
Step: 11010, train/epoch: 2.62018084526062
Step: 11020, train/loss: 0.0
Step: 11020, train/grad_norm: 4.5319964669943147e-07
Step: 11020, train/learning_rate: 6.2906551647756714e-06
Step: 11020, train/epoch: 2.622560739517212
Step: 11030, train/loss: 0.0
Step: 11030, train/grad_norm: 1.3579646918060462e-07
Step: 11030, train/learning_rate: 6.250991646084003e-06
Step: 11030, train/epoch: 2.6249403953552246
Step: 11040, train/loss: 0.0
Step: 11040, train/grad_norm: 5.392784714786103e-06
Step: 11040, train/learning_rate: 6.211328127392335e-06
Step: 11040, train/epoch: 2.6273202896118164
Step: 11050, train/loss: 0.0
Step: 11050, train/grad_norm: 9.391683875037415e-08
Step: 11050, train/learning_rate: 6.171664153953316e-06
Step: 11050, train/epoch: 2.629700183868408
Step: 11060, train/loss: 0.0
Step: 11060, train/grad_norm: 7.344280561483174e-07
Step: 11060, train/learning_rate: 6.132000635261647e-06
Step: 11060, train/epoch: 2.632080078125
Step: 11070, train/loss: 0.0
Step: 11070, train/grad_norm: 2.9184991944930516e-05
Step: 11070, train/learning_rate: 6.092337116569979e-06
Step: 11070, train/epoch: 2.6344597339630127
Step: 11080, train/loss: 0.0
Step: 11080, train/grad_norm: 1.1935340808122419e-05
Step: 11080, train/learning_rate: 6.05267314313096e-06
Step: 11080, train/epoch: 2.6368396282196045
Step: 11090, train/loss: 0.0
Step: 11090, train/grad_norm: 6.195123773977684e-07
Step: 11090, train/learning_rate: 6.013009624439292e-06
Step: 11090, train/epoch: 2.6392195224761963
Step: 11100, train/loss: 0.0
Step: 11100, train/grad_norm: 1.0635157821070607e-07
Step: 11100, train/learning_rate: 5.973346105747623e-06
Step: 11100, train/epoch: 2.641599178314209
Step: 11110, train/loss: 0.0
Step: 11110, train/grad_norm: 6.38227248828116e-08
Step: 11110, train/learning_rate: 5.933682587055955e-06
Step: 11110, train/epoch: 2.643979072570801
Step: 11120, train/loss: 0.0
Step: 11120, train/grad_norm: 1.7440153214920429e-06
Step: 11120, train/learning_rate: 5.894018613616936e-06
Step: 11120, train/epoch: 2.6463589668273926
Step: 11130, train/loss: 0.0
Step: 11130, train/grad_norm: 2.99884277410456e-07
Step: 11130, train/learning_rate: 5.854355094925268e-06
Step: 11130, train/epoch: 2.6487386226654053
Step: 11140, train/loss: 0.15000000596046448
Step: 11140, train/grad_norm: 4.0930444811237976e-05
Step: 11140, train/learning_rate: 5.814691576233599e-06
Step: 11140, train/epoch: 2.651118516921997
Step: 11150, train/loss: 0.0
Step: 11150, train/grad_norm: 1.376350724058284e-06
Step: 11150, train/learning_rate: 5.77502760279458e-06
Step: 11150, train/epoch: 2.653498411178589
Step: 11160, train/loss: 0.0
Step: 11160, train/grad_norm: 7.180597094702534e-07
Step: 11160, train/learning_rate: 5.735364084102912e-06
Step: 11160, train/epoch: 2.6558780670166016
Step: 11170, train/loss: 0.0
Step: 11170, train/grad_norm: 3.845794651624601e-07
Step: 11170, train/learning_rate: 5.695700565411244e-06
Step: 11170, train/epoch: 2.6582579612731934
Step: 11180, train/loss: 0.0
Step: 11180, train/grad_norm: 2.8231008286638826e-07
Step: 11180, train/learning_rate: 5.656036591972224e-06
Step: 11180, train/epoch: 2.660637855529785
Step: 11190, train/loss: 0.0
Step: 11190, train/grad_norm: 1.7458217144012451
Step: 11190, train/learning_rate: 5.616373073280556e-06
Step: 11190, train/epoch: 2.663017511367798
Step: 11200, train/loss: 0.0
Step: 11200, train/grad_norm: 4.886341571364028e-07
Step: 11200, train/learning_rate: 5.576709554588888e-06
Step: 11200, train/epoch: 2.6653974056243896
Step: 11210, train/loss: 0.0
Step: 11210, train/grad_norm: 2.412631658899045e-07
Step: 11210, train/learning_rate: 5.5370460358972196e-06
Step: 11210, train/epoch: 2.6677772998809814
Step: 11220, train/loss: 0.0
Step: 11220, train/grad_norm: 4.945158593727683e-07
Step: 11220, train/learning_rate: 5.4973820624582e-06
Step: 11220, train/epoch: 2.670156955718994
Step: 11230, train/loss: 0.0
Step: 11230, train/grad_norm: 1.227905272571661e-06
Step: 11230, train/learning_rate: 5.457718543766532e-06
Step: 11230, train/epoch: 2.672536849975586
Step: 11240, train/loss: 0.0
Step: 11240, train/grad_norm: 3.853615908155916e-06
Step: 11240, train/learning_rate: 5.418055025074864e-06
Step: 11240, train/epoch: 2.6749167442321777
Step: 11250, train/loss: 0.0
Step: 11250, train/grad_norm: 7.637470116605982e-05
Step: 11250, train/learning_rate: 5.378391051635845e-06
Step: 11250, train/epoch: 2.6772966384887695
Step: 11260, train/loss: 0.0
Step: 11260, train/grad_norm: 1.8313872640618456e-10
Step: 11260, train/learning_rate: 5.338727532944176e-06
Step: 11260, train/epoch: 2.6796762943267822
Step: 11270, train/loss: 0.0
Step: 11270, train/grad_norm: 1.2139149685452821e-09
Step: 11270, train/learning_rate: 5.299064014252508e-06
Step: 11270, train/epoch: 2.682056188583374
Step: 11280, train/loss: 0.0
Step: 11280, train/grad_norm: 1.2044091590723838e-06
Step: 11280, train/learning_rate: 5.25940049556084e-06
Step: 11280, train/epoch: 2.684436082839966
Step: 11290, train/loss: 0.0
Step: 11290, train/grad_norm: 0.0007857456803321838
Step: 11290, train/learning_rate: 5.219736522121821e-06
Step: 11290, train/epoch: 2.6868157386779785
Step: 11300, train/loss: 0.0
Step: 11300, train/grad_norm: 0.00042439778917469084
Step: 11300, train/learning_rate: 5.180073003430152e-06
Step: 11300, train/epoch: 2.6891956329345703
Step: 11310, train/loss: 0.0
Step: 11310, train/grad_norm: 0.0014928964665159583
Step: 11310, train/learning_rate: 5.140409484738484e-06
Step: 11310, train/epoch: 2.691575527191162
Step: 11320, train/loss: 0.0
Step: 11320, train/grad_norm: 3.4070128549501533e-06
Step: 11320, train/learning_rate: 5.100745511299465e-06
Step: 11320, train/epoch: 2.693955183029175
Step: 11330, train/loss: 0.0
Step: 11330, train/grad_norm: 4.890596869699948e-07
Step: 11330, train/learning_rate: 5.0610819926077966e-06
Step: 11330, train/epoch: 2.6963350772857666
Step: 11340, train/loss: 0.0
Step: 11340, train/grad_norm: 3.420977634505107e-07
Step: 11340, train/learning_rate: 5.021418473916128e-06
Step: 11340, train/epoch: 2.6987149715423584
Step: 11350, train/loss: 0.0
Step: 11350, train/grad_norm: 6.646662882303644e-07
Step: 11350, train/learning_rate: 4.981754500477109e-06
Step: 11350, train/epoch: 2.701094627380371
Step: 11360, train/loss: 0.0
Step: 11360, train/grad_norm: 1.362605416943552e-06
Step: 11360, train/learning_rate: 4.942090981785441e-06
Step: 11360, train/epoch: 2.703474521636963
Step: 11370, train/loss: 0.0
Step: 11370, train/grad_norm: 0.0007557346834801137
Step: 11370, train/learning_rate: 4.9024274630937725e-06
Step: 11370, train/epoch: 2.7058544158935547
Step: 11380, train/loss: 0.0
Step: 11380, train/grad_norm: 4.20703190684435e-06
Step: 11380, train/learning_rate: 4.862763944402104e-06
Step: 11380, train/epoch: 2.7082340717315674
Step: 11390, train/loss: 0.0
Step: 11390, train/grad_norm: 2.1976720745442435e-05
Step: 11390, train/learning_rate: 4.823099970963085e-06
Step: 11390, train/epoch: 2.710613965988159
Step: 11400, train/loss: 0.0
Step: 11400, train/grad_norm: 4.426957730174763e-06
Step: 11400, train/learning_rate: 4.783436452271417e-06
Step: 11400, train/epoch: 2.712993860244751
Step: 11410, train/loss: 0.0
Step: 11410, train/grad_norm: 2.185718585678842e-06
Step: 11410, train/learning_rate: 4.7437729335797485e-06
Step: 11410, train/epoch: 2.7153735160827637
Step: 11420, train/loss: 0.0
Step: 11420, train/grad_norm: 3.423626822041115e-07
Step: 11420, train/learning_rate: 4.704108960140729e-06
Step: 11420, train/epoch: 2.7177534103393555
Step: 11430, train/loss: 0.0
Step: 11430, train/grad_norm: 4.6793311980763974e-07
Step: 11430, train/learning_rate: 4.664445441449061e-06
Step: 11430, train/epoch: 2.7201333045959473
Step: 11440, train/loss: 0.0
Step: 11440, train/grad_norm: 7.025956847428461e-07
Step: 11440, train/learning_rate: 4.624781922757393e-06
Step: 11440, train/epoch: 2.722513198852539
Step: 11450, train/loss: 0.0
Step: 11450, train/grad_norm: 9.969017264666036e-05
Step: 11450, train/learning_rate: 4.5851184040657245e-06
Step: 11450, train/epoch: 2.7248928546905518
Step: 11460, train/loss: 0.0
Step: 11460, train/grad_norm: 1.0346521776227746e-05
Step: 11460, train/learning_rate: 4.545454430626705e-06
Step: 11460, train/epoch: 2.7272727489471436
Step: 11470, train/loss: 0.0
Step: 11470, train/grad_norm: 0.0033809314481914043
Step: 11470, train/learning_rate: 4.505790911935037e-06
Step: 11470, train/epoch: 2.7296526432037354
Step: 11480, train/loss: 0.0
Step: 11480, train/grad_norm: 5.2972159210185055e-06
Step: 11480, train/learning_rate: 4.466127393243369e-06
Step: 11480, train/epoch: 2.732032299041748
Step: 11490, train/loss: 0.0
Step: 11490, train/grad_norm: 1.3090667607684736e-06
Step: 11490, train/learning_rate: 4.4264634198043495e-06
Step: 11490, train/epoch: 2.73441219329834
Step: 11500, train/loss: 0.0
Step: 11500, train/grad_norm: 2.445046993670985e-05
Step: 11500, train/learning_rate: 4.386799901112681e-06
Step: 11500, train/epoch: 2.7367920875549316
Step: 11510, train/loss: 0.0
Step: 11510, train/grad_norm: 0.00022372719831764698
Step: 11510, train/learning_rate: 4.347136382421013e-06
Step: 11510, train/epoch: 2.7391717433929443
Step: 11520, train/loss: 0.0
Step: 11520, train/grad_norm: 8.026527780202741e-07
Step: 11520, train/learning_rate: 4.307472408981994e-06
Step: 11520, train/epoch: 2.741551637649536
Step: 11530, train/loss: 0.0
Step: 11530, train/grad_norm: 4.686728061642498e-06
Step: 11530, train/learning_rate: 4.2678088902903255e-06
Step: 11530, train/epoch: 2.743931531906128
Step: 11540, train/loss: 0.0
Step: 11540, train/grad_norm: 4.197159796603955e-06
Step: 11540, train/learning_rate: 4.228145371598657e-06
Step: 11540, train/epoch: 2.7463111877441406
Step: 11550, train/loss: 0.0
Step: 11550, train/grad_norm: 5.155433200343396e-07
Step: 11550, train/learning_rate: 4.188481852906989e-06
Step: 11550, train/epoch: 2.7486910820007324
Step: 11560, train/loss: 0.0
Step: 11560, train/grad_norm: 3.6904355056321947e-06
Step: 11560, train/learning_rate: 4.14881787946797e-06
Step: 11560, train/epoch: 2.751070976257324
Step: 11570, train/loss: 0.0
Step: 11570, train/grad_norm: 1.844205144152511e-07
Step: 11570, train/learning_rate: 4.1091543607763015e-06
Step: 11570, train/epoch: 2.753450632095337
Step: 11580, train/loss: 0.0
Step: 11580, train/grad_norm: 1.638622677546664e-07
Step: 11580, train/learning_rate: 4.069490842084633e-06
Step: 11580, train/epoch: 2.7558305263519287
Step: 11590, train/loss: 0.0
Step: 11590, train/grad_norm: 2.8091168502442088e-08
Step: 11590, train/learning_rate: 4.029826868645614e-06
Step: 11590, train/epoch: 2.7582104206085205
Step: 11600, train/loss: 0.0
Step: 11600, train/grad_norm: 7.145136351027759e-06
Step: 11600, train/learning_rate: 3.990163349953946e-06
Step: 11600, train/epoch: 2.760590076446533
Step: 11610, train/loss: 0.0
Step: 11610, train/grad_norm: 9.525886071060086e-07
Step: 11610, train/learning_rate: 3.9504998312622774e-06
Step: 11610, train/epoch: 2.762969970703125
Step: 11620, train/loss: 0.0
Step: 11620, train/grad_norm: 5.72657711472857e-07
Step: 11620, train/learning_rate: 3.910836312570609e-06
Step: 11620, train/epoch: 2.765349864959717
Step: 11630, train/loss: 0.0
Step: 11630, train/grad_norm: 0.0001504120882600546
Step: 11630, train/learning_rate: 3.87117233913159e-06
Step: 11630, train/epoch: 2.7677297592163086
Step: 11640, train/loss: 0.00039999998989515007
Step: 11640, train/grad_norm: 1.5518008922299487e-07
Step: 11640, train/learning_rate: 3.831508820439922e-06
Step: 11640, train/epoch: 2.7701094150543213
Step: 11650, train/loss: 0.0
Step: 11650, train/grad_norm: 2.7731729801416805e-07
Step: 11650, train/learning_rate: 3.791845074374578e-06
Step: 11650, train/epoch: 2.772489309310913
Step: 11660, train/loss: 0.0
Step: 11660, train/grad_norm: 9.9536585196347e-08
Step: 11660, train/learning_rate: 3.7521815556829097e-06
Step: 11660, train/epoch: 2.774869203567505
Step: 11670, train/loss: 0.0
Step: 11670, train/grad_norm: 5.142516101841466e-07
Step: 11670, train/learning_rate: 3.712517809617566e-06
Step: 11670, train/epoch: 2.7772488594055176
Step: 11680, train/loss: 0.0
Step: 11680, train/grad_norm: 2.9513489607779775e-07
Step: 11680, train/learning_rate: 3.6728542909258977e-06
Step: 11680, train/epoch: 2.7796287536621094
Step: 11690, train/loss: 0.0
Step: 11690, train/grad_norm: 7.610855732309574e-07
Step: 11690, train/learning_rate: 3.633190544860554e-06
Step: 11690, train/epoch: 2.782008647918701
Step: 11700, train/loss: 0.0
Step: 11700, train/grad_norm: 3.1813587497708795e-07
Step: 11700, train/learning_rate: 3.59352679879521e-06
Step: 11700, train/epoch: 2.784388303756714
Step: 11710, train/loss: 0.0
Step: 11710, train/grad_norm: 3.643416732757032e-07
Step: 11710, train/learning_rate: 3.553863280103542e-06
Step: 11710, train/epoch: 2.7867681980133057
Step: 11720, train/loss: 0.0
Step: 11720, train/grad_norm: 1.9000402971869335e-05
Step: 11720, train/learning_rate: 3.514199534038198e-06
Step: 11720, train/epoch: 2.7891480922698975
Step: 11730, train/loss: 0.0
Step: 11730, train/grad_norm: 2.8185016276438546e-07
Step: 11730, train/learning_rate: 3.47453601534653e-06
Step: 11730, train/epoch: 2.79152774810791
Step: 11740, train/loss: 0.0
Step: 11740, train/grad_norm: 0.00047660095151513815
Step: 11740, train/learning_rate: 3.434872269281186e-06
Step: 11740, train/epoch: 2.793907642364502
Step: 11750, train/loss: 0.0
Step: 11750, train/grad_norm: 6.385326400959457e-07
Step: 11750, train/learning_rate: 3.3952085232158424e-06
Step: 11750, train/epoch: 2.7962875366210938
Step: 11760, train/loss: 0.00039999998989515007
Step: 11760, train/grad_norm: 7.60764748974907e-08
Step: 11760, train/learning_rate: 3.355545004524174e-06
Step: 11760, train/epoch: 2.7986671924591064
Step: 11770, train/loss: 0.0
Step: 11770, train/grad_norm: 0.0011859434889629483
Step: 11770, train/learning_rate: 3.3158812584588304e-06
Step: 11770, train/epoch: 2.8010470867156982
Step: 11780, train/loss: 0.0
Step: 11780, train/grad_norm: 1.2352078329058713e-06
Step: 11780, train/learning_rate: 3.276217739767162e-06
Step: 11780, train/epoch: 2.80342698097229
Step: 11790, train/loss: 0.0
Step: 11790, train/grad_norm: 3.5456490877550095e-05
Step: 11790, train/learning_rate: 3.2365539937018184e-06
Step: 11790, train/epoch: 2.805806875228882
Step: 11800, train/loss: 0.0
Step: 11800, train/grad_norm: 3.632276275311597e-05
Step: 11800, train/learning_rate: 3.19689047501015e-06
Step: 11800, train/epoch: 2.8081865310668945
Step: 11810, train/loss: 0.0
Step: 11810, train/grad_norm: 4.3913377112403396e-07
Step: 11810, train/learning_rate: 3.1572267289448064e-06
Step: 11810, train/epoch: 2.8105664253234863
Step: 11820, train/loss: 0.0
Step: 11820, train/grad_norm: 1.6842284367157845e-06
Step: 11820, train/learning_rate: 3.1175629828794627e-06
Step: 11820, train/epoch: 2.812946319580078
Step: 11830, train/loss: 0.0
Step: 11830, train/grad_norm: 1.1017327494755591e-07
Step: 11830, train/learning_rate: 3.0778994641877944e-06
Step: 11830, train/epoch: 2.815325975418091
Step: 11840, train/loss: 0.0
Step: 11840, train/grad_norm: 7.346677080022346e-07
Step: 11840, train/learning_rate: 3.0382357181224506e-06
Step: 11840, train/epoch: 2.8177058696746826
Step: 11850, train/loss: 9.999999747378752e-05
Step: 11850, train/grad_norm: 0.0011571015929803252
Step: 11850, train/learning_rate: 2.9985721994307823e-06
Step: 11850, train/epoch: 2.8200857639312744
Step: 11860, train/loss: 0.0
Step: 11860, train/grad_norm: 2.0806435259146383e-06
Step: 11860, train/learning_rate: 2.9589084533654386e-06
Step: 11860, train/epoch: 2.822465419769287
Step: 11870, train/loss: 0.0
Step: 11870, train/grad_norm: 3.3658895404187206e-07
Step: 11870, train/learning_rate: 2.919244707300095e-06
Step: 11870, train/epoch: 2.824845314025879
Step: 11880, train/loss: 0.0
Step: 11880, train/grad_norm: 3.8538109947694466e-06
Step: 11880, train/learning_rate: 2.8795811886084266e-06
Step: 11880, train/epoch: 2.8272252082824707
Step: 11890, train/loss: 0.0
Step: 11890, train/grad_norm: 1.5089924545463873e-06
Step: 11890, train/learning_rate: 2.839917442543083e-06
Step: 11890, train/epoch: 2.8296048641204834
Step: 11900, train/loss: 0.0
Step: 11900, train/grad_norm: 1.531839473045693e-07
Step: 11900, train/learning_rate: 2.8002539238514146e-06
Step: 11900, train/epoch: 2.831984758377075
Step: 11910, train/loss: 0.0
Step: 11910, train/grad_norm: 4.6242573148447264e-07
Step: 11910, train/learning_rate: 2.760590177786071e-06
Step: 11910, train/epoch: 2.834364652633667
Step: 11920, train/loss: 0.0
Step: 11920, train/grad_norm: 5.038629069531453e-08
Step: 11920, train/learning_rate: 2.720926431720727e-06
Step: 11920, train/epoch: 2.8367443084716797
Step: 11930, train/loss: 0.0
Step: 11930, train/grad_norm: 8.66350944761507e-07
Step: 11930, train/learning_rate: 2.681262913029059e-06
Step: 11930, train/epoch: 2.8391242027282715
Step: 11940, train/loss: 0.0
Step: 11940, train/grad_norm: 5.230982225157277e-08
Step: 11940, train/learning_rate: 2.641599166963715e-06
Step: 11940, train/epoch: 2.8415040969848633
Step: 11950, train/loss: 0.0
Step: 11950, train/grad_norm: 3.9747479263496643e-07
Step: 11950, train/learning_rate: 2.601935648272047e-06
Step: 11950, train/epoch: 2.843883752822876
Step: 11960, train/loss: 0.0
Step: 11960, train/grad_norm: 8.202912482602187e-08
Step: 11960, train/learning_rate: 2.562271902206703e-06
Step: 11960, train/epoch: 2.8462636470794678
Step: 11970, train/loss: 0.0
Step: 11970, train/grad_norm: 5.698091740669042e-07
Step: 11970, train/learning_rate: 2.522608383515035e-06
Step: 11970, train/epoch: 2.8486435413360596
Step: 11980, train/loss: 0.0
Step: 11980, train/grad_norm: 7.317132713069441e-07
Step: 11980, train/learning_rate: 2.482944637449691e-06
Step: 11980, train/epoch: 2.8510234355926514
Step: 11990, train/loss: 0.0203000009059906
Step: 11990, train/grad_norm: 3.2125353754963726e-07
Step: 11990, train/learning_rate: 2.4432808913843473e-06
Step: 11990, train/epoch: 2.853403091430664
Step: 12000, train/loss: 0.0
Step: 12000, train/grad_norm: 4.070498562214198e-06
Step: 12000, train/learning_rate: 2.403617372692679e-06
Step: 12000, train/epoch: 2.855782985687256
Step: 12010, train/loss: 0.0
Step: 12010, train/grad_norm: 1.4901834219926968e-06
Step: 12010, train/learning_rate: 2.3639536266273353e-06
Step: 12010, train/epoch: 2.8581628799438477
Step: 12020, train/loss: 0.0
Step: 12020, train/grad_norm: 4.781917596119456e-06
Step: 12020, train/learning_rate: 2.324290107935667e-06
Step: 12020, train/epoch: 2.8605425357818604
Step: 12030, train/loss: 0.0
Step: 12030, train/grad_norm: 4.188827006146312e-06
Step: 12030, train/learning_rate: 2.2846263618703233e-06
Step: 12030, train/epoch: 2.862922430038452
Step: 12040, train/loss: 0.0
Step: 12040, train/grad_norm: 3.773506614379585e-05
Step: 12040, train/learning_rate: 2.2449626158049796e-06
Step: 12040, train/epoch: 2.865302324295044
Step: 12050, train/loss: 0.003800000064074993
Step: 12050, train/grad_norm: 4.288749551051296e-06
Step: 12050, train/learning_rate: 2.2052990971133113e-06
Step: 12050, train/epoch: 2.8676819801330566
Step: 12060, train/loss: 0.0
Step: 12060, train/grad_norm: 3.6098893474445504e-07
Step: 12060, train/learning_rate: 2.1656353510479676e-06
Step: 12060, train/epoch: 2.8700618743896484
Step: 12070, train/loss: 0.0
Step: 12070, train/grad_norm: 1.5633451084795524e-06
Step: 12070, train/learning_rate: 2.1259718323562993e-06
Step: 12070, train/epoch: 2.8724417686462402
Step: 12080, train/loss: 0.0
Step: 12080, train/grad_norm: 2.309685669388273e-06
Step: 12080, train/learning_rate: 2.0863080862909555e-06
Step: 12080, train/epoch: 2.874821424484253
Step: 12090, train/loss: 0.0
Step: 12090, train/grad_norm: 3.3771257790249365e-07
Step: 12090, train/learning_rate: 2.0466445675992873e-06
Step: 12090, train/epoch: 2.8772013187408447
Step: 12100, train/loss: 0.0
Step: 12100, train/grad_norm: 1.1870354228449287e-06
Step: 12100, train/learning_rate: 2.0069808215339435e-06
Step: 12100, train/epoch: 2.8795812129974365
Step: 12110, train/loss: 0.0
Step: 12110, train/grad_norm: 4.090574066140107e-07
Step: 12110, train/learning_rate: 1.9673170754686e-06
Step: 12110, train/epoch: 2.881960868835449
Step: 12120, train/loss: 0.0
Step: 12120, train/grad_norm: 1.737922320899088e-05
Step: 12120, train/learning_rate: 1.9276535567769315e-06
Step: 12120, train/epoch: 2.884340763092041
Step: 12130, train/loss: 0.0
Step: 12130, train/grad_norm: 1.1947897291975096e-05
Step: 12130, train/learning_rate: 1.8879898107115878e-06
Step: 12130, train/epoch: 2.886720657348633
Step: 12140, train/loss: 0.0
Step: 12140, train/grad_norm: 3.3491569411125965e-06
Step: 12140, train/learning_rate: 1.8483261783330818e-06
Step: 12140, train/epoch: 2.8891003131866455
Step: 12150, train/loss: 0.0
Step: 12150, train/grad_norm: 8.956165515883185e-07
Step: 12150, train/learning_rate: 1.8086625459545758e-06
Step: 12150, train/epoch: 2.8914802074432373
Step: 12160, train/loss: 0.0
Step: 12160, train/grad_norm: 5.5407105037375e-07
Step: 12160, train/learning_rate: 1.7689989135760698e-06
Step: 12160, train/epoch: 2.893860101699829
Step: 12170, train/loss: 0.0
Step: 12170, train/grad_norm: 1.9978581633495196e-07
Step: 12170, train/learning_rate: 1.7293352811975637e-06
Step: 12170, train/epoch: 2.896239995956421
Step: 12180, train/loss: 0.0
Step: 12180, train/grad_norm: 8.002926733752247e-06
Step: 12180, train/learning_rate: 1.68967153513222e-06
Step: 12180, train/epoch: 2.8986196517944336
Step: 12190, train/loss: 0.0
Step: 12190, train/grad_norm: 6.522684543597279e-07
Step: 12190, train/learning_rate: 1.650007902753714e-06
Step: 12190, train/epoch: 2.9009995460510254
Step: 12200, train/loss: 0.0
Step: 12200, train/grad_norm: 1.5939214108584565e-06
Step: 12200, train/learning_rate: 1.610344270375208e-06
Step: 12200, train/epoch: 2.903379440307617
Step: 12210, train/loss: 0.0
Step: 12210, train/grad_norm: 6.722066245856695e-06
Step: 12210, train/learning_rate: 1.570680637996702e-06
Step: 12210, train/epoch: 2.90575909614563
Step: 12220, train/loss: 0.0
Step: 12220, train/grad_norm: 4.37842857081705e-07
Step: 12220, train/learning_rate: 1.531017005618196e-06
Step: 12220, train/epoch: 2.9081389904022217
Step: 12230, train/loss: 0.0
Step: 12230, train/grad_norm: 1.5611946935223386e-07
Step: 12230, train/learning_rate: 1.49135337323969e-06
Step: 12230, train/epoch: 2.9105188846588135
Step: 12240, train/loss: 0.0
Step: 12240, train/grad_norm: 2.469224682499771e-07
Step: 12240, train/learning_rate: 1.4516896271743462e-06
Step: 12240, train/epoch: 2.912898540496826
Step: 12250, train/loss: 0.0
Step: 12250, train/grad_norm: 2.7250337097939337e-07
Step: 12250, train/learning_rate: 1.4120259947958402e-06
Step: 12250, train/epoch: 2.915278434753418
Step: 12260, train/loss: 0.0
Step: 12260, train/grad_norm: 8.298639841086697e-06
Step: 12260, train/learning_rate: 1.3723623624173342e-06
Step: 12260, train/epoch: 2.9176583290100098
Step: 12270, train/loss: 0.0
Step: 12270, train/grad_norm: 3.29677982335852e-06
Step: 12270, train/learning_rate: 1.3326987300388282e-06
Step: 12270, train/epoch: 2.9200379848480225
Step: 12280, train/loss: 0.0
Step: 12280, train/grad_norm: 7.999794433999341e-06
Step: 12280, train/learning_rate: 1.2930350976603222e-06
Step: 12280, train/epoch: 2.9224178791046143
Step: 12290, train/loss: 0.0
Step: 12290, train/grad_norm: 3.196610691702517e-07
Step: 12290, train/learning_rate: 1.2533714652818162e-06
Step: 12290, train/epoch: 2.924797773361206
Step: 12300, train/loss: 0.0
Step: 12300, train/grad_norm: 2.3995035007828847e-06
Step: 12300, train/learning_rate: 1.2137077192164725e-06
Step: 12300, train/epoch: 2.9271774291992188
Step: 12310, train/loss: 0.0
Step: 12310, train/grad_norm: 1.8677587831916753e-07
Step: 12310, train/learning_rate: 1.1740440868379665e-06
Step: 12310, train/epoch: 2.9295573234558105
Step: 12320, train/loss: 0.0
Step: 12320, train/grad_norm: 6.0184785979799926e-05
Step: 12320, train/learning_rate: 1.1343804544594605e-06
Step: 12320, train/epoch: 2.9319372177124023
Step: 12330, train/loss: 0.0
Step: 12330, train/grad_norm: 3.7861459531995934e-06
Step: 12330, train/learning_rate: 1.0947168220809544e-06
Step: 12330, train/epoch: 2.934316873550415
Step: 12340, train/loss: 0.0
Step: 12340, train/grad_norm: 7.493522389268037e-07
Step: 12340, train/learning_rate: 1.0550531897024484e-06
Step: 12340, train/epoch: 2.936696767807007
Step: 12350, train/loss: 0.0
Step: 12350, train/grad_norm: 1.3838696304446785e-06
Step: 12350, train/learning_rate: 1.0153894436371047e-06
Step: 12350, train/epoch: 2.9390766620635986
Step: 12360, train/loss: 0.0
Step: 12360, train/grad_norm: 3.143482985024093e-08
Step: 12360, train/learning_rate: 9.757258112585987e-07
Step: 12360, train/epoch: 2.9414565563201904
Step: 12370, train/loss: 0.0
Step: 12370, train/grad_norm: 1.1886666015925584e-06
Step: 12370, train/learning_rate: 9.360621788800927e-07
Step: 12370, train/epoch: 2.943836212158203
Step: 12380, train/loss: 0.0
Step: 12380, train/grad_norm: 5.646356839861255e-06
Step: 12380, train/learning_rate: 8.963985465015867e-07
Step: 12380, train/epoch: 2.946216106414795
Step: 12390, train/loss: 0.0
Step: 12390, train/grad_norm: 8.435185350208485e-07
Step: 12390, train/learning_rate: 8.567349141230807e-07
Step: 12390, train/epoch: 2.9485960006713867
Step: 12400, train/loss: 0.0
Step: 12400, train/grad_norm: 8.905641152523458e-06
Step: 12400, train/learning_rate: 8.170712249011558e-07
Step: 12400, train/epoch: 2.9509756565093994
Step: 12410, train/loss: 0.0
Step: 12410, train/grad_norm: 8.032410505620646e-07
Step: 12410, train/learning_rate: 7.774075925226498e-07
Step: 12410, train/epoch: 2.953355550765991
Step: 12420, train/loss: 0.0
Step: 12420, train/grad_norm: 2.6767934286908712e-06
Step: 12420, train/learning_rate: 7.377439033007249e-07
Step: 12420, train/epoch: 2.955735445022583
Step: 12430, train/loss: 0.0
Step: 12430, train/grad_norm: 7.411311457872216e-07
Step: 12430, train/learning_rate: 6.980802709222189e-07
Step: 12430, train/epoch: 2.9581151008605957
Step: 12440, train/loss: 0.0
Step: 12440, train/grad_norm: 2.405186478426913e-06
Step: 12440, train/learning_rate: 6.584166385437129e-07
Step: 12440, train/epoch: 2.9604949951171875
Step: 12450, train/loss: 0.0
Step: 12450, train/grad_norm: 1.845228614003047e-12
Step: 12450, train/learning_rate: 6.18752949321788e-07
Step: 12450, train/epoch: 2.9628748893737793
Step: 12460, train/loss: 0.0
Step: 12460, train/grad_norm: 2.4618691440991824e-07
Step: 12460, train/learning_rate: 5.79089316943282e-07
Step: 12460, train/epoch: 2.965254545211792
Step: 12470, train/loss: 0.0
Step: 12470, train/grad_norm: 6.315633527265163e-06
Step: 12470, train/learning_rate: 5.39425684564776e-07
Step: 12470, train/epoch: 2.967634439468384
Step: 12480, train/loss: 0.0
Step: 12480, train/grad_norm: 1.4557312510987686e-07
Step: 12480, train/learning_rate: 4.997619953428512e-07
Step: 12480, train/epoch: 2.9700143337249756
Step: 12490, train/loss: 0.0
Step: 12490, train/grad_norm: 1.2879881978733465e-05
Step: 12490, train/learning_rate: 4.6009836296434514e-07
Step: 12490, train/epoch: 2.9723939895629883
Step: 12500, train/loss: 0.0
Step: 12500, train/grad_norm: 8.094503982647439e-07
Step: 12500, train/learning_rate: 4.204347021641297e-07
Step: 12500, train/epoch: 2.97477388381958
Step: 12510, train/loss: 0.0
Step: 12510, train/grad_norm: 3.581974624466966e-06
Step: 12510, train/learning_rate: 3.807710697856237e-07
Step: 12510, train/epoch: 2.977153778076172
Step: 12520, train/loss: 0.0
Step: 12520, train/grad_norm: 2.9830715675416286e-07
Step: 12520, train/learning_rate: 3.4110740898540826e-07
Step: 12520, train/epoch: 2.9795336723327637
Step: 12530, train/loss: 0.0
Step: 12530, train/grad_norm: 2.508977274828794e-07
Step: 12530, train/learning_rate: 3.014437481851928e-07
Step: 12530, train/epoch: 2.9819133281707764
Step: 12540, train/loss: 0.0
Step: 12540, train/grad_norm: 6.362961357808672e-07
Step: 12540, train/learning_rate: 2.617801158066868e-07
Step: 12540, train/epoch: 2.984293222427368
Step: 12550, train/loss: 0.0
Step: 12550, train/grad_norm: 2.932978588887636e-07
Step: 12550, train/learning_rate: 2.2211645500647137e-07
Step: 12550, train/epoch: 2.98667311668396
Step: 12560, train/loss: 0.0
Step: 12560, train/grad_norm: 2.238282650068868e-05
Step: 12560, train/learning_rate: 1.8245279420625593e-07
Step: 12560, train/epoch: 2.9890527725219727
Step: 12570, train/loss: 0.0
Step: 12570, train/grad_norm: 5.440108907350805e-06
Step: 12570, train/learning_rate: 1.427891476168952e-07
Step: 12570, train/epoch: 2.9914326667785645
Step: 12580, train/loss: 0.0
Step: 12580, train/grad_norm: 4.382226325105876e-05
Step: 12580, train/learning_rate: 1.0312549392210713e-07
Step: 12580, train/epoch: 2.9938125610351562
Step: 12590, train/loss: 0.0
Step: 12590, train/grad_norm: 1.015170028040302e-06
Step: 12590, train/learning_rate: 6.346184022731904e-08
Step: 12590, train/epoch: 2.996192216873169
Step: 12600, train/loss: 0.0
Step: 12600, train/grad_norm: 0.001176289631985128
Step: 12600, train/learning_rate: 2.379819186160148e-08
Step: 12600, train/epoch: 2.9985721111297607
Step: 12606, eval/loss: 0.002042477484792471
Step: 12606, eval/accuracy: 0.9997223615646362
Step: 12606, eval/f1: 0.9997068047523499
Step: 12606, eval/runtime: 7988.41259765625
Step: 12606, eval/samples_per_second: 0.9020000100135803
Step: 12606, eval/steps_per_second: 0.11299999803304672
Step: 12606, train/epoch: 3.0
Step: 12606, train/train_runtime: 126447.9609375
Step: 12606, train/train_samples_per_second: 0.796999990940094
Step: 12606, train/train_steps_per_second: 0.10000000149011612
Step: 12606, train/total_flos: 2.1235818045284286e+19
Step: 12606, train/train_loss: 0.0004971224698238075
Step: 12606, train/epoch: 3.0
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=6ffd0164-0f41-4bbe-b905-7dda371dfd20">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[27]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.summary.summary_iterator</span> <span class="kn">import</span> <span class="n">summary_iterator</span>

<span class="n">logs_directory</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">'./'</span><span class="p">,</span> <span class="n">project_name</span><span class="p">,</span> <span class="s1">'logs'</span><span class="p">)</span>
<span class="n">file_pattern</span> <span class="o">=</span> <span class="s1">'events.out.tfevents.*'</span>

<span class="n">event_files</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">logs_directory</span><span class="p">,</span> <span class="n">file_pattern</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">extract_metrics</span><span class="p">(</span><span class="n">event_files</span><span class="p">):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">last_train_loss</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">for</span> <span class="n">event_file</span> <span class="ow">in</span> <span class="n">event_files</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">summary_iterator</span><span class="p">(</span><span class="n">event_file</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">e</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">v</span><span class="o">.</span><span class="n">HasField</span><span class="p">(</span><span class="s1">'simple_value'</span><span class="p">):</span>
                    <span class="n">step</span> <span class="o">=</span> <span class="n">e</span><span class="o">.</span><span class="n">step</span>
                    <span class="n">metric_name</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">'/'</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                    <span class="n">metric_value</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">simple_value</span>

                    <span class="n">formatted_value</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">metric_value</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">"</span>

                    <span class="k">if</span> <span class="s1">'train/loss'</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="p">:</span>
                        <span class="n">last_train_loss</span> <span class="o">=</span> <span class="n">formatted_value</span>

                    <span class="k">if</span> <span class="s1">'eval'</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="p">:</span>
                        <span class="n">entry</span> <span class="o">=</span> <span class="nb">next</span><span class="p">((</span><span class="n">item</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">data</span> <span class="k">if</span> <span class="n">item</span><span class="p">[</span><span class="s1">'Step'</span><span class="p">]</span> <span class="o">==</span> <span class="n">step</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="n">entry</span><span class="p">:</span>
                            <span class="n">entry</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'Step'</span><span class="p">:</span> <span class="n">step</span><span class="p">,</span> <span class="s1">'Train Loss'</span><span class="p">:</span> <span class="n">last_train_loss</span><span class="p">,</span> <span class="s1">'Eval Loss'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">'Accuracy'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">'F1'</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>
                            <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">entry</span><span class="p">)</span>
                        <span class="k">if</span> <span class="s1">'loss'</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="p">:</span>
                            <span class="n">entry</span><span class="p">[</span><span class="s1">'Eval Loss'</span><span class="p">]</span> <span class="o">=</span> <span class="n">formatted_value</span>
                        <span class="k">elif</span> <span class="s1">'accuracy'</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="p">:</span>
                            <span class="n">entry</span><span class="p">[</span><span class="s1">'Accuracy'</span><span class="p">]</span> <span class="o">=</span> <span class="n">formatted_value</span>
                        <span class="k">elif</span> <span class="s1">'f1'</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="p">:</span>
                            <span class="n">entry</span><span class="p">[</span><span class="s1">'F1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">formatted_value</span>

    <span class="k">return</span> <span class="n">data</span>

<span class="n">metrics_data</span> <span class="o">=</span> <span class="n">extract_metrics</span><span class="p">(</span><span class="n">event_files</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">metrics_data</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">'Step'</span><span class="p">)</span>

<span class="n">file_path</span> <span class="o">=</span> <span class="s2">"./images/"</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s2">"_Checkpoint_Data.csv"</span>
<span class="n">df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>    Step Train Loss Eval Loss  Accuracy        F1
0   4202   0.151200  0.091785  0.981674  0.980719
1   8404   0.000000  0.003845  0.999583  0.999560
2  12606   0.000000  0.002042  0.999722  0.999707
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=37cc5f92-d47f-4356-8fad-d9b64b6f5361">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[28]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">fillna</span><span class="p">({</span>
    <span class="s1">'Eval Loss'</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="s1">'inf'</span><span class="p">),</span>
    <span class="s1">'Accuracy'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">'F1'</span><span class="p">:</span> <span class="mi">0</span>
<span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">df</span><span class="p">[</span><span class="s1">'Eval Loss'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Eval Loss'</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'Accuracy'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Accuracy'</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'F1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'F1'</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>

<span class="n">df</span><span class="p">[</span><span class="s1">'Eval Loss Rank'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Eval Loss'</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'Accuracy Rank'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Accuracy'</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'F1 Rank'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'F1'</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">df</span><span class="p">[</span><span class="s1">'Rank Sum'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Eval Loss Rank'</span><span class="p">]</span> <span class="o">+</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Accuracy Rank'</span><span class="p">]</span> <span class="o">+</span> <span class="n">df</span><span class="p">[</span><span class="s1">'F1 Rank'</span><span class="p">]</span>

<span class="n">best_checkpoint</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">'Rank Sum'</span><span class="p">]</span><span class="o">.</span><span class="n">idxmin</span><span class="p">()]</span>

<span class="n">checkpoint_folder_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"checkpoint-</span><span class="si">{</span><span class="n">best_checkpoint</span><span class="p">[</span><span class="s1">'Step'</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Best Checkpoint Step: </span><span class="si">{</span><span class="n">checkpoint_folder_name</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">best_checkpoint</span><span class="p">[[</span><span class="s1">'Step'</span><span class="p">,</span> <span class="s1">'Train Loss'</span><span class="p">,</span> <span class="s1">'Eval Loss'</span><span class="p">,</span> <span class="s1">'Accuracy'</span><span class="p">,</span> <span class="s1">'F1'</span><span class="p">,</span> <span class="s1">'Rank Sum'</span><span class="p">]])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Best Checkpoint Step: checkpoint-12606
Step             12606
Train Loss    0.000000
Eval Loss     0.002042
Accuracy      0.999722
F1            0.999707
Rank Sum           3.0
Name: 2, dtype: object
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=5c3c3999-8c32-4a25-aaca-2ec9036b69ed">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Run-TensorBoard">Run TensorBoard<a class="anchor-link" href="#Run-TensorBoard"></a></h3><p>tensorboard --logdir=~/kuk/Praxis/praxis-Llama-2-7b-hf-small-finetune/logs --host=0.0.0.0</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=f8f36022-dc73-492f-998c-43e5ed1a6f46">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="PAUSE-SCRIPT">PAUSE SCRIPT<a class="anchor-link" href="#PAUSE-SCRIPT"></a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=444ae65b-241c-47ef-bbc2-81f3a2ce50e9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[29]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># My flag to pause the script, set to True to pause</span>
<span class="n">pause_script</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=c8be3672-68e0-44f8-b8b1-31290665658d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[30]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">StopExecution</span><span class="p">(</span><span class="ne">Exception</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">_render_traceback_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Script Paused"</span><span class="p">)</span>
        <span class="k">pass</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=eb3b5ed2-5320-49c0-9e61-90d6f0942f7a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[31]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="n">pause_script</span><span class="p">:</span>
    <span class="k">raise</span> <span class="n">StopExecution</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=19be6e26-d888-4da0-b3f8-836d68ac2051">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Testing">Testing<a class="anchor-link" href="#Testing"></a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=b5111194-5eeb-4104-8df0-f977a6b716e0">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[32]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>

<span class="n">bnb_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span>
    <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="s2">"nf4"</span><span class="p">,</span>
    <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
<span class="p">)</span>

<span class="n">base_model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">checkpoint</span><span class="p">,</span>
    <span class="n">quantization_config</span><span class="o">=</span><span class="n">bnb_config</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span>
    <span class="n">num_labels</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">token</span><span class="o">=</span><span class="n">access_token</span><span class="p">,</span>
    <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">eval_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">checkpoint</span><span class="p">,</span>
    <span class="n">token</span><span class="o">=</span><span class="n">access_token</span><span class="p">,</span>
    <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Loading checkpoint shards:   0%|          | 0/30 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-70B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=530fa55f-c8c4-485f-a093-09bf2175d586">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[33]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">PeftModel</span>
<span class="n">test_checkpoint_name</span> <span class="o">=</span> <span class="n">checkpoint_folder_name</span>
<span class="n">ft_model</span> <span class="o">=</span> <span class="n">PeftModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model</span><span class="p">,</span> <span class="n">project_name</span><span class="o">+</span><span class="s1">'/'</span><span class="o">+</span><span class="n">test_checkpoint_name</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=27354c4f-95cc-4d1f-ac64-c5e6b4a842ae">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[34]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># if torch.cuda.device_count() &gt; 1:</span>
<span class="c1">#     print("Using", torch.cuda.device_count(), "GPUs!")</span>
<span class="c1">#     ft_model = torch.nn.DataParallel(ft_model)</span>

<span class="c1"># ft_model.cuda()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=eee9ab6f-a7d5-4dd5-9436-2f6f6e2bffaf">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[35]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tqdm.auto</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="n">processed_predictions</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">tokenized_test_ds</span><span class="p">):</span>

        <span class="n">eval_prompt</span> <span class="o">=</span> <span class="n">record</span><span class="p">[</span><span class="s1">'article'</span><span class="p">]</span>
        <span class="n">model_input</span> <span class="o">=</span> <span class="n">tokenize_fn</span><span class="p">({</span><span class="s1">'article'</span><span class="p">:</span> <span class="n">eval_prompt</span><span class="p">})</span>

        <span class="c1"># model_input = {k: v.to('cuda') for k, v in model_input.items()}</span>
        
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">ft_model</span><span class="p">(</span><span class="o">**</span><span class="n">model_input</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>
        
        <span class="n">prediction</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>  <span class="c1"># Use .item() to get a Python number</span>
        <span class="n">processed_predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">prediction</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>  0%|          | 0/7203 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=80f06a2c-5ded-4da0-8232-145383967c9d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Accuracy-and-F1">Accuracy and F1<a class="anchor-link" href="#Accuracy-and-F1"></a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=6e891bc5-55e0-4c43-a035-0edc37dcb6e3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[36]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">true_articles</span> <span class="o">=</span> <span class="n">tokenized_test_ds</span><span class="p">[</span><span class="s1">'label'</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=6bf33824-d7cc-4d22-af4d-7d44e569c2b9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[37]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span><span class="p">,</span> <span class="n">accuracy_score</span>

<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">true_articles</span><span class="p">,</span> <span class="n">processed_predictions</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"accuracy:"</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>accuracy: 0.9994446758295155
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=9b37dc24-3bc0-48a0-ace9-a360bae91169">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[38]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">true_articles</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=a29ebf68-632f-49d1-b903-407ff05b5569">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[39]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">processed_predictions</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=fb4d8350-a8d1-4853-a9f8-74ae9ea36bde">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[40]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">f1_score</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">true_articles</span><span class="p">,</span> <span class="n">processed_predictions</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">'macro'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"f1_score:"</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>f1_score: 0.9994140400971312
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=2dcaac14-2b83-4c24-b83a-f6d0e73a2d2a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Confusion-Matrix">Confusion Matrix<a class="anchor-link" href="#Confusion-Matrix"></a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=57e64afb-e3ee-4c79-8228-b2d79806229e">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[41]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">ConfusionMatrixDisplay</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">tokenized_test_ds</span><span class="p">[</span><span class="s1">'label'</span><span class="p">],</span> <span class="n">processed_predictions</span><span class="p">)</span>

<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'human'</span><span class="p">,</span> <span class="s1">'machine'</span><span class="p">]</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="o">=</span><span class="n">cm</span><span class="p">,</span> <span class="n">display_labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Blues</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=af7d6f55-fb4f-4195-ac3f-93852ecd4b65">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[42]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">file_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">project_name</span><span class="si">}</span><span class="s2">-v12.ipynb"</span>
<span class="n">html_file_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">file_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">'.ipynb'</span><span class="p">,</span><span class="w"> </span><span class="s1">'.html'</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span>

<span class="n">command</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"jupyter nbconvert '</span><span class="si">{</span><span class="n">file_name</span><span class="si">}</span><span class="s2">' --to html --output-dir './html' --output '</span><span class="si">{</span><span class="n">html_file_name</span><span class="si">}</span><span class="s2">'"</span>
<span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">system</span><span class="p">(</span><span class="n">command</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[NbConvertApp] Converting notebook praxis-Meta-Llama-3-70B-small-finetune-v12.ipynb to html
[NbConvertApp] Writing 707834 bytes to html/praxis-Meta-Llama-3-70B-small-finetune-v12.html
</pre>
</div>
</div>
</div>
</div>
</div>
</main>
</body>
</html>
