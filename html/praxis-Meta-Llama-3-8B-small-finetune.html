<!DOCTYPE html>

<html lang="en">
<head><meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>praxis-Meta-Llama-3-8B-small-finetune-v12</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<style type="text/css">
    pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: var(--jp-cell-editor-active-background) }
.highlight { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) }
.highlight .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */
.highlight .err { color: var(--jp-mirror-editor-error-color) } /* Error */
.highlight .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */
.highlight .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */
.highlight .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */
.highlight .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */
.highlight .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */
.highlight .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */
.highlight .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */
.highlight .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */
.highlight .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */
.highlight .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */
.highlight .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */
.highlight .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */
.highlight .pm { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation.Marker */
.highlight .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */
.highlight .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */
.highlight .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */
.highlight .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */
.highlight .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */
.highlight .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */
.highlight .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */
.highlight .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */
.highlight .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */
.highlight .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */
.highlight .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */
.highlight .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */
.highlight .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */
.highlight .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */
.highlight .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */
.highlight .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */
.highlight .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */
.highlight .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */
.highlight .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */
.highlight .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */
  </style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
 * Mozilla scrollbar styling
 */

/* use standard opaque scrollbars for most nodes */
[data-jp-theme-scrollbars='true'] {
  scrollbar-color: rgb(var(--jp-scrollbar-thumb-color))
    var(--jp-scrollbar-background-color);
}

/* for code nodes, use a transparent style of scrollbar. These selectors
 * will match lower in the tree, and so will override the above */
[data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar,
[data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
}

/* tiny scrollbar */

.jp-scrollbar-tiny {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
  scrollbar-width: thin;
}

/* tiny scrollbar */

.jp-scrollbar-tiny::-webkit-scrollbar,
.jp-scrollbar-tiny::-webkit-scrollbar-corner {
  background-color: transparent;
  height: 4px;
  width: 4px;
}

.jp-scrollbar-tiny::-webkit-scrollbar-thumb {
  background: rgba(var(--jp-scrollbar-thumb-color), 0.5);
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:horizontal {
  border-left: 0 solid transparent;
  border-right: 0 solid transparent;
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:vertical {
  border-top: 0 solid transparent;
  border-bottom: 0 solid transparent;
}

/*
 * Lumino
 */

.lm-ScrollBar[data-orientation='horizontal'] {
  min-height: 16px;
  max-height: 16px;
  min-width: 45px;
  border-top: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] {
  min-width: 16px;
  max-width: 16px;
  min-height: 45px;
  border-left: 1px solid #a0a0a0;
}

.lm-ScrollBar-button {
  background-color: #f0f0f0;
  background-position: center center;
  min-height: 15px;
  max-height: 15px;
  min-width: 15px;
  max-width: 15px;
}

.lm-ScrollBar-button:hover {
  background-color: #dadada;
}

.lm-ScrollBar-button.lm-mod-active {
  background-color: #cdcdcd;
}

.lm-ScrollBar-track {
  background: #f0f0f0;
}

.lm-ScrollBar-thumb {
  background: #cdcdcd;
}

.lm-ScrollBar-thumb:hover {
  background: #bababa;
}

.lm-ScrollBar-thumb.lm-mod-active {
  background: #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal'] .lm-ScrollBar-thumb {
  height: 100%;
  min-width: 15px;
  border-left: 1px solid #a0a0a0;
  border-right: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] .lm-ScrollBar-thumb {
  width: 100%;
  min-height: 15px;
  border-top: 1px solid #a0a0a0;
  border-bottom: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-left);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-right);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-up);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-down);
  background-size: 17px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Widget {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
}

.lm-Widget.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.lm-AccordionPanel[data-orientation='horizontal'] > .lm-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  display: flex;
  flex-direction: column;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-CommandPalette-search {
  flex: 0 0 auto;
}

.lm-CommandPalette-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  min-height: 0;
  overflow: auto;
  list-style-type: none;
}

.lm-CommandPalette-header {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-CommandPalette-item {
  display: flex;
  flex-direction: row;
}

.lm-CommandPalette-itemIcon {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemContent {
  flex: 1 1 auto;
  overflow: hidden;
}

.lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemLabel {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-close-icon {
  border: 1px solid transparent;
  background-color: transparent;
  position: absolute;
  z-index: 1;
  right: 3%;
  top: 0;
  bottom: 0;
  margin: auto;
  padding: 7px 0;
  display: none;
  vertical-align: middle;
  outline: 0;
  cursor: pointer;
}
.lm-close-icon:after {
  content: 'X';
  display: block;
  width: 15px;
  height: 15px;
  text-align: center;
  color: #000;
  font-weight: normal;
  font-size: 12px;
  cursor: pointer;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-DockPanel {
  z-index: 0;
}

.lm-DockPanel-widget {
  z-index: 0;
}

.lm-DockPanel-tabBar {
  z-index: 1;
}

.lm-DockPanel-handle {
  z-index: 2;
}

.lm-DockPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-DockPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-DockPanel-handle[data-orientation='horizontal'] {
  cursor: ew-resize;
}

.lm-DockPanel-handle[data-orientation='vertical'] {
  cursor: ns-resize;
}

.lm-DockPanel-handle[data-orientation='horizontal']:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-DockPanel-handle[data-orientation='vertical']:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

.lm-DockPanel-overlay {
  z-index: 3;
  box-sizing: border-box;
  pointer-events: none;
}

.lm-DockPanel-overlay.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Menu {
  z-index: 10000;
  position: absolute;
  white-space: nowrap;
  overflow-x: hidden;
  overflow-y: auto;
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-Menu-content {
  margin: 0;
  padding: 0;
  display: table;
  list-style-type: none;
}

.lm-Menu-item {
  display: table-row;
}

.lm-Menu-item.lm-mod-hidden,
.lm-Menu-item.lm-mod-collapsed {
  display: none !important;
}

.lm-Menu-itemIcon,
.lm-Menu-itemSubmenuIcon {
  display: table-cell;
  text-align: center;
}

.lm-Menu-itemLabel {
  display: table-cell;
  text-align: left;
}

.lm-Menu-itemShortcut {
  display: table-cell;
  text-align: right;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-MenuBar {
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-MenuBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex-direction: row;
  list-style-type: none;
}

.lm-MenuBar-item {
  box-sizing: border-box;
}

.lm-MenuBar-itemIcon,
.lm-MenuBar-itemLabel {
  display: inline-block;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-ScrollBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-ScrollBar[data-orientation='horizontal'] {
  flex-direction: row;
}

.lm-ScrollBar[data-orientation='vertical'] {
  flex-direction: column;
}

.lm-ScrollBar-button {
  box-sizing: border-box;
  flex: 0 0 auto;
}

.lm-ScrollBar-track {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
  flex: 1 1 auto;
}

.lm-ScrollBar-thumb {
  box-sizing: border-box;
  position: absolute;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-SplitPanel-child {
  z-index: 0;
}

.lm-SplitPanel-handle {
  z-index: 1;
}

.lm-SplitPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-SplitPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle {
  cursor: ew-resize;
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle {
  cursor: ns-resize;
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-TabBar[data-orientation='horizontal'] {
  flex-direction: row;
  align-items: flex-end;
}

.lm-TabBar[data-orientation='vertical'] {
  flex-direction: column;
  align-items: flex-end;
}

.lm-TabBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex: 1 1 auto;
  list-style-type: none;
}

.lm-TabBar[data-orientation='horizontal'] > .lm-TabBar-content {
  flex-direction: row;
}

.lm-TabBar[data-orientation='vertical'] > .lm-TabBar-content {
  flex-direction: column;
}

.lm-TabBar-tab {
  display: flex;
  flex-direction: row;
  box-sizing: border-box;
  overflow: hidden;
  touch-action: none; /* Disable native Drag/Drop */
}

.lm-TabBar-tabIcon,
.lm-TabBar-tabCloseIcon {
  flex: 0 0 auto;
}

.lm-TabBar-tabLabel {
  flex: 1 1 auto;
  overflow: hidden;
  white-space: nowrap;
}

.lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
}

.lm-TabBar-tab.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar-addButton.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab {
  position: relative;
}

.lm-TabBar.lm-mod-dragging[data-orientation='horizontal'] .lm-TabBar-tab {
  left: 0;
  transition: left 150ms ease;
}

.lm-TabBar.lm-mod-dragging[data-orientation='vertical'] .lm-TabBar-tab {
  top: 0;
  transition: top 150ms ease;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging {
  transition: none;
}

.lm-TabBar-tabLabel .lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
  background: inherit;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabPanel-tabBar {
  z-index: 1;
}

.lm-TabPanel-stackedPanel {
  z-index: 0;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapse {
  display: flex;
  flex-direction: column;
  align-items: stretch;
}

.jp-Collapse-header {
  padding: 1px 12px;
  background-color: var(--jp-layout-color1);
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  align-items: center;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  text-transform: uppercase;
  user-select: none;
}

.jp-Collapser-icon {
  height: 16px;
}

.jp-Collapse-header-collapsed .jp-Collapser-icon {
  transform: rotate(-90deg);
  margin: auto 0;
}

.jp-Collapser-title {
  line-height: 25px;
}

.jp-Collapse-contents {
  padding: 0 12px;
  background-color: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensureUiComponents() in @jupyterlab/buildutils */

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

/* Icons urls */

:root {
  --jp-icon-add-above: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5MikiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik00Ljc1IDQuOTMwNjZINi42MjVWNi44MDU2NkM2LjYyNSA3LjAxMTkxIDYuNzkzNzUgNy4xODA2NiA3IDcuMTgwNjZDNy4yMDYyNSA3LjE4MDY2IDcuMzc1IDcuMDExOTEgNy4zNzUgNi44MDU2NlY0LjkzMDY2SDkuMjVDOS40NTYyNSA0LjkzMDY2IDkuNjI1IDQuNzYxOTEgOS42MjUgNC41NTU2NkM5LjYyNSA0LjM0OTQxIDkuNDU2MjUgNC4xODA2NiA5LjI1IDQuMTgwNjZINy4zNzVWMi4zMDU2NkM3LjM3NSAyLjA5OTQxIDcuMjA2MjUgMS45MzA2NiA3IDEuOTMwNjZDNi43OTM3NSAxLjkzMDY2IDYuNjI1IDIuMDk5NDEgNi42MjUgMi4zMDU2NlY0LjE4MDY2SDQuNzVDNC41NDM3NSA0LjE4MDY2IDQuMzc1IDQuMzQ5NDEgNC4zNzUgNC41NTU2NkM0LjM3NSA0Ljc2MTkxIDQuNTQzNzUgNC45MzA2NiA0Ljc1IDQuOTMwNjZaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC43Ii8+CjwvZz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTExLjUgOS41VjExLjVMMi41IDExLjVWOS41TDExLjUgOS41Wk0xMiA4QzEyLjU1MjMgOCAxMyA4LjQ0NzcyIDEzIDlWMTJDMTMgMTIuNTUyMyAxMi41NTIzIDEzIDEyIDEzTDIgMTNDMS40NDc3MiAxMyAxIDEyLjU1MjMgMSAxMlY5QzEgOC40NDc3MiAxLjQ0NzcxIDggMiA4TDEyIDhaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5MiI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KC0xIDAgMCAxIDEwIDEuNTU1NjYpIi8+CjwvY2xpcFBhdGg+CjwvZGVmcz4KPC9zdmc+Cg==);
  --jp-icon-add-below: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5OCkiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik05LjI1IDEwLjA2OTNMNy4zNzUgMTAuMDY5M0w3LjM3NSA4LjE5NDM0QzcuMzc1IDcuOTg4MDkgNy4yMDYyNSA3LjgxOTM0IDcgNy44MTkzNEM2Ljc5Mzc1IDcuODE5MzQgNi42MjUgNy45ODgwOSA2LjYyNSA4LjE5NDM0TDYuNjI1IDEwLjA2OTNMNC43NSAxMC4wNjkzQzQuNTQzNzUgMTAuMDY5MyA0LjM3NSAxMC4yMzgxIDQuMzc1IDEwLjQ0NDNDNC4zNzUgMTAuNjUwNiA0LjU0Mzc1IDEwLjgxOTMgNC43NSAxMC44MTkzTDYuNjI1IDEwLjgxOTNMNi42MjUgMTIuNjk0M0M2LjYyNSAxMi45MDA2IDYuNzkzNzUgMTMuMDY5MyA3IDEzLjA2OTNDNy4yMDYyNSAxMy4wNjkzIDcuMzc1IDEyLjkwMDYgNy4zNzUgMTIuNjk0M0w3LjM3NSAxMC44MTkzTDkuMjUgMTAuODE5M0M5LjQ1NjI1IDEwLjgxOTMgOS42MjUgMTAuNjUwNiA5LjYyNSAxMC40NDQzQzkuNjI1IDEwLjIzODEgOS40NTYyNSAxMC4wNjkzIDkuMjUgMTAuMDY5M1oiIGZpbGw9IiM2MTYxNjEiIHN0cm9rZT0iIzYxNjE2MSIgc3Ryb2tlLXdpZHRoPSIwLjciLz4KPC9nPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMi41IDUuNUwyLjUgMy41TDExLjUgMy41TDExLjUgNS41TDIuNSA1LjVaTTIgN0MxLjQ0NzcyIDcgMSA2LjU1MjI4IDEgNkwxIDNDMSAyLjQ0NzcyIDEuNDQ3NzIgMiAyIDJMMTIgMkMxMi41NTIzIDIgMTMgMi40NDc3MiAxMyAzTDEzIDZDMTMgNi41NTIyOSAxMi41NTIzIDcgMTIgN0wyIDdaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5OCI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KDEgMS43NDg0NmUtMDcgMS43NDg0NmUtMDcgLTEgNCAxMy40NDQzKSIvPgo8L2NsaXBQYXRoPgo8L2RlZnM+Cjwvc3ZnPgo=);
  --jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bell: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE2IDE2IiB2ZXJzaW9uPSIxLjEiPgogICA8cGF0aCBjbGFzcz0ianAtaWNvbjIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMzMzMzMzIgogICAgICBkPSJtOCAwLjI5Yy0xLjQgMC0yLjcgMC43My0zLjYgMS44LTEuMiAxLjUtMS40IDMuNC0xLjUgNS4yLTAuMTggMi4yLTAuNDQgNC0yLjMgNS4zbDAuMjggMS4zaDVjMC4wMjYgMC42NiAwLjMyIDEuMSAwLjcxIDEuNSAwLjg0IDAuNjEgMiAwLjYxIDIuOCAwIDAuNTItMC40IDAuNi0xIDAuNzEtMS41aDVsMC4yOC0xLjNjLTEuOS0wLjk3LTIuMi0zLjMtMi4zLTUuMy0wLjEzLTEuOC0wLjI2LTMuNy0xLjUtNS4yLTAuODUtMS0yLjItMS44LTMuNi0xLjh6bTAgMS40YzAuODggMCAxLjkgMC41NSAyLjUgMS4zIDAuODggMS4xIDEuMSAyLjcgMS4yIDQuNCAwLjEzIDEuNyAwLjIzIDMuNiAxLjMgNS4yaC0xMGMxLjEtMS42IDEuMi0zLjQgMS4zLTUuMiAwLjEzLTEuNyAwLjMtMy4zIDEuMi00LjQgMC41OS0wLjcyIDEuNi0xLjMgMi41LTEuM3ptLTAuNzQgMTJoMS41Yy0wLjAwMTUgMC4yOCAwLjAxNSAwLjc5LTAuNzQgMC43OS0wLjczIDAuMDAxNi0wLjcyLTAuNTMtMC43NC0wLjc5eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-bug-dot: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiPgogICAgICAgIDxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMTcuMTkgOEgyMFYxMEgxNy45MUMxNy45NiAxMC4zMyAxOCAxMC42NiAxOCAxMVYxMkgyMFYxNEgxOC41SDE4VjE0LjAyNzVDMTUuNzUgMTQuMjc2MiAxNCAxNi4xODM3IDE0IDE4LjVDMTQgMTkuMjA4IDE0LjE2MzUgMTkuODc3OSAxNC40NTQ5IDIwLjQ3MzlDMTMuNzA2MyAyMC44MTE3IDEyLjg3NTcgMjEgMTIgMjFDOS43OCAyMSA3Ljg1IDE5Ljc5IDYuODEgMThINFYxNkg2LjA5QzYuMDQgMTUuNjcgNiAxNS4zNCA2IDE1VjE0SDRWMTJINlYxMUM2IDEwLjY2IDYuMDQgMTAuMzMgNi4wOSAxMEg0VjhINi44MUM3LjI2IDcuMjIgNy44OCA2LjU1IDguNjIgNi4wNEw3IDQuNDFMOC40MSAzTDEwLjU5IDUuMTdDMTEuMDQgNS4wNiAxMS41MSA1IDEyIDVDMTIuNDkgNSAxMi45NiA1LjA2IDEzLjQyIDUuMTdMMTUuNTkgM0wxNyA0LjQxTDE1LjM3IDYuMDRDMTYuMTIgNi41NSAxNi43NCA3LjIyIDE3LjE5IDhaTTEwIDE2SDE0VjE0SDEwVjE2Wk0xMCAxMkgxNFYxMEgxMFYxMloiIGZpbGw9IiM2MTYxNjEiLz4KICAgICAgICA8cGF0aCBkPSJNMjIgMTguNUMyMiAyMC40MzMgMjAuNDMzIDIyIDE4LjUgMjJDMTYuNTY3IDIyIDE1IDIwLjQzMyAxNSAxOC41QzE1IDE2LjU2NyAxNi41NjcgMTUgMTguNSAxNUMyMC40MzMgMTUgMjIgMTYuNTY3IDIyIDE4LjVaIiBmaWxsPSIjNjE2MTYxIi8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yMCA4aC0yLjgxYy0uNDUtLjc4LTEuMDctMS40NS0xLjgyLTEuOTZMMTcgNC40MSAxNS41OSAzbC0yLjE3IDIuMTdDMTIuOTYgNS4wNiAxMi40OSA1IDEyIDVjLS40OSAwLS45Ni4wNi0xLjQxLjE3TDguNDEgMyA3IDQuNDFsMS42MiAxLjYzQzcuODggNi41NSA3LjI2IDcuMjIgNi44MSA4SDR2MmgyLjA5Yy0uMDUuMzMtLjA5LjY2LS4wOSAxdjFINHYyaDJ2MWMwIC4zNC4wNC42Ny4wOSAxSDR2MmgyLjgxYzEuMDQgMS43OSAyLjk3IDMgNS4xOSAzczQuMTUtMS4yMSA1LjE5LTNIMjB2LTJoLTIuMDljLjA1LS4zMy4wOS0uNjYuMDktMXYtMWgydi0yaC0ydi0xYzAtLjM0LS4wNC0uNjctLjA5LTFIMjBWOHptLTYgOGgtNHYtMmg0djJ6bTAtNGgtNHYtMmg0djJ6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-build: url(data:image/svg+xml;base64,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);
  --jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik05IDE2LjE3TDQuODMgMTJsLTEuNDIgMS40MUw5IDE5IDIxIDdsLTEuNDEtMS40MXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBzaGFwZS1yZW5kZXJpbmc9Imdlb21ldHJpY1ByZWNpc2lvbiI+CiAgICA8cGF0aCBkPSJNNi41OSwzLjQxTDIsOEw2LjU5LDEyLjZMOCwxMS4xOEw0LjgyLDhMOCw0LjgyTDYuNTksMy40MU0xMi40MSwzLjQxTDExLDQuODJMMTQuMTgsOEwxMSwxMS4xOEwxMi40MSwxMi42TDE3LDhMMTIuNDEsMy40MU0yMS41OSwxMS41OUwxMy41LDE5LjY4TDkuODMsMTZMOC40MiwxNy40MUwxMy41LDIyLjVMMjMsMTNMMjEuNTksMTEuNTlaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTExLjQgMTguNkw2LjggMTRMMTEuNCA5LjRMMTAgOEw0IDE0TDEwIDIwTDExLjQgMTguNlpNMTYuNiAxOC42TDIxLjIgMTRMMTYuNiA5LjRMMTggOEwyNCAxNEwxOCAyMEwxNi42IDE4LjZWMTguNloiLz4KCTwvZz4KPC9zdmc+Cg==);
  --jp-icon-collapse-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNNiAxM3YyaDh2LTJ6IiAvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1jb25zb2xlLWljb24tYmFja2dyb3VuZC1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtY29uc29sZS1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIj4KICAgIDxwYXRoIGQ9Ik0xMDUgMTI3LjNoNDB2MTIuOGgtNDB6TTUxLjEgNzdMNzQgOTkuOWwtMjMuMyAyMy4zIDEwLjUgMTAuNSAyMy4zLTIzLjNMOTUgOTkuOSA4NC41IDg5LjQgNjEuNiA2Ni41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copyright: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGVuYWJsZS1iYWNrZ3JvdW5kPSJuZXcgMCAwIDI0IDI0IiBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCI+CiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0xMS44OCw5LjE0YzEuMjgsMC4wNiwxLjYxLDEuMTUsMS42MywxLjY2aDEuNzljLTAuMDgtMS45OC0xLjQ5LTMuMTktMy40NS0zLjE5QzkuNjQsNy42MSw4LDksOCwxMi4xNCBjMCwxLjk0LDAuOTMsNC4yNCwzLjg0LDQuMjRjMi4yMiwwLDMuNDEtMS42NSwzLjQ0LTIuOTVoLTEuNzljLTAuMDMsMC41OS0wLjQ1LDEuMzgtMS42MywxLjQ0QzEwLjU1LDE0LjgzLDEwLDEzLjgxLDEwLDEyLjE0IEMxMCw5LjI1LDExLjI4LDkuMTYsMTEuODgsOS4xNHogTTEyLDJDNi40OCwyLDIsNi40OCwyLDEyczQuNDgsMTAsMTAsMTBzMTAtNC40OCwxMC0xMFMxNy41MiwyLDEyLDJ6IE0xMiwyMGMtNC40MSwwLTgtMy41OS04LTggczMuNTktOCw4LThzOCwzLjU5LDgsOFMxNi40MSwyMCwxMiwyMHoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-delete: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2cHgiIGhlaWdodD0iMTZweCI+CiAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIiAvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjI2MjYyIiBkPSJNNiAxOWMwIDEuMS45IDIgMiAyaDhjMS4xIDAgMi0uOSAyLTJWN0g2djEyek0xOSA0aC0zLjVsLTEtMWgtNWwtMSAxSDV2MmgxNFY0eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-duplicate: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTIuNzk5OTggMC44NzVIOC44OTU4MkM5LjIwMDYxIDAuODc1IDkuNDQ5OTggMS4xMzkxNCA5LjQ0OTk4IDEuNDYxOThDOS40NDk5OCAxLjc4NDgyIDkuMjAwNjEgMi4wNDg5NiA4Ljg5NTgyIDIuMDQ4OTZIMy4zNTQxNUMzLjA0OTM2IDIuMDQ4OTYgMi43OTk5OCAyLjMxMzEgMi43OTk5OCAyLjYzNTk0VjkuNjc5NjlDMi43OTk5OCAxMC4wMDI1IDIuNTUwNjEgMTAuMjY2NyAyLjI0NTgyIDEwLjI2NjdDMS45NDEwMyAxMC4yNjY3IDEuNjkxNjUgMTAuMDAyNSAxLjY5MTY1IDkuNjc5NjlWMi4wNDg5NkMxLjY5MTY1IDEuNDAzMjggMi4xOTA0IDAuODc1IDIuNzk5OTggMC44NzVaTTUuMzY2NjUgMTEuOVY0LjU1SDExLjA4MzNWMTEuOUg1LjM2NjY1Wk00LjE0MTY1IDQuMTQxNjdDNC4xNDE2NSAzLjY5MDYzIDQuNTA3MjggMy4zMjUgNC45NTgzMiAzLjMyNUgxMS40OTE3QzExLjk0MjcgMy4zMjUgMTIuMzA4MyAzLjY5MDYzIDEyLjMwODMgNC4xNDE2N1YxMi4zMDgzQzEyLjMwODMgMTIuNzU5NCAxMS45NDI3IDEzLjEyNSAxMS40OTE3IDEzLjEyNUg0Ljk1ODMyQzQuNTA3MjggMTMuMTI1IDQuMTQxNjUgMTIuNzU5NCA0LjE0MTY1IDEyLjMwODNWNC4xNDE2N1oiIGZpbGw9IiM2MTYxNjEiLz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNOS40MzU3NCA4LjI2NTA3SDguMzY0MzFWOS4zMzY1QzguMzY0MzEgOS40NTQzNSA4LjI2Nzg4IDkuNTUwNzggOC4xNTAwMiA5LjU1MDc4QzguMDMyMTcgOS41NTA3OCA3LjkzNTc0IDkuNDU0MzUgNy45MzU3NCA5LjMzNjVWOC4yNjUwN0g2Ljg2NDMxQzYuNzQ2NDUgOC4yNjUwNyA2LjY1MDAyIDguMTY4NjQgNi42NTAwMiA4LjA1MDc4QzYuNjUwMDIgNy45MzI5MiA2Ljc0NjQ1IDcuODM2NSA2Ljg2NDMxIDcuODM2NUg3LjkzNTc0VjYuNzY1MDdDNy45MzU3NCA2LjY0NzIxIDguMDMyMTcgNi41NTA3OCA4LjE1MDAyIDYuNTUwNzhDOC4yNjc4OCA2LjU1MDc4IDguMzY0MzEgNi42NDcyMSA4LjM2NDMxIDYuNzY1MDdWNy44MzY1SDkuNDM1NzRDOS41NTM2IDcuODM2NSA5LjY1MDAyIDcuOTMyOTIgOS42NTAwMiA4LjA1MDc4QzkuNjUwMDIgOC4xNjg2NCA5LjU1MzYgOC4yNjUwNyA5LjQzNTc0IDguMjY1MDdaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC41Ii8+Cjwvc3ZnPgo=);
  --jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-error: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj48Y2lyY2xlIGN4PSIxMiIgY3k9IjE5IiByPSIyIi8+PHBhdGggZD0iTTEwIDNoNHYxMmgtNHoiLz48L2c+CjxwYXRoIGZpbGw9Im5vbmUiIGQ9Ik0wIDBoMjR2MjRIMHoiLz4KPC9zdmc+Cg==);
  --jp-icon-expand-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNMTEgMTBIOXYzSDZ2MmgzdjNoMnYtM2gzdi0yaC0zeiIgLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-filter-dot: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWRvdCIgZmlsbD0iI0ZGRiI+CiAgICA8Y2lyY2xlIGN4PSIxOCIgY3k9IjE3IiByPSIzIj48L2NpcmNsZT4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-filter: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-folder-favorite: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgwVjB6IiBmaWxsPSJub25lIi8+PHBhdGggY2xhc3M9ImpwLWljb24zIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzYxNjE2MSIgZD0iTTIwIDZoLThsLTItMkg0Yy0xLjEgMC0yIC45LTIgMnYxMmMwIDEuMS45IDIgMiAyaDE2YzEuMSAwIDItLjkgMi0yVjhjMC0xLjEtLjktMi0yLTJ6bS0yLjA2IDExTDE1IDE1LjI4IDEyLjA2IDE3bC43OC0zLjMzLTIuNTktMi4yNCAzLjQxLS4yOUwxNSA4bDEuMzQgMy4xNCAzLjQxLjI5LTIuNTkgMi4yNC43OCAzLjMzeiIvPgo8L3N2Zz4K);
  --jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-home: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPjxwYXRoIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xMCAyMHYtNmg0djZoNXYtOGgzTDEyIDMgMiAxMmgzdjh6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==);
  --jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-info: url(data:image/svg+xml;base64,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);
  --jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaW5zcGVjdG9yLWljb24tY29sb3IganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K);
  --jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtanNvbi1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0Y5QTgyNSI+CiAgICA8cGF0aCBkPSJNMjAuMiAxMS44Yy0xLjYgMC0xLjcuNS0xLjcgMSAwIC40LjEuOS4xIDEuMy4xLjUuMS45LjEgMS4zIDAgMS43LTEuNCAyLjMtMy41IDIuM2gtLjl2LTEuOWguNWMxLjEgMCAxLjQgMCAxLjQtLjggMC0uMyAwLS42LS4xLTEgMC0uNC0uMS0uOC0uMS0xLjIgMC0xLjMgMC0xLjggMS4zLTItMS4zLS4yLTEuMy0uNy0xLjMtMiAwLS40LjEtLjguMS0xLjIuMS0uNC4xLS43LjEtMSAwLS44LS40LS43LTEuNC0uOGgtLjVWNC4xaC45YzIuMiAwIDMuNS43IDMuNSAyLjMgMCAuNC0uMS45LS4xIDEuMy0uMS41LS4xLjktLjEgMS4zIDAgLjUuMiAxIDEuNyAxdjEuOHpNMS44IDEwLjFjMS42IDAgMS43LS41IDEuNy0xIDAtLjQtLjEtLjktLjEtMS4zLS4xLS41LS4xLS45LS4xLTEuMyAwLTEuNiAxLjQtMi4zIDMuNS0yLjNoLjl2MS45aC0uNWMtMSAwLTEuNCAwLTEuNC44IDAgLjMgMCAuNi4xIDEgMCAuMi4xLjYuMSAxIDAgMS4zIDAgMS44LTEuMyAyQzYgMTEuMiA2IDExLjcgNiAxM2MwIC40LS4xLjgtLjEgMS4yLS4xLjMtLjEuNy0uMSAxIDAgLjguMy44IDEuNC44aC41djEuOWgtLjljLTIuMSAwLTMuNS0uNi0zLjUtMi4zIDAtLjQuMS0uOS4xLTEuMy4xLS41LjEtLjkuMS0xLjMgMC0uNS0uMi0xLTEuNy0xdi0xLjl6Ii8+CiAgICA8Y2lyY2xlIGN4PSIxMSIgY3k9IjEzLjgiIHI9IjIuMSIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSI4LjIiIHI9IjIuMSIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-julia: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDMyNSAzMDAiPgogIDxnIGNsYXNzPSJqcC1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjY2IzYzMzIj4KICAgIDxwYXRoIGQ9Ik0gMTUwLjg5ODQzOCAyMjUgQyAxNTAuODk4NDM4IDI2Ni40MjE4NzUgMTE3LjMyMDMxMiAzMDAgNzUuODk4NDM4IDMwMCBDIDM0LjQ3NjU2MiAzMDAgMC44OTg0MzggMjY2LjQyMTg3NSAwLjg5ODQzOCAyMjUgQyAwLjg5ODQzOCAxODMuNTc4MTI1IDM0LjQ3NjU2MiAxNTAgNzUuODk4NDM4IDE1MCBDIDExNy4zMjAzMTIgMTUwIDE1MC44OTg0MzggMTgzLjU3ODEyNSAxNTAuODk4NDM4IDIyNSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzM4OTgyNiI+CiAgICA8cGF0aCBkPSJNIDIzNy41IDc1IEMgMjM3LjUgMTE2LjQyMTg3NSAyMDMuOTIxODc1IDE1MCAxNjIuNSAxNTAgQyAxMjEuMDc4MTI1IDE1MCA4Ny41IDExNi40MjE4NzUgODcuNSA3NSBDIDg3LjUgMzMuNTc4MTI1IDEyMS4wNzgxMjUgMCAxNjIuNSAwIEMgMjAzLjkyMTg3NSAwIDIzNy41IDMzLjU3ODEyNSAyMzcuNSA3NSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzk1NThiMiI+CiAgICA8cGF0aCBkPSJNIDMyNC4xMDE1NjIgMjI1IEMgMzI0LjEwMTU2MiAyNjYuNDIxODc1IDI5MC41MjM0MzggMzAwIDI0OS4xMDE1NjIgMzAwIEMgMjA3LjY3OTY4OCAzMDAgMTc0LjEwMTU2MiAyNjYuNDIxODc1IDE3NC4xMDE1NjIgMjI1IEMgMTc0LjEwMTU2MiAxODMuNTc4MTI1IDIwNy42Nzk2ODggMTUwIDI0OS4xMDE1NjIgMTUwIEMgMjkwLjUyMzQzOCAxNTAgMzI0LjEwMTU2MiAxODMuNTc4MTI1IDMyNC4xMDE1NjIgMjI1Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgPGcgY2xhc3M9ImpwLWp1cHl0ZXItaWNvbi1jb2xvciIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-jupyter: url(data:image/svg+xml;base64,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);
  --jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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);
  --jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K);
  --jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==);
  --jp-icon-launch: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMzIgMzIiIHdpZHRoPSIzMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yNiwyOEg2YTIuMDAyNywyLjAwMjcsMCwwLDEtMi0yVjZBMi4wMDI3LDIuMDAyNywwLDAsMSw2LDRIMTZWNkg2VjI2SDI2VjE2aDJWMjZBMi4wMDI3LDIuMDAyNywwLDAsMSwyNiwyOFoiLz4KICAgIDxwb2x5Z29uIHBvaW50cz0iMjAgMiAyMCA0IDI2LjU4NiA0IDE4IDEyLjU4NiAxOS40MTQgMTQgMjggNS40MTQgMjggMTIgMzAgMTIgMzAgMiAyMCAyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==);
  --jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4K);
  --jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-move-down: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMTIuNDcxIDcuNTI4OTlDMTIuNzYzMiA3LjIzNjg0IDEyLjc2MzIgNi43NjMxNiAxMi40NzEgNi40NzEwMVY2LjQ3MTAxQzEyLjE3OSA2LjE3OTA1IDExLjcwNTcgNi4xNzg4NCAxMS40MTM1IDYuNDcwNTRMNy43NSAxMC4xMjc1VjEuNzVDNy43NSAxLjMzNTc5IDcuNDE0MjEgMSA3IDFWMUM2LjU4NTc5IDEgNi4yNSAxLjMzNTc5IDYuMjUgMS43NVYxMC4xMjc1TDIuNTk3MjYgNi40NjgyMkMyLjMwMzM4IDYuMTczODEgMS44MjY0MSA2LjE3MzU5IDEuNTMyMjYgNi40Njc3NFY2LjQ2Nzc0QzEuMjM4MyA2Ljc2MTcgMS4yMzgzIDcuMjM4MyAxLjUzMjI2IDcuNTMyMjZMNi4yOTI4OSAxMi4yOTI5QzYuNjgzNDIgMTIuNjgzNCA3LjMxNjU4IDEyLjY4MzQgNy43MDcxMSAxMi4yOTI5TDEyLjQ3MSA3LjUyODk5WiIgZmlsbD0iIzYxNjE2MSIvPgo8L3N2Zz4K);
  --jp-icon-move-up: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMS41Mjg5OSA2LjQ3MTAxQzEuMjM2ODQgNi43NjMxNiAxLjIzNjg0IDcuMjM2ODQgMS41Mjg5OSA3LjUyODk5VjcuNTI4OTlDMS44MjA5NSA3LjgyMDk1IDIuMjk0MjYgNy44MjExNiAyLjU4NjQ5IDcuNTI5NDZMNi4yNSAzLjg3MjVWMTIuMjVDNi4yNSAxMi42NjQyIDYuNTg1NzkgMTMgNyAxM1YxM0M3LjQxNDIxIDEzIDcuNzUgMTIuNjY0MiA3Ljc1IDEyLjI1VjMuODcyNUwxMS40MDI3IDcuNTMxNzhDMTEuNjk2NiA3LjgyNjE5IDEyLjE3MzYgNy44MjY0MSAxMi40Njc3IDcuNTMyMjZWNy41MzIyNkMxMi43NjE3IDcuMjM4MyAxMi43NjE3IDYuNzYxNyAxMi40Njc3IDYuNDY3NzRMNy43MDcxMSAxLjcwNzExQzcuMzE2NTggMS4zMTY1OCA2LjY4MzQyIDEuMzE2NTggNi4yOTI4OSAxLjcwNzExTDEuNTI4OTkgNi40NzEwMVoiIGZpbGw9IiM2MTYxNjEiLz4KPC9zdmc+Cg==);
  --jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=);
  --jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtbm90ZWJvb2staWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-numbering: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTQgMTlINlYxOS41SDVWMjAuNUg2VjIxSDRWMjJIN1YxOEg0VjE5Wk01IDEwSDZWNkg0VjdINVYxMFpNNCAxM0g1LjhMNCAxNS4xVjE2SDdWMTVINS4yTDcgMTIuOVYxMkg0VjEzWk05IDdWOUgyM1Y3SDlaTTkgMjFIMjNWMTlIOVYyMVpNOSAxNUgyM1YxM0g5VjE1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-offline-bolt: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDIuMDJjLTUuNTEgMC05Ljk4IDQuNDctOS45OCA5Ljk4czQuNDcgOS45OCA5Ljk4IDkuOTggOS45OC00LjQ3IDkuOTgtOS45OFMxNy41MSAyLjAyIDEyIDIuMDJ6TTExLjQ4IDIwdi02LjI2SDhMMTMgNHY2LjI2aDMuMzVMMTEuNDggMjB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-pdf: url(data:image/svg+xml;base64,PHN2ZwogICB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyMiAyMiIgd2lkdGg9IjE2Ij4KICAgIDxwYXRoIHRyYW5zZm9ybT0icm90YXRlKDQ1KSIgY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0ZGMkEyQSIKICAgICAgIGQ9Im0gMjIuMzQ0MzY5LC0zLjAxNjM2NDIgaCA1LjYzODYwNCB2IDEuNTc5MjQzMyBoIC0zLjU0OTIyNyB2IDEuNTA4NjkyOTkgaCAzLjMzNzU3NiBWIDEuNjUwODE1NCBoIC0zLjMzNzU3NiB2IDMuNDM1MjYxMyBoIC0yLjA4OTM3NyB6IG0gLTcuMTM2NDQ0LDEuNTc5MjQzMyB2IDQuOTQzOTU0MyBoIDAuNzQ4OTIgcSAxLjI4MDc2MSwwIDEuOTUzNzAzLC0wLjYzNDk1MzUgMC42NzgzNjksLTAuNjM0OTUzNSAwLjY3ODM2OSwtMS44NDUxNjQxIDAsLTEuMjA0NzgzNTUgLTAuNjcyOTQyLC0xLjgzNDMxMDExIC0wLjY3Mjk0MiwtMC42Mjk1MjY1OSAtMS45NTkxMywtMC42Mjk1MjY1OSB6IG0gLTIuMDg5Mzc3LC0xLjU3OTI0MzMgaCAyLjIwMzM0MyBxIDEuODQ1MTY0LDAgMi43NDYwMzksMC4yNjU5MjA3IDAuOTA2MzAxLDAuMjYwNDkzNyAxLjU1MjEwOCwwLjg5MDAyMDMgMC41Njk4MywwLjU0ODEyMjMgMC44NDY2MDUsMS4yNjQ0ODAwNiAwLjI3Njc3NCwwLjcxNjM1NzgxIDAuMjc2Nzc0LDEuNjIyNjU4OTQgMCwwLjkxNzE1NTEgLTAuMjc2Nzc0LDEuNjM4OTM5OSAtMC4yNzY3NzUsMC43MTYzNTc4IC0wLjg0NjYwNSwxLjI2NDQ4IC0wLjY1MTIzNCwwLjYyOTUyNjYgLTEuNTYyOTYyLDAuODk1NDQ3MyAtMC45MTE3MjgsMC4yNjA0OTM3IC0yLjczNTE4NSwwLjI2MDQ5MzcgaCAtMi4yMDMzNDMgeiBtIC04LjE0NTg1NjUsMCBoIDMuNDY3ODIzIHEgMS41NDY2ODE2LDAgMi4zNzE1Nzg1LDAuNjg5MjIzIDAuODMwMzI0LDAuNjgzNzk2MSAwLjgzMDMyNCwxLjk1MzcwMzE0IDAsMS4yNzUzMzM5NyAtMC44MzAzMjQsMS45NjQ1NTcwNiBRIDkuOTg3MTk2MSwyLjI3NDkxNSA4LjQ0MDUxNDUsMi4yNzQ5MTUgSCA3LjA2MjA2ODQgViA1LjA4NjA3NjcgSCA0Ljk3MjY5MTUgWiBtIDIuMDg5Mzc2OSwxLjUxNDExOTkgdiAyLjI2MzAzOTQzIGggMS4xNTU5NDEgcSAwLjYwNzgxODgsMCAwLjkzODg2MjksLTAuMjkzMDU1NDcgMC4zMzEwNDQxLC0wLjI5ODQ4MjQxIDAuMzMxMDQ0MSwtMC44NDExNzc3MiAwLC0wLjU0MjY5NTMxIC0wLjMzMTA0NDEsLTAuODM1NzUwNzQgLTAuMzMxMDQ0MSwtMC4yOTMwNTU1IC0wLjkzODg2MjksLTAuMjkzMDU1NSB6IgovPgo8L3N2Zz4K);
  --jp-icon-python: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iLTEwIC0xMCAxMzEuMTYxMzYxNjk0MzM1OTQgMTMyLjM4ODk5OTkzODk2NDg0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMzA2OTk4IiBkPSJNIDU0LjkxODc4NSw5LjE5Mjc0MjFlLTQgQyA1MC4zMzUxMzIsMC4wMjIyMTcyNyA0NS45NTc4NDYsMC40MTMxMzY5NyA0Mi4xMDYyODUsMS4wOTQ2NjkzIDMwLjc2MDA2OSwzLjA5OTE3MzEgMjguNzAwMDM2LDcuMjk0NzcxNCAyOC43MDAwMzUsMTUuMDMyMTY5IHYgMTAuMjE4NzUgaCAyNi44MTI1IHYgMy40MDYyNSBoIC0yNi44MTI1IC0xMC4wNjI1IGMgLTcuNzkyNDU5LDAgLTE0LjYxNTc1ODgsNC42ODM3MTcgLTE2Ljc0OTk5OTgsMTMuNTkzNzUgLTIuNDYxODE5OTgsMTAuMjEyOTY2IC0yLjU3MTAxNTA4LDE2LjU4NjAyMyAwLDI3LjI1IDEuOTA1OTI4Myw3LjkzNzg1MiA2LjQ1NzU0MzIsMTMuNTkzNzQ4IDE0LjI0OTk5OTgsMTMuNTkzNzUgaCA5LjIxODc1IHYgLTEyLjI1IGMgMCwtOC44NDk5MDIgNy42NTcxNDQsLTE2LjY1NjI0OCAxNi43NSwtMTYuNjU2MjUgaCAyNi43ODEyNSBjIDcuNDU0OTUxLDAgMTMuNDA2MjUzLC02LjEzODE2NCAxMy40MDYyNSwtMTMuNjI1IHYgLTI1LjUzMTI1IGMgMCwtNy4yNjYzMzg2IC02LjEyOTk4LC0xMi43MjQ3NzcxIC0xMy40MDYyNSwtMTMuOTM3NDk5NyBDIDY0LjI4MTU0OCwwLjMyNzk0Mzk3IDU5LjUwMjQzOCwtMC4wMjAzNzkwMyA1NC45MTg3ODUsOS4xOTI3NDIxZS00IFogbSAtMTQuNSw4LjIxODc1MDEyNTc5IGMgMi43Njk1NDcsMCA1LjAzMTI1LDIuMjk4NjQ1NiA1LjAzMTI1LDUuMTI0OTk5NiAtMmUtNiwyLjgxNjMzNiAtMi4yNjE3MDMsNS4wOTM3NSAtNS4wMzEyNSw1LjA5Mzc1IC0yLjc3OTQ3NiwtMWUtNiAtNS4wMzEyNSwtMi4yNzc0MTUgLTUuMDMxMjUsLTUuMDkzNzUgLTEwZS03LC0yLjgyNjM1MyAyLjI1MTc3NCwtNS4xMjQ5OTk2IDUuMDMxMjUsLTUuMTI0OTk5NiB6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2ZmZDQzYiIgZD0ibSA4NS42Mzc1MzUsMjguNjU3MTY5IHYgMTEuOTA2MjUgYyAwLDkuMjMwNzU1IC03LjgyNTg5NSwxNi45OTk5OTkgLTE2Ljc1LDE3IGggLTI2Ljc4MTI1IGMgLTcuMzM1ODMzLDAgLTEzLjQwNjI0OSw2LjI3ODQ4MyAtMTMuNDA2MjUsMTMuNjI1IHYgMjUuNTMxMjQ3IGMgMCw3LjI2NjM0NCA2LjMxODU4OCwxMS41NDAzMjQgMTMuNDA2MjUsMTMuNjI1MDA0IDguNDg3MzMxLDIuNDk1NjEgMTYuNjI2MjM3LDIuOTQ2NjMgMjYuNzgxMjUsMCA2Ljc1MDE1NSwtMS45NTQzOSAxMy40MDYyNTMsLTUuODg3NjEgMTMuNDA2MjUsLTEzLjYyNTAwNCBWIDg2LjUwMDkxOSBoIC0yNi43ODEyNSB2IC0zLjQwNjI1IGggMjYuNzgxMjUgMTMuNDA2MjU0IGMgNy43OTI0NjEsMCAxMC42OTYyNTEsLTUuNDM1NDA4IDEzLjQwNjI0MSwtMTMuNTkzNzUgMi43OTkzMywtOC4zOTg4ODYgMi42ODAyMiwtMTYuNDc1Nzc2IDAsLTI3LjI1IC0xLjkyNTc4LC03Ljc1NzQ0MSAtNS42MDM4NywtMTMuNTkzNzUgLTEzLjQwNjI0MSwtMTMuNTkzNzUgeiBtIC0xNS4wNjI1LDY0LjY1NjI1IGMgMi43Nzk0NzgsM2UtNiA1LjAzMTI1LDIuMjc3NDE3IDUuMDMxMjUsNS4wOTM3NDcgLTJlLTYsMi44MjYzNTQgLTIuMjUxNzc1LDUuMTI1MDA0IC01LjAzMTI1LDUuMTI1MDA0IC0yLjc2OTU1LDAgLTUuMDMxMjUsLTIuMjk4NjUgLTUuMDMxMjUsLTUuMTI1MDA0IDJlLTYsLTIuODE2MzMgMi4yNjE2OTcsLTUuMDkzNzQ3IDUuMDMxMjUsLTUuMDkzNzQ3IHoiLz4KPC9zdmc+Cg==);
  --jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==);
  --jp-icon-react: url(data:image/svg+xml;base64,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);
  --jp-icon-redo: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZD0iTTE4LjQgMTAuNkMxNi41NSA4Ljk5IDE0LjE1IDggMTEuNSA4Yy00LjY1IDAtOC41OCAzLjAzLTkuOTYgNy4yMkwzLjkgMTZjMS4wNS0zLjE5IDQuMDUtNS41IDcuNi01LjUgMS45NSAwIDMuNzMuNzIgNS4xMiAxLjg4TDEzIDE2aDlWN2wtMy42IDMuNnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==);
  --jp-icon-share: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTSAxOCAyIEMgMTYuMzU0OTkgMiAxNSAzLjM1NDk5MDQgMTUgNSBDIDE1IDUuMTkwOTUyOSAxNS4wMjE3OTEgNS4zNzcxMjI0IDE1LjA1NjY0MSA1LjU1ODU5MzggTCA3LjkyMTg3NSA5LjcyMDcwMzEgQyA3LjM5ODUzOTkgOS4yNzc4NTM5IDYuNzMyMDc3MSA5IDYgOSBDIDQuMzU0OTkwNCA5IDMgMTAuMzU0OTkgMyAxMiBDIDMgMTMuNjQ1MDEgNC4zNTQ5OTA0IDE1IDYgMTUgQyA2LjczMjA3NzEgMTUgNy4zOTg1Mzk5IDE0LjcyMjE0NiA3LjkyMTg3NSAxNC4yNzkyOTcgTCAxNS4wNTY2NDEgMTguNDM5NDUzIEMgMTUuMDIxNTU1IDE4LjYyMTUxNCAxNSAxOC44MDgzODYgMTUgMTkgQyAxNSAyMC42NDUwMSAxNi4zNTQ5OSAyMiAxOCAyMiBDIDE5LjY0NTAxIDIyIDIxIDIwLjY0NTAxIDIxIDE5IEMgMjEgMTcuMzU0OTkgMTkuNjQ1MDEgMTYgMTggMTYgQyAxNy4yNjc0OCAxNiAxNi42MDE1OTMgMTYuMjc5MzI4IDE2LjA3ODEyNSAxNi43MjI2NTYgTCA4Ljk0MzM1OTQgMTIuNTU4NTk0IEMgOC45NzgyMDk1IDEyLjM3NzEyMiA5IDEyLjE5MDk1MyA5IDEyIEMgOSAxMS44MDkwNDcgOC45NzgyMDk1IDExLjYyMjg3OCA4Ljk0MzM1OTQgMTEuNDQxNDA2IEwgMTYuMDc4MTI1IDcuMjc5Mjk2OSBDIDE2LjYwMTQ2IDcuNzIyMTQ2MSAxNy4yNjc5MjMgOCAxOCA4IEMgMTkuNjQ1MDEgOCAyMSA2LjY0NTAwOTYgMjEgNSBDIDIxIDMuMzU0OTkwNCAxOS42NDUwMSAyIDE4IDIgeiBNIDE4IDQgQyAxOC41NjQxMjkgNCAxOSA0LjQzNTg3MDYgMTkgNSBDIDE5IDUuNTY0MTI5NCAxOC41NjQxMjkgNiAxOCA2IEMgMTcuNDM1ODcxIDYgMTcgNS41NjQxMjk0IDE3IDUgQyAxNyA0LjQzNTg3MDYgMTcuNDM1ODcxIDQgMTggNCB6IE0gNiAxMSBDIDYuNTY0MTI5NCAxMSA3IDExLjQzNTg3MSA3IDEyIEMgNyAxMi41NjQxMjkgNi41NjQxMjk0IDEzIDYgMTMgQyA1LjQzNTg3MDYgMTMgNSAxMi41NjQxMjkgNSAxMiBDIDUgMTEuNDM1ODcxIDUuNDM1ODcwNiAxMSA2IDExIHogTSAxOCAxOCBDIDE4LjU2NDEyOSAxOCAxOSAxOC40MzU4NzEgMTkgMTkgQyAxOSAxOS41NjQxMjkgMTguNTY0MTI5IDIwIDE4IDIwIEMgMTcuNDM1ODcxIDIwIDE3IDE5LjU2NDEyOSAxNyAxOSBDIDE3IDE4LjQzNTg3MSAxNy40MzU4NzEgMTggMTggMTggeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-table-rows: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMSw4SDNWNGgxOFY4eiBNMjEsMTBIM3Y0aDE4VjEweiBNMjEsMTZIM3Y0aDE4VjE2eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-tag: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjgiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCA0MyAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTI4LjgzMzIgMTIuMzM0TDMyLjk5OTggMTYuNTAwN0wzNy4xNjY1IDEyLjMzNEgyOC44MzMyWiIvPgoJCTxwYXRoIGQ9Ik0xNi4yMDk1IDIxLjYxMDRDMTUuNjg3MyAyMi4xMjk5IDE0Ljg0NDMgMjIuMTI5OSAxNC4zMjQ4IDIxLjYxMDRMNi45ODI5IDE0LjcyNDVDNi41NzI0IDE0LjMzOTQgNi4wODMxMyAxMy42MDk4IDYuMDQ3ODYgMTMuMDQ4MkM1Ljk1MzQ3IDExLjUyODggNi4wMjAwMiA4LjYxOTQ0IDYuMDY2MjEgNy4wNzY5NUM2LjA4MjgxIDYuNTE0NzcgNi41NTU0OCA2LjA0MzQ3IDcuMTE4MDQgNi4wMzA1NUM5LjA4ODYzIDUuOTg0NzMgMTMuMjYzOCA1LjkzNTc5IDEzLjY1MTggNi4zMjQyNUwyMS43MzY5IDEzLjYzOUMyMi4yNTYgMTQuMTU4NSAyMS43ODUxIDE1LjQ3MjQgMjEuMjYyIDE1Ljk5NDZMMTYuMjA5NSAyMS42MTA0Wk05Ljc3NTg1IDguMjY1QzkuMzM1NTEgNy44MjU2NiA4LjYyMzUxIDcuODI1NjYgOC4xODI4IDguMjY1QzcuNzQzNDYgOC43MDU3MSA3Ljc0MzQ2IDkuNDE3MzMgOC4xODI4IDkuODU2NjdDOC42MjM4MiAxMC4yOTY0IDkuMzM1ODIgMTAuMjk2NCA5Ljc3NTg1IDkuODU2NjdDMTAuMjE1NiA5LjQxNzMzIDEwLjIxNTYgOC43MDUzMyA5Ljc3NTg1IDguMjY1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-terminal: url(data:image/svg+xml;base64,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);
  --jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtdGV4dC1lZGl0b3ItaWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xNSAxNUgzdjJoMTJ2LTJ6bTAtOEgzdjJoMTJWN3pNMyAxM2gxOHYtMkgzdjJ6bTAgOGgxOHYtMkgzdjJ6TTMgM3YyaDE4VjNIM3oiLz4KPC9zdmc+Cg==);
  --jp-icon-toc: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik03LDVIMjFWN0g3VjVNNywxM1YxMUgyMVYxM0g3TTQsNC41QTEuNSwxLjUgMCAwLDEgNS41LDZBMS41LDEuNSAwIDAsMSA0LDcuNUExLjUsMS41IDAgMCwxIDIuNSw2QTEuNSwxLjUgMCAwLDEgNCw0LjVNNCwxMC41QTEuNSwxLjUgMCAwLDEgNS41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMy41QTEuNSwxLjUgMCAwLDEgMi41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMC41TTcsMTlWMTdIMjFWMTlIN000LDE2LjVBMS41LDEuNSAwIDAsMSA1LjUsMThBMS41LDEuNSAwIDAsMSA0LDE5LjVBMS41LDEuNSAwIDAsMSAyLjUsMThBMS41LDEuNSAwIDAsMSA0LDE2LjVaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tree-view: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMiAxMVYzaC03djNIOVYzSDJ2OGg3VjhoMnYxMGg0djNoN3YtOGgtN3YzaC0yVjhoMnYzeiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K);
  --jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-user: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE2IDdhNCA0IDAgMTEtOCAwIDQgNCAwIDAxOCAwek0xMiAxNGE3IDcgMCAwMC03IDdoMTRhNyA3IDAgMDAtNy03eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-users: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZlcnNpb249IjEuMSIgdmlld0JveD0iMCAwIDM2IDI0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogPGcgY2xhc3M9ImpwLWljb24zIiB0cmFuc2Zvcm09Im1hdHJpeCgxLjczMjcgMCAwIDEuNzMyNyAtMy42MjgyIC4wOTk1NzcpIiBmaWxsPSIjNjE2MTYxIj4KICA8cGF0aCB0cmFuc2Zvcm09Im1hdHJpeCgxLjUsMCwwLDEuNSwwLC02KSIgZD0ibTEyLjE4NiA3LjUwOThjLTEuMDUzNSAwLTEuOTc1NyAwLjU2NjUtMi40Nzg1IDEuNDEwMiAwLjc1MDYxIDAuMzEyNzcgMS4zOTc0IDAuODI2NDggMS44NzMgMS40NzI3aDMuNDg2M2MwLTEuNTkyLTEuMjg4OS0yLjg4MjgtMi44ODA5LTIuODgyOHoiLz4KICA8cGF0aCBkPSJtMjAuNDY1IDIuMzg5NWEyLjE4ODUgMi4xODg1IDAgMCAxLTIuMTg4NCAyLjE4ODUgMi4xODg1IDIuMTg4NSAwIDAgMS0yLjE4ODUtMi4xODg1IDIuMTg4NSAyLjE4ODUgMCAwIDEgMi4xODg1LTIuMTg4NSAyLjE4ODUgMi4xODg1IDAgMCAxIDIuMTg4NCAyLjE4ODV6Ii8+CiAgPHBhdGggdHJhbnNmb3JtPSJtYXRyaXgoMS41LDAsMCwxLjUsMCwtNikiIGQ9Im0zLjU4OTggOC40MjE5Yy0xLjExMjYgMC0yLjAxMzcgMC45MDExMS0yLjAxMzcgMi4wMTM3aDIuODE0NWMwLjI2Nzk3LTAuMzczMDkgMC41OTA3LTAuNzA0MzUgMC45NTg5OC0wLjk3ODUyLTAuMzQ0MzMtMC42MTY4OC0xLjAwMzEtMS4wMzUyLTEuNzU5OC0xLjAzNTJ6Ii8+CiAgPHBhdGggZD0ibTYuOTE1NCA0LjYyM2ExLjUyOTQgMS41Mjk0IDAgMCAxLTEuNTI5NCAxLjUyOTQgMS41Mjk0IDEuNTI5NCAwIDAgMS0xLjUyOTQtMS41Mjk0IDEuNTI5NCAxLjUyOTQgMCAwIDEgMS41Mjk0LTEuNTI5NCAxLjUyOTQgMS41Mjk0IDAgMCAxIDEuNTI5NCAxLjUyOTR6Ii8+CiAgPHBhdGggZD0ibTYuMTM1IDEzLjUzNWMwLTMuMjM5MiAyLjYyNTktNS44NjUgNS44NjUtNS44NjUgMy4yMzkyIDAgNS44NjUgMi42MjU5IDUuODY1IDUuODY1eiIvPgogIDxjaXJjbGUgY3g9IjEyIiBjeT0iMy43Njg1IiByPSIyLjk2ODUiLz4KIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-word: url(data:image/svg+xml;base64,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);
  --jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K);
}

/* Icon CSS class declarations */

.jp-AddAboveIcon {
  background-image: var(--jp-icon-add-above);
}

.jp-AddBelowIcon {
  background-image: var(--jp-icon-add-below);
}

.jp-AddIcon {
  background-image: var(--jp-icon-add);
}

.jp-BellIcon {
  background-image: var(--jp-icon-bell);
}

.jp-BugDotIcon {
  background-image: var(--jp-icon-bug-dot);
}

.jp-BugIcon {
  background-image: var(--jp-icon-bug);
}

.jp-BuildIcon {
  background-image: var(--jp-icon-build);
}

.jp-CaretDownEmptyIcon {
  background-image: var(--jp-icon-caret-down-empty);
}

.jp-CaretDownEmptyThinIcon {
  background-image: var(--jp-icon-caret-down-empty-thin);
}

.jp-CaretDownIcon {
  background-image: var(--jp-icon-caret-down);
}

.jp-CaretLeftIcon {
  background-image: var(--jp-icon-caret-left);
}

.jp-CaretRightIcon {
  background-image: var(--jp-icon-caret-right);
}

.jp-CaretUpEmptyThinIcon {
  background-image: var(--jp-icon-caret-up-empty-thin);
}

.jp-CaretUpIcon {
  background-image: var(--jp-icon-caret-up);
}

.jp-CaseSensitiveIcon {
  background-image: var(--jp-icon-case-sensitive);
}

.jp-CheckIcon {
  background-image: var(--jp-icon-check);
}

.jp-CircleEmptyIcon {
  background-image: var(--jp-icon-circle-empty);
}

.jp-CircleIcon {
  background-image: var(--jp-icon-circle);
}

.jp-ClearIcon {
  background-image: var(--jp-icon-clear);
}

.jp-CloseIcon {
  background-image: var(--jp-icon-close);
}

.jp-CodeCheckIcon {
  background-image: var(--jp-icon-code-check);
}

.jp-CodeIcon {
  background-image: var(--jp-icon-code);
}

.jp-CollapseAllIcon {
  background-image: var(--jp-icon-collapse-all);
}

.jp-ConsoleIcon {
  background-image: var(--jp-icon-console);
}

.jp-CopyIcon {
  background-image: var(--jp-icon-copy);
}

.jp-CopyrightIcon {
  background-image: var(--jp-icon-copyright);
}

.jp-CutIcon {
  background-image: var(--jp-icon-cut);
}

.jp-DeleteIcon {
  background-image: var(--jp-icon-delete);
}

.jp-DownloadIcon {
  background-image: var(--jp-icon-download);
}

.jp-DuplicateIcon {
  background-image: var(--jp-icon-duplicate);
}

.jp-EditIcon {
  background-image: var(--jp-icon-edit);
}

.jp-EllipsesIcon {
  background-image: var(--jp-icon-ellipses);
}

.jp-ErrorIcon {
  background-image: var(--jp-icon-error);
}

.jp-ExpandAllIcon {
  background-image: var(--jp-icon-expand-all);
}

.jp-ExtensionIcon {
  background-image: var(--jp-icon-extension);
}

.jp-FastForwardIcon {
  background-image: var(--jp-icon-fast-forward);
}

.jp-FileIcon {
  background-image: var(--jp-icon-file);
}

.jp-FileUploadIcon {
  background-image: var(--jp-icon-file-upload);
}

.jp-FilterDotIcon {
  background-image: var(--jp-icon-filter-dot);
}

.jp-FilterIcon {
  background-image: var(--jp-icon-filter);
}

.jp-FilterListIcon {
  background-image: var(--jp-icon-filter-list);
}

.jp-FolderFavoriteIcon {
  background-image: var(--jp-icon-folder-favorite);
}

.jp-FolderIcon {
  background-image: var(--jp-icon-folder);
}

.jp-HomeIcon {
  background-image: var(--jp-icon-home);
}

.jp-Html5Icon {
  background-image: var(--jp-icon-html5);
}

.jp-ImageIcon {
  background-image: var(--jp-icon-image);
}

.jp-InfoIcon {
  background-image: var(--jp-icon-info);
}

.jp-InspectorIcon {
  background-image: var(--jp-icon-inspector);
}

.jp-JsonIcon {
  background-image: var(--jp-icon-json);
}

.jp-JuliaIcon {
  background-image: var(--jp-icon-julia);
}

.jp-JupyterFaviconIcon {
  background-image: var(--jp-icon-jupyter-favicon);
}

.jp-JupyterIcon {
  background-image: var(--jp-icon-jupyter);
}

.jp-JupyterlabWordmarkIcon {
  background-image: var(--jp-icon-jupyterlab-wordmark);
}

.jp-KernelIcon {
  background-image: var(--jp-icon-kernel);
}

.jp-KeyboardIcon {
  background-image: var(--jp-icon-keyboard);
}

.jp-LaunchIcon {
  background-image: var(--jp-icon-launch);
}

.jp-LauncherIcon {
  background-image: var(--jp-icon-launcher);
}

.jp-LineFormIcon {
  background-image: var(--jp-icon-line-form);
}

.jp-LinkIcon {
  background-image: var(--jp-icon-link);
}

.jp-ListIcon {
  background-image: var(--jp-icon-list);
}

.jp-MarkdownIcon {
  background-image: var(--jp-icon-markdown);
}

.jp-MoveDownIcon {
  background-image: var(--jp-icon-move-down);
}

.jp-MoveUpIcon {
  background-image: var(--jp-icon-move-up);
}

.jp-NewFolderIcon {
  background-image: var(--jp-icon-new-folder);
}

.jp-NotTrustedIcon {
  background-image: var(--jp-icon-not-trusted);
}

.jp-NotebookIcon {
  background-image: var(--jp-icon-notebook);
}

.jp-NumberingIcon {
  background-image: var(--jp-icon-numbering);
}

.jp-OfflineBoltIcon {
  background-image: var(--jp-icon-offline-bolt);
}

.jp-PaletteIcon {
  background-image: var(--jp-icon-palette);
}

.jp-PasteIcon {
  background-image: var(--jp-icon-paste);
}

.jp-PdfIcon {
  background-image: var(--jp-icon-pdf);
}

.jp-PythonIcon {
  background-image: var(--jp-icon-python);
}

.jp-RKernelIcon {
  background-image: var(--jp-icon-r-kernel);
}

.jp-ReactIcon {
  background-image: var(--jp-icon-react);
}

.jp-RedoIcon {
  background-image: var(--jp-icon-redo);
}

.jp-RefreshIcon {
  background-image: var(--jp-icon-refresh);
}

.jp-RegexIcon {
  background-image: var(--jp-icon-regex);
}

.jp-RunIcon {
  background-image: var(--jp-icon-run);
}

.jp-RunningIcon {
  background-image: var(--jp-icon-running);
}

.jp-SaveIcon {
  background-image: var(--jp-icon-save);
}

.jp-SearchIcon {
  background-image: var(--jp-icon-search);
}

.jp-SettingsIcon {
  background-image: var(--jp-icon-settings);
}

.jp-ShareIcon {
  background-image: var(--jp-icon-share);
}

.jp-SpreadsheetIcon {
  background-image: var(--jp-icon-spreadsheet);
}

.jp-StopIcon {
  background-image: var(--jp-icon-stop);
}

.jp-TabIcon {
  background-image: var(--jp-icon-tab);
}

.jp-TableRowsIcon {
  background-image: var(--jp-icon-table-rows);
}

.jp-TagIcon {
  background-image: var(--jp-icon-tag);
}

.jp-TerminalIcon {
  background-image: var(--jp-icon-terminal);
}

.jp-TextEditorIcon {
  background-image: var(--jp-icon-text-editor);
}

.jp-TocIcon {
  background-image: var(--jp-icon-toc);
}

.jp-TreeViewIcon {
  background-image: var(--jp-icon-tree-view);
}

.jp-TrustedIcon {
  background-image: var(--jp-icon-trusted);
}

.jp-UndoIcon {
  background-image: var(--jp-icon-undo);
}

.jp-UserIcon {
  background-image: var(--jp-icon-user);
}

.jp-UsersIcon {
  background-image: var(--jp-icon-users);
}

.jp-VegaIcon {
  background-image: var(--jp-icon-vega);
}

.jp-WordIcon {
  background-image: var(--jp-icon-word);
}

.jp-YamlIcon {
  background-image: var(--jp-icon-yaml);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

.jp-Icon,
.jp-MaterialIcon {
  background-position: center;
  background-repeat: no-repeat;
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-cover {
  background-position: center;
  background-repeat: no-repeat;
  background-size: cover;
}

/**
 * (DEPRECATED) Support for specific CSS icon sizes
 */

.jp-Icon-16 {
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-18 {
  background-size: 18px;
  min-width: 18px;
  min-height: 18px;
}

.jp-Icon-20 {
  background-size: 20px;
  min-width: 20px;
  min-height: 20px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.lm-TabBar .lm-TabBar-addButton {
  align-items: center;
  display: flex;
  padding: 4px;
  padding-bottom: 5px;
  margin-right: 1px;
  background-color: var(--jp-layout-color2);
}

.lm-TabBar .lm-TabBar-addButton:hover {
  background-color: var(--jp-layout-color1);
}

.lm-DockPanel-tabBar .lm-TabBar-tab {
  width: var(--jp-private-horizontal-tab-width);
}

.lm-DockPanel-tabBar .lm-TabBar-content {
  flex: unset;
}

.lm-DockPanel-tabBar[data-orientation='horizontal'] {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for icons as inline SVG HTMLElements
 */

/* recolor the primary elements of an icon */
.jp-icon0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-accent0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-accent1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-accent2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-accent3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-accent4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-accent0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-accent1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-accent2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-accent3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-accent4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-none[fill] {
  fill: none;
}

.jp-icon-none[stroke] {
  stroke: none;
}

/* brand icon colors. Same for light and dark */
.jp-icon-brand0[fill] {
  fill: var(--jp-brand-color0);
}

.jp-icon-brand1[fill] {
  fill: var(--jp-brand-color1);
}

.jp-icon-brand2[fill] {
  fill: var(--jp-brand-color2);
}

.jp-icon-brand3[fill] {
  fill: var(--jp-brand-color3);
}

.jp-icon-brand4[fill] {
  fill: var(--jp-brand-color4);
}

.jp-icon-brand0[stroke] {
  stroke: var(--jp-brand-color0);
}

.jp-icon-brand1[stroke] {
  stroke: var(--jp-brand-color1);
}

.jp-icon-brand2[stroke] {
  stroke: var(--jp-brand-color2);
}

.jp-icon-brand3[stroke] {
  stroke: var(--jp-brand-color3);
}

.jp-icon-brand4[stroke] {
  stroke: var(--jp-brand-color4);
}

/* warn icon colors. Same for light and dark */
.jp-icon-warn0[fill] {
  fill: var(--jp-warn-color0);
}

.jp-icon-warn1[fill] {
  fill: var(--jp-warn-color1);
}

.jp-icon-warn2[fill] {
  fill: var(--jp-warn-color2);
}

.jp-icon-warn3[fill] {
  fill: var(--jp-warn-color3);
}

.jp-icon-warn0[stroke] {
  stroke: var(--jp-warn-color0);
}

.jp-icon-warn1[stroke] {
  stroke: var(--jp-warn-color1);
}

.jp-icon-warn2[stroke] {
  stroke: var(--jp-warn-color2);
}

.jp-icon-warn3[stroke] {
  stroke: var(--jp-warn-color3);
}

/* icon colors that contrast well with each other and most backgrounds */
.jp-icon-contrast0[fill] {
  fill: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[fill] {
  fill: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[fill] {
  fill: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[fill] {
  fill: var(--jp-icon-contrast-color3);
}

.jp-icon-contrast0[stroke] {
  stroke: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[stroke] {
  stroke: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[stroke] {
  stroke: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[stroke] {
  stroke: var(--jp-icon-contrast-color3);
}

.jp-icon-dot[fill] {
  fill: var(--jp-warn-color0);
}

.jp-jupyter-icon-color[fill] {
  fill: var(--jp-jupyter-icon-color, var(--jp-warn-color0));
}

.jp-notebook-icon-color[fill] {
  fill: var(--jp-notebook-icon-color, var(--jp-warn-color0));
}

.jp-json-icon-color[fill] {
  fill: var(--jp-json-icon-color, var(--jp-warn-color1));
}

.jp-console-icon-color[fill] {
  fill: var(--jp-console-icon-color, white);
}

.jp-console-icon-background-color[fill] {
  fill: var(--jp-console-icon-background-color, var(--jp-brand-color1));
}

.jp-terminal-icon-color[fill] {
  fill: var(--jp-terminal-icon-color, var(--jp-layout-color2));
}

.jp-terminal-icon-background-color[fill] {
  fill: var(
    --jp-terminal-icon-background-color,
    var(--jp-inverse-layout-color2)
  );
}

.jp-text-editor-icon-color[fill] {
  fill: var(--jp-text-editor-icon-color, var(--jp-inverse-layout-color3));
}

.jp-inspector-icon-color[fill] {
  fill: var(--jp-inspector-icon-color, var(--jp-inverse-layout-color3));
}

/* CSS for icons in selected filebrowser listing items */
.jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

.jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* stylelint-disable selector-max-class, selector-max-compound-selectors */

/**
* TODO: come up with non css-hack solution for showing the busy icon on top
*  of the close icon
* CSS for complex behavior of close icon of tabs in the main area tabbar
*/
.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon3[fill] {
  fill: none;
}

.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: var(--jp-inverse-layout-color3);
}

/* stylelint-enable selector-max-class, selector-max-compound-selectors */

/* CSS for icons in status bar */
#jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

#jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* special handling for splash icon CSS. While the theme CSS reloads during
   splash, the splash icon can loose theming. To prevent that, we set a
   default for its color variable */
:root {
  --jp-warn-color0: var(--md-orange-700);
}

/* not sure what to do with this one, used in filebrowser listing */
.jp-DragIcon {
  margin-right: 4px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for alt colors for icons as inline SVG HTMLElements
 */

/* alt recolor the primary elements of an icon */
.jp-icon-alt .jp-icon0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-alt .jp-icon0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* alt recolor the accent elements of an icon */
.jp-icon-alt .jp-icon-accent0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-alt .jp-icon-accent0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-icon-hoverShow:not(:hover) .jp-icon-hoverShow-content {
  display: none !important;
}

/**
 * Support for hover colors for icons as inline SVG HTMLElements
 */

/**
 * regular colors
 */

/* recolor the primary elements of an icon */
.jp-icon-hover :hover .jp-icon0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-hover :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-hover :hover .jp-icon-none-hover[fill] {
  fill: none;
}

.jp-icon-hover :hover .jp-icon-none-hover[stroke] {
  stroke: none;
}

/**
 * inverse colors
 */

/* inverse recolor the primary elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* inverse recolor the accent elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-IFrame {
  width: 100%;
  height: 100%;
}

.jp-IFrame > iframe {
  border: none;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-IFrame {
  position: relative;
}

body.lm-mod-override-cursor .jp-IFrame::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-HoverBox {
  position: fixed;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FormGroup-content fieldset {
  border: none;
  padding: 0;
  min-width: 0;
  width: 100%;
}

/* stylelint-disable selector-max-type */

.jp-FormGroup-content fieldset .jp-inputFieldWrapper input,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper select,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper textarea {
  font-size: var(--jp-content-font-size2);
  border-color: var(--jp-input-border-color);
  border-style: solid;
  border-radius: var(--jp-border-radius);
  border-width: 1px;
  padding: 6px 8px;
  background: none;
  color: var(--jp-ui-font-color0);
  height: inherit;
}

.jp-FormGroup-content fieldset input[type='checkbox'] {
  position: relative;
  top: 2px;
  margin-left: 0;
}

.jp-FormGroup-content button.jp-mod-styled {
  cursor: pointer;
}

.jp-FormGroup-content .checkbox label {
  cursor: pointer;
  font-size: var(--jp-content-font-size1);
}

.jp-FormGroup-content .jp-root > fieldset > legend {
  display: none;
}

.jp-FormGroup-content .jp-root > fieldset > p {
  display: none;
}

/** copy of `input.jp-mod-styled:focus` style */
.jp-FormGroup-content fieldset input:focus,
.jp-FormGroup-content fieldset select:focus {
  -moz-outline-radius: unset;
  outline: var(--jp-border-width) solid var(--md-blue-500);
  outline-offset: -1px;
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-FormGroup-content fieldset input:hover:not(:focus),
.jp-FormGroup-content fieldset select:hover:not(:focus) {
  background-color: var(--jp-border-color2);
}

/* stylelint-enable selector-max-type */

.jp-FormGroup-content .checkbox .field-description {
  /* Disable default description field for checkbox:
   because other widgets do not have description fields,
   we add descriptions to each widget on the field level.
  */
  display: none;
}

.jp-FormGroup-content #root__description {
  display: none;
}

.jp-FormGroup-content .jp-modifiedIndicator {
  width: 5px;
  background-color: var(--jp-brand-color2);
  margin-top: 0;
  margin-left: calc(var(--jp-private-settingeditor-modifier-indent) * -1);
  flex-shrink: 0;
}

.jp-FormGroup-content .jp-modifiedIndicator.jp-errorIndicator {
  background-color: var(--jp-error-color0);
  margin-right: 0.5em;
}

/* RJSF ARRAY style */

.jp-arrayFieldWrapper legend {
  font-size: var(--jp-content-font-size2);
  color: var(--jp-ui-font-color0);
  flex-basis: 100%;
  padding: 4px 0;
  font-weight: var(--jp-content-heading-font-weight);
  border-bottom: 1px solid var(--jp-border-color2);
}

.jp-arrayFieldWrapper .field-description {
  padding: 4px 0;
  white-space: pre-wrap;
}

.jp-arrayFieldWrapper .array-item {
  width: 100%;
  border: 1px solid var(--jp-border-color2);
  border-radius: 4px;
  margin: 4px;
}

.jp-ArrayOperations {
  display: flex;
  margin-left: 8px;
}

.jp-ArrayOperationsButton {
  margin: 2px;
}

.jp-ArrayOperationsButton .jp-icon3[fill] {
  fill: var(--jp-ui-font-color0);
}

button.jp-ArrayOperationsButton.jp-mod-styled:disabled {
  cursor: not-allowed;
  opacity: 0.5;
}

/* RJSF form validation error */

.jp-FormGroup-content .validationErrors {
  color: var(--jp-error-color0);
}

/* Hide panel level error as duplicated the field level error */
.jp-FormGroup-content .panel.errors {
  display: none;
}

/* RJSF normal content (settings-editor) */

.jp-FormGroup-contentNormal {
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-FormGroup-contentItem {
  margin-left: 7px;
  color: var(--jp-ui-font-color0);
}

.jp-FormGroup-contentNormal .jp-FormGroup-description {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-default {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-fieldLabel {
  font-size: var(--jp-content-font-size1);
  font-weight: normal;
  min-width: 120px;
}

.jp-FormGroup-contentNormal fieldset:not(:first-child) {
  margin-left: 7px;
}

.jp-FormGroup-contentNormal .field-array-of-string .array-item {
  /* Display `jp-ArrayOperations` buttons side-by-side with content except
    for small screens where flex-wrap will place them one below the other.
  */
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-objectFieldWrapper .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

/* RJSF compact content (metadata-form) */

.jp-FormGroup-content.jp-FormGroup-contentCompact {
  width: 100%;
}

.jp-FormGroup-contentCompact .form-group {
  display: flex;
  padding: 0.5em 0.2em 0.5em 0;
}

.jp-FormGroup-contentCompact
  .jp-FormGroup-compactTitle
  .jp-FormGroup-description {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color2);
}

.jp-FormGroup-contentCompact .jp-FormGroup-fieldLabel {
  padding-bottom: 0.3em;
}

.jp-FormGroup-contentCompact .jp-inputFieldWrapper .form-control {
  width: 100%;
  box-sizing: border-box;
}

.jp-FormGroup-contentCompact .jp-arrayFieldWrapper .jp-FormGroup-compactTitle {
  padding-bottom: 7px;
}

.jp-FormGroup-contentCompact
  .jp-objectFieldWrapper
  .jp-objectFieldWrapper
  .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

.jp-FormGroup-contentCompact ul.error-detail {
  margin-block-start: 0.5em;
  margin-block-end: 0.5em;
  padding-inline-start: 1em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-SidePanel {
  display: flex;
  flex-direction: column;
  min-width: var(--jp-sidebar-min-width);
  overflow-y: auto;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  font-size: var(--jp-ui-font-size1);
}

.jp-SidePanel-header {
  flex: 0 0 auto;
  display: flex;
  border-bottom: var(--jp-border-width) solid var(--jp-border-color2);
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin: 0;
  padding: 2px;
  text-transform: uppercase;
}

.jp-SidePanel-toolbar {
  flex: 0 0 auto;
}

.jp-SidePanel-content {
  flex: 1 1 auto;
}

.jp-SidePanel-toolbar,
.jp-AccordionPanel-toolbar {
  height: var(--jp-private-toolbar-height);
}

.jp-SidePanel-toolbar.jp-Toolbar-micro {
  display: none;
}

.lm-AccordionPanel .jp-AccordionPanel-title {
  box-sizing: border-box;
  line-height: 25px;
  margin: 0;
  display: flex;
  align-items: center;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  font-size: var(--jp-ui-font-size0);
}

.jp-AccordionPanel-title {
  cursor: pointer;
  user-select: none;
  -moz-user-select: none;
  -webkit-user-select: none;
  text-transform: uppercase;
}

.lm-AccordionPanel[data-orientation='horizontal'] > .jp-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleLabel {
  user-select: none;
  text-overflow: ellipsis;
  white-space: nowrap;
  overflow: hidden;
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleCollapser {
  transform: rotate(-90deg);
  margin: auto 0;
  height: 16px;
}

.jp-AccordionPanel-title.lm-mod-expanded .lm-AccordionPanel-titleCollapser {
  transform: rotate(0deg);
}

.lm-AccordionPanel .jp-AccordionPanel-toolbar {
  background: none;
  box-shadow: none;
  border: none;
  margin-left: auto;
}

.lm-AccordionPanel .lm-SplitPanel-handle:hover {
  background: var(--jp-layout-color3);
}

.jp-text-truncated {
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Spinner {
  position: absolute;
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 10;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-layout-color0);
  outline: none;
}

.jp-SpinnerContent {
  font-size: 10px;
  margin: 50px auto;
  text-indent: -9999em;
  width: 3em;
  height: 3em;
  border-radius: 50%;
  background: var(--jp-brand-color3);
  background: linear-gradient(
    to right,
    #f37626 10%,
    rgba(255, 255, 255, 0) 42%
  );
  position: relative;
  animation: load3 1s infinite linear, fadeIn 1s;
}

.jp-SpinnerContent::before {
  width: 50%;
  height: 50%;
  background: #f37626;
  border-radius: 100% 0 0;
  position: absolute;
  top: 0;
  left: 0;
  content: '';
}

.jp-SpinnerContent::after {
  background: var(--jp-layout-color0);
  width: 75%;
  height: 75%;
  border-radius: 50%;
  content: '';
  margin: auto;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
}

@keyframes fadeIn {
  0% {
    opacity: 0;
  }

  100% {
    opacity: 1;
  }
}

@keyframes load3 {
  0% {
    transform: rotate(0deg);
  }

  100% {
    transform: rotate(360deg);
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

button.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: none;
  box-sizing: border-box;
  text-align: center;
  line-height: 32px;
  height: 32px;
  padding: 0 12px;
  letter-spacing: 0.8px;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input.jp-mod-styled {
  background: var(--jp-input-background);
  height: 28px;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color1);
  padding-left: 7px;
  padding-right: 7px;
  font-size: var(--jp-ui-font-size2);
  color: var(--jp-ui-font-color0);
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input[type='checkbox'].jp-mod-styled {
  appearance: checkbox;
  -webkit-appearance: checkbox;
  -moz-appearance: checkbox;
  height: auto;
}

input.jp-mod-styled:focus {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-select-wrapper {
  display: flex;
  position: relative;
  flex-direction: column;
  padding: 1px;
  background-color: var(--jp-layout-color1);
  box-sizing: border-box;
  margin-bottom: 12px;
}

.jp-select-wrapper:not(.multiple) {
  height: 28px;
}

.jp-select-wrapper.jp-mod-focused select.jp-mod-styled {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-input-active-background);
}

select.jp-mod-styled:hover {
  cursor: pointer;
  color: var(--jp-ui-font-color0);
  background-color: var(--jp-input-hover-background);
  box-shadow: inset 0 0 1px rgba(0, 0, 0, 0.5);
}

select.jp-mod-styled {
  flex: 1 1 auto;
  width: 100%;
  font-size: var(--jp-ui-font-size2);
  background: var(--jp-input-background);
  color: var(--jp-ui-font-color0);
  padding: 0 25px 0 8px;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

select.jp-mod-styled:not([multiple]) {
  height: 32px;
}

select.jp-mod-styled[multiple] {
  max-height: 200px;
  overflow-y: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-switch {
  display: flex;
  align-items: center;
  padding-left: 4px;
  padding-right: 4px;
  font-size: var(--jp-ui-font-size1);
  background-color: transparent;
  color: var(--jp-ui-font-color1);
  border: none;
  height: 20px;
}

.jp-switch:hover {
  background-color: var(--jp-layout-color2);
}

.jp-switch-label {
  margin-right: 5px;
  font-family: var(--jp-ui-font-family);
}

.jp-switch-track {
  cursor: pointer;
  background-color: var(--jp-switch-color, var(--jp-border-color1));
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 34px;
  height: 16px;
  width: 35px;
  position: relative;
}

.jp-switch-track::before {
  content: '';
  position: absolute;
  height: 10px;
  width: 10px;
  margin: 3px;
  left: 0;
  background-color: var(--jp-ui-inverse-font-color1);
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 50%;
}

.jp-switch[aria-checked='true'] .jp-switch-track {
  background-color: var(--jp-switch-true-position-color, var(--jp-warn-color0));
}

.jp-switch[aria-checked='true'] .jp-switch-track::before {
  /* track width (35) - margins (3 + 3) - thumb width (10) */
  left: 19px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toolbar-height: calc(
    28px + var(--jp-border-width)
  ); /* leave 28px for content */
}

.jp-Toolbar {
  color: var(--jp-ui-font-color1);
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: 2px;
  z-index: 8;
  overflow-x: hidden;
}

/* Toolbar items */

.jp-Toolbar > .jp-Toolbar-item.jp-Toolbar-spacer {
  flex-grow: 1;
  flex-shrink: 1;
}

.jp-Toolbar-item.jp-Toolbar-kernelStatus {
  display: inline-block;
  width: 32px;
  background-repeat: no-repeat;
  background-position: center;
  background-size: 16px;
}

.jp-Toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  display: flex;
  padding-left: 1px;
  padding-right: 1px;
  font-size: var(--jp-ui-font-size1);
  line-height: var(--jp-private-toolbar-height);
  height: 100%;
}

/* Toolbar buttons */

/* This is the div we use to wrap the react component into a Widget */
div.jp-ToolbarButton {
  color: transparent;
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0;
  margin: 0;
}

button.jp-ToolbarButtonComponent {
  background: var(--jp-layout-color1);
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0 6px;
  margin: 0;
  height: 24px;
  border-radius: var(--jp-border-radius);
  display: flex;
  align-items: center;
  text-align: center;
  font-size: 14px;
  min-width: unset;
  min-height: unset;
}

button.jp-ToolbarButtonComponent:disabled {
  opacity: 0.4;
}

button.jp-ToolbarButtonComponent > span {
  padding: 0;
  flex: 0 0 auto;
}

button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label {
  font-size: var(--jp-ui-font-size1);
  line-height: 100%;
  padding-left: 2px;
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar.jp-Toolbar-micro {
  padding: 0;
  min-height: 0;
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar {
  border: none;
  box-shadow: none;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-WindowedPanel-outer {
  position: relative;
  overflow-y: auto;
}

.jp-WindowedPanel-inner {
  position: relative;
}

.jp-WindowedPanel-window {
  position: absolute;
  left: 0;
  right: 0;
  overflow: visible;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* Sibling imports */

body {
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
}

/* Disable native link decoration styles everywhere outside of dialog boxes */
a {
  text-decoration: unset;
  color: unset;
}

a:hover {
  text-decoration: unset;
  color: unset;
}

/* Accessibility for links inside dialog box text */
.jp-Dialog-content a {
  text-decoration: revert;
  color: var(--jp-content-link-color);
}

.jp-Dialog-content a:hover {
  text-decoration: revert;
}

/* Styles for ui-components */
.jp-Button {
  color: var(--jp-ui-font-color2);
  border-radius: var(--jp-border-radius);
  padding: 0 12px;
  font-size: var(--jp-ui-font-size1);

  /* Copy from blueprint 3 */
  display: inline-flex;
  flex-direction: row;
  border: none;
  cursor: pointer;
  align-items: center;
  justify-content: center;
  text-align: left;
  vertical-align: middle;
  min-height: 30px;
  min-width: 30px;
}

.jp-Button:disabled {
  cursor: not-allowed;
}

.jp-Button:empty {
  padding: 0 !important;
}

.jp-Button.jp-mod-small {
  min-height: 24px;
  min-width: 24px;
  font-size: 12px;
  padding: 0 7px;
}

/* Use our own theme for hover styles */
.jp-Button.jp-mod-minimal:hover {
  background-color: var(--jp-layout-color2);
}

.jp-Button.jp-mod-minimal {
  background: none;
}

.jp-InputGroup {
  display: block;
  position: relative;
}

.jp-InputGroup input {
  box-sizing: border-box;
  border: none;
  border-radius: 0;
  background-color: transparent;
  color: var(--jp-ui-font-color0);
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
  padding-bottom: 0;
  padding-top: 0;
  padding-left: 10px;
  padding-right: 28px;
  position: relative;
  width: 100%;
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  font-size: 14px;
  font-weight: 400;
  height: 30px;
  line-height: 30px;
  outline: none;
  vertical-align: middle;
}

.jp-InputGroup input:focus {
  box-shadow: inset 0 0 0 var(--jp-border-width)
      var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-InputGroup input:disabled {
  cursor: not-allowed;
  resize: block;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input:disabled ~ span {
  cursor: not-allowed;
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input::placeholder,
input::placeholder {
  color: var(--jp-ui-font-color2);
}

.jp-InputGroupAction {
  position: absolute;
  bottom: 1px;
  right: 0;
  padding: 6px;
}

.jp-HTMLSelect.jp-DefaultStyle select {
  background-color: initial;
  border: none;
  border-radius: 0;
  box-shadow: none;
  color: var(--jp-ui-font-color0);
  display: block;
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  height: 24px;
  line-height: 14px;
  padding: 0 25px 0 10px;
  text-align: left;
  -moz-appearance: none;
  -webkit-appearance: none;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
  cursor: not-allowed;
  resize: block;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled ~ span {
  cursor: not-allowed;
}

/* Use our own theme for hover and option styles */
/* stylelint-disable-next-line selector-max-type */
.jp-HTMLSelect.jp-DefaultStyle select:hover,
.jp-HTMLSelect.jp-DefaultStyle select > option {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color0);
}

select {
  box-sizing: border-box;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-StatusBar-Widget {
  display: flex;
  align-items: center;
  background: var(--jp-layout-color2);
  min-height: var(--jp-statusbar-height);
  justify-content: space-between;
  padding: 0 10px;
}

.jp-StatusBar-Left {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-StatusBar-Middle {
  display: flex;
  align-items: center;
}

.jp-StatusBar-Right {
  display: flex;
  align-items: center;
  flex-direction: row-reverse;
}

.jp-StatusBar-Item {
  max-height: var(--jp-statusbar-height);
  margin: 0 2px;
  height: var(--jp-statusbar-height);
  white-space: nowrap;
  text-overflow: ellipsis;
  color: var(--jp-ui-font-color1);
  padding: 0 6px;
}

.jp-mod-highlighted:hover {
  background-color: var(--jp-layout-color3);
}

.jp-mod-clicked {
  background-color: var(--jp-brand-color1);
}

.jp-mod-clicked:hover {
  background-color: var(--jp-brand-color0);
}

.jp-mod-clicked .jp-StatusBar-TextItem {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-StatusBar-HoverItem {
  box-shadow: '0px 4px 4px rgba(0, 0, 0, 0.25)';
}

.jp-StatusBar-TextItem {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  line-height: 24px;
  color: var(--jp-ui-font-color1);
}

.jp-StatusBar-GroupItem {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-Statusbar-ProgressCircle svg {
  display: block;
  margin: 0 auto;
  width: 16px;
  height: 24px;
  align-self: normal;
}

.jp-Statusbar-ProgressCircle path {
  fill: var(--jp-inverse-layout-color3);
}

.jp-Statusbar-ProgressBar-progress-bar {
  height: 10px;
  width: 100px;
  border: solid 0.25px var(--jp-brand-color2);
  border-radius: 3px;
  overflow: hidden;
  align-self: center;
}

.jp-Statusbar-ProgressBar-progress-bar > div {
  background-color: var(--jp-brand-color2);
  background-image: linear-gradient(
    -45deg,
    rgba(255, 255, 255, 0.2) 25%,
    transparent 25%,
    transparent 50%,
    rgba(255, 255, 255, 0.2) 50%,
    rgba(255, 255, 255, 0.2) 75%,
    transparent 75%,
    transparent
  );
  background-size: 40px 40px;
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 14px;
  color: #fff;
  text-align: center;
  animation: jp-Statusbar-ExecutionTime-progress-bar 2s linear infinite;
}

.jp-Statusbar-ProgressBar-progress-bar p {
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
  font-size: var(--jp-ui-font-size1);
  line-height: 10px;
  width: 100px;
}

@keyframes jp-Statusbar-ExecutionTime-progress-bar {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 40px 40px;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-commandpalette-search-height: 28px;
}

/*-----------------------------------------------------------------------------
| Overall styles
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  padding-bottom: 0;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Modal variant
|----------------------------------------------------------------------------*/

.jp-ModalCommandPalette {
  position: absolute;
  z-index: 10000;
  top: 38px;
  left: 30%;
  margin: 0;
  padding: 4px;
  width: 40%;
  box-shadow: var(--jp-elevation-z4);
  border-radius: 4px;
  background: var(--jp-layout-color0);
}

.jp-ModalCommandPalette .lm-CommandPalette {
  max-height: 40vh;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-close-icon::after {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-header {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-item {
  margin-left: 4px;
  margin-right: 4px;
}

.jp-ModalCommandPalette
  .lm-CommandPalette
  .lm-CommandPalette-item.lm-mod-disabled {
  display: none;
}

/*-----------------------------------------------------------------------------
| Search
|----------------------------------------------------------------------------*/

.lm-CommandPalette-search {
  padding: 4px;
  background-color: var(--jp-layout-color1);
  z-index: 2;
}

.lm-CommandPalette-wrapper {
  overflow: overlay;
  padding: 0 9px;
  background-color: var(--jp-input-active-background);
  height: 30px;
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
}

.lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper {
  box-shadow: inset 0 0 0 1px var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-SearchIconGroup {
  color: white;
  background-color: var(--jp-brand-color1);
  position: absolute;
  top: 4px;
  right: 4px;
  padding: 5px 5px 1px;
}

.jp-SearchIconGroup svg {
  height: 20px;
  width: 20px;
}

.jp-SearchIconGroup .jp-icon3[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-input {
  background: transparent;
  width: calc(100% - 18px);
  float: left;
  border: none;
  outline: none;
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  line-height: var(--jp-private-commandpalette-search-height);
}

.lm-CommandPalette-input::-webkit-input-placeholder,
.lm-CommandPalette-input::-moz-placeholder,
.lm-CommandPalette-input:-ms-input-placeholder {
  color: var(--jp-ui-font-color2);
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Results
|----------------------------------------------------------------------------*/

.lm-CommandPalette-header:first-child {
  margin-top: 0;
}

.lm-CommandPalette-header {
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin-top: 8px;
  padding: 8px 0 8px 12px;
  text-transform: uppercase;
}

.lm-CommandPalette-header.lm-mod-active {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-header > mark {
  background-color: transparent;
  font-weight: bold;
  color: var(--jp-ui-font-color1);
}

.lm-CommandPalette-item {
  padding: 4px 12px 4px 4px;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  font-weight: 400;
  display: flex;
}

.lm-CommandPalette-item.lm-mod-disabled {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item.lm-mod-active {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item.lm-mod-active .lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-inverse-font-color0);
}

.lm-CommandPalette-item.lm-mod-active .jp-icon-selectable[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled) {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled) {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-itemContent {
  overflow: hidden;
}

.lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.lm-CommandPalette-item.lm-mod-disabled mark {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item .lm-CommandPalette-itemIcon {
  margin: 0 4px 0 0;
  position: relative;
  width: 16px;
  top: 2px;
  flex: 0 0 auto;
}

.lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon {
  opacity: 0.6;
}

.lm-CommandPalette-item .lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemCaption {
  display: none;
}

.lm-CommandPalette-content {
  background-color: var(--jp-layout-color1);
}

.lm-CommandPalette-content:empty::after {
  content: 'No results';
  margin: auto;
  margin-top: 20px;
  width: 100px;
  display: block;
  font-size: var(--jp-ui-font-size2);
  font-family: var(--jp-ui-font-family);
  font-weight: lighter;
}

.lm-CommandPalette-emptyMessage {
  text-align: center;
  margin-top: 24px;
  line-height: 1.32;
  padding: 0 8px;
  color: var(--jp-content-font-color3);
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Dialog {
  position: absolute;
  z-index: 10000;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  top: 0;
  left: 0;
  margin: 0;
  padding: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-dialog-background);
}

.jp-Dialog-content {
  display: flex;
  flex-direction: column;
  margin-left: auto;
  margin-right: auto;
  background: var(--jp-layout-color1);
  padding: 24px 24px 12px;
  min-width: 300px;
  min-height: 150px;
  max-width: 1000px;
  max-height: 500px;
  box-sizing: border-box;
  box-shadow: var(--jp-elevation-z20);
  word-wrap: break-word;
  border-radius: var(--jp-border-radius);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color1);
  resize: both;
}

.jp-Dialog-content.jp-Dialog-content-small {
  max-width: 500px;
}

.jp-Dialog-button {
  overflow: visible;
}

button.jp-Dialog-button:focus {
  outline: 1px solid var(--jp-brand-color1);
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button:focus::-moz-focus-inner {
  border: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus {
  outline: 1px solid var(--jp-accept-color-normal, var(--jp-brand-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus {
  outline: 1px solid var(--jp-warn-color-normal, var(--jp-error-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline: 1px solid var(--jp-reject-color-normal, var(--md-grey-600));
}

button.jp-Dialog-close-button {
  padding: 0;
  height: 100%;
  min-width: unset;
  min-height: unset;
}

.jp-Dialog-header {
  display: flex;
  justify-content: space-between;
  flex: 0 0 auto;
  padding-bottom: 12px;
  font-size: var(--jp-ui-font-size3);
  font-weight: 400;
  color: var(--jp-ui-font-color1);
}

.jp-Dialog-body {
  display: flex;
  flex-direction: column;
  flex: 1 1 auto;
  font-size: var(--jp-ui-font-size1);
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

.jp-Dialog-footer {
  display: flex;
  flex-direction: row;
  justify-content: flex-end;
  align-items: center;
  flex: 0 0 auto;
  margin-left: -12px;
  margin-right: -12px;
  padding: 12px;
}

.jp-Dialog-checkbox {
  padding-right: 5px;
}

.jp-Dialog-checkbox > input:focus-visible {
  outline: 1px solid var(--jp-input-active-border-color);
  outline-offset: 1px;
}

.jp-Dialog-spacer {
  flex: 1 1 auto;
}

.jp-Dialog-title {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.jp-Dialog-body > .jp-select-wrapper {
  width: 100%;
}

.jp-Dialog-body > button {
  padding: 0 16px;
}

.jp-Dialog-body > label {
  line-height: 1.4;
  color: var(--jp-ui-font-color0);
}

.jp-Dialog-button.jp-mod-styled:not(:last-child) {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Input-Boolean-Dialog {
  flex-direction: row-reverse;
  align-items: end;
  width: 100%;
}

.jp-Input-Boolean-Dialog > label {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MainAreaWidget > :focus {
  outline: none;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error {
  padding: 6px;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error > pre {
  width: auto;
  padding: 10px;
  background: var(--jp-error-color3);
  border: var(--jp-border-width) solid var(--jp-error-color1);
  border-radius: var(--jp-border-radius);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  white-space: pre-wrap;
  word-wrap: break-word;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/**
 * google-material-color v1.2.6
 * https://github.com/danlevan/google-material-color
 */
:root {
  --md-red-50: #ffebee;
  --md-red-100: #ffcdd2;
  --md-red-200: #ef9a9a;
  --md-red-300: #e57373;
  --md-red-400: #ef5350;
  --md-red-500: #f44336;
  --md-red-600: #e53935;
  --md-red-700: #d32f2f;
  --md-red-800: #c62828;
  --md-red-900: #b71c1c;
  --md-red-A100: #ff8a80;
  --md-red-A200: #ff5252;
  --md-red-A400: #ff1744;
  --md-red-A700: #d50000;
  --md-pink-50: #fce4ec;
  --md-pink-100: #f8bbd0;
  --md-pink-200: #f48fb1;
  --md-pink-300: #f06292;
  --md-pink-400: #ec407a;
  --md-pink-500: #e91e63;
  --md-pink-600: #d81b60;
  --md-pink-700: #c2185b;
  --md-pink-800: #ad1457;
  --md-pink-900: #880e4f;
  --md-pink-A100: #ff80ab;
  --md-pink-A200: #ff4081;
  --md-pink-A400: #f50057;
  --md-pink-A700: #c51162;
  --md-purple-50: #f3e5f5;
  --md-purple-100: #e1bee7;
  --md-purple-200: #ce93d8;
  --md-purple-300: #ba68c8;
  --md-purple-400: #ab47bc;
  --md-purple-500: #9c27b0;
  --md-purple-600: #8e24aa;
  --md-purple-700: #7b1fa2;
  --md-purple-800: #6a1b9a;
  --md-purple-900: #4a148c;
  --md-purple-A100: #ea80fc;
  --md-purple-A200: #e040fb;
  --md-purple-A400: #d500f9;
  --md-purple-A700: #a0f;
  --md-deep-purple-50: #ede7f6;
  --md-deep-purple-100: #d1c4e9;
  --md-deep-purple-200: #b39ddb;
  --md-deep-purple-300: #9575cd;
  --md-deep-purple-400: #7e57c2;
  --md-deep-purple-500: #673ab7;
  --md-deep-purple-600: #5e35b1;
  --md-deep-purple-700: #512da8;
  --md-deep-purple-800: #4527a0;
  --md-deep-purple-900: #311b92;
  --md-deep-purple-A100: #b388ff;
  --md-deep-purple-A200: #7c4dff;
  --md-deep-purple-A400: #651fff;
  --md-deep-purple-A700: #6200ea;
  --md-indigo-50: #e8eaf6;
  --md-indigo-100: #c5cae9;
  --md-indigo-200: #9fa8da;
  --md-indigo-300: #7986cb;
  --md-indigo-400: #5c6bc0;
  --md-indigo-500: #3f51b5;
  --md-indigo-600: #3949ab;
  --md-indigo-700: #303f9f;
  --md-indigo-800: #283593;
  --md-indigo-900: #1a237e;
  --md-indigo-A100: #8c9eff;
  --md-indigo-A200: #536dfe;
  --md-indigo-A400: #3d5afe;
  --md-indigo-A700: #304ffe;
  --md-blue-50: #e3f2fd;
  --md-blue-100: #bbdefb;
  --md-blue-200: #90caf9;
  --md-blue-300: #64b5f6;
  --md-blue-400: #42a5f5;
  --md-blue-500: #2196f3;
  --md-blue-600: #1e88e5;
  --md-blue-700: #1976d2;
  --md-blue-800: #1565c0;
  --md-blue-900: #0d47a1;
  --md-blue-A100: #82b1ff;
  --md-blue-A200: #448aff;
  --md-blue-A400: #2979ff;
  --md-blue-A700: #2962ff;
  --md-light-blue-50: #e1f5fe;
  --md-light-blue-100: #b3e5fc;
  --md-light-blue-200: #81d4fa;
  --md-light-blue-300: #4fc3f7;
  --md-light-blue-400: #29b6f6;
  --md-light-blue-500: #03a9f4;
  --md-light-blue-600: #039be5;
  --md-light-blue-700: #0288d1;
  --md-light-blue-800: #0277bd;
  --md-light-blue-900: #01579b;
  --md-light-blue-A100: #80d8ff;
  --md-light-blue-A200: #40c4ff;
  --md-light-blue-A400: #00b0ff;
  --md-light-blue-A700: #0091ea;
  --md-cyan-50: #e0f7fa;
  --md-cyan-100: #b2ebf2;
  --md-cyan-200: #80deea;
  --md-cyan-300: #4dd0e1;
  --md-cyan-400: #26c6da;
  --md-cyan-500: #00bcd4;
  --md-cyan-600: #00acc1;
  --md-cyan-700: #0097a7;
  --md-cyan-800: #00838f;
  --md-cyan-900: #006064;
  --md-cyan-A100: #84ffff;
  --md-cyan-A200: #18ffff;
  --md-cyan-A400: #00e5ff;
  --md-cyan-A700: #00b8d4;
  --md-teal-50: #e0f2f1;
  --md-teal-100: #b2dfdb;
  --md-teal-200: #80cbc4;
  --md-teal-300: #4db6ac;
  --md-teal-400: #26a69a;
  --md-teal-500: #009688;
  --md-teal-600: #00897b;
  --md-teal-700: #00796b;
  --md-teal-800: #00695c;
  --md-teal-900: #004d40;
  --md-teal-A100: #a7ffeb;
  --md-teal-A200: #64ffda;
  --md-teal-A400: #1de9b6;
  --md-teal-A700: #00bfa5;
  --md-green-50: #e8f5e9;
  --md-green-100: #c8e6c9;
  --md-green-200: #a5d6a7;
  --md-green-300: #81c784;
  --md-green-400: #66bb6a;
  --md-green-500: #4caf50;
  --md-green-600: #43a047;
  --md-green-700: #388e3c;
  --md-green-800: #2e7d32;
  --md-green-900: #1b5e20;
  --md-green-A100: #b9f6ca;
  --md-green-A200: #69f0ae;
  --md-green-A400: #00e676;
  --md-green-A700: #00c853;
  --md-light-green-50: #f1f8e9;
  --md-light-green-100: #dcedc8;
  --md-light-green-200: #c5e1a5;
  --md-light-green-300: #aed581;
  --md-light-green-400: #9ccc65;
  --md-light-green-500: #8bc34a;
  --md-light-green-600: #7cb342;
  --md-light-green-700: #689f38;
  --md-light-green-800: #558b2f;
  --md-light-green-900: #33691e;
  --md-light-green-A100: #ccff90;
  --md-light-green-A200: #b2ff59;
  --md-light-green-A400: #76ff03;
  --md-light-green-A700: #64dd17;
  --md-lime-50: #f9fbe7;
  --md-lime-100: #f0f4c3;
  --md-lime-200: #e6ee9c;
  --md-lime-300: #dce775;
  --md-lime-400: #d4e157;
  --md-lime-500: #cddc39;
  --md-lime-600: #c0ca33;
  --md-lime-700: #afb42b;
  --md-lime-800: #9e9d24;
  --md-lime-900: #827717;
  --md-lime-A100: #f4ff81;
  --md-lime-A200: #eeff41;
  --md-lime-A400: #c6ff00;
  --md-lime-A700: #aeea00;
  --md-yellow-50: #fffde7;
  --md-yellow-100: #fff9c4;
  --md-yellow-200: #fff59d;
  --md-yellow-300: #fff176;
  --md-yellow-400: #ffee58;
  --md-yellow-500: #ffeb3b;
  --md-yellow-600: #fdd835;
  --md-yellow-700: #fbc02d;
  --md-yellow-800: #f9a825;
  --md-yellow-900: #f57f17;
  --md-yellow-A100: #ffff8d;
  --md-yellow-A200: #ff0;
  --md-yellow-A400: #ffea00;
  --md-yellow-A700: #ffd600;
  --md-amber-50: #fff8e1;
  --md-amber-100: #ffecb3;
  --md-amber-200: #ffe082;
  --md-amber-300: #ffd54f;
  --md-amber-400: #ffca28;
  --md-amber-500: #ffc107;
  --md-amber-600: #ffb300;
  --md-amber-700: #ffa000;
  --md-amber-800: #ff8f00;
  --md-amber-900: #ff6f00;
  --md-amber-A100: #ffe57f;
  --md-amber-A200: #ffd740;
  --md-amber-A400: #ffc400;
  --md-amber-A700: #ffab00;
  --md-orange-50: #fff3e0;
  --md-orange-100: #ffe0b2;
  --md-orange-200: #ffcc80;
  --md-orange-300: #ffb74d;
  --md-orange-400: #ffa726;
  --md-orange-500: #ff9800;
  --md-orange-600: #fb8c00;
  --md-orange-700: #f57c00;
  --md-orange-800: #ef6c00;
  --md-orange-900: #e65100;
  --md-orange-A100: #ffd180;
  --md-orange-A200: #ffab40;
  --md-orange-A400: #ff9100;
  --md-orange-A700: #ff6d00;
  --md-deep-orange-50: #fbe9e7;
  --md-deep-orange-100: #ffccbc;
  --md-deep-orange-200: #ffab91;
  --md-deep-orange-300: #ff8a65;
  --md-deep-orange-400: #ff7043;
  --md-deep-orange-500: #ff5722;
  --md-deep-orange-600: #f4511e;
  --md-deep-orange-700: #e64a19;
  --md-deep-orange-800: #d84315;
  --md-deep-orange-900: #bf360c;
  --md-deep-orange-A100: #ff9e80;
  --md-deep-orange-A200: #ff6e40;
  --md-deep-orange-A400: #ff3d00;
  --md-deep-orange-A700: #dd2c00;
  --md-brown-50: #efebe9;
  --md-brown-100: #d7ccc8;
  --md-brown-200: #bcaaa4;
  --md-brown-300: #a1887f;
  --md-brown-400: #8d6e63;
  --md-brown-500: #795548;
  --md-brown-600: #6d4c41;
  --md-brown-700: #5d4037;
  --md-brown-800: #4e342e;
  --md-brown-900: #3e2723;
  --md-grey-50: #fafafa;
  --md-grey-100: #f5f5f5;
  --md-grey-200: #eee;
  --md-grey-300: #e0e0e0;
  --md-grey-400: #bdbdbd;
  --md-grey-500: #9e9e9e;
  --md-grey-600: #757575;
  --md-grey-700: #616161;
  --md-grey-800: #424242;
  --md-grey-900: #212121;
  --md-blue-grey-50: #eceff1;
  --md-blue-grey-100: #cfd8dc;
  --md-blue-grey-200: #b0bec5;
  --md-blue-grey-300: #90a4ae;
  --md-blue-grey-400: #78909c;
  --md-blue-grey-500: #607d8b;
  --md-blue-grey-600: #546e7a;
  --md-blue-grey-700: #455a64;
  --md-blue-grey-800: #37474f;
  --md-blue-grey-900: #263238;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| RenderedText
|----------------------------------------------------------------------------*/

:root {
  /* This is the padding value to fill the gaps between lines containing spans with background color. */
  --jp-private-code-span-padding: calc(
    (var(--jp-code-line-height) - 1) * var(--jp-code-font-size) / 2
  );
}

.jp-RenderedText {
  text-align: left;
  padding-left: var(--jp-code-padding);
  line-height: var(--jp-code-line-height);
  font-family: var(--jp-code-font-family);
}

.jp-RenderedText pre,
.jp-RenderedJavaScript pre,
.jp-RenderedHTMLCommon pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
  border: none;
  margin: 0;
  padding: 0;
}

.jp-RenderedText pre a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* console foregrounds and backgrounds */
.jp-RenderedText pre .ansi-black-fg {
  color: #3e424d;
}

.jp-RenderedText pre .ansi-red-fg {
  color: #e75c58;
}

.jp-RenderedText pre .ansi-green-fg {
  color: #00a250;
}

.jp-RenderedText pre .ansi-yellow-fg {
  color: #ddb62b;
}

.jp-RenderedText pre .ansi-blue-fg {
  color: #208ffb;
}

.jp-RenderedText pre .ansi-magenta-fg {
  color: #d160c4;
}

.jp-RenderedText pre .ansi-cyan-fg {
  color: #60c6c8;
}

.jp-RenderedText pre .ansi-white-fg {
  color: #c5c1b4;
}

.jp-RenderedText pre .ansi-black-bg {
  background-color: #3e424d;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-bg {
  background-color: #e75c58;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-bg {
  background-color: #00a250;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-bg {
  background-color: #ddb62b;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-bg {
  background-color: #208ffb;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-bg {
  background-color: #d160c4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-bg {
  background-color: #60c6c8;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-bg {
  background-color: #c5c1b4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-black-intense-fg {
  color: #282c36;
}

.jp-RenderedText pre .ansi-red-intense-fg {
  color: #b22b31;
}

.jp-RenderedText pre .ansi-green-intense-fg {
  color: #007427;
}

.jp-RenderedText pre .ansi-yellow-intense-fg {
  color: #b27d12;
}

.jp-RenderedText pre .ansi-blue-intense-fg {
  color: #0065ca;
}

.jp-RenderedText pre .ansi-magenta-intense-fg {
  color: #a03196;
}

.jp-RenderedText pre .ansi-cyan-intense-fg {
  color: #258f8f;
}

.jp-RenderedText pre .ansi-white-intense-fg {
  color: #a1a6b2;
}

.jp-RenderedText pre .ansi-black-intense-bg {
  background-color: #282c36;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-intense-bg {
  background-color: #b22b31;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-intense-bg {
  background-color: #007427;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-intense-bg {
  background-color: #b27d12;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-intense-bg {
  background-color: #0065ca;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-intense-bg {
  background-color: #a03196;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-intense-bg {
  background-color: #258f8f;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-intense-bg {
  background-color: #a1a6b2;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-default-inverse-fg {
  color: var(--jp-ui-inverse-font-color0);
}

.jp-RenderedText pre .ansi-default-inverse-bg {
  background-color: var(--jp-inverse-layout-color0);
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-bold {
  font-weight: bold;
}

.jp-RenderedText pre .ansi-underline {
  text-decoration: underline;
}

.jp-RenderedText[data-mime-type='application/vnd.jupyter.stderr'] {
  background: var(--jp-rendermime-error-background);
  padding-top: var(--jp-code-padding);
}

/*-----------------------------------------------------------------------------
| RenderedLatex
|----------------------------------------------------------------------------*/

.jp-RenderedLatex {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);
}

/* Left-justify outputs.*/
.jp-OutputArea-output.jp-RenderedLatex {
  padding: var(--jp-code-padding);
  text-align: left;
}

/*-----------------------------------------------------------------------------
| RenderedHTML
|----------------------------------------------------------------------------*/

.jp-RenderedHTMLCommon {
  color: var(--jp-content-font-color1);
  font-family: var(--jp-content-font-family);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);

  /* Give a bit more R padding on Markdown text to keep line lengths reasonable */
  padding-right: 20px;
}

.jp-RenderedHTMLCommon em {
  font-style: italic;
}

.jp-RenderedHTMLCommon strong {
  font-weight: bold;
}

.jp-RenderedHTMLCommon u {
  text-decoration: underline;
}

.jp-RenderedHTMLCommon a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* Headings */

.jp-RenderedHTMLCommon h1,
.jp-RenderedHTMLCommon h2,
.jp-RenderedHTMLCommon h3,
.jp-RenderedHTMLCommon h4,
.jp-RenderedHTMLCommon h5,
.jp-RenderedHTMLCommon h6 {
  line-height: var(--jp-content-heading-line-height);
  font-weight: var(--jp-content-heading-font-weight);
  font-style: normal;
  margin: var(--jp-content-heading-margin-top) 0
    var(--jp-content-heading-margin-bottom) 0;
}

.jp-RenderedHTMLCommon h1:first-child,
.jp-RenderedHTMLCommon h2:first-child,
.jp-RenderedHTMLCommon h3:first-child,
.jp-RenderedHTMLCommon h4:first-child,
.jp-RenderedHTMLCommon h5:first-child,
.jp-RenderedHTMLCommon h6:first-child {
  margin-top: calc(0.5 * var(--jp-content-heading-margin-top));
}

.jp-RenderedHTMLCommon h1:last-child,
.jp-RenderedHTMLCommon h2:last-child,
.jp-RenderedHTMLCommon h3:last-child,
.jp-RenderedHTMLCommon h4:last-child,
.jp-RenderedHTMLCommon h5:last-child,
.jp-RenderedHTMLCommon h6:last-child {
  margin-bottom: calc(0.5 * var(--jp-content-heading-margin-bottom));
}

.jp-RenderedHTMLCommon h1 {
  font-size: var(--jp-content-font-size5);
}

.jp-RenderedHTMLCommon h2 {
  font-size: var(--jp-content-font-size4);
}

.jp-RenderedHTMLCommon h3 {
  font-size: var(--jp-content-font-size3);
}

.jp-RenderedHTMLCommon h4 {
  font-size: var(--jp-content-font-size2);
}

.jp-RenderedHTMLCommon h5 {
  font-size: var(--jp-content-font-size1);
}

.jp-RenderedHTMLCommon h6 {
  font-size: var(--jp-content-font-size0);
}

/* Lists */

/* stylelint-disable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon ul:not(.list-inline),
.jp-RenderedHTMLCommon ol:not(.list-inline) {
  padding-left: 2em;
}

.jp-RenderedHTMLCommon ul {
  list-style: disc;
}

.jp-RenderedHTMLCommon ul ul {
  list-style: square;
}

.jp-RenderedHTMLCommon ul ul ul {
  list-style: circle;
}

.jp-RenderedHTMLCommon ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol ol {
  list-style: upper-alpha;
}

.jp-RenderedHTMLCommon ol ol ol {
  list-style: lower-alpha;
}

.jp-RenderedHTMLCommon ol ol ol ol {
  list-style: lower-roman;
}

.jp-RenderedHTMLCommon ol ol ol ol ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol,
.jp-RenderedHTMLCommon ul {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon ul ul,
.jp-RenderedHTMLCommon ul ol,
.jp-RenderedHTMLCommon ol ul,
.jp-RenderedHTMLCommon ol ol {
  margin-bottom: 0;
}

/* stylelint-enable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon hr {
  color: var(--jp-border-color2);
  background-color: var(--jp-border-color1);
  margin-top: 1em;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon > pre {
  margin: 1.5em 2em;
}

.jp-RenderedHTMLCommon pre,
.jp-RenderedHTMLCommon code {
  border: 0;
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  line-height: var(--jp-code-line-height);
  padding: 0;
  white-space: pre-wrap;
}

.jp-RenderedHTMLCommon :not(pre) > code {
  background-color: var(--jp-layout-color2);
  padding: 1px 5px;
}

/* Tables */

.jp-RenderedHTMLCommon table {
  border-collapse: collapse;
  border-spacing: 0;
  border: none;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  table-layout: fixed;
  margin-left: auto;
  margin-bottom: 1em;
  margin-right: auto;
}

.jp-RenderedHTMLCommon thead {
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  vertical-align: bottom;
}

.jp-RenderedHTMLCommon td,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon tr {
  vertical-align: middle;
  padding: 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}

.jp-RenderedMarkdown.jp-RenderedHTMLCommon td,
.jp-RenderedMarkdown.jp-RenderedHTMLCommon th {
  max-width: none;
}

:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon td,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon th,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon tr {
  text-align: right;
}

.jp-RenderedHTMLCommon th {
  font-weight: bold;
}

.jp-RenderedHTMLCommon tbody tr:nth-child(odd) {
  background: var(--jp-layout-color0);
}

.jp-RenderedHTMLCommon tbody tr:nth-child(even) {
  background: var(--jp-rendermime-table-row-background);
}

.jp-RenderedHTMLCommon tbody tr:hover {
  background: var(--jp-rendermime-table-row-hover-background);
}

.jp-RenderedHTMLCommon p {
  text-align: left;
  margin: 0;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon img {
  -moz-force-broken-image-icon: 1;
}

/* Restrict to direct children as other images could be nested in other content. */
.jp-RenderedHTMLCommon > img {
  display: block;
  margin-left: 0;
  margin-right: 0;
  margin-bottom: 1em;
}

/* Change color behind transparent images if they need it... */
[data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-light-background {
  background-color: var(--jp-inverse-layout-color1);
}

[data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-dark-background {
  background-color: var(--jp-inverse-layout-color1);
}

.jp-RenderedHTMLCommon img,
.jp-RenderedImage img,
.jp-RenderedHTMLCommon svg,
.jp-RenderedSVG svg {
  max-width: 100%;
  height: auto;
}

.jp-RenderedHTMLCommon img.jp-mod-unconfined,
.jp-RenderedImage img.jp-mod-unconfined,
.jp-RenderedHTMLCommon svg.jp-mod-unconfined,
.jp-RenderedSVG svg.jp-mod-unconfined {
  max-width: none;
}

.jp-RenderedHTMLCommon .alert {
  padding: var(--jp-notebook-padding);
  border: var(--jp-border-width) solid transparent;
  border-radius: var(--jp-border-radius);
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon .alert-info {
  color: var(--jp-info-color0);
  background-color: var(--jp-info-color3);
  border-color: var(--jp-info-color2);
}

.jp-RenderedHTMLCommon .alert-info hr {
  border-color: var(--jp-info-color3);
}

.jp-RenderedHTMLCommon .alert-info > p:last-child,
.jp-RenderedHTMLCommon .alert-info > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-warning {
  color: var(--jp-warn-color0);
  background-color: var(--jp-warn-color3);
  border-color: var(--jp-warn-color2);
}

.jp-RenderedHTMLCommon .alert-warning hr {
  border-color: var(--jp-warn-color3);
}

.jp-RenderedHTMLCommon .alert-warning > p:last-child,
.jp-RenderedHTMLCommon .alert-warning > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-success {
  color: var(--jp-success-color0);
  background-color: var(--jp-success-color3);
  border-color: var(--jp-success-color2);
}

.jp-RenderedHTMLCommon .alert-success hr {
  border-color: var(--jp-success-color3);
}

.jp-RenderedHTMLCommon .alert-success > p:last-child,
.jp-RenderedHTMLCommon .alert-success > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-danger {
  color: var(--jp-error-color0);
  background-color: var(--jp-error-color3);
  border-color: var(--jp-error-color2);
}

.jp-RenderedHTMLCommon .alert-danger hr {
  border-color: var(--jp-error-color3);
}

.jp-RenderedHTMLCommon .alert-danger > p:last-child,
.jp-RenderedHTMLCommon .alert-danger > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon blockquote {
  margin: 1em 2em;
  padding: 0 1em;
  border-left: 5px solid var(--jp-border-color2);
}

a.jp-InternalAnchorLink {
  visibility: hidden;
  margin-left: 8px;
  color: var(--md-blue-800);
}

h1:hover .jp-InternalAnchorLink,
h2:hover .jp-InternalAnchorLink,
h3:hover .jp-InternalAnchorLink,
h4:hover .jp-InternalAnchorLink,
h5:hover .jp-InternalAnchorLink,
h6:hover .jp-InternalAnchorLink {
  visibility: visible;
}

.jp-RenderedHTMLCommon kbd {
  background-color: var(--jp-rendermime-table-row-background);
  border: 1px solid var(--jp-border-color0);
  border-bottom-color: var(--jp-border-color2);
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
  display: inline-block;
  font-size: var(--jp-ui-font-size0);
  line-height: 1em;
  padding: 0.2em 0.5em;
}

/* Most direct children of .jp-RenderedHTMLCommon have a margin-bottom of 1.0.
 * At the bottom of cells this is a bit too much as there is also spacing
 * between cells. Going all the way to 0 gets too tight between markdown and
 * code cells.
 */
.jp-RenderedHTMLCommon > *:last-child {
  margin-bottom: 0.5em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-cursor-backdrop {
  position: fixed;
  width: 200px;
  height: 200px;
  margin-top: -100px;
  margin-left: -100px;
  will-change: transform;
  z-index: 100;
}

.lm-mod-drag-image {
  will-change: transform;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-lineFormSearch {
  padding: 4px 12px;
  background-color: var(--jp-layout-color2);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
  font-size: var(--jp-ui-font-size1);
}

.jp-lineFormCaption {
  font-size: var(--jp-ui-font-size0);
  line-height: var(--jp-ui-font-size1);
  margin-top: 4px;
  color: var(--jp-ui-font-color0);
}

.jp-baseLineForm {
  border: none;
  border-radius: 0;
  position: absolute;
  background-size: 16px;
  background-repeat: no-repeat;
  background-position: center;
  outline: none;
}

.jp-lineFormButtonContainer {
  top: 4px;
  right: 8px;
  height: 24px;
  padding: 0 12px;
  width: 12px;
}

.jp-lineFormButtonIcon {
  top: 0;
  right: 0;
  background-color: var(--jp-brand-color1);
  height: 100%;
  width: 100%;
  box-sizing: border-box;
  padding: 4px 6px;
}

.jp-lineFormButton {
  top: 0;
  right: 0;
  background-color: transparent;
  height: 100%;
  width: 100%;
  box-sizing: border-box;
}

.jp-lineFormWrapper {
  overflow: hidden;
  padding: 0 8px;
  border: 1px solid var(--jp-border-color0);
  background-color: var(--jp-input-active-background);
  height: 22px;
}

.jp-lineFormWrapperFocusWithin {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-lineFormInput {
  background: transparent;
  width: 200px;
  height: 100%;
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  line-height: 28px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-JSONEditor {
  display: flex;
  flex-direction: column;
  width: 100%;
}

.jp-JSONEditor-host {
  flex: 1 1 auto;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  background: var(--jp-layout-color0);
  min-height: 50px;
  padding: 1px;
}

.jp-JSONEditor.jp-mod-error .jp-JSONEditor-host {
  border-color: red;
  outline-color: red;
}

.jp-JSONEditor-header {
  display: flex;
  flex: 1 0 auto;
  padding: 0 0 0 12px;
}

.jp-JSONEditor-header label {
  flex: 0 0 auto;
}

.jp-JSONEditor-commitButton {
  height: 16px;
  width: 16px;
  background-size: 18px;
  background-repeat: no-repeat;
  background-position: center;
}

.jp-JSONEditor-host.jp-mod-focused {
  background-color: var(--jp-input-active-background);
  border: 1px solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

.jp-Editor.jp-mod-dropTarget {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/
.jp-DocumentSearch-input {
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  font-size: var(--jp-ui-font-size1);
  background-color: var(--jp-layout-color0);
  font-family: var(--jp-ui-font-family);
  padding: 2px 1px;
  resize: none;
}

.jp-DocumentSearch-overlay {
  position: absolute;
  background-color: var(--jp-toolbar-background);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  border-left: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  top: 0;
  right: 0;
  z-index: 7;
  min-width: 405px;
  padding: 2px;
  font-size: var(--jp-ui-font-size1);

  --jp-private-document-search-button-height: 20px;
}

.jp-DocumentSearch-overlay button {
  background-color: var(--jp-toolbar-background);
  outline: 0;
}

.jp-DocumentSearch-overlay button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-overlay button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-overlay-row {
  display: flex;
  align-items: center;
  margin-bottom: 2px;
}

.jp-DocumentSearch-button-content {
  display: inline-block;
  cursor: pointer;
  box-sizing: border-box;
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-button-content svg {
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-input-wrapper {
  border: var(--jp-border-width) solid var(--jp-border-color0);
  display: flex;
  background-color: var(--jp-layout-color0);
  margin: 2px;
}

.jp-DocumentSearch-input-wrapper:focus-within {
  border-color: var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper {
  all: initial;
  overflow: hidden;
  display: inline-block;
  border: none;
  box-sizing: border-box;
}

.jp-DocumentSearch-toggle-wrapper {
  width: 14px;
  height: 14px;
}

.jp-DocumentSearch-button-wrapper {
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
}

.jp-DocumentSearch-toggle-wrapper:focus,
.jp-DocumentSearch-button-wrapper:focus {
  outline: var(--jp-border-width) solid
    var(--jp-cell-editor-active-border-color);
  outline-offset: -1px;
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper,
.jp-DocumentSearch-button-content:focus {
  outline: none;
}

.jp-DocumentSearch-toggle-placeholder {
  width: 5px;
}

.jp-DocumentSearch-input-button::before {
  display: block;
  padding-top: 100%;
}

.jp-DocumentSearch-input-button-off {
  opacity: var(--jp-search-toggle-off-opacity);
}

.jp-DocumentSearch-input-button-off:hover {
  opacity: var(--jp-search-toggle-hover-opacity);
}

.jp-DocumentSearch-input-button-on {
  opacity: var(--jp-search-toggle-on-opacity);
}

.jp-DocumentSearch-index-counter {
  padding-left: 10px;
  padding-right: 10px;
  user-select: none;
  min-width: 35px;
  display: inline-block;
}

.jp-DocumentSearch-up-down-wrapper {
  display: inline-block;
  padding-right: 2px;
  margin-left: auto;
  white-space: nowrap;
}

.jp-DocumentSearch-spacer {
  margin-left: auto;
}

.jp-DocumentSearch-up-down-wrapper button {
  outline: 0;
  border: none;
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
  vertical-align: middle;
  margin: 1px 5px 2px;
}

.jp-DocumentSearch-up-down-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-up-down-button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-filter-button {
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-filter-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled:hover {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-search-options {
  padding: 0 8px;
  margin-left: 3px;
  width: 100%;
  display: grid;
  justify-content: start;
  grid-template-columns: 1fr 1fr;
  align-items: center;
  justify-items: stretch;
}

.jp-DocumentSearch-search-filter-disabled {
  color: var(--jp-ui-font-color2);
}

.jp-DocumentSearch-search-filter {
  display: flex;
  align-items: center;
  user-select: none;
}

.jp-DocumentSearch-regex-error {
  color: var(--jp-error-color0);
}

.jp-DocumentSearch-replace-button-wrapper {
  overflow: hidden;
  display: inline-block;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color0);
  margin: auto 2px;
  padding: 1px 4px;
  height: calc(var(--jp-private-document-search-button-height) + 2px);
}

.jp-DocumentSearch-replace-button-wrapper:focus {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-replace-button {
  display: inline-block;
  text-align: center;
  cursor: pointer;
  box-sizing: border-box;
  color: var(--jp-ui-font-color1);

  /* height - 2 * (padding of wrapper) */
  line-height: calc(var(--jp-private-document-search-button-height) - 2px);
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-replace-button:focus {
  outline: none;
}

.jp-DocumentSearch-replace-wrapper-class {
  margin-left: 14px;
  display: flex;
}

.jp-DocumentSearch-replace-toggle {
  border: none;
  background-color: var(--jp-toolbar-background);
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-replace-toggle:hover {
  background-color: var(--jp-layout-color2);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.cm-editor {
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  border: 0;
  border-radius: 0;
  height: auto;

  /* Changed to auto to autogrow */
}

.cm-editor pre {
  padding: 0 var(--jp-code-padding);
}

.jp-CodeMirrorEditor[data-type='inline'] .cm-dialog {
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
}

.jp-CodeMirrorEditor {
  cursor: text;
}

/* When zoomed out 67% and 33% on a screen of 1440 width x 900 height */
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width1) solid
      var(--jp-editor-cursor-color);
  }
}

/* When zoomed out less than 33% */
@media screen and (min-width: 4320px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width2) solid
      var(--jp-editor-cursor-color);
  }
}

.cm-editor.jp-mod-readOnly .cm-cursor {
  display: none;
}

.jp-CollaboratorCursor {
  border-left: 5px solid transparent;
  border-right: 5px solid transparent;
  border-top: none;
  border-bottom: 3px solid;
  background-clip: content-box;
  margin-left: -5px;
  margin-right: -5px;
}

.cm-searching,
.cm-searching span {
  /* `.cm-searching span`: we need to override syntax highlighting */
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.cm-searching::selection,
.cm-searching span::selection {
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.jp-current-match > .cm-searching,
.jp-current-match > .cm-searching span,
.cm-searching > .jp-current-match,
.cm-searching > .jp-current-match span {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.jp-current-match > .cm-searching::selection,
.cm-searching > .jp-current-match::selection,
.jp-current-match > .cm-searching span::selection {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.cm-trailingspace {
  background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAYAAAB4ka1VAAAAsElEQVQIHQGlAFr/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7+r3zKmT0/+pk9P/7+r3zAAAAAAAAAAABAAAAAAAAAAA6OPzM+/q9wAAAAAA6OPzMwAAAAAAAAAAAgAAAAAAAAAAGR8NiRQaCgAZIA0AGR8NiQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQyoYJ/SY80UAAAAASUVORK5CYII=);
  background-position: center left;
  background-repeat: repeat-x;
}

.jp-CollaboratorCursor-hover {
  position: absolute;
  z-index: 1;
  transform: translateX(-50%);
  color: white;
  border-radius: 3px;
  padding-left: 4px;
  padding-right: 4px;
  padding-top: 1px;
  padding-bottom: 1px;
  text-align: center;
  font-size: var(--jp-ui-font-size1);
  white-space: nowrap;
}

.jp-CodeMirror-ruler {
  border-left: 1px dashed var(--jp-border-color2);
}

/* Styles for shared cursors (remote cursor locations and selected ranges) */
.jp-CodeMirrorEditor .cm-ySelectionCaret {
  position: relative;
  border-left: 1px solid black;
  margin-left: -1px;
  margin-right: -1px;
  box-sizing: border-box;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret > .cm-ySelectionInfo {
  white-space: nowrap;
  position: absolute;
  top: -1.15em;
  padding-bottom: 0.05em;
  left: -1px;
  font-size: 0.95em;
  font-family: var(--jp-ui-font-family);
  font-weight: bold;
  line-height: normal;
  user-select: none;
  color: white;
  padding-left: 2px;
  padding-right: 2px;
  z-index: 101;
  transition: opacity 0.3s ease-in-out;
}

.jp-CodeMirrorEditor .cm-ySelectionInfo {
  transition-delay: 0.7s;
  opacity: 0;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret:hover > .cm-ySelectionInfo {
  opacity: 1;
  transition-delay: 0s;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MimeDocument {
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-filebrowser-button-height: 28px;
  --jp-private-filebrowser-button-width: 48px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FileBrowser .jp-SidePanel-content {
  display: flex;
  flex-direction: column;
}

.jp-FileBrowser-toolbar.jp-Toolbar {
  flex-wrap: wrap;
  row-gap: 12px;
  border-bottom: none;
  height: auto;
  margin: 8px 12px 0;
  box-shadow: none;
  padding: 0;
  justify-content: flex-start;
}

.jp-FileBrowser-Panel {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
}

.jp-BreadCrumbs {
  flex: 0 0 auto;
  margin: 8px 12px;
}

.jp-BreadCrumbs-item {
  margin: 0 2px;
  padding: 0 2px;
  border-radius: var(--jp-border-radius);
  cursor: pointer;
}

.jp-BreadCrumbs-item:hover {
  background-color: var(--jp-layout-color2);
}

.jp-BreadCrumbs-item:first-child {
  margin-left: 0;
}

.jp-BreadCrumbs-item.jp-mod-dropTarget {
  background-color: var(--jp-brand-color2);
  opacity: 0.7;
}

/*-----------------------------------------------------------------------------
| Buttons
|----------------------------------------------------------------------------*/

.jp-FileBrowser-toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  padding-left: 0;
  padding-right: 2px;
  align-items: center;
  height: unset;
}

.jp-FileBrowser-toolbar > .jp-Toolbar-item .jp-ToolbarButtonComponent {
  width: 40px;
}

/*-----------------------------------------------------------------------------
| Other styles
|----------------------------------------------------------------------------*/

.jp-FileDialog.jp-mod-conflict input {
  color: var(--jp-error-color1);
}

.jp-FileDialog .jp-new-name-title {
  margin-top: 12px;
}

.jp-LastModified-hidden {
  display: none;
}

.jp-FileSize-hidden {
  display: none;
}

.jp-FileBrowser .lm-AccordionPanel > h3:first-child {
  display: none;
}

/*-----------------------------------------------------------------------------
| DirListing
|----------------------------------------------------------------------------*/

.jp-DirListing {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
  outline: 0;
}

.jp-DirListing-header {
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  align-items: center;
  overflow: hidden;
  border-top: var(--jp-border-width) solid var(--jp-border-color2);
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
}

.jp-DirListing-headerItem {
  padding: 4px 12px 2px;
  font-weight: 500;
}

.jp-DirListing-headerItem:hover {
  background: var(--jp-layout-color2);
}

.jp-DirListing-headerItem.jp-id-name {
  flex: 1 0 84px;
}

.jp-DirListing-headerItem.jp-id-modified {
  flex: 0 0 112px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-DirListing-headerItem.jp-id-filesize {
  flex: 0 0 75px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-id-narrow {
  display: none;
  flex: 0 0 5px;
  padding: 4px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
  color: var(--jp-border-color2);
}

.jp-DirListing-narrow .jp-id-narrow {
  display: block;
}

.jp-DirListing-narrow .jp-id-modified,
.jp-DirListing-narrow .jp-DirListing-itemModified {
  display: none;
}

.jp-DirListing-headerItem.jp-mod-selected {
  font-weight: 600;
}

/* increase specificity to override bundled default */
.jp-DirListing-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  list-style-type: none;
  overflow: auto;
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-content mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.jp-DirListing-content .jp-DirListing-item.jp-mod-selected mark {
  color: var(--jp-ui-inverse-font-color0);
}

/* Style the directory listing content when a user drops a file to upload */
.jp-DirListing.jp-mod-native-drop .jp-DirListing-content {
  outline: 5px dashed rgba(128, 128, 128, 0.5);
  outline-offset: -10px;
  cursor: copy;
}

.jp-DirListing-item {
  display: flex;
  flex-direction: row;
  align-items: center;
  padding: 4px 12px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-DirListing-checkboxWrapper {
  /* Increases hit area of checkbox. */
  padding: 4px;
}

.jp-DirListing-header
  .jp-DirListing-checkboxWrapper
  + .jp-DirListing-headerItem {
  padding-left: 4px;
}

.jp-DirListing-content .jp-DirListing-checkboxWrapper {
  position: relative;
  left: -4px;
  margin: -4px 0 -4px -8px;
}

.jp-DirListing-checkboxWrapper.jp-mod-visible {
  visibility: visible;
}

/* For devices that support hovering, hide checkboxes until hovered, selected...
*/
@media (hover: hover) {
  .jp-DirListing-checkboxWrapper {
    visibility: hidden;
  }

  .jp-DirListing-item:hover .jp-DirListing-checkboxWrapper,
  .jp-DirListing-item.jp-mod-selected .jp-DirListing-checkboxWrapper {
    visibility: visible;
  }
}

.jp-DirListing-item[data-is-dot] {
  opacity: 75%;
}

.jp-DirListing-item.jp-mod-selected {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.jp-DirListing-item.jp-mod-dropTarget {
  background: var(--jp-brand-color3);
}

.jp-DirListing-item:hover:not(.jp-mod-selected) {
  background: var(--jp-layout-color2);
}

.jp-DirListing-itemIcon {
  flex: 0 0 20px;
  margin-right: 4px;
}

.jp-DirListing-itemText {
  flex: 1 0 64px;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  user-select: none;
}

.jp-DirListing-itemText:focus {
  outline-width: 2px;
  outline-color: var(--jp-inverse-layout-color1);
  outline-style: solid;
  outline-offset: 1px;
}

.jp-DirListing-item.jp-mod-selected .jp-DirListing-itemText:focus {
  outline-color: var(--jp-layout-color1);
}

.jp-DirListing-itemModified {
  flex: 0 0 125px;
  text-align: right;
}

.jp-DirListing-itemFileSize {
  flex: 0 0 90px;
  text-align: right;
}

.jp-DirListing-editor {
  flex: 1 0 64px;
  outline: none;
  border: none;
  color: var(--jp-ui-font-color1);
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon::before {
  color: var(--jp-success-color1);
  content: '\25CF';
  font-size: 8px;
  position: absolute;
  left: -8px;
}

.jp-DirListing-item.jp-mod-running.jp-mod-selected
  .jp-DirListing-itemIcon::before {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-DirListing-item.lm-mod-drag-image,
.jp-DirListing-item.jp-mod-selected.lm-mod-drag-image {
  font-size: var(--jp-ui-font-size1);
  padding-left: 4px;
  margin-left: 4px;
  width: 160px;
  background-color: var(--jp-ui-inverse-font-color2);
  box-shadow: var(--jp-elevation-z2);
  border-radius: 0;
  color: var(--jp-ui-font-color1);
  transform: translateX(-40%) translateY(-58%);
}

.jp-Document {
  min-width: 120px;
  min-height: 120px;
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Main OutputArea
| OutputArea has a list of Outputs
|----------------------------------------------------------------------------*/

.jp-OutputArea {
  overflow-y: auto;
}

.jp-OutputArea-child {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-OutputPrompt {
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-outprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
  opacity: var(--jp-cell-prompt-opacity);

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-OutputArea-prompt {
  display: table-cell;
  vertical-align: top;
}

.jp-OutputArea-output {
  display: table-cell;
  width: 100%;
  height: auto;
  overflow: auto;
  user-select: text;
  -moz-user-select: text;
  -webkit-user-select: text;
  -ms-user-select: text;
}

.jp-OutputArea .jp-RenderedText {
  padding-left: 1ch;
}

/**
 * Prompt overlay.
 */

.jp-OutputArea-promptOverlay {
  position: absolute;
  top: 0;
  width: var(--jp-cell-prompt-width);
  height: 100%;
  opacity: 0.5;
}

.jp-OutputArea-promptOverlay:hover {
  background: var(--jp-layout-color2);
  box-shadow: inset 0 0 1px var(--jp-inverse-layout-color0);
  cursor: zoom-out;
}

.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay:hover {
  cursor: zoom-in;
}

/**
 * Isolated output.
 */
.jp-OutputArea-output.jp-mod-isolated {
  width: 100%;
  display: block;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated {
  position: relative;
}

body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/* pre */

.jp-OutputArea-output pre {
  border: none;
  margin: 0;
  padding: 0;
  overflow-x: auto;
  overflow-y: auto;
  word-break: break-all;
  word-wrap: break-word;
  white-space: pre-wrap;
}

/* tables */

.jp-OutputArea-output.jp-RenderedHTMLCommon table {
  margin-left: 0;
  margin-right: 0;
}

/* description lists */

.jp-OutputArea-output dl,
.jp-OutputArea-output dt,
.jp-OutputArea-output dd {
  display: block;
}

.jp-OutputArea-output dl {
  width: 100%;
  overflow: hidden;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dt {
  font-weight: bold;
  float: left;
  width: 20%;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dd {
  float: left;
  width: 80%;
  padding: 0;
  margin: 0;
}

.jp-TrimmedOutputs pre {
  background: var(--jp-layout-color3);
  font-size: calc(var(--jp-code-font-size) * 1.4);
  text-align: center;
  text-transform: uppercase;
}

/* Hide the gutter in case of
 *  - nested output areas (e.g. in the case of output widgets)
 *  - mirrored output areas
 */
.jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt {
  display: none;
}

/* Hide empty lines in the output area, for instance due to cleared widgets */
.jp-OutputArea-prompt:empty {
  padding: 0;
  border: 0;
}

/*-----------------------------------------------------------------------------
| executeResult is added to any Output-result for the display of the object
| returned by a cell
|----------------------------------------------------------------------------*/

.jp-OutputArea-output.jp-OutputArea-executeResult {
  margin-left: 0;
  width: 100%;
}

/* Text output with the Out[] prompt needs a top padding to match the
 * alignment of the Out[] prompt itself.
 */
.jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output {
  padding-top: var(--jp-code-padding);
  border-top: var(--jp-border-width) solid transparent;
}

/*-----------------------------------------------------------------------------
| The Stdin output
|----------------------------------------------------------------------------*/

.jp-Stdin-prompt {
  color: var(--jp-content-font-color0);
  padding-right: var(--jp-code-padding);
  vertical-align: baseline;
  flex: 0 0 auto;
}

.jp-Stdin-input {
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  color: inherit;
  background-color: inherit;
  width: 42%;
  min-width: 200px;

  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;

  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0 0.25em;
  margin: 0 0.25em;
  flex: 0 0 70%;
}

.jp-Stdin-input::placeholder {
  opacity: 0;
}

.jp-Stdin-input:focus {
  box-shadow: none;
}

.jp-Stdin-input:focus::placeholder {
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Output Area View
|----------------------------------------------------------------------------*/

.jp-LinkedOutputView .jp-OutputArea {
  height: 100%;
  display: block;
}

.jp-LinkedOutputView .jp-OutputArea-output:only-child {
  height: 100%;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

@media print {
  .jp-OutputArea-child {
    break-inside: avoid-page;
  }
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-OutputPrompt {
    display: table-row;
    text-align: left;
  }

  .jp-OutputArea-child .jp-OutputArea-output {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }
}

/* Trimmed outputs warning */
.jp-TrimmedOutputs > a {
  margin: 10px;
  text-decoration: none;
  cursor: pointer;
}

.jp-TrimmedOutputs > a:hover {
  text-decoration: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Table of Contents
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toc-active-width: 4px;
}

.jp-TableOfContents {
  display: flex;
  flex-direction: column;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  height: 100%;
}

.jp-TableOfContents-placeholder {
  text-align: center;
}

.jp-TableOfContents-placeholderContent {
  color: var(--jp-content-font-color2);
  padding: 8px;
}

.jp-TableOfContents-placeholderContent > h3 {
  margin-bottom: var(--jp-content-heading-margin-bottom);
}

.jp-TableOfContents .jp-SidePanel-content {
  overflow-y: auto;
}

.jp-TableOfContents-tree {
  margin: 4px;
}

.jp-TableOfContents ol {
  list-style-type: none;
}

/* stylelint-disable-next-line selector-max-type */
.jp-TableOfContents li > ol {
  /* Align left border with triangle icon center */
  padding-left: 11px;
}

.jp-TableOfContents-content {
  /* left margin for the active heading indicator */
  margin: 0 0 0 var(--jp-private-toc-active-width);
  padding: 0;
  background-color: var(--jp-layout-color1);
}

.jp-tocItem {
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-tocItem-heading {
  display: flex;
  cursor: pointer;
}

.jp-tocItem-heading:hover {
  background-color: var(--jp-layout-color2);
}

.jp-tocItem-content {
  display: block;
  padding: 4px 0;
  white-space: nowrap;
  text-overflow: ellipsis;
  overflow-x: hidden;
}

.jp-tocItem-collapser {
  height: 20px;
  margin: 2px 2px 0;
  padding: 0;
  background: none;
  border: none;
  cursor: pointer;
}

.jp-tocItem-collapser:hover {
  background-color: var(--jp-layout-color3);
}

/* Active heading indicator */

.jp-tocItem-heading::before {
  content: ' ';
  background: transparent;
  width: var(--jp-private-toc-active-width);
  height: 24px;
  position: absolute;
  left: 0;
  border-radius: var(--jp-border-radius);
}

.jp-tocItem-heading.jp-tocItem-active::before {
  background-color: var(--jp-brand-color1);
}

.jp-tocItem-heading:hover.jp-tocItem-active::before {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapser {
  flex: 0 0 var(--jp-cell-collapser-width);
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
  border-radius: var(--jp-border-radius);
  opacity: 1;
}

.jp-Collapser-child {
  display: block;
  width: 100%;
  box-sizing: border-box;

  /* height: 100% doesn't work because the height of its parent is computed from content */
  position: absolute;
  top: 0;
  bottom: 0;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Hiding collapsers in print mode.

Note: input and output wrappers have "display: block" propery in print mode.
*/

@media print {
  .jp-Collapser {
    display: none;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Header/Footer
|----------------------------------------------------------------------------*/

/* Hidden by zero height by default */
.jp-CellHeader,
.jp-CellFooter {
  height: 0;
  width: 100%;
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Input
|----------------------------------------------------------------------------*/

/* All input areas */
.jp-InputArea {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-InputArea-editor {
  display: table-cell;
  overflow: hidden;
  vertical-align: top;

  /* This is the non-active, default styling */
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  background: var(--jp-cell-editor-background);
}

.jp-InputPrompt {
  display: table-cell;
  vertical-align: top;
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-inprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  opacity: var(--jp-cell-prompt-opacity);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-InputArea-editor {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }

  .jp-InputPrompt {
    display: table-row;
    text-align: left;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Placeholder {
  display: table;
  table-layout: fixed;
  width: 100%;
}

.jp-Placeholder-prompt {
  display: table-cell;
  box-sizing: border-box;
}

.jp-Placeholder-content {
  display: table-cell;
  padding: 4px 6px;
  border: 1px solid transparent;
  border-radius: 0;
  background: none;
  box-sizing: border-box;
  cursor: pointer;
}

.jp-Placeholder-contentContainer {
  display: flex;
}

.jp-Placeholder-content:hover,
.jp-InputPlaceholder > .jp-Placeholder-content:hover {
  border-color: var(--jp-layout-color3);
}

.jp-Placeholder-content .jp-MoreHorizIcon {
  width: 32px;
  height: 16px;
  border: 1px solid transparent;
  border-radius: var(--jp-border-radius);
}

.jp-Placeholder-content .jp-MoreHorizIcon:hover {
  border: 1px solid var(--jp-border-color1);
  box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.25);
  background-color: var(--jp-layout-color0);
}

.jp-PlaceholderText {
  white-space: nowrap;
  overflow-x: hidden;
  color: var(--jp-inverse-layout-color3);
  font-family: var(--jp-code-font-family);
}

.jp-InputPlaceholder > .jp-Placeholder-content {
  border-color: var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-cell-scrolling-output-offset: 5px;
}

/*-----------------------------------------------------------------------------
| Cell
|----------------------------------------------------------------------------*/

.jp-Cell {
  padding: var(--jp-cell-padding);
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Common input/output
|----------------------------------------------------------------------------*/

.jp-Cell-inputWrapper,
.jp-Cell-outputWrapper {
  display: flex;
  flex-direction: row;
  padding: 0;
  margin: 0;

  /* Added to reveal the box-shadow on the input and output collapsers. */
  overflow: visible;
}

/* Only input/output areas inside cells */
.jp-Cell-inputArea,
.jp-Cell-outputArea {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Collapser
|----------------------------------------------------------------------------*/

/* Make the output collapser disappear when there is not output, but do so
 * in a manner that leaves it in the layout and preserves its width.
 */
.jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser {
  border: none !important;
  background: transparent !important;
}

.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser {
  min-height: var(--jp-cell-collapser-min-height);
}

/*-----------------------------------------------------------------------------
| Output
|----------------------------------------------------------------------------*/

/* Put a space between input and output when there IS output */
.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper {
  margin-top: 5px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea {
  overflow-y: auto;
  max-height: 24em;
  margin-left: var(--jp-private-cell-scrolling-output-offset);
  resize: vertical;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea[style*='height'] {
  max-height: unset;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea::after {
  content: ' ';
  box-shadow: inset 0 0 6px 2px rgb(0 0 0 / 30%);
  width: 100%;
  height: 100%;
  position: sticky;
  bottom: 0;
  top: 0;
  margin-top: -50%;
  float: left;
  display: block;
  pointer-events: none;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-child {
  padding-top: 6px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt {
  width: calc(
    var(--jp-cell-prompt-width) - var(--jp-private-cell-scrolling-output-offset)
  );
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay {
  left: calc(-1 * var(--jp-private-cell-scrolling-output-offset));
}

/*-----------------------------------------------------------------------------
| CodeCell
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| MarkdownCell
|----------------------------------------------------------------------------*/

.jp-MarkdownOutput {
  display: table-cell;
  width: 100%;
  margin-top: 0;
  margin-bottom: 0;
  padding-left: var(--jp-code-padding);
}

.jp-MarkdownOutput.jp-RenderedHTMLCommon {
  overflow: auto;
}

/* collapseHeadingButton (show always if hiddenCellsButton is _not_ shown) */
.jp-collapseHeadingButton {
  display: flex;
  min-height: var(--jp-cell-collapser-min-height);
  font-size: var(--jp-code-font-size);
  position: absolute;
  background-color: transparent;
  background-size: 25px;
  background-repeat: no-repeat;
  background-position-x: center;
  background-position-y: top;
  background-image: var(--jp-icon-caret-down);
  right: 0;
  top: 0;
  bottom: 0;
}

.jp-collapseHeadingButton.jp-mod-collapsed {
  background-image: var(--jp-icon-caret-right);
}

/*
 set the container font size to match that of content
 so that the nested collapse buttons have the right size
*/
.jp-MarkdownCell .jp-InputPrompt {
  font-size: var(--jp-content-font-size1);
}

/*
  Align collapseHeadingButton with cell top header
  The font sizes are identical to the ones in packages/rendermime/style/base.css
*/
.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='1'] {
  font-size: var(--jp-content-font-size5);
  background-position-y: calc(0.3 * var(--jp-content-font-size5));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='2'] {
  font-size: var(--jp-content-font-size4);
  background-position-y: calc(0.3 * var(--jp-content-font-size4));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='3'] {
  font-size: var(--jp-content-font-size3);
  background-position-y: calc(0.3 * var(--jp-content-font-size3));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='4'] {
  font-size: var(--jp-content-font-size2);
  background-position-y: calc(0.3 * var(--jp-content-font-size2));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='5'] {
  font-size: var(--jp-content-font-size1);
  background-position-y: top;
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='6'] {
  font-size: var(--jp-content-font-size0);
  background-position-y: top;
}

/* collapseHeadingButton (show only on (hover,active) if hiddenCellsButton is shown) */
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-collapseHeadingButton {
  display: none;
}

.jp-Notebook.jp-mod-showHiddenCellsButton
  :is(.jp-MarkdownCell:hover, .jp-mod-active)
  .jp-collapseHeadingButton {
  display: flex;
}

/* showHiddenCellsButton (only show if jp-mod-showHiddenCellsButton is set, which
is a consequence of the showHiddenCellsButton option in Notebook Settings)*/
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton {
  margin-left: calc(var(--jp-cell-prompt-width) + 2 * var(--jp-code-padding));
  margin-top: var(--jp-code-padding);
  border: 1px solid var(--jp-border-color2);
  background-color: var(--jp-border-color3) !important;
  color: var(--jp-content-font-color0) !important;
  display: flex;
}

.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton:hover {
  background-color: var(--jp-border-color2) !important;
}

.jp-showHiddenCellsButton {
  display: none;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Using block instead of flex to allow the use of the break-inside CSS property for
cell outputs.
*/

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-notebook-toolbar-padding: 2px 5px 2px 2px;
}

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-NotebookPanel-toolbar {
  padding: var(--jp-notebook-toolbar-padding);

  /* disable paint containment from lumino 2.0 default strict CSS containment */
  contain: style size !important;
}

.jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused {
  border: none;
  box-shadow: none;
}

.jp-Notebook-toolbarCellTypeDropdown select {
  height: 24px;
  font-size: var(--jp-ui-font-size1);
  line-height: 14px;
  border-radius: 0;
  display: block;
}

.jp-Notebook-toolbarCellTypeDropdown span {
  top: 5px !important;
}

.jp-Toolbar-responsive-popup {
  position: absolute;
  height: fit-content;
  display: flex;
  flex-direction: row;
  flex-wrap: wrap;
  justify-content: flex-end;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: var(--jp-notebook-toolbar-padding);
  z-index: 1;
  right: 0;
  top: 0;
}

.jp-Toolbar > .jp-Toolbar-responsive-opener {
  margin-left: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-Notebook-ExecutionIndicator {
  position: relative;
  display: inline-block;
  height: 100%;
  z-index: 9997;
}

.jp-Notebook-ExecutionIndicator-tooltip {
  visibility: hidden;
  height: auto;
  width: max-content;
  width: -moz-max-content;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color1);
  text-align: justify;
  border-radius: 6px;
  padding: 0 5px;
  position: fixed;
  display: table;
}

.jp-Notebook-ExecutionIndicator-tooltip.up {
  transform: translateX(-50%) translateY(-100%) translateY(-32px);
}

.jp-Notebook-ExecutionIndicator-tooltip.down {
  transform: translateX(calc(-100% + 16px)) translateY(5px);
}

.jp-Notebook-ExecutionIndicator-tooltip.hidden {
  display: none;
}

.jp-Notebook-ExecutionIndicator:hover .jp-Notebook-ExecutionIndicator-tooltip {
  visibility: visible;
}

.jp-Notebook-ExecutionIndicator span {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  color: var(--jp-ui-font-color1);
  line-height: 24px;
  display: block;
}

.jp-Notebook-ExecutionIndicator-progress-bar {
  display: flex;
  justify-content: center;
  height: 100%;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*
 * Execution indicator
 */
.jp-tocItem-content::after {
  content: '';

  /* Must be identical to form a circle */
  width: 12px;
  height: 12px;
  background: none;
  border: none;
  position: absolute;
  right: 0;
}

.jp-tocItem-content[data-running='0']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background: none;
}

.jp-tocItem-content[data-running='1']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background-color: var(--jp-inverse-layout-color3);
}

.jp-tocItem-content[data-running='0'],
.jp-tocItem-content[data-running='1'] {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Notebook-footer {
  height: 27px;
  margin-left: calc(
    var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
      var(--jp-cell-padding)
  );
  width: calc(
    100% -
      (
        var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
          var(--jp-cell-padding) + var(--jp-cell-padding)
      )
  );
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  color: var(--jp-ui-font-color3);
  margin-top: 6px;
  background: none;
  cursor: pointer;
}

.jp-Notebook-footer:focus {
  border-color: var(--jp-cell-editor-active-border-color);
}

/* For devices that support hovering, hide footer until hover */
@media (hover: hover) {
  .jp-Notebook-footer {
    opacity: 0;
  }

  .jp-Notebook-footer:focus,
  .jp-Notebook-footer:hover {
    opacity: 1;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Imports
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-side-by-side-output-size: 1fr;
  --jp-side-by-side-resized-cell: var(--jp-side-by-side-output-size);
  --jp-private-notebook-dragImage-width: 304px;
  --jp-private-notebook-dragImage-height: 36px;
  --jp-private-notebook-selected-color: var(--md-blue-400);
  --jp-private-notebook-active-color: var(--md-green-400);
}

/*-----------------------------------------------------------------------------
| Notebook
|----------------------------------------------------------------------------*/

/* stylelint-disable selector-max-class */

.jp-NotebookPanel {
  display: block;
  height: 100%;
}

.jp-NotebookPanel.jp-Document {
  min-width: 240px;
  min-height: 120px;
}

.jp-Notebook {
  padding: var(--jp-notebook-padding);
  outline: none;
  overflow: auto;
  background: var(--jp-layout-color0);
}

.jp-Notebook.jp-mod-scrollPastEnd::after {
  display: block;
  content: '';
  min-height: var(--jp-notebook-scroll-padding);
}

.jp-MainAreaWidget-ContainStrict .jp-Notebook * {
  contain: strict;
}

.jp-Notebook .jp-Cell {
  overflow: visible;
}

.jp-Notebook .jp-Cell .jp-InputPrompt {
  cursor: move;
}

/*-----------------------------------------------------------------------------
| Notebook state related styling
|
| The notebook and cells each have states, here are the possibilities:
|
| - Notebook
|   - Command
|   - Edit
| - Cell
|   - None
|   - Active (only one can be active)
|   - Selected (the cells actions are applied to)
|   - Multiselected (when multiple selected, the cursor)
|   - No outputs
|----------------------------------------------------------------------------*/

/* Command or edit modes */

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

/* cell is active */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser {
  background: var(--jp-brand-color1);
}

/* cell is dirty */
.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt {
  color: var(--jp-warn-color1);
}

.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt::before {
  color: var(--jp-warn-color1);
  content: '';
}

.jp-Notebook .jp-Cell.jp-mod-active.jp-mod-dirty .jp-Collapser {
  background: var(--jp-warn-color1);
}

/* collapser is hovered */
.jp-Notebook .jp-Cell .jp-Collapser:hover {
  box-shadow: var(--jp-elevation-z2);
  background: var(--jp-brand-color1);
  opacity: var(--jp-cell-collapser-not-active-hover-opacity);
}

/* cell is active and collapser is hovered */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/* Command mode */

.jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected {
  background: var(--jp-notebook-multiselected-color);
}

.jp-Notebook.jp-mod-commandMode
  .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected) {
  background: transparent;
}

/* Edit mode */

.jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-cell-editor-active-background);
}

/*-----------------------------------------------------------------------------
| Notebook drag and drop
|----------------------------------------------------------------------------*/

.jp-Notebook-cell.jp-mod-dropSource {
  opacity: 0.5;
}

.jp-Notebook-cell.jp-mod-dropTarget,
.jp-Notebook.jp-mod-commandMode
  .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget {
  border-top-color: var(--jp-private-notebook-selected-color);
  border-top-style: solid;
  border-top-width: 2px;
}

.jp-dragImage {
  display: block;
  flex-direction: row;
  width: var(--jp-private-notebook-dragImage-width);
  height: var(--jp-private-notebook-dragImage-height);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
  overflow: visible;
}

.jp-dragImage-singlePrompt {
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

.jp-dragImage .jp-dragImage-content {
  flex: 1 1 auto;
  z-index: 2;
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  line-height: var(--jp-code-line-height);
  padding: var(--jp-code-padding);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background-color);
  color: var(--jp-content-font-color3);
  text-align: left;
  margin: 4px 4px 4px 0;
}

.jp-dragImage .jp-dragImage-prompt {
  flex: 0 0 auto;
  min-width: 36px;
  color: var(--jp-cell-inprompt-font-color);
  padding: var(--jp-code-padding);
  padding-left: 12px;
  font-family: var(--jp-cell-prompt-font-family);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: 1.9;
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
}

.jp-dragImage-multipleBack {
  z-index: -1;
  position: absolute;
  height: 32px;
  width: 300px;
  top: 8px;
  left: 8px;
  background: var(--jp-layout-color2);
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

/*-----------------------------------------------------------------------------
| Cell toolbar
|----------------------------------------------------------------------------*/

.jp-NotebookTools {
  display: block;
  min-width: var(--jp-sidebar-min-width);
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
    * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  overflow: auto;
}

.jp-ActiveCellTool {
  padding: 12px 0;
  display: flex;
}

.jp-ActiveCellTool-Content {
  flex: 1 1 auto;
}

.jp-ActiveCellTool .jp-ActiveCellTool-CellContent {
  background: var(--jp-cell-editor-background);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  min-height: 29px;
}

.jp-ActiveCellTool .jp-InputPrompt {
  min-width: calc(var(--jp-cell-prompt-width) * 0.75);
}

.jp-ActiveCellTool-CellContent > pre {
  padding: 5px 4px;
  margin: 0;
  white-space: normal;
}

.jp-MetadataEditorTool {
  flex-direction: column;
  padding: 12px 0;
}

.jp-RankedPanel > :not(:first-child) {
  margin-top: 12px;
}

.jp-KeySelector select.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: var(--jp-border-width) solid var(--jp-border-color1);
}

.jp-KeySelector label,
.jp-MetadataEditorTool label,
.jp-NumberSetter label {
  line-height: 1.4;
}

.jp-NotebookTools .jp-select-wrapper {
  margin-top: 4px;
  margin-bottom: 0;
}

.jp-NumberSetter input {
  width: 100%;
  margin-top: 4px;
}

.jp-NotebookTools .jp-Collapse {
  margin-top: 16px;
}

/*-----------------------------------------------------------------------------
| Presentation Mode (.jp-mod-presentationMode)
|----------------------------------------------------------------------------*/

.jp-mod-presentationMode .jp-Notebook {
  --jp-content-font-size1: var(--jp-content-presentation-font-size1);
  --jp-code-font-size: var(--jp-code-presentation-font-size);
}

.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt,
.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt {
  flex: 0 0 110px;
}

/*-----------------------------------------------------------------------------
| Side-by-side Mode (.jp-mod-sideBySide)
|----------------------------------------------------------------------------*/
.jp-mod-sideBySide.jp-Notebook .jp-Notebook-cell {
  margin-top: 3em;
  margin-bottom: 3em;
  margin-left: 5%;
  margin-right: 5%;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell {
  display: grid;
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-output-size)
    );
  grid-template-rows: auto minmax(0, 1fr) auto;
  grid-template-areas:
    'header header header'
    'input handle output'
    'footer footer footer';
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell.jp-mod-resizedCell {
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-resized-cell)
    );
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellHeader {
  grid-area: header;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-inputWrapper {
  grid-area: input;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-outputWrapper {
  /* overwrite the default margin (no vertical separation needed in side by side move */
  margin-top: 0;
  grid-area: output;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellFooter {
  grid-area: footer;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle {
  grid-area: handle;
  user-select: none;
  display: block;
  height: 100%;
  cursor: ew-resize;
  padding: 0 var(--jp-cell-padding);
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle::after {
  content: '';
  display: block;
  background: var(--jp-border-color2);
  height: 100%;
  width: 5px;
}

.jp-mod-sideBySide.jp-Notebook
  .jp-CodeCell.jp-mod-resizedCell
  .jp-CellResizeHandle::after {
  background: var(--jp-border-color0);
}

.jp-CellResizeHandle {
  display: none;
}

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Cell-Placeholder {
  padding-left: 55px;
}

.jp-Cell-Placeholder-wrapper {
  background: #fff;
  border: 1px solid;
  border-color: #e5e6e9 #dfe0e4 #d0d1d5;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  margin: 10px 15px;
}

.jp-Cell-Placeholder-wrapper-inner {
  padding: 15px;
  position: relative;
}

.jp-Cell-Placeholder-wrapper-body {
  background-repeat: repeat;
  background-size: 50% auto;
}

.jp-Cell-Placeholder-wrapper-body div {
  background: #f6f7f8;
  background-image: -webkit-linear-gradient(
    left,
    #f6f7f8 0%,
    #edeef1 20%,
    #f6f7f8 40%,
    #f6f7f8 100%
  );
  background-repeat: no-repeat;
  background-size: 800px 104px;
  height: 104px;
  position: absolute;
  right: 15px;
  left: 15px;
  top: 15px;
}

div.jp-Cell-Placeholder-h1 {
  top: 20px;
  height: 20px;
  left: 15px;
  width: 150px;
}

div.jp-Cell-Placeholder-h2 {
  left: 15px;
  top: 50px;
  height: 10px;
  width: 100px;
}

div.jp-Cell-Placeholder-content-1,
div.jp-Cell-Placeholder-content-2,
div.jp-Cell-Placeholder-content-3 {
  left: 15px;
  right: 15px;
  height: 10px;
}

div.jp-Cell-Placeholder-content-1 {
  top: 100px;
}

div.jp-Cell-Placeholder-content-2 {
  top: 120px;
}

div.jp-Cell-Placeholder-content-3 {
  top: 140px;
}

</style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
The following CSS variables define the main, public API for styling JupyterLab.
These variables should be used by all plugins wherever possible. In other
words, plugins should not define custom colors, sizes, etc unless absolutely
necessary. This enables users to change the visual theme of JupyterLab
by changing these variables.

Many variables appear in an ordered sequence (0,1,2,3). These sequences
are designed to work well together, so for example, `--jp-border-color1` should
be used with `--jp-layout-color1`. The numbers have the following meanings:

* 0: super-primary, reserved for special emphasis
* 1: primary, most important under normal situations
* 2: secondary, next most important under normal situations
* 3: tertiary, next most important under normal situations

Throughout JupyterLab, we are mostly following principles from Google's
Material Design when selecting colors. We are not, however, following
all of MD as it is not optimized for dense, information rich UIs.
*/

:root {
  /* Elevation
   *
   * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here:
   *
   * https://github.com/material-components/material-components-web
   * https://material-components-web.appspot.com/elevation.html
   */

  --jp-shadow-base-lightness: 0;
  --jp-shadow-umbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.2
  );
  --jp-shadow-penumbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.14
  );
  --jp-shadow-ambient-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.12
  );
  --jp-elevation-z0: none;
  --jp-elevation-z1: 0 2px 1px -1px var(--jp-shadow-umbra-color),
    0 1px 1px 0 var(--jp-shadow-penumbra-color),
    0 1px 3px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z2: 0 3px 1px -2px var(--jp-shadow-umbra-color),
    0 2px 2px 0 var(--jp-shadow-penumbra-color),
    0 1px 5px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z4: 0 2px 4px -1px var(--jp-shadow-umbra-color),
    0 4px 5px 0 var(--jp-shadow-penumbra-color),
    0 1px 10px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z6: 0 3px 5px -1px var(--jp-shadow-umbra-color),
    0 6px 10px 0 var(--jp-shadow-penumbra-color),
    0 1px 18px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z8: 0 5px 5px -3px var(--jp-shadow-umbra-color),
    0 8px 10px 1px var(--jp-shadow-penumbra-color),
    0 3px 14px 2px var(--jp-shadow-ambient-color);
  --jp-elevation-z12: 0 7px 8px -4px var(--jp-shadow-umbra-color),
    0 12px 17px 2px var(--jp-shadow-penumbra-color),
    0 5px 22px 4px var(--jp-shadow-ambient-color);
  --jp-elevation-z16: 0 8px 10px -5px var(--jp-shadow-umbra-color),
    0 16px 24px 2px var(--jp-shadow-penumbra-color),
    0 6px 30px 5px var(--jp-shadow-ambient-color);
  --jp-elevation-z20: 0 10px 13px -6px var(--jp-shadow-umbra-color),
    0 20px 31px 3px var(--jp-shadow-penumbra-color),
    0 8px 38px 7px var(--jp-shadow-ambient-color);
  --jp-elevation-z24: 0 11px 15px -7px var(--jp-shadow-umbra-color),
    0 24px 38px 3px var(--jp-shadow-penumbra-color),
    0 9px 46px 8px var(--jp-shadow-ambient-color);

  /* Borders
   *
   * The following variables, specify the visual styling of borders in JupyterLab.
   */

  --jp-border-width: 1px;
  --jp-border-color0: var(--md-grey-400);
  --jp-border-color1: var(--md-grey-400);
  --jp-border-color2: var(--md-grey-300);
  --jp-border-color3: var(--md-grey-200);
  --jp-inverse-border-color: var(--md-grey-600);
  --jp-border-radius: 2px;

  /* UI Fonts
   *
   * The UI font CSS variables are used for the typography all of the JupyterLab
   * user interface elements that are not directly user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-ui-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-ui-font-scale-factor: 1.2;
  --jp-ui-font-size0: 0.83333em;
  --jp-ui-font-size1: 13px; /* Base font size */
  --jp-ui-font-size2: 1.2em;
  --jp-ui-font-size3: 1.44em;
  --jp-ui-font-family: system-ui, -apple-system, blinkmacsystemfont, 'Segoe UI',
    helvetica, arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji',
    'Segoe UI Symbol';

  /*
   * Use these font colors against the corresponding main layout colors.
   * In a light theme, these go from dark to light.
   */

  /* Defaults use Material Design specification */
  --jp-ui-font-color0: rgba(0, 0, 0, 1);
  --jp-ui-font-color1: rgba(0, 0, 0, 0.87);
  --jp-ui-font-color2: rgba(0, 0, 0, 0.54);
  --jp-ui-font-color3: rgba(0, 0, 0, 0.38);

  /*
   * Use these against the brand/accent/warn/error colors.
   * These will typically go from light to darker, in both a dark and light theme.
   */

  --jp-ui-inverse-font-color0: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color1: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color2: rgba(255, 255, 255, 0.7);
  --jp-ui-inverse-font-color3: rgba(255, 255, 255, 0.5);

  /* Content Fonts
   *
   * Content font variables are used for typography of user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-content-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-content-line-height: 1.6;
  --jp-content-font-scale-factor: 1.2;
  --jp-content-font-size0: 0.83333em;
  --jp-content-font-size1: 14px; /* Base font size */
  --jp-content-font-size2: 1.2em;
  --jp-content-font-size3: 1.44em;
  --jp-content-font-size4: 1.728em;
  --jp-content-font-size5: 2.0736em;

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-content-presentation-font-size1: 17px;
  --jp-content-heading-line-height: 1;
  --jp-content-heading-margin-top: 1.2em;
  --jp-content-heading-margin-bottom: 0.8em;
  --jp-content-heading-font-weight: 500;

  /* Defaults use Material Design specification */
  --jp-content-font-color0: rgba(0, 0, 0, 1);
  --jp-content-font-color1: rgba(0, 0, 0, 0.87);
  --jp-content-font-color2: rgba(0, 0, 0, 0.54);
  --jp-content-font-color3: rgba(0, 0, 0, 0.38);
  --jp-content-link-color: var(--md-blue-900);
  --jp-content-font-family: system-ui, -apple-system, blinkmacsystemfont,
    'Segoe UI', helvetica, arial, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol';

  /*
   * Code Fonts
   *
   * Code font variables are used for typography of code and other monospaces content.
   */

  --jp-code-font-size: 13px;
  --jp-code-line-height: 1.3077; /* 17px for 13px base */
  --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */
  --jp-code-font-family-default: menlo, consolas, 'DejaVu Sans Mono', monospace;
  --jp-code-font-family: var(--jp-code-font-family-default);

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-code-presentation-font-size: 16px;

  /* may need to tweak cursor width if you change font size */
  --jp-code-cursor-width0: 1.4px;
  --jp-code-cursor-width1: 2px;
  --jp-code-cursor-width2: 4px;

  /* Layout
   *
   * The following are the main layout colors use in JupyterLab. In a light
   * theme these would go from light to dark.
   */

  --jp-layout-color0: white;
  --jp-layout-color1: white;
  --jp-layout-color2: var(--md-grey-200);
  --jp-layout-color3: var(--md-grey-400);
  --jp-layout-color4: var(--md-grey-600);

  /* Inverse Layout
   *
   * The following are the inverse layout colors use in JupyterLab. In a light
   * theme these would go from dark to light.
   */

  --jp-inverse-layout-color0: #111;
  --jp-inverse-layout-color1: var(--md-grey-900);
  --jp-inverse-layout-color2: var(--md-grey-800);
  --jp-inverse-layout-color3: var(--md-grey-700);
  --jp-inverse-layout-color4: var(--md-grey-600);

  /* Brand/accent */

  --jp-brand-color0: var(--md-blue-900);
  --jp-brand-color1: var(--md-blue-700);
  --jp-brand-color2: var(--md-blue-300);
  --jp-brand-color3: var(--md-blue-100);
  --jp-brand-color4: var(--md-blue-50);
  --jp-accent-color0: var(--md-green-900);
  --jp-accent-color1: var(--md-green-700);
  --jp-accent-color2: var(--md-green-300);
  --jp-accent-color3: var(--md-green-100);

  /* State colors (warn, error, success, info) */

  --jp-warn-color0: var(--md-orange-900);
  --jp-warn-color1: var(--md-orange-700);
  --jp-warn-color2: var(--md-orange-300);
  --jp-warn-color3: var(--md-orange-100);
  --jp-error-color0: var(--md-red-900);
  --jp-error-color1: var(--md-red-700);
  --jp-error-color2: var(--md-red-300);
  --jp-error-color3: var(--md-red-100);
  --jp-success-color0: var(--md-green-900);
  --jp-success-color1: var(--md-green-700);
  --jp-success-color2: var(--md-green-300);
  --jp-success-color3: var(--md-green-100);
  --jp-info-color0: var(--md-cyan-900);
  --jp-info-color1: var(--md-cyan-700);
  --jp-info-color2: var(--md-cyan-300);
  --jp-info-color3: var(--md-cyan-100);

  /* Cell specific styles */

  --jp-cell-padding: 5px;
  --jp-cell-collapser-width: 8px;
  --jp-cell-collapser-min-height: 20px;
  --jp-cell-collapser-not-active-hover-opacity: 0.6;
  --jp-cell-editor-background: var(--md-grey-100);
  --jp-cell-editor-border-color: var(--md-grey-300);
  --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-cell-editor-active-background: var(--jp-layout-color0);
  --jp-cell-editor-active-border-color: var(--jp-brand-color1);
  --jp-cell-prompt-width: 64px;
  --jp-cell-prompt-font-family: var(--jp-code-font-family-default);
  --jp-cell-prompt-letter-spacing: 0;
  --jp-cell-prompt-opacity: 1;
  --jp-cell-prompt-not-active-opacity: 0.5;
  --jp-cell-prompt-not-active-font-color: var(--md-grey-700);

  /* A custom blend of MD grey and blue 600
   * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */
  --jp-cell-inprompt-font-color: #307fc1;

  /* A custom blend of MD grey and orange 600
   * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */
  --jp-cell-outprompt-font-color: #bf5b3d;

  /* Notebook specific styles */

  --jp-notebook-padding: 10px;
  --jp-notebook-select-background: var(--jp-layout-color1);
  --jp-notebook-multiselected-color: var(--md-blue-50);

  /* The scroll padding is calculated to fill enough space at the bottom of the
  notebook to show one single-line cell (with appropriate padding) at the top
  when the notebook is scrolled all the way to the bottom. We also subtract one
  pixel so that no scrollbar appears if we have just one single-line cell in the
  notebook. This padding is to enable a 'scroll past end' feature in a notebook.
  */
  --jp-notebook-scroll-padding: calc(
    100% - var(--jp-code-font-size) * var(--jp-code-line-height) -
      var(--jp-code-padding) - var(--jp-cell-padding) - 1px
  );

  /* Rendermime styles */

  --jp-rendermime-error-background: #fdd;
  --jp-rendermime-table-row-background: var(--md-grey-100);
  --jp-rendermime-table-row-hover-background: var(--md-light-blue-50);

  /* Dialog specific styles */

  --jp-dialog-background: rgba(0, 0, 0, 0.25);

  /* Console specific styles */

  --jp-console-padding: 10px;

  /* Toolbar specific styles */

  --jp-toolbar-border-color: var(--jp-border-color1);
  --jp-toolbar-micro-height: 8px;
  --jp-toolbar-background: var(--jp-layout-color1);
  --jp-toolbar-box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.24);
  --jp-toolbar-header-margin: 4px 4px 0 4px;
  --jp-toolbar-active-background: var(--md-grey-300);

  /* Statusbar specific styles */

  --jp-statusbar-height: 24px;

  /* Input field styles */

  --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-input-active-background: var(--jp-layout-color1);
  --jp-input-hover-background: var(--jp-layout-color1);
  --jp-input-background: var(--md-grey-100);
  --jp-input-border-color: var(--jp-inverse-border-color);
  --jp-input-active-border-color: var(--jp-brand-color1);
  --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3);

  /* General editor styles */

  --jp-editor-selected-background: #d9d9d9;
  --jp-editor-selected-focused-background: #d7d4f0;
  --jp-editor-cursor-color: var(--jp-ui-font-color0);

  /* Code mirror specific styles */

  --jp-mirror-editor-keyword-color: #008000;
  --jp-mirror-editor-atom-color: #88f;
  --jp-mirror-editor-number-color: #080;
  --jp-mirror-editor-def-color: #00f;
  --jp-mirror-editor-variable-color: var(--md-grey-900);
  --jp-mirror-editor-variable-2-color: rgb(0, 54, 109);
  --jp-mirror-editor-variable-3-color: #085;
  --jp-mirror-editor-punctuation-color: #05a;
  --jp-mirror-editor-property-color: #05a;
  --jp-mirror-editor-operator-color: #a2f;
  --jp-mirror-editor-comment-color: #408080;
  --jp-mirror-editor-string-color: #ba2121;
  --jp-mirror-editor-string-2-color: #708;
  --jp-mirror-editor-meta-color: #a2f;
  --jp-mirror-editor-qualifier-color: #555;
  --jp-mirror-editor-builtin-color: #008000;
  --jp-mirror-editor-bracket-color: #997;
  --jp-mirror-editor-tag-color: #170;
  --jp-mirror-editor-attribute-color: #00c;
  --jp-mirror-editor-header-color: blue;
  --jp-mirror-editor-quote-color: #090;
  --jp-mirror-editor-link-color: #00c;
  --jp-mirror-editor-error-color: #f00;
  --jp-mirror-editor-hr-color: #999;

  /*
    RTC user specific colors.
    These colors are used for the cursor, username in the editor,
    and the icon of the user.
  */

  --jp-collaborator-color1: #ffad8e;
  --jp-collaborator-color2: #dac83d;
  --jp-collaborator-color3: #72dd76;
  --jp-collaborator-color4: #00e4d0;
  --jp-collaborator-color5: #45d4ff;
  --jp-collaborator-color6: #e2b1ff;
  --jp-collaborator-color7: #ff9de6;

  /* Vega extension styles */

  --jp-vega-background: white;

  /* Sidebar-related styles */

  --jp-sidebar-min-width: 250px;

  /* Search-related styles */

  --jp-search-toggle-off-opacity: 0.5;
  --jp-search-toggle-hover-opacity: 0.8;
  --jp-search-toggle-on-opacity: 1;
  --jp-search-selected-match-background-color: rgb(245, 200, 0);
  --jp-search-selected-match-color: black;
  --jp-search-unselected-match-background-color: var(
    --jp-inverse-layout-color0
  );
  --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0);

  /* Icon colors that work well with light or dark backgrounds */
  --jp-icon-contrast-color0: var(--md-purple-600);
  --jp-icon-contrast-color1: var(--md-green-600);
  --jp-icon-contrast-color2: var(--md-pink-600);
  --jp-icon-contrast-color3: var(--md-blue-600);

  /* Button colors */
  --jp-accept-color-normal: var(--md-blue-700);
  --jp-accept-color-hover: var(--md-blue-800);
  --jp-accept-color-active: var(--md-blue-900);
  --jp-warn-color-normal: var(--md-red-700);
  --jp-warn-color-hover: var(--md-red-800);
  --jp-warn-color-active: var(--md-red-900);
  --jp-reject-color-normal: var(--md-grey-600);
  --jp-reject-color-hover: var(--md-grey-700);
  --jp-reject-color-active: var(--md-grey-800);

  /* File or activity icons and switch semantic variables */
  --jp-jupyter-icon-color: #f37626;
  --jp-notebook-icon-color: #f37626;
  --jp-json-icon-color: var(--md-orange-700);
  --jp-console-icon-background-color: var(--md-blue-700);
  --jp-console-icon-color: white;
  --jp-terminal-icon-background-color: var(--md-grey-800);
  --jp-terminal-icon-color: var(--md-grey-200);
  --jp-text-editor-icon-color: var(--md-grey-700);
  --jp-inspector-icon-color: var(--md-grey-700);
  --jp-switch-color: var(--md-grey-400);
  --jp-switch-true-position-color: var(--md-orange-900);
}
</style>
<style type="text/css">
/* Force rendering true colors when outputing to pdf */
* {
  -webkit-print-color-adjust: exact;
}

/* Misc */
a.anchor-link {
  display: none;
}

/* Input area styling */
.jp-InputArea {
  overflow: hidden;
}

.jp-InputArea-editor {
  overflow: hidden;
}

.cm-editor.cm-s-jupyter .highlight pre {
/* weird, but --jp-code-padding defined to be 5px but 4px horizontal padding is hardcoded for pre.cm-line */
  padding: var(--jp-code-padding) 4px;
  margin: 0;

  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
  color: inherit;

}

.jp-OutputArea-output pre {
  line-height: inherit;
  font-family: inherit;
}

.jp-RenderedText pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
}

/* Hiding the collapser by default */
.jp-Collapser {
  display: none;
}

@page {
    margin: 0.5in; /* Margin for each printed piece of paper */
}

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}
</style>
<!-- Load mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML-full,Safe"> </script>
<!-- MathJax configuration -->
<script type="text/x-mathjax-config">
    init_mathjax = function() {
        if (window.MathJax) {
        // MathJax loaded
            MathJax.Hub.Config({
                TeX: {
                    equationNumbers: {
                    autoNumber: "AMS",
                    useLabelIds: true
                    }
                },
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true,
                    processEnvironments: true
                },
                displayAlign: 'center',
                CommonHTML: {
                    linebreaks: {
                    automatic: true
                    }
                }
            });

            MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
        }
    }
    init_mathjax();
    </script>
<!-- End of mathjax configuration --><script type="module">
  document.addEventListener("DOMContentLoaded", async () => {
    const diagrams = document.querySelectorAll(".jp-Mermaid > pre.mermaid");
    // do not load mermaidjs if not needed
    if (!diagrams.length) {
      return;
    }
    const mermaid = (await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs")).default;
    const parser = new DOMParser();

    mermaid.initialize({
      maxTextSize: 100000,
      maxEdges: 100000,
      startOnLoad: false,
      fontFamily: window
        .getComputedStyle(document.body)
        .getPropertyValue("--jp-ui-font-family"),
      theme: document.querySelector("body[data-jp-theme-light='true']")
        ? "default"
        : "dark",
    });

    let _nextMermaidId = 0;

    function makeMermaidImage(svg) {
      const img = document.createElement("img");
      const doc = parser.parseFromString(svg, "image/svg+xml");
      const svgEl = doc.querySelector("svg");
      const { maxWidth } = svgEl?.style || {};
      const firstTitle = doc.querySelector("title");
      const firstDesc = doc.querySelector("desc");

      img.setAttribute("src", `data:image/svg+xml,${encodeURIComponent(svg)}`);
      if (maxWidth) {
        img.width = parseInt(maxWidth);
      }
      if (firstTitle) {
        img.setAttribute("alt", firstTitle.textContent);
      }
      if (firstDesc) {
        const caption = document.createElement("figcaption");
        caption.className = "sr-only";
        caption.textContent = firstDesc.textContent;
        return [img, caption];
      }
      return [img];
    }

    async function makeMermaidError(text) {
      let errorMessage = "";
      try {
        await mermaid.parse(text);
      } catch (err) {
        errorMessage = `${err}`;
      }

      const result = document.createElement("details");
      result.className = 'jp-RenderedMermaid-Details';
      const summary = document.createElement("summary");
      summary.className = 'jp-RenderedMermaid-Summary';
      const pre = document.createElement("pre");
      const code = document.createElement("code");
      code.innerText = text;
      pre.appendChild(code);
      summary.appendChild(pre);
      result.appendChild(summary);

      const warning = document.createElement("pre");
      warning.innerText = errorMessage;
      result.appendChild(warning);
      return [result];
    }

    async function renderOneMarmaid(src) {
      const id = `jp-mermaid-${_nextMermaidId++}`;
      const parent = src.parentNode;
      let raw = src.textContent.trim();
      const el = document.createElement("div");
      el.style.visibility = "hidden";
      document.body.appendChild(el);
      let results = null;
      let output = null;
      try {
        let { svg } = await mermaid.render(id, raw, el);
        svg = cleanMermaidSvg(svg);
        results = makeMermaidImage(svg);
        output = document.createElement("figure");
        results.map(output.appendChild, output);
      } catch (err) {
        parent.classList.add("jp-mod-warning");
        results = await makeMermaidError(raw);
        output = results[0];
      } finally {
        el.remove();
      }
      parent.classList.add("jp-RenderedMermaid");
      parent.appendChild(output);
    }


    /**
     * Post-process to ensure mermaid diagrams contain only valid SVG and XHTML.
     */
    function cleanMermaidSvg(svg) {
      return svg.replace(RE_VOID_ELEMENT, replaceVoidElement);
    }


    /**
     * A regular expression for all void elements, which may include attributes and
     * a slash.
     *
     * @see https://developer.mozilla.org/en-US/docs/Glossary/Void_element
     *
     * Of these, only `<br>` is generated by Mermaid in place of `\n`,
     * but _any_ "malformed" tag will break the SVG rendering entirely.
     */
    const RE_VOID_ELEMENT =
      /<\s*(area|base|br|col|embed|hr|img|input|link|meta|param|source|track|wbr)\s*([^>]*?)\s*>/gi;

    /**
     * Ensure a void element is closed with a slash, preserving any attributes.
     */
    function replaceVoidElement(match, tag, rest) {
      rest = rest.trim();
      if (!rest.endsWith('/')) {
        rest = `${rest} /`;
      }
      return `<${tag} ${rest}>`;
    }

    void Promise.all([...diagrams].map(renderOneMarmaid));
  });
</script>
<style>
  .jp-Mermaid:not(.jp-RenderedMermaid) {
    display: none;
  }

  .jp-RenderedMermaid {
    overflow: auto;
    display: flex;
  }

  .jp-RenderedMermaid.jp-mod-warning {
    width: auto;
    padding: 0.5em;
    margin-top: 0.5em;
    border: var(--jp-border-width) solid var(--jp-warn-color2);
    border-radius: var(--jp-border-radius);
    color: var(--jp-ui-font-color1);
    font-size: var(--jp-ui-font-size1);
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  .jp-RenderedMermaid figure {
    margin: 0;
    overflow: auto;
    max-width: 100%;
  }

  .jp-RenderedMermaid img {
    max-width: 100%;
  }

  .jp-RenderedMermaid-Details > pre {
    margin-top: 1em;
  }

  .jp-RenderedMermaid-Summary {
    color: var(--jp-warn-color2);
  }

  .jp-RenderedMermaid:not(.jp-mod-warning) pre {
    display: none;
  }

  .jp-RenderedMermaid-Summary > pre {
    display: inline-block;
    white-space: normal;
  }
</style>
<!-- End of mermaid configuration --></head>
<body class="jp-Notebook" data-jp-theme-light="true" data-jp-theme-name="JupyterLab Light">
<main>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=73c2d2c9-4a4b-48ca-90a3-1ccd120ca08b">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h1 id="Fine-tuning-using-Llama-3-8B">Fine tuning using Llama 3 8B<a class="anchor-link" href="#Fine-tuning-using-Llama-3-8B"></a></h1>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=5b5bef3d-89b9-42ed-b158-a39bd61f6a31">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Base-Model-and-Quantization">Base Model and Quantization<a class="anchor-link" href="#Base-Model-and-Quantization"></a></h3>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=938123ed-15b0-45ee-b622-d9bbe5fe3a48">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>4-bit quantization alongside specific computational optimizations</p>
<p>load_in_4bit: This option likely enables loading and storing tensors in 4-bit precision. This can significantly reduce memory usage at the cost of precision. Enabling this suggests that you're optimizing for memory efficiency, potentially to fit larger models or datasets in memory.</p>
<p>bnb_4bit_use_double_quant: This indicates the use of a double quantization process for 4-bit representation. Double quantization might be used to improve the precision of the 4-bit quantized values, potentially mitigating some of the precision loss associated with low-bit quantization.</p>
<p>bnb_4bit_quant_type="nf4": This specifies the quantization type or algorithm used for converting tensors to 4-bit representations. "nf4" might refer to a specific quantization scheme optimized for neural network weights and activations. The exact nature of "nf4" would depend on the documentation of the BitsAndBytes library, but it suggests an approach tailored to maintain as much information as possible within the 4-bit limitation.</p>
<p>bnb_4bit_compute_dtype=torch.bfloat16: This sets the data type for computations using 4-bit quantized tensors to torch.bfloat16, which is a 16-bit floating-point representation that offers a good balance between precision and memory usage. By performing computations in bfloat16, the configuration aims to maintain computational accuracy and efficiency, particularly on hardware that supports bfloat16 operations natively.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=dfe5f014-527c-4a83-863b-4b6330c72ed5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[12]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># GPU 0: NVIDIA GeForce RTX 4090</span>
<span class="c1"># GPU 1: NVIDIA GeForce RTX 4090</span>
<span class="c1"># GPU 2: NVIDIA GeForce RTX 4090</span>
<span class="c1"># GPU 3: NVIDIA GeForce RTX 3090 Ti</span>
<span class="c1"># GPU 4: NVIDIA GeForce RTX 3090 Ti</span>
<span class="c1"># GPU 5: NVIDIA GeForce RTX 3090</span>
<span class="c1"># GPU 6: NVIDIA GeForce RTX 3090</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"CUDA_VISIBLE_DEVICES"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"0"</span>  <span class="c1"># ""makes all visible, "0" GPU 0 visible</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=53ffaa11-3936-40a0-8f2a-3d083ff2afef">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Supress-warnings">Supress warnings<a class="anchor-link" href="#Supress-warnings"></a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=e77f7e3d-3af3-4dc8-9571-d13398c29ee9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[13]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">UserWarning</span><span class="p">)</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">FutureWarning</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=ef535c7a-848e-4c6e-a692-208f98610d82">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Inspect-the-base-model">Inspect the base model<a class="anchor-link" href="#Inspect-the-base-model"></a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=8bfef1a8-c05a-4e14-af0e-358bfb04352a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[14]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">access_token</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HF_TOKEN"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=89cce100-9b2e-4675-bfda-f029e7c4056e">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[4]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">login</span>
<span class="n">login</span><span class="p">(</span><span class="n">access_token</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>VBox(children=(HTML(value='&lt;center&gt; &lt;img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=15ec782b-0776-4354-ad08-66f1e9e50187">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[15]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s2">"Meta-Llama-3-8B"</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"meta-llama/"</span><span class="o">+</span><span class="n">model_name</span>

<span class="n">bnb_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span>
    <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="s2">"nf4"</span><span class="p">,</span>
    <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=8698b0c2-d371-4a2e-bddf-185563991892">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[30]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">bnb_config</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">token</span><span class="o">=</span><span class="n">access_token</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Loading checkpoint shards:   0%|          | 0/4 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=83f74939-3ab3-4011-90cb-8e90c22ea162">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[6]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torchinfo</span> <span class="kn">import</span> <span class="n">summary</span>
<span class="n">summary</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[6]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>================================================================================
Layer (type:depth-idx)                                  Param #
================================================================================
LlamaForSequenceClassification                          --
LlamaModel: 1-1                                       --
    Embedding: 2-1                                   525,336,576
    ModuleList: 2-2                                  --
        LlamaDecoderLayer: 3-1                      109,060,096
        LlamaDecoderLayer: 3-2                      109,060,096
        LlamaDecoderLayer: 3-3                      109,060,096
        LlamaDecoderLayer: 3-4                      109,060,096
        LlamaDecoderLayer: 3-5                      109,060,096
        LlamaDecoderLayer: 3-6                      109,060,096
        LlamaDecoderLayer: 3-7                      109,060,096
        LlamaDecoderLayer: 3-8                      109,060,096
        LlamaDecoderLayer: 3-9                      109,060,096
        LlamaDecoderLayer: 3-10                     109,060,096
        LlamaDecoderLayer: 3-11                     109,060,096
        LlamaDecoderLayer: 3-12                     109,060,096
        LlamaDecoderLayer: 3-13                     109,060,096
        LlamaDecoderLayer: 3-14                     109,060,096
        LlamaDecoderLayer: 3-15                     109,060,096
        LlamaDecoderLayer: 3-16                     109,060,096
        LlamaDecoderLayer: 3-17                     109,060,096
        LlamaDecoderLayer: 3-18                     109,060,096
        LlamaDecoderLayer: 3-19                     109,060,096
        LlamaDecoderLayer: 3-20                     109,060,096
        LlamaDecoderLayer: 3-21                     109,060,096
        LlamaDecoderLayer: 3-22                     109,060,096
        LlamaDecoderLayer: 3-23                     109,060,096
        LlamaDecoderLayer: 3-24                     109,060,096
        LlamaDecoderLayer: 3-25                     109,060,096
        LlamaDecoderLayer: 3-26                     109,060,096
        LlamaDecoderLayer: 3-27                     109,060,096
        LlamaDecoderLayer: 3-28                     109,060,096
        LlamaDecoderLayer: 3-29                     109,060,096
        LlamaDecoderLayer: 3-30                     109,060,096
        LlamaDecoderLayer: 3-31                     109,060,096
        LlamaDecoderLayer: 3-32                     109,060,096
    LlamaRMSNorm: 2-3                                4,096
Linear: 1-2                                           8,192
================================================================================
Total params: 4,015,271,936
Trainable params: 525,611,008
Non-trainable params: 3,489,660,928
================================================================================</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=49bbe47c-4430-4f4a-90d8-1113bd320a18">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[7]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[7]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>LlamaForSequenceClassification(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (score): Linear(in_features=4096, out_features=2, bias=False)
)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=6627b1da-57d6-4a17-8ea3-746b5cb1f3d9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Load-the-news-dataset-from-pickle-file">Load the news dataset from pickle file<a class="anchor-link" href="#Load-the-news-dataset-from-pickle-file"></a></h3><p>If any of the check_files don't exist then load the pickle file</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=1cc372cc-0cae-4b49-a2d7-8e57f58244a7">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[16]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pickle</span>

<span class="n">base_path</span> <span class="o">=</span> <span class="s1">'./data/'</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">base_path</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">file_name</span> <span class="o">=</span> <span class="s1">'news_small_dataset.pkl'</span>
<span class="n">file_path</span> <span class="o">=</span> <span class="n">base_path</span><span class="o">+</span><span class="n">file_name</span>

<span class="k">def</span> <span class="nf">pickle_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">file_path</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="s1">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
        <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">file</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Dataset has been pickled to: </span><span class="si">{</span><span class="n">file_path</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">load_pickle_dataset</span><span class="p">(</span><span class="n">file_path</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="s1">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Dataset has been loaded from: </span><span class="si">{</span><span class="n">file_path</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dataset</span>

<span class="k">def</span> <span class="nf">check_files_exists</span><span class="p">(</span><span class="n">file_names</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">file_names</span><span class="p">:</span>
        <span class="n">file_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">base_path</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">file_path</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="kc">False</span>

<span class="c1"># if these files exist we do not want to load the news_dataset.pkl to tokenize and make these files</span>
<span class="n">check_files</span> <span class="o">=</span> <span class="p">[</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_train_ds.pkl'</span><span class="p">,</span> <span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_eval_ds.pkl'</span><span class="p">,</span> <span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_test_ds.pkl'</span><span class="p">]</span>

<span class="k">if</span> <span class="n">check_files_exists</span><span class="p">(</span><span class="n">check_files</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"At least one of the specified files already exists. Not loading new dataset."</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">news_split_ds</span> <span class="o">=</span> <span class="n">load_pickle_dataset</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">news_split_ds</span><span class="p">)</span>
    <span class="n">total_rows</span> <span class="o">=</span> <span class="p">(</span><span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'train'</span><span class="p">]</span><span class="o">.</span><span class="n">num_rows</span> <span class="o">+</span>
              <span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'eval'</span><span class="p">]</span><span class="o">.</span><span class="n">num_rows</span> <span class="o">+</span>
              <span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'test'</span><span class="p">]</span><span class="o">.</span><span class="n">num_rows</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Total number of rows:"</span><span class="p">,</span> <span class="n">total_rows</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Dataset loaded successfully."</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>At least one of the specified files already exists. Not loading new dataset.
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=8ad450a2-6cef-432e-9e63-7ff985b4726e">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Tokenization-of-data">Tokenization of data<a class="anchor-link" href="#Tokenization-of-data"></a></h3>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=186f093c-7487-44ae-ad95-9ee8d24f04c7">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>
return_tensors="pt": This argument configures the tokenizer to output PyTorch ("pt") tensors. If you're working with TensorFlow, you'd use "tf" instead, and for NumPy arrays, you could omit this argument or set return_tensors to None.
</p>
<p>
Direct Model Input: By converting the tokenized input into tensors, the output can be directly used as input to a PyTorch model, fitting seamlessly into the data processing pipeline for model training or inference.

<p>Handling of Batch Inputs: This approach also supports batch inputs. If you pass a list of texts to the tokenizer with return_tensors="pt", it will automatically pad the sequences to the maximum length in the batch, returning a tensor where the first dimension is the batch size.</p>
<p>Padding and Truncation: The padding=True and truncation=True arguments ensure that all sequences are padded to the same length (up to max_length) and are truncated if they exceed this length, which is important for processing sequences in batches.</p>
</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=88c3a38c-1d24-4a1e-8d96-1b7c2ce162c7">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[31]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">token</span><span class="o">=</span><span class="n">access_token</span><span class="p">)</span>

<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">tokenize_fn</span><span class="p">(</span><span class="n">news</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">news</span><span class="p">[</span><span class="s1">'article'</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=02efc4f0-742a-4838-887d-5556a26ae15f">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Tokenize-train,-evaluation,-and-test-datasets">Tokenize train, evaluation, and test datasets<a class="anchor-link" href="#Tokenize-train,-evaluation,-and-test-datasets"></a></h3><p>If any of the check files exist then don't run tokenization and save some time.
Else load the pickle files that already exist.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=0f3e3101-df30-4af2-9976-49ba0cf44d62">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[18]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">check_files_exists</span><span class="p">(</span><span class="n">check_files</span><span class="p">):</span>
    <span class="n">tokenized_train_ds</span> <span class="o">=</span> <span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'train'</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_fn</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">tokenized_eval_ds</span> <span class="o">=</span> <span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'eval'</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_fn</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">tokenized_test_ds</span> <span class="o">=</span> <span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'test'</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_fn</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">tokenized_train_ds</span><span class="o">.</span><span class="n">features</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">tokenized_eval_ds</span><span class="o">.</span><span class="n">features</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">tokenized_test_ds</span><span class="o">.</span><span class="n">features</span><span class="p">)</span>
    
    <span class="n">pickle_dataset</span><span class="p">(</span><span class="n">tokenized_train_ds</span><span class="p">,</span> <span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_train_ds.pkl'</span><span class="p">)</span>
    <span class="n">pickle_dataset</span><span class="p">(</span><span class="n">tokenized_eval_ds</span><span class="p">,</span> <span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_eval_ds.pkl'</span><span class="p">)</span>
    <span class="n">pickle_dataset</span><span class="p">(</span><span class="n">tokenized_test_ds</span><span class="p">,</span> <span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_test_ds.pkl'</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Files already exist, so load datasets"</span><span class="p">)</span>
    <span class="n">tokenized_train_ds</span> <span class="o">=</span> <span class="n">load_pickle_dataset</span><span class="p">(</span><span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_train_ds.pkl'</span><span class="p">)</span>
    <span class="n">tokenized_eval_ds</span> <span class="o">=</span> <span class="n">load_pickle_dataset</span><span class="p">(</span><span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_eval_ds.pkl'</span><span class="p">)</span>
    <span class="n">tokenized_test_ds</span> <span class="o">=</span> <span class="n">load_pickle_dataset</span><span class="p">(</span><span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_test_ds.pkl'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Files already exist, so load datasets
Dataset has been loaded from: ./data/Meta-Llama-3-8B-small_tokenized_train_ds.pkl
Dataset has been loaded from: ./data/Meta-Llama-3-8B-small_tokenized_eval_ds.pkl
Dataset has been loaded from: ./data/Meta-Llama-3-8B-small_tokenized_test_ds.pkl
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=307cc4fb-9766-4b0f-943b-4a1c90053e09">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Look-at-the-tokenized-data">Look at the tokenized data<a class="anchor-link" href="#Look-at-the-tokenized-data"></a></h3><p>Notice what the actual data looks like, and then the tokenized data which is a bunch of numbers, and then the attention mask at the end.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=44321182-e1b9-4570-bd24-7a5cf42ba504">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[11]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">count_train_records</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenized_train_ds</span><span class="p">)</span>
<span class="n">count_eval_records</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenized_eval_ds</span><span class="p">)</span>
<span class="n">count_test_records</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenized_test_ds</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of records in training dataset: </span><span class="si">{</span><span class="n">count_train_records</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of records in evaluation dataset: </span><span class="si">{</span><span class="n">count_eval_records</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of records in test dataset: </span><span class="si">{</span><span class="n">count_test_records</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">count_total_records</span> <span class="o">=</span> <span class="n">count_train_records</span> <span class="o">+</span> <span class="n">count_eval_records</span> <span class="o">+</span> <span class="n">count_test_records</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Total number of records: </span><span class="si">{</span><span class="n">count_total_records</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Number of records in training dataset: 33611
Number of records in evaluation dataset: 7203
Number of records in test dataset: 7203
Total number of records: 48017
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=64be678b-d2a8-403e-bcc8-ef9e7eabb0cf">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[12]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">first_record</span> <span class="o">=</span> <span class="n">tokenized_train_ds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">first_record</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>{'article': "In a year where homicides, rapes and robberies increased slightly, New York City still saw serious crime drop 1.7 percent in 2015, continuing an overall decline that began in the 1990s, NYPD Commissioner William Bratton said Monday.\nAt a news conference with Mayor Bill de Blasio, Bratton touted last years crime statistics, which he said, when combined with an even larger decline in 2014, put to rest the fear that substantial decreases couldnt continue under the new administration at City Hall.\nWhile we have had some fluctuation, some increases in certain categories, the overall trend in all our crime categories continues to go down, Bratton told reporters. It was a very good year for us, 2015.\nHomicides increased by 4.5 percent in 2015, rising to 350 from 333 in the prior year, which was the lowest since 1994, said Deputy Commissioner Dermot Shea. Rapes increased 6 percent and robberies rose 2 percent, said Shea, who is in charge of data collection and operations for the NYPD.\nThe lower overall crime statistics came about due to what Shea called targeted enforcement, where cops make quality arrests even though the overall number of apprehensions was the lowest in the city since 2003.\nTwo boroughs  Manhattan and the Bronx  actually saw serious crimes increase by 3 percent and 4 percent, respectively, Shea said. Manhattans increase was driven by more robberies, while the Bronx, although seeing an overall crime increase, had what he said was a phenomenal reduction in shootings. Citywide, shootings were down in 2015 about 3 percent, to 1,103 from 1,172 in 2014.\nShea largely attributed the 2015 increase in rapes to victims coming forward with complaints about attacks from years past.\nSign up to get the latest updates Get Newsday's Breaking News alerts in your inbox. By clicking Sign up, you agree to our privacy policy.\nTwenty percent of these rapes didnt happen in 2015, he said.\nThe NYPD has seen an increase in rapes involving single women who, after a night of drinking, get into cabs of all kinds and are attacked, Shea said.\nThey get driven, and passing out and waking up in a desolate area, and they get sexually attacked. This is something, really, that people need to be exceptionally aware of, and like any case in New York City, the buddy system works, said Shea, referring to the need for people to travel in pairs when taking a cab at night.\nBratton and police brass hope to build upon the continuing drop in overall crime by using technology such as ShotSpotter and a newly minted GPS system for police cars.\nJessica Tisch, NYPD deputy commissioner for technology, said ShotSpotter, an acoustical system that detects gunfire, identified gunshots in 1,672 cases, mostly in Brooklyn. Of those alerts, 74 percent didnt have any 911 calls from the public associated with them.\nTisch said ShotSpotter helped police recover ballistic evidence in 19 percent of the gunfire alerts. In 22 percent of those cases, Tisch said, cops were able to make positive matches of bullets with those from guns used in earlier shootings.\nTisch also highlighted a special GPS system being tried in about 5,000 patrol cars that allows the NYPD to see where its vehicles are and to track their movements over a 24-hour period, as well as gather information about the officers driving.\n", 'label': 0, 'input_ids': [128000, 644, 264, 1060, 1405, 89495, 11, 96330, 323, 71650, 552, 7319, 10284, 11, 1561, 4356, 4409, 2103, 5602, 6129, 9977, 6068, 220, 16, 13, 22, 3346, 304, 220, 679, 20, 11, 14691, 459, 8244, 18174, 430, 6137, 304, 279, 220, 2550, 15, 82, 11, 74255, 30454, 12656, 3320, 266, 783, 1071, 7159, 627, 1688, 264, 3754, 10017, 449, 22868, 8766, 409, 93493, 11, 3320, 266, 783, 67528, 1566, 1060, 753, 9977, 13443, 11, 902, 568, 1071, 11, 994, 11093, 449, 459, 1524, 8294, 18174, 304, 220, 679, 19, 11, 2231, 311, 2800, 279, 8850, 430, 12190, 43154, 7846, 1431, 3136, 1234, 279, 502, 8735, 520, 4409, 11166, 627, 2118, 8142, 584, 617, 1047, 1063, 39388, 4090, 11, 1063, 12992, 304, 3738, 11306, 11, 279, 8244, 9327, 304, 682, 1057, 9977, 11306, 9731, 311, 733, 1523, 2476, 3320, 266, 783, 3309, 19578, 13, 1054, 2181, 574, 264, 1633, 1695, 1060, 369, 603, 11, 220, 679, 20, 627, 39, 3151, 3422, 7319, 555, 220, 19, 13, 20, 3346, 304, 220, 679, 20, 11, 16448, 311, 220, 8652, 505, 220, 8765, 304, 279, 4972, 1060, 11, 902, 574, 279, 15821, 2533, 220, 2550, 19, 11, 1071, 32724, 30454, 76508, 354, 86068, 13, 432, 9521, 7319, 220, 21, 3346, 323, 71650, 552, 16392, 220, 17, 3346, 11, 1071, 86068, 11, 889, 374, 304, 6900, 315, 828, 4526, 323, 7677, 369, 279, 74255, 627, 791, 4827, 8244, 9977, 13443, 3782, 922, 4245, 311, 1148, 86068, 2663, 1054, 5775, 291, 13627, 2476, 1405, 35317, 1304, 4367, 38811, 1524, 3582, 279, 8244, 1396, 315, 47291, 4769, 574, 279, 15821, 304, 279, 3363, 2533, 220, 1049, 18, 627, 11874, 66841, 82, 2001, 29890, 323, 279, 66236, 2001, 3604, 5602, 6129, 17073, 5376, 555, 220, 18, 3346, 323, 220, 19, 3346, 11, 15947, 11, 86068, 1071, 13, 29890, 753, 5376, 574, 16625, 555, 810, 71650, 552, 11, 1418, 279, 66236, 11, 8051, 9298, 459, 8244, 9977, 5376, 11, 1047, 1148, 568, 1071, 574, 264, 1054, 15112, 6431, 278, 863, 14278, 304, 44861, 13, 4409, 9328, 11, 44861, 1051, 1523, 304, 220, 679, 20, 922, 220, 18, 3346, 11, 311, 220, 16, 11, 6889, 505, 220, 16, 11, 10861, 304, 220, 679, 19, 627, 8100, 64, 14090, 30706, 279, 220, 679, 20, 5376, 304, 96330, 311, 12697, 5108, 4741, 449, 21859, 922, 8951, 505, 1667, 3347, 627, 7412, 709, 311, 636, 279, 5652, 9013, 2175, 5513, 1316, 596, 52624, 5513, 30350, 304, 701, 23732, 13, 3296, 18965, 7220, 709, 11, 499, 7655, 311, 1057, 12625, 4947, 627, 2118, 76896, 3346, 315, 1521, 96330, 3287, 1431, 3621, 304, 220, 679, 20, 2476, 568, 1071, 627, 791, 74255, 706, 3970, 459, 5376, 304, 96330, 16239, 3254, 3278, 889, 11, 1306, 264, 3814, 315, 16558, 11, 636, 1139, 272, 3518, 315, 682, 13124, 323, 527, 18855, 11, 86068, 1071, 627, 46690, 636, 16625, 11, 323, 12579, 704, 323, 48728, 709, 304, 264, 951, 34166, 3158, 11, 323, 814, 636, 27681, 18855, 13, 1115, 374, 2555, 11, 2216, 11, 430, 1274, 1205, 311, 387, 48298, 8010, 315, 11, 323, 1093, 904, 1162, 304, 1561, 4356], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=b2d7f4f8-9eb3-4feb-86e7-fb0dad88753f">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Turn-on-accelerate">Turn on accelerate<a class="anchor-link" href="#Turn-on-accelerate"></a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=f3eab29e-fa47-4a13-abc4-d6425ae741b6">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[19]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">accelerate</span> <span class="kn">import</span> <span class="n">FullyShardedDataParallelPlugin</span><span class="p">,</span> <span class="n">Accelerator</span>
<span class="kn">from</span> <span class="nn">torch.distributed.fsdp.fully_sharded_data_parallel</span> <span class="kn">import</span> <span class="n">FullOptimStateDictConfig</span><span class="p">,</span> <span class="n">FullStateDictConfig</span>

<span class="n">fsdp_plugin</span> <span class="o">=</span> <span class="n">FullyShardedDataParallelPlugin</span><span class="p">(</span>
    <span class="n">state_dict_config</span><span class="o">=</span><span class="n">FullStateDictConfig</span><span class="p">(</span><span class="n">offload_to_cpu</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rank0_only</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="n">optim_state_dict_config</span><span class="o">=</span><span class="n">FullOptimStateDictConfig</span><span class="p">(</span><span class="n">offload_to_cpu</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rank0_only</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
<span class="p">)</span>

<span class="n">accelerator</span> <span class="o">=</span> <span class="n">Accelerator</span><span class="p">(</span><span class="n">fsdp_plugin</span><span class="o">=</span><span class="n">fsdp_plugin</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=22e040f1-e596-476d-9f26-ffa6f8a4a548">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="LoRA---Low-Rank-Adaptation">LoRA - Low-Rank Adaptation<a class="anchor-link" href="#LoRA---Low-Rank-Adaptation"></a></h3>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=bc8b8091-6179-4df0-b5bd-b29ec2dde4d5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>LoRA, short for Low-Rank Adaptation, is a technique designed to efficiently fine-tune large pre-trained models without the need to update all the model parameters, significantly reducing the computational and memory overhead typically associated with training. LoRA targets the challenge of adapting massive models, particularly in natural language processing (NLP) and computer vision, to specialized tasks while keeping the resource requirements manageable.</p>
<p>
Gradient checkpointing for the model. Gradient checkpointing is a technique used to reduce memory usage during the training of deep neural networks by trading compute for memory. It works by storing a minimal set of intermediate activations during the forward pass and then recomputing the others during the backward pass. This is particularly useful for training large models or using larger batch sizes.
</p>
<p>
The forward pass is the process where the input data is passed through the network from the input layer to the output layer. During this pass, the network performs a series of computations at each layer, applying weights to the inputs, adding biases (if applicable), and passing the result through an activation function. The final output of the forward pass is the prediction made by the network. The main goal of the forward pass is to compute the output given the current state of the model's parameters (weights and biases). This output is then used to calculate the loss, which quantifies how well the model's predictions match the actual labels.
</p>
<p>
The backward pass, or backpropagation, is the process of computing the gradient of the loss function with respect to each parameter in the network. This involves applying the chain rule of calculus to take derivatives step-by-step from the output layer back to the input layer. Essentially, it calculates how much each parameter contributed to the error in the prediction. The purpose of the backward pass is to update the model's parameters in a way that minimally reduces the loss, improving the model's predictions. The gradients calculated during this pass indicate the direction in which each parameter should be adjusted to decrease the error. Using an optimization algorithm (e.g., Stochastic Gradient Descent), these gradients are then used to update the weights to minimize the loss.
</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=41956597-0441-4bc6-aefe-fc4b0d5349c5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>
r: This parameter specifies the rank of the low-rank matrices that are introduced by LoRA. A smaller rank means fewer parameters to train, leading to a more memory-efficient fine-tuning process.
</p>
<p>
lora_alpha: This multiplier adjusts the scale of the LoRA parameters. A higher value increases the capacity of the LoRA adjustments to the original model weights.
</p>
<p>
target_modules: Lists the specific parts of the model to which LoRA will be applied. These typically correspond to components within transformer blocks, such as the query, key, value, and output projections in attention mechanisms, as well as any additional modules relevant to the model architecture.
</p>
<p>
bias: Specifies how biases are treated in the adaptation process. In this case, biases are not adjusted ("none").
</p>
<p>
lora_dropout: Sets the dropout rate for the LoRA parameters, helping to prevent overfitting during fine-tuning. The dropout rate is a hyperparameter used in the training of neural networks, representing the probability that a given neuron (or unit) is temporarily "dropped" from the network during a specific iteration of training. This means that the neuron will not participate in the forward pass and its contribution to the backward pass (gradient computation) is also ignored during that iteration. Dropout is applied randomly to a subset of neurons in the network at each training step.
</p>
<p>
task_type: Indicates the type of task for which the model is being fine-tuned. The example uses TaskType.SEQ_CLS, 
suggesting a sequence classification task, such as sentiment analysis or document classification. In my case a binary classification of machine versus human generated text.
</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=0532bdc6-4317-474b-859c-4e5d2fe6f1bd">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[14]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">prepare_model_for_kbit_training</span><span class="p">,</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">get_peft_model</span><span class="p">,</span> <span class="n">TaskType</span>

<span class="n">model</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">prepare_model_for_kbit_training</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span>
    <span class="n">r</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span>
        <span class="s2">"q_proj"</span><span class="p">,</span>
        <span class="s2">"k_proj"</span><span class="p">,</span>
        <span class="s2">"v_proj"</span><span class="p">,</span>
        <span class="s2">"o_proj"</span><span class="p">,</span>
        <span class="s2">"gate_proj"</span><span class="p">,</span>
        <span class="s2">"up_proj"</span><span class="p">,</span>
        <span class="s2">"down_proj"</span><span class="p">,</span>
        <span class="s2">"lm_head"</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="n">bias</span><span class="o">=</span><span class="s2">"none"</span><span class="p">,</span>
    <span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">task_type</span><span class="o">=</span><span class="n">TaskType</span><span class="o">.</span><span class="n">SEQ_CLS</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">accelerator</span><span class="o">.</span><span class="n">prepare_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=1cd29494-4453-4c01-b878-ef2f4533d34d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Inspect-the-model">Inspect the model<a class="anchor-link" href="#Inspect-the-model"></a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=d5058f6d-185d-45af-83b9-bb7f61d4bee3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[15]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">print_trainable_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">trainable_params</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">all_param</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
        <span class="n">all_param</span> <span class="o">+=</span> <span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="n">trainable_params</span> <span class="o">+=</span> <span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"trainable params: </span><span class="si">{</span><span class="n">trainable_params</span><span class="si">}</span><span class="s2"> || all params: </span><span class="si">{</span><span class="n">all_param</span><span class="si">}</span><span class="s2"> || trainable%: </span><span class="si">{</span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">trainable_params</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">all_param</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=b14fe8f2-eccd-4cfe-9d20-ef48bc178842">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[16]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">print_trainable_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">model</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>trainable params: 20979712 || all params: 4036251648 || trainable%: 0.5197820609226791
</pre>
</div>
</div>
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[16]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>PeftModelForSequenceClassification(
  (base_model): LoraModel(
    (model): LlamaForSequenceClassification(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaSdpaAttention(
              (q_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (v_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=14336, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (up_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=14336, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (down_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=14336, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm()
            (post_attention_layernorm): LlamaRMSNorm()
          )
        )
        (norm): LlamaRMSNorm()
      )
      (score): ModulesToSaveWrapper(
        (original_module): Linear(in_features=4096, out_features=2, bias=False)
        (modules_to_save): ModuleDict(
          (default): Linear(in_features=4096, out_features=2, bias=False)
        )
      )
    )
  )
)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=e17b4782-03f8-4335-bfda-2a65794bff2c">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Look-at-hardware">Look at hardware<a class="anchor-link" href="#Look-at-hardware"></a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=c27c3328-1926-4adc-8696-b3b343ec4afd">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[17]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Available GPUs: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"GPU </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_name</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Available GPUs: 1
GPU 0: NVIDIA GeForce RTX 4090
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=bdf68d0e-8786-489d-a4b8-b7478513efea">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[18]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">!</span>nvidia-smi
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Mon May 27 09:12:23 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 3090        Off |   00000000:01:00.0 Off |                  N/A |
| 31%   30C    P8             35W /  420W |      10MiB /  24576MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA GeForce RTX 4090        Off |   00000000:02:00.0  On |                  Off |
|  0%   47C    P2             69W /  450W |    6198MiB /  24564MiB |     10%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA GeForce RTX 3090 Ti     Off |   00000000:2B:00.0 Off |                  Off |
| 30%   35C    P8             24W /  450W |      10MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA GeForce RTX 3090        Off |   00000000:41:00.0 Off |                  N/A |
| 32%   29C    P8             17W /  420W |      10MiB /  24576MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA GeForce RTX 4090        Off |   00000000:42:00.0 Off |                  Off |
|  0%   50C    P2             62W /  450W |    4083MiB /  24564MiB |     28%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA GeForce RTX 4090        Off |   00000000:61:00.0 Off |                  Off |
|  0%   41C    P8             11W /  450W |      11MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA GeForce RTX 3090 Ti     Off |   00000000:62:00.0 Off |                  Off |
| 30%   32C    P8             16W /  450W |      10MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A      2513      G   /usr/lib/xorg/Xorg                              4MiB |
|    1   N/A  N/A      2513      G   /usr/lib/xorg/Xorg                             54MiB |
|    1   N/A  N/A      2596      G   /usr/bin/gnome-shell                           13MiB |
|    1   N/A  N/A     16287      C   /usr/bin/python3                             6114MiB |
|    2   N/A  N/A      2513      G   /usr/lib/xorg/Xorg                              4MiB |
|    3   N/A  N/A      2513      G   /usr/lib/xorg/Xorg                              4MiB |
|    4   N/A  N/A      2513      G   /usr/lib/xorg/Xorg                              4MiB |
|    4   N/A  N/A     16301      C   /usr/bin/python3                             4066MiB |
|    5   N/A  N/A      2513      G   /usr/lib/xorg/Xorg                              4MiB |
|    6   N/A  N/A      2513      G   /usr/lib/xorg/Xorg                              4MiB |
+-----------------------------------------------------------------------------------------+
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=2340f2d1-1cc7-454e-bad6-bf4ac2c46fc9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">model</span><span class="o">.</span><span class="n">is_parallelizable</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">model</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=1199841a-6519-4422-8259-2fdbb114e466">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[21]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span><span class="p">,</span> <span class="n">accuracy_score</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">logits_and_labels</span><span class="p">):</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">logits_and_labels</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
    <span class="n">f1</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">'macro'</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">'accuracy'</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span> <span class="s1">'f1'</span><span class="p">:</span> <span class="n">f1</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=b8c7f3ee-6997-4ed7-b28e-5d574831fb08">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[22]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">project_name</span> <span class="o">=</span> <span class="s2">"praxis-"</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s2">"-small-finetune"</span>
<span class="n">output_dir_path</span> <span class="o">=</span> <span class="s2">"./"</span> <span class="o">+</span> <span class="n">project_name</span>
<span class="n">output_dir_path</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[22]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>'./praxis-Meta-Llama-3-8B-small-finetune'</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=e7a69bde-1d50-46a0-9838-00812afbdb16">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[23]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=ca48a7aa-41d7-4c57-bfec-093d22bf4b54">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p><strong>output_dir</strong> (<code>output_dir_path</code>): This specifies the directory where outputs such as model checkpoints and logs will be saved. It's important for organizing the outputs of your training sessions.</p>
<p><strong>warmup_steps</strong> (<code>5</code>): This parameter sets the number of steps during which the learning rate will gradually increase from zero to the initially set learning rate. This warmup phase helps stabilize the model's training early on, preventing the model from diverging due to high gradient values at the start.</p>
<p><strong>per_device_train_batch_size</strong> (<code>32</code>): This sets the number of training examples to process on each device (like a GPU) during training. A higher batch size can speed up training but may require more memory.</p>
<p><strong>per_device_eval_batch_size</strong> (<code>32</code>): Similar to the training batch size, this is the number of examples to process on each device during evaluation. It determines how quickly the model can process the evaluation data.</p>
<p><strong>num_train_epochs</strong> (<code>10</code>): This defines the total number of times the training process should iterate over the entire dataset. More epochs can lead to better learning but also risk overfitting if too high.</p>
<p><strong>gradient_checkpointing</strong> (<code>False</code>): If set to <code>True</code>, this would enable gradient checkpointing to reduce memory usage at the cost of longer training time. It's useful for very deep models that otherwise would not fit into GPU memory.</p>
<p><strong>gradient_accumulation_steps</strong> (<code>2</code>): This setting allows you to accumulate gradients over multiple steps before performing an update on the model's weights. It's a way to effectively increase the batch size without increasing the memory load, which can be helpful when dealing with hardware constraints.</p>
<p><strong>max_steps</strong> (<code>500</code>): This is the maximum number of training steps to execute, regardless of how many epochs are set. Training will stop when this number of steps is reached.</p>
<p><strong>learning_rate</strong> (<code>2.5e-5</code>): This is the step size at which the optimizer updates the models weights. A smaller learning rate might lead to better fine-tuning but slower convergence, and vice versa.</p>
<p><strong>logging_steps</strong> (<code>200</code>): Specifies how often to log training information. The setting determines after how many steps new logs should be created, which might include loss and other metrics. More frequent logging provides finer-grained visibility into the training progress but can add computational overhead.</p>
<p><strong>bf16</strong> (<code>True</code>): This would enable training using bfloat16 precision, which is a mixed precision format with fewer bits than the standard single-precision floating point (fp32). Like fp16, it can reduce memory usage and potentially speed up training if supported by the hardware. It's particularly useful on TPUs and newer GPUs that support this format. (4090)</p>
<p><strong>fp16</strong> (<code>True</code>): This enables half-precision floating point (16-bit) training. It reduces memory usage and can speed up training, provided the hardware (like modern GPUs) supports it. (3090 and 4090)</p>
<p><strong>optim</strong> (<code>"paged_adamw_8bit"</code>): Specifies the optimizer to use. "paged_adamw_8bit" might refer to a variation of the AdamW optimizer that is optimized for lower precision and memory bandwidth, enhancing training speed and efficiency.</p>
<p><strong>logging_dir</strong> (<code>"./logs"</code>): This specifies the directory where training logs should be saved. It's used to store logs if you are using a logging framework or callback that writes out logs to files. Organizing logs in a specific directory is helpful for post-training analysis and for monitoring the training process through tools like TensorBoard.</p>
<p><strong>save_strategy</strong> (<code>"epoch"</code>): This determines how often to save model checkpoints. Setting it to <code>"epoch"</code> means the model will save checkpoints at the end of each epoch.</p>
<p><strong>save_steps</strong> (<code>50</code>): This is closely related to the <code>save_strategy</code> when set to "steps". It defines how often to save the model, specifically after how many training steps. A lower number means more frequent saves, which increases disk I/O but provides more restore points for training.</p>
<p><strong>evaluation_strategy</strong> (<code>"epoch"</code>): This configures when the model should be evaluated against the evaluation dataset. Like <code>save_strategy</code>, setting this to <code>"epoch"</code> triggers evaluations at the end of each epoch, providing feedback on model performance after it has seen the entire training dataset.</p>
<p><strong>eval_steps</strong> (<code>50</code>): This parameter determines how often to evaluate the model if the <code>evaluation_strategy</code> is set to "steps". Similar to <code>logging_steps</code>, setting this affects how frequently the model's performance is assessed on the evaluation dataset during the training process. More frequent evaluations provide a closer look at the model's performance but at the cost of increased computational overhead.</p>
<p><strong>do_eval</strong> (<code>True</code>): This flag enables the evaluation of the model on the evaluation dataset. If <code>True</code>, it will use the evaluation dataset to assess model performance based on metrics</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=d8f3cd41-fd17-4fd8-83b8-54f464df79ff">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Training">Training<a class="anchor-link" href="#Training"></a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=2df4e2a5-2a08-434b-983a-f6cd42f33da3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[23]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">transformers</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="n">transformers</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">777</span><span class="p">)</span>

<span class="k">if</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_train_ds</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">tokenized_eval_ds</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">transformers</span><span class="o">.</span><span class="n">TrainingArguments</span><span class="p">(</span>
        <span class="n">output_dir</span><span class="o">=</span><span class="n">output_dir_path</span><span class="p">,</span>
        <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">logging_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">logging_dir</span><span class="o">=</span><span class="n">output_dir_path</span><span class="o">+</span><span class="s2">"/logs"</span><span class="p">,</span>
        <span class="n">evaluation_strategy</span><span class="o">=</span><span class="s1">'epoch'</span><span class="p">,</span>
        <span class="n">save_strategy</span><span class="o">=</span><span class="s1">'epoch'</span><span class="p">,</span>
        <span class="n">bf16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">optim</span><span class="o">=</span><span class="s2">"paged_adamw_8bit"</span><span class="p">,</span>
        <span class="n">do_eval</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>
    <span class="n">data_collator</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">DataCollatorWithPadding</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">),</span>
<span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">resume_from_checkpoint</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># Turn to True if power goes out...</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>2024-05-27 09:12:24.473094: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-27 09:12:25.080874: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
<span class="ansi-blue-intense-fg ansi-bold">wandb</span>: Currently logged in as: <span class="ansi-yellow-fg">nispoe</span>. Use <span class="ansi-bold">`wandb login --relogin`</span> to force relogin
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
Tracking run with wandb version 0.17.0
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
Run data is saved locally in <code>/home/nispoe/kuk/Praxis/wandb/run-20240527_091226-uhpml8j2</code>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
Syncing run <strong><a href="https://wandb.ai/nispoe/huggingface/runs/uhpml8j2" target="_blank">revived-butterfly-344</a></strong> to <a href="https://wandb.ai/nispoe/huggingface" target="_blank">Weights &amp; Biases</a> (<a href="https://wandb.me/run" target="_blank">docs</a>)<br/>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
 View project at <a href="https://wandb.ai/nispoe/huggingface" target="_blank">https://wandb.ai/nispoe/huggingface</a>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
 View run at <a href="https://wandb.ai/nispoe/huggingface/runs/uhpml8j2" target="_blank">https://wandb.ai/nispoe/huggingface/runs/uhpml8j2</a>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<div>
<progress max="42020" style="width:300px; height:20px; vertical-align: middle;" value="42020"></progress>
      [42020/42020 6:53:49, Epoch 10/10]
    </div>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: left;">
<th>Epoch</th>
<th>Training Loss</th>
<th>Validation Loss</th>
<th>Accuracy</th>
<th>F1</th>
</tr>
</thead>
<tbody>
<tr>
<td>9</td>
<td>0.000000</td>
<td>0.037216</td>
<td>0.997085</td>
<td>0.996918</td>
</tr>
<tr>
<td>10</td>
<td>0.000000</td>
<td>0.026434</td>
<td>0.997918</td>
<td>0.997800</td>
</tr>
</tbody>
</table><p>
</p></div>
</div>
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[23]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>TrainOutput(global_step=42020, training_loss=1.8165165869637025e-05, metrics={'train_runtime': 24834.0702, 'train_samples_per_second': 13.534, 'train_steps_per_second': 1.692, 'total_flos': 7.22830417723392e+18, 'train_loss': 1.8165165869637025e-05, 'epoch': 10.0})</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=0d13982d-53cb-4c88-bd32-a98d4a5a9874">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Determine-best-checkpoint">Determine best checkpoint<a class="anchor-link" href="#Determine-best-checkpoint"></a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=8bd570c7-8feb-4b69-bda7-9c678bfd92c3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[24]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">!</span>ls<span class="w"> </span>-ltr<span class="w"> </span><span class="o">{</span>output_dir_path<span class="o">}</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>total 44
drwxrwxr-x 2 nispoe nispoe 4096 May 26 04:32 checkpoint-4202
drwxrwxr-x 2 nispoe nispoe 4096 May 26 07:59 checkpoint-8404
drwxrwxr-x 2 nispoe nispoe 4096 May 26 11:26 checkpoint-12606
drwxrwxr-x 2 nispoe nispoe 4096 May 26 14:53 checkpoint-16808
drwxrwxr-x 2 nispoe nispoe 4096 May 26 18:21 checkpoint-21010
drwxrwxr-x 2 nispoe nispoe 4096 May 26 21:48 checkpoint-25212
drwxrwxr-x 2 nispoe nispoe 4096 May 27 01:15 checkpoint-29414
drwxrwxr-x 2 nispoe nispoe 4096 May 27 04:42 checkpoint-33616
drwxr-xr-x 2 nispoe nispoe 4096 May 27 09:12 logs
drwxrwxr-x 2 nispoe nispoe 4096 May 27 12:39 checkpoint-37818
drwxrwxr-x 2 nispoe nispoe 4096 May 27 16:06 checkpoint-42020
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=97771269-d77c-4c1b-b264-34e082db6cbc">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[24]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.python.summary.summary_iterator</span> <span class="kn">import</span> <span class="n">summary_iterator</span>
<span class="kn">import</span> <span class="nn">glob</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># Construct the logs directory path</span>
<span class="n">logs_directory</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">'./'</span><span class="p">,</span> <span class="n">project_name</span><span class="p">,</span> <span class="s1">'logs'</span><span class="p">)</span>
<span class="n">file_pattern</span> <span class="o">=</span> <span class="s1">'events.out.tfevents.*'</span>

<span class="c1"># Retrieve all event files matching the pattern</span>
<span class="n">event_files</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">logs_directory</span><span class="p">,</span> <span class="n">file_pattern</span><span class="p">))</span>

<span class="c1"># Function to print out TensorBoard event logs</span>
<span class="k">def</span> <span class="nf">print_events_from_file</span><span class="p">(</span><span class="n">event_files</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">event_file</span> <span class="ow">in</span> <span class="n">event_files</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Reading events from file: </span><span class="si">{</span><span class="n">event_file</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">summary_iterator</span><span class="p">(</span><span class="n">event_file</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">e</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">v</span><span class="o">.</span><span class="n">HasField</span><span class="p">(</span><span class="s1">'simple_value'</span><span class="p">):</span>
                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Step: </span><span class="si">{</span><span class="n">e</span><span class="o">.</span><span class="n">step</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">v</span><span class="o">.</span><span class="n">simple_value</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>  <span class="c1"># Just in case the event file is not readable</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Failed to read </span><span class="si">{</span><span class="n">event_file</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">print_events_from_file</span><span class="p">(</span><span class="n">event_files</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Reading events from file: ./praxis-Meta-Llama-3-8B-small-finetune/logs/events.out.tfevents.1716819146.hephaestus.16287.0
Step: 33620, train/loss: 0.0
Step: 33620, train/grad_norm: 1.1912689501514251e-08
Step: 33620, train/learning_rate: 9.995240361604374e-06
Step: 33620, train/epoch: 8.000951766967773
Step: 33630, train/loss: 0.0
Step: 33630, train/grad_norm: 2.9801533463214014e-10
Step: 33630, train/learning_rate: 9.983341442421079e-06
Step: 33630, train/epoch: 8.003332138061523
Step: 33640, train/loss: 0.0
Step: 33640, train/grad_norm: 2.53775306324755e-10
Step: 33640, train/learning_rate: 9.971442523237783e-06
Step: 33640, train/epoch: 8.005711555480957
Step: 33650, train/loss: 0.0
Step: 33650, train/grad_norm: 7.385338118837126e-09
Step: 33650, train/learning_rate: 9.959542694559786e-06
Step: 33650, train/epoch: 8.00809097290039
Step: 33660, train/loss: 0.0
Step: 33660, train/grad_norm: 4.258890484720723e-08
Step: 33660, train/learning_rate: 9.947643775376491e-06
Step: 33660, train/epoch: 8.01047134399414
Step: 33670, train/loss: 0.0
Step: 33670, train/grad_norm: 1.0427200075602272e-10
Step: 33670, train/learning_rate: 9.935744856193196e-06
Step: 33670, train/epoch: 8.012850761413574
Step: 33680, train/loss: 0.0
Step: 33680, train/grad_norm: 7.328372220172241e-08
Step: 33680, train/learning_rate: 9.9238459370099e-06
Step: 33680, train/epoch: 8.015231132507324
Step: 33690, train/loss: 0.0
Step: 33690, train/grad_norm: 1.1585224335419753e-08
Step: 33690, train/learning_rate: 9.911947017826606e-06
Step: 33690, train/epoch: 8.017610549926758
Step: 33700, train/loss: 0.0
Step: 33700, train/grad_norm: 8.051807043329973e-08
Step: 33700, train/learning_rate: 9.900047189148609e-06
Step: 33700, train/epoch: 8.019990921020508
Step: 33710, train/loss: 0.0
Step: 33710, train/grad_norm: 8.476674473456569e-09
Step: 33710, train/learning_rate: 9.888148269965313e-06
Step: 33710, train/epoch: 8.022370338439941
Step: 33720, train/loss: 0.0
Step: 33720, train/grad_norm: 5.01833352650749e-10
Step: 33720, train/learning_rate: 9.876249350782018e-06
Step: 33720, train/epoch: 8.024749755859375
Step: 33730, train/loss: 0.0
Step: 33730, train/grad_norm: 8.810956408922266e-10
Step: 33730, train/learning_rate: 9.864350431598723e-06
Step: 33730, train/epoch: 8.027130126953125
Step: 33740, train/loss: 0.0
Step: 33740, train/grad_norm: 4.6975663359205555e-09
Step: 33740, train/learning_rate: 9.852451512415428e-06
Step: 33740, train/epoch: 8.029509544372559
Step: 33750, train/loss: 0.0
Step: 33750, train/grad_norm: 3.959787136409432e-06
Step: 33750, train/learning_rate: 9.84055168373743e-06
Step: 33750, train/epoch: 8.031889915466309
Step: 33760, train/loss: 0.0
Step: 33760, train/grad_norm: 5.088438115308236e-09
Step: 33760, train/learning_rate: 9.828652764554136e-06
Step: 33760, train/epoch: 8.034269332885742
Step: 33770, train/loss: 0.0
Step: 33770, train/grad_norm: 3.1380469778241604e-08
Step: 33770, train/learning_rate: 9.81675384537084e-06
Step: 33770, train/epoch: 8.036648750305176
Step: 33780, train/loss: 0.0
Step: 33780, train/grad_norm: 1.6449258621875629e-09
Step: 33780, train/learning_rate: 9.804854926187545e-06
Step: 33780, train/epoch: 8.039029121398926
Step: 33790, train/loss: 0.0
Step: 33790, train/grad_norm: 4.23351275458117e-06
Step: 33790, train/learning_rate: 9.79295600700425e-06
Step: 33790, train/epoch: 8.04140853881836
Step: 33800, train/loss: 0.0
Step: 33800, train/grad_norm: 4.656527607949101e-09
Step: 33800, train/learning_rate: 9.781057087820955e-06
Step: 33800, train/epoch: 8.04378890991211
Step: 33810, train/loss: 0.0
Step: 33810, train/grad_norm: 2.4476496385261726e-09
Step: 33810, train/learning_rate: 9.769157259142958e-06
Step: 33810, train/epoch: 8.046168327331543
Step: 33820, train/loss: 0.0
Step: 33820, train/grad_norm: 2.1158008678412443e-10
Step: 33820, train/learning_rate: 9.757258339959662e-06
Step: 33820, train/epoch: 8.048548698425293
Step: 33830, train/loss: 0.0
Step: 33830, train/grad_norm: 2.0097989050782417e-08
Step: 33830, train/learning_rate: 9.745359420776367e-06
Step: 33830, train/epoch: 8.050928115844727
Step: 33840, train/loss: 0.0
Step: 33840, train/grad_norm: 3.3671287980041598e-09
Step: 33840, train/learning_rate: 9.733460501593072e-06
Step: 33840, train/epoch: 8.05330753326416
Step: 33850, train/loss: 0.0
Step: 33850, train/grad_norm: 1.4045318152966502e-07
Step: 33850, train/learning_rate: 9.721561582409777e-06
Step: 33850, train/epoch: 8.05568790435791
Step: 33860, train/loss: 0.0
Step: 33860, train/grad_norm: 4.448083679164938e-09
Step: 33860, train/learning_rate: 9.70966175373178e-06
Step: 33860, train/epoch: 8.058067321777344
Step: 33870, train/loss: 0.0
Step: 33870, train/grad_norm: 2.458735992583172e-10
Step: 33870, train/learning_rate: 9.697762834548485e-06
Step: 33870, train/epoch: 8.060447692871094
Step: 33880, train/loss: 0.0
Step: 33880, train/grad_norm: 6.9831846971624145e-09
Step: 33880, train/learning_rate: 9.68586391536519e-06
Step: 33880, train/epoch: 8.062827110290527
Step: 33890, train/loss: 0.0
Step: 33890, train/grad_norm: 5.415248338636047e-08
Step: 33890, train/learning_rate: 9.673964996181894e-06
Step: 33890, train/epoch: 8.065207481384277
Step: 33900, train/loss: 0.0
Step: 33900, train/grad_norm: 5.466846531021474e-09
Step: 33900, train/learning_rate: 9.662066076998599e-06
Step: 33900, train/epoch: 8.067586898803711
Step: 33910, train/loss: 0.0
Step: 33910, train/grad_norm: 5.658372174366377e-06
Step: 33910, train/learning_rate: 9.650166248320602e-06
Step: 33910, train/epoch: 8.069966316223145
Step: 33920, train/loss: 0.0
Step: 33920, train/grad_norm: 1.8127636849385453e-06
Step: 33920, train/learning_rate: 9.638267329137307e-06
Step: 33920, train/epoch: 8.072346687316895
Step: 33930, train/loss: 0.0
Step: 33930, train/grad_norm: 1.620941825208888e-09
Step: 33930, train/learning_rate: 9.626368409954011e-06
Step: 33930, train/epoch: 8.074726104736328
Step: 33940, train/loss: 0.0
Step: 33940, train/grad_norm: 4.326148328459567e-09
Step: 33940, train/learning_rate: 9.614469490770716e-06
Step: 33940, train/epoch: 8.077106475830078
Step: 33950, train/loss: 0.0
Step: 33950, train/grad_norm: 5.303022021507786e-07
Step: 33950, train/learning_rate: 9.602570571587421e-06
Step: 33950, train/epoch: 8.079485893249512
Step: 33960, train/loss: 0.0
Step: 33960, train/grad_norm: 4.383938989604985e-09
Step: 33960, train/learning_rate: 9.590670742909424e-06
Step: 33960, train/epoch: 8.081865310668945
Step: 33970, train/loss: 0.0
Step: 33970, train/grad_norm: 1.8695080825636978e-06
Step: 33970, train/learning_rate: 9.578771823726129e-06
Step: 33970, train/epoch: 8.084245681762695
Step: 33980, train/loss: 0.0
Step: 33980, train/grad_norm: 2.3673310534100267e-10
Step: 33980, train/learning_rate: 9.566872904542834e-06
Step: 33980, train/epoch: 8.086625099182129
Step: 33990, train/loss: 0.0
Step: 33990, train/grad_norm: 4.093203642696608e-07
Step: 33990, train/learning_rate: 9.554973985359538e-06
Step: 33990, train/epoch: 8.089005470275879
Step: 34000, train/loss: 0.0
Step: 34000, train/grad_norm: 2.0375283404927558e-10
Step: 34000, train/learning_rate: 9.543075066176243e-06
Step: 34000, train/epoch: 8.091384887695312
Step: 34010, train/loss: 0.0
Step: 34010, train/grad_norm: 2.106608221197348e-08
Step: 34010, train/learning_rate: 9.531175237498246e-06
Step: 34010, train/epoch: 8.093765258789062
Step: 34020, train/loss: 0.0
Step: 34020, train/grad_norm: 3.690502410336194e-07
Step: 34020, train/learning_rate: 9.519276318314951e-06
Step: 34020, train/epoch: 8.096144676208496
Step: 34030, train/loss: 0.0
Step: 34030, train/grad_norm: 5.80729242471989e-08
Step: 34030, train/learning_rate: 9.507377399131656e-06
Step: 34030, train/epoch: 8.09852409362793
Step: 34040, train/loss: 0.0
Step: 34040, train/grad_norm: 6.95029189756724e-09
Step: 34040, train/learning_rate: 9.49547847994836e-06
Step: 34040, train/epoch: 8.10090446472168
Step: 34050, train/loss: 0.0
Step: 34050, train/grad_norm: 1.630108936723218e-09
Step: 34050, train/learning_rate: 9.483579560765065e-06
Step: 34050, train/epoch: 8.103283882141113
Step: 34060, train/loss: 0.0
Step: 34060, train/grad_norm: 5.3063859972724e-09
Step: 34060, train/learning_rate: 9.471679732087068e-06
Step: 34060, train/epoch: 8.105664253234863
Step: 34070, train/loss: 0.0
Step: 34070, train/grad_norm: 1.0803518669944001e-11
Step: 34070, train/learning_rate: 9.459780812903773e-06
Step: 34070, train/epoch: 8.108043670654297
Step: 34080, train/loss: 0.0
Step: 34080, train/grad_norm: 1.184349093819037e-05
Step: 34080, train/learning_rate: 9.447881893720478e-06
Step: 34080, train/epoch: 8.110424041748047
Step: 34090, train/loss: 0.0
Step: 34090, train/grad_norm: 3.79671138972526e-09
Step: 34090, train/learning_rate: 9.435982974537183e-06
Step: 34090, train/epoch: 8.11280345916748
Step: 34100, train/loss: 0.0
Step: 34100, train/grad_norm: 1.586053177682345e-09
Step: 34100, train/learning_rate: 9.424084055353887e-06
Step: 34100, train/epoch: 8.115182876586914
Step: 34110, train/loss: 0.0
Step: 34110, train/grad_norm: 4.860735702827412e-10
Step: 34110, train/learning_rate: 9.41218422667589e-06
Step: 34110, train/epoch: 8.117563247680664
Step: 34120, train/loss: 0.0
Step: 34120, train/grad_norm: 2.0787067622762834e-09
Step: 34120, train/learning_rate: 9.400285307492595e-06
Step: 34120, train/epoch: 8.119942665100098
Step: 34130, train/loss: 0.0
Step: 34130, train/grad_norm: 2.4900126405213996e-08
Step: 34130, train/learning_rate: 9.3883863883093e-06
Step: 34130, train/epoch: 8.122323036193848
Step: 34140, train/loss: 0.0
Step: 34140, train/grad_norm: 1.84103446372319e-05
Step: 34140, train/learning_rate: 9.376487469126005e-06
Step: 34140, train/epoch: 8.124702453613281
Step: 34150, train/loss: 0.0
Step: 34150, train/grad_norm: 1.045811992561596e-09
Step: 34150, train/learning_rate: 9.36458854994271e-06
Step: 34150, train/epoch: 8.127081871032715
Step: 34160, train/loss: 0.0
Step: 34160, train/grad_norm: 7.141829314605275e-07
Step: 34160, train/learning_rate: 9.352689630759414e-06
Step: 34160, train/epoch: 8.129462242126465
Step: 34170, train/loss: 0.0
Step: 34170, train/grad_norm: 1.5299422839731847e-09
Step: 34170, train/learning_rate: 9.340789802081417e-06
Step: 34170, train/epoch: 8.131841659545898
Step: 34180, train/loss: 0.0
Step: 34180, train/grad_norm: 3.8735108454091005e-09
Step: 34180, train/learning_rate: 9.328890882898122e-06
Step: 34180, train/epoch: 8.134222030639648
Step: 34190, train/loss: 0.0
Step: 34190, train/grad_norm: 1.783468511762365e-11
Step: 34190, train/learning_rate: 9.316991963714827e-06
Step: 34190, train/epoch: 8.136601448059082
Step: 34200, train/loss: 0.0
Step: 34200, train/grad_norm: 1.3273704357885663e-09
Step: 34200, train/learning_rate: 9.305093044531532e-06
Step: 34200, train/epoch: 8.138981819152832
Step: 34210, train/loss: 0.0
Step: 34210, train/grad_norm: 8.749805324725912e-09
Step: 34210, train/learning_rate: 9.293194125348236e-06
Step: 34210, train/epoch: 8.141361236572266
Step: 34220, train/loss: 0.0
Step: 34220, train/grad_norm: 3.0634557113273786e-09
Step: 34220, train/learning_rate: 9.28129429667024e-06
Step: 34220, train/epoch: 8.1437406539917
Step: 34230, train/loss: 0.0
Step: 34230, train/grad_norm: 5.5626848904921644e-08
Step: 34230, train/learning_rate: 9.269395377486944e-06
Step: 34230, train/epoch: 8.14612102508545
Step: 34240, train/loss: 0.0
Step: 34240, train/grad_norm: 1.1481928741119418e-09
Step: 34240, train/learning_rate: 9.257496458303649e-06
Step: 34240, train/epoch: 8.148500442504883
Step: 34250, train/loss: 0.0
Step: 34250, train/grad_norm: 0.00020668348588515073
Step: 34250, train/learning_rate: 9.245597539120354e-06
Step: 34250, train/epoch: 8.150880813598633
Step: 34260, train/loss: 0.0
Step: 34260, train/grad_norm: 7.261201417918528e-09
Step: 34260, train/learning_rate: 9.233698619937059e-06
Step: 34260, train/epoch: 8.153260231018066
Step: 34270, train/loss: 0.0
Step: 34270, train/grad_norm: 1.0969499752988554e-09
Step: 34270, train/learning_rate: 9.221798791259062e-06
Step: 34270, train/epoch: 8.155640602111816
Step: 34280, train/loss: 0.0
Step: 34280, train/grad_norm: 1.093400326876437e-12
Step: 34280, train/learning_rate: 9.209899872075766e-06
Step: 34280, train/epoch: 8.15802001953125
Step: 34290, train/loss: 0.0
Step: 34290, train/grad_norm: 1.3737913584499495e-10
Step: 34290, train/learning_rate: 9.198000952892471e-06
Step: 34290, train/epoch: 8.160399436950684
Step: 34300, train/loss: 0.0
Step: 34300, train/grad_norm: 9.610527434630867e-13
Step: 34300, train/learning_rate: 9.186102033709176e-06
Step: 34300, train/epoch: 8.162779808044434
Step: 34310, train/loss: 0.0
Step: 34310, train/grad_norm: 1.215410883048662e-09
Step: 34310, train/learning_rate: 9.17420311452588e-06
Step: 34310, train/epoch: 8.165159225463867
Step: 34320, train/loss: 0.0
Step: 34320, train/grad_norm: 2.8451579510146985e-06
Step: 34320, train/learning_rate: 9.162303285847884e-06
Step: 34320, train/epoch: 8.167539596557617
Step: 34330, train/loss: 0.0
Step: 34330, train/grad_norm: 6.554745635156678e-10
Step: 34330, train/learning_rate: 9.150404366664588e-06
Step: 34330, train/epoch: 8.16991901397705
Step: 34340, train/loss: 0.0
Step: 34340, train/grad_norm: 2.2106092512075293e-09
Step: 34340, train/learning_rate: 9.138505447481293e-06
Step: 34340, train/epoch: 8.172298431396484
Step: 34350, train/loss: 0.0
Step: 34350, train/grad_norm: 1.7018724207673586e-08
Step: 34350, train/learning_rate: 9.126606528297998e-06
Step: 34350, train/epoch: 8.174678802490234
Step: 34360, train/loss: 0.0
Step: 34360, train/grad_norm: 4.2707006286946125e-06
Step: 34360, train/learning_rate: 9.114707609114703e-06
Step: 34360, train/epoch: 8.177058219909668
Step: 34370, train/loss: 0.0
Step: 34370, train/grad_norm: 5.501410438313314e-10
Step: 34370, train/learning_rate: 9.102807780436706e-06
Step: 34370, train/epoch: 8.179438591003418
Step: 34380, train/loss: 0.0
Step: 34380, train/grad_norm: 9.796413991125519e-08
Step: 34380, train/learning_rate: 9.09090886125341e-06
Step: 34380, train/epoch: 8.181818008422852
Step: 34390, train/loss: 0.0
Step: 34390, train/grad_norm: 2.0544066448024978e-11
Step: 34390, train/learning_rate: 9.079009942070115e-06
Step: 34390, train/epoch: 8.184198379516602
Step: 34400, train/loss: 0.0
Step: 34400, train/grad_norm: 3.276874593893808e-08
Step: 34400, train/learning_rate: 9.06711102288682e-06
Step: 34400, train/epoch: 8.186577796936035
Step: 34410, train/loss: 0.0
Step: 34410, train/grad_norm: 2.7769371246932906e-09
Step: 34410, train/learning_rate: 9.055212103703525e-06
Step: 34410, train/epoch: 8.188957214355469
Step: 34420, train/loss: 0.0
Step: 34420, train/grad_norm: 2.926644482315055e-10
Step: 34420, train/learning_rate: 9.043312275025528e-06
Step: 34420, train/epoch: 8.191337585449219
Step: 34430, train/loss: 0.0
Step: 34430, train/grad_norm: 4.077300719274035e-09
Step: 34430, train/learning_rate: 9.031413355842233e-06
Step: 34430, train/epoch: 8.193717002868652
Step: 34440, train/loss: 0.0
Step: 34440, train/grad_norm: 6.578993599903882e-11
Step: 34440, train/learning_rate: 9.019514436658937e-06
Step: 34440, train/epoch: 8.196097373962402
Step: 34450, train/loss: 0.0
Step: 34450, train/grad_norm: 1.8497827625196805e-07
Step: 34450, train/learning_rate: 9.007615517475642e-06
Step: 34450, train/epoch: 8.198476791381836
Step: 34460, train/loss: 0.0
Step: 34460, train/grad_norm: 2.1903149516333542e-08
Step: 34460, train/learning_rate: 8.995716598292347e-06
Step: 34460, train/epoch: 8.200857162475586
Step: 34470, train/loss: 0.0
Step: 34470, train/grad_norm: 4.6773457995641365e-08
Step: 34470, train/learning_rate: 8.983817679109052e-06
Step: 34470, train/epoch: 8.20323657989502
Step: 34480, train/loss: 0.0
Step: 34480, train/grad_norm: 3.4347074073792783e-09
Step: 34480, train/learning_rate: 8.971917850431055e-06
Step: 34480, train/epoch: 8.205615997314453
Step: 34490, train/loss: 0.0
Step: 34490, train/grad_norm: 3.1514346687799843e-09
Step: 34490, train/learning_rate: 8.96001893124776e-06
Step: 34490, train/epoch: 8.207996368408203
Step: 34500, train/loss: 0.0
Step: 34500, train/grad_norm: 1.5460457913896164e-10
Step: 34500, train/learning_rate: 8.948120012064464e-06
Step: 34500, train/epoch: 8.210375785827637
Step: 34510, train/loss: 0.0
Step: 34510, train/grad_norm: 1.4857397534484562e-09
Step: 34510, train/learning_rate: 8.93622109288117e-06
Step: 34510, train/epoch: 8.212756156921387
Step: 34520, train/loss: 0.0
Step: 34520, train/grad_norm: 3.27118575282892e-11
Step: 34520, train/learning_rate: 8.924322173697874e-06
Step: 34520, train/epoch: 8.21513557434082
Step: 34530, train/loss: 0.0
Step: 34530, train/grad_norm: 5.913356631026545e-07
Step: 34530, train/learning_rate: 8.912422345019877e-06
Step: 34530, train/epoch: 8.21751594543457
Step: 34540, train/loss: 0.0
Step: 34540, train/grad_norm: 5.170821282263205e-07
Step: 34540, train/learning_rate: 8.900523425836582e-06
Step: 34540, train/epoch: 8.219895362854004
Step: 34550, train/loss: 0.0
Step: 34550, train/grad_norm: 1.4663893710409326e-10
Step: 34550, train/learning_rate: 8.888624506653287e-06
Step: 34550, train/epoch: 8.222274780273438
Step: 34560, train/loss: 0.0
Step: 34560, train/grad_norm: 1.521677922555753e-10
Step: 34560, train/learning_rate: 8.876725587469991e-06
Step: 34560, train/epoch: 8.224655151367188
Step: 34570, train/loss: 0.0
Step: 34570, train/grad_norm: 4.4857330294689746e-07
Step: 34570, train/learning_rate: 8.864826668286696e-06
Step: 34570, train/epoch: 8.227034568786621
Step: 34580, train/loss: 0.0
Step: 34580, train/grad_norm: 9.228909766534343e-05
Step: 34580, train/learning_rate: 8.852926839608699e-06
Step: 34580, train/epoch: 8.229414939880371
Step: 34590, train/loss: 0.0
Step: 34590, train/grad_norm: 7.51548268063118e-10
Step: 34590, train/learning_rate: 8.841027920425404e-06
Step: 34590, train/epoch: 8.231794357299805
Step: 34600, train/loss: 0.0
Step: 34600, train/grad_norm: 2.2546437321580015e-05
Step: 34600, train/learning_rate: 8.829129001242109e-06
Step: 34600, train/epoch: 8.234173774719238
Step: 34610, train/loss: 0.0
Step: 34610, train/grad_norm: 5.253146468930936e-07
Step: 34610, train/learning_rate: 8.817230082058813e-06
Step: 34610, train/epoch: 8.236554145812988
Step: 34620, train/loss: 0.0
Step: 34620, train/grad_norm: 5.363955390969011e-10
Step: 34620, train/learning_rate: 8.805331162875518e-06
Step: 34620, train/epoch: 8.238933563232422
Step: 34630, train/loss: 0.0
Step: 34630, train/grad_norm: 1.3189940251123744e-09
Step: 34630, train/learning_rate: 8.793431334197521e-06
Step: 34630, train/epoch: 8.241313934326172
Step: 34640, train/loss: 0.0
Step: 34640, train/grad_norm: 1.029944352026746e-09
Step: 34640, train/learning_rate: 8.781532415014226e-06
Step: 34640, train/epoch: 8.243693351745605
Step: 34650, train/loss: 0.0
Step: 34650, train/grad_norm: 1.812928029920613e-09
Step: 34650, train/learning_rate: 8.76963349583093e-06
Step: 34650, train/epoch: 8.246073722839355
Step: 34660, train/loss: 0.0
Step: 34660, train/grad_norm: 8.185389255288555e-09
Step: 34660, train/learning_rate: 8.757734576647636e-06
Step: 34660, train/epoch: 8.248453140258789
Step: 34670, train/loss: 0.0
Step: 34670, train/grad_norm: 4.228796868233076e-09
Step: 34670, train/learning_rate: 8.74583565746434e-06
Step: 34670, train/epoch: 8.250832557678223
Step: 34680, train/loss: 0.0
Step: 34680, train/grad_norm: 5.447501098387875e-05
Step: 34680, train/learning_rate: 8.733935828786343e-06
Step: 34680, train/epoch: 8.253212928771973
Step: 34690, train/loss: 0.0
Step: 34690, train/grad_norm: 5.923592993894999e-07
Step: 34690, train/learning_rate: 8.722036909603048e-06
Step: 34690, train/epoch: 8.255592346191406
Step: 34700, train/loss: 0.0
Step: 34700, train/grad_norm: 1.2098793078507697e-09
Step: 34700, train/learning_rate: 8.710137990419753e-06
Step: 34700, train/epoch: 8.257972717285156
Step: 34710, train/loss: 0.0
Step: 34710, train/grad_norm: 2.3320252395819807e-09
Step: 34710, train/learning_rate: 8.698239071236458e-06
Step: 34710, train/epoch: 8.26035213470459
Step: 34720, train/loss: 0.0
Step: 34720, train/grad_norm: 1.4601793107971162e-09
Step: 34720, train/learning_rate: 8.686340152053162e-06
Step: 34720, train/epoch: 8.26273250579834
Step: 34730, train/loss: 0.0
Step: 34730, train/grad_norm: 5.256140411802335e-06
Step: 34730, train/learning_rate: 8.674440323375165e-06
Step: 34730, train/epoch: 8.265111923217773
Step: 34740, train/loss: 0.0
Step: 34740, train/grad_norm: 2.4826103981467895e-05
Step: 34740, train/learning_rate: 8.66254140419187e-06
Step: 34740, train/epoch: 8.267491340637207
Step: 34750, train/loss: 0.0
Step: 34750, train/grad_norm: 4.758797800263892e-10
Step: 34750, train/learning_rate: 8.650642485008575e-06
Step: 34750, train/epoch: 8.269871711730957
Step: 34760, train/loss: 0.0
Step: 34760, train/grad_norm: 1.4078820775864642e-08
Step: 34760, train/learning_rate: 8.63874356582528e-06
Step: 34760, train/epoch: 8.27225112915039
Step: 34770, train/loss: 0.0
Step: 34770, train/grad_norm: 1.4691013916490192e-07
Step: 34770, train/learning_rate: 8.626844646641985e-06
Step: 34770, train/epoch: 8.27463150024414
Step: 34780, train/loss: 0.0
Step: 34780, train/grad_norm: 5.652763812591388e-10
Step: 34780, train/learning_rate: 8.614944817963988e-06
Step: 34780, train/epoch: 8.277010917663574
Step: 34790, train/loss: 0.0
Step: 34790, train/grad_norm: 2.407345989219323e-10
Step: 34790, train/learning_rate: 8.603045898780692e-06
Step: 34790, train/epoch: 8.279390335083008
Step: 34800, train/loss: 0.0
Step: 34800, train/grad_norm: 2.5289878635703644e-07
Step: 34800, train/learning_rate: 8.591146979597397e-06
Step: 34800, train/epoch: 8.281770706176758
Step: 34810, train/loss: 0.0
Step: 34810, train/grad_norm: 5.996901381877251e-05
Step: 34810, train/learning_rate: 8.579248060414102e-06
Step: 34810, train/epoch: 8.284150123596191
Step: 34820, train/loss: 0.0
Step: 34820, train/grad_norm: 8.852355654065391e-11
Step: 34820, train/learning_rate: 8.567349141230807e-06
Step: 34820, train/epoch: 8.286530494689941
Step: 34830, train/loss: 0.0
Step: 34830, train/grad_norm: 1.2666861266552587e-06
Step: 34830, train/learning_rate: 8.555450222047511e-06
Step: 34830, train/epoch: 8.288909912109375
Step: 34840, train/loss: 0.0
Step: 34840, train/grad_norm: 1.7652579398230728e-09
Step: 34840, train/learning_rate: 8.543550393369514e-06
Step: 34840, train/epoch: 8.291290283203125
Step: 34850, train/loss: 0.0
Step: 34850, train/grad_norm: 7.809273783188075e-10
Step: 34850, train/learning_rate: 8.53165147418622e-06
Step: 34850, train/epoch: 8.293669700622559
Step: 34860, train/loss: 0.0
Step: 34860, train/grad_norm: 0.0005100144189782441
Step: 34860, train/learning_rate: 8.519752555002924e-06
Step: 34860, train/epoch: 8.296049118041992
Step: 34870, train/loss: 0.0
Step: 34870, train/grad_norm: 6.200767121988804e-10
Step: 34870, train/learning_rate: 8.507853635819629e-06
Step: 34870, train/epoch: 8.298429489135742
Step: 34880, train/loss: 0.0
Step: 34880, train/grad_norm: 9.784002941159997e-07
Step: 34880, train/learning_rate: 8.495954716636334e-06
Step: 34880, train/epoch: 8.300808906555176
Step: 34890, train/loss: 0.0
Step: 34890, train/grad_norm: 6.0378213539991066e-09
Step: 34890, train/learning_rate: 8.484054887958337e-06
Step: 34890, train/epoch: 8.303189277648926
Step: 34900, train/loss: 0.0
Step: 34900, train/grad_norm: 7.089579980856797e-07
Step: 34900, train/learning_rate: 8.472155968775041e-06
Step: 34900, train/epoch: 8.30556869506836
Step: 34910, train/loss: 0.0
Step: 34910, train/grad_norm: 3.221614919279858e-10
Step: 34910, train/learning_rate: 8.460257049591746e-06
Step: 34910, train/epoch: 8.30794906616211
Step: 34920, train/loss: 0.0
Step: 34920, train/grad_norm: 2.199966911575757e-05
Step: 34920, train/learning_rate: 8.448358130408451e-06
Step: 34920, train/epoch: 8.310328483581543
Step: 34930, train/loss: 0.0
Step: 34930, train/grad_norm: 1.7954567965716706e-07
Step: 34930, train/learning_rate: 8.436459211225156e-06
Step: 34930, train/epoch: 8.312707901000977
Step: 34940, train/loss: 0.0
Step: 34940, train/grad_norm: 1.7820119824207836e-09
Step: 34940, train/learning_rate: 8.424559382547159e-06
Step: 34940, train/epoch: 8.315088272094727
Step: 34950, train/loss: 0.0
Step: 34950, train/grad_norm: 6.929969345037534e-07
Step: 34950, train/learning_rate: 8.412660463363864e-06
Step: 34950, train/epoch: 8.31746768951416
Step: 34960, train/loss: 0.0
Step: 34960, train/grad_norm: 1.5490514426730329e-09
Step: 34960, train/learning_rate: 8.400761544180568e-06
Step: 34960, train/epoch: 8.31984806060791
Step: 34970, train/loss: 0.0
Step: 34970, train/grad_norm: 4.08421785280666e-09
Step: 34970, train/learning_rate: 8.388862624997273e-06
Step: 34970, train/epoch: 8.322227478027344
Step: 34980, train/loss: 0.0
Step: 34980, train/grad_norm: 3.0817219887069314e-09
Step: 34980, train/learning_rate: 8.376963705813978e-06
Step: 34980, train/epoch: 8.324606895446777
Step: 34990, train/loss: 0.0
Step: 34990, train/grad_norm: 1.3017776190338282e-08
Step: 34990, train/learning_rate: 8.365063877135981e-06
Step: 34990, train/epoch: 8.326987266540527
Step: 35000, train/loss: 0.0
Step: 35000, train/grad_norm: 2.4410112819950314e-10
Step: 35000, train/learning_rate: 8.353164957952686e-06
Step: 35000, train/epoch: 8.329366683959961
Step: 35010, train/loss: 0.0
Step: 35010, train/grad_norm: 3.436108952925565e-09
Step: 35010, train/learning_rate: 8.34126603876939e-06
Step: 35010, train/epoch: 8.331747055053711
Step: 35020, train/loss: 0.0
Step: 35020, train/grad_norm: 5.95230676481151e-06
Step: 35020, train/learning_rate: 8.329367119586095e-06
Step: 35020, train/epoch: 8.334126472473145
Step: 35030, train/loss: 0.0
Step: 35030, train/grad_norm: 5.880984872419504e-07
Step: 35030, train/learning_rate: 8.3174682004028e-06
Step: 35030, train/epoch: 8.336506843566895
Step: 35040, train/loss: 0.0
Step: 35040, train/grad_norm: 5.679465786556648e-10
Step: 35040, train/learning_rate: 8.305568371724803e-06
Step: 35040, train/epoch: 8.338886260986328
Step: 35050, train/loss: 0.0
Step: 35050, train/grad_norm: 1.026652207691825e-09
Step: 35050, train/learning_rate: 8.293669452541508e-06
Step: 35050, train/epoch: 8.341265678405762
Step: 35060, train/loss: 0.0
Step: 35060, train/grad_norm: 2.225343798123447e-10
Step: 35060, train/learning_rate: 8.281770533358213e-06
Step: 35060, train/epoch: 8.343646049499512
Step: 35070, train/loss: 0.0
Step: 35070, train/grad_norm: 1.7373376071105895e-09
Step: 35070, train/learning_rate: 8.269871614174917e-06
Step: 35070, train/epoch: 8.346025466918945
Step: 35080, train/loss: 0.0
Step: 35080, train/grad_norm: 1.1783046829805244e-05
Step: 35080, train/learning_rate: 8.257972694991622e-06
Step: 35080, train/epoch: 8.348405838012695
Step: 35090, train/loss: 0.0
Step: 35090, train/grad_norm: 4.745256965144051e-10
Step: 35090, train/learning_rate: 8.246072866313625e-06
Step: 35090, train/epoch: 8.350785255432129
Step: 35100, train/loss: 0.0
Step: 35100, train/grad_norm: 6.748437919590344e-10
Step: 35100, train/learning_rate: 8.23417394713033e-06
Step: 35100, train/epoch: 8.353165626525879
Step: 35110, train/loss: 0.0
Step: 35110, train/grad_norm: 7.408844648892909e-10
Step: 35110, train/learning_rate: 8.222275027947035e-06
Step: 35110, train/epoch: 8.355545043945312
Step: 35120, train/loss: 0.0
Step: 35120, train/grad_norm: 4.312635581982249e-09
Step: 35120, train/learning_rate: 8.21037610876374e-06
Step: 35120, train/epoch: 8.357924461364746
Step: 35130, train/loss: 0.0
Step: 35130, train/grad_norm: 2.0753034846165974e-08
Step: 35130, train/learning_rate: 8.198477189580444e-06
Step: 35130, train/epoch: 8.360304832458496
Step: 35140, train/loss: 0.0
Step: 35140, train/grad_norm: 2.2410828748320455e-09
Step: 35140, train/learning_rate: 8.186578270397149e-06
Step: 35140, train/epoch: 8.36268424987793
Step: 35150, train/loss: 0.0
Step: 35150, train/grad_norm: 7.582219296864423e-08
Step: 35150, train/learning_rate: 8.174678441719152e-06
Step: 35150, train/epoch: 8.36506462097168
Step: 35160, train/loss: 0.0
Step: 35160, train/grad_norm: 1.2785049685604122e-09
Step: 35160, train/learning_rate: 8.162779522535857e-06
Step: 35160, train/epoch: 8.367444038391113
Step: 35170, train/loss: 0.0
Step: 35170, train/grad_norm: 3.765568479252579e-08
Step: 35170, train/learning_rate: 8.150880603352562e-06
Step: 35170, train/epoch: 8.369823455810547
Step: 35180, train/loss: 0.0
Step: 35180, train/grad_norm: 1.9503709758339483e-10
Step: 35180, train/learning_rate: 8.138981684169266e-06
Step: 35180, train/epoch: 8.372203826904297
Step: 35190, train/loss: 0.0
Step: 35190, train/grad_norm: 1.5882235804287603e-10
Step: 35190, train/learning_rate: 8.127082764985971e-06
Step: 35190, train/epoch: 8.37458324432373
Step: 35200, train/loss: 0.0
Step: 35200, train/grad_norm: 1.0468558002685313e-06
Step: 35200, train/learning_rate: 8.115182936307974e-06
Step: 35200, train/epoch: 8.37696361541748
Step: 35210, train/loss: 0.0
Step: 35210, train/grad_norm: 4.102237127767694e-08
Step: 35210, train/learning_rate: 8.103284017124679e-06
Step: 35210, train/epoch: 8.379343032836914
Step: 35220, train/loss: 0.0
Step: 35220, train/grad_norm: 1.9440908882728536e-09
Step: 35220, train/learning_rate: 8.091385097941384e-06
Step: 35220, train/epoch: 8.381723403930664
Step: 35230, train/loss: 0.0
Step: 35230, train/grad_norm: 9.89973977993941e-06
Step: 35230, train/learning_rate: 8.079486178758088e-06
Step: 35230, train/epoch: 8.384102821350098
Step: 35240, train/loss: 0.0
Step: 35240, train/grad_norm: 9.496311577095184e-06
Step: 35240, train/learning_rate: 8.067587259574793e-06
Step: 35240, train/epoch: 8.386482238769531
Step: 35250, train/loss: 0.0
Step: 35250, train/grad_norm: 5.479868114832698e-09
Step: 35250, train/learning_rate: 8.055687430896796e-06
Step: 35250, train/epoch: 8.388862609863281
Step: 35260, train/loss: 0.0
Step: 35260, train/grad_norm: 1.9006955731537545e-10
Step: 35260, train/learning_rate: 8.043788511713501e-06
Step: 35260, train/epoch: 8.391242027282715
Step: 35270, train/loss: 0.0
Step: 35270, train/grad_norm: 3.955036742553375e-09
Step: 35270, train/learning_rate: 8.031889592530206e-06
Step: 35270, train/epoch: 8.393622398376465
Step: 35280, train/loss: 0.0
Step: 35280, train/grad_norm: 2.5011245852191166e-10
Step: 35280, train/learning_rate: 8.01999067334691e-06
Step: 35280, train/epoch: 8.396001815795898
Step: 35290, train/loss: 0.0
Step: 35290, train/grad_norm: 1.753312384167316e-09
Step: 35290, train/learning_rate: 8.008091754163615e-06
Step: 35290, train/epoch: 8.398382186889648
Step: 35300, train/loss: 0.0
Step: 35300, train/grad_norm: 4.350150106802175e-08
Step: 35300, train/learning_rate: 7.996191925485618e-06
Step: 35300, train/epoch: 8.400761604309082
Step: 35310, train/loss: 0.0
Step: 35310, train/grad_norm: 5.097739452786243e-10
Step: 35310, train/learning_rate: 7.984293006302323e-06
Step: 35310, train/epoch: 8.403141021728516
Step: 35320, train/loss: 0.0
Step: 35320, train/grad_norm: 1.4505103784756557e-08
Step: 35320, train/learning_rate: 7.972394087119028e-06
Step: 35320, train/epoch: 8.405521392822266
Step: 35330, train/loss: 0.0
Step: 35330, train/grad_norm: 3.673610582399789e-10
Step: 35330, train/learning_rate: 7.960495167935733e-06
Step: 35330, train/epoch: 8.4079008102417
Step: 35340, train/loss: 0.0
Step: 35340, train/grad_norm: 1.665073412482343e-09
Step: 35340, train/learning_rate: 7.948596248752438e-06
Step: 35340, train/epoch: 8.41028118133545
Step: 35350, train/loss: 0.0
Step: 35350, train/grad_norm: 6.567449599970132e-05
Step: 35350, train/learning_rate: 7.93669642007444e-06
Step: 35350, train/epoch: 8.412660598754883
Step: 35360, train/loss: 0.0
Step: 35360, train/grad_norm: 3.7862066259997107e-10
Step: 35360, train/learning_rate: 7.924797500891145e-06
Step: 35360, train/epoch: 8.415040016174316
Step: 35370, train/loss: 0.0
Step: 35370, train/grad_norm: 1.1277215272720298e-10
Step: 35370, train/learning_rate: 7.91289858170785e-06
Step: 35370, train/epoch: 8.417420387268066
Step: 35380, train/loss: 0.0
Step: 35380, train/grad_norm: 6.343545244646975e-09
Step: 35380, train/learning_rate: 7.900999662524555e-06
Step: 35380, train/epoch: 8.4197998046875
Step: 35390, train/loss: 0.0
Step: 35390, train/grad_norm: 1.3790690900350455e-07
Step: 35390, train/learning_rate: 7.88910074334126e-06
Step: 35390, train/epoch: 8.42218017578125
Step: 35400, train/loss: 0.0
Step: 35400, train/grad_norm: 1.788693992921253e-07
Step: 35400, train/learning_rate: 7.877200914663263e-06
Step: 35400, train/epoch: 8.424559593200684
Step: 35410, train/loss: 0.0
Step: 35410, train/grad_norm: 1.5308708611883048e-07
Step: 35410, train/learning_rate: 7.865301995479967e-06
Step: 35410, train/epoch: 8.426939964294434
Step: 35420, train/loss: 0.0
Step: 35420, train/grad_norm: 1.0593328880759145e-08
Step: 35420, train/learning_rate: 7.853403076296672e-06
Step: 35420, train/epoch: 8.429319381713867
Step: 35430, train/loss: 0.0
Step: 35430, train/grad_norm: 1.080758926796932e-09
Step: 35430, train/learning_rate: 7.841504157113377e-06
Step: 35430, train/epoch: 8.4316987991333
Step: 35440, train/loss: 0.0
Step: 35440, train/grad_norm: 3.1776673381500586e-07
Step: 35440, train/learning_rate: 7.829605237930082e-06
Step: 35440, train/epoch: 8.43407917022705
Step: 35450, train/loss: 0.0
Step: 35450, train/grad_norm: 6.704315325123389e-08
Step: 35450, train/learning_rate: 7.817705409252085e-06
Step: 35450, train/epoch: 8.436458587646484
Step: 35460, train/loss: 0.0
Step: 35460, train/grad_norm: 4.733633929276948e-09
Step: 35460, train/learning_rate: 7.80580649006879e-06
Step: 35460, train/epoch: 8.438838958740234
Step: 35470, train/loss: 0.0
Step: 35470, train/grad_norm: 5.189717811049377e-08
Step: 35470, train/learning_rate: 7.793907570885494e-06
Step: 35470, train/epoch: 8.441218376159668
Step: 35480, train/loss: 0.0
Step: 35480, train/grad_norm: 6.516632233832809e-10
Step: 35480, train/learning_rate: 7.782008651702199e-06
Step: 35480, train/epoch: 8.443598747253418
Step: 35490, train/loss: 0.0
Step: 35490, train/grad_norm: 9.312284832141415e-10
Step: 35490, train/learning_rate: 7.770109732518904e-06
Step: 35490, train/epoch: 8.445978164672852
Step: 35500, train/loss: 0.0
Step: 35500, train/grad_norm: 2.287186049443335e-07
Step: 35500, train/learning_rate: 7.758210813335609e-06
Step: 35500, train/epoch: 8.448357582092285
Step: 35510, train/loss: 0.0
Step: 35510, train/grad_norm: 7.348448072264091e-09
Step: 35510, train/learning_rate: 7.746310984657612e-06
Step: 35510, train/epoch: 8.450737953186035
Step: 35520, train/loss: 0.0
Step: 35520, train/grad_norm: 7.264569745757399e-08
Step: 35520, train/learning_rate: 7.734412065474316e-06
Step: 35520, train/epoch: 8.453117370605469
Step: 35530, train/loss: 0.0
Step: 35530, train/grad_norm: 2.410958932497209e-10
Step: 35530, train/learning_rate: 7.722513146291021e-06
Step: 35530, train/epoch: 8.455497741699219
Step: 35540, train/loss: 0.0
Step: 35540, train/grad_norm: 2.109140240236229e-08
Step: 35540, train/learning_rate: 7.710614227107726e-06
Step: 35540, train/epoch: 8.457877159118652
Step: 35550, train/loss: 0.0
Step: 35550, train/grad_norm: 2.788691499944207e-10
Step: 35550, train/learning_rate: 7.69871530792443e-06
Step: 35550, train/epoch: 8.460256576538086
Step: 35560, train/loss: 0.0
Step: 35560, train/grad_norm: 5.345145159818543e-11
Step: 35560, train/learning_rate: 7.686815479246434e-06
Step: 35560, train/epoch: 8.462636947631836
Step: 35570, train/loss: 0.0
Step: 35570, train/grad_norm: 5.621244781650603e-06
Step: 35570, train/learning_rate: 7.674916560063139e-06
Step: 35570, train/epoch: 8.46501636505127
Step: 35580, train/loss: 0.0
Step: 35580, train/grad_norm: 3.133875679850462e-06
Step: 35580, train/learning_rate: 7.663017640879843e-06
Step: 35580, train/epoch: 8.46739673614502
Step: 35590, train/loss: 0.0
Step: 35590, train/grad_norm: 2.0167051388852997e-06
Step: 35590, train/learning_rate: 7.651118721696548e-06
Step: 35590, train/epoch: 8.469776153564453
Step: 35600, train/loss: 0.0
Step: 35600, train/grad_norm: 2.9305807780488635e-11
Step: 35600, train/learning_rate: 7.639219802513253e-06
Step: 35600, train/epoch: 8.472156524658203
Step: 35610, train/loss: 0.0
Step: 35610, train/grad_norm: 9.644375298378804e-10
Step: 35610, train/learning_rate: 7.627320428582607e-06
Step: 35610, train/epoch: 8.474535942077637
Step: 35620, train/loss: 0.0
Step: 35620, train/grad_norm: 3.837993922672922e-09
Step: 35620, train/learning_rate: 7.615421054651961e-06
Step: 35620, train/epoch: 8.47691535949707
Step: 35630, train/loss: 0.0
Step: 35630, train/grad_norm: 7.740087043517363e-11
Step: 35630, train/learning_rate: 7.6035221354686655e-06
Step: 35630, train/epoch: 8.47929573059082
Step: 35640, train/loss: 0.0
Step: 35640, train/grad_norm: 6.315875822338057e-08
Step: 35640, train/learning_rate: 7.59162321628537e-06
Step: 35640, train/epoch: 8.481675148010254
Step: 35650, train/loss: 0.0
Step: 35650, train/grad_norm: 1.6644982059332847e-09
Step: 35650, train/learning_rate: 7.579723842354724e-06
Step: 35650, train/epoch: 8.484055519104004
Step: 35660, train/loss: 0.0
Step: 35660, train/grad_norm: 3.596209441347753e-10
Step: 35660, train/learning_rate: 7.567824923171429e-06
Step: 35660, train/epoch: 8.486434936523438
Step: 35670, train/loss: 0.0
Step: 35670, train/grad_norm: 3.936859684472438e-06
Step: 35670, train/learning_rate: 7.555925549240783e-06
Step: 35670, train/epoch: 8.488815307617188
Step: 35680, train/loss: 0.0
Step: 35680, train/grad_norm: 6.02403182892175e-10
Step: 35680, train/learning_rate: 7.544026630057488e-06
Step: 35680, train/epoch: 8.491194725036621
Step: 35690, train/loss: 0.0
Step: 35690, train/grad_norm: 9.683760436018929e-05
Step: 35690, train/learning_rate: 7.532127710874192e-06
Step: 35690, train/epoch: 8.493574142456055
Step: 35700, train/loss: 0.0
Step: 35700, train/grad_norm: 2.630364178912714e-05
Step: 35700, train/learning_rate: 7.520228336943546e-06
Step: 35700, train/epoch: 8.495954513549805
Step: 35710, train/loss: 0.0
Step: 35710, train/grad_norm: 4.205144796287641e-05
Step: 35710, train/learning_rate: 7.508329417760251e-06
Step: 35710, train/epoch: 8.498333930969238
Step: 35720, train/loss: 0.0
Step: 35720, train/grad_norm: 1.1430130175682507e-09
Step: 35720, train/learning_rate: 7.496430498576956e-06
Step: 35720, train/epoch: 8.500714302062988
Step: 35730, train/loss: 0.0
Step: 35730, train/grad_norm: 2.772399625428079e-07
Step: 35730, train/learning_rate: 7.48453112464631e-06
Step: 35730, train/epoch: 8.503093719482422
Step: 35740, train/loss: 0.0
Step: 35740, train/grad_norm: 1.4148281479720026e-06
Step: 35740, train/learning_rate: 7.4726322054630145e-06
Step: 35740, train/epoch: 8.505473136901855
Step: 35750, train/loss: 0.0
Step: 35750, train/grad_norm: 1.8882617780491273e-07
Step: 35750, train/learning_rate: 7.4607328315323684e-06
Step: 35750, train/epoch: 8.507853507995605
Step: 35760, train/loss: 0.0
Step: 35760, train/grad_norm: 1.9568975773154307e-08
Step: 35760, train/learning_rate: 7.448833912349073e-06
Step: 35760, train/epoch: 8.510232925415039
Step: 35770, train/loss: 0.0
Step: 35770, train/grad_norm: 4.615596935764188e-07
Step: 35770, train/learning_rate: 7.436934993165778e-06
Step: 35770, train/epoch: 8.512613296508789
Step: 35780, train/loss: 0.0
Step: 35780, train/grad_norm: 4.028458420179959e-07
Step: 35780, train/learning_rate: 7.425035619235132e-06
Step: 35780, train/epoch: 8.514992713928223
Step: 35790, train/loss: 0.0
Step: 35790, train/grad_norm: 2.1857566867566902e-09
Step: 35790, train/learning_rate: 7.413136700051837e-06
Step: 35790, train/epoch: 8.517373085021973
Step: 35800, train/loss: 0.0
Step: 35800, train/grad_norm: 2.1005350792080435e-09
Step: 35800, train/learning_rate: 7.4012373261211906e-06
Step: 35800, train/epoch: 8.519752502441406
Step: 35810, train/loss: 0.0
Step: 35810, train/grad_norm: 3.475925380858058e-10
Step: 35810, train/learning_rate: 7.389338406937895e-06
Step: 35810, train/epoch: 8.52213191986084
Step: 35820, train/loss: 0.0
Step: 35820, train/grad_norm: 5.460958352188072e-09
Step: 35820, train/learning_rate: 7.3774394877546e-06
Step: 35820, train/epoch: 8.52451229095459
Step: 35830, train/loss: 0.0
Step: 35830, train/grad_norm: 1.710925623399362e-09
Step: 35830, train/learning_rate: 7.365540113823954e-06
Step: 35830, train/epoch: 8.526891708374023
Step: 35840, train/loss: 0.0
Step: 35840, train/grad_norm: 2.908095098064223e-07
Step: 35840, train/learning_rate: 7.353641194640659e-06
Step: 35840, train/epoch: 8.529272079467773
Step: 35850, train/loss: 0.0
Step: 35850, train/grad_norm: 2.654196307183554e-10
Step: 35850, train/learning_rate: 7.341741820710013e-06
Step: 35850, train/epoch: 8.531651496887207
Step: 35860, train/loss: 0.0
Step: 35860, train/grad_norm: 5.035059175151346e-12
Step: 35860, train/learning_rate: 7.3298429015267175e-06
Step: 35860, train/epoch: 8.534031867980957
Step: 35870, train/loss: 0.0
Step: 35870, train/grad_norm: 9.565059144733823e-07
Step: 35870, train/learning_rate: 7.317943982343422e-06
Step: 35870, train/epoch: 8.53641128540039
Step: 35880, train/loss: 0.0
Step: 35880, train/grad_norm: 1.1530199373055439e-07
Step: 35880, train/learning_rate: 7.306044608412776e-06
Step: 35880, train/epoch: 8.538790702819824
Step: 35890, train/loss: 0.0
Step: 35890, train/grad_norm: 9.552150181946217e-10
Step: 35890, train/learning_rate: 7.294145689229481e-06
Step: 35890, train/epoch: 8.541171073913574
Step: 35900, train/loss: 0.0
Step: 35900, train/grad_norm: 9.478451090672024e-08
Step: 35900, train/learning_rate: 7.282246770046186e-06
Step: 35900, train/epoch: 8.543550491333008
Step: 35910, train/loss: 0.0
Step: 35910, train/grad_norm: 3.869505604825463e-09
Step: 35910, train/learning_rate: 7.27034739611554e-06
Step: 35910, train/epoch: 8.545930862426758
Step: 35920, train/loss: 0.0
Step: 35920, train/grad_norm: 1.1739675898070345e-09
Step: 35920, train/learning_rate: 7.258448476932244e-06
Step: 35920, train/epoch: 8.548310279846191
Step: 35930, train/loss: 0.0
Step: 35930, train/grad_norm: 7.706579263633273e-10
Step: 35930, train/learning_rate: 7.246549103001598e-06
Step: 35930, train/epoch: 8.550689697265625
Step: 35940, train/loss: 0.0
Step: 35940, train/grad_norm: 1.236369140400484e-08
Step: 35940, train/learning_rate: 7.234650183818303e-06
Step: 35940, train/epoch: 8.553070068359375
Step: 35950, train/loss: 0.0
Step: 35950, train/grad_norm: 1.5294978616964272e-09
Step: 35950, train/learning_rate: 7.222751264635008e-06
Step: 35950, train/epoch: 8.555449485778809
Step: 35960, train/loss: 0.0
Step: 35960, train/grad_norm: 4.7842679151699485e-08
Step: 35960, train/learning_rate: 7.210851890704362e-06
Step: 35960, train/epoch: 8.557829856872559
Step: 35970, train/loss: 0.0
Step: 35970, train/grad_norm: 1.994832299345717e-09
Step: 35970, train/learning_rate: 7.1989529715210665e-06
Step: 35970, train/epoch: 8.560209274291992
Step: 35980, train/loss: 0.0
Step: 35980, train/grad_norm: 1.096018498181195e-09
Step: 35980, train/learning_rate: 7.18705359759042e-06
Step: 35980, train/epoch: 8.562589645385742
Step: 35990, train/loss: 0.0
Step: 35990, train/grad_norm: 1.6440603545220256e-08
Step: 35990, train/learning_rate: 7.175154678407125e-06
Step: 35990, train/epoch: 8.564969062805176
Step: 36000, train/loss: 0.0
Step: 36000, train/grad_norm: 2.2198516896193565e-12
Step: 36000, train/learning_rate: 7.16325575922383e-06
Step: 36000, train/epoch: 8.56734848022461
Step: 36010, train/loss: 0.0
Step: 36010, train/grad_norm: 3.455826345089008e-07
Step: 36010, train/learning_rate: 7.151356385293184e-06
Step: 36010, train/epoch: 8.56972885131836
Step: 36020, train/loss: 0.0
Step: 36020, train/grad_norm: 8.241348048443342e-09
Step: 36020, train/learning_rate: 7.139457466109889e-06
Step: 36020, train/epoch: 8.572108268737793
Step: 36030, train/loss: 0.0
Step: 36030, train/grad_norm: 1.206989785895729e-10
Step: 36030, train/learning_rate: 7.1275580921792425e-06
Step: 36030, train/epoch: 8.574488639831543
Step: 36040, train/loss: 0.0
Step: 36040, train/grad_norm: 5.827336391206472e-09
Step: 36040, train/learning_rate: 7.115659172995947e-06
Step: 36040, train/epoch: 8.576868057250977
Step: 36050, train/loss: 0.0
Step: 36050, train/grad_norm: 1.0126112783837016e-06
Step: 36050, train/learning_rate: 7.103760253812652e-06
Step: 36050, train/epoch: 8.579248428344727
Step: 36060, train/loss: 0.0
Step: 36060, train/grad_norm: 7.68126753314391e-08
Step: 36060, train/learning_rate: 7.091860879882006e-06
Step: 36060, train/epoch: 8.58162784576416
Step: 36070, train/loss: 0.0
Step: 36070, train/grad_norm: 1.3604629423014103e-08
Step: 36070, train/learning_rate: 7.079961960698711e-06
Step: 36070, train/epoch: 8.584007263183594
Step: 36080, train/loss: 0.0
Step: 36080, train/grad_norm: 2.8405132412867573e-11
Step: 36080, train/learning_rate: 7.0680630415154155e-06
Step: 36080, train/epoch: 8.586387634277344
Step: 36090, train/loss: 0.0
Step: 36090, train/grad_norm: 4.64557956547651e-08
Step: 36090, train/learning_rate: 7.0561636675847694e-06
Step: 36090, train/epoch: 8.588767051696777
Step: 36100, train/loss: 0.0
Step: 36100, train/grad_norm: 5.5542049182122355e-08
Step: 36100, train/learning_rate: 7.044264748401474e-06
Step: 36100, train/epoch: 8.591147422790527
Step: 36110, train/loss: 0.0
Step: 36110, train/grad_norm: 2.9299917514435947e-07
Step: 36110, train/learning_rate: 7.032365374470828e-06
Step: 36110, train/epoch: 8.593526840209961
Step: 36120, train/loss: 0.0
Step: 36120, train/grad_norm: 6.155440814836766e-07
Step: 36120, train/learning_rate: 7.020466455287533e-06
Step: 36120, train/epoch: 8.595906257629395
Step: 36130, train/loss: 0.0
Step: 36130, train/grad_norm: 1.7059006495401263e-05
Step: 36130, train/learning_rate: 7.008567536104238e-06
Step: 36130, train/epoch: 8.598286628723145
Step: 36140, train/loss: 0.0
Step: 36140, train/grad_norm: 7.536418537845702e-12
Step: 36140, train/learning_rate: 6.9966681621735916e-06
Step: 36140, train/epoch: 8.600666046142578
Step: 36150, train/loss: 0.0
Step: 36150, train/grad_norm: 7.302990212565419e-09
Step: 36150, train/learning_rate: 6.984769242990296e-06
Step: 36150, train/epoch: 8.603046417236328
Step: 36160, train/loss: 0.0
Step: 36160, train/grad_norm: 5.327766006146817e-11
Step: 36160, train/learning_rate: 6.97286986905965e-06
Step: 36160, train/epoch: 8.605425834655762
Step: 36170, train/loss: 0.0
Step: 36170, train/grad_norm: 1.7705974642012734e-07
Step: 36170, train/learning_rate: 6.960970949876355e-06
Step: 36170, train/epoch: 8.607806205749512
Step: 36180, train/loss: 0.0
Step: 36180, train/grad_norm: 3.8819473502371693e-07
Step: 36180, train/learning_rate: 6.94907203069306e-06
Step: 36180, train/epoch: 8.610185623168945
Step: 36190, train/loss: 0.0
Step: 36190, train/grad_norm: 1.4313140127342194e-05
Step: 36190, train/learning_rate: 6.937172656762414e-06
Step: 36190, train/epoch: 8.612565040588379
Step: 36200, train/loss: 0.0
Step: 36200, train/grad_norm: 1.393944160099636e-07
Step: 36200, train/learning_rate: 6.9252737375791185e-06
Step: 36200, train/epoch: 8.614945411682129
Step: 36210, train/loss: 0.0
Step: 36210, train/grad_norm: 1.5588608182071084e-10
Step: 36210, train/learning_rate: 6.913374363648472e-06
Step: 36210, train/epoch: 8.617324829101562
Step: 36220, train/loss: 0.0
Step: 36220, train/grad_norm: 1.4650920832082193e-07
Step: 36220, train/learning_rate: 6.901475444465177e-06
Step: 36220, train/epoch: 8.619705200195312
Step: 36230, train/loss: 0.0
Step: 36230, train/grad_norm: 2.0825330349083515e-09
Step: 36230, train/learning_rate: 6.889576525281882e-06
Step: 36230, train/epoch: 8.622084617614746
Step: 36240, train/loss: 0.0
Step: 36240, train/grad_norm: 1.0927132088056624e-08
Step: 36240, train/learning_rate: 6.877677151351236e-06
Step: 36240, train/epoch: 8.624464988708496
Step: 36250, train/loss: 0.0
Step: 36250, train/grad_norm: 3.983382512728895e-09
Step: 36250, train/learning_rate: 6.865778232167941e-06
Step: 36250, train/epoch: 8.62684440612793
Step: 36260, train/loss: 0.0
Step: 36260, train/grad_norm: 6.732186363933579e-09
Step: 36260, train/learning_rate: 6.853879312984645e-06
Step: 36260, train/epoch: 8.629223823547363
Step: 36270, train/loss: 0.0
Step: 36270, train/grad_norm: 5.254860298009589e-06
Step: 36270, train/learning_rate: 6.841979939053999e-06
Step: 36270, train/epoch: 8.631604194641113
Step: 36280, train/loss: 0.0
Step: 36280, train/grad_norm: 1.2115543235324822e-08
Step: 36280, train/learning_rate: 6.830081019870704e-06
Step: 36280, train/epoch: 8.633983612060547
Step: 36290, train/loss: 0.0
Step: 36290, train/grad_norm: 2.817548638844869e-09
Step: 36290, train/learning_rate: 6.818181645940058e-06
Step: 36290, train/epoch: 8.636363983154297
Step: 36300, train/loss: 0.0
Step: 36300, train/grad_norm: 5.356151788760144e-08
Step: 36300, train/learning_rate: 6.806282726756763e-06
Step: 36300, train/epoch: 8.63874340057373
Step: 36310, train/loss: 0.0
Step: 36310, train/grad_norm: 1.646422928547331e-11
Step: 36310, train/learning_rate: 6.7943838075734675e-06
Step: 36310, train/epoch: 8.641122817993164
Step: 36320, train/loss: 0.0
Step: 36320, train/grad_norm: 1.0906659575482536e-08
Step: 36320, train/learning_rate: 6.782484433642821e-06
Step: 36320, train/epoch: 8.643503189086914
Step: 36330, train/loss: 0.0
Step: 36330, train/grad_norm: 5.494758283930423e-07
Step: 36330, train/learning_rate: 6.770585514459526e-06
Step: 36330, train/epoch: 8.645882606506348
Step: 36340, train/loss: 0.0
Step: 36340, train/grad_norm: 1.6424692161898236e-10
Step: 36340, train/learning_rate: 6.75868614052888e-06
Step: 36340, train/epoch: 8.648262977600098
Step: 36350, train/loss: 0.0
Step: 36350, train/grad_norm: 9.187298838364555e-11
Step: 36350, train/learning_rate: 6.746787221345585e-06
Step: 36350, train/epoch: 8.650642395019531
Step: 36360, train/loss: 0.0
Step: 36360, train/grad_norm: 4.8181010292580595e-09
Step: 36360, train/learning_rate: 6.73488830216229e-06
Step: 36360, train/epoch: 8.653022766113281
Step: 36370, train/loss: 0.0
Step: 36370, train/grad_norm: 2.194051603510161e-06
Step: 36370, train/learning_rate: 6.7229889282316435e-06
Step: 36370, train/epoch: 8.655402183532715
Step: 36380, train/loss: 0.0
Step: 36380, train/grad_norm: 4.670501851933295e-08
Step: 36380, train/learning_rate: 6.711090009048348e-06
Step: 36380, train/epoch: 8.657781600952148
Step: 36390, train/loss: 0.0
Step: 36390, train/grad_norm: 8.605388623905696e-11
Step: 36390, train/learning_rate: 6.699190635117702e-06
Step: 36390, train/epoch: 8.660161972045898
Step: 36400, train/loss: 0.0
Step: 36400, train/grad_norm: 1.7023792375781e-09
Step: 36400, train/learning_rate: 6.687291715934407e-06
Step: 36400, train/epoch: 8.662541389465332
Step: 36410, train/loss: 0.0
Step: 36410, train/grad_norm: 8.24596779747111e-10
Step: 36410, train/learning_rate: 6.675392796751112e-06
Step: 36410, train/epoch: 8.664921760559082
Step: 36420, train/loss: 0.0
Step: 36420, train/grad_norm: 5.647921019757973e-10
Step: 36420, train/learning_rate: 6.663493422820466e-06
Step: 36420, train/epoch: 8.667301177978516
Step: 36430, train/loss: 0.0
Step: 36430, train/grad_norm: 1.4267880033003166e-05
Step: 36430, train/learning_rate: 6.65159450363717e-06
Step: 36430, train/epoch: 8.669681549072266
Step: 36440, train/loss: 0.0
Step: 36440, train/grad_norm: 1.6196185503858374e-09
Step: 36440, train/learning_rate: 6.639695584453875e-06
Step: 36440, train/epoch: 8.6720609664917
Step: 36450, train/loss: 0.0
Step: 36450, train/grad_norm: 1.6796146695696734e-09
Step: 36450, train/learning_rate: 6.627796210523229e-06
Step: 36450, train/epoch: 8.674440383911133
Step: 36460, train/loss: 0.0
Step: 36460, train/grad_norm: 1.329260470583904e-07
Step: 36460, train/learning_rate: 6.615897291339934e-06
Step: 36460, train/epoch: 8.676820755004883
Step: 36470, train/loss: 0.0
Step: 36470, train/grad_norm: 5.08786346387069e-09
Step: 36470, train/learning_rate: 6.603997917409288e-06
Step: 36470, train/epoch: 8.679200172424316
Step: 36480, train/loss: 0.0
Step: 36480, train/grad_norm: 1.4613158683118854e-08
Step: 36480, train/learning_rate: 6.5920989982259925e-06
Step: 36480, train/epoch: 8.681580543518066
Step: 36490, train/loss: 0.0
Step: 36490, train/grad_norm: 1.8059357387301134e-07
Step: 36490, train/learning_rate: 6.580200079042697e-06
Step: 36490, train/epoch: 8.6839599609375
Step: 36500, train/loss: 0.0
Step: 36500, train/grad_norm: 4.906507911073277e-08
Step: 36500, train/learning_rate: 6.568300705112051e-06
Step: 36500, train/epoch: 8.686339378356934
Step: 36510, train/loss: 0.0
Step: 36510, train/grad_norm: 4.2031038901768625e-05
Step: 36510, train/learning_rate: 6.556401785928756e-06
Step: 36510, train/epoch: 8.688719749450684
Step: 36520, train/loss: 0.0
Step: 36520, train/grad_norm: 7.570466209472215e-07
Step: 36520, train/learning_rate: 6.54450241199811e-06
Step: 36520, train/epoch: 8.691099166870117
Step: 36530, train/loss: 0.0
Step: 36530, train/grad_norm: 2.2902371199506888e-07
Step: 36530, train/learning_rate: 6.532603492814815e-06
Step: 36530, train/epoch: 8.693479537963867
Step: 36540, train/loss: 0.0
Step: 36540, train/grad_norm: 1.0880302880877935e-07
Step: 36540, train/learning_rate: 6.5207045736315195e-06
Step: 36540, train/epoch: 8.6958589553833
Step: 36550, train/loss: 0.0
Step: 36550, train/grad_norm: 4.4127482112799044e-08
Step: 36550, train/learning_rate: 6.508805199700873e-06
Step: 36550, train/epoch: 8.69823932647705
Step: 36560, train/loss: 0.0
Step: 36560, train/grad_norm: 6.992822432216883e-10
Step: 36560, train/learning_rate: 6.496906280517578e-06
Step: 36560, train/epoch: 8.700618743896484
Step: 36570, train/loss: 0.0
Step: 36570, train/grad_norm: 8.083320324203669e-08
Step: 36570, train/learning_rate: 6.485007361334283e-06
Step: 36570, train/epoch: 8.702998161315918
Step: 36580, train/loss: 0.0
Step: 36580, train/grad_norm: 3.060656242337423e-11
Step: 36580, train/learning_rate: 6.473107987403637e-06
Step: 36580, train/epoch: 8.705378532409668
Step: 36590, train/loss: 9.999999747378752e-05
Step: 36590, train/grad_norm: 1.2195109366786028e-09
Step: 36590, train/learning_rate: 6.461209068220342e-06
Step: 36590, train/epoch: 8.707757949829102
Step: 36600, train/loss: 0.0
Step: 36600, train/grad_norm: 4.176854417892173e-09
Step: 36600, train/learning_rate: 6.4493096942896955e-06
Step: 36600, train/epoch: 8.710138320922852
Step: 36610, train/loss: 0.0
Step: 36610, train/grad_norm: 1.1863646953713669e-08
Step: 36610, train/learning_rate: 6.4374107751064e-06
Step: 36610, train/epoch: 8.712517738342285
Step: 36620, train/loss: 0.0
Step: 36620, train/grad_norm: 5.814229453449116e-08
Step: 36620, train/learning_rate: 6.425511855923105e-06
Step: 36620, train/epoch: 8.714898109436035
Step: 36630, train/loss: 0.0
Step: 36630, train/grad_norm: 1.7511232741629357e-10
Step: 36630, train/learning_rate: 6.413612481992459e-06
Step: 36630, train/epoch: 8.717277526855469
Step: 36640, train/loss: 0.0
Step: 36640, train/grad_norm: 2.876847426946938e-09
Step: 36640, train/learning_rate: 6.401713562809164e-06
Step: 36640, train/epoch: 8.719656944274902
Step: 36650, train/loss: 9.999999747378752e-05
Step: 36650, train/grad_norm: 1.9225358804941806e-10
Step: 36650, train/learning_rate: 6.389814188878518e-06
Step: 36650, train/epoch: 8.722037315368652
Step: 36660, train/loss: 0.0
Step: 36660, train/grad_norm: 8.627051961695997e-09
Step: 36660, train/learning_rate: 6.377915269695222e-06
Step: 36660, train/epoch: 8.724416732788086
Step: 36670, train/loss: 0.0
Step: 36670, train/grad_norm: 1.1062455396171345e-09
Step: 36670, train/learning_rate: 6.366016350511927e-06
Step: 36670, train/epoch: 8.726797103881836
Step: 36680, train/loss: 0.0
Step: 36680, train/grad_norm: 2.851758586219777e-11
Step: 36680, train/learning_rate: 6.354116976581281e-06
Step: 36680, train/epoch: 8.72917652130127
Step: 36690, train/loss: 0.0
Step: 36690, train/grad_norm: 1.368128943468605e-09
Step: 36690, train/learning_rate: 6.342218057397986e-06
Step: 36690, train/epoch: 8.731555938720703
Step: 36700, train/loss: 0.0
Step: 36700, train/grad_norm: 4.3525452042558754e-07
Step: 36700, train/learning_rate: 6.33031868346734e-06
Step: 36700, train/epoch: 8.733936309814453
Step: 36710, train/loss: 0.0
Step: 36710, train/grad_norm: 2.1188568954944031e-10
Step: 36710, train/learning_rate: 6.3184197642840445e-06
Step: 36710, train/epoch: 8.736315727233887
Step: 36720, train/loss: 0.0
Step: 36720, train/grad_norm: 3.180005148095688e-10
Step: 36720, train/learning_rate: 6.306520845100749e-06
Step: 36720, train/epoch: 8.738696098327637
Step: 36730, train/loss: 0.0
Step: 36730, train/grad_norm: 1.0722577492572327e-08
Step: 36730, train/learning_rate: 6.294621471170103e-06
Step: 36730, train/epoch: 8.74107551574707
Step: 36740, train/loss: 0.0
Step: 36740, train/grad_norm: 2.1997024202136117e-09
Step: 36740, train/learning_rate: 6.282722551986808e-06
Step: 36740, train/epoch: 8.74345588684082
Step: 36750, train/loss: 0.0
Step: 36750, train/grad_norm: 6.145445957672635e-11
Step: 36750, train/learning_rate: 6.270823632803513e-06
Step: 36750, train/epoch: 8.745835304260254
Step: 36760, train/loss: 0.0
Step: 36760, train/grad_norm: 1.4373402024148163e-08
Step: 36760, train/learning_rate: 6.258924258872867e-06
Step: 36760, train/epoch: 8.748214721679688
Step: 36770, train/loss: 0.0
Step: 36770, train/grad_norm: 3.0460579836244506e-08
Step: 36770, train/learning_rate: 6.247025339689571e-06
Step: 36770, train/epoch: 8.750595092773438
Step: 36780, train/loss: 0.0
Step: 36780, train/grad_norm: 1.1828114043765936e-09
Step: 36780, train/learning_rate: 6.235125965758925e-06
Step: 36780, train/epoch: 8.752974510192871
Step: 36790, train/loss: 0.0
Step: 36790, train/grad_norm: 6.151041481716746e-11
Step: 36790, train/learning_rate: 6.22322704657563e-06
Step: 36790, train/epoch: 8.755354881286621
Step: 36800, train/loss: 0.0
Step: 36800, train/grad_norm: 1.8040120153273165e-07
Step: 36800, train/learning_rate: 6.211328127392335e-06
Step: 36800, train/epoch: 8.757734298706055
Step: 36810, train/loss: 0.0
Step: 36810, train/grad_norm: 5.107555489658466e-10
Step: 36810, train/learning_rate: 6.199428753461689e-06
Step: 36810, train/epoch: 8.760114669799805
Step: 36820, train/loss: 0.0
Step: 36820, train/grad_norm: 5.077594344982117e-09
Step: 36820, train/learning_rate: 6.1875298342783935e-06
Step: 36820, train/epoch: 8.762494087219238
Step: 36830, train/loss: 0.0
Step: 36830, train/grad_norm: 6.502589400270153e-08
Step: 36830, train/learning_rate: 6.1756304603477474e-06
Step: 36830, train/epoch: 8.764873504638672
Step: 36840, train/loss: 0.0
Step: 36840, train/grad_norm: 1.7722390222019158e-10
Step: 36840, train/learning_rate: 6.163731541164452e-06
Step: 36840, train/epoch: 8.767253875732422
Step: 36850, train/loss: 0.0
Step: 36850, train/grad_norm: 6.741493474571314e-11
Step: 36850, train/learning_rate: 6.151832621981157e-06
Step: 36850, train/epoch: 8.769633293151855
Step: 36860, train/loss: 0.0
Step: 36860, train/grad_norm: 8.294684938903174e-10
Step: 36860, train/learning_rate: 6.139933248050511e-06
Step: 36860, train/epoch: 8.772013664245605
Step: 36870, train/loss: 0.0
Step: 36870, train/grad_norm: 4.694729938137243e-08
Step: 36870, train/learning_rate: 6.128034328867216e-06
Step: 36870, train/epoch: 8.774393081665039
Step: 36880, train/loss: 0.0
Step: 36880, train/grad_norm: 5.572712291623816e-10
Step: 36880, train/learning_rate: 6.1161349549365696e-06
Step: 36880, train/epoch: 8.776772499084473
Step: 36890, train/loss: 0.0
Step: 36890, train/grad_norm: 4.760472918641234e-11
Step: 36890, train/learning_rate: 6.104236035753274e-06
Step: 36890, train/epoch: 8.779152870178223
Step: 36900, train/loss: 0.0
Step: 36900, train/grad_norm: 3.4373537002863053e-12
Step: 36900, train/learning_rate: 6.092337116569979e-06
Step: 36900, train/epoch: 8.781532287597656
Step: 36910, train/loss: 0.0
Step: 36910, train/grad_norm: 4.1971624398229324e-08
Step: 36910, train/learning_rate: 6.080437742639333e-06
Step: 36910, train/epoch: 8.783912658691406
Step: 36920, train/loss: 0.0
Step: 36920, train/grad_norm: 1.3305019586073996e-10
Step: 36920, train/learning_rate: 6.068538823456038e-06
Step: 36920, train/epoch: 8.78629207611084
Step: 36930, train/loss: 0.0
Step: 36930, train/grad_norm: 7.9280984555119e-11
Step: 36930, train/learning_rate: 6.0566399042727426e-06
Step: 36930, train/epoch: 8.78867244720459
Step: 36940, train/loss: 0.0
Step: 36940, train/grad_norm: 7.55323720236234e-11
Step: 36940, train/learning_rate: 6.0447405303420965e-06
Step: 36940, train/epoch: 8.791051864624023
Step: 36950, train/loss: 0.0
Step: 36950, train/grad_norm: 2.939967366777374e-11
Step: 36950, train/learning_rate: 6.032841611158801e-06
Step: 36950, train/epoch: 8.793431282043457
Step: 36960, train/loss: 0.0
Step: 36960, train/grad_norm: 5.229618279221915e-13
Step: 36960, train/learning_rate: 6.020942237228155e-06
Step: 36960, train/epoch: 8.795811653137207
Step: 36970, train/loss: 0.0
Step: 36970, train/grad_norm: 2.820682993842638e-07
Step: 36970, train/learning_rate: 6.00904331804486e-06
Step: 36970, train/epoch: 8.79819107055664
Step: 36980, train/loss: 0.0
Step: 36980, train/grad_norm: 3.4373604762549803e-07
Step: 36980, train/learning_rate: 5.997144398861565e-06
Step: 36980, train/epoch: 8.80057144165039
Step: 36990, train/loss: 0.0
Step: 36990, train/grad_norm: 2.454447478594801e-10
Step: 36990, train/learning_rate: 5.985245024930919e-06
Step: 36990, train/epoch: 8.802950859069824
Step: 37000, train/loss: 0.0
Step: 37000, train/grad_norm: 3.1263232003198027e-09
Step: 37000, train/learning_rate: 5.973346105747623e-06
Step: 37000, train/epoch: 8.805331230163574
Step: 37010, train/loss: 0.0
Step: 37010, train/grad_norm: 2.0403683187453225e-09
Step: 37010, train/learning_rate: 5.961446731816977e-06
Step: 37010, train/epoch: 8.807710647583008
Step: 37020, train/loss: 0.0
Step: 37020, train/grad_norm: 2.258351727846275e-08
Step: 37020, train/learning_rate: 5.949547812633682e-06
Step: 37020, train/epoch: 8.810090065002441
Step: 37030, train/loss: 0.0
Step: 37030, train/grad_norm: 1.2577619501463744e-10
Step: 37030, train/learning_rate: 5.937648893450387e-06
Step: 37030, train/epoch: 8.812470436096191
Step: 37040, train/loss: 0.0
Step: 37040, train/grad_norm: 3.322102662406401e-11
Step: 37040, train/learning_rate: 5.925749519519741e-06
Step: 37040, train/epoch: 8.814849853515625
Step: 37050, train/loss: 0.0
Step: 37050, train/grad_norm: 4.457848534755726e-10
Step: 37050, train/learning_rate: 5.9138506003364455e-06
Step: 37050, train/epoch: 8.817230224609375
Step: 37060, train/loss: 0.0
Step: 37060, train/grad_norm: 6.839780408718354e-10
Step: 37060, train/learning_rate: 5.901951226405799e-06
Step: 37060, train/epoch: 8.819609642028809
Step: 37070, train/loss: 0.0
Step: 37070, train/grad_norm: 3.169571272110261e-10
Step: 37070, train/learning_rate: 5.890052307222504e-06
Step: 37070, train/epoch: 8.821989059448242
Step: 37080, train/loss: 0.0
Step: 37080, train/grad_norm: 6.090131177138858e-11
Step: 37080, train/learning_rate: 5.878153388039209e-06
Step: 37080, train/epoch: 8.824369430541992
Step: 37090, train/loss: 0.00019999999494757503
Step: 37090, train/grad_norm: 8.605427481711558e-09
Step: 37090, train/learning_rate: 5.866254014108563e-06
Step: 37090, train/epoch: 8.826748847961426
Step: 37100, train/loss: 0.0
Step: 37100, train/grad_norm: 3.828513339687589e-10
Step: 37100, train/learning_rate: 5.854355094925268e-06
Step: 37100, train/epoch: 8.829129219055176
Step: 37110, train/loss: 0.0
Step: 37110, train/grad_norm: 1.2773307389579713e-07
Step: 37110, train/learning_rate: 5.842456175741972e-06
Step: 37110, train/epoch: 8.83150863647461
Step: 37120, train/loss: 0.0
Step: 37120, train/grad_norm: 1.210185587297019e-07
Step: 37120, train/learning_rate: 5.830556801811326e-06
Step: 37120, train/epoch: 8.83388900756836
Step: 37130, train/loss: 0.0
Step: 37130, train/grad_norm: 4.318204460673769e-09
Step: 37130, train/learning_rate: 5.818657882628031e-06
Step: 37130, train/epoch: 8.836268424987793
Step: 37140, train/loss: 0.0
Step: 37140, train/grad_norm: 2.895985187767458e-11
Step: 37140, train/learning_rate: 5.806758508697385e-06
Step: 37140, train/epoch: 8.838647842407227
Step: 37150, train/loss: 0.0
Step: 37150, train/grad_norm: 1.450500608513039e-09
Step: 37150, train/learning_rate: 5.79485958951409e-06
Step: 37150, train/epoch: 8.841028213500977
Step: 37160, train/loss: 0.0
Step: 37160, train/grad_norm: 3.902797196531083e-09
Step: 37160, train/learning_rate: 5.7829606703307945e-06
Step: 37160, train/epoch: 8.84340763092041
Step: 37170, train/loss: 0.0
Step: 37170, train/grad_norm: 0.00010599378583719954
Step: 37170, train/learning_rate: 5.771061296400148e-06
Step: 37170, train/epoch: 8.84578800201416
Step: 37180, train/loss: 0.0
Step: 37180, train/grad_norm: 1.1600025627556132e-11
Step: 37180, train/learning_rate: 5.759162377216853e-06
Step: 37180, train/epoch: 8.848167419433594
Step: 37190, train/loss: 0.0
Step: 37190, train/grad_norm: 2.6920328309643082e-05
Step: 37190, train/learning_rate: 5.747263003286207e-06
Step: 37190, train/epoch: 8.850547790527344
Step: 37200, train/loss: 0.0
Step: 37200, train/grad_norm: 2.783392127891915e-10
Step: 37200, train/learning_rate: 5.735364084102912e-06
Step: 37200, train/epoch: 8.852927207946777
Step: 37210, train/loss: 0.0
Step: 37210, train/grad_norm: 5.256758583982446e-09
Step: 37210, train/learning_rate: 5.723465164919617e-06
Step: 37210, train/epoch: 8.855306625366211
Step: 37220, train/loss: 0.0
Step: 37220, train/grad_norm: 2.8260413453828903e-10
Step: 37220, train/learning_rate: 5.7115657909889705e-06
Step: 37220, train/epoch: 8.857686996459961
Step: 37230, train/loss: 0.0
Step: 37230, train/grad_norm: 3.260126613113812e-09
Step: 37230, train/learning_rate: 5.699666871805675e-06
Step: 37230, train/epoch: 8.860066413879395
Step: 37240, train/loss: 0.0
Step: 37240, train/grad_norm: 1.0330086785970138e-10
Step: 37240, train/learning_rate: 5.68776795262238e-06
Step: 37240, train/epoch: 8.862446784973145
Step: 37250, train/loss: 0.0
Step: 37250, train/grad_norm: 1.2995365672452408e-08
Step: 37250, train/learning_rate: 5.675868578691734e-06
Step: 37250, train/epoch: 8.864826202392578
Step: 37260, train/loss: 0.0
Step: 37260, train/grad_norm: 8.060515597207996e-07
Step: 37260, train/learning_rate: 5.663969659508439e-06
Step: 37260, train/epoch: 8.867205619812012
Step: 37270, train/loss: 0.0
Step: 37270, train/grad_norm: 1.3953784794296098e-09
Step: 37270, train/learning_rate: 5.652070285577793e-06
Step: 37270, train/epoch: 8.869585990905762
Step: 37280, train/loss: 0.0
Step: 37280, train/grad_norm: 2.2545583178157358e-09
Step: 37280, train/learning_rate: 5.6401713663944975e-06
Step: 37280, train/epoch: 8.871965408325195
Step: 37290, train/loss: 0.0
Step: 37290, train/grad_norm: 6.30369468126446e-05
Step: 37290, train/learning_rate: 5.628272447211202e-06
Step: 37290, train/epoch: 8.874345779418945
Step: 37300, train/loss: 0.0
Step: 37300, train/grad_norm: 1.7558672027462308e-07
Step: 37300, train/learning_rate: 5.616373073280556e-06
Step: 37300, train/epoch: 8.876725196838379
Step: 37310, train/loss: 0.0
Step: 37310, train/grad_norm: 6.130490781686149e-09
Step: 37310, train/learning_rate: 5.604474154097261e-06
Step: 37310, train/epoch: 8.879105567932129
Step: 37320, train/loss: 0.07580000162124634
Step: 37320, train/grad_norm: 4.466799818914069e-09
Step: 37320, train/learning_rate: 5.592574780166615e-06
Step: 37320, train/epoch: 8.881484985351562
Step: 37330, train/loss: 0.0
Step: 37330, train/grad_norm: 1.2551862660359348e-08
Step: 37330, train/learning_rate: 5.58067586098332e-06
Step: 37330, train/epoch: 8.883864402770996
Step: 37340, train/loss: 0.0
Step: 37340, train/grad_norm: 4.494997938309098e-06
Step: 37340, train/learning_rate: 5.568776941800024e-06
Step: 37340, train/epoch: 8.886244773864746
Step: 37350, train/loss: 0.0
Step: 37350, train/grad_norm: 0.012030190788209438
Step: 37350, train/learning_rate: 5.556877567869378e-06
Step: 37350, train/epoch: 8.88862419128418
Step: 37360, train/loss: 9.999999747378752e-05
Step: 37360, train/grad_norm: 1.073714273047699e-08
Step: 37360, train/learning_rate: 5.544978648686083e-06
Step: 37360, train/epoch: 8.89100456237793
Step: 37370, train/loss: 0.0
Step: 37370, train/grad_norm: 1.8607176910379053e-09
Step: 37370, train/learning_rate: 5.533079274755437e-06
Step: 37370, train/epoch: 8.893383979797363
Step: 37380, train/loss: 0.0
Step: 37380, train/grad_norm: 4.678096132693099e-09
Step: 37380, train/learning_rate: 5.521180355572142e-06
Step: 37380, train/epoch: 8.895764350891113
Step: 37390, train/loss: 0.0
Step: 37390, train/grad_norm: 2.541967747404783e-09
Step: 37390, train/learning_rate: 5.5092814363888465e-06
Step: 37390, train/epoch: 8.898143768310547
Step: 37400, train/loss: 0.0
Step: 37400, train/grad_norm: 2.3984181307667995e-09
Step: 37400, train/learning_rate: 5.4973820624582e-06
Step: 37400, train/epoch: 8.90052318572998
Step: 37410, train/loss: 0.0
Step: 37410, train/grad_norm: 5.676127276532661e-11
Step: 37410, train/learning_rate: 5.485483143274905e-06
Step: 37410, train/epoch: 8.90290355682373
Step: 37420, train/loss: 0.0
Step: 37420, train/grad_norm: 3.492322403486625e-11
Step: 37420, train/learning_rate: 5.47358422409161e-06
Step: 37420, train/epoch: 8.905282974243164
Step: 37430, train/loss: 0.0
Step: 37430, train/grad_norm: 5.848118433959826e-08
Step: 37430, train/learning_rate: 5.461684850160964e-06
Step: 37430, train/epoch: 8.907663345336914
Step: 37440, train/loss: 0.0
Step: 37440, train/grad_norm: 3.2111230341413943e-10
Step: 37440, train/learning_rate: 5.449785930977669e-06
Step: 37440, train/epoch: 8.910042762756348
Step: 37450, train/loss: 0.0
Step: 37450, train/grad_norm: 1.3346250682388927e-07
Step: 37450, train/learning_rate: 5.4378865570470225e-06
Step: 37450, train/epoch: 8.912422180175781
Step: 37460, train/loss: 0.0
Step: 37460, train/grad_norm: 1.1901619689780318e-09
Step: 37460, train/learning_rate: 5.425987637863727e-06
Step: 37460, train/epoch: 8.914802551269531
Step: 37470, train/loss: 0.0
Step: 37470, train/grad_norm: 6.586538172509426e-12
Step: 37470, train/learning_rate: 5.414088718680432e-06
Step: 37470, train/epoch: 8.917181968688965
Step: 37480, train/loss: 0.0
Step: 37480, train/grad_norm: 4.682250143162037e-09
Step: 37480, train/learning_rate: 5.402189344749786e-06
Step: 37480, train/epoch: 8.919562339782715
Step: 37490, train/loss: 0.0
Step: 37490, train/grad_norm: 5.14706367804596e-11
Step: 37490, train/learning_rate: 5.390290425566491e-06
Step: 37490, train/epoch: 8.921941757202148
Step: 37500, train/loss: 0.0
Step: 37500, train/grad_norm: 6.054896167784207e-11
Step: 37500, train/learning_rate: 5.378391051635845e-06
Step: 37500, train/epoch: 8.924322128295898
Step: 37510, train/loss: 0.0
Step: 37510, train/grad_norm: 1.6125684121348627e-10
Step: 37510, train/learning_rate: 5.366492132452549e-06
Step: 37510, train/epoch: 8.926701545715332
Step: 37520, train/loss: 0.0
Step: 37520, train/grad_norm: 4.400846620455923e-08
Step: 37520, train/learning_rate: 5.354593213269254e-06
Step: 37520, train/epoch: 8.929080963134766
Step: 37530, train/loss: 0.0
Step: 37530, train/grad_norm: 2.523647708585486e-07
Step: 37530, train/learning_rate: 5.342693839338608e-06
Step: 37530, train/epoch: 8.931461334228516
Step: 37540, train/loss: 0.0
Step: 37540, train/grad_norm: 1.5525203345134742e-09
Step: 37540, train/learning_rate: 5.330794920155313e-06
Step: 37540, train/epoch: 8.93384075164795
Step: 37550, train/loss: 0.0
Step: 37550, train/grad_norm: 1.2355150236231793e-09
Step: 37550, train/learning_rate: 5.318895546224667e-06
Step: 37550, train/epoch: 8.9362211227417
Step: 37560, train/loss: 0.0
Step: 37560, train/grad_norm: 5.250610363161101e-11
Step: 37560, train/learning_rate: 5.3069966270413715e-06
Step: 37560, train/epoch: 8.938600540161133
Step: 37570, train/loss: 0.0
Step: 37570, train/grad_norm: 5.4213328271046635e-11
Step: 37570, train/learning_rate: 5.295097707858076e-06
Step: 37570, train/epoch: 8.940980911254883
Step: 37580, train/loss: 0.0
Step: 37580, train/grad_norm: 1.0221262204679338e-11
Step: 37580, train/learning_rate: 5.28319833392743e-06
Step: 37580, train/epoch: 8.943360328674316
Step: 37590, train/loss: 0.0
Step: 37590, train/grad_norm: 4.0815681501271683e-08
Step: 37590, train/learning_rate: 5.271299414744135e-06
Step: 37590, train/epoch: 8.94573974609375
Step: 37600, train/loss: 0.0
Step: 37600, train/grad_norm: 0.0007237673853524029
Step: 37600, train/learning_rate: 5.25940049556084e-06
Step: 37600, train/epoch: 8.9481201171875
Step: 37610, train/loss: 0.0
Step: 37610, train/grad_norm: 9.715528648257532e-08
Step: 37610, train/learning_rate: 5.247501121630194e-06
Step: 37610, train/epoch: 8.950499534606934
Step: 37620, train/loss: 0.0
Step: 37620, train/grad_norm: 0.019388213753700256
Step: 37620, train/learning_rate: 5.2356022024468984e-06
Step: 37620, train/epoch: 8.952879905700684
Step: 37630, train/loss: 0.0
Step: 37630, train/grad_norm: 9.872854889181326e-07
Step: 37630, train/learning_rate: 5.223702828516252e-06
Step: 37630, train/epoch: 8.955259323120117
Step: 37640, train/loss: 0.0
Step: 37640, train/grad_norm: 1.7433053611792815e-10
Step: 37640, train/learning_rate: 5.211803909332957e-06
Step: 37640, train/epoch: 8.957639694213867
Step: 37650, train/loss: 0.0
Step: 37650, train/grad_norm: 1.430872575980402e-08
Step: 37650, train/learning_rate: 5.199904990149662e-06
Step: 37650, train/epoch: 8.9600191116333
Step: 37660, train/loss: 0.0
Step: 37660, train/grad_norm: 2.0588590832204545e-09
Step: 37660, train/learning_rate: 5.188005616219016e-06
Step: 37660, train/epoch: 8.962398529052734
Step: 37670, train/loss: 0.0
Step: 37670, train/grad_norm: 1.5740823755550082e-08
Step: 37670, train/learning_rate: 5.1761066970357206e-06
Step: 37670, train/epoch: 8.964778900146484
Step: 37680, train/loss: 0.0
Step: 37680, train/grad_norm: 1.3240392289504133e-11
Step: 37680, train/learning_rate: 5.1642073231050745e-06
Step: 37680, train/epoch: 8.967158317565918
Step: 37690, train/loss: 0.0
Step: 37690, train/grad_norm: 1.0129941330205838e-10
Step: 37690, train/learning_rate: 5.152308403921779e-06
Step: 37690, train/epoch: 8.969538688659668
Step: 37700, train/loss: 0.0
Step: 37700, train/grad_norm: 9.88860847428441e-07
Step: 37700, train/learning_rate: 5.140409484738484e-06
Step: 37700, train/epoch: 8.971918106079102
Step: 37710, train/loss: 0.0
Step: 37710, train/grad_norm: 5.144203119034074e-11
Step: 37710, train/learning_rate: 5.128510110807838e-06
Step: 37710, train/epoch: 8.974297523498535
Step: 37720, train/loss: 0.0
Step: 37720, train/grad_norm: 2.4900546069517304e-09
Step: 37720, train/learning_rate: 5.116611191624543e-06
Step: 37720, train/epoch: 8.976677894592285
Step: 37730, train/loss: 0.0
Step: 37730, train/grad_norm: 4.531134495389111e-11
Step: 37730, train/learning_rate: 5.104711817693897e-06
Step: 37730, train/epoch: 8.979057312011719
Step: 37740, train/loss: 0.0
Step: 37740, train/grad_norm: 1.0325706956137992e-10
Step: 37740, train/learning_rate: 5.092812898510601e-06
Step: 37740, train/epoch: 8.981437683105469
Step: 37750, train/loss: 0.0
Step: 37750, train/grad_norm: 1.2551928385562405e-08
Step: 37750, train/learning_rate: 5.080913979327306e-06
Step: 37750, train/epoch: 8.983817100524902
Step: 37760, train/loss: 0.0
Step: 37760, train/grad_norm: 5.921193405811209e-06
Step: 37760, train/learning_rate: 5.06901460539666e-06
Step: 37760, train/epoch: 8.986197471618652
Step: 37770, train/loss: 0.0
Step: 37770, train/grad_norm: 1.306407204637594e-10
Step: 37770, train/learning_rate: 5.057115686213365e-06
Step: 37770, train/epoch: 8.988576889038086
Step: 37780, train/loss: 0.0
Step: 37780, train/grad_norm: 4.6533998876441274e-10
Step: 37780, train/learning_rate: 5.04521676703007e-06
Step: 37780, train/epoch: 8.99095630645752
Step: 37790, train/loss: 0.0
Step: 37790, train/grad_norm: 4.376643119030632e-06
Step: 37790, train/learning_rate: 5.0333173930994235e-06
Step: 37790, train/epoch: 8.99333667755127
Step: 37800, train/loss: 0.0
Step: 37800, train/grad_norm: 2.4224029004926706e-08
Step: 37800, train/learning_rate: 5.021418473916128e-06
Step: 37800, train/epoch: 8.995716094970703
Step: 37810, train/loss: 0.0
Step: 37810, train/grad_norm: 4.758768046286832e-09
Step: 37810, train/learning_rate: 5.009519099985482e-06
Step: 37810, train/epoch: 8.998096466064453
Step: 37818, eval/loss: 0.03721603751182556
Step: 37818, eval/accuracy: 0.9970845580101013
Step: 37818, eval/f1: 0.9969180822372437
Step: 37818, eval/runtime: 735.8663940429688
Step: 37818, eval/samples_per_second: 9.788000106811523
Step: 37818, eval/steps_per_second: 1.2239999771118164
Step: 37818, train/epoch: 9.0
Step: 37820, train/loss: 0.0
Step: 37820, train/grad_norm: 6.08053269957054e-08
Step: 37820, train/learning_rate: 4.997620180802187e-06
Step: 37820, train/epoch: 9.000475883483887
Step: 37830, train/loss: 0.0
Step: 37830, train/grad_norm: 1.4753118193766568e-07
Step: 37830, train/learning_rate: 4.985721261618892e-06
Step: 37830, train/epoch: 9.002856254577637
Step: 37840, train/loss: 0.0
Step: 37840, train/grad_norm: 5.454019014194955e-09
Step: 37840, train/learning_rate: 4.973821887688246e-06
Step: 37840, train/epoch: 9.00523567199707
Step: 37850, train/loss: 0.0
Step: 37850, train/grad_norm: 8.18216994158405e-10
Step: 37850, train/learning_rate: 4.96192296850495e-06
Step: 37850, train/epoch: 9.007615089416504
Step: 37860, train/loss: 0.0
Step: 37860, train/grad_norm: 7.616884900585319e-10
Step: 37860, train/learning_rate: 4.950023594574304e-06
Step: 37860, train/epoch: 9.009995460510254
Step: 37870, train/loss: 0.0
Step: 37870, train/grad_norm: 1.0086349810922712e-10
Step: 37870, train/learning_rate: 4.938124675391009e-06
Step: 37870, train/epoch: 9.012374877929688
Step: 37880, train/loss: 0.0
Step: 37880, train/grad_norm: 8.280209851108111e-12
Step: 37880, train/learning_rate: 4.926225756207714e-06
Step: 37880, train/epoch: 9.014755249023438
Step: 37890, train/loss: 0.0
Step: 37890, train/grad_norm: 1.6122749357805333e-08
Step: 37890, train/learning_rate: 4.914326382277068e-06
Step: 37890, train/epoch: 9.017134666442871
Step: 37900, train/loss: 0.0
Step: 37900, train/grad_norm: 7.204598723165034e-11
Step: 37900, train/learning_rate: 4.9024274630937725e-06
Step: 37900, train/epoch: 9.019514083862305
Step: 37910, train/loss: 0.0
Step: 37910, train/grad_norm: 1.7148997499827345e-10
Step: 37910, train/learning_rate: 4.890528543910477e-06
Step: 37910, train/epoch: 9.021894454956055
Step: 37920, train/loss: 0.0
Step: 37920, train/grad_norm: 1.2448667874309649e-08
Step: 37920, train/learning_rate: 4.878629169979831e-06
Step: 37920, train/epoch: 9.024273872375488
Step: 37930, train/loss: 0.0
Step: 37930, train/grad_norm: 1.3242652807665678e-10
Step: 37930, train/learning_rate: 4.866730250796536e-06
Step: 37930, train/epoch: 9.026654243469238
Step: 37940, train/loss: 0.0
Step: 37940, train/grad_norm: 2.821024880708678e-12
Step: 37940, train/learning_rate: 4.85483087686589e-06
Step: 37940, train/epoch: 9.029033660888672
Step: 37950, train/loss: 0.0
Step: 37950, train/grad_norm: 7.450916619800019e-12
Step: 37950, train/learning_rate: 4.842931957682595e-06
Step: 37950, train/epoch: 9.031414031982422
Step: 37960, train/loss: 0.0
Step: 37960, train/grad_norm: 1.789414172392867e-10
Step: 37960, train/learning_rate: 4.8310330384992994e-06
Step: 37960, train/epoch: 9.033793449401855
Step: 37970, train/loss: 0.0
Step: 37970, train/grad_norm: 4.394671293539432e-09
Step: 37970, train/learning_rate: 4.819133664568653e-06
Step: 37970, train/epoch: 9.036172866821289
Step: 37980, train/loss: 0.0
Step: 37980, train/grad_norm: 1.2018681161407585e-08
Step: 37980, train/learning_rate: 4.807234745385358e-06
Step: 37980, train/epoch: 9.038553237915039
Step: 37990, train/loss: 0.0
Step: 37990, train/grad_norm: 1.1523701459736913e-07
Step: 37990, train/learning_rate: 4.795335371454712e-06
Step: 37990, train/epoch: 9.040932655334473
Step: 38000, train/loss: 0.0
Step: 38000, train/grad_norm: 2.9732635198342905e-07
Step: 38000, train/learning_rate: 4.783436452271417e-06
Step: 38000, train/epoch: 9.043313026428223
Step: 38010, train/loss: 0.0
Step: 38010, train/grad_norm: 1.256651172010237e-10
Step: 38010, train/learning_rate: 4.7715375330881216e-06
Step: 38010, train/epoch: 9.045692443847656
Step: 38020, train/loss: 0.0
Step: 38020, train/grad_norm: 5.845743444865548e-09
Step: 38020, train/learning_rate: 4.7596381591574755e-06
Step: 38020, train/epoch: 9.048072814941406
Step: 38030, train/loss: 0.0
Step: 38030, train/grad_norm: 8.352643576792218e-10
Step: 38030, train/learning_rate: 4.74773923997418e-06
Step: 38030, train/epoch: 9.05045223236084
Step: 38040, train/loss: 0.0
Step: 38040, train/grad_norm: 2.5876497125598696e-11
Step: 38040, train/learning_rate: 4.735839866043534e-06
Step: 38040, train/epoch: 9.052831649780273
Step: 38050, train/loss: 0.0
Step: 38050, train/grad_norm: 3.9489278513826775e-10
Step: 38050, train/learning_rate: 4.723940946860239e-06
Step: 38050, train/epoch: 9.055212020874023
Step: 38060, train/loss: 0.0
Step: 38060, train/grad_norm: 8.760273617625103e-10
Step: 38060, train/learning_rate: 4.712042027676944e-06
Step: 38060, train/epoch: 9.057591438293457
Step: 38070, train/loss: 0.0
Step: 38070, train/grad_norm: 1.519007003514261e-10
Step: 38070, train/learning_rate: 4.700142653746298e-06
Step: 38070, train/epoch: 9.059971809387207
Step: 38080, train/loss: 0.0
Step: 38080, train/grad_norm: 3.7401065355879837e-07
Step: 38080, train/learning_rate: 4.688243734563002e-06
Step: 38080, train/epoch: 9.06235122680664
Step: 38090, train/loss: 0.0
Step: 38090, train/grad_norm: 1.8336471896418516e-08
Step: 38090, train/learning_rate: 4.676344815379707e-06
Step: 38090, train/epoch: 9.064730644226074
Step: 38100, train/loss: 0.0
Step: 38100, train/grad_norm: 2.329625659047707e-10
Step: 38100, train/learning_rate: 4.664445441449061e-06
Step: 38100, train/epoch: 9.067111015319824
Step: 38110, train/loss: 0.0
Step: 38110, train/grad_norm: 8.943182860932097e-10
Step: 38110, train/learning_rate: 4.652546522265766e-06
Step: 38110, train/epoch: 9.069490432739258
Step: 38120, train/loss: 0.0
Step: 38120, train/grad_norm: 5.113647283394585e-10
Step: 38120, train/learning_rate: 4.64064714833512e-06
Step: 38120, train/epoch: 9.071870803833008
Step: 38130, train/loss: 0.0
Step: 38130, train/grad_norm: 3.734293430035507e-11
Step: 38130, train/learning_rate: 4.6287482291518245e-06
Step: 38130, train/epoch: 9.074250221252441
Step: 38140, train/loss: 0.0
Step: 38140, train/grad_norm: 1.3619467331693613e-08
Step: 38140, train/learning_rate: 4.616849309968529e-06
Step: 38140, train/epoch: 9.076630592346191
Step: 38150, train/loss: 0.0
Step: 38150, train/grad_norm: 1.2739164167996364e-09
Step: 38150, train/learning_rate: 4.604949936037883e-06
Step: 38150, train/epoch: 9.079010009765625
Step: 38160, train/loss: 0.0
Step: 38160, train/grad_norm: 3.430505103096948e-06
Step: 38160, train/learning_rate: 4.593051016854588e-06
Step: 38160, train/epoch: 9.081389427185059
Step: 38170, train/loss: 0.0
Step: 38170, train/grad_norm: 4.463628425210864e-11
Step: 38170, train/learning_rate: 4.581151642923942e-06
Step: 38170, train/epoch: 9.083769798278809
Step: 38180, train/loss: 0.0
Step: 38180, train/grad_norm: 1.3486804562035104e-07
Step: 38180, train/learning_rate: 4.569252723740647e-06
Step: 38180, train/epoch: 9.086149215698242
Step: 38190, train/loss: 0.0
Step: 38190, train/grad_norm: 2.377239383122287e-08
Step: 38190, train/learning_rate: 4.557353804557351e-06
Step: 38190, train/epoch: 9.088529586791992
Step: 38200, train/loss: 0.0
Step: 38200, train/grad_norm: 7.442535476798184e-11
Step: 38200, train/learning_rate: 4.545454430626705e-06
Step: 38200, train/epoch: 9.090909004211426
Step: 38210, train/loss: 0.0
Step: 38210, train/grad_norm: 8.498680648116874e-10
Step: 38210, train/learning_rate: 4.53355551144341e-06
Step: 38210, train/epoch: 9.093289375305176
Step: 38220, train/loss: 0.0
Step: 38220, train/grad_norm: 1.8087982223136123e-09
Step: 38220, train/learning_rate: 4.521656137512764e-06
Step: 38220, train/epoch: 9.09566879272461
Step: 38230, train/loss: 0.0
Step: 38230, train/grad_norm: 4.926846708386279e-10
Step: 38230, train/learning_rate: 4.509757218329469e-06
Step: 38230, train/epoch: 9.098048210144043
Step: 38240, train/loss: 0.0
Step: 38240, train/grad_norm: 1.4462021580285978e-09
Step: 38240, train/learning_rate: 4.4978582991461735e-06
Step: 38240, train/epoch: 9.100428581237793
Step: 38250, train/loss: 0.0
Step: 38250, train/grad_norm: 8.661383242569709e-12
Step: 38250, train/learning_rate: 4.485958925215527e-06
Step: 38250, train/epoch: 9.102807998657227
Step: 38260, train/loss: 0.0
Step: 38260, train/grad_norm: 6.816481823435083e-10
Step: 38260, train/learning_rate: 4.474060006032232e-06
Step: 38260, train/epoch: 9.105188369750977
Step: 38270, train/loss: 0.0
Step: 38270, train/grad_norm: 1.9484371449607352e-08
Step: 38270, train/learning_rate: 4.462161086848937e-06
Step: 38270, train/epoch: 9.10756778717041
Step: 38280, train/loss: 0.0
Step: 38280, train/grad_norm: 1.9287810459900356e-08
Step: 38280, train/learning_rate: 4.450261712918291e-06
Step: 38280, train/epoch: 9.109947204589844
Step: 38290, train/loss: 0.0
Step: 38290, train/grad_norm: 3.1308384773609532e-09
Step: 38290, train/learning_rate: 4.438362793734996e-06
Step: 38290, train/epoch: 9.112327575683594
Step: 38300, train/loss: 0.0
Step: 38300, train/grad_norm: 7.600997165013723e-09
Step: 38300, train/learning_rate: 4.4264634198043495e-06
Step: 38300, train/epoch: 9.114706993103027
Step: 38310, train/loss: 0.0
Step: 38310, train/grad_norm: 7.84823139809987e-09
Step: 38310, train/learning_rate: 4.414564500621054e-06
Step: 38310, train/epoch: 9.117087364196777
Step: 38320, train/loss: 0.0
Step: 38320, train/grad_norm: 4.1154961383238486e-11
Step: 38320, train/learning_rate: 4.402665581437759e-06
Step: 38320, train/epoch: 9.119466781616211
Step: 38330, train/loss: 0.0
Step: 38330, train/grad_norm: 5.0008538976520356e-12
Step: 38330, train/learning_rate: 4.390766207507113e-06
Step: 38330, train/epoch: 9.121847152709961
Step: 38340, train/loss: 0.0
Step: 38340, train/grad_norm: 5.591635154900132e-09
Step: 38340, train/learning_rate: 4.378867288323818e-06
Step: 38340, train/epoch: 9.124226570129395
Step: 38350, train/loss: 0.0
Step: 38350, train/grad_norm: 4.055251794088388e-12
Step: 38350, train/learning_rate: 4.366967914393172e-06
Step: 38350, train/epoch: 9.126605987548828
Step: 38360, train/loss: 0.0
Step: 38360, train/grad_norm: 6.663432999820529e-11
Step: 38360, train/learning_rate: 4.3550689952098764e-06
Step: 38360, train/epoch: 9.128986358642578
Step: 38370, train/loss: 0.0
Step: 38370, train/grad_norm: 6.563147941784919e-11
Step: 38370, train/learning_rate: 4.343170076026581e-06
Step: 38370, train/epoch: 9.131365776062012
Step: 38380, train/loss: 0.0
Step: 38380, train/grad_norm: 1.5283024845658133e-09
Step: 38380, train/learning_rate: 4.331270702095935e-06
Step: 38380, train/epoch: 9.133746147155762
Step: 38390, train/loss: 0.0
Step: 38390, train/grad_norm: 7.981918598298776e-11
Step: 38390, train/learning_rate: 4.31937178291264e-06
Step: 38390, train/epoch: 9.136125564575195
Step: 38400, train/loss: 0.0
Step: 38400, train/grad_norm: 6.502153610199457e-06
Step: 38400, train/learning_rate: 4.307472408981994e-06
Step: 38400, train/epoch: 9.138505935668945
Step: 38410, train/loss: 0.0
Step: 38410, train/grad_norm: 6.083299836090461e-11
Step: 38410, train/learning_rate: 4.2955734897986986e-06
Step: 38410, train/epoch: 9.140885353088379
Step: 38420, train/loss: 0.0
Step: 38420, train/grad_norm: 5.380873968618971e-09
Step: 38420, train/learning_rate: 4.283674570615403e-06
Step: 38420, train/epoch: 9.143264770507812
Step: 38430, train/loss: 0.0
Step: 38430, train/grad_norm: 4.29729141160351e-10
Step: 38430, train/learning_rate: 4.271775196684757e-06
Step: 38430, train/epoch: 9.145645141601562
Step: 38440, train/loss: 0.0
Step: 38440, train/grad_norm: 2.319279435170074e-09
Step: 38440, train/learning_rate: 4.259876277501462e-06
Step: 38440, train/epoch: 9.148024559020996
Step: 38450, train/loss: 0.0
Step: 38450, train/grad_norm: 1.136638116960853e-09
Step: 38450, train/learning_rate: 4.247977358318167e-06
Step: 38450, train/epoch: 9.150404930114746
Step: 38460, train/loss: 0.0
Step: 38460, train/grad_norm: 1.5370646977430624e-09
Step: 38460, train/learning_rate: 4.236077984387521e-06
Step: 38460, train/epoch: 9.15278434753418
Step: 38470, train/loss: 0.0
Step: 38470, train/grad_norm: 7.844393579148345e-09
Step: 38470, train/learning_rate: 4.2241790652042255e-06
Step: 38470, train/epoch: 9.155163764953613
Step: 38480, train/loss: 0.0
Step: 38480, train/grad_norm: 5.2727634758387154e-11
Step: 38480, train/learning_rate: 4.212279691273579e-06
Step: 38480, train/epoch: 9.157544136047363
Step: 38490, train/loss: 0.0
Step: 38490, train/grad_norm: 1.1222033302615841e-09
Step: 38490, train/learning_rate: 4.200380772090284e-06
Step: 38490, train/epoch: 9.159923553466797
Step: 38500, train/loss: 0.0
Step: 38500, train/grad_norm: 2.275989841038495e-09
Step: 38500, train/learning_rate: 4.188481852906989e-06
Step: 38500, train/epoch: 9.162303924560547
Step: 38510, train/loss: 0.0
Step: 38510, train/grad_norm: 3.507131807189978e-10
Step: 38510, train/learning_rate: 4.176582478976343e-06
Step: 38510, train/epoch: 9.16468334197998
Step: 38520, train/loss: 0.0
Step: 38520, train/grad_norm: 1.6323675744445154e-10
Step: 38520, train/learning_rate: 4.164683559793048e-06
Step: 38520, train/epoch: 9.16706371307373
Step: 38530, train/loss: 0.0
Step: 38530, train/grad_norm: 2.0146252666108921e-07
Step: 38530, train/learning_rate: 4.1527841858624015e-06
Step: 38530, train/epoch: 9.169443130493164
Step: 38540, train/loss: 0.0
Step: 38540, train/grad_norm: 3.3751621497657425e-09
Step: 38540, train/learning_rate: 4.140885266679106e-06
Step: 38540, train/epoch: 9.171822547912598
Step: 38550, train/loss: 0.0
Step: 38550, train/grad_norm: 1.818730860358997e-10
Step: 38550, train/learning_rate: 4.128986347495811e-06
Step: 38550, train/epoch: 9.174202919006348
Step: 38560, train/loss: 0.0
Step: 38560, train/grad_norm: 1.8321753003647245e-08
Step: 38560, train/learning_rate: 4.117086973565165e-06
Step: 38560, train/epoch: 9.176582336425781
Step: 38570, train/loss: 0.0
Step: 38570, train/grad_norm: 4.871097414316239e-10
Step: 38570, train/learning_rate: 4.10518805438187e-06
Step: 38570, train/epoch: 9.178962707519531
Step: 38580, train/loss: 0.0
Step: 38580, train/grad_norm: 3.3553063105706826e-10
Step: 38580, train/learning_rate: 4.0932891351985745e-06
Step: 38580, train/epoch: 9.181342124938965
Step: 38590, train/loss: 0.0
Step: 38590, train/grad_norm: 9.55542756031491e-09
Step: 38590, train/learning_rate: 4.081389761267928e-06
Step: 38590, train/epoch: 9.183722496032715
Step: 38600, train/loss: 0.0
Step: 38600, train/grad_norm: 5.0921569044737325e-11
Step: 38600, train/learning_rate: 4.069490842084633e-06
Step: 38600, train/epoch: 9.186101913452148
Step: 38610, train/loss: 0.0
Step: 38610, train/grad_norm: 2.3108576684138793e-13
Step: 38610, train/learning_rate: 4.057591468153987e-06
Step: 38610, train/epoch: 9.188481330871582
Step: 38620, train/loss: 0.0
Step: 38620, train/grad_norm: 2.7782570688472674e-10
Step: 38620, train/learning_rate: 4.045692548970692e-06
Step: 38620, train/epoch: 9.190861701965332
Step: 38630, train/loss: 0.0
Step: 38630, train/grad_norm: 1.0302252800853395e-10
Step: 38630, train/learning_rate: 4.033793629787397e-06
Step: 38630, train/epoch: 9.193241119384766
Step: 38640, train/loss: 0.0
Step: 38640, train/grad_norm: 6.8730698216112884e-12
Step: 38640, train/learning_rate: 4.0218942558567505e-06
Step: 38640, train/epoch: 9.195621490478516
Step: 38650, train/loss: 0.0
Step: 38650, train/grad_norm: 1.5286701071648423e-10
Step: 38650, train/learning_rate: 4.009995336673455e-06
Step: 38650, train/epoch: 9.19800090789795
Step: 38660, train/loss: 0.0
Step: 38660, train/grad_norm: 4.15679330956209e-08
Step: 38660, train/learning_rate: 3.998095962742809e-06
Step: 38660, train/epoch: 9.200380325317383
Step: 38670, train/loss: 0.0
Step: 38670, train/grad_norm: 4.925676620054498e-12
Step: 38670, train/learning_rate: 3.986197043559514e-06
Step: 38670, train/epoch: 9.202760696411133
Step: 38680, train/loss: 0.0
Step: 38680, train/grad_norm: 3.8310639383087874e-11
Step: 38680, train/learning_rate: 3.974298124376219e-06
Step: 38680, train/epoch: 9.205140113830566
Step: 38690, train/loss: 0.0
Step: 38690, train/grad_norm: 5.409119332999701e-12
Step: 38690, train/learning_rate: 3.962398750445573e-06
Step: 38690, train/epoch: 9.207520484924316
Step: 38700, train/loss: 0.0
Step: 38700, train/grad_norm: 1.697194740302166e-08
Step: 38700, train/learning_rate: 3.9504998312622774e-06
Step: 38700, train/epoch: 9.20989990234375
Step: 38710, train/loss: 0.0
Step: 38710, train/grad_norm: 1.0053178289837206e-09
Step: 38710, train/learning_rate: 3.938600457331631e-06
Step: 38710, train/epoch: 9.2122802734375
Step: 38720, train/loss: 0.0
Step: 38720, train/grad_norm: 9.963199015805557e-11
Step: 38720, train/learning_rate: 3.926701538148336e-06
Step: 38720, train/epoch: 9.214659690856934
Step: 38730, train/loss: 0.0
Step: 38730, train/grad_norm: 3.9974157317601566e-08
Step: 38730, train/learning_rate: 3.914802618965041e-06
Step: 38730, train/epoch: 9.217039108276367
Step: 38740, train/loss: 0.0
Step: 38740, train/grad_norm: 2.4694901679112036e-10
Step: 38740, train/learning_rate: 3.902903245034395e-06
Step: 38740, train/epoch: 9.219419479370117
Step: 38750, train/loss: 0.0
Step: 38750, train/grad_norm: 1.0744860556854974e-09
Step: 38750, train/learning_rate: 3.8910043258510996e-06
Step: 38750, train/epoch: 9.22179889678955
Step: 38760, train/loss: 0.0
Step: 38760, train/grad_norm: 1.4392574805566216e-11
Step: 38760, train/learning_rate: 3.879105406667804e-06
Step: 38760, train/epoch: 9.2241792678833
Step: 38770, train/loss: 0.0
Step: 38770, train/grad_norm: 1.0666066252440487e-08
Step: 38770, train/learning_rate: 3.867206032737158e-06
Step: 38770, train/epoch: 9.226558685302734
Step: 38780, train/loss: 0.0
Step: 38780, train/grad_norm: 2.786860464620844e-10
Step: 38780, train/learning_rate: 3.855307113553863e-06
Step: 38780, train/epoch: 9.228939056396484
Step: 38790, train/loss: 0.0
Step: 38790, train/grad_norm: 1.5258111440985545e-10
Step: 38790, train/learning_rate: 3.843407739623217e-06
Step: 38790, train/epoch: 9.231318473815918
Step: 38800, train/loss: 0.0
Step: 38800, train/grad_norm: 4.148274683757336e-07
Step: 38800, train/learning_rate: 3.831508820439922e-06
Step: 38800, train/epoch: 9.233697891235352
Step: 38810, train/loss: 0.0
Step: 38810, train/grad_norm: 2.4055388792021404e-09
Step: 38810, train/learning_rate: 3.8196099012566265e-06
Step: 38810, train/epoch: 9.236078262329102
Step: 38820, train/loss: 0.0
Step: 38820, train/grad_norm: 3.88300165354849e-08
Step: 38820, train/learning_rate: 3.8077105273259804e-06
Step: 38820, train/epoch: 9.238457679748535
Step: 38830, train/loss: 0.0
Step: 38830, train/grad_norm: 1.6276929457048617e-11
Step: 38830, train/learning_rate: 3.795811608142685e-06
Step: 38830, train/epoch: 9.240838050842285
Step: 38840, train/loss: 0.0
Step: 38840, train/grad_norm: 2.8601004897765847e-10
Step: 38840, train/learning_rate: 3.7839124615857145e-06
Step: 38840, train/epoch: 9.243217468261719
Step: 38850, train/loss: 0.0
Step: 38850, train/grad_norm: 1.0053057621917105e-06
Step: 38850, train/learning_rate: 3.772013315028744e-06
Step: 38850, train/epoch: 9.245596885681152
Step: 38860, train/loss: 0.0
Step: 38860, train/grad_norm: 2.680066479854304e-08
Step: 38860, train/learning_rate: 3.760114168471773e-06
Step: 38860, train/epoch: 9.247977256774902
Step: 38870, train/loss: 0.0
Step: 38870, train/grad_norm: 6.895554065522447e-07
Step: 38870, train/learning_rate: 3.748215249288478e-06
Step: 38870, train/epoch: 9.250356674194336
Step: 38880, train/loss: 0.0
Step: 38880, train/grad_norm: 2.9363851794528273e-08
Step: 38880, train/learning_rate: 3.7363161027315073e-06
Step: 38880, train/epoch: 9.252737045288086
Step: 38890, train/loss: 0.0
Step: 38890, train/grad_norm: 2.6128005314873803e-10
Step: 38890, train/learning_rate: 3.7244169561745366e-06
Step: 38890, train/epoch: 9.25511646270752
Step: 38900, train/loss: 0.0
Step: 38900, train/grad_norm: 4.204950698749599e-08
Step: 38900, train/learning_rate: 3.712517809617566e-06
Step: 38900, train/epoch: 9.25749683380127
Step: 38910, train/loss: 0.0
Step: 38910, train/grad_norm: 4.181706092509785e-09
Step: 38910, train/learning_rate: 3.7006186630605953e-06
Step: 38910, train/epoch: 9.259876251220703
Step: 38920, train/loss: 0.0
Step: 38920, train/grad_norm: 4.5532833059525046e-10
Step: 38920, train/learning_rate: 3.6887197438773e-06
Step: 38920, train/epoch: 9.262255668640137
Step: 38930, train/loss: 0.0
Step: 38930, train/grad_norm: 3.0604141443291155e-09
Step: 38930, train/learning_rate: 3.6768205973203294e-06
Step: 38930, train/epoch: 9.264636039733887
Step: 38940, train/loss: 0.0
Step: 38940, train/grad_norm: 9.006921430909642e-09
Step: 38940, train/learning_rate: 3.6649214507633587e-06
Step: 38940, train/epoch: 9.26701545715332
Step: 38950, train/loss: 0.0
Step: 38950, train/grad_norm: 2.2388879639123616e-10
Step: 38950, train/learning_rate: 3.653022304206388e-06
Step: 38950, train/epoch: 9.26939582824707
Step: 38960, train/loss: 0.0
Step: 38960, train/grad_norm: 3.094986489315943e-10
Step: 38960, train/learning_rate: 3.641123385023093e-06
Step: 38960, train/epoch: 9.271775245666504
Step: 38970, train/loss: 0.0
Step: 38970, train/grad_norm: 1.4380219681697781e-08
Step: 38970, train/learning_rate: 3.629224238466122e-06
Step: 38970, train/epoch: 9.274155616760254
Step: 38980, train/loss: 0.0
Step: 38980, train/grad_norm: 2.4764248984787685e-10
Step: 38980, train/learning_rate: 3.6173250919091515e-06
Step: 38980, train/epoch: 9.276535034179688
Step: 38990, train/loss: 0.0
Step: 38990, train/grad_norm: 8.004217733059704e-09
Step: 38990, train/learning_rate: 3.605425945352181e-06
Step: 38990, train/epoch: 9.278914451599121
Step: 39000, train/loss: 0.0
Step: 39000, train/grad_norm: 6.044571787544584e-11
Step: 39000, train/learning_rate: 3.59352679879521e-06
Step: 39000, train/epoch: 9.281294822692871
Step: 39010, train/loss: 0.0
Step: 39010, train/grad_norm: 2.2402080190886409e-10
Step: 39010, train/learning_rate: 3.581627879611915e-06
Step: 39010, train/epoch: 9.283674240112305
Step: 39020, train/loss: 0.0
Step: 39020, train/grad_norm: 2.565559320544253e-08
Step: 39020, train/learning_rate: 3.5697287330549443e-06
Step: 39020, train/epoch: 9.286054611206055
Step: 39030, train/loss: 0.0
Step: 39030, train/grad_norm: 2.3641963053933068e-08
Step: 39030, train/learning_rate: 3.5578295864979737e-06
Step: 39030, train/epoch: 9.288434028625488
Step: 39040, train/loss: 0.0
Step: 39040, train/grad_norm: 1.9407128681869779e-10
Step: 39040, train/learning_rate: 3.545930439941003e-06
Step: 39040, train/epoch: 9.290813446044922
Step: 39050, train/loss: 0.0
Step: 39050, train/grad_norm: 1.2537794413791659e-10
Step: 39050, train/learning_rate: 3.5340315207577078e-06
Step: 39050, train/epoch: 9.293193817138672
Step: 39060, train/loss: 0.0
Step: 39060, train/grad_norm: 3.938625070531998e-08
Step: 39060, train/learning_rate: 3.522132374200737e-06
Step: 39060, train/epoch: 9.295573234558105
Step: 39070, train/loss: 0.0
Step: 39070, train/grad_norm: 2.6514106821196037e-07
Step: 39070, train/learning_rate: 3.5102332276437664e-06
Step: 39070, train/epoch: 9.297953605651855
Step: 39080, train/loss: 0.0
Step: 39080, train/grad_norm: 7.008438274169748e-10
Step: 39080, train/learning_rate: 3.4983340810867958e-06
Step: 39080, train/epoch: 9.300333023071289
Step: 39090, train/loss: 0.0
Step: 39090, train/grad_norm: 9.442698001294048e-08
Step: 39090, train/learning_rate: 3.486434934529825e-06
Step: 39090, train/epoch: 9.302713394165039
Step: 39100, train/loss: 0.0
Step: 39100, train/grad_norm: 1.61268405313858e-13
Step: 39100, train/learning_rate: 3.47453601534653e-06
Step: 39100, train/epoch: 9.305092811584473
Step: 39110, train/loss: 0.0
Step: 39110, train/grad_norm: 1.1470583416439695e-07
Step: 39110, train/learning_rate: 3.4626368687895592e-06
Step: 39110, train/epoch: 9.307472229003906
Step: 39120, train/loss: 0.0
Step: 39120, train/grad_norm: 9.58228909553327e-08
Step: 39120, train/learning_rate: 3.4507377222325886e-06
Step: 39120, train/epoch: 9.309852600097656
Step: 39130, train/loss: 0.0
Step: 39130, train/grad_norm: 5.121561912346806e-07
Step: 39130, train/learning_rate: 3.438838575675618e-06
Step: 39130, train/epoch: 9.31223201751709
Step: 39140, train/loss: 0.0
Step: 39140, train/grad_norm: 2.1913343362101045e-10
Step: 39140, train/learning_rate: 3.4269396564923227e-06
Step: 39140, train/epoch: 9.31461238861084
Step: 39150, train/loss: 0.0
Step: 39150, train/grad_norm: 1.3239086001703981e-05
Step: 39150, train/learning_rate: 3.415040509935352e-06
Step: 39150, train/epoch: 9.316991806030273
Step: 39160, train/loss: 0.0
Step: 39160, train/grad_norm: 5.08904065554816e-07
Step: 39160, train/learning_rate: 3.4031413633783814e-06
Step: 39160, train/epoch: 9.319372177124023
Step: 39170, train/loss: 0.0
Step: 39170, train/grad_norm: 4.3794196047386436e-10
Step: 39170, train/learning_rate: 3.3912422168214107e-06
Step: 39170, train/epoch: 9.321751594543457
Step: 39180, train/loss: 0.0
Step: 39180, train/grad_norm: 4.886062665576674e-10
Step: 39180, train/learning_rate: 3.37934307026444e-06
Step: 39180, train/epoch: 9.32413101196289
Step: 39190, train/loss: 0.0
Step: 39190, train/grad_norm: 9.659474609069463e-11
Step: 39190, train/learning_rate: 3.367444151081145e-06
Step: 39190, train/epoch: 9.32651138305664
Step: 39200, train/loss: 0.0
Step: 39200, train/grad_norm: 9.589696398082381e-11
Step: 39200, train/learning_rate: 3.355545004524174e-06
Step: 39200, train/epoch: 9.328890800476074
Step: 39210, train/loss: 0.0
Step: 39210, train/grad_norm: 5.319844564866116e-09
Step: 39210, train/learning_rate: 3.3436458579672035e-06
Step: 39210, train/epoch: 9.331271171569824
Step: 39220, train/loss: 0.0
Step: 39220, train/grad_norm: 2.7791594026105315e-10
Step: 39220, train/learning_rate: 3.331746711410233e-06
Step: 39220, train/epoch: 9.333650588989258
Step: 39230, train/loss: 0.0
Step: 39230, train/grad_norm: 2.0229964370344078e-09
Step: 39230, train/learning_rate: 3.3198477922269376e-06
Step: 39230, train/epoch: 9.336030006408691
Step: 39240, train/loss: 0.0
Step: 39240, train/grad_norm: 6.441268629586716e-10
Step: 39240, train/learning_rate: 3.307948645669967e-06
Step: 39240, train/epoch: 9.338410377502441
Step: 39250, train/loss: 0.0
Step: 39250, train/grad_norm: 1.029313466460735e-06
Step: 39250, train/learning_rate: 3.2960494991129963e-06
Step: 39250, train/epoch: 9.340789794921875
Step: 39260, train/loss: 0.0
Step: 39260, train/grad_norm: 7.519289080271108e-10
Step: 39260, train/learning_rate: 3.2841503525560256e-06
Step: 39260, train/epoch: 9.343170166015625
Step: 39270, train/loss: 0.0
Step: 39270, train/grad_norm: 2.2262758747615408e-08
Step: 39270, train/learning_rate: 3.272251205999055e-06
Step: 39270, train/epoch: 9.345549583435059
Step: 39280, train/loss: 0.0
Step: 39280, train/grad_norm: 6.151203102433556e-09
Step: 39280, train/learning_rate: 3.2603522868157597e-06
Step: 39280, train/epoch: 9.347929954528809
Step: 39290, train/loss: 0.0
Step: 39290, train/grad_norm: 3.4140174021146663e-10
Step: 39290, train/learning_rate: 3.248453140258789e-06
Step: 39290, train/epoch: 9.350309371948242
Step: 39300, train/loss: 0.0
Step: 39300, train/grad_norm: 3.9495662917943264e-07
Step: 39300, train/learning_rate: 3.2365539937018184e-06
Step: 39300, train/epoch: 9.352688789367676
Step: 39310, train/loss: 0.0
Step: 39310, train/grad_norm: 1.725781961781081e-09
Step: 39310, train/learning_rate: 3.2246548471448477e-06
Step: 39310, train/epoch: 9.355069160461426
Step: 39320, train/loss: 0.0
Step: 39320, train/grad_norm: 7.3114088117165466e-09
Step: 39320, train/learning_rate: 3.2127559279615525e-06
Step: 39320, train/epoch: 9.35744857788086
Step: 39330, train/loss: 0.0
Step: 39330, train/grad_norm: 3.382941704543896e-09
Step: 39330, train/learning_rate: 3.200856781404582e-06
Step: 39330, train/epoch: 9.35982894897461
Step: 39340, train/loss: 0.0
Step: 39340, train/grad_norm: 7.830242565454171e-10
Step: 39340, train/learning_rate: 3.188957634847611e-06
Step: 39340, train/epoch: 9.362208366394043
Step: 39350, train/loss: 0.0
Step: 39350, train/grad_norm: 6.390626743069117e-12
Step: 39350, train/learning_rate: 3.1770584882906405e-06
Step: 39350, train/epoch: 9.364588737487793
Step: 39360, train/loss: 0.0
Step: 39360, train/grad_norm: 7.094135001661783e-11
Step: 39360, train/learning_rate: 3.16515934173367e-06
Step: 39360, train/epoch: 9.366968154907227
Step: 39370, train/loss: 0.0
Step: 39370, train/grad_norm: 1.991218567809483e-08
Step: 39370, train/learning_rate: 3.1532604225503746e-06
Step: 39370, train/epoch: 9.36934757232666
Step: 39380, train/loss: 0.0
Step: 39380, train/grad_norm: 5.4394971166216166e-11
Step: 39380, train/learning_rate: 3.141361275993404e-06
Step: 39380, train/epoch: 9.37172794342041
Step: 39390, train/loss: 0.0
Step: 39390, train/grad_norm: 9.119841593019373e-08
Step: 39390, train/learning_rate: 3.1294621294364333e-06
Step: 39390, train/epoch: 9.374107360839844
Step: 39400, train/loss: 0.0
Step: 39400, train/grad_norm: 2.8655669837007736e-08
Step: 39400, train/learning_rate: 3.1175629828794627e-06
Step: 39400, train/epoch: 9.376487731933594
Step: 39410, train/loss: 0.0
Step: 39410, train/grad_norm: 2.367227247557224e-10
Step: 39410, train/learning_rate: 3.1056640636961674e-06
Step: 39410, train/epoch: 9.378867149353027
Step: 39420, train/loss: 0.0
Step: 39420, train/grad_norm: 1.6686006176769297e-07
Step: 39420, train/learning_rate: 3.0937649171391968e-06
Step: 39420, train/epoch: 9.381246566772461
Step: 39430, train/loss: 0.0
Step: 39430, train/grad_norm: 5.060858676575464e-11
Step: 39430, train/learning_rate: 3.081865770582226e-06
Step: 39430, train/epoch: 9.383626937866211
Step: 39440, train/loss: 0.0
Step: 39440, train/grad_norm: 1.4566896311407618e-07
Step: 39440, train/learning_rate: 3.0699666240252554e-06
Step: 39440, train/epoch: 9.386006355285645
Step: 39450, train/loss: 0.0
Step: 39450, train/grad_norm: 2.608266811421345e-07
Step: 39450, train/learning_rate: 3.0580674774682848e-06
Step: 39450, train/epoch: 9.388386726379395
Step: 39460, train/loss: 0.0
Step: 39460, train/grad_norm: 2.0478672091428507e-09
Step: 39460, train/learning_rate: 3.0461685582849896e-06
Step: 39460, train/epoch: 9.390766143798828
Step: 39470, train/loss: 0.0
Step: 39470, train/grad_norm: 1.932743209920318e-08
Step: 39470, train/learning_rate: 3.034269411728019e-06
Step: 39470, train/epoch: 9.393146514892578
Step: 39480, train/loss: 0.0
Step: 39480, train/grad_norm: 2.2485882947287372e-11
Step: 39480, train/learning_rate: 3.0223702651710482e-06
Step: 39480, train/epoch: 9.395525932312012
Step: 39490, train/loss: 0.0
Step: 39490, train/grad_norm: 3.6971389838491575e-10
Step: 39490, train/learning_rate: 3.0104711186140776e-06
Step: 39490, train/epoch: 9.397905349731445
Step: 39500, train/loss: 0.0
Step: 39500, train/grad_norm: 8.187830502492943e-08
Step: 39500, train/learning_rate: 2.9985721994307823e-06
Step: 39500, train/epoch: 9.400285720825195
Step: 39510, train/loss: 0.0
Step: 39510, train/grad_norm: 4.952002655045362e-06
Step: 39510, train/learning_rate: 2.9866730528738117e-06
Step: 39510, train/epoch: 9.402665138244629
Step: 39520, train/loss: 0.0
Step: 39520, train/grad_norm: 1.6672427882724605e-09
Step: 39520, train/learning_rate: 2.974773906316841e-06
Step: 39520, train/epoch: 9.405045509338379
Step: 39530, train/loss: 0.0
Step: 39530, train/grad_norm: 6.148483500112434e-09
Step: 39530, train/learning_rate: 2.9628747597598704e-06
Step: 39530, train/epoch: 9.407424926757812
Step: 39540, train/loss: 0.0
Step: 39540, train/grad_norm: 8.385202532323888e-10
Step: 39540, train/learning_rate: 2.9509756132028997e-06
Step: 39540, train/epoch: 9.409805297851562
Step: 39550, train/loss: 0.0
Step: 39550, train/grad_norm: 3.6648328816113462e-09
Step: 39550, train/learning_rate: 2.9390766940196045e-06
Step: 39550, train/epoch: 9.412184715270996
Step: 39560, train/loss: 0.0
Step: 39560, train/grad_norm: 1.1259553706066683e-10
Step: 39560, train/learning_rate: 2.927177547462634e-06
Step: 39560, train/epoch: 9.41456413269043
Step: 39570, train/loss: 0.0
Step: 39570, train/grad_norm: 2.6792302751377228e-11
Step: 39570, train/learning_rate: 2.915278400905663e-06
Step: 39570, train/epoch: 9.41694450378418
Step: 39580, train/loss: 0.0
Step: 39580, train/grad_norm: 5.360154542444207e-10
Step: 39580, train/learning_rate: 2.9033792543486925e-06
Step: 39580, train/epoch: 9.419323921203613
Step: 39590, train/loss: 0.0
Step: 39590, train/grad_norm: 4.744135821965756e-07
Step: 39590, train/learning_rate: 2.8914803351653973e-06
Step: 39590, train/epoch: 9.421704292297363
Step: 39600, train/loss: 0.0
Step: 39600, train/grad_norm: 2.8998019985237988e-08
Step: 39600, train/learning_rate: 2.8795811886084266e-06
Step: 39600, train/epoch: 9.424083709716797
Step: 39610, train/loss: 0.0
Step: 39610, train/grad_norm: 3.7763023263970297e-10
Step: 39610, train/learning_rate: 2.867682042051456e-06
Step: 39610, train/epoch: 9.42646312713623
Step: 39620, train/loss: 0.0
Step: 39620, train/grad_norm: 1.657337023175387e-08
Step: 39620, train/learning_rate: 2.8557828954944853e-06
Step: 39620, train/epoch: 9.42884349822998
Step: 39630, train/loss: 0.0
Step: 39630, train/grad_norm: 7.006665025954817e-09
Step: 39630, train/learning_rate: 2.84388397631119e-06
Step: 39630, train/epoch: 9.431222915649414
Step: 39640, train/loss: 0.0
Step: 39640, train/grad_norm: 9.559591340746465e-09
Step: 39640, train/learning_rate: 2.8319848297542194e-06
Step: 39640, train/epoch: 9.433603286743164
Step: 39650, train/loss: 0.0
Step: 39650, train/grad_norm: 6.976312505457827e-08
Step: 39650, train/learning_rate: 2.8200856831972487e-06
Step: 39650, train/epoch: 9.435982704162598
Step: 39660, train/loss: 0.0
Step: 39660, train/grad_norm: 2.22492779755612e-09
Step: 39660, train/learning_rate: 2.808186536640278e-06
Step: 39660, train/epoch: 9.438363075256348
Step: 39670, train/loss: 0.0
Step: 39670, train/grad_norm: 6.684431355097331e-07
Step: 39670, train/learning_rate: 2.7962873900833074e-06
Step: 39670, train/epoch: 9.440742492675781
Step: 39680, train/loss: 0.0
Step: 39680, train/grad_norm: 8.935552298083849e-11
Step: 39680, train/learning_rate: 2.784388470900012e-06
Step: 39680, train/epoch: 9.443121910095215
Step: 39690, train/loss: 0.0
Step: 39690, train/grad_norm: 1.4688061877876635e-09
Step: 39690, train/learning_rate: 2.7724893243430415e-06
Step: 39690, train/epoch: 9.445502281188965
Step: 39700, train/loss: 0.0
Step: 39700, train/grad_norm: 1.3523445696606018e-09
Step: 39700, train/learning_rate: 2.760590177786071e-06
Step: 39700, train/epoch: 9.447881698608398
Step: 39710, train/loss: 0.0
Step: 39710, train/grad_norm: 4.4209653299320806e-12
Step: 39710, train/learning_rate: 2.7486910312291e-06
Step: 39710, train/epoch: 9.450262069702148
Step: 39720, train/loss: 0.0
Step: 39720, train/grad_norm: 1.8570844417808985e-08
Step: 39720, train/learning_rate: 2.736792112045805e-06
Step: 39720, train/epoch: 9.452641487121582
Step: 39730, train/loss: 0.0
Step: 39730, train/grad_norm: 2.1675642614127355e-08
Step: 39730, train/learning_rate: 2.7248929654888343e-06
Step: 39730, train/epoch: 9.455021858215332
Step: 39740, train/loss: 0.0
Step: 39740, train/grad_norm: 3.180427865512314e-10
Step: 39740, train/learning_rate: 2.7129938189318636e-06
Step: 39740, train/epoch: 9.457401275634766
Step: 39750, train/loss: 0.0
Step: 39750, train/grad_norm: 2.308072621914903e-09
Step: 39750, train/learning_rate: 2.701094672374893e-06
Step: 39750, train/epoch: 9.4597806930542
Step: 39760, train/loss: 0.0
Step: 39760, train/grad_norm: 1.4528463154750426e-10
Step: 39760, train/learning_rate: 2.6891955258179223e-06
Step: 39760, train/epoch: 9.46216106414795
Step: 39770, train/loss: 0.0
Step: 39770, train/grad_norm: 7.379814981334221e-09
Step: 39770, train/learning_rate: 2.677296606634627e-06
Step: 39770, train/epoch: 9.464540481567383
Step: 39780, train/loss: 0.0
Step: 39780, train/grad_norm: 3.569671780390138e-10
Step: 39780, train/learning_rate: 2.6653974600776564e-06
Step: 39780, train/epoch: 9.466920852661133
Step: 39790, train/loss: 0.0
Step: 39790, train/grad_norm: 1.4409093296308129e-07
Step: 39790, train/learning_rate: 2.6534983135206858e-06
Step: 39790, train/epoch: 9.469300270080566
Step: 39800, train/loss: 0.0
Step: 39800, train/grad_norm: 4.3248633840864414e-11
Step: 39800, train/learning_rate: 2.641599166963715e-06
Step: 39800, train/epoch: 9.4716796875
Step: 39810, train/loss: 0.0
Step: 39810, train/grad_norm: 2.4609247972762205e-10
Step: 39810, train/learning_rate: 2.62970024778042e-06
Step: 39810, train/epoch: 9.47406005859375
Step: 39820, train/loss: 0.0
Step: 39820, train/grad_norm: 1.3122594122449982e-09
Step: 39820, train/learning_rate: 2.6178011012234492e-06
Step: 39820, train/epoch: 9.476439476013184
Step: 39830, train/loss: 0.0
Step: 39830, train/grad_norm: 1.4483074295412735e-08
Step: 39830, train/learning_rate: 2.6059019546664786e-06
Step: 39830, train/epoch: 9.478819847106934
Step: 39840, train/loss: 0.0
Step: 39840, train/grad_norm: 1.922255066233447e-08
Step: 39840, train/learning_rate: 2.594002808109508e-06
Step: 39840, train/epoch: 9.481199264526367
Step: 39850, train/loss: 0.0
Step: 39850, train/grad_norm: 3.6142588921705965e-09
Step: 39850, train/learning_rate: 2.5821036615525372e-06
Step: 39850, train/epoch: 9.483579635620117
Step: 39860, train/loss: 0.0
Step: 39860, train/grad_norm: 7.992170480974892e-09
Step: 39860, train/learning_rate: 2.570204742369242e-06
Step: 39860, train/epoch: 9.48595905303955
Step: 39870, train/loss: 0.0
Step: 39870, train/grad_norm: 3.20166537726152e-09
Step: 39870, train/learning_rate: 2.5583055958122713e-06
Step: 39870, train/epoch: 9.488338470458984
Step: 39880, train/loss: 0.0
Step: 39880, train/grad_norm: 6.326570517467545e-11
Step: 39880, train/learning_rate: 2.5464064492553007e-06
Step: 39880, train/epoch: 9.490718841552734
Step: 39890, train/loss: 0.0
Step: 39890, train/grad_norm: 1.9880300072827595e-08
Step: 39890, train/learning_rate: 2.53450730269833e-06
Step: 39890, train/epoch: 9.493098258972168
Step: 39900, train/loss: 0.0
Step: 39900, train/grad_norm: 2.0467769701326688e-08
Step: 39900, train/learning_rate: 2.522608383515035e-06
Step: 39900, train/epoch: 9.495478630065918
Step: 39910, train/loss: 0.0
Step: 39910, train/grad_norm: 2.7715996164801027e-09
Step: 39910, train/learning_rate: 2.510709236958064e-06
Step: 39910, train/epoch: 9.497858047485352
Step: 39920, train/loss: 0.0
Step: 39920, train/grad_norm: 1.916660075096388e-08
Step: 39920, train/learning_rate: 2.4988100904010935e-06
Step: 39920, train/epoch: 9.500238418579102
Step: 39930, train/loss: 0.0
Step: 39930, train/grad_norm: 9.510071841134504e-09
Step: 39930, train/learning_rate: 2.486910943844123e-06
Step: 39930, train/epoch: 9.502617835998535
Step: 39940, train/loss: 0.0
Step: 39940, train/grad_norm: 3.534190717857655e-10
Step: 39940, train/learning_rate: 2.475011797287152e-06
Step: 39940, train/epoch: 9.504997253417969
Step: 39950, train/loss: 0.0
Step: 39950, train/grad_norm: 1.7340284763633917e-10
Step: 39950, train/learning_rate: 2.463112878103857e-06
Step: 39950, train/epoch: 9.507377624511719
Step: 39960, train/loss: 0.0
Step: 39960, train/grad_norm: 4.1199172962080866e-09
Step: 39960, train/learning_rate: 2.4512137315468863e-06
Step: 39960, train/epoch: 9.509757041931152
Step: 39970, train/loss: 0.0
Step: 39970, train/grad_norm: 1.359369977693703e-10
Step: 39970, train/learning_rate: 2.4393145849899156e-06
Step: 39970, train/epoch: 9.512137413024902
Step: 39980, train/loss: 0.0
Step: 39980, train/grad_norm: 2.881749727734473e-09
Step: 39980, train/learning_rate: 2.427415438432945e-06
Step: 39980, train/epoch: 9.514516830444336
Step: 39990, train/loss: 0.0
Step: 39990, train/grad_norm: 9.551727186973835e-10
Step: 39990, train/learning_rate: 2.4155165192496497e-06
Step: 39990, train/epoch: 9.51689624786377
Step: 40000, train/loss: 0.0
Step: 40000, train/grad_norm: 1.0588600309058283e-11
Step: 40000, train/learning_rate: 2.403617372692679e-06
Step: 40000, train/epoch: 9.51927661895752
Step: 40010, train/loss: 0.0
Step: 40010, train/grad_norm: 3.1995043059396266e-08
Step: 40010, train/learning_rate: 2.3917182261357084e-06
Step: 40010, train/epoch: 9.521656036376953
Step: 40020, train/loss: 0.0
Step: 40020, train/grad_norm: 7.88222542791317e-10
Step: 40020, train/learning_rate: 2.3798190795787377e-06
Step: 40020, train/epoch: 9.524036407470703
Step: 40030, train/loss: 9.999999747378752e-05
Step: 40030, train/grad_norm: 1.3306968860149482e-08
Step: 40030, train/learning_rate: 2.367919933021767e-06
Step: 40030, train/epoch: 9.526415824890137
Step: 40040, train/loss: 0.0
Step: 40040, train/grad_norm: 1.8665399466044619e-06
Step: 40040, train/learning_rate: 2.356021013838472e-06
Step: 40040, train/epoch: 9.528796195983887
Step: 40050, train/loss: 0.0
Step: 40050, train/grad_norm: 1.088139285343459e-07
Step: 40050, train/learning_rate: 2.344121867281501e-06
Step: 40050, train/epoch: 9.53117561340332
Step: 40060, train/loss: 0.0
Step: 40060, train/grad_norm: 5.86226400756118e-09
Step: 40060, train/learning_rate: 2.3322227207245305e-06
Step: 40060, train/epoch: 9.533555030822754
Step: 40070, train/loss: 0.0
Step: 40070, train/grad_norm: 6.85980827697108e-10
Step: 40070, train/learning_rate: 2.32032357416756e-06
Step: 40070, train/epoch: 9.535935401916504
Step: 40080, train/loss: 0.0
Step: 40080, train/grad_norm: 2.4724844567558524e-12
Step: 40080, train/learning_rate: 2.3084246549842646e-06
Step: 40080, train/epoch: 9.538314819335938
Step: 40090, train/loss: 0.0
Step: 40090, train/grad_norm: 3.1266805117224905e-11
Step: 40090, train/learning_rate: 2.296525508427294e-06
Step: 40090, train/epoch: 9.540695190429688
Step: 40100, train/loss: 0.0
Step: 40100, train/grad_norm: 7.629782361462389e-10
Step: 40100, train/learning_rate: 2.2846263618703233e-06
Step: 40100, train/epoch: 9.543074607849121
Step: 40110, train/loss: 0.0
Step: 40110, train/grad_norm: 5.455312850760619e-13
Step: 40110, train/learning_rate: 2.2727272153133526e-06
Step: 40110, train/epoch: 9.545454978942871
Step: 40120, train/loss: 0.0
Step: 40120, train/grad_norm: 2.6709279232761673e-10
Step: 40120, train/learning_rate: 2.260828068756382e-06
Step: 40120, train/epoch: 9.547834396362305
Step: 40130, train/loss: 0.0
Step: 40130, train/grad_norm: 3.213711588512247e-11
Step: 40130, train/learning_rate: 2.2489291495730868e-06
Step: 40130, train/epoch: 9.550213813781738
Step: 40140, train/loss: 0.0
Step: 40140, train/grad_norm: 3.932015157914748e-09
Step: 40140, train/learning_rate: 2.237030003016116e-06
Step: 40140, train/epoch: 9.552594184875488
Step: 40150, train/loss: 0.0
Step: 40150, train/grad_norm: 1.303558372356406e-09
Step: 40150, train/learning_rate: 2.2251308564591454e-06
Step: 40150, train/epoch: 9.554973602294922
Step: 40160, train/loss: 0.0
Step: 40160, train/grad_norm: 1.2462599841001065e-07
Step: 40160, train/learning_rate: 2.2132317099021748e-06
Step: 40160, train/epoch: 9.557353973388672
Step: 40170, train/loss: 0.0
Step: 40170, train/grad_norm: 4.0265867085054197e-08
Step: 40170, train/learning_rate: 2.2013327907188796e-06
Step: 40170, train/epoch: 9.559733390808105
Step: 40180, train/loss: 0.0
Step: 40180, train/grad_norm: 6.682951081415922e-09
Step: 40180, train/learning_rate: 2.189433644161909e-06
Step: 40180, train/epoch: 9.562112808227539
Step: 40190, train/loss: 0.0
Step: 40190, train/grad_norm: 6.559802007144455e-11
Step: 40190, train/learning_rate: 2.1775344976049382e-06
Step: 40190, train/epoch: 9.564493179321289
Step: 40200, train/loss: 0.0
Step: 40200, train/grad_norm: 3.4541139393162723e-10
Step: 40200, train/learning_rate: 2.1656353510479676e-06
Step: 40200, train/epoch: 9.566872596740723
Step: 40210, train/loss: 0.0
Step: 40210, train/grad_norm: 7.703142568260546e-10
Step: 40210, train/learning_rate: 2.153736204490997e-06
Step: 40210, train/epoch: 9.569252967834473
Step: 40220, train/loss: 0.0
Step: 40220, train/grad_norm: 1.8492004528525285e-05
Step: 40220, train/learning_rate: 2.1418372853077017e-06
Step: 40220, train/epoch: 9.571632385253906
Step: 40230, train/loss: 0.0
Step: 40230, train/grad_norm: 1.0861823246388624e-10
Step: 40230, train/learning_rate: 2.129938138750731e-06
Step: 40230, train/epoch: 9.574012756347656
Step: 40240, train/loss: 0.0
Step: 40240, train/grad_norm: 4.962310510420176e-12
Step: 40240, train/learning_rate: 2.1180389921937604e-06
Step: 40240, train/epoch: 9.57639217376709
Step: 40250, train/loss: 0.0
Step: 40250, train/grad_norm: 3.17573523034298e-10
Step: 40250, train/learning_rate: 2.1061398456367897e-06
Step: 40250, train/epoch: 9.578771591186523
Step: 40260, train/loss: 0.0
Step: 40260, train/grad_norm: 1.0483960921625624e-11
Step: 40260, train/learning_rate: 2.0942409264534945e-06
Step: 40260, train/epoch: 9.581151962280273
Step: 40270, train/loss: 0.0
Step: 40270, train/grad_norm: 1.893768164507037e-08
Step: 40270, train/learning_rate: 2.082341779896524e-06
Step: 40270, train/epoch: 9.583531379699707
Step: 40280, train/loss: 0.0
Step: 40280, train/grad_norm: 2.1326846422198287e-07
Step: 40280, train/learning_rate: 2.070442633339553e-06
Step: 40280, train/epoch: 9.585911750793457
Step: 40290, train/loss: 0.0
Step: 40290, train/grad_norm: 6.5405311375221764e-12
Step: 40290, train/learning_rate: 2.0585434867825825e-06
Step: 40290, train/epoch: 9.58829116821289
Step: 40300, train/loss: 0.0
Step: 40300, train/grad_norm: 2.4452953550913037e-11
Step: 40300, train/learning_rate: 2.0466445675992873e-06
Step: 40300, train/epoch: 9.59067153930664
Step: 40310, train/loss: 0.0
Step: 40310, train/grad_norm: 1.0925885085555365e-08
Step: 40310, train/learning_rate: 2.0347454210423166e-06
Step: 40310, train/epoch: 9.593050956726074
Step: 40320, train/loss: 0.0
Step: 40320, train/grad_norm: 8.511599758342925e-11
Step: 40320, train/learning_rate: 2.022846274485346e-06
Step: 40320, train/epoch: 9.595430374145508
Step: 40330, train/loss: 0.0
Step: 40330, train/grad_norm: 1.0789130006072511e-10
Step: 40330, train/learning_rate: 2.0109471279283753e-06
Step: 40330, train/epoch: 9.597810745239258
Step: 40340, train/loss: 0.0
Step: 40340, train/grad_norm: 2.0339036010952327e-11
Step: 40340, train/learning_rate: 1.9990479813714046e-06
Step: 40340, train/epoch: 9.600190162658691
Step: 40350, train/loss: 0.0
Step: 40350, train/grad_norm: 3.831173156498835e-10
Step: 40350, train/learning_rate: 1.9871490621881094e-06
Step: 40350, train/epoch: 9.602570533752441
Step: 40360, train/loss: 0.0
Step: 40360, train/grad_norm: 2.226210327194167e-10
Step: 40360, train/learning_rate: 1.9752499156311387e-06
Step: 40360, train/epoch: 9.604949951171875
Step: 40370, train/loss: 0.0
Step: 40370, train/grad_norm: 3.806749138135501e-09
Step: 40370, train/learning_rate: 1.963350769074168e-06
Step: 40370, train/epoch: 9.607329368591309
Step: 40380, train/loss: 0.0
Step: 40380, train/grad_norm: 8.291137221227984e-11
Step: 40380, train/learning_rate: 1.9514516225171974e-06
Step: 40380, train/epoch: 9.609709739685059
Step: 40390, train/loss: 0.0
Step: 40390, train/grad_norm: 1.425406570554344e-09
Step: 40390, train/learning_rate: 1.939552703333902e-06
Step: 40390, train/epoch: 9.612089157104492
Step: 40400, train/loss: 0.0
Step: 40400, train/grad_norm: 9.273962153777404e-11
Step: 40400, train/learning_rate: 1.9276535567769315e-06
Step: 40400, train/epoch: 9.614469528198242
Step: 40410, train/loss: 0.0
Step: 40410, train/grad_norm: 1.778187180834223e-10
Step: 40410, train/learning_rate: 1.915754410219961e-06
Step: 40410, train/epoch: 9.616848945617676
Step: 40420, train/loss: 0.0
Step: 40420, train/grad_norm: 3.598703279816817e-11
Step: 40420, train/learning_rate: 1.9038552636629902e-06
Step: 40420, train/epoch: 9.619229316711426
Step: 40430, train/loss: 0.0
Step: 40430, train/grad_norm: 7.212153617375261e-12
Step: 40430, train/learning_rate: 1.8919562307928572e-06
Step: 40430, train/epoch: 9.62160873413086
Step: 40440, train/loss: 0.0
Step: 40440, train/grad_norm: 1.0831174845904457e-10
Step: 40440, train/learning_rate: 1.8800570842358866e-06
Step: 40440, train/epoch: 9.623988151550293
Step: 40450, train/loss: 0.0
Step: 40450, train/grad_norm: 2.959816419734196e-11
Step: 40450, train/learning_rate: 1.8681580513657536e-06
Step: 40450, train/epoch: 9.626368522644043
Step: 40460, train/loss: 0.0
Step: 40460, train/grad_norm: 8.20200966700213e-07
Step: 40460, train/learning_rate: 1.856258904808783e-06
Step: 40460, train/epoch: 9.628747940063477
Step: 40470, train/loss: 0.0
Step: 40470, train/grad_norm: 9.470371153952328e-09
Step: 40470, train/learning_rate: 1.84435987193865e-06
Step: 40470, train/epoch: 9.631128311157227
Step: 40480, train/loss: 0.0
Step: 40480, train/grad_norm: 1.822743865564913e-11
Step: 40480, train/learning_rate: 1.8324607253816794e-06
Step: 40480, train/epoch: 9.63350772857666
Step: 40490, train/loss: 0.0
Step: 40490, train/grad_norm: 8.542359708485492e-09
Step: 40490, train/learning_rate: 1.8205616925115464e-06
Step: 40490, train/epoch: 9.63588809967041
Step: 40500, train/loss: 0.0
Step: 40500, train/grad_norm: 3.243326496260579e-09
Step: 40500, train/learning_rate: 1.8086625459545758e-06
Step: 40500, train/epoch: 9.638267517089844
Step: 40510, train/loss: 0.0
Step: 40510, train/grad_norm: 2.1942357264492784e-08
Step: 40510, train/learning_rate: 1.796763399397605e-06
Step: 40510, train/epoch: 9.640646934509277
Step: 40520, train/loss: 0.0
Step: 40520, train/grad_norm: 3.415328729463457e-13
Step: 40520, train/learning_rate: 1.7848643665274722e-06
Step: 40520, train/epoch: 9.643027305603027
Step: 40530, train/loss: 0.0
Step: 40530, train/grad_norm: 7.008317481904669e-08
Step: 40530, train/learning_rate: 1.7729652199705015e-06
Step: 40530, train/epoch: 9.645406723022461
Step: 40540, train/loss: 0.0
Step: 40540, train/grad_norm: 2.1819435147563127e-09
Step: 40540, train/learning_rate: 1.7610661871003686e-06
Step: 40540, train/epoch: 9.647787094116211
Step: 40550, train/loss: 0.0
Step: 40550, train/grad_norm: 1.9362086334240303e-07
Step: 40550, train/learning_rate: 1.7491670405433979e-06
Step: 40550, train/epoch: 9.650166511535645
Step: 40560, train/loss: 0.0
Step: 40560, train/grad_norm: 8.294350664073136e-07
Step: 40560, train/learning_rate: 1.737268007673265e-06
Step: 40560, train/epoch: 9.652546882629395
Step: 40570, train/loss: 0.0
Step: 40570, train/grad_norm: 4.56474701787668e-12
Step: 40570, train/learning_rate: 1.7253688611162943e-06
Step: 40570, train/epoch: 9.654926300048828
Step: 40580, train/loss: 0.0
Step: 40580, train/grad_norm: 2.2047855208207068e-10
Step: 40580, train/learning_rate: 1.7134698282461613e-06
Step: 40580, train/epoch: 9.657305717468262
Step: 40590, train/loss: 0.0
Step: 40590, train/grad_norm: 9.360776043187968e-13
Step: 40590, train/learning_rate: 1.7015706816891907e-06
Step: 40590, train/epoch: 9.659686088562012
Step: 40600, train/loss: 0.0
Step: 40600, train/grad_norm: 6.500040089507664e-11
Step: 40600, train/learning_rate: 1.68967153513222e-06
Step: 40600, train/epoch: 9.662065505981445
Step: 40610, train/loss: 0.0
Step: 40610, train/grad_norm: 2.1699779750861126e-09
Step: 40610, train/learning_rate: 1.677772502262087e-06
Step: 40610, train/epoch: 9.664445877075195
Step: 40620, train/loss: 0.0
Step: 40620, train/grad_norm: 2.5559492655702343e-07
Step: 40620, train/learning_rate: 1.6658733557051164e-06
Step: 40620, train/epoch: 9.666825294494629
Step: 40630, train/loss: 0.0
Step: 40630, train/grad_norm: 1.3654098962589956e-09
Step: 40630, train/learning_rate: 1.6539743228349835e-06
Step: 40630, train/epoch: 9.669204711914062
Step: 40640, train/loss: 0.0
Step: 40640, train/grad_norm: 5.432239658098581e-10
Step: 40640, train/learning_rate: 1.6420751762780128e-06
Step: 40640, train/epoch: 9.671585083007812
Step: 40650, train/loss: 0.0
Step: 40650, train/grad_norm: 1.620537315449866e-10
Step: 40650, train/learning_rate: 1.6301761434078799e-06
Step: 40650, train/epoch: 9.673964500427246
Step: 40660, train/loss: 0.0
Step: 40660, train/grad_norm: 2.96491199647253e-11
Step: 40660, train/learning_rate: 1.6182769968509092e-06
Step: 40660, train/epoch: 9.676344871520996
Step: 40670, train/loss: 0.0
Step: 40670, train/grad_norm: 2.2033071200855403e-10
Step: 40670, train/learning_rate: 1.6063779639807763e-06
Step: 40670, train/epoch: 9.67872428894043
Step: 40680, train/loss: 0.0
Step: 40680, train/grad_norm: 1.855999945746256e-12
Step: 40680, train/learning_rate: 1.5944788174238056e-06
Step: 40680, train/epoch: 9.68110466003418
Step: 40690, train/loss: 0.0
Step: 40690, train/grad_norm: 8.385687699785649e-10
Step: 40690, train/learning_rate: 1.582579670866835e-06
Step: 40690, train/epoch: 9.683484077453613
Step: 40700, train/loss: 0.0
Step: 40700, train/grad_norm: 4.647893736553499e-10
Step: 40700, train/learning_rate: 1.570680637996702e-06
Step: 40700, train/epoch: 9.685863494873047
Step: 40710, train/loss: 0.0
Step: 40710, train/grad_norm: 7.056609518940604e-08
Step: 40710, train/learning_rate: 1.5587814914397313e-06
Step: 40710, train/epoch: 9.688243865966797
Step: 40720, train/loss: 0.0
Step: 40720, train/grad_norm: 1.5476172426920343e-11
Step: 40720, train/learning_rate: 1.5468824585695984e-06
Step: 40720, train/epoch: 9.69062328338623
Step: 40730, train/loss: 0.0
Step: 40730, train/grad_norm: 1.8077449259745748e-10
Step: 40730, train/learning_rate: 1.5349833120126277e-06
Step: 40730, train/epoch: 9.69300365447998
Step: 40740, train/loss: 0.0
Step: 40740, train/grad_norm: 8.115290557608734e-12
Step: 40740, train/learning_rate: 1.5230842791424948e-06
Step: 40740, train/epoch: 9.695383071899414
Step: 40750, train/loss: 0.0
Step: 40750, train/grad_norm: 4.379665519138598e-09
Step: 40750, train/learning_rate: 1.5111851325855241e-06
Step: 40750, train/epoch: 9.697763442993164
Step: 40760, train/loss: 0.0
Step: 40760, train/grad_norm: 3.523304981101205e-09
Step: 40760, train/learning_rate: 1.4992860997153912e-06
Step: 40760, train/epoch: 9.700142860412598
Step: 40770, train/loss: 0.0
Step: 40770, train/grad_norm: 5.393845960155463e-12
Step: 40770, train/learning_rate: 1.4873869531584205e-06
Step: 40770, train/epoch: 9.702522277832031
Step: 40780, train/loss: 0.0
Step: 40780, train/grad_norm: 2.9507424414987327e-07
Step: 40780, train/learning_rate: 1.4754878066014498e-06
Step: 40780, train/epoch: 9.704902648925781
Step: 40790, train/loss: 0.0
Step: 40790, train/grad_norm: 1.1237757746584975e-07
Step: 40790, train/learning_rate: 1.463588773731317e-06
Step: 40790, train/epoch: 9.707282066345215
Step: 40800, train/loss: 0.0
Step: 40800, train/grad_norm: 1.8786042943474968e-08
Step: 40800, train/learning_rate: 1.4516896271743462e-06
Step: 40800, train/epoch: 9.709662437438965
Step: 40810, train/loss: 0.0
Step: 40810, train/grad_norm: 2.330630410884993e-10
Step: 40810, train/learning_rate: 1.4397905943042133e-06
Step: 40810, train/epoch: 9.712041854858398
Step: 40820, train/loss: 0.0
Step: 40820, train/grad_norm: 2.6028072058759122e-11
Step: 40820, train/learning_rate: 1.4278914477472426e-06
Step: 40820, train/epoch: 9.714421272277832
Step: 40830, train/loss: 0.0
Step: 40830, train/grad_norm: 7.247072941751753e-10
Step: 40830, train/learning_rate: 1.4159924148771097e-06
Step: 40830, train/epoch: 9.716801643371582
Step: 40840, train/loss: 0.0
Step: 40840, train/grad_norm: 2.9623947739310097e-09
Step: 40840, train/learning_rate: 1.404093268320139e-06
Step: 40840, train/epoch: 9.719181060791016
Step: 40850, train/loss: 0.0
Step: 40850, train/grad_norm: 1.1249958430104012e-11
Step: 40850, train/learning_rate: 1.392194235450006e-06
Step: 40850, train/epoch: 9.721561431884766
Step: 40860, train/loss: 0.0
Step: 40860, train/grad_norm: 2.910201524208844e-10
Step: 40860, train/learning_rate: 1.3802950888930354e-06
Step: 40860, train/epoch: 9.7239408493042
Step: 40870, train/loss: 0.0
Step: 40870, train/grad_norm: 2.656236619547059e-10
Step: 40870, train/learning_rate: 1.3683960560229025e-06
Step: 40870, train/epoch: 9.72632122039795
Step: 40880, train/loss: 0.0
Step: 40880, train/grad_norm: 1.258892212696594e-09
Step: 40880, train/learning_rate: 1.3564969094659318e-06
Step: 40880, train/epoch: 9.728700637817383
Step: 40890, train/loss: 0.0
Step: 40890, train/grad_norm: 9.663065902998369e-09
Step: 40890, train/learning_rate: 1.3445977629089612e-06
Step: 40890, train/epoch: 9.731080055236816
Step: 40900, train/loss: 0.0
Step: 40900, train/grad_norm: 2.9030558512666005e-10
Step: 40900, train/learning_rate: 1.3326987300388282e-06
Step: 40900, train/epoch: 9.733460426330566
Step: 40910, train/loss: 0.0
Step: 40910, train/grad_norm: 1.6138906183682522e-11
Step: 40910, train/learning_rate: 1.3207995834818576e-06
Step: 40910, train/epoch: 9.73583984375
Step: 40920, train/loss: 0.0
Step: 40920, train/grad_norm: 1.7844584143666964e-10
Step: 40920, train/learning_rate: 1.3089005506117246e-06
Step: 40920, train/epoch: 9.73822021484375
Step: 40930, train/loss: 0.0
Step: 40930, train/grad_norm: 7.208065255781548e-09
Step: 40930, train/learning_rate: 1.297001404054754e-06
Step: 40930, train/epoch: 9.740599632263184
Step: 40940, train/loss: 0.0
Step: 40940, train/grad_norm: 1.1080678596897542e-09
Step: 40940, train/learning_rate: 1.285102371184621e-06
Step: 40940, train/epoch: 9.742980003356934
Step: 40950, train/loss: 0.0
Step: 40950, train/grad_norm: 3.0161581565657514e-11
Step: 40950, train/learning_rate: 1.2732032246276503e-06
Step: 40950, train/epoch: 9.745359420776367
Step: 40960, train/loss: 0.0
Step: 40960, train/grad_norm: 9.05774587278696e-12
Step: 40960, train/learning_rate: 1.2613041917575174e-06
Step: 40960, train/epoch: 9.7477388381958
Step: 40970, train/loss: 0.0
Step: 40970, train/grad_norm: 9.249079280237993e-10
Step: 40970, train/learning_rate: 1.2494050452005467e-06
Step: 40970, train/epoch: 9.75011920928955
Step: 40980, train/loss: 0.0
Step: 40980, train/grad_norm: 4.736649406034132e-10
Step: 40980, train/learning_rate: 1.237505898643576e-06
Step: 40980, train/epoch: 9.752498626708984
Step: 40990, train/loss: 0.0
Step: 40990, train/grad_norm: 1.8509791033238798e-08
Step: 40990, train/learning_rate: 1.2256068657734431e-06
Step: 40990, train/epoch: 9.754878997802734
Step: 41000, train/loss: 0.0
Step: 41000, train/grad_norm: 2.904942085490969e-11
Step: 41000, train/learning_rate: 1.2137077192164725e-06
Step: 41000, train/epoch: 9.757258415222168
Step: 41010, train/loss: 0.0
Step: 41010, train/grad_norm: 1.869785749841757e-11
Step: 41010, train/learning_rate: 1.2018086863463395e-06
Step: 41010, train/epoch: 9.759637832641602
Step: 41020, train/loss: 0.0
Step: 41020, train/grad_norm: 5.246038603523573e-10
Step: 41020, train/learning_rate: 1.1899095397893689e-06
Step: 41020, train/epoch: 9.762018203735352
Step: 41030, train/loss: 0.0
Step: 41030, train/grad_norm: 2.4323448810648074e-10
Step: 41030, train/learning_rate: 1.178010506919236e-06
Step: 41030, train/epoch: 9.764397621154785
Step: 41040, train/loss: 0.0
Step: 41040, train/grad_norm: 1.1975217692472562e-11
Step: 41040, train/learning_rate: 1.1661113603622653e-06
Step: 41040, train/epoch: 9.766777992248535
Step: 41050, train/loss: 0.0
Step: 41050, train/grad_norm: 2.384706210278864e-09
Step: 41050, train/learning_rate: 1.1542123274921323e-06
Step: 41050, train/epoch: 9.769157409667969
Step: 41060, train/loss: 0.0
Step: 41060, train/grad_norm: 1.079230524392294e-10
Step: 41060, train/learning_rate: 1.1423131809351617e-06
Step: 41060, train/epoch: 9.771537780761719
Step: 41070, train/loss: 0.0
Step: 41070, train/grad_norm: 8.305097581873255e-11
Step: 41070, train/learning_rate: 1.130414034378191e-06
Step: 41070, train/epoch: 9.773917198181152
Step: 41080, train/loss: 0.0
Step: 41080, train/grad_norm: 2.022387118882918e-11
Step: 41080, train/learning_rate: 1.118515001508058e-06
Step: 41080, train/epoch: 9.776296615600586
Step: 41090, train/loss: 0.0
Step: 41090, train/grad_norm: 8.420187290469894e-12
Step: 41090, train/learning_rate: 1.1066158549510874e-06
Step: 41090, train/epoch: 9.778676986694336
Step: 41100, train/loss: 0.0
Step: 41100, train/grad_norm: 7.2598385636446494e-12
Step: 41100, train/learning_rate: 1.0947168220809544e-06
Step: 41100, train/epoch: 9.78105640411377
Step: 41110, train/loss: 0.0
Step: 41110, train/grad_norm: 2.37087172116901e-09
Step: 41110, train/learning_rate: 1.0828176755239838e-06
Step: 41110, train/epoch: 9.78343677520752
Step: 41120, train/loss: 0.0
Step: 41120, train/grad_norm: 3.9883490954295553e-10
Step: 41120, train/learning_rate: 1.0709186426538508e-06
Step: 41120, train/epoch: 9.785816192626953
Step: 41130, train/loss: 0.0
Step: 41130, train/grad_norm: 6.534163488058908e-11
Step: 41130, train/learning_rate: 1.0590194960968802e-06
Step: 41130, train/epoch: 9.788196563720703
Step: 41140, train/loss: 0.0
Step: 41140, train/grad_norm: 4.252185714648249e-09
Step: 41140, train/learning_rate: 1.0471204632267472e-06
Step: 41140, train/epoch: 9.790575981140137
Step: 41150, train/loss: 0.0
Step: 41150, train/grad_norm: 1.8363128725940037e-11
Step: 41150, train/learning_rate: 1.0352213166697766e-06
Step: 41150, train/epoch: 9.79295539855957
Step: 41160, train/loss: 0.0
Step: 41160, train/grad_norm: 4.702663172068133e-12
Step: 41160, train/learning_rate: 1.0233222837996436e-06
Step: 41160, train/epoch: 9.79533576965332
Step: 41170, train/loss: 0.0
Step: 41170, train/grad_norm: 7.863832252041902e-09
Step: 41170, train/learning_rate: 1.011423137242673e-06
Step: 41170, train/epoch: 9.797715187072754
Step: 41180, train/loss: 0.0
Step: 41180, train/grad_norm: 8.106214484382424e-12
Step: 41180, train/learning_rate: 9.995239906857023e-07
Step: 41180, train/epoch: 9.800095558166504
Step: 41190, train/loss: 0.0
Step: 41190, train/grad_norm: 4.11795337412757e-12
Step: 41190, train/learning_rate: 9.876249578155694e-07
Step: 41190, train/epoch: 9.802474975585938
Step: 41200, train/loss: 0.0
Step: 41200, train/grad_norm: 1.632772562987217e-11
Step: 41200, train/learning_rate: 9.757258112585987e-07
Step: 41200, train/epoch: 9.804854393005371
Step: 41210, train/loss: 0.0
Step: 41210, train/grad_norm: 5.9495501858464195e-09
Step: 41210, train/learning_rate: 9.638267783884658e-07
Step: 41210, train/epoch: 9.807234764099121
Step: 41220, train/loss: 0.0
Step: 41220, train/grad_norm: 3.6091554189709996e-09
Step: 41220, train/learning_rate: 9.519276318314951e-07
Step: 41220, train/epoch: 9.809614181518555
Step: 41230, train/loss: 0.0
Step: 41230, train/grad_norm: 4.739808263758138e-13
Step: 41230, train/learning_rate: 9.400285421179433e-07
Step: 41230, train/epoch: 9.811994552612305
Step: 41240, train/loss: 0.0
Step: 41240, train/grad_norm: 1.2404980376246044e-09
Step: 41240, train/learning_rate: 9.281294524043915e-07
Step: 41240, train/epoch: 9.814373970031738
Step: 41250, train/loss: 0.0
Step: 41250, train/grad_norm: 1.1532391708257705e-09
Step: 41250, train/learning_rate: 9.162303626908397e-07
Step: 41250, train/epoch: 9.816754341125488
Step: 41260, train/loss: 0.0
Step: 41260, train/grad_norm: 3.9808267793262075e-11
Step: 41260, train/learning_rate: 9.043312729772879e-07
Step: 41260, train/epoch: 9.819133758544922
Step: 41270, train/loss: 0.0
Step: 41270, train/grad_norm: 3.3608043431554435e-11
Step: 41270, train/learning_rate: 8.924321832637361e-07
Step: 41270, train/epoch: 9.821513175964355
Step: 41280, train/loss: 0.0
Step: 41280, train/grad_norm: 4.980290668754606e-07
Step: 41280, train/learning_rate: 8.805330935501843e-07
Step: 41280, train/epoch: 9.823893547058105
Step: 41290, train/loss: 0.0
Step: 41290, train/grad_norm: 8.545252179470442e-12
Step: 41290, train/learning_rate: 8.686340038366325e-07
Step: 41290, train/epoch: 9.826272964477539
Step: 41300, train/loss: 0.0
Step: 41300, train/grad_norm: 5.00257724134201e-10
Step: 41300, train/learning_rate: 8.567349141230807e-07
Step: 41300, train/epoch: 9.828653335571289
Step: 41310, train/loss: 0.0
Step: 41310, train/grad_norm: 3.7363192509527743e-13
Step: 41310, train/learning_rate: 8.4483576756611e-07
Step: 41310, train/epoch: 9.831032752990723
Step: 41320, train/loss: 0.0
Step: 41320, train/grad_norm: 1.6634737676213263e-06
Step: 41320, train/learning_rate: 8.329366778525582e-07
Step: 41320, train/epoch: 9.833413124084473
Step: 41330, train/loss: 0.0
Step: 41330, train/grad_norm: 1.386635667399716e-10
Step: 41330, train/learning_rate: 8.210375881390064e-07
Step: 41330, train/epoch: 9.835792541503906
Step: 41340, train/loss: 0.0
Step: 41340, train/grad_norm: 1.1997573941269213e-11
Step: 41340, train/learning_rate: 8.091384984254546e-07
Step: 41340, train/epoch: 9.83817195892334
Step: 41350, train/loss: 0.0
Step: 41350, train/grad_norm: 1.0496780111779458e-09
Step: 41350, train/learning_rate: 7.972394087119028e-07
Step: 41350, train/epoch: 9.84055233001709
Step: 41360, train/loss: 0.0
Step: 41360, train/grad_norm: 1.5632355132577658e-10
Step: 41360, train/learning_rate: 7.85340318998351e-07
Step: 41360, train/epoch: 9.842931747436523
Step: 41370, train/loss: 0.0
Step: 41370, train/grad_norm: 4.136949949273827e-10
Step: 41370, train/learning_rate: 7.734412292847992e-07
Step: 41370, train/epoch: 9.845312118530273
Step: 41380, train/loss: 0.0
Step: 41380, train/grad_norm: 1.3594478320833048e-10
Step: 41380, train/learning_rate: 7.615421395712474e-07
Step: 41380, train/epoch: 9.847691535949707
Step: 41390, train/loss: 0.0
Step: 41390, train/grad_norm: 2.7971105986956957e-10
Step: 41390, train/learning_rate: 7.496430498576956e-07
Step: 41390, train/epoch: 9.85007095336914
Step: 41400, train/loss: 0.0
Step: 41400, train/grad_norm: 9.783166637911123e-11
Step: 41400, train/learning_rate: 7.377439033007249e-07
Step: 41400, train/epoch: 9.85245132446289
Step: 41410, train/loss: 0.0
Step: 41410, train/grad_norm: 7.172573646130331e-11
Step: 41410, train/learning_rate: 7.258448135871731e-07
Step: 41410, train/epoch: 9.854830741882324
Step: 41420, train/loss: 0.0
Step: 41420, train/grad_norm: 2.4035623269469397e-08
Step: 41420, train/learning_rate: 7.139457238736213e-07
Step: 41420, train/epoch: 9.857211112976074
Step: 41430, train/loss: 0.0
Step: 41430, train/grad_norm: 4.068714115623706e-11
Step: 41430, train/learning_rate: 7.020466341600695e-07
Step: 41430, train/epoch: 9.859590530395508
Step: 41440, train/loss: 0.0
Step: 41440, train/grad_norm: 2.6047580758969957e-11
Step: 41440, train/learning_rate: 6.901475444465177e-07
Step: 41440, train/epoch: 9.861970901489258
Step: 41450, train/loss: 0.0
Step: 41450, train/grad_norm: 1.1298502133882948e-09
Step: 41450, train/learning_rate: 6.782484547329659e-07
Step: 41450, train/epoch: 9.864350318908691
Step: 41460, train/loss: 0.0
Step: 41460, train/grad_norm: 3.3387772191417753e-07
Step: 41460, train/learning_rate: 6.663493650194141e-07
Step: 41460, train/epoch: 9.866729736328125
Step: 41470, train/loss: 0.0
Step: 41470, train/grad_norm: 1.2030157252318485e-11
Step: 41470, train/learning_rate: 6.544502753058623e-07
Step: 41470, train/epoch: 9.869110107421875
Step: 41480, train/loss: 0.0
Step: 41480, train/grad_norm: 4.0188491423620576e-10
Step: 41480, train/learning_rate: 6.425511855923105e-07
Step: 41480, train/epoch: 9.871489524841309
Step: 41490, train/loss: 0.0
Step: 41490, train/grad_norm: 1.385021763944394e-09
Step: 41490, train/learning_rate: 6.306520958787587e-07
Step: 41490, train/epoch: 9.873869895935059
Step: 41500, train/loss: 0.0
Step: 41500, train/grad_norm: 2.443798635676231e-11
Step: 41500, train/learning_rate: 6.18752949321788e-07
Step: 41500, train/epoch: 9.876249313354492
Step: 41510, train/loss: 0.0
Step: 41510, train/grad_norm: 1.5201414779730804e-11
Step: 41510, train/learning_rate: 6.068538596082362e-07
Step: 41510, train/epoch: 9.878629684448242
Step: 41520, train/loss: 0.0
Step: 41520, train/grad_norm: 8.47177536456023e-11
Step: 41520, train/learning_rate: 5.949547698946844e-07
Step: 41520, train/epoch: 9.881009101867676
Step: 41530, train/loss: 0.0
Step: 41530, train/grad_norm: 1.6491480056557428e-11
Step: 41530, train/learning_rate: 5.830556801811326e-07
Step: 41530, train/epoch: 9.88338851928711
Step: 41540, train/loss: 0.0
Step: 41540, train/grad_norm: 4.449271680251332e-12
Step: 41540, train/learning_rate: 5.711565904675808e-07
Step: 41540, train/epoch: 9.88576889038086
Step: 41550, train/loss: 0.0
Step: 41550, train/grad_norm: 1.6244043610225134e-10
Step: 41550, train/learning_rate: 5.59257500754029e-07
Step: 41550, train/epoch: 9.888148307800293
Step: 41560, train/loss: 0.0
Step: 41560, train/grad_norm: 5.29436633422975e-11
Step: 41560, train/learning_rate: 5.473584110404772e-07
Step: 41560, train/epoch: 9.890528678894043
Step: 41570, train/loss: 0.0
Step: 41570, train/grad_norm: 2.71553640684985e-10
Step: 41570, train/learning_rate: 5.354593213269254e-07
Step: 41570, train/epoch: 9.892908096313477
Step: 41580, train/loss: 0.0
Step: 41580, train/grad_norm: 1.5554826870989302e-10
Step: 41580, train/learning_rate: 5.235602316133736e-07
Step: 41580, train/epoch: 9.89528751373291
Step: 41590, train/loss: 0.0
Step: 41590, train/grad_norm: 8.4659140808796e-11
Step: 41590, train/learning_rate: 5.116611418998218e-07
Step: 41590, train/epoch: 9.89766788482666
Step: 41600, train/loss: 0.0
Step: 41600, train/grad_norm: 2.203587833038423e-11
Step: 41600, train/learning_rate: 4.997619953428512e-07
Step: 41600, train/epoch: 9.900047302246094
Step: 41610, train/loss: 0.0
Step: 41610, train/grad_norm: 1.6740377972723763e-09
Step: 41610, train/learning_rate: 4.878629056292993e-07
Step: 41610, train/epoch: 9.902427673339844
Step: 41620, train/loss: 0.0
Step: 41620, train/grad_norm: 1.7113285788461496e-10
Step: 41620, train/learning_rate: 4.7596381591574755e-07
Step: 41620, train/epoch: 9.904807090759277
Step: 41630, train/loss: 0.0
Step: 41630, train/grad_norm: 1.0890592594403614e-10
Step: 41630, train/learning_rate: 4.6406472620219574e-07
Step: 41630, train/epoch: 9.907187461853027
Step: 41640, train/loss: 0.0
Step: 41640, train/grad_norm: 2.662958964450013e-09
Step: 41640, train/learning_rate: 4.5216563648864394e-07
Step: 41640, train/epoch: 9.909566879272461
Step: 41650, train/loss: 0.0
Step: 41650, train/grad_norm: 4.7923420787299165e-09
Step: 41650, train/learning_rate: 4.4026654677509214e-07
Step: 41650, train/epoch: 9.911946296691895
Step: 41660, train/loss: 0.0
Step: 41660, train/grad_norm: 9.722756627583706e-11
Step: 41660, train/learning_rate: 4.2836745706154034e-07
Step: 41660, train/epoch: 9.914326667785645
Step: 41670, train/loss: 0.0
Step: 41670, train/grad_norm: 4.793929489488313e-11
Step: 41670, train/learning_rate: 4.164683389262791e-07
Step: 41670, train/epoch: 9.916706085205078
Step: 41680, train/loss: 0.0
Step: 41680, train/grad_norm: 3.949247612961004e-12
Step: 41680, train/learning_rate: 4.045692492127273e-07
Step: 41680, train/epoch: 9.919086456298828
Step: 41690, train/loss: 0.0
Step: 41690, train/grad_norm: 1.4441361440020728e-09
Step: 41690, train/learning_rate: 3.926701594991755e-07
Step: 41690, train/epoch: 9.921465873718262
Step: 41700, train/loss: 0.0
Step: 41700, train/grad_norm: 2.7170186100988758e-08
Step: 41700, train/learning_rate: 3.807710697856237e-07
Step: 41700, train/epoch: 9.923846244812012
Step: 41710, train/loss: 0.0
Step: 41710, train/grad_norm: 2.595950299166816e-08
Step: 41710, train/learning_rate: 3.6887195165036246e-07
Step: 41710, train/epoch: 9.926225662231445
Step: 41720, train/loss: 0.0
Step: 41720, train/grad_norm: 7.516742436819435e-12
Step: 41720, train/learning_rate: 3.5697286193681066e-07
Step: 41720, train/epoch: 9.928605079650879
Step: 41730, train/loss: 0.0
Step: 41730, train/grad_norm: 6.137509389603224e-10
Step: 41730, train/learning_rate: 3.4507377222325886e-07
Step: 41730, train/epoch: 9.930985450744629
Step: 41740, train/loss: 0.0
Step: 41740, train/grad_norm: 1.215074405536143e-07
Step: 41740, train/learning_rate: 3.3317468250970705e-07
Step: 41740, train/epoch: 9.933364868164062
Step: 41750, train/loss: 0.0
Step: 41750, train/grad_norm: 1.5922612187080354e-11
Step: 41750, train/learning_rate: 3.2127559279615525e-07
Step: 41750, train/epoch: 9.935745239257812
Step: 41760, train/loss: 0.0
Step: 41760, train/grad_norm: 1.8715528682378135e-08
Step: 41760, train/learning_rate: 3.09376474660894e-07
Step: 41760, train/epoch: 9.938124656677246
Step: 41770, train/loss: 0.0
Step: 41770, train/grad_norm: 1.426976203866559e-09
Step: 41770, train/learning_rate: 2.974773849473422e-07
Step: 41770, train/epoch: 9.94050407409668
Step: 41780, train/loss: 0.0
Step: 41780, train/grad_norm: 1.0805759842469342e-08
Step: 41780, train/learning_rate: 2.855782952337904e-07
Step: 41780, train/epoch: 9.94288444519043
Step: 41790, train/loss: 0.0
Step: 41790, train/grad_norm: 2.3440267088448152e-11
Step: 41790, train/learning_rate: 2.736792055202386e-07
Step: 41790, train/epoch: 9.945263862609863
Step: 41800, train/loss: 0.0
Step: 41800, train/grad_norm: 1.7859669299014058e-08
Step: 41800, train/learning_rate: 2.617801158066868e-07
Step: 41800, train/epoch: 9.947644233703613
Step: 41810, train/loss: 0.0
Step: 41810, train/grad_norm: 2.8093647200244654e-11
Step: 41810, train/learning_rate: 2.498809976714256e-07
Step: 41810, train/epoch: 9.950023651123047
Step: 41820, train/loss: 0.0
Step: 41820, train/grad_norm: 3.2297597929442645e-10
Step: 41820, train/learning_rate: 2.3798190795787377e-07
Step: 41820, train/epoch: 9.952404022216797
Step: 41830, train/loss: 0.0
Step: 41830, train/grad_norm: 1.6497046784191838e-12
Step: 41830, train/learning_rate: 2.2608281824432197e-07
Step: 41830, train/epoch: 9.95478343963623
Step: 41840, train/loss: 0.0
Step: 41840, train/grad_norm: 5.411933401422431e-11
Step: 41840, train/learning_rate: 2.1418372853077017e-07
Step: 41840, train/epoch: 9.957162857055664
Step: 41850, train/loss: 0.0
Step: 41850, train/grad_norm: 5.938931124660485e-09
Step: 41850, train/learning_rate: 2.0228462460636365e-07
Step: 41850, train/epoch: 9.959543228149414
Step: 41860, train/loss: 0.0
Step: 41860, train/grad_norm: 1.4146812610249526e-08
Step: 41860, train/learning_rate: 1.9038553489281185e-07
Step: 41860, train/epoch: 9.961922645568848
Step: 41870, train/loss: 0.0
Step: 41870, train/grad_norm: 1.2065855259368874e-10
Step: 41870, train/learning_rate: 1.7848643096840533e-07
Step: 41870, train/epoch: 9.964303016662598
Step: 41880, train/loss: 0.0
Step: 41880, train/grad_norm: 8.948366353456194e-12
Step: 41880, train/learning_rate: 1.6658734125485353e-07
Step: 41880, train/epoch: 9.966682434082031
Step: 41890, train/loss: 0.0
Step: 41890, train/grad_norm: 4.034834966137879e-10
Step: 41890, train/learning_rate: 1.54688237330447e-07
Step: 41890, train/epoch: 9.969062805175781
Step: 41900, train/loss: 0.0
Step: 41900, train/grad_norm: 2.0486055074542264e-09
Step: 41900, train/learning_rate: 1.427891476168952e-07
Step: 41900, train/epoch: 9.971442222595215
Step: 41910, train/loss: 0.0
Step: 41910, train/grad_norm: 4.406921442351441e-12
Step: 41910, train/learning_rate: 1.308900579033434e-07
Step: 41910, train/epoch: 9.973821640014648
Step: 41920, train/loss: 0.0
Step: 41920, train/grad_norm: 4.273258136233693e-11
Step: 41920, train/learning_rate: 1.1899095397893689e-07
Step: 41920, train/epoch: 9.976202011108398
Step: 41930, train/loss: 0.0
Step: 41930, train/grad_norm: 2.9647200250337846e-09
Step: 41930, train/learning_rate: 1.0709186426538508e-07
Step: 41930, train/epoch: 9.978581428527832
Step: 41940, train/loss: 0.0
Step: 41940, train/grad_norm: 2.455516345811759e-10
Step: 41940, train/learning_rate: 9.519276744640592e-08
Step: 41940, train/epoch: 9.980961799621582
Step: 41950, train/loss: 0.0
Step: 41950, train/grad_norm: 1.0882440087955914e-10
Step: 41950, train/learning_rate: 8.329367062742676e-08
Step: 41950, train/epoch: 9.983341217041016
Step: 41960, train/loss: 0.0
Step: 41960, train/grad_norm: 1.011577523968299e-06
Step: 41960, train/learning_rate: 7.13945738084476e-08
Step: 41960, train/epoch: 9.98572063446045
Step: 41970, train/loss: 0.0
Step: 41970, train/grad_norm: 1.321671244669531e-10
Step: 41970, train/learning_rate: 5.949547698946844e-08
Step: 41970, train/epoch: 9.9881010055542
Step: 41980, train/loss: 0.0
Step: 41980, train/grad_norm: 2.649364660101128e-06
Step: 41980, train/learning_rate: 4.759638372320296e-08
Step: 41980, train/epoch: 9.990480422973633
Step: 41990, train/loss: 0.0
Step: 41990, train/grad_norm: 4.0470251860824646e-07
Step: 41990, train/learning_rate: 3.56972869042238e-08
Step: 41990, train/epoch: 9.992860794067383
Step: 42000, train/loss: 0.0
Step: 42000, train/grad_norm: 2.3200064092065986e-09
Step: 42000, train/learning_rate: 2.379819186160148e-08
Step: 42000, train/epoch: 9.995240211486816
Step: 42010, train/loss: 0.0
Step: 42010, train/grad_norm: 1.5106613876492148e-11
Step: 42010, train/learning_rate: 1.189909593080074e-08
Step: 42010, train/epoch: 9.997620582580566
Step: 42020, train/loss: 0.0
Step: 42020, train/grad_norm: 3.000943243902654e-11
Step: 42020, train/learning_rate: 0.0
Step: 42020, train/epoch: 10.0
Step: 42020, eval/loss: 0.026434287428855896
Step: 42020, eval/accuracy: 0.9979175329208374
Step: 42020, eval/f1: 0.9978004097938538
Step: 42020, eval/runtime: 735.6821899414062
Step: 42020, eval/samples_per_second: 9.791000366210938
Step: 42020, eval/steps_per_second: 1.225000023841858
Step: 42020, train/epoch: 10.0
Step: 42020, train/train_runtime: 24834.0703125
Step: 42020, train/train_samples_per_second: 13.534000396728516
Step: 42020, train/train_steps_per_second: 1.6920000314712524
Step: 42020, train/total_flos: 7.228304339964527e+18
Step: 42020, train/train_loss: 1.8165166693506762e-05
Step: 42020, train/epoch: 10.0
Reading events from file: ./praxis-Meta-Llama-3-8B-small-finetune/logs/events.out.tfevents.1716703540.hephaestus.3559.0
Step: 10, train/loss: 1.0338000059127808
Step: 10, train/grad_norm: 38.22437286376953
Step: 10, train/learning_rate: 4.998810254619457e-05
Step: 10, train/epoch: 0.0023798190522938967
Step: 20, train/loss: 1.6258000135421753
Step: 20, train/grad_norm: 46.61274719238281
Step: 20, train/learning_rate: 4.997620271751657e-05
Step: 20, train/epoch: 0.004759638104587793
Step: 30, train/loss: 0.7152000069618225
Step: 30, train/grad_norm: 57.222103118896484
Step: 30, train/learning_rate: 4.9964302888838574e-05
Step: 30, train/epoch: 0.007139457389712334
Step: 40, train/loss: 0.5835000276565552
Step: 40, train/grad_norm: 32.341461181640625
Step: 40, train/learning_rate: 4.995240306016058e-05
Step: 40, train/epoch: 0.009519276209175587
Step: 50, train/loss: 0.13899999856948853
Step: 50, train/grad_norm: 35.56700134277344
Step: 50, train/learning_rate: 4.994050323148258e-05
Step: 50, train/epoch: 0.011899095959961414
Step: 60, train/loss: 0.396699994802475
Step: 60, train/grad_norm: 0.39749234914779663
Step: 60, train/learning_rate: 4.992860704078339e-05
Step: 60, train/epoch: 0.014278914779424667
Step: 70, train/loss: 0.28839999437332153
Step: 70, train/grad_norm: 51.415138244628906
Step: 70, train/learning_rate: 4.9916707212105393e-05
Step: 70, train/epoch: 0.016658734530210495
Step: 80, train/loss: 0.22100000083446503
Step: 80, train/grad_norm: 0.0004488879640121013
Step: 80, train/learning_rate: 4.9904807383427396e-05
Step: 80, train/epoch: 0.019038552418351173
Step: 90, train/loss: 0.0017000000225380063
Step: 90, train/grad_norm: 5.8799738326342776e-05
Step: 90, train/learning_rate: 4.98929075547494e-05
Step: 90, train/epoch: 0.021418372169137
Step: 100, train/loss: 0.11389999836683273
Step: 100, train/grad_norm: 0.5106410980224609
Step: 100, train/learning_rate: 4.98810077260714e-05
Step: 100, train/epoch: 0.02379819191992283
Step: 110, train/loss: 0.04780000075697899
Step: 110, train/grad_norm: 0.07356052845716476
Step: 110, train/learning_rate: 4.986911153537221e-05
Step: 110, train/epoch: 0.026178009808063507
Step: 120, train/loss: 0.16060000658035278
Step: 120, train/grad_norm: 0.005677969194948673
Step: 120, train/learning_rate: 4.9857211706694216e-05
Step: 120, train/epoch: 0.028557829558849335
Step: 130, train/loss: 0.3086000084877014
Step: 130, train/grad_norm: 0.0608791820704937
Step: 130, train/learning_rate: 4.984531187801622e-05
Step: 130, train/epoch: 0.030937649309635162
Step: 140, train/loss: 0.4738999903202057
Step: 140, train/grad_norm: 31.786584854125977
Step: 140, train/learning_rate: 4.983341204933822e-05
Step: 140, train/epoch: 0.03331746906042099
Step: 150, train/loss: 0.24009999632835388
Step: 150, train/grad_norm: 40.08671569824219
Step: 150, train/learning_rate: 4.9821512220660225e-05
Step: 150, train/epoch: 0.03569728881120682
Step: 160, train/loss: 0.09939999878406525
Step: 160, train/grad_norm: 0.04649800434708595
Step: 160, train/learning_rate: 4.9809616029961035e-05
Step: 160, train/epoch: 0.03807710483670235
Step: 170, train/loss: 0.10209999978542328
Step: 170, train/grad_norm: 0.003927656449377537
Step: 170, train/learning_rate: 4.979771620128304e-05
Step: 170, train/epoch: 0.040456924587488174
Step: 180, train/loss: 0.2378000020980835
Step: 180, train/grad_norm: 85.36735534667969
Step: 180, train/learning_rate: 4.978581637260504e-05
Step: 180, train/epoch: 0.042836744338274
Step: 190, train/loss: 0.1501999944448471
Step: 190, train/grad_norm: 0.0041765132918953896
Step: 190, train/learning_rate: 4.9773916543927044e-05
Step: 190, train/epoch: 0.04521656408905983
Step: 200, train/loss: 0.0006000000284984708
Step: 200, train/grad_norm: 1.3265514098748099e-05
Step: 200, train/learning_rate: 4.976201671524905e-05
Step: 200, train/epoch: 0.04759638383984566
Step: 210, train/loss: 9.999999747378752e-05
Step: 210, train/grad_norm: 8.362947119167075e-05
Step: 210, train/learning_rate: 4.975012052454986e-05
Step: 210, train/epoch: 0.049976203590631485
Step: 220, train/loss: 0.21629999577999115
Step: 220, train/grad_norm: 0.004477555863559246
Step: 220, train/learning_rate: 4.973822069587186e-05
Step: 220, train/epoch: 0.052356019616127014
Step: 230, train/loss: 0.06270000338554382
Step: 230, train/grad_norm: 0.0025520464405417442
Step: 230, train/learning_rate: 4.972632086719386e-05
Step: 230, train/epoch: 0.05473583936691284
Step: 240, train/loss: 0.3702999949455261
Step: 240, train/grad_norm: 0.002624068409204483
Step: 240, train/learning_rate: 4.9714421038515866e-05
Step: 240, train/epoch: 0.05711565911769867
Step: 250, train/loss: 0.0005000000237487257
Step: 250, train/grad_norm: 0.6662973761558533
Step: 250, train/learning_rate: 4.970252120983787e-05
Step: 250, train/epoch: 0.0594954788684845
Step: 260, train/loss: 0.39899998903274536
Step: 260, train/grad_norm: 0.004200705327093601
Step: 260, train/learning_rate: 4.969062501913868e-05
Step: 260, train/epoch: 0.061875298619270325
Step: 270, train/loss: 0.23420000076293945
Step: 270, train/grad_norm: 0.00015776244981680065
Step: 270, train/learning_rate: 4.967872519046068e-05
Step: 270, train/epoch: 0.06425511837005615
Step: 280, train/loss: 0.0005000000237487257
Step: 280, train/grad_norm: 0.0001771159004420042
Step: 280, train/learning_rate: 4.9666825361782685e-05
Step: 280, train/epoch: 0.06663493812084198
Step: 290, train/loss: 0.11159999668598175
Step: 290, train/grad_norm: 5.340149402618408
Step: 290, train/learning_rate: 4.965492553310469e-05
Step: 290, train/epoch: 0.06901475787162781
Step: 300, train/loss: 9.999999747378752e-05
Step: 300, train/grad_norm: 0.02068246901035309
Step: 300, train/learning_rate: 4.964302570442669e-05
Step: 300, train/epoch: 0.07139457762241364
Step: 310, train/loss: 0.3515999913215637
Step: 310, train/grad_norm: 0.00017351334099657834
Step: 310, train/learning_rate: 4.96311295137275e-05
Step: 310, train/epoch: 0.07377438992261887
Step: 320, train/loss: 0.0010999999940395355
Step: 320, train/grad_norm: 0.0555928535759449
Step: 320, train/learning_rate: 4.9619229685049504e-05
Step: 320, train/epoch: 0.0761542096734047
Step: 330, train/loss: 0.16410000622272491
Step: 330, train/grad_norm: 2.5046905648196116e-05
Step: 330, train/learning_rate: 4.960732985637151e-05
Step: 330, train/epoch: 0.07853402942419052
Step: 340, train/loss: 0.23469999432563782
Step: 340, train/grad_norm: 0.0008498348179273307
Step: 340, train/learning_rate: 4.959543002769351e-05
Step: 340, train/epoch: 0.08091384917497635
Step: 350, train/loss: 0.3073999881744385
Step: 350, train/grad_norm: 0.002391129033640027
Step: 350, train/learning_rate: 4.958353019901551e-05
Step: 350, train/epoch: 0.08329366892576218
Step: 360, train/loss: 0.06459999829530716
Step: 360, train/grad_norm: 0.0008358280174434185
Step: 360, train/learning_rate: 4.957163400831632e-05
Step: 360, train/epoch: 0.085673488676548
Step: 370, train/loss: 0.000699999975040555
Step: 370, train/grad_norm: 0.17000553011894226
Step: 370, train/learning_rate: 4.9559734179638326e-05
Step: 370, train/epoch: 0.08805330842733383
Step: 380, train/loss: 0.2994999885559082
Step: 380, train/grad_norm: 7.80609130859375
Step: 380, train/learning_rate: 4.954783435096033e-05
Step: 380, train/epoch: 0.09043312817811966
Step: 390, train/loss: 0.3598000109195709
Step: 390, train/grad_norm: 65.19515228271484
Step: 390, train/learning_rate: 4.953593452228233e-05
Step: 390, train/epoch: 0.09281294792890549
Step: 400, train/loss: 0.17659999430179596
Step: 400, train/grad_norm: 0.10783077776432037
Step: 400, train/learning_rate: 4.9524034693604335e-05
Step: 400, train/epoch: 0.09519276767969131
Step: 410, train/loss: 0.15070000290870667
Step: 410, train/grad_norm: 0.00828490685671568
Step: 410, train/learning_rate: 4.9512138502905145e-05
Step: 410, train/epoch: 0.09757258743047714
Step: 420, train/loss: 0.0
Step: 420, train/grad_norm: 6.277814827626571e-05
Step: 420, train/learning_rate: 4.950023867422715e-05
Step: 420, train/epoch: 0.09995240718126297
Step: 430, train/loss: 0.026399999856948853
Step: 430, train/grad_norm: 0.00019275833619758487
Step: 430, train/learning_rate: 4.948833884554915e-05
Step: 430, train/epoch: 0.1023322194814682
Step: 440, train/loss: 0.17069999873638153
Step: 440, train/grad_norm: 6.695956108160317e-05
Step: 440, train/learning_rate: 4.9476439016871154e-05
Step: 440, train/epoch: 0.10471203923225403
Step: 450, train/loss: 0.0
Step: 450, train/grad_norm: 0.0034415978007018566
Step: 450, train/learning_rate: 4.946453918819316e-05
Step: 450, train/epoch: 0.10709185898303986
Step: 460, train/loss: 0.01360000018030405
Step: 460, train/grad_norm: 0.0009242277592420578
Step: 460, train/learning_rate: 4.945264299749397e-05
Step: 460, train/epoch: 0.10947167873382568
Step: 470, train/loss: 0.057100001722574234
Step: 470, train/grad_norm: 0.022987673059105873
Step: 470, train/learning_rate: 4.944074316881597e-05
Step: 470, train/epoch: 0.11185149848461151
Step: 480, train/loss: 0.00019999999494757503
Step: 480, train/grad_norm: 8.177087693184149e-06
Step: 480, train/learning_rate: 4.9428843340137973e-05
Step: 480, train/epoch: 0.11423131823539734
Step: 490, train/loss: 0.21979999542236328
Step: 490, train/grad_norm: 0.004896396771073341
Step: 490, train/learning_rate: 4.9416943511459976e-05
Step: 490, train/epoch: 0.11661113798618317
Step: 500, train/loss: 9.999999747378752e-05
Step: 500, train/grad_norm: 0.0014712187694385648
Step: 500, train/learning_rate: 4.940504368278198e-05
Step: 500, train/epoch: 0.118990957736969
Step: 510, train/loss: 0.11580000072717667
Step: 510, train/grad_norm: 0.003260350087657571
Step: 510, train/learning_rate: 4.939314749208279e-05
Step: 510, train/epoch: 0.12137077748775482
Step: 520, train/loss: 0.0
Step: 520, train/grad_norm: 0.0001600875548319891
Step: 520, train/learning_rate: 4.938124766340479e-05
Step: 520, train/epoch: 0.12375059723854065
Step: 530, train/loss: 0.01759999990463257
Step: 530, train/grad_norm: 0.7880423665046692
Step: 530, train/learning_rate: 4.9369347834726796e-05
Step: 530, train/epoch: 0.12613041698932648
Step: 540, train/loss: 0.0
Step: 540, train/grad_norm: 0.00017238134751096368
Step: 540, train/learning_rate: 4.93574480060488e-05
Step: 540, train/epoch: 0.1285102367401123
Step: 550, train/loss: 0.00019999999494757503
Step: 550, train/grad_norm: 3.975823710788973e-05
Step: 550, train/learning_rate: 4.93455481773708e-05
Step: 550, train/epoch: 0.13089005649089813
Step: 560, train/loss: 0.16869999468326569
Step: 560, train/grad_norm: 0.11354180425405502
Step: 560, train/learning_rate: 4.933365198667161e-05
Step: 560, train/epoch: 0.13326987624168396
Step: 570, train/loss: 0.0035000001080334187
Step: 570, train/grad_norm: 40.46937561035156
Step: 570, train/learning_rate: 4.9321752157993615e-05
Step: 570, train/epoch: 0.1356496959924698
Step: 580, train/loss: 0.00019999999494757503
Step: 580, train/grad_norm: 0.0004364993656054139
Step: 580, train/learning_rate: 4.930985232931562e-05
Step: 580, train/epoch: 0.13802951574325562
Step: 590, train/loss: 0.19300000369548798
Step: 590, train/grad_norm: 9.772404882824048e-05
Step: 590, train/learning_rate: 4.929795250063762e-05
Step: 590, train/epoch: 0.14040933549404144
Step: 600, train/loss: 0.13680000603199005
Step: 600, train/grad_norm: 0.04850132390856743
Step: 600, train/learning_rate: 4.9286052671959624e-05
Step: 600, train/epoch: 0.14278915524482727
Step: 610, train/loss: 0.0017999999690800905
Step: 610, train/grad_norm: 0.018455862998962402
Step: 610, train/learning_rate: 4.9274156481260434e-05
Step: 610, train/epoch: 0.1451689600944519
Step: 620, train/loss: 0.22849999368190765
Step: 620, train/grad_norm: 0.016807975247502327
Step: 620, train/learning_rate: 4.926225665258244e-05
Step: 620, train/epoch: 0.14754877984523773
Step: 630, train/loss: 9.999999747378752e-05
Step: 630, train/grad_norm: 0.0006593966390937567
Step: 630, train/learning_rate: 4.925035682390444e-05
Step: 630, train/epoch: 0.14992859959602356
Step: 640, train/loss: 0.00019999999494757503
Step: 640, train/grad_norm: 9.8373936907592e-07
Step: 640, train/learning_rate: 4.923845699522644e-05
Step: 640, train/epoch: 0.1523084193468094
Step: 650, train/loss: 0.0
Step: 650, train/grad_norm: 8.207038263208233e-06
Step: 650, train/learning_rate: 4.9226557166548446e-05
Step: 650, train/epoch: 0.15468823909759521
Step: 660, train/loss: 0.0
Step: 660, train/grad_norm: 0.0007826224900782108
Step: 660, train/learning_rate: 4.9214660975849256e-05
Step: 660, train/epoch: 0.15706805884838104
Step: 670, train/loss: 0.10939999669790268
Step: 670, train/grad_norm: 0.020311441272497177
Step: 670, train/learning_rate: 4.920276114717126e-05
Step: 670, train/epoch: 0.15944787859916687
Step: 680, train/loss: 0.07129999995231628
Step: 680, train/grad_norm: 0.02046533115208149
Step: 680, train/learning_rate: 4.919086131849326e-05
Step: 680, train/epoch: 0.1618276983499527
Step: 690, train/loss: 0.30959999561309814
Step: 690, train/grad_norm: 0.05770295113325119
Step: 690, train/learning_rate: 4.9178961489815265e-05
Step: 690, train/epoch: 0.16420751810073853
Step: 700, train/loss: 0.0007999999797903001
Step: 700, train/grad_norm: 0.20930936932563782
Step: 700, train/learning_rate: 4.916706166113727e-05
Step: 700, train/epoch: 0.16658733785152435
Step: 710, train/loss: 0.1565999984741211
Step: 710, train/grad_norm: 0.005231175571680069
Step: 710, train/learning_rate: 4.915516547043808e-05
Step: 710, train/epoch: 0.16896715760231018
Step: 720, train/loss: 0.004399999976158142
Step: 720, train/grad_norm: 0.01664413884282112
Step: 720, train/learning_rate: 4.914326564176008e-05
Step: 720, train/epoch: 0.171346977353096
Step: 730, train/loss: 0.0026000000070780516
Step: 730, train/grad_norm: 3.210150316590443e-05
Step: 730, train/learning_rate: 4.9131365813082084e-05
Step: 730, train/epoch: 0.17372679710388184
Step: 740, train/loss: 0.048900000751018524
Step: 740, train/grad_norm: 0.0006035245605744421
Step: 740, train/learning_rate: 4.911946598440409e-05
Step: 740, train/epoch: 0.17610661685466766
Step: 750, train/loss: 0.23880000412464142
Step: 750, train/grad_norm: 18.525575637817383
Step: 750, train/learning_rate: 4.910756615572609e-05
Step: 750, train/epoch: 0.1784864366054535
Step: 760, train/loss: 0.04690000042319298
Step: 760, train/grad_norm: 0.01524418406188488
Step: 760, train/learning_rate: 4.90956699650269e-05
Step: 760, train/epoch: 0.18086625635623932
Step: 770, train/loss: 0.0005000000237487257
Step: 770, train/grad_norm: 0.02777743525803089
Step: 770, train/learning_rate: 4.90837701363489e-05
Step: 770, train/epoch: 0.18324607610702515
Step: 780, train/loss: 0.1509999930858612
Step: 780, train/grad_norm: 0.0010902245994657278
Step: 780, train/learning_rate: 4.9071870307670906e-05
Step: 780, train/epoch: 0.18562589585781097
Step: 790, train/loss: 0.16509999334812164
Step: 790, train/grad_norm: 0.021951081231236458
Step: 790, train/learning_rate: 4.905997047899291e-05
Step: 790, train/epoch: 0.1880057156085968
Step: 800, train/loss: 0.0005000000237487257
Step: 800, train/grad_norm: 0.07506835460662842
Step: 800, train/learning_rate: 4.904807065031491e-05
Step: 800, train/epoch: 0.19038553535938263
Step: 810, train/loss: 0.010099999606609344
Step: 810, train/grad_norm: 0.0016697908286005259
Step: 810, train/learning_rate: 4.903617445961572e-05
Step: 810, train/epoch: 0.19276535511016846
Step: 820, train/loss: 0.22370000183582306
Step: 820, train/grad_norm: 2.481135606765747
Step: 820, train/learning_rate: 4.9024274630937725e-05
Step: 820, train/epoch: 0.19514517486095428
Step: 830, train/loss: 0.009800000116229057
Step: 830, train/grad_norm: 0.0006164148799143732
Step: 830, train/learning_rate: 4.901237480225973e-05
Step: 830, train/epoch: 0.1975249946117401
Step: 840, train/loss: 0.06270000338554382
Step: 840, train/grad_norm: 86.53096008300781
Step: 840, train/learning_rate: 4.900047497358173e-05
Step: 840, train/epoch: 0.19990481436252594
Step: 850, train/loss: 0.0003000000142492354
Step: 850, train/grad_norm: 0.003941051196306944
Step: 850, train/learning_rate: 4.8988575144903734e-05
Step: 850, train/epoch: 0.20228461921215057
Step: 860, train/loss: 0.00019999999494757503
Step: 860, train/grad_norm: 1.5926611423492432
Step: 860, train/learning_rate: 4.8976678954204544e-05
Step: 860, train/epoch: 0.2046644389629364
Step: 870, train/loss: 0.22300000488758087
Step: 870, train/grad_norm: 0.03316309675574303
Step: 870, train/learning_rate: 4.896477912552655e-05
Step: 870, train/epoch: 0.20704425871372223
Step: 880, train/loss: 0.07500000298023224
Step: 880, train/grad_norm: 0.0002462363336235285
Step: 880, train/learning_rate: 4.895287929684855e-05
Step: 880, train/epoch: 0.20942407846450806
Step: 890, train/loss: 0.05400000140070915
Step: 890, train/grad_norm: 0.0005800490616820753
Step: 890, train/learning_rate: 4.8940979468170553e-05
Step: 890, train/epoch: 0.21180389821529388
Step: 900, train/loss: 0.17890000343322754
Step: 900, train/grad_norm: 0.007587810512632132
Step: 900, train/learning_rate: 4.8929079639492556e-05
Step: 900, train/epoch: 0.2141837179660797
Step: 910, train/loss: 0.1306000053882599
Step: 910, train/grad_norm: 0.006369376089423895
Step: 910, train/learning_rate: 4.8917183448793367e-05
Step: 910, train/epoch: 0.21656353771686554
Step: 920, train/loss: 0.09799999743700027
Step: 920, train/grad_norm: 75.49241638183594
Step: 920, train/learning_rate: 4.890528362011537e-05
Step: 920, train/epoch: 0.21894335746765137
Step: 930, train/loss: 0.0632999986410141
Step: 930, train/grad_norm: 11.271445274353027
Step: 930, train/learning_rate: 4.889338379143737e-05
Step: 930, train/epoch: 0.2213231772184372
Step: 940, train/loss: 0.14839999377727509
Step: 940, train/grad_norm: 0.015041954815387726
Step: 940, train/learning_rate: 4.8881483962759376e-05
Step: 940, train/epoch: 0.22370299696922302
Step: 950, train/loss: 0.001500000013038516
Step: 950, train/grad_norm: 0.0005978063563816249
Step: 950, train/learning_rate: 4.886958413408138e-05
Step: 950, train/epoch: 0.22608281672000885
Step: 960, train/loss: 9.999999747378752e-05
Step: 960, train/grad_norm: 0.00012713194882962853
Step: 960, train/learning_rate: 4.885768794338219e-05
Step: 960, train/epoch: 0.22846263647079468
Step: 970, train/loss: 0.08749999850988388
Step: 970, train/grad_norm: 5.4937194363446906e-05
Step: 970, train/learning_rate: 4.884578811470419e-05
Step: 970, train/epoch: 0.2308424562215805
Step: 980, train/loss: 0.36230000853538513
Step: 980, train/grad_norm: 0.009496639482676983
Step: 980, train/learning_rate: 4.8833888286026195e-05
Step: 980, train/epoch: 0.23322227597236633
Step: 990, train/loss: 0.18950000405311584
Step: 990, train/grad_norm: 0.00022265460574999452
Step: 990, train/learning_rate: 4.88219884573482e-05
Step: 990, train/epoch: 0.23560209572315216
Step: 1000, train/loss: 0.09130000323057175
Step: 1000, train/grad_norm: 0.0018132847035303712
Step: 1000, train/learning_rate: 4.88100886286702e-05
Step: 1000, train/epoch: 0.237981915473938
Step: 1010, train/loss: 0.12720000743865967
Step: 1010, train/grad_norm: 0.8056929111480713
Step: 1010, train/learning_rate: 4.879819243797101e-05
Step: 1010, train/epoch: 0.24036173522472382
Step: 1020, train/loss: 0.0005000000237487257
Step: 1020, train/grad_norm: 0.0014120438136160374
Step: 1020, train/learning_rate: 4.8786292609293014e-05
Step: 1020, train/epoch: 0.24274155497550964
Step: 1030, train/loss: 0.18639999628067017
Step: 1030, train/grad_norm: 0.0009318003430962563
Step: 1030, train/learning_rate: 4.877439278061502e-05
Step: 1030, train/epoch: 0.24512137472629547
Step: 1040, train/loss: 0.04919999837875366
Step: 1040, train/grad_norm: 0.00021267456759233028
Step: 1040, train/learning_rate: 4.876249295193702e-05
Step: 1040, train/epoch: 0.2475011944770813
Step: 1050, train/loss: 0.00039999998989515007
Step: 1050, train/grad_norm: 0.00048031489131972194
Step: 1050, train/learning_rate: 4.875059676123783e-05
Step: 1050, train/epoch: 0.24988101422786713
Step: 1060, train/loss: 0.0
Step: 1060, train/grad_norm: 0.004194180015474558
Step: 1060, train/learning_rate: 4.873869693255983e-05
Step: 1060, train/epoch: 0.25226083397865295
Step: 1070, train/loss: 0.11590000241994858
Step: 1070, train/grad_norm: 0.0017716975416988134
Step: 1070, train/learning_rate: 4.8726797103881836e-05
Step: 1070, train/epoch: 0.2546406388282776
Step: 1080, train/loss: 0.13609999418258667
Step: 1080, train/grad_norm: 0.00011856516357511282
Step: 1080, train/learning_rate: 4.871489727520384e-05
Step: 1080, train/epoch: 0.2570204734802246
Step: 1090, train/loss: 0.0649000033736229
Step: 1090, train/grad_norm: 0.32159870862960815
Step: 1090, train/learning_rate: 4.870299744652584e-05
Step: 1090, train/epoch: 0.25940027832984924
Step: 1100, train/loss: 0.0020000000949949026
Step: 1100, train/grad_norm: 23.23936653137207
Step: 1100, train/learning_rate: 4.869110125582665e-05
Step: 1100, train/epoch: 0.26178011298179626
Step: 1110, train/loss: 0.14069999754428864
Step: 1110, train/grad_norm: 0.00011831979645648971
Step: 1110, train/learning_rate: 4.8679201427148655e-05
Step: 1110, train/epoch: 0.2641599178314209
Step: 1120, train/loss: 0.040800001472234726
Step: 1120, train/grad_norm: 0.0077147940173745155
Step: 1120, train/learning_rate: 4.866730159847066e-05
Step: 1120, train/epoch: 0.2665397524833679
Step: 1130, train/loss: 0.0006000000284984708
Step: 1130, train/grad_norm: 0.0018465485190972686
Step: 1130, train/learning_rate: 4.865540176979266e-05
Step: 1130, train/epoch: 0.26891955733299255
Step: 1140, train/loss: 0.1800999939441681
Step: 1140, train/grad_norm: 0.00015307731518987566
Step: 1140, train/learning_rate: 4.8643501941114664e-05
Step: 1140, train/epoch: 0.2712993919849396
Step: 1150, train/loss: 0.02239999920129776
Step: 1150, train/grad_norm: 8.977005927590653e-05
Step: 1150, train/learning_rate: 4.8631605750415474e-05
Step: 1150, train/epoch: 0.2736791968345642
Step: 1160, train/loss: 0.01889999955892563
Step: 1160, train/grad_norm: 4.678043842315674
Step: 1160, train/learning_rate: 4.861970592173748e-05
Step: 1160, train/epoch: 0.27605903148651123
Step: 1170, train/loss: 0.002199999988079071
Step: 1170, train/grad_norm: 15.2273530960083
Step: 1170, train/learning_rate: 4.860780609305948e-05
Step: 1170, train/epoch: 0.27843883633613586
Step: 1180, train/loss: 0.1339000016450882
Step: 1180, train/grad_norm: 4.2039446270791814e-05
Step: 1180, train/learning_rate: 4.859590626438148e-05
Step: 1180, train/epoch: 0.2808186709880829
Step: 1190, train/loss: 9.999999747378752e-05
Step: 1190, train/grad_norm: 8.43023371999152e-06
Step: 1190, train/learning_rate: 4.8584006435703486e-05
Step: 1190, train/epoch: 0.2831984758377075
Step: 1200, train/loss: 0.1476999968290329
Step: 1200, train/grad_norm: 106.4422607421875
Step: 1200, train/learning_rate: 4.8572110245004296e-05
Step: 1200, train/epoch: 0.28557831048965454
Step: 1210, train/loss: 0.0989999994635582
Step: 1210, train/grad_norm: 0.0009058061405085027
Step: 1210, train/learning_rate: 4.85602104163263e-05
Step: 1210, train/epoch: 0.2879581153392792
Step: 1220, train/loss: 0.005400000140070915
Step: 1220, train/grad_norm: 0.006234182510524988
Step: 1220, train/learning_rate: 4.85483105876483e-05
Step: 1220, train/epoch: 0.2903379201889038
Step: 1230, train/loss: 0.04960000142455101
Step: 1230, train/grad_norm: 0.020580561831593513
Step: 1230, train/learning_rate: 4.8536410758970305e-05
Step: 1230, train/epoch: 0.29271775484085083
Step: 1240, train/loss: 0.1932000070810318
Step: 1240, train/grad_norm: 33.843650817871094
Step: 1240, train/learning_rate: 4.852451093029231e-05
Step: 1240, train/epoch: 0.29509755969047546
Step: 1250, train/loss: 0.06549999862909317
Step: 1250, train/grad_norm: 0.004421189893037081
Step: 1250, train/learning_rate: 4.851261473959312e-05
Step: 1250, train/epoch: 0.2974773943424225
Step: 1260, train/loss: 0.0
Step: 1260, train/grad_norm: 0.020717542618513107
Step: 1260, train/learning_rate: 4.850071491091512e-05
Step: 1260, train/epoch: 0.2998571991920471
Step: 1270, train/loss: 0.009100000374019146
Step: 1270, train/grad_norm: 0.001958608627319336
Step: 1270, train/learning_rate: 4.8488815082237124e-05
Step: 1270, train/epoch: 0.30223703384399414
Step: 1280, train/loss: 0.1062999963760376
Step: 1280, train/grad_norm: 7.801629635650897e-07
Step: 1280, train/learning_rate: 4.847691525355913e-05
Step: 1280, train/epoch: 0.3046168386936188
Step: 1290, train/loss: 0.0
Step: 1290, train/grad_norm: 1.4295426808530465e-05
Step: 1290, train/learning_rate: 4.846501542488113e-05
Step: 1290, train/epoch: 0.3069966733455658
Step: 1300, train/loss: 0.22619999945163727
Step: 1300, train/grad_norm: 0.036730363965034485
Step: 1300, train/learning_rate: 4.845311923418194e-05
Step: 1300, train/epoch: 0.30937647819519043
Step: 1310, train/loss: 0.04259999841451645
Step: 1310, train/grad_norm: 0.001197901088744402
Step: 1310, train/learning_rate: 4.8441219405503944e-05
Step: 1310, train/epoch: 0.31175631284713745
Step: 1320, train/loss: 0.027400000020861626
Step: 1320, train/grad_norm: 0.02625091001391411
Step: 1320, train/learning_rate: 4.8429319576825947e-05
Step: 1320, train/epoch: 0.3141361176967621
Step: 1330, train/loss: 0.053199999034404755
Step: 1330, train/grad_norm: 9.99650510493666e-05
Step: 1330, train/learning_rate: 4.841741974814795e-05
Step: 1330, train/epoch: 0.3165159523487091
Step: 1340, train/loss: 9.999999747378752e-05
Step: 1340, train/grad_norm: 0.0025246383156627417
Step: 1340, train/learning_rate: 4.840551991946995e-05
Step: 1340, train/epoch: 0.31889575719833374
Step: 1350, train/loss: 0.03359999880194664
Step: 1350, train/grad_norm: 0.0004782222385983914
Step: 1350, train/learning_rate: 4.839362372877076e-05
Step: 1350, train/epoch: 0.32127559185028076
Step: 1360, train/loss: 0.0
Step: 1360, train/grad_norm: 4.730856915102777e-07
Step: 1360, train/learning_rate: 4.8381723900092766e-05
Step: 1360, train/epoch: 0.3236553966999054
Step: 1370, train/loss: 0.0
Step: 1370, train/grad_norm: 0.00020368225523270667
Step: 1370, train/learning_rate: 4.836982407141477e-05
Step: 1370, train/epoch: 0.3260352313518524
Step: 1380, train/loss: 0.0
Step: 1380, train/grad_norm: 3.881692828144878e-05
Step: 1380, train/learning_rate: 4.835792424273677e-05
Step: 1380, train/epoch: 0.32841503620147705
Step: 1390, train/loss: 0.0
Step: 1390, train/grad_norm: 0.0001811935071600601
Step: 1390, train/learning_rate: 4.8346024414058775e-05
Step: 1390, train/epoch: 0.3307948708534241
Step: 1400, train/loss: 0.03629999980330467
Step: 1400, train/grad_norm: 9.46193904383108e-06
Step: 1400, train/learning_rate: 4.8334128223359585e-05
Step: 1400, train/epoch: 0.3331746757030487
Step: 1410, train/loss: 0.1762000024318695
Step: 1410, train/grad_norm: 6.770031905034557e-05
Step: 1410, train/learning_rate: 4.832222839468159e-05
Step: 1410, train/epoch: 0.3355545103549957
Step: 1420, train/loss: 0.03060000017285347
Step: 1420, train/grad_norm: 4.293434176361188e-05
Step: 1420, train/learning_rate: 4.831032856600359e-05
Step: 1420, train/epoch: 0.33793431520462036
Step: 1430, train/loss: 0.28600001335144043
Step: 1430, train/grad_norm: 83.67232513427734
Step: 1430, train/learning_rate: 4.8298428737325594e-05
Step: 1430, train/epoch: 0.3403141498565674
Step: 1440, train/loss: 0.0017999999690800905
Step: 1440, train/grad_norm: 6.683997344225645e-05
Step: 1440, train/learning_rate: 4.82865289086476e-05
Step: 1440, train/epoch: 0.342693954706192
Step: 1450, train/loss: 0.00039999998989515007
Step: 1450, train/grad_norm: 0.000153748071170412
Step: 1450, train/learning_rate: 4.827463271794841e-05
Step: 1450, train/epoch: 0.34507375955581665
Step: 1460, train/loss: 0.14900000393390656
Step: 1460, train/grad_norm: 152.43165588378906
Step: 1460, train/learning_rate: 4.826273288927041e-05
Step: 1460, train/epoch: 0.34745359420776367
Step: 1470, train/loss: 0.2312999963760376
Step: 1470, train/grad_norm: 9.974642307497561e-05
Step: 1470, train/learning_rate: 4.825083306059241e-05
Step: 1470, train/epoch: 0.3498333990573883
Step: 1480, train/loss: 0.01979999989271164
Step: 1480, train/grad_norm: 0.004152556881308556
Step: 1480, train/learning_rate: 4.8238933231914416e-05
Step: 1480, train/epoch: 0.3522132337093353
Step: 1490, train/loss: 0.0006000000284984708
Step: 1490, train/grad_norm: 0.011748254299163818
Step: 1490, train/learning_rate: 4.822703340323642e-05
Step: 1490, train/epoch: 0.35459303855895996
Step: 1500, train/loss: 0.0003000000142492354
Step: 1500, train/grad_norm: 0.0003254773619119078
Step: 1500, train/learning_rate: 4.821513721253723e-05
Step: 1500, train/epoch: 0.356972873210907
Step: 1510, train/loss: 0.0007999999797903001
Step: 1510, train/grad_norm: 1.5482803064514883e-05
Step: 1510, train/learning_rate: 4.820323738385923e-05
Step: 1510, train/epoch: 0.3593526780605316
Step: 1520, train/loss: 0.19380000233650208
Step: 1520, train/grad_norm: 0.00014067496522329748
Step: 1520, train/learning_rate: 4.8191337555181235e-05
Step: 1520, train/epoch: 0.36173251271247864
Step: 1530, train/loss: 9.999999747378752e-05
Step: 1530, train/grad_norm: 0.0182427279651165
Step: 1530, train/learning_rate: 4.817943772650324e-05
Step: 1530, train/epoch: 0.36411231756210327
Step: 1540, train/loss: 0.04230000078678131
Step: 1540, train/grad_norm: 0.0019186623394489288
Step: 1540, train/learning_rate: 4.816753789782524e-05
Step: 1540, train/epoch: 0.3664921522140503
Step: 1550, train/loss: 0.012000000104308128
Step: 1550, train/grad_norm: 0.0003052199608646333
Step: 1550, train/learning_rate: 4.815564170712605e-05
Step: 1550, train/epoch: 0.3688719570636749
Step: 1560, train/loss: 0.0010999999940395355
Step: 1560, train/grad_norm: 4.650948540074751e-05
Step: 1560, train/learning_rate: 4.8143741878448054e-05
Step: 1560, train/epoch: 0.37125179171562195
Step: 1570, train/loss: 0.0044999998062849045
Step: 1570, train/grad_norm: 0.0003286670835223049
Step: 1570, train/learning_rate: 4.813184204977006e-05
Step: 1570, train/epoch: 0.3736315965652466
Step: 1580, train/loss: 0.0
Step: 1580, train/grad_norm: 7.763749454170465e-05
Step: 1580, train/learning_rate: 4.811994222109206e-05
Step: 1580, train/epoch: 0.3760114312171936
Step: 1590, train/loss: 0.0
Step: 1590, train/grad_norm: 0.0003523066989146173
Step: 1590, train/learning_rate: 4.810804239241406e-05
Step: 1590, train/epoch: 0.37839123606681824
Step: 1600, train/loss: 0.0
Step: 1600, train/grad_norm: 0.00013641185068991035
Step: 1600, train/learning_rate: 4.809614620171487e-05
Step: 1600, train/epoch: 0.38077107071876526
Step: 1610, train/loss: 0.07880000025033951
Step: 1610, train/grad_norm: 0.00017294692224822938
Step: 1610, train/learning_rate: 4.8084246373036876e-05
Step: 1610, train/epoch: 0.3831508755683899
Step: 1620, train/loss: 0.0005000000237487257
Step: 1620, train/grad_norm: 0.00017326562374364585
Step: 1620, train/learning_rate: 4.807234654435888e-05
Step: 1620, train/epoch: 0.3855307102203369
Step: 1630, train/loss: 0.0
Step: 1630, train/grad_norm: 0.00038625652086921036
Step: 1630, train/learning_rate: 4.806044671568088e-05
Step: 1630, train/epoch: 0.38791051506996155
Step: 1640, train/loss: 0.05090000107884407
Step: 1640, train/grad_norm: 0.002010996686294675
Step: 1640, train/learning_rate: 4.8048546887002885e-05
Step: 1640, train/epoch: 0.39029034972190857
Step: 1650, train/loss: 0.000699999975040555
Step: 1650, train/grad_norm: 4.3237909267190844e-05
Step: 1650, train/learning_rate: 4.8036650696303695e-05
Step: 1650, train/epoch: 0.3926701545715332
Step: 1660, train/loss: 0.14730000495910645
Step: 1660, train/grad_norm: 0.00023694262199569494
Step: 1660, train/learning_rate: 4.80247508676257e-05
Step: 1660, train/epoch: 0.3950499892234802
Step: 1670, train/loss: 9.999999747378752e-05
Step: 1670, train/grad_norm: 0.0005329475970938802
Step: 1670, train/learning_rate: 4.80128510389477e-05
Step: 1670, train/epoch: 0.39742979407310486
Step: 1680, train/loss: 0.0012000000569969416
Step: 1680, train/grad_norm: 8.09723132988438e-06
Step: 1680, train/learning_rate: 4.8000951210269704e-05
Step: 1680, train/epoch: 0.3998096287250519
Step: 1690, train/loss: 0.014700000174343586
Step: 1690, train/grad_norm: 0.0018231698777526617
Step: 1690, train/learning_rate: 4.798905138159171e-05
Step: 1690, train/epoch: 0.4021894335746765
Step: 1700, train/loss: 0.0
Step: 1700, train/grad_norm: 0.00032987535814754665
Step: 1700, train/learning_rate: 4.797715519089252e-05
Step: 1700, train/epoch: 0.40456923842430115
Step: 1710, train/loss: 9.999999747378752e-05
Step: 1710, train/grad_norm: 0.03576289489865303
Step: 1710, train/learning_rate: 4.796525536221452e-05
Step: 1710, train/epoch: 0.40694907307624817
Step: 1720, train/loss: 0.1088000014424324
Step: 1720, train/grad_norm: 4.2654024582589045e-05
Step: 1720, train/learning_rate: 4.7953355533536524e-05
Step: 1720, train/epoch: 0.4093288779258728
Step: 1730, train/loss: 9.999999747378752e-05
Step: 1730, train/grad_norm: 0.014136782847344875
Step: 1730, train/learning_rate: 4.7941455704858527e-05
Step: 1730, train/epoch: 0.4117087125778198
Step: 1740, train/loss: 0.0982000008225441
Step: 1740, train/grad_norm: 2.3794378648744896e-05
Step: 1740, train/learning_rate: 4.792955587618053e-05
Step: 1740, train/epoch: 0.41408851742744446
Step: 1750, train/loss: 0.002899999963119626
Step: 1750, train/grad_norm: 37.18581008911133
Step: 1750, train/learning_rate: 4.791765968548134e-05
Step: 1750, train/epoch: 0.4164683520793915
Step: 1760, train/loss: 0.014800000004470348
Step: 1760, train/grad_norm: 9.504644367552828e-06
Step: 1760, train/learning_rate: 4.790575985680334e-05
Step: 1760, train/epoch: 0.4188481569290161
Step: 1770, train/loss: 0.3781000077724457
Step: 1770, train/grad_norm: 99.29926300048828
Step: 1770, train/learning_rate: 4.7893860028125346e-05
Step: 1770, train/epoch: 0.42122799158096313
Step: 1780, train/loss: 0.0
Step: 1780, train/grad_norm: 0.00031827075872570276
Step: 1780, train/learning_rate: 4.788196019944735e-05
Step: 1780, train/epoch: 0.42360779643058777
Step: 1790, train/loss: 0.013500000350177288
Step: 1790, train/grad_norm: 0.0003878924762830138
Step: 1790, train/learning_rate: 4.787006037076935e-05
Step: 1790, train/epoch: 0.4259876310825348
Step: 1800, train/loss: 0.08640000224113464
Step: 1800, train/grad_norm: 151.38662719726562
Step: 1800, train/learning_rate: 4.785816418007016e-05
Step: 1800, train/epoch: 0.4283674359321594
Step: 1810, train/loss: 0.0
Step: 1810, train/grad_norm: 0.00015827569586690515
Step: 1810, train/learning_rate: 4.7846264351392165e-05
Step: 1810, train/epoch: 0.43074727058410645
Step: 1820, train/loss: 0.057500001043081284
Step: 1820, train/grad_norm: 0.0018691947916522622
Step: 1820, train/learning_rate: 4.783436452271417e-05
Step: 1820, train/epoch: 0.4331270754337311
Step: 1830, train/loss: 0.0
Step: 1830, train/grad_norm: 0.10643646121025085
Step: 1830, train/learning_rate: 4.782246469403617e-05
Step: 1830, train/epoch: 0.4355069100856781
Step: 1840, train/loss: 0.015300000086426735
Step: 1840, train/grad_norm: 94.76712799072266
Step: 1840, train/learning_rate: 4.7810564865358174e-05
Step: 1840, train/epoch: 0.43788671493530273
Step: 1850, train/loss: 0.05999999865889549
Step: 1850, train/grad_norm: 87.55941772460938
Step: 1850, train/learning_rate: 4.7798668674658984e-05
Step: 1850, train/epoch: 0.44026654958724976
Step: 1860, train/loss: 0.0754999965429306
Step: 1860, train/grad_norm: 0.03404739871621132
Step: 1860, train/learning_rate: 4.778676884598099e-05
Step: 1860, train/epoch: 0.4426463544368744
Step: 1870, train/loss: 0.0
Step: 1870, train/grad_norm: 0.0011969833867624402
Step: 1870, train/learning_rate: 4.777486901730299e-05
Step: 1870, train/epoch: 0.4450261890888214
Step: 1880, train/loss: 9.999999747378752e-05
Step: 1880, train/grad_norm: 0.010387585498392582
Step: 1880, train/learning_rate: 4.776296918862499e-05
Step: 1880, train/epoch: 0.44740599393844604
Step: 1890, train/loss: 0.03929999843239784
Step: 1890, train/grad_norm: 0.046799495816230774
Step: 1890, train/learning_rate: 4.7751069359946996e-05
Step: 1890, train/epoch: 0.44978582859039307
Step: 1900, train/loss: 0.008899999782443047
Step: 1900, train/grad_norm: 9.225934263668023e-06
Step: 1900, train/learning_rate: 4.7739173169247806e-05
Step: 1900, train/epoch: 0.4521656334400177
Step: 1910, train/loss: 0.0
Step: 1910, train/grad_norm: 1.7325121007161215e-05
Step: 1910, train/learning_rate: 4.772727334056981e-05
Step: 1910, train/epoch: 0.4545454680919647
Step: 1920, train/loss: 0.12269999831914902
Step: 1920, train/grad_norm: 0.00018049955542664975
Step: 1920, train/learning_rate: 4.771537351189181e-05
Step: 1920, train/epoch: 0.45692527294158936
Step: 1930, train/loss: 0.1500999927520752
Step: 1930, train/grad_norm: 0.005985594820231199
Step: 1930, train/learning_rate: 4.7703473683213815e-05
Step: 1930, train/epoch: 0.4593051075935364
Step: 1940, train/loss: 9.999999747378752e-05
Step: 1940, train/grad_norm: 0.0019989842548966408
Step: 1940, train/learning_rate: 4.769157385453582e-05
Step: 1940, train/epoch: 0.461684912443161
Step: 1950, train/loss: 0.0020000000949949026
Step: 1950, train/grad_norm: 6.83697362546809e-05
Step: 1950, train/learning_rate: 4.767967766383663e-05
Step: 1950, train/epoch: 0.46406471729278564
Step: 1960, train/loss: 0.0003000000142492354
Step: 1960, train/grad_norm: 8.465595783491153e-06
Step: 1960, train/learning_rate: 4.766777783515863e-05
Step: 1960, train/epoch: 0.46644455194473267
Step: 1970, train/loss: 9.999999747378752e-05
Step: 1970, train/grad_norm: 2.9095583158778027e-05
Step: 1970, train/learning_rate: 4.7655878006480634e-05
Step: 1970, train/epoch: 0.4688243567943573
Step: 1980, train/loss: 9.999999747378752e-05
Step: 1980, train/grad_norm: 3.968243254348636e-05
Step: 1980, train/learning_rate: 4.764397817780264e-05
Step: 1980, train/epoch: 0.4712041914463043
Step: 1990, train/loss: 0.0
Step: 1990, train/grad_norm: 0.0032546871807426214
Step: 1990, train/learning_rate: 4.763207834912464e-05
Step: 1990, train/epoch: 0.47358399629592896
Step: 2000, train/loss: 0.0
Step: 2000, train/grad_norm: 3.924324846593663e-05
Step: 2000, train/learning_rate: 4.762018215842545e-05
Step: 2000, train/epoch: 0.475963830947876
Step: 2010, train/loss: 0.22660000622272491
Step: 2010, train/grad_norm: 0.0065882583148777485
Step: 2010, train/learning_rate: 4.760828232974745e-05
Step: 2010, train/epoch: 0.4783436357975006
Step: 2020, train/loss: 0.0
Step: 2020, train/grad_norm: 0.0014971541240811348
Step: 2020, train/learning_rate: 4.7596382501069456e-05
Step: 2020, train/epoch: 0.48072347044944763
Step: 2030, train/loss: 9.999999747378752e-05
Step: 2030, train/grad_norm: 0.0003526271029841155
Step: 2030, train/learning_rate: 4.758448267239146e-05
Step: 2030, train/epoch: 0.48310327529907227
Step: 2040, train/loss: 0.0
Step: 2040, train/grad_norm: 0.0031306592281907797
Step: 2040, train/learning_rate: 4.757258284371346e-05
Step: 2040, train/epoch: 0.4854831099510193
Step: 2050, train/loss: 0.09030000120401382
Step: 2050, train/grad_norm: 0.00442312378436327
Step: 2050, train/learning_rate: 4.756068665301427e-05
Step: 2050, train/epoch: 0.4878629148006439
Step: 2060, train/loss: 9.999999747378752e-05
Step: 2060, train/grad_norm: 0.0035944932606071234
Step: 2060, train/learning_rate: 4.7548786824336275e-05
Step: 2060, train/epoch: 0.49024274945259094
Step: 2070, train/loss: 0.008200000040233135
Step: 2070, train/grad_norm: 0.011188337579369545
Step: 2070, train/learning_rate: 4.753688699565828e-05
Step: 2070, train/epoch: 0.4926225543022156
Step: 2080, train/loss: 0.19529999792575836
Step: 2080, train/grad_norm: 0.001426647650077939
Step: 2080, train/learning_rate: 4.752498716698028e-05
Step: 2080, train/epoch: 0.4950023889541626
Step: 2090, train/loss: 0.029100000858306885
Step: 2090, train/grad_norm: 71.83830261230469
Step: 2090, train/learning_rate: 4.7513087338302284e-05
Step: 2090, train/epoch: 0.49738219380378723
Step: 2100, train/loss: 0.0032999999821186066
Step: 2100, train/grad_norm: 0.0011740499176084995
Step: 2100, train/learning_rate: 4.7501191147603095e-05
Step: 2100, train/epoch: 0.49976202845573425
Step: 2110, train/loss: 0.00019999999494757503
Step: 2110, train/grad_norm: 0.008327892050147057
Step: 2110, train/learning_rate: 4.74892913189251e-05
Step: 2110, train/epoch: 0.5021418333053589
Step: 2120, train/loss: 0.3192000091075897
Step: 2120, train/grad_norm: 38.970760345458984
Step: 2120, train/learning_rate: 4.74773914902471e-05
Step: 2120, train/epoch: 0.5045216679573059
Step: 2130, train/loss: 9.999999747378752e-05
Step: 2130, train/grad_norm: 0.00011999873095192015
Step: 2130, train/learning_rate: 4.7465491661569104e-05
Step: 2130, train/epoch: 0.5069015026092529
Step: 2140, train/loss: 0.06549999862909317
Step: 2140, train/grad_norm: 6.313931407930795e-06
Step: 2140, train/learning_rate: 4.7453591832891107e-05
Step: 2140, train/epoch: 0.5092812776565552
Step: 2150, train/loss: 0.0
Step: 2150, train/grad_norm: 3.1479303288506344e-05
Step: 2150, train/learning_rate: 4.744169564219192e-05
Step: 2150, train/epoch: 0.5116611123085022
Step: 2160, train/loss: 0.00019999999494757503
Step: 2160, train/grad_norm: 4.631511910702102e-05
Step: 2160, train/learning_rate: 4.742979581351392e-05
Step: 2160, train/epoch: 0.5140409469604492
Step: 2170, train/loss: 0.048500001430511475
Step: 2170, train/grad_norm: 6.962442057556473e-06
Step: 2170, train/learning_rate: 4.741789598483592e-05
Step: 2170, train/epoch: 0.5164207816123962
Step: 2180, train/loss: 0.000699999975040555
Step: 2180, train/grad_norm: 9.301280078943819e-05
Step: 2180, train/learning_rate: 4.7405996156157926e-05
Step: 2180, train/epoch: 0.5188005566596985
Step: 2190, train/loss: 0.0
Step: 2190, train/grad_norm: 3.61853999493178e-05
Step: 2190, train/learning_rate: 4.739409632747993e-05
Step: 2190, train/epoch: 0.5211803913116455
Step: 2200, train/loss: 0.3158999979496002
Step: 2200, train/grad_norm: 0.00029873609310016036
Step: 2200, train/learning_rate: 4.738220013678074e-05
Step: 2200, train/epoch: 0.5235602259635925
Step: 2210, train/loss: 0.11500000208616257
Step: 2210, train/grad_norm: 0.0016619112575426698
Step: 2210, train/learning_rate: 4.737030030810274e-05
Step: 2210, train/epoch: 0.5259400010108948
Step: 2220, train/loss: 0.029500000178813934
Step: 2220, train/grad_norm: 0.0024491643998771906
Step: 2220, train/learning_rate: 4.7358400479424745e-05
Step: 2220, train/epoch: 0.5283198356628418
Step: 2230, train/loss: 0.04410000145435333
Step: 2230, train/grad_norm: 0.0010553982574492693
Step: 2230, train/learning_rate: 4.734650065074675e-05
Step: 2230, train/epoch: 0.5306996703147888
Step: 2240, train/loss: 0.0
Step: 2240, train/grad_norm: 0.0007532209274359047
Step: 2240, train/learning_rate: 4.733460082206875e-05
Step: 2240, train/epoch: 0.5330795049667358
Step: 2250, train/loss: 0.004399999976158142
Step: 2250, train/grad_norm: 0.010549644008278847
Step: 2250, train/learning_rate: 4.732270463136956e-05
Step: 2250, train/epoch: 0.5354592800140381
Step: 2260, train/loss: 0.13130000233650208
Step: 2260, train/grad_norm: 0.00012067341594956815
Step: 2260, train/learning_rate: 4.7310804802691564e-05
Step: 2260, train/epoch: 0.5378391146659851
Step: 2270, train/loss: 9.999999747378752e-05
Step: 2270, train/grad_norm: 0.00012927890929859132
Step: 2270, train/learning_rate: 4.729890497401357e-05
Step: 2270, train/epoch: 0.5402189493179321
Step: 2280, train/loss: 0.0005000000237487257
Step: 2280, train/grad_norm: 1.8165619621868245e-05
Step: 2280, train/learning_rate: 4.728700514533557e-05
Step: 2280, train/epoch: 0.5425987839698792
Step: 2290, train/loss: 9.999999747378752e-05
Step: 2290, train/grad_norm: 0.007672734092921019
Step: 2290, train/learning_rate: 4.727510531665757e-05
Step: 2290, train/epoch: 0.5449785590171814
Step: 2300, train/loss: 9.999999747378752e-05
Step: 2300, train/grad_norm: 3.171450589434244e-05
Step: 2300, train/learning_rate: 4.726320912595838e-05
Step: 2300, train/epoch: 0.5473583936691284
Step: 2310, train/loss: 0.0003000000142492354
Step: 2310, train/grad_norm: 5.452402547234669e-05
Step: 2310, train/learning_rate: 4.7251309297280386e-05
Step: 2310, train/epoch: 0.5497382283210754
Step: 2320, train/loss: 0.0
Step: 2320, train/grad_norm: 3.449746827755007e-06
Step: 2320, train/learning_rate: 4.723940946860239e-05
Step: 2320, train/epoch: 0.5521180629730225
Step: 2330, train/loss: 0.25780001282691956
Step: 2330, train/grad_norm: 0.00014141283463686705
Step: 2330, train/learning_rate: 4.722750963992439e-05
Step: 2330, train/epoch: 0.5544978380203247
Step: 2340, train/loss: 0.04479999840259552
Step: 2340, train/grad_norm: 0.0003719983797054738
Step: 2340, train/learning_rate: 4.7215609811246395e-05
Step: 2340, train/epoch: 0.5568776726722717
Step: 2350, train/loss: 0.23649999499320984
Step: 2350, train/grad_norm: 0.20767566561698914
Step: 2350, train/learning_rate: 4.7203713620547205e-05
Step: 2350, train/epoch: 0.5592575073242188
Step: 2360, train/loss: 0.0
Step: 2360, train/grad_norm: 0.000243256232351996
Step: 2360, train/learning_rate: 4.719181379186921e-05
Step: 2360, train/epoch: 0.5616373419761658
Step: 2370, train/loss: 0.0
Step: 2370, train/grad_norm: 0.0008071286138147116
Step: 2370, train/learning_rate: 4.717991396319121e-05
Step: 2370, train/epoch: 0.564017117023468
Step: 2380, train/loss: 0.21739999949932098
Step: 2380, train/grad_norm: 0.00011429828009568155
Step: 2380, train/learning_rate: 4.7168014134513214e-05
Step: 2380, train/epoch: 0.566396951675415
Step: 2390, train/loss: 0.0006000000284984708
Step: 2390, train/grad_norm: 0.001325570628978312
Step: 2390, train/learning_rate: 4.7156117943814024e-05
Step: 2390, train/epoch: 0.5687767863273621
Step: 2400, train/loss: 0.048700001090765
Step: 2400, train/grad_norm: 0.004808282013982534
Step: 2400, train/learning_rate: 4.714421811513603e-05
Step: 2400, train/epoch: 0.5711566209793091
Step: 2410, train/loss: 0.002099999925121665
Step: 2410, train/grad_norm: 0.0002113882073899731
Step: 2410, train/learning_rate: 4.713231828645803e-05
Step: 2410, train/epoch: 0.5735363960266113
Step: 2420, train/loss: 0.01269999984651804
Step: 2420, train/grad_norm: 0.026701143011450768
Step: 2420, train/learning_rate: 4.712041845778003e-05
Step: 2420, train/epoch: 0.5759162306785583
Step: 2430, train/loss: 0.00019999999494757503
Step: 2430, train/grad_norm: 6.456133269239217e-05
Step: 2430, train/learning_rate: 4.7108518629102036e-05
Step: 2430, train/epoch: 0.5782960653305054
Step: 2440, train/loss: 0.031300000846385956
Step: 2440, train/grad_norm: 143.78932189941406
Step: 2440, train/learning_rate: 4.7096622438402846e-05
Step: 2440, train/epoch: 0.5806758403778076
Step: 2450, train/loss: 0.07320000231266022
Step: 2450, train/grad_norm: 0.012065457180142403
Step: 2450, train/learning_rate: 4.708472260972485e-05
Step: 2450, train/epoch: 0.5830556750297546
Step: 2460, train/loss: 0.03180000185966492
Step: 2460, train/grad_norm: 0.0004042515065521002
Step: 2460, train/learning_rate: 4.707282278104685e-05
Step: 2460, train/epoch: 0.5854355096817017
Step: 2470, train/loss: 0.06069999933242798
Step: 2470, train/grad_norm: 0.005085172597318888
Step: 2470, train/learning_rate: 4.7060922952368855e-05
Step: 2470, train/epoch: 0.5878153443336487
Step: 2480, train/loss: 0.00019999999494757503
Step: 2480, train/grad_norm: 0.0013364801416173577
Step: 2480, train/learning_rate: 4.704902312369086e-05
Step: 2480, train/epoch: 0.5901951193809509
Step: 2490, train/loss: 0.0010999999940395355
Step: 2490, train/grad_norm: 0.00029227614868432283
Step: 2490, train/learning_rate: 4.703712693299167e-05
Step: 2490, train/epoch: 0.592574954032898
Step: 2500, train/loss: 0.010099999606609344
Step: 2500, train/grad_norm: 4.273853392078308e-06
Step: 2500, train/learning_rate: 4.702522710431367e-05
Step: 2500, train/epoch: 0.594954788684845
Step: 2510, train/loss: 0.03180000185966492
Step: 2510, train/grad_norm: 8.316214916703757e-07
Step: 2510, train/learning_rate: 4.7013327275635675e-05
Step: 2510, train/epoch: 0.597334623336792
Step: 2520, train/loss: 0.0007999999797903001
Step: 2520, train/grad_norm: 4.3830300455738325e-06
Step: 2520, train/learning_rate: 4.700142744695768e-05
Step: 2520, train/epoch: 0.5997143983840942
Step: 2530, train/loss: 0.0
Step: 2530, train/grad_norm: 1.046449597197352e-06
Step: 2530, train/learning_rate: 4.698952761827968e-05
Step: 2530, train/epoch: 0.6020942330360413
Step: 2540, train/loss: 0.0
Step: 2540, train/grad_norm: 6.3576480897609144e-06
Step: 2540, train/learning_rate: 4.697763142758049e-05
Step: 2540, train/epoch: 0.6044740676879883
Step: 2550, train/loss: 0.0
Step: 2550, train/grad_norm: 0.006535535678267479
Step: 2550, train/learning_rate: 4.6965731598902494e-05
Step: 2550, train/epoch: 0.6068539023399353
Step: 2560, train/loss: 0.007499999832361937
Step: 2560, train/grad_norm: 0.0005677214358001947
Step: 2560, train/learning_rate: 4.69538317702245e-05
Step: 2560, train/epoch: 0.6092336773872375
Step: 2570, train/loss: 0.0
Step: 2570, train/grad_norm: 4.099245052202605e-05
Step: 2570, train/learning_rate: 4.69419319415465e-05
Step: 2570, train/epoch: 0.6116135120391846
Step: 2580, train/loss: 0.0430000014603138
Step: 2580, train/grad_norm: 0.023414049297571182
Step: 2580, train/learning_rate: 4.69300321128685e-05
Step: 2580, train/epoch: 0.6139933466911316
Step: 2590, train/loss: 0.007400000002235174
Step: 2590, train/grad_norm: 1.0802086762851104e-05
Step: 2590, train/learning_rate: 4.691813592216931e-05
Step: 2590, train/epoch: 0.6163731813430786
Step: 2600, train/loss: 0.25
Step: 2600, train/grad_norm: 0.0008032215991988778
Step: 2600, train/learning_rate: 4.6906236093491316e-05
Step: 2600, train/epoch: 0.6187529563903809
Step: 2610, train/loss: 0.00039999998989515007
Step: 2610, train/grad_norm: 1.2567479643621482e-06
Step: 2610, train/learning_rate: 4.689433626481332e-05
Step: 2610, train/epoch: 0.6211327910423279
Step: 2620, train/loss: 0.0
Step: 2620, train/grad_norm: 1.008233357424615e-05
Step: 2620, train/learning_rate: 4.688243643613532e-05
Step: 2620, train/epoch: 0.6235126256942749
Step: 2630, train/loss: 0.2563000023365021
Step: 2630, train/grad_norm: 0.013989804312586784
Step: 2630, train/learning_rate: 4.6870536607457325e-05
Step: 2630, train/epoch: 0.6258924603462219
Step: 2640, train/loss: 0.0003000000142492354
Step: 2640, train/grad_norm: 2.308526381966658e-05
Step: 2640, train/learning_rate: 4.6858640416758135e-05
Step: 2640, train/epoch: 0.6282722353935242
Step: 2650, train/loss: 0.32510000467300415
Step: 2650, train/grad_norm: 8.8490080088377e-05
Step: 2650, train/learning_rate: 4.684674058808014e-05
Step: 2650, train/epoch: 0.6306520700454712
Step: 2660, train/loss: 0.03359999880194664
Step: 2660, train/grad_norm: 0.0010567967547103763
Step: 2660, train/learning_rate: 4.683484075940214e-05
Step: 2660, train/epoch: 0.6330319046974182
Step: 2670, train/loss: 0.003599999938160181
Step: 2670, train/grad_norm: 159.52322387695312
Step: 2670, train/learning_rate: 4.6822940930724144e-05
Step: 2670, train/epoch: 0.6354116797447205
Step: 2680, train/loss: 0.10329999774694443
Step: 2680, train/grad_norm: 0.0002465969882905483
Step: 2680, train/learning_rate: 4.681104110204615e-05
Step: 2680, train/epoch: 0.6377915143966675
Step: 2690, train/loss: 0.0
Step: 2690, train/grad_norm: 7.625218131579459e-05
Step: 2690, train/learning_rate: 4.679914491134696e-05
Step: 2690, train/epoch: 0.6401713490486145
Step: 2700, train/loss: 0.0
Step: 2700, train/grad_norm: 3.7495323340408504e-05
Step: 2700, train/learning_rate: 4.678724508266896e-05
Step: 2700, train/epoch: 0.6425511837005615
Step: 2710, train/loss: 0.0
Step: 2710, train/grad_norm: 1.925769538502209e-06
Step: 2710, train/learning_rate: 4.677534525399096e-05
Step: 2710, train/epoch: 0.6449309587478638
Step: 2720, train/loss: 0.0
Step: 2720, train/grad_norm: 8.828751015244052e-05
Step: 2720, train/learning_rate: 4.6763445425312966e-05
Step: 2720, train/epoch: 0.6473107933998108
Step: 2730, train/loss: 0.0
Step: 2730, train/grad_norm: 0.03222743049263954
Step: 2730, train/learning_rate: 4.675154559663497e-05
Step: 2730, train/epoch: 0.6496906280517578
Step: 2740, train/loss: 0.0
Step: 2740, train/grad_norm: 4.9747959565138444e-06
Step: 2740, train/learning_rate: 4.673964940593578e-05
Step: 2740, train/epoch: 0.6520704627037048
Step: 2750, train/loss: 0.16179999709129333
Step: 2750, train/grad_norm: 6.306051363935694e-05
Step: 2750, train/learning_rate: 4.672774957725778e-05
Step: 2750, train/epoch: 0.6544502377510071
Step: 2760, train/loss: 0.15080000460147858
Step: 2760, train/grad_norm: 0.0006145529914647341
Step: 2760, train/learning_rate: 4.6715849748579785e-05
Step: 2760, train/epoch: 0.6568300724029541
Step: 2770, train/loss: 0.03889999911189079
Step: 2770, train/grad_norm: 0.0004183158162049949
Step: 2770, train/learning_rate: 4.670394991990179e-05
Step: 2770, train/epoch: 0.6592099070549011
Step: 2780, train/loss: 0.01360000018030405
Step: 2780, train/grad_norm: 1.6291862266371027e-05
Step: 2780, train/learning_rate: 4.669205009122379e-05
Step: 2780, train/epoch: 0.6615897417068481
Step: 2790, train/loss: 0.06560000032186508
Step: 2790, train/grad_norm: 0.003318183356896043
Step: 2790, train/learning_rate: 4.66801539005246e-05
Step: 2790, train/epoch: 0.6639695167541504
Step: 2800, train/loss: 0.1370999962091446
Step: 2800, train/grad_norm: 69.5655746459961
Step: 2800, train/learning_rate: 4.6668254071846604e-05
Step: 2800, train/epoch: 0.6663493514060974
Step: 2810, train/loss: 0.05260000005364418
Step: 2810, train/grad_norm: 0.00501981470733881
Step: 2810, train/learning_rate: 4.665635424316861e-05
Step: 2810, train/epoch: 0.6687291860580444
Step: 2820, train/loss: 0.03869999945163727
Step: 2820, train/grad_norm: 0.5095908641815186
Step: 2820, train/learning_rate: 4.664445441449061e-05
Step: 2820, train/epoch: 0.6711090207099915
Step: 2830, train/loss: 0.19429999589920044
Step: 2830, train/grad_norm: 0.17426709830760956
Step: 2830, train/learning_rate: 4.663255458581261e-05
Step: 2830, train/epoch: 0.6734887957572937
Step: 2840, train/loss: 0.0032999999821186066
Step: 2840, train/grad_norm: 0.0210274625569582
Step: 2840, train/learning_rate: 4.6620658395113423e-05
Step: 2840, train/epoch: 0.6758686304092407
Step: 2850, train/loss: 0.04839999973773956
Step: 2850, train/grad_norm: 4.948865534970537e-05
Step: 2850, train/learning_rate: 4.6608758566435426e-05
Step: 2850, train/epoch: 0.6782484650611877
Step: 2860, train/loss: 0.0
Step: 2860, train/grad_norm: 1.0649282558006234e-05
Step: 2860, train/learning_rate: 4.659685873775743e-05
Step: 2860, train/epoch: 0.6806282997131348
Step: 2870, train/loss: 0.15060000121593475
Step: 2870, train/grad_norm: 0.0049971588887274265
Step: 2870, train/learning_rate: 4.658495890907943e-05
Step: 2870, train/epoch: 0.683008074760437
Step: 2880, train/loss: 0.0640999972820282
Step: 2880, train/grad_norm: 0.00013842927000951022
Step: 2880, train/learning_rate: 4.6573059080401435e-05
Step: 2880, train/epoch: 0.685387909412384
Step: 2890, train/loss: 0.04910000041127205
Step: 2890, train/grad_norm: 1.7469690646976233e-05
Step: 2890, train/learning_rate: 4.6561162889702246e-05
Step: 2890, train/epoch: 0.687767744064331
Step: 2900, train/loss: 0.10859999805688858
Step: 2900, train/grad_norm: 0.27878984808921814
Step: 2900, train/learning_rate: 4.654926306102425e-05
Step: 2900, train/epoch: 0.6901475191116333
Step: 2910, train/loss: 9.999999747378752e-05
Step: 2910, train/grad_norm: 5.2147363021504134e-05
Step: 2910, train/learning_rate: 4.653736323234625e-05
Step: 2910, train/epoch: 0.6925273537635803
Step: 2920, train/loss: 0.0608999989926815
Step: 2920, train/grad_norm: 0.00026698948931880295
Step: 2920, train/learning_rate: 4.6525463403668255e-05
Step: 2920, train/epoch: 0.6949071884155273
Step: 2930, train/loss: 0.0
Step: 2930, train/grad_norm: 9.75192742771469e-05
Step: 2930, train/learning_rate: 4.651356357499026e-05
Step: 2930, train/epoch: 0.6972870230674744
Step: 2940, train/loss: 0.0
Step: 2940, train/grad_norm: 0.00034325389424338937
Step: 2940, train/learning_rate: 4.650166738429107e-05
Step: 2940, train/epoch: 0.6996667981147766
Step: 2950, train/loss: 0.0012000000569969416
Step: 2950, train/grad_norm: 10.241902351379395
Step: 2950, train/learning_rate: 4.648976755561307e-05
Step: 2950, train/epoch: 0.7020466327667236
Step: 2960, train/loss: 0.3151000142097473
Step: 2960, train/grad_norm: 5.60567932552658e-05
Step: 2960, train/learning_rate: 4.6477867726935074e-05
Step: 2960, train/epoch: 0.7044264674186707
Step: 2970, train/loss: 0.17159999907016754
Step: 2970, train/grad_norm: 0.002934342948719859
Step: 2970, train/learning_rate: 4.646596789825708e-05
Step: 2970, train/epoch: 0.7068063020706177
Step: 2980, train/loss: 0.08659999817609787
Step: 2980, train/grad_norm: 0.050090957432985306
Step: 2980, train/learning_rate: 4.645406806957908e-05
Step: 2980, train/epoch: 0.7091860771179199
Step: 2990, train/loss: 0.015599999576807022
Step: 2990, train/grad_norm: 0.3448147773742676
Step: 2990, train/learning_rate: 4.644217187887989e-05
Step: 2990, train/epoch: 0.7115659117698669
Step: 3000, train/loss: 0.004600000102072954
Step: 3000, train/grad_norm: 9.372707427246496e-05
Step: 3000, train/learning_rate: 4.643027205020189e-05
Step: 3000, train/epoch: 0.713945746421814
Step: 3010, train/loss: 0.1995999962091446
Step: 3010, train/grad_norm: 1.713538949843496e-05
Step: 3010, train/learning_rate: 4.6418372221523896e-05
Step: 3010, train/epoch: 0.716325581073761
Step: 3020, train/loss: 0.0003000000142492354
Step: 3020, train/grad_norm: 2.158940553665161
Step: 3020, train/learning_rate: 4.64064723928459e-05
Step: 3020, train/epoch: 0.7187053561210632
Step: 3030, train/loss: 0.0
Step: 3030, train/grad_norm: 9.115180728258565e-05
Step: 3030, train/learning_rate: 4.63945725641679e-05
Step: 3030, train/epoch: 0.7210851907730103
Step: 3040, train/loss: 0.00019999999494757503
Step: 3040, train/grad_norm: 0.0002974826202262193
Step: 3040, train/learning_rate: 4.638267637346871e-05
Step: 3040, train/epoch: 0.7234650254249573
Step: 3050, train/loss: 0.0
Step: 3050, train/grad_norm: 0.00014018556976225227
Step: 3050, train/learning_rate: 4.6370776544790715e-05
Step: 3050, train/epoch: 0.7258448600769043
Step: 3060, train/loss: 0.03629999980330467
Step: 3060, train/grad_norm: 3.1965371363185113e-06
Step: 3060, train/learning_rate: 4.635887671611272e-05
Step: 3060, train/epoch: 0.7282246351242065
Step: 3070, train/loss: 0.0005000000237487257
Step: 3070, train/grad_norm: 1.1922498941421509
Step: 3070, train/learning_rate: 4.634697688743472e-05
Step: 3070, train/epoch: 0.7306044697761536
Step: 3080, train/loss: 0.17870000004768372
Step: 3080, train/grad_norm: 85.80633544921875
Step: 3080, train/learning_rate: 4.6335077058756724e-05
Step: 3080, train/epoch: 0.7329843044281006
Step: 3090, train/loss: 0.17219999432563782
Step: 3090, train/grad_norm: 0.00020490576571319252
Step: 3090, train/learning_rate: 4.6323180868057534e-05
Step: 3090, train/epoch: 0.7353641390800476
Step: 3100, train/loss: 0.06930000334978104
Step: 3100, train/grad_norm: 2.4664421289344318e-05
Step: 3100, train/learning_rate: 4.631128103937954e-05
Step: 3100, train/epoch: 0.7377439141273499
Step: 3110, train/loss: 0.023399999365210533
Step: 3110, train/grad_norm: 0.0001846687082434073
Step: 3110, train/learning_rate: 4.629938121070154e-05
Step: 3110, train/epoch: 0.7401237487792969
Step: 3120, train/loss: 9.999999747378752e-05
Step: 3120, train/grad_norm: 5.014328780816868e-05
Step: 3120, train/learning_rate: 4.628748138202354e-05
Step: 3120, train/epoch: 0.7425035834312439
Step: 3130, train/loss: 0.0007999999797903001
Step: 3130, train/grad_norm: 9.034021786646917e-06
Step: 3130, train/learning_rate: 4.6275581553345546e-05
Step: 3130, train/epoch: 0.7448834180831909
Step: 3140, train/loss: 0.1776999980211258
Step: 3140, train/grad_norm: 6.654304797848454e-06
Step: 3140, train/learning_rate: 4.6263685362646356e-05
Step: 3140, train/epoch: 0.7472631931304932
Step: 3150, train/loss: 0.04610000178217888
Step: 3150, train/grad_norm: 3.0081861041253433e-05
Step: 3150, train/learning_rate: 4.625178553396836e-05
Step: 3150, train/epoch: 0.7496430277824402
Step: 3160, train/loss: 9.999999747378752e-05
Step: 3160, train/grad_norm: 0.0006595283048227429
Step: 3160, train/learning_rate: 4.623988570529036e-05
Step: 3160, train/epoch: 0.7520228624343872
Step: 3170, train/loss: 0.0003000000142492354
Step: 3170, train/grad_norm: 1.546775221824646
Step: 3170, train/learning_rate: 4.6227985876612365e-05
Step: 3170, train/epoch: 0.7544026374816895
Step: 3180, train/loss: 0.006599999964237213
Step: 3180, train/grad_norm: 1.107898606278468e-05
Step: 3180, train/learning_rate: 4.621608604793437e-05
Step: 3180, train/epoch: 0.7567824721336365
Step: 3190, train/loss: 0.14569999277591705
Step: 3190, train/grad_norm: 6.365143053699285e-05
Step: 3190, train/learning_rate: 4.620418985723518e-05
Step: 3190, train/epoch: 0.7591623067855835
Step: 3200, train/loss: 0.0
Step: 3200, train/grad_norm: 3.6649284993472975e-06
Step: 3200, train/learning_rate: 4.619229002855718e-05
Step: 3200, train/epoch: 0.7615421414375305
Step: 3210, train/loss: 0.04670000076293945
Step: 3210, train/grad_norm: 0.005046192556619644
Step: 3210, train/learning_rate: 4.6180390199879184e-05
Step: 3210, train/epoch: 0.7639219164848328
Step: 3220, train/loss: 0.0
Step: 3220, train/grad_norm: 0.00041244388557970524
Step: 3220, train/learning_rate: 4.616849037120119e-05
Step: 3220, train/epoch: 0.7663017511367798
Step: 3230, train/loss: 0.03060000017285347
Step: 3230, train/grad_norm: 6.776926511520287e-06
Step: 3230, train/learning_rate: 4.615659054252319e-05
Step: 3230, train/epoch: 0.7686815857887268
Step: 3240, train/loss: 0.0003000000142492354
Step: 3240, train/grad_norm: 1.6256128219538368e-05
Step: 3240, train/learning_rate: 4.6144694351824e-05
Step: 3240, train/epoch: 0.7710614204406738
Step: 3250, train/loss: 0.0
Step: 3250, train/grad_norm: 0.006149949971586466
Step: 3250, train/learning_rate: 4.6132794523146003e-05
Step: 3250, train/epoch: 0.7734411954879761
Step: 3260, train/loss: 0.0
Step: 3260, train/grad_norm: 0.0005482137785293162
Step: 3260, train/learning_rate: 4.6120894694468006e-05
Step: 3260, train/epoch: 0.7758210301399231
Step: 3270, train/loss: 0.03139999881386757
Step: 3270, train/grad_norm: 1.3653036603500368e-06
Step: 3270, train/learning_rate: 4.610899486579001e-05
Step: 3270, train/epoch: 0.7782008647918701
Step: 3280, train/loss: 9.999999747378752e-05
Step: 3280, train/grad_norm: 0.0005227811634540558
Step: 3280, train/learning_rate: 4.609709503711201e-05
Step: 3280, train/epoch: 0.7805806994438171
Step: 3290, train/loss: 9.999999747378752e-05
Step: 3290, train/grad_norm: 0.00037815675023011863
Step: 3290, train/learning_rate: 4.608519884641282e-05
Step: 3290, train/epoch: 0.7829604744911194
Step: 3300, train/loss: 0.15950000286102295
Step: 3300, train/grad_norm: 1.1286587323411368e-05
Step: 3300, train/learning_rate: 4.6073299017734826e-05
Step: 3300, train/epoch: 0.7853403091430664
Step: 3310, train/loss: 9.999999747378752e-05
Step: 3310, train/grad_norm: 0.0002548500197008252
Step: 3310, train/learning_rate: 4.606139918905683e-05
Step: 3310, train/epoch: 0.7877201437950134
Step: 3320, train/loss: 0.23520000278949738
Step: 3320, train/grad_norm: 0.003007265040650964
Step: 3320, train/learning_rate: 4.604949936037883e-05
Step: 3320, train/epoch: 0.7900999784469604
Step: 3330, train/loss: 0.0
Step: 3330, train/grad_norm: 0.0011677640723064542
Step: 3330, train/learning_rate: 4.6037599531700835e-05
Step: 3330, train/epoch: 0.7924797534942627
Step: 3340, train/loss: 0.05860000103712082
Step: 3340, train/grad_norm: 0.0003680069057736546
Step: 3340, train/learning_rate: 4.6025703341001645e-05
Step: 3340, train/epoch: 0.7948595881462097
Step: 3350, train/loss: 0.00039999998989515007
Step: 3350, train/grad_norm: 0.00025570549769327044
Step: 3350, train/learning_rate: 4.601380351232365e-05
Step: 3350, train/epoch: 0.7972394227981567
Step: 3360, train/loss: 0.08129999786615372
Step: 3360, train/grad_norm: 0.00017957462114281952
Step: 3360, train/learning_rate: 4.600190368364565e-05
Step: 3360, train/epoch: 0.7996192574501038
Step: 3370, train/loss: 0.0
Step: 3370, train/grad_norm: 4.008972155133961e-06
Step: 3370, train/learning_rate: 4.5990003854967654e-05
Step: 3370, train/epoch: 0.801999032497406
Step: 3380, train/loss: 0.0
Step: 3380, train/grad_norm: 3.66164676961489e-05
Step: 3380, train/learning_rate: 4.597810402628966e-05
Step: 3380, train/epoch: 0.804378867149353
Step: 3390, train/loss: 0.009499999694526196
Step: 3390, train/grad_norm: 0.0006555201252922416
Step: 3390, train/learning_rate: 4.596620783559047e-05
Step: 3390, train/epoch: 0.8067587018013
Step: 3400, train/loss: 0.1054999977350235
Step: 3400, train/grad_norm: 0.0002595230471342802
Step: 3400, train/learning_rate: 4.595430800691247e-05
Step: 3400, train/epoch: 0.8091384768486023
Step: 3410, train/loss: 0.09960000216960907
Step: 3410, train/grad_norm: 0.012353690341114998
Step: 3410, train/learning_rate: 4.594240817823447e-05
Step: 3410, train/epoch: 0.8115183115005493
Step: 3420, train/loss: 0.0
Step: 3420, train/grad_norm: 0.0054432726465165615
Step: 3420, train/learning_rate: 4.5930508349556476e-05
Step: 3420, train/epoch: 0.8138981461524963
Step: 3430, train/loss: 0.05119999870657921
Step: 3430, train/grad_norm: 2.1076531410217285
Step: 3430, train/learning_rate: 4.591860852087848e-05
Step: 3430, train/epoch: 0.8162779808044434
Step: 3440, train/loss: 0.040800001472234726
Step: 3440, train/grad_norm: 36.26115417480469
Step: 3440, train/learning_rate: 4.590671233017929e-05
Step: 3440, train/epoch: 0.8186577558517456
Step: 3450, train/loss: 0.0
Step: 3450, train/grad_norm: 0.0006482072058133781
Step: 3450, train/learning_rate: 4.589481250150129e-05
Step: 3450, train/epoch: 0.8210375905036926
Step: 3460, train/loss: 0.04899999871850014
Step: 3460, train/grad_norm: 0.00023946644796524197
Step: 3460, train/learning_rate: 4.5882912672823295e-05
Step: 3460, train/epoch: 0.8234174251556396
Step: 3470, train/loss: 0.11050000041723251
Step: 3470, train/grad_norm: 9.976543515222147e-05
Step: 3470, train/learning_rate: 4.58710128441453e-05
Step: 3470, train/epoch: 0.8257972598075867
Step: 3480, train/loss: 0.00019999999494757503
Step: 3480, train/grad_norm: 0.0001777731959009543
Step: 3480, train/learning_rate: 4.58591130154673e-05
Step: 3480, train/epoch: 0.8281770348548889
Step: 3490, train/loss: 0.0
Step: 3490, train/grad_norm: 0.0006913962424732745
Step: 3490, train/learning_rate: 4.584721682476811e-05
Step: 3490, train/epoch: 0.8305568695068359
Step: 3500, train/loss: 0.025800000876188278
Step: 3500, train/grad_norm: 8.215555453716661e-07
Step: 3500, train/learning_rate: 4.5835316996090114e-05
Step: 3500, train/epoch: 0.832936704158783
Step: 3510, train/loss: 0.0
Step: 3510, train/grad_norm: 3.2206837204284966e-05
Step: 3510, train/learning_rate: 4.582341716741212e-05
Step: 3510, train/epoch: 0.83531653881073
Step: 3520, train/loss: 0.008899999782443047
Step: 3520, train/grad_norm: 3.009226929862052e-06
Step: 3520, train/learning_rate: 4.581151733873412e-05
Step: 3520, train/epoch: 0.8376963138580322
Step: 3530, train/loss: 0.03460000082850456
Step: 3530, train/grad_norm: 0.05158105120062828
Step: 3530, train/learning_rate: 4.579961751005612e-05
Step: 3530, train/epoch: 0.8400761485099792
Step: 3540, train/loss: 0.0
Step: 3540, train/grad_norm: 0.02509627304971218
Step: 3540, train/learning_rate: 4.578772131935693e-05
Step: 3540, train/epoch: 0.8424559831619263
Step: 3550, train/loss: 0.0
Step: 3550, train/grad_norm: 4.8452700866619125e-05
Step: 3550, train/learning_rate: 4.5775821490678936e-05
Step: 3550, train/epoch: 0.8448358178138733
Step: 3560, train/loss: 0.12470000237226486
Step: 3560, train/grad_norm: 0.13171274960041046
Step: 3560, train/learning_rate: 4.576392166200094e-05
Step: 3560, train/epoch: 0.8472155928611755
Step: 3570, train/loss: 0.21330000460147858
Step: 3570, train/grad_norm: 2.3989176042960025e-05
Step: 3570, train/learning_rate: 4.575202183332294e-05
Step: 3570, train/epoch: 0.8495954275131226
Step: 3580, train/loss: 0.06239999830722809
Step: 3580, train/grad_norm: 0.3414139449596405
Step: 3580, train/learning_rate: 4.5740122004644945e-05
Step: 3580, train/epoch: 0.8519752621650696
Step: 3590, train/loss: 0.016200000420212746
Step: 3590, train/grad_norm: 6.522581770695979e-06
Step: 3590, train/learning_rate: 4.5728225813945755e-05
Step: 3590, train/epoch: 0.8543550968170166
Step: 3600, train/loss: 0.0
Step: 3600, train/grad_norm: 2.6167661417275667e-05
Step: 3600, train/learning_rate: 4.571632598526776e-05
Step: 3600, train/epoch: 0.8567348718643188
Step: 3610, train/loss: 0.00019999999494757503
Step: 3610, train/grad_norm: 2.8493224817793816e-05
Step: 3610, train/learning_rate: 4.570442615658976e-05
Step: 3610, train/epoch: 0.8591147065162659
Step: 3620, train/loss: 0.0
Step: 3620, train/grad_norm: 6.552219019795302e-06
Step: 3620, train/learning_rate: 4.5692526327911764e-05
Step: 3620, train/epoch: 0.8614945411682129
Step: 3630, train/loss: 0.11169999837875366
Step: 3630, train/grad_norm: 3.288939478807151e-05
Step: 3630, train/learning_rate: 4.568062649923377e-05
Step: 3630, train/epoch: 0.8638743162155151
Step: 3640, train/loss: 0.0010000000474974513
Step: 3640, train/grad_norm: 3.842246951535344e-05
Step: 3640, train/learning_rate: 4.566873030853458e-05
Step: 3640, train/epoch: 0.8662541508674622
Step: 3650, train/loss: 0.17440000176429749
Step: 3650, train/grad_norm: 36.13633346557617
Step: 3650, train/learning_rate: 4.565683047985658e-05
Step: 3650, train/epoch: 0.8686339855194092
Step: 3660, train/loss: 0.018699999898672104
Step: 3660, train/grad_norm: 0.0026749339886009693
Step: 3660, train/learning_rate: 4.5644930651178584e-05
Step: 3660, train/epoch: 0.8710138201713562
Step: 3670, train/loss: 0.07479999959468842
Step: 3670, train/grad_norm: 0.0026283159386366606
Step: 3670, train/learning_rate: 4.5633030822500587e-05
Step: 3670, train/epoch: 0.8733935952186584
Step: 3680, train/loss: 0.013199999928474426
Step: 3680, train/grad_norm: 0.04243278130888939
Step: 3680, train/learning_rate: 4.562113099382259e-05
Step: 3680, train/epoch: 0.8757734298706055
Step: 3690, train/loss: 9.999999747378752e-05
Step: 3690, train/grad_norm: 0.20455288887023926
Step: 3690, train/learning_rate: 4.56092348031234e-05
Step: 3690, train/epoch: 0.8781532645225525
Step: 3700, train/loss: 0.0006000000284984708
Step: 3700, train/grad_norm: 3.417895277380012e-05
Step: 3700, train/learning_rate: 4.55973349744454e-05
Step: 3700, train/epoch: 0.8805330991744995
Step: 3710, train/loss: 0.0
Step: 3710, train/grad_norm: 2.444118649691518e-07
Step: 3710, train/learning_rate: 4.5585435145767406e-05
Step: 3710, train/epoch: 0.8829128742218018
Step: 3720, train/loss: 0.15160000324249268
Step: 3720, train/grad_norm: 9.172905265586451e-05
Step: 3720, train/learning_rate: 4.557353531708941e-05
Step: 3720, train/epoch: 0.8852927088737488
Step: 3730, train/loss: 9.999999747378752e-05
Step: 3730, train/grad_norm: 0.0007181796245276928
Step: 3730, train/learning_rate: 4.556163912639022e-05
Step: 3730, train/epoch: 0.8876725435256958
Step: 3740, train/loss: 0.001500000013038516
Step: 3740, train/grad_norm: 1.5571729818475433e-05
Step: 3740, train/learning_rate: 4.554973929771222e-05
Step: 3740, train/epoch: 0.8900523781776428
Step: 3750, train/loss: 0.0
Step: 3750, train/grad_norm: 7.570112302346388e-06
Step: 3750, train/learning_rate: 4.5537839469034225e-05
Step: 3750, train/epoch: 0.8924321532249451
Step: 3760, train/loss: 0.0
Step: 3760, train/grad_norm: 7.995373744051903e-05
Step: 3760, train/learning_rate: 4.552593964035623e-05
Step: 3760, train/epoch: 0.8948119878768921
Step: 3770, train/loss: 0.49709999561309814
Step: 3770, train/grad_norm: 2.9357536845964205e-07
Step: 3770, train/learning_rate: 4.551403981167823e-05
Step: 3770, train/epoch: 0.8971918225288391
Step: 3780, train/loss: 0.0
Step: 3780, train/grad_norm: 0.0855531170964241
Step: 3780, train/learning_rate: 4.550214362097904e-05
Step: 3780, train/epoch: 0.8995716571807861
Step: 3790, train/loss: 0.04569999873638153
Step: 3790, train/grad_norm: 4.387560238683363e-06
Step: 3790, train/learning_rate: 4.5490243792301044e-05
Step: 3790, train/epoch: 0.9019514322280884
Step: 3800, train/loss: 0.0
Step: 3800, train/grad_norm: 0.0001053657615557313
Step: 3800, train/learning_rate: 4.547834396362305e-05
Step: 3800, train/epoch: 0.9043312668800354
Step: 3810, train/loss: 9.999999747378752e-05
Step: 3810, train/grad_norm: 0.016382629051804543
Step: 3810, train/learning_rate: 4.546644413494505e-05
Step: 3810, train/epoch: 0.9067111015319824
Step: 3820, train/loss: 0.19249999523162842
Step: 3820, train/grad_norm: 5.391645390773192e-05
Step: 3820, train/learning_rate: 4.545454430626705e-05
Step: 3820, train/epoch: 0.9090909361839294
Step: 3830, train/loss: 0.0
Step: 3830, train/grad_norm: 0.0004545230185613036
Step: 3830, train/learning_rate: 4.544264811556786e-05
Step: 3830, train/epoch: 0.9114707112312317
Step: 3840, train/loss: 0.0
Step: 3840, train/grad_norm: 0.03880452737212181
Step: 3840, train/learning_rate: 4.5430748286889866e-05
Step: 3840, train/epoch: 0.9138505458831787
Step: 3850, train/loss: 9.999999747378752e-05
Step: 3850, train/grad_norm: 0.0031986006069928408
Step: 3850, train/learning_rate: 4.541884845821187e-05
Step: 3850, train/epoch: 0.9162303805351257
Step: 3860, train/loss: 0.0032999999821186066
Step: 3860, train/grad_norm: 0.09559748321771622
Step: 3860, train/learning_rate: 4.540694862953387e-05
Step: 3860, train/epoch: 0.9186102151870728
Step: 3870, train/loss: 0.0003000000142492354
Step: 3870, train/grad_norm: 0.00011313735012663528
Step: 3870, train/learning_rate: 4.5395048800855875e-05
Step: 3870, train/epoch: 0.920989990234375
Step: 3880, train/loss: 0.002199999988079071
Step: 3880, train/grad_norm: 0.011623112484812737
Step: 3880, train/learning_rate: 4.5383152610156685e-05
Step: 3880, train/epoch: 0.923369824886322
Step: 3890, train/loss: 0.00019999999494757503
Step: 3890, train/grad_norm: 3.721494795172475e-05
Step: 3890, train/learning_rate: 4.537125278147869e-05
Step: 3890, train/epoch: 0.925749659538269
Step: 3900, train/loss: 0.010400000028312206
Step: 3900, train/grad_norm: 1.0100698943915631e-07
Step: 3900, train/learning_rate: 4.535935295280069e-05
Step: 3900, train/epoch: 0.9281294345855713
Step: 3910, train/loss: 0.0
Step: 3910, train/grad_norm: 8.326644092448987e-06
Step: 3910, train/learning_rate: 4.5347453124122694e-05
Step: 3910, train/epoch: 0.9305092692375183
Step: 3920, train/loss: 0.0
Step: 3920, train/grad_norm: 1.9838736875499308e-08
Step: 3920, train/learning_rate: 4.53355532954447e-05
Step: 3920, train/epoch: 0.9328891038894653
Step: 3930, train/loss: 0.0012000000569969416
Step: 3930, train/grad_norm: 0.005925929173827171
Step: 3930, train/learning_rate: 4.532365710474551e-05
Step: 3930, train/epoch: 0.9352689385414124
Step: 3940, train/loss: 0.00019999999494757503
Step: 3940, train/grad_norm: 0.0014172384981065989
Step: 3940, train/learning_rate: 4.531175727606751e-05
Step: 3940, train/epoch: 0.9376487135887146
Step: 3950, train/loss: 9.999999747378752e-05
Step: 3950, train/grad_norm: 0.08968240022659302
Step: 3950, train/learning_rate: 4.529985744738951e-05
Step: 3950, train/epoch: 0.9400285482406616
Step: 3960, train/loss: 0.0
Step: 3960, train/grad_norm: 3.256093918935221e-07
Step: 3960, train/learning_rate: 4.5287957618711516e-05
Step: 3960, train/epoch: 0.9424083828926086
Step: 3970, train/loss: 0.1151999980211258
Step: 3970, train/grad_norm: 7.036088209133595e-05
Step: 3970, train/learning_rate: 4.527605779003352e-05
Step: 3970, train/epoch: 0.9447882175445557
Step: 3980, train/loss: 0.0
Step: 3980, train/grad_norm: 8.179774795280537e-07
Step: 3980, train/learning_rate: 4.526416159933433e-05
Step: 3980, train/epoch: 0.9471679925918579
Step: 3990, train/loss: 0.08399999886751175
Step: 3990, train/grad_norm: 3.388343827737117e-07
Step: 3990, train/learning_rate: 4.525226177065633e-05
Step: 3990, train/epoch: 0.9495478272438049
Step: 4000, train/loss: 0.0
Step: 4000, train/grad_norm: 8.036705168024127e-08
Step: 4000, train/learning_rate: 4.5240361941978335e-05
Step: 4000, train/epoch: 0.951927661895752
Step: 4010, train/loss: 0.0
Step: 4010, train/grad_norm: 5.5383438848366495e-06
Step: 4010, train/learning_rate: 4.522846211330034e-05
Step: 4010, train/epoch: 0.954307496547699
Step: 4020, train/loss: 0.14579999446868896
Step: 4020, train/grad_norm: 0.0004718904383480549
Step: 4020, train/learning_rate: 4.521656228462234e-05
Step: 4020, train/epoch: 0.9566872715950012
Step: 4030, train/loss: 0.2596000134944916
Step: 4030, train/grad_norm: 8.230601451941766e-06
Step: 4030, train/learning_rate: 4.520466609392315e-05
Step: 4030, train/epoch: 0.9590671062469482
Step: 4040, train/loss: 9.999999747378752e-05
Step: 4040, train/grad_norm: 0.1201467290520668
Step: 4040, train/learning_rate: 4.5192766265245155e-05
Step: 4040, train/epoch: 0.9614469408988953
Step: 4050, train/loss: 0.00019999999494757503
Step: 4050, train/grad_norm: 0.0005865797866135836
Step: 4050, train/learning_rate: 4.518086643656716e-05
Step: 4050, train/epoch: 0.9638267755508423
Step: 4060, train/loss: 0.04699999839067459
Step: 4060, train/grad_norm: 0.19664667546749115
Step: 4060, train/learning_rate: 4.516896660788916e-05
Step: 4060, train/epoch: 0.9662065505981445
Step: 4070, train/loss: 0.0851999968290329
Step: 4070, train/grad_norm: 0.0006411376525647938
Step: 4070, train/learning_rate: 4.5157066779211164e-05
Step: 4070, train/epoch: 0.9685863852500916
Step: 4080, train/loss: 0.0
Step: 4080, train/grad_norm: 1.580560638103634e-05
Step: 4080, train/learning_rate: 4.5145170588511974e-05
Step: 4080, train/epoch: 0.9709662199020386
Step: 4090, train/loss: 0.07760000228881836
Step: 4090, train/grad_norm: 0.0004680522542912513
Step: 4090, train/learning_rate: 4.513327075983398e-05
Step: 4090, train/epoch: 0.9733460545539856
Step: 4100, train/loss: 0.00039999998989515007
Step: 4100, train/grad_norm: 0.000505211588460952
Step: 4100, train/learning_rate: 4.512137093115598e-05
Step: 4100, train/epoch: 0.9757258296012878
Step: 4110, train/loss: 0.0
Step: 4110, train/grad_norm: 2.7117866920889355e-05
Step: 4110, train/learning_rate: 4.510947110247798e-05
Step: 4110, train/epoch: 0.9781056642532349
Step: 4120, train/loss: 0.12269999831914902
Step: 4120, train/grad_norm: 8.482646762786317e-07
Step: 4120, train/learning_rate: 4.5097571273799986e-05
Step: 4120, train/epoch: 0.9804854989051819
Step: 4130, train/loss: 0.02449999935925007
Step: 4130, train/grad_norm: 329.44561767578125
Step: 4130, train/learning_rate: 4.5085675083100796e-05
Step: 4130, train/epoch: 0.9828652739524841
Step: 4140, train/loss: 0.0
Step: 4140, train/grad_norm: 3.164228928653756e-06
Step: 4140, train/learning_rate: 4.50737752544228e-05
Step: 4140, train/epoch: 0.9852451086044312
Step: 4150, train/loss: 0.0
Step: 4150, train/grad_norm: 0.00031585065880790353
Step: 4150, train/learning_rate: 4.50618754257448e-05
Step: 4150, train/epoch: 0.9876249432563782
Step: 4160, train/loss: 0.15860000252723694
Step: 4160, train/grad_norm: 1.8680245830182685e-06
Step: 4160, train/learning_rate: 4.5049975597066805e-05
Step: 4160, train/epoch: 0.9900047779083252
Step: 4170, train/loss: 0.0
Step: 4170, train/grad_norm: 8.623350140624098e-07
Step: 4170, train/learning_rate: 4.503807576838881e-05
Step: 4170, train/epoch: 0.9923845529556274
Step: 4180, train/loss: 0.18019999563694
Step: 4180, train/grad_norm: 1.3716365288019006e-07
Step: 4180, train/learning_rate: 4.502617957768962e-05
Step: 4180, train/epoch: 0.9947643876075745
Step: 4190, train/loss: 0.2770000100135803
Step: 4190, train/grad_norm: 117.31941986083984
Step: 4190, train/learning_rate: 4.501427974901162e-05
Step: 4190, train/epoch: 0.9971442222595215
Step: 4200, train/loss: 0.00039999998989515007
Step: 4200, train/grad_norm: 0.25867125391960144
Step: 4200, train/learning_rate: 4.5002379920333624e-05
Step: 4200, train/epoch: 0.9995240569114685
Step: 4202, eval/loss: 0.0124383345246315
Step: 4202, eval/accuracy: 0.9959738850593567
Step: 4202, eval/f1: 0.9957509636878967
Step: 4202, eval/runtime: 735.7144165039062
Step: 4202, eval/samples_per_second: 9.789999961853027
Step: 4202, eval/steps_per_second: 1.225000023841858
Step: 4202, train/epoch: 1.0
Step: 4210, train/loss: 0.0
Step: 4210, train/grad_norm: 0.0033472718205302954
Step: 4210, train/learning_rate: 4.499048009165563e-05
Step: 4210, train/epoch: 1.0019038915634155
Step: 4220, train/loss: 0.08219999819993973
Step: 4220, train/grad_norm: 0.001150157768279314
Step: 4220, train/learning_rate: 4.497858026297763e-05
Step: 4220, train/epoch: 1.0042836666107178
Step: 4230, train/loss: 0.00019999999494757503
Step: 4230, train/grad_norm: 1.8966473362524994e-05
Step: 4230, train/learning_rate: 4.496668407227844e-05
Step: 4230, train/epoch: 1.00666344165802
Step: 4240, train/loss: 9.999999747378752e-05
Step: 4240, train/grad_norm: 0.7322254180908203
Step: 4240, train/learning_rate: 4.495478424360044e-05
Step: 4240, train/epoch: 1.0090433359146118
Step: 4250, train/loss: 0.0
Step: 4250, train/grad_norm: 0.0050171478651463985
Step: 4250, train/learning_rate: 4.4942884414922446e-05
Step: 4250, train/epoch: 1.011423110961914
Step: 4260, train/loss: 0.14300000667572021
Step: 4260, train/grad_norm: 0.00016873172717168927
Step: 4260, train/learning_rate: 4.493098458624445e-05
Step: 4260, train/epoch: 1.0138030052185059
Step: 4270, train/loss: 0.0
Step: 4270, train/grad_norm: 4.117886874155374e-06
Step: 4270, train/learning_rate: 4.491908475756645e-05
Step: 4270, train/epoch: 1.016182780265808
Step: 4280, train/loss: 0.005100000184029341
Step: 4280, train/grad_norm: 5.726628387492383e-06
Step: 4280, train/learning_rate: 4.490718856686726e-05
Step: 4280, train/epoch: 1.0185625553131104
Step: 4290, train/loss: 0.0
Step: 4290, train/grad_norm: 1.3164013580535538e-05
Step: 4290, train/learning_rate: 4.4895288738189265e-05
Step: 4290, train/epoch: 1.0209424495697021
Step: 4300, train/loss: 0.1071000024676323
Step: 4300, train/grad_norm: 4.10820975957904e-05
Step: 4300, train/learning_rate: 4.488338890951127e-05
Step: 4300, train/epoch: 1.0233222246170044
Step: 4310, train/loss: 0.09179999679327011
Step: 4310, train/grad_norm: 6.265091360546649e-05
Step: 4310, train/learning_rate: 4.487148908083327e-05
Step: 4310, train/epoch: 1.0257019996643066
Step: 4320, train/loss: 0.0019000000320374966
Step: 4320, train/grad_norm: 5.161437456990825e-06
Step: 4320, train/learning_rate: 4.4859589252155274e-05
Step: 4320, train/epoch: 1.0280818939208984
Step: 4330, train/loss: 0.07039999961853027
Step: 4330, train/grad_norm: 4.646605702873785e-06
Step: 4330, train/learning_rate: 4.4847693061456084e-05
Step: 4330, train/epoch: 1.0304616689682007
Step: 4340, train/loss: 0.0
Step: 4340, train/grad_norm: 0.0007444195216521621
Step: 4340, train/learning_rate: 4.483579323277809e-05
Step: 4340, train/epoch: 1.0328415632247925
Step: 4350, train/loss: 0.0
Step: 4350, train/grad_norm: 0.008913157507777214
Step: 4350, train/learning_rate: 4.482389340410009e-05
Step: 4350, train/epoch: 1.0352213382720947
Step: 4360, train/loss: 0.00019999999494757503
Step: 4360, train/grad_norm: 0.0006212585722096264
Step: 4360, train/learning_rate: 4.481199357542209e-05
Step: 4360, train/epoch: 1.037601113319397
Step: 4370, train/loss: 0.05999999865889549
Step: 4370, train/grad_norm: 0.0039041077252477407
Step: 4370, train/learning_rate: 4.4800093746744096e-05
Step: 4370, train/epoch: 1.0399810075759888
Step: 4380, train/loss: 0.027300000190734863
Step: 4380, train/grad_norm: 92.35316467285156
Step: 4380, train/learning_rate: 4.4788197556044906e-05
Step: 4380, train/epoch: 1.042360782623291
Step: 4390, train/loss: 0.0
Step: 4390, train/grad_norm: 0.006110844202339649
Step: 4390, train/learning_rate: 4.477629772736691e-05
Step: 4390, train/epoch: 1.0447405576705933
Step: 4400, train/loss: 0.002099999925121665
Step: 4400, train/grad_norm: 26.384960174560547
Step: 4400, train/learning_rate: 4.476439789868891e-05
Step: 4400, train/epoch: 1.047120451927185
Step: 4410, train/loss: 0.040800001472234726
Step: 4410, train/grad_norm: 1.565531056257896e-05
Step: 4410, train/learning_rate: 4.4752498070010915e-05
Step: 4410, train/epoch: 1.0495002269744873
Step: 4420, train/loss: 9.999999747378752e-05
Step: 4420, train/grad_norm: 0.021297480911016464
Step: 4420, train/learning_rate: 4.474059824133292e-05
Step: 4420, train/epoch: 1.0518800020217896
Step: 4430, train/loss: 0.0
Step: 4430, train/grad_norm: 6.94971220127627e-07
Step: 4430, train/learning_rate: 4.472870205063373e-05
Step: 4430, train/epoch: 1.0542598962783813
Step: 4440, train/loss: 0.003700000001117587
Step: 4440, train/grad_norm: 3.7607212561852066e-06
Step: 4440, train/learning_rate: 4.471680222195573e-05
Step: 4440, train/epoch: 1.0566396713256836
Step: 4450, train/loss: 0.13439999520778656
Step: 4450, train/grad_norm: 5.975194198981626e-06
Step: 4450, train/learning_rate: 4.4704902393277735e-05
Step: 4450, train/epoch: 1.0590195655822754
Step: 4460, train/loss: 0.0003000000142492354
Step: 4460, train/grad_norm: 0.00012550866813398898
Step: 4460, train/learning_rate: 4.469300256459974e-05
Step: 4460, train/epoch: 1.0613993406295776
Step: 4470, train/loss: 9.999999747378752e-05
Step: 4470, train/grad_norm: 0.021763591095805168
Step: 4470, train/learning_rate: 4.468110273592174e-05
Step: 4470, train/epoch: 1.0637791156768799
Step: 4480, train/loss: 0.0003000000142492354
Step: 4480, train/grad_norm: 6.9427250082299e-07
Step: 4480, train/learning_rate: 4.466920654522255e-05
Step: 4480, train/epoch: 1.0661590099334717
Step: 4490, train/loss: 0.0
Step: 4490, train/grad_norm: 1.7143092918558978e-05
Step: 4490, train/learning_rate: 4.4657306716544554e-05
Step: 4490, train/epoch: 1.068538784980774
Step: 4500, train/loss: 0.009100000374019146
Step: 4500, train/grad_norm: 0.01642962172627449
Step: 4500, train/learning_rate: 4.464540688786656e-05
Step: 4500, train/epoch: 1.0709185600280762
Step: 4510, train/loss: 0.0
Step: 4510, train/grad_norm: 4.035370693600271e-06
Step: 4510, train/learning_rate: 4.463350705918856e-05
Step: 4510, train/epoch: 1.073298454284668
Step: 4520, train/loss: 0.0
Step: 4520, train/grad_norm: 1.8393398931948468e-05
Step: 4520, train/learning_rate: 4.462160723051056e-05
Step: 4520, train/epoch: 1.0756782293319702
Step: 4530, train/loss: 0.0015999999595806003
Step: 4530, train/grad_norm: 9.827027724895743e-07
Step: 4530, train/learning_rate: 4.460971103981137e-05
Step: 4530, train/epoch: 1.078058123588562
Step: 4540, train/loss: 0.1054999977350235
Step: 4540, train/grad_norm: 4.57471287518274e-06
Step: 4540, train/learning_rate: 4.4597811211133376e-05
Step: 4540, train/epoch: 1.0804378986358643
Step: 4550, train/loss: 0.0
Step: 4550, train/grad_norm: 3.919189111911692e-06
Step: 4550, train/learning_rate: 4.458591138245538e-05
Step: 4550, train/epoch: 1.0828176736831665
Step: 4560, train/loss: 0.0
Step: 4560, train/grad_norm: 1.833849455579184e-05
Step: 4560, train/learning_rate: 4.457401155377738e-05
Step: 4560, train/epoch: 1.0851975679397583
Step: 4570, train/loss: 0.0
Step: 4570, train/grad_norm: 1.826000698201824e-05
Step: 4570, train/learning_rate: 4.4562111725099385e-05
Step: 4570, train/epoch: 1.0875773429870605
Step: 4580, train/loss: 0.0012000000569969416
Step: 4580, train/grad_norm: 7.324373200390255e-06
Step: 4580, train/learning_rate: 4.4550215534400195e-05
Step: 4580, train/epoch: 1.0899571180343628
Step: 4590, train/loss: 0.0007999999797903001
Step: 4590, train/grad_norm: 1.2261473329999717e-06
Step: 4590, train/learning_rate: 4.45383157057222e-05
Step: 4590, train/epoch: 1.0923370122909546
Step: 4600, train/loss: 0.0
Step: 4600, train/grad_norm: 5.838140282321547e-07
Step: 4600, train/learning_rate: 4.45264158770442e-05
Step: 4600, train/epoch: 1.0947167873382568
Step: 4610, train/loss: 0.0
Step: 4610, train/grad_norm: 7.4557779043971095e-06
Step: 4610, train/learning_rate: 4.4514516048366204e-05
Step: 4610, train/epoch: 1.097096562385559
Step: 4620, train/loss: 0.04259999841451645
Step: 4620, train/grad_norm: 1.5843863366171718e-05
Step: 4620, train/learning_rate: 4.450261621968821e-05
Step: 4620, train/epoch: 1.0994764566421509
Step: 4630, train/loss: 0.00019999999494757503
Step: 4630, train/grad_norm: 1.236219731026722e-07
Step: 4630, train/learning_rate: 4.449072002898902e-05
Step: 4630, train/epoch: 1.1018562316894531
Step: 4640, train/loss: 0.0
Step: 4640, train/grad_norm: 1.6130125004565343e-06
Step: 4640, train/learning_rate: 4.447882020031102e-05
Step: 4640, train/epoch: 1.104236125946045
Step: 4650, train/loss: 0.002099999925121665
Step: 4650, train/grad_norm: 7.988602135355904e-08
Step: 4650, train/learning_rate: 4.446692037163302e-05
Step: 4650, train/epoch: 1.1066159009933472
Step: 4660, train/loss: 0.0
Step: 4660, train/grad_norm: 0.0006345611182041466
Step: 4660, train/learning_rate: 4.4455020542955026e-05
Step: 4660, train/epoch: 1.1089956760406494
Step: 4670, train/loss: 0.0
Step: 4670, train/grad_norm: 3.4570716707094107e-06
Step: 4670, train/learning_rate: 4.444312071427703e-05
Step: 4670, train/epoch: 1.1113755702972412
Step: 4680, train/loss: 0.0
Step: 4680, train/grad_norm: 4.040044743192084e-08
Step: 4680, train/learning_rate: 4.443122452357784e-05
Step: 4680, train/epoch: 1.1137553453445435
Step: 4690, train/loss: 0.00139999995008111
Step: 4690, train/grad_norm: 1.6303711163345724e-05
Step: 4690, train/learning_rate: 4.441932469489984e-05
Step: 4690, train/epoch: 1.1161351203918457
Step: 4700, train/loss: 0.0
Step: 4700, train/grad_norm: 1.648978908974641e-08
Step: 4700, train/learning_rate: 4.4407424866221845e-05
Step: 4700, train/epoch: 1.1185150146484375
Step: 4710, train/loss: 0.09260000288486481
Step: 4710, train/grad_norm: 1.2172604613169824e-07
Step: 4710, train/learning_rate: 4.439552503754385e-05
Step: 4710, train/epoch: 1.1208947896957397
Step: 4720, train/loss: 0.0
Step: 4720, train/grad_norm: 8.375937454729865e-07
Step: 4720, train/learning_rate: 4.438362520886585e-05
Step: 4720, train/epoch: 1.1232746839523315
Step: 4730, train/loss: 0.0
Step: 4730, train/grad_norm: 1.4560298950527795e-05
Step: 4730, train/learning_rate: 4.437172901816666e-05
Step: 4730, train/epoch: 1.1256544589996338
Step: 4740, train/loss: 9.999999747378752e-05
Step: 4740, train/grad_norm: 4.537733730103355e-06
Step: 4740, train/learning_rate: 4.4359829189488664e-05
Step: 4740, train/epoch: 1.128034234046936
Step: 4750, train/loss: 0.008999999612569809
Step: 4750, train/grad_norm: 3.662703875306761e-06
Step: 4750, train/learning_rate: 4.434792936081067e-05
Step: 4750, train/epoch: 1.1304141283035278
Step: 4760, train/loss: 0.0
Step: 4760, train/grad_norm: 1.9044859072891995e-05
Step: 4760, train/learning_rate: 4.433602953213267e-05
Step: 4760, train/epoch: 1.13279390335083
Step: 4770, train/loss: 0.11500000208616257
Step: 4770, train/grad_norm: 3.281856334069744e-05
Step: 4770, train/learning_rate: 4.432412970345467e-05
Step: 4770, train/epoch: 1.1351736783981323
Step: 4780, train/loss: 9.999999747378752e-05
Step: 4780, train/grad_norm: 0.00025071666459552944
Step: 4780, train/learning_rate: 4.4312233512755483e-05
Step: 4780, train/epoch: 1.1375535726547241
Step: 4790, train/loss: 0.012500000186264515
Step: 4790, train/grad_norm: 4.100185833522119e-06
Step: 4790, train/learning_rate: 4.4300333684077486e-05
Step: 4790, train/epoch: 1.1399333477020264
Step: 4800, train/loss: 0.0
Step: 4800, train/grad_norm: 1.334800145968984e-07
Step: 4800, train/learning_rate: 4.428843385539949e-05
Step: 4800, train/epoch: 1.1423132419586182
Step: 4810, train/loss: 0.0
Step: 4810, train/grad_norm: 0.00020897314243484288
Step: 4810, train/learning_rate: 4.427653402672149e-05
Step: 4810, train/epoch: 1.1446930170059204
Step: 4820, train/loss: 0.002899999963119626
Step: 4820, train/grad_norm: 2.809755130783742e-07
Step: 4820, train/learning_rate: 4.4264634198043495e-05
Step: 4820, train/epoch: 1.1470727920532227
Step: 4830, train/loss: 0.0
Step: 4830, train/grad_norm: 0.0002238614106317982
Step: 4830, train/learning_rate: 4.4252738007344306e-05
Step: 4830, train/epoch: 1.1494526863098145
Step: 4840, train/loss: 0.0
Step: 4840, train/grad_norm: 4.485703357204329e-06
Step: 4840, train/learning_rate: 4.424083817866631e-05
Step: 4840, train/epoch: 1.1518324613571167
Step: 4850, train/loss: 0.0
Step: 4850, train/grad_norm: 0.002546727890148759
Step: 4850, train/learning_rate: 4.422893834998831e-05
Step: 4850, train/epoch: 1.154212236404419
Step: 4860, train/loss: 0.00039999998989515007
Step: 4860, train/grad_norm: 4.12870895161177e-06
Step: 4860, train/learning_rate: 4.4217038521310315e-05
Step: 4860, train/epoch: 1.1565921306610107
Step: 4870, train/loss: 0.0
Step: 4870, train/grad_norm: 1.0348015166528057e-05
Step: 4870, train/learning_rate: 4.420513869263232e-05
Step: 4870, train/epoch: 1.158971905708313
Step: 4880, train/loss: 0.0
Step: 4880, train/grad_norm: 8.867320502758957e-06
Step: 4880, train/learning_rate: 4.419324250193313e-05
Step: 4880, train/epoch: 1.1613516807556152
Step: 4890, train/loss: 0.00430000014603138
Step: 4890, train/grad_norm: 1.304342447383533e-07
Step: 4890, train/learning_rate: 4.418134267325513e-05
Step: 4890, train/epoch: 1.163731575012207
Step: 4900, train/loss: 0.0010000000474974513
Step: 4900, train/grad_norm: 8.074134530033916e-05
Step: 4900, train/learning_rate: 4.4169442844577134e-05
Step: 4900, train/epoch: 1.1661113500595093
Step: 4910, train/loss: 0.0
Step: 4910, train/grad_norm: 0.003171258606016636
Step: 4910, train/learning_rate: 4.415754301589914e-05
Step: 4910, train/epoch: 1.168491244316101
Step: 4920, train/loss: 0.0
Step: 4920, train/grad_norm: 5.3594662858813535e-06
Step: 4920, train/learning_rate: 4.414564318722114e-05
Step: 4920, train/epoch: 1.1708710193634033
Step: 4930, train/loss: 0.0
Step: 4930, train/grad_norm: 1.8362769083068997e-07
Step: 4930, train/learning_rate: 4.413374699652195e-05
Step: 4930, train/epoch: 1.1732507944107056
Step: 4940, train/loss: 0.0
Step: 4940, train/grad_norm: 1.0262771077407251e-08
Step: 4940, train/learning_rate: 4.412184716784395e-05
Step: 4940, train/epoch: 1.1756306886672974
Step: 4950, train/loss: 0.0
Step: 4950, train/grad_norm: 3.235163603676483e-08
Step: 4950, train/learning_rate: 4.4109947339165956e-05
Step: 4950, train/epoch: 1.1780104637145996
Step: 4960, train/loss: 0.0
Step: 4960, train/grad_norm: 1.3674324691237416e-05
Step: 4960, train/learning_rate: 4.409804751048796e-05
Step: 4960, train/epoch: 1.1803902387619019
Step: 4970, train/loss: 0.0
Step: 4970, train/grad_norm: 0.02955750934779644
Step: 4970, train/learning_rate: 4.408614768180996e-05
Step: 4970, train/epoch: 1.1827701330184937
Step: 4980, train/loss: 0.017400000244379044
Step: 4980, train/grad_norm: 4.2340238337601477e-07
Step: 4980, train/learning_rate: 4.407425149111077e-05
Step: 4980, train/epoch: 1.185149908065796
Step: 4990, train/loss: 0.010400000028312206
Step: 4990, train/grad_norm: 2.490172619218356e-07
Step: 4990, train/learning_rate: 4.4062351662432775e-05
Step: 4990, train/epoch: 1.1875298023223877
Step: 5000, train/loss: 0.00039999998989515007
Step: 5000, train/grad_norm: 5.275293233353295e-07
Step: 5000, train/learning_rate: 4.405045183375478e-05
Step: 5000, train/epoch: 1.18990957736969
Step: 5010, train/loss: 0.21250000596046448
Step: 5010, train/grad_norm: 2.500129812688101e-05
Step: 5010, train/learning_rate: 4.403855200507678e-05
Step: 5010, train/epoch: 1.1922893524169922
Step: 5020, train/loss: 0.0
Step: 5020, train/grad_norm: 4.162536788498983e-05
Step: 5020, train/learning_rate: 4.4026652176398784e-05
Step: 5020, train/epoch: 1.194669246673584
Step: 5030, train/loss: 0.007699999958276749
Step: 5030, train/grad_norm: 77.72220611572266
Step: 5030, train/learning_rate: 4.4014755985699594e-05
Step: 5030, train/epoch: 1.1970490217208862
Step: 5040, train/loss: 0.0
Step: 5040, train/grad_norm: 0.00015405005251523107
Step: 5040, train/learning_rate: 4.40028561570216e-05
Step: 5040, train/epoch: 1.1994287967681885
Step: 5050, train/loss: 0.0
Step: 5050, train/grad_norm: 3.150987595290644e-07
Step: 5050, train/learning_rate: 4.39909563283436e-05
Step: 5050, train/epoch: 1.2018086910247803
Step: 5060, train/loss: 0.0
Step: 5060, train/grad_norm: 8.237585120696167e-07
Step: 5060, train/learning_rate: 4.39790564996656e-05
Step: 5060, train/epoch: 1.2041884660720825
Step: 5070, train/loss: 0.0
Step: 5070, train/grad_norm: 2.9500444753693955e-08
Step: 5070, train/learning_rate: 4.396716030896641e-05
Step: 5070, train/epoch: 1.2065683603286743
Step: 5080, train/loss: 0.0003000000142492354
Step: 5080, train/grad_norm: 1.6048123541168025e-07
Step: 5080, train/learning_rate: 4.3955260480288416e-05
Step: 5080, train/epoch: 1.2089481353759766
Step: 5090, train/loss: 0.0
Step: 5090, train/grad_norm: 0.0002031362528214231
Step: 5090, train/learning_rate: 4.394336065161042e-05
Step: 5090, train/epoch: 1.2113279104232788
Step: 5100, train/loss: 0.03840000182390213
Step: 5100, train/grad_norm: 8.640332680442953e-07
Step: 5100, train/learning_rate: 4.393146082293242e-05
Step: 5100, train/epoch: 1.2137078046798706
Step: 5110, train/loss: 0.015399999916553497
Step: 5110, train/grad_norm: 5.237155437469482
Step: 5110, train/learning_rate: 4.3919560994254425e-05
Step: 5110, train/epoch: 1.2160875797271729
Step: 5120, train/loss: 0.0
Step: 5120, train/grad_norm: 5.949450951447943e-06
Step: 5120, train/learning_rate: 4.3907664803555235e-05
Step: 5120, train/epoch: 1.218467354774475
Step: 5130, train/loss: 0.0
Step: 5130, train/grad_norm: 1.9691697161761113e-09
Step: 5130, train/learning_rate: 4.389576497487724e-05
Step: 5130, train/epoch: 1.220847249031067
Step: 5140, train/loss: 0.18359999358654022
Step: 5140, train/grad_norm: 2.316224794185473e-07
Step: 5140, train/learning_rate: 4.388386514619924e-05
Step: 5140, train/epoch: 1.2232270240783691
Step: 5150, train/loss: 0.0
Step: 5150, train/grad_norm: 2.1369205569499172e-05
Step: 5150, train/learning_rate: 4.3871965317521244e-05
Step: 5150, train/epoch: 1.2256067991256714
Step: 5160, train/loss: 0.0035000001080334187
Step: 5160, train/grad_norm: 1.606470732440357e-06
Step: 5160, train/learning_rate: 4.386006548884325e-05
Step: 5160, train/epoch: 1.2279866933822632
Step: 5170, train/loss: 0.0
Step: 5170, train/grad_norm: 0.012087110430002213
Step: 5170, train/learning_rate: 4.384816929814406e-05
Step: 5170, train/epoch: 1.2303664684295654
Step: 5180, train/loss: 0.0
Step: 5180, train/grad_norm: 8.613397949375212e-05
Step: 5180, train/learning_rate: 4.383626946946606e-05
Step: 5180, train/epoch: 1.2327463626861572
Step: 5190, train/loss: 0.10400000214576721
Step: 5190, train/grad_norm: 5.5498436267953366e-05
Step: 5190, train/learning_rate: 4.3824369640788063e-05
Step: 5190, train/epoch: 1.2351261377334595
Step: 5200, train/loss: 0.01899999938905239
Step: 5200, train/grad_norm: 0.0001735981204546988
Step: 5200, train/learning_rate: 4.3812469812110066e-05
Step: 5200, train/epoch: 1.2375059127807617
Step: 5210, train/loss: 0.0
Step: 5210, train/grad_norm: 0.0018790059257298708
Step: 5210, train/learning_rate: 4.380056998343207e-05
Step: 5210, train/epoch: 1.2398858070373535
Step: 5220, train/loss: 0.0
Step: 5220, train/grad_norm: 1.2478518328862265e-05
Step: 5220, train/learning_rate: 4.378867379273288e-05
Step: 5220, train/epoch: 1.2422655820846558
Step: 5230, train/loss: 0.13850000500679016
Step: 5230, train/grad_norm: 1.1036452178814216e-06
Step: 5230, train/learning_rate: 4.377677396405488e-05
Step: 5230, train/epoch: 1.244645357131958
Step: 5240, train/loss: 0.0
Step: 5240, train/grad_norm: 0.0014224669430404902
Step: 5240, train/learning_rate: 4.3764874135376886e-05
Step: 5240, train/epoch: 1.2470252513885498
Step: 5250, train/loss: 0.07620000094175339
Step: 5250, train/grad_norm: 1.0607947160679032e-06
Step: 5250, train/learning_rate: 4.375297430669889e-05
Step: 5250, train/epoch: 1.249405026435852
Step: 5260, train/loss: 0.0010000000474974513
Step: 5260, train/grad_norm: 0.0019857885781675577
Step: 5260, train/learning_rate: 4.374107447802089e-05
Step: 5260, train/epoch: 1.2517849206924438
Step: 5270, train/loss: 0.0
Step: 5270, train/grad_norm: 7.108338468242437e-05
Step: 5270, train/learning_rate: 4.37291782873217e-05
Step: 5270, train/epoch: 1.254164695739746
Step: 5280, train/loss: 0.0
Step: 5280, train/grad_norm: 7.207730959635228e-05
Step: 5280, train/learning_rate: 4.3717278458643705e-05
Step: 5280, train/epoch: 1.2565444707870483
Step: 5290, train/loss: 0.19259999692440033
Step: 5290, train/grad_norm: 36.74481964111328
Step: 5290, train/learning_rate: 4.370537862996571e-05
Step: 5290, train/epoch: 1.2589243650436401
Step: 5300, train/loss: 9.999999747378752e-05
Step: 5300, train/grad_norm: 0.005637043621391058
Step: 5300, train/learning_rate: 4.369347880128771e-05
Step: 5300, train/epoch: 1.2613041400909424
Step: 5310, train/loss: 0.0013000000035390258
Step: 5310, train/grad_norm: 1.898485243145842e-05
Step: 5310, train/learning_rate: 4.3681578972609714e-05
Step: 5310, train/epoch: 1.2636839151382446
Step: 5320, train/loss: 0.0
Step: 5320, train/grad_norm: 0.0003046954807359725
Step: 5320, train/learning_rate: 4.3669682781910524e-05
Step: 5320, train/epoch: 1.2660638093948364
Step: 5330, train/loss: 0.0
Step: 5330, train/grad_norm: 1.6575220797676593e-05
Step: 5330, train/learning_rate: 4.365778295323253e-05
Step: 5330, train/epoch: 1.2684435844421387
Step: 5340, train/loss: 0.0
Step: 5340, train/grad_norm: 1.4038276958672213e-06
Step: 5340, train/learning_rate: 4.364588312455453e-05
Step: 5340, train/epoch: 1.270823359489441
Step: 5350, train/loss: 0.0
Step: 5350, train/grad_norm: 1.4563529475708492e-05
Step: 5350, train/learning_rate: 4.363398329587653e-05
Step: 5350, train/epoch: 1.2732032537460327
Step: 5360, train/loss: 0.0
Step: 5360, train/grad_norm: 6.264713738346472e-05
Step: 5360, train/learning_rate: 4.3622083467198536e-05
Step: 5360, train/epoch: 1.275583028793335
Step: 5370, train/loss: 0.01810000091791153
Step: 5370, train/grad_norm: 9.939631127053872e-06
Step: 5370, train/learning_rate: 4.3610187276499346e-05
Step: 5370, train/epoch: 1.2779629230499268
Step: 5380, train/loss: 0.14059999585151672
Step: 5380, train/grad_norm: 1.7673768525128253e-05
Step: 5380, train/learning_rate: 4.359828744782135e-05
Step: 5380, train/epoch: 1.280342698097229
Step: 5390, train/loss: 0.003800000064074993
Step: 5390, train/grad_norm: 0.0002731134882196784
Step: 5390, train/learning_rate: 4.358638761914335e-05
Step: 5390, train/epoch: 1.2827224731445312
Step: 5400, train/loss: 0.0
Step: 5400, train/grad_norm: 2.7578351364354603e-05
Step: 5400, train/learning_rate: 4.3574487790465355e-05
Step: 5400, train/epoch: 1.285102367401123
Step: 5410, train/loss: 0.040800001472234726
Step: 5410, train/grad_norm: 3.7053618143545464e-05
Step: 5410, train/learning_rate: 4.356258796178736e-05
Step: 5410, train/epoch: 1.2874821424484253
Step: 5420, train/loss: 9.999999747378752e-05
Step: 5420, train/grad_norm: 0.16190174221992493
Step: 5420, train/learning_rate: 4.355069177108817e-05
Step: 5420, train/epoch: 1.2898619174957275
Step: 5430, train/loss: 0.13359999656677246
Step: 5430, train/grad_norm: 0.0002104712330037728
Step: 5430, train/learning_rate: 4.353879194241017e-05
Step: 5430, train/epoch: 1.2922418117523193
Step: 5440, train/loss: 0.0
Step: 5440, train/grad_norm: 3.9741164073348045e-05
Step: 5440, train/learning_rate: 4.3526892113732174e-05
Step: 5440, train/epoch: 1.2946215867996216
Step: 5450, train/loss: 0.0
Step: 5450, train/grad_norm: 0.00024724783725105226
Step: 5450, train/learning_rate: 4.351499228505418e-05
Step: 5450, train/epoch: 1.2970014810562134
Step: 5460, train/loss: 0.0
Step: 5460, train/grad_norm: 1.5176171473285649e-05
Step: 5460, train/learning_rate: 4.350309245637618e-05
Step: 5460, train/epoch: 1.2993812561035156
Step: 5470, train/loss: 0.0
Step: 5470, train/grad_norm: 0.00040847380296327174
Step: 5470, train/learning_rate: 4.349119626567699e-05
Step: 5470, train/epoch: 1.3017610311508179
Step: 5480, train/loss: 9.999999747378752e-05
Step: 5480, train/grad_norm: 0.0001499941135989502
Step: 5480, train/learning_rate: 4.347929643699899e-05
Step: 5480, train/epoch: 1.3041409254074097
Step: 5490, train/loss: 0.0
Step: 5490, train/grad_norm: 9.46541695157066e-06
Step: 5490, train/learning_rate: 4.3467396608320996e-05
Step: 5490, train/epoch: 1.306520700454712
Step: 5500, train/loss: 0.0
Step: 5500, train/grad_norm: 0.18245486915111542
Step: 5500, train/learning_rate: 4.3455496779643e-05
Step: 5500, train/epoch: 1.3089004755020142
Step: 5510, train/loss: 9.999999747378752e-05
Step: 5510, train/grad_norm: 1.000903739623027e-05
Step: 5510, train/learning_rate: 4.3443596950965e-05
Step: 5510, train/epoch: 1.311280369758606
Step: 5520, train/loss: 0.0
Step: 5520, train/grad_norm: 0.00015068062930367887
Step: 5520, train/learning_rate: 4.343170076026581e-05
Step: 5520, train/epoch: 1.3136601448059082
Step: 5530, train/loss: 0.0
Step: 5530, train/grad_norm: 0.0005596984992735088
Step: 5530, train/learning_rate: 4.3419800931587815e-05
Step: 5530, train/epoch: 1.3160400390625
Step: 5540, train/loss: 0.0044999998062849045
Step: 5540, train/grad_norm: 1.1390127383492654e-06
Step: 5540, train/learning_rate: 4.340790110290982e-05
Step: 5540, train/epoch: 1.3184198141098022
Step: 5550, train/loss: 0.0658000037074089
Step: 5550, train/grad_norm: 236.5458221435547
Step: 5550, train/learning_rate: 4.339600127423182e-05
Step: 5550, train/epoch: 1.3207995891571045
Step: 5560, train/loss: 0.0
Step: 5560, train/grad_norm: 0.001882264157757163
Step: 5560, train/learning_rate: 4.3384101445553824e-05
Step: 5560, train/epoch: 1.3231794834136963
Step: 5570, train/loss: 0.0
Step: 5570, train/grad_norm: 2.0145582311670296e-05
Step: 5570, train/learning_rate: 4.3372205254854634e-05
Step: 5570, train/epoch: 1.3255592584609985
Step: 5580, train/loss: 0.4250999987125397
Step: 5580, train/grad_norm: 4.886801616521552e-05
Step: 5580, train/learning_rate: 4.336030542617664e-05
Step: 5580, train/epoch: 1.3279390335083008
Step: 5590, train/loss: 0.02019999921321869
Step: 5590, train/grad_norm: 0.00772937573492527
Step: 5590, train/learning_rate: 4.334840559749864e-05
Step: 5590, train/epoch: 1.3303189277648926
Step: 5600, train/loss: 0.013500000350177288
Step: 5600, train/grad_norm: 0.0012730532325804234
Step: 5600, train/learning_rate: 4.3336505768820643e-05
Step: 5600, train/epoch: 1.3326987028121948
Step: 5610, train/loss: 9.999999747378752e-05
Step: 5610, train/grad_norm: 5.1387414714554325e-05
Step: 5610, train/learning_rate: 4.3324605940142646e-05
Step: 5610, train/epoch: 1.335078477859497
Step: 5620, train/loss: 0.016599999740719795
Step: 5620, train/grad_norm: 0.009562662802636623
Step: 5620, train/learning_rate: 4.3312709749443457e-05
Step: 5620, train/epoch: 1.3374583721160889
Step: 5630, train/loss: 9.999999747378752e-05
Step: 5630, train/grad_norm: 0.00023595915990881622
Step: 5630, train/learning_rate: 4.330080992076546e-05
Step: 5630, train/epoch: 1.3398381471633911
Step: 5640, train/loss: 0.0
Step: 5640, train/grad_norm: 0.0012986132642254233
Step: 5640, train/learning_rate: 4.328891009208746e-05
Step: 5640, train/epoch: 1.342218041419983
Step: 5650, train/loss: 0.0
Step: 5650, train/grad_norm: 0.00010511715663596988
Step: 5650, train/learning_rate: 4.3277010263409466e-05
Step: 5650, train/epoch: 1.3445978164672852
Step: 5660, train/loss: 0.0
Step: 5660, train/grad_norm: 1.2990113646083046e-05
Step: 5660, train/learning_rate: 4.326511043473147e-05
Step: 5660, train/epoch: 1.3469775915145874
Step: 5670, train/loss: 0.007000000216066837
Step: 5670, train/grad_norm: 6.386118184309453e-05
Step: 5670, train/learning_rate: 4.325321424403228e-05
Step: 5670, train/epoch: 1.3493574857711792
Step: 5680, train/loss: 0.0
Step: 5680, train/grad_norm: 2.2522443032357842e-05
Step: 5680, train/learning_rate: 4.324131441535428e-05
Step: 5680, train/epoch: 1.3517372608184814
Step: 5690, train/loss: 0.0
Step: 5690, train/grad_norm: 4.2196011236228514e-06
Step: 5690, train/learning_rate: 4.3229414586676285e-05
Step: 5690, train/epoch: 1.3541170358657837
Step: 5700, train/loss: 0.01600000075995922
Step: 5700, train/grad_norm: 2.0533241695375182e-05
Step: 5700, train/learning_rate: 4.321751475799829e-05
Step: 5700, train/epoch: 1.3564969301223755
Step: 5710, train/loss: 0.0
Step: 5710, train/grad_norm: 6.204498390616209e-07
Step: 5710, train/learning_rate: 4.320561492932029e-05
Step: 5710, train/epoch: 1.3588767051696777
Step: 5720, train/loss: 0.0
Step: 5720, train/grad_norm: 0.00010833324631676078
Step: 5720, train/learning_rate: 4.31937187386211e-05
Step: 5720, train/epoch: 1.3612565994262695
Step: 5730, train/loss: 0.0
Step: 5730, train/grad_norm: 0.0007315760012716055
Step: 5730, train/learning_rate: 4.3181818909943104e-05
Step: 5730, train/epoch: 1.3636363744735718
Step: 5740, train/loss: 0.0
Step: 5740, train/grad_norm: 0.0001798958401195705
Step: 5740, train/learning_rate: 4.316991908126511e-05
Step: 5740, train/epoch: 1.366016149520874
Step: 5750, train/loss: 0.0
Step: 5750, train/grad_norm: 9.489131116424687e-06
Step: 5750, train/learning_rate: 4.315801925258711e-05
Step: 5750, train/epoch: 1.3683960437774658
Step: 5760, train/loss: 0.0
Step: 5760, train/grad_norm: 2.624184162414167e-05
Step: 5760, train/learning_rate: 4.314611942390911e-05
Step: 5760, train/epoch: 1.370775818824768
Step: 5770, train/loss: 0.0
Step: 5770, train/grad_norm: 1.1049389740946935e-06
Step: 5770, train/learning_rate: 4.313422323320992e-05
Step: 5770, train/epoch: 1.3731555938720703
Step: 5780, train/loss: 0.0
Step: 5780, train/grad_norm: 0.00012059495929861441
Step: 5780, train/learning_rate: 4.3122323404531926e-05
Step: 5780, train/epoch: 1.375535488128662
Step: 5790, train/loss: 0.0
Step: 5790, train/grad_norm: 0.00028871523682028055
Step: 5790, train/learning_rate: 4.311042357585393e-05
Step: 5790, train/epoch: 1.3779152631759644
Step: 5800, train/loss: 0.06520000100135803
Step: 5800, train/grad_norm: 7.288345659617335e-05
Step: 5800, train/learning_rate: 4.309852374717593e-05
Step: 5800, train/epoch: 1.3802950382232666
Step: 5810, train/loss: 0.0019000000320374966
Step: 5810, train/grad_norm: 2.662948281795252e-05
Step: 5810, train/learning_rate: 4.3086623918497935e-05
Step: 5810, train/epoch: 1.3826749324798584
Step: 5820, train/loss: 9.999999747378752e-05
Step: 5820, train/grad_norm: 0.01830093376338482
Step: 5820, train/learning_rate: 4.3074727727798745e-05
Step: 5820, train/epoch: 1.3850547075271606
Step: 5830, train/loss: 9.999999747378752e-05
Step: 5830, train/grad_norm: 0.1037079393863678
Step: 5830, train/learning_rate: 4.306282789912075e-05
Step: 5830, train/epoch: 1.3874346017837524
Step: 5840, train/loss: 0.21719999611377716
Step: 5840, train/grad_norm: 0.004236895591020584
Step: 5840, train/learning_rate: 4.305092807044275e-05
Step: 5840, train/epoch: 1.3898143768310547
Step: 5850, train/loss: 0.26409998536109924
Step: 5850, train/grad_norm: 40.939083099365234
Step: 5850, train/learning_rate: 4.3039028241764754e-05
Step: 5850, train/epoch: 1.392194151878357
Step: 5860, train/loss: 0.035599999129772186
Step: 5860, train/grad_norm: 0.018888132646679878
Step: 5860, train/learning_rate: 4.302712841308676e-05
Step: 5860, train/epoch: 1.3945740461349487
Step: 5870, train/loss: 0.0003000000142492354
Step: 5870, train/grad_norm: 0.02125748246908188
Step: 5870, train/learning_rate: 4.301523222238757e-05
Step: 5870, train/epoch: 1.396953821182251
Step: 5880, train/loss: 9.999999747378752e-05
Step: 5880, train/grad_norm: 0.0004212303610984236
Step: 5880, train/learning_rate: 4.300333239370957e-05
Step: 5880, train/epoch: 1.3993335962295532
Step: 5890, train/loss: 0.0
Step: 5890, train/grad_norm: 0.0001641910639591515
Step: 5890, train/learning_rate: 4.299143256503157e-05
Step: 5890, train/epoch: 1.401713490486145
Step: 5900, train/loss: 0.06560000032186508
Step: 5900, train/grad_norm: 3.51725539076142e-05
Step: 5900, train/learning_rate: 4.2979532736353576e-05
Step: 5900, train/epoch: 1.4040932655334473
Step: 5910, train/loss: 0.0
Step: 5910, train/grad_norm: 8.896441613615025e-06
Step: 5910, train/learning_rate: 4.296763290767558e-05
Step: 5910, train/epoch: 1.406473159790039
Step: 5920, train/loss: 0.0
Step: 5920, train/grad_norm: 0.001758175203576684
Step: 5920, train/learning_rate: 4.295573671697639e-05
Step: 5920, train/epoch: 1.4088529348373413
Step: 5930, train/loss: 0.0
Step: 5930, train/grad_norm: 6.316345661616651e-06
Step: 5930, train/learning_rate: 4.294383688829839e-05
Step: 5930, train/epoch: 1.4112327098846436
Step: 5940, train/loss: 0.0
Step: 5940, train/grad_norm: 2.9986697427375475e-06
Step: 5940, train/learning_rate: 4.2931937059620395e-05
Step: 5940, train/epoch: 1.4136126041412354
Step: 5950, train/loss: 0.16859999299049377
Step: 5950, train/grad_norm: 2.9517434086301364e-05
Step: 5950, train/learning_rate: 4.29200372309424e-05
Step: 5950, train/epoch: 1.4159923791885376
Step: 5960, train/loss: 0.0
Step: 5960, train/grad_norm: 5.333403350959998e-06
Step: 5960, train/learning_rate: 4.29081374022644e-05
Step: 5960, train/epoch: 1.4183721542358398
Step: 5970, train/loss: 0.0
Step: 5970, train/grad_norm: 3.715775892487727e-05
Step: 5970, train/learning_rate: 4.289624121156521e-05
Step: 5970, train/epoch: 1.4207520484924316
Step: 5980, train/loss: 0.0
Step: 5980, train/grad_norm: 1.2385740774334408e-05
Step: 5980, train/learning_rate: 4.2884341382887214e-05
Step: 5980, train/epoch: 1.4231318235397339
Step: 5990, train/loss: 0.0
Step: 5990, train/grad_norm: 3.293484951427672e-06
Step: 5990, train/learning_rate: 4.287244155420922e-05
Step: 5990, train/epoch: 1.4255117177963257
Step: 6000, train/loss: 0.0
Step: 6000, train/grad_norm: 8.317008905578405e-06
Step: 6000, train/learning_rate: 4.286054172553122e-05
Step: 6000, train/epoch: 1.427891492843628
Step: 6010, train/loss: 0.0
Step: 6010, train/grad_norm: 0.08858087658882141
Step: 6010, train/learning_rate: 4.2848641896853223e-05
Step: 6010, train/epoch: 1.4302712678909302
Step: 6020, train/loss: 0.019500000402331352
Step: 6020, train/grad_norm: 6.03284817657368e-08
Step: 6020, train/learning_rate: 4.2836745706154034e-05
Step: 6020, train/epoch: 1.432651162147522
Step: 6030, train/loss: 0.1257999986410141
Step: 6030, train/grad_norm: 0.00018935582193080336
Step: 6030, train/learning_rate: 4.2824845877476037e-05
Step: 6030, train/epoch: 1.4350309371948242
Step: 6040, train/loss: 0.0008999999845400453
Step: 6040, train/grad_norm: 0.00010205471335211769
Step: 6040, train/learning_rate: 4.281294604879804e-05
Step: 6040, train/epoch: 1.4374107122421265
Step: 6050, train/loss: 0.08630000054836273
Step: 6050, train/grad_norm: 0.00034676247742027044
Step: 6050, train/learning_rate: 4.280104622012004e-05
Step: 6050, train/epoch: 1.4397906064987183
Step: 6060, train/loss: 0.02449999935925007
Step: 6060, train/grad_norm: 0.003138727741315961
Step: 6060, train/learning_rate: 4.2789146391442046e-05
Step: 6060, train/epoch: 1.4421703815460205
Step: 6070, train/loss: 0.0
Step: 6070, train/grad_norm: 0.00019863586931023747
Step: 6070, train/learning_rate: 4.2777250200742856e-05
Step: 6070, train/epoch: 1.4445501565933228
Step: 6080, train/loss: 0.0005000000237487257
Step: 6080, train/grad_norm: 0.0018414724618196487
Step: 6080, train/learning_rate: 4.276535037206486e-05
Step: 6080, train/epoch: 1.4469300508499146
Step: 6090, train/loss: 0.0
Step: 6090, train/grad_norm: 0.00025534353335388005
Step: 6090, train/learning_rate: 4.275345054338686e-05
Step: 6090, train/epoch: 1.4493098258972168
Step: 6100, train/loss: 0.0019000000320374966
Step: 6100, train/grad_norm: 0.14666490256786346
Step: 6100, train/learning_rate: 4.2741550714708865e-05
Step: 6100, train/epoch: 1.4516897201538086
Step: 6110, train/loss: 0.0
Step: 6110, train/grad_norm: 1.611532388778869e-05
Step: 6110, train/learning_rate: 4.272965088603087e-05
Step: 6110, train/epoch: 1.4540694952011108
Step: 6120, train/loss: 0.05700000002980232
Step: 6120, train/grad_norm: 345.65789794921875
Step: 6120, train/learning_rate: 4.271775469533168e-05
Step: 6120, train/epoch: 1.456449270248413
Step: 6130, train/loss: 0.0
Step: 6130, train/grad_norm: 3.7946806514810305e-06
Step: 6130, train/learning_rate: 4.270585486665368e-05
Step: 6130, train/epoch: 1.4588291645050049
Step: 6140, train/loss: 0.11860000342130661
Step: 6140, train/grad_norm: 0.0012665175599977374
Step: 6140, train/learning_rate: 4.2693955037975684e-05
Step: 6140, train/epoch: 1.4612089395523071
Step: 6150, train/loss: 0.002199999988079071
Step: 6150, train/grad_norm: 0.0006936051067896187
Step: 6150, train/learning_rate: 4.268205520929769e-05
Step: 6150, train/epoch: 1.4635887145996094
Step: 6160, train/loss: 0.0
Step: 6160, train/grad_norm: 2.198644506279379e-05
Step: 6160, train/learning_rate: 4.267015538061969e-05
Step: 6160, train/epoch: 1.4659686088562012
Step: 6170, train/loss: 0.0
Step: 6170, train/grad_norm: 5.010103268432431e-05
Step: 6170, train/learning_rate: 4.26582591899205e-05
Step: 6170, train/epoch: 1.4683483839035034
Step: 6180, train/loss: 0.0
Step: 6180, train/grad_norm: 1.7189829577546334e-06
Step: 6180, train/learning_rate: 4.26463593612425e-05
Step: 6180, train/epoch: 1.4707282781600952
Step: 6190, train/loss: 0.0
Step: 6190, train/grad_norm: 1.6222421663769637e-06
Step: 6190, train/learning_rate: 4.2634459532564506e-05
Step: 6190, train/epoch: 1.4731080532073975
Step: 6200, train/loss: 0.0
Step: 6200, train/grad_norm: 0.0008923175628297031
Step: 6200, train/learning_rate: 4.262255970388651e-05
Step: 6200, train/epoch: 1.4754878282546997
Step: 6210, train/loss: 0.0
Step: 6210, train/grad_norm: 0.0003287445870228112
Step: 6210, train/learning_rate: 4.261065987520851e-05
Step: 6210, train/epoch: 1.4778677225112915
Step: 6220, train/loss: 0.0006000000284984708
Step: 6220, train/grad_norm: 9.976181172532961e-05
Step: 6220, train/learning_rate: 4.259876368450932e-05
Step: 6220, train/epoch: 1.4802474975585938
Step: 6230, train/loss: 0.08829999715089798
Step: 6230, train/grad_norm: 0.0006524306954815984
Step: 6230, train/learning_rate: 4.2586863855831325e-05
Step: 6230, train/epoch: 1.482627272605896
Step: 6240, train/loss: 0.11800000071525574
Step: 6240, train/grad_norm: 0.0018485235050320625
Step: 6240, train/learning_rate: 4.257496402715333e-05
Step: 6240, train/epoch: 1.4850071668624878
Step: 6250, train/loss: 0.009800000116229057
Step: 6250, train/grad_norm: 0.0013779450673609972
Step: 6250, train/learning_rate: 4.256306419847533e-05
Step: 6250, train/epoch: 1.48738694190979
Step: 6260, train/loss: 0.020400000736117363
Step: 6260, train/grad_norm: 218.91552734375
Step: 6260, train/learning_rate: 4.2551164369797334e-05
Step: 6260, train/epoch: 1.4897668361663818
Step: 6270, train/loss: 0.007400000002235174
Step: 6270, train/grad_norm: 0.06237147003412247
Step: 6270, train/learning_rate: 4.2539268179098144e-05
Step: 6270, train/epoch: 1.492146611213684
Step: 6280, train/loss: 9.999999747378752e-05
Step: 6280, train/grad_norm: 0.00011596514377743006
Step: 6280, train/learning_rate: 4.252736835042015e-05
Step: 6280, train/epoch: 1.4945263862609863
Step: 6290, train/loss: 0.10660000145435333
Step: 6290, train/grad_norm: 1.7779779227566905e-05
Step: 6290, train/learning_rate: 4.251546852174215e-05
Step: 6290, train/epoch: 1.4969062805175781
Step: 6300, train/loss: 0.0
Step: 6300, train/grad_norm: 1.6794798284536228e-05
Step: 6300, train/learning_rate: 4.250356869306415e-05
Step: 6300, train/epoch: 1.4992860555648804
Step: 6310, train/loss: 0.0
Step: 6310, train/grad_norm: 0.0005507290479727089
Step: 6310, train/learning_rate: 4.2491668864386156e-05
Step: 6310, train/epoch: 1.5016658306121826
Step: 6320, train/loss: 0.1023000031709671
Step: 6320, train/grad_norm: 3.456133708823472e-05
Step: 6320, train/learning_rate: 4.2479772673686966e-05
Step: 6320, train/epoch: 1.5040457248687744
Step: 6330, train/loss: 0.08489999920129776
Step: 6330, train/grad_norm: 3.2409232517238706e-05
Step: 6330, train/learning_rate: 4.246787284500897e-05
Step: 6330, train/epoch: 1.5064254999160767
Step: 6340, train/loss: 0.0
Step: 6340, train/grad_norm: 2.9008388082729653e-05
Step: 6340, train/learning_rate: 4.245597301633097e-05
Step: 6340, train/epoch: 1.508805274963379
Step: 6350, train/loss: 0.00019999999494757503
Step: 6350, train/grad_norm: 4.4292592065175995e-05
Step: 6350, train/learning_rate: 4.2444073187652975e-05
Step: 6350, train/epoch: 1.5111851692199707
Step: 6360, train/loss: 0.0
Step: 6360, train/grad_norm: 1.2590666301548481e-05
Step: 6360, train/learning_rate: 4.243217335897498e-05
Step: 6360, train/epoch: 1.513564944267273
Step: 6370, train/loss: 0.0
Step: 6370, train/grad_norm: 6.239559297682717e-05
Step: 6370, train/learning_rate: 4.242027716827579e-05
Step: 6370, train/epoch: 1.5159448385238647
Step: 6380, train/loss: 0.09839999675750732
Step: 6380, train/grad_norm: 0.0004226818564347923
Step: 6380, train/learning_rate: 4.240837733959779e-05
Step: 6380, train/epoch: 1.518324613571167
Step: 6390, train/loss: 0.000699999975040555
Step: 6390, train/grad_norm: 11.22902774810791
Step: 6390, train/learning_rate: 4.2396477510919794e-05
Step: 6390, train/epoch: 1.5207043886184692
Step: 6400, train/loss: 0.0
Step: 6400, train/grad_norm: 5.24359529663343e-06
Step: 6400, train/learning_rate: 4.23845776822418e-05
Step: 6400, train/epoch: 1.523084282875061
Step: 6410, train/loss: 0.0
Step: 6410, train/grad_norm: 1.14047452370869e-06
Step: 6410, train/learning_rate: 4.237268149154261e-05
Step: 6410, train/epoch: 1.5254640579223633
Step: 6420, train/loss: 0.1875
Step: 6420, train/grad_norm: 9.907289495458826e-05
Step: 6420, train/learning_rate: 4.236078166286461e-05
Step: 6420, train/epoch: 1.5278438329696655
Step: 6430, train/loss: 0.0
Step: 6430, train/grad_norm: 6.21301878709346e-05
Step: 6430, train/learning_rate: 4.2348881834186614e-05
Step: 6430, train/epoch: 1.5302237272262573
Step: 6440, train/loss: 0.0
Step: 6440, train/grad_norm: 4.822258233616594e-06
Step: 6440, train/learning_rate: 4.2336982005508617e-05
Step: 6440, train/epoch: 1.5326035022735596
Step: 6450, train/loss: 0.0
Step: 6450, train/grad_norm: 0.00010513036249903962
Step: 6450, train/learning_rate: 4.232508217683062e-05
Step: 6450, train/epoch: 1.5349833965301514
Step: 6460, train/loss: 0.09860000014305115
Step: 6460, train/grad_norm: 0.00016596874047536403
Step: 6460, train/learning_rate: 4.231318598613143e-05
Step: 6460, train/epoch: 1.5373631715774536
Step: 6470, train/loss: 0.0019000000320374966
Step: 6470, train/grad_norm: 0.0008955363882705569
Step: 6470, train/learning_rate: 4.230128615745343e-05
Step: 6470, train/epoch: 1.5397429466247559
Step: 6480, train/loss: 0.0
Step: 6480, train/grad_norm: 0.008997468277812004
Step: 6480, train/learning_rate: 4.2289386328775436e-05
Step: 6480, train/epoch: 1.5421228408813477
Step: 6490, train/loss: 0.0
Step: 6490, train/grad_norm: 1.3242019122117199e-05
Step: 6490, train/learning_rate: 4.227748650009744e-05
Step: 6490, train/epoch: 1.54450261592865
Step: 6500, train/loss: 0.0
Step: 6500, train/grad_norm: 0.340777188539505
Step: 6500, train/learning_rate: 4.226558667141944e-05
Step: 6500, train/epoch: 1.5468823909759521
Step: 6510, train/loss: 0.03629999980330467
Step: 6510, train/grad_norm: 3.999600153292704e-07
Step: 6510, train/learning_rate: 4.225369048072025e-05
Step: 6510, train/epoch: 1.549262285232544
Step: 6520, train/loss: 0.0010999999940395355
Step: 6520, train/grad_norm: 2.18046970985597e-05
Step: 6520, train/learning_rate: 4.2241790652042255e-05
Step: 6520, train/epoch: 1.5516420602798462
Step: 6530, train/loss: 0.0
Step: 6530, train/grad_norm: 0.00025946591631509364
Step: 6530, train/learning_rate: 4.222989082336426e-05
Step: 6530, train/epoch: 1.5540218353271484
Step: 6540, train/loss: 0.0
Step: 6540, train/grad_norm: 0.00017943218699656427
Step: 6540, train/learning_rate: 4.221799099468626e-05
Step: 6540, train/epoch: 1.5564017295837402
Step: 6550, train/loss: 0.026000000536441803
Step: 6550, train/grad_norm: 5.252969640423544e-05
Step: 6550, train/learning_rate: 4.2206091166008264e-05
Step: 6550, train/epoch: 1.5587815046310425
Step: 6560, train/loss: 0.01810000091791153
Step: 6560, train/grad_norm: 8.196735507226549e-06
Step: 6560, train/learning_rate: 4.2194194975309074e-05
Step: 6560, train/epoch: 1.5611613988876343
Step: 6570, train/loss: 0.0
Step: 6570, train/grad_norm: 1.9481341951177455e-05
Step: 6570, train/learning_rate: 4.218229514663108e-05
Step: 6570, train/epoch: 1.5635411739349365
Step: 6580, train/loss: 0.0
Step: 6580, train/grad_norm: 6.0160757129779086e-05
Step: 6580, train/learning_rate: 4.217039531795308e-05
Step: 6580, train/epoch: 1.5659209489822388
Step: 6590, train/loss: 0.0
Step: 6590, train/grad_norm: 9.17099168873392e-05
Step: 6590, train/learning_rate: 4.215849548927508e-05
Step: 6590, train/epoch: 1.5683008432388306
Step: 6600, train/loss: 0.0
Step: 6600, train/grad_norm: 0.3683793544769287
Step: 6600, train/learning_rate: 4.2146595660597086e-05
Step: 6600, train/epoch: 1.5706806182861328
Step: 6610, train/loss: 0.0
Step: 6610, train/grad_norm: 3.82039143005386e-05
Step: 6610, train/learning_rate: 4.2134699469897896e-05
Step: 6610, train/epoch: 1.573060393333435
Step: 6620, train/loss: 0.0
Step: 6620, train/grad_norm: 0.00015257074846886098
Step: 6620, train/learning_rate: 4.21227996412199e-05
Step: 6620, train/epoch: 1.5754402875900269
Step: 6630, train/loss: 0.0
Step: 6630, train/grad_norm: 7.344501500483602e-05
Step: 6630, train/learning_rate: 4.21108998125419e-05
Step: 6630, train/epoch: 1.577820062637329
Step: 6640, train/loss: 0.0
Step: 6640, train/grad_norm: 1.3089622825646074e-06
Step: 6640, train/learning_rate: 4.2098999983863905e-05
Step: 6640, train/epoch: 1.580199956893921
Step: 6650, train/loss: 9.999999747378752e-05
Step: 6650, train/grad_norm: 7.375043060164899e-05
Step: 6650, train/learning_rate: 4.208710015518591e-05
Step: 6650, train/epoch: 1.5825797319412231
Step: 6660, train/loss: 0.00039999998989515007
Step: 6660, train/grad_norm: 0.0014927636366337538
Step: 6660, train/learning_rate: 4.207520396448672e-05
Step: 6660, train/epoch: 1.5849595069885254
Step: 6670, train/loss: 0.0
Step: 6670, train/grad_norm: 1.4152412575185735e-07
Step: 6670, train/learning_rate: 4.206330413580872e-05
Step: 6670, train/epoch: 1.5873394012451172
Step: 6680, train/loss: 0.0
Step: 6680, train/grad_norm: 6.0110596677986905e-05
Step: 6680, train/learning_rate: 4.2051404307130724e-05
Step: 6680, train/epoch: 1.5897191762924194
Step: 6690, train/loss: 0.02590000070631504
Step: 6690, train/grad_norm: 2.1592232712919213e-07
Step: 6690, train/learning_rate: 4.203950447845273e-05
Step: 6690, train/epoch: 1.5920989513397217
Step: 6700, train/loss: 0.1656000018119812
Step: 6700, train/grad_norm: 0.0016450861003249884
Step: 6700, train/learning_rate: 4.202760464977473e-05
Step: 6700, train/epoch: 1.5944788455963135
Step: 6710, train/loss: 0.003700000001117587
Step: 6710, train/grad_norm: 9.518308797851205e-05
Step: 6710, train/learning_rate: 4.201570845907554e-05
Step: 6710, train/epoch: 1.5968586206436157
Step: 6720, train/loss: 0.0035000001080334187
Step: 6720, train/grad_norm: 39.57929229736328
Step: 6720, train/learning_rate: 4.200380863039754e-05
Step: 6720, train/epoch: 1.5992385149002075
Step: 6730, train/loss: 0.0
Step: 6730, train/grad_norm: 4.382127372082323e-06
Step: 6730, train/learning_rate: 4.1991908801719546e-05
Step: 6730, train/epoch: 1.6016182899475098
Step: 6740, train/loss: 0.0
Step: 6740, train/grad_norm: 0.00044692677329294384
Step: 6740, train/learning_rate: 4.198000897304155e-05
Step: 6740, train/epoch: 1.603998064994812
Step: 6750, train/loss: 0.0
Step: 6750, train/grad_norm: 0.0003106226213276386
Step: 6750, train/learning_rate: 4.196810914436355e-05
Step: 6750, train/epoch: 1.6063779592514038
Step: 6760, train/loss: 0.09730000048875809
Step: 6760, train/grad_norm: 2.943298932223115e-05
Step: 6760, train/learning_rate: 4.195621295366436e-05
Step: 6760, train/epoch: 1.608757734298706
Step: 6770, train/loss: 0.0
Step: 6770, train/grad_norm: 0.00023031274031382054
Step: 6770, train/learning_rate: 4.1944313124986365e-05
Step: 6770, train/epoch: 1.6111375093460083
Step: 6780, train/loss: 0.003000000026077032
Step: 6780, train/grad_norm: 3.615490868469351e-06
Step: 6780, train/learning_rate: 4.193241329630837e-05
Step: 6780, train/epoch: 1.6135174036026
Step: 6790, train/loss: 0.0
Step: 6790, train/grad_norm: 7.467043161568654e-08
Step: 6790, train/learning_rate: 4.192051346763037e-05
Step: 6790, train/epoch: 1.6158971786499023
Step: 6800, train/loss: 0.0
Step: 6800, train/grad_norm: 0.0056501044891774654
Step: 6800, train/learning_rate: 4.1908613638952374e-05
Step: 6800, train/epoch: 1.6182769536972046
Step: 6810, train/loss: 0.0
Step: 6810, train/grad_norm: 4.655958491639467e-06
Step: 6810, train/learning_rate: 4.1896717448253185e-05
Step: 6810, train/epoch: 1.6206568479537964
Step: 6820, train/loss: 0.0
Step: 6820, train/grad_norm: 8.017926234060724e-07
Step: 6820, train/learning_rate: 4.188481761957519e-05
Step: 6820, train/epoch: 1.6230366230010986
Step: 6830, train/loss: 0.0
Step: 6830, train/grad_norm: 4.818179149879143e-06
Step: 6830, train/learning_rate: 4.187291779089719e-05
Step: 6830, train/epoch: 1.6254165172576904
Step: 6840, train/loss: 0.0
Step: 6840, train/grad_norm: 0.00010623252455843613
Step: 6840, train/learning_rate: 4.1861017962219194e-05
Step: 6840, train/epoch: 1.6277962923049927
Step: 6850, train/loss: 0.0
Step: 6850, train/grad_norm: 9.726550160849001e-06
Step: 6850, train/learning_rate: 4.1849118133541197e-05
Step: 6850, train/epoch: 1.630176067352295
Step: 6860, train/loss: 0.17339999973773956
Step: 6860, train/grad_norm: 6.6698259615805e-05
Step: 6860, train/learning_rate: 4.183722194284201e-05
Step: 6860, train/epoch: 1.6325559616088867
Step: 6870, train/loss: 0.0
Step: 6870, train/grad_norm: 9.169044460577425e-06
Step: 6870, train/learning_rate: 4.182532211416401e-05
Step: 6870, train/epoch: 1.634935736656189
Step: 6880, train/loss: 0.08699999749660492
Step: 6880, train/grad_norm: 131.61953735351562
Step: 6880, train/learning_rate: 4.181342228548601e-05
Step: 6880, train/epoch: 1.6373155117034912
Step: 6890, train/loss: 0.21879999339580536
Step: 6890, train/grad_norm: 2.968767876154743e-05
Step: 6890, train/learning_rate: 4.1801522456808016e-05
Step: 6890, train/epoch: 1.639695405960083
Step: 6900, train/loss: 0.0013000000035390258
Step: 6900, train/grad_norm: 12.083600044250488
Step: 6900, train/learning_rate: 4.178962262813002e-05
Step: 6900, train/epoch: 1.6420751810073853
Step: 6910, train/loss: 0.022299999371170998
Step: 6910, train/grad_norm: 0.0001867511309683323
Step: 6910, train/learning_rate: 4.177772643743083e-05
Step: 6910, train/epoch: 1.644455075263977
Step: 6920, train/loss: 0.0
Step: 6920, train/grad_norm: 2.5299064873252064e-05
Step: 6920, train/learning_rate: 4.176582660875283e-05
Step: 6920, train/epoch: 1.6468348503112793
Step: 6930, train/loss: 0.06440000236034393
Step: 6930, train/grad_norm: 0.000858047918882221
Step: 6930, train/learning_rate: 4.1753926780074835e-05
Step: 6930, train/epoch: 1.6492146253585815
Step: 6940, train/loss: 0.00039999998989515007
Step: 6940, train/grad_norm: 1.313022858084878e-05
Step: 6940, train/learning_rate: 4.174202695139684e-05
Step: 6940, train/epoch: 1.6515945196151733
Step: 6950, train/loss: 0.0
Step: 6950, train/grad_norm: 2.668155502760783e-05
Step: 6950, train/learning_rate: 4.173012712271884e-05
Step: 6950, train/epoch: 1.6539742946624756
Step: 6960, train/loss: 0.0
Step: 6960, train/grad_norm: 0.0001512368326075375
Step: 6960, train/learning_rate: 4.171823093201965e-05
Step: 6960, train/epoch: 1.6563540697097778
Step: 6970, train/loss: 0.17970000207424164
Step: 6970, train/grad_norm: 0.002832237631082535
Step: 6970, train/learning_rate: 4.1706331103341654e-05
Step: 6970, train/epoch: 1.6587339639663696
Step: 6980, train/loss: 0.0010000000474974513
Step: 6980, train/grad_norm: 0.000345421489328146
Step: 6980, train/learning_rate: 4.169443127466366e-05
Step: 6980, train/epoch: 1.6611137390136719
Step: 6990, train/loss: 0.000699999975040555
Step: 6990, train/grad_norm: 0.001388126634992659
Step: 6990, train/learning_rate: 4.168253144598566e-05
Step: 6990, train/epoch: 1.6634936332702637
Step: 7000, train/loss: 0.05040000006556511
Step: 7000, train/grad_norm: 3.5925491829402745e-05
Step: 7000, train/learning_rate: 4.167063161730766e-05
Step: 7000, train/epoch: 1.665873408317566
Step: 7010, train/loss: 0.0
Step: 7010, train/grad_norm: 3.551857662387192e-05
Step: 7010, train/learning_rate: 4.165873542660847e-05
Step: 7010, train/epoch: 1.6682531833648682
Step: 7020, train/loss: 0.20270000398159027
Step: 7020, train/grad_norm: 1.2044063169014407e-06
Step: 7020, train/learning_rate: 4.1646835597930476e-05
Step: 7020, train/epoch: 1.67063307762146
Step: 7030, train/loss: 0.11810000240802765
Step: 7030, train/grad_norm: 0.28629356622695923
Step: 7030, train/learning_rate: 4.163493576925248e-05
Step: 7030, train/epoch: 1.6730128526687622
Step: 7040, train/loss: 0.010700000450015068
Step: 7040, train/grad_norm: 102.17096710205078
Step: 7040, train/learning_rate: 4.162303594057448e-05
Step: 7040, train/epoch: 1.6753926277160645
Step: 7050, train/loss: 9.999999747378752e-05
Step: 7050, train/grad_norm: 0.016889268532395363
Step: 7050, train/learning_rate: 4.1611136111896485e-05
Step: 7050, train/epoch: 1.6777725219726562
Step: 7060, train/loss: 0.0
Step: 7060, train/grad_norm: 3.5602945445134537e-06
Step: 7060, train/learning_rate: 4.1599239921197295e-05
Step: 7060, train/epoch: 1.6801522970199585
Step: 7070, train/loss: 0.0
Step: 7070, train/grad_norm: 0.0008748928084969521
Step: 7070, train/learning_rate: 4.15873400925193e-05
Step: 7070, train/epoch: 1.6825320720672607
Step: 7080, train/loss: 0.09920000284910202
Step: 7080, train/grad_norm: 0.00012305231939535588
Step: 7080, train/learning_rate: 4.15754402638413e-05
Step: 7080, train/epoch: 1.6849119663238525
Step: 7090, train/loss: 0.0
Step: 7090, train/grad_norm: 1.3564513210440055e-05
Step: 7090, train/learning_rate: 4.1563540435163304e-05
Step: 7090, train/epoch: 1.6872917413711548
Step: 7100, train/loss: 0.0
Step: 7100, train/grad_norm: 3.531884203766822e-06
Step: 7100, train/learning_rate: 4.155164060648531e-05
Step: 7100, train/epoch: 1.6896716356277466
Step: 7110, train/loss: 0.004699999932199717
Step: 7110, train/grad_norm: 2.8375328838592395e-05
Step: 7110, train/learning_rate: 4.153974441578612e-05
Step: 7110, train/epoch: 1.6920514106750488
Step: 7120, train/loss: 0.0
Step: 7120, train/grad_norm: 5.237179357209243e-05
Step: 7120, train/learning_rate: 4.152784458710812e-05
Step: 7120, train/epoch: 1.694431185722351
Step: 7130, train/loss: 0.0
Step: 7130, train/grad_norm: 5.260910711513134e-07
Step: 7130, train/learning_rate: 4.151594475843012e-05
Step: 7130, train/epoch: 1.6968110799789429
Step: 7140, train/loss: 0.0
Step: 7140, train/grad_norm: 8.220805369774098e-08
Step: 7140, train/learning_rate: 4.1504044929752126e-05
Step: 7140, train/epoch: 1.6991908550262451
Step: 7150, train/loss: 9.999999747378752e-05
Step: 7150, train/grad_norm: 8.805292850411206e-07
Step: 7150, train/learning_rate: 4.149214510107413e-05
Step: 7150, train/epoch: 1.7015706300735474
Step: 7160, train/loss: 0.0
Step: 7160, train/grad_norm: 1.2073406878698734e-06
Step: 7160, train/learning_rate: 4.148024891037494e-05
Step: 7160, train/epoch: 1.7039505243301392
Step: 7170, train/loss: 0.12269999831914902
Step: 7170, train/grad_norm: 6.671244818790001e-07
Step: 7170, train/learning_rate: 4.146834908169694e-05
Step: 7170, train/epoch: 1.7063302993774414
Step: 7180, train/loss: 0.0
Step: 7180, train/grad_norm: 0.0024719827342778444
Step: 7180, train/learning_rate: 4.1456449253018945e-05
Step: 7180, train/epoch: 1.7087101936340332
Step: 7190, train/loss: 0.01489999983459711
Step: 7190, train/grad_norm: 1.0903303518716712e-05
Step: 7190, train/learning_rate: 4.144454942434095e-05
Step: 7190, train/epoch: 1.7110899686813354
Step: 7200, train/loss: 0.0
Step: 7200, train/grad_norm: 8.6365929519161e-07
Step: 7200, train/learning_rate: 4.143264959566295e-05
Step: 7200, train/epoch: 1.7134697437286377
Step: 7210, train/loss: 0.16599999368190765
Step: 7210, train/grad_norm: 193.98448181152344
Step: 7210, train/learning_rate: 4.142075340496376e-05
Step: 7210, train/epoch: 1.7158496379852295
Step: 7220, train/loss: 0.042899999767541885
Step: 7220, train/grad_norm: 25.09250831604004
Step: 7220, train/learning_rate: 4.1408853576285765e-05
Step: 7220, train/epoch: 1.7182294130325317
Step: 7230, train/loss: 0.0
Step: 7230, train/grad_norm: 5.378399237088161e-07
Step: 7230, train/learning_rate: 4.139695374760777e-05
Step: 7230, train/epoch: 1.720609188079834
Step: 7240, train/loss: 0.34450000524520874
Step: 7240, train/grad_norm: 1.307834463659674e-05
Step: 7240, train/learning_rate: 4.138505391892977e-05
Step: 7240, train/epoch: 1.7229890823364258
Step: 7250, train/loss: 0.009399999864399433
Step: 7250, train/grad_norm: 2.6193029043497518e-05
Step: 7250, train/learning_rate: 4.1373154090251774e-05
Step: 7250, train/epoch: 1.725368857383728
Step: 7260, train/loss: 9.999999747378752e-05
Step: 7260, train/grad_norm: 1.3882934581488371e-05
Step: 7260, train/learning_rate: 4.1361257899552584e-05
Step: 7260, train/epoch: 1.7277486324310303
Step: 7270, train/loss: 0.18050000071525574
Step: 7270, train/grad_norm: 7.105075928848237e-05
Step: 7270, train/learning_rate: 4.134935807087459e-05
Step: 7270, train/epoch: 1.730128526687622
Step: 7280, train/loss: 0.30320000648498535
Step: 7280, train/grad_norm: 86.7056655883789
Step: 7280, train/learning_rate: 4.133745824219659e-05
Step: 7280, train/epoch: 1.7325083017349243
Step: 7290, train/loss: 0.003599999938160181
Step: 7290, train/grad_norm: 0.00015401106793433428
Step: 7290, train/learning_rate: 4.132555841351859e-05
Step: 7290, train/epoch: 1.7348881959915161
Step: 7300, train/loss: 0.0
Step: 7300, train/grad_norm: 0.0013516013277694583
Step: 7300, train/learning_rate: 4.1313658584840596e-05
Step: 7300, train/epoch: 1.7372679710388184
Step: 7310, train/loss: 0.061000000685453415
Step: 7310, train/grad_norm: 0.040098048746585846
Step: 7310, train/learning_rate: 4.1301762394141406e-05
Step: 7310, train/epoch: 1.7396477460861206
Step: 7320, train/loss: 0.002300000051036477
Step: 7320, train/grad_norm: 5.721964771510102e-05
Step: 7320, train/learning_rate: 4.128986256546341e-05
Step: 7320, train/epoch: 1.7420276403427124
Step: 7330, train/loss: 0.00039999998989515007
Step: 7330, train/grad_norm: 0.006062184926122427
Step: 7330, train/learning_rate: 4.127796273678541e-05
Step: 7330, train/epoch: 1.7444074153900146
Step: 7340, train/loss: 0.0
Step: 7340, train/grad_norm: 0.0001762350439094007
Step: 7340, train/learning_rate: 4.1266062908107415e-05
Step: 7340, train/epoch: 1.746787190437317
Step: 7350, train/loss: 0.0044999998062849045
Step: 7350, train/grad_norm: 24.388418197631836
Step: 7350, train/learning_rate: 4.125416307942942e-05
Step: 7350, train/epoch: 1.7491670846939087
Step: 7360, train/loss: 0.0478999987244606
Step: 7360, train/grad_norm: 3.275895833969116
Step: 7360, train/learning_rate: 4.124226688873023e-05
Step: 7360, train/epoch: 1.751546859741211
Step: 7370, train/loss: 0.05079999938607216
Step: 7370, train/grad_norm: 2.2118422985076904
Step: 7370, train/learning_rate: 4.123036706005223e-05
Step: 7370, train/epoch: 1.7539267539978027
Step: 7380, train/loss: 0.0008999999845400453
Step: 7380, train/grad_norm: 5.326131486071972e-06
Step: 7380, train/learning_rate: 4.1218467231374234e-05
Step: 7380, train/epoch: 1.756306529045105
Step: 7390, train/loss: 0.0
Step: 7390, train/grad_norm: 0.019309397786855698
Step: 7390, train/learning_rate: 4.120656740269624e-05
Step: 7390, train/epoch: 1.7586863040924072
Step: 7400, train/loss: 0.0
Step: 7400, train/grad_norm: 2.9342709240154363e-07
Step: 7400, train/learning_rate: 4.119466757401824e-05
Step: 7400, train/epoch: 1.761066198348999
Step: 7410, train/loss: 0.00019999999494757503
Step: 7410, train/grad_norm: 8.787385041841844e-08
Step: 7410, train/learning_rate: 4.118277138331905e-05
Step: 7410, train/epoch: 1.7634459733963013
Step: 7420, train/loss: 0.0
Step: 7420, train/grad_norm: 2.1816308048983046e-07
Step: 7420, train/learning_rate: 4.117087155464105e-05
Step: 7420, train/epoch: 1.7658257484436035
Step: 7430, train/loss: 0.00039999998989515007
Step: 7430, train/grad_norm: 2.2558031886887875e-08
Step: 7430, train/learning_rate: 4.1158971725963056e-05
Step: 7430, train/epoch: 1.7682056427001953
Step: 7440, train/loss: 0.0
Step: 7440, train/grad_norm: 2.8424496090195817e-09
Step: 7440, train/learning_rate: 4.114707189728506e-05
Step: 7440, train/epoch: 1.7705854177474976
Step: 7450, train/loss: 0.0
Step: 7450, train/grad_norm: 0.0793365091085434
Step: 7450, train/learning_rate: 4.113517206860706e-05
Step: 7450, train/epoch: 1.7729653120040894
Step: 7460, train/loss: 0.0
Step: 7460, train/grad_norm: 5.931405056003314e-08
Step: 7460, train/learning_rate: 4.112327587790787e-05
Step: 7460, train/epoch: 1.7753450870513916
Step: 7470, train/loss: 0.0
Step: 7470, train/grad_norm: 3.63680385362386e-07
Step: 7470, train/learning_rate: 4.1111376049229875e-05
Step: 7470, train/epoch: 1.7777248620986938
Step: 7480, train/loss: 0.0
Step: 7480, train/grad_norm: 2.1921628956533823e-07
Step: 7480, train/learning_rate: 4.109947622055188e-05
Step: 7480, train/epoch: 1.7801047563552856
Step: 7490, train/loss: 0.0
Step: 7490, train/grad_norm: 0.000496167573146522
Step: 7490, train/learning_rate: 4.108757639187388e-05
Step: 7490, train/epoch: 1.782484531402588
Step: 7500, train/loss: 0.0
Step: 7500, train/grad_norm: 1.0157071850258603e-09
Step: 7500, train/learning_rate: 4.1075676563195884e-05
Step: 7500, train/epoch: 1.7848643064498901
Step: 7510, train/loss: 0.0
Step: 7510, train/grad_norm: 3.593593334016987e-08
Step: 7510, train/learning_rate: 4.1063780372496694e-05
Step: 7510, train/epoch: 1.787244200706482
Step: 7520, train/loss: 0.21559999883174896
Step: 7520, train/grad_norm: 0.008554855361580849
Step: 7520, train/learning_rate: 4.10518805438187e-05
Step: 7520, train/epoch: 1.7896239757537842
Step: 7530, train/loss: 0.0
Step: 7530, train/grad_norm: 0.09529104083776474
Step: 7530, train/learning_rate: 4.10399807151407e-05
Step: 7530, train/epoch: 1.7920037508010864
Step: 7540, train/loss: 0.0
Step: 7540, train/grad_norm: 0.00011469851597212255
Step: 7540, train/learning_rate: 4.10280808864627e-05
Step: 7540, train/epoch: 1.7943836450576782
Step: 7550, train/loss: 0.09920000284910202
Step: 7550, train/grad_norm: 0.0003146535309497267
Step: 7550, train/learning_rate: 4.1016181057784706e-05
Step: 7550, train/epoch: 1.7967634201049805
Step: 7560, train/loss: 0.0
Step: 7560, train/grad_norm: 0.00042159025906585157
Step: 7560, train/learning_rate: 4.1004284867085516e-05
Step: 7560, train/epoch: 1.7991433143615723
Step: 7570, train/loss: 0.11169999837875366
Step: 7570, train/grad_norm: 0.00015814267680980265
Step: 7570, train/learning_rate: 4.099238503840752e-05
Step: 7570, train/epoch: 1.8015230894088745
Step: 7580, train/loss: 0.0
Step: 7580, train/grad_norm: 3.0257688194978982e-06
Step: 7580, train/learning_rate: 4.098048520972952e-05
Step: 7580, train/epoch: 1.8039028644561768
Step: 7590, train/loss: 0.0
Step: 7590, train/grad_norm: 3.1709318136563525e-05
Step: 7590, train/learning_rate: 4.0968585381051525e-05
Step: 7590, train/epoch: 1.8062827587127686
Step: 7600, train/loss: 0.0
Step: 7600, train/grad_norm: 0.0018985208589583635
Step: 7600, train/learning_rate: 4.095668555237353e-05
Step: 7600, train/epoch: 1.8086625337600708
Step: 7610, train/loss: 0.0
Step: 7610, train/grad_norm: 1.8340975657338277e-05
Step: 7610, train/learning_rate: 4.094478936167434e-05
Step: 7610, train/epoch: 1.811042308807373
Step: 7620, train/loss: 0.0
Step: 7620, train/grad_norm: 0.00047469171113334596
Step: 7620, train/learning_rate: 4.093288953299634e-05
Step: 7620, train/epoch: 1.8134222030639648
Step: 7630, train/loss: 0.0
Step: 7630, train/grad_norm: 8.124939631670713e-05
Step: 7630, train/learning_rate: 4.0920989704318345e-05
Step: 7630, train/epoch: 1.815801978111267
Step: 7640, train/loss: 0.0
Step: 7640, train/grad_norm: 5.345220870367484e-06
Step: 7640, train/learning_rate: 4.090908987564035e-05
Step: 7640, train/epoch: 1.8181818723678589
Step: 7650, train/loss: 0.0
Step: 7650, train/grad_norm: 0.013106272555887699
Step: 7650, train/learning_rate: 4.089719004696235e-05
Step: 7650, train/epoch: 1.8205616474151611
Step: 7660, train/loss: 9.999999747378752e-05
Step: 7660, train/grad_norm: 9.843042789725587e-05
Step: 7660, train/learning_rate: 4.088529385626316e-05
Step: 7660, train/epoch: 1.8229414224624634
Step: 7670, train/loss: 0.0
Step: 7670, train/grad_norm: 1.2854557098762598e-05
Step: 7670, train/learning_rate: 4.0873394027585164e-05
Step: 7670, train/epoch: 1.8253213167190552
Step: 7680, train/loss: 9.999999747378752e-05
Step: 7680, train/grad_norm: 0.0003974552091676742
Step: 7680, train/learning_rate: 4.086149419890717e-05
Step: 7680, train/epoch: 1.8277010917663574
Step: 7690, train/loss: 0.0
Step: 7690, train/grad_norm: 7.151313911890611e-05
Step: 7690, train/learning_rate: 4.084959437022917e-05
Step: 7690, train/epoch: 1.8300808668136597
Step: 7700, train/loss: 0.0
Step: 7700, train/grad_norm: 1.177724334411323e-05
Step: 7700, train/learning_rate: 4.083769454155117e-05
Step: 7700, train/epoch: 1.8324607610702515
Step: 7710, train/loss: 0.0
Step: 7710, train/grad_norm: 0.0004067991394549608
Step: 7710, train/learning_rate: 4.082579835085198e-05
Step: 7710, train/epoch: 1.8348405361175537
Step: 7720, train/loss: 0.0
Step: 7720, train/grad_norm: 9.881824780677562e-08
Step: 7720, train/learning_rate: 4.0813898522173986e-05
Step: 7720, train/epoch: 1.8372204303741455
Step: 7730, train/loss: 0.0
Step: 7730, train/grad_norm: 0.1410037726163864
Step: 7730, train/learning_rate: 4.080199869349599e-05
Step: 7730, train/epoch: 1.8396002054214478
Step: 7740, train/loss: 0.0003000000142492354
Step: 7740, train/grad_norm: 7.301338195800781
Step: 7740, train/learning_rate: 4.079009886481799e-05
Step: 7740, train/epoch: 1.84197998046875
Step: 7750, train/loss: 0.0
Step: 7750, train/grad_norm: 1.6041610706452047e-06
Step: 7750, train/learning_rate: 4.07782026741188e-05
Step: 7750, train/epoch: 1.8443598747253418
Step: 7760, train/loss: 0.0032999999821186066
Step: 7760, train/grad_norm: 70.77173614501953
Step: 7760, train/learning_rate: 4.0766302845440805e-05
Step: 7760, train/epoch: 1.846739649772644
Step: 7770, train/loss: 0.0
Step: 7770, train/grad_norm: 4.312702003517188e-06
Step: 7770, train/learning_rate: 4.075440301676281e-05
Step: 7770, train/epoch: 1.8491194248199463
Step: 7780, train/loss: 0.0430000014603138
Step: 7780, train/grad_norm: 3.050165560125606e-06
Step: 7780, train/learning_rate: 4.074250318808481e-05
Step: 7780, train/epoch: 1.851499319076538
Step: 7790, train/loss: 9.999999747378752e-05
Step: 7790, train/grad_norm: 1.896373760246206e-07
Step: 7790, train/learning_rate: 4.0730603359406814e-05
Step: 7790, train/epoch: 1.8538790941238403
Step: 7800, train/loss: 0.1850000023841858
Step: 7800, train/grad_norm: 8.926358532335144e-07
Step: 7800, train/learning_rate: 4.0718707168707624e-05
Step: 7800, train/epoch: 1.8562588691711426
Step: 7810, train/loss: 0.00039999998989515007
Step: 7810, train/grad_norm: 1.2291538715362549
Step: 7810, train/learning_rate: 4.070680734002963e-05
Step: 7810, train/epoch: 1.8586387634277344
Step: 7820, train/loss: 0.0
Step: 7820, train/grad_norm: 3.4387747291475534e-05
Step: 7820, train/learning_rate: 4.069490751135163e-05
Step: 7820, train/epoch: 1.8610185384750366
Step: 7830, train/loss: 0.13279999792575836
Step: 7830, train/grad_norm: 0.0017225578194484115
Step: 7830, train/learning_rate: 4.068300768267363e-05
Step: 7830, train/epoch: 1.8633984327316284
Step: 7840, train/loss: 0.0006000000284984708
Step: 7840, train/grad_norm: 0.005115226376801729
Step: 7840, train/learning_rate: 4.0671107853995636e-05
Step: 7840, train/epoch: 1.8657782077789307
Step: 7850, train/loss: 9.999999747378752e-05
Step: 7850, train/grad_norm: 0.016114279627799988
Step: 7850, train/learning_rate: 4.0659211663296446e-05
Step: 7850, train/epoch: 1.868157982826233
Step: 7860, train/loss: 0.05689999833703041
Step: 7860, train/grad_norm: 1.4814531823503785e-06
Step: 7860, train/learning_rate: 4.064731183461845e-05
Step: 7860, train/epoch: 1.8705378770828247
Step: 7870, train/loss: 0.0
Step: 7870, train/grad_norm: 1.0130164582733414e-06
Step: 7870, train/learning_rate: 4.063541200594045e-05
Step: 7870, train/epoch: 1.872917652130127
Step: 7880, train/loss: 0.0
Step: 7880, train/grad_norm: 2.395682940914412e-07
Step: 7880, train/learning_rate: 4.0623512177262455e-05
Step: 7880, train/epoch: 1.8752974271774292
Step: 7890, train/loss: 0.06019999831914902
Step: 7890, train/grad_norm: 8.130875357892364e-06
Step: 7890, train/learning_rate: 4.061161234858446e-05
Step: 7890, train/epoch: 1.877677321434021
Step: 7900, train/loss: 0.0
Step: 7900, train/grad_norm: 3.3689752854115795e-06
Step: 7900, train/learning_rate: 4.059971615788527e-05
Step: 7900, train/epoch: 1.8800570964813232
Step: 7910, train/loss: 0.029500000178813934
Step: 7910, train/grad_norm: 0.00048353226156905293
Step: 7910, train/learning_rate: 4.058781632920727e-05
Step: 7910, train/epoch: 1.882436990737915
Step: 7920, train/loss: 0.011300000362098217
Step: 7920, train/grad_norm: 1.4517093404720072e-05
Step: 7920, train/learning_rate: 4.0575916500529274e-05
Step: 7920, train/epoch: 1.8848167657852173
Step: 7930, train/loss: 0.06210000067949295
Step: 7930, train/grad_norm: 5.001764293410815e-06
Step: 7930, train/learning_rate: 4.056401667185128e-05
Step: 7930, train/epoch: 1.8871965408325195
Step: 7940, train/loss: 0.05000000074505806
Step: 7940, train/grad_norm: 0.002445786725729704
Step: 7940, train/learning_rate: 4.055211684317328e-05
Step: 7940, train/epoch: 1.8895764350891113
Step: 7950, train/loss: 9.999999747378752e-05
Step: 7950, train/grad_norm: 1.4189136891218368e-05
Step: 7950, train/learning_rate: 4.054022065247409e-05
Step: 7950, train/epoch: 1.8919562101364136
Step: 7960, train/loss: 0.0
Step: 7960, train/grad_norm: 0.0009018130949698389
Step: 7960, train/learning_rate: 4.0528320823796093e-05
Step: 7960, train/epoch: 1.8943359851837158
Step: 7970, train/loss: 0.0729999989271164
Step: 7970, train/grad_norm: 2.360845246585086e-05
Step: 7970, train/learning_rate: 4.0516420995118096e-05
Step: 7970, train/epoch: 1.8967158794403076
Step: 7980, train/loss: 9.999999747378752e-05
Step: 7980, train/grad_norm: 0.00031805827165953815
Step: 7980, train/learning_rate: 4.05045211664401e-05
Step: 7980, train/epoch: 1.8990956544876099
Step: 7990, train/loss: 9.999999747378752e-05
Step: 7990, train/grad_norm: 0.0028717070817947388
Step: 7990, train/learning_rate: 4.04926213377621e-05
Step: 7990, train/epoch: 1.901475429534912
Step: 8000, train/loss: 0.09099999815225601
Step: 8000, train/grad_norm: 0.0012172386050224304
Step: 8000, train/learning_rate: 4.048072514706291e-05
Step: 8000, train/epoch: 1.903855323791504
Step: 8010, train/loss: 0.0
Step: 8010, train/grad_norm: 2.826845502568176e-06
Step: 8010, train/learning_rate: 4.0468825318384916e-05
Step: 8010, train/epoch: 1.9062350988388062
Step: 8020, train/loss: 0.1436000019311905
Step: 8020, train/grad_norm: 188.05581665039062
Step: 8020, train/learning_rate: 4.045692548970692e-05
Step: 8020, train/epoch: 1.908614993095398
Step: 8030, train/loss: 9.999999747378752e-05
Step: 8030, train/grad_norm: 0.0005197719438001513
Step: 8030, train/learning_rate: 4.044502566102892e-05
Step: 8030, train/epoch: 1.9109947681427002
Step: 8040, train/loss: 0.0044999998062849045
Step: 8040, train/grad_norm: 0.0007881688070483506
Step: 8040, train/learning_rate: 4.0433125832350925e-05
Step: 8040, train/epoch: 1.9133745431900024
Step: 8050, train/loss: 0.0
Step: 8050, train/grad_norm: 2.2478818664239952e-06
Step: 8050, train/learning_rate: 4.0421229641651735e-05
Step: 8050, train/epoch: 1.9157544374465942
Step: 8060, train/loss: 9.999999747378752e-05
Step: 8060, train/grad_norm: 3.6286408544583537e-07
Step: 8060, train/learning_rate: 4.040932981297374e-05
Step: 8060, train/epoch: 1.9181342124938965
Step: 8070, train/loss: 9.999999747378752e-05
Step: 8070, train/grad_norm: 3.412693061477512e-08
Step: 8070, train/learning_rate: 4.039742998429574e-05
Step: 8070, train/epoch: 1.9205139875411987
Step: 8080, train/loss: 0.0
Step: 8080, train/grad_norm: 0.04852468520402908
Step: 8080, train/learning_rate: 4.0385530155617744e-05
Step: 8080, train/epoch: 1.9228938817977905
Step: 8090, train/loss: 0.015799999237060547
Step: 8090, train/grad_norm: 286.5940246582031
Step: 8090, train/learning_rate: 4.037363032693975e-05
Step: 8090, train/epoch: 1.9252736568450928
Step: 8100, train/loss: 0.036400001496076584
Step: 8100, train/grad_norm: 8.690578397363424e-05
Step: 8100, train/learning_rate: 4.036173413624056e-05
Step: 8100, train/epoch: 1.9276535511016846
Step: 8110, train/loss: 0.0
Step: 8110, train/grad_norm: 3.6586030205398856e-08
Step: 8110, train/learning_rate: 4.034983430756256e-05
Step: 8110, train/epoch: 1.9300333261489868
Step: 8120, train/loss: 0.045099999755620956
Step: 8120, train/grad_norm: 1.3672270142706111e-06
Step: 8120, train/learning_rate: 4.033793447888456e-05
Step: 8120, train/epoch: 1.932413101196289
Step: 8130, train/loss: 0.0
Step: 8130, train/grad_norm: 9.42454825292316e-09
Step: 8130, train/learning_rate: 4.0326034650206566e-05
Step: 8130, train/epoch: 1.9347929954528809
Step: 8140, train/loss: 0.005400000140070915
Step: 8140, train/grad_norm: 8.850517588143703e-06
Step: 8140, train/learning_rate: 4.031413482152857e-05
Step: 8140, train/epoch: 1.937172770500183
Step: 8150, train/loss: 0.006399999838322401
Step: 8150, train/grad_norm: 6.052497383279842e-07
Step: 8150, train/learning_rate: 4.030223863082938e-05
Step: 8150, train/epoch: 1.9395525455474854
Step: 8160, train/loss: 0.0
Step: 8160, train/grad_norm: 6.617042469514445e-09
Step: 8160, train/learning_rate: 4.029033880215138e-05
Step: 8160, train/epoch: 1.9419324398040771
Step: 8170, train/loss: 0.07460000365972519
Step: 8170, train/grad_norm: 6.787597328639094e-08
Step: 8170, train/learning_rate: 4.0278438973473385e-05
Step: 8170, train/epoch: 1.9443122148513794
Step: 8180, train/loss: 0.0
Step: 8180, train/grad_norm: 1.6430070900241844e-05
Step: 8180, train/learning_rate: 4.026653914479539e-05
Step: 8180, train/epoch: 1.9466921091079712
Step: 8190, train/loss: 0.0
Step: 8190, train/grad_norm: 1.238631597288986e-07
Step: 8190, train/learning_rate: 4.025463931611739e-05
Step: 8190, train/epoch: 1.9490718841552734
Step: 8200, train/loss: 0.0
Step: 8200, train/grad_norm: 4.229126773225289e-07
Step: 8200, train/learning_rate: 4.02427431254182e-05
Step: 8200, train/epoch: 1.9514516592025757
Step: 8210, train/loss: 0.15700000524520874
Step: 8210, train/grad_norm: 0.033462345600128174
Step: 8210, train/learning_rate: 4.0230843296740204e-05
Step: 8210, train/epoch: 1.9538315534591675
Step: 8220, train/loss: 0.0340999998152256
Step: 8220, train/grad_norm: 0.0009245480177924037
Step: 8220, train/learning_rate: 4.021894346806221e-05
Step: 8220, train/epoch: 1.9562113285064697
Step: 8230, train/loss: 0.0
Step: 8230, train/grad_norm: 0.00021674876916222274
Step: 8230, train/learning_rate: 4.020704363938421e-05
Step: 8230, train/epoch: 1.958591103553772
Step: 8240, train/loss: 0.0
Step: 8240, train/grad_norm: 4.142733087064698e-05
Step: 8240, train/learning_rate: 4.019514381070621e-05
Step: 8240, train/epoch: 1.9609709978103638
Step: 8250, train/loss: 0.0008999999845400453
Step: 8250, train/grad_norm: 9.313908958574757e-05
Step: 8250, train/learning_rate: 4.018324762000702e-05
Step: 8250, train/epoch: 1.963350772857666
Step: 8260, train/loss: 0.0
Step: 8260, train/grad_norm: 0.25529348850250244
Step: 8260, train/learning_rate: 4.0171347791329026e-05
Step: 8260, train/epoch: 1.9657305479049683
Step: 8270, train/loss: 0.0
Step: 8270, train/grad_norm: 0.00024336250498890877
Step: 8270, train/learning_rate: 4.015944796265103e-05
Step: 8270, train/epoch: 1.96811044216156
Step: 8280, train/loss: 0.0
Step: 8280, train/grad_norm: 2.062517836520783e-07
Step: 8280, train/learning_rate: 4.014754813397303e-05
Step: 8280, train/epoch: 1.9704902172088623
Step: 8290, train/loss: 0.0017999999690800905
Step: 8290, train/grad_norm: 1.1695515240717214e-06
Step: 8290, train/learning_rate: 4.0135648305295035e-05
Step: 8290, train/epoch: 1.972870111465454
Step: 8300, train/loss: 0.0
Step: 8300, train/grad_norm: 4.446783918865549e-07
Step: 8300, train/learning_rate: 4.0123752114595845e-05
Step: 8300, train/epoch: 1.9752498865127563
Step: 8310, train/loss: 0.000699999975040555
Step: 8310, train/grad_norm: 4.366590928839287e-07
Step: 8310, train/learning_rate: 4.011185228591785e-05
Step: 8310, train/epoch: 1.9776296615600586
Step: 8320, train/loss: 0.0
Step: 8320, train/grad_norm: 2.2900477958387455e-08
Step: 8320, train/learning_rate: 4.009995245723985e-05
Step: 8320, train/epoch: 1.9800095558166504
Step: 8330, train/loss: 0.1242000013589859
Step: 8330, train/grad_norm: 77.91629791259766
Step: 8330, train/learning_rate: 4.0088052628561854e-05
Step: 8330, train/epoch: 1.9823893308639526
Step: 8340, train/loss: 0.0
Step: 8340, train/grad_norm: 1.4539654102918576e-06
Step: 8340, train/learning_rate: 4.007615279988386e-05
Step: 8340, train/epoch: 1.9847691059112549
Step: 8350, train/loss: 0.0
Step: 8350, train/grad_norm: 1.3996262850923813e-06
Step: 8350, train/learning_rate: 4.006425660918467e-05
Step: 8350, train/epoch: 1.9871490001678467
Step: 8360, train/loss: 0.024299999698996544
Step: 8360, train/grad_norm: 0.00011712726700352505
Step: 8360, train/learning_rate: 4.005235678050667e-05
Step: 8360, train/epoch: 1.989528775215149
Step: 8370, train/loss: 0.05270000174641609
Step: 8370, train/grad_norm: 318.9845275878906
Step: 8370, train/learning_rate: 4.0040456951828673e-05
Step: 8370, train/epoch: 1.9919086694717407
Step: 8380, train/loss: 0.00930000003427267
Step: 8380, train/grad_norm: 7.598388833685021e-07
Step: 8380, train/learning_rate: 4.0028557123150676e-05
Step: 8380, train/epoch: 1.994288444519043
Step: 8390, train/loss: 0.0
Step: 8390, train/grad_norm: 5.318306239843196e-09
Step: 8390, train/learning_rate: 4.001665729447268e-05
Step: 8390, train/epoch: 1.9966682195663452
Step: 8400, train/loss: 0.0010000000474974513
Step: 8400, train/grad_norm: 5.35390381628531e-06
Step: 8400, train/learning_rate: 4.000476110377349e-05
Step: 8400, train/epoch: 1.999048113822937
Step: 8404, eval/loss: 0.014532235451042652
Step: 8404, eval/accuracy: 0.9977787137031555
Step: 8404, eval/f1: 0.997653603553772
Step: 8404, eval/runtime: 735.6661987304688
Step: 8404, eval/samples_per_second: 9.791000366210938
Step: 8404, eval/steps_per_second: 1.225000023841858
Step: 8404, train/epoch: 2.0
Step: 8410, train/loss: 0.0
Step: 8410, train/grad_norm: 6.220221138164561e-08
Step: 8410, train/learning_rate: 3.999286127509549e-05
Step: 8410, train/epoch: 2.0014278888702393
Step: 8420, train/loss: 0.0
Step: 8420, train/grad_norm: 1.8787073940984556e-07
Step: 8420, train/learning_rate: 3.9980961446417496e-05
Step: 8420, train/epoch: 2.003807783126831
Step: 8430, train/loss: 0.0
Step: 8430, train/grad_norm: 5.191888021727209e-07
Step: 8430, train/learning_rate: 3.99690616177395e-05
Step: 8430, train/epoch: 2.0061874389648438
Step: 8440, train/loss: 0.01209999993443489
Step: 8440, train/grad_norm: 0.000827302981633693
Step: 8440, train/learning_rate: 3.99571617890615e-05
Step: 8440, train/epoch: 2.0085673332214355
Step: 8450, train/loss: 0.0
Step: 8450, train/grad_norm: 0.00023573581711389124
Step: 8450, train/learning_rate: 3.994526559836231e-05
Step: 8450, train/epoch: 2.0109472274780273
Step: 8460, train/loss: 0.10090000182390213
Step: 8460, train/grad_norm: 3.9773868820702774e-07
Step: 8460, train/learning_rate: 3.9933365769684315e-05
Step: 8460, train/epoch: 2.01332688331604
Step: 8470, train/loss: 0.0
Step: 8470, train/grad_norm: 0.0003072403487749398
Step: 8470, train/learning_rate: 3.992146594100632e-05
Step: 8470, train/epoch: 2.015706777572632
Step: 8480, train/loss: 0.0003000000142492354
Step: 8480, train/grad_norm: 2.4250348360510543e-05
Step: 8480, train/learning_rate: 3.990956611232832e-05
Step: 8480, train/epoch: 2.0180866718292236
Step: 8490, train/loss: 0.0
Step: 8490, train/grad_norm: 0.00012361691915430129
Step: 8490, train/learning_rate: 3.9897666283650324e-05
Step: 8490, train/epoch: 2.0204663276672363
Step: 8500, train/loss: 0.1696999967098236
Step: 8500, train/grad_norm: 2.0473640560680906e-08
Step: 8500, train/learning_rate: 3.9885770092951134e-05
Step: 8500, train/epoch: 2.022846221923828
Step: 8510, train/loss: 0.008899999782443047
Step: 8510, train/grad_norm: 2.4725437697270536e-07
Step: 8510, train/learning_rate: 3.987387026427314e-05
Step: 8510, train/epoch: 2.02522611618042
Step: 8520, train/loss: 0.0
Step: 8520, train/grad_norm: 1.7700539700626905e-08
Step: 8520, train/learning_rate: 3.986197043559514e-05
Step: 8520, train/epoch: 2.0276060104370117
Step: 8530, train/loss: 0.0
Step: 8530, train/grad_norm: 1.3042800617313333e-08
Step: 8530, train/learning_rate: 3.985007060691714e-05
Step: 8530, train/epoch: 2.0299856662750244
Step: 8540, train/loss: 0.0
Step: 8540, train/grad_norm: 1.365525292840175e-07
Step: 8540, train/learning_rate: 3.9838170778239146e-05
Step: 8540, train/epoch: 2.032365560531616
Step: 8550, train/loss: 0.0
Step: 8550, train/grad_norm: 1.8070004159653763e-08
Step: 8550, train/learning_rate: 3.9826274587539956e-05
Step: 8550, train/epoch: 2.034745454788208
Step: 8560, train/loss: 0.0
Step: 8560, train/grad_norm: 5.630278021584445e-09
Step: 8560, train/learning_rate: 3.981437475886196e-05
Step: 8560, train/epoch: 2.0371251106262207
Step: 8570, train/loss: 0.0
Step: 8570, train/grad_norm: 3.830321475106757e-06
Step: 8570, train/learning_rate: 3.980247493018396e-05
Step: 8570, train/epoch: 2.0395050048828125
Step: 8580, train/loss: 0.1679999977350235
Step: 8580, train/grad_norm: 1.597082075477374e-07
Step: 8580, train/learning_rate: 3.9790575101505965e-05
Step: 8580, train/epoch: 2.0418848991394043
Step: 8590, train/loss: 0.0
Step: 8590, train/grad_norm: 0.000922511680983007
Step: 8590, train/learning_rate: 3.977867527282797e-05
Step: 8590, train/epoch: 2.044264554977417
Step: 8600, train/loss: 0.0
Step: 8600, train/grad_norm: 2.4341881726286374e-05
Step: 8600, train/learning_rate: 3.976677908212878e-05
Step: 8600, train/epoch: 2.046644449234009
Step: 8610, train/loss: 0.014399999752640724
Step: 8610, train/grad_norm: 4.862784862780245e-06
Step: 8610, train/learning_rate: 3.975487925345078e-05
Step: 8610, train/epoch: 2.0490243434906006
Step: 8620, train/loss: 0.0
Step: 8620, train/grad_norm: 6.031620500834833e-07
Step: 8620, train/learning_rate: 3.9742979424772784e-05
Step: 8620, train/epoch: 2.0514039993286133
Step: 8630, train/loss: 9.999999747378752e-05
Step: 8630, train/grad_norm: 0.39750173687934875
Step: 8630, train/learning_rate: 3.973107959609479e-05
Step: 8630, train/epoch: 2.053783893585205
Step: 8640, train/loss: 0.0
Step: 8640, train/grad_norm: 3.818624350060418e-08
Step: 8640, train/learning_rate: 3.971917976741679e-05
Step: 8640, train/epoch: 2.056163787841797
Step: 8650, train/loss: 0.0
Step: 8650, train/grad_norm: 6.8780227593379095e-06
Step: 8650, train/learning_rate: 3.97072835767176e-05
Step: 8650, train/epoch: 2.0585434436798096
Step: 8660, train/loss: 0.0
Step: 8660, train/grad_norm: 2.7350719022933845e-08
Step: 8660, train/learning_rate: 3.96953837480396e-05
Step: 8660, train/epoch: 2.0609233379364014
Step: 8670, train/loss: 0.0
Step: 8670, train/grad_norm: 1.5649116903659888e-05
Step: 8670, train/learning_rate: 3.9683483919361606e-05
Step: 8670, train/epoch: 2.063303232192993
Step: 8680, train/loss: 0.0
Step: 8680, train/grad_norm: 4.131825903641584e-07
Step: 8680, train/learning_rate: 3.967158409068361e-05
Step: 8680, train/epoch: 2.065683126449585
Step: 8690, train/loss: 0.0
Step: 8690, train/grad_norm: 0.00022087614343035966
Step: 8690, train/learning_rate: 3.965968426200561e-05
Step: 8690, train/epoch: 2.0680627822875977
Step: 8700, train/loss: 0.0
Step: 8700, train/grad_norm: 3.169951057202525e-08
Step: 8700, train/learning_rate: 3.964778807130642e-05
Step: 8700, train/epoch: 2.0704426765441895
Step: 8710, train/loss: 9.999999747378752e-05
Step: 8710, train/grad_norm: 1.7945384067630243e-09
Step: 8710, train/learning_rate: 3.9635888242628425e-05
Step: 8710, train/epoch: 2.0728225708007812
Step: 8720, train/loss: 0.0
Step: 8720, train/grad_norm: 1.4404816006674537e-08
Step: 8720, train/learning_rate: 3.962398841395043e-05
Step: 8720, train/epoch: 2.075202226638794
Step: 8730, train/loss: 0.0
Step: 8730, train/grad_norm: 2.9671949164367106e-07
Step: 8730, train/learning_rate: 3.961208858527243e-05
Step: 8730, train/epoch: 2.0775821208953857
Step: 8740, train/loss: 0.0
Step: 8740, train/grad_norm: 1.3337805739865871e-06
Step: 8740, train/learning_rate: 3.9600188756594434e-05
Step: 8740, train/epoch: 2.0799620151519775
Step: 8750, train/loss: 0.0
Step: 8750, train/grad_norm: 5.927687652729219e-06
Step: 8750, train/learning_rate: 3.9588292565895244e-05
Step: 8750, train/epoch: 2.0823416709899902
Step: 8760, train/loss: 0.002899999963119626
Step: 8760, train/grad_norm: 2.485579216227052e-06
Step: 8760, train/learning_rate: 3.957639273721725e-05
Step: 8760, train/epoch: 2.084721565246582
Step: 8770, train/loss: 0.0
Step: 8770, train/grad_norm: 3.110757162971822e-08
Step: 8770, train/learning_rate: 3.956449290853925e-05
Step: 8770, train/epoch: 2.087101459503174
Step: 8780, train/loss: 0.0
Step: 8780, train/grad_norm: 3.478012544633202e-08
Step: 8780, train/learning_rate: 3.9552593079861253e-05
Step: 8780, train/epoch: 2.0894811153411865
Step: 8790, train/loss: 0.1460999995470047
Step: 8790, train/grad_norm: 1.171334702121385e-06
Step: 8790, train/learning_rate: 3.9540693251183257e-05
Step: 8790, train/epoch: 2.0918610095977783
Step: 8800, train/loss: 0.0
Step: 8800, train/grad_norm: 3.4240218838021974e-07
Step: 8800, train/learning_rate: 3.9528797060484067e-05
Step: 8800, train/epoch: 2.09424090385437
Step: 8810, train/loss: 0.0
Step: 8810, train/grad_norm: 1.5495491112460513e-08
Step: 8810, train/learning_rate: 3.951689723180607e-05
Step: 8810, train/epoch: 2.096620559692383
Step: 8820, train/loss: 0.014999999664723873
Step: 8820, train/grad_norm: 6.630162260989891e-06
Step: 8820, train/learning_rate: 3.950499740312807e-05
Step: 8820, train/epoch: 2.0990004539489746
Step: 8830, train/loss: 0.0
Step: 8830, train/grad_norm: 7.449318673025118e-06
Step: 8830, train/learning_rate: 3.9493097574450076e-05
Step: 8830, train/epoch: 2.1013803482055664
Step: 8840, train/loss: 0.0
Step: 8840, train/grad_norm: 9.097396286961157e-06
Step: 8840, train/learning_rate: 3.948119774577208e-05
Step: 8840, train/epoch: 2.103760004043579
Step: 8850, train/loss: 0.0
Step: 8850, train/grad_norm: 1.6962927418262552e-07
Step: 8850, train/learning_rate: 3.946930155507289e-05
Step: 8850, train/epoch: 2.106139898300171
Step: 8860, train/loss: 0.0
Step: 8860, train/grad_norm: 4.545312549453229e-06
Step: 8860, train/learning_rate: 3.945740172639489e-05
Step: 8860, train/epoch: 2.1085197925567627
Step: 8870, train/loss: 9.999999747378752e-05
Step: 8870, train/grad_norm: 7.953323688525415e-08
Step: 8870, train/learning_rate: 3.9445501897716895e-05
Step: 8870, train/epoch: 2.1108996868133545
Step: 8880, train/loss: 0.0
Step: 8880, train/grad_norm: 5.921132810726704e-07
Step: 8880, train/learning_rate: 3.94336020690389e-05
Step: 8880, train/epoch: 2.113279342651367
Step: 8890, train/loss: 0.0
Step: 8890, train/grad_norm: 4.784228124776746e-09
Step: 8890, train/learning_rate: 3.94217022403609e-05
Step: 8890, train/epoch: 2.115659236907959
Step: 8900, train/loss: 0.0008999999845400453
Step: 8900, train/grad_norm: 0.0025732354260981083
Step: 8900, train/learning_rate: 3.940980604966171e-05
Step: 8900, train/epoch: 2.118039131164551
Step: 8910, train/loss: 0.0
Step: 8910, train/grad_norm: 6.959200504752516e-08
Step: 8910, train/learning_rate: 3.9397906220983714e-05
Step: 8910, train/epoch: 2.1204187870025635
Step: 8920, train/loss: 0.0
Step: 8920, train/grad_norm: 7.81852360631774e-09
Step: 8920, train/learning_rate: 3.938600639230572e-05
Step: 8920, train/epoch: 2.1227986812591553
Step: 8930, train/loss: 0.0
Step: 8930, train/grad_norm: 8.643552064313553e-06
Step: 8930, train/learning_rate: 3.937410656362772e-05
Step: 8930, train/epoch: 2.125178575515747
Step: 8940, train/loss: 0.0
Step: 8940, train/grad_norm: 9.910751941788476e-06
Step: 8940, train/learning_rate: 3.936220673494972e-05
Step: 8940, train/epoch: 2.1275582313537598
Step: 8950, train/loss: 0.04670000076293945
Step: 8950, train/grad_norm: 5.191232048673555e-05
Step: 8950, train/learning_rate: 3.935031054425053e-05
Step: 8950, train/epoch: 2.1299381256103516
Step: 8960, train/loss: 0.00019999999494757503
Step: 8960, train/grad_norm: 0.0001961751695489511
Step: 8960, train/learning_rate: 3.9338410715572536e-05
Step: 8960, train/epoch: 2.1323180198669434
Step: 8970, train/loss: 0.0
Step: 8970, train/grad_norm: 8.25197105314146e-07
Step: 8970, train/learning_rate: 3.932651088689454e-05
Step: 8970, train/epoch: 2.134697675704956
Step: 8980, train/loss: 0.11720000207424164
Step: 8980, train/grad_norm: 2.0666953787440434e-05
Step: 8980, train/learning_rate: 3.931461105821654e-05
Step: 8980, train/epoch: 2.137077569961548
Step: 8990, train/loss: 0.002899999963119626
Step: 8990, train/grad_norm: 5.708151462613387e-08
Step: 8990, train/learning_rate: 3.9302711229538545e-05
Step: 8990, train/epoch: 2.1394574642181396
Step: 9000, train/loss: 0.0
Step: 9000, train/grad_norm: 1.1221428621865925e-06
Step: 9000, train/learning_rate: 3.9290815038839355e-05
Step: 9000, train/epoch: 2.1418371200561523
Step: 9010, train/loss: 0.0
Step: 9010, train/grad_norm: 3.7447158263148594e-08
Step: 9010, train/learning_rate: 3.927891521016136e-05
Step: 9010, train/epoch: 2.144217014312744
Step: 9020, train/loss: 0.0
Step: 9020, train/grad_norm: 3.9930124330567196e-05
Step: 9020, train/learning_rate: 3.926701538148336e-05
Step: 9020, train/epoch: 2.146596908569336
Step: 9030, train/loss: 0.01640000008046627
Step: 9030, train/grad_norm: 3.145501921153482e-07
Step: 9030, train/learning_rate: 3.9255115552805364e-05
Step: 9030, train/epoch: 2.1489765644073486
Step: 9040, train/loss: 0.0
Step: 9040, train/grad_norm: 4.183844339422649e-06
Step: 9040, train/learning_rate: 3.924321572412737e-05
Step: 9040, train/epoch: 2.1513564586639404
Step: 9050, train/loss: 9.999999747378752e-05
Step: 9050, train/grad_norm: 1.99959958990803e-05
Step: 9050, train/learning_rate: 3.923131953342818e-05
Step: 9050, train/epoch: 2.1537363529205322
Step: 9060, train/loss: 0.0
Step: 9060, train/grad_norm: 2.0998245986447728e-07
Step: 9060, train/learning_rate: 3.921941970475018e-05
Step: 9060, train/epoch: 2.156116247177124
Step: 9070, train/loss: 0.0
Step: 9070, train/grad_norm: 8.502498758389265e-07
Step: 9070, train/learning_rate: 3.920751987607218e-05
Step: 9070, train/epoch: 2.1584959030151367
Step: 9080, train/loss: 0.0
Step: 9080, train/grad_norm: 6.974252642066858e-08
Step: 9080, train/learning_rate: 3.9195620047394186e-05
Step: 9080, train/epoch: 2.1608757972717285
Step: 9090, train/loss: 0.0
Step: 9090, train/grad_norm: 1.5241701589729928e-07
Step: 9090, train/learning_rate: 3.918372021871619e-05
Step: 9090, train/epoch: 2.1632556915283203
Step: 9100, train/loss: 0.0
Step: 9100, train/grad_norm: 1.100482904803357e-06
Step: 9100, train/learning_rate: 3.9171824028017e-05
Step: 9100, train/epoch: 2.165635347366333
Step: 9110, train/loss: 0.0
Step: 9110, train/grad_norm: 7.511250714742346e-07
Step: 9110, train/learning_rate: 3.9159924199339e-05
Step: 9110, train/epoch: 2.168015241622925
Step: 9120, train/loss: 0.0
Step: 9120, train/grad_norm: 4.8126509000212536e-08
Step: 9120, train/learning_rate: 3.9148024370661005e-05
Step: 9120, train/epoch: 2.1703951358795166
Step: 9130, train/loss: 0.0
Step: 9130, train/grad_norm: 3.3208311833732296e-06
Step: 9130, train/learning_rate: 3.913612454198301e-05
Step: 9130, train/epoch: 2.1727747917175293
Step: 9140, train/loss: 0.0
Step: 9140, train/grad_norm: 1.8777750465659437e-09
Step: 9140, train/learning_rate: 3.912422835128382e-05
Step: 9140, train/epoch: 2.175154685974121
Step: 9150, train/loss: 0.0
Step: 9150, train/grad_norm: 0.00012621341738849878
Step: 9150, train/learning_rate: 3.911232852260582e-05
Step: 9150, train/epoch: 2.177534580230713
Step: 9160, train/loss: 0.0
Step: 9160, train/grad_norm: 3.938206646125764e-06
Step: 9160, train/learning_rate: 3.9100428693927824e-05
Step: 9160, train/epoch: 2.1799142360687256
Step: 9170, train/loss: 0.0
Step: 9170, train/grad_norm: 1.9163664433108352e-07
Step: 9170, train/learning_rate: 3.908852886524983e-05
Step: 9170, train/epoch: 2.1822941303253174
Step: 9180, train/loss: 0.0
Step: 9180, train/grad_norm: 1.1185092034793342e-06
Step: 9180, train/learning_rate: 3.907662903657183e-05
Step: 9180, train/epoch: 2.184674024581909
Step: 9190, train/loss: 0.0
Step: 9190, train/grad_norm: 3.7672344888051157e-07
Step: 9190, train/learning_rate: 3.906473284587264e-05
Step: 9190, train/epoch: 2.187053680419922
Step: 9200, train/loss: 0.0
Step: 9200, train/grad_norm: 9.484515395286053e-09
Step: 9200, train/learning_rate: 3.9052833017194644e-05
Step: 9200, train/epoch: 2.1894335746765137
Step: 9210, train/loss: 0.0
Step: 9210, train/grad_norm: 2.623867203510599e-07
Step: 9210, train/learning_rate: 3.904093318851665e-05
Step: 9210, train/epoch: 2.1918134689331055
Step: 9220, train/loss: 0.0
Step: 9220, train/grad_norm: 1.255624624718621e-06
Step: 9220, train/learning_rate: 3.902903335983865e-05
Step: 9220, train/epoch: 2.194193124771118
Step: 9230, train/loss: 0.0
Step: 9230, train/grad_norm: 2.08746399721349e-07
Step: 9230, train/learning_rate: 3.901713353116065e-05
Step: 9230, train/epoch: 2.19657301902771
Step: 9240, train/loss: 0.0
Step: 9240, train/grad_norm: 1.263866433731664e-08
Step: 9240, train/learning_rate: 3.900523734046146e-05
Step: 9240, train/epoch: 2.1989529132843018
Step: 9250, train/loss: 0.0
Step: 9250, train/grad_norm: 1.0732495638876571e-06
Step: 9250, train/learning_rate: 3.8993337511783466e-05
Step: 9250, train/epoch: 2.2013328075408936
Step: 9260, train/loss: 0.0
Step: 9260, train/grad_norm: 2.088542942146887e-06
Step: 9260, train/learning_rate: 3.898143768310547e-05
Step: 9260, train/epoch: 2.2037124633789062
Step: 9270, train/loss: 0.0
Step: 9270, train/grad_norm: 4.6837790534937085e-08
Step: 9270, train/learning_rate: 3.896953785442747e-05
Step: 9270, train/epoch: 2.206092357635498
Step: 9280, train/loss: 0.0
Step: 9280, train/grad_norm: 6.402499419522201e-09
Step: 9280, train/learning_rate: 3.8957638025749475e-05
Step: 9280, train/epoch: 2.20847225189209
Step: 9290, train/loss: 0.0
Step: 9290, train/grad_norm: 4.926961949536235e-09
Step: 9290, train/learning_rate: 3.8945741835050285e-05
Step: 9290, train/epoch: 2.2108519077301025
Step: 9300, train/loss: 0.0
Step: 9300, train/grad_norm: 9.975669672712684e-05
Step: 9300, train/learning_rate: 3.893384200637229e-05
Step: 9300, train/epoch: 2.2132318019866943
Step: 9310, train/loss: 0.0
Step: 9310, train/grad_norm: 9.134486589346125e-08
Step: 9310, train/learning_rate: 3.892194217769429e-05
Step: 9310, train/epoch: 2.215611696243286
Step: 9320, train/loss: 0.0
Step: 9320, train/grad_norm: 2.25359775285483e-09
Step: 9320, train/learning_rate: 3.8910042349016294e-05
Step: 9320, train/epoch: 2.217991352081299
Step: 9330, train/loss: 0.0
Step: 9330, train/grad_norm: 7.739199503475902e-09
Step: 9330, train/learning_rate: 3.88981425203383e-05
Step: 9330, train/epoch: 2.2203712463378906
Step: 9340, train/loss: 0.0
Step: 9340, train/grad_norm: 1.2984641140079134e-09
Step: 9340, train/learning_rate: 3.888624632963911e-05
Step: 9340, train/epoch: 2.2227511405944824
Step: 9350, train/loss: 0.0
Step: 9350, train/grad_norm: 7.433207116491758e-08
Step: 9350, train/learning_rate: 3.887434650096111e-05
Step: 9350, train/epoch: 2.225130796432495
Step: 9360, train/loss: 0.0
Step: 9360, train/grad_norm: 4.2848288472896456e-08
Step: 9360, train/learning_rate: 3.886244667228311e-05
Step: 9360, train/epoch: 2.227510690689087
Step: 9370, train/loss: 0.0
Step: 9370, train/grad_norm: 1.3424503286785239e-08
Step: 9370, train/learning_rate: 3.8850546843605116e-05
Step: 9370, train/epoch: 2.2298905849456787
Step: 9380, train/loss: 0.0
Step: 9380, train/grad_norm: 1.513698610899894e-10
Step: 9380, train/learning_rate: 3.883864701492712e-05
Step: 9380, train/epoch: 2.2322702407836914
Step: 9390, train/loss: 0.0
Step: 9390, train/grad_norm: 0.0026361264754086733
Step: 9390, train/learning_rate: 3.882675082422793e-05
Step: 9390, train/epoch: 2.234650135040283
Step: 9400, train/loss: 0.0
Step: 9400, train/grad_norm: 5.7821537780000654e-08
Step: 9400, train/learning_rate: 3.881485099554993e-05
Step: 9400, train/epoch: 2.237030029296875
Step: 9410, train/loss: 0.12110000103712082
Step: 9410, train/grad_norm: 6.952310741326073e-06
Step: 9410, train/learning_rate: 3.8802951166871935e-05
Step: 9410, train/epoch: 2.239409923553467
Step: 9420, train/loss: 0.2281000018119812
Step: 9420, train/grad_norm: 0.08454343676567078
Step: 9420, train/learning_rate: 3.879105133819394e-05
Step: 9420, train/epoch: 2.2417895793914795
Step: 9430, train/loss: 0.04430000111460686
Step: 9430, train/grad_norm: 8.23013124318095e-06
Step: 9430, train/learning_rate: 3.877915150951594e-05
Step: 9430, train/epoch: 2.2441694736480713
Step: 9440, train/loss: 0.0
Step: 9440, train/grad_norm: 0.00020382305956445634
Step: 9440, train/learning_rate: 3.876725531881675e-05
Step: 9440, train/epoch: 2.246549367904663
Step: 9450, train/loss: 0.0
Step: 9450, train/grad_norm: 8.075883670244366e-06
Step: 9450, train/learning_rate: 3.8755355490138754e-05
Step: 9450, train/epoch: 2.248929023742676
Step: 9460, train/loss: 0.0015999999595806003
Step: 9460, train/grad_norm: 1.0634622782390579e-07
Step: 9460, train/learning_rate: 3.874345566146076e-05
Step: 9460, train/epoch: 2.2513089179992676
Step: 9470, train/loss: 0.0
Step: 9470, train/grad_norm: 2.3299808162846602e-05
Step: 9470, train/learning_rate: 3.873155583278276e-05
Step: 9470, train/epoch: 2.2536888122558594
Step: 9480, train/loss: 0.0
Step: 9480, train/grad_norm: 0.0021553365513682365
Step: 9480, train/learning_rate: 3.871965600410476e-05
Step: 9480, train/epoch: 2.256068468093872
Step: 9490, train/loss: 0.11640000343322754
Step: 9490, train/grad_norm: 1.049667571351165e-05
Step: 9490, train/learning_rate: 3.870775981340557e-05
Step: 9490, train/epoch: 2.258448362350464
Step: 9500, train/loss: 0.0017000000225380063
Step: 9500, train/grad_norm: 4.607398750522407e-06
Step: 9500, train/learning_rate: 3.8695859984727576e-05
Step: 9500, train/epoch: 2.2608282566070557
Step: 9510, train/loss: 0.0
Step: 9510, train/grad_norm: 6.915981794008985e-05
Step: 9510, train/learning_rate: 3.868396015604958e-05
Step: 9510, train/epoch: 2.2632079124450684
Step: 9520, train/loss: 0.01489999983459711
Step: 9520, train/grad_norm: 1.4740281528702326e-07
Step: 9520, train/learning_rate: 3.867206032737158e-05
Step: 9520, train/epoch: 2.26558780670166
Step: 9530, train/loss: 0.0
Step: 9530, train/grad_norm: 3.2162308343686163e-05
Step: 9530, train/learning_rate: 3.8660160498693585e-05
Step: 9530, train/epoch: 2.267967700958252
Step: 9540, train/loss: 0.0
Step: 9540, train/grad_norm: 1.0703764985464659e-08
Step: 9540, train/learning_rate: 3.8648264307994395e-05
Step: 9540, train/epoch: 2.2703473567962646
Step: 9550, train/loss: 0.0
Step: 9550, train/grad_norm: 0.0033663988579064608
Step: 9550, train/learning_rate: 3.86363644793164e-05
Step: 9550, train/epoch: 2.2727272510528564
Step: 9560, train/loss: 0.14390000700950623
Step: 9560, train/grad_norm: 0.003642926923930645
Step: 9560, train/learning_rate: 3.86244646506384e-05
Step: 9560, train/epoch: 2.2751071453094482
Step: 9570, train/loss: 0.002099999925121665
Step: 9570, train/grad_norm: 2.8456941436161287e-05
Step: 9570, train/learning_rate: 3.8612564821960405e-05
Step: 9570, train/epoch: 2.277486801147461
Step: 9580, train/loss: 0.0
Step: 9580, train/grad_norm: 0.00020834036695305258
Step: 9580, train/learning_rate: 3.860066499328241e-05
Step: 9580, train/epoch: 2.2798666954040527
Step: 9590, train/loss: 0.0
Step: 9590, train/grad_norm: 0.0031088765244930983
Step: 9590, train/learning_rate: 3.858876880258322e-05
Step: 9590, train/epoch: 2.2822465896606445
Step: 9600, train/loss: 0.0
Step: 9600, train/grad_norm: 0.0005522621213458478
Step: 9600, train/learning_rate: 3.857686897390522e-05
Step: 9600, train/epoch: 2.2846264839172363
Step: 9610, train/loss: 0.0027000000700354576
Step: 9610, train/grad_norm: 1.372795941279037e-05
Step: 9610, train/learning_rate: 3.8564969145227224e-05
Step: 9610, train/epoch: 2.287006139755249
Step: 9620, train/loss: 0.06289999932050705
Step: 9620, train/grad_norm: 197.64830017089844
Step: 9620, train/learning_rate: 3.855306931654923e-05
Step: 9620, train/epoch: 2.289386034011841
Step: 9630, train/loss: 0.0003000000142492354
Step: 9630, train/grad_norm: 3.322017846585368e-06
Step: 9630, train/learning_rate: 3.854116948787123e-05
Step: 9630, train/epoch: 2.2917659282684326
Step: 9640, train/loss: 0.0
Step: 9640, train/grad_norm: 8.808414690975042e-07
Step: 9640, train/learning_rate: 3.852927329717204e-05
Step: 9640, train/epoch: 2.2941455841064453
Step: 9650, train/loss: 0.2328999936580658
Step: 9650, train/grad_norm: 1.1043450285797007e-05
Step: 9650, train/learning_rate: 3.851737346849404e-05
Step: 9650, train/epoch: 2.296525478363037
Step: 9660, train/loss: 0.000699999975040555
Step: 9660, train/grad_norm: 3.871194839477539
Step: 9660, train/learning_rate: 3.8505473639816046e-05
Step: 9660, train/epoch: 2.298905372619629
Step: 9670, train/loss: 0.11089999973773956
Step: 9670, train/grad_norm: 1.6755244359956123e-05
Step: 9670, train/learning_rate: 3.849357381113805e-05
Step: 9670, train/epoch: 2.3012850284576416
Step: 9680, train/loss: 0.0
Step: 9680, train/grad_norm: 0.0004454375884961337
Step: 9680, train/learning_rate: 3.848167398246005e-05
Step: 9680, train/epoch: 2.3036649227142334
Step: 9690, train/loss: 0.0
Step: 9690, train/grad_norm: 6.232540908968076e-05
Step: 9690, train/learning_rate: 3.846977779176086e-05
Step: 9690, train/epoch: 2.306044816970825
Step: 9700, train/loss: 0.0
Step: 9700, train/grad_norm: 0.0004041299398522824
Step: 9700, train/learning_rate: 3.8457877963082865e-05
Step: 9700, train/epoch: 2.308424472808838
Step: 9710, train/loss: 0.0
Step: 9710, train/grad_norm: 1.2612608202289266e-07
Step: 9710, train/learning_rate: 3.844597813440487e-05
Step: 9710, train/epoch: 2.3108043670654297
Step: 9720, train/loss: 0.0
Step: 9720, train/grad_norm: 4.449415428098291e-06
Step: 9720, train/learning_rate: 3.843407830572687e-05
Step: 9720, train/epoch: 2.3131842613220215
Step: 9730, train/loss: 0.00019999999494757503
Step: 9730, train/grad_norm: 1.144788257079199e-06
Step: 9730, train/learning_rate: 3.8422178477048874e-05
Step: 9730, train/epoch: 2.315563917160034
Step: 9740, train/loss: 0.0
Step: 9740, train/grad_norm: 2.158024727805241e-07
Step: 9740, train/learning_rate: 3.8410282286349684e-05
Step: 9740, train/epoch: 2.317943811416626
Step: 9750, train/loss: 0.0
Step: 9750, train/grad_norm: 8.242362127930392e-06
Step: 9750, train/learning_rate: 3.839838245767169e-05
Step: 9750, train/epoch: 2.3203237056732178
Step: 9760, train/loss: 0.0
Step: 9760, train/grad_norm: 8.039329259190708e-06
Step: 9760, train/learning_rate: 3.838648262899369e-05
Step: 9760, train/epoch: 2.3227033615112305
Step: 9770, train/loss: 0.0
Step: 9770, train/grad_norm: 8.521940486616586e-08
Step: 9770, train/learning_rate: 3.837458280031569e-05
Step: 9770, train/epoch: 2.3250832557678223
Step: 9780, train/loss: 0.0
Step: 9780, train/grad_norm: 2.2503392926864763e-08
Step: 9780, train/learning_rate: 3.8362682971637696e-05
Step: 9780, train/epoch: 2.327463150024414
Step: 9790, train/loss: 0.0
Step: 9790, train/grad_norm: 1.0828512841953852e-07
Step: 9790, train/learning_rate: 3.8350786780938506e-05
Step: 9790, train/epoch: 2.329843044281006
Step: 9800, train/loss: 0.0
Step: 9800, train/grad_norm: 4.24557811129489e-06
Step: 9800, train/learning_rate: 3.833888695226051e-05
Step: 9800, train/epoch: 2.3322227001190186
Step: 9810, train/loss: 0.0
Step: 9810, train/grad_norm: 7.961512164911255e-05
Step: 9810, train/learning_rate: 3.832698712358251e-05
Step: 9810, train/epoch: 2.3346025943756104
Step: 9820, train/loss: 0.0
Step: 9820, train/grad_norm: 5.646138106385479e-06
Step: 9820, train/learning_rate: 3.8315087294904515e-05
Step: 9820, train/epoch: 2.336982488632202
Step: 9830, train/loss: 0.0
Step: 9830, train/grad_norm: 9.699739678126207e-08
Step: 9830, train/learning_rate: 3.830318746622652e-05
Step: 9830, train/epoch: 2.339362144470215
Step: 9840, train/loss: 0.0
Step: 9840, train/grad_norm: 6.212704465724528e-05
Step: 9840, train/learning_rate: 3.829129127552733e-05
Step: 9840, train/epoch: 2.3417420387268066
Step: 9850, train/loss: 0.0
Step: 9850, train/grad_norm: 3.9593145629623905e-05
Step: 9850, train/learning_rate: 3.827939144684933e-05
Step: 9850, train/epoch: 2.3441219329833984
Step: 9860, train/loss: 0.0
Step: 9860, train/grad_norm: 1.4498371456284076e-05
Step: 9860, train/learning_rate: 3.8267491618171334e-05
Step: 9860, train/epoch: 2.346501588821411
Step: 9870, train/loss: 0.0
Step: 9870, train/grad_norm: 0.0005121016874909401
Step: 9870, train/learning_rate: 3.825559178949334e-05
Step: 9870, train/epoch: 2.348881483078003
Step: 9880, train/loss: 0.0
Step: 9880, train/grad_norm: 0.0001450339623261243
Step: 9880, train/learning_rate: 3.824369196081534e-05
Step: 9880, train/epoch: 2.3512613773345947
Step: 9890, train/loss: 0.0
Step: 9890, train/grad_norm: 2.340051651117392e-05
Step: 9890, train/learning_rate: 3.823179577011615e-05
Step: 9890, train/epoch: 2.3536410331726074
Step: 9900, train/loss: 0.0
Step: 9900, train/grad_norm: 3.7908701955302604e-08
Step: 9900, train/learning_rate: 3.821989594143815e-05
Step: 9900, train/epoch: 2.356020927429199
Step: 9910, train/loss: 0.0
Step: 9910, train/grad_norm: 4.25993675889913e-07
Step: 9910, train/learning_rate: 3.8207996112760156e-05
Step: 9910, train/epoch: 2.358400821685791
Step: 9920, train/loss: 0.0
Step: 9920, train/grad_norm: 4.482917148607157e-08
Step: 9920, train/learning_rate: 3.819609628408216e-05
Step: 9920, train/epoch: 2.3607804775238037
Step: 9930, train/loss: 0.0
Step: 9930, train/grad_norm: 6.702039456740749e-08
Step: 9930, train/learning_rate: 3.818419645540416e-05
Step: 9930, train/epoch: 2.3631603717803955
Step: 9940, train/loss: 0.0
Step: 9940, train/grad_norm: 2.7597465305007063e-05
Step: 9940, train/learning_rate: 3.817230026470497e-05
Step: 9940, train/epoch: 2.3655402660369873
Step: 9950, train/loss: 9.999999747378752e-05
Step: 9950, train/grad_norm: 8.594220446411782e-08
Step: 9950, train/learning_rate: 3.8160400436026976e-05
Step: 9950, train/epoch: 2.367919921875
Step: 9960, train/loss: 0.0
Step: 9960, train/grad_norm: 7.719282990592546e-09
Step: 9960, train/learning_rate: 3.814850060734898e-05
Step: 9960, train/epoch: 2.370299816131592
Step: 9970, train/loss: 0.0640999972820282
Step: 9970, train/grad_norm: 4.8847400648810435e-06
Step: 9970, train/learning_rate: 3.813660077867098e-05
Step: 9970, train/epoch: 2.3726797103881836
Step: 9980, train/loss: 0.0
Step: 9980, train/grad_norm: 1.6662926327626337e-06
Step: 9980, train/learning_rate: 3.8124700949992985e-05
Step: 9980, train/epoch: 2.3750596046447754
Step: 9990, train/loss: 0.0
Step: 9990, train/grad_norm: 1.6960763105089427e-07
Step: 9990, train/learning_rate: 3.8112804759293795e-05
Step: 9990, train/epoch: 2.377439260482788
Step: 10000, train/loss: 0.0
Step: 10000, train/grad_norm: 7.772329269073452e-08
Step: 10000, train/learning_rate: 3.81009049306158e-05
Step: 10000, train/epoch: 2.37981915473938
Step: 10010, train/loss: 0.0
Step: 10010, train/grad_norm: 3.2389650073127996e-07
Step: 10010, train/learning_rate: 3.80890051019378e-05
Step: 10010, train/epoch: 2.3821990489959717
Step: 10020, train/loss: 0.0
Step: 10020, train/grad_norm: 5.151855293661356e-05
Step: 10020, train/learning_rate: 3.8077105273259804e-05
Step: 10020, train/epoch: 2.3845787048339844
Step: 10030, train/loss: 0.03460000082850456
Step: 10030, train/grad_norm: 3.6801975511480123e-07
Step: 10030, train/learning_rate: 3.806520544458181e-05
Step: 10030, train/epoch: 2.386958599090576
Step: 10040, train/loss: 0.0
Step: 10040, train/grad_norm: 0.10743677616119385
Step: 10040, train/learning_rate: 3.805330925388262e-05
Step: 10040, train/epoch: 2.389338493347168
Step: 10050, train/loss: 0.0
Step: 10050, train/grad_norm: 4.8668027829990024e-08
Step: 10050, train/learning_rate: 3.804140942520462e-05
Step: 10050, train/epoch: 2.3917181491851807
Step: 10060, train/loss: 0.2766000032424927
Step: 10060, train/grad_norm: 2.492693056410644e-07
Step: 10060, train/learning_rate: 3.802950959652662e-05
Step: 10060, train/epoch: 2.3940980434417725
Step: 10070, train/loss: 0.0
Step: 10070, train/grad_norm: 0.0002033395430771634
Step: 10070, train/learning_rate: 3.8017609767848626e-05
Step: 10070, train/epoch: 2.3964779376983643
Step: 10080, train/loss: 0.0007999999797903001
Step: 10080, train/grad_norm: 7.146231837396044e-06
Step: 10080, train/learning_rate: 3.800570993917063e-05
Step: 10080, train/epoch: 2.398857593536377
Step: 10090, train/loss: 0.0
Step: 10090, train/grad_norm: 7.881824785727076e-06
Step: 10090, train/learning_rate: 3.799381374847144e-05
Step: 10090, train/epoch: 2.4012374877929688
Step: 10100, train/loss: 0.0
Step: 10100, train/grad_norm: 0.0009159601759165525
Step: 10100, train/learning_rate: 3.798191391979344e-05
Step: 10100, train/epoch: 2.4036173820495605
Step: 10110, train/loss: 0.0
Step: 10110, train/grad_norm: 2.433778831800737e-07
Step: 10110, train/learning_rate: 3.7970014091115445e-05
Step: 10110, train/epoch: 2.4059970378875732
Step: 10120, train/loss: 0.0
Step: 10120, train/grad_norm: 3.971625574195059e-06
Step: 10120, train/learning_rate: 3.795811426243745e-05
Step: 10120, train/epoch: 2.408376932144165
Step: 10130, train/loss: 0.0
Step: 10130, train/grad_norm: 0.00035986327566206455
Step: 10130, train/learning_rate: 3.794621443375945e-05
Step: 10130, train/epoch: 2.410756826400757
Step: 10140, train/loss: 0.0
Step: 10140, train/grad_norm: 6.976684971959912e-07
Step: 10140, train/learning_rate: 3.793431824306026e-05
Step: 10140, train/epoch: 2.4131367206573486
Step: 10150, train/loss: 0.0
Step: 10150, train/grad_norm: 1.595767713524765e-07
Step: 10150, train/learning_rate: 3.7922418414382264e-05
Step: 10150, train/epoch: 2.4155163764953613
Step: 10160, train/loss: 0.0
Step: 10160, train/grad_norm: 4.3537735905374575e-07
Step: 10160, train/learning_rate: 3.791051858570427e-05
Step: 10160, train/epoch: 2.417896270751953
Step: 10170, train/loss: 0.0
Step: 10170, train/grad_norm: 8.594943210482597e-06
Step: 10170, train/learning_rate: 3.789861875702627e-05
Step: 10170, train/epoch: 2.420276165008545
Step: 10180, train/loss: 0.0
Step: 10180, train/grad_norm: 1.1560324963966195e-07
Step: 10180, train/learning_rate: 3.788671892834827e-05
Step: 10180, train/epoch: 2.4226558208465576
Step: 10190, train/loss: 0.0
Step: 10190, train/grad_norm: 0.05519041046500206
Step: 10190, train/learning_rate: 3.787482273764908e-05
Step: 10190, train/epoch: 2.4250357151031494
Step: 10200, train/loss: 0.0
Step: 10200, train/grad_norm: 1.2639881106224493e-06
Step: 10200, train/learning_rate: 3.7862922908971086e-05
Step: 10200, train/epoch: 2.427415609359741
Step: 10210, train/loss: 0.0
Step: 10210, train/grad_norm: 0.0016165736597031355
Step: 10210, train/learning_rate: 3.785102308029309e-05
Step: 10210, train/epoch: 2.429795265197754
Step: 10220, train/loss: 0.0
Step: 10220, train/grad_norm: 2.875528252843651e-06
Step: 10220, train/learning_rate: 3.783912325161509e-05
Step: 10220, train/epoch: 2.4321751594543457
Step: 10230, train/loss: 0.0
Step: 10230, train/grad_norm: 5.839185064360208e-07
Step: 10230, train/learning_rate: 3.7827223422937095e-05
Step: 10230, train/epoch: 2.4345550537109375
Step: 10240, train/loss: 0.006099999882280827
Step: 10240, train/grad_norm: 47.88362503051758
Step: 10240, train/learning_rate: 3.7815327232237905e-05
Step: 10240, train/epoch: 2.43693470954895
Step: 10250, train/loss: 0.0
Step: 10250, train/grad_norm: 1.5602945495629683e-05
Step: 10250, train/learning_rate: 3.780342740355991e-05
Step: 10250, train/epoch: 2.439314603805542
Step: 10260, train/loss: 9.999999747378752e-05
Step: 10260, train/grad_norm: 0.00027183780912309885
Step: 10260, train/learning_rate: 3.779152757488191e-05
Step: 10260, train/epoch: 2.441694498062134
Step: 10270, train/loss: 0.0
Step: 10270, train/grad_norm: 3.725974693224998e-06
Step: 10270, train/learning_rate: 3.7779627746203914e-05
Step: 10270, train/epoch: 2.4440741539001465
Step: 10280, train/loss: 0.0
Step: 10280, train/grad_norm: 5.6949566840103216e-08
Step: 10280, train/learning_rate: 3.776772791752592e-05
Step: 10280, train/epoch: 2.4464540481567383
Step: 10290, train/loss: 0.0
Step: 10290, train/grad_norm: 4.57417399957194e-07
Step: 10290, train/learning_rate: 3.775583172682673e-05
Step: 10290, train/epoch: 2.44883394241333
Step: 10300, train/loss: 0.0
Step: 10300, train/grad_norm: 7.450652447005268e-06
Step: 10300, train/learning_rate: 3.774393189814873e-05
Step: 10300, train/epoch: 2.4512135982513428
Step: 10310, train/loss: 0.0
Step: 10310, train/grad_norm: 1.1075817383243702e-06
Step: 10310, train/learning_rate: 3.7732032069470733e-05
Step: 10310, train/epoch: 2.4535934925079346
Step: 10320, train/loss: 0.002099999925121665
Step: 10320, train/grad_norm: 2.137644332833588e-05
Step: 10320, train/learning_rate: 3.7720132240792736e-05
Step: 10320, train/epoch: 2.4559733867645264
Step: 10330, train/loss: 0.13130000233650208
Step: 10330, train/grad_norm: 0.0003144322836305946
Step: 10330, train/learning_rate: 3.770823241211474e-05
Step: 10330, train/epoch: 2.458353281021118
Step: 10340, train/loss: 0.0
Step: 10340, train/grad_norm: 8.427504383234918e-08
Step: 10340, train/learning_rate: 3.769633622141555e-05
Step: 10340, train/epoch: 2.460732936859131
Step: 10350, train/loss: 0.0
Step: 10350, train/grad_norm: 9.136182779911906e-05
Step: 10350, train/learning_rate: 3.768443639273755e-05
Step: 10350, train/epoch: 2.4631128311157227
Step: 10360, train/loss: 0.0
Step: 10360, train/grad_norm: 9.49133827816695e-05
Step: 10360, train/learning_rate: 3.7672536564059556e-05
Step: 10360, train/epoch: 2.4654927253723145
Step: 10370, train/loss: 0.11169999837875366
Step: 10370, train/grad_norm: 0.00015940038429107517
Step: 10370, train/learning_rate: 3.766063673538156e-05
Step: 10370, train/epoch: 2.467872381210327
Step: 10380, train/loss: 0.0
Step: 10380, train/grad_norm: 1.8041882654529218e-08
Step: 10380, train/learning_rate: 3.764873690670356e-05
Step: 10380, train/epoch: 2.470252275466919
Step: 10390, train/loss: 0.00019999999494757503
Step: 10390, train/grad_norm: 1.0460900739417411e-05
Step: 10390, train/learning_rate: 3.763684071600437e-05
Step: 10390, train/epoch: 2.4726321697235107
Step: 10400, train/loss: 0.1143999993801117
Step: 10400, train/grad_norm: 6.820576459176664e-07
Step: 10400, train/learning_rate: 3.7624940887326375e-05
Step: 10400, train/epoch: 2.4750118255615234
Step: 10410, train/loss: 0.0
Step: 10410, train/grad_norm: 0.00016984782996587455
Step: 10410, train/learning_rate: 3.761304105864838e-05
Step: 10410, train/epoch: 2.4773917198181152
Step: 10420, train/loss: 0.0
Step: 10420, train/grad_norm: 5.5746364523656666e-05
Step: 10420, train/learning_rate: 3.760114122997038e-05
Step: 10420, train/epoch: 2.479771614074707
Step: 10430, train/loss: 0.0
Step: 10430, train/grad_norm: 1.46583852256299e-05
Step: 10430, train/learning_rate: 3.7589241401292384e-05
Step: 10430, train/epoch: 2.4821512699127197
Step: 10440, train/loss: 0.06520000100135803
Step: 10440, train/grad_norm: 2.1645099934630707e-07
Step: 10440, train/learning_rate: 3.7577345210593194e-05
Step: 10440, train/epoch: 2.4845311641693115
Step: 10450, train/loss: 0.0
Step: 10450, train/grad_norm: 0.0002630481612868607
Step: 10450, train/learning_rate: 3.75654453819152e-05
Step: 10450, train/epoch: 2.4869110584259033
Step: 10460, train/loss: 0.0019000000320374966
Step: 10460, train/grad_norm: 7.65125435009395e-07
Step: 10460, train/learning_rate: 3.75535455532372e-05
Step: 10460, train/epoch: 2.489290714263916
Step: 10470, train/loss: 0.0
Step: 10470, train/grad_norm: 9.767437586560845e-05
Step: 10470, train/learning_rate: 3.75416457245592e-05
Step: 10470, train/epoch: 2.491670608520508
Step: 10480, train/loss: 0.0
Step: 10480, train/grad_norm: 5.459648491523694e-06
Step: 10480, train/learning_rate: 3.752974953386001e-05
Step: 10480, train/epoch: 2.4940505027770996
Step: 10490, train/loss: 0.0
Step: 10490, train/grad_norm: 1.5073486565597705e-06
Step: 10490, train/learning_rate: 3.7517849705182016e-05
Step: 10490, train/epoch: 2.4964301586151123
Step: 10500, train/loss: 0.11479999870061874
Step: 10500, train/grad_norm: 0.0003677152853924781
Step: 10500, train/learning_rate: 3.750594987650402e-05
Step: 10500, train/epoch: 2.498810052871704
Step: 10510, train/loss: 0.0
Step: 10510, train/grad_norm: 0.001592206652276218
Step: 10510, train/learning_rate: 3.749405004782602e-05
Step: 10510, train/epoch: 2.501189947128296
Step: 10520, train/loss: 0.0
Step: 10520, train/grad_norm: 0.00016715798119548708
Step: 10520, train/learning_rate: 3.7482150219148025e-05
Step: 10520, train/epoch: 2.5035698413848877
Step: 10530, train/loss: 0.0723000019788742
Step: 10530, train/grad_norm: 0.00010592129547148943
Step: 10530, train/learning_rate: 3.7470254028448835e-05
Step: 10530, train/epoch: 2.5059494972229004
Step: 10540, train/loss: 0.17970000207424164
Step: 10540, train/grad_norm: 1.0532453416089993e-05
Step: 10540, train/learning_rate: 3.745835419977084e-05
Step: 10540, train/epoch: 2.508329391479492
Step: 10550, train/loss: 0.002199999988079071
Step: 10550, train/grad_norm: 0.019099583849310875
Step: 10550, train/learning_rate: 3.744645437109284e-05
Step: 10550, train/epoch: 2.510709285736084
Step: 10560, train/loss: 0.00039999998989515007
Step: 10560, train/grad_norm: 0.035099465399980545
Step: 10560, train/learning_rate: 3.7434554542414844e-05
Step: 10560, train/epoch: 2.5130889415740967
Step: 10570, train/loss: 0.0
Step: 10570, train/grad_norm: 0.004586623050272465
Step: 10570, train/learning_rate: 3.742265471373685e-05
Step: 10570, train/epoch: 2.5154688358306885
Step: 10580, train/loss: 0.0
Step: 10580, train/grad_norm: 8.191340020857751e-05
Step: 10580, train/learning_rate: 3.741075852303766e-05
Step: 10580, train/epoch: 2.5178487300872803
Step: 10590, train/loss: 0.0
Step: 10590, train/grad_norm: 0.0005650990642607212
Step: 10590, train/learning_rate: 3.739885869435966e-05
Step: 10590, train/epoch: 2.520228385925293
Step: 10600, train/loss: 0.02930000051856041
Step: 10600, train/grad_norm: 0.0008632277604192495
Step: 10600, train/learning_rate: 3.738695886568166e-05
Step: 10600, train/epoch: 2.5226082801818848
Step: 10610, train/loss: 0.0
Step: 10610, train/grad_norm: 0.001140402164310217
Step: 10610, train/learning_rate: 3.7375059037003666e-05
Step: 10610, train/epoch: 2.5249881744384766
Step: 10620, train/loss: 0.0
Step: 10620, train/grad_norm: 1.3603965953734587e-07
Step: 10620, train/learning_rate: 3.736315920832567e-05
Step: 10620, train/epoch: 2.5273678302764893
Step: 10630, train/loss: 0.00860000029206276
Step: 10630, train/grad_norm: 0.000171938692801632
Step: 10630, train/learning_rate: 3.735126301762648e-05
Step: 10630, train/epoch: 2.529747724533081
Step: 10640, train/loss: 0.0
Step: 10640, train/grad_norm: 6.058124313312874e-07
Step: 10640, train/learning_rate: 3.733936318894848e-05
Step: 10640, train/epoch: 2.532127618789673
Step: 10650, train/loss: 0.0
Step: 10650, train/grad_norm: 7.34938680579944e-07
Step: 10650, train/learning_rate: 3.7327463360270485e-05
Step: 10650, train/epoch: 2.5345072746276855
Step: 10660, train/loss: 0.0
Step: 10660, train/grad_norm: 5.660502822024682e-08
Step: 10660, train/learning_rate: 3.731556353159249e-05
Step: 10660, train/epoch: 2.5368871688842773
Step: 10670, train/loss: 0.1453000009059906
Step: 10670, train/grad_norm: 96.06866455078125
Step: 10670, train/learning_rate: 3.730366370291449e-05
Step: 10670, train/epoch: 2.539267063140869
Step: 10680, train/loss: 0.0
Step: 10680, train/grad_norm: 7.331704546231776e-05
Step: 10680, train/learning_rate: 3.72917675122153e-05
Step: 10680, train/epoch: 2.541646718978882
Step: 10690, train/loss: 0.0
Step: 10690, train/grad_norm: 0.0001522819366073236
Step: 10690, train/learning_rate: 3.7279867683537304e-05
Step: 10690, train/epoch: 2.5440266132354736
Step: 10700, train/loss: 0.0
Step: 10700, train/grad_norm: 0.00010415990254841745
Step: 10700, train/learning_rate: 3.726796785485931e-05
Step: 10700, train/epoch: 2.5464065074920654
Step: 10710, train/loss: 0.0007999999797903001
Step: 10710, train/grad_norm: 3.2415798614238156e-06
Step: 10710, train/learning_rate: 3.725606802618131e-05
Step: 10710, train/epoch: 2.5487864017486572
Step: 10720, train/loss: 0.0
Step: 10720, train/grad_norm: 2.560392999839678e-07
Step: 10720, train/learning_rate: 3.7244168197503313e-05
Step: 10720, train/epoch: 2.55116605758667
Step: 10730, train/loss: 0.07850000262260437
Step: 10730, train/grad_norm: 4.2762127122841775e-05
Step: 10730, train/learning_rate: 3.7232272006804124e-05
Step: 10730, train/epoch: 2.5535459518432617
Step: 10740, train/loss: 0.0032999999821186066
Step: 10740, train/grad_norm: 36.00658416748047
Step: 10740, train/learning_rate: 3.7220372178126127e-05
Step: 10740, train/epoch: 2.5559258460998535
Step: 10750, train/loss: 0.003100000089034438
Step: 10750, train/grad_norm: 0.0001720747968647629
Step: 10750, train/learning_rate: 3.720847234944813e-05
Step: 10750, train/epoch: 2.558305501937866
Step: 10760, train/loss: 0.1581999957561493
Step: 10760, train/grad_norm: 0.00011906666622962803
Step: 10760, train/learning_rate: 3.719657252077013e-05
Step: 10760, train/epoch: 2.560685396194458
Step: 10770, train/loss: 0.0
Step: 10770, train/grad_norm: 0.0009256198536604643
Step: 10770, train/learning_rate: 3.7184672692092136e-05
Step: 10770, train/epoch: 2.56306529045105
Step: 10780, train/loss: 0.0
Step: 10780, train/grad_norm: 0.00013375328853726387
Step: 10780, train/learning_rate: 3.7172776501392946e-05
Step: 10780, train/epoch: 2.5654449462890625
Step: 10790, train/loss: 0.0
Step: 10790, train/grad_norm: 0.0011535794474184513
Step: 10790, train/learning_rate: 3.716087667271495e-05
Step: 10790, train/epoch: 2.5678248405456543
Step: 10800, train/loss: 0.0
Step: 10800, train/grad_norm: 5.667251343766111e-07
Step: 10800, train/learning_rate: 3.714897684403695e-05
Step: 10800, train/epoch: 2.570204734802246
Step: 10810, train/loss: 0.0
Step: 10810, train/grad_norm: 0.0008375484612770379
Step: 10810, train/learning_rate: 3.7137077015358955e-05
Step: 10810, train/epoch: 2.572584390640259
Step: 10820, train/loss: 0.0
Step: 10820, train/grad_norm: 6.41308433841914e-05
Step: 10820, train/learning_rate: 3.712517718668096e-05
Step: 10820, train/epoch: 2.5749642848968506
Step: 10830, train/loss: 0.0
Step: 10830, train/grad_norm: 1.1033042028429918e-05
Step: 10830, train/learning_rate: 3.711328099598177e-05
Step: 10830, train/epoch: 2.5773441791534424
Step: 10840, train/loss: 0.0
Step: 10840, train/grad_norm: 8.173532478394918e-06
Step: 10840, train/learning_rate: 3.710138116730377e-05
Step: 10840, train/epoch: 2.579723834991455
Step: 10850, train/loss: 0.0
Step: 10850, train/grad_norm: 1.8853514120564796e-05
Step: 10850, train/learning_rate: 3.7089481338625774e-05
Step: 10850, train/epoch: 2.582103729248047
Step: 10860, train/loss: 0.0
Step: 10860, train/grad_norm: 0.00019688122847583145
Step: 10860, train/learning_rate: 3.707758150994778e-05
Step: 10860, train/epoch: 2.5844836235046387
Step: 10870, train/loss: 0.0071000000461936
Step: 10870, train/grad_norm: 116.03899383544922
Step: 10870, train/learning_rate: 3.706568168126978e-05
Step: 10870, train/epoch: 2.5868632793426514
Step: 10880, train/loss: 0.0
Step: 10880, train/grad_norm: 7.75114222051343e-06
Step: 10880, train/learning_rate: 3.705378549057059e-05
Step: 10880, train/epoch: 2.589243173599243
Step: 10890, train/loss: 0.0
Step: 10890, train/grad_norm: 5.343699285731418e-06
Step: 10890, train/learning_rate: 3.704188566189259e-05
Step: 10890, train/epoch: 2.591623067855835
Step: 10900, train/loss: 0.11559999734163284
Step: 10900, train/grad_norm: 8.324964255734812e-07
Step: 10900, train/learning_rate: 3.7029985833214596e-05
Step: 10900, train/epoch: 2.5940029621124268
Step: 10910, train/loss: 0.0006000000284984708
Step: 10910, train/grad_norm: 3.5189671516418457
Step: 10910, train/learning_rate: 3.70180860045366e-05
Step: 10910, train/epoch: 2.5963826179504395
Step: 10920, train/loss: 0.0
Step: 10920, train/grad_norm: 5.588084306396013e-08
Step: 10920, train/learning_rate: 3.70061861758586e-05
Step: 10920, train/epoch: 2.5987625122070312
Step: 10930, train/loss: 0.0
Step: 10930, train/grad_norm: 1.2310312058616546e-07
Step: 10930, train/learning_rate: 3.699428998515941e-05
Step: 10930, train/epoch: 2.601142406463623
Step: 10940, train/loss: 0.0
Step: 10940, train/grad_norm: 1.9903225734196894e-07
Step: 10940, train/learning_rate: 3.6982390156481415e-05
Step: 10940, train/epoch: 2.6035220623016357
Step: 10950, train/loss: 0.14300000667572021
Step: 10950, train/grad_norm: 0.00010095482139149681
Step: 10950, train/learning_rate: 3.697049032780342e-05
Step: 10950, train/epoch: 2.6059019565582275
Step: 10960, train/loss: 0.0
Step: 10960, train/grad_norm: 3.062060159209068e-06
Step: 10960, train/learning_rate: 3.695859049912542e-05
Step: 10960, train/epoch: 2.6082818508148193
Step: 10970, train/loss: 0.0
Step: 10970, train/grad_norm: 3.674559138744371e-06
Step: 10970, train/learning_rate: 3.6946690670447424e-05
Step: 10970, train/epoch: 2.610661506652832
Step: 10980, train/loss: 0.0
Step: 10980, train/grad_norm: 7.321569228224689e-06
Step: 10980, train/learning_rate: 3.6934794479748234e-05
Step: 10980, train/epoch: 2.613041400909424
Step: 10990, train/loss: 0.0
Step: 10990, train/grad_norm: 7.408378621676093e-08
Step: 10990, train/learning_rate: 3.692289465107024e-05
Step: 10990, train/epoch: 2.6154212951660156
Step: 11000, train/loss: 0.0
Step: 11000, train/grad_norm: 6.069345204195997e-08
Step: 11000, train/learning_rate: 3.691099482239224e-05
Step: 11000, train/epoch: 2.6178009510040283
Step: 11010, train/loss: 0.0
Step: 11010, train/grad_norm: 6.313263867241403e-08
Step: 11010, train/learning_rate: 3.689909499371424e-05
Step: 11010, train/epoch: 2.62018084526062
Step: 11020, train/loss: 0.0
Step: 11020, train/grad_norm: 4.140701094001997e-06
Step: 11020, train/learning_rate: 3.6887195165036246e-05
Step: 11020, train/epoch: 2.622560739517212
Step: 11030, train/loss: 0.0
Step: 11030, train/grad_norm: 2.023051770549955e-08
Step: 11030, train/learning_rate: 3.6875298974337056e-05
Step: 11030, train/epoch: 2.6249403953552246
Step: 11040, train/loss: 0.0
Step: 11040, train/grad_norm: 1.861989289864141e-06
Step: 11040, train/learning_rate: 3.686339914565906e-05
Step: 11040, train/epoch: 2.6273202896118164
Step: 11050, train/loss: 0.0
Step: 11050, train/grad_norm: 1.1959374823788949e-08
Step: 11050, train/learning_rate: 3.685149931698106e-05
Step: 11050, train/epoch: 2.629700183868408
Step: 11060, train/loss: 0.0
Step: 11060, train/grad_norm: 1.1110139439551858e-06
Step: 11060, train/learning_rate: 3.6839599488303065e-05
Step: 11060, train/epoch: 2.632080078125
Step: 11070, train/loss: 0.0
Step: 11070, train/grad_norm: 6.104520480221254e-07
Step: 11070, train/learning_rate: 3.682769965962507e-05
Step: 11070, train/epoch: 2.6344597339630127
Step: 11080, train/loss: 0.0
Step: 11080, train/grad_norm: 0.0004025376692879945
Step: 11080, train/learning_rate: 3.681580346892588e-05
Step: 11080, train/epoch: 2.6368396282196045
Step: 11090, train/loss: 0.0
Step: 11090, train/grad_norm: 7.037018804112449e-06
Step: 11090, train/learning_rate: 3.680390364024788e-05
Step: 11090, train/epoch: 2.6392195224761963
Step: 11100, train/loss: 0.0
Step: 11100, train/grad_norm: 2.260795461950238e-08
Step: 11100, train/learning_rate: 3.6792003811569884e-05
Step: 11100, train/epoch: 2.641599178314209
Step: 11110, train/loss: 0.0
Step: 11110, train/grad_norm: 2.453580464134575e-06
Step: 11110, train/learning_rate: 3.678010398289189e-05
Step: 11110, train/epoch: 2.643979072570801
Step: 11120, train/loss: 0.003000000026077032
Step: 11120, train/grad_norm: 2.0868883439106867e-06
Step: 11120, train/learning_rate: 3.676820415421389e-05
Step: 11120, train/epoch: 2.6463589668273926
Step: 11130, train/loss: 0.15860000252723694
Step: 11130, train/grad_norm: 152.43080139160156
Step: 11130, train/learning_rate: 3.67563079635147e-05
Step: 11130, train/epoch: 2.6487386226654053
Step: 11140, train/loss: 0.00039999998989515007
Step: 11140, train/grad_norm: 0.07772525399923325
Step: 11140, train/learning_rate: 3.6744408134836704e-05
Step: 11140, train/epoch: 2.651118516921997
Step: 11150, train/loss: 0.12359999865293503
Step: 11150, train/grad_norm: 1.2537439033621922e-05
Step: 11150, train/learning_rate: 3.6732508306158707e-05
Step: 11150, train/epoch: 2.653498411178589
Step: 11160, train/loss: 0.0015999999595806003
Step: 11160, train/grad_norm: 0.3570459187030792
Step: 11160, train/learning_rate: 3.672060847748071e-05
Step: 11160, train/epoch: 2.6558780670166016
Step: 11170, train/loss: 9.999999747378752e-05
Step: 11170, train/grad_norm: 4.4930476406079833e-07
Step: 11170, train/learning_rate: 3.670870864880271e-05
Step: 11170, train/epoch: 2.6582579612731934
Step: 11180, train/loss: 0.0038999998942017555
Step: 11180, train/grad_norm: 8.790406695879938e-07
Step: 11180, train/learning_rate: 3.669681245810352e-05
Step: 11180, train/epoch: 2.660637855529785
Step: 11190, train/loss: 0.0
Step: 11190, train/grad_norm: 5.399260771810077e-06
Step: 11190, train/learning_rate: 3.6684912629425526e-05
Step: 11190, train/epoch: 2.663017511367798
Step: 11200, train/loss: 0.15549999475479126
Step: 11200, train/grad_norm: 3.4232630619079885e-10
Step: 11200, train/learning_rate: 3.667301280074753e-05
Step: 11200, train/epoch: 2.6653974056243896
Step: 11210, train/loss: 0.0
Step: 11210, train/grad_norm: 2.256097975106286e-09
Step: 11210, train/learning_rate: 3.666111297206953e-05
Step: 11210, train/epoch: 2.6677772998809814
Step: 11220, train/loss: 0.1054999977350235
Step: 11220, train/grad_norm: 2.953399587113381e-07
Step: 11220, train/learning_rate: 3.6649213143391535e-05
Step: 11220, train/epoch: 2.670156955718994
Step: 11230, train/loss: 0.0414000004529953
Step: 11230, train/grad_norm: 1.4829332428689668e-07
Step: 11230, train/learning_rate: 3.6637316952692345e-05
Step: 11230, train/epoch: 2.672536849975586
Step: 11240, train/loss: 0.0
Step: 11240, train/grad_norm: 6.250373107974383e-09
Step: 11240, train/learning_rate: 3.662541712401435e-05
Step: 11240, train/epoch: 2.6749167442321777
Step: 11250, train/loss: 0.0
Step: 11250, train/grad_norm: 5.704213890567189e-06
Step: 11250, train/learning_rate: 3.661351729533635e-05
Step: 11250, train/epoch: 2.6772966384887695
Step: 11260, train/loss: 0.06800000369548798
Step: 11260, train/grad_norm: 2.9709485982465367e-09
Step: 11260, train/learning_rate: 3.6601617466658354e-05
Step: 11260, train/epoch: 2.6796762943267822
Step: 11270, train/loss: 0.006899999920278788
Step: 11270, train/grad_norm: 3.847475671392431e-09
Step: 11270, train/learning_rate: 3.658971763798036e-05
Step: 11270, train/epoch: 2.682056188583374
Step: 11280, train/loss: 0.0
Step: 11280, train/grad_norm: 0.00038103258702903986
Step: 11280, train/learning_rate: 3.657782144728117e-05
Step: 11280, train/epoch: 2.684436082839966
Step: 11290, train/loss: 0.0
Step: 11290, train/grad_norm: 2.445961229113891e-07
Step: 11290, train/learning_rate: 3.656592161860317e-05
Step: 11290, train/epoch: 2.6868157386779785
Step: 11300, train/loss: 0.3127000033855438
Step: 11300, train/grad_norm: 1.0777235104342253e-08
Step: 11300, train/learning_rate: 3.655402178992517e-05
Step: 11300, train/epoch: 2.6891956329345703
Step: 11310, train/loss: 0.0
Step: 11310, train/grad_norm: 4.4723626047016296e-07
Step: 11310, train/learning_rate: 3.6542121961247176e-05
Step: 11310, train/epoch: 2.691575527191162
Step: 11320, train/loss: 0.020600000396370888
Step: 11320, train/grad_norm: 0.0006969431997276843
Step: 11320, train/learning_rate: 3.653022213256918e-05
Step: 11320, train/epoch: 2.693955183029175
Step: 11330, train/loss: 0.0
Step: 11330, train/grad_norm: 8.91632723210023e-08
Step: 11330, train/learning_rate: 3.651832594186999e-05
Step: 11330, train/epoch: 2.6963350772857666
Step: 11340, train/loss: 0.14800000190734863
Step: 11340, train/grad_norm: 2.8863091916520034e-09
Step: 11340, train/learning_rate: 3.650642611319199e-05
Step: 11340, train/epoch: 2.6987149715423584
Step: 11350, train/loss: 0.0
Step: 11350, train/grad_norm: 5.0065118273323606e-08
Step: 11350, train/learning_rate: 3.6494526284513995e-05
Step: 11350, train/epoch: 2.701094627380371
Step: 11360, train/loss: 0.0013000000035390258
Step: 11360, train/grad_norm: 1.8062396520690527e-06
Step: 11360, train/learning_rate: 3.6482626455836e-05
Step: 11360, train/epoch: 2.703474521636963
Step: 11370, train/loss: 0.0
Step: 11370, train/grad_norm: 1.7221739767592226e-07
Step: 11370, train/learning_rate: 3.6470726627158e-05
Step: 11370, train/epoch: 2.7058544158935547
Step: 11380, train/loss: 0.19449999928474426
Step: 11380, train/grad_norm: 1.8509339497541077e-05
Step: 11380, train/learning_rate: 3.645883043645881e-05
Step: 11380, train/epoch: 2.7082340717315674
Step: 11390, train/loss: 0.0
Step: 11390, train/grad_norm: 3.915772595064482e-06
Step: 11390, train/learning_rate: 3.6446930607780814e-05
Step: 11390, train/epoch: 2.710613965988159
Step: 11400, train/loss: 0.013799999840557575
Step: 11400, train/grad_norm: 0.00011956736852880567
Step: 11400, train/learning_rate: 3.643503077910282e-05
Step: 11400, train/epoch: 2.712993860244751
Step: 11410, train/loss: 9.999999747378752e-05
Step: 11410, train/grad_norm: 3.233640427424689e-06
Step: 11410, train/learning_rate: 3.642313095042482e-05
Step: 11410, train/epoch: 2.7153735160827637
Step: 11420, train/loss: 0.0
Step: 11420, train/grad_norm: 3.598459670683951e-06
Step: 11420, train/learning_rate: 3.641123112174682e-05
Step: 11420, train/epoch: 2.7177534103393555
Step: 11430, train/loss: 0.0
Step: 11430, train/grad_norm: 5.878978299733717e-06
Step: 11430, train/learning_rate: 3.639933493104763e-05
Step: 11430, train/epoch: 2.7201333045959473
Step: 11440, train/loss: 0.0
Step: 11440, train/grad_norm: 4.117535354453139e-06
Step: 11440, train/learning_rate: 3.6387435102369636e-05
Step: 11440, train/epoch: 2.722513198852539
Step: 11450, train/loss: 0.0
Step: 11450, train/grad_norm: 7.257980314534507e-07
Step: 11450, train/learning_rate: 3.637553527369164e-05
Step: 11450, train/epoch: 2.7248928546905518
Step: 11460, train/loss: 0.0
Step: 11460, train/grad_norm: 4.261256663085078e-07
Step: 11460, train/learning_rate: 3.636363544501364e-05
Step: 11460, train/epoch: 2.7272727489471436
Step: 11470, train/loss: 0.0
Step: 11470, train/grad_norm: 0.0003057218564208597
Step: 11470, train/learning_rate: 3.6351735616335645e-05
Step: 11470, train/epoch: 2.7296526432037354
Step: 11480, train/loss: 0.0
Step: 11480, train/grad_norm: 4.1541502469044644e-06
Step: 11480, train/learning_rate: 3.6339839425636455e-05
Step: 11480, train/epoch: 2.732032299041748
Step: 11490, train/loss: 0.0
Step: 11490, train/grad_norm: 3.38606383820661e-07
Step: 11490, train/learning_rate: 3.632793959695846e-05
Step: 11490, train/epoch: 2.73441219329834
Step: 11500, train/loss: 0.007199999876320362
Step: 11500, train/grad_norm: 1.232690647157142e-05
Step: 11500, train/learning_rate: 3.631603976828046e-05
Step: 11500, train/epoch: 2.7367920875549316
Step: 11510, train/loss: 0.0
Step: 11510, train/grad_norm: 1.772545510903001e-05
Step: 11510, train/learning_rate: 3.6304139939602464e-05
Step: 11510, train/epoch: 2.7391717433929443
Step: 11520, train/loss: 0.0
Step: 11520, train/grad_norm: 1.4103548551247513e-07
Step: 11520, train/learning_rate: 3.629224011092447e-05
Step: 11520, train/epoch: 2.741551637649536
Step: 11530, train/loss: 0.0
Step: 11530, train/grad_norm: 7.879492045503866e-07
Step: 11530, train/learning_rate: 3.628034392022528e-05
Step: 11530, train/epoch: 2.743931531906128
Step: 11540, train/loss: 0.00139999995008111
Step: 11540, train/grad_norm: 3.9445052379960543e-07
Step: 11540, train/learning_rate: 3.626844409154728e-05
Step: 11540, train/epoch: 2.7463111877441406
Step: 11550, train/loss: 0.19769999384880066
Step: 11550, train/grad_norm: 5.582661515290965e-07
Step: 11550, train/learning_rate: 3.6256544262869284e-05
Step: 11550, train/epoch: 2.7486910820007324
Step: 11560, train/loss: 0.12189999967813492
Step: 11560, train/grad_norm: 3.7412613892229274e-05
Step: 11560, train/learning_rate: 3.6244644434191287e-05
Step: 11560, train/epoch: 2.751070976257324
Step: 11570, train/loss: 0.028699999675154686
Step: 11570, train/grad_norm: 4.5061851778882556e-06
Step: 11570, train/learning_rate: 3.623274460551329e-05
Step: 11570, train/epoch: 2.753450632095337
Step: 11580, train/loss: 0.0
Step: 11580, train/grad_norm: 0.00021390593610703945
Step: 11580, train/learning_rate: 3.62208484148141e-05
Step: 11580, train/epoch: 2.7558305263519287
Step: 11590, train/loss: 0.0
Step: 11590, train/grad_norm: 1.3962158845970407e-05
Step: 11590, train/learning_rate: 3.62089485861361e-05
Step: 11590, train/epoch: 2.7582104206085205
Step: 11600, train/loss: 0.1257999986410141
Step: 11600, train/grad_norm: 8.681355393491685e-05
Step: 11600, train/learning_rate: 3.6197048757458106e-05
Step: 11600, train/epoch: 2.760590076446533
Step: 11610, train/loss: 0.0
Step: 11610, train/grad_norm: 4.605762569553917e-06
Step: 11610, train/learning_rate: 3.618514892878011e-05
Step: 11610, train/epoch: 2.762969970703125
Step: 11620, train/loss: 9.999999747378752e-05
Step: 11620, train/grad_norm: 0.7023497223854065
Step: 11620, train/learning_rate: 3.617324910010211e-05
Step: 11620, train/epoch: 2.765349864959717
Step: 11630, train/loss: 0.0
Step: 11630, train/grad_norm: 0.0007689399062655866
Step: 11630, train/learning_rate: 3.616135290940292e-05
Step: 11630, train/epoch: 2.7677297592163086
Step: 11640, train/loss: 0.0
Step: 11640, train/grad_norm: 3.31023293256294e-05
Step: 11640, train/learning_rate: 3.6149453080724925e-05
Step: 11640, train/epoch: 2.7701094150543213
Step: 11650, train/loss: 0.17470000684261322
Step: 11650, train/grad_norm: 0.0002967696636915207
Step: 11650, train/learning_rate: 3.613755325204693e-05
Step: 11650, train/epoch: 2.772489309310913
Step: 11660, train/loss: 0.0
Step: 11660, train/grad_norm: 0.08657099306583405
Step: 11660, train/learning_rate: 3.612565342336893e-05
Step: 11660, train/epoch: 2.774869203567505
Step: 11670, train/loss: 0.000699999975040555
Step: 11670, train/grad_norm: 0.007549799047410488
Step: 11670, train/learning_rate: 3.6113753594690934e-05
Step: 11670, train/epoch: 2.7772488594055176
Step: 11680, train/loss: 0.0
Step: 11680, train/grad_norm: 0.000676535302773118
Step: 11680, train/learning_rate: 3.6101857403991744e-05
Step: 11680, train/epoch: 2.7796287536621094
Step: 11690, train/loss: 0.0
Step: 11690, train/grad_norm: 2.0309313185862266e-05
Step: 11690, train/learning_rate: 3.608995757531375e-05
Step: 11690, train/epoch: 2.782008647918701
Step: 11700, train/loss: 0.0
Step: 11700, train/grad_norm: 3.169949559378438e-05
Step: 11700, train/learning_rate: 3.607805774663575e-05
Step: 11700, train/epoch: 2.784388303756714
Step: 11710, train/loss: 0.0
Step: 11710, train/grad_norm: 0.0005352845182642341
Step: 11710, train/learning_rate: 3.606615791795775e-05
Step: 11710, train/epoch: 2.7867681980133057
Step: 11720, train/loss: 0.0
Step: 11720, train/grad_norm: 6.082933032303117e-05
Step: 11720, train/learning_rate: 3.6054258089279756e-05
Step: 11720, train/epoch: 2.7891480922698975
Step: 11730, train/loss: 0.0
Step: 11730, train/grad_norm: 2.0127417883486487e-05
Step: 11730, train/learning_rate: 3.6042361898580566e-05
Step: 11730, train/epoch: 2.79152774810791
Step: 11740, train/loss: 0.0
Step: 11740, train/grad_norm: 0.0022536246106028557
Step: 11740, train/learning_rate: 3.603046206990257e-05
Step: 11740, train/epoch: 2.793907642364502
Step: 11750, train/loss: 0.0
Step: 11750, train/grad_norm: 0.0010538012720644474
Step: 11750, train/learning_rate: 3.601856224122457e-05
Step: 11750, train/epoch: 2.7962875366210938
Step: 11760, train/loss: 0.0
Step: 11760, train/grad_norm: 0.00018648788682185113
Step: 11760, train/learning_rate: 3.6006662412546575e-05
Step: 11760, train/epoch: 2.7986671924591064
Step: 11770, train/loss: 0.001500000013038516
Step: 11770, train/grad_norm: 10.122875213623047
Step: 11770, train/learning_rate: 3.599476258386858e-05
Step: 11770, train/epoch: 2.8010470867156982
Step: 11780, train/loss: 0.0
Step: 11780, train/grad_norm: 6.006674539094092e-06
Step: 11780, train/learning_rate: 3.598286639316939e-05
Step: 11780, train/epoch: 2.80342698097229
Step: 11790, train/loss: 9.999999747378752e-05
Step: 11790, train/grad_norm: 8.482117118546739e-05
Step: 11790, train/learning_rate: 3.597096656449139e-05
Step: 11790, train/epoch: 2.805806875228882
Step: 11800, train/loss: 0.0
Step: 11800, train/grad_norm: 3.3115686619566986e-06
Step: 11800, train/learning_rate: 3.5959066735813394e-05
Step: 11800, train/epoch: 2.8081865310668945
Step: 11810, train/loss: 0.0
Step: 11810, train/grad_norm: 7.680156159040052e-06
Step: 11810, train/learning_rate: 3.59471669071354e-05
Step: 11810, train/epoch: 2.8105664253234863
Step: 11820, train/loss: 0.000699999975040555
Step: 11820, train/grad_norm: 1.8300721421837807e-05
Step: 11820, train/learning_rate: 3.593527071643621e-05
Step: 11820, train/epoch: 2.812946319580078
Step: 11830, train/loss: 0.0
Step: 11830, train/grad_norm: 4.93109375554468e-09
Step: 11830, train/learning_rate: 3.592337088775821e-05
Step: 11830, train/epoch: 2.815325975418091
Step: 11840, train/loss: 0.18050000071525574
Step: 11840, train/grad_norm: 9.49285185924964e-06
Step: 11840, train/learning_rate: 3.591147105908021e-05
Step: 11840, train/epoch: 2.8177058696746826
Step: 11850, train/loss: 0.0
Step: 11850, train/grad_norm: 0.006048203911632299
Step: 11850, train/learning_rate: 3.5899571230402216e-05
Step: 11850, train/epoch: 2.8200857639312744
Step: 11860, train/loss: 0.0003000000142492354
Step: 11860, train/grad_norm: 0.0002470462641213089
Step: 11860, train/learning_rate: 3.588767140172422e-05
Step: 11860, train/epoch: 2.822465419769287
Step: 11870, train/loss: 0.1378999948501587
Step: 11870, train/grad_norm: 0.0009826794266700745
Step: 11870, train/learning_rate: 3.587577521102503e-05
Step: 11870, train/epoch: 2.824845314025879
Step: 11880, train/loss: 0.0017000000225380063
Step: 11880, train/grad_norm: 0.004271913319826126
Step: 11880, train/learning_rate: 3.586387538234703e-05
Step: 11880, train/epoch: 2.8272252082824707
Step: 11890, train/loss: 0.0
Step: 11890, train/grad_norm: 1.0643392670317553e-05
Step: 11890, train/learning_rate: 3.5851975553669035e-05
Step: 11890, train/epoch: 2.8296048641204834
Step: 11900, train/loss: 0.0003000000142492354
Step: 11900, train/grad_norm: 3.1352912628790364e-05
Step: 11900, train/learning_rate: 3.584007572499104e-05
Step: 11900, train/epoch: 2.831984758377075
Step: 11910, train/loss: 0.0
Step: 11910, train/grad_norm: 4.803740921488497e-06
Step: 11910, train/learning_rate: 3.582817589631304e-05
Step: 11910, train/epoch: 2.834364652633667
Step: 11920, train/loss: 0.0
Step: 11920, train/grad_norm: 4.820913545700023e-06
Step: 11920, train/learning_rate: 3.581627970561385e-05
Step: 11920, train/epoch: 2.8367443084716797
Step: 11930, train/loss: 0.0
Step: 11930, train/grad_norm: 2.6644033823686186e-06
Step: 11930, train/learning_rate: 3.5804379876935855e-05
Step: 11930, train/epoch: 2.8391242027282715
Step: 11940, train/loss: 9.999999747378752e-05
Step: 11940, train/grad_norm: 1.9770381243233714e-07
Step: 11940, train/learning_rate: 3.579248004825786e-05
Step: 11940, train/epoch: 2.8415040969848633
Step: 11950, train/loss: 0.0
Step: 11950, train/grad_norm: 4.293105121178087e-06
Step: 11950, train/learning_rate: 3.578058021957986e-05
Step: 11950, train/epoch: 2.843883752822876
Step: 11960, train/loss: 0.0
Step: 11960, train/grad_norm: 2.983959575431072e-06
Step: 11960, train/learning_rate: 3.5768680390901864e-05
Step: 11960, train/epoch: 2.8462636470794678
Step: 11970, train/loss: 0.0
Step: 11970, train/grad_norm: 3.3762221391953062e-06
Step: 11970, train/learning_rate: 3.5756784200202674e-05
Step: 11970, train/epoch: 2.8486435413360596
Step: 11980, train/loss: 0.012799999676644802
Step: 11980, train/grad_norm: 178.01441955566406
Step: 11980, train/learning_rate: 3.574488437152468e-05
Step: 11980, train/epoch: 2.8510234355926514
Step: 11990, train/loss: 0.19140000641345978
Step: 11990, train/grad_norm: 4.227001227263827e-06
Step: 11990, train/learning_rate: 3.573298454284668e-05
Step: 11990, train/epoch: 2.853403091430664
Step: 12000, train/loss: 0.0
Step: 12000, train/grad_norm: 1.3884336112823803e-05
Step: 12000, train/learning_rate: 3.572108471416868e-05
Step: 12000, train/epoch: 2.855782985687256
Step: 12010, train/loss: 9.999999747378752e-05
Step: 12010, train/grad_norm: 0.00017073920753318816
Step: 12010, train/learning_rate: 3.5709184885490686e-05
Step: 12010, train/epoch: 2.8581628799438477
Step: 12020, train/loss: 0.0
Step: 12020, train/grad_norm: 4.1085854718403425e-06
Step: 12020, train/learning_rate: 3.5697288694791496e-05
Step: 12020, train/epoch: 2.8605425357818604
Step: 12030, train/loss: 0.0
Step: 12030, train/grad_norm: 7.781662134220824e-06
Step: 12030, train/learning_rate: 3.56853888661135e-05
Step: 12030, train/epoch: 2.862922430038452
Step: 12040, train/loss: 0.0
Step: 12040, train/grad_norm: 4.702300429926254e-06
Step: 12040, train/learning_rate: 3.56734890374355e-05
Step: 12040, train/epoch: 2.865302324295044
Step: 12050, train/loss: 0.0
Step: 12050, train/grad_norm: 1.4785522580496036e-05
Step: 12050, train/learning_rate: 3.5661589208757505e-05
Step: 12050, train/epoch: 2.8676819801330566
Step: 12060, train/loss: 0.0
Step: 12060, train/grad_norm: 1.2445111678971443e-06
Step: 12060, train/learning_rate: 3.564968938007951e-05
Step: 12060, train/epoch: 2.8700618743896484
Step: 12070, train/loss: 0.0
Step: 12070, train/grad_norm: 3.758389311769861e-06
Step: 12070, train/learning_rate: 3.563779318938032e-05
Step: 12070, train/epoch: 2.8724417686462402
Step: 12080, train/loss: 0.03849999979138374
Step: 12080, train/grad_norm: 9.593421054887585e-06
Step: 12080, train/learning_rate: 3.562589336070232e-05
Step: 12080, train/epoch: 2.874821424484253
Step: 12090, train/loss: 0.0
Step: 12090, train/grad_norm: 0.0001549919688841328
Step: 12090, train/learning_rate: 3.5613993532024324e-05
Step: 12090, train/epoch: 2.8772013187408447
Step: 12100, train/loss: 0.0
Step: 12100, train/grad_norm: 8.543193689547479e-05
Step: 12100, train/learning_rate: 3.560209370334633e-05
Step: 12100, train/epoch: 2.8795812129974365
Step: 12110, train/loss: 0.0
Step: 12110, train/grad_norm: 9.584663530404214e-06
Step: 12110, train/learning_rate: 3.559019387466833e-05
Step: 12110, train/epoch: 2.881960868835449
Step: 12120, train/loss: 0.0
Step: 12120, train/grad_norm: 2.781289913400542e-05
Step: 12120, train/learning_rate: 3.557829768396914e-05
Step: 12120, train/epoch: 2.884340763092041
Step: 12130, train/loss: 0.0
Step: 12130, train/grad_norm: 0.0004267137555871159
Step: 12130, train/learning_rate: 3.556639785529114e-05
Step: 12130, train/epoch: 2.886720657348633
Step: 12140, train/loss: 0.0
Step: 12140, train/grad_norm: 5.838136530655902e-06
Step: 12140, train/learning_rate: 3.5554498026613146e-05
Step: 12140, train/epoch: 2.8891003131866455
Step: 12150, train/loss: 0.0
Step: 12150, train/grad_norm: 5.135448736837134e-05
Step: 12150, train/learning_rate: 3.554259819793515e-05
Step: 12150, train/epoch: 2.8914802074432373
Step: 12160, train/loss: 0.00019999999494757503
Step: 12160, train/grad_norm: 5.436114065560105e-07
Step: 12160, train/learning_rate: 3.553069836925715e-05
Step: 12160, train/epoch: 2.893860101699829
Step: 12170, train/loss: 0.0
Step: 12170, train/grad_norm: 1.0734988364902165e-07
Step: 12170, train/learning_rate: 3.551880217855796e-05
Step: 12170, train/epoch: 2.896239995956421
Step: 12180, train/loss: 0.0
Step: 12180, train/grad_norm: 5.0057273881520814e-08
Step: 12180, train/learning_rate: 3.5506902349879965e-05
Step: 12180, train/epoch: 2.8986196517944336
Step: 12190, train/loss: 0.13130000233650208
Step: 12190, train/grad_norm: 0.0005660812603309751
Step: 12190, train/learning_rate: 3.549500252120197e-05
Step: 12190, train/epoch: 2.9009995460510254
Step: 12200, train/loss: 0.0
Step: 12200, train/grad_norm: 0.004707945976406336
Step: 12200, train/learning_rate: 3.548310269252397e-05
Step: 12200, train/epoch: 2.903379440307617
Step: 12210, train/loss: 9.999999747378752e-05
Step: 12210, train/grad_norm: 0.0001985489798244089
Step: 12210, train/learning_rate: 3.5471202863845974e-05
Step: 12210, train/epoch: 2.90575909614563
Step: 12220, train/loss: 0.0006000000284984708
Step: 12220, train/grad_norm: 1.3047863831161521e-05
Step: 12220, train/learning_rate: 3.5459306673146784e-05
Step: 12220, train/epoch: 2.9081389904022217
Step: 12230, train/loss: 0.0
Step: 12230, train/grad_norm: 0.0006693566683679819
Step: 12230, train/learning_rate: 3.544740684446879e-05
Step: 12230, train/epoch: 2.9105188846588135
Step: 12240, train/loss: 0.0885000005364418
Step: 12240, train/grad_norm: 1.1865131455124356e-05
Step: 12240, train/learning_rate: 3.543550701579079e-05
Step: 12240, train/epoch: 2.912898540496826
Step: 12250, train/loss: 0.019099999219179153
Step: 12250, train/grad_norm: 5.192510434426367e-05
Step: 12250, train/learning_rate: 3.542360718711279e-05
Step: 12250, train/epoch: 2.915278434753418
Step: 12260, train/loss: 0.0
Step: 12260, train/grad_norm: 0.00012095129204681143
Step: 12260, train/learning_rate: 3.5411707358434796e-05
Step: 12260, train/epoch: 2.9176583290100098
Step: 12270, train/loss: 0.0
Step: 12270, train/grad_norm: 9.842267172643915e-05
Step: 12270, train/learning_rate: 3.5399811167735606e-05
Step: 12270, train/epoch: 2.9200379848480225
Step: 12280, train/loss: 0.0
Step: 12280, train/grad_norm: 0.0026611844077706337
Step: 12280, train/learning_rate: 3.538791133905761e-05
Step: 12280, train/epoch: 2.9224178791046143
Step: 12290, train/loss: 0.0
Step: 12290, train/grad_norm: 3.7941590562695637e-06
Step: 12290, train/learning_rate: 3.537601151037961e-05
Step: 12290, train/epoch: 2.924797773361206
Step: 12300, train/loss: 0.0006000000284984708
Step: 12300, train/grad_norm: 0.0004126884159632027
Step: 12300, train/learning_rate: 3.5364111681701615e-05
Step: 12300, train/epoch: 2.9271774291992188
Step: 12310, train/loss: 0.14079999923706055
Step: 12310, train/grad_norm: 6.466919148806483e-05
Step: 12310, train/learning_rate: 3.535221185302362e-05
Step: 12310, train/epoch: 2.9295573234558105
Step: 12320, train/loss: 0.0005000000237487257
Step: 12320, train/grad_norm: 0.051192983984947205
Step: 12320, train/learning_rate: 3.534031566232443e-05
Step: 12320, train/epoch: 2.9319372177124023
Step: 12330, train/loss: 0.02250000089406967
Step: 12330, train/grad_norm: 110.74881744384766
Step: 12330, train/learning_rate: 3.532841583364643e-05
Step: 12330, train/epoch: 2.934316873550415
Step: 12340, train/loss: 0.0
Step: 12340, train/grad_norm: 1.5136993170017377e-06
Step: 12340, train/learning_rate: 3.5316516004968435e-05
Step: 12340, train/epoch: 2.936696767807007
Step: 12350, train/loss: 0.0
Step: 12350, train/grad_norm: 6.360558018059237e-06
Step: 12350, train/learning_rate: 3.530461617629044e-05
Step: 12350, train/epoch: 2.9390766620635986
Step: 12360, train/loss: 0.0
Step: 12360, train/grad_norm: 3.175568252800076e-08
Step: 12360, train/learning_rate: 3.529271634761244e-05
Step: 12360, train/epoch: 2.9414565563201904
Step: 12370, train/loss: 9.999999747378752e-05
Step: 12370, train/grad_norm: 4.3976905317322235e-07
Step: 12370, train/learning_rate: 3.528082015691325e-05
Step: 12370, train/epoch: 2.943836212158203
Step: 12380, train/loss: 0.0
Step: 12380, train/grad_norm: 1.879263982118573e-05
Step: 12380, train/learning_rate: 3.5268920328235254e-05
Step: 12380, train/epoch: 2.946216106414795
Step: 12390, train/loss: 0.0
Step: 12390, train/grad_norm: 0.00015320780221372843
Step: 12390, train/learning_rate: 3.525702049955726e-05
Step: 12390, train/epoch: 2.9485960006713867
Step: 12400, train/loss: 0.0
Step: 12400, train/grad_norm: 0.024772511795163155
Step: 12400, train/learning_rate: 3.524512067087926e-05
Step: 12400, train/epoch: 2.9509756565093994
Step: 12410, train/loss: 0.0
Step: 12410, train/grad_norm: 0.00024154620768968016
Step: 12410, train/learning_rate: 3.523322084220126e-05
Step: 12410, train/epoch: 2.953355550765991
Step: 12420, train/loss: 0.0
Step: 12420, train/grad_norm: 0.00022075523156672716
Step: 12420, train/learning_rate: 3.522132465150207e-05
Step: 12420, train/epoch: 2.955735445022583
Step: 12430, train/loss: 0.0
Step: 12430, train/grad_norm: 3.843843842332717e-06
Step: 12430, train/learning_rate: 3.5209424822824076e-05
Step: 12430, train/epoch: 2.9581151008605957
Step: 12440, train/loss: 0.11949999630451202
Step: 12440, train/grad_norm: 2.3405737010762095e-05
Step: 12440, train/learning_rate: 3.519752499414608e-05
Step: 12440, train/epoch: 2.9604949951171875
Step: 12450, train/loss: 0.0
Step: 12450, train/grad_norm: 2.372103846681739e-09
Step: 12450, train/learning_rate: 3.518562516546808e-05
Step: 12450, train/epoch: 2.9628748893737793
Step: 12460, train/loss: 0.0
Step: 12460, train/grad_norm: 0.0002109512424794957
Step: 12460, train/learning_rate: 3.5173725336790085e-05
Step: 12460, train/epoch: 2.965254545211792
Step: 12470, train/loss: 0.0
Step: 12470, train/grad_norm: 0.0015698651550337672
Step: 12470, train/learning_rate: 3.5161829146090895e-05
Step: 12470, train/epoch: 2.967634439468384
Step: 12480, train/loss: 0.0
Step: 12480, train/grad_norm: 1.4001656154505326e-06
Step: 12480, train/learning_rate: 3.51499293174129e-05
Step: 12480, train/epoch: 2.9700143337249756
Step: 12490, train/loss: 0.0
Step: 12490, train/grad_norm: 0.0006361693376675248
Step: 12490, train/learning_rate: 3.51380294887349e-05
Step: 12490, train/epoch: 2.9723939895629883
Step: 12500, train/loss: 0.0
Step: 12500, train/grad_norm: 1.0196764378633816e-05
Step: 12500, train/learning_rate: 3.5126129660056904e-05
Step: 12500, train/epoch: 2.97477388381958
Step: 12510, train/loss: 0.026000000536441803
Step: 12510, train/grad_norm: 0.0030941166914999485
Step: 12510, train/learning_rate: 3.511422983137891e-05
Step: 12510, train/epoch: 2.977153778076172
Step: 12520, train/loss: 0.0
Step: 12520, train/grad_norm: 7.803107291692868e-05
Step: 12520, train/learning_rate: 3.510233364067972e-05
Step: 12520, train/epoch: 2.9795336723327637
Step: 12530, train/loss: 0.0
Step: 12530, train/grad_norm: 3.525719193930854e-06
Step: 12530, train/learning_rate: 3.509043381200172e-05
Step: 12530, train/epoch: 2.9819133281707764
Step: 12540, train/loss: 0.0
Step: 12540, train/grad_norm: 6.307109742920147e-06
Step: 12540, train/learning_rate: 3.507853398332372e-05
Step: 12540, train/epoch: 2.984293222427368
Step: 12550, train/loss: 0.0
Step: 12550, train/grad_norm: 1.7141286662081257e-05
Step: 12550, train/learning_rate: 3.5066634154645726e-05
Step: 12550, train/epoch: 2.98667311668396
Step: 12560, train/loss: 0.005100000184029341
Step: 12560, train/grad_norm: 4.08411979151424e-05
Step: 12560, train/learning_rate: 3.505473432596773e-05
Step: 12560, train/epoch: 2.9890527725219727
Step: 12570, train/loss: 0.0
Step: 12570, train/grad_norm: 6.437293791350385e-07
Step: 12570, train/learning_rate: 3.504283813526854e-05
Step: 12570, train/epoch: 2.9914326667785645
Step: 12580, train/loss: 0.00279999990016222
Step: 12580, train/grad_norm: 1.0405115972389467e-05
Step: 12580, train/learning_rate: 3.503093830659054e-05
Step: 12580, train/epoch: 2.9938125610351562
Step: 12590, train/loss: 0.0
Step: 12590, train/grad_norm: 4.0164970414480194e-05
Step: 12590, train/learning_rate: 3.5019038477912545e-05
Step: 12590, train/epoch: 2.996192216873169
Step: 12600, train/loss: 0.0
Step: 12600, train/grad_norm: 1.0750203728093766e-05
Step: 12600, train/learning_rate: 3.500713864923455e-05
Step: 12600, train/epoch: 2.9985721111297607
Step: 12606, eval/loss: 0.017988771200180054
Step: 12606, eval/accuracy: 0.9983340501785278
Step: 12606, eval/f1: 0.9982404708862305
Step: 12606, eval/runtime: 735.6787719726562
Step: 12606, eval/samples_per_second: 9.791000366210938
Step: 12606, eval/steps_per_second: 1.225000023841858
Step: 12606, train/epoch: 3.0
Step: 12610, train/loss: 0.0
Step: 12610, train/grad_norm: 1.8085043848259374e-05
Step: 12610, train/learning_rate: 3.499523882055655e-05
Step: 12610, train/epoch: 3.0009520053863525
Step: 12620, train/loss: 0.0
Step: 12620, train/grad_norm: 0.0004447149985935539
Step: 12620, train/learning_rate: 3.498334262985736e-05
Step: 12620, train/epoch: 3.0033316612243652
Step: 12630, train/loss: 0.0
Step: 12630, train/grad_norm: 0.0005757937906309962
Step: 12630, train/learning_rate: 3.4971442801179364e-05
Step: 12630, train/epoch: 3.005711555480957
Step: 12640, train/loss: 0.0
Step: 12640, train/grad_norm: 1.9320899809827097e-05
Step: 12640, train/learning_rate: 3.495954297250137e-05
Step: 12640, train/epoch: 3.008091449737549
Step: 12650, train/loss: 0.0
Step: 12650, train/grad_norm: 1.5382808271624526e-07
Step: 12650, train/learning_rate: 3.494764314382337e-05
Step: 12650, train/epoch: 3.0104711055755615
Step: 12660, train/loss: 0.0
Step: 12660, train/grad_norm: 3.886379545292584e-06
Step: 12660, train/learning_rate: 3.493574331514537e-05
Step: 12660, train/epoch: 3.0128509998321533
Step: 12670, train/loss: 0.0
Step: 12670, train/grad_norm: 2.063710599031765e-05
Step: 12670, train/learning_rate: 3.4923847124446183e-05
Step: 12670, train/epoch: 3.015230894088745
Step: 12680, train/loss: 0.0
Step: 12680, train/grad_norm: 6.906278713358915e-07
Step: 12680, train/learning_rate: 3.4911947295768186e-05
Step: 12680, train/epoch: 3.017610549926758
Step: 12690, train/loss: 9.999999747378752e-05
Step: 12690, train/grad_norm: 1.1133976840937976e-05
Step: 12690, train/learning_rate: 3.490004746709019e-05
Step: 12690, train/epoch: 3.0199904441833496
Step: 12700, train/loss: 0.0
Step: 12700, train/grad_norm: 4.016504135506693e-06
Step: 12700, train/learning_rate: 3.488814763841219e-05
Step: 12700, train/epoch: 3.0223703384399414
Step: 12710, train/loss: 0.0
Step: 12710, train/grad_norm: 1.083553513581137e-07
Step: 12710, train/learning_rate: 3.4876247809734195e-05
Step: 12710, train/epoch: 3.024750232696533
Step: 12720, train/loss: 0.0
Step: 12720, train/grad_norm: 9.393592392825667e-08
Step: 12720, train/learning_rate: 3.4864351619035006e-05
Step: 12720, train/epoch: 3.027129888534546
Step: 12730, train/loss: 0.0
Step: 12730, train/grad_norm: 4.4797410225783096e-08
Step: 12730, train/learning_rate: 3.485245179035701e-05
Step: 12730, train/epoch: 3.0295097827911377
Step: 12740, train/loss: 0.0
Step: 12740, train/grad_norm: 3.801115326496074e-06
Step: 12740, train/learning_rate: 3.484055196167901e-05
Step: 12740, train/epoch: 3.0318896770477295
Step: 12750, train/loss: 0.08829999715089798
Step: 12750, train/grad_norm: 4.881137647316791e-05
Step: 12750, train/learning_rate: 3.4828652133001015e-05
Step: 12750, train/epoch: 3.034269332885742
Step: 12760, train/loss: 0.0
Step: 12760, train/grad_norm: 0.0006949426606297493
Step: 12760, train/learning_rate: 3.481675230432302e-05
Step: 12760, train/epoch: 3.036649227142334
Step: 12770, train/loss: 0.0
Step: 12770, train/grad_norm: 9.849984326446015e-13
Step: 12770, train/learning_rate: 3.480485611362383e-05
Step: 12770, train/epoch: 3.039029121398926
Step: 12780, train/loss: 0.0
Step: 12780, train/grad_norm: 0.0013917782343924046
Step: 12780, train/learning_rate: 3.479295628494583e-05
Step: 12780, train/epoch: 3.0414087772369385
Step: 12790, train/loss: 0.0
Step: 12790, train/grad_norm: 0.0008553409716114402
Step: 12790, train/learning_rate: 3.4781056456267834e-05
Step: 12790, train/epoch: 3.0437886714935303
Step: 12800, train/loss: 0.0
Step: 12800, train/grad_norm: 0.0527871772646904
Step: 12800, train/learning_rate: 3.476915662758984e-05
Step: 12800, train/epoch: 3.046168565750122
Step: 12810, train/loss: 0.0
Step: 12810, train/grad_norm: 6.593023954337696e-06
Step: 12810, train/learning_rate: 3.475725679891184e-05
Step: 12810, train/epoch: 3.0485482215881348
Step: 12820, train/loss: 0.0
Step: 12820, train/grad_norm: 5.901267286390066e-05
Step: 12820, train/learning_rate: 3.474536060821265e-05
Step: 12820, train/epoch: 3.0509281158447266
Step: 12830, train/loss: 0.0
Step: 12830, train/grad_norm: 0.00022234991774894297
Step: 12830, train/learning_rate: 3.473346077953465e-05
Step: 12830, train/epoch: 3.0533080101013184
Step: 12840, train/loss: 0.0
Step: 12840, train/grad_norm: 1.3068287444184534e-05
Step: 12840, train/learning_rate: 3.4721560950856656e-05
Step: 12840, train/epoch: 3.055687665939331
Step: 12850, train/loss: 0.0
Step: 12850, train/grad_norm: 0.00045395264169201255
Step: 12850, train/learning_rate: 3.470966112217866e-05
Step: 12850, train/epoch: 3.058067560195923
Step: 12860, train/loss: 0.0
Step: 12860, train/grad_norm: 1.2590364349307492e-05
Step: 12860, train/learning_rate: 3.469776129350066e-05
Step: 12860, train/epoch: 3.0604474544525146
Step: 12870, train/loss: 0.0
Step: 12870, train/grad_norm: 7.546537381131202e-05
Step: 12870, train/learning_rate: 3.468586510280147e-05
Step: 12870, train/epoch: 3.0628271102905273
Step: 12880, train/loss: 0.0
Step: 12880, train/grad_norm: 0.00048193903057835996
Step: 12880, train/learning_rate: 3.4673965274123475e-05
Step: 12880, train/epoch: 3.065207004547119
Step: 12890, train/loss: 0.0
Step: 12890, train/grad_norm: 8.220327799790539e-06
Step: 12890, train/learning_rate: 3.466206544544548e-05
Step: 12890, train/epoch: 3.067586898803711
Step: 12900, train/loss: 0.0
Step: 12900, train/grad_norm: 9.813482392928563e-06
Step: 12900, train/learning_rate: 3.465016561676748e-05
Step: 12900, train/epoch: 3.0699667930603027
Step: 12910, train/loss: 0.0
Step: 12910, train/grad_norm: 1.0635483249643585e-06
Step: 12910, train/learning_rate: 3.4638265788089484e-05
Step: 12910, train/epoch: 3.0723464488983154
Step: 12920, train/loss: 0.00039999998989515007
Step: 12920, train/grad_norm: 5.401311136665754e-05
Step: 12920, train/learning_rate: 3.4626369597390294e-05
Step: 12920, train/epoch: 3.0747263431549072
Step: 12930, train/loss: 0.0
Step: 12930, train/grad_norm: 0.009196819737553596
Step: 12930, train/learning_rate: 3.46144697687123e-05
Step: 12930, train/epoch: 3.077106237411499
Step: 12940, train/loss: 0.0
Step: 12940, train/grad_norm: 0.062380123883485794
Step: 12940, train/learning_rate: 3.46025699400343e-05
Step: 12940, train/epoch: 3.0794858932495117
Step: 12950, train/loss: 0.0
Step: 12950, train/grad_norm: 0.000748211401514709
Step: 12950, train/learning_rate: 3.45906701113563e-05
Step: 12950, train/epoch: 3.0818657875061035
Step: 12960, train/loss: 0.38609999418258667
Step: 12960, train/grad_norm: 1.2062450593930407e-08
Step: 12960, train/learning_rate: 3.4578770282678306e-05
Step: 12960, train/epoch: 3.0842456817626953
Step: 12970, train/loss: 0.1031000018119812
Step: 12970, train/grad_norm: 2.674484562703583e-07
Step: 12970, train/learning_rate: 3.4566874091979116e-05
Step: 12970, train/epoch: 3.086625337600708
Step: 12980, train/loss: 9.999999747378752e-05
Step: 12980, train/grad_norm: 2.3350863557425328e-05
Step: 12980, train/learning_rate: 3.455497426330112e-05
Step: 12980, train/epoch: 3.0890052318573
Step: 12990, train/loss: 9.999999747378752e-05
Step: 12990, train/grad_norm: 0.0002357085613766685
Step: 12990, train/learning_rate: 3.454307443462312e-05
Step: 12990, train/epoch: 3.0913851261138916
Step: 13000, train/loss: 0.0
Step: 13000, train/grad_norm: 9.419423440704122e-05
Step: 13000, train/learning_rate: 3.4531174605945125e-05
Step: 13000, train/epoch: 3.0937647819519043
Step: 13010, train/loss: 0.0
Step: 13010, train/grad_norm: 5.230039278103504e-06
Step: 13010, train/learning_rate: 3.451927477726713e-05
Step: 13010, train/epoch: 3.096144676208496
Step: 13020, train/loss: 0.0
Step: 13020, train/grad_norm: 1.2246628102730028e-05
Step: 13020, train/learning_rate: 3.450737858656794e-05
Step: 13020, train/epoch: 3.098524570465088
Step: 13030, train/loss: 0.0
Step: 13030, train/grad_norm: 0.00024118561123032123
Step: 13030, train/learning_rate: 3.449547875788994e-05
Step: 13030, train/epoch: 3.1009042263031006
Step: 13040, train/loss: 0.0
Step: 13040, train/grad_norm: 5.383427037486399e-07
Step: 13040, train/learning_rate: 3.4483578929211944e-05
Step: 13040, train/epoch: 3.1032841205596924
Step: 13050, train/loss: 0.0
Step: 13050, train/grad_norm: 3.0549492180398374e-07
Step: 13050, train/learning_rate: 3.447167910053395e-05
Step: 13050, train/epoch: 3.105664014816284
Step: 13060, train/loss: 0.010999999940395355
Step: 13060, train/grad_norm: 6.578903253284807e-07
Step: 13060, train/learning_rate: 3.445977927185595e-05
Step: 13060, train/epoch: 3.108043670654297
Step: 13070, train/loss: 0.0
Step: 13070, train/grad_norm: 1.3826418410189945e-07
Step: 13070, train/learning_rate: 3.444788308115676e-05
Step: 13070, train/epoch: 3.1104235649108887
Step: 13080, train/loss: 0.0
Step: 13080, train/grad_norm: 2.0244256049295473e-08
Step: 13080, train/learning_rate: 3.4435983252478763e-05
Step: 13080, train/epoch: 3.1128034591674805
Step: 13090, train/loss: 0.0
Step: 13090, train/grad_norm: 2.6786315387994364e-09
Step: 13090, train/learning_rate: 3.4424083423800766e-05
Step: 13090, train/epoch: 3.1151833534240723
Step: 13100, train/loss: 0.0
Step: 13100, train/grad_norm: 4.69947902814738e-09
Step: 13100, train/learning_rate: 3.441218359512277e-05
Step: 13100, train/epoch: 3.117563009262085
Step: 13110, train/loss: 0.0
Step: 13110, train/grad_norm: 0.08104365319013596
Step: 13110, train/learning_rate: 3.440028376644477e-05
Step: 13110, train/epoch: 3.1199429035186768
Step: 13120, train/loss: 0.0
Step: 13120, train/grad_norm: 1.2909117685921956e-07
Step: 13120, train/learning_rate: 3.438838757574558e-05
Step: 13120, train/epoch: 3.1223227977752686
Step: 13130, train/loss: 0.0
Step: 13130, train/grad_norm: 7.676634794506754e-08
Step: 13130, train/learning_rate: 3.4376487747067586e-05
Step: 13130, train/epoch: 3.1247024536132812
Step: 13140, train/loss: 0.0
Step: 13140, train/grad_norm: 8.988934609988064e-07
Step: 13140, train/learning_rate: 3.436458791838959e-05
Step: 13140, train/epoch: 3.127082347869873
Step: 13150, train/loss: 0.0
Step: 13150, train/grad_norm: 4.364577321780416e-08
Step: 13150, train/learning_rate: 3.435268808971159e-05
Step: 13150, train/epoch: 3.129462242126465
Step: 13160, train/loss: 0.0
Step: 13160, train/grad_norm: 3.395047679077834e-05
Step: 13160, train/learning_rate: 3.43407918990124e-05
Step: 13160, train/epoch: 3.1318418979644775
Step: 13170, train/loss: 0.0
Step: 13170, train/grad_norm: 3.9310499744260596e-08
Step: 13170, train/learning_rate: 3.4328892070334405e-05
Step: 13170, train/epoch: 3.1342217922210693
Step: 13180, train/loss: 0.0
Step: 13180, train/grad_norm: 3.8287666370706575e-07
Step: 13180, train/learning_rate: 3.431699224165641e-05
Step: 13180, train/epoch: 3.136601686477661
Step: 13190, train/loss: 0.0
Step: 13190, train/grad_norm: 4.6765158003836405e-06
Step: 13190, train/learning_rate: 3.430509241297841e-05
Step: 13190, train/epoch: 3.138981342315674
Step: 13200, train/loss: 0.0
Step: 13200, train/grad_norm: 2.0316037989687175e-05
Step: 13200, train/learning_rate: 3.4293192584300414e-05
Step: 13200, train/epoch: 3.1413612365722656
Step: 13210, train/loss: 0.0
Step: 13210, train/grad_norm: 4.8621970449858054e-08
Step: 13210, train/learning_rate: 3.4281296393601224e-05
Step: 13210, train/epoch: 3.1437411308288574
Step: 13220, train/loss: 0.0
Step: 13220, train/grad_norm: 8.366131623915862e-06
Step: 13220, train/learning_rate: 3.426939656492323e-05
Step: 13220, train/epoch: 3.14612078666687
Step: 13230, train/loss: 0.11410000175237656
Step: 13230, train/grad_norm: 5.248732122709043e-06
Step: 13230, train/learning_rate: 3.425749673624523e-05
Step: 13230, train/epoch: 3.148500680923462
Step: 13240, train/loss: 0.0
Step: 13240, train/grad_norm: 4.361460810287099e-07
Step: 13240, train/learning_rate: 3.424559690756723e-05
Step: 13240, train/epoch: 3.1508805751800537
Step: 13250, train/loss: 0.0
Step: 13250, train/grad_norm: 0.0963679850101471
Step: 13250, train/learning_rate: 3.4233697078889236e-05
Step: 13250, train/epoch: 3.1532604694366455
Step: 13260, train/loss: 0.0
Step: 13260, train/grad_norm: 5.186568614590215e-07
Step: 13260, train/learning_rate: 3.4221800888190046e-05
Step: 13260, train/epoch: 3.155640125274658
Step: 13270, train/loss: 0.0
Step: 13270, train/grad_norm: 7.727429874648806e-06
Step: 13270, train/learning_rate: 3.420990105951205e-05
Step: 13270, train/epoch: 3.15802001953125
Step: 13280, train/loss: 0.012199999764561653
Step: 13280, train/grad_norm: 1.2948260064149508e-06
Step: 13280, train/learning_rate: 3.419800123083405e-05
Step: 13280, train/epoch: 3.160399913787842
Step: 13290, train/loss: 0.0
Step: 13290, train/grad_norm: 5.2947284530091565e-06
Step: 13290, train/learning_rate: 3.4186101402156055e-05
Step: 13290, train/epoch: 3.1627795696258545
Step: 13300, train/loss: 0.0
Step: 13300, train/grad_norm: 0.0003831216599792242
Step: 13300, train/learning_rate: 3.417420157347806e-05
Step: 13300, train/epoch: 3.1651594638824463
Step: 13310, train/loss: 0.0
Step: 13310, train/grad_norm: 0.01138065755367279
Step: 13310, train/learning_rate: 3.416230538277887e-05
Step: 13310, train/epoch: 3.167539358139038
Step: 13320, train/loss: 0.03889999911189079
Step: 13320, train/grad_norm: 1.116225831765405e-07
Step: 13320, train/learning_rate: 3.415040555410087e-05
Step: 13320, train/epoch: 3.169919013977051
Step: 13330, train/loss: 0.0
Step: 13330, train/grad_norm: 2.879296578939261e-09
Step: 13330, train/learning_rate: 3.4138505725422874e-05
Step: 13330, train/epoch: 3.1722989082336426
Step: 13340, train/loss: 0.0
Step: 13340, train/grad_norm: 0.0007343444740399718
Step: 13340, train/learning_rate: 3.412660589674488e-05
Step: 13340, train/epoch: 3.1746788024902344
Step: 13350, train/loss: 0.0
Step: 13350, train/grad_norm: 9.67899893566937e-09
Step: 13350, train/learning_rate: 3.411470606806688e-05
Step: 13350, train/epoch: 3.177058458328247
Step: 13360, train/loss: 0.0
Step: 13360, train/grad_norm: 1.7950734648719546e-11
Step: 13360, train/learning_rate: 3.410280987736769e-05
Step: 13360, train/epoch: 3.179438352584839
Step: 13370, train/loss: 0.0
Step: 13370, train/grad_norm: 0.003043445758521557
Step: 13370, train/learning_rate: 3.409091004868969e-05
Step: 13370, train/epoch: 3.1818182468414307
Step: 13380, train/loss: 0.0
Step: 13380, train/grad_norm: 0.010234590619802475
Step: 13380, train/learning_rate: 3.4079010220011696e-05
Step: 13380, train/epoch: 3.1841979026794434
Step: 13390, train/loss: 0.0
Step: 13390, train/grad_norm: 0.0005844742991030216
Step: 13390, train/learning_rate: 3.40671103913337e-05
Step: 13390, train/epoch: 3.186577796936035
Step: 13400, train/loss: 9.999999747378752e-05
Step: 13400, train/grad_norm: 9.876764961447293e-10
Step: 13400, train/learning_rate: 3.40552105626557e-05
Step: 13400, train/epoch: 3.188957691192627
Step: 13410, train/loss: 0.0
Step: 13410, train/grad_norm: 5.868026117100689e-12
Step: 13410, train/learning_rate: 3.404331437195651e-05
Step: 13410, train/epoch: 3.1913373470306396
Step: 13420, train/loss: 0.0
Step: 13420, train/grad_norm: 4.369192083686357e-06
Step: 13420, train/learning_rate: 3.4031414543278515e-05
Step: 13420, train/epoch: 3.1937172412872314
Step: 13430, train/loss: 0.0
Step: 13430, train/grad_norm: 7.982146144058788e-08
Step: 13430, train/learning_rate: 3.401951471460052e-05
Step: 13430, train/epoch: 3.1960971355438232
Step: 13440, train/loss: 0.0
Step: 13440, train/grad_norm: 2.5464486422266397e-11
Step: 13440, train/learning_rate: 3.400761488592252e-05
Step: 13440, train/epoch: 3.198477029800415
Step: 13450, train/loss: 9.999999747378752e-05
Step: 13450, train/grad_norm: 2.4724700509182185e-08
Step: 13450, train/learning_rate: 3.3995715057244524e-05
Step: 13450, train/epoch: 3.2008566856384277
Step: 13460, train/loss: 0.0
Step: 13460, train/grad_norm: 7.763671305838216e-07
Step: 13460, train/learning_rate: 3.3983818866545334e-05
Step: 13460, train/epoch: 3.2032365798950195
Step: 13470, train/loss: 0.0
Step: 13470, train/grad_norm: 2.3822416261864987e-11
Step: 13470, train/learning_rate: 3.397191903786734e-05
Step: 13470, train/epoch: 3.2056164741516113
Step: 13480, train/loss: 0.0
Step: 13480, train/grad_norm: 5.643947224598378e-05
Step: 13480, train/learning_rate: 3.396001920918934e-05
Step: 13480, train/epoch: 3.207996129989624
Step: 13490, train/loss: 0.0
Step: 13490, train/grad_norm: 4.272810656402726e-06
Step: 13490, train/learning_rate: 3.3948119380511343e-05
Step: 13490, train/epoch: 3.210376024246216
Step: 13500, train/loss: 0.0
Step: 13500, train/grad_norm: 1.3984588660698183e-11
Step: 13500, train/learning_rate: 3.3936219551833346e-05
Step: 13500, train/epoch: 3.2127559185028076
Step: 13510, train/loss: 0.0
Step: 13510, train/grad_norm: 7.099400511911824e-10
Step: 13510, train/learning_rate: 3.3924323361134157e-05
Step: 13510, train/epoch: 3.2151355743408203
Step: 13520, train/loss: 0.0
Step: 13520, train/grad_norm: 4.1722240246011566e-11
Step: 13520, train/learning_rate: 3.391242353245616e-05
Step: 13520, train/epoch: 3.217515468597412
Step: 13530, train/loss: 0.0
Step: 13530, train/grad_norm: 9.086061764085329e-11
Step: 13530, train/learning_rate: 3.390052370377816e-05
Step: 13530, train/epoch: 3.219895362854004
Step: 13540, train/loss: 0.0
Step: 13540, train/grad_norm: 9.894611519012386e-11
Step: 13540, train/learning_rate: 3.3888623875100166e-05
Step: 13540, train/epoch: 3.2222750186920166
Step: 13550, train/loss: 0.0
Step: 13550, train/grad_norm: 1.8991480388308446e-08
Step: 13550, train/learning_rate: 3.387672404642217e-05
Step: 13550, train/epoch: 3.2246549129486084
Step: 13560, train/loss: 0.0
Step: 13560, train/grad_norm: 1.1966749813296929e-09
Step: 13560, train/learning_rate: 3.386482785572298e-05
Step: 13560, train/epoch: 3.2270348072052
Step: 13570, train/loss: 0.0
Step: 13570, train/grad_norm: 3.931987721528252e-11
Step: 13570, train/learning_rate: 3.385292802704498e-05
Step: 13570, train/epoch: 3.229414463043213
Step: 13580, train/loss: 0.0
Step: 13580, train/grad_norm: 7.0177882602329156e-12
Step: 13580, train/learning_rate: 3.3841028198366985e-05
Step: 13580, train/epoch: 3.2317943572998047
Step: 13590, train/loss: 0.0
Step: 13590, train/grad_norm: 3.296204198477426e-08
Step: 13590, train/learning_rate: 3.382912836968899e-05
Step: 13590, train/epoch: 3.2341742515563965
Step: 13600, train/loss: 0.0
Step: 13600, train/grad_norm: 6.234470578680984e-11
Step: 13600, train/learning_rate: 3.381722854101099e-05
Step: 13600, train/epoch: 3.236553907394409
Step: 13610, train/loss: 0.0
Step: 13610, train/grad_norm: 1.5353072146950808e-09
Step: 13610, train/learning_rate: 3.38053323503118e-05
Step: 13610, train/epoch: 3.238933801651001
Step: 13620, train/loss: 0.0
Step: 13620, train/grad_norm: 4.5802556192242605e-10
Step: 13620, train/learning_rate: 3.3793432521633804e-05
Step: 13620, train/epoch: 3.2413136959075928
Step: 13630, train/loss: 0.0
Step: 13630, train/grad_norm: 5.010057066101581e-06
Step: 13630, train/learning_rate: 3.378153269295581e-05
Step: 13630, train/epoch: 3.2436935901641846
Step: 13640, train/loss: 0.0
Step: 13640, train/grad_norm: 0.008941718377172947
Step: 13640, train/learning_rate: 3.376963286427781e-05
Step: 13640, train/epoch: 3.2460732460021973
Step: 13650, train/loss: 0.0
Step: 13650, train/grad_norm: 4.4381237573887233e-10
Step: 13650, train/learning_rate: 3.375773303559981e-05
Step: 13650, train/epoch: 3.248453140258789
Step: 13660, train/loss: 0.0
Step: 13660, train/grad_norm: 6.250225936810239e-08
Step: 13660, train/learning_rate: 3.374583684490062e-05
Step: 13660, train/epoch: 3.250833034515381
Step: 13670, train/loss: 0.0
Step: 13670, train/grad_norm: 1.3128317322141925e-09
Step: 13670, train/learning_rate: 3.3733937016222626e-05
Step: 13670, train/epoch: 3.2532126903533936
Step: 13680, train/loss: 0.0
Step: 13680, train/grad_norm: 1.8153890612992996e-09
Step: 13680, train/learning_rate: 3.372203718754463e-05
Step: 13680, train/epoch: 3.2555925846099854
Step: 13690, train/loss: 0.0
Step: 13690, train/grad_norm: 1.7675622075330466e-05
Step: 13690, train/learning_rate: 3.371013735886663e-05
Step: 13690, train/epoch: 3.257972478866577
Step: 13700, train/loss: 0.0
Step: 13700, train/grad_norm: 8.520801252076549e-13
Step: 13700, train/learning_rate: 3.3698237530188635e-05
Step: 13700, train/epoch: 3.26035213470459
Step: 13710, train/loss: 0.0
Step: 13710, train/grad_norm: 5.090432750876062e-06
Step: 13710, train/learning_rate: 3.3686341339489445e-05
Step: 13710, train/epoch: 3.2627320289611816
Step: 13720, train/loss: 0.03590000048279762
Step: 13720, train/grad_norm: 4.763212047009802e-10
Step: 13720, train/learning_rate: 3.367444151081145e-05
Step: 13720, train/epoch: 3.2651119232177734
Step: 13730, train/loss: 0.1023000031709671
Step: 13730, train/grad_norm: 6.748471226281083e-10
Step: 13730, train/learning_rate: 3.366254168213345e-05
Step: 13730, train/epoch: 3.267491579055786
Step: 13740, train/loss: 0.0
Step: 13740, train/grad_norm: 6.262888200581074e-05
Step: 13740, train/learning_rate: 3.3650641853455454e-05
Step: 13740, train/epoch: 3.269871473312378
Step: 13750, train/loss: 0.0
Step: 13750, train/grad_norm: 7.731606466165886e-10
Step: 13750, train/learning_rate: 3.363874202477746e-05
Step: 13750, train/epoch: 3.2722513675689697
Step: 13760, train/loss: 0.0
Step: 13760, train/grad_norm: 3.0312818921629514e-09
Step: 13760, train/learning_rate: 3.362684583407827e-05
Step: 13760, train/epoch: 3.2746310234069824
Step: 13770, train/loss: 0.0
Step: 13770, train/grad_norm: 6.261979129673634e-11
Step: 13770, train/learning_rate: 3.361494600540027e-05
Step: 13770, train/epoch: 3.277010917663574
Step: 13780, train/loss: 0.0
Step: 13780, train/grad_norm: 4.926498604618246e-07
Step: 13780, train/learning_rate: 3.360304617672227e-05
Step: 13780, train/epoch: 3.279390811920166
Step: 13790, train/loss: 0.00019999999494757503
Step: 13790, train/grad_norm: 8.889701348380186e-06
Step: 13790, train/learning_rate: 3.3591146348044276e-05
Step: 13790, train/epoch: 3.2817704677581787
Step: 13800, train/loss: 0.0
Step: 13800, train/grad_norm: 1.2135567724336216e-11
Step: 13800, train/learning_rate: 3.357924651936628e-05
Step: 13800, train/epoch: 3.2841503620147705
Step: 13810, train/loss: 0.12189999967813492
Step: 13810, train/grad_norm: 9.296523550972324e-10
Step: 13810, train/learning_rate: 3.356735032866709e-05
Step: 13810, train/epoch: 3.2865302562713623
Step: 13820, train/loss: 0.0
Step: 13820, train/grad_norm: 9.831944680627203e-07
Step: 13820, train/learning_rate: 3.355545049998909e-05
Step: 13820, train/epoch: 3.288910150527954
Step: 13830, train/loss: 0.00019999999494757503
Step: 13830, train/grad_norm: 5.228589998296229e-06
Step: 13830, train/learning_rate: 3.3543550671311095e-05
Step: 13830, train/epoch: 3.291289806365967
Step: 13840, train/loss: 0.0
Step: 13840, train/grad_norm: 6.546532760332013e-10
Step: 13840, train/learning_rate: 3.35316508426331e-05
Step: 13840, train/epoch: 3.2936697006225586
Step: 13850, train/loss: 0.0
Step: 13850, train/grad_norm: 2.906018004011912e-08
Step: 13850, train/learning_rate: 3.35197510139551e-05
Step: 13850, train/epoch: 3.2960495948791504
Step: 13860, train/loss: 0.0
Step: 13860, train/grad_norm: 0.0005271414993330836
Step: 13860, train/learning_rate: 3.350785482325591e-05
Step: 13860, train/epoch: 3.298429250717163
Step: 13870, train/loss: 0.0
Step: 13870, train/grad_norm: 7.37258289973397e-08
Step: 13870, train/learning_rate: 3.3495954994577914e-05
Step: 13870, train/epoch: 3.300809144973755
Step: 13880, train/loss: 0.0
Step: 13880, train/grad_norm: 4.18136458790741e-08
Step: 13880, train/learning_rate: 3.348405516589992e-05
Step: 13880, train/epoch: 3.3031890392303467
Step: 13890, train/loss: 0.0
Step: 13890, train/grad_norm: 0.00016822051838971674
Step: 13890, train/learning_rate: 3.347215533722192e-05
Step: 13890, train/epoch: 3.3055686950683594
Step: 13900, train/loss: 0.0
Step: 13900, train/grad_norm: 4.564115840821614e-08
Step: 13900, train/learning_rate: 3.3460255508543923e-05
Step: 13900, train/epoch: 3.307948589324951
Step: 13910, train/loss: 0.0
Step: 13910, train/grad_norm: 9.436303127774437e-11
Step: 13910, train/learning_rate: 3.3448359317844734e-05
Step: 13910, train/epoch: 3.310328483581543
Step: 13920, train/loss: 0.0005000000237487257
Step: 13920, train/grad_norm: 9.505174425328278e-09
Step: 13920, train/learning_rate: 3.3436459489166737e-05
Step: 13920, train/epoch: 3.3127081394195557
Step: 13930, train/loss: 0.0
Step: 13930, train/grad_norm: 7.044012183439474e-13
Step: 13930, train/learning_rate: 3.342455966048874e-05
Step: 13930, train/epoch: 3.3150880336761475
Step: 13940, train/loss: 0.13979999721050262
Step: 13940, train/grad_norm: 3.4170490881280102e-09
Step: 13940, train/learning_rate: 3.341265983181074e-05
Step: 13940, train/epoch: 3.3174679279327393
Step: 13950, train/loss: 0.0
Step: 13950, train/grad_norm: 3.7489050752625985e-10
Step: 13950, train/learning_rate: 3.3400760003132746e-05
Step: 13950, train/epoch: 3.319847583770752
Step: 13960, train/loss: 0.0
Step: 13960, train/grad_norm: 6.724867884777552e-10
Step: 13960, train/learning_rate: 3.3388863812433556e-05
Step: 13960, train/epoch: 3.3222274780273438
Step: 13970, train/loss: 0.0
Step: 13970, train/grad_norm: 0.4004983603954315
Step: 13970, train/learning_rate: 3.337696398375556e-05
Step: 13970, train/epoch: 3.3246073722839355
Step: 13980, train/loss: 0.0
Step: 13980, train/grad_norm: 4.3350909667561055e-11
Step: 13980, train/learning_rate: 3.336506415507756e-05
Step: 13980, train/epoch: 3.3269872665405273
Step: 13990, train/loss: 0.0
Step: 13990, train/grad_norm: 0.01598461903631687
Step: 13990, train/learning_rate: 3.3353164326399565e-05
Step: 13990, train/epoch: 3.32936692237854
Step: 14000, train/loss: 9.999999747378752e-05
Step: 14000, train/grad_norm: 1.2815631889040446e-09
Step: 14000, train/learning_rate: 3.334126449772157e-05
Step: 14000, train/epoch: 3.331746816635132
Step: 14010, train/loss: 0.0
Step: 14010, train/grad_norm: 1.0045293069582684e-10
Step: 14010, train/learning_rate: 3.332936830702238e-05
Step: 14010, train/epoch: 3.3341267108917236
Step: 14020, train/loss: 0.06989999860525131
Step: 14020, train/grad_norm: 3.8439412763935366e-10
Step: 14020, train/learning_rate: 3.331746847834438e-05
Step: 14020, train/epoch: 3.3365063667297363
Step: 14030, train/loss: 0.0
Step: 14030, train/grad_norm: 3.5612217175184924e-08
Step: 14030, train/learning_rate: 3.3305568649666384e-05
Step: 14030, train/epoch: 3.338886260986328
Step: 14040, train/loss: 0.0
Step: 14040, train/grad_norm: 2.3048146502446798e-08
Step: 14040, train/learning_rate: 3.329366882098839e-05
Step: 14040, train/epoch: 3.34126615524292
Step: 14050, train/loss: 0.0
Step: 14050, train/grad_norm: 1.2368839286125421e-10
Step: 14050, train/learning_rate: 3.328176899231039e-05
Step: 14050, train/epoch: 3.3436458110809326
Step: 14060, train/loss: 0.0
Step: 14060, train/grad_norm: 2.703791608382744e-07
Step: 14060, train/learning_rate: 3.32698728016112e-05
Step: 14060, train/epoch: 3.3460257053375244
Step: 14070, train/loss: 0.0
Step: 14070, train/grad_norm: 1.045870057225784e-10
Step: 14070, train/learning_rate: 3.32579729729332e-05
Step: 14070, train/epoch: 3.348405599594116
Step: 14080, train/loss: 0.0
Step: 14080, train/grad_norm: 1.0407686934499338e-09
Step: 14080, train/learning_rate: 3.3246073144255206e-05
Step: 14080, train/epoch: 3.350785255432129
Step: 14090, train/loss: 0.0
Step: 14090, train/grad_norm: 5.834640770530086e-09
Step: 14090, train/learning_rate: 3.323417331557721e-05
Step: 14090, train/epoch: 3.3531651496887207
Step: 14100, train/loss: 0.0
Step: 14100, train/grad_norm: 3.332820952550719e-08
Step: 14100, train/learning_rate: 3.322227348689921e-05
Step: 14100, train/epoch: 3.3555450439453125
Step: 14110, train/loss: 0.0
Step: 14110, train/grad_norm: 1.2263174475890537e-08
Step: 14110, train/learning_rate: 3.321037729620002e-05
Step: 14110, train/epoch: 3.357924699783325
Step: 14120, train/loss: 0.0
Step: 14120, train/grad_norm: 2.2475335481608738e-10
Step: 14120, train/learning_rate: 3.3198477467522025e-05
Step: 14120, train/epoch: 3.360304594039917
Step: 14130, train/loss: 0.0
Step: 14130, train/grad_norm: 3.0917091109472494e-09
Step: 14130, train/learning_rate: 3.318657763884403e-05
Step: 14130, train/epoch: 3.362684488296509
Step: 14140, train/loss: 0.0
Step: 14140, train/grad_norm: 2.549046485000872e-08
Step: 14140, train/learning_rate: 3.317467781016603e-05
Step: 14140, train/epoch: 3.3650641441345215
Step: 14150, train/loss: 0.0
Step: 14150, train/grad_norm: 1.8298493159818463e-05
Step: 14150, train/learning_rate: 3.3162777981488034e-05
Step: 14150, train/epoch: 3.3674440383911133
Step: 14160, train/loss: 0.0
Step: 14160, train/grad_norm: 1.6914775358145562e-07
Step: 14160, train/learning_rate: 3.3150881790788844e-05
Step: 14160, train/epoch: 3.369823932647705
Step: 14170, train/loss: 0.0
Step: 14170, train/grad_norm: 2.8601784829440646e-10
Step: 14170, train/learning_rate: 3.313898196211085e-05
Step: 14170, train/epoch: 3.372203826904297
Step: 14180, train/loss: 0.0
Step: 14180, train/grad_norm: 3.2297453600449444e-08
Step: 14180, train/learning_rate: 3.312708213343285e-05
Step: 14180, train/epoch: 3.3745834827423096
Step: 14190, train/loss: 0.0
Step: 14190, train/grad_norm: 2.2185179204825545e-07
Step: 14190, train/learning_rate: 3.311518230475485e-05
Step: 14190, train/epoch: 3.3769633769989014
Step: 14200, train/loss: 0.0
Step: 14200, train/grad_norm: 2.1662243909759127e-07
Step: 14200, train/learning_rate: 3.3103282476076856e-05
Step: 14200, train/epoch: 3.379343271255493
Step: 14210, train/loss: 0.0
Step: 14210, train/grad_norm: 3.217798916210768e-09
Step: 14210, train/learning_rate: 3.3091386285377666e-05
Step: 14210, train/epoch: 3.381722927093506
Step: 14220, train/loss: 0.0
Step: 14220, train/grad_norm: 7.142515556779472e-08
Step: 14220, train/learning_rate: 3.307948645669967e-05
Step: 14220, train/epoch: 3.3841028213500977
Step: 14230, train/loss: 0.0
Step: 14230, train/grad_norm: 8.819512231639237e-09
Step: 14230, train/learning_rate: 3.306758662802167e-05
Step: 14230, train/epoch: 3.3864827156066895
Step: 14240, train/loss: 0.04360000044107437
Step: 14240, train/grad_norm: 4.375353819341399e-05
Step: 14240, train/learning_rate: 3.3055686799343675e-05
Step: 14240, train/epoch: 3.388862371444702
Step: 14250, train/loss: 0.0
Step: 14250, train/grad_norm: 2.260715703528149e-08
Step: 14250, train/learning_rate: 3.304378697066568e-05
Step: 14250, train/epoch: 3.391242265701294
Step: 14260, train/loss: 0.03280000016093254
Step: 14260, train/grad_norm: 2.1846678066594905e-07
Step: 14260, train/learning_rate: 3.303189077996649e-05
Step: 14260, train/epoch: 3.3936221599578857
Step: 14270, train/loss: 0.13050000369548798
Step: 14270, train/grad_norm: 1.130427129680811e-08
Step: 14270, train/learning_rate: 3.301999095128849e-05
Step: 14270, train/epoch: 3.3960018157958984
Step: 14280, train/loss: 0.0
Step: 14280, train/grad_norm: 9.772961817589021e-08
Step: 14280, train/learning_rate: 3.3008091122610494e-05
Step: 14280, train/epoch: 3.3983817100524902
Step: 14290, train/loss: 0.0
Step: 14290, train/grad_norm: 1.3675435006632597e-08
Step: 14290, train/learning_rate: 3.29961912939325e-05
Step: 14290, train/epoch: 3.400761604309082
Step: 14300, train/loss: 9.999999747378752e-05
Step: 14300, train/grad_norm: 0.23721656203269958
Step: 14300, train/learning_rate: 3.29842914652545e-05
Step: 14300, train/epoch: 3.4031412601470947
Step: 14310, train/loss: 0.0
Step: 14310, train/grad_norm: 3.4282436445209896e-08
Step: 14310, train/learning_rate: 3.297239527455531e-05
Step: 14310, train/epoch: 3.4055211544036865
Step: 14320, train/loss: 0.0
Step: 14320, train/grad_norm: 3.289890457836009e-07
Step: 14320, train/learning_rate: 3.2960495445877314e-05
Step: 14320, train/epoch: 3.4079010486602783
Step: 14330, train/loss: 0.0
Step: 14330, train/grad_norm: 4.558710497803986e-05
Step: 14330, train/learning_rate: 3.294859561719932e-05
Step: 14330, train/epoch: 3.410280704498291
Step: 14340, train/loss: 0.0
Step: 14340, train/grad_norm: 3.1347895856015384e-05
Step: 14340, train/learning_rate: 3.293669578852132e-05
Step: 14340, train/epoch: 3.412660598754883
Step: 14350, train/loss: 0.0
Step: 14350, train/grad_norm: 6.3247314052716774e-09
Step: 14350, train/learning_rate: 3.292479595984332e-05
Step: 14350, train/epoch: 3.4150404930114746
Step: 14360, train/loss: 0.0
Step: 14360, train/grad_norm: 8.359393746104615e-07
Step: 14360, train/learning_rate: 3.291289976914413e-05
Step: 14360, train/epoch: 3.4174203872680664
Step: 14370, train/loss: 0.0
Step: 14370, train/grad_norm: 0.0010943259112536907
Step: 14370, train/learning_rate: 3.2900999940466136e-05
Step: 14370, train/epoch: 3.419800043106079
Step: 14380, train/loss: 9.999999747378752e-05
Step: 14380, train/grad_norm: 1.8159884348278865e-05
Step: 14380, train/learning_rate: 3.288910011178814e-05
Step: 14380, train/epoch: 3.422179937362671
Step: 14390, train/loss: 0.0
Step: 14390, train/grad_norm: 3.3041960278978877e-09
Step: 14390, train/learning_rate: 3.287720028311014e-05
Step: 14390, train/epoch: 3.4245598316192627
Step: 14400, train/loss: 0.2093999981880188
Step: 14400, train/grad_norm: 4.827983843824768e-07
Step: 14400, train/learning_rate: 3.2865300454432145e-05
Step: 14400, train/epoch: 3.4269394874572754
Step: 14410, train/loss: 0.0
Step: 14410, train/grad_norm: 0.001161691383458674
Step: 14410, train/learning_rate: 3.2853404263732955e-05
Step: 14410, train/epoch: 3.429319381713867
Step: 14420, train/loss: 0.0
Step: 14420, train/grad_norm: 7.181968726399646e-07
Step: 14420, train/learning_rate: 3.284150443505496e-05
Step: 14420, train/epoch: 3.431699275970459
Step: 14430, train/loss: 0.0
Step: 14430, train/grad_norm: 8.262346091214567e-05
Step: 14430, train/learning_rate: 3.282960460637696e-05
Step: 14430, train/epoch: 3.4340789318084717
Step: 14440, train/loss: 0.0
Step: 14440, train/grad_norm: 1.3175221624806e-07
Step: 14440, train/learning_rate: 3.2817704777698964e-05
Step: 14440, train/epoch: 3.4364588260650635
Step: 14450, train/loss: 0.0
Step: 14450, train/grad_norm: 4.1257742850575596e-05
Step: 14450, train/learning_rate: 3.280580494902097e-05
Step: 14450, train/epoch: 3.4388387203216553
Step: 14460, train/loss: 0.20430000126361847
Step: 14460, train/grad_norm: 76.013916015625
Step: 14460, train/learning_rate: 3.279390875832178e-05
Step: 14460, train/epoch: 3.441218376159668
Step: 14470, train/loss: 0.0
Step: 14470, train/grad_norm: 0.0007821295876055956
Step: 14470, train/learning_rate: 3.278200892964378e-05
Step: 14470, train/epoch: 3.4435982704162598
Step: 14480, train/loss: 9.999999747378752e-05
Step: 14480, train/grad_norm: 0.0001649610057938844
Step: 14480, train/learning_rate: 3.277010910096578e-05
Step: 14480, train/epoch: 3.4459781646728516
Step: 14490, train/loss: 9.999999747378752e-05
Step: 14490, train/grad_norm: 0.011523770168423653
Step: 14490, train/learning_rate: 3.2758209272287786e-05
Step: 14490, train/epoch: 3.4483578205108643
Step: 14500, train/loss: 0.0
Step: 14500, train/grad_norm: 1.5764037016197108e-05
Step: 14500, train/learning_rate: 3.2746313081588596e-05
Step: 14500, train/epoch: 3.450737714767456
Step: 14510, train/loss: 0.0
Step: 14510, train/grad_norm: 3.95668905639468e-07
Step: 14510, train/learning_rate: 3.27344132529106e-05
Step: 14510, train/epoch: 3.453117609024048
Step: 14520, train/loss: 0.0
Step: 14520, train/grad_norm: 0.00014983382425270975
Step: 14520, train/learning_rate: 3.27225134242326e-05
Step: 14520, train/epoch: 3.4554972648620605
Step: 14530, train/loss: 0.0
Step: 14530, train/grad_norm: 2.185812991228886e-05
Step: 14530, train/learning_rate: 3.2710613595554605e-05
Step: 14530, train/epoch: 3.4578771591186523
Step: 14540, train/loss: 0.0
Step: 14540, train/grad_norm: 7.747687646997292e-08
Step: 14540, train/learning_rate: 3.269871376687661e-05
Step: 14540, train/epoch: 3.460257053375244
Step: 14550, train/loss: 0.0
Step: 14550, train/grad_norm: 1.3294079508341383e-06
Step: 14550, train/learning_rate: 3.268681757617742e-05
Step: 14550, train/epoch: 3.462636947631836
Step: 14560, train/loss: 0.0
Step: 14560, train/grad_norm: 6.221738857448145e-08
Step: 14560, train/learning_rate: 3.267491774749942e-05
Step: 14560, train/epoch: 3.4650166034698486
Step: 14570, train/loss: 0.0006000000284984708
Step: 14570, train/grad_norm: 5.231556497165002e-05
Step: 14570, train/learning_rate: 3.2663017918821424e-05
Step: 14570, train/epoch: 3.4673964977264404
Step: 14580, train/loss: 0.0017000000225380063
Step: 14580, train/grad_norm: 3.846379968308611e-06
Step: 14580, train/learning_rate: 3.265111809014343e-05
Step: 14580, train/epoch: 3.4697763919830322
Step: 14590, train/loss: 0.0
Step: 14590, train/grad_norm: 1.1588785127969459e-05
Step: 14590, train/learning_rate: 3.263921826146543e-05
Step: 14590, train/epoch: 3.472156047821045
Step: 14600, train/loss: 0.02889999933540821
Step: 14600, train/grad_norm: 1.4868190191918984e-05
Step: 14600, train/learning_rate: 3.262732207076624e-05
Step: 14600, train/epoch: 3.4745359420776367
Step: 14610, train/loss: 0.0
Step: 14610, train/grad_norm: 0.0005106761236675084
Step: 14610, train/learning_rate: 3.261542224208824e-05
Step: 14610, train/epoch: 3.4769158363342285
Step: 14620, train/loss: 0.0
Step: 14620, train/grad_norm: 4.694274321082048e-05
Step: 14620, train/learning_rate: 3.2603522413410246e-05
Step: 14620, train/epoch: 3.479295492172241
Step: 14630, train/loss: 0.07159999758005142
Step: 14630, train/grad_norm: 1.7699720444852574e-07
Step: 14630, train/learning_rate: 3.259162258473225e-05
Step: 14630, train/epoch: 3.481675386428833
Step: 14640, train/loss: 0.0007999999797903001
Step: 14640, train/grad_norm: 3.93606370607813e-07
Step: 14640, train/learning_rate: 3.257972275605425e-05
Step: 14640, train/epoch: 3.484055280685425
Step: 14650, train/loss: 0.0
Step: 14650, train/grad_norm: 1.3141469423771923e-07
Step: 14650, train/learning_rate: 3.256782656535506e-05
Step: 14650, train/epoch: 3.4864349365234375
Step: 14660, train/loss: 0.0
Step: 14660, train/grad_norm: 4.2200012928539365e-10
Step: 14660, train/learning_rate: 3.2555926736677065e-05
Step: 14660, train/epoch: 3.4888148307800293
Step: 14670, train/loss: 0.0
Step: 14670, train/grad_norm: 4.6859963731549215e-06
Step: 14670, train/learning_rate: 3.254402690799907e-05
Step: 14670, train/epoch: 3.491194725036621
Step: 14680, train/loss: 0.0
Step: 14680, train/grad_norm: 2.9506334612960927e-05
Step: 14680, train/learning_rate: 3.253212707932107e-05
Step: 14680, train/epoch: 3.493574380874634
Step: 14690, train/loss: 0.0
Step: 14690, train/grad_norm: 0.0004980241064913571
Step: 14690, train/learning_rate: 3.2520227250643075e-05
Step: 14690, train/epoch: 3.4959542751312256
Step: 14700, train/loss: 0.016300000250339508
Step: 14700, train/grad_norm: 0.027590934187173843
Step: 14700, train/learning_rate: 3.2508331059943885e-05
Step: 14700, train/epoch: 3.4983341693878174
Step: 14710, train/loss: 0.0
Step: 14710, train/grad_norm: 7.244001949402445e-07
Step: 14710, train/learning_rate: 3.249643123126589e-05
Step: 14710, train/epoch: 3.500714063644409
Step: 14720, train/loss: 0.0
Step: 14720, train/grad_norm: 3.991744961240329e-07
Step: 14720, train/learning_rate: 3.248453140258789e-05
Step: 14720, train/epoch: 3.503093719482422
Step: 14730, train/loss: 0.0
Step: 14730, train/grad_norm: 0.00011323758371872827
Step: 14730, train/learning_rate: 3.2472631573909894e-05
Step: 14730, train/epoch: 3.5054736137390137
Step: 14740, train/loss: 0.0
Step: 14740, train/grad_norm: 1.1179376087966375e-05
Step: 14740, train/learning_rate: 3.24607317452319e-05
Step: 14740, train/epoch: 3.5078535079956055
Step: 14750, train/loss: 0.05860000103712082
Step: 14750, train/grad_norm: 4.37610333392513e-06
Step: 14750, train/learning_rate: 3.244883555453271e-05
Step: 14750, train/epoch: 3.510233163833618
Step: 14760, train/loss: 0.0
Step: 14760, train/grad_norm: 1.0319863008589891e-07
Step: 14760, train/learning_rate: 3.243693572585471e-05
Step: 14760, train/epoch: 3.51261305809021
Step: 14770, train/loss: 0.0
Step: 14770, train/grad_norm: 0.0005011037574149668
Step: 14770, train/learning_rate: 3.242503589717671e-05
Step: 14770, train/epoch: 3.5149929523468018
Step: 14780, train/loss: 0.0
Step: 14780, train/grad_norm: 0.00013209215831011534
Step: 14780, train/learning_rate: 3.2413136068498716e-05
Step: 14780, train/epoch: 3.5173726081848145
Step: 14790, train/loss: 0.0
Step: 14790, train/grad_norm: 4.398266639782378e-08
Step: 14790, train/learning_rate: 3.240123623982072e-05
Step: 14790, train/epoch: 3.5197525024414062
Step: 14800, train/loss: 0.026000000536441803
Step: 14800, train/grad_norm: 6.660228973487392e-05
Step: 14800, train/learning_rate: 3.238934004912153e-05
Step: 14800, train/epoch: 3.522132396697998
Step: 14810, train/loss: 0.0
Step: 14810, train/grad_norm: 0.00019770411017816514
Step: 14810, train/learning_rate: 3.237744022044353e-05
Step: 14810, train/epoch: 3.5245120525360107
Step: 14820, train/loss: 0.0
Step: 14820, train/grad_norm: 0.00014296305016614497
Step: 14820, train/learning_rate: 3.2365540391765535e-05
Step: 14820, train/epoch: 3.5268919467926025
Step: 14830, train/loss: 0.0
Step: 14830, train/grad_norm: 0.0005082783754914999
Step: 14830, train/learning_rate: 3.235364056308754e-05
Step: 14830, train/epoch: 3.5292718410491943
Step: 14840, train/loss: 0.0
Step: 14840, train/grad_norm: 0.0002425888815196231
Step: 14840, train/learning_rate: 3.234174073440954e-05
Step: 14840, train/epoch: 3.531651496887207
Step: 14850, train/loss: 0.0
Step: 14850, train/grad_norm: 0.08222421258687973
Step: 14850, train/learning_rate: 3.232984454371035e-05
Step: 14850, train/epoch: 3.534031391143799
Step: 14860, train/loss: 0.0
Step: 14860, train/grad_norm: 2.8417014164006105e-06
Step: 14860, train/learning_rate: 3.2317944715032354e-05
Step: 14860, train/epoch: 3.5364112854003906
Step: 14870, train/loss: 0.0
Step: 14870, train/grad_norm: 0.00013205829600337893
Step: 14870, train/learning_rate: 3.230604488635436e-05
Step: 14870, train/epoch: 3.5387909412384033
Step: 14880, train/loss: 0.0
Step: 14880, train/grad_norm: 2.905805160935415e-07
Step: 14880, train/learning_rate: 3.229414505767636e-05
Step: 14880, train/epoch: 3.541170835494995
Step: 14890, train/loss: 0.0
Step: 14890, train/grad_norm: 1.6776840539023397e-07
Step: 14890, train/learning_rate: 3.228224522899836e-05
Step: 14890, train/epoch: 3.543550729751587
Step: 14900, train/loss: 0.0
Step: 14900, train/grad_norm: 7.322930040132292e-10
Step: 14900, train/learning_rate: 3.227034903829917e-05
Step: 14900, train/epoch: 3.5459306240081787
Step: 14910, train/loss: 0.0
Step: 14910, train/grad_norm: 2.7846635930472985e-07
Step: 14910, train/learning_rate: 3.2258449209621176e-05
Step: 14910, train/epoch: 3.5483102798461914
Step: 14920, train/loss: 0.0
Step: 14920, train/grad_norm: 2.9931172321084887e-05
Step: 14920, train/learning_rate: 3.224654938094318e-05
Step: 14920, train/epoch: 3.550690174102783
Step: 14930, train/loss: 0.0
Step: 14930, train/grad_norm: 1.334361243721105e-08
Step: 14930, train/learning_rate: 3.223464955226518e-05
Step: 14930, train/epoch: 3.553070068359375
Step: 14940, train/loss: 0.0
Step: 14940, train/grad_norm: 2.5721086785779335e-05
Step: 14940, train/learning_rate: 3.2222749723587185e-05
Step: 14940, train/epoch: 3.5554497241973877
Step: 14950, train/loss: 0.0
Step: 14950, train/grad_norm: 0.053973812609910965
Step: 14950, train/learning_rate: 3.2210853532887995e-05
Step: 14950, train/epoch: 3.5578296184539795
Step: 14960, train/loss: 0.0
Step: 14960, train/grad_norm: 5.006136660767879e-08
Step: 14960, train/learning_rate: 3.219895370421e-05
Step: 14960, train/epoch: 3.5602095127105713
Step: 14970, train/loss: 0.0
Step: 14970, train/grad_norm: 5.366748519008979e-05
Step: 14970, train/learning_rate: 3.2187053875532e-05
Step: 14970, train/epoch: 3.562589168548584
Step: 14980, train/loss: 0.0
Step: 14980, train/grad_norm: 3.366918299718691e-09
Step: 14980, train/learning_rate: 3.2175154046854004e-05
Step: 14980, train/epoch: 3.564969062805176
Step: 14990, train/loss: 0.0
Step: 14990, train/grad_norm: 2.9043203397804973e-09
Step: 14990, train/learning_rate: 3.216325421817601e-05
Step: 14990, train/epoch: 3.5673489570617676
Step: 15000, train/loss: 0.0
Step: 15000, train/grad_norm: 4.252695084971947e-09
Step: 15000, train/learning_rate: 3.215135802747682e-05
Step: 15000, train/epoch: 3.5697286128997803
Step: 15010, train/loss: 0.0
Step: 15010, train/grad_norm: 3.85357923349261e-09
Step: 15010, train/learning_rate: 3.213945819879882e-05
Step: 15010, train/epoch: 3.572108507156372
Step: 15020, train/loss: 0.0
Step: 15020, train/grad_norm: 1.6029174787490774e-07
Step: 15020, train/learning_rate: 3.212755837012082e-05
Step: 15020, train/epoch: 3.574488401412964
Step: 15030, train/loss: 0.0
Step: 15030, train/grad_norm: 1.661930504326392e-08
Step: 15030, train/learning_rate: 3.2115658541442826e-05
Step: 15030, train/epoch: 3.5768680572509766
Step: 15040, train/loss: 0.0
Step: 15040, train/grad_norm: 3.3241853714116587e-09
Step: 15040, train/learning_rate: 3.210375871276483e-05
Step: 15040, train/epoch: 3.5792479515075684
Step: 15050, train/loss: 0.0
Step: 15050, train/grad_norm: 6.814744768490755e-09
Step: 15050, train/learning_rate: 3.209186252206564e-05
Step: 15050, train/epoch: 3.58162784576416
Step: 15060, train/loss: 0.0
Step: 15060, train/grad_norm: 6.543392828461947e-06
Step: 15060, train/learning_rate: 3.207996269338764e-05
Step: 15060, train/epoch: 3.584007501602173
Step: 15070, train/loss: 0.0
Step: 15070, train/grad_norm: 1.711109254287635e-09
Step: 15070, train/learning_rate: 3.2068062864709646e-05
Step: 15070, train/epoch: 3.5863873958587646
Step: 15080, train/loss: 0.0
Step: 15080, train/grad_norm: 3.565998052401653e-10
Step: 15080, train/learning_rate: 3.205616303603165e-05
Step: 15080, train/epoch: 3.5887672901153564
Step: 15090, train/loss: 0.0
Step: 15090, train/grad_norm: 1.8234999288324616e-07
Step: 15090, train/learning_rate: 3.204426320735365e-05
Step: 15090, train/epoch: 3.5911471843719482
Step: 15100, train/loss: 9.999999747378752e-05
Step: 15100, train/grad_norm: 9.890941399248732e-09
Step: 15100, train/learning_rate: 3.203236701665446e-05
Step: 15100, train/epoch: 3.593526840209961
Step: 15110, train/loss: 0.03240000084042549
Step: 15110, train/grad_norm: 6.351756620670557e-11
Step: 15110, train/learning_rate: 3.2020467187976465e-05
Step: 15110, train/epoch: 3.5959067344665527
Step: 15120, train/loss: 0.0
Step: 15120, train/grad_norm: 3.730068254270691e-09
Step: 15120, train/learning_rate: 3.200856735929847e-05
Step: 15120, train/epoch: 3.5982866287231445
Step: 15130, train/loss: 0.0
Step: 15130, train/grad_norm: 1.6973054905999874e-10
Step: 15130, train/learning_rate: 3.199666753062047e-05
Step: 15130, train/epoch: 3.6006662845611572
Step: 15140, train/loss: 0.0
Step: 15140, train/grad_norm: 3.487870401386317e-07
Step: 15140, train/learning_rate: 3.1984767701942474e-05
Step: 15140, train/epoch: 3.603046178817749
Step: 15150, train/loss: 0.0
Step: 15150, train/grad_norm: 8.831039899348525e-09
Step: 15150, train/learning_rate: 3.1972871511243284e-05
Step: 15150, train/epoch: 3.605426073074341
Step: 15160, train/loss: 0.0
Step: 15160, train/grad_norm: 2.8434063437110524e-10
Step: 15160, train/learning_rate: 3.196097168256529e-05
Step: 15160, train/epoch: 3.6078057289123535
Step: 15170, train/loss: 0.0
Step: 15170, train/grad_norm: 6.120241202722809e-09
Step: 15170, train/learning_rate: 3.194907185388729e-05
Step: 15170, train/epoch: 3.6101856231689453
Step: 15180, train/loss: 0.0
Step: 15180, train/grad_norm: 3.9296858744819474e-07
Step: 15180, train/learning_rate: 3.193717202520929e-05
Step: 15180, train/epoch: 3.612565517425537
Step: 15190, train/loss: 0.0
Step: 15190, train/grad_norm: 1.8735878126729943e-10
Step: 15190, train/learning_rate: 3.1925272196531296e-05
Step: 15190, train/epoch: 3.61494517326355
Step: 15200, train/loss: 0.0
Step: 15200, train/grad_norm: 8.365096704210373e-08
Step: 15200, train/learning_rate: 3.1913376005832106e-05
Step: 15200, train/epoch: 3.6173250675201416
Step: 15210, train/loss: 0.0
Step: 15210, train/grad_norm: 6.9706738159425186e-09
Step: 15210, train/learning_rate: 3.190147617715411e-05
Step: 15210, train/epoch: 3.6197049617767334
Step: 15220, train/loss: 0.0
Step: 15220, train/grad_norm: 4.520623458903472e-11
Step: 15220, train/learning_rate: 3.188957634847611e-05
Step: 15220, train/epoch: 3.622084617614746
Step: 15230, train/loss: 0.0
Step: 15230, train/grad_norm: 1.648090108830047e-09
Step: 15230, train/learning_rate: 3.1877676519798115e-05
Step: 15230, train/epoch: 3.624464511871338
Step: 15240, train/loss: 0.0
Step: 15240, train/grad_norm: 1.528707294085052e-08
Step: 15240, train/learning_rate: 3.186577669112012e-05
Step: 15240, train/epoch: 3.6268444061279297
Step: 15250, train/loss: 0.0
Step: 15250, train/grad_norm: 3.647652846439087e-08
Step: 15250, train/learning_rate: 3.185388050042093e-05
Step: 15250, train/epoch: 3.6292240619659424
Step: 15260, train/loss: 0.0
Step: 15260, train/grad_norm: 3.091595246473844e-08
Step: 15260, train/learning_rate: 3.184198067174293e-05
Step: 15260, train/epoch: 3.631603956222534
Step: 15270, train/loss: 0.0
Step: 15270, train/grad_norm: 2.623951544933334e-08
Step: 15270, train/learning_rate: 3.1830080843064934e-05
Step: 15270, train/epoch: 3.633983850479126
Step: 15280, train/loss: 0.0
Step: 15280, train/grad_norm: 1.0705783815012637e-08
Step: 15280, train/learning_rate: 3.181818101438694e-05
Step: 15280, train/epoch: 3.6363637447357178
Step: 15290, train/loss: 0.0
Step: 15290, train/grad_norm: 9.909918718964406e-11
Step: 15290, train/learning_rate: 3.180628118570894e-05
Step: 15290, train/epoch: 3.6387434005737305
Step: 15300, train/loss: 0.0
Step: 15300, train/grad_norm: 3.145921567693222e-08
Step: 15300, train/learning_rate: 3.179438499500975e-05
Step: 15300, train/epoch: 3.6411232948303223
Step: 15310, train/loss: 0.0
Step: 15310, train/grad_norm: 3.984810537094319e-10
Step: 15310, train/learning_rate: 3.178248516633175e-05
Step: 15310, train/epoch: 3.643503189086914
Step: 15320, train/loss: 0.0
Step: 15320, train/grad_norm: 5.730205199228067e-09
Step: 15320, train/learning_rate: 3.1770585337653756e-05
Step: 15320, train/epoch: 3.6458828449249268
Step: 15330, train/loss: 0.0
Step: 15330, train/grad_norm: 1.1481749995212454e-09
Step: 15330, train/learning_rate: 3.175868550897576e-05
Step: 15330, train/epoch: 3.6482627391815186
Step: 15340, train/loss: 0.0
Step: 15340, train/grad_norm: 8.437064935584715e-10
Step: 15340, train/learning_rate: 3.174678568029776e-05
Step: 15340, train/epoch: 3.6506426334381104
Step: 15350, train/loss: 0.0
Step: 15350, train/grad_norm: 4.318003288261707e-09
Step: 15350, train/learning_rate: 3.173488948959857e-05
Step: 15350, train/epoch: 3.653022289276123
Step: 15360, train/loss: 0.0
Step: 15360, train/grad_norm: 2.765503381851886e-10
Step: 15360, train/learning_rate: 3.1722989660920575e-05
Step: 15360, train/epoch: 3.655402183532715
Step: 15370, train/loss: 0.0
Step: 15370, train/grad_norm: 3.8432837468072023e-10
Step: 15370, train/learning_rate: 3.171108983224258e-05
Step: 15370, train/epoch: 3.6577820777893066
Step: 15380, train/loss: 0.0
Step: 15380, train/grad_norm: 2.6049087331614373e-09
Step: 15380, train/learning_rate: 3.169919000356458e-05
Step: 15380, train/epoch: 3.6601617336273193
Step: 15390, train/loss: 0.0
Step: 15390, train/grad_norm: 1.8327259154737874e-10
Step: 15390, train/learning_rate: 3.1687290174886584e-05
Step: 15390, train/epoch: 3.662541627883911
Step: 15400, train/loss: 0.0
Step: 15400, train/grad_norm: 1.459982690299455e-09
Step: 15400, train/learning_rate: 3.1675393984187394e-05
Step: 15400, train/epoch: 3.664921522140503
Step: 15410, train/loss: 0.0
Step: 15410, train/grad_norm: 2.0649749965517827e-10
Step: 15410, train/learning_rate: 3.16634941555094e-05
Step: 15410, train/epoch: 3.6673011779785156
Step: 15420, train/loss: 0.0
Step: 15420, train/grad_norm: 1.3193733883198888e-09
Step: 15420, train/learning_rate: 3.16515943268314e-05
Step: 15420, train/epoch: 3.6696810722351074
Step: 15430, train/loss: 0.0
Step: 15430, train/grad_norm: 1.4857872709939102e-09
Step: 15430, train/learning_rate: 3.1639694498153403e-05
Step: 15430, train/epoch: 3.672060966491699
Step: 15440, train/loss: 0.0
Step: 15440, train/grad_norm: 0.0001665791351115331
Step: 15440, train/learning_rate: 3.1627794669475406e-05
Step: 15440, train/epoch: 3.674440860748291
Step: 15450, train/loss: 0.0
Step: 15450, train/grad_norm: 7.998902096240101e-10
Step: 15450, train/learning_rate: 3.1615898478776217e-05
Step: 15450, train/epoch: 3.6768205165863037
Step: 15460, train/loss: 0.0
Step: 15460, train/grad_norm: 3.717862995245014e-08
Step: 15460, train/learning_rate: 3.160399865009822e-05
Step: 15460, train/epoch: 3.6792004108428955
Step: 15470, train/loss: 0.0
Step: 15470, train/grad_norm: 5.010669212879293e-08
Step: 15470, train/learning_rate: 3.159209882142022e-05
Step: 15470, train/epoch: 3.6815803050994873
Step: 15480, train/loss: 0.11949999630451202
Step: 15480, train/grad_norm: 8.673604497744236e-06
Step: 15480, train/learning_rate: 3.1580198992742226e-05
Step: 15480, train/epoch: 3.6839599609375
Step: 15490, train/loss: 0.0
Step: 15490, train/grad_norm: 0.0001694695238256827
Step: 15490, train/learning_rate: 3.156829916406423e-05
Step: 15490, train/epoch: 3.686339855194092
Step: 15500, train/loss: 0.0
Step: 15500, train/grad_norm: 8.118891114961002e-10
Step: 15500, train/learning_rate: 3.155640297336504e-05
Step: 15500, train/epoch: 3.6887197494506836
Step: 15510, train/loss: 0.07500000298023224
Step: 15510, train/grad_norm: 5.504461331184984e-09
Step: 15510, train/learning_rate: 3.154450314468704e-05
Step: 15510, train/epoch: 3.6910994052886963
Step: 15520, train/loss: 0.0
Step: 15520, train/grad_norm: 1.642886493513629e-09
Step: 15520, train/learning_rate: 3.1532603316009045e-05
Step: 15520, train/epoch: 3.693479299545288
Step: 15530, train/loss: 0.0
Step: 15530, train/grad_norm: 1.8108139432371217e-09
Step: 15530, train/learning_rate: 3.152070348733105e-05
Step: 15530, train/epoch: 3.69585919380188
Step: 15540, train/loss: 0.0
Step: 15540, train/grad_norm: 1.8942725432680163e-07
Step: 15540, train/learning_rate: 3.150880365865305e-05
Step: 15540, train/epoch: 3.6982388496398926
Step: 15550, train/loss: 0.0
Step: 15550, train/grad_norm: 1.2123695825039249e-08
Step: 15550, train/learning_rate: 3.149690746795386e-05
Step: 15550, train/epoch: 3.7006187438964844
Step: 15560, train/loss: 0.0
Step: 15560, train/grad_norm: 7.624210596191006e-09
Step: 15560, train/learning_rate: 3.1485007639275864e-05
Step: 15560, train/epoch: 3.702998638153076
Step: 15570, train/loss: 0.0
Step: 15570, train/grad_norm: 0.00010149522859137505
Step: 15570, train/learning_rate: 3.147310781059787e-05
Step: 15570, train/epoch: 3.705378293991089
Step: 15580, train/loss: 0.0
Step: 15580, train/grad_norm: 2.163012879918824e-09
Step: 15580, train/learning_rate: 3.146120798191987e-05
Step: 15580, train/epoch: 3.7077581882476807
Step: 15590, train/loss: 0.0
Step: 15590, train/grad_norm: 1.0474153766537597e-09
Step: 15590, train/learning_rate: 3.144930815324187e-05
Step: 15590, train/epoch: 3.7101380825042725
Step: 15600, train/loss: 0.2976999878883362
Step: 15600, train/grad_norm: 0.04082019627094269
Step: 15600, train/learning_rate: 3.143741196254268e-05
Step: 15600, train/epoch: 3.712517738342285
Step: 15610, train/loss: 0.26809999346733093
Step: 15610, train/grad_norm: 0.07434700429439545
Step: 15610, train/learning_rate: 3.1425512133864686e-05
Step: 15610, train/epoch: 3.714897632598877
Step: 15620, train/loss: 0.0
Step: 15620, train/grad_norm: 0.00037680339301005006
Step: 15620, train/learning_rate: 3.141361230518669e-05
Step: 15620, train/epoch: 3.7172775268554688
Step: 15630, train/loss: 0.0003000000142492354
Step: 15630, train/grad_norm: 4.759554940392263e-05
Step: 15630, train/learning_rate: 3.140171247650869e-05
Step: 15630, train/epoch: 3.7196574211120605
Step: 15640, train/loss: 0.0005000000237487257
Step: 15640, train/grad_norm: 10.639437675476074
Step: 15640, train/learning_rate: 3.1389812647830695e-05
Step: 15640, train/epoch: 3.7220370769500732
Step: 15650, train/loss: 0.0
Step: 15650, train/grad_norm: 5.465799404191785e-07
Step: 15650, train/learning_rate: 3.1377916457131505e-05
Step: 15650, train/epoch: 3.724416971206665
Step: 15660, train/loss: 0.0
Step: 15660, train/grad_norm: 6.570780364256734e-09
Step: 15660, train/learning_rate: 3.136601662845351e-05
Step: 15660, train/epoch: 3.726796865463257
Step: 15670, train/loss: 0.0
Step: 15670, train/grad_norm: 6.5299921025996355e-09
Step: 15670, train/learning_rate: 3.135411679977551e-05
Step: 15670, train/epoch: 3.7291765213012695
Step: 15680, train/loss: 0.0
Step: 15680, train/grad_norm: 1.7463160917330356e-09
Step: 15680, train/learning_rate: 3.1342216971097514e-05
Step: 15680, train/epoch: 3.7315564155578613
Step: 15690, train/loss: 0.0
Step: 15690, train/grad_norm: 5.081153831021368e-10
Step: 15690, train/learning_rate: 3.133031714241952e-05
Step: 15690, train/epoch: 3.733936309814453
Step: 15700, train/loss: 0.0
Step: 15700, train/grad_norm: 2.7202618824162528e-08
Step: 15700, train/learning_rate: 3.131842095172033e-05
Step: 15700, train/epoch: 3.736315965652466
Step: 15710, train/loss: 0.28119999170303345
Step: 15710, train/grad_norm: 4.179368261247873e-06
Step: 15710, train/learning_rate: 3.130652112304233e-05
Step: 15710, train/epoch: 3.7386958599090576
Step: 15720, train/loss: 0.0
Step: 15720, train/grad_norm: 1.076027092494769e-05
Step: 15720, train/learning_rate: 3.129462129436433e-05
Step: 15720, train/epoch: 3.7410757541656494
Step: 15730, train/loss: 0.0
Step: 15730, train/grad_norm: 0.0001866558159235865
Step: 15730, train/learning_rate: 3.1282721465686336e-05
Step: 15730, train/epoch: 3.743455410003662
Step: 15740, train/loss: 0.0
Step: 15740, train/grad_norm: 0.00012772981426678598
Step: 15740, train/learning_rate: 3.127082163700834e-05
Step: 15740, train/epoch: 3.745835304260254
Step: 15750, train/loss: 0.0
Step: 15750, train/grad_norm: 0.0009305953281000257
Step: 15750, train/learning_rate: 3.125892544630915e-05
Step: 15750, train/epoch: 3.7482151985168457
Step: 15760, train/loss: 0.0
Step: 15760, train/grad_norm: 1.632761154723994e-06
Step: 15760, train/learning_rate: 3.124702561763115e-05
Step: 15760, train/epoch: 3.7505948543548584
Step: 15770, train/loss: 0.0
Step: 15770, train/grad_norm: 7.494472356484039e-06
Step: 15770, train/learning_rate: 3.1235125788953155e-05
Step: 15770, train/epoch: 3.75297474861145
Step: 15780, train/loss: 0.0
Step: 15780, train/grad_norm: 0.0001539752702228725
Step: 15780, train/learning_rate: 3.122322596027516e-05
Step: 15780, train/epoch: 3.755354642868042
Step: 15790, train/loss: 0.003800000064074993
Step: 15790, train/grad_norm: 50.99338150024414
Step: 15790, train/learning_rate: 3.121132613159716e-05
Step: 15790, train/epoch: 3.7577342987060547
Step: 15800, train/loss: 0.0
Step: 15800, train/grad_norm: 5.528593101189472e-05
Step: 15800, train/learning_rate: 3.119942994089797e-05
Step: 15800, train/epoch: 3.7601141929626465
Step: 15810, train/loss: 0.09260000288486481
Step: 15810, train/grad_norm: 3.6499342968454584e-05
Step: 15810, train/learning_rate: 3.1187530112219974e-05
Step: 15810, train/epoch: 3.7624940872192383
Step: 15820, train/loss: 0.0
Step: 15820, train/grad_norm: 3.1804775062482804e-05
Step: 15820, train/learning_rate: 3.117563028354198e-05
Step: 15820, train/epoch: 3.76487398147583
Step: 15830, train/loss: 0.13989999890327454
Step: 15830, train/grad_norm: 0.016495943069458008
Step: 15830, train/learning_rate: 3.116373045486398e-05
Step: 15830, train/epoch: 3.7672536373138428
Step: 15840, train/loss: 0.0
Step: 15840, train/grad_norm: 1.1424403965065721e-06
Step: 15840, train/learning_rate: 3.115183426416479e-05
Step: 15840, train/epoch: 3.7696335315704346
Step: 15850, train/loss: 0.0
Step: 15850, train/grad_norm: 5.2687224524561316e-05
Step: 15850, train/learning_rate: 3.1139934435486794e-05
Step: 15850, train/epoch: 3.7720134258270264
Step: 15860, train/loss: 0.0
Step: 15860, train/grad_norm: 0.0001390360266668722
Step: 15860, train/learning_rate: 3.1128034606808797e-05
Step: 15860, train/epoch: 3.774393081665039
Step: 15870, train/loss: 0.08709999918937683
Step: 15870, train/grad_norm: 0.00039737290353514254
Step: 15870, train/learning_rate: 3.11161347781308e-05
Step: 15870, train/epoch: 3.776772975921631
Step: 15880, train/loss: 0.2304999977350235
Step: 15880, train/grad_norm: 8.499591058352962e-05
Step: 15880, train/learning_rate: 3.11042349494528e-05
Step: 15880, train/epoch: 3.7791528701782227
Step: 15890, train/loss: 9.999999747378752e-05
Step: 15890, train/grad_norm: 0.00021341499814298004
Step: 15890, train/learning_rate: 3.109233875875361e-05
Step: 15890, train/epoch: 3.7815325260162354
Step: 15900, train/loss: 0.0
Step: 15900, train/grad_norm: 4.516135049925651e-06
Step: 15900, train/learning_rate: 3.1080438930075616e-05
Step: 15900, train/epoch: 3.783912420272827
Step: 15910, train/loss: 0.1462000012397766
Step: 15910, train/grad_norm: 48.9300651550293
Step: 15910, train/learning_rate: 3.106853910139762e-05
Step: 15910, train/epoch: 3.786292314529419
Step: 15920, train/loss: 9.999999747378752e-05
Step: 15920, train/grad_norm: 0.34693241119384766
Step: 15920, train/learning_rate: 3.105663927271962e-05
Step: 15920, train/epoch: 3.7886719703674316
Step: 15930, train/loss: 0.0
Step: 15930, train/grad_norm: 1.621752198843751e-05
Step: 15930, train/learning_rate: 3.1044739444041625e-05
Step: 15930, train/epoch: 3.7910518646240234
Step: 15940, train/loss: 0.0
Step: 15940, train/grad_norm: 0.0009104677592404187
Step: 15940, train/learning_rate: 3.1032843253342435e-05
Step: 15940, train/epoch: 3.7934317588806152
Step: 15950, train/loss: 0.00039999998989515007
Step: 15950, train/grad_norm: 0.2367863953113556
Step: 15950, train/learning_rate: 3.102094342466444e-05
Step: 15950, train/epoch: 3.795811414718628
Step: 15960, train/loss: 0.0
Step: 15960, train/grad_norm: 3.253730392316356e-05
Step: 15960, train/learning_rate: 3.100904359598644e-05
Step: 15960, train/epoch: 3.7981913089752197
Step: 15970, train/loss: 0.05700000002980232
Step: 15970, train/grad_norm: 0.01521170511841774
Step: 15970, train/learning_rate: 3.0997143767308444e-05
Step: 15970, train/epoch: 3.8005712032318115
Step: 15980, train/loss: 0.0
Step: 15980, train/grad_norm: 5.686822532879887e-07
Step: 15980, train/learning_rate: 3.098524393863045e-05
Step: 15980, train/epoch: 3.802950859069824
Step: 15990, train/loss: 0.0
Step: 15990, train/grad_norm: 3.997772637376329e-06
Step: 15990, train/learning_rate: 3.097334774793126e-05
Step: 15990, train/epoch: 3.805330753326416
Step: 16000, train/loss: 0.0
Step: 16000, train/grad_norm: 0.0001584202254889533
Step: 16000, train/learning_rate: 3.096144791925326e-05
Step: 16000, train/epoch: 3.807710647583008
Step: 16010, train/loss: 0.0015999999595806003
Step: 16010, train/grad_norm: 4.3622131329357217e-07
Step: 16010, train/learning_rate: 3.094954809057526e-05
Step: 16010, train/epoch: 3.8100905418395996
Step: 16020, train/loss: 0.0
Step: 16020, train/grad_norm: 8.880889799911529e-05
Step: 16020, train/learning_rate: 3.0937648261897266e-05
Step: 16020, train/epoch: 3.8124701976776123
Step: 16030, train/loss: 0.0
Step: 16030, train/grad_norm: 0.0003455110709182918
Step: 16030, train/learning_rate: 3.092574843321927e-05
Step: 16030, train/epoch: 3.814850091934204
Step: 16040, train/loss: 0.0
Step: 16040, train/grad_norm: 4.813514031809518e-09
Step: 16040, train/learning_rate: 3.091385224252008e-05
Step: 16040, train/epoch: 3.817229986190796
Step: 16050, train/loss: 0.0
Step: 16050, train/grad_norm: 7.818017877525563e-08
Step: 16050, train/learning_rate: 3.090195241384208e-05
Step: 16050, train/epoch: 3.8196096420288086
Step: 16060, train/loss: 9.999999747378752e-05
Step: 16060, train/grad_norm: 2.9072174584143795e-05
Step: 16060, train/learning_rate: 3.0890052585164085e-05
Step: 16060, train/epoch: 3.8219895362854004
Step: 16070, train/loss: 0.0
Step: 16070, train/grad_norm: 2.5209450541296974e-05
Step: 16070, train/learning_rate: 3.087815275648609e-05
Step: 16070, train/epoch: 3.824369430541992
Step: 16080, train/loss: 0.0
Step: 16080, train/grad_norm: 0.004473675042390823
Step: 16080, train/learning_rate: 3.086625292780809e-05
Step: 16080, train/epoch: 3.826749086380005
Step: 16090, train/loss: 0.0
Step: 16090, train/grad_norm: 4.839184271077102e-07
Step: 16090, train/learning_rate: 3.08543567371089e-05
Step: 16090, train/epoch: 3.8291289806365967
Step: 16100, train/loss: 0.0
Step: 16100, train/grad_norm: 4.430436462143916e-08
Step: 16100, train/learning_rate: 3.0842456908430904e-05
Step: 16100, train/epoch: 3.8315088748931885
Step: 16110, train/loss: 0.0
Step: 16110, train/grad_norm: 4.621082627664919e-09
Step: 16110, train/learning_rate: 3.083055707975291e-05
Step: 16110, train/epoch: 3.833888530731201
Step: 16120, train/loss: 0.0
Step: 16120, train/grad_norm: 5.099600457469933e-07
Step: 16120, train/learning_rate: 3.081865725107491e-05
Step: 16120, train/epoch: 3.836268424987793
Step: 16130, train/loss: 0.0
Step: 16130, train/grad_norm: 9.149641300609801e-06
Step: 16130, train/learning_rate: 3.080675742239691e-05
Step: 16130, train/epoch: 3.8386483192443848
Step: 16140, train/loss: 0.0
Step: 16140, train/grad_norm: 0.00014258146984502673
Step: 16140, train/learning_rate: 3.079486123169772e-05
Step: 16140, train/epoch: 3.8410279750823975
Step: 16150, train/loss: 0.0
Step: 16150, train/grad_norm: 2.0757543950367108e-07
Step: 16150, train/learning_rate: 3.0782961403019726e-05
Step: 16150, train/epoch: 3.8434078693389893
Step: 16160, train/loss: 0.0
Step: 16160, train/grad_norm: 2.859168546365254e-07
Step: 16160, train/learning_rate: 3.077106157434173e-05
Step: 16160, train/epoch: 3.845787763595581
Step: 16170, train/loss: 0.0
Step: 16170, train/grad_norm: 1.910006042216139e-10
Step: 16170, train/learning_rate: 3.075916174566373e-05
Step: 16170, train/epoch: 3.848167657852173
Step: 16180, train/loss: 0.0
Step: 16180, train/grad_norm: 5.467214577947743e-06
Step: 16180, train/learning_rate: 3.0747261916985735e-05
Step: 16180, train/epoch: 3.8505473136901855
Step: 16190, train/loss: 0.0
Step: 16190, train/grad_norm: 2.2577928859845997e-08
Step: 16190, train/learning_rate: 3.0735365726286545e-05
Step: 16190, train/epoch: 3.8529272079467773
Step: 16200, train/loss: 0.0
Step: 16200, train/grad_norm: 3.070818638661876e-05
Step: 16200, train/learning_rate: 3.072346589760855e-05
Step: 16200, train/epoch: 3.855307102203369
Step: 16210, train/loss: 0.0
Step: 16210, train/grad_norm: 2.712744208110962e-05
Step: 16210, train/learning_rate: 3.071156606893055e-05
Step: 16210, train/epoch: 3.857686758041382
Step: 16220, train/loss: 0.0
Step: 16220, train/grad_norm: 5.647218131343834e-06
Step: 16220, train/learning_rate: 3.0699666240252554e-05
Step: 16220, train/epoch: 3.8600666522979736
Step: 16230, train/loss: 0.0
Step: 16230, train/grad_norm: 9.236140741109011e-09
Step: 16230, train/learning_rate: 3.068776641157456e-05
Step: 16230, train/epoch: 3.8624465465545654
Step: 16240, train/loss: 9.999999747378752e-05
Step: 16240, train/grad_norm: 4.501761436462402
Step: 16240, train/learning_rate: 3.067587022087537e-05
Step: 16240, train/epoch: 3.864826202392578
Step: 16250, train/loss: 0.0
Step: 16250, train/grad_norm: 7.493128578062169e-06
Step: 16250, train/learning_rate: 3.066397039219737e-05
Step: 16250, train/epoch: 3.86720609664917
Step: 16260, train/loss: 9.999999747378752e-05
Step: 16260, train/grad_norm: 1.3707042612054465e-08
Step: 16260, train/learning_rate: 3.0652070563519374e-05
Step: 16260, train/epoch: 3.8695859909057617
Step: 16270, train/loss: 0.0
Step: 16270, train/grad_norm: 1.0483455525900354e-07
Step: 16270, train/learning_rate: 3.0640170734841377e-05
Step: 16270, train/epoch: 3.8719656467437744
Step: 16280, train/loss: 0.0
Step: 16280, train/grad_norm: 1.0188469623528817e-08
Step: 16280, train/learning_rate: 3.062827090616338e-05
Step: 16280, train/epoch: 3.874345541000366
Step: 16290, train/loss: 0.0
Step: 16290, train/grad_norm: 6.368269106493685e-11
Step: 16290, train/learning_rate: 3.061637471546419e-05
Step: 16290, train/epoch: 3.876725435256958
Step: 16300, train/loss: 0.0
Step: 16300, train/grad_norm: 6.35050567687756e-10
Step: 16300, train/learning_rate: 3.060447488678619e-05
Step: 16300, train/epoch: 3.8791050910949707
Step: 16310, train/loss: 0.0
Step: 16310, train/grad_norm: 1.6987470985441178e-08
Step: 16310, train/learning_rate: 3.0592575058108196e-05
Step: 16310, train/epoch: 3.8814849853515625
Step: 16320, train/loss: 0.0
Step: 16320, train/grad_norm: 5.095307074043376e-07
Step: 16320, train/learning_rate: 3.05806752294302e-05
Step: 16320, train/epoch: 3.8838648796081543
Step: 16330, train/loss: 0.0
Step: 16330, train/grad_norm: 2.786028963086551e-09
Step: 16330, train/learning_rate: 3.05687754007522e-05
Step: 16330, train/epoch: 3.886244535446167
Step: 16340, train/loss: 0.0
Step: 16340, train/grad_norm: 2.9636725074055903e-08
Step: 16340, train/learning_rate: 3.055687921005301e-05
Step: 16340, train/epoch: 3.888624429702759
Step: 16350, train/loss: 0.0
Step: 16350, train/grad_norm: 6.085592303861631e-06
Step: 16350, train/learning_rate: 3.0544979381375015e-05
Step: 16350, train/epoch: 3.8910043239593506
Step: 16360, train/loss: 0.006899999920278788
Step: 16360, train/grad_norm: 8.032967069304675e-10
Step: 16360, train/learning_rate: 3.053307955269702e-05
Step: 16360, train/epoch: 3.8933842182159424
Step: 16370, train/loss: 0.0
Step: 16370, train/grad_norm: 1.371659452686913e-09
Step: 16370, train/learning_rate: 3.052117972401902e-05
Step: 16370, train/epoch: 3.895763874053955
Step: 16380, train/loss: 0.0
Step: 16380, train/grad_norm: 5.538604241905887e-09
Step: 16380, train/learning_rate: 3.0509281714330427e-05
Step: 16380, train/epoch: 3.898143768310547
Step: 16390, train/loss: 0.0
Step: 16390, train/grad_norm: 1.3926881365478039e-05
Step: 16390, train/learning_rate: 3.049738188565243e-05
Step: 16390, train/epoch: 3.9005236625671387
Step: 16400, train/loss: 0.0
Step: 16400, train/grad_norm: 3.3487413020338863e-07
Step: 16400, train/learning_rate: 3.0485483875963837e-05
Step: 16400, train/epoch: 3.9029033184051514
Step: 16410, train/loss: 0.13359999656677246
Step: 16410, train/grad_norm: 5.459651788441988e-07
Step: 16410, train/learning_rate: 3.047358404728584e-05
Step: 16410, train/epoch: 3.905283212661743
Step: 16420, train/loss: 0.0
Step: 16420, train/grad_norm: 1.253078778518102e-07
Step: 16420, train/learning_rate: 3.0461684218607843e-05
Step: 16420, train/epoch: 3.907663106918335
Step: 16430, train/loss: 0.0
Step: 16430, train/grad_norm: 3.8126952404127223e-06
Step: 16430, train/learning_rate: 3.044978620891925e-05
Step: 16430, train/epoch: 3.9100427627563477
Step: 16440, train/loss: 0.0
Step: 16440, train/grad_norm: 0.0003861922596115619
Step: 16440, train/learning_rate: 3.0437886380241252e-05
Step: 16440, train/epoch: 3.9124226570129395
Step: 16450, train/loss: 9.999999747378752e-05
Step: 16450, train/grad_norm: 3.12994519191534e-09
Step: 16450, train/learning_rate: 3.042598837055266e-05
Step: 16450, train/epoch: 3.9148025512695312
Step: 16460, train/loss: 0.0
Step: 16460, train/grad_norm: 9.510937593049107e-10
Step: 16460, train/learning_rate: 3.0414088541874662e-05
Step: 16460, train/epoch: 3.917182207107544
Step: 16470, train/loss: 0.0
Step: 16470, train/grad_norm: 1.1549933454091388e-08
Step: 16470, train/learning_rate: 3.0402188713196665e-05
Step: 16470, train/epoch: 3.9195621013641357
Step: 16480, train/loss: 0.0
Step: 16480, train/grad_norm: 2.5190598496038774e-09
Step: 16480, train/learning_rate: 3.039029070350807e-05
Step: 16480, train/epoch: 3.9219419956207275
Step: 16490, train/loss: 0.0
Step: 16490, train/grad_norm: 1.1428261892376668e-08
Step: 16490, train/learning_rate: 3.0378390874830075e-05
Step: 16490, train/epoch: 3.9243216514587402
Step: 16500, train/loss: 0.0
Step: 16500, train/grad_norm: 1.4605711307069669e-08
Step: 16500, train/learning_rate: 3.036649286514148e-05
Step: 16500, train/epoch: 3.926701545715332
Step: 16510, train/loss: 0.0
Step: 16510, train/grad_norm: 1.1091647600380838e-09
Step: 16510, train/learning_rate: 3.0354593036463484e-05
Step: 16510, train/epoch: 3.929081439971924
Step: 16520, train/loss: 0.0
Step: 16520, train/grad_norm: 8.415428851549223e-07
Step: 16520, train/learning_rate: 3.0342693207785487e-05
Step: 16520, train/epoch: 3.9314610958099365
Step: 16530, train/loss: 0.0
Step: 16530, train/grad_norm: 1.43410687769574e-08
Step: 16530, train/learning_rate: 3.0330795198096894e-05
Step: 16530, train/epoch: 3.9338409900665283
Step: 16540, train/loss: 0.0
Step: 16540, train/grad_norm: 1.1034726554726149e-07
Step: 16540, train/learning_rate: 3.0318895369418897e-05
Step: 16540, train/epoch: 3.93622088432312
Step: 16550, train/loss: 0.0
Step: 16550, train/grad_norm: 8.563193903965427e-11
Step: 16550, train/learning_rate: 3.0306997359730303e-05
Step: 16550, train/epoch: 3.938600778579712
Step: 16560, train/loss: 0.0
Step: 16560, train/grad_norm: 7.50353201794951e-09
Step: 16560, train/learning_rate: 3.0295097531052306e-05
Step: 16560, train/epoch: 3.9409804344177246
Step: 16570, train/loss: 0.0
Step: 16570, train/grad_norm: 6.147548106127942e-07
Step: 16570, train/learning_rate: 3.028319770237431e-05
Step: 16570, train/epoch: 3.9433603286743164
Step: 16580, train/loss: 0.0
Step: 16580, train/grad_norm: 7.060210009512957e-06
Step: 16580, train/learning_rate: 3.0271299692685716e-05
Step: 16580, train/epoch: 3.945740222930908
Step: 16590, train/loss: 0.0
Step: 16590, train/grad_norm: 2.267806387123983e-08
Step: 16590, train/learning_rate: 3.025939986400772e-05
Step: 16590, train/epoch: 3.948119878768921
Step: 16600, train/loss: 0.0
Step: 16600, train/grad_norm: 0.00017749123799148947
Step: 16600, train/learning_rate: 3.0247501854319125e-05
Step: 16600, train/epoch: 3.9504997730255127
Step: 16610, train/loss: 0.0
Step: 16610, train/grad_norm: 8.3120843541451e-10
Step: 16610, train/learning_rate: 3.023560202564113e-05
Step: 16610, train/epoch: 3.9528796672821045
Step: 16620, train/loss: 0.0
Step: 16620, train/grad_norm: 1.6438083116909752e-09
Step: 16620, train/learning_rate: 3.022370219696313e-05
Step: 16620, train/epoch: 3.955259323120117
Step: 16630, train/loss: 0.0
Step: 16630, train/grad_norm: 2.7470605790114178e-09
Step: 16630, train/learning_rate: 3.0211804187274538e-05
Step: 16630, train/epoch: 3.957639217376709
Step: 16640, train/loss: 0.0
Step: 16640, train/grad_norm: 3.056475961216165e-08
Step: 16640, train/learning_rate: 3.019990435859654e-05
Step: 16640, train/epoch: 3.960019111633301
Step: 16650, train/loss: 0.0
Step: 16650, train/grad_norm: 1.3204710569425515e-07
Step: 16650, train/learning_rate: 3.0188006348907948e-05
Step: 16650, train/epoch: 3.9623987674713135
Step: 16660, train/loss: 0.0
Step: 16660, train/grad_norm: 1.1164718927147987e-08
Step: 16660, train/learning_rate: 3.017610652022995e-05
Step: 16660, train/epoch: 3.9647786617279053
Step: 16670, train/loss: 0.0
Step: 16670, train/grad_norm: 1.9661942474158423e-07
Step: 16670, train/learning_rate: 3.0164206691551954e-05
Step: 16670, train/epoch: 3.967158555984497
Step: 16680, train/loss: 0.0
Step: 16680, train/grad_norm: 4.6333687997446305e-09
Step: 16680, train/learning_rate: 3.015230868186336e-05
Step: 16680, train/epoch: 3.9695382118225098
Step: 16690, train/loss: 0.0
Step: 16690, train/grad_norm: 2.9507020826713415e-07
Step: 16690, train/learning_rate: 3.0140408853185363e-05
Step: 16690, train/epoch: 3.9719181060791016
Step: 16700, train/loss: 0.0
Step: 16700, train/grad_norm: 8.584593302884969e-08
Step: 16700, train/learning_rate: 3.012851084349677e-05
Step: 16700, train/epoch: 3.9742980003356934
Step: 16710, train/loss: 0.09220000356435776
Step: 16710, train/grad_norm: 2.3930937231853022e-08
Step: 16710, train/learning_rate: 3.0116611014818773e-05
Step: 16710, train/epoch: 3.976677656173706
Step: 16720, train/loss: 0.0
Step: 16720, train/grad_norm: 5.0537364586489275e-05
Step: 16720, train/learning_rate: 3.0104711186140776e-05
Step: 16720, train/epoch: 3.979057550430298
Step: 16730, train/loss: 0.0
Step: 16730, train/grad_norm: 5.3624851226175e-09
Step: 16730, train/learning_rate: 3.0092813176452182e-05
Step: 16730, train/epoch: 3.9814374446868896
Step: 16740, train/loss: 0.0
Step: 16740, train/grad_norm: 0.007742808200418949
Step: 16740, train/learning_rate: 3.0080913347774185e-05
Step: 16740, train/epoch: 3.9838173389434814
Step: 16750, train/loss: 0.0
Step: 16750, train/grad_norm: 5.337714981124009e-08
Step: 16750, train/learning_rate: 3.0069015338085592e-05
Step: 16750, train/epoch: 3.986196994781494
Step: 16760, train/loss: 0.0
Step: 16760, train/grad_norm: 7.824275627399402e-08
Step: 16760, train/learning_rate: 3.0057115509407595e-05
Step: 16760, train/epoch: 3.988576889038086
Step: 16770, train/loss: 0.0
Step: 16770, train/grad_norm: 0.001259682816453278
Step: 16770, train/learning_rate: 3.0045215680729598e-05
Step: 16770, train/epoch: 3.9909567832946777
Step: 16780, train/loss: 0.0
Step: 16780, train/grad_norm: 0.06459446996450424
Step: 16780, train/learning_rate: 3.0033317671041004e-05
Step: 16780, train/epoch: 3.9933364391326904
Step: 16790, train/loss: 0.0
Step: 16790, train/grad_norm: 1.3796748135064263e-06
Step: 16790, train/learning_rate: 3.0021417842363007e-05
Step: 16790, train/epoch: 3.9957163333892822
Step: 16800, train/loss: 0.0
Step: 16800, train/grad_norm: 2.9368987952693715e-07
Step: 16800, train/learning_rate: 3.0009519832674414e-05
Step: 16800, train/epoch: 3.998096227645874
Step: 16808, eval/loss: 0.07619981467723846
Step: 16808, eval/accuracy: 0.9933360815048218
Step: 16808, eval/f1: 0.9929405450820923
Step: 16808, eval/runtime: 735.6367797851562
Step: 16808, eval/samples_per_second: 9.791999816894531
Step: 16808, eval/steps_per_second: 1.225000023841858
Step: 16808, train/epoch: 4.0
Step: 16810, train/loss: 0.0
Step: 16810, train/grad_norm: 1.992251874582962e-08
Step: 16810, train/learning_rate: 2.9997620003996417e-05
Step: 16810, train/epoch: 4.000475883483887
Step: 16820, train/loss: 0.0
Step: 16820, train/grad_norm: 6.731291080086521e-09
Step: 16820, train/learning_rate: 2.9985721994307823e-05
Step: 16820, train/epoch: 4.0028557777404785
Step: 16830, train/loss: 0.0
Step: 16830, train/grad_norm: 0.00012861927098128945
Step: 16830, train/learning_rate: 2.9973822165629826e-05
Step: 16830, train/epoch: 4.00523567199707
Step: 16840, train/loss: 0.0
Step: 16840, train/grad_norm: 1.0343705980631057e-06
Step: 16840, train/learning_rate: 2.996192233695183e-05
Step: 16840, train/epoch: 4.007615566253662
Step: 16850, train/loss: 0.0
Step: 16850, train/grad_norm: 8.924195604720353e-09
Step: 16850, train/learning_rate: 2.9950024327263236e-05
Step: 16850, train/epoch: 4.009995460510254
Step: 16860, train/loss: 0.0
Step: 16860, train/grad_norm: 1.5443444567608822e-07
Step: 16860, train/learning_rate: 2.993812449858524e-05
Step: 16860, train/epoch: 4.0123748779296875
Step: 16870, train/loss: 0.0
Step: 16870, train/grad_norm: 1.7366716065225773e-07
Step: 16870, train/learning_rate: 2.9926226488896646e-05
Step: 16870, train/epoch: 4.014754772186279
Step: 16880, train/loss: 0.0
Step: 16880, train/grad_norm: 0.002097432967275381
Step: 16880, train/learning_rate: 2.991432666021865e-05
Step: 16880, train/epoch: 4.017134666442871
Step: 16890, train/loss: 0.0
Step: 16890, train/grad_norm: 6.546577946409116e-09
Step: 16890, train/learning_rate: 2.990242683154065e-05
Step: 16890, train/epoch: 4.019514560699463
Step: 16900, train/loss: 0.0
Step: 16900, train/grad_norm: 6.107226369067575e-08
Step: 16900, train/learning_rate: 2.9890528821852058e-05
Step: 16900, train/epoch: 4.021894454956055
Step: 16910, train/loss: 0.0
Step: 16910, train/grad_norm: 7.793283884893754e-07
Step: 16910, train/learning_rate: 2.987862899317406e-05
Step: 16910, train/epoch: 4.0242743492126465
Step: 16920, train/loss: 0.0
Step: 16920, train/grad_norm: 0.00011455400090198964
Step: 16920, train/learning_rate: 2.9866730983485468e-05
Step: 16920, train/epoch: 4.02665376663208
Step: 16930, train/loss: 0.0
Step: 16930, train/grad_norm: 2.435037060877221e-09
Step: 16930, train/learning_rate: 2.985483115480747e-05
Step: 16930, train/epoch: 4.029033660888672
Step: 16940, train/loss: 0.0
Step: 16940, train/grad_norm: 3.372196388795601e-08
Step: 16940, train/learning_rate: 2.9842931326129474e-05
Step: 16940, train/epoch: 4.031413555145264
Step: 16950, train/loss: 0.0
Step: 16950, train/grad_norm: 5.620086994895246e-06
Step: 16950, train/learning_rate: 2.983103331644088e-05
Step: 16950, train/epoch: 4.0337934494018555
Step: 16960, train/loss: 0.0
Step: 16960, train/grad_norm: 0.00027900878922082484
Step: 16960, train/learning_rate: 2.9819133487762883e-05
Step: 16960, train/epoch: 4.036173343658447
Step: 16970, train/loss: 0.0
Step: 16970, train/grad_norm: 4.480927850991634e-10
Step: 16970, train/learning_rate: 2.980723547807429e-05
Step: 16970, train/epoch: 4.038553237915039
Step: 16980, train/loss: 0.0
Step: 16980, train/grad_norm: 2.10087396368408e-08
Step: 16980, train/learning_rate: 2.9795335649396293e-05
Step: 16980, train/epoch: 4.040932655334473
Step: 16990, train/loss: 0.0
Step: 16990, train/grad_norm: 3.009769378170546e-10
Step: 16990, train/learning_rate: 2.9783435820718296e-05
Step: 16990, train/epoch: 4.0433125495910645
Step: 17000, train/loss: 0.0
Step: 17000, train/grad_norm: 1.2429137719038863e-08
Step: 17000, train/learning_rate: 2.9771537811029702e-05
Step: 17000, train/epoch: 4.045692443847656
Step: 17010, train/loss: 0.0
Step: 17010, train/grad_norm: 1.852518653322477e-05
Step: 17010, train/learning_rate: 2.9759637982351705e-05
Step: 17010, train/epoch: 4.048072338104248
Step: 17020, train/loss: 9.999999747378752e-05
Step: 17020, train/grad_norm: 2.0011354351368027e-08
Step: 17020, train/learning_rate: 2.9747739972663112e-05
Step: 17020, train/epoch: 4.05045223236084
Step: 17030, train/loss: 0.0
Step: 17030, train/grad_norm: 3.3121347886577723e-09
Step: 17030, train/learning_rate: 2.9735840143985115e-05
Step: 17030, train/epoch: 4.052832126617432
Step: 17040, train/loss: 0.0
Step: 17040, train/grad_norm: 9.095227682109908e-09
Step: 17040, train/learning_rate: 2.9723940315307118e-05
Step: 17040, train/epoch: 4.055212020874023
Step: 17050, train/loss: 0.0
Step: 17050, train/grad_norm: 2.446187918891951e-09
Step: 17050, train/learning_rate: 2.9712042305618525e-05
Step: 17050, train/epoch: 4.057591438293457
Step: 17060, train/loss: 0.3156000077724457
Step: 17060, train/grad_norm: 8.213768865061866e-08
Step: 17060, train/learning_rate: 2.9700142476940528e-05
Step: 17060, train/epoch: 4.059971332550049
Step: 17070, train/loss: 0.0
Step: 17070, train/grad_norm: 8.091014969124899e-09
Step: 17070, train/learning_rate: 2.9688244467251934e-05
Step: 17070, train/epoch: 4.062351226806641
Step: 17080, train/loss: 0.0
Step: 17080, train/grad_norm: 9.658486277430711e-08
Step: 17080, train/learning_rate: 2.9676344638573937e-05
Step: 17080, train/epoch: 4.064731121063232
Step: 17090, train/loss: 0.06210000067949295
Step: 17090, train/grad_norm: 9.163654723920445e-09
Step: 17090, train/learning_rate: 2.966444480989594e-05
Step: 17090, train/epoch: 4.067111015319824
Step: 17100, train/loss: 9.999999747378752e-05
Step: 17100, train/grad_norm: 5.163898094906472e-05
Step: 17100, train/learning_rate: 2.9652546800207347e-05
Step: 17100, train/epoch: 4.069490909576416
Step: 17110, train/loss: 0.0
Step: 17110, train/grad_norm: 1.821772421806145e-08
Step: 17110, train/learning_rate: 2.964064697152935e-05
Step: 17110, train/epoch: 4.07187032699585
Step: 17120, train/loss: 9.999999747378752e-05
Step: 17120, train/grad_norm: 8.626234659914189e-08
Step: 17120, train/learning_rate: 2.9628748961840756e-05
Step: 17120, train/epoch: 4.074250221252441
Step: 17130, train/loss: 0.0
Step: 17130, train/grad_norm: 0.002809796016663313
Step: 17130, train/learning_rate: 2.961684913316276e-05
Step: 17130, train/epoch: 4.076630115509033
Step: 17140, train/loss: 0.0
Step: 17140, train/grad_norm: 9.899015367409447e-07
Step: 17140, train/learning_rate: 2.9604949304484762e-05
Step: 17140, train/epoch: 4.079010009765625
Step: 17150, train/loss: 0.0
Step: 17150, train/grad_norm: 3.2647952252773393e-07
Step: 17150, train/learning_rate: 2.959305129479617e-05
Step: 17150, train/epoch: 4.081389904022217
Step: 17160, train/loss: 0.0
Step: 17160, train/grad_norm: 2.8808519346057437e-05
Step: 17160, train/learning_rate: 2.9581151466118172e-05
Step: 17160, train/epoch: 4.083769798278809
Step: 17170, train/loss: 0.0
Step: 17170, train/grad_norm: 1.1889079054583362e-08
Step: 17170, train/learning_rate: 2.956925345642958e-05
Step: 17170, train/epoch: 4.086149215698242
Step: 17180, train/loss: 0.0
Step: 17180, train/grad_norm: 4.632574018614832e-06
Step: 17180, train/learning_rate: 2.955735362775158e-05
Step: 17180, train/epoch: 4.088529109954834
Step: 17190, train/loss: 0.0
Step: 17190, train/grad_norm: 1.1419880152629958e-08
Step: 17190, train/learning_rate: 2.9545453799073584e-05
Step: 17190, train/epoch: 4.090909004211426
Step: 17200, train/loss: 0.0
Step: 17200, train/grad_norm: 5.648871592711657e-06
Step: 17200, train/learning_rate: 2.953355578938499e-05
Step: 17200, train/epoch: 4.093288898468018
Step: 17210, train/loss: 0.0006000000284984708
Step: 17210, train/grad_norm: 4.647851437056261e-09
Step: 17210, train/learning_rate: 2.9521655960706994e-05
Step: 17210, train/epoch: 4.095668792724609
Step: 17220, train/loss: 0.3125
Step: 17220, train/grad_norm: 1.1962012536059774e-07
Step: 17220, train/learning_rate: 2.95097579510184e-05
Step: 17220, train/epoch: 4.098048686981201
Step: 17230, train/loss: 0.0
Step: 17230, train/grad_norm: 0.0008152727968990803
Step: 17230, train/learning_rate: 2.9497858122340403e-05
Step: 17230, train/epoch: 4.100428581237793
Step: 17240, train/loss: 0.0
Step: 17240, train/grad_norm: 0.060682374984025955
Step: 17240, train/learning_rate: 2.9485958293662407e-05
Step: 17240, train/epoch: 4.102807998657227
Step: 17250, train/loss: 0.00019999999494757503
Step: 17250, train/grad_norm: 0.00022052587883081287
Step: 17250, train/learning_rate: 2.9474060283973813e-05
Step: 17250, train/epoch: 4.105187892913818
Step: 17260, train/loss: 0.0
Step: 17260, train/grad_norm: 2.948200972241466e-06
Step: 17260, train/learning_rate: 2.9462160455295816e-05
Step: 17260, train/epoch: 4.10756778717041
Step: 17270, train/loss: 0.04430000111460686
Step: 17270, train/grad_norm: 7.721124347881414e-06
Step: 17270, train/learning_rate: 2.9450262445607223e-05
Step: 17270, train/epoch: 4.109947681427002
Step: 17280, train/loss: 0.0
Step: 17280, train/grad_norm: 5.356780457077548e-05
Step: 17280, train/learning_rate: 2.9438362616929226e-05
Step: 17280, train/epoch: 4.112327575683594
Step: 17290, train/loss: 0.0
Step: 17290, train/grad_norm: 2.6240279112244025e-05
Step: 17290, train/learning_rate: 2.942646278825123e-05
Step: 17290, train/epoch: 4.1147074699401855
Step: 17300, train/loss: 0.0
Step: 17300, train/grad_norm: 0.0029585552401840687
Step: 17300, train/learning_rate: 2.9414564778562635e-05
Step: 17300, train/epoch: 4.117086887359619
Step: 17310, train/loss: 0.0
Step: 17310, train/grad_norm: 0.02552473545074463
Step: 17310, train/learning_rate: 2.9402664949884638e-05
Step: 17310, train/epoch: 4.119466781616211
Step: 17320, train/loss: 0.027899999171495438
Step: 17320, train/grad_norm: 4.3298109630995896e-06
Step: 17320, train/learning_rate: 2.9390766940196045e-05
Step: 17320, train/epoch: 4.121846675872803
Step: 17330, train/loss: 0.023000000044703484
Step: 17330, train/grad_norm: 1.1873313269461505e-05
Step: 17330, train/learning_rate: 2.9378867111518048e-05
Step: 17330, train/epoch: 4.1242265701293945
Step: 17340, train/loss: 0.0
Step: 17340, train/grad_norm: 2.240118192275986e-05
Step: 17340, train/learning_rate: 2.936696728284005e-05
Step: 17340, train/epoch: 4.126606464385986
Step: 17350, train/loss: 0.0008999999845400453
Step: 17350, train/grad_norm: 3.6573917441273807e-06
Step: 17350, train/learning_rate: 2.9355069273151457e-05
Step: 17350, train/epoch: 4.128986358642578
Step: 17360, train/loss: 0.0
Step: 17360, train/grad_norm: 0.0004089563153684139
Step: 17360, train/learning_rate: 2.934316944447346e-05
Step: 17360, train/epoch: 4.13136625289917
Step: 17370, train/loss: 0.0
Step: 17370, train/grad_norm: 2.6254181761942164e-07
Step: 17370, train/learning_rate: 2.9331271434784867e-05
Step: 17370, train/epoch: 4.1337456703186035
Step: 17380, train/loss: 0.0
Step: 17380, train/grad_norm: 0.0007377044530585408
Step: 17380, train/learning_rate: 2.931937160610687e-05
Step: 17380, train/epoch: 4.136125564575195
Step: 17390, train/loss: 0.0
Step: 17390, train/grad_norm: 0.0018903251038864255
Step: 17390, train/learning_rate: 2.9307471777428873e-05
Step: 17390, train/epoch: 4.138505458831787
Step: 17400, train/loss: 0.06019999831914902
Step: 17400, train/grad_norm: 1.3272519026941154e-05
Step: 17400, train/learning_rate: 2.929557376774028e-05
Step: 17400, train/epoch: 4.140885353088379
Step: 17410, train/loss: 0.0
Step: 17410, train/grad_norm: 2.875365680665709e-06
Step: 17410, train/learning_rate: 2.9283673939062282e-05
Step: 17410, train/epoch: 4.143265247344971
Step: 17420, train/loss: 0.0
Step: 17420, train/grad_norm: 4.761424861499108e-06
Step: 17420, train/learning_rate: 2.927177592937369e-05
Step: 17420, train/epoch: 4.1456451416015625
Step: 17430, train/loss: 0.0
Step: 17430, train/grad_norm: 1.3934334219811717e-07
Step: 17430, train/learning_rate: 2.9259876100695692e-05
Step: 17430, train/epoch: 4.148024559020996
Step: 17440, train/loss: 0.0
Step: 17440, train/grad_norm: 9.155504812952131e-06
Step: 17440, train/learning_rate: 2.9247976272017695e-05
Step: 17440, train/epoch: 4.150404453277588
Step: 17450, train/loss: 0.0
Step: 17450, train/grad_norm: 1.2637489987810113e-07
Step: 17450, train/learning_rate: 2.92360782623291e-05
Step: 17450, train/epoch: 4.15278434753418
Step: 17460, train/loss: 0.0
Step: 17460, train/grad_norm: 4.610044470609864e-06
Step: 17460, train/learning_rate: 2.9224178433651105e-05
Step: 17460, train/epoch: 4.1551642417907715
Step: 17470, train/loss: 0.0
Step: 17470, train/grad_norm: 3.909599399776198e-06
Step: 17470, train/learning_rate: 2.921228042396251e-05
Step: 17470, train/epoch: 4.157544136047363
Step: 17480, train/loss: 0.0
Step: 17480, train/grad_norm: 2.82563678410952e-06
Step: 17480, train/learning_rate: 2.9200380595284514e-05
Step: 17480, train/epoch: 4.159924030303955
Step: 17490, train/loss: 0.0
Step: 17490, train/grad_norm: 0.00010591207683319226
Step: 17490, train/learning_rate: 2.9188480766606517e-05
Step: 17490, train/epoch: 4.162303447723389
Step: 17500, train/loss: 9.999999747378752e-05
Step: 17500, train/grad_norm: 2.5256955268559977e-05
Step: 17500, train/learning_rate: 2.9176582756917924e-05
Step: 17500, train/epoch: 4.1646833419799805
Step: 17510, train/loss: 0.0
Step: 17510, train/grad_norm: 2.8793058959308837e-07
Step: 17510, train/learning_rate: 2.9164682928239927e-05
Step: 17510, train/epoch: 4.167063236236572
Step: 17520, train/loss: 0.0
Step: 17520, train/grad_norm: 6.561356713064015e-06
Step: 17520, train/learning_rate: 2.9152784918551333e-05
Step: 17520, train/epoch: 4.169443130493164
Step: 17530, train/loss: 0.0
Step: 17530, train/grad_norm: 7.215048390207812e-05
Step: 17530, train/learning_rate: 2.9140885089873336e-05
Step: 17530, train/epoch: 4.171823024749756
Step: 17540, train/loss: 0.0
Step: 17540, train/grad_norm: 1.0231651970116218e-07
Step: 17540, train/learning_rate: 2.9128987080184743e-05
Step: 17540, train/epoch: 4.174202919006348
Step: 17550, train/loss: 0.0
Step: 17550, train/grad_norm: 7.608327450725483e-06
Step: 17550, train/learning_rate: 2.9117087251506746e-05
Step: 17550, train/epoch: 4.1765828132629395
Step: 17560, train/loss: 0.0
Step: 17560, train/grad_norm: 6.335200396279106e-06
Step: 17560, train/learning_rate: 2.910518742282875e-05
Step: 17560, train/epoch: 4.178962230682373
Step: 17570, train/loss: 0.0
Step: 17570, train/grad_norm: 6.5588187680987176e-06
Step: 17570, train/learning_rate: 2.9093289413140155e-05
Step: 17570, train/epoch: 4.181342124938965
Step: 17580, train/loss: 0.0
Step: 17580, train/grad_norm: 1.5189669284154661e-05
Step: 17580, train/learning_rate: 2.908138958446216e-05
Step: 17580, train/epoch: 4.183722019195557
Step: 17590, train/loss: 0.0
Step: 17590, train/grad_norm: 0.0005856555653735995
Step: 17590, train/learning_rate: 2.9069491574773565e-05
Step: 17590, train/epoch: 4.186101913452148
Step: 17600, train/loss: 0.0
Step: 17600, train/grad_norm: 0.00010975781333399937
Step: 17600, train/learning_rate: 2.9057591746095568e-05
Step: 17600, train/epoch: 4.18848180770874
Step: 17610, train/loss: 0.0
Step: 17610, train/grad_norm: 1.754474681092688e-07
Step: 17610, train/learning_rate: 2.904569191741757e-05
Step: 17610, train/epoch: 4.190861701965332
Step: 17620, train/loss: 0.0
Step: 17620, train/grad_norm: 0.00020090062753297389
Step: 17620, train/learning_rate: 2.9033793907728978e-05
Step: 17620, train/epoch: 4.193241119384766
Step: 17630, train/loss: 0.0
Step: 17630, train/grad_norm: 1.417590738128638e-07
Step: 17630, train/learning_rate: 2.902189407905098e-05
Step: 17630, train/epoch: 4.195621013641357
Step: 17640, train/loss: 0.0
Step: 17640, train/grad_norm: 2.601888695608068e-07
Step: 17640, train/learning_rate: 2.9009996069362387e-05
Step: 17640, train/epoch: 4.198000907897949
Step: 17650, train/loss: 0.0
Step: 17650, train/grad_norm: 8.431522360297095e-07
Step: 17650, train/learning_rate: 2.899809624068439e-05
Step: 17650, train/epoch: 4.200380802154541
Step: 17660, train/loss: 0.0
Step: 17660, train/grad_norm: 6.953956471988931e-05
Step: 17660, train/learning_rate: 2.8986196412006393e-05
Step: 17660, train/epoch: 4.202760696411133
Step: 17670, train/loss: 0.09650000184774399
Step: 17670, train/grad_norm: 4.4712680391967297e-05
Step: 17670, train/learning_rate: 2.89742984023178e-05
Step: 17670, train/epoch: 4.205140590667725
Step: 17680, train/loss: 0.00019999999494757503
Step: 17680, train/grad_norm: 0.01481903251260519
Step: 17680, train/learning_rate: 2.8962398573639803e-05
Step: 17680, train/epoch: 4.207520008087158
Step: 17690, train/loss: 0.0
Step: 17690, train/grad_norm: 2.718902123888256e-06
Step: 17690, train/learning_rate: 2.895050056395121e-05
Step: 17690, train/epoch: 4.20989990234375
Step: 17700, train/loss: 0.0
Step: 17700, train/grad_norm: 8.092820280580781e-07
Step: 17700, train/learning_rate: 2.8938600735273212e-05
Step: 17700, train/epoch: 4.212279796600342
Step: 17710, train/loss: 0.0
Step: 17710, train/grad_norm: 4.689752231001876e-08
Step: 17710, train/learning_rate: 2.8926700906595215e-05
Step: 17710, train/epoch: 4.214659690856934
Step: 17720, train/loss: 0.0
Step: 17720, train/grad_norm: 0.0003483723266981542
Step: 17720, train/learning_rate: 2.8914802896906622e-05
Step: 17720, train/epoch: 4.217039585113525
Step: 17730, train/loss: 0.0
Step: 17730, train/grad_norm: 3.5721308222491643e-07
Step: 17730, train/learning_rate: 2.8902903068228625e-05
Step: 17730, train/epoch: 4.219419479370117
Step: 17740, train/loss: 0.0
Step: 17740, train/grad_norm: 1.20482400234323e-06
Step: 17740, train/learning_rate: 2.889100505854003e-05
Step: 17740, train/epoch: 4.221799373626709
Step: 17750, train/loss: 0.0
Step: 17750, train/grad_norm: 1.0938319974229671e-06
Step: 17750, train/learning_rate: 2.8879105229862034e-05
Step: 17750, train/epoch: 4.224178791046143
Step: 17760, train/loss: 0.0
Step: 17760, train/grad_norm: 6.044576730346307e-05
Step: 17760, train/learning_rate: 2.8867205401184037e-05
Step: 17760, train/epoch: 4.226558685302734
Step: 17770, train/loss: 0.000699999975040555
Step: 17770, train/grad_norm: 1.2488271750044078e-06
Step: 17770, train/learning_rate: 2.8855307391495444e-05
Step: 17770, train/epoch: 4.228938579559326
Step: 17780, train/loss: 0.0
Step: 17780, train/grad_norm: 3.256922582295374e-06
Step: 17780, train/learning_rate: 2.8843407562817447e-05
Step: 17780, train/epoch: 4.231318473815918
Step: 17790, train/loss: 0.008200000040233135
Step: 17790, train/grad_norm: 2.1600769741780823e-06
Step: 17790, train/learning_rate: 2.8831509553128853e-05
Step: 17790, train/epoch: 4.23369836807251
Step: 17800, train/loss: 0.0
Step: 17800, train/grad_norm: 4.865674441134615e-07
Step: 17800, train/learning_rate: 2.8819609724450856e-05
Step: 17800, train/epoch: 4.236078262329102
Step: 17810, train/loss: 0.0
Step: 17810, train/grad_norm: 3.25254222843796e-06
Step: 17810, train/learning_rate: 2.880770989577286e-05
Step: 17810, train/epoch: 4.238457679748535
Step: 17820, train/loss: 0.0
Step: 17820, train/grad_norm: 5.162167508387938e-05
Step: 17820, train/learning_rate: 2.8795811886084266e-05
Step: 17820, train/epoch: 4.240837574005127
Step: 17830, train/loss: 0.0
Step: 17830, train/grad_norm: 6.102970655774698e-05
Step: 17830, train/learning_rate: 2.878391205740627e-05
Step: 17830, train/epoch: 4.243217468261719
Step: 17840, train/loss: 0.0
Step: 17840, train/grad_norm: 0.0003112766135018319
Step: 17840, train/learning_rate: 2.8772014047717676e-05
Step: 17840, train/epoch: 4.2455973625183105
Step: 17850, train/loss: 0.0
Step: 17850, train/grad_norm: 4.691381150223606e-07
Step: 17850, train/learning_rate: 2.876011421903968e-05
Step: 17850, train/epoch: 4.247977256774902
Step: 17860, train/loss: 0.0
Step: 17860, train/grad_norm: 6.4477517298655584e-06
Step: 17860, train/learning_rate: 2.874821439036168e-05
Step: 17860, train/epoch: 4.250357151031494
Step: 17870, train/loss: 0.0
Step: 17870, train/grad_norm: 5.47164916042675e-07
Step: 17870, train/learning_rate: 2.8736316380673088e-05
Step: 17870, train/epoch: 4.252736568450928
Step: 17880, train/loss: 0.0
Step: 17880, train/grad_norm: 4.881715653937135e-07
Step: 17880, train/learning_rate: 2.872441655199509e-05
Step: 17880, train/epoch: 4.2551164627075195
Step: 17890, train/loss: 0.0
Step: 17890, train/grad_norm: 9.035677521751495e-07
Step: 17890, train/learning_rate: 2.8712518542306498e-05
Step: 17890, train/epoch: 4.257496356964111
Step: 17900, train/loss: 0.0
Step: 17900, train/grad_norm: 2.494701050181902e-07
Step: 17900, train/learning_rate: 2.87006187136285e-05
Step: 17900, train/epoch: 4.259876251220703
Step: 17910, train/loss: 0.0
Step: 17910, train/grad_norm: 8.202929961953487e-07
Step: 17910, train/learning_rate: 2.8688718884950504e-05
Step: 17910, train/epoch: 4.262256145477295
Step: 17920, train/loss: 0.0
Step: 17920, train/grad_norm: 1.1169673598487861e-05
Step: 17920, train/learning_rate: 2.867682087526191e-05
Step: 17920, train/epoch: 4.264636039733887
Step: 17930, train/loss: 0.0
Step: 17930, train/grad_norm: 9.657924238126725e-06
Step: 17930, train/learning_rate: 2.8664921046583913e-05
Step: 17930, train/epoch: 4.2670159339904785
Step: 17940, train/loss: 0.0
Step: 17940, train/grad_norm: 6.589120857825037e-07
Step: 17940, train/learning_rate: 2.865302303689532e-05
Step: 17940, train/epoch: 4.269395351409912
Step: 17950, train/loss: 0.012299999594688416
Step: 17950, train/grad_norm: 6.353441222017864e-06
Step: 17950, train/learning_rate: 2.8641123208217323e-05
Step: 17950, train/epoch: 4.271775245666504
Step: 17960, train/loss: 0.0
Step: 17960, train/grad_norm: 4.523332790995482e-06
Step: 17960, train/learning_rate: 2.8629223379539326e-05
Step: 17960, train/epoch: 4.274155139923096
Step: 17970, train/loss: 0.0
Step: 17970, train/grad_norm: 4.94839014208992e-08
Step: 17970, train/learning_rate: 2.8617325369850732e-05
Step: 17970, train/epoch: 4.2765350341796875
Step: 17980, train/loss: 0.0
Step: 17980, train/grad_norm: 0.0013532924931496382
Step: 17980, train/learning_rate: 2.8605425541172735e-05
Step: 17980, train/epoch: 4.278914928436279
Step: 17990, train/loss: 0.0
Step: 17990, train/grad_norm: 4.55783010693267e-05
Step: 17990, train/learning_rate: 2.8593527531484142e-05
Step: 17990, train/epoch: 4.281294822692871
Step: 18000, train/loss: 0.0
Step: 18000, train/grad_norm: 9.831448011254906e-08
Step: 18000, train/learning_rate: 2.8581627702806145e-05
Step: 18000, train/epoch: 4.283674240112305
Step: 18010, train/loss: 0.0
Step: 18010, train/grad_norm: 4.4106304386559714e-08
Step: 18010, train/learning_rate: 2.8569727874128148e-05
Step: 18010, train/epoch: 4.2860541343688965
Step: 18020, train/loss: 0.0
Step: 18020, train/grad_norm: 7.30960962869176e-08
Step: 18020, train/learning_rate: 2.8557829864439555e-05
Step: 18020, train/epoch: 4.288434028625488
Step: 18030, train/loss: 0.0
Step: 18030, train/grad_norm: 1.5973651557033008e-07
Step: 18030, train/learning_rate: 2.8545930035761558e-05
Step: 18030, train/epoch: 4.29081392288208
Step: 18040, train/loss: 0.0
Step: 18040, train/grad_norm: 1.8148456604194507e-07
Step: 18040, train/learning_rate: 2.8534032026072964e-05
Step: 18040, train/epoch: 4.293193817138672
Step: 18050, train/loss: 0.0
Step: 18050, train/grad_norm: 1.2643855029637052e-07
Step: 18050, train/learning_rate: 2.8522132197394967e-05
Step: 18050, train/epoch: 4.295573711395264
Step: 18060, train/loss: 0.0
Step: 18060, train/grad_norm: 0.001386538497172296
Step: 18060, train/learning_rate: 2.851023236871697e-05
Step: 18060, train/epoch: 4.297953128814697
Step: 18070, train/loss: 0.0
Step: 18070, train/grad_norm: 6.174010422910214e-07
Step: 18070, train/learning_rate: 2.8498334359028377e-05
Step: 18070, train/epoch: 4.300333023071289
Step: 18080, train/loss: 0.0
Step: 18080, train/grad_norm: 0.00021040237334091216
Step: 18080, train/learning_rate: 2.848643453035038e-05
Step: 18080, train/epoch: 4.302712917327881
Step: 18090, train/loss: 0.0
Step: 18090, train/grad_norm: 0.001650327700190246
Step: 18090, train/learning_rate: 2.8474536520661786e-05
Step: 18090, train/epoch: 4.305092811584473
Step: 18100, train/loss: 0.0
Step: 18100, train/grad_norm: 2.5941066184032024e-08
Step: 18100, train/learning_rate: 2.846263669198379e-05
Step: 18100, train/epoch: 4.3074727058410645
Step: 18110, train/loss: 0.0
Step: 18110, train/grad_norm: 5.2398754633031785e-06
Step: 18110, train/learning_rate: 2.8450736863305792e-05
Step: 18110, train/epoch: 4.309852600097656
Step: 18120, train/loss: 0.0
Step: 18120, train/grad_norm: 4.2862734517257195e-07
Step: 18120, train/learning_rate: 2.84388388536172e-05
Step: 18120, train/epoch: 4.312232494354248
Step: 18130, train/loss: 0.0
Step: 18130, train/grad_norm: 2.8651277261815267e-07
Step: 18130, train/learning_rate: 2.8426939024939202e-05
Step: 18130, train/epoch: 4.314611911773682
Step: 18140, train/loss: 0.0
Step: 18140, train/grad_norm: 3.338712417644274e-08
Step: 18140, train/learning_rate: 2.841504101525061e-05
Step: 18140, train/epoch: 4.316991806030273
Step: 18150, train/loss: 0.0005000000237487257
Step: 18150, train/grad_norm: 2.5493383759567223e-07
Step: 18150, train/learning_rate: 2.840314118657261e-05
Step: 18150, train/epoch: 4.319371700286865
Step: 18160, train/loss: 0.0
Step: 18160, train/grad_norm: 2.4586523750258493e-07
Step: 18160, train/learning_rate: 2.8391241357894614e-05
Step: 18160, train/epoch: 4.321751594543457
Step: 18170, train/loss: 0.16019999980926514
Step: 18170, train/grad_norm: 93.36823272705078
Step: 18170, train/learning_rate: 2.837934334820602e-05
Step: 18170, train/epoch: 4.324131488800049
Step: 18180, train/loss: 0.0
Step: 18180, train/grad_norm: 9.409920949110528e-07
Step: 18180, train/learning_rate: 2.8367443519528024e-05
Step: 18180, train/epoch: 4.326511383056641
Step: 18190, train/loss: 0.0
Step: 18190, train/grad_norm: 6.241199912437878e-08
Step: 18190, train/learning_rate: 2.835554550983943e-05
Step: 18190, train/epoch: 4.328890800476074
Step: 18200, train/loss: 0.0
Step: 18200, train/grad_norm: 3.5299653973197564e-06
Step: 18200, train/learning_rate: 2.8343645681161433e-05
Step: 18200, train/epoch: 4.331270694732666
Step: 18210, train/loss: 0.0
Step: 18210, train/grad_norm: 3.8535185012733564e-05
Step: 18210, train/learning_rate: 2.833174767147284e-05
Step: 18210, train/epoch: 4.333650588989258
Step: 18220, train/loss: 0.0
Step: 18220, train/grad_norm: 3.2628284429847554e-07
Step: 18220, train/learning_rate: 2.8319847842794843e-05
Step: 18220, train/epoch: 4.33603048324585
Step: 18230, train/loss: 0.0
Step: 18230, train/grad_norm: 3.793170617427677e-05
Step: 18230, train/learning_rate: 2.8307948014116846e-05
Step: 18230, train/epoch: 4.338410377502441
Step: 18240, train/loss: 0.0
Step: 18240, train/grad_norm: 0.00012586891534738243
Step: 18240, train/learning_rate: 2.8296050004428253e-05
Step: 18240, train/epoch: 4.340790271759033
Step: 18250, train/loss: 0.0
Step: 18250, train/grad_norm: 7.69491889514029e-05
Step: 18250, train/learning_rate: 2.8284150175750256e-05
Step: 18250, train/epoch: 4.343169689178467
Step: 18260, train/loss: 9.999999747378752e-05
Step: 18260, train/grad_norm: 5.983662987318894e-08
Step: 18260, train/learning_rate: 2.8272252166061662e-05
Step: 18260, train/epoch: 4.345549583435059
Step: 18270, train/loss: 0.0
Step: 18270, train/grad_norm: 2.4621963348181453e-06
Step: 18270, train/learning_rate: 2.8260352337383665e-05
Step: 18270, train/epoch: 4.34792947769165
Step: 18280, train/loss: 0.0
Step: 18280, train/grad_norm: 6.60194200463593e-05
Step: 18280, train/learning_rate: 2.8248452508705668e-05
Step: 18280, train/epoch: 4.350309371948242
Step: 18290, train/loss: 0.0
Step: 18290, train/grad_norm: 1.777567035787797e-07
Step: 18290, train/learning_rate: 2.8236554499017075e-05
Step: 18290, train/epoch: 4.352689266204834
Step: 18300, train/loss: 0.0
Step: 18300, train/grad_norm: 2.461609938109177e-08
Step: 18300, train/learning_rate: 2.8224654670339078e-05
Step: 18300, train/epoch: 4.355069160461426
Step: 18310, train/loss: 0.0
Step: 18310, train/grad_norm: 2.42905883851563e-07
Step: 18310, train/learning_rate: 2.8212756660650484e-05
Step: 18310, train/epoch: 4.357449054718018
Step: 18320, train/loss: 0.0
Step: 18320, train/grad_norm: 7.049128271319205e-06
Step: 18320, train/learning_rate: 2.8200856831972487e-05
Step: 18320, train/epoch: 4.359828472137451
Step: 18330, train/loss: 0.0
Step: 18330, train/grad_norm: 0.00039135123370215297
Step: 18330, train/learning_rate: 2.818895700329449e-05
Step: 18330, train/epoch: 4.362208366394043
Step: 18340, train/loss: 0.0
Step: 18340, train/grad_norm: 3.395377845549774e-08
Step: 18340, train/learning_rate: 2.8177058993605897e-05
Step: 18340, train/epoch: 4.364588260650635
Step: 18350, train/loss: 0.0
Step: 18350, train/grad_norm: 9.874689794742153e-07
Step: 18350, train/learning_rate: 2.81651591649279e-05
Step: 18350, train/epoch: 4.366968154907227
Step: 18360, train/loss: 0.0
Step: 18360, train/grad_norm: 7.000220648478717e-05
Step: 18360, train/learning_rate: 2.8153261155239306e-05
Step: 18360, train/epoch: 4.369348049163818
Step: 18370, train/loss: 0.014100000262260437
Step: 18370, train/grad_norm: 2.7062505978392437e-06
Step: 18370, train/learning_rate: 2.814136132656131e-05
Step: 18370, train/epoch: 4.37172794342041
Step: 18380, train/loss: 0.0
Step: 18380, train/grad_norm: 4.097607941844217e-08
Step: 18380, train/learning_rate: 2.8129461497883312e-05
Step: 18380, train/epoch: 4.374107360839844
Step: 18390, train/loss: 0.0
Step: 18390, train/grad_norm: 7.386502147710416e-07
Step: 18390, train/learning_rate: 2.811756348819472e-05
Step: 18390, train/epoch: 4.3764872550964355
Step: 18400, train/loss: 0.0
Step: 18400, train/grad_norm: 0.03561420738697052
Step: 18400, train/learning_rate: 2.8105663659516722e-05
Step: 18400, train/epoch: 4.378867149353027
Step: 18410, train/loss: 0.0
Step: 18410, train/grad_norm: 0.0005854510236531496
Step: 18410, train/learning_rate: 2.809376564982813e-05
Step: 18410, train/epoch: 4.381247043609619
Step: 18420, train/loss: 0.00039999998989515007
Step: 18420, train/grad_norm: 1.2654415968427202e-06
Step: 18420, train/learning_rate: 2.808186582115013e-05
Step: 18420, train/epoch: 4.383626937866211
Step: 18430, train/loss: 0.0
Step: 18430, train/grad_norm: 6.183152123639957e-08
Step: 18430, train/learning_rate: 2.8069965992472135e-05
Step: 18430, train/epoch: 4.386006832122803
Step: 18440, train/loss: 0.0
Step: 18440, train/grad_norm: 6.916736765560927e-06
Step: 18440, train/learning_rate: 2.805806798278354e-05
Step: 18440, train/epoch: 4.388386249542236
Step: 18450, train/loss: 0.0
Step: 18450, train/grad_norm: 2.0131288692937233e-05
Step: 18450, train/learning_rate: 2.8046168154105544e-05
Step: 18450, train/epoch: 4.390766143798828
Step: 18460, train/loss: 0.0
Step: 18460, train/grad_norm: 1.4858529766570427e-06
Step: 18460, train/learning_rate: 2.803427014441695e-05
Step: 18460, train/epoch: 4.39314603805542
Step: 18470, train/loss: 0.0
Step: 18470, train/grad_norm: 9.246590479961014e-07
Step: 18470, train/learning_rate: 2.8022370315738954e-05
Step: 18470, train/epoch: 4.395525932312012
Step: 18480, train/loss: 0.0
Step: 18480, train/grad_norm: 3.2687553357391153e-06
Step: 18480, train/learning_rate: 2.8010470487060957e-05
Step: 18480, train/epoch: 4.3979058265686035
Step: 18490, train/loss: 0.0
Step: 18490, train/grad_norm: 2.217172550444957e-05
Step: 18490, train/learning_rate: 2.7998572477372363e-05
Step: 18490, train/epoch: 4.400285720825195
Step: 18500, train/loss: 0.0
Step: 18500, train/grad_norm: 2.032521351225114e-08
Step: 18500, train/learning_rate: 2.7986672648694366e-05
Step: 18500, train/epoch: 4.402665615081787
Step: 18510, train/loss: 0.0
Step: 18510, train/grad_norm: 0.0007912046858109534
Step: 18510, train/learning_rate: 2.7974774639005773e-05
Step: 18510, train/epoch: 4.405045032501221
Step: 18520, train/loss: 0.0
Step: 18520, train/grad_norm: 9.27948846651816e-08
Step: 18520, train/learning_rate: 2.7962874810327776e-05
Step: 18520, train/epoch: 4.4074249267578125
Step: 18530, train/loss: 0.0
Step: 18530, train/grad_norm: 2.3266807147592772e-07
Step: 18530, train/learning_rate: 2.795097498164978e-05
Step: 18530, train/epoch: 4.409804821014404
Step: 18540, train/loss: 0.0
Step: 18540, train/grad_norm: 3.7965060073474888e-06
Step: 18540, train/learning_rate: 2.7939076971961185e-05
Step: 18540, train/epoch: 4.412184715270996
Step: 18550, train/loss: 0.0
Step: 18550, train/grad_norm: 9.503270916866313e-07
Step: 18550, train/learning_rate: 2.792717714328319e-05
Step: 18550, train/epoch: 4.414564609527588
Step: 18560, train/loss: 0.0
Step: 18560, train/grad_norm: 9.900387340167072e-06
Step: 18560, train/learning_rate: 2.7915279133594595e-05
Step: 18560, train/epoch: 4.41694450378418
Step: 18570, train/loss: 0.0
Step: 18570, train/grad_norm: 6.9616876707812025e-09
Step: 18570, train/learning_rate: 2.7903379304916598e-05
Step: 18570, train/epoch: 4.419323921203613
Step: 18580, train/loss: 0.0
Step: 18580, train/grad_norm: 1.20166396300192e-06
Step: 18580, train/learning_rate: 2.78914794762386e-05
Step: 18580, train/epoch: 4.421703815460205
Step: 18590, train/loss: 0.0
Step: 18590, train/grad_norm: 9.0653462393675e-05
Step: 18590, train/learning_rate: 2.7879581466550007e-05
Step: 18590, train/epoch: 4.424083709716797
Step: 18600, train/loss: 0.0
Step: 18600, train/grad_norm: 4.931232979288325e-06
Step: 18600, train/learning_rate: 2.786768163787201e-05
Step: 18600, train/epoch: 4.426463603973389
Step: 18610, train/loss: 0.0
Step: 18610, train/grad_norm: 1.076774515240686e-05
Step: 18610, train/learning_rate: 2.7855783628183417e-05
Step: 18610, train/epoch: 4.4288434982299805
Step: 18620, train/loss: 0.0
Step: 18620, train/grad_norm: 1.4733062016603071e-05
Step: 18620, train/learning_rate: 2.784388379950542e-05
Step: 18620, train/epoch: 4.431223392486572
Step: 18630, train/loss: 0.0
Step: 18630, train/grad_norm: 3.1549943741993047e-06
Step: 18630, train/learning_rate: 2.7831983970827423e-05
Step: 18630, train/epoch: 4.433602809906006
Step: 18640, train/loss: 0.0
Step: 18640, train/grad_norm: 8.683226724315318e-07
Step: 18640, train/learning_rate: 2.782008596113883e-05
Step: 18640, train/epoch: 4.435982704162598
Step: 18650, train/loss: 0.0
Step: 18650, train/grad_norm: 3.728265483005089e-07
Step: 18650, train/learning_rate: 2.7808186132460833e-05
Step: 18650, train/epoch: 4.4383625984191895
Step: 18660, train/loss: 0.0
Step: 18660, train/grad_norm: 0.050073932856321335
Step: 18660, train/learning_rate: 2.779628812277224e-05
Step: 18660, train/epoch: 4.440742492675781
Step: 18670, train/loss: 0.0
Step: 18670, train/grad_norm: 5.223343464422214e-07
Step: 18670, train/learning_rate: 2.7784388294094242e-05
Step: 18670, train/epoch: 4.443122386932373
Step: 18680, train/loss: 0.0
Step: 18680, train/grad_norm: 8.608928965259111e-07
Step: 18680, train/learning_rate: 2.7772488465416245e-05
Step: 18680, train/epoch: 4.445502281188965
Step: 18690, train/loss: 0.0
Step: 18690, train/grad_norm: 1.882567488564746e-07
Step: 18690, train/learning_rate: 2.7760590455727652e-05
Step: 18690, train/epoch: 4.447882175445557
Step: 18700, train/loss: 0.19380000233650208
Step: 18700, train/grad_norm: 4.800368742507999e-07
Step: 18700, train/learning_rate: 2.7748690627049655e-05
Step: 18700, train/epoch: 4.45026159286499
Step: 18710, train/loss: 0.0
Step: 18710, train/grad_norm: 2.0042700725753093e-06
Step: 18710, train/learning_rate: 2.773679261736106e-05
Step: 18710, train/epoch: 4.452641487121582
Step: 18720, train/loss: 0.1031000018119812
Step: 18720, train/grad_norm: 2.7415590011514723e-05
Step: 18720, train/learning_rate: 2.7724892788683064e-05
Step: 18720, train/epoch: 4.455021381378174
Step: 18730, train/loss: 0.0
Step: 18730, train/grad_norm: 7.686522440053523e-05
Step: 18730, train/learning_rate: 2.7712992960005067e-05
Step: 18730, train/epoch: 4.457401275634766
Step: 18740, train/loss: 0.0
Step: 18740, train/grad_norm: 0.0019394303672015667
Step: 18740, train/learning_rate: 2.7701094950316474e-05
Step: 18740, train/epoch: 4.459781169891357
Step: 18750, train/loss: 9.999999747378752e-05
Step: 18750, train/grad_norm: 1.1898699767698417e-06
Step: 18750, train/learning_rate: 2.7689195121638477e-05
Step: 18750, train/epoch: 4.462161064147949
Step: 18760, train/loss: 0.0
Step: 18760, train/grad_norm: 2.183108165354497e-07
Step: 18760, train/learning_rate: 2.7677297111949883e-05
Step: 18760, train/epoch: 4.464540481567383
Step: 18770, train/loss: 0.0003000000142492354
Step: 18770, train/grad_norm: 1.814722949688985e-08
Step: 18770, train/learning_rate: 2.7665397283271886e-05
Step: 18770, train/epoch: 4.466920375823975
Step: 18780, train/loss: 0.0
Step: 18780, train/grad_norm: 3.838842133063736e-08
Step: 18780, train/learning_rate: 2.765349745459389e-05
Step: 18780, train/epoch: 4.469300270080566
Step: 18790, train/loss: 0.0
Step: 18790, train/grad_norm: 0.00012998133024666458
Step: 18790, train/learning_rate: 2.7641599444905296e-05
Step: 18790, train/epoch: 4.471680164337158
Step: 18800, train/loss: 0.0
Step: 18800, train/grad_norm: 3.4801380479621002e-06
Step: 18800, train/learning_rate: 2.76296996162273e-05
Step: 18800, train/epoch: 4.47406005859375
Step: 18810, train/loss: 0.0
Step: 18810, train/grad_norm: 5.548063654714497e-06
Step: 18810, train/learning_rate: 2.7617801606538706e-05
Step: 18810, train/epoch: 4.476439952850342
Step: 18820, train/loss: 0.01899999938905239
Step: 18820, train/grad_norm: 3.455764030491082e-08
Step: 18820, train/learning_rate: 2.760590177786071e-05
Step: 18820, train/epoch: 4.478819847106934
Step: 18830, train/loss: 0.0
Step: 18830, train/grad_norm: 6.163223133626161e-06
Step: 18830, train/learning_rate: 2.759400194918271e-05
Step: 18830, train/epoch: 4.481199264526367
Step: 18840, train/loss: 0.20749999582767487
Step: 18840, train/grad_norm: 0.00011192206875421107
Step: 18840, train/learning_rate: 2.7582103939494118e-05
Step: 18840, train/epoch: 4.483579158782959
Step: 18850, train/loss: 0.0
Step: 18850, train/grad_norm: 2.9527036531362683e-05
Step: 18850, train/learning_rate: 2.757020411081612e-05
Step: 18850, train/epoch: 4.485959053039551
Step: 18860, train/loss: 0.0
Step: 18860, train/grad_norm: 0.0068253157660365105
Step: 18860, train/learning_rate: 2.7558306101127528e-05
Step: 18860, train/epoch: 4.488338947296143
Step: 18870, train/loss: 0.0008999999845400453
Step: 18870, train/grad_norm: 0.00029393043951131403
Step: 18870, train/learning_rate: 2.754640627244953e-05
Step: 18870, train/epoch: 4.490718841552734
Step: 18880, train/loss: 0.0
Step: 18880, train/grad_norm: 0.00016689243784639984
Step: 18880, train/learning_rate: 2.7534508262760937e-05
Step: 18880, train/epoch: 4.493098735809326
Step: 18890, train/loss: 0.032999999821186066
Step: 18890, train/grad_norm: 119.19972229003906
Step: 18890, train/learning_rate: 2.752260843408294e-05
Step: 18890, train/epoch: 4.49547815322876
Step: 18900, train/loss: 0.016200000420212746
Step: 18900, train/grad_norm: 3.8972459151409566e-05
Step: 18900, train/learning_rate: 2.7510708605404943e-05
Step: 18900, train/epoch: 4.497858047485352
Step: 18910, train/loss: 0.002199999988079071
Step: 18910, train/grad_norm: 1.2497486750362441e-05
Step: 18910, train/learning_rate: 2.749881059571635e-05
Step: 18910, train/epoch: 4.500237941741943
Step: 18920, train/loss: 0.0
Step: 18920, train/grad_norm: 5.940130904491525e-06
Step: 18920, train/learning_rate: 2.7486910767038353e-05
Step: 18920, train/epoch: 4.502617835998535
Step: 18930, train/loss: 0.0
Step: 18930, train/grad_norm: 5.122912511978939e-07
Step: 18930, train/learning_rate: 2.747501275734976e-05
Step: 18930, train/epoch: 4.504997730255127
Step: 18940, train/loss: 0.0
Step: 18940, train/grad_norm: 1.2290355698496569e-05
Step: 18940, train/learning_rate: 2.7463112928671762e-05
Step: 18940, train/epoch: 4.507377624511719
Step: 18950, train/loss: 0.00039999998989515007
Step: 18950, train/grad_norm: 4.593922138214111
Step: 18950, train/learning_rate: 2.7451213099993765e-05
Step: 18950, train/epoch: 4.509757041931152
Step: 18960, train/loss: 0.0
Step: 18960, train/grad_norm: 6.717034921166487e-06
Step: 18960, train/learning_rate: 2.7439315090305172e-05
Step: 18960, train/epoch: 4.512136936187744
Step: 18970, train/loss: 0.0005000000237487257
Step: 18970, train/grad_norm: 8.654167175292969
Step: 18970, train/learning_rate: 2.7427415261627175e-05
Step: 18970, train/epoch: 4.514516830444336
Step: 18980, train/loss: 0.0
Step: 18980, train/grad_norm: 4.374522859507124e-07
Step: 18980, train/learning_rate: 2.741551725193858e-05
Step: 18980, train/epoch: 4.516896724700928
Step: 18990, train/loss: 0.0
Step: 18990, train/grad_norm: 1.4934603314031847e-06
Step: 18990, train/learning_rate: 2.7403617423260584e-05
Step: 18990, train/epoch: 4.5192766189575195
Step: 19000, train/loss: 0.0
Step: 19000, train/grad_norm: 7.158367907322827e-07
Step: 19000, train/learning_rate: 2.7391717594582587e-05
Step: 19000, train/epoch: 4.521656513214111
Step: 19010, train/loss: 0.0
Step: 19010, train/grad_norm: 1.8658545286598383e-07
Step: 19010, train/learning_rate: 2.7379819584893994e-05
Step: 19010, train/epoch: 4.524036407470703
Step: 19020, train/loss: 0.0
Step: 19020, train/grad_norm: 1.8349678043705353e-07
Step: 19020, train/learning_rate: 2.7367919756215997e-05
Step: 19020, train/epoch: 4.526415824890137
Step: 19030, train/loss: 0.0
Step: 19030, train/grad_norm: 2.9370487482083263e-06
Step: 19030, train/learning_rate: 2.7356021746527404e-05
Step: 19030, train/epoch: 4.5287957191467285
Step: 19040, train/loss: 0.0
Step: 19040, train/grad_norm: 8.374000572075602e-06
Step: 19040, train/learning_rate: 2.7344121917849407e-05
Step: 19040, train/epoch: 4.53117561340332
Step: 19050, train/loss: 0.0
Step: 19050, train/grad_norm: 6.087175279390067e-06
Step: 19050, train/learning_rate: 2.733222208917141e-05
Step: 19050, train/epoch: 4.533555507659912
Step: 19060, train/loss: 0.0
Step: 19060, train/grad_norm: 2.3859163889028423e-07
Step: 19060, train/learning_rate: 2.7320324079482816e-05
Step: 19060, train/epoch: 4.535935401916504
Step: 19070, train/loss: 0.0
Step: 19070, train/grad_norm: 1.4318956118586357e-06
Step: 19070, train/learning_rate: 2.730842425080482e-05
Step: 19070, train/epoch: 4.538315296173096
Step: 19080, train/loss: 0.0
Step: 19080, train/grad_norm: 9.285400665248744e-06
Step: 19080, train/learning_rate: 2.7296526241116226e-05
Step: 19080, train/epoch: 4.540694713592529
Step: 19090, train/loss: 0.13130000233650208
Step: 19090, train/grad_norm: 2.514438222078752e-07
Step: 19090, train/learning_rate: 2.728462641243823e-05
Step: 19090, train/epoch: 4.543074607849121
Step: 19100, train/loss: 0.0
Step: 19100, train/grad_norm: 0.001387632335536182
Step: 19100, train/learning_rate: 2.7272726583760232e-05
Step: 19100, train/epoch: 4.545454502105713
Step: 19110, train/loss: 0.0
Step: 19110, train/grad_norm: 0.001492183655500412
Step: 19110, train/learning_rate: 2.7260828574071638e-05
Step: 19110, train/epoch: 4.547834396362305
Step: 19120, train/loss: 0.0
Step: 19120, train/grad_norm: 0.0005728612886741757
Step: 19120, train/learning_rate: 2.724892874539364e-05
Step: 19120, train/epoch: 4.5502142906188965
Step: 19130, train/loss: 0.0
Step: 19130, train/grad_norm: 6.402976578101516e-05
Step: 19130, train/learning_rate: 2.7237030735705048e-05
Step: 19130, train/epoch: 4.552594184875488
Step: 19140, train/loss: 0.0
Step: 19140, train/grad_norm: 0.011129633523523808
Step: 19140, train/learning_rate: 2.722513090702705e-05
Step: 19140, train/epoch: 4.554973602294922
Step: 19150, train/loss: 0.0
Step: 19150, train/grad_norm: 3.251955831728992e-06
Step: 19150, train/learning_rate: 2.7213231078349054e-05
Step: 19150, train/epoch: 4.557353496551514
Step: 19160, train/loss: 0.0
Step: 19160, train/grad_norm: 0.001475074328482151
Step: 19160, train/learning_rate: 2.720133306866046e-05
Step: 19160, train/epoch: 4.5597333908081055
Step: 19170, train/loss: 0.0
Step: 19170, train/grad_norm: 9.610435517970473e-05
Step: 19170, train/learning_rate: 2.7189433239982463e-05
Step: 19170, train/epoch: 4.562113285064697
Step: 19180, train/loss: 9.999999747378752e-05
Step: 19180, train/grad_norm: 0.0007106921402737498
Step: 19180, train/learning_rate: 2.717753523029387e-05
Step: 19180, train/epoch: 4.564493179321289
Step: 19190, train/loss: 0.0
Step: 19190, train/grad_norm: 3.0316428478727175e-07
Step: 19190, train/learning_rate: 2.7165635401615873e-05
Step: 19190, train/epoch: 4.566873073577881
Step: 19200, train/loss: 0.0
Step: 19200, train/grad_norm: 3.0223761768866098e-06
Step: 19200, train/learning_rate: 2.7153735572937876e-05
Step: 19200, train/epoch: 4.569252967834473
Step: 19210, train/loss: 0.0
Step: 19210, train/grad_norm: 9.524191000309656e-07
Step: 19210, train/learning_rate: 2.7141837563249283e-05
Step: 19210, train/epoch: 4.571632385253906
Step: 19220, train/loss: 0.0
Step: 19220, train/grad_norm: 1.028978999784158e-06
Step: 19220, train/learning_rate: 2.7129937734571286e-05
Step: 19220, train/epoch: 4.574012279510498
Step: 19230, train/loss: 0.0
Step: 19230, train/grad_norm: 1.1576218639675062e-05
Step: 19230, train/learning_rate: 2.7118039724882692e-05
Step: 19230, train/epoch: 4.57639217376709
Step: 19240, train/loss: 0.0
Step: 19240, train/grad_norm: 1.5662458281440195e-06
Step: 19240, train/learning_rate: 2.7106139896204695e-05
Step: 19240, train/epoch: 4.578772068023682
Step: 19250, train/loss: 0.0
Step: 19250, train/grad_norm: 9.206594313582173e-07
Step: 19250, train/learning_rate: 2.7094240067526698e-05
Step: 19250, train/epoch: 4.581151962280273
Step: 19260, train/loss: 0.0
Step: 19260, train/grad_norm: 4.099816123925848e-06
Step: 19260, train/learning_rate: 2.7082342057838105e-05
Step: 19260, train/epoch: 4.583531856536865
Step: 19270, train/loss: 0.0
Step: 19270, train/grad_norm: 1.6233940414167591e-07
Step: 19270, train/learning_rate: 2.7070442229160108e-05
Step: 19270, train/epoch: 4.585911273956299
Step: 19280, train/loss: 0.0
Step: 19280, train/grad_norm: 8.496929240209283e-07
Step: 19280, train/learning_rate: 2.7058544219471514e-05
Step: 19280, train/epoch: 4.588291168212891
Step: 19290, train/loss: 0.0
Step: 19290, train/grad_norm: 0.00015654967864975333
Step: 19290, train/learning_rate: 2.7046644390793517e-05
Step: 19290, train/epoch: 4.590671062469482
Step: 19300, train/loss: 0.0
Step: 19300, train/grad_norm: 7.685568448323465e-07
Step: 19300, train/learning_rate: 2.703474456211552e-05
Step: 19300, train/epoch: 4.593050956726074
Step: 19310, train/loss: 0.0
Step: 19310, train/grad_norm: 5.353948395736552e-08
Step: 19310, train/learning_rate: 2.7022846552426927e-05
Step: 19310, train/epoch: 4.595430850982666
Step: 19320, train/loss: 0.0
Step: 19320, train/grad_norm: 6.980263424338773e-05
Step: 19320, train/learning_rate: 2.701094672374893e-05
Step: 19320, train/epoch: 4.597810745239258
Step: 19330, train/loss: 0.0
Step: 19330, train/grad_norm: 5.80912740133499e-07
Step: 19330, train/learning_rate: 2.6999048714060336e-05
Step: 19330, train/epoch: 4.600190162658691
Step: 19340, train/loss: 0.0
Step: 19340, train/grad_norm: 2.660033896972891e-05
Step: 19340, train/learning_rate: 2.698714888538234e-05
Step: 19340, train/epoch: 4.602570056915283
Step: 19350, train/loss: 0.0
Step: 19350, train/grad_norm: 2.179493350240591e-07
Step: 19350, train/learning_rate: 2.6975249056704342e-05
Step: 19350, train/epoch: 4.604949951171875
Step: 19360, train/loss: 0.0
Step: 19360, train/grad_norm: 4.716478542832192e-06
Step: 19360, train/learning_rate: 2.696335104701575e-05
Step: 19360, train/epoch: 4.607329845428467
Step: 19370, train/loss: 0.0
Step: 19370, train/grad_norm: 5.369061000237707e-06
Step: 19370, train/learning_rate: 2.6951451218337752e-05
Step: 19370, train/epoch: 4.609709739685059
Step: 19380, train/loss: 0.0
Step: 19380, train/grad_norm: 9.653938832343556e-06
Step: 19380, train/learning_rate: 2.693955320864916e-05
Step: 19380, train/epoch: 4.61208963394165
Step: 19390, train/loss: 0.10220000147819519
Step: 19390, train/grad_norm: 4.954792530043051e-06
Step: 19390, train/learning_rate: 2.692765337997116e-05
Step: 19390, train/epoch: 4.614469528198242
Step: 19400, train/loss: 0.1062999963760376
Step: 19400, train/grad_norm: 1.6345986296073534e-05
Step: 19400, train/learning_rate: 2.6915753551293164e-05
Step: 19400, train/epoch: 4.616848945617676
Step: 19410, train/loss: 0.0
Step: 19410, train/grad_norm: 2.6525312932790257e-05
Step: 19410, train/learning_rate: 2.690385554160457e-05
Step: 19410, train/epoch: 4.619228839874268
Step: 19420, train/loss: 0.0
Step: 19420, train/grad_norm: 2.7871728036643617e-08
Step: 19420, train/learning_rate: 2.6891955712926574e-05
Step: 19420, train/epoch: 4.621608734130859
Step: 19430, train/loss: 0.0
Step: 19430, train/grad_norm: 3.0072089430177584e-05
Step: 19430, train/learning_rate: 2.688005770323798e-05
Step: 19430, train/epoch: 4.623988628387451
Step: 19440, train/loss: 0.0
Step: 19440, train/grad_norm: 3.836040195892565e-05
Step: 19440, train/learning_rate: 2.6868157874559984e-05
Step: 19440, train/epoch: 4.626368522644043
Step: 19450, train/loss: 0.1023000031709671
Step: 19450, train/grad_norm: 2.4879223929019645e-05
Step: 19450, train/learning_rate: 2.6856258045881987e-05
Step: 19450, train/epoch: 4.628748416900635
Step: 19460, train/loss: 0.0
Step: 19460, train/grad_norm: 0.0030675060115754604
Step: 19460, train/learning_rate: 2.6844360036193393e-05
Step: 19460, train/epoch: 4.631127834320068
Step: 19470, train/loss: 0.021299999207258224
Step: 19470, train/grad_norm: 1.7607548215892166e-05
Step: 19470, train/learning_rate: 2.6832460207515396e-05
Step: 19470, train/epoch: 4.63350772857666
Step: 19480, train/loss: 9.999999747378752e-05
Step: 19480, train/grad_norm: 1.4221108358469792e-05
Step: 19480, train/learning_rate: 2.6820562197826803e-05
Step: 19480, train/epoch: 4.635887622833252
Step: 19490, train/loss: 0.0
Step: 19490, train/grad_norm: 1.6221751138800755e-05
Step: 19490, train/learning_rate: 2.6808662369148806e-05
Step: 19490, train/epoch: 4.638267517089844
Step: 19500, train/loss: 0.0
Step: 19500, train/grad_norm: 6.923662532187791e-09
Step: 19500, train/learning_rate: 2.679676254047081e-05
Step: 19500, train/epoch: 4.6406474113464355
Step: 19510, train/loss: 0.0
Step: 19510, train/grad_norm: 8.852159112393565e-07
Step: 19510, train/learning_rate: 2.6784864530782215e-05
Step: 19510, train/epoch: 4.643027305603027
Step: 19520, train/loss: 0.0
Step: 19520, train/grad_norm: 9.461302283853001e-07
Step: 19520, train/learning_rate: 2.6772964702104218e-05
Step: 19520, train/epoch: 4.645406723022461
Step: 19530, train/loss: 0.0
Step: 19530, train/grad_norm: 4.210362021694891e-06
Step: 19530, train/learning_rate: 2.6761066692415625e-05
Step: 19530, train/epoch: 4.647786617279053
Step: 19540, train/loss: 0.0
Step: 19540, train/grad_norm: 6.730807058374921e-07
Step: 19540, train/learning_rate: 2.6749166863737628e-05
Step: 19540, train/epoch: 4.6501665115356445
Step: 19550, train/loss: 0.0006000000284984708
Step: 19550, train/grad_norm: 2.4461914449602773e-07
Step: 19550, train/learning_rate: 2.6737268854049034e-05
Step: 19550, train/epoch: 4.652546405792236
Step: 19560, train/loss: 0.0
Step: 19560, train/grad_norm: 6.950413990125526e-06
Step: 19560, train/learning_rate: 2.6725369025371037e-05
Step: 19560, train/epoch: 4.654926300048828
Step: 19570, train/loss: 0.0
Step: 19570, train/grad_norm: 0.00016072514699772
Step: 19570, train/learning_rate: 2.671346919669304e-05
Step: 19570, train/epoch: 4.65730619430542
Step: 19580, train/loss: 0.14219999313354492
Step: 19580, train/grad_norm: 5.48405951121822e-05
Step: 19580, train/learning_rate: 2.6701571187004447e-05
Step: 19580, train/epoch: 4.659686088562012
Step: 19590, train/loss: 0.0
Step: 19590, train/grad_norm: 2.010425305343233e-05
Step: 19590, train/learning_rate: 2.668967135832645e-05
Step: 19590, train/epoch: 4.662065505981445
Step: 19600, train/loss: 0.0
Step: 19600, train/grad_norm: 7.065273734951916e-07
Step: 19600, train/learning_rate: 2.6677773348637857e-05
Step: 19600, train/epoch: 4.664445400238037
Step: 19610, train/loss: 0.0
Step: 19610, train/grad_norm: 0.00018289861327502877
Step: 19610, train/learning_rate: 2.666587351995986e-05
Step: 19610, train/epoch: 4.666825294494629
Step: 19620, train/loss: 0.07500000298023224
Step: 19620, train/grad_norm: 1.3194902237501083e-07
Step: 19620, train/learning_rate: 2.6653973691281863e-05
Step: 19620, train/epoch: 4.669205188751221
Step: 19630, train/loss: 0.0
Step: 19630, train/grad_norm: 0.014459938742220402
Step: 19630, train/learning_rate: 2.664207568159327e-05
Step: 19630, train/epoch: 4.6715850830078125
Step: 19640, train/loss: 0.0
Step: 19640, train/grad_norm: 9.829751434153877e-06
Step: 19640, train/learning_rate: 2.6630175852915272e-05
Step: 19640, train/epoch: 4.673964977264404
Step: 19650, train/loss: 0.00019999999494757503
Step: 19650, train/grad_norm: 2.0772871778262925e-07
Step: 19650, train/learning_rate: 2.661827784322668e-05
Step: 19650, train/epoch: 4.676344394683838
Step: 19660, train/loss: 0.0
Step: 19660, train/grad_norm: 1.4431170711759478e-06
Step: 19660, train/learning_rate: 2.660637801454868e-05
Step: 19660, train/epoch: 4.67872428894043
Step: 19670, train/loss: 0.0
Step: 19670, train/grad_norm: 2.2817803255748004e-05
Step: 19670, train/learning_rate: 2.6594478185870685e-05
Step: 19670, train/epoch: 4.6811041831970215
Step: 19680, train/loss: 0.0
Step: 19680, train/grad_norm: 8.942622571339598e-07
Step: 19680, train/learning_rate: 2.658258017618209e-05
Step: 19680, train/epoch: 4.683484077453613
Step: 19690, train/loss: 0.0
Step: 19690, train/grad_norm: 1.1788103648768811e-07
Step: 19690, train/learning_rate: 2.6570680347504094e-05
Step: 19690, train/epoch: 4.685863971710205
Step: 19700, train/loss: 0.0
Step: 19700, train/grad_norm: 7.177924317147699e-07
Step: 19700, train/learning_rate: 2.65587823378155e-05
Step: 19700, train/epoch: 4.688243865966797
Step: 19710, train/loss: 0.0
Step: 19710, train/grad_norm: 3.652853047242388e-05
Step: 19710, train/learning_rate: 2.6546882509137504e-05
Step: 19710, train/epoch: 4.6906232833862305
Step: 19720, train/loss: 0.0
Step: 19720, train/grad_norm: 1.8145001376979053e-05
Step: 19720, train/learning_rate: 2.6534982680459507e-05
Step: 19720, train/epoch: 4.693003177642822
Step: 19730, train/loss: 0.0
Step: 19730, train/grad_norm: 7.130765879992396e-07
Step: 19730, train/learning_rate: 2.6523084670770913e-05
Step: 19730, train/epoch: 4.695383071899414
Step: 19740, train/loss: 0.0
Step: 19740, train/grad_norm: 4.043778517370811e-06
Step: 19740, train/learning_rate: 2.6511184842092916e-05
Step: 19740, train/epoch: 4.697762966156006
Step: 19750, train/loss: 0.0
Step: 19750, train/grad_norm: 5.669200504598848e-07
Step: 19750, train/learning_rate: 2.6499286832404323e-05
Step: 19750, train/epoch: 4.700142860412598
Step: 19760, train/loss: 0.0
Step: 19760, train/grad_norm: 1.4328382349049207e-05
Step: 19760, train/learning_rate: 2.6487387003726326e-05
Step: 19760, train/epoch: 4.7025227546691895
Step: 19770, train/loss: 0.0
Step: 19770, train/grad_norm: 8.3448518125806e-05
Step: 19770, train/learning_rate: 2.647548717504833e-05
Step: 19770, train/epoch: 4.704902648925781
Step: 19780, train/loss: 0.0
Step: 19780, train/grad_norm: 5.940773007750977e-06
Step: 19780, train/learning_rate: 2.6463589165359735e-05
Step: 19780, train/epoch: 4.707282066345215
Step: 19790, train/loss: 0.0
Step: 19790, train/grad_norm: 3.559196386504482e-07
Step: 19790, train/learning_rate: 2.645168933668174e-05
Step: 19790, train/epoch: 4.709661960601807
Step: 19800, train/loss: 0.0
Step: 19800, train/grad_norm: 0.00013688574836123735
Step: 19800, train/learning_rate: 2.6439791326993145e-05
Step: 19800, train/epoch: 4.712041854858398
Step: 19810, train/loss: 0.0003000000142492354
Step: 19810, train/grad_norm: 2.2737298011779785
Step: 19810, train/learning_rate: 2.6427891498315148e-05
Step: 19810, train/epoch: 4.71442174911499
Step: 19820, train/loss: 0.0
Step: 19820, train/grad_norm: 0.04923506826162338
Step: 19820, train/learning_rate: 2.641599166963715e-05
Step: 19820, train/epoch: 4.716801643371582
Step: 19830, train/loss: 0.04820000007748604
Step: 19830, train/grad_norm: 4.817867704787204e-08
Step: 19830, train/learning_rate: 2.6404093659948558e-05
Step: 19830, train/epoch: 4.719181537628174
Step: 19840, train/loss: 0.0
Step: 19840, train/grad_norm: 1.4056440704734996e-06
Step: 19840, train/learning_rate: 2.639219383127056e-05
Step: 19840, train/epoch: 4.721560955047607
Step: 19850, train/loss: 0.0
Step: 19850, train/grad_norm: 6.493436899290828e-07
Step: 19850, train/learning_rate: 2.6380295821581967e-05
Step: 19850, train/epoch: 4.723940849304199
Step: 19860, train/loss: 9.999999747378752e-05
Step: 19860, train/grad_norm: 6.417756139853736e-07
Step: 19860, train/learning_rate: 2.636839599290397e-05
Step: 19860, train/epoch: 4.726320743560791
Step: 19870, train/loss: 0.0
Step: 19870, train/grad_norm: 5.019656441618281e-07
Step: 19870, train/learning_rate: 2.6356496164225973e-05
Step: 19870, train/epoch: 4.728700637817383
Step: 19880, train/loss: 0.0
Step: 19880, train/grad_norm: 4.260942887412966e-07
Step: 19880, train/learning_rate: 2.634459815453738e-05
Step: 19880, train/epoch: 4.731080532073975
Step: 19890, train/loss: 0.1234000027179718
Step: 19890, train/grad_norm: 335.1839294433594
Step: 19890, train/learning_rate: 2.6332698325859383e-05
Step: 19890, train/epoch: 4.733460426330566
Step: 19900, train/loss: 0.0
Step: 19900, train/grad_norm: 0.00030885785236023366
Step: 19900, train/learning_rate: 2.632080031617079e-05
Step: 19900, train/epoch: 4.73583984375
Step: 19910, train/loss: 0.0
Step: 19910, train/grad_norm: 9.818377293413505e-06
Step: 19910, train/learning_rate: 2.6308900487492792e-05
Step: 19910, train/epoch: 4.738219738006592
Step: 19920, train/loss: 0.0
Step: 19920, train/grad_norm: 8.517264177498873e-06
Step: 19920, train/learning_rate: 2.6297000658814795e-05
Step: 19920, train/epoch: 4.740599632263184
Step: 19930, train/loss: 0.0
Step: 19930, train/grad_norm: 3.3335187055172355e-08
Step: 19930, train/learning_rate: 2.6285102649126202e-05
Step: 19930, train/epoch: 4.742979526519775
Step: 19940, train/loss: 0.0
Step: 19940, train/grad_norm: 2.048771534646221e-07
Step: 19940, train/learning_rate: 2.6273202820448205e-05
Step: 19940, train/epoch: 4.745359420776367
Step: 19950, train/loss: 0.0
Step: 19950, train/grad_norm: 5.980457018495144e-08
Step: 19950, train/learning_rate: 2.626130481075961e-05
Step: 19950, train/epoch: 4.747739315032959
Step: 19960, train/loss: 0.0
Step: 19960, train/grad_norm: 7.897426002045904e-08
Step: 19960, train/learning_rate: 2.6249404982081614e-05
Step: 19960, train/epoch: 4.750119209289551
Step: 19970, train/loss: 0.0
Step: 19970, train/grad_norm: 2.794925713089924e-08
Step: 19970, train/learning_rate: 2.6237505153403617e-05
Step: 19970, train/epoch: 4.752498626708984
Step: 19980, train/loss: 0.0
Step: 19980, train/grad_norm: 1.921831227491566e-08
Step: 19980, train/learning_rate: 2.6225607143715024e-05
Step: 19980, train/epoch: 4.754878520965576
Step: 19990, train/loss: 0.0
Step: 19990, train/grad_norm: 3.1121319921112445e-07
Step: 19990, train/learning_rate: 2.6213707315037027e-05
Step: 19990, train/epoch: 4.757258415222168
Step: 20000, train/loss: 0.0
Step: 20000, train/grad_norm: 5.004607714909071e-07
Step: 20000, train/learning_rate: 2.6201809305348434e-05
Step: 20000, train/epoch: 4.75963830947876
Step: 20010, train/loss: 0.0
Step: 20010, train/grad_norm: 9.264836009492683e-09
Step: 20010, train/learning_rate: 2.6189909476670437e-05
Step: 20010, train/epoch: 4.762018203735352
Step: 20020, train/loss: 0.0
Step: 20020, train/grad_norm: 3.3304326052530087e-07
Step: 20020, train/learning_rate: 2.617800964799244e-05
Step: 20020, train/epoch: 4.764398097991943
Step: 20030, train/loss: 0.0
Step: 20030, train/grad_norm: 2.485328707280132e-07
Step: 20030, train/learning_rate: 2.6166111638303846e-05
Step: 20030, train/epoch: 4.766777515411377
Step: 20040, train/loss: 0.0
Step: 20040, train/grad_norm: 5.154283257979841e-07
Step: 20040, train/learning_rate: 2.615421180962585e-05
Step: 20040, train/epoch: 4.769157409667969
Step: 20050, train/loss: 0.0
Step: 20050, train/grad_norm: 3.4866079658968374e-05
Step: 20050, train/learning_rate: 2.6142313799937256e-05
Step: 20050, train/epoch: 4.7715373039245605
Step: 20060, train/loss: 0.0
Step: 20060, train/grad_norm: 0.008097562938928604
Step: 20060, train/learning_rate: 2.613041397125926e-05
Step: 20060, train/epoch: 4.773917198181152
Step: 20070, train/loss: 0.0
Step: 20070, train/grad_norm: 6.789500912418589e-05
Step: 20070, train/learning_rate: 2.6118514142581262e-05
Step: 20070, train/epoch: 4.776297092437744
Step: 20080, train/loss: 0.0
Step: 20080, train/grad_norm: 4.920605647384946e-07
Step: 20080, train/learning_rate: 2.6106616132892668e-05
Step: 20080, train/epoch: 4.778676986694336
Step: 20090, train/loss: 0.1890999972820282
Step: 20090, train/grad_norm: 4.216452964556083e-08
Step: 20090, train/learning_rate: 2.609471630421467e-05
Step: 20090, train/epoch: 4.7810564041137695
Step: 20100, train/loss: 0.0
Step: 20100, train/grad_norm: 0.0011012816103175282
Step: 20100, train/learning_rate: 2.6082818294526078e-05
Step: 20100, train/epoch: 4.783436298370361
Step: 20110, train/loss: 0.0
Step: 20110, train/grad_norm: 0.018232913687825203
Step: 20110, train/learning_rate: 2.607091846584808e-05
Step: 20110, train/epoch: 4.785816192626953
Step: 20120, train/loss: 9.999999747378752e-05
Step: 20120, train/grad_norm: 3.376550739631057e-05
Step: 20120, train/learning_rate: 2.6059018637170084e-05
Step: 20120, train/epoch: 4.788196086883545
Step: 20130, train/loss: 0.0
Step: 20130, train/grad_norm: 2.961268819490215e-06
Step: 20130, train/learning_rate: 2.604712062748149e-05
Step: 20130, train/epoch: 4.790575981140137
Step: 20140, train/loss: 0.0
Step: 20140, train/grad_norm: 1.302139753533993e-05
Step: 20140, train/learning_rate: 2.6035220798803493e-05
Step: 20140, train/epoch: 4.7929558753967285
Step: 20150, train/loss: 0.0
Step: 20150, train/grad_norm: 0.001136892125941813
Step: 20150, train/learning_rate: 2.60233227891149e-05
Step: 20150, train/epoch: 4.79533576965332
Step: 20160, train/loss: 0.0
Step: 20160, train/grad_norm: 2.640710476953245e-07
Step: 20160, train/learning_rate: 2.6011422960436903e-05
Step: 20160, train/epoch: 4.797715187072754
Step: 20170, train/loss: 0.0
Step: 20170, train/grad_norm: 1.290879663429223e-05
Step: 20170, train/learning_rate: 2.5999523131758906e-05
Step: 20170, train/epoch: 4.800095081329346
Step: 20180, train/loss: 0.0
Step: 20180, train/grad_norm: 9.56609146669507e-05
Step: 20180, train/learning_rate: 2.5987625122070312e-05
Step: 20180, train/epoch: 4.8024749755859375
Step: 20190, train/loss: 0.0
Step: 20190, train/grad_norm: 2.3971088012331165e-05
Step: 20190, train/learning_rate: 2.5975725293392316e-05
Step: 20190, train/epoch: 4.804854869842529
Step: 20200, train/loss: 0.0
Step: 20200, train/grad_norm: 1.9130754935758887e-07
Step: 20200, train/learning_rate: 2.5963827283703722e-05
Step: 20200, train/epoch: 4.807234764099121
Step: 20210, train/loss: 0.0
Step: 20210, train/grad_norm: 4.506186996877659e-06
Step: 20210, train/learning_rate: 2.5951927455025725e-05
Step: 20210, train/epoch: 4.809614658355713
Step: 20220, train/loss: 0.10000000149011612
Step: 20220, train/grad_norm: 1.0063708941743243e-05
Step: 20220, train/learning_rate: 2.594002944533713e-05
Step: 20220, train/epoch: 4.8119940757751465
Step: 20230, train/loss: 0.0
Step: 20230, train/grad_norm: 1.6210188391596603e-07
Step: 20230, train/learning_rate: 2.5928129616659135e-05
Step: 20230, train/epoch: 4.814373970031738
Step: 20240, train/loss: 0.0
Step: 20240, train/grad_norm: 4.130381967115682e-06
Step: 20240, train/learning_rate: 2.5916229787981138e-05
Step: 20240, train/epoch: 4.81675386428833
Step: 20250, train/loss: 0.0
Step: 20250, train/grad_norm: 2.6818537662620656e-05
Step: 20250, train/learning_rate: 2.5904331778292544e-05
Step: 20250, train/epoch: 4.819133758544922
Step: 20260, train/loss: 0.0
Step: 20260, train/grad_norm: 2.1506324628717266e-05
Step: 20260, train/learning_rate: 2.5892431949614547e-05
Step: 20260, train/epoch: 4.821513652801514
Step: 20270, train/loss: 0.0
Step: 20270, train/grad_norm: 2.388117366081133e-07
Step: 20270, train/learning_rate: 2.5880533939925954e-05
Step: 20270, train/epoch: 4.8238935470581055
Step: 20280, train/loss: 0.0
Step: 20280, train/grad_norm: 9.264754044124857e-05
Step: 20280, train/learning_rate: 2.5868634111247957e-05
Step: 20280, train/epoch: 4.826273441314697
Step: 20290, train/loss: 0.0
Step: 20290, train/grad_norm: 4.247639481036458e-06
Step: 20290, train/learning_rate: 2.585673428256996e-05
Step: 20290, train/epoch: 4.828652858734131
Step: 20300, train/loss: 0.0
Step: 20300, train/grad_norm: 5.721421985072084e-07
Step: 20300, train/learning_rate: 2.5844836272881366e-05
Step: 20300, train/epoch: 4.831032752990723
Step: 20310, train/loss: 0.0
Step: 20310, train/grad_norm: 8.905287540983409e-05
Step: 20310, train/learning_rate: 2.583293644420337e-05
Step: 20310, train/epoch: 4.8334126472473145
Step: 20320, train/loss: 0.0
Step: 20320, train/grad_norm: 3.38387280862662e-06
Step: 20320, train/learning_rate: 2.5821038434514776e-05
Step: 20320, train/epoch: 4.835792541503906
Step: 20330, train/loss: 0.0
Step: 20330, train/grad_norm: 4.1443079680902883e-05
Step: 20330, train/learning_rate: 2.580913860583678e-05
Step: 20330, train/epoch: 4.838172435760498
Step: 20340, train/loss: 0.0
Step: 20340, train/grad_norm: 9.444157456073299e-08
Step: 20340, train/learning_rate: 2.5797238777158782e-05
Step: 20340, train/epoch: 4.84055233001709
Step: 20350, train/loss: 0.0
Step: 20350, train/grad_norm: 4.1640669223852456e-05
Step: 20350, train/learning_rate: 2.578534076747019e-05
Step: 20350, train/epoch: 4.842931747436523
Step: 20360, train/loss: 0.0
Step: 20360, train/grad_norm: 3.9570764442942163e-07
Step: 20360, train/learning_rate: 2.577344093879219e-05
Step: 20360, train/epoch: 4.845311641693115
Step: 20370, train/loss: 0.0
Step: 20370, train/grad_norm: 6.885803031764226e-06
Step: 20370, train/learning_rate: 2.5761542929103598e-05
Step: 20370, train/epoch: 4.847691535949707
Step: 20380, train/loss: 0.0
Step: 20380, train/grad_norm: 3.758897946681827e-05
Step: 20380, train/learning_rate: 2.57496431004256e-05
Step: 20380, train/epoch: 4.850071430206299
Step: 20390, train/loss: 0.0
Step: 20390, train/grad_norm: 1.5698320567025803e-05
Step: 20390, train/learning_rate: 2.5737743271747604e-05
Step: 20390, train/epoch: 4.852451324462891
Step: 20400, train/loss: 0.0
Step: 20400, train/grad_norm: 1.5748689463634946e-07
Step: 20400, train/learning_rate: 2.572584526205901e-05
Step: 20400, train/epoch: 4.854831218719482
Step: 20410, train/loss: 0.0
Step: 20410, train/grad_norm: 6.346346026475658e-07
Step: 20410, train/learning_rate: 2.5713945433381014e-05
Step: 20410, train/epoch: 4.857210636138916
Step: 20420, train/loss: 0.0
Step: 20420, train/grad_norm: 0.00019486973178572953
Step: 20420, train/learning_rate: 2.570204742369242e-05
Step: 20420, train/epoch: 4.859590530395508
Step: 20430, train/loss: 0.0
Step: 20430, train/grad_norm: 0.00010609927267068997
Step: 20430, train/learning_rate: 2.5690147595014423e-05
Step: 20430, train/epoch: 4.8619704246521
Step: 20440, train/loss: 0.0
Step: 20440, train/grad_norm: 1.3198809938330669e-05
Step: 20440, train/learning_rate: 2.5678247766336426e-05
Step: 20440, train/epoch: 4.864350318908691
Step: 20450, train/loss: 9.999999747378752e-05
Step: 20450, train/grad_norm: 4.374111597371666e-07
Step: 20450, train/learning_rate: 2.5666349756647833e-05
Step: 20450, train/epoch: 4.866730213165283
Step: 20460, train/loss: 0.0
Step: 20460, train/grad_norm: 7.617527444381267e-05
Step: 20460, train/learning_rate: 2.5654449927969836e-05
Step: 20460, train/epoch: 4.869110107421875
Step: 20470, train/loss: 0.0
Step: 20470, train/grad_norm: 1.6364124633128085e-07
Step: 20470, train/learning_rate: 2.5642551918281242e-05
Step: 20470, train/epoch: 4.871490001678467
Step: 20480, train/loss: 0.0
Step: 20480, train/grad_norm: 3.3993825354627916e-07
Step: 20480, train/learning_rate: 2.5630652089603245e-05
Step: 20480, train/epoch: 4.8738694190979
Step: 20490, train/loss: 0.1046999990940094
Step: 20490, train/grad_norm: 5.026083454140462e-05
Step: 20490, train/learning_rate: 2.5618752260925248e-05
Step: 20490, train/epoch: 4.876249313354492
Step: 20500, train/loss: 0.0
Step: 20500, train/grad_norm: 4.408832765534498e-08
Step: 20500, train/learning_rate: 2.5606854251236655e-05
Step: 20500, train/epoch: 4.878629207611084
Step: 20510, train/loss: 0.0
Step: 20510, train/grad_norm: 1.5517234430717508e-07
Step: 20510, train/learning_rate: 2.5594954422558658e-05
Step: 20510, train/epoch: 4.881009101867676
Step: 20520, train/loss: 0.10159999877214432
Step: 20520, train/grad_norm: 4.6852636614858056e-07
Step: 20520, train/learning_rate: 2.5583056412870064e-05
Step: 20520, train/epoch: 4.883388996124268
Step: 20530, train/loss: 0.00019999999494757503
Step: 20530, train/grad_norm: 0.7444496750831604
Step: 20530, train/learning_rate: 2.5571156584192067e-05
Step: 20530, train/epoch: 4.885768890380859
Step: 20540, train/loss: 0.0
Step: 20540, train/grad_norm: 2.8452527089939395e-07
Step: 20540, train/learning_rate: 2.555925675551407e-05
Step: 20540, train/epoch: 4.888148307800293
Step: 20550, train/loss: 0.0
Step: 20550, train/grad_norm: 6.398030564014334e-06
Step: 20550, train/learning_rate: 2.5547358745825477e-05
Step: 20550, train/epoch: 4.890528202056885
Step: 20560, train/loss: 0.0
Step: 20560, train/grad_norm: 1.848138595050841e-06
Step: 20560, train/learning_rate: 2.553545891714748e-05
Step: 20560, train/epoch: 4.892908096313477
Step: 20570, train/loss: 0.0
Step: 20570, train/grad_norm: 1.4096821132625337e-07
Step: 20570, train/learning_rate: 2.5523560907458887e-05
Step: 20570, train/epoch: 4.895287990570068
Step: 20580, train/loss: 0.0
Step: 20580, train/grad_norm: 4.679023732023779e-06
Step: 20580, train/learning_rate: 2.551166107878089e-05
Step: 20580, train/epoch: 4.89766788482666
Step: 20590, train/loss: 0.0
Step: 20590, train/grad_norm: 2.4532273101840474e-08
Step: 20590, train/learning_rate: 2.5499761250102893e-05
Step: 20590, train/epoch: 4.900047779083252
Step: 20600, train/loss: 0.0
Step: 20600, train/grad_norm: 7.1247181949729566e-06
Step: 20600, train/learning_rate: 2.54878632404143e-05
Step: 20600, train/epoch: 4.9024271965026855
Step: 20610, train/loss: 0.0
Step: 20610, train/grad_norm: 3.4331726794789574e-08
Step: 20610, train/learning_rate: 2.5475963411736302e-05
Step: 20610, train/epoch: 4.904807090759277
Step: 20620, train/loss: 0.0
Step: 20620, train/grad_norm: 5.168772432284641e-08
Step: 20620, train/learning_rate: 2.546406540204771e-05
Step: 20620, train/epoch: 4.907186985015869
Step: 20630, train/loss: 0.0
Step: 20630, train/grad_norm: 0.00019049069669563323
Step: 20630, train/learning_rate: 2.545216557336971e-05
Step: 20630, train/epoch: 4.909566879272461
Step: 20640, train/loss: 0.0
Step: 20640, train/grad_norm: 0.0002017559454543516
Step: 20640, train/learning_rate: 2.5440265744691715e-05
Step: 20640, train/epoch: 4.911946773529053
Step: 20650, train/loss: 0.0
Step: 20650, train/grad_norm: 6.805813086430135e-07
Step: 20650, train/learning_rate: 2.542836773500312e-05
Step: 20650, train/epoch: 4.9143266677856445
Step: 20660, train/loss: 0.0
Step: 20660, train/grad_norm: 3.647872290457599e-06
Step: 20660, train/learning_rate: 2.5416467906325124e-05
Step: 20660, train/epoch: 4.916706562042236
Step: 20670, train/loss: 0.0
Step: 20670, train/grad_norm: 1.0003070201491937e-05
Step: 20670, train/learning_rate: 2.540456989663653e-05
Step: 20670, train/epoch: 4.91908597946167
Step: 20680, train/loss: 0.027300000190734863
Step: 20680, train/grad_norm: 1.42342670983453e-07
Step: 20680, train/learning_rate: 2.5392670067958534e-05
Step: 20680, train/epoch: 4.921465873718262
Step: 20690, train/loss: 0.0
Step: 20690, train/grad_norm: 3.434935536006378e-07
Step: 20690, train/learning_rate: 2.5380770239280537e-05
Step: 20690, train/epoch: 4.9238457679748535
Step: 20700, train/loss: 0.0
Step: 20700, train/grad_norm: 1.5895606338744983e-05
Step: 20700, train/learning_rate: 2.5368872229591943e-05
Step: 20700, train/epoch: 4.926225662231445
Step: 20710, train/loss: 0.030300000682473183
Step: 20710, train/grad_norm: 0.005948379170149565
Step: 20710, train/learning_rate: 2.5356972400913946e-05
Step: 20710, train/epoch: 4.928605556488037
Step: 20720, train/loss: 0.0
Step: 20720, train/grad_norm: 7.038616445242951e-07
Step: 20720, train/learning_rate: 2.5345074391225353e-05
Step: 20720, train/epoch: 4.930985450744629
Step: 20730, train/loss: 0.0
Step: 20730, train/grad_norm: 4.530280421022326e-06
Step: 20730, train/learning_rate: 2.5333174562547356e-05
Step: 20730, train/epoch: 4.9333648681640625
Step: 20740, train/loss: 0.0
Step: 20740, train/grad_norm: 2.62866415141616e-05
Step: 20740, train/learning_rate: 2.532127473386936e-05
Step: 20740, train/epoch: 4.935744762420654
Step: 20750, train/loss: 0.0
Step: 20750, train/grad_norm: 3.7073502880957676e-06
Step: 20750, train/learning_rate: 2.5309376724180765e-05
Step: 20750, train/epoch: 4.938124656677246
Step: 20760, train/loss: 0.0
Step: 20760, train/grad_norm: 0.00019122083904221654
Step: 20760, train/learning_rate: 2.529747689550277e-05
Step: 20760, train/epoch: 4.940504550933838
Step: 20770, train/loss: 9.999999747378752e-05
Step: 20770, train/grad_norm: 7.3994392550957855e-06
Step: 20770, train/learning_rate: 2.5285578885814175e-05
Step: 20770, train/epoch: 4.94288444519043
Step: 20780, train/loss: 0.0
Step: 20780, train/grad_norm: 0.0012753254268318415
Step: 20780, train/learning_rate: 2.5273679057136178e-05
Step: 20780, train/epoch: 4.9452643394470215
Step: 20790, train/loss: 0.0
Step: 20790, train/grad_norm: 2.899069841078017e-05
Step: 20790, train/learning_rate: 2.526177922845818e-05
Step: 20790, train/epoch: 4.947643756866455
Step: 20800, train/loss: 0.0
Step: 20800, train/grad_norm: 4.1639744097210496e-08
Step: 20800, train/learning_rate: 2.5249881218769588e-05
Step: 20800, train/epoch: 4.950023651123047
Step: 20810, train/loss: 0.0
Step: 20810, train/grad_norm: 6.006727744534146e-06
Step: 20810, train/learning_rate: 2.523798139009159e-05
Step: 20810, train/epoch: 4.952403545379639
Step: 20820, train/loss: 0.0
Step: 20820, train/grad_norm: 4.61509007436689e-05
Step: 20820, train/learning_rate: 2.5226083380402997e-05
Step: 20820, train/epoch: 4.9547834396362305
Step: 20830, train/loss: 0.0
Step: 20830, train/grad_norm: 1.1228472658331157e-06
Step: 20830, train/learning_rate: 2.5214183551725e-05
Step: 20830, train/epoch: 4.957163333892822
Step: 20840, train/loss: 0.01889999955892563
Step: 20840, train/grad_norm: 0.0002946376916952431
Step: 20840, train/learning_rate: 2.5202283723047003e-05
Step: 20840, train/epoch: 4.959543228149414
Step: 20850, train/loss: 0.1859000027179718
Step: 20850, train/grad_norm: 1.9300703570479527e-05
Step: 20850, train/learning_rate: 2.519038571335841e-05
Step: 20850, train/epoch: 4.961923122406006
Step: 20860, train/loss: 0.0
Step: 20860, train/grad_norm: 9.760641842149198e-05
Step: 20860, train/learning_rate: 2.5178485884680413e-05
Step: 20860, train/epoch: 4.9643025398254395
Step: 20870, train/loss: 0.0
Step: 20870, train/grad_norm: 0.0008317522588185966
Step: 20870, train/learning_rate: 2.516658787499182e-05
Step: 20870, train/epoch: 4.966682434082031
Step: 20880, train/loss: 0.0
Step: 20880, train/grad_norm: 7.455874583683908e-05
Step: 20880, train/learning_rate: 2.5154688046313822e-05
Step: 20880, train/epoch: 4.969062328338623
Step: 20890, train/loss: 0.00930000003427267
Step: 20890, train/grad_norm: 1.296518439630745e-05
Step: 20890, train/learning_rate: 2.514279003662523e-05
Step: 20890, train/epoch: 4.971442222595215
Step: 20900, train/loss: 0.0
Step: 20900, train/grad_norm: 8.875377534423023e-05
Step: 20900, train/learning_rate: 2.5130890207947232e-05
Step: 20900, train/epoch: 4.973822116851807
Step: 20910, train/loss: 0.0
Step: 20910, train/grad_norm: 0.002101497258991003
Step: 20910, train/learning_rate: 2.5118990379269235e-05
Step: 20910, train/epoch: 4.976202011108398
Step: 20920, train/loss: 9.999999747378752e-05
Step: 20920, train/grad_norm: 0.0006102995248511434
Step: 20920, train/learning_rate: 2.510709236958064e-05
Step: 20920, train/epoch: 4.978581428527832
Step: 20930, train/loss: 0.0
Step: 20930, train/grad_norm: 0.00016110132855828851
Step: 20930, train/learning_rate: 2.5095192540902644e-05
Step: 20930, train/epoch: 4.980961322784424
Step: 20940, train/loss: 0.0
Step: 20940, train/grad_norm: 0.0002804816758725792
Step: 20940, train/learning_rate: 2.508329453121405e-05
Step: 20940, train/epoch: 4.983341217041016
Step: 20950, train/loss: 0.0
Step: 20950, train/grad_norm: 0.000459051167126745
Step: 20950, train/learning_rate: 2.5071394702536054e-05
Step: 20950, train/epoch: 4.985721111297607
Step: 20960, train/loss: 0.1039000004529953
Step: 20960, train/grad_norm: 8.560570131521672e-05
Step: 20960, train/learning_rate: 2.5059494873858057e-05
Step: 20960, train/epoch: 4.988101005554199
Step: 20970, train/loss: 0.07519999891519547
Step: 20970, train/grad_norm: 0.00036351801827549934
Step: 20970, train/learning_rate: 2.5047596864169464e-05
Step: 20970, train/epoch: 4.990480899810791
Step: 20980, train/loss: 9.999999747378752e-05
Step: 20980, train/grad_norm: 0.001776779186911881
Step: 20980, train/learning_rate: 2.5035697035491467e-05
Step: 20980, train/epoch: 4.992860317230225
Step: 20990, train/loss: 0.0
Step: 20990, train/grad_norm: 7.404575444525108e-05
Step: 20990, train/learning_rate: 2.5023799025802873e-05
Step: 20990, train/epoch: 4.995240211486816
Step: 21000, train/loss: 0.0
Step: 21000, train/grad_norm: 0.0002306416427018121
Step: 21000, train/learning_rate: 2.5011899197124876e-05
Step: 21000, train/epoch: 4.997620105743408
Step: 21010, train/loss: 0.0
Step: 21010, train/grad_norm: 3.082653620367637e-06
Step: 21010, train/learning_rate: 2.499999936844688e-05
Step: 21010, train/epoch: 5.0
Step: 21010, eval/loss: 0.033993616700172424
Step: 21010, eval/accuracy: 0.9943079352378845
Step: 21010, eval/f1: 0.9940046668052673
Step: 21010, eval/runtime: 735.6925048828125
Step: 21010, eval/samples_per_second: 9.791000366210938
Step: 21010, eval/steps_per_second: 1.225000023841858
Step: 21010, train/epoch: 5.0
Step: 21020, train/loss: 0.0
Step: 21020, train/grad_norm: 0.000361861806595698
Step: 21020, train/learning_rate: 2.4988101358758286e-05
Step: 21020, train/epoch: 5.002379894256592
Step: 21030, train/loss: 0.0
Step: 21030, train/grad_norm: 3.291076427558437e-05
Step: 21030, train/learning_rate: 2.497620153008029e-05
Step: 21030, train/epoch: 5.004759788513184
Step: 21040, train/loss: 0.0
Step: 21040, train/grad_norm: 7.981097587617114e-05
Step: 21040, train/learning_rate: 2.4964303520391695e-05
Step: 21040, train/epoch: 5.007139682769775
Step: 21050, train/loss: 0.0
Step: 21050, train/grad_norm: 1.220467129314784e-05
Step: 21050, train/learning_rate: 2.4952403691713698e-05
Step: 21050, train/epoch: 5.009519100189209
Step: 21060, train/loss: 0.00019999999494757503
Step: 21060, train/grad_norm: 7.60446300773765e-06
Step: 21060, train/learning_rate: 2.49405038630357e-05
Step: 21060, train/epoch: 5.011898994445801
Step: 21070, train/loss: 0.0
Step: 21070, train/grad_norm: 6.06404341851885e-07
Step: 21070, train/learning_rate: 2.4928605853347108e-05
Step: 21070, train/epoch: 5.014278888702393
Step: 21080, train/loss: 0.0
Step: 21080, train/grad_norm: 4.7176006773952395e-05
Step: 21080, train/learning_rate: 2.491670602466911e-05
Step: 21080, train/epoch: 5.016658782958984
Step: 21090, train/loss: 0.0
Step: 21090, train/grad_norm: 3.3805893053795444e-06
Step: 21090, train/learning_rate: 2.4904808014980517e-05
Step: 21090, train/epoch: 5.019038677215576
Step: 21100, train/loss: 0.0008999999845400453
Step: 21100, train/grad_norm: 3.868949818297551e-07
Step: 21100, train/learning_rate: 2.489290818630252e-05
Step: 21100, train/epoch: 5.021418571472168
Step: 21110, train/loss: 0.0
Step: 21110, train/grad_norm: 2.392707028775476e-05
Step: 21110, train/learning_rate: 2.4881008357624523e-05
Step: 21110, train/epoch: 5.023797988891602
Step: 21120, train/loss: 0.0
Step: 21120, train/grad_norm: 1.965829499184224e-09
Step: 21120, train/learning_rate: 2.486911034793593e-05
Step: 21120, train/epoch: 5.026177883148193
Step: 21130, train/loss: 0.0
Step: 21130, train/grad_norm: 1.062608816937427e-06
Step: 21130, train/learning_rate: 2.4857210519257933e-05
Step: 21130, train/epoch: 5.028557777404785
Step: 21140, train/loss: 0.0
Step: 21140, train/grad_norm: 2.1691576534976775e-07
Step: 21140, train/learning_rate: 2.484531250956934e-05
Step: 21140, train/epoch: 5.030937671661377
Step: 21150, train/loss: 0.0
Step: 21150, train/grad_norm: 8.94309050636366e-06
Step: 21150, train/learning_rate: 2.4833412680891342e-05
Step: 21150, train/epoch: 5.033317565917969
Step: 21160, train/loss: 0.0
Step: 21160, train/grad_norm: 1.7110311091528274e-05
Step: 21160, train/learning_rate: 2.4821512852213345e-05
Step: 21160, train/epoch: 5.0356974601745605
Step: 21170, train/loss: 0.0
Step: 21170, train/grad_norm: 1.4022781442690757e-06
Step: 21170, train/learning_rate: 2.4809614842524752e-05
Step: 21170, train/epoch: 5.038076877593994
Step: 21180, train/loss: 0.0
Step: 21180, train/grad_norm: 1.1637672287179157e-05
Step: 21180, train/learning_rate: 2.4797715013846755e-05
Step: 21180, train/epoch: 5.040456771850586
Step: 21190, train/loss: 0.15160000324249268
Step: 21190, train/grad_norm: 9.740698914129098e-09
Step: 21190, train/learning_rate: 2.478581700415816e-05
Step: 21190, train/epoch: 5.042836666107178
Step: 21200, train/loss: 0.0
Step: 21200, train/grad_norm: 1.2640911336347926e-05
Step: 21200, train/learning_rate: 2.4773917175480165e-05
Step: 21200, train/epoch: 5.0452165603637695
Step: 21210, train/loss: 0.0
Step: 21210, train/grad_norm: 6.41547512714169e-06
Step: 21210, train/learning_rate: 2.4762017346802168e-05
Step: 21210, train/epoch: 5.047596454620361
Step: 21220, train/loss: 0.0
Step: 21220, train/grad_norm: 1.1483433581815916e-06
Step: 21220, train/learning_rate: 2.4750119337113574e-05
Step: 21220, train/epoch: 5.049976348876953
Step: 21230, train/loss: 9.999999747378752e-05
Step: 21230, train/grad_norm: 1.5909554349491373e-05
Step: 21230, train/learning_rate: 2.4738219508435577e-05
Step: 21230, train/epoch: 5.052356243133545
Step: 21240, train/loss: 0.0
Step: 21240, train/grad_norm: 8.644348525876921e-08
Step: 21240, train/learning_rate: 2.4726321498746984e-05
Step: 21240, train/epoch: 5.0547356605529785
Step: 21250, train/loss: 0.0
Step: 21250, train/grad_norm: 4.2319035742366395e-07
Step: 21250, train/learning_rate: 2.4714421670068987e-05
Step: 21250, train/epoch: 5.05711555480957
Step: 21260, train/loss: 0.0
Step: 21260, train/grad_norm: 4.2778624447237235e-06
Step: 21260, train/learning_rate: 2.470252184139099e-05
Step: 21260, train/epoch: 5.059495449066162
Step: 21270, train/loss: 0.0
Step: 21270, train/grad_norm: 4.921920208289521e-06
Step: 21270, train/learning_rate: 2.4690623831702396e-05
Step: 21270, train/epoch: 5.061875343322754
Step: 21280, train/loss: 0.0
Step: 21280, train/grad_norm: 0.0009290222660638392
Step: 21280, train/learning_rate: 2.46787240030244e-05
Step: 21280, train/epoch: 5.064255237579346
Step: 21290, train/loss: 0.0
Step: 21290, train/grad_norm: 0.0001737968996167183
Step: 21290, train/learning_rate: 2.4666825993335806e-05
Step: 21290, train/epoch: 5.0666351318359375
Step: 21300, train/loss: 0.0
Step: 21300, train/grad_norm: 1.3943005114924745e-06
Step: 21300, train/learning_rate: 2.465492616465781e-05
Step: 21300, train/epoch: 5.069014549255371
Step: 21310, train/loss: 0.0
Step: 21310, train/grad_norm: 5.776658440481697e-07
Step: 21310, train/learning_rate: 2.4643026335979812e-05
Step: 21310, train/epoch: 5.071394443511963
Step: 21320, train/loss: 0.0
Step: 21320, train/grad_norm: 6.024078658128929e-08
Step: 21320, train/learning_rate: 2.463112832629122e-05
Step: 21320, train/epoch: 5.073774337768555
Step: 21330, train/loss: 0.0
Step: 21330, train/grad_norm: 1.099517885450041e-05
Step: 21330, train/learning_rate: 2.461922849761322e-05
Step: 21330, train/epoch: 5.0761542320251465
Step: 21340, train/loss: 0.0
Step: 21340, train/grad_norm: 8.20660625322489e-06
Step: 21340, train/learning_rate: 2.4607330487924628e-05
Step: 21340, train/epoch: 5.078534126281738
Step: 21350, train/loss: 0.0
Step: 21350, train/grad_norm: 2.7051406505052e-05
Step: 21350, train/learning_rate: 2.459543065924663e-05
Step: 21350, train/epoch: 5.08091402053833
Step: 21360, train/loss: 0.0
Step: 21360, train/grad_norm: 3.161100323723076e-07
Step: 21360, train/learning_rate: 2.4583530830568634e-05
Step: 21360, train/epoch: 5.083293437957764
Step: 21370, train/loss: 0.0
Step: 21370, train/grad_norm: 1.1884900459335768e-06
Step: 21370, train/learning_rate: 2.457163282088004e-05
Step: 21370, train/epoch: 5.0856733322143555
Step: 21380, train/loss: 0.0
Step: 21380, train/grad_norm: 9.432860679225996e-07
Step: 21380, train/learning_rate: 2.4559732992202044e-05
Step: 21380, train/epoch: 5.088053226470947
Step: 21390, train/loss: 0.0
Step: 21390, train/grad_norm: 8.767389772401657e-06
Step: 21390, train/learning_rate: 2.454783498251345e-05
Step: 21390, train/epoch: 5.090433120727539
Step: 21400, train/loss: 0.0
Step: 21400, train/grad_norm: 1.1546507039383869e-06
Step: 21400, train/learning_rate: 2.4535935153835453e-05
Step: 21400, train/epoch: 5.092813014984131
Step: 21410, train/loss: 0.0
Step: 21410, train/grad_norm: 0.001742269261740148
Step: 21410, train/learning_rate: 2.4524035325157456e-05
Step: 21410, train/epoch: 5.095192909240723
Step: 21420, train/loss: 0.0
Step: 21420, train/grad_norm: 2.2394341669951245e-07
Step: 21420, train/learning_rate: 2.4512137315468863e-05
Step: 21420, train/epoch: 5.0975728034973145
Step: 21430, train/loss: 0.0
Step: 21430, train/grad_norm: 8.81971664057346e-06
Step: 21430, train/learning_rate: 2.4500237486790866e-05
Step: 21430, train/epoch: 5.099952220916748
Step: 21440, train/loss: 0.0
Step: 21440, train/grad_norm: 1.5736669922716828e-07
Step: 21440, train/learning_rate: 2.4488339477102272e-05
Step: 21440, train/epoch: 5.10233211517334
Step: 21450, train/loss: 0.0
Step: 21450, train/grad_norm: 2.4407745513599366e-05
Step: 21450, train/learning_rate: 2.4476439648424275e-05
Step: 21450, train/epoch: 5.104712009429932
Step: 21460, train/loss: 0.0
Step: 21460, train/grad_norm: 7.251147053466411e-06
Step: 21460, train/learning_rate: 2.4464539819746278e-05
Step: 21460, train/epoch: 5.107091903686523
Step: 21470, train/loss: 0.0
Step: 21470, train/grad_norm: 1.296123446081765e-05
Step: 21470, train/learning_rate: 2.4452641810057685e-05
Step: 21470, train/epoch: 5.109471797943115
Step: 21480, train/loss: 0.0
Step: 21480, train/grad_norm: 6.481233754129789e-07
Step: 21480, train/learning_rate: 2.4440741981379688e-05
Step: 21480, train/epoch: 5.111851692199707
Step: 21490, train/loss: 0.0
Step: 21490, train/grad_norm: 7.137884495023172e-06
Step: 21490, train/learning_rate: 2.4428843971691094e-05
Step: 21490, train/epoch: 5.114231109619141
Step: 21500, train/loss: 0.0
Step: 21500, train/grad_norm: 1.6977148575847423e-08
Step: 21500, train/learning_rate: 2.4416944143013097e-05
Step: 21500, train/epoch: 5.116611003875732
Step: 21510, train/loss: 0.0
Step: 21510, train/grad_norm: 2.759298070031946e-07
Step: 21510, train/learning_rate: 2.44050443143351e-05
Step: 21510, train/epoch: 5.118990898132324
Step: 21520, train/loss: 0.0
Step: 21520, train/grad_norm: 2.966957026728778e-07
Step: 21520, train/learning_rate: 2.4393146304646507e-05
Step: 21520, train/epoch: 5.121370792388916
Step: 21530, train/loss: 0.0
Step: 21530, train/grad_norm: 6.889810174470767e-05
Step: 21530, train/learning_rate: 2.438124647596851e-05
Step: 21530, train/epoch: 5.123750686645508
Step: 21540, train/loss: 0.0
Step: 21540, train/grad_norm: 7.73761985328747e-06
Step: 21540, train/learning_rate: 2.4369348466279916e-05
Step: 21540, train/epoch: 5.1261305809021
Step: 21550, train/loss: 0.0
Step: 21550, train/grad_norm: 5.017815055907704e-06
Step: 21550, train/learning_rate: 2.435744863760192e-05
Step: 21550, train/epoch: 5.128509998321533
Step: 21560, train/loss: 0.0
Step: 21560, train/grad_norm: 9.23285824683262e-06
Step: 21560, train/learning_rate: 2.4345550627913326e-05
Step: 21560, train/epoch: 5.130889892578125
Step: 21570, train/loss: 0.0
Step: 21570, train/grad_norm: 9.228424460161477e-06
Step: 21570, train/learning_rate: 2.433365079923533e-05
Step: 21570, train/epoch: 5.133269786834717
Step: 21580, train/loss: 0.0
Step: 21580, train/grad_norm: 4.1393145693291444e-06
Step: 21580, train/learning_rate: 2.4321750970557332e-05
Step: 21580, train/epoch: 5.135649681091309
Step: 21590, train/loss: 0.0
Step: 21590, train/grad_norm: 2.6551788323558867e-05
Step: 21590, train/learning_rate: 2.430985296086874e-05
Step: 21590, train/epoch: 5.1380295753479
Step: 21600, train/loss: 0.0
Step: 21600, train/grad_norm: 9.658618182584178e-06
Step: 21600, train/learning_rate: 2.429795313219074e-05
Step: 21600, train/epoch: 5.140409469604492
Step: 21610, train/loss: 0.15000000596046448
Step: 21610, train/grad_norm: 5.291171305543685e-07
Step: 21610, train/learning_rate: 2.4286055122502148e-05
Step: 21610, train/epoch: 5.142789363861084
Step: 21620, train/loss: 0.0003000000142492354
Step: 21620, train/grad_norm: 0.02729383483529091
Step: 21620, train/learning_rate: 2.427415529382415e-05
Step: 21620, train/epoch: 5.145168781280518
Step: 21630, train/loss: 0.0
Step: 21630, train/grad_norm: 2.983515742016607e-06
Step: 21630, train/learning_rate: 2.4262255465146154e-05
Step: 21630, train/epoch: 5.147548675537109
Step: 21640, train/loss: 0.0
Step: 21640, train/grad_norm: 5.0682098517995655e-09
Step: 21640, train/learning_rate: 2.425035745545756e-05
Step: 21640, train/epoch: 5.149928569793701
Step: 21650, train/loss: 0.0
Step: 21650, train/grad_norm: 4.43302297981063e-07
Step: 21650, train/learning_rate: 2.4238457626779564e-05
Step: 21650, train/epoch: 5.152308464050293
Step: 21660, train/loss: 0.0
Step: 21660, train/grad_norm: 0.0008161939913406968
Step: 21660, train/learning_rate: 2.422655961709097e-05
Step: 21660, train/epoch: 5.154688358306885
Step: 21670, train/loss: 0.0
Step: 21670, train/grad_norm: 5.839946304320165e-09
Step: 21670, train/learning_rate: 2.4214659788412973e-05
Step: 21670, train/epoch: 5.157068252563477
Step: 21680, train/loss: 0.0
Step: 21680, train/grad_norm: 1.2759417586494237e-05
Step: 21680, train/learning_rate: 2.4202759959734976e-05
Step: 21680, train/epoch: 5.15944766998291
Step: 21690, train/loss: 0.0
Step: 21690, train/grad_norm: 4.61596209788695e-06
Step: 21690, train/learning_rate: 2.4190861950046383e-05
Step: 21690, train/epoch: 5.161827564239502
Step: 21700, train/loss: 0.0
Step: 21700, train/grad_norm: 2.9019106477790046e-06
Step: 21700, train/learning_rate: 2.4178962121368386e-05
Step: 21700, train/epoch: 5.164207458496094
Step: 21710, train/loss: 0.0
Step: 21710, train/grad_norm: 1.0925656170002185e-05
Step: 21710, train/learning_rate: 2.4167064111679792e-05
Step: 21710, train/epoch: 5.1665873527526855
Step: 21720, train/loss: 0.0
Step: 21720, train/grad_norm: 4.002180276074796e-07
Step: 21720, train/learning_rate: 2.4155164283001795e-05
Step: 21720, train/epoch: 5.168967247009277
Step: 21730, train/loss: 0.0
Step: 21730, train/grad_norm: 1.3168257737561362e-06
Step: 21730, train/learning_rate: 2.41432644543238e-05
Step: 21730, train/epoch: 5.171347141265869
Step: 21740, train/loss: 0.0
Step: 21740, train/grad_norm: 1.5507697526118136e-07
Step: 21740, train/learning_rate: 2.4131366444635205e-05
Step: 21740, train/epoch: 5.173726558685303
Step: 21750, train/loss: 0.0
Step: 21750, train/grad_norm: 7.4076060840866376e-09
Step: 21750, train/learning_rate: 2.4119466615957208e-05
Step: 21750, train/epoch: 5.1761064529418945
Step: 21760, train/loss: 0.0
Step: 21760, train/grad_norm: 3.998045006170514e-09
Step: 21760, train/learning_rate: 2.4107568606268615e-05
Step: 21760, train/epoch: 5.178486347198486
Step: 21770, train/loss: 0.037300001829862595
Step: 21770, train/grad_norm: 1.8373638610569287e-08
Step: 21770, train/learning_rate: 2.4095668777590618e-05
Step: 21770, train/epoch: 5.180866241455078
Step: 21780, train/loss: 0.0
Step: 21780, train/grad_norm: 0.00046748813474550843
Step: 21780, train/learning_rate: 2.408376894891262e-05
Step: 21780, train/epoch: 5.18324613571167
Step: 21790, train/loss: 0.0
Step: 21790, train/grad_norm: 0.00024315995688084513
Step: 21790, train/learning_rate: 2.4071870939224027e-05
Step: 21790, train/epoch: 5.185626029968262
Step: 21800, train/loss: 0.0
Step: 21800, train/grad_norm: 0.0014364778762683272
Step: 21800, train/learning_rate: 2.405997111054603e-05
Step: 21800, train/epoch: 5.1880059242248535
Step: 21810, train/loss: 0.0
Step: 21810, train/grad_norm: 4.1323291952721775e-06
Step: 21810, train/learning_rate: 2.4048073100857437e-05
Step: 21810, train/epoch: 5.190385341644287
Step: 21820, train/loss: 0.0
Step: 21820, train/grad_norm: 3.844076672976371e-06
Step: 21820, train/learning_rate: 2.403617327217944e-05
Step: 21820, train/epoch: 5.192765235900879
Step: 21830, train/loss: 0.14219999313354492
Step: 21830, train/grad_norm: 6.9039333538967185e-06
Step: 21830, train/learning_rate: 2.4024273443501443e-05
Step: 21830, train/epoch: 5.195145130157471
Step: 21840, train/loss: 0.0
Step: 21840, train/grad_norm: 2.268395110149868e-05
Step: 21840, train/learning_rate: 2.401237543381285e-05
Step: 21840, train/epoch: 5.1975250244140625
Step: 21850, train/loss: 0.0
Step: 21850, train/grad_norm: 3.294934504083358e-05
Step: 21850, train/learning_rate: 2.4000475605134852e-05
Step: 21850, train/epoch: 5.199904918670654
Step: 21860, train/loss: 0.0
Step: 21860, train/grad_norm: 0.0011110863415524364
Step: 21860, train/learning_rate: 2.398857759544626e-05
Step: 21860, train/epoch: 5.202284812927246
Step: 21870, train/loss: 0.0
Step: 21870, train/grad_norm: 0.0002975940005853772
Step: 21870, train/learning_rate: 2.3976677766768262e-05
Step: 21870, train/epoch: 5.20466423034668
Step: 21880, train/loss: 0.0
Step: 21880, train/grad_norm: 5.172970531930332e-07
Step: 21880, train/learning_rate: 2.3964777938090265e-05
Step: 21880, train/epoch: 5.2070441246032715
Step: 21890, train/loss: 0.0
Step: 21890, train/grad_norm: 0.001627497375011444
Step: 21890, train/learning_rate: 2.395287992840167e-05
Step: 21890, train/epoch: 5.209424018859863
Step: 21900, train/loss: 0.0
Step: 21900, train/grad_norm: 0.00033646574593149126
Step: 21900, train/learning_rate: 2.3940980099723674e-05
Step: 21900, train/epoch: 5.211803913116455
Step: 21910, train/loss: 0.0
Step: 21910, train/grad_norm: 9.602064210412209e-07
Step: 21910, train/learning_rate: 2.392908209003508e-05
Step: 21910, train/epoch: 5.214183807373047
Step: 21920, train/loss: 0.0
Step: 21920, train/grad_norm: 6.153302092570812e-05
Step: 21920, train/learning_rate: 2.3917182261357084e-05
Step: 21920, train/epoch: 5.216563701629639
Step: 21930, train/loss: 0.0
Step: 21930, train/grad_norm: 5.3519273933488876e-05
Step: 21930, train/learning_rate: 2.3905282432679087e-05
Step: 21930, train/epoch: 5.2189435958862305
Step: 21940, train/loss: 0.0
Step: 21940, train/grad_norm: 0.00022480558254756033
Step: 21940, train/learning_rate: 2.3893384422990493e-05
Step: 21940, train/epoch: 5.221323013305664
Step: 21950, train/loss: 0.0
Step: 21950, train/grad_norm: 0.003290317952632904
Step: 21950, train/learning_rate: 2.3881484594312496e-05
Step: 21950, train/epoch: 5.223702907562256
Step: 21960, train/loss: 0.0
Step: 21960, train/grad_norm: 3.876323171425611e-05
Step: 21960, train/learning_rate: 2.3869586584623903e-05
Step: 21960, train/epoch: 5.226082801818848
Step: 21970, train/loss: 0.0
Step: 21970, train/grad_norm: 0.00018311492749489844
Step: 21970, train/learning_rate: 2.3857686755945906e-05
Step: 21970, train/epoch: 5.2284626960754395
Step: 21980, train/loss: 0.0
Step: 21980, train/grad_norm: 0.0002315108140464872
Step: 21980, train/learning_rate: 2.384578692726791e-05
Step: 21980, train/epoch: 5.230842590332031
Step: 21990, train/loss: 0.0
Step: 21990, train/grad_norm: 4.724485734186601e-06
Step: 21990, train/learning_rate: 2.3833888917579316e-05
Step: 21990, train/epoch: 5.233222484588623
Step: 22000, train/loss: 9.999999747378752e-05
Step: 22000, train/grad_norm: 6.388307610905031e-06
Step: 22000, train/learning_rate: 2.382198908890132e-05
Step: 22000, train/epoch: 5.235601902008057
Step: 22010, train/loss: 0.0
Step: 22010, train/grad_norm: 1.386889834975591e-05
Step: 22010, train/learning_rate: 2.3810091079212725e-05
Step: 22010, train/epoch: 5.237981796264648
Step: 22020, train/loss: 0.0
Step: 22020, train/grad_norm: 3.5113821184040717e-08
Step: 22020, train/learning_rate: 2.3798191250534728e-05
Step: 22020, train/epoch: 5.24036169052124
Step: 22030, train/loss: 0.0
Step: 22030, train/grad_norm: 1.195118386476679e-07
Step: 22030, train/learning_rate: 2.378629142185673e-05
Step: 22030, train/epoch: 5.242741584777832
Step: 22040, train/loss: 0.0
Step: 22040, train/grad_norm: 1.0729986144042414e-07
Step: 22040, train/learning_rate: 2.3774393412168138e-05
Step: 22040, train/epoch: 5.245121479034424
Step: 22050, train/loss: 0.0
Step: 22050, train/grad_norm: 3.681446401060384e-07
Step: 22050, train/learning_rate: 2.376249358349014e-05
Step: 22050, train/epoch: 5.247501373291016
Step: 22060, train/loss: 0.0
Step: 22060, train/grad_norm: 4.753223947773222e-08
Step: 22060, train/learning_rate: 2.3750595573801547e-05
Step: 22060, train/epoch: 5.249880790710449
Step: 22070, train/loss: 0.0
Step: 22070, train/grad_norm: 1.5639888317764417e-07
Step: 22070, train/learning_rate: 2.373869574512355e-05
Step: 22070, train/epoch: 5.252260684967041
Step: 22080, train/loss: 0.0
Step: 22080, train/grad_norm: 4.451699169294443e-07
Step: 22080, train/learning_rate: 2.3726795916445553e-05
Step: 22080, train/epoch: 5.254640579223633
Step: 22090, train/loss: 0.0
Step: 22090, train/grad_norm: 6.375837102723381e-08
Step: 22090, train/learning_rate: 2.371489790675696e-05
Step: 22090, train/epoch: 5.257020473480225
Step: 22100, train/loss: 0.0
Step: 22100, train/grad_norm: 4.618717412085971e-06
Step: 22100, train/learning_rate: 2.3702998078078963e-05
Step: 22100, train/epoch: 5.259400367736816
Step: 22110, train/loss: 0.0
Step: 22110, train/grad_norm: 1.0449750789121026e-06
Step: 22110, train/learning_rate: 2.369110006839037e-05
Step: 22110, train/epoch: 5.261780261993408
Step: 22120, train/loss: 0.0
Step: 22120, train/grad_norm: 5.793087254346574e-08
Step: 22120, train/learning_rate: 2.3679200239712372e-05
Step: 22120, train/epoch: 5.26416015625
Step: 22130, train/loss: 0.0
Step: 22130, train/grad_norm: 0.003240716876462102
Step: 22130, train/learning_rate: 2.3667300411034375e-05
Step: 22130, train/epoch: 5.266539573669434
Step: 22140, train/loss: 0.0
Step: 22140, train/grad_norm: 9.593737360091836e-08
Step: 22140, train/learning_rate: 2.3655402401345782e-05
Step: 22140, train/epoch: 5.268919467926025
Step: 22150, train/loss: 0.0
Step: 22150, train/grad_norm: 1.3419803508440964e-05
Step: 22150, train/learning_rate: 2.3643502572667785e-05
Step: 22150, train/epoch: 5.271299362182617
Step: 22160, train/loss: 0.0
Step: 22160, train/grad_norm: 4.07847863925781e-07
Step: 22160, train/learning_rate: 2.363160456297919e-05
Step: 22160, train/epoch: 5.273679256439209
Step: 22170, train/loss: 0.0
Step: 22170, train/grad_norm: 7.65752304232592e-07
Step: 22170, train/learning_rate: 2.3619704734301195e-05
Step: 22170, train/epoch: 5.276059150695801
Step: 22180, train/loss: 0.0
Step: 22180, train/grad_norm: 9.679719369160011e-06
Step: 22180, train/learning_rate: 2.3607804905623198e-05
Step: 22180, train/epoch: 5.278439044952393
Step: 22190, train/loss: 0.0
Step: 22190, train/grad_norm: 1.5725801858934574e-05
Step: 22190, train/learning_rate: 2.3595906895934604e-05
Step: 22190, train/epoch: 5.280818462371826
Step: 22200, train/loss: 0.0
Step: 22200, train/grad_norm: 4.368958798295353e-06
Step: 22200, train/learning_rate: 2.3584007067256607e-05
Step: 22200, train/epoch: 5.283198356628418
Step: 22210, train/loss: 0.0
Step: 22210, train/grad_norm: 7.7456988378799e-08
Step: 22210, train/learning_rate: 2.3572109057568014e-05
Step: 22210, train/epoch: 5.28557825088501
Step: 22220, train/loss: 0.0
Step: 22220, train/grad_norm: 5.569455083787034e-07
Step: 22220, train/learning_rate: 2.3560209228890017e-05
Step: 22220, train/epoch: 5.287958145141602
Step: 22230, train/loss: 0.0
Step: 22230, train/grad_norm: 1.6721201973268762e-05
Step: 22230, train/learning_rate: 2.3548311219201423e-05
Step: 22230, train/epoch: 5.290338039398193
Step: 22240, train/loss: 0.0
Step: 22240, train/grad_norm: 0.0007851189584471285
Step: 22240, train/learning_rate: 2.3536411390523426e-05
Step: 22240, train/epoch: 5.292717933654785
Step: 22250, train/loss: 0.0
Step: 22250, train/grad_norm: 6.0211906571794316e-09
Step: 22250, train/learning_rate: 2.352451156184543e-05
Step: 22250, train/epoch: 5.295097351074219
Step: 22260, train/loss: 0.0
Step: 22260, train/grad_norm: 2.1658161486470817e-08
Step: 22260, train/learning_rate: 2.3512613552156836e-05
Step: 22260, train/epoch: 5.2974772453308105
Step: 22270, train/loss: 0.0
Step: 22270, train/grad_norm: 1.891524874508832e-07
Step: 22270, train/learning_rate: 2.350071372347884e-05
Step: 22270, train/epoch: 5.299857139587402
Step: 22280, train/loss: 0.0
Step: 22280, train/grad_norm: 8.329122351824481e-07
Step: 22280, train/learning_rate: 2.3488815713790245e-05
Step: 22280, train/epoch: 5.302237033843994
Step: 22290, train/loss: 0.003000000026077032
Step: 22290, train/grad_norm: 8.425744226769893e-07
Step: 22290, train/learning_rate: 2.347691588511225e-05
Step: 22290, train/epoch: 5.304616928100586
Step: 22300, train/loss: 0.0
Step: 22300, train/grad_norm: 0.0013632233021780849
Step: 22300, train/learning_rate: 2.346501605643425e-05
Step: 22300, train/epoch: 5.306996822357178
Step: 22310, train/loss: 0.14219999313354492
Step: 22310, train/grad_norm: 2.0617379448140127e-07
Step: 22310, train/learning_rate: 2.3453118046745658e-05
Step: 22310, train/epoch: 5.3093767166137695
Step: 22320, train/loss: 0.0
Step: 22320, train/grad_norm: 9.749386663315818e-05
Step: 22320, train/learning_rate: 2.344121821806766e-05
Step: 22320, train/epoch: 5.311756134033203
Step: 22330, train/loss: 0.0
Step: 22330, train/grad_norm: 0.0003369258774910122
Step: 22330, train/learning_rate: 2.3429320208379067e-05
Step: 22330, train/epoch: 5.314136028289795
Step: 22340, train/loss: 0.00019999999494757503
Step: 22340, train/grad_norm: 0.007921659387648106
Step: 22340, train/learning_rate: 2.341742037970107e-05
Step: 22340, train/epoch: 5.316515922546387
Step: 22350, train/loss: 0.0
Step: 22350, train/grad_norm: 2.7269163638266036e-07
Step: 22350, train/learning_rate: 2.3405520551023073e-05
Step: 22350, train/epoch: 5.3188958168029785
Step: 22360, train/loss: 0.0
Step: 22360, train/grad_norm: 2.069865416842731e-07
Step: 22360, train/learning_rate: 2.339362254133448e-05
Step: 22360, train/epoch: 5.32127571105957
Step: 22370, train/loss: 0.0
Step: 22370, train/grad_norm: 0.0007823407067917287
Step: 22370, train/learning_rate: 2.3381722712656483e-05
Step: 22370, train/epoch: 5.323655605316162
Step: 22380, train/loss: 0.0
Step: 22380, train/grad_norm: 2.3384149244520813e-05
Step: 22380, train/learning_rate: 2.336982470296789e-05
Step: 22380, train/epoch: 5.326035022735596
Step: 22390, train/loss: 0.0
Step: 22390, train/grad_norm: 5.445313945529051e-06
Step: 22390, train/learning_rate: 2.3357924874289893e-05
Step: 22390, train/epoch: 5.3284149169921875
Step: 22400, train/loss: 0.0
Step: 22400, train/grad_norm: 1.0278903346261359e-06
Step: 22400, train/learning_rate: 2.3346025045611896e-05
Step: 22400, train/epoch: 5.330794811248779
Step: 22410, train/loss: 0.0
Step: 22410, train/grad_norm: 1.0518030649109278e-05
Step: 22410, train/learning_rate: 2.3334127035923302e-05
Step: 22410, train/epoch: 5.333174705505371
Step: 22420, train/loss: 0.0
Step: 22420, train/grad_norm: 0.00038606065209023654
Step: 22420, train/learning_rate: 2.3322227207245305e-05
Step: 22420, train/epoch: 5.335554599761963
Step: 22430, train/loss: 0.0
Step: 22430, train/grad_norm: 9.732731996336952e-05
Step: 22430, train/learning_rate: 2.3310329197556712e-05
Step: 22430, train/epoch: 5.337934494018555
Step: 22440, train/loss: 0.0
Step: 22440, train/grad_norm: 0.00020729580137412995
Step: 22440, train/learning_rate: 2.3298429368878715e-05
Step: 22440, train/epoch: 5.340313911437988
Step: 22450, train/loss: 0.0
Step: 22450, train/grad_norm: 0.06316019594669342
Step: 22450, train/learning_rate: 2.3286529540200718e-05
Step: 22450, train/epoch: 5.34269380569458
Step: 22460, train/loss: 0.0
Step: 22460, train/grad_norm: 0.00029492308385670185
Step: 22460, train/learning_rate: 2.3274631530512124e-05
Step: 22460, train/epoch: 5.345073699951172
Step: 22470, train/loss: 0.0
Step: 22470, train/grad_norm: 5.7386409935134e-06
Step: 22470, train/learning_rate: 2.3262731701834127e-05
Step: 22470, train/epoch: 5.347453594207764
Step: 22480, train/loss: 0.0
Step: 22480, train/grad_norm: 6.10585345839354e-07
Step: 22480, train/learning_rate: 2.3250833692145534e-05
Step: 22480, train/epoch: 5.3498334884643555
Step: 22490, train/loss: 0.0
Step: 22490, train/grad_norm: 4.522259041550569e-05
Step: 22490, train/learning_rate: 2.3238933863467537e-05
Step: 22490, train/epoch: 5.352213382720947
Step: 22500, train/loss: 0.0
Step: 22500, train/grad_norm: 0.0004231410857755691
Step: 22500, train/learning_rate: 2.322703403478954e-05
Step: 22500, train/epoch: 5.354593276977539
Step: 22510, train/loss: 0.0
Step: 22510, train/grad_norm: 1.235913805430755e-05
Step: 22510, train/learning_rate: 2.3215136025100946e-05
Step: 22510, train/epoch: 5.356972694396973
Step: 22520, train/loss: 0.0
Step: 22520, train/grad_norm: 3.7559934753517155e-06
Step: 22520, train/learning_rate: 2.320323619642295e-05
Step: 22520, train/epoch: 5.3593525886535645
Step: 22530, train/loss: 0.00019999999494757503
Step: 22530, train/grad_norm: 0.0014277425361797214
Step: 22530, train/learning_rate: 2.3191338186734356e-05
Step: 22530, train/epoch: 5.361732482910156
Step: 22540, train/loss: 0.0
Step: 22540, train/grad_norm: 8.49870230013039e-06
Step: 22540, train/learning_rate: 2.317943835805636e-05
Step: 22540, train/epoch: 5.364112377166748
Step: 22550, train/loss: 0.032999999821186066
Step: 22550, train/grad_norm: 5.463766683533322e-07
Step: 22550, train/learning_rate: 2.3167538529378362e-05
Step: 22550, train/epoch: 5.36649227142334
Step: 22560, train/loss: 0.0
Step: 22560, train/grad_norm: 2.3271819372894242e-05
Step: 22560, train/learning_rate: 2.315564051968977e-05
Step: 22560, train/epoch: 5.368872165679932
Step: 22570, train/loss: 0.0
Step: 22570, train/grad_norm: 0.0019126118859276175
Step: 22570, train/learning_rate: 2.314374069101177e-05
Step: 22570, train/epoch: 5.371251583099365
Step: 22580, train/loss: 0.025800000876188278
Step: 22580, train/grad_norm: 9.71405279415194e-06
Step: 22580, train/learning_rate: 2.3131842681323178e-05
Step: 22580, train/epoch: 5.373631477355957
Step: 22590, train/loss: 0.0
Step: 22590, train/grad_norm: 3.744802961591631e-05
Step: 22590, train/learning_rate: 2.311994285264518e-05
Step: 22590, train/epoch: 5.376011371612549
Step: 22600, train/loss: 0.0
Step: 22600, train/grad_norm: 0.0001071109581971541
Step: 22600, train/learning_rate: 2.3108043023967184e-05
Step: 22600, train/epoch: 5.378391265869141
Step: 22610, train/loss: 0.0
Step: 22610, train/grad_norm: 0.00011159047426190227
Step: 22610, train/learning_rate: 2.309614501427859e-05
Step: 22610, train/epoch: 5.380771160125732
Step: 22620, train/loss: 0.0
Step: 22620, train/grad_norm: 1.1959693893004442e-06
Step: 22620, train/learning_rate: 2.3084245185600594e-05
Step: 22620, train/epoch: 5.383151054382324
Step: 22630, train/loss: 0.0
Step: 22630, train/grad_norm: 3.257764319641865e-06
Step: 22630, train/learning_rate: 2.3072347175912e-05
Step: 22630, train/epoch: 5.385530471801758
Step: 22640, train/loss: 0.0
Step: 22640, train/grad_norm: 3.858467607642524e-06
Step: 22640, train/learning_rate: 2.3060447347234003e-05
Step: 22640, train/epoch: 5.38791036605835
Step: 22650, train/loss: 0.0
Step: 22650, train/grad_norm: 2.9250034572214645e-07
Step: 22650, train/learning_rate: 2.3048547518556006e-05
Step: 22650, train/epoch: 5.390290260314941
Step: 22660, train/loss: 0.0
Step: 22660, train/grad_norm: 5.309166795086639e-07
Step: 22660, train/learning_rate: 2.3036649508867413e-05
Step: 22660, train/epoch: 5.392670154571533
Step: 22670, train/loss: 0.0
Step: 22670, train/grad_norm: 2.7726118787541054e-05
Step: 22670, train/learning_rate: 2.3024749680189416e-05
Step: 22670, train/epoch: 5.395050048828125
Step: 22680, train/loss: 0.0
Step: 22680, train/grad_norm: 1.2965614359927713e-06
Step: 22680, train/learning_rate: 2.3012851670500822e-05
Step: 22680, train/epoch: 5.397429943084717
Step: 22690, train/loss: 0.0
Step: 22690, train/grad_norm: 0.001066878903657198
Step: 22690, train/learning_rate: 2.3000951841822825e-05
Step: 22690, train/epoch: 5.399809837341309
Step: 22700, train/loss: 0.0
Step: 22700, train/grad_norm: 1.0533133263379568e-06
Step: 22700, train/learning_rate: 2.298905201314483e-05
Step: 22700, train/epoch: 5.402189254760742
Step: 22710, train/loss: 0.0
Step: 22710, train/grad_norm: 4.3041213757533114e-06
Step: 22710, train/learning_rate: 2.2977154003456235e-05
Step: 22710, train/epoch: 5.404569149017334
Step: 22720, train/loss: 0.0
Step: 22720, train/grad_norm: 0.00026963959680870175
Step: 22720, train/learning_rate: 2.2965254174778238e-05
Step: 22720, train/epoch: 5.406949043273926
Step: 22730, train/loss: 0.0
Step: 22730, train/grad_norm: 6.677169608337863e-07
Step: 22730, train/learning_rate: 2.2953356165089644e-05
Step: 22730, train/epoch: 5.409328937530518
Step: 22740, train/loss: 0.0
Step: 22740, train/grad_norm: 4.039418126922101e-05
Step: 22740, train/learning_rate: 2.2941456336411647e-05
Step: 22740, train/epoch: 5.411708831787109
Step: 22750, train/loss: 0.0
Step: 22750, train/grad_norm: 8.553783118259162e-05
Step: 22750, train/learning_rate: 2.292955650773365e-05
Step: 22750, train/epoch: 5.414088726043701
Step: 22760, train/loss: 0.0
Step: 22760, train/grad_norm: 6.45097898086533e-05
Step: 22760, train/learning_rate: 2.2917658498045057e-05
Step: 22760, train/epoch: 5.416468143463135
Step: 22770, train/loss: 0.0
Step: 22770, train/grad_norm: 4.3915366632063524e-07
Step: 22770, train/learning_rate: 2.290575866936706e-05
Step: 22770, train/epoch: 5.418848037719727
Step: 22780, train/loss: 0.0
Step: 22780, train/grad_norm: 7.20662711728437e-08
Step: 22780, train/learning_rate: 2.2893860659678467e-05
Step: 22780, train/epoch: 5.421227931976318
Step: 22790, train/loss: 0.0
Step: 22790, train/grad_norm: 2.3428478016285226e-05
Step: 22790, train/learning_rate: 2.288196083100047e-05
Step: 22790, train/epoch: 5.42360782623291
Step: 22800, train/loss: 0.0
Step: 22800, train/grad_norm: 7.159946562751429e-06
Step: 22800, train/learning_rate: 2.2870061002322473e-05
Step: 22800, train/epoch: 5.425987720489502
Step: 22810, train/loss: 0.0
Step: 22810, train/grad_norm: 1.0662610293366015e-05
Step: 22810, train/learning_rate: 2.285816299263388e-05
Step: 22810, train/epoch: 5.428367614746094
Step: 22820, train/loss: 0.0
Step: 22820, train/grad_norm: 2.8882921014883323e-06
Step: 22820, train/learning_rate: 2.2846263163955882e-05
Step: 22820, train/epoch: 5.430747032165527
Step: 22830, train/loss: 0.0
Step: 22830, train/grad_norm: 3.5591078813013155e-06
Step: 22830, train/learning_rate: 2.283436515426729e-05
Step: 22830, train/epoch: 5.433126926422119
Step: 22840, train/loss: 0.0
Step: 22840, train/grad_norm: 9.828118891164195e-06
Step: 22840, train/learning_rate: 2.2822465325589292e-05
Step: 22840, train/epoch: 5.435506820678711
Step: 22850, train/loss: 0.0
Step: 22850, train/grad_norm: 2.7399871669331333e-06
Step: 22850, train/learning_rate: 2.2810565496911295e-05
Step: 22850, train/epoch: 5.437886714935303
Step: 22860, train/loss: 0.0
Step: 22860, train/grad_norm: 5.840868766426865e-07
Step: 22860, train/learning_rate: 2.27986674872227e-05
Step: 22860, train/epoch: 5.4402666091918945
Step: 22870, train/loss: 0.0
Step: 22870, train/grad_norm: 2.0436915292521007e-05
Step: 22870, train/learning_rate: 2.2786767658544704e-05
Step: 22870, train/epoch: 5.442646503448486
Step: 22880, train/loss: 0.0
Step: 22880, train/grad_norm: 1.8804121282300912e-07
Step: 22880, train/learning_rate: 2.277486964885611e-05
Step: 22880, train/epoch: 5.445026397705078
Step: 22890, train/loss: 0.0
Step: 22890, train/grad_norm: 3.1079420637070143e-07
Step: 22890, train/learning_rate: 2.2762969820178114e-05
Step: 22890, train/epoch: 5.447405815124512
Step: 22900, train/loss: 0.0
Step: 22900, train/grad_norm: 1.077653223546804e-06
Step: 22900, train/learning_rate: 2.275107181048952e-05
Step: 22900, train/epoch: 5.4497857093811035
Step: 22910, train/loss: 0.0
Step: 22910, train/grad_norm: 1.0472385980619947e-07
Step: 22910, train/learning_rate: 2.2739171981811523e-05
Step: 22910, train/epoch: 5.452165603637695
Step: 22920, train/loss: 0.0
Step: 22920, train/grad_norm: 1.4920984767741174e-06
Step: 22920, train/learning_rate: 2.2727272153133526e-05
Step: 22920, train/epoch: 5.454545497894287
Step: 22930, train/loss: 0.0
Step: 22930, train/grad_norm: 7.176364306360483e-05
Step: 22930, train/learning_rate: 2.2715374143444933e-05
Step: 22930, train/epoch: 5.456925392150879
Step: 22940, train/loss: 0.0
Step: 22940, train/grad_norm: 4.705657829617849e-06
Step: 22940, train/learning_rate: 2.2703474314766936e-05
Step: 22940, train/epoch: 5.459305286407471
Step: 22950, train/loss: 0.0
Step: 22950, train/grad_norm: 2.10567577596521e-05
Step: 22950, train/learning_rate: 2.2691576305078343e-05
Step: 22950, train/epoch: 5.461684703826904
Step: 22960, train/loss: 0.0
Step: 22960, train/grad_norm: 4.82396944789798e-06
Step: 22960, train/learning_rate: 2.2679676476400346e-05
Step: 22960, train/epoch: 5.464064598083496
Step: 22970, train/loss: 0.0
Step: 22970, train/grad_norm: 8.401150262216106e-05
Step: 22970, train/learning_rate: 2.266777664772235e-05
Step: 22970, train/epoch: 5.466444492340088
Step: 22980, train/loss: 0.0
Step: 22980, train/grad_norm: 7.9584839340896e-07
Step: 22980, train/learning_rate: 2.2655878638033755e-05
Step: 22980, train/epoch: 5.46882438659668
Step: 22990, train/loss: 0.0
Step: 22990, train/grad_norm: 4.380240170576144e-06
Step: 22990, train/learning_rate: 2.2643978809355758e-05
Step: 22990, train/epoch: 5.4712042808532715
Step: 23000, train/loss: 0.0
Step: 23000, train/grad_norm: 6.897620096424362e-06
Step: 23000, train/learning_rate: 2.2632080799667165e-05
Step: 23000, train/epoch: 5.473584175109863
Step: 23010, train/loss: 0.0
Step: 23010, train/grad_norm: 3.715116747571301e-07
Step: 23010, train/learning_rate: 2.2620180970989168e-05
Step: 23010, train/epoch: 5.475963592529297
Step: 23020, train/loss: 0.0
Step: 23020, train/grad_norm: 3.8361959013855085e-05
Step: 23020, train/learning_rate: 2.260828114231117e-05
Step: 23020, train/epoch: 5.478343486785889
Step: 23030, train/loss: 0.0
Step: 23030, train/grad_norm: 0.0002986641484312713
Step: 23030, train/learning_rate: 2.2596383132622577e-05
Step: 23030, train/epoch: 5.4807233810424805
Step: 23040, train/loss: 0.0
Step: 23040, train/grad_norm: 5.114526970828592e-07
Step: 23040, train/learning_rate: 2.258448330394458e-05
Step: 23040, train/epoch: 5.483103275299072
Step: 23050, train/loss: 0.0
Step: 23050, train/grad_norm: 4.371111572254449e-06
Step: 23050, train/learning_rate: 2.2572585294255987e-05
Step: 23050, train/epoch: 5.485483169555664
Step: 23060, train/loss: 0.0
Step: 23060, train/grad_norm: 2.2735714821919828e-07
Step: 23060, train/learning_rate: 2.256068546557799e-05
Step: 23060, train/epoch: 5.487863063812256
Step: 23070, train/loss: 0.0
Step: 23070, train/grad_norm: 2.2630089802078146e-07
Step: 23070, train/learning_rate: 2.2548785636899993e-05
Step: 23070, train/epoch: 5.490242958068848
Step: 23080, train/loss: 0.0
Step: 23080, train/grad_norm: 9.111628429536722e-08
Step: 23080, train/learning_rate: 2.25368876272114e-05
Step: 23080, train/epoch: 5.492622375488281
Step: 23090, train/loss: 0.0
Step: 23090, train/grad_norm: 2.0833073222092935e-07
Step: 23090, train/learning_rate: 2.2524987798533402e-05
Step: 23090, train/epoch: 5.495002269744873
Step: 23100, train/loss: 0.0
Step: 23100, train/grad_norm: 3.7762756619486026e-06
Step: 23100, train/learning_rate: 2.251308978884481e-05
Step: 23100, train/epoch: 5.497382164001465
Step: 23110, train/loss: 0.0
Step: 23110, train/grad_norm: 5.547815817408264e-05
Step: 23110, train/learning_rate: 2.2501189960166812e-05
Step: 23110, train/epoch: 5.499762058258057
Step: 23120, train/loss: 0.0
Step: 23120, train/grad_norm: 2.5411330284441647e-07
Step: 23120, train/learning_rate: 2.2489290131488815e-05
Step: 23120, train/epoch: 5.502141952514648
Step: 23130, train/loss: 0.0
Step: 23130, train/grad_norm: 1.4079906804909115e-06
Step: 23130, train/learning_rate: 2.247739212180022e-05
Step: 23130, train/epoch: 5.50452184677124
Step: 23140, train/loss: 0.0
Step: 23140, train/grad_norm: 5.233181218500249e-05
Step: 23140, train/learning_rate: 2.2465492293122225e-05
Step: 23140, train/epoch: 5.506901264190674
Step: 23150, train/loss: 0.0
Step: 23150, train/grad_norm: 1.8676904289804952e-07
Step: 23150, train/learning_rate: 2.245359428343363e-05
Step: 23150, train/epoch: 5.509281158447266
Step: 23160, train/loss: 0.0
Step: 23160, train/grad_norm: 1.1489335065562045e-06
Step: 23160, train/learning_rate: 2.2441694454755634e-05
Step: 23160, train/epoch: 5.511661052703857
Step: 23170, train/loss: 0.0
Step: 23170, train/grad_norm: 1.7875784408261097e-07
Step: 23170, train/learning_rate: 2.2429794626077637e-05
Step: 23170, train/epoch: 5.514040946960449
Step: 23180, train/loss: 0.0
Step: 23180, train/grad_norm: 0.00011168406490469351
Step: 23180, train/learning_rate: 2.2417896616389044e-05
Step: 23180, train/epoch: 5.516420841217041
Step: 23190, train/loss: 0.0
Step: 23190, train/grad_norm: 0.0006625032401643693
Step: 23190, train/learning_rate: 2.2405996787711047e-05
Step: 23190, train/epoch: 5.518800735473633
Step: 23200, train/loss: 0.0
Step: 23200, train/grad_norm: 1.1728625395335257e-06
Step: 23200, train/learning_rate: 2.2394098778022453e-05
Step: 23200, train/epoch: 5.521180152893066
Step: 23210, train/loss: 0.0
Step: 23210, train/grad_norm: 0.0018538250587880611
Step: 23210, train/learning_rate: 2.2382198949344456e-05
Step: 23210, train/epoch: 5.523560047149658
Step: 23220, train/loss: 0.0
Step: 23220, train/grad_norm: 3.3445851386204595e-06
Step: 23220, train/learning_rate: 2.237029912066646e-05
Step: 23220, train/epoch: 5.52593994140625
Step: 23230, train/loss: 0.0
Step: 23230, train/grad_norm: 4.187103854746965e-07
Step: 23230, train/learning_rate: 2.2358401110977866e-05
Step: 23230, train/epoch: 5.528319835662842
Step: 23240, train/loss: 0.0
Step: 23240, train/grad_norm: 1.230189809575677e-05
Step: 23240, train/learning_rate: 2.234650128229987e-05
Step: 23240, train/epoch: 5.530699729919434
Step: 23250, train/loss: 0.0
Step: 23250, train/grad_norm: 9.25831227505114e-06
Step: 23250, train/learning_rate: 2.2334603272611275e-05
Step: 23250, train/epoch: 5.533079624176025
Step: 23260, train/loss: 0.0
Step: 23260, train/grad_norm: 6.970275023832073e-08
Step: 23260, train/learning_rate: 2.232270344393328e-05
Step: 23260, train/epoch: 5.535459518432617
Step: 23270, train/loss: 0.0
Step: 23270, train/grad_norm: 1.2165569387434516e-06
Step: 23270, train/learning_rate: 2.231080361525528e-05
Step: 23270, train/epoch: 5.537838935852051
Step: 23280, train/loss: 0.0
Step: 23280, train/grad_norm: 2.3024053064091277e-07
Step: 23280, train/learning_rate: 2.2298905605566688e-05
Step: 23280, train/epoch: 5.540218830108643
Step: 23290, train/loss: 0.0
Step: 23290, train/grad_norm: 0.00015141379844862968
Step: 23290, train/learning_rate: 2.228700577688869e-05
Step: 23290, train/epoch: 5.542598724365234
Step: 23300, train/loss: 0.0
Step: 23300, train/grad_norm: 6.00087801672089e-08
Step: 23300, train/learning_rate: 2.2275107767200097e-05
Step: 23300, train/epoch: 5.544978618621826
Step: 23310, train/loss: 0.0
Step: 23310, train/grad_norm: 1.364452236884972e-05
Step: 23310, train/learning_rate: 2.22632079385221e-05
Step: 23310, train/epoch: 5.547358512878418
Step: 23320, train/loss: 0.0
Step: 23320, train/grad_norm: 0.00016833943664096296
Step: 23320, train/learning_rate: 2.2251308109844103e-05
Step: 23320, train/epoch: 5.54973840713501
Step: 23330, train/loss: 0.0
Step: 23330, train/grad_norm: 2.242911932626157e-06
Step: 23330, train/learning_rate: 2.223941010015551e-05
Step: 23330, train/epoch: 5.552117824554443
Step: 23340, train/loss: 0.0
Step: 23340, train/grad_norm: 1.6521096313226735e-07
Step: 23340, train/learning_rate: 2.2227510271477513e-05
Step: 23340, train/epoch: 5.554497718811035
Step: 23350, train/loss: 0.0
Step: 23350, train/grad_norm: 0.019206831231713295
Step: 23350, train/learning_rate: 2.221561226178892e-05
Step: 23350, train/epoch: 5.556877613067627
Step: 23360, train/loss: 0.0
Step: 23360, train/grad_norm: 2.3969484175268008e-08
Step: 23360, train/learning_rate: 2.2203712433110923e-05
Step: 23360, train/epoch: 5.559257507324219
Step: 23370, train/loss: 0.0
Step: 23370, train/grad_norm: 4.826285362469207e-07
Step: 23370, train/learning_rate: 2.2191812604432926e-05
Step: 23370, train/epoch: 5.5616374015808105
Step: 23380, train/loss: 0.0
Step: 23380, train/grad_norm: 5.573392058977333e-07
Step: 23380, train/learning_rate: 2.2179914594744332e-05
Step: 23380, train/epoch: 5.564017295837402
Step: 23390, train/loss: 0.0
Step: 23390, train/grad_norm: 1.094195809514531e-07
Step: 23390, train/learning_rate: 2.2168014766066335e-05
Step: 23390, train/epoch: 5.566397190093994
Step: 23400, train/loss: 0.0
Step: 23400, train/grad_norm: 8.844578474054288e-07
Step: 23400, train/learning_rate: 2.2156116756377742e-05
Step: 23400, train/epoch: 5.568776607513428
Step: 23410, train/loss: 0.0
Step: 23410, train/grad_norm: 1.2436042595709296e-07
Step: 23410, train/learning_rate: 2.2144216927699745e-05
Step: 23410, train/epoch: 5.5711565017700195
Step: 23420, train/loss: 0.0
Step: 23420, train/grad_norm: 4.456475289771333e-05
Step: 23420, train/learning_rate: 2.2132317099021748e-05
Step: 23420, train/epoch: 5.573536396026611
Step: 23430, train/loss: 0.0
Step: 23430, train/grad_norm: 9.90472358353145e-07
Step: 23430, train/learning_rate: 2.2120419089333154e-05
Step: 23430, train/epoch: 5.575916290283203
Step: 23440, train/loss: 0.0
Step: 23440, train/grad_norm: 1.523832260375002e-08
Step: 23440, train/learning_rate: 2.2108519260655157e-05
Step: 23440, train/epoch: 5.578296184539795
Step: 23450, train/loss: 0.0
Step: 23450, train/grad_norm: 8.96041910891654e-07
Step: 23450, train/learning_rate: 2.2096621250966564e-05
Step: 23450, train/epoch: 5.580676078796387
Step: 23460, train/loss: 0.0
Step: 23460, train/grad_norm: 1.3420680033959798e-06
Step: 23460, train/learning_rate: 2.2084721422288567e-05
Step: 23460, train/epoch: 5.58305549621582
Step: 23470, train/loss: 9.999999747378752e-05
Step: 23470, train/grad_norm: 9.270296175145631e-08
Step: 23470, train/learning_rate: 2.207282159361057e-05
Step: 23470, train/epoch: 5.585435390472412
Step: 23480, train/loss: 0.0
Step: 23480, train/grad_norm: 6.121373630207927e-09
Step: 23480, train/learning_rate: 2.2060923583921976e-05
Step: 23480, train/epoch: 5.587815284729004
Step: 23490, train/loss: 0.0
Step: 23490, train/grad_norm: 1.4926464245945681e-05
Step: 23490, train/learning_rate: 2.204902375524398e-05
Step: 23490, train/epoch: 5.590195178985596
Step: 23500, train/loss: 0.0
Step: 23500, train/grad_norm: 4.624938654274047e-08
Step: 23500, train/learning_rate: 2.2037125745555386e-05
Step: 23500, train/epoch: 5.5925750732421875
Step: 23510, train/loss: 0.0
Step: 23510, train/grad_norm: 0.00013431960542220622
Step: 23510, train/learning_rate: 2.202522591687739e-05
Step: 23510, train/epoch: 5.594954967498779
Step: 23520, train/loss: 0.0
Step: 23520, train/grad_norm: 1.1391077237021818e-07
Step: 23520, train/learning_rate: 2.2013326088199392e-05
Step: 23520, train/epoch: 5.597334384918213
Step: 23530, train/loss: 0.0
Step: 23530, train/grad_norm: 3.850802272609144e-07
Step: 23530, train/learning_rate: 2.20014280785108e-05
Step: 23530, train/epoch: 5.599714279174805
Step: 23540, train/loss: 0.0
Step: 23540, train/grad_norm: 3.289087180746719e-05
Step: 23540, train/learning_rate: 2.19895282498328e-05
Step: 23540, train/epoch: 5.6020941734313965
Step: 23550, train/loss: 0.0
Step: 23550, train/grad_norm: 6.269218744137106e-08
Step: 23550, train/learning_rate: 2.1977630240144208e-05
Step: 23550, train/epoch: 5.604474067687988
Step: 23560, train/loss: 0.0
Step: 23560, train/grad_norm: 6.1420115571309e-07
Step: 23560, train/learning_rate: 2.196573041146621e-05
Step: 23560, train/epoch: 5.60685396194458
Step: 23570, train/loss: 0.0
Step: 23570, train/grad_norm: 2.1625565338467823e-08
Step: 23570, train/learning_rate: 2.1953832401777618e-05
Step: 23570, train/epoch: 5.609233856201172
Step: 23580, train/loss: 0.0
Step: 23580, train/grad_norm: 1.508899458713131e-07
Step: 23580, train/learning_rate: 2.194193257309962e-05
Step: 23580, train/epoch: 5.611613750457764
Step: 23590, train/loss: 0.0
Step: 23590, train/grad_norm: 1.150526252047257e-08
Step: 23590, train/learning_rate: 2.1930032744421624e-05
Step: 23590, train/epoch: 5.613993167877197
Step: 23600, train/loss: 0.0
Step: 23600, train/grad_norm: 6.3718218257236e-08
Step: 23600, train/learning_rate: 2.191813473473303e-05
Step: 23600, train/epoch: 5.616373062133789
Step: 23610, train/loss: 0.0
Step: 23610, train/grad_norm: 7.439115989882339e-09
Step: 23610, train/learning_rate: 2.1906234906055033e-05
Step: 23610, train/epoch: 5.618752956390381
Step: 23620, train/loss: 0.0
Step: 23620, train/grad_norm: 8.077757129854035e-09
Step: 23620, train/learning_rate: 2.189433689636644e-05
Step: 23620, train/epoch: 5.621132850646973
Step: 23630, train/loss: 0.0
Step: 23630, train/grad_norm: 9.238886988782724e-09
Step: 23630, train/learning_rate: 2.1882437067688443e-05
Step: 23630, train/epoch: 5.6235127449035645
Step: 23640, train/loss: 0.0
Step: 23640, train/grad_norm: 0.0006658637430518866
Step: 23640, train/learning_rate: 2.1870537239010446e-05
Step: 23640, train/epoch: 5.625892639160156
Step: 23650, train/loss: 0.0
Step: 23650, train/grad_norm: 1.9993535715912003e-07
Step: 23650, train/learning_rate: 2.1858639229321852e-05
Step: 23650, train/epoch: 5.62827205657959
Step: 23660, train/loss: 0.0
Step: 23660, train/grad_norm: 1.6786890455477987e-07
Step: 23660, train/learning_rate: 2.1846739400643855e-05
Step: 23660, train/epoch: 5.630651950836182
Step: 23670, train/loss: 0.0
Step: 23670, train/grad_norm: 1.5289630894699258e-08
Step: 23670, train/learning_rate: 2.1834841390955262e-05
Step: 23670, train/epoch: 5.633031845092773
Step: 23680, train/loss: 0.0
Step: 23680, train/grad_norm: 5.772659150693471e-08
Step: 23680, train/learning_rate: 2.1822941562277265e-05
Step: 23680, train/epoch: 5.635411739349365
Step: 23690, train/loss: 0.0
Step: 23690, train/grad_norm: 5.762723187530128e-09
Step: 23690, train/learning_rate: 2.1811041733599268e-05
Step: 23690, train/epoch: 5.637791633605957
Step: 23700, train/loss: 0.0
Step: 23700, train/grad_norm: 1.0542661854628932e-08
Step: 23700, train/learning_rate: 2.1799143723910674e-05
Step: 23700, train/epoch: 5.640171527862549
Step: 23710, train/loss: 0.0
Step: 23710, train/grad_norm: 1.3783979113668465e-07
Step: 23710, train/learning_rate: 2.1787243895232677e-05
Step: 23710, train/epoch: 5.642550945281982
Step: 23720, train/loss: 0.0
Step: 23720, train/grad_norm: 7.693444215028933e-10
Step: 23720, train/learning_rate: 2.1775345885544084e-05
Step: 23720, train/epoch: 5.644930839538574
Step: 23730, train/loss: 0.0
Step: 23730, train/grad_norm: 7.976574067924957e-09
Step: 23730, train/learning_rate: 2.1763446056866087e-05
Step: 23730, train/epoch: 5.647310733795166
Step: 23740, train/loss: 0.0
Step: 23740, train/grad_norm: 2.6097605854147332e-08
Step: 23740, train/learning_rate: 2.175154622818809e-05
Step: 23740, train/epoch: 5.649690628051758
Step: 23750, train/loss: 0.0
Step: 23750, train/grad_norm: 1.6888516540802812e-07
Step: 23750, train/learning_rate: 2.1739648218499497e-05
Step: 23750, train/epoch: 5.65207052230835
Step: 23760, train/loss: 0.0
Step: 23760, train/grad_norm: 5.5299242518458414e-08
Step: 23760, train/learning_rate: 2.17277483898215e-05
Step: 23760, train/epoch: 5.654450416564941
Step: 23770, train/loss: 0.0
Step: 23770, train/grad_norm: 3.951041449568038e-08
Step: 23770, train/learning_rate: 2.1715850380132906e-05
Step: 23770, train/epoch: 5.656830310821533
Step: 23780, train/loss: 0.0
Step: 23780, train/grad_norm: 1.0068670341922825e-08
Step: 23780, train/learning_rate: 2.170395055145491e-05
Step: 23780, train/epoch: 5.659209728240967
Step: 23790, train/loss: 0.0
Step: 23790, train/grad_norm: 4.269726261441065e-08
Step: 23790, train/learning_rate: 2.1692050722776912e-05
Step: 23790, train/epoch: 5.661589622497559
Step: 23800, train/loss: 0.0
Step: 23800, train/grad_norm: 3.774830670266738e-09
Step: 23800, train/learning_rate: 2.168015271308832e-05
Step: 23800, train/epoch: 5.66396951675415
Step: 23810, train/loss: 0.0
Step: 23810, train/grad_norm: 3.770084049392608e-06
Step: 23810, train/learning_rate: 2.1668252884410322e-05
Step: 23810, train/epoch: 5.666349411010742
Step: 23820, train/loss: 0.0
Step: 23820, train/grad_norm: 8.00678421342127e-08
Step: 23820, train/learning_rate: 2.1656354874721728e-05
Step: 23820, train/epoch: 5.668729305267334
Step: 23830, train/loss: 0.0
Step: 23830, train/grad_norm: 5.381616574595682e-07
Step: 23830, train/learning_rate: 2.164445504604373e-05
Step: 23830, train/epoch: 5.671109199523926
Step: 23840, train/loss: 0.0
Step: 23840, train/grad_norm: 3.9590369738107256e-07
Step: 23840, train/learning_rate: 2.1632555217365734e-05
Step: 23840, train/epoch: 5.673488616943359
Step: 23850, train/loss: 0.0
Step: 23850, train/grad_norm: 4.719081516668666e-07
Step: 23850, train/learning_rate: 2.162065720767714e-05
Step: 23850, train/epoch: 5.675868511199951
Step: 23860, train/loss: 0.0
Step: 23860, train/grad_norm: 2.391875568719115e-07
Step: 23860, train/learning_rate: 2.1608757378999144e-05
Step: 23860, train/epoch: 5.678248405456543
Step: 23870, train/loss: 0.0
Step: 23870, train/grad_norm: 9.318835703098216e-10
Step: 23870, train/learning_rate: 2.159685936931055e-05
Step: 23870, train/epoch: 5.680628299713135
Step: 23880, train/loss: 0.0
Step: 23880, train/grad_norm: 1.2056310652042157e-06
Step: 23880, train/learning_rate: 2.1584959540632553e-05
Step: 23880, train/epoch: 5.683008193969727
Step: 23890, train/loss: 0.0
Step: 23890, train/grad_norm: 3.331050990595941e-08
Step: 23890, train/learning_rate: 2.1573059711954556e-05
Step: 23890, train/epoch: 5.685388088226318
Step: 23900, train/loss: 0.0
Step: 23900, train/grad_norm: 2.399173695266654e-07
Step: 23900, train/learning_rate: 2.1561161702265963e-05
Step: 23900, train/epoch: 5.687767505645752
Step: 23910, train/loss: 0.0
Step: 23910, train/grad_norm: 1.2568152740755067e-08
Step: 23910, train/learning_rate: 2.1549261873587966e-05
Step: 23910, train/epoch: 5.690147399902344
Step: 23920, train/loss: 0.0
Step: 23920, train/grad_norm: 2.0214811158325574e-08
Step: 23920, train/learning_rate: 2.1537363863899373e-05
Step: 23920, train/epoch: 5.6925272941589355
Step: 23930, train/loss: 0.0
Step: 23930, train/grad_norm: 0.00046998157631605864
Step: 23930, train/learning_rate: 2.1525464035221376e-05
Step: 23930, train/epoch: 5.694907188415527
Step: 23940, train/loss: 0.0
Step: 23940, train/grad_norm: 5.19392031606003e-08
Step: 23940, train/learning_rate: 2.151356420654338e-05
Step: 23940, train/epoch: 5.697287082672119
Step: 23950, train/loss: 0.0
Step: 23950, train/grad_norm: 4.169162082234834e-08
Step: 23950, train/learning_rate: 2.1501666196854785e-05
Step: 23950, train/epoch: 5.699666976928711
Step: 23960, train/loss: 0.0
Step: 23960, train/grad_norm: 4.656533747038338e-06
Step: 23960, train/learning_rate: 2.1489766368176788e-05
Step: 23960, train/epoch: 5.702046871185303
Step: 23970, train/loss: 0.0
Step: 23970, train/grad_norm: 3.40197630066541e-08
Step: 23970, train/learning_rate: 2.1477868358488195e-05
Step: 23970, train/epoch: 5.704426288604736
Step: 23980, train/loss: 0.0
Step: 23980, train/grad_norm: 4.369938366721726e-08
Step: 23980, train/learning_rate: 2.1465968529810198e-05
Step: 23980, train/epoch: 5.706806182861328
Step: 23990, train/loss: 0.0
Step: 23990, train/grad_norm: 3.78175535331593e-08
Step: 23990, train/learning_rate: 2.14540687011322e-05
Step: 23990, train/epoch: 5.70918607711792
Step: 24000, train/loss: 0.0
Step: 24000, train/grad_norm: 5.886091258844317e-08
Step: 24000, train/learning_rate: 2.1442170691443607e-05
Step: 24000, train/epoch: 5.711565971374512
Step: 24010, train/loss: 0.0
Step: 24010, train/grad_norm: 4.5204801608633716e-06
Step: 24010, train/learning_rate: 2.143027086276561e-05
Step: 24010, train/epoch: 5.7139458656311035
Step: 24020, train/loss: 0.0
Step: 24020, train/grad_norm: 4.081317683812813e-08
Step: 24020, train/learning_rate: 2.1418372853077017e-05
Step: 24020, train/epoch: 5.716325759887695
Step: 24030, train/loss: 0.0
Step: 24030, train/grad_norm: 3.245470679758e-06
Step: 24030, train/learning_rate: 2.140647302439902e-05
Step: 24030, train/epoch: 5.718705177307129
Step: 24040, train/loss: 0.0
Step: 24040, train/grad_norm: 2.246969943442423e-09
Step: 24040, train/learning_rate: 2.1394573195721023e-05
Step: 24040, train/epoch: 5.721085071563721
Step: 24050, train/loss: 0.0
Step: 24050, train/grad_norm: 4.2263362587391384e-08
Step: 24050, train/learning_rate: 2.138267518603243e-05
Step: 24050, train/epoch: 5.7234649658203125
Step: 24060, train/loss: 0.09960000216960907
Step: 24060, train/grad_norm: 1.0654928495057447e-08
Step: 24060, train/learning_rate: 2.1370775357354432e-05
Step: 24060, train/epoch: 5.725844860076904
Step: 24070, train/loss: 0.0
Step: 24070, train/grad_norm: 1.4811176640705526e-07
Step: 24070, train/learning_rate: 2.135887734766584e-05
Step: 24070, train/epoch: 5.728224754333496
Step: 24080, train/loss: 0.0
Step: 24080, train/grad_norm: 6.115194992162287e-05
Step: 24080, train/learning_rate: 2.1346977518987842e-05
Step: 24080, train/epoch: 5.730604648590088
Step: 24090, train/loss: 0.00279999990016222
Step: 24090, train/grad_norm: 34.423274993896484
Step: 24090, train/learning_rate: 2.1335077690309845e-05
Step: 24090, train/epoch: 5.7329840660095215
Step: 24100, train/loss: 0.0
Step: 24100, train/grad_norm: 0.00035893937456421554
Step: 24100, train/learning_rate: 2.132317968062125e-05
Step: 24100, train/epoch: 5.735363960266113
Step: 24110, train/loss: 0.0
Step: 24110, train/grad_norm: 1.7653531969585856e-08
Step: 24110, train/learning_rate: 2.1311279851943254e-05
Step: 24110, train/epoch: 5.737743854522705
Step: 24120, train/loss: 0.0
Step: 24120, train/grad_norm: 1.2124401109758765e-06
Step: 24120, train/learning_rate: 2.129938184225466e-05
Step: 24120, train/epoch: 5.740123748779297
Step: 24130, train/loss: 0.0
Step: 24130, train/grad_norm: 1.0670864725170759e-07
Step: 24130, train/learning_rate: 2.1287482013576664e-05
Step: 24130, train/epoch: 5.742503643035889
Step: 24140, train/loss: 9.999999747378752e-05
Step: 24140, train/grad_norm: 1.0925053750554525e-08
Step: 24140, train/learning_rate: 2.1275582184898667e-05
Step: 24140, train/epoch: 5.7448835372924805
Step: 24150, train/loss: 0.0
Step: 24150, train/grad_norm: 0.0007177651859819889
Step: 24150, train/learning_rate: 2.1263684175210074e-05
Step: 24150, train/epoch: 5.747263431549072
Step: 24160, train/loss: 0.0
Step: 24160, train/grad_norm: 2.3950414984597046e-08
Step: 24160, train/learning_rate: 2.1251784346532077e-05
Step: 24160, train/epoch: 5.749642848968506
Step: 24170, train/loss: 0.17339999973773956
Step: 24170, train/grad_norm: 7.162962901929859e-06
Step: 24170, train/learning_rate: 2.1239886336843483e-05
Step: 24170, train/epoch: 5.752022743225098
Step: 24180, train/loss: 0.0
Step: 24180, train/grad_norm: 3.0022160899534356e-06
Step: 24180, train/learning_rate: 2.1227986508165486e-05
Step: 24180, train/epoch: 5.7544026374816895
Step: 24190, train/loss: 0.1687999963760376
Step: 24190, train/grad_norm: 0.029893292114138603
Step: 24190, train/learning_rate: 2.121608667948749e-05
Step: 24190, train/epoch: 5.756782531738281
Step: 24200, train/loss: 9.999999747378752e-05
Step: 24200, train/grad_norm: 0.006171183194965124
Step: 24200, train/learning_rate: 2.1204188669798896e-05
Step: 24200, train/epoch: 5.759162425994873
Step: 24210, train/loss: 0.0
Step: 24210, train/grad_norm: 0.0005780230858363211
Step: 24210, train/learning_rate: 2.11922888411209e-05
Step: 24210, train/epoch: 5.761542320251465
Step: 24220, train/loss: 0.004800000227987766
Step: 24220, train/grad_norm: 3.5494318581186235e-05
Step: 24220, train/learning_rate: 2.1180390831432305e-05
Step: 24220, train/epoch: 5.763921737670898
Step: 24230, train/loss: 0.0008999999845400453
Step: 24230, train/grad_norm: 0.0007627269951626658
Step: 24230, train/learning_rate: 2.1168491002754308e-05
Step: 24230, train/epoch: 5.76630163192749
Step: 24240, train/loss: 0.0
Step: 24240, train/grad_norm: 2.1195199906287598e-07
Step: 24240, train/learning_rate: 2.1156592993065715e-05
Step: 24240, train/epoch: 5.768681526184082
Step: 24250, train/loss: 0.0
Step: 24250, train/grad_norm: 2.51190758717712e-05
Step: 24250, train/learning_rate: 2.1144693164387718e-05
Step: 24250, train/epoch: 5.771061420440674
Step: 24260, train/loss: 0.0
Step: 24260, train/grad_norm: 1.266336880689778e-06
Step: 24260, train/learning_rate: 2.113279333570972e-05
Step: 24260, train/epoch: 5.773441314697266
Step: 24270, train/loss: 0.0031999999191612005
Step: 24270, train/grad_norm: 1.762730619248032e-07
Step: 24270, train/learning_rate: 2.1120895326021127e-05
Step: 24270, train/epoch: 5.775821208953857
Step: 24280, train/loss: 0.0
Step: 24280, train/grad_norm: 2.5911826014635153e-05
Step: 24280, train/learning_rate: 2.110899549734313e-05
Step: 24280, train/epoch: 5.778200626373291
Step: 24290, train/loss: 0.0
Step: 24290, train/grad_norm: 5.930066322434868e-07
Step: 24290, train/learning_rate: 2.1097097487654537e-05
Step: 24290, train/epoch: 5.780580520629883
Step: 24300, train/loss: 0.003700000001117587
Step: 24300, train/grad_norm: 1.8279504843121686e-07
Step: 24300, train/learning_rate: 2.108519765897654e-05
Step: 24300, train/epoch: 5.782960414886475
Step: 24310, train/loss: 0.0
Step: 24310, train/grad_norm: 5.095363667351194e-05
Step: 24310, train/learning_rate: 2.1073297830298543e-05
Step: 24310, train/epoch: 5.785340309143066
Step: 24320, train/loss: 0.0
Step: 24320, train/grad_norm: 3.738722398338723e-08
Step: 24320, train/learning_rate: 2.106139982060995e-05
Step: 24320, train/epoch: 5.787720203399658
Step: 24330, train/loss: 0.13750000298023224
Step: 24330, train/grad_norm: 5.971273964178181e-08
Step: 24330, train/learning_rate: 2.1049499991931953e-05
Step: 24330, train/epoch: 5.79010009765625
Step: 24340, train/loss: 0.0
Step: 24340, train/grad_norm: 0.004751170519739389
Step: 24340, train/learning_rate: 2.103760198224336e-05
Step: 24340, train/epoch: 5.792479991912842
Step: 24350, train/loss: 0.0
Step: 24350, train/grad_norm: 3.56981627191999e-06
Step: 24350, train/learning_rate: 2.1025702153565362e-05
Step: 24350, train/epoch: 5.794859409332275
Step: 24360, train/loss: 0.0
Step: 24360, train/grad_norm: 2.894728459068574e-05
Step: 24360, train/learning_rate: 2.1013802324887365e-05
Step: 24360, train/epoch: 5.797239303588867
Step: 24370, train/loss: 0.0
Step: 24370, train/grad_norm: 0.00014684020425193012
Step: 24370, train/learning_rate: 2.100190431519877e-05
Step: 24370, train/epoch: 5.799619197845459
Step: 24380, train/loss: 0.0
Step: 24380, train/grad_norm: 3.836547421087744e-06
Step: 24380, train/learning_rate: 2.0990004486520775e-05
Step: 24380, train/epoch: 5.801999092102051
Step: 24390, train/loss: 0.0
Step: 24390, train/grad_norm: 6.43954194856633e-07
Step: 24390, train/learning_rate: 2.097810647683218e-05
Step: 24390, train/epoch: 5.804378986358643
Step: 24400, train/loss: 0.0
Step: 24400, train/grad_norm: 8.775010655881488e-07
Step: 24400, train/learning_rate: 2.0966206648154184e-05
Step: 24400, train/epoch: 5.806758880615234
Step: 24410, train/loss: 0.014800000004470348
Step: 24410, train/grad_norm: 0.012898683547973633
Step: 24410, train/learning_rate: 2.0954306819476187e-05
Step: 24410, train/epoch: 5.809138298034668
Step: 24420, train/loss: 0.0
Step: 24420, train/grad_norm: 1.0066200957226101e-06
Step: 24420, train/learning_rate: 2.0942408809787594e-05
Step: 24420, train/epoch: 5.81151819229126
Step: 24430, train/loss: 0.0
Step: 24430, train/grad_norm: 1.1950697626161855e-05
Step: 24430, train/learning_rate: 2.0930508981109597e-05
Step: 24430, train/epoch: 5.813898086547852
Step: 24440, train/loss: 0.0
Step: 24440, train/grad_norm: 0.0005467624869197607
Step: 24440, train/learning_rate: 2.0918610971421003e-05
Step: 24440, train/epoch: 5.816277980804443
Step: 24450, train/loss: 0.02710000053048134
Step: 24450, train/grad_norm: 9.160744230030105e-05
Step: 24450, train/learning_rate: 2.0906711142743006e-05
Step: 24450, train/epoch: 5.818657875061035
Step: 24460, train/loss: 0.0
Step: 24460, train/grad_norm: 6.477497436208068e-07
Step: 24460, train/learning_rate: 2.089481131406501e-05
Step: 24460, train/epoch: 5.821037769317627
Step: 24470, train/loss: 0.0
Step: 24470, train/grad_norm: 4.024042482342338e-06
Step: 24470, train/learning_rate: 2.0882913304376416e-05
Step: 24470, train/epoch: 5.8234171867370605
Step: 24480, train/loss: 0.0
Step: 24480, train/grad_norm: 3.48704543284839e-06
Step: 24480, train/learning_rate: 2.087101347569842e-05
Step: 24480, train/epoch: 5.825797080993652
Step: 24490, train/loss: 0.0
Step: 24490, train/grad_norm: 4.234632069710642e-05
Step: 24490, train/learning_rate: 2.0859115466009825e-05
Step: 24490, train/epoch: 5.828176975250244
Step: 24500, train/loss: 0.0
Step: 24500, train/grad_norm: 0.0001820476318243891
Step: 24500, train/learning_rate: 2.084721563733183e-05
Step: 24500, train/epoch: 5.830556869506836
Step: 24510, train/loss: 0.0
Step: 24510, train/grad_norm: 3.538439705152996e-05
Step: 24510, train/learning_rate: 2.083531580865383e-05
Step: 24510, train/epoch: 5.832936763763428
Step: 24520, train/loss: 0.0
Step: 24520, train/grad_norm: 1.918536867151488e-07
Step: 24520, train/learning_rate: 2.0823417798965238e-05
Step: 24520, train/epoch: 5.8353166580200195
Step: 24530, train/loss: 0.0
Step: 24530, train/grad_norm: 1.0213592105401403e-07
Step: 24530, train/learning_rate: 2.081151797028724e-05
Step: 24530, train/epoch: 5.837696552276611
Step: 24540, train/loss: 0.0
Step: 24540, train/grad_norm: 0.0018087252974510193
Step: 24540, train/learning_rate: 2.0799619960598648e-05
Step: 24540, train/epoch: 5.840075969696045
Step: 24550, train/loss: 0.0
Step: 24550, train/grad_norm: 9.734313977105558e-08
Step: 24550, train/learning_rate: 2.078772013192065e-05
Step: 24550, train/epoch: 5.842455863952637
Step: 24560, train/loss: 0.0
Step: 24560, train/grad_norm: 5.475024863699218e-06
Step: 24560, train/learning_rate: 2.0775820303242654e-05
Step: 24560, train/epoch: 5.8448357582092285
Step: 24570, train/loss: 0.0
Step: 24570, train/grad_norm: 8.669709495734423e-05
Step: 24570, train/learning_rate: 2.076392229355406e-05
Step: 24570, train/epoch: 5.84721565246582
Step: 24580, train/loss: 0.0
Step: 24580, train/grad_norm: 4.046254105105618e-07
Step: 24580, train/learning_rate: 2.0752022464876063e-05
Step: 24580, train/epoch: 5.849595546722412
Step: 24590, train/loss: 0.0
Step: 24590, train/grad_norm: 3.989349352195859e-06
Step: 24590, train/learning_rate: 2.074012445518747e-05
Step: 24590, train/epoch: 5.851975440979004
Step: 24600, train/loss: 0.0
Step: 24600, train/grad_norm: 3.148335963487625e-06
Step: 24600, train/learning_rate: 2.0728224626509473e-05
Step: 24600, train/epoch: 5.8543548583984375
Step: 24610, train/loss: 0.0
Step: 24610, train/grad_norm: 1.2317350694956986e-07
Step: 24610, train/learning_rate: 2.0716324797831476e-05
Step: 24610, train/epoch: 5.856734752655029
Step: 24620, train/loss: 0.0
Step: 24620, train/grad_norm: 2.349848671201471e-08
Step: 24620, train/learning_rate: 2.0704426788142882e-05
Step: 24620, train/epoch: 5.859114646911621
Step: 24630, train/loss: 0.0
Step: 24630, train/grad_norm: 0.0002744118100963533
Step: 24630, train/learning_rate: 2.0692526959464885e-05
Step: 24630, train/epoch: 5.861494541168213
Step: 24640, train/loss: 0.0
Step: 24640, train/grad_norm: 1.0964231478283182e-07
Step: 24640, train/learning_rate: 2.0680628949776292e-05
Step: 24640, train/epoch: 5.863874435424805
Step: 24650, train/loss: 0.0
Step: 24650, train/grad_norm: 8.47059425268526e-07
Step: 24650, train/learning_rate: 2.0668729121098295e-05
Step: 24650, train/epoch: 5.8662543296813965
Step: 24660, train/loss: 0.0
Step: 24660, train/grad_norm: 0.00013087822298984975
Step: 24660, train/learning_rate: 2.0656829292420298e-05
Step: 24660, train/epoch: 5.86863374710083
Step: 24670, train/loss: 0.0
Step: 24670, train/grad_norm: 0.0001769340451573953
Step: 24670, train/learning_rate: 2.0644931282731704e-05
Step: 24670, train/epoch: 5.871013641357422
Step: 24680, train/loss: 0.0
Step: 24680, train/grad_norm: 4.999496741220355e-05
Step: 24680, train/learning_rate: 2.0633031454053707e-05
Step: 24680, train/epoch: 5.873393535614014
Step: 24690, train/loss: 0.0
Step: 24690, train/grad_norm: 2.2848487901683256e-07
Step: 24690, train/learning_rate: 2.0621133444365114e-05
Step: 24690, train/epoch: 5.8757734298706055
Step: 24700, train/loss: 0.08709999918937683
Step: 24700, train/grad_norm: 402.44305419921875
Step: 24700, train/learning_rate: 2.0609233615687117e-05
Step: 24700, train/epoch: 5.878153324127197
Step: 24710, train/loss: 0.0
Step: 24710, train/grad_norm: 3.1370200304081663e-07
Step: 24710, train/learning_rate: 2.059733378700912e-05
Step: 24710, train/epoch: 5.880533218383789
Step: 24720, train/loss: 0.0
Step: 24720, train/grad_norm: 1.5988542145350948e-05
Step: 24720, train/learning_rate: 2.0585435777320527e-05
Step: 24720, train/epoch: 5.882913112640381
Step: 24730, train/loss: 0.0
Step: 24730, train/grad_norm: 1.0118565114680678e-05
Step: 24730, train/learning_rate: 2.057353594864253e-05
Step: 24730, train/epoch: 5.8852925300598145
Step: 24740, train/loss: 0.02419999986886978
Step: 24740, train/grad_norm: 7.961476740092621e-07
Step: 24740, train/learning_rate: 2.0561637938953936e-05
Step: 24740, train/epoch: 5.887672424316406
Step: 24750, train/loss: 0.0
Step: 24750, train/grad_norm: 6.785555888200179e-05
Step: 24750, train/learning_rate: 2.054973811027594e-05
Step: 24750, train/epoch: 5.890052318572998
Step: 24760, train/loss: 0.0
Step: 24760, train/grad_norm: 6.40023426967673e-06
Step: 24760, train/learning_rate: 2.0537838281597942e-05
Step: 24760, train/epoch: 5.89243221282959
Step: 24770, train/loss: 0.0
Step: 24770, train/grad_norm: 3.2438247217214666e-06
Step: 24770, train/learning_rate: 2.052594027190935e-05
Step: 24770, train/epoch: 5.894812107086182
Step: 24780, train/loss: 0.0
Step: 24780, train/grad_norm: 1.0249974735643264e-07
Step: 24780, train/learning_rate: 2.051404044323135e-05
Step: 24780, train/epoch: 5.897192001342773
Step: 24790, train/loss: 0.0
Step: 24790, train/grad_norm: 1.2259580728368746e-07
Step: 24790, train/learning_rate: 2.0502142433542758e-05
Step: 24790, train/epoch: 5.899571418762207
Step: 24800, train/loss: 0.0
Step: 24800, train/grad_norm: 0.0001250928035005927
Step: 24800, train/learning_rate: 2.049024260486476e-05
Step: 24800, train/epoch: 5.901951313018799
Step: 24810, train/loss: 0.08910000324249268
Step: 24810, train/grad_norm: 2.84666430161451e-06
Step: 24810, train/learning_rate: 2.0478342776186764e-05
Step: 24810, train/epoch: 5.904331207275391
Step: 24820, train/loss: 0.0
Step: 24820, train/grad_norm: 0.002410171553492546
Step: 24820, train/learning_rate: 2.046644476649817e-05
Step: 24820, train/epoch: 5.906711101531982
Step: 24830, train/loss: 0.0
Step: 24830, train/grad_norm: 0.03359091281890869
Step: 24830, train/learning_rate: 2.0454544937820174e-05
Step: 24830, train/epoch: 5.909090995788574
Step: 24840, train/loss: 0.0
Step: 24840, train/grad_norm: 0.00039417811785824597
Step: 24840, train/learning_rate: 2.044264692813158e-05
Step: 24840, train/epoch: 5.911470890045166
Step: 24850, train/loss: 0.0
Step: 24850, train/grad_norm: 4.840103429160081e-07
Step: 24850, train/learning_rate: 2.0430747099453583e-05
Step: 24850, train/epoch: 5.913850784301758
Step: 24860, train/loss: 0.0
Step: 24860, train/grad_norm: 6.403214456440764e-07
Step: 24860, train/learning_rate: 2.0418847270775586e-05
Step: 24860, train/epoch: 5.916230201721191
Step: 24870, train/loss: 0.0
Step: 24870, train/grad_norm: 2.2982534574111924e-05
Step: 24870, train/learning_rate: 2.0406949261086993e-05
Step: 24870, train/epoch: 5.918610095977783
Step: 24880, train/loss: 0.0
Step: 24880, train/grad_norm: 5.60363241675077e-06
Step: 24880, train/learning_rate: 2.0395049432408996e-05
Step: 24880, train/epoch: 5.920989990234375
Step: 24890, train/loss: 0.0
Step: 24890, train/grad_norm: 4.4584458009921946e-06
Step: 24890, train/learning_rate: 2.0383151422720402e-05
Step: 24890, train/epoch: 5.923369884490967
Step: 24900, train/loss: 0.0
Step: 24900, train/grad_norm: 0.00020120127010159194
Step: 24900, train/learning_rate: 2.0371251594042405e-05
Step: 24900, train/epoch: 5.925749778747559
Step: 24910, train/loss: 0.0
Step: 24910, train/grad_norm: 1.0070092457681312e-06
Step: 24910, train/learning_rate: 2.0359353584353812e-05
Step: 24910, train/epoch: 5.92812967300415
Step: 24920, train/loss: 0.0
Step: 24920, train/grad_norm: 5.947810066686543e-08
Step: 24920, train/learning_rate: 2.0347453755675815e-05
Step: 24920, train/epoch: 5.930509090423584
Step: 24930, train/loss: 0.0
Step: 24930, train/grad_norm: 8.109846589832159e-07
Step: 24930, train/learning_rate: 2.0335553926997818e-05
Step: 24930, train/epoch: 5.932888984680176
Step: 24940, train/loss: 0.0
Step: 24940, train/grad_norm: 3.2466984976053936e-06
Step: 24940, train/learning_rate: 2.0323655917309225e-05
Step: 24940, train/epoch: 5.935268878936768
Step: 24950, train/loss: 0.0
Step: 24950, train/grad_norm: 3.7765679735457525e-05
Step: 24950, train/learning_rate: 2.0311756088631228e-05
Step: 24950, train/epoch: 5.937648773193359
Step: 24960, train/loss: 0.0
Step: 24960, train/grad_norm: 1.083639631360711e-06
Step: 24960, train/learning_rate: 2.0299858078942634e-05
Step: 24960, train/epoch: 5.940028667449951
Step: 24970, train/loss: 0.0
Step: 24970, train/grad_norm: 2.8486651899584103e-06
Step: 24970, train/learning_rate: 2.0287958250264637e-05
Step: 24970, train/epoch: 5.942408561706543
Step: 24980, train/loss: 0.0
Step: 24980, train/grad_norm: 8.899198178369261e-08
Step: 24980, train/learning_rate: 2.027605842158664e-05
Step: 24980, train/epoch: 5.944787979125977
Step: 24990, train/loss: 0.0
Step: 24990, train/grad_norm: 1.4395317293747212e-07
Step: 24990, train/learning_rate: 2.0264160411898047e-05
Step: 24990, train/epoch: 5.947167873382568
Step: 25000, train/loss: 0.0
Step: 25000, train/grad_norm: 6.8014569478691556e-06
Step: 25000, train/learning_rate: 2.025226058322005e-05
Step: 25000, train/epoch: 5.94954776763916
Step: 25010, train/loss: 0.0
Step: 25010, train/grad_norm: 4.001908382633701e-05
Step: 25010, train/learning_rate: 2.0240362573531456e-05
Step: 25010, train/epoch: 5.951927661895752
Step: 25020, train/loss: 0.0
Step: 25020, train/grad_norm: 4.66137976218306e-07
Step: 25020, train/learning_rate: 2.022846274485346e-05
Step: 25020, train/epoch: 5.954307556152344
Step: 25030, train/loss: 0.0
Step: 25030, train/grad_norm: 0.0009102369658648968
Step: 25030, train/learning_rate: 2.0216562916175462e-05
Step: 25030, train/epoch: 5.9566874504089355
Step: 25040, train/loss: 0.0
Step: 25040, train/grad_norm: 2.7671904945236747e-07
Step: 25040, train/learning_rate: 2.020466490648687e-05
Step: 25040, train/epoch: 5.959067344665527
Step: 25050, train/loss: 0.0
Step: 25050, train/grad_norm: 8.864431947586127e-06
Step: 25050, train/learning_rate: 2.0192765077808872e-05
Step: 25050, train/epoch: 5.961446762084961
Step: 25060, train/loss: 0.0
Step: 25060, train/grad_norm: 8.046088623814285e-06
Step: 25060, train/learning_rate: 2.018086706812028e-05
Step: 25060, train/epoch: 5.963826656341553
Step: 25070, train/loss: 0.0
Step: 25070, train/grad_norm: 2.936074906756403e-06
Step: 25070, train/learning_rate: 2.016896723944228e-05
Step: 25070, train/epoch: 5.9662065505981445
Step: 25080, train/loss: 0.0
Step: 25080, train/grad_norm: 1.72441632457776e-05
Step: 25080, train/learning_rate: 2.0157067410764284e-05
Step: 25080, train/epoch: 5.968586444854736
Step: 25090, train/loss: 0.0
Step: 25090, train/grad_norm: 5.722026344301412e-06
Step: 25090, train/learning_rate: 2.014516940107569e-05
Step: 25090, train/epoch: 5.970966339111328
Step: 25100, train/loss: 0.0
Step: 25100, train/grad_norm: 7.658799586351961e-05
Step: 25100, train/learning_rate: 2.0133269572397694e-05
Step: 25100, train/epoch: 5.97334623336792
Step: 25110, train/loss: 0.0
Step: 25110, train/grad_norm: 3.151716373395175e-05
Step: 25110, train/learning_rate: 2.01213715627091e-05
Step: 25110, train/epoch: 5.9757256507873535
Step: 25120, train/loss: 0.0
Step: 25120, train/grad_norm: 4.818333536604769e-07
Step: 25120, train/learning_rate: 2.0109471734031104e-05
Step: 25120, train/epoch: 5.978105545043945
Step: 25130, train/loss: 0.0
Step: 25130, train/grad_norm: 0.0005497407983057201
Step: 25130, train/learning_rate: 2.0097571905353107e-05
Step: 25130, train/epoch: 5.980485439300537
Step: 25140, train/loss: 0.0
Step: 25140, train/grad_norm: 2.862711653506267e-06
Step: 25140, train/learning_rate: 2.0085673895664513e-05
Step: 25140, train/epoch: 5.982865333557129
Step: 25150, train/loss: 0.0
Step: 25150, train/grad_norm: 3.701795321831014e-06
Step: 25150, train/learning_rate: 2.0073774066986516e-05
Step: 25150, train/epoch: 5.985245227813721
Step: 25160, train/loss: 0.0
Step: 25160, train/grad_norm: 6.832654548816208e-07
Step: 25160, train/learning_rate: 2.0061876057297923e-05
Step: 25160, train/epoch: 5.9876251220703125
Step: 25170, train/loss: 0.0
Step: 25170, train/grad_norm: 0.00018583843484520912
Step: 25170, train/learning_rate: 2.0049976228619926e-05
Step: 25170, train/epoch: 5.990004539489746
Step: 25180, train/loss: 0.0
Step: 25180, train/grad_norm: 3.619852805059054e-06
Step: 25180, train/learning_rate: 2.003807639994193e-05
Step: 25180, train/epoch: 5.992384433746338
Step: 25190, train/loss: 0.0
Step: 25190, train/grad_norm: 3.767096359297284e-07
Step: 25190, train/learning_rate: 2.0026178390253335e-05
Step: 25190, train/epoch: 5.99476432800293
Step: 25200, train/loss: 0.0
Step: 25200, train/grad_norm: 3.165121711390384e-07
Step: 25200, train/learning_rate: 2.0014278561575338e-05
Step: 25200, train/epoch: 5.9971442222595215
Step: 25210, train/loss: 0.0
Step: 25210, train/grad_norm: 1.3185775742385886e-06
Step: 25210, train/learning_rate: 2.0002380551886745e-05
Step: 25210, train/epoch: 5.999524116516113
Step: 25212, eval/loss: 0.020162349566817284
Step: 25212, eval/accuracy: 0.9973621964454651
Step: 25212, eval/f1: 0.9972146153450012
Step: 25212, eval/runtime: 735.702392578125
Step: 25212, eval/samples_per_second: 9.791000366210938
Step: 25212, eval/steps_per_second: 1.225000023841858
Step: 25212, train/epoch: 6.0
Step: 25220, train/loss: 0.0
Step: 25220, train/grad_norm: 6.322496483335271e-05
Step: 25220, train/learning_rate: 1.9990480723208748e-05
Step: 25220, train/epoch: 6.001904010772705
Step: 25230, train/loss: 0.0
Step: 25230, train/grad_norm: 3.28378846461419e-05
Step: 25230, train/learning_rate: 1.997858089453075e-05
Step: 25230, train/epoch: 6.004283905029297
Step: 25240, train/loss: 0.0
Step: 25240, train/grad_norm: 4.312130670314218e-07
Step: 25240, train/learning_rate: 1.9966682884842157e-05
Step: 25240, train/epoch: 6.0066633224487305
Step: 25250, train/loss: 0.0
Step: 25250, train/grad_norm: 5.985624375171028e-05
Step: 25250, train/learning_rate: 1.995478305616416e-05
Step: 25250, train/epoch: 6.009043216705322
Step: 25260, train/loss: 0.0
Step: 25260, train/grad_norm: 0.0011127976467832923
Step: 25260, train/learning_rate: 1.9942885046475567e-05
Step: 25260, train/epoch: 6.011423110961914
Step: 25270, train/loss: 0.0
Step: 25270, train/grad_norm: 4.589418949763058e-06
Step: 25270, train/learning_rate: 1.993098521779757e-05
Step: 25270, train/epoch: 6.013803005218506
Step: 25280, train/loss: 0.0
Step: 25280, train/grad_norm: 8.270104956409341e-08
Step: 25280, train/learning_rate: 1.9919085389119573e-05
Step: 25280, train/epoch: 6.016182899475098
Step: 25290, train/loss: 0.0
Step: 25290, train/grad_norm: 5.335820333129959e-06
Step: 25290, train/learning_rate: 1.990718737943098e-05
Step: 25290, train/epoch: 6.0185627937316895
Step: 25300, train/loss: 0.0
Step: 25300, train/grad_norm: 1.6537600458832458e-05
Step: 25300, train/learning_rate: 1.9895287550752982e-05
Step: 25300, train/epoch: 6.020942211151123
Step: 25310, train/loss: 0.0
Step: 25310, train/grad_norm: 9.124718758357631e-07
Step: 25310, train/learning_rate: 1.988338954106439e-05
Step: 25310, train/epoch: 6.023322105407715
Step: 25320, train/loss: 0.0
Step: 25320, train/grad_norm: 0.0002001027314690873
Step: 25320, train/learning_rate: 1.9871489712386392e-05
Step: 25320, train/epoch: 6.025701999664307
Step: 25330, train/loss: 0.0
Step: 25330, train/grad_norm: 3.6269040720071644e-05
Step: 25330, train/learning_rate: 1.9859589883708395e-05
Step: 25330, train/epoch: 6.028081893920898
Step: 25340, train/loss: 0.0
Step: 25340, train/grad_norm: 8.25912991331279e-07
Step: 25340, train/learning_rate: 1.98476918740198e-05
Step: 25340, train/epoch: 6.03046178817749
Step: 25350, train/loss: 0.0
Step: 25350, train/grad_norm: 1.20983077067649e-06
Step: 25350, train/learning_rate: 1.9835792045341805e-05
Step: 25350, train/epoch: 6.032841682434082
Step: 25360, train/loss: 0.0
Step: 25360, train/grad_norm: 2.9671859920199495e-06
Step: 25360, train/learning_rate: 1.982389403565321e-05
Step: 25360, train/epoch: 6.035221099853516
Step: 25370, train/loss: 0.0
Step: 25370, train/grad_norm: 3.5290370306029217e-06
Step: 25370, train/learning_rate: 1.9811994206975214e-05
Step: 25370, train/epoch: 6.037600994110107
Step: 25380, train/loss: 0.0
Step: 25380, train/grad_norm: 9.4013648777036e-07
Step: 25380, train/learning_rate: 1.9800094378297217e-05
Step: 25380, train/epoch: 6.039980888366699
Step: 25390, train/loss: 0.0
Step: 25390, train/grad_norm: 1.4341128917294554e-05
Step: 25390, train/learning_rate: 1.9788196368608624e-05
Step: 25390, train/epoch: 6.042360782623291
Step: 25400, train/loss: 0.0
Step: 25400, train/grad_norm: 9.959241288015619e-05
Step: 25400, train/learning_rate: 1.9776296539930627e-05
Step: 25400, train/epoch: 6.044740676879883
Step: 25410, train/loss: 0.0
Step: 25410, train/grad_norm: 7.023995607369216e-08
Step: 25410, train/learning_rate: 1.9764398530242033e-05
Step: 25410, train/epoch: 6.047120571136475
Step: 25420, train/loss: 0.0
Step: 25420, train/grad_norm: 1.5990777058050298e-07
Step: 25420, train/learning_rate: 1.9752498701564036e-05
Step: 25420, train/epoch: 6.049500465393066
Step: 25430, train/loss: 0.0
Step: 25430, train/grad_norm: 3.653711999618281e-08
Step: 25430, train/learning_rate: 1.974059887288604e-05
Step: 25430, train/epoch: 6.0518798828125
Step: 25440, train/loss: 0.0
Step: 25440, train/grad_norm: 1.0906414900091477e-06
Step: 25440, train/learning_rate: 1.9728700863197446e-05
Step: 25440, train/epoch: 6.054259777069092
Step: 25450, train/loss: 0.0
Step: 25450, train/grad_norm: 5.926847848058969e-07
Step: 25450, train/learning_rate: 1.971680103451945e-05
Step: 25450, train/epoch: 6.056639671325684
Step: 25460, train/loss: 0.0
Step: 25460, train/grad_norm: 2.7841528208227828e-05
Step: 25460, train/learning_rate: 1.9704903024830855e-05
Step: 25460, train/epoch: 6.059019565582275
Step: 25470, train/loss: 0.0
Step: 25470, train/grad_norm: 3.0940012948121876e-05
Step: 25470, train/learning_rate: 1.969300319615286e-05
Step: 25470, train/epoch: 6.061399459838867
Step: 25480, train/loss: 0.0
Step: 25480, train/grad_norm: 2.2348235972913244e-07
Step: 25480, train/learning_rate: 1.968110336747486e-05
Step: 25480, train/epoch: 6.063779354095459
Step: 25490, train/loss: 0.0
Step: 25490, train/grad_norm: 2.6444487843946263e-07
Step: 25490, train/learning_rate: 1.9669205357786268e-05
Step: 25490, train/epoch: 6.066158771514893
Step: 25500, train/loss: 0.0
Step: 25500, train/grad_norm: 5.047815193393035e-06
Step: 25500, train/learning_rate: 1.965730552910827e-05
Step: 25500, train/epoch: 6.068538665771484
Step: 25510, train/loss: 0.0
Step: 25510, train/grad_norm: 1.1255842480295541e-07
Step: 25510, train/learning_rate: 1.9645407519419678e-05
Step: 25510, train/epoch: 6.070918560028076
Step: 25520, train/loss: 0.0
Step: 25520, train/grad_norm: 4.2683740275606397e-07
Step: 25520, train/learning_rate: 1.963350769074168e-05
Step: 25520, train/epoch: 6.073298454284668
Step: 25530, train/loss: 0.0
Step: 25530, train/grad_norm: 5.026076905778609e-07
Step: 25530, train/learning_rate: 1.9621607862063684e-05
Step: 25530, train/epoch: 6.07567834854126
Step: 25540, train/loss: 0.0
Step: 25540, train/grad_norm: 3.190788993379101e-05
Step: 25540, train/learning_rate: 1.960970985237509e-05
Step: 25540, train/epoch: 6.078058242797852
Step: 25550, train/loss: 0.0
Step: 25550, train/grad_norm: 3.852991358144209e-05
Step: 25550, train/learning_rate: 1.9597810023697093e-05
Step: 25550, train/epoch: 6.080437660217285
Step: 25560, train/loss: 0.0
Step: 25560, train/grad_norm: 2.0503023279161425e-07
Step: 25560, train/learning_rate: 1.95859120140085e-05
Step: 25560, train/epoch: 6.082817554473877
Step: 25570, train/loss: 0.0
Step: 25570, train/grad_norm: 6.397752372322429e-07
Step: 25570, train/learning_rate: 1.9574012185330503e-05
Step: 25570, train/epoch: 6.085197448730469
Step: 25580, train/loss: 0.0
Step: 25580, train/grad_norm: 2.9653702426912787e-07
Step: 25580, train/learning_rate: 1.956211417564191e-05
Step: 25580, train/epoch: 6.0875773429870605
Step: 25590, train/loss: 0.0
Step: 25590, train/grad_norm: 9.368328619530075e-07
Step: 25590, train/learning_rate: 1.9550214346963912e-05
Step: 25590, train/epoch: 6.089957237243652
Step: 25600, train/loss: 0.0
Step: 25600, train/grad_norm: 5.713746418223309e-07
Step: 25600, train/learning_rate: 1.9538314518285915e-05
Step: 25600, train/epoch: 6.092337131500244
Step: 25610, train/loss: 0.0
Step: 25610, train/grad_norm: 6.122878858150216e-06
Step: 25610, train/learning_rate: 1.9526416508597322e-05
Step: 25610, train/epoch: 6.094717025756836
Step: 25620, train/loss: 0.0
Step: 25620, train/grad_norm: 3.763819051982864e-07
Step: 25620, train/learning_rate: 1.9514516679919325e-05
Step: 25620, train/epoch: 6.0970964431762695
Step: 25630, train/loss: 0.0
Step: 25630, train/grad_norm: 3.3262958254454134e-07
Step: 25630, train/learning_rate: 1.950261867023073e-05
Step: 25630, train/epoch: 6.099476337432861
Step: 25640, train/loss: 0.0
Step: 25640, train/grad_norm: 1.736939481133959e-08
Step: 25640, train/learning_rate: 1.9490718841552734e-05
Step: 25640, train/epoch: 6.101856231689453
Step: 25650, train/loss: 0.0
Step: 25650, train/grad_norm: 8.583240429516081e-08
Step: 25650, train/learning_rate: 1.9478819012874737e-05
Step: 25650, train/epoch: 6.104236125946045
Step: 25660, train/loss: 0.0
Step: 25660, train/grad_norm: 2.1090249902044889e-07
Step: 25660, train/learning_rate: 1.9466921003186144e-05
Step: 25660, train/epoch: 6.106616020202637
Step: 25670, train/loss: 0.0
Step: 25670, train/grad_norm: 4.362200201057931e-08
Step: 25670, train/learning_rate: 1.9455021174508147e-05
Step: 25670, train/epoch: 6.1089959144592285
Step: 25680, train/loss: 0.0
Step: 25680, train/grad_norm: 3.623027168941917e-06
Step: 25680, train/learning_rate: 1.9443123164819553e-05
Step: 25680, train/epoch: 6.111375331878662
Step: 25690, train/loss: 0.0
Step: 25690, train/grad_norm: 6.513619865700093e-08
Step: 25690, train/learning_rate: 1.9431223336141557e-05
Step: 25690, train/epoch: 6.113755226135254
Step: 25700, train/loss: 0.0
Step: 25700, train/grad_norm: 6.603643782909785e-07
Step: 25700, train/learning_rate: 1.941932350746356e-05
Step: 25700, train/epoch: 6.116135120391846
Step: 25710, train/loss: 0.0
Step: 25710, train/grad_norm: 1.0994656740592745e-08
Step: 25710, train/learning_rate: 1.9407425497774966e-05
Step: 25710, train/epoch: 6.1185150146484375
Step: 25720, train/loss: 0.0
Step: 25720, train/grad_norm: 2.824878642115891e-08
Step: 25720, train/learning_rate: 1.939552566909697e-05
Step: 25720, train/epoch: 6.120894908905029
Step: 25730, train/loss: 0.0
Step: 25730, train/grad_norm: 2.6542318209976656e-07
Step: 25730, train/learning_rate: 1.9383627659408376e-05
Step: 25730, train/epoch: 6.123274803161621
Step: 25740, train/loss: 0.0
Step: 25740, train/grad_norm: 1.810721350636868e-08
Step: 25740, train/learning_rate: 1.937172783073038e-05
Step: 25740, train/epoch: 6.125654220581055
Step: 25750, train/loss: 0.0
Step: 25750, train/grad_norm: 7.861782069085166e-05
Step: 25750, train/learning_rate: 1.935982800205238e-05
Step: 25750, train/epoch: 6.1280341148376465
Step: 25760, train/loss: 0.0
Step: 25760, train/grad_norm: 1.5530056771240197e-05
Step: 25760, train/learning_rate: 1.9347929992363788e-05
Step: 25760, train/epoch: 6.130414009094238
Step: 25770, train/loss: 0.0
Step: 25770, train/grad_norm: 4.208017969631328e-07
Step: 25770, train/learning_rate: 1.933603016368579e-05
Step: 25770, train/epoch: 6.13279390335083
Step: 25780, train/loss: 0.0
Step: 25780, train/grad_norm: 2.1292134988470934e-05
Step: 25780, train/learning_rate: 1.9324132153997198e-05
Step: 25780, train/epoch: 6.135173797607422
Step: 25790, train/loss: 0.0
Step: 25790, train/grad_norm: 1.0208551515233921e-07
Step: 25790, train/learning_rate: 1.93122323253192e-05
Step: 25790, train/epoch: 6.137553691864014
Step: 25800, train/loss: 0.0
Step: 25800, train/grad_norm: 0.0010782105382531881
Step: 25800, train/learning_rate: 1.9300332496641204e-05
Step: 25800, train/epoch: 6.1399335861206055
Step: 25810, train/loss: 0.0
Step: 25810, train/grad_norm: 2.3705649709881982e-07
Step: 25810, train/learning_rate: 1.928843448695261e-05
Step: 25810, train/epoch: 6.142313003540039
Step: 25820, train/loss: 0.0
Step: 25820, train/grad_norm: 5.917927410337143e-06
Step: 25820, train/learning_rate: 1.9276534658274613e-05
Step: 25820, train/epoch: 6.144692897796631
Step: 25830, train/loss: 0.0
Step: 25830, train/grad_norm: 1.6040287675878062e-07
Step: 25830, train/learning_rate: 1.926463664858602e-05
Step: 25830, train/epoch: 6.147072792053223
Step: 25840, train/loss: 0.0
Step: 25840, train/grad_norm: 3.196126954208012e-07
Step: 25840, train/learning_rate: 1.9252736819908023e-05
Step: 25840, train/epoch: 6.1494526863098145
Step: 25850, train/loss: 0.0
Step: 25850, train/grad_norm: 0.00012317043729126453
Step: 25850, train/learning_rate: 1.9240836991230026e-05
Step: 25850, train/epoch: 6.151832580566406
Step: 25860, train/loss: 0.0
Step: 25860, train/grad_norm: 6.529352504003327e-06
Step: 25860, train/learning_rate: 1.9228938981541432e-05
Step: 25860, train/epoch: 6.154212474822998
Step: 25870, train/loss: 0.0
Step: 25870, train/grad_norm: 8.570723002776504e-05
Step: 25870, train/learning_rate: 1.9217039152863435e-05
Step: 25870, train/epoch: 6.156591892242432
Step: 25880, train/loss: 0.0
Step: 25880, train/grad_norm: 2.3668924598041485e-07
Step: 25880, train/learning_rate: 1.9205141143174842e-05
Step: 25880, train/epoch: 6.158971786499023
Step: 25890, train/loss: 0.0
Step: 25890, train/grad_norm: 6.215685033339469e-08
Step: 25890, train/learning_rate: 1.9193241314496845e-05
Step: 25890, train/epoch: 6.161351680755615
Step: 25900, train/loss: 0.0
Step: 25900, train/grad_norm: 2.2962333900977683e-07
Step: 25900, train/learning_rate: 1.9181341485818848e-05
Step: 25900, train/epoch: 6.163731575012207
Step: 25910, train/loss: 0.0
Step: 25910, train/grad_norm: 1.2067589523212519e-06
Step: 25910, train/learning_rate: 1.9169443476130255e-05
Step: 25910, train/epoch: 6.166111469268799
Step: 25920, train/loss: 0.0
Step: 25920, train/grad_norm: 5.834304062091178e-08
Step: 25920, train/learning_rate: 1.9157543647452258e-05
Step: 25920, train/epoch: 6.168491363525391
Step: 25930, train/loss: 0.0
Step: 25930, train/grad_norm: 3.077982455579331e-06
Step: 25930, train/learning_rate: 1.9145645637763664e-05
Step: 25930, train/epoch: 6.170870780944824
Step: 25940, train/loss: 0.0
Step: 25940, train/grad_norm: 7.408167448375025e-07
Step: 25940, train/learning_rate: 1.9133745809085667e-05
Step: 25940, train/epoch: 6.173250675201416
Step: 25950, train/loss: 0.0
Step: 25950, train/grad_norm: 0.00014266731159295887
Step: 25950, train/learning_rate: 1.912184598040767e-05
Step: 25950, train/epoch: 6.175630569458008
Step: 25960, train/loss: 0.0
Step: 25960, train/grad_norm: 5.614315679025594e-09
Step: 25960, train/learning_rate: 1.9109947970719077e-05
Step: 25960, train/epoch: 6.1780104637146
Step: 25970, train/loss: 0.0
Step: 25970, train/grad_norm: 4.3120769532833947e-07
Step: 25970, train/learning_rate: 1.909804814204108e-05
Step: 25970, train/epoch: 6.180390357971191
Step: 25980, train/loss: 0.0
Step: 25980, train/grad_norm: 0.00016503635561093688
Step: 25980, train/learning_rate: 1.9086150132352486e-05
Step: 25980, train/epoch: 6.182770252227783
Step: 25990, train/loss: 0.0
Step: 25990, train/grad_norm: 3.3244414225919172e-06
Step: 25990, train/learning_rate: 1.907425030367449e-05
Step: 25990, train/epoch: 6.185150146484375
Step: 26000, train/loss: 0.0
Step: 26000, train/grad_norm: 1.0052120558157185e-07
Step: 26000, train/learning_rate: 1.9062350474996492e-05
Step: 26000, train/epoch: 6.187529563903809
Step: 26010, train/loss: 0.0
Step: 26010, train/grad_norm: 3.6901079170093e-07
Step: 26010, train/learning_rate: 1.90504524653079e-05
Step: 26010, train/epoch: 6.1899094581604
Step: 26020, train/loss: 0.0
Step: 26020, train/grad_norm: 8.30208762181428e-07
Step: 26020, train/learning_rate: 1.9038552636629902e-05
Step: 26020, train/epoch: 6.192289352416992
Step: 26030, train/loss: 0.0
Step: 26030, train/grad_norm: 6.918438884895295e-06
Step: 26030, train/learning_rate: 1.902665462694131e-05
Step: 26030, train/epoch: 6.194669246673584
Step: 26040, train/loss: 0.0
Step: 26040, train/grad_norm: 3.877531185025873e-09
Step: 26040, train/learning_rate: 1.901475479826331e-05
Step: 26040, train/epoch: 6.197049140930176
Step: 26050, train/loss: 0.0
Step: 26050, train/grad_norm: 4.188256298220949e-06
Step: 26050, train/learning_rate: 1.9002854969585314e-05
Step: 26050, train/epoch: 6.199429035186768
Step: 26060, train/loss: 0.0
Step: 26060, train/grad_norm: 3.0715973480255343e-06
Step: 26060, train/learning_rate: 1.899095695989672e-05
Step: 26060, train/epoch: 6.201808452606201
Step: 26070, train/loss: 0.0
Step: 26070, train/grad_norm: 4.906217509415001e-05
Step: 26070, train/learning_rate: 1.8979057131218724e-05
Step: 26070, train/epoch: 6.204188346862793
Step: 26080, train/loss: 0.0
Step: 26080, train/grad_norm: 2.268906058588982e-07
Step: 26080, train/learning_rate: 1.896715912153013e-05
Step: 26080, train/epoch: 6.206568241119385
Step: 26090, train/loss: 0.0
Step: 26090, train/grad_norm: 1.250336811864372e-08
Step: 26090, train/learning_rate: 1.8955259292852134e-05
Step: 26090, train/epoch: 6.208948135375977
Step: 26100, train/loss: 0.0
Step: 26100, train/grad_norm: 1.6703037886145466e-07
Step: 26100, train/learning_rate: 1.8943359464174137e-05
Step: 26100, train/epoch: 6.211328029632568
Step: 26110, train/loss: 0.0
Step: 26110, train/grad_norm: 4.6994823605928104e-06
Step: 26110, train/learning_rate: 1.8931461454485543e-05
Step: 26110, train/epoch: 6.21370792388916
Step: 26120, train/loss: 0.0
Step: 26120, train/grad_norm: 6.243070060918399e-08
Step: 26120, train/learning_rate: 1.8919561625807546e-05
Step: 26120, train/epoch: 6.216087341308594
Step: 26130, train/loss: 0.0
Step: 26130, train/grad_norm: 4.110308537974561e-08
Step: 26130, train/learning_rate: 1.8907663616118953e-05
Step: 26130, train/epoch: 6.2184672355651855
Step: 26140, train/loss: 0.0
Step: 26140, train/grad_norm: 3.938393638236448e-05
Step: 26140, train/learning_rate: 1.8895763787440956e-05
Step: 26140, train/epoch: 6.220847129821777
Step: 26150, train/loss: 0.0
Step: 26150, train/grad_norm: 4.25582015850523e-07
Step: 26150, train/learning_rate: 1.888386395876296e-05
Step: 26150, train/epoch: 6.223227024078369
Step: 26160, train/loss: 0.0
Step: 26160, train/grad_norm: 4.6337689241227054e-08
Step: 26160, train/learning_rate: 1.8871965949074365e-05
Step: 26160, train/epoch: 6.225606918334961
Step: 26170, train/loss: 0.0
Step: 26170, train/grad_norm: 1.5647641404825663e-08
Step: 26170, train/learning_rate: 1.8860066120396368e-05
Step: 26170, train/epoch: 6.227986812591553
Step: 26180, train/loss: 0.0
Step: 26180, train/grad_norm: 1.9718910948540724e-08
Step: 26180, train/learning_rate: 1.8848168110707775e-05
Step: 26180, train/epoch: 6.2303667068481445
Step: 26190, train/loss: 0.0
Step: 26190, train/grad_norm: 2.6995239821303585e-08
Step: 26190, train/learning_rate: 1.8836268282029778e-05
Step: 26190, train/epoch: 6.232746124267578
Step: 26200, train/loss: 0.0
Step: 26200, train/grad_norm: 0.0004060555074829608
Step: 26200, train/learning_rate: 1.882436845335178e-05
Step: 26200, train/epoch: 6.23512601852417
Step: 26210, train/loss: 0.0
Step: 26210, train/grad_norm: 8.761659046285786e-06
Step: 26210, train/learning_rate: 1.8812470443663187e-05
Step: 26210, train/epoch: 6.237505912780762
Step: 26220, train/loss: 0.0
Step: 26220, train/grad_norm: 5.927177859632593e-09
Step: 26220, train/learning_rate: 1.880057061498519e-05
Step: 26220, train/epoch: 6.2398858070373535
Step: 26230, train/loss: 0.0008999999845400453
Step: 26230, train/grad_norm: 3.298524461570196e-06
Step: 26230, train/learning_rate: 1.8788672605296597e-05
Step: 26230, train/epoch: 6.242265701293945
Step: 26240, train/loss: 0.0
Step: 26240, train/grad_norm: 1.9247175941927708e-07
Step: 26240, train/learning_rate: 1.87767727766186e-05
Step: 26240, train/epoch: 6.244645595550537
Step: 26250, train/loss: 0.0
Step: 26250, train/grad_norm: 1.1029906090698205e-06
Step: 26250, train/learning_rate: 1.8764874766930006e-05
Step: 26250, train/epoch: 6.247025012969971
Step: 26260, train/loss: 0.0
Step: 26260, train/grad_norm: 4.256879151398607e-07
Step: 26260, train/learning_rate: 1.875297493825201e-05
Step: 26260, train/epoch: 6.2494049072265625
Step: 26270, train/loss: 0.17270000278949738
Step: 26270, train/grad_norm: 80.445556640625
Step: 26270, train/learning_rate: 1.8741075109574012e-05
Step: 26270, train/epoch: 6.251784801483154
Step: 26280, train/loss: 0.0
Step: 26280, train/grad_norm: 0.0004989375011064112
Step: 26280, train/learning_rate: 1.872917709988542e-05
Step: 26280, train/epoch: 6.254164695739746
Step: 26290, train/loss: 0.0
Step: 26290, train/grad_norm: 0.0002572597877588123
Step: 26290, train/learning_rate: 1.8717277271207422e-05
Step: 26290, train/epoch: 6.256544589996338
Step: 26300, train/loss: 9.999999747378752e-05
Step: 26300, train/grad_norm: 0.002444384852424264
Step: 26300, train/learning_rate: 1.870537926151883e-05
Step: 26300, train/epoch: 6.25892448425293
Step: 26310, train/loss: 0.0
Step: 26310, train/grad_norm: 3.0440221507888054e-06
Step: 26310, train/learning_rate: 1.869347943284083e-05
Step: 26310, train/epoch: 6.2613043785095215
Step: 26320, train/loss: 0.0
Step: 26320, train/grad_norm: 8.027765579754487e-05
Step: 26320, train/learning_rate: 1.8681579604162835e-05
Step: 26320, train/epoch: 6.263683795928955
Step: 26330, train/loss: 0.0
Step: 26330, train/grad_norm: 1.975737905013375e-05
Step: 26330, train/learning_rate: 1.866968159447424e-05
Step: 26330, train/epoch: 6.266063690185547
Step: 26340, train/loss: 0.0
Step: 26340, train/grad_norm: 1.705496288195718e-05
Step: 26340, train/learning_rate: 1.8657781765796244e-05
Step: 26340, train/epoch: 6.268443584442139
Step: 26350, train/loss: 0.0
Step: 26350, train/grad_norm: 1.017228669297765e-06
Step: 26350, train/learning_rate: 1.864588375610765e-05
Step: 26350, train/epoch: 6.2708234786987305
Step: 26360, train/loss: 0.0
Step: 26360, train/grad_norm: 5.252077244222164e-06
Step: 26360, train/learning_rate: 1.8633983927429654e-05
Step: 26360, train/epoch: 6.273203372955322
Step: 26370, train/loss: 0.0
Step: 26370, train/grad_norm: 5.516968303709291e-05
Step: 26370, train/learning_rate: 1.8622084098751657e-05
Step: 26370, train/epoch: 6.275583267211914
Step: 26380, train/loss: 0.0
Step: 26380, train/grad_norm: 1.6397153785874252e-06
Step: 26380, train/learning_rate: 1.8610186089063063e-05
Step: 26380, train/epoch: 6.277962684631348
Step: 26390, train/loss: 0.0
Step: 26390, train/grad_norm: 5.825017979077529e-07
Step: 26390, train/learning_rate: 1.8598286260385066e-05
Step: 26390, train/epoch: 6.2803425788879395
Step: 26400, train/loss: 0.0
Step: 26400, train/grad_norm: 0.0005296799936331809
Step: 26400, train/learning_rate: 1.8586388250696473e-05
Step: 26400, train/epoch: 6.282722473144531
Step: 26410, train/loss: 0.0
Step: 26410, train/grad_norm: 8.626147973700427e-07
Step: 26410, train/learning_rate: 1.8574488422018476e-05
Step: 26410, train/epoch: 6.285102367401123
Step: 26420, train/loss: 0.0
Step: 26420, train/grad_norm: 2.9186992378527066e-06
Step: 26420, train/learning_rate: 1.856258859334048e-05
Step: 26420, train/epoch: 6.287482261657715
Step: 26430, train/loss: 0.0
Step: 26430, train/grad_norm: 0.0003020325966645032
Step: 26430, train/learning_rate: 1.8550690583651885e-05
Step: 26430, train/epoch: 6.289862155914307
Step: 26440, train/loss: 0.0
Step: 26440, train/grad_norm: 5.1043075472989585e-06
Step: 26440, train/learning_rate: 1.853879075497389e-05
Step: 26440, train/epoch: 6.29224157333374
Step: 26450, train/loss: 0.0
Step: 26450, train/grad_norm: 0.0001778796431608498
Step: 26450, train/learning_rate: 1.8526892745285295e-05
Step: 26450, train/epoch: 6.294621467590332
Step: 26460, train/loss: 0.0
Step: 26460, train/grad_norm: 7.291364454431459e-06
Step: 26460, train/learning_rate: 1.8514992916607298e-05
Step: 26460, train/epoch: 6.297001361846924
Step: 26470, train/loss: 0.0
Step: 26470, train/grad_norm: 3.7395732306322316e-06
Step: 26470, train/learning_rate: 1.85030930879293e-05
Step: 26470, train/epoch: 6.299381256103516
Step: 26480, train/loss: 0.0
Step: 26480, train/grad_norm: 5.580536708293948e-07
Step: 26480, train/learning_rate: 1.8491195078240708e-05
Step: 26480, train/epoch: 6.301761150360107
Step: 26490, train/loss: 0.0
Step: 26490, train/grad_norm: 4.573155092657544e-05
Step: 26490, train/learning_rate: 1.847929524956271e-05
Step: 26490, train/epoch: 6.304141044616699
Step: 26500, train/loss: 0.0
Step: 26500, train/grad_norm: 2.6701873139245436e-05
Step: 26500, train/learning_rate: 1.8467397239874117e-05
Step: 26500, train/epoch: 6.306520938873291
Step: 26510, train/loss: 0.0
Step: 26510, train/grad_norm: 2.718412815738702e-06
Step: 26510, train/learning_rate: 1.845549741119612e-05
Step: 26510, train/epoch: 6.308900356292725
Step: 26520, train/loss: 0.0
Step: 26520, train/grad_norm: 0.00013199710519984365
Step: 26520, train/learning_rate: 1.8443597582518123e-05
Step: 26520, train/epoch: 6.311280250549316
Step: 26530, train/loss: 0.0
Step: 26530, train/grad_norm: 2.683154343685601e-05
Step: 26530, train/learning_rate: 1.843169957282953e-05
Step: 26530, train/epoch: 6.313660144805908
Step: 26540, train/loss: 0.0
Step: 26540, train/grad_norm: 2.783590616672882e-06
Step: 26540, train/learning_rate: 1.8419799744151533e-05
Step: 26540, train/epoch: 6.3160400390625
Step: 26550, train/loss: 0.0
Step: 26550, train/grad_norm: 9.198372936225496e-06
Step: 26550, train/learning_rate: 1.840790173446294e-05
Step: 26550, train/epoch: 6.318419933319092
Step: 26560, train/loss: 0.0
Step: 26560, train/grad_norm: 0.0007350221858359873
Step: 26560, train/learning_rate: 1.8396001905784942e-05
Step: 26560, train/epoch: 6.320799827575684
Step: 26570, train/loss: 0.0
Step: 26570, train/grad_norm: 0.0035656422842293978
Step: 26570, train/learning_rate: 1.8384102077106945e-05
Step: 26570, train/epoch: 6.323179244995117
Step: 26580, train/loss: 0.0
Step: 26580, train/grad_norm: 1.372934002574766e-05
Step: 26580, train/learning_rate: 1.8372204067418352e-05
Step: 26580, train/epoch: 6.325559139251709
Step: 26590, train/loss: 0.0
Step: 26590, train/grad_norm: 7.02186525813886e-07
Step: 26590, train/learning_rate: 1.8360304238740355e-05
Step: 26590, train/epoch: 6.327939033508301
Step: 26600, train/loss: 0.0
Step: 26600, train/grad_norm: 1.2045549624417617e-07
Step: 26600, train/learning_rate: 1.834840622905176e-05
Step: 26600, train/epoch: 6.330318927764893
Step: 26610, train/loss: 0.0
Step: 26610, train/grad_norm: 5.550359105654934e-07
Step: 26610, train/learning_rate: 1.8336506400373764e-05
Step: 26610, train/epoch: 6.332698822021484
Step: 26620, train/loss: 0.0
Step: 26620, train/grad_norm: 4.5103583090622124e-08
Step: 26620, train/learning_rate: 1.8324606571695767e-05
Step: 26620, train/epoch: 6.335078716278076
Step: 26630, train/loss: 0.0
Step: 26630, train/grad_norm: 3.338374654049403e-06
Step: 26630, train/learning_rate: 1.8312708562007174e-05
Step: 26630, train/epoch: 6.33745813369751
Step: 26640, train/loss: 0.04859999939799309
Step: 26640, train/grad_norm: 5.621548098133644e-07
Step: 26640, train/learning_rate: 1.8300808733329177e-05
Step: 26640, train/epoch: 6.339838027954102
Step: 26650, train/loss: 0.0003000000142492354
Step: 26650, train/grad_norm: 0.00012922374298796058
Step: 26650, train/learning_rate: 1.8288910723640583e-05
Step: 26650, train/epoch: 6.342217922210693
Step: 26660, train/loss: 0.0
Step: 26660, train/grad_norm: 1.7025502074830001e-06
Step: 26660, train/learning_rate: 1.8277010894962586e-05
Step: 26660, train/epoch: 6.344597816467285
Step: 26670, train/loss: 0.0
Step: 26670, train/grad_norm: 3.4780657642841106e-07
Step: 26670, train/learning_rate: 1.826511106628459e-05
Step: 26670, train/epoch: 6.346977710723877
Step: 26680, train/loss: 0.0
Step: 26680, train/grad_norm: 4.1247872104577255e-06
Step: 26680, train/learning_rate: 1.8253213056595996e-05
Step: 26680, train/epoch: 6.349357604980469
Step: 26690, train/loss: 0.0
Step: 26690, train/grad_norm: 9.6475716304667e-08
Step: 26690, train/learning_rate: 1.8241313227918e-05
Step: 26690, train/epoch: 6.3517374992370605
Step: 26700, train/loss: 0.0
Step: 26700, train/grad_norm: 1.0531876796449069e-05
Step: 26700, train/learning_rate: 1.8229415218229406e-05
Step: 26700, train/epoch: 6.354116916656494
Step: 26710, train/loss: 0.0
Step: 26710, train/grad_norm: 6.040898369974457e-05
Step: 26710, train/learning_rate: 1.821751538955141e-05
Step: 26710, train/epoch: 6.356496810913086
Step: 26720, train/loss: 0.0
Step: 26720, train/grad_norm: 7.65158915783104e-07
Step: 26720, train/learning_rate: 1.820561556087341e-05
Step: 26720, train/epoch: 6.358876705169678
Step: 26730, train/loss: 0.0
Step: 26730, train/grad_norm: 3.6530408920043556e-08
Step: 26730, train/learning_rate: 1.8193717551184818e-05
Step: 26730, train/epoch: 6.3612565994262695
Step: 26740, train/loss: 0.0
Step: 26740, train/grad_norm: 6.723279710740826e-08
Step: 26740, train/learning_rate: 1.818181772250682e-05
Step: 26740, train/epoch: 6.363636493682861
Step: 26750, train/loss: 0.0
Step: 26750, train/grad_norm: 9.11508450940346e-08
Step: 26750, train/learning_rate: 1.8169919712818228e-05
Step: 26750, train/epoch: 6.366016387939453
Step: 26760, train/loss: 0.0
Step: 26760, train/grad_norm: 2.5623108967920416e-08
Step: 26760, train/learning_rate: 1.815801988414023e-05
Step: 26760, train/epoch: 6.368395805358887
Step: 26770, train/loss: 0.0
Step: 26770, train/grad_norm: 1.546156767062712e-07
Step: 26770, train/learning_rate: 1.8146120055462234e-05
Step: 26770, train/epoch: 6.3707756996154785
Step: 26780, train/loss: 0.0
Step: 26780, train/grad_norm: 9.019931326292863e-07
Step: 26780, train/learning_rate: 1.813422204577364e-05
Step: 26780, train/epoch: 6.37315559387207
Step: 26790, train/loss: 0.056299999356269836
Step: 26790, train/grad_norm: 0.00018965796334668994
Step: 26790, train/learning_rate: 1.8122322217095643e-05
Step: 26790, train/epoch: 6.375535488128662
Step: 26800, train/loss: 0.0
Step: 26800, train/grad_norm: 6.414037488866597e-05
Step: 26800, train/learning_rate: 1.811042420740705e-05
Step: 26800, train/epoch: 6.377915382385254
Step: 26810, train/loss: 0.0005000000237487257
Step: 26810, train/grad_norm: 2.2095858298598614e-07
Step: 26810, train/learning_rate: 1.8098524378729053e-05
Step: 26810, train/epoch: 6.380295276641846
Step: 26820, train/loss: 0.0
Step: 26820, train/grad_norm: 3.1338706207861833e-07
Step: 26820, train/learning_rate: 1.8086624550051056e-05
Step: 26820, train/epoch: 6.382674694061279
Step: 26830, train/loss: 0.0
Step: 26830, train/grad_norm: 5.182147333471221e-08
Step: 26830, train/learning_rate: 1.8074726540362462e-05
Step: 26830, train/epoch: 6.385054588317871
Step: 26840, train/loss: 0.0
Step: 26840, train/grad_norm: 4.3780539726867573e-07
Step: 26840, train/learning_rate: 1.8062826711684465e-05
Step: 26840, train/epoch: 6.387434482574463
Step: 26850, train/loss: 0.0
Step: 26850, train/grad_norm: 4.793770358446636e-07
Step: 26850, train/learning_rate: 1.8050928701995872e-05
Step: 26850, train/epoch: 6.389814376831055
Step: 26860, train/loss: 0.0
Step: 26860, train/grad_norm: 6.907219471941062e-07
Step: 26860, train/learning_rate: 1.8039028873317875e-05
Step: 26860, train/epoch: 6.3921942710876465
Step: 26870, train/loss: 0.0
Step: 26870, train/grad_norm: 1.9907602109014988e-05
Step: 26870, train/learning_rate: 1.8027129044639878e-05
Step: 26870, train/epoch: 6.394574165344238
Step: 26880, train/loss: 0.0
Step: 26880, train/grad_norm: 1.5862866575844237e-07
Step: 26880, train/learning_rate: 1.8015231034951285e-05
Step: 26880, train/epoch: 6.39695405960083
Step: 26890, train/loss: 0.0
Step: 26890, train/grad_norm: 3.5064217627223115e-06
Step: 26890, train/learning_rate: 1.8003331206273288e-05
Step: 26890, train/epoch: 6.399333477020264
Step: 26900, train/loss: 0.0
Step: 26900, train/grad_norm: 2.237809866301177e-07
Step: 26900, train/learning_rate: 1.7991433196584694e-05
Step: 26900, train/epoch: 6.4017133712768555
Step: 26910, train/loss: 0.0
Step: 26910, train/grad_norm: 7.716507610666667e-08
Step: 26910, train/learning_rate: 1.7979533367906697e-05
Step: 26910, train/epoch: 6.404093265533447
Step: 26920, train/loss: 0.0
Step: 26920, train/grad_norm: 1.000382265914368e-08
Step: 26920, train/learning_rate: 1.7967635358218104e-05
Step: 26920, train/epoch: 6.406473159790039
Step: 26930, train/loss: 0.0
Step: 26930, train/grad_norm: 1.2716295714199077e-06
Step: 26930, train/learning_rate: 1.7955735529540107e-05
Step: 26930, train/epoch: 6.408853054046631
Step: 26940, train/loss: 0.0
Step: 26940, train/grad_norm: 6.873401758866748e-08
Step: 26940, train/learning_rate: 1.794383570086211e-05
Step: 26940, train/epoch: 6.411232948303223
Step: 26950, train/loss: 0.0
Step: 26950, train/grad_norm: 5.432130365079502e-06
Step: 26950, train/learning_rate: 1.7931937691173516e-05
Step: 26950, train/epoch: 6.413612365722656
Step: 26960, train/loss: 0.0
Step: 26960, train/grad_norm: 2.544398398640624e-07
Step: 26960, train/learning_rate: 1.792003786249552e-05
Step: 26960, train/epoch: 6.415992259979248
Step: 26970, train/loss: 0.0
Step: 26970, train/grad_norm: 0.001635927357710898
Step: 26970, train/learning_rate: 1.7908139852806926e-05
Step: 26970, train/epoch: 6.41837215423584
Step: 26980, train/loss: 0.0
Step: 26980, train/grad_norm: 1.1804986321806155e-08
Step: 26980, train/learning_rate: 1.789624002412893e-05
Step: 26980, train/epoch: 6.420752048492432
Step: 26990, train/loss: 0.0
Step: 26990, train/grad_norm: 1.136385776590032e-07
Step: 26990, train/learning_rate: 1.7884340195450932e-05
Step: 26990, train/epoch: 6.423131942749023
Step: 27000, train/loss: 0.0
Step: 27000, train/grad_norm: 1.442209764945801e-07
Step: 27000, train/learning_rate: 1.787244218576234e-05
Step: 27000, train/epoch: 6.425511837005615
Step: 27010, train/loss: 0.0
Step: 27010, train/grad_norm: 9.38019013574376e-08
Step: 27010, train/learning_rate: 1.786054235708434e-05
Step: 27010, train/epoch: 6.427891254425049
Step: 27020, train/loss: 0.0
Step: 27020, train/grad_norm: 5.950482318439754e-06
Step: 27020, train/learning_rate: 1.7848644347395748e-05
Step: 27020, train/epoch: 6.430271148681641
Step: 27030, train/loss: 0.0
Step: 27030, train/grad_norm: 8.523960559614352e-08
Step: 27030, train/learning_rate: 1.783674451871775e-05
Step: 27030, train/epoch: 6.432651042938232
Step: 27040, train/loss: 0.0
Step: 27040, train/grad_norm: 1.6225263607339002e-05
Step: 27040, train/learning_rate: 1.7824844690039754e-05
Step: 27040, train/epoch: 6.435030937194824
Step: 27050, train/loss: 0.0
Step: 27050, train/grad_norm: 3.0469082048512064e-05
Step: 27050, train/learning_rate: 1.781294668035116e-05
Step: 27050, train/epoch: 6.437410831451416
Step: 27060, train/loss: 0.0
Step: 27060, train/grad_norm: 7.689072845096234e-06
Step: 27060, train/learning_rate: 1.7801046851673163e-05
Step: 27060, train/epoch: 6.439790725708008
Step: 27070, train/loss: 0.0
Step: 27070, train/grad_norm: 1.9451866251074534e-07
Step: 27070, train/learning_rate: 1.778914884198457e-05
Step: 27070, train/epoch: 6.4421706199646
Step: 27080, train/loss: 0.0
Step: 27080, train/grad_norm: 4.542877377389232e-06
Step: 27080, train/learning_rate: 1.7777249013306573e-05
Step: 27080, train/epoch: 6.444550037384033
Step: 27090, train/loss: 0.0
Step: 27090, train/grad_norm: 1.0802303222590126e-05
Step: 27090, train/learning_rate: 1.7765349184628576e-05
Step: 27090, train/epoch: 6.446929931640625
Step: 27100, train/loss: 0.0
Step: 27100, train/grad_norm: 7.355872185144108e-06
Step: 27100, train/learning_rate: 1.7753451174939983e-05
Step: 27100, train/epoch: 6.449309825897217
Step: 27110, train/loss: 0.0
Step: 27110, train/grad_norm: 1.6039532056311145e-05
Step: 27110, train/learning_rate: 1.7741551346261986e-05
Step: 27110, train/epoch: 6.451689720153809
Step: 27120, train/loss: 0.0
Step: 27120, train/grad_norm: 3.825315104677429e-07
Step: 27120, train/learning_rate: 1.7729653336573392e-05
Step: 27120, train/epoch: 6.4540696144104
Step: 27130, train/loss: 0.0
Step: 27130, train/grad_norm: 2.1018395557348413e-07
Step: 27130, train/learning_rate: 1.7717753507895395e-05
Step: 27130, train/epoch: 6.456449508666992
Step: 27140, train/loss: 0.0
Step: 27140, train/grad_norm: 1.4276197113360922e-08
Step: 27140, train/learning_rate: 1.7705853679217398e-05
Step: 27140, train/epoch: 6.458828926086426
Step: 27150, train/loss: 0.0
Step: 27150, train/grad_norm: 1.5946089604312874e-07
Step: 27150, train/learning_rate: 1.7693955669528805e-05
Step: 27150, train/epoch: 6.461208820343018
Step: 27160, train/loss: 0.0
Step: 27160, train/grad_norm: 1.567936891433419e-08
Step: 27160, train/learning_rate: 1.7682055840850808e-05
Step: 27160, train/epoch: 6.463588714599609
Step: 27170, train/loss: 0.0
Step: 27170, train/grad_norm: 4.996096336640221e-08
Step: 27170, train/learning_rate: 1.7670157831162214e-05
Step: 27170, train/epoch: 6.465968608856201
Step: 27180, train/loss: 0.0
Step: 27180, train/grad_norm: 3.055868669221695e-09
Step: 27180, train/learning_rate: 1.7658258002484217e-05
Step: 27180, train/epoch: 6.468348503112793
Step: 27190, train/loss: 0.0
Step: 27190, train/grad_norm: 1.3721564995350377e-09
Step: 27190, train/learning_rate: 1.764635817380622e-05
Step: 27190, train/epoch: 6.470728397369385
Step: 27200, train/loss: 0.0
Step: 27200, train/grad_norm: 5.0790323058436115e-08
Step: 27200, train/learning_rate: 1.7634460164117627e-05
Step: 27200, train/epoch: 6.473107814788818
Step: 27210, train/loss: 0.0
Step: 27210, train/grad_norm: 8.54708787301206e-07
Step: 27210, train/learning_rate: 1.762256033543963e-05
Step: 27210, train/epoch: 6.47548770904541
Step: 27220, train/loss: 0.0
Step: 27220, train/grad_norm: 2.808713706059507e-09
Step: 27220, train/learning_rate: 1.7610662325751036e-05
Step: 27220, train/epoch: 6.477867603302002
Step: 27230, train/loss: 0.0
Step: 27230, train/grad_norm: 1.2969778140359267e-07
Step: 27230, train/learning_rate: 1.759876249707304e-05
Step: 27230, train/epoch: 6.480247497558594
Step: 27240, train/loss: 0.0
Step: 27240, train/grad_norm: 3.3512874697549933e-09
Step: 27240, train/learning_rate: 1.7586862668395042e-05
Step: 27240, train/epoch: 6.4826273918151855
Step: 27250, train/loss: 0.0
Step: 27250, train/grad_norm: 9.635076594349812e-07
Step: 27250, train/learning_rate: 1.757496465870645e-05
Step: 27250, train/epoch: 6.485007286071777
Step: 27260, train/loss: 0.0
Step: 27260, train/grad_norm: 3.4497209533412843e-09
Step: 27260, train/learning_rate: 1.7563064830028452e-05
Step: 27260, train/epoch: 6.487387180328369
Step: 27270, train/loss: 0.0
Step: 27270, train/grad_norm: 3.2170039503398584e-06
Step: 27270, train/learning_rate: 1.755116682033986e-05
Step: 27270, train/epoch: 6.489766597747803
Step: 27280, train/loss: 0.0
Step: 27280, train/grad_norm: 1.3246570063074614e-08
Step: 27280, train/learning_rate: 1.753926699166186e-05
Step: 27280, train/epoch: 6.4921464920043945
Step: 27290, train/loss: 0.0
Step: 27290, train/grad_norm: 5.8069232977686625e-08
Step: 27290, train/learning_rate: 1.7527367162983865e-05
Step: 27290, train/epoch: 6.494526386260986
Step: 27300, train/loss: 0.0
Step: 27300, train/grad_norm: 1.2255867432031664e-07
Step: 27300, train/learning_rate: 1.751546915329527e-05
Step: 27300, train/epoch: 6.496906280517578
Step: 27310, train/loss: 0.0
Step: 27310, train/grad_norm: 4.5202409637568053e-07
Step: 27310, train/learning_rate: 1.7503569324617274e-05
Step: 27310, train/epoch: 6.49928617477417
Step: 27320, train/loss: 0.0
Step: 27320, train/grad_norm: 4.342320281125467e-08
Step: 27320, train/learning_rate: 1.749167131492868e-05
Step: 27320, train/epoch: 6.501666069030762
Step: 27330, train/loss: 0.0
Step: 27330, train/grad_norm: 1.1592561577344895e-06
Step: 27330, train/learning_rate: 1.7479771486250684e-05
Step: 27330, train/epoch: 6.504045486450195
Step: 27340, train/loss: 0.0
Step: 27340, train/grad_norm: 1.7351554859601492e-08
Step: 27340, train/learning_rate: 1.7467871657572687e-05
Step: 27340, train/epoch: 6.506425380706787
Step: 27350, train/loss: 0.0
Step: 27350, train/grad_norm: 1.1319830406364417e-07
Step: 27350, train/learning_rate: 1.7455973647884093e-05
Step: 27350, train/epoch: 6.508805274963379
Step: 27360, train/loss: 0.0
Step: 27360, train/grad_norm: 2.51991072452995e-09
Step: 27360, train/learning_rate: 1.7444073819206096e-05
Step: 27360, train/epoch: 6.511185169219971
Step: 27370, train/loss: 0.0
Step: 27370, train/grad_norm: 5.026079179515364e-07
Step: 27370, train/learning_rate: 1.7432175809517503e-05
Step: 27370, train/epoch: 6.5135650634765625
Step: 27380, train/loss: 0.0
Step: 27380, train/grad_norm: 5.211244769043333e-08
Step: 27380, train/learning_rate: 1.7420275980839506e-05
Step: 27380, train/epoch: 6.515944957733154
Step: 27390, train/loss: 0.0
Step: 27390, train/grad_norm: 6.533597485258724e-08
Step: 27390, train/learning_rate: 1.740837615216151e-05
Step: 27390, train/epoch: 6.518324375152588
Step: 27400, train/loss: 0.0
Step: 27400, train/grad_norm: 4.7542565084768285e-07
Step: 27400, train/learning_rate: 1.7396478142472915e-05
Step: 27400, train/epoch: 6.52070426940918
Step: 27410, train/loss: 0.0
Step: 27410, train/grad_norm: 1.4948241533474516e-09
Step: 27410, train/learning_rate: 1.738457831379492e-05
Step: 27410, train/epoch: 6.5230841636657715
Step: 27420, train/loss: 0.0
Step: 27420, train/grad_norm: 0.00012218079064041376
Step: 27420, train/learning_rate: 1.7372680304106325e-05
Step: 27420, train/epoch: 6.525464057922363
Step: 27430, train/loss: 0.0
Step: 27430, train/grad_norm: 1.457673981519747e-08
Step: 27430, train/learning_rate: 1.7360780475428328e-05
Step: 27430, train/epoch: 6.527843952178955
Step: 27440, train/loss: 0.0
Step: 27440, train/grad_norm: 5.213766751666071e-09
Step: 27440, train/learning_rate: 1.734888064675033e-05
Step: 27440, train/epoch: 6.530223846435547
Step: 27450, train/loss: 0.0
Step: 27450, train/grad_norm: 3.744230525626335e-08
Step: 27450, train/learning_rate: 1.7336982637061737e-05
Step: 27450, train/epoch: 6.532603740692139
Step: 27460, train/loss: 0.0
Step: 27460, train/grad_norm: 5.921772753936239e-06
Step: 27460, train/learning_rate: 1.732508280838374e-05
Step: 27460, train/epoch: 6.534983158111572
Step: 27470, train/loss: 0.0
Step: 27470, train/grad_norm: 1.0186092680442016e-07
Step: 27470, train/learning_rate: 1.7313184798695147e-05
Step: 27470, train/epoch: 6.537363052368164
Step: 27480, train/loss: 0.0
Step: 27480, train/grad_norm: 8.268138174116757e-09
Step: 27480, train/learning_rate: 1.730128497001715e-05
Step: 27480, train/epoch: 6.539742946624756
Step: 27490, train/loss: 0.0
Step: 27490, train/grad_norm: 2.8166563424747437e-05
Step: 27490, train/learning_rate: 1.7289385141339153e-05
Step: 27490, train/epoch: 6.542122840881348
Step: 27500, train/loss: 0.0
Step: 27500, train/grad_norm: 2.067005411277023e-08
Step: 27500, train/learning_rate: 1.727748713165056e-05
Step: 27500, train/epoch: 6.5445027351379395
Step: 27510, train/loss: 0.0
Step: 27510, train/grad_norm: 4.445542344910791e-06
Step: 27510, train/learning_rate: 1.7265587302972563e-05
Step: 27510, train/epoch: 6.546882629394531
Step: 27520, train/loss: 0.0
Step: 27520, train/grad_norm: 4.455199320574366e-09
Step: 27520, train/learning_rate: 1.725368929328397e-05
Step: 27520, train/epoch: 6.549262046813965
Step: 27530, train/loss: 0.0
Step: 27530, train/grad_norm: 5.609736604128557e-07
Step: 27530, train/learning_rate: 1.7241789464605972e-05
Step: 27530, train/epoch: 6.551641941070557
Step: 27540, train/loss: 0.0
Step: 27540, train/grad_norm: 0.0001609098690096289
Step: 27540, train/learning_rate: 1.7229889635927975e-05
Step: 27540, train/epoch: 6.554021835327148
Step: 27550, train/loss: 0.0
Step: 27550, train/grad_norm: 1.748004763157951e-08
Step: 27550, train/learning_rate: 1.7217991626239382e-05
Step: 27550, train/epoch: 6.55640172958374
Step: 27560, train/loss: 0.0
Step: 27560, train/grad_norm: 4.574719714156572e-09
Step: 27560, train/learning_rate: 1.7206091797561385e-05
Step: 27560, train/epoch: 6.558781623840332
Step: 27570, train/loss: 0.0
Step: 27570, train/grad_norm: 3.909192614059975e-08
Step: 27570, train/learning_rate: 1.719419378787279e-05
Step: 27570, train/epoch: 6.561161518096924
Step: 27580, train/loss: 0.0
Step: 27580, train/grad_norm: 2.4797703890300227e-09
Step: 27580, train/learning_rate: 1.7182293959194794e-05
Step: 27580, train/epoch: 6.563540935516357
Step: 27590, train/loss: 0.0
Step: 27590, train/grad_norm: 7.051945249969549e-09
Step: 27590, train/learning_rate: 1.71703959495062e-05
Step: 27590, train/epoch: 6.565920829772949
Step: 27600, train/loss: 0.0
Step: 27600, train/grad_norm: 1.5971647338020034e-09
Step: 27600, train/learning_rate: 1.7158496120828204e-05
Step: 27600, train/epoch: 6.568300724029541
Step: 27610, train/loss: 0.0
Step: 27610, train/grad_norm: 4.075378910783911e-06
Step: 27610, train/learning_rate: 1.7146596292150207e-05
Step: 27610, train/epoch: 6.570680618286133
Step: 27620, train/loss: 0.0
Step: 27620, train/grad_norm: 3.22019805025775e-05
Step: 27620, train/learning_rate: 1.7134698282461613e-05
Step: 27620, train/epoch: 6.573060512542725
Step: 27630, train/loss: 0.0
Step: 27630, train/grad_norm: 3.186643766639463e-07
Step: 27630, train/learning_rate: 1.7122798453783616e-05
Step: 27630, train/epoch: 6.575440406799316
Step: 27640, train/loss: 0.0
Step: 27640, train/grad_norm: 2.2328373461277806e-07
Step: 27640, train/learning_rate: 1.7110900444095023e-05
Step: 27640, train/epoch: 6.577820301055908
Step: 27650, train/loss: 0.0
Step: 27650, train/grad_norm: 3.929637387045659e-06
Step: 27650, train/learning_rate: 1.7099000615417026e-05
Step: 27650, train/epoch: 6.580199718475342
Step: 27660, train/loss: 0.0
Step: 27660, train/grad_norm: 3.7342401810747106e-06
Step: 27660, train/learning_rate: 1.708710078673903e-05
Step: 27660, train/epoch: 6.582579612731934
Step: 27670, train/loss: 0.0
Step: 27670, train/grad_norm: 2.5157074560411274e-05
Step: 27670, train/learning_rate: 1.7075202777050436e-05
Step: 27670, train/epoch: 6.584959506988525
Step: 27680, train/loss: 0.0
Step: 27680, train/grad_norm: 9.108070742058771e-09
Step: 27680, train/learning_rate: 1.706330294837244e-05
Step: 27680, train/epoch: 6.587339401245117
Step: 27690, train/loss: 0.0
Step: 27690, train/grad_norm: 9.091803008232091e-07
Step: 27690, train/learning_rate: 1.7051404938683845e-05
Step: 27690, train/epoch: 6.589719295501709
Step: 27700, train/loss: 0.0
Step: 27700, train/grad_norm: 2.9278877988758722e-08
Step: 27700, train/learning_rate: 1.7039505110005848e-05
Step: 27700, train/epoch: 6.592099189758301
Step: 27710, train/loss: 0.0
Step: 27710, train/grad_norm: 3.973008144697587e-09
Step: 27710, train/learning_rate: 1.702760528132785e-05
Step: 27710, train/epoch: 6.594478607177734
Step: 27720, train/loss: 0.0
Step: 27720, train/grad_norm: 8.697415410097165e-08
Step: 27720, train/learning_rate: 1.7015707271639258e-05
Step: 27720, train/epoch: 6.596858501434326
Step: 27730, train/loss: 0.0
Step: 27730, train/grad_norm: 7.279941627302833e-08
Step: 27730, train/learning_rate: 1.700380744296126e-05
Step: 27730, train/epoch: 6.599238395690918
Step: 27740, train/loss: 0.0
Step: 27740, train/grad_norm: 2.4719642510717676e-07
Step: 27740, train/learning_rate: 1.6991909433272667e-05
Step: 27740, train/epoch: 6.60161828994751
Step: 27750, train/loss: 0.0
Step: 27750, train/grad_norm: 9.847068213275634e-06
Step: 27750, train/learning_rate: 1.698000960459467e-05
Step: 27750, train/epoch: 6.603998184204102
Step: 27760, train/loss: 0.0
Step: 27760, train/grad_norm: 7.5276652751199435e-06
Step: 27760, train/learning_rate: 1.6968109775916673e-05
Step: 27760, train/epoch: 6.606378078460693
Step: 27770, train/loss: 0.0
Step: 27770, train/grad_norm: 4.1609975909295827e-08
Step: 27770, train/learning_rate: 1.695621176622808e-05
Step: 27770, train/epoch: 6.608757972717285
Step: 27780, train/loss: 0.0
Step: 27780, train/grad_norm: 6.11926651572503e-08
Step: 27780, train/learning_rate: 1.6944311937550083e-05
Step: 27780, train/epoch: 6.611137390136719
Step: 27790, train/loss: 0.0
Step: 27790, train/grad_norm: 6.68361010980334e-08
Step: 27790, train/learning_rate: 1.693241392786149e-05
Step: 27790, train/epoch: 6.6135172843933105
Step: 27800, train/loss: 0.0
Step: 27800, train/grad_norm: 8.44689020595979e-06
Step: 27800, train/learning_rate: 1.6920514099183492e-05
Step: 27800, train/epoch: 6.615897178649902
Step: 27810, train/loss: 0.0
Step: 27810, train/grad_norm: 4.799100565833214e-07
Step: 27810, train/learning_rate: 1.6908614270505495e-05
Step: 27810, train/epoch: 6.618277072906494
Step: 27820, train/loss: 0.0
Step: 27820, train/grad_norm: 3.142076376860814e-09
Step: 27820, train/learning_rate: 1.6896716260816902e-05
Step: 27820, train/epoch: 6.620656967163086
Step: 27830, train/loss: 0.0
Step: 27830, train/grad_norm: 3.3794658520491794e-06
Step: 27830, train/learning_rate: 1.6884816432138905e-05
Step: 27830, train/epoch: 6.623036861419678
Step: 27840, train/loss: 0.0
Step: 27840, train/grad_norm: 3.650334861049487e-07
Step: 27840, train/learning_rate: 1.687291842245031e-05
Step: 27840, train/epoch: 6.625416278839111
Step: 27850, train/loss: 0.0
Step: 27850, train/grad_norm: 1.8352730535298178e-07
Step: 27850, train/learning_rate: 1.6861018593772314e-05
Step: 27850, train/epoch: 6.627796173095703
Step: 27860, train/loss: 0.0
Step: 27860, train/grad_norm: 3.742303078979603e-06
Step: 27860, train/learning_rate: 1.6849118765094317e-05
Step: 27860, train/epoch: 6.630176067352295
Step: 27870, train/loss: 0.0
Step: 27870, train/grad_norm: 2.1735298005864934e-09
Step: 27870, train/learning_rate: 1.6837220755405724e-05
Step: 27870, train/epoch: 6.632555961608887
Step: 27880, train/loss: 0.0
Step: 27880, train/grad_norm: 1.6784809986347682e-08
Step: 27880, train/learning_rate: 1.6825320926727727e-05
Step: 27880, train/epoch: 6.6349358558654785
Step: 27890, train/loss: 0.0
Step: 27890, train/grad_norm: 6.379941019929447e-09
Step: 27890, train/learning_rate: 1.6813422917039134e-05
Step: 27890, train/epoch: 6.63731575012207
Step: 27900, train/loss: 0.0
Step: 27900, train/grad_norm: 1.2617300537698384e-09
Step: 27900, train/learning_rate: 1.6801523088361137e-05
Step: 27900, train/epoch: 6.639695167541504
Step: 27910, train/loss: 0.0
Step: 27910, train/grad_norm: 1.602626120700279e-08
Step: 27910, train/learning_rate: 1.678962325968314e-05
Step: 27910, train/epoch: 6.642075061798096
Step: 27920, train/loss: 0.0
Step: 27920, train/grad_norm: 1.4064369224797701e-07
Step: 27920, train/learning_rate: 1.6777725249994546e-05
Step: 27920, train/epoch: 6.6444549560546875
Step: 27930, train/loss: 0.0
Step: 27930, train/grad_norm: 1.3684770294730697e-07
Step: 27930, train/learning_rate: 1.676582542131655e-05
Step: 27930, train/epoch: 6.646834850311279
Step: 27940, train/loss: 0.0
Step: 27940, train/grad_norm: 5.362029042998984e-09
Step: 27940, train/learning_rate: 1.6753927411627956e-05
Step: 27940, train/epoch: 6.649214744567871
Step: 27950, train/loss: 0.0
Step: 27950, train/grad_norm: 1.8185150052474341e-09
Step: 27950, train/learning_rate: 1.674202758294996e-05
Step: 27950, train/epoch: 6.651594638824463
Step: 27960, train/loss: 0.0
Step: 27960, train/grad_norm: 7.771120726829395e-05
Step: 27960, train/learning_rate: 1.6730127754271962e-05
Step: 27960, train/epoch: 6.653974533081055
Step: 27970, train/loss: 0.0
Step: 27970, train/grad_norm: 2.792714258248452e-05
Step: 27970, train/learning_rate: 1.6718229744583368e-05
Step: 27970, train/epoch: 6.656353950500488
Step: 27980, train/loss: 0.06759999692440033
Step: 27980, train/grad_norm: 4.262000885546513e-08
Step: 27980, train/learning_rate: 1.670632991590537e-05
Step: 27980, train/epoch: 6.65873384475708
Step: 27990, train/loss: 0.0
Step: 27990, train/grad_norm: 1.4511570043396205e-06
Step: 27990, train/learning_rate: 1.6694431906216778e-05
Step: 27990, train/epoch: 6.661113739013672
Step: 28000, train/loss: 0.0
Step: 28000, train/grad_norm: 0.0001384155184496194
Step: 28000, train/learning_rate: 1.668253207753878e-05
Step: 28000, train/epoch: 6.663493633270264
Step: 28010, train/loss: 0.0
Step: 28010, train/grad_norm: 2.0610059436876327e-07
Step: 28010, train/learning_rate: 1.6670632248860784e-05
Step: 28010, train/epoch: 6.6658735275268555
Step: 28020, train/loss: 0.0
Step: 28020, train/grad_norm: 1.050030178362249e-07
Step: 28020, train/learning_rate: 1.665873423917219e-05
Step: 28020, train/epoch: 6.668253421783447
Step: 28030, train/loss: 0.020600000396370888
Step: 28030, train/grad_norm: 208.6178741455078
Step: 28030, train/learning_rate: 1.6646834410494193e-05
Step: 28030, train/epoch: 6.670632839202881
Step: 28040, train/loss: 0.0
Step: 28040, train/grad_norm: 2.9199964046711102e-06
Step: 28040, train/learning_rate: 1.66349364008056e-05
Step: 28040, train/epoch: 6.673012733459473
Step: 28050, train/loss: 0.0
Step: 28050, train/grad_norm: 2.8127901785524045e-08
Step: 28050, train/learning_rate: 1.6623036572127603e-05
Step: 28050, train/epoch: 6.6753926277160645
Step: 28060, train/loss: 0.0
Step: 28060, train/grad_norm: 1.1096652769992943e-06
Step: 28060, train/learning_rate: 1.6611136743449606e-05
Step: 28060, train/epoch: 6.677772521972656
Step: 28070, train/loss: 0.0
Step: 28070, train/grad_norm: 1.3626310746417403e-08
Step: 28070, train/learning_rate: 1.6599238733761013e-05
Step: 28070, train/epoch: 6.680152416229248
Step: 28080, train/loss: 0.0
Step: 28080, train/grad_norm: 1.8837795323634055e-08
Step: 28080, train/learning_rate: 1.6587338905083016e-05
Step: 28080, train/epoch: 6.68253231048584
Step: 28090, train/loss: 0.0
Step: 28090, train/grad_norm: 1.2229930689500179e-05
Step: 28090, train/learning_rate: 1.6575440895394422e-05
Step: 28090, train/epoch: 6.684911727905273
Step: 28100, train/loss: 0.0
Step: 28100, train/grad_norm: 7.801958190611913e-08
Step: 28100, train/learning_rate: 1.6563541066716425e-05
Step: 28100, train/epoch: 6.687291622161865
Step: 28110, train/loss: 0.0
Step: 28110, train/grad_norm: 1.6959478671196848e-05
Step: 28110, train/learning_rate: 1.6551641238038428e-05
Step: 28110, train/epoch: 6.689671516418457
Step: 28120, train/loss: 0.0
Step: 28120, train/grad_norm: 5.954817424935754e-06
Step: 28120, train/learning_rate: 1.6539743228349835e-05
Step: 28120, train/epoch: 6.692051410675049
Step: 28130, train/loss: 0.0
Step: 28130, train/grad_norm: 2.0572416659092596e-08
Step: 28130, train/learning_rate: 1.6527843399671838e-05
Step: 28130, train/epoch: 6.694431304931641
Step: 28140, train/loss: 0.0
Step: 28140, train/grad_norm: 0.001999827101826668
Step: 28140, train/learning_rate: 1.6515945389983244e-05
Step: 28140, train/epoch: 6.696811199188232
Step: 28150, train/loss: 0.0
Step: 28150, train/grad_norm: 1.0033435415834902e-08
Step: 28150, train/learning_rate: 1.6504045561305247e-05
Step: 28150, train/epoch: 6.699191093444824
Step: 28160, train/loss: 0.0
Step: 28160, train/grad_norm: 7.436047326336848e-06
Step: 28160, train/learning_rate: 1.649214573262725e-05
Step: 28160, train/epoch: 6.701570510864258
Step: 28170, train/loss: 0.0
Step: 28170, train/grad_norm: 3.184249308674225e-08
Step: 28170, train/learning_rate: 1.6480247722938657e-05
Step: 28170, train/epoch: 6.70395040512085
Step: 28180, train/loss: 0.0
Step: 28180, train/grad_norm: 1.0142373696453433e-07
Step: 28180, train/learning_rate: 1.646834789426066e-05
Step: 28180, train/epoch: 6.706330299377441
Step: 28190, train/loss: 0.0
Step: 28190, train/grad_norm: 3.3012494071726906e-08
Step: 28190, train/learning_rate: 1.6456449884572066e-05
Step: 28190, train/epoch: 6.708710193634033
Step: 28200, train/loss: 0.033399999141693115
Step: 28200, train/grad_norm: 1.826253281933532e-08
Step: 28200, train/learning_rate: 1.644455005589407e-05
Step: 28200, train/epoch: 6.711090087890625
Step: 28210, train/loss: 0.0
Step: 28210, train/grad_norm: 5.341778432921274e-07
Step: 28210, train/learning_rate: 1.6432650227216072e-05
Step: 28210, train/epoch: 6.713469982147217
Step: 28220, train/loss: 0.0
Step: 28220, train/grad_norm: 5.573739869646488e-08
Step: 28220, train/learning_rate: 1.642075221752748e-05
Step: 28220, train/epoch: 6.71584939956665
Step: 28230, train/loss: 0.0
Step: 28230, train/grad_norm: 6.445103650776218e-08
Step: 28230, train/learning_rate: 1.6408852388849482e-05
Step: 28230, train/epoch: 6.718229293823242
Step: 28240, train/loss: 0.0
Step: 28240, train/grad_norm: 3.6586500300472835e-06
Step: 28240, train/learning_rate: 1.639695437916089e-05
Step: 28240, train/epoch: 6.720609188079834
Step: 28250, train/loss: 0.0
Step: 28250, train/grad_norm: 1.9769365167121578e-07
Step: 28250, train/learning_rate: 1.638505455048289e-05
Step: 28250, train/epoch: 6.722989082336426
Step: 28260, train/loss: 0.0
Step: 28260, train/grad_norm: 0.00014151200593914837
Step: 28260, train/learning_rate: 1.6373156540794298e-05
Step: 28260, train/epoch: 6.725368976593018
Step: 28270, train/loss: 0.03009999915957451
Step: 28270, train/grad_norm: 0.00021343080152291805
Step: 28270, train/learning_rate: 1.63612567121163e-05
Step: 28270, train/epoch: 6.727748870849609
Step: 28280, train/loss: 0.0
Step: 28280, train/grad_norm: 7.171654942794703e-06
Step: 28280, train/learning_rate: 1.6349356883438304e-05
Step: 28280, train/epoch: 6.730128288269043
Step: 28290, train/loss: 0.0
Step: 28290, train/grad_norm: 2.7923167067456234e-07
Step: 28290, train/learning_rate: 1.633745887374971e-05
Step: 28290, train/epoch: 6.732508182525635
Step: 28300, train/loss: 0.0
Step: 28300, train/grad_norm: 1.6565000748869352e-07
Step: 28300, train/learning_rate: 1.6325559045071714e-05
Step: 28300, train/epoch: 6.734888076782227
Step: 28310, train/loss: 0.14219999313354492
Step: 28310, train/grad_norm: 1.3347602134672343e-07
Step: 28310, train/learning_rate: 1.631366103538312e-05
Step: 28310, train/epoch: 6.737267971038818
Step: 28320, train/loss: 0.0
Step: 28320, train/grad_norm: 3.122048894965701e-07
Step: 28320, train/learning_rate: 1.6301761206705123e-05
Step: 28320, train/epoch: 6.73964786529541
Step: 28330, train/loss: 0.0
Step: 28330, train/grad_norm: 6.600796041311696e-05
Step: 28330, train/learning_rate: 1.6289861378027126e-05
Step: 28330, train/epoch: 6.742027759552002
Step: 28340, train/loss: 0.0
Step: 28340, train/grad_norm: 2.0584731828421354e-05
Step: 28340, train/learning_rate: 1.6277963368338533e-05
Step: 28340, train/epoch: 6.744407653808594
Step: 28350, train/loss: 0.0
Step: 28350, train/grad_norm: 5.143518251315982e-07
Step: 28350, train/learning_rate: 1.6266063539660536e-05
Step: 28350, train/epoch: 6.746787071228027
Step: 28360, train/loss: 0.0
Step: 28360, train/grad_norm: 3.95287685250878e-07
Step: 28360, train/learning_rate: 1.6254165529971942e-05
Step: 28360, train/epoch: 6.749166965484619
Step: 28370, train/loss: 0.0
Step: 28370, train/grad_norm: 2.5908519774020533e-07
Step: 28370, train/learning_rate: 1.6242265701293945e-05
Step: 28370, train/epoch: 6.751546859741211
Step: 28380, train/loss: 0.0
Step: 28380, train/grad_norm: 0.00010794691479532048
Step: 28380, train/learning_rate: 1.623036587261595e-05
Step: 28380, train/epoch: 6.753926753997803
Step: 28390, train/loss: 0.0
Step: 28390, train/grad_norm: 1.6096315448521636e-05
Step: 28390, train/learning_rate: 1.6218467862927355e-05
Step: 28390, train/epoch: 6.7563066482543945
Step: 28400, train/loss: 0.0
Step: 28400, train/grad_norm: 8.841476665111259e-05
Step: 28400, train/learning_rate: 1.6206568034249358e-05
Step: 28400, train/epoch: 6.758686542510986
Step: 28410, train/loss: 0.0
Step: 28410, train/grad_norm: 2.840569504769519e-05
Step: 28410, train/learning_rate: 1.6194670024560764e-05
Step: 28410, train/epoch: 6.76106595993042
Step: 28420, train/loss: 0.0
Step: 28420, train/grad_norm: 9.891359695757274e-06
Step: 28420, train/learning_rate: 1.6182770195882767e-05
Step: 28420, train/epoch: 6.763445854187012
Step: 28430, train/loss: 0.0
Step: 28430, train/grad_norm: 5.490781518346921e-07
Step: 28430, train/learning_rate: 1.617087036720477e-05
Step: 28430, train/epoch: 6.7658257484436035
Step: 28440, train/loss: 0.0
Step: 28440, train/grad_norm: 2.9170054403948598e-05
Step: 28440, train/learning_rate: 1.6158972357516177e-05
Step: 28440, train/epoch: 6.768205642700195
Step: 28450, train/loss: 0.0
Step: 28450, train/grad_norm: 0.0012707171263173223
Step: 28450, train/learning_rate: 1.614707252883818e-05
Step: 28450, train/epoch: 6.770585536956787
Step: 28460, train/loss: 0.0
Step: 28460, train/grad_norm: 6.65263041810249e-06
Step: 28460, train/learning_rate: 1.6135174519149587e-05
Step: 28460, train/epoch: 6.772965431213379
Step: 28470, train/loss: 0.0
Step: 28470, train/grad_norm: 0.0003240770020056516
Step: 28470, train/learning_rate: 1.612327469047159e-05
Step: 28470, train/epoch: 6.7753448486328125
Step: 28480, train/loss: 0.0
Step: 28480, train/grad_norm: 4.080721555510536e-05
Step: 28480, train/learning_rate: 1.6111374861793593e-05
Step: 28480, train/epoch: 6.777724742889404
Step: 28490, train/loss: 0.0
Step: 28490, train/grad_norm: 2.405418490525335e-05
Step: 28490, train/learning_rate: 1.6099476852105e-05
Step: 28490, train/epoch: 6.780104637145996
Step: 28500, train/loss: 0.0
Step: 28500, train/grad_norm: 3.4003070936705626e-07
Step: 28500, train/learning_rate: 1.6087577023427002e-05
Step: 28500, train/epoch: 6.782484531402588
Step: 28510, train/loss: 0.0
Step: 28510, train/grad_norm: 5.255289579508826e-05
Step: 28510, train/learning_rate: 1.607567901373841e-05
Step: 28510, train/epoch: 6.78486442565918
Step: 28520, train/loss: 0.13519999384880066
Step: 28520, train/grad_norm: 123.70653533935547
Step: 28520, train/learning_rate: 1.606377918506041e-05
Step: 28520, train/epoch: 6.7872443199157715
Step: 28530, train/loss: 0.0
Step: 28530, train/grad_norm: 0.0009374662768095732
Step: 28530, train/learning_rate: 1.6051879356382415e-05
Step: 28530, train/epoch: 6.789624214172363
Step: 28540, train/loss: 0.0
Step: 28540, train/grad_norm: 0.00022469769464805722
Step: 28540, train/learning_rate: 1.603998134669382e-05
Step: 28540, train/epoch: 6.792003631591797
Step: 28550, train/loss: 0.0
Step: 28550, train/grad_norm: 0.0019343460444360971
Step: 28550, train/learning_rate: 1.6028081518015824e-05
Step: 28550, train/epoch: 6.794383525848389
Step: 28560, train/loss: 0.0
Step: 28560, train/grad_norm: 0.0011025626445189118
Step: 28560, train/learning_rate: 1.601618350832723e-05
Step: 28560, train/epoch: 6.7967634201049805
Step: 28570, train/loss: 0.0
Step: 28570, train/grad_norm: 1.3653311725647654e-06
Step: 28570, train/learning_rate: 1.6004283679649234e-05
Step: 28570, train/epoch: 6.799143314361572
Step: 28580, train/loss: 0.0
Step: 28580, train/grad_norm: 1.0833846317837015e-06
Step: 28580, train/learning_rate: 1.5992383850971237e-05
Step: 28580, train/epoch: 6.801523208618164
Step: 28590, train/loss: 0.006399999838322401
Step: 28590, train/grad_norm: 61.91199493408203
Step: 28590, train/learning_rate: 1.5980485841282643e-05
Step: 28590, train/epoch: 6.803903102874756
Step: 28600, train/loss: 0.0
Step: 28600, train/grad_norm: 0.0008051058975979686
Step: 28600, train/learning_rate: 1.5968586012604646e-05
Step: 28600, train/epoch: 6.8062825202941895
Step: 28610, train/loss: 0.0
Step: 28610, train/grad_norm: 0.02058805525302887
Step: 28610, train/learning_rate: 1.5956688002916053e-05
Step: 28610, train/epoch: 6.808662414550781
Step: 28620, train/loss: 0.0
Step: 28620, train/grad_norm: 0.10364187508821487
Step: 28620, train/learning_rate: 1.5944788174238056e-05
Step: 28620, train/epoch: 6.811042308807373
Step: 28630, train/loss: 0.0
Step: 28630, train/grad_norm: 0.0028463450726121664
Step: 28630, train/learning_rate: 1.593288834556006e-05
Step: 28630, train/epoch: 6.813422203063965
Step: 28640, train/loss: 0.0
Step: 28640, train/grad_norm: 3.5079756344202906e-05
Step: 28640, train/learning_rate: 1.5920990335871466e-05
Step: 28640, train/epoch: 6.815802097320557
Step: 28650, train/loss: 0.0
Step: 28650, train/grad_norm: 2.085012766883665e-07
Step: 28650, train/learning_rate: 1.590909050719347e-05
Step: 28650, train/epoch: 6.818181991577148
Step: 28660, train/loss: 0.0
Step: 28660, train/grad_norm: 1.591258978805854e-06
Step: 28660, train/learning_rate: 1.5897192497504875e-05
Step: 28660, train/epoch: 6.820561408996582
Step: 28670, train/loss: 0.0
Step: 28670, train/grad_norm: 1.0943924877437894e-07
Step: 28670, train/learning_rate: 1.5885292668826878e-05
Step: 28670, train/epoch: 6.822941303253174
Step: 28680, train/loss: 0.0
Step: 28680, train/grad_norm: 6.027199106029002e-06
Step: 28680, train/learning_rate: 1.587339284014888e-05
Step: 28680, train/epoch: 6.825321197509766
Step: 28690, train/loss: 0.0005000000237487257
Step: 28690, train/grad_norm: 1.3756196892700245e-07
Step: 28690, train/learning_rate: 1.5861494830460288e-05
Step: 28690, train/epoch: 6.827701091766357
Step: 28700, train/loss: 0.0
Step: 28700, train/grad_norm: 1.5974023881426547e-06
Step: 28700, train/learning_rate: 1.584959500178229e-05
Step: 28700, train/epoch: 6.830080986022949
Step: 28710, train/loss: 0.0010000000474974513
Step: 28710, train/grad_norm: 0.00019067316316068172
Step: 28710, train/learning_rate: 1.5837696992093697e-05
Step: 28710, train/epoch: 6.832460880279541
Step: 28720, train/loss: 0.0
Step: 28720, train/grad_norm: 2.709266118472442e-05
Step: 28720, train/learning_rate: 1.58257971634157e-05
Step: 28720, train/epoch: 6.834840774536133
Step: 28730, train/loss: 0.0
Step: 28730, train/grad_norm: 8.938056453189347e-06
Step: 28730, train/learning_rate: 1.5813897334737703e-05
Step: 28730, train/epoch: 6.837220191955566
Step: 28740, train/loss: 0.0
Step: 28740, train/grad_norm: 1.1004066436726134e-05
Step: 28740, train/learning_rate: 1.580199932504911e-05
Step: 28740, train/epoch: 6.839600086212158
Step: 28750, train/loss: 0.0
Step: 28750, train/grad_norm: 5.981712547509233e-08
Step: 28750, train/learning_rate: 1.5790099496371113e-05
Step: 28750, train/epoch: 6.84197998046875
Step: 28760, train/loss: 0.0044999998062849045
Step: 28760, train/grad_norm: 5.9340212743563825e-08
Step: 28760, train/learning_rate: 1.577820148668252e-05
Step: 28760, train/epoch: 6.844359874725342
Step: 28770, train/loss: 0.0
Step: 28770, train/grad_norm: 1.1871413789776852e-06
Step: 28770, train/learning_rate: 1.5766301658004522e-05
Step: 28770, train/epoch: 6.846739768981934
Step: 28780, train/loss: 0.0
Step: 28780, train/grad_norm: 2.6926511509373086e-07
Step: 28780, train/learning_rate: 1.5754401829326525e-05
Step: 28780, train/epoch: 6.849119663238525
Step: 28790, train/loss: 0.0
Step: 28790, train/grad_norm: 2.754042043306981e-07
Step: 28790, train/learning_rate: 1.5742503819637932e-05
Step: 28790, train/epoch: 6.851499080657959
Step: 28800, train/loss: 0.011699999682605267
Step: 28800, train/grad_norm: 4.121191523154266e-06
Step: 28800, train/learning_rate: 1.5730603990959935e-05
Step: 28800, train/epoch: 6.853878974914551
Step: 28810, train/loss: 0.0
Step: 28810, train/grad_norm: 3.393680572116864e-06
Step: 28810, train/learning_rate: 1.571870598127134e-05
Step: 28810, train/epoch: 6.856258869171143
Step: 28820, train/loss: 0.0
Step: 28820, train/grad_norm: 6.263335308176465e-06
Step: 28820, train/learning_rate: 1.5706806152593344e-05
Step: 28820, train/epoch: 6.858638763427734
Step: 28830, train/loss: 0.0
Step: 28830, train/grad_norm: 4.100563273823354e-06
Step: 28830, train/learning_rate: 1.5694906323915347e-05
Step: 28830, train/epoch: 6.861018657684326
Step: 28840, train/loss: 0.0
Step: 28840, train/grad_norm: 3.5828682598548767e-07
Step: 28840, train/learning_rate: 1.5683008314226754e-05
Step: 28840, train/epoch: 6.863398551940918
Step: 28850, train/loss: 0.0
Step: 28850, train/grad_norm: 7.084066601237282e-05
Step: 28850, train/learning_rate: 1.5671108485548757e-05
Step: 28850, train/epoch: 6.865777969360352
Step: 28860, train/loss: 0.0
Step: 28860, train/grad_norm: 3.0419471386267105e-06
Step: 28860, train/learning_rate: 1.5659210475860164e-05
Step: 28860, train/epoch: 6.868157863616943
Step: 28870, train/loss: 0.0
Step: 28870, train/grad_norm: 2.8063163881597575e-06
Step: 28870, train/learning_rate: 1.5647310647182167e-05
Step: 28870, train/epoch: 6.870537757873535
Step: 28880, train/loss: 0.0
Step: 28880, train/grad_norm: 6.120698685663228e-07
Step: 28880, train/learning_rate: 1.563541081850417e-05
Step: 28880, train/epoch: 6.872917652130127
Step: 28890, train/loss: 0.0
Step: 28890, train/grad_norm: 0.010920632630586624
Step: 28890, train/learning_rate: 1.5623512808815576e-05
Step: 28890, train/epoch: 6.875297546386719
Step: 28900, train/loss: 0.0
Step: 28900, train/grad_norm: 7.131320307962596e-05
Step: 28900, train/learning_rate: 1.561161298013758e-05
Step: 28900, train/epoch: 6.8776774406433105
Step: 28910, train/loss: 0.0
Step: 28910, train/grad_norm: 5.0309299695072696e-06
Step: 28910, train/learning_rate: 1.5599714970448986e-05
Step: 28910, train/epoch: 6.880057334899902
Step: 28920, train/loss: 0.0
Step: 28920, train/grad_norm: 7.17391444027271e-08
Step: 28920, train/learning_rate: 1.558781514177099e-05
Step: 28920, train/epoch: 6.882436752319336
Step: 28930, train/loss: 9.999999747378752e-05
Step: 28930, train/grad_norm: 1.1080324213708082e-08
Step: 28930, train/learning_rate: 1.5575917132082395e-05
Step: 28930, train/epoch: 6.884816646575928
Step: 28940, train/loss: 0.0
Step: 28940, train/grad_norm: 2.623006878366141e-07
Step: 28940, train/learning_rate: 1.5564017303404398e-05
Step: 28940, train/epoch: 6.8871965408325195
Step: 28950, train/loss: 0.0
Step: 28950, train/grad_norm: 0.001156807760708034
Step: 28950, train/learning_rate: 1.55521174747264e-05
Step: 28950, train/epoch: 6.889576435089111
Step: 28960, train/loss: 0.0
Step: 28960, train/grad_norm: 0.0009949445957317948
Step: 28960, train/learning_rate: 1.5540219465037808e-05
Step: 28960, train/epoch: 6.891956329345703
Step: 28970, train/loss: 9.999999747378752e-05
Step: 28970, train/grad_norm: 0.00018383938004262745
Step: 28970, train/learning_rate: 1.552831963635981e-05
Step: 28970, train/epoch: 6.894336223602295
Step: 28980, train/loss: 0.0
Step: 28980, train/grad_norm: 5.990369089658998e-08
Step: 28980, train/learning_rate: 1.5516421626671217e-05
Step: 28980, train/epoch: 6.8967156410217285
Step: 28990, train/loss: 0.0
Step: 28990, train/grad_norm: 0.14460228383541107
Step: 28990, train/learning_rate: 1.550452179799322e-05
Step: 28990, train/epoch: 6.89909553527832
Step: 29000, train/loss: 0.0
Step: 29000, train/grad_norm: 1.8507186894112237e-08
Step: 29000, train/learning_rate: 1.5492621969315223e-05
Step: 29000, train/epoch: 6.901475429534912
Step: 29010, train/loss: 0.0
Step: 29010, train/grad_norm: 1.0065852507068485e-07
Step: 29010, train/learning_rate: 1.548072395962663e-05
Step: 29010, train/epoch: 6.903855323791504
Step: 29020, train/loss: 0.000699999975040555
Step: 29020, train/grad_norm: 6.868472155474592e-07
Step: 29020, train/learning_rate: 1.5468824130948633e-05
Step: 29020, train/epoch: 6.906235218048096
Step: 29030, train/loss: 0.0
Step: 29030, train/grad_norm: 2.3866775222813885e-07
Step: 29030, train/learning_rate: 1.545692612126004e-05
Step: 29030, train/epoch: 6.9086151123046875
Step: 29040, train/loss: 0.0
Step: 29040, train/grad_norm: 2.7546846013137838e-06
Step: 29040, train/learning_rate: 1.5445026292582043e-05
Step: 29040, train/epoch: 6.910994529724121
Step: 29050, train/loss: 0.0
Step: 29050, train/grad_norm: 4.27994336860138e-07
Step: 29050, train/learning_rate: 1.5433126463904046e-05
Step: 29050, train/epoch: 6.913374423980713
Step: 29060, train/loss: 0.0
Step: 29060, train/grad_norm: 5.306664441206976e-09
Step: 29060, train/learning_rate: 1.5421228454215452e-05
Step: 29060, train/epoch: 6.915754318237305
Step: 29070, train/loss: 0.0
Step: 29070, train/grad_norm: 1.961670506034352e-07
Step: 29070, train/learning_rate: 1.5409328625537455e-05
Step: 29070, train/epoch: 6.9181342124938965
Step: 29080, train/loss: 0.0
Step: 29080, train/grad_norm: 2.73028588669913e-13
Step: 29080, train/learning_rate: 1.539743061584886e-05
Step: 29080, train/epoch: 6.920514106750488
Step: 29090, train/loss: 0.0
Step: 29090, train/grad_norm: 3.7859681611962515e-08
Step: 29090, train/learning_rate: 1.5385530787170865e-05
Step: 29090, train/epoch: 6.92289400100708
Step: 29100, train/loss: 0.0
Step: 29100, train/grad_norm: 8.425547548540635e-07
Step: 29100, train/learning_rate: 1.5373630958492868e-05
Step: 29100, train/epoch: 6.925273895263672
Step: 29110, train/loss: 9.999999747378752e-05
Step: 29110, train/grad_norm: 2.1801341063110158e-05
Step: 29110, train/learning_rate: 1.5361732948804274e-05
Step: 29110, train/epoch: 6.9276533126831055
Step: 29120, train/loss: 0.0
Step: 29120, train/grad_norm: 1.6901954280612586e-09
Step: 29120, train/learning_rate: 1.5349833120126277e-05
Step: 29120, train/epoch: 6.930033206939697
Step: 29130, train/loss: 0.0
Step: 29130, train/grad_norm: 5.716778872510986e-08
Step: 29130, train/learning_rate: 1.5337935110437684e-05
Step: 29130, train/epoch: 6.932413101196289
Step: 29140, train/loss: 0.0
Step: 29140, train/grad_norm: 2.031319112916208e-09
Step: 29140, train/learning_rate: 1.5326035281759687e-05
Step: 29140, train/epoch: 6.934792995452881
Step: 29150, train/loss: 0.0
Step: 29150, train/grad_norm: 5.239252800492977e-07
Step: 29150, train/learning_rate: 1.531413545308169e-05
Step: 29150, train/epoch: 6.937172889709473
Step: 29160, train/loss: 0.0
Step: 29160, train/grad_norm: 2.966193735076672e-09
Step: 29160, train/learning_rate: 1.5302237443393096e-05
Step: 29160, train/epoch: 6.9395527839660645
Step: 29170, train/loss: 0.0
Step: 29170, train/grad_norm: 6.387734856616589e-07
Step: 29170, train/learning_rate: 1.52903376147151e-05
Step: 29170, train/epoch: 6.941932201385498
Step: 29180, train/loss: 0.0
Step: 29180, train/grad_norm: 7.731709494862571e-09
Step: 29180, train/learning_rate: 1.5278439605026506e-05
Step: 29180, train/epoch: 6.94431209564209
Step: 29190, train/loss: 0.0
Step: 29190, train/grad_norm: 5.204920938695068e-09
Step: 29190, train/learning_rate: 1.526653977634851e-05
Step: 29190, train/epoch: 6.946691989898682
Step: 29200, train/loss: 0.0
Step: 29200, train/grad_norm: 3.199808196185927e-09
Step: 29200, train/learning_rate: 1.5254640857165214e-05
Step: 29200, train/epoch: 6.949071884155273
Step: 29210, train/loss: 0.0
Step: 29210, train/grad_norm: 7.828804271525769e-09
Step: 29210, train/learning_rate: 1.5242741937981918e-05
Step: 29210, train/epoch: 6.951451778411865
Step: 29220, train/loss: 0.0
Step: 29220, train/grad_norm: 3.4761224014800973e-06
Step: 29220, train/learning_rate: 1.5230842109303921e-05
Step: 29220, train/epoch: 6.953831672668457
Step: 29230, train/loss: 0.0
Step: 29230, train/grad_norm: 4.6632829542314624e-11
Step: 29230, train/learning_rate: 1.5218943190120626e-05
Step: 29230, train/epoch: 6.956211090087891
Step: 29240, train/loss: 0.0
Step: 29240, train/grad_norm: 9.685670192993712e-06
Step: 29240, train/learning_rate: 1.5207044270937331e-05
Step: 29240, train/epoch: 6.958590984344482
Step: 29250, train/loss: 0.0
Step: 29250, train/grad_norm: 4.0031682857488704e-08
Step: 29250, train/learning_rate: 1.5195145351754036e-05
Step: 29250, train/epoch: 6.960970878601074
Step: 29260, train/loss: 0.0
Step: 29260, train/grad_norm: 3.267949522101077e-10
Step: 29260, train/learning_rate: 1.518324643257074e-05
Step: 29260, train/epoch: 6.963350772857666
Step: 29270, train/loss: 0.0
Step: 29270, train/grad_norm: 3.9516359606750484e-07
Step: 29270, train/learning_rate: 1.5171346603892744e-05
Step: 29270, train/epoch: 6.965730667114258
Step: 29280, train/loss: 0.0
Step: 29280, train/grad_norm: 4.71518435407603e-10
Step: 29280, train/learning_rate: 1.5159447684709448e-05
Step: 29280, train/epoch: 6.96811056137085
Step: 29290, train/loss: 0.0
Step: 29290, train/grad_norm: 5.210752715356648e-05
Step: 29290, train/learning_rate: 1.5147548765526153e-05
Step: 29290, train/epoch: 6.970490455627441
Step: 29300, train/loss: 0.0
Step: 29300, train/grad_norm: 3.714633267648537e-09
Step: 29300, train/learning_rate: 1.5135649846342858e-05
Step: 29300, train/epoch: 6.972869873046875
Step: 29310, train/loss: 0.0
Step: 29310, train/grad_norm: 9.067177231258938e-09
Step: 29310, train/learning_rate: 1.5123750927159563e-05
Step: 29310, train/epoch: 6.975249767303467
Step: 29320, train/loss: 0.0
Step: 29320, train/grad_norm: 4.2095762986527063e-10
Step: 29320, train/learning_rate: 1.5111851098481566e-05
Step: 29320, train/epoch: 6.977629661560059
Step: 29330, train/loss: 0.0
Step: 29330, train/grad_norm: 6.311954621196492e-06
Step: 29330, train/learning_rate: 1.509995217929827e-05
Step: 29330, train/epoch: 6.98000955581665
Step: 29340, train/loss: 0.0
Step: 29340, train/grad_norm: 5.482374998422301e-09
Step: 29340, train/learning_rate: 1.5088053260114975e-05
Step: 29340, train/epoch: 6.982389450073242
Step: 29350, train/loss: 0.0
Step: 29350, train/grad_norm: 2.766854800828611e-10
Step: 29350, train/learning_rate: 1.507615434093168e-05
Step: 29350, train/epoch: 6.984769344329834
Step: 29360, train/loss: 0.0
Step: 29360, train/grad_norm: 5.90900217645185e-10
Step: 29360, train/learning_rate: 1.5064255421748385e-05
Step: 29360, train/epoch: 6.987148761749268
Step: 29370, train/loss: 0.0
Step: 29370, train/grad_norm: 1.1321040460643417e-07
Step: 29370, train/learning_rate: 1.5052355593070388e-05
Step: 29370, train/epoch: 6.989528656005859
Step: 29380, train/loss: 0.0
Step: 29380, train/grad_norm: 8.11189337923679e-09
Step: 29380, train/learning_rate: 1.5040456673887093e-05
Step: 29380, train/epoch: 6.991908550262451
Step: 29390, train/loss: 0.0
Step: 29390, train/grad_norm: 1.9507400850216072e-07
Step: 29390, train/learning_rate: 1.5028557754703797e-05
Step: 29390, train/epoch: 6.994288444519043
Step: 29400, train/loss: 0.0
Step: 29400, train/grad_norm: 5.170775474461209e-10
Step: 29400, train/learning_rate: 1.5016658835520502e-05
Step: 29400, train/epoch: 6.996668338775635
Step: 29410, train/loss: 0.0
Step: 29410, train/grad_norm: 1.957379325290276e-08
Step: 29410, train/learning_rate: 1.5004759916337207e-05
Step: 29410, train/epoch: 6.999048233032227
Step: 29414, eval/loss: 0.03513983637094498
Step: 29414, eval/accuracy: 0.9972233772277832
Step: 29414, eval/f1: 0.9970650672912598
Step: 29414, eval/runtime: 735.9965209960938
Step: 29414, eval/samples_per_second: 9.786999702453613
Step: 29414, eval/steps_per_second: 1.2239999771118164
Step: 29414, train/epoch: 7.0
Step: 29420, train/loss: 0.0
Step: 29420, train/grad_norm: 1.552650985559012e-07
Step: 29420, train/learning_rate: 1.4992860997153912e-05
Step: 29420, train/epoch: 7.001428127288818
Step: 29430, train/loss: 0.0
Step: 29430, train/grad_norm: 1.4603265818813327e-10
Step: 29430, train/learning_rate: 1.4980961168475915e-05
Step: 29430, train/epoch: 7.003807544708252
Step: 29440, train/loss: 0.0
Step: 29440, train/grad_norm: 4.136934197984665e-11
Step: 29440, train/learning_rate: 1.496906224929262e-05
Step: 29440, train/epoch: 7.006187438964844
Step: 29450, train/loss: 0.0
Step: 29450, train/grad_norm: 3.128116654593782e-09
Step: 29450, train/learning_rate: 1.4957163330109324e-05
Step: 29450, train/epoch: 7.0085673332214355
Step: 29460, train/loss: 0.0
Step: 29460, train/grad_norm: 1.046489206402157e-08
Step: 29460, train/learning_rate: 1.4945264410926029e-05
Step: 29460, train/epoch: 7.010947227478027
Step: 29470, train/loss: 0.0
Step: 29470, train/grad_norm: 1.0512133741258367e-07
Step: 29470, train/learning_rate: 1.4933365491742734e-05
Step: 29470, train/epoch: 7.013327121734619
Step: 29480, train/loss: 0.0
Step: 29480, train/grad_norm: 3.771998180468472e-08
Step: 29480, train/learning_rate: 1.4921465663064737e-05
Step: 29480, train/epoch: 7.015707015991211
Step: 29490, train/loss: 0.0
Step: 29490, train/grad_norm: 1.6281663794970314e-09
Step: 29490, train/learning_rate: 1.4909566743881442e-05
Step: 29490, train/epoch: 7.0180864334106445
Step: 29500, train/loss: 0.0
Step: 29500, train/grad_norm: 1.9173487686430235e-08
Step: 29500, train/learning_rate: 1.4897667824698146e-05
Step: 29500, train/epoch: 7.020466327667236
Step: 29510, train/loss: 0.0
Step: 29510, train/grad_norm: 4.349439223005902e-06
Step: 29510, train/learning_rate: 1.4885768905514851e-05
Step: 29510, train/epoch: 7.022846221923828
Step: 29520, train/loss: 0.0
Step: 29520, train/grad_norm: 6.8393908314590135e-09
Step: 29520, train/learning_rate: 1.4873869986331556e-05
Step: 29520, train/epoch: 7.02522611618042
Step: 29530, train/loss: 0.0
Step: 29530, train/grad_norm: 8.628280170341895e-07
Step: 29530, train/learning_rate: 1.4861970157653559e-05
Step: 29530, train/epoch: 7.027606010437012
Step: 29540, train/loss: 0.0
Step: 29540, train/grad_norm: 6.552506093271404e-09
Step: 29540, train/learning_rate: 1.4850071238470264e-05
Step: 29540, train/epoch: 7.0299859046936035
Step: 29550, train/loss: 0.0
Step: 29550, train/grad_norm: 1.561804674565792e-05
Step: 29550, train/learning_rate: 1.4838172319286969e-05
Step: 29550, train/epoch: 7.032365322113037
Step: 29560, train/loss: 0.0
Step: 29560, train/grad_norm: 6.215261638686798e-09
Step: 29560, train/learning_rate: 1.4826273400103673e-05
Step: 29560, train/epoch: 7.034745216369629
Step: 29570, train/loss: 0.0
Step: 29570, train/grad_norm: 7.181868988404005e-11
Step: 29570, train/learning_rate: 1.4814374480920378e-05
Step: 29570, train/epoch: 7.037125110626221
Step: 29580, train/loss: 0.0
Step: 29580, train/grad_norm: 1.2563467635118286e-06
Step: 29580, train/learning_rate: 1.4802474652242381e-05
Step: 29580, train/epoch: 7.0395050048828125
Step: 29590, train/loss: 0.0
Step: 29590, train/grad_norm: 5.647575207490263e-08
Step: 29590, train/learning_rate: 1.4790575733059086e-05
Step: 29590, train/epoch: 7.041884899139404
Step: 29600, train/loss: 0.0
Step: 29600, train/grad_norm: 1.7267220986383336e-10
Step: 29600, train/learning_rate: 1.477867681387579e-05
Step: 29600, train/epoch: 7.044264793395996
Step: 29610, train/loss: 0.0
Step: 29610, train/grad_norm: 8.563520559334847e-09
Step: 29610, train/learning_rate: 1.4766777894692495e-05
Step: 29610, train/epoch: 7.046644687652588
Step: 29620, train/loss: 0.0
Step: 29620, train/grad_norm: 3.2497666779818246e-07
Step: 29620, train/learning_rate: 1.47548789755092e-05
Step: 29620, train/epoch: 7.0490241050720215
Step: 29630, train/loss: 0.0
Step: 29630, train/grad_norm: 4.466458449314814e-06
Step: 29630, train/learning_rate: 1.4742979146831203e-05
Step: 29630, train/epoch: 7.051403999328613
Step: 29640, train/loss: 0.0
Step: 29640, train/grad_norm: 1.0860524285449813e-10
Step: 29640, train/learning_rate: 1.4731080227647908e-05
Step: 29640, train/epoch: 7.053783893585205
Step: 29650, train/loss: 0.0
Step: 29650, train/grad_norm: 1.4198783901520073e-05
Step: 29650, train/learning_rate: 1.4719181308464613e-05
Step: 29650, train/epoch: 7.056163787841797
Step: 29660, train/loss: 0.0
Step: 29660, train/grad_norm: 0.0002626886998768896
Step: 29660, train/learning_rate: 1.4707282389281318e-05
Step: 29660, train/epoch: 7.058543682098389
Step: 29670, train/loss: 0.0
Step: 29670, train/grad_norm: 7.877255514543435e-10
Step: 29670, train/learning_rate: 1.4695383470098022e-05
Step: 29670, train/epoch: 7.0609235763549805
Step: 29680, train/loss: 0.0
Step: 29680, train/grad_norm: 1.3947121235702298e-09
Step: 29680, train/learning_rate: 1.4683483641420025e-05
Step: 29680, train/epoch: 7.063302993774414
Step: 29690, train/loss: 0.0
Step: 29690, train/grad_norm: 4.008761100848801e-10
Step: 29690, train/learning_rate: 1.467158472223673e-05
Step: 29690, train/epoch: 7.065682888031006
Step: 29700, train/loss: 0.0
Step: 29700, train/grad_norm: 0.00011376894690329209
Step: 29700, train/learning_rate: 1.4659685803053435e-05
Step: 29700, train/epoch: 7.068062782287598
Step: 29710, train/loss: 0.0
Step: 29710, train/grad_norm: 4.444889456500789e-10
Step: 29710, train/learning_rate: 1.464778688387014e-05
Step: 29710, train/epoch: 7.0704426765441895
Step: 29720, train/loss: 0.0
Step: 29720, train/grad_norm: 7.426219195139083e-09
Step: 29720, train/learning_rate: 1.4635887964686844e-05
Step: 29720, train/epoch: 7.072822570800781
Step: 29730, train/loss: 0.0
Step: 29730, train/grad_norm: 1.2113941794122951e-11
Step: 29730, train/learning_rate: 1.4623988136008848e-05
Step: 29730, train/epoch: 7.075202465057373
Step: 29740, train/loss: 0.0
Step: 29740, train/grad_norm: 4.2690618329288554e-07
Step: 29740, train/learning_rate: 1.4612089216825552e-05
Step: 29740, train/epoch: 7.077581882476807
Step: 29750, train/loss: 0.0
Step: 29750, train/grad_norm: 1.4854935059815944e-09
Step: 29750, train/learning_rate: 1.4600190297642257e-05
Step: 29750, train/epoch: 7.079961776733398
Step: 29760, train/loss: 0.0
Step: 29760, train/grad_norm: 7.232862312550603e-12
Step: 29760, train/learning_rate: 1.4588291378458962e-05
Step: 29760, train/epoch: 7.08234167098999
Step: 29770, train/loss: 0.0
Step: 29770, train/grad_norm: 4.54293294751551e-05
Step: 29770, train/learning_rate: 1.4576392459275667e-05
Step: 29770, train/epoch: 7.084721565246582
Step: 29780, train/loss: 0.0
Step: 29780, train/grad_norm: 4.031593947573242e-10
Step: 29780, train/learning_rate: 1.4564493540092371e-05
Step: 29780, train/epoch: 7.087101459503174
Step: 29790, train/loss: 0.0
Step: 29790, train/grad_norm: 1.4221942024050804e-07
Step: 29790, train/learning_rate: 1.4552593711414374e-05
Step: 29790, train/epoch: 7.089481353759766
Step: 29800, train/loss: 0.0007999999797903001
Step: 29800, train/grad_norm: 3.6818629922663604e-08
Step: 29800, train/learning_rate: 1.454069479223108e-05
Step: 29800, train/epoch: 7.091861248016357
Step: 29810, train/loss: 0.0
Step: 29810, train/grad_norm: 2.8503174213240223e-12
Step: 29810, train/learning_rate: 1.4528795873047784e-05
Step: 29810, train/epoch: 7.094240665435791
Step: 29820, train/loss: 0.0
Step: 29820, train/grad_norm: 1.803917484721751e-06
Step: 29820, train/learning_rate: 1.4516896953864489e-05
Step: 29820, train/epoch: 7.096620559692383
Step: 29830, train/loss: 0.0
Step: 29830, train/grad_norm: 1.695619977759577e-09
Step: 29830, train/learning_rate: 1.4504998034681194e-05
Step: 29830, train/epoch: 7.099000453948975
Step: 29840, train/loss: 0.0
Step: 29840, train/grad_norm: 1.866868437616631e-09
Step: 29840, train/learning_rate: 1.4493098206003197e-05
Step: 29840, train/epoch: 7.101380348205566
Step: 29850, train/loss: 0.0737999975681305
Step: 29850, train/grad_norm: 3.571763995680044e-09
Step: 29850, train/learning_rate: 1.4481199286819901e-05
Step: 29850, train/epoch: 7.103760242462158
Step: 29860, train/loss: 0.0
Step: 29860, train/grad_norm: 3.78892206498449e-09
Step: 29860, train/learning_rate: 1.4469300367636606e-05
Step: 29860, train/epoch: 7.10614013671875
Step: 29870, train/loss: 0.0
Step: 29870, train/grad_norm: 1.5114771656499215e-08
Step: 29870, train/learning_rate: 1.4457401448453311e-05
Step: 29870, train/epoch: 7.108519554138184
Step: 29880, train/loss: 0.0
Step: 29880, train/grad_norm: 3.33326823920288e-08
Step: 29880, train/learning_rate: 1.4445502529270016e-05
Step: 29880, train/epoch: 7.110899448394775
Step: 29890, train/loss: 0.0
Step: 29890, train/grad_norm: 1.0803739314724226e-05
Step: 29890, train/learning_rate: 1.4433602700592019e-05
Step: 29890, train/epoch: 7.113279342651367
Step: 29900, train/loss: 0.0
Step: 29900, train/grad_norm: 5.672400220646523e-06
Step: 29900, train/learning_rate: 1.4421703781408723e-05
Step: 29900, train/epoch: 7.115659236907959
Step: 29910, train/loss: 0.0
Step: 29910, train/grad_norm: 5.107732704345835e-06
Step: 29910, train/learning_rate: 1.4409804862225428e-05
Step: 29910, train/epoch: 7.118039131164551
Step: 29920, train/loss: 0.0
Step: 29920, train/grad_norm: 4.357813850219827e-06
Step: 29920, train/learning_rate: 1.4397905943042133e-05
Step: 29920, train/epoch: 7.120419025421143
Step: 29930, train/loss: 0.0
Step: 29930, train/grad_norm: 3.3229065332651686e-11
Step: 29930, train/learning_rate: 1.4386007023858838e-05
Step: 29930, train/epoch: 7.122798442840576
Step: 29940, train/loss: 0.0
Step: 29940, train/grad_norm: 1.5221134574971984e-08
Step: 29940, train/learning_rate: 1.437410719518084e-05
Step: 29940, train/epoch: 7.125178337097168
Step: 29950, train/loss: 0.0
Step: 29950, train/grad_norm: 1.75574399463585e-08
Step: 29950, train/learning_rate: 1.4362208275997546e-05
Step: 29950, train/epoch: 7.12755823135376
Step: 29960, train/loss: 0.0
Step: 29960, train/grad_norm: 1.2858086817502112e-10
Step: 29960, train/learning_rate: 1.435030935681425e-05
Step: 29960, train/epoch: 7.129938125610352
Step: 29970, train/loss: 0.0
Step: 29970, train/grad_norm: 6.484291437125478e-10
Step: 29970, train/learning_rate: 1.4338410437630955e-05
Step: 29970, train/epoch: 7.132318019866943
Step: 29980, train/loss: 0.0
Step: 29980, train/grad_norm: 3.079144335060846e-07
Step: 29980, train/learning_rate: 1.432651151844766e-05
Step: 29980, train/epoch: 7.134697914123535
Step: 29990, train/loss: 0.0
Step: 29990, train/grad_norm: 6.740330515953019e-09
Step: 29990, train/learning_rate: 1.4314611689769663e-05
Step: 29990, train/epoch: 7.137077808380127
Step: 30000, train/loss: 0.0
Step: 30000, train/grad_norm: 3.060622688622061e-08
Step: 30000, train/learning_rate: 1.4302712770586368e-05
Step: 30000, train/epoch: 7.1394572257995605
Step: 30010, train/loss: 0.0
Step: 30010, train/grad_norm: 1.6103113509302602e-08
Step: 30010, train/learning_rate: 1.4290813851403072e-05
Step: 30010, train/epoch: 7.141837120056152
Step: 30020, train/loss: 0.0
Step: 30020, train/grad_norm: 5.943124961049762e-07
Step: 30020, train/learning_rate: 1.4278914932219777e-05
Step: 30020, train/epoch: 7.144217014312744
Step: 30030, train/loss: 0.0
Step: 30030, train/grad_norm: 2.508407170864757e-08
Step: 30030, train/learning_rate: 1.4267016013036482e-05
Step: 30030, train/epoch: 7.146596908569336
Step: 30040, train/loss: 0.0
Step: 30040, train/grad_norm: 3.4451883568209496e-09
Step: 30040, train/learning_rate: 1.4255116184358485e-05
Step: 30040, train/epoch: 7.148976802825928
Step: 30050, train/loss: 0.0
Step: 30050, train/grad_norm: 1.6852695905456017e-09
Step: 30050, train/learning_rate: 1.424321726517519e-05
Step: 30050, train/epoch: 7.1513566970825195
Step: 30060, train/loss: 0.0
Step: 30060, train/grad_norm: 8.16002110326508e-09
Step: 30060, train/learning_rate: 1.4231318345991895e-05
Step: 30060, train/epoch: 7.153736114501953
Step: 30070, train/loss: 0.0
Step: 30070, train/grad_norm: 7.537827029224786e-10
Step: 30070, train/learning_rate: 1.42194194268086e-05
Step: 30070, train/epoch: 7.156116008758545
Step: 30080, train/loss: 0.0
Step: 30080, train/grad_norm: 1.0975128361678799e-06
Step: 30080, train/learning_rate: 1.4207520507625304e-05
Step: 30080, train/epoch: 7.158495903015137
Step: 30090, train/loss: 0.0
Step: 30090, train/grad_norm: 1.0124861837823573e-08
Step: 30090, train/learning_rate: 1.4195620678947307e-05
Step: 30090, train/epoch: 7.1608757972717285
Step: 30100, train/loss: 0.0
Step: 30100, train/grad_norm: 3.485485233767349e-09
Step: 30100, train/learning_rate: 1.4183721759764012e-05
Step: 30100, train/epoch: 7.16325569152832
Step: 30110, train/loss: 0.0
Step: 30110, train/grad_norm: 2.3291907069733497e-08
Step: 30110, train/learning_rate: 1.4171822840580717e-05
Step: 30110, train/epoch: 7.165635585784912
Step: 30120, train/loss: 0.0
Step: 30120, train/grad_norm: 9.503395403953618e-09
Step: 30120, train/learning_rate: 1.4159923921397422e-05
Step: 30120, train/epoch: 7.168015003204346
Step: 30130, train/loss: 0.0
Step: 30130, train/grad_norm: 2.8074717306481034e-07
Step: 30130, train/learning_rate: 1.4148025002214126e-05
Step: 30130, train/epoch: 7.1703948974609375
Step: 30140, train/loss: 0.0
Step: 30140, train/grad_norm: 1.548618677738034e-09
Step: 30140, train/learning_rate: 1.4136126083030831e-05
Step: 30140, train/epoch: 7.172774791717529
Step: 30150, train/loss: 0.0
Step: 30150, train/grad_norm: 8.388773009571082e-10
Step: 30150, train/learning_rate: 1.4124226254352834e-05
Step: 30150, train/epoch: 7.175154685974121
Step: 30160, train/loss: 0.0
Step: 30160, train/grad_norm: 2.435406563916498e-11
Step: 30160, train/learning_rate: 1.4112327335169539e-05
Step: 30160, train/epoch: 7.177534580230713
Step: 30170, train/loss: 0.0
Step: 30170, train/grad_norm: 1.167853511674366e-07
Step: 30170, train/learning_rate: 1.4100428415986244e-05
Step: 30170, train/epoch: 7.179914474487305
Step: 30180, train/loss: 0.0
Step: 30180, train/grad_norm: 2.0996708816056753e-09
Step: 30180, train/learning_rate: 1.4088529496802948e-05
Step: 30180, train/epoch: 7.1822943687438965
Step: 30190, train/loss: 0.0
Step: 30190, train/grad_norm: 3.8887377762364395e-09
Step: 30190, train/learning_rate: 1.4076630577619653e-05
Step: 30190, train/epoch: 7.18467378616333
Step: 30200, train/loss: 0.0
Step: 30200, train/grad_norm: 1.1747254546889963e-07
Step: 30200, train/learning_rate: 1.4064730748941656e-05
Step: 30200, train/epoch: 7.187053680419922
Step: 30210, train/loss: 0.0
Step: 30210, train/grad_norm: 1.088472458832257e-06
Step: 30210, train/learning_rate: 1.4052831829758361e-05
Step: 30210, train/epoch: 7.189433574676514
Step: 30220, train/loss: 0.0
Step: 30220, train/grad_norm: 3.4451935748691653e-10
Step: 30220, train/learning_rate: 1.4040932910575066e-05
Step: 30220, train/epoch: 7.1918134689331055
Step: 30230, train/loss: 0.0
Step: 30230, train/grad_norm: 6.673434455864088e-14
Step: 30230, train/learning_rate: 1.402903399139177e-05
Step: 30230, train/epoch: 7.194193363189697
Step: 30240, train/loss: 0.0
Step: 30240, train/grad_norm: 1.0732287591963541e-07
Step: 30240, train/learning_rate: 1.4017135072208475e-05
Step: 30240, train/epoch: 7.196573257446289
Step: 30250, train/loss: 0.0
Step: 30250, train/grad_norm: 1.4699219619274118e-09
Step: 30250, train/learning_rate: 1.4005235243530478e-05
Step: 30250, train/epoch: 7.198952674865723
Step: 30260, train/loss: 0.0
Step: 30260, train/grad_norm: 1.289267359538826e-09
Step: 30260, train/learning_rate: 1.3993336324347183e-05
Step: 30260, train/epoch: 7.2013325691223145
Step: 30270, train/loss: 0.0
Step: 30270, train/grad_norm: 1.12096664395267e-07
Step: 30270, train/learning_rate: 1.3981437405163888e-05
Step: 30270, train/epoch: 7.203712463378906
Step: 30280, train/loss: 0.0
Step: 30280, train/grad_norm: 3.275487642895314e-06
Step: 30280, train/learning_rate: 1.3969538485980593e-05
Step: 30280, train/epoch: 7.206092357635498
Step: 30290, train/loss: 0.0
Step: 30290, train/grad_norm: 1.2897737633466022e-06
Step: 30290, train/learning_rate: 1.3957639566797297e-05
Step: 30290, train/epoch: 7.20847225189209
Step: 30300, train/loss: 0.0
Step: 30300, train/grad_norm: 7.731137396937982e-10
Step: 30300, train/learning_rate: 1.39457397381193e-05
Step: 30300, train/epoch: 7.210852146148682
Step: 30310, train/loss: 0.0
Step: 30310, train/grad_norm: 1.0836909147826645e-08
Step: 30310, train/learning_rate: 1.3933840818936005e-05
Step: 30310, train/epoch: 7.213231563568115
Step: 30320, train/loss: 0.0
Step: 30320, train/grad_norm: 2.0309834369847124e-10
Step: 30320, train/learning_rate: 1.392194189975271e-05
Step: 30320, train/epoch: 7.215611457824707
Step: 30330, train/loss: 0.0
Step: 30330, train/grad_norm: 3.4036978791007755e-10
Step: 30330, train/learning_rate: 1.3910042980569415e-05
Step: 30330, train/epoch: 7.217991352081299
Step: 30340, train/loss: 0.0
Step: 30340, train/grad_norm: 9.623040142514583e-10
Step: 30340, train/learning_rate: 1.389814406138612e-05
Step: 30340, train/epoch: 7.220371246337891
Step: 30350, train/loss: 0.0
Step: 30350, train/grad_norm: 1.0479642154059832e-10
Step: 30350, train/learning_rate: 1.3886244232708123e-05
Step: 30350, train/epoch: 7.222751140594482
Step: 30360, train/loss: 0.0
Step: 30360, train/grad_norm: 0.00012947135837748647
Step: 30360, train/learning_rate: 1.3874345313524827e-05
Step: 30360, train/epoch: 7.225131034851074
Step: 30370, train/loss: 0.0
Step: 30370, train/grad_norm: 8.288221238217375e-07
Step: 30370, train/learning_rate: 1.3862446394341532e-05
Step: 30370, train/epoch: 7.227510929107666
Step: 30380, train/loss: 0.0
Step: 30380, train/grad_norm: 6.379047845506136e-10
Step: 30380, train/learning_rate: 1.3850547475158237e-05
Step: 30380, train/epoch: 7.2298903465271
Step: 30390, train/loss: 0.0
Step: 30390, train/grad_norm: 2.0064844008516047e-08
Step: 30390, train/learning_rate: 1.3838648555974942e-05
Step: 30390, train/epoch: 7.232270240783691
Step: 30400, train/loss: 0.0
Step: 30400, train/grad_norm: 1.4945449322567583e-09
Step: 30400, train/learning_rate: 1.3826748727296945e-05
Step: 30400, train/epoch: 7.234650135040283
Step: 30410, train/loss: 0.0
Step: 30410, train/grad_norm: 4.466796781343874e-06
Step: 30410, train/learning_rate: 1.381484980811365e-05
Step: 30410, train/epoch: 7.237030029296875
Step: 30420, train/loss: 0.0
Step: 30420, train/grad_norm: 4.662243924258291e-09
Step: 30420, train/learning_rate: 1.3802950888930354e-05
Step: 30420, train/epoch: 7.239409923553467
Step: 30430, train/loss: 0.0
Step: 30430, train/grad_norm: 1.4189939889774905e-08
Step: 30430, train/learning_rate: 1.3791051969747059e-05
Step: 30430, train/epoch: 7.241789817810059
Step: 30440, train/loss: 0.0
Step: 30440, train/grad_norm: 2.65488231399047e-09
Step: 30440, train/learning_rate: 1.3779153050563764e-05
Step: 30440, train/epoch: 7.244169235229492
Step: 30450, train/loss: 0.0
Step: 30450, train/grad_norm: 6.747356007252847e-10
Step: 30450, train/learning_rate: 1.3767254131380469e-05
Step: 30450, train/epoch: 7.246549129486084
Step: 30460, train/loss: 0.0
Step: 30460, train/grad_norm: 7.724516990492702e-07
Step: 30460, train/learning_rate: 1.3755354302702472e-05
Step: 30460, train/epoch: 7.248929023742676
Step: 30470, train/loss: 0.0
Step: 30470, train/grad_norm: 8.288978947668113e-10
Step: 30470, train/learning_rate: 1.3743455383519176e-05
Step: 30470, train/epoch: 7.251308917999268
Step: 30480, train/loss: 0.0
Step: 30480, train/grad_norm: 1.965526852387711e-09
Step: 30480, train/learning_rate: 1.3731556464335881e-05
Step: 30480, train/epoch: 7.253688812255859
Step: 30490, train/loss: 0.0
Step: 30490, train/grad_norm: 6.046972367279579e-10
Step: 30490, train/learning_rate: 1.3719657545152586e-05
Step: 30490, train/epoch: 7.256068706512451
Step: 30500, train/loss: 0.0
Step: 30500, train/grad_norm: 7.448486827321688e-10
Step: 30500, train/learning_rate: 1.370775862596929e-05
Step: 30500, train/epoch: 7.258448123931885
Step: 30510, train/loss: 0.0
Step: 30510, train/grad_norm: 4.2777997810716784e-10
Step: 30510, train/learning_rate: 1.3695858797291294e-05
Step: 30510, train/epoch: 7.260828018188477
Step: 30520, train/loss: 0.05389999970793724
Step: 30520, train/grad_norm: 3.1984237480742195e-09
Step: 30520, train/learning_rate: 1.3683959878107999e-05
Step: 30520, train/epoch: 7.263207912445068
Step: 30530, train/loss: 0.0
Step: 30530, train/grad_norm: 2.710725937049574e-07
Step: 30530, train/learning_rate: 1.3672060958924703e-05
Step: 30530, train/epoch: 7.26558780670166
Step: 30540, train/loss: 0.0
Step: 30540, train/grad_norm: 5.572785011231929e-10
Step: 30540, train/learning_rate: 1.3660162039741408e-05
Step: 30540, train/epoch: 7.267967700958252
Step: 30550, train/loss: 0.0
Step: 30550, train/grad_norm: 9.763253956407425e-07
Step: 30550, train/learning_rate: 1.3648263120558113e-05
Step: 30550, train/epoch: 7.270347595214844
Step: 30560, train/loss: 0.0
Step: 30560, train/grad_norm: 2.5387796032116938e-11
Step: 30560, train/learning_rate: 1.3636363291880116e-05
Step: 30560, train/epoch: 7.2727274894714355
Step: 30570, train/loss: 0.0
Step: 30570, train/grad_norm: 1.4575483042733595e-08
Step: 30570, train/learning_rate: 1.362446437269682e-05
Step: 30570, train/epoch: 7.275106906890869
Step: 30580, train/loss: 0.0
Step: 30580, train/grad_norm: 2.21257634436256e-09
Step: 30580, train/learning_rate: 1.3612565453513525e-05
Step: 30580, train/epoch: 7.277486801147461
Step: 30590, train/loss: 0.0
Step: 30590, train/grad_norm: 5.19118385430839e-12
Step: 30590, train/learning_rate: 1.360066653433023e-05
Step: 30590, train/epoch: 7.279866695404053
Step: 30600, train/loss: 9.999999747378752e-05
Step: 30600, train/grad_norm: 3.236555988495038e-12
Step: 30600, train/learning_rate: 1.3588767615146935e-05
Step: 30600, train/epoch: 7.2822465896606445
Step: 30610, train/loss: 0.0
Step: 30610, train/grad_norm: 5.470853547961951e-09
Step: 30610, train/learning_rate: 1.3576867786468938e-05
Step: 30610, train/epoch: 7.284626483917236
Step: 30620, train/loss: 0.0
Step: 30620, train/grad_norm: 9.295924246544018e-05
Step: 30620, train/learning_rate: 1.3564968867285643e-05
Step: 30620, train/epoch: 7.287006378173828
Step: 30630, train/loss: 0.0
Step: 30630, train/grad_norm: 1.7747659342148836e-08
Step: 30630, train/learning_rate: 1.3553069948102348e-05
Step: 30630, train/epoch: 7.289385795593262
Step: 30640, train/loss: 0.0
Step: 30640, train/grad_norm: 1.6928926243053866e-06
Step: 30640, train/learning_rate: 1.3541171028919052e-05
Step: 30640, train/epoch: 7.2917656898498535
Step: 30650, train/loss: 0.0
Step: 30650, train/grad_norm: 7.331677807087544e-06
Step: 30650, train/learning_rate: 1.3529272109735757e-05
Step: 30650, train/epoch: 7.294145584106445
Step: 30660, train/loss: 0.0
Step: 30660, train/grad_norm: 3.02987968048285e-09
Step: 30660, train/learning_rate: 1.351737228105776e-05
Step: 30660, train/epoch: 7.296525478363037
Step: 30670, train/loss: 0.0
Step: 30670, train/grad_norm: 1.1530373100754332e-08
Step: 30670, train/learning_rate: 1.3505473361874465e-05
Step: 30670, train/epoch: 7.298905372619629
Step: 30680, train/loss: 0.0
Step: 30680, train/grad_norm: 1.447800741516403e-07
Step: 30680, train/learning_rate: 1.349357444269117e-05
Step: 30680, train/epoch: 7.301285266876221
Step: 30690, train/loss: 0.0
Step: 30690, train/grad_norm: 1.2573637686585926e-09
Step: 30690, train/learning_rate: 1.3481675523507874e-05
Step: 30690, train/epoch: 7.303664684295654
Step: 30700, train/loss: 0.0
Step: 30700, train/grad_norm: 3.932802428607829e-06
Step: 30700, train/learning_rate: 1.346977660432458e-05
Step: 30700, train/epoch: 7.306044578552246
Step: 30710, train/loss: 0.0
Step: 30710, train/grad_norm: 8.131571860303666e-08
Step: 30710, train/learning_rate: 1.3457876775646582e-05
Step: 30710, train/epoch: 7.308424472808838
Step: 30720, train/loss: 0.0
Step: 30720, train/grad_norm: 4.518150944932131e-06
Step: 30720, train/learning_rate: 1.3445977856463287e-05
Step: 30720, train/epoch: 7.31080436706543
Step: 30730, train/loss: 0.0
Step: 30730, train/grad_norm: 3.424668193474645e-08
Step: 30730, train/learning_rate: 1.3434078937279992e-05
Step: 30730, train/epoch: 7.3131842613220215
Step: 30740, train/loss: 0.0
Step: 30740, train/grad_norm: 3.422982564060817e-09
Step: 30740, train/learning_rate: 1.3422180018096697e-05
Step: 30740, train/epoch: 7.315564155578613
Step: 30750, train/loss: 0.0
Step: 30750, train/grad_norm: 1.2758697494064108e-06
Step: 30750, train/learning_rate: 1.3410281098913401e-05
Step: 30750, train/epoch: 7.317944049835205
Step: 30760, train/loss: 0.0
Step: 30760, train/grad_norm: 4.570136297843419e-06
Step: 30760, train/learning_rate: 1.3398381270235404e-05
Step: 30760, train/epoch: 7.320323467254639
Step: 30770, train/loss: 0.0
Step: 30770, train/grad_norm: 4.257489027992278e-09
Step: 30770, train/learning_rate: 1.3386482351052109e-05
Step: 30770, train/epoch: 7.3227033615112305
Step: 30780, train/loss: 0.0
Step: 30780, train/grad_norm: 1.1115501408198725e-08
Step: 30780, train/learning_rate: 1.3374583431868814e-05
Step: 30780, train/epoch: 7.325083255767822
Step: 30790, train/loss: 0.0
Step: 30790, train/grad_norm: 8.42156896396773e-06
Step: 30790, train/learning_rate: 1.3362684512685519e-05
Step: 30790, train/epoch: 7.327463150024414
Step: 30800, train/loss: 0.0
Step: 30800, train/grad_norm: 3.5281291776989576e-11
Step: 30800, train/learning_rate: 1.3350785593502223e-05
Step: 30800, train/epoch: 7.329843044281006
Step: 30810, train/loss: 0.0
Step: 30810, train/grad_norm: 1.2626365231138692e-10
Step: 30810, train/learning_rate: 1.3338886674318928e-05
Step: 30810, train/epoch: 7.332222938537598
Step: 30820, train/loss: 0.0
Step: 30820, train/grad_norm: 1.3261553988286323e-07
Step: 30820, train/learning_rate: 1.3326986845640931e-05
Step: 30820, train/epoch: 7.334602355957031
Step: 30830, train/loss: 0.0
Step: 30830, train/grad_norm: 1.3874386362022761e-10
Step: 30830, train/learning_rate: 1.3315087926457636e-05
Step: 30830, train/epoch: 7.336982250213623
Step: 30840, train/loss: 0.0
Step: 30840, train/grad_norm: 1.691007156523483e-08
Step: 30840, train/learning_rate: 1.330318900727434e-05
Step: 30840, train/epoch: 7.339362144470215
Step: 30850, train/loss: 0.0
Step: 30850, train/grad_norm: 1.805399847398803e-07
Step: 30850, train/learning_rate: 1.3291290088091046e-05
Step: 30850, train/epoch: 7.341742038726807
Step: 30860, train/loss: 0.0
Step: 30860, train/grad_norm: 1.060770848937409e-08
Step: 30860, train/learning_rate: 1.327939116890775e-05
Step: 30860, train/epoch: 7.344121932983398
Step: 30870, train/loss: 0.0
Step: 30870, train/grad_norm: 5.775695512966195e-07
Step: 30870, train/learning_rate: 1.3267491340229753e-05
Step: 30870, train/epoch: 7.34650182723999
Step: 30880, train/loss: 0.0
Step: 30880, train/grad_norm: 3.8611625008400097e-10
Step: 30880, train/learning_rate: 1.3255592421046458e-05
Step: 30880, train/epoch: 7.348881721496582
Step: 30890, train/loss: 0.0
Step: 30890, train/grad_norm: 4.618892379681938e-09
Step: 30890, train/learning_rate: 1.3243693501863163e-05
Step: 30890, train/epoch: 7.351261138916016
Step: 30900, train/loss: 0.0
Step: 30900, train/grad_norm: 6.306653532739404e-10
Step: 30900, train/learning_rate: 1.3231794582679868e-05
Step: 30900, train/epoch: 7.353641033172607
Step: 30910, train/loss: 0.0
Step: 30910, train/grad_norm: 2.4301847201257942e-09
Step: 30910, train/learning_rate: 1.3219895663496573e-05
Step: 30910, train/epoch: 7.356020927429199
Step: 30920, train/loss: 0.0
Step: 30920, train/grad_norm: 1.599470851942897e-05
Step: 30920, train/learning_rate: 1.3207995834818576e-05
Step: 30920, train/epoch: 7.358400821685791
Step: 30930, train/loss: 0.0
Step: 30930, train/grad_norm: 2.9879176910441174e-09
Step: 30930, train/learning_rate: 1.319609691563528e-05
Step: 30930, train/epoch: 7.360780715942383
Step: 30940, train/loss: 0.0
Step: 30940, train/grad_norm: 1.6049588325017794e-08
Step: 30940, train/learning_rate: 1.3184197996451985e-05
Step: 30940, train/epoch: 7.363160610198975
Step: 30950, train/loss: 0.0
Step: 30950, train/grad_norm: 7.228424969696334e-09
Step: 30950, train/learning_rate: 1.317229907726869e-05
Step: 30950, train/epoch: 7.365540027618408
Step: 30960, train/loss: 0.0
Step: 30960, train/grad_norm: 4.77023531786358e-09
Step: 30960, train/learning_rate: 1.3160400158085395e-05
Step: 30960, train/epoch: 7.367919921875
Step: 30970, train/loss: 0.0
Step: 30970, train/grad_norm: 5.965253846440532e-10
Step: 30970, train/learning_rate: 1.3148500329407398e-05
Step: 30970, train/epoch: 7.370299816131592
Step: 30980, train/loss: 0.0
Step: 30980, train/grad_norm: 1.2385594772013064e-09
Step: 30980, train/learning_rate: 1.3136601410224102e-05
Step: 30980, train/epoch: 7.372679710388184
Step: 30990, train/loss: 0.0
Step: 30990, train/grad_norm: 1.8077925267867556e-10
Step: 30990, train/learning_rate: 1.3124702491040807e-05
Step: 30990, train/epoch: 7.375059604644775
Step: 31000, train/loss: 0.0
Step: 31000, train/grad_norm: 1.2798108128819763e-09
Step: 31000, train/learning_rate: 1.3112803571857512e-05
Step: 31000, train/epoch: 7.377439498901367
Step: 31010, train/loss: 0.0
Step: 31010, train/grad_norm: 2.1832620689110627e-07
Step: 31010, train/learning_rate: 1.3100904652674217e-05
Step: 31010, train/epoch: 7.379818916320801
Step: 31020, train/loss: 0.0
Step: 31020, train/grad_norm: 2.17197060337071e-09
Step: 31020, train/learning_rate: 1.308900482399622e-05
Step: 31020, train/epoch: 7.382198810577393
Step: 31030, train/loss: 0.0
Step: 31030, train/grad_norm: 1.123403762903763e-05
Step: 31030, train/learning_rate: 1.3077105904812925e-05
Step: 31030, train/epoch: 7.384578704833984
Step: 31040, train/loss: 0.0
Step: 31040, train/grad_norm: 1.8128590454580262e-05
Step: 31040, train/learning_rate: 1.306520698562963e-05
Step: 31040, train/epoch: 7.386958599090576
Step: 31050, train/loss: 0.0
Step: 31050, train/grad_norm: 6.098326732484338e-09
Step: 31050, train/learning_rate: 1.3053308066446334e-05
Step: 31050, train/epoch: 7.389338493347168
Step: 31060, train/loss: 0.0
Step: 31060, train/grad_norm: 9.67893853953683e-09
Step: 31060, train/learning_rate: 1.3041409147263039e-05
Step: 31060, train/epoch: 7.39171838760376
Step: 31070, train/loss: 0.0
Step: 31070, train/grad_norm: 9.233941389297229e-10
Step: 31070, train/learning_rate: 1.3029509318585042e-05
Step: 31070, train/epoch: 7.394098281860352
Step: 31080, train/loss: 0.0
Step: 31080, train/grad_norm: 7.644245236804181e-09
Step: 31080, train/learning_rate: 1.3017610399401747e-05
Step: 31080, train/epoch: 7.396477699279785
Step: 31090, train/loss: 0.0
Step: 31090, train/grad_norm: 4.49122353529674e-07
Step: 31090, train/learning_rate: 1.3005711480218451e-05
Step: 31090, train/epoch: 7.398857593536377
Step: 31100, train/loss: 0.0
Step: 31100, train/grad_norm: 1.029647131445266e-10
Step: 31100, train/learning_rate: 1.2993812561035156e-05
Step: 31100, train/epoch: 7.401237487792969
Step: 31110, train/loss: 0.0
Step: 31110, train/grad_norm: 2.9604535489724526e-10
Step: 31110, train/learning_rate: 1.2981913641851861e-05
Step: 31110, train/epoch: 7.4036173820495605
Step: 31120, train/loss: 0.0
Step: 31120, train/grad_norm: 3.7125787457625847e-06
Step: 31120, train/learning_rate: 1.2970014722668566e-05
Step: 31120, train/epoch: 7.405997276306152
Step: 31130, train/loss: 0.0
Step: 31130, train/grad_norm: 7.82560061196591e-09
Step: 31130, train/learning_rate: 1.2958114893990569e-05
Step: 31130, train/epoch: 7.408377170562744
Step: 31140, train/loss: 0.0
Step: 31140, train/grad_norm: 3.2837871088986503e-08
Step: 31140, train/learning_rate: 1.2946215974807274e-05
Step: 31140, train/epoch: 7.410756587982178
Step: 31150, train/loss: 0.0
Step: 31150, train/grad_norm: 2.830805367892708e-09
Step: 31150, train/learning_rate: 1.2934317055623978e-05
Step: 31150, train/epoch: 7.4131364822387695
Step: 31160, train/loss: 0.0
Step: 31160, train/grad_norm: 5.6400741854645275e-09
Step: 31160, train/learning_rate: 1.2922418136440683e-05
Step: 31160, train/epoch: 7.415516376495361
Step: 31170, train/loss: 0.0
Step: 31170, train/grad_norm: 1.7946808483770837e-08
Step: 31170, train/learning_rate: 1.2910519217257388e-05
Step: 31170, train/epoch: 7.417896270751953
Step: 31180, train/loss: 0.0
Step: 31180, train/grad_norm: 3.041645824097827e-10
Step: 31180, train/learning_rate: 1.2898619388579391e-05
Step: 31180, train/epoch: 7.420276165008545
Step: 31190, train/loss: 0.0
Step: 31190, train/grad_norm: 3.1620431855117204e-06
Step: 31190, train/learning_rate: 1.2886720469396096e-05
Step: 31190, train/epoch: 7.422656059265137
Step: 31200, train/loss: 0.0
Step: 31200, train/grad_norm: 1.5359639116141466e-09
Step: 31200, train/learning_rate: 1.28748215502128e-05
Step: 31200, train/epoch: 7.42503547668457
Step: 31210, train/loss: 0.0
Step: 31210, train/grad_norm: 6.174818878434962e-09
Step: 31210, train/learning_rate: 1.2862922631029505e-05
Step: 31210, train/epoch: 7.427415370941162
Step: 31220, train/loss: 0.0
Step: 31220, train/grad_norm: 2.4241650908862766e-09
Step: 31220, train/learning_rate: 1.285102371184621e-05
Step: 31220, train/epoch: 7.429795265197754
Step: 31230, train/loss: 0.0
Step: 31230, train/grad_norm: 1.7011682018619467e-07
Step: 31230, train/learning_rate: 1.2839123883168213e-05
Step: 31230, train/epoch: 7.432175159454346
Step: 31240, train/loss: 0.0
Step: 31240, train/grad_norm: 8.641320891911164e-05
Step: 31240, train/learning_rate: 1.2827224963984918e-05
Step: 31240, train/epoch: 7.4345550537109375
Step: 31250, train/loss: 0.0
Step: 31250, train/grad_norm: 9.319093940973744e-09
Step: 31250, train/learning_rate: 1.2815326044801623e-05
Step: 31250, train/epoch: 7.436934947967529
Step: 31260, train/loss: 0.0
Step: 31260, train/grad_norm: 2.4314374513778603e-06
Step: 31260, train/learning_rate: 1.2803427125618327e-05
Step: 31260, train/epoch: 7.439314842224121
Step: 31270, train/loss: 0.0
Step: 31270, train/grad_norm: 5.434407739812741e-06
Step: 31270, train/learning_rate: 1.2791528206435032e-05
Step: 31270, train/epoch: 7.441694259643555
Step: 31280, train/loss: 0.0
Step: 31280, train/grad_norm: 1.8859747541455363e-08
Step: 31280, train/learning_rate: 1.2779628377757035e-05
Step: 31280, train/epoch: 7.4440741539001465
Step: 31290, train/loss: 0.0
Step: 31290, train/grad_norm: 4.812235587792202e-08
Step: 31290, train/learning_rate: 1.276772945857374e-05
Step: 31290, train/epoch: 7.446454048156738
Step: 31300, train/loss: 0.0
Step: 31300, train/grad_norm: 2.221534032287309e-07
Step: 31300, train/learning_rate: 1.2755830539390445e-05
Step: 31300, train/epoch: 7.44883394241333
Step: 31310, train/loss: 0.0
Step: 31310, train/grad_norm: 1.9901897907459443e-09
Step: 31310, train/learning_rate: 1.274393162020715e-05
Step: 31310, train/epoch: 7.451213836669922
Step: 31320, train/loss: 0.0
Step: 31320, train/grad_norm: 5.043077067057311e-10
Step: 31320, train/learning_rate: 1.2732032701023854e-05
Step: 31320, train/epoch: 7.453593730926514
Step: 31330, train/loss: 0.0
Step: 31330, train/grad_norm: 7.656318246063165e-09
Step: 31330, train/learning_rate: 1.2720132872345857e-05
Step: 31330, train/epoch: 7.455973148345947
Step: 31340, train/loss: 0.0
Step: 31340, train/grad_norm: 9.629600583593856e-08
Step: 31340, train/learning_rate: 1.2708233953162562e-05
Step: 31340, train/epoch: 7.458353042602539
Step: 31350, train/loss: 0.0
Step: 31350, train/grad_norm: 5.6727698094505286e-09
Step: 31350, train/learning_rate: 1.2696335033979267e-05
Step: 31350, train/epoch: 7.460732936859131
Step: 31360, train/loss: 0.0
Step: 31360, train/grad_norm: 9.04683272739959e-10
Step: 31360, train/learning_rate: 1.2684436114795972e-05
Step: 31360, train/epoch: 7.463112831115723
Step: 31370, train/loss: 0.0
Step: 31370, train/grad_norm: 5.733324087486835e-06
Step: 31370, train/learning_rate: 1.2672537195612676e-05
Step: 31370, train/epoch: 7.4654927253723145
Step: 31380, train/loss: 0.0
Step: 31380, train/grad_norm: 8.565051445863503e-10
Step: 31380, train/learning_rate: 1.266063736693468e-05
Step: 31380, train/epoch: 7.467872619628906
Step: 31390, train/loss: 0.0
Step: 31390, train/grad_norm: 1.0348511630953183e-10
Step: 31390, train/learning_rate: 1.2648738447751384e-05
Step: 31390, train/epoch: 7.47025203704834
Step: 31400, train/loss: 0.0
Step: 31400, train/grad_norm: 6.258887852439443e-10
Step: 31400, train/learning_rate: 1.2636839528568089e-05
Step: 31400, train/epoch: 7.472631931304932
Step: 31410, train/loss: 0.0
Step: 31410, train/grad_norm: 1.268528393438828e-07
Step: 31410, train/learning_rate: 1.2624940609384794e-05
Step: 31410, train/epoch: 7.475011825561523
Step: 31420, train/loss: 0.0
Step: 31420, train/grad_norm: 2.4251406216535543e-08
Step: 31420, train/learning_rate: 1.2613041690201499e-05
Step: 31420, train/epoch: 7.477391719818115
Step: 31430, train/loss: 0.0
Step: 31430, train/grad_norm: 6.347373321391458e-11
Step: 31430, train/learning_rate: 1.2601141861523502e-05
Step: 31430, train/epoch: 7.479771614074707
Step: 31440, train/loss: 0.0
Step: 31440, train/grad_norm: 3.174054441501539e-08
Step: 31440, train/learning_rate: 1.2589242942340206e-05
Step: 31440, train/epoch: 7.482151508331299
Step: 31450, train/loss: 0.0
Step: 31450, train/grad_norm: 4.6107004325612877e-10
Step: 31450, train/learning_rate: 1.2577344023156911e-05
Step: 31450, train/epoch: 7.484531402587891
Step: 31460, train/loss: 0.0
Step: 31460, train/grad_norm: 2.772191365352228e-09
Step: 31460, train/learning_rate: 1.2565445103973616e-05
Step: 31460, train/epoch: 7.486910820007324
Step: 31470, train/loss: 0.02419999986886978
Step: 31470, train/grad_norm: 4.532951347613334e-09
Step: 31470, train/learning_rate: 1.255354618479032e-05
Step: 31470, train/epoch: 7.489290714263916
Step: 31480, train/loss: 0.0
Step: 31480, train/grad_norm: 8.692012387623294e-12
Step: 31480, train/learning_rate: 1.2541647265607025e-05
Step: 31480, train/epoch: 7.491670608520508
Step: 31490, train/loss: 0.0
Step: 31490, train/grad_norm: 6.749503733693984e-11
Step: 31490, train/learning_rate: 1.2529747436929028e-05
Step: 31490, train/epoch: 7.4940505027771
Step: 31500, train/loss: 0.0
Step: 31500, train/grad_norm: 3.883199983789609e-09
Step: 31500, train/learning_rate: 1.2517848517745733e-05
Step: 31500, train/epoch: 7.496430397033691
Step: 31510, train/loss: 0.0
Step: 31510, train/grad_norm: 8.18558465454089e-09
Step: 31510, train/learning_rate: 1.2505949598562438e-05
Step: 31510, train/epoch: 7.498810291290283
Step: 31520, train/loss: 0.0
Step: 31520, train/grad_norm: 1.17746506989036e-11
Step: 31520, train/learning_rate: 1.2494050679379143e-05
Step: 31520, train/epoch: 7.501189708709717
Step: 31530, train/loss: 0.0
Step: 31530, train/grad_norm: 8.882209634464289e-09
Step: 31530, train/learning_rate: 1.2482151760195848e-05
Step: 31530, train/epoch: 7.503569602966309
Step: 31540, train/loss: 0.0
Step: 31540, train/grad_norm: 7.05683428359194e-11
Step: 31540, train/learning_rate: 1.247025193151785e-05
Step: 31540, train/epoch: 7.5059494972229
Step: 31550, train/loss: 0.0
Step: 31550, train/grad_norm: 5.14422726638486e-10
Step: 31550, train/learning_rate: 1.2458353012334555e-05
Step: 31550, train/epoch: 7.508329391479492
Step: 31560, train/loss: 0.0
Step: 31560, train/grad_norm: 5.561343607851654e-10
Step: 31560, train/learning_rate: 1.244645409315126e-05
Step: 31560, train/epoch: 7.510709285736084
Step: 31570, train/loss: 0.0
Step: 31570, train/grad_norm: 1.3065214687912885e-08
Step: 31570, train/learning_rate: 1.2434555173967965e-05
Step: 31570, train/epoch: 7.513089179992676
Step: 31580, train/loss: 0.0
Step: 31580, train/grad_norm: 3.442015383825492e-08
Step: 31580, train/learning_rate: 1.242265625478467e-05
Step: 31580, train/epoch: 7.515468597412109
Step: 31590, train/loss: 0.0
Step: 31590, train/grad_norm: 3.4476986456866143e-06
Step: 31590, train/learning_rate: 1.2410756426106673e-05
Step: 31590, train/epoch: 7.517848491668701
Step: 31600, train/loss: 0.0
Step: 31600, train/grad_norm: 1.2921009151511953e-08
Step: 31600, train/learning_rate: 1.2398857506923378e-05
Step: 31600, train/epoch: 7.520228385925293
Step: 31610, train/loss: 0.1664000004529953
Step: 31610, train/grad_norm: 1.0908171361734276e-06
Step: 31610, train/learning_rate: 1.2386958587740082e-05
Step: 31610, train/epoch: 7.522608280181885
Step: 31620, train/loss: 0.0
Step: 31620, train/grad_norm: 8.400116371376498e-07
Step: 31620, train/learning_rate: 1.2375059668556787e-05
Step: 31620, train/epoch: 7.524988174438477
Step: 31630, train/loss: 0.0
Step: 31630, train/grad_norm: 6.764981208107201e-06
Step: 31630, train/learning_rate: 1.2363160749373492e-05
Step: 31630, train/epoch: 7.527368068695068
Step: 31640, train/loss: 0.0
Step: 31640, train/grad_norm: 1.0932133420737955e-07
Step: 31640, train/learning_rate: 1.2351260920695495e-05
Step: 31640, train/epoch: 7.52974796295166
Step: 31650, train/loss: 0.0
Step: 31650, train/grad_norm: 5.359804049476224e-07
Step: 31650, train/learning_rate: 1.23393620015122e-05
Step: 31650, train/epoch: 7.532127380371094
Step: 31660, train/loss: 0.0
Step: 31660, train/grad_norm: 2.8423156095414015e-07
Step: 31660, train/learning_rate: 1.2327463082328904e-05
Step: 31660, train/epoch: 7.5345072746276855
Step: 31670, train/loss: 0.0
Step: 31670, train/grad_norm: 4.5836674189558835e-07
Step: 31670, train/learning_rate: 1.231556416314561e-05
Step: 31670, train/epoch: 7.536887168884277
Step: 31680, train/loss: 0.0
Step: 31680, train/grad_norm: 7.263194135020967e-08
Step: 31680, train/learning_rate: 1.2303665243962314e-05
Step: 31680, train/epoch: 7.539267063140869
Step: 31690, train/loss: 0.0
Step: 31690, train/grad_norm: 4.106322037955579e-08
Step: 31690, train/learning_rate: 1.2291765415284317e-05
Step: 31690, train/epoch: 7.541646957397461
Step: 31700, train/loss: 0.0
Step: 31700, train/grad_norm: 1.496155306313085e-07
Step: 31700, train/learning_rate: 1.2279866496101022e-05
Step: 31700, train/epoch: 7.544026851654053
Step: 31710, train/loss: 0.0
Step: 31710, train/grad_norm: 1.21015361287391e-07
Step: 31710, train/learning_rate: 1.2267967576917727e-05
Step: 31710, train/epoch: 7.546406269073486
Step: 31720, train/loss: 0.0
Step: 31720, train/grad_norm: 2.880681222450221e-07
Step: 31720, train/learning_rate: 1.2256068657734431e-05
Step: 31720, train/epoch: 7.548786163330078
Step: 31730, train/loss: 0.0
Step: 31730, train/grad_norm: 2.7200639962643436e-08
Step: 31730, train/learning_rate: 1.2244169738551136e-05
Step: 31730, train/epoch: 7.55116605758667
Step: 31740, train/loss: 0.0
Step: 31740, train/grad_norm: 3.981923327955883e-06
Step: 31740, train/learning_rate: 1.2232269909873139e-05
Step: 31740, train/epoch: 7.553545951843262
Step: 31750, train/loss: 0.0
Step: 31750, train/grad_norm: 1.1523598963947279e-08
Step: 31750, train/learning_rate: 1.2220370990689844e-05
Step: 31750, train/epoch: 7.5559258460998535
Step: 31760, train/loss: 0.0
Step: 31760, train/grad_norm: 2.0649745025025368e-08
Step: 31760, train/learning_rate: 1.2208472071506549e-05
Step: 31760, train/epoch: 7.558305740356445
Step: 31770, train/loss: 0.0
Step: 31770, train/grad_norm: 4.412053385749459e-06
Step: 31770, train/learning_rate: 1.2196573152323253e-05
Step: 31770, train/epoch: 7.560685157775879
Step: 31780, train/loss: 0.0
Step: 31780, train/grad_norm: 3.2335332633692815e-08
Step: 31780, train/learning_rate: 1.2184674233139958e-05
Step: 31780, train/epoch: 7.563065052032471
Step: 31790, train/loss: 0.0
Step: 31790, train/grad_norm: 1.089637771656271e-05
Step: 31790, train/learning_rate: 1.2172775313956663e-05
Step: 31790, train/epoch: 7.5654449462890625
Step: 31800, train/loss: 0.0
Step: 31800, train/grad_norm: 7.726355732984302e-08
Step: 31800, train/learning_rate: 1.2160875485278666e-05
Step: 31800, train/epoch: 7.567824840545654
Step: 31810, train/loss: 0.0
Step: 31810, train/grad_norm: 4.153395423145412e-07
Step: 31810, train/learning_rate: 1.214897656609537e-05
Step: 31810, train/epoch: 7.570204734802246
Step: 31820, train/loss: 0.0
Step: 31820, train/grad_norm: 3.3264196730442563e-08
Step: 31820, train/learning_rate: 1.2137077646912076e-05
Step: 31820, train/epoch: 7.572584629058838
Step: 31830, train/loss: 0.0
Step: 31830, train/grad_norm: 7.85605425335234e-06
Step: 31830, train/learning_rate: 1.212517872772878e-05
Step: 31830, train/epoch: 7.57496452331543
Step: 31840, train/loss: 0.0
Step: 31840, train/grad_norm: 1.1414844891532994e-07
Step: 31840, train/learning_rate: 1.2113279808545485e-05
Step: 31840, train/epoch: 7.577343940734863
Step: 31850, train/loss: 0.0
Step: 31850, train/grad_norm: 4.736222525281164e-10
Step: 31850, train/learning_rate: 1.2101379979867488e-05
Step: 31850, train/epoch: 7.579723834991455
Step: 31860, train/loss: 0.0
Step: 31860, train/grad_norm: 0.0014992185169830918
Step: 31860, train/learning_rate: 1.2089481060684193e-05
Step: 31860, train/epoch: 7.582103729248047
Step: 31870, train/loss: 0.0
Step: 31870, train/grad_norm: 3.967700104112737e-05
Step: 31870, train/learning_rate: 1.2077582141500898e-05
Step: 31870, train/epoch: 7.584483623504639
Step: 31880, train/loss: 0.0
Step: 31880, train/grad_norm: 9.205288051816751e-07
Step: 31880, train/learning_rate: 1.2065683222317602e-05
Step: 31880, train/epoch: 7.5868635177612305
Step: 31890, train/loss: 0.0
Step: 31890, train/grad_norm: 5.959358517770852e-08
Step: 31890, train/learning_rate: 1.2053784303134307e-05
Step: 31890, train/epoch: 7.589243412017822
Step: 31900, train/loss: 0.0
Step: 31900, train/grad_norm: 9.237720405508298e-06
Step: 31900, train/learning_rate: 1.204188447445631e-05
Step: 31900, train/epoch: 7.591622829437256
Step: 31910, train/loss: 0.0
Step: 31910, train/grad_norm: 3.3829161338871927e-07
Step: 31910, train/learning_rate: 1.2029985555273015e-05
Step: 31910, train/epoch: 7.594002723693848
Step: 31920, train/loss: 0.0
Step: 31920, train/grad_norm: 5.970462457760561e-10
Step: 31920, train/learning_rate: 1.201808663608972e-05
Step: 31920, train/epoch: 7.5963826179504395
Step: 31930, train/loss: 0.0
Step: 31930, train/grad_norm: 1.9330755052227033e-10
Step: 31930, train/learning_rate: 1.2006187716906425e-05
Step: 31930, train/epoch: 7.598762512207031
Step: 31940, train/loss: 0.0
Step: 31940, train/grad_norm: 4.542771876003826e-06
Step: 31940, train/learning_rate: 1.199428879772313e-05
Step: 31940, train/epoch: 7.601142406463623
Step: 31950, train/loss: 0.0
Step: 31950, train/grad_norm: 1.0670611061414093e-07
Step: 31950, train/learning_rate: 1.1982388969045132e-05
Step: 31950, train/epoch: 7.603522300720215
Step: 31960, train/loss: 0.0
Step: 31960, train/grad_norm: 6.86257180859684e-06
Step: 31960, train/learning_rate: 1.1970490049861837e-05
Step: 31960, train/epoch: 7.605901718139648
Step: 31970, train/loss: 0.0
Step: 31970, train/grad_norm: 6.14024315837014e-08
Step: 31970, train/learning_rate: 1.1958591130678542e-05
Step: 31970, train/epoch: 7.60828161239624
Step: 31980, train/loss: 0.0
Step: 31980, train/grad_norm: 2.981288105274871e-07
Step: 31980, train/learning_rate: 1.1946692211495247e-05
Step: 31980, train/epoch: 7.610661506652832
Step: 31990, train/loss: 0.0
Step: 31990, train/grad_norm: 8.582349983043969e-05
Step: 31990, train/learning_rate: 1.1934793292311952e-05
Step: 31990, train/epoch: 7.613041400909424
Step: 32000, train/loss: 0.0
Step: 32000, train/grad_norm: 9.387174486619188e-07
Step: 32000, train/learning_rate: 1.1922893463633955e-05
Step: 32000, train/epoch: 7.615421295166016
Step: 32010, train/loss: 0.0
Step: 32010, train/grad_norm: 3.295578062534332e-05
Step: 32010, train/learning_rate: 1.191099454445066e-05
Step: 32010, train/epoch: 7.617801189422607
Step: 32020, train/loss: 0.0
Step: 32020, train/grad_norm: 5.015485271542275e-07
Step: 32020, train/learning_rate: 1.1899095625267364e-05
Step: 32020, train/epoch: 7.620181083679199
Step: 32030, train/loss: 0.0
Step: 32030, train/grad_norm: 4.028961484436877e-06
Step: 32030, train/learning_rate: 1.1887196706084069e-05
Step: 32030, train/epoch: 7.622560501098633
Step: 32040, train/loss: 0.0
Step: 32040, train/grad_norm: 3.590696451283293e-07
Step: 32040, train/learning_rate: 1.1875297786900774e-05
Step: 32040, train/epoch: 7.624940395355225
Step: 32050, train/loss: 0.0
Step: 32050, train/grad_norm: 2.804548593360323e-08
Step: 32050, train/learning_rate: 1.1863397958222777e-05
Step: 32050, train/epoch: 7.627320289611816
Step: 32060, train/loss: 0.0
Step: 32060, train/grad_norm: 6.833270305151018e-08
Step: 32060, train/learning_rate: 1.1851499039039481e-05
Step: 32060, train/epoch: 7.629700183868408
Step: 32070, train/loss: 0.0
Step: 32070, train/grad_norm: 8.172036558562468e-08
Step: 32070, train/learning_rate: 1.1839600119856186e-05
Step: 32070, train/epoch: 7.632080078125
Step: 32080, train/loss: 9.999999747378752e-05
Step: 32080, train/grad_norm: 2.1996743271301966e-07
Step: 32080, train/learning_rate: 1.1827701200672891e-05
Step: 32080, train/epoch: 7.634459972381592
Step: 32090, train/loss: 0.0
Step: 32090, train/grad_norm: 1.1467012756005701e-10
Step: 32090, train/learning_rate: 1.1815802281489596e-05
Step: 32090, train/epoch: 7.636839389801025
Step: 32100, train/loss: 0.0
Step: 32100, train/grad_norm: 1.2999951809478105e-11
Step: 32100, train/learning_rate: 1.1803902452811599e-05
Step: 32100, train/epoch: 7.639219284057617
Step: 32110, train/loss: 0.0
Step: 32110, train/grad_norm: 2.81403522706114e-09
Step: 32110, train/learning_rate: 1.1792003533628304e-05
Step: 32110, train/epoch: 7.641599178314209
Step: 32120, train/loss: 0.0
Step: 32120, train/grad_norm: 3.0959070045355475e-06
Step: 32120, train/learning_rate: 1.1780104614445008e-05
Step: 32120, train/epoch: 7.643979072570801
Step: 32130, train/loss: 0.0
Step: 32130, train/grad_norm: 1.1071774608240048e-09
Step: 32130, train/learning_rate: 1.1768205695261713e-05
Step: 32130, train/epoch: 7.646358966827393
Step: 32140, train/loss: 0.0
Step: 32140, train/grad_norm: 2.2435139857002184e-10
Step: 32140, train/learning_rate: 1.1756306776078418e-05
Step: 32140, train/epoch: 7.648738861083984
Step: 32150, train/loss: 0.0
Step: 32150, train/grad_norm: 2.14566853173892e-09
Step: 32150, train/learning_rate: 1.1744407856895123e-05
Step: 32150, train/epoch: 7.651118278503418
Step: 32160, train/loss: 0.0
Step: 32160, train/grad_norm: 1.6656442891616052e-10
Step: 32160, train/learning_rate: 1.1732508028217126e-05
Step: 32160, train/epoch: 7.65349817276001
Step: 32170, train/loss: 0.05510000139474869
Step: 32170, train/grad_norm: 3.4205496074468655e-11
Step: 32170, train/learning_rate: 1.172060910903383e-05
Step: 32170, train/epoch: 7.655878067016602
Step: 32180, train/loss: 0.0
Step: 32180, train/grad_norm: 5.290725635376248e-10
Step: 32180, train/learning_rate: 1.1708710189850535e-05
Step: 32180, train/epoch: 7.658257961273193
Step: 32190, train/loss: 0.0
Step: 32190, train/grad_norm: 5.263370295160996e-10
Step: 32190, train/learning_rate: 1.169681127066724e-05
Step: 32190, train/epoch: 7.660637855529785
Step: 32200, train/loss: 0.0
Step: 32200, train/grad_norm: 5.62918989199801e-10
Step: 32200, train/learning_rate: 1.1684912351483945e-05
Step: 32200, train/epoch: 7.663017749786377
Step: 32210, train/loss: 0.0
Step: 32210, train/grad_norm: 2.7169988481290375e-10
Step: 32210, train/learning_rate: 1.1673012522805948e-05
Step: 32210, train/epoch: 7.665397644042969
Step: 32220, train/loss: 0.0
Step: 32220, train/grad_norm: 1.448485359434315e-10
Step: 32220, train/learning_rate: 1.1661113603622653e-05
Step: 32220, train/epoch: 7.667777061462402
Step: 32230, train/loss: 0.0
Step: 32230, train/grad_norm: 2.251328984348433e-10
Step: 32230, train/learning_rate: 1.1649214684439357e-05
Step: 32230, train/epoch: 7.670156955718994
Step: 32240, train/loss: 0.0
Step: 32240, train/grad_norm: 2.099565410418336e-09
Step: 32240, train/learning_rate: 1.1637315765256062e-05
Step: 32240, train/epoch: 7.672536849975586
Step: 32250, train/loss: 0.0
Step: 32250, train/grad_norm: 5.70722917814237e-08
Step: 32250, train/learning_rate: 1.1625416846072767e-05
Step: 32250, train/epoch: 7.674916744232178
Step: 32260, train/loss: 0.0
Step: 32260, train/grad_norm: 5.317665454640519e-07
Step: 32260, train/learning_rate: 1.161351701739477e-05
Step: 32260, train/epoch: 7.6772966384887695
Step: 32270, train/loss: 0.0
Step: 32270, train/grad_norm: 1.1040657103833507e-11
Step: 32270, train/learning_rate: 1.1601618098211475e-05
Step: 32270, train/epoch: 7.679676532745361
Step: 32280, train/loss: 0.0
Step: 32280, train/grad_norm: 2.0162327363237864e-10
Step: 32280, train/learning_rate: 1.158971917902818e-05
Step: 32280, train/epoch: 7.682055950164795
Step: 32290, train/loss: 0.0
Step: 32290, train/grad_norm: 4.723054644273361e-06
Step: 32290, train/learning_rate: 1.1577820259844884e-05
Step: 32290, train/epoch: 7.684435844421387
Step: 32300, train/loss: 0.0
Step: 32300, train/grad_norm: 1.3186461922387593e-09
Step: 32300, train/learning_rate: 1.1565921340661589e-05
Step: 32300, train/epoch: 7.6868157386779785
Step: 32310, train/loss: 0.0
Step: 32310, train/grad_norm: 2.6406784314758625e-08
Step: 32310, train/learning_rate: 1.1554021511983592e-05
Step: 32310, train/epoch: 7.68919563293457
Step: 32320, train/loss: 0.0
Step: 32320, train/grad_norm: 0.0005030362517572939
Step: 32320, train/learning_rate: 1.1542122592800297e-05
Step: 32320, train/epoch: 7.691575527191162
Step: 32330, train/loss: 0.0
Step: 32330, train/grad_norm: 1.7694218312769294e-10
Step: 32330, train/learning_rate: 1.1530223673617002e-05
Step: 32330, train/epoch: 7.693955421447754
Step: 32340, train/loss: 0.0
Step: 32340, train/grad_norm: 4.5667730708132126e-10
Step: 32340, train/learning_rate: 1.1518324754433706e-05
Step: 32340, train/epoch: 7.696335315704346
Step: 32350, train/loss: 0.0
Step: 32350, train/grad_norm: 4.0692219727134216e-07
Step: 32350, train/learning_rate: 1.1506425835250411e-05
Step: 32350, train/epoch: 7.698714733123779
Step: 32360, train/loss: 0.0
Step: 32360, train/grad_norm: 7.142269886628583e-09
Step: 32360, train/learning_rate: 1.1494526006572414e-05
Step: 32360, train/epoch: 7.701094627380371
Step: 32370, train/loss: 0.0
Step: 32370, train/grad_norm: 9.961861335838762e-10
Step: 32370, train/learning_rate: 1.1482627087389119e-05
Step: 32370, train/epoch: 7.703474521636963
Step: 32380, train/loss: 0.0
Step: 32380, train/grad_norm: 1.1409768063685988e-07
Step: 32380, train/learning_rate: 1.1470728168205824e-05
Step: 32380, train/epoch: 7.705854415893555
Step: 32390, train/loss: 0.0
Step: 32390, train/grad_norm: 1.1647800945979725e-08
Step: 32390, train/learning_rate: 1.1458829249022529e-05
Step: 32390, train/epoch: 7.7082343101501465
Step: 32400, train/loss: 0.0
Step: 32400, train/grad_norm: 3.0822671082120223e-09
Step: 32400, train/learning_rate: 1.1446930329839233e-05
Step: 32400, train/epoch: 7.710614204406738
Step: 32410, train/loss: 0.0
Step: 32410, train/grad_norm: 1.1661424537123821e-07
Step: 32410, train/learning_rate: 1.1435030501161236e-05
Step: 32410, train/epoch: 7.712993621826172
Step: 32420, train/loss: 0.0
Step: 32420, train/grad_norm: 8.085644154220972e-09
Step: 32420, train/learning_rate: 1.1423131581977941e-05
Step: 32420, train/epoch: 7.715373516082764
Step: 32430, train/loss: 0.0
Step: 32430, train/grad_norm: 4.161840871930167e-10
Step: 32430, train/learning_rate: 1.1411232662794646e-05
Step: 32430, train/epoch: 7.7177534103393555
Step: 32440, train/loss: 0.0
Step: 32440, train/grad_norm: 2.746571636791373e-09
Step: 32440, train/learning_rate: 1.139933374361135e-05
Step: 32440, train/epoch: 7.720133304595947
Step: 32450, train/loss: 0.0
Step: 32450, train/grad_norm: 1.7361627691059311e-09
Step: 32450, train/learning_rate: 1.1387434824428055e-05
Step: 32450, train/epoch: 7.722513198852539
Step: 32460, train/loss: 0.0
Step: 32460, train/grad_norm: 8.374508411179704e-07
Step: 32460, train/learning_rate: 1.137553590524476e-05
Step: 32460, train/epoch: 7.724893093109131
Step: 32470, train/loss: 0.0
Step: 32470, train/grad_norm: 9.334785744385954e-08
Step: 32470, train/learning_rate: 1.1363636076566763e-05
Step: 32470, train/epoch: 7.7272725105285645
Step: 32480, train/loss: 0.0
Step: 32480, train/grad_norm: 3.434836026716681e-10
Step: 32480, train/learning_rate: 1.1351737157383468e-05
Step: 32480, train/epoch: 7.729652404785156
Step: 32490, train/loss: 0.0
Step: 32490, train/grad_norm: 1.5782287976495724e-10
Step: 32490, train/learning_rate: 1.1339838238200173e-05
Step: 32490, train/epoch: 7.732032299041748
Step: 32500, train/loss: 0.0
Step: 32500, train/grad_norm: 3.638207090261858e-06
Step: 32500, train/learning_rate: 1.1327939319016878e-05
Step: 32500, train/epoch: 7.73441219329834
Step: 32510, train/loss: 0.0
Step: 32510, train/grad_norm: 8.646863534522709e-06
Step: 32510, train/learning_rate: 1.1316040399833582e-05
Step: 32510, train/epoch: 7.736792087554932
Step: 32520, train/loss: 0.0
Step: 32520, train/grad_norm: 4.810974587599048e-07
Step: 32520, train/learning_rate: 1.1304140571155585e-05
Step: 32520, train/epoch: 7.739171981811523
Step: 32530, train/loss: 0.0
Step: 32530, train/grad_norm: 1.804605354038813e-08
Step: 32530, train/learning_rate: 1.129224165197229e-05
Step: 32530, train/epoch: 7.741551876068115
Step: 32540, train/loss: 0.0
Step: 32540, train/grad_norm: 3.903495748858177e-09
Step: 32540, train/learning_rate: 1.1280342732788995e-05
Step: 32540, train/epoch: 7.743931293487549
Step: 32550, train/loss: 0.0
Step: 32550, train/grad_norm: 1.2598097565152955e-10
Step: 32550, train/learning_rate: 1.12684438136057e-05
Step: 32550, train/epoch: 7.746311187744141
Step: 32560, train/loss: 0.0
Step: 32560, train/grad_norm: 2.1961908680534492e-10
Step: 32560, train/learning_rate: 1.1256544894422404e-05
Step: 32560, train/epoch: 7.748691082000732
Step: 32570, train/loss: 0.0
Step: 32570, train/grad_norm: 2.41251213450866e-10
Step: 32570, train/learning_rate: 1.1244645065744407e-05
Step: 32570, train/epoch: 7.751070976257324
Step: 32580, train/loss: 0.0
Step: 32580, train/grad_norm: 1.0080533630052457e-10
Step: 32580, train/learning_rate: 1.1232746146561112e-05
Step: 32580, train/epoch: 7.753450870513916
Step: 32590, train/loss: 0.0
Step: 32590, train/grad_norm: 6.27538021547025e-09
Step: 32590, train/learning_rate: 1.1220847227377817e-05
Step: 32590, train/epoch: 7.755830764770508
Step: 32600, train/loss: 0.0
Step: 32600, train/grad_norm: 5.49688309092744e-07
Step: 32600, train/learning_rate: 1.1208948308194522e-05
Step: 32600, train/epoch: 7.758210182189941
Step: 32610, train/loss: 0.0
Step: 32610, train/grad_norm: 6.248260797647731e-10
Step: 32610, train/learning_rate: 1.1197049389011227e-05
Step: 32610, train/epoch: 7.760590076446533
Step: 32620, train/loss: 0.0
Step: 32620, train/grad_norm: 2.354318517916454e-09
Step: 32620, train/learning_rate: 1.118514956033323e-05
Step: 32620, train/epoch: 7.762969970703125
Step: 32630, train/loss: 0.0
Step: 32630, train/grad_norm: 1.39176881824854e-11
Step: 32630, train/learning_rate: 1.1173250641149934e-05
Step: 32630, train/epoch: 7.765349864959717
Step: 32640, train/loss: 0.0
Step: 32640, train/grad_norm: 1.5018366639196756e-07
Step: 32640, train/learning_rate: 1.116135172196664e-05
Step: 32640, train/epoch: 7.767729759216309
Step: 32650, train/loss: 0.0
Step: 32650, train/grad_norm: 7.160719661669646e-08
Step: 32650, train/learning_rate: 1.1149452802783344e-05
Step: 32650, train/epoch: 7.7701096534729
Step: 32660, train/loss: 0.0
Step: 32660, train/grad_norm: 1.0100349072672543e-06
Step: 32660, train/learning_rate: 1.1137553883600049e-05
Step: 32660, train/epoch: 7.772489070892334
Step: 32670, train/loss: 0.0
Step: 32670, train/grad_norm: 2.7152233239569057e-10
Step: 32670, train/learning_rate: 1.1125654054922052e-05
Step: 32670, train/epoch: 7.774868965148926
Step: 32680, train/loss: 0.0
Step: 32680, train/grad_norm: 3.3952033845707774e-05
Step: 32680, train/learning_rate: 1.1113755135738757e-05
Step: 32680, train/epoch: 7.777248859405518
Step: 32690, train/loss: 0.0
Step: 32690, train/grad_norm: 7.901665988185869e-10
Step: 32690, train/learning_rate: 1.1101856216555461e-05
Step: 32690, train/epoch: 7.779628753662109
Step: 32700, train/loss: 0.0
Step: 32700, train/grad_norm: 6.672758456893746e-10
Step: 32700, train/learning_rate: 1.1089957297372166e-05
Step: 32700, train/epoch: 7.782008647918701
Step: 32710, train/loss: 0.0
Step: 32710, train/grad_norm: 2.8759872955363974e-11
Step: 32710, train/learning_rate: 1.1078058378188871e-05
Step: 32710, train/epoch: 7.784388542175293
Step: 32720, train/loss: 0.0
Step: 32720, train/grad_norm: 4.260581665249674e-09
Step: 32720, train/learning_rate: 1.1066158549510874e-05
Step: 32720, train/epoch: 7.786768436431885
Step: 32730, train/loss: 0.0
Step: 32730, train/grad_norm: 7.720057371152222e-10
Step: 32730, train/learning_rate: 1.1054259630327579e-05
Step: 32730, train/epoch: 7.789147853851318
Step: 32740, train/loss: 0.0
Step: 32740, train/grad_norm: 2.4741166892994215e-09
Step: 32740, train/learning_rate: 1.1042360711144283e-05
Step: 32740, train/epoch: 7.79152774810791
Step: 32750, train/loss: 0.0
Step: 32750, train/grad_norm: 1.2717754627189493e-10
Step: 32750, train/learning_rate: 1.1030461791960988e-05
Step: 32750, train/epoch: 7.793907642364502
Step: 32760, train/loss: 0.0
Step: 32760, train/grad_norm: 1.4683662641346018e-07
Step: 32760, train/learning_rate: 1.1018562872777693e-05
Step: 32760, train/epoch: 7.796287536621094
Step: 32770, train/loss: 0.0
Step: 32770, train/grad_norm: 7.862725396989845e-06
Step: 32770, train/learning_rate: 1.1006663044099696e-05
Step: 32770, train/epoch: 7.7986674308776855
Step: 32780, train/loss: 0.0
Step: 32780, train/grad_norm: 4.574279500957346e-06
Step: 32780, train/learning_rate: 1.09947641249164e-05
Step: 32780, train/epoch: 7.801047325134277
Step: 32790, train/loss: 0.0
Step: 32790, train/grad_norm: 2.3308935226395988e-07
Step: 32790, train/learning_rate: 1.0982865205733106e-05
Step: 32790, train/epoch: 7.803426742553711
Step: 32800, train/loss: 0.0
Step: 32800, train/grad_norm: 5.697975868912408e-09
Step: 32800, train/learning_rate: 1.097096628654981e-05
Step: 32800, train/epoch: 7.805806636810303
Step: 32810, train/loss: 0.0
Step: 32810, train/grad_norm: 5.15936392275762e-07
Step: 32810, train/learning_rate: 1.0959067367366515e-05
Step: 32810, train/epoch: 7.8081865310668945
Step: 32820, train/loss: 0.0
Step: 32820, train/grad_norm: 1.6573329375546564e-08
Step: 32820, train/learning_rate: 1.094716844818322e-05
Step: 32820, train/epoch: 7.810566425323486
Step: 32830, train/loss: 0.0
Step: 32830, train/grad_norm: 1.1645173714214252e-09
Step: 32830, train/learning_rate: 1.0935268619505223e-05
Step: 32830, train/epoch: 7.812946319580078
Step: 32840, train/loss: 0.0
Step: 32840, train/grad_norm: 2.0487109964051342e-07
Step: 32840, train/learning_rate: 1.0923369700321928e-05
Step: 32840, train/epoch: 7.81532621383667
Step: 32850, train/loss: 0.0
Step: 32850, train/grad_norm: 5.8372222611069446e-08
Step: 32850, train/learning_rate: 1.0911470781138632e-05
Step: 32850, train/epoch: 7.8177056312561035
Step: 32860, train/loss: 0.0
Step: 32860, train/grad_norm: 1.089195400538756e-09
Step: 32860, train/learning_rate: 1.0899571861955337e-05
Step: 32860, train/epoch: 7.820085525512695
Step: 32870, train/loss: 0.0
Step: 32870, train/grad_norm: 4.500564543263863e-08
Step: 32870, train/learning_rate: 1.0887672942772042e-05
Step: 32870, train/epoch: 7.822465419769287
Step: 32880, train/loss: 0.0
Step: 32880, train/grad_norm: 6.536078900332143e-10
Step: 32880, train/learning_rate: 1.0875773114094045e-05
Step: 32880, train/epoch: 7.824845314025879
Step: 32890, train/loss: 0.0
Step: 32890, train/grad_norm: 1.748022706400329e-13
Step: 32890, train/learning_rate: 1.086387419491075e-05
Step: 32890, train/epoch: 7.827225208282471
Step: 32900, train/loss: 0.0
Step: 32900, train/grad_norm: 2.2506962960022747e-09
Step: 32900, train/learning_rate: 1.0851975275727455e-05
Step: 32900, train/epoch: 7.8296051025390625
Step: 32910, train/loss: 0.0
Step: 32910, train/grad_norm: 9.473286738392872e-11
Step: 32910, train/learning_rate: 1.084007635654416e-05
Step: 32910, train/epoch: 7.831984996795654
Step: 32920, train/loss: 0.0
Step: 32920, train/grad_norm: 4.777755293616792e-07
Step: 32920, train/learning_rate: 1.0828177437360864e-05
Step: 32920, train/epoch: 7.834364414215088
Step: 32930, train/loss: 0.0
Step: 32930, train/grad_norm: 4.3778175529141095e-10
Step: 32930, train/learning_rate: 1.0816277608682867e-05
Step: 32930, train/epoch: 7.83674430847168
Step: 32940, train/loss: 0.0
Step: 32940, train/grad_norm: 5.09014386196327e-09
Step: 32940, train/learning_rate: 1.0804378689499572e-05
Step: 32940, train/epoch: 7.8391242027282715
Step: 32950, train/loss: 0.0
Step: 32950, train/grad_norm: 3.06462681010089e-07
Step: 32950, train/learning_rate: 1.0792479770316277e-05
Step: 32950, train/epoch: 7.841504096984863
Step: 32960, train/loss: 0.0
Step: 32960, train/grad_norm: 1.029123897211548e-09
Step: 32960, train/learning_rate: 1.0780580851132981e-05
Step: 32960, train/epoch: 7.843883991241455
Step: 32970, train/loss: 0.0
Step: 32970, train/grad_norm: 2.569983337252779e-09
Step: 32970, train/learning_rate: 1.0768681931949686e-05
Step: 32970, train/epoch: 7.846263885498047
Step: 32980, train/loss: 0.0
Step: 32980, train/grad_norm: 3.017867129528895e-07
Step: 32980, train/learning_rate: 1.075678210327169e-05
Step: 32980, train/epoch: 7.8486433029174805
Step: 32990, train/loss: 0.0
Step: 32990, train/grad_norm: 8.754989266890334e-08
Step: 32990, train/learning_rate: 1.0744883184088394e-05
Step: 32990, train/epoch: 7.851023197174072
Step: 33000, train/loss: 0.0
Step: 33000, train/grad_norm: 1.7257561921724118e-05
Step: 33000, train/learning_rate: 1.0732984264905099e-05
Step: 33000, train/epoch: 7.853403091430664
Step: 33010, train/loss: 0.0
Step: 33010, train/grad_norm: 6.571643784702985e-10
Step: 33010, train/learning_rate: 1.0721085345721804e-05
Step: 33010, train/epoch: 7.855782985687256
Step: 33020, train/loss: 0.0
Step: 33020, train/grad_norm: 4.987304791370661e-09
Step: 33020, train/learning_rate: 1.0709186426538508e-05
Step: 33020, train/epoch: 7.858162879943848
Step: 33030, train/loss: 0.0
Step: 33030, train/grad_norm: 2.669126217824669e-07
Step: 33030, train/learning_rate: 1.0697286597860511e-05
Step: 33030, train/epoch: 7.8605427742004395
Step: 33040, train/loss: 0.0
Step: 33040, train/grad_norm: 6.197250268513699e-09
Step: 33040, train/learning_rate: 1.0685387678677216e-05
Step: 33040, train/epoch: 7.862922191619873
Step: 33050, train/loss: 0.0
Step: 33050, train/grad_norm: 6.876652136611483e-09
Step: 33050, train/learning_rate: 1.0673488759493921e-05
Step: 33050, train/epoch: 7.865302085876465
Step: 33060, train/loss: 0.0
Step: 33060, train/grad_norm: 2.530629572561338e-08
Step: 33060, train/learning_rate: 1.0661589840310626e-05
Step: 33060, train/epoch: 7.867681980133057
Step: 33070, train/loss: 0.0
Step: 33070, train/grad_norm: 1.4278914939325205e-08
Step: 33070, train/learning_rate: 1.064969092112733e-05
Step: 33070, train/epoch: 7.870061874389648
Step: 33080, train/loss: 0.0
Step: 33080, train/grad_norm: 1.6123017365643477e-09
Step: 33080, train/learning_rate: 1.0637791092449334e-05
Step: 33080, train/epoch: 7.87244176864624
Step: 33090, train/loss: 0.0
Step: 33090, train/grad_norm: 5.138068459586975e-08
Step: 33090, train/learning_rate: 1.0625892173266038e-05
Step: 33090, train/epoch: 7.874821662902832
Step: 33100, train/loss: 0.0
Step: 33100, train/grad_norm: 7.141124913623287e-10
Step: 33100, train/learning_rate: 1.0613993254082743e-05
Step: 33100, train/epoch: 7.877201557159424
Step: 33110, train/loss: 0.0
Step: 33110, train/grad_norm: 3.069711951297904e-08
Step: 33110, train/learning_rate: 1.0602094334899448e-05
Step: 33110, train/epoch: 7.879580974578857
Step: 33120, train/loss: 0.0
Step: 33120, train/grad_norm: 3.547695470729195e-10
Step: 33120, train/learning_rate: 1.0590195415716153e-05
Step: 33120, train/epoch: 7.881960868835449
Step: 33130, train/loss: 0.0
Step: 33130, train/grad_norm: 1.2899213919226327e-09
Step: 33130, train/learning_rate: 1.0578296496532857e-05
Step: 33130, train/epoch: 7.884340763092041
Step: 33140, train/loss: 0.0
Step: 33140, train/grad_norm: 9.79568142156495e-08
Step: 33140, train/learning_rate: 1.056639666785486e-05
Step: 33140, train/epoch: 7.886720657348633
Step: 33150, train/loss: 0.0
Step: 33150, train/grad_norm: 8.438818355216426e-08
Step: 33150, train/learning_rate: 1.0554497748671565e-05
Step: 33150, train/epoch: 7.889100551605225
Step: 33160, train/loss: 0.0
Step: 33160, train/grad_norm: 1.009173100641192e-08
Step: 33160, train/learning_rate: 1.054259882948827e-05
Step: 33160, train/epoch: 7.891480445861816
Step: 33170, train/loss: 0.0
Step: 33170, train/grad_norm: 1.7233765803315038e-10
Step: 33170, train/learning_rate: 1.0530699910304975e-05
Step: 33170, train/epoch: 7.89385986328125
Step: 33180, train/loss: 0.0
Step: 33180, train/grad_norm: 1.363787305308506e-07
Step: 33180, train/learning_rate: 1.051880099112168e-05
Step: 33180, train/epoch: 7.896239757537842
Step: 33190, train/loss: 0.0
Step: 33190, train/grad_norm: 2.013206490403263e-09
Step: 33190, train/learning_rate: 1.0506901162443683e-05
Step: 33190, train/epoch: 7.898619651794434
Step: 33200, train/loss: 0.0
Step: 33200, train/grad_norm: 7.455365266650915e-05
Step: 33200, train/learning_rate: 1.0495002243260387e-05
Step: 33200, train/epoch: 7.900999546051025
Step: 33210, train/loss: 0.0
Step: 33210, train/grad_norm: 5.367513153942127e-07
Step: 33210, train/learning_rate: 1.0483103324077092e-05
Step: 33210, train/epoch: 7.903379440307617
Step: 33220, train/loss: 0.0
Step: 33220, train/grad_norm: 7.758119480172354e-09
Step: 33220, train/learning_rate: 1.0471204404893797e-05
Step: 33220, train/epoch: 7.905759334564209
Step: 33230, train/loss: 0.0
Step: 33230, train/grad_norm: 5.198514190851711e-05
Step: 33230, train/learning_rate: 1.0459305485710502e-05
Step: 33230, train/epoch: 7.908138751983643
Step: 33240, train/loss: 0.0
Step: 33240, train/grad_norm: 7.619046726858869e-09
Step: 33240, train/learning_rate: 1.0447405657032505e-05
Step: 33240, train/epoch: 7.910518646240234
Step: 33250, train/loss: 0.0
Step: 33250, train/grad_norm: 4.385768193060358e-09
Step: 33250, train/learning_rate: 1.043550673784921e-05
Step: 33250, train/epoch: 7.912898540496826
Step: 33260, train/loss: 0.0
Step: 33260, train/grad_norm: 5.43226530425045e-09
Step: 33260, train/learning_rate: 1.0423607818665914e-05
Step: 33260, train/epoch: 7.915278434753418
Step: 33270, train/loss: 0.0
Step: 33270, train/grad_norm: 3.440958096234681e-08
Step: 33270, train/learning_rate: 1.0411708899482619e-05
Step: 33270, train/epoch: 7.91765832901001
Step: 33280, train/loss: 0.0
Step: 33280, train/grad_norm: 2.0135695422140998e-07
Step: 33280, train/learning_rate: 1.0399809980299324e-05
Step: 33280, train/epoch: 7.920038223266602
Step: 33290, train/loss: 0.0
Step: 33290, train/grad_norm: 4.354539839823701e-09
Step: 33290, train/learning_rate: 1.0387910151621327e-05
Step: 33290, train/epoch: 7.922418117523193
Step: 33300, train/loss: 0.0
Step: 33300, train/grad_norm: 1.1898036722524097e-10
Step: 33300, train/learning_rate: 1.0376011232438032e-05
Step: 33300, train/epoch: 7.924797534942627
Step: 33310, train/loss: 0.0
Step: 33310, train/grad_norm: 2.2708204205912352e-09
Step: 33310, train/learning_rate: 1.0364112313254736e-05
Step: 33310, train/epoch: 7.927177429199219
Step: 33320, train/loss: 0.0
Step: 33320, train/grad_norm: 4.829397770578225e-09
Step: 33320, train/learning_rate: 1.0352213394071441e-05
Step: 33320, train/epoch: 7.9295573234558105
Step: 33330, train/loss: 0.0
Step: 33330, train/grad_norm: 8.595745839556912e-07
Step: 33330, train/learning_rate: 1.0340314474888146e-05
Step: 33330, train/epoch: 7.931937217712402
Step: 33340, train/loss: 0.0
Step: 33340, train/grad_norm: 1.573082926142888e-07
Step: 33340, train/learning_rate: 1.0328414646210149e-05
Step: 33340, train/epoch: 7.934317111968994
Step: 33350, train/loss: 0.0
Step: 33350, train/grad_norm: 7.160019777074922e-07
Step: 33350, train/learning_rate: 1.0316515727026854e-05
Step: 33350, train/epoch: 7.936697006225586
Step: 33360, train/loss: 0.0
Step: 33360, train/grad_norm: 6.701452548441011e-07
Step: 33360, train/learning_rate: 1.0304616807843558e-05
Step: 33360, train/epoch: 7.9390764236450195
Step: 33370, train/loss: 0.1046999990940094
Step: 33370, train/grad_norm: 1.0324995081134603e-08
Step: 33370, train/learning_rate: 1.0292717888660263e-05
Step: 33370, train/epoch: 7.941456317901611
Step: 33380, train/loss: 0.0
Step: 33380, train/grad_norm: 8.055753255575837e-07
Step: 33380, train/learning_rate: 1.0280818969476968e-05
Step: 33380, train/epoch: 7.943836212158203
Step: 33390, train/loss: 0.0
Step: 33390, train/grad_norm: 4.551060361901449e-10
Step: 33390, train/learning_rate: 1.0268919140798971e-05
Step: 33390, train/epoch: 7.946216106414795
Step: 33400, train/loss: 0.0
Step: 33400, train/grad_norm: 4.007679188511304e-10
Step: 33400, train/learning_rate: 1.0257020221615676e-05
Step: 33400, train/epoch: 7.948596000671387
Step: 33410, train/loss: 0.0
Step: 33410, train/grad_norm: 3.3823444045566475e-10
Step: 33410, train/learning_rate: 1.024512130243238e-05
Step: 33410, train/epoch: 7.9509758949279785
Step: 33420, train/loss: 0.0
Step: 33420, train/grad_norm: 5.393488322624762e-09
Step: 33420, train/learning_rate: 1.0233222383249085e-05
Step: 33420, train/epoch: 7.953355312347412
Step: 33430, train/loss: 0.0
Step: 33430, train/grad_norm: 2.710605606637273e-08
Step: 33430, train/learning_rate: 1.022132346406579e-05
Step: 33430, train/epoch: 7.955735206604004
Step: 33440, train/loss: 9.999999747378752e-05
Step: 33440, train/grad_norm: 1.1868162008710215e-09
Step: 33440, train/learning_rate: 1.0209423635387793e-05
Step: 33440, train/epoch: 7.958115100860596
Step: 33450, train/loss: 0.0
Step: 33450, train/grad_norm: 1.0345196921335287e-09
Step: 33450, train/learning_rate: 1.0197524716204498e-05
Step: 33450, train/epoch: 7.9604949951171875
Step: 33460, train/loss: 0.0
Step: 33460, train/grad_norm: 9.046618454355837e-10
Step: 33460, train/learning_rate: 1.0185625797021203e-05
Step: 33460, train/epoch: 7.962874889373779
Step: 33470, train/loss: 0.0
Step: 33470, train/grad_norm: 4.5159822548157535e-06
Step: 33470, train/learning_rate: 1.0173726877837908e-05
Step: 33470, train/epoch: 7.965254783630371
Step: 33480, train/loss: 0.0
Step: 33480, train/grad_norm: 8.043283372671794e-09
Step: 33480, train/learning_rate: 1.0161827958654612e-05
Step: 33480, train/epoch: 7.967634677886963
Step: 33490, train/loss: 0.0
Step: 33490, train/grad_norm: 2.4311674451382714e-07
Step: 33490, train/learning_rate: 1.0149929039471317e-05
Step: 33490, train/epoch: 7.9700140953063965
Step: 33500, train/loss: 0.0
Step: 33500, train/grad_norm: 5.389078694406635e-08
Step: 33500, train/learning_rate: 1.013802921079332e-05
Step: 33500, train/epoch: 7.972393989562988
Step: 33510, train/loss: 0.0
Step: 33510, train/grad_norm: 2.2408752642106428e-13
Step: 33510, train/learning_rate: 1.0126130291610025e-05
Step: 33510, train/epoch: 7.97477388381958
Step: 33520, train/loss: 0.0
Step: 33520, train/grad_norm: 7.681342708565353e-07
Step: 33520, train/learning_rate: 1.011423137242673e-05
Step: 33520, train/epoch: 7.977153778076172
Step: 33530, train/loss: 0.0
Step: 33530, train/grad_norm: 5.273893301238619e-12
Step: 33530, train/learning_rate: 1.0102332453243434e-05
Step: 33530, train/epoch: 7.979533672332764
Step: 33540, train/loss: 0.0
Step: 33540, train/grad_norm: 2.8552273878545975e-09
Step: 33540, train/learning_rate: 1.009043353406014e-05
Step: 33540, train/epoch: 7.9819135665893555
Step: 33550, train/loss: 0.0
Step: 33550, train/grad_norm: 0.00014044855197425932
Step: 33550, train/learning_rate: 1.0078533705382142e-05
Step: 33550, train/epoch: 7.984292984008789
Step: 33560, train/loss: 0.0
Step: 33560, train/grad_norm: 4.750588256108301e-10
Step: 33560, train/learning_rate: 1.0066634786198847e-05
Step: 33560, train/epoch: 7.986672878265381
Step: 33570, train/loss: 0.0
Step: 33570, train/grad_norm: 7.714387351143159e-08
Step: 33570, train/learning_rate: 1.0054735867015552e-05
Step: 33570, train/epoch: 7.989052772521973
Step: 33580, train/loss: 0.0
Step: 33580, train/grad_norm: 1.807363645411897e-07
Step: 33580, train/learning_rate: 1.0042836947832257e-05
Step: 33580, train/epoch: 7.9914326667785645
Step: 33590, train/loss: 0.0
Step: 33590, train/grad_norm: 2.164581014429956e-10
Step: 33590, train/learning_rate: 1.0030938028648961e-05
Step: 33590, train/epoch: 7.993812561035156
Step: 33600, train/loss: 0.0
Step: 33600, train/grad_norm: 4.958557869860769e-11
Step: 33600, train/learning_rate: 1.0019038199970964e-05
Step: 33600, train/epoch: 7.996192455291748
Step: 33610, train/loss: 0.0
Step: 33610, train/grad_norm: 5.017311566213323e-11
Step: 33610, train/learning_rate: 1.0007139280787669e-05
Step: 33610, train/epoch: 7.998571872711182
Step: 33616, eval/loss: 0.038123831152915955
Step: 33616, eval/accuracy: 0.9970845580101013
Step: 33616, eval/f1: 0.9969180822372437
Step: 33616, eval/runtime: 735.8587036132812
Step: 33616, eval/samples_per_second: 9.788999557495117
Step: 33616, eval/steps_per_second: 1.2239999771118164
Step: 33616, train/epoch: 8.0
Step: 33620, train/loss: 0.0
Step: 33620, train/grad_norm: 1.1912689501514251e-08
Step: 33620, train/learning_rate: 9.995240361604374e-06
Step: 33620, train/epoch: 8.000951766967773
Step: 33630, train/loss: 0.0
Step: 33630, train/grad_norm: 2.9801533463214014e-10
Step: 33630, train/learning_rate: 9.983341442421079e-06
Step: 33630, train/epoch: 8.003332138061523
Step: 33640, train/loss: 0.0
Step: 33640, train/grad_norm: 2.53775306324755e-10
Step: 33640, train/learning_rate: 9.971442523237783e-06
Step: 33640, train/epoch: 8.005711555480957
Step: 33650, train/loss: 0.0
Step: 33650, train/grad_norm: 7.385338118837126e-09
Step: 33650, train/learning_rate: 9.959542694559786e-06
Step: 33650, train/epoch: 8.00809097290039
Step: 33660, train/loss: 0.0
Step: 33660, train/grad_norm: 4.258890484720723e-08
Step: 33660, train/learning_rate: 9.947643775376491e-06
Step: 33660, train/epoch: 8.01047134399414
Step: 33670, train/loss: 0.0
Step: 33670, train/grad_norm: 1.0427200075602272e-10
Step: 33670, train/learning_rate: 9.935744856193196e-06
Step: 33670, train/epoch: 8.012850761413574
Step: 33680, train/loss: 0.0
Step: 33680, train/grad_norm: 7.328372220172241e-08
Step: 33680, train/learning_rate: 9.9238459370099e-06
Step: 33680, train/epoch: 8.015231132507324
Step: 33690, train/loss: 0.0
Step: 33690, train/grad_norm: 1.1585224335419753e-08
Step: 33690, train/learning_rate: 9.911947017826606e-06
Step: 33690, train/epoch: 8.017610549926758
Step: 33700, train/loss: 0.0
Step: 33700, train/grad_norm: 8.051807043329973e-08
Step: 33700, train/learning_rate: 9.900047189148609e-06
Step: 33700, train/epoch: 8.019990921020508
Step: 33710, train/loss: 0.0
Step: 33710, train/grad_norm: 8.476674473456569e-09
Step: 33710, train/learning_rate: 9.888148269965313e-06
Step: 33710, train/epoch: 8.022370338439941
Step: 33720, train/loss: 0.0
Step: 33720, train/grad_norm: 5.01833352650749e-10
Step: 33720, train/learning_rate: 9.876249350782018e-06
Step: 33720, train/epoch: 8.024749755859375
Step: 33730, train/loss: 0.0
Step: 33730, train/grad_norm: 8.810956408922266e-10
Step: 33730, train/learning_rate: 9.864350431598723e-06
Step: 33730, train/epoch: 8.027130126953125
Step: 33740, train/loss: 0.0
Step: 33740, train/grad_norm: 4.6975663359205555e-09
Step: 33740, train/learning_rate: 9.852451512415428e-06
Step: 33740, train/epoch: 8.029509544372559
Step: 33750, train/loss: 0.0
Step: 33750, train/grad_norm: 3.974564151576487e-06
Step: 33750, train/learning_rate: 9.84055168373743e-06
Step: 33750, train/epoch: 8.031889915466309
Step: 33760, train/loss: 0.0
Step: 33760, train/grad_norm: 5.4134825511198414e-09
Step: 33760, train/learning_rate: 9.828652764554136e-06
Step: 33760, train/epoch: 8.034269332885742
Step: 33770, train/loss: 0.0
Step: 33770, train/grad_norm: 3.1391252264256764e-08
Step: 33770, train/learning_rate: 9.81675384537084e-06
Step: 33770, train/epoch: 8.036648750305176
Step: 33780, train/loss: 0.0
Step: 33780, train/grad_norm: 1.6180442541369189e-09
Step: 33780, train/learning_rate: 9.804854926187545e-06
Step: 33780, train/epoch: 8.039029121398926
Step: 33790, train/loss: 0.0
Step: 33790, train/grad_norm: 4.077495759702288e-06
Step: 33790, train/learning_rate: 9.79295600700425e-06
Step: 33790, train/epoch: 8.04140853881836
Step: 33800, train/loss: 0.0
Step: 33800, train/grad_norm: 4.171019529763953e-09
Step: 33800, train/learning_rate: 9.781057087820955e-06
Step: 33800, train/epoch: 8.04378890991211
Step: 33810, train/loss: 0.0
Step: 33810, train/grad_norm: 2.4500106388103404e-09
Step: 33810, train/learning_rate: 9.769157259142958e-06
Step: 33810, train/epoch: 8.046168327331543
Step: 33820, train/loss: 0.0
Step: 33820, train/grad_norm: 2.115459335483294e-10
Step: 33820, train/learning_rate: 9.757258339959662e-06
Step: 33820, train/epoch: 8.048548698425293
Step: 33830, train/loss: 0.0
Step: 33830, train/grad_norm: 1.7762017634481708e-08
Step: 33830, train/learning_rate: 9.745359420776367e-06
Step: 33830, train/epoch: 8.050928115844727
Step: 33840, train/loss: 0.0
Step: 33840, train/grad_norm: 3.4022014094858832e-09
Step: 33840, train/learning_rate: 9.733460501593072e-06
Step: 33840, train/epoch: 8.05330753326416
Step: 33850, train/loss: 0.0
Step: 33850, train/grad_norm: 1.347469407164681e-07
Step: 33850, train/learning_rate: 9.721561582409777e-06
Step: 33850, train/epoch: 8.05568790435791
Step: 33860, train/loss: 0.0
Step: 33860, train/grad_norm: 4.0220586861039465e-09
Step: 33860, train/learning_rate: 9.70966175373178e-06
Step: 33860, train/epoch: 8.058067321777344
Step: 33870, train/loss: 0.0
Step: 33870, train/grad_norm: 2.4585447566671803e-10
Step: 33870, train/learning_rate: 9.697762834548485e-06
Step: 33870, train/epoch: 8.060447692871094
Step: 33880, train/loss: 0.0
Step: 33880, train/grad_norm: 7.3008528111984106e-09
Step: 33880, train/learning_rate: 9.68586391536519e-06
Step: 33880, train/epoch: 8.062827110290527
Step: 33890, train/loss: 0.0
Step: 33890, train/grad_norm: 5.407168401916351e-08
Step: 33890, train/learning_rate: 9.673964996181894e-06
Step: 33890, train/epoch: 8.065207481384277
Step: 33900, train/loss: 0.0
Step: 33900, train/grad_norm: 6.11399020300496e-09
Step: 33900, train/learning_rate: 9.662066076998599e-06
Step: 33900, train/epoch: 8.067586898803711
Step: 33910, train/loss: 0.0
Step: 33910, train/grad_norm: 5.6936023611342534e-06
Step: 33910, train/learning_rate: 9.650166248320602e-06
Step: 33910, train/epoch: 8.069966316223145
Step: 33920, train/loss: 0.0
Step: 33920, train/grad_norm: 1.9679796423588414e-06
Step: 33920, train/learning_rate: 9.638267329137307e-06
Step: 33920, train/epoch: 8.072346687316895
Step: 33930, train/loss: 0.0
Step: 33930, train/grad_norm: 1.6246021194987748e-09
Step: 33930, train/learning_rate: 9.626368409954011e-06
Step: 33930, train/epoch: 8.074726104736328
Step: 33940, train/loss: 0.0
Step: 33940, train/grad_norm: 4.312583623544697e-09
Step: 33940, train/learning_rate: 9.614469490770716e-06
Step: 33940, train/epoch: 8.077106475830078
Step: 33950, train/loss: 0.0
Step: 33950, train/grad_norm: 5.312107873578498e-07
Step: 33950, train/learning_rate: 9.602570571587421e-06
Step: 33950, train/epoch: 8.079485893249512
Step: 33960, train/loss: 0.0
Step: 33960, train/grad_norm: 4.392536556707682e-09
Step: 33960, train/learning_rate: 9.590670742909424e-06
Step: 33960, train/epoch: 8.081865310668945
Step: 33970, train/loss: 0.0
Step: 33970, train/grad_norm: 5.602620603895048e-06
Step: 33970, train/learning_rate: 9.578771823726129e-06
Step: 33970, train/epoch: 8.084245681762695
Step: 33980, train/loss: 0.0
Step: 33980, train/grad_norm: 2.2552711087531208e-10
Step: 33980, train/learning_rate: 9.566872904542834e-06
Step: 33980, train/epoch: 8.086625099182129
Step: 33990, train/loss: 0.0
Step: 33990, train/grad_norm: 4.1052237520489143e-07
Step: 33990, train/learning_rate: 9.554973985359538e-06
Step: 33990, train/epoch: 8.089005470275879
Step: 34000, train/loss: 0.0
Step: 34000, train/grad_norm: 2.0334130212962265e-10
Step: 34000, train/learning_rate: 9.543075066176243e-06
Step: 34000, train/epoch: 8.091384887695312
Step: 34010, train/loss: 0.0
Step: 34010, train/grad_norm: 2.1069796574124666e-08
Step: 34010, train/learning_rate: 9.531175237498246e-06
Step: 34010, train/epoch: 8.093765258789062
Step: 34020, train/loss: 0.0
Step: 34020, train/grad_norm: 3.701919126797293e-07
Step: 34020, train/learning_rate: 9.519276318314951e-06
Step: 34020, train/epoch: 8.096144676208496
Step: 34030, train/loss: 0.0
Step: 34030, train/grad_norm: 5.7962601829331106e-08
Step: 34030, train/learning_rate: 9.507377399131656e-06
Step: 34030, train/epoch: 8.09852409362793
Step: 34040, train/loss: 0.0
Step: 34040, train/grad_norm: 6.973130517451409e-09
Step: 34040, train/learning_rate: 9.49547847994836e-06
Step: 34040, train/epoch: 8.10090446472168
Step: 34050, train/loss: 0.0
Step: 34050, train/grad_norm: 1.6307210026766938e-09
Step: 34050, train/learning_rate: 9.483579560765065e-06
Step: 34050, train/epoch: 8.103283882141113
Step: 34060, train/loss: 0.0
Step: 34060, train/grad_norm: 5.306401096305535e-09
Step: 34060, train/learning_rate: 9.471679732087068e-06
Step: 34060, train/epoch: 8.105664253234863
Step: 34070, train/loss: 0.0
Step: 34070, train/grad_norm: 1.1271939978629852e-11
Step: 34070, train/learning_rate: 9.459780812903773e-06
Step: 34070, train/epoch: 8.108043670654297
Step: 34080, train/loss: 0.0
Step: 34080, train/grad_norm: 1.2619678273040336e-05
Step: 34080, train/learning_rate: 9.447881893720478e-06
Step: 34080, train/epoch: 8.110424041748047
Step: 34090, train/loss: 0.0
Step: 34090, train/grad_norm: 3.556270167237585e-09
Step: 34090, train/learning_rate: 9.435982974537183e-06
Step: 34090, train/epoch: 8.11280345916748
Step: 34100, train/loss: 0.0
Step: 34100, train/grad_norm: 1.5872011482898074e-09
Step: 34100, train/learning_rate: 9.424084055353887e-06
Step: 34100, train/epoch: 8.115182876586914
Step: 34110, train/loss: 0.0
Step: 34110, train/grad_norm: 5.51501955214917e-10
Step: 34110, train/learning_rate: 9.41218422667589e-06
Step: 34110, train/epoch: 8.117563247680664
Step: 34120, train/loss: 0.0
Step: 34120, train/grad_norm: 2.1057124932610805e-09
Step: 34120, train/learning_rate: 9.400285307492595e-06
Step: 34120, train/epoch: 8.119942665100098
Step: 34130, train/loss: 0.0
Step: 34130, train/grad_norm: 2.5026565708685666e-08
Step: 34130, train/learning_rate: 9.3883863883093e-06
Step: 34130, train/epoch: 8.122323036193848
Step: 34140, train/loss: 0.0
Step: 34140, train/grad_norm: 1.884105586213991e-05
Step: 34140, train/learning_rate: 9.376487469126005e-06
Step: 34140, train/epoch: 8.124702453613281
Step: 34150, train/loss: 0.0
Step: 34150, train/grad_norm: 1.0935435890147005e-09
Step: 34150, train/learning_rate: 9.36458854994271e-06
Step: 34150, train/epoch: 8.127081871032715
Step: 34160, train/loss: 0.0
Step: 34160, train/grad_norm: 8.095788075479504e-07
Step: 34160, train/learning_rate: 9.352689630759414e-06
Step: 34160, train/epoch: 8.129462242126465
Step: 34170, train/loss: 0.0
Step: 34170, train/grad_norm: 1.3603977944143253e-09
Step: 34170, train/learning_rate: 9.340789802081417e-06
Step: 34170, train/epoch: 8.131841659545898
Step: 34180, train/loss: 0.0
Step: 34180, train/grad_norm: 3.5835536760231435e-09
Step: 34180, train/learning_rate: 9.328890882898122e-06
Step: 34180, train/epoch: 8.134222030639648
Step: 34190, train/loss: 0.0
Step: 34190, train/grad_norm: 1.7822739811768074e-11
Step: 34190, train/learning_rate: 9.316991963714827e-06
Step: 34190, train/epoch: 8.136601448059082
Step: 34200, train/loss: 0.0
Step: 34200, train/grad_norm: 1.327221776925569e-09
Step: 34200, train/learning_rate: 9.305093044531532e-06
Step: 34200, train/epoch: 8.138981819152832
Step: 34210, train/loss: 0.0
Step: 34210, train/grad_norm: 7.727469331086922e-09
Step: 34210, train/learning_rate: 9.293194125348236e-06
Step: 34210, train/epoch: 8.141361236572266
Step: 34220, train/loss: 0.0
Step: 34220, train/grad_norm: 2.7354960518977123e-09
Step: 34220, train/learning_rate: 9.28129429667024e-06
Step: 34220, train/epoch: 8.1437406539917
Step: 34230, train/loss: 0.0
Step: 34230, train/grad_norm: 6.315143963320224e-08
Step: 34230, train/learning_rate: 9.269395377486944e-06
Step: 34230, train/epoch: 8.14612102508545
Step: 34240, train/loss: 0.0
Step: 34240, train/grad_norm: 1.2563114992758528e-09
Step: 34240, train/learning_rate: 9.257496458303649e-06
Step: 34240, train/epoch: 8.148500442504883
Step: 34250, train/loss: 0.0
Step: 34250, train/grad_norm: 0.0002005843707593158
Step: 34250, train/learning_rate: 9.245597539120354e-06
Step: 34250, train/epoch: 8.150880813598633
Step: 34260, train/loss: 0.0
Step: 34260, train/grad_norm: 7.264747914348391e-09
Step: 34260, train/learning_rate: 9.233698619937059e-06
Step: 34260, train/epoch: 8.153260231018066
Step: 34270, train/loss: 0.0
Step: 34270, train/grad_norm: 1.1912312247730483e-09
Step: 34270, train/learning_rate: 9.221798791259062e-06
Step: 34270, train/epoch: 8.155640602111816
Step: 34280, train/loss: 0.0
Step: 34280, train/grad_norm: 1.0084614350189258e-12
Step: 34280, train/learning_rate: 9.209899872075766e-06
Step: 34280, train/epoch: 8.15802001953125
Step: 34290, train/loss: 0.0
Step: 34290, train/grad_norm: 1.3600105763789117e-10
Step: 34290, train/learning_rate: 9.198000952892471e-06
Step: 34290, train/epoch: 8.160399436950684
Step: 34300, train/loss: 0.0
Step: 34300, train/grad_norm: 7.697029320331839e-13
Step: 34300, train/learning_rate: 9.186102033709176e-06
Step: 34300, train/epoch: 8.162779808044434
Step: 34310, train/loss: 0.0
Step: 34310, train/grad_norm: 1.2150657147103061e-09
Step: 34310, train/learning_rate: 9.17420311452588e-06
Step: 34310, train/epoch: 8.165159225463867
Step: 34320, train/loss: 0.0
Step: 34320, train/grad_norm: 2.9516445465560537e-06
Step: 34320, train/learning_rate: 9.162303285847884e-06
Step: 34320, train/epoch: 8.167539596557617
Step: 34330, train/loss: 0.0
Step: 34330, train/grad_norm: 6.564955246091131e-10
Step: 34330, train/learning_rate: 9.150404366664588e-06
Step: 34330, train/epoch: 8.16991901397705
Step: 34340, train/loss: 0.0
Step: 34340, train/grad_norm: 2.212144245561376e-09
Step: 34340, train/learning_rate: 9.138505447481293e-06
Step: 34340, train/epoch: 8.172298431396484
Step: 34350, train/loss: 0.0
Step: 34350, train/grad_norm: 1.9224099645498427e-08
Step: 34350, train/learning_rate: 9.126606528297998e-06
Step: 34350, train/epoch: 8.174678802490234
Step: 34360, train/loss: 0.0
Step: 34360, train/grad_norm: 4.300280579627724e-06
Step: 34360, train/learning_rate: 9.114707609114703e-06
Step: 34360, train/epoch: 8.177058219909668
Step: 34370, train/loss: 0.0
Step: 34370, train/grad_norm: 5.509474543252679e-10
Step: 34370, train/learning_rate: 9.102807780436706e-06
Step: 34370, train/epoch: 8.179438591003418
Step: 34380, train/loss: 0.0
Step: 34380, train/grad_norm: 9.978445802971692e-08
Step: 34380, train/learning_rate: 9.09090886125341e-06
Step: 34380, train/epoch: 8.181818008422852
Step: 34390, train/loss: 0.0
Step: 34390, train/grad_norm: 2.0337039344231478e-11
Step: 34390, train/learning_rate: 9.079009942070115e-06
Step: 34390, train/epoch: 8.184198379516602
Step: 34400, train/loss: 0.0
Step: 34400, train/grad_norm: 3.28429834439703e-08
Step: 34400, train/learning_rate: 9.06711102288682e-06
Step: 34400, train/epoch: 8.186577796936035
Step: 34410, train/loss: 0.0
Step: 34410, train/grad_norm: 2.8142834729294464e-09
Step: 34410, train/learning_rate: 9.055212103703525e-06
Step: 34410, train/epoch: 8.188957214355469
Step: 34420, train/loss: 0.0
Step: 34420, train/grad_norm: 2.927608433456186e-10
Step: 34420, train/learning_rate: 9.043312275025528e-06
Step: 34420, train/epoch: 8.191337585449219
Step: 34430, train/loss: 0.0
Step: 34430, train/grad_norm: 4.0747258900353245e-09
Step: 34430, train/learning_rate: 9.031413355842233e-06
Step: 34430, train/epoch: 8.193717002868652
Step: 34440, train/loss: 0.0
Step: 34440, train/grad_norm: 6.591962392610284e-11
Step: 34440, train/learning_rate: 9.019514436658937e-06
Step: 34440, train/epoch: 8.196097373962402
Step: 34450, train/loss: 0.0
Step: 34450, train/grad_norm: 1.8766525045066373e-07
Step: 34450, train/learning_rate: 9.007615517475642e-06
Step: 34450, train/epoch: 8.198476791381836
Step: 34460, train/loss: 0.0
Step: 34460, train/grad_norm: 1.9504984294371752e-08
Step: 34460, train/learning_rate: 8.995716598292347e-06
Step: 34460, train/epoch: 8.200857162475586
Step: 34470, train/loss: 0.0
Step: 34470, train/grad_norm: 4.644319062663271e-08
Step: 34470, train/learning_rate: 8.983817679109052e-06
Step: 34470, train/epoch: 8.20323657989502
Step: 34480, train/loss: 0.0
Step: 34480, train/grad_norm: 3.436663176259458e-09
Step: 34480, train/learning_rate: 8.971917850431055e-06
Step: 34480, train/epoch: 8.205615997314453
Step: 34490, train/loss: 0.0
Step: 34490, train/grad_norm: 3.134804416049519e-09
Step: 34490, train/learning_rate: 8.96001893124776e-06
Step: 34490, train/epoch: 8.207996368408203
Step: 34500, train/loss: 0.0
Step: 34500, train/grad_norm: 1.5483228588131226e-10
Step: 34500, train/learning_rate: 8.948120012064464e-06
Step: 34500, train/epoch: 8.210375785827637
Step: 34510, train/loss: 0.0
Step: 34510, train/grad_norm: 1.4875051190799127e-09
Step: 34510, train/learning_rate: 8.93622109288117e-06
Step: 34510, train/epoch: 8.212756156921387
Step: 34520, train/loss: 0.0
Step: 34520, train/grad_norm: 3.5964089345474903e-11
Step: 34520, train/learning_rate: 8.924322173697874e-06
Step: 34520, train/epoch: 8.21513557434082
Step: 34530, train/loss: 0.0
Step: 34530, train/grad_norm: 5.914084795222152e-07
Step: 34530, train/learning_rate: 8.912422345019877e-06
Step: 34530, train/epoch: 8.21751594543457
Step: 34540, train/loss: 0.0
Step: 34540, train/grad_norm: 5.17123964982602e-07
Step: 34540, train/learning_rate: 8.900523425836582e-06
Step: 34540, train/epoch: 8.219895362854004
Step: 34550, train/loss: 0.0
Step: 34550, train/grad_norm: 1.466154003759712e-10
Step: 34550, train/learning_rate: 8.888624506653287e-06
Step: 34550, train/epoch: 8.222274780273438
Step: 34560, train/loss: 0.0
Step: 34560, train/grad_norm: 1.5215630144727044e-10
Step: 34560, train/learning_rate: 8.876725587469991e-06
Step: 34560, train/epoch: 8.224655151367188
Step: 34570, train/loss: 0.0
Step: 34570, train/grad_norm: 4.4637818064074963e-07
Step: 34570, train/learning_rate: 8.864826668286696e-06
Step: 34570, train/epoch: 8.227034568786621
Step: 34580, train/loss: 0.0
Step: 34580, train/grad_norm: 9.176073945127428e-05
Step: 34580, train/learning_rate: 8.852926839608699e-06
Step: 34580, train/epoch: 8.229414939880371
Step: 34590, train/loss: 0.0
Step: 34590, train/grad_norm: 7.512203081816438e-10
Step: 34590, train/learning_rate: 8.841027920425404e-06
Step: 34590, train/epoch: 8.231794357299805
Step: 34600, train/loss: 0.0
Step: 34600, train/grad_norm: 2.244617098767776e-05
Step: 34600, train/learning_rate: 8.829129001242109e-06
Step: 34600, train/epoch: 8.234173774719238
Step: 34610, train/loss: 0.0
Step: 34610, train/grad_norm: 4.687641990130942e-07
Step: 34610, train/learning_rate: 8.817230082058813e-06
Step: 34610, train/epoch: 8.236554145812988
Step: 34620, train/loss: 0.0
Step: 34620, train/grad_norm: 5.37684785584247e-10
Step: 34620, train/learning_rate: 8.805331162875518e-06
Step: 34620, train/epoch: 8.238933563232422
Step: 34630, train/loss: 0.0
Step: 34630, train/grad_norm: 1.303060770396769e-09
Step: 34630, train/learning_rate: 8.793431334197521e-06
Step: 34630, train/epoch: 8.241313934326172
Step: 34640, train/loss: 0.0
Step: 34640, train/grad_norm: 1.0310182707584659e-09
Step: 34640, train/learning_rate: 8.781532415014226e-06
Step: 34640, train/epoch: 8.243693351745605
Step: 34650, train/loss: 0.0
Step: 34650, train/grad_norm: 1.8116504962861768e-09
Step: 34650, train/learning_rate: 8.76963349583093e-06
Step: 34650, train/epoch: 8.246073722839355
Step: 34660, train/loss: 0.0
Step: 34660, train/grad_norm: 9.284218727145799e-09
Step: 34660, train/learning_rate: 8.757734576647636e-06
Step: 34660, train/epoch: 8.248453140258789
Step: 34670, train/loss: 0.0
Step: 34670, train/grad_norm: 3.732623987673378e-09
Step: 34670, train/learning_rate: 8.74583565746434e-06
Step: 34670, train/epoch: 8.250832557678223
Step: 34680, train/loss: 0.0
Step: 34680, train/grad_norm: 5.513022551895119e-05
Step: 34680, train/learning_rate: 8.733935828786343e-06
Step: 34680, train/epoch: 8.253212928771973
Step: 34690, train/loss: 0.0
Step: 34690, train/grad_norm: 5.918473107158206e-07
Step: 34690, train/learning_rate: 8.722036909603048e-06
Step: 34690, train/epoch: 8.255592346191406
Step: 34700, train/loss: 0.0
Step: 34700, train/grad_norm: 1.2095445756088452e-09
Step: 34700, train/learning_rate: 8.710137990419753e-06
Step: 34700, train/epoch: 8.257972717285156
Step: 34710, train/loss: 0.0
Step: 34710, train/grad_norm: 2.331251192089212e-09
Step: 34710, train/learning_rate: 8.698239071236458e-06
Step: 34710, train/epoch: 8.26035213470459
Step: 34720, train/loss: 0.0
Step: 34720, train/grad_norm: 1.4036320994392781e-09
Step: 34720, train/learning_rate: 8.686340152053162e-06
Step: 34720, train/epoch: 8.26273250579834
Step: 34730, train/loss: 0.0
Step: 34730, train/grad_norm: 5.21837682754267e-06
Step: 34730, train/learning_rate: 8.674440323375165e-06
Step: 34730, train/epoch: 8.265111923217773
Step: 34740, train/loss: 0.0
Step: 34740, train/grad_norm: 2.537361615395639e-05
Step: 34740, train/learning_rate: 8.66254140419187e-06
Step: 34740, train/epoch: 8.267491340637207
Step: 34750, train/loss: 0.0
Step: 34750, train/grad_norm: 4.610120063475165e-10
Step: 34750, train/learning_rate: 8.650642485008575e-06
Step: 34750, train/epoch: 8.269871711730957
Step: 34760, train/loss: 0.0
Step: 34760, train/grad_norm: 1.415989103747961e-08
Step: 34760, train/learning_rate: 8.63874356582528e-06
Step: 34760, train/epoch: 8.27225112915039
Step: 34770, train/loss: 0.0
Step: 34770, train/grad_norm: 1.4496905009764305e-07
Step: 34770, train/learning_rate: 8.626844646641985e-06
Step: 34770, train/epoch: 8.27463150024414
Step: 34780, train/loss: 0.0
Step: 34780, train/grad_norm: 5.852785478488443e-10
Step: 34780, train/learning_rate: 8.614944817963988e-06
Step: 34780, train/epoch: 8.277010917663574
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=6ffd0164-0f41-4bbe-b905-7dda371dfd20">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[9]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.summary.summary_iterator</span> <span class="kn">import</span> <span class="n">summary_iterator</span>

<span class="n">logs_directory</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">'./'</span><span class="p">,</span> <span class="n">project_name</span><span class="p">,</span> <span class="s1">'logs'</span><span class="p">)</span>
<span class="n">file_pattern</span> <span class="o">=</span> <span class="s1">'events.out.tfevents.*'</span>

<span class="n">event_files</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">logs_directory</span><span class="p">,</span> <span class="n">file_pattern</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">extract_metrics</span><span class="p">(</span><span class="n">event_files</span><span class="p">):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">last_train_loss</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">for</span> <span class="n">event_file</span> <span class="ow">in</span> <span class="n">event_files</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">summary_iterator</span><span class="p">(</span><span class="n">event_file</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">e</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">v</span><span class="o">.</span><span class="n">HasField</span><span class="p">(</span><span class="s1">'simple_value'</span><span class="p">):</span>
                    <span class="n">step</span> <span class="o">=</span> <span class="n">e</span><span class="o">.</span><span class="n">step</span>
                    <span class="n">metric_name</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">'/'</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                    <span class="n">metric_value</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">simple_value</span>

                    <span class="n">formatted_value</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">metric_value</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">"</span>

                    <span class="k">if</span> <span class="s1">'train/loss'</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="p">:</span>
                        <span class="n">last_train_loss</span> <span class="o">=</span> <span class="n">formatted_value</span>

                    <span class="k">if</span> <span class="s1">'eval'</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="p">:</span>
                        <span class="n">entry</span> <span class="o">=</span> <span class="nb">next</span><span class="p">((</span><span class="n">item</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">data</span> <span class="k">if</span> <span class="n">item</span><span class="p">[</span><span class="s1">'Step'</span><span class="p">]</span> <span class="o">==</span> <span class="n">step</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="n">entry</span><span class="p">:</span>
                            <span class="n">entry</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'Step'</span><span class="p">:</span> <span class="n">step</span><span class="p">,</span> <span class="s1">'Train Loss'</span><span class="p">:</span> <span class="n">last_train_loss</span><span class="p">,</span> <span class="s1">'Eval Loss'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">'Accuracy'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">'F1'</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>
                            <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">entry</span><span class="p">)</span>
                        <span class="k">if</span> <span class="s1">'loss'</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="p">:</span>
                            <span class="n">entry</span><span class="p">[</span><span class="s1">'Eval Loss'</span><span class="p">]</span> <span class="o">=</span> <span class="n">formatted_value</span>
                        <span class="k">elif</span> <span class="s1">'accuracy'</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="p">:</span>
                            <span class="n">entry</span><span class="p">[</span><span class="s1">'Accuracy'</span><span class="p">]</span> <span class="o">=</span> <span class="n">formatted_value</span>
                        <span class="k">elif</span> <span class="s1">'f1'</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="p">:</span>
                            <span class="n">entry</span><span class="p">[</span><span class="s1">'F1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">formatted_value</span>

    <span class="k">return</span> <span class="n">data</span>

<span class="n">metrics_data</span> <span class="o">=</span> <span class="n">extract_metrics</span><span class="p">(</span><span class="n">event_files</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">metrics_data</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">'Step'</span><span class="p">)</span>

<span class="n">file_path</span> <span class="o">=</span> <span class="s2">"./images/"</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s2">"_Checkpoint_Data.csv"</span>
<span class="n">df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>    Step Train Loss Eval Loss  Accuracy        F1
2   4202   0.000400  0.012438  0.995974  0.995751
3   8404   0.001000  0.014532  0.997779  0.997654
4  12606   0.000000  0.017989  0.998334  0.998240
5  16808   0.000000  0.076200  0.993336  0.992941
6  21010   0.000000  0.033994  0.994308  0.994005
7  25212   0.000000  0.020162  0.997362  0.997215
8  29414   0.000000  0.035140  0.997223  0.997065
9  33616   0.000000  0.038124  0.997085  0.996918
0  37818   0.000000  0.037216  0.997085  0.996918
1  42020   0.000000  0.026434  0.997918  0.997800
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=37cc5f92-d47f-4356-8fad-d9b64b6f5361">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[25]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">fillna</span><span class="p">({</span>
    <span class="s1">'Eval Loss'</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="s1">'inf'</span><span class="p">),</span>
    <span class="s1">'Accuracy'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">'F1'</span><span class="p">:</span> <span class="mi">0</span>
<span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">df</span><span class="p">[</span><span class="s1">'Eval Loss'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Eval Loss'</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'Accuracy'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Accuracy'</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'F1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'F1'</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>

<span class="n">df</span><span class="p">[</span><span class="s1">'Eval Loss Rank'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Eval Loss'</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'Accuracy Rank'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Accuracy'</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'F1 Rank'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'F1'</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">df</span><span class="p">[</span><span class="s1">'Rank Sum'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Eval Loss Rank'</span><span class="p">]</span> <span class="o">+</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Accuracy Rank'</span><span class="p">]</span> <span class="o">+</span> <span class="n">df</span><span class="p">[</span><span class="s1">'F1 Rank'</span><span class="p">]</span>

<span class="n">best_checkpoint</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">'Rank Sum'</span><span class="p">]</span><span class="o">.</span><span class="n">idxmin</span><span class="p">()]</span>

<span class="n">checkpoint_folder_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"checkpoint-</span><span class="si">{</span><span class="n">best_checkpoint</span><span class="p">[</span><span class="s1">'Step'</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Best Checkpoint Step: </span><span class="si">{</span><span class="n">checkpoint_folder_name</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">best_checkpoint</span><span class="p">[[</span><span class="s1">'Step'</span><span class="p">,</span> <span class="s1">'Train Loss'</span><span class="p">,</span> <span class="s1">'Eval Loss'</span><span class="p">,</span> <span class="s1">'Accuracy'</span><span class="p">,</span> <span class="s1">'F1'</span><span class="p">,</span> <span class="s1">'Rank Sum'</span><span class="p">]])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Best Checkpoint Step: checkpoint-12606
Step             12606
Train Loss    0.000000
Eval Loss     0.017989
Accuracy      0.998334
F1             0.99824
Rank Sum           5.0
Name: 4, dtype: object
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=5c3c3999-8c32-4a25-aaca-2ec9036b69ed">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Run-TensorBoard">Run TensorBoard<a class="anchor-link" href="#Run-TensorBoard"></a></h3><p>tensorboard --logdir=~/kuk/Praxis/praxis-Llama-2-7b-hf-small-finetune/logs --host=0.0.0.0</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=f8f36022-dc73-492f-998c-43e5ed1a6f46">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="PAUSE-SCRIPT">PAUSE SCRIPT<a class="anchor-link" href="#PAUSE-SCRIPT"></a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=444ae65b-241c-47ef-bbc2-81f3a2ce50e9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[28]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># My flag to pause the script, set to True to pause</span>
<span class="n">pause_script</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=c8be3672-68e0-44f8-b8b1-31290665658d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[29]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">StopExecution</span><span class="p">(</span><span class="ne">Exception</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">_render_traceback_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Script Paused"</span><span class="p">)</span>
        <span class="k">pass</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=eb3b5ed2-5320-49c0-9e61-90d6f0942f7a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[30]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="n">pause_script</span><span class="p">:</span>
    <span class="k">raise</span> <span class="n">StopExecution</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=19be6e26-d888-4da0-b3f8-836d68ac2051">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Testing">Testing<a class="anchor-link" href="#Testing"></a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=b5111194-5eeb-4104-8df0-f977a6b716e0">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[32]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>

<span class="n">bnb_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span>
    <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="s2">"nf4"</span><span class="p">,</span>
    <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
<span class="p">)</span>

<span class="n">base_model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">checkpoint</span><span class="p">,</span>
    <span class="n">quantization_config</span><span class="o">=</span><span class="n">bnb_config</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span>
    <span class="n">num_labels</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">token</span><span class="o">=</span><span class="n">access_token</span><span class="p">,</span>
    <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">eval_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">checkpoint</span><span class="p">,</span>
    <span class="n">token</span><span class="o">=</span><span class="n">access_token</span><span class="p">,</span>
    <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Loading checkpoint shards:   0%|          | 0/4 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=530fa55f-c8c4-485f-a093-09bf2175d586">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[33]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">PeftModel</span>
<span class="n">test_checkpoint_name</span> <span class="o">=</span> <span class="n">checkpoint_folder_name</span>
<span class="n">ft_model</span> <span class="o">=</span> <span class="n">PeftModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model</span><span class="p">,</span> <span class="n">project_name</span><span class="o">+</span><span class="s1">'/'</span><span class="o">+</span><span class="n">test_checkpoint_name</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=27354c4f-95cc-4d1f-ac64-c5e6b4a842ae">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[28]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Using"</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">(),</span> <span class="s2">"GPUs!"</span><span class="p">)</span>
    <span class="n">ft_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">ft_model</span><span class="p">)</span>

<span class="n">ft_model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Using 7 GPUs!
</pre>
</div>
</div>
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[28]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>DataParallel(
  (module): PeftModelForSequenceClassification(
    (base_model): LoraModel(
      (model): LlamaForSequenceClassification(
        (model): LlamaModel(
          (embed_tokens): Embedding(128256, 4096)
          (layers): ModuleList(
            (0-31): 32 x LlamaDecoderLayer(
              (self_attn): LlamaSdpaAttention(
                (q_proj): lora.Linear4bit(
                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=8, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=8, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
                (k_proj): lora.Linear4bit(
                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=8, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=8, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
                (v_proj): lora.Linear4bit(
                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=8, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=8, out_features=1024, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
                (o_proj): lora.Linear4bit(
                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=8, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=8, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): lora.Linear4bit(
                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=8, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=8, out_features=14336, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
                (up_proj): lora.Linear4bit(
                  (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=8, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=8, out_features=14336, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
                (down_proj): lora.Linear4bit(
                  (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=14336, out_features=8, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=8, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
                (act_fn): SiLU()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
          )
          (norm): LlamaRMSNorm()
        )
        (score): ModulesToSaveWrapper(
          (original_module): Linear(in_features=4096, out_features=2, bias=False)
          (modules_to_save): ModuleDict(
            (default): Linear(in_features=4096, out_features=2, bias=False)
          )
        )
      )
    )
  )
)</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=eee9ab6f-a7d5-4dd5-9436-2f6f6e2bffaf">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[34]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tqdm.auto</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="n">processed_predictions</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">tokenized_test_ds</span><span class="p">):</span>

        <span class="n">eval_prompt</span> <span class="o">=</span> <span class="n">record</span><span class="p">[</span><span class="s1">'article'</span><span class="p">]</span>
        <span class="n">model_input</span> <span class="o">=</span> <span class="n">tokenize_fn</span><span class="p">({</span><span class="s1">'article'</span><span class="p">:</span> <span class="n">eval_prompt</span><span class="p">})</span>

        <span class="c1"># model_input = {k: v.to('cuda') for k, v in model_input.items()}</span>
        
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">ft_model</span><span class="p">(</span><span class="o">**</span><span class="n">model_input</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>
        
        <span class="n">prediction</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>  <span class="c1"># Use .item() to get a Python number</span>
        <span class="n">processed_predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">prediction</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>  0%|          | 0/7203 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=80f06a2c-5ded-4da0-8232-145383967c9d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Accuracy-and-F1">Accuracy and F1<a class="anchor-link" href="#Accuracy-and-F1"></a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=6e891bc5-55e0-4c43-a035-0edc37dcb6e3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[35]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">true_articles</span> <span class="o">=</span> <span class="n">tokenized_test_ds</span><span class="p">[</span><span class="s1">'label'</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=6bf33824-d7cc-4d22-af4d-7d44e569c2b9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[36]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span><span class="p">,</span> <span class="n">accuracy_score</span>

<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">true_articles</span><span class="p">,</span> <span class="n">processed_predictions</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"accuracy:"</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>accuracy: 0.9981951964459254
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=9b37dc24-3bc0-48a0-ace9-a360bae91169">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[37]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">true_articles</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=a29ebf68-632f-49d1-b903-407ff05b5569">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[38]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">processed_predictions</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=fb4d8350-a8d1-4853-a9f8-74ae9ea36bde">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[39]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">f1_score</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">true_articles</span><span class="p">,</span> <span class="n">processed_predictions</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">'macro'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"f1_score:"</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>f1_score: 0.998096267015024
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=2dcaac14-2b83-4c24-b83a-f6d0e73a2d2a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Confusion-Matrix">Confusion Matrix<a class="anchor-link" href="#Confusion-Matrix"></a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=57e64afb-e3ee-4c79-8228-b2d79806229e">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[40]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">ConfusionMatrixDisplay</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">tokenized_test_ds</span><span class="p">[</span><span class="s1">'label'</span><span class="p">],</span> <span class="n">processed_predictions</span><span class="p">)</span>

<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'human'</span><span class="p">,</span> <span class="s1">'machine'</span><span class="p">]</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="o">=</span><span class="n">cm</span><span class="p">,</span> <span class="n">display_labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Blues</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=af7d6f55-fb4f-4195-ac3f-93852ecd4b65">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In[43]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">file_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">project_name</span><span class="si">}</span><span class="s2">-v12.ipynb"</span>
<span class="n">html_file_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">file_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">'.ipynb'</span><span class="p">,</span><span class="w"> </span><span class="s1">'.html'</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span>

<span class="n">command</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"jupyter nbconvert '</span><span class="si">{</span><span class="n">file_name</span><span class="si">}</span><span class="s2">' --to html --output-dir './html' --output '</span><span class="si">{</span><span class="n">html_file_name</span><span class="si">}</span><span class="s2">'"</span>
<span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">system</span><span class="p">(</span><span class="n">command</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[NbConvertApp] Converting notebook praxis-Meta-Llama-3-8B-small-finetune-v11.ipynb to html
[NbConvertApp] WARNING | Alternative text is missing on 1 image(s).
[NbConvertApp] Writing 1295257 bytes to html/praxis-Meta-Llama-3-8B-small-finetune-v11.html
</pre>
</div>
</div>
</div>
</div>
</div>
</main>
</body>
</html>
