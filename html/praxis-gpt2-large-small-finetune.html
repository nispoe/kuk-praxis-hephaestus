<!DOCTYPE html>

<html lang="en">
<head><meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>praxis-gpt2-large-small-finetune-v12</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<style type="text/css">
    pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: var(--jp-cell-editor-active-background) }
.highlight { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) }
.highlight .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */
.highlight .err { color: var(--jp-mirror-editor-error-color) } /* Error */
.highlight .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */
.highlight .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */
.highlight .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */
.highlight .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */
.highlight .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */
.highlight .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */
.highlight .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */
.highlight .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */
.highlight .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */
.highlight .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */
.highlight .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */
.highlight .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */
.highlight .pm { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation.Marker */
.highlight .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */
.highlight .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */
.highlight .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */
.highlight .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */
.highlight .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */
.highlight .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */
.highlight .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */
.highlight .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */
.highlight .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */
.highlight .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */
.highlight .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */
.highlight .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */
.highlight .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */
.highlight .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */
.highlight .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */
.highlight .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */
.highlight .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */
.highlight .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */
.highlight .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */
.highlight .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */
  </style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
 * Mozilla scrollbar styling
 */

/* use standard opaque scrollbars for most nodes */
[data-jp-theme-scrollbars='true'] {
  scrollbar-color: rgb(var(--jp-scrollbar-thumb-color))
    var(--jp-scrollbar-background-color);
}

/* for code nodes, use a transparent style of scrollbar. These selectors
 * will match lower in the tree, and so will override the above */
[data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar,
[data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
}

/* tiny scrollbar */

.jp-scrollbar-tiny {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
  scrollbar-width: thin;
}

/* tiny scrollbar */

.jp-scrollbar-tiny::-webkit-scrollbar,
.jp-scrollbar-tiny::-webkit-scrollbar-corner {
  background-color: transparent;
  height: 4px;
  width: 4px;
}

.jp-scrollbar-tiny::-webkit-scrollbar-thumb {
  background: rgba(var(--jp-scrollbar-thumb-color), 0.5);
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:horizontal {
  border-left: 0 solid transparent;
  border-right: 0 solid transparent;
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:vertical {
  border-top: 0 solid transparent;
  border-bottom: 0 solid transparent;
}

/*
 * Lumino
 */

.lm-ScrollBar[data-orientation='horizontal'] {
  min-height: 16px;
  max-height: 16px;
  min-width: 45px;
  border-top: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] {
  min-width: 16px;
  max-width: 16px;
  min-height: 45px;
  border-left: 1px solid #a0a0a0;
}

.lm-ScrollBar-button {
  background-color: #f0f0f0;
  background-position: center center;
  min-height: 15px;
  max-height: 15px;
  min-width: 15px;
  max-width: 15px;
}

.lm-ScrollBar-button:hover {
  background-color: #dadada;
}

.lm-ScrollBar-button.lm-mod-active {
  background-color: #cdcdcd;
}

.lm-ScrollBar-track {
  background: #f0f0f0;
}

.lm-ScrollBar-thumb {
  background: #cdcdcd;
}

.lm-ScrollBar-thumb:hover {
  background: #bababa;
}

.lm-ScrollBar-thumb.lm-mod-active {
  background: #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal'] .lm-ScrollBar-thumb {
  height: 100%;
  min-width: 15px;
  border-left: 1px solid #a0a0a0;
  border-right: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] .lm-ScrollBar-thumb {
  width: 100%;
  min-height: 15px;
  border-top: 1px solid #a0a0a0;
  border-bottom: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-left);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-right);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-up);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-down);
  background-size: 17px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Widget {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
}

.lm-Widget.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.lm-AccordionPanel[data-orientation='horizontal'] > .lm-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  display: flex;
  flex-direction: column;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-CommandPalette-search {
  flex: 0 0 auto;
}

.lm-CommandPalette-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  min-height: 0;
  overflow: auto;
  list-style-type: none;
}

.lm-CommandPalette-header {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-CommandPalette-item {
  display: flex;
  flex-direction: row;
}

.lm-CommandPalette-itemIcon {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemContent {
  flex: 1 1 auto;
  overflow: hidden;
}

.lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemLabel {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-close-icon {
  border: 1px solid transparent;
  background-color: transparent;
  position: absolute;
  z-index: 1;
  right: 3%;
  top: 0;
  bottom: 0;
  margin: auto;
  padding: 7px 0;
  display: none;
  vertical-align: middle;
  outline: 0;
  cursor: pointer;
}
.lm-close-icon:after {
  content: 'X';
  display: block;
  width: 15px;
  height: 15px;
  text-align: center;
  color: #000;
  font-weight: normal;
  font-size: 12px;
  cursor: pointer;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-DockPanel {
  z-index: 0;
}

.lm-DockPanel-widget {
  z-index: 0;
}

.lm-DockPanel-tabBar {
  z-index: 1;
}

.lm-DockPanel-handle {
  z-index: 2;
}

.lm-DockPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-DockPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-DockPanel-handle[data-orientation='horizontal'] {
  cursor: ew-resize;
}

.lm-DockPanel-handle[data-orientation='vertical'] {
  cursor: ns-resize;
}

.lm-DockPanel-handle[data-orientation='horizontal']:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-DockPanel-handle[data-orientation='vertical']:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

.lm-DockPanel-overlay {
  z-index: 3;
  box-sizing: border-box;
  pointer-events: none;
}

.lm-DockPanel-overlay.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Menu {
  z-index: 10000;
  position: absolute;
  white-space: nowrap;
  overflow-x: hidden;
  overflow-y: auto;
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-Menu-content {
  margin: 0;
  padding: 0;
  display: table;
  list-style-type: none;
}

.lm-Menu-item {
  display: table-row;
}

.lm-Menu-item.lm-mod-hidden,
.lm-Menu-item.lm-mod-collapsed {
  display: none !important;
}

.lm-Menu-itemIcon,
.lm-Menu-itemSubmenuIcon {
  display: table-cell;
  text-align: center;
}

.lm-Menu-itemLabel {
  display: table-cell;
  text-align: left;
}

.lm-Menu-itemShortcut {
  display: table-cell;
  text-align: right;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-MenuBar {
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-MenuBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex-direction: row;
  list-style-type: none;
}

.lm-MenuBar-item {
  box-sizing: border-box;
}

.lm-MenuBar-itemIcon,
.lm-MenuBar-itemLabel {
  display: inline-block;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-ScrollBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-ScrollBar[data-orientation='horizontal'] {
  flex-direction: row;
}

.lm-ScrollBar[data-orientation='vertical'] {
  flex-direction: column;
}

.lm-ScrollBar-button {
  box-sizing: border-box;
  flex: 0 0 auto;
}

.lm-ScrollBar-track {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
  flex: 1 1 auto;
}

.lm-ScrollBar-thumb {
  box-sizing: border-box;
  position: absolute;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-SplitPanel-child {
  z-index: 0;
}

.lm-SplitPanel-handle {
  z-index: 1;
}

.lm-SplitPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-SplitPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle {
  cursor: ew-resize;
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle {
  cursor: ns-resize;
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-TabBar[data-orientation='horizontal'] {
  flex-direction: row;
  align-items: flex-end;
}

.lm-TabBar[data-orientation='vertical'] {
  flex-direction: column;
  align-items: flex-end;
}

.lm-TabBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex: 1 1 auto;
  list-style-type: none;
}

.lm-TabBar[data-orientation='horizontal'] > .lm-TabBar-content {
  flex-direction: row;
}

.lm-TabBar[data-orientation='vertical'] > .lm-TabBar-content {
  flex-direction: column;
}

.lm-TabBar-tab {
  display: flex;
  flex-direction: row;
  box-sizing: border-box;
  overflow: hidden;
  touch-action: none; /* Disable native Drag/Drop */
}

.lm-TabBar-tabIcon,
.lm-TabBar-tabCloseIcon {
  flex: 0 0 auto;
}

.lm-TabBar-tabLabel {
  flex: 1 1 auto;
  overflow: hidden;
  white-space: nowrap;
}

.lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
}

.lm-TabBar-tab.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar-addButton.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab {
  position: relative;
}

.lm-TabBar.lm-mod-dragging[data-orientation='horizontal'] .lm-TabBar-tab {
  left: 0;
  transition: left 150ms ease;
}

.lm-TabBar.lm-mod-dragging[data-orientation='vertical'] .lm-TabBar-tab {
  top: 0;
  transition: top 150ms ease;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging {
  transition: none;
}

.lm-TabBar-tabLabel .lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
  background: inherit;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabPanel-tabBar {
  z-index: 1;
}

.lm-TabPanel-stackedPanel {
  z-index: 0;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapse {
  display: flex;
  flex-direction: column;
  align-items: stretch;
}

.jp-Collapse-header {
  padding: 1px 12px;
  background-color: var(--jp-layout-color1);
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  align-items: center;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  text-transform: uppercase;
  user-select: none;
}

.jp-Collapser-icon {
  height: 16px;
}

.jp-Collapse-header-collapsed .jp-Collapser-icon {
  transform: rotate(-90deg);
  margin: auto 0;
}

.jp-Collapser-title {
  line-height: 25px;
}

.jp-Collapse-contents {
  padding: 0 12px;
  background-color: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensureUiComponents() in @jupyterlab/buildutils */

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

/* Icons urls */

:root {
  --jp-icon-add-above: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5MikiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik00Ljc1IDQuOTMwNjZINi42MjVWNi44MDU2NkM2LjYyNSA3LjAxMTkxIDYuNzkzNzUgNy4xODA2NiA3IDcuMTgwNjZDNy4yMDYyNSA3LjE4MDY2IDcuMzc1IDcuMDExOTEgNy4zNzUgNi44MDU2NlY0LjkzMDY2SDkuMjVDOS40NTYyNSA0LjkzMDY2IDkuNjI1IDQuNzYxOTEgOS42MjUgNC41NTU2NkM5LjYyNSA0LjM0OTQxIDkuNDU2MjUgNC4xODA2NiA5LjI1IDQuMTgwNjZINy4zNzVWMi4zMDU2NkM3LjM3NSAyLjA5OTQxIDcuMjA2MjUgMS45MzA2NiA3IDEuOTMwNjZDNi43OTM3NSAxLjkzMDY2IDYuNjI1IDIuMDk5NDEgNi42MjUgMi4zMDU2NlY0LjE4MDY2SDQuNzVDNC41NDM3NSA0LjE4MDY2IDQuMzc1IDQuMzQ5NDEgNC4zNzUgNC41NTU2NkM0LjM3NSA0Ljc2MTkxIDQuNTQzNzUgNC45MzA2NiA0Ljc1IDQuOTMwNjZaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC43Ii8+CjwvZz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTExLjUgOS41VjExLjVMMi41IDExLjVWOS41TDExLjUgOS41Wk0xMiA4QzEyLjU1MjMgOCAxMyA4LjQ0NzcyIDEzIDlWMTJDMTMgMTIuNTUyMyAxMi41NTIzIDEzIDEyIDEzTDIgMTNDMS40NDc3MiAxMyAxIDEyLjU1MjMgMSAxMlY5QzEgOC40NDc3MiAxLjQ0NzcxIDggMiA4TDEyIDhaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5MiI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KC0xIDAgMCAxIDEwIDEuNTU1NjYpIi8+CjwvY2xpcFBhdGg+CjwvZGVmcz4KPC9zdmc+Cg==);
  --jp-icon-add-below: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5OCkiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik05LjI1IDEwLjA2OTNMNy4zNzUgMTAuMDY5M0w3LjM3NSA4LjE5NDM0QzcuMzc1IDcuOTg4MDkgNy4yMDYyNSA3LjgxOTM0IDcgNy44MTkzNEM2Ljc5Mzc1IDcuODE5MzQgNi42MjUgNy45ODgwOSA2LjYyNSA4LjE5NDM0TDYuNjI1IDEwLjA2OTNMNC43NSAxMC4wNjkzQzQuNTQzNzUgMTAuMDY5MyA0LjM3NSAxMC4yMzgxIDQuMzc1IDEwLjQ0NDNDNC4zNzUgMTAuNjUwNiA0LjU0Mzc1IDEwLjgxOTMgNC43NSAxMC44MTkzTDYuNjI1IDEwLjgxOTNMNi42MjUgMTIuNjk0M0M2LjYyNSAxMi45MDA2IDYuNzkzNzUgMTMuMDY5MyA3IDEzLjA2OTNDNy4yMDYyNSAxMy4wNjkzIDcuMzc1IDEyLjkwMDYgNy4zNzUgMTIuNjk0M0w3LjM3NSAxMC44MTkzTDkuMjUgMTAuODE5M0M5LjQ1NjI1IDEwLjgxOTMgOS42MjUgMTAuNjUwNiA5LjYyNSAxMC40NDQzQzkuNjI1IDEwLjIzODEgOS40NTYyNSAxMC4wNjkzIDkuMjUgMTAuMDY5M1oiIGZpbGw9IiM2MTYxNjEiIHN0cm9rZT0iIzYxNjE2MSIgc3Ryb2tlLXdpZHRoPSIwLjciLz4KPC9nPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMi41IDUuNUwyLjUgMy41TDExLjUgMy41TDExLjUgNS41TDIuNSA1LjVaTTIgN0MxLjQ0NzcyIDcgMSA2LjU1MjI4IDEgNkwxIDNDMSAyLjQ0NzcyIDEuNDQ3NzIgMiAyIDJMMTIgMkMxMi41NTIzIDIgMTMgMi40NDc3MiAxMyAzTDEzIDZDMTMgNi41NTIyOSAxMi41NTIzIDcgMTIgN0wyIDdaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5OCI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KDEgMS43NDg0NmUtMDcgMS43NDg0NmUtMDcgLTEgNCAxMy40NDQzKSIvPgo8L2NsaXBQYXRoPgo8L2RlZnM+Cjwvc3ZnPgo=);
  --jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bell: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE2IDE2IiB2ZXJzaW9uPSIxLjEiPgogICA8cGF0aCBjbGFzcz0ianAtaWNvbjIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMzMzMzMzIgogICAgICBkPSJtOCAwLjI5Yy0xLjQgMC0yLjcgMC43My0zLjYgMS44LTEuMiAxLjUtMS40IDMuNC0xLjUgNS4yLTAuMTggMi4yLTAuNDQgNC0yLjMgNS4zbDAuMjggMS4zaDVjMC4wMjYgMC42NiAwLjMyIDEuMSAwLjcxIDEuNSAwLjg0IDAuNjEgMiAwLjYxIDIuOCAwIDAuNTItMC40IDAuNi0xIDAuNzEtMS41aDVsMC4yOC0xLjNjLTEuOS0wLjk3LTIuMi0zLjMtMi4zLTUuMy0wLjEzLTEuOC0wLjI2LTMuNy0xLjUtNS4yLTAuODUtMS0yLjItMS44LTMuNi0xLjh6bTAgMS40YzAuODggMCAxLjkgMC41NSAyLjUgMS4zIDAuODggMS4xIDEuMSAyLjcgMS4yIDQuNCAwLjEzIDEuNyAwLjIzIDMuNiAxLjMgNS4yaC0xMGMxLjEtMS42IDEuMi0zLjQgMS4zLTUuMiAwLjEzLTEuNyAwLjMtMy4zIDEuMi00LjQgMC41OS0wLjcyIDEuNi0xLjMgMi41LTEuM3ptLTAuNzQgMTJoMS41Yy0wLjAwMTUgMC4yOCAwLjAxNSAwLjc5LTAuNzQgMC43OS0wLjczIDAuMDAxNi0wLjcyLTAuNTMtMC43NC0wLjc5eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-bug-dot: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiPgogICAgICAgIDxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMTcuMTkgOEgyMFYxMEgxNy45MUMxNy45NiAxMC4zMyAxOCAxMC42NiAxOCAxMVYxMkgyMFYxNEgxOC41SDE4VjE0LjAyNzVDMTUuNzUgMTQuMjc2MiAxNCAxNi4xODM3IDE0IDE4LjVDMTQgMTkuMjA4IDE0LjE2MzUgMTkuODc3OSAxNC40NTQ5IDIwLjQ3MzlDMTMuNzA2MyAyMC44MTE3IDEyLjg3NTcgMjEgMTIgMjFDOS43OCAyMSA3Ljg1IDE5Ljc5IDYuODEgMThINFYxNkg2LjA5QzYuMDQgMTUuNjcgNiAxNS4zNCA2IDE1VjE0SDRWMTJINlYxMUM2IDEwLjY2IDYuMDQgMTAuMzMgNi4wOSAxMEg0VjhINi44MUM3LjI2IDcuMjIgNy44OCA2LjU1IDguNjIgNi4wNEw3IDQuNDFMOC40MSAzTDEwLjU5IDUuMTdDMTEuMDQgNS4wNiAxMS41MSA1IDEyIDVDMTIuNDkgNSAxMi45NiA1LjA2IDEzLjQyIDUuMTdMMTUuNTkgM0wxNyA0LjQxTDE1LjM3IDYuMDRDMTYuMTIgNi41NSAxNi43NCA3LjIyIDE3LjE5IDhaTTEwIDE2SDE0VjE0SDEwVjE2Wk0xMCAxMkgxNFYxMEgxMFYxMloiIGZpbGw9IiM2MTYxNjEiLz4KICAgICAgICA8cGF0aCBkPSJNMjIgMTguNUMyMiAyMC40MzMgMjAuNDMzIDIyIDE4LjUgMjJDMTYuNTY3IDIyIDE1IDIwLjQzMyAxNSAxOC41QzE1IDE2LjU2NyAxNi41NjcgMTUgMTguNSAxNUMyMC40MzMgMTUgMjIgMTYuNTY3IDIyIDE4LjVaIiBmaWxsPSIjNjE2MTYxIi8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yMCA4aC0yLjgxYy0uNDUtLjc4LTEuMDctMS40NS0xLjgyLTEuOTZMMTcgNC40MSAxNS41OSAzbC0yLjE3IDIuMTdDMTIuOTYgNS4wNiAxMi40OSA1IDEyIDVjLS40OSAwLS45Ni4wNi0xLjQxLjE3TDguNDEgMyA3IDQuNDFsMS42MiAxLjYzQzcuODggNi41NSA3LjI2IDcuMjIgNi44MSA4SDR2MmgyLjA5Yy0uMDUuMzMtLjA5LjY2LS4wOSAxdjFINHYyaDJ2MWMwIC4zNC4wNC42Ny4wOSAxSDR2MmgyLjgxYzEuMDQgMS43OSAyLjk3IDMgNS4xOSAzczQuMTUtMS4yMSA1LjE5LTNIMjB2LTJoLTIuMDljLjA1LS4zMy4wOS0uNjYuMDktMXYtMWgydi0yaC0ydi0xYzAtLjM0LS4wNC0uNjctLjA5LTFIMjBWOHptLTYgOGgtNHYtMmg0djJ6bTAtNGgtNHYtMmg0djJ6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-build: url(data:image/svg+xml;base64,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);
  --jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik05IDE2LjE3TDQuODMgMTJsLTEuNDIgMS40MUw5IDE5IDIxIDdsLTEuNDEtMS40MXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBzaGFwZS1yZW5kZXJpbmc9Imdlb21ldHJpY1ByZWNpc2lvbiI+CiAgICA8cGF0aCBkPSJNNi41OSwzLjQxTDIsOEw2LjU5LDEyLjZMOCwxMS4xOEw0LjgyLDhMOCw0LjgyTDYuNTksMy40MU0xMi40MSwzLjQxTDExLDQuODJMMTQuMTgsOEwxMSwxMS4xOEwxMi40MSwxMi42TDE3LDhMMTIuNDEsMy40MU0yMS41OSwxMS41OUwxMy41LDE5LjY4TDkuODMsMTZMOC40MiwxNy40MUwxMy41LDIyLjVMMjMsMTNMMjEuNTksMTEuNTlaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTExLjQgMTguNkw2LjggMTRMMTEuNCA5LjRMMTAgOEw0IDE0TDEwIDIwTDExLjQgMTguNlpNMTYuNiAxOC42TDIxLjIgMTRMMTYuNiA5LjRMMTggOEwyNCAxNEwxOCAyMEwxNi42IDE4LjZWMTguNloiLz4KCTwvZz4KPC9zdmc+Cg==);
  --jp-icon-collapse-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNNiAxM3YyaDh2LTJ6IiAvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1jb25zb2xlLWljb24tYmFja2dyb3VuZC1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtY29uc29sZS1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIj4KICAgIDxwYXRoIGQ9Ik0xMDUgMTI3LjNoNDB2MTIuOGgtNDB6TTUxLjEgNzdMNzQgOTkuOWwtMjMuMyAyMy4zIDEwLjUgMTAuNSAyMy4zLTIzLjNMOTUgOTkuOSA4NC41IDg5LjQgNjEuNiA2Ni41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copyright: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGVuYWJsZS1iYWNrZ3JvdW5kPSJuZXcgMCAwIDI0IDI0IiBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCI+CiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0xMS44OCw5LjE0YzEuMjgsMC4wNiwxLjYxLDEuMTUsMS42MywxLjY2aDEuNzljLTAuMDgtMS45OC0xLjQ5LTMuMTktMy40NS0zLjE5QzkuNjQsNy42MSw4LDksOCwxMi4xNCBjMCwxLjk0LDAuOTMsNC4yNCwzLjg0LDQuMjRjMi4yMiwwLDMuNDEtMS42NSwzLjQ0LTIuOTVoLTEuNzljLTAuMDMsMC41OS0wLjQ1LDEuMzgtMS42MywxLjQ0QzEwLjU1LDE0LjgzLDEwLDEzLjgxLDEwLDEyLjE0IEMxMCw5LjI1LDExLjI4LDkuMTYsMTEuODgsOS4xNHogTTEyLDJDNi40OCwyLDIsNi40OCwyLDEyczQuNDgsMTAsMTAsMTBzMTAtNC40OCwxMC0xMFMxNy41MiwyLDEyLDJ6IE0xMiwyMGMtNC40MSwwLTgtMy41OS04LTggczMuNTktOCw4LThzOCwzLjU5LDgsOFMxNi40MSwyMCwxMiwyMHoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-delete: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2cHgiIGhlaWdodD0iMTZweCI+CiAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIiAvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjI2MjYyIiBkPSJNNiAxOWMwIDEuMS45IDIgMiAyaDhjMS4xIDAgMi0uOSAyLTJWN0g2djEyek0xOSA0aC0zLjVsLTEtMWgtNWwtMSAxSDV2MmgxNFY0eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-duplicate: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTIuNzk5OTggMC44NzVIOC44OTU4MkM5LjIwMDYxIDAuODc1IDkuNDQ5OTggMS4xMzkxNCA5LjQ0OTk4IDEuNDYxOThDOS40NDk5OCAxLjc4NDgyIDkuMjAwNjEgMi4wNDg5NiA4Ljg5NTgyIDIuMDQ4OTZIMy4zNTQxNUMzLjA0OTM2IDIuMDQ4OTYgMi43OTk5OCAyLjMxMzEgMi43OTk5OCAyLjYzNTk0VjkuNjc5NjlDMi43OTk5OCAxMC4wMDI1IDIuNTUwNjEgMTAuMjY2NyAyLjI0NTgyIDEwLjI2NjdDMS45NDEwMyAxMC4yNjY3IDEuNjkxNjUgMTAuMDAyNSAxLjY5MTY1IDkuNjc5NjlWMi4wNDg5NkMxLjY5MTY1IDEuNDAzMjggMi4xOTA0IDAuODc1IDIuNzk5OTggMC44NzVaTTUuMzY2NjUgMTEuOVY0LjU1SDExLjA4MzNWMTEuOUg1LjM2NjY1Wk00LjE0MTY1IDQuMTQxNjdDNC4xNDE2NSAzLjY5MDYzIDQuNTA3MjggMy4zMjUgNC45NTgzMiAzLjMyNUgxMS40OTE3QzExLjk0MjcgMy4zMjUgMTIuMzA4MyAzLjY5MDYzIDEyLjMwODMgNC4xNDE2N1YxMi4zMDgzQzEyLjMwODMgMTIuNzU5NCAxMS45NDI3IDEzLjEyNSAxMS40OTE3IDEzLjEyNUg0Ljk1ODMyQzQuNTA3MjggMTMuMTI1IDQuMTQxNjUgMTIuNzU5NCA0LjE0MTY1IDEyLjMwODNWNC4xNDE2N1oiIGZpbGw9IiM2MTYxNjEiLz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNOS40MzU3NCA4LjI2NTA3SDguMzY0MzFWOS4zMzY1QzguMzY0MzEgOS40NTQzNSA4LjI2Nzg4IDkuNTUwNzggOC4xNTAwMiA5LjU1MDc4QzguMDMyMTcgOS41NTA3OCA3LjkzNTc0IDkuNDU0MzUgNy45MzU3NCA5LjMzNjVWOC4yNjUwN0g2Ljg2NDMxQzYuNzQ2NDUgOC4yNjUwNyA2LjY1MDAyIDguMTY4NjQgNi42NTAwMiA4LjA1MDc4QzYuNjUwMDIgNy45MzI5MiA2Ljc0NjQ1IDcuODM2NSA2Ljg2NDMxIDcuODM2NUg3LjkzNTc0VjYuNzY1MDdDNy45MzU3NCA2LjY0NzIxIDguMDMyMTcgNi41NTA3OCA4LjE1MDAyIDYuNTUwNzhDOC4yNjc4OCA2LjU1MDc4IDguMzY0MzEgNi42NDcyMSA4LjM2NDMxIDYuNzY1MDdWNy44MzY1SDkuNDM1NzRDOS41NTM2IDcuODM2NSA5LjY1MDAyIDcuOTMyOTIgOS42NTAwMiA4LjA1MDc4QzkuNjUwMDIgOC4xNjg2NCA5LjU1MzYgOC4yNjUwNyA5LjQzNTc0IDguMjY1MDdaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC41Ii8+Cjwvc3ZnPgo=);
  --jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-error: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj48Y2lyY2xlIGN4PSIxMiIgY3k9IjE5IiByPSIyIi8+PHBhdGggZD0iTTEwIDNoNHYxMmgtNHoiLz48L2c+CjxwYXRoIGZpbGw9Im5vbmUiIGQ9Ik0wIDBoMjR2MjRIMHoiLz4KPC9zdmc+Cg==);
  --jp-icon-expand-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNMTEgMTBIOXYzSDZ2MmgzdjNoMnYtM2gzdi0yaC0zeiIgLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-filter-dot: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWRvdCIgZmlsbD0iI0ZGRiI+CiAgICA8Y2lyY2xlIGN4PSIxOCIgY3k9IjE3IiByPSIzIj48L2NpcmNsZT4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-filter: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-folder-favorite: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgwVjB6IiBmaWxsPSJub25lIi8+PHBhdGggY2xhc3M9ImpwLWljb24zIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzYxNjE2MSIgZD0iTTIwIDZoLThsLTItMkg0Yy0xLjEgMC0yIC45LTIgMnYxMmMwIDEuMS45IDIgMiAyaDE2YzEuMSAwIDItLjkgMi0yVjhjMC0xLjEtLjktMi0yLTJ6bS0yLjA2IDExTDE1IDE1LjI4IDEyLjA2IDE3bC43OC0zLjMzLTIuNTktMi4yNCAzLjQxLS4yOUwxNSA4bDEuMzQgMy4xNCAzLjQxLjI5LTIuNTkgMi4yNC43OCAzLjMzeiIvPgo8L3N2Zz4K);
  --jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-home: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPjxwYXRoIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xMCAyMHYtNmg0djZoNXYtOGgzTDEyIDMgMiAxMmgzdjh6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==);
  --jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-info: url(data:image/svg+xml;base64,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);
  --jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaW5zcGVjdG9yLWljb24tY29sb3IganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K);
  --jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtanNvbi1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0Y5QTgyNSI+CiAgICA8cGF0aCBkPSJNMjAuMiAxMS44Yy0xLjYgMC0xLjcuNS0xLjcgMSAwIC40LjEuOS4xIDEuMy4xLjUuMS45LjEgMS4zIDAgMS43LTEuNCAyLjMtMy41IDIuM2gtLjl2LTEuOWguNWMxLjEgMCAxLjQgMCAxLjQtLjggMC0uMyAwLS42LS4xLTEgMC0uNC0uMS0uOC0uMS0xLjIgMC0xLjMgMC0xLjggMS4zLTItMS4zLS4yLTEuMy0uNy0xLjMtMiAwLS40LjEtLjguMS0xLjIuMS0uNC4xLS43LjEtMSAwLS44LS40LS43LTEuNC0uOGgtLjVWNC4xaC45YzIuMiAwIDMuNS43IDMuNSAyLjMgMCAuNC0uMS45LS4xIDEuMy0uMS41LS4xLjktLjEgMS4zIDAgLjUuMiAxIDEuNyAxdjEuOHpNMS44IDEwLjFjMS42IDAgMS43LS41IDEuNy0xIDAtLjQtLjEtLjktLjEtMS4zLS4xLS41LS4xLS45LS4xLTEuMyAwLTEuNiAxLjQtMi4zIDMuNS0yLjNoLjl2MS45aC0uNWMtMSAwLTEuNCAwLTEuNC44IDAgLjMgMCAuNi4xIDEgMCAuMi4xLjYuMSAxIDAgMS4zIDAgMS44LTEuMyAyQzYgMTEuMiA2IDExLjcgNiAxM2MwIC40LS4xLjgtLjEgMS4yLS4xLjMtLjEuNy0uMSAxIDAgLjguMy44IDEuNC44aC41djEuOWgtLjljLTIuMSAwLTMuNS0uNi0zLjUtMi4zIDAtLjQuMS0uOS4xLTEuMy4xLS41LjEtLjkuMS0xLjMgMC0uNS0uMi0xLTEuNy0xdi0xLjl6Ii8+CiAgICA8Y2lyY2xlIGN4PSIxMSIgY3k9IjEzLjgiIHI9IjIuMSIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSI4LjIiIHI9IjIuMSIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-julia: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDMyNSAzMDAiPgogIDxnIGNsYXNzPSJqcC1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjY2IzYzMzIj4KICAgIDxwYXRoIGQ9Ik0gMTUwLjg5ODQzOCAyMjUgQyAxNTAuODk4NDM4IDI2Ni40MjE4NzUgMTE3LjMyMDMxMiAzMDAgNzUuODk4NDM4IDMwMCBDIDM0LjQ3NjU2MiAzMDAgMC44OTg0MzggMjY2LjQyMTg3NSAwLjg5ODQzOCAyMjUgQyAwLjg5ODQzOCAxODMuNTc4MTI1IDM0LjQ3NjU2MiAxNTAgNzUuODk4NDM4IDE1MCBDIDExNy4zMjAzMTIgMTUwIDE1MC44OTg0MzggMTgzLjU3ODEyNSAxNTAuODk4NDM4IDIyNSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzM4OTgyNiI+CiAgICA8cGF0aCBkPSJNIDIzNy41IDc1IEMgMjM3LjUgMTE2LjQyMTg3NSAyMDMuOTIxODc1IDE1MCAxNjIuNSAxNTAgQyAxMjEuMDc4MTI1IDE1MCA4Ny41IDExNi40MjE4NzUgODcuNSA3NSBDIDg3LjUgMzMuNTc4MTI1IDEyMS4wNzgxMjUgMCAxNjIuNSAwIEMgMjAzLjkyMTg3NSAwIDIzNy41IDMzLjU3ODEyNSAyMzcuNSA3NSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzk1NThiMiI+CiAgICA8cGF0aCBkPSJNIDMyNC4xMDE1NjIgMjI1IEMgMzI0LjEwMTU2MiAyNjYuNDIxODc1IDI5MC41MjM0MzggMzAwIDI0OS4xMDE1NjIgMzAwIEMgMjA3LjY3OTY4OCAzMDAgMTc0LjEwMTU2MiAyNjYuNDIxODc1IDE3NC4xMDE1NjIgMjI1IEMgMTc0LjEwMTU2MiAxODMuNTc4MTI1IDIwNy42Nzk2ODggMTUwIDI0OS4xMDE1NjIgMTUwIEMgMjkwLjUyMzQzOCAxNTAgMzI0LjEwMTU2MiAxODMuNTc4MTI1IDMyNC4xMDE1NjIgMjI1Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgPGcgY2xhc3M9ImpwLWp1cHl0ZXItaWNvbi1jb2xvciIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-jupyter: url(data:image/svg+xml;base64,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);
  --jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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);
  --jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K);
  --jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==);
  --jp-icon-launch: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMzIgMzIiIHdpZHRoPSIzMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yNiwyOEg2YTIuMDAyNywyLjAwMjcsMCwwLDEtMi0yVjZBMi4wMDI3LDIuMDAyNywwLDAsMSw2LDRIMTZWNkg2VjI2SDI2VjE2aDJWMjZBMi4wMDI3LDIuMDAyNywwLDAsMSwyNiwyOFoiLz4KICAgIDxwb2x5Z29uIHBvaW50cz0iMjAgMiAyMCA0IDI2LjU4NiA0IDE4IDEyLjU4NiAxOS40MTQgMTQgMjggNS40MTQgMjggMTIgMzAgMTIgMzAgMiAyMCAyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==);
  --jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4K);
  --jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-move-down: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMTIuNDcxIDcuNTI4OTlDMTIuNzYzMiA3LjIzNjg0IDEyLjc2MzIgNi43NjMxNiAxMi40NzEgNi40NzEwMVY2LjQ3MTAxQzEyLjE3OSA2LjE3OTA1IDExLjcwNTcgNi4xNzg4NCAxMS40MTM1IDYuNDcwNTRMNy43NSAxMC4xMjc1VjEuNzVDNy43NSAxLjMzNTc5IDcuNDE0MjEgMSA3IDFWMUM2LjU4NTc5IDEgNi4yNSAxLjMzNTc5IDYuMjUgMS43NVYxMC4xMjc1TDIuNTk3MjYgNi40NjgyMkMyLjMwMzM4IDYuMTczODEgMS44MjY0MSA2LjE3MzU5IDEuNTMyMjYgNi40Njc3NFY2LjQ2Nzc0QzEuMjM4MyA2Ljc2MTcgMS4yMzgzIDcuMjM4MyAxLjUzMjI2IDcuNTMyMjZMNi4yOTI4OSAxMi4yOTI5QzYuNjgzNDIgMTIuNjgzNCA3LjMxNjU4IDEyLjY4MzQgNy43MDcxMSAxMi4yOTI5TDEyLjQ3MSA3LjUyODk5WiIgZmlsbD0iIzYxNjE2MSIvPgo8L3N2Zz4K);
  --jp-icon-move-up: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMS41Mjg5OSA2LjQ3MTAxQzEuMjM2ODQgNi43NjMxNiAxLjIzNjg0IDcuMjM2ODQgMS41Mjg5OSA3LjUyODk5VjcuNTI4OTlDMS44MjA5NSA3LjgyMDk1IDIuMjk0MjYgNy44MjExNiAyLjU4NjQ5IDcuNTI5NDZMNi4yNSAzLjg3MjVWMTIuMjVDNi4yNSAxMi42NjQyIDYuNTg1NzkgMTMgNyAxM1YxM0M3LjQxNDIxIDEzIDcuNzUgMTIuNjY0MiA3Ljc1IDEyLjI1VjMuODcyNUwxMS40MDI3IDcuNTMxNzhDMTEuNjk2NiA3LjgyNjE5IDEyLjE3MzYgNy44MjY0MSAxMi40Njc3IDcuNTMyMjZWNy41MzIyNkMxMi43NjE3IDcuMjM4MyAxMi43NjE3IDYuNzYxNyAxMi40Njc3IDYuNDY3NzRMNy43MDcxMSAxLjcwNzExQzcuMzE2NTggMS4zMTY1OCA2LjY4MzQyIDEuMzE2NTggNi4yOTI4OSAxLjcwNzExTDEuNTI4OTkgNi40NzEwMVoiIGZpbGw9IiM2MTYxNjEiLz4KPC9zdmc+Cg==);
  --jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=);
  --jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtbm90ZWJvb2staWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-numbering: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTQgMTlINlYxOS41SDVWMjAuNUg2VjIxSDRWMjJIN1YxOEg0VjE5Wk01IDEwSDZWNkg0VjdINVYxMFpNNCAxM0g1LjhMNCAxNS4xVjE2SDdWMTVINS4yTDcgMTIuOVYxMkg0VjEzWk05IDdWOUgyM1Y3SDlaTTkgMjFIMjNWMTlIOVYyMVpNOSAxNUgyM1YxM0g5VjE1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-offline-bolt: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDIuMDJjLTUuNTEgMC05Ljk4IDQuNDctOS45OCA5Ljk4czQuNDcgOS45OCA5Ljk4IDkuOTggOS45OC00LjQ3IDkuOTgtOS45OFMxNy41MSAyLjAyIDEyIDIuMDJ6TTExLjQ4IDIwdi02LjI2SDhMMTMgNHY2LjI2aDMuMzVMMTEuNDggMjB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-pdf: url(data:image/svg+xml;base64,PHN2ZwogICB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyMiAyMiIgd2lkdGg9IjE2Ij4KICAgIDxwYXRoIHRyYW5zZm9ybT0icm90YXRlKDQ1KSIgY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0ZGMkEyQSIKICAgICAgIGQ9Im0gMjIuMzQ0MzY5LC0zLjAxNjM2NDIgaCA1LjYzODYwNCB2IDEuNTc5MjQzMyBoIC0zLjU0OTIyNyB2IDEuNTA4NjkyOTkgaCAzLjMzNzU3NiBWIDEuNjUwODE1NCBoIC0zLjMzNzU3NiB2IDMuNDM1MjYxMyBoIC0yLjA4OTM3NyB6IG0gLTcuMTM2NDQ0LDEuNTc5MjQzMyB2IDQuOTQzOTU0MyBoIDAuNzQ4OTIgcSAxLjI4MDc2MSwwIDEuOTUzNzAzLC0wLjYzNDk1MzUgMC42NzgzNjksLTAuNjM0OTUzNSAwLjY3ODM2OSwtMS44NDUxNjQxIDAsLTEuMjA0NzgzNTUgLTAuNjcyOTQyLC0xLjgzNDMxMDExIC0wLjY3Mjk0MiwtMC42Mjk1MjY1OSAtMS45NTkxMywtMC42Mjk1MjY1OSB6IG0gLTIuMDg5Mzc3LC0xLjU3OTI0MzMgaCAyLjIwMzM0MyBxIDEuODQ1MTY0LDAgMi43NDYwMzksMC4yNjU5MjA3IDAuOTA2MzAxLDAuMjYwNDkzNyAxLjU1MjEwOCwwLjg5MDAyMDMgMC41Njk4MywwLjU0ODEyMjMgMC44NDY2MDUsMS4yNjQ0ODAwNiAwLjI3Njc3NCwwLjcxNjM1NzgxIDAuMjc2Nzc0LDEuNjIyNjU4OTQgMCwwLjkxNzE1NTEgLTAuMjc2Nzc0LDEuNjM4OTM5OSAtMC4yNzY3NzUsMC43MTYzNTc4IC0wLjg0NjYwNSwxLjI2NDQ4IC0wLjY1MTIzNCwwLjYyOTUyNjYgLTEuNTYyOTYyLDAuODk1NDQ3MyAtMC45MTE3MjgsMC4yNjA0OTM3IC0yLjczNTE4NSwwLjI2MDQ5MzcgaCAtMi4yMDMzNDMgeiBtIC04LjE0NTg1NjUsMCBoIDMuNDY3ODIzIHEgMS41NDY2ODE2LDAgMi4zNzE1Nzg1LDAuNjg5MjIzIDAuODMwMzI0LDAuNjgzNzk2MSAwLjgzMDMyNCwxLjk1MzcwMzE0IDAsMS4yNzUzMzM5NyAtMC44MzAzMjQsMS45NjQ1NTcwNiBRIDkuOTg3MTk2MSwyLjI3NDkxNSA4LjQ0MDUxNDUsMi4yNzQ5MTUgSCA3LjA2MjA2ODQgViA1LjA4NjA3NjcgSCA0Ljk3MjY5MTUgWiBtIDIuMDg5Mzc2OSwxLjUxNDExOTkgdiAyLjI2MzAzOTQzIGggMS4xNTU5NDEgcSAwLjYwNzgxODgsMCAwLjkzODg2MjksLTAuMjkzMDU1NDcgMC4zMzEwNDQxLC0wLjI5ODQ4MjQxIDAuMzMxMDQ0MSwtMC44NDExNzc3MiAwLC0wLjU0MjY5NTMxIC0wLjMzMTA0NDEsLTAuODM1NzUwNzQgLTAuMzMxMDQ0MSwtMC4yOTMwNTU1IC0wLjkzODg2MjksLTAuMjkzMDU1NSB6IgovPgo8L3N2Zz4K);
  --jp-icon-python: url(data:image/svg+xml;base64,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);
  --jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==);
  --jp-icon-react: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMTUwIDE1MCA1NDEuOSAyOTUuMyI+CiAgPGcgY2xhc3M9ImpwLWljb24tYnJhbmQyIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzYxREFGQiI+CiAgICA8cGF0aCBkPSJNNjY2LjMgMjk2LjVjMC0zMi41LTQwLjctNjMuMy0xMDMuMS04Mi40IDE0LjQtNjMuNiA4LTExNC4yLTIwLjItMTMwLjQtNi41LTMuOC0xNC4xLTUuNi0yMi40LTUuNnYyMi4zYzQuNiAwIDguMy45IDExLjQgMi42IDEzLjYgNy44IDE5LjUgMzcuNSAxNC45IDc1LjctMS4xIDkuNC0yLjkgMTkuMy01LjEgMjkuNC0xOS42LTQuOC00MS04LjUtNjMuNS0xMC45LTEzLjUtMTguNS0yNy41LTM1LjMtNDEuNi01MCAzMi42LTMwLjMgNjMuMi00Ni45IDg0LTQ2LjlWNzhjLTI3LjUgMC02My41IDE5LjYtOTkuOSA1My42LTM2LjQtMzMuOC03Mi40LTUzLjItOTkuOS01My4ydjIyLjNjMjAuNyAwIDUxLjQgMTYuNSA4NCA0Ni42LTE0IDE0LjctMjggMzEuNC00MS4zIDQ5LjktMjIuNiAyLjQtNDQgNi4xLTYzLjYgMTEtMi4zLTEwLTQtMTkuNy01LjItMjktNC43LTM4LjIgMS4xLTY3LjkgMTQuNi03NS44IDMtMS44IDYuOS0yLjYgMTEuNS0yLjZWNzguNWMtOC40IDAtMTYgMS44LTIyLjYgNS42LTI4LjEgMTYuMi0zNC40IDY2LjctMTkuOSAxMzAuMS02Mi4yIDE5LjItMTAyLjcgNDkuOS0xMDIuNyA4Mi4zIDAgMzIuNSA0MC43IDYzLjMgMTAzLjEgODIuNC0xNC40IDYzLjYtOCAxMTQuMiAyMC4yIDEzMC40IDYuNSAzLjggMTQuMSA1LjYgMjIuNSA1LjYgMjcuNSAwIDYzLjUtMTkuNiA5OS45LTUzLjYgMzYuNCAzMy44IDcyLjQgNTMuMiA5OS45IDUzLjIgOC40IDAgMTYtMS44IDIyLjYtNS42IDI4LjEtMTYuMiAzNC40LTY2LjcgMTkuOS0xMzAuMSA2Mi0xOS4xIDEwMi41LTQ5LjkgMTAyLjUtODIuM3ptLTEzMC4yLTY2LjdjLTMuNyAxMi45LTguMyAyNi4yLTEzLjUgMzkuNS00LjEtOC04LjQtMTYtMTMuMS0yNC00LjYtOC05LjUtMTUuOC0xNC40LTIzLjQgMTQuMiAyLjEgMjcuOSA0LjcgNDEgNy45em0tNDUuOCAxMDYuNWMtNy44IDEzLjUtMTUuOCAyNi4zLTI0LjEgMzguMi0xNC45IDEuMy0zMCAyLTQ1LjIgMi0xNS4xIDAtMzAuMi0uNy00NS0xLjktOC4zLTExLjktMTYuNC0yNC42LTI0LjItMzgtNy42LTEzLjEtMTQuNS0yNi40LTIwLjgtMzkuOCA2LjItMTMuNCAxMy4yLTI2LjggMjAuNy0zOS45IDcuOC0xMy41IDE1LjgtMjYuMyAyNC4xLTM4LjIgMTQuOS0xLjMgMzAtMiA0NS4yLTIgMTUuMSAwIDMwLjIuNyA0NSAxLjkgOC4zIDExLjkgMTYuNCAyNC42IDI0LjIgMzggNy42IDEzLjEgMTQuNSAyNi40IDIwLjggMzkuOC02LjMgMTMuNC0xMy4yIDI2LjgtMjAuNyAzOS45em0zMi4zLTEzYzUuNCAxMy40IDEwIDI2LjggMTMuOCAzOS44LTEzLjEgMy4yLTI2LjkgNS45LTQxLjIgOCA0LjktNy43IDkuOC0xNS42IDE0LjQtMjMuNyA0LjYtOCA4LjktMTYuMSAxMy0yNC4xek00MjEuMiA0MzBjLTkuMy05LjYtMTguNi0yMC4zLTI3LjgtMzIgOSAuNCAxOC4yLjcgMjcuNS43IDkuNCAwIDE4LjctLjIgMjcuOC0uNy05IDExLjctMTguMyAyMi40LTI3LjUgMzJ6bS03NC40LTU4LjljLTE0LjItMi4xLTI3LjktNC43LTQxLTcuOSAzLjctMTIuOSA4LjMtMjYuMiAxMy41LTM5LjUgNC4xIDggOC40IDE2IDEzLjEgMjQgNC43IDggOS41IDE1LjggMTQuNCAyMy40ek00MjAuNyAxNjNjOS4zIDkuNiAxOC42IDIwLjMgMjcuOCAzMi05LS40LTE4LjItLjctMjcuNS0uNy05LjQgMC0xOC43LjItMjcuOC43IDktMTEuNyAxOC4zLTIyLjQgMjcuNS0zMnptLTc0IDU4LjljLTQuOSA3LjctOS44IDE1LjYtMTQuNCAyMy43LTQuNiA4LTguOSAxNi0xMyAyNC01LjQtMTMuNC0xMC0yNi44LTEzLjgtMzkuOCAxMy4xLTMuMSAyNi45LTUuOCA0MS4yLTcuOXptLTkwLjUgMTI1LjJjLTM1LjQtMTUuMS01OC4zLTM0LjktNTguMy01MC42IDAtMTUuNyAyMi45LTM1LjYgNTguMy01MC42IDguNi0zLjcgMTgtNyAyNy43LTEwLjEgNS43IDE5LjYgMTMuMiA0MCAyMi41IDYwLjktOS4yIDIwLjgtMTYuNiA0MS4xLTIyLjIgNjAuNi05LjktMy4xLTE5LjMtNi41LTI4LTEwLjJ6TTMxMCA0OTBjLTEzLjYtNy44LTE5LjUtMzcuNS0xNC45LTc1LjcgMS4xLTkuNCAyLjktMTkuMyA1LjEtMjkuNCAxOS42IDQuOCA0MSA4LjUgNjMuNSAxMC45IDEzLjUgMTguNSAyNy41IDM1LjMgNDEuNiA1MC0zMi42IDMwLjMtNjMuMiA0Ni45LTg0IDQ2LjktNC41LS4xLTguMy0xLTExLjMtMi43em0yMzcuMi03Ni4yYzQuNyAzOC4yLTEuMSA2Ny45LTE0LjYgNzUuOC0zIDEuOC02LjkgMi42LTExLjUgMi42LTIwLjcgMC01MS40LTE2LjUtODQtNDYuNiAxNC0xNC43IDI4LTMxLjQgNDEuMy00OS45IDIyLjYtMi40IDQ0LTYuMSA2My42LTExIDIuMyAxMC4xIDQuMSAxOS44IDUuMiAyOS4xem0zOC41LTY2LjdjLTguNiAzLjctMTggNy0yNy43IDEwLjEtNS43LTE5LjYtMTMuMi00MC0yMi41LTYwLjkgOS4yLTIwLjggMTYuNi00MS4xIDIyLjItNjAuNiA5LjkgMy4xIDE5LjMgNi41IDI4LjEgMTAuMiAzNS40IDE1LjEgNTguMyAzNC45IDU4LjMgNTAuNi0uMSAxNS43LTIzIDM1LjYtNTguNCA1MC42ek0zMjAuOCA3OC40eiIvPgogICAgPGNpcmNsZSBjeD0iNDIwLjkiIGN5PSIyOTYuNSIgcj0iNDUuNyIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-redo: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZD0iTTE4LjQgMTAuNkMxNi41NSA4Ljk5IDE0LjE1IDggMTEuNSA4Yy00LjY1IDAtOC41OCAzLjAzLTkuOTYgNy4yMkwzLjkgMTZjMS4wNS0zLjE5IDQuMDUtNS41IDcuNi01LjUgMS45NSAwIDMuNzMuNzIgNS4xMiAxLjg4TDEzIDE2aDlWN2wtMy42IDMuNnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==);
  --jp-icon-share: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTSAxOCAyIEMgMTYuMzU0OTkgMiAxNSAzLjM1NDk5MDQgMTUgNSBDIDE1IDUuMTkwOTUyOSAxNS4wMjE3OTEgNS4zNzcxMjI0IDE1LjA1NjY0MSA1LjU1ODU5MzggTCA3LjkyMTg3NSA5LjcyMDcwMzEgQyA3LjM5ODUzOTkgOS4yNzc4NTM5IDYuNzMyMDc3MSA5IDYgOSBDIDQuMzU0OTkwNCA5IDMgMTAuMzU0OTkgMyAxMiBDIDMgMTMuNjQ1MDEgNC4zNTQ5OTA0IDE1IDYgMTUgQyA2LjczMjA3NzEgMTUgNy4zOTg1Mzk5IDE0LjcyMjE0NiA3LjkyMTg3NSAxNC4yNzkyOTcgTCAxNS4wNTY2NDEgMTguNDM5NDUzIEMgMTUuMDIxNTU1IDE4LjYyMTUxNCAxNSAxOC44MDgzODYgMTUgMTkgQyAxNSAyMC42NDUwMSAxNi4zNTQ5OSAyMiAxOCAyMiBDIDE5LjY0NTAxIDIyIDIxIDIwLjY0NTAxIDIxIDE5IEMgMjEgMTcuMzU0OTkgMTkuNjQ1MDEgMTYgMTggMTYgQyAxNy4yNjc0OCAxNiAxNi42MDE1OTMgMTYuMjc5MzI4IDE2LjA3ODEyNSAxNi43MjI2NTYgTCA4Ljk0MzM1OTQgMTIuNTU4NTk0IEMgOC45NzgyMDk1IDEyLjM3NzEyMiA5IDEyLjE5MDk1MyA5IDEyIEMgOSAxMS44MDkwNDcgOC45NzgyMDk1IDExLjYyMjg3OCA4Ljk0MzM1OTQgMTEuNDQxNDA2IEwgMTYuMDc4MTI1IDcuMjc5Mjk2OSBDIDE2LjYwMTQ2IDcuNzIyMTQ2MSAxNy4yNjc5MjMgOCAxOCA4IEMgMTkuNjQ1MDEgOCAyMSA2LjY0NTAwOTYgMjEgNSBDIDIxIDMuMzU0OTkwNCAxOS42NDUwMSAyIDE4IDIgeiBNIDE4IDQgQyAxOC41NjQxMjkgNCAxOSA0LjQzNTg3MDYgMTkgNSBDIDE5IDUuNTY0MTI5NCAxOC41NjQxMjkgNiAxOCA2IEMgMTcuNDM1ODcxIDYgMTcgNS41NjQxMjk0IDE3IDUgQyAxNyA0LjQzNTg3MDYgMTcuNDM1ODcxIDQgMTggNCB6IE0gNiAxMSBDIDYuNTY0MTI5NCAxMSA3IDExLjQzNTg3MSA3IDEyIEMgNyAxMi41NjQxMjkgNi41NjQxMjk0IDEzIDYgMTMgQyA1LjQzNTg3MDYgMTMgNSAxMi41NjQxMjkgNSAxMiBDIDUgMTEuNDM1ODcxIDUuNDM1ODcwNiAxMSA2IDExIHogTSAxOCAxOCBDIDE4LjU2NDEyOSAxOCAxOSAxOC40MzU4NzEgMTkgMTkgQyAxOSAxOS41NjQxMjkgMTguNTY0MTI5IDIwIDE4IDIwIEMgMTcuNDM1ODcxIDIwIDE3IDE5LjU2NDEyOSAxNyAxOSBDIDE3IDE4LjQzNTg3MSAxNy40MzU4NzEgMTggMTggMTggeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-table-rows: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMSw4SDNWNGgxOFY4eiBNMjEsMTBIM3Y0aDE4VjEweiBNMjEsMTZIM3Y0aDE4VjE2eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-tag: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjgiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCA0MyAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTI4LjgzMzIgMTIuMzM0TDMyLjk5OTggMTYuNTAwN0wzNy4xNjY1IDEyLjMzNEgyOC44MzMyWiIvPgoJCTxwYXRoIGQ9Ik0xNi4yMDk1IDIxLjYxMDRDMTUuNjg3MyAyMi4xMjk5IDE0Ljg0NDMgMjIuMTI5OSAxNC4zMjQ4IDIxLjYxMDRMNi45ODI5IDE0LjcyNDVDNi41NzI0IDE0LjMzOTQgNi4wODMxMyAxMy42MDk4IDYuMDQ3ODYgMTMuMDQ4MkM1Ljk1MzQ3IDExLjUyODggNi4wMjAwMiA4LjYxOTQ0IDYuMDY2MjEgNy4wNzY5NUM2LjA4MjgxIDYuNTE0NzcgNi41NTU0OCA2LjA0MzQ3IDcuMTE4MDQgNi4wMzA1NUM5LjA4ODYzIDUuOTg0NzMgMTMuMjYzOCA1LjkzNTc5IDEzLjY1MTggNi4zMjQyNUwyMS43MzY5IDEzLjYzOUMyMi4yNTYgMTQuMTU4NSAyMS43ODUxIDE1LjQ3MjQgMjEuMjYyIDE1Ljk5NDZMMTYuMjA5NSAyMS42MTA0Wk05Ljc3NTg1IDguMjY1QzkuMzM1NTEgNy44MjU2NiA4LjYyMzUxIDcuODI1NjYgOC4xODI4IDguMjY1QzcuNzQzNDYgOC43MDU3MSA3Ljc0MzQ2IDkuNDE3MzMgOC4xODI4IDkuODU2NjdDOC42MjM4MiAxMC4yOTY0IDkuMzM1ODIgMTAuMjk2NCA5Ljc3NTg1IDkuODU2NjdDMTAuMjE1NiA5LjQxNzMzIDEwLjIxNTYgOC43MDUzMyA5Ljc3NTg1IDguMjY1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-terminal: url(data:image/svg+xml;base64,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);
  --jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtdGV4dC1lZGl0b3ItaWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xNSAxNUgzdjJoMTJ2LTJ6bTAtOEgzdjJoMTJWN3pNMyAxM2gxOHYtMkgzdjJ6bTAgOGgxOHYtMkgzdjJ6TTMgM3YyaDE4VjNIM3oiLz4KPC9zdmc+Cg==);
  --jp-icon-toc: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik03LDVIMjFWN0g3VjVNNywxM1YxMUgyMVYxM0g3TTQsNC41QTEuNSwxLjUgMCAwLDEgNS41LDZBMS41LDEuNSAwIDAsMSA0LDcuNUExLjUsMS41IDAgMCwxIDIuNSw2QTEuNSwxLjUgMCAwLDEgNCw0LjVNNCwxMC41QTEuNSwxLjUgMCAwLDEgNS41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMy41QTEuNSwxLjUgMCAwLDEgMi41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMC41TTcsMTlWMTdIMjFWMTlIN000LDE2LjVBMS41LDEuNSAwIDAsMSA1LjUsMThBMS41LDEuNSAwIDAsMSA0LDE5LjVBMS41LDEuNSAwIDAsMSAyLjUsMThBMS41LDEuNSAwIDAsMSA0LDE2LjVaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tree-view: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMiAxMVYzaC03djNIOVYzSDJ2OGg3VjhoMnYxMGg0djNoN3YtOGgtN3YzaC0yVjhoMnYzeiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K);
  --jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-user: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE2IDdhNCA0IDAgMTEtOCAwIDQgNCAwIDAxOCAwek0xMiAxNGE3IDcgMCAwMC03IDdoMTRhNyA3IDAgMDAtNy03eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-users: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZlcnNpb249IjEuMSIgdmlld0JveD0iMCAwIDM2IDI0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogPGcgY2xhc3M9ImpwLWljb24zIiB0cmFuc2Zvcm09Im1hdHJpeCgxLjczMjcgMCAwIDEuNzMyNyAtMy42MjgyIC4wOTk1NzcpIiBmaWxsPSIjNjE2MTYxIj4KICA8cGF0aCB0cmFuc2Zvcm09Im1hdHJpeCgxLjUsMCwwLDEuNSwwLC02KSIgZD0ibTEyLjE4NiA3LjUwOThjLTEuMDUzNSAwLTEuOTc1NyAwLjU2NjUtMi40Nzg1IDEuNDEwMiAwLjc1MDYxIDAuMzEyNzcgMS4zOTc0IDAuODI2NDggMS44NzMgMS40NzI3aDMuNDg2M2MwLTEuNTkyLTEuMjg4OS0yLjg4MjgtMi44ODA5LTIuODgyOHoiLz4KICA8cGF0aCBkPSJtMjAuNDY1IDIuMzg5NWEyLjE4ODUgMi4xODg1IDAgMCAxLTIuMTg4NCAyLjE4ODUgMi4xODg1IDIuMTg4NSAwIDAgMS0yLjE4ODUtMi4xODg1IDIuMTg4NSAyLjE4ODUgMCAwIDEgMi4xODg1LTIuMTg4NSAyLjE4ODUgMi4xODg1IDAgMCAxIDIuMTg4NCAyLjE4ODV6Ii8+CiAgPHBhdGggdHJhbnNmb3JtPSJtYXRyaXgoMS41LDAsMCwxLjUsMCwtNikiIGQ9Im0zLjU4OTggOC40MjE5Yy0xLjExMjYgMC0yLjAxMzcgMC45MDExMS0yLjAxMzcgMi4wMTM3aDIuODE0NWMwLjI2Nzk3LTAuMzczMDkgMC41OTA3LTAuNzA0MzUgMC45NTg5OC0wLjk3ODUyLTAuMzQ0MzMtMC42MTY4OC0xLjAwMzEtMS4wMzUyLTEuNzU5OC0xLjAzNTJ6Ii8+CiAgPHBhdGggZD0ibTYuOTE1NCA0LjYyM2ExLjUyOTQgMS41Mjk0IDAgMCAxLTEuNTI5NCAxLjUyOTQgMS41Mjk0IDEuNTI5NCAwIDAgMS0xLjUyOTQtMS41Mjk0IDEuNTI5NCAxLjUyOTQgMCAwIDEgMS41Mjk0LTEuNTI5NCAxLjUyOTQgMS41Mjk0IDAgMCAxIDEuNTI5NCAxLjUyOTR6Ii8+CiAgPHBhdGggZD0ibTYuMTM1IDEzLjUzNWMwLTMuMjM5MiAyLjYyNTktNS44NjUgNS44NjUtNS44NjUgMy4yMzkyIDAgNS44NjUgMi42MjU5IDUuODY1IDUuODY1eiIvPgogIDxjaXJjbGUgY3g9IjEyIiBjeT0iMy43Njg1IiByPSIyLjk2ODUiLz4KIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-word: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KIDxnIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzQxNDE0MSI+CiAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiA8L2c+CiA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSguNDMgLjA0MDEpIiBmaWxsPSIjZmZmIj4KICA8cGF0aCBkPSJtNC4xNCA4Ljc2cTAuMDY4Mi0xLjg5IDIuNDItMS44OSAxLjE2IDAgMS42OCAwLjQyIDAuNTY3IDAuNDEgMC41NjcgMS4xNnYzLjQ3cTAgMC40NjIgMC41MTQgMC40NjIgMC4xMDMgMCAwLjItMC4wMjMxdjAuNzE0cS0wLjM5OSAwLjEwMy0wLjY1MSAwLjEwMy0wLjQ1MiAwLTAuNjkzLTAuMjItMC4yMzEtMC4yLTAuMjg0LTAuNjYyLTAuOTU2IDAuODcyLTIgMC44NzItMC45MDMgMC0xLjQ3LTAuNDcyLTAuNTI1LTAuNDcyLTAuNTI1LTEuMjYgMC0wLjI2MiAwLjA0NTItMC40NzIgMC4wNTY3LTAuMjIgMC4xMTYtMC4zNzggMC4wNjgyLTAuMTY4IDAuMjMxLTAuMzA0IDAuMTU4LTAuMTQ3IDAuMjYyLTAuMjQyIDAuMTE2LTAuMDkxNCAwLjM2OC0wLjE2OCAwLjI2Mi0wLjA5MTQgMC4zOTktMC4xMjYgMC4xMzYtMC4wNDUyIDAuNDcyLTAuMTAzIDAuMzM2LTAuMDU3OCAwLjUwNC0wLjA3OTggMC4xNTgtMC4wMjMxIDAuNTY3LTAuMDc5OCAwLjU1Ni0wLjA2ODIgMC43NzctMC4yMjEgMC4yMi0wLjE1MiAwLjIyLTAuNDQxdi0wLjI1MnEwLTAuNDMtMC4zNTctMC42NjItMC4zMzYtMC4yMzEtMC45NzYtMC4yMzEtMC42NjIgMC0wLjk5OCAwLjI2Mi0wLjMzNiAwLjI1Mi0wLjM5OSAwLjc5OHptMS44OSAzLjY4cTAuNzg4IDAgMS4yNi0wLjQxIDAuNTA0LTAuNDIgMC41MDQtMC45MDN2LTEuMDVxLTAuMjg0IDAuMTM2LTAuODYxIDAuMjMxLTAuNTY3IDAuMDkxNC0wLjk4NyAwLjE1OC0wLjQyIDAuMDY4Mi0wLjc2NiAwLjMyNi0wLjMzNiAwLjI1Mi0wLjMzNiAwLjcwNHQwLjMwNCAwLjcwNCAwLjg2MSAwLjI1MnoiIHN0cm9rZS13aWR0aD0iMS4wNSIvPgogIDxwYXRoIGQ9Im0xMCA0LjU2aDAuOTQ1djMuMTVxMC42NTEtMC45NzYgMS44OS0wLjk3NiAxLjE2IDAgMS44OSAwLjg0IDAuNjgyIDAuODQgMC42ODIgMi4zMSAwIDEuNDctMC43MDQgMi40Mi0wLjcwNCAwLjg4Mi0xLjg5IDAuODgyLTEuMjYgMC0xLjg5LTEuMDJ2MC43NjZoLTAuODV6bTIuNjIgMy4wNHEtMC43NDYgMC0xLjE2IDAuNjQtMC40NTIgMC42My0wLjQ1MiAxLjY4IDAgMS4wNSAwLjQ1MiAxLjY4dDEuMTYgMC42M3EwLjc3NyAwIDEuMjYtMC42MyAwLjQ5NC0wLjY0IDAuNDk0LTEuNjggMC0xLjA1LTAuNDcyLTEuNjgtMC40NjItMC42NC0xLjI2LTAuNjR6IiBzdHJva2Utd2lkdGg9IjEuMDUiLz4KICA8cGF0aCBkPSJtMi43MyAxNS44IDEzLjYgMC4wMDgxYzAuMDA2OSAwIDAtMi42IDAtMi42IDAtMC4wMDc4LTEuMTUgMC0xLjE1IDAtMC4wMDY5IDAtMC4wMDgzIDEuNS0wLjAwODMgMS41LTJlLTMgLTAuMDAxNC0xMS4zLTAuMDAxNC0xMS4zLTAuMDAxNGwtMC4wMDU5Mi0xLjVjMC0wLjAwNzgtMS4xNyAwLjAwMTMtMS4xNyAwLjAwMTN6IiBzdHJva2Utd2lkdGg9Ii45NzUiLz4KIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K);
}

/* Icon CSS class declarations */

.jp-AddAboveIcon {
  background-image: var(--jp-icon-add-above);
}

.jp-AddBelowIcon {
  background-image: var(--jp-icon-add-below);
}

.jp-AddIcon {
  background-image: var(--jp-icon-add);
}

.jp-BellIcon {
  background-image: var(--jp-icon-bell);
}

.jp-BugDotIcon {
  background-image: var(--jp-icon-bug-dot);
}

.jp-BugIcon {
  background-image: var(--jp-icon-bug);
}

.jp-BuildIcon {
  background-image: var(--jp-icon-build);
}

.jp-CaretDownEmptyIcon {
  background-image: var(--jp-icon-caret-down-empty);
}

.jp-CaretDownEmptyThinIcon {
  background-image: var(--jp-icon-caret-down-empty-thin);
}

.jp-CaretDownIcon {
  background-image: var(--jp-icon-caret-down);
}

.jp-CaretLeftIcon {
  background-image: var(--jp-icon-caret-left);
}

.jp-CaretRightIcon {
  background-image: var(--jp-icon-caret-right);
}

.jp-CaretUpEmptyThinIcon {
  background-image: var(--jp-icon-caret-up-empty-thin);
}

.jp-CaretUpIcon {
  background-image: var(--jp-icon-caret-up);
}

.jp-CaseSensitiveIcon {
  background-image: var(--jp-icon-case-sensitive);
}

.jp-CheckIcon {
  background-image: var(--jp-icon-check);
}

.jp-CircleEmptyIcon {
  background-image: var(--jp-icon-circle-empty);
}

.jp-CircleIcon {
  background-image: var(--jp-icon-circle);
}

.jp-ClearIcon {
  background-image: var(--jp-icon-clear);
}

.jp-CloseIcon {
  background-image: var(--jp-icon-close);
}

.jp-CodeCheckIcon {
  background-image: var(--jp-icon-code-check);
}

.jp-CodeIcon {
  background-image: var(--jp-icon-code);
}

.jp-CollapseAllIcon {
  background-image: var(--jp-icon-collapse-all);
}

.jp-ConsoleIcon {
  background-image: var(--jp-icon-console);
}

.jp-CopyIcon {
  background-image: var(--jp-icon-copy);
}

.jp-CopyrightIcon {
  background-image: var(--jp-icon-copyright);
}

.jp-CutIcon {
  background-image: var(--jp-icon-cut);
}

.jp-DeleteIcon {
  background-image: var(--jp-icon-delete);
}

.jp-DownloadIcon {
  background-image: var(--jp-icon-download);
}

.jp-DuplicateIcon {
  background-image: var(--jp-icon-duplicate);
}

.jp-EditIcon {
  background-image: var(--jp-icon-edit);
}

.jp-EllipsesIcon {
  background-image: var(--jp-icon-ellipses);
}

.jp-ErrorIcon {
  background-image: var(--jp-icon-error);
}

.jp-ExpandAllIcon {
  background-image: var(--jp-icon-expand-all);
}

.jp-ExtensionIcon {
  background-image: var(--jp-icon-extension);
}

.jp-FastForwardIcon {
  background-image: var(--jp-icon-fast-forward);
}

.jp-FileIcon {
  background-image: var(--jp-icon-file);
}

.jp-FileUploadIcon {
  background-image: var(--jp-icon-file-upload);
}

.jp-FilterDotIcon {
  background-image: var(--jp-icon-filter-dot);
}

.jp-FilterIcon {
  background-image: var(--jp-icon-filter);
}

.jp-FilterListIcon {
  background-image: var(--jp-icon-filter-list);
}

.jp-FolderFavoriteIcon {
  background-image: var(--jp-icon-folder-favorite);
}

.jp-FolderIcon {
  background-image: var(--jp-icon-folder);
}

.jp-HomeIcon {
  background-image: var(--jp-icon-home);
}

.jp-Html5Icon {
  background-image: var(--jp-icon-html5);
}

.jp-ImageIcon {
  background-image: var(--jp-icon-image);
}

.jp-InfoIcon {
  background-image: var(--jp-icon-info);
}

.jp-InspectorIcon {
  background-image: var(--jp-icon-inspector);
}

.jp-JsonIcon {
  background-image: var(--jp-icon-json);
}

.jp-JuliaIcon {
  background-image: var(--jp-icon-julia);
}

.jp-JupyterFaviconIcon {
  background-image: var(--jp-icon-jupyter-favicon);
}

.jp-JupyterIcon {
  background-image: var(--jp-icon-jupyter);
}

.jp-JupyterlabWordmarkIcon {
  background-image: var(--jp-icon-jupyterlab-wordmark);
}

.jp-KernelIcon {
  background-image: var(--jp-icon-kernel);
}

.jp-KeyboardIcon {
  background-image: var(--jp-icon-keyboard);
}

.jp-LaunchIcon {
  background-image: var(--jp-icon-launch);
}

.jp-LauncherIcon {
  background-image: var(--jp-icon-launcher);
}

.jp-LineFormIcon {
  background-image: var(--jp-icon-line-form);
}

.jp-LinkIcon {
  background-image: var(--jp-icon-link);
}

.jp-ListIcon {
  background-image: var(--jp-icon-list);
}

.jp-MarkdownIcon {
  background-image: var(--jp-icon-markdown);
}

.jp-MoveDownIcon {
  background-image: var(--jp-icon-move-down);
}

.jp-MoveUpIcon {
  background-image: var(--jp-icon-move-up);
}

.jp-NewFolderIcon {
  background-image: var(--jp-icon-new-folder);
}

.jp-NotTrustedIcon {
  background-image: var(--jp-icon-not-trusted);
}

.jp-NotebookIcon {
  background-image: var(--jp-icon-notebook);
}

.jp-NumberingIcon {
  background-image: var(--jp-icon-numbering);
}

.jp-OfflineBoltIcon {
  background-image: var(--jp-icon-offline-bolt);
}

.jp-PaletteIcon {
  background-image: var(--jp-icon-palette);
}

.jp-PasteIcon {
  background-image: var(--jp-icon-paste);
}

.jp-PdfIcon {
  background-image: var(--jp-icon-pdf);
}

.jp-PythonIcon {
  background-image: var(--jp-icon-python);
}

.jp-RKernelIcon {
  background-image: var(--jp-icon-r-kernel);
}

.jp-ReactIcon {
  background-image: var(--jp-icon-react);
}

.jp-RedoIcon {
  background-image: var(--jp-icon-redo);
}

.jp-RefreshIcon {
  background-image: var(--jp-icon-refresh);
}

.jp-RegexIcon {
  background-image: var(--jp-icon-regex);
}

.jp-RunIcon {
  background-image: var(--jp-icon-run);
}

.jp-RunningIcon {
  background-image: var(--jp-icon-running);
}

.jp-SaveIcon {
  background-image: var(--jp-icon-save);
}

.jp-SearchIcon {
  background-image: var(--jp-icon-search);
}

.jp-SettingsIcon {
  background-image: var(--jp-icon-settings);
}

.jp-ShareIcon {
  background-image: var(--jp-icon-share);
}

.jp-SpreadsheetIcon {
  background-image: var(--jp-icon-spreadsheet);
}

.jp-StopIcon {
  background-image: var(--jp-icon-stop);
}

.jp-TabIcon {
  background-image: var(--jp-icon-tab);
}

.jp-TableRowsIcon {
  background-image: var(--jp-icon-table-rows);
}

.jp-TagIcon {
  background-image: var(--jp-icon-tag);
}

.jp-TerminalIcon {
  background-image: var(--jp-icon-terminal);
}

.jp-TextEditorIcon {
  background-image: var(--jp-icon-text-editor);
}

.jp-TocIcon {
  background-image: var(--jp-icon-toc);
}

.jp-TreeViewIcon {
  background-image: var(--jp-icon-tree-view);
}

.jp-TrustedIcon {
  background-image: var(--jp-icon-trusted);
}

.jp-UndoIcon {
  background-image: var(--jp-icon-undo);
}

.jp-UserIcon {
  background-image: var(--jp-icon-user);
}

.jp-UsersIcon {
  background-image: var(--jp-icon-users);
}

.jp-VegaIcon {
  background-image: var(--jp-icon-vega);
}

.jp-WordIcon {
  background-image: var(--jp-icon-word);
}

.jp-YamlIcon {
  background-image: var(--jp-icon-yaml);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

.jp-Icon,
.jp-MaterialIcon {
  background-position: center;
  background-repeat: no-repeat;
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-cover {
  background-position: center;
  background-repeat: no-repeat;
  background-size: cover;
}

/**
 * (DEPRECATED) Support for specific CSS icon sizes
 */

.jp-Icon-16 {
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-18 {
  background-size: 18px;
  min-width: 18px;
  min-height: 18px;
}

.jp-Icon-20 {
  background-size: 20px;
  min-width: 20px;
  min-height: 20px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.lm-TabBar .lm-TabBar-addButton {
  align-items: center;
  display: flex;
  padding: 4px;
  padding-bottom: 5px;
  margin-right: 1px;
  background-color: var(--jp-layout-color2);
}

.lm-TabBar .lm-TabBar-addButton:hover {
  background-color: var(--jp-layout-color1);
}

.lm-DockPanel-tabBar .lm-TabBar-tab {
  width: var(--jp-private-horizontal-tab-width);
}

.lm-DockPanel-tabBar .lm-TabBar-content {
  flex: unset;
}

.lm-DockPanel-tabBar[data-orientation='horizontal'] {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for icons as inline SVG HTMLElements
 */

/* recolor the primary elements of an icon */
.jp-icon0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-accent0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-accent1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-accent2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-accent3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-accent4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-accent0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-accent1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-accent2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-accent3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-accent4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-none[fill] {
  fill: none;
}

.jp-icon-none[stroke] {
  stroke: none;
}

/* brand icon colors. Same for light and dark */
.jp-icon-brand0[fill] {
  fill: var(--jp-brand-color0);
}

.jp-icon-brand1[fill] {
  fill: var(--jp-brand-color1);
}

.jp-icon-brand2[fill] {
  fill: var(--jp-brand-color2);
}

.jp-icon-brand3[fill] {
  fill: var(--jp-brand-color3);
}

.jp-icon-brand4[fill] {
  fill: var(--jp-brand-color4);
}

.jp-icon-brand0[stroke] {
  stroke: var(--jp-brand-color0);
}

.jp-icon-brand1[stroke] {
  stroke: var(--jp-brand-color1);
}

.jp-icon-brand2[stroke] {
  stroke: var(--jp-brand-color2);
}

.jp-icon-brand3[stroke] {
  stroke: var(--jp-brand-color3);
}

.jp-icon-brand4[stroke] {
  stroke: var(--jp-brand-color4);
}

/* warn icon colors. Same for light and dark */
.jp-icon-warn0[fill] {
  fill: var(--jp-warn-color0);
}

.jp-icon-warn1[fill] {
  fill: var(--jp-warn-color1);
}

.jp-icon-warn2[fill] {
  fill: var(--jp-warn-color2);
}

.jp-icon-warn3[fill] {
  fill: var(--jp-warn-color3);
}

.jp-icon-warn0[stroke] {
  stroke: var(--jp-warn-color0);
}

.jp-icon-warn1[stroke] {
  stroke: var(--jp-warn-color1);
}

.jp-icon-warn2[stroke] {
  stroke: var(--jp-warn-color2);
}

.jp-icon-warn3[stroke] {
  stroke: var(--jp-warn-color3);
}

/* icon colors that contrast well with each other and most backgrounds */
.jp-icon-contrast0[fill] {
  fill: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[fill] {
  fill: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[fill] {
  fill: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[fill] {
  fill: var(--jp-icon-contrast-color3);
}

.jp-icon-contrast0[stroke] {
  stroke: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[stroke] {
  stroke: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[stroke] {
  stroke: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[stroke] {
  stroke: var(--jp-icon-contrast-color3);
}

.jp-icon-dot[fill] {
  fill: var(--jp-warn-color0);
}

.jp-jupyter-icon-color[fill] {
  fill: var(--jp-jupyter-icon-color, var(--jp-warn-color0));
}

.jp-notebook-icon-color[fill] {
  fill: var(--jp-notebook-icon-color, var(--jp-warn-color0));
}

.jp-json-icon-color[fill] {
  fill: var(--jp-json-icon-color, var(--jp-warn-color1));
}

.jp-console-icon-color[fill] {
  fill: var(--jp-console-icon-color, white);
}

.jp-console-icon-background-color[fill] {
  fill: var(--jp-console-icon-background-color, var(--jp-brand-color1));
}

.jp-terminal-icon-color[fill] {
  fill: var(--jp-terminal-icon-color, var(--jp-layout-color2));
}

.jp-terminal-icon-background-color[fill] {
  fill: var(
    --jp-terminal-icon-background-color,
    var(--jp-inverse-layout-color2)
  );
}

.jp-text-editor-icon-color[fill] {
  fill: var(--jp-text-editor-icon-color, var(--jp-inverse-layout-color3));
}

.jp-inspector-icon-color[fill] {
  fill: var(--jp-inspector-icon-color, var(--jp-inverse-layout-color3));
}

/* CSS for icons in selected filebrowser listing items */
.jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

.jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* stylelint-disable selector-max-class, selector-max-compound-selectors */

/**
* TODO: come up with non css-hack solution for showing the busy icon on top
*  of the close icon
* CSS for complex behavior of close icon of tabs in the main area tabbar
*/
.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon3[fill] {
  fill: none;
}

.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: var(--jp-inverse-layout-color3);
}

/* stylelint-enable selector-max-class, selector-max-compound-selectors */

/* CSS for icons in status bar */
#jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

#jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* special handling for splash icon CSS. While the theme CSS reloads during
   splash, the splash icon can loose theming. To prevent that, we set a
   default for its color variable */
:root {
  --jp-warn-color0: var(--md-orange-700);
}

/* not sure what to do with this one, used in filebrowser listing */
.jp-DragIcon {
  margin-right: 4px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for alt colors for icons as inline SVG HTMLElements
 */

/* alt recolor the primary elements of an icon */
.jp-icon-alt .jp-icon0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-alt .jp-icon0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* alt recolor the accent elements of an icon */
.jp-icon-alt .jp-icon-accent0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-alt .jp-icon-accent0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-icon-hoverShow:not(:hover) .jp-icon-hoverShow-content {
  display: none !important;
}

/**
 * Support for hover colors for icons as inline SVG HTMLElements
 */

/**
 * regular colors
 */

/* recolor the primary elements of an icon */
.jp-icon-hover :hover .jp-icon0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-hover :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-hover :hover .jp-icon-none-hover[fill] {
  fill: none;
}

.jp-icon-hover :hover .jp-icon-none-hover[stroke] {
  stroke: none;
}

/**
 * inverse colors
 */

/* inverse recolor the primary elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* inverse recolor the accent elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-IFrame {
  width: 100%;
  height: 100%;
}

.jp-IFrame > iframe {
  border: none;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-IFrame {
  position: relative;
}

body.lm-mod-override-cursor .jp-IFrame::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-HoverBox {
  position: fixed;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FormGroup-content fieldset {
  border: none;
  padding: 0;
  min-width: 0;
  width: 100%;
}

/* stylelint-disable selector-max-type */

.jp-FormGroup-content fieldset .jp-inputFieldWrapper input,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper select,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper textarea {
  font-size: var(--jp-content-font-size2);
  border-color: var(--jp-input-border-color);
  border-style: solid;
  border-radius: var(--jp-border-radius);
  border-width: 1px;
  padding: 6px 8px;
  background: none;
  color: var(--jp-ui-font-color0);
  height: inherit;
}

.jp-FormGroup-content fieldset input[type='checkbox'] {
  position: relative;
  top: 2px;
  margin-left: 0;
}

.jp-FormGroup-content button.jp-mod-styled {
  cursor: pointer;
}

.jp-FormGroup-content .checkbox label {
  cursor: pointer;
  font-size: var(--jp-content-font-size1);
}

.jp-FormGroup-content .jp-root > fieldset > legend {
  display: none;
}

.jp-FormGroup-content .jp-root > fieldset > p {
  display: none;
}

/** copy of `input.jp-mod-styled:focus` style */
.jp-FormGroup-content fieldset input:focus,
.jp-FormGroup-content fieldset select:focus {
  -moz-outline-radius: unset;
  outline: var(--jp-border-width) solid var(--md-blue-500);
  outline-offset: -1px;
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-FormGroup-content fieldset input:hover:not(:focus),
.jp-FormGroup-content fieldset select:hover:not(:focus) {
  background-color: var(--jp-border-color2);
}

/* stylelint-enable selector-max-type */

.jp-FormGroup-content .checkbox .field-description {
  /* Disable default description field for checkbox:
   because other widgets do not have description fields,
   we add descriptions to each widget on the field level.
  */
  display: none;
}

.jp-FormGroup-content #root__description {
  display: none;
}

.jp-FormGroup-content .jp-modifiedIndicator {
  width: 5px;
  background-color: var(--jp-brand-color2);
  margin-top: 0;
  margin-left: calc(var(--jp-private-settingeditor-modifier-indent) * -1);
  flex-shrink: 0;
}

.jp-FormGroup-content .jp-modifiedIndicator.jp-errorIndicator {
  background-color: var(--jp-error-color0);
  margin-right: 0.5em;
}

/* RJSF ARRAY style */

.jp-arrayFieldWrapper legend {
  font-size: var(--jp-content-font-size2);
  color: var(--jp-ui-font-color0);
  flex-basis: 100%;
  padding: 4px 0;
  font-weight: var(--jp-content-heading-font-weight);
  border-bottom: 1px solid var(--jp-border-color2);
}

.jp-arrayFieldWrapper .field-description {
  padding: 4px 0;
  white-space: pre-wrap;
}

.jp-arrayFieldWrapper .array-item {
  width: 100%;
  border: 1px solid var(--jp-border-color2);
  border-radius: 4px;
  margin: 4px;
}

.jp-ArrayOperations {
  display: flex;
  margin-left: 8px;
}

.jp-ArrayOperationsButton {
  margin: 2px;
}

.jp-ArrayOperationsButton .jp-icon3[fill] {
  fill: var(--jp-ui-font-color0);
}

button.jp-ArrayOperationsButton.jp-mod-styled:disabled {
  cursor: not-allowed;
  opacity: 0.5;
}

/* RJSF form validation error */

.jp-FormGroup-content .validationErrors {
  color: var(--jp-error-color0);
}

/* Hide panel level error as duplicated the field level error */
.jp-FormGroup-content .panel.errors {
  display: none;
}

/* RJSF normal content (settings-editor) */

.jp-FormGroup-contentNormal {
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-FormGroup-contentItem {
  margin-left: 7px;
  color: var(--jp-ui-font-color0);
}

.jp-FormGroup-contentNormal .jp-FormGroup-description {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-default {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-fieldLabel {
  font-size: var(--jp-content-font-size1);
  font-weight: normal;
  min-width: 120px;
}

.jp-FormGroup-contentNormal fieldset:not(:first-child) {
  margin-left: 7px;
}

.jp-FormGroup-contentNormal .field-array-of-string .array-item {
  /* Display `jp-ArrayOperations` buttons side-by-side with content except
    for small screens where flex-wrap will place them one below the other.
  */
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-objectFieldWrapper .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

/* RJSF compact content (metadata-form) */

.jp-FormGroup-content.jp-FormGroup-contentCompact {
  width: 100%;
}

.jp-FormGroup-contentCompact .form-group {
  display: flex;
  padding: 0.5em 0.2em 0.5em 0;
}

.jp-FormGroup-contentCompact
  .jp-FormGroup-compactTitle
  .jp-FormGroup-description {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color2);
}

.jp-FormGroup-contentCompact .jp-FormGroup-fieldLabel {
  padding-bottom: 0.3em;
}

.jp-FormGroup-contentCompact .jp-inputFieldWrapper .form-control {
  width: 100%;
  box-sizing: border-box;
}

.jp-FormGroup-contentCompact .jp-arrayFieldWrapper .jp-FormGroup-compactTitle {
  padding-bottom: 7px;
}

.jp-FormGroup-contentCompact
  .jp-objectFieldWrapper
  .jp-objectFieldWrapper
  .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

.jp-FormGroup-contentCompact ul.error-detail {
  margin-block-start: 0.5em;
  margin-block-end: 0.5em;
  padding-inline-start: 1em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-SidePanel {
  display: flex;
  flex-direction: column;
  min-width: var(--jp-sidebar-min-width);
  overflow-y: auto;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  font-size: var(--jp-ui-font-size1);
}

.jp-SidePanel-header {
  flex: 0 0 auto;
  display: flex;
  border-bottom: var(--jp-border-width) solid var(--jp-border-color2);
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin: 0;
  padding: 2px;
  text-transform: uppercase;
}

.jp-SidePanel-toolbar {
  flex: 0 0 auto;
}

.jp-SidePanel-content {
  flex: 1 1 auto;
}

.jp-SidePanel-toolbar,
.jp-AccordionPanel-toolbar {
  height: var(--jp-private-toolbar-height);
}

.jp-SidePanel-toolbar.jp-Toolbar-micro {
  display: none;
}

.lm-AccordionPanel .jp-AccordionPanel-title {
  box-sizing: border-box;
  line-height: 25px;
  margin: 0;
  display: flex;
  align-items: center;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  font-size: var(--jp-ui-font-size0);
}

.jp-AccordionPanel-title {
  cursor: pointer;
  user-select: none;
  -moz-user-select: none;
  -webkit-user-select: none;
  text-transform: uppercase;
}

.lm-AccordionPanel[data-orientation='horizontal'] > .jp-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleLabel {
  user-select: none;
  text-overflow: ellipsis;
  white-space: nowrap;
  overflow: hidden;
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleCollapser {
  transform: rotate(-90deg);
  margin: auto 0;
  height: 16px;
}

.jp-AccordionPanel-title.lm-mod-expanded .lm-AccordionPanel-titleCollapser {
  transform: rotate(0deg);
}

.lm-AccordionPanel .jp-AccordionPanel-toolbar {
  background: none;
  box-shadow: none;
  border: none;
  margin-left: auto;
}

.lm-AccordionPanel .lm-SplitPanel-handle:hover {
  background: var(--jp-layout-color3);
}

.jp-text-truncated {
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Spinner {
  position: absolute;
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 10;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-layout-color0);
  outline: none;
}

.jp-SpinnerContent {
  font-size: 10px;
  margin: 50px auto;
  text-indent: -9999em;
  width: 3em;
  height: 3em;
  border-radius: 50%;
  background: var(--jp-brand-color3);
  background: linear-gradient(
    to right,
    #f37626 10%,
    rgba(255, 255, 255, 0) 42%
  );
  position: relative;
  animation: load3 1s infinite linear, fadeIn 1s;
}

.jp-SpinnerContent::before {
  width: 50%;
  height: 50%;
  background: #f37626;
  border-radius: 100% 0 0;
  position: absolute;
  top: 0;
  left: 0;
  content: '';
}

.jp-SpinnerContent::after {
  background: var(--jp-layout-color0);
  width: 75%;
  height: 75%;
  border-radius: 50%;
  content: '';
  margin: auto;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
}

@keyframes fadeIn {
  0% {
    opacity: 0;
  }

  100% {
    opacity: 1;
  }
}

@keyframes load3 {
  0% {
    transform: rotate(0deg);
  }

  100% {
    transform: rotate(360deg);
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

button.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: none;
  box-sizing: border-box;
  text-align: center;
  line-height: 32px;
  height: 32px;
  padding: 0 12px;
  letter-spacing: 0.8px;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input.jp-mod-styled {
  background: var(--jp-input-background);
  height: 28px;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color1);
  padding-left: 7px;
  padding-right: 7px;
  font-size: var(--jp-ui-font-size2);
  color: var(--jp-ui-font-color0);
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input[type='checkbox'].jp-mod-styled {
  appearance: checkbox;
  -webkit-appearance: checkbox;
  -moz-appearance: checkbox;
  height: auto;
}

input.jp-mod-styled:focus {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-select-wrapper {
  display: flex;
  position: relative;
  flex-direction: column;
  padding: 1px;
  background-color: var(--jp-layout-color1);
  box-sizing: border-box;
  margin-bottom: 12px;
}

.jp-select-wrapper:not(.multiple) {
  height: 28px;
}

.jp-select-wrapper.jp-mod-focused select.jp-mod-styled {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-input-active-background);
}

select.jp-mod-styled:hover {
  cursor: pointer;
  color: var(--jp-ui-font-color0);
  background-color: var(--jp-input-hover-background);
  box-shadow: inset 0 0 1px rgba(0, 0, 0, 0.5);
}

select.jp-mod-styled {
  flex: 1 1 auto;
  width: 100%;
  font-size: var(--jp-ui-font-size2);
  background: var(--jp-input-background);
  color: var(--jp-ui-font-color0);
  padding: 0 25px 0 8px;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

select.jp-mod-styled:not([multiple]) {
  height: 32px;
}

select.jp-mod-styled[multiple] {
  max-height: 200px;
  overflow-y: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-switch {
  display: flex;
  align-items: center;
  padding-left: 4px;
  padding-right: 4px;
  font-size: var(--jp-ui-font-size1);
  background-color: transparent;
  color: var(--jp-ui-font-color1);
  border: none;
  height: 20px;
}

.jp-switch:hover {
  background-color: var(--jp-layout-color2);
}

.jp-switch-label {
  margin-right: 5px;
  font-family: var(--jp-ui-font-family);
}

.jp-switch-track {
  cursor: pointer;
  background-color: var(--jp-switch-color, var(--jp-border-color1));
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 34px;
  height: 16px;
  width: 35px;
  position: relative;
}

.jp-switch-track::before {
  content: '';
  position: absolute;
  height: 10px;
  width: 10px;
  margin: 3px;
  left: 0;
  background-color: var(--jp-ui-inverse-font-color1);
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 50%;
}

.jp-switch[aria-checked='true'] .jp-switch-track {
  background-color: var(--jp-switch-true-position-color, var(--jp-warn-color0));
}

.jp-switch[aria-checked='true'] .jp-switch-track::before {
  /* track width (35) - margins (3 + 3) - thumb width (10) */
  left: 19px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toolbar-height: calc(
    28px + var(--jp-border-width)
  ); /* leave 28px for content */
}

.jp-Toolbar {
  color: var(--jp-ui-font-color1);
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: 2px;
  z-index: 8;
  overflow-x: hidden;
}

/* Toolbar items */

.jp-Toolbar > .jp-Toolbar-item.jp-Toolbar-spacer {
  flex-grow: 1;
  flex-shrink: 1;
}

.jp-Toolbar-item.jp-Toolbar-kernelStatus {
  display: inline-block;
  width: 32px;
  background-repeat: no-repeat;
  background-position: center;
  background-size: 16px;
}

.jp-Toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  display: flex;
  padding-left: 1px;
  padding-right: 1px;
  font-size: var(--jp-ui-font-size1);
  line-height: var(--jp-private-toolbar-height);
  height: 100%;
}

/* Toolbar buttons */

/* This is the div we use to wrap the react component into a Widget */
div.jp-ToolbarButton {
  color: transparent;
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0;
  margin: 0;
}

button.jp-ToolbarButtonComponent {
  background: var(--jp-layout-color1);
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0 6px;
  margin: 0;
  height: 24px;
  border-radius: var(--jp-border-radius);
  display: flex;
  align-items: center;
  text-align: center;
  font-size: 14px;
  min-width: unset;
  min-height: unset;
}

button.jp-ToolbarButtonComponent:disabled {
  opacity: 0.4;
}

button.jp-ToolbarButtonComponent > span {
  padding: 0;
  flex: 0 0 auto;
}

button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label {
  font-size: var(--jp-ui-font-size1);
  line-height: 100%;
  padding-left: 2px;
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar.jp-Toolbar-micro {
  padding: 0;
  min-height: 0;
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar {
  border: none;
  box-shadow: none;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-WindowedPanel-outer {
  position: relative;
  overflow-y: auto;
}

.jp-WindowedPanel-inner {
  position: relative;
}

.jp-WindowedPanel-window {
  position: absolute;
  left: 0;
  right: 0;
  overflow: visible;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* Sibling imports */

body {
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
}

/* Disable native link decoration styles everywhere outside of dialog boxes */
a {
  text-decoration: unset;
  color: unset;
}

a:hover {
  text-decoration: unset;
  color: unset;
}

/* Accessibility for links inside dialog box text */
.jp-Dialog-content a {
  text-decoration: revert;
  color: var(--jp-content-link-color);
}

.jp-Dialog-content a:hover {
  text-decoration: revert;
}

/* Styles for ui-components */
.jp-Button {
  color: var(--jp-ui-font-color2);
  border-radius: var(--jp-border-radius);
  padding: 0 12px;
  font-size: var(--jp-ui-font-size1);

  /* Copy from blueprint 3 */
  display: inline-flex;
  flex-direction: row;
  border: none;
  cursor: pointer;
  align-items: center;
  justify-content: center;
  text-align: left;
  vertical-align: middle;
  min-height: 30px;
  min-width: 30px;
}

.jp-Button:disabled {
  cursor: not-allowed;
}

.jp-Button:empty {
  padding: 0 !important;
}

.jp-Button.jp-mod-small {
  min-height: 24px;
  min-width: 24px;
  font-size: 12px;
  padding: 0 7px;
}

/* Use our own theme for hover styles */
.jp-Button.jp-mod-minimal:hover {
  background-color: var(--jp-layout-color2);
}

.jp-Button.jp-mod-minimal {
  background: none;
}

.jp-InputGroup {
  display: block;
  position: relative;
}

.jp-InputGroup input {
  box-sizing: border-box;
  border: none;
  border-radius: 0;
  background-color: transparent;
  color: var(--jp-ui-font-color0);
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
  padding-bottom: 0;
  padding-top: 0;
  padding-left: 10px;
  padding-right: 28px;
  position: relative;
  width: 100%;
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  font-size: 14px;
  font-weight: 400;
  height: 30px;
  line-height: 30px;
  outline: none;
  vertical-align: middle;
}

.jp-InputGroup input:focus {
  box-shadow: inset 0 0 0 var(--jp-border-width)
      var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-InputGroup input:disabled {
  cursor: not-allowed;
  resize: block;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input:disabled ~ span {
  cursor: not-allowed;
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input::placeholder,
input::placeholder {
  color: var(--jp-ui-font-color2);
}

.jp-InputGroupAction {
  position: absolute;
  bottom: 1px;
  right: 0;
  padding: 6px;
}

.jp-HTMLSelect.jp-DefaultStyle select {
  background-color: initial;
  border: none;
  border-radius: 0;
  box-shadow: none;
  color: var(--jp-ui-font-color0);
  display: block;
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  height: 24px;
  line-height: 14px;
  padding: 0 25px 0 10px;
  text-align: left;
  -moz-appearance: none;
  -webkit-appearance: none;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
  cursor: not-allowed;
  resize: block;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled ~ span {
  cursor: not-allowed;
}

/* Use our own theme for hover and option styles */
/* stylelint-disable-next-line selector-max-type */
.jp-HTMLSelect.jp-DefaultStyle select:hover,
.jp-HTMLSelect.jp-DefaultStyle select > option {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color0);
}

select {
  box-sizing: border-box;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-StatusBar-Widget {
  display: flex;
  align-items: center;
  background: var(--jp-layout-color2);
  min-height: var(--jp-statusbar-height);
  justify-content: space-between;
  padding: 0 10px;
}

.jp-StatusBar-Left {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-StatusBar-Middle {
  display: flex;
  align-items: center;
}

.jp-StatusBar-Right {
  display: flex;
  align-items: center;
  flex-direction: row-reverse;
}

.jp-StatusBar-Item {
  max-height: var(--jp-statusbar-height);
  margin: 0 2px;
  height: var(--jp-statusbar-height);
  white-space: nowrap;
  text-overflow: ellipsis;
  color: var(--jp-ui-font-color1);
  padding: 0 6px;
}

.jp-mod-highlighted:hover {
  background-color: var(--jp-layout-color3);
}

.jp-mod-clicked {
  background-color: var(--jp-brand-color1);
}

.jp-mod-clicked:hover {
  background-color: var(--jp-brand-color0);
}

.jp-mod-clicked .jp-StatusBar-TextItem {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-StatusBar-HoverItem {
  box-shadow: '0px 4px 4px rgba(0, 0, 0, 0.25)';
}

.jp-StatusBar-TextItem {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  line-height: 24px;
  color: var(--jp-ui-font-color1);
}

.jp-StatusBar-GroupItem {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-Statusbar-ProgressCircle svg {
  display: block;
  margin: 0 auto;
  width: 16px;
  height: 24px;
  align-self: normal;
}

.jp-Statusbar-ProgressCircle path {
  fill: var(--jp-inverse-layout-color3);
}

.jp-Statusbar-ProgressBar-progress-bar {
  height: 10px;
  width: 100px;
  border: solid 0.25px var(--jp-brand-color2);
  border-radius: 3px;
  overflow: hidden;
  align-self: center;
}

.jp-Statusbar-ProgressBar-progress-bar > div {
  background-color: var(--jp-brand-color2);
  background-image: linear-gradient(
    -45deg,
    rgba(255, 255, 255, 0.2) 25%,
    transparent 25%,
    transparent 50%,
    rgba(255, 255, 255, 0.2) 50%,
    rgba(255, 255, 255, 0.2) 75%,
    transparent 75%,
    transparent
  );
  background-size: 40px 40px;
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 14px;
  color: #fff;
  text-align: center;
  animation: jp-Statusbar-ExecutionTime-progress-bar 2s linear infinite;
}

.jp-Statusbar-ProgressBar-progress-bar p {
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
  font-size: var(--jp-ui-font-size1);
  line-height: 10px;
  width: 100px;
}

@keyframes jp-Statusbar-ExecutionTime-progress-bar {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 40px 40px;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-commandpalette-search-height: 28px;
}

/*-----------------------------------------------------------------------------
| Overall styles
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  padding-bottom: 0;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Modal variant
|----------------------------------------------------------------------------*/

.jp-ModalCommandPalette {
  position: absolute;
  z-index: 10000;
  top: 38px;
  left: 30%;
  margin: 0;
  padding: 4px;
  width: 40%;
  box-shadow: var(--jp-elevation-z4);
  border-radius: 4px;
  background: var(--jp-layout-color0);
}

.jp-ModalCommandPalette .lm-CommandPalette {
  max-height: 40vh;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-close-icon::after {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-header {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-item {
  margin-left: 4px;
  margin-right: 4px;
}

.jp-ModalCommandPalette
  .lm-CommandPalette
  .lm-CommandPalette-item.lm-mod-disabled {
  display: none;
}

/*-----------------------------------------------------------------------------
| Search
|----------------------------------------------------------------------------*/

.lm-CommandPalette-search {
  padding: 4px;
  background-color: var(--jp-layout-color1);
  z-index: 2;
}

.lm-CommandPalette-wrapper {
  overflow: overlay;
  padding: 0 9px;
  background-color: var(--jp-input-active-background);
  height: 30px;
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
}

.lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper {
  box-shadow: inset 0 0 0 1px var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-SearchIconGroup {
  color: white;
  background-color: var(--jp-brand-color1);
  position: absolute;
  top: 4px;
  right: 4px;
  padding: 5px 5px 1px;
}

.jp-SearchIconGroup svg {
  height: 20px;
  width: 20px;
}

.jp-SearchIconGroup .jp-icon3[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-input {
  background: transparent;
  width: calc(100% - 18px);
  float: left;
  border: none;
  outline: none;
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  line-height: var(--jp-private-commandpalette-search-height);
}

.lm-CommandPalette-input::-webkit-input-placeholder,
.lm-CommandPalette-input::-moz-placeholder,
.lm-CommandPalette-input:-ms-input-placeholder {
  color: var(--jp-ui-font-color2);
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Results
|----------------------------------------------------------------------------*/

.lm-CommandPalette-header:first-child {
  margin-top: 0;
}

.lm-CommandPalette-header {
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin-top: 8px;
  padding: 8px 0 8px 12px;
  text-transform: uppercase;
}

.lm-CommandPalette-header.lm-mod-active {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-header > mark {
  background-color: transparent;
  font-weight: bold;
  color: var(--jp-ui-font-color1);
}

.lm-CommandPalette-item {
  padding: 4px 12px 4px 4px;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  font-weight: 400;
  display: flex;
}

.lm-CommandPalette-item.lm-mod-disabled {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item.lm-mod-active {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item.lm-mod-active .lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-inverse-font-color0);
}

.lm-CommandPalette-item.lm-mod-active .jp-icon-selectable[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled) {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled) {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-itemContent {
  overflow: hidden;
}

.lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.lm-CommandPalette-item.lm-mod-disabled mark {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item .lm-CommandPalette-itemIcon {
  margin: 0 4px 0 0;
  position: relative;
  width: 16px;
  top: 2px;
  flex: 0 0 auto;
}

.lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon {
  opacity: 0.6;
}

.lm-CommandPalette-item .lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemCaption {
  display: none;
}

.lm-CommandPalette-content {
  background-color: var(--jp-layout-color1);
}

.lm-CommandPalette-content:empty::after {
  content: 'No results';
  margin: auto;
  margin-top: 20px;
  width: 100px;
  display: block;
  font-size: var(--jp-ui-font-size2);
  font-family: var(--jp-ui-font-family);
  font-weight: lighter;
}

.lm-CommandPalette-emptyMessage {
  text-align: center;
  margin-top: 24px;
  line-height: 1.32;
  padding: 0 8px;
  color: var(--jp-content-font-color3);
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Dialog {
  position: absolute;
  z-index: 10000;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  top: 0;
  left: 0;
  margin: 0;
  padding: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-dialog-background);
}

.jp-Dialog-content {
  display: flex;
  flex-direction: column;
  margin-left: auto;
  margin-right: auto;
  background: var(--jp-layout-color1);
  padding: 24px 24px 12px;
  min-width: 300px;
  min-height: 150px;
  max-width: 1000px;
  max-height: 500px;
  box-sizing: border-box;
  box-shadow: var(--jp-elevation-z20);
  word-wrap: break-word;
  border-radius: var(--jp-border-radius);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color1);
  resize: both;
}

.jp-Dialog-content.jp-Dialog-content-small {
  max-width: 500px;
}

.jp-Dialog-button {
  overflow: visible;
}

button.jp-Dialog-button:focus {
  outline: 1px solid var(--jp-brand-color1);
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button:focus::-moz-focus-inner {
  border: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus {
  outline: 1px solid var(--jp-accept-color-normal, var(--jp-brand-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus {
  outline: 1px solid var(--jp-warn-color-normal, var(--jp-error-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline: 1px solid var(--jp-reject-color-normal, var(--md-grey-600));
}

button.jp-Dialog-close-button {
  padding: 0;
  height: 100%;
  min-width: unset;
  min-height: unset;
}

.jp-Dialog-header {
  display: flex;
  justify-content: space-between;
  flex: 0 0 auto;
  padding-bottom: 12px;
  font-size: var(--jp-ui-font-size3);
  font-weight: 400;
  color: var(--jp-ui-font-color1);
}

.jp-Dialog-body {
  display: flex;
  flex-direction: column;
  flex: 1 1 auto;
  font-size: var(--jp-ui-font-size1);
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

.jp-Dialog-footer {
  display: flex;
  flex-direction: row;
  justify-content: flex-end;
  align-items: center;
  flex: 0 0 auto;
  margin-left: -12px;
  margin-right: -12px;
  padding: 12px;
}

.jp-Dialog-checkbox {
  padding-right: 5px;
}

.jp-Dialog-checkbox > input:focus-visible {
  outline: 1px solid var(--jp-input-active-border-color);
  outline-offset: 1px;
}

.jp-Dialog-spacer {
  flex: 1 1 auto;
}

.jp-Dialog-title {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.jp-Dialog-body > .jp-select-wrapper {
  width: 100%;
}

.jp-Dialog-body > button {
  padding: 0 16px;
}

.jp-Dialog-body > label {
  line-height: 1.4;
  color: var(--jp-ui-font-color0);
}

.jp-Dialog-button.jp-mod-styled:not(:last-child) {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Input-Boolean-Dialog {
  flex-direction: row-reverse;
  align-items: end;
  width: 100%;
}

.jp-Input-Boolean-Dialog > label {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MainAreaWidget > :focus {
  outline: none;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error {
  padding: 6px;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error > pre {
  width: auto;
  padding: 10px;
  background: var(--jp-error-color3);
  border: var(--jp-border-width) solid var(--jp-error-color1);
  border-radius: var(--jp-border-radius);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  white-space: pre-wrap;
  word-wrap: break-word;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/**
 * google-material-color v1.2.6
 * https://github.com/danlevan/google-material-color
 */
:root {
  --md-red-50: #ffebee;
  --md-red-100: #ffcdd2;
  --md-red-200: #ef9a9a;
  --md-red-300: #e57373;
  --md-red-400: #ef5350;
  --md-red-500: #f44336;
  --md-red-600: #e53935;
  --md-red-700: #d32f2f;
  --md-red-800: #c62828;
  --md-red-900: #b71c1c;
  --md-red-A100: #ff8a80;
  --md-red-A200: #ff5252;
  --md-red-A400: #ff1744;
  --md-red-A700: #d50000;
  --md-pink-50: #fce4ec;
  --md-pink-100: #f8bbd0;
  --md-pink-200: #f48fb1;
  --md-pink-300: #f06292;
  --md-pink-400: #ec407a;
  --md-pink-500: #e91e63;
  --md-pink-600: #d81b60;
  --md-pink-700: #c2185b;
  --md-pink-800: #ad1457;
  --md-pink-900: #880e4f;
  --md-pink-A100: #ff80ab;
  --md-pink-A200: #ff4081;
  --md-pink-A400: #f50057;
  --md-pink-A700: #c51162;
  --md-purple-50: #f3e5f5;
  --md-purple-100: #e1bee7;
  --md-purple-200: #ce93d8;
  --md-purple-300: #ba68c8;
  --md-purple-400: #ab47bc;
  --md-purple-500: #9c27b0;
  --md-purple-600: #8e24aa;
  --md-purple-700: #7b1fa2;
  --md-purple-800: #6a1b9a;
  --md-purple-900: #4a148c;
  --md-purple-A100: #ea80fc;
  --md-purple-A200: #e040fb;
  --md-purple-A400: #d500f9;
  --md-purple-A700: #a0f;
  --md-deep-purple-50: #ede7f6;
  --md-deep-purple-100: #d1c4e9;
  --md-deep-purple-200: #b39ddb;
  --md-deep-purple-300: #9575cd;
  --md-deep-purple-400: #7e57c2;
  --md-deep-purple-500: #673ab7;
  --md-deep-purple-600: #5e35b1;
  --md-deep-purple-700: #512da8;
  --md-deep-purple-800: #4527a0;
  --md-deep-purple-900: #311b92;
  --md-deep-purple-A100: #b388ff;
  --md-deep-purple-A200: #7c4dff;
  --md-deep-purple-A400: #651fff;
  --md-deep-purple-A700: #6200ea;
  --md-indigo-50: #e8eaf6;
  --md-indigo-100: #c5cae9;
  --md-indigo-200: #9fa8da;
  --md-indigo-300: #7986cb;
  --md-indigo-400: #5c6bc0;
  --md-indigo-500: #3f51b5;
  --md-indigo-600: #3949ab;
  --md-indigo-700: #303f9f;
  --md-indigo-800: #283593;
  --md-indigo-900: #1a237e;
  --md-indigo-A100: #8c9eff;
  --md-indigo-A200: #536dfe;
  --md-indigo-A400: #3d5afe;
  --md-indigo-A700: #304ffe;
  --md-blue-50: #e3f2fd;
  --md-blue-100: #bbdefb;
  --md-blue-200: #90caf9;
  --md-blue-300: #64b5f6;
  --md-blue-400: #42a5f5;
  --md-blue-500: #2196f3;
  --md-blue-600: #1e88e5;
  --md-blue-700: #1976d2;
  --md-blue-800: #1565c0;
  --md-blue-900: #0d47a1;
  --md-blue-A100: #82b1ff;
  --md-blue-A200: #448aff;
  --md-blue-A400: #2979ff;
  --md-blue-A700: #2962ff;
  --md-light-blue-50: #e1f5fe;
  --md-light-blue-100: #b3e5fc;
  --md-light-blue-200: #81d4fa;
  --md-light-blue-300: #4fc3f7;
  --md-light-blue-400: #29b6f6;
  --md-light-blue-500: #03a9f4;
  --md-light-blue-600: #039be5;
  --md-light-blue-700: #0288d1;
  --md-light-blue-800: #0277bd;
  --md-light-blue-900: #01579b;
  --md-light-blue-A100: #80d8ff;
  --md-light-blue-A200: #40c4ff;
  --md-light-blue-A400: #00b0ff;
  --md-light-blue-A700: #0091ea;
  --md-cyan-50: #e0f7fa;
  --md-cyan-100: #b2ebf2;
  --md-cyan-200: #80deea;
  --md-cyan-300: #4dd0e1;
  --md-cyan-400: #26c6da;
  --md-cyan-500: #00bcd4;
  --md-cyan-600: #00acc1;
  --md-cyan-700: #0097a7;
  --md-cyan-800: #00838f;
  --md-cyan-900: #006064;
  --md-cyan-A100: #84ffff;
  --md-cyan-A200: #18ffff;
  --md-cyan-A400: #00e5ff;
  --md-cyan-A700: #00b8d4;
  --md-teal-50: #e0f2f1;
  --md-teal-100: #b2dfdb;
  --md-teal-200: #80cbc4;
  --md-teal-300: #4db6ac;
  --md-teal-400: #26a69a;
  --md-teal-500: #009688;
  --md-teal-600: #00897b;
  --md-teal-700: #00796b;
  --md-teal-800: #00695c;
  --md-teal-900: #004d40;
  --md-teal-A100: #a7ffeb;
  --md-teal-A200: #64ffda;
  --md-teal-A400: #1de9b6;
  --md-teal-A700: #00bfa5;
  --md-green-50: #e8f5e9;
  --md-green-100: #c8e6c9;
  --md-green-200: #a5d6a7;
  --md-green-300: #81c784;
  --md-green-400: #66bb6a;
  --md-green-500: #4caf50;
  --md-green-600: #43a047;
  --md-green-700: #388e3c;
  --md-green-800: #2e7d32;
  --md-green-900: #1b5e20;
  --md-green-A100: #b9f6ca;
  --md-green-A200: #69f0ae;
  --md-green-A400: #00e676;
  --md-green-A700: #00c853;
  --md-light-green-50: #f1f8e9;
  --md-light-green-100: #dcedc8;
  --md-light-green-200: #c5e1a5;
  --md-light-green-300: #aed581;
  --md-light-green-400: #9ccc65;
  --md-light-green-500: #8bc34a;
  --md-light-green-600: #7cb342;
  --md-light-green-700: #689f38;
  --md-light-green-800: #558b2f;
  --md-light-green-900: #33691e;
  --md-light-green-A100: #ccff90;
  --md-light-green-A200: #b2ff59;
  --md-light-green-A400: #76ff03;
  --md-light-green-A700: #64dd17;
  --md-lime-50: #f9fbe7;
  --md-lime-100: #f0f4c3;
  --md-lime-200: #e6ee9c;
  --md-lime-300: #dce775;
  --md-lime-400: #d4e157;
  --md-lime-500: #cddc39;
  --md-lime-600: #c0ca33;
  --md-lime-700: #afb42b;
  --md-lime-800: #9e9d24;
  --md-lime-900: #827717;
  --md-lime-A100: #f4ff81;
  --md-lime-A200: #eeff41;
  --md-lime-A400: #c6ff00;
  --md-lime-A700: #aeea00;
  --md-yellow-50: #fffde7;
  --md-yellow-100: #fff9c4;
  --md-yellow-200: #fff59d;
  --md-yellow-300: #fff176;
  --md-yellow-400: #ffee58;
  --md-yellow-500: #ffeb3b;
  --md-yellow-600: #fdd835;
  --md-yellow-700: #fbc02d;
  --md-yellow-800: #f9a825;
  --md-yellow-900: #f57f17;
  --md-yellow-A100: #ffff8d;
  --md-yellow-A200: #ff0;
  --md-yellow-A400: #ffea00;
  --md-yellow-A700: #ffd600;
  --md-amber-50: #fff8e1;
  --md-amber-100: #ffecb3;
  --md-amber-200: #ffe082;
  --md-amber-300: #ffd54f;
  --md-amber-400: #ffca28;
  --md-amber-500: #ffc107;
  --md-amber-600: #ffb300;
  --md-amber-700: #ffa000;
  --md-amber-800: #ff8f00;
  --md-amber-900: #ff6f00;
  --md-amber-A100: #ffe57f;
  --md-amber-A200: #ffd740;
  --md-amber-A400: #ffc400;
  --md-amber-A700: #ffab00;
  --md-orange-50: #fff3e0;
  --md-orange-100: #ffe0b2;
  --md-orange-200: #ffcc80;
  --md-orange-300: #ffb74d;
  --md-orange-400: #ffa726;
  --md-orange-500: #ff9800;
  --md-orange-600: #fb8c00;
  --md-orange-700: #f57c00;
  --md-orange-800: #ef6c00;
  --md-orange-900: #e65100;
  --md-orange-A100: #ffd180;
  --md-orange-A200: #ffab40;
  --md-orange-A400: #ff9100;
  --md-orange-A700: #ff6d00;
  --md-deep-orange-50: #fbe9e7;
  --md-deep-orange-100: #ffccbc;
  --md-deep-orange-200: #ffab91;
  --md-deep-orange-300: #ff8a65;
  --md-deep-orange-400: #ff7043;
  --md-deep-orange-500: #ff5722;
  --md-deep-orange-600: #f4511e;
  --md-deep-orange-700: #e64a19;
  --md-deep-orange-800: #d84315;
  --md-deep-orange-900: #bf360c;
  --md-deep-orange-A100: #ff9e80;
  --md-deep-orange-A200: #ff6e40;
  --md-deep-orange-A400: #ff3d00;
  --md-deep-orange-A700: #dd2c00;
  --md-brown-50: #efebe9;
  --md-brown-100: #d7ccc8;
  --md-brown-200: #bcaaa4;
  --md-brown-300: #a1887f;
  --md-brown-400: #8d6e63;
  --md-brown-500: #795548;
  --md-brown-600: #6d4c41;
  --md-brown-700: #5d4037;
  --md-brown-800: #4e342e;
  --md-brown-900: #3e2723;
  --md-grey-50: #fafafa;
  --md-grey-100: #f5f5f5;
  --md-grey-200: #eee;
  --md-grey-300: #e0e0e0;
  --md-grey-400: #bdbdbd;
  --md-grey-500: #9e9e9e;
  --md-grey-600: #757575;
  --md-grey-700: #616161;
  --md-grey-800: #424242;
  --md-grey-900: #212121;
  --md-blue-grey-50: #eceff1;
  --md-blue-grey-100: #cfd8dc;
  --md-blue-grey-200: #b0bec5;
  --md-blue-grey-300: #90a4ae;
  --md-blue-grey-400: #78909c;
  --md-blue-grey-500: #607d8b;
  --md-blue-grey-600: #546e7a;
  --md-blue-grey-700: #455a64;
  --md-blue-grey-800: #37474f;
  --md-blue-grey-900: #263238;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| RenderedText
|----------------------------------------------------------------------------*/

:root {
  /* This is the padding value to fill the gaps between lines containing spans with background color. */
  --jp-private-code-span-padding: calc(
    (var(--jp-code-line-height) - 1) * var(--jp-code-font-size) / 2
  );
}

.jp-RenderedText {
  text-align: left;
  padding-left: var(--jp-code-padding);
  line-height: var(--jp-code-line-height);
  font-family: var(--jp-code-font-family);
}

.jp-RenderedText pre,
.jp-RenderedJavaScript pre,
.jp-RenderedHTMLCommon pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
  border: none;
  margin: 0;
  padding: 0;
}

.jp-RenderedText pre a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* console foregrounds and backgrounds */
.jp-RenderedText pre .ansi-black-fg {
  color: #3e424d;
}

.jp-RenderedText pre .ansi-red-fg {
  color: #e75c58;
}

.jp-RenderedText pre .ansi-green-fg {
  color: #00a250;
}

.jp-RenderedText pre .ansi-yellow-fg {
  color: #ddb62b;
}

.jp-RenderedText pre .ansi-blue-fg {
  color: #208ffb;
}

.jp-RenderedText pre .ansi-magenta-fg {
  color: #d160c4;
}

.jp-RenderedText pre .ansi-cyan-fg {
  color: #60c6c8;
}

.jp-RenderedText pre .ansi-white-fg {
  color: #c5c1b4;
}

.jp-RenderedText pre .ansi-black-bg {
  background-color: #3e424d;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-bg {
  background-color: #e75c58;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-bg {
  background-color: #00a250;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-bg {
  background-color: #ddb62b;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-bg {
  background-color: #208ffb;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-bg {
  background-color: #d160c4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-bg {
  background-color: #60c6c8;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-bg {
  background-color: #c5c1b4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-black-intense-fg {
  color: #282c36;
}

.jp-RenderedText pre .ansi-red-intense-fg {
  color: #b22b31;
}

.jp-RenderedText pre .ansi-green-intense-fg {
  color: #007427;
}

.jp-RenderedText pre .ansi-yellow-intense-fg {
  color: #b27d12;
}

.jp-RenderedText pre .ansi-blue-intense-fg {
  color: #0065ca;
}

.jp-RenderedText pre .ansi-magenta-intense-fg {
  color: #a03196;
}

.jp-RenderedText pre .ansi-cyan-intense-fg {
  color: #258f8f;
}

.jp-RenderedText pre .ansi-white-intense-fg {
  color: #a1a6b2;
}

.jp-RenderedText pre .ansi-black-intense-bg {
  background-color: #282c36;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-intense-bg {
  background-color: #b22b31;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-intense-bg {
  background-color: #007427;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-intense-bg {
  background-color: #b27d12;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-intense-bg {
  background-color: #0065ca;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-intense-bg {
  background-color: #a03196;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-intense-bg {
  background-color: #258f8f;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-intense-bg {
  background-color: #a1a6b2;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-default-inverse-fg {
  color: var(--jp-ui-inverse-font-color0);
}

.jp-RenderedText pre .ansi-default-inverse-bg {
  background-color: var(--jp-inverse-layout-color0);
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-bold {
  font-weight: bold;
}

.jp-RenderedText pre .ansi-underline {
  text-decoration: underline;
}

.jp-RenderedText[data-mime-type='application/vnd.jupyter.stderr'] {
  background: var(--jp-rendermime-error-background);
  padding-top: var(--jp-code-padding);
}

/*-----------------------------------------------------------------------------
| RenderedLatex
|----------------------------------------------------------------------------*/

.jp-RenderedLatex {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);
}

/* Left-justify outputs.*/
.jp-OutputArea-output.jp-RenderedLatex {
  padding: var(--jp-code-padding);
  text-align: left;
}

/*-----------------------------------------------------------------------------
| RenderedHTML
|----------------------------------------------------------------------------*/

.jp-RenderedHTMLCommon {
  color: var(--jp-content-font-color1);
  font-family: var(--jp-content-font-family);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);

  /* Give a bit more R padding on Markdown text to keep line lengths reasonable */
  padding-right: 20px;
}

.jp-RenderedHTMLCommon em {
  font-style: italic;
}

.jp-RenderedHTMLCommon strong {
  font-weight: bold;
}

.jp-RenderedHTMLCommon u {
  text-decoration: underline;
}

.jp-RenderedHTMLCommon a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* Headings */

.jp-RenderedHTMLCommon h1,
.jp-RenderedHTMLCommon h2,
.jp-RenderedHTMLCommon h3,
.jp-RenderedHTMLCommon h4,
.jp-RenderedHTMLCommon h5,
.jp-RenderedHTMLCommon h6 {
  line-height: var(--jp-content-heading-line-height);
  font-weight: var(--jp-content-heading-font-weight);
  font-style: normal;
  margin: var(--jp-content-heading-margin-top) 0
    var(--jp-content-heading-margin-bottom) 0;
}

.jp-RenderedHTMLCommon h1:first-child,
.jp-RenderedHTMLCommon h2:first-child,
.jp-RenderedHTMLCommon h3:first-child,
.jp-RenderedHTMLCommon h4:first-child,
.jp-RenderedHTMLCommon h5:first-child,
.jp-RenderedHTMLCommon h6:first-child {
  margin-top: calc(0.5 * var(--jp-content-heading-margin-top));
}

.jp-RenderedHTMLCommon h1:last-child,
.jp-RenderedHTMLCommon h2:last-child,
.jp-RenderedHTMLCommon h3:last-child,
.jp-RenderedHTMLCommon h4:last-child,
.jp-RenderedHTMLCommon h5:last-child,
.jp-RenderedHTMLCommon h6:last-child {
  margin-bottom: calc(0.5 * var(--jp-content-heading-margin-bottom));
}

.jp-RenderedHTMLCommon h1 {
  font-size: var(--jp-content-font-size5);
}

.jp-RenderedHTMLCommon h2 {
  font-size: var(--jp-content-font-size4);
}

.jp-RenderedHTMLCommon h3 {
  font-size: var(--jp-content-font-size3);
}

.jp-RenderedHTMLCommon h4 {
  font-size: var(--jp-content-font-size2);
}

.jp-RenderedHTMLCommon h5 {
  font-size: var(--jp-content-font-size1);
}

.jp-RenderedHTMLCommon h6 {
  font-size: var(--jp-content-font-size0);
}

/* Lists */

/* stylelint-disable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon ul:not(.list-inline),
.jp-RenderedHTMLCommon ol:not(.list-inline) {
  padding-left: 2em;
}

.jp-RenderedHTMLCommon ul {
  list-style: disc;
}

.jp-RenderedHTMLCommon ul ul {
  list-style: square;
}

.jp-RenderedHTMLCommon ul ul ul {
  list-style: circle;
}

.jp-RenderedHTMLCommon ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol ol {
  list-style: upper-alpha;
}

.jp-RenderedHTMLCommon ol ol ol {
  list-style: lower-alpha;
}

.jp-RenderedHTMLCommon ol ol ol ol {
  list-style: lower-roman;
}

.jp-RenderedHTMLCommon ol ol ol ol ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol,
.jp-RenderedHTMLCommon ul {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon ul ul,
.jp-RenderedHTMLCommon ul ol,
.jp-RenderedHTMLCommon ol ul,
.jp-RenderedHTMLCommon ol ol {
  margin-bottom: 0;
}

/* stylelint-enable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon hr {
  color: var(--jp-border-color2);
  background-color: var(--jp-border-color1);
  margin-top: 1em;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon > pre {
  margin: 1.5em 2em;
}

.jp-RenderedHTMLCommon pre,
.jp-RenderedHTMLCommon code {
  border: 0;
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  line-height: var(--jp-code-line-height);
  padding: 0;
  white-space: pre-wrap;
}

.jp-RenderedHTMLCommon :not(pre) > code {
  background-color: var(--jp-layout-color2);
  padding: 1px 5px;
}

/* Tables */

.jp-RenderedHTMLCommon table {
  border-collapse: collapse;
  border-spacing: 0;
  border: none;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  table-layout: fixed;
  margin-left: auto;
  margin-bottom: 1em;
  margin-right: auto;
}

.jp-RenderedHTMLCommon thead {
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  vertical-align: bottom;
}

.jp-RenderedHTMLCommon td,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon tr {
  vertical-align: middle;
  padding: 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}

.jp-RenderedMarkdown.jp-RenderedHTMLCommon td,
.jp-RenderedMarkdown.jp-RenderedHTMLCommon th {
  max-width: none;
}

:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon td,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon th,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon tr {
  text-align: right;
}

.jp-RenderedHTMLCommon th {
  font-weight: bold;
}

.jp-RenderedHTMLCommon tbody tr:nth-child(odd) {
  background: var(--jp-layout-color0);
}

.jp-RenderedHTMLCommon tbody tr:nth-child(even) {
  background: var(--jp-rendermime-table-row-background);
}

.jp-RenderedHTMLCommon tbody tr:hover {
  background: var(--jp-rendermime-table-row-hover-background);
}

.jp-RenderedHTMLCommon p {
  text-align: left;
  margin: 0;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon img {
  -moz-force-broken-image-icon: 1;
}

/* Restrict to direct children as other images could be nested in other content. */
.jp-RenderedHTMLCommon > img {
  display: block;
  margin-left: 0;
  margin-right: 0;
  margin-bottom: 1em;
}

/* Change color behind transparent images if they need it... */
[data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-light-background {
  background-color: var(--jp-inverse-layout-color1);
}

[data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-dark-background {
  background-color: var(--jp-inverse-layout-color1);
}

.jp-RenderedHTMLCommon img,
.jp-RenderedImage img,
.jp-RenderedHTMLCommon svg,
.jp-RenderedSVG svg {
  max-width: 100%;
  height: auto;
}

.jp-RenderedHTMLCommon img.jp-mod-unconfined,
.jp-RenderedImage img.jp-mod-unconfined,
.jp-RenderedHTMLCommon svg.jp-mod-unconfined,
.jp-RenderedSVG svg.jp-mod-unconfined {
  max-width: none;
}

.jp-RenderedHTMLCommon .alert {
  padding: var(--jp-notebook-padding);
  border: var(--jp-border-width) solid transparent;
  border-radius: var(--jp-border-radius);
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon .alert-info {
  color: var(--jp-info-color0);
  background-color: var(--jp-info-color3);
  border-color: var(--jp-info-color2);
}

.jp-RenderedHTMLCommon .alert-info hr {
  border-color: var(--jp-info-color3);
}

.jp-RenderedHTMLCommon .alert-info > p:last-child,
.jp-RenderedHTMLCommon .alert-info > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-warning {
  color: var(--jp-warn-color0);
  background-color: var(--jp-warn-color3);
  border-color: var(--jp-warn-color2);
}

.jp-RenderedHTMLCommon .alert-warning hr {
  border-color: var(--jp-warn-color3);
}

.jp-RenderedHTMLCommon .alert-warning > p:last-child,
.jp-RenderedHTMLCommon .alert-warning > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-success {
  color: var(--jp-success-color0);
  background-color: var(--jp-success-color3);
  border-color: var(--jp-success-color2);
}

.jp-RenderedHTMLCommon .alert-success hr {
  border-color: var(--jp-success-color3);
}

.jp-RenderedHTMLCommon .alert-success > p:last-child,
.jp-RenderedHTMLCommon .alert-success > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-danger {
  color: var(--jp-error-color0);
  background-color: var(--jp-error-color3);
  border-color: var(--jp-error-color2);
}

.jp-RenderedHTMLCommon .alert-danger hr {
  border-color: var(--jp-error-color3);
}

.jp-RenderedHTMLCommon .alert-danger > p:last-child,
.jp-RenderedHTMLCommon .alert-danger > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon blockquote {
  margin: 1em 2em;
  padding: 0 1em;
  border-left: 5px solid var(--jp-border-color2);
}

a.jp-InternalAnchorLink {
  visibility: hidden;
  margin-left: 8px;
  color: var(--md-blue-800);
}

h1:hover .jp-InternalAnchorLink,
h2:hover .jp-InternalAnchorLink,
h3:hover .jp-InternalAnchorLink,
h4:hover .jp-InternalAnchorLink,
h5:hover .jp-InternalAnchorLink,
h6:hover .jp-InternalAnchorLink {
  visibility: visible;
}

.jp-RenderedHTMLCommon kbd {
  background-color: var(--jp-rendermime-table-row-background);
  border: 1px solid var(--jp-border-color0);
  border-bottom-color: var(--jp-border-color2);
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
  display: inline-block;
  font-size: var(--jp-ui-font-size0);
  line-height: 1em;
  padding: 0.2em 0.5em;
}

/* Most direct children of .jp-RenderedHTMLCommon have a margin-bottom of 1.0.
 * At the bottom of cells this is a bit too much as there is also spacing
 * between cells. Going all the way to 0 gets too tight between markdown and
 * code cells.
 */
.jp-RenderedHTMLCommon > *:last-child {
  margin-bottom: 0.5em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-cursor-backdrop {
  position: fixed;
  width: 200px;
  height: 200px;
  margin-top: -100px;
  margin-left: -100px;
  will-change: transform;
  z-index: 100;
}

.lm-mod-drag-image {
  will-change: transform;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-lineFormSearch {
  padding: 4px 12px;
  background-color: var(--jp-layout-color2);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
  font-size: var(--jp-ui-font-size1);
}

.jp-lineFormCaption {
  font-size: var(--jp-ui-font-size0);
  line-height: var(--jp-ui-font-size1);
  margin-top: 4px;
  color: var(--jp-ui-font-color0);
}

.jp-baseLineForm {
  border: none;
  border-radius: 0;
  position: absolute;
  background-size: 16px;
  background-repeat: no-repeat;
  background-position: center;
  outline: none;
}

.jp-lineFormButtonContainer {
  top: 4px;
  right: 8px;
  height: 24px;
  padding: 0 12px;
  width: 12px;
}

.jp-lineFormButtonIcon {
  top: 0;
  right: 0;
  background-color: var(--jp-brand-color1);
  height: 100%;
  width: 100%;
  box-sizing: border-box;
  padding: 4px 6px;
}

.jp-lineFormButton {
  top: 0;
  right: 0;
  background-color: transparent;
  height: 100%;
  width: 100%;
  box-sizing: border-box;
}

.jp-lineFormWrapper {
  overflow: hidden;
  padding: 0 8px;
  border: 1px solid var(--jp-border-color0);
  background-color: var(--jp-input-active-background);
  height: 22px;
}

.jp-lineFormWrapperFocusWithin {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-lineFormInput {
  background: transparent;
  width: 200px;
  height: 100%;
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  line-height: 28px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-JSONEditor {
  display: flex;
  flex-direction: column;
  width: 100%;
}

.jp-JSONEditor-host {
  flex: 1 1 auto;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  background: var(--jp-layout-color0);
  min-height: 50px;
  padding: 1px;
}

.jp-JSONEditor.jp-mod-error .jp-JSONEditor-host {
  border-color: red;
  outline-color: red;
}

.jp-JSONEditor-header {
  display: flex;
  flex: 1 0 auto;
  padding: 0 0 0 12px;
}

.jp-JSONEditor-header label {
  flex: 0 0 auto;
}

.jp-JSONEditor-commitButton {
  height: 16px;
  width: 16px;
  background-size: 18px;
  background-repeat: no-repeat;
  background-position: center;
}

.jp-JSONEditor-host.jp-mod-focused {
  background-color: var(--jp-input-active-background);
  border: 1px solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

.jp-Editor.jp-mod-dropTarget {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/
.jp-DocumentSearch-input {
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  font-size: var(--jp-ui-font-size1);
  background-color: var(--jp-layout-color0);
  font-family: var(--jp-ui-font-family);
  padding: 2px 1px;
  resize: none;
}

.jp-DocumentSearch-overlay {
  position: absolute;
  background-color: var(--jp-toolbar-background);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  border-left: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  top: 0;
  right: 0;
  z-index: 7;
  min-width: 405px;
  padding: 2px;
  font-size: var(--jp-ui-font-size1);

  --jp-private-document-search-button-height: 20px;
}

.jp-DocumentSearch-overlay button {
  background-color: var(--jp-toolbar-background);
  outline: 0;
}

.jp-DocumentSearch-overlay button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-overlay button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-overlay-row {
  display: flex;
  align-items: center;
  margin-bottom: 2px;
}

.jp-DocumentSearch-button-content {
  display: inline-block;
  cursor: pointer;
  box-sizing: border-box;
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-button-content svg {
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-input-wrapper {
  border: var(--jp-border-width) solid var(--jp-border-color0);
  display: flex;
  background-color: var(--jp-layout-color0);
  margin: 2px;
}

.jp-DocumentSearch-input-wrapper:focus-within {
  border-color: var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper {
  all: initial;
  overflow: hidden;
  display: inline-block;
  border: none;
  box-sizing: border-box;
}

.jp-DocumentSearch-toggle-wrapper {
  width: 14px;
  height: 14px;
}

.jp-DocumentSearch-button-wrapper {
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
}

.jp-DocumentSearch-toggle-wrapper:focus,
.jp-DocumentSearch-button-wrapper:focus {
  outline: var(--jp-border-width) solid
    var(--jp-cell-editor-active-border-color);
  outline-offset: -1px;
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper,
.jp-DocumentSearch-button-content:focus {
  outline: none;
}

.jp-DocumentSearch-toggle-placeholder {
  width: 5px;
}

.jp-DocumentSearch-input-button::before {
  display: block;
  padding-top: 100%;
}

.jp-DocumentSearch-input-button-off {
  opacity: var(--jp-search-toggle-off-opacity);
}

.jp-DocumentSearch-input-button-off:hover {
  opacity: var(--jp-search-toggle-hover-opacity);
}

.jp-DocumentSearch-input-button-on {
  opacity: var(--jp-search-toggle-on-opacity);
}

.jp-DocumentSearch-index-counter {
  padding-left: 10px;
  padding-right: 10px;
  user-select: none;
  min-width: 35px;
  display: inline-block;
}

.jp-DocumentSearch-up-down-wrapper {
  display: inline-block;
  padding-right: 2px;
  margin-left: auto;
  white-space: nowrap;
}

.jp-DocumentSearch-spacer {
  margin-left: auto;
}

.jp-DocumentSearch-up-down-wrapper button {
  outline: 0;
  border: none;
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
  vertical-align: middle;
  margin: 1px 5px 2px;
}

.jp-DocumentSearch-up-down-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-up-down-button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-filter-button {
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-filter-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled:hover {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-search-options {
  padding: 0 8px;
  margin-left: 3px;
  width: 100%;
  display: grid;
  justify-content: start;
  grid-template-columns: 1fr 1fr;
  align-items: center;
  justify-items: stretch;
}

.jp-DocumentSearch-search-filter-disabled {
  color: var(--jp-ui-font-color2);
}

.jp-DocumentSearch-search-filter {
  display: flex;
  align-items: center;
  user-select: none;
}

.jp-DocumentSearch-regex-error {
  color: var(--jp-error-color0);
}

.jp-DocumentSearch-replace-button-wrapper {
  overflow: hidden;
  display: inline-block;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color0);
  margin: auto 2px;
  padding: 1px 4px;
  height: calc(var(--jp-private-document-search-button-height) + 2px);
}

.jp-DocumentSearch-replace-button-wrapper:focus {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-replace-button {
  display: inline-block;
  text-align: center;
  cursor: pointer;
  box-sizing: border-box;
  color: var(--jp-ui-font-color1);

  /* height - 2 * (padding of wrapper) */
  line-height: calc(var(--jp-private-document-search-button-height) - 2px);
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-replace-button:focus {
  outline: none;
}

.jp-DocumentSearch-replace-wrapper-class {
  margin-left: 14px;
  display: flex;
}

.jp-DocumentSearch-replace-toggle {
  border: none;
  background-color: var(--jp-toolbar-background);
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-replace-toggle:hover {
  background-color: var(--jp-layout-color2);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.cm-editor {
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  border: 0;
  border-radius: 0;
  height: auto;

  /* Changed to auto to autogrow */
}

.cm-editor pre {
  padding: 0 var(--jp-code-padding);
}

.jp-CodeMirrorEditor[data-type='inline'] .cm-dialog {
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
}

.jp-CodeMirrorEditor {
  cursor: text;
}

/* When zoomed out 67% and 33% on a screen of 1440 width x 900 height */
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width1) solid
      var(--jp-editor-cursor-color);
  }
}

/* When zoomed out less than 33% */
@media screen and (min-width: 4320px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width2) solid
      var(--jp-editor-cursor-color);
  }
}

.cm-editor.jp-mod-readOnly .cm-cursor {
  display: none;
}

.jp-CollaboratorCursor {
  border-left: 5px solid transparent;
  border-right: 5px solid transparent;
  border-top: none;
  border-bottom: 3px solid;
  background-clip: content-box;
  margin-left: -5px;
  margin-right: -5px;
}

.cm-searching,
.cm-searching span {
  /* `.cm-searching span`: we need to override syntax highlighting */
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.cm-searching::selection,
.cm-searching span::selection {
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.jp-current-match > .cm-searching,
.jp-current-match > .cm-searching span,
.cm-searching > .jp-current-match,
.cm-searching > .jp-current-match span {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.jp-current-match > .cm-searching::selection,
.cm-searching > .jp-current-match::selection,
.jp-current-match > .cm-searching span::selection {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.cm-trailingspace {
  background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAYAAAB4ka1VAAAAsElEQVQIHQGlAFr/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7+r3zKmT0/+pk9P/7+r3zAAAAAAAAAAABAAAAAAAAAAA6OPzM+/q9wAAAAAA6OPzMwAAAAAAAAAAAgAAAAAAAAAAGR8NiRQaCgAZIA0AGR8NiQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQyoYJ/SY80UAAAAASUVORK5CYII=);
  background-position: center left;
  background-repeat: repeat-x;
}

.jp-CollaboratorCursor-hover {
  position: absolute;
  z-index: 1;
  transform: translateX(-50%);
  color: white;
  border-radius: 3px;
  padding-left: 4px;
  padding-right: 4px;
  padding-top: 1px;
  padding-bottom: 1px;
  text-align: center;
  font-size: var(--jp-ui-font-size1);
  white-space: nowrap;
}

.jp-CodeMirror-ruler {
  border-left: 1px dashed var(--jp-border-color2);
}

/* Styles for shared cursors (remote cursor locations and selected ranges) */
.jp-CodeMirrorEditor .cm-ySelectionCaret {
  position: relative;
  border-left: 1px solid black;
  margin-left: -1px;
  margin-right: -1px;
  box-sizing: border-box;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret > .cm-ySelectionInfo {
  white-space: nowrap;
  position: absolute;
  top: -1.15em;
  padding-bottom: 0.05em;
  left: -1px;
  font-size: 0.95em;
  font-family: var(--jp-ui-font-family);
  font-weight: bold;
  line-height: normal;
  user-select: none;
  color: white;
  padding-left: 2px;
  padding-right: 2px;
  z-index: 101;
  transition: opacity 0.3s ease-in-out;
}

.jp-CodeMirrorEditor .cm-ySelectionInfo {
  transition-delay: 0.7s;
  opacity: 0;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret:hover > .cm-ySelectionInfo {
  opacity: 1;
  transition-delay: 0s;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MimeDocument {
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-filebrowser-button-height: 28px;
  --jp-private-filebrowser-button-width: 48px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FileBrowser .jp-SidePanel-content {
  display: flex;
  flex-direction: column;
}

.jp-FileBrowser-toolbar.jp-Toolbar {
  flex-wrap: wrap;
  row-gap: 12px;
  border-bottom: none;
  height: auto;
  margin: 8px 12px 0;
  box-shadow: none;
  padding: 0;
  justify-content: flex-start;
}

.jp-FileBrowser-Panel {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
}

.jp-BreadCrumbs {
  flex: 0 0 auto;
  margin: 8px 12px;
}

.jp-BreadCrumbs-item {
  margin: 0 2px;
  padding: 0 2px;
  border-radius: var(--jp-border-radius);
  cursor: pointer;
}

.jp-BreadCrumbs-item:hover {
  background-color: var(--jp-layout-color2);
}

.jp-BreadCrumbs-item:first-child {
  margin-left: 0;
}

.jp-BreadCrumbs-item.jp-mod-dropTarget {
  background-color: var(--jp-brand-color2);
  opacity: 0.7;
}

/*-----------------------------------------------------------------------------
| Buttons
|----------------------------------------------------------------------------*/

.jp-FileBrowser-toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  padding-left: 0;
  padding-right: 2px;
  align-items: center;
  height: unset;
}

.jp-FileBrowser-toolbar > .jp-Toolbar-item .jp-ToolbarButtonComponent {
  width: 40px;
}

/*-----------------------------------------------------------------------------
| Other styles
|----------------------------------------------------------------------------*/

.jp-FileDialog.jp-mod-conflict input {
  color: var(--jp-error-color1);
}

.jp-FileDialog .jp-new-name-title {
  margin-top: 12px;
}

.jp-LastModified-hidden {
  display: none;
}

.jp-FileSize-hidden {
  display: none;
}

.jp-FileBrowser .lm-AccordionPanel > h3:first-child {
  display: none;
}

/*-----------------------------------------------------------------------------
| DirListing
|----------------------------------------------------------------------------*/

.jp-DirListing {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
  outline: 0;
}

.jp-DirListing-header {
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  align-items: center;
  overflow: hidden;
  border-top: var(--jp-border-width) solid var(--jp-border-color2);
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
}

.jp-DirListing-headerItem {
  padding: 4px 12px 2px;
  font-weight: 500;
}

.jp-DirListing-headerItem:hover {
  background: var(--jp-layout-color2);
}

.jp-DirListing-headerItem.jp-id-name {
  flex: 1 0 84px;
}

.jp-DirListing-headerItem.jp-id-modified {
  flex: 0 0 112px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-DirListing-headerItem.jp-id-filesize {
  flex: 0 0 75px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-id-narrow {
  display: none;
  flex: 0 0 5px;
  padding: 4px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
  color: var(--jp-border-color2);
}

.jp-DirListing-narrow .jp-id-narrow {
  display: block;
}

.jp-DirListing-narrow .jp-id-modified,
.jp-DirListing-narrow .jp-DirListing-itemModified {
  display: none;
}

.jp-DirListing-headerItem.jp-mod-selected {
  font-weight: 600;
}

/* increase specificity to override bundled default */
.jp-DirListing-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  list-style-type: none;
  overflow: auto;
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-content mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.jp-DirListing-content .jp-DirListing-item.jp-mod-selected mark {
  color: var(--jp-ui-inverse-font-color0);
}

/* Style the directory listing content when a user drops a file to upload */
.jp-DirListing.jp-mod-native-drop .jp-DirListing-content {
  outline: 5px dashed rgba(128, 128, 128, 0.5);
  outline-offset: -10px;
  cursor: copy;
}

.jp-DirListing-item {
  display: flex;
  flex-direction: row;
  align-items: center;
  padding: 4px 12px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-DirListing-checkboxWrapper {
  /* Increases hit area of checkbox. */
  padding: 4px;
}

.jp-DirListing-header
  .jp-DirListing-checkboxWrapper
  + .jp-DirListing-headerItem {
  padding-left: 4px;
}

.jp-DirListing-content .jp-DirListing-checkboxWrapper {
  position: relative;
  left: -4px;
  margin: -4px 0 -4px -8px;
}

.jp-DirListing-checkboxWrapper.jp-mod-visible {
  visibility: visible;
}

/* For devices that support hovering, hide checkboxes until hovered, selected...
*/
@media (hover: hover) {
  .jp-DirListing-checkboxWrapper {
    visibility: hidden;
  }

  .jp-DirListing-item:hover .jp-DirListing-checkboxWrapper,
  .jp-DirListing-item.jp-mod-selected .jp-DirListing-checkboxWrapper {
    visibility: visible;
  }
}

.jp-DirListing-item[data-is-dot] {
  opacity: 75%;
}

.jp-DirListing-item.jp-mod-selected {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.jp-DirListing-item.jp-mod-dropTarget {
  background: var(--jp-brand-color3);
}

.jp-DirListing-item:hover:not(.jp-mod-selected) {
  background: var(--jp-layout-color2);
}

.jp-DirListing-itemIcon {
  flex: 0 0 20px;
  margin-right: 4px;
}

.jp-DirListing-itemText {
  flex: 1 0 64px;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  user-select: none;
}

.jp-DirListing-itemText:focus {
  outline-width: 2px;
  outline-color: var(--jp-inverse-layout-color1);
  outline-style: solid;
  outline-offset: 1px;
}

.jp-DirListing-item.jp-mod-selected .jp-DirListing-itemText:focus {
  outline-color: var(--jp-layout-color1);
}

.jp-DirListing-itemModified {
  flex: 0 0 125px;
  text-align: right;
}

.jp-DirListing-itemFileSize {
  flex: 0 0 90px;
  text-align: right;
}

.jp-DirListing-editor {
  flex: 1 0 64px;
  outline: none;
  border: none;
  color: var(--jp-ui-font-color1);
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon::before {
  color: var(--jp-success-color1);
  content: '\25CF';
  font-size: 8px;
  position: absolute;
  left: -8px;
}

.jp-DirListing-item.jp-mod-running.jp-mod-selected
  .jp-DirListing-itemIcon::before {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-DirListing-item.lm-mod-drag-image,
.jp-DirListing-item.jp-mod-selected.lm-mod-drag-image {
  font-size: var(--jp-ui-font-size1);
  padding-left: 4px;
  margin-left: 4px;
  width: 160px;
  background-color: var(--jp-ui-inverse-font-color2);
  box-shadow: var(--jp-elevation-z2);
  border-radius: 0;
  color: var(--jp-ui-font-color1);
  transform: translateX(-40%) translateY(-58%);
}

.jp-Document {
  min-width: 120px;
  min-height: 120px;
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Main OutputArea
| OutputArea has a list of Outputs
|----------------------------------------------------------------------------*/

.jp-OutputArea {
  overflow-y: auto;
}

.jp-OutputArea-child {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-OutputPrompt {
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-outprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
  opacity: var(--jp-cell-prompt-opacity);

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-OutputArea-prompt {
  display: table-cell;
  vertical-align: top;
}

.jp-OutputArea-output {
  display: table-cell;
  width: 100%;
  height: auto;
  overflow: auto;
  user-select: text;
  -moz-user-select: text;
  -webkit-user-select: text;
  -ms-user-select: text;
}

.jp-OutputArea .jp-RenderedText {
  padding-left: 1ch;
}

/**
 * Prompt overlay.
 */

.jp-OutputArea-promptOverlay {
  position: absolute;
  top: 0;
  width: var(--jp-cell-prompt-width);
  height: 100%;
  opacity: 0.5;
}

.jp-OutputArea-promptOverlay:hover {
  background: var(--jp-layout-color2);
  box-shadow: inset 0 0 1px var(--jp-inverse-layout-color0);
  cursor: zoom-out;
}

.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay:hover {
  cursor: zoom-in;
}

/**
 * Isolated output.
 */
.jp-OutputArea-output.jp-mod-isolated {
  width: 100%;
  display: block;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated {
  position: relative;
}

body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/* pre */

.jp-OutputArea-output pre {
  border: none;
  margin: 0;
  padding: 0;
  overflow-x: auto;
  overflow-y: auto;
  word-break: break-all;
  word-wrap: break-word;
  white-space: pre-wrap;
}

/* tables */

.jp-OutputArea-output.jp-RenderedHTMLCommon table {
  margin-left: 0;
  margin-right: 0;
}

/* description lists */

.jp-OutputArea-output dl,
.jp-OutputArea-output dt,
.jp-OutputArea-output dd {
  display: block;
}

.jp-OutputArea-output dl {
  width: 100%;
  overflow: hidden;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dt {
  font-weight: bold;
  float: left;
  width: 20%;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dd {
  float: left;
  width: 80%;
  padding: 0;
  margin: 0;
}

.jp-TrimmedOutputs pre {
  background: var(--jp-layout-color3);
  font-size: calc(var(--jp-code-font-size) * 1.4);
  text-align: center;
  text-transform: uppercase;
}

/* Hide the gutter in case of
 *  - nested output areas (e.g. in the case of output widgets)
 *  - mirrored output areas
 */
.jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt {
  display: none;
}

/* Hide empty lines in the output area, for instance due to cleared widgets */
.jp-OutputArea-prompt:empty {
  padding: 0;
  border: 0;
}

/*-----------------------------------------------------------------------------
| executeResult is added to any Output-result for the display of the object
| returned by a cell
|----------------------------------------------------------------------------*/

.jp-OutputArea-output.jp-OutputArea-executeResult {
  margin-left: 0;
  width: 100%;
}

/* Text output with the Out[] prompt needs a top padding to match the
 * alignment of the Out[] prompt itself.
 */
.jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output {
  padding-top: var(--jp-code-padding);
  border-top: var(--jp-border-width) solid transparent;
}

/*-----------------------------------------------------------------------------
| The Stdin output
|----------------------------------------------------------------------------*/

.jp-Stdin-prompt {
  color: var(--jp-content-font-color0);
  padding-right: var(--jp-code-padding);
  vertical-align: baseline;
  flex: 0 0 auto;
}

.jp-Stdin-input {
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  color: inherit;
  background-color: inherit;
  width: 42%;
  min-width: 200px;

  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;

  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0 0.25em;
  margin: 0 0.25em;
  flex: 0 0 70%;
}

.jp-Stdin-input::placeholder {
  opacity: 0;
}

.jp-Stdin-input:focus {
  box-shadow: none;
}

.jp-Stdin-input:focus::placeholder {
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Output Area View
|----------------------------------------------------------------------------*/

.jp-LinkedOutputView .jp-OutputArea {
  height: 100%;
  display: block;
}

.jp-LinkedOutputView .jp-OutputArea-output:only-child {
  height: 100%;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

@media print {
  .jp-OutputArea-child {
    break-inside: avoid-page;
  }
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-OutputPrompt {
    display: table-row;
    text-align: left;
  }

  .jp-OutputArea-child .jp-OutputArea-output {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }
}

/* Trimmed outputs warning */
.jp-TrimmedOutputs > a {
  margin: 10px;
  text-decoration: none;
  cursor: pointer;
}

.jp-TrimmedOutputs > a:hover {
  text-decoration: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Table of Contents
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toc-active-width: 4px;
}

.jp-TableOfContents {
  display: flex;
  flex-direction: column;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  height: 100%;
}

.jp-TableOfContents-placeholder {
  text-align: center;
}

.jp-TableOfContents-placeholderContent {
  color: var(--jp-content-font-color2);
  padding: 8px;
}

.jp-TableOfContents-placeholderContent > h3 {
  margin-bottom: var(--jp-content-heading-margin-bottom);
}

.jp-TableOfContents .jp-SidePanel-content {
  overflow-y: auto;
}

.jp-TableOfContents-tree {
  margin: 4px;
}

.jp-TableOfContents ol {
  list-style-type: none;
}

/* stylelint-disable-next-line selector-max-type */
.jp-TableOfContents li > ol {
  /* Align left border with triangle icon center */
  padding-left: 11px;
}

.jp-TableOfContents-content {
  /* left margin for the active heading indicator */
  margin: 0 0 0 var(--jp-private-toc-active-width);
  padding: 0;
  background-color: var(--jp-layout-color1);
}

.jp-tocItem {
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-tocItem-heading {
  display: flex;
  cursor: pointer;
}

.jp-tocItem-heading:hover {
  background-color: var(--jp-layout-color2);
}

.jp-tocItem-content {
  display: block;
  padding: 4px 0;
  white-space: nowrap;
  text-overflow: ellipsis;
  overflow-x: hidden;
}

.jp-tocItem-collapser {
  height: 20px;
  margin: 2px 2px 0;
  padding: 0;
  background: none;
  border: none;
  cursor: pointer;
}

.jp-tocItem-collapser:hover {
  background-color: var(--jp-layout-color3);
}

/* Active heading indicator */

.jp-tocItem-heading::before {
  content: ' ';
  background: transparent;
  width: var(--jp-private-toc-active-width);
  height: 24px;
  position: absolute;
  left: 0;
  border-radius: var(--jp-border-radius);
}

.jp-tocItem-heading.jp-tocItem-active::before {
  background-color: var(--jp-brand-color1);
}

.jp-tocItem-heading:hover.jp-tocItem-active::before {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapser {
  flex: 0 0 var(--jp-cell-collapser-width);
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
  border-radius: var(--jp-border-radius);
  opacity: 1;
}

.jp-Collapser-child {
  display: block;
  width: 100%;
  box-sizing: border-box;

  /* height: 100% doesn't work because the height of its parent is computed from content */
  position: absolute;
  top: 0;
  bottom: 0;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Hiding collapsers in print mode.

Note: input and output wrappers have "display: block" propery in print mode.
*/

@media print {
  .jp-Collapser {
    display: none;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Header/Footer
|----------------------------------------------------------------------------*/

/* Hidden by zero height by default */
.jp-CellHeader,
.jp-CellFooter {
  height: 0;
  width: 100%;
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Input
|----------------------------------------------------------------------------*/

/* All input areas */
.jp-InputArea {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-InputArea-editor {
  display: table-cell;
  overflow: hidden;
  vertical-align: top;

  /* This is the non-active, default styling */
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  background: var(--jp-cell-editor-background);
}

.jp-InputPrompt {
  display: table-cell;
  vertical-align: top;
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-inprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  opacity: var(--jp-cell-prompt-opacity);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-InputArea-editor {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }

  .jp-InputPrompt {
    display: table-row;
    text-align: left;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Placeholder {
  display: table;
  table-layout: fixed;
  width: 100%;
}

.jp-Placeholder-prompt {
  display: table-cell;
  box-sizing: border-box;
}

.jp-Placeholder-content {
  display: table-cell;
  padding: 4px 6px;
  border: 1px solid transparent;
  border-radius: 0;
  background: none;
  box-sizing: border-box;
  cursor: pointer;
}

.jp-Placeholder-contentContainer {
  display: flex;
}

.jp-Placeholder-content:hover,
.jp-InputPlaceholder > .jp-Placeholder-content:hover {
  border-color: var(--jp-layout-color3);
}

.jp-Placeholder-content .jp-MoreHorizIcon {
  width: 32px;
  height: 16px;
  border: 1px solid transparent;
  border-radius: var(--jp-border-radius);
}

.jp-Placeholder-content .jp-MoreHorizIcon:hover {
  border: 1px solid var(--jp-border-color1);
  box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.25);
  background-color: var(--jp-layout-color0);
}

.jp-PlaceholderText {
  white-space: nowrap;
  overflow-x: hidden;
  color: var(--jp-inverse-layout-color3);
  font-family: var(--jp-code-font-family);
}

.jp-InputPlaceholder > .jp-Placeholder-content {
  border-color: var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-cell-scrolling-output-offset: 5px;
}

/*-----------------------------------------------------------------------------
| Cell
|----------------------------------------------------------------------------*/

.jp-Cell {
  padding: var(--jp-cell-padding);
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Common input/output
|----------------------------------------------------------------------------*/

.jp-Cell-inputWrapper,
.jp-Cell-outputWrapper {
  display: flex;
  flex-direction: row;
  padding: 0;
  margin: 0;

  /* Added to reveal the box-shadow on the input and output collapsers. */
  overflow: visible;
}

/* Only input/output areas inside cells */
.jp-Cell-inputArea,
.jp-Cell-outputArea {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Collapser
|----------------------------------------------------------------------------*/

/* Make the output collapser disappear when there is not output, but do so
 * in a manner that leaves it in the layout and preserves its width.
 */
.jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser {
  border: none !important;
  background: transparent !important;
}

.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser {
  min-height: var(--jp-cell-collapser-min-height);
}

/*-----------------------------------------------------------------------------
| Output
|----------------------------------------------------------------------------*/

/* Put a space between input and output when there IS output */
.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper {
  margin-top: 5px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea {
  overflow-y: auto;
  max-height: 24em;
  margin-left: var(--jp-private-cell-scrolling-output-offset);
  resize: vertical;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea[style*='height'] {
  max-height: unset;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea::after {
  content: ' ';
  box-shadow: inset 0 0 6px 2px rgb(0 0 0 / 30%);
  width: 100%;
  height: 100%;
  position: sticky;
  bottom: 0;
  top: 0;
  margin-top: -50%;
  float: left;
  display: block;
  pointer-events: none;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-child {
  padding-top: 6px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt {
  width: calc(
    var(--jp-cell-prompt-width) - var(--jp-private-cell-scrolling-output-offset)
  );
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay {
  left: calc(-1 * var(--jp-private-cell-scrolling-output-offset));
}

/*-----------------------------------------------------------------------------
| CodeCell
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| MarkdownCell
|----------------------------------------------------------------------------*/

.jp-MarkdownOutput {
  display: table-cell;
  width: 100%;
  margin-top: 0;
  margin-bottom: 0;
  padding-left: var(--jp-code-padding);
}

.jp-MarkdownOutput.jp-RenderedHTMLCommon {
  overflow: auto;
}

/* collapseHeadingButton (show always if hiddenCellsButton is _not_ shown) */
.jp-collapseHeadingButton {
  display: flex;
  min-height: var(--jp-cell-collapser-min-height);
  font-size: var(--jp-code-font-size);
  position: absolute;
  background-color: transparent;
  background-size: 25px;
  background-repeat: no-repeat;
  background-position-x: center;
  background-position-y: top;
  background-image: var(--jp-icon-caret-down);
  right: 0;
  top: 0;
  bottom: 0;
}

.jp-collapseHeadingButton.jp-mod-collapsed {
  background-image: var(--jp-icon-caret-right);
}

/*
 set the container font size to match that of content
 so that the nested collapse buttons have the right size
*/
.jp-MarkdownCell .jp-InputPrompt {
  font-size: var(--jp-content-font-size1);
}

/*
  Align collapseHeadingButton with cell top header
  The font sizes are identical to the ones in packages/rendermime/style/base.css
*/
.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='1'] {
  font-size: var(--jp-content-font-size5);
  background-position-y: calc(0.3 * var(--jp-content-font-size5));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='2'] {
  font-size: var(--jp-content-font-size4);
  background-position-y: calc(0.3 * var(--jp-content-font-size4));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='3'] {
  font-size: var(--jp-content-font-size3);
  background-position-y: calc(0.3 * var(--jp-content-font-size3));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='4'] {
  font-size: var(--jp-content-font-size2);
  background-position-y: calc(0.3 * var(--jp-content-font-size2));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='5'] {
  font-size: var(--jp-content-font-size1);
  background-position-y: top;
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='6'] {
  font-size: var(--jp-content-font-size0);
  background-position-y: top;
}

/* collapseHeadingButton (show only on (hover,active) if hiddenCellsButton is shown) */
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-collapseHeadingButton {
  display: none;
}

.jp-Notebook.jp-mod-showHiddenCellsButton
  :is(.jp-MarkdownCell:hover, .jp-mod-active)
  .jp-collapseHeadingButton {
  display: flex;
}

/* showHiddenCellsButton (only show if jp-mod-showHiddenCellsButton is set, which
is a consequence of the showHiddenCellsButton option in Notebook Settings)*/
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton {
  margin-left: calc(var(--jp-cell-prompt-width) + 2 * var(--jp-code-padding));
  margin-top: var(--jp-code-padding);
  border: 1px solid var(--jp-border-color2);
  background-color: var(--jp-border-color3) !important;
  color: var(--jp-content-font-color0) !important;
  display: flex;
}

.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton:hover {
  background-color: var(--jp-border-color2) !important;
}

.jp-showHiddenCellsButton {
  display: none;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Using block instead of flex to allow the use of the break-inside CSS property for
cell outputs.
*/

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-notebook-toolbar-padding: 2px 5px 2px 2px;
}

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-NotebookPanel-toolbar {
  padding: var(--jp-notebook-toolbar-padding);

  /* disable paint containment from lumino 2.0 default strict CSS containment */
  contain: style size !important;
}

.jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused {
  border: none;
  box-shadow: none;
}

.jp-Notebook-toolbarCellTypeDropdown select {
  height: 24px;
  font-size: var(--jp-ui-font-size1);
  line-height: 14px;
  border-radius: 0;
  display: block;
}

.jp-Notebook-toolbarCellTypeDropdown span {
  top: 5px !important;
}

.jp-Toolbar-responsive-popup {
  position: absolute;
  height: fit-content;
  display: flex;
  flex-direction: row;
  flex-wrap: wrap;
  justify-content: flex-end;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: var(--jp-notebook-toolbar-padding);
  z-index: 1;
  right: 0;
  top: 0;
}

.jp-Toolbar > .jp-Toolbar-responsive-opener {
  margin-left: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-Notebook-ExecutionIndicator {
  position: relative;
  display: inline-block;
  height: 100%;
  z-index: 9997;
}

.jp-Notebook-ExecutionIndicator-tooltip {
  visibility: hidden;
  height: auto;
  width: max-content;
  width: -moz-max-content;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color1);
  text-align: justify;
  border-radius: 6px;
  padding: 0 5px;
  position: fixed;
  display: table;
}

.jp-Notebook-ExecutionIndicator-tooltip.up {
  transform: translateX(-50%) translateY(-100%) translateY(-32px);
}

.jp-Notebook-ExecutionIndicator-tooltip.down {
  transform: translateX(calc(-100% + 16px)) translateY(5px);
}

.jp-Notebook-ExecutionIndicator-tooltip.hidden {
  display: none;
}

.jp-Notebook-ExecutionIndicator:hover .jp-Notebook-ExecutionIndicator-tooltip {
  visibility: visible;
}

.jp-Notebook-ExecutionIndicator span {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  color: var(--jp-ui-font-color1);
  line-height: 24px;
  display: block;
}

.jp-Notebook-ExecutionIndicator-progress-bar {
  display: flex;
  justify-content: center;
  height: 100%;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*
 * Execution indicator
 */
.jp-tocItem-content::after {
  content: '';

  /* Must be identical to form a circle */
  width: 12px;
  height: 12px;
  background: none;
  border: none;
  position: absolute;
  right: 0;
}

.jp-tocItem-content[data-running='0']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background: none;
}

.jp-tocItem-content[data-running='1']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background-color: var(--jp-inverse-layout-color3);
}

.jp-tocItem-content[data-running='0'],
.jp-tocItem-content[data-running='1'] {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Notebook-footer {
  height: 27px;
  margin-left: calc(
    var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
      var(--jp-cell-padding)
  );
  width: calc(
    100% -
      (
        var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
          var(--jp-cell-padding) + var(--jp-cell-padding)
      )
  );
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  color: var(--jp-ui-font-color3);
  margin-top: 6px;
  background: none;
  cursor: pointer;
}

.jp-Notebook-footer:focus {
  border-color: var(--jp-cell-editor-active-border-color);
}

/* For devices that support hovering, hide footer until hover */
@media (hover: hover) {
  .jp-Notebook-footer {
    opacity: 0;
  }

  .jp-Notebook-footer:focus,
  .jp-Notebook-footer:hover {
    opacity: 1;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Imports
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-side-by-side-output-size: 1fr;
  --jp-side-by-side-resized-cell: var(--jp-side-by-side-output-size);
  --jp-private-notebook-dragImage-width: 304px;
  --jp-private-notebook-dragImage-height: 36px;
  --jp-private-notebook-selected-color: var(--md-blue-400);
  --jp-private-notebook-active-color: var(--md-green-400);
}

/*-----------------------------------------------------------------------------
| Notebook
|----------------------------------------------------------------------------*/

/* stylelint-disable selector-max-class */

.jp-NotebookPanel {
  display: block;
  height: 100%;
}

.jp-NotebookPanel.jp-Document {
  min-width: 240px;
  min-height: 120px;
}

.jp-Notebook {
  padding: var(--jp-notebook-padding);
  outline: none;
  overflow: auto;
  background: var(--jp-layout-color0);
}

.jp-Notebook.jp-mod-scrollPastEnd::after {
  display: block;
  content: '';
  min-height: var(--jp-notebook-scroll-padding);
}

.jp-MainAreaWidget-ContainStrict .jp-Notebook * {
  contain: strict;
}

.jp-Notebook .jp-Cell {
  overflow: visible;
}

.jp-Notebook .jp-Cell .jp-InputPrompt {
  cursor: move;
}

/*-----------------------------------------------------------------------------
| Notebook state related styling
|
| The notebook and cells each have states, here are the possibilities:
|
| - Notebook
|   - Command
|   - Edit
| - Cell
|   - None
|   - Active (only one can be active)
|   - Selected (the cells actions are applied to)
|   - Multiselected (when multiple selected, the cursor)
|   - No outputs
|----------------------------------------------------------------------------*/

/* Command or edit modes */

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

/* cell is active */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser {
  background: var(--jp-brand-color1);
}

/* cell is dirty */
.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt {
  color: var(--jp-warn-color1);
}

.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt::before {
  color: var(--jp-warn-color1);
  content: '•';
}

.jp-Notebook .jp-Cell.jp-mod-active.jp-mod-dirty .jp-Collapser {
  background: var(--jp-warn-color1);
}

/* collapser is hovered */
.jp-Notebook .jp-Cell .jp-Collapser:hover {
  box-shadow: var(--jp-elevation-z2);
  background: var(--jp-brand-color1);
  opacity: var(--jp-cell-collapser-not-active-hover-opacity);
}

/* cell is active and collapser is hovered */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/* Command mode */

.jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected {
  background: var(--jp-notebook-multiselected-color);
}

.jp-Notebook.jp-mod-commandMode
  .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected) {
  background: transparent;
}

/* Edit mode */

.jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-cell-editor-active-background);
}

/*-----------------------------------------------------------------------------
| Notebook drag and drop
|----------------------------------------------------------------------------*/

.jp-Notebook-cell.jp-mod-dropSource {
  opacity: 0.5;
}

.jp-Notebook-cell.jp-mod-dropTarget,
.jp-Notebook.jp-mod-commandMode
  .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget {
  border-top-color: var(--jp-private-notebook-selected-color);
  border-top-style: solid;
  border-top-width: 2px;
}

.jp-dragImage {
  display: block;
  flex-direction: row;
  width: var(--jp-private-notebook-dragImage-width);
  height: var(--jp-private-notebook-dragImage-height);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
  overflow: visible;
}

.jp-dragImage-singlePrompt {
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

.jp-dragImage .jp-dragImage-content {
  flex: 1 1 auto;
  z-index: 2;
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  line-height: var(--jp-code-line-height);
  padding: var(--jp-code-padding);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background-color);
  color: var(--jp-content-font-color3);
  text-align: left;
  margin: 4px 4px 4px 0;
}

.jp-dragImage .jp-dragImage-prompt {
  flex: 0 0 auto;
  min-width: 36px;
  color: var(--jp-cell-inprompt-font-color);
  padding: var(--jp-code-padding);
  padding-left: 12px;
  font-family: var(--jp-cell-prompt-font-family);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: 1.9;
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
}

.jp-dragImage-multipleBack {
  z-index: -1;
  position: absolute;
  height: 32px;
  width: 300px;
  top: 8px;
  left: 8px;
  background: var(--jp-layout-color2);
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

/*-----------------------------------------------------------------------------
| Cell toolbar
|----------------------------------------------------------------------------*/

.jp-NotebookTools {
  display: block;
  min-width: var(--jp-sidebar-min-width);
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
    * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  overflow: auto;
}

.jp-ActiveCellTool {
  padding: 12px 0;
  display: flex;
}

.jp-ActiveCellTool-Content {
  flex: 1 1 auto;
}

.jp-ActiveCellTool .jp-ActiveCellTool-CellContent {
  background: var(--jp-cell-editor-background);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  min-height: 29px;
}

.jp-ActiveCellTool .jp-InputPrompt {
  min-width: calc(var(--jp-cell-prompt-width) * 0.75);
}

.jp-ActiveCellTool-CellContent > pre {
  padding: 5px 4px;
  margin: 0;
  white-space: normal;
}

.jp-MetadataEditorTool {
  flex-direction: column;
  padding: 12px 0;
}

.jp-RankedPanel > :not(:first-child) {
  margin-top: 12px;
}

.jp-KeySelector select.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: var(--jp-border-width) solid var(--jp-border-color1);
}

.jp-KeySelector label,
.jp-MetadataEditorTool label,
.jp-NumberSetter label {
  line-height: 1.4;
}

.jp-NotebookTools .jp-select-wrapper {
  margin-top: 4px;
  margin-bottom: 0;
}

.jp-NumberSetter input {
  width: 100%;
  margin-top: 4px;
}

.jp-NotebookTools .jp-Collapse {
  margin-top: 16px;
}

/*-----------------------------------------------------------------------------
| Presentation Mode (.jp-mod-presentationMode)
|----------------------------------------------------------------------------*/

.jp-mod-presentationMode .jp-Notebook {
  --jp-content-font-size1: var(--jp-content-presentation-font-size1);
  --jp-code-font-size: var(--jp-code-presentation-font-size);
}

.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt,
.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt {
  flex: 0 0 110px;
}

/*-----------------------------------------------------------------------------
| Side-by-side Mode (.jp-mod-sideBySide)
|----------------------------------------------------------------------------*/
.jp-mod-sideBySide.jp-Notebook .jp-Notebook-cell {
  margin-top: 3em;
  margin-bottom: 3em;
  margin-left: 5%;
  margin-right: 5%;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell {
  display: grid;
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-output-size)
    );
  grid-template-rows: auto minmax(0, 1fr) auto;
  grid-template-areas:
    'header header header'
    'input handle output'
    'footer footer footer';
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell.jp-mod-resizedCell {
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-resized-cell)
    );
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellHeader {
  grid-area: header;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-inputWrapper {
  grid-area: input;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-outputWrapper {
  /* overwrite the default margin (no vertical separation needed in side by side move */
  margin-top: 0;
  grid-area: output;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellFooter {
  grid-area: footer;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle {
  grid-area: handle;
  user-select: none;
  display: block;
  height: 100%;
  cursor: ew-resize;
  padding: 0 var(--jp-cell-padding);
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle::after {
  content: '';
  display: block;
  background: var(--jp-border-color2);
  height: 100%;
  width: 5px;
}

.jp-mod-sideBySide.jp-Notebook
  .jp-CodeCell.jp-mod-resizedCell
  .jp-CellResizeHandle::after {
  background: var(--jp-border-color0);
}

.jp-CellResizeHandle {
  display: none;
}

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Cell-Placeholder {
  padding-left: 55px;
}

.jp-Cell-Placeholder-wrapper {
  background: #fff;
  border: 1px solid;
  border-color: #e5e6e9 #dfe0e4 #d0d1d5;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  margin: 10px 15px;
}

.jp-Cell-Placeholder-wrapper-inner {
  padding: 15px;
  position: relative;
}

.jp-Cell-Placeholder-wrapper-body {
  background-repeat: repeat;
  background-size: 50% auto;
}

.jp-Cell-Placeholder-wrapper-body div {
  background: #f6f7f8;
  background-image: -webkit-linear-gradient(
    left,
    #f6f7f8 0%,
    #edeef1 20%,
    #f6f7f8 40%,
    #f6f7f8 100%
  );
  background-repeat: no-repeat;
  background-size: 800px 104px;
  height: 104px;
  position: absolute;
  right: 15px;
  left: 15px;
  top: 15px;
}

div.jp-Cell-Placeholder-h1 {
  top: 20px;
  height: 20px;
  left: 15px;
  width: 150px;
}

div.jp-Cell-Placeholder-h2 {
  left: 15px;
  top: 50px;
  height: 10px;
  width: 100px;
}

div.jp-Cell-Placeholder-content-1,
div.jp-Cell-Placeholder-content-2,
div.jp-Cell-Placeholder-content-3 {
  left: 15px;
  right: 15px;
  height: 10px;
}

div.jp-Cell-Placeholder-content-1 {
  top: 100px;
}

div.jp-Cell-Placeholder-content-2 {
  top: 120px;
}

div.jp-Cell-Placeholder-content-3 {
  top: 140px;
}

</style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
The following CSS variables define the main, public API for styling JupyterLab.
These variables should be used by all plugins wherever possible. In other
words, plugins should not define custom colors, sizes, etc unless absolutely
necessary. This enables users to change the visual theme of JupyterLab
by changing these variables.

Many variables appear in an ordered sequence (0,1,2,3). These sequences
are designed to work well together, so for example, `--jp-border-color1` should
be used with `--jp-layout-color1`. The numbers have the following meanings:

* 0: super-primary, reserved for special emphasis
* 1: primary, most important under normal situations
* 2: secondary, next most important under normal situations
* 3: tertiary, next most important under normal situations

Throughout JupyterLab, we are mostly following principles from Google's
Material Design when selecting colors. We are not, however, following
all of MD as it is not optimized for dense, information rich UIs.
*/

:root {
  /* Elevation
   *
   * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here:
   *
   * https://github.com/material-components/material-components-web
   * https://material-components-web.appspot.com/elevation.html
   */

  --jp-shadow-base-lightness: 0;
  --jp-shadow-umbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.2
  );
  --jp-shadow-penumbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.14
  );
  --jp-shadow-ambient-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.12
  );
  --jp-elevation-z0: none;
  --jp-elevation-z1: 0 2px 1px -1px var(--jp-shadow-umbra-color),
    0 1px 1px 0 var(--jp-shadow-penumbra-color),
    0 1px 3px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z2: 0 3px 1px -2px var(--jp-shadow-umbra-color),
    0 2px 2px 0 var(--jp-shadow-penumbra-color),
    0 1px 5px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z4: 0 2px 4px -1px var(--jp-shadow-umbra-color),
    0 4px 5px 0 var(--jp-shadow-penumbra-color),
    0 1px 10px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z6: 0 3px 5px -1px var(--jp-shadow-umbra-color),
    0 6px 10px 0 var(--jp-shadow-penumbra-color),
    0 1px 18px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z8: 0 5px 5px -3px var(--jp-shadow-umbra-color),
    0 8px 10px 1px var(--jp-shadow-penumbra-color),
    0 3px 14px 2px var(--jp-shadow-ambient-color);
  --jp-elevation-z12: 0 7px 8px -4px var(--jp-shadow-umbra-color),
    0 12px 17px 2px var(--jp-shadow-penumbra-color),
    0 5px 22px 4px var(--jp-shadow-ambient-color);
  --jp-elevation-z16: 0 8px 10px -5px var(--jp-shadow-umbra-color),
    0 16px 24px 2px var(--jp-shadow-penumbra-color),
    0 6px 30px 5px var(--jp-shadow-ambient-color);
  --jp-elevation-z20: 0 10px 13px -6px var(--jp-shadow-umbra-color),
    0 20px 31px 3px var(--jp-shadow-penumbra-color),
    0 8px 38px 7px var(--jp-shadow-ambient-color);
  --jp-elevation-z24: 0 11px 15px -7px var(--jp-shadow-umbra-color),
    0 24px 38px 3px var(--jp-shadow-penumbra-color),
    0 9px 46px 8px var(--jp-shadow-ambient-color);

  /* Borders
   *
   * The following variables, specify the visual styling of borders in JupyterLab.
   */

  --jp-border-width: 1px;
  --jp-border-color0: var(--md-grey-400);
  --jp-border-color1: var(--md-grey-400);
  --jp-border-color2: var(--md-grey-300);
  --jp-border-color3: var(--md-grey-200);
  --jp-inverse-border-color: var(--md-grey-600);
  --jp-border-radius: 2px;

  /* UI Fonts
   *
   * The UI font CSS variables are used for the typography all of the JupyterLab
   * user interface elements that are not directly user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-ui-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-ui-font-scale-factor: 1.2;
  --jp-ui-font-size0: 0.83333em;
  --jp-ui-font-size1: 13px; /* Base font size */
  --jp-ui-font-size2: 1.2em;
  --jp-ui-font-size3: 1.44em;
  --jp-ui-font-family: system-ui, -apple-system, blinkmacsystemfont, 'Segoe UI',
    helvetica, arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji',
    'Segoe UI Symbol';

  /*
   * Use these font colors against the corresponding main layout colors.
   * In a light theme, these go from dark to light.
   */

  /* Defaults use Material Design specification */
  --jp-ui-font-color0: rgba(0, 0, 0, 1);
  --jp-ui-font-color1: rgba(0, 0, 0, 0.87);
  --jp-ui-font-color2: rgba(0, 0, 0, 0.54);
  --jp-ui-font-color3: rgba(0, 0, 0, 0.38);

  /*
   * Use these against the brand/accent/warn/error colors.
   * These will typically go from light to darker, in both a dark and light theme.
   */

  --jp-ui-inverse-font-color0: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color1: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color2: rgba(255, 255, 255, 0.7);
  --jp-ui-inverse-font-color3: rgba(255, 255, 255, 0.5);

  /* Content Fonts
   *
   * Content font variables are used for typography of user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-content-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-content-line-height: 1.6;
  --jp-content-font-scale-factor: 1.2;
  --jp-content-font-size0: 0.83333em;
  --jp-content-font-size1: 14px; /* Base font size */
  --jp-content-font-size2: 1.2em;
  --jp-content-font-size3: 1.44em;
  --jp-content-font-size4: 1.728em;
  --jp-content-font-size5: 2.0736em;

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-content-presentation-font-size1: 17px;
  --jp-content-heading-line-height: 1;
  --jp-content-heading-margin-top: 1.2em;
  --jp-content-heading-margin-bottom: 0.8em;
  --jp-content-heading-font-weight: 500;

  /* Defaults use Material Design specification */
  --jp-content-font-color0: rgba(0, 0, 0, 1);
  --jp-content-font-color1: rgba(0, 0, 0, 0.87);
  --jp-content-font-color2: rgba(0, 0, 0, 0.54);
  --jp-content-font-color3: rgba(0, 0, 0, 0.38);
  --jp-content-link-color: var(--md-blue-900);
  --jp-content-font-family: system-ui, -apple-system, blinkmacsystemfont,
    'Segoe UI', helvetica, arial, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol';

  /*
   * Code Fonts
   *
   * Code font variables are used for typography of code and other monospaces content.
   */

  --jp-code-font-size: 13px;
  --jp-code-line-height: 1.3077; /* 17px for 13px base */
  --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */
  --jp-code-font-family-default: menlo, consolas, 'DejaVu Sans Mono', monospace;
  --jp-code-font-family: var(--jp-code-font-family-default);

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-code-presentation-font-size: 16px;

  /* may need to tweak cursor width if you change font size */
  --jp-code-cursor-width0: 1.4px;
  --jp-code-cursor-width1: 2px;
  --jp-code-cursor-width2: 4px;

  /* Layout
   *
   * The following are the main layout colors use in JupyterLab. In a light
   * theme these would go from light to dark.
   */

  --jp-layout-color0: white;
  --jp-layout-color1: white;
  --jp-layout-color2: var(--md-grey-200);
  --jp-layout-color3: var(--md-grey-400);
  --jp-layout-color4: var(--md-grey-600);

  /* Inverse Layout
   *
   * The following are the inverse layout colors use in JupyterLab. In a light
   * theme these would go from dark to light.
   */

  --jp-inverse-layout-color0: #111;
  --jp-inverse-layout-color1: var(--md-grey-900);
  --jp-inverse-layout-color2: var(--md-grey-800);
  --jp-inverse-layout-color3: var(--md-grey-700);
  --jp-inverse-layout-color4: var(--md-grey-600);

  /* Brand/accent */

  --jp-brand-color0: var(--md-blue-900);
  --jp-brand-color1: var(--md-blue-700);
  --jp-brand-color2: var(--md-blue-300);
  --jp-brand-color3: var(--md-blue-100);
  --jp-brand-color4: var(--md-blue-50);
  --jp-accent-color0: var(--md-green-900);
  --jp-accent-color1: var(--md-green-700);
  --jp-accent-color2: var(--md-green-300);
  --jp-accent-color3: var(--md-green-100);

  /* State colors (warn, error, success, info) */

  --jp-warn-color0: var(--md-orange-900);
  --jp-warn-color1: var(--md-orange-700);
  --jp-warn-color2: var(--md-orange-300);
  --jp-warn-color3: var(--md-orange-100);
  --jp-error-color0: var(--md-red-900);
  --jp-error-color1: var(--md-red-700);
  --jp-error-color2: var(--md-red-300);
  --jp-error-color3: var(--md-red-100);
  --jp-success-color0: var(--md-green-900);
  --jp-success-color1: var(--md-green-700);
  --jp-success-color2: var(--md-green-300);
  --jp-success-color3: var(--md-green-100);
  --jp-info-color0: var(--md-cyan-900);
  --jp-info-color1: var(--md-cyan-700);
  --jp-info-color2: var(--md-cyan-300);
  --jp-info-color3: var(--md-cyan-100);

  /* Cell specific styles */

  --jp-cell-padding: 5px;
  --jp-cell-collapser-width: 8px;
  --jp-cell-collapser-min-height: 20px;
  --jp-cell-collapser-not-active-hover-opacity: 0.6;
  --jp-cell-editor-background: var(--md-grey-100);
  --jp-cell-editor-border-color: var(--md-grey-300);
  --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-cell-editor-active-background: var(--jp-layout-color0);
  --jp-cell-editor-active-border-color: var(--jp-brand-color1);
  --jp-cell-prompt-width: 64px;
  --jp-cell-prompt-font-family: var(--jp-code-font-family-default);
  --jp-cell-prompt-letter-spacing: 0;
  --jp-cell-prompt-opacity: 1;
  --jp-cell-prompt-not-active-opacity: 0.5;
  --jp-cell-prompt-not-active-font-color: var(--md-grey-700);

  /* A custom blend of MD grey and blue 600
   * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */
  --jp-cell-inprompt-font-color: #307fc1;

  /* A custom blend of MD grey and orange 600
   * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */
  --jp-cell-outprompt-font-color: #bf5b3d;

  /* Notebook specific styles */

  --jp-notebook-padding: 10px;
  --jp-notebook-select-background: var(--jp-layout-color1);
  --jp-notebook-multiselected-color: var(--md-blue-50);

  /* The scroll padding is calculated to fill enough space at the bottom of the
  notebook to show one single-line cell (with appropriate padding) at the top
  when the notebook is scrolled all the way to the bottom. We also subtract one
  pixel so that no scrollbar appears if we have just one single-line cell in the
  notebook. This padding is to enable a 'scroll past end' feature in a notebook.
  */
  --jp-notebook-scroll-padding: calc(
    100% - var(--jp-code-font-size) * var(--jp-code-line-height) -
      var(--jp-code-padding) - var(--jp-cell-padding) - 1px
  );

  /* Rendermime styles */

  --jp-rendermime-error-background: #fdd;
  --jp-rendermime-table-row-background: var(--md-grey-100);
  --jp-rendermime-table-row-hover-background: var(--md-light-blue-50);

  /* Dialog specific styles */

  --jp-dialog-background: rgba(0, 0, 0, 0.25);

  /* Console specific styles */

  --jp-console-padding: 10px;

  /* Toolbar specific styles */

  --jp-toolbar-border-color: var(--jp-border-color1);
  --jp-toolbar-micro-height: 8px;
  --jp-toolbar-background: var(--jp-layout-color1);
  --jp-toolbar-box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.24);
  --jp-toolbar-header-margin: 4px 4px 0 4px;
  --jp-toolbar-active-background: var(--md-grey-300);

  /* Statusbar specific styles */

  --jp-statusbar-height: 24px;

  /* Input field styles */

  --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-input-active-background: var(--jp-layout-color1);
  --jp-input-hover-background: var(--jp-layout-color1);
  --jp-input-background: var(--md-grey-100);
  --jp-input-border-color: var(--jp-inverse-border-color);
  --jp-input-active-border-color: var(--jp-brand-color1);
  --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3);

  /* General editor styles */

  --jp-editor-selected-background: #d9d9d9;
  --jp-editor-selected-focused-background: #d7d4f0;
  --jp-editor-cursor-color: var(--jp-ui-font-color0);

  /* Code mirror specific styles */

  --jp-mirror-editor-keyword-color: #008000;
  --jp-mirror-editor-atom-color: #88f;
  --jp-mirror-editor-number-color: #080;
  --jp-mirror-editor-def-color: #00f;
  --jp-mirror-editor-variable-color: var(--md-grey-900);
  --jp-mirror-editor-variable-2-color: rgb(0, 54, 109);
  --jp-mirror-editor-variable-3-color: #085;
  --jp-mirror-editor-punctuation-color: #05a;
  --jp-mirror-editor-property-color: #05a;
  --jp-mirror-editor-operator-color: #a2f;
  --jp-mirror-editor-comment-color: #408080;
  --jp-mirror-editor-string-color: #ba2121;
  --jp-mirror-editor-string-2-color: #708;
  --jp-mirror-editor-meta-color: #a2f;
  --jp-mirror-editor-qualifier-color: #555;
  --jp-mirror-editor-builtin-color: #008000;
  --jp-mirror-editor-bracket-color: #997;
  --jp-mirror-editor-tag-color: #170;
  --jp-mirror-editor-attribute-color: #00c;
  --jp-mirror-editor-header-color: blue;
  --jp-mirror-editor-quote-color: #090;
  --jp-mirror-editor-link-color: #00c;
  --jp-mirror-editor-error-color: #f00;
  --jp-mirror-editor-hr-color: #999;

  /*
    RTC user specific colors.
    These colors are used for the cursor, username in the editor,
    and the icon of the user.
  */

  --jp-collaborator-color1: #ffad8e;
  --jp-collaborator-color2: #dac83d;
  --jp-collaborator-color3: #72dd76;
  --jp-collaborator-color4: #00e4d0;
  --jp-collaborator-color5: #45d4ff;
  --jp-collaborator-color6: #e2b1ff;
  --jp-collaborator-color7: #ff9de6;

  /* Vega extension styles */

  --jp-vega-background: white;

  /* Sidebar-related styles */

  --jp-sidebar-min-width: 250px;

  /* Search-related styles */

  --jp-search-toggle-off-opacity: 0.5;
  --jp-search-toggle-hover-opacity: 0.8;
  --jp-search-toggle-on-opacity: 1;
  --jp-search-selected-match-background-color: rgb(245, 200, 0);
  --jp-search-selected-match-color: black;
  --jp-search-unselected-match-background-color: var(
    --jp-inverse-layout-color0
  );
  --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0);

  /* Icon colors that work well with light or dark backgrounds */
  --jp-icon-contrast-color0: var(--md-purple-600);
  --jp-icon-contrast-color1: var(--md-green-600);
  --jp-icon-contrast-color2: var(--md-pink-600);
  --jp-icon-contrast-color3: var(--md-blue-600);

  /* Button colors */
  --jp-accept-color-normal: var(--md-blue-700);
  --jp-accept-color-hover: var(--md-blue-800);
  --jp-accept-color-active: var(--md-blue-900);
  --jp-warn-color-normal: var(--md-red-700);
  --jp-warn-color-hover: var(--md-red-800);
  --jp-warn-color-active: var(--md-red-900);
  --jp-reject-color-normal: var(--md-grey-600);
  --jp-reject-color-hover: var(--md-grey-700);
  --jp-reject-color-active: var(--md-grey-800);

  /* File or activity icons and switch semantic variables */
  --jp-jupyter-icon-color: #f37626;
  --jp-notebook-icon-color: #f37626;
  --jp-json-icon-color: var(--md-orange-700);
  --jp-console-icon-background-color: var(--md-blue-700);
  --jp-console-icon-color: white;
  --jp-terminal-icon-background-color: var(--md-grey-800);
  --jp-terminal-icon-color: var(--md-grey-200);
  --jp-text-editor-icon-color: var(--md-grey-700);
  --jp-inspector-icon-color: var(--md-grey-700);
  --jp-switch-color: var(--md-grey-400);
  --jp-switch-true-position-color: var(--md-orange-900);
}
</style>
<style type="text/css">
/* Force rendering true colors when outputing to pdf */
* {
  -webkit-print-color-adjust: exact;
}

/* Misc */
a.anchor-link {
  display: none;
}

/* Input area styling */
.jp-InputArea {
  overflow: hidden;
}

.jp-InputArea-editor {
  overflow: hidden;
}

.cm-editor.cm-s-jupyter .highlight pre {
/* weird, but --jp-code-padding defined to be 5px but 4px horizontal padding is hardcoded for pre.cm-line */
  padding: var(--jp-code-padding) 4px;
  margin: 0;

  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
  color: inherit;

}

.jp-OutputArea-output pre {
  line-height: inherit;
  font-family: inherit;
}

.jp-RenderedText pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
}

/* Hiding the collapser by default */
.jp-Collapser {
  display: none;
}

@page {
    margin: 0.5in; /* Margin for each printed piece of paper */
}

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}
</style>
<!-- Load mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML-full,Safe"> </script>
<!-- MathJax configuration -->
<script type="text/x-mathjax-config">
    init_mathjax = function() {
        if (window.MathJax) {
        // MathJax loaded
            MathJax.Hub.Config({
                TeX: {
                    equationNumbers: {
                    autoNumber: "AMS",
                    useLabelIds: true
                    }
                },
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true,
                    processEnvironments: true
                },
                displayAlign: 'center',
                CommonHTML: {
                    linebreaks: {
                    automatic: true
                    }
                }
            });

            MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
        }
    }
    init_mathjax();
    </script>
<!-- End of mathjax configuration --><script type="module">
  document.addEventListener("DOMContentLoaded", async () => {
    const diagrams = document.querySelectorAll(".jp-Mermaid > pre.mermaid");
    // do not load mermaidjs if not needed
    if (!diagrams.length) {
      return;
    }
    const mermaid = (await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs")).default;
    const parser = new DOMParser();

    mermaid.initialize({
      maxTextSize: 100000,
      maxEdges: 100000,
      startOnLoad: false,
      fontFamily: window
        .getComputedStyle(document.body)
        .getPropertyValue("--jp-ui-font-family"),
      theme: document.querySelector("body[data-jp-theme-light='true']")
        ? "default"
        : "dark",
    });

    let _nextMermaidId = 0;

    function makeMermaidImage(svg) {
      const img = document.createElement("img");
      const doc = parser.parseFromString(svg, "image/svg+xml");
      const svgEl = doc.querySelector("svg");
      const { maxWidth } = svgEl?.style || {};
      const firstTitle = doc.querySelector("title");
      const firstDesc = doc.querySelector("desc");

      img.setAttribute("src", `data:image/svg+xml,${encodeURIComponent(svg)}`);
      if (maxWidth) {
        img.width = parseInt(maxWidth);
      }
      if (firstTitle) {
        img.setAttribute("alt", firstTitle.textContent);
      }
      if (firstDesc) {
        const caption = document.createElement("figcaption");
        caption.className = "sr-only";
        caption.textContent = firstDesc.textContent;
        return [img, caption];
      }
      return [img];
    }

    async function makeMermaidError(text) {
      let errorMessage = "";
      try {
        await mermaid.parse(text);
      } catch (err) {
        errorMessage = `${err}`;
      }

      const result = document.createElement("details");
      result.className = 'jp-RenderedMermaid-Details';
      const summary = document.createElement("summary");
      summary.className = 'jp-RenderedMermaid-Summary';
      const pre = document.createElement("pre");
      const code = document.createElement("code");
      code.innerText = text;
      pre.appendChild(code);
      summary.appendChild(pre);
      result.appendChild(summary);

      const warning = document.createElement("pre");
      warning.innerText = errorMessage;
      result.appendChild(warning);
      return [result];
    }

    async function renderOneMarmaid(src) {
      const id = `jp-mermaid-${_nextMermaidId++}`;
      const parent = src.parentNode;
      let raw = src.textContent.trim();
      const el = document.createElement("div");
      el.style.visibility = "hidden";
      document.body.appendChild(el);
      let results = null;
      let output = null;
      try {
        let { svg } = await mermaid.render(id, raw, el);
        svg = cleanMermaidSvg(svg);
        results = makeMermaidImage(svg);
        output = document.createElement("figure");
        results.map(output.appendChild, output);
      } catch (err) {
        parent.classList.add("jp-mod-warning");
        results = await makeMermaidError(raw);
        output = results[0];
      } finally {
        el.remove();
      }
      parent.classList.add("jp-RenderedMermaid");
      parent.appendChild(output);
    }


    /**
     * Post-process to ensure mermaid diagrams contain only valid SVG and XHTML.
     */
    function cleanMermaidSvg(svg) {
      return svg.replace(RE_VOID_ELEMENT, replaceVoidElement);
    }


    /**
     * A regular expression for all void elements, which may include attributes and
     * a slash.
     *
     * @see https://developer.mozilla.org/en-US/docs/Glossary/Void_element
     *
     * Of these, only `<br>` is generated by Mermaid in place of `\n`,
     * but _any_ "malformed" tag will break the SVG rendering entirely.
     */
    const RE_VOID_ELEMENT =
      /<\s*(area|base|br|col|embed|hr|img|input|link|meta|param|source|track|wbr)\s*([^>]*?)\s*>/gi;

    /**
     * Ensure a void element is closed with a slash, preserving any attributes.
     */
    function replaceVoidElement(match, tag, rest) {
      rest = rest.trim();
      if (!rest.endsWith('/')) {
        rest = `${rest} /`;
      }
      return `<${tag} ${rest}>`;
    }

    void Promise.all([...diagrams].map(renderOneMarmaid));
  });
</script>
<style>
  .jp-Mermaid:not(.jp-RenderedMermaid) {
    display: none;
  }

  .jp-RenderedMermaid {
    overflow: auto;
    display: flex;
  }

  .jp-RenderedMermaid.jp-mod-warning {
    width: auto;
    padding: 0.5em;
    margin-top: 0.5em;
    border: var(--jp-border-width) solid var(--jp-warn-color2);
    border-radius: var(--jp-border-radius);
    color: var(--jp-ui-font-color1);
    font-size: var(--jp-ui-font-size1);
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  .jp-RenderedMermaid figure {
    margin: 0;
    overflow: auto;
    max-width: 100%;
  }

  .jp-RenderedMermaid img {
    max-width: 100%;
  }

  .jp-RenderedMermaid-Details > pre {
    margin-top: 1em;
  }

  .jp-RenderedMermaid-Summary {
    color: var(--jp-warn-color2);
  }

  .jp-RenderedMermaid:not(.jp-mod-warning) pre {
    display: none;
  }

  .jp-RenderedMermaid-Summary > pre {
    display: inline-block;
    white-space: normal;
  }
</style>
<!-- End of mermaid configuration --></head>
<body class="jp-Notebook" data-jp-theme-light="true" data-jp-theme-name="JupyterLab Light">
<main>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=73c2d2c9-4a4b-48ca-90a3-1ccd120ca08b">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h1 id="Fine-tuning-using-GPT2-Large">Fine tuning using GPT2 Large<a class="anchor-link" href="#Fine-tuning-using-GPT2-Large">¶</a></h1>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=5b5bef3d-89b9-42ed-b158-a39bd61f6a31">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Base-Model-and-Quantization">Base Model and Quantization<a class="anchor-link" href="#Base-Model-and-Quantization">¶</a></h3>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=938123ed-15b0-45ee-b622-d9bbe5fe3a48">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>4-bit quantization alongside specific computational optimizations</p>
<p>load_in_4bit: This option likely enables loading and storing tensors in 4-bit precision. This can significantly reduce memory usage at the cost of precision. Enabling this suggests that you're optimizing for memory efficiency, potentially to fit larger models or datasets in memory.</p>
<p>bnb_4bit_use_double_quant: This indicates the use of a double quantization process for 4-bit representation. Double quantization might be used to improve the precision of the 4-bit quantized values, potentially mitigating some of the precision loss associated with low-bit quantization.</p>
<p>bnb_4bit_quant_type="nf4": This specifies the quantization type or algorithm used for converting tensors to 4-bit representations. "nf4" might refer to a specific quantization scheme optimized for neural network weights and activations. The exact nature of "nf4" would depend on the documentation of the BitsAndBytes library, but it suggests an approach tailored to maintain as much information as possible within the 4-bit limitation.</p>
<p>bnb_4bit_compute_dtype=torch.bfloat16: This sets the data type for computations using 4-bit quantized tensors to torch.bfloat16, which is a 16-bit floating-point representation that offers a good balance between precision and memory usage. By performing computations in bfloat16, the configuration aims to maintain computational accuracy and efficiency, particularly on hardware that supports bfloat16 operations natively.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=dfe5f014-527c-4a83-863b-4b6330c72ed5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [1]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># GPU 0: NVIDIA GeForce RTX 4090</span>
<span class="c1"># GPU 1: NVIDIA GeForce RTX 4090</span>
<span class="c1"># GPU 2: NVIDIA GeForce RTX 4090</span>
<span class="c1"># GPU 3: NVIDIA GeForce RTX 3090 Ti</span>
<span class="c1"># GPU 4: NVIDIA GeForce RTX 3090 Ti</span>
<span class="c1"># GPU 5: NVIDIA GeForce RTX 3090</span>
<span class="c1"># GPU 6: NVIDIA GeForce RTX 3090</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"CUDA_VISIBLE_DEVICES"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"6"</span>  <span class="c1"># ""makes all visible, "0" GPU 0 visible</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=53ffaa11-3936-40a0-8f2a-3d083ff2afef">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Supress-warnings">Supress warnings<a class="anchor-link" href="#Supress-warnings">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=e77f7e3d-3af3-4dc8-9571-d13398c29ee9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [2]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">UserWarning</span><span class="p">)</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">FutureWarning</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=ef535c7a-848e-4c6e-a692-208f98610d82">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Inspect-the-base-model">Inspect the base model<a class="anchor-link" href="#Inspect-the-base-model">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=15ec782b-0776-4354-ad08-66f1e9e50187">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [3]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s2">"gpt2-large"</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"openai-community/"</span><span class="o">+</span><span class="n">model_name</span>

<span class="n">bnb_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span>
    <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="s2">"nf4"</span><span class="p">,</span>
    <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
<span class="p">)</span>

<span class="n">access_token</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HF_TOKEN"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">bnb_config</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">token</span><span class="o">=</span><span class="n">access_token</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>config.json:   0%|          | 0.00/666 [00:00&lt;?, ?B/s]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>model.safetensors:   0%|          | 0.00/3.25G [00:00&lt;?, ?B/s]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2-large and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=83f74939-3ab3-4011-90cb-8e90c22ea162">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [4]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torchinfo</span> <span class="kn">import</span> <span class="n">summary</span>
<span class="n">summary</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[4]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>===========================================================================
Layer (type:depth-idx)                             Param #
===========================================================================
GPT2ForSequenceClassification                      --
├─GPT2Model: 1-1                                   --
│    └─Embedding: 2-1                              64,328,960
│    └─Embedding: 2-2                              1,310,720
│    └─Dropout: 2-3                                --
│    └─ModuleList: 2-4                             --
│    │    └─GPT2Block: 3-1                         9,847,040
│    │    └─GPT2Block: 3-2                         9,847,040
│    │    └─GPT2Block: 3-3                         9,847,040
│    │    └─GPT2Block: 3-4                         9,847,040
│    │    └─GPT2Block: 3-5                         9,847,040
│    │    └─GPT2Block: 3-6                         9,847,040
│    │    └─GPT2Block: 3-7                         9,847,040
│    │    └─GPT2Block: 3-8                         9,847,040
│    │    └─GPT2Block: 3-9                         9,847,040
│    │    └─GPT2Block: 3-10                        9,847,040
│    │    └─GPT2Block: 3-11                        9,847,040
│    │    └─GPT2Block: 3-12                        9,847,040
│    │    └─GPT2Block: 3-13                        9,847,040
│    │    └─GPT2Block: 3-14                        9,847,040
│    │    └─GPT2Block: 3-15                        9,847,040
│    │    └─GPT2Block: 3-16                        9,847,040
│    │    └─GPT2Block: 3-17                        9,847,040
│    │    └─GPT2Block: 3-18                        9,847,040
│    │    └─GPT2Block: 3-19                        9,847,040
│    │    └─GPT2Block: 3-20                        9,847,040
│    │    └─GPT2Block: 3-21                        9,847,040
│    │    └─GPT2Block: 3-22                        9,847,040
│    │    └─GPT2Block: 3-23                        9,847,040
│    │    └─GPT2Block: 3-24                        9,847,040
│    │    └─GPT2Block: 3-25                        9,847,040
│    │    └─GPT2Block: 3-26                        9,847,040
│    │    └─GPT2Block: 3-27                        9,847,040
│    │    └─GPT2Block: 3-28                        9,847,040
│    │    └─GPT2Block: 3-29                        9,847,040
│    │    └─GPT2Block: 3-30                        9,847,040
│    │    └─GPT2Block: 3-31                        9,847,040
│    │    └─GPT2Block: 3-32                        9,847,040
│    │    └─GPT2Block: 3-33                        9,847,040
│    │    └─GPT2Block: 3-34                        9,847,040
│    │    └─GPT2Block: 3-35                        9,847,040
│    │    └─GPT2Block: 3-36                        9,847,040
│    └─LayerNorm: 2-5                              2,560
├─Linear: 1-2                                      2,560
===========================================================================
Total params: 420,138,240
Trainable params: 65,829,120
Non-trainable params: 354,309,120
===========================================================================</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=49bbe47c-4430-4f4a-90d8-1113bd320a18">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [5]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[5]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>GPT2ForSequenceClassification(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 1280)
    (wpe): Embedding(1024, 1280)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-35): 36 x GPT2Block(
        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Linear4bit(in_features=1280, out_features=3840, bias=True)
          (c_proj): Linear4bit(in_features=1280, out_features=1280, bias=True)
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Linear4bit(in_features=1280, out_features=5120, bias=True)
          (c_proj): Linear4bit(in_features=5120, out_features=1280, bias=True)
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
  )
  (score): Linear(in_features=1280, out_features=2, bias=False)
)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=6627b1da-57d6-4a17-8ea3-746b5cb1f3d9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Load-the-news-dataset-from-pickle-file">Load the news dataset from pickle file<a class="anchor-link" href="#Load-the-news-dataset-from-pickle-file">¶</a></h3><p>If any of the check_files don't exist then load the pickle file</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=1cc372cc-0cae-4b49-a2d7-8e57f58244a7">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [4]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pickle</span>

<span class="n">base_path</span> <span class="o">=</span> <span class="s1">'./data/'</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">base_path</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">file_name</span> <span class="o">=</span> <span class="s1">'news_small_dataset.pkl'</span>
<span class="n">file_path</span> <span class="o">=</span> <span class="n">base_path</span><span class="o">+</span><span class="n">file_name</span>

<span class="k">def</span> <span class="nf">pickle_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">file_path</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="s1">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
        <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">file</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Dataset has been pickled to: </span><span class="si">{</span><span class="n">file_path</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">load_pickle_dataset</span><span class="p">(</span><span class="n">file_path</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="s1">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Dataset has been loaded from: </span><span class="si">{</span><span class="n">file_path</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dataset</span>

<span class="k">def</span> <span class="nf">check_files_exists</span><span class="p">(</span><span class="n">file_names</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">file_names</span><span class="p">:</span>
        <span class="n">file_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">base_path</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">file_path</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="kc">False</span>

<span class="c1"># if these files exist we do not want to load the news_dataset.pkl to tokenize and make these files</span>
<span class="n">check_files</span> <span class="o">=</span> <span class="p">[</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_train_ds.pkl'</span><span class="p">,</span> <span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_eval_ds.pkl'</span><span class="p">,</span> <span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_test_ds.pkl'</span><span class="p">]</span>

<span class="k">if</span> <span class="n">check_files_exists</span><span class="p">(</span><span class="n">check_files</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"At least one of the specified files already exists. Not loading new dataset."</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">news_split_ds</span> <span class="o">=</span> <span class="n">load_pickle_dataset</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">news_split_ds</span><span class="p">)</span>
    <span class="n">total_rows</span> <span class="o">=</span> <span class="p">(</span><span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'train'</span><span class="p">]</span><span class="o">.</span><span class="n">num_rows</span> <span class="o">+</span>
              <span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'eval'</span><span class="p">]</span><span class="o">.</span><span class="n">num_rows</span> <span class="o">+</span>
              <span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'test'</span><span class="p">]</span><span class="o">.</span><span class="n">num_rows</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Total number of rows:"</span><span class="p">,</span> <span class="n">total_rows</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Dataset loaded successfully."</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>At least one of the specified files already exists. Not loading new dataset.
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=8ad450a2-6cef-432e-9e63-7ff985b4726e">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Tokenization-of-data">Tokenization of data<a class="anchor-link" href="#Tokenization-of-data">¶</a></h3>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=186f093c-7487-44ae-ad95-9ee8d24f04c7">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>
return_tensors="pt": This argument configures the tokenizer to output PyTorch ("pt") tensors. If you're working with TensorFlow, you'd use "tf" instead, and for NumPy arrays, you could omit this argument or set return_tensors to None.
</p>
<p>
Direct Model Input: By converting the tokenized input into tensors, the output can be directly used as input to a PyTorch model, fitting seamlessly into the data processing pipeline for model training or inference.

<p>Handling of Batch Inputs: This approach also supports batch inputs. If you pass a list of texts to the tokenizer with return_tensors="pt", it will automatically pad the sequences to the maximum length in the batch, returning a tensor where the first dimension is the batch size.</p>
<p>Padding and Truncation: The padding=True and truncation=True arguments ensure that all sequences are padded to the same length (up to max_length) and are truncated if they exceed this length, which is important for processing sequences in batches.</p>
</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=88c3a38c-1d24-4a1e-8d96-1b7c2ce162c7">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [5]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">token</span><span class="o">=</span><span class="n">access_token</span><span class="p">)</span>

<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">tokenize_fn</span><span class="p">(</span><span class="n">news</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">news</span><span class="p">[</span><span class="s1">'article'</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>tokenizer_config.json:   0%|          | 0.00/26.0 [00:00&lt;?, ?B/s]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>vocab.json:   0%|          | 0.00/1.04M [00:00&lt;?, ?B/s]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>merges.txt:   0%|          | 0.00/456k [00:00&lt;?, ?B/s]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>tokenizer.json:   0%|          | 0.00/1.36M [00:00&lt;?, ?B/s]</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=02efc4f0-742a-4838-887d-5556a26ae15f">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Tokenize-train,-evaluation,-and-test-datasets">Tokenize train, evaluation, and test datasets<a class="anchor-link" href="#Tokenize-train,-evaluation,-and-test-datasets">¶</a></h3><p>If any of the check files exist then don't run tokenization and save some time.
Else load the pickle files that already exist.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=0f3e3101-df30-4af2-9976-49ba0cf44d62">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [6]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">check_files_exists</span><span class="p">(</span><span class="n">check_files</span><span class="p">):</span>
    <span class="n">tokenized_train_ds</span> <span class="o">=</span> <span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'train'</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_fn</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">tokenized_eval_ds</span> <span class="o">=</span> <span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'eval'</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_fn</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">tokenized_test_ds</span> <span class="o">=</span> <span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'test'</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_fn</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">tokenized_train_ds</span><span class="o">.</span><span class="n">features</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">tokenized_eval_ds</span><span class="o">.</span><span class="n">features</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">tokenized_test_ds</span><span class="o">.</span><span class="n">features</span><span class="p">)</span>
    
    <span class="n">pickle_dataset</span><span class="p">(</span><span class="n">tokenized_train_ds</span><span class="p">,</span> <span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_train_ds.pkl'</span><span class="p">)</span>
    <span class="n">pickle_dataset</span><span class="p">(</span><span class="n">tokenized_eval_ds</span><span class="p">,</span> <span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_eval_ds.pkl'</span><span class="p">)</span>
    <span class="n">pickle_dataset</span><span class="p">(</span><span class="n">tokenized_test_ds</span><span class="p">,</span> <span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_test_ds.pkl'</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Files already exist, so load datasets"</span><span class="p">)</span>
    <span class="n">tokenized_train_ds</span> <span class="o">=</span> <span class="n">load_pickle_dataset</span><span class="p">(</span><span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_train_ds.pkl'</span><span class="p">)</span>
    <span class="n">tokenized_eval_ds</span> <span class="o">=</span> <span class="n">load_pickle_dataset</span><span class="p">(</span><span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_eval_ds.pkl'</span><span class="p">)</span>
    <span class="n">tokenized_test_ds</span> <span class="o">=</span> <span class="n">load_pickle_dataset</span><span class="p">(</span><span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_test_ds.pkl'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Files already exist, so load datasets
Dataset has been loaded from: ./data/gpt2-large-small_tokenized_train_ds.pkl
Dataset has been loaded from: ./data/gpt2-large-small_tokenized_eval_ds.pkl
Dataset has been loaded from: ./data/gpt2-large-small_tokenized_test_ds.pkl
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=307cc4fb-9766-4b0f-943b-4a1c90053e09">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Look-at-the-tokenized-data">Look at the tokenized data<a class="anchor-link" href="#Look-at-the-tokenized-data">¶</a></h3><p>Notice what the actual data looks like, and then the tokenized data which is a bunch of numbers, and then the attention mask at the end.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=44321182-e1b9-4570-bd24-7a5cf42ba504">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [9]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">count_train_records</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenized_train_ds</span><span class="p">)</span>
<span class="n">count_eval_records</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenized_eval_ds</span><span class="p">)</span>
<span class="n">count_test_records</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenized_test_ds</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of records in training dataset: </span><span class="si">{</span><span class="n">count_train_records</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of records in evaluation dataset: </span><span class="si">{</span><span class="n">count_eval_records</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of records in test dataset: </span><span class="si">{</span><span class="n">count_test_records</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">count_total_records</span> <span class="o">=</span> <span class="n">count_train_records</span> <span class="o">+</span> <span class="n">count_eval_records</span> <span class="o">+</span> <span class="n">count_test_records</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Total number of records: </span><span class="si">{</span><span class="n">count_total_records</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Number of records in training dataset: 33611
Number of records in evaluation dataset: 7203
Number of records in test dataset: 7203
Total number of records: 48017
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=64be678b-d2a8-403e-bcc8-ef9e7eabb0cf">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [10]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">first_record</span> <span class="o">=</span> <span class="n">tokenized_train_ds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">first_record</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>{'article': "In a year where homicides, rapes and robberies increased slightly, New York City still saw serious crime drop 1.7 percent in 2015, continuing an overall decline that began in the 1990s, NYPD Commissioner William Bratton said Monday.\nAt a news conference with Mayor Bill de Blasio, Bratton touted last year’s crime statistics, which he said, when combined with an even larger decline in 2014, put to rest the fear that substantial decreases couldn’t continue under the new administration at City Hall.\n“While we have had some fluctuation, some increases in certain categories, the overall trend in all our crime categories continues to go down,” Bratton told reporters. “It was a very good year for us, 2015.\nHomicides increased by 4.5 percent in 2015, rising to 350 from 333 in the prior year, which was the lowest since 1994, said Deputy Commissioner Dermot Shea. Rapes increased 6 percent and robberies rose 2 percent, said Shea, who is in charge of data collection and operations for the NYPD.\nThe lower overall crime statistics came about due to what Shea called “targeted enforcement,” where cops make quality arrests even though the overall number of apprehensions was the lowest in the city since 2003.\nTwo boroughs — Manhattan and the Bronx — actually saw serious crimes increase by 3 percent and 4 percent, respectively, Shea said. Manhattan’s increase was driven by more robberies, while the Bronx, although seeing an overall crime increase, had what he said was a “phenomenal” reduction in shootings. Citywide, shootings were down in 2015 about 3 percent, to 1,103 from 1,172 in 2014.\nShea largely attributed the 2015 increase in rapes to victims coming forward with complaints about attacks from years past.\nSign up to get the latest updates Get Newsday's Breaking News alerts in your inbox. By clicking Sign up, you agree to our privacy policy.\n“Twenty percent of these rapes didn’t happen in 2015,” he said.\nThe NYPD has seen an increase in rapes involving single women who, after a night of drinking, get into cabs of all kinds and are attacked, Shea said.\n“They get driven, and passing out and waking up in a desolate area, and they get sexually attacked. This is something, really, that people need to be exceptionally aware of, and like any case in New York City, the buddy system works,” said Shea, referring to the need for people to travel in pairs when taking a cab at night.\nBratton and police brass hope to build upon the continuing drop in overall crime by using technology such as ShotSpotter and a newly minted GPS system for police cars.\nJessica Tisch, NYPD deputy commissioner for technology, said ShotSpotter, an acoustical system that detects gunfire, identified gunshots in 1,672 cases, mostly in Brooklyn. Of those alerts, 74 percent didn’t have any 911 calls from the public associated with them.\nTisch said ShotSpotter helped police recover ballistic evidence in 19 percent of the gunfire alerts. In 22 percent of those cases, Tisch said, cops were able to make positive matches of bullets with those from guns used in earlier shootings.\nTisch also highlighted a special GPS system being tried in about 5,000 patrol cars that allows the NYPD to see where its vehicles are and to track their movements over a 24-hour period, as well as gather information about the officers’ driving.\n", 'label': 0, 'input_ids': [818, 257, 614, 810, 33025, 11, 37459, 290, 43774, 3220, 4622, 11, 968, 1971, 2254, 991, 2497, 2726, 4065, 4268, 352, 13, 22, 1411, 287, 1853, 11, 8282, 281, 4045, 7794, 326, 2540, 287, 262, 6303, 82, 11, 27615, 13270, 3977, 1709, 38680, 531, 3321, 13, 198, 2953, 257, 1705, 4495, 351, 10106, 3941, 390, 36200, 11, 1709, 38680, 28275, 938, 614, 447, 247, 82, 4065, 7869, 11, 543, 339, 531, 11, 618, 5929, 351, 281, 772, 4025, 7794, 287, 1946, 11, 1234, 284, 1334, 262, 3252, 326, 8904, 20638, 3521, 447, 247, 83, 2555, 739, 262, 649, 3662, 379, 2254, 4789, 13, 198, 447, 250, 3633, 356, 423, 550, 617, 19180, 2288, 11, 617, 5732, 287, 1728, 9376, 11, 262, 4045, 5182, 287, 477, 674, 4065, 9376, 4477, 284, 467, 866, 11, 447, 251, 1709, 38680, 1297, 7638, 13, 564, 250, 1026, 373, 257, 845, 922, 614, 329, 514, 11, 1853, 13, 198, 39, 10179, 1460, 3220, 416, 604, 13, 20, 1411, 287, 1853, 11, 7396, 284, 13803, 422, 23460, 287, 262, 3161, 614, 11, 543, 373, 262, 9016, 1201, 9162, 11, 531, 15110, 13270, 360, 7780, 313, 42368, 13, 371, 7916, 3220, 718, 1411, 290, 43774, 8278, 362, 1411, 11, 531, 42368, 11, 508, 318, 287, 3877, 286, 1366, 4947, 290, 4560, 329, 262, 27615, 13, 198, 464, 2793, 4045, 4065, 7869, 1625, 546, 2233, 284, 644, 42368, 1444, 564, 250, 16793, 276, 5394, 11, 447, 251, 810, 14073, 787, 3081, 14794, 772, 996, 262, 4045, 1271, 286, 31887, 507, 373, 262, 9016, 287, 262, 1748, 1201, 5816, 13, 198, 7571, 33534, 82, 851, 13458, 290, 262, 32486, 851, 1682, 2497, 2726, 6741, 2620, 416, 513, 1411, 290, 604, 1411, 11, 8148, 11, 42368, 531, 13, 13458, 447, 247, 82, 2620, 373, 7986, 416, 517, 43774, 11, 981, 262, 32486, 11, 3584, 4379, 281, 4045, 4065, 2620, 11, 550, 644, 339, 531, 373, 257, 564, 250, 31024, 3674, 282, 447, 251, 7741, 287, 17690, 13, 2254, 4421, 11, 17690, 547, 866, 287, 1853, 546, 513, 1411, 11, 284, 352, 11, 15197, 422, 352, 11, 23628, 287, 1946, 13, 198, 3347, 64, 5688, 14183, 262, 1853, 2620, 287, 37459, 284, 4970, 2406, 2651, 351, 9687, 546, 3434, 422, 812, 1613, 13, 198, 11712, 510, 284, 651, 262, 3452, 5992, 3497, 3000, 820, 338, 24942, 3000, 21675, 287, 534, 13734, 13, 2750, 12264, 5865, 510, 11, 345, 4236, 284, 674, 6782, 2450, 13, 198, 447, 250, 34096, 1411, 286, 777, 37459, 1422, 447, 247, 83, 1645, 287, 1853, 11, 447, 251, 339, 531, 13, 198, 464, 27615, 468, 1775, 281, 2620, 287, 37459, 7411, 2060, 1466, 508, 11, 706, 257, 1755, 286, 7722, 11, 651, 656, 269, 8937, 286, 477, 6982, 290, 389, 7384, 11, 42368, 531, 13, 198, 447, 250, 2990, 651, 7986, 11, 290, 6427, 503, 290, 23137, 510, 287, 257, 50244, 1989, 11, 290, 484, 651, 11363, 7384, 13, 770, 318, 1223, 11, 1107, 11, 326, 661, 761, 284, 307, 24822, 3910, 286, 11, 290, 588, 597, 1339, 287, 968, 1971, 2254, 11, 262, 24407, 1080, 2499, 11, 447], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=b2d7f4f8-9eb3-4feb-86e7-fb0dad88753f">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Turn-on-accelerate">Turn on accelerate<a class="anchor-link" href="#Turn-on-accelerate">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=f3eab29e-fa47-4a13-abc4-d6425ae741b6">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [7]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">accelerate</span> <span class="kn">import</span> <span class="n">FullyShardedDataParallelPlugin</span><span class="p">,</span> <span class="n">Accelerator</span>
<span class="kn">from</span> <span class="nn">torch.distributed.fsdp.fully_sharded_data_parallel</span> <span class="kn">import</span> <span class="n">FullOptimStateDictConfig</span><span class="p">,</span> <span class="n">FullStateDictConfig</span>

<span class="n">fsdp_plugin</span> <span class="o">=</span> <span class="n">FullyShardedDataParallelPlugin</span><span class="p">(</span>
    <span class="n">state_dict_config</span><span class="o">=</span><span class="n">FullStateDictConfig</span><span class="p">(</span><span class="n">offload_to_cpu</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rank0_only</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="n">optim_state_dict_config</span><span class="o">=</span><span class="n">FullOptimStateDictConfig</span><span class="p">(</span><span class="n">offload_to_cpu</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rank0_only</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
<span class="p">)</span>

<span class="n">accelerator</span> <span class="o">=</span> <span class="n">Accelerator</span><span class="p">(</span><span class="n">fsdp_plugin</span><span class="o">=</span><span class="n">fsdp_plugin</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=22e040f1-e596-476d-9f26-ffa6f8a4a548">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="LoRA---Low-Rank-Adaptation">LoRA - Low-Rank Adaptation<a class="anchor-link" href="#LoRA---Low-Rank-Adaptation">¶</a></h3>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=bc8b8091-6179-4df0-b5bd-b29ec2dde4d5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>LoRA, short for Low-Rank Adaptation, is a technique designed to efficiently fine-tune large pre-trained models without the need to update all the model parameters, significantly reducing the computational and memory overhead typically associated with training. LoRA targets the challenge of adapting massive models, particularly in natural language processing (NLP) and computer vision, to specialized tasks while keeping the resource requirements manageable.</p>
<p>
Gradient checkpointing for the model. Gradient checkpointing is a technique used to reduce memory usage during the training of deep neural networks by trading compute for memory. It works by storing a minimal set of intermediate activations during the forward pass and then recomputing the others during the backward pass. This is particularly useful for training large models or using larger batch sizes.
</p>
<p>
The forward pass is the process where the input data is passed through the network from the input layer to the output layer. During this pass, the network performs a series of computations at each layer, applying weights to the inputs, adding biases (if applicable), and passing the result through an activation function. The final output of the forward pass is the prediction made by the network. The main goal of the forward pass is to compute the output given the current state of the model's parameters (weights and biases). This output is then used to calculate the loss, which quantifies how well the model's predictions match the actual labels.
</p>
<p>
The backward pass, or backpropagation, is the process of computing the gradient of the loss function with respect to each parameter in the network. This involves applying the chain rule of calculus to take derivatives step-by-step from the output layer back to the input layer. Essentially, it calculates how much each parameter contributed to the error in the prediction. The purpose of the backward pass is to update the model's parameters in a way that minimally reduces the loss, improving the model's predictions. The gradients calculated during this pass indicate the direction in which each parameter should be adjusted to decrease the error. Using an optimization algorithm (e.g., Stochastic Gradient Descent), these gradients are then used to update the weights to minimize the loss.
</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=41956597-0441-4bc6-aefe-fc4b0d5349c5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>
r: This parameter specifies the rank of the low-rank matrices that are introduced by LoRA. A smaller rank means fewer parameters to train, leading to a more memory-efficient fine-tuning process.
</p>
<p>
lora_alpha: This multiplier adjusts the scale of the LoRA parameters. A higher value increases the capacity of the LoRA adjustments to the original model weights.
</p>
<p>
target_modules: Lists the specific parts of the model to which LoRA will be applied. These typically correspond to components within transformer blocks, such as the query, key, value, and output projections in attention mechanisms, as well as any additional modules relevant to the model architecture.
</p>
<p>
bias: Specifies how biases are treated in the adaptation process. In this case, biases are not adjusted ("none").
</p>
<p>
lora_dropout: Sets the dropout rate for the LoRA parameters, helping to prevent overfitting during fine-tuning. The dropout rate is a hyperparameter used in the training of neural networks, representing the probability that a given neuron (or unit) is temporarily "dropped" from the network during a specific iteration of training. This means that the neuron will not participate in the forward pass and its contribution to the backward pass (gradient computation) is also ignored during that iteration. Dropout is applied randomly to a subset of neurons in the network at each training step.
</p>
<p>
task_type: Indicates the type of task for which the model is being fine-tuned. The example uses TaskType.SEQ_CLS, 
suggesting a sequence classification task, such as sentiment analysis or document classification. In my case a binary classification of machine versus human generated text.
</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=0532bdc6-4317-474b-859c-4e5d2fe6f1bd">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [12]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">prepare_model_for_kbit_training</span><span class="p">,</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">get_peft_model</span><span class="p">,</span> <span class="n">TaskType</span>

<span class="n">model</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">prepare_model_for_kbit_training</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span>
    <span class="n">r</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span>
        <span class="s2">"c_attn"</span><span class="p">,</span>
        <span class="s2">"c_proj"</span><span class="p">,</span>
        <span class="s2">"c_fc"</span><span class="p">,</span>
        <span class="s2">"c_proj"</span><span class="p">,</span>
        <span class="s2">"lm_head"</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="n">bias</span><span class="o">=</span><span class="s2">"none"</span><span class="p">,</span>
    <span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">task_type</span><span class="o">=</span><span class="n">TaskType</span><span class="o">.</span><span class="n">SEQ_CLS</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">accelerator</span><span class="o">.</span><span class="n">prepare_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=1cd29494-4453-4c01-b878-ef2f4533d34d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Inspect-the-model">Inspect the model<a class="anchor-link" href="#Inspect-the-model">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=d5058f6d-185d-45af-83b9-bb7f61d4bee3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [13]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">print_trainable_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">trainable_params</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">all_param</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
        <span class="n">all_param</span> <span class="o">+=</span> <span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="n">trainable_params</span> <span class="o">+=</span> <span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"trainable params: </span><span class="si">{</span><span class="n">trainable_params</span><span class="si">}</span><span class="s2"> || all params: </span><span class="si">{</span><span class="n">all_param</span><span class="si">}</span><span class="s2"> || trainable%: </span><span class="si">{</span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">trainable_params</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">all_param</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=b14fe8f2-eccd-4cfe-9d20-ef48bc178842">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [14]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">print_trainable_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">model</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>trainable params: 5900800 || all params: 426039040 || trainable%: 1.385037390000691
</pre>
</div>
</div>
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[14]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>PeftModelForSequenceClassification(
  (base_model): LoraModel(
    (model): GPT2ForSequenceClassification(
      (transformer): GPT2Model(
        (wte): Embedding(50257, 1280)
        (wpe): Embedding(1024, 1280)
        (drop): Dropout(p=0.1, inplace=False)
        (h): ModuleList(
          (0-35): 36 x GPT2Block(
            (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=1280, out_features=3840, bias=True)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=1280, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=3840, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (c_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=True)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=1280, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1280, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=1280, out_features=5120, bias=True)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=1280, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=5120, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (c_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=5120, out_features=1280, bias=True)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=5120, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1280, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      )
      (score): ModulesToSaveWrapper(
        (original_module): Linear(in_features=1280, out_features=2, bias=False)
        (modules_to_save): ModuleDict(
          (default): Linear(in_features=1280, out_features=2, bias=False)
        )
      )
    )
  )
)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=e17b4782-03f8-4335-bfda-2a65794bff2c">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Look-at-hardware">Look at hardware<a class="anchor-link" href="#Look-at-hardware">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=c27c3328-1926-4adc-8696-b3b343ec4afd">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [15]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Available GPUs: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"GPU </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_name</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Available GPUs: 1
GPU 0: NVIDIA GeForce RTX 3090
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=bdf68d0e-8786-489d-a4b8-b7478513efea">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [16]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">!</span>nvidia-smi
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Thu May 23 03:37:32 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 3090        Off |   00000000:01:00.0 Off |                  N/A |
| 31%   42C    P2            130W /  420W |    1512MiB /  24576MiB |     16%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA GeForce RTX 4090        Off |   00000000:02:00.0 Off |                  Off |
|  0%   40C    P8             21W /  450W |    4361MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA GeForce RTX 3090 Ti     Off |   00000000:2A:00.0 Off |                  Off |
| 49%   57C    P2            354W /  450W |    1698MiB /  24564MiB |     92%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA GeForce RTX 3090        Off |   00000000:41:00.0 Off |                  N/A |
| 33%   40C    P2             57W /  420W |    1166MiB /  24576MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA GeForce RTX 4090        Off |   00000000:42:00.0 Off |                  Off |
|  0%   49C    P8             15W /  450W |      10MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA GeForce RTX 4090        Off |   00000000:61:00.0 Off |                  Off |
|  0%   40C    P8             15W /  450W |      10MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA GeForce RTX 3090 Ti     Off |   00000000:62:00.0 Off |                  Off |
| 32%   49C    P2            353W /  450W |    4768MiB /  24564MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A      2561      G   /usr/bin/gnome-shell                            4MiB |
|    0   N/A  N/A     12702      C   /usr/bin/python3                             1498MiB |
|    1   N/A  N/A      2561      G   /usr/bin/gnome-shell                           95MiB |
|    1   N/A  N/A     13002      C   /usr/bin/python3                             4256MiB |
|    2   N/A  N/A      2561      G   /usr/bin/gnome-shell                            4MiB |
|    2   N/A  N/A     12510      C   /usr/bin/python3                             1684MiB |
|    3   N/A  N/A      2561      G   /usr/bin/gnome-shell                            4MiB |
|    3   N/A  N/A     12816      C   /usr/bin/python3                             1152MiB |
|    4   N/A  N/A      2561      G   /usr/bin/gnome-shell                            6MiB |
|    5   N/A  N/A      2561      G   /usr/bin/gnome-shell                            6MiB |
|    6   N/A  N/A      2561      G   /usr/bin/gnome-shell                            4MiB |
|    6   N/A  N/A     12625      C   /usr/bin/python3                             4754MiB |
+-----------------------------------------------------------------------------------------+
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=2340f2d1-1cc7-454e-bad6-bf4ac2c46fc9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [8]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">model</span><span class="o">.</span><span class="n">is_parallelizable</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">model</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=1199841a-6519-4422-8259-2fdbb114e466">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [9]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span><span class="p">,</span> <span class="n">accuracy_score</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">logits_and_labels</span><span class="p">):</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">logits_and_labels</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
    <span class="n">f1</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">'macro'</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">'accuracy'</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span> <span class="s1">'f1'</span><span class="p">:</span> <span class="n">f1</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=b8c7f3ee-6997-4ed7-b28e-5d574831fb08">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [10]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">project_name</span> <span class="o">=</span> <span class="s2">"praxis-"</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s2">"-small-finetune"</span>
<span class="n">output_dir_path</span> <span class="o">=</span> <span class="s2">"./"</span> <span class="o">+</span> <span class="n">project_name</span>
<span class="n">output_dir_path</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[10]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>'./praxis-gpt2-large-small-finetune'</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=e7a69bde-1d50-46a0-9838-00812afbdb16">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [20]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=ca48a7aa-41d7-4c57-bfec-093d22bf4b54">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p><strong>output_dir</strong> (<code>output_dir_path</code>): This specifies the directory where outputs such as model checkpoints and logs will be saved. It's important for organizing the outputs of your training sessions.</p>
<p><strong>warmup_steps</strong> (<code>5</code>): This parameter sets the number of steps during which the learning rate will gradually increase from zero to the initially set learning rate. This warmup phase helps stabilize the model's training early on, preventing the model from diverging due to high gradient values at the start.</p>
<p><strong>per_device_train_batch_size</strong> (<code>32</code>): This sets the number of training examples to process on each device (like a GPU) during training. A higher batch size can speed up training but may require more memory.</p>
<p><strong>per_device_eval_batch_size</strong> (<code>32</code>): Similar to the training batch size, this is the number of examples to process on each device during evaluation. It determines how quickly the model can process the evaluation data.</p>
<p><strong>num_train_epochs</strong> (<code>10</code>): This defines the total number of times the training process should iterate over the entire dataset. More epochs can lead to better learning but also risk overfitting if too high.</p>
<p><strong>gradient_checkpointing</strong> (<code>False</code>): If set to <code>True</code>, this would enable gradient checkpointing to reduce memory usage at the cost of longer training time. It's useful for very deep models that otherwise would not fit into GPU memory.</p>
<p><strong>gradient_accumulation_steps</strong> (<code>2</code>): This setting allows you to accumulate gradients over multiple steps before performing an update on the model's weights. It's a way to effectively increase the batch size without increasing the memory load, which can be helpful when dealing with hardware constraints.</p>
<p><strong>max_steps</strong> (<code>500</code>): This is the maximum number of training steps to execute, regardless of how many epochs are set. Training will stop when this number of steps is reached.</p>
<p><strong>learning_rate</strong> (<code>2.5e-5</code>): This is the step size at which the optimizer updates the model’s weights. A smaller learning rate might lead to better fine-tuning but slower convergence, and vice versa.</p>
<p><strong>logging_steps</strong> (<code>200</code>): Specifies how often to log training information. The setting determines after how many steps new logs should be created, which might include loss and other metrics. More frequent logging provides finer-grained visibility into the training progress but can add computational overhead.</p>
<p><strong>bf16</strong> (<code>True</code>): This would enable training using bfloat16 precision, which is a mixed precision format with fewer bits than the standard single-precision floating point (fp32). Like fp16, it can reduce memory usage and potentially speed up training if supported by the hardware. It's particularly useful on TPUs and newer GPUs that support this format. (4090)</p>
<p><strong>fp16</strong> (<code>True</code>): This enables half-precision floating point (16-bit) training. It reduces memory usage and can speed up training, provided the hardware (like modern GPUs) supports it. (3090 and 4090)</p>
<p><strong>optim</strong> (<code>"paged_adamw_8bit"</code>): Specifies the optimizer to use. "paged_adamw_8bit" might refer to a variation of the AdamW optimizer that is optimized for lower precision and memory bandwidth, enhancing training speed and efficiency.</p>
<p><strong>logging_dir</strong> (<code>"./logs"</code>): This specifies the directory where training logs should be saved. It's used to store logs if you are using a logging framework or callback that writes out logs to files. Organizing logs in a specific directory is helpful for post-training analysis and for monitoring the training process through tools like TensorBoard.</p>
<p><strong>save_strategy</strong> (<code>"epoch"</code>): This determines how often to save model checkpoints. Setting it to <code>"epoch"</code> means the model will save checkpoints at the end of each epoch.</p>
<p><strong>save_steps</strong> (<code>50</code>): This is closely related to the <code>save_strategy</code> when set to "steps". It defines how often to save the model, specifically after how many training steps. A lower number means more frequent saves, which increases disk I/O but provides more restore points for training.</p>
<p><strong>evaluation_strategy</strong> (<code>"epoch"</code>): This configures when the model should be evaluated against the evaluation dataset. Like <code>save_strategy</code>, setting this to <code>"epoch"</code> triggers evaluations at the end of each epoch, providing feedback on model performance after it has seen the entire training dataset.</p>
<p><strong>eval_steps</strong> (<code>50</code>): This parameter determines how often to evaluate the model if the <code>evaluation_strategy</code> is set to "steps". Similar to <code>logging_steps</code>, setting this affects how frequently the model's performance is assessed on the evaluation dataset during the training process. More frequent evaluations provide a closer look at the model's performance but at the cost of increased computational overhead.</p>
<p><strong>do_eval</strong> (<code>True</code>): This flag enables the evaluation of the model on the evaluation dataset. If <code>True</code>, it will use the evaluation dataset to assess model performance based on metrics</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=d8f3cd41-fd17-4fd8-83b8-54f464df79ff">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Training">Training<a class="anchor-link" href="#Training">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=2df4e2a5-2a08-434b-983a-f6cd42f33da3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [21]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">transformers</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="n">transformers</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">777</span><span class="p">)</span>

<span class="k">if</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_train_ds</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">tokenized_eval_ds</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">transformers</span><span class="o">.</span><span class="n">TrainingArguments</span><span class="p">(</span>
        <span class="n">output_dir</span><span class="o">=</span><span class="n">output_dir_path</span><span class="p">,</span>
        <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">logging_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">logging_dir</span><span class="o">=</span><span class="n">output_dir_path</span><span class="o">+</span><span class="s2">"/logs"</span><span class="p">,</span>
        <span class="n">evaluation_strategy</span><span class="o">=</span><span class="s1">'epoch'</span><span class="p">,</span>
        <span class="n">save_strategy</span><span class="o">=</span><span class="s1">'epoch'</span><span class="p">,</span>
        <span class="n">bf16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">optim</span><span class="o">=</span><span class="s2">"paged_adamw_8bit"</span><span class="p">,</span>
        <span class="n">do_eval</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>
    <span class="n">data_collator</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">DataCollatorWithPadding</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">),</span>
<span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">resume_from_checkpoint</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># Turn to True if power goes out...</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>2024-05-23 03:37:33.590672: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-23 03:37:34.310573: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
<span class="ansi-blue-intense-fg ansi-bold">wandb</span>: Currently logged in as: <span class="ansi-yellow-fg">nispoe</span>. Use <span class="ansi-bold">`wandb login --relogin`</span> to force relogin
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
Tracking run with wandb version 0.17.0
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
Run data is saved locally in <code>/home/nispoe/kuk/Praxis/wandb/run-20240523_033735-d4l316ac</code>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
Syncing run <strong><a href="https://wandb.ai/nispoe/huggingface/runs/d4l316ac" target="_blank">expert-totem-330</a></strong> to <a href="https://wandb.ai/nispoe/huggingface" target="_blank">Weights &amp; Biases</a> (<a href="https://wandb.me/run" target="_blank">docs</a>)<br/>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
 View project at <a href="https://wandb.ai/nispoe/huggingface" target="_blank">https://wandb.ai/nispoe/huggingface</a>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
 View run at <a href="https://wandb.ai/nispoe/huggingface/runs/d4l316ac" target="_blank">https://wandb.ai/nispoe/huggingface/runs/d4l316ac</a>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<div>
<progress max="42020" style="width:300px; height:20px; vertical-align: middle;" value="42020"></progress>
      [42020/42020 14:42:25, Epoch 10/10]
    </div>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: left;">
<th>Epoch</th>
<th>Training Loss</th>
<th>Validation Loss</th>
<th>Accuracy</th>
<th>F1</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0.158900</td>
<td>0.224847</td>
<td>0.919617</td>
<td>0.913330</td>
</tr>
<tr>
<td>2</td>
<td>0.308700</td>
<td>0.192232</td>
<td>0.935582</td>
<td>0.932584</td>
</tr>
<tr>
<td>3</td>
<td>0.217800</td>
<td>0.219186</td>
<td>0.941969</td>
<td>0.939364</td>
</tr>
<tr>
<td>4</td>
<td>0.255700</td>
<td>0.207931</td>
<td>0.948771</td>
<td>0.945228</td>
</tr>
<tr>
<td>5</td>
<td>0.298000</td>
<td>0.259813</td>
<td>0.946134</td>
<td>0.942236</td>
</tr>
<tr>
<td>6</td>
<td>0.005000</td>
<td>0.308163</td>
<td>0.948216</td>
<td>0.945742</td>
</tr>
<tr>
<td>7</td>
<td>0.066700</td>
<td>0.326928</td>
<td>0.951131</td>
<td>0.947972</td>
</tr>
<tr>
<td>8</td>
<td>0.000100</td>
<td>0.384752</td>
<td>0.951548</td>
<td>0.949027</td>
</tr>
<tr>
<td>9</td>
<td>0.000000</td>
<td>0.401674</td>
<td>0.952381</td>
<td>0.949276</td>
</tr>
<tr>
<td>10</td>
<td>0.000000</td>
<td>0.424807</td>
<td>0.953353</td>
<td>0.950458</td>
</tr>
</tbody>
</table><p>
</p></div>
</div>
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[21]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>TrainOutput(global_step=42020, training_loss=0.10335866133894452, metrics={'train_runtime': 52948.5332, 'train_samples_per_second': 6.348, 'train_steps_per_second': 0.794, 'total_flos': 7.375296788692992e+17, 'train_loss': 0.10335866133894452, 'epoch': 10.0})</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=0d13982d-53cb-4c88-bd32-a98d4a5a9874">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Determine-best-checkpoint">Determine best checkpoint<a class="anchor-link" href="#Determine-best-checkpoint">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=8bd570c7-8feb-4b69-bda7-9c678bfd92c3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [22]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">!</span>ls<span class="w"> </span>-ltr<span class="w"> </span><span class="o">{</span>output_dir_path<span class="o">}</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>total 44
drwxr-xr-x 2 nispoe nispoe 4096 May 23 03:37 logs
drwxrwxr-x 2 nispoe nispoe 4096 May 23 05:06 checkpoint-4202
drwxrwxr-x 2 nispoe nispoe 4096 May 23 06:35 checkpoint-8404
drwxrwxr-x 2 nispoe nispoe 4096 May 23 08:03 checkpoint-12606
drwxrwxr-x 2 nispoe nispoe 4096 May 23 09:31 checkpoint-16808
drwxrwxr-x 2 nispoe nispoe 4096 May 23 10:59 checkpoint-21010
drwxrwxr-x 2 nispoe nispoe 4096 May 23 12:27 checkpoint-25212
drwxrwxr-x 2 nispoe nispoe 4096 May 23 13:55 checkpoint-29414
drwxrwxr-x 2 nispoe nispoe 4096 May 23 15:23 checkpoint-33616
drwxrwxr-x 2 nispoe nispoe 4096 May 23 16:51 checkpoint-37818
drwxrwxr-x 2 nispoe nispoe 4096 May 23 18:20 checkpoint-42020
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=97771269-d77c-4c1b-b264-34e082db6cbc">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [12]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.python.summary.summary_iterator</span> <span class="kn">import</span> <span class="n">summary_iterator</span>
<span class="kn">import</span> <span class="nn">glob</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># Construct the logs directory path</span>
<span class="n">logs_directory</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">'./'</span><span class="p">,</span> <span class="n">project_name</span><span class="p">,</span> <span class="s1">'logs'</span><span class="p">)</span>
<span class="n">file_pattern</span> <span class="o">=</span> <span class="s1">'events.out.tfevents.*'</span>

<span class="c1"># Retrieve all event files matching the pattern</span>
<span class="n">event_files</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">logs_directory</span><span class="p">,</span> <span class="n">file_pattern</span><span class="p">))</span>

<span class="c1"># Function to print out TensorBoard event logs</span>
<span class="k">def</span> <span class="nf">print_events_from_file</span><span class="p">(</span><span class="n">event_files</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">event_file</span> <span class="ow">in</span> <span class="n">event_files</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Reading events from file: </span><span class="si">{</span><span class="n">event_file</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">summary_iterator</span><span class="p">(</span><span class="n">event_file</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">e</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">v</span><span class="o">.</span><span class="n">HasField</span><span class="p">(</span><span class="s1">'simple_value'</span><span class="p">):</span>
                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Step: </span><span class="si">{</span><span class="n">e</span><span class="o">.</span><span class="n">step</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">v</span><span class="o">.</span><span class="n">simple_value</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>  <span class="c1"># Just in case the event file is not readable</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Failed to read </span><span class="si">{</span><span class="n">event_file</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">print_events_from_file</span><span class="p">(</span><span class="n">event_files</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Reading events from file: ./praxis-gpt2-large-small-finetune/logs/events.out.tfevents.1716453455.hephaestus.12816.0
Step: 10, train/loss: 0.8551999926567078
Step: 10, train/grad_norm: 7.100120544433594
Step: 10, train/learning_rate: 4.998810254619457e-05
Step: 10, train/epoch: 0.0023798190522938967
Step: 20, train/loss: 0.6898999810218811
Step: 20, train/grad_norm: 5.648610591888428
Step: 20, train/learning_rate: 4.997620271751657e-05
Step: 20, train/epoch: 0.004759638104587793
Step: 30, train/loss: 0.7228999733924866
Step: 30, train/grad_norm: 6.521562576293945
Step: 30, train/learning_rate: 4.9964302888838574e-05
Step: 30, train/epoch: 0.007139457389712334
Step: 40, train/loss: 0.8070999979972839
Step: 40, train/grad_norm: 7.84426212310791
Step: 40, train/learning_rate: 4.995240306016058e-05
Step: 40, train/epoch: 0.009519276209175587
Step: 50, train/loss: 0.7793999910354614
Step: 50, train/grad_norm: 4.650295734405518
Step: 50, train/learning_rate: 4.994050323148258e-05
Step: 50, train/epoch: 0.011899095959961414
Step: 60, train/loss: 0.7276999950408936
Step: 60, train/grad_norm: 6.527633190155029
Step: 60, train/learning_rate: 4.992860704078339e-05
Step: 60, train/epoch: 0.014278914779424667
Step: 70, train/loss: 0.6769999861717224
Step: 70, train/grad_norm: 7.10400390625
Step: 70, train/learning_rate: 4.9916707212105393e-05
Step: 70, train/epoch: 0.016658734530210495
Step: 80, train/loss: 0.6920999884605408
Step: 80, train/grad_norm: 7.220883369445801
Step: 80, train/learning_rate: 4.9904807383427396e-05
Step: 80, train/epoch: 0.019038552418351173
Step: 90, train/loss: 0.6732000112533569
Step: 90, train/grad_norm: 8.844443321228027
Step: 90, train/learning_rate: 4.98929075547494e-05
Step: 90, train/epoch: 0.021418372169137
Step: 100, train/loss: 0.7567999958992004
Step: 100, train/grad_norm: 9.761140823364258
Step: 100, train/learning_rate: 4.98810077260714e-05
Step: 100, train/epoch: 0.02379819191992283
Step: 110, train/loss: 0.7559000253677368
Step: 110, train/grad_norm: 5.939005374908447
Step: 110, train/learning_rate: 4.986911153537221e-05
Step: 110, train/epoch: 0.026178009808063507
Step: 120, train/loss: 0.7577999830245972
Step: 120, train/grad_norm: 6.9715895652771
Step: 120, train/learning_rate: 4.9857211706694216e-05
Step: 120, train/epoch: 0.028557829558849335
Step: 130, train/loss: 0.7328000068664551
Step: 130, train/grad_norm: 7.053349018096924
Step: 130, train/learning_rate: 4.984531187801622e-05
Step: 130, train/epoch: 0.030937649309635162
Step: 140, train/loss: 0.7026000022888184
Step: 140, train/grad_norm: 7.825043201446533
Step: 140, train/learning_rate: 4.983341204933822e-05
Step: 140, train/epoch: 0.03331746906042099
Step: 150, train/loss: 0.7240999937057495
Step: 150, train/grad_norm: 6.708011627197266
Step: 150, train/learning_rate: 4.9821512220660225e-05
Step: 150, train/epoch: 0.03569728881120682
Step: 160, train/loss: 0.6992999911308289
Step: 160, train/grad_norm: 6.707331657409668
Step: 160, train/learning_rate: 4.9809616029961035e-05
Step: 160, train/epoch: 0.03807710483670235
Step: 170, train/loss: 0.671500027179718
Step: 170, train/grad_norm: 9.25143051147461
Step: 170, train/learning_rate: 4.979771620128304e-05
Step: 170, train/epoch: 0.040456924587488174
Step: 180, train/loss: 0.7458000183105469
Step: 180, train/grad_norm: 10.381182670593262
Step: 180, train/learning_rate: 4.978581637260504e-05
Step: 180, train/epoch: 0.042836744338274
Step: 190, train/loss: 0.6626999974250793
Step: 190, train/grad_norm: 7.691777229309082
Step: 190, train/learning_rate: 4.9773916543927044e-05
Step: 190, train/epoch: 0.04521656408905983
Step: 200, train/loss: 0.7368000149726868
Step: 200, train/grad_norm: 6.423411846160889
Step: 200, train/learning_rate: 4.976201671524905e-05
Step: 200, train/epoch: 0.04759638383984566
Step: 210, train/loss: 0.6843000054359436
Step: 210, train/grad_norm: 8.940497398376465
Step: 210, train/learning_rate: 4.975012052454986e-05
Step: 210, train/epoch: 0.049976203590631485
Step: 220, train/loss: 0.6657999753952026
Step: 220, train/grad_norm: 7.389530181884766
Step: 220, train/learning_rate: 4.973822069587186e-05
Step: 220, train/epoch: 0.052356019616127014
Step: 230, train/loss: 0.6636000275611877
Step: 230, train/grad_norm: 8.274113655090332
Step: 230, train/learning_rate: 4.972632086719386e-05
Step: 230, train/epoch: 0.05473583936691284
Step: 240, train/loss: 0.6895999908447266
Step: 240, train/grad_norm: 7.856265544891357
Step: 240, train/learning_rate: 4.9714421038515866e-05
Step: 240, train/epoch: 0.05711565911769867
Step: 250, train/loss: 0.6129000186920166
Step: 250, train/grad_norm: 7.139277458190918
Step: 250, train/learning_rate: 4.970252120983787e-05
Step: 250, train/epoch: 0.0594954788684845
Step: 260, train/loss: 0.6858999729156494
Step: 260, train/grad_norm: 6.965616703033447
Step: 260, train/learning_rate: 4.969062501913868e-05
Step: 260, train/epoch: 0.061875298619270325
Step: 270, train/loss: 0.6801999807357788
Step: 270, train/grad_norm: 5.696887493133545
Step: 270, train/learning_rate: 4.967872519046068e-05
Step: 270, train/epoch: 0.06425511837005615
Step: 280, train/loss: 0.6696000099182129
Step: 280, train/grad_norm: 9.368303298950195
Step: 280, train/learning_rate: 4.9666825361782685e-05
Step: 280, train/epoch: 0.06663493812084198
Step: 290, train/loss: 0.7703999876976013
Step: 290, train/grad_norm: 10.919240951538086
Step: 290, train/learning_rate: 4.965492553310469e-05
Step: 290, train/epoch: 0.06901475787162781
Step: 300, train/loss: 0.65829998254776
Step: 300, train/grad_norm: 5.297540187835693
Step: 300, train/learning_rate: 4.964302570442669e-05
Step: 300, train/epoch: 0.07139457762241364
Step: 310, train/loss: 0.7235999703407288
Step: 310, train/grad_norm: 8.851469039916992
Step: 310, train/learning_rate: 4.96311295137275e-05
Step: 310, train/epoch: 0.07377438992261887
Step: 320, train/loss: 0.7565000057220459
Step: 320, train/grad_norm: 9.08541488647461
Step: 320, train/learning_rate: 4.9619229685049504e-05
Step: 320, train/epoch: 0.0761542096734047
Step: 330, train/loss: 0.6140000224113464
Step: 330, train/grad_norm: 6.9793596267700195
Step: 330, train/learning_rate: 4.960732985637151e-05
Step: 330, train/epoch: 0.07853402942419052
Step: 340, train/loss: 0.7555000185966492
Step: 340, train/grad_norm: 7.41993522644043
Step: 340, train/learning_rate: 4.959543002769351e-05
Step: 340, train/epoch: 0.08091384917497635
Step: 350, train/loss: 0.7310000061988831
Step: 350, train/grad_norm: 7.362585067749023
Step: 350, train/learning_rate: 4.958353019901551e-05
Step: 350, train/epoch: 0.08329366892576218
Step: 360, train/loss: 0.6097999811172485
Step: 360, train/grad_norm: 7.781685829162598
Step: 360, train/learning_rate: 4.957163400831632e-05
Step: 360, train/epoch: 0.085673488676548
Step: 370, train/loss: 0.7121000289916992
Step: 370, train/grad_norm: 7.685330867767334
Step: 370, train/learning_rate: 4.9559734179638326e-05
Step: 370, train/epoch: 0.08805330842733383
Step: 380, train/loss: 0.5688999891281128
Step: 380, train/grad_norm: 5.025183200836182
Step: 380, train/learning_rate: 4.954783435096033e-05
Step: 380, train/epoch: 0.09043312817811966
Step: 390, train/loss: 0.6064000129699707
Step: 390, train/grad_norm: 8.665024757385254
Step: 390, train/learning_rate: 4.953593452228233e-05
Step: 390, train/epoch: 0.09281294792890549
Step: 400, train/loss: 0.5975000262260437
Step: 400, train/grad_norm: 9.136650085449219
Step: 400, train/learning_rate: 4.9524034693604335e-05
Step: 400, train/epoch: 0.09519276767969131
Step: 410, train/loss: 0.6427000164985657
Step: 410, train/grad_norm: 7.828423023223877
Step: 410, train/learning_rate: 4.9512138502905145e-05
Step: 410, train/epoch: 0.09757258743047714
Step: 420, train/loss: 0.5077999830245972
Step: 420, train/grad_norm: 4.649221420288086
Step: 420, train/learning_rate: 4.950023867422715e-05
Step: 420, train/epoch: 0.09995240718126297
Step: 430, train/loss: 0.6014999747276306
Step: 430, train/grad_norm: 9.020304679870605
Step: 430, train/learning_rate: 4.948833884554915e-05
Step: 430, train/epoch: 0.1023322194814682
Step: 440, train/loss: 0.5849000215530396
Step: 440, train/grad_norm: 9.668200492858887
Step: 440, train/learning_rate: 4.9476439016871154e-05
Step: 440, train/epoch: 0.10471203923225403
Step: 450, train/loss: 0.5328999757766724
Step: 450, train/grad_norm: 6.7928690910339355
Step: 450, train/learning_rate: 4.946453918819316e-05
Step: 450, train/epoch: 0.10709185898303986
Step: 460, train/loss: 0.489300012588501
Step: 460, train/grad_norm: 5.5020856857299805
Step: 460, train/learning_rate: 4.945264299749397e-05
Step: 460, train/epoch: 0.10947167873382568
Step: 470, train/loss: 0.6431999802589417
Step: 470, train/grad_norm: 5.457697868347168
Step: 470, train/learning_rate: 4.944074316881597e-05
Step: 470, train/epoch: 0.11185149848461151
Step: 480, train/loss: 0.5557000041007996
Step: 480, train/grad_norm: 3.6364922523498535
Step: 480, train/learning_rate: 4.9428843340137973e-05
Step: 480, train/epoch: 0.11423131823539734
Step: 490, train/loss: 0.5360999703407288
Step: 490, train/grad_norm: 6.716977596282959
Step: 490, train/learning_rate: 4.9416943511459976e-05
Step: 490, train/epoch: 0.11661113798618317
Step: 500, train/loss: 0.43059998750686646
Step: 500, train/grad_norm: 3.084404706954956
Step: 500, train/learning_rate: 4.940504368278198e-05
Step: 500, train/epoch: 0.118990957736969
Step: 510, train/loss: 0.4684000015258789
Step: 510, train/grad_norm: 5.2345170974731445
Step: 510, train/learning_rate: 4.939314749208279e-05
Step: 510, train/epoch: 0.12137077748775482
Step: 520, train/loss: 0.5295000076293945
Step: 520, train/grad_norm: 3.4763853549957275
Step: 520, train/learning_rate: 4.938124766340479e-05
Step: 520, train/epoch: 0.12375059723854065
Step: 530, train/loss: 0.5304999947547913
Step: 530, train/grad_norm: 17.784759521484375
Step: 530, train/learning_rate: 4.9369347834726796e-05
Step: 530, train/epoch: 0.12613041698932648
Step: 540, train/loss: 0.48829999566078186
Step: 540, train/grad_norm: 5.985165596008301
Step: 540, train/learning_rate: 4.93574480060488e-05
Step: 540, train/epoch: 0.1285102367401123
Step: 550, train/loss: 0.47760000824928284
Step: 550, train/grad_norm: 5.308067798614502
Step: 550, train/learning_rate: 4.93455481773708e-05
Step: 550, train/epoch: 0.13089005649089813
Step: 560, train/loss: 0.5152999758720398
Step: 560, train/grad_norm: 6.443341255187988
Step: 560, train/learning_rate: 4.933365198667161e-05
Step: 560, train/epoch: 0.13326987624168396
Step: 570, train/loss: 0.43529999256134033
Step: 570, train/grad_norm: 4.117165565490723
Step: 570, train/learning_rate: 4.9321752157993615e-05
Step: 570, train/epoch: 0.1356496959924698
Step: 580, train/loss: 0.424699991941452
Step: 580, train/grad_norm: 5.35001277923584
Step: 580, train/learning_rate: 4.930985232931562e-05
Step: 580, train/epoch: 0.13802951574325562
Step: 590, train/loss: 0.46560001373291016
Step: 590, train/grad_norm: 4.287328720092773
Step: 590, train/learning_rate: 4.929795250063762e-05
Step: 590, train/epoch: 0.14040933549404144
Step: 600, train/loss: 0.7175999879837036
Step: 600, train/grad_norm: 6.143311977386475
Step: 600, train/learning_rate: 4.9286052671959624e-05
Step: 600, train/epoch: 0.14278915524482727
Step: 610, train/loss: 0.4101000130176544
Step: 610, train/grad_norm: 4.6918721199035645
Step: 610, train/learning_rate: 4.9274156481260434e-05
Step: 610, train/epoch: 0.1451689600944519
Step: 620, train/loss: 0.453900009393692
Step: 620, train/grad_norm: 4.288401126861572
Step: 620, train/learning_rate: 4.926225665258244e-05
Step: 620, train/epoch: 0.14754877984523773
Step: 630, train/loss: 0.4909000098705292
Step: 630, train/grad_norm: 9.606141090393066
Step: 630, train/learning_rate: 4.925035682390444e-05
Step: 630, train/epoch: 0.14992859959602356
Step: 640, train/loss: 0.5045999884605408
Step: 640, train/grad_norm: 7.629023551940918
Step: 640, train/learning_rate: 4.923845699522644e-05
Step: 640, train/epoch: 0.1523084193468094
Step: 650, train/loss: 0.487199991941452
Step: 650, train/grad_norm: 13.91455364227295
Step: 650, train/learning_rate: 4.9226557166548446e-05
Step: 650, train/epoch: 0.15468823909759521
Step: 660, train/loss: 0.420199990272522
Step: 660, train/grad_norm: 12.921806335449219
Step: 660, train/learning_rate: 4.9214660975849256e-05
Step: 660, train/epoch: 0.15706805884838104
Step: 670, train/loss: 0.5929999947547913
Step: 670, train/grad_norm: 5.116776943206787
Step: 670, train/learning_rate: 4.920276114717126e-05
Step: 670, train/epoch: 0.15944787859916687
Step: 680, train/loss: 0.46540001034736633
Step: 680, train/grad_norm: 10.789108276367188
Step: 680, train/learning_rate: 4.919086131849326e-05
Step: 680, train/epoch: 0.1618276983499527
Step: 690, train/loss: 0.45840001106262207
Step: 690, train/grad_norm: 3.9364452362060547
Step: 690, train/learning_rate: 4.9178961489815265e-05
Step: 690, train/epoch: 0.16420751810073853
Step: 700, train/loss: 0.4332999885082245
Step: 700, train/grad_norm: 14.1598539352417
Step: 700, train/learning_rate: 4.916706166113727e-05
Step: 700, train/epoch: 0.16658733785152435
Step: 710, train/loss: 0.4587000012397766
Step: 710, train/grad_norm: 6.167932510375977
Step: 710, train/learning_rate: 4.915516547043808e-05
Step: 710, train/epoch: 0.16896715760231018
Step: 720, train/loss: 0.4366999864578247
Step: 720, train/grad_norm: 5.941322326660156
Step: 720, train/learning_rate: 4.914326564176008e-05
Step: 720, train/epoch: 0.171346977353096
Step: 730, train/loss: 0.39959999918937683
Step: 730, train/grad_norm: 10.446808815002441
Step: 730, train/learning_rate: 4.9131365813082084e-05
Step: 730, train/epoch: 0.17372679710388184
Step: 740, train/loss: 0.45320001244544983
Step: 740, train/grad_norm: 12.653345108032227
Step: 740, train/learning_rate: 4.911946598440409e-05
Step: 740, train/epoch: 0.17610661685466766
Step: 750, train/loss: 0.4690000116825104
Step: 750, train/grad_norm: 2.204700231552124
Step: 750, train/learning_rate: 4.910756615572609e-05
Step: 750, train/epoch: 0.1784864366054535
Step: 760, train/loss: 0.36480000615119934
Step: 760, train/grad_norm: 5.767719268798828
Step: 760, train/learning_rate: 4.90956699650269e-05
Step: 760, train/epoch: 0.18086625635623932
Step: 770, train/loss: 0.4050000011920929
Step: 770, train/grad_norm: 6.205139636993408
Step: 770, train/learning_rate: 4.90837701363489e-05
Step: 770, train/epoch: 0.18324607610702515
Step: 780, train/loss: 0.367900013923645
Step: 780, train/grad_norm: 6.536998748779297
Step: 780, train/learning_rate: 4.9071870307670906e-05
Step: 780, train/epoch: 0.18562589585781097
Step: 790, train/loss: 0.40880000591278076
Step: 790, train/grad_norm: 6.526260852813721
Step: 790, train/learning_rate: 4.905997047899291e-05
Step: 790, train/epoch: 0.1880057156085968
Step: 800, train/loss: 0.47510001063346863
Step: 800, train/grad_norm: 8.289239883422852
Step: 800, train/learning_rate: 4.904807065031491e-05
Step: 800, train/epoch: 0.19038553535938263
Step: 810, train/loss: 0.3716999888420105
Step: 810, train/grad_norm: 3.22589111328125
Step: 810, train/learning_rate: 4.903617445961572e-05
Step: 810, train/epoch: 0.19276535511016846
Step: 820, train/loss: 0.36880001425743103
Step: 820, train/grad_norm: 5.06797456741333
Step: 820, train/learning_rate: 4.9024274630937725e-05
Step: 820, train/epoch: 0.19514517486095428
Step: 830, train/loss: 0.38029998540878296
Step: 830, train/grad_norm: 13.099085807800293
Step: 830, train/learning_rate: 4.901237480225973e-05
Step: 830, train/epoch: 0.1975249946117401
Step: 840, train/loss: 0.41609999537467957
Step: 840, train/grad_norm: 14.734464645385742
Step: 840, train/learning_rate: 4.900047497358173e-05
Step: 840, train/epoch: 0.19990481436252594
Step: 850, train/loss: 0.26669999957084656
Step: 850, train/grad_norm: 7.133120536804199
Step: 850, train/learning_rate: 4.8988575144903734e-05
Step: 850, train/epoch: 0.20228461921215057
Step: 860, train/loss: 0.3066999912261963
Step: 860, train/grad_norm: 13.628317832946777
Step: 860, train/learning_rate: 4.8976678954204544e-05
Step: 860, train/epoch: 0.2046644389629364
Step: 870, train/loss: 0.3215999901294708
Step: 870, train/grad_norm: 4.1142191886901855
Step: 870, train/learning_rate: 4.896477912552655e-05
Step: 870, train/epoch: 0.20704425871372223
Step: 880, train/loss: 0.2797999978065491
Step: 880, train/grad_norm: 5.484179973602295
Step: 880, train/learning_rate: 4.895287929684855e-05
Step: 880, train/epoch: 0.20942407846450806
Step: 890, train/loss: 0.39309999346733093
Step: 890, train/grad_norm: 11.384241104125977
Step: 890, train/learning_rate: 4.8940979468170553e-05
Step: 890, train/epoch: 0.21180389821529388
Step: 900, train/loss: 0.48249998688697815
Step: 900, train/grad_norm: 9.41218376159668
Step: 900, train/learning_rate: 4.8929079639492556e-05
Step: 900, train/epoch: 0.2141837179660797
Step: 910, train/loss: 0.31209999322891235
Step: 910, train/grad_norm: 5.7601237297058105
Step: 910, train/learning_rate: 4.8917183448793367e-05
Step: 910, train/epoch: 0.21656353771686554
Step: 920, train/loss: 0.27070000767707825
Step: 920, train/grad_norm: 18.198013305664062
Step: 920, train/learning_rate: 4.890528362011537e-05
Step: 920, train/epoch: 0.21894335746765137
Step: 930, train/loss: 0.3224000036716461
Step: 930, train/grad_norm: 10.413795471191406
Step: 930, train/learning_rate: 4.889338379143737e-05
Step: 930, train/epoch: 0.2213231772184372
Step: 940, train/loss: 0.3434000015258789
Step: 940, train/grad_norm: 14.59812068939209
Step: 940, train/learning_rate: 4.8881483962759376e-05
Step: 940, train/epoch: 0.22370299696922302
Step: 950, train/loss: 0.16689999401569366
Step: 950, train/grad_norm: 9.70592212677002
Step: 950, train/learning_rate: 4.886958413408138e-05
Step: 950, train/epoch: 0.22608281672000885
Step: 960, train/loss: 0.6011000275611877
Step: 960, train/grad_norm: 8.535191535949707
Step: 960, train/learning_rate: 4.885768794338219e-05
Step: 960, train/epoch: 0.22846263647079468
Step: 970, train/loss: 0.26980000734329224
Step: 970, train/grad_norm: 14.000151634216309
Step: 970, train/learning_rate: 4.884578811470419e-05
Step: 970, train/epoch: 0.2308424562215805
Step: 980, train/loss: 0.42239999771118164
Step: 980, train/grad_norm: 1.3469182252883911
Step: 980, train/learning_rate: 4.8833888286026195e-05
Step: 980, train/epoch: 0.23322227597236633
Step: 990, train/loss: 0.41200000047683716
Step: 990, train/grad_norm: 11.889046669006348
Step: 990, train/learning_rate: 4.88219884573482e-05
Step: 990, train/epoch: 0.23560209572315216
Step: 1000, train/loss: 0.357699990272522
Step: 1000, train/grad_norm: 9.355470657348633
Step: 1000, train/learning_rate: 4.88100886286702e-05
Step: 1000, train/epoch: 0.237981915473938
Step: 1010, train/loss: 0.3492000102996826
Step: 1010, train/grad_norm: 9.50534439086914
Step: 1010, train/learning_rate: 4.879819243797101e-05
Step: 1010, train/epoch: 0.24036173522472382
Step: 1020, train/loss: 0.41819998621940613
Step: 1020, train/grad_norm: 4.53497838973999
Step: 1020, train/learning_rate: 4.8786292609293014e-05
Step: 1020, train/epoch: 0.24274155497550964
Step: 1030, train/loss: 0.37290000915527344
Step: 1030, train/grad_norm: 12.045733451843262
Step: 1030, train/learning_rate: 4.877439278061502e-05
Step: 1030, train/epoch: 0.24512137472629547
Step: 1040, train/loss: 0.387800008058548
Step: 1040, train/grad_norm: 8.717670440673828
Step: 1040, train/learning_rate: 4.876249295193702e-05
Step: 1040, train/epoch: 0.2475011944770813
Step: 1050, train/loss: 0.4271000027656555
Step: 1050, train/grad_norm: 4.7214555740356445
Step: 1050, train/learning_rate: 4.875059676123783e-05
Step: 1050, train/epoch: 0.24988101422786713
Step: 1060, train/loss: 0.39570000767707825
Step: 1060, train/grad_norm: 0.5565336346626282
Step: 1060, train/learning_rate: 4.873869693255983e-05
Step: 1060, train/epoch: 0.25226083397865295
Step: 1070, train/loss: 0.4691999852657318
Step: 1070, train/grad_norm: 3.877540111541748
Step: 1070, train/learning_rate: 4.8726797103881836e-05
Step: 1070, train/epoch: 0.2546406388282776
Step: 1080, train/loss: 0.47369998693466187
Step: 1080, train/grad_norm: 5.960870742797852
Step: 1080, train/learning_rate: 4.871489727520384e-05
Step: 1080, train/epoch: 0.2570204734802246
Step: 1090, train/loss: 0.22910000383853912
Step: 1090, train/grad_norm: 1.8876100778579712
Step: 1090, train/learning_rate: 4.870299744652584e-05
Step: 1090, train/epoch: 0.25940027832984924
Step: 1100, train/loss: 0.32420000433921814
Step: 1100, train/grad_norm: 13.955580711364746
Step: 1100, train/learning_rate: 4.869110125582665e-05
Step: 1100, train/epoch: 0.26178011298179626
Step: 1110, train/loss: 0.5716999769210815
Step: 1110, train/grad_norm: 6.877084732055664
Step: 1110, train/learning_rate: 4.8679201427148655e-05
Step: 1110, train/epoch: 0.2641599178314209
Step: 1120, train/loss: 0.359499990940094
Step: 1120, train/grad_norm: 7.933028697967529
Step: 1120, train/learning_rate: 4.866730159847066e-05
Step: 1120, train/epoch: 0.2665397524833679
Step: 1130, train/loss: 0.40869998931884766
Step: 1130, train/grad_norm: 13.346365928649902
Step: 1130, train/learning_rate: 4.865540176979266e-05
Step: 1130, train/epoch: 0.26891955733299255
Step: 1140, train/loss: 0.2928999960422516
Step: 1140, train/grad_norm: 8.603372573852539
Step: 1140, train/learning_rate: 4.8643501941114664e-05
Step: 1140, train/epoch: 0.2712993919849396
Step: 1150, train/loss: 0.4325999915599823
Step: 1150, train/grad_norm: 13.874534606933594
Step: 1150, train/learning_rate: 4.8631605750415474e-05
Step: 1150, train/epoch: 0.2736791968345642
Step: 1160, train/loss: 0.26249998807907104
Step: 1160, train/grad_norm: 6.817043781280518
Step: 1160, train/learning_rate: 4.861970592173748e-05
Step: 1160, train/epoch: 0.27605903148651123
Step: 1170, train/loss: 0.4869000017642975
Step: 1170, train/grad_norm: 9.298770904541016
Step: 1170, train/learning_rate: 4.860780609305948e-05
Step: 1170, train/epoch: 0.27843883633613586
Step: 1180, train/loss: 0.23669999837875366
Step: 1180, train/grad_norm: 8.407005310058594
Step: 1180, train/learning_rate: 4.859590626438148e-05
Step: 1180, train/epoch: 0.2808186709880829
Step: 1190, train/loss: 0.39719998836517334
Step: 1190, train/grad_norm: 5.360493183135986
Step: 1190, train/learning_rate: 4.8584006435703486e-05
Step: 1190, train/epoch: 0.2831984758377075
Step: 1200, train/loss: 0.5544999837875366
Step: 1200, train/grad_norm: 11.676898956298828
Step: 1200, train/learning_rate: 4.8572110245004296e-05
Step: 1200, train/epoch: 0.28557831048965454
Step: 1210, train/loss: 0.19449999928474426
Step: 1210, train/grad_norm: 15.926640510559082
Step: 1210, train/learning_rate: 4.85602104163263e-05
Step: 1210, train/epoch: 0.2879581153392792
Step: 1220, train/loss: 0.47780001163482666
Step: 1220, train/grad_norm: 8.376925468444824
Step: 1220, train/learning_rate: 4.85483105876483e-05
Step: 1220, train/epoch: 0.2903379201889038
Step: 1230, train/loss: 0.3037000000476837
Step: 1230, train/grad_norm: 30.05551528930664
Step: 1230, train/learning_rate: 4.8536410758970305e-05
Step: 1230, train/epoch: 0.29271775484085083
Step: 1240, train/loss: 0.4512999951839447
Step: 1240, train/grad_norm: 14.353979110717773
Step: 1240, train/learning_rate: 4.852451093029231e-05
Step: 1240, train/epoch: 0.29509755969047546
Step: 1250, train/loss: 0.3677000105381012
Step: 1250, train/grad_norm: 5.883076190948486
Step: 1250, train/learning_rate: 4.851261473959312e-05
Step: 1250, train/epoch: 0.2974773943424225
Step: 1260, train/loss: 0.3788999915122986
Step: 1260, train/grad_norm: 8.98264217376709
Step: 1260, train/learning_rate: 4.850071491091512e-05
Step: 1260, train/epoch: 0.2998571991920471
Step: 1270, train/loss: 0.28949999809265137
Step: 1270, train/grad_norm: 8.603490829467773
Step: 1270, train/learning_rate: 4.8488815082237124e-05
Step: 1270, train/epoch: 0.30223703384399414
Step: 1280, train/loss: 0.3156999945640564
Step: 1280, train/grad_norm: 14.026289939880371
Step: 1280, train/learning_rate: 4.847691525355913e-05
Step: 1280, train/epoch: 0.3046168386936188
Step: 1290, train/loss: 0.15060000121593475
Step: 1290, train/grad_norm: 0.30045998096466064
Step: 1290, train/learning_rate: 4.846501542488113e-05
Step: 1290, train/epoch: 0.3069966733455658
Step: 1300, train/loss: 0.39320001006126404
Step: 1300, train/grad_norm: 8.851921081542969
Step: 1300, train/learning_rate: 4.845311923418194e-05
Step: 1300, train/epoch: 0.30937647819519043
Step: 1310, train/loss: 0.3610999882221222
Step: 1310, train/grad_norm: 15.595401763916016
Step: 1310, train/learning_rate: 4.8441219405503944e-05
Step: 1310, train/epoch: 0.31175631284713745
Step: 1320, train/loss: 0.3752000033855438
Step: 1320, train/grad_norm: 11.261553764343262
Step: 1320, train/learning_rate: 4.8429319576825947e-05
Step: 1320, train/epoch: 0.3141361176967621
Step: 1330, train/loss: 0.24729999899864197
Step: 1330, train/grad_norm: 1.2083361148834229
Step: 1330, train/learning_rate: 4.841741974814795e-05
Step: 1330, train/epoch: 0.3165159523487091
Step: 1340, train/loss: 0.23240000009536743
Step: 1340, train/grad_norm: 1.7752498388290405
Step: 1340, train/learning_rate: 4.840551991946995e-05
Step: 1340, train/epoch: 0.31889575719833374
Step: 1350, train/loss: 0.35120001435279846
Step: 1350, train/grad_norm: 6.784293174743652
Step: 1350, train/learning_rate: 4.839362372877076e-05
Step: 1350, train/epoch: 0.32127559185028076
Step: 1360, train/loss: 0.2865000069141388
Step: 1360, train/grad_norm: 5.709946155548096
Step: 1360, train/learning_rate: 4.8381723900092766e-05
Step: 1360, train/epoch: 0.3236553966999054
Step: 1370, train/loss: 0.3537999987602234
Step: 1370, train/grad_norm: 12.493870735168457
Step: 1370, train/learning_rate: 4.836982407141477e-05
Step: 1370, train/epoch: 0.3260352313518524
Step: 1380, train/loss: 0.33559998869895935
Step: 1380, train/grad_norm: 11.819862365722656
Step: 1380, train/learning_rate: 4.835792424273677e-05
Step: 1380, train/epoch: 0.32841503620147705
Step: 1390, train/loss: 0.4542999863624573
Step: 1390, train/grad_norm: 9.991095542907715
Step: 1390, train/learning_rate: 4.8346024414058775e-05
Step: 1390, train/epoch: 0.3307948708534241
Step: 1400, train/loss: 0.35600000619888306
Step: 1400, train/grad_norm: 3.3564717769622803
Step: 1400, train/learning_rate: 4.8334128223359585e-05
Step: 1400, train/epoch: 0.3331746757030487
Step: 1410, train/loss: 0.3149999976158142
Step: 1410, train/grad_norm: 14.596794128417969
Step: 1410, train/learning_rate: 4.832222839468159e-05
Step: 1410, train/epoch: 0.3355545103549957
Step: 1420, train/loss: 0.3391999900341034
Step: 1420, train/grad_norm: 7.171910762786865
Step: 1420, train/learning_rate: 4.831032856600359e-05
Step: 1420, train/epoch: 0.33793431520462036
Step: 1430, train/loss: 0.2906000018119812
Step: 1430, train/grad_norm: 6.239994049072266
Step: 1430, train/learning_rate: 4.8298428737325594e-05
Step: 1430, train/epoch: 0.3403141498565674
Step: 1440, train/loss: 0.2980000078678131
Step: 1440, train/grad_norm: 9.494361877441406
Step: 1440, train/learning_rate: 4.82865289086476e-05
Step: 1440, train/epoch: 0.342693954706192
Step: 1450, train/loss: 0.30160000920295715
Step: 1450, train/grad_norm: 8.238676071166992
Step: 1450, train/learning_rate: 4.827463271794841e-05
Step: 1450, train/epoch: 0.34507375955581665
Step: 1460, train/loss: 0.22830000519752502
Step: 1460, train/grad_norm: 1.7857112884521484
Step: 1460, train/learning_rate: 4.826273288927041e-05
Step: 1460, train/epoch: 0.34745359420776367
Step: 1470, train/loss: 0.40610000491142273
Step: 1470, train/grad_norm: 15.021484375
Step: 1470, train/learning_rate: 4.825083306059241e-05
Step: 1470, train/epoch: 0.3498333990573883
Step: 1480, train/loss: 0.3508000075817108
Step: 1480, train/grad_norm: 21.093063354492188
Step: 1480, train/learning_rate: 4.8238933231914416e-05
Step: 1480, train/epoch: 0.3522132337093353
Step: 1490, train/loss: 0.13600000739097595
Step: 1490, train/grad_norm: 5.351532459259033
Step: 1490, train/learning_rate: 4.822703340323642e-05
Step: 1490, train/epoch: 0.35459303855895996
Step: 1500, train/loss: 0.24050000309944153
Step: 1500, train/grad_norm: 8.240772247314453
Step: 1500, train/learning_rate: 4.821513721253723e-05
Step: 1500, train/epoch: 0.356972873210907
Step: 1510, train/loss: 0.3864000141620636
Step: 1510, train/grad_norm: 11.872204780578613
Step: 1510, train/learning_rate: 4.820323738385923e-05
Step: 1510, train/epoch: 0.3593526780605316
Step: 1520, train/loss: 0.4036000072956085
Step: 1520, train/grad_norm: 1.4701650142669678
Step: 1520, train/learning_rate: 4.8191337555181235e-05
Step: 1520, train/epoch: 0.36173251271247864
Step: 1530, train/loss: 0.46470001339912415
Step: 1530, train/grad_norm: 10.211980819702148
Step: 1530, train/learning_rate: 4.817943772650324e-05
Step: 1530, train/epoch: 0.36411231756210327
Step: 1540, train/loss: 0.27000001072883606
Step: 1540, train/grad_norm: 14.258322715759277
Step: 1540, train/learning_rate: 4.816753789782524e-05
Step: 1540, train/epoch: 0.3664921522140503
Step: 1550, train/loss: 0.4499000012874603
Step: 1550, train/grad_norm: 2.382321357727051
Step: 1550, train/learning_rate: 4.815564170712605e-05
Step: 1550, train/epoch: 0.3688719570636749
Step: 1560, train/loss: 0.25380000472068787
Step: 1560, train/grad_norm: 2.5882372856140137
Step: 1560, train/learning_rate: 4.8143741878448054e-05
Step: 1560, train/epoch: 0.37125179171562195
Step: 1570, train/loss: 0.28290000557899475
Step: 1570, train/grad_norm: 14.305208206176758
Step: 1570, train/learning_rate: 4.813184204977006e-05
Step: 1570, train/epoch: 0.3736315965652466
Step: 1580, train/loss: 0.4198000133037567
Step: 1580, train/grad_norm: 12.95875358581543
Step: 1580, train/learning_rate: 4.811994222109206e-05
Step: 1580, train/epoch: 0.3760114312171936
Step: 1590, train/loss: 0.3353999853134155
Step: 1590, train/grad_norm: 3.08262300491333
Step: 1590, train/learning_rate: 4.810804239241406e-05
Step: 1590, train/epoch: 0.37839123606681824
Step: 1600, train/loss: 0.26010000705718994
Step: 1600, train/grad_norm: 18.36802101135254
Step: 1600, train/learning_rate: 4.809614620171487e-05
Step: 1600, train/epoch: 0.38077107071876526
Step: 1610, train/loss: 0.3736000061035156
Step: 1610, train/grad_norm: 7.599438190460205
Step: 1610, train/learning_rate: 4.8084246373036876e-05
Step: 1610, train/epoch: 0.3831508755683899
Step: 1620, train/loss: 0.5040000081062317
Step: 1620, train/grad_norm: 19.03084373474121
Step: 1620, train/learning_rate: 4.807234654435888e-05
Step: 1620, train/epoch: 0.3855307102203369
Step: 1630, train/loss: 0.313400000333786
Step: 1630, train/grad_norm: 5.9646220207214355
Step: 1630, train/learning_rate: 4.806044671568088e-05
Step: 1630, train/epoch: 0.38791051506996155
Step: 1640, train/loss: 0.36910000443458557
Step: 1640, train/grad_norm: 6.529045581817627
Step: 1640, train/learning_rate: 4.8048546887002885e-05
Step: 1640, train/epoch: 0.39029034972190857
Step: 1650, train/loss: 0.4016999900341034
Step: 1650, train/grad_norm: 15.475188255310059
Step: 1650, train/learning_rate: 4.8036650696303695e-05
Step: 1650, train/epoch: 0.3926701545715332
Step: 1660, train/loss: 0.2660999894142151
Step: 1660, train/grad_norm: 2.456979990005493
Step: 1660, train/learning_rate: 4.80247508676257e-05
Step: 1660, train/epoch: 0.3950499892234802
Step: 1670, train/loss: 0.3328999876976013
Step: 1670, train/grad_norm: 8.690262794494629
Step: 1670, train/learning_rate: 4.80128510389477e-05
Step: 1670, train/epoch: 0.39742979407310486
Step: 1680, train/loss: 0.23010000586509705
Step: 1680, train/grad_norm: 8.497071266174316
Step: 1680, train/learning_rate: 4.8000951210269704e-05
Step: 1680, train/epoch: 0.3998096287250519
Step: 1690, train/loss: 0.35420000553131104
Step: 1690, train/grad_norm: 14.811062812805176
Step: 1690, train/learning_rate: 4.798905138159171e-05
Step: 1690, train/epoch: 0.4021894335746765
Step: 1700, train/loss: 0.20759999752044678
Step: 1700, train/grad_norm: 0.6487597227096558
Step: 1700, train/learning_rate: 4.797715519089252e-05
Step: 1700, train/epoch: 0.40456923842430115
Step: 1710, train/loss: 0.5220000147819519
Step: 1710, train/grad_norm: 3.1356208324432373
Step: 1710, train/learning_rate: 4.796525536221452e-05
Step: 1710, train/epoch: 0.40694907307624817
Step: 1720, train/loss: 0.47350001335144043
Step: 1720, train/grad_norm: 12.674829483032227
Step: 1720, train/learning_rate: 4.7953355533536524e-05
Step: 1720, train/epoch: 0.4093288779258728
Step: 1730, train/loss: 0.30239999294281006
Step: 1730, train/grad_norm: 7.198310852050781
Step: 1730, train/learning_rate: 4.7941455704858527e-05
Step: 1730, train/epoch: 0.4117087125778198
Step: 1740, train/loss: 0.4074000120162964
Step: 1740, train/grad_norm: 10.828824996948242
Step: 1740, train/learning_rate: 4.792955587618053e-05
Step: 1740, train/epoch: 0.41408851742744446
Step: 1750, train/loss: 0.4569000005722046
Step: 1750, train/grad_norm: 25.234357833862305
Step: 1750, train/learning_rate: 4.791765968548134e-05
Step: 1750, train/epoch: 0.4164683520793915
Step: 1760, train/loss: 0.32919999957084656
Step: 1760, train/grad_norm: 4.012948513031006
Step: 1760, train/learning_rate: 4.790575985680334e-05
Step: 1760, train/epoch: 0.4188481569290161
Step: 1770, train/loss: 0.4011000096797943
Step: 1770, train/grad_norm: 12.28742504119873
Step: 1770, train/learning_rate: 4.7893860028125346e-05
Step: 1770, train/epoch: 0.42122799158096313
Step: 1780, train/loss: 0.25589999556541443
Step: 1780, train/grad_norm: 5.443411350250244
Step: 1780, train/learning_rate: 4.788196019944735e-05
Step: 1780, train/epoch: 0.42360779643058777
Step: 1790, train/loss: 0.1914999932050705
Step: 1790, train/grad_norm: 7.025436878204346
Step: 1790, train/learning_rate: 4.787006037076935e-05
Step: 1790, train/epoch: 0.4259876310825348
Step: 1800, train/loss: 0.303600013256073
Step: 1800, train/grad_norm: 19.545591354370117
Step: 1800, train/learning_rate: 4.785816418007016e-05
Step: 1800, train/epoch: 0.4283674359321594
Step: 1810, train/loss: 0.28780001401901245
Step: 1810, train/grad_norm: 0.9630167484283447
Step: 1810, train/learning_rate: 4.7846264351392165e-05
Step: 1810, train/epoch: 0.43074727058410645
Step: 1820, train/loss: 0.27889999747276306
Step: 1820, train/grad_norm: 30.700498580932617
Step: 1820, train/learning_rate: 4.783436452271417e-05
Step: 1820, train/epoch: 0.4331270754337311
Step: 1830, train/loss: 0.2939000129699707
Step: 1830, train/grad_norm: 10.268660545349121
Step: 1830, train/learning_rate: 4.782246469403617e-05
Step: 1830, train/epoch: 0.4355069100856781
Step: 1840, train/loss: 0.4318999946117401
Step: 1840, train/grad_norm: 9.844583511352539
Step: 1840, train/learning_rate: 4.7810564865358174e-05
Step: 1840, train/epoch: 0.43788671493530273
Step: 1850, train/loss: 0.2948000133037567
Step: 1850, train/grad_norm: 20.45771026611328
Step: 1850, train/learning_rate: 4.7798668674658984e-05
Step: 1850, train/epoch: 0.44026654958724976
Step: 1860, train/loss: 0.44190001487731934
Step: 1860, train/grad_norm: 9.665921211242676
Step: 1860, train/learning_rate: 4.778676884598099e-05
Step: 1860, train/epoch: 0.4426463544368744
Step: 1870, train/loss: 0.4059999883174896
Step: 1870, train/grad_norm: 5.663783550262451
Step: 1870, train/learning_rate: 4.777486901730299e-05
Step: 1870, train/epoch: 0.4450261890888214
Step: 1880, train/loss: 0.4196000099182129
Step: 1880, train/grad_norm: 10.584158897399902
Step: 1880, train/learning_rate: 4.776296918862499e-05
Step: 1880, train/epoch: 0.44740599393844604
Step: 1890, train/loss: 0.2678999900817871
Step: 1890, train/grad_norm: 8.344011306762695
Step: 1890, train/learning_rate: 4.7751069359946996e-05
Step: 1890, train/epoch: 0.44978582859039307
Step: 1900, train/loss: 0.273499995470047
Step: 1900, train/grad_norm: 9.323576927185059
Step: 1900, train/learning_rate: 4.7739173169247806e-05
Step: 1900, train/epoch: 0.4521656334400177
Step: 1910, train/loss: 0.1396999955177307
Step: 1910, train/grad_norm: 0.7511630058288574
Step: 1910, train/learning_rate: 4.772727334056981e-05
Step: 1910, train/epoch: 0.4545454680919647
Step: 1920, train/loss: 0.32109999656677246
Step: 1920, train/grad_norm: 12.552872657775879
Step: 1920, train/learning_rate: 4.771537351189181e-05
Step: 1920, train/epoch: 0.45692527294158936
Step: 1930, train/loss: 0.36070001125335693
Step: 1930, train/grad_norm: 9.102118492126465
Step: 1930, train/learning_rate: 4.7703473683213815e-05
Step: 1930, train/epoch: 0.4593051075935364
Step: 1940, train/loss: 0.3598000109195709
Step: 1940, train/grad_norm: 9.33024787902832
Step: 1940, train/learning_rate: 4.769157385453582e-05
Step: 1940, train/epoch: 0.461684912443161
Step: 1950, train/loss: 0.3734000027179718
Step: 1950, train/grad_norm: 14.7578125
Step: 1950, train/learning_rate: 4.767967766383663e-05
Step: 1950, train/epoch: 0.46406471729278564
Step: 1960, train/loss: 0.34040001034736633
Step: 1960, train/grad_norm: 10.610481262207031
Step: 1960, train/learning_rate: 4.766777783515863e-05
Step: 1960, train/epoch: 0.46644455194473267
Step: 1970, train/loss: 0.3091999888420105
Step: 1970, train/grad_norm: 3.679908037185669
Step: 1970, train/learning_rate: 4.7655878006480634e-05
Step: 1970, train/epoch: 0.4688243567943573
Step: 1980, train/loss: 0.35179999470710754
Step: 1980, train/grad_norm: 2.6386702060699463
Step: 1980, train/learning_rate: 4.764397817780264e-05
Step: 1980, train/epoch: 0.4712041914463043
Step: 1990, train/loss: 0.22689999639987946
Step: 1990, train/grad_norm: 2.0776076316833496
Step: 1990, train/learning_rate: 4.763207834912464e-05
Step: 1990, train/epoch: 0.47358399629592896
Step: 2000, train/loss: 0.5934000015258789
Step: 2000, train/grad_norm: 17.94670295715332
Step: 2000, train/learning_rate: 4.762018215842545e-05
Step: 2000, train/epoch: 0.475963830947876
Step: 2010, train/loss: 0.2700999975204468
Step: 2010, train/grad_norm: 13.428177833557129
Step: 2010, train/learning_rate: 4.760828232974745e-05
Step: 2010, train/epoch: 0.4783436357975006
Step: 2020, train/loss: 0.2535000145435333
Step: 2020, train/grad_norm: 12.677547454833984
Step: 2020, train/learning_rate: 4.7596382501069456e-05
Step: 2020, train/epoch: 0.48072347044944763
Step: 2030, train/loss: 0.23890000581741333
Step: 2030, train/grad_norm: 7.713940620422363
Step: 2030, train/learning_rate: 4.758448267239146e-05
Step: 2030, train/epoch: 0.48310327529907227
Step: 2040, train/loss: 0.17350000143051147
Step: 2040, train/grad_norm: 1.2874350547790527
Step: 2040, train/learning_rate: 4.757258284371346e-05
Step: 2040, train/epoch: 0.4854831099510193
Step: 2050, train/loss: 0.2766999900341034
Step: 2050, train/grad_norm: 0.24571338295936584
Step: 2050, train/learning_rate: 4.756068665301427e-05
Step: 2050, train/epoch: 0.4878629148006439
Step: 2060, train/loss: 0.20669999718666077
Step: 2060, train/grad_norm: 9.445430755615234
Step: 2060, train/learning_rate: 4.7548786824336275e-05
Step: 2060, train/epoch: 0.49024274945259094
Step: 2070, train/loss: 0.4862000048160553
Step: 2070, train/grad_norm: 12.660149574279785
Step: 2070, train/learning_rate: 4.753688699565828e-05
Step: 2070, train/epoch: 0.4926225543022156
Step: 2080, train/loss: 0.2937999963760376
Step: 2080, train/grad_norm: 4.068136215209961
Step: 2080, train/learning_rate: 4.752498716698028e-05
Step: 2080, train/epoch: 0.4950023889541626
Step: 2090, train/loss: 0.22220000624656677
Step: 2090, train/grad_norm: 6.202012538909912
Step: 2090, train/learning_rate: 4.7513087338302284e-05
Step: 2090, train/epoch: 0.49738219380378723
Step: 2100, train/loss: 0.3991999924182892
Step: 2100, train/grad_norm: 8.81919002532959
Step: 2100, train/learning_rate: 4.7501191147603095e-05
Step: 2100, train/epoch: 0.49976202845573425
Step: 2110, train/loss: 0.5085999965667725
Step: 2110, train/grad_norm: 21.118051528930664
Step: 2110, train/learning_rate: 4.74892913189251e-05
Step: 2110, train/epoch: 0.5021418333053589
Step: 2120, train/loss: 0.5297999978065491
Step: 2120, train/grad_norm: 4.1388068199157715
Step: 2120, train/learning_rate: 4.74773914902471e-05
Step: 2120, train/epoch: 0.5045216679573059
Step: 2130, train/loss: 0.3009999990463257
Step: 2130, train/grad_norm: 4.181911945343018
Step: 2130, train/learning_rate: 4.7465491661569104e-05
Step: 2130, train/epoch: 0.5069015026092529
Step: 2140, train/loss: 0.22339999675750732
Step: 2140, train/grad_norm: 1.2795850038528442
Step: 2140, train/learning_rate: 4.7453591832891107e-05
Step: 2140, train/epoch: 0.5092812776565552
Step: 2150, train/loss: 0.23280000686645508
Step: 2150, train/grad_norm: 13.186579704284668
Step: 2150, train/learning_rate: 4.744169564219192e-05
Step: 2150, train/epoch: 0.5116611123085022
Step: 2160, train/loss: 0.36239999532699585
Step: 2160, train/grad_norm: 13.664153099060059
Step: 2160, train/learning_rate: 4.742979581351392e-05
Step: 2160, train/epoch: 0.5140409469604492
Step: 2170, train/loss: 0.3095000088214874
Step: 2170, train/grad_norm: 1.9783165454864502
Step: 2170, train/learning_rate: 4.741789598483592e-05
Step: 2170, train/epoch: 0.5164207816123962
Step: 2180, train/loss: 0.31060001254081726
Step: 2180, train/grad_norm: 1.7215629816055298
Step: 2180, train/learning_rate: 4.7405996156157926e-05
Step: 2180, train/epoch: 0.5188005566596985
Step: 2190, train/loss: 0.35190001130104065
Step: 2190, train/grad_norm: 0.9354364275932312
Step: 2190, train/learning_rate: 4.739409632747993e-05
Step: 2190, train/epoch: 0.5211803913116455
Step: 2200, train/loss: 0.2214999943971634
Step: 2200, train/grad_norm: 1.3480318784713745
Step: 2200, train/learning_rate: 4.738220013678074e-05
Step: 2200, train/epoch: 0.5235602259635925
Step: 2210, train/loss: 0.37059998512268066
Step: 2210, train/grad_norm: 1.822890043258667
Step: 2210, train/learning_rate: 4.737030030810274e-05
Step: 2210, train/epoch: 0.5259400010108948
Step: 2220, train/loss: 0.2924000024795532
Step: 2220, train/grad_norm: 1.3649094104766846
Step: 2220, train/learning_rate: 4.7358400479424745e-05
Step: 2220, train/epoch: 0.5283198356628418
Step: 2230, train/loss: 0.21119999885559082
Step: 2230, train/grad_norm: 1.4347681999206543
Step: 2230, train/learning_rate: 4.734650065074675e-05
Step: 2230, train/epoch: 0.5306996703147888
Step: 2240, train/loss: 0.4659999907016754
Step: 2240, train/grad_norm: 7.129088878631592
Step: 2240, train/learning_rate: 4.733460082206875e-05
Step: 2240, train/epoch: 0.5330795049667358
Step: 2250, train/loss: 0.30959999561309814
Step: 2250, train/grad_norm: 14.368399620056152
Step: 2250, train/learning_rate: 4.732270463136956e-05
Step: 2250, train/epoch: 0.5354592800140381
Step: 2260, train/loss: 0.304500013589859
Step: 2260, train/grad_norm: 2.286661386489868
Step: 2260, train/learning_rate: 4.7310804802691564e-05
Step: 2260, train/epoch: 0.5378391146659851
Step: 2270, train/loss: 0.1761000007390976
Step: 2270, train/grad_norm: 1.1553317308425903
Step: 2270, train/learning_rate: 4.729890497401357e-05
Step: 2270, train/epoch: 0.5402189493179321
Step: 2280, train/loss: 0.32910001277923584
Step: 2280, train/grad_norm: 8.375314712524414
Step: 2280, train/learning_rate: 4.728700514533557e-05
Step: 2280, train/epoch: 0.5425987839698792
Step: 2290, train/loss: 0.25200000405311584
Step: 2290, train/grad_norm: 9.421124458312988
Step: 2290, train/learning_rate: 4.727510531665757e-05
Step: 2290, train/epoch: 0.5449785590171814
Step: 2300, train/loss: 0.30970001220703125
Step: 2300, train/grad_norm: 6.805668354034424
Step: 2300, train/learning_rate: 4.726320912595838e-05
Step: 2300, train/epoch: 0.5473583936691284
Step: 2310, train/loss: 0.18709999322891235
Step: 2310, train/grad_norm: 9.802263259887695
Step: 2310, train/learning_rate: 4.7251309297280386e-05
Step: 2310, train/epoch: 0.5497382283210754
Step: 2320, train/loss: 0.31299999356269836
Step: 2320, train/grad_norm: 10.346068382263184
Step: 2320, train/learning_rate: 4.723940946860239e-05
Step: 2320, train/epoch: 0.5521180629730225
Step: 2330, train/loss: 0.3783999979496002
Step: 2330, train/grad_norm: 10.034276962280273
Step: 2330, train/learning_rate: 4.722750963992439e-05
Step: 2330, train/epoch: 0.5544978380203247
Step: 2340, train/loss: 0.45339998602867126
Step: 2340, train/grad_norm: 24.69694709777832
Step: 2340, train/learning_rate: 4.7215609811246395e-05
Step: 2340, train/epoch: 0.5568776726722717
Step: 2350, train/loss: 0.29789999127388
Step: 2350, train/grad_norm: 7.595271110534668
Step: 2350, train/learning_rate: 4.7203713620547205e-05
Step: 2350, train/epoch: 0.5592575073242188
Step: 2360, train/loss: 0.19900000095367432
Step: 2360, train/grad_norm: 6.508819580078125
Step: 2360, train/learning_rate: 4.719181379186921e-05
Step: 2360, train/epoch: 0.5616373419761658
Step: 2370, train/loss: 0.25760000944137573
Step: 2370, train/grad_norm: 9.051889419555664
Step: 2370, train/learning_rate: 4.717991396319121e-05
Step: 2370, train/epoch: 0.564017117023468
Step: 2380, train/loss: 0.3582000136375427
Step: 2380, train/grad_norm: 10.479549407958984
Step: 2380, train/learning_rate: 4.7168014134513214e-05
Step: 2380, train/epoch: 0.566396951675415
Step: 2390, train/loss: 0.39649999141693115
Step: 2390, train/grad_norm: 8.972155570983887
Step: 2390, train/learning_rate: 4.7156117943814024e-05
Step: 2390, train/epoch: 0.5687767863273621
Step: 2400, train/loss: 0.2587999999523163
Step: 2400, train/grad_norm: 6.386730194091797
Step: 2400, train/learning_rate: 4.714421811513603e-05
Step: 2400, train/epoch: 0.5711566209793091
Step: 2410, train/loss: 0.2784999907016754
Step: 2410, train/grad_norm: 10.995529174804688
Step: 2410, train/learning_rate: 4.713231828645803e-05
Step: 2410, train/epoch: 0.5735363960266113
Step: 2420, train/loss: 0.257099986076355
Step: 2420, train/grad_norm: 6.427201271057129
Step: 2420, train/learning_rate: 4.712041845778003e-05
Step: 2420, train/epoch: 0.5759162306785583
Step: 2430, train/loss: 0.19789999723434448
Step: 2430, train/grad_norm: 2.486557960510254
Step: 2430, train/learning_rate: 4.7108518629102036e-05
Step: 2430, train/epoch: 0.5782960653305054
Step: 2440, train/loss: 0.19040000438690186
Step: 2440, train/grad_norm: 6.1480488777160645
Step: 2440, train/learning_rate: 4.7096622438402846e-05
Step: 2440, train/epoch: 0.5806758403778076
Step: 2450, train/loss: 0.3700000047683716
Step: 2450, train/grad_norm: 21.581531524658203
Step: 2450, train/learning_rate: 4.708472260972485e-05
Step: 2450, train/epoch: 0.5830556750297546
Step: 2460, train/loss: 0.09139999747276306
Step: 2460, train/grad_norm: 0.22442570328712463
Step: 2460, train/learning_rate: 4.707282278104685e-05
Step: 2460, train/epoch: 0.5854355096817017
Step: 2470, train/loss: 0.3149999976158142
Step: 2470, train/grad_norm: 20.591957092285156
Step: 2470, train/learning_rate: 4.7060922952368855e-05
Step: 2470, train/epoch: 0.5878153443336487
Step: 2480, train/loss: 0.40149998664855957
Step: 2480, train/grad_norm: 6.732030391693115
Step: 2480, train/learning_rate: 4.704902312369086e-05
Step: 2480, train/epoch: 0.5901951193809509
Step: 2490, train/loss: 0.5292999744415283
Step: 2490, train/grad_norm: 7.2650146484375
Step: 2490, train/learning_rate: 4.703712693299167e-05
Step: 2490, train/epoch: 0.592574954032898
Step: 2500, train/loss: 0.33000001311302185
Step: 2500, train/grad_norm: 10.311654090881348
Step: 2500, train/learning_rate: 4.702522710431367e-05
Step: 2500, train/epoch: 0.594954788684845
Step: 2510, train/loss: 0.22190000116825104
Step: 2510, train/grad_norm: 8.988954544067383
Step: 2510, train/learning_rate: 4.7013327275635675e-05
Step: 2510, train/epoch: 0.597334623336792
Step: 2520, train/loss: 0.22050000727176666
Step: 2520, train/grad_norm: 0.6338173747062683
Step: 2520, train/learning_rate: 4.700142744695768e-05
Step: 2520, train/epoch: 0.5997143983840942
Step: 2530, train/loss: 0.19920000433921814
Step: 2530, train/grad_norm: 3.5029191970825195
Step: 2530, train/learning_rate: 4.698952761827968e-05
Step: 2530, train/epoch: 0.6020942330360413
Step: 2540, train/loss: 0.1868000030517578
Step: 2540, train/grad_norm: 2.9506795406341553
Step: 2540, train/learning_rate: 4.697763142758049e-05
Step: 2540, train/epoch: 0.6044740676879883
Step: 2550, train/loss: 0.39100000262260437
Step: 2550, train/grad_norm: 21.058996200561523
Step: 2550, train/learning_rate: 4.6965731598902494e-05
Step: 2550, train/epoch: 0.6068539023399353
Step: 2560, train/loss: 0.30230000615119934
Step: 2560, train/grad_norm: 11.92634105682373
Step: 2560, train/learning_rate: 4.69538317702245e-05
Step: 2560, train/epoch: 0.6092336773872375
Step: 2570, train/loss: 0.35690000653266907
Step: 2570, train/grad_norm: 14.726683616638184
Step: 2570, train/learning_rate: 4.69419319415465e-05
Step: 2570, train/epoch: 0.6116135120391846
Step: 2580, train/loss: 0.3693999946117401
Step: 2580, train/grad_norm: 5.525390625
Step: 2580, train/learning_rate: 4.69300321128685e-05
Step: 2580, train/epoch: 0.6139933466911316
Step: 2590, train/loss: 0.20479999482631683
Step: 2590, train/grad_norm: 6.598810195922852
Step: 2590, train/learning_rate: 4.691813592216931e-05
Step: 2590, train/epoch: 0.6163731813430786
Step: 2600, train/loss: 0.2409999966621399
Step: 2600, train/grad_norm: 0.24643467366695404
Step: 2600, train/learning_rate: 4.6906236093491316e-05
Step: 2600, train/epoch: 0.6187529563903809
Step: 2610, train/loss: 0.34610000252723694
Step: 2610, train/grad_norm: 6.699496746063232
Step: 2610, train/learning_rate: 4.689433626481332e-05
Step: 2610, train/epoch: 0.6211327910423279
Step: 2620, train/loss: 0.24070000648498535
Step: 2620, train/grad_norm: 6.126290321350098
Step: 2620, train/learning_rate: 4.688243643613532e-05
Step: 2620, train/epoch: 0.6235126256942749
Step: 2630, train/loss: 0.23759999871253967
Step: 2630, train/grad_norm: 13.379744529724121
Step: 2630, train/learning_rate: 4.6870536607457325e-05
Step: 2630, train/epoch: 0.6258924603462219
Step: 2640, train/loss: 0.2621000111103058
Step: 2640, train/grad_norm: 0.3603530824184418
Step: 2640, train/learning_rate: 4.6858640416758135e-05
Step: 2640, train/epoch: 0.6282722353935242
Step: 2650, train/loss: 0.2757999897003174
Step: 2650, train/grad_norm: 16.642057418823242
Step: 2650, train/learning_rate: 4.684674058808014e-05
Step: 2650, train/epoch: 0.6306520700454712
Step: 2660, train/loss: 0.4047999978065491
Step: 2660, train/grad_norm: 5.900774002075195
Step: 2660, train/learning_rate: 4.683484075940214e-05
Step: 2660, train/epoch: 0.6330319046974182
Step: 2670, train/loss: 0.18219999969005585
Step: 2670, train/grad_norm: 7.868365287780762
Step: 2670, train/learning_rate: 4.6822940930724144e-05
Step: 2670, train/epoch: 0.6354116797447205
Step: 2680, train/loss: 0.3278000056743622
Step: 2680, train/grad_norm: 1.020721197128296
Step: 2680, train/learning_rate: 4.681104110204615e-05
Step: 2680, train/epoch: 0.6377915143966675
Step: 2690, train/loss: 0.29440000653266907
Step: 2690, train/grad_norm: 1.6303282976150513
Step: 2690, train/learning_rate: 4.679914491134696e-05
Step: 2690, train/epoch: 0.6401713490486145
Step: 2700, train/loss: 0.2257000058889389
Step: 2700, train/grad_norm: 11.871072769165039
Step: 2700, train/learning_rate: 4.678724508266896e-05
Step: 2700, train/epoch: 0.6425511837005615
Step: 2710, train/loss: 0.33250001072883606
Step: 2710, train/grad_norm: 5.386849880218506
Step: 2710, train/learning_rate: 4.677534525399096e-05
Step: 2710, train/epoch: 0.6449309587478638
Step: 2720, train/loss: 0.3571999967098236
Step: 2720, train/grad_norm: 2.2511179447174072
Step: 2720, train/learning_rate: 4.6763445425312966e-05
Step: 2720, train/epoch: 0.6473107933998108
Step: 2730, train/loss: 0.29269999265670776
Step: 2730, train/grad_norm: 3.8310658931732178
Step: 2730, train/learning_rate: 4.675154559663497e-05
Step: 2730, train/epoch: 0.6496906280517578
Step: 2740, train/loss: 0.262800008058548
Step: 2740, train/grad_norm: 7.621912956237793
Step: 2740, train/learning_rate: 4.673964940593578e-05
Step: 2740, train/epoch: 0.6520704627037048
Step: 2750, train/loss: 0.26170000433921814
Step: 2750, train/grad_norm: 4.704961776733398
Step: 2750, train/learning_rate: 4.672774957725778e-05
Step: 2750, train/epoch: 0.6544502377510071
Step: 2760, train/loss: 0.34599998593330383
Step: 2760, train/grad_norm: 0.3586677014827728
Step: 2760, train/learning_rate: 4.6715849748579785e-05
Step: 2760, train/epoch: 0.6568300724029541
Step: 2770, train/loss: 0.289000004529953
Step: 2770, train/grad_norm: 2.4456775188446045
Step: 2770, train/learning_rate: 4.670394991990179e-05
Step: 2770, train/epoch: 0.6592099070549011
Step: 2780, train/loss: 0.3000999987125397
Step: 2780, train/grad_norm: 0.6605042219161987
Step: 2780, train/learning_rate: 4.669205009122379e-05
Step: 2780, train/epoch: 0.6615897417068481
Step: 2790, train/loss: 0.3571000099182129
Step: 2790, train/grad_norm: 22.827232360839844
Step: 2790, train/learning_rate: 4.66801539005246e-05
Step: 2790, train/epoch: 0.6639695167541504
Step: 2800, train/loss: 0.5324000120162964
Step: 2800, train/grad_norm: 17.229089736938477
Step: 2800, train/learning_rate: 4.6668254071846604e-05
Step: 2800, train/epoch: 0.6663493514060974
Step: 2810, train/loss: 0.2854999899864197
Step: 2810, train/grad_norm: 13.306921005249023
Step: 2810, train/learning_rate: 4.665635424316861e-05
Step: 2810, train/epoch: 0.6687291860580444
Step: 2820, train/loss: 0.3391000032424927
Step: 2820, train/grad_norm: 17.53904914855957
Step: 2820, train/learning_rate: 4.664445441449061e-05
Step: 2820, train/epoch: 0.6711090207099915
Step: 2830, train/loss: 0.32440000772476196
Step: 2830, train/grad_norm: 11.344667434692383
Step: 2830, train/learning_rate: 4.663255458581261e-05
Step: 2830, train/epoch: 0.6734887957572937
Step: 2840, train/loss: 0.2070000022649765
Step: 2840, train/grad_norm: 2.761636734008789
Step: 2840, train/learning_rate: 4.6620658395113423e-05
Step: 2840, train/epoch: 0.6758686304092407
Step: 2850, train/loss: 0.30300000309944153
Step: 2850, train/grad_norm: 5.678342819213867
Step: 2850, train/learning_rate: 4.6608758566435426e-05
Step: 2850, train/epoch: 0.6782484650611877
Step: 2860, train/loss: 0.28850001096725464
Step: 2860, train/grad_norm: 5.719038963317871
Step: 2860, train/learning_rate: 4.659685873775743e-05
Step: 2860, train/epoch: 0.6806282997131348
Step: 2870, train/loss: 0.3610999882221222
Step: 2870, train/grad_norm: 1.2517099380493164
Step: 2870, train/learning_rate: 4.658495890907943e-05
Step: 2870, train/epoch: 0.683008074760437
Step: 2880, train/loss: 0.26089999079704285
Step: 2880, train/grad_norm: 6.830155372619629
Step: 2880, train/learning_rate: 4.6573059080401435e-05
Step: 2880, train/epoch: 0.685387909412384
Step: 2890, train/loss: 0.3474999964237213
Step: 2890, train/grad_norm: 1.069972038269043
Step: 2890, train/learning_rate: 4.6561162889702246e-05
Step: 2890, train/epoch: 0.687767744064331
Step: 2900, train/loss: 0.2791000008583069
Step: 2900, train/grad_norm: 4.921020984649658
Step: 2900, train/learning_rate: 4.654926306102425e-05
Step: 2900, train/epoch: 0.6901475191116333
Step: 2910, train/loss: 0.2824000120162964
Step: 2910, train/grad_norm: 15.946602821350098
Step: 2910, train/learning_rate: 4.653736323234625e-05
Step: 2910, train/epoch: 0.6925273537635803
Step: 2920, train/loss: 0.2865000069141388
Step: 2920, train/grad_norm: 0.24230895936489105
Step: 2920, train/learning_rate: 4.6525463403668255e-05
Step: 2920, train/epoch: 0.6949071884155273
Step: 2930, train/loss: 0.19040000438690186
Step: 2930, train/grad_norm: 5.410773277282715
Step: 2930, train/learning_rate: 4.651356357499026e-05
Step: 2930, train/epoch: 0.6972870230674744
Step: 2940, train/loss: 0.2858000099658966
Step: 2940, train/grad_norm: 8.195169448852539
Step: 2940, train/learning_rate: 4.650166738429107e-05
Step: 2940, train/epoch: 0.6996667981147766
Step: 2950, train/loss: 0.3310000002384186
Step: 2950, train/grad_norm: 20.465599060058594
Step: 2950, train/learning_rate: 4.648976755561307e-05
Step: 2950, train/epoch: 0.7020466327667236
Step: 2960, train/loss: 0.4648999869823456
Step: 2960, train/grad_norm: 6.165002346038818
Step: 2960, train/learning_rate: 4.6477867726935074e-05
Step: 2960, train/epoch: 0.7044264674186707
Step: 2970, train/loss: 0.36320000886917114
Step: 2970, train/grad_norm: 0.364867240190506
Step: 2970, train/learning_rate: 4.646596789825708e-05
Step: 2970, train/epoch: 0.7068063020706177
Step: 2980, train/loss: 0.33959999680519104
Step: 2980, train/grad_norm: 6.376012325286865
Step: 2980, train/learning_rate: 4.645406806957908e-05
Step: 2980, train/epoch: 0.7091860771179199
Step: 2990, train/loss: 0.24570000171661377
Step: 2990, train/grad_norm: 0.4485134482383728
Step: 2990, train/learning_rate: 4.644217187887989e-05
Step: 2990, train/epoch: 0.7115659117698669
Step: 3000, train/loss: 0.2718000113964081
Step: 3000, train/grad_norm: 8.566644668579102
Step: 3000, train/learning_rate: 4.643027205020189e-05
Step: 3000, train/epoch: 0.713945746421814
Step: 3010, train/loss: 0.2249000072479248
Step: 3010, train/grad_norm: 15.51443862915039
Step: 3010, train/learning_rate: 4.6418372221523896e-05
Step: 3010, train/epoch: 0.716325581073761
Step: 3020, train/loss: 0.3513999879360199
Step: 3020, train/grad_norm: 1.7739784717559814
Step: 3020, train/learning_rate: 4.64064723928459e-05
Step: 3020, train/epoch: 0.7187053561210632
Step: 3030, train/loss: 0.18709999322891235
Step: 3030, train/grad_norm: 10.341272354125977
Step: 3030, train/learning_rate: 4.63945725641679e-05
Step: 3030, train/epoch: 0.7210851907730103
Step: 3040, train/loss: 0.31119999289512634
Step: 3040, train/grad_norm: 3.6537580490112305
Step: 3040, train/learning_rate: 4.638267637346871e-05
Step: 3040, train/epoch: 0.7234650254249573
Step: 3050, train/loss: 0.20110000669956207
Step: 3050, train/grad_norm: 2.495094060897827
Step: 3050, train/learning_rate: 4.6370776544790715e-05
Step: 3050, train/epoch: 0.7258448600769043
Step: 3060, train/loss: 0.31310001015663147
Step: 3060, train/grad_norm: 7.725636005401611
Step: 3060, train/learning_rate: 4.635887671611272e-05
Step: 3060, train/epoch: 0.7282246351242065
Step: 3070, train/loss: 0.29190000891685486
Step: 3070, train/grad_norm: 7.961844444274902
Step: 3070, train/learning_rate: 4.634697688743472e-05
Step: 3070, train/epoch: 0.7306044697761536
Step: 3080, train/loss: 0.27239999175071716
Step: 3080, train/grad_norm: 1.2134373188018799
Step: 3080, train/learning_rate: 4.6335077058756724e-05
Step: 3080, train/epoch: 0.7329843044281006
Step: 3090, train/loss: 0.34049999713897705
Step: 3090, train/grad_norm: 9.147000312805176
Step: 3090, train/learning_rate: 4.6323180868057534e-05
Step: 3090, train/epoch: 0.7353641390800476
Step: 3100, train/loss: 0.23800000548362732
Step: 3100, train/grad_norm: 14.497760772705078
Step: 3100, train/learning_rate: 4.631128103937954e-05
Step: 3100, train/epoch: 0.7377439141273499
Step: 3110, train/loss: 0.3359000086784363
Step: 3110, train/grad_norm: 11.273880958557129
Step: 3110, train/learning_rate: 4.629938121070154e-05
Step: 3110, train/epoch: 0.7401237487792969
Step: 3120, train/loss: 0.29319998621940613
Step: 3120, train/grad_norm: 18.87594223022461
Step: 3120, train/learning_rate: 4.628748138202354e-05
Step: 3120, train/epoch: 0.7425035834312439
Step: 3130, train/loss: 0.2702000141143799
Step: 3130, train/grad_norm: 0.32396212220191956
Step: 3130, train/learning_rate: 4.6275581553345546e-05
Step: 3130, train/epoch: 0.7448834180831909
Step: 3140, train/loss: 0.3531000018119812
Step: 3140, train/grad_norm: 23.145795822143555
Step: 3140, train/learning_rate: 4.6263685362646356e-05
Step: 3140, train/epoch: 0.7472631931304932
Step: 3150, train/loss: 0.2484000027179718
Step: 3150, train/grad_norm: 17.660316467285156
Step: 3150, train/learning_rate: 4.625178553396836e-05
Step: 3150, train/epoch: 0.7496430277824402
Step: 3160, train/loss: 0.35569998621940613
Step: 3160, train/grad_norm: 4.00849723815918
Step: 3160, train/learning_rate: 4.623988570529036e-05
Step: 3160, train/epoch: 0.7520228624343872
Step: 3170, train/loss: 0.30640000104904175
Step: 3170, train/grad_norm: 7.015502452850342
Step: 3170, train/learning_rate: 4.6227985876612365e-05
Step: 3170, train/epoch: 0.7544026374816895
Step: 3180, train/loss: 0.3127000033855438
Step: 3180, train/grad_norm: 16.329879760742188
Step: 3180, train/learning_rate: 4.621608604793437e-05
Step: 3180, train/epoch: 0.7567824721336365
Step: 3190, train/loss: 0.2533000111579895
Step: 3190, train/grad_norm: 12.673648834228516
Step: 3190, train/learning_rate: 4.620418985723518e-05
Step: 3190, train/epoch: 0.7591623067855835
Step: 3200, train/loss: 0.32519999146461487
Step: 3200, train/grad_norm: 8.559232711791992
Step: 3200, train/learning_rate: 4.619229002855718e-05
Step: 3200, train/epoch: 0.7615421414375305
Step: 3210, train/loss: 0.24199999868869781
Step: 3210, train/grad_norm: 8.339897155761719
Step: 3210, train/learning_rate: 4.6180390199879184e-05
Step: 3210, train/epoch: 0.7639219164848328
Step: 3220, train/loss: 0.20819999277591705
Step: 3220, train/grad_norm: 12.18371868133545
Step: 3220, train/learning_rate: 4.616849037120119e-05
Step: 3220, train/epoch: 0.7663017511367798
Step: 3230, train/loss: 0.2874000072479248
Step: 3230, train/grad_norm: 0.6096246838569641
Step: 3230, train/learning_rate: 4.615659054252319e-05
Step: 3230, train/epoch: 0.7686815857887268
Step: 3240, train/loss: 0.2870999872684479
Step: 3240, train/grad_norm: 4.866147041320801
Step: 3240, train/learning_rate: 4.6144694351824e-05
Step: 3240, train/epoch: 0.7710614204406738
Step: 3250, train/loss: 0.29330000281333923
Step: 3250, train/grad_norm: 8.992002487182617
Step: 3250, train/learning_rate: 4.6132794523146003e-05
Step: 3250, train/epoch: 0.7734411954879761
Step: 3260, train/loss: 0.19130000472068787
Step: 3260, train/grad_norm: 1.660369634628296
Step: 3260, train/learning_rate: 4.6120894694468006e-05
Step: 3260, train/epoch: 0.7758210301399231
Step: 3270, train/loss: 0.23880000412464142
Step: 3270, train/grad_norm: 9.676962852478027
Step: 3270, train/learning_rate: 4.610899486579001e-05
Step: 3270, train/epoch: 0.7782008647918701
Step: 3280, train/loss: 0.2150000035762787
Step: 3280, train/grad_norm: 11.76818561553955
Step: 3280, train/learning_rate: 4.609709503711201e-05
Step: 3280, train/epoch: 0.7805806994438171
Step: 3290, train/loss: 0.11729999631643295
Step: 3290, train/grad_norm: 0.5349085330963135
Step: 3290, train/learning_rate: 4.608519884641282e-05
Step: 3290, train/epoch: 0.7829604744911194
Step: 3300, train/loss: 0.40369999408721924
Step: 3300, train/grad_norm: 2.1284291744232178
Step: 3300, train/learning_rate: 4.6073299017734826e-05
Step: 3300, train/epoch: 0.7853403091430664
Step: 3310, train/loss: 0.19499999284744263
Step: 3310, train/grad_norm: 0.37868911027908325
Step: 3310, train/learning_rate: 4.606139918905683e-05
Step: 3310, train/epoch: 0.7877201437950134
Step: 3320, train/loss: 0.22609999775886536
Step: 3320, train/grad_norm: 1.3940324783325195
Step: 3320, train/learning_rate: 4.604949936037883e-05
Step: 3320, train/epoch: 0.7900999784469604
Step: 3330, train/loss: 0.2329999953508377
Step: 3330, train/grad_norm: 9.463035583496094
Step: 3330, train/learning_rate: 4.6037599531700835e-05
Step: 3330, train/epoch: 0.7924797534942627
Step: 3340, train/loss: 0.3287999927997589
Step: 3340, train/grad_norm: 10.840632438659668
Step: 3340, train/learning_rate: 4.6025703341001645e-05
Step: 3340, train/epoch: 0.7948595881462097
Step: 3350, train/loss: 0.2750999927520752
Step: 3350, train/grad_norm: 17.451828002929688
Step: 3350, train/learning_rate: 4.601380351232365e-05
Step: 3350, train/epoch: 0.7972394227981567
Step: 3360, train/loss: 0.21250000596046448
Step: 3360, train/grad_norm: 0.15679189562797546
Step: 3360, train/learning_rate: 4.600190368364565e-05
Step: 3360, train/epoch: 0.7996192574501038
Step: 3370, train/loss: 0.4390999972820282
Step: 3370, train/grad_norm: 18.595870971679688
Step: 3370, train/learning_rate: 4.5990003854967654e-05
Step: 3370, train/epoch: 0.801999032497406
Step: 3380, train/loss: 0.1868000030517578
Step: 3380, train/grad_norm: 13.781937599182129
Step: 3380, train/learning_rate: 4.597810402628966e-05
Step: 3380, train/epoch: 0.804378867149353
Step: 3390, train/loss: 0.426800012588501
Step: 3390, train/grad_norm: 10.90756607055664
Step: 3390, train/learning_rate: 4.596620783559047e-05
Step: 3390, train/epoch: 0.8067587018013
Step: 3400, train/loss: 0.6980000138282776
Step: 3400, train/grad_norm: 16.131357192993164
Step: 3400, train/learning_rate: 4.595430800691247e-05
Step: 3400, train/epoch: 0.8091384768486023
Step: 3410, train/loss: 0.2897000014781952
Step: 3410, train/grad_norm: 3.12691593170166
Step: 3410, train/learning_rate: 4.594240817823447e-05
Step: 3410, train/epoch: 0.8115183115005493
Step: 3420, train/loss: 0.2696000039577484
Step: 3420, train/grad_norm: 6.559033393859863
Step: 3420, train/learning_rate: 4.5930508349556476e-05
Step: 3420, train/epoch: 0.8138981461524963
Step: 3430, train/loss: 0.2304999977350235
Step: 3430, train/grad_norm: 11.617267608642578
Step: 3430, train/learning_rate: 4.591860852087848e-05
Step: 3430, train/epoch: 0.8162779808044434
Step: 3440, train/loss: 0.35440000891685486
Step: 3440, train/grad_norm: 11.052424430847168
Step: 3440, train/learning_rate: 4.590671233017929e-05
Step: 3440, train/epoch: 0.8186577558517456
Step: 3450, train/loss: 0.21330000460147858
Step: 3450, train/grad_norm: 1.4707196950912476
Step: 3450, train/learning_rate: 4.589481250150129e-05
Step: 3450, train/epoch: 0.8210375905036926
Step: 3460, train/loss: 0.21150000393390656
Step: 3460, train/grad_norm: 7.4198479652404785
Step: 3460, train/learning_rate: 4.5882912672823295e-05
Step: 3460, train/epoch: 0.8234174251556396
Step: 3470, train/loss: 0.31839999556541443
Step: 3470, train/grad_norm: 20.673297882080078
Step: 3470, train/learning_rate: 4.58710128441453e-05
Step: 3470, train/epoch: 0.8257972598075867
Step: 3480, train/loss: 0.1898999959230423
Step: 3480, train/grad_norm: 6.554008960723877
Step: 3480, train/learning_rate: 4.58591130154673e-05
Step: 3480, train/epoch: 0.8281770348548889
Step: 3490, train/loss: 0.3285999894142151
Step: 3490, train/grad_norm: 10.19253921508789
Step: 3490, train/learning_rate: 4.584721682476811e-05
Step: 3490, train/epoch: 0.8305568695068359
Step: 3500, train/loss: 0.18930000066757202
Step: 3500, train/grad_norm: 0.4563501179218292
Step: 3500, train/learning_rate: 4.5835316996090114e-05
Step: 3500, train/epoch: 0.832936704158783
Step: 3510, train/loss: 0.1745000034570694
Step: 3510, train/grad_norm: 1.29030442237854
Step: 3510, train/learning_rate: 4.582341716741212e-05
Step: 3510, train/epoch: 0.83531653881073
Step: 3520, train/loss: 0.3125
Step: 3520, train/grad_norm: 19.421070098876953
Step: 3520, train/learning_rate: 4.581151733873412e-05
Step: 3520, train/epoch: 0.8376963138580322
Step: 3530, train/loss: 0.2831000089645386
Step: 3530, train/grad_norm: 10.29845905303955
Step: 3530, train/learning_rate: 4.579961751005612e-05
Step: 3530, train/epoch: 0.8400761485099792
Step: 3540, train/loss: 0.2020999938249588
Step: 3540, train/grad_norm: 12.956583023071289
Step: 3540, train/learning_rate: 4.578772131935693e-05
Step: 3540, train/epoch: 0.8424559831619263
Step: 3550, train/loss: 0.17839999496936798
Step: 3550, train/grad_norm: 19.06424903869629
Step: 3550, train/learning_rate: 4.5775821490678936e-05
Step: 3550, train/epoch: 0.8448358178138733
Step: 3560, train/loss: 0.5497999787330627
Step: 3560, train/grad_norm: 10.794845581054688
Step: 3560, train/learning_rate: 4.576392166200094e-05
Step: 3560, train/epoch: 0.8472155928611755
Step: 3570, train/loss: 0.12880000472068787
Step: 3570, train/grad_norm: 4.346005439758301
Step: 3570, train/learning_rate: 4.575202183332294e-05
Step: 3570, train/epoch: 0.8495954275131226
Step: 3580, train/loss: 0.1606999933719635
Step: 3580, train/grad_norm: 9.274115562438965
Step: 3580, train/learning_rate: 4.5740122004644945e-05
Step: 3580, train/epoch: 0.8519752621650696
Step: 3590, train/loss: 0.09220000356435776
Step: 3590, train/grad_norm: 0.8723357915878296
Step: 3590, train/learning_rate: 4.5728225813945755e-05
Step: 3590, train/epoch: 0.8543550968170166
Step: 3600, train/loss: 0.2879999876022339
Step: 3600, train/grad_norm: 2.103990316390991
Step: 3600, train/learning_rate: 4.571632598526776e-05
Step: 3600, train/epoch: 0.8567348718643188
Step: 3610, train/loss: 0.35359999537467957
Step: 3610, train/grad_norm: 2.9839024543762207
Step: 3610, train/learning_rate: 4.570442615658976e-05
Step: 3610, train/epoch: 0.8591147065162659
Step: 3620, train/loss: 0.4196000099182129
Step: 3620, train/grad_norm: 11.805657386779785
Step: 3620, train/learning_rate: 4.5692526327911764e-05
Step: 3620, train/epoch: 0.8614945411682129
Step: 3630, train/loss: 0.40400001406669617
Step: 3630, train/grad_norm: 8.996373176574707
Step: 3630, train/learning_rate: 4.568062649923377e-05
Step: 3630, train/epoch: 0.8638743162155151
Step: 3640, train/loss: 0.09950000047683716
Step: 3640, train/grad_norm: 18.86302947998047
Step: 3640, train/learning_rate: 4.566873030853458e-05
Step: 3640, train/epoch: 0.8662541508674622
Step: 3650, train/loss: 0.2727000117301941
Step: 3650, train/grad_norm: 8.900477409362793
Step: 3650, train/learning_rate: 4.565683047985658e-05
Step: 3650, train/epoch: 0.8686339855194092
Step: 3660, train/loss: 0.20309999585151672
Step: 3660, train/grad_norm: 8.406110763549805
Step: 3660, train/learning_rate: 4.5644930651178584e-05
Step: 3660, train/epoch: 0.8710138201713562
Step: 3670, train/loss: 0.5697000026702881
Step: 3670, train/grad_norm: 2.3660199642181396
Step: 3670, train/learning_rate: 4.5633030822500587e-05
Step: 3670, train/epoch: 0.8733935952186584
Step: 3680, train/loss: 0.41499999165534973
Step: 3680, train/grad_norm: 11.039105415344238
Step: 3680, train/learning_rate: 4.562113099382259e-05
Step: 3680, train/epoch: 0.8757734298706055
Step: 3690, train/loss: 0.14309999346733093
Step: 3690, train/grad_norm: 3.531554698944092
Step: 3690, train/learning_rate: 4.56092348031234e-05
Step: 3690, train/epoch: 0.8781532645225525
Step: 3700, train/loss: 0.24120000004768372
Step: 3700, train/grad_norm: 18.024404525756836
Step: 3700, train/learning_rate: 4.55973349744454e-05
Step: 3700, train/epoch: 0.8805330991744995
Step: 3710, train/loss: 0.3319999873638153
Step: 3710, train/grad_norm: 1.0867369174957275
Step: 3710, train/learning_rate: 4.5585435145767406e-05
Step: 3710, train/epoch: 0.8829128742218018
Step: 3720, train/loss: 0.19419999420642853
Step: 3720, train/grad_norm: 9.788019180297852
Step: 3720, train/learning_rate: 4.557353531708941e-05
Step: 3720, train/epoch: 0.8852927088737488
Step: 3730, train/loss: 0.5271999835968018
Step: 3730, train/grad_norm: 13.578498840332031
Step: 3730, train/learning_rate: 4.556163912639022e-05
Step: 3730, train/epoch: 0.8876725435256958
Step: 3740, train/loss: 0.5015000104904175
Step: 3740, train/grad_norm: 8.41240119934082
Step: 3740, train/learning_rate: 4.554973929771222e-05
Step: 3740, train/epoch: 0.8900523781776428
Step: 3750, train/loss: 0.4214000105857849
Step: 3750, train/grad_norm: 19.8964786529541
Step: 3750, train/learning_rate: 4.5537839469034225e-05
Step: 3750, train/epoch: 0.8924321532249451
Step: 3760, train/loss: 0.22779999673366547
Step: 3760, train/grad_norm: 1.3223832845687866
Step: 3760, train/learning_rate: 4.552593964035623e-05
Step: 3760, train/epoch: 0.8948119878768921
Step: 3770, train/loss: 0.4180999994277954
Step: 3770, train/grad_norm: 2.563840389251709
Step: 3770, train/learning_rate: 4.551403981167823e-05
Step: 3770, train/epoch: 0.8971918225288391
Step: 3780, train/loss: 0.2011999934911728
Step: 3780, train/grad_norm: 6.899628639221191
Step: 3780, train/learning_rate: 4.550214362097904e-05
Step: 3780, train/epoch: 0.8995716571807861
Step: 3790, train/loss: 0.27390000224113464
Step: 3790, train/grad_norm: 0.4137355387210846
Step: 3790, train/learning_rate: 4.5490243792301044e-05
Step: 3790, train/epoch: 0.9019514322280884
Step: 3800, train/loss: 0.20520000159740448
Step: 3800, train/grad_norm: 5.524416446685791
Step: 3800, train/learning_rate: 4.547834396362305e-05
Step: 3800, train/epoch: 0.9043312668800354
Step: 3810, train/loss: 0.22280000150203705
Step: 3810, train/grad_norm: 11.186627388000488
Step: 3810, train/learning_rate: 4.546644413494505e-05
Step: 3810, train/epoch: 0.9067111015319824
Step: 3820, train/loss: 0.2565999925136566
Step: 3820, train/grad_norm: 12.286787033081055
Step: 3820, train/learning_rate: 4.545454430626705e-05
Step: 3820, train/epoch: 0.9090909361839294
Step: 3830, train/loss: 0.2556999921798706
Step: 3830, train/grad_norm: 9.623045921325684
Step: 3830, train/learning_rate: 4.544264811556786e-05
Step: 3830, train/epoch: 0.9114707112312317
Step: 3840, train/loss: 0.22059999406337738
Step: 3840, train/grad_norm: 23.93857765197754
Step: 3840, train/learning_rate: 4.5430748286889866e-05
Step: 3840, train/epoch: 0.9138505458831787
Step: 3850, train/loss: 0.18469999730587006
Step: 3850, train/grad_norm: 4.744579315185547
Step: 3850, train/learning_rate: 4.541884845821187e-05
Step: 3850, train/epoch: 0.9162303805351257
Step: 3860, train/loss: 0.4830999970436096
Step: 3860, train/grad_norm: 23.896963119506836
Step: 3860, train/learning_rate: 4.540694862953387e-05
Step: 3860, train/epoch: 0.9186102151870728
Step: 3870, train/loss: 0.3197999894618988
Step: 3870, train/grad_norm: 20.41391372680664
Step: 3870, train/learning_rate: 4.5395048800855875e-05
Step: 3870, train/epoch: 0.920989990234375
Step: 3880, train/loss: 0.22599999606609344
Step: 3880, train/grad_norm: 0.4670620560646057
Step: 3880, train/learning_rate: 4.5383152610156685e-05
Step: 3880, train/epoch: 0.923369824886322
Step: 3890, train/loss: 0.2994999885559082
Step: 3890, train/grad_norm: 1.2909773588180542
Step: 3890, train/learning_rate: 4.537125278147869e-05
Step: 3890, train/epoch: 0.925749659538269
Step: 3900, train/loss: 0.24230000376701355
Step: 3900, train/grad_norm: 5.902773380279541
Step: 3900, train/learning_rate: 4.535935295280069e-05
Step: 3900, train/epoch: 0.9281294345855713
Step: 3910, train/loss: 0.19419999420642853
Step: 3910, train/grad_norm: 0.9289236068725586
Step: 3910, train/learning_rate: 4.5347453124122694e-05
Step: 3910, train/epoch: 0.9305092692375183
Step: 3920, train/loss: 0.1518000066280365
Step: 3920, train/grad_norm: 3.6425440311431885
Step: 3920, train/learning_rate: 4.53355532954447e-05
Step: 3920, train/epoch: 0.9328891038894653
Step: 3930, train/loss: 0.1923000067472458
Step: 3930, train/grad_norm: 10.893600463867188
Step: 3930, train/learning_rate: 4.532365710474551e-05
Step: 3930, train/epoch: 0.9352689385414124
Step: 3940, train/loss: 0.23109999299049377
Step: 3940, train/grad_norm: 0.7025224566459656
Step: 3940, train/learning_rate: 4.531175727606751e-05
Step: 3940, train/epoch: 0.9376487135887146
Step: 3950, train/loss: 0.14030000567436218
Step: 3950, train/grad_norm: 17.71805763244629
Step: 3950, train/learning_rate: 4.529985744738951e-05
Step: 3950, train/epoch: 0.9400285482406616
Step: 3960, train/loss: 0.1289999932050705
Step: 3960, train/grad_norm: 5.82067346572876
Step: 3960, train/learning_rate: 4.5287957618711516e-05
Step: 3960, train/epoch: 0.9424083828926086
Step: 3970, train/loss: 0.2476000040769577
Step: 3970, train/grad_norm: 18.41299819946289
Step: 3970, train/learning_rate: 4.527605779003352e-05
Step: 3970, train/epoch: 0.9447882175445557
Step: 3980, train/loss: 0.36660000681877136
Step: 3980, train/grad_norm: 20.293344497680664
Step: 3980, train/learning_rate: 4.526416159933433e-05
Step: 3980, train/epoch: 0.9471679925918579
Step: 3990, train/loss: 0.2721000015735626
Step: 3990, train/grad_norm: 1.8296680450439453
Step: 3990, train/learning_rate: 4.525226177065633e-05
Step: 3990, train/epoch: 0.9495478272438049
Step: 4000, train/loss: 0.15870000422000885
Step: 4000, train/grad_norm: 0.3716791570186615
Step: 4000, train/learning_rate: 4.5240361941978335e-05
Step: 4000, train/epoch: 0.951927661895752
Step: 4010, train/loss: 0.1979999989271164
Step: 4010, train/grad_norm: 15.577493667602539
Step: 4010, train/learning_rate: 4.522846211330034e-05
Step: 4010, train/epoch: 0.954307496547699
Step: 4020, train/loss: 0.40880000591278076
Step: 4020, train/grad_norm: 11.917228698730469
Step: 4020, train/learning_rate: 4.521656228462234e-05
Step: 4020, train/epoch: 0.9566872715950012
Step: 4030, train/loss: 0.15150000154972076
Step: 4030, train/grad_norm: 16.96875
Step: 4030, train/learning_rate: 4.520466609392315e-05
Step: 4030, train/epoch: 0.9590671062469482
Step: 4040, train/loss: 0.20900000631809235
Step: 4040, train/grad_norm: 6.428564548492432
Step: 4040, train/learning_rate: 4.5192766265245155e-05
Step: 4040, train/epoch: 0.9614469408988953
Step: 4050, train/loss: 0.15199999511241913
Step: 4050, train/grad_norm: 6.847236633300781
Step: 4050, train/learning_rate: 4.518086643656716e-05
Step: 4050, train/epoch: 0.9638267755508423
Step: 4060, train/loss: 0.28029999136924744
Step: 4060, train/grad_norm: 1.8820827007293701
Step: 4060, train/learning_rate: 4.516896660788916e-05
Step: 4060, train/epoch: 0.9662065505981445
Step: 4070, train/loss: 0.2345999926328659
Step: 4070, train/grad_norm: 0.22852198779582977
Step: 4070, train/learning_rate: 4.5157066779211164e-05
Step: 4070, train/epoch: 0.9685863852500916
Step: 4080, train/loss: 0.20509999990463257
Step: 4080, train/grad_norm: 0.8159741163253784
Step: 4080, train/learning_rate: 4.5145170588511974e-05
Step: 4080, train/epoch: 0.9709662199020386
Step: 4090, train/loss: 0.3075000047683716
Step: 4090, train/grad_norm: 3.7024075984954834
Step: 4090, train/learning_rate: 4.513327075983398e-05
Step: 4090, train/epoch: 0.9733460545539856
Step: 4100, train/loss: 0.37220001220703125
Step: 4100, train/grad_norm: 0.1158725917339325
Step: 4100, train/learning_rate: 4.512137093115598e-05
Step: 4100, train/epoch: 0.9757258296012878
Step: 4110, train/loss: 0.30070000886917114
Step: 4110, train/grad_norm: 6.888951301574707
Step: 4110, train/learning_rate: 4.510947110247798e-05
Step: 4110, train/epoch: 0.9781056642532349
Step: 4120, train/loss: 0.23739999532699585
Step: 4120, train/grad_norm: 5.408717155456543
Step: 4120, train/learning_rate: 4.5097571273799986e-05
Step: 4120, train/epoch: 0.9804854989051819
Step: 4130, train/loss: 0.16680000722408295
Step: 4130, train/grad_norm: 6.016138553619385
Step: 4130, train/learning_rate: 4.5085675083100796e-05
Step: 4130, train/epoch: 0.9828652739524841
Step: 4140, train/loss: 0.31540000438690186
Step: 4140, train/grad_norm: 2.1195290088653564
Step: 4140, train/learning_rate: 4.50737752544228e-05
Step: 4140, train/epoch: 0.9852451086044312
Step: 4150, train/loss: 0.1501999944448471
Step: 4150, train/grad_norm: 0.5673801898956299
Step: 4150, train/learning_rate: 4.50618754257448e-05
Step: 4150, train/epoch: 0.9876249432563782
Step: 4160, train/loss: 0.13860000669956207
Step: 4160, train/grad_norm: 20.15843391418457
Step: 4160, train/learning_rate: 4.5049975597066805e-05
Step: 4160, train/epoch: 0.9900047779083252
Step: 4170, train/loss: 0.1670999974012375
Step: 4170, train/grad_norm: 0.052331939339637756
Step: 4170, train/learning_rate: 4.503807576838881e-05
Step: 4170, train/epoch: 0.9923845529556274
Step: 4180, train/loss: 0.38429999351501465
Step: 4180, train/grad_norm: 9.852644920349121
Step: 4180, train/learning_rate: 4.502617957768962e-05
Step: 4180, train/epoch: 0.9947643876075745
Step: 4190, train/loss: 0.43140000104904175
Step: 4190, train/grad_norm: 10.236520767211914
Step: 4190, train/learning_rate: 4.501427974901162e-05
Step: 4190, train/epoch: 0.9971442222595215
Step: 4200, train/loss: 0.15889999270439148
Step: 4200, train/grad_norm: 4.398046970367432
Step: 4200, train/learning_rate: 4.5002379920333624e-05
Step: 4200, train/epoch: 0.9995240569114685
Step: 4202, eval/loss: 0.2248470038175583
Step: 4202, eval/accuracy: 0.9196168184280396
Step: 4202, eval/f1: 0.9133297801017761
Step: 4202, eval/runtime: 299.9975891113281
Step: 4202, eval/samples_per_second: 24.010000228881836
Step: 4202, eval/steps_per_second: 3.003000020980835
Step: 4202, train/epoch: 1.0
Step: 4210, train/loss: 0.2094999998807907
Step: 4210, train/grad_norm: 5.401916980743408
Step: 4210, train/learning_rate: 4.499048009165563e-05
Step: 4210, train/epoch: 1.0019038915634155
Step: 4220, train/loss: 0.19140000641345978
Step: 4220, train/grad_norm: 8.020957946777344
Step: 4220, train/learning_rate: 4.497858026297763e-05
Step: 4220, train/epoch: 1.0042836666107178
Step: 4230, train/loss: 0.1573999971151352
Step: 4230, train/grad_norm: 8.342514991760254
Step: 4230, train/learning_rate: 4.496668407227844e-05
Step: 4230, train/epoch: 1.00666344165802
Step: 4240, train/loss: 0.13079999387264252
Step: 4240, train/grad_norm: 3.7178962230682373
Step: 4240, train/learning_rate: 4.495478424360044e-05
Step: 4240, train/epoch: 1.0090433359146118
Step: 4250, train/loss: 0.06710000336170197
Step: 4250, train/grad_norm: 12.691252708435059
Step: 4250, train/learning_rate: 4.4942884414922446e-05
Step: 4250, train/epoch: 1.011423110961914
Step: 4260, train/loss: 0.3179999887943268
Step: 4260, train/grad_norm: 35.11282730102539
Step: 4260, train/learning_rate: 4.493098458624445e-05
Step: 4260, train/epoch: 1.0138030052185059
Step: 4270, train/loss: 0.35659998655319214
Step: 4270, train/grad_norm: 16.026338577270508
Step: 4270, train/learning_rate: 4.491908475756645e-05
Step: 4270, train/epoch: 1.016182780265808
Step: 4280, train/loss: 0.42320001125335693
Step: 4280, train/grad_norm: 6.375600337982178
Step: 4280, train/learning_rate: 4.490718856686726e-05
Step: 4280, train/epoch: 1.0185625553131104
Step: 4290, train/loss: 0.3619000017642975
Step: 4290, train/grad_norm: 0.016634779050946236
Step: 4290, train/learning_rate: 4.4895288738189265e-05
Step: 4290, train/epoch: 1.0209424495697021
Step: 4300, train/loss: 0.1664000004529953
Step: 4300, train/grad_norm: 6.287405967712402
Step: 4300, train/learning_rate: 4.488338890951127e-05
Step: 4300, train/epoch: 1.0233222246170044
Step: 4310, train/loss: 0.2013999968767166
Step: 4310, train/grad_norm: 14.629152297973633
Step: 4310, train/learning_rate: 4.487148908083327e-05
Step: 4310, train/epoch: 1.0257019996643066
Step: 4320, train/loss: 0.16509999334812164
Step: 4320, train/grad_norm: 0.06041717901825905
Step: 4320, train/learning_rate: 4.4859589252155274e-05
Step: 4320, train/epoch: 1.0280818939208984
Step: 4330, train/loss: 0.09679999947547913
Step: 4330, train/grad_norm: 5.052001476287842
Step: 4330, train/learning_rate: 4.4847693061456084e-05
Step: 4330, train/epoch: 1.0304616689682007
Step: 4340, train/loss: 0.2687000036239624
Step: 4340, train/grad_norm: 6.87756872177124
Step: 4340, train/learning_rate: 4.483579323277809e-05
Step: 4340, train/epoch: 1.0328415632247925
Step: 4350, train/loss: 0.43459999561309814
Step: 4350, train/grad_norm: 9.785887718200684
Step: 4350, train/learning_rate: 4.482389340410009e-05
Step: 4350, train/epoch: 1.0352213382720947
Step: 4360, train/loss: 0.21979999542236328
Step: 4360, train/grad_norm: 0.6443868279457092
Step: 4360, train/learning_rate: 4.481199357542209e-05
Step: 4360, train/epoch: 1.037601113319397
Step: 4370, train/loss: 0.23469999432563782
Step: 4370, train/grad_norm: 15.118436813354492
Step: 4370, train/learning_rate: 4.4800093746744096e-05
Step: 4370, train/epoch: 1.0399810075759888
Step: 4380, train/loss: 0.38519999384880066
Step: 4380, train/grad_norm: 12.824528694152832
Step: 4380, train/learning_rate: 4.4788197556044906e-05
Step: 4380, train/epoch: 1.042360782623291
Step: 4390, train/loss: 0.1256999969482422
Step: 4390, train/grad_norm: 10.941153526306152
Step: 4390, train/learning_rate: 4.477629772736691e-05
Step: 4390, train/epoch: 1.0447405576705933
Step: 4400, train/loss: 0.24869999289512634
Step: 4400, train/grad_norm: 12.375982284545898
Step: 4400, train/learning_rate: 4.476439789868891e-05
Step: 4400, train/epoch: 1.047120451927185
Step: 4410, train/loss: 0.21899999678134918
Step: 4410, train/grad_norm: 9.05161190032959
Step: 4410, train/learning_rate: 4.4752498070010915e-05
Step: 4410, train/epoch: 1.0495002269744873
Step: 4420, train/loss: 0.18019999563694
Step: 4420, train/grad_norm: 5.097748279571533
Step: 4420, train/learning_rate: 4.474059824133292e-05
Step: 4420, train/epoch: 1.0518800020217896
Step: 4430, train/loss: 0.1451999992132187
Step: 4430, train/grad_norm: 11.21727180480957
Step: 4430, train/learning_rate: 4.472870205063373e-05
Step: 4430, train/epoch: 1.0542598962783813
Step: 4440, train/loss: 0.08349999785423279
Step: 4440, train/grad_norm: 0.05030510202050209
Step: 4440, train/learning_rate: 4.471680222195573e-05
Step: 4440, train/epoch: 1.0566396713256836
Step: 4450, train/loss: 0.2207999974489212
Step: 4450, train/grad_norm: 0.6037352085113525
Step: 4450, train/learning_rate: 4.4704902393277735e-05
Step: 4450, train/epoch: 1.0590195655822754
Step: 4460, train/loss: 0.2621999979019165
Step: 4460, train/grad_norm: 0.25090986490249634
Step: 4460, train/learning_rate: 4.469300256459974e-05
Step: 4460, train/epoch: 1.0613993406295776
Step: 4470, train/loss: 0.21860000491142273
Step: 4470, train/grad_norm: 12.046032905578613
Step: 4470, train/learning_rate: 4.468110273592174e-05
Step: 4470, train/epoch: 1.0637791156768799
Step: 4480, train/loss: 0.2903999984264374
Step: 4480, train/grad_norm: 0.15116888284683228
Step: 4480, train/learning_rate: 4.466920654522255e-05
Step: 4480, train/epoch: 1.0661590099334717
Step: 4490, train/loss: 0.21220000088214874
Step: 4490, train/grad_norm: 3.8124382495880127
Step: 4490, train/learning_rate: 4.4657306716544554e-05
Step: 4490, train/epoch: 1.068538784980774
Step: 4500, train/loss: 0.26100000739097595
Step: 4500, train/grad_norm: 9.306201934814453
Step: 4500, train/learning_rate: 4.464540688786656e-05
Step: 4500, train/epoch: 1.0709185600280762
Step: 4510, train/loss: 0.2029000073671341
Step: 4510, train/grad_norm: 0.4062995910644531
Step: 4510, train/learning_rate: 4.463350705918856e-05
Step: 4510, train/epoch: 1.073298454284668
Step: 4520, train/loss: 0.1324000060558319
Step: 4520, train/grad_norm: 4.923394680023193
Step: 4520, train/learning_rate: 4.462160723051056e-05
Step: 4520, train/epoch: 1.0756782293319702
Step: 4530, train/loss: 0.19519999623298645
Step: 4530, train/grad_norm: 2.628005027770996
Step: 4530, train/learning_rate: 4.460971103981137e-05
Step: 4530, train/epoch: 1.078058123588562
Step: 4540, train/loss: 0.188400000333786
Step: 4540, train/grad_norm: 47.598915100097656
Step: 4540, train/learning_rate: 4.4597811211133376e-05
Step: 4540, train/epoch: 1.0804378986358643
Step: 4550, train/loss: 0.09300000220537186
Step: 4550, train/grad_norm: 0.3592042624950409
Step: 4550, train/learning_rate: 4.458591138245538e-05
Step: 4550, train/epoch: 1.0828176736831665
Step: 4560, train/loss: 0.3188999891281128
Step: 4560, train/grad_norm: 1.6086808443069458
Step: 4560, train/learning_rate: 4.457401155377738e-05
Step: 4560, train/epoch: 1.0851975679397583
Step: 4570, train/loss: 0.19820000231266022
Step: 4570, train/grad_norm: 0.9171634912490845
Step: 4570, train/learning_rate: 4.4562111725099385e-05
Step: 4570, train/epoch: 1.0875773429870605
Step: 4580, train/loss: 0.12680000066757202
Step: 4580, train/grad_norm: 0.3791918158531189
Step: 4580, train/learning_rate: 4.4550215534400195e-05
Step: 4580, train/epoch: 1.0899571180343628
Step: 4590, train/loss: 0.31779998540878296
Step: 4590, train/grad_norm: 8.423480987548828
Step: 4590, train/learning_rate: 4.45383157057222e-05
Step: 4590, train/epoch: 1.0923370122909546
Step: 4600, train/loss: 0.21549999713897705
Step: 4600, train/grad_norm: 16.789751052856445
Step: 4600, train/learning_rate: 4.45264158770442e-05
Step: 4600, train/epoch: 1.0947167873382568
Step: 4610, train/loss: 0.125900000333786
Step: 4610, train/grad_norm: 0.7294071912765503
Step: 4610, train/learning_rate: 4.4514516048366204e-05
Step: 4610, train/epoch: 1.097096562385559
Step: 4620, train/loss: 0.2069000005722046
Step: 4620, train/grad_norm: 0.17247274518013
Step: 4620, train/learning_rate: 4.450261621968821e-05
Step: 4620, train/epoch: 1.0994764566421509
Step: 4630, train/loss: 0.2167000025510788
Step: 4630, train/grad_norm: 0.005479938816279173
Step: 4630, train/learning_rate: 4.449072002898902e-05
Step: 4630, train/epoch: 1.1018562316894531
Step: 4640, train/loss: 0.2786000072956085
Step: 4640, train/grad_norm: 22.223791122436523
Step: 4640, train/learning_rate: 4.447882020031102e-05
Step: 4640, train/epoch: 1.104236125946045
Step: 4650, train/loss: 0.04259999841451645
Step: 4650, train/grad_norm: 0.07794831693172455
Step: 4650, train/learning_rate: 4.446692037163302e-05
Step: 4650, train/epoch: 1.1066159009933472
Step: 4660, train/loss: 0.120899997651577
Step: 4660, train/grad_norm: 1.7785115242004395
Step: 4660, train/learning_rate: 4.4455020542955026e-05
Step: 4660, train/epoch: 1.1089956760406494
Step: 4670, train/loss: 0.22769999504089355
Step: 4670, train/grad_norm: 0.028465913608670235
Step: 4670, train/learning_rate: 4.444312071427703e-05
Step: 4670, train/epoch: 1.1113755702972412
Step: 4680, train/loss: 0.16380000114440918
Step: 4680, train/grad_norm: 0.07329830527305603
Step: 4680, train/learning_rate: 4.443122452357784e-05
Step: 4680, train/epoch: 1.1137553453445435
Step: 4690, train/loss: 0.21580000221729279
Step: 4690, train/grad_norm: 23.168350219726562
Step: 4690, train/learning_rate: 4.441932469489984e-05
Step: 4690, train/epoch: 1.1161351203918457
Step: 4700, train/loss: 0.3684999942779541
Step: 4700, train/grad_norm: 8.174043655395508
Step: 4700, train/learning_rate: 4.4407424866221845e-05
Step: 4700, train/epoch: 1.1185150146484375
Step: 4710, train/loss: 0.19009999930858612
Step: 4710, train/grad_norm: 3.826284885406494
Step: 4710, train/learning_rate: 4.439552503754385e-05
Step: 4710, train/epoch: 1.1208947896957397
Step: 4720, train/loss: 0.2143000066280365
Step: 4720, train/grad_norm: 3.2150425910949707
Step: 4720, train/learning_rate: 4.438362520886585e-05
Step: 4720, train/epoch: 1.1232746839523315
Step: 4730, train/loss: 0.14339999854564667
Step: 4730, train/grad_norm: 5.13751220703125
Step: 4730, train/learning_rate: 4.437172901816666e-05
Step: 4730, train/epoch: 1.1256544589996338
Step: 4740, train/loss: 0.4230000078678131
Step: 4740, train/grad_norm: 0.643084704875946
Step: 4740, train/learning_rate: 4.4359829189488664e-05
Step: 4740, train/epoch: 1.128034234046936
Step: 4750, train/loss: 0.26759999990463257
Step: 4750, train/grad_norm: 20.37302589416504
Step: 4750, train/learning_rate: 4.434792936081067e-05
Step: 4750, train/epoch: 1.1304141283035278
Step: 4760, train/loss: 0.12710000574588776
Step: 4760, train/grad_norm: 0.22063322365283966
Step: 4760, train/learning_rate: 4.433602953213267e-05
Step: 4760, train/epoch: 1.13279390335083
Step: 4770, train/loss: 0.27970001101493835
Step: 4770, train/grad_norm: 1.8515511751174927
Step: 4770, train/learning_rate: 4.432412970345467e-05
Step: 4770, train/epoch: 1.1351736783981323
Step: 4780, train/loss: 0.0617000013589859
Step: 4780, train/grad_norm: 10.7245512008667
Step: 4780, train/learning_rate: 4.4312233512755483e-05
Step: 4780, train/epoch: 1.1375535726547241
Step: 4790, train/loss: 0.1720000058412552
Step: 4790, train/grad_norm: 0.10510609298944473
Step: 4790, train/learning_rate: 4.4300333684077486e-05
Step: 4790, train/epoch: 1.1399333477020264
Step: 4800, train/loss: 0.2590999901294708
Step: 4800, train/grad_norm: 0.43967950344085693
Step: 4800, train/learning_rate: 4.428843385539949e-05
Step: 4800, train/epoch: 1.1423132419586182
Step: 4810, train/loss: 0.13449999690055847
Step: 4810, train/grad_norm: 7.3967604637146
Step: 4810, train/learning_rate: 4.427653402672149e-05
Step: 4810, train/epoch: 1.1446930170059204
Step: 4820, train/loss: 0.2590000033378601
Step: 4820, train/grad_norm: 0.07697173207998276
Step: 4820, train/learning_rate: 4.4264634198043495e-05
Step: 4820, train/epoch: 1.1470727920532227
Step: 4830, train/loss: 0.22830000519752502
Step: 4830, train/grad_norm: 6.576517105102539
Step: 4830, train/learning_rate: 4.4252738007344306e-05
Step: 4830, train/epoch: 1.1494526863098145
Step: 4840, train/loss: 0.27090001106262207
Step: 4840, train/grad_norm: 9.560636520385742
Step: 4840, train/learning_rate: 4.424083817866631e-05
Step: 4840, train/epoch: 1.1518324613571167
Step: 4850, train/loss: 0.3059999942779541
Step: 4850, train/grad_norm: 13.892972946166992
Step: 4850, train/learning_rate: 4.422893834998831e-05
Step: 4850, train/epoch: 1.154212236404419
Step: 4860, train/loss: 0.17720000445842743
Step: 4860, train/grad_norm: 0.14189130067825317
Step: 4860, train/learning_rate: 4.4217038521310315e-05
Step: 4860, train/epoch: 1.1565921306610107
Step: 4870, train/loss: 0.2709999978542328
Step: 4870, train/grad_norm: 4.037262916564941
Step: 4870, train/learning_rate: 4.420513869263232e-05
Step: 4870, train/epoch: 1.158971905708313
Step: 4880, train/loss: 0.16750000417232513
Step: 4880, train/grad_norm: 12.323185920715332
Step: 4880, train/learning_rate: 4.419324250193313e-05
Step: 4880, train/epoch: 1.1613516807556152
Step: 4890, train/loss: 0.15279999375343323
Step: 4890, train/grad_norm: 0.5488601922988892
Step: 4890, train/learning_rate: 4.418134267325513e-05
Step: 4890, train/epoch: 1.163731575012207
Step: 4900, train/loss: 0.34310001134872437
Step: 4900, train/grad_norm: 11.736433029174805
Step: 4900, train/learning_rate: 4.4169442844577134e-05
Step: 4900, train/epoch: 1.1661113500595093
Step: 4910, train/loss: 0.18880000710487366
Step: 4910, train/grad_norm: 1.4000251293182373
Step: 4910, train/learning_rate: 4.415754301589914e-05
Step: 4910, train/epoch: 1.168491244316101
Step: 4920, train/loss: 0.2818000018596649
Step: 4920, train/grad_norm: 13.690677642822266
Step: 4920, train/learning_rate: 4.414564318722114e-05
Step: 4920, train/epoch: 1.1708710193634033
Step: 4930, train/loss: 0.1477999985218048
Step: 4930, train/grad_norm: 0.40422093868255615
Step: 4930, train/learning_rate: 4.413374699652195e-05
Step: 4930, train/epoch: 1.1732507944107056
Step: 4940, train/loss: 0.0851999968290329
Step: 4940, train/grad_norm: 5.831392288208008
Step: 4940, train/learning_rate: 4.412184716784395e-05
Step: 4940, train/epoch: 1.1756306886672974
Step: 4950, train/loss: 0.17000000178813934
Step: 4950, train/grad_norm: 12.522839546203613
Step: 4950, train/learning_rate: 4.4109947339165956e-05
Step: 4950, train/epoch: 1.1780104637145996
Step: 4960, train/loss: 0.19529999792575836
Step: 4960, train/grad_norm: 7.9264817237854
Step: 4960, train/learning_rate: 4.409804751048796e-05
Step: 4960, train/epoch: 1.1803902387619019
Step: 4970, train/loss: 0.23330000042915344
Step: 4970, train/grad_norm: 2.899780511856079
Step: 4970, train/learning_rate: 4.408614768180996e-05
Step: 4970, train/epoch: 1.1827701330184937
Step: 4980, train/loss: 0.22439999878406525
Step: 4980, train/grad_norm: 0.1354357898235321
Step: 4980, train/learning_rate: 4.407425149111077e-05
Step: 4980, train/epoch: 1.185149908065796
Step: 4990, train/loss: 0.17270000278949738
Step: 4990, train/grad_norm: 2.1339621543884277
Step: 4990, train/learning_rate: 4.4062351662432775e-05
Step: 4990, train/epoch: 1.1875298023223877
Step: 5000, train/loss: 0.23569999635219574
Step: 5000, train/grad_norm: 5.9596076011657715
Step: 5000, train/learning_rate: 4.405045183375478e-05
Step: 5000, train/epoch: 1.18990957736969
Step: 5010, train/loss: 0.10119999945163727
Step: 5010, train/grad_norm: 5.739253520965576
Step: 5010, train/learning_rate: 4.403855200507678e-05
Step: 5010, train/epoch: 1.1922893524169922
Step: 5020, train/loss: 0.125900000333786
Step: 5020, train/grad_norm: 6.526054382324219
Step: 5020, train/learning_rate: 4.4026652176398784e-05
Step: 5020, train/epoch: 1.194669246673584
Step: 5030, train/loss: 0.3257000148296356
Step: 5030, train/grad_norm: 8.202936172485352
Step: 5030, train/learning_rate: 4.4014755985699594e-05
Step: 5030, train/epoch: 1.1970490217208862
Step: 5040, train/loss: 0.18000000715255737
Step: 5040, train/grad_norm: 0.8462213277816772
Step: 5040, train/learning_rate: 4.40028561570216e-05
Step: 5040, train/epoch: 1.1994287967681885
Step: 5050, train/loss: 0.26589998602867126
Step: 5050, train/grad_norm: 18.700754165649414
Step: 5050, train/learning_rate: 4.39909563283436e-05
Step: 5050, train/epoch: 1.2018086910247803
Step: 5060, train/loss: 0.4083000123500824
Step: 5060, train/grad_norm: 7.996042251586914
Step: 5060, train/learning_rate: 4.39790564996656e-05
Step: 5060, train/epoch: 1.2041884660720825
Step: 5070, train/loss: 0.14399999380111694
Step: 5070, train/grad_norm: 1.0992857217788696
Step: 5070, train/learning_rate: 4.396716030896641e-05
Step: 5070, train/epoch: 1.2065683603286743
Step: 5080, train/loss: 0.1177000030875206
Step: 5080, train/grad_norm: 9.79658031463623
Step: 5080, train/learning_rate: 4.3955260480288416e-05
Step: 5080, train/epoch: 1.2089481353759766
Step: 5090, train/loss: 0.335099995136261
Step: 5090, train/grad_norm: 20.565271377563477
Step: 5090, train/learning_rate: 4.394336065161042e-05
Step: 5090, train/epoch: 1.2113279104232788
Step: 5100, train/loss: 0.13699999451637268
Step: 5100, train/grad_norm: 13.37424373626709
Step: 5100, train/learning_rate: 4.393146082293242e-05
Step: 5100, train/epoch: 1.2137078046798706
Step: 5110, train/loss: 0.18940000236034393
Step: 5110, train/grad_norm: 1.0930300951004028
Step: 5110, train/learning_rate: 4.3919560994254425e-05
Step: 5110, train/epoch: 1.2160875797271729
Step: 5120, train/loss: 0.289900004863739
Step: 5120, train/grad_norm: 2.531860828399658
Step: 5120, train/learning_rate: 4.3907664803555235e-05
Step: 5120, train/epoch: 1.218467354774475
Step: 5130, train/loss: 0.34369999170303345
Step: 5130, train/grad_norm: 8.613889694213867
Step: 5130, train/learning_rate: 4.389576497487724e-05
Step: 5130, train/epoch: 1.220847249031067
Step: 5140, train/loss: 0.25279998779296875
Step: 5140, train/grad_norm: 14.537628173828125
Step: 5140, train/learning_rate: 4.388386514619924e-05
Step: 5140, train/epoch: 1.2232270240783691
Step: 5150, train/loss: 0.31139999628067017
Step: 5150, train/grad_norm: 8.447121620178223
Step: 5150, train/learning_rate: 4.3871965317521244e-05
Step: 5150, train/epoch: 1.2256067991256714
Step: 5160, train/loss: 0.2671999931335449
Step: 5160, train/grad_norm: 11.670421600341797
Step: 5160, train/learning_rate: 4.386006548884325e-05
Step: 5160, train/epoch: 1.2279866933822632
Step: 5170, train/loss: 0.23000000417232513
Step: 5170, train/grad_norm: 3.25380802154541
Step: 5170, train/learning_rate: 4.384816929814406e-05
Step: 5170, train/epoch: 1.2303664684295654
Step: 5180, train/loss: 0.17270000278949738
Step: 5180, train/grad_norm: 2.595567226409912
Step: 5180, train/learning_rate: 4.383626946946606e-05
Step: 5180, train/epoch: 1.2327463626861572
Step: 5190, train/loss: 0.13650000095367432
Step: 5190, train/grad_norm: 9.920801162719727
Step: 5190, train/learning_rate: 4.3824369640788063e-05
Step: 5190, train/epoch: 1.2351261377334595
Step: 5200, train/loss: 0.1437000036239624
Step: 5200, train/grad_norm: 3.28471040725708
Step: 5200, train/learning_rate: 4.3812469812110066e-05
Step: 5200, train/epoch: 1.2375059127807617
Step: 5210, train/loss: 0.23440000414848328
Step: 5210, train/grad_norm: 4.836111068725586
Step: 5210, train/learning_rate: 4.380056998343207e-05
Step: 5210, train/epoch: 1.2398858070373535
Step: 5220, train/loss: 0.19290000200271606
Step: 5220, train/grad_norm: 1.937652349472046
Step: 5220, train/learning_rate: 4.378867379273288e-05
Step: 5220, train/epoch: 1.2422655820846558
Step: 5230, train/loss: 0.1687999963760376
Step: 5230, train/grad_norm: 2.103992223739624
Step: 5230, train/learning_rate: 4.377677396405488e-05
Step: 5230, train/epoch: 1.244645357131958
Step: 5240, train/loss: 0.31380000710487366
Step: 5240, train/grad_norm: 0.6863937377929688
Step: 5240, train/learning_rate: 4.3764874135376886e-05
Step: 5240, train/epoch: 1.2470252513885498
Step: 5250, train/loss: 0.147599995136261
Step: 5250, train/grad_norm: 10.607117652893066
Step: 5250, train/learning_rate: 4.375297430669889e-05
Step: 5250, train/epoch: 1.249405026435852
Step: 5260, train/loss: 0.09960000216960907
Step: 5260, train/grad_norm: 0.09040915966033936
Step: 5260, train/learning_rate: 4.374107447802089e-05
Step: 5260, train/epoch: 1.2517849206924438
Step: 5270, train/loss: 0.4537999927997589
Step: 5270, train/grad_norm: 10.32589054107666
Step: 5270, train/learning_rate: 4.37291782873217e-05
Step: 5270, train/epoch: 1.254164695739746
Step: 5280, train/loss: 0.24310000240802765
Step: 5280, train/grad_norm: 0.35663095116615295
Step: 5280, train/learning_rate: 4.3717278458643705e-05
Step: 5280, train/epoch: 1.2565444707870483
Step: 5290, train/loss: 0.05570000037550926
Step: 5290, train/grad_norm: 14.81864070892334
Step: 5290, train/learning_rate: 4.370537862996571e-05
Step: 5290, train/epoch: 1.2589243650436401
Step: 5300, train/loss: 0.3427000045776367
Step: 5300, train/grad_norm: 6.130232334136963
Step: 5300, train/learning_rate: 4.369347880128771e-05
Step: 5300, train/epoch: 1.2613041400909424
Step: 5310, train/loss: 0.4214000105857849
Step: 5310, train/grad_norm: 12.258431434631348
Step: 5310, train/learning_rate: 4.3681578972609714e-05
Step: 5310, train/epoch: 1.2636839151382446
Step: 5320, train/loss: 0.2630999982357025
Step: 5320, train/grad_norm: 3.321100950241089
Step: 5320, train/learning_rate: 4.3669682781910524e-05
Step: 5320, train/epoch: 1.2660638093948364
Step: 5330, train/loss: 0.05779999867081642
Step: 5330, train/grad_norm: 2.1032650470733643
Step: 5330, train/learning_rate: 4.365778295323253e-05
Step: 5330, train/epoch: 1.2684435844421387
Step: 5340, train/loss: 0.2793000042438507
Step: 5340, train/grad_norm: 22.30088996887207
Step: 5340, train/learning_rate: 4.364588312455453e-05
Step: 5340, train/epoch: 1.270823359489441
Step: 5350, train/loss: 0.4115999937057495
Step: 5350, train/grad_norm: 1.6480756998062134
Step: 5350, train/learning_rate: 4.363398329587653e-05
Step: 5350, train/epoch: 1.2732032537460327
Step: 5360, train/loss: 0.043299999088048935
Step: 5360, train/grad_norm: 0.11419103294610977
Step: 5360, train/learning_rate: 4.3622083467198536e-05
Step: 5360, train/epoch: 1.275583028793335
Step: 5370, train/loss: 0.13349999487400055
Step: 5370, train/grad_norm: 7.605289459228516
Step: 5370, train/learning_rate: 4.3610187276499346e-05
Step: 5370, train/epoch: 1.2779629230499268
Step: 5380, train/loss: 0.2304999977350235
Step: 5380, train/grad_norm: 15.035144805908203
Step: 5380, train/learning_rate: 4.359828744782135e-05
Step: 5380, train/epoch: 1.280342698097229
Step: 5390, train/loss: 0.2802000045776367
Step: 5390, train/grad_norm: 13.074581146240234
Step: 5390, train/learning_rate: 4.358638761914335e-05
Step: 5390, train/epoch: 1.2827224731445312
Step: 5400, train/loss: 0.18770000338554382
Step: 5400, train/grad_norm: 1.2227500677108765
Step: 5400, train/learning_rate: 4.3574487790465355e-05
Step: 5400, train/epoch: 1.285102367401123
Step: 5410, train/loss: 0.17759999632835388
Step: 5410, train/grad_norm: 3.7641549110412598
Step: 5410, train/learning_rate: 4.356258796178736e-05
Step: 5410, train/epoch: 1.2874821424484253
Step: 5420, train/loss: 0.2939000129699707
Step: 5420, train/grad_norm: 8.349982261657715
Step: 5420, train/learning_rate: 4.355069177108817e-05
Step: 5420, train/epoch: 1.2898619174957275
Step: 5430, train/loss: 0.14219999313354492
Step: 5430, train/grad_norm: 1.914447546005249
Step: 5430, train/learning_rate: 4.353879194241017e-05
Step: 5430, train/epoch: 1.2922418117523193
Step: 5440, train/loss: 0.23479999601840973
Step: 5440, train/grad_norm: 16.733503341674805
Step: 5440, train/learning_rate: 4.3526892113732174e-05
Step: 5440, train/epoch: 1.2946215867996216
Step: 5450, train/loss: 0.21459999680519104
Step: 5450, train/grad_norm: 11.798439979553223
Step: 5450, train/learning_rate: 4.351499228505418e-05
Step: 5450, train/epoch: 1.2970014810562134
Step: 5460, train/loss: 0.18539999425411224
Step: 5460, train/grad_norm: 11.640036582946777
Step: 5460, train/learning_rate: 4.350309245637618e-05
Step: 5460, train/epoch: 1.2993812561035156
Step: 5470, train/loss: 0.10100000351667404
Step: 5470, train/grad_norm: 20.063512802124023
Step: 5470, train/learning_rate: 4.349119626567699e-05
Step: 5470, train/epoch: 1.3017610311508179
Step: 5480, train/loss: 0.31290000677108765
Step: 5480, train/grad_norm: 18.267629623413086
Step: 5480, train/learning_rate: 4.347929643699899e-05
Step: 5480, train/epoch: 1.3041409254074097
Step: 5490, train/loss: 0.21950000524520874
Step: 5490, train/grad_norm: 13.755964279174805
Step: 5490, train/learning_rate: 4.3467396608320996e-05
Step: 5490, train/epoch: 1.306520700454712
Step: 5500, train/loss: 0.1899999976158142
Step: 5500, train/grad_norm: 14.724749565124512
Step: 5500, train/learning_rate: 4.3455496779643e-05
Step: 5500, train/epoch: 1.3089004755020142
Step: 5510, train/loss: 0.4120999872684479
Step: 5510, train/grad_norm: 0.7437998652458191
Step: 5510, train/learning_rate: 4.3443596950965e-05
Step: 5510, train/epoch: 1.311280369758606
Step: 5520, train/loss: 0.20329999923706055
Step: 5520, train/grad_norm: 15.198346138000488
Step: 5520, train/learning_rate: 4.343170076026581e-05
Step: 5520, train/epoch: 1.3136601448059082
Step: 5530, train/loss: 0.1315000057220459
Step: 5530, train/grad_norm: 24.59493637084961
Step: 5530, train/learning_rate: 4.3419800931587815e-05
Step: 5530, train/epoch: 1.3160400390625
Step: 5540, train/loss: 0.16419999301433563
Step: 5540, train/grad_norm: 0.07804881781339645
Step: 5540, train/learning_rate: 4.340790110290982e-05
Step: 5540, train/epoch: 1.3184198141098022
Step: 5550, train/loss: 0.16539999842643738
Step: 5550, train/grad_norm: 4.285092353820801
Step: 5550, train/learning_rate: 4.339600127423182e-05
Step: 5550, train/epoch: 1.3207995891571045
Step: 5560, train/loss: 0.17260000109672546
Step: 5560, train/grad_norm: 12.864147186279297
Step: 5560, train/learning_rate: 4.3384101445553824e-05
Step: 5560, train/epoch: 1.3231794834136963
Step: 5570, train/loss: 0.21150000393390656
Step: 5570, train/grad_norm: 0.3994444012641907
Step: 5570, train/learning_rate: 4.3372205254854634e-05
Step: 5570, train/epoch: 1.3255592584609985
Step: 5580, train/loss: 0.2506999969482422
Step: 5580, train/grad_norm: 0.35437849164009094
Step: 5580, train/learning_rate: 4.336030542617664e-05
Step: 5580, train/epoch: 1.3279390335083008
Step: 5590, train/loss: 0.08810000121593475
Step: 5590, train/grad_norm: 0.601502537727356
Step: 5590, train/learning_rate: 4.334840559749864e-05
Step: 5590, train/epoch: 1.3303189277648926
Step: 5600, train/loss: 0.12800000607967377
Step: 5600, train/grad_norm: 12.735960960388184
Step: 5600, train/learning_rate: 4.3336505768820643e-05
Step: 5600, train/epoch: 1.3326987028121948
Step: 5610, train/loss: 0.1378999948501587
Step: 5610, train/grad_norm: 8.434530258178711
Step: 5610, train/learning_rate: 4.3324605940142646e-05
Step: 5610, train/epoch: 1.335078477859497
Step: 5620, train/loss: 0.2736999988555908
Step: 5620, train/grad_norm: 12.949578285217285
Step: 5620, train/learning_rate: 4.3312709749443457e-05
Step: 5620, train/epoch: 1.3374583721160889
Step: 5630, train/loss: 0.6757000088691711
Step: 5630, train/grad_norm: 21.432348251342773
Step: 5630, train/learning_rate: 4.330080992076546e-05
Step: 5630, train/epoch: 1.3398381471633911
Step: 5640, train/loss: 0.478300005197525
Step: 5640, train/grad_norm: 14.62195110321045
Step: 5640, train/learning_rate: 4.328891009208746e-05
Step: 5640, train/epoch: 1.342218041419983
Step: 5650, train/loss: 0.14020000398159027
Step: 5650, train/grad_norm: 6.169875621795654
Step: 5650, train/learning_rate: 4.3277010263409466e-05
Step: 5650, train/epoch: 1.3445978164672852
Step: 5660, train/loss: 0.25850000977516174
Step: 5660, train/grad_norm: 10.58705997467041
Step: 5660, train/learning_rate: 4.326511043473147e-05
Step: 5660, train/epoch: 1.3469775915145874
Step: 5670, train/loss: 0.12150000035762787
Step: 5670, train/grad_norm: 8.644441604614258
Step: 5670, train/learning_rate: 4.325321424403228e-05
Step: 5670, train/epoch: 1.3493574857711792
Step: 5680, train/loss: 0.19110000133514404
Step: 5680, train/grad_norm: 6.398301601409912
Step: 5680, train/learning_rate: 4.324131441535428e-05
Step: 5680, train/epoch: 1.3517372608184814
Step: 5690, train/loss: 0.24279999732971191
Step: 5690, train/grad_norm: 8.060891151428223
Step: 5690, train/learning_rate: 4.3229414586676285e-05
Step: 5690, train/epoch: 1.3541170358657837
Step: 5700, train/loss: 0.16609999537467957
Step: 5700, train/grad_norm: 1.4364676475524902
Step: 5700, train/learning_rate: 4.321751475799829e-05
Step: 5700, train/epoch: 1.3564969301223755
Step: 5710, train/loss: 0.10199999809265137
Step: 5710, train/grad_norm: 4.050617218017578
Step: 5710, train/learning_rate: 4.320561492932029e-05
Step: 5710, train/epoch: 1.3588767051696777
Step: 5720, train/loss: 0.17720000445842743
Step: 5720, train/grad_norm: 1.4755867719650269
Step: 5720, train/learning_rate: 4.31937187386211e-05
Step: 5720, train/epoch: 1.3612565994262695
Step: 5730, train/loss: 0.23559999465942383
Step: 5730, train/grad_norm: 3.74534010887146
Step: 5730, train/learning_rate: 4.3181818909943104e-05
Step: 5730, train/epoch: 1.3636363744735718
Step: 5740, train/loss: 0.1225999966263771
Step: 5740, train/grad_norm: 1.1804449558258057
Step: 5740, train/learning_rate: 4.316991908126511e-05
Step: 5740, train/epoch: 1.366016149520874
Step: 5750, train/loss: 0.17790000140666962
Step: 5750, train/grad_norm: 13.722875595092773
Step: 5750, train/learning_rate: 4.315801925258711e-05
Step: 5750, train/epoch: 1.3683960437774658
Step: 5760, train/loss: 0.27869999408721924
Step: 5760, train/grad_norm: 11.173376083374023
Step: 5760, train/learning_rate: 4.314611942390911e-05
Step: 5760, train/epoch: 1.370775818824768
Step: 5770, train/loss: 0.26080000400543213
Step: 5770, train/grad_norm: 1.6586202383041382
Step: 5770, train/learning_rate: 4.313422323320992e-05
Step: 5770, train/epoch: 1.3731555938720703
Step: 5780, train/loss: 0.10679999738931656
Step: 5780, train/grad_norm: 1.4241018295288086
Step: 5780, train/learning_rate: 4.3122323404531926e-05
Step: 5780, train/epoch: 1.375535488128662
Step: 5790, train/loss: 0.1331000030040741
Step: 5790, train/grad_norm: 3.7766056060791016
Step: 5790, train/learning_rate: 4.311042357585393e-05
Step: 5790, train/epoch: 1.3779152631759644
Step: 5800, train/loss: 0.3398999869823456
Step: 5800, train/grad_norm: 15.719161033630371
Step: 5800, train/learning_rate: 4.309852374717593e-05
Step: 5800, train/epoch: 1.3802950382232666
Step: 5810, train/loss: 0.13439999520778656
Step: 5810, train/grad_norm: 0.04954226315021515
Step: 5810, train/learning_rate: 4.3086623918497935e-05
Step: 5810, train/epoch: 1.3826749324798584
Step: 5820, train/loss: 0.262800008058548
Step: 5820, train/grad_norm: 1.9910918474197388
Step: 5820, train/learning_rate: 4.3074727727798745e-05
Step: 5820, train/epoch: 1.3850547075271606
Step: 5830, train/loss: 0.20730000734329224
Step: 5830, train/grad_norm: 0.7890052199363708
Step: 5830, train/learning_rate: 4.306282789912075e-05
Step: 5830, train/epoch: 1.3874346017837524
Step: 5840, train/loss: 0.31130000948905945
Step: 5840, train/grad_norm: 8.39484691619873
Step: 5840, train/learning_rate: 4.305092807044275e-05
Step: 5840, train/epoch: 1.3898143768310547
Step: 5850, train/loss: 0.10239999741315842
Step: 5850, train/grad_norm: 0.12969404458999634
Step: 5850, train/learning_rate: 4.3039028241764754e-05
Step: 5850, train/epoch: 1.392194151878357
Step: 5860, train/loss: 0.2623000144958496
Step: 5860, train/grad_norm: 8.250639915466309
Step: 5860, train/learning_rate: 4.302712841308676e-05
Step: 5860, train/epoch: 1.3945740461349487
Step: 5870, train/loss: 0.18150000274181366
Step: 5870, train/grad_norm: 0.5839362740516663
Step: 5870, train/learning_rate: 4.301523222238757e-05
Step: 5870, train/epoch: 1.396953821182251
Step: 5880, train/loss: 0.26499998569488525
Step: 5880, train/grad_norm: 22.11266326904297
Step: 5880, train/learning_rate: 4.300333239370957e-05
Step: 5880, train/epoch: 1.3993335962295532
Step: 5890, train/loss: 0.376800000667572
Step: 5890, train/grad_norm: 0.13411293923854828
Step: 5890, train/learning_rate: 4.299143256503157e-05
Step: 5890, train/epoch: 1.401713490486145
Step: 5900, train/loss: 0.21130000054836273
Step: 5900, train/grad_norm: 3.187281847000122
Step: 5900, train/learning_rate: 4.2979532736353576e-05
Step: 5900, train/epoch: 1.4040932655334473
Step: 5910, train/loss: 0.3666999936103821
Step: 5910, train/grad_norm: 3.5352859497070312
Step: 5910, train/learning_rate: 4.296763290767558e-05
Step: 5910, train/epoch: 1.406473159790039
Step: 5920, train/loss: 0.14910000562667847
Step: 5920, train/grad_norm: 12.270208358764648
Step: 5920, train/learning_rate: 4.295573671697639e-05
Step: 5920, train/epoch: 1.4088529348373413
Step: 5930, train/loss: 0.09239999949932098
Step: 5930, train/grad_norm: 3.3190035820007324
Step: 5930, train/learning_rate: 4.294383688829839e-05
Step: 5930, train/epoch: 1.4112327098846436
Step: 5940, train/loss: 0.10050000250339508
Step: 5940, train/grad_norm: 11.426849365234375
Step: 5940, train/learning_rate: 4.2931937059620395e-05
Step: 5940, train/epoch: 1.4136126041412354
Step: 5950, train/loss: 0.2946000099182129
Step: 5950, train/grad_norm: 0.23764416575431824
Step: 5950, train/learning_rate: 4.29200372309424e-05
Step: 5950, train/epoch: 1.4159923791885376
Step: 5960, train/loss: 0.3522000014781952
Step: 5960, train/grad_norm: 24.141876220703125
Step: 5960, train/learning_rate: 4.29081374022644e-05
Step: 5960, train/epoch: 1.4183721542358398
Step: 5970, train/loss: 0.11699999868869781
Step: 5970, train/grad_norm: 0.047109443694353104
Step: 5970, train/learning_rate: 4.289624121156521e-05
Step: 5970, train/epoch: 1.4207520484924316
Step: 5980, train/loss: 0.1785999983549118
Step: 5980, train/grad_norm: 4.198361873626709
Step: 5980, train/learning_rate: 4.2884341382887214e-05
Step: 5980, train/epoch: 1.4231318235397339
Step: 5990, train/loss: 0.20059999823570251
Step: 5990, train/grad_norm: 0.8907216787338257
Step: 5990, train/learning_rate: 4.287244155420922e-05
Step: 5990, train/epoch: 1.4255117177963257
Step: 6000, train/loss: 0.2597000002861023
Step: 6000, train/grad_norm: 0.31660565733909607
Step: 6000, train/learning_rate: 4.286054172553122e-05
Step: 6000, train/epoch: 1.427891492843628
Step: 6010, train/loss: 0.09640000015497208
Step: 6010, train/grad_norm: 6.893317222595215
Step: 6010, train/learning_rate: 4.2848641896853223e-05
Step: 6010, train/epoch: 1.4302712678909302
Step: 6020, train/loss: 0.21819999814033508
Step: 6020, train/grad_norm: 4.436023712158203
Step: 6020, train/learning_rate: 4.2836745706154034e-05
Step: 6020, train/epoch: 1.432651162147522
Step: 6030, train/loss: 0.3357999920845032
Step: 6030, train/grad_norm: 7.6164422035217285
Step: 6030, train/learning_rate: 4.2824845877476037e-05
Step: 6030, train/epoch: 1.4350309371948242
Step: 6040, train/loss: 0.2939999997615814
Step: 6040, train/grad_norm: 0.4888867735862732
Step: 6040, train/learning_rate: 4.281294604879804e-05
Step: 6040, train/epoch: 1.4374107122421265
Step: 6050, train/loss: 0.18870000541210175
Step: 6050, train/grad_norm: 5.503464698791504
Step: 6050, train/learning_rate: 4.280104622012004e-05
Step: 6050, train/epoch: 1.4397906064987183
Step: 6060, train/loss: 0.3034999966621399
Step: 6060, train/grad_norm: 7.7563042640686035
Step: 6060, train/learning_rate: 4.2789146391442046e-05
Step: 6060, train/epoch: 1.4421703815460205
Step: 6070, train/loss: 0.10459999740123749
Step: 6070, train/grad_norm: 15.437904357910156
Step: 6070, train/learning_rate: 4.2777250200742856e-05
Step: 6070, train/epoch: 1.4445501565933228
Step: 6080, train/loss: 0.19629999995231628
Step: 6080, train/grad_norm: 3.0237722396850586
Step: 6080, train/learning_rate: 4.276535037206486e-05
Step: 6080, train/epoch: 1.4469300508499146
Step: 6090, train/loss: 0.24580000340938568
Step: 6090, train/grad_norm: 2.696732997894287
Step: 6090, train/learning_rate: 4.275345054338686e-05
Step: 6090, train/epoch: 1.4493098258972168
Step: 6100, train/loss: 0.2085999995470047
Step: 6100, train/grad_norm: 4.875088214874268
Step: 6100, train/learning_rate: 4.2741550714708865e-05
Step: 6100, train/epoch: 1.4516897201538086
Step: 6110, train/loss: 0.06199999898672104
Step: 6110, train/grad_norm: 0.0347270630300045
Step: 6110, train/learning_rate: 4.272965088603087e-05
Step: 6110, train/epoch: 1.4540694952011108
Step: 6120, train/loss: 0.21660000085830688
Step: 6120, train/grad_norm: 16.20861053466797
Step: 6120, train/learning_rate: 4.271775469533168e-05
Step: 6120, train/epoch: 1.456449270248413
Step: 6130, train/loss: 0.2199999988079071
Step: 6130, train/grad_norm: 0.00857597216963768
Step: 6130, train/learning_rate: 4.270585486665368e-05
Step: 6130, train/epoch: 1.4588291645050049
Step: 6140, train/loss: 0.017400000244379044
Step: 6140, train/grad_norm: 4.289789199829102
Step: 6140, train/learning_rate: 4.2693955037975684e-05
Step: 6140, train/epoch: 1.4612089395523071
Step: 6150, train/loss: 0.13259999454021454
Step: 6150, train/grad_norm: 0.08987589925527573
Step: 6150, train/learning_rate: 4.268205520929769e-05
Step: 6150, train/epoch: 1.4635887145996094
Step: 6160, train/loss: 0.15809999406337738
Step: 6160, train/grad_norm: 6.64030122756958
Step: 6160, train/learning_rate: 4.267015538061969e-05
Step: 6160, train/epoch: 1.4659686088562012
Step: 6170, train/loss: 0.07150000333786011
Step: 6170, train/grad_norm: 16.48626136779785
Step: 6170, train/learning_rate: 4.26582591899205e-05
Step: 6170, train/epoch: 1.4683483839035034
Step: 6180, train/loss: 0.13040000200271606
Step: 6180, train/grad_norm: 1.557085633277893
Step: 6180, train/learning_rate: 4.26463593612425e-05
Step: 6180, train/epoch: 1.4707282781600952
Step: 6190, train/loss: 0.07370000332593918
Step: 6190, train/grad_norm: 0.029216215014457703
Step: 6190, train/learning_rate: 4.2634459532564506e-05
Step: 6190, train/epoch: 1.4731080532073975
Step: 6200, train/loss: 0.6096000075340271
Step: 6200, train/grad_norm: 0.36715587973594666
Step: 6200, train/learning_rate: 4.262255970388651e-05
Step: 6200, train/epoch: 1.4754878282546997
Step: 6210, train/loss: 0.11140000075101852
Step: 6210, train/grad_norm: 0.36468207836151123
Step: 6210, train/learning_rate: 4.261065987520851e-05
Step: 6210, train/epoch: 1.4778677225112915
Step: 6220, train/loss: 0.24279999732971191
Step: 6220, train/grad_norm: 0.21776893734931946
Step: 6220, train/learning_rate: 4.259876368450932e-05
Step: 6220, train/epoch: 1.4802474975585938
Step: 6230, train/loss: 0.1527000069618225
Step: 6230, train/grad_norm: 0.008251368068158627
Step: 6230, train/learning_rate: 4.2586863855831325e-05
Step: 6230, train/epoch: 1.482627272605896
Step: 6240, train/loss: 0.263700008392334
Step: 6240, train/grad_norm: 0.07967853546142578
Step: 6240, train/learning_rate: 4.257496402715333e-05
Step: 6240, train/epoch: 1.4850071668624878
Step: 6250, train/loss: 0.29589998722076416
Step: 6250, train/grad_norm: 7.108532905578613
Step: 6250, train/learning_rate: 4.256306419847533e-05
Step: 6250, train/epoch: 1.48738694190979
Step: 6260, train/loss: 0.18700000643730164
Step: 6260, train/grad_norm: 16.09739875793457
Step: 6260, train/learning_rate: 4.2551164369797334e-05
Step: 6260, train/epoch: 1.4897668361663818
Step: 6270, train/loss: 0.3659000098705292
Step: 6270, train/grad_norm: 0.019058356061577797
Step: 6270, train/learning_rate: 4.2539268179098144e-05
Step: 6270, train/epoch: 1.492146611213684
Step: 6280, train/loss: 0.2605000138282776
Step: 6280, train/grad_norm: 7.109134197235107
Step: 6280, train/learning_rate: 4.252736835042015e-05
Step: 6280, train/epoch: 1.4945263862609863
Step: 6290, train/loss: 0.24310000240802765
Step: 6290, train/grad_norm: 6.837545871734619
Step: 6290, train/learning_rate: 4.251546852174215e-05
Step: 6290, train/epoch: 1.4969062805175781
Step: 6300, train/loss: 0.15240000188350677
Step: 6300, train/grad_norm: 2.2305679321289062
Step: 6300, train/learning_rate: 4.250356869306415e-05
Step: 6300, train/epoch: 1.4992860555648804
Step: 6310, train/loss: 0.1388999968767166
Step: 6310, train/grad_norm: 17.704355239868164
Step: 6310, train/learning_rate: 4.2491668864386156e-05
Step: 6310, train/epoch: 1.5016658306121826
Step: 6320, train/loss: 0.22010000050067902
Step: 6320, train/grad_norm: 12.250359535217285
Step: 6320, train/learning_rate: 4.2479772673686966e-05
Step: 6320, train/epoch: 1.5040457248687744
Step: 6330, train/loss: 0.1923000067472458
Step: 6330, train/grad_norm: 0.5085326433181763
Step: 6330, train/learning_rate: 4.246787284500897e-05
Step: 6330, train/epoch: 1.5064254999160767
Step: 6340, train/loss: 0.17080000042915344
Step: 6340, train/grad_norm: 15.234856605529785
Step: 6340, train/learning_rate: 4.245597301633097e-05
Step: 6340, train/epoch: 1.508805274963379
Step: 6350, train/loss: 0.11779999732971191
Step: 6350, train/grad_norm: 4.9631452560424805
Step: 6350, train/learning_rate: 4.2444073187652975e-05
Step: 6350, train/epoch: 1.5111851692199707
Step: 6360, train/loss: 0.34880000352859497
Step: 6360, train/grad_norm: 11.821950912475586
Step: 6360, train/learning_rate: 4.243217335897498e-05
Step: 6360, train/epoch: 1.513564944267273
Step: 6370, train/loss: 0.06459999829530716
Step: 6370, train/grad_norm: 0.07762270420789719
Step: 6370, train/learning_rate: 4.242027716827579e-05
Step: 6370, train/epoch: 1.5159448385238647
Step: 6380, train/loss: 0.15770000219345093
Step: 6380, train/grad_norm: 0.4223920702934265
Step: 6380, train/learning_rate: 4.240837733959779e-05
Step: 6380, train/epoch: 1.518324613571167
Step: 6390, train/loss: 0.16750000417232513
Step: 6390, train/grad_norm: 2.2804930210113525
Step: 6390, train/learning_rate: 4.2396477510919794e-05
Step: 6390, train/epoch: 1.5207043886184692
Step: 6400, train/loss: 0.19030000269412994
Step: 6400, train/grad_norm: 2.018329620361328
Step: 6400, train/learning_rate: 4.23845776822418e-05
Step: 6400, train/epoch: 1.523084282875061
Step: 6410, train/loss: 0.23669999837875366
Step: 6410, train/grad_norm: 0.7583805918693542
Step: 6410, train/learning_rate: 4.237268149154261e-05
Step: 6410, train/epoch: 1.5254640579223633
Step: 6420, train/loss: 0.12520000338554382
Step: 6420, train/grad_norm: 0.08409053087234497
Step: 6420, train/learning_rate: 4.236078166286461e-05
Step: 6420, train/epoch: 1.5278438329696655
Step: 6430, train/loss: 0.09830000251531601
Step: 6430, train/grad_norm: 0.45152217149734497
Step: 6430, train/learning_rate: 4.2348881834186614e-05
Step: 6430, train/epoch: 1.5302237272262573
Step: 6440, train/loss: 0.4535999894142151
Step: 6440, train/grad_norm: 0.35758474469184875
Step: 6440, train/learning_rate: 4.2336982005508617e-05
Step: 6440, train/epoch: 1.5326035022735596
Step: 6450, train/loss: 0.28859999775886536
Step: 6450, train/grad_norm: 0.0382094569504261
Step: 6450, train/learning_rate: 4.232508217683062e-05
Step: 6450, train/epoch: 1.5349833965301514
Step: 6460, train/loss: 0.06669999659061432
Step: 6460, train/grad_norm: 1.523128867149353
Step: 6460, train/learning_rate: 4.231318598613143e-05
Step: 6460, train/epoch: 1.5373631715774536
Step: 6470, train/loss: 0.21559999883174896
Step: 6470, train/grad_norm: 16.079553604125977
Step: 6470, train/learning_rate: 4.230128615745343e-05
Step: 6470, train/epoch: 1.5397429466247559
Step: 6480, train/loss: 0.10809999704360962
Step: 6480, train/grad_norm: 0.04116608947515488
Step: 6480, train/learning_rate: 4.2289386328775436e-05
Step: 6480, train/epoch: 1.5421228408813477
Step: 6490, train/loss: 0.14409999549388885
Step: 6490, train/grad_norm: 0.7251169085502625
Step: 6490, train/learning_rate: 4.227748650009744e-05
Step: 6490, train/epoch: 1.54450261592865
Step: 6500, train/loss: 0.19509999454021454
Step: 6500, train/grad_norm: 11.169549942016602
Step: 6500, train/learning_rate: 4.226558667141944e-05
Step: 6500, train/epoch: 1.5468823909759521
Step: 6510, train/loss: 0.07410000264644623
Step: 6510, train/grad_norm: 0.022065041586756706
Step: 6510, train/learning_rate: 4.225369048072025e-05
Step: 6510, train/epoch: 1.549262285232544
Step: 6520, train/loss: 0.1597999930381775
Step: 6520, train/grad_norm: 24.936565399169922
Step: 6520, train/learning_rate: 4.2241790652042255e-05
Step: 6520, train/epoch: 1.5516420602798462
Step: 6530, train/loss: 0.22310000658035278
Step: 6530, train/grad_norm: 18.909549713134766
Step: 6530, train/learning_rate: 4.222989082336426e-05
Step: 6530, train/epoch: 1.5540218353271484
Step: 6540, train/loss: 0.28630000352859497
Step: 6540, train/grad_norm: 0.622279167175293
Step: 6540, train/learning_rate: 4.221799099468626e-05
Step: 6540, train/epoch: 1.5564017295837402
Step: 6550, train/loss: 0.14630000293254852
Step: 6550, train/grad_norm: 0.1663278490304947
Step: 6550, train/learning_rate: 4.2206091166008264e-05
Step: 6550, train/epoch: 1.5587815046310425
Step: 6560, train/loss: 0.302700012922287
Step: 6560, train/grad_norm: 8.132145881652832
Step: 6560, train/learning_rate: 4.2194194975309074e-05
Step: 6560, train/epoch: 1.5611613988876343
Step: 6570, train/loss: 0.0908999964594841
Step: 6570, train/grad_norm: 0.2191675305366516
Step: 6570, train/learning_rate: 4.218229514663108e-05
Step: 6570, train/epoch: 1.5635411739349365
Step: 6580, train/loss: 0.14880000054836273
Step: 6580, train/grad_norm: 0.01866319589316845
Step: 6580, train/learning_rate: 4.217039531795308e-05
Step: 6580, train/epoch: 1.5659209489822388
Step: 6590, train/loss: 0.13819999992847443
Step: 6590, train/grad_norm: 0.012874112464487553
Step: 6590, train/learning_rate: 4.215849548927508e-05
Step: 6590, train/epoch: 1.5683008432388306
Step: 6600, train/loss: 0.2718000113964081
Step: 6600, train/grad_norm: 11.106691360473633
Step: 6600, train/learning_rate: 4.2146595660597086e-05
Step: 6600, train/epoch: 1.5706806182861328
Step: 6610, train/loss: 0.2890999913215637
Step: 6610, train/grad_norm: 4.306662559509277
Step: 6610, train/learning_rate: 4.2134699469897896e-05
Step: 6610, train/epoch: 1.573060393333435
Step: 6620, train/loss: 0.11940000206232071
Step: 6620, train/grad_norm: 9.207165718078613
Step: 6620, train/learning_rate: 4.21227996412199e-05
Step: 6620, train/epoch: 1.5754402875900269
Step: 6630, train/loss: 0.2054000049829483
Step: 6630, train/grad_norm: 17.57774543762207
Step: 6630, train/learning_rate: 4.21108998125419e-05
Step: 6630, train/epoch: 1.577820062637329
Step: 6640, train/loss: 0.37560001015663147
Step: 6640, train/grad_norm: 0.8023149967193604
Step: 6640, train/learning_rate: 4.2098999983863905e-05
Step: 6640, train/epoch: 1.580199956893921
Step: 6650, train/loss: 0.3068000078201294
Step: 6650, train/grad_norm: 0.4884546101093292
Step: 6650, train/learning_rate: 4.208710015518591e-05
Step: 6650, train/epoch: 1.5825797319412231
Step: 6660, train/loss: 0.15209999680519104
Step: 6660, train/grad_norm: 0.7636561989784241
Step: 6660, train/learning_rate: 4.207520396448672e-05
Step: 6660, train/epoch: 1.5849595069885254
Step: 6670, train/loss: 0.31450000405311584
Step: 6670, train/grad_norm: 17.613550186157227
Step: 6670, train/learning_rate: 4.206330413580872e-05
Step: 6670, train/epoch: 1.5873394012451172
Step: 6680, train/loss: 0.06369999796152115
Step: 6680, train/grad_norm: 15.72780704498291
Step: 6680, train/learning_rate: 4.2051404307130724e-05
Step: 6680, train/epoch: 1.5897191762924194
Step: 6690, train/loss: 0.18479999899864197
Step: 6690, train/grad_norm: 0.020946696400642395
Step: 6690, train/learning_rate: 4.203950447845273e-05
Step: 6690, train/epoch: 1.5920989513397217
Step: 6700, train/loss: 0.4771000146865845
Step: 6700, train/grad_norm: 11.987740516662598
Step: 6700, train/learning_rate: 4.202760464977473e-05
Step: 6700, train/epoch: 1.5944788455963135
Step: 6710, train/loss: 0.12870000302791595
Step: 6710, train/grad_norm: 1.6322808265686035
Step: 6710, train/learning_rate: 4.201570845907554e-05
Step: 6710, train/epoch: 1.5968586206436157
Step: 6720, train/loss: 0.20020000636577606
Step: 6720, train/grad_norm: 12.813295364379883
Step: 6720, train/learning_rate: 4.200380863039754e-05
Step: 6720, train/epoch: 1.5992385149002075
Step: 6730, train/loss: 0.025699999183416367
Step: 6730, train/grad_norm: 0.339832603931427
Step: 6730, train/learning_rate: 4.1991908801719546e-05
Step: 6730, train/epoch: 1.6016182899475098
Step: 6740, train/loss: 0.16509999334812164
Step: 6740, train/grad_norm: 6.674767017364502
Step: 6740, train/learning_rate: 4.198000897304155e-05
Step: 6740, train/epoch: 1.603998064994812
Step: 6750, train/loss: 0.6169000267982483
Step: 6750, train/grad_norm: 24.61233901977539
Step: 6750, train/learning_rate: 4.196810914436355e-05
Step: 6750, train/epoch: 1.6063779592514038
Step: 6760, train/loss: 0.4016999900341034
Step: 6760, train/grad_norm: 10.61369514465332
Step: 6760, train/learning_rate: 4.195621295366436e-05
Step: 6760, train/epoch: 1.608757734298706
Step: 6770, train/loss: 0.23229999840259552
Step: 6770, train/grad_norm: 5.960363388061523
Step: 6770, train/learning_rate: 4.1944313124986365e-05
Step: 6770, train/epoch: 1.6111375093460083
Step: 6780, train/loss: 0.08959999680519104
Step: 6780, train/grad_norm: 0.07434088736772537
Step: 6780, train/learning_rate: 4.193241329630837e-05
Step: 6780, train/epoch: 1.6135174036026
Step: 6790, train/loss: 0.16359999775886536
Step: 6790, train/grad_norm: 0.503170371055603
Step: 6790, train/learning_rate: 4.192051346763037e-05
Step: 6790, train/epoch: 1.6158971786499023
Step: 6800, train/loss: 0.11069999635219574
Step: 6800, train/grad_norm: 10.97026538848877
Step: 6800, train/learning_rate: 4.1908613638952374e-05
Step: 6800, train/epoch: 1.6182769536972046
Step: 6810, train/loss: 0.2752000093460083
Step: 6810, train/grad_norm: 6.173064708709717
Step: 6810, train/learning_rate: 4.1896717448253185e-05
Step: 6810, train/epoch: 1.6206568479537964
Step: 6820, train/loss: 0.11079999804496765
Step: 6820, train/grad_norm: 0.18748289346694946
Step: 6820, train/learning_rate: 4.188481761957519e-05
Step: 6820, train/epoch: 1.6230366230010986
Step: 6830, train/loss: 0.12780000269412994
Step: 6830, train/grad_norm: 0.035749562084674835
Step: 6830, train/learning_rate: 4.187291779089719e-05
Step: 6830, train/epoch: 1.6254165172576904
Step: 6840, train/loss: 0.27649998664855957
Step: 6840, train/grad_norm: 0.05309784412384033
Step: 6840, train/learning_rate: 4.1861017962219194e-05
Step: 6840, train/epoch: 1.6277962923049927
Step: 6850, train/loss: 0.19120000302791595
Step: 6850, train/grad_norm: 15.014036178588867
Step: 6850, train/learning_rate: 4.1849118133541197e-05
Step: 6850, train/epoch: 1.630176067352295
Step: 6860, train/loss: 0.1770000010728836
Step: 6860, train/grad_norm: 2.1700141429901123
Step: 6860, train/learning_rate: 4.183722194284201e-05
Step: 6860, train/epoch: 1.6325559616088867
Step: 6870, train/loss: 0.2483000010251999
Step: 6870, train/grad_norm: 0.1752844601869583
Step: 6870, train/learning_rate: 4.182532211416401e-05
Step: 6870, train/epoch: 1.634935736656189
Step: 6880, train/loss: 0.42260000109672546
Step: 6880, train/grad_norm: 33.50888442993164
Step: 6880, train/learning_rate: 4.181342228548601e-05
Step: 6880, train/epoch: 1.6373155117034912
Step: 6890, train/loss: 0.2281000018119812
Step: 6890, train/grad_norm: 0.6959161758422852
Step: 6890, train/learning_rate: 4.1801522456808016e-05
Step: 6890, train/epoch: 1.639695405960083
Step: 6900, train/loss: 0.10350000113248825
Step: 6900, train/grad_norm: 2.0345590114593506
Step: 6900, train/learning_rate: 4.178962262813002e-05
Step: 6900, train/epoch: 1.6420751810073853
Step: 6910, train/loss: 0.08959999680519104
Step: 6910, train/grad_norm: 11.497814178466797
Step: 6910, train/learning_rate: 4.177772643743083e-05
Step: 6910, train/epoch: 1.644455075263977
Step: 6920, train/loss: 0.12880000472068787
Step: 6920, train/grad_norm: 9.942974090576172
Step: 6920, train/learning_rate: 4.176582660875283e-05
Step: 6920, train/epoch: 1.6468348503112793
Step: 6930, train/loss: 0.26159998774528503
Step: 6930, train/grad_norm: 0.045036882162094116
Step: 6930, train/learning_rate: 4.1753926780074835e-05
Step: 6930, train/epoch: 1.6492146253585815
Step: 6940, train/loss: 0.061500001698732376
Step: 6940, train/grad_norm: 12.953742027282715
Step: 6940, train/learning_rate: 4.174202695139684e-05
Step: 6940, train/epoch: 1.6515945196151733
Step: 6950, train/loss: 0.2955000102519989
Step: 6950, train/grad_norm: 15.71220874786377
Step: 6950, train/learning_rate: 4.173012712271884e-05
Step: 6950, train/epoch: 1.6539742946624756
Step: 6960, train/loss: 0.20819999277591705
Step: 6960, train/grad_norm: 3.527486562728882
Step: 6960, train/learning_rate: 4.171823093201965e-05
Step: 6960, train/epoch: 1.6563540697097778
Step: 6970, train/loss: 0.36169999837875366
Step: 6970, train/grad_norm: 9.600847244262695
Step: 6970, train/learning_rate: 4.1706331103341654e-05
Step: 6970, train/epoch: 1.6587339639663696
Step: 6980, train/loss: 0.219200000166893
Step: 6980, train/grad_norm: 1.894966959953308
Step: 6980, train/learning_rate: 4.169443127466366e-05
Step: 6980, train/epoch: 1.6611137390136719
Step: 6990, train/loss: 0.10080000013113022
Step: 6990, train/grad_norm: 0.09526081383228302
Step: 6990, train/learning_rate: 4.168253144598566e-05
Step: 6990, train/epoch: 1.6634936332702637
Step: 7000, train/loss: 0.21070000529289246
Step: 7000, train/grad_norm: 3.422034740447998
Step: 7000, train/learning_rate: 4.167063161730766e-05
Step: 7000, train/epoch: 1.665873408317566
Step: 7010, train/loss: 0.1590999960899353
Step: 7010, train/grad_norm: 7.149020671844482
Step: 7010, train/learning_rate: 4.165873542660847e-05
Step: 7010, train/epoch: 1.6682531833648682
Step: 7020, train/loss: 0.3337000012397766
Step: 7020, train/grad_norm: 13.977920532226562
Step: 7020, train/learning_rate: 4.1646835597930476e-05
Step: 7020, train/epoch: 1.67063307762146
Step: 7030, train/loss: 0.3151000142097473
Step: 7030, train/grad_norm: 9.23664379119873
Step: 7030, train/learning_rate: 4.163493576925248e-05
Step: 7030, train/epoch: 1.6730128526687622
Step: 7040, train/loss: 0.28040000796318054
Step: 7040, train/grad_norm: 4.179283142089844
Step: 7040, train/learning_rate: 4.162303594057448e-05
Step: 7040, train/epoch: 1.6753926277160645
Step: 7050, train/loss: 0.2085999995470047
Step: 7050, train/grad_norm: 12.010698318481445
Step: 7050, train/learning_rate: 4.1611136111896485e-05
Step: 7050, train/epoch: 1.6777725219726562
Step: 7060, train/loss: 0.24709999561309814
Step: 7060, train/grad_norm: 1.0926737785339355
Step: 7060, train/learning_rate: 4.1599239921197295e-05
Step: 7060, train/epoch: 1.6801522970199585
Step: 7070, train/loss: 0.43529999256134033
Step: 7070, train/grad_norm: 14.51181411743164
Step: 7070, train/learning_rate: 4.15873400925193e-05
Step: 7070, train/epoch: 1.6825320720672607
Step: 7080, train/loss: 0.15299999713897705
Step: 7080, train/grad_norm: 6.564089775085449
Step: 7080, train/learning_rate: 4.15754402638413e-05
Step: 7080, train/epoch: 1.6849119663238525
Step: 7090, train/loss: 0.2791999876499176
Step: 7090, train/grad_norm: 15.916224479675293
Step: 7090, train/learning_rate: 4.1563540435163304e-05
Step: 7090, train/epoch: 1.6872917413711548
Step: 7100, train/loss: 0.2160000056028366
Step: 7100, train/grad_norm: 0.10216735303401947
Step: 7100, train/learning_rate: 4.155164060648531e-05
Step: 7100, train/epoch: 1.6896716356277466
Step: 7110, train/loss: 0.2786000072956085
Step: 7110, train/grad_norm: 11.99560260772705
Step: 7110, train/learning_rate: 4.153974441578612e-05
Step: 7110, train/epoch: 1.6920514106750488
Step: 7120, train/loss: 0.1234000027179718
Step: 7120, train/grad_norm: 8.500176429748535
Step: 7120, train/learning_rate: 4.152784458710812e-05
Step: 7120, train/epoch: 1.694431185722351
Step: 7130, train/loss: 0.26010000705718994
Step: 7130, train/grad_norm: 0.4076317250728607
Step: 7130, train/learning_rate: 4.151594475843012e-05
Step: 7130, train/epoch: 1.6968110799789429
Step: 7140, train/loss: 0.15070000290870667
Step: 7140, train/grad_norm: 0.6271193623542786
Step: 7140, train/learning_rate: 4.1504044929752126e-05
Step: 7140, train/epoch: 1.6991908550262451
Step: 7150, train/loss: 0.20309999585151672
Step: 7150, train/grad_norm: 0.1052083745598793
Step: 7150, train/learning_rate: 4.149214510107413e-05
Step: 7150, train/epoch: 1.7015706300735474
Step: 7160, train/loss: 0.2574000060558319
Step: 7160, train/grad_norm: 13.25474739074707
Step: 7160, train/learning_rate: 4.148024891037494e-05
Step: 7160, train/epoch: 1.7039505243301392
Step: 7170, train/loss: 0.07859999686479568
Step: 7170, train/grad_norm: 6.1634087562561035
Step: 7170, train/learning_rate: 4.146834908169694e-05
Step: 7170, train/epoch: 1.7063302993774414
Step: 7180, train/loss: 0.09099999815225601
Step: 7180, train/grad_norm: 5.004995822906494
Step: 7180, train/learning_rate: 4.1456449253018945e-05
Step: 7180, train/epoch: 1.7087101936340332
Step: 7190, train/loss: 0.08129999786615372
Step: 7190, train/grad_norm: 0.37846627831459045
Step: 7190, train/learning_rate: 4.144454942434095e-05
Step: 7190, train/epoch: 1.7110899686813354
Step: 7200, train/loss: 0.30550000071525574
Step: 7200, train/grad_norm: 33.97346115112305
Step: 7200, train/learning_rate: 4.143264959566295e-05
Step: 7200, train/epoch: 1.7134697437286377
Step: 7210, train/loss: 0.28619998693466187
Step: 7210, train/grad_norm: 9.70316219329834
Step: 7210, train/learning_rate: 4.142075340496376e-05
Step: 7210, train/epoch: 1.7158496379852295
Step: 7220, train/loss: 0.25209999084472656
Step: 7220, train/grad_norm: 22.39144515991211
Step: 7220, train/learning_rate: 4.1408853576285765e-05
Step: 7220, train/epoch: 1.7182294130325317
Step: 7230, train/loss: 0.31769999861717224
Step: 7230, train/grad_norm: 2.2073097229003906
Step: 7230, train/learning_rate: 4.139695374760777e-05
Step: 7230, train/epoch: 1.720609188079834
Step: 7240, train/loss: 0.15680000185966492
Step: 7240, train/grad_norm: 12.449711799621582
Step: 7240, train/learning_rate: 4.138505391892977e-05
Step: 7240, train/epoch: 1.7229890823364258
Step: 7250, train/loss: 0.15860000252723694
Step: 7250, train/grad_norm: 0.5829247236251831
Step: 7250, train/learning_rate: 4.1373154090251774e-05
Step: 7250, train/epoch: 1.725368857383728
Step: 7260, train/loss: 0.03180000185966492
Step: 7260, train/grad_norm: 1.3619168996810913
Step: 7260, train/learning_rate: 4.1361257899552584e-05
Step: 7260, train/epoch: 1.7277486324310303
Step: 7270, train/loss: 0.15950000286102295
Step: 7270, train/grad_norm: 13.990120887756348
Step: 7270, train/learning_rate: 4.134935807087459e-05
Step: 7270, train/epoch: 1.730128526687622
Step: 7280, train/loss: 0.5109999775886536
Step: 7280, train/grad_norm: 13.157550811767578
Step: 7280, train/learning_rate: 4.133745824219659e-05
Step: 7280, train/epoch: 1.7325083017349243
Step: 7290, train/loss: 0.23549999296665192
Step: 7290, train/grad_norm: 0.14493794739246368
Step: 7290, train/learning_rate: 4.132555841351859e-05
Step: 7290, train/epoch: 1.7348881959915161
Step: 7300, train/loss: 0.2362000048160553
Step: 7300, train/grad_norm: 10.44194221496582
Step: 7300, train/learning_rate: 4.1313658584840596e-05
Step: 7300, train/epoch: 1.7372679710388184
Step: 7310, train/loss: 0.20559999346733093
Step: 7310, train/grad_norm: 10.872554779052734
Step: 7310, train/learning_rate: 4.1301762394141406e-05
Step: 7310, train/epoch: 1.7396477460861206
Step: 7320, train/loss: 0.1404999941587448
Step: 7320, train/grad_norm: 7.590803623199463
Step: 7320, train/learning_rate: 4.128986256546341e-05
Step: 7320, train/epoch: 1.7420276403427124
Step: 7330, train/loss: 0.14000000059604645
Step: 7330, train/grad_norm: 0.42873525619506836
Step: 7330, train/learning_rate: 4.127796273678541e-05
Step: 7330, train/epoch: 1.7444074153900146
Step: 7340, train/loss: 0.20180000364780426
Step: 7340, train/grad_norm: 12.769797325134277
Step: 7340, train/learning_rate: 4.1266062908107415e-05
Step: 7340, train/epoch: 1.746787190437317
Step: 7350, train/loss: 0.2928999960422516
Step: 7350, train/grad_norm: 0.42771074175834656
Step: 7350, train/learning_rate: 4.125416307942942e-05
Step: 7350, train/epoch: 1.7491670846939087
Step: 7360, train/loss: 0.29440000653266907
Step: 7360, train/grad_norm: 1.6273339986801147
Step: 7360, train/learning_rate: 4.124226688873023e-05
Step: 7360, train/epoch: 1.751546859741211
Step: 7370, train/loss: 0.19339999556541443
Step: 7370, train/grad_norm: 2.6044809818267822
Step: 7370, train/learning_rate: 4.123036706005223e-05
Step: 7370, train/epoch: 1.7539267539978027
Step: 7380, train/loss: 0.1664000004529953
Step: 7380, train/grad_norm: 8.96121883392334
Step: 7380, train/learning_rate: 4.1218467231374234e-05
Step: 7380, train/epoch: 1.756306529045105
Step: 7390, train/loss: 0.08250000327825546
Step: 7390, train/grad_norm: 0.04658886790275574
Step: 7390, train/learning_rate: 4.120656740269624e-05
Step: 7390, train/epoch: 1.7586863040924072
Step: 7400, train/loss: 0.08060000091791153
Step: 7400, train/grad_norm: 0.35687634348869324
Step: 7400, train/learning_rate: 4.119466757401824e-05
Step: 7400, train/epoch: 1.761066198348999
Step: 7410, train/loss: 0.11749999970197678
Step: 7410, train/grad_norm: 0.2070690393447876
Step: 7410, train/learning_rate: 4.118277138331905e-05
Step: 7410, train/epoch: 1.7634459733963013
Step: 7420, train/loss: 0.2152000069618225
Step: 7420, train/grad_norm: 1.069836139678955
Step: 7420, train/learning_rate: 4.117087155464105e-05
Step: 7420, train/epoch: 1.7658257484436035
Step: 7430, train/loss: 0.322299987077713
Step: 7430, train/grad_norm: 9.892077445983887
Step: 7430, train/learning_rate: 4.1158971725963056e-05
Step: 7430, train/epoch: 1.7682056427001953
Step: 7440, train/loss: 0.15629999339580536
Step: 7440, train/grad_norm: 4.095987796783447
Step: 7440, train/learning_rate: 4.114707189728506e-05
Step: 7440, train/epoch: 1.7705854177474976
Step: 7450, train/loss: 0.17589999735355377
Step: 7450, train/grad_norm: 10.306754112243652
Step: 7450, train/learning_rate: 4.113517206860706e-05
Step: 7450, train/epoch: 1.7729653120040894
Step: 7460, train/loss: 0.17100000381469727
Step: 7460, train/grad_norm: 3.9735076427459717
Step: 7460, train/learning_rate: 4.112327587790787e-05
Step: 7460, train/epoch: 1.7753450870513916
Step: 7470, train/loss: 0.34279999136924744
Step: 7470, train/grad_norm: 11.75085163116455
Step: 7470, train/learning_rate: 4.1111376049229875e-05
Step: 7470, train/epoch: 1.7777248620986938
Step: 7480, train/loss: 0.1103999987244606
Step: 7480, train/grad_norm: 24.00527572631836
Step: 7480, train/learning_rate: 4.109947622055188e-05
Step: 7480, train/epoch: 1.7801047563552856
Step: 7490, train/loss: 0.12200000137090683
Step: 7490, train/grad_norm: 25.0324649810791
Step: 7490, train/learning_rate: 4.108757639187388e-05
Step: 7490, train/epoch: 1.782484531402588
Step: 7500, train/loss: 0.19830000400543213
Step: 7500, train/grad_norm: 0.07295342534780502
Step: 7500, train/learning_rate: 4.1075676563195884e-05
Step: 7500, train/epoch: 1.7848643064498901
Step: 7510, train/loss: 0.13830000162124634
Step: 7510, train/grad_norm: 7.247035026550293
Step: 7510, train/learning_rate: 4.1063780372496694e-05
Step: 7510, train/epoch: 1.787244200706482
Step: 7520, train/loss: 0.26249998807907104
Step: 7520, train/grad_norm: 18.617748260498047
Step: 7520, train/learning_rate: 4.10518805438187e-05
Step: 7520, train/epoch: 1.7896239757537842
Step: 7530, train/loss: 0.357699990272522
Step: 7530, train/grad_norm: 12.564786911010742
Step: 7530, train/learning_rate: 4.10399807151407e-05
Step: 7530, train/epoch: 1.7920037508010864
Step: 7540, train/loss: 0.08160000294446945
Step: 7540, train/grad_norm: 5.851608753204346
Step: 7540, train/learning_rate: 4.10280808864627e-05
Step: 7540, train/epoch: 1.7943836450576782
Step: 7550, train/loss: 0.19349999725818634
Step: 7550, train/grad_norm: 17.43292808532715
Step: 7550, train/learning_rate: 4.1016181057784706e-05
Step: 7550, train/epoch: 1.7967634201049805
Step: 7560, train/loss: 0.2378000020980835
Step: 7560, train/grad_norm: 19.740951538085938
Step: 7560, train/learning_rate: 4.1004284867085516e-05
Step: 7560, train/epoch: 1.7991433143615723
Step: 7570, train/loss: 0.155799999833107
Step: 7570, train/grad_norm: 0.4395236372947693
Step: 7570, train/learning_rate: 4.099238503840752e-05
Step: 7570, train/epoch: 1.8015230894088745
Step: 7580, train/loss: 0.07829999923706055
Step: 7580, train/grad_norm: 2.1228532791137695
Step: 7580, train/learning_rate: 4.098048520972952e-05
Step: 7580, train/epoch: 1.8039028644561768
Step: 7590, train/loss: 0.09650000184774399
Step: 7590, train/grad_norm: 0.17968401312828064
Step: 7590, train/learning_rate: 4.0968585381051525e-05
Step: 7590, train/epoch: 1.8062827587127686
Step: 7600, train/loss: 0.11420000344514847
Step: 7600, train/grad_norm: 0.09987790137529373
Step: 7600, train/learning_rate: 4.095668555237353e-05
Step: 7600, train/epoch: 1.8086625337600708
Step: 7610, train/loss: 0.329800009727478
Step: 7610, train/grad_norm: 53.907466888427734
Step: 7610, train/learning_rate: 4.094478936167434e-05
Step: 7610, train/epoch: 1.811042308807373
Step: 7620, train/loss: 0.3181999921798706
Step: 7620, train/grad_norm: 0.33800074458122253
Step: 7620, train/learning_rate: 4.093288953299634e-05
Step: 7620, train/epoch: 1.8134222030639648
Step: 7630, train/loss: 0.34439998865127563
Step: 7630, train/grad_norm: 0.17437924444675446
Step: 7630, train/learning_rate: 4.0920989704318345e-05
Step: 7630, train/epoch: 1.815801978111267
Step: 7640, train/loss: 0.07649999856948853
Step: 7640, train/grad_norm: 0.14739304780960083
Step: 7640, train/learning_rate: 4.090908987564035e-05
Step: 7640, train/epoch: 1.8181818723678589
Step: 7650, train/loss: 0.18520000576972961
Step: 7650, train/grad_norm: 8.24493408203125
Step: 7650, train/learning_rate: 4.089719004696235e-05
Step: 7650, train/epoch: 1.8205616474151611
Step: 7660, train/loss: 0.18289999663829803
Step: 7660, train/grad_norm: 7.478830337524414
Step: 7660, train/learning_rate: 4.088529385626316e-05
Step: 7660, train/epoch: 1.8229414224624634
Step: 7670, train/loss: 0.19009999930858612
Step: 7670, train/grad_norm: 3.642956256866455
Step: 7670, train/learning_rate: 4.0873394027585164e-05
Step: 7670, train/epoch: 1.8253213167190552
Step: 7680, train/loss: 0.3037000000476837
Step: 7680, train/grad_norm: 0.007357816677540541
Step: 7680, train/learning_rate: 4.086149419890717e-05
Step: 7680, train/epoch: 1.8277010917663574
Step: 7690, train/loss: 0.11490000039339066
Step: 7690, train/grad_norm: 0.7950226664543152
Step: 7690, train/learning_rate: 4.084959437022917e-05
Step: 7690, train/epoch: 1.8300808668136597
Step: 7700, train/loss: 0.16089999675750732
Step: 7700, train/grad_norm: 10.398919105529785
Step: 7700, train/learning_rate: 4.083769454155117e-05
Step: 7700, train/epoch: 1.8324607610702515
Step: 7710, train/loss: 0.09529999643564224
Step: 7710, train/grad_norm: 0.2376483678817749
Step: 7710, train/learning_rate: 4.082579835085198e-05
Step: 7710, train/epoch: 1.8348405361175537
Step: 7720, train/loss: 0.31139999628067017
Step: 7720, train/grad_norm: 4.809235095977783
Step: 7720, train/learning_rate: 4.0813898522173986e-05
Step: 7720, train/epoch: 1.8372204303741455
Step: 7730, train/loss: 0.13729999959468842
Step: 7730, train/grad_norm: 1.2524782419204712
Step: 7730, train/learning_rate: 4.080199869349599e-05
Step: 7730, train/epoch: 1.8396002054214478
Step: 7740, train/loss: 0.17810000479221344
Step: 7740, train/grad_norm: 0.24075451493263245
Step: 7740, train/learning_rate: 4.079009886481799e-05
Step: 7740, train/epoch: 1.84197998046875
Step: 7750, train/loss: 0.13300000131130219
Step: 7750, train/grad_norm: 1.7948737144470215
Step: 7750, train/learning_rate: 4.07782026741188e-05
Step: 7750, train/epoch: 1.8443598747253418
Step: 7760, train/loss: 0.19290000200271606
Step: 7760, train/grad_norm: 0.19068211317062378
Step: 7760, train/learning_rate: 4.0766302845440805e-05
Step: 7760, train/epoch: 1.846739649772644
Step: 7770, train/loss: 0.12189999967813492
Step: 7770, train/grad_norm: 23.007835388183594
Step: 7770, train/learning_rate: 4.075440301676281e-05
Step: 7770, train/epoch: 1.8491194248199463
Step: 7780, train/loss: 0.11410000175237656
Step: 7780, train/grad_norm: 8.665885925292969
Step: 7780, train/learning_rate: 4.074250318808481e-05
Step: 7780, train/epoch: 1.851499319076538
Step: 7790, train/loss: 0.33959999680519104
Step: 7790, train/grad_norm: 1.5333493947982788
Step: 7790, train/learning_rate: 4.0730603359406814e-05
Step: 7790, train/epoch: 1.8538790941238403
Step: 7800, train/loss: 0.24199999868869781
Step: 7800, train/grad_norm: 6.356671333312988
Step: 7800, train/learning_rate: 4.0718707168707624e-05
Step: 7800, train/epoch: 1.8562588691711426
Step: 7810, train/loss: 0.2572000026702881
Step: 7810, train/grad_norm: 23.279945373535156
Step: 7810, train/learning_rate: 4.070680734002963e-05
Step: 7810, train/epoch: 1.8586387634277344
Step: 7820, train/loss: 0.12890000641345978
Step: 7820, train/grad_norm: 0.06181085854768753
Step: 7820, train/learning_rate: 4.069490751135163e-05
Step: 7820, train/epoch: 1.8610185384750366
Step: 7830, train/loss: 0.1979999989271164
Step: 7830, train/grad_norm: 0.17681045830249786
Step: 7830, train/learning_rate: 4.068300768267363e-05
Step: 7830, train/epoch: 1.8633984327316284
Step: 7840, train/loss: 0.1876000016927719
Step: 7840, train/grad_norm: 2.566235065460205
Step: 7840, train/learning_rate: 4.0671107853995636e-05
Step: 7840, train/epoch: 1.8657782077789307
Step: 7850, train/loss: 0.25769999623298645
Step: 7850, train/grad_norm: 0.08034396171569824
Step: 7850, train/learning_rate: 4.0659211663296446e-05
Step: 7850, train/epoch: 1.868157982826233
Step: 7860, train/loss: 0.0649000033736229
Step: 7860, train/grad_norm: 3.4117817878723145
Step: 7860, train/learning_rate: 4.064731183461845e-05
Step: 7860, train/epoch: 1.8705378770828247
Step: 7870, train/loss: 0.11760000139474869
Step: 7870, train/grad_norm: 0.13051599264144897
Step: 7870, train/learning_rate: 4.063541200594045e-05
Step: 7870, train/epoch: 1.872917652130127
Step: 7880, train/loss: 0.11190000176429749
Step: 7880, train/grad_norm: 20.422630310058594
Step: 7880, train/learning_rate: 4.0623512177262455e-05
Step: 7880, train/epoch: 1.8752974271774292
Step: 7890, train/loss: 0.14869999885559082
Step: 7890, train/grad_norm: 16.71771812438965
Step: 7890, train/learning_rate: 4.061161234858446e-05
Step: 7890, train/epoch: 1.877677321434021
Step: 7900, train/loss: 0.093299999833107
Step: 7900, train/grad_norm: 15.451011657714844
Step: 7900, train/learning_rate: 4.059971615788527e-05
Step: 7900, train/epoch: 1.8800570964813232
Step: 7910, train/loss: 0.3758000135421753
Step: 7910, train/grad_norm: 27.73299217224121
Step: 7910, train/learning_rate: 4.058781632920727e-05
Step: 7910, train/epoch: 1.882436990737915
Step: 7920, train/loss: 0.2206999957561493
Step: 7920, train/grad_norm: 0.015039193443953991
Step: 7920, train/learning_rate: 4.0575916500529274e-05
Step: 7920, train/epoch: 1.8848167657852173
Step: 7930, train/loss: 0.35249999165534973
Step: 7930, train/grad_norm: 15.092041969299316
Step: 7930, train/learning_rate: 4.056401667185128e-05
Step: 7930, train/epoch: 1.8871965408325195
Step: 7940, train/loss: 0.11259999871253967
Step: 7940, train/grad_norm: 15.738245010375977
Step: 7940, train/learning_rate: 4.055211684317328e-05
Step: 7940, train/epoch: 1.8895764350891113
Step: 7950, train/loss: 0.24940000474452972
Step: 7950, train/grad_norm: 3.761751651763916
Step: 7950, train/learning_rate: 4.054022065247409e-05
Step: 7950, train/epoch: 1.8919562101364136
Step: 7960, train/loss: 0.1429000049829483
Step: 7960, train/grad_norm: 8.96530532836914
Step: 7960, train/learning_rate: 4.0528320823796093e-05
Step: 7960, train/epoch: 1.8943359851837158
Step: 7970, train/loss: 0.33970001339912415
Step: 7970, train/grad_norm: 23.103158950805664
Step: 7970, train/learning_rate: 4.0516420995118096e-05
Step: 7970, train/epoch: 1.8967158794403076
Step: 7980, train/loss: 0.5131000280380249
Step: 7980, train/grad_norm: 15.766802787780762
Step: 7980, train/learning_rate: 4.05045211664401e-05
Step: 7980, train/epoch: 1.8990956544876099
Step: 7990, train/loss: 0.09009999781847
Step: 7990, train/grad_norm: 5.233661651611328
Step: 7990, train/learning_rate: 4.04926213377621e-05
Step: 7990, train/epoch: 1.901475429534912
Step: 8000, train/loss: 0.2632000148296356
Step: 8000, train/grad_norm: 5.921225547790527
Step: 8000, train/learning_rate: 4.048072514706291e-05
Step: 8000, train/epoch: 1.903855323791504
Step: 8010, train/loss: 0.3041999936103821
Step: 8010, train/grad_norm: 4.806313514709473
Step: 8010, train/learning_rate: 4.0468825318384916e-05
Step: 8010, train/epoch: 1.9062350988388062
Step: 8020, train/loss: 0.14419999718666077
Step: 8020, train/grad_norm: 1.5160870552062988
Step: 8020, train/learning_rate: 4.045692548970692e-05
Step: 8020, train/epoch: 1.908614993095398
Step: 8030, train/loss: 0.1453000009059906
Step: 8030, train/grad_norm: 0.30471399426460266
Step: 8030, train/learning_rate: 4.044502566102892e-05
Step: 8030, train/epoch: 1.9109947681427002
Step: 8040, train/loss: 0.3531999886035919
Step: 8040, train/grad_norm: 7.381572723388672
Step: 8040, train/learning_rate: 4.0433125832350925e-05
Step: 8040, train/epoch: 1.9133745431900024
Step: 8050, train/loss: 0.210099995136261
Step: 8050, train/grad_norm: 11.003212928771973
Step: 8050, train/learning_rate: 4.0421229641651735e-05
Step: 8050, train/epoch: 1.9157544374465942
Step: 8060, train/loss: 0.16140000522136688
Step: 8060, train/grad_norm: 0.5339863896369934
Step: 8060, train/learning_rate: 4.040932981297374e-05
Step: 8060, train/epoch: 1.9181342124938965
Step: 8070, train/loss: 0.2547999918460846
Step: 8070, train/grad_norm: 21.265825271606445
Step: 8070, train/learning_rate: 4.039742998429574e-05
Step: 8070, train/epoch: 1.9205139875411987
Step: 8080, train/loss: 0.19750000536441803
Step: 8080, train/grad_norm: 4.047096252441406
Step: 8080, train/learning_rate: 4.0385530155617744e-05
Step: 8080, train/epoch: 1.9228938817977905
Step: 8090, train/loss: 0.225600004196167
Step: 8090, train/grad_norm: 13.880659103393555
Step: 8090, train/learning_rate: 4.037363032693975e-05
Step: 8090, train/epoch: 1.9252736568450928
Step: 8100, train/loss: 0.181099995970726
Step: 8100, train/grad_norm: 0.6905409097671509
Step: 8100, train/learning_rate: 4.036173413624056e-05
Step: 8100, train/epoch: 1.9276535511016846
Step: 8110, train/loss: 0.08009999990463257
Step: 8110, train/grad_norm: 9.373866081237793
Step: 8110, train/learning_rate: 4.034983430756256e-05
Step: 8110, train/epoch: 1.9300333261489868
Step: 8120, train/loss: 0.19740000367164612
Step: 8120, train/grad_norm: 0.18996986746788025
Step: 8120, train/learning_rate: 4.033793447888456e-05
Step: 8120, train/epoch: 1.932413101196289
Step: 8130, train/loss: 0.2084999978542328
Step: 8130, train/grad_norm: 10.601520538330078
Step: 8130, train/learning_rate: 4.0326034650206566e-05
Step: 8130, train/epoch: 1.9347929954528809
Step: 8140, train/loss: 0.14560000598430634
Step: 8140, train/grad_norm: 0.18913866579532623
Step: 8140, train/learning_rate: 4.031413482152857e-05
Step: 8140, train/epoch: 1.937172770500183
Step: 8150, train/loss: 0.17299999296665192
Step: 8150, train/grad_norm: 12.252930641174316
Step: 8150, train/learning_rate: 4.030223863082938e-05
Step: 8150, train/epoch: 1.9395525455474854
Step: 8160, train/loss: 0.06800000369548798
Step: 8160, train/grad_norm: 0.582852303981781
Step: 8160, train/learning_rate: 4.029033880215138e-05
Step: 8160, train/epoch: 1.9419324398040771
Step: 8170, train/loss: 0.2386000007390976
Step: 8170, train/grad_norm: 1.5901885032653809
Step: 8170, train/learning_rate: 4.0278438973473385e-05
Step: 8170, train/epoch: 1.9443122148513794
Step: 8180, train/loss: 0.1459999978542328
Step: 8180, train/grad_norm: 2.2762370109558105
Step: 8180, train/learning_rate: 4.026653914479539e-05
Step: 8180, train/epoch: 1.9466921091079712
Step: 8190, train/loss: 0.13539999723434448
Step: 8190, train/grad_norm: 1.2396084070205688
Step: 8190, train/learning_rate: 4.025463931611739e-05
Step: 8190, train/epoch: 1.9490718841552734
Step: 8200, train/loss: 0.2143000066280365
Step: 8200, train/grad_norm: 22.357131958007812
Step: 8200, train/learning_rate: 4.02427431254182e-05
Step: 8200, train/epoch: 1.9514516592025757
Step: 8210, train/loss: 0.032499998807907104
Step: 8210, train/grad_norm: 0.15115348994731903
Step: 8210, train/learning_rate: 4.0230843296740204e-05
Step: 8210, train/epoch: 1.9538315534591675
Step: 8220, train/loss: 0.2483000010251999
Step: 8220, train/grad_norm: 16.360578536987305
Step: 8220, train/learning_rate: 4.021894346806221e-05
Step: 8220, train/epoch: 1.9562113285064697
Step: 8230, train/loss: 0.09790000319480896
Step: 8230, train/grad_norm: 27.18042755126953
Step: 8230, train/learning_rate: 4.020704363938421e-05
Step: 8230, train/epoch: 1.958591103553772
Step: 8240, train/loss: 0.08669999986886978
Step: 8240, train/grad_norm: 18.999536514282227
Step: 8240, train/learning_rate: 4.019514381070621e-05
Step: 8240, train/epoch: 1.9609709978103638
Step: 8250, train/loss: 0.2599000036716461
Step: 8250, train/grad_norm: 15.46550178527832
Step: 8250, train/learning_rate: 4.018324762000702e-05
Step: 8250, train/epoch: 1.963350772857666
Step: 8260, train/loss: 0.38449999690055847
Step: 8260, train/grad_norm: 11.087540626525879
Step: 8260, train/learning_rate: 4.0171347791329026e-05
Step: 8260, train/epoch: 1.9657305479049683
Step: 8270, train/loss: 0.06889999657869339
Step: 8270, train/grad_norm: 0.3815716505050659
Step: 8270, train/learning_rate: 4.015944796265103e-05
Step: 8270, train/epoch: 1.96811044216156
Step: 8280, train/loss: 0.3732999861240387
Step: 8280, train/grad_norm: 4.23469352722168
Step: 8280, train/learning_rate: 4.014754813397303e-05
Step: 8280, train/epoch: 1.9704902172088623
Step: 8290, train/loss: 0.09149999916553497
Step: 8290, train/grad_norm: 0.08216333389282227
Step: 8290, train/learning_rate: 4.0135648305295035e-05
Step: 8290, train/epoch: 1.972870111465454
Step: 8300, train/loss: 0.18960000574588776
Step: 8300, train/grad_norm: 9.031458854675293
Step: 8300, train/learning_rate: 4.0123752114595845e-05
Step: 8300, train/epoch: 1.9752498865127563
Step: 8310, train/loss: 0.1915999948978424
Step: 8310, train/grad_norm: 0.0677139014005661
Step: 8310, train/learning_rate: 4.011185228591785e-05
Step: 8310, train/epoch: 1.9776296615600586
Step: 8320, train/loss: 0.326200008392334
Step: 8320, train/grad_norm: 0.05025738850235939
Step: 8320, train/learning_rate: 4.009995245723985e-05
Step: 8320, train/epoch: 1.9800095558166504
Step: 8330, train/loss: 0.23579999804496765
Step: 8330, train/grad_norm: 16.52101707458496
Step: 8330, train/learning_rate: 4.0088052628561854e-05
Step: 8330, train/epoch: 1.9823893308639526
Step: 8340, train/loss: 0.10819999873638153
Step: 8340, train/grad_norm: 1.842186689376831
Step: 8340, train/learning_rate: 4.007615279988386e-05
Step: 8340, train/epoch: 1.9847691059112549
Step: 8350, train/loss: 0.2513999938964844
Step: 8350, train/grad_norm: 0.11901085823774338
Step: 8350, train/learning_rate: 4.006425660918467e-05
Step: 8350, train/epoch: 1.9871490001678467
Step: 8360, train/loss: 0.17900000512599945
Step: 8360, train/grad_norm: 0.850003182888031
Step: 8360, train/learning_rate: 4.005235678050667e-05
Step: 8360, train/epoch: 1.989528775215149
Step: 8370, train/loss: 0.19629999995231628
Step: 8370, train/grad_norm: 7.790121555328369
Step: 8370, train/learning_rate: 4.0040456951828673e-05
Step: 8370, train/epoch: 1.9919086694717407
Step: 8380, train/loss: 0.14589999616146088
Step: 8380, train/grad_norm: 8.1483154296875
Step: 8380, train/learning_rate: 4.0028557123150676e-05
Step: 8380, train/epoch: 1.994288444519043
Step: 8390, train/loss: 0.11649999767541885
Step: 8390, train/grad_norm: 1.6927990913391113
Step: 8390, train/learning_rate: 4.001665729447268e-05
Step: 8390, train/epoch: 1.9966682195663452
Step: 8400, train/loss: 0.30869999527931213
Step: 8400, train/grad_norm: 0.4198606014251709
Step: 8400, train/learning_rate: 4.000476110377349e-05
Step: 8400, train/epoch: 1.999048113822937
Step: 8404, eval/loss: 0.19223228096961975
Step: 8404, eval/accuracy: 0.9355823993682861
Step: 8404, eval/f1: 0.932584285736084
Step: 8404, eval/runtime: 297.23858642578125
Step: 8404, eval/samples_per_second: 24.232999801635742
Step: 8404, eval/steps_per_second: 3.0309998989105225
Step: 8404, train/epoch: 2.0
Step: 8410, train/loss: 0.36469998955726624
Step: 8410, train/grad_norm: 5.347321510314941
Step: 8410, train/learning_rate: 3.999286127509549e-05
Step: 8410, train/epoch: 2.0014278888702393
Step: 8420, train/loss: 0.20409999787807465
Step: 8420, train/grad_norm: 17.748939514160156
Step: 8420, train/learning_rate: 3.9980961446417496e-05
Step: 8420, train/epoch: 2.003807783126831
Step: 8430, train/loss: 0.1988999992609024
Step: 8430, train/grad_norm: 8.655216217041016
Step: 8430, train/learning_rate: 3.99690616177395e-05
Step: 8430, train/epoch: 2.0061874389648438
Step: 8440, train/loss: 0.1404999941587448
Step: 8440, train/grad_norm: 7.509321689605713
Step: 8440, train/learning_rate: 3.99571617890615e-05
Step: 8440, train/epoch: 2.0085673332214355
Step: 8450, train/loss: 0.27309998869895935
Step: 8450, train/grad_norm: 0.3896409869194031
Step: 8450, train/learning_rate: 3.994526559836231e-05
Step: 8450, train/epoch: 2.0109472274780273
Step: 8460, train/loss: 0.05849999934434891
Step: 8460, train/grad_norm: 1.1042461395263672
Step: 8460, train/learning_rate: 3.9933365769684315e-05
Step: 8460, train/epoch: 2.01332688331604
Step: 8470, train/loss: 0.16300000250339508
Step: 8470, train/grad_norm: 0.1828317642211914
Step: 8470, train/learning_rate: 3.992146594100632e-05
Step: 8470, train/epoch: 2.015706777572632
Step: 8480, train/loss: 0.0982000008225441
Step: 8480, train/grad_norm: 2.8190672397613525
Step: 8480, train/learning_rate: 3.990956611232832e-05
Step: 8480, train/epoch: 2.0180866718292236
Step: 8490, train/loss: 0.09520000219345093
Step: 8490, train/grad_norm: 0.40492933988571167
Step: 8490, train/learning_rate: 3.9897666283650324e-05
Step: 8490, train/epoch: 2.0204663276672363
Step: 8500, train/loss: 0.010499999858438969
Step: 8500, train/grad_norm: 0.05100320652127266
Step: 8500, train/learning_rate: 3.9885770092951134e-05
Step: 8500, train/epoch: 2.022846221923828
Step: 8510, train/loss: 0.08169999718666077
Step: 8510, train/grad_norm: 10.766501426696777
Step: 8510, train/learning_rate: 3.987387026427314e-05
Step: 8510, train/epoch: 2.02522611618042
Step: 8520, train/loss: 0.021800000220537186
Step: 8520, train/grad_norm: 0.3355071246623993
Step: 8520, train/learning_rate: 3.986197043559514e-05
Step: 8520, train/epoch: 2.0276060104370117
Step: 8530, train/loss: 0.25220000743865967
Step: 8530, train/grad_norm: 17.399169921875
Step: 8530, train/learning_rate: 3.985007060691714e-05
Step: 8530, train/epoch: 2.0299856662750244
Step: 8540, train/loss: 0.1581999957561493
Step: 8540, train/grad_norm: 0.23557379841804504
Step: 8540, train/learning_rate: 3.9838170778239146e-05
Step: 8540, train/epoch: 2.032365560531616
Step: 8550, train/loss: 0.08269999921321869
Step: 8550, train/grad_norm: 3.6435546875
Step: 8550, train/learning_rate: 3.9826274587539956e-05
Step: 8550, train/epoch: 2.034745454788208
Step: 8560, train/loss: 0.1080000028014183
Step: 8560, train/grad_norm: 0.016159486025571823
Step: 8560, train/learning_rate: 3.981437475886196e-05
Step: 8560, train/epoch: 2.0371251106262207
Step: 8570, train/loss: 0.14329999685287476
Step: 8570, train/grad_norm: 23.465688705444336
Step: 8570, train/learning_rate: 3.980247493018396e-05
Step: 8570, train/epoch: 2.0395050048828125
Step: 8580, train/loss: 0.07639999687671661
Step: 8580, train/grad_norm: 17.687152862548828
Step: 8580, train/learning_rate: 3.9790575101505965e-05
Step: 8580, train/epoch: 2.0418848991394043
Step: 8590, train/loss: 0.1428000032901764
Step: 8590, train/grad_norm: 0.09694775193929672
Step: 8590, train/learning_rate: 3.977867527282797e-05
Step: 8590, train/epoch: 2.044264554977417
Step: 8600, train/loss: 0.33570000529289246
Step: 8600, train/grad_norm: 9.759387016296387
Step: 8600, train/learning_rate: 3.976677908212878e-05
Step: 8600, train/epoch: 2.046644449234009
Step: 8610, train/loss: 0.0658000037074089
Step: 8610, train/grad_norm: 0.10405456274747849
Step: 8610, train/learning_rate: 3.975487925345078e-05
Step: 8610, train/epoch: 2.0490243434906006
Step: 8620, train/loss: 0.06849999725818634
Step: 8620, train/grad_norm: 0.07378136366605759
Step: 8620, train/learning_rate: 3.9742979424772784e-05
Step: 8620, train/epoch: 2.0514039993286133
Step: 8630, train/loss: 0.1597999930381775
Step: 8630, train/grad_norm: 31.563241958618164
Step: 8630, train/learning_rate: 3.973107959609479e-05
Step: 8630, train/epoch: 2.053783893585205
Step: 8640, train/loss: 0.28189998865127563
Step: 8640, train/grad_norm: 0.1701446920633316
Step: 8640, train/learning_rate: 3.971917976741679e-05
Step: 8640, train/epoch: 2.056163787841797
Step: 8650, train/loss: 0.16680000722408295
Step: 8650, train/grad_norm: 0.09676272422075272
Step: 8650, train/learning_rate: 3.97072835767176e-05
Step: 8650, train/epoch: 2.0585434436798096
Step: 8660, train/loss: 0.2775999903678894
Step: 8660, train/grad_norm: 15.272205352783203
Step: 8660, train/learning_rate: 3.96953837480396e-05
Step: 8660, train/epoch: 2.0609233379364014
Step: 8670, train/loss: 0.15320000052452087
Step: 8670, train/grad_norm: 0.19962579011917114
Step: 8670, train/learning_rate: 3.9683483919361606e-05
Step: 8670, train/epoch: 2.063303232192993
Step: 8680, train/loss: 0.1678999960422516
Step: 8680, train/grad_norm: 10.558021545410156
Step: 8680, train/learning_rate: 3.967158409068361e-05
Step: 8680, train/epoch: 2.065683126449585
Step: 8690, train/loss: 0.02160000056028366
Step: 8690, train/grad_norm: 0.2619003355503082
Step: 8690, train/learning_rate: 3.965968426200561e-05
Step: 8690, train/epoch: 2.0680627822875977
Step: 8700, train/loss: 0.05620000138878822
Step: 8700, train/grad_norm: 0.14307129383087158
Step: 8700, train/learning_rate: 3.964778807130642e-05
Step: 8700, train/epoch: 2.0704426765441895
Step: 8710, train/loss: 0.09070000052452087
Step: 8710, train/grad_norm: 0.16687864065170288
Step: 8710, train/learning_rate: 3.9635888242628425e-05
Step: 8710, train/epoch: 2.0728225708007812
Step: 8720, train/loss: 0.10490000247955322
Step: 8720, train/grad_norm: 0.045674920082092285
Step: 8720, train/learning_rate: 3.962398841395043e-05
Step: 8720, train/epoch: 2.075202226638794
Step: 8730, train/loss: 0.007300000172108412
Step: 8730, train/grad_norm: 0.04256194457411766
Step: 8730, train/learning_rate: 3.961208858527243e-05
Step: 8730, train/epoch: 2.0775821208953857
Step: 8740, train/loss: 0.2770000100135803
Step: 8740, train/grad_norm: 20.751462936401367
Step: 8740, train/learning_rate: 3.9600188756594434e-05
Step: 8740, train/epoch: 2.0799620151519775
Step: 8750, train/loss: 0.3458999991416931
Step: 8750, train/grad_norm: 3.75412654876709
Step: 8750, train/learning_rate: 3.9588292565895244e-05
Step: 8750, train/epoch: 2.0823416709899902
Step: 8760, train/loss: 0.039900001138448715
Step: 8760, train/grad_norm: 7.816598892211914
Step: 8760, train/learning_rate: 3.957639273721725e-05
Step: 8760, train/epoch: 2.084721565246582
Step: 8770, train/loss: 0.21389999985694885
Step: 8770, train/grad_norm: 0.01998843066394329
Step: 8770, train/learning_rate: 3.956449290853925e-05
Step: 8770, train/epoch: 2.087101459503174
Step: 8780, train/loss: 0.15950000286102295
Step: 8780, train/grad_norm: 0.003934511449187994
Step: 8780, train/learning_rate: 3.9552593079861253e-05
Step: 8780, train/epoch: 2.0894811153411865
Step: 8790, train/loss: 0.2946000099182129
Step: 8790, train/grad_norm: 29.870336532592773
Step: 8790, train/learning_rate: 3.9540693251183257e-05
Step: 8790, train/epoch: 2.0918610095977783
Step: 8800, train/loss: 0.14970000088214874
Step: 8800, train/grad_norm: 16.032136917114258
Step: 8800, train/learning_rate: 3.9528797060484067e-05
Step: 8800, train/epoch: 2.09424090385437
Step: 8810, train/loss: 0.23890000581741333
Step: 8810, train/grad_norm: 3.0975918769836426
Step: 8810, train/learning_rate: 3.951689723180607e-05
Step: 8810, train/epoch: 2.096620559692383
Step: 8820, train/loss: 0.1679999977350235
Step: 8820, train/grad_norm: 19.337806701660156
Step: 8820, train/learning_rate: 3.950499740312807e-05
Step: 8820, train/epoch: 2.0990004539489746
Step: 8830, train/loss: 0.04179999977350235
Step: 8830, train/grad_norm: 0.009978520683944225
Step: 8830, train/learning_rate: 3.9493097574450076e-05
Step: 8830, train/epoch: 2.1013803482055664
Step: 8840, train/loss: 0.14980000257492065
Step: 8840, train/grad_norm: 0.40037593245506287
Step: 8840, train/learning_rate: 3.948119774577208e-05
Step: 8840, train/epoch: 2.103760004043579
Step: 8850, train/loss: 0.25929999351501465
Step: 8850, train/grad_norm: 0.03542068600654602
Step: 8850, train/learning_rate: 3.946930155507289e-05
Step: 8850, train/epoch: 2.106139898300171
Step: 8860, train/loss: 0.15860000252723694
Step: 8860, train/grad_norm: 4.378743648529053
Step: 8860, train/learning_rate: 3.945740172639489e-05
Step: 8860, train/epoch: 2.1085197925567627
Step: 8870, train/loss: 0.5
Step: 8870, train/grad_norm: 26.853593826293945
Step: 8870, train/learning_rate: 3.9445501897716895e-05
Step: 8870, train/epoch: 2.1108996868133545
Step: 8880, train/loss: 0.13269999623298645
Step: 8880, train/grad_norm: 1.0695443153381348
Step: 8880, train/learning_rate: 3.94336020690389e-05
Step: 8880, train/epoch: 2.113279342651367
Step: 8890, train/loss: 0.258899986743927
Step: 8890, train/grad_norm: 0.014849678613245487
Step: 8890, train/learning_rate: 3.94217022403609e-05
Step: 8890, train/epoch: 2.115659236907959
Step: 8900, train/loss: 0.12690000236034393
Step: 8900, train/grad_norm: 0.23214904963970184
Step: 8900, train/learning_rate: 3.940980604966171e-05
Step: 8900, train/epoch: 2.118039131164551
Step: 8910, train/loss: 0.10220000147819519
Step: 8910, train/grad_norm: 14.408595085144043
Step: 8910, train/learning_rate: 3.9397906220983714e-05
Step: 8910, train/epoch: 2.1204187870025635
Step: 8920, train/loss: 0.3411000072956085
Step: 8920, train/grad_norm: 8.563092231750488
Step: 8920, train/learning_rate: 3.938600639230572e-05
Step: 8920, train/epoch: 2.1227986812591553
Step: 8930, train/loss: 0.2842999994754791
Step: 8930, train/grad_norm: 20.009765625
Step: 8930, train/learning_rate: 3.937410656362772e-05
Step: 8930, train/epoch: 2.125178575515747
Step: 8940, train/loss: 0.20229999721050262
Step: 8940, train/grad_norm: 0.326940655708313
Step: 8940, train/learning_rate: 3.936220673494972e-05
Step: 8940, train/epoch: 2.1275582313537598
Step: 8950, train/loss: 0.053199999034404755
Step: 8950, train/grad_norm: 2.1539862155914307
Step: 8950, train/learning_rate: 3.935031054425053e-05
Step: 8950, train/epoch: 2.1299381256103516
Step: 8960, train/loss: 0.12080000340938568
Step: 8960, train/grad_norm: 8.005207061767578
Step: 8960, train/learning_rate: 3.9338410715572536e-05
Step: 8960, train/epoch: 2.1323180198669434
Step: 8970, train/loss: 0.1379999965429306
Step: 8970, train/grad_norm: 9.993988990783691
Step: 8970, train/learning_rate: 3.932651088689454e-05
Step: 8970, train/epoch: 2.134697675704956
Step: 8980, train/loss: 0.25440001487731934
Step: 8980, train/grad_norm: 11.431239128112793
Step: 8980, train/learning_rate: 3.931461105821654e-05
Step: 8980, train/epoch: 2.137077569961548
Step: 8990, train/loss: 0.09239999949932098
Step: 8990, train/grad_norm: 0.06561053544282913
Step: 8990, train/learning_rate: 3.9302711229538545e-05
Step: 8990, train/epoch: 2.1394574642181396
Step: 9000, train/loss: 0.12150000035762787
Step: 9000, train/grad_norm: 0.002053016796708107
Step: 9000, train/learning_rate: 3.9290815038839355e-05
Step: 9000, train/epoch: 2.1418371200561523
Step: 9010, train/loss: 0.2320999950170517
Step: 9010, train/grad_norm: 0.07597656548023224
Step: 9010, train/learning_rate: 3.927891521016136e-05
Step: 9010, train/epoch: 2.144217014312744
Step: 9020, train/loss: 0.1589999943971634
Step: 9020, train/grad_norm: 0.9021450877189636
Step: 9020, train/learning_rate: 3.926701538148336e-05
Step: 9020, train/epoch: 2.146596908569336
Step: 9030, train/loss: 0.36480000615119934
Step: 9030, train/grad_norm: 30.43967628479004
Step: 9030, train/learning_rate: 3.9255115552805364e-05
Step: 9030, train/epoch: 2.1489765644073486
Step: 9040, train/loss: 0.20340000092983246
Step: 9040, train/grad_norm: 10.529949188232422
Step: 9040, train/learning_rate: 3.924321572412737e-05
Step: 9040, train/epoch: 2.1513564586639404
Step: 9050, train/loss: 0.21870000660419464
Step: 9050, train/grad_norm: 7.8232293128967285
Step: 9050, train/learning_rate: 3.923131953342818e-05
Step: 9050, train/epoch: 2.1537363529205322
Step: 9060, train/loss: 0.12470000237226486
Step: 9060, train/grad_norm: 0.089621901512146
Step: 9060, train/learning_rate: 3.921941970475018e-05
Step: 9060, train/epoch: 2.156116247177124
Step: 9070, train/loss: 0.14000000059604645
Step: 9070, train/grad_norm: 3.687669277191162
Step: 9070, train/learning_rate: 3.920751987607218e-05
Step: 9070, train/epoch: 2.1584959030151367
Step: 9080, train/loss: 0.0803999975323677
Step: 9080, train/grad_norm: 0.4889754056930542
Step: 9080, train/learning_rate: 3.9195620047394186e-05
Step: 9080, train/epoch: 2.1608757972717285
Step: 9090, train/loss: 0.1648000031709671
Step: 9090, train/grad_norm: 0.4991033971309662
Step: 9090, train/learning_rate: 3.918372021871619e-05
Step: 9090, train/epoch: 2.1632556915283203
Step: 9100, train/loss: 0.3059000074863434
Step: 9100, train/grad_norm: 5.363603591918945
Step: 9100, train/learning_rate: 3.9171824028017e-05
Step: 9100, train/epoch: 2.165635347366333
Step: 9110, train/loss: 0.22550000250339508
Step: 9110, train/grad_norm: 22.536128997802734
Step: 9110, train/learning_rate: 3.9159924199339e-05
Step: 9110, train/epoch: 2.168015241622925
Step: 9120, train/loss: 0.12129999697208405
Step: 9120, train/grad_norm: 19.83595085144043
Step: 9120, train/learning_rate: 3.9148024370661005e-05
Step: 9120, train/epoch: 2.1703951358795166
Step: 9130, train/loss: 0.11949999630451202
Step: 9130, train/grad_norm: 0.0391281396150589
Step: 9130, train/learning_rate: 3.913612454198301e-05
Step: 9130, train/epoch: 2.1727747917175293
Step: 9140, train/loss: 0.01140000019222498
Step: 9140, train/grad_norm: 0.01921594887971878
Step: 9140, train/learning_rate: 3.912422835128382e-05
Step: 9140, train/epoch: 2.175154685974121
Step: 9150, train/loss: 0.30379998683929443
Step: 9150, train/grad_norm: 15.333118438720703
Step: 9150, train/learning_rate: 3.911232852260582e-05
Step: 9150, train/epoch: 2.177534580230713
Step: 9160, train/loss: 0.11699999868869781
Step: 9160, train/grad_norm: 21.18401336669922
Step: 9160, train/learning_rate: 3.9100428693927824e-05
Step: 9160, train/epoch: 2.1799142360687256
Step: 9170, train/loss: 0.357699990272522
Step: 9170, train/grad_norm: 26.073780059814453
Step: 9170, train/learning_rate: 3.908852886524983e-05
Step: 9170, train/epoch: 2.1822941303253174
Step: 9180, train/loss: 0.16110000014305115
Step: 9180, train/grad_norm: 15.428705215454102
Step: 9180, train/learning_rate: 3.907662903657183e-05
Step: 9180, train/epoch: 2.184674024581909
Step: 9190, train/loss: 0.2874000072479248
Step: 9190, train/grad_norm: 8.14205265045166
Step: 9190, train/learning_rate: 3.906473284587264e-05
Step: 9190, train/epoch: 2.187053680419922
Step: 9200, train/loss: 0.05889999866485596
Step: 9200, train/grad_norm: 12.171599388122559
Step: 9200, train/learning_rate: 3.9052833017194644e-05
Step: 9200, train/epoch: 2.1894335746765137
Step: 9210, train/loss: 0.21860000491142273
Step: 9210, train/grad_norm: 0.029531329870224
Step: 9210, train/learning_rate: 3.904093318851665e-05
Step: 9210, train/epoch: 2.1918134689331055
Step: 9220, train/loss: 0.2948000133037567
Step: 9220, train/grad_norm: 0.5390483140945435
Step: 9220, train/learning_rate: 3.902903335983865e-05
Step: 9220, train/epoch: 2.194193124771118
Step: 9230, train/loss: 0.08950000256299973
Step: 9230, train/grad_norm: 8.371207237243652
Step: 9230, train/learning_rate: 3.901713353116065e-05
Step: 9230, train/epoch: 2.19657301902771
Step: 9240, train/loss: 0.149399995803833
Step: 9240, train/grad_norm: 8.469013214111328
Step: 9240, train/learning_rate: 3.900523734046146e-05
Step: 9240, train/epoch: 2.1989529132843018
Step: 9250, train/loss: 0.10819999873638153
Step: 9250, train/grad_norm: 5.770468235015869
Step: 9250, train/learning_rate: 3.8993337511783466e-05
Step: 9250, train/epoch: 2.2013328075408936
Step: 9260, train/loss: 0.1251000016927719
Step: 9260, train/grad_norm: 0.4423986077308655
Step: 9260, train/learning_rate: 3.898143768310547e-05
Step: 9260, train/epoch: 2.2037124633789062
Step: 9270, train/loss: 0.2498999983072281
Step: 9270, train/grad_norm: 0.011436102911829948
Step: 9270, train/learning_rate: 3.896953785442747e-05
Step: 9270, train/epoch: 2.206092357635498
Step: 9280, train/loss: 0.14159999787807465
Step: 9280, train/grad_norm: 0.5024543404579163
Step: 9280, train/learning_rate: 3.8957638025749475e-05
Step: 9280, train/epoch: 2.20847225189209
Step: 9290, train/loss: 0.05000000074505806
Step: 9290, train/grad_norm: 1.791064977645874
Step: 9290, train/learning_rate: 3.8945741835050285e-05
Step: 9290, train/epoch: 2.2108519077301025
Step: 9300, train/loss: 0.08540000021457672
Step: 9300, train/grad_norm: 22.460046768188477
Step: 9300, train/learning_rate: 3.893384200637229e-05
Step: 9300, train/epoch: 2.2132318019866943
Step: 9310, train/loss: 0.08799999952316284
Step: 9310, train/grad_norm: 0.06208664923906326
Step: 9310, train/learning_rate: 3.892194217769429e-05
Step: 9310, train/epoch: 2.215611696243286
Step: 9320, train/loss: 0.07020000368356705
Step: 9320, train/grad_norm: 0.03715682774782181
Step: 9320, train/learning_rate: 3.8910042349016294e-05
Step: 9320, train/epoch: 2.217991352081299
Step: 9330, train/loss: 0.08609999716281891
Step: 9330, train/grad_norm: 0.18445414304733276
Step: 9330, train/learning_rate: 3.88981425203383e-05
Step: 9330, train/epoch: 2.2203712463378906
Step: 9340, train/loss: 0.08190000057220459
Step: 9340, train/grad_norm: 13.45713996887207
Step: 9340, train/learning_rate: 3.888624632963911e-05
Step: 9340, train/epoch: 2.2227511405944824
Step: 9350, train/loss: 0.17569999396800995
Step: 9350, train/grad_norm: 2.369981050491333
Step: 9350, train/learning_rate: 3.887434650096111e-05
Step: 9350, train/epoch: 2.225130796432495
Step: 9360, train/loss: 0.3190000057220459
Step: 9360, train/grad_norm: 3.629192352294922
Step: 9360, train/learning_rate: 3.886244667228311e-05
Step: 9360, train/epoch: 2.227510690689087
Step: 9370, train/loss: 0.10350000113248825
Step: 9370, train/grad_norm: 0.0062697455286979675
Step: 9370, train/learning_rate: 3.8850546843605116e-05
Step: 9370, train/epoch: 2.2298905849456787
Step: 9380, train/loss: 0.05079999938607216
Step: 9380, train/grad_norm: 0.013939634896814823
Step: 9380, train/learning_rate: 3.883864701492712e-05
Step: 9380, train/epoch: 2.2322702407836914
Step: 9390, train/loss: 0.3003000020980835
Step: 9390, train/grad_norm: 53.854095458984375
Step: 9390, train/learning_rate: 3.882675082422793e-05
Step: 9390, train/epoch: 2.234650135040283
Step: 9400, train/loss: 0.1890999972820282
Step: 9400, train/grad_norm: 0.0035022241063416004
Step: 9400, train/learning_rate: 3.881485099554993e-05
Step: 9400, train/epoch: 2.237030029296875
Step: 9410, train/loss: 0.0771000012755394
Step: 9410, train/grad_norm: 34.34511184692383
Step: 9410, train/learning_rate: 3.8802951166871935e-05
Step: 9410, train/epoch: 2.239409923553467
Step: 9420, train/loss: 0.2953999936580658
Step: 9420, train/grad_norm: 2.473839282989502
Step: 9420, train/learning_rate: 3.879105133819394e-05
Step: 9420, train/epoch: 2.2417895793914795
Step: 9430, train/loss: 0.02250000089406967
Step: 9430, train/grad_norm: 9.536890029907227
Step: 9430, train/learning_rate: 3.877915150951594e-05
Step: 9430, train/epoch: 2.2441694736480713
Step: 9440, train/loss: 0.10499999672174454
Step: 9440, train/grad_norm: 0.029128776863217354
Step: 9440, train/learning_rate: 3.876725531881675e-05
Step: 9440, train/epoch: 2.246549367904663
Step: 9450, train/loss: 0.04839999973773956
Step: 9450, train/grad_norm: 16.546764373779297
Step: 9450, train/learning_rate: 3.8755355490138754e-05
Step: 9450, train/epoch: 2.248929023742676
Step: 9460, train/loss: 0.11219999939203262
Step: 9460, train/grad_norm: 0.013127000071108341
Step: 9460, train/learning_rate: 3.874345566146076e-05
Step: 9460, train/epoch: 2.2513089179992676
Step: 9470, train/loss: 0.18400000035762787
Step: 9470, train/grad_norm: 0.03283087536692619
Step: 9470, train/learning_rate: 3.873155583278276e-05
Step: 9470, train/epoch: 2.2536888122558594
Step: 9480, train/loss: 0.03139999881386757
Step: 9480, train/grad_norm: 15.480978965759277
Step: 9480, train/learning_rate: 3.871965600410476e-05
Step: 9480, train/epoch: 2.256068468093872
Step: 9490, train/loss: 0.3314000070095062
Step: 9490, train/grad_norm: 0.19235901534557343
Step: 9490, train/learning_rate: 3.870775981340557e-05
Step: 9490, train/epoch: 2.258448362350464
Step: 9500, train/loss: 0.05860000103712082
Step: 9500, train/grad_norm: 0.02930602617561817
Step: 9500, train/learning_rate: 3.8695859984727576e-05
Step: 9500, train/epoch: 2.2608282566070557
Step: 9510, train/loss: 0.3239000141620636
Step: 9510, train/grad_norm: 0.04836014285683632
Step: 9510, train/learning_rate: 3.868396015604958e-05
Step: 9510, train/epoch: 2.2632079124450684
Step: 9520, train/loss: 0.10559999942779541
Step: 9520, train/grad_norm: 0.4372639060020447
Step: 9520, train/learning_rate: 3.867206032737158e-05
Step: 9520, train/epoch: 2.26558780670166
Step: 9530, train/loss: 0.0966000035405159
Step: 9530, train/grad_norm: 0.09518665820360184
Step: 9530, train/learning_rate: 3.8660160498693585e-05
Step: 9530, train/epoch: 2.267967700958252
Step: 9540, train/loss: 0.26159998774528503
Step: 9540, train/grad_norm: 6.3418402671813965
Step: 9540, train/learning_rate: 3.8648264307994395e-05
Step: 9540, train/epoch: 2.2703473567962646
Step: 9550, train/loss: 0.11299999803304672
Step: 9550, train/grad_norm: 0.13265642523765564
Step: 9550, train/learning_rate: 3.86363644793164e-05
Step: 9550, train/epoch: 2.2727272510528564
Step: 9560, train/loss: 0.31290000677108765
Step: 9560, train/grad_norm: 20.551483154296875
Step: 9560, train/learning_rate: 3.86244646506384e-05
Step: 9560, train/epoch: 2.2751071453094482
Step: 9570, train/loss: 0.06840000301599503
Step: 9570, train/grad_norm: 1.1443675756454468
Step: 9570, train/learning_rate: 3.8612564821960405e-05
Step: 9570, train/epoch: 2.277486801147461
Step: 9580, train/loss: 0.1648000031709671
Step: 9580, train/grad_norm: 0.0882524773478508
Step: 9580, train/learning_rate: 3.860066499328241e-05
Step: 9580, train/epoch: 2.2798666954040527
Step: 9590, train/loss: 0.05550000071525574
Step: 9590, train/grad_norm: 0.05378509685397148
Step: 9590, train/learning_rate: 3.858876880258322e-05
Step: 9590, train/epoch: 2.2822465896606445
Step: 9600, train/loss: 0.09889999777078629
Step: 9600, train/grad_norm: 27.452722549438477
Step: 9600, train/learning_rate: 3.857686897390522e-05
Step: 9600, train/epoch: 2.2846264839172363
Step: 9610, train/loss: 0.12800000607967377
Step: 9610, train/grad_norm: 0.44198185205459595
Step: 9610, train/learning_rate: 3.8564969145227224e-05
Step: 9610, train/epoch: 2.287006139755249
Step: 9620, train/loss: 0.1103999987244606
Step: 9620, train/grad_norm: 28.282766342163086
Step: 9620, train/learning_rate: 3.855306931654923e-05
Step: 9620, train/epoch: 2.289386034011841
Step: 9630, train/loss: 0.1860000044107437
Step: 9630, train/grad_norm: 0.005490253213793039
Step: 9630, train/learning_rate: 3.854116948787123e-05
Step: 9630, train/epoch: 2.2917659282684326
Step: 9640, train/loss: 0.10930000245571136
Step: 9640, train/grad_norm: 3.6049342155456543
Step: 9640, train/learning_rate: 3.852927329717204e-05
Step: 9640, train/epoch: 2.2941455841064453
Step: 9650, train/loss: 0.16329999268054962
Step: 9650, train/grad_norm: 0.00694916769862175
Step: 9650, train/learning_rate: 3.851737346849404e-05
Step: 9650, train/epoch: 2.296525478363037
Step: 9660, train/loss: 0.2563000023365021
Step: 9660, train/grad_norm: 28.63014030456543
Step: 9660, train/learning_rate: 3.8505473639816046e-05
Step: 9660, train/epoch: 2.298905372619629
Step: 9670, train/loss: 0.13379999995231628
Step: 9670, train/grad_norm: 7.307464122772217
Step: 9670, train/learning_rate: 3.849357381113805e-05
Step: 9670, train/epoch: 2.3012850284576416
Step: 9680, train/loss: 0.251800000667572
Step: 9680, train/grad_norm: 0.2174614667892456
Step: 9680, train/learning_rate: 3.848167398246005e-05
Step: 9680, train/epoch: 2.3036649227142334
Step: 9690, train/loss: 0.3037000000476837
Step: 9690, train/grad_norm: 23.79142951965332
Step: 9690, train/learning_rate: 3.846977779176086e-05
Step: 9690, train/epoch: 2.306044816970825
Step: 9700, train/loss: 0.3190999925136566
Step: 9700, train/grad_norm: 26.594831466674805
Step: 9700, train/learning_rate: 3.8457877963082865e-05
Step: 9700, train/epoch: 2.308424472808838
Step: 9710, train/loss: 0.21449999511241913
Step: 9710, train/grad_norm: 0.5582966208457947
Step: 9710, train/learning_rate: 3.844597813440487e-05
Step: 9710, train/epoch: 2.3108043670654297
Step: 9720, train/loss: 0.10350000113248825
Step: 9720, train/grad_norm: 0.4351229667663574
Step: 9720, train/learning_rate: 3.843407830572687e-05
Step: 9720, train/epoch: 2.3131842613220215
Step: 9730, train/loss: 0.020999999716877937
Step: 9730, train/grad_norm: 0.6127268671989441
Step: 9730, train/learning_rate: 3.8422178477048874e-05
Step: 9730, train/epoch: 2.315563917160034
Step: 9740, train/loss: 0.25270000100135803
Step: 9740, train/grad_norm: 0.07911932468414307
Step: 9740, train/learning_rate: 3.8410282286349684e-05
Step: 9740, train/epoch: 2.317943811416626
Step: 9750, train/loss: 0.057100001722574234
Step: 9750, train/grad_norm: 3.3287642002105713
Step: 9750, train/learning_rate: 3.839838245767169e-05
Step: 9750, train/epoch: 2.3203237056732178
Step: 9760, train/loss: 0.16050000488758087
Step: 9760, train/grad_norm: 21.19619369506836
Step: 9760, train/learning_rate: 3.838648262899369e-05
Step: 9760, train/epoch: 2.3227033615112305
Step: 9770, train/loss: 0.22990000247955322
Step: 9770, train/grad_norm: 0.07104481011629105
Step: 9770, train/learning_rate: 3.837458280031569e-05
Step: 9770, train/epoch: 2.3250832557678223
Step: 9780, train/loss: 0.08060000091791153
Step: 9780, train/grad_norm: 0.006951869931071997
Step: 9780, train/learning_rate: 3.8362682971637696e-05
Step: 9780, train/epoch: 2.327463150024414
Step: 9790, train/loss: 0.15809999406337738
Step: 9790, train/grad_norm: 0.1459314525127411
Step: 9790, train/learning_rate: 3.8350786780938506e-05
Step: 9790, train/epoch: 2.329843044281006
Step: 9800, train/loss: 0.03739999979734421
Step: 9800, train/grad_norm: 0.009623532183468342
Step: 9800, train/learning_rate: 3.833888695226051e-05
Step: 9800, train/epoch: 2.3322227001190186
Step: 9810, train/loss: 0.00800000037997961
Step: 9810, train/grad_norm: 7.061463356018066
Step: 9810, train/learning_rate: 3.832698712358251e-05
Step: 9810, train/epoch: 2.3346025943756104
Step: 9820, train/loss: 0.17030000686645508
Step: 9820, train/grad_norm: 18.920934677124023
Step: 9820, train/learning_rate: 3.8315087294904515e-05
Step: 9820, train/epoch: 2.336982488632202
Step: 9830, train/loss: 0.12359999865293503
Step: 9830, train/grad_norm: 4.1121745109558105
Step: 9830, train/learning_rate: 3.830318746622652e-05
Step: 9830, train/epoch: 2.339362144470215
Step: 9840, train/loss: 0.04089999943971634
Step: 9840, train/grad_norm: 0.5374040007591248
Step: 9840, train/learning_rate: 3.829129127552733e-05
Step: 9840, train/epoch: 2.3417420387268066
Step: 9850, train/loss: 0.26829999685287476
Step: 9850, train/grad_norm: 12.821476936340332
Step: 9850, train/learning_rate: 3.827939144684933e-05
Step: 9850, train/epoch: 2.3441219329833984
Step: 9860, train/loss: 0.10740000009536743
Step: 9860, train/grad_norm: 19.712480545043945
Step: 9860, train/learning_rate: 3.8267491618171334e-05
Step: 9860, train/epoch: 2.346501588821411
Step: 9870, train/loss: 0.2937999963760376
Step: 9870, train/grad_norm: 23.841829299926758
Step: 9870, train/learning_rate: 3.825559178949334e-05
Step: 9870, train/epoch: 2.348881483078003
Step: 9880, train/loss: 0.21040000021457672
Step: 9880, train/grad_norm: 22.993616104125977
Step: 9880, train/learning_rate: 3.824369196081534e-05
Step: 9880, train/epoch: 2.3512613773345947
Step: 9890, train/loss: 0.03590000048279762
Step: 9890, train/grad_norm: 7.414112091064453
Step: 9890, train/learning_rate: 3.823179577011615e-05
Step: 9890, train/epoch: 2.3536410331726074
Step: 9900, train/loss: 0.14499999582767487
Step: 9900, train/grad_norm: 0.00905629899352789
Step: 9900, train/learning_rate: 3.821989594143815e-05
Step: 9900, train/epoch: 2.356020927429199
Step: 9910, train/loss: 0.0885000005364418
Step: 9910, train/grad_norm: 0.03983538970351219
Step: 9910, train/learning_rate: 3.8207996112760156e-05
Step: 9910, train/epoch: 2.358400821685791
Step: 9920, train/loss: 0.11429999768733978
Step: 9920, train/grad_norm: 24.519123077392578
Step: 9920, train/learning_rate: 3.819609628408216e-05
Step: 9920, train/epoch: 2.3607804775238037
Step: 9930, train/loss: 0.19300000369548798
Step: 9930, train/grad_norm: 0.005986375734210014
Step: 9930, train/learning_rate: 3.818419645540416e-05
Step: 9930, train/epoch: 2.3631603717803955
Step: 9940, train/loss: 0.018400000408291817
Step: 9940, train/grad_norm: 0.0712493509054184
Step: 9940, train/learning_rate: 3.817230026470497e-05
Step: 9940, train/epoch: 2.3655402660369873
Step: 9950, train/loss: 0.13809999823570251
Step: 9950, train/grad_norm: 16.24167823791504
Step: 9950, train/learning_rate: 3.8160400436026976e-05
Step: 9950, train/epoch: 2.367919921875
Step: 9960, train/loss: 0.2061000019311905
Step: 9960, train/grad_norm: 37.39373779296875
Step: 9960, train/learning_rate: 3.814850060734898e-05
Step: 9960, train/epoch: 2.370299816131592
Step: 9970, train/loss: 0.2468000054359436
Step: 9970, train/grad_norm: 28.614517211914062
Step: 9970, train/learning_rate: 3.813660077867098e-05
Step: 9970, train/epoch: 2.3726797103881836
Step: 9980, train/loss: 0.27160000801086426
Step: 9980, train/grad_norm: 3.5148260593414307
Step: 9980, train/learning_rate: 3.8124700949992985e-05
Step: 9980, train/epoch: 2.3750596046447754
Step: 9990, train/loss: 0.2651999890804291
Step: 9990, train/grad_norm: 35.682193756103516
Step: 9990, train/learning_rate: 3.8112804759293795e-05
Step: 9990, train/epoch: 2.377439260482788
Step: 10000, train/loss: 0.0738999992609024
Step: 10000, train/grad_norm: 0.21844449639320374
Step: 10000, train/learning_rate: 3.81009049306158e-05
Step: 10000, train/epoch: 2.37981915473938
Step: 10010, train/loss: 0.15379999577999115
Step: 10010, train/grad_norm: 0.1532798409461975
Step: 10010, train/learning_rate: 3.80890051019378e-05
Step: 10010, train/epoch: 2.3821990489959717
Step: 10020, train/loss: 0.12060000002384186
Step: 10020, train/grad_norm: 2.2681543827056885
Step: 10020, train/learning_rate: 3.8077105273259804e-05
Step: 10020, train/epoch: 2.3845787048339844
Step: 10030, train/loss: 0.1876000016927719
Step: 10030, train/grad_norm: 25.00522232055664
Step: 10030, train/learning_rate: 3.806520544458181e-05
Step: 10030, train/epoch: 2.386958599090576
Step: 10040, train/loss: 0.08649999648332596
Step: 10040, train/grad_norm: 19.748342514038086
Step: 10040, train/learning_rate: 3.805330925388262e-05
Step: 10040, train/epoch: 2.389338493347168
Step: 10050, train/loss: 0.08839999884366989
Step: 10050, train/grad_norm: 26.284578323364258
Step: 10050, train/learning_rate: 3.804140942520462e-05
Step: 10050, train/epoch: 2.3917181491851807
Step: 10060, train/loss: 0.3089999854564667
Step: 10060, train/grad_norm: 8.006499290466309
Step: 10060, train/learning_rate: 3.802950959652662e-05
Step: 10060, train/epoch: 2.3940980434417725
Step: 10070, train/loss: 0.10540000349283218
Step: 10070, train/grad_norm: 0.1442893147468567
Step: 10070, train/learning_rate: 3.8017609767848626e-05
Step: 10070, train/epoch: 2.3964779376983643
Step: 10080, train/loss: 0.06549999862909317
Step: 10080, train/grad_norm: 9.97752571105957
Step: 10080, train/learning_rate: 3.800570993917063e-05
Step: 10080, train/epoch: 2.398857593536377
Step: 10090, train/loss: 0.07169999927282333
Step: 10090, train/grad_norm: 0.5407559275627136
Step: 10090, train/learning_rate: 3.799381374847144e-05
Step: 10090, train/epoch: 2.4012374877929688
Step: 10100, train/loss: 0.1606999933719635
Step: 10100, train/grad_norm: 8.579848289489746
Step: 10100, train/learning_rate: 3.798191391979344e-05
Step: 10100, train/epoch: 2.4036173820495605
Step: 10110, train/loss: 0.09220000356435776
Step: 10110, train/grad_norm: 0.01062149740755558
Step: 10110, train/learning_rate: 3.7970014091115445e-05
Step: 10110, train/epoch: 2.4059970378875732
Step: 10120, train/loss: 0.29809999465942383
Step: 10120, train/grad_norm: 19.017284393310547
Step: 10120, train/learning_rate: 3.795811426243745e-05
Step: 10120, train/epoch: 2.408376932144165
Step: 10130, train/loss: 0.24120000004768372
Step: 10130, train/grad_norm: 16.07455062866211
Step: 10130, train/learning_rate: 3.794621443375945e-05
Step: 10130, train/epoch: 2.410756826400757
Step: 10140, train/loss: 0.08299999684095383
Step: 10140, train/grad_norm: 0.03230517730116844
Step: 10140, train/learning_rate: 3.793431824306026e-05
Step: 10140, train/epoch: 2.4131367206573486
Step: 10150, train/loss: 0.2272000014781952
Step: 10150, train/grad_norm: 0.007100215647369623
Step: 10150, train/learning_rate: 3.7922418414382264e-05
Step: 10150, train/epoch: 2.4155163764953613
Step: 10160, train/loss: 0.08139999955892563
Step: 10160, train/grad_norm: 7.829741954803467
Step: 10160, train/learning_rate: 3.791051858570427e-05
Step: 10160, train/epoch: 2.417896270751953
Step: 10170, train/loss: 0.10769999772310257
Step: 10170, train/grad_norm: 0.015868134796619415
Step: 10170, train/learning_rate: 3.789861875702627e-05
Step: 10170, train/epoch: 2.420276165008545
Step: 10180, train/loss: 0.1152999997138977
Step: 10180, train/grad_norm: 10.191326141357422
Step: 10180, train/learning_rate: 3.788671892834827e-05
Step: 10180, train/epoch: 2.4226558208465576
Step: 10190, train/loss: 0.1753000020980835
Step: 10190, train/grad_norm: 1.2066373825073242
Step: 10190, train/learning_rate: 3.787482273764908e-05
Step: 10190, train/epoch: 2.4250357151031494
Step: 10200, train/loss: 0.006800000090152025
Step: 10200, train/grad_norm: 0.03066370263695717
Step: 10200, train/learning_rate: 3.7862922908971086e-05
Step: 10200, train/epoch: 2.427415609359741
Step: 10210, train/loss: 0.05469999834895134
Step: 10210, train/grad_norm: 16.147428512573242
Step: 10210, train/learning_rate: 3.785102308029309e-05
Step: 10210, train/epoch: 2.429795265197754
Step: 10220, train/loss: 0.11469999700784683
Step: 10220, train/grad_norm: 12.376299858093262
Step: 10220, train/learning_rate: 3.783912325161509e-05
Step: 10220, train/epoch: 2.4321751594543457
Step: 10230, train/loss: 0.13359999656677246
Step: 10230, train/grad_norm: 14.628384590148926
Step: 10230, train/learning_rate: 3.7827223422937095e-05
Step: 10230, train/epoch: 2.4345550537109375
Step: 10240, train/loss: 0.25209999084472656
Step: 10240, train/grad_norm: 15.914741516113281
Step: 10240, train/learning_rate: 3.7815327232237905e-05
Step: 10240, train/epoch: 2.43693470954895
Step: 10250, train/loss: 0.1387999951839447
Step: 10250, train/grad_norm: 7.840354919433594
Step: 10250, train/learning_rate: 3.780342740355991e-05
Step: 10250, train/epoch: 2.439314603805542
Step: 10260, train/loss: 0.26739999651908875
Step: 10260, train/grad_norm: 1.40267014503479
Step: 10260, train/learning_rate: 3.779152757488191e-05
Step: 10260, train/epoch: 2.441694498062134
Step: 10270, train/loss: 0.11320000141859055
Step: 10270, train/grad_norm: 17.188873291015625
Step: 10270, train/learning_rate: 3.7779627746203914e-05
Step: 10270, train/epoch: 2.4440741539001465
Step: 10280, train/loss: 0.24289999902248383
Step: 10280, train/grad_norm: 29.395509719848633
Step: 10280, train/learning_rate: 3.776772791752592e-05
Step: 10280, train/epoch: 2.4464540481567383
Step: 10290, train/loss: 0.11789999902248383
Step: 10290, train/grad_norm: 0.007397809997200966
Step: 10290, train/learning_rate: 3.775583172682673e-05
Step: 10290, train/epoch: 2.44883394241333
Step: 10300, train/loss: 0.11710000038146973
Step: 10300, train/grad_norm: 0.0810033529996872
Step: 10300, train/learning_rate: 3.774393189814873e-05
Step: 10300, train/epoch: 2.4512135982513428
Step: 10310, train/loss: 0.14949999749660492
Step: 10310, train/grad_norm: 20.708629608154297
Step: 10310, train/learning_rate: 3.7732032069470733e-05
Step: 10310, train/epoch: 2.4535934925079346
Step: 10320, train/loss: 0.15469999611377716
Step: 10320, train/grad_norm: 0.2809874713420868
Step: 10320, train/learning_rate: 3.7720132240792736e-05
Step: 10320, train/epoch: 2.4559733867645264
Step: 10330, train/loss: 0.10819999873638153
Step: 10330, train/grad_norm: 2.3739407062530518
Step: 10330, train/learning_rate: 3.770823241211474e-05
Step: 10330, train/epoch: 2.458353281021118
Step: 10340, train/loss: 0.05829999968409538
Step: 10340, train/grad_norm: 0.0024827911984175444
Step: 10340, train/learning_rate: 3.769633622141555e-05
Step: 10340, train/epoch: 2.460732936859131
Step: 10350, train/loss: 0.1647000014781952
Step: 10350, train/grad_norm: 2.6317362785339355
Step: 10350, train/learning_rate: 3.768443639273755e-05
Step: 10350, train/epoch: 2.4631128311157227
Step: 10360, train/loss: 0.08380000293254852
Step: 10360, train/grad_norm: 8.453508377075195
Step: 10360, train/learning_rate: 3.7672536564059556e-05
Step: 10360, train/epoch: 2.4654927253723145
Step: 10370, train/loss: 0.006899999920278788
Step: 10370, train/grad_norm: 3.231735944747925
Step: 10370, train/learning_rate: 3.766063673538156e-05
Step: 10370, train/epoch: 2.467872381210327
Step: 10380, train/loss: 0.12489999830722809
Step: 10380, train/grad_norm: 1.5435343980789185
Step: 10380, train/learning_rate: 3.764873690670356e-05
Step: 10380, train/epoch: 2.470252275466919
Step: 10390, train/loss: 0.2224999964237213
Step: 10390, train/grad_norm: 9.55937671661377
Step: 10390, train/learning_rate: 3.763684071600437e-05
Step: 10390, train/epoch: 2.4726321697235107
Step: 10400, train/loss: 0.26019999384880066
Step: 10400, train/grad_norm: 7.428677082061768
Step: 10400, train/learning_rate: 3.7624940887326375e-05
Step: 10400, train/epoch: 2.4750118255615234
Step: 10410, train/loss: 0.2741999924182892
Step: 10410, train/grad_norm: 17.931577682495117
Step: 10410, train/learning_rate: 3.761304105864838e-05
Step: 10410, train/epoch: 2.4773917198181152
Step: 10420, train/loss: 0.08780000358819962
Step: 10420, train/grad_norm: 4.668248653411865
Step: 10420, train/learning_rate: 3.760114122997038e-05
Step: 10420, train/epoch: 2.479771614074707
Step: 10430, train/loss: 0.13660000264644623
Step: 10430, train/grad_norm: 3.8773534297943115
Step: 10430, train/learning_rate: 3.7589241401292384e-05
Step: 10430, train/epoch: 2.4821512699127197
Step: 10440, train/loss: 0.1193000003695488
Step: 10440, train/grad_norm: 5.1332879066467285
Step: 10440, train/learning_rate: 3.7577345210593194e-05
Step: 10440, train/epoch: 2.4845311641693115
Step: 10450, train/loss: 0.3346000015735626
Step: 10450, train/grad_norm: 10.2103853225708
Step: 10450, train/learning_rate: 3.75654453819152e-05
Step: 10450, train/epoch: 2.4869110584259033
Step: 10460, train/loss: 0.08340000361204147
Step: 10460, train/grad_norm: 0.011033321730792522
Step: 10460, train/learning_rate: 3.75535455532372e-05
Step: 10460, train/epoch: 2.489290714263916
Step: 10470, train/loss: 0.07280000299215317
Step: 10470, train/grad_norm: 6.030576229095459
Step: 10470, train/learning_rate: 3.75416457245592e-05
Step: 10470, train/epoch: 2.491670608520508
Step: 10480, train/loss: 0.36739999055862427
Step: 10480, train/grad_norm: 0.02234664000570774
Step: 10480, train/learning_rate: 3.752974953386001e-05
Step: 10480, train/epoch: 2.4940505027770996
Step: 10490, train/loss: 0.07090000063180923
Step: 10490, train/grad_norm: 0.745524525642395
Step: 10490, train/learning_rate: 3.7517849705182016e-05
Step: 10490, train/epoch: 2.4964301586151123
Step: 10500, train/loss: 0.13809999823570251
Step: 10500, train/grad_norm: 0.21637120842933655
Step: 10500, train/learning_rate: 3.750594987650402e-05
Step: 10500, train/epoch: 2.498810052871704
Step: 10510, train/loss: 0.1111999973654747
Step: 10510, train/grad_norm: 16.91570472717285
Step: 10510, train/learning_rate: 3.749405004782602e-05
Step: 10510, train/epoch: 2.501189947128296
Step: 10520, train/loss: 0.05009999871253967
Step: 10520, train/grad_norm: 0.09632997959852219
Step: 10520, train/learning_rate: 3.7482150219148025e-05
Step: 10520, train/epoch: 2.5035698413848877
Step: 10530, train/loss: 0.1404000073671341
Step: 10530, train/grad_norm: 25.304702758789062
Step: 10530, train/learning_rate: 3.7470254028448835e-05
Step: 10530, train/epoch: 2.5059494972229004
Step: 10540, train/loss: 0.21469999849796295
Step: 10540, train/grad_norm: 9.055435180664062
Step: 10540, train/learning_rate: 3.745835419977084e-05
Step: 10540, train/epoch: 2.508329391479492
Step: 10550, train/loss: 0.15559999644756317
Step: 10550, train/grad_norm: 0.15366947650909424
Step: 10550, train/learning_rate: 3.744645437109284e-05
Step: 10550, train/epoch: 2.510709285736084
Step: 10560, train/loss: 0.2671000063419342
Step: 10560, train/grad_norm: 12.70756721496582
Step: 10560, train/learning_rate: 3.7434554542414844e-05
Step: 10560, train/epoch: 2.5130889415740967
Step: 10570, train/loss: 0.0689999982714653
Step: 10570, train/grad_norm: 0.863135039806366
Step: 10570, train/learning_rate: 3.742265471373685e-05
Step: 10570, train/epoch: 2.5154688358306885
Step: 10580, train/loss: 0.211899995803833
Step: 10580, train/grad_norm: 0.24972839653491974
Step: 10580, train/learning_rate: 3.741075852303766e-05
Step: 10580, train/epoch: 2.5178487300872803
Step: 10590, train/loss: 0.2766999900341034
Step: 10590, train/grad_norm: 8.749988555908203
Step: 10590, train/learning_rate: 3.739885869435966e-05
Step: 10590, train/epoch: 2.520228385925293
Step: 10600, train/loss: 0.4625000059604645
Step: 10600, train/grad_norm: 0.06526719033718109
Step: 10600, train/learning_rate: 3.738695886568166e-05
Step: 10600, train/epoch: 2.5226082801818848
Step: 10610, train/loss: 0.10530000180006027
Step: 10610, train/grad_norm: 11.388448715209961
Step: 10610, train/learning_rate: 3.7375059037003666e-05
Step: 10610, train/epoch: 2.5249881744384766
Step: 10620, train/loss: 0.06759999692440033
Step: 10620, train/grad_norm: 11.121535301208496
Step: 10620, train/learning_rate: 3.736315920832567e-05
Step: 10620, train/epoch: 2.5273678302764893
Step: 10630, train/loss: 0.04019999876618385
Step: 10630, train/grad_norm: 25.41219711303711
Step: 10630, train/learning_rate: 3.735126301762648e-05
Step: 10630, train/epoch: 2.529747724533081
Step: 10640, train/loss: 0.1005999967455864
Step: 10640, train/grad_norm: 0.010383284650743008
Step: 10640, train/learning_rate: 3.733936318894848e-05
Step: 10640, train/epoch: 2.532127618789673
Step: 10650, train/loss: 0.17880000174045563
Step: 10650, train/grad_norm: 23.789661407470703
Step: 10650, train/learning_rate: 3.7327463360270485e-05
Step: 10650, train/epoch: 2.5345072746276855
Step: 10660, train/loss: 0.27469998598098755
Step: 10660, train/grad_norm: 0.5725083351135254
Step: 10660, train/learning_rate: 3.731556353159249e-05
Step: 10660, train/epoch: 2.5368871688842773
Step: 10670, train/loss: 0.17659999430179596
Step: 10670, train/grad_norm: 0.6343193650245667
Step: 10670, train/learning_rate: 3.730366370291449e-05
Step: 10670, train/epoch: 2.539267063140869
Step: 10680, train/loss: 0.05389999970793724
Step: 10680, train/grad_norm: 0.14636026322841644
Step: 10680, train/learning_rate: 3.72917675122153e-05
Step: 10680, train/epoch: 2.541646718978882
Step: 10690, train/loss: 0.09059999883174896
Step: 10690, train/grad_norm: 0.560154139995575
Step: 10690, train/learning_rate: 3.7279867683537304e-05
Step: 10690, train/epoch: 2.5440266132354736
Step: 10700, train/loss: 0.16940000653266907
Step: 10700, train/grad_norm: 0.21457889676094055
Step: 10700, train/learning_rate: 3.726796785485931e-05
Step: 10700, train/epoch: 2.5464065074920654
Step: 10710, train/loss: 0.15410000085830688
Step: 10710, train/grad_norm: 2.688563585281372
Step: 10710, train/learning_rate: 3.725606802618131e-05
Step: 10710, train/epoch: 2.5487864017486572
Step: 10720, train/loss: 0.2985999882221222
Step: 10720, train/grad_norm: 0.01188378781080246
Step: 10720, train/learning_rate: 3.7244168197503313e-05
Step: 10720, train/epoch: 2.55116605758667
Step: 10730, train/loss: 0.2354000061750412
Step: 10730, train/grad_norm: 3.6786251068115234
Step: 10730, train/learning_rate: 3.7232272006804124e-05
Step: 10730, train/epoch: 2.5535459518432617
Step: 10740, train/loss: 0.2563000023365021
Step: 10740, train/grad_norm: 29.16702651977539
Step: 10740, train/learning_rate: 3.7220372178126127e-05
Step: 10740, train/epoch: 2.5559258460998535
Step: 10750, train/loss: 0.020899999886751175
Step: 10750, train/grad_norm: 0.5071806907653809
Step: 10750, train/learning_rate: 3.720847234944813e-05
Step: 10750, train/epoch: 2.558305501937866
Step: 10760, train/loss: 0.1817999929189682
Step: 10760, train/grad_norm: 0.014221841469407082
Step: 10760, train/learning_rate: 3.719657252077013e-05
Step: 10760, train/epoch: 2.560685396194458
Step: 10770, train/loss: 0.2736000120639801
Step: 10770, train/grad_norm: 27.227645874023438
Step: 10770, train/learning_rate: 3.7184672692092136e-05
Step: 10770, train/epoch: 2.56306529045105
Step: 10780, train/loss: 0.1160999983549118
Step: 10780, train/grad_norm: 32.754947662353516
Step: 10780, train/learning_rate: 3.7172776501392946e-05
Step: 10780, train/epoch: 2.5654449462890625
Step: 10790, train/loss: 0.12080000340938568
Step: 10790, train/grad_norm: 0.1884864866733551
Step: 10790, train/learning_rate: 3.716087667271495e-05
Step: 10790, train/epoch: 2.5678248405456543
Step: 10800, train/loss: 0.1995999962091446
Step: 10800, train/grad_norm: 0.08474592864513397
Step: 10800, train/learning_rate: 3.714897684403695e-05
Step: 10800, train/epoch: 2.570204734802246
Step: 10810, train/loss: 0.42089998722076416
Step: 10810, train/grad_norm: 20.122310638427734
Step: 10810, train/learning_rate: 3.7137077015358955e-05
Step: 10810, train/epoch: 2.572584390640259
Step: 10820, train/loss: 0.1039000004529953
Step: 10820, train/grad_norm: 12.301215171813965
Step: 10820, train/learning_rate: 3.712517718668096e-05
Step: 10820, train/epoch: 2.5749642848968506
Step: 10830, train/loss: 0.09929999709129333
Step: 10830, train/grad_norm: 2.808736801147461
Step: 10830, train/learning_rate: 3.711328099598177e-05
Step: 10830, train/epoch: 2.5773441791534424
Step: 10840, train/loss: 0.17180000245571136
Step: 10840, train/grad_norm: 13.237329483032227
Step: 10840, train/learning_rate: 3.710138116730377e-05
Step: 10840, train/epoch: 2.579723834991455
Step: 10850, train/loss: 0.11249999701976776
Step: 10850, train/grad_norm: 10.395047187805176
Step: 10850, train/learning_rate: 3.7089481338625774e-05
Step: 10850, train/epoch: 2.582103729248047
Step: 10860, train/loss: 0.05130000039935112
Step: 10860, train/grad_norm: 15.074413299560547
Step: 10860, train/learning_rate: 3.707758150994778e-05
Step: 10860, train/epoch: 2.5844836235046387
Step: 10870, train/loss: 0.21899999678134918
Step: 10870, train/grad_norm: 15.851499557495117
Step: 10870, train/learning_rate: 3.706568168126978e-05
Step: 10870, train/epoch: 2.5868632793426514
Step: 10880, train/loss: 0.09359999746084213
Step: 10880, train/grad_norm: 2.8879342079162598
Step: 10880, train/learning_rate: 3.705378549057059e-05
Step: 10880, train/epoch: 2.589243173599243
Step: 10890, train/loss: 0.18310000002384186
Step: 10890, train/grad_norm: 0.026645945385098457
Step: 10890, train/learning_rate: 3.704188566189259e-05
Step: 10890, train/epoch: 2.591623067855835
Step: 10900, train/loss: 0.2840000092983246
Step: 10900, train/grad_norm: 10.465574264526367
Step: 10900, train/learning_rate: 3.7029985833214596e-05
Step: 10900, train/epoch: 2.5940029621124268
Step: 10910, train/loss: 0.10050000250339508
Step: 10910, train/grad_norm: 1.3099461793899536
Step: 10910, train/learning_rate: 3.70180860045366e-05
Step: 10910, train/epoch: 2.5963826179504395
Step: 10920, train/loss: 0.3018999993801117
Step: 10920, train/grad_norm: 0.09091886132955551
Step: 10920, train/learning_rate: 3.70061861758586e-05
Step: 10920, train/epoch: 2.5987625122070312
Step: 10930, train/loss: 0.09189999848604202
Step: 10930, train/grad_norm: 3.6412110328674316
Step: 10930, train/learning_rate: 3.699428998515941e-05
Step: 10930, train/epoch: 2.601142406463623
Step: 10940, train/loss: 0.19609999656677246
Step: 10940, train/grad_norm: 12.608479499816895
Step: 10940, train/learning_rate: 3.6982390156481415e-05
Step: 10940, train/epoch: 2.6035220623016357
Step: 10950, train/loss: 0.18469999730587006
Step: 10950, train/grad_norm: 18.145835876464844
Step: 10950, train/learning_rate: 3.697049032780342e-05
Step: 10950, train/epoch: 2.6059019565582275
Step: 10960, train/loss: 0.3066999912261963
Step: 10960, train/grad_norm: 0.9066882133483887
Step: 10960, train/learning_rate: 3.695859049912542e-05
Step: 10960, train/epoch: 2.6082818508148193
Step: 10970, train/loss: 0.2587999999523163
Step: 10970, train/grad_norm: 8.280884742736816
Step: 10970, train/learning_rate: 3.6946690670447424e-05
Step: 10970, train/epoch: 2.610661506652832
Step: 10980, train/loss: 0.0608999989926815
Step: 10980, train/grad_norm: 5.3620991706848145
Step: 10980, train/learning_rate: 3.6934794479748234e-05
Step: 10980, train/epoch: 2.613041400909424
Step: 10990, train/loss: 0.11550000309944153
Step: 10990, train/grad_norm: 3.1310715675354004
Step: 10990, train/learning_rate: 3.692289465107024e-05
Step: 10990, train/epoch: 2.6154212951660156
Step: 11000, train/loss: 0.3310999870300293
Step: 11000, train/grad_norm: 21.693147659301758
Step: 11000, train/learning_rate: 3.691099482239224e-05
Step: 11000, train/epoch: 2.6178009510040283
Step: 11010, train/loss: 0.11739999800920486
Step: 11010, train/grad_norm: 1.7660436630249023
Step: 11010, train/learning_rate: 3.689909499371424e-05
Step: 11010, train/epoch: 2.62018084526062
Step: 11020, train/loss: 0.15520000457763672
Step: 11020, train/grad_norm: 0.1777544468641281
Step: 11020, train/learning_rate: 3.6887195165036246e-05
Step: 11020, train/epoch: 2.622560739517212
Step: 11030, train/loss: 0.13339999318122864
Step: 11030, train/grad_norm: 0.04687280207872391
Step: 11030, train/learning_rate: 3.6875298974337056e-05
Step: 11030, train/epoch: 2.6249403953552246
Step: 11040, train/loss: 0.22550000250339508
Step: 11040, train/grad_norm: 19.082374572753906
Step: 11040, train/learning_rate: 3.686339914565906e-05
Step: 11040, train/epoch: 2.6273202896118164
Step: 11050, train/loss: 0.1826000064611435
Step: 11050, train/grad_norm: 0.002532480750232935
Step: 11050, train/learning_rate: 3.685149931698106e-05
Step: 11050, train/epoch: 2.629700183868408
Step: 11060, train/loss: 0.1648000031709671
Step: 11060, train/grad_norm: 2.515043258666992
Step: 11060, train/learning_rate: 3.6839599488303065e-05
Step: 11060, train/epoch: 2.632080078125
Step: 11070, train/loss: 0.06920000165700912
Step: 11070, train/grad_norm: 9.89758014678955
Step: 11070, train/learning_rate: 3.682769965962507e-05
Step: 11070, train/epoch: 2.6344597339630127
Step: 11080, train/loss: 0.2393999993801117
Step: 11080, train/grad_norm: 3.833024740219116
Step: 11080, train/learning_rate: 3.681580346892588e-05
Step: 11080, train/epoch: 2.6368396282196045
Step: 11090, train/loss: 0.18289999663829803
Step: 11090, train/grad_norm: 30.565706253051758
Step: 11090, train/learning_rate: 3.680390364024788e-05
Step: 11090, train/epoch: 2.6392195224761963
Step: 11100, train/loss: 0.16979999840259552
Step: 11100, train/grad_norm: 7.094235897064209
Step: 11100, train/learning_rate: 3.6792003811569884e-05
Step: 11100, train/epoch: 2.641599178314209
Step: 11110, train/loss: 0.11230000108480453
Step: 11110, train/grad_norm: 0.009622889570891857
Step: 11110, train/learning_rate: 3.678010398289189e-05
Step: 11110, train/epoch: 2.643979072570801
Step: 11120, train/loss: 0.13930000364780426
Step: 11120, train/grad_norm: 0.12292104214429855
Step: 11120, train/learning_rate: 3.676820415421389e-05
Step: 11120, train/epoch: 2.6463589668273926
Step: 11130, train/loss: 0.01940000057220459
Step: 11130, train/grad_norm: 1.1739779710769653
Step: 11130, train/learning_rate: 3.67563079635147e-05
Step: 11130, train/epoch: 2.6487386226654053
Step: 11140, train/loss: 0.1777999997138977
Step: 11140, train/grad_norm: 0.7565256953239441
Step: 11140, train/learning_rate: 3.6744408134836704e-05
Step: 11140, train/epoch: 2.651118516921997
Step: 11150, train/loss: 0.19300000369548798
Step: 11150, train/grad_norm: 30.156789779663086
Step: 11150, train/learning_rate: 3.6732508306158707e-05
Step: 11150, train/epoch: 2.653498411178589
Step: 11160, train/loss: 0.1770000010728836
Step: 11160, train/grad_norm: 18.86020278930664
Step: 11160, train/learning_rate: 3.672060847748071e-05
Step: 11160, train/epoch: 2.6558780670166016
Step: 11170, train/loss: 0.051500000059604645
Step: 11170, train/grad_norm: 1.1401185989379883
Step: 11170, train/learning_rate: 3.670870864880271e-05
Step: 11170, train/epoch: 2.6582579612731934
Step: 11180, train/loss: 0.09080000221729279
Step: 11180, train/grad_norm: 0.21887914836406708
Step: 11180, train/learning_rate: 3.669681245810352e-05
Step: 11180, train/epoch: 2.660637855529785
Step: 11190, train/loss: 0.02710000053048134
Step: 11190, train/grad_norm: 4.739867687225342
Step: 11190, train/learning_rate: 3.6684912629425526e-05
Step: 11190, train/epoch: 2.663017511367798
Step: 11200, train/loss: 0.05909999832510948
Step: 11200, train/grad_norm: 0.5890412330627441
Step: 11200, train/learning_rate: 3.667301280074753e-05
Step: 11200, train/epoch: 2.6653974056243896
Step: 11210, train/loss: 0.10729999840259552
Step: 11210, train/grad_norm: 1.4773917198181152
Step: 11210, train/learning_rate: 3.666111297206953e-05
Step: 11210, train/epoch: 2.6677772998809814
Step: 11220, train/loss: 0.024700000882148743
Step: 11220, train/grad_norm: 0.029463665559887886
Step: 11220, train/learning_rate: 3.6649213143391535e-05
Step: 11220, train/epoch: 2.670156955718994
Step: 11230, train/loss: 0.24250000715255737
Step: 11230, train/grad_norm: 10.906767845153809
Step: 11230, train/learning_rate: 3.6637316952692345e-05
Step: 11230, train/epoch: 2.672536849975586
Step: 11240, train/loss: 0.09700000286102295
Step: 11240, train/grad_norm: 0.9411609768867493
Step: 11240, train/learning_rate: 3.662541712401435e-05
Step: 11240, train/epoch: 2.6749167442321777
Step: 11250, train/loss: 0.039000000804662704
Step: 11250, train/grad_norm: 9.03543758392334
Step: 11250, train/learning_rate: 3.661351729533635e-05
Step: 11250, train/epoch: 2.6772966384887695
Step: 11260, train/loss: 0.04100000113248825
Step: 11260, train/grad_norm: 0.8367700576782227
Step: 11260, train/learning_rate: 3.6601617466658354e-05
Step: 11260, train/epoch: 2.6796762943267822
Step: 11270, train/loss: 0.07569999992847443
Step: 11270, train/grad_norm: 0.004741166718304157
Step: 11270, train/learning_rate: 3.658971763798036e-05
Step: 11270, train/epoch: 2.682056188583374
Step: 11280, train/loss: 0.37310001254081726
Step: 11280, train/grad_norm: 46.4598388671875
Step: 11280, train/learning_rate: 3.657782144728117e-05
Step: 11280, train/epoch: 2.684436082839966
Step: 11290, train/loss: 0.10760000348091125
Step: 11290, train/grad_norm: 17.885221481323242
Step: 11290, train/learning_rate: 3.656592161860317e-05
Step: 11290, train/epoch: 2.6868157386779785
Step: 11300, train/loss: 0.13910000026226044
Step: 11300, train/grad_norm: 0.10443870723247528
Step: 11300, train/learning_rate: 3.655402178992517e-05
Step: 11300, train/epoch: 2.6891956329345703
Step: 11310, train/loss: 0.029200000688433647
Step: 11310, train/grad_norm: 9.555355072021484
Step: 11310, train/learning_rate: 3.6542121961247176e-05
Step: 11310, train/epoch: 2.691575527191162
Step: 11320, train/loss: 0.30399999022483826
Step: 11320, train/grad_norm: 0.5539989471435547
Step: 11320, train/learning_rate: 3.653022213256918e-05
Step: 11320, train/epoch: 2.693955183029175
Step: 11330, train/loss: 0.2257000058889389
Step: 11330, train/grad_norm: 0.10749827325344086
Step: 11330, train/learning_rate: 3.651832594186999e-05
Step: 11330, train/epoch: 2.6963350772857666
Step: 11340, train/loss: 0.08020000159740448
Step: 11340, train/grad_norm: 0.05963706970214844
Step: 11340, train/learning_rate: 3.650642611319199e-05
Step: 11340, train/epoch: 2.6987149715423584
Step: 11350, train/loss: 0.10679999738931656
Step: 11350, train/grad_norm: 11.489676475524902
Step: 11350, train/learning_rate: 3.6494526284513995e-05
Step: 11350, train/epoch: 2.701094627380371
Step: 11360, train/loss: 0.10530000180006027
Step: 11360, train/grad_norm: 1.2278614044189453
Step: 11360, train/learning_rate: 3.6482626455836e-05
Step: 11360, train/epoch: 2.703474521636963
Step: 11370, train/loss: 0.11819999665021896
Step: 11370, train/grad_norm: 0.08874739706516266
Step: 11370, train/learning_rate: 3.6470726627158e-05
Step: 11370, train/epoch: 2.7058544158935547
Step: 11380, train/loss: 0.03720000013709068
Step: 11380, train/grad_norm: 0.046762868762016296
Step: 11380, train/learning_rate: 3.645883043645881e-05
Step: 11380, train/epoch: 2.7082340717315674
Step: 11390, train/loss: 0.04610000178217888
Step: 11390, train/grad_norm: 5.0182929039001465
Step: 11390, train/learning_rate: 3.6446930607780814e-05
Step: 11390, train/epoch: 2.710613965988159
Step: 11400, train/loss: 0.13510000705718994
Step: 11400, train/grad_norm: 21.956960678100586
Step: 11400, train/learning_rate: 3.643503077910282e-05
Step: 11400, train/epoch: 2.712993860244751
Step: 11410, train/loss: 0.1607999950647354
Step: 11410, train/grad_norm: 2.7410409450531006
Step: 11410, train/learning_rate: 3.642313095042482e-05
Step: 11410, train/epoch: 2.7153735160827637
Step: 11420, train/loss: 0.21279999613761902
Step: 11420, train/grad_norm: 0.030428115278482437
Step: 11420, train/learning_rate: 3.641123112174682e-05
Step: 11420, train/epoch: 2.7177534103393555
Step: 11430, train/loss: 0.1306000053882599
Step: 11430, train/grad_norm: 19.060340881347656
Step: 11430, train/learning_rate: 3.639933493104763e-05
Step: 11430, train/epoch: 2.7201333045959473
Step: 11440, train/loss: 0.0877000018954277
Step: 11440, train/grad_norm: 0.03687586262822151
Step: 11440, train/learning_rate: 3.6387435102369636e-05
Step: 11440, train/epoch: 2.722513198852539
Step: 11450, train/loss: 0.06449999660253525
Step: 11450, train/grad_norm: 0.9278234839439392
Step: 11450, train/learning_rate: 3.637553527369164e-05
Step: 11450, train/epoch: 2.7248928546905518
Step: 11460, train/loss: 0.30379998683929443
Step: 11460, train/grad_norm: 0.41273564100265503
Step: 11460, train/learning_rate: 3.636363544501364e-05
Step: 11460, train/epoch: 2.7272727489471436
Step: 11470, train/loss: 0.10700000077486038
Step: 11470, train/grad_norm: 0.17497922480106354
Step: 11470, train/learning_rate: 3.6351735616335645e-05
Step: 11470, train/epoch: 2.7296526432037354
Step: 11480, train/loss: 0.13989999890327454
Step: 11480, train/grad_norm: 12.553997993469238
Step: 11480, train/learning_rate: 3.6339839425636455e-05
Step: 11480, train/epoch: 2.732032299041748
Step: 11490, train/loss: 0.05660000070929527
Step: 11490, train/grad_norm: 0.0033676670864224434
Step: 11490, train/learning_rate: 3.632793959695846e-05
Step: 11490, train/epoch: 2.73441219329834
Step: 11500, train/loss: 0.15850000083446503
Step: 11500, train/grad_norm: 7.452965259552002
Step: 11500, train/learning_rate: 3.631603976828046e-05
Step: 11500, train/epoch: 2.7367920875549316
Step: 11510, train/loss: 0.18629999458789825
Step: 11510, train/grad_norm: 0.01535077579319477
Step: 11510, train/learning_rate: 3.6304139939602464e-05
Step: 11510, train/epoch: 2.7391717433929443
Step: 11520, train/loss: 0.17710000276565552
Step: 11520, train/grad_norm: 0.06994826346635818
Step: 11520, train/learning_rate: 3.629224011092447e-05
Step: 11520, train/epoch: 2.741551637649536
Step: 11530, train/loss: 0.12269999831914902
Step: 11530, train/grad_norm: 0.8375889658927917
Step: 11530, train/learning_rate: 3.628034392022528e-05
Step: 11530, train/epoch: 2.743931531906128
Step: 11540, train/loss: 0.07760000228881836
Step: 11540, train/grad_norm: 5.397059440612793
Step: 11540, train/learning_rate: 3.626844409154728e-05
Step: 11540, train/epoch: 2.7463111877441406
Step: 11550, train/loss: 0.06539999693632126
Step: 11550, train/grad_norm: 0.04532555490732193
Step: 11550, train/learning_rate: 3.6256544262869284e-05
Step: 11550, train/epoch: 2.7486910820007324
Step: 11560, train/loss: 0.11720000207424164
Step: 11560, train/grad_norm: 0.06765899807214737
Step: 11560, train/learning_rate: 3.6244644434191287e-05
Step: 11560, train/epoch: 2.751070976257324
Step: 11570, train/loss: 0.3287000060081482
Step: 11570, train/grad_norm: 0.004688066430389881
Step: 11570, train/learning_rate: 3.623274460551329e-05
Step: 11570, train/epoch: 2.753450632095337
Step: 11580, train/loss: 0.16359999775886536
Step: 11580, train/grad_norm: 0.15934713184833527
Step: 11580, train/learning_rate: 3.62208484148141e-05
Step: 11580, train/epoch: 2.7558305263519287
Step: 11590, train/loss: 0.22210000455379486
Step: 11590, train/grad_norm: 0.38403207063674927
Step: 11590, train/learning_rate: 3.62089485861361e-05
Step: 11590, train/epoch: 2.7582104206085205
Step: 11600, train/loss: 0.05350000038743019
Step: 11600, train/grad_norm: 0.2652710974216461
Step: 11600, train/learning_rate: 3.6197048757458106e-05
Step: 11600, train/epoch: 2.760590076446533
Step: 11610, train/loss: 0.01140000019222498
Step: 11610, train/grad_norm: 0.025079837068915367
Step: 11610, train/learning_rate: 3.618514892878011e-05
Step: 11610, train/epoch: 2.762969970703125
Step: 11620, train/loss: 0.13050000369548798
Step: 11620, train/grad_norm: 0.018913542851805687
Step: 11620, train/learning_rate: 3.617324910010211e-05
Step: 11620, train/epoch: 2.765349864959717
Step: 11630, train/loss: 0.04050000011920929
Step: 11630, train/grad_norm: 16.448322296142578
Step: 11630, train/learning_rate: 3.616135290940292e-05
Step: 11630, train/epoch: 2.7677297592163086
Step: 11640, train/loss: 0.07900000363588333
Step: 11640, train/grad_norm: 0.018011929467320442
Step: 11640, train/learning_rate: 3.6149453080724925e-05
Step: 11640, train/epoch: 2.7701094150543213
Step: 11650, train/loss: 0.10440000146627426
Step: 11650, train/grad_norm: 0.06788922846317291
Step: 11650, train/learning_rate: 3.613755325204693e-05
Step: 11650, train/epoch: 2.772489309310913
Step: 11660, train/loss: 0.2784999907016754
Step: 11660, train/grad_norm: 0.018051929771900177
Step: 11660, train/learning_rate: 3.612565342336893e-05
Step: 11660, train/epoch: 2.774869203567505
Step: 11670, train/loss: 0.2743000090122223
Step: 11670, train/grad_norm: 3.454662561416626
Step: 11670, train/learning_rate: 3.6113753594690934e-05
Step: 11670, train/epoch: 2.7772488594055176
Step: 11680, train/loss: 0.05770000070333481
Step: 11680, train/grad_norm: 0.019627630710601807
Step: 11680, train/learning_rate: 3.6101857403991744e-05
Step: 11680, train/epoch: 2.7796287536621094
Step: 11690, train/loss: 0.26190000772476196
Step: 11690, train/grad_norm: 14.450551986694336
Step: 11690, train/learning_rate: 3.608995757531375e-05
Step: 11690, train/epoch: 2.782008647918701
Step: 11700, train/loss: 0.09120000153779984
Step: 11700, train/grad_norm: 8.3574857711792
Step: 11700, train/learning_rate: 3.607805774663575e-05
Step: 11700, train/epoch: 2.784388303756714
Step: 11710, train/loss: 0.16359999775886536
Step: 11710, train/grad_norm: 0.22180801630020142
Step: 11710, train/learning_rate: 3.606615791795775e-05
Step: 11710, train/epoch: 2.7867681980133057
Step: 11720, train/loss: 0.10090000182390213
Step: 11720, train/grad_norm: 2.44484281539917
Step: 11720, train/learning_rate: 3.6054258089279756e-05
Step: 11720, train/epoch: 2.7891480922698975
Step: 11730, train/loss: 0.226500004529953
Step: 11730, train/grad_norm: 0.40669146180152893
Step: 11730, train/learning_rate: 3.6042361898580566e-05
Step: 11730, train/epoch: 2.79152774810791
Step: 11740, train/loss: 0.11710000038146973
Step: 11740, train/grad_norm: 10.858104705810547
Step: 11740, train/learning_rate: 3.603046206990257e-05
Step: 11740, train/epoch: 2.793907642364502
Step: 11750, train/loss: 0.1006999984383583
Step: 11750, train/grad_norm: 8.159000396728516
Step: 11750, train/learning_rate: 3.601856224122457e-05
Step: 11750, train/epoch: 2.7962875366210938
Step: 11760, train/loss: 0.10189999639987946
Step: 11760, train/grad_norm: 0.8580294251441956
Step: 11760, train/learning_rate: 3.6006662412546575e-05
Step: 11760, train/epoch: 2.7986671924591064
Step: 11770, train/loss: 0.12099999934434891
Step: 11770, train/grad_norm: 2.721623420715332
Step: 11770, train/learning_rate: 3.599476258386858e-05
Step: 11770, train/epoch: 2.8010470867156982
Step: 11780, train/loss: 0.0560000017285347
Step: 11780, train/grad_norm: 4.02366828918457
Step: 11780, train/learning_rate: 3.598286639316939e-05
Step: 11780, train/epoch: 2.80342698097229
Step: 11790, train/loss: 0.19370000064373016
Step: 11790, train/grad_norm: 5.775432109832764
Step: 11790, train/learning_rate: 3.597096656449139e-05
Step: 11790, train/epoch: 2.805806875228882
Step: 11800, train/loss: 0.375
Step: 11800, train/grad_norm: 4.894827365875244
Step: 11800, train/learning_rate: 3.5959066735813394e-05
Step: 11800, train/epoch: 2.8081865310668945
Step: 11810, train/loss: 0.08839999884366989
Step: 11810, train/grad_norm: 0.008878275752067566
Step: 11810, train/learning_rate: 3.59471669071354e-05
Step: 11810, train/epoch: 2.8105664253234863
Step: 11820, train/loss: 0.1664000004529953
Step: 11820, train/grad_norm: 56.08469772338867
Step: 11820, train/learning_rate: 3.593527071643621e-05
Step: 11820, train/epoch: 2.812946319580078
Step: 11830, train/loss: 0.16369999945163727
Step: 11830, train/grad_norm: 18.664365768432617
Step: 11830, train/learning_rate: 3.592337088775821e-05
Step: 11830, train/epoch: 2.815325975418091
Step: 11840, train/loss: 0.06289999932050705
Step: 11840, train/grad_norm: 0.211832657456398
Step: 11840, train/learning_rate: 3.591147105908021e-05
Step: 11840, train/epoch: 2.8177058696746826
Step: 11850, train/loss: 0.2989000082015991
Step: 11850, train/grad_norm: 0.15858492255210876
Step: 11850, train/learning_rate: 3.5899571230402216e-05
Step: 11850, train/epoch: 2.8200857639312744
Step: 11860, train/loss: 0.1542000025510788
Step: 11860, train/grad_norm: 0.07832503318786621
Step: 11860, train/learning_rate: 3.588767140172422e-05
Step: 11860, train/epoch: 2.822465419769287
Step: 11870, train/loss: 0.2084999978542328
Step: 11870, train/grad_norm: 0.008029872551560402
Step: 11870, train/learning_rate: 3.587577521102503e-05
Step: 11870, train/epoch: 2.824845314025879
Step: 11880, train/loss: 0.12549999356269836
Step: 11880, train/grad_norm: 3.78603458404541
Step: 11880, train/learning_rate: 3.586387538234703e-05
Step: 11880, train/epoch: 2.8272252082824707
Step: 11890, train/loss: 0.16940000653266907
Step: 11890, train/grad_norm: 1.3383491039276123
Step: 11890, train/learning_rate: 3.5851975553669035e-05
Step: 11890, train/epoch: 2.8296048641204834
Step: 11900, train/loss: 0.29280000925064087
Step: 11900, train/grad_norm: 5.108360767364502
Step: 11900, train/learning_rate: 3.584007572499104e-05
Step: 11900, train/epoch: 2.831984758377075
Step: 11910, train/loss: 0.09889999777078629
Step: 11910, train/grad_norm: 0.07981346547603607
Step: 11910, train/learning_rate: 3.582817589631304e-05
Step: 11910, train/epoch: 2.834364652633667
Step: 11920, train/loss: 0.06809999793767929
Step: 11920, train/grad_norm: 8.525188446044922
Step: 11920, train/learning_rate: 3.581627970561385e-05
Step: 11920, train/epoch: 2.8367443084716797
Step: 11930, train/loss: 0.1776999980211258
Step: 11930, train/grad_norm: 0.5554218292236328
Step: 11930, train/learning_rate: 3.5804379876935855e-05
Step: 11930, train/epoch: 2.8391242027282715
Step: 11940, train/loss: 0.14579999446868896
Step: 11940, train/grad_norm: 29.966205596923828
Step: 11940, train/learning_rate: 3.579248004825786e-05
Step: 11940, train/epoch: 2.8415040969848633
Step: 11950, train/loss: 0.2809999883174896
Step: 11950, train/grad_norm: 33.14031219482422
Step: 11950, train/learning_rate: 3.578058021957986e-05
Step: 11950, train/epoch: 2.843883752822876
Step: 11960, train/loss: 0.2705000042915344
Step: 11960, train/grad_norm: 22.42881965637207
Step: 11960, train/learning_rate: 3.5768680390901864e-05
Step: 11960, train/epoch: 2.8462636470794678
Step: 11970, train/loss: 0.053599998354911804
Step: 11970, train/grad_norm: 0.029782183468341827
Step: 11970, train/learning_rate: 3.5756784200202674e-05
Step: 11970, train/epoch: 2.8486435413360596
Step: 11980, train/loss: 0.1867000013589859
Step: 11980, train/grad_norm: 22.18560218811035
Step: 11980, train/learning_rate: 3.574488437152468e-05
Step: 11980, train/epoch: 2.8510234355926514
Step: 11990, train/loss: 0.2513999938964844
Step: 11990, train/grad_norm: 0.03614508733153343
Step: 11990, train/learning_rate: 3.573298454284668e-05
Step: 11990, train/epoch: 2.853403091430664
Step: 12000, train/loss: 0.13650000095367432
Step: 12000, train/grad_norm: 0.07224975526332855
Step: 12000, train/learning_rate: 3.572108471416868e-05
Step: 12000, train/epoch: 2.855782985687256
Step: 12010, train/loss: 0.062300000339746475
Step: 12010, train/grad_norm: 0.03890012949705124
Step: 12010, train/learning_rate: 3.5709184885490686e-05
Step: 12010, train/epoch: 2.8581628799438477
Step: 12020, train/loss: 0.07079999893903732
Step: 12020, train/grad_norm: 5.481832027435303
Step: 12020, train/learning_rate: 3.5697288694791496e-05
Step: 12020, train/epoch: 2.8605425357818604
Step: 12030, train/loss: 0.08150000125169754
Step: 12030, train/grad_norm: 0.0060368734411895275
Step: 12030, train/learning_rate: 3.56853888661135e-05
Step: 12030, train/epoch: 2.862922430038452
Step: 12040, train/loss: 0.04560000076889992
Step: 12040, train/grad_norm: 36.68233871459961
Step: 12040, train/learning_rate: 3.56734890374355e-05
Step: 12040, train/epoch: 2.865302324295044
Step: 12050, train/loss: 0.009399999864399433
Step: 12050, train/grad_norm: 8.603110313415527
Step: 12050, train/learning_rate: 3.5661589208757505e-05
Step: 12050, train/epoch: 2.8676819801330566
Step: 12060, train/loss: 0.16539999842643738
Step: 12060, train/grad_norm: 2.672131061553955
Step: 12060, train/learning_rate: 3.564968938007951e-05
Step: 12060, train/epoch: 2.8700618743896484
Step: 12070, train/loss: 0.053199999034404755
Step: 12070, train/grad_norm: 0.01126150880008936
Step: 12070, train/learning_rate: 3.563779318938032e-05
Step: 12070, train/epoch: 2.8724417686462402
Step: 12080, train/loss: 0.1354999989271164
Step: 12080, train/grad_norm: 0.04024908319115639
Step: 12080, train/learning_rate: 3.562589336070232e-05
Step: 12080, train/epoch: 2.874821424484253
Step: 12090, train/loss: 0.0544000007212162
Step: 12090, train/grad_norm: 6.6336989402771
Step: 12090, train/learning_rate: 3.5613993532024324e-05
Step: 12090, train/epoch: 2.8772013187408447
Step: 12100, train/loss: 0.2329999953508377
Step: 12100, train/grad_norm: 13.618012428283691
Step: 12100, train/learning_rate: 3.560209370334633e-05
Step: 12100, train/epoch: 2.8795812129974365
Step: 12110, train/loss: 0.06549999862909317
Step: 12110, train/grad_norm: 0.18216073513031006
Step: 12110, train/learning_rate: 3.559019387466833e-05
Step: 12110, train/epoch: 2.881960868835449
Step: 12120, train/loss: 0.0738999992609024
Step: 12120, train/grad_norm: 11.946907043457031
Step: 12120, train/learning_rate: 3.557829768396914e-05
Step: 12120, train/epoch: 2.884340763092041
Step: 12130, train/loss: 0.1949000060558319
Step: 12130, train/grad_norm: 8.737398147583008
Step: 12130, train/learning_rate: 3.556639785529114e-05
Step: 12130, train/epoch: 2.886720657348633
Step: 12140, train/loss: 0.16820000112056732
Step: 12140, train/grad_norm: 39.046478271484375
Step: 12140, train/learning_rate: 3.5554498026613146e-05
Step: 12140, train/epoch: 2.8891003131866455
Step: 12150, train/loss: 0.25679999589920044
Step: 12150, train/grad_norm: 0.016078295186161995
Step: 12150, train/learning_rate: 3.554259819793515e-05
Step: 12150, train/epoch: 2.8914802074432373
Step: 12160, train/loss: 0.15379999577999115
Step: 12160, train/grad_norm: 26.950197219848633
Step: 12160, train/learning_rate: 3.553069836925715e-05
Step: 12160, train/epoch: 2.893860101699829
Step: 12170, train/loss: 0.023099999874830246
Step: 12170, train/grad_norm: 0.00817661639302969
Step: 12170, train/learning_rate: 3.551880217855796e-05
Step: 12170, train/epoch: 2.896239995956421
Step: 12180, train/loss: 0.1387999951839447
Step: 12180, train/grad_norm: 0.04566962644457817
Step: 12180, train/learning_rate: 3.5506902349879965e-05
Step: 12180, train/epoch: 2.8986196517944336
Step: 12190, train/loss: 0.44760000705718994
Step: 12190, train/grad_norm: 33.8968391418457
Step: 12190, train/learning_rate: 3.549500252120197e-05
Step: 12190, train/epoch: 2.9009995460510254
Step: 12200, train/loss: 0.18770000338554382
Step: 12200, train/grad_norm: 3.6477575302124023
Step: 12200, train/learning_rate: 3.548310269252397e-05
Step: 12200, train/epoch: 2.903379440307617
Step: 12210, train/loss: 0.2370000034570694
Step: 12210, train/grad_norm: 37.79534912109375
Step: 12210, train/learning_rate: 3.5471202863845974e-05
Step: 12210, train/epoch: 2.90575909614563
Step: 12220, train/loss: 0.1370999962091446
Step: 12220, train/grad_norm: 1.7782238721847534
Step: 12220, train/learning_rate: 3.5459306673146784e-05
Step: 12220, train/epoch: 2.9081389904022217
Step: 12230, train/loss: 0.09459999948740005
Step: 12230, train/grad_norm: 5.366296291351318
Step: 12230, train/learning_rate: 3.544740684446879e-05
Step: 12230, train/epoch: 2.9105188846588135
Step: 12240, train/loss: 0.11890000104904175
Step: 12240, train/grad_norm: 0.02930263616144657
Step: 12240, train/learning_rate: 3.543550701579079e-05
Step: 12240, train/epoch: 2.912898540496826
Step: 12250, train/loss: 0.05559999868273735
Step: 12250, train/grad_norm: 0.024060722440481186
Step: 12250, train/learning_rate: 3.542360718711279e-05
Step: 12250, train/epoch: 2.915278434753418
Step: 12260, train/loss: 0.05209999904036522
Step: 12260, train/grad_norm: 0.08003608882427216
Step: 12260, train/learning_rate: 3.5411707358434796e-05
Step: 12260, train/epoch: 2.9176583290100098
Step: 12270, train/loss: 0.1185000017285347
Step: 12270, train/grad_norm: 0.039034243673086166
Step: 12270, train/learning_rate: 3.5399811167735606e-05
Step: 12270, train/epoch: 2.9200379848480225
Step: 12280, train/loss: 0.2522999942302704
Step: 12280, train/grad_norm: 0.4482136070728302
Step: 12280, train/learning_rate: 3.538791133905761e-05
Step: 12280, train/epoch: 2.9224178791046143
Step: 12290, train/loss: 0.03660000115633011
Step: 12290, train/grad_norm: 0.003981955349445343
Step: 12290, train/learning_rate: 3.537601151037961e-05
Step: 12290, train/epoch: 2.924797773361206
Step: 12300, train/loss: 0.20720000565052032
Step: 12300, train/grad_norm: 1.9795477390289307
Step: 12300, train/learning_rate: 3.5364111681701615e-05
Step: 12300, train/epoch: 2.9271774291992188
Step: 12310, train/loss: 0.27709999680519104
Step: 12310, train/grad_norm: 14.521132469177246
Step: 12310, train/learning_rate: 3.535221185302362e-05
Step: 12310, train/epoch: 2.9295573234558105
Step: 12320, train/loss: 0.15000000596046448
Step: 12320, train/grad_norm: 0.6579398512840271
Step: 12320, train/learning_rate: 3.534031566232443e-05
Step: 12320, train/epoch: 2.9319372177124023
Step: 12330, train/loss: 0.25189998745918274
Step: 12330, train/grad_norm: 12.115296363830566
Step: 12330, train/learning_rate: 3.532841583364643e-05
Step: 12330, train/epoch: 2.934316873550415
Step: 12340, train/loss: 0.12380000203847885
Step: 12340, train/grad_norm: 0.018839409574866295
Step: 12340, train/learning_rate: 3.5316516004968435e-05
Step: 12340, train/epoch: 2.936696767807007
Step: 12350, train/loss: 0.11400000005960464
Step: 12350, train/grad_norm: 0.5909380316734314
Step: 12350, train/learning_rate: 3.530461617629044e-05
Step: 12350, train/epoch: 2.9390766620635986
Step: 12360, train/loss: 0.05739999935030937
Step: 12360, train/grad_norm: 11.878968238830566
Step: 12360, train/learning_rate: 3.529271634761244e-05
Step: 12360, train/epoch: 2.9414565563201904
Step: 12370, train/loss: 0.041200000792741776
Step: 12370, train/grad_norm: 0.0917610451579094
Step: 12370, train/learning_rate: 3.528082015691325e-05
Step: 12370, train/epoch: 2.943836212158203
Step: 12380, train/loss: 0.11779999732971191
Step: 12380, train/grad_norm: 0.08159434050321579
Step: 12380, train/learning_rate: 3.5268920328235254e-05
Step: 12380, train/epoch: 2.946216106414795
Step: 12390, train/loss: 0.06710000336170197
Step: 12390, train/grad_norm: 19.548381805419922
Step: 12390, train/learning_rate: 3.525702049955726e-05
Step: 12390, train/epoch: 2.9485960006713867
Step: 12400, train/loss: 0.41449999809265137
Step: 12400, train/grad_norm: 16.841466903686523
Step: 12400, train/learning_rate: 3.524512067087926e-05
Step: 12400, train/epoch: 2.9509756565093994
Step: 12410, train/loss: 0.06930000334978104
Step: 12410, train/grad_norm: 3.7062106132507324
Step: 12410, train/learning_rate: 3.523322084220126e-05
Step: 12410, train/epoch: 2.953355550765991
Step: 12420, train/loss: 0.1770000010728836
Step: 12420, train/grad_norm: 0.07764016091823578
Step: 12420, train/learning_rate: 3.522132465150207e-05
Step: 12420, train/epoch: 2.955735445022583
Step: 12430, train/loss: 0.07050000131130219
Step: 12430, train/grad_norm: 0.12051478028297424
Step: 12430, train/learning_rate: 3.5209424822824076e-05
Step: 12430, train/epoch: 2.9581151008605957
Step: 12440, train/loss: 0.10369999706745148
Step: 12440, train/grad_norm: 30.732784271240234
Step: 12440, train/learning_rate: 3.519752499414608e-05
Step: 12440, train/epoch: 2.9604949951171875
Step: 12450, train/loss: 0.07919999957084656
Step: 12450, train/grad_norm: 1.5071907043457031
Step: 12450, train/learning_rate: 3.518562516546808e-05
Step: 12450, train/epoch: 2.9628748893737793
Step: 12460, train/loss: 0.12479999661445618
Step: 12460, train/grad_norm: 19.45246124267578
Step: 12460, train/learning_rate: 3.5173725336790085e-05
Step: 12460, train/epoch: 2.965254545211792
Step: 12470, train/loss: 0.09049999713897705
Step: 12470, train/grad_norm: 0.0679405927658081
Step: 12470, train/learning_rate: 3.5161829146090895e-05
Step: 12470, train/epoch: 2.967634439468384
Step: 12480, train/loss: 0.09839999675750732
Step: 12480, train/grad_norm: 2.81248140335083
Step: 12480, train/learning_rate: 3.51499293174129e-05
Step: 12480, train/epoch: 2.9700143337249756
Step: 12490, train/loss: 0.2840999960899353
Step: 12490, train/grad_norm: 41.58457946777344
Step: 12490, train/learning_rate: 3.51380294887349e-05
Step: 12490, train/epoch: 2.9723939895629883
Step: 12500, train/loss: 0.13099999725818634
Step: 12500, train/grad_norm: 0.030976945534348488
Step: 12500, train/learning_rate: 3.5126129660056904e-05
Step: 12500, train/epoch: 2.97477388381958
Step: 12510, train/loss: 0.18400000035762787
Step: 12510, train/grad_norm: 0.06499731540679932
Step: 12510, train/learning_rate: 3.511422983137891e-05
Step: 12510, train/epoch: 2.977153778076172
Step: 12520, train/loss: 0.1624000072479248
Step: 12520, train/grad_norm: 0.21069756150245667
Step: 12520, train/learning_rate: 3.510233364067972e-05
Step: 12520, train/epoch: 2.9795336723327637
Step: 12530, train/loss: 0.19359999895095825
Step: 12530, train/grad_norm: 0.005020021926611662
Step: 12530, train/learning_rate: 3.509043381200172e-05
Step: 12530, train/epoch: 2.9819133281707764
Step: 12540, train/loss: 0.14239999651908875
Step: 12540, train/grad_norm: 13.431059837341309
Step: 12540, train/learning_rate: 3.507853398332372e-05
Step: 12540, train/epoch: 2.984293222427368
Step: 12550, train/loss: 0.20919999480247498
Step: 12550, train/grad_norm: 10.484539031982422
Step: 12550, train/learning_rate: 3.5066634154645726e-05
Step: 12550, train/epoch: 2.98667311668396
Step: 12560, train/loss: 0.1054999977350235
Step: 12560, train/grad_norm: 20.260568618774414
Step: 12560, train/learning_rate: 3.505473432596773e-05
Step: 12560, train/epoch: 2.9890527725219727
Step: 12570, train/loss: 0.25
Step: 12570, train/grad_norm: 19.47062110900879
Step: 12570, train/learning_rate: 3.504283813526854e-05
Step: 12570, train/epoch: 2.9914326667785645
Step: 12580, train/loss: 0.3156000077724457
Step: 12580, train/grad_norm: 23.346683502197266
Step: 12580, train/learning_rate: 3.503093830659054e-05
Step: 12580, train/epoch: 2.9938125610351562
Step: 12590, train/loss: 0.08739999681711197
Step: 12590, train/grad_norm: 0.012486401945352554
Step: 12590, train/learning_rate: 3.5019038477912545e-05
Step: 12590, train/epoch: 2.996192216873169
Step: 12600, train/loss: 0.21780000627040863
Step: 12600, train/grad_norm: 0.0752929300069809
Step: 12600, train/learning_rate: 3.500713864923455e-05
Step: 12600, train/epoch: 2.9985721111297607
Step: 12606, eval/loss: 0.21918630599975586
Step: 12606, eval/accuracy: 0.9419686198234558
Step: 12606, eval/f1: 0.9393635392189026
Step: 12606, eval/runtime: 296.8627014160156
Step: 12606, eval/samples_per_second: 24.263999938964844
Step: 12606, eval/steps_per_second: 3.0350000858306885
Step: 12606, train/epoch: 3.0
Step: 12610, train/loss: 0.38260000944137573
Step: 12610, train/grad_norm: 0.7887923121452332
Step: 12610, train/learning_rate: 3.499523882055655e-05
Step: 12610, train/epoch: 3.0009520053863525
Step: 12620, train/loss: 0.11829999834299088
Step: 12620, train/grad_norm: 35.5378303527832
Step: 12620, train/learning_rate: 3.498334262985736e-05
Step: 12620, train/epoch: 3.0033316612243652
Step: 12630, train/loss: 0.04820000007748604
Step: 12630, train/grad_norm: 0.11302445083856583
Step: 12630, train/learning_rate: 3.4971442801179364e-05
Step: 12630, train/epoch: 3.005711555480957
Step: 12640, train/loss: 0.04659999907016754
Step: 12640, train/grad_norm: 2.6493513584136963
Step: 12640, train/learning_rate: 3.495954297250137e-05
Step: 12640, train/epoch: 3.008091449737549
Step: 12650, train/loss: 0.08129999786615372
Step: 12650, train/grad_norm: 4.010270118713379
Step: 12650, train/learning_rate: 3.494764314382337e-05
Step: 12650, train/epoch: 3.0104711055755615
Step: 12660, train/loss: 0.21320000290870667
Step: 12660, train/grad_norm: 18.538536071777344
Step: 12660, train/learning_rate: 3.493574331514537e-05
Step: 12660, train/epoch: 3.0128509998321533
Step: 12670, train/loss: 0.04520000144839287
Step: 12670, train/grad_norm: 0.2995372712612152
Step: 12670, train/learning_rate: 3.4923847124446183e-05
Step: 12670, train/epoch: 3.015230894088745
Step: 12680, train/loss: 0.10989999771118164
Step: 12680, train/grad_norm: 0.019589856266975403
Step: 12680, train/learning_rate: 3.4911947295768186e-05
Step: 12680, train/epoch: 3.017610549926758
Step: 12690, train/loss: 0.06830000132322311
Step: 12690, train/grad_norm: 2.391122341156006
Step: 12690, train/learning_rate: 3.490004746709019e-05
Step: 12690, train/epoch: 3.0199904441833496
Step: 12700, train/loss: 0.052299998700618744
Step: 12700, train/grad_norm: 25.801847457885742
Step: 12700, train/learning_rate: 3.488814763841219e-05
Step: 12700, train/epoch: 3.0223703384399414
Step: 12710, train/loss: 0.08290000259876251
Step: 12710, train/grad_norm: 0.24267151951789856
Step: 12710, train/learning_rate: 3.4876247809734195e-05
Step: 12710, train/epoch: 3.024750232696533
Step: 12720, train/loss: 0.08349999785423279
Step: 12720, train/grad_norm: 10.938986778259277
Step: 12720, train/learning_rate: 3.4864351619035006e-05
Step: 12720, train/epoch: 3.027129888534546
Step: 12730, train/loss: 0.21410000324249268
Step: 12730, train/grad_norm: 0.02572695165872574
Step: 12730, train/learning_rate: 3.485245179035701e-05
Step: 12730, train/epoch: 3.0295097827911377
Step: 12740, train/loss: 0.012799999676644802
Step: 12740, train/grad_norm: 0.027483034878969193
Step: 12740, train/learning_rate: 3.484055196167901e-05
Step: 12740, train/epoch: 3.0318896770477295
Step: 12750, train/loss: 0.02879999950528145
Step: 12750, train/grad_norm: 0.7760912775993347
Step: 12750, train/learning_rate: 3.4828652133001015e-05
Step: 12750, train/epoch: 3.034269332885742
Step: 12760, train/loss: 0.09939999878406525
Step: 12760, train/grad_norm: 0.008351778611540794
Step: 12760, train/learning_rate: 3.481675230432302e-05
Step: 12760, train/epoch: 3.036649227142334
Step: 12770, train/loss: 0.05609999969601631
Step: 12770, train/grad_norm: 0.01575620099902153
Step: 12770, train/learning_rate: 3.480485611362383e-05
Step: 12770, train/epoch: 3.039029121398926
Step: 12780, train/loss: 0.004399999976158142
Step: 12780, train/grad_norm: 0.007260410115122795
Step: 12780, train/learning_rate: 3.479295628494583e-05
Step: 12780, train/epoch: 3.0414087772369385
Step: 12790, train/loss: 0.02969999983906746
Step: 12790, train/grad_norm: 0.06840680539608002
Step: 12790, train/learning_rate: 3.4781056456267834e-05
Step: 12790, train/epoch: 3.0437886714935303
Step: 12800, train/loss: 0.0617000013589859
Step: 12800, train/grad_norm: 0.006086973939090967
Step: 12800, train/learning_rate: 3.476915662758984e-05
Step: 12800, train/epoch: 3.046168565750122
Step: 12810, train/loss: 0.0681999996304512
Step: 12810, train/grad_norm: 0.008845215663313866
Step: 12810, train/learning_rate: 3.475725679891184e-05
Step: 12810, train/epoch: 3.0485482215881348
Step: 12820, train/loss: 0.059300001710653305
Step: 12820, train/grad_norm: 0.0005479143001139164
Step: 12820, train/learning_rate: 3.474536060821265e-05
Step: 12820, train/epoch: 3.0509281158447266
Step: 12830, train/loss: 0.0940999984741211
Step: 12830, train/grad_norm: 7.36430549621582
Step: 12830, train/learning_rate: 3.473346077953465e-05
Step: 12830, train/epoch: 3.0533080101013184
Step: 12840, train/loss: 0.07880000025033951
Step: 12840, train/grad_norm: 9.800541877746582
Step: 12840, train/learning_rate: 3.4721560950856656e-05
Step: 12840, train/epoch: 3.055687665939331
Step: 12850, train/loss: 0.057999998331069946
Step: 12850, train/grad_norm: 23.994796752929688
Step: 12850, train/learning_rate: 3.470966112217866e-05
Step: 12850, train/epoch: 3.058067560195923
Step: 12860, train/loss: 0.1177000030875206
Step: 12860, train/grad_norm: 3.1999809741973877
Step: 12860, train/learning_rate: 3.469776129350066e-05
Step: 12860, train/epoch: 3.0604474544525146
Step: 12870, train/loss: 0.06849999725818634
Step: 12870, train/grad_norm: 0.002695953007787466
Step: 12870, train/learning_rate: 3.468586510280147e-05
Step: 12870, train/epoch: 3.0628271102905273
Step: 12880, train/loss: 0.0737999975681305
Step: 12880, train/grad_norm: 3.1028456687927246
Step: 12880, train/learning_rate: 3.4673965274123475e-05
Step: 12880, train/epoch: 3.065207004547119
Step: 12890, train/loss: 0.04410000145435333
Step: 12890, train/grad_norm: 0.004486388061195612
Step: 12890, train/learning_rate: 3.466206544544548e-05
Step: 12890, train/epoch: 3.067586898803711
Step: 12900, train/loss: 0.2556999921798706
Step: 12900, train/grad_norm: 10.017366409301758
Step: 12900, train/learning_rate: 3.465016561676748e-05
Step: 12900, train/epoch: 3.0699667930603027
Step: 12910, train/loss: 0.003100000089034438
Step: 12910, train/grad_norm: 0.04004673659801483
Step: 12910, train/learning_rate: 3.4638265788089484e-05
Step: 12910, train/epoch: 3.0723464488983154
Step: 12920, train/loss: 0.11110000312328339
Step: 12920, train/grad_norm: 0.011552628129720688
Step: 12920, train/learning_rate: 3.4626369597390294e-05
Step: 12920, train/epoch: 3.0747263431549072
Step: 12930, train/loss: 0.3783999979496002
Step: 12930, train/grad_norm: 0.10392606258392334
Step: 12930, train/learning_rate: 3.46144697687123e-05
Step: 12930, train/epoch: 3.077106237411499
Step: 12940, train/loss: 0.039799999445676804
Step: 12940, train/grad_norm: 2.1694347858428955
Step: 12940, train/learning_rate: 3.46025699400343e-05
Step: 12940, train/epoch: 3.0794858932495117
Step: 12950, train/loss: 0.03999999910593033
Step: 12950, train/grad_norm: 25.126001358032227
Step: 12950, train/learning_rate: 3.45906701113563e-05
Step: 12950, train/epoch: 3.0818657875061035
Step: 12960, train/loss: 0.05090000107884407
Step: 12960, train/grad_norm: 11.47039794921875
Step: 12960, train/learning_rate: 3.4578770282678306e-05
Step: 12960, train/epoch: 3.0842456817626953
Step: 12970, train/loss: 0.10989999771118164
Step: 12970, train/grad_norm: 0.2641112506389618
Step: 12970, train/learning_rate: 3.4566874091979116e-05
Step: 12970, train/epoch: 3.086625337600708
Step: 12980, train/loss: 0.0035000001080334187
Step: 12980, train/grad_norm: 0.3212135136127472
Step: 12980, train/learning_rate: 3.455497426330112e-05
Step: 12980, train/epoch: 3.0890052318573
Step: 12990, train/loss: 0.10639999806880951
Step: 12990, train/grad_norm: 30.796701431274414
Step: 12990, train/learning_rate: 3.454307443462312e-05
Step: 12990, train/epoch: 3.0913851261138916
Step: 13000, train/loss: 0.02070000022649765
Step: 13000, train/grad_norm: 0.029661716893315315
Step: 13000, train/learning_rate: 3.4531174605945125e-05
Step: 13000, train/epoch: 3.0937647819519043
Step: 13010, train/loss: 0.0494999997317791
Step: 13010, train/grad_norm: 0.1407390981912613
Step: 13010, train/learning_rate: 3.451927477726713e-05
Step: 13010, train/epoch: 3.096144676208496
Step: 13020, train/loss: 0.22699999809265137
Step: 13020, train/grad_norm: 0.05657380446791649
Step: 13020, train/learning_rate: 3.450737858656794e-05
Step: 13020, train/epoch: 3.098524570465088
Step: 13030, train/loss: 0.006500000134110451
Step: 13030, train/grad_norm: 0.7292041778564453
Step: 13030, train/learning_rate: 3.449547875788994e-05
Step: 13030, train/epoch: 3.1009042263031006
Step: 13040, train/loss: 0.22259999811649323
Step: 13040, train/grad_norm: 22.204946517944336
Step: 13040, train/learning_rate: 3.4483578929211944e-05
Step: 13040, train/epoch: 3.1032841205596924
Step: 13050, train/loss: 0.10170000046491623
Step: 13050, train/grad_norm: 23.974313735961914
Step: 13050, train/learning_rate: 3.447167910053395e-05
Step: 13050, train/epoch: 3.105664014816284
Step: 13060, train/loss: 0.17229999601840973
Step: 13060, train/grad_norm: 0.16066020727157593
Step: 13060, train/learning_rate: 3.445977927185595e-05
Step: 13060, train/epoch: 3.108043670654297
Step: 13070, train/loss: 0.060600001364946365
Step: 13070, train/grad_norm: 0.16255614161491394
Step: 13070, train/learning_rate: 3.444788308115676e-05
Step: 13070, train/epoch: 3.1104235649108887
Step: 13080, train/loss: 0.06390000134706497
Step: 13080, train/grad_norm: 0.013769016601145267
Step: 13080, train/learning_rate: 3.4435983252478763e-05
Step: 13080, train/epoch: 3.1128034591674805
Step: 13090, train/loss: 0.09570000320672989
Step: 13090, train/grad_norm: 0.5524450540542603
Step: 13090, train/learning_rate: 3.4424083423800766e-05
Step: 13090, train/epoch: 3.1151833534240723
Step: 13100, train/loss: 0.16449999809265137
Step: 13100, train/grad_norm: 25.553606033325195
Step: 13100, train/learning_rate: 3.441218359512277e-05
Step: 13100, train/epoch: 3.117563009262085
Step: 13110, train/loss: 0.25519999861717224
Step: 13110, train/grad_norm: 0.0048576779663562775
Step: 13110, train/learning_rate: 3.440028376644477e-05
Step: 13110, train/epoch: 3.1199429035186768
Step: 13120, train/loss: 0.14059999585151672
Step: 13120, train/grad_norm: 37.2563591003418
Step: 13120, train/learning_rate: 3.438838757574558e-05
Step: 13120, train/epoch: 3.1223227977752686
Step: 13130, train/loss: 0.16859999299049377
Step: 13130, train/grad_norm: 0.014888090081512928
Step: 13130, train/learning_rate: 3.4376487747067586e-05
Step: 13130, train/epoch: 3.1247024536132812
Step: 13140, train/loss: 0.13680000603199005
Step: 13140, train/grad_norm: 0.2433331310749054
Step: 13140, train/learning_rate: 3.436458791838959e-05
Step: 13140, train/epoch: 3.127082347869873
Step: 13150, train/loss: 0.05290000140666962
Step: 13150, train/grad_norm: 0.533510684967041
Step: 13150, train/learning_rate: 3.435268808971159e-05
Step: 13150, train/epoch: 3.129462242126465
Step: 13160, train/loss: 0.10010000318288803
Step: 13160, train/grad_norm: 9.133469581604004
Step: 13160, train/learning_rate: 3.43407918990124e-05
Step: 13160, train/epoch: 3.1318418979644775
Step: 13170, train/loss: 0.05000000074505806
Step: 13170, train/grad_norm: 13.11669635772705
Step: 13170, train/learning_rate: 3.4328892070334405e-05
Step: 13170, train/epoch: 3.1342217922210693
Step: 13180, train/loss: 0.05480000004172325
Step: 13180, train/grad_norm: 1.609445333480835
Step: 13180, train/learning_rate: 3.431699224165641e-05
Step: 13180, train/epoch: 3.136601686477661
Step: 13190, train/loss: 0.05999999865889549
Step: 13190, train/grad_norm: 0.04231439530849457
Step: 13190, train/learning_rate: 3.430509241297841e-05
Step: 13190, train/epoch: 3.138981342315674
Step: 13200, train/loss: 0.07289999723434448
Step: 13200, train/grad_norm: 0.34997832775115967
Step: 13200, train/learning_rate: 3.4293192584300414e-05
Step: 13200, train/epoch: 3.1413612365722656
Step: 13210, train/loss: 0.13580000400543213
Step: 13210, train/grad_norm: 1.3336145877838135
Step: 13210, train/learning_rate: 3.4281296393601224e-05
Step: 13210, train/epoch: 3.1437411308288574
Step: 13220, train/loss: 0.1460999995470047
Step: 13220, train/grad_norm: 0.2718825042247772
Step: 13220, train/learning_rate: 3.426939656492323e-05
Step: 13220, train/epoch: 3.14612078666687
Step: 13230, train/loss: 0.10980000346899033
Step: 13230, train/grad_norm: 6.971290111541748
Step: 13230, train/learning_rate: 3.425749673624523e-05
Step: 13230, train/epoch: 3.148500680923462
Step: 13240, train/loss: 0.1168999969959259
Step: 13240, train/grad_norm: 1.3693664073944092
Step: 13240, train/learning_rate: 3.424559690756723e-05
Step: 13240, train/epoch: 3.1508805751800537
Step: 13250, train/loss: 0.09640000015497208
Step: 13250, train/grad_norm: 15.087202072143555
Step: 13250, train/learning_rate: 3.4233697078889236e-05
Step: 13250, train/epoch: 3.1532604694366455
Step: 13260, train/loss: 0.15139999985694885
Step: 13260, train/grad_norm: 0.2557474374771118
Step: 13260, train/learning_rate: 3.4221800888190046e-05
Step: 13260, train/epoch: 3.155640125274658
Step: 13270, train/loss: 0.017799999564886093
Step: 13270, train/grad_norm: 4.029191970825195
Step: 13270, train/learning_rate: 3.420990105951205e-05
Step: 13270, train/epoch: 3.15802001953125
Step: 13280, train/loss: 0.12479999661445618
Step: 13280, train/grad_norm: 0.029366755858063698
Step: 13280, train/learning_rate: 3.419800123083405e-05
Step: 13280, train/epoch: 3.160399913787842
Step: 13290, train/loss: 0.1386999934911728
Step: 13290, train/grad_norm: 0.1454940140247345
Step: 13290, train/learning_rate: 3.4186101402156055e-05
Step: 13290, train/epoch: 3.1627795696258545
Step: 13300, train/loss: 0.060600001364946365
Step: 13300, train/grad_norm: 1.7580575942993164
Step: 13300, train/learning_rate: 3.417420157347806e-05
Step: 13300, train/epoch: 3.1651594638824463
Step: 13310, train/loss: 0.08959999680519104
Step: 13310, train/grad_norm: 1.4399652481079102
Step: 13310, train/learning_rate: 3.416230538277887e-05
Step: 13310, train/epoch: 3.167539358139038
Step: 13320, train/loss: 0.009200000204145908
Step: 13320, train/grad_norm: 0.18725669384002686
Step: 13320, train/learning_rate: 3.415040555410087e-05
Step: 13320, train/epoch: 3.169919013977051
Step: 13330, train/loss: 0.30790001153945923
Step: 13330, train/grad_norm: 0.005919426679611206
Step: 13330, train/learning_rate: 3.4138505725422874e-05
Step: 13330, train/epoch: 3.1722989082336426
Step: 13340, train/loss: 0.13429999351501465
Step: 13340, train/grad_norm: 10.010209083557129
Step: 13340, train/learning_rate: 3.412660589674488e-05
Step: 13340, train/epoch: 3.1746788024902344
Step: 13350, train/loss: 0.019200000911951065
Step: 13350, train/grad_norm: 0.024635884910821915
Step: 13350, train/learning_rate: 3.411470606806688e-05
Step: 13350, train/epoch: 3.177058458328247
Step: 13360, train/loss: 0.10540000349283218
Step: 13360, train/grad_norm: 0.005512390285730362
Step: 13360, train/learning_rate: 3.410280987736769e-05
Step: 13360, train/epoch: 3.179438352584839
Step: 13370, train/loss: 0.08349999785423279
Step: 13370, train/grad_norm: 0.017675133422017097
Step: 13370, train/learning_rate: 3.409091004868969e-05
Step: 13370, train/epoch: 3.1818182468414307
Step: 13380, train/loss: 0.14030000567436218
Step: 13380, train/grad_norm: 23.02018165588379
Step: 13380, train/learning_rate: 3.4079010220011696e-05
Step: 13380, train/epoch: 3.1841979026794434
Step: 13390, train/loss: 0.1429000049829483
Step: 13390, train/grad_norm: 0.03275231271982193
Step: 13390, train/learning_rate: 3.40671103913337e-05
Step: 13390, train/epoch: 3.186577796936035
Step: 13400, train/loss: 0.10819999873638153
Step: 13400, train/grad_norm: 32.52794647216797
Step: 13400, train/learning_rate: 3.40552105626557e-05
Step: 13400, train/epoch: 3.188957691192627
Step: 13410, train/loss: 0.030300000682473183
Step: 13410, train/grad_norm: 0.006324762012809515
Step: 13410, train/learning_rate: 3.404331437195651e-05
Step: 13410, train/epoch: 3.1913373470306396
Step: 13420, train/loss: 0.28690001368522644
Step: 13420, train/grad_norm: 1.4245291948318481
Step: 13420, train/learning_rate: 3.4031414543278515e-05
Step: 13420, train/epoch: 3.1937172412872314
Step: 13430, train/loss: 0.07159999758005142
Step: 13430, train/grad_norm: 0.07812298834323883
Step: 13430, train/learning_rate: 3.401951471460052e-05
Step: 13430, train/epoch: 3.1960971355438232
Step: 13440, train/loss: 0.016699999570846558
Step: 13440, train/grad_norm: 0.012326844967901707
Step: 13440, train/learning_rate: 3.400761488592252e-05
Step: 13440, train/epoch: 3.198477029800415
Step: 13450, train/loss: 0.10109999775886536
Step: 13450, train/grad_norm: 19.881874084472656
Step: 13450, train/learning_rate: 3.3995715057244524e-05
Step: 13450, train/epoch: 3.2008566856384277
Step: 13460, train/loss: 0.15629999339580536
Step: 13460, train/grad_norm: 0.07800991833209991
Step: 13460, train/learning_rate: 3.3983818866545334e-05
Step: 13460, train/epoch: 3.2032365798950195
Step: 13470, train/loss: 0.08500000089406967
Step: 13470, train/grad_norm: 17.094966888427734
Step: 13470, train/learning_rate: 3.397191903786734e-05
Step: 13470, train/epoch: 3.2056164741516113
Step: 13480, train/loss: 0.065700002014637
Step: 13480, train/grad_norm: 0.5071161389350891
Step: 13480, train/learning_rate: 3.396001920918934e-05
Step: 13480, train/epoch: 3.207996129989624
Step: 13490, train/loss: 0.027300000190734863
Step: 13490, train/grad_norm: 0.10522279143333435
Step: 13490, train/learning_rate: 3.3948119380511343e-05
Step: 13490, train/epoch: 3.210376024246216
Step: 13500, train/loss: 0.32989999651908875
Step: 13500, train/grad_norm: 0.0065331291407346725
Step: 13500, train/learning_rate: 3.3936219551833346e-05
Step: 13500, train/epoch: 3.2127559185028076
Step: 13510, train/loss: 0.1088000014424324
Step: 13510, train/grad_norm: 0.3109186589717865
Step: 13510, train/learning_rate: 3.3924323361134157e-05
Step: 13510, train/epoch: 3.2151355743408203
Step: 13520, train/loss: 0.004900000058114529
Step: 13520, train/grad_norm: 0.031196458265185356
Step: 13520, train/learning_rate: 3.391242353245616e-05
Step: 13520, train/epoch: 3.217515468597412
Step: 13530, train/loss: 0.08370000123977661
Step: 13530, train/grad_norm: 0.015515899285674095
Step: 13530, train/learning_rate: 3.390052370377816e-05
Step: 13530, train/epoch: 3.219895362854004
Step: 13540, train/loss: 0.22450000047683716
Step: 13540, train/grad_norm: 0.0023934957571327686
Step: 13540, train/learning_rate: 3.3888623875100166e-05
Step: 13540, train/epoch: 3.2222750186920166
Step: 13550, train/loss: 0.04699999839067459
Step: 13550, train/grad_norm: 13.848362922668457
Step: 13550, train/learning_rate: 3.387672404642217e-05
Step: 13550, train/epoch: 3.2246549129486084
Step: 13560, train/loss: 0.050200000405311584
Step: 13560, train/grad_norm: 0.4437621533870697
Step: 13560, train/learning_rate: 3.386482785572298e-05
Step: 13560, train/epoch: 3.2270348072052
Step: 13570, train/loss: 0.016899999231100082
Step: 13570, train/grad_norm: 0.013638442382216454
Step: 13570, train/learning_rate: 3.385292802704498e-05
Step: 13570, train/epoch: 3.229414463043213
Step: 13580, train/loss: 0.18119999766349792
Step: 13580, train/grad_norm: 0.08596546202898026
Step: 13580, train/learning_rate: 3.3841028198366985e-05
Step: 13580, train/epoch: 3.2317943572998047
Step: 13590, train/loss: 0.1655000001192093
Step: 13590, train/grad_norm: 10.022420883178711
Step: 13590, train/learning_rate: 3.382912836968899e-05
Step: 13590, train/epoch: 3.2341742515563965
Step: 13600, train/loss: 0.3012999892234802
Step: 13600, train/grad_norm: 0.026026032865047455
Step: 13600, train/learning_rate: 3.381722854101099e-05
Step: 13600, train/epoch: 3.236553907394409
Step: 13610, train/loss: 0.2371000051498413
Step: 13610, train/grad_norm: 23.36912727355957
Step: 13610, train/learning_rate: 3.38053323503118e-05
Step: 13610, train/epoch: 3.238933801651001
Step: 13620, train/loss: 0.020099999383091927
Step: 13620, train/grad_norm: 4.335780143737793
Step: 13620, train/learning_rate: 3.3793432521633804e-05
Step: 13620, train/epoch: 3.2413136959075928
Step: 13630, train/loss: 0.2126999944448471
Step: 13630, train/grad_norm: 0.23547425866127014
Step: 13630, train/learning_rate: 3.378153269295581e-05
Step: 13630, train/epoch: 3.2436935901641846
Step: 13640, train/loss: 0.18940000236034393
Step: 13640, train/grad_norm: 1.437495470046997
Step: 13640, train/learning_rate: 3.376963286427781e-05
Step: 13640, train/epoch: 3.2460732460021973
Step: 13650, train/loss: 0.0877000018954277
Step: 13650, train/grad_norm: 0.004324480425566435
Step: 13650, train/learning_rate: 3.375773303559981e-05
Step: 13650, train/epoch: 3.248453140258789
Step: 13660, train/loss: 0.16580000519752502
Step: 13660, train/grad_norm: 0.42593276500701904
Step: 13660, train/learning_rate: 3.374583684490062e-05
Step: 13660, train/epoch: 3.250833034515381
Step: 13670, train/loss: 0.42739999294281006
Step: 13670, train/grad_norm: 23.158519744873047
Step: 13670, train/learning_rate: 3.3733937016222626e-05
Step: 13670, train/epoch: 3.2532126903533936
Step: 13680, train/loss: 0.015200000256299973
Step: 13680, train/grad_norm: 0.0738542377948761
Step: 13680, train/learning_rate: 3.372203718754463e-05
Step: 13680, train/epoch: 3.2555925846099854
Step: 13690, train/loss: 0.07739999890327454
Step: 13690, train/grad_norm: 11.920426368713379
Step: 13690, train/learning_rate: 3.371013735886663e-05
Step: 13690, train/epoch: 3.257972478866577
Step: 13700, train/loss: 0.09130000323057175
Step: 13700, train/grad_norm: 0.005761855747550726
Step: 13700, train/learning_rate: 3.3698237530188635e-05
Step: 13700, train/epoch: 3.26035213470459
Step: 13710, train/loss: 0.09790000319480896
Step: 13710, train/grad_norm: 0.013273974880576134
Step: 13710, train/learning_rate: 3.3686341339489445e-05
Step: 13710, train/epoch: 3.2627320289611816
Step: 13720, train/loss: 0.06449999660253525
Step: 13720, train/grad_norm: 0.02621210739016533
Step: 13720, train/learning_rate: 3.367444151081145e-05
Step: 13720, train/epoch: 3.2651119232177734
Step: 13730, train/loss: 0.1818999946117401
Step: 13730, train/grad_norm: 1.5354737043380737
Step: 13730, train/learning_rate: 3.366254168213345e-05
Step: 13730, train/epoch: 3.267491579055786
Step: 13740, train/loss: 0.38940000534057617
Step: 13740, train/grad_norm: 0.367313414812088
Step: 13740, train/learning_rate: 3.3650641853455454e-05
Step: 13740, train/epoch: 3.269871473312378
Step: 13750, train/loss: 0.03880000114440918
Step: 13750, train/grad_norm: 0.002830242272466421
Step: 13750, train/learning_rate: 3.363874202477746e-05
Step: 13750, train/epoch: 3.2722513675689697
Step: 13760, train/loss: 0.002899999963119626
Step: 13760, train/grad_norm: 4.090723991394043
Step: 13760, train/learning_rate: 3.362684583407827e-05
Step: 13760, train/epoch: 3.2746310234069824
Step: 13770, train/loss: 0.04470000043511391
Step: 13770, train/grad_norm: 0.11013930290937424
Step: 13770, train/learning_rate: 3.361494600540027e-05
Step: 13770, train/epoch: 3.277010917663574
Step: 13780, train/loss: 0.20649999380111694
Step: 13780, train/grad_norm: 0.09255971014499664
Step: 13780, train/learning_rate: 3.360304617672227e-05
Step: 13780, train/epoch: 3.279390811920166
Step: 13790, train/loss: 0.2935999929904938
Step: 13790, train/grad_norm: 15.091324806213379
Step: 13790, train/learning_rate: 3.3591146348044276e-05
Step: 13790, train/epoch: 3.2817704677581787
Step: 13800, train/loss: 0.05290000140666962
Step: 13800, train/grad_norm: 4.307624816894531
Step: 13800, train/learning_rate: 3.357924651936628e-05
Step: 13800, train/epoch: 3.2841503620147705
Step: 13810, train/loss: 0.03959999978542328
Step: 13810, train/grad_norm: 1.3123791217803955
Step: 13810, train/learning_rate: 3.356735032866709e-05
Step: 13810, train/epoch: 3.2865302562713623
Step: 13820, train/loss: 0.11599999666213989
Step: 13820, train/grad_norm: 3.9292452335357666
Step: 13820, train/learning_rate: 3.355545049998909e-05
Step: 13820, train/epoch: 3.288910150527954
Step: 13830, train/loss: 0.10689999908208847
Step: 13830, train/grad_norm: 0.004701279569417238
Step: 13830, train/learning_rate: 3.3543550671311095e-05
Step: 13830, train/epoch: 3.291289806365967
Step: 13840, train/loss: 0.12890000641345978
Step: 13840, train/grad_norm: 0.06638918071985245
Step: 13840, train/learning_rate: 3.35316508426331e-05
Step: 13840, train/epoch: 3.2936697006225586
Step: 13850, train/loss: 0.2214999943971634
Step: 13850, train/grad_norm: 18.777490615844727
Step: 13850, train/learning_rate: 3.35197510139551e-05
Step: 13850, train/epoch: 3.2960495948791504
Step: 13860, train/loss: 0.07840000092983246
Step: 13860, train/grad_norm: 17.953882217407227
Step: 13860, train/learning_rate: 3.350785482325591e-05
Step: 13860, train/epoch: 3.298429250717163
Step: 13870, train/loss: 0.054099999368190765
Step: 13870, train/grad_norm: 6.618526458740234
Step: 13870, train/learning_rate: 3.3495954994577914e-05
Step: 13870, train/epoch: 3.300809144973755
Step: 13880, train/loss: 0.02319999970495701
Step: 13880, train/grad_norm: 0.021419838070869446
Step: 13880, train/learning_rate: 3.348405516589992e-05
Step: 13880, train/epoch: 3.3031890392303467
Step: 13890, train/loss: 0.16869999468326569
Step: 13890, train/grad_norm: 2.180269479751587
Step: 13890, train/learning_rate: 3.347215533722192e-05
Step: 13890, train/epoch: 3.3055686950683594
Step: 13900, train/loss: 0.07240000367164612
Step: 13900, train/grad_norm: 1.496987223625183
Step: 13900, train/learning_rate: 3.3460255508543923e-05
Step: 13900, train/epoch: 3.307948589324951
Step: 13910, train/loss: 0.005900000222027302
Step: 13910, train/grad_norm: 0.021659640595316887
Step: 13910, train/learning_rate: 3.3448359317844734e-05
Step: 13910, train/epoch: 3.310328483581543
Step: 13920, train/loss: 0.11649999767541885
Step: 13920, train/grad_norm: 9.201940536499023
Step: 13920, train/learning_rate: 3.3436459489166737e-05
Step: 13920, train/epoch: 3.3127081394195557
Step: 13930, train/loss: 0.05090000107884407
Step: 13930, train/grad_norm: 0.016217220574617386
Step: 13930, train/learning_rate: 3.342455966048874e-05
Step: 13930, train/epoch: 3.3150880336761475
Step: 13940, train/loss: 0.18479999899864197
Step: 13940, train/grad_norm: 55.78770446777344
Step: 13940, train/learning_rate: 3.341265983181074e-05
Step: 13940, train/epoch: 3.3174679279327393
Step: 13950, train/loss: 0.007199999876320362
Step: 13950, train/grad_norm: 0.05172013118863106
Step: 13950, train/learning_rate: 3.3400760003132746e-05
Step: 13950, train/epoch: 3.319847583770752
Step: 13960, train/loss: 0.045499999076128006
Step: 13960, train/grad_norm: 0.03181025758385658
Step: 13960, train/learning_rate: 3.3388863812433556e-05
Step: 13960, train/epoch: 3.3222274780273438
Step: 13970, train/loss: 0.0957999974489212
Step: 13970, train/grad_norm: 20.3990421295166
Step: 13970, train/learning_rate: 3.337696398375556e-05
Step: 13970, train/epoch: 3.3246073722839355
Step: 13980, train/loss: 0.12960000336170197
Step: 13980, train/grad_norm: 0.041862376034259796
Step: 13980, train/learning_rate: 3.336506415507756e-05
Step: 13980, train/epoch: 3.3269872665405273
Step: 13990, train/loss: 0.21559999883174896
Step: 13990, train/grad_norm: 7.221917152404785
Step: 13990, train/learning_rate: 3.3353164326399565e-05
Step: 13990, train/epoch: 3.32936692237854
Step: 14000, train/loss: 0.23420000076293945
Step: 14000, train/grad_norm: 32.01505661010742
Step: 14000, train/learning_rate: 3.334126449772157e-05
Step: 14000, train/epoch: 3.331746816635132
Step: 14010, train/loss: 0.0803999975323677
Step: 14010, train/grad_norm: 1.9804229736328125
Step: 14010, train/learning_rate: 3.332936830702238e-05
Step: 14010, train/epoch: 3.3341267108917236
Step: 14020, train/loss: 0.04839999973773956
Step: 14020, train/grad_norm: 0.017733268439769745
Step: 14020, train/learning_rate: 3.331746847834438e-05
Step: 14020, train/epoch: 3.3365063667297363
Step: 14030, train/loss: 0.1185000017285347
Step: 14030, train/grad_norm: 0.05450928583741188
Step: 14030, train/learning_rate: 3.3305568649666384e-05
Step: 14030, train/epoch: 3.338886260986328
Step: 14040, train/loss: 0.0771000012755394
Step: 14040, train/grad_norm: 3.5829782485961914
Step: 14040, train/learning_rate: 3.329366882098839e-05
Step: 14040, train/epoch: 3.34126615524292
Step: 14050, train/loss: 0.042100001126527786
Step: 14050, train/grad_norm: 13.207128524780273
Step: 14050, train/learning_rate: 3.328176899231039e-05
Step: 14050, train/epoch: 3.3436458110809326
Step: 14060, train/loss: 0.026599999517202377
Step: 14060, train/grad_norm: 0.15244115889072418
Step: 14060, train/learning_rate: 3.32698728016112e-05
Step: 14060, train/epoch: 3.3460257053375244
Step: 14070, train/loss: 0.11670000106096268
Step: 14070, train/grad_norm: 0.014765491709113121
Step: 14070, train/learning_rate: 3.32579729729332e-05
Step: 14070, train/epoch: 3.348405599594116
Step: 14080, train/loss: 0.17219999432563782
Step: 14080, train/grad_norm: 0.09263774007558823
Step: 14080, train/learning_rate: 3.3246073144255206e-05
Step: 14080, train/epoch: 3.350785255432129
Step: 14090, train/loss: 0.006800000090152025
Step: 14090, train/grad_norm: 0.2556067407131195
Step: 14090, train/learning_rate: 3.323417331557721e-05
Step: 14090, train/epoch: 3.3531651496887207
Step: 14100, train/loss: 0.03590000048279762
Step: 14100, train/grad_norm: 0.0038292838726192713
Step: 14100, train/learning_rate: 3.322227348689921e-05
Step: 14100, train/epoch: 3.3555450439453125
Step: 14110, train/loss: 0.06790000200271606
Step: 14110, train/grad_norm: 1.6197720766067505
Step: 14110, train/learning_rate: 3.321037729620002e-05
Step: 14110, train/epoch: 3.357924699783325
Step: 14120, train/loss: 0.05079999938607216
Step: 14120, train/grad_norm: 0.0037342619616538286
Step: 14120, train/learning_rate: 3.3198477467522025e-05
Step: 14120, train/epoch: 3.360304594039917
Step: 14130, train/loss: 0.14259999990463257
Step: 14130, train/grad_norm: 0.030842747539281845
Step: 14130, train/learning_rate: 3.318657763884403e-05
Step: 14130, train/epoch: 3.362684488296509
Step: 14140, train/loss: 0.06440000236034393
Step: 14140, train/grad_norm: 13.495135307312012
Step: 14140, train/learning_rate: 3.317467781016603e-05
Step: 14140, train/epoch: 3.3650641441345215
Step: 14150, train/loss: 0.15049999952316284
Step: 14150, train/grad_norm: 0.00614131148904562
Step: 14150, train/learning_rate: 3.3162777981488034e-05
Step: 14150, train/epoch: 3.3674440383911133
Step: 14160, train/loss: 0.09669999778270721
Step: 14160, train/grad_norm: 2.1313347816467285
Step: 14160, train/learning_rate: 3.3150881790788844e-05
Step: 14160, train/epoch: 3.369823932647705
Step: 14170, train/loss: 0.0925000011920929
Step: 14170, train/grad_norm: 0.05638701468706131
Step: 14170, train/learning_rate: 3.313898196211085e-05
Step: 14170, train/epoch: 3.372203826904297
Step: 14180, train/loss: 0.06589999794960022
Step: 14180, train/grad_norm: 28.333513259887695
Step: 14180, train/learning_rate: 3.312708213343285e-05
Step: 14180, train/epoch: 3.3745834827423096
Step: 14190, train/loss: 0.06939999759197235
Step: 14190, train/grad_norm: 65.31941223144531
Step: 14190, train/learning_rate: 3.311518230475485e-05
Step: 14190, train/epoch: 3.3769633769989014
Step: 14200, train/loss: 0.08139999955892563
Step: 14200, train/grad_norm: 0.0035353440325707197
Step: 14200, train/learning_rate: 3.3103282476076856e-05
Step: 14200, train/epoch: 3.379343271255493
Step: 14210, train/loss: 0.1151999980211258
Step: 14210, train/grad_norm: 0.11920780688524246
Step: 14210, train/learning_rate: 3.3091386285377666e-05
Step: 14210, train/epoch: 3.381722927093506
Step: 14220, train/loss: 0.0034000000450760126
Step: 14220, train/grad_norm: 1.6515916585922241
Step: 14220, train/learning_rate: 3.307948645669967e-05
Step: 14220, train/epoch: 3.3841028213500977
Step: 14230, train/loss: 0.06120000034570694
Step: 14230, train/grad_norm: 0.0073310621082782745
Step: 14230, train/learning_rate: 3.306758662802167e-05
Step: 14230, train/epoch: 3.3864827156066895
Step: 14240, train/loss: 0.13600000739097595
Step: 14240, train/grad_norm: 0.005828627850860357
Step: 14240, train/learning_rate: 3.3055686799343675e-05
Step: 14240, train/epoch: 3.388862371444702
Step: 14250, train/loss: 0.04690000042319298
Step: 14250, train/grad_norm: 0.021760014817118645
Step: 14250, train/learning_rate: 3.304378697066568e-05
Step: 14250, train/epoch: 3.391242265701294
Step: 14260, train/loss: 0.14350000023841858
Step: 14260, train/grad_norm: 0.8697156310081482
Step: 14260, train/learning_rate: 3.303189077996649e-05
Step: 14260, train/epoch: 3.3936221599578857
Step: 14270, train/loss: 0.07370000332593918
Step: 14270, train/grad_norm: 0.000857573701068759
Step: 14270, train/learning_rate: 3.301999095128849e-05
Step: 14270, train/epoch: 3.3960018157958984
Step: 14280, train/loss: 0.15039999783039093
Step: 14280, train/grad_norm: 0.015687892213463783
Step: 14280, train/learning_rate: 3.3008091122610494e-05
Step: 14280, train/epoch: 3.3983817100524902
Step: 14290, train/loss: 0.08449999988079071
Step: 14290, train/grad_norm: 0.2246563732624054
Step: 14290, train/learning_rate: 3.29961912939325e-05
Step: 14290, train/epoch: 3.400761604309082
Step: 14300, train/loss: 0.03759999945759773
Step: 14300, train/grad_norm: 35.88844299316406
Step: 14300, train/learning_rate: 3.29842914652545e-05
Step: 14300, train/epoch: 3.4031412601470947
Step: 14310, train/loss: 0.10769999772310257
Step: 14310, train/grad_norm: 0.016517456620931625
Step: 14310, train/learning_rate: 3.297239527455531e-05
Step: 14310, train/epoch: 3.4055211544036865
Step: 14320, train/loss: 0.24529999494552612
Step: 14320, train/grad_norm: 0.010484898462891579
Step: 14320, train/learning_rate: 3.2960495445877314e-05
Step: 14320, train/epoch: 3.4079010486602783
Step: 14330, train/loss: 0.07919999957084656
Step: 14330, train/grad_norm: 0.2848079204559326
Step: 14330, train/learning_rate: 3.294859561719932e-05
Step: 14330, train/epoch: 3.410280704498291
Step: 14340, train/loss: 0.17960000038146973
Step: 14340, train/grad_norm: 15.322920799255371
Step: 14340, train/learning_rate: 3.293669578852132e-05
Step: 14340, train/epoch: 3.412660598754883
Step: 14350, train/loss: 0.01209999993443489
Step: 14350, train/grad_norm: 0.0021321328822523355
Step: 14350, train/learning_rate: 3.292479595984332e-05
Step: 14350, train/epoch: 3.4150404930114746
Step: 14360, train/loss: 0.00570000009611249
Step: 14360, train/grad_norm: 3.0791518688201904
Step: 14360, train/learning_rate: 3.291289976914413e-05
Step: 14360, train/epoch: 3.4174203872680664
Step: 14370, train/loss: 0.19449999928474426
Step: 14370, train/grad_norm: 0.07160963118076324
Step: 14370, train/learning_rate: 3.2900999940466136e-05
Step: 14370, train/epoch: 3.419800043106079
Step: 14380, train/loss: 0.14710000157356262
Step: 14380, train/grad_norm: 0.062376197427511215
Step: 14380, train/learning_rate: 3.288910011178814e-05
Step: 14380, train/epoch: 3.422179937362671
Step: 14390, train/loss: 0.06759999692440033
Step: 14390, train/grad_norm: 0.010511725209653378
Step: 14390, train/learning_rate: 3.287720028311014e-05
Step: 14390, train/epoch: 3.4245598316192627
Step: 14400, train/loss: 0.15219999849796295
Step: 14400, train/grad_norm: 1.6372345685958862
Step: 14400, train/learning_rate: 3.2865300454432145e-05
Step: 14400, train/epoch: 3.4269394874572754
Step: 14410, train/loss: 0.12939999997615814
Step: 14410, train/grad_norm: 0.01754014939069748
Step: 14410, train/learning_rate: 3.2853404263732955e-05
Step: 14410, train/epoch: 3.429319381713867
Step: 14420, train/loss: 0.07400000095367432
Step: 14420, train/grad_norm: 0.018561728298664093
Step: 14420, train/learning_rate: 3.284150443505496e-05
Step: 14420, train/epoch: 3.431699275970459
Step: 14430, train/loss: 0.16050000488758087
Step: 14430, train/grad_norm: 7.4543304443359375
Step: 14430, train/learning_rate: 3.282960460637696e-05
Step: 14430, train/epoch: 3.4340789318084717
Step: 14440, train/loss: 0.19099999964237213
Step: 14440, train/grad_norm: 31.31737518310547
Step: 14440, train/learning_rate: 3.2817704777698964e-05
Step: 14440, train/epoch: 3.4364588260650635
Step: 14450, train/loss: 0.06319999694824219
Step: 14450, train/grad_norm: 19.949831008911133
Step: 14450, train/learning_rate: 3.280580494902097e-05
Step: 14450, train/epoch: 3.4388387203216553
Step: 14460, train/loss: 0.3862000107765198
Step: 14460, train/grad_norm: 24.90964698791504
Step: 14460, train/learning_rate: 3.279390875832178e-05
Step: 14460, train/epoch: 3.441218376159668
Step: 14470, train/loss: 0.08910000324249268
Step: 14470, train/grad_norm: 0.002658646088093519
Step: 14470, train/learning_rate: 3.278200892964378e-05
Step: 14470, train/epoch: 3.4435982704162598
Step: 14480, train/loss: 0.11270000040531158
Step: 14480, train/grad_norm: 14.885716438293457
Step: 14480, train/learning_rate: 3.277010910096578e-05
Step: 14480, train/epoch: 3.4459781646728516
Step: 14490, train/loss: 0.02669999934732914
Step: 14490, train/grad_norm: 0.0518827810883522
Step: 14490, train/learning_rate: 3.2758209272287786e-05
Step: 14490, train/epoch: 3.4483578205108643
Step: 14500, train/loss: 0.004900000058114529
Step: 14500, train/grad_norm: 0.014537875540554523
Step: 14500, train/learning_rate: 3.2746313081588596e-05
Step: 14500, train/epoch: 3.450737714767456
Step: 14510, train/loss: 0.1387999951839447
Step: 14510, train/grad_norm: 0.004089769907295704
Step: 14510, train/learning_rate: 3.27344132529106e-05
Step: 14510, train/epoch: 3.453117609024048
Step: 14520, train/loss: 0.01590000092983246
Step: 14520, train/grad_norm: 0.013508602045476437
Step: 14520, train/learning_rate: 3.27225134242326e-05
Step: 14520, train/epoch: 3.4554972648620605
Step: 14530, train/loss: 0.06430000066757202
Step: 14530, train/grad_norm: 0.36541005969047546
Step: 14530, train/learning_rate: 3.2710613595554605e-05
Step: 14530, train/epoch: 3.4578771591186523
Step: 14540, train/loss: 0.01360000018030405
Step: 14540, train/grad_norm: 13.904325485229492
Step: 14540, train/learning_rate: 3.269871376687661e-05
Step: 14540, train/epoch: 3.460257053375244
Step: 14550, train/loss: 0.021299999207258224
Step: 14550, train/grad_norm: 12.18765926361084
Step: 14550, train/learning_rate: 3.268681757617742e-05
Step: 14550, train/epoch: 3.462636947631836
Step: 14560, train/loss: 0.20319999754428864
Step: 14560, train/grad_norm: 0.5933035612106323
Step: 14560, train/learning_rate: 3.267491774749942e-05
Step: 14560, train/epoch: 3.4650166034698486
Step: 14570, train/loss: 0.15440000593662262
Step: 14570, train/grad_norm: 26.60295867919922
Step: 14570, train/learning_rate: 3.2663017918821424e-05
Step: 14570, train/epoch: 3.4673964977264404
Step: 14580, train/loss: 0.20630000531673431
Step: 14580, train/grad_norm: 0.05829611048102379
Step: 14580, train/learning_rate: 3.265111809014343e-05
Step: 14580, train/epoch: 3.4697763919830322
Step: 14590, train/loss: 0.16439999639987946
Step: 14590, train/grad_norm: 6.602424144744873
Step: 14590, train/learning_rate: 3.263921826146543e-05
Step: 14590, train/epoch: 3.472156047821045
Step: 14600, train/loss: 0.1720000058412552
Step: 14600, train/grad_norm: 0.3468360900878906
Step: 14600, train/learning_rate: 3.262732207076624e-05
Step: 14600, train/epoch: 3.4745359420776367
Step: 14610, train/loss: 0.011900000274181366
Step: 14610, train/grad_norm: 1.3383355140686035
Step: 14610, train/learning_rate: 3.261542224208824e-05
Step: 14610, train/epoch: 3.4769158363342285
Step: 14620, train/loss: 0.011599999852478504
Step: 14620, train/grad_norm: 4.595609664916992
Step: 14620, train/learning_rate: 3.2603522413410246e-05
Step: 14620, train/epoch: 3.479295492172241
Step: 14630, train/loss: 0.020400000736117363
Step: 14630, train/grad_norm: 0.005236628465354443
Step: 14630, train/learning_rate: 3.259162258473225e-05
Step: 14630, train/epoch: 3.481675386428833
Step: 14640, train/loss: 0.04749999940395355
Step: 14640, train/grad_norm: 16.461585998535156
Step: 14640, train/learning_rate: 3.257972275605425e-05
Step: 14640, train/epoch: 3.484055280685425
Step: 14650, train/loss: 0.1005999967455864
Step: 14650, train/grad_norm: 15.863990783691406
Step: 14650, train/learning_rate: 3.256782656535506e-05
Step: 14650, train/epoch: 3.4864349365234375
Step: 14660, train/loss: 0.06480000168085098
Step: 14660, train/grad_norm: 0.565339207649231
Step: 14660, train/learning_rate: 3.2555926736677065e-05
Step: 14660, train/epoch: 3.4888148307800293
Step: 14670, train/loss: 0.538100004196167
Step: 14670, train/grad_norm: 0.005014515481889248
Step: 14670, train/learning_rate: 3.254402690799907e-05
Step: 14670, train/epoch: 3.491194725036621
Step: 14680, train/loss: 0.18289999663829803
Step: 14680, train/grad_norm: 14.644864082336426
Step: 14680, train/learning_rate: 3.253212707932107e-05
Step: 14680, train/epoch: 3.493574380874634
Step: 14690, train/loss: 0.13330000638961792
Step: 14690, train/grad_norm: 14.306684494018555
Step: 14690, train/learning_rate: 3.2520227250643075e-05
Step: 14690, train/epoch: 3.4959542751312256
Step: 14700, train/loss: 0.10830000042915344
Step: 14700, train/grad_norm: 24.691734313964844
Step: 14700, train/learning_rate: 3.2508331059943885e-05
Step: 14700, train/epoch: 3.4983341693878174
Step: 14710, train/loss: 0.054499998688697815
Step: 14710, train/grad_norm: 5.095062732696533
Step: 14710, train/learning_rate: 3.249643123126589e-05
Step: 14710, train/epoch: 3.500714063644409
Step: 14720, train/loss: 0.030899999663233757
Step: 14720, train/grad_norm: 5.893198013305664
Step: 14720, train/learning_rate: 3.248453140258789e-05
Step: 14720, train/epoch: 3.503093719482422
Step: 14730, train/loss: 0.08560000360012054
Step: 14730, train/grad_norm: 0.06086592376232147
Step: 14730, train/learning_rate: 3.2472631573909894e-05
Step: 14730, train/epoch: 3.5054736137390137
Step: 14740, train/loss: 0.16179999709129333
Step: 14740, train/grad_norm: 0.054145101457834244
Step: 14740, train/learning_rate: 3.24607317452319e-05
Step: 14740, train/epoch: 3.5078535079956055
Step: 14750, train/loss: 0.10189999639987946
Step: 14750, train/grad_norm: 0.09165526926517487
Step: 14750, train/learning_rate: 3.244883555453271e-05
Step: 14750, train/epoch: 3.510233163833618
Step: 14760, train/loss: 0.08150000125169754
Step: 14760, train/grad_norm: 0.23724320530891418
Step: 14760, train/learning_rate: 3.243693572585471e-05
Step: 14760, train/epoch: 3.51261305809021
Step: 14770, train/loss: 0.016899999231100082
Step: 14770, train/grad_norm: 0.47813570499420166
Step: 14770, train/learning_rate: 3.242503589717671e-05
Step: 14770, train/epoch: 3.5149929523468018
Step: 14780, train/loss: 0.10360000282526016
Step: 14780, train/grad_norm: 9.890722274780273
Step: 14780, train/learning_rate: 3.2413136068498716e-05
Step: 14780, train/epoch: 3.5173726081848145
Step: 14790, train/loss: 0.09260000288486481
Step: 14790, train/grad_norm: 0.019453015178442
Step: 14790, train/learning_rate: 3.240123623982072e-05
Step: 14790, train/epoch: 3.5197525024414062
Step: 14800, train/loss: 0.05609999969601631
Step: 14800, train/grad_norm: 4.851080894470215
Step: 14800, train/learning_rate: 3.238934004912153e-05
Step: 14800, train/epoch: 3.522132396697998
Step: 14810, train/loss: 0.04699999839067459
Step: 14810, train/grad_norm: 32.16681671142578
Step: 14810, train/learning_rate: 3.237744022044353e-05
Step: 14810, train/epoch: 3.5245120525360107
Step: 14820, train/loss: 0.05469999834895134
Step: 14820, train/grad_norm: 6.920374393463135
Step: 14820, train/learning_rate: 3.2365540391765535e-05
Step: 14820, train/epoch: 3.5268919467926025
Step: 14830, train/loss: 0.05290000140666962
Step: 14830, train/grad_norm: 0.05649377778172493
Step: 14830, train/learning_rate: 3.235364056308754e-05
Step: 14830, train/epoch: 3.5292718410491943
Step: 14840, train/loss: 0.050200000405311584
Step: 14840, train/grad_norm: 0.6773974299430847
Step: 14840, train/learning_rate: 3.234174073440954e-05
Step: 14840, train/epoch: 3.531651496887207
Step: 14850, train/loss: 0.044599998742341995
Step: 14850, train/grad_norm: 23.110811233520508
Step: 14850, train/learning_rate: 3.232984454371035e-05
Step: 14850, train/epoch: 3.534031391143799
Step: 14860, train/loss: 0.0877000018954277
Step: 14860, train/grad_norm: 26.133514404296875
Step: 14860, train/learning_rate: 3.2317944715032354e-05
Step: 14860, train/epoch: 3.5364112854003906
Step: 14870, train/loss: 0.10260000079870224
Step: 14870, train/grad_norm: 0.4480533301830292
Step: 14870, train/learning_rate: 3.230604488635436e-05
Step: 14870, train/epoch: 3.5387909412384033
Step: 14880, train/loss: 0.2858000099658966
Step: 14880, train/grad_norm: 0.12002034485340118
Step: 14880, train/learning_rate: 3.229414505767636e-05
Step: 14880, train/epoch: 3.541170835494995
Step: 14890, train/loss: 0.17919999361038208
Step: 14890, train/grad_norm: 29.560956954956055
Step: 14890, train/learning_rate: 3.228224522899836e-05
Step: 14890, train/epoch: 3.543550729751587
Step: 14900, train/loss: 0.09290000051259995
Step: 14900, train/grad_norm: 0.005419822875410318
Step: 14900, train/learning_rate: 3.227034903829917e-05
Step: 14900, train/epoch: 3.5459306240081787
Step: 14910, train/loss: 0.13539999723434448
Step: 14910, train/grad_norm: 0.008781402371823788
Step: 14910, train/learning_rate: 3.2258449209621176e-05
Step: 14910, train/epoch: 3.5483102798461914
Step: 14920, train/loss: 0.12150000035762787
Step: 14920, train/grad_norm: 49.81638717651367
Step: 14920, train/learning_rate: 3.224654938094318e-05
Step: 14920, train/epoch: 3.550690174102783
Step: 14930, train/loss: 0.12080000340938568
Step: 14930, train/grad_norm: 0.010610805824398994
Step: 14930, train/learning_rate: 3.223464955226518e-05
Step: 14930, train/epoch: 3.553070068359375
Step: 14940, train/loss: 0.17870000004768372
Step: 14940, train/grad_norm: 0.09706435352563858
Step: 14940, train/learning_rate: 3.2222749723587185e-05
Step: 14940, train/epoch: 3.5554497241973877
Step: 14950, train/loss: 0.0044999998062849045
Step: 14950, train/grad_norm: 1.4635546207427979
Step: 14950, train/learning_rate: 3.2210853532887995e-05
Step: 14950, train/epoch: 3.5578296184539795
Step: 14960, train/loss: 0.027699999511241913
Step: 14960, train/grad_norm: 26.982162475585938
Step: 14960, train/learning_rate: 3.219895370421e-05
Step: 14960, train/epoch: 3.5602095127105713
Step: 14970, train/loss: 0.1890999972820282
Step: 14970, train/grad_norm: 13.401178359985352
Step: 14970, train/learning_rate: 3.2187053875532e-05
Step: 14970, train/epoch: 3.562589168548584
Step: 14980, train/loss: 0.037300001829862595
Step: 14980, train/grad_norm: 0.001240551471710205
Step: 14980, train/learning_rate: 3.2175154046854004e-05
Step: 14980, train/epoch: 3.564969062805176
Step: 14990, train/loss: 0.05779999867081642
Step: 14990, train/grad_norm: 0.020853104069828987
Step: 14990, train/learning_rate: 3.216325421817601e-05
Step: 14990, train/epoch: 3.5673489570617676
Step: 15000, train/loss: 0.048700001090765
Step: 15000, train/grad_norm: 0.008022142574191093
Step: 15000, train/learning_rate: 3.215135802747682e-05
Step: 15000, train/epoch: 3.5697286128997803
Step: 15010, train/loss: 0.11400000005960464
Step: 15010, train/grad_norm: 0.007106421049684286
Step: 15010, train/learning_rate: 3.213945819879882e-05
Step: 15010, train/epoch: 3.572108507156372
Step: 15020, train/loss: 0.18289999663829803
Step: 15020, train/grad_norm: 0.326028436422348
Step: 15020, train/learning_rate: 3.212755837012082e-05
Step: 15020, train/epoch: 3.574488401412964
Step: 15030, train/loss: 0.013399999588727951
Step: 15030, train/grad_norm: 0.07727095484733582
Step: 15030, train/learning_rate: 3.2115658541442826e-05
Step: 15030, train/epoch: 3.5768680572509766
Step: 15040, train/loss: 0.09679999947547913
Step: 15040, train/grad_norm: 6.11140251159668
Step: 15040, train/learning_rate: 3.210375871276483e-05
Step: 15040, train/epoch: 3.5792479515075684
Step: 15050, train/loss: 0.09860000014305115
Step: 15050, train/grad_norm: 0.017741860821843147
Step: 15050, train/learning_rate: 3.209186252206564e-05
Step: 15050, train/epoch: 3.58162784576416
Step: 15060, train/loss: 0.0031999999191612005
Step: 15060, train/grad_norm: 1.936025619506836
Step: 15060, train/learning_rate: 3.207996269338764e-05
Step: 15060, train/epoch: 3.584007501602173
Step: 15070, train/loss: 0.0754999965429306
Step: 15070, train/grad_norm: 0.0019693910144269466
Step: 15070, train/learning_rate: 3.2068062864709646e-05
Step: 15070, train/epoch: 3.5863873958587646
Step: 15080, train/loss: 0.10140000283718109
Step: 15080, train/grad_norm: 0.005905333906412125
Step: 15080, train/learning_rate: 3.205616303603165e-05
Step: 15080, train/epoch: 3.5887672901153564
Step: 15090, train/loss: 0.1688999980688095
Step: 15090, train/grad_norm: 0.03422347456216812
Step: 15090, train/learning_rate: 3.204426320735365e-05
Step: 15090, train/epoch: 3.5911471843719482
Step: 15100, train/loss: 0.09589999914169312
Step: 15100, train/grad_norm: 0.009761177934706211
Step: 15100, train/learning_rate: 3.203236701665446e-05
Step: 15100, train/epoch: 3.593526840209961
Step: 15110, train/loss: 0.00839999970048666
Step: 15110, train/grad_norm: 0.3574380576610565
Step: 15110, train/learning_rate: 3.2020467187976465e-05
Step: 15110, train/epoch: 3.5959067344665527
Step: 15120, train/loss: 0.01720000058412552
Step: 15120, train/grad_norm: 0.05855870246887207
Step: 15120, train/learning_rate: 3.200856735929847e-05
Step: 15120, train/epoch: 3.5982866287231445
Step: 15130, train/loss: 0.12870000302791595
Step: 15130, train/grad_norm: 0.004980472382158041
Step: 15130, train/learning_rate: 3.199666753062047e-05
Step: 15130, train/epoch: 3.6006662845611572
Step: 15140, train/loss: 0.009399999864399433
Step: 15140, train/grad_norm: 0.04194995388388634
Step: 15140, train/learning_rate: 3.1984767701942474e-05
Step: 15140, train/epoch: 3.603046178817749
Step: 15150, train/loss: 0.061500001698732376
Step: 15150, train/grad_norm: 2.27531099319458
Step: 15150, train/learning_rate: 3.1972871511243284e-05
Step: 15150, train/epoch: 3.605426073074341
Step: 15160, train/loss: 0.012799999676644802
Step: 15160, train/grad_norm: 0.591526210308075
Step: 15160, train/learning_rate: 3.196097168256529e-05
Step: 15160, train/epoch: 3.6078057289123535
Step: 15170, train/loss: 0.5407999753952026
Step: 15170, train/grad_norm: 0.01904710754752159
Step: 15170, train/learning_rate: 3.194907185388729e-05
Step: 15170, train/epoch: 3.6101856231689453
Step: 15180, train/loss: 0.03550000116229057
Step: 15180, train/grad_norm: 0.004994956310838461
Step: 15180, train/learning_rate: 3.193717202520929e-05
Step: 15180, train/epoch: 3.612565517425537
Step: 15190, train/loss: 0.21709999442100525
Step: 15190, train/grad_norm: 0.014922097325325012
Step: 15190, train/learning_rate: 3.1925272196531296e-05
Step: 15190, train/epoch: 3.61494517326355
Step: 15200, train/loss: 0.13740000128746033
Step: 15200, train/grad_norm: 0.016802409663796425
Step: 15200, train/learning_rate: 3.1913376005832106e-05
Step: 15200, train/epoch: 3.6173250675201416
Step: 15210, train/loss: 0.10509999841451645
Step: 15210, train/grad_norm: 16.393678665161133
Step: 15210, train/learning_rate: 3.190147617715411e-05
Step: 15210, train/epoch: 3.6197049617767334
Step: 15220, train/loss: 0.011500000022351742
Step: 15220, train/grad_norm: 0.01949012093245983
Step: 15220, train/learning_rate: 3.188957634847611e-05
Step: 15220, train/epoch: 3.622084617614746
Step: 15230, train/loss: 0.0071000000461936
Step: 15230, train/grad_norm: 11.058334350585938
Step: 15230, train/learning_rate: 3.1877676519798115e-05
Step: 15230, train/epoch: 3.624464511871338
Step: 15240, train/loss: 0.25
Step: 15240, train/grad_norm: 0.1551099717617035
Step: 15240, train/learning_rate: 3.186577669112012e-05
Step: 15240, train/epoch: 3.6268444061279297
Step: 15250, train/loss: 0.18160000443458557
Step: 15250, train/grad_norm: 1.159626841545105
Step: 15250, train/learning_rate: 3.185388050042093e-05
Step: 15250, train/epoch: 3.6292240619659424
Step: 15260, train/loss: 0.12229999899864197
Step: 15260, train/grad_norm: 3.4682774543762207
Step: 15260, train/learning_rate: 3.184198067174293e-05
Step: 15260, train/epoch: 3.631603956222534
Step: 15270, train/loss: 0.25200000405311584
Step: 15270, train/grad_norm: 0.011394338682293892
Step: 15270, train/learning_rate: 3.1830080843064934e-05
Step: 15270, train/epoch: 3.633983850479126
Step: 15280, train/loss: 0.030300000682473183
Step: 15280, train/grad_norm: 0.021865610033273697
Step: 15280, train/learning_rate: 3.181818101438694e-05
Step: 15280, train/epoch: 3.6363637447357178
Step: 15290, train/loss: 0.06080000102519989
Step: 15290, train/grad_norm: 0.00225272704847157
Step: 15290, train/learning_rate: 3.180628118570894e-05
Step: 15290, train/epoch: 3.6387434005737305
Step: 15300, train/loss: 0.27239999175071716
Step: 15300, train/grad_norm: 0.012683412060141563
Step: 15300, train/learning_rate: 3.179438499500975e-05
Step: 15300, train/epoch: 3.6411232948303223
Step: 15310, train/loss: 0.039799999445676804
Step: 15310, train/grad_norm: 0.004963913932442665
Step: 15310, train/learning_rate: 3.178248516633175e-05
Step: 15310, train/epoch: 3.643503189086914
Step: 15320, train/loss: 0.008700000122189522
Step: 15320, train/grad_norm: 1.8035619258880615
Step: 15320, train/learning_rate: 3.1770585337653756e-05
Step: 15320, train/epoch: 3.6458828449249268
Step: 15330, train/loss: 0.06750000268220901
Step: 15330, train/grad_norm: 13.495104789733887
Step: 15330, train/learning_rate: 3.175868550897576e-05
Step: 15330, train/epoch: 3.6482627391815186
Step: 15340, train/loss: 0.13269999623298645
Step: 15340, train/grad_norm: 0.007568407338112593
Step: 15340, train/learning_rate: 3.174678568029776e-05
Step: 15340, train/epoch: 3.6506426334381104
Step: 15350, train/loss: 0.11969999969005585
Step: 15350, train/grad_norm: 0.7286425232887268
Step: 15350, train/learning_rate: 3.173488948959857e-05
Step: 15350, train/epoch: 3.653022289276123
Step: 15360, train/loss: 0.1623000055551529
Step: 15360, train/grad_norm: 0.0036338642239570618
Step: 15360, train/learning_rate: 3.1722989660920575e-05
Step: 15360, train/epoch: 3.655402183532715
Step: 15370, train/loss: 0.10170000046491623
Step: 15370, train/grad_norm: 1.3455853462219238
Step: 15370, train/learning_rate: 3.171108983224258e-05
Step: 15370, train/epoch: 3.6577820777893066
Step: 15380, train/loss: 0.09709999710321426
Step: 15380, train/grad_norm: 0.009202434681355953
Step: 15380, train/learning_rate: 3.169919000356458e-05
Step: 15380, train/epoch: 3.6601617336273193
Step: 15390, train/loss: 0.16979999840259552
Step: 15390, train/grad_norm: 0.007108364254236221
Step: 15390, train/learning_rate: 3.1687290174886584e-05
Step: 15390, train/epoch: 3.662541627883911
Step: 15400, train/loss: 0.06780000030994415
Step: 15400, train/grad_norm: 32.38503646850586
Step: 15400, train/learning_rate: 3.1675393984187394e-05
Step: 15400, train/epoch: 3.664921522140503
Step: 15410, train/loss: 0.04230000078678131
Step: 15410, train/grad_norm: 0.026949414983391762
Step: 15410, train/learning_rate: 3.16634941555094e-05
Step: 15410, train/epoch: 3.6673011779785156
Step: 15420, train/loss: 0.13899999856948853
Step: 15420, train/grad_norm: 0.002937737852334976
Step: 15420, train/learning_rate: 3.16515943268314e-05
Step: 15420, train/epoch: 3.6696810722351074
Step: 15430, train/loss: 0.2296999990940094
Step: 15430, train/grad_norm: 0.0028783325105905533
Step: 15430, train/learning_rate: 3.1639694498153403e-05
Step: 15430, train/epoch: 3.672060966491699
Step: 15440, train/loss: 0.12290000170469284
Step: 15440, train/grad_norm: 10.04853343963623
Step: 15440, train/learning_rate: 3.1627794669475406e-05
Step: 15440, train/epoch: 3.674440860748291
Step: 15450, train/loss: 0.09009999781847
Step: 15450, train/grad_norm: 0.0029246408957988024
Step: 15450, train/learning_rate: 3.1615898478776217e-05
Step: 15450, train/epoch: 3.6768205165863037
Step: 15460, train/loss: 0.15549999475479126
Step: 15460, train/grad_norm: 21.17727279663086
Step: 15460, train/learning_rate: 3.160399865009822e-05
Step: 15460, train/epoch: 3.6792004108428955
Step: 15470, train/loss: 0.31839999556541443
Step: 15470, train/grad_norm: 18.757022857666016
Step: 15470, train/learning_rate: 3.159209882142022e-05
Step: 15470, train/epoch: 3.6815803050994873
Step: 15480, train/loss: 0.12070000171661377
Step: 15480, train/grad_norm: 0.12112930417060852
Step: 15480, train/learning_rate: 3.1580198992742226e-05
Step: 15480, train/epoch: 3.6839599609375
Step: 15490, train/loss: 0.42590001225471497
Step: 15490, train/grad_norm: 8.548056602478027
Step: 15490, train/learning_rate: 3.156829916406423e-05
Step: 15490, train/epoch: 3.686339855194092
Step: 15500, train/loss: 0.22390000522136688
Step: 15500, train/grad_norm: 22.284727096557617
Step: 15500, train/learning_rate: 3.155640297336504e-05
Step: 15500, train/epoch: 3.6887197494506836
Step: 15510, train/loss: 0.06289999932050705
Step: 15510, train/grad_norm: 0.05438416823744774
Step: 15510, train/learning_rate: 3.154450314468704e-05
Step: 15510, train/epoch: 3.6910994052886963
Step: 15520, train/loss: 0.03669999912381172
Step: 15520, train/grad_norm: 0.008014439605176449
Step: 15520, train/learning_rate: 3.1532603316009045e-05
Step: 15520, train/epoch: 3.693479299545288
Step: 15530, train/loss: 0.03959999978542328
Step: 15530, train/grad_norm: 0.005674393381923437
Step: 15530, train/learning_rate: 3.152070348733105e-05
Step: 15530, train/epoch: 3.69585919380188
Step: 15540, train/loss: 0.0658000037074089
Step: 15540, train/grad_norm: 0.03889031708240509
Step: 15540, train/learning_rate: 3.150880365865305e-05
Step: 15540, train/epoch: 3.6982388496398926
Step: 15550, train/loss: 0.029600000008940697
Step: 15550, train/grad_norm: 0.06794799119234085
Step: 15550, train/learning_rate: 3.149690746795386e-05
Step: 15550, train/epoch: 3.7006187438964844
Step: 15560, train/loss: 0.06960000097751617
Step: 15560, train/grad_norm: 0.8515601754188538
Step: 15560, train/learning_rate: 3.1485007639275864e-05
Step: 15560, train/epoch: 3.702998638153076
Step: 15570, train/loss: 0.11209999769926071
Step: 15570, train/grad_norm: 0.04261653497815132
Step: 15570, train/learning_rate: 3.147310781059787e-05
Step: 15570, train/epoch: 3.705378293991089
Step: 15580, train/loss: 0.15549999475479126
Step: 15580, train/grad_norm: 0.012099011801183224
Step: 15580, train/learning_rate: 3.146120798191987e-05
Step: 15580, train/epoch: 3.7077581882476807
Step: 15590, train/loss: 0.07779999822378159
Step: 15590, train/grad_norm: 0.11967775970697403
Step: 15590, train/learning_rate: 3.144930815324187e-05
Step: 15590, train/epoch: 3.7101380825042725
Step: 15600, train/loss: 0.08399999886751175
Step: 15600, train/grad_norm: 0.07426442205905914
Step: 15600, train/learning_rate: 3.143741196254268e-05
Step: 15600, train/epoch: 3.712517738342285
Step: 15610, train/loss: 0.04149999842047691
Step: 15610, train/grad_norm: 10.692018508911133
Step: 15610, train/learning_rate: 3.1425512133864686e-05
Step: 15610, train/epoch: 3.714897632598877
Step: 15620, train/loss: 0.06390000134706497
Step: 15620, train/grad_norm: 0.007135035935789347
Step: 15620, train/learning_rate: 3.141361230518669e-05
Step: 15620, train/epoch: 3.7172775268554688
Step: 15630, train/loss: 0.0934000015258789
Step: 15630, train/grad_norm: 0.6802613735198975
Step: 15630, train/learning_rate: 3.140171247650869e-05
Step: 15630, train/epoch: 3.7196574211120605
Step: 15640, train/loss: 0.11230000108480453
Step: 15640, train/grad_norm: 14.589556694030762
Step: 15640, train/learning_rate: 3.1389812647830695e-05
Step: 15640, train/epoch: 3.7220370769500732
Step: 15650, train/loss: 0.1088000014424324
Step: 15650, train/grad_norm: 0.6399011611938477
Step: 15650, train/learning_rate: 3.1377916457131505e-05
Step: 15650, train/epoch: 3.724416971206665
Step: 15660, train/loss: 0.07029999792575836
Step: 15660, train/grad_norm: 25.890432357788086
Step: 15660, train/learning_rate: 3.136601662845351e-05
Step: 15660, train/epoch: 3.726796865463257
Step: 15670, train/loss: 0.09290000051259995
Step: 15670, train/grad_norm: 0.01113441027700901
Step: 15670, train/learning_rate: 3.135411679977551e-05
Step: 15670, train/epoch: 3.7291765213012695
Step: 15680, train/loss: 0.03480000048875809
Step: 15680, train/grad_norm: 0.001691309968009591
Step: 15680, train/learning_rate: 3.1342216971097514e-05
Step: 15680, train/epoch: 3.7315564155578613
Step: 15690, train/loss: 0.13910000026226044
Step: 15690, train/grad_norm: 0.012049568817019463
Step: 15690, train/learning_rate: 3.133031714241952e-05
Step: 15690, train/epoch: 3.733936309814453
Step: 15700, train/loss: 0.24410000443458557
Step: 15700, train/grad_norm: 0.9942829608917236
Step: 15700, train/learning_rate: 3.131842095172033e-05
Step: 15700, train/epoch: 3.736315965652466
Step: 15710, train/loss: 0.058800000697374344
Step: 15710, train/grad_norm: 26.592771530151367
Step: 15710, train/learning_rate: 3.130652112304233e-05
Step: 15710, train/epoch: 3.7386958599090576
Step: 15720, train/loss: 0.07050000131130219
Step: 15720, train/grad_norm: 0.003727379022166133
Step: 15720, train/learning_rate: 3.129462129436433e-05
Step: 15720, train/epoch: 3.7410757541656494
Step: 15730, train/loss: 0.023800000548362732
Step: 15730, train/grad_norm: 4.881998062133789
Step: 15730, train/learning_rate: 3.1282721465686336e-05
Step: 15730, train/epoch: 3.743455410003662
Step: 15740, train/loss: 0.13689999282360077
Step: 15740, train/grad_norm: 25.667343139648438
Step: 15740, train/learning_rate: 3.127082163700834e-05
Step: 15740, train/epoch: 3.745835304260254
Step: 15750, train/loss: 0.03840000182390213
Step: 15750, train/grad_norm: 0.004822933580726385
Step: 15750, train/learning_rate: 3.125892544630915e-05
Step: 15750, train/epoch: 3.7482151985168457
Step: 15760, train/loss: 0.12839999794960022
Step: 15760, train/grad_norm: 0.017667438834905624
Step: 15760, train/learning_rate: 3.124702561763115e-05
Step: 15760, train/epoch: 3.7505948543548584
Step: 15770, train/loss: 0.3483999967575073
Step: 15770, train/grad_norm: 0.033609990030527115
Step: 15770, train/learning_rate: 3.1235125788953155e-05
Step: 15770, train/epoch: 3.75297474861145
Step: 15780, train/loss: 0.018799999728798866
Step: 15780, train/grad_norm: 15.082491874694824
Step: 15780, train/learning_rate: 3.122322596027516e-05
Step: 15780, train/epoch: 3.755354642868042
Step: 15790, train/loss: 0.05429999902844429
Step: 15790, train/grad_norm: 31.07796287536621
Step: 15790, train/learning_rate: 3.121132613159716e-05
Step: 15790, train/epoch: 3.7577342987060547
Step: 15800, train/loss: 0.4278999865055084
Step: 15800, train/grad_norm: 0.04691027104854584
Step: 15800, train/learning_rate: 3.119942994089797e-05
Step: 15800, train/epoch: 3.7601141929626465
Step: 15810, train/loss: 0.05559999868273735
Step: 15810, train/grad_norm: 23.023141860961914
Step: 15810, train/learning_rate: 3.1187530112219974e-05
Step: 15810, train/epoch: 3.7624940872192383
Step: 15820, train/loss: 0.14650000631809235
Step: 15820, train/grad_norm: 0.002329135313630104
Step: 15820, train/learning_rate: 3.117563028354198e-05
Step: 15820, train/epoch: 3.76487398147583
Step: 15830, train/loss: 0.061900001019239426
Step: 15830, train/grad_norm: 18.82754135131836
Step: 15830, train/learning_rate: 3.116373045486398e-05
Step: 15830, train/epoch: 3.7672536373138428
Step: 15840, train/loss: 0.1768999993801117
Step: 15840, train/grad_norm: 0.23092156648635864
Step: 15840, train/learning_rate: 3.115183426416479e-05
Step: 15840, train/epoch: 3.7696335315704346
Step: 15850, train/loss: 0.01080000028014183
Step: 15850, train/grad_norm: 1.2293413877487183
Step: 15850, train/learning_rate: 3.1139934435486794e-05
Step: 15850, train/epoch: 3.7720134258270264
Step: 15860, train/loss: 0.042500000447034836
Step: 15860, train/grad_norm: 1.7599903345108032
Step: 15860, train/learning_rate: 3.1128034606808797e-05
Step: 15860, train/epoch: 3.774393081665039
Step: 15870, train/loss: 0.06939999759197235
Step: 15870, train/grad_norm: 0.035807766020298004
Step: 15870, train/learning_rate: 3.11161347781308e-05
Step: 15870, train/epoch: 3.776772975921631
Step: 15880, train/loss: 0.19779999554157257
Step: 15880, train/grad_norm: 0.009098256006836891
Step: 15880, train/learning_rate: 3.11042349494528e-05
Step: 15880, train/epoch: 3.7791528701782227
Step: 15890, train/loss: 0.06260000169277191
Step: 15890, train/grad_norm: 11.052310943603516
Step: 15890, train/learning_rate: 3.109233875875361e-05
Step: 15890, train/epoch: 3.7815325260162354
Step: 15900, train/loss: 0.22709999978542328
Step: 15900, train/grad_norm: 1.7121886014938354
Step: 15900, train/learning_rate: 3.1080438930075616e-05
Step: 15900, train/epoch: 3.783912420272827
Step: 15910, train/loss: 0.059300001710653305
Step: 15910, train/grad_norm: 9.671426773071289
Step: 15910, train/learning_rate: 3.106853910139762e-05
Step: 15910, train/epoch: 3.786292314529419
Step: 15920, train/loss: 0.05689999833703041
Step: 15920, train/grad_norm: 11.975423812866211
Step: 15920, train/learning_rate: 3.105663927271962e-05
Step: 15920, train/epoch: 3.7886719703674316
Step: 15930, train/loss: 0.09780000150203705
Step: 15930, train/grad_norm: 0.1775503307580948
Step: 15930, train/learning_rate: 3.1044739444041625e-05
Step: 15930, train/epoch: 3.7910518646240234
Step: 15940, train/loss: 0.09430000185966492
Step: 15940, train/grad_norm: 5.986849784851074
Step: 15940, train/learning_rate: 3.1032843253342435e-05
Step: 15940, train/epoch: 3.7934317588806152
Step: 15950, train/loss: 0.004399999976158142
Step: 15950, train/grad_norm: 5.569598197937012
Step: 15950, train/learning_rate: 3.102094342466444e-05
Step: 15950, train/epoch: 3.795811414718628
Step: 15960, train/loss: 0.2370000034570694
Step: 15960, train/grad_norm: 49.93559646606445
Step: 15960, train/learning_rate: 3.100904359598644e-05
Step: 15960, train/epoch: 3.7981913089752197
Step: 15970, train/loss: 0.10289999842643738
Step: 15970, train/grad_norm: 18.158573150634766
Step: 15970, train/learning_rate: 3.0997143767308444e-05
Step: 15970, train/epoch: 3.8005712032318115
Step: 15980, train/loss: 0.033900000154972076
Step: 15980, train/grad_norm: 0.9293128848075867
Step: 15980, train/learning_rate: 3.098524393863045e-05
Step: 15980, train/epoch: 3.802950859069824
Step: 15990, train/loss: 0.18459999561309814
Step: 15990, train/grad_norm: 24.668899536132812
Step: 15990, train/learning_rate: 3.097334774793126e-05
Step: 15990, train/epoch: 3.805330753326416
Step: 16000, train/loss: 0.09300000220537186
Step: 16000, train/grad_norm: 0.30098769068717957
Step: 16000, train/learning_rate: 3.096144791925326e-05
Step: 16000, train/epoch: 3.807710647583008
Step: 16010, train/loss: 0.2468000054359436
Step: 16010, train/grad_norm: 21.512319564819336
Step: 16010, train/learning_rate: 3.094954809057526e-05
Step: 16010, train/epoch: 3.8100905418395996
Step: 16020, train/loss: 0.09679999947547913
Step: 16020, train/grad_norm: 0.05176110193133354
Step: 16020, train/learning_rate: 3.0937648261897266e-05
Step: 16020, train/epoch: 3.8124701976776123
Step: 16030, train/loss: 0.021700000390410423
Step: 16030, train/grad_norm: 5.556018829345703
Step: 16030, train/learning_rate: 3.092574843321927e-05
Step: 16030, train/epoch: 3.814850091934204
Step: 16040, train/loss: 0.12540000677108765
Step: 16040, train/grad_norm: 0.0019343250896781683
Step: 16040, train/learning_rate: 3.091385224252008e-05
Step: 16040, train/epoch: 3.817229986190796
Step: 16050, train/loss: 0.05849999934434891
Step: 16050, train/grad_norm: 0.004633142147213221
Step: 16050, train/learning_rate: 3.090195241384208e-05
Step: 16050, train/epoch: 3.8196096420288086
Step: 16060, train/loss: 0.3546999990940094
Step: 16060, train/grad_norm: 0.34956368803977966
Step: 16060, train/learning_rate: 3.0890052585164085e-05
Step: 16060, train/epoch: 3.8219895362854004
Step: 16070, train/loss: 0.06809999793767929
Step: 16070, train/grad_norm: 0.143477201461792
Step: 16070, train/learning_rate: 3.087815275648609e-05
Step: 16070, train/epoch: 3.824369430541992
Step: 16080, train/loss: 0.18199999630451202
Step: 16080, train/grad_norm: 24.629154205322266
Step: 16080, train/learning_rate: 3.086625292780809e-05
Step: 16080, train/epoch: 3.826749086380005
Step: 16090, train/loss: 0.3353999853134155
Step: 16090, train/grad_norm: 0.4904818832874298
Step: 16090, train/learning_rate: 3.08543567371089e-05
Step: 16090, train/epoch: 3.8291289806365967
Step: 16100, train/loss: 0.044199999421834946
Step: 16100, train/grad_norm: 0.09944356977939606
Step: 16100, train/learning_rate: 3.0842456908430904e-05
Step: 16100, train/epoch: 3.8315088748931885
Step: 16110, train/loss: 0.026599999517202377
Step: 16110, train/grad_norm: 0.231398344039917
Step: 16110, train/learning_rate: 3.083055707975291e-05
Step: 16110, train/epoch: 3.833888530731201
Step: 16120, train/loss: 0.04190000146627426
Step: 16120, train/grad_norm: 20.551071166992188
Step: 16120, train/learning_rate: 3.081865725107491e-05
Step: 16120, train/epoch: 3.836268424987793
Step: 16130, train/loss: 0.2207999974489212
Step: 16130, train/grad_norm: 1.3741379976272583
Step: 16130, train/learning_rate: 3.080675742239691e-05
Step: 16130, train/epoch: 3.8386483192443848
Step: 16140, train/loss: 0.13019999861717224
Step: 16140, train/grad_norm: 37.93016052246094
Step: 16140, train/learning_rate: 3.079486123169772e-05
Step: 16140, train/epoch: 3.8410279750823975
Step: 16150, train/loss: 0.012299999594688416
Step: 16150, train/grad_norm: 4.840208053588867
Step: 16150, train/learning_rate: 3.0782961403019726e-05
Step: 16150, train/epoch: 3.8434078693389893
Step: 16160, train/loss: 0.07850000262260437
Step: 16160, train/grad_norm: 1.1157090663909912
Step: 16160, train/learning_rate: 3.077106157434173e-05
Step: 16160, train/epoch: 3.845787763595581
Step: 16170, train/loss: 0.038100000470876694
Step: 16170, train/grad_norm: 50.63246536254883
Step: 16170, train/learning_rate: 3.075916174566373e-05
Step: 16170, train/epoch: 3.848167657852173
Step: 16180, train/loss: 0.022299999371170998
Step: 16180, train/grad_norm: 4.679333686828613
Step: 16180, train/learning_rate: 3.0747261916985735e-05
Step: 16180, train/epoch: 3.8505473136901855
Step: 16190, train/loss: 0.20739999413490295
Step: 16190, train/grad_norm: 0.06210518255829811
Step: 16190, train/learning_rate: 3.0735365726286545e-05
Step: 16190, train/epoch: 3.8529272079467773
Step: 16200, train/loss: 0.15199999511241913
Step: 16200, train/grad_norm: 2.03405499458313
Step: 16200, train/learning_rate: 3.072346589760855e-05
Step: 16200, train/epoch: 3.855307102203369
Step: 16210, train/loss: 0.030500000342726707
Step: 16210, train/grad_norm: 0.23759733140468597
Step: 16210, train/learning_rate: 3.071156606893055e-05
Step: 16210, train/epoch: 3.857686758041382
Step: 16220, train/loss: 0.029500000178813934
Step: 16220, train/grad_norm: 0.02660372108221054
Step: 16220, train/learning_rate: 3.0699666240252554e-05
Step: 16220, train/epoch: 3.8600666522979736
Step: 16230, train/loss: 0.029100000858306885
Step: 16230, train/grad_norm: 0.10999055206775665
Step: 16230, train/learning_rate: 3.068776641157456e-05
Step: 16230, train/epoch: 3.8624465465545654
Step: 16240, train/loss: 0.006300000008195639
Step: 16240, train/grad_norm: 0.12134265154600143
Step: 16240, train/learning_rate: 3.067587022087537e-05
Step: 16240, train/epoch: 3.864826202392578
Step: 16250, train/loss: 0.06960000097751617
Step: 16250, train/grad_norm: 19.973913192749023
Step: 16250, train/learning_rate: 3.066397039219737e-05
Step: 16250, train/epoch: 3.86720609664917
Step: 16260, train/loss: 0.08940000087022781
Step: 16260, train/grad_norm: 0.006303042639046907
Step: 16260, train/learning_rate: 3.0652070563519374e-05
Step: 16260, train/epoch: 3.8695859909057617
Step: 16270, train/loss: 0.030799999833106995
Step: 16270, train/grad_norm: 0.051582008600234985
Step: 16270, train/learning_rate: 3.0640170734841377e-05
Step: 16270, train/epoch: 3.8719656467437744
Step: 16280, train/loss: 0.04639999940991402
Step: 16280, train/grad_norm: 0.001449652947485447
Step: 16280, train/learning_rate: 3.062827090616338e-05
Step: 16280, train/epoch: 3.874345541000366
Step: 16290, train/loss: 0.04899999871850014
Step: 16290, train/grad_norm: 0.02262098900973797
Step: 16290, train/learning_rate: 3.061637471546419e-05
Step: 16290, train/epoch: 3.876725435256958
Step: 16300, train/loss: 0.05719999969005585
Step: 16300, train/grad_norm: 1.5976898670196533
Step: 16300, train/learning_rate: 3.060447488678619e-05
Step: 16300, train/epoch: 3.8791050910949707
Step: 16310, train/loss: 0.09780000150203705
Step: 16310, train/grad_norm: 0.21625278890132904
Step: 16310, train/learning_rate: 3.0592575058108196e-05
Step: 16310, train/epoch: 3.8814849853515625
Step: 16320, train/loss: 0.08349999785423279
Step: 16320, train/grad_norm: 0.004296069499105215
Step: 16320, train/learning_rate: 3.05806752294302e-05
Step: 16320, train/epoch: 3.8838648796081543
Step: 16330, train/loss: 0.1550000011920929
Step: 16330, train/grad_norm: 0.537140965461731
Step: 16330, train/learning_rate: 3.05687754007522e-05
Step: 16330, train/epoch: 3.886244535446167
Step: 16340, train/loss: 0.1509999930858612
Step: 16340, train/grad_norm: 0.027741270139813423
Step: 16340, train/learning_rate: 3.055687921005301e-05
Step: 16340, train/epoch: 3.888624429702759
Step: 16350, train/loss: 0.15600000321865082
Step: 16350, train/grad_norm: 1.493599772453308
Step: 16350, train/learning_rate: 3.0544979381375015e-05
Step: 16350, train/epoch: 3.8910043239593506
Step: 16360, train/loss: 0.1387999951839447
Step: 16360, train/grad_norm: 0.027983147650957108
Step: 16360, train/learning_rate: 3.053307955269702e-05
Step: 16360, train/epoch: 3.8933842182159424
Step: 16370, train/loss: 0.18860000371932983
Step: 16370, train/grad_norm: 0.019718794152140617
Step: 16370, train/learning_rate: 3.052117972401902e-05
Step: 16370, train/epoch: 3.895763874053955
Step: 16380, train/loss: 0.06589999794960022
Step: 16380, train/grad_norm: 5.042333126068115
Step: 16380, train/learning_rate: 3.0509281714330427e-05
Step: 16380, train/epoch: 3.898143768310547
Step: 16390, train/loss: 0.004399999976158142
Step: 16390, train/grad_norm: 6.700531005859375
Step: 16390, train/learning_rate: 3.049738188565243e-05
Step: 16390, train/epoch: 3.9005236625671387
Step: 16400, train/loss: 0.11649999767541885
Step: 16400, train/grad_norm: 2.40006947517395
Step: 16400, train/learning_rate: 3.0485483875963837e-05
Step: 16400, train/epoch: 3.9029033184051514
Step: 16410, train/loss: 0.1696999967098236
Step: 16410, train/grad_norm: 0.027764014899730682
Step: 16410, train/learning_rate: 3.047358404728584e-05
Step: 16410, train/epoch: 3.905283212661743
Step: 16420, train/loss: 0.027699999511241913
Step: 16420, train/grad_norm: 0.01085534319281578
Step: 16420, train/learning_rate: 3.0461684218607843e-05
Step: 16420, train/epoch: 3.907663106918335
Step: 16430, train/loss: 0.08240000158548355
Step: 16430, train/grad_norm: 6.6219377517700195
Step: 16430, train/learning_rate: 3.044978620891925e-05
Step: 16430, train/epoch: 3.9100427627563477
Step: 16440, train/loss: 0.2971999943256378
Step: 16440, train/grad_norm: 0.021658696234226227
Step: 16440, train/learning_rate: 3.0437886380241252e-05
Step: 16440, train/epoch: 3.9124226570129395
Step: 16450, train/loss: 0.19670000672340393
Step: 16450, train/grad_norm: 0.01878896728157997
Step: 16450, train/learning_rate: 3.042598837055266e-05
Step: 16450, train/epoch: 3.9148025512695312
Step: 16460, train/loss: 0.0877000018954277
Step: 16460, train/grad_norm: 0.13088275492191315
Step: 16460, train/learning_rate: 3.0414088541874662e-05
Step: 16460, train/epoch: 3.917182207107544
Step: 16470, train/loss: 0.06650000065565109
Step: 16470, train/grad_norm: 0.005986443255096674
Step: 16470, train/learning_rate: 3.0402188713196665e-05
Step: 16470, train/epoch: 3.9195621013641357
Step: 16480, train/loss: 0.05550000071525574
Step: 16480, train/grad_norm: 30.516130447387695
Step: 16480, train/learning_rate: 3.039029070350807e-05
Step: 16480, train/epoch: 3.9219419956207275
Step: 16490, train/loss: 0.24619999527931213
Step: 16490, train/grad_norm: 36.481048583984375
Step: 16490, train/learning_rate: 3.0378390874830075e-05
Step: 16490, train/epoch: 3.9243216514587402
Step: 16500, train/loss: 0.04830000177025795
Step: 16500, train/grad_norm: 1.6410057544708252
Step: 16500, train/learning_rate: 3.036649286514148e-05
Step: 16500, train/epoch: 3.926701545715332
Step: 16510, train/loss: 0.12349999696016312
Step: 16510, train/grad_norm: 0.025927701964974403
Step: 16510, train/learning_rate: 3.0354593036463484e-05
Step: 16510, train/epoch: 3.929081439971924
Step: 16520, train/loss: 0.17239999771118164
Step: 16520, train/grad_norm: 6.4138078689575195
Step: 16520, train/learning_rate: 3.0342693207785487e-05
Step: 16520, train/epoch: 3.9314610958099365
Step: 16530, train/loss: 0.3269999921321869
Step: 16530, train/grad_norm: 13.17673110961914
Step: 16530, train/learning_rate: 3.0330795198096894e-05
Step: 16530, train/epoch: 3.9338409900665283
Step: 16540, train/loss: 0.042100001126527786
Step: 16540, train/grad_norm: 31.47776222229004
Step: 16540, train/learning_rate: 3.0318895369418897e-05
Step: 16540, train/epoch: 3.93622088432312
Step: 16550, train/loss: 0.009999999776482582
Step: 16550, train/grad_norm: 0.007595323026180267
Step: 16550, train/learning_rate: 3.0306997359730303e-05
Step: 16550, train/epoch: 3.938600778579712
Step: 16560, train/loss: 0.09009999781847
Step: 16560, train/grad_norm: 0.01283932663500309
Step: 16560, train/learning_rate: 3.0295097531052306e-05
Step: 16560, train/epoch: 3.9409804344177246
Step: 16570, train/loss: 0.12200000137090683
Step: 16570, train/grad_norm: 3.3838717937469482
Step: 16570, train/learning_rate: 3.028319770237431e-05
Step: 16570, train/epoch: 3.9433603286743164
Step: 16580, train/loss: 0.08349999785423279
Step: 16580, train/grad_norm: 33.818546295166016
Step: 16580, train/learning_rate: 3.0271299692685716e-05
Step: 16580, train/epoch: 3.945740222930908
Step: 16590, train/loss: 0.09449999779462814
Step: 16590, train/grad_norm: 0.4412454664707184
Step: 16590, train/learning_rate: 3.025939986400772e-05
Step: 16590, train/epoch: 3.948119878768921
Step: 16600, train/loss: 0.07349999994039536
Step: 16600, train/grad_norm: 0.29514259099960327
Step: 16600, train/learning_rate: 3.0247501854319125e-05
Step: 16600, train/epoch: 3.9504997730255127
Step: 16610, train/loss: 0.029600000008940697
Step: 16610, train/grad_norm: 0.05426879599690437
Step: 16610, train/learning_rate: 3.023560202564113e-05
Step: 16610, train/epoch: 3.9528796672821045
Step: 16620, train/loss: 0.2061000019311905
Step: 16620, train/grad_norm: 0.1039939895272255
Step: 16620, train/learning_rate: 3.022370219696313e-05
Step: 16620, train/epoch: 3.955259323120117
Step: 16630, train/loss: 0.06920000165700912
Step: 16630, train/grad_norm: 0.667493999004364
Step: 16630, train/learning_rate: 3.0211804187274538e-05
Step: 16630, train/epoch: 3.957639217376709
Step: 16640, train/loss: 0.08540000021457672
Step: 16640, train/grad_norm: 0.022914830595254898
Step: 16640, train/learning_rate: 3.019990435859654e-05
Step: 16640, train/epoch: 3.960019111633301
Step: 16650, train/loss: 0.003000000026077032
Step: 16650, train/grad_norm: 0.11646391451358795
Step: 16650, train/learning_rate: 3.0188006348907948e-05
Step: 16650, train/epoch: 3.9623987674713135
Step: 16660, train/loss: 0.003599999938160181
Step: 16660, train/grad_norm: 5.646330833435059
Step: 16660, train/learning_rate: 3.017610652022995e-05
Step: 16660, train/epoch: 3.9647786617279053
Step: 16670, train/loss: 0.051899999380111694
Step: 16670, train/grad_norm: 0.04871542379260063
Step: 16670, train/learning_rate: 3.0164206691551954e-05
Step: 16670, train/epoch: 3.967158555984497
Step: 16680, train/loss: 0.10899999737739563
Step: 16680, train/grad_norm: 0.008298996835947037
Step: 16680, train/learning_rate: 3.015230868186336e-05
Step: 16680, train/epoch: 3.9695382118225098
Step: 16690, train/loss: 0.05040000006556511
Step: 16690, train/grad_norm: 0.029619557783007622
Step: 16690, train/learning_rate: 3.0140408853185363e-05
Step: 16690, train/epoch: 3.9719181060791016
Step: 16700, train/loss: 0.07190000265836716
Step: 16700, train/grad_norm: 0.8119359016418457
Step: 16700, train/learning_rate: 3.012851084349677e-05
Step: 16700, train/epoch: 3.9742980003356934
Step: 16710, train/loss: 0.29600000381469727
Step: 16710, train/grad_norm: 0.014113961718976498
Step: 16710, train/learning_rate: 3.0116611014818773e-05
Step: 16710, train/epoch: 3.976677656173706
Step: 16720, train/loss: 0.04470000043511391
Step: 16720, train/grad_norm: 0.08608415722846985
Step: 16720, train/learning_rate: 3.0104711186140776e-05
Step: 16720, train/epoch: 3.979057550430298
Step: 16730, train/loss: 0.14309999346733093
Step: 16730, train/grad_norm: 0.018283996731042862
Step: 16730, train/learning_rate: 3.0092813176452182e-05
Step: 16730, train/epoch: 3.9814374446868896
Step: 16740, train/loss: 0.04490000009536743
Step: 16740, train/grad_norm: 0.4303351938724518
Step: 16740, train/learning_rate: 3.0080913347774185e-05
Step: 16740, train/epoch: 3.9838173389434814
Step: 16750, train/loss: 0.023900000378489494
Step: 16750, train/grad_norm: 0.008891666308045387
Step: 16750, train/learning_rate: 3.0069015338085592e-05
Step: 16750, train/epoch: 3.986196994781494
Step: 16760, train/loss: 0.19099999964237213
Step: 16760, train/grad_norm: 48.805110931396484
Step: 16760, train/learning_rate: 3.0057115509407595e-05
Step: 16760, train/epoch: 3.988576889038086
Step: 16770, train/loss: 0.060499999672174454
Step: 16770, train/grad_norm: 0.015777476131916046
Step: 16770, train/learning_rate: 3.0045215680729598e-05
Step: 16770, train/epoch: 3.9909567832946777
Step: 16780, train/loss: 0.17139999568462372
Step: 16780, train/grad_norm: 15.28260612487793
Step: 16780, train/learning_rate: 3.0033317671041004e-05
Step: 16780, train/epoch: 3.9933364391326904
Step: 16790, train/loss: 0.13249999284744263
Step: 16790, train/grad_norm: 0.03573238104581833
Step: 16790, train/learning_rate: 3.0021417842363007e-05
Step: 16790, train/epoch: 3.9957163333892822
Step: 16800, train/loss: 0.2556999921798706
Step: 16800, train/grad_norm: 6.358902931213379
Step: 16800, train/learning_rate: 3.0009519832674414e-05
Step: 16800, train/epoch: 3.998096227645874
Step: 16808, eval/loss: 0.20793083310127258
Step: 16808, eval/accuracy: 0.9487713575363159
Step: 16808, eval/f1: 0.9452282786369324
Step: 16808, eval/runtime: 296.9179992675781
Step: 16808, eval/samples_per_second: 24.259000778198242
Step: 16808, eval/steps_per_second: 3.0350000858306885
Step: 16808, train/epoch: 4.0
Step: 16810, train/loss: 0.18019999563694
Step: 16810, train/grad_norm: 0.12433785200119019
Step: 16810, train/learning_rate: 2.9997620003996417e-05
Step: 16810, train/epoch: 4.000475883483887
Step: 16820, train/loss: 0.03550000116229057
Step: 16820, train/grad_norm: 1.8372581005096436
Step: 16820, train/learning_rate: 2.9985721994307823e-05
Step: 16820, train/epoch: 4.0028557777404785
Step: 16830, train/loss: 0.0017999999690800905
Step: 16830, train/grad_norm: 0.03794145584106445
Step: 16830, train/learning_rate: 2.9973822165629826e-05
Step: 16830, train/epoch: 4.00523567199707
Step: 16840, train/loss: 0.012199999764561653
Step: 16840, train/grad_norm: 3.434548854827881
Step: 16840, train/learning_rate: 2.996192233695183e-05
Step: 16840, train/epoch: 4.007615566253662
Step: 16850, train/loss: 0.023800000548362732
Step: 16850, train/grad_norm: 0.034402210265398026
Step: 16850, train/learning_rate: 2.9950024327263236e-05
Step: 16850, train/epoch: 4.009995460510254
Step: 16860, train/loss: 0.10029999911785126
Step: 16860, train/grad_norm: 11.678404808044434
Step: 16860, train/learning_rate: 2.993812449858524e-05
Step: 16860, train/epoch: 4.0123748779296875
Step: 16870, train/loss: 0.05009999871253967
Step: 16870, train/grad_norm: 0.03079255484044552
Step: 16870, train/learning_rate: 2.9926226488896646e-05
Step: 16870, train/epoch: 4.014754772186279
Step: 16880, train/loss: 0.06710000336170197
Step: 16880, train/grad_norm: 0.007849371060729027
Step: 16880, train/learning_rate: 2.991432666021865e-05
Step: 16880, train/epoch: 4.017134666442871
Step: 16890, train/loss: 0.05460000038146973
Step: 16890, train/grad_norm: 0.006069609429687262
Step: 16890, train/learning_rate: 2.990242683154065e-05
Step: 16890, train/epoch: 4.019514560699463
Step: 16900, train/loss: 0.09549999982118607
Step: 16900, train/grad_norm: 0.12216793745756149
Step: 16900, train/learning_rate: 2.9890528821852058e-05
Step: 16900, train/epoch: 4.021894454956055
Step: 16910, train/loss: 0.0020000000949949026
Step: 16910, train/grad_norm: 0.13890324532985687
Step: 16910, train/learning_rate: 2.987862899317406e-05
Step: 16910, train/epoch: 4.0242743492126465
Step: 16920, train/loss: 0.21570000052452087
Step: 16920, train/grad_norm: 36.66585922241211
Step: 16920, train/learning_rate: 2.9866730983485468e-05
Step: 16920, train/epoch: 4.02665376663208
Step: 16930, train/loss: 0.23199999332427979
Step: 16930, train/grad_norm: 9.9715576171875
Step: 16930, train/learning_rate: 2.985483115480747e-05
Step: 16930, train/epoch: 4.029033660888672
Step: 16940, train/loss: 0.003599999938160181
Step: 16940, train/grad_norm: 2.932555913925171
Step: 16940, train/learning_rate: 2.9842931326129474e-05
Step: 16940, train/epoch: 4.031413555145264
Step: 16950, train/loss: 0.010900000110268593
Step: 16950, train/grad_norm: 0.35731321573257446
Step: 16950, train/learning_rate: 2.983103331644088e-05
Step: 16950, train/epoch: 4.0337934494018555
Step: 16960, train/loss: 0.006200000178068876
Step: 16960, train/grad_norm: 0.03980359807610512
Step: 16960, train/learning_rate: 2.9819133487762883e-05
Step: 16960, train/epoch: 4.036173343658447
Step: 16970, train/loss: 0.13449999690055847
Step: 16970, train/grad_norm: 0.0017070616595447063
Step: 16970, train/learning_rate: 2.980723547807429e-05
Step: 16970, train/epoch: 4.038553237915039
Step: 16980, train/loss: 0.005499999970197678
Step: 16980, train/grad_norm: 0.009384247474372387
Step: 16980, train/learning_rate: 2.9795335649396293e-05
Step: 16980, train/epoch: 4.040932655334473
Step: 16990, train/loss: 0.013500000350177288
Step: 16990, train/grad_norm: 6.177410125732422
Step: 16990, train/learning_rate: 2.9783435820718296e-05
Step: 16990, train/epoch: 4.0433125495910645
Step: 17000, train/loss: 0.02879999950528145
Step: 17000, train/grad_norm: 0.0021909342613071203
Step: 17000, train/learning_rate: 2.9771537811029702e-05
Step: 17000, train/epoch: 4.045692443847656
Step: 17010, train/loss: 0.2231999933719635
Step: 17010, train/grad_norm: 52.14906311035156
Step: 17010, train/learning_rate: 2.9759637982351705e-05
Step: 17010, train/epoch: 4.048072338104248
Step: 17020, train/loss: 0.03739999979734421
Step: 17020, train/grad_norm: 26.7303409576416
Step: 17020, train/learning_rate: 2.9747739972663112e-05
Step: 17020, train/epoch: 4.05045223236084
Step: 17030, train/loss: 0.007400000002235174
Step: 17030, train/grad_norm: 3.5857455730438232
Step: 17030, train/learning_rate: 2.9735840143985115e-05
Step: 17030, train/epoch: 4.052832126617432
Step: 17040, train/loss: 0.04349999874830246
Step: 17040, train/grad_norm: 0.04927222058176994
Step: 17040, train/learning_rate: 2.9723940315307118e-05
Step: 17040, train/epoch: 4.055212020874023
Step: 17050, train/loss: 0.07410000264644623
Step: 17050, train/grad_norm: 0.012027480639517307
Step: 17050, train/learning_rate: 2.9712042305618525e-05
Step: 17050, train/epoch: 4.057591438293457
Step: 17060, train/loss: 0.14300000667572021
Step: 17060, train/grad_norm: 0.09481699764728546
Step: 17060, train/learning_rate: 2.9700142476940528e-05
Step: 17060, train/epoch: 4.059971332550049
Step: 17070, train/loss: 0.004900000058114529
Step: 17070, train/grad_norm: 0.0022216325160115957
Step: 17070, train/learning_rate: 2.9688244467251934e-05
Step: 17070, train/epoch: 4.062351226806641
Step: 17080, train/loss: 0.04190000146627426
Step: 17080, train/grad_norm: 0.18129345774650574
Step: 17080, train/learning_rate: 2.9676344638573937e-05
Step: 17080, train/epoch: 4.064731121063232
Step: 17090, train/loss: 0.05380000174045563
Step: 17090, train/grad_norm: 0.4639767110347748
Step: 17090, train/learning_rate: 2.966444480989594e-05
Step: 17090, train/epoch: 4.067111015319824
Step: 17100, train/loss: 0.09560000151395798
Step: 17100, train/grad_norm: 0.011838776059448719
Step: 17100, train/learning_rate: 2.9652546800207347e-05
Step: 17100, train/epoch: 4.069490909576416
Step: 17110, train/loss: 0.2337999939918518
Step: 17110, train/grad_norm: 24.544139862060547
Step: 17110, train/learning_rate: 2.964064697152935e-05
Step: 17110, train/epoch: 4.07187032699585
Step: 17120, train/loss: 0.14659999310970306
Step: 17120, train/grad_norm: 49.95813751220703
Step: 17120, train/learning_rate: 2.9628748961840756e-05
Step: 17120, train/epoch: 4.074250221252441
Step: 17130, train/loss: 0.009200000204145908
Step: 17130, train/grad_norm: 0.06912782043218613
Step: 17130, train/learning_rate: 2.961684913316276e-05
Step: 17130, train/epoch: 4.076630115509033
Step: 17140, train/loss: 0.1005999967455864
Step: 17140, train/grad_norm: 0.06988681107759476
Step: 17140, train/learning_rate: 2.9604949304484762e-05
Step: 17140, train/epoch: 4.079010009765625
Step: 17150, train/loss: 0.06080000102519989
Step: 17150, train/grad_norm: 17.502561569213867
Step: 17150, train/learning_rate: 2.959305129479617e-05
Step: 17150, train/epoch: 4.081389904022217
Step: 17160, train/loss: 0.00139999995008111
Step: 17160, train/grad_norm: 1.9972565174102783
Step: 17160, train/learning_rate: 2.9581151466118172e-05
Step: 17160, train/epoch: 4.083769798278809
Step: 17170, train/loss: 0.060499999672174454
Step: 17170, train/grad_norm: 0.0032767087686806917
Step: 17170, train/learning_rate: 2.956925345642958e-05
Step: 17170, train/epoch: 4.086149215698242
Step: 17180, train/loss: 0.04910000041127205
Step: 17180, train/grad_norm: 1.9476679563522339
Step: 17180, train/learning_rate: 2.955735362775158e-05
Step: 17180, train/epoch: 4.088529109954834
Step: 17190, train/loss: 0.3483999967575073
Step: 17190, train/grad_norm: 0.0015191789716482162
Step: 17190, train/learning_rate: 2.9545453799073584e-05
Step: 17190, train/epoch: 4.090909004211426
Step: 17200, train/loss: 0.17479999363422394
Step: 17200, train/grad_norm: 0.029098836705088615
Step: 17200, train/learning_rate: 2.953355578938499e-05
Step: 17200, train/epoch: 4.093288898468018
Step: 17210, train/loss: 0.012900000438094139
Step: 17210, train/grad_norm: 0.024501562118530273
Step: 17210, train/learning_rate: 2.9521655960706994e-05
Step: 17210, train/epoch: 4.095668792724609
Step: 17220, train/loss: 0.03889999911189079
Step: 17220, train/grad_norm: 0.014478737488389015
Step: 17220, train/learning_rate: 2.95097579510184e-05
Step: 17220, train/epoch: 4.098048686981201
Step: 17230, train/loss: 0.030899999663233757
Step: 17230, train/grad_norm: 0.0015414373483508825
Step: 17230, train/learning_rate: 2.9497858122340403e-05
Step: 17230, train/epoch: 4.100428581237793
Step: 17240, train/loss: 0.032600000500679016
Step: 17240, train/grad_norm: 0.06212392821907997
Step: 17240, train/learning_rate: 2.9485958293662407e-05
Step: 17240, train/epoch: 4.102807998657227
Step: 17250, train/loss: 0.10300000011920929
Step: 17250, train/grad_norm: 1.594947338104248
Step: 17250, train/learning_rate: 2.9474060283973813e-05
Step: 17250, train/epoch: 4.105187892913818
Step: 17260, train/loss: 0.1454000025987625
Step: 17260, train/grad_norm: 4.097670078277588
Step: 17260, train/learning_rate: 2.9462160455295816e-05
Step: 17260, train/epoch: 4.10756778717041
Step: 17270, train/loss: 0.0003000000142492354
Step: 17270, train/grad_norm: 0.001124406000599265
Step: 17270, train/learning_rate: 2.9450262445607223e-05
Step: 17270, train/epoch: 4.109947681427002
Step: 17280, train/loss: 0.12690000236034393
Step: 17280, train/grad_norm: 0.06478828191757202
Step: 17280, train/learning_rate: 2.9438362616929226e-05
Step: 17280, train/epoch: 4.112327575683594
Step: 17290, train/loss: 0.12049999833106995
Step: 17290, train/grad_norm: 0.003169580362737179
Step: 17290, train/learning_rate: 2.942646278825123e-05
Step: 17290, train/epoch: 4.1147074699401855
Step: 17300, train/loss: 0.08209999650716782
Step: 17300, train/grad_norm: 0.015605902299284935
Step: 17300, train/learning_rate: 2.9414564778562635e-05
Step: 17300, train/epoch: 4.117086887359619
Step: 17310, train/loss: 0.0038999998942017555
Step: 17310, train/grad_norm: 0.007156809791922569
Step: 17310, train/learning_rate: 2.9402664949884638e-05
Step: 17310, train/epoch: 4.119466781616211
Step: 17320, train/loss: 0.004699999932199717
Step: 17320, train/grad_norm: 0.2660803496837616
Step: 17320, train/learning_rate: 2.9390766940196045e-05
Step: 17320, train/epoch: 4.121846675872803
Step: 17330, train/loss: 0.16519999504089355
Step: 17330, train/grad_norm: 0.008979827165603638
Step: 17330, train/learning_rate: 2.9378867111518048e-05
Step: 17330, train/epoch: 4.1242265701293945
Step: 17340, train/loss: 0.008899999782443047
Step: 17340, train/grad_norm: 14.828421592712402
Step: 17340, train/learning_rate: 2.936696728284005e-05
Step: 17340, train/epoch: 4.126606464385986
Step: 17350, train/loss: 0.024800000712275505
Step: 17350, train/grad_norm: 0.006681727711111307
Step: 17350, train/learning_rate: 2.9355069273151457e-05
Step: 17350, train/epoch: 4.128986358642578
Step: 17360, train/loss: 0.24390000104904175
Step: 17360, train/grad_norm: 1.3817944526672363
Step: 17360, train/learning_rate: 2.934316944447346e-05
Step: 17360, train/epoch: 4.13136625289917
Step: 17370, train/loss: 0.02590000070631504
Step: 17370, train/grad_norm: 0.007039888296276331
Step: 17370, train/learning_rate: 2.9331271434784867e-05
Step: 17370, train/epoch: 4.1337456703186035
Step: 17380, train/loss: 0.04019999876618385
Step: 17380, train/grad_norm: 0.03332680091261864
Step: 17380, train/learning_rate: 2.931937160610687e-05
Step: 17380, train/epoch: 4.136125564575195
Step: 17390, train/loss: 0.004900000058114529
Step: 17390, train/grad_norm: 0.009969457052648067
Step: 17390, train/learning_rate: 2.9307471777428873e-05
Step: 17390, train/epoch: 4.138505458831787
Step: 17400, train/loss: 0.01850000023841858
Step: 17400, train/grad_norm: 1.1972315311431885
Step: 17400, train/learning_rate: 2.929557376774028e-05
Step: 17400, train/epoch: 4.140885353088379
Step: 17410, train/loss: 0.005799999926239252
Step: 17410, train/grad_norm: 2.9901010990142822
Step: 17410, train/learning_rate: 2.9283673939062282e-05
Step: 17410, train/epoch: 4.143265247344971
Step: 17420, train/loss: 0.10369999706745148
Step: 17420, train/grad_norm: 1.0568243265151978
Step: 17420, train/learning_rate: 2.927177592937369e-05
Step: 17420, train/epoch: 4.1456451416015625
Step: 17430, train/loss: 0.06109999865293503
Step: 17430, train/grad_norm: 0.9971410036087036
Step: 17430, train/learning_rate: 2.9259876100695692e-05
Step: 17430, train/epoch: 4.148024559020996
Step: 17440, train/loss: 0.028699999675154686
Step: 17440, train/grad_norm: 0.6274036169052124
Step: 17440, train/learning_rate: 2.9247976272017695e-05
Step: 17440, train/epoch: 4.150404453277588
Step: 17450, train/loss: 0.024000000208616257
Step: 17450, train/grad_norm: 16.779714584350586
Step: 17450, train/learning_rate: 2.92360782623291e-05
Step: 17450, train/epoch: 4.15278434753418
Step: 17460, train/loss: 0.002300000051036477
Step: 17460, train/grad_norm: 0.012212728150188923
Step: 17460, train/learning_rate: 2.9224178433651105e-05
Step: 17460, train/epoch: 4.1551642417907715
Step: 17470, train/loss: 0.07689999788999557
Step: 17470, train/grad_norm: 1.0062562227249146
Step: 17470, train/learning_rate: 2.921228042396251e-05
Step: 17470, train/epoch: 4.157544136047363
Step: 17480, train/loss: 0.043699998408555984
Step: 17480, train/grad_norm: 1.011291265487671
Step: 17480, train/learning_rate: 2.9200380595284514e-05
Step: 17480, train/epoch: 4.159924030303955
Step: 17490, train/loss: 0.17569999396800995
Step: 17490, train/grad_norm: 3.8053228855133057
Step: 17490, train/learning_rate: 2.9188480766606517e-05
Step: 17490, train/epoch: 4.162303447723389
Step: 17500, train/loss: 0.00039999998989515007
Step: 17500, train/grad_norm: 0.05832361429929733
Step: 17500, train/learning_rate: 2.9176582756917924e-05
Step: 17500, train/epoch: 4.1646833419799805
Step: 17510, train/loss: 0.008100000210106373
Step: 17510, train/grad_norm: 0.03855153173208237
Step: 17510, train/learning_rate: 2.9164682928239927e-05
Step: 17510, train/epoch: 4.167063236236572
Step: 17520, train/loss: 0.11379999667406082
Step: 17520, train/grad_norm: 24.787626266479492
Step: 17520, train/learning_rate: 2.9152784918551333e-05
Step: 17520, train/epoch: 4.169443130493164
Step: 17530, train/loss: 0.10809999704360962
Step: 17530, train/grad_norm: 0.0028963210061192513
Step: 17530, train/learning_rate: 2.9140885089873336e-05
Step: 17530, train/epoch: 4.171823024749756
Step: 17540, train/loss: 0.023600000888109207
Step: 17540, train/grad_norm: 0.04850902780890465
Step: 17540, train/learning_rate: 2.9128987080184743e-05
Step: 17540, train/epoch: 4.174202919006348
Step: 17550, train/loss: 0.02979999966919422
Step: 17550, train/grad_norm: 33.90887451171875
Step: 17550, train/learning_rate: 2.9117087251506746e-05
Step: 17550, train/epoch: 4.1765828132629395
Step: 17560, train/loss: 0.011900000274181366
Step: 17560, train/grad_norm: 19.366348266601562
Step: 17560, train/learning_rate: 2.910518742282875e-05
Step: 17560, train/epoch: 4.178962230682373
Step: 17570, train/loss: 0.05119999870657921
Step: 17570, train/grad_norm: 0.008548961021006107
Step: 17570, train/learning_rate: 2.9093289413140155e-05
Step: 17570, train/epoch: 4.181342124938965
Step: 17580, train/loss: 0.14890000224113464
Step: 17580, train/grad_norm: 0.018673516809940338
Step: 17580, train/learning_rate: 2.908138958446216e-05
Step: 17580, train/epoch: 4.183722019195557
Step: 17590, train/loss: 0.10379999876022339
Step: 17590, train/grad_norm: 0.9212915897369385
Step: 17590, train/learning_rate: 2.9069491574773565e-05
Step: 17590, train/epoch: 4.186101913452148
Step: 17600, train/loss: 0.0681999996304512
Step: 17600, train/grad_norm: 0.5061363577842712
Step: 17600, train/learning_rate: 2.9057591746095568e-05
Step: 17600, train/epoch: 4.18848180770874
Step: 17610, train/loss: 0.01269999984651804
Step: 17610, train/grad_norm: 0.0014196543488651514
Step: 17610, train/learning_rate: 2.904569191741757e-05
Step: 17610, train/epoch: 4.190861701965332
Step: 17620, train/loss: 0.02759999968111515
Step: 17620, train/grad_norm: 25.41136360168457
Step: 17620, train/learning_rate: 2.9033793907728978e-05
Step: 17620, train/epoch: 4.193241119384766
Step: 17630, train/loss: 0.15919999778270721
Step: 17630, train/grad_norm: 0.024937348440289497
Step: 17630, train/learning_rate: 2.902189407905098e-05
Step: 17630, train/epoch: 4.195621013641357
Step: 17640, train/loss: 0.09549999982118607
Step: 17640, train/grad_norm: 0.010878504253923893
Step: 17640, train/learning_rate: 2.9009996069362387e-05
Step: 17640, train/epoch: 4.198000907897949
Step: 17650, train/loss: 0.017999999225139618
Step: 17650, train/grad_norm: 0.002275945618748665
Step: 17650, train/learning_rate: 2.899809624068439e-05
Step: 17650, train/epoch: 4.200380802154541
Step: 17660, train/loss: 0.0012000000569969416
Step: 17660, train/grad_norm: 0.054160941392183304
Step: 17660, train/learning_rate: 2.8986196412006393e-05
Step: 17660, train/epoch: 4.202760696411133
Step: 17670, train/loss: 0.042500000447034836
Step: 17670, train/grad_norm: 23.958927154541016
Step: 17670, train/learning_rate: 2.89742984023178e-05
Step: 17670, train/epoch: 4.205140590667725
Step: 17680, train/loss: 0.11270000040531158
Step: 17680, train/grad_norm: 28.55156707763672
Step: 17680, train/learning_rate: 2.8962398573639803e-05
Step: 17680, train/epoch: 4.207520008087158
Step: 17690, train/loss: 0.12380000203847885
Step: 17690, train/grad_norm: 61.11152267456055
Step: 17690, train/learning_rate: 2.895050056395121e-05
Step: 17690, train/epoch: 4.20989990234375
Step: 17700, train/loss: 0.0010000000474974513
Step: 17700, train/grad_norm: 0.3920484185218811
Step: 17700, train/learning_rate: 2.8938600735273212e-05
Step: 17700, train/epoch: 4.212279796600342
Step: 17710, train/loss: 0.0019000000320374966
Step: 17710, train/grad_norm: 0.17395633459091187
Step: 17710, train/learning_rate: 2.8926700906595215e-05
Step: 17710, train/epoch: 4.214659690856934
Step: 17720, train/loss: 0.11789999902248383
Step: 17720, train/grad_norm: 0.27957743406295776
Step: 17720, train/learning_rate: 2.8914802896906622e-05
Step: 17720, train/epoch: 4.217039585113525
Step: 17730, train/loss: 0.23260000348091125
Step: 17730, train/grad_norm: 29.202096939086914
Step: 17730, train/learning_rate: 2.8902903068228625e-05
Step: 17730, train/epoch: 4.219419479370117
Step: 17740, train/loss: 0.06769999861717224
Step: 17740, train/grad_norm: 25.5556640625
Step: 17740, train/learning_rate: 2.889100505854003e-05
Step: 17740, train/epoch: 4.221799373626709
Step: 17750, train/loss: 0.2150000035762787
Step: 17750, train/grad_norm: 0.041711460798978806
Step: 17750, train/learning_rate: 2.8879105229862034e-05
Step: 17750, train/epoch: 4.224178791046143
Step: 17760, train/loss: 0.062300000339746475
Step: 17760, train/grad_norm: 8.62493896484375
Step: 17760, train/learning_rate: 2.8867205401184037e-05
Step: 17760, train/epoch: 4.226558685302734
Step: 17770, train/loss: 0.008100000210106373
Step: 17770, train/grad_norm: 0.8185882568359375
Step: 17770, train/learning_rate: 2.8855307391495444e-05
Step: 17770, train/epoch: 4.228938579559326
Step: 17780, train/loss: 0.05270000174641609
Step: 17780, train/grad_norm: 14.897933006286621
Step: 17780, train/learning_rate: 2.8843407562817447e-05
Step: 17780, train/epoch: 4.231318473815918
Step: 17790, train/loss: 0.1256999969482422
Step: 17790, train/grad_norm: 4.921179294586182
Step: 17790, train/learning_rate: 2.8831509553128853e-05
Step: 17790, train/epoch: 4.23369836807251
Step: 17800, train/loss: 0.05050000175833702
Step: 17800, train/grad_norm: 0.12336106598377228
Step: 17800, train/learning_rate: 2.8819609724450856e-05
Step: 17800, train/epoch: 4.236078262329102
Step: 17810, train/loss: 0.08070000261068344
Step: 17810, train/grad_norm: 19.217172622680664
Step: 17810, train/learning_rate: 2.880770989577286e-05
Step: 17810, train/epoch: 4.238457679748535
Step: 17820, train/loss: 0.05730000138282776
Step: 17820, train/grad_norm: 0.07418281584978104
Step: 17820, train/learning_rate: 2.8795811886084266e-05
Step: 17820, train/epoch: 4.240837574005127
Step: 17830, train/loss: 0.20479999482631683
Step: 17830, train/grad_norm: 0.06679080426692963
Step: 17830, train/learning_rate: 2.878391205740627e-05
Step: 17830, train/epoch: 4.243217468261719
Step: 17840, train/loss: 0.17020000517368317
Step: 17840, train/grad_norm: 21.950927734375
Step: 17840, train/learning_rate: 2.8772014047717676e-05
Step: 17840, train/epoch: 4.2455973625183105
Step: 17850, train/loss: 0.04340000078082085
Step: 17850, train/grad_norm: 0.003961338195949793
Step: 17850, train/learning_rate: 2.876011421903968e-05
Step: 17850, train/epoch: 4.247977256774902
Step: 17860, train/loss: 0.02630000002682209
Step: 17860, train/grad_norm: 42.2229118347168
Step: 17860, train/learning_rate: 2.874821439036168e-05
Step: 17860, train/epoch: 4.250357151031494
Step: 17870, train/loss: 0.0010999999940395355
Step: 17870, train/grad_norm: 0.039799515157938004
Step: 17870, train/learning_rate: 2.8736316380673088e-05
Step: 17870, train/epoch: 4.252736568450928
Step: 17880, train/loss: 0.05400000140070915
Step: 17880, train/grad_norm: 0.9648764729499817
Step: 17880, train/learning_rate: 2.872441655199509e-05
Step: 17880, train/epoch: 4.2551164627075195
Step: 17890, train/loss: 0.0007999999797903001
Step: 17890, train/grad_norm: 0.06980961561203003
Step: 17890, train/learning_rate: 2.8712518542306498e-05
Step: 17890, train/epoch: 4.257496356964111
Step: 17900, train/loss: 0.003100000089034438
Step: 17900, train/grad_norm: 0.047847483307123184
Step: 17900, train/learning_rate: 2.87006187136285e-05
Step: 17900, train/epoch: 4.259876251220703
Step: 17910, train/loss: 0.10540000349283218
Step: 17910, train/grad_norm: 0.001088388729840517
Step: 17910, train/learning_rate: 2.8688718884950504e-05
Step: 17910, train/epoch: 4.262256145477295
Step: 17920, train/loss: 0.008200000040233135
Step: 17920, train/grad_norm: 0.018665723502635956
Step: 17920, train/learning_rate: 2.867682087526191e-05
Step: 17920, train/epoch: 4.264636039733887
Step: 17930, train/loss: 0.01510000042617321
Step: 17930, train/grad_norm: 0.027393680065870285
Step: 17930, train/learning_rate: 2.8664921046583913e-05
Step: 17930, train/epoch: 4.2670159339904785
Step: 17940, train/loss: 0.026000000536441803
Step: 17940, train/grad_norm: 2.657181978225708
Step: 17940, train/learning_rate: 2.865302303689532e-05
Step: 17940, train/epoch: 4.269395351409912
Step: 17950, train/loss: 0.025200000032782555
Step: 17950, train/grad_norm: 0.04896625876426697
Step: 17950, train/learning_rate: 2.8641123208217323e-05
Step: 17950, train/epoch: 4.271775245666504
Step: 17960, train/loss: 0.004100000020116568
Step: 17960, train/grad_norm: 0.043767403811216354
Step: 17960, train/learning_rate: 2.8629223379539326e-05
Step: 17960, train/epoch: 4.274155139923096
Step: 17970, train/loss: 0.16619999706745148
Step: 17970, train/grad_norm: 1.1640236377716064
Step: 17970, train/learning_rate: 2.8617325369850732e-05
Step: 17970, train/epoch: 4.2765350341796875
Step: 17980, train/loss: 0.007000000216066837
Step: 17980, train/grad_norm: 1.4919949769973755
Step: 17980, train/learning_rate: 2.8605425541172735e-05
Step: 17980, train/epoch: 4.278914928436279
Step: 17990, train/loss: 0.010300000198185444
Step: 17990, train/grad_norm: 0.2980358600616455
Step: 17990, train/learning_rate: 2.8593527531484142e-05
Step: 17990, train/epoch: 4.281294822692871
Step: 18000, train/loss: 0.06279999762773514
Step: 18000, train/grad_norm: 0.0750235840678215
Step: 18000, train/learning_rate: 2.8581627702806145e-05
Step: 18000, train/epoch: 4.283674240112305
Step: 18010, train/loss: 0.08969999849796295
Step: 18010, train/grad_norm: 0.08071889728307724
Step: 18010, train/learning_rate: 2.8569727874128148e-05
Step: 18010, train/epoch: 4.2860541343688965
Step: 18020, train/loss: 0.0031999999191612005
Step: 18020, train/grad_norm: 0.016459425911307335
Step: 18020, train/learning_rate: 2.8557829864439555e-05
Step: 18020, train/epoch: 4.288434028625488
Step: 18030, train/loss: 0.3131999969482422
Step: 18030, train/grad_norm: 0.007628877647221088
Step: 18030, train/learning_rate: 2.8545930035761558e-05
Step: 18030, train/epoch: 4.29081392288208
Step: 18040, train/loss: 0.24079999327659607
Step: 18040, train/grad_norm: 0.013115697540342808
Step: 18040, train/learning_rate: 2.8534032026072964e-05
Step: 18040, train/epoch: 4.293193817138672
Step: 18050, train/loss: 0.12439999729394913
Step: 18050, train/grad_norm: 0.020636560395359993
Step: 18050, train/learning_rate: 2.8522132197394967e-05
Step: 18050, train/epoch: 4.295573711395264
Step: 18060, train/loss: 0.0006000000284984708
Step: 18060, train/grad_norm: 0.0019461804768070579
Step: 18060, train/learning_rate: 2.851023236871697e-05
Step: 18060, train/epoch: 4.297953128814697
Step: 18070, train/loss: 0.2062000036239624
Step: 18070, train/grad_norm: 42.73672866821289
Step: 18070, train/learning_rate: 2.8498334359028377e-05
Step: 18070, train/epoch: 4.300333023071289
Step: 18080, train/loss: 0.02449999935925007
Step: 18080, train/grad_norm: 0.026794390752911568
Step: 18080, train/learning_rate: 2.848643453035038e-05
Step: 18080, train/epoch: 4.302712917327881
Step: 18090, train/loss: 0.042100001126527786
Step: 18090, train/grad_norm: 0.02564605139195919
Step: 18090, train/learning_rate: 2.8474536520661786e-05
Step: 18090, train/epoch: 4.305092811584473
Step: 18100, train/loss: 0.007199999876320362
Step: 18100, train/grad_norm: 0.05552919581532478
Step: 18100, train/learning_rate: 2.846263669198379e-05
Step: 18100, train/epoch: 4.3074727058410645
Step: 18110, train/loss: 0.06129999831318855
Step: 18110, train/grad_norm: 7.817936420440674
Step: 18110, train/learning_rate: 2.8450736863305792e-05
Step: 18110, train/epoch: 4.309852600097656
Step: 18120, train/loss: 0.0026000000070780516
Step: 18120, train/grad_norm: 0.016170240938663483
Step: 18120, train/learning_rate: 2.84388388536172e-05
Step: 18120, train/epoch: 4.312232494354248
Step: 18130, train/loss: 0.07970000058412552
Step: 18130, train/grad_norm: 0.0036072726361453533
Step: 18130, train/learning_rate: 2.8426939024939202e-05
Step: 18130, train/epoch: 4.314611911773682
Step: 18140, train/loss: 0.04670000076293945
Step: 18140, train/grad_norm: 0.013698752038180828
Step: 18140, train/learning_rate: 2.841504101525061e-05
Step: 18140, train/epoch: 4.316991806030273
Step: 18150, train/loss: 0.2978000044822693
Step: 18150, train/grad_norm: 0.030631758272647858
Step: 18150, train/learning_rate: 2.840314118657261e-05
Step: 18150, train/epoch: 4.319371700286865
Step: 18160, train/loss: 0.07479999959468842
Step: 18160, train/grad_norm: 0.2073211669921875
Step: 18160, train/learning_rate: 2.8391241357894614e-05
Step: 18160, train/epoch: 4.321751594543457
Step: 18170, train/loss: 0.1543000042438507
Step: 18170, train/grad_norm: 0.04501720890402794
Step: 18170, train/learning_rate: 2.837934334820602e-05
Step: 18170, train/epoch: 4.324131488800049
Step: 18180, train/loss: 0.0142000000923872
Step: 18180, train/grad_norm: 0.010480809956789017
Step: 18180, train/learning_rate: 2.8367443519528024e-05
Step: 18180, train/epoch: 4.326511383056641
Step: 18190, train/loss: 0.11460000276565552
Step: 18190, train/grad_norm: 0.03577062115073204
Step: 18190, train/learning_rate: 2.835554550983943e-05
Step: 18190, train/epoch: 4.328890800476074
Step: 18200, train/loss: 0.131400004029274
Step: 18200, train/grad_norm: 19.394001007080078
Step: 18200, train/learning_rate: 2.8343645681161433e-05
Step: 18200, train/epoch: 4.331270694732666
Step: 18210, train/loss: 0.01600000075995922
Step: 18210, train/grad_norm: 0.9781272411346436
Step: 18210, train/learning_rate: 2.833174767147284e-05
Step: 18210, train/epoch: 4.333650588989258
Step: 18220, train/loss: 0.0027000000700354576
Step: 18220, train/grad_norm: 0.014697768725454807
Step: 18220, train/learning_rate: 2.8319847842794843e-05
Step: 18220, train/epoch: 4.33603048324585
Step: 18230, train/loss: 0.029600000008940697
Step: 18230, train/grad_norm: 0.0006497557042166591
Step: 18230, train/learning_rate: 2.8307948014116846e-05
Step: 18230, train/epoch: 4.338410377502441
Step: 18240, train/loss: 0.15230000019073486
Step: 18240, train/grad_norm: 0.008216075599193573
Step: 18240, train/learning_rate: 2.8296050004428253e-05
Step: 18240, train/epoch: 4.340790271759033
Step: 18250, train/loss: 0.2076999992132187
Step: 18250, train/grad_norm: 5.539700508117676
Step: 18250, train/learning_rate: 2.8284150175750256e-05
Step: 18250, train/epoch: 4.343169689178467
Step: 18260, train/loss: 0.011099999770522118
Step: 18260, train/grad_norm: 0.3962365984916687
Step: 18260, train/learning_rate: 2.8272252166061662e-05
Step: 18260, train/epoch: 4.345549583435059
Step: 18270, train/loss: 0.0007999999797903001
Step: 18270, train/grad_norm: 0.018902061507105827
Step: 18270, train/learning_rate: 2.8260352337383665e-05
Step: 18270, train/epoch: 4.34792947769165
Step: 18280, train/loss: 0.00570000009611249
Step: 18280, train/grad_norm: 12.372760772705078
Step: 18280, train/learning_rate: 2.8248452508705668e-05
Step: 18280, train/epoch: 4.350309371948242
Step: 18290, train/loss: 0.00019999999494757503
Step: 18290, train/grad_norm: 0.005799780134111643
Step: 18290, train/learning_rate: 2.8236554499017075e-05
Step: 18290, train/epoch: 4.352689266204834
Step: 18300, train/loss: 0.15639999508857727
Step: 18300, train/grad_norm: 12.888898849487305
Step: 18300, train/learning_rate: 2.8224654670339078e-05
Step: 18300, train/epoch: 4.355069160461426
Step: 18310, train/loss: 0.04340000078082085
Step: 18310, train/grad_norm: 0.013884615153074265
Step: 18310, train/learning_rate: 2.8212756660650484e-05
Step: 18310, train/epoch: 4.357449054718018
Step: 18320, train/loss: 0.006000000052154064
Step: 18320, train/grad_norm: 0.07965054363012314
Step: 18320, train/learning_rate: 2.8200856831972487e-05
Step: 18320, train/epoch: 4.359828472137451
Step: 18330, train/loss: 0.26739999651908875
Step: 18330, train/grad_norm: 0.012344255112111568
Step: 18330, train/learning_rate: 2.818895700329449e-05
Step: 18330, train/epoch: 4.362208366394043
Step: 18340, train/loss: 0.13910000026226044
Step: 18340, train/grad_norm: 81.83111572265625
Step: 18340, train/learning_rate: 2.8177058993605897e-05
Step: 18340, train/epoch: 4.364588260650635
Step: 18350, train/loss: 0.20239999890327454
Step: 18350, train/grad_norm: 0.0002239889290649444
Step: 18350, train/learning_rate: 2.81651591649279e-05
Step: 18350, train/epoch: 4.366968154907227
Step: 18360, train/loss: 0.1850000023841858
Step: 18360, train/grad_norm: 20.083965301513672
Step: 18360, train/learning_rate: 2.8153261155239306e-05
Step: 18360, train/epoch: 4.369348049163818
Step: 18370, train/loss: 0.014399999752640724
Step: 18370, train/grad_norm: 9.130889892578125
Step: 18370, train/learning_rate: 2.814136132656131e-05
Step: 18370, train/epoch: 4.37172794342041
Step: 18380, train/loss: 0.09610000252723694
Step: 18380, train/grad_norm: 0.026828359812498093
Step: 18380, train/learning_rate: 2.8129461497883312e-05
Step: 18380, train/epoch: 4.374107360839844
Step: 18390, train/loss: 0.1509999930858612
Step: 18390, train/grad_norm: 0.0032597254030406475
Step: 18390, train/learning_rate: 2.811756348819472e-05
Step: 18390, train/epoch: 4.3764872550964355
Step: 18400, train/loss: 0.22789999842643738
Step: 18400, train/grad_norm: 0.10018118470907211
Step: 18400, train/learning_rate: 2.8105663659516722e-05
Step: 18400, train/epoch: 4.378867149353027
Step: 18410, train/loss: 0.00800000037997961
Step: 18410, train/grad_norm: 0.005222264211624861
Step: 18410, train/learning_rate: 2.809376564982813e-05
Step: 18410, train/epoch: 4.381247043609619
Step: 18420, train/loss: 0.2538999915122986
Step: 18420, train/grad_norm: 27.01083755493164
Step: 18420, train/learning_rate: 2.808186582115013e-05
Step: 18420, train/epoch: 4.383626937866211
Step: 18430, train/loss: 0.018300000578165054
Step: 18430, train/grad_norm: 0.004539187066257
Step: 18430, train/learning_rate: 2.8069965992472135e-05
Step: 18430, train/epoch: 4.386006832122803
Step: 18440, train/loss: 0.07930000126361847
Step: 18440, train/grad_norm: 0.09874488413333893
Step: 18440, train/learning_rate: 2.805806798278354e-05
Step: 18440, train/epoch: 4.388386249542236
Step: 18450, train/loss: 0.005100000184029341
Step: 18450, train/grad_norm: 0.001030400744639337
Step: 18450, train/learning_rate: 2.8046168154105544e-05
Step: 18450, train/epoch: 4.390766143798828
Step: 18460, train/loss: 0.0892999991774559
Step: 18460, train/grad_norm: 43.79401397705078
Step: 18460, train/learning_rate: 2.803427014441695e-05
Step: 18460, train/epoch: 4.39314603805542
Step: 18470, train/loss: 0.04230000078678131
Step: 18470, train/grad_norm: 0.008311999961733818
Step: 18470, train/learning_rate: 2.8022370315738954e-05
Step: 18470, train/epoch: 4.395525932312012
Step: 18480, train/loss: 0.055799998342990875
Step: 18480, train/grad_norm: 0.003484301036223769
Step: 18480, train/learning_rate: 2.8010470487060957e-05
Step: 18480, train/epoch: 4.3979058265686035
Step: 18490, train/loss: 0.03020000085234642
Step: 18490, train/grad_norm: 0.08567456156015396
Step: 18490, train/learning_rate: 2.7998572477372363e-05
Step: 18490, train/epoch: 4.400285720825195
Step: 18500, train/loss: 0.10639999806880951
Step: 18500, train/grad_norm: 0.12002521008253098
Step: 18500, train/learning_rate: 2.7986672648694366e-05
Step: 18500, train/epoch: 4.402665615081787
Step: 18510, train/loss: 0.05420000106096268
Step: 18510, train/grad_norm: 11.400294303894043
Step: 18510, train/learning_rate: 2.7974774639005773e-05
Step: 18510, train/epoch: 4.405045032501221
Step: 18520, train/loss: 0.03189999982714653
Step: 18520, train/grad_norm: 0.0333828330039978
Step: 18520, train/learning_rate: 2.7962874810327776e-05
Step: 18520, train/epoch: 4.4074249267578125
Step: 18530, train/loss: 0.09229999780654907
Step: 18530, train/grad_norm: 0.004054289776831865
Step: 18530, train/learning_rate: 2.795097498164978e-05
Step: 18530, train/epoch: 4.409804821014404
Step: 18540, train/loss: 0.10109999775886536
Step: 18540, train/grad_norm: 0.014738759957253933
Step: 18540, train/learning_rate: 2.7939076971961185e-05
Step: 18540, train/epoch: 4.412184715270996
Step: 18550, train/loss: 0.11140000075101852
Step: 18550, train/grad_norm: 0.02634526789188385
Step: 18550, train/learning_rate: 2.792717714328319e-05
Step: 18550, train/epoch: 4.414564609527588
Step: 18560, train/loss: 0.002400000113993883
Step: 18560, train/grad_norm: 0.0900418683886528
Step: 18560, train/learning_rate: 2.7915279133594595e-05
Step: 18560, train/epoch: 4.41694450378418
Step: 18570, train/loss: 0.11159999668598175
Step: 18570, train/grad_norm: 0.3911801874637604
Step: 18570, train/learning_rate: 2.7903379304916598e-05
Step: 18570, train/epoch: 4.419323921203613
Step: 18580, train/loss: 0.1274999976158142
Step: 18580, train/grad_norm: 10.106945037841797
Step: 18580, train/learning_rate: 2.78914794762386e-05
Step: 18580, train/epoch: 4.421703815460205
Step: 18590, train/loss: 0.22859999537467957
Step: 18590, train/grad_norm: 0.039818134158849716
Step: 18590, train/learning_rate: 2.7879581466550007e-05
Step: 18590, train/epoch: 4.424083709716797
Step: 18600, train/loss: 0.19580000638961792
Step: 18600, train/grad_norm: 0.0017942069098353386
Step: 18600, train/learning_rate: 2.786768163787201e-05
Step: 18600, train/epoch: 4.426463603973389
Step: 18610, train/loss: 0.125
Step: 18610, train/grad_norm: 14.954792022705078
Step: 18610, train/learning_rate: 2.7855783628183417e-05
Step: 18610, train/epoch: 4.4288434982299805
Step: 18620, train/loss: 0.10580000281333923
Step: 18620, train/grad_norm: 1.4014277458190918
Step: 18620, train/learning_rate: 2.784388379950542e-05
Step: 18620, train/epoch: 4.431223392486572
Step: 18630, train/loss: 0.1145000010728836
Step: 18630, train/grad_norm: 25.37962532043457
Step: 18630, train/learning_rate: 2.7831983970827423e-05
Step: 18630, train/epoch: 4.433602809906006
Step: 18640, train/loss: 0.17890000343322754
Step: 18640, train/grad_norm: 24.02713966369629
Step: 18640, train/learning_rate: 2.782008596113883e-05
Step: 18640, train/epoch: 4.435982704162598
Step: 18650, train/loss: 0.03720000013709068
Step: 18650, train/grad_norm: 0.28213414549827576
Step: 18650, train/learning_rate: 2.7808186132460833e-05
Step: 18650, train/epoch: 4.4383625984191895
Step: 18660, train/loss: 0.003599999938160181
Step: 18660, train/grad_norm: 7.042698860168457
Step: 18660, train/learning_rate: 2.779628812277224e-05
Step: 18660, train/epoch: 4.440742492675781
Step: 18670, train/loss: 0.11060000211000443
Step: 18670, train/grad_norm: 0.004583136178553104
Step: 18670, train/learning_rate: 2.7784388294094242e-05
Step: 18670, train/epoch: 4.443122386932373
Step: 18680, train/loss: 0.09449999779462814
Step: 18680, train/grad_norm: 0.11092258244752884
Step: 18680, train/learning_rate: 2.7772488465416245e-05
Step: 18680, train/epoch: 4.445502281188965
Step: 18690, train/loss: 0.15410000085830688
Step: 18690, train/grad_norm: 0.030041445046663284
Step: 18690, train/learning_rate: 2.7760590455727652e-05
Step: 18690, train/epoch: 4.447882175445557
Step: 18700, train/loss: 0.01889999955892563
Step: 18700, train/grad_norm: 0.3130938708782196
Step: 18700, train/learning_rate: 2.7748690627049655e-05
Step: 18700, train/epoch: 4.45026159286499
Step: 18710, train/loss: 0.0013000000035390258
Step: 18710, train/grad_norm: 0.19982054829597473
Step: 18710, train/learning_rate: 2.773679261736106e-05
Step: 18710, train/epoch: 4.452641487121582
Step: 18720, train/loss: 0.010400000028312206
Step: 18720, train/grad_norm: 8.242552757263184
Step: 18720, train/learning_rate: 2.7724892788683064e-05
Step: 18720, train/epoch: 4.455021381378174
Step: 18730, train/loss: 0.007699999958276749
Step: 18730, train/grad_norm: 1.2557941675186157
Step: 18730, train/learning_rate: 2.7712992960005067e-05
Step: 18730, train/epoch: 4.457401275634766
Step: 18740, train/loss: 0.030300000682473183
Step: 18740, train/grad_norm: 0.04975982755422592
Step: 18740, train/learning_rate: 2.7701094950316474e-05
Step: 18740, train/epoch: 4.459781169891357
Step: 18750, train/loss: 0.01759999990463257
Step: 18750, train/grad_norm: 4.004457473754883
Step: 18750, train/learning_rate: 2.7689195121638477e-05
Step: 18750, train/epoch: 4.462161064147949
Step: 18760, train/loss: 0.00839999970048666
Step: 18760, train/grad_norm: 5.667309284210205
Step: 18760, train/learning_rate: 2.7677297111949883e-05
Step: 18760, train/epoch: 4.464540481567383
Step: 18770, train/loss: 0.09109999984502792
Step: 18770, train/grad_norm: 0.0007440947811119258
Step: 18770, train/learning_rate: 2.7665397283271886e-05
Step: 18770, train/epoch: 4.466920375823975
Step: 18780, train/loss: 0.08959999680519104
Step: 18780, train/grad_norm: 30.513120651245117
Step: 18780, train/learning_rate: 2.765349745459389e-05
Step: 18780, train/epoch: 4.469300270080566
Step: 18790, train/loss: 0.0017000000225380063
Step: 18790, train/grad_norm: 0.04865476116538048
Step: 18790, train/learning_rate: 2.7641599444905296e-05
Step: 18790, train/epoch: 4.471680164337158
Step: 18800, train/loss: 0.023900000378489494
Step: 18800, train/grad_norm: 0.32915621995925903
Step: 18800, train/learning_rate: 2.76296996162273e-05
Step: 18800, train/epoch: 4.47406005859375
Step: 18810, train/loss: 0.05640000104904175
Step: 18810, train/grad_norm: 0.01252018567174673
Step: 18810, train/learning_rate: 2.7617801606538706e-05
Step: 18810, train/epoch: 4.476439952850342
Step: 18820, train/loss: 0.04170000180602074
Step: 18820, train/grad_norm: 40.30960464477539
Step: 18820, train/learning_rate: 2.760590177786071e-05
Step: 18820, train/epoch: 4.478819847106934
Step: 18830, train/loss: 0.0012000000569969416
Step: 18830, train/grad_norm: 0.0027202097699046135
Step: 18830, train/learning_rate: 2.759400194918271e-05
Step: 18830, train/epoch: 4.481199264526367
Step: 18840, train/loss: 0.31150001287460327
Step: 18840, train/grad_norm: 49.43224334716797
Step: 18840, train/learning_rate: 2.7582103939494118e-05
Step: 18840, train/epoch: 4.483579158782959
Step: 18850, train/loss: 0.02879999950528145
Step: 18850, train/grad_norm: 0.01648443378508091
Step: 18850, train/learning_rate: 2.757020411081612e-05
Step: 18850, train/epoch: 4.485959053039551
Step: 18860, train/loss: 0.023600000888109207
Step: 18860, train/grad_norm: 0.006417009513825178
Step: 18860, train/learning_rate: 2.7558306101127528e-05
Step: 18860, train/epoch: 4.488338947296143
Step: 18870, train/loss: 0.1234000027179718
Step: 18870, train/grad_norm: 0.0015100693562999368
Step: 18870, train/learning_rate: 2.754640627244953e-05
Step: 18870, train/epoch: 4.490718841552734
Step: 18880, train/loss: 0.021900000050663948
Step: 18880, train/grad_norm: 1.9831961393356323
Step: 18880, train/learning_rate: 2.7534508262760937e-05
Step: 18880, train/epoch: 4.493098735809326
Step: 18890, train/loss: 0.0005000000237487257
Step: 18890, train/grad_norm: 0.016919424757361412
Step: 18890, train/learning_rate: 2.752260843408294e-05
Step: 18890, train/epoch: 4.49547815322876
Step: 18900, train/loss: 0.08020000159740448
Step: 18900, train/grad_norm: 0.04777618125081062
Step: 18900, train/learning_rate: 2.7510708605404943e-05
Step: 18900, train/epoch: 4.497858047485352
Step: 18910, train/loss: 0.12559999525547028
Step: 18910, train/grad_norm: 0.0044795554131269455
Step: 18910, train/learning_rate: 2.749881059571635e-05
Step: 18910, train/epoch: 4.500237941741943
Step: 18920, train/loss: 0.019300000742077827
Step: 18920, train/grad_norm: 49.326026916503906
Step: 18920, train/learning_rate: 2.7486910767038353e-05
Step: 18920, train/epoch: 4.502617835998535
Step: 18930, train/loss: 0.029100000858306885
Step: 18930, train/grad_norm: 0.19803693890571594
Step: 18930, train/learning_rate: 2.747501275734976e-05
Step: 18930, train/epoch: 4.504997730255127
Step: 18940, train/loss: 0.22120000422000885
Step: 18940, train/grad_norm: 0.0015061458107084036
Step: 18940, train/learning_rate: 2.7463112928671762e-05
Step: 18940, train/epoch: 4.507377624511719
Step: 18950, train/loss: 0.09489999711513519
Step: 18950, train/grad_norm: 1.723119854927063
Step: 18950, train/learning_rate: 2.7451213099993765e-05
Step: 18950, train/epoch: 4.509757041931152
Step: 18960, train/loss: 0.06960000097751617
Step: 18960, train/grad_norm: 0.0026265420019626617
Step: 18960, train/learning_rate: 2.7439315090305172e-05
Step: 18960, train/epoch: 4.512136936187744
Step: 18970, train/loss: 0.015399999916553497
Step: 18970, train/grad_norm: 0.1251065880060196
Step: 18970, train/learning_rate: 2.7427415261627175e-05
Step: 18970, train/epoch: 4.514516830444336
Step: 18980, train/loss: 0.21850000321865082
Step: 18980, train/grad_norm: 0.010322927497327328
Step: 18980, train/learning_rate: 2.741551725193858e-05
Step: 18980, train/epoch: 4.516896724700928
Step: 18990, train/loss: 0.013000000268220901
Step: 18990, train/grad_norm: 1.186970829963684
Step: 18990, train/learning_rate: 2.7403617423260584e-05
Step: 18990, train/epoch: 4.5192766189575195
Step: 19000, train/loss: 0.16500000655651093
Step: 19000, train/grad_norm: 18.88625717163086
Step: 19000, train/learning_rate: 2.7391717594582587e-05
Step: 19000, train/epoch: 4.521656513214111
Step: 19010, train/loss: 0.028999999165534973
Step: 19010, train/grad_norm: 0.002207942074164748
Step: 19010, train/learning_rate: 2.7379819584893994e-05
Step: 19010, train/epoch: 4.524036407470703
Step: 19020, train/loss: 0.07400000095367432
Step: 19020, train/grad_norm: 0.048121288418769836
Step: 19020, train/learning_rate: 2.7367919756215997e-05
Step: 19020, train/epoch: 4.526415824890137
Step: 19030, train/loss: 0.04919999837875366
Step: 19030, train/grad_norm: 0.03432305157184601
Step: 19030, train/learning_rate: 2.7356021746527404e-05
Step: 19030, train/epoch: 4.5287957191467285
Step: 19040, train/loss: 0.06759999692440033
Step: 19040, train/grad_norm: 35.50371170043945
Step: 19040, train/learning_rate: 2.7344121917849407e-05
Step: 19040, train/epoch: 4.53117561340332
Step: 19050, train/loss: 0.23559999465942383
Step: 19050, train/grad_norm: 45.76825714111328
Step: 19050, train/learning_rate: 2.733222208917141e-05
Step: 19050, train/epoch: 4.533555507659912
Step: 19060, train/loss: 0.01889999955892563
Step: 19060, train/grad_norm: 0.01987568289041519
Step: 19060, train/learning_rate: 2.7320324079482816e-05
Step: 19060, train/epoch: 4.535935401916504
Step: 19070, train/loss: 0.13429999351501465
Step: 19070, train/grad_norm: 0.18317760527133942
Step: 19070, train/learning_rate: 2.730842425080482e-05
Step: 19070, train/epoch: 4.538315296173096
Step: 19080, train/loss: 0.05000000074505806
Step: 19080, train/grad_norm: 29.639938354492188
Step: 19080, train/learning_rate: 2.7296526241116226e-05
Step: 19080, train/epoch: 4.540694713592529
Step: 19090, train/loss: 0.062300000339746475
Step: 19090, train/grad_norm: 0.033582985401153564
Step: 19090, train/learning_rate: 2.728462641243823e-05
Step: 19090, train/epoch: 4.543074607849121
Step: 19100, train/loss: 0.0012000000569969416
Step: 19100, train/grad_norm: 0.012346860021352768
Step: 19100, train/learning_rate: 2.7272726583760232e-05
Step: 19100, train/epoch: 4.545454502105713
Step: 19110, train/loss: 0.14509999752044678
Step: 19110, train/grad_norm: 0.009699159301817417
Step: 19110, train/learning_rate: 2.7260828574071638e-05
Step: 19110, train/epoch: 4.547834396362305
Step: 19120, train/loss: 0.09470000118017197
Step: 19120, train/grad_norm: 0.02850053645670414
Step: 19120, train/learning_rate: 2.724892874539364e-05
Step: 19120, train/epoch: 4.5502142906188965
Step: 19130, train/loss: 0.05000000074505806
Step: 19130, train/grad_norm: 0.18410992622375488
Step: 19130, train/learning_rate: 2.7237030735705048e-05
Step: 19130, train/epoch: 4.552594184875488
Step: 19140, train/loss: 0.08720000088214874
Step: 19140, train/grad_norm: 0.006308764684945345
Step: 19140, train/learning_rate: 2.722513090702705e-05
Step: 19140, train/epoch: 4.554973602294922
Step: 19150, train/loss: 0.16030000150203705
Step: 19150, train/grad_norm: 0.001089423312805593
Step: 19150, train/learning_rate: 2.7213231078349054e-05
Step: 19150, train/epoch: 4.557353496551514
Step: 19160, train/loss: 0.006399999838322401
Step: 19160, train/grad_norm: 0.2697575092315674
Step: 19160, train/learning_rate: 2.720133306866046e-05
Step: 19160, train/epoch: 4.5597333908081055
Step: 19170, train/loss: 0.026100000366568565
Step: 19170, train/grad_norm: 0.006927875801920891
Step: 19170, train/learning_rate: 2.7189433239982463e-05
Step: 19170, train/epoch: 4.562113285064697
Step: 19180, train/loss: 0.036400001496076584
Step: 19180, train/grad_norm: 0.02149949222803116
Step: 19180, train/learning_rate: 2.717753523029387e-05
Step: 19180, train/epoch: 4.564493179321289
Step: 19190, train/loss: 0.012000000104308128
Step: 19190, train/grad_norm: 0.2155522108078003
Step: 19190, train/learning_rate: 2.7165635401615873e-05
Step: 19190, train/epoch: 4.566873073577881
Step: 19200, train/loss: 0.13079999387264252
Step: 19200, train/grad_norm: 0.005745662376284599
Step: 19200, train/learning_rate: 2.7153735572937876e-05
Step: 19200, train/epoch: 4.569252967834473
Step: 19210, train/loss: 0.010700000450015068
Step: 19210, train/grad_norm: 20.757604598999023
Step: 19210, train/learning_rate: 2.7141837563249283e-05
Step: 19210, train/epoch: 4.571632385253906
Step: 19220, train/loss: 0.06390000134706497
Step: 19220, train/grad_norm: 0.03308361768722534
Step: 19220, train/learning_rate: 2.7129937734571286e-05
Step: 19220, train/epoch: 4.574012279510498
Step: 19230, train/loss: 0.05559999868273735
Step: 19230, train/grad_norm: 0.003992574755102396
Step: 19230, train/learning_rate: 2.7118039724882692e-05
Step: 19230, train/epoch: 4.57639217376709
Step: 19240, train/loss: 0.05820000171661377
Step: 19240, train/grad_norm: 0.002020458457991481
Step: 19240, train/learning_rate: 2.7106139896204695e-05
Step: 19240, train/epoch: 4.578772068023682
Step: 19250, train/loss: 0.03920000046491623
Step: 19250, train/grad_norm: 47.51350784301758
Step: 19250, train/learning_rate: 2.7094240067526698e-05
Step: 19250, train/epoch: 4.581151962280273
Step: 19260, train/loss: 0.08470000326633453
Step: 19260, train/grad_norm: 0.0026142888236790895
Step: 19260, train/learning_rate: 2.7082342057838105e-05
Step: 19260, train/epoch: 4.583531856536865
Step: 19270, train/loss: 0.008299999870359898
Step: 19270, train/grad_norm: 0.006620543077588081
Step: 19270, train/learning_rate: 2.7070442229160108e-05
Step: 19270, train/epoch: 4.585911273956299
Step: 19280, train/loss: 0.017500000074505806
Step: 19280, train/grad_norm: 21.772178649902344
Step: 19280, train/learning_rate: 2.7058544219471514e-05
Step: 19280, train/epoch: 4.588291168212891
Step: 19290, train/loss: 0.04919999837875366
Step: 19290, train/grad_norm: 26.65421485900879
Step: 19290, train/learning_rate: 2.7046644390793517e-05
Step: 19290, train/epoch: 4.590671062469482
Step: 19300, train/loss: 0.19089999794960022
Step: 19300, train/grad_norm: 57.95320129394531
Step: 19300, train/learning_rate: 2.703474456211552e-05
Step: 19300, train/epoch: 4.593050956726074
Step: 19310, train/loss: 0.1476999968290329
Step: 19310, train/grad_norm: 0.2675834596157074
Step: 19310, train/learning_rate: 2.7022846552426927e-05
Step: 19310, train/epoch: 4.595430850982666
Step: 19320, train/loss: 0.1835000067949295
Step: 19320, train/grad_norm: 31.516677856445312
Step: 19320, train/learning_rate: 2.701094672374893e-05
Step: 19320, train/epoch: 4.597810745239258
Step: 19330, train/loss: 0.007899999618530273
Step: 19330, train/grad_norm: 0.046853337436914444
Step: 19330, train/learning_rate: 2.6999048714060336e-05
Step: 19330, train/epoch: 4.600190162658691
Step: 19340, train/loss: 0.003000000026077032
Step: 19340, train/grad_norm: 0.044246867299079895
Step: 19340, train/learning_rate: 2.698714888538234e-05
Step: 19340, train/epoch: 4.602570056915283
Step: 19350, train/loss: 0.0017000000225380063
Step: 19350, train/grad_norm: 0.010361856780946255
Step: 19350, train/learning_rate: 2.6975249056704342e-05
Step: 19350, train/epoch: 4.604949951171875
Step: 19360, train/loss: 0.13179999589920044
Step: 19360, train/grad_norm: 0.053558651357889175
Step: 19360, train/learning_rate: 2.696335104701575e-05
Step: 19360, train/epoch: 4.607329845428467
Step: 19370, train/loss: 0.006300000008195639
Step: 19370, train/grad_norm: 22.488157272338867
Step: 19370, train/learning_rate: 2.6951451218337752e-05
Step: 19370, train/epoch: 4.609709739685059
Step: 19380, train/loss: 0.014700000174343586
Step: 19380, train/grad_norm: 0.04501362890005112
Step: 19380, train/learning_rate: 2.693955320864916e-05
Step: 19380, train/epoch: 4.61208963394165
Step: 19390, train/loss: 0.015399999916553497
Step: 19390, train/grad_norm: 0.13497507572174072
Step: 19390, train/learning_rate: 2.692765337997116e-05
Step: 19390, train/epoch: 4.614469528198242
Step: 19400, train/loss: 0.001500000013038516
Step: 19400, train/grad_norm: 0.0005760814528912306
Step: 19400, train/learning_rate: 2.6915753551293164e-05
Step: 19400, train/epoch: 4.616848945617676
Step: 19410, train/loss: 0.008500000461935997
Step: 19410, train/grad_norm: 0.25866377353668213
Step: 19410, train/learning_rate: 2.690385554160457e-05
Step: 19410, train/epoch: 4.619228839874268
Step: 19420, train/loss: 0.00019999999494757503
Step: 19420, train/grad_norm: 0.05012219399213791
Step: 19420, train/learning_rate: 2.6891955712926574e-05
Step: 19420, train/epoch: 4.621608734130859
Step: 19430, train/loss: 0.08150000125169754
Step: 19430, train/grad_norm: 8.558429718017578
Step: 19430, train/learning_rate: 2.688005770323798e-05
Step: 19430, train/epoch: 4.623988628387451
Step: 19440, train/loss: 0.05490000173449516
Step: 19440, train/grad_norm: 41.25190353393555
Step: 19440, train/learning_rate: 2.6868157874559984e-05
Step: 19440, train/epoch: 4.626368522644043
Step: 19450, train/loss: 0.014399999752640724
Step: 19450, train/grad_norm: 0.0007437860476784408
Step: 19450, train/learning_rate: 2.6856258045881987e-05
Step: 19450, train/epoch: 4.628748416900635
Step: 19460, train/loss: 0.01679999940097332
Step: 19460, train/grad_norm: 0.001014753244817257
Step: 19460, train/learning_rate: 2.6844360036193393e-05
Step: 19460, train/epoch: 4.631127834320068
Step: 19470, train/loss: 0.13269999623298645
Step: 19470, train/grad_norm: 0.037862252444028854
Step: 19470, train/learning_rate: 2.6832460207515396e-05
Step: 19470, train/epoch: 4.63350772857666
Step: 19480, train/loss: 0.00570000009611249
Step: 19480, train/grad_norm: 0.0037577981129288673
Step: 19480, train/learning_rate: 2.6820562197826803e-05
Step: 19480, train/epoch: 4.635887622833252
Step: 19490, train/loss: 0.0008999999845400453
Step: 19490, train/grad_norm: 1.4784811735153198
Step: 19490, train/learning_rate: 2.6808662369148806e-05
Step: 19490, train/epoch: 4.638267517089844
Step: 19500, train/loss: 0.05889999866485596
Step: 19500, train/grad_norm: 0.0001976202183868736
Step: 19500, train/learning_rate: 2.679676254047081e-05
Step: 19500, train/epoch: 4.6406474113464355
Step: 19510, train/loss: 0.022299999371170998
Step: 19510, train/grad_norm: 0.029509782791137695
Step: 19510, train/learning_rate: 2.6784864530782215e-05
Step: 19510, train/epoch: 4.643027305603027
Step: 19520, train/loss: 0.029100000858306885
Step: 19520, train/grad_norm: 0.000991118373349309
Step: 19520, train/learning_rate: 2.6772964702104218e-05
Step: 19520, train/epoch: 4.645406723022461
Step: 19530, train/loss: 0.3718000054359436
Step: 19530, train/grad_norm: 0.6590318083763123
Step: 19530, train/learning_rate: 2.6761066692415625e-05
Step: 19530, train/epoch: 4.647786617279053
Step: 19540, train/loss: 0.1445000022649765
Step: 19540, train/grad_norm: 0.2408401519060135
Step: 19540, train/learning_rate: 2.6749166863737628e-05
Step: 19540, train/epoch: 4.6501665115356445
Step: 19550, train/loss: 0.2881999909877777
Step: 19550, train/grad_norm: 0.36265498399734497
Step: 19550, train/learning_rate: 2.6737268854049034e-05
Step: 19550, train/epoch: 4.652546405792236
Step: 19560, train/loss: 0.003700000001117587
Step: 19560, train/grad_norm: 0.0018846266902983189
Step: 19560, train/learning_rate: 2.6725369025371037e-05
Step: 19560, train/epoch: 4.654926300048828
Step: 19570, train/loss: 0.23260000348091125
Step: 19570, train/grad_norm: 9.971447944641113
Step: 19570, train/learning_rate: 2.671346919669304e-05
Step: 19570, train/epoch: 4.65730619430542
Step: 19580, train/loss: 0.045899998396635056
Step: 19580, train/grad_norm: 12.080794334411621
Step: 19580, train/learning_rate: 2.6701571187004447e-05
Step: 19580, train/epoch: 4.659686088562012
Step: 19590, train/loss: 0.0737999975681305
Step: 19590, train/grad_norm: 0.08494079858064651
Step: 19590, train/learning_rate: 2.668967135832645e-05
Step: 19590, train/epoch: 4.662065505981445
Step: 19600, train/loss: 0.08940000087022781
Step: 19600, train/grad_norm: 0.02749127708375454
Step: 19600, train/learning_rate: 2.6677773348637857e-05
Step: 19600, train/epoch: 4.664445400238037
Step: 19610, train/loss: 0.0015999999595806003
Step: 19610, train/grad_norm: 0.03632086515426636
Step: 19610, train/learning_rate: 2.666587351995986e-05
Step: 19610, train/epoch: 4.666825294494629
Step: 19620, train/loss: 0.04830000177025795
Step: 19620, train/grad_norm: 32.93854522705078
Step: 19620, train/learning_rate: 2.6653973691281863e-05
Step: 19620, train/epoch: 4.669205188751221
Step: 19630, train/loss: 0.12359999865293503
Step: 19630, train/grad_norm: 0.3935307562351227
Step: 19630, train/learning_rate: 2.664207568159327e-05
Step: 19630, train/epoch: 4.6715850830078125
Step: 19640, train/loss: 0.010099999606609344
Step: 19640, train/grad_norm: 0.03782039135694504
Step: 19640, train/learning_rate: 2.6630175852915272e-05
Step: 19640, train/epoch: 4.673964977264404
Step: 19650, train/loss: 0.14329999685287476
Step: 19650, train/grad_norm: 0.0032622686121612787
Step: 19650, train/learning_rate: 2.661827784322668e-05
Step: 19650, train/epoch: 4.676344394683838
Step: 19660, train/loss: 0.009999999776482582
Step: 19660, train/grad_norm: 23.122997283935547
Step: 19660, train/learning_rate: 2.660637801454868e-05
Step: 19660, train/epoch: 4.67872428894043
Step: 19670, train/loss: 0.0010000000474974513
Step: 19670, train/grad_norm: 2.344014883041382
Step: 19670, train/learning_rate: 2.6594478185870685e-05
Step: 19670, train/epoch: 4.6811041831970215
Step: 19680, train/loss: 0.2222999930381775
Step: 19680, train/grad_norm: 8.441582679748535
Step: 19680, train/learning_rate: 2.658258017618209e-05
Step: 19680, train/epoch: 4.683484077453613
Step: 19690, train/loss: 0.2117999941110611
Step: 19690, train/grad_norm: 0.003187543246895075
Step: 19690, train/learning_rate: 2.6570680347504094e-05
Step: 19690, train/epoch: 4.685863971710205
Step: 19700, train/loss: 0.12290000170469284
Step: 19700, train/grad_norm: 50.78327560424805
Step: 19700, train/learning_rate: 2.65587823378155e-05
Step: 19700, train/epoch: 4.688243865966797
Step: 19710, train/loss: 0.0038999998942017555
Step: 19710, train/grad_norm: 5.9904375076293945
Step: 19710, train/learning_rate: 2.6546882509137504e-05
Step: 19710, train/epoch: 4.6906232833862305
Step: 19720, train/loss: 0.03880000114440918
Step: 19720, train/grad_norm: 35.199764251708984
Step: 19720, train/learning_rate: 2.6534982680459507e-05
Step: 19720, train/epoch: 4.693003177642822
Step: 19730, train/loss: 0.1185000017285347
Step: 19730, train/grad_norm: 0.0074562495574355125
Step: 19730, train/learning_rate: 2.6523084670770913e-05
Step: 19730, train/epoch: 4.695383071899414
Step: 19740, train/loss: 0.11800000071525574
Step: 19740, train/grad_norm: 0.006039428990334272
Step: 19740, train/learning_rate: 2.6511184842092916e-05
Step: 19740, train/epoch: 4.697762966156006
Step: 19750, train/loss: 0.10589999705553055
Step: 19750, train/grad_norm: 0.0330330915749073
Step: 19750, train/learning_rate: 2.6499286832404323e-05
Step: 19750, train/epoch: 4.700142860412598
Step: 19760, train/loss: 0.08060000091791153
Step: 19760, train/grad_norm: 30.30100440979004
Step: 19760, train/learning_rate: 2.6487387003726326e-05
Step: 19760, train/epoch: 4.7025227546691895
Step: 19770, train/loss: 0.005200000014156103
Step: 19770, train/grad_norm: 0.007889038883149624
Step: 19770, train/learning_rate: 2.647548717504833e-05
Step: 19770, train/epoch: 4.704902648925781
Step: 19780, train/loss: 0.07370000332593918
Step: 19780, train/grad_norm: 0.005270799156278372
Step: 19780, train/learning_rate: 2.6463589165359735e-05
Step: 19780, train/epoch: 4.707282066345215
Step: 19790, train/loss: 0.07150000333786011
Step: 19790, train/grad_norm: 0.011712737381458282
Step: 19790, train/learning_rate: 2.645168933668174e-05
Step: 19790, train/epoch: 4.709661960601807
Step: 19800, train/loss: 0.005499999970197678
Step: 19800, train/grad_norm: 15.105472564697266
Step: 19800, train/learning_rate: 2.6439791326993145e-05
Step: 19800, train/epoch: 4.712041854858398
Step: 19810, train/loss: 0.002400000113993883
Step: 19810, train/grad_norm: 2.557772636413574
Step: 19810, train/learning_rate: 2.6427891498315148e-05
Step: 19810, train/epoch: 4.71442174911499
Step: 19820, train/loss: 0.0035000001080334187
Step: 19820, train/grad_norm: 0.0328926183283329
Step: 19820, train/learning_rate: 2.641599166963715e-05
Step: 19820, train/epoch: 4.716801643371582
Step: 19830, train/loss: 0.0031999999191612005
Step: 19830, train/grad_norm: 0.0003997049934696406
Step: 19830, train/learning_rate: 2.6404093659948558e-05
Step: 19830, train/epoch: 4.719181537628174
Step: 19840, train/loss: 0.0778999999165535
Step: 19840, train/grad_norm: 0.08916231244802475
Step: 19840, train/learning_rate: 2.639219383127056e-05
Step: 19840, train/epoch: 4.721560955047607
Step: 19850, train/loss: 0.07270000129938126
Step: 19850, train/grad_norm: 0.0014165266184136271
Step: 19850, train/learning_rate: 2.6380295821581967e-05
Step: 19850, train/epoch: 4.723940849304199
Step: 19860, train/loss: 0.11469999700784683
Step: 19860, train/grad_norm: 0.32348111271858215
Step: 19860, train/learning_rate: 2.636839599290397e-05
Step: 19860, train/epoch: 4.726320743560791
Step: 19870, train/loss: 0.0006000000284984708
Step: 19870, train/grad_norm: 1.3864390850067139
Step: 19870, train/learning_rate: 2.6356496164225973e-05
Step: 19870, train/epoch: 4.728700637817383
Step: 19880, train/loss: 0.01889999955892563
Step: 19880, train/grad_norm: 18.437952041625977
Step: 19880, train/learning_rate: 2.634459815453738e-05
Step: 19880, train/epoch: 4.731080532073975
Step: 19890, train/loss: 0.1907999962568283
Step: 19890, train/grad_norm: 0.7014572620391846
Step: 19890, train/learning_rate: 2.6332698325859383e-05
Step: 19890, train/epoch: 4.733460426330566
Step: 19900, train/loss: 0.25679999589920044
Step: 19900, train/grad_norm: 0.0005383135285228491
Step: 19900, train/learning_rate: 2.632080031617079e-05
Step: 19900, train/epoch: 4.73583984375
Step: 19910, train/loss: 0.004000000189989805
Step: 19910, train/grad_norm: 0.6877091526985168
Step: 19910, train/learning_rate: 2.6308900487492792e-05
Step: 19910, train/epoch: 4.738219738006592
Step: 19920, train/loss: 0.07940000295639038
Step: 19920, train/grad_norm: 28.654674530029297
Step: 19920, train/learning_rate: 2.6297000658814795e-05
Step: 19920, train/epoch: 4.740599632263184
Step: 19930, train/loss: 0.06129999831318855
Step: 19930, train/grad_norm: 0.0013480371562764049
Step: 19930, train/learning_rate: 2.6285102649126202e-05
Step: 19930, train/epoch: 4.742979526519775
Step: 19940, train/loss: 0.1746000051498413
Step: 19940, train/grad_norm: 1.043465495109558
Step: 19940, train/learning_rate: 2.6273202820448205e-05
Step: 19940, train/epoch: 4.745359420776367
Step: 19950, train/loss: 0.030300000682473183
Step: 19950, train/grad_norm: 0.0006862498703412712
Step: 19950, train/learning_rate: 2.626130481075961e-05
Step: 19950, train/epoch: 4.747739315032959
Step: 19960, train/loss: 0.058800000697374344
Step: 19960, train/grad_norm: 0.03686673939228058
Step: 19960, train/learning_rate: 2.6249404982081614e-05
Step: 19960, train/epoch: 4.750119209289551
Step: 19970, train/loss: 0.07500000298023224
Step: 19970, train/grad_norm: 0.030334394425153732
Step: 19970, train/learning_rate: 2.6237505153403617e-05
Step: 19970, train/epoch: 4.752498626708984
Step: 19980, train/loss: 0.07400000095367432
Step: 19980, train/grad_norm: 0.0003863125166390091
Step: 19980, train/learning_rate: 2.6225607143715024e-05
Step: 19980, train/epoch: 4.754878520965576
Step: 19990, train/loss: 0.004100000020116568
Step: 19990, train/grad_norm: 0.0012575977016240358
Step: 19990, train/learning_rate: 2.6213707315037027e-05
Step: 19990, train/epoch: 4.757258415222168
Step: 20000, train/loss: 0.00570000009611249
Step: 20000, train/grad_norm: 0.004605702590197325
Step: 20000, train/learning_rate: 2.6201809305348434e-05
Step: 20000, train/epoch: 4.75963830947876
Step: 20010, train/loss: 0.07999999821186066
Step: 20010, train/grad_norm: 27.79542350769043
Step: 20010, train/learning_rate: 2.6189909476670437e-05
Step: 20010, train/epoch: 4.762018203735352
Step: 20020, train/loss: 0.05310000106692314
Step: 20020, train/grad_norm: 0.05609051510691643
Step: 20020, train/learning_rate: 2.617800964799244e-05
Step: 20020, train/epoch: 4.764398097991943
Step: 20030, train/loss: 0.0003000000142492354
Step: 20030, train/grad_norm: 0.02597764879465103
Step: 20030, train/learning_rate: 2.6166111638303846e-05
Step: 20030, train/epoch: 4.766777515411377
Step: 20040, train/loss: 0.08370000123977661
Step: 20040, train/grad_norm: 0.07388550043106079
Step: 20040, train/learning_rate: 2.615421180962585e-05
Step: 20040, train/epoch: 4.769157409667969
Step: 20050, train/loss: 0.22339999675750732
Step: 20050, train/grad_norm: 0.032390933483839035
Step: 20050, train/learning_rate: 2.6142313799937256e-05
Step: 20050, train/epoch: 4.7715373039245605
Step: 20060, train/loss: 0.01730000041425228
Step: 20060, train/grad_norm: 1.9607245922088623
Step: 20060, train/learning_rate: 2.613041397125926e-05
Step: 20060, train/epoch: 4.773917198181152
Step: 20070, train/loss: 0.1868000030517578
Step: 20070, train/grad_norm: 0.020834678784012794
Step: 20070, train/learning_rate: 2.6118514142581262e-05
Step: 20070, train/epoch: 4.776297092437744
Step: 20080, train/loss: 0.11490000039339066
Step: 20080, train/grad_norm: 1.6093237400054932
Step: 20080, train/learning_rate: 2.6106616132892668e-05
Step: 20080, train/epoch: 4.778676986694336
Step: 20090, train/loss: 0.1225999966263771
Step: 20090, train/grad_norm: 0.1365112066268921
Step: 20090, train/learning_rate: 2.609471630421467e-05
Step: 20090, train/epoch: 4.7810564041137695
Step: 20100, train/loss: 0.08659999817609787
Step: 20100, train/grad_norm: 0.015334207564592361
Step: 20100, train/learning_rate: 2.6082818294526078e-05
Step: 20100, train/epoch: 4.783436298370361
Step: 20110, train/loss: 0.07649999856948853
Step: 20110, train/grad_norm: 0.02750139683485031
Step: 20110, train/learning_rate: 2.607091846584808e-05
Step: 20110, train/epoch: 4.785816192626953
Step: 20120, train/loss: 0.006599999964237213
Step: 20120, train/grad_norm: 3.6682605743408203
Step: 20120, train/learning_rate: 2.6059018637170084e-05
Step: 20120, train/epoch: 4.788196086883545
Step: 20130, train/loss: 0.03229999914765358
Step: 20130, train/grad_norm: 0.0008925469010137022
Step: 20130, train/learning_rate: 2.604712062748149e-05
Step: 20130, train/epoch: 4.790575981140137
Step: 20140, train/loss: 0.013799999840557575
Step: 20140, train/grad_norm: 0.061132367700338364
Step: 20140, train/learning_rate: 2.6035220798803493e-05
Step: 20140, train/epoch: 4.7929558753967285
Step: 20150, train/loss: 0.08590000122785568
Step: 20150, train/grad_norm: 0.15742729604244232
Step: 20150, train/learning_rate: 2.60233227891149e-05
Step: 20150, train/epoch: 4.79533576965332
Step: 20160, train/loss: 0.21539999544620514
Step: 20160, train/grad_norm: 0.00021661522623617202
Step: 20160, train/learning_rate: 2.6011422960436903e-05
Step: 20160, train/epoch: 4.797715187072754
Step: 20170, train/loss: 0.04129999876022339
Step: 20170, train/grad_norm: 2.493295907974243
Step: 20170, train/learning_rate: 2.5999523131758906e-05
Step: 20170, train/epoch: 4.800095081329346
Step: 20180, train/loss: 0.17090000212192535
Step: 20180, train/grad_norm: 0.01347957830876112
Step: 20180, train/learning_rate: 2.5987625122070312e-05
Step: 20180, train/epoch: 4.8024749755859375
Step: 20190, train/loss: 0.06920000165700912
Step: 20190, train/grad_norm: 0.03899285942316055
Step: 20190, train/learning_rate: 2.5975725293392316e-05
Step: 20190, train/epoch: 4.804854869842529
Step: 20200, train/loss: 0.0706000030040741
Step: 20200, train/grad_norm: 0.2068241685628891
Step: 20200, train/learning_rate: 2.5963827283703722e-05
Step: 20200, train/epoch: 4.807234764099121
Step: 20210, train/loss: 0.11479999870061874
Step: 20210, train/grad_norm: 0.006903227884322405
Step: 20210, train/learning_rate: 2.5951927455025725e-05
Step: 20210, train/epoch: 4.809614658355713
Step: 20220, train/loss: 0.020800000056624413
Step: 20220, train/grad_norm: 0.0246451273560524
Step: 20220, train/learning_rate: 2.594002944533713e-05
Step: 20220, train/epoch: 4.8119940757751465
Step: 20230, train/loss: 0.00039999998989515007
Step: 20230, train/grad_norm: 0.0004113473987672478
Step: 20230, train/learning_rate: 2.5928129616659135e-05
Step: 20230, train/epoch: 4.814373970031738
Step: 20240, train/loss: 0.14319999516010284
Step: 20240, train/grad_norm: 31.742795944213867
Step: 20240, train/learning_rate: 2.5916229787981138e-05
Step: 20240, train/epoch: 4.81675386428833
Step: 20250, train/loss: 0.08609999716281891
Step: 20250, train/grad_norm: 43.3322639465332
Step: 20250, train/learning_rate: 2.5904331778292544e-05
Step: 20250, train/epoch: 4.819133758544922
Step: 20260, train/loss: 0.04610000178217888
Step: 20260, train/grad_norm: 0.006601488683372736
Step: 20260, train/learning_rate: 2.5892431949614547e-05
Step: 20260, train/epoch: 4.821513652801514
Step: 20270, train/loss: 0.09960000216960907
Step: 20270, train/grad_norm: 0.0036587510257959366
Step: 20270, train/learning_rate: 2.5880533939925954e-05
Step: 20270, train/epoch: 4.8238935470581055
Step: 20280, train/loss: 0.0013000000035390258
Step: 20280, train/grad_norm: 1.4225760698318481
Step: 20280, train/learning_rate: 2.5868634111247957e-05
Step: 20280, train/epoch: 4.826273441314697
Step: 20290, train/loss: 0.01360000018030405
Step: 20290, train/grad_norm: 0.018935197964310646
Step: 20290, train/learning_rate: 2.585673428256996e-05
Step: 20290, train/epoch: 4.828652858734131
Step: 20300, train/loss: 0.021900000050663948
Step: 20300, train/grad_norm: 0.00019059714395552874
Step: 20300, train/learning_rate: 2.5844836272881366e-05
Step: 20300, train/epoch: 4.831032752990723
Step: 20310, train/loss: 0.031599998474121094
Step: 20310, train/grad_norm: 0.3230873644351959
Step: 20310, train/learning_rate: 2.583293644420337e-05
Step: 20310, train/epoch: 4.8334126472473145
Step: 20320, train/loss: 0.13529999554157257
Step: 20320, train/grad_norm: 0.002568761119619012
Step: 20320, train/learning_rate: 2.5821038434514776e-05
Step: 20320, train/epoch: 4.835792541503906
Step: 20330, train/loss: 0.06539999693632126
Step: 20330, train/grad_norm: 0.004298481624573469
Step: 20330, train/learning_rate: 2.580913860583678e-05
Step: 20330, train/epoch: 4.838172435760498
Step: 20340, train/loss: 0.21279999613761902
Step: 20340, train/grad_norm: 9.938313484191895
Step: 20340, train/learning_rate: 2.5797238777158782e-05
Step: 20340, train/epoch: 4.84055233001709
Step: 20350, train/loss: 0.08060000091791153
Step: 20350, train/grad_norm: 9.872118949890137
Step: 20350, train/learning_rate: 2.578534076747019e-05
Step: 20350, train/epoch: 4.842931747436523
Step: 20360, train/loss: 0.1542000025510788
Step: 20360, train/grad_norm: 0.04675110802054405
Step: 20360, train/learning_rate: 2.577344093879219e-05
Step: 20360, train/epoch: 4.845311641693115
Step: 20370, train/loss: 0.018799999728798866
Step: 20370, train/grad_norm: 0.5074158906936646
Step: 20370, train/learning_rate: 2.5761542929103598e-05
Step: 20370, train/epoch: 4.847691535949707
Step: 20380, train/loss: 0.05849999934434891
Step: 20380, train/grad_norm: 0.09272035211324692
Step: 20380, train/learning_rate: 2.57496431004256e-05
Step: 20380, train/epoch: 4.850071430206299
Step: 20390, train/loss: 0.2378000020980835
Step: 20390, train/grad_norm: 42.60296630859375
Step: 20390, train/learning_rate: 2.5737743271747604e-05
Step: 20390, train/epoch: 4.852451324462891
Step: 20400, train/loss: 0.0020000000949949026
Step: 20400, train/grad_norm: 0.02215382270514965
Step: 20400, train/learning_rate: 2.572584526205901e-05
Step: 20400, train/epoch: 4.854831218719482
Step: 20410, train/loss: 0.006899999920278788
Step: 20410, train/grad_norm: 4.13110876083374
Step: 20410, train/learning_rate: 2.5713945433381014e-05
Step: 20410, train/epoch: 4.857210636138916
Step: 20420, train/loss: 0.05959999933838844
Step: 20420, train/grad_norm: 0.04077297821640968
Step: 20420, train/learning_rate: 2.570204742369242e-05
Step: 20420, train/epoch: 4.859590530395508
Step: 20430, train/loss: 0.0006000000284984708
Step: 20430, train/grad_norm: 0.006746985949575901
Step: 20430, train/learning_rate: 2.5690147595014423e-05
Step: 20430, train/epoch: 4.8619704246521
Step: 20440, train/loss: 0.0013000000035390258
Step: 20440, train/grad_norm: 0.022869255393743515
Step: 20440, train/learning_rate: 2.5678247766336426e-05
Step: 20440, train/epoch: 4.864350318908691
Step: 20450, train/loss: 0.1111999973654747
Step: 20450, train/grad_norm: 7.463646411895752
Step: 20450, train/learning_rate: 2.5666349756647833e-05
Step: 20450, train/epoch: 4.866730213165283
Step: 20460, train/loss: 0.0568000003695488
Step: 20460, train/grad_norm: 0.03060108982026577
Step: 20460, train/learning_rate: 2.5654449927969836e-05
Step: 20460, train/epoch: 4.869110107421875
Step: 20470, train/loss: 0.06949999928474426
Step: 20470, train/grad_norm: 0.00141722965054214
Step: 20470, train/learning_rate: 2.5642551918281242e-05
Step: 20470, train/epoch: 4.871490001678467
Step: 20480, train/loss: 0.0013000000035390258
Step: 20480, train/grad_norm: 0.026204559952020645
Step: 20480, train/learning_rate: 2.5630652089603245e-05
Step: 20480, train/epoch: 4.8738694190979
Step: 20490, train/loss: 0.0551999993622303
Step: 20490, train/grad_norm: 0.15644073486328125
Step: 20490, train/learning_rate: 2.5618752260925248e-05
Step: 20490, train/epoch: 4.876249313354492
Step: 20500, train/loss: 0.022600000724196434
Step: 20500, train/grad_norm: 0.004424244165420532
Step: 20500, train/learning_rate: 2.5606854251236655e-05
Step: 20500, train/epoch: 4.878629207611084
Step: 20510, train/loss: 0.1923000067472458
Step: 20510, train/grad_norm: 1.2820757627487183
Step: 20510, train/learning_rate: 2.5594954422558658e-05
Step: 20510, train/epoch: 4.881009101867676
Step: 20520, train/loss: 0.17309999465942383
Step: 20520, train/grad_norm: 0.0008978596888482571
Step: 20520, train/learning_rate: 2.5583056412870064e-05
Step: 20520, train/epoch: 4.883388996124268
Step: 20530, train/loss: 0.07519999891519547
Step: 20530, train/grad_norm: 23.711549758911133
Step: 20530, train/learning_rate: 2.5571156584192067e-05
Step: 20530, train/epoch: 4.885768890380859
Step: 20540, train/loss: 0.1404999941587448
Step: 20540, train/grad_norm: 0.0016871794359758496
Step: 20540, train/learning_rate: 2.555925675551407e-05
Step: 20540, train/epoch: 4.888148307800293
Step: 20550, train/loss: 0.07819999754428864
Step: 20550, train/grad_norm: 0.004253628198057413
Step: 20550, train/learning_rate: 2.5547358745825477e-05
Step: 20550, train/epoch: 4.890528202056885
Step: 20560, train/loss: 0.12530000507831573
Step: 20560, train/grad_norm: 0.17171292006969452
Step: 20560, train/learning_rate: 2.553545891714748e-05
Step: 20560, train/epoch: 4.892908096313477
Step: 20570, train/loss: 0.002300000051036477
Step: 20570, train/grad_norm: 0.00462812464684248
Step: 20570, train/learning_rate: 2.5523560907458887e-05
Step: 20570, train/epoch: 4.895287990570068
Step: 20580, train/loss: 0.07989999651908875
Step: 20580, train/grad_norm: 0.007731558755040169
Step: 20580, train/learning_rate: 2.551166107878089e-05
Step: 20580, train/epoch: 4.89766788482666
Step: 20590, train/loss: 0.08460000157356262
Step: 20590, train/grad_norm: 0.08032195270061493
Step: 20590, train/learning_rate: 2.5499761250102893e-05
Step: 20590, train/epoch: 4.900047779083252
Step: 20600, train/loss: 0.009399999864399433
Step: 20600, train/grad_norm: 0.022788293659687042
Step: 20600, train/learning_rate: 2.54878632404143e-05
Step: 20600, train/epoch: 4.9024271965026855
Step: 20610, train/loss: 0.045099999755620956
Step: 20610, train/grad_norm: 0.0007016416057012975
Step: 20610, train/learning_rate: 2.5475963411736302e-05
Step: 20610, train/epoch: 4.904807090759277
Step: 20620, train/loss: 0.008999999612569809
Step: 20620, train/grad_norm: 0.002855337457731366
Step: 20620, train/learning_rate: 2.546406540204771e-05
Step: 20620, train/epoch: 4.907186985015869
Step: 20630, train/loss: 0.19359999895095825
Step: 20630, train/grad_norm: 0.028929252177476883
Step: 20630, train/learning_rate: 2.545216557336971e-05
Step: 20630, train/epoch: 4.909566879272461
Step: 20640, train/loss: 0.030700000002980232
Step: 20640, train/grad_norm: 25.01062774658203
Step: 20640, train/learning_rate: 2.5440265744691715e-05
Step: 20640, train/epoch: 4.911946773529053
Step: 20650, train/loss: 0.0038999998942017555
Step: 20650, train/grad_norm: 0.02985447086393833
Step: 20650, train/learning_rate: 2.542836773500312e-05
Step: 20650, train/epoch: 4.9143266677856445
Step: 20660, train/loss: 0.004000000189989805
Step: 20660, train/grad_norm: 2.532684326171875
Step: 20660, train/learning_rate: 2.5416467906325124e-05
Step: 20660, train/epoch: 4.916706562042236
Step: 20670, train/loss: 0.050999999046325684
Step: 20670, train/grad_norm: 1.5205501317977905
Step: 20670, train/learning_rate: 2.540456989663653e-05
Step: 20670, train/epoch: 4.91908597946167
Step: 20680, train/loss: 0.011800000444054604
Step: 20680, train/grad_norm: 2.3190340995788574
Step: 20680, train/learning_rate: 2.5392670067958534e-05
Step: 20680, train/epoch: 4.921465873718262
Step: 20690, train/loss: 0.06700000166893005
Step: 20690, train/grad_norm: 0.002074647229164839
Step: 20690, train/learning_rate: 2.5380770239280537e-05
Step: 20690, train/epoch: 4.9238457679748535
Step: 20700, train/loss: 0.019700000062584877
Step: 20700, train/grad_norm: 3.306433916091919
Step: 20700, train/learning_rate: 2.5368872229591943e-05
Step: 20700, train/epoch: 4.926225662231445
Step: 20710, train/loss: 0.12530000507831573
Step: 20710, train/grad_norm: 2.5242409706115723
Step: 20710, train/learning_rate: 2.5356972400913946e-05
Step: 20710, train/epoch: 4.928605556488037
Step: 20720, train/loss: 0.05950000137090683
Step: 20720, train/grad_norm: 0.2262815684080124
Step: 20720, train/learning_rate: 2.5345074391225353e-05
Step: 20720, train/epoch: 4.930985450744629
Step: 20730, train/loss: 0.023099999874830246
Step: 20730, train/grad_norm: 0.45029956102371216
Step: 20730, train/learning_rate: 2.5333174562547356e-05
Step: 20730, train/epoch: 4.9333648681640625
Step: 20740, train/loss: 0.013700000010430813
Step: 20740, train/grad_norm: 0.0031736777164041996
Step: 20740, train/learning_rate: 2.532127473386936e-05
Step: 20740, train/epoch: 4.935744762420654
Step: 20750, train/loss: 0.07530000060796738
Step: 20750, train/grad_norm: 6.255913257598877
Step: 20750, train/learning_rate: 2.5309376724180765e-05
Step: 20750, train/epoch: 4.938124656677246
Step: 20760, train/loss: 0.13330000638961792
Step: 20760, train/grad_norm: 10.61805248260498
Step: 20760, train/learning_rate: 2.529747689550277e-05
Step: 20760, train/epoch: 4.940504550933838
Step: 20770, train/loss: 0.13410000503063202
Step: 20770, train/grad_norm: 0.19376049935817719
Step: 20770, train/learning_rate: 2.5285578885814175e-05
Step: 20770, train/epoch: 4.94288444519043
Step: 20780, train/loss: 0.0038999998942017555
Step: 20780, train/grad_norm: 4.618315696716309
Step: 20780, train/learning_rate: 2.5273679057136178e-05
Step: 20780, train/epoch: 4.9452643394470215
Step: 20790, train/loss: 0.06379999965429306
Step: 20790, train/grad_norm: 78.24925231933594
Step: 20790, train/learning_rate: 2.526177922845818e-05
Step: 20790, train/epoch: 4.947643756866455
Step: 20800, train/loss: 0.02250000089406967
Step: 20800, train/grad_norm: 1.5463253259658813
Step: 20800, train/learning_rate: 2.5249881218769588e-05
Step: 20800, train/epoch: 4.950023651123047
Step: 20810, train/loss: 0.17260000109672546
Step: 20810, train/grad_norm: 0.4404277801513672
Step: 20810, train/learning_rate: 2.523798139009159e-05
Step: 20810, train/epoch: 4.952403545379639
Step: 20820, train/loss: 0.0575999990105629
Step: 20820, train/grad_norm: 0.292244553565979
Step: 20820, train/learning_rate: 2.5226083380402997e-05
Step: 20820, train/epoch: 4.9547834396362305
Step: 20830, train/loss: 0.10220000147819519
Step: 20830, train/grad_norm: 0.2875606417655945
Step: 20830, train/learning_rate: 2.5214183551725e-05
Step: 20830, train/epoch: 4.957163333892822
Step: 20840, train/loss: 0.11140000075101852
Step: 20840, train/grad_norm: 0.0019734003581106663
Step: 20840, train/learning_rate: 2.5202283723047003e-05
Step: 20840, train/epoch: 4.959543228149414
Step: 20850, train/loss: 0.125
Step: 20850, train/grad_norm: 0.019348757341504097
Step: 20850, train/learning_rate: 2.519038571335841e-05
Step: 20850, train/epoch: 4.961923122406006
Step: 20860, train/loss: 0.013299999758601189
Step: 20860, train/grad_norm: 0.0033872940111905336
Step: 20860, train/learning_rate: 2.5178485884680413e-05
Step: 20860, train/epoch: 4.9643025398254395
Step: 20870, train/loss: 0.002400000113993883
Step: 20870, train/grad_norm: 0.0055013117380440235
Step: 20870, train/learning_rate: 2.516658787499182e-05
Step: 20870, train/epoch: 4.966682434082031
Step: 20880, train/loss: 0.12219999730587006
Step: 20880, train/grad_norm: 0.0006374040967784822
Step: 20880, train/learning_rate: 2.5154688046313822e-05
Step: 20880, train/epoch: 4.969062328338623
Step: 20890, train/loss: 0.023900000378489494
Step: 20890, train/grad_norm: 0.002445908496156335
Step: 20890, train/learning_rate: 2.514279003662523e-05
Step: 20890, train/epoch: 4.971442222595215
Step: 20900, train/loss: 0.07649999856948853
Step: 20900, train/grad_norm: 0.0006976565346121788
Step: 20900, train/learning_rate: 2.5130890207947232e-05
Step: 20900, train/epoch: 4.973822116851807
Step: 20910, train/loss: 0.2425999939441681
Step: 20910, train/grad_norm: 1.2241568565368652
Step: 20910, train/learning_rate: 2.5118990379269235e-05
Step: 20910, train/epoch: 4.976202011108398
Step: 20920, train/loss: 0.022700000554323196
Step: 20920, train/grad_norm: 7.708479881286621
Step: 20920, train/learning_rate: 2.510709236958064e-05
Step: 20920, train/epoch: 4.978581428527832
Step: 20930, train/loss: 0.019999999552965164
Step: 20930, train/grad_norm: 0.017827032133936882
Step: 20930, train/learning_rate: 2.5095192540902644e-05
Step: 20930, train/epoch: 4.980961322784424
Step: 20940, train/loss: 0.16140000522136688
Step: 20940, train/grad_norm: 40.15142822265625
Step: 20940, train/learning_rate: 2.508329453121405e-05
Step: 20940, train/epoch: 4.983341217041016
Step: 20950, train/loss: 0.000699999975040555
Step: 20950, train/grad_norm: 0.01053887140005827
Step: 20950, train/learning_rate: 2.5071394702536054e-05
Step: 20950, train/epoch: 4.985721111297607
Step: 20960, train/loss: 0.12950000166893005
Step: 20960, train/grad_norm: 0.11544337123632431
Step: 20960, train/learning_rate: 2.5059494873858057e-05
Step: 20960, train/epoch: 4.988101005554199
Step: 20970, train/loss: 0.04809999838471413
Step: 20970, train/grad_norm: 0.002814467530697584
Step: 20970, train/learning_rate: 2.5047596864169464e-05
Step: 20970, train/epoch: 4.990480899810791
Step: 20980, train/loss: 0.13840000331401825
Step: 20980, train/grad_norm: 0.008191459812223911
Step: 20980, train/learning_rate: 2.5035697035491467e-05
Step: 20980, train/epoch: 4.992860317230225
Step: 20990, train/loss: 0.05000000074505806
Step: 20990, train/grad_norm: 0.011089332401752472
Step: 20990, train/learning_rate: 2.5023799025802873e-05
Step: 20990, train/epoch: 4.995240211486816
Step: 21000, train/loss: 0.053700000047683716
Step: 21000, train/grad_norm: 0.036400631070137024
Step: 21000, train/learning_rate: 2.5011899197124876e-05
Step: 21000, train/epoch: 4.997620105743408
Step: 21010, train/loss: 0.2980000078678131
Step: 21010, train/grad_norm: 122.04052734375
Step: 21010, train/learning_rate: 2.499999936844688e-05
Step: 21010, train/epoch: 5.0
Step: 21010, eval/loss: 0.25981298089027405
Step: 21010, eval/accuracy: 0.946133553981781
Step: 21010, eval/f1: 0.9422361254692078
Step: 21010, eval/runtime: 296.20001220703125
Step: 21010, eval/samples_per_second: 24.31800079345703
Step: 21010, eval/steps_per_second: 3.0420000553131104
Step: 21010, train/epoch: 5.0
Step: 21020, train/loss: 0.003800000064074993
Step: 21020, train/grad_norm: 0.040709540247917175
Step: 21020, train/learning_rate: 2.4988101358758286e-05
Step: 21020, train/epoch: 5.002379894256592
Step: 21030, train/loss: 0.018799999728798866
Step: 21030, train/grad_norm: 0.0007974132895469666
Step: 21030, train/learning_rate: 2.497620153008029e-05
Step: 21030, train/epoch: 5.004759788513184
Step: 21040, train/loss: 0.048500001430511475
Step: 21040, train/grad_norm: 0.03513157740235329
Step: 21040, train/learning_rate: 2.4964303520391695e-05
Step: 21040, train/epoch: 5.007139682769775
Step: 21050, train/loss: 0.10109999775886536
Step: 21050, train/grad_norm: 49.17777633666992
Step: 21050, train/learning_rate: 2.4952403691713698e-05
Step: 21050, train/epoch: 5.009519100189209
Step: 21060, train/loss: 0.07890000194311142
Step: 21060, train/grad_norm: 1.326661229133606
Step: 21060, train/learning_rate: 2.49405038630357e-05
Step: 21060, train/epoch: 5.011898994445801
Step: 21070, train/loss: 0.10000000149011612
Step: 21070, train/grad_norm: 0.0069681922905147076
Step: 21070, train/learning_rate: 2.4928605853347108e-05
Step: 21070, train/epoch: 5.014278888702393
Step: 21080, train/loss: 0.0015999999595806003
Step: 21080, train/grad_norm: 1.3259872198104858
Step: 21080, train/learning_rate: 2.491670602466911e-05
Step: 21080, train/epoch: 5.016658782958984
Step: 21090, train/loss: 0.0012000000569969416
Step: 21090, train/grad_norm: 0.0011841004015877843
Step: 21090, train/learning_rate: 2.4904808014980517e-05
Step: 21090, train/epoch: 5.019038677215576
Step: 21100, train/loss: 0.017799999564886093
Step: 21100, train/grad_norm: 0.0034370701760053635
Step: 21100, train/learning_rate: 2.489290818630252e-05
Step: 21100, train/epoch: 5.021418571472168
Step: 21110, train/loss: 0.06210000067949295
Step: 21110, train/grad_norm: 0.01418215874582529
Step: 21110, train/learning_rate: 2.4881008357624523e-05
Step: 21110, train/epoch: 5.023797988891602
Step: 21120, train/loss: 0.000699999975040555
Step: 21120, train/grad_norm: 0.0028884413186460733
Step: 21120, train/learning_rate: 2.486911034793593e-05
Step: 21120, train/epoch: 5.026177883148193
Step: 21130, train/loss: 0.014499999582767487
Step: 21130, train/grad_norm: 0.5361965298652649
Step: 21130, train/learning_rate: 2.4857210519257933e-05
Step: 21130, train/epoch: 5.028557777404785
Step: 21140, train/loss: 9.999999747378752e-05
Step: 21140, train/grad_norm: 0.011126935482025146
Step: 21140, train/learning_rate: 2.484531250956934e-05
Step: 21140, train/epoch: 5.030937671661377
Step: 21150, train/loss: 0.0005000000237487257
Step: 21150, train/grad_norm: 0.030769625678658485
Step: 21150, train/learning_rate: 2.4833412680891342e-05
Step: 21150, train/epoch: 5.033317565917969
Step: 21160, train/loss: 0.13349999487400055
Step: 21160, train/grad_norm: 0.008431842550635338
Step: 21160, train/learning_rate: 2.4821512852213345e-05
Step: 21160, train/epoch: 5.0356974601745605
Step: 21170, train/loss: 0.03530000150203705
Step: 21170, train/grad_norm: 0.04920223355293274
Step: 21170, train/learning_rate: 2.4809614842524752e-05
Step: 21170, train/epoch: 5.038076877593994
Step: 21180, train/loss: 0.07320000231266022
Step: 21180, train/grad_norm: 25.611114501953125
Step: 21180, train/learning_rate: 2.4797715013846755e-05
Step: 21180, train/epoch: 5.040456771850586
Step: 21190, train/loss: 0.02319999970495701
Step: 21190, train/grad_norm: 21.303821563720703
Step: 21190, train/learning_rate: 2.478581700415816e-05
Step: 21190, train/epoch: 5.042836666107178
Step: 21200, train/loss: 0.061799999326467514
Step: 21200, train/grad_norm: 0.002507922938093543
Step: 21200, train/learning_rate: 2.4773917175480165e-05
Step: 21200, train/epoch: 5.0452165603637695
Step: 21210, train/loss: 0.06419999897480011
Step: 21210, train/grad_norm: 0.0006466949125751853
Step: 21210, train/learning_rate: 2.4762017346802168e-05
Step: 21210, train/epoch: 5.047596454620361
Step: 21220, train/loss: 0.01080000028014183
Step: 21220, train/grad_norm: 0.0017196517437696457
Step: 21220, train/learning_rate: 2.4750119337113574e-05
Step: 21220, train/epoch: 5.049976348876953
Step: 21230, train/loss: 0.007699999958276749
Step: 21230, train/grad_norm: 0.006424461025744677
Step: 21230, train/learning_rate: 2.4738219508435577e-05
Step: 21230, train/epoch: 5.052356243133545
Step: 21240, train/loss: 0.021400000900030136
Step: 21240, train/grad_norm: 0.018387049436569214
Step: 21240, train/learning_rate: 2.4726321498746984e-05
Step: 21240, train/epoch: 5.0547356605529785
Step: 21250, train/loss: 0.01640000008046627
Step: 21250, train/grad_norm: 0.008615349419414997
Step: 21250, train/learning_rate: 2.4714421670068987e-05
Step: 21250, train/epoch: 5.05711555480957
Step: 21260, train/loss: 0.0003000000142492354
Step: 21260, train/grad_norm: 0.05418749898672104
Step: 21260, train/learning_rate: 2.470252184139099e-05
Step: 21260, train/epoch: 5.059495449066162
Step: 21270, train/loss: 0.06310000270605087
Step: 21270, train/grad_norm: 0.0006964250351302326
Step: 21270, train/learning_rate: 2.4690623831702396e-05
Step: 21270, train/epoch: 5.061875343322754
Step: 21280, train/loss: 0.05209999904036522
Step: 21280, train/grad_norm: 0.009913783520460129
Step: 21280, train/learning_rate: 2.46787240030244e-05
Step: 21280, train/epoch: 5.064255237579346
Step: 21290, train/loss: 0.1875
Step: 21290, train/grad_norm: 14.569032669067383
Step: 21290, train/learning_rate: 2.4666825993335806e-05
Step: 21290, train/epoch: 5.0666351318359375
Step: 21300, train/loss: 0.09040000289678574
Step: 21300, train/grad_norm: 0.0012292657047510147
Step: 21300, train/learning_rate: 2.465492616465781e-05
Step: 21300, train/epoch: 5.069014549255371
Step: 21310, train/loss: 0.003800000064074993
Step: 21310, train/grad_norm: 8.763144493103027
Step: 21310, train/learning_rate: 2.4643026335979812e-05
Step: 21310, train/epoch: 5.071394443511963
Step: 21320, train/loss: 0.06459999829530716
Step: 21320, train/grad_norm: 0.06490416079759598
Step: 21320, train/learning_rate: 2.463112832629122e-05
Step: 21320, train/epoch: 5.073774337768555
Step: 21330, train/loss: 0.00930000003427267
Step: 21330, train/grad_norm: 1.276346206665039
Step: 21330, train/learning_rate: 2.461922849761322e-05
Step: 21330, train/epoch: 5.0761542320251465
Step: 21340, train/loss: 0.0934000015258789
Step: 21340, train/grad_norm: 0.00023478901130147278
Step: 21340, train/learning_rate: 2.4607330487924628e-05
Step: 21340, train/epoch: 5.078534126281738
Step: 21350, train/loss: 0.05829999968409538
Step: 21350, train/grad_norm: 0.02490193210542202
Step: 21350, train/learning_rate: 2.459543065924663e-05
Step: 21350, train/epoch: 5.08091402053833
Step: 21360, train/loss: 0.008899999782443047
Step: 21360, train/grad_norm: 0.001439527841284871
Step: 21360, train/learning_rate: 2.4583530830568634e-05
Step: 21360, train/epoch: 5.083293437957764
Step: 21370, train/loss: 0.015799999237060547
Step: 21370, train/grad_norm: 0.19549746811389923
Step: 21370, train/learning_rate: 2.457163282088004e-05
Step: 21370, train/epoch: 5.0856733322143555
Step: 21380, train/loss: 0.000699999975040555
Step: 21380, train/grad_norm: 0.0007316233240999281
Step: 21380, train/learning_rate: 2.4559732992202044e-05
Step: 21380, train/epoch: 5.088053226470947
Step: 21390, train/loss: 0.052799999713897705
Step: 21390, train/grad_norm: 0.0011471625184640288
Step: 21390, train/learning_rate: 2.454783498251345e-05
Step: 21390, train/epoch: 5.090433120727539
Step: 21400, train/loss: 0.041099999099969864
Step: 21400, train/grad_norm: 0.008923638612031937
Step: 21400, train/learning_rate: 2.4535935153835453e-05
Step: 21400, train/epoch: 5.092813014984131
Step: 21410, train/loss: 0.004699999932199717
Step: 21410, train/grad_norm: 0.45014244318008423
Step: 21410, train/learning_rate: 2.4524035325157456e-05
Step: 21410, train/epoch: 5.095192909240723
Step: 21420, train/loss: 9.999999747378752e-05
Step: 21420, train/grad_norm: 0.0003345355798956007
Step: 21420, train/learning_rate: 2.4512137315468863e-05
Step: 21420, train/epoch: 5.0975728034973145
Step: 21430, train/loss: 0.11980000138282776
Step: 21430, train/grad_norm: 16.456748962402344
Step: 21430, train/learning_rate: 2.4500237486790866e-05
Step: 21430, train/epoch: 5.099952220916748
Step: 21440, train/loss: 0.0020000000949949026
Step: 21440, train/grad_norm: 0.0046836007386446
Step: 21440, train/learning_rate: 2.4488339477102272e-05
Step: 21440, train/epoch: 5.10233211517334
Step: 21450, train/loss: 0.028300000354647636
Step: 21450, train/grad_norm: 0.0076233819127082825
Step: 21450, train/learning_rate: 2.4476439648424275e-05
Step: 21450, train/epoch: 5.104712009429932
Step: 21460, train/loss: 0.04259999841451645
Step: 21460, train/grad_norm: 36.76039505004883
Step: 21460, train/learning_rate: 2.4464539819746278e-05
Step: 21460, train/epoch: 5.107091903686523
Step: 21470, train/loss: 9.999999747378752e-05
Step: 21470, train/grad_norm: 0.0005752262659370899
Step: 21470, train/learning_rate: 2.4452641810057685e-05
Step: 21470, train/epoch: 5.109471797943115
Step: 21480, train/loss: 0.04259999841451645
Step: 21480, train/grad_norm: 0.020620672032237053
Step: 21480, train/learning_rate: 2.4440741981379688e-05
Step: 21480, train/epoch: 5.111851692199707
Step: 21490, train/loss: 9.999999747378752e-05
Step: 21490, train/grad_norm: 0.00015573496057186276
Step: 21490, train/learning_rate: 2.4428843971691094e-05
Step: 21490, train/epoch: 5.114231109619141
Step: 21500, train/loss: 0.08609999716281891
Step: 21500, train/grad_norm: 0.00261922855861485
Step: 21500, train/learning_rate: 2.4416944143013097e-05
Step: 21500, train/epoch: 5.116611003875732
Step: 21510, train/loss: 9.999999747378752e-05
Step: 21510, train/grad_norm: 0.025192193686962128
Step: 21510, train/learning_rate: 2.44050443143351e-05
Step: 21510, train/epoch: 5.118990898132324
Step: 21520, train/loss: 0.0012000000569969416
Step: 21520, train/grad_norm: 0.0019433419220149517
Step: 21520, train/learning_rate: 2.4393146304646507e-05
Step: 21520, train/epoch: 5.121370792388916
Step: 21530, train/loss: 0.04520000144839287
Step: 21530, train/grad_norm: 0.00042297274922020733
Step: 21530, train/learning_rate: 2.438124647596851e-05
Step: 21530, train/epoch: 5.123750686645508
Step: 21540, train/loss: 0.0013000000035390258
Step: 21540, train/grad_norm: 0.0016663409769535065
Step: 21540, train/learning_rate: 2.4369348466279916e-05
Step: 21540, train/epoch: 5.1261305809021
Step: 21550, train/loss: 0.004000000189989805
Step: 21550, train/grad_norm: 0.10134796053171158
Step: 21550, train/learning_rate: 2.435744863760192e-05
Step: 21550, train/epoch: 5.128509998321533
Step: 21560, train/loss: 0.07909999787807465
Step: 21560, train/grad_norm: 8.529009819030762
Step: 21560, train/learning_rate: 2.4345550627913326e-05
Step: 21560, train/epoch: 5.130889892578125
Step: 21570, train/loss: 0.00570000009611249
Step: 21570, train/grad_norm: 11.011672019958496
Step: 21570, train/learning_rate: 2.433365079923533e-05
Step: 21570, train/epoch: 5.133269786834717
Step: 21580, train/loss: 0.0071000000461936
Step: 21580, train/grad_norm: 0.07798698544502258
Step: 21580, train/learning_rate: 2.4321750970557332e-05
Step: 21580, train/epoch: 5.135649681091309
Step: 21590, train/loss: 0.3328000009059906
Step: 21590, train/grad_norm: 0.00119215517770499
Step: 21590, train/learning_rate: 2.430985296086874e-05
Step: 21590, train/epoch: 5.1380295753479
Step: 21600, train/loss: 0.04149999842047691
Step: 21600, train/grad_norm: 51.26189422607422
Step: 21600, train/learning_rate: 2.429795313219074e-05
Step: 21600, train/epoch: 5.140409469604492
Step: 21610, train/loss: 0.1882999986410141
Step: 21610, train/grad_norm: 51.027767181396484
Step: 21610, train/learning_rate: 2.4286055122502148e-05
Step: 21610, train/epoch: 5.142789363861084
Step: 21620, train/loss: 0.02419999986886978
Step: 21620, train/grad_norm: 0.007283671759068966
Step: 21620, train/learning_rate: 2.427415529382415e-05
Step: 21620, train/epoch: 5.145168781280518
Step: 21630, train/loss: 0.14300000667572021
Step: 21630, train/grad_norm: 0.0025359380524605513
Step: 21630, train/learning_rate: 2.4262255465146154e-05
Step: 21630, train/epoch: 5.147548675537109
Step: 21640, train/loss: 0.006000000052154064
Step: 21640, train/grad_norm: 0.002630309434607625
Step: 21640, train/learning_rate: 2.425035745545756e-05
Step: 21640, train/epoch: 5.149928569793701
Step: 21650, train/loss: 0.013399999588727951
Step: 21650, train/grad_norm: 0.011448915116488934
Step: 21650, train/learning_rate: 2.4238457626779564e-05
Step: 21650, train/epoch: 5.152308464050293
Step: 21660, train/loss: 0.11890000104904175
Step: 21660, train/grad_norm: 0.005642938893288374
Step: 21660, train/learning_rate: 2.422655961709097e-05
Step: 21660, train/epoch: 5.154688358306885
Step: 21670, train/loss: 0.00019999999494757503
Step: 21670, train/grad_norm: 0.009495890699326992
Step: 21670, train/learning_rate: 2.4214659788412973e-05
Step: 21670, train/epoch: 5.157068252563477
Step: 21680, train/loss: 0.09009999781847
Step: 21680, train/grad_norm: 0.0006946289213374257
Step: 21680, train/learning_rate: 2.4202759959734976e-05
Step: 21680, train/epoch: 5.15944766998291
Step: 21690, train/loss: 0.006800000090152025
Step: 21690, train/grad_norm: 0.01808580942451954
Step: 21690, train/learning_rate: 2.4190861950046383e-05
Step: 21690, train/epoch: 5.161827564239502
Step: 21700, train/loss: 0.19769999384880066
Step: 21700, train/grad_norm: 0.001144339214079082
Step: 21700, train/learning_rate: 2.4178962121368386e-05
Step: 21700, train/epoch: 5.164207458496094
Step: 21710, train/loss: 0.09910000115633011
Step: 21710, train/grad_norm: 48.84274673461914
Step: 21710, train/learning_rate: 2.4167064111679792e-05
Step: 21710, train/epoch: 5.1665873527526855
Step: 21720, train/loss: 0.03669999912381172
Step: 21720, train/grad_norm: 0.008728746324777603
Step: 21720, train/learning_rate: 2.4155164283001795e-05
Step: 21720, train/epoch: 5.168967247009277
Step: 21730, train/loss: 0.0019000000320374966
Step: 21730, train/grad_norm: 0.0008079479448497295
Step: 21730, train/learning_rate: 2.41432644543238e-05
Step: 21730, train/epoch: 5.171347141265869
Step: 21740, train/loss: 0.06530000269412994
Step: 21740, train/grad_norm: 0.0019717456307262182
Step: 21740, train/learning_rate: 2.4131366444635205e-05
Step: 21740, train/epoch: 5.173726558685303
Step: 21750, train/loss: 0.058800000697374344
Step: 21750, train/grad_norm: 75.04813385009766
Step: 21750, train/learning_rate: 2.4119466615957208e-05
Step: 21750, train/epoch: 5.1761064529418945
Step: 21760, train/loss: 0.08820000290870667
Step: 21760, train/grad_norm: 0.01816970854997635
Step: 21760, train/learning_rate: 2.4107568606268615e-05
Step: 21760, train/epoch: 5.178486347198486
Step: 21770, train/loss: 0.0
Step: 21770, train/grad_norm: 0.0005659882444888353
Step: 21770, train/learning_rate: 2.4095668777590618e-05
Step: 21770, train/epoch: 5.180866241455078
Step: 21780, train/loss: 0.01889999955892563
Step: 21780, train/grad_norm: 33.812713623046875
Step: 21780, train/learning_rate: 2.408376894891262e-05
Step: 21780, train/epoch: 5.18324613571167
Step: 21790, train/loss: 0.002099999925121665
Step: 21790, train/grad_norm: 0.09264165908098221
Step: 21790, train/learning_rate: 2.4071870939224027e-05
Step: 21790, train/epoch: 5.185626029968262
Step: 21800, train/loss: 9.999999747378752e-05
Step: 21800, train/grad_norm: 0.0030389472376555204
Step: 21800, train/learning_rate: 2.405997111054603e-05
Step: 21800, train/epoch: 5.1880059242248535
Step: 21810, train/loss: 0.0003000000142492354
Step: 21810, train/grad_norm: 0.021834101527929306
Step: 21810, train/learning_rate: 2.4048073100857437e-05
Step: 21810, train/epoch: 5.190385341644287
Step: 21820, train/loss: 0.011099999770522118
Step: 21820, train/grad_norm: 8.111681938171387
Step: 21820, train/learning_rate: 2.403617327217944e-05
Step: 21820, train/epoch: 5.192765235900879
Step: 21830, train/loss: 0.029100000858306885
Step: 21830, train/grad_norm: 79.96339416503906
Step: 21830, train/learning_rate: 2.4024273443501443e-05
Step: 21830, train/epoch: 5.195145130157471
Step: 21840, train/loss: 0.05900000035762787
Step: 21840, train/grad_norm: 0.0053913104347884655
Step: 21840, train/learning_rate: 2.401237543381285e-05
Step: 21840, train/epoch: 5.1975250244140625
Step: 21850, train/loss: 0.04919999837875366
Step: 21850, train/grad_norm: 0.0009966522920876741
Step: 21850, train/learning_rate: 2.4000475605134852e-05
Step: 21850, train/epoch: 5.199904918670654
Step: 21860, train/loss: 0.005100000184029341
Step: 21860, train/grad_norm: 3.5246357917785645
Step: 21860, train/learning_rate: 2.398857759544626e-05
Step: 21860, train/epoch: 5.202284812927246
Step: 21870, train/loss: 0.10050000250339508
Step: 21870, train/grad_norm: 0.06144733354449272
Step: 21870, train/learning_rate: 2.3976677766768262e-05
Step: 21870, train/epoch: 5.20466423034668
Step: 21880, train/loss: 0.07509999722242355
Step: 21880, train/grad_norm: 0.00046927144285291433
Step: 21880, train/learning_rate: 2.3964777938090265e-05
Step: 21880, train/epoch: 5.2070441246032715
Step: 21890, train/loss: 0.06390000134706497
Step: 21890, train/grad_norm: 0.5552918314933777
Step: 21890, train/learning_rate: 2.395287992840167e-05
Step: 21890, train/epoch: 5.209424018859863
Step: 21900, train/loss: 0.24289999902248383
Step: 21900, train/grad_norm: 56.54603958129883
Step: 21900, train/learning_rate: 2.3940980099723674e-05
Step: 21900, train/epoch: 5.211803913116455
Step: 21910, train/loss: 0.002300000051036477
Step: 21910, train/grad_norm: 0.0017307413509115577
Step: 21910, train/learning_rate: 2.392908209003508e-05
Step: 21910, train/epoch: 5.214183807373047
Step: 21920, train/loss: 0.037300001829862595
Step: 21920, train/grad_norm: 3.39709210395813
Step: 21920, train/learning_rate: 2.3917182261357084e-05
Step: 21920, train/epoch: 5.216563701629639
Step: 21930, train/loss: 0.17910000681877136
Step: 21930, train/grad_norm: 25.369531631469727
Step: 21930, train/learning_rate: 2.3905282432679087e-05
Step: 21930, train/epoch: 5.2189435958862305
Step: 21940, train/loss: 9.999999747378752e-05
Step: 21940, train/grad_norm: 0.029581354930996895
Step: 21940, train/learning_rate: 2.3893384422990493e-05
Step: 21940, train/epoch: 5.221323013305664
Step: 21950, train/loss: 0.0010999999940395355
Step: 21950, train/grad_norm: 0.0017115370137616992
Step: 21950, train/learning_rate: 2.3881484594312496e-05
Step: 21950, train/epoch: 5.223702907562256
Step: 21960, train/loss: 0.012400000356137753
Step: 21960, train/grad_norm: 0.001911957748234272
Step: 21960, train/learning_rate: 2.3869586584623903e-05
Step: 21960, train/epoch: 5.226082801818848
Step: 21970, train/loss: 0.11410000175237656
Step: 21970, train/grad_norm: 0.0674709603190422
Step: 21970, train/learning_rate: 2.3857686755945906e-05
Step: 21970, train/epoch: 5.2284626960754395
Step: 21980, train/loss: 0.057100001722574234
Step: 21980, train/grad_norm: 2.1156938076019287
Step: 21980, train/learning_rate: 2.384578692726791e-05
Step: 21980, train/epoch: 5.230842590332031
Step: 21990, train/loss: 0.0007999999797903001
Step: 21990, train/grad_norm: 0.07001753151416779
Step: 21990, train/learning_rate: 2.3833888917579316e-05
Step: 21990, train/epoch: 5.233222484588623
Step: 22000, train/loss: 0.017000000923871994
Step: 22000, train/grad_norm: 2.964148759841919
Step: 22000, train/learning_rate: 2.382198908890132e-05
Step: 22000, train/epoch: 5.235601902008057
Step: 22010, train/loss: 0.007600000128149986
Step: 22010, train/grad_norm: 0.012869395315647125
Step: 22010, train/learning_rate: 2.3810091079212725e-05
Step: 22010, train/epoch: 5.237981796264648
Step: 22020, train/loss: 0.015799999237060547
Step: 22020, train/grad_norm: 8.185324668884277
Step: 22020, train/learning_rate: 2.3798191250534728e-05
Step: 22020, train/epoch: 5.24036169052124
Step: 22030, train/loss: 0.021700000390410423
Step: 22030, train/grad_norm: 0.0023545122239738703
Step: 22030, train/learning_rate: 2.378629142185673e-05
Step: 22030, train/epoch: 5.242741584777832
Step: 22040, train/loss: 0.09430000185966492
Step: 22040, train/grad_norm: 2.316525459289551
Step: 22040, train/learning_rate: 2.3774393412168138e-05
Step: 22040, train/epoch: 5.245121479034424
Step: 22050, train/loss: 0.29100000858306885
Step: 22050, train/grad_norm: 22.907184600830078
Step: 22050, train/learning_rate: 2.376249358349014e-05
Step: 22050, train/epoch: 5.247501373291016
Step: 22060, train/loss: 0.000699999975040555
Step: 22060, train/grad_norm: 0.008391027338802814
Step: 22060, train/learning_rate: 2.3750595573801547e-05
Step: 22060, train/epoch: 5.249880790710449
Step: 22070, train/loss: 0.0024999999441206455
Step: 22070, train/grad_norm: 0.00010025811934610829
Step: 22070, train/learning_rate: 2.373869574512355e-05
Step: 22070, train/epoch: 5.252260684967041
Step: 22080, train/loss: 0.04410000145435333
Step: 22080, train/grad_norm: 0.023162465542554855
Step: 22080, train/learning_rate: 2.3726795916445553e-05
Step: 22080, train/epoch: 5.254640579223633
Step: 22090, train/loss: 0.07980000227689743
Step: 22090, train/grad_norm: 0.0006300705135799944
Step: 22090, train/learning_rate: 2.371489790675696e-05
Step: 22090, train/epoch: 5.257020473480225
Step: 22100, train/loss: 0.0006000000284984708
Step: 22100, train/grad_norm: 0.0026158075779676437
Step: 22100, train/learning_rate: 2.3702998078078963e-05
Step: 22100, train/epoch: 5.259400367736816
Step: 22110, train/loss: 0.017100000753998756
Step: 22110, train/grad_norm: 0.0008693949203006923
Step: 22110, train/learning_rate: 2.369110006839037e-05
Step: 22110, train/epoch: 5.261780261993408
Step: 22120, train/loss: 0.0005000000237487257
Step: 22120, train/grad_norm: 0.000326355395372957
Step: 22120, train/learning_rate: 2.3679200239712372e-05
Step: 22120, train/epoch: 5.26416015625
Step: 22130, train/loss: 0.009600000455975533
Step: 22130, train/grad_norm: 0.0009823585860431194
Step: 22130, train/learning_rate: 2.3667300411034375e-05
Step: 22130, train/epoch: 5.266539573669434
Step: 22140, train/loss: 0.00419999985024333
Step: 22140, train/grad_norm: 0.003139953827485442
Step: 22140, train/learning_rate: 2.3655402401345782e-05
Step: 22140, train/epoch: 5.268919467926025
Step: 22150, train/loss: 0.0754999965429306
Step: 22150, train/grad_norm: 0.024143517017364502
Step: 22150, train/learning_rate: 2.3643502572667785e-05
Step: 22150, train/epoch: 5.271299362182617
Step: 22160, train/loss: 0.015799999237060547
Step: 22160, train/grad_norm: 0.00461703771725297
Step: 22160, train/learning_rate: 2.363160456297919e-05
Step: 22160, train/epoch: 5.273679256439209
Step: 22170, train/loss: 0.0007999999797903001
Step: 22170, train/grad_norm: 0.009441506117582321
Step: 22170, train/learning_rate: 2.3619704734301195e-05
Step: 22170, train/epoch: 5.276059150695801
Step: 22180, train/loss: 0.025699999183416367
Step: 22180, train/grad_norm: 0.003922825679183006
Step: 22180, train/learning_rate: 2.3607804905623198e-05
Step: 22180, train/epoch: 5.278439044952393
Step: 22190, train/loss: 0.19480000436306
Step: 22190, train/grad_norm: 0.0005792858428321779
Step: 22190, train/learning_rate: 2.3595906895934604e-05
Step: 22190, train/epoch: 5.280818462371826
Step: 22200, train/loss: 0.1062999963760376
Step: 22200, train/grad_norm: 9.172196388244629
Step: 22200, train/learning_rate: 2.3584007067256607e-05
Step: 22200, train/epoch: 5.283198356628418
Step: 22210, train/loss: 0.00570000009611249
Step: 22210, train/grad_norm: 0.030722521245479584
Step: 22210, train/learning_rate: 2.3572109057568014e-05
Step: 22210, train/epoch: 5.28557825088501
Step: 22220, train/loss: 0.000699999975040555
Step: 22220, train/grad_norm: 0.0031152155715972185
Step: 22220, train/learning_rate: 2.3560209228890017e-05
Step: 22220, train/epoch: 5.287958145141602
Step: 22230, train/loss: 0.250900000333786
Step: 22230, train/grad_norm: 0.013895770534873009
Step: 22230, train/learning_rate: 2.3548311219201423e-05
Step: 22230, train/epoch: 5.290338039398193
Step: 22240, train/loss: 0.0008999999845400453
Step: 22240, train/grad_norm: 0.8677654266357422
Step: 22240, train/learning_rate: 2.3536411390523426e-05
Step: 22240, train/epoch: 5.292717933654785
Step: 22250, train/loss: 0.12359999865293503
Step: 22250, train/grad_norm: 0.017400838434696198
Step: 22250, train/learning_rate: 2.352451156184543e-05
Step: 22250, train/epoch: 5.295097351074219
Step: 22260, train/loss: 0.03779999911785126
Step: 22260, train/grad_norm: 1.2790875434875488
Step: 22260, train/learning_rate: 2.3512613552156836e-05
Step: 22260, train/epoch: 5.2974772453308105
Step: 22270, train/loss: 0.006399999838322401
Step: 22270, train/grad_norm: 7.477369785308838
Step: 22270, train/learning_rate: 2.350071372347884e-05
Step: 22270, train/epoch: 5.299857139587402
Step: 22280, train/loss: 0.00019999999494757503
Step: 22280, train/grad_norm: 0.0015460021095350385
Step: 22280, train/learning_rate: 2.3488815713790245e-05
Step: 22280, train/epoch: 5.302237033843994
Step: 22290, train/loss: 0.046300001442432404
Step: 22290, train/grad_norm: 0.001637043198570609
Step: 22290, train/learning_rate: 2.347691588511225e-05
Step: 22290, train/epoch: 5.304616928100586
Step: 22300, train/loss: 0.003800000064074993
Step: 22300, train/grad_norm: 0.04826914519071579
Step: 22300, train/learning_rate: 2.346501605643425e-05
Step: 22300, train/epoch: 5.306996822357178
Step: 22310, train/loss: 0.02979999966919422
Step: 22310, train/grad_norm: 0.00678155617788434
Step: 22310, train/learning_rate: 2.3453118046745658e-05
Step: 22310, train/epoch: 5.3093767166137695
Step: 22320, train/loss: 0.07289999723434448
Step: 22320, train/grad_norm: 0.0029453993774950504
Step: 22320, train/learning_rate: 2.344121821806766e-05
Step: 22320, train/epoch: 5.311756134033203
Step: 22330, train/loss: 0.0024999999441206455
Step: 22330, train/grad_norm: 0.05600907281041145
Step: 22330, train/learning_rate: 2.3429320208379067e-05
Step: 22330, train/epoch: 5.314136028289795
Step: 22340, train/loss: 0.10010000318288803
Step: 22340, train/grad_norm: 0.004072357434779406
Step: 22340, train/learning_rate: 2.341742037970107e-05
Step: 22340, train/epoch: 5.316515922546387
Step: 22350, train/loss: 0.06199999898672104
Step: 22350, train/grad_norm: 35.655696868896484
Step: 22350, train/learning_rate: 2.3405520551023073e-05
Step: 22350, train/epoch: 5.3188958168029785
Step: 22360, train/loss: 0.0012000000569969416
Step: 22360, train/grad_norm: 0.0022668142337352037
Step: 22360, train/learning_rate: 2.339362254133448e-05
Step: 22360, train/epoch: 5.32127571105957
Step: 22370, train/loss: 0.00019999999494757503
Step: 22370, train/grad_norm: 0.003366416087374091
Step: 22370, train/learning_rate: 2.3381722712656483e-05
Step: 22370, train/epoch: 5.323655605316162
Step: 22380, train/loss: 0.002199999988079071
Step: 22380, train/grad_norm: 1.1703089475631714
Step: 22380, train/learning_rate: 2.336982470296789e-05
Step: 22380, train/epoch: 5.326035022735596
Step: 22390, train/loss: 0.06069999933242798
Step: 22390, train/grad_norm: 0.002325242618098855
Step: 22390, train/learning_rate: 2.3357924874289893e-05
Step: 22390, train/epoch: 5.3284149169921875
Step: 22400, train/loss: 0.08919999748468399
Step: 22400, train/grad_norm: 0.02517511509358883
Step: 22400, train/learning_rate: 2.3346025045611896e-05
Step: 22400, train/epoch: 5.330794811248779
Step: 22410, train/loss: 0.024900000542402267
Step: 22410, train/grad_norm: 1.3255659341812134
Step: 22410, train/learning_rate: 2.3334127035923302e-05
Step: 22410, train/epoch: 5.333174705505371
Step: 22420, train/loss: 0.04270000010728836
Step: 22420, train/grad_norm: 0.0059823766350746155
Step: 22420, train/learning_rate: 2.3322227207245305e-05
Step: 22420, train/epoch: 5.335554599761963
Step: 22430, train/loss: 0.0010000000474974513
Step: 22430, train/grad_norm: 0.009229781106114388
Step: 22430, train/learning_rate: 2.3310329197556712e-05
Step: 22430, train/epoch: 5.337934494018555
Step: 22440, train/loss: 0.005900000222027302
Step: 22440, train/grad_norm: 0.2436375468969345
Step: 22440, train/learning_rate: 2.3298429368878715e-05
Step: 22440, train/epoch: 5.340313911437988
Step: 22450, train/loss: 0.06790000200271606
Step: 22450, train/grad_norm: 0.09627339988946915
Step: 22450, train/learning_rate: 2.3286529540200718e-05
Step: 22450, train/epoch: 5.34269380569458
Step: 22460, train/loss: 0.0015999999595806003
Step: 22460, train/grad_norm: 0.010911140590906143
Step: 22460, train/learning_rate: 2.3274631530512124e-05
Step: 22460, train/epoch: 5.345073699951172
Step: 22470, train/loss: 0.004399999976158142
Step: 22470, train/grad_norm: 0.20599709451198578
Step: 22470, train/learning_rate: 2.3262731701834127e-05
Step: 22470, train/epoch: 5.347453594207764
Step: 22480, train/loss: 0.006800000090152025
Step: 22480, train/grad_norm: 0.002271025674417615
Step: 22480, train/learning_rate: 2.3250833692145534e-05
Step: 22480, train/epoch: 5.3498334884643555
Step: 22490, train/loss: 0.05380000174045563
Step: 22490, train/grad_norm: 0.013911675661802292
Step: 22490, train/learning_rate: 2.3238933863467537e-05
Step: 22490, train/epoch: 5.352213382720947
Step: 22500, train/loss: 0.29989999532699585
Step: 22500, train/grad_norm: 0.14001032710075378
Step: 22500, train/learning_rate: 2.322703403478954e-05
Step: 22500, train/epoch: 5.354593276977539
Step: 22510, train/loss: 0.0803999975323677
Step: 22510, train/grad_norm: 0.03417471423745155
Step: 22510, train/learning_rate: 2.3215136025100946e-05
Step: 22510, train/epoch: 5.356972694396973
Step: 22520, train/loss: 0.058400001376867294
Step: 22520, train/grad_norm: 57.85032653808594
Step: 22520, train/learning_rate: 2.320323619642295e-05
Step: 22520, train/epoch: 5.3593525886535645
Step: 22530, train/loss: 0.009399999864399433
Step: 22530, train/grad_norm: 12.727876663208008
Step: 22530, train/learning_rate: 2.3191338186734356e-05
Step: 22530, train/epoch: 5.361732482910156
Step: 22540, train/loss: 0.010200000368058681
Step: 22540, train/grad_norm: 0.019907906651496887
Step: 22540, train/learning_rate: 2.317943835805636e-05
Step: 22540, train/epoch: 5.364112377166748
Step: 22550, train/loss: 0.0010000000474974513
Step: 22550, train/grad_norm: 0.0006209357525222003
Step: 22550, train/learning_rate: 2.3167538529378362e-05
Step: 22550, train/epoch: 5.36649227142334
Step: 22560, train/loss: 0.11479999870061874
Step: 22560, train/grad_norm: 18.15907859802246
Step: 22560, train/learning_rate: 2.315564051968977e-05
Step: 22560, train/epoch: 5.368872165679932
Step: 22570, train/loss: 0.005900000222027302
Step: 22570, train/grad_norm: 0.027609998360276222
Step: 22570, train/learning_rate: 2.314374069101177e-05
Step: 22570, train/epoch: 5.371251583099365
Step: 22580, train/loss: 0.004699999932199717
Step: 22580, train/grad_norm: 0.08097558468580246
Step: 22580, train/learning_rate: 2.3131842681323178e-05
Step: 22580, train/epoch: 5.373631477355957
Step: 22590, train/loss: 0.10400000214576721
Step: 22590, train/grad_norm: 3.724440574645996
Step: 22590, train/learning_rate: 2.311994285264518e-05
Step: 22590, train/epoch: 5.376011371612549
Step: 22600, train/loss: 0.12790000438690186
Step: 22600, train/grad_norm: 13.485677719116211
Step: 22600, train/learning_rate: 2.3108043023967184e-05
Step: 22600, train/epoch: 5.378391265869141
Step: 22610, train/loss: 0.17239999771118164
Step: 22610, train/grad_norm: 6.426733016967773
Step: 22610, train/learning_rate: 2.309614501427859e-05
Step: 22610, train/epoch: 5.380771160125732
Step: 22620, train/loss: 0.050999999046325684
Step: 22620, train/grad_norm: 0.0007897741161286831
Step: 22620, train/learning_rate: 2.3084245185600594e-05
Step: 22620, train/epoch: 5.383151054382324
Step: 22630, train/loss: 0.04610000178217888
Step: 22630, train/grad_norm: 0.0016810233937576413
Step: 22630, train/learning_rate: 2.3072347175912e-05
Step: 22630, train/epoch: 5.385530471801758
Step: 22640, train/loss: 0.04969999939203262
Step: 22640, train/grad_norm: 36.396419525146484
Step: 22640, train/learning_rate: 2.3060447347234003e-05
Step: 22640, train/epoch: 5.38791036605835
Step: 22650, train/loss: 0.05299999937415123
Step: 22650, train/grad_norm: 0.0017848844872787595
Step: 22650, train/learning_rate: 2.3048547518556006e-05
Step: 22650, train/epoch: 5.390290260314941
Step: 22660, train/loss: 0.00019999999494757503
Step: 22660, train/grad_norm: 8.71384545462206e-05
Step: 22660, train/learning_rate: 2.3036649508867413e-05
Step: 22660, train/epoch: 5.392670154571533
Step: 22670, train/loss: 0.0008999999845400453
Step: 22670, train/grad_norm: 0.17256708443164825
Step: 22670, train/learning_rate: 2.3024749680189416e-05
Step: 22670, train/epoch: 5.395050048828125
Step: 22680, train/loss: 0.001500000013038516
Step: 22680, train/grad_norm: 0.0033010225743055344
Step: 22680, train/learning_rate: 2.3012851670500822e-05
Step: 22680, train/epoch: 5.397429943084717
Step: 22690, train/loss: 0.08240000158548355
Step: 22690, train/grad_norm: 0.005032976157963276
Step: 22690, train/learning_rate: 2.3000951841822825e-05
Step: 22690, train/epoch: 5.399809837341309
Step: 22700, train/loss: 0.017899999395012856
Step: 22700, train/grad_norm: 0.036880772560834885
Step: 22700, train/learning_rate: 2.298905201314483e-05
Step: 22700, train/epoch: 5.402189254760742
Step: 22710, train/loss: 0.08720000088214874
Step: 22710, train/grad_norm: 0.0009161406778730452
Step: 22710, train/learning_rate: 2.2977154003456235e-05
Step: 22710, train/epoch: 5.404569149017334
Step: 22720, train/loss: 0.006099999882280827
Step: 22720, train/grad_norm: 0.009625915437936783
Step: 22720, train/learning_rate: 2.2965254174778238e-05
Step: 22720, train/epoch: 5.406949043273926
Step: 22730, train/loss: 0.011699999682605267
Step: 22730, train/grad_norm: 0.01998014934360981
Step: 22730, train/learning_rate: 2.2953356165089644e-05
Step: 22730, train/epoch: 5.409328937530518
Step: 22740, train/loss: 0.05829999968409538
Step: 22740, train/grad_norm: 0.020081767812371254
Step: 22740, train/learning_rate: 2.2941456336411647e-05
Step: 22740, train/epoch: 5.411708831787109
Step: 22750, train/loss: 0.011599999852478504
Step: 22750, train/grad_norm: 2.9049222469329834
Step: 22750, train/learning_rate: 2.292955650773365e-05
Step: 22750, train/epoch: 5.414088726043701
Step: 22760, train/loss: 0.09759999811649323
Step: 22760, train/grad_norm: 0.027534861117601395
Step: 22760, train/learning_rate: 2.2917658498045057e-05
Step: 22760, train/epoch: 5.416468143463135
Step: 22770, train/loss: 0.00039999998989515007
Step: 22770, train/grad_norm: 0.0012526260688900948
Step: 22770, train/learning_rate: 2.290575866936706e-05
Step: 22770, train/epoch: 5.418848037719727
Step: 22780, train/loss: 0.0010000000474974513
Step: 22780, train/grad_norm: 0.008821330033242702
Step: 22780, train/learning_rate: 2.2893860659678467e-05
Step: 22780, train/epoch: 5.421227931976318
Step: 22790, train/loss: 0.031300000846385956
Step: 22790, train/grad_norm: 0.00036119561991654336
Step: 22790, train/learning_rate: 2.288196083100047e-05
Step: 22790, train/epoch: 5.42360782623291
Step: 22800, train/loss: 0.10509999841451645
Step: 22800, train/grad_norm: 0.004995399620383978
Step: 22800, train/learning_rate: 2.2870061002322473e-05
Step: 22800, train/epoch: 5.425987720489502
Step: 22810, train/loss: 0.06480000168085098
Step: 22810, train/grad_norm: 0.0021107885986566544
Step: 22810, train/learning_rate: 2.285816299263388e-05
Step: 22810, train/epoch: 5.428367614746094
Step: 22820, train/loss: 0.0005000000237487257
Step: 22820, train/grad_norm: 0.002800946356728673
Step: 22820, train/learning_rate: 2.2846263163955882e-05
Step: 22820, train/epoch: 5.430747032165527
Step: 22830, train/loss: 0.06199999898672104
Step: 22830, train/grad_norm: 0.020402098074555397
Step: 22830, train/learning_rate: 2.283436515426729e-05
Step: 22830, train/epoch: 5.433126926422119
Step: 22840, train/loss: 9.999999747378752e-05
Step: 22840, train/grad_norm: 0.0005991054931655526
Step: 22840, train/learning_rate: 2.2822465325589292e-05
Step: 22840, train/epoch: 5.435506820678711
Step: 22850, train/loss: 0.0007999999797903001
Step: 22850, train/grad_norm: 0.0022013476118445396
Step: 22850, train/learning_rate: 2.2810565496911295e-05
Step: 22850, train/epoch: 5.437886714935303
Step: 22860, train/loss: 0.04809999838471413
Step: 22860, train/grad_norm: 0.0012671134900301695
Step: 22860, train/learning_rate: 2.27986674872227e-05
Step: 22860, train/epoch: 5.4402666091918945
Step: 22870, train/loss: 0.21150000393390656
Step: 22870, train/grad_norm: 0.01229861844331026
Step: 22870, train/learning_rate: 2.2786767658544704e-05
Step: 22870, train/epoch: 5.442646503448486
Step: 22880, train/loss: 0.01119999960064888
Step: 22880, train/grad_norm: 0.00032570972689427435
Step: 22880, train/learning_rate: 2.277486964885611e-05
Step: 22880, train/epoch: 5.445026397705078
Step: 22890, train/loss: 0.1437000036239624
Step: 22890, train/grad_norm: 0.00878599938005209
Step: 22890, train/learning_rate: 2.2762969820178114e-05
Step: 22890, train/epoch: 5.447405815124512
Step: 22900, train/loss: 0.0024999999441206455
Step: 22900, train/grad_norm: 0.0006700126687064767
Step: 22900, train/learning_rate: 2.275107181048952e-05
Step: 22900, train/epoch: 5.4497857093811035
Step: 22910, train/loss: 0.07620000094175339
Step: 22910, train/grad_norm: 0.0013834277633577585
Step: 22910, train/learning_rate: 2.2739171981811523e-05
Step: 22910, train/epoch: 5.452165603637695
Step: 22920, train/loss: 0.00039999998989515007
Step: 22920, train/grad_norm: 0.0025510715786367655
Step: 22920, train/learning_rate: 2.2727272153133526e-05
Step: 22920, train/epoch: 5.454545497894287
Step: 22930, train/loss: 0.009399999864399433
Step: 22930, train/grad_norm: 0.0026784685906022787
Step: 22930, train/learning_rate: 2.2715374143444933e-05
Step: 22930, train/epoch: 5.456925392150879
Step: 22940, train/loss: 0.0272000003606081
Step: 22940, train/grad_norm: 0.0014940904220566154
Step: 22940, train/learning_rate: 2.2703474314766936e-05
Step: 22940, train/epoch: 5.459305286407471
Step: 22950, train/loss: 0.08049999922513962
Step: 22950, train/grad_norm: 0.0008794399327598512
Step: 22950, train/learning_rate: 2.2691576305078343e-05
Step: 22950, train/epoch: 5.461684703826904
Step: 22960, train/loss: 0.0005000000237487257
Step: 22960, train/grad_norm: 0.0018271770095452666
Step: 22960, train/learning_rate: 2.2679676476400346e-05
Step: 22960, train/epoch: 5.464064598083496
Step: 22970, train/loss: 0.002199999988079071
Step: 22970, train/grad_norm: 0.0011237628059461713
Step: 22970, train/learning_rate: 2.266777664772235e-05
Step: 22970, train/epoch: 5.466444492340088
Step: 22980, train/loss: 0.008500000461935997
Step: 22980, train/grad_norm: 0.0006326339789666235
Step: 22980, train/learning_rate: 2.2655878638033755e-05
Step: 22980, train/epoch: 5.46882438659668
Step: 22990, train/loss: 0.09969999641180038
Step: 22990, train/grad_norm: 0.002416664268821478
Step: 22990, train/learning_rate: 2.2643978809355758e-05
Step: 22990, train/epoch: 5.4712042808532715
Step: 23000, train/loss: 0.12809999287128448
Step: 23000, train/grad_norm: 16.410797119140625
Step: 23000, train/learning_rate: 2.2632080799667165e-05
Step: 23000, train/epoch: 5.473584175109863
Step: 23010, train/loss: 0.08179999887943268
Step: 23010, train/grad_norm: 0.0009138014866039157
Step: 23010, train/learning_rate: 2.2620180970989168e-05
Step: 23010, train/epoch: 5.475963592529297
Step: 23020, train/loss: 0.05270000174641609
Step: 23020, train/grad_norm: 0.4646843373775482
Step: 23020, train/learning_rate: 2.260828114231117e-05
Step: 23020, train/epoch: 5.478343486785889
Step: 23030, train/loss: 0.08649999648332596
Step: 23030, train/grad_norm: 12.435373306274414
Step: 23030, train/learning_rate: 2.2596383132622577e-05
Step: 23030, train/epoch: 5.4807233810424805
Step: 23040, train/loss: 0.07090000063180923
Step: 23040, train/grad_norm: 0.0011920143151655793
Step: 23040, train/learning_rate: 2.258448330394458e-05
Step: 23040, train/epoch: 5.483103275299072
Step: 23050, train/loss: 0.028200000524520874
Step: 23050, train/grad_norm: 0.0017328619724139571
Step: 23050, train/learning_rate: 2.2572585294255987e-05
Step: 23050, train/epoch: 5.485483169555664
Step: 23060, train/loss: 0.07240000367164612
Step: 23060, train/grad_norm: 0.002976253628730774
Step: 23060, train/learning_rate: 2.256068546557799e-05
Step: 23060, train/epoch: 5.487863063812256
Step: 23070, train/loss: 0.004800000227987766
Step: 23070, train/grad_norm: 0.005921775940805674
Step: 23070, train/learning_rate: 2.2548785636899993e-05
Step: 23070, train/epoch: 5.490242958068848
Step: 23080, train/loss: 0.048900000751018524
Step: 23080, train/grad_norm: 0.13572266697883606
Step: 23080, train/learning_rate: 2.25368876272114e-05
Step: 23080, train/epoch: 5.492622375488281
Step: 23090, train/loss: 0.031300000846385956
Step: 23090, train/grad_norm: 0.0012833031360059977
Step: 23090, train/learning_rate: 2.2524987798533402e-05
Step: 23090, train/epoch: 5.495002269744873
Step: 23100, train/loss: 0.2321999967098236
Step: 23100, train/grad_norm: 18.47756576538086
Step: 23100, train/learning_rate: 2.251308978884481e-05
Step: 23100, train/epoch: 5.497382164001465
Step: 23110, train/loss: 0.012600000016391277
Step: 23110, train/grad_norm: 0.0035431282594799995
Step: 23110, train/learning_rate: 2.2501189960166812e-05
Step: 23110, train/epoch: 5.499762058258057
Step: 23120, train/loss: 0.0421999990940094
Step: 23120, train/grad_norm: 0.060035694390535355
Step: 23120, train/learning_rate: 2.2489290131488815e-05
Step: 23120, train/epoch: 5.502141952514648
Step: 23130, train/loss: 0.0003000000142492354
Step: 23130, train/grad_norm: 0.00998031347990036
Step: 23130, train/learning_rate: 2.247739212180022e-05
Step: 23130, train/epoch: 5.50452184677124
Step: 23140, train/loss: 0.05530000105500221
Step: 23140, train/grad_norm: 0.01010254304856062
Step: 23140, train/learning_rate: 2.2465492293122225e-05
Step: 23140, train/epoch: 5.506901264190674
Step: 23150, train/loss: 0.03590000048279762
Step: 23150, train/grad_norm: 0.0011139489943161607
Step: 23150, train/learning_rate: 2.245359428343363e-05
Step: 23150, train/epoch: 5.509281158447266
Step: 23160, train/loss: 0.03530000150203705
Step: 23160, train/grad_norm: 54.55206298828125
Step: 23160, train/learning_rate: 2.2441694454755634e-05
Step: 23160, train/epoch: 5.511661052703857
Step: 23170, train/loss: 0.042399998754262924
Step: 23170, train/grad_norm: 0.003877128241583705
Step: 23170, train/learning_rate: 2.2429794626077637e-05
Step: 23170, train/epoch: 5.514040946960449
Step: 23180, train/loss: 0.06920000165700912
Step: 23180, train/grad_norm: 1.3182743787765503
Step: 23180, train/learning_rate: 2.2417896616389044e-05
Step: 23180, train/epoch: 5.516420841217041
Step: 23190, train/loss: 0.04439999908208847
Step: 23190, train/grad_norm: 0.002430572174489498
Step: 23190, train/learning_rate: 2.2405996787711047e-05
Step: 23190, train/epoch: 5.518800735473633
Step: 23200, train/loss: 0.08980000019073486
Step: 23200, train/grad_norm: 0.004806323908269405
Step: 23200, train/learning_rate: 2.2394098778022453e-05
Step: 23200, train/epoch: 5.521180152893066
Step: 23210, train/loss: 0.03689999878406525
Step: 23210, train/grad_norm: 0.049673955887556076
Step: 23210, train/learning_rate: 2.2382198949344456e-05
Step: 23210, train/epoch: 5.523560047149658
Step: 23220, train/loss: 0.020099999383091927
Step: 23220, train/grad_norm: 7.023676872253418
Step: 23220, train/learning_rate: 2.237029912066646e-05
Step: 23220, train/epoch: 5.52593994140625
Step: 23230, train/loss: 0.11060000211000443
Step: 23230, train/grad_norm: 0.019937952980399132
Step: 23230, train/learning_rate: 2.2358401110977866e-05
Step: 23230, train/epoch: 5.528319835662842
Step: 23240, train/loss: 0.04540000110864639
Step: 23240, train/grad_norm: 0.012787730433046818
Step: 23240, train/learning_rate: 2.234650128229987e-05
Step: 23240, train/epoch: 5.530699729919434
Step: 23250, train/loss: 0.05130000039935112
Step: 23250, train/grad_norm: 0.0030122180469334126
Step: 23250, train/learning_rate: 2.2334603272611275e-05
Step: 23250, train/epoch: 5.533079624176025
Step: 23260, train/loss: 0.125
Step: 23260, train/grad_norm: 0.05079524219036102
Step: 23260, train/learning_rate: 2.232270344393328e-05
Step: 23260, train/epoch: 5.535459518432617
Step: 23270, train/loss: 0.06750000268220901
Step: 23270, train/grad_norm: 0.0002469054888933897
Step: 23270, train/learning_rate: 2.231080361525528e-05
Step: 23270, train/epoch: 5.537838935852051
Step: 23280, train/loss: 0.17810000479221344
Step: 23280, train/grad_norm: 0.0025630993768572807
Step: 23280, train/learning_rate: 2.2298905605566688e-05
Step: 23280, train/epoch: 5.540218830108643
Step: 23290, train/loss: 0.04259999841451645
Step: 23290, train/grad_norm: 0.40603724122047424
Step: 23290, train/learning_rate: 2.228700577688869e-05
Step: 23290, train/epoch: 5.542598724365234
Step: 23300, train/loss: 0.016599999740719795
Step: 23300, train/grad_norm: 0.0015054137911647558
Step: 23300, train/learning_rate: 2.2275107767200097e-05
Step: 23300, train/epoch: 5.544978618621826
Step: 23310, train/loss: 9.999999747378752e-05
Step: 23310, train/grad_norm: 0.001167377340607345
Step: 23310, train/learning_rate: 2.22632079385221e-05
Step: 23310, train/epoch: 5.547358512878418
Step: 23320, train/loss: 0.0013000000035390258
Step: 23320, train/grad_norm: 1.7214856147766113
Step: 23320, train/learning_rate: 2.2251308109844103e-05
Step: 23320, train/epoch: 5.54973840713501
Step: 23330, train/loss: 0.0005000000237487257
Step: 23330, train/grad_norm: 0.018854720517992973
Step: 23330, train/learning_rate: 2.223941010015551e-05
Step: 23330, train/epoch: 5.552117824554443
Step: 23340, train/loss: 0.07800000160932541
Step: 23340, train/grad_norm: 0.002650435781106353
Step: 23340, train/learning_rate: 2.2227510271477513e-05
Step: 23340, train/epoch: 5.554497718811035
Step: 23350, train/loss: 0.000699999975040555
Step: 23350, train/grad_norm: 0.0009687436977401376
Step: 23350, train/learning_rate: 2.221561226178892e-05
Step: 23350, train/epoch: 5.556877613067627
Step: 23360, train/loss: 0.0003000000142492354
Step: 23360, train/grad_norm: 0.0026062463875859976
Step: 23360, train/learning_rate: 2.2203712433110923e-05
Step: 23360, train/epoch: 5.559257507324219
Step: 23370, train/loss: 0.07069999724626541
Step: 23370, train/grad_norm: 4.210631847381592
Step: 23370, train/learning_rate: 2.2191812604432926e-05
Step: 23370, train/epoch: 5.5616374015808105
Step: 23380, train/loss: 0.07289999723434448
Step: 23380, train/grad_norm: 1.0763102769851685
Step: 23380, train/learning_rate: 2.2179914594744332e-05
Step: 23380, train/epoch: 5.564017295837402
Step: 23390, train/loss: 0.013100000098347664
Step: 23390, train/grad_norm: 0.00019851303659379482
Step: 23390, train/learning_rate: 2.2168014766066335e-05
Step: 23390, train/epoch: 5.566397190093994
Step: 23400, train/loss: 0.04830000177025795
Step: 23400, train/grad_norm: 0.002791615668684244
Step: 23400, train/learning_rate: 2.2156116756377742e-05
Step: 23400, train/epoch: 5.568776607513428
Step: 23410, train/loss: 9.999999747378752e-05
Step: 23410, train/grad_norm: 0.010990055277943611
Step: 23410, train/learning_rate: 2.2144216927699745e-05
Step: 23410, train/epoch: 5.5711565017700195
Step: 23420, train/loss: 0.0010000000474974513
Step: 23420, train/grad_norm: 0.002085003536194563
Step: 23420, train/learning_rate: 2.2132317099021748e-05
Step: 23420, train/epoch: 5.573536396026611
Step: 23430, train/loss: 0.0035000001080334187
Step: 23430, train/grad_norm: 0.0268853809684515
Step: 23430, train/learning_rate: 2.2120419089333154e-05
Step: 23430, train/epoch: 5.575916290283203
Step: 23440, train/loss: 0.019200000911951065
Step: 23440, train/grad_norm: 0.0007512605516240001
Step: 23440, train/learning_rate: 2.2108519260655157e-05
Step: 23440, train/epoch: 5.578296184539795
Step: 23450, train/loss: 0.14470000565052032
Step: 23450, train/grad_norm: 0.3227621018886566
Step: 23450, train/learning_rate: 2.2096621250966564e-05
Step: 23450, train/epoch: 5.580676078796387
Step: 23460, train/loss: 0.0012000000569969416
Step: 23460, train/grad_norm: 0.45461156964302063
Step: 23460, train/learning_rate: 2.2084721422288567e-05
Step: 23460, train/epoch: 5.58305549621582
Step: 23470, train/loss: 0.06120000034570694
Step: 23470, train/grad_norm: 0.0008074803627096117
Step: 23470, train/learning_rate: 2.207282159361057e-05
Step: 23470, train/epoch: 5.585435390472412
Step: 23480, train/loss: 9.999999747378752e-05
Step: 23480, train/grad_norm: 0.0007361499592661858
Step: 23480, train/learning_rate: 2.2060923583921976e-05
Step: 23480, train/epoch: 5.587815284729004
Step: 23490, train/loss: 9.999999747378752e-05
Step: 23490, train/grad_norm: 0.031955622136592865
Step: 23490, train/learning_rate: 2.204902375524398e-05
Step: 23490, train/epoch: 5.590195178985596
Step: 23500, train/loss: 0.044199999421834946
Step: 23500, train/grad_norm: 0.0025163821410387754
Step: 23500, train/learning_rate: 2.2037125745555386e-05
Step: 23500, train/epoch: 5.5925750732421875
Step: 23510, train/loss: 0.08919999748468399
Step: 23510, train/grad_norm: 0.016993431374430656
Step: 23510, train/learning_rate: 2.202522591687739e-05
Step: 23510, train/epoch: 5.594954967498779
Step: 23520, train/loss: 0.08410000056028366
Step: 23520, train/grad_norm: 0.005305329337716103
Step: 23520, train/learning_rate: 2.2013326088199392e-05
Step: 23520, train/epoch: 5.597334384918213
Step: 23530, train/loss: 0.010999999940395355
Step: 23530, train/grad_norm: 0.029552603140473366
Step: 23530, train/learning_rate: 2.20014280785108e-05
Step: 23530, train/epoch: 5.599714279174805
Step: 23540, train/loss: 0.14900000393390656
Step: 23540, train/grad_norm: 27.02161407470703
Step: 23540, train/learning_rate: 2.19895282498328e-05
Step: 23540, train/epoch: 5.6020941734313965
Step: 23550, train/loss: 0.006000000052154064
Step: 23550, train/grad_norm: 0.003127241274341941
Step: 23550, train/learning_rate: 2.1977630240144208e-05
Step: 23550, train/epoch: 5.604474067687988
Step: 23560, train/loss: 0.0003000000142492354
Step: 23560, train/grad_norm: 0.050634514540433884
Step: 23560, train/learning_rate: 2.196573041146621e-05
Step: 23560, train/epoch: 5.60685396194458
Step: 23570, train/loss: 0.11509999632835388
Step: 23570, train/grad_norm: 2.3421597480773926
Step: 23570, train/learning_rate: 2.1953832401777618e-05
Step: 23570, train/epoch: 5.609233856201172
Step: 23580, train/loss: 0.010300000198185444
Step: 23580, train/grad_norm: 2.618659257888794
Step: 23580, train/learning_rate: 2.194193257309962e-05
Step: 23580, train/epoch: 5.611613750457764
Step: 23590, train/loss: 0.07590000331401825
Step: 23590, train/grad_norm: 0.004439861048012972
Step: 23590, train/learning_rate: 2.1930032744421624e-05
Step: 23590, train/epoch: 5.613993167877197
Step: 23600, train/loss: 0.0071000000461936
Step: 23600, train/grad_norm: 0.029848679900169373
Step: 23600, train/learning_rate: 2.191813473473303e-05
Step: 23600, train/epoch: 5.616373062133789
Step: 23610, train/loss: 0.031099999323487282
Step: 23610, train/grad_norm: 4.7365680075017735e-05
Step: 23610, train/learning_rate: 2.1906234906055033e-05
Step: 23610, train/epoch: 5.618752956390381
Step: 23620, train/loss: 0.17589999735355377
Step: 23620, train/grad_norm: 0.0029257095884531736
Step: 23620, train/learning_rate: 2.189433689636644e-05
Step: 23620, train/epoch: 5.621132850646973
Step: 23630, train/loss: 0.02810000069439411
Step: 23630, train/grad_norm: 11.935696601867676
Step: 23630, train/learning_rate: 2.1882437067688443e-05
Step: 23630, train/epoch: 5.6235127449035645
Step: 23640, train/loss: 0.0015999999595806003
Step: 23640, train/grad_norm: 0.0020963093265891075
Step: 23640, train/learning_rate: 2.1870537239010446e-05
Step: 23640, train/epoch: 5.625892639160156
Step: 23650, train/loss: 0.0478999987244606
Step: 23650, train/grad_norm: 9.109131813049316
Step: 23650, train/learning_rate: 2.1858639229321852e-05
Step: 23650, train/epoch: 5.62827205657959
Step: 23660, train/loss: 0.10379999876022339
Step: 23660, train/grad_norm: 0.008619384840130806
Step: 23660, train/learning_rate: 2.1846739400643855e-05
Step: 23660, train/epoch: 5.630651950836182
Step: 23670, train/loss: 0.05389999970793724
Step: 23670, train/grad_norm: 0.0028036509174853563
Step: 23670, train/learning_rate: 2.1834841390955262e-05
Step: 23670, train/epoch: 5.633031845092773
Step: 23680, train/loss: 0.002400000113993883
Step: 23680, train/grad_norm: 0.036185186356306076
Step: 23680, train/learning_rate: 2.1822941562277265e-05
Step: 23680, train/epoch: 5.635411739349365
Step: 23690, train/loss: 0.00019999999494757503
Step: 23690, train/grad_norm: 0.0030969323124736547
Step: 23690, train/learning_rate: 2.1811041733599268e-05
Step: 23690, train/epoch: 5.637791633605957
Step: 23700, train/loss: 0.005400000140070915
Step: 23700, train/grad_norm: 0.007255020085722208
Step: 23700, train/learning_rate: 2.1799143723910674e-05
Step: 23700, train/epoch: 5.640171527862549
Step: 23710, train/loss: 0.1678999960422516
Step: 23710, train/grad_norm: 2.374779462814331
Step: 23710, train/learning_rate: 2.1787243895232677e-05
Step: 23710, train/epoch: 5.642550945281982
Step: 23720, train/loss: 0.3093999922275543
Step: 23720, train/grad_norm: 0.00030874641379341483
Step: 23720, train/learning_rate: 2.1775345885544084e-05
Step: 23720, train/epoch: 5.644930839538574
Step: 23730, train/loss: 0.07050000131130219
Step: 23730, train/grad_norm: 2.1925647258758545
Step: 23730, train/learning_rate: 2.1763446056866087e-05
Step: 23730, train/epoch: 5.647310733795166
Step: 23740, train/loss: 0.20640000700950623
Step: 23740, train/grad_norm: 3.1083335876464844
Step: 23740, train/learning_rate: 2.175154622818809e-05
Step: 23740, train/epoch: 5.649690628051758
Step: 23750, train/loss: 0.023499999195337296
Step: 23750, train/grad_norm: 0.005476009100675583
Step: 23750, train/learning_rate: 2.1739648218499497e-05
Step: 23750, train/epoch: 5.65207052230835
Step: 23760, train/loss: 0.0966000035405159
Step: 23760, train/grad_norm: 34.414833068847656
Step: 23760, train/learning_rate: 2.17277483898215e-05
Step: 23760, train/epoch: 5.654450416564941
Step: 23770, train/loss: 0.04830000177025795
Step: 23770, train/grad_norm: 0.10861315578222275
Step: 23770, train/learning_rate: 2.1715850380132906e-05
Step: 23770, train/epoch: 5.656830310821533
Step: 23780, train/loss: 0.006599999964237213
Step: 23780, train/grad_norm: 0.0010455487063154578
Step: 23780, train/learning_rate: 2.170395055145491e-05
Step: 23780, train/epoch: 5.659209728240967
Step: 23790, train/loss: 0.07919999957084656
Step: 23790, train/grad_norm: 0.011961637996137142
Step: 23790, train/learning_rate: 2.1692050722776912e-05
Step: 23790, train/epoch: 5.661589622497559
Step: 23800, train/loss: 0.004000000189989805
Step: 23800, train/grad_norm: 0.0026732683181762695
Step: 23800, train/learning_rate: 2.168015271308832e-05
Step: 23800, train/epoch: 5.66396951675415
Step: 23810, train/loss: 0.04050000011920929
Step: 23810, train/grad_norm: 0.0031480654142796993
Step: 23810, train/learning_rate: 2.1668252884410322e-05
Step: 23810, train/epoch: 5.666349411010742
Step: 23820, train/loss: 0.0006000000284984708
Step: 23820, train/grad_norm: 0.004124960862100124
Step: 23820, train/learning_rate: 2.1656354874721728e-05
Step: 23820, train/epoch: 5.668729305267334
Step: 23830, train/loss: 0.029899999499320984
Step: 23830, train/grad_norm: 2.7352802753448486
Step: 23830, train/learning_rate: 2.164445504604373e-05
Step: 23830, train/epoch: 5.671109199523926
Step: 23840, train/loss: 0.11580000072717667
Step: 23840, train/grad_norm: 0.0008491600165143609
Step: 23840, train/learning_rate: 2.1632555217365734e-05
Step: 23840, train/epoch: 5.673488616943359
Step: 23850, train/loss: 0.09080000221729279
Step: 23850, train/grad_norm: 0.011129907332360744
Step: 23850, train/learning_rate: 2.162065720767714e-05
Step: 23850, train/epoch: 5.675868511199951
Step: 23860, train/loss: 0.10050000250339508
Step: 23860, train/grad_norm: 2.158447265625
Step: 23860, train/learning_rate: 2.1608757378999144e-05
Step: 23860, train/epoch: 5.678248405456543
Step: 23870, train/loss: 0.00279999990016222
Step: 23870, train/grad_norm: 0.006788319908082485
Step: 23870, train/learning_rate: 2.159685936931055e-05
Step: 23870, train/epoch: 5.680628299713135
Step: 23880, train/loss: 0.00039999998989515007
Step: 23880, train/grad_norm: 0.0036820757668465376
Step: 23880, train/learning_rate: 2.1584959540632553e-05
Step: 23880, train/epoch: 5.683008193969727
Step: 23890, train/loss: 0.011800000444054604
Step: 23890, train/grad_norm: 0.0011791695142164826
Step: 23890, train/learning_rate: 2.1573059711954556e-05
Step: 23890, train/epoch: 5.685388088226318
Step: 23900, train/loss: 0.0017000000225380063
Step: 23900, train/grad_norm: 0.0043744882568717
Step: 23900, train/learning_rate: 2.1561161702265963e-05
Step: 23900, train/epoch: 5.687767505645752
Step: 23910, train/loss: 0.0008999999845400453
Step: 23910, train/grad_norm: 0.0007429068791680038
Step: 23910, train/learning_rate: 2.1549261873587966e-05
Step: 23910, train/epoch: 5.690147399902344
Step: 23920, train/loss: 0.02329999953508377
Step: 23920, train/grad_norm: 0.0061050159856677055
Step: 23920, train/learning_rate: 2.1537363863899373e-05
Step: 23920, train/epoch: 5.6925272941589355
Step: 23930, train/loss: 0.10689999908208847
Step: 23930, train/grad_norm: 0.11355919390916824
Step: 23930, train/learning_rate: 2.1525464035221376e-05
Step: 23930, train/epoch: 5.694907188415527
Step: 23940, train/loss: 0.000699999975040555
Step: 23940, train/grad_norm: 0.007323272060602903
Step: 23940, train/learning_rate: 2.151356420654338e-05
Step: 23940, train/epoch: 5.697287082672119
Step: 23950, train/loss: 0.01080000028014183
Step: 23950, train/grad_norm: 0.0007544347317889333
Step: 23950, train/learning_rate: 2.1501666196854785e-05
Step: 23950, train/epoch: 5.699666976928711
Step: 23960, train/loss: 0.03689999878406525
Step: 23960, train/grad_norm: 0.005205860827118158
Step: 23960, train/learning_rate: 2.1489766368176788e-05
Step: 23960, train/epoch: 5.702046871185303
Step: 23970, train/loss: 0.00039999998989515007
Step: 23970, train/grad_norm: 0.3288114070892334
Step: 23970, train/learning_rate: 2.1477868358488195e-05
Step: 23970, train/epoch: 5.704426288604736
Step: 23980, train/loss: 0.043800000101327896
Step: 23980, train/grad_norm: 0.0014192262897267938
Step: 23980, train/learning_rate: 2.1465968529810198e-05
Step: 23980, train/epoch: 5.706806182861328
Step: 23990, train/loss: 9.999999747378752e-05
Step: 23990, train/grad_norm: 0.0021219474729150534
Step: 23990, train/learning_rate: 2.14540687011322e-05
Step: 23990, train/epoch: 5.70918607711792
Step: 24000, train/loss: 0.008799999952316284
Step: 24000, train/grad_norm: 0.007420167792588472
Step: 24000, train/learning_rate: 2.1442170691443607e-05
Step: 24000, train/epoch: 5.711565971374512
Step: 24010, train/loss: 0.0031999999191612005
Step: 24010, train/grad_norm: 0.0036598858423531055
Step: 24010, train/learning_rate: 2.143027086276561e-05
Step: 24010, train/epoch: 5.7139458656311035
Step: 24020, train/loss: 0.09889999777078629
Step: 24020, train/grad_norm: 0.0036235798615962267
Step: 24020, train/learning_rate: 2.1418372853077017e-05
Step: 24020, train/epoch: 5.716325759887695
Step: 24030, train/loss: 0.07590000331401825
Step: 24030, train/grad_norm: 58.21773147583008
Step: 24030, train/learning_rate: 2.140647302439902e-05
Step: 24030, train/epoch: 5.718705177307129
Step: 24040, train/loss: 0.13369999825954437
Step: 24040, train/grad_norm: 0.02022557333111763
Step: 24040, train/learning_rate: 2.1394573195721023e-05
Step: 24040, train/epoch: 5.721085071563721
Step: 24050, train/loss: 0.05090000107884407
Step: 24050, train/grad_norm: 0.008982162922620773
Step: 24050, train/learning_rate: 2.138267518603243e-05
Step: 24050, train/epoch: 5.7234649658203125
Step: 24060, train/loss: 0.059300001710653305
Step: 24060, train/grad_norm: 1.3091182708740234
Step: 24060, train/learning_rate: 2.1370775357354432e-05
Step: 24060, train/epoch: 5.725844860076904
Step: 24070, train/loss: 0.0034000000450760126
Step: 24070, train/grad_norm: 0.0033266963437199593
Step: 24070, train/learning_rate: 2.135887734766584e-05
Step: 24070, train/epoch: 5.728224754333496
Step: 24080, train/loss: 0.2402999997138977
Step: 24080, train/grad_norm: 35.123897552490234
Step: 24080, train/learning_rate: 2.1346977518987842e-05
Step: 24080, train/epoch: 5.730604648590088
Step: 24090, train/loss: 0.1873999983072281
Step: 24090, train/grad_norm: 0.23418466746807098
Step: 24090, train/learning_rate: 2.1335077690309845e-05
Step: 24090, train/epoch: 5.7329840660095215
Step: 24100, train/loss: 0.0006000000284984708
Step: 24100, train/grad_norm: 0.06107790023088455
Step: 24100, train/learning_rate: 2.132317968062125e-05
Step: 24100, train/epoch: 5.735363960266113
Step: 24110, train/loss: 0.0006000000284984708
Step: 24110, train/grad_norm: 0.026696702465415
Step: 24110, train/learning_rate: 2.1311279851943254e-05
Step: 24110, train/epoch: 5.737743854522705
Step: 24120, train/loss: 0.06790000200271606
Step: 24120, train/grad_norm: 0.0027360667008906603
Step: 24120, train/learning_rate: 2.129938184225466e-05
Step: 24120, train/epoch: 5.740123748779297
Step: 24130, train/loss: 0.0005000000237487257
Step: 24130, train/grad_norm: 0.03616376593708992
Step: 24130, train/learning_rate: 2.1287482013576664e-05
Step: 24130, train/epoch: 5.742503643035889
Step: 24140, train/loss: 0.010099999606609344
Step: 24140, train/grad_norm: 0.030733386054635048
Step: 24140, train/learning_rate: 2.1275582184898667e-05
Step: 24140, train/epoch: 5.7448835372924805
Step: 24150, train/loss: 0.05939999967813492
Step: 24150, train/grad_norm: 0.0003921101742889732
Step: 24150, train/learning_rate: 2.1263684175210074e-05
Step: 24150, train/epoch: 5.747263431549072
Step: 24160, train/loss: 0.1753000020980835
Step: 24160, train/grad_norm: 32.185848236083984
Step: 24160, train/learning_rate: 2.1251784346532077e-05
Step: 24160, train/epoch: 5.749642848968506
Step: 24170, train/loss: 0.0019000000320374966
Step: 24170, train/grad_norm: 0.24900361895561218
Step: 24170, train/learning_rate: 2.1239886336843483e-05
Step: 24170, train/epoch: 5.752022743225098
Step: 24180, train/loss: 0.038100000470876694
Step: 24180, train/grad_norm: 0.009601516649127007
Step: 24180, train/learning_rate: 2.1227986508165486e-05
Step: 24180, train/epoch: 5.7544026374816895
Step: 24190, train/loss: 0.04470000043511391
Step: 24190, train/grad_norm: 0.05208161845803261
Step: 24190, train/learning_rate: 2.121608667948749e-05
Step: 24190, train/epoch: 5.756782531738281
Step: 24200, train/loss: 0.10509999841451645
Step: 24200, train/grad_norm: 0.041300736367702484
Step: 24200, train/learning_rate: 2.1204188669798896e-05
Step: 24200, train/epoch: 5.759162425994873
Step: 24210, train/loss: 0.056299999356269836
Step: 24210, train/grad_norm: 0.0026748075615614653
Step: 24210, train/learning_rate: 2.11922888411209e-05
Step: 24210, train/epoch: 5.761542320251465
Step: 24220, train/loss: 0.29269999265670776
Step: 24220, train/grad_norm: 0.767204761505127
Step: 24220, train/learning_rate: 2.1180390831432305e-05
Step: 24220, train/epoch: 5.763921737670898
Step: 24230, train/loss: 0.06469999998807907
Step: 24230, train/grad_norm: 0.004249385558068752
Step: 24230, train/learning_rate: 2.1168491002754308e-05
Step: 24230, train/epoch: 5.76630163192749
Step: 24240, train/loss: 0.01889999955892563
Step: 24240, train/grad_norm: 0.0009404304437339306
Step: 24240, train/learning_rate: 2.1156592993065715e-05
Step: 24240, train/epoch: 5.768681526184082
Step: 24250, train/loss: 0.12330000102519989
Step: 24250, train/grad_norm: 0.00433383509516716
Step: 24250, train/learning_rate: 2.1144693164387718e-05
Step: 24250, train/epoch: 5.771061420440674
Step: 24260, train/loss: 0.020800000056624413
Step: 24260, train/grad_norm: 0.0010998056968674064
Step: 24260, train/learning_rate: 2.113279333570972e-05
Step: 24260, train/epoch: 5.773441314697266
Step: 24270, train/loss: 0.048700001090765
Step: 24270, train/grad_norm: 0.0006836965912953019
Step: 24270, train/learning_rate: 2.1120895326021127e-05
Step: 24270, train/epoch: 5.775821208953857
Step: 24280, train/loss: 0.005499999970197678
Step: 24280, train/grad_norm: 14.933212280273438
Step: 24280, train/learning_rate: 2.110899549734313e-05
Step: 24280, train/epoch: 5.778200626373291
Step: 24290, train/loss: 0.004399999976158142
Step: 24290, train/grad_norm: 0.04914497211575508
Step: 24290, train/learning_rate: 2.1097097487654537e-05
Step: 24290, train/epoch: 5.780580520629883
Step: 24300, train/loss: 0.0714000016450882
Step: 24300, train/grad_norm: 0.0008879126398824155
Step: 24300, train/learning_rate: 2.108519765897654e-05
Step: 24300, train/epoch: 5.782960414886475
Step: 24310, train/loss: 0.0017999999690800905
Step: 24310, train/grad_norm: 0.0020022245589643717
Step: 24310, train/learning_rate: 2.1073297830298543e-05
Step: 24310, train/epoch: 5.785340309143066
Step: 24320, train/loss: 0.002400000113993883
Step: 24320, train/grad_norm: 0.04422725364565849
Step: 24320, train/learning_rate: 2.106139982060995e-05
Step: 24320, train/epoch: 5.787720203399658
Step: 24330, train/loss: 0.0026000000070780516
Step: 24330, train/grad_norm: 0.0021750126034021378
Step: 24330, train/learning_rate: 2.1049499991931953e-05
Step: 24330, train/epoch: 5.79010009765625
Step: 24340, train/loss: 0.017799999564886093
Step: 24340, train/grad_norm: 0.0006286255666054785
Step: 24340, train/learning_rate: 2.103760198224336e-05
Step: 24340, train/epoch: 5.792479991912842
Step: 24350, train/loss: 0.0869000032544136
Step: 24350, train/grad_norm: 0.0011235774727538228
Step: 24350, train/learning_rate: 2.1025702153565362e-05
Step: 24350, train/epoch: 5.794859409332275
Step: 24360, train/loss: 9.999999747378752e-05
Step: 24360, train/grad_norm: 0.035348910838365555
Step: 24360, train/learning_rate: 2.1013802324887365e-05
Step: 24360, train/epoch: 5.797239303588867
Step: 24370, train/loss: 0.12430000305175781
Step: 24370, train/grad_norm: 0.0008122610161080956
Step: 24370, train/learning_rate: 2.100190431519877e-05
Step: 24370, train/epoch: 5.799619197845459
Step: 24380, train/loss: 0.04899999871850014
Step: 24380, train/grad_norm: 0.0010866952361539006
Step: 24380, train/learning_rate: 2.0990004486520775e-05
Step: 24380, train/epoch: 5.801999092102051
Step: 24390, train/loss: 0.01140000019222498
Step: 24390, train/grad_norm: 0.00191517046187073
Step: 24390, train/learning_rate: 2.097810647683218e-05
Step: 24390, train/epoch: 5.804378986358643
Step: 24400, train/loss: 0.11789999902248383
Step: 24400, train/grad_norm: 4.928028583526611
Step: 24400, train/learning_rate: 2.0966206648154184e-05
Step: 24400, train/epoch: 5.806758880615234
Step: 24410, train/loss: 0.14749999344348907
Step: 24410, train/grad_norm: 46.601539611816406
Step: 24410, train/learning_rate: 2.0954306819476187e-05
Step: 24410, train/epoch: 5.809138298034668
Step: 24420, train/loss: 0.03460000082850456
Step: 24420, train/grad_norm: 33.46369552612305
Step: 24420, train/learning_rate: 2.0942408809787594e-05
Step: 24420, train/epoch: 5.81151819229126
Step: 24430, train/loss: 0.0869000032544136
Step: 24430, train/grad_norm: 16.088214874267578
Step: 24430, train/learning_rate: 2.0930508981109597e-05
Step: 24430, train/epoch: 5.813898086547852
Step: 24440, train/loss: 0.006099999882280827
Step: 24440, train/grad_norm: 0.04372403398156166
Step: 24440, train/learning_rate: 2.0918610971421003e-05
Step: 24440, train/epoch: 5.816277980804443
Step: 24450, train/loss: 0.03280000016093254
Step: 24450, train/grad_norm: 0.17714549601078033
Step: 24450, train/learning_rate: 2.0906711142743006e-05
Step: 24450, train/epoch: 5.818657875061035
Step: 24460, train/loss: 0.04919999837875366
Step: 24460, train/grad_norm: 46.63661193847656
Step: 24460, train/learning_rate: 2.089481131406501e-05
Step: 24460, train/epoch: 5.821037769317627
Step: 24470, train/loss: 0.056699998676776886
Step: 24470, train/grad_norm: 0.01807415299117565
Step: 24470, train/learning_rate: 2.0882913304376416e-05
Step: 24470, train/epoch: 5.8234171867370605
Step: 24480, train/loss: 0.010400000028312206
Step: 24480, train/grad_norm: 16.35047149658203
Step: 24480, train/learning_rate: 2.087101347569842e-05
Step: 24480, train/epoch: 5.825797080993652
Step: 24490, train/loss: 0.000699999975040555
Step: 24490, train/grad_norm: 0.04031084477901459
Step: 24490, train/learning_rate: 2.0859115466009825e-05
Step: 24490, train/epoch: 5.828176975250244
Step: 24500, train/loss: 0.07069999724626541
Step: 24500, train/grad_norm: 0.021942617371678352
Step: 24500, train/learning_rate: 2.084721563733183e-05
Step: 24500, train/epoch: 5.830556869506836
Step: 24510, train/loss: 0.02070000022649765
Step: 24510, train/grad_norm: 0.0005986602045595646
Step: 24510, train/learning_rate: 2.083531580865383e-05
Step: 24510, train/epoch: 5.832936763763428
Step: 24520, train/loss: 0.0997999981045723
Step: 24520, train/grad_norm: 0.012570898979902267
Step: 24520, train/learning_rate: 2.0823417798965238e-05
Step: 24520, train/epoch: 5.8353166580200195
Step: 24530, train/loss: 0.0989999994635582
Step: 24530, train/grad_norm: 1.7232587337493896
Step: 24530, train/learning_rate: 2.081151797028724e-05
Step: 24530, train/epoch: 5.837696552276611
Step: 24540, train/loss: 0.010200000368058681
Step: 24540, train/grad_norm: 0.007256450597196817
Step: 24540, train/learning_rate: 2.0799619960598648e-05
Step: 24540, train/epoch: 5.840075969696045
Step: 24550, train/loss: 0.0706000030040741
Step: 24550, train/grad_norm: 0.0007090845028869808
Step: 24550, train/learning_rate: 2.078772013192065e-05
Step: 24550, train/epoch: 5.842455863952637
Step: 24560, train/loss: 0.006200000178068876
Step: 24560, train/grad_norm: 11.576800346374512
Step: 24560, train/learning_rate: 2.0775820303242654e-05
Step: 24560, train/epoch: 5.8448357582092285
Step: 24570, train/loss: 0.0013000000035390258
Step: 24570, train/grad_norm: 0.020740140229463577
Step: 24570, train/learning_rate: 2.076392229355406e-05
Step: 24570, train/epoch: 5.84721565246582
Step: 24580, train/loss: 0.00019999999494757503
Step: 24580, train/grad_norm: 0.004149191547185183
Step: 24580, train/learning_rate: 2.0752022464876063e-05
Step: 24580, train/epoch: 5.849595546722412
Step: 24590, train/loss: 0.11990000307559967
Step: 24590, train/grad_norm: 0.10007927566766739
Step: 24590, train/learning_rate: 2.074012445518747e-05
Step: 24590, train/epoch: 5.851975440979004
Step: 24600, train/loss: 0.09839999675750732
Step: 24600, train/grad_norm: 0.009752855636179447
Step: 24600, train/learning_rate: 2.0728224626509473e-05
Step: 24600, train/epoch: 5.8543548583984375
Step: 24610, train/loss: 0.0142000000923872
Step: 24610, train/grad_norm: 0.011681444942951202
Step: 24610, train/learning_rate: 2.0716324797831476e-05
Step: 24610, train/epoch: 5.856734752655029
Step: 24620, train/loss: 0.02879999950528145
Step: 24620, train/grad_norm: 0.0002441861142870039
Step: 24620, train/learning_rate: 2.0704426788142882e-05
Step: 24620, train/epoch: 5.859114646911621
Step: 24630, train/loss: 0.002899999963119626
Step: 24630, train/grad_norm: 0.005742245353758335
Step: 24630, train/learning_rate: 2.0692526959464885e-05
Step: 24630, train/epoch: 5.861494541168213
Step: 24640, train/loss: 0.17900000512599945
Step: 24640, train/grad_norm: 0.04047253355383873
Step: 24640, train/learning_rate: 2.0680628949776292e-05
Step: 24640, train/epoch: 5.863874435424805
Step: 24650, train/loss: 0.05079999938607216
Step: 24650, train/grad_norm: 0.0012249789433553815
Step: 24650, train/learning_rate: 2.0668729121098295e-05
Step: 24650, train/epoch: 5.8662543296813965
Step: 24660, train/loss: 0.00019999999494757503
Step: 24660, train/grad_norm: 0.0027805815916508436
Step: 24660, train/learning_rate: 2.0656829292420298e-05
Step: 24660, train/epoch: 5.86863374710083
Step: 24670, train/loss: 0.1720999926328659
Step: 24670, train/grad_norm: 32.77080154418945
Step: 24670, train/learning_rate: 2.0644931282731704e-05
Step: 24670, train/epoch: 5.871013641357422
Step: 24680, train/loss: 0.00019999999494757503
Step: 24680, train/grad_norm: 0.007798437960445881
Step: 24680, train/learning_rate: 2.0633031454053707e-05
Step: 24680, train/epoch: 5.873393535614014
Step: 24690, train/loss: 0.02290000021457672
Step: 24690, train/grad_norm: 0.026160424575209618
Step: 24690, train/learning_rate: 2.0621133444365114e-05
Step: 24690, train/epoch: 5.8757734298706055
Step: 24700, train/loss: 0.09130000323057175
Step: 24700, train/grad_norm: 25.153051376342773
Step: 24700, train/learning_rate: 2.0609233615687117e-05
Step: 24700, train/epoch: 5.878153324127197
Step: 24710, train/loss: 0.025200000032782555
Step: 24710, train/grad_norm: 0.00012255484762135893
Step: 24710, train/learning_rate: 2.059733378700912e-05
Step: 24710, train/epoch: 5.880533218383789
Step: 24720, train/loss: 9.999999747378752e-05
Step: 24720, train/grad_norm: 0.0011746383970603347
Step: 24720, train/learning_rate: 2.0585435777320527e-05
Step: 24720, train/epoch: 5.882913112640381
Step: 24730, train/loss: 9.999999747378752e-05
Step: 24730, train/grad_norm: 0.00023369694827124476
Step: 24730, train/learning_rate: 2.057353594864253e-05
Step: 24730, train/epoch: 5.8852925300598145
Step: 24740, train/loss: 0.0617000013589859
Step: 24740, train/grad_norm: 0.00029840614297427237
Step: 24740, train/learning_rate: 2.0561637938953936e-05
Step: 24740, train/epoch: 5.887672424316406
Step: 24750, train/loss: 9.999999747378752e-05
Step: 24750, train/grad_norm: 0.0015266641275957227
Step: 24750, train/learning_rate: 2.054973811027594e-05
Step: 24750, train/epoch: 5.890052318572998
Step: 24760, train/loss: 0.009499999694526196
Step: 24760, train/grad_norm: 0.0027469864580780268
Step: 24760, train/learning_rate: 2.0537838281597942e-05
Step: 24760, train/epoch: 5.89243221282959
Step: 24770, train/loss: 0.0066999997943639755
Step: 24770, train/grad_norm: 0.055114299058914185
Step: 24770, train/learning_rate: 2.052594027190935e-05
Step: 24770, train/epoch: 5.894812107086182
Step: 24780, train/loss: 0.005200000014156103
Step: 24780, train/grad_norm: 0.000365998042980209
Step: 24780, train/learning_rate: 2.051404044323135e-05
Step: 24780, train/epoch: 5.897192001342773
Step: 24790, train/loss: 0.018200000748038292
Step: 24790, train/grad_norm: 0.0006014620885252953
Step: 24790, train/learning_rate: 2.0502142433542758e-05
Step: 24790, train/epoch: 5.899571418762207
Step: 24800, train/loss: 0.0003000000142492354
Step: 24800, train/grad_norm: 0.3707246780395508
Step: 24800, train/learning_rate: 2.049024260486476e-05
Step: 24800, train/epoch: 5.901951313018799
Step: 24810, train/loss: 0.15389999747276306
Step: 24810, train/grad_norm: 8.678813934326172
Step: 24810, train/learning_rate: 2.0478342776186764e-05
Step: 24810, train/epoch: 5.904331207275391
Step: 24820, train/loss: 0.09300000220537186
Step: 24820, train/grad_norm: 8.672958374023438
Step: 24820, train/learning_rate: 2.046644476649817e-05
Step: 24820, train/epoch: 5.906711101531982
Step: 24830, train/loss: 0.011900000274181366
Step: 24830, train/grad_norm: 0.005275316536426544
Step: 24830, train/learning_rate: 2.0454544937820174e-05
Step: 24830, train/epoch: 5.909090995788574
Step: 24840, train/loss: 0.029999999329447746
Step: 24840, train/grad_norm: 9.862735748291016
Step: 24840, train/learning_rate: 2.044264692813158e-05
Step: 24840, train/epoch: 5.911470890045166
Step: 24850, train/loss: 0.006000000052154064
Step: 24850, train/grad_norm: 0.06998513638973236
Step: 24850, train/learning_rate: 2.0430747099453583e-05
Step: 24850, train/epoch: 5.913850784301758
Step: 24860, train/loss: 0.11029999703168869
Step: 24860, train/grad_norm: 0.014536249451339245
Step: 24860, train/learning_rate: 2.0418847270775586e-05
Step: 24860, train/epoch: 5.916230201721191
Step: 24870, train/loss: 0.0608999989926815
Step: 24870, train/grad_norm: 0.8621857762336731
Step: 24870, train/learning_rate: 2.0406949261086993e-05
Step: 24870, train/epoch: 5.918610095977783
Step: 24880, train/loss: 0.00019999999494757503
Step: 24880, train/grad_norm: 0.11582347005605698
Step: 24880, train/learning_rate: 2.0395049432408996e-05
Step: 24880, train/epoch: 5.920989990234375
Step: 24890, train/loss: 0.19750000536441803
Step: 24890, train/grad_norm: 0.009524479508399963
Step: 24890, train/learning_rate: 2.0383151422720402e-05
Step: 24890, train/epoch: 5.923369884490967
Step: 24900, train/loss: 9.999999747378752e-05
Step: 24900, train/grad_norm: 0.0022421085741370916
Step: 24900, train/learning_rate: 2.0371251594042405e-05
Step: 24900, train/epoch: 5.925749778747559
Step: 24910, train/loss: 9.999999747378752e-05
Step: 24910, train/grad_norm: 0.04216470196843147
Step: 24910, train/learning_rate: 2.0359353584353812e-05
Step: 24910, train/epoch: 5.92812967300415
Step: 24920, train/loss: 0.05350000038743019
Step: 24920, train/grad_norm: 0.00032328790985047817
Step: 24920, train/learning_rate: 2.0347453755675815e-05
Step: 24920, train/epoch: 5.930509090423584
Step: 24930, train/loss: 0.010300000198185444
Step: 24930, train/grad_norm: 0.0009604936349205673
Step: 24930, train/learning_rate: 2.0335553926997818e-05
Step: 24930, train/epoch: 5.932888984680176
Step: 24940, train/loss: 0.09189999848604202
Step: 24940, train/grad_norm: 0.0002948096371255815
Step: 24940, train/learning_rate: 2.0323655917309225e-05
Step: 24940, train/epoch: 5.935268878936768
Step: 24950, train/loss: 0.10189999639987946
Step: 24950, train/grad_norm: 38.87101364135742
Step: 24950, train/learning_rate: 2.0311756088631228e-05
Step: 24950, train/epoch: 5.937648773193359
Step: 24960, train/loss: 0.1251000016927719
Step: 24960, train/grad_norm: 52.27503967285156
Step: 24960, train/learning_rate: 2.0299858078942634e-05
Step: 24960, train/epoch: 5.940028667449951
Step: 24970, train/loss: 0.0017000000225380063
Step: 24970, train/grad_norm: 0.028955966234207153
Step: 24970, train/learning_rate: 2.0287958250264637e-05
Step: 24970, train/epoch: 5.942408561706543
Step: 24980, train/loss: 0.0714000016450882
Step: 24980, train/grad_norm: 0.0004870455013588071
Step: 24980, train/learning_rate: 2.027605842158664e-05
Step: 24980, train/epoch: 5.944787979125977
Step: 24990, train/loss: 0.0007999999797903001
Step: 24990, train/grad_norm: 0.0033584809862077236
Step: 24990, train/learning_rate: 2.0264160411898047e-05
Step: 24990, train/epoch: 5.947167873382568
Step: 25000, train/loss: 0.09889999777078629
Step: 25000, train/grad_norm: 0.011240119114518166
Step: 25000, train/learning_rate: 2.025226058322005e-05
Step: 25000, train/epoch: 5.94954776763916
Step: 25010, train/loss: 0.022299999371170998
Step: 25010, train/grad_norm: 0.002084914827719331
Step: 25010, train/learning_rate: 2.0240362573531456e-05
Step: 25010, train/epoch: 5.951927661895752
Step: 25020, train/loss: 0.0012000000569969416
Step: 25020, train/grad_norm: 0.016423499211668968
Step: 25020, train/learning_rate: 2.022846274485346e-05
Step: 25020, train/epoch: 5.954307556152344
Step: 25030, train/loss: 0.11089999973773956
Step: 25030, train/grad_norm: 0.0029429655987769365
Step: 25030, train/learning_rate: 2.0216562916175462e-05
Step: 25030, train/epoch: 5.9566874504089355
Step: 25040, train/loss: 0.11469999700784683
Step: 25040, train/grad_norm: 47.09776306152344
Step: 25040, train/learning_rate: 2.020466490648687e-05
Step: 25040, train/epoch: 5.959067344665527
Step: 25050, train/loss: 0.04800000041723251
Step: 25050, train/grad_norm: 0.0005409649456851184
Step: 25050, train/learning_rate: 2.0192765077808872e-05
Step: 25050, train/epoch: 5.961446762084961
Step: 25060, train/loss: 0.061799999326467514
Step: 25060, train/grad_norm: 0.1301671266555786
Step: 25060, train/learning_rate: 2.018086706812028e-05
Step: 25060, train/epoch: 5.963826656341553
Step: 25070, train/loss: 0.010599999688565731
Step: 25070, train/grad_norm: 0.009797020815312862
Step: 25070, train/learning_rate: 2.016896723944228e-05
Step: 25070, train/epoch: 5.9662065505981445
Step: 25080, train/loss: 0.12370000034570694
Step: 25080, train/grad_norm: 0.0327720008790493
Step: 25080, train/learning_rate: 2.0157067410764284e-05
Step: 25080, train/epoch: 5.968586444854736
Step: 25090, train/loss: 0.1340000033378601
Step: 25090, train/grad_norm: 0.08466419577598572
Step: 25090, train/learning_rate: 2.014516940107569e-05
Step: 25090, train/epoch: 5.970966339111328
Step: 25100, train/loss: 0.04879999905824661
Step: 25100, train/grad_norm: 4.842529296875
Step: 25100, train/learning_rate: 2.0133269572397694e-05
Step: 25100, train/epoch: 5.97334623336792
Step: 25110, train/loss: 0.014999999664723873
Step: 25110, train/grad_norm: 0.003616960719227791
Step: 25110, train/learning_rate: 2.01213715627091e-05
Step: 25110, train/epoch: 5.9757256507873535
Step: 25120, train/loss: 0.08320000022649765
Step: 25120, train/grad_norm: 0.012221424840390682
Step: 25120, train/learning_rate: 2.0109471734031104e-05
Step: 25120, train/epoch: 5.978105545043945
Step: 25130, train/loss: 0.031199999153614044
Step: 25130, train/grad_norm: 0.0028296136297285557
Step: 25130, train/learning_rate: 2.0097571905353107e-05
Step: 25130, train/epoch: 5.980485439300537
Step: 25140, train/loss: 0.13079999387264252
Step: 25140, train/grad_norm: 0.0010838615708053112
Step: 25140, train/learning_rate: 2.0085673895664513e-05
Step: 25140, train/epoch: 5.982865333557129
Step: 25150, train/loss: 0.00570000009611249
Step: 25150, train/grad_norm: 4.308577537536621
Step: 25150, train/learning_rate: 2.0073774066986516e-05
Step: 25150, train/epoch: 5.985245227813721
Step: 25160, train/loss: 0.02810000069439411
Step: 25160, train/grad_norm: 0.004792597610503435
Step: 25160, train/learning_rate: 2.0061876057297923e-05
Step: 25160, train/epoch: 5.9876251220703125
Step: 25170, train/loss: 0.0010999999940395355
Step: 25170, train/grad_norm: 3.730564832687378
Step: 25170, train/learning_rate: 2.0049976228619926e-05
Step: 25170, train/epoch: 5.990004539489746
Step: 25180, train/loss: 0.05889999866485596
Step: 25180, train/grad_norm: 0.004389995243400335
Step: 25180, train/learning_rate: 2.003807639994193e-05
Step: 25180, train/epoch: 5.992384433746338
Step: 25190, train/loss: 0.0851999968290329
Step: 25190, train/grad_norm: 0.022670850157737732
Step: 25190, train/learning_rate: 2.0026178390253335e-05
Step: 25190, train/epoch: 5.99476432800293
Step: 25200, train/loss: 0.0674000009894371
Step: 25200, train/grad_norm: 0.004015272483229637
Step: 25200, train/learning_rate: 2.0014278561575338e-05
Step: 25200, train/epoch: 5.9971442222595215
Step: 25210, train/loss: 0.004999999888241291
Step: 25210, train/grad_norm: 0.005282161291688681
Step: 25210, train/learning_rate: 2.0002380551886745e-05
Step: 25210, train/epoch: 5.999524116516113
Step: 25212, eval/loss: 0.3081629276275635
Step: 25212, eval/accuracy: 0.9482160210609436
Step: 25212, eval/f1: 0.9457423090934753
Step: 25212, eval/runtime: 295.89959716796875
Step: 25212, eval/samples_per_second: 24.343000411987305
Step: 25212, eval/steps_per_second: 3.0450000762939453
Step: 25212, train/epoch: 6.0
Step: 25220, train/loss: 0.0003000000142492354
Step: 25220, train/grad_norm: 0.03360540792346001
Step: 25220, train/learning_rate: 1.9990480723208748e-05
Step: 25220, train/epoch: 6.001904010772705
Step: 25230, train/loss: 0.0035000001080334187
Step: 25230, train/grad_norm: 0.3276364207267761
Step: 25230, train/learning_rate: 1.997858089453075e-05
Step: 25230, train/epoch: 6.004283905029297
Step: 25240, train/loss: 9.999999747378752e-05
Step: 25240, train/grad_norm: 0.0001343286712653935
Step: 25240, train/learning_rate: 1.9966682884842157e-05
Step: 25240, train/epoch: 6.0066633224487305
Step: 25250, train/loss: 0.061400000005960464
Step: 25250, train/grad_norm: 57.79338073730469
Step: 25250, train/learning_rate: 1.995478305616416e-05
Step: 25250, train/epoch: 6.009043216705322
Step: 25260, train/loss: 0.00039999998989515007
Step: 25260, train/grad_norm: 0.17422908544540405
Step: 25260, train/learning_rate: 1.9942885046475567e-05
Step: 25260, train/epoch: 6.011423110961914
Step: 25270, train/loss: 0.025299999862909317
Step: 25270, train/grad_norm: 0.001397139742039144
Step: 25270, train/learning_rate: 1.993098521779757e-05
Step: 25270, train/epoch: 6.013803005218506
Step: 25280, train/loss: 0.00019999999494757503
Step: 25280, train/grad_norm: 0.0005683377385139465
Step: 25280, train/learning_rate: 1.9919085389119573e-05
Step: 25280, train/epoch: 6.016182899475098
Step: 25290, train/loss: 0.08839999884366989
Step: 25290, train/grad_norm: 0.0036110945511609316
Step: 25290, train/learning_rate: 1.990718737943098e-05
Step: 25290, train/epoch: 6.0185627937316895
Step: 25300, train/loss: 9.999999747378752e-05
Step: 25300, train/grad_norm: 0.012024487368762493
Step: 25300, train/learning_rate: 1.9895287550752982e-05
Step: 25300, train/epoch: 6.020942211151123
Step: 25310, train/loss: 0.0052999998442828655
Step: 25310, train/grad_norm: 0.0007070936262607574
Step: 25310, train/learning_rate: 1.988338954106439e-05
Step: 25310, train/epoch: 6.023322105407715
Step: 25320, train/loss: 0.0007999999797903001
Step: 25320, train/grad_norm: 0.9464412927627563
Step: 25320, train/learning_rate: 1.9871489712386392e-05
Step: 25320, train/epoch: 6.025701999664307
Step: 25330, train/loss: 0.01759999990463257
Step: 25330, train/grad_norm: 0.002074794378131628
Step: 25330, train/learning_rate: 1.9859589883708395e-05
Step: 25330, train/epoch: 6.028081893920898
Step: 25340, train/loss: 0.03610000014305115
Step: 25340, train/grad_norm: 0.005717417225241661
Step: 25340, train/learning_rate: 1.98476918740198e-05
Step: 25340, train/epoch: 6.03046178817749
Step: 25350, train/loss: 0.0835999995470047
Step: 25350, train/grad_norm: 0.012523152865469456
Step: 25350, train/learning_rate: 1.9835792045341805e-05
Step: 25350, train/epoch: 6.032841682434082
Step: 25360, train/loss: 0.15389999747276306
Step: 25360, train/grad_norm: 0.0010909989941865206
Step: 25360, train/learning_rate: 1.982389403565321e-05
Step: 25360, train/epoch: 6.035221099853516
Step: 25370, train/loss: 0.0272000003606081
Step: 25370, train/grad_norm: 69.46236419677734
Step: 25370, train/learning_rate: 1.9811994206975214e-05
Step: 25370, train/epoch: 6.037600994110107
Step: 25380, train/loss: 0.010099999606609344
Step: 25380, train/grad_norm: 0.00908923614770174
Step: 25380, train/learning_rate: 1.9800094378297217e-05
Step: 25380, train/epoch: 6.039980888366699
Step: 25390, train/loss: 0.012900000438094139
Step: 25390, train/grad_norm: 0.04319065436720848
Step: 25390, train/learning_rate: 1.9788196368608624e-05
Step: 25390, train/epoch: 6.042360782623291
Step: 25400, train/loss: 0.0034000000450760126
Step: 25400, train/grad_norm: 0.0001850764238042757
Step: 25400, train/learning_rate: 1.9776296539930627e-05
Step: 25400, train/epoch: 6.044740676879883
Step: 25410, train/loss: 0.016300000250339508
Step: 25410, train/grad_norm: 0.0021990276873111725
Step: 25410, train/learning_rate: 1.9764398530242033e-05
Step: 25410, train/epoch: 6.047120571136475
Step: 25420, train/loss: 9.999999747378752e-05
Step: 25420, train/grad_norm: 0.0027145196218043566
Step: 25420, train/learning_rate: 1.9752498701564036e-05
Step: 25420, train/epoch: 6.049500465393066
Step: 25430, train/loss: 9.999999747378752e-05
Step: 25430, train/grad_norm: 0.001454878831282258
Step: 25430, train/learning_rate: 1.974059887288604e-05
Step: 25430, train/epoch: 6.0518798828125
Step: 25440, train/loss: 0.00019999999494757503
Step: 25440, train/grad_norm: 0.0046391854993999004
Step: 25440, train/learning_rate: 1.9728700863197446e-05
Step: 25440, train/epoch: 6.054259777069092
Step: 25450, train/loss: 0.1216999962925911
Step: 25450, train/grad_norm: 0.0012369194300845265
Step: 25450, train/learning_rate: 1.971680103451945e-05
Step: 25450, train/epoch: 6.056639671325684
Step: 25460, train/loss: 0.1111999973654747
Step: 25460, train/grad_norm: 0.0023417435586452484
Step: 25460, train/learning_rate: 1.9704903024830855e-05
Step: 25460, train/epoch: 6.059019565582275
Step: 25470, train/loss: 0.00279999990016222
Step: 25470, train/grad_norm: 0.0009342076373286545
Step: 25470, train/learning_rate: 1.969300319615286e-05
Step: 25470, train/epoch: 6.061399459838867
Step: 25480, train/loss: 0.03229999914765358
Step: 25480, train/grad_norm: 0.1996258944272995
Step: 25480, train/learning_rate: 1.968110336747486e-05
Step: 25480, train/epoch: 6.063779354095459
Step: 25490, train/loss: 0.01940000057220459
Step: 25490, train/grad_norm: 0.00039365110569633543
Step: 25490, train/learning_rate: 1.9669205357786268e-05
Step: 25490, train/epoch: 6.066158771514893
Step: 25500, train/loss: 0.005100000184029341
Step: 25500, train/grad_norm: 11.014415740966797
Step: 25500, train/learning_rate: 1.965730552910827e-05
Step: 25500, train/epoch: 6.068538665771484
Step: 25510, train/loss: 0.007499999832361937
Step: 25510, train/grad_norm: 0.00015311654715333134
Step: 25510, train/learning_rate: 1.9645407519419678e-05
Step: 25510, train/epoch: 6.070918560028076
Step: 25520, train/loss: 0.034699998795986176
Step: 25520, train/grad_norm: 44.379173278808594
Step: 25520, train/learning_rate: 1.963350769074168e-05
Step: 25520, train/epoch: 6.073298454284668
Step: 25530, train/loss: 9.999999747378752e-05
Step: 25530, train/grad_norm: 0.06791183352470398
Step: 25530, train/learning_rate: 1.9621607862063684e-05
Step: 25530, train/epoch: 6.07567834854126
Step: 25540, train/loss: 0.0005000000237487257
Step: 25540, train/grad_norm: 0.003972948994487524
Step: 25540, train/learning_rate: 1.960970985237509e-05
Step: 25540, train/epoch: 6.078058242797852
Step: 25550, train/loss: 0.1444000005722046
Step: 25550, train/grad_norm: 0.3691413402557373
Step: 25550, train/learning_rate: 1.9597810023697093e-05
Step: 25550, train/epoch: 6.080437660217285
Step: 25560, train/loss: 0.07940000295639038
Step: 25560, train/grad_norm: 0.004171895794570446
Step: 25560, train/learning_rate: 1.95859120140085e-05
Step: 25560, train/epoch: 6.082817554473877
Step: 25570, train/loss: 0.0575999990105629
Step: 25570, train/grad_norm: 0.025965042412281036
Step: 25570, train/learning_rate: 1.9574012185330503e-05
Step: 25570, train/epoch: 6.085197448730469
Step: 25580, train/loss: 0.008299999870359898
Step: 25580, train/grad_norm: 1.9749048948287964
Step: 25580, train/learning_rate: 1.956211417564191e-05
Step: 25580, train/epoch: 6.0875773429870605
Step: 25590, train/loss: 0.009200000204145908
Step: 25590, train/grad_norm: 0.00025373001699335873
Step: 25590, train/learning_rate: 1.9550214346963912e-05
Step: 25590, train/epoch: 6.089957237243652
Step: 25600, train/loss: 0.019200000911951065
Step: 25600, train/grad_norm: 0.014130613766610622
Step: 25600, train/learning_rate: 1.9538314518285915e-05
Step: 25600, train/epoch: 6.092337131500244
Step: 25610, train/loss: 0.00019999999494757503
Step: 25610, train/grad_norm: 0.0004908493719995022
Step: 25610, train/learning_rate: 1.9526416508597322e-05
Step: 25610, train/epoch: 6.094717025756836
Step: 25620, train/loss: 0.014800000004470348
Step: 25620, train/grad_norm: 0.3348245620727539
Step: 25620, train/learning_rate: 1.9514516679919325e-05
Step: 25620, train/epoch: 6.0970964431762695
Step: 25630, train/loss: 0.005499999970197678
Step: 25630, train/grad_norm: 0.0017223836621269584
Step: 25630, train/learning_rate: 1.950261867023073e-05
Step: 25630, train/epoch: 6.099476337432861
Step: 25640, train/loss: 0.1657000035047531
Step: 25640, train/grad_norm: 14.242297172546387
Step: 25640, train/learning_rate: 1.9490718841552734e-05
Step: 25640, train/epoch: 6.101856231689453
Step: 25650, train/loss: 0.00039999998989515007
Step: 25650, train/grad_norm: 0.0006909137591719627
Step: 25650, train/learning_rate: 1.9478819012874737e-05
Step: 25650, train/epoch: 6.104236125946045
Step: 25660, train/loss: 0.0005000000237487257
Step: 25660, train/grad_norm: 0.07557962089776993
Step: 25660, train/learning_rate: 1.9466921003186144e-05
Step: 25660, train/epoch: 6.106616020202637
Step: 25670, train/loss: 0.013899999670684338
Step: 25670, train/grad_norm: 18.424314498901367
Step: 25670, train/learning_rate: 1.9455021174508147e-05
Step: 25670, train/epoch: 6.1089959144592285
Step: 25680, train/loss: 0.0006000000284984708
Step: 25680, train/grad_norm: 8.923785935621709e-05
Step: 25680, train/learning_rate: 1.9443123164819553e-05
Step: 25680, train/epoch: 6.111375331878662
Step: 25690, train/loss: 0.058800000697374344
Step: 25690, train/grad_norm: 0.0021822263952344656
Step: 25690, train/learning_rate: 1.9431223336141557e-05
Step: 25690, train/epoch: 6.113755226135254
Step: 25700, train/loss: 0.030500000342726707
Step: 25700, train/grad_norm: 2.4410414695739746
Step: 25700, train/learning_rate: 1.941932350746356e-05
Step: 25700, train/epoch: 6.116135120391846
Step: 25710, train/loss: 0.05909999832510948
Step: 25710, train/grad_norm: 0.0025638872757554054
Step: 25710, train/learning_rate: 1.9407425497774966e-05
Step: 25710, train/epoch: 6.1185150146484375
Step: 25720, train/loss: 0.025599999353289604
Step: 25720, train/grad_norm: 9.25848726183176e-05
Step: 25720, train/learning_rate: 1.939552566909697e-05
Step: 25720, train/epoch: 6.120894908905029
Step: 25730, train/loss: 0.052400000393390656
Step: 25730, train/grad_norm: 0.00018926369375549257
Step: 25730, train/learning_rate: 1.9383627659408376e-05
Step: 25730, train/epoch: 6.123274803161621
Step: 25740, train/loss: 0.0010999999940395355
Step: 25740, train/grad_norm: 0.002562357345595956
Step: 25740, train/learning_rate: 1.937172783073038e-05
Step: 25740, train/epoch: 6.125654220581055
Step: 25750, train/loss: 0.0754999965429306
Step: 25750, train/grad_norm: 9.659701347351074
Step: 25750, train/learning_rate: 1.935982800205238e-05
Step: 25750, train/epoch: 6.1280341148376465
Step: 25760, train/loss: 0.023399999365210533
Step: 25760, train/grad_norm: 0.0025492655113339424
Step: 25760, train/learning_rate: 1.9347929992363788e-05
Step: 25760, train/epoch: 6.130414009094238
Step: 25770, train/loss: 0.0006000000284984708
Step: 25770, train/grad_norm: 0.023291712626814842
Step: 25770, train/learning_rate: 1.933603016368579e-05
Step: 25770, train/epoch: 6.13279390335083
Step: 25780, train/loss: 0.00139999995008111
Step: 25780, train/grad_norm: 7.141820907592773
Step: 25780, train/learning_rate: 1.9324132153997198e-05
Step: 25780, train/epoch: 6.135173797607422
Step: 25790, train/loss: 0.008299999870359898
Step: 25790, train/grad_norm: 0.0019470633706077933
Step: 25790, train/learning_rate: 1.93122323253192e-05
Step: 25790, train/epoch: 6.137553691864014
Step: 25800, train/loss: 0.00019999999494757503
Step: 25800, train/grad_norm: 0.0007888947729952633
Step: 25800, train/learning_rate: 1.9300332496641204e-05
Step: 25800, train/epoch: 6.1399335861206055
Step: 25810, train/loss: 0.07590000331401825
Step: 25810, train/grad_norm: 0.00015744261327199638
Step: 25810, train/learning_rate: 1.928843448695261e-05
Step: 25810, train/epoch: 6.142313003540039
Step: 25820, train/loss: 0.09189999848604202
Step: 25820, train/grad_norm: 0.2806064784526825
Step: 25820, train/learning_rate: 1.9276534658274613e-05
Step: 25820, train/epoch: 6.144692897796631
Step: 25830, train/loss: 0.01140000019222498
Step: 25830, train/grad_norm: 0.004650776740163565
Step: 25830, train/learning_rate: 1.926463664858602e-05
Step: 25830, train/epoch: 6.147072792053223
Step: 25840, train/loss: 0.066600002348423
Step: 25840, train/grad_norm: 0.0007765951449982822
Step: 25840, train/learning_rate: 1.9252736819908023e-05
Step: 25840, train/epoch: 6.1494526863098145
Step: 25850, train/loss: 0.002099999925121665
Step: 25850, train/grad_norm: 3.275578022003174
Step: 25850, train/learning_rate: 1.9240836991230026e-05
Step: 25850, train/epoch: 6.151832580566406
Step: 25860, train/loss: 9.999999747378752e-05
Step: 25860, train/grad_norm: 0.008944071829319
Step: 25860, train/learning_rate: 1.9228938981541432e-05
Step: 25860, train/epoch: 6.154212474822998
Step: 25870, train/loss: 0.04259999841451645
Step: 25870, train/grad_norm: 0.0007802238687872887
Step: 25870, train/learning_rate: 1.9217039152863435e-05
Step: 25870, train/epoch: 6.156591892242432
Step: 25880, train/loss: 0.03970000147819519
Step: 25880, train/grad_norm: 70.44547271728516
Step: 25880, train/learning_rate: 1.9205141143174842e-05
Step: 25880, train/epoch: 6.158971786499023
Step: 25890, train/loss: 0.025200000032782555
Step: 25890, train/grad_norm: 0.00020269765809644014
Step: 25890, train/learning_rate: 1.9193241314496845e-05
Step: 25890, train/epoch: 6.161351680755615
Step: 25900, train/loss: 0.029600000008940697
Step: 25900, train/grad_norm: 0.001181565341539681
Step: 25900, train/learning_rate: 1.9181341485818848e-05
Step: 25900, train/epoch: 6.163731575012207
Step: 25910, train/loss: 0.1785999983549118
Step: 25910, train/grad_norm: 0.08085682988166809
Step: 25910, train/learning_rate: 1.9169443476130255e-05
Step: 25910, train/epoch: 6.166111469268799
Step: 25920, train/loss: 0.0
Step: 25920, train/grad_norm: 0.0005074776709079742
Step: 25920, train/learning_rate: 1.9157543647452258e-05
Step: 25920, train/epoch: 6.168491363525391
Step: 25930, train/loss: 0.010900000110268593
Step: 25930, train/grad_norm: 0.2588444650173187
Step: 25930, train/learning_rate: 1.9145645637763664e-05
Step: 25930, train/epoch: 6.170870780944824
Step: 25940, train/loss: 0.11469999700784683
Step: 25940, train/grad_norm: 0.0013825552305206656
Step: 25940, train/learning_rate: 1.9133745809085667e-05
Step: 25940, train/epoch: 6.173250675201416
Step: 25950, train/loss: 0.10540000349283218
Step: 25950, train/grad_norm: 0.05325401946902275
Step: 25950, train/learning_rate: 1.912184598040767e-05
Step: 25950, train/epoch: 6.175630569458008
Step: 25960, train/loss: 0.02019999921321869
Step: 25960, train/grad_norm: 0.036219507455825806
Step: 25960, train/learning_rate: 1.9109947970719077e-05
Step: 25960, train/epoch: 6.1780104637146
Step: 25970, train/loss: 9.999999747378752e-05
Step: 25970, train/grad_norm: 0.2822088897228241
Step: 25970, train/learning_rate: 1.909804814204108e-05
Step: 25970, train/epoch: 6.180390357971191
Step: 25980, train/loss: 0.004800000227987766
Step: 25980, train/grad_norm: 0.0014909144956618547
Step: 25980, train/learning_rate: 1.9086150132352486e-05
Step: 25980, train/epoch: 6.182770252227783
Step: 25990, train/loss: 0.0
Step: 25990, train/grad_norm: 0.0007717700791545212
Step: 25990, train/learning_rate: 1.907425030367449e-05
Step: 25990, train/epoch: 6.185150146484375
Step: 26000, train/loss: 0.0
Step: 26000, train/grad_norm: 0.0009620405617170036
Step: 26000, train/learning_rate: 1.9062350474996492e-05
Step: 26000, train/epoch: 6.187529563903809
Step: 26010, train/loss: 9.999999747378752e-05
Step: 26010, train/grad_norm: 0.005902701523154974
Step: 26010, train/learning_rate: 1.90504524653079e-05
Step: 26010, train/epoch: 6.1899094581604
Step: 26020, train/loss: 9.999999747378752e-05
Step: 26020, train/grad_norm: 0.2861195206642151
Step: 26020, train/learning_rate: 1.9038552636629902e-05
Step: 26020, train/epoch: 6.192289352416992
Step: 26030, train/loss: 0.00019999999494757503
Step: 26030, train/grad_norm: 0.0430898480117321
Step: 26030, train/learning_rate: 1.902665462694131e-05
Step: 26030, train/epoch: 6.194669246673584
Step: 26040, train/loss: 9.999999747378752e-05
Step: 26040, train/grad_norm: 0.001150638097897172
Step: 26040, train/learning_rate: 1.901475479826331e-05
Step: 26040, train/epoch: 6.197049140930176
Step: 26050, train/loss: 0.021800000220537186
Step: 26050, train/grad_norm: 26.54612922668457
Step: 26050, train/learning_rate: 1.9002854969585314e-05
Step: 26050, train/epoch: 6.199429035186768
Step: 26060, train/loss: 0.265500009059906
Step: 26060, train/grad_norm: 0.0062011792324483395
Step: 26060, train/learning_rate: 1.899095695989672e-05
Step: 26060, train/epoch: 6.201808452606201
Step: 26070, train/loss: 0.010700000450015068
Step: 26070, train/grad_norm: 3.0327444076538086
Step: 26070, train/learning_rate: 1.8979057131218724e-05
Step: 26070, train/epoch: 6.204188346862793
Step: 26080, train/loss: 0.08269999921321869
Step: 26080, train/grad_norm: 0.02132115699350834
Step: 26080, train/learning_rate: 1.896715912153013e-05
Step: 26080, train/epoch: 6.206568241119385
Step: 26090, train/loss: 0.0
Step: 26090, train/grad_norm: 0.0005158534040674567
Step: 26090, train/learning_rate: 1.8955259292852134e-05
Step: 26090, train/epoch: 6.208948135375977
Step: 26100, train/loss: 0.051899999380111694
Step: 26100, train/grad_norm: 0.1414516121149063
Step: 26100, train/learning_rate: 1.8943359464174137e-05
Step: 26100, train/epoch: 6.211328029632568
Step: 26110, train/loss: 0.026200000196695328
Step: 26110, train/grad_norm: 75.12920379638672
Step: 26110, train/learning_rate: 1.8931461454485543e-05
Step: 26110, train/epoch: 6.21370792388916
Step: 26120, train/loss: 0.0
Step: 26120, train/grad_norm: 0.01282487902790308
Step: 26120, train/learning_rate: 1.8919561625807546e-05
Step: 26120, train/epoch: 6.216087341308594
Step: 26130, train/loss: 9.999999747378752e-05
Step: 26130, train/grad_norm: 0.00039698759792372584
Step: 26130, train/learning_rate: 1.8907663616118953e-05
Step: 26130, train/epoch: 6.2184672355651855
Step: 26140, train/loss: 0.0
Step: 26140, train/grad_norm: 0.0002830300363712013
Step: 26140, train/learning_rate: 1.8895763787440956e-05
Step: 26140, train/epoch: 6.220847129821777
Step: 26150, train/loss: 0.08560000360012054
Step: 26150, train/grad_norm: 0.06283552199602127
Step: 26150, train/learning_rate: 1.888386395876296e-05
Step: 26150, train/epoch: 6.223227024078369
Step: 26160, train/loss: 0.03999999910593033
Step: 26160, train/grad_norm: 0.0029188652988523245
Step: 26160, train/learning_rate: 1.8871965949074365e-05
Step: 26160, train/epoch: 6.225606918334961
Step: 26170, train/loss: 0.001500000013038516
Step: 26170, train/grad_norm: 1.698625922203064
Step: 26170, train/learning_rate: 1.8860066120396368e-05
Step: 26170, train/epoch: 6.227986812591553
Step: 26180, train/loss: 0.013799999840557575
Step: 26180, train/grad_norm: 0.001803748426027596
Step: 26180, train/learning_rate: 1.8848168110707775e-05
Step: 26180, train/epoch: 6.2303667068481445
Step: 26190, train/loss: 0.013500000350177288
Step: 26190, train/grad_norm: 0.0012992165284231305
Step: 26190, train/learning_rate: 1.8836268282029778e-05
Step: 26190, train/epoch: 6.232746124267578
Step: 26200, train/loss: 0.0005000000237487257
Step: 26200, train/grad_norm: 0.000971082306932658
Step: 26200, train/learning_rate: 1.882436845335178e-05
Step: 26200, train/epoch: 6.23512601852417
Step: 26210, train/loss: 0.004900000058114529
Step: 26210, train/grad_norm: 0.00031435940763913095
Step: 26210, train/learning_rate: 1.8812470443663187e-05
Step: 26210, train/epoch: 6.237505912780762
Step: 26220, train/loss: 0.004100000020116568
Step: 26220, train/grad_norm: 0.0001390924007864669
Step: 26220, train/learning_rate: 1.880057061498519e-05
Step: 26220, train/epoch: 6.2398858070373535
Step: 26230, train/loss: 0.00019999999494757503
Step: 26230, train/grad_norm: 0.007746228948235512
Step: 26230, train/learning_rate: 1.8788672605296597e-05
Step: 26230, train/epoch: 6.242265701293945
Step: 26240, train/loss: 0.005799999926239252
Step: 26240, train/grad_norm: 0.007725427858531475
Step: 26240, train/learning_rate: 1.87767727766186e-05
Step: 26240, train/epoch: 6.244645595550537
Step: 26250, train/loss: 0.007199999876320362
Step: 26250, train/grad_norm: 0.14548617601394653
Step: 26250, train/learning_rate: 1.8764874766930006e-05
Step: 26250, train/epoch: 6.247025012969971
Step: 26260, train/loss: 0.053700000047683716
Step: 26260, train/grad_norm: 0.0004245807940606028
Step: 26260, train/learning_rate: 1.875297493825201e-05
Step: 26260, train/epoch: 6.2494049072265625
Step: 26270, train/loss: 0.014700000174343586
Step: 26270, train/grad_norm: 0.0011762370122596622
Step: 26270, train/learning_rate: 1.8741075109574012e-05
Step: 26270, train/epoch: 6.251784801483154
Step: 26280, train/loss: 0.11479999870061874
Step: 26280, train/grad_norm: 7.542592356912792e-05
Step: 26280, train/learning_rate: 1.872917709988542e-05
Step: 26280, train/epoch: 6.254164695739746
Step: 26290, train/loss: 0.0007999999797903001
Step: 26290, train/grad_norm: 0.0004406514926813543
Step: 26290, train/learning_rate: 1.8717277271207422e-05
Step: 26290, train/epoch: 6.256544589996338
Step: 26300, train/loss: 0.03929999843239784
Step: 26300, train/grad_norm: 0.1105312705039978
Step: 26300, train/learning_rate: 1.870537926151883e-05
Step: 26300, train/epoch: 6.25892448425293
Step: 26310, train/loss: 0.11180000007152557
Step: 26310, train/grad_norm: 46.552711486816406
Step: 26310, train/learning_rate: 1.869347943284083e-05
Step: 26310, train/epoch: 6.2613043785095215
Step: 26320, train/loss: 0.01810000091791153
Step: 26320, train/grad_norm: 0.00019543727103155106
Step: 26320, train/learning_rate: 1.8681579604162835e-05
Step: 26320, train/epoch: 6.263683795928955
Step: 26330, train/loss: 0.007000000216066837
Step: 26330, train/grad_norm: 9.25138228922151e-05
Step: 26330, train/learning_rate: 1.866968159447424e-05
Step: 26330, train/epoch: 6.266063690185547
Step: 26340, train/loss: 0.1761000007390976
Step: 26340, train/grad_norm: 0.016297712922096252
Step: 26340, train/learning_rate: 1.8657781765796244e-05
Step: 26340, train/epoch: 6.268443584442139
Step: 26350, train/loss: 0.0015999999595806003
Step: 26350, train/grad_norm: 0.023617302998900414
Step: 26350, train/learning_rate: 1.864588375610765e-05
Step: 26350, train/epoch: 6.2708234786987305
Step: 26360, train/loss: 0.0031999999191612005
Step: 26360, train/grad_norm: 0.005041163414716721
Step: 26360, train/learning_rate: 1.8633983927429654e-05
Step: 26360, train/epoch: 6.273203372955322
Step: 26370, train/loss: 0.10999999940395355
Step: 26370, train/grad_norm: 9.014178276062012
Step: 26370, train/learning_rate: 1.8622084098751657e-05
Step: 26370, train/epoch: 6.275583267211914
Step: 26380, train/loss: 0.0005000000237487257
Step: 26380, train/grad_norm: 0.013365453109145164
Step: 26380, train/learning_rate: 1.8610186089063063e-05
Step: 26380, train/epoch: 6.277962684631348
Step: 26390, train/loss: 0.009399999864399433
Step: 26390, train/grad_norm: 0.09346440434455872
Step: 26390, train/learning_rate: 1.8598286260385066e-05
Step: 26390, train/epoch: 6.2803425788879395
Step: 26400, train/loss: 0.01889999955892563
Step: 26400, train/grad_norm: 0.0004421203047968447
Step: 26400, train/learning_rate: 1.8586388250696473e-05
Step: 26400, train/epoch: 6.282722473144531
Step: 26410, train/loss: 0.0010999999940395355
Step: 26410, train/grad_norm: 0.005900354124605656
Step: 26410, train/learning_rate: 1.8574488422018476e-05
Step: 26410, train/epoch: 6.285102367401123
Step: 26420, train/loss: 0.0006000000284984708
Step: 26420, train/grad_norm: 0.0468660444021225
Step: 26420, train/learning_rate: 1.856258859334048e-05
Step: 26420, train/epoch: 6.287482261657715
Step: 26430, train/loss: 0.06549999862909317
Step: 26430, train/grad_norm: 0.5312349200248718
Step: 26430, train/learning_rate: 1.8550690583651885e-05
Step: 26430, train/epoch: 6.289862155914307
Step: 26440, train/loss: 0.0032999999821186066
Step: 26440, train/grad_norm: 0.0018736491911113262
Step: 26440, train/learning_rate: 1.853879075497389e-05
Step: 26440, train/epoch: 6.29224157333374
Step: 26450, train/loss: 9.999999747378752e-05
Step: 26450, train/grad_norm: 0.04157422482967377
Step: 26450, train/learning_rate: 1.8526892745285295e-05
Step: 26450, train/epoch: 6.294621467590332
Step: 26460, train/loss: 0.03959999978542328
Step: 26460, train/grad_norm: 0.12935182452201843
Step: 26460, train/learning_rate: 1.8514992916607298e-05
Step: 26460, train/epoch: 6.297001361846924
Step: 26470, train/loss: 0.05570000037550926
Step: 26470, train/grad_norm: 47.88798141479492
Step: 26470, train/learning_rate: 1.85030930879293e-05
Step: 26470, train/epoch: 6.299381256103516
Step: 26480, train/loss: 0.0006000000284984708
Step: 26480, train/grad_norm: 0.00634561525657773
Step: 26480, train/learning_rate: 1.8491195078240708e-05
Step: 26480, train/epoch: 6.301761150360107
Step: 26490, train/loss: 9.999999747378752e-05
Step: 26490, train/grad_norm: 0.0005706992815248668
Step: 26490, train/learning_rate: 1.847929524956271e-05
Step: 26490, train/epoch: 6.304141044616699
Step: 26500, train/loss: 0.05590000003576279
Step: 26500, train/grad_norm: 2.10843563079834
Step: 26500, train/learning_rate: 1.8467397239874117e-05
Step: 26500, train/epoch: 6.306520938873291
Step: 26510, train/loss: 0.002400000113993883
Step: 26510, train/grad_norm: 0.0032363873906433582
Step: 26510, train/learning_rate: 1.845549741119612e-05
Step: 26510, train/epoch: 6.308900356292725
Step: 26520, train/loss: 0.002400000113993883
Step: 26520, train/grad_norm: 0.0016628112643957138
Step: 26520, train/learning_rate: 1.8443597582518123e-05
Step: 26520, train/epoch: 6.311280250549316
Step: 26530, train/loss: 9.999999747378752e-05
Step: 26530, train/grad_norm: 0.022715875878930092
Step: 26530, train/learning_rate: 1.843169957282953e-05
Step: 26530, train/epoch: 6.313660144805908
Step: 26540, train/loss: 9.999999747378752e-05
Step: 26540, train/grad_norm: 0.040491845458745956
Step: 26540, train/learning_rate: 1.8419799744151533e-05
Step: 26540, train/epoch: 6.3160400390625
Step: 26550, train/loss: 0.03150000050663948
Step: 26550, train/grad_norm: 0.000554855156224221
Step: 26550, train/learning_rate: 1.840790173446294e-05
Step: 26550, train/epoch: 6.318419933319092
Step: 26560, train/loss: 0.0632999986410141
Step: 26560, train/grad_norm: 8.6317777633667
Step: 26560, train/learning_rate: 1.8396001905784942e-05
Step: 26560, train/epoch: 6.320799827575684
Step: 26570, train/loss: 0.07540000230073929
Step: 26570, train/grad_norm: 18.172380447387695
Step: 26570, train/learning_rate: 1.8384102077106945e-05
Step: 26570, train/epoch: 6.323179244995117
Step: 26580, train/loss: 0.016499999910593033
Step: 26580, train/grad_norm: 0.00016334162501152605
Step: 26580, train/learning_rate: 1.8372204067418352e-05
Step: 26580, train/epoch: 6.325559139251709
Step: 26590, train/loss: 0.027499999850988388
Step: 26590, train/grad_norm: 0.0061865379102528095
Step: 26590, train/learning_rate: 1.8360304238740355e-05
Step: 26590, train/epoch: 6.327939033508301
Step: 26600, train/loss: 0.041999999433755875
Step: 26600, train/grad_norm: 0.0135292187333107
Step: 26600, train/learning_rate: 1.834840622905176e-05
Step: 26600, train/epoch: 6.330318927764893
Step: 26610, train/loss: 0.18219999969005585
Step: 26610, train/grad_norm: 0.9866514801979065
Step: 26610, train/learning_rate: 1.8336506400373764e-05
Step: 26610, train/epoch: 6.332698822021484
Step: 26620, train/loss: 0.004600000102072954
Step: 26620, train/grad_norm: 0.01667916774749756
Step: 26620, train/learning_rate: 1.8324606571695767e-05
Step: 26620, train/epoch: 6.335078716278076
Step: 26630, train/loss: 0.0007999999797903001
Step: 26630, train/grad_norm: 0.0021401988342404366
Step: 26630, train/learning_rate: 1.8312708562007174e-05
Step: 26630, train/epoch: 6.33745813369751
Step: 26640, train/loss: 0.07689999788999557
Step: 26640, train/grad_norm: 0.22586023807525635
Step: 26640, train/learning_rate: 1.8300808733329177e-05
Step: 26640, train/epoch: 6.339838027954102
Step: 26650, train/loss: 0.022600000724196434
Step: 26650, train/grad_norm: 0.1941400021314621
Step: 26650, train/learning_rate: 1.8288910723640583e-05
Step: 26650, train/epoch: 6.342217922210693
Step: 26660, train/loss: 0.02810000069439411
Step: 26660, train/grad_norm: 0.03283388167619705
Step: 26660, train/learning_rate: 1.8277010894962586e-05
Step: 26660, train/epoch: 6.344597816467285
Step: 26670, train/loss: 0.00039999998989515007
Step: 26670, train/grad_norm: 0.003449228359386325
Step: 26670, train/learning_rate: 1.826511106628459e-05
Step: 26670, train/epoch: 6.346977710723877
Step: 26680, train/loss: 9.999999747378752e-05
Step: 26680, train/grad_norm: 0.031877677887678146
Step: 26680, train/learning_rate: 1.8253213056595996e-05
Step: 26680, train/epoch: 6.349357604980469
Step: 26690, train/loss: 0.007300000172108412
Step: 26690, train/grad_norm: 0.006569262128323317
Step: 26690, train/learning_rate: 1.8241313227918e-05
Step: 26690, train/epoch: 6.3517374992370605
Step: 26700, train/loss: 9.999999747378752e-05
Step: 26700, train/grad_norm: 0.002935651456937194
Step: 26700, train/learning_rate: 1.8229415218229406e-05
Step: 26700, train/epoch: 6.354116916656494
Step: 26710, train/loss: 0.09350000321865082
Step: 26710, train/grad_norm: 0.00015011518553365022
Step: 26710, train/learning_rate: 1.821751538955141e-05
Step: 26710, train/epoch: 6.356496810913086
Step: 26720, train/loss: 0.0071000000461936
Step: 26720, train/grad_norm: 0.26354196667671204
Step: 26720, train/learning_rate: 1.820561556087341e-05
Step: 26720, train/epoch: 6.358876705169678
Step: 26730, train/loss: 0.0003000000142492354
Step: 26730, train/grad_norm: 0.5908501148223877
Step: 26730, train/learning_rate: 1.8193717551184818e-05
Step: 26730, train/epoch: 6.3612565994262695
Step: 26740, train/loss: 9.999999747378752e-05
Step: 26740, train/grad_norm: 0.1286097913980484
Step: 26740, train/learning_rate: 1.818181772250682e-05
Step: 26740, train/epoch: 6.363636493682861
Step: 26750, train/loss: 0.04820000007748604
Step: 26750, train/grad_norm: 0.001457478734664619
Step: 26750, train/learning_rate: 1.8169919712818228e-05
Step: 26750, train/epoch: 6.366016387939453
Step: 26760, train/loss: 0.0017000000225380063
Step: 26760, train/grad_norm: 0.048621803522109985
Step: 26760, train/learning_rate: 1.815801988414023e-05
Step: 26760, train/epoch: 6.368395805358887
Step: 26770, train/loss: 0.024299999698996544
Step: 26770, train/grad_norm: 0.0015807336894795299
Step: 26770, train/learning_rate: 1.8146120055462234e-05
Step: 26770, train/epoch: 6.3707756996154785
Step: 26780, train/loss: 0.10559999942779541
Step: 26780, train/grad_norm: 0.03245822712779045
Step: 26780, train/learning_rate: 1.813422204577364e-05
Step: 26780, train/epoch: 6.37315559387207
Step: 26790, train/loss: 0.03189999982714653
Step: 26790, train/grad_norm: 0.036558352410793304
Step: 26790, train/learning_rate: 1.8122322217095643e-05
Step: 26790, train/epoch: 6.375535488128662
Step: 26800, train/loss: 0.0
Step: 26800, train/grad_norm: 0.00748924957588315
Step: 26800, train/learning_rate: 1.811042420740705e-05
Step: 26800, train/epoch: 6.377915382385254
Step: 26810, train/loss: 0.0006000000284984708
Step: 26810, train/grad_norm: 0.0029610167257487774
Step: 26810, train/learning_rate: 1.8098524378729053e-05
Step: 26810, train/epoch: 6.380295276641846
Step: 26820, train/loss: 0.016200000420212746
Step: 26820, train/grad_norm: 0.00012864862219430506
Step: 26820, train/learning_rate: 1.8086624550051056e-05
Step: 26820, train/epoch: 6.382674694061279
Step: 26830, train/loss: 0.10260000079870224
Step: 26830, train/grad_norm: 0.004775083623826504
Step: 26830, train/learning_rate: 1.8074726540362462e-05
Step: 26830, train/epoch: 6.385054588317871
Step: 26840, train/loss: 0.12099999934434891
Step: 26840, train/grad_norm: 0.0007113259634934366
Step: 26840, train/learning_rate: 1.8062826711684465e-05
Step: 26840, train/epoch: 6.387434482574463
Step: 26850, train/loss: 0.019099999219179153
Step: 26850, train/grad_norm: 0.0031057049054652452
Step: 26850, train/learning_rate: 1.8050928701995872e-05
Step: 26850, train/epoch: 6.389814376831055
Step: 26860, train/loss: 0.008999999612569809
Step: 26860, train/grad_norm: 3.992102861404419
Step: 26860, train/learning_rate: 1.8039028873317875e-05
Step: 26860, train/epoch: 6.3921942710876465
Step: 26870, train/loss: 0.0003000000142492354
Step: 26870, train/grad_norm: 0.09863332659006119
Step: 26870, train/learning_rate: 1.8027129044639878e-05
Step: 26870, train/epoch: 6.394574165344238
Step: 26880, train/loss: 0.026200000196695328
Step: 26880, train/grad_norm: 0.0017345091328024864
Step: 26880, train/learning_rate: 1.8015231034951285e-05
Step: 26880, train/epoch: 6.39695405960083
Step: 26890, train/loss: 0.00019999999494757503
Step: 26890, train/grad_norm: 0.04047689214348793
Step: 26890, train/learning_rate: 1.8003331206273288e-05
Step: 26890, train/epoch: 6.399333477020264
Step: 26900, train/loss: 0.09279999881982803
Step: 26900, train/grad_norm: 3.5499899240676314e-05
Step: 26900, train/learning_rate: 1.7991433196584694e-05
Step: 26900, train/epoch: 6.4017133712768555
Step: 26910, train/loss: 0.0003000000142492354
Step: 26910, train/grad_norm: 0.0018511662492528558
Step: 26910, train/learning_rate: 1.7979533367906697e-05
Step: 26910, train/epoch: 6.404093265533447
Step: 26920, train/loss: 0.0006000000284984708
Step: 26920, train/grad_norm: 0.0036899999249726534
Step: 26920, train/learning_rate: 1.7967635358218104e-05
Step: 26920, train/epoch: 6.406473159790039
Step: 26930, train/loss: 0.00039999998989515007
Step: 26930, train/grad_norm: 0.2629750669002533
Step: 26930, train/learning_rate: 1.7955735529540107e-05
Step: 26930, train/epoch: 6.408853054046631
Step: 26940, train/loss: 0.006300000008195639
Step: 26940, train/grad_norm: 0.028639495372772217
Step: 26940, train/learning_rate: 1.794383570086211e-05
Step: 26940, train/epoch: 6.411232948303223
Step: 26950, train/loss: 0.003800000064074993
Step: 26950, train/grad_norm: 0.7317174673080444
Step: 26950, train/learning_rate: 1.7931937691173516e-05
Step: 26950, train/epoch: 6.413612365722656
Step: 26960, train/loss: 0.06620000302791595
Step: 26960, train/grad_norm: 0.0006764705176465213
Step: 26960, train/learning_rate: 1.792003786249552e-05
Step: 26960, train/epoch: 6.415992259979248
Step: 26970, train/loss: 0.04230000078678131
Step: 26970, train/grad_norm: 0.01202407106757164
Step: 26970, train/learning_rate: 1.7908139852806926e-05
Step: 26970, train/epoch: 6.41837215423584
Step: 26980, train/loss: 0.0052999998442828655
Step: 26980, train/grad_norm: 2.2263782024383545
Step: 26980, train/learning_rate: 1.789624002412893e-05
Step: 26980, train/epoch: 6.420752048492432
Step: 26990, train/loss: 0.022299999371170998
Step: 26990, train/grad_norm: 0.00019834801787510514
Step: 26990, train/learning_rate: 1.7884340195450932e-05
Step: 26990, train/epoch: 6.423131942749023
Step: 27000, train/loss: 9.999999747378752e-05
Step: 27000, train/grad_norm: 0.0004367744841147214
Step: 27000, train/learning_rate: 1.787244218576234e-05
Step: 27000, train/epoch: 6.425511837005615
Step: 27010, train/loss: 0.018799999728798866
Step: 27010, train/grad_norm: 0.0022909678518772125
Step: 27010, train/learning_rate: 1.786054235708434e-05
Step: 27010, train/epoch: 6.427891254425049
Step: 27020, train/loss: 0.0005000000237487257
Step: 27020, train/grad_norm: 0.015667956322431564
Step: 27020, train/learning_rate: 1.7848644347395748e-05
Step: 27020, train/epoch: 6.430271148681641
Step: 27030, train/loss: 0.0
Step: 27030, train/grad_norm: 0.00048026457079686224
Step: 27030, train/learning_rate: 1.783674451871775e-05
Step: 27030, train/epoch: 6.432651042938232
Step: 27040, train/loss: 0.03139999881386757
Step: 27040, train/grad_norm: 0.31794458627700806
Step: 27040, train/learning_rate: 1.7824844690039754e-05
Step: 27040, train/epoch: 6.435030937194824
Step: 27050, train/loss: 0.006500000134110451
Step: 27050, train/grad_norm: 5.096181392669678
Step: 27050, train/learning_rate: 1.781294668035116e-05
Step: 27050, train/epoch: 6.437410831451416
Step: 27060, train/loss: 0.0003000000142492354
Step: 27060, train/grad_norm: 0.6082920432090759
Step: 27060, train/learning_rate: 1.7801046851673163e-05
Step: 27060, train/epoch: 6.439790725708008
Step: 27070, train/loss: 0.02199999988079071
Step: 27070, train/grad_norm: 71.20455932617188
Step: 27070, train/learning_rate: 1.778914884198457e-05
Step: 27070, train/epoch: 6.4421706199646
Step: 27080, train/loss: 0.00019999999494757503
Step: 27080, train/grad_norm: 0.005146872717887163
Step: 27080, train/learning_rate: 1.7777249013306573e-05
Step: 27080, train/epoch: 6.444550037384033
Step: 27090, train/loss: 0.0348999984562397
Step: 27090, train/grad_norm: 0.0026061772368848324
Step: 27090, train/learning_rate: 1.7765349184628576e-05
Step: 27090, train/epoch: 6.446929931640625
Step: 27100, train/loss: 0.2345000058412552
Step: 27100, train/grad_norm: 0.002929382026195526
Step: 27100, train/learning_rate: 1.7753451174939983e-05
Step: 27100, train/epoch: 6.449309825897217
Step: 27110, train/loss: 0.024800000712275505
Step: 27110, train/grad_norm: 60.89905548095703
Step: 27110, train/learning_rate: 1.7741551346261986e-05
Step: 27110, train/epoch: 6.451689720153809
Step: 27120, train/loss: 0.054499998688697815
Step: 27120, train/grad_norm: 0.11273639649152756
Step: 27120, train/learning_rate: 1.7729653336573392e-05
Step: 27120, train/epoch: 6.4540696144104
Step: 27130, train/loss: 9.999999747378752e-05
Step: 27130, train/grad_norm: 0.001374135259538889
Step: 27130, train/learning_rate: 1.7717753507895395e-05
Step: 27130, train/epoch: 6.456449508666992
Step: 27140, train/loss: 9.999999747378752e-05
Step: 27140, train/grad_norm: 0.001103285700082779
Step: 27140, train/learning_rate: 1.7705853679217398e-05
Step: 27140, train/epoch: 6.458828926086426
Step: 27150, train/loss: 0.08910000324249268
Step: 27150, train/grad_norm: 59.42053985595703
Step: 27150, train/learning_rate: 1.7693955669528805e-05
Step: 27150, train/epoch: 6.461208820343018
Step: 27160, train/loss: 0.0007999999797903001
Step: 27160, train/grad_norm: 0.008856610395014286
Step: 27160, train/learning_rate: 1.7682055840850808e-05
Step: 27160, train/epoch: 6.463588714599609
Step: 27170, train/loss: 0.01850000023841858
Step: 27170, train/grad_norm: 0.0001287842751480639
Step: 27170, train/learning_rate: 1.7670157831162214e-05
Step: 27170, train/epoch: 6.465968608856201
Step: 27180, train/loss: 0.0008999999845400453
Step: 27180, train/grad_norm: 0.0008162482408806682
Step: 27180, train/learning_rate: 1.7658258002484217e-05
Step: 27180, train/epoch: 6.468348503112793
Step: 27190, train/loss: 0.0
Step: 27190, train/grad_norm: 0.0008546752505935729
Step: 27190, train/learning_rate: 1.764635817380622e-05
Step: 27190, train/epoch: 6.470728397369385
Step: 27200, train/loss: 9.999999747378752e-05
Step: 27200, train/grad_norm: 0.33105894923210144
Step: 27200, train/learning_rate: 1.7634460164117627e-05
Step: 27200, train/epoch: 6.473107814788818
Step: 27210, train/loss: 0.005799999926239252
Step: 27210, train/grad_norm: 0.028718795627355576
Step: 27210, train/learning_rate: 1.762256033543963e-05
Step: 27210, train/epoch: 6.47548770904541
Step: 27220, train/loss: 9.999999747378752e-05
Step: 27220, train/grad_norm: 0.0001133395271608606
Step: 27220, train/learning_rate: 1.7610662325751036e-05
Step: 27220, train/epoch: 6.477867603302002
Step: 27230, train/loss: 0.07100000232458115
Step: 27230, train/grad_norm: 27.497474670410156
Step: 27230, train/learning_rate: 1.759876249707304e-05
Step: 27230, train/epoch: 6.480247497558594
Step: 27240, train/loss: 0.0035000001080334187
Step: 27240, train/grad_norm: 0.002414087764918804
Step: 27240, train/learning_rate: 1.7586862668395042e-05
Step: 27240, train/epoch: 6.4826273918151855
Step: 27250, train/loss: 0.0015999999595806003
Step: 27250, train/grad_norm: 3.7538952827453613
Step: 27250, train/learning_rate: 1.757496465870645e-05
Step: 27250, train/epoch: 6.485007286071777
Step: 27260, train/loss: 0.019899999722838402
Step: 27260, train/grad_norm: 0.0011460232781246305
Step: 27260, train/learning_rate: 1.7563064830028452e-05
Step: 27260, train/epoch: 6.487387180328369
Step: 27270, train/loss: 0.0010999999940395355
Step: 27270, train/grad_norm: 0.0003871768421959132
Step: 27270, train/learning_rate: 1.755116682033986e-05
Step: 27270, train/epoch: 6.489766597747803
Step: 27280, train/loss: 0.10540000349283218
Step: 27280, train/grad_norm: 0.021141450852155685
Step: 27280, train/learning_rate: 1.753926699166186e-05
Step: 27280, train/epoch: 6.4921464920043945
Step: 27290, train/loss: 0.00279999990016222
Step: 27290, train/grad_norm: 0.002279141219332814
Step: 27290, train/learning_rate: 1.7527367162983865e-05
Step: 27290, train/epoch: 6.494526386260986
Step: 27300, train/loss: 0.0
Step: 27300, train/grad_norm: 0.00202813814394176
Step: 27300, train/learning_rate: 1.751546915329527e-05
Step: 27300, train/epoch: 6.496906280517578
Step: 27310, train/loss: 0.00039999998989515007
Step: 27310, train/grad_norm: 1.3637385368347168
Step: 27310, train/learning_rate: 1.7503569324617274e-05
Step: 27310, train/epoch: 6.49928617477417
Step: 27320, train/loss: 0.0005000000237487257
Step: 27320, train/grad_norm: 0.003925086464732885
Step: 27320, train/learning_rate: 1.749167131492868e-05
Step: 27320, train/epoch: 6.501666069030762
Step: 27330, train/loss: 0.0032999999821186066
Step: 27330, train/grad_norm: 13.669987678527832
Step: 27330, train/learning_rate: 1.7479771486250684e-05
Step: 27330, train/epoch: 6.504045486450195
Step: 27340, train/loss: 9.999999747378752e-05
Step: 27340, train/grad_norm: 0.0010008071549236774
Step: 27340, train/learning_rate: 1.7467871657572687e-05
Step: 27340, train/epoch: 6.506425380706787
Step: 27350, train/loss: 0.04560000076889992
Step: 27350, train/grad_norm: 0.09559124708175659
Step: 27350, train/learning_rate: 1.7455973647884093e-05
Step: 27350, train/epoch: 6.508805274963379
Step: 27360, train/loss: 0.0
Step: 27360, train/grad_norm: 0.00042318928171880543
Step: 27360, train/learning_rate: 1.7444073819206096e-05
Step: 27360, train/epoch: 6.511185169219971
Step: 27370, train/loss: 0.07000000029802322
Step: 27370, train/grad_norm: 0.007803888991475105
Step: 27370, train/learning_rate: 1.7432175809517503e-05
Step: 27370, train/epoch: 6.5135650634765625
Step: 27380, train/loss: 0.03150000050663948
Step: 27380, train/grad_norm: 0.0006930496892891824
Step: 27380, train/learning_rate: 1.7420275980839506e-05
Step: 27380, train/epoch: 6.515944957733154
Step: 27390, train/loss: 0.017899999395012856
Step: 27390, train/grad_norm: 0.00044937620987184346
Step: 27390, train/learning_rate: 1.740837615216151e-05
Step: 27390, train/epoch: 6.518324375152588
Step: 27400, train/loss: 0.0478999987244606
Step: 27400, train/grad_norm: 0.005408996716141701
Step: 27400, train/learning_rate: 1.7396478142472915e-05
Step: 27400, train/epoch: 6.52070426940918
Step: 27410, train/loss: 9.999999747378752e-05
Step: 27410, train/grad_norm: 0.0024130605161190033
Step: 27410, train/learning_rate: 1.738457831379492e-05
Step: 27410, train/epoch: 6.5230841636657715
Step: 27420, train/loss: 0.010700000450015068
Step: 27420, train/grad_norm: 0.00017130386549979448
Step: 27420, train/learning_rate: 1.7372680304106325e-05
Step: 27420, train/epoch: 6.525464057922363
Step: 27430, train/loss: 0.0052999998442828655
Step: 27430, train/grad_norm: 0.0003950150276068598
Step: 27430, train/learning_rate: 1.7360780475428328e-05
Step: 27430, train/epoch: 6.527843952178955
Step: 27440, train/loss: 0.04859999939799309
Step: 27440, train/grad_norm: 0.0001463308435631916
Step: 27440, train/learning_rate: 1.734888064675033e-05
Step: 27440, train/epoch: 6.530223846435547
Step: 27450, train/loss: 0.0640999972820282
Step: 27450, train/grad_norm: 0.0011562269646674395
Step: 27450, train/learning_rate: 1.7336982637061737e-05
Step: 27450, train/epoch: 6.532603740692139
Step: 27460, train/loss: 9.999999747378752e-05
Step: 27460, train/grad_norm: 0.000816771003883332
Step: 27460, train/learning_rate: 1.732508280838374e-05
Step: 27460, train/epoch: 6.534983158111572
Step: 27470, train/loss: 0.017899999395012856
Step: 27470, train/grad_norm: 0.5617703795433044
Step: 27470, train/learning_rate: 1.7313184798695147e-05
Step: 27470, train/epoch: 6.537363052368164
Step: 27480, train/loss: 9.999999747378752e-05
Step: 27480, train/grad_norm: 0.0007149504963308573
Step: 27480, train/learning_rate: 1.730128497001715e-05
Step: 27480, train/epoch: 6.539742946624756
Step: 27490, train/loss: 0.1623000055551529
Step: 27490, train/grad_norm: 40.593544006347656
Step: 27490, train/learning_rate: 1.7289385141339153e-05
Step: 27490, train/epoch: 6.542122840881348
Step: 27500, train/loss: 0.06509999930858612
Step: 27500, train/grad_norm: 0.0010802792385220528
Step: 27500, train/learning_rate: 1.727748713165056e-05
Step: 27500, train/epoch: 6.5445027351379395
Step: 27510, train/loss: 9.999999747378752e-05
Step: 27510, train/grad_norm: 2.5546907636453398e-05
Step: 27510, train/learning_rate: 1.7265587302972563e-05
Step: 27510, train/epoch: 6.546882629394531
Step: 27520, train/loss: 0.00419999985024333
Step: 27520, train/grad_norm: 0.0033016065135598183
Step: 27520, train/learning_rate: 1.725368929328397e-05
Step: 27520, train/epoch: 6.549262046813965
Step: 27530, train/loss: 0.004999999888241291
Step: 27530, train/grad_norm: 0.006394608877599239
Step: 27530, train/learning_rate: 1.7241789464605972e-05
Step: 27530, train/epoch: 6.551641941070557
Step: 27540, train/loss: 0.025499999523162842
Step: 27540, train/grad_norm: 0.9032872319221497
Step: 27540, train/learning_rate: 1.7229889635927975e-05
Step: 27540, train/epoch: 6.554021835327148
Step: 27550, train/loss: 0.016699999570846558
Step: 27550, train/grad_norm: 0.007257514633238316
Step: 27550, train/learning_rate: 1.7217991626239382e-05
Step: 27550, train/epoch: 6.55640172958374
Step: 27560, train/loss: 0.011099999770522118
Step: 27560, train/grad_norm: 27.4102783203125
Step: 27560, train/learning_rate: 1.7206091797561385e-05
Step: 27560, train/epoch: 6.558781623840332
Step: 27570, train/loss: 0.004900000058114529
Step: 27570, train/grad_norm: 30.83589744567871
Step: 27570, train/learning_rate: 1.719419378787279e-05
Step: 27570, train/epoch: 6.561161518096924
Step: 27580, train/loss: 0.01140000019222498
Step: 27580, train/grad_norm: 3.910710074706003e-05
Step: 27580, train/learning_rate: 1.7182293959194794e-05
Step: 27580, train/epoch: 6.563540935516357
Step: 27590, train/loss: 0.20559999346733093
Step: 27590, train/grad_norm: 0.0015609514666721225
Step: 27590, train/learning_rate: 1.71703959495062e-05
Step: 27590, train/epoch: 6.565920829772949
Step: 27600, train/loss: 0.007899999618530273
Step: 27600, train/grad_norm: 3.958783781854436e-05
Step: 27600, train/learning_rate: 1.7158496120828204e-05
Step: 27600, train/epoch: 6.568300724029541
Step: 27610, train/loss: 0.024700000882148743
Step: 27610, train/grad_norm: 0.0010827522492036223
Step: 27610, train/learning_rate: 1.7146596292150207e-05
Step: 27610, train/epoch: 6.570680618286133
Step: 27620, train/loss: 0.1062999963760376
Step: 27620, train/grad_norm: 0.25721994042396545
Step: 27620, train/learning_rate: 1.7134698282461613e-05
Step: 27620, train/epoch: 6.573060512542725
Step: 27630, train/loss: 9.999999747378752e-05
Step: 27630, train/grad_norm: 0.00041373769636265934
Step: 27630, train/learning_rate: 1.7122798453783616e-05
Step: 27630, train/epoch: 6.575440406799316
Step: 27640, train/loss: 0.07900000363588333
Step: 27640, train/grad_norm: 59.55043029785156
Step: 27640, train/learning_rate: 1.7110900444095023e-05
Step: 27640, train/epoch: 6.577820301055908
Step: 27650, train/loss: 0.0
Step: 27650, train/grad_norm: 0.00137222686316818
Step: 27650, train/learning_rate: 1.7099000615417026e-05
Step: 27650, train/epoch: 6.580199718475342
Step: 27660, train/loss: 0.0
Step: 27660, train/grad_norm: 0.0022771197836846113
Step: 27660, train/learning_rate: 1.708710078673903e-05
Step: 27660, train/epoch: 6.582579612731934
Step: 27670, train/loss: 0.0
Step: 27670, train/grad_norm: 7.74734144215472e-05
Step: 27670, train/learning_rate: 1.7075202777050436e-05
Step: 27670, train/epoch: 6.584959506988525
Step: 27680, train/loss: 0.0010999999940395355
Step: 27680, train/grad_norm: 4.8437126679345965e-05
Step: 27680, train/learning_rate: 1.706330294837244e-05
Step: 27680, train/epoch: 6.587339401245117
Step: 27690, train/loss: 0.0017000000225380063
Step: 27690, train/grad_norm: 0.0004912603180855513
Step: 27690, train/learning_rate: 1.7051404938683845e-05
Step: 27690, train/epoch: 6.589719295501709
Step: 27700, train/loss: 0.04690000042319298
Step: 27700, train/grad_norm: 0.0005919499672017992
Step: 27700, train/learning_rate: 1.7039505110005848e-05
Step: 27700, train/epoch: 6.592099189758301
Step: 27710, train/loss: 0.00019999999494757503
Step: 27710, train/grad_norm: 0.0002507777826394886
Step: 27710, train/learning_rate: 1.702760528132785e-05
Step: 27710, train/epoch: 6.594478607177734
Step: 27720, train/loss: 0.002899999963119626
Step: 27720, train/grad_norm: 0.0008195497212000191
Step: 27720, train/learning_rate: 1.7015707271639258e-05
Step: 27720, train/epoch: 6.596858501434326
Step: 27730, train/loss: 0.041200000792741776
Step: 27730, train/grad_norm: 0.007078866474330425
Step: 27730, train/learning_rate: 1.700380744296126e-05
Step: 27730, train/epoch: 6.599238395690918
Step: 27740, train/loss: 0.05739999935030937
Step: 27740, train/grad_norm: 0.0176637414842844
Step: 27740, train/learning_rate: 1.6991909433272667e-05
Step: 27740, train/epoch: 6.60161828994751
Step: 27750, train/loss: 9.999999747378752e-05
Step: 27750, train/grad_norm: 0.004687677603214979
Step: 27750, train/learning_rate: 1.698000960459467e-05
Step: 27750, train/epoch: 6.603998184204102
Step: 27760, train/loss: 0.06480000168085098
Step: 27760, train/grad_norm: 0.00822238065302372
Step: 27760, train/learning_rate: 1.6968109775916673e-05
Step: 27760, train/epoch: 6.606378078460693
Step: 27770, train/loss: 0.00039999998989515007
Step: 27770, train/grad_norm: 0.03857585787773132
Step: 27770, train/learning_rate: 1.695621176622808e-05
Step: 27770, train/epoch: 6.608757972717285
Step: 27780, train/loss: 0.0034000000450760126
Step: 27780, train/grad_norm: 0.016297660768032074
Step: 27780, train/learning_rate: 1.6944311937550083e-05
Step: 27780, train/epoch: 6.611137390136719
Step: 27790, train/loss: 0.0406000018119812
Step: 27790, train/grad_norm: 0.009889768436551094
Step: 27790, train/learning_rate: 1.693241392786149e-05
Step: 27790, train/epoch: 6.6135172843933105
Step: 27800, train/loss: 0.00419999985024333
Step: 27800, train/grad_norm: 0.00027529397630132735
Step: 27800, train/learning_rate: 1.6920514099183492e-05
Step: 27800, train/epoch: 6.615897178649902
Step: 27810, train/loss: 0.0868000015616417
Step: 27810, train/grad_norm: 0.00021213761647231877
Step: 27810, train/learning_rate: 1.6908614270505495e-05
Step: 27810, train/epoch: 6.618277072906494
Step: 27820, train/loss: 0.0
Step: 27820, train/grad_norm: 0.0003826241008937359
Step: 27820, train/learning_rate: 1.6896716260816902e-05
Step: 27820, train/epoch: 6.620656967163086
Step: 27830, train/loss: 0.00019999999494757503
Step: 27830, train/grad_norm: 0.015435373410582542
Step: 27830, train/learning_rate: 1.6884816432138905e-05
Step: 27830, train/epoch: 6.623036861419678
Step: 27840, train/loss: 0.0
Step: 27840, train/grad_norm: 5.8664682001108304e-05
Step: 27840, train/learning_rate: 1.687291842245031e-05
Step: 27840, train/epoch: 6.625416278839111
Step: 27850, train/loss: 0.00039999998989515007
Step: 27850, train/grad_norm: 1.8829628229141235
Step: 27850, train/learning_rate: 1.6861018593772314e-05
Step: 27850, train/epoch: 6.627796173095703
Step: 27860, train/loss: 0.007199999876320362
Step: 27860, train/grad_norm: 0.404379278421402
Step: 27860, train/learning_rate: 1.6849118765094317e-05
Step: 27860, train/epoch: 6.630176067352295
Step: 27870, train/loss: 0.02810000069439411
Step: 27870, train/grad_norm: 0.00031461837352253497
Step: 27870, train/learning_rate: 1.6837220755405724e-05
Step: 27870, train/epoch: 6.632555961608887
Step: 27880, train/loss: 9.999999747378752e-05
Step: 27880, train/grad_norm: 0.004797881934791803
Step: 27880, train/learning_rate: 1.6825320926727727e-05
Step: 27880, train/epoch: 6.6349358558654785
Step: 27890, train/loss: 0.065700002014637
Step: 27890, train/grad_norm: 0.24384881556034088
Step: 27890, train/learning_rate: 1.6813422917039134e-05
Step: 27890, train/epoch: 6.63731575012207
Step: 27900, train/loss: 0.0
Step: 27900, train/grad_norm: 6.975931319175288e-05
Step: 27900, train/learning_rate: 1.6801523088361137e-05
Step: 27900, train/epoch: 6.639695167541504
Step: 27910, train/loss: 0.022199999541044235
Step: 27910, train/grad_norm: 0.00017825045506469905
Step: 27910, train/learning_rate: 1.678962325968314e-05
Step: 27910, train/epoch: 6.642075061798096
Step: 27920, train/loss: 0.005499999970197678
Step: 27920, train/grad_norm: 0.46453481912612915
Step: 27920, train/learning_rate: 1.6777725249994546e-05
Step: 27920, train/epoch: 6.6444549560546875
Step: 27930, train/loss: 0.019999999552965164
Step: 27930, train/grad_norm: 0.022224631160497665
Step: 27930, train/learning_rate: 1.676582542131655e-05
Step: 27930, train/epoch: 6.646834850311279
Step: 27940, train/loss: 0.010200000368058681
Step: 27940, train/grad_norm: 0.001315192086622119
Step: 27940, train/learning_rate: 1.6753927411627956e-05
Step: 27940, train/epoch: 6.649214744567871
Step: 27950, train/loss: 0.040699999779462814
Step: 27950, train/grad_norm: 4.061078652739525e-05
Step: 27950, train/learning_rate: 1.674202758294996e-05
Step: 27950, train/epoch: 6.651594638824463
Step: 27960, train/loss: 0.009499999694526196
Step: 27960, train/grad_norm: 3.6062824726104736
Step: 27960, train/learning_rate: 1.6730127754271962e-05
Step: 27960, train/epoch: 6.653974533081055
Step: 27970, train/loss: 0.08829999715089798
Step: 27970, train/grad_norm: 119.25511169433594
Step: 27970, train/learning_rate: 1.6718229744583368e-05
Step: 27970, train/epoch: 6.656353950500488
Step: 27980, train/loss: 0.05119999870657921
Step: 27980, train/grad_norm: 0.0018988486845046282
Step: 27980, train/learning_rate: 1.670632991590537e-05
Step: 27980, train/epoch: 6.65873384475708
Step: 27990, train/loss: 0.06949999928474426
Step: 27990, train/grad_norm: 0.0005241818143986166
Step: 27990, train/learning_rate: 1.6694431906216778e-05
Step: 27990, train/epoch: 6.661113739013672
Step: 28000, train/loss: 0.0066999997943639755
Step: 28000, train/grad_norm: 44.91667938232422
Step: 28000, train/learning_rate: 1.668253207753878e-05
Step: 28000, train/epoch: 6.663493633270264
Step: 28010, train/loss: 0.0
Step: 28010, train/grad_norm: 0.0005817258497700095
Step: 28010, train/learning_rate: 1.6670632248860784e-05
Step: 28010, train/epoch: 6.6658735275268555
Step: 28020, train/loss: 0.0071000000461936
Step: 28020, train/grad_norm: 25.053329467773438
Step: 28020, train/learning_rate: 1.665873423917219e-05
Step: 28020, train/epoch: 6.668253421783447
Step: 28030, train/loss: 0.06859999895095825
Step: 28030, train/grad_norm: 0.010032832622528076
Step: 28030, train/learning_rate: 1.6646834410494193e-05
Step: 28030, train/epoch: 6.670632839202881
Step: 28040, train/loss: 0.09019999951124191
Step: 28040, train/grad_norm: 0.0005303475190885365
Step: 28040, train/learning_rate: 1.66349364008056e-05
Step: 28040, train/epoch: 6.673012733459473
Step: 28050, train/loss: 0.0
Step: 28050, train/grad_norm: 0.00020693233818747103
Step: 28050, train/learning_rate: 1.6623036572127603e-05
Step: 28050, train/epoch: 6.6753926277160645
Step: 28060, train/loss: 0.029400000348687172
Step: 28060, train/grad_norm: 0.0007614499190822244
Step: 28060, train/learning_rate: 1.6611136743449606e-05
Step: 28060, train/epoch: 6.677772521972656
Step: 28070, train/loss: 9.999999747378752e-05
Step: 28070, train/grad_norm: 0.00019249068282078952
Step: 28070, train/learning_rate: 1.6599238733761013e-05
Step: 28070, train/epoch: 6.680152416229248
Step: 28080, train/loss: 0.0
Step: 28080, train/grad_norm: 0.014088823460042477
Step: 28080, train/learning_rate: 1.6587338905083016e-05
Step: 28080, train/epoch: 6.68253231048584
Step: 28090, train/loss: 0.09849999845027924
Step: 28090, train/grad_norm: 42.87211608886719
Step: 28090, train/learning_rate: 1.6575440895394422e-05
Step: 28090, train/epoch: 6.684911727905273
Step: 28100, train/loss: 0.004399999976158142
Step: 28100, train/grad_norm: 7.958681817399338e-05
Step: 28100, train/learning_rate: 1.6563541066716425e-05
Step: 28100, train/epoch: 6.687291622161865
Step: 28110, train/loss: 0.002899999963119626
Step: 28110, train/grad_norm: 0.21316957473754883
Step: 28110, train/learning_rate: 1.6551641238038428e-05
Step: 28110, train/epoch: 6.689671516418457
Step: 28120, train/loss: 0.001500000013038516
Step: 28120, train/grad_norm: 0.0016131740994751453
Step: 28120, train/learning_rate: 1.6539743228349835e-05
Step: 28120, train/epoch: 6.692051410675049
Step: 28130, train/loss: 0.07930000126361847
Step: 28130, train/grad_norm: 0.0019051687559112906
Step: 28130, train/learning_rate: 1.6527843399671838e-05
Step: 28130, train/epoch: 6.694431304931641
Step: 28140, train/loss: 0.11739999800920486
Step: 28140, train/grad_norm: 0.008184150792658329
Step: 28140, train/learning_rate: 1.6515945389983244e-05
Step: 28140, train/epoch: 6.696811199188232
Step: 28150, train/loss: 0.05090000107884407
Step: 28150, train/grad_norm: 0.0006794873042963445
Step: 28150, train/learning_rate: 1.6504045561305247e-05
Step: 28150, train/epoch: 6.699191093444824
Step: 28160, train/loss: 0.033799998462200165
Step: 28160, train/grad_norm: 14.584582328796387
Step: 28160, train/learning_rate: 1.649214573262725e-05
Step: 28160, train/epoch: 6.701570510864258
Step: 28170, train/loss: 0.002099999925121665
Step: 28170, train/grad_norm: 0.00017353946168441325
Step: 28170, train/learning_rate: 1.6480247722938657e-05
Step: 28170, train/epoch: 6.70395040512085
Step: 28180, train/loss: 0.0
Step: 28180, train/grad_norm: 0.0007797181606292725
Step: 28180, train/learning_rate: 1.646834789426066e-05
Step: 28180, train/epoch: 6.706330299377441
Step: 28190, train/loss: 0.002300000051036477
Step: 28190, train/grad_norm: 8.402538299560547
Step: 28190, train/learning_rate: 1.6456449884572066e-05
Step: 28190, train/epoch: 6.708710193634033
Step: 28200, train/loss: 0.1054999977350235
Step: 28200, train/grad_norm: 0.0018597536254674196
Step: 28200, train/learning_rate: 1.644455005589407e-05
Step: 28200, train/epoch: 6.711090087890625
Step: 28210, train/loss: 0.0008999999845400453
Step: 28210, train/grad_norm: 0.0010632779449224472
Step: 28210, train/learning_rate: 1.6432650227216072e-05
Step: 28210, train/epoch: 6.713469982147217
Step: 28220, train/loss: 0.00019999999494757503
Step: 28220, train/grad_norm: 0.0006425966857932508
Step: 28220, train/learning_rate: 1.642075221752748e-05
Step: 28220, train/epoch: 6.71584939956665
Step: 28230, train/loss: 0.07259999960660934
Step: 28230, train/grad_norm: 4.1621704440331087e-05
Step: 28230, train/learning_rate: 1.6408852388849482e-05
Step: 28230, train/epoch: 6.718229293823242
Step: 28240, train/loss: 0.00019999999494757503
Step: 28240, train/grad_norm: 0.0011486849980428815
Step: 28240, train/learning_rate: 1.639695437916089e-05
Step: 28240, train/epoch: 6.720609188079834
Step: 28250, train/loss: 0.0003000000142492354
Step: 28250, train/grad_norm: 0.0011738360626623034
Step: 28250, train/learning_rate: 1.638505455048289e-05
Step: 28250, train/epoch: 6.722989082336426
Step: 28260, train/loss: 0.0003000000142492354
Step: 28260, train/grad_norm: 0.0007626254227943718
Step: 28260, train/learning_rate: 1.6373156540794298e-05
Step: 28260, train/epoch: 6.725368976593018
Step: 28270, train/loss: 0.12110000103712082
Step: 28270, train/grad_norm: 0.0013539791107177734
Step: 28270, train/learning_rate: 1.63612567121163e-05
Step: 28270, train/epoch: 6.727748870849609
Step: 28280, train/loss: 0.00279999990016222
Step: 28280, train/grad_norm: 0.005427185446023941
Step: 28280, train/learning_rate: 1.6349356883438304e-05
Step: 28280, train/epoch: 6.730128288269043
Step: 28290, train/loss: 0.0
Step: 28290, train/grad_norm: 0.0006157737225294113
Step: 28290, train/learning_rate: 1.633745887374971e-05
Step: 28290, train/epoch: 6.732508182525635
Step: 28300, train/loss: 0.04780000075697899
Step: 28300, train/grad_norm: 0.0004391054098960012
Step: 28300, train/learning_rate: 1.6325559045071714e-05
Step: 28300, train/epoch: 6.734888076782227
Step: 28310, train/loss: 0.05889999866485596
Step: 28310, train/grad_norm: 2.8119224225520156e-05
Step: 28310, train/learning_rate: 1.631366103538312e-05
Step: 28310, train/epoch: 6.737267971038818
Step: 28320, train/loss: 0.24789999425411224
Step: 28320, train/grad_norm: 0.0037570262793451548
Step: 28320, train/learning_rate: 1.6301761206705123e-05
Step: 28320, train/epoch: 6.73964786529541
Step: 28330, train/loss: 0.13729999959468842
Step: 28330, train/grad_norm: 0.04941004887223244
Step: 28330, train/learning_rate: 1.6289861378027126e-05
Step: 28330, train/epoch: 6.742027759552002
Step: 28340, train/loss: 0.06080000102519989
Step: 28340, train/grad_norm: 0.0005630686646327376
Step: 28340, train/learning_rate: 1.6277963368338533e-05
Step: 28340, train/epoch: 6.744407653808594
Step: 28350, train/loss: 9.999999747378752e-05
Step: 28350, train/grad_norm: 5.507794412551448e-05
Step: 28350, train/learning_rate: 1.6266063539660536e-05
Step: 28350, train/epoch: 6.746787071228027
Step: 28360, train/loss: 0.1729000061750412
Step: 28360, train/grad_norm: 47.23675537109375
Step: 28360, train/learning_rate: 1.6254165529971942e-05
Step: 28360, train/epoch: 6.749166965484619
Step: 28370, train/loss: 0.000699999975040555
Step: 28370, train/grad_norm: 0.01059657335281372
Step: 28370, train/learning_rate: 1.6242265701293945e-05
Step: 28370, train/epoch: 6.751546859741211
Step: 28380, train/loss: 0.00800000037997961
Step: 28380, train/grad_norm: 0.0019141831435263157
Step: 28380, train/learning_rate: 1.623036587261595e-05
Step: 28380, train/epoch: 6.753926753997803
Step: 28390, train/loss: 0.027699999511241913
Step: 28390, train/grad_norm: 19.116943359375
Step: 28390, train/learning_rate: 1.6218467862927355e-05
Step: 28390, train/epoch: 6.7563066482543945
Step: 28400, train/loss: 9.999999747378752e-05
Step: 28400, train/grad_norm: 0.0013673772336915135
Step: 28400, train/learning_rate: 1.6206568034249358e-05
Step: 28400, train/epoch: 6.758686542510986
Step: 28410, train/loss: 0.13779999315738678
Step: 28410, train/grad_norm: 0.0036688605323433876
Step: 28410, train/learning_rate: 1.6194670024560764e-05
Step: 28410, train/epoch: 6.76106595993042
Step: 28420, train/loss: 0.16050000488758087
Step: 28420, train/grad_norm: 0.00042060771374963224
Step: 28420, train/learning_rate: 1.6182770195882767e-05
Step: 28420, train/epoch: 6.763445854187012
Step: 28430, train/loss: 0.0052999998442828655
Step: 28430, train/grad_norm: 0.0022962254006415606
Step: 28430, train/learning_rate: 1.617087036720477e-05
Step: 28430, train/epoch: 6.7658257484436035
Step: 28440, train/loss: 0.025200000032782555
Step: 28440, train/grad_norm: 0.0009897201089188457
Step: 28440, train/learning_rate: 1.6158972357516177e-05
Step: 28440, train/epoch: 6.768205642700195
Step: 28450, train/loss: 0.06769999861717224
Step: 28450, train/grad_norm: 4.723511695861816
Step: 28450, train/learning_rate: 1.614707252883818e-05
Step: 28450, train/epoch: 6.770585536956787
Step: 28460, train/loss: 0.02370000071823597
Step: 28460, train/grad_norm: 0.001242009224370122
Step: 28460, train/learning_rate: 1.6135174519149587e-05
Step: 28460, train/epoch: 6.772965431213379
Step: 28470, train/loss: 0.10660000145435333
Step: 28470, train/grad_norm: 0.6682685017585754
Step: 28470, train/learning_rate: 1.612327469047159e-05
Step: 28470, train/epoch: 6.7753448486328125
Step: 28480, train/loss: 0.00019999999494757503
Step: 28480, train/grad_norm: 0.005450150463730097
Step: 28480, train/learning_rate: 1.6111374861793593e-05
Step: 28480, train/epoch: 6.777724742889404
Step: 28490, train/loss: 0.006000000052154064
Step: 28490, train/grad_norm: 0.5288255214691162
Step: 28490, train/learning_rate: 1.6099476852105e-05
Step: 28490, train/epoch: 6.780104637145996
Step: 28500, train/loss: 0.00019999999494757503
Step: 28500, train/grad_norm: 0.00821754802018404
Step: 28500, train/learning_rate: 1.6087577023427002e-05
Step: 28500, train/epoch: 6.782484531402588
Step: 28510, train/loss: 0.00039999998989515007
Step: 28510, train/grad_norm: 0.24073024094104767
Step: 28510, train/learning_rate: 1.607567901373841e-05
Step: 28510, train/epoch: 6.78486442565918
Step: 28520, train/loss: 0.004600000102072954
Step: 28520, train/grad_norm: 9.048505783081055
Step: 28520, train/learning_rate: 1.606377918506041e-05
Step: 28520, train/epoch: 6.7872443199157715
Step: 28530, train/loss: 0.0003000000142492354
Step: 28530, train/grad_norm: 0.005489235278218985
Step: 28530, train/learning_rate: 1.6051879356382415e-05
Step: 28530, train/epoch: 6.789624214172363
Step: 28540, train/loss: 0.00430000014603138
Step: 28540, train/grad_norm: 24.550844192504883
Step: 28540, train/learning_rate: 1.603998134669382e-05
Step: 28540, train/epoch: 6.792003631591797
Step: 28550, train/loss: 0.007799999788403511
Step: 28550, train/grad_norm: 0.0004910121206194162
Step: 28550, train/learning_rate: 1.6028081518015824e-05
Step: 28550, train/epoch: 6.794383525848389
Step: 28560, train/loss: 0.0
Step: 28560, train/grad_norm: 0.0037842667661607265
Step: 28560, train/learning_rate: 1.601618350832723e-05
Step: 28560, train/epoch: 6.7967634201049805
Step: 28570, train/loss: 0.07479999959468842
Step: 28570, train/grad_norm: 1.2379698753356934
Step: 28570, train/learning_rate: 1.6004283679649234e-05
Step: 28570, train/epoch: 6.799143314361572
Step: 28580, train/loss: 0.15559999644756317
Step: 28580, train/grad_norm: 14.825439453125
Step: 28580, train/learning_rate: 1.5992383850971237e-05
Step: 28580, train/epoch: 6.801523208618164
Step: 28590, train/loss: 0.00279999990016222
Step: 28590, train/grad_norm: 0.09260454028844833
Step: 28590, train/learning_rate: 1.5980485841282643e-05
Step: 28590, train/epoch: 6.803903102874756
Step: 28600, train/loss: 0.0210999995470047
Step: 28600, train/grad_norm: 0.004316005855798721
Step: 28600, train/learning_rate: 1.5968586012604646e-05
Step: 28600, train/epoch: 6.8062825202941895
Step: 28610, train/loss: 0.003599999938160181
Step: 28610, train/grad_norm: 14.538996696472168
Step: 28610, train/learning_rate: 1.5956688002916053e-05
Step: 28610, train/epoch: 6.808662414550781
Step: 28620, train/loss: 0.000699999975040555
Step: 28620, train/grad_norm: 0.007235170807689428
Step: 28620, train/learning_rate: 1.5944788174238056e-05
Step: 28620, train/epoch: 6.811042308807373
Step: 28630, train/loss: 0.01080000028014183
Step: 28630, train/grad_norm: 0.07109450548887253
Step: 28630, train/learning_rate: 1.593288834556006e-05
Step: 28630, train/epoch: 6.813422203063965
Step: 28640, train/loss: 0.0
Step: 28640, train/grad_norm: 0.0002560808206908405
Step: 28640, train/learning_rate: 1.5920990335871466e-05
Step: 28640, train/epoch: 6.815802097320557
Step: 28650, train/loss: 0.04039999842643738
Step: 28650, train/grad_norm: 7.91277561802417e-05
Step: 28650, train/learning_rate: 1.590909050719347e-05
Step: 28650, train/epoch: 6.818181991577148
Step: 28660, train/loss: 0.08030000329017639
Step: 28660, train/grad_norm: 0.19838601350784302
Step: 28660, train/learning_rate: 1.5897192497504875e-05
Step: 28660, train/epoch: 6.820561408996582
Step: 28670, train/loss: 0.0007999999797903001
Step: 28670, train/grad_norm: 0.004344442393630743
Step: 28670, train/learning_rate: 1.5885292668826878e-05
Step: 28670, train/epoch: 6.822941303253174
Step: 28680, train/loss: 0.09600000083446503
Step: 28680, train/grad_norm: 26.48008155822754
Step: 28680, train/learning_rate: 1.587339284014888e-05
Step: 28680, train/epoch: 6.825321197509766
Step: 28690, train/loss: 0.006599999964237213
Step: 28690, train/grad_norm: 0.0023276356514543295
Step: 28690, train/learning_rate: 1.5861494830460288e-05
Step: 28690, train/epoch: 6.827701091766357
Step: 28700, train/loss: 0.00039999998989515007
Step: 28700, train/grad_norm: 0.0025712032802402973
Step: 28700, train/learning_rate: 1.584959500178229e-05
Step: 28700, train/epoch: 6.830080986022949
Step: 28710, train/loss: 0.0003000000142492354
Step: 28710, train/grad_norm: 0.0007374764536507428
Step: 28710, train/learning_rate: 1.5837696992093697e-05
Step: 28710, train/epoch: 6.832460880279541
Step: 28720, train/loss: 0.005100000184029341
Step: 28720, train/grad_norm: 0.026574978604912758
Step: 28720, train/learning_rate: 1.58257971634157e-05
Step: 28720, train/epoch: 6.834840774536133
Step: 28730, train/loss: 0.0027000000700354576
Step: 28730, train/grad_norm: 0.0002810306614264846
Step: 28730, train/learning_rate: 1.5813897334737703e-05
Step: 28730, train/epoch: 6.837220191955566
Step: 28740, train/loss: 0.09600000083446503
Step: 28740, train/grad_norm: 0.01188051886856556
Step: 28740, train/learning_rate: 1.580199932504911e-05
Step: 28740, train/epoch: 6.839600086212158
Step: 28750, train/loss: 9.999999747378752e-05
Step: 28750, train/grad_norm: 0.14632880687713623
Step: 28750, train/learning_rate: 1.5790099496371113e-05
Step: 28750, train/epoch: 6.84197998046875
Step: 28760, train/loss: 0.002199999988079071
Step: 28760, train/grad_norm: 1.9043866395950317
Step: 28760, train/learning_rate: 1.577820148668252e-05
Step: 28760, train/epoch: 6.844359874725342
Step: 28770, train/loss: 0.019700000062584877
Step: 28770, train/grad_norm: 0.09681038558483124
Step: 28770, train/learning_rate: 1.5766301658004522e-05
Step: 28770, train/epoch: 6.846739768981934
Step: 28780, train/loss: 0.009100000374019146
Step: 28780, train/grad_norm: 0.12380862981081009
Step: 28780, train/learning_rate: 1.5754401829326525e-05
Step: 28780, train/epoch: 6.849119663238525
Step: 28790, train/loss: 0.008299999870359898
Step: 28790, train/grad_norm: 5.581226348876953
Step: 28790, train/learning_rate: 1.5742503819637932e-05
Step: 28790, train/epoch: 6.851499080657959
Step: 28800, train/loss: 0.023000000044703484
Step: 28800, train/grad_norm: 0.00035838852636516094
Step: 28800, train/learning_rate: 1.5730603990959935e-05
Step: 28800, train/epoch: 6.853878974914551
Step: 28810, train/loss: 0.0017000000225380063
Step: 28810, train/grad_norm: 0.2915887236595154
Step: 28810, train/learning_rate: 1.571870598127134e-05
Step: 28810, train/epoch: 6.856258869171143
Step: 28820, train/loss: 0.0005000000237487257
Step: 28820, train/grad_norm: 0.000516471394803375
Step: 28820, train/learning_rate: 1.5706806152593344e-05
Step: 28820, train/epoch: 6.858638763427734
Step: 28830, train/loss: 0.00019999999494757503
Step: 28830, train/grad_norm: 0.00033158709993585944
Step: 28830, train/learning_rate: 1.5694906323915347e-05
Step: 28830, train/epoch: 6.861018657684326
Step: 28840, train/loss: 0.007699999958276749
Step: 28840, train/grad_norm: 7.193620695034042e-05
Step: 28840, train/learning_rate: 1.5683008314226754e-05
Step: 28840, train/epoch: 6.863398551940918
Step: 28850, train/loss: 0.061400000005960464
Step: 28850, train/grad_norm: 47.01601028442383
Step: 28850, train/learning_rate: 1.5671108485548757e-05
Step: 28850, train/epoch: 6.865777969360352
Step: 28860, train/loss: 0.036400001496076584
Step: 28860, train/grad_norm: 0.00010499706695554778
Step: 28860, train/learning_rate: 1.5659210475860164e-05
Step: 28860, train/epoch: 6.868157863616943
Step: 28870, train/loss: 0.09220000356435776
Step: 28870, train/grad_norm: 0.00026715241256169975
Step: 28870, train/learning_rate: 1.5647310647182167e-05
Step: 28870, train/epoch: 6.870537757873535
Step: 28880, train/loss: 0.0
Step: 28880, train/grad_norm: 0.00048232756671495736
Step: 28880, train/learning_rate: 1.563541081850417e-05
Step: 28880, train/epoch: 6.872917652130127
Step: 28890, train/loss: 0.02800000086426735
Step: 28890, train/grad_norm: 80.1994400024414
Step: 28890, train/learning_rate: 1.5623512808815576e-05
Step: 28890, train/epoch: 6.875297546386719
Step: 28900, train/loss: 9.999999747378752e-05
Step: 28900, train/grad_norm: 0.006215355359017849
Step: 28900, train/learning_rate: 1.561161298013758e-05
Step: 28900, train/epoch: 6.8776774406433105
Step: 28910, train/loss: 0.00019999999494757503
Step: 28910, train/grad_norm: 0.02854299731552601
Step: 28910, train/learning_rate: 1.5599714970448986e-05
Step: 28910, train/epoch: 6.880057334899902
Step: 28920, train/loss: 9.999999747378752e-05
Step: 28920, train/grad_norm: 0.007195677142590284
Step: 28920, train/learning_rate: 1.558781514177099e-05
Step: 28920, train/epoch: 6.882436752319336
Step: 28930, train/loss: 0.0892999991774559
Step: 28930, train/grad_norm: 0.22707457840442657
Step: 28930, train/learning_rate: 1.5575917132082395e-05
Step: 28930, train/epoch: 6.884816646575928
Step: 28940, train/loss: 0.011099999770522118
Step: 28940, train/grad_norm: 0.2175132781267166
Step: 28940, train/learning_rate: 1.5564017303404398e-05
Step: 28940, train/epoch: 6.8871965408325195
Step: 28950, train/loss: 0.008999999612569809
Step: 28950, train/grad_norm: 0.003512596944347024
Step: 28950, train/learning_rate: 1.55521174747264e-05
Step: 28950, train/epoch: 6.889576435089111
Step: 28960, train/loss: 0.042500000447034836
Step: 28960, train/grad_norm: 0.007026043254882097
Step: 28960, train/learning_rate: 1.5540219465037808e-05
Step: 28960, train/epoch: 6.891956329345703
Step: 28970, train/loss: 0.09929999709129333
Step: 28970, train/grad_norm: 0.0008026764844544232
Step: 28970, train/learning_rate: 1.552831963635981e-05
Step: 28970, train/epoch: 6.894336223602295
Step: 28980, train/loss: 0.01759999990463257
Step: 28980, train/grad_norm: 5.597151175606996e-05
Step: 28980, train/learning_rate: 1.5516421626671217e-05
Step: 28980, train/epoch: 6.8967156410217285
Step: 28990, train/loss: 9.999999747378752e-05
Step: 28990, train/grad_norm: 0.39748263359069824
Step: 28990, train/learning_rate: 1.550452179799322e-05
Step: 28990, train/epoch: 6.89909553527832
Step: 29000, train/loss: 0.09839999675750732
Step: 29000, train/grad_norm: 0.005917715840041637
Step: 29000, train/learning_rate: 1.5492621969315223e-05
Step: 29000, train/epoch: 6.901475429534912
Step: 29010, train/loss: 0.08030000329017639
Step: 29010, train/grad_norm: 0.009414019994437695
Step: 29010, train/learning_rate: 1.548072395962663e-05
Step: 29010, train/epoch: 6.903855323791504
Step: 29020, train/loss: 0.14069999754428864
Step: 29020, train/grad_norm: 3.7483561038970947
Step: 29020, train/learning_rate: 1.5468824130948633e-05
Step: 29020, train/epoch: 6.906235218048096
Step: 29030, train/loss: 0.02590000070631504
Step: 29030, train/grad_norm: 0.0011102736461907625
Step: 29030, train/learning_rate: 1.545692612126004e-05
Step: 29030, train/epoch: 6.9086151123046875
Step: 29040, train/loss: 0.021700000390410423
Step: 29040, train/grad_norm: 0.010422496125102043
Step: 29040, train/learning_rate: 1.5445026292582043e-05
Step: 29040, train/epoch: 6.910994529724121
Step: 29050, train/loss: 0.008799999952316284
Step: 29050, train/grad_norm: 0.003107695607468486
Step: 29050, train/learning_rate: 1.5433126463904046e-05
Step: 29050, train/epoch: 6.913374423980713
Step: 29060, train/loss: 0.10719999670982361
Step: 29060, train/grad_norm: 0.04802922531962395
Step: 29060, train/learning_rate: 1.5421228454215452e-05
Step: 29060, train/epoch: 6.915754318237305
Step: 29070, train/loss: 0.10660000145435333
Step: 29070, train/grad_norm: 0.0031658634543418884
Step: 29070, train/learning_rate: 1.5409328625537455e-05
Step: 29070, train/epoch: 6.9181342124938965
Step: 29080, train/loss: 0.06459999829530716
Step: 29080, train/grad_norm: 0.0019262024434283376
Step: 29080, train/learning_rate: 1.539743061584886e-05
Step: 29080, train/epoch: 6.920514106750488
Step: 29090, train/loss: 0.07829999923706055
Step: 29090, train/grad_norm: 41.16252136230469
Step: 29090, train/learning_rate: 1.5385530787170865e-05
Step: 29090, train/epoch: 6.92289400100708
Step: 29100, train/loss: 0.0012000000569969416
Step: 29100, train/grad_norm: 0.00017724663484841585
Step: 29100, train/learning_rate: 1.5373630958492868e-05
Step: 29100, train/epoch: 6.925273895263672
Step: 29110, train/loss: 0.11379999667406082
Step: 29110, train/grad_norm: 0.006178998854011297
Step: 29110, train/learning_rate: 1.5361732948804274e-05
Step: 29110, train/epoch: 6.9276533126831055
Step: 29120, train/loss: 0.00019999999494757503
Step: 29120, train/grad_norm: 0.0005962594295851886
Step: 29120, train/learning_rate: 1.5349833120126277e-05
Step: 29120, train/epoch: 6.930033206939697
Step: 29130, train/loss: 0.11640000343322754
Step: 29130, train/grad_norm: 0.00022956178872846067
Step: 29130, train/learning_rate: 1.5337935110437684e-05
Step: 29130, train/epoch: 6.932413101196289
Step: 29140, train/loss: 0.01810000091791153
Step: 29140, train/grad_norm: 0.019171971827745438
Step: 29140, train/learning_rate: 1.5326035281759687e-05
Step: 29140, train/epoch: 6.934792995452881
Step: 29150, train/loss: 0.0015999999595806003
Step: 29150, train/grad_norm: 0.0034628906287252903
Step: 29150, train/learning_rate: 1.531413545308169e-05
Step: 29150, train/epoch: 6.937172889709473
Step: 29160, train/loss: 0.10939999669790268
Step: 29160, train/grad_norm: 22.86726951599121
Step: 29160, train/learning_rate: 1.5302237443393096e-05
Step: 29160, train/epoch: 6.9395527839660645
Step: 29170, train/loss: 0.025200000032782555
Step: 29170, train/grad_norm: 61.222511291503906
Step: 29170, train/learning_rate: 1.52903376147151e-05
Step: 29170, train/epoch: 6.941932201385498
Step: 29180, train/loss: 0.00019999999494757503
Step: 29180, train/grad_norm: 0.005663818214088678
Step: 29180, train/learning_rate: 1.5278439605026506e-05
Step: 29180, train/epoch: 6.94431209564209
Step: 29190, train/loss: 0.0006000000284984708
Step: 29190, train/grad_norm: 0.0007119697984308004
Step: 29190, train/learning_rate: 1.526653977634851e-05
Step: 29190, train/epoch: 6.946691989898682
Step: 29200, train/loss: 0.12839999794960022
Step: 29200, train/grad_norm: 26.721479415893555
Step: 29200, train/learning_rate: 1.5254640857165214e-05
Step: 29200, train/epoch: 6.949071884155273
Step: 29210, train/loss: 0.03689999878406525
Step: 29210, train/grad_norm: 50.16134262084961
Step: 29210, train/learning_rate: 1.5242741937981918e-05
Step: 29210, train/epoch: 6.951451778411865
Step: 29220, train/loss: 0.00570000009611249
Step: 29220, train/grad_norm: 0.19200439751148224
Step: 29220, train/learning_rate: 1.5230842109303921e-05
Step: 29220, train/epoch: 6.953831672668457
Step: 29230, train/loss: 0.00019999999494757503
Step: 29230, train/grad_norm: 0.010327771306037903
Step: 29230, train/learning_rate: 1.5218943190120626e-05
Step: 29230, train/epoch: 6.956211090087891
Step: 29240, train/loss: 0.00019999999494757503
Step: 29240, train/grad_norm: 0.025614969432353973
Step: 29240, train/learning_rate: 1.5207044270937331e-05
Step: 29240, train/epoch: 6.958590984344482
Step: 29250, train/loss: 0.00139999995008111
Step: 29250, train/grad_norm: 0.2288568764925003
Step: 29250, train/learning_rate: 1.5195145351754036e-05
Step: 29250, train/epoch: 6.960970878601074
Step: 29260, train/loss: 0.00139999995008111
Step: 29260, train/grad_norm: 1.4052865505218506
Step: 29260, train/learning_rate: 1.518324643257074e-05
Step: 29260, train/epoch: 6.963350772857666
Step: 29270, train/loss: 0.00419999985024333
Step: 29270, train/grad_norm: 0.00939163938164711
Step: 29270, train/learning_rate: 1.5171346603892744e-05
Step: 29270, train/epoch: 6.965730667114258
Step: 29280, train/loss: 0.0
Step: 29280, train/grad_norm: 0.00046953922719694674
Step: 29280, train/learning_rate: 1.5159447684709448e-05
Step: 29280, train/epoch: 6.96811056137085
Step: 29290, train/loss: 0.1039000004529953
Step: 29290, train/grad_norm: 35.07982635498047
Step: 29290, train/learning_rate: 1.5147548765526153e-05
Step: 29290, train/epoch: 6.970490455627441
Step: 29300, train/loss: 0.2240000069141388
Step: 29300, train/grad_norm: 82.1680908203125
Step: 29300, train/learning_rate: 1.5135649846342858e-05
Step: 29300, train/epoch: 6.972869873046875
Step: 29310, train/loss: 0.16259999573230743
Step: 29310, train/grad_norm: 0.003674976760521531
Step: 29310, train/learning_rate: 1.5123750927159563e-05
Step: 29310, train/epoch: 6.975249767303467
Step: 29320, train/loss: 0.04969999939203262
Step: 29320, train/grad_norm: 0.0034404941834509373
Step: 29320, train/learning_rate: 1.5111851098481566e-05
Step: 29320, train/epoch: 6.977629661560059
Step: 29330, train/loss: 0.0017000000225380063
Step: 29330, train/grad_norm: 0.18335138261318207
Step: 29330, train/learning_rate: 1.509995217929827e-05
Step: 29330, train/epoch: 6.98000955581665
Step: 29340, train/loss: 0.0017000000225380063
Step: 29340, train/grad_norm: 0.013040498830378056
Step: 29340, train/learning_rate: 1.5088053260114975e-05
Step: 29340, train/epoch: 6.982389450073242
Step: 29350, train/loss: 0.03189999982714653
Step: 29350, train/grad_norm: 0.0299738347530365
Step: 29350, train/learning_rate: 1.507615434093168e-05
Step: 29350, train/epoch: 6.984769344329834
Step: 29360, train/loss: 0.00570000009611249
Step: 29360, train/grad_norm: 0.004001641180366278
Step: 29360, train/learning_rate: 1.5064255421748385e-05
Step: 29360, train/epoch: 6.987148761749268
Step: 29370, train/loss: 0.07819999754428864
Step: 29370, train/grad_norm: 0.00010901426139753312
Step: 29370, train/learning_rate: 1.5052355593070388e-05
Step: 29370, train/epoch: 6.989528656005859
Step: 29380, train/loss: 0.03359999880194664
Step: 29380, train/grad_norm: 49.565155029296875
Step: 29380, train/learning_rate: 1.5040456673887093e-05
Step: 29380, train/epoch: 6.991908550262451
Step: 29390, train/loss: 0.00039999998989515007
Step: 29390, train/grad_norm: 0.07898872345685959
Step: 29390, train/learning_rate: 1.5028557754703797e-05
Step: 29390, train/epoch: 6.994288444519043
Step: 29400, train/loss: 0.03999999910593033
Step: 29400, train/grad_norm: 0.0012516004499047995
Step: 29400, train/learning_rate: 1.5016658835520502e-05
Step: 29400, train/epoch: 6.996668338775635
Step: 29410, train/loss: 0.06669999659061432
Step: 29410, train/grad_norm: 0.006309388671070337
Step: 29410, train/learning_rate: 1.5004759916337207e-05
Step: 29410, train/epoch: 6.999048233032227
Step: 29414, eval/loss: 0.32692834734916687
Step: 29414, eval/accuracy: 0.9511314630508423
Step: 29414, eval/f1: 0.9479720592498779
Step: 29414, eval/runtime: 295.99090576171875
Step: 29414, eval/samples_per_second: 24.334999084472656
Step: 29414, eval/steps_per_second: 3.0439999103546143
Step: 29414, train/epoch: 7.0
Step: 29420, train/loss: 0.09179999679327011
Step: 29420, train/grad_norm: 0.0010280427522957325
Step: 29420, train/learning_rate: 1.4992860997153912e-05
Step: 29420, train/epoch: 7.001428127288818
Step: 29430, train/loss: 0.02800000086426735
Step: 29430, train/grad_norm: 0.27549245953559875
Step: 29430, train/learning_rate: 1.4980961168475915e-05
Step: 29430, train/epoch: 7.003807544708252
Step: 29440, train/loss: 9.999999747378752e-05
Step: 29440, train/grad_norm: 0.003937785979360342
Step: 29440, train/learning_rate: 1.496906224929262e-05
Step: 29440, train/epoch: 7.006187438964844
Step: 29450, train/loss: 0.004699999932199717
Step: 29450, train/grad_norm: 0.007487618830054998
Step: 29450, train/learning_rate: 1.4957163330109324e-05
Step: 29450, train/epoch: 7.0085673332214355
Step: 29460, train/loss: 0.10000000149011612
Step: 29460, train/grad_norm: 0.3702155649662018
Step: 29460, train/learning_rate: 1.4945264410926029e-05
Step: 29460, train/epoch: 7.010947227478027
Step: 29470, train/loss: 0.0015999999595806003
Step: 29470, train/grad_norm: 0.4112832844257355
Step: 29470, train/learning_rate: 1.4933365491742734e-05
Step: 29470, train/epoch: 7.013327121734619
Step: 29480, train/loss: 0.0003000000142492354
Step: 29480, train/grad_norm: 0.027742832899093628
Step: 29480, train/learning_rate: 1.4921465663064737e-05
Step: 29480, train/epoch: 7.015707015991211
Step: 29490, train/loss: 0.0005000000237487257
Step: 29490, train/grad_norm: 9.022486483445391e-05
Step: 29490, train/learning_rate: 1.4909566743881442e-05
Step: 29490, train/epoch: 7.0180864334106445
Step: 29500, train/loss: 0.02329999953508377
Step: 29500, train/grad_norm: 0.0003771152114495635
Step: 29500, train/learning_rate: 1.4897667824698146e-05
Step: 29500, train/epoch: 7.020466327667236
Step: 29510, train/loss: 0.0
Step: 29510, train/grad_norm: 0.007221616338938475
Step: 29510, train/learning_rate: 1.4885768905514851e-05
Step: 29510, train/epoch: 7.022846221923828
Step: 29520, train/loss: 9.999999747378752e-05
Step: 29520, train/grad_norm: 0.00037266986328177154
Step: 29520, train/learning_rate: 1.4873869986331556e-05
Step: 29520, train/epoch: 7.02522611618042
Step: 29530, train/loss: 0.003000000026077032
Step: 29530, train/grad_norm: 8.790680885314941
Step: 29530, train/learning_rate: 1.4861970157653559e-05
Step: 29530, train/epoch: 7.027606010437012
Step: 29540, train/loss: 0.0
Step: 29540, train/grad_norm: 0.00026398777845315635
Step: 29540, train/learning_rate: 1.4850071238470264e-05
Step: 29540, train/epoch: 7.0299859046936035
Step: 29550, train/loss: 0.0003000000142492354
Step: 29550, train/grad_norm: 0.0027660843916237354
Step: 29550, train/learning_rate: 1.4838172319286969e-05
Step: 29550, train/epoch: 7.032365322113037
Step: 29560, train/loss: 0.000699999975040555
Step: 29560, train/grad_norm: 0.0004051627474837005
Step: 29560, train/learning_rate: 1.4826273400103673e-05
Step: 29560, train/epoch: 7.034745216369629
Step: 29570, train/loss: 0.027400000020861626
Step: 29570, train/grad_norm: 0.0033214325085282326
Step: 29570, train/learning_rate: 1.4814374480920378e-05
Step: 29570, train/epoch: 7.037125110626221
Step: 29580, train/loss: 9.999999747378752e-05
Step: 29580, train/grad_norm: 0.030703535303473473
Step: 29580, train/learning_rate: 1.4802474652242381e-05
Step: 29580, train/epoch: 7.0395050048828125
Step: 29590, train/loss: 0.003700000001117587
Step: 29590, train/grad_norm: 21.450119018554688
Step: 29590, train/learning_rate: 1.4790575733059086e-05
Step: 29590, train/epoch: 7.041884899139404
Step: 29600, train/loss: 0.0357000008225441
Step: 29600, train/grad_norm: 0.011319753713905811
Step: 29600, train/learning_rate: 1.477867681387579e-05
Step: 29600, train/epoch: 7.044264793395996
Step: 29610, train/loss: 0.03139999881386757
Step: 29610, train/grad_norm: 56.201961517333984
Step: 29610, train/learning_rate: 1.4766777894692495e-05
Step: 29610, train/epoch: 7.046644687652588
Step: 29620, train/loss: 0.0
Step: 29620, train/grad_norm: 0.0009748211596161127
Step: 29620, train/learning_rate: 1.47548789755092e-05
Step: 29620, train/epoch: 7.0490241050720215
Step: 29630, train/loss: 0.07109999656677246
Step: 29630, train/grad_norm: 0.0008011606405489147
Step: 29630, train/learning_rate: 1.4742979146831203e-05
Step: 29630, train/epoch: 7.051403999328613
Step: 29640, train/loss: 0.0013000000035390258
Step: 29640, train/grad_norm: 0.0015158699825406075
Step: 29640, train/learning_rate: 1.4731080227647908e-05
Step: 29640, train/epoch: 7.053783893585205
Step: 29650, train/loss: 0.002300000051036477
Step: 29650, train/grad_norm: 0.0014420473016798496
Step: 29650, train/learning_rate: 1.4719181308464613e-05
Step: 29650, train/epoch: 7.056163787841797
Step: 29660, train/loss: 0.00039999998989515007
Step: 29660, train/grad_norm: 0.0073477597907185555
Step: 29660, train/learning_rate: 1.4707282389281318e-05
Step: 29660, train/epoch: 7.058543682098389
Step: 29670, train/loss: 0.002199999988079071
Step: 29670, train/grad_norm: 0.0007717471453361213
Step: 29670, train/learning_rate: 1.4695383470098022e-05
Step: 29670, train/epoch: 7.0609235763549805
Step: 29680, train/loss: 0.051600001752376556
Step: 29680, train/grad_norm: 7.610843022121117e-05
Step: 29680, train/learning_rate: 1.4683483641420025e-05
Step: 29680, train/epoch: 7.063302993774414
Step: 29690, train/loss: 0.0
Step: 29690, train/grad_norm: 0.005497116129845381
Step: 29690, train/learning_rate: 1.467158472223673e-05
Step: 29690, train/epoch: 7.065682888031006
Step: 29700, train/loss: 0.0
Step: 29700, train/grad_norm: 0.000841332774143666
Step: 29700, train/learning_rate: 1.4659685803053435e-05
Step: 29700, train/epoch: 7.068062782287598
Step: 29710, train/loss: 0.0
Step: 29710, train/grad_norm: 0.0005025176214985549
Step: 29710, train/learning_rate: 1.464778688387014e-05
Step: 29710, train/epoch: 7.0704426765441895
Step: 29720, train/loss: 0.0010000000474974513
Step: 29720, train/grad_norm: 0.000723133038263768
Step: 29720, train/learning_rate: 1.4635887964686844e-05
Step: 29720, train/epoch: 7.072822570800781
Step: 29730, train/loss: 9.999999747378752e-05
Step: 29730, train/grad_norm: 0.00021623495558742434
Step: 29730, train/learning_rate: 1.4623988136008848e-05
Step: 29730, train/epoch: 7.075202465057373
Step: 29740, train/loss: 0.01119999960064888
Step: 29740, train/grad_norm: 1.9215818610973656e-05
Step: 29740, train/learning_rate: 1.4612089216825552e-05
Step: 29740, train/epoch: 7.077581882476807
Step: 29750, train/loss: 0.0
Step: 29750, train/grad_norm: 0.0001143191329902038
Step: 29750, train/learning_rate: 1.4600190297642257e-05
Step: 29750, train/epoch: 7.079961776733398
Step: 29760, train/loss: 0.0006000000284984708
Step: 29760, train/grad_norm: 0.0001371497055515647
Step: 29760, train/learning_rate: 1.4588291378458962e-05
Step: 29760, train/epoch: 7.08234167098999
Step: 29770, train/loss: 0.0027000000700354576
Step: 29770, train/grad_norm: 0.00031819569994695485
Step: 29770, train/learning_rate: 1.4576392459275667e-05
Step: 29770, train/epoch: 7.084721565246582
Step: 29780, train/loss: 0.0
Step: 29780, train/grad_norm: 8.585944306105375e-05
Step: 29780, train/learning_rate: 1.4564493540092371e-05
Step: 29780, train/epoch: 7.087101459503174
Step: 29790, train/loss: 0.0003000000142492354
Step: 29790, train/grad_norm: 0.0009162374772131443
Step: 29790, train/learning_rate: 1.4552593711414374e-05
Step: 29790, train/epoch: 7.089481353759766
Step: 29800, train/loss: 0.0
Step: 29800, train/grad_norm: 8.468857231491711e-06
Step: 29800, train/learning_rate: 1.454069479223108e-05
Step: 29800, train/epoch: 7.091861248016357
Step: 29810, train/loss: 0.0
Step: 29810, train/grad_norm: 0.0004209947364870459
Step: 29810, train/learning_rate: 1.4528795873047784e-05
Step: 29810, train/epoch: 7.094240665435791
Step: 29820, train/loss: 0.00039999998989515007
Step: 29820, train/grad_norm: 0.0008105982560664415
Step: 29820, train/learning_rate: 1.4516896953864489e-05
Step: 29820, train/epoch: 7.096620559692383
Step: 29830, train/loss: 0.001500000013038516
Step: 29830, train/grad_norm: 0.0001448647235520184
Step: 29830, train/learning_rate: 1.4504998034681194e-05
Step: 29830, train/epoch: 7.099000453948975
Step: 29840, train/loss: 0.04320000112056732
Step: 29840, train/grad_norm: 9.734537889016792e-05
Step: 29840, train/learning_rate: 1.4493098206003197e-05
Step: 29840, train/epoch: 7.101380348205566
Step: 29850, train/loss: 0.09650000184774399
Step: 29850, train/grad_norm: 54.62224578857422
Step: 29850, train/learning_rate: 1.4481199286819901e-05
Step: 29850, train/epoch: 7.103760242462158
Step: 29860, train/loss: 9.999999747378752e-05
Step: 29860, train/grad_norm: 0.00038843179936520755
Step: 29860, train/learning_rate: 1.4469300367636606e-05
Step: 29860, train/epoch: 7.10614013671875
Step: 29870, train/loss: 0.01510000042617321
Step: 29870, train/grad_norm: 0.0029384703375399113
Step: 29870, train/learning_rate: 1.4457401448453311e-05
Step: 29870, train/epoch: 7.108519554138184
Step: 29880, train/loss: 0.0
Step: 29880, train/grad_norm: 0.03219722956418991
Step: 29880, train/learning_rate: 1.4445502529270016e-05
Step: 29880, train/epoch: 7.110899448394775
Step: 29890, train/loss: 0.0
Step: 29890, train/grad_norm: 6.145916995592415e-05
Step: 29890, train/learning_rate: 1.4433602700592019e-05
Step: 29890, train/epoch: 7.113279342651367
Step: 29900, train/loss: 0.03200000151991844
Step: 29900, train/grad_norm: 24.019533157348633
Step: 29900, train/learning_rate: 1.4421703781408723e-05
Step: 29900, train/epoch: 7.115659236907959
Step: 29910, train/loss: 0.06870000064373016
Step: 29910, train/grad_norm: 0.00011154520325362682
Step: 29910, train/learning_rate: 1.4409804862225428e-05
Step: 29910, train/epoch: 7.118039131164551
Step: 29920, train/loss: 0.0010000000474974513
Step: 29920, train/grad_norm: 1.1571435928344727
Step: 29920, train/learning_rate: 1.4397905943042133e-05
Step: 29920, train/epoch: 7.120419025421143
Step: 29930, train/loss: 0.007199999876320362
Step: 29930, train/grad_norm: 0.0011804127134382725
Step: 29930, train/learning_rate: 1.4386007023858838e-05
Step: 29930, train/epoch: 7.122798442840576
Step: 29940, train/loss: 0.0
Step: 29940, train/grad_norm: 0.012689718045294285
Step: 29940, train/learning_rate: 1.437410719518084e-05
Step: 29940, train/epoch: 7.125178337097168
Step: 29950, train/loss: 0.0026000000070780516
Step: 29950, train/grad_norm: 0.010477232746779919
Step: 29950, train/learning_rate: 1.4362208275997546e-05
Step: 29950, train/epoch: 7.12755823135376
Step: 29960, train/loss: 9.999999747378752e-05
Step: 29960, train/grad_norm: 0.0003142126079183072
Step: 29960, train/learning_rate: 1.435030935681425e-05
Step: 29960, train/epoch: 7.129938125610352
Step: 29970, train/loss: 9.999999747378752e-05
Step: 29970, train/grad_norm: 0.23181498050689697
Step: 29970, train/learning_rate: 1.4338410437630955e-05
Step: 29970, train/epoch: 7.132318019866943
Step: 29980, train/loss: 0.00019999999494757503
Step: 29980, train/grad_norm: 0.0019460972398519516
Step: 29980, train/learning_rate: 1.432651151844766e-05
Step: 29980, train/epoch: 7.134697914123535
Step: 29990, train/loss: 0.0008999999845400453
Step: 29990, train/grad_norm: 0.0003948818484786898
Step: 29990, train/learning_rate: 1.4314611689769663e-05
Step: 29990, train/epoch: 7.137077808380127
Step: 30000, train/loss: 0.0006000000284984708
Step: 30000, train/grad_norm: 0.0001999924425035715
Step: 30000, train/learning_rate: 1.4302712770586368e-05
Step: 30000, train/epoch: 7.1394572257995605
Step: 30010, train/loss: 0.0215000007301569
Step: 30010, train/grad_norm: 90.05685424804688
Step: 30010, train/learning_rate: 1.4290813851403072e-05
Step: 30010, train/epoch: 7.141837120056152
Step: 30020, train/loss: 0.02019999921321869
Step: 30020, train/grad_norm: 9.526229405310005e-05
Step: 30020, train/learning_rate: 1.4278914932219777e-05
Step: 30020, train/epoch: 7.144217014312744
Step: 30030, train/loss: 9.999999747378752e-05
Step: 30030, train/grad_norm: 0.00021463079610839486
Step: 30030, train/learning_rate: 1.4267016013036482e-05
Step: 30030, train/epoch: 7.146596908569336
Step: 30040, train/loss: 0.0
Step: 30040, train/grad_norm: 0.00020070480240974575
Step: 30040, train/learning_rate: 1.4255116184358485e-05
Step: 30040, train/epoch: 7.148976802825928
Step: 30050, train/loss: 0.048900000751018524
Step: 30050, train/grad_norm: 0.012647398747503757
Step: 30050, train/learning_rate: 1.424321726517519e-05
Step: 30050, train/epoch: 7.1513566970825195
Step: 30060, train/loss: 0.0
Step: 30060, train/grad_norm: 7.572844333481044e-05
Step: 30060, train/learning_rate: 1.4231318345991895e-05
Step: 30060, train/epoch: 7.153736114501953
Step: 30070, train/loss: 0.16329999268054962
Step: 30070, train/grad_norm: 8.540590351913124e-05
Step: 30070, train/learning_rate: 1.42194194268086e-05
Step: 30070, train/epoch: 7.156116008758545
Step: 30080, train/loss: 0.002199999988079071
Step: 30080, train/grad_norm: 0.0010177071671932936
Step: 30080, train/learning_rate: 1.4207520507625304e-05
Step: 30080, train/epoch: 7.158495903015137
Step: 30090, train/loss: 0.05009999871253967
Step: 30090, train/grad_norm: 6.16704928688705e-05
Step: 30090, train/learning_rate: 1.4195620678947307e-05
Step: 30090, train/epoch: 7.1608757972717285
Step: 30100, train/loss: 0.0
Step: 30100, train/grad_norm: 0.0002764803357422352
Step: 30100, train/learning_rate: 1.4183721759764012e-05
Step: 30100, train/epoch: 7.16325569152832
Step: 30110, train/loss: 0.057500001043081284
Step: 30110, train/grad_norm: 46.05060577392578
Step: 30110, train/learning_rate: 1.4171822840580717e-05
Step: 30110, train/epoch: 7.165635585784912
Step: 30120, train/loss: 0.049300000071525574
Step: 30120, train/grad_norm: 0.0003301090036984533
Step: 30120, train/learning_rate: 1.4159923921397422e-05
Step: 30120, train/epoch: 7.168015003204346
Step: 30130, train/loss: 0.0
Step: 30130, train/grad_norm: 0.0007124283583834767
Step: 30130, train/learning_rate: 1.4148025002214126e-05
Step: 30130, train/epoch: 7.1703948974609375
Step: 30140, train/loss: 0.0066999997943639755
Step: 30140, train/grad_norm: 0.000144799065310508
Step: 30140, train/learning_rate: 1.4136126083030831e-05
Step: 30140, train/epoch: 7.172774791717529
Step: 30150, train/loss: 0.010200000368058681
Step: 30150, train/grad_norm: 0.0009629391715861857
Step: 30150, train/learning_rate: 1.4124226254352834e-05
Step: 30150, train/epoch: 7.175154685974121
Step: 30160, train/loss: 0.003599999938160181
Step: 30160, train/grad_norm: 0.010172190144658089
Step: 30160, train/learning_rate: 1.4112327335169539e-05
Step: 30160, train/epoch: 7.177534580230713
Step: 30170, train/loss: 0.02539999969303608
Step: 30170, train/grad_norm: 0.0015475049149245024
Step: 30170, train/learning_rate: 1.4100428415986244e-05
Step: 30170, train/epoch: 7.179914474487305
Step: 30180, train/loss: 0.0005000000237487257
Step: 30180, train/grad_norm: 3.4467978477478027
Step: 30180, train/learning_rate: 1.4088529496802948e-05
Step: 30180, train/epoch: 7.1822943687438965
Step: 30190, train/loss: 0.0066999997943639755
Step: 30190, train/grad_norm: 0.0004102843231521547
Step: 30190, train/learning_rate: 1.4076630577619653e-05
Step: 30190, train/epoch: 7.18467378616333
Step: 30200, train/loss: 0.05469999834895134
Step: 30200, train/grad_norm: 0.030243797227740288
Step: 30200, train/learning_rate: 1.4064730748941656e-05
Step: 30200, train/epoch: 7.187053680419922
Step: 30210, train/loss: 0.06800000369548798
Step: 30210, train/grad_norm: 0.01830522157251835
Step: 30210, train/learning_rate: 1.4052831829758361e-05
Step: 30210, train/epoch: 7.189433574676514
Step: 30220, train/loss: 9.999999747378752e-05
Step: 30220, train/grad_norm: 0.13773103058338165
Step: 30220, train/learning_rate: 1.4040932910575066e-05
Step: 30220, train/epoch: 7.1918134689331055
Step: 30230, train/loss: 0.06780000030994415
Step: 30230, train/grad_norm: 0.00021258769265841693
Step: 30230, train/learning_rate: 1.402903399139177e-05
Step: 30230, train/epoch: 7.194193363189697
Step: 30240, train/loss: 0.0007999999797903001
Step: 30240, train/grad_norm: 0.0013053019065409899
Step: 30240, train/learning_rate: 1.4017135072208475e-05
Step: 30240, train/epoch: 7.196573257446289
Step: 30250, train/loss: 0.00019999999494757503
Step: 30250, train/grad_norm: 0.0017255805432796478
Step: 30250, train/learning_rate: 1.4005235243530478e-05
Step: 30250, train/epoch: 7.198952674865723
Step: 30260, train/loss: 0.0
Step: 30260, train/grad_norm: 0.0008571245707571507
Step: 30260, train/learning_rate: 1.3993336324347183e-05
Step: 30260, train/epoch: 7.2013325691223145
Step: 30270, train/loss: 0.0071000000461936
Step: 30270, train/grad_norm: 0.00010120119259227067
Step: 30270, train/learning_rate: 1.3981437405163888e-05
Step: 30270, train/epoch: 7.203712463378906
Step: 30280, train/loss: 0.000699999975040555
Step: 30280, train/grad_norm: 2.355607509613037
Step: 30280, train/learning_rate: 1.3969538485980593e-05
Step: 30280, train/epoch: 7.206092357635498
Step: 30290, train/loss: 0.011900000274181366
Step: 30290, train/grad_norm: 49.77981948852539
Step: 30290, train/learning_rate: 1.3957639566797297e-05
Step: 30290, train/epoch: 7.20847225189209
Step: 30300, train/loss: 0.11190000176429749
Step: 30300, train/grad_norm: 0.005332572385668755
Step: 30300, train/learning_rate: 1.39457397381193e-05
Step: 30300, train/epoch: 7.210852146148682
Step: 30310, train/loss: 0.10920000076293945
Step: 30310, train/grad_norm: 0.0013181745307520032
Step: 30310, train/learning_rate: 1.3933840818936005e-05
Step: 30310, train/epoch: 7.213231563568115
Step: 30320, train/loss: 0.0006000000284984708
Step: 30320, train/grad_norm: 5.896705624763854e-05
Step: 30320, train/learning_rate: 1.392194189975271e-05
Step: 30320, train/epoch: 7.215611457824707
Step: 30330, train/loss: 0.004800000227987766
Step: 30330, train/grad_norm: 0.013216513209044933
Step: 30330, train/learning_rate: 1.3910042980569415e-05
Step: 30330, train/epoch: 7.217991352081299
Step: 30340, train/loss: 0.0
Step: 30340, train/grad_norm: 0.00022727677423972636
Step: 30340, train/learning_rate: 1.389814406138612e-05
Step: 30340, train/epoch: 7.220371246337891
Step: 30350, train/loss: 0.015699999406933784
Step: 30350, train/grad_norm: 0.000993648893199861
Step: 30350, train/learning_rate: 1.3886244232708123e-05
Step: 30350, train/epoch: 7.222751140594482
Step: 30360, train/loss: 0.007199999876320362
Step: 30360, train/grad_norm: 0.003480171784758568
Step: 30360, train/learning_rate: 1.3874345313524827e-05
Step: 30360, train/epoch: 7.225131034851074
Step: 30370, train/loss: 0.00019999999494757503
Step: 30370, train/grad_norm: 0.016451377421617508
Step: 30370, train/learning_rate: 1.3862446394341532e-05
Step: 30370, train/epoch: 7.227510929107666
Step: 30380, train/loss: 0.009999999776482582
Step: 30380, train/grad_norm: 8.722569327801466e-05
Step: 30380, train/learning_rate: 1.3850547475158237e-05
Step: 30380, train/epoch: 7.2298903465271
Step: 30390, train/loss: 0.00019999999494757503
Step: 30390, train/grad_norm: 9.968946687877178e-05
Step: 30390, train/learning_rate: 1.3838648555974942e-05
Step: 30390, train/epoch: 7.232270240783691
Step: 30400, train/loss: 0.0
Step: 30400, train/grad_norm: 0.00018356673535890877
Step: 30400, train/learning_rate: 1.3826748727296945e-05
Step: 30400, train/epoch: 7.234650135040283
Step: 30410, train/loss: 0.0
Step: 30410, train/grad_norm: 0.0011779679916799068
Step: 30410, train/learning_rate: 1.381484980811365e-05
Step: 30410, train/epoch: 7.237030029296875
Step: 30420, train/loss: 0.0
Step: 30420, train/grad_norm: 7.093665772117674e-05
Step: 30420, train/learning_rate: 1.3802950888930354e-05
Step: 30420, train/epoch: 7.239409923553467
Step: 30430, train/loss: 0.00019999999494757503
Step: 30430, train/grad_norm: 0.00061135517898947
Step: 30430, train/learning_rate: 1.3791051969747059e-05
Step: 30430, train/epoch: 7.241789817810059
Step: 30440, train/loss: 0.005100000184029341
Step: 30440, train/grad_norm: 0.03206080198287964
Step: 30440, train/learning_rate: 1.3779153050563764e-05
Step: 30440, train/epoch: 7.244169235229492
Step: 30450, train/loss: 0.0017000000225380063
Step: 30450, train/grad_norm: 0.00023577921092510223
Step: 30450, train/learning_rate: 1.3767254131380469e-05
Step: 30450, train/epoch: 7.246549129486084
Step: 30460, train/loss: 0.00019999999494757503
Step: 30460, train/grad_norm: 1.60767522174865e-05
Step: 30460, train/learning_rate: 1.3755354302702472e-05
Step: 30460, train/epoch: 7.248929023742676
Step: 30470, train/loss: 0.07209999859333038
Step: 30470, train/grad_norm: 0.0008242353214882314
Step: 30470, train/learning_rate: 1.3743455383519176e-05
Step: 30470, train/epoch: 7.251308917999268
Step: 30480, train/loss: 0.0003000000142492354
Step: 30480, train/grad_norm: 0.016433971002697945
Step: 30480, train/learning_rate: 1.3731556464335881e-05
Step: 30480, train/epoch: 7.253688812255859
Step: 30490, train/loss: 0.009700000286102295
Step: 30490, train/grad_norm: 0.0006819285335950553
Step: 30490, train/learning_rate: 1.3719657545152586e-05
Step: 30490, train/epoch: 7.256068706512451
Step: 30500, train/loss: 0.09610000252723694
Step: 30500, train/grad_norm: 0.054851092398166656
Step: 30500, train/learning_rate: 1.370775862596929e-05
Step: 30500, train/epoch: 7.258448123931885
Step: 30510, train/loss: 0.06350000202655792
Step: 30510, train/grad_norm: 72.4271469116211
Step: 30510, train/learning_rate: 1.3695858797291294e-05
Step: 30510, train/epoch: 7.260828018188477
Step: 30520, train/loss: 0.13189999759197235
Step: 30520, train/grad_norm: 0.0003412864462006837
Step: 30520, train/learning_rate: 1.3683959878107999e-05
Step: 30520, train/epoch: 7.263207912445068
Step: 30530, train/loss: 0.0006000000284984708
Step: 30530, train/grad_norm: 0.05993841215968132
Step: 30530, train/learning_rate: 1.3672060958924703e-05
Step: 30530, train/epoch: 7.26558780670166
Step: 30540, train/loss: 0.013399999588727951
Step: 30540, train/grad_norm: 0.003277181414887309
Step: 30540, train/learning_rate: 1.3660162039741408e-05
Step: 30540, train/epoch: 7.267967700958252
Step: 30550, train/loss: 0.0010000000474974513
Step: 30550, train/grad_norm: 0.0007338448776863515
Step: 30550, train/learning_rate: 1.3648263120558113e-05
Step: 30550, train/epoch: 7.270347595214844
Step: 30560, train/loss: 0.0003000000142492354
Step: 30560, train/grad_norm: 0.0003657550550997257
Step: 30560, train/learning_rate: 1.3636363291880116e-05
Step: 30560, train/epoch: 7.2727274894714355
Step: 30570, train/loss: 0.00019999999494757503
Step: 30570, train/grad_norm: 0.003507294226437807
Step: 30570, train/learning_rate: 1.362446437269682e-05
Step: 30570, train/epoch: 7.275106906890869
Step: 30580, train/loss: 0.0006000000284984708
Step: 30580, train/grad_norm: 0.00010840821778401732
Step: 30580, train/learning_rate: 1.3612565453513525e-05
Step: 30580, train/epoch: 7.277486801147461
Step: 30590, train/loss: 0.0013000000035390258
Step: 30590, train/grad_norm: 0.007748673669993877
Step: 30590, train/learning_rate: 1.360066653433023e-05
Step: 30590, train/epoch: 7.279866695404053
Step: 30600, train/loss: 0.0005000000237487257
Step: 30600, train/grad_norm: 1.049255132675171
Step: 30600, train/learning_rate: 1.3588767615146935e-05
Step: 30600, train/epoch: 7.2822465896606445
Step: 30610, train/loss: 0.0
Step: 30610, train/grad_norm: 0.0050299460999667645
Step: 30610, train/learning_rate: 1.3576867786468938e-05
Step: 30610, train/epoch: 7.284626483917236
Step: 30620, train/loss: 0.08760000020265579
Step: 30620, train/grad_norm: 30.34716796875
Step: 30620, train/learning_rate: 1.3564968867285643e-05
Step: 30620, train/epoch: 7.287006378173828
Step: 30630, train/loss: 0.0017999999690800905
Step: 30630, train/grad_norm: 13.285988807678223
Step: 30630, train/learning_rate: 1.3553069948102348e-05
Step: 30630, train/epoch: 7.289385795593262
Step: 30640, train/loss: 0.0010999999940395355
Step: 30640, train/grad_norm: 0.0002689971588551998
Step: 30640, train/learning_rate: 1.3541171028919052e-05
Step: 30640, train/epoch: 7.2917656898498535
Step: 30650, train/loss: 0.04670000076293945
Step: 30650, train/grad_norm: 0.0003987982345279306
Step: 30650, train/learning_rate: 1.3529272109735757e-05
Step: 30650, train/epoch: 7.294145584106445
Step: 30660, train/loss: 9.999999747378752e-05
Step: 30660, train/grad_norm: 0.019614147022366524
Step: 30660, train/learning_rate: 1.351737228105776e-05
Step: 30660, train/epoch: 7.296525478363037
Step: 30670, train/loss: 0.012400000356137753
Step: 30670, train/grad_norm: 0.00023606458853464574
Step: 30670, train/learning_rate: 1.3505473361874465e-05
Step: 30670, train/epoch: 7.298905372619629
Step: 30680, train/loss: 0.0026000000070780516
Step: 30680, train/grad_norm: 12.978489875793457
Step: 30680, train/learning_rate: 1.349357444269117e-05
Step: 30680, train/epoch: 7.301285266876221
Step: 30690, train/loss: 9.999999747378752e-05
Step: 30690, train/grad_norm: 0.0019599299412220716
Step: 30690, train/learning_rate: 1.3481675523507874e-05
Step: 30690, train/epoch: 7.303664684295654
Step: 30700, train/loss: 0.11060000211000443
Step: 30700, train/grad_norm: 0.0006319370004348457
Step: 30700, train/learning_rate: 1.346977660432458e-05
Step: 30700, train/epoch: 7.306044578552246
Step: 30710, train/loss: 0.0066999997943639755
Step: 30710, train/grad_norm: 0.004722106270492077
Step: 30710, train/learning_rate: 1.3457876775646582e-05
Step: 30710, train/epoch: 7.308424472808838
Step: 30720, train/loss: 0.0
Step: 30720, train/grad_norm: 0.0007501603104174137
Step: 30720, train/learning_rate: 1.3445977856463287e-05
Step: 30720, train/epoch: 7.31080436706543
Step: 30730, train/loss: 9.999999747378752e-05
Step: 30730, train/grad_norm: 0.0019361601443961263
Step: 30730, train/learning_rate: 1.3434078937279992e-05
Step: 30730, train/epoch: 7.3131842613220215
Step: 30740, train/loss: 0.040699999779462814
Step: 30740, train/grad_norm: 0.0002378406934440136
Step: 30740, train/learning_rate: 1.3422180018096697e-05
Step: 30740, train/epoch: 7.315564155578613
Step: 30750, train/loss: 0.093299999833107
Step: 30750, train/grad_norm: 14.641948699951172
Step: 30750, train/learning_rate: 1.3410281098913401e-05
Step: 30750, train/epoch: 7.317944049835205
Step: 30760, train/loss: 0.00019999999494757503
Step: 30760, train/grad_norm: 0.01199632603675127
Step: 30760, train/learning_rate: 1.3398381270235404e-05
Step: 30760, train/epoch: 7.320323467254639
Step: 30770, train/loss: 0.0
Step: 30770, train/grad_norm: 2.0984522052458487e-05
Step: 30770, train/learning_rate: 1.3386482351052109e-05
Step: 30770, train/epoch: 7.3227033615112305
Step: 30780, train/loss: 0.00019999999494757503
Step: 30780, train/grad_norm: 0.0002586454793345183
Step: 30780, train/learning_rate: 1.3374583431868814e-05
Step: 30780, train/epoch: 7.325083255767822
Step: 30790, train/loss: 9.999999747378752e-05
Step: 30790, train/grad_norm: 0.000566136441193521
Step: 30790, train/learning_rate: 1.3362684512685519e-05
Step: 30790, train/epoch: 7.327463150024414
Step: 30800, train/loss: 9.999999747378752e-05
Step: 30800, train/grad_norm: 0.0662732943892479
Step: 30800, train/learning_rate: 1.3350785593502223e-05
Step: 30800, train/epoch: 7.329843044281006
Step: 30810, train/loss: 0.1088000014424324
Step: 30810, train/grad_norm: 0.00044297182466834784
Step: 30810, train/learning_rate: 1.3338886674318928e-05
Step: 30810, train/epoch: 7.332222938537598
Step: 30820, train/loss: 0.010700000450015068
Step: 30820, train/grad_norm: 0.0003862972662318498
Step: 30820, train/learning_rate: 1.3326986845640931e-05
Step: 30820, train/epoch: 7.334602355957031
Step: 30830, train/loss: 9.999999747378752e-05
Step: 30830, train/grad_norm: 0.07448606938123703
Step: 30830, train/learning_rate: 1.3315087926457636e-05
Step: 30830, train/epoch: 7.336982250213623
Step: 30840, train/loss: 0.017100000753998756
Step: 30840, train/grad_norm: 0.0008643837063573301
Step: 30840, train/learning_rate: 1.330318900727434e-05
Step: 30840, train/epoch: 7.339362144470215
Step: 30850, train/loss: 0.013700000010430813
Step: 30850, train/grad_norm: 0.04026857018470764
Step: 30850, train/learning_rate: 1.3291290088091046e-05
Step: 30850, train/epoch: 7.341742038726807
Step: 30860, train/loss: 0.00039999998989515007
Step: 30860, train/grad_norm: 0.38136377930641174
Step: 30860, train/learning_rate: 1.327939116890775e-05
Step: 30860, train/epoch: 7.344121932983398
Step: 30870, train/loss: 0.0017999999690800905
Step: 30870, train/grad_norm: 0.0006204297533258796
Step: 30870, train/learning_rate: 1.3267491340229753e-05
Step: 30870, train/epoch: 7.34650182723999
Step: 30880, train/loss: 0.042500000447034836
Step: 30880, train/grad_norm: 0.0006181030767038465
Step: 30880, train/learning_rate: 1.3255592421046458e-05
Step: 30880, train/epoch: 7.348881721496582
Step: 30890, train/loss: 0.05900000035762787
Step: 30890, train/grad_norm: 0.0005003648111596704
Step: 30890, train/learning_rate: 1.3243693501863163e-05
Step: 30890, train/epoch: 7.351261138916016
Step: 30900, train/loss: 9.999999747378752e-05
Step: 30900, train/grad_norm: 0.001145059708505869
Step: 30900, train/learning_rate: 1.3231794582679868e-05
Step: 30900, train/epoch: 7.353641033172607
Step: 30910, train/loss: 0.000699999975040555
Step: 30910, train/grad_norm: 0.0008534273947589099
Step: 30910, train/learning_rate: 1.3219895663496573e-05
Step: 30910, train/epoch: 7.356020927429199
Step: 30920, train/loss: 0.00039999998989515007
Step: 30920, train/grad_norm: 1.846143126487732
Step: 30920, train/learning_rate: 1.3207995834818576e-05
Step: 30920, train/epoch: 7.358400821685791
Step: 30930, train/loss: 0.021400000900030136
Step: 30930, train/grad_norm: 0.0022735532838851213
Step: 30930, train/learning_rate: 1.319609691563528e-05
Step: 30930, train/epoch: 7.360780715942383
Step: 30940, train/loss: 0.010099999606609344
Step: 30940, train/grad_norm: 0.00047351597459055483
Step: 30940, train/learning_rate: 1.3184197996451985e-05
Step: 30940, train/epoch: 7.363160610198975
Step: 30950, train/loss: 9.999999747378752e-05
Step: 30950, train/grad_norm: 0.021847281605005264
Step: 30950, train/learning_rate: 1.317229907726869e-05
Step: 30950, train/epoch: 7.365540027618408
Step: 30960, train/loss: 0.007000000216066837
Step: 30960, train/grad_norm: 0.0007484002271667123
Step: 30960, train/learning_rate: 1.3160400158085395e-05
Step: 30960, train/epoch: 7.367919921875
Step: 30970, train/loss: 0.0608999989926815
Step: 30970, train/grad_norm: 2.8947947025299072
Step: 30970, train/learning_rate: 1.3148500329407398e-05
Step: 30970, train/epoch: 7.370299816131592
Step: 30980, train/loss: 0.032499998807907104
Step: 30980, train/grad_norm: 0.0001664308219915256
Step: 30980, train/learning_rate: 1.3136601410224102e-05
Step: 30980, train/epoch: 7.372679710388184
Step: 30990, train/loss: 0.13079999387264252
Step: 30990, train/grad_norm: 0.0003020587028004229
Step: 30990, train/learning_rate: 1.3124702491040807e-05
Step: 30990, train/epoch: 7.375059604644775
Step: 31000, train/loss: 0.0034000000450760126
Step: 31000, train/grad_norm: 0.006635442841798067
Step: 31000, train/learning_rate: 1.3112803571857512e-05
Step: 31000, train/epoch: 7.377439498901367
Step: 31010, train/loss: 9.999999747378752e-05
Step: 31010, train/grad_norm: 0.000562152941711247
Step: 31010, train/learning_rate: 1.3100904652674217e-05
Step: 31010, train/epoch: 7.379818916320801
Step: 31020, train/loss: 0.0017999999690800905
Step: 31020, train/grad_norm: 0.012589501217007637
Step: 31020, train/learning_rate: 1.308900482399622e-05
Step: 31020, train/epoch: 7.382198810577393
Step: 31030, train/loss: 0.07989999651908875
Step: 31030, train/grad_norm: 0.011499705724418163
Step: 31030, train/learning_rate: 1.3077105904812925e-05
Step: 31030, train/epoch: 7.384578704833984
Step: 31040, train/loss: 0.03189999982714653
Step: 31040, train/grad_norm: 58.5299072265625
Step: 31040, train/learning_rate: 1.306520698562963e-05
Step: 31040, train/epoch: 7.386958599090576
Step: 31050, train/loss: 0.05649999901652336
Step: 31050, train/grad_norm: 0.0013370485976338387
Step: 31050, train/learning_rate: 1.3053308066446334e-05
Step: 31050, train/epoch: 7.389338493347168
Step: 31060, train/loss: 0.06019999831914902
Step: 31060, train/grad_norm: 0.005940694827586412
Step: 31060, train/learning_rate: 1.3041409147263039e-05
Step: 31060, train/epoch: 7.39171838760376
Step: 31070, train/loss: 0.002400000113993883
Step: 31070, train/grad_norm: 0.0019866584334522486
Step: 31070, train/learning_rate: 1.3029509318585042e-05
Step: 31070, train/epoch: 7.394098281860352
Step: 31080, train/loss: 0.04639999940991402
Step: 31080, train/grad_norm: 0.0007114327745512128
Step: 31080, train/learning_rate: 1.3017610399401747e-05
Step: 31080, train/epoch: 7.396477699279785
Step: 31090, train/loss: 0.0003000000142492354
Step: 31090, train/grad_norm: 0.010152147151529789
Step: 31090, train/learning_rate: 1.3005711480218451e-05
Step: 31090, train/epoch: 7.398857593536377
Step: 31100, train/loss: 0.05169999971985817
Step: 31100, train/grad_norm: 0.004594195634126663
Step: 31100, train/learning_rate: 1.2993812561035156e-05
Step: 31100, train/epoch: 7.401237487792969
Step: 31110, train/loss: 0.13680000603199005
Step: 31110, train/grad_norm: 0.0011056214570999146
Step: 31110, train/learning_rate: 1.2981913641851861e-05
Step: 31110, train/epoch: 7.4036173820495605
Step: 31120, train/loss: 0.002099999925121665
Step: 31120, train/grad_norm: 0.0025210401508957148
Step: 31120, train/learning_rate: 1.2970014722668566e-05
Step: 31120, train/epoch: 7.405997276306152
Step: 31130, train/loss: 0.00019999999494757503
Step: 31130, train/grad_norm: 0.0033051539212465286
Step: 31130, train/learning_rate: 1.2958114893990569e-05
Step: 31130, train/epoch: 7.408377170562744
Step: 31140, train/loss: 9.999999747378752e-05
Step: 31140, train/grad_norm: 0.00017557632236275822
Step: 31140, train/learning_rate: 1.2946215974807274e-05
Step: 31140, train/epoch: 7.410756587982178
Step: 31150, train/loss: 0.031700000166893005
Step: 31150, train/grad_norm: 0.00042434202623553574
Step: 31150, train/learning_rate: 1.2934317055623978e-05
Step: 31150, train/epoch: 7.4131364822387695
Step: 31160, train/loss: 9.999999747378752e-05
Step: 31160, train/grad_norm: 0.00039270418346859515
Step: 31160, train/learning_rate: 1.2922418136440683e-05
Step: 31160, train/epoch: 7.415516376495361
Step: 31170, train/loss: 9.999999747378752e-05
Step: 31170, train/grad_norm: 0.017859045416116714
Step: 31170, train/learning_rate: 1.2910519217257388e-05
Step: 31170, train/epoch: 7.417896270751953
Step: 31180, train/loss: 0.0
Step: 31180, train/grad_norm: 0.0002739931223914027
Step: 31180, train/learning_rate: 1.2898619388579391e-05
Step: 31180, train/epoch: 7.420276165008545
Step: 31190, train/loss: 0.0031999999191612005
Step: 31190, train/grad_norm: 0.0008981010760180652
Step: 31190, train/learning_rate: 1.2886720469396096e-05
Step: 31190, train/epoch: 7.422656059265137
Step: 31200, train/loss: 0.0
Step: 31200, train/grad_norm: 0.0018963712500408292
Step: 31200, train/learning_rate: 1.28748215502128e-05
Step: 31200, train/epoch: 7.42503547668457
Step: 31210, train/loss: 0.0
Step: 31210, train/grad_norm: 0.0008464620914310217
Step: 31210, train/learning_rate: 1.2862922631029505e-05
Step: 31210, train/epoch: 7.427415370941162
Step: 31220, train/loss: 0.0
Step: 31220, train/grad_norm: 0.0042611388489604
Step: 31220, train/learning_rate: 1.285102371184621e-05
Step: 31220, train/epoch: 7.429795265197754
Step: 31230, train/loss: 0.05389999970793724
Step: 31230, train/grad_norm: 0.008633356541395187
Step: 31230, train/learning_rate: 1.2839123883168213e-05
Step: 31230, train/epoch: 7.432175159454346
Step: 31240, train/loss: 0.00019999999494757503
Step: 31240, train/grad_norm: 0.0006253392202779651
Step: 31240, train/learning_rate: 1.2827224963984918e-05
Step: 31240, train/epoch: 7.4345550537109375
Step: 31250, train/loss: 0.01549999974668026
Step: 31250, train/grad_norm: 61.815250396728516
Step: 31250, train/learning_rate: 1.2815326044801623e-05
Step: 31250, train/epoch: 7.436934947967529
Step: 31260, train/loss: 0.001500000013038516
Step: 31260, train/grad_norm: 7.578713893890381
Step: 31260, train/learning_rate: 1.2803427125618327e-05
Step: 31260, train/epoch: 7.439314842224121
Step: 31270, train/loss: 0.001500000013038516
Step: 31270, train/grad_norm: 0.00023682958271820098
Step: 31270, train/learning_rate: 1.2791528206435032e-05
Step: 31270, train/epoch: 7.441694259643555
Step: 31280, train/loss: 9.999999747378752e-05
Step: 31280, train/grad_norm: 0.00013684900477528572
Step: 31280, train/learning_rate: 1.2779628377757035e-05
Step: 31280, train/epoch: 7.4440741539001465
Step: 31290, train/loss: 0.1298000067472458
Step: 31290, train/grad_norm: 0.018383020535111427
Step: 31290, train/learning_rate: 1.276772945857374e-05
Step: 31290, train/epoch: 7.446454048156738
Step: 31300, train/loss: 0.06639999896287918
Step: 31300, train/grad_norm: 0.003783788997679949
Step: 31300, train/learning_rate: 1.2755830539390445e-05
Step: 31300, train/epoch: 7.44883394241333
Step: 31310, train/loss: 9.999999747378752e-05
Step: 31310, train/grad_norm: 0.0010807730723172426
Step: 31310, train/learning_rate: 1.274393162020715e-05
Step: 31310, train/epoch: 7.451213836669922
Step: 31320, train/loss: 0.0
Step: 31320, train/grad_norm: 0.0007758699939586222
Step: 31320, train/learning_rate: 1.2732032701023854e-05
Step: 31320, train/epoch: 7.453593730926514
Step: 31330, train/loss: 0.06800000369548798
Step: 31330, train/grad_norm: 0.0028495306614786386
Step: 31330, train/learning_rate: 1.2720132872345857e-05
Step: 31330, train/epoch: 7.455973148345947
Step: 31340, train/loss: 0.03660000115633011
Step: 31340, train/grad_norm: 0.0005234851851128042
Step: 31340, train/learning_rate: 1.2708233953162562e-05
Step: 31340, train/epoch: 7.458353042602539
Step: 31350, train/loss: 0.0031999999191612005
Step: 31350, train/grad_norm: 0.00043396229739300907
Step: 31350, train/learning_rate: 1.2696335033979267e-05
Step: 31350, train/epoch: 7.460732936859131
Step: 31360, train/loss: 0.006599999964237213
Step: 31360, train/grad_norm: 0.001526017440482974
Step: 31360, train/learning_rate: 1.2684436114795972e-05
Step: 31360, train/epoch: 7.463112831115723
Step: 31370, train/loss: 0.0
Step: 31370, train/grad_norm: 0.0012364375870674849
Step: 31370, train/learning_rate: 1.2672537195612676e-05
Step: 31370, train/epoch: 7.4654927253723145
Step: 31380, train/loss: 9.999999747378752e-05
Step: 31380, train/grad_norm: 5.1282800995977595e-05
Step: 31380, train/learning_rate: 1.266063736693468e-05
Step: 31380, train/epoch: 7.467872619628906
Step: 31390, train/loss: 0.01600000075995922
Step: 31390, train/grad_norm: 0.0003791032650042325
Step: 31390, train/learning_rate: 1.2648738447751384e-05
Step: 31390, train/epoch: 7.47025203704834
Step: 31400, train/loss: 0.013399999588727951
Step: 31400, train/grad_norm: 9.776329040527344
Step: 31400, train/learning_rate: 1.2636839528568089e-05
Step: 31400, train/epoch: 7.472631931304932
Step: 31410, train/loss: 0.1062999963760376
Step: 31410, train/grad_norm: 0.0004265721363481134
Step: 31410, train/learning_rate: 1.2624940609384794e-05
Step: 31410, train/epoch: 7.475011825561523
Step: 31420, train/loss: 0.0
Step: 31420, train/grad_norm: 0.0005131526268087327
Step: 31420, train/learning_rate: 1.2613041690201499e-05
Step: 31420, train/epoch: 7.477391719818115
Step: 31430, train/loss: 0.00019999999494757503
Step: 31430, train/grad_norm: 0.0015751002356410027
Step: 31430, train/learning_rate: 1.2601141861523502e-05
Step: 31430, train/epoch: 7.479771614074707
Step: 31440, train/loss: 9.999999747378752e-05
Step: 31440, train/grad_norm: 0.005207796581089497
Step: 31440, train/learning_rate: 1.2589242942340206e-05
Step: 31440, train/epoch: 7.482151508331299
Step: 31450, train/loss: 0.017400000244379044
Step: 31450, train/grad_norm: 0.00011684373748721555
Step: 31450, train/learning_rate: 1.2577344023156911e-05
Step: 31450, train/epoch: 7.484531402587891
Step: 31460, train/loss: 0.0005000000237487257
Step: 31460, train/grad_norm: 0.0434127040207386
Step: 31460, train/learning_rate: 1.2565445103973616e-05
Step: 31460, train/epoch: 7.486910820007324
Step: 31470, train/loss: 9.999999747378752e-05
Step: 31470, train/grad_norm: 0.00010562603711150587
Step: 31470, train/learning_rate: 1.255354618479032e-05
Step: 31470, train/epoch: 7.489290714263916
Step: 31480, train/loss: 9.999999747378752e-05
Step: 31480, train/grad_norm: 0.29488405585289
Step: 31480, train/learning_rate: 1.2541647265607025e-05
Step: 31480, train/epoch: 7.491670608520508
Step: 31490, train/loss: 0.06019999831914902
Step: 31490, train/grad_norm: 0.0009514039847999811
Step: 31490, train/learning_rate: 1.2529747436929028e-05
Step: 31490, train/epoch: 7.4940505027771
Step: 31500, train/loss: 0.03460000082850456
Step: 31500, train/grad_norm: 0.0006689041620120406
Step: 31500, train/learning_rate: 1.2517848517745733e-05
Step: 31500, train/epoch: 7.496430397033691
Step: 31510, train/loss: 0.00019999999494757503
Step: 31510, train/grad_norm: 0.00016629838501103222
Step: 31510, train/learning_rate: 1.2505949598562438e-05
Step: 31510, train/epoch: 7.498810291290283
Step: 31520, train/loss: 0.023099999874830246
Step: 31520, train/grad_norm: 0.0007989551522769034
Step: 31520, train/learning_rate: 1.2494050679379143e-05
Step: 31520, train/epoch: 7.501189708709717
Step: 31530, train/loss: 0.05429999902844429
Step: 31530, train/grad_norm: 50.80012512207031
Step: 31530, train/learning_rate: 1.2482151760195848e-05
Step: 31530, train/epoch: 7.503569602966309
Step: 31540, train/loss: 0.00019999999494757503
Step: 31540, train/grad_norm: 0.00581796420738101
Step: 31540, train/learning_rate: 1.247025193151785e-05
Step: 31540, train/epoch: 7.5059494972229
Step: 31550, train/loss: 0.0012000000569969416
Step: 31550, train/grad_norm: 0.0003800519625656307
Step: 31550, train/learning_rate: 1.2458353012334555e-05
Step: 31550, train/epoch: 7.508329391479492
Step: 31560, train/loss: 0.0003000000142492354
Step: 31560, train/grad_norm: 0.4699152410030365
Step: 31560, train/learning_rate: 1.244645409315126e-05
Step: 31560, train/epoch: 7.510709285736084
Step: 31570, train/loss: 0.0731000006198883
Step: 31570, train/grad_norm: 0.0003612167201936245
Step: 31570, train/learning_rate: 1.2434555173967965e-05
Step: 31570, train/epoch: 7.513089179992676
Step: 31580, train/loss: 0.0
Step: 31580, train/grad_norm: 0.00012182302452856675
Step: 31580, train/learning_rate: 1.242265625478467e-05
Step: 31580, train/epoch: 7.515468597412109
Step: 31590, train/loss: 0.09380000084638596
Step: 31590, train/grad_norm: 0.14742834866046906
Step: 31590, train/learning_rate: 1.2410756426106673e-05
Step: 31590, train/epoch: 7.517848491668701
Step: 31600, train/loss: 0.00019999999494757503
Step: 31600, train/grad_norm: 0.0006413474911823869
Step: 31600, train/learning_rate: 1.2398857506923378e-05
Step: 31600, train/epoch: 7.520228385925293
Step: 31610, train/loss: 9.999999747378752e-05
Step: 31610, train/grad_norm: 0.00017035195196513087
Step: 31610, train/learning_rate: 1.2386958587740082e-05
Step: 31610, train/epoch: 7.522608280181885
Step: 31620, train/loss: 0.0006000000284984708
Step: 31620, train/grad_norm: 0.006740379147231579
Step: 31620, train/learning_rate: 1.2375059668556787e-05
Step: 31620, train/epoch: 7.524988174438477
Step: 31630, train/loss: 0.0471000000834465
Step: 31630, train/grad_norm: 0.00013921312347520143
Step: 31630, train/learning_rate: 1.2363160749373492e-05
Step: 31630, train/epoch: 7.527368068695068
Step: 31640, train/loss: 0.0
Step: 31640, train/grad_norm: 0.0003059705486521125
Step: 31640, train/learning_rate: 1.2351260920695495e-05
Step: 31640, train/epoch: 7.52974796295166
Step: 31650, train/loss: 0.0003000000142492354
Step: 31650, train/grad_norm: 0.003420306835323572
Step: 31650, train/learning_rate: 1.23393620015122e-05
Step: 31650, train/epoch: 7.532127380371094
Step: 31660, train/loss: 0.14069999754428864
Step: 31660, train/grad_norm: 0.0009386249003000557
Step: 31660, train/learning_rate: 1.2327463082328904e-05
Step: 31660, train/epoch: 7.5345072746276855
Step: 31670, train/loss: 9.999999747378752e-05
Step: 31670, train/grad_norm: 0.21672211587429047
Step: 31670, train/learning_rate: 1.231556416314561e-05
Step: 31670, train/epoch: 7.536887168884277
Step: 31680, train/loss: 0.0010000000474974513
Step: 31680, train/grad_norm: 0.0001358192239422351
Step: 31680, train/learning_rate: 1.2303665243962314e-05
Step: 31680, train/epoch: 7.539267063140869
Step: 31690, train/loss: 0.0
Step: 31690, train/grad_norm: 5.724731454392895e-05
Step: 31690, train/learning_rate: 1.2291765415284317e-05
Step: 31690, train/epoch: 7.541646957397461
Step: 31700, train/loss: 0.016100000590085983
Step: 31700, train/grad_norm: 0.005518215708434582
Step: 31700, train/learning_rate: 1.2279866496101022e-05
Step: 31700, train/epoch: 7.544026851654053
Step: 31710, train/loss: 9.999999747378752e-05
Step: 31710, train/grad_norm: 0.00015227684343699366
Step: 31710, train/learning_rate: 1.2267967576917727e-05
Step: 31710, train/epoch: 7.546406269073486
Step: 31720, train/loss: 0.0
Step: 31720, train/grad_norm: 8.638580766273662e-05
Step: 31720, train/learning_rate: 1.2256068657734431e-05
Step: 31720, train/epoch: 7.548786163330078
Step: 31730, train/loss: 0.0006000000284984708
Step: 31730, train/grad_norm: 4.6513203415088356e-05
Step: 31730, train/learning_rate: 1.2244169738551136e-05
Step: 31730, train/epoch: 7.55116605758667
Step: 31740, train/loss: 9.999999747378752e-05
Step: 31740, train/grad_norm: 0.0003622315707616508
Step: 31740, train/learning_rate: 1.2232269909873139e-05
Step: 31740, train/epoch: 7.553545951843262
Step: 31750, train/loss: 0.03460000082850456
Step: 31750, train/grad_norm: 0.003714349353685975
Step: 31750, train/learning_rate: 1.2220370990689844e-05
Step: 31750, train/epoch: 7.5559258460998535
Step: 31760, train/loss: 0.01860000006854534
Step: 31760, train/grad_norm: 5.510672417585738e-05
Step: 31760, train/learning_rate: 1.2208472071506549e-05
Step: 31760, train/epoch: 7.558305740356445
Step: 31770, train/loss: 0.026000000536441803
Step: 31770, train/grad_norm: 0.0005429108277894557
Step: 31770, train/learning_rate: 1.2196573152323253e-05
Step: 31770, train/epoch: 7.560685157775879
Step: 31780, train/loss: 0.0010999999940395355
Step: 31780, train/grad_norm: 0.033297374844551086
Step: 31780, train/learning_rate: 1.2184674233139958e-05
Step: 31780, train/epoch: 7.563065052032471
Step: 31790, train/loss: 0.0
Step: 31790, train/grad_norm: 0.00021013972582295537
Step: 31790, train/learning_rate: 1.2172775313956663e-05
Step: 31790, train/epoch: 7.5654449462890625
Step: 31800, train/loss: 0.019099999219179153
Step: 31800, train/grad_norm: 0.0007187005248852074
Step: 31800, train/learning_rate: 1.2160875485278666e-05
Step: 31800, train/epoch: 7.567824840545654
Step: 31810, train/loss: 0.039500001817941666
Step: 31810, train/grad_norm: 5.505009175976738e-05
Step: 31810, train/learning_rate: 1.214897656609537e-05
Step: 31810, train/epoch: 7.570204734802246
Step: 31820, train/loss: 0.002400000113993883
Step: 31820, train/grad_norm: 0.000235520230489783
Step: 31820, train/learning_rate: 1.2137077646912076e-05
Step: 31820, train/epoch: 7.572584629058838
Step: 31830, train/loss: 0.0
Step: 31830, train/grad_norm: 2.8664861019933596e-05
Step: 31830, train/learning_rate: 1.212517872772878e-05
Step: 31830, train/epoch: 7.57496452331543
Step: 31840, train/loss: 0.07699999958276749
Step: 31840, train/grad_norm: 0.007207782939076424
Step: 31840, train/learning_rate: 1.2113279808545485e-05
Step: 31840, train/epoch: 7.577343940734863
Step: 31850, train/loss: 0.0010000000474974513
Step: 31850, train/grad_norm: 0.0009789254982024431
Step: 31850, train/learning_rate: 1.2101379979867488e-05
Step: 31850, train/epoch: 7.579723834991455
Step: 31860, train/loss: 0.0
Step: 31860, train/grad_norm: 0.0002773632586468011
Step: 31860, train/learning_rate: 1.2089481060684193e-05
Step: 31860, train/epoch: 7.582103729248047
Step: 31870, train/loss: 9.999999747378752e-05
Step: 31870, train/grad_norm: 0.08017902821302414
Step: 31870, train/learning_rate: 1.2077582141500898e-05
Step: 31870, train/epoch: 7.584483623504639
Step: 31880, train/loss: 0.000699999975040555
Step: 31880, train/grad_norm: 0.002072330564260483
Step: 31880, train/learning_rate: 1.2065683222317602e-05
Step: 31880, train/epoch: 7.5868635177612305
Step: 31890, train/loss: 0.027400000020861626
Step: 31890, train/grad_norm: 0.013497882522642612
Step: 31890, train/learning_rate: 1.2053784303134307e-05
Step: 31890, train/epoch: 7.589243412017822
Step: 31900, train/loss: 0.0
Step: 31900, train/grad_norm: 0.00879459735006094
Step: 31900, train/learning_rate: 1.204188447445631e-05
Step: 31900, train/epoch: 7.591622829437256
Step: 31910, train/loss: 0.007699999958276749
Step: 31910, train/grad_norm: 9.87815874395892e-05
Step: 31910, train/learning_rate: 1.2029985555273015e-05
Step: 31910, train/epoch: 7.594002723693848
Step: 31920, train/loss: 0.057999998331069946
Step: 31920, train/grad_norm: 9.418220724910498e-05
Step: 31920, train/learning_rate: 1.201808663608972e-05
Step: 31920, train/epoch: 7.5963826179504395
Step: 31930, train/loss: 0.09969999641180038
Step: 31930, train/grad_norm: 0.011470413766801357
Step: 31930, train/learning_rate: 1.2006187716906425e-05
Step: 31930, train/epoch: 7.598762512207031
Step: 31940, train/loss: 0.03889999911189079
Step: 31940, train/grad_norm: 7.382962212432176e-05
Step: 31940, train/learning_rate: 1.199428879772313e-05
Step: 31940, train/epoch: 7.601142406463623
Step: 31950, train/loss: 0.0
Step: 31950, train/grad_norm: 0.00017804001981858164
Step: 31950, train/learning_rate: 1.1982388969045132e-05
Step: 31950, train/epoch: 7.603522300720215
Step: 31960, train/loss: 0.0013000000035390258
Step: 31960, train/grad_norm: 0.024569513276219368
Step: 31960, train/learning_rate: 1.1970490049861837e-05
Step: 31960, train/epoch: 7.605901718139648
Step: 31970, train/loss: 0.001500000013038516
Step: 31970, train/grad_norm: 0.00022431219986174256
Step: 31970, train/learning_rate: 1.1958591130678542e-05
Step: 31970, train/epoch: 7.60828161239624
Step: 31980, train/loss: 0.02539999969303608
Step: 31980, train/grad_norm: 0.0004251536156516522
Step: 31980, train/learning_rate: 1.1946692211495247e-05
Step: 31980, train/epoch: 7.610661506652832
Step: 31990, train/loss: 0.003100000089034438
Step: 31990, train/grad_norm: 9.422045695828274e-05
Step: 31990, train/learning_rate: 1.1934793292311952e-05
Step: 31990, train/epoch: 7.613041400909424
Step: 32000, train/loss: 9.999999747378752e-05
Step: 32000, train/grad_norm: 0.00013321930600795895
Step: 32000, train/learning_rate: 1.1922893463633955e-05
Step: 32000, train/epoch: 7.615421295166016
Step: 32010, train/loss: 0.15119999647140503
Step: 32010, train/grad_norm: 83.1910400390625
Step: 32010, train/learning_rate: 1.191099454445066e-05
Step: 32010, train/epoch: 7.617801189422607
Step: 32020, train/loss: 0.06369999796152115
Step: 32020, train/grad_norm: 74.77709197998047
Step: 32020, train/learning_rate: 1.1899095625267364e-05
Step: 32020, train/epoch: 7.620181083679199
Step: 32030, train/loss: 0.002099999925121665
Step: 32030, train/grad_norm: 19.365270614624023
Step: 32030, train/learning_rate: 1.1887196706084069e-05
Step: 32030, train/epoch: 7.622560501098633
Step: 32040, train/loss: 0.0017000000225380063
Step: 32040, train/grad_norm: 4.9119920731754974e-05
Step: 32040, train/learning_rate: 1.1875297786900774e-05
Step: 32040, train/epoch: 7.624940395355225
Step: 32050, train/loss: 0.15440000593662262
Step: 32050, train/grad_norm: 9.53670751187019e-05
Step: 32050, train/learning_rate: 1.1863397958222777e-05
Step: 32050, train/epoch: 7.627320289611816
Step: 32060, train/loss: 0.0005000000237487257
Step: 32060, train/grad_norm: 6.938254955457523e-05
Step: 32060, train/learning_rate: 1.1851499039039481e-05
Step: 32060, train/epoch: 7.629700183868408
Step: 32070, train/loss: 0.0
Step: 32070, train/grad_norm: 0.011793414130806923
Step: 32070, train/learning_rate: 1.1839600119856186e-05
Step: 32070, train/epoch: 7.632080078125
Step: 32080, train/loss: 0.0007999999797903001
Step: 32080, train/grad_norm: 0.00016808483633212745
Step: 32080, train/learning_rate: 1.1827701200672891e-05
Step: 32080, train/epoch: 7.634459972381592
Step: 32090, train/loss: 0.07500000298023224
Step: 32090, train/grad_norm: 1.0811465472215787e-05
Step: 32090, train/learning_rate: 1.1815802281489596e-05
Step: 32090, train/epoch: 7.636839389801025
Step: 32100, train/loss: 0.057500001043081284
Step: 32100, train/grad_norm: 0.00027970477822236717
Step: 32100, train/learning_rate: 1.1803902452811599e-05
Step: 32100, train/epoch: 7.639219284057617
Step: 32110, train/loss: 0.0
Step: 32110, train/grad_norm: 0.003245835890993476
Step: 32110, train/learning_rate: 1.1792003533628304e-05
Step: 32110, train/epoch: 7.641599178314209
Step: 32120, train/loss: 0.059300001710653305
Step: 32120, train/grad_norm: 93.22156524658203
Step: 32120, train/learning_rate: 1.1780104614445008e-05
Step: 32120, train/epoch: 7.643979072570801
Step: 32130, train/loss: 0.0
Step: 32130, train/grad_norm: 8.436944881395902e-06
Step: 32130, train/learning_rate: 1.1768205695261713e-05
Step: 32130, train/epoch: 7.646358966827393
Step: 32140, train/loss: 9.999999747378752e-05
Step: 32140, train/grad_norm: 0.0006072206306271255
Step: 32140, train/learning_rate: 1.1756306776078418e-05
Step: 32140, train/epoch: 7.648738861083984
Step: 32150, train/loss: 0.06369999796152115
Step: 32150, train/grad_norm: 0.001928939949721098
Step: 32150, train/learning_rate: 1.1744407856895123e-05
Step: 32150, train/epoch: 7.651118278503418
Step: 32160, train/loss: 0.0017000000225380063
Step: 32160, train/grad_norm: 0.09881039708852768
Step: 32160, train/learning_rate: 1.1732508028217126e-05
Step: 32160, train/epoch: 7.65349817276001
Step: 32170, train/loss: 9.999999747378752e-05
Step: 32170, train/grad_norm: 0.001189464470371604
Step: 32170, train/learning_rate: 1.172060910903383e-05
Step: 32170, train/epoch: 7.655878067016602
Step: 32180, train/loss: 0.010099999606609344
Step: 32180, train/grad_norm: 55.13277816772461
Step: 32180, train/learning_rate: 1.1708710189850535e-05
Step: 32180, train/epoch: 7.658257961273193
Step: 32190, train/loss: 0.061000000685453415
Step: 32190, train/grad_norm: 4.6973098505986854e-05
Step: 32190, train/learning_rate: 1.169681127066724e-05
Step: 32190, train/epoch: 7.660637855529785
Step: 32200, train/loss: 0.0
Step: 32200, train/grad_norm: 5.2835795941064134e-05
Step: 32200, train/learning_rate: 1.1684912351483945e-05
Step: 32200, train/epoch: 7.663017749786377
Step: 32210, train/loss: 0.00019999999494757503
Step: 32210, train/grad_norm: 0.0007184178684838116
Step: 32210, train/learning_rate: 1.1673012522805948e-05
Step: 32210, train/epoch: 7.665397644042969
Step: 32220, train/loss: 0.0
Step: 32220, train/grad_norm: 0.002692426787689328
Step: 32220, train/learning_rate: 1.1661113603622653e-05
Step: 32220, train/epoch: 7.667777061462402
Step: 32230, train/loss: 9.999999747378752e-05
Step: 32230, train/grad_norm: 0.001712440513074398
Step: 32230, train/learning_rate: 1.1649214684439357e-05
Step: 32230, train/epoch: 7.670156955718994
Step: 32240, train/loss: 0.0013000000035390258
Step: 32240, train/grad_norm: 0.0006037725834175944
Step: 32240, train/learning_rate: 1.1637315765256062e-05
Step: 32240, train/epoch: 7.672536849975586
Step: 32250, train/loss: 0.07150000333786011
Step: 32250, train/grad_norm: 60.0966796875
Step: 32250, train/learning_rate: 1.1625416846072767e-05
Step: 32250, train/epoch: 7.674916744232178
Step: 32260, train/loss: 0.09889999777078629
Step: 32260, train/grad_norm: 86.82362365722656
Step: 32260, train/learning_rate: 1.161351701739477e-05
Step: 32260, train/epoch: 7.6772966384887695
Step: 32270, train/loss: 0.0
Step: 32270, train/grad_norm: 0.0001510822185082361
Step: 32270, train/learning_rate: 1.1601618098211475e-05
Step: 32270, train/epoch: 7.679676532745361
Step: 32280, train/loss: 0.0
Step: 32280, train/grad_norm: 0.01188899576663971
Step: 32280, train/learning_rate: 1.158971917902818e-05
Step: 32280, train/epoch: 7.682055950164795
Step: 32290, train/loss: 0.0006000000284984708
Step: 32290, train/grad_norm: 2.955701893370133e-05
Step: 32290, train/learning_rate: 1.1577820259844884e-05
Step: 32290, train/epoch: 7.684435844421387
Step: 32300, train/loss: 0.027000000700354576
Step: 32300, train/grad_norm: 0.0005854471237398684
Step: 32300, train/learning_rate: 1.1565921340661589e-05
Step: 32300, train/epoch: 7.6868157386779785
Step: 32310, train/loss: 0.17110000550746918
Step: 32310, train/grad_norm: 0.00016606391000095755
Step: 32310, train/learning_rate: 1.1554021511983592e-05
Step: 32310, train/epoch: 7.68919563293457
Step: 32320, train/loss: 0.03799999877810478
Step: 32320, train/grad_norm: 0.9669502973556519
Step: 32320, train/learning_rate: 1.1542122592800297e-05
Step: 32320, train/epoch: 7.691575527191162
Step: 32330, train/loss: 0.0
Step: 32330, train/grad_norm: 6.340833351714537e-05
Step: 32330, train/learning_rate: 1.1530223673617002e-05
Step: 32330, train/epoch: 7.693955421447754
Step: 32340, train/loss: 0.10159999877214432
Step: 32340, train/grad_norm: 5.754820449510589e-05
Step: 32340, train/learning_rate: 1.1518324754433706e-05
Step: 32340, train/epoch: 7.696335315704346
Step: 32350, train/loss: 0.08479999750852585
Step: 32350, train/grad_norm: 2.972515176224988e-05
Step: 32350, train/learning_rate: 1.1506425835250411e-05
Step: 32350, train/epoch: 7.698714733123779
Step: 32360, train/loss: 0.0
Step: 32360, train/grad_norm: 0.0001356985158054158
Step: 32360, train/learning_rate: 1.1494526006572414e-05
Step: 32360, train/epoch: 7.701094627380371
Step: 32370, train/loss: 0.013100000098347664
Step: 32370, train/grad_norm: 0.00040471041575074196
Step: 32370, train/learning_rate: 1.1482627087389119e-05
Step: 32370, train/epoch: 7.703474521636963
Step: 32380, train/loss: 0.05820000171661377
Step: 32380, train/grad_norm: 5.4887208534637466e-05
Step: 32380, train/learning_rate: 1.1470728168205824e-05
Step: 32380, train/epoch: 7.705854415893555
Step: 32390, train/loss: 0.00019999999494757503
Step: 32390, train/grad_norm: 0.0004402184276841581
Step: 32390, train/learning_rate: 1.1458829249022529e-05
Step: 32390, train/epoch: 7.7082343101501465
Step: 32400, train/loss: 9.999999747378752e-05
Step: 32400, train/grad_norm: 0.00548006035387516
Step: 32400, train/learning_rate: 1.1446930329839233e-05
Step: 32400, train/epoch: 7.710614204406738
Step: 32410, train/loss: 9.999999747378752e-05
Step: 32410, train/grad_norm: 0.001600949326530099
Step: 32410, train/learning_rate: 1.1435030501161236e-05
Step: 32410, train/epoch: 7.712993621826172
Step: 32420, train/loss: 0.018400000408291817
Step: 32420, train/grad_norm: 9.417327880859375
Step: 32420, train/learning_rate: 1.1423131581977941e-05
Step: 32420, train/epoch: 7.715373516082764
Step: 32430, train/loss: 0.0008999999845400453
Step: 32430, train/grad_norm: 4.738448143005371
Step: 32430, train/learning_rate: 1.1411232662794646e-05
Step: 32430, train/epoch: 7.7177534103393555
Step: 32440, train/loss: 0.007699999958276749
Step: 32440, train/grad_norm: 2.520939233363606e-05
Step: 32440, train/learning_rate: 1.139933374361135e-05
Step: 32440, train/epoch: 7.720133304595947
Step: 32450, train/loss: 0.0
Step: 32450, train/grad_norm: 0.0011688742088153958
Step: 32450, train/learning_rate: 1.1387434824428055e-05
Step: 32450, train/epoch: 7.722513198852539
Step: 32460, train/loss: 0.11559999734163284
Step: 32460, train/grad_norm: 5.679675268766005e-06
Step: 32460, train/learning_rate: 1.137553590524476e-05
Step: 32460, train/epoch: 7.724893093109131
Step: 32470, train/loss: 0.013899999670684338
Step: 32470, train/grad_norm: 0.0001655141677474603
Step: 32470, train/learning_rate: 1.1363636076566763e-05
Step: 32470, train/epoch: 7.7272725105285645
Step: 32480, train/loss: 0.02969999983906746
Step: 32480, train/grad_norm: 5.113283623359166e-05
Step: 32480, train/learning_rate: 1.1351737157383468e-05
Step: 32480, train/epoch: 7.729652404785156
Step: 32490, train/loss: 9.999999747378752e-05
Step: 32490, train/grad_norm: 0.043446317315101624
Step: 32490, train/learning_rate: 1.1339838238200173e-05
Step: 32490, train/epoch: 7.732032299041748
Step: 32500, train/loss: 9.999999747378752e-05
Step: 32500, train/grad_norm: 0.00030986074125394225
Step: 32500, train/learning_rate: 1.1327939319016878e-05
Step: 32500, train/epoch: 7.73441219329834
Step: 32510, train/loss: 0.06639999896287918
Step: 32510, train/grad_norm: 2.2543414161191322e-05
Step: 32510, train/learning_rate: 1.1316040399833582e-05
Step: 32510, train/epoch: 7.736792087554932
Step: 32520, train/loss: 0.003700000001117587
Step: 32520, train/grad_norm: 9.327240695711225e-05
Step: 32520, train/learning_rate: 1.1304140571155585e-05
Step: 32520, train/epoch: 7.739171981811523
Step: 32530, train/loss: 0.07829999923706055
Step: 32530, train/grad_norm: 0.00011432004976086318
Step: 32530, train/learning_rate: 1.129224165197229e-05
Step: 32530, train/epoch: 7.741551876068115
Step: 32540, train/loss: 0.0
Step: 32540, train/grad_norm: 0.0004905186942778528
Step: 32540, train/learning_rate: 1.1280342732788995e-05
Step: 32540, train/epoch: 7.743931293487549
Step: 32550, train/loss: 0.0
Step: 32550, train/grad_norm: 0.0003418586275074631
Step: 32550, train/learning_rate: 1.12684438136057e-05
Step: 32550, train/epoch: 7.746311187744141
Step: 32560, train/loss: 0.0
Step: 32560, train/grad_norm: 0.00036765242111869156
Step: 32560, train/learning_rate: 1.1256544894422404e-05
Step: 32560, train/epoch: 7.748691082000732
Step: 32570, train/loss: 0.045099999755620956
Step: 32570, train/grad_norm: 0.00010939173080259934
Step: 32570, train/learning_rate: 1.1244645065744407e-05
Step: 32570, train/epoch: 7.751070976257324
Step: 32580, train/loss: 0.010200000368058681
Step: 32580, train/grad_norm: 0.0004837992019020021
Step: 32580, train/learning_rate: 1.1232746146561112e-05
Step: 32580, train/epoch: 7.753450870513916
Step: 32590, train/loss: 0.0
Step: 32590, train/grad_norm: 0.0008462459081783891
Step: 32590, train/learning_rate: 1.1220847227377817e-05
Step: 32590, train/epoch: 7.755830764770508
Step: 32600, train/loss: 9.999999747378752e-05
Step: 32600, train/grad_norm: 4.5859720557928085e-05
Step: 32600, train/learning_rate: 1.1208948308194522e-05
Step: 32600, train/epoch: 7.758210182189941
Step: 32610, train/loss: 0.0019000000320374966
Step: 32610, train/grad_norm: 0.001903628115542233
Step: 32610, train/learning_rate: 1.1197049389011227e-05
Step: 32610, train/epoch: 7.760590076446533
Step: 32620, train/loss: 0.00019999999494757503
Step: 32620, train/grad_norm: 0.6500632166862488
Step: 32620, train/learning_rate: 1.118514956033323e-05
Step: 32620, train/epoch: 7.762969970703125
Step: 32630, train/loss: 0.00019999999494757503
Step: 32630, train/grad_norm: 0.0003461747837718576
Step: 32630, train/learning_rate: 1.1173250641149934e-05
Step: 32630, train/epoch: 7.765349864959717
Step: 32640, train/loss: 0.0
Step: 32640, train/grad_norm: 0.0176229327917099
Step: 32640, train/learning_rate: 1.116135172196664e-05
Step: 32640, train/epoch: 7.767729759216309
Step: 32650, train/loss: 0.0005000000237487257
Step: 32650, train/grad_norm: 1.603387713432312
Step: 32650, train/learning_rate: 1.1149452802783344e-05
Step: 32650, train/epoch: 7.7701096534729
Step: 32660, train/loss: 0.016499999910593033
Step: 32660, train/grad_norm: 0.00024648199905641377
Step: 32660, train/learning_rate: 1.1137553883600049e-05
Step: 32660, train/epoch: 7.772489070892334
Step: 32670, train/loss: 0.00039999998989515007
Step: 32670, train/grad_norm: 0.0004681710561271757
Step: 32670, train/learning_rate: 1.1125654054922052e-05
Step: 32670, train/epoch: 7.774868965148926
Step: 32680, train/loss: 0.03370000049471855
Step: 32680, train/grad_norm: 0.004853866528719664
Step: 32680, train/learning_rate: 1.1113755135738757e-05
Step: 32680, train/epoch: 7.777248859405518
Step: 32690, train/loss: 0.0
Step: 32690, train/grad_norm: 0.00017140849377028644
Step: 32690, train/learning_rate: 1.1101856216555461e-05
Step: 32690, train/epoch: 7.779628753662109
Step: 32700, train/loss: 0.0
Step: 32700, train/grad_norm: 0.000119695883768145
Step: 32700, train/learning_rate: 1.1089957297372166e-05
Step: 32700, train/epoch: 7.782008647918701
Step: 32710, train/loss: 0.0
Step: 32710, train/grad_norm: 0.0002836551866494119
Step: 32710, train/learning_rate: 1.1078058378188871e-05
Step: 32710, train/epoch: 7.784388542175293
Step: 32720, train/loss: 0.0003000000142492354
Step: 32720, train/grad_norm: 1.432692289352417
Step: 32720, train/learning_rate: 1.1066158549510874e-05
Step: 32720, train/epoch: 7.786768436431885
Step: 32730, train/loss: 0.00019999999494757503
Step: 32730, train/grad_norm: 0.0003439899592194706
Step: 32730, train/learning_rate: 1.1054259630327579e-05
Step: 32730, train/epoch: 7.789147853851318
Step: 32740, train/loss: 0.04170000180602074
Step: 32740, train/grad_norm: 0.0017166461329907179
Step: 32740, train/learning_rate: 1.1042360711144283e-05
Step: 32740, train/epoch: 7.79152774810791
Step: 32750, train/loss: 0.17110000550746918
Step: 32750, train/grad_norm: 0.0009639004711061716
Step: 32750, train/learning_rate: 1.1030461791960988e-05
Step: 32750, train/epoch: 7.793907642364502
Step: 32760, train/loss: 0.0731000006198883
Step: 32760, train/grad_norm: 0.03991328552365303
Step: 32760, train/learning_rate: 1.1018562872777693e-05
Step: 32760, train/epoch: 7.796287536621094
Step: 32770, train/loss: 0.0017000000225380063
Step: 32770, train/grad_norm: 0.0002641412429511547
Step: 32770, train/learning_rate: 1.1006663044099696e-05
Step: 32770, train/epoch: 7.7986674308776855
Step: 32780, train/loss: 0.004100000020116568
Step: 32780, train/grad_norm: 0.0008494869107380509
Step: 32780, train/learning_rate: 1.09947641249164e-05
Step: 32780, train/epoch: 7.801047325134277
Step: 32790, train/loss: 0.06650000065565109
Step: 32790, train/grad_norm: 134.19253540039062
Step: 32790, train/learning_rate: 1.0982865205733106e-05
Step: 32790, train/epoch: 7.803426742553711
Step: 32800, train/loss: 0.00039999998989515007
Step: 32800, train/grad_norm: 4.933191303280182e-05
Step: 32800, train/learning_rate: 1.097096628654981e-05
Step: 32800, train/epoch: 7.805806636810303
Step: 32810, train/loss: 0.00019999999494757503
Step: 32810, train/grad_norm: 0.0003153962315991521
Step: 32810, train/learning_rate: 1.0959067367366515e-05
Step: 32810, train/epoch: 7.8081865310668945
Step: 32820, train/loss: 0.05090000107884407
Step: 32820, train/grad_norm: 0.00036624542553909123
Step: 32820, train/learning_rate: 1.094716844818322e-05
Step: 32820, train/epoch: 7.810566425323486
Step: 32830, train/loss: 0.0005000000237487257
Step: 32830, train/grad_norm: 5.647146826959215e-05
Step: 32830, train/learning_rate: 1.0935268619505223e-05
Step: 32830, train/epoch: 7.812946319580078
Step: 32840, train/loss: 0.004900000058114529
Step: 32840, train/grad_norm: 0.0029943231493234634
Step: 32840, train/learning_rate: 1.0923369700321928e-05
Step: 32840, train/epoch: 7.81532621383667
Step: 32850, train/loss: 0.09730000048875809
Step: 32850, train/grad_norm: 0.08811839669942856
Step: 32850, train/learning_rate: 1.0911470781138632e-05
Step: 32850, train/epoch: 7.8177056312561035
Step: 32860, train/loss: 0.014000000432133675
Step: 32860, train/grad_norm: 0.00015157597954384983
Step: 32860, train/learning_rate: 1.0899571861955337e-05
Step: 32860, train/epoch: 7.820085525512695
Step: 32870, train/loss: 0.03150000050663948
Step: 32870, train/grad_norm: 0.0032497006468474865
Step: 32870, train/learning_rate: 1.0887672942772042e-05
Step: 32870, train/epoch: 7.822465419769287
Step: 32880, train/loss: 0.0
Step: 32880, train/grad_norm: 2.9047139378235443e-06
Step: 32880, train/learning_rate: 1.0875773114094045e-05
Step: 32880, train/epoch: 7.824845314025879
Step: 32890, train/loss: 9.999999747378752e-05
Step: 32890, train/grad_norm: 0.00012686525587923825
Step: 32890, train/learning_rate: 1.086387419491075e-05
Step: 32890, train/epoch: 7.827225208282471
Step: 32900, train/loss: 0.03060000017285347
Step: 32900, train/grad_norm: 0.00016500978381372988
Step: 32900, train/learning_rate: 1.0851975275727455e-05
Step: 32900, train/epoch: 7.8296051025390625
Step: 32910, train/loss: 0.0034000000450760126
Step: 32910, train/grad_norm: 0.023749476298689842
Step: 32910, train/learning_rate: 1.084007635654416e-05
Step: 32910, train/epoch: 7.831984996795654
Step: 32920, train/loss: 0.000699999975040555
Step: 32920, train/grad_norm: 0.0009050585795193911
Step: 32920, train/learning_rate: 1.0828177437360864e-05
Step: 32920, train/epoch: 7.834364414215088
Step: 32930, train/loss: 0.0003000000142492354
Step: 32930, train/grad_norm: 0.10503239929676056
Step: 32930, train/learning_rate: 1.0816277608682867e-05
Step: 32930, train/epoch: 7.83674430847168
Step: 32940, train/loss: 0.06989999860525131
Step: 32940, train/grad_norm: 2.124753952026367
Step: 32940, train/learning_rate: 1.0804378689499572e-05
Step: 32940, train/epoch: 7.8391242027282715
Step: 32950, train/loss: 0.005900000222027302
Step: 32950, train/grad_norm: 0.00023889148724265397
Step: 32950, train/learning_rate: 1.0792479770316277e-05
Step: 32950, train/epoch: 7.841504096984863
Step: 32960, train/loss: 0.002099999925121665
Step: 32960, train/grad_norm: 0.00027261898503638804
Step: 32960, train/learning_rate: 1.0780580851132981e-05
Step: 32960, train/epoch: 7.843883991241455
Step: 32970, train/loss: 0.00019999999494757503
Step: 32970, train/grad_norm: 0.00013038754696026444
Step: 32970, train/learning_rate: 1.0768681931949686e-05
Step: 32970, train/epoch: 7.846263885498047
Step: 32980, train/loss: 0.007600000128149986
Step: 32980, train/grad_norm: 0.009892147965729237
Step: 32980, train/learning_rate: 1.075678210327169e-05
Step: 32980, train/epoch: 7.8486433029174805
Step: 32990, train/loss: 0.05590000003576279
Step: 32990, train/grad_norm: 0.00040123722283169627
Step: 32990, train/learning_rate: 1.0744883184088394e-05
Step: 32990, train/epoch: 7.851023197174072
Step: 33000, train/loss: 9.999999747378752e-05
Step: 33000, train/grad_norm: 0.4489766061306
Step: 33000, train/learning_rate: 1.0732984264905099e-05
Step: 33000, train/epoch: 7.853403091430664
Step: 33010, train/loss: 0.07419999688863754
Step: 33010, train/grad_norm: 6.308418232947588e-05
Step: 33010, train/learning_rate: 1.0721085345721804e-05
Step: 33010, train/epoch: 7.855782985687256
Step: 33020, train/loss: 0.05389999970793724
Step: 33020, train/grad_norm: 0.18625378608703613
Step: 33020, train/learning_rate: 1.0709186426538508e-05
Step: 33020, train/epoch: 7.858162879943848
Step: 33030, train/loss: 9.999999747378752e-05
Step: 33030, train/grad_norm: 0.00015975204587448388
Step: 33030, train/learning_rate: 1.0697286597860511e-05
Step: 33030, train/epoch: 7.8605427742004395
Step: 33040, train/loss: 9.999999747378752e-05
Step: 33040, train/grad_norm: 0.01168963685631752
Step: 33040, train/learning_rate: 1.0685387678677216e-05
Step: 33040, train/epoch: 7.862922191619873
Step: 33050, train/loss: 0.0005000000237487257
Step: 33050, train/grad_norm: 0.001110006938688457
Step: 33050, train/learning_rate: 1.0673488759493921e-05
Step: 33050, train/epoch: 7.865302085876465
Step: 33060, train/loss: 0.0035000001080334187
Step: 33060, train/grad_norm: 0.0016873283311724663
Step: 33060, train/learning_rate: 1.0661589840310626e-05
Step: 33060, train/epoch: 7.867681980133057
Step: 33070, train/loss: 0.0
Step: 33070, train/grad_norm: 0.00035214124363847077
Step: 33070, train/learning_rate: 1.064969092112733e-05
Step: 33070, train/epoch: 7.870061874389648
Step: 33080, train/loss: 0.04699999839067459
Step: 33080, train/grad_norm: 3.0682902433909476e-05
Step: 33080, train/learning_rate: 1.0637791092449334e-05
Step: 33080, train/epoch: 7.87244176864624
Step: 33090, train/loss: 0.0
Step: 33090, train/grad_norm: 9.270937880501151e-05
Step: 33090, train/learning_rate: 1.0625892173266038e-05
Step: 33090, train/epoch: 7.874821662902832
Step: 33100, train/loss: 0.09939999878406525
Step: 33100, train/grad_norm: 0.00013624437269754708
Step: 33100, train/learning_rate: 1.0613993254082743e-05
Step: 33100, train/epoch: 7.877201557159424
Step: 33110, train/loss: 0.007400000002235174
Step: 33110, train/grad_norm: 4.1235791286453605e-05
Step: 33110, train/learning_rate: 1.0602094334899448e-05
Step: 33110, train/epoch: 7.879580974578857
Step: 33120, train/loss: 0.007899999618530273
Step: 33120, train/grad_norm: 0.0006627168622799218
Step: 33120, train/learning_rate: 1.0590195415716153e-05
Step: 33120, train/epoch: 7.881960868835449
Step: 33130, train/loss: 0.0
Step: 33130, train/grad_norm: 0.0001265547762159258
Step: 33130, train/learning_rate: 1.0578296496532857e-05
Step: 33130, train/epoch: 7.884340763092041
Step: 33140, train/loss: 0.002199999988079071
Step: 33140, train/grad_norm: 0.00030712864827364683
Step: 33140, train/learning_rate: 1.056639666785486e-05
Step: 33140, train/epoch: 7.886720657348633
Step: 33150, train/loss: 0.009800000116229057
Step: 33150, train/grad_norm: 0.0005274575087241828
Step: 33150, train/learning_rate: 1.0554497748671565e-05
Step: 33150, train/epoch: 7.889100551605225
Step: 33160, train/loss: 0.07540000230073929
Step: 33160, train/grad_norm: 0.0012216131435707211
Step: 33160, train/learning_rate: 1.054259882948827e-05
Step: 33160, train/epoch: 7.891480445861816
Step: 33170, train/loss: 0.08879999816417694
Step: 33170, train/grad_norm: 0.0007048994884826243
Step: 33170, train/learning_rate: 1.0530699910304975e-05
Step: 33170, train/epoch: 7.89385986328125
Step: 33180, train/loss: 0.06870000064373016
Step: 33180, train/grad_norm: 37.44969177246094
Step: 33180, train/learning_rate: 1.051880099112168e-05
Step: 33180, train/epoch: 7.896239757537842
Step: 33190, train/loss: 0.1088000014424324
Step: 33190, train/grad_norm: 0.00012575427535921335
Step: 33190, train/learning_rate: 1.0506901162443683e-05
Step: 33190, train/epoch: 7.898619651794434
Step: 33200, train/loss: 9.999999747378752e-05
Step: 33200, train/grad_norm: 0.08069852739572525
Step: 33200, train/learning_rate: 1.0495002243260387e-05
Step: 33200, train/epoch: 7.900999546051025
Step: 33210, train/loss: 0.0
Step: 33210, train/grad_norm: 0.0012827665777876973
Step: 33210, train/learning_rate: 1.0483103324077092e-05
Step: 33210, train/epoch: 7.903379440307617
Step: 33220, train/loss: 0.05790000036358833
Step: 33220, train/grad_norm: 9.990396938519552e-05
Step: 33220, train/learning_rate: 1.0471204404893797e-05
Step: 33220, train/epoch: 7.905759334564209
Step: 33230, train/loss: 9.999999747378752e-05
Step: 33230, train/grad_norm: 0.00024111611128319055
Step: 33230, train/learning_rate: 1.0459305485710502e-05
Step: 33230, train/epoch: 7.908138751983643
Step: 33240, train/loss: 0.0032999999821186066
Step: 33240, train/grad_norm: 0.0009461439913138747
Step: 33240, train/learning_rate: 1.0447405657032505e-05
Step: 33240, train/epoch: 7.910518646240234
Step: 33250, train/loss: 0.0005000000237487257
Step: 33250, train/grad_norm: 0.00030707792029716074
Step: 33250, train/learning_rate: 1.043550673784921e-05
Step: 33250, train/epoch: 7.912898540496826
Step: 33260, train/loss: 0.0044999998062849045
Step: 33260, train/grad_norm: 0.0005501651321537793
Step: 33260, train/learning_rate: 1.0423607818665914e-05
Step: 33260, train/epoch: 7.915278434753418
Step: 33270, train/loss: 0.0020000000949949026
Step: 33270, train/grad_norm: 0.0006473013199865818
Step: 33270, train/learning_rate: 1.0411708899482619e-05
Step: 33270, train/epoch: 7.91765832901001
Step: 33280, train/loss: 0.08079999685287476
Step: 33280, train/grad_norm: 0.8086454272270203
Step: 33280, train/learning_rate: 1.0399809980299324e-05
Step: 33280, train/epoch: 7.920038223266602
Step: 33290, train/loss: 0.0005000000237487257
Step: 33290, train/grad_norm: 0.002483916701748967
Step: 33290, train/learning_rate: 1.0387910151621327e-05
Step: 33290, train/epoch: 7.922418117523193
Step: 33300, train/loss: 0.0
Step: 33300, train/grad_norm: 3.753931014216505e-05
Step: 33300, train/learning_rate: 1.0376011232438032e-05
Step: 33300, train/epoch: 7.924797534942627
Step: 33310, train/loss: 9.999999747378752e-05
Step: 33310, train/grad_norm: 0.023135170340538025
Step: 33310, train/learning_rate: 1.0364112313254736e-05
Step: 33310, train/epoch: 7.927177429199219
Step: 33320, train/loss: 0.0
Step: 33320, train/grad_norm: 0.0012889057397842407
Step: 33320, train/learning_rate: 1.0352213394071441e-05
Step: 33320, train/epoch: 7.9295573234558105
Step: 33330, train/loss: 0.018799999728798866
Step: 33330, train/grad_norm: 0.006652730517089367
Step: 33330, train/learning_rate: 1.0340314474888146e-05
Step: 33330, train/epoch: 7.931937217712402
Step: 33340, train/loss: 0.0003000000142492354
Step: 33340, train/grad_norm: 0.00018986104987561703
Step: 33340, train/learning_rate: 1.0328414646210149e-05
Step: 33340, train/epoch: 7.934317111968994
Step: 33350, train/loss: 0.0027000000700354576
Step: 33350, train/grad_norm: 3.880319491145201e-05
Step: 33350, train/learning_rate: 1.0316515727026854e-05
Step: 33350, train/epoch: 7.936697006225586
Step: 33360, train/loss: 0.2215999960899353
Step: 33360, train/grad_norm: 1.138445258140564
Step: 33360, train/learning_rate: 1.0304616807843558e-05
Step: 33360, train/epoch: 7.9390764236450195
Step: 33370, train/loss: 0.003000000026077032
Step: 33370, train/grad_norm: 0.0002479926042724401
Step: 33370, train/learning_rate: 1.0292717888660263e-05
Step: 33370, train/epoch: 7.941456317901611
Step: 33380, train/loss: 0.0
Step: 33380, train/grad_norm: 0.00393039220944047
Step: 33380, train/learning_rate: 1.0280818969476968e-05
Step: 33380, train/epoch: 7.943836212158203
Step: 33390, train/loss: 0.002400000113993883
Step: 33390, train/grad_norm: 0.0001858646428445354
Step: 33390, train/learning_rate: 1.0268919140798971e-05
Step: 33390, train/epoch: 7.946216106414795
Step: 33400, train/loss: 0.010400000028312206
Step: 33400, train/grad_norm: 6.239912181627005e-05
Step: 33400, train/learning_rate: 1.0257020221615676e-05
Step: 33400, train/epoch: 7.948596000671387
Step: 33410, train/loss: 0.01889999955892563
Step: 33410, train/grad_norm: 96.9781723022461
Step: 33410, train/learning_rate: 1.024512130243238e-05
Step: 33410, train/epoch: 7.9509758949279785
Step: 33420, train/loss: 0.08320000022649765
Step: 33420, train/grad_norm: 4.367201108834706e-05
Step: 33420, train/learning_rate: 1.0233222383249085e-05
Step: 33420, train/epoch: 7.953355312347412
Step: 33430, train/loss: 0.007799999788403511
Step: 33430, train/grad_norm: 5.2080638852203265e-05
Step: 33430, train/learning_rate: 1.022132346406579e-05
Step: 33430, train/epoch: 7.955735206604004
Step: 33440, train/loss: 0.0
Step: 33440, train/grad_norm: 5.7614572142483667e-05
Step: 33440, train/learning_rate: 1.0209423635387793e-05
Step: 33440, train/epoch: 7.958115100860596
Step: 33450, train/loss: 0.043800000101327896
Step: 33450, train/grad_norm: 0.0008058624225668609
Step: 33450, train/learning_rate: 1.0197524716204498e-05
Step: 33450, train/epoch: 7.9604949951171875
Step: 33460, train/loss: 0.014700000174343586
Step: 33460, train/grad_norm: 0.014671842567622662
Step: 33460, train/learning_rate: 1.0185625797021203e-05
Step: 33460, train/epoch: 7.962874889373779
Step: 33470, train/loss: 0.005499999970197678
Step: 33470, train/grad_norm: 0.0009871945949271321
Step: 33470, train/learning_rate: 1.0173726877837908e-05
Step: 33470, train/epoch: 7.965254783630371
Step: 33480, train/loss: 0.0
Step: 33480, train/grad_norm: 0.0001520121149951592
Step: 33480, train/learning_rate: 1.0161827958654612e-05
Step: 33480, train/epoch: 7.967634677886963
Step: 33490, train/loss: 0.0
Step: 33490, train/grad_norm: 0.03290051594376564
Step: 33490, train/learning_rate: 1.0149929039471317e-05
Step: 33490, train/epoch: 7.9700140953063965
Step: 33500, train/loss: 0.012799999676644802
Step: 33500, train/grad_norm: 0.00013180576206650585
Step: 33500, train/learning_rate: 1.013802921079332e-05
Step: 33500, train/epoch: 7.972393989562988
Step: 33510, train/loss: 0.009200000204145908
Step: 33510, train/grad_norm: 2.216538086941e-05
Step: 33510, train/learning_rate: 1.0126130291610025e-05
Step: 33510, train/epoch: 7.97477388381958
Step: 33520, train/loss: 0.04399999976158142
Step: 33520, train/grad_norm: 0.005246118176728487
Step: 33520, train/learning_rate: 1.011423137242673e-05
Step: 33520, train/epoch: 7.977153778076172
Step: 33530, train/loss: 0.00039999998989515007
Step: 33530, train/grad_norm: 0.20396734774112701
Step: 33530, train/learning_rate: 1.0102332453243434e-05
Step: 33530, train/epoch: 7.979533672332764
Step: 33540, train/loss: 0.004600000102072954
Step: 33540, train/grad_norm: 0.0012628135737031698
Step: 33540, train/learning_rate: 1.009043353406014e-05
Step: 33540, train/epoch: 7.9819135665893555
Step: 33550, train/loss: 0.2079000025987625
Step: 33550, train/grad_norm: 0.00020564967417158186
Step: 33550, train/learning_rate: 1.0078533705382142e-05
Step: 33550, train/epoch: 7.984292984008789
Step: 33560, train/loss: 0.00019999999494757503
Step: 33560, train/grad_norm: 0.00018384115537628531
Step: 33560, train/learning_rate: 1.0066634786198847e-05
Step: 33560, train/epoch: 7.986672878265381
Step: 33570, train/loss: 0.1054999977350235
Step: 33570, train/grad_norm: 0.17473597824573517
Step: 33570, train/learning_rate: 1.0054735867015552e-05
Step: 33570, train/epoch: 7.989052772521973
Step: 33580, train/loss: 0.00139999995008111
Step: 33580, train/grad_norm: 0.0005964722367934883
Step: 33580, train/learning_rate: 1.0042836947832257e-05
Step: 33580, train/epoch: 7.9914326667785645
Step: 33590, train/loss: 0.01759999990463257
Step: 33590, train/grad_norm: 5.4187530622584745e-05
Step: 33590, train/learning_rate: 1.0030938028648961e-05
Step: 33590, train/epoch: 7.993812561035156
Step: 33600, train/loss: 0.11509999632835388
Step: 33600, train/grad_norm: 2.350881914026104e-05
Step: 33600, train/learning_rate: 1.0019038199970964e-05
Step: 33600, train/epoch: 7.996192455291748
Step: 33610, train/loss: 9.999999747378752e-05
Step: 33610, train/grad_norm: 5.1476505177561194e-05
Step: 33610, train/learning_rate: 1.0007139280787669e-05
Step: 33610, train/epoch: 7.998571872711182
Step: 33616, eval/loss: 0.3847517967224121
Step: 33616, eval/accuracy: 0.9515479803085327
Step: 33616, eval/f1: 0.9490265846252441
Step: 33616, eval/runtime: 295.8092956542969
Step: 33616, eval/samples_per_second: 24.350000381469727
Step: 33616, eval/steps_per_second: 3.0460000038146973
Step: 33616, train/epoch: 8.0
Step: 33620, train/loss: 0.0
Step: 33620, train/grad_norm: 0.00026750838151201606
Step: 33620, train/learning_rate: 9.995240361604374e-06
Step: 33620, train/epoch: 8.000951766967773
Step: 33630, train/loss: 0.1362999975681305
Step: 33630, train/grad_norm: 86.01812744140625
Step: 33630, train/learning_rate: 9.983341442421079e-06
Step: 33630, train/epoch: 8.003332138061523
Step: 33640, train/loss: 9.999999747378752e-05
Step: 33640, train/grad_norm: 2.622314241307322e-05
Step: 33640, train/learning_rate: 9.971442523237783e-06
Step: 33640, train/epoch: 8.005711555480957
Step: 33650, train/loss: 0.002400000113993883
Step: 33650, train/grad_norm: 0.00021861610002815723
Step: 33650, train/learning_rate: 9.959542694559786e-06
Step: 33650, train/epoch: 8.00809097290039
Step: 33660, train/loss: 0.014499999582767487
Step: 33660, train/grad_norm: 0.0037815195973962545
Step: 33660, train/learning_rate: 9.947643775376491e-06
Step: 33660, train/epoch: 8.01047134399414
Step: 33670, train/loss: 9.999999747378752e-05
Step: 33670, train/grad_norm: 0.004989949520677328
Step: 33670, train/learning_rate: 9.935744856193196e-06
Step: 33670, train/epoch: 8.012850761413574
Step: 33680, train/loss: 0.0038999998942017555
Step: 33680, train/grad_norm: 0.40077757835388184
Step: 33680, train/learning_rate: 9.9238459370099e-06
Step: 33680, train/epoch: 8.015231132507324
Step: 33690, train/loss: 0.0
Step: 33690, train/grad_norm: 0.00011133218504255638
Step: 33690, train/learning_rate: 9.911947017826606e-06
Step: 33690, train/epoch: 8.017610549926758
Step: 33700, train/loss: 0.005100000184029341
Step: 33700, train/grad_norm: 20.218297958374023
Step: 33700, train/learning_rate: 9.900047189148609e-06
Step: 33700, train/epoch: 8.019990921020508
Step: 33710, train/loss: 0.000699999975040555
Step: 33710, train/grad_norm: 0.053922612220048904
Step: 33710, train/learning_rate: 9.888148269965313e-06
Step: 33710, train/epoch: 8.022370338439941
Step: 33720, train/loss: 0.04580000042915344
Step: 33720, train/grad_norm: 0.005383235868066549
Step: 33720, train/learning_rate: 9.876249350782018e-06
Step: 33720, train/epoch: 8.024749755859375
Step: 33730, train/loss: 0.0010000000474974513
Step: 33730, train/grad_norm: 0.003579983487725258
Step: 33730, train/learning_rate: 9.864350431598723e-06
Step: 33730, train/epoch: 8.027130126953125
Step: 33740, train/loss: 0.0019000000320374966
Step: 33740, train/grad_norm: 3.562658093869686e-05
Step: 33740, train/learning_rate: 9.852451512415428e-06
Step: 33740, train/epoch: 8.029509544372559
Step: 33750, train/loss: 0.09380000084638596
Step: 33750, train/grad_norm: 3.41518571076449e-05
Step: 33750, train/learning_rate: 9.84055168373743e-06
Step: 33750, train/epoch: 8.031889915466309
Step: 33760, train/loss: 0.0005000000237487257
Step: 33760, train/grad_norm: 0.003603228833526373
Step: 33760, train/learning_rate: 9.828652764554136e-06
Step: 33760, train/epoch: 8.034269332885742
Step: 33770, train/loss: 0.06759999692440033
Step: 33770, train/grad_norm: 0.0008551834616810083
Step: 33770, train/learning_rate: 9.81675384537084e-06
Step: 33770, train/epoch: 8.036648750305176
Step: 33780, train/loss: 0.0031999999191612005
Step: 33780, train/grad_norm: 0.001049215323291719
Step: 33780, train/learning_rate: 9.804854926187545e-06
Step: 33780, train/epoch: 8.039029121398926
Step: 33790, train/loss: 0.009200000204145908
Step: 33790, train/grad_norm: 0.07027989625930786
Step: 33790, train/learning_rate: 9.79295600700425e-06
Step: 33790, train/epoch: 8.04140853881836
Step: 33800, train/loss: 0.0010999999940395355
Step: 33800, train/grad_norm: 0.00027234689332544804
Step: 33800, train/learning_rate: 9.781057087820955e-06
Step: 33800, train/epoch: 8.04378890991211
Step: 33810, train/loss: 0.0
Step: 33810, train/grad_norm: 0.0003152183198835701
Step: 33810, train/learning_rate: 9.769157259142958e-06
Step: 33810, train/epoch: 8.046168327331543
Step: 33820, train/loss: 9.999999747378752e-05
Step: 33820, train/grad_norm: 0.0023010761942714453
Step: 33820, train/learning_rate: 9.757258339959662e-06
Step: 33820, train/epoch: 8.048548698425293
Step: 33830, train/loss: 0.0
Step: 33830, train/grad_norm: 0.000237090906011872
Step: 33830, train/learning_rate: 9.745359420776367e-06
Step: 33830, train/epoch: 8.050928115844727
Step: 33840, train/loss: 9.999999747378752e-05
Step: 33840, train/grad_norm: 0.23010361194610596
Step: 33840, train/learning_rate: 9.733460501593072e-06
Step: 33840, train/epoch: 8.05330753326416
Step: 33850, train/loss: 0.0003000000142492354
Step: 33850, train/grad_norm: 7.415724394377321e-05
Step: 33850, train/learning_rate: 9.721561582409777e-06
Step: 33850, train/epoch: 8.05568790435791
Step: 33860, train/loss: 0.0
Step: 33860, train/grad_norm: 8.062599954428151e-05
Step: 33860, train/learning_rate: 9.70966175373178e-06
Step: 33860, train/epoch: 8.058067321777344
Step: 33870, train/loss: 9.999999747378752e-05
Step: 33870, train/grad_norm: 0.0014622308081015944
Step: 33870, train/learning_rate: 9.697762834548485e-06
Step: 33870, train/epoch: 8.060447692871094
Step: 33880, train/loss: 0.0
Step: 33880, train/grad_norm: 3.128660682705231e-05
Step: 33880, train/learning_rate: 9.68586391536519e-06
Step: 33880, train/epoch: 8.062827110290527
Step: 33890, train/loss: 0.0008999999845400453
Step: 33890, train/grad_norm: 3.9814131014281884e-05
Step: 33890, train/learning_rate: 9.673964996181894e-06
Step: 33890, train/epoch: 8.065207481384277
Step: 33900, train/loss: 0.0013000000035390258
Step: 33900, train/grad_norm: 2.3411383153870702e-05
Step: 33900, train/learning_rate: 9.662066076998599e-06
Step: 33900, train/epoch: 8.067586898803711
Step: 33910, train/loss: 0.0005000000237487257
Step: 33910, train/grad_norm: 4.764525147038512e-05
Step: 33910, train/learning_rate: 9.650166248320602e-06
Step: 33910, train/epoch: 8.069966316223145
Step: 33920, train/loss: 0.0
Step: 33920, train/grad_norm: 0.00037813320523127913
Step: 33920, train/learning_rate: 9.638267329137307e-06
Step: 33920, train/epoch: 8.072346687316895
Step: 33930, train/loss: 0.00019999999494757503
Step: 33930, train/grad_norm: 6.670419679721817e-05
Step: 33930, train/learning_rate: 9.626368409954011e-06
Step: 33930, train/epoch: 8.074726104736328
Step: 33940, train/loss: 0.00019999999494757503
Step: 33940, train/grad_norm: 0.0008946304442360997
Step: 33940, train/learning_rate: 9.614469490770716e-06
Step: 33940, train/epoch: 8.077106475830078
Step: 33950, train/loss: 0.0
Step: 33950, train/grad_norm: 0.0033425164874643087
Step: 33950, train/learning_rate: 9.602570571587421e-06
Step: 33950, train/epoch: 8.079485893249512
Step: 33960, train/loss: 0.0
Step: 33960, train/grad_norm: 0.0001401232002535835
Step: 33960, train/learning_rate: 9.590670742909424e-06
Step: 33960, train/epoch: 8.081865310668945
Step: 33970, train/loss: 0.043299999088048935
Step: 33970, train/grad_norm: 3.504347478155978e-05
Step: 33970, train/learning_rate: 9.578771823726129e-06
Step: 33970, train/epoch: 8.084245681762695
Step: 33980, train/loss: 9.999999747378752e-05
Step: 33980, train/grad_norm: 0.00137210707180202
Step: 33980, train/learning_rate: 9.566872904542834e-06
Step: 33980, train/epoch: 8.086625099182129
Step: 33990, train/loss: 0.0
Step: 33990, train/grad_norm: 0.00016583754040766507
Step: 33990, train/learning_rate: 9.554973985359538e-06
Step: 33990, train/epoch: 8.089005470275879
Step: 34000, train/loss: 0.0
Step: 34000, train/grad_norm: 7.170451863203198e-05
Step: 34000, train/learning_rate: 9.543075066176243e-06
Step: 34000, train/epoch: 8.091384887695312
Step: 34010, train/loss: 0.0
Step: 34010, train/grad_norm: 0.00014695648860652
Step: 34010, train/learning_rate: 9.531175237498246e-06
Step: 34010, train/epoch: 8.093765258789062
Step: 34020, train/loss: 0.0003000000142492354
Step: 34020, train/grad_norm: 0.0002949699992313981
Step: 34020, train/learning_rate: 9.519276318314951e-06
Step: 34020, train/epoch: 8.096144676208496
Step: 34030, train/loss: 0.002300000051036477
Step: 34030, train/grad_norm: 0.00012000864080619067
Step: 34030, train/learning_rate: 9.507377399131656e-06
Step: 34030, train/epoch: 8.09852409362793
Step: 34040, train/loss: 0.02459999918937683
Step: 34040, train/grad_norm: 9.421394497621804e-05
Step: 34040, train/learning_rate: 9.49547847994836e-06
Step: 34040, train/epoch: 8.10090446472168
Step: 34050, train/loss: 0.0
Step: 34050, train/grad_norm: 0.06595644354820251
Step: 34050, train/learning_rate: 9.483579560765065e-06
Step: 34050, train/epoch: 8.103283882141113
Step: 34060, train/loss: 0.0
Step: 34060, train/grad_norm: 0.00015383328718598932
Step: 34060, train/learning_rate: 9.471679732087068e-06
Step: 34060, train/epoch: 8.105664253234863
Step: 34070, train/loss: 0.006000000052154064
Step: 34070, train/grad_norm: 0.00017026159912347794
Step: 34070, train/learning_rate: 9.459780812903773e-06
Step: 34070, train/epoch: 8.108043670654297
Step: 34080, train/loss: 0.022700000554323196
Step: 34080, train/grad_norm: 1.6554069588892162e-05
Step: 34080, train/learning_rate: 9.447881893720478e-06
Step: 34080, train/epoch: 8.110424041748047
Step: 34090, train/loss: 0.00039999998989515007
Step: 34090, train/grad_norm: 0.0005720924818888307
Step: 34090, train/learning_rate: 9.435982974537183e-06
Step: 34090, train/epoch: 8.11280345916748
Step: 34100, train/loss: 9.999999747378752e-05
Step: 34100, train/grad_norm: 0.011534304358065128
Step: 34100, train/learning_rate: 9.424084055353887e-06
Step: 34100, train/epoch: 8.115182876586914
Step: 34110, train/loss: 0.03909999877214432
Step: 34110, train/grad_norm: 0.03665362298488617
Step: 34110, train/learning_rate: 9.41218422667589e-06
Step: 34110, train/epoch: 8.117563247680664
Step: 34120, train/loss: 0.00039999998989515007
Step: 34120, train/grad_norm: 5.922911441302858e-05
Step: 34120, train/learning_rate: 9.400285307492595e-06
Step: 34120, train/epoch: 8.119942665100098
Step: 34130, train/loss: 0.0
Step: 34130, train/grad_norm: 3.4880315070040524e-05
Step: 34130, train/learning_rate: 9.3883863883093e-06
Step: 34130, train/epoch: 8.122323036193848
Step: 34140, train/loss: 0.0
Step: 34140, train/grad_norm: 0.00010294538515154272
Step: 34140, train/learning_rate: 9.376487469126005e-06
Step: 34140, train/epoch: 8.124702453613281
Step: 34150, train/loss: 0.09459999948740005
Step: 34150, train/grad_norm: 0.0006077299476601183
Step: 34150, train/learning_rate: 9.36458854994271e-06
Step: 34150, train/epoch: 8.127081871032715
Step: 34160, train/loss: 0.0
Step: 34160, train/grad_norm: 0.00024641992058604956
Step: 34160, train/learning_rate: 9.352689630759414e-06
Step: 34160, train/epoch: 8.129462242126465
Step: 34170, train/loss: 0.0
Step: 34170, train/grad_norm: 4.193412678432651e-05
Step: 34170, train/learning_rate: 9.340789802081417e-06
Step: 34170, train/epoch: 8.131841659545898
Step: 34180, train/loss: 0.0
Step: 34180, train/grad_norm: 6.526114157168195e-05
Step: 34180, train/learning_rate: 9.328890882898122e-06
Step: 34180, train/epoch: 8.134222030639648
Step: 34190, train/loss: 0.00139999995008111
Step: 34190, train/grad_norm: 0.00023505391436628997
Step: 34190, train/learning_rate: 9.316991963714827e-06
Step: 34190, train/epoch: 8.136601448059082
Step: 34200, train/loss: 0.0008999999845400453
Step: 34200, train/grad_norm: 0.0001828634412959218
Step: 34200, train/learning_rate: 9.305093044531532e-06
Step: 34200, train/epoch: 8.138981819152832
Step: 34210, train/loss: 0.11100000143051147
Step: 34210, train/grad_norm: 0.0005100013222545385
Step: 34210, train/learning_rate: 9.293194125348236e-06
Step: 34210, train/epoch: 8.141361236572266
Step: 34220, train/loss: 0.003000000026077032
Step: 34220, train/grad_norm: 7.002447091508657e-05
Step: 34220, train/learning_rate: 9.28129429667024e-06
Step: 34220, train/epoch: 8.1437406539917
Step: 34230, train/loss: 9.999999747378752e-05
Step: 34230, train/grad_norm: 0.0010270945494994521
Step: 34230, train/learning_rate: 9.269395377486944e-06
Step: 34230, train/epoch: 8.14612102508545
Step: 34240, train/loss: 9.999999747378752e-05
Step: 34240, train/grad_norm: 0.0003967312804888934
Step: 34240, train/learning_rate: 9.257496458303649e-06
Step: 34240, train/epoch: 8.148500442504883
Step: 34250, train/loss: 0.010700000450015068
Step: 34250, train/grad_norm: 29.273366928100586
Step: 34250, train/learning_rate: 9.245597539120354e-06
Step: 34250, train/epoch: 8.150880813598633
Step: 34260, train/loss: 0.0
Step: 34260, train/grad_norm: 2.8877659133286215e-05
Step: 34260, train/learning_rate: 9.233698619937059e-06
Step: 34260, train/epoch: 8.153260231018066
Step: 34270, train/loss: 0.0
Step: 34270, train/grad_norm: 0.00034692822373472154
Step: 34270, train/learning_rate: 9.221798791259062e-06
Step: 34270, train/epoch: 8.155640602111816
Step: 34280, train/loss: 9.999999747378752e-05
Step: 34280, train/grad_norm: 2.971101366711082e-06
Step: 34280, train/learning_rate: 9.209899872075766e-06
Step: 34280, train/epoch: 8.15802001953125
Step: 34290, train/loss: 0.0
Step: 34290, train/grad_norm: 0.01166272722184658
Step: 34290, train/learning_rate: 9.198000952892471e-06
Step: 34290, train/epoch: 8.160399436950684
Step: 34300, train/loss: 0.0
Step: 34300, train/grad_norm: 0.0009021268924698234
Step: 34300, train/learning_rate: 9.186102033709176e-06
Step: 34300, train/epoch: 8.162779808044434
Step: 34310, train/loss: 0.0
Step: 34310, train/grad_norm: 3.269390435889363e-05
Step: 34310, train/learning_rate: 9.17420311452588e-06
Step: 34310, train/epoch: 8.165159225463867
Step: 34320, train/loss: 0.0
Step: 34320, train/grad_norm: 0.0008694156422279775
Step: 34320, train/learning_rate: 9.162303285847884e-06
Step: 34320, train/epoch: 8.167539596557617
Step: 34330, train/loss: 0.07249999791383743
Step: 34330, train/grad_norm: 3.3239834010601044e-05
Step: 34330, train/learning_rate: 9.150404366664588e-06
Step: 34330, train/epoch: 8.16991901397705
Step: 34340, train/loss: 9.999999747378752e-05
Step: 34340, train/grad_norm: 0.00034150652936659753
Step: 34340, train/learning_rate: 9.138505447481293e-06
Step: 34340, train/epoch: 8.172298431396484
Step: 34350, train/loss: 0.0
Step: 34350, train/grad_norm: 0.00017174937238451093
Step: 34350, train/learning_rate: 9.126606528297998e-06
Step: 34350, train/epoch: 8.174678802490234
Step: 34360, train/loss: 9.999999747378752e-05
Step: 34360, train/grad_norm: 0.2621457874774933
Step: 34360, train/learning_rate: 9.114707609114703e-06
Step: 34360, train/epoch: 8.177058219909668
Step: 34370, train/loss: 0.0005000000237487257
Step: 34370, train/grad_norm: 0.0006236623739823699
Step: 34370, train/learning_rate: 9.102807780436706e-06
Step: 34370, train/epoch: 8.179438591003418
Step: 34380, train/loss: 9.999999747378752e-05
Step: 34380, train/grad_norm: 0.00986532587558031
Step: 34380, train/learning_rate: 9.09090886125341e-06
Step: 34380, train/epoch: 8.181818008422852
Step: 34390, train/loss: 0.0024999999441206455
Step: 34390, train/grad_norm: 0.00013195499195717275
Step: 34390, train/learning_rate: 9.079009942070115e-06
Step: 34390, train/epoch: 8.184198379516602
Step: 34400, train/loss: 0.0003000000142492354
Step: 34400, train/grad_norm: 0.0009422885486856103
Step: 34400, train/learning_rate: 9.06711102288682e-06
Step: 34400, train/epoch: 8.186577796936035
Step: 34410, train/loss: 0.0
Step: 34410, train/grad_norm: 0.008646911941468716
Step: 34410, train/learning_rate: 9.055212103703525e-06
Step: 34410, train/epoch: 8.188957214355469
Step: 34420, train/loss: 0.0
Step: 34420, train/grad_norm: 0.0006886338233016431
Step: 34420, train/learning_rate: 9.043312275025528e-06
Step: 34420, train/epoch: 8.191337585449219
Step: 34430, train/loss: 0.0012000000569969416
Step: 34430, train/grad_norm: 6.12566655036062e-05
Step: 34430, train/learning_rate: 9.031413355842233e-06
Step: 34430, train/epoch: 8.193717002868652
Step: 34440, train/loss: 0.0005000000237487257
Step: 34440, train/grad_norm: 0.008077087812125683
Step: 34440, train/learning_rate: 9.019514436658937e-06
Step: 34440, train/epoch: 8.196097373962402
Step: 34450, train/loss: 0.002400000113993883
Step: 34450, train/grad_norm: 15.645129203796387
Step: 34450, train/learning_rate: 9.007615517475642e-06
Step: 34450, train/epoch: 8.198476791381836
Step: 34460, train/loss: 0.0
Step: 34460, train/grad_norm: 0.002415528753772378
Step: 34460, train/learning_rate: 8.995716598292347e-06
Step: 34460, train/epoch: 8.200857162475586
Step: 34470, train/loss: 0.00039999998989515007
Step: 34470, train/grad_norm: 0.00014828481653239578
Step: 34470, train/learning_rate: 8.983817679109052e-06
Step: 34470, train/epoch: 8.20323657989502
Step: 34480, train/loss: 0.006200000178068876
Step: 34480, train/grad_norm: 0.0007220261031761765
Step: 34480, train/learning_rate: 8.971917850431055e-06
Step: 34480, train/epoch: 8.205615997314453
Step: 34490, train/loss: 0.0019000000320374966
Step: 34490, train/grad_norm: 1.951013291545678e-05
Step: 34490, train/learning_rate: 8.96001893124776e-06
Step: 34490, train/epoch: 8.207996368408203
Step: 34500, train/loss: 0.0
Step: 34500, train/grad_norm: 0.12463930994272232
Step: 34500, train/learning_rate: 8.948120012064464e-06
Step: 34500, train/epoch: 8.210375785827637
Step: 34510, train/loss: 0.0
Step: 34510, train/grad_norm: 4.296211773180403e-05
Step: 34510, train/learning_rate: 8.93622109288117e-06
Step: 34510, train/epoch: 8.212756156921387
Step: 34520, train/loss: 0.0
Step: 34520, train/grad_norm: 0.0001881140487967059
Step: 34520, train/learning_rate: 8.924322173697874e-06
Step: 34520, train/epoch: 8.21513557434082
Step: 34530, train/loss: 0.0
Step: 34530, train/grad_norm: 0.0007615507929585874
Step: 34530, train/learning_rate: 8.912422345019877e-06
Step: 34530, train/epoch: 8.21751594543457
Step: 34540, train/loss: 9.999999747378752e-05
Step: 34540, train/grad_norm: 0.013002933003008366
Step: 34540, train/learning_rate: 8.900523425836582e-06
Step: 34540, train/epoch: 8.219895362854004
Step: 34550, train/loss: 0.0
Step: 34550, train/grad_norm: 0.00010959344945149496
Step: 34550, train/learning_rate: 8.888624506653287e-06
Step: 34550, train/epoch: 8.222274780273438
Step: 34560, train/loss: 9.999999747378752e-05
Step: 34560, train/grad_norm: 0.0003452385135460645
Step: 34560, train/learning_rate: 8.876725587469991e-06
Step: 34560, train/epoch: 8.224655151367188
Step: 34570, train/loss: 0.00039999998989515007
Step: 34570, train/grad_norm: 0.0032246855553239584
Step: 34570, train/learning_rate: 8.864826668286696e-06
Step: 34570, train/epoch: 8.227034568786621
Step: 34580, train/loss: 0.000699999975040555
Step: 34580, train/grad_norm: 3.822855234146118
Step: 34580, train/learning_rate: 8.852926839608699e-06
Step: 34580, train/epoch: 8.229414939880371
Step: 34590, train/loss: 0.0
Step: 34590, train/grad_norm: 0.00042267702519893646
Step: 34590, train/learning_rate: 8.841027920425404e-06
Step: 34590, train/epoch: 8.231794357299805
Step: 34600, train/loss: 0.0
Step: 34600, train/grad_norm: 0.002461449708789587
Step: 34600, train/learning_rate: 8.829129001242109e-06
Step: 34600, train/epoch: 8.234173774719238
Step: 34610, train/loss: 0.05429999902844429
Step: 34610, train/grad_norm: 8.801354124443606e-05
Step: 34610, train/learning_rate: 8.817230082058813e-06
Step: 34610, train/epoch: 8.236554145812988
Step: 34620, train/loss: 0.0
Step: 34620, train/grad_norm: 0.00014865408593323082
Step: 34620, train/learning_rate: 8.805331162875518e-06
Step: 34620, train/epoch: 8.238933563232422
Step: 34630, train/loss: 0.0
Step: 34630, train/grad_norm: 1.6218571545323357e-05
Step: 34630, train/learning_rate: 8.793431334197521e-06
Step: 34630, train/epoch: 8.241313934326172
Step: 34640, train/loss: 0.0
Step: 34640, train/grad_norm: 0.0004988873261027038
Step: 34640, train/learning_rate: 8.781532415014226e-06
Step: 34640, train/epoch: 8.243693351745605
Step: 34650, train/loss: 0.002300000051036477
Step: 34650, train/grad_norm: 1.6253321518888697e-05
Step: 34650, train/learning_rate: 8.76963349583093e-06
Step: 34650, train/epoch: 8.246073722839355
Step: 34660, train/loss: 0.0
Step: 34660, train/grad_norm: 3.3890952181536704e-05
Step: 34660, train/learning_rate: 8.757734576647636e-06
Step: 34660, train/epoch: 8.248453140258789
Step: 34670, train/loss: 0.0
Step: 34670, train/grad_norm: 0.00024678721092641354
Step: 34670, train/learning_rate: 8.74583565746434e-06
Step: 34670, train/epoch: 8.250832557678223
Step: 34680, train/loss: 0.0003000000142492354
Step: 34680, train/grad_norm: 4.154294583713636e-05
Step: 34680, train/learning_rate: 8.733935828786343e-06
Step: 34680, train/epoch: 8.253212928771973
Step: 34690, train/loss: 0.0
Step: 34690, train/grad_norm: 5.5765143770258874e-05
Step: 34690, train/learning_rate: 8.722036909603048e-06
Step: 34690, train/epoch: 8.255592346191406
Step: 34700, train/loss: 0.0026000000070780516
Step: 34700, train/grad_norm: 0.0003302897384855896
Step: 34700, train/learning_rate: 8.710137990419753e-06
Step: 34700, train/epoch: 8.257972717285156
Step: 34710, train/loss: 0.0
Step: 34710, train/grad_norm: 3.919170194421895e-05
Step: 34710, train/learning_rate: 8.698239071236458e-06
Step: 34710, train/epoch: 8.26035213470459
Step: 34720, train/loss: 0.0007999999797903001
Step: 34720, train/grad_norm: 3.9083945751190186
Step: 34720, train/learning_rate: 8.686340152053162e-06
Step: 34720, train/epoch: 8.26273250579834
Step: 34730, train/loss: 0.0
Step: 34730, train/grad_norm: 3.212894080206752e-05
Step: 34730, train/learning_rate: 8.674440323375165e-06
Step: 34730, train/epoch: 8.265111923217773
Step: 34740, train/loss: 9.999999747378752e-05
Step: 34740, train/grad_norm: 0.0022509091068059206
Step: 34740, train/learning_rate: 8.66254140419187e-06
Step: 34740, train/epoch: 8.267491340637207
Step: 34750, train/loss: 0.25859999656677246
Step: 34750, train/grad_norm: 8.900518878363073e-06
Step: 34750, train/learning_rate: 8.650642485008575e-06
Step: 34750, train/epoch: 8.269871711730957
Step: 34760, train/loss: 0.1257999986410141
Step: 34760, train/grad_norm: 0.0015761185204610229
Step: 34760, train/learning_rate: 8.63874356582528e-06
Step: 34760, train/epoch: 8.27225112915039
Step: 34770, train/loss: 0.0010000000474974513
Step: 34770, train/grad_norm: 0.00010409578680992126
Step: 34770, train/learning_rate: 8.626844646641985e-06
Step: 34770, train/epoch: 8.27463150024414
Step: 34780, train/loss: 0.0471000000834465
Step: 34780, train/grad_norm: 0.18647755682468414
Step: 34780, train/learning_rate: 8.614944817963988e-06
Step: 34780, train/epoch: 8.277010917663574
Step: 34790, train/loss: 0.08479999750852585
Step: 34790, train/grad_norm: 0.00011700421600835398
Step: 34790, train/learning_rate: 8.603045898780692e-06
Step: 34790, train/epoch: 8.279390335083008
Step: 34800, train/loss: 0.0003000000142492354
Step: 34800, train/grad_norm: 1.7394415140151978
Step: 34800, train/learning_rate: 8.591146979597397e-06
Step: 34800, train/epoch: 8.281770706176758
Step: 34810, train/loss: 0.009100000374019146
Step: 34810, train/grad_norm: 1.1230196832912043e-05
Step: 34810, train/learning_rate: 8.579248060414102e-06
Step: 34810, train/epoch: 8.284150123596191
Step: 34820, train/loss: 0.00019999999494757503
Step: 34820, train/grad_norm: 0.00043271639151498675
Step: 34820, train/learning_rate: 8.567349141230807e-06
Step: 34820, train/epoch: 8.286530494689941
Step: 34830, train/loss: 0.0
Step: 34830, train/grad_norm: 0.000478076544823125
Step: 34830, train/learning_rate: 8.555450222047511e-06
Step: 34830, train/epoch: 8.288909912109375
Step: 34840, train/loss: 0.24379999935626984
Step: 34840, train/grad_norm: 1.5171677659964189e-05
Step: 34840, train/learning_rate: 8.543550393369514e-06
Step: 34840, train/epoch: 8.291290283203125
Step: 34850, train/loss: 0.0
Step: 34850, train/grad_norm: 0.002572489669546485
Step: 34850, train/learning_rate: 8.53165147418622e-06
Step: 34850, train/epoch: 8.293669700622559
Step: 34860, train/loss: 0.08990000188350677
Step: 34860, train/grad_norm: 60.77625274658203
Step: 34860, train/learning_rate: 8.519752555002924e-06
Step: 34860, train/epoch: 8.296049118041992
Step: 34870, train/loss: 9.999999747378752e-05
Step: 34870, train/grad_norm: 0.00010555267363088205
Step: 34870, train/learning_rate: 8.507853635819629e-06
Step: 34870, train/epoch: 8.298429489135742
Step: 34880, train/loss: 9.999999747378752e-05
Step: 34880, train/grad_norm: 0.003559389617294073
Step: 34880, train/learning_rate: 8.495954716636334e-06
Step: 34880, train/epoch: 8.300808906555176
Step: 34890, train/loss: 0.0
Step: 34890, train/grad_norm: 0.0007747563067823648
Step: 34890, train/learning_rate: 8.484054887958337e-06
Step: 34890, train/epoch: 8.303189277648926
Step: 34900, train/loss: 0.0333000011742115
Step: 34900, train/grad_norm: 5.867458457942121e-05
Step: 34900, train/learning_rate: 8.472155968775041e-06
Step: 34900, train/epoch: 8.30556869506836
Step: 34910, train/loss: 9.999999747378752e-05
Step: 34910, train/grad_norm: 0.6939335465431213
Step: 34910, train/learning_rate: 8.460257049591746e-06
Step: 34910, train/epoch: 8.30794906616211
Step: 34920, train/loss: 0.015699999406933784
Step: 34920, train/grad_norm: 0.00298985349945724
Step: 34920, train/learning_rate: 8.448358130408451e-06
Step: 34920, train/epoch: 8.310328483581543
Step: 34930, train/loss: 0.0
Step: 34930, train/grad_norm: 0.0008909663883969188
Step: 34930, train/learning_rate: 8.436459211225156e-06
Step: 34930, train/epoch: 8.312707901000977
Step: 34940, train/loss: 9.999999747378752e-05
Step: 34940, train/grad_norm: 0.006254964973777533
Step: 34940, train/learning_rate: 8.424559382547159e-06
Step: 34940, train/epoch: 8.315088272094727
Step: 34950, train/loss: 0.00559999980032444
Step: 34950, train/grad_norm: 0.0006515122368000448
Step: 34950, train/learning_rate: 8.412660463363864e-06
Step: 34950, train/epoch: 8.31746768951416
Step: 34960, train/loss: 9.999999747378752e-05
Step: 34960, train/grad_norm: 0.01627848856151104
Step: 34960, train/learning_rate: 8.400761544180568e-06
Step: 34960, train/epoch: 8.31984806060791
Step: 34970, train/loss: 0.0
Step: 34970, train/grad_norm: 0.004349268041551113
Step: 34970, train/learning_rate: 8.388862624997273e-06
Step: 34970, train/epoch: 8.322227478027344
Step: 34980, train/loss: 0.00019999999494757503
Step: 34980, train/grad_norm: 0.7107332944869995
Step: 34980, train/learning_rate: 8.376963705813978e-06
Step: 34980, train/epoch: 8.324606895446777
Step: 34990, train/loss: 0.00019999999494757503
Step: 34990, train/grad_norm: 0.0014527837047353387
Step: 34990, train/learning_rate: 8.365063877135981e-06
Step: 34990, train/epoch: 8.326987266540527
Step: 35000, train/loss: 0.04470000043511391
Step: 35000, train/grad_norm: 0.00015361691475845873
Step: 35000, train/learning_rate: 8.353164957952686e-06
Step: 35000, train/epoch: 8.329366683959961
Step: 35010, train/loss: 0.0
Step: 35010, train/grad_norm: 4.831661135540344e-05
Step: 35010, train/learning_rate: 8.34126603876939e-06
Step: 35010, train/epoch: 8.331747055053711
Step: 35020, train/loss: 0.013000000268220901
Step: 35020, train/grad_norm: 0.0013350534718483686
Step: 35020, train/learning_rate: 8.329367119586095e-06
Step: 35020, train/epoch: 8.334126472473145
Step: 35030, train/loss: 0.0
Step: 35030, train/grad_norm: 0.042030442506074905
Step: 35030, train/learning_rate: 8.3174682004028e-06
Step: 35030, train/epoch: 8.336506843566895
Step: 35040, train/loss: 9.999999747378752e-05
Step: 35040, train/grad_norm: 0.00028519489569589496
Step: 35040, train/learning_rate: 8.305568371724803e-06
Step: 35040, train/epoch: 8.338886260986328
Step: 35050, train/loss: 0.0
Step: 35050, train/grad_norm: 0.00030249665724113584
Step: 35050, train/learning_rate: 8.293669452541508e-06
Step: 35050, train/epoch: 8.341265678405762
Step: 35060, train/loss: 0.094200000166893
Step: 35060, train/grad_norm: 0.00016678741667419672
Step: 35060, train/learning_rate: 8.281770533358213e-06
Step: 35060, train/epoch: 8.343646049499512
Step: 35070, train/loss: 0.0
Step: 35070, train/grad_norm: 0.0012667321134358644
Step: 35070, train/learning_rate: 8.269871614174917e-06
Step: 35070, train/epoch: 8.346025466918945
Step: 35080, train/loss: 0.00039999998989515007
Step: 35080, train/grad_norm: 0.00024617230519652367
Step: 35080, train/learning_rate: 8.257972694991622e-06
Step: 35080, train/epoch: 8.348405838012695
Step: 35090, train/loss: 0.00430000014603138
Step: 35090, train/grad_norm: 0.00013266989844851196
Step: 35090, train/learning_rate: 8.246072866313625e-06
Step: 35090, train/epoch: 8.350785255432129
Step: 35100, train/loss: 0.0
Step: 35100, train/grad_norm: 8.773148874752223e-05
Step: 35100, train/learning_rate: 8.23417394713033e-06
Step: 35100, train/epoch: 8.353165626525879
Step: 35110, train/loss: 0.004999999888241291
Step: 35110, train/grad_norm: 0.0021273107267916203
Step: 35110, train/learning_rate: 8.222275027947035e-06
Step: 35110, train/epoch: 8.355545043945312
Step: 35120, train/loss: 0.0
Step: 35120, train/grad_norm: 0.007893879897892475
Step: 35120, train/learning_rate: 8.21037610876374e-06
Step: 35120, train/epoch: 8.357924461364746
Step: 35130, train/loss: 9.999999747378752e-05
Step: 35130, train/grad_norm: 2.4390883481828496e-05
Step: 35130, train/learning_rate: 8.198477189580444e-06
Step: 35130, train/epoch: 8.360304832458496
Step: 35140, train/loss: 0.023600000888109207
Step: 35140, train/grad_norm: 5.205392153584398e-05
Step: 35140, train/learning_rate: 8.186578270397149e-06
Step: 35140, train/epoch: 8.36268424987793
Step: 35150, train/loss: 0.0
Step: 35150, train/grad_norm: 0.001990046352148056
Step: 35150, train/learning_rate: 8.174678441719152e-06
Step: 35150, train/epoch: 8.36506462097168
Step: 35160, train/loss: 0.03689999878406525
Step: 35160, train/grad_norm: 0.00026808568509295583
Step: 35160, train/learning_rate: 8.162779522535857e-06
Step: 35160, train/epoch: 8.367444038391113
Step: 35170, train/loss: 0.0
Step: 35170, train/grad_norm: 0.00027334640617482364
Step: 35170, train/learning_rate: 8.150880603352562e-06
Step: 35170, train/epoch: 8.369823455810547
Step: 35180, train/loss: 0.0
Step: 35180, train/grad_norm: 0.001032409374602139
Step: 35180, train/learning_rate: 8.138981684169266e-06
Step: 35180, train/epoch: 8.372203826904297
Step: 35190, train/loss: 0.0010000000474974513
Step: 35190, train/grad_norm: 0.00042738657793961465
Step: 35190, train/learning_rate: 8.127082764985971e-06
Step: 35190, train/epoch: 8.37458324432373
Step: 35200, train/loss: 0.0
Step: 35200, train/grad_norm: 0.00016763336316216737
Step: 35200, train/learning_rate: 8.115182936307974e-06
Step: 35200, train/epoch: 8.37696361541748
Step: 35210, train/loss: 0.0006000000284984708
Step: 35210, train/grad_norm: 3.400686025619507
Step: 35210, train/learning_rate: 8.103284017124679e-06
Step: 35210, train/epoch: 8.379343032836914
Step: 35220, train/loss: 0.0008999999845400453
Step: 35220, train/grad_norm: 0.020072270184755325
Step: 35220, train/learning_rate: 8.091385097941384e-06
Step: 35220, train/epoch: 8.381723403930664
Step: 35230, train/loss: 9.999999747378752e-05
Step: 35230, train/grad_norm: 4.5203647459857166e-05
Step: 35230, train/learning_rate: 8.079486178758088e-06
Step: 35230, train/epoch: 8.384102821350098
Step: 35240, train/loss: 0.0015999999595806003
Step: 35240, train/grad_norm: 0.0006764081190340221
Step: 35240, train/learning_rate: 8.067587259574793e-06
Step: 35240, train/epoch: 8.386482238769531
Step: 35250, train/loss: 0.0
Step: 35250, train/grad_norm: 0.010945559479296207
Step: 35250, train/learning_rate: 8.055687430896796e-06
Step: 35250, train/epoch: 8.388862609863281
Step: 35260, train/loss: 0.0
Step: 35260, train/grad_norm: 4.371016621007584e-05
Step: 35260, train/learning_rate: 8.043788511713501e-06
Step: 35260, train/epoch: 8.391242027282715
Step: 35270, train/loss: 9.999999747378752e-05
Step: 35270, train/grad_norm: 0.0070426887832582
Step: 35270, train/learning_rate: 8.031889592530206e-06
Step: 35270, train/epoch: 8.393622398376465
Step: 35280, train/loss: 0.0006000000284984708
Step: 35280, train/grad_norm: 2.054772267001681e-05
Step: 35280, train/learning_rate: 8.01999067334691e-06
Step: 35280, train/epoch: 8.396001815795898
Step: 35290, train/loss: 0.0
Step: 35290, train/grad_norm: 7.869178807595745e-05
Step: 35290, train/learning_rate: 8.008091754163615e-06
Step: 35290, train/epoch: 8.398382186889648
Step: 35300, train/loss: 0.0
Step: 35300, train/grad_norm: 3.787829336943105e-05
Step: 35300, train/learning_rate: 7.996191925485618e-06
Step: 35300, train/epoch: 8.400761604309082
Step: 35310, train/loss: 0.0
Step: 35310, train/grad_norm: 2.0759274775628e-05
Step: 35310, train/learning_rate: 7.984293006302323e-06
Step: 35310, train/epoch: 8.403141021728516
Step: 35320, train/loss: 0.002199999988079071
Step: 35320, train/grad_norm: 1.0618956366670318e-05
Step: 35320, train/learning_rate: 7.972394087119028e-06
Step: 35320, train/epoch: 8.405521392822266
Step: 35330, train/loss: 0.034699998795986176
Step: 35330, train/grad_norm: 79.82475280761719
Step: 35330, train/learning_rate: 7.960495167935733e-06
Step: 35330, train/epoch: 8.4079008102417
Step: 35340, train/loss: 0.006000000052154064
Step: 35340, train/grad_norm: 9.31286922423169e-05
Step: 35340, train/learning_rate: 7.948596248752438e-06
Step: 35340, train/epoch: 8.41028118133545
Step: 35350, train/loss: 0.0
Step: 35350, train/grad_norm: 0.026234542950987816
Step: 35350, train/learning_rate: 7.93669642007444e-06
Step: 35350, train/epoch: 8.412660598754883
Step: 35360, train/loss: 0.019099999219179153
Step: 35360, train/grad_norm: 23.0408935546875
Step: 35360, train/learning_rate: 7.924797500891145e-06
Step: 35360, train/epoch: 8.415040016174316
Step: 35370, train/loss: 0.0
Step: 35370, train/grad_norm: 0.002645525150001049
Step: 35370, train/learning_rate: 7.91289858170785e-06
Step: 35370, train/epoch: 8.417420387268066
Step: 35380, train/loss: 0.0003000000142492354
Step: 35380, train/grad_norm: 2.2402822971343994
Step: 35380, train/learning_rate: 7.900999662524555e-06
Step: 35380, train/epoch: 8.4197998046875
Step: 35390, train/loss: 0.0007999999797903001
Step: 35390, train/grad_norm: 7.856518268585205
Step: 35390, train/learning_rate: 7.88910074334126e-06
Step: 35390, train/epoch: 8.42218017578125
Step: 35400, train/loss: 0.0
Step: 35400, train/grad_norm: 0.009945591911673546
Step: 35400, train/learning_rate: 7.877200914663263e-06
Step: 35400, train/epoch: 8.424559593200684
Step: 35410, train/loss: 0.0
Step: 35410, train/grad_norm: 0.0001926938130054623
Step: 35410, train/learning_rate: 7.865301995479967e-06
Step: 35410, train/epoch: 8.426939964294434
Step: 35420, train/loss: 0.010700000450015068
Step: 35420, train/grad_norm: 2.0197619960526936e-05
Step: 35420, train/learning_rate: 7.853403076296672e-06
Step: 35420, train/epoch: 8.429319381713867
Step: 35430, train/loss: 9.999999747378752e-05
Step: 35430, train/grad_norm: 0.0018919719150289893
Step: 35430, train/learning_rate: 7.841504157113377e-06
Step: 35430, train/epoch: 8.4316987991333
Step: 35440, train/loss: 0.0
Step: 35440, train/grad_norm: 0.06541839241981506
Step: 35440, train/learning_rate: 7.829605237930082e-06
Step: 35440, train/epoch: 8.43407917022705
Step: 35450, train/loss: 0.02199999988079071
Step: 35450, train/grad_norm: 0.00045305900857783854
Step: 35450, train/learning_rate: 7.817705409252085e-06
Step: 35450, train/epoch: 8.436458587646484
Step: 35460, train/loss: 0.0007999999797903001
Step: 35460, train/grad_norm: 0.0005576742114499211
Step: 35460, train/learning_rate: 7.80580649006879e-06
Step: 35460, train/epoch: 8.438838958740234
Step: 35470, train/loss: 0.14920000731945038
Step: 35470, train/grad_norm: 2.4382281480939128e-05
Step: 35470, train/learning_rate: 7.793907570885494e-06
Step: 35470, train/epoch: 8.441218376159668
Step: 35480, train/loss: 0.10000000149011612
Step: 35480, train/grad_norm: 84.63734436035156
Step: 35480, train/learning_rate: 7.782008651702199e-06
Step: 35480, train/epoch: 8.443598747253418
Step: 35490, train/loss: 0.0008999999845400453
Step: 35490, train/grad_norm: 4.0012884710449725e-05
Step: 35490, train/learning_rate: 7.770109732518904e-06
Step: 35490, train/epoch: 8.445978164672852
Step: 35500, train/loss: 0.0
Step: 35500, train/grad_norm: 6.720754026900977e-05
Step: 35500, train/learning_rate: 7.758210813335609e-06
Step: 35500, train/epoch: 8.448357582092285
Step: 35510, train/loss: 0.0
Step: 35510, train/grad_norm: 6.766295700799674e-05
Step: 35510, train/learning_rate: 7.746310984657612e-06
Step: 35510, train/epoch: 8.450737953186035
Step: 35520, train/loss: 0.0
Step: 35520, train/grad_norm: 0.0008785749087110162
Step: 35520, train/learning_rate: 7.734412065474316e-06
Step: 35520, train/epoch: 8.453117370605469
Step: 35530, train/loss: 0.0034000000450760126
Step: 35530, train/grad_norm: 2.8361160730128177e-05
Step: 35530, train/learning_rate: 7.722513146291021e-06
Step: 35530, train/epoch: 8.455497741699219
Step: 35540, train/loss: 0.00039999998989515007
Step: 35540, train/grad_norm: 0.0001242894068127498
Step: 35540, train/learning_rate: 7.710614227107726e-06
Step: 35540, train/epoch: 8.457877159118652
Step: 35550, train/loss: 0.0
Step: 35550, train/grad_norm: 9.903530008159578e-05
Step: 35550, train/learning_rate: 7.69871530792443e-06
Step: 35550, train/epoch: 8.460256576538086
Step: 35560, train/loss: 9.999999747378752e-05
Step: 35560, train/grad_norm: 0.37309011816978455
Step: 35560, train/learning_rate: 7.686815479246434e-06
Step: 35560, train/epoch: 8.462636947631836
Step: 35570, train/loss: 0.0
Step: 35570, train/grad_norm: 7.669424667255953e-05
Step: 35570, train/learning_rate: 7.674916560063139e-06
Step: 35570, train/epoch: 8.46501636505127
Step: 35580, train/loss: 0.00019999999494757503
Step: 35580, train/grad_norm: 1.3198091437516268e-05
Step: 35580, train/learning_rate: 7.663017640879843e-06
Step: 35580, train/epoch: 8.46739673614502
Step: 35590, train/loss: 0.0
Step: 35590, train/grad_norm: 0.0006257590721361339
Step: 35590, train/learning_rate: 7.651118721696548e-06
Step: 35590, train/epoch: 8.469776153564453
Step: 35600, train/loss: 0.0
Step: 35600, train/grad_norm: 6.62494421703741e-05
Step: 35600, train/learning_rate: 7.639219802513253e-06
Step: 35600, train/epoch: 8.472156524658203
Step: 35610, train/loss: 0.0
Step: 35610, train/grad_norm: 0.0010751128429546952
Step: 35610, train/learning_rate: 7.627320428582607e-06
Step: 35610, train/epoch: 8.474535942077637
Step: 35620, train/loss: 0.0
Step: 35620, train/grad_norm: 2.0352616047603078e-05
Step: 35620, train/learning_rate: 7.615421054651961e-06
Step: 35620, train/epoch: 8.47691535949707
Step: 35630, train/loss: 0.010700000450015068
Step: 35630, train/grad_norm: 0.0002037839003605768
Step: 35630, train/learning_rate: 7.6035221354686655e-06
Step: 35630, train/epoch: 8.47929573059082
Step: 35640, train/loss: 0.0
Step: 35640, train/grad_norm: 0.0022040994372218847
Step: 35640, train/learning_rate: 7.59162321628537e-06
Step: 35640, train/epoch: 8.481675148010254
Step: 35650, train/loss: 0.0575999990105629
Step: 35650, train/grad_norm: 0.4564638137817383
Step: 35650, train/learning_rate: 7.579723842354724e-06
Step: 35650, train/epoch: 8.484055519104004
Step: 35660, train/loss: 0.0
Step: 35660, train/grad_norm: 6.55314142932184e-05
Step: 35660, train/learning_rate: 7.567824923171429e-06
Step: 35660, train/epoch: 8.486434936523438
Step: 35670, train/loss: 0.0
Step: 35670, train/grad_norm: 6.531373946927488e-05
Step: 35670, train/learning_rate: 7.555925549240783e-06
Step: 35670, train/epoch: 8.488815307617188
Step: 35680, train/loss: 0.0
Step: 35680, train/grad_norm: 0.0021246711257845163
Step: 35680, train/learning_rate: 7.544026630057488e-06
Step: 35680, train/epoch: 8.491194725036621
Step: 35690, train/loss: 0.003599999938160181
Step: 35690, train/grad_norm: 0.08831354230642319
Step: 35690, train/learning_rate: 7.532127710874192e-06
Step: 35690, train/epoch: 8.493574142456055
Step: 35700, train/loss: 0.00039999998989515007
Step: 35700, train/grad_norm: 7.587563595734537e-05
Step: 35700, train/learning_rate: 7.520228336943546e-06
Step: 35700, train/epoch: 8.495954513549805
Step: 35710, train/loss: 0.011300000362098217
Step: 35710, train/grad_norm: 0.024282347410917282
Step: 35710, train/learning_rate: 7.508329417760251e-06
Step: 35710, train/epoch: 8.498333930969238
Step: 35720, train/loss: 0.0008999999845400453
Step: 35720, train/grad_norm: 0.0005568815977312624
Step: 35720, train/learning_rate: 7.496430498576956e-06
Step: 35720, train/epoch: 8.500714302062988
Step: 35730, train/loss: 0.0
Step: 35730, train/grad_norm: 0.00014985330926720053
Step: 35730, train/learning_rate: 7.48453112464631e-06
Step: 35730, train/epoch: 8.503093719482422
Step: 35740, train/loss: 0.005799999926239252
Step: 35740, train/grad_norm: 0.00022009818349033594
Step: 35740, train/learning_rate: 7.4726322054630145e-06
Step: 35740, train/epoch: 8.505473136901855
Step: 35750, train/loss: 9.999999747378752e-05
Step: 35750, train/grad_norm: 2.9896164051024243e-05
Step: 35750, train/learning_rate: 7.4607328315323684e-06
Step: 35750, train/epoch: 8.507853507995605
Step: 35760, train/loss: 0.0
Step: 35760, train/grad_norm: 0.0813199058175087
Step: 35760, train/learning_rate: 7.448833912349073e-06
Step: 35760, train/epoch: 8.510232925415039
Step: 35770, train/loss: 0.003700000001117587
Step: 35770, train/grad_norm: 22.206026077270508
Step: 35770, train/learning_rate: 7.436934993165778e-06
Step: 35770, train/epoch: 8.512613296508789
Step: 35780, train/loss: 0.0
Step: 35780, train/grad_norm: 0.03708525747060776
Step: 35780, train/learning_rate: 7.425035619235132e-06
Step: 35780, train/epoch: 8.514992713928223
Step: 35790, train/loss: 0.0
Step: 35790, train/grad_norm: 1.4998213373473845e-05
Step: 35790, train/learning_rate: 7.413136700051837e-06
Step: 35790, train/epoch: 8.517373085021973
Step: 35800, train/loss: 0.0005000000237487257
Step: 35800, train/grad_norm: 1.2144019819970708e-05
Step: 35800, train/learning_rate: 7.4012373261211906e-06
Step: 35800, train/epoch: 8.519752502441406
Step: 35810, train/loss: 0.11410000175237656
Step: 35810, train/grad_norm: 2.979832061100751e-05
Step: 35810, train/learning_rate: 7.389338406937895e-06
Step: 35810, train/epoch: 8.52213191986084
Step: 35820, train/loss: 0.06759999692440033
Step: 35820, train/grad_norm: 44.87807846069336
Step: 35820, train/learning_rate: 7.3774394877546e-06
Step: 35820, train/epoch: 8.52451229095459
Step: 35830, train/loss: 0.0
Step: 35830, train/grad_norm: 0.030260175466537476
Step: 35830, train/learning_rate: 7.365540113823954e-06
Step: 35830, train/epoch: 8.526891708374023
Step: 35840, train/loss: 0.0015999999595806003
Step: 35840, train/grad_norm: 0.0006128782988525927
Step: 35840, train/learning_rate: 7.353641194640659e-06
Step: 35840, train/epoch: 8.529272079467773
Step: 35850, train/loss: 0.053199999034404755
Step: 35850, train/grad_norm: 1.1402853488107212e-05
Step: 35850, train/learning_rate: 7.341741820710013e-06
Step: 35850, train/epoch: 8.531651496887207
Step: 35860, train/loss: 0.0007999999797903001
Step: 35860, train/grad_norm: 0.0010837685549631715
Step: 35860, train/learning_rate: 7.3298429015267175e-06
Step: 35860, train/epoch: 8.534031867980957
Step: 35870, train/loss: 0.0
Step: 35870, train/grad_norm: 0.00032214526436291635
Step: 35870, train/learning_rate: 7.317943982343422e-06
Step: 35870, train/epoch: 8.53641128540039
Step: 35880, train/loss: 0.0
Step: 35880, train/grad_norm: 8.543724106857553e-05
Step: 35880, train/learning_rate: 7.306044608412776e-06
Step: 35880, train/epoch: 8.538790702819824
Step: 35890, train/loss: 0.0
Step: 35890, train/grad_norm: 0.02643648535013199
Step: 35890, train/learning_rate: 7.294145689229481e-06
Step: 35890, train/epoch: 8.541171073913574
Step: 35900, train/loss: 0.00019999999494757503
Step: 35900, train/grad_norm: 1.552120374981314e-05
Step: 35900, train/learning_rate: 7.282246770046186e-06
Step: 35900, train/epoch: 8.543550491333008
Step: 35910, train/loss: 0.0
Step: 35910, train/grad_norm: 0.0024817336816340685
Step: 35910, train/learning_rate: 7.27034739611554e-06
Step: 35910, train/epoch: 8.545930862426758
Step: 35920, train/loss: 0.0003000000142492354
Step: 35920, train/grad_norm: 1.0097823178512044e-05
Step: 35920, train/learning_rate: 7.258448476932244e-06
Step: 35920, train/epoch: 8.548310279846191
Step: 35930, train/loss: 0.0
Step: 35930, train/grad_norm: 2.2273543436313048e-05
Step: 35930, train/learning_rate: 7.246549103001598e-06
Step: 35930, train/epoch: 8.550689697265625
Step: 35940, train/loss: 0.0
Step: 35940, train/grad_norm: 0.0036956057883799076
Step: 35940, train/learning_rate: 7.234650183818303e-06
Step: 35940, train/epoch: 8.553070068359375
Step: 35950, train/loss: 0.0
Step: 35950, train/grad_norm: 1.3889825822843704e-05
Step: 35950, train/learning_rate: 7.222751264635008e-06
Step: 35950, train/epoch: 8.555449485778809
Step: 35960, train/loss: 0.0
Step: 35960, train/grad_norm: 0.00017436630150768906
Step: 35960, train/learning_rate: 7.210851890704362e-06
Step: 35960, train/epoch: 8.557829856872559
Step: 35970, train/loss: 0.0
Step: 35970, train/grad_norm: 0.0005020195967517793
Step: 35970, train/learning_rate: 7.1989529715210665e-06
Step: 35970, train/epoch: 8.560209274291992
Step: 35980, train/loss: 0.0
Step: 35980, train/grad_norm: 5.4667852964485064e-05
Step: 35980, train/learning_rate: 7.18705359759042e-06
Step: 35980, train/epoch: 8.562589645385742
Step: 35990, train/loss: 0.0
Step: 35990, train/grad_norm: 0.00014297511370386928
Step: 35990, train/learning_rate: 7.175154678407125e-06
Step: 35990, train/epoch: 8.564969062805176
Step: 36000, train/loss: 0.0
Step: 36000, train/grad_norm: 0.00046004803152754903
Step: 36000, train/learning_rate: 7.16325575922383e-06
Step: 36000, train/epoch: 8.56734848022461
Step: 36010, train/loss: 0.0012000000569969416
Step: 36010, train/grad_norm: 0.0003366531745996326
Step: 36010, train/learning_rate: 7.151356385293184e-06
Step: 36010, train/epoch: 8.56972885131836
Step: 36020, train/loss: 0.0
Step: 36020, train/grad_norm: 0.00025115173775702715
Step: 36020, train/learning_rate: 7.139457466109889e-06
Step: 36020, train/epoch: 8.572108268737793
Step: 36030, train/loss: 9.999999747378752e-05
Step: 36030, train/grad_norm: 0.006751460954546928
Step: 36030, train/learning_rate: 7.1275580921792425e-06
Step: 36030, train/epoch: 8.574488639831543
Step: 36040, train/loss: 0.0
Step: 36040, train/grad_norm: 0.00889675784856081
Step: 36040, train/learning_rate: 7.115659172995947e-06
Step: 36040, train/epoch: 8.576868057250977
Step: 36050, train/loss: 0.0
Step: 36050, train/grad_norm: 0.0002097538235830143
Step: 36050, train/learning_rate: 7.103760253812652e-06
Step: 36050, train/epoch: 8.579248428344727
Step: 36060, train/loss: 0.0
Step: 36060, train/grad_norm: 0.00015573915152344853
Step: 36060, train/learning_rate: 7.091860879882006e-06
Step: 36060, train/epoch: 8.58162784576416
Step: 36070, train/loss: 0.0
Step: 36070, train/grad_norm: 2.5464503778493963e-05
Step: 36070, train/learning_rate: 7.079961960698711e-06
Step: 36070, train/epoch: 8.584007263183594
Step: 36080, train/loss: 0.0003000000142492354
Step: 36080, train/grad_norm: 1.4604743228119332e-05
Step: 36080, train/learning_rate: 7.0680630415154155e-06
Step: 36080, train/epoch: 8.586387634277344
Step: 36090, train/loss: 0.01810000091791153
Step: 36090, train/grad_norm: 0.035765282809734344
Step: 36090, train/learning_rate: 7.0561636675847694e-06
Step: 36090, train/epoch: 8.588767051696777
Step: 36100, train/loss: 0.00039999998989515007
Step: 36100, train/grad_norm: 0.0017159510171040893
Step: 36100, train/learning_rate: 7.044264748401474e-06
Step: 36100, train/epoch: 8.591147422790527
Step: 36110, train/loss: 0.0
Step: 36110, train/grad_norm: 0.00035560797550715506
Step: 36110, train/learning_rate: 7.032365374470828e-06
Step: 36110, train/epoch: 8.593526840209961
Step: 36120, train/loss: 0.01549999974668026
Step: 36120, train/grad_norm: 45.64142990112305
Step: 36120, train/learning_rate: 7.020466455287533e-06
Step: 36120, train/epoch: 8.595906257629395
Step: 36130, train/loss: 0.0
Step: 36130, train/grad_norm: 0.0050984579138457775
Step: 36130, train/learning_rate: 7.008567536104238e-06
Step: 36130, train/epoch: 8.598286628723145
Step: 36140, train/loss: 0.0007999999797903001
Step: 36140, train/grad_norm: 4.952877134201117e-06
Step: 36140, train/learning_rate: 6.9966681621735916e-06
Step: 36140, train/epoch: 8.600666046142578
Step: 36150, train/loss: 0.0
Step: 36150, train/grad_norm: 0.00661863898858428
Step: 36150, train/learning_rate: 6.984769242990296e-06
Step: 36150, train/epoch: 8.603046417236328
Step: 36160, train/loss: 0.0
Step: 36160, train/grad_norm: 0.0005447827279567719
Step: 36160, train/learning_rate: 6.97286986905965e-06
Step: 36160, train/epoch: 8.605425834655762
Step: 36170, train/loss: 0.0
Step: 36170, train/grad_norm: 1.2249939572939184e-05
Step: 36170, train/learning_rate: 6.960970949876355e-06
Step: 36170, train/epoch: 8.607806205749512
Step: 36180, train/loss: 0.003100000089034438
Step: 36180, train/grad_norm: 0.0004032314463984221
Step: 36180, train/learning_rate: 6.94907203069306e-06
Step: 36180, train/epoch: 8.610185623168945
Step: 36190, train/loss: 0.10159999877214432
Step: 36190, train/grad_norm: 8.413639443460852e-05
Step: 36190, train/learning_rate: 6.937172656762414e-06
Step: 36190, train/epoch: 8.612565040588379
Step: 36200, train/loss: 0.0
Step: 36200, train/grad_norm: 6.66122286929749e-05
Step: 36200, train/learning_rate: 6.9252737375791185e-06
Step: 36200, train/epoch: 8.614945411682129
Step: 36210, train/loss: 0.0019000000320374966
Step: 36210, train/grad_norm: 0.014890916645526886
Step: 36210, train/learning_rate: 6.913374363648472e-06
Step: 36210, train/epoch: 8.617324829101562
Step: 36220, train/loss: 0.0
Step: 36220, train/grad_norm: 0.00019708130275830626
Step: 36220, train/learning_rate: 6.901475444465177e-06
Step: 36220, train/epoch: 8.619705200195312
Step: 36230, train/loss: 0.027000000700354576
Step: 36230, train/grad_norm: 6.012592712067999e-05
Step: 36230, train/learning_rate: 6.889576525281882e-06
Step: 36230, train/epoch: 8.622084617614746
Step: 36240, train/loss: 0.0
Step: 36240, train/grad_norm: 0.00012265059922356158
Step: 36240, train/learning_rate: 6.877677151351236e-06
Step: 36240, train/epoch: 8.624464988708496
Step: 36250, train/loss: 0.13050000369548798
Step: 36250, train/grad_norm: 9.831710485741496e-06
Step: 36250, train/learning_rate: 6.865778232167941e-06
Step: 36250, train/epoch: 8.62684440612793
Step: 36260, train/loss: 0.0012000000569969416
Step: 36260, train/grad_norm: 6.456081390380859
Step: 36260, train/learning_rate: 6.853879312984645e-06
Step: 36260, train/epoch: 8.629223823547363
Step: 36270, train/loss: 0.0
Step: 36270, train/grad_norm: 6.929406663402915e-05
Step: 36270, train/learning_rate: 6.841979939053999e-06
Step: 36270, train/epoch: 8.631604194641113
Step: 36280, train/loss: 0.10520000010728836
Step: 36280, train/grad_norm: 0.4177088737487793
Step: 36280, train/learning_rate: 6.830081019870704e-06
Step: 36280, train/epoch: 8.633983612060547
Step: 36290, train/loss: 0.0
Step: 36290, train/grad_norm: 1.5582718333462253e-05
Step: 36290, train/learning_rate: 6.818181645940058e-06
Step: 36290, train/epoch: 8.636363983154297
Step: 36300, train/loss: 0.0
Step: 36300, train/grad_norm: 1.7850239601102658e-05
Step: 36300, train/learning_rate: 6.806282726756763e-06
Step: 36300, train/epoch: 8.63874340057373
Step: 36310, train/loss: 0.0
Step: 36310, train/grad_norm: 8.226311911130324e-05
Step: 36310, train/learning_rate: 6.7943838075734675e-06
Step: 36310, train/epoch: 8.641122817993164
Step: 36320, train/loss: 0.0006000000284984708
Step: 36320, train/grad_norm: 4.184673525742255e-05
Step: 36320, train/learning_rate: 6.782484433642821e-06
Step: 36320, train/epoch: 8.643503189086914
Step: 36330, train/loss: 0.0006000000284984708
Step: 36330, train/grad_norm: 0.11572019010782242
Step: 36330, train/learning_rate: 6.770585514459526e-06
Step: 36330, train/epoch: 8.645882606506348
Step: 36340, train/loss: 0.0
Step: 36340, train/grad_norm: 5.3633033530786633e-05
Step: 36340, train/learning_rate: 6.75868614052888e-06
Step: 36340, train/epoch: 8.648262977600098
Step: 36350, train/loss: 0.0
Step: 36350, train/grad_norm: 0.00032248234492726624
Step: 36350, train/learning_rate: 6.746787221345585e-06
Step: 36350, train/epoch: 8.650642395019531
Step: 36360, train/loss: 0.05530000105500221
Step: 36360, train/grad_norm: 4.309129963075975e-06
Step: 36360, train/learning_rate: 6.73488830216229e-06
Step: 36360, train/epoch: 8.653022766113281
Step: 36370, train/loss: 0.0034000000450760126
Step: 36370, train/grad_norm: 1.1501106200739741e-06
Step: 36370, train/learning_rate: 6.7229889282316435e-06
Step: 36370, train/epoch: 8.655402183532715
Step: 36380, train/loss: 0.1687999963760376
Step: 36380, train/grad_norm: 6.71160378260538e-05
Step: 36380, train/learning_rate: 6.711090009048348e-06
Step: 36380, train/epoch: 8.657781600952148
Step: 36390, train/loss: 0.0024999999441206455
Step: 36390, train/grad_norm: 0.0007266327738761902
Step: 36390, train/learning_rate: 6.699190635117702e-06
Step: 36390, train/epoch: 8.660161972045898
Step: 36400, train/loss: 0.0851999968290329
Step: 36400, train/grad_norm: 7.261997234309092e-05
Step: 36400, train/learning_rate: 6.687291715934407e-06
Step: 36400, train/epoch: 8.662541389465332
Step: 36410, train/loss: 0.019500000402331352
Step: 36410, train/grad_norm: 0.00028155880863778293
Step: 36410, train/learning_rate: 6.675392796751112e-06
Step: 36410, train/epoch: 8.664921760559082
Step: 36420, train/loss: 0.00039999998989515007
Step: 36420, train/grad_norm: 3.55155611038208
Step: 36420, train/learning_rate: 6.663493422820466e-06
Step: 36420, train/epoch: 8.667301177978516
Step: 36430, train/loss: 0.0
Step: 36430, train/grad_norm: 0.0005286256782710552
Step: 36430, train/learning_rate: 6.65159450363717e-06
Step: 36430, train/epoch: 8.669681549072266
Step: 36440, train/loss: 0.0017000000225380063
Step: 36440, train/grad_norm: 0.0001810483227018267
Step: 36440, train/learning_rate: 6.639695584453875e-06
Step: 36440, train/epoch: 8.6720609664917
Step: 36450, train/loss: 0.0
Step: 36450, train/grad_norm: 0.0012300584930926561
Step: 36450, train/learning_rate: 6.627796210523229e-06
Step: 36450, train/epoch: 8.674440383911133
Step: 36460, train/loss: 0.00019999999494757503
Step: 36460, train/grad_norm: 2.2896290829521604e-05
Step: 36460, train/learning_rate: 6.615897291339934e-06
Step: 36460, train/epoch: 8.676820755004883
Step: 36470, train/loss: 0.0006000000284984708
Step: 36470, train/grad_norm: 5.559788769460283e-05
Step: 36470, train/learning_rate: 6.603997917409288e-06
Step: 36470, train/epoch: 8.679200172424316
Step: 36480, train/loss: 0.0006000000284984708
Step: 36480, train/grad_norm: 2.811528444290161
Step: 36480, train/learning_rate: 6.5920989982259925e-06
Step: 36480, train/epoch: 8.681580543518066
Step: 36490, train/loss: 9.999999747378752e-05
Step: 36490, train/grad_norm: 0.0003016094269696623
Step: 36490, train/learning_rate: 6.580200079042697e-06
Step: 36490, train/epoch: 8.6839599609375
Step: 36500, train/loss: 0.0
Step: 36500, train/grad_norm: 0.00017595481767784804
Step: 36500, train/learning_rate: 6.568300705112051e-06
Step: 36500, train/epoch: 8.686339378356934
Step: 36510, train/loss: 0.0
Step: 36510, train/grad_norm: 0.00013278033293318003
Step: 36510, train/learning_rate: 6.556401785928756e-06
Step: 36510, train/epoch: 8.688719749450684
Step: 36520, train/loss: 0.0
Step: 36520, train/grad_norm: 0.00010540602670516819
Step: 36520, train/learning_rate: 6.54450241199811e-06
Step: 36520, train/epoch: 8.691099166870117
Step: 36530, train/loss: 0.00800000037997961
Step: 36530, train/grad_norm: 7.240739822387695
Step: 36530, train/learning_rate: 6.532603492814815e-06
Step: 36530, train/epoch: 8.693479537963867
Step: 36540, train/loss: 0.0
Step: 36540, train/grad_norm: 4.288975560484687e-06
Step: 36540, train/learning_rate: 6.5207045736315195e-06
Step: 36540, train/epoch: 8.6958589553833
Step: 36550, train/loss: 0.0
Step: 36550, train/grad_norm: 0.00024620917974971235
Step: 36550, train/learning_rate: 6.508805199700873e-06
Step: 36550, train/epoch: 8.69823932647705
Step: 36560, train/loss: 0.0
Step: 36560, train/grad_norm: 0.00022834706760477275
Step: 36560, train/learning_rate: 6.496906280517578e-06
Step: 36560, train/epoch: 8.700618743896484
Step: 36570, train/loss: 0.0
Step: 36570, train/grad_norm: 0.0025409450754523277
Step: 36570, train/learning_rate: 6.485007361334283e-06
Step: 36570, train/epoch: 8.702998161315918
Step: 36580, train/loss: 0.0
Step: 36580, train/grad_norm: 3.0195458748494275e-05
Step: 36580, train/learning_rate: 6.473107987403637e-06
Step: 36580, train/epoch: 8.705378532409668
Step: 36590, train/loss: 0.0
Step: 36590, train/grad_norm: 0.000303141976473853
Step: 36590, train/learning_rate: 6.461209068220342e-06
Step: 36590, train/epoch: 8.707757949829102
Step: 36600, train/loss: 0.0003000000142492354
Step: 36600, train/grad_norm: 3.521816688589752e-05
Step: 36600, train/learning_rate: 6.4493096942896955e-06
Step: 36600, train/epoch: 8.710138320922852
Step: 36610, train/loss: 0.0
Step: 36610, train/grad_norm: 0.00010908536205533892
Step: 36610, train/learning_rate: 6.4374107751064e-06
Step: 36610, train/epoch: 8.712517738342285
Step: 36620, train/loss: 9.999999747378752e-05
Step: 36620, train/grad_norm: 0.0005475170910358429
Step: 36620, train/learning_rate: 6.425511855923105e-06
Step: 36620, train/epoch: 8.714898109436035
Step: 36630, train/loss: 0.0032999999821186066
Step: 36630, train/grad_norm: 0.00012459610297810286
Step: 36630, train/learning_rate: 6.413612481992459e-06
Step: 36630, train/epoch: 8.717277526855469
Step: 36640, train/loss: 0.0
Step: 36640, train/grad_norm: 0.0016780667938292027
Step: 36640, train/learning_rate: 6.401713562809164e-06
Step: 36640, train/epoch: 8.719656944274902
Step: 36650, train/loss: 0.0
Step: 36650, train/grad_norm: 0.0003825211897492409
Step: 36650, train/learning_rate: 6.389814188878518e-06
Step: 36650, train/epoch: 8.722037315368652
Step: 36660, train/loss: 9.999999747378752e-05
Step: 36660, train/grad_norm: 0.0006872504018247128
Step: 36660, train/learning_rate: 6.377915269695222e-06
Step: 36660, train/epoch: 8.724416732788086
Step: 36670, train/loss: 0.15549999475479126
Step: 36670, train/grad_norm: 0.00022735772654414177
Step: 36670, train/learning_rate: 6.366016350511927e-06
Step: 36670, train/epoch: 8.726797103881836
Step: 36680, train/loss: 0.017500000074505806
Step: 36680, train/grad_norm: 4.513555541052483e-05
Step: 36680, train/learning_rate: 6.354116976581281e-06
Step: 36680, train/epoch: 8.72917652130127
Step: 36690, train/loss: 0.0005000000237487257
Step: 36690, train/grad_norm: 0.0001667277974775061
Step: 36690, train/learning_rate: 6.342218057397986e-06
Step: 36690, train/epoch: 8.731555938720703
Step: 36700, train/loss: 0.0
Step: 36700, train/grad_norm: 0.00023947263252921402
Step: 36700, train/learning_rate: 6.33031868346734e-06
Step: 36700, train/epoch: 8.733936309814453
Step: 36710, train/loss: 0.0
Step: 36710, train/grad_norm: 0.004240753129124641
Step: 36710, train/learning_rate: 6.3184197642840445e-06
Step: 36710, train/epoch: 8.736315727233887
Step: 36720, train/loss: 0.0
Step: 36720, train/grad_norm: 0.0006988964159972966
Step: 36720, train/learning_rate: 6.306520845100749e-06
Step: 36720, train/epoch: 8.738696098327637
Step: 36730, train/loss: 0.0
Step: 36730, train/grad_norm: 0.006773080676794052
Step: 36730, train/learning_rate: 6.294621471170103e-06
Step: 36730, train/epoch: 8.74107551574707
Step: 36740, train/loss: 0.0
Step: 36740, train/grad_norm: 4.0290822653332725e-05
Step: 36740, train/learning_rate: 6.282722551986808e-06
Step: 36740, train/epoch: 8.74345588684082
Step: 36750, train/loss: 9.999999747378752e-05
Step: 36750, train/grad_norm: 0.0002473078202456236
Step: 36750, train/learning_rate: 6.270823632803513e-06
Step: 36750, train/epoch: 8.745835304260254
Step: 36760, train/loss: 0.07909999787807465
Step: 36760, train/grad_norm: 0.0018065423937514424
Step: 36760, train/learning_rate: 6.258924258872867e-06
Step: 36760, train/epoch: 8.748214721679688
Step: 36770, train/loss: 0.0
Step: 36770, train/grad_norm: 0.002021322725340724
Step: 36770, train/learning_rate: 6.247025339689571e-06
Step: 36770, train/epoch: 8.750595092773438
Step: 36780, train/loss: 0.0044999998062849045
Step: 36780, train/grad_norm: 0.00026486910064704716
Step: 36780, train/learning_rate: 6.235125965758925e-06
Step: 36780, train/epoch: 8.752974510192871
Step: 36790, train/loss: 0.0
Step: 36790, train/grad_norm: 0.0003267661086283624
Step: 36790, train/learning_rate: 6.22322704657563e-06
Step: 36790, train/epoch: 8.755354881286621
Step: 36800, train/loss: 0.0
Step: 36800, train/grad_norm: 0.00012588354002218693
Step: 36800, train/learning_rate: 6.211328127392335e-06
Step: 36800, train/epoch: 8.757734298706055
Step: 36810, train/loss: 0.0026000000070780516
Step: 36810, train/grad_norm: 0.0001920899812830612
Step: 36810, train/learning_rate: 6.199428753461689e-06
Step: 36810, train/epoch: 8.760114669799805
Step: 36820, train/loss: 0.0
Step: 36820, train/grad_norm: 0.003614812856540084
Step: 36820, train/learning_rate: 6.1875298342783935e-06
Step: 36820, train/epoch: 8.762494087219238
Step: 36830, train/loss: 9.999999747378752e-05
Step: 36830, train/grad_norm: 4.8927042371360585e-05
Step: 36830, train/learning_rate: 6.1756304603477474e-06
Step: 36830, train/epoch: 8.764873504638672
Step: 36840, train/loss: 0.0005000000237487257
Step: 36840, train/grad_norm: 0.0018022252479568124
Step: 36840, train/learning_rate: 6.163731541164452e-06
Step: 36840, train/epoch: 8.767253875732422
Step: 36850, train/loss: 0.0
Step: 36850, train/grad_norm: 0.00043379751150496304
Step: 36850, train/learning_rate: 6.151832621981157e-06
Step: 36850, train/epoch: 8.769633293151855
Step: 36860, train/loss: 0.00019999999494757503
Step: 36860, train/grad_norm: 0.0009938585571944714
Step: 36860, train/learning_rate: 6.139933248050511e-06
Step: 36860, train/epoch: 8.772013664245605
Step: 36870, train/loss: 0.0
Step: 36870, train/grad_norm: 9.098070586333051e-05
Step: 36870, train/learning_rate: 6.128034328867216e-06
Step: 36870, train/epoch: 8.774393081665039
Step: 36880, train/loss: 0.0008999999845400453
Step: 36880, train/grad_norm: 0.0006018182612024248
Step: 36880, train/learning_rate: 6.1161349549365696e-06
Step: 36880, train/epoch: 8.776772499084473
Step: 36890, train/loss: 0.0024999999441206455
Step: 36890, train/grad_norm: 0.00018625881057232618
Step: 36890, train/learning_rate: 6.104236035753274e-06
Step: 36890, train/epoch: 8.779152870178223
Step: 36900, train/loss: 0.0
Step: 36900, train/grad_norm: 0.0001361282484140247
Step: 36900, train/learning_rate: 6.092337116569979e-06
Step: 36900, train/epoch: 8.781532287597656
Step: 36910, train/loss: 0.0
Step: 36910, train/grad_norm: 0.0003279128286521882
Step: 36910, train/learning_rate: 6.080437742639333e-06
Step: 36910, train/epoch: 8.783912658691406
Step: 36920, train/loss: 0.0
Step: 36920, train/grad_norm: 0.0007486868998967111
Step: 36920, train/learning_rate: 6.068538823456038e-06
Step: 36920, train/epoch: 8.78629207611084
Step: 36930, train/loss: 0.000699999975040555
Step: 36930, train/grad_norm: 0.00826655887067318
Step: 36930, train/learning_rate: 6.0566399042727426e-06
Step: 36930, train/epoch: 8.78867244720459
Step: 36940, train/loss: 0.0
Step: 36940, train/grad_norm: 0.0002334264136152342
Step: 36940, train/learning_rate: 6.0447405303420965e-06
Step: 36940, train/epoch: 8.791051864624023
Step: 36950, train/loss: 0.0
Step: 36950, train/grad_norm: 0.00037655033520422876
Step: 36950, train/learning_rate: 6.032841611158801e-06
Step: 36950, train/epoch: 8.793431282043457
Step: 36960, train/loss: 0.0003000000142492354
Step: 36960, train/grad_norm: 0.021833259612321854
Step: 36960, train/learning_rate: 6.020942237228155e-06
Step: 36960, train/epoch: 8.795811653137207
Step: 36970, train/loss: 0.000699999975040555
Step: 36970, train/grad_norm: 5.1446568249957636e-05
Step: 36970, train/learning_rate: 6.00904331804486e-06
Step: 36970, train/epoch: 8.79819107055664
Step: 36980, train/loss: 0.00039999998989515007
Step: 36980, train/grad_norm: 9.701965609565377e-05
Step: 36980, train/learning_rate: 5.997144398861565e-06
Step: 36980, train/epoch: 8.80057144165039
Step: 36990, train/loss: 0.0010000000474974513
Step: 36990, train/grad_norm: 3.079159796470776e-05
Step: 36990, train/learning_rate: 5.985245024930919e-06
Step: 36990, train/epoch: 8.802950859069824
Step: 37000, train/loss: 0.0
Step: 37000, train/grad_norm: 0.0007562600658275187
Step: 37000, train/learning_rate: 5.973346105747623e-06
Step: 37000, train/epoch: 8.805331230163574
Step: 37010, train/loss: 9.999999747378752e-05
Step: 37010, train/grad_norm: 0.00019706113380379975
Step: 37010, train/learning_rate: 5.961446731816977e-06
Step: 37010, train/epoch: 8.807710647583008
Step: 37020, train/loss: 0.0
Step: 37020, train/grad_norm: 0.0030449728947132826
Step: 37020, train/learning_rate: 5.949547812633682e-06
Step: 37020, train/epoch: 8.810090065002441
Step: 37030, train/loss: 0.001500000013038516
Step: 37030, train/grad_norm: 1.299828290939331
Step: 37030, train/learning_rate: 5.937648893450387e-06
Step: 37030, train/epoch: 8.812470436096191
Step: 37040, train/loss: 0.07850000262260437
Step: 37040, train/grad_norm: 6.016033694322687e-06
Step: 37040, train/learning_rate: 5.925749519519741e-06
Step: 37040, train/epoch: 8.814849853515625
Step: 37050, train/loss: 0.0
Step: 37050, train/grad_norm: 0.0008305161027237773
Step: 37050, train/learning_rate: 5.9138506003364455e-06
Step: 37050, train/epoch: 8.817230224609375
Step: 37060, train/loss: 0.0
Step: 37060, train/grad_norm: 0.00043223900138400495
Step: 37060, train/learning_rate: 5.901951226405799e-06
Step: 37060, train/epoch: 8.819609642028809
Step: 37070, train/loss: 0.04650000110268593
Step: 37070, train/grad_norm: 0.01975933276116848
Step: 37070, train/learning_rate: 5.890052307222504e-06
Step: 37070, train/epoch: 8.821989059448242
Step: 37080, train/loss: 0.0
Step: 37080, train/grad_norm: 0.0012555682333186269
Step: 37080, train/learning_rate: 5.878153388039209e-06
Step: 37080, train/epoch: 8.824369430541992
Step: 37090, train/loss: 0.0
Step: 37090, train/grad_norm: 5.587118357652798e-05
Step: 37090, train/learning_rate: 5.866254014108563e-06
Step: 37090, train/epoch: 8.826748847961426
Step: 37100, train/loss: 0.0020000000949949026
Step: 37100, train/grad_norm: 1.3779147593595553e-05
Step: 37100, train/learning_rate: 5.854355094925268e-06
Step: 37100, train/epoch: 8.829129219055176
Step: 37110, train/loss: 0.008799999952316284
Step: 37110, train/grad_norm: 0.0013918336480855942
Step: 37110, train/learning_rate: 5.842456175741972e-06
Step: 37110, train/epoch: 8.83150863647461
Step: 37120, train/loss: 0.014000000432133675
Step: 37120, train/grad_norm: 0.0007111968006938696
Step: 37120, train/learning_rate: 5.830556801811326e-06
Step: 37120, train/epoch: 8.83388900756836
Step: 37130, train/loss: 0.02239999920129776
Step: 37130, train/grad_norm: 0.0006964591448195279
Step: 37130, train/learning_rate: 5.818657882628031e-06
Step: 37130, train/epoch: 8.836268424987793
Step: 37140, train/loss: 0.03629999980330467
Step: 37140, train/grad_norm: 77.56820678710938
Step: 37140, train/learning_rate: 5.806758508697385e-06
Step: 37140, train/epoch: 8.838647842407227
Step: 37150, train/loss: 0.00019999999494757503
Step: 37150, train/grad_norm: 1.6594860426266678e-05
Step: 37150, train/learning_rate: 5.79485958951409e-06
Step: 37150, train/epoch: 8.841028213500977
Step: 37160, train/loss: 0.010300000198185444
Step: 37160, train/grad_norm: 0.6364535093307495
Step: 37160, train/learning_rate: 5.7829606703307945e-06
Step: 37160, train/epoch: 8.84340763092041
Step: 37170, train/loss: 0.03530000150203705
Step: 37170, train/grad_norm: 6.780229568481445
Step: 37170, train/learning_rate: 5.771061296400148e-06
Step: 37170, train/epoch: 8.84578800201416
Step: 37180, train/loss: 0.0794999971985817
Step: 37180, train/grad_norm: 6.367133028106764e-05
Step: 37180, train/learning_rate: 5.759162377216853e-06
Step: 37180, train/epoch: 8.848167419433594
Step: 37190, train/loss: 0.0003000000142492354
Step: 37190, train/grad_norm: 0.0003491777752060443
Step: 37190, train/learning_rate: 5.747263003286207e-06
Step: 37190, train/epoch: 8.850547790527344
Step: 37200, train/loss: 0.0
Step: 37200, train/grad_norm: 0.009568124078214169
Step: 37200, train/learning_rate: 5.735364084102912e-06
Step: 37200, train/epoch: 8.852927207946777
Step: 37210, train/loss: 0.0
Step: 37210, train/grad_norm: 0.00030698420596309006
Step: 37210, train/learning_rate: 5.723465164919617e-06
Step: 37210, train/epoch: 8.855306625366211
Step: 37220, train/loss: 0.0
Step: 37220, train/grad_norm: 3.1644984119338915e-05
Step: 37220, train/learning_rate: 5.7115657909889705e-06
Step: 37220, train/epoch: 8.857686996459961
Step: 37230, train/loss: 9.999999747378752e-05
Step: 37230, train/grad_norm: 0.0004269825585652143
Step: 37230, train/learning_rate: 5.699666871805675e-06
Step: 37230, train/epoch: 8.860066413879395
Step: 37240, train/loss: 0.039500001817941666
Step: 37240, train/grad_norm: 0.034439872950315475
Step: 37240, train/learning_rate: 5.68776795262238e-06
Step: 37240, train/epoch: 8.862446784973145
Step: 37250, train/loss: 0.0
Step: 37250, train/grad_norm: 1.0685073903005105e-05
Step: 37250, train/learning_rate: 5.675868578691734e-06
Step: 37250, train/epoch: 8.864826202392578
Step: 37260, train/loss: 0.0
Step: 37260, train/grad_norm: 0.0011810802388936281
Step: 37260, train/learning_rate: 5.663969659508439e-06
Step: 37260, train/epoch: 8.867205619812012
Step: 37270, train/loss: 0.0
Step: 37270, train/grad_norm: 0.0005662900512106717
Step: 37270, train/learning_rate: 5.652070285577793e-06
Step: 37270, train/epoch: 8.869585990905762
Step: 37280, train/loss: 0.005799999926239252
Step: 37280, train/grad_norm: 0.0002954444207716733
Step: 37280, train/learning_rate: 5.6401713663944975e-06
Step: 37280, train/epoch: 8.871965408325195
Step: 37290, train/loss: 0.0
Step: 37290, train/grad_norm: 0.0004638344107661396
Step: 37290, train/learning_rate: 5.628272447211202e-06
Step: 37290, train/epoch: 8.874345779418945
Step: 37300, train/loss: 0.0
Step: 37300, train/grad_norm: 6.259090878302231e-05
Step: 37300, train/learning_rate: 5.616373073280556e-06
Step: 37300, train/epoch: 8.876725196838379
Step: 37310, train/loss: 0.0010999999940395355
Step: 37310, train/grad_norm: 0.003039384726434946
Step: 37310, train/learning_rate: 5.604474154097261e-06
Step: 37310, train/epoch: 8.879105567932129
Step: 37320, train/loss: 0.0
Step: 37320, train/grad_norm: 8.108421752694994e-05
Step: 37320, train/learning_rate: 5.592574780166615e-06
Step: 37320, train/epoch: 8.881484985351562
Step: 37330, train/loss: 0.05779999867081642
Step: 37330, train/grad_norm: 0.015799282118678093
Step: 37330, train/learning_rate: 5.58067586098332e-06
Step: 37330, train/epoch: 8.883864402770996
Step: 37340, train/loss: 0.0005000000237487257
Step: 37340, train/grad_norm: 0.0003073858970310539
Step: 37340, train/learning_rate: 5.568776941800024e-06
Step: 37340, train/epoch: 8.886244773864746
Step: 37350, train/loss: 0.01119999960064888
Step: 37350, train/grad_norm: 1.6931288242340088
Step: 37350, train/learning_rate: 5.556877567869378e-06
Step: 37350, train/epoch: 8.88862419128418
Step: 37360, train/loss: 0.0
Step: 37360, train/grad_norm: 0.03463512286543846
Step: 37360, train/learning_rate: 5.544978648686083e-06
Step: 37360, train/epoch: 8.89100456237793
Step: 37370, train/loss: 0.0
Step: 37370, train/grad_norm: 0.0016720237908884883
Step: 37370, train/learning_rate: 5.533079274755437e-06
Step: 37370, train/epoch: 8.893383979797363
Step: 37380, train/loss: 0.0
Step: 37380, train/grad_norm: 1.8372211343375966e-05
Step: 37380, train/learning_rate: 5.521180355572142e-06
Step: 37380, train/epoch: 8.895764350891113
Step: 37390, train/loss: 0.0008999999845400453
Step: 37390, train/grad_norm: 0.0026309601962566376
Step: 37390, train/learning_rate: 5.5092814363888465e-06
Step: 37390, train/epoch: 8.898143768310547
Step: 37400, train/loss: 0.0032999999821186066
Step: 37400, train/grad_norm: 7.196616206783801e-05
Step: 37400, train/learning_rate: 5.4973820624582e-06
Step: 37400, train/epoch: 8.90052318572998
Step: 37410, train/loss: 0.0
Step: 37410, train/grad_norm: 0.0008755626622587442
Step: 37410, train/learning_rate: 5.485483143274905e-06
Step: 37410, train/epoch: 8.90290355682373
Step: 37420, train/loss: 0.01679999940097332
Step: 37420, train/grad_norm: 0.0010111278388649225
Step: 37420, train/learning_rate: 5.47358422409161e-06
Step: 37420, train/epoch: 8.905282974243164
Step: 37430, train/loss: 0.0003000000142492354
Step: 37430, train/grad_norm: 2.4234832380898297e-05
Step: 37430, train/learning_rate: 5.461684850160964e-06
Step: 37430, train/epoch: 8.907663345336914
Step: 37440, train/loss: 0.01850000023841858
Step: 37440, train/grad_norm: 1.2058560969308019e-05
Step: 37440, train/learning_rate: 5.449785930977669e-06
Step: 37440, train/epoch: 8.910042762756348
Step: 37450, train/loss: 0.0
Step: 37450, train/grad_norm: 0.14963537454605103
Step: 37450, train/learning_rate: 5.4378865570470225e-06
Step: 37450, train/epoch: 8.912422180175781
Step: 37460, train/loss: 0.0010999999940395355
Step: 37460, train/grad_norm: 0.029532622545957565
Step: 37460, train/learning_rate: 5.425987637863727e-06
Step: 37460, train/epoch: 8.914802551269531
Step: 37470, train/loss: 0.0
Step: 37470, train/grad_norm: 0.005441389512270689
Step: 37470, train/learning_rate: 5.414088718680432e-06
Step: 37470, train/epoch: 8.917181968688965
Step: 37480, train/loss: 0.08100000023841858
Step: 37480, train/grad_norm: 0.00046580814523622394
Step: 37480, train/learning_rate: 5.402189344749786e-06
Step: 37480, train/epoch: 8.919562339782715
Step: 37490, train/loss: 0.006800000090152025
Step: 37490, train/grad_norm: 0.00018070233636535704
Step: 37490, train/learning_rate: 5.390290425566491e-06
Step: 37490, train/epoch: 8.921941757202148
Step: 37500, train/loss: 0.04859999939799309
Step: 37500, train/grad_norm: 2.332681651751045e-05
Step: 37500, train/learning_rate: 5.378391051635845e-06
Step: 37500, train/epoch: 8.924322128295898
Step: 37510, train/loss: 0.00039999998989515007
Step: 37510, train/grad_norm: 7.970556907821447e-05
Step: 37510, train/learning_rate: 5.366492132452549e-06
Step: 37510, train/epoch: 8.926701545715332
Step: 37520, train/loss: 0.07720000296831131
Step: 37520, train/grad_norm: 0.00013245058653410524
Step: 37520, train/learning_rate: 5.354593213269254e-06
Step: 37520, train/epoch: 8.929080963134766
Step: 37530, train/loss: 0.0
Step: 37530, train/grad_norm: 4.882068606093526e-05
Step: 37530, train/learning_rate: 5.342693839338608e-06
Step: 37530, train/epoch: 8.931461334228516
Step: 37540, train/loss: 0.04580000042915344
Step: 37540, train/grad_norm: 0.0002034633798757568
Step: 37540, train/learning_rate: 5.330794920155313e-06
Step: 37540, train/epoch: 8.93384075164795
Step: 37550, train/loss: 0.00019999999494757503
Step: 37550, train/grad_norm: 0.001033429871313274
Step: 37550, train/learning_rate: 5.318895546224667e-06
Step: 37550, train/epoch: 8.9362211227417
Step: 37560, train/loss: 0.0
Step: 37560, train/grad_norm: 0.0004012495919596404
Step: 37560, train/learning_rate: 5.3069966270413715e-06
Step: 37560, train/epoch: 8.938600540161133
Step: 37570, train/loss: 0.0
Step: 37570, train/grad_norm: 3.535679934429936e-05
Step: 37570, train/learning_rate: 5.295097707858076e-06
Step: 37570, train/epoch: 8.940980911254883
Step: 37580, train/loss: 9.999999747378752e-05
Step: 37580, train/grad_norm: 0.000645245483610779
Step: 37580, train/learning_rate: 5.28319833392743e-06
Step: 37580, train/epoch: 8.943360328674316
Step: 37590, train/loss: 0.0
Step: 37590, train/grad_norm: 2.8840493541792966e-05
Step: 37590, train/learning_rate: 5.271299414744135e-06
Step: 37590, train/epoch: 8.94573974609375
Step: 37600, train/loss: 0.0015999999595806003
Step: 37600, train/grad_norm: 0.009020942263305187
Step: 37600, train/learning_rate: 5.25940049556084e-06
Step: 37600, train/epoch: 8.9481201171875
Step: 37610, train/loss: 0.02969999983906746
Step: 37610, train/grad_norm: 0.0029418596532195807
Step: 37610, train/learning_rate: 5.247501121630194e-06
Step: 37610, train/epoch: 8.950499534606934
Step: 37620, train/loss: 0.0026000000070780516
Step: 37620, train/grad_norm: 0.004863356240093708
Step: 37620, train/learning_rate: 5.2356022024468984e-06
Step: 37620, train/epoch: 8.952879905700684
Step: 37630, train/loss: 0.0
Step: 37630, train/grad_norm: 0.0009518399019725621
Step: 37630, train/learning_rate: 5.223702828516252e-06
Step: 37630, train/epoch: 8.955259323120117
Step: 37640, train/loss: 0.0
Step: 37640, train/grad_norm: 6.122890681581339e-06
Step: 37640, train/learning_rate: 5.211803909332957e-06
Step: 37640, train/epoch: 8.957639694213867
Step: 37650, train/loss: 0.0
Step: 37650, train/grad_norm: 0.011280364356935024
Step: 37650, train/learning_rate: 5.199904990149662e-06
Step: 37650, train/epoch: 8.9600191116333
Step: 37660, train/loss: 0.034699998795986176
Step: 37660, train/grad_norm: 0.0006505343480966985
Step: 37660, train/learning_rate: 5.188005616219016e-06
Step: 37660, train/epoch: 8.962398529052734
Step: 37670, train/loss: 0.0
Step: 37670, train/grad_norm: 0.0001705729082459584
Step: 37670, train/learning_rate: 5.1761066970357206e-06
Step: 37670, train/epoch: 8.964778900146484
Step: 37680, train/loss: 0.0
Step: 37680, train/grad_norm: 0.0046743303537368774
Step: 37680, train/learning_rate: 5.1642073231050745e-06
Step: 37680, train/epoch: 8.967158317565918
Step: 37690, train/loss: 0.05860000103712082
Step: 37690, train/grad_norm: 1.4202907550497912e-05
Step: 37690, train/learning_rate: 5.152308403921779e-06
Step: 37690, train/epoch: 8.969538688659668
Step: 37700, train/loss: 9.999999747378752e-05
Step: 37700, train/grad_norm: 0.0007117035565897822
Step: 37700, train/learning_rate: 5.140409484738484e-06
Step: 37700, train/epoch: 8.971918106079102
Step: 37710, train/loss: 9.999999747378752e-05
Step: 37710, train/grad_norm: 0.20861588418483734
Step: 37710, train/learning_rate: 5.128510110807838e-06
Step: 37710, train/epoch: 8.974297523498535
Step: 37720, train/loss: 0.0
Step: 37720, train/grad_norm: 0.0009310123277828097
Step: 37720, train/learning_rate: 5.116611191624543e-06
Step: 37720, train/epoch: 8.976677894592285
Step: 37730, train/loss: 9.999999747378752e-05
Step: 37730, train/grad_norm: 0.0002320662169950083
Step: 37730, train/learning_rate: 5.104711817693897e-06
Step: 37730, train/epoch: 8.979057312011719
Step: 37740, train/loss: 0.0
Step: 37740, train/grad_norm: 7.357695722021163e-05
Step: 37740, train/learning_rate: 5.092812898510601e-06
Step: 37740, train/epoch: 8.981437683105469
Step: 37750, train/loss: 0.0005000000237487257
Step: 37750, train/grad_norm: 3.4086647033691406
Step: 37750, train/learning_rate: 5.080913979327306e-06
Step: 37750, train/epoch: 8.983817100524902
Step: 37760, train/loss: 0.000699999975040555
Step: 37760, train/grad_norm: 4.196026930003427e-05
Step: 37760, train/learning_rate: 5.06901460539666e-06
Step: 37760, train/epoch: 8.986197471618652
Step: 37770, train/loss: 0.0003000000142492354
Step: 37770, train/grad_norm: 0.002999508986249566
Step: 37770, train/learning_rate: 5.057115686213365e-06
Step: 37770, train/epoch: 8.988576889038086
Step: 37780, train/loss: 0.0010999999940395355
Step: 37780, train/grad_norm: 0.0005604101461358368
Step: 37780, train/learning_rate: 5.04521676703007e-06
Step: 37780, train/epoch: 8.99095630645752
Step: 37790, train/loss: 9.999999747378752e-05
Step: 37790, train/grad_norm: 0.00095078389858827
Step: 37790, train/learning_rate: 5.0333173930994235e-06
Step: 37790, train/epoch: 8.99333667755127
Step: 37800, train/loss: 0.0
Step: 37800, train/grad_norm: 0.0031001484021544456
Step: 37800, train/learning_rate: 5.021418473916128e-06
Step: 37800, train/epoch: 8.995716094970703
Step: 37810, train/loss: 0.0
Step: 37810, train/grad_norm: 0.0005616646376438439
Step: 37810, train/learning_rate: 5.009519099985482e-06
Step: 37810, train/epoch: 8.998096466064453
Step: 37818, eval/loss: 0.4016737639904022
Step: 37818, eval/accuracy: 0.9523809552192688
Step: 37818, eval/f1: 0.9492763876914978
Step: 37818, eval/runtime: 296.0072937011719
Step: 37818, eval/samples_per_second: 24.333999633789062
Step: 37818, eval/steps_per_second: 3.0439999103546143
Step: 37818, train/epoch: 9.0
Step: 37820, train/loss: 0.0
Step: 37820, train/grad_norm: 0.00012097603030269966
Step: 37820, train/learning_rate: 4.997620180802187e-06
Step: 37820, train/epoch: 9.000475883483887
Step: 37830, train/loss: 0.0
Step: 37830, train/grad_norm: 0.0022614614572376013
Step: 37830, train/learning_rate: 4.985721261618892e-06
Step: 37830, train/epoch: 9.002856254577637
Step: 37840, train/loss: 0.0
Step: 37840, train/grad_norm: 0.00012733202311210334
Step: 37840, train/learning_rate: 4.973821887688246e-06
Step: 37840, train/epoch: 9.00523567199707
Step: 37850, train/loss: 0.00019999999494757503
Step: 37850, train/grad_norm: 0.009234383702278137
Step: 37850, train/learning_rate: 4.96192296850495e-06
Step: 37850, train/epoch: 9.007615089416504
Step: 37860, train/loss: 0.0
Step: 37860, train/grad_norm: 0.0007127007120288908
Step: 37860, train/learning_rate: 4.950023594574304e-06
Step: 37860, train/epoch: 9.009995460510254
Step: 37870, train/loss: 0.0010000000474974513
Step: 37870, train/grad_norm: 0.0009883277816697955
Step: 37870, train/learning_rate: 4.938124675391009e-06
Step: 37870, train/epoch: 9.012374877929688
Step: 37880, train/loss: 0.0
Step: 37880, train/grad_norm: 0.007725932635366917
Step: 37880, train/learning_rate: 4.926225756207714e-06
Step: 37880, train/epoch: 9.014755249023438
Step: 37890, train/loss: 0.0
Step: 37890, train/grad_norm: 2.49547138082562e-05
Step: 37890, train/learning_rate: 4.914326382277068e-06
Step: 37890, train/epoch: 9.017134666442871
Step: 37900, train/loss: 0.0
Step: 37900, train/grad_norm: 0.00044546788558363914
Step: 37900, train/learning_rate: 4.9024274630937725e-06
Step: 37900, train/epoch: 9.019514083862305
Step: 37910, train/loss: 0.0
Step: 37910, train/grad_norm: 0.0002568707277532667
Step: 37910, train/learning_rate: 4.890528543910477e-06
Step: 37910, train/epoch: 9.021894454956055
Step: 37920, train/loss: 0.0
Step: 37920, train/grad_norm: 0.01427338644862175
Step: 37920, train/learning_rate: 4.878629169979831e-06
Step: 37920, train/epoch: 9.024273872375488
Step: 37930, train/loss: 0.0
Step: 37930, train/grad_norm: 0.002036488149315119
Step: 37930, train/learning_rate: 4.866730250796536e-06
Step: 37930, train/epoch: 9.026654243469238
Step: 37940, train/loss: 0.0
Step: 37940, train/grad_norm: 0.005106145516037941
Step: 37940, train/learning_rate: 4.85483087686589e-06
Step: 37940, train/epoch: 9.029033660888672
Step: 37950, train/loss: 0.0
Step: 37950, train/grad_norm: 0.000750189065001905
Step: 37950, train/learning_rate: 4.842931957682595e-06
Step: 37950, train/epoch: 9.031414031982422
Step: 37960, train/loss: 0.013399999588727951
Step: 37960, train/grad_norm: 0.000398146832594648
Step: 37960, train/learning_rate: 4.8310330384992994e-06
Step: 37960, train/epoch: 9.033793449401855
Step: 37970, train/loss: 0.0
Step: 37970, train/grad_norm: 4.811371582036372e-06
Step: 37970, train/learning_rate: 4.819133664568653e-06
Step: 37970, train/epoch: 9.036172866821289
Step: 37980, train/loss: 0.0
Step: 37980, train/grad_norm: 9.247569687431678e-05
Step: 37980, train/learning_rate: 4.807234745385358e-06
Step: 37980, train/epoch: 9.038553237915039
Step: 37990, train/loss: 0.05119999870657921
Step: 37990, train/grad_norm: 95.40336608886719
Step: 37990, train/learning_rate: 4.795335371454712e-06
Step: 37990, train/epoch: 9.040932655334473
Step: 38000, train/loss: 9.999999747378752e-05
Step: 38000, train/grad_norm: 0.07725661993026733
Step: 38000, train/learning_rate: 4.783436452271417e-06
Step: 38000, train/epoch: 9.043313026428223
Step: 38010, train/loss: 0.09229999780654907
Step: 38010, train/grad_norm: 4.699786222772673e-05
Step: 38010, train/learning_rate: 4.7715375330881216e-06
Step: 38010, train/epoch: 9.045692443847656
Step: 38020, train/loss: 0.148499995470047
Step: 38020, train/grad_norm: 0.00011042843107134104
Step: 38020, train/learning_rate: 4.7596381591574755e-06
Step: 38020, train/epoch: 9.048072814941406
Step: 38030, train/loss: 0.09799999743700027
Step: 38030, train/grad_norm: 65.06906127929688
Step: 38030, train/learning_rate: 4.74773923997418e-06
Step: 38030, train/epoch: 9.05045223236084
Step: 38040, train/loss: 0.0
Step: 38040, train/grad_norm: 0.0006716718780808151
Step: 38040, train/learning_rate: 4.735839866043534e-06
Step: 38040, train/epoch: 9.052831649780273
Step: 38050, train/loss: 0.0
Step: 38050, train/grad_norm: 0.00042455256334505975
Step: 38050, train/learning_rate: 4.723940946860239e-06
Step: 38050, train/epoch: 9.055212020874023
Step: 38060, train/loss: 0.004699999932199717
Step: 38060, train/grad_norm: 0.007034700363874435
Step: 38060, train/learning_rate: 4.712042027676944e-06
Step: 38060, train/epoch: 9.057591438293457
Step: 38070, train/loss: 0.060100000351667404
Step: 38070, train/grad_norm: 6.101642793510109e-05
Step: 38070, train/learning_rate: 4.700142653746298e-06
Step: 38070, train/epoch: 9.059971809387207
Step: 38080, train/loss: 0.0
Step: 38080, train/grad_norm: 1.493883155490039e-05
Step: 38080, train/learning_rate: 4.688243734563002e-06
Step: 38080, train/epoch: 9.06235122680664
Step: 38090, train/loss: 0.0
Step: 38090, train/grad_norm: 6.96165079716593e-06
Step: 38090, train/learning_rate: 4.676344815379707e-06
Step: 38090, train/epoch: 9.064730644226074
Step: 38100, train/loss: 0.0
Step: 38100, train/grad_norm: 0.0001632283820072189
Step: 38100, train/learning_rate: 4.664445441449061e-06
Step: 38100, train/epoch: 9.067111015319824
Step: 38110, train/loss: 0.0
Step: 38110, train/grad_norm: 0.0004238061956129968
Step: 38110, train/learning_rate: 4.652546522265766e-06
Step: 38110, train/epoch: 9.069490432739258
Step: 38120, train/loss: 0.0
Step: 38120, train/grad_norm: 0.0001480880891904235
Step: 38120, train/learning_rate: 4.64064714833512e-06
Step: 38120, train/epoch: 9.071870803833008
Step: 38130, train/loss: 0.0
Step: 38130, train/grad_norm: 0.00012329485616646707
Step: 38130, train/learning_rate: 4.6287482291518245e-06
Step: 38130, train/epoch: 9.074250221252441
Step: 38140, train/loss: 9.999999747378752e-05
Step: 38140, train/grad_norm: 0.006453132256865501
Step: 38140, train/learning_rate: 4.616849309968529e-06
Step: 38140, train/epoch: 9.076630592346191
Step: 38150, train/loss: 0.0038999998942017555
Step: 38150, train/grad_norm: 0.31504714488983154
Step: 38150, train/learning_rate: 4.604949936037883e-06
Step: 38150, train/epoch: 9.079010009765625
Step: 38160, train/loss: 0.0026000000070780516
Step: 38160, train/grad_norm: 5.3056910473969765e-06
Step: 38160, train/learning_rate: 4.593051016854588e-06
Step: 38160, train/epoch: 9.081389427185059
Step: 38170, train/loss: 0.0
Step: 38170, train/grad_norm: 0.001935775624588132
Step: 38170, train/learning_rate: 4.581151642923942e-06
Step: 38170, train/epoch: 9.083769798278809
Step: 38180, train/loss: 9.999999747378752e-05
Step: 38180, train/grad_norm: 0.001805367530323565
Step: 38180, train/learning_rate: 4.569252723740647e-06
Step: 38180, train/epoch: 9.086149215698242
Step: 38190, train/loss: 9.999999747378752e-05
Step: 38190, train/grad_norm: 2.6595713279675692e-05
Step: 38190, train/learning_rate: 4.557353804557351e-06
Step: 38190, train/epoch: 9.088529586791992
Step: 38200, train/loss: 0.025200000032782555
Step: 38200, train/grad_norm: 71.9379653930664
Step: 38200, train/learning_rate: 4.545454430626705e-06
Step: 38200, train/epoch: 9.090909004211426
Step: 38210, train/loss: 9.999999747378752e-05
Step: 38210, train/grad_norm: 0.00012373036588542163
Step: 38210, train/learning_rate: 4.53355551144341e-06
Step: 38210, train/epoch: 9.093289375305176
Step: 38220, train/loss: 0.0
Step: 38220, train/grad_norm: 0.0014749584952369332
Step: 38220, train/learning_rate: 4.521656137512764e-06
Step: 38220, train/epoch: 9.09566879272461
Step: 38230, train/loss: 0.0003000000142492354
Step: 38230, train/grad_norm: 0.00020080652029719204
Step: 38230, train/learning_rate: 4.509757218329469e-06
Step: 38230, train/epoch: 9.098048210144043
Step: 38240, train/loss: 0.0003000000142492354
Step: 38240, train/grad_norm: 0.0016804317710921168
Step: 38240, train/learning_rate: 4.4978582991461735e-06
Step: 38240, train/epoch: 9.100428581237793
Step: 38250, train/loss: 0.0
Step: 38250, train/grad_norm: 1.674095619819127e-05
Step: 38250, train/learning_rate: 4.485958925215527e-06
Step: 38250, train/epoch: 9.102807998657227
Step: 38260, train/loss: 0.0006000000284984708
Step: 38260, train/grad_norm: 0.0007601407705806196
Step: 38260, train/learning_rate: 4.474060006032232e-06
Step: 38260, train/epoch: 9.105188369750977
Step: 38270, train/loss: 9.999999747378752e-05
Step: 38270, train/grad_norm: 0.00023625738685950637
Step: 38270, train/learning_rate: 4.462161086848937e-06
Step: 38270, train/epoch: 9.10756778717041
Step: 38280, train/loss: 0.0
Step: 38280, train/grad_norm: 0.022011831402778625
Step: 38280, train/learning_rate: 4.450261712918291e-06
Step: 38280, train/epoch: 9.109947204589844
Step: 38290, train/loss: 0.016100000590085983
Step: 38290, train/grad_norm: 0.0003798080433625728
Step: 38290, train/learning_rate: 4.438362793734996e-06
Step: 38290, train/epoch: 9.112327575683594
Step: 38300, train/loss: 0.0010999999940395355
Step: 38300, train/grad_norm: 0.00012619806511793286
Step: 38300, train/learning_rate: 4.4264634198043495e-06
Step: 38300, train/epoch: 9.114706993103027
Step: 38310, train/loss: 9.999999747378752e-05
Step: 38310, train/grad_norm: 8.896404324332252e-05
Step: 38310, train/learning_rate: 4.414564500621054e-06
Step: 38310, train/epoch: 9.117087364196777
Step: 38320, train/loss: 0.0
Step: 38320, train/grad_norm: 0.0005164126050658524
Step: 38320, train/learning_rate: 4.402665581437759e-06
Step: 38320, train/epoch: 9.119466781616211
Step: 38330, train/loss: 0.0
Step: 38330, train/grad_norm: 0.0003312310145702213
Step: 38330, train/learning_rate: 4.390766207507113e-06
Step: 38330, train/epoch: 9.121847152709961
Step: 38340, train/loss: 9.999999747378752e-05
Step: 38340, train/grad_norm: 3.7537942262133583e-05
Step: 38340, train/learning_rate: 4.378867288323818e-06
Step: 38340, train/epoch: 9.124226570129395
Step: 38350, train/loss: 0.004399999976158142
Step: 38350, train/grad_norm: 1.1206124327145517e-05
Step: 38350, train/learning_rate: 4.366967914393172e-06
Step: 38350, train/epoch: 9.126605987548828
Step: 38360, train/loss: 0.00019999999494757503
Step: 38360, train/grad_norm: 1.615218752704095e-05
Step: 38360, train/learning_rate: 4.3550689952098764e-06
Step: 38360, train/epoch: 9.128986358642578
Step: 38370, train/loss: 0.0
Step: 38370, train/grad_norm: 2.584248431958258e-05
Step: 38370, train/learning_rate: 4.343170076026581e-06
Step: 38370, train/epoch: 9.131365776062012
Step: 38380, train/loss: 0.0010000000474974513
Step: 38380, train/grad_norm: 0.004169645253568888
Step: 38380, train/learning_rate: 4.331270702095935e-06
Step: 38380, train/epoch: 9.133746147155762
Step: 38390, train/loss: 0.0
Step: 38390, train/grad_norm: 0.00018534700211603194
Step: 38390, train/learning_rate: 4.31937178291264e-06
Step: 38390, train/epoch: 9.136125564575195
Step: 38400, train/loss: 0.00419999985024333
Step: 38400, train/grad_norm: 0.000785587471909821
Step: 38400, train/learning_rate: 4.307472408981994e-06
Step: 38400, train/epoch: 9.138505935668945
Step: 38410, train/loss: 0.009800000116229057
Step: 38410, train/grad_norm: 0.00043763648136518896
Step: 38410, train/learning_rate: 4.2955734897986986e-06
Step: 38410, train/epoch: 9.140885353088379
Step: 38420, train/loss: 0.0
Step: 38420, train/grad_norm: 2.7830103135784157e-05
Step: 38420, train/learning_rate: 4.283674570615403e-06
Step: 38420, train/epoch: 9.143264770507812
Step: 38430, train/loss: 0.0
Step: 38430, train/grad_norm: 7.28196173440665e-05
Step: 38430, train/learning_rate: 4.271775196684757e-06
Step: 38430, train/epoch: 9.145645141601562
Step: 38440, train/loss: 0.013199999928474426
Step: 38440, train/grad_norm: 0.003977500833570957
Step: 38440, train/learning_rate: 4.259876277501462e-06
Step: 38440, train/epoch: 9.148024559020996
Step: 38450, train/loss: 0.0
Step: 38450, train/grad_norm: 0.0001132134857471101
Step: 38450, train/learning_rate: 4.247977358318167e-06
Step: 38450, train/epoch: 9.150404930114746
Step: 38460, train/loss: 0.0007999999797903001
Step: 38460, train/grad_norm: 4.924068343825638e-05
Step: 38460, train/learning_rate: 4.236077984387521e-06
Step: 38460, train/epoch: 9.15278434753418
Step: 38470, train/loss: 0.0
Step: 38470, train/grad_norm: 3.8076603232184425e-05
Step: 38470, train/learning_rate: 4.2241790652042255e-06
Step: 38470, train/epoch: 9.155163764953613
Step: 38480, train/loss: 0.017100000753998756
Step: 38480, train/grad_norm: 0.00027966589550487697
Step: 38480, train/learning_rate: 4.212279691273579e-06
Step: 38480, train/epoch: 9.157544136047363
Step: 38490, train/loss: 9.999999747378752e-05
Step: 38490, train/grad_norm: 0.0003015812544617802
Step: 38490, train/learning_rate: 4.200380772090284e-06
Step: 38490, train/epoch: 9.159923553466797
Step: 38500, train/loss: 0.0
Step: 38500, train/grad_norm: 9.342445991933346e-05
Step: 38500, train/learning_rate: 4.188481852906989e-06
Step: 38500, train/epoch: 9.162303924560547
Step: 38510, train/loss: 0.0
Step: 38510, train/grad_norm: 0.000647938228212297
Step: 38510, train/learning_rate: 4.176582478976343e-06
Step: 38510, train/epoch: 9.16468334197998
Step: 38520, train/loss: 0.0
Step: 38520, train/grad_norm: 2.878567647712771e-05
Step: 38520, train/learning_rate: 4.164683559793048e-06
Step: 38520, train/epoch: 9.16706371307373
Step: 38530, train/loss: 9.999999747378752e-05
Step: 38530, train/grad_norm: 4.678820005210582e-06
Step: 38530, train/learning_rate: 4.1527841858624015e-06
Step: 38530, train/epoch: 9.169443130493164
Step: 38540, train/loss: 0.0
Step: 38540, train/grad_norm: 0.00029773887945339084
Step: 38540, train/learning_rate: 4.140885266679106e-06
Step: 38540, train/epoch: 9.171822547912598
Step: 38550, train/loss: 0.1071000024676323
Step: 38550, train/grad_norm: 0.00020032712200190872
Step: 38550, train/learning_rate: 4.128986347495811e-06
Step: 38550, train/epoch: 9.174202919006348
Step: 38560, train/loss: 0.0
Step: 38560, train/grad_norm: 2.4186650989577174e-05
Step: 38560, train/learning_rate: 4.117086973565165e-06
Step: 38560, train/epoch: 9.176582336425781
Step: 38570, train/loss: 0.00019999999494757503
Step: 38570, train/grad_norm: 1.6606991266598925e-05
Step: 38570, train/learning_rate: 4.10518805438187e-06
Step: 38570, train/epoch: 9.178962707519531
Step: 38580, train/loss: 0.12449999898672104
Step: 38580, train/grad_norm: 0.0002726944803725928
Step: 38580, train/learning_rate: 4.0932891351985745e-06
Step: 38580, train/epoch: 9.181342124938965
Step: 38590, train/loss: 0.0
Step: 38590, train/grad_norm: 8.95627454156056e-06
Step: 38590, train/learning_rate: 4.081389761267928e-06
Step: 38590, train/epoch: 9.183722496032715
Step: 38600, train/loss: 0.00019999999494757503
Step: 38600, train/grad_norm: 0.0025349934585392475
Step: 38600, train/learning_rate: 4.069490842084633e-06
Step: 38600, train/epoch: 9.186101913452148
Step: 38610, train/loss: 9.999999747378752e-05
Step: 38610, train/grad_norm: 0.09709258377552032
Step: 38610, train/learning_rate: 4.057591468153987e-06
Step: 38610, train/epoch: 9.188481330871582
Step: 38620, train/loss: 0.11020000278949738
Step: 38620, train/grad_norm: 0.007542290724813938
Step: 38620, train/learning_rate: 4.045692548970692e-06
Step: 38620, train/epoch: 9.190861701965332
Step: 38630, train/loss: 0.0003000000142492354
Step: 38630, train/grad_norm: 0.022188525646924973
Step: 38630, train/learning_rate: 4.033793629787397e-06
Step: 38630, train/epoch: 9.193241119384766
Step: 38640, train/loss: 0.00039999998989515007
Step: 38640, train/grad_norm: 0.0006432526861317456
Step: 38640, train/learning_rate: 4.0218942558567505e-06
Step: 38640, train/epoch: 9.195621490478516
Step: 38650, train/loss: 0.00019999999494757503
Step: 38650, train/grad_norm: 0.0005949190235696733
Step: 38650, train/learning_rate: 4.009995336673455e-06
Step: 38650, train/epoch: 9.19800090789795
Step: 38660, train/loss: 0.0
Step: 38660, train/grad_norm: 0.2531323730945587
Step: 38660, train/learning_rate: 3.998095962742809e-06
Step: 38660, train/epoch: 9.200380325317383
Step: 38670, train/loss: 0.0
Step: 38670, train/grad_norm: 0.00011448429722804576
Step: 38670, train/learning_rate: 3.986197043559514e-06
Step: 38670, train/epoch: 9.202760696411133
Step: 38680, train/loss: 0.13689999282360077
Step: 38680, train/grad_norm: 1.1273818017798476e-05
Step: 38680, train/learning_rate: 3.974298124376219e-06
Step: 38680, train/epoch: 9.205140113830566
Step: 38690, train/loss: 0.0
Step: 38690, train/grad_norm: 0.00014648938667960465
Step: 38690, train/learning_rate: 3.962398750445573e-06
Step: 38690, train/epoch: 9.207520484924316
Step: 38700, train/loss: 0.0
Step: 38700, train/grad_norm: 7.088614893291378e-06
Step: 38700, train/learning_rate: 3.9504998312622774e-06
Step: 38700, train/epoch: 9.20989990234375
Step: 38710, train/loss: 0.0
Step: 38710, train/grad_norm: 0.0016996990889310837
Step: 38710, train/learning_rate: 3.938600457331631e-06
Step: 38710, train/epoch: 9.2122802734375
Step: 38720, train/loss: 0.03849999979138374
Step: 38720, train/grad_norm: 2.0761995983775705e-05
Step: 38720, train/learning_rate: 3.926701538148336e-06
Step: 38720, train/epoch: 9.214659690856934
Step: 38730, train/loss: 0.0017999999690800905
Step: 38730, train/grad_norm: 0.0018029756611213088
Step: 38730, train/learning_rate: 3.914802618965041e-06
Step: 38730, train/epoch: 9.217039108276367
Step: 38740, train/loss: 0.0
Step: 38740, train/grad_norm: 7.085506513249129e-05
Step: 38740, train/learning_rate: 3.902903245034395e-06
Step: 38740, train/epoch: 9.219419479370117
Step: 38750, train/loss: 0.0008999999845400453
Step: 38750, train/grad_norm: 0.00028358178678900003
Step: 38750, train/learning_rate: 3.8910043258510996e-06
Step: 38750, train/epoch: 9.22179889678955
Step: 38760, train/loss: 0.0
Step: 38760, train/grad_norm: 0.0001536160270916298
Step: 38760, train/learning_rate: 3.879105406667804e-06
Step: 38760, train/epoch: 9.2241792678833
Step: 38770, train/loss: 0.0
Step: 38770, train/grad_norm: 4.65689881821163e-05
Step: 38770, train/learning_rate: 3.867206032737158e-06
Step: 38770, train/epoch: 9.226558685302734
Step: 38780, train/loss: 0.0
Step: 38780, train/grad_norm: 9.696222150523681e-06
Step: 38780, train/learning_rate: 3.855307113553863e-06
Step: 38780, train/epoch: 9.228939056396484
Step: 38790, train/loss: 0.0
Step: 38790, train/grad_norm: 0.00394437275826931
Step: 38790, train/learning_rate: 3.843407739623217e-06
Step: 38790, train/epoch: 9.231318473815918
Step: 38800, train/loss: 0.031700000166893005
Step: 38800, train/grad_norm: 0.0003393215883988887
Step: 38800, train/learning_rate: 3.831508820439922e-06
Step: 38800, train/epoch: 9.233697891235352
Step: 38810, train/loss: 0.09309999644756317
Step: 38810, train/grad_norm: 1.641666494833771e-05
Step: 38810, train/learning_rate: 3.8196099012566265e-06
Step: 38810, train/epoch: 9.236078262329102
Step: 38820, train/loss: 0.0
Step: 38820, train/grad_norm: 0.027644142508506775
Step: 38820, train/learning_rate: 3.8077105273259804e-06
Step: 38820, train/epoch: 9.238457679748535
Step: 38830, train/loss: 0.009100000374019146
Step: 38830, train/grad_norm: 7.534914038842544e-05
Step: 38830, train/learning_rate: 3.795811608142685e-06
Step: 38830, train/epoch: 9.240838050842285
Step: 38840, train/loss: 0.0
Step: 38840, train/grad_norm: 1.4545740668836515e-05
Step: 38840, train/learning_rate: 3.7839124615857145e-06
Step: 38840, train/epoch: 9.243217468261719
Step: 38850, train/loss: 0.1234000027179718
Step: 38850, train/grad_norm: 7.808445843693335e-06
Step: 38850, train/learning_rate: 3.772013315028744e-06
Step: 38850, train/epoch: 9.245596885681152
Step: 38860, train/loss: 0.05260000005364418
Step: 38860, train/grad_norm: 0.00011110061313956976
Step: 38860, train/learning_rate: 3.760114168471773e-06
Step: 38860, train/epoch: 9.247977256774902
Step: 38870, train/loss: 0.0
Step: 38870, train/grad_norm: 0.010878250002861023
Step: 38870, train/learning_rate: 3.748215249288478e-06
Step: 38870, train/epoch: 9.250356674194336
Step: 38880, train/loss: 0.0
Step: 38880, train/grad_norm: 0.005036249756813049
Step: 38880, train/learning_rate: 3.7363161027315073e-06
Step: 38880, train/epoch: 9.252737045288086
Step: 38890, train/loss: 0.0
Step: 38890, train/grad_norm: 0.0001839512406149879
Step: 38890, train/learning_rate: 3.7244169561745366e-06
Step: 38890, train/epoch: 9.25511646270752
Step: 38900, train/loss: 0.0003000000142492354
Step: 38900, train/grad_norm: 0.0021006695460528135
Step: 38900, train/learning_rate: 3.712517809617566e-06
Step: 38900, train/epoch: 9.25749683380127
Step: 38910, train/loss: 0.049800001084804535
Step: 38910, train/grad_norm: 5.0249807827640325e-05
Step: 38910, train/learning_rate: 3.7006186630605953e-06
Step: 38910, train/epoch: 9.259876251220703
Step: 38920, train/loss: 0.0
Step: 38920, train/grad_norm: 0.0003208299749530852
Step: 38920, train/learning_rate: 3.6887197438773e-06
Step: 38920, train/epoch: 9.262255668640137
Step: 38930, train/loss: 0.0
Step: 38930, train/grad_norm: 0.0013208757154643536
Step: 38930, train/learning_rate: 3.6768205973203294e-06
Step: 38930, train/epoch: 9.264636039733887
Step: 38940, train/loss: 0.0
Step: 38940, train/grad_norm: 0.0013038341421633959
Step: 38940, train/learning_rate: 3.6649214507633587e-06
Step: 38940, train/epoch: 9.26701545715332
Step: 38950, train/loss: 9.999999747378752e-05
Step: 38950, train/grad_norm: 3.957264925702475e-05
Step: 38950, train/learning_rate: 3.653022304206388e-06
Step: 38950, train/epoch: 9.26939582824707
Step: 38960, train/loss: 0.0
Step: 38960, train/grad_norm: 3.598596958909184e-05
Step: 38960, train/learning_rate: 3.641123385023093e-06
Step: 38960, train/epoch: 9.271775245666504
Step: 38970, train/loss: 0.0
Step: 38970, train/grad_norm: 0.01837771013379097
Step: 38970, train/learning_rate: 3.629224238466122e-06
Step: 38970, train/epoch: 9.274155616760254
Step: 38980, train/loss: 0.0
Step: 38980, train/grad_norm: 2.829593540809583e-05
Step: 38980, train/learning_rate: 3.6173250919091515e-06
Step: 38980, train/epoch: 9.276535034179688
Step: 38990, train/loss: 0.0
Step: 38990, train/grad_norm: 3.4334309020778164e-05
Step: 38990, train/learning_rate: 3.605425945352181e-06
Step: 38990, train/epoch: 9.278914451599121
Step: 39000, train/loss: 0.0013000000035390258
Step: 39000, train/grad_norm: 2.4103757823468186e-05
Step: 39000, train/learning_rate: 3.59352679879521e-06
Step: 39000, train/epoch: 9.281294822692871
Step: 39010, train/loss: 0.1257999986410141
Step: 39010, train/grad_norm: 0.0001604182180017233
Step: 39010, train/learning_rate: 3.581627879611915e-06
Step: 39010, train/epoch: 9.283674240112305
Step: 39020, train/loss: 0.021299999207258224
Step: 39020, train/grad_norm: 2.8741244022967294e-05
Step: 39020, train/learning_rate: 3.5697287330549443e-06
Step: 39020, train/epoch: 9.286054611206055
Step: 39030, train/loss: 0.0
Step: 39030, train/grad_norm: 0.0026775081641972065
Step: 39030, train/learning_rate: 3.5578295864979737e-06
Step: 39030, train/epoch: 9.288434028625488
Step: 39040, train/loss: 0.0003000000142492354
Step: 39040, train/grad_norm: 0.005237187258899212
Step: 39040, train/learning_rate: 3.545930439941003e-06
Step: 39040, train/epoch: 9.290813446044922
Step: 39050, train/loss: 0.003599999938160181
Step: 39050, train/grad_norm: 4.527006603893824e-05
Step: 39050, train/learning_rate: 3.5340315207577078e-06
Step: 39050, train/epoch: 9.293193817138672
Step: 39060, train/loss: 0.0
Step: 39060, train/grad_norm: 0.0005820802762173116
Step: 39060, train/learning_rate: 3.522132374200737e-06
Step: 39060, train/epoch: 9.295573234558105
Step: 39070, train/loss: 9.999999747378752e-05
Step: 39070, train/grad_norm: 0.0005638896254822612
Step: 39070, train/learning_rate: 3.5102332276437664e-06
Step: 39070, train/epoch: 9.297953605651855
Step: 39080, train/loss: 0.0
Step: 39080, train/grad_norm: 1.5034431271487847e-05
Step: 39080, train/learning_rate: 3.4983340810867958e-06
Step: 39080, train/epoch: 9.300333023071289
Step: 39090, train/loss: 9.999999747378752e-05
Step: 39090, train/grad_norm: 3.4784243325702846e-05
Step: 39090, train/learning_rate: 3.486434934529825e-06
Step: 39090, train/epoch: 9.302713394165039
Step: 39100, train/loss: 0.0
Step: 39100, train/grad_norm: 2.5394716431037523e-05
Step: 39100, train/learning_rate: 3.47453601534653e-06
Step: 39100, train/epoch: 9.305092811584473
Step: 39110, train/loss: 0.03370000049471855
Step: 39110, train/grad_norm: 0.0008442244725301862
Step: 39110, train/learning_rate: 3.4626368687895592e-06
Step: 39110, train/epoch: 9.307472229003906
Step: 39120, train/loss: 0.0
Step: 39120, train/grad_norm: 2.7845885597344022e-06
Step: 39120, train/learning_rate: 3.4507377222325886e-06
Step: 39120, train/epoch: 9.309852600097656
Step: 39130, train/loss: 0.0
Step: 39130, train/grad_norm: 0.0006157989846542478
Step: 39130, train/learning_rate: 3.438838575675618e-06
Step: 39130, train/epoch: 9.31223201751709
Step: 39140, train/loss: 0.0
Step: 39140, train/grad_norm: 0.005248086526989937
Step: 39140, train/learning_rate: 3.4269396564923227e-06
Step: 39140, train/epoch: 9.31461238861084
Step: 39150, train/loss: 0.03009999915957451
Step: 39150, train/grad_norm: 132.1686553955078
Step: 39150, train/learning_rate: 3.415040509935352e-06
Step: 39150, train/epoch: 9.316991806030273
Step: 39160, train/loss: 0.0
Step: 39160, train/grad_norm: 0.00010877418390009552
Step: 39160, train/learning_rate: 3.4031413633783814e-06
Step: 39160, train/epoch: 9.319372177124023
Step: 39170, train/loss: 0.0010000000474974513
Step: 39170, train/grad_norm: 2.6188563424511813e-05
Step: 39170, train/learning_rate: 3.3912422168214107e-06
Step: 39170, train/epoch: 9.321751594543457
Step: 39180, train/loss: 0.0
Step: 39180, train/grad_norm: 0.0009092062246054411
Step: 39180, train/learning_rate: 3.37934307026444e-06
Step: 39180, train/epoch: 9.32413101196289
Step: 39190, train/loss: 9.999999747378752e-05
Step: 39190, train/grad_norm: 0.000172862593899481
Step: 39190, train/learning_rate: 3.367444151081145e-06
Step: 39190, train/epoch: 9.32651138305664
Step: 39200, train/loss: 0.04360000044107437
Step: 39200, train/grad_norm: 0.000605121604166925
Step: 39200, train/learning_rate: 3.355545004524174e-06
Step: 39200, train/epoch: 9.328890800476074
Step: 39210, train/loss: 0.003100000089034438
Step: 39210, train/grad_norm: 0.0018821910489350557
Step: 39210, train/learning_rate: 3.3436458579672035e-06
Step: 39210, train/epoch: 9.331271171569824
Step: 39220, train/loss: 0.0
Step: 39220, train/grad_norm: 0.001148707466199994
Step: 39220, train/learning_rate: 3.331746711410233e-06
Step: 39220, train/epoch: 9.333650588989258
Step: 39230, train/loss: 9.999999747378752e-05
Step: 39230, train/grad_norm: 0.0003712203761097044
Step: 39230, train/learning_rate: 3.3198477922269376e-06
Step: 39230, train/epoch: 9.336030006408691
Step: 39240, train/loss: 0.0
Step: 39240, train/grad_norm: 0.0001085832409444265
Step: 39240, train/learning_rate: 3.307948645669967e-06
Step: 39240, train/epoch: 9.338410377502441
Step: 39250, train/loss: 0.0
Step: 39250, train/grad_norm: 0.0003384924493730068
Step: 39250, train/learning_rate: 3.2960494991129963e-06
Step: 39250, train/epoch: 9.340789794921875
Step: 39260, train/loss: 0.00019999999494757503
Step: 39260, train/grad_norm: 0.0006694667390547693
Step: 39260, train/learning_rate: 3.2841503525560256e-06
Step: 39260, train/epoch: 9.343170166015625
Step: 39270, train/loss: 0.0
Step: 39270, train/grad_norm: 6.200067218742333e-06
Step: 39270, train/learning_rate: 3.272251205999055e-06
Step: 39270, train/epoch: 9.345549583435059
Step: 39280, train/loss: 0.0
Step: 39280, train/grad_norm: 0.0001965289266081527
Step: 39280, train/learning_rate: 3.2603522868157597e-06
Step: 39280, train/epoch: 9.347929954528809
Step: 39290, train/loss: 0.0020000000949949026
Step: 39290, train/grad_norm: 0.00032224852475337684
Step: 39290, train/learning_rate: 3.248453140258789e-06
Step: 39290, train/epoch: 9.350309371948242
Step: 39300, train/loss: 0.0
Step: 39300, train/grad_norm: 8.968341717263684e-06
Step: 39300, train/learning_rate: 3.2365539937018184e-06
Step: 39300, train/epoch: 9.352688789367676
Step: 39310, train/loss: 0.0
Step: 39310, train/grad_norm: 7.694497617194429e-05
Step: 39310, train/learning_rate: 3.2246548471448477e-06
Step: 39310, train/epoch: 9.355069160461426
Step: 39320, train/loss: 0.0
Step: 39320, train/grad_norm: 5.2173916628817096e-05
Step: 39320, train/learning_rate: 3.2127559279615525e-06
Step: 39320, train/epoch: 9.35744857788086
Step: 39330, train/loss: 0.00039999998989515007
Step: 39330, train/grad_norm: 2.675355911254883
Step: 39330, train/learning_rate: 3.200856781404582e-06
Step: 39330, train/epoch: 9.35982894897461
Step: 39340, train/loss: 0.0
Step: 39340, train/grad_norm: 0.0002788347192108631
Step: 39340, train/learning_rate: 3.188957634847611e-06
Step: 39340, train/epoch: 9.362208366394043
Step: 39350, train/loss: 0.0
Step: 39350, train/grad_norm: 0.0001540218072477728
Step: 39350, train/learning_rate: 3.1770584882906405e-06
Step: 39350, train/epoch: 9.364588737487793
Step: 39360, train/loss: 9.999999747378752e-05
Step: 39360, train/grad_norm: 2.725487865973264e-05
Step: 39360, train/learning_rate: 3.16515934173367e-06
Step: 39360, train/epoch: 9.366968154907227
Step: 39370, train/loss: 9.999999747378752e-05
Step: 39370, train/grad_norm: 0.8074420690536499
Step: 39370, train/learning_rate: 3.1532604225503746e-06
Step: 39370, train/epoch: 9.36934757232666
Step: 39380, train/loss: 0.13249999284744263
Step: 39380, train/grad_norm: 8.836350025376305e-05
Step: 39380, train/learning_rate: 3.141361275993404e-06
Step: 39380, train/epoch: 9.37172794342041
Step: 39390, train/loss: 9.999999747378752e-05
Step: 39390, train/grad_norm: 6.724773265887052e-05
Step: 39390, train/learning_rate: 3.1294621294364333e-06
Step: 39390, train/epoch: 9.374107360839844
Step: 39400, train/loss: 0.0
Step: 39400, train/grad_norm: 1.0820463103300426e-05
Step: 39400, train/learning_rate: 3.1175629828794627e-06
Step: 39400, train/epoch: 9.376487731933594
Step: 39410, train/loss: 9.999999747378752e-05
Step: 39410, train/grad_norm: 0.0001659644622122869
Step: 39410, train/learning_rate: 3.1056640636961674e-06
Step: 39410, train/epoch: 9.378867149353027
Step: 39420, train/loss: 0.024399999529123306
Step: 39420, train/grad_norm: 0.001657017506659031
Step: 39420, train/learning_rate: 3.0937649171391968e-06
Step: 39420, train/epoch: 9.381246566772461
Step: 39430, train/loss: 0.0
Step: 39430, train/grad_norm: 5.647370926453732e-06
Step: 39430, train/learning_rate: 3.081865770582226e-06
Step: 39430, train/epoch: 9.383626937866211
Step: 39440, train/loss: 0.0005000000237487257
Step: 39440, train/grad_norm: 9.438514098292217e-05
Step: 39440, train/learning_rate: 3.0699666240252554e-06
Step: 39440, train/epoch: 9.386006355285645
Step: 39450, train/loss: 0.0
Step: 39450, train/grad_norm: 0.0004570285964291543
Step: 39450, train/learning_rate: 3.0580674774682848e-06
Step: 39450, train/epoch: 9.388386726379395
Step: 39460, train/loss: 0.0003000000142492354
Step: 39460, train/grad_norm: 2.3365462311630836e-06
Step: 39460, train/learning_rate: 3.0461685582849896e-06
Step: 39460, train/epoch: 9.390766143798828
Step: 39470, train/loss: 0.0
Step: 39470, train/grad_norm: 1.6918904293561354e-05
Step: 39470, train/learning_rate: 3.034269411728019e-06
Step: 39470, train/epoch: 9.393146514892578
Step: 39480, train/loss: 0.0
Step: 39480, train/grad_norm: 0.00013841759937349707
Step: 39480, train/learning_rate: 3.0223702651710482e-06
Step: 39480, train/epoch: 9.395525932312012
Step: 39490, train/loss: 0.00019999999494757503
Step: 39490, train/grad_norm: 0.00013135529297869653
Step: 39490, train/learning_rate: 3.0104711186140776e-06
Step: 39490, train/epoch: 9.397905349731445
Step: 39500, train/loss: 0.0
Step: 39500, train/grad_norm: 6.021917943144217e-05
Step: 39500, train/learning_rate: 2.9985721994307823e-06
Step: 39500, train/epoch: 9.400285720825195
Step: 39510, train/loss: 0.02419999986886978
Step: 39510, train/grad_norm: 59.79811096191406
Step: 39510, train/learning_rate: 2.9866730528738117e-06
Step: 39510, train/epoch: 9.402665138244629
Step: 39520, train/loss: 0.11779999732971191
Step: 39520, train/grad_norm: 2.5713134164107032e-05
Step: 39520, train/learning_rate: 2.974773906316841e-06
Step: 39520, train/epoch: 9.405045509338379
Step: 39530, train/loss: 0.0
Step: 39530, train/grad_norm: 0.001442065229639411
Step: 39530, train/learning_rate: 2.9628747597598704e-06
Step: 39530, train/epoch: 9.407424926757812
Step: 39540, train/loss: 0.014600000344216824
Step: 39540, train/grad_norm: 0.26002538204193115
Step: 39540, train/learning_rate: 2.9509756132028997e-06
Step: 39540, train/epoch: 9.409805297851562
Step: 39550, train/loss: 0.0
Step: 39550, train/grad_norm: 3.439275315031409e-05
Step: 39550, train/learning_rate: 2.9390766940196045e-06
Step: 39550, train/epoch: 9.412184715270996
Step: 39560, train/loss: 0.00019999999494757503
Step: 39560, train/grad_norm: 2.1019295672886074e-06
Step: 39560, train/learning_rate: 2.927177547462634e-06
Step: 39560, train/epoch: 9.41456413269043
Step: 39570, train/loss: 0.001500000013038516
Step: 39570, train/grad_norm: 1.5456096662092023e-05
Step: 39570, train/learning_rate: 2.915278400905663e-06
Step: 39570, train/epoch: 9.41694450378418
Step: 39580, train/loss: 0.00039999998989515007
Step: 39580, train/grad_norm: 8.744983460928779e-06
Step: 39580, train/learning_rate: 2.9033792543486925e-06
Step: 39580, train/epoch: 9.419323921203613
Step: 39590, train/loss: 0.0
Step: 39590, train/grad_norm: 0.0005225249915383756
Step: 39590, train/learning_rate: 2.8914803351653973e-06
Step: 39590, train/epoch: 9.421704292297363
Step: 39600, train/loss: 0.013100000098347664
Step: 39600, train/grad_norm: 0.001253087306395173
Step: 39600, train/learning_rate: 2.8795811886084266e-06
Step: 39600, train/epoch: 9.424083709716797
Step: 39610, train/loss: 0.0
Step: 39610, train/grad_norm: 0.0002790546859614551
Step: 39610, train/learning_rate: 2.867682042051456e-06
Step: 39610, train/epoch: 9.42646312713623
Step: 39620, train/loss: 0.004399999976158142
Step: 39620, train/grad_norm: 0.24635164439678192
Step: 39620, train/learning_rate: 2.8557828954944853e-06
Step: 39620, train/epoch: 9.42884349822998
Step: 39630, train/loss: 0.06520000100135803
Step: 39630, train/grad_norm: 0.0008392567397095263
Step: 39630, train/learning_rate: 2.84388397631119e-06
Step: 39630, train/epoch: 9.431222915649414
Step: 39640, train/loss: 0.0008999999845400453
Step: 39640, train/grad_norm: 1.9780462025664747e-05
Step: 39640, train/learning_rate: 2.8319848297542194e-06
Step: 39640, train/epoch: 9.433603286743164
Step: 39650, train/loss: 0.0
Step: 39650, train/grad_norm: 4.130433353566332e-06
Step: 39650, train/learning_rate: 2.8200856831972487e-06
Step: 39650, train/epoch: 9.435982704162598
Step: 39660, train/loss: 0.0003000000142492354
Step: 39660, train/grad_norm: 6.194333400344476e-05
Step: 39660, train/learning_rate: 2.808186536640278e-06
Step: 39660, train/epoch: 9.438363075256348
Step: 39670, train/loss: 0.0
Step: 39670, train/grad_norm: 9.538162885291968e-06
Step: 39670, train/learning_rate: 2.7962873900833074e-06
Step: 39670, train/epoch: 9.440742492675781
Step: 39680, train/loss: 0.0
Step: 39680, train/grad_norm: 0.001134199439547956
Step: 39680, train/learning_rate: 2.784388470900012e-06
Step: 39680, train/epoch: 9.443121910095215
Step: 39690, train/loss: 0.0
Step: 39690, train/grad_norm: 2.6973966669174843e-06
Step: 39690, train/learning_rate: 2.7724893243430415e-06
Step: 39690, train/epoch: 9.445502281188965
Step: 39700, train/loss: 0.0
Step: 39700, train/grad_norm: 0.00011614178220042959
Step: 39700, train/learning_rate: 2.760590177786071e-06
Step: 39700, train/epoch: 9.447881698608398
Step: 39710, train/loss: 0.0008999999845400453
Step: 39710, train/grad_norm: 0.0006271725287660956
Step: 39710, train/learning_rate: 2.7486910312291e-06
Step: 39710, train/epoch: 9.450262069702148
Step: 39720, train/loss: 0.000699999975040555
Step: 39720, train/grad_norm: 0.00032687708153389394
Step: 39720, train/learning_rate: 2.736792112045805e-06
Step: 39720, train/epoch: 9.452641487121582
Step: 39730, train/loss: 0.0
Step: 39730, train/grad_norm: 0.0002686167135834694
Step: 39730, train/learning_rate: 2.7248929654888343e-06
Step: 39730, train/epoch: 9.455021858215332
Step: 39740, train/loss: 9.999999747378752e-05
Step: 39740, train/grad_norm: 0.0030275280587375164
Step: 39740, train/learning_rate: 2.7129938189318636e-06
Step: 39740, train/epoch: 9.457401275634766
Step: 39750, train/loss: 0.00019999999494757503
Step: 39750, train/grad_norm: 2.5001306533813477
Step: 39750, train/learning_rate: 2.701094672374893e-06
Step: 39750, train/epoch: 9.4597806930542
Step: 39760, train/loss: 0.0
Step: 39760, train/grad_norm: 9.069313819054514e-06
Step: 39760, train/learning_rate: 2.6891955258179223e-06
Step: 39760, train/epoch: 9.46216106414795
Step: 39770, train/loss: 0.0
Step: 39770, train/grad_norm: 0.003008224768564105
Step: 39770, train/learning_rate: 2.677296606634627e-06
Step: 39770, train/epoch: 9.464540481567383
Step: 39780, train/loss: 0.0
Step: 39780, train/grad_norm: 0.0008278064779005945
Step: 39780, train/learning_rate: 2.6653974600776564e-06
Step: 39780, train/epoch: 9.466920852661133
Step: 39790, train/loss: 0.05119999870657921
Step: 39790, train/grad_norm: 0.0036057725083082914
Step: 39790, train/learning_rate: 2.6534983135206858e-06
Step: 39790, train/epoch: 9.469300270080566
Step: 39800, train/loss: 0.0003000000142492354
Step: 39800, train/grad_norm: 0.0002759102499112487
Step: 39800, train/learning_rate: 2.641599166963715e-06
Step: 39800, train/epoch: 9.4716796875
Step: 39810, train/loss: 9.999999747378752e-05
Step: 39810, train/grad_norm: 0.0029615352395921946
Step: 39810, train/learning_rate: 2.62970024778042e-06
Step: 39810, train/epoch: 9.47406005859375
Step: 39820, train/loss: 0.0
Step: 39820, train/grad_norm: 7.062000076984987e-05
Step: 39820, train/learning_rate: 2.6178011012234492e-06
Step: 39820, train/epoch: 9.476439476013184
Step: 39830, train/loss: 0.004600000102072954
Step: 39830, train/grad_norm: 1.80671540874755e-05
Step: 39830, train/learning_rate: 2.6059019546664786e-06
Step: 39830, train/epoch: 9.478819847106934
Step: 39840, train/loss: 0.0
Step: 39840, train/grad_norm: 0.00038970407331362367
Step: 39840, train/learning_rate: 2.594002808109508e-06
Step: 39840, train/epoch: 9.481199264526367
Step: 39850, train/loss: 0.0
Step: 39850, train/grad_norm: 0.0007710086647421122
Step: 39850, train/learning_rate: 2.5821036615525372e-06
Step: 39850, train/epoch: 9.483579635620117
Step: 39860, train/loss: 0.0010000000474974513
Step: 39860, train/grad_norm: 1.0539029062783811e-05
Step: 39860, train/learning_rate: 2.570204742369242e-06
Step: 39860, train/epoch: 9.48595905303955
Step: 39870, train/loss: 9.999999747378752e-05
Step: 39870, train/grad_norm: 0.022647002711892128
Step: 39870, train/learning_rate: 2.5583055958122713e-06
Step: 39870, train/epoch: 9.488338470458984
Step: 39880, train/loss: 0.0
Step: 39880, train/grad_norm: 1.8056860426440835e-05
Step: 39880, train/learning_rate: 2.5464064492553007e-06
Step: 39880, train/epoch: 9.490718841552734
Step: 39890, train/loss: 0.0
Step: 39890, train/grad_norm: 6.781209231121466e-05
Step: 39890, train/learning_rate: 2.53450730269833e-06
Step: 39890, train/epoch: 9.493098258972168
Step: 39900, train/loss: 0.0
Step: 39900, train/grad_norm: 0.0013633925700560212
Step: 39900, train/learning_rate: 2.522608383515035e-06
Step: 39900, train/epoch: 9.495478630065918
Step: 39910, train/loss: 0.0
Step: 39910, train/grad_norm: 0.00018634398293215781
Step: 39910, train/learning_rate: 2.510709236958064e-06
Step: 39910, train/epoch: 9.497858047485352
Step: 39920, train/loss: 0.029200000688433647
Step: 39920, train/grad_norm: 2.6947773221763782e-06
Step: 39920, train/learning_rate: 2.4988100904010935e-06
Step: 39920, train/epoch: 9.500238418579102
Step: 39930, train/loss: 9.999999747378752e-05
Step: 39930, train/grad_norm: 0.00030963055905885994
Step: 39930, train/learning_rate: 2.486910943844123e-06
Step: 39930, train/epoch: 9.502617835998535
Step: 39940, train/loss: 0.0
Step: 39940, train/grad_norm: 0.00017330526316072792
Step: 39940, train/learning_rate: 2.475011797287152e-06
Step: 39940, train/epoch: 9.504997253417969
Step: 39950, train/loss: 0.0005000000237487257
Step: 39950, train/grad_norm: 3.2213454687735066e-05
Step: 39950, train/learning_rate: 2.463112878103857e-06
Step: 39950, train/epoch: 9.507377624511719
Step: 39960, train/loss: 0.0
Step: 39960, train/grad_norm: 9.77758681983687e-05
Step: 39960, train/learning_rate: 2.4512137315468863e-06
Step: 39960, train/epoch: 9.509757041931152
Step: 39970, train/loss: 0.15860000252723694
Step: 39970, train/grad_norm: 1.932347731781192e-05
Step: 39970, train/learning_rate: 2.4393145849899156e-06
Step: 39970, train/epoch: 9.512137413024902
Step: 39980, train/loss: 9.999999747378752e-05
Step: 39980, train/grad_norm: 0.00033407320734113455
Step: 39980, train/learning_rate: 2.427415438432945e-06
Step: 39980, train/epoch: 9.514516830444336
Step: 39990, train/loss: 0.0
Step: 39990, train/grad_norm: 0.018948059529066086
Step: 39990, train/learning_rate: 2.4155165192496497e-06
Step: 39990, train/epoch: 9.51689624786377
Step: 40000, train/loss: 0.0
Step: 40000, train/grad_norm: 3.8291145756375045e-06
Step: 40000, train/learning_rate: 2.403617372692679e-06
Step: 40000, train/epoch: 9.51927661895752
Step: 40010, train/loss: 0.00019999999494757503
Step: 40010, train/grad_norm: 2.1135580027475953e-05
Step: 40010, train/learning_rate: 2.3917182261357084e-06
Step: 40010, train/epoch: 9.521656036376953
Step: 40020, train/loss: 0.0020000000949949026
Step: 40020, train/grad_norm: 0.00015093726688064635
Step: 40020, train/learning_rate: 2.3798190795787377e-06
Step: 40020, train/epoch: 9.524036407470703
Step: 40030, train/loss: 0.008899999782443047
Step: 40030, train/grad_norm: 57.377098083496094
Step: 40030, train/learning_rate: 2.367919933021767e-06
Step: 40030, train/epoch: 9.526415824890137
Step: 40040, train/loss: 0.012799999676644802
Step: 40040, train/grad_norm: 0.005169808864593506
Step: 40040, train/learning_rate: 2.356021013838472e-06
Step: 40040, train/epoch: 9.528796195983887
Step: 40050, train/loss: 9.999999747378752e-05
Step: 40050, train/grad_norm: 0.024783428758382797
Step: 40050, train/learning_rate: 2.344121867281501e-06
Step: 40050, train/epoch: 9.53117561340332
Step: 40060, train/loss: 0.0
Step: 40060, train/grad_norm: 6.783096614526585e-05
Step: 40060, train/learning_rate: 2.3322227207245305e-06
Step: 40060, train/epoch: 9.533555030822754
Step: 40070, train/loss: 0.0
Step: 40070, train/grad_norm: 3.4509378110669786e-06
Step: 40070, train/learning_rate: 2.32032357416756e-06
Step: 40070, train/epoch: 9.535935401916504
Step: 40080, train/loss: 0.0003000000142492354
Step: 40080, train/grad_norm: 2.8145313262939453
Step: 40080, train/learning_rate: 2.3084246549842646e-06
Step: 40080, train/epoch: 9.538314819335938
Step: 40090, train/loss: 0.0
Step: 40090, train/grad_norm: 2.389322844464914e-06
Step: 40090, train/learning_rate: 2.296525508427294e-06
Step: 40090, train/epoch: 9.540695190429688
Step: 40100, train/loss: 0.0
Step: 40100, train/grad_norm: 6.5865638134710025e-06
Step: 40100, train/learning_rate: 2.2846263618703233e-06
Step: 40100, train/epoch: 9.543074607849121
Step: 40110, train/loss: 0.0
Step: 40110, train/grad_norm: 0.0015758336521685123
Step: 40110, train/learning_rate: 2.2727272153133526e-06
Step: 40110, train/epoch: 9.545454978942871
Step: 40120, train/loss: 0.0
Step: 40120, train/grad_norm: 5.2662148846138734e-06
Step: 40120, train/learning_rate: 2.260828068756382e-06
Step: 40120, train/epoch: 9.547834396362305
Step: 40130, train/loss: 0.0
Step: 40130, train/grad_norm: 2.6313625767215854e-06
Step: 40130, train/learning_rate: 2.2489291495730868e-06
Step: 40130, train/epoch: 9.550213813781738
Step: 40140, train/loss: 0.008799999952316284
Step: 40140, train/grad_norm: 0.00021359224047046155
Step: 40140, train/learning_rate: 2.237030003016116e-06
Step: 40140, train/epoch: 9.552594184875488
Step: 40150, train/loss: 0.00019999999494757503
Step: 40150, train/grad_norm: 0.00012872922525275499
Step: 40150, train/learning_rate: 2.2251308564591454e-06
Step: 40150, train/epoch: 9.554973602294922
Step: 40160, train/loss: 0.0
Step: 40160, train/grad_norm: 5.579669959843159e-05
Step: 40160, train/learning_rate: 2.2132317099021748e-06
Step: 40160, train/epoch: 9.557353973388672
Step: 40170, train/loss: 0.0006000000284984708
Step: 40170, train/grad_norm: 4.643038272857666
Step: 40170, train/learning_rate: 2.2013327907188796e-06
Step: 40170, train/epoch: 9.559733390808105
Step: 40180, train/loss: 0.0
Step: 40180, train/grad_norm: 0.00021058504353277385
Step: 40180, train/learning_rate: 2.189433644161909e-06
Step: 40180, train/epoch: 9.562112808227539
Step: 40190, train/loss: 0.0
Step: 40190, train/grad_norm: 0.0003202955995220691
Step: 40190, train/learning_rate: 2.1775344976049382e-06
Step: 40190, train/epoch: 9.564493179321289
Step: 40200, train/loss: 0.0
Step: 40200, train/grad_norm: 3.6020835977979004e-05
Step: 40200, train/learning_rate: 2.1656353510479676e-06
Step: 40200, train/epoch: 9.566872596740723
Step: 40210, train/loss: 9.999999747378752e-05
Step: 40210, train/grad_norm: 4.4014986997353844e-06
Step: 40210, train/learning_rate: 2.153736204490997e-06
Step: 40210, train/epoch: 9.569252967834473
Step: 40220, train/loss: 0.0
Step: 40220, train/grad_norm: 0.0001702235167613253
Step: 40220, train/learning_rate: 2.1418372853077017e-06
Step: 40220, train/epoch: 9.571632385253906
Step: 40230, train/loss: 0.0
Step: 40230, train/grad_norm: 2.9063576221233234e-05
Step: 40230, train/learning_rate: 2.129938138750731e-06
Step: 40230, train/epoch: 9.574012756347656
Step: 40240, train/loss: 0.0
Step: 40240, train/grad_norm: 2.909693466790486e-05
Step: 40240, train/learning_rate: 2.1180389921937604e-06
Step: 40240, train/epoch: 9.57639217376709
Step: 40250, train/loss: 0.0
Step: 40250, train/grad_norm: 2.4926495825638995e-05
Step: 40250, train/learning_rate: 2.1061398456367897e-06
Step: 40250, train/epoch: 9.578771591186523
Step: 40260, train/loss: 0.0
Step: 40260, train/grad_norm: 4.699092824012041e-05
Step: 40260, train/learning_rate: 2.0942409264534945e-06
Step: 40260, train/epoch: 9.581151962280273
Step: 40270, train/loss: 0.0
Step: 40270, train/grad_norm: 7.048372026474681e-06
Step: 40270, train/learning_rate: 2.082341779896524e-06
Step: 40270, train/epoch: 9.583531379699707
Step: 40280, train/loss: 0.007600000128149986
Step: 40280, train/grad_norm: 0.0058067613281309605
Step: 40280, train/learning_rate: 2.070442633339553e-06
Step: 40280, train/epoch: 9.585911750793457
Step: 40290, train/loss: 0.014399999752640724
Step: 40290, train/grad_norm: 5.2911786042386666e-05
Step: 40290, train/learning_rate: 2.0585434867825825e-06
Step: 40290, train/epoch: 9.58829116821289
Step: 40300, train/loss: 9.999999747378752e-05
Step: 40300, train/grad_norm: 0.0013411300024017692
Step: 40300, train/learning_rate: 2.0466445675992873e-06
Step: 40300, train/epoch: 9.59067153930664
Step: 40310, train/loss: 9.999999747378752e-05
Step: 40310, train/grad_norm: 7.596968771395041e-06
Step: 40310, train/learning_rate: 2.0347454210423166e-06
Step: 40310, train/epoch: 9.593050956726074
Step: 40320, train/loss: 0.09669999778270721
Step: 40320, train/grad_norm: 0.00012971620890311897
Step: 40320, train/learning_rate: 2.022846274485346e-06
Step: 40320, train/epoch: 9.595430374145508
Step: 40330, train/loss: 9.999999747378752e-05
Step: 40330, train/grad_norm: 6.0610418586293235e-05
Step: 40330, train/learning_rate: 2.0109471279283753e-06
Step: 40330, train/epoch: 9.597810745239258
Step: 40340, train/loss: 0.00019999999494757503
Step: 40340, train/grad_norm: 0.012597325257956982
Step: 40340, train/learning_rate: 1.9990479813714046e-06
Step: 40340, train/epoch: 9.600190162658691
Step: 40350, train/loss: 0.0
Step: 40350, train/grad_norm: 1.545392478874419e-05
Step: 40350, train/learning_rate: 1.9871490621881094e-06
Step: 40350, train/epoch: 9.602570533752441
Step: 40360, train/loss: 0.0
Step: 40360, train/grad_norm: 0.002012222073972225
Step: 40360, train/learning_rate: 1.9752499156311387e-06
Step: 40360, train/epoch: 9.604949951171875
Step: 40370, train/loss: 0.0
Step: 40370, train/grad_norm: 0.0007757065468467772
Step: 40370, train/learning_rate: 1.963350769074168e-06
Step: 40370, train/epoch: 9.607329368591309
Step: 40380, train/loss: 0.0020000000949949026
Step: 40380, train/grad_norm: 4.548735432763351e-07
Step: 40380, train/learning_rate: 1.9514516225171974e-06
Step: 40380, train/epoch: 9.609709739685059
Step: 40390, train/loss: 0.0
Step: 40390, train/grad_norm: 0.0008004373521544039
Step: 40390, train/learning_rate: 1.939552703333902e-06
Step: 40390, train/epoch: 9.612089157104492
Step: 40400, train/loss: 0.0
Step: 40400, train/grad_norm: 2.1832529455423355e-05
Step: 40400, train/learning_rate: 1.9276535567769315e-06
Step: 40400, train/epoch: 9.614469528198242
Step: 40410, train/loss: 0.0
Step: 40410, train/grad_norm: 0.0003549381217453629
Step: 40410, train/learning_rate: 1.915754410219961e-06
Step: 40410, train/epoch: 9.616848945617676
Step: 40420, train/loss: 0.0
Step: 40420, train/grad_norm: 0.0008633897523395717
Step: 40420, train/learning_rate: 1.9038552636629902e-06
Step: 40420, train/epoch: 9.619229316711426
Step: 40430, train/loss: 0.0
Step: 40430, train/grad_norm: 0.00029603636357933283
Step: 40430, train/learning_rate: 1.8919562307928572e-06
Step: 40430, train/epoch: 9.62160873413086
Step: 40440, train/loss: 0.002300000051036477
Step: 40440, train/grad_norm: 6.168433174025267e-05
Step: 40440, train/learning_rate: 1.8800570842358866e-06
Step: 40440, train/epoch: 9.623988151550293
Step: 40450, train/loss: 0.0
Step: 40450, train/grad_norm: 0.0011804284295067191
Step: 40450, train/learning_rate: 1.8681580513657536e-06
Step: 40450, train/epoch: 9.626368522644043
Step: 40460, train/loss: 0.0005000000237487257
Step: 40460, train/grad_norm: 3.2224299502559006e-05
Step: 40460, train/learning_rate: 1.856258904808783e-06
Step: 40460, train/epoch: 9.628747940063477
Step: 40470, train/loss: 0.0
Step: 40470, train/grad_norm: 0.001637865207158029
Step: 40470, train/learning_rate: 1.84435987193865e-06
Step: 40470, train/epoch: 9.631128311157227
Step: 40480, train/loss: 0.00019999999494757503
Step: 40480, train/grad_norm: 2.113454138452653e-05
Step: 40480, train/learning_rate: 1.8324607253816794e-06
Step: 40480, train/epoch: 9.63350772857666
Step: 40490, train/loss: 9.999999747378752e-05
Step: 40490, train/grad_norm: 0.010494386777281761
Step: 40490, train/learning_rate: 1.8205616925115464e-06
Step: 40490, train/epoch: 9.63588809967041
Step: 40500, train/loss: 9.999999747378752e-05
Step: 40500, train/grad_norm: 0.0028305859304964542
Step: 40500, train/learning_rate: 1.8086625459545758e-06
Step: 40500, train/epoch: 9.638267517089844
Step: 40510, train/loss: 0.0
Step: 40510, train/grad_norm: 6.220230716280639e-05
Step: 40510, train/learning_rate: 1.796763399397605e-06
Step: 40510, train/epoch: 9.640646934509277
Step: 40520, train/loss: 0.00019999999494757503
Step: 40520, train/grad_norm: 5.582117955782451e-05
Step: 40520, train/learning_rate: 1.7848643665274722e-06
Step: 40520, train/epoch: 9.643027305603027
Step: 40530, train/loss: 0.0003000000142492354
Step: 40530, train/grad_norm: 0.0011699924943968654
Step: 40530, train/learning_rate: 1.7729652199705015e-06
Step: 40530, train/epoch: 9.645406723022461
Step: 40540, train/loss: 0.001500000013038516
Step: 40540, train/grad_norm: 0.00010120421211468056
Step: 40540, train/learning_rate: 1.7610661871003686e-06
Step: 40540, train/epoch: 9.647787094116211
Step: 40550, train/loss: 0.05979999899864197
Step: 40550, train/grad_norm: 96.55168914794922
Step: 40550, train/learning_rate: 1.7491670405433979e-06
Step: 40550, train/epoch: 9.650166511535645
Step: 40560, train/loss: 0.0
Step: 40560, train/grad_norm: 6.9525981416518334e-06
Step: 40560, train/learning_rate: 1.737268007673265e-06
Step: 40560, train/epoch: 9.652546882629395
Step: 40570, train/loss: 0.0
Step: 40570, train/grad_norm: 7.755654223728925e-06
Step: 40570, train/learning_rate: 1.7253688611162943e-06
Step: 40570, train/epoch: 9.654926300048828
Step: 40580, train/loss: 0.1054999977350235
Step: 40580, train/grad_norm: 95.04583740234375
Step: 40580, train/learning_rate: 1.7134698282461613e-06
Step: 40580, train/epoch: 9.657305717468262
Step: 40590, train/loss: 0.04859999939799309
Step: 40590, train/grad_norm: 1.9138567949994467e-05
Step: 40590, train/learning_rate: 1.7015706816891907e-06
Step: 40590, train/epoch: 9.659686088562012
Step: 40600, train/loss: 0.07769999653100967
Step: 40600, train/grad_norm: 0.00011313485447317362
Step: 40600, train/learning_rate: 1.68967153513222e-06
Step: 40600, train/epoch: 9.662065505981445
Step: 40610, train/loss: 0.0
Step: 40610, train/grad_norm: 0.00036547327181324363
Step: 40610, train/learning_rate: 1.677772502262087e-06
Step: 40610, train/epoch: 9.664445877075195
Step: 40620, train/loss: 0.0005000000237487257
Step: 40620, train/grad_norm: 2.4048955310718156e-05
Step: 40620, train/learning_rate: 1.6658733557051164e-06
Step: 40620, train/epoch: 9.666825294494629
Step: 40630, train/loss: 0.0
Step: 40630, train/grad_norm: 4.351189636508934e-05
Step: 40630, train/learning_rate: 1.6539743228349835e-06
Step: 40630, train/epoch: 9.669204711914062
Step: 40640, train/loss: 0.0
Step: 40640, train/grad_norm: 4.856045779888518e-05
Step: 40640, train/learning_rate: 1.6420751762780128e-06
Step: 40640, train/epoch: 9.671585083007812
Step: 40650, train/loss: 0.0
Step: 40650, train/grad_norm: 8.823100506560877e-05
Step: 40650, train/learning_rate: 1.6301761434078799e-06
Step: 40650, train/epoch: 9.673964500427246
Step: 40660, train/loss: 0.0
Step: 40660, train/grad_norm: 0.00012292194878682494
Step: 40660, train/learning_rate: 1.6182769968509092e-06
Step: 40660, train/epoch: 9.676344871520996
Step: 40670, train/loss: 0.01590000092983246
Step: 40670, train/grad_norm: 0.017812645062804222
Step: 40670, train/learning_rate: 1.6063779639807763e-06
Step: 40670, train/epoch: 9.67872428894043
Step: 40680, train/loss: 0.0
Step: 40680, train/grad_norm: 0.000602280255407095
Step: 40680, train/learning_rate: 1.5944788174238056e-06
Step: 40680, train/epoch: 9.68110466003418
Step: 40690, train/loss: 0.0
Step: 40690, train/grad_norm: 4.3833388190250844e-05
Step: 40690, train/learning_rate: 1.582579670866835e-06
Step: 40690, train/epoch: 9.683484077453613
Step: 40700, train/loss: 9.999999747378752e-05
Step: 40700, train/grad_norm: 0.003042198484763503
Step: 40700, train/learning_rate: 1.570680637996702e-06
Step: 40700, train/epoch: 9.685863494873047
Step: 40710, train/loss: 0.0
Step: 40710, train/grad_norm: 3.119307439192198e-05
Step: 40710, train/learning_rate: 1.5587814914397313e-06
Step: 40710, train/epoch: 9.688243865966797
Step: 40720, train/loss: 0.0019000000320374966
Step: 40720, train/grad_norm: 0.00015491039084736258
Step: 40720, train/learning_rate: 1.5468824585695984e-06
Step: 40720, train/epoch: 9.69062328338623
Step: 40730, train/loss: 0.0
Step: 40730, train/grad_norm: 0.0007724445313215256
Step: 40730, train/learning_rate: 1.5349833120126277e-06
Step: 40730, train/epoch: 9.69300365447998
Step: 40740, train/loss: 0.0
Step: 40740, train/grad_norm: 7.788588845869526e-05
Step: 40740, train/learning_rate: 1.5230842791424948e-06
Step: 40740, train/epoch: 9.695383071899414
Step: 40750, train/loss: 0.00019999999494757503
Step: 40750, train/grad_norm: 3.546231164364144e-05
Step: 40750, train/learning_rate: 1.5111851325855241e-06
Step: 40750, train/epoch: 9.697763442993164
Step: 40760, train/loss: 0.0
Step: 40760, train/grad_norm: 0.0001960083027370274
Step: 40760, train/learning_rate: 1.4992860997153912e-06
Step: 40760, train/epoch: 9.700142860412598
Step: 40770, train/loss: 0.0
Step: 40770, train/grad_norm: 7.194556019385345e-06
Step: 40770, train/learning_rate: 1.4873869531584205e-06
Step: 40770, train/epoch: 9.702522277832031
Step: 40780, train/loss: 0.04259999841451645
Step: 40780, train/grad_norm: 0.006186626851558685
Step: 40780, train/learning_rate: 1.4754878066014498e-06
Step: 40780, train/epoch: 9.704902648925781
Step: 40790, train/loss: 0.00570000009611249
Step: 40790, train/grad_norm: 2.8868902518297546e-05
Step: 40790, train/learning_rate: 1.463588773731317e-06
Step: 40790, train/epoch: 9.707282066345215
Step: 40800, train/loss: 0.0
Step: 40800, train/grad_norm: 0.002601313404738903
Step: 40800, train/learning_rate: 1.4516896271743462e-06
Step: 40800, train/epoch: 9.709662437438965
Step: 40810, train/loss: 0.00039999998989515007
Step: 40810, train/grad_norm: 3.372049570083618
Step: 40810, train/learning_rate: 1.4397905943042133e-06
Step: 40810, train/epoch: 9.712041854858398
Step: 40820, train/loss: 0.03799999877810478
Step: 40820, train/grad_norm: 4.699386863649124e-06
Step: 40820, train/learning_rate: 1.4278914477472426e-06
Step: 40820, train/epoch: 9.714421272277832
Step: 40830, train/loss: 0.0
Step: 40830, train/grad_norm: 7.1997178565652575e-06
Step: 40830, train/learning_rate: 1.4159924148771097e-06
Step: 40830, train/epoch: 9.716801643371582
Step: 40840, train/loss: 0.0
Step: 40840, train/grad_norm: 0.0005439117667265236
Step: 40840, train/learning_rate: 1.404093268320139e-06
Step: 40840, train/epoch: 9.719181060791016
Step: 40850, train/loss: 0.07890000194311142
Step: 40850, train/grad_norm: 1.87836976692779e-05
Step: 40850, train/learning_rate: 1.392194235450006e-06
Step: 40850, train/epoch: 9.721561431884766
Step: 40860, train/loss: 0.0
Step: 40860, train/grad_norm: 2.465587022015825e-05
Step: 40860, train/learning_rate: 1.3802950888930354e-06
Step: 40860, train/epoch: 9.7239408493042
Step: 40870, train/loss: 0.0
Step: 40870, train/grad_norm: 1.1483149137347937e-05
Step: 40870, train/learning_rate: 1.3683960560229025e-06
Step: 40870, train/epoch: 9.72632122039795
Step: 40880, train/loss: 0.0
Step: 40880, train/grad_norm: 2.09618019653135e-06
Step: 40880, train/learning_rate: 1.3564969094659318e-06
Step: 40880, train/epoch: 9.728700637817383
Step: 40890, train/loss: 0.0
Step: 40890, train/grad_norm: 0.0006199820782057941
Step: 40890, train/learning_rate: 1.3445977629089612e-06
Step: 40890, train/epoch: 9.731080055236816
Step: 40900, train/loss: 0.04410000145435333
Step: 40900, train/grad_norm: 0.01601671800017357
Step: 40900, train/learning_rate: 1.3326987300388282e-06
Step: 40900, train/epoch: 9.733460426330566
Step: 40910, train/loss: 0.004100000020116568
Step: 40910, train/grad_norm: 2.1989601009408943e-05
Step: 40910, train/learning_rate: 1.3207995834818576e-06
Step: 40910, train/epoch: 9.73583984375
Step: 40920, train/loss: 0.0
Step: 40920, train/grad_norm: 0.0024472784716635942
Step: 40920, train/learning_rate: 1.3089005506117246e-06
Step: 40920, train/epoch: 9.73822021484375
Step: 40930, train/loss: 0.02710000053048134
Step: 40930, train/grad_norm: 108.00589752197266
Step: 40930, train/learning_rate: 1.297001404054754e-06
Step: 40930, train/epoch: 9.740599632263184
Step: 40940, train/loss: 9.999999747378752e-05
Step: 40940, train/grad_norm: 0.0004001123597845435
Step: 40940, train/learning_rate: 1.285102371184621e-06
Step: 40940, train/epoch: 9.742980003356934
Step: 40950, train/loss: 0.0
Step: 40950, train/grad_norm: 0.00013097707414999604
Step: 40950, train/learning_rate: 1.2732032246276503e-06
Step: 40950, train/epoch: 9.745359420776367
Step: 40960, train/loss: 0.0
Step: 40960, train/grad_norm: 1.848032115958631e-05
Step: 40960, train/learning_rate: 1.2613041917575174e-06
Step: 40960, train/epoch: 9.7477388381958
Step: 40970, train/loss: 0.0
Step: 40970, train/grad_norm: 0.0013951716246083379
Step: 40970, train/learning_rate: 1.2494050452005467e-06
Step: 40970, train/epoch: 9.75011920928955
Step: 40980, train/loss: 0.0
Step: 40980, train/grad_norm: 5.2439496357692406e-05
Step: 40980, train/learning_rate: 1.237505898643576e-06
Step: 40980, train/epoch: 9.752498626708984
Step: 40990, train/loss: 0.0
Step: 40990, train/grad_norm: 3.795358497882262e-05
Step: 40990, train/learning_rate: 1.2256068657734431e-06
Step: 40990, train/epoch: 9.754878997802734
Step: 41000, train/loss: 0.0003000000142492354
Step: 41000, train/grad_norm: 0.00012435928510967642
Step: 41000, train/learning_rate: 1.2137077192164725e-06
Step: 41000, train/epoch: 9.757258415222168
Step: 41010, train/loss: 0.0
Step: 41010, train/grad_norm: 0.0002559639106038958
Step: 41010, train/learning_rate: 1.2018086863463395e-06
Step: 41010, train/epoch: 9.759637832641602
Step: 41020, train/loss: 0.1039000004529953
Step: 41020, train/grad_norm: 41.54288864135742
Step: 41020, train/learning_rate: 1.1899095397893689e-06
Step: 41020, train/epoch: 9.762018203735352
Step: 41030, train/loss: 9.999999747378752e-05
Step: 41030, train/grad_norm: 3.015124775629374e-06
Step: 41030, train/learning_rate: 1.178010506919236e-06
Step: 41030, train/epoch: 9.764397621154785
Step: 41040, train/loss: 0.0
Step: 41040, train/grad_norm: 0.0037677122745662928
Step: 41040, train/learning_rate: 1.1661113603622653e-06
Step: 41040, train/epoch: 9.766777992248535
Step: 41050, train/loss: 0.0
Step: 41050, train/grad_norm: 0.00020210043294355273
Step: 41050, train/learning_rate: 1.1542123274921323e-06
Step: 41050, train/epoch: 9.769157409667969
Step: 41060, train/loss: 0.08749999850988388
Step: 41060, train/grad_norm: 0.00025154289323836565
Step: 41060, train/learning_rate: 1.1423131809351617e-06
Step: 41060, train/epoch: 9.771537780761719
Step: 41070, train/loss: 0.0
Step: 41070, train/grad_norm: 1.4314563486550469e-05
Step: 41070, train/learning_rate: 1.130414034378191e-06
Step: 41070, train/epoch: 9.773917198181152
Step: 41080, train/loss: 9.999999747378752e-05
Step: 41080, train/grad_norm: 4.169036401435733e-05
Step: 41080, train/learning_rate: 1.118515001508058e-06
Step: 41080, train/epoch: 9.776296615600586
Step: 41090, train/loss: 0.0
Step: 41090, train/grad_norm: 0.0005594361573457718
Step: 41090, train/learning_rate: 1.1066158549510874e-06
Step: 41090, train/epoch: 9.778676986694336
Step: 41100, train/loss: 0.000699999975040555
Step: 41100, train/grad_norm: 0.00043915666174143553
Step: 41100, train/learning_rate: 1.0947168220809544e-06
Step: 41100, train/epoch: 9.78105640411377
Step: 41110, train/loss: 0.0
Step: 41110, train/grad_norm: 3.719693995662965e-05
Step: 41110, train/learning_rate: 1.0828176755239838e-06
Step: 41110, train/epoch: 9.78343677520752
Step: 41120, train/loss: 9.999999747378752e-05
Step: 41120, train/grad_norm: 0.0010295645333826542
Step: 41120, train/learning_rate: 1.0709186426538508e-06
Step: 41120, train/epoch: 9.785816192626953
Step: 41130, train/loss: 0.09690000116825104
Step: 41130, train/grad_norm: 1.0175582247029524e-05
Step: 41130, train/learning_rate: 1.0590194960968802e-06
Step: 41130, train/epoch: 9.788196563720703
Step: 41140, train/loss: 0.0
Step: 41140, train/grad_norm: 8.463033009320498e-05
Step: 41140, train/learning_rate: 1.0471204632267472e-06
Step: 41140, train/epoch: 9.790575981140137
Step: 41150, train/loss: 0.00019999999494757503
Step: 41150, train/grad_norm: 0.00015712168533354998
Step: 41150, train/learning_rate: 1.0352213166697766e-06
Step: 41150, train/epoch: 9.79295539855957
Step: 41160, train/loss: 9.999999747378752e-05
Step: 41160, train/grad_norm: 0.0003020547446794808
Step: 41160, train/learning_rate: 1.0233222837996436e-06
Step: 41160, train/epoch: 9.79533576965332
Step: 41170, train/loss: 0.0003000000142492354
Step: 41170, train/grad_norm: 0.00015042899758554995
Step: 41170, train/learning_rate: 1.011423137242673e-06
Step: 41170, train/epoch: 9.797715187072754
Step: 41180, train/loss: 0.004100000020116568
Step: 41180, train/grad_norm: 4.839092798647471e-05
Step: 41180, train/learning_rate: 9.995239906857023e-07
Step: 41180, train/epoch: 9.800095558166504
Step: 41190, train/loss: 0.0
Step: 41190, train/grad_norm: 2.650321448527393e-06
Step: 41190, train/learning_rate: 9.876249578155694e-07
Step: 41190, train/epoch: 9.802474975585938
Step: 41200, train/loss: 0.0
Step: 41200, train/grad_norm: 5.5183500080602244e-05
Step: 41200, train/learning_rate: 9.757258112585987e-07
Step: 41200, train/epoch: 9.804854393005371
Step: 41210, train/loss: 0.013799999840557575
Step: 41210, train/grad_norm: 7.570093631744385
Step: 41210, train/learning_rate: 9.638267783884658e-07
Step: 41210, train/epoch: 9.807234764099121
Step: 41220, train/loss: 0.02500000037252903
Step: 41220, train/grad_norm: 1.2245795005583204e-05
Step: 41220, train/learning_rate: 9.519276318314951e-07
Step: 41220, train/epoch: 9.809614181518555
Step: 41230, train/loss: 0.0
Step: 41230, train/grad_norm: 0.00041458779014647007
Step: 41230, train/learning_rate: 9.400285421179433e-07
Step: 41230, train/epoch: 9.811994552612305
Step: 41240, train/loss: 9.999999747378752e-05
Step: 41240, train/grad_norm: 0.0007404171628877521
Step: 41240, train/learning_rate: 9.281294524043915e-07
Step: 41240, train/epoch: 9.814373970031738
Step: 41250, train/loss: 0.00039999998989515007
Step: 41250, train/grad_norm: 1.458139419555664
Step: 41250, train/learning_rate: 9.162303626908397e-07
Step: 41250, train/epoch: 9.816754341125488
Step: 41260, train/loss: 0.0
Step: 41260, train/grad_norm: 0.00017646631749812514
Step: 41260, train/learning_rate: 9.043312729772879e-07
Step: 41260, train/epoch: 9.819133758544922
Step: 41270, train/loss: 0.0
Step: 41270, train/grad_norm: 0.0004183414566796273
Step: 41270, train/learning_rate: 8.924321832637361e-07
Step: 41270, train/epoch: 9.821513175964355
Step: 41280, train/loss: 0.0
Step: 41280, train/grad_norm: 0.00041535351192578673
Step: 41280, train/learning_rate: 8.805330935501843e-07
Step: 41280, train/epoch: 9.823893547058105
Step: 41290, train/loss: 0.0
Step: 41290, train/grad_norm: 0.002203036565333605
Step: 41290, train/learning_rate: 8.686340038366325e-07
Step: 41290, train/epoch: 9.826272964477539
Step: 41300, train/loss: 0.041099999099969864
Step: 41300, train/grad_norm: 0.004316210281103849
Step: 41300, train/learning_rate: 8.567349141230807e-07
Step: 41300, train/epoch: 9.828653335571289
Step: 41310, train/loss: 0.0013000000035390258
Step: 41310, train/grad_norm: 0.001504496787674725
Step: 41310, train/learning_rate: 8.4483576756611e-07
Step: 41310, train/epoch: 9.831032752990723
Step: 41320, train/loss: 0.01140000019222498
Step: 41320, train/grad_norm: 0.0001447587419534102
Step: 41320, train/learning_rate: 8.329366778525582e-07
Step: 41320, train/epoch: 9.833413124084473
Step: 41330, train/loss: 0.0
Step: 41330, train/grad_norm: 0.001098843989893794
Step: 41330, train/learning_rate: 8.210375881390064e-07
Step: 41330, train/epoch: 9.835792541503906
Step: 41340, train/loss: 0.0066999997943639755
Step: 41340, train/grad_norm: 8.895221981219947e-06
Step: 41340, train/learning_rate: 8.091384984254546e-07
Step: 41340, train/epoch: 9.83817195892334
Step: 41350, train/loss: 0.0
Step: 41350, train/grad_norm: 0.00014003674732521176
Step: 41350, train/learning_rate: 7.972394087119028e-07
Step: 41350, train/epoch: 9.84055233001709
Step: 41360, train/loss: 0.0005000000237487257
Step: 41360, train/grad_norm: 0.00016449154645670205
Step: 41360, train/learning_rate: 7.85340318998351e-07
Step: 41360, train/epoch: 9.842931747436523
Step: 41370, train/loss: 0.0
Step: 41370, train/grad_norm: 2.4689290512469597e-05
Step: 41370, train/learning_rate: 7.734412292847992e-07
Step: 41370, train/epoch: 9.845312118530273
Step: 41380, train/loss: 0.0
Step: 41380, train/grad_norm: 0.00012076088751200587
Step: 41380, train/learning_rate: 7.615421395712474e-07
Step: 41380, train/epoch: 9.847691535949707
Step: 41390, train/loss: 0.0
Step: 41390, train/grad_norm: 6.11526338616386e-05
Step: 41390, train/learning_rate: 7.496430498576956e-07
Step: 41390, train/epoch: 9.85007095336914
Step: 41400, train/loss: 0.0
Step: 41400, train/grad_norm: 0.0014381466899067163
Step: 41400, train/learning_rate: 7.377439033007249e-07
Step: 41400, train/epoch: 9.85245132446289
Step: 41410, train/loss: 0.0032999999821186066
Step: 41410, train/grad_norm: 8.018202061066404e-05
Step: 41410, train/learning_rate: 7.258448135871731e-07
Step: 41410, train/epoch: 9.854830741882324
Step: 41420, train/loss: 0.0
Step: 41420, train/grad_norm: 1.0472002031747252e-05
Step: 41420, train/learning_rate: 7.139457238736213e-07
Step: 41420, train/epoch: 9.857211112976074
Step: 41430, train/loss: 0.0
Step: 41430, train/grad_norm: 5.69119447391131e-06
Step: 41430, train/learning_rate: 7.020466341600695e-07
Step: 41430, train/epoch: 9.859590530395508
Step: 41440, train/loss: 0.0
Step: 41440, train/grad_norm: 1.4126267160463613e-05
Step: 41440, train/learning_rate: 6.901475444465177e-07
Step: 41440, train/epoch: 9.861970901489258
Step: 41450, train/loss: 0.0
Step: 41450, train/grad_norm: 5.663568663294427e-05
Step: 41450, train/learning_rate: 6.782484547329659e-07
Step: 41450, train/epoch: 9.864350318908691
Step: 41460, train/loss: 0.16500000655651093
Step: 41460, train/grad_norm: 48.50248336791992
Step: 41460, train/learning_rate: 6.663493650194141e-07
Step: 41460, train/epoch: 9.866729736328125
Step: 41470, train/loss: 0.054499998688697815
Step: 41470, train/grad_norm: 0.0028533064760267735
Step: 41470, train/learning_rate: 6.544502753058623e-07
Step: 41470, train/epoch: 9.869110107421875
Step: 41480, train/loss: 0.0
Step: 41480, train/grad_norm: 0.0002532166545279324
Step: 41480, train/learning_rate: 6.425511855923105e-07
Step: 41480, train/epoch: 9.871489524841309
Step: 41490, train/loss: 0.0
Step: 41490, train/grad_norm: 3.5112869227305055e-05
Step: 41490, train/learning_rate: 6.306520958787587e-07
Step: 41490, train/epoch: 9.873869895935059
Step: 41500, train/loss: 0.0
Step: 41500, train/grad_norm: 0.0005016278591938317
Step: 41500, train/learning_rate: 6.18752949321788e-07
Step: 41500, train/epoch: 9.876249313354492
Step: 41510, train/loss: 0.00039999998989515007
Step: 41510, train/grad_norm: 0.00020471934112720191
Step: 41510, train/learning_rate: 6.068538596082362e-07
Step: 41510, train/epoch: 9.878629684448242
Step: 41520, train/loss: 0.00559999980032444
Step: 41520, train/grad_norm: 0.006522622890770435
Step: 41520, train/learning_rate: 5.949547698946844e-07
Step: 41520, train/epoch: 9.881009101867676
Step: 41530, train/loss: 0.002199999988079071
Step: 41530, train/grad_norm: 0.00042299824417568743
Step: 41530, train/learning_rate: 5.830556801811326e-07
Step: 41530, train/epoch: 9.88338851928711
Step: 41540, train/loss: 0.0
Step: 41540, train/grad_norm: 2.2715441446052864e-05
Step: 41540, train/learning_rate: 5.711565904675808e-07
Step: 41540, train/epoch: 9.88576889038086
Step: 41550, train/loss: 0.00019999999494757503
Step: 41550, train/grad_norm: 7.038895091682207e-06
Step: 41550, train/learning_rate: 5.59257500754029e-07
Step: 41550, train/epoch: 9.888148307800293
Step: 41560, train/loss: 0.037300001829862595
Step: 41560, train/grad_norm: 7.477833423763514e-05
Step: 41560, train/learning_rate: 5.473584110404772e-07
Step: 41560, train/epoch: 9.890528678894043
Step: 41570, train/loss: 0.0006000000284984708
Step: 41570, train/grad_norm: 4.436820983886719
Step: 41570, train/learning_rate: 5.354593213269254e-07
Step: 41570, train/epoch: 9.892908096313477
Step: 41580, train/loss: 9.999999747378752e-05
Step: 41580, train/grad_norm: 0.24234260618686676
Step: 41580, train/learning_rate: 5.235602316133736e-07
Step: 41580, train/epoch: 9.89528751373291
Step: 41590, train/loss: 0.0034000000450760126
Step: 41590, train/grad_norm: 4.1196948586730286e-05
Step: 41590, train/learning_rate: 5.116611418998218e-07
Step: 41590, train/epoch: 9.89766788482666
Step: 41600, train/loss: 0.0
Step: 41600, train/grad_norm: 0.0003201596555300057
Step: 41600, train/learning_rate: 4.997619953428512e-07
Step: 41600, train/epoch: 9.900047302246094
Step: 41610, train/loss: 0.0
Step: 41610, train/grad_norm: 0.0021972451359033585
Step: 41610, train/learning_rate: 4.878629056292993e-07
Step: 41610, train/epoch: 9.902427673339844
Step: 41620, train/loss: 0.0
Step: 41620, train/grad_norm: 3.572276546037756e-05
Step: 41620, train/learning_rate: 4.7596381591574755e-07
Step: 41620, train/epoch: 9.904807090759277
Step: 41630, train/loss: 0.0
Step: 41630, train/grad_norm: 1.5078204341989476e-05
Step: 41630, train/learning_rate: 4.6406472620219574e-07
Step: 41630, train/epoch: 9.907187461853027
Step: 41640, train/loss: 0.0
Step: 41640, train/grad_norm: 3.9291353459702805e-05
Step: 41640, train/learning_rate: 4.5216563648864394e-07
Step: 41640, train/epoch: 9.909566879272461
Step: 41650, train/loss: 9.999999747378752e-05
Step: 41650, train/grad_norm: 2.7423100618761964e-05
Step: 41650, train/learning_rate: 4.4026654677509214e-07
Step: 41650, train/epoch: 9.911946296691895
Step: 41660, train/loss: 0.0031999999191612005
Step: 41660, train/grad_norm: 0.0006815767264924943
Step: 41660, train/learning_rate: 4.2836745706154034e-07
Step: 41660, train/epoch: 9.914326667785645
Step: 41670, train/loss: 0.0
Step: 41670, train/grad_norm: 0.0019315957324579358
Step: 41670, train/learning_rate: 4.164683389262791e-07
Step: 41670, train/epoch: 9.916706085205078
Step: 41680, train/loss: 0.0
Step: 41680, train/grad_norm: 0.0009426662581972778
Step: 41680, train/learning_rate: 4.045692492127273e-07
Step: 41680, train/epoch: 9.919086456298828
Step: 41690, train/loss: 0.00039999998989515007
Step: 41690, train/grad_norm: 5.210571998759406e-06
Step: 41690, train/learning_rate: 3.926701594991755e-07
Step: 41690, train/epoch: 9.921465873718262
Step: 41700, train/loss: 0.00039999998989515007
Step: 41700, train/grad_norm: 0.00014463945990428329
Step: 41700, train/learning_rate: 3.807710697856237e-07
Step: 41700, train/epoch: 9.923846244812012
Step: 41710, train/loss: 0.0
Step: 41710, train/grad_norm: 0.060884471982717514
Step: 41710, train/learning_rate: 3.6887195165036246e-07
Step: 41710, train/epoch: 9.926225662231445
Step: 41720, train/loss: 0.0
Step: 41720, train/grad_norm: 2.8657603252213448e-05
Step: 41720, train/learning_rate: 3.5697286193681066e-07
Step: 41720, train/epoch: 9.928605079650879
Step: 41730, train/loss: 0.0
Step: 41730, train/grad_norm: 3.849126005661674e-05
Step: 41730, train/learning_rate: 3.4507377222325886e-07
Step: 41730, train/epoch: 9.930985450744629
Step: 41740, train/loss: 0.0
Step: 41740, train/grad_norm: 1.6635338397463784e-05
Step: 41740, train/learning_rate: 3.3317468250970705e-07
Step: 41740, train/epoch: 9.933364868164062
Step: 41750, train/loss: 0.020400000736117363
Step: 41750, train/grad_norm: 5.369066275306977e-05
Step: 41750, train/learning_rate: 3.2127559279615525e-07
Step: 41750, train/epoch: 9.935745239257812
Step: 41760, train/loss: 0.0
Step: 41760, train/grad_norm: 0.0003357723180670291
Step: 41760, train/learning_rate: 3.09376474660894e-07
Step: 41760, train/epoch: 9.938124656677246
Step: 41770, train/loss: 0.0
Step: 41770, train/grad_norm: 4.425042334332829e-06
Step: 41770, train/learning_rate: 2.974773849473422e-07
Step: 41770, train/epoch: 9.94050407409668
Step: 41780, train/loss: 0.0013000000035390258
Step: 41780, train/grad_norm: 1.2928641808684915e-05
Step: 41780, train/learning_rate: 2.855782952337904e-07
Step: 41780, train/epoch: 9.94288444519043
Step: 41790, train/loss: 0.0
Step: 41790, train/grad_norm: 6.532944098580629e-06
Step: 41790, train/learning_rate: 2.736792055202386e-07
Step: 41790, train/epoch: 9.945263862609863
Step: 41800, train/loss: 0.0
Step: 41800, train/grad_norm: 8.648392395116389e-06
Step: 41800, train/learning_rate: 2.617801158066868e-07
Step: 41800, train/epoch: 9.947644233703613
Step: 41810, train/loss: 9.999999747378752e-05
Step: 41810, train/grad_norm: 8.920316759031266e-05
Step: 41810, train/learning_rate: 2.498809976714256e-07
Step: 41810, train/epoch: 9.950023651123047
Step: 41820, train/loss: 0.0006000000284984708
Step: 41820, train/grad_norm: 2.7817534373753006e-06
Step: 41820, train/learning_rate: 2.3798190795787377e-07
Step: 41820, train/epoch: 9.952404022216797
Step: 41830, train/loss: 0.02199999988079071
Step: 41830, train/grad_norm: 3.283251135144383e-05
Step: 41830, train/learning_rate: 2.2608281824432197e-07
Step: 41830, train/epoch: 9.95478343963623
Step: 41840, train/loss: 0.0
Step: 41840, train/grad_norm: 0.0005144046735949814
Step: 41840, train/learning_rate: 2.1418372853077017e-07
Step: 41840, train/epoch: 9.957162857055664
Step: 41850, train/loss: 0.0
Step: 41850, train/grad_norm: 0.0003086184442508966
Step: 41850, train/learning_rate: 2.0228462460636365e-07
Step: 41850, train/epoch: 9.959543228149414
Step: 41860, train/loss: 9.999999747378752e-05
Step: 41860, train/grad_norm: 0.00040668551810085773
Step: 41860, train/learning_rate: 1.9038553489281185e-07
Step: 41860, train/epoch: 9.961922645568848
Step: 41870, train/loss: 0.0
Step: 41870, train/grad_norm: 3.850551001960412e-05
Step: 41870, train/learning_rate: 1.7848643096840533e-07
Step: 41870, train/epoch: 9.964303016662598
Step: 41880, train/loss: 0.0
Step: 41880, train/grad_norm: 1.4708341041114181e-05
Step: 41880, train/learning_rate: 1.6658734125485353e-07
Step: 41880, train/epoch: 9.966682434082031
Step: 41890, train/loss: 9.999999747378752e-05
Step: 41890, train/grad_norm: 9.548498201183975e-05
Step: 41890, train/learning_rate: 1.54688237330447e-07
Step: 41890, train/epoch: 9.969062805175781
Step: 41900, train/loss: 0.0
Step: 41900, train/grad_norm: 0.0011884646955877542
Step: 41900, train/learning_rate: 1.427891476168952e-07
Step: 41900, train/epoch: 9.971442222595215
Step: 41910, train/loss: 0.0005000000237487257
Step: 41910, train/grad_norm: 0.00037862148019485176
Step: 41910, train/learning_rate: 1.308900579033434e-07
Step: 41910, train/epoch: 9.973821640014648
Step: 41920, train/loss: 9.999999747378752e-05
Step: 41920, train/grad_norm: 1.7753554857335985e-05
Step: 41920, train/learning_rate: 1.1899095397893689e-07
Step: 41920, train/epoch: 9.976202011108398
Step: 41930, train/loss: 0.0
Step: 41930, train/grad_norm: 6.581841444130987e-05
Step: 41930, train/learning_rate: 1.0709186426538508e-07
Step: 41930, train/epoch: 9.978581428527832
Step: 41940, train/loss: 0.0
Step: 41940, train/grad_norm: 0.00016547997074667364
Step: 41940, train/learning_rate: 9.519276744640592e-08
Step: 41940, train/epoch: 9.980961799621582
Step: 41950, train/loss: 0.0
Step: 41950, train/grad_norm: 5.800796134280972e-05
Step: 41950, train/learning_rate: 8.329367062742676e-08
Step: 41950, train/epoch: 9.983341217041016
Step: 41960, train/loss: 0.0
Step: 41960, train/grad_norm: 0.0012658133637160063
Step: 41960, train/learning_rate: 7.13945738084476e-08
Step: 41960, train/epoch: 9.98572063446045
Step: 41970, train/loss: 0.017500000074505806
Step: 41970, train/grad_norm: 9.877341653918847e-06
Step: 41970, train/learning_rate: 5.949547698946844e-08
Step: 41970, train/epoch: 9.9881010055542
Step: 41980, train/loss: 0.0
Step: 41980, train/grad_norm: 4.5612603571498767e-05
Step: 41980, train/learning_rate: 4.759638372320296e-08
Step: 41980, train/epoch: 9.990480422973633
Step: 41990, train/loss: 0.0
Step: 41990, train/grad_norm: 0.005346020217984915
Step: 41990, train/learning_rate: 3.56972869042238e-08
Step: 41990, train/epoch: 9.992860794067383
Step: 42000, train/loss: 0.0
Step: 42000, train/grad_norm: 0.00023940303071867675
Step: 42000, train/learning_rate: 2.379819186160148e-08
Step: 42000, train/epoch: 9.995240211486816
Step: 42010, train/loss: 0.0
Step: 42010, train/grad_norm: 2.157040398742538e-05
Step: 42010, train/learning_rate: 1.189909593080074e-08
Step: 42010, train/epoch: 9.997620582580566
Step: 42020, train/loss: 0.0
Step: 42020, train/grad_norm: 0.0007006184896454215
Step: 42020, train/learning_rate: 0.0
Step: 42020, train/epoch: 10.0
Step: 42020, eval/loss: 0.42480698227882385
Step: 42020, eval/accuracy: 0.9533527493476868
Step: 42020, eval/f1: 0.9504584670066833
Step: 42020, eval/runtime: 295.93798828125
Step: 42020, eval/samples_per_second: 24.34000015258789
Step: 42020, eval/steps_per_second: 3.0450000762939453
Step: 42020, train/epoch: 10.0
Step: 42020, train/train_runtime: 52948.53125
Step: 42020, train/train_samples_per_second: 6.3480000495910645
Step: 42020, train/train_steps_per_second: 0.7940000295639038
Step: 42020, train/total_flos: 7.375296537653412e+17
Step: 42020, train/train_loss: 0.1033586636185646
Step: 42020, train/epoch: 10.0
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=6ffd0164-0f41-4bbe-b905-7dda371dfd20">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [13]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.summary.summary_iterator</span> <span class="kn">import</span> <span class="n">summary_iterator</span>

<span class="n">logs_directory</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">'./'</span><span class="p">,</span> <span class="n">project_name</span><span class="p">,</span> <span class="s1">'logs'</span><span class="p">)</span>
<span class="n">file_pattern</span> <span class="o">=</span> <span class="s1">'events.out.tfevents.*'</span>

<span class="n">event_files</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">logs_directory</span><span class="p">,</span> <span class="n">file_pattern</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">extract_metrics</span><span class="p">(</span><span class="n">event_files</span><span class="p">):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">last_train_loss</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">for</span> <span class="n">event_file</span> <span class="ow">in</span> <span class="n">event_files</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">summary_iterator</span><span class="p">(</span><span class="n">event_file</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">e</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">v</span><span class="o">.</span><span class="n">HasField</span><span class="p">(</span><span class="s1">'simple_value'</span><span class="p">):</span>
                    <span class="n">step</span> <span class="o">=</span> <span class="n">e</span><span class="o">.</span><span class="n">step</span>
                    <span class="n">metric_name</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">'/'</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                    <span class="n">metric_value</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">simple_value</span>

                    <span class="n">formatted_value</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">metric_value</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">"</span>

                    <span class="k">if</span> <span class="s1">'train/loss'</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="p">:</span>
                        <span class="n">last_train_loss</span> <span class="o">=</span> <span class="n">formatted_value</span>

                    <span class="k">if</span> <span class="s1">'eval'</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="p">:</span>
                        <span class="n">entry</span> <span class="o">=</span> <span class="nb">next</span><span class="p">((</span><span class="n">item</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">data</span> <span class="k">if</span> <span class="n">item</span><span class="p">[</span><span class="s1">'Step'</span><span class="p">]</span> <span class="o">==</span> <span class="n">step</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="n">entry</span><span class="p">:</span>
                            <span class="n">entry</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'Step'</span><span class="p">:</span> <span class="n">step</span><span class="p">,</span> <span class="s1">'Train Loss'</span><span class="p">:</span> <span class="n">last_train_loss</span><span class="p">,</span> <span class="s1">'Eval Loss'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">'Accuracy'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">'F1'</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>
                            <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">entry</span><span class="p">)</span>
                        <span class="k">if</span> <span class="s1">'loss'</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="p">:</span>
                            <span class="n">entry</span><span class="p">[</span><span class="s1">'Eval Loss'</span><span class="p">]</span> <span class="o">=</span> <span class="n">formatted_value</span>
                        <span class="k">elif</span> <span class="s1">'accuracy'</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="p">:</span>
                            <span class="n">entry</span><span class="p">[</span><span class="s1">'Accuracy'</span><span class="p">]</span> <span class="o">=</span> <span class="n">formatted_value</span>
                        <span class="k">elif</span> <span class="s1">'f1'</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="p">:</span>
                            <span class="n">entry</span><span class="p">[</span><span class="s1">'F1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">formatted_value</span>

    <span class="k">return</span> <span class="n">data</span>

<span class="n">metrics_data</span> <span class="o">=</span> <span class="n">extract_metrics</span><span class="p">(</span><span class="n">event_files</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">metrics_data</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">'Step'</span><span class="p">)</span>

<span class="n">file_path</span> <span class="o">=</span> <span class="s2">"./images/"</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s2">"_Checkpoint_Data.csv"</span>
<span class="n">df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>    Step Train Loss Eval Loss  Accuracy        F1
0   4202   0.158900  0.224847  0.919617  0.913330
1   8404   0.308700  0.192232  0.935582  0.932584
2  12606   0.217800  0.219186  0.941969  0.939364
3  16808   0.255700  0.207931  0.948771  0.945228
4  21010   0.298000  0.259813  0.946134  0.942236
5  25212   0.005000  0.308163  0.948216  0.945742
6  29414   0.066700  0.326928  0.951131  0.947972
7  33616   0.000100  0.384752  0.951548  0.949027
8  37818   0.000000  0.401674  0.952381  0.949276
9  42020   0.000000  0.424807  0.953353  0.950458
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=37cc5f92-d47f-4356-8fad-d9b64b6f5361">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [14]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">fillna</span><span class="p">({</span>
    <span class="s1">'Eval Loss'</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="s1">'inf'</span><span class="p">),</span>
    <span class="s1">'Accuracy'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">'F1'</span><span class="p">:</span> <span class="mi">0</span>
<span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">df</span><span class="p">[</span><span class="s1">'Eval Loss'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Eval Loss'</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'Accuracy'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Accuracy'</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'F1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'F1'</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>

<span class="n">df</span><span class="p">[</span><span class="s1">'Eval Loss Rank'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Eval Loss'</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'Accuracy Rank'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Accuracy'</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'F1 Rank'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'F1'</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">df</span><span class="p">[</span><span class="s1">'Rank Sum'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Eval Loss Rank'</span><span class="p">]</span> <span class="o">+</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Accuracy Rank'</span><span class="p">]</span> <span class="o">+</span> <span class="n">df</span><span class="p">[</span><span class="s1">'F1 Rank'</span><span class="p">]</span>

<span class="n">best_checkpoint</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">'Rank Sum'</span><span class="p">]</span><span class="o">.</span><span class="n">idxmin</span><span class="p">()]</span>

<span class="n">checkpoint_folder_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"checkpoint-</span><span class="si">{</span><span class="n">best_checkpoint</span><span class="p">[</span><span class="s1">'Step'</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Best Checkpoint Step: </span><span class="si">{</span><span class="n">checkpoint_folder_name</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">best_checkpoint</span><span class="p">[[</span><span class="s1">'Step'</span><span class="p">,</span> <span class="s1">'Train Loss'</span><span class="p">,</span> <span class="s1">'Eval Loss'</span><span class="p">,</span> <span class="s1">'Accuracy'</span><span class="p">,</span> <span class="s1">'F1'</span><span class="p">,</span> <span class="s1">'Rank Sum'</span><span class="p">]])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Best Checkpoint Step: checkpoint-42020
Step             42020
Train Loss    0.000000
Eval Loss     0.424807
Accuracy      0.953353
F1            0.950458
Rank Sum          12.0
Name: 9, dtype: object
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=5c3c3999-8c32-4a25-aaca-2ec9036b69ed">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Run-TensorBoard">Run TensorBoard<a class="anchor-link" href="#Run-TensorBoard">¶</a></h3><p>tensorboard --logdir=~/kuk/Praxis/praxis-Llama-2-7b-hf-small-finetune/logs --host=0.0.0.0</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=f8f36022-dc73-492f-998c-43e5ed1a6f46">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="PAUSE-SCRIPT">PAUSE SCRIPT<a class="anchor-link" href="#PAUSE-SCRIPT">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=444ae65b-241c-47ef-bbc2-81f3a2ce50e9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [26]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># My flag to pause the script, set to True to pause</span>
<span class="n">pause_script</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=c8be3672-68e0-44f8-b8b1-31290665658d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [27]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">StopExecution</span><span class="p">(</span><span class="ne">Exception</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">_render_traceback_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Script Paused"</span><span class="p">)</span>
        <span class="k">pass</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=eb3b5ed2-5320-49c0-9e61-90d6f0942f7a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [28]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="n">pause_script</span><span class="p">:</span>
    <span class="k">raise</span> <span class="n">StopExecution</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=19be6e26-d888-4da0-b3f8-836d68ac2051">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Testing">Testing<a class="anchor-link" href="#Testing">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=b5111194-5eeb-4104-8df0-f977a6b716e0">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [29]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>

<span class="n">bnb_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span>
    <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="s2">"nf4"</span><span class="p">,</span>
    <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
<span class="p">)</span>

<span class="n">base_model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">checkpoint</span><span class="p">,</span>
    <span class="n">quantization_config</span><span class="o">=</span><span class="n">bnb_config</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span>
    <span class="n">num_labels</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">token</span><span class="o">=</span><span class="n">access_token</span><span class="p">,</span>
    <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">eval_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">checkpoint</span><span class="p">,</span>
    <span class="n">token</span><span class="o">=</span><span class="n">access_token</span><span class="p">,</span>
    <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2-large and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=530fa55f-c8c4-485f-a093-09bf2175d586">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [30]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">PeftModel</span>
<span class="n">test_checkpoint_name</span> <span class="o">=</span> <span class="n">checkpoint_folder_name</span>
<span class="n">ft_model</span> <span class="o">=</span> <span class="n">PeftModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model</span><span class="p">,</span> <span class="n">project_name</span><span class="o">+</span><span class="s1">'/'</span><span class="o">+</span><span class="n">test_checkpoint_name</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=27354c4f-95cc-4d1f-ac64-c5e6b4a842ae">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [31]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Using"</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">(),</span> <span class="s2">"GPUs!"</span><span class="p">)</span>
    <span class="n">ft_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">ft_model</span><span class="p">)</span>

<span class="n">ft_model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[31]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>PeftModelForSequenceClassification(
  (base_model): LoraModel(
    (model): GPT2ForSequenceClassification(
      (transformer): GPT2Model(
        (wte): Embedding(50257, 1280)
        (wpe): Embedding(1024, 1280)
        (drop): Dropout(p=0.1, inplace=False)
        (h): ModuleList(
          (0-35): 36 x GPT2Block(
            (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=1280, out_features=3840, bias=True)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=1280, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=3840, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (c_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=True)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=1280, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1280, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=1280, out_features=5120, bias=True)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=1280, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=5120, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (c_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=5120, out_features=1280, bias=True)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=5120, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1280, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      )
      (score): ModulesToSaveWrapper(
        (original_module): Linear(in_features=1280, out_features=2, bias=False)
        (modules_to_save): ModuleDict(
          (default): Linear(in_features=1280, out_features=2, bias=False)
        )
      )
    )
  )
)</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=eee9ab6f-a7d5-4dd5-9436-2f6f6e2bffaf">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [32]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tqdm.auto</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="n">processed_predictions</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">tokenized_test_ds</span><span class="p">):</span>

        <span class="n">eval_prompt</span> <span class="o">=</span> <span class="n">record</span><span class="p">[</span><span class="s1">'article'</span><span class="p">]</span>
        <span class="n">model_input</span> <span class="o">=</span> <span class="n">tokenize_fn</span><span class="p">({</span><span class="s1">'article'</span><span class="p">:</span> <span class="n">eval_prompt</span><span class="p">})</span>

        <span class="c1"># model_input = {k: v.to('cuda') for k, v in model_input.items()}</span>
        
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">ft_model</span><span class="p">(</span><span class="o">**</span><span class="n">model_input</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>
        
        <span class="n">prediction</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>  <span class="c1"># Use .item() to get a Python number</span>
        <span class="n">processed_predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">prediction</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>  0%|          | 0/7203 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=80f06a2c-5ded-4da0-8232-145383967c9d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Accuracy-and-F1">Accuracy and F1<a class="anchor-link" href="#Accuracy-and-F1">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=6e891bc5-55e0-4c43-a035-0edc37dcb6e3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [33]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">true_articles</span> <span class="o">=</span> <span class="n">tokenized_test_ds</span><span class="p">[</span><span class="s1">'label'</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=6bf33824-d7cc-4d22-af4d-7d44e569c2b9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [34]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span><span class="p">,</span> <span class="n">accuracy_score</span>

<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">true_articles</span><span class="p">,</span> <span class="n">processed_predictions</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"accuracy:"</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>accuracy: 0.9555740663612384
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=9b37dc24-3bc0-48a0-ace9-a360bae91169">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [35]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">true_articles</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=a29ebf68-632f-49d1-b903-407ff05b5569">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [36]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">processed_predictions</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=fb4d8350-a8d1-4853-a9f8-74ae9ea36bde">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [37]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">f1_score</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">true_articles</span><span class="p">,</span> <span class="n">processed_predictions</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">'macro'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"f1_score:"</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>f1_score: 0.9528843919491377
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=2dcaac14-2b83-4c24-b83a-f6d0e73a2d2a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Confusion-Matrix">Confusion Matrix<a class="anchor-link" href="#Confusion-Matrix">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=57e64afb-e3ee-4c79-8228-b2d79806229e">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [38]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">ConfusionMatrixDisplay</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">tokenized_test_ds</span><span class="p">[</span><span class="s1">'label'</span><span class="p">],</span> <span class="n">processed_predictions</span><span class="p">)</span>

<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'human'</span><span class="p">,</span> <span class="s1">'machine'</span><span class="p">]</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="o">=</span><span class="n">cm</span><span class="p">,</span> <span class="n">display_labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Blues</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=af7d6f55-fb4f-4195-ac3f-93852ecd4b65">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [40]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">file_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">project_name</span><span class="si">}</span><span class="s2">-v12.ipynb"</span>
<span class="n">html_file_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">file_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">'.ipynb'</span><span class="p">,</span><span class="w"> </span><span class="s1">'.html'</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span>

<span class="n">command</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"jupyter nbconvert '</span><span class="si">{</span><span class="n">file_name</span><span class="si">}</span><span class="s2">' --to html --output-dir './html' --output '</span><span class="si">{</span><span class="n">html_file_name</span><span class="si">}</span><span class="s2">'"</span>
<span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">system</span><span class="p">(</span><span class="n">command</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[NbConvertApp] Converting notebook praxis-gpt2-large-small-finetune-v10.ipynb to html
[NbConvertApp] WARNING | Alternative text is missing on 1 image(s).
[NbConvertApp] Writing 1308479 bytes to html/praxis-gpt2-large-small-finetune-v10.html
</pre>
</div>
</div>
</div>
</div>
</div>
</main>
</body>
</html>
