<!DOCTYPE html>

<html lang="en">
<head><meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>praxis-Mistral-7B-v0.1-small-finetune-v12</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<style type="text/css">
    pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: var(--jp-cell-editor-active-background) }
.highlight { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) }
.highlight .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */
.highlight .err { color: var(--jp-mirror-editor-error-color) } /* Error */
.highlight .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */
.highlight .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */
.highlight .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */
.highlight .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */
.highlight .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */
.highlight .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */
.highlight .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */
.highlight .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */
.highlight .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */
.highlight .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */
.highlight .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */
.highlight .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */
.highlight .pm { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation.Marker */
.highlight .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */
.highlight .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */
.highlight .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */
.highlight .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */
.highlight .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */
.highlight .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */
.highlight .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */
.highlight .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */
.highlight .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */
.highlight .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */
.highlight .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */
.highlight .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */
.highlight .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */
.highlight .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */
.highlight .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */
.highlight .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */
.highlight .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */
.highlight .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */
.highlight .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */
.highlight .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */
  </style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
 * Mozilla scrollbar styling
 */

/* use standard opaque scrollbars for most nodes */
[data-jp-theme-scrollbars='true'] {
  scrollbar-color: rgb(var(--jp-scrollbar-thumb-color))
    var(--jp-scrollbar-background-color);
}

/* for code nodes, use a transparent style of scrollbar. These selectors
 * will match lower in the tree, and so will override the above */
[data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar,
[data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
}

/* tiny scrollbar */

.jp-scrollbar-tiny {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
  scrollbar-width: thin;
}

/* tiny scrollbar */

.jp-scrollbar-tiny::-webkit-scrollbar,
.jp-scrollbar-tiny::-webkit-scrollbar-corner {
  background-color: transparent;
  height: 4px;
  width: 4px;
}

.jp-scrollbar-tiny::-webkit-scrollbar-thumb {
  background: rgba(var(--jp-scrollbar-thumb-color), 0.5);
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:horizontal {
  border-left: 0 solid transparent;
  border-right: 0 solid transparent;
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:vertical {
  border-top: 0 solid transparent;
  border-bottom: 0 solid transparent;
}

/*
 * Lumino
 */

.lm-ScrollBar[data-orientation='horizontal'] {
  min-height: 16px;
  max-height: 16px;
  min-width: 45px;
  border-top: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] {
  min-width: 16px;
  max-width: 16px;
  min-height: 45px;
  border-left: 1px solid #a0a0a0;
}

.lm-ScrollBar-button {
  background-color: #f0f0f0;
  background-position: center center;
  min-height: 15px;
  max-height: 15px;
  min-width: 15px;
  max-width: 15px;
}

.lm-ScrollBar-button:hover {
  background-color: #dadada;
}

.lm-ScrollBar-button.lm-mod-active {
  background-color: #cdcdcd;
}

.lm-ScrollBar-track {
  background: #f0f0f0;
}

.lm-ScrollBar-thumb {
  background: #cdcdcd;
}

.lm-ScrollBar-thumb:hover {
  background: #bababa;
}

.lm-ScrollBar-thumb.lm-mod-active {
  background: #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal'] .lm-ScrollBar-thumb {
  height: 100%;
  min-width: 15px;
  border-left: 1px solid #a0a0a0;
  border-right: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] .lm-ScrollBar-thumb {
  width: 100%;
  min-height: 15px;
  border-top: 1px solid #a0a0a0;
  border-bottom: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-left);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-right);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-up);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-down);
  background-size: 17px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Widget {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
}

.lm-Widget.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.lm-AccordionPanel[data-orientation='horizontal'] > .lm-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  display: flex;
  flex-direction: column;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-CommandPalette-search {
  flex: 0 0 auto;
}

.lm-CommandPalette-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  min-height: 0;
  overflow: auto;
  list-style-type: none;
}

.lm-CommandPalette-header {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-CommandPalette-item {
  display: flex;
  flex-direction: row;
}

.lm-CommandPalette-itemIcon {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemContent {
  flex: 1 1 auto;
  overflow: hidden;
}

.lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemLabel {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-close-icon {
  border: 1px solid transparent;
  background-color: transparent;
  position: absolute;
  z-index: 1;
  right: 3%;
  top: 0;
  bottom: 0;
  margin: auto;
  padding: 7px 0;
  display: none;
  vertical-align: middle;
  outline: 0;
  cursor: pointer;
}
.lm-close-icon:after {
  content: 'X';
  display: block;
  width: 15px;
  height: 15px;
  text-align: center;
  color: #000;
  font-weight: normal;
  font-size: 12px;
  cursor: pointer;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-DockPanel {
  z-index: 0;
}

.lm-DockPanel-widget {
  z-index: 0;
}

.lm-DockPanel-tabBar {
  z-index: 1;
}

.lm-DockPanel-handle {
  z-index: 2;
}

.lm-DockPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-DockPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-DockPanel-handle[data-orientation='horizontal'] {
  cursor: ew-resize;
}

.lm-DockPanel-handle[data-orientation='vertical'] {
  cursor: ns-resize;
}

.lm-DockPanel-handle[data-orientation='horizontal']:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-DockPanel-handle[data-orientation='vertical']:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

.lm-DockPanel-overlay {
  z-index: 3;
  box-sizing: border-box;
  pointer-events: none;
}

.lm-DockPanel-overlay.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Menu {
  z-index: 10000;
  position: absolute;
  white-space: nowrap;
  overflow-x: hidden;
  overflow-y: auto;
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-Menu-content {
  margin: 0;
  padding: 0;
  display: table;
  list-style-type: none;
}

.lm-Menu-item {
  display: table-row;
}

.lm-Menu-item.lm-mod-hidden,
.lm-Menu-item.lm-mod-collapsed {
  display: none !important;
}

.lm-Menu-itemIcon,
.lm-Menu-itemSubmenuIcon {
  display: table-cell;
  text-align: center;
}

.lm-Menu-itemLabel {
  display: table-cell;
  text-align: left;
}

.lm-Menu-itemShortcut {
  display: table-cell;
  text-align: right;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-MenuBar {
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-MenuBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex-direction: row;
  list-style-type: none;
}

.lm-MenuBar-item {
  box-sizing: border-box;
}

.lm-MenuBar-itemIcon,
.lm-MenuBar-itemLabel {
  display: inline-block;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-ScrollBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-ScrollBar[data-orientation='horizontal'] {
  flex-direction: row;
}

.lm-ScrollBar[data-orientation='vertical'] {
  flex-direction: column;
}

.lm-ScrollBar-button {
  box-sizing: border-box;
  flex: 0 0 auto;
}

.lm-ScrollBar-track {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
  flex: 1 1 auto;
}

.lm-ScrollBar-thumb {
  box-sizing: border-box;
  position: absolute;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-SplitPanel-child {
  z-index: 0;
}

.lm-SplitPanel-handle {
  z-index: 1;
}

.lm-SplitPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-SplitPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle {
  cursor: ew-resize;
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle {
  cursor: ns-resize;
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-TabBar[data-orientation='horizontal'] {
  flex-direction: row;
  align-items: flex-end;
}

.lm-TabBar[data-orientation='vertical'] {
  flex-direction: column;
  align-items: flex-end;
}

.lm-TabBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex: 1 1 auto;
  list-style-type: none;
}

.lm-TabBar[data-orientation='horizontal'] > .lm-TabBar-content {
  flex-direction: row;
}

.lm-TabBar[data-orientation='vertical'] > .lm-TabBar-content {
  flex-direction: column;
}

.lm-TabBar-tab {
  display: flex;
  flex-direction: row;
  box-sizing: border-box;
  overflow: hidden;
  touch-action: none; /* Disable native Drag/Drop */
}

.lm-TabBar-tabIcon,
.lm-TabBar-tabCloseIcon {
  flex: 0 0 auto;
}

.lm-TabBar-tabLabel {
  flex: 1 1 auto;
  overflow: hidden;
  white-space: nowrap;
}

.lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
}

.lm-TabBar-tab.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar-addButton.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab {
  position: relative;
}

.lm-TabBar.lm-mod-dragging[data-orientation='horizontal'] .lm-TabBar-tab {
  left: 0;
  transition: left 150ms ease;
}

.lm-TabBar.lm-mod-dragging[data-orientation='vertical'] .lm-TabBar-tab {
  top: 0;
  transition: top 150ms ease;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging {
  transition: none;
}

.lm-TabBar-tabLabel .lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
  background: inherit;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabPanel-tabBar {
  z-index: 1;
}

.lm-TabPanel-stackedPanel {
  z-index: 0;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapse {
  display: flex;
  flex-direction: column;
  align-items: stretch;
}

.jp-Collapse-header {
  padding: 1px 12px;
  background-color: var(--jp-layout-color1);
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  align-items: center;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  text-transform: uppercase;
  user-select: none;
}

.jp-Collapser-icon {
  height: 16px;
}

.jp-Collapse-header-collapsed .jp-Collapser-icon {
  transform: rotate(-90deg);
  margin: auto 0;
}

.jp-Collapser-title {
  line-height: 25px;
}

.jp-Collapse-contents {
  padding: 0 12px;
  background-color: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensureUiComponents() in @jupyterlab/buildutils */

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

/* Icons urls */

:root {
  --jp-icon-add-above: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5MikiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik00Ljc1IDQuOTMwNjZINi42MjVWNi44MDU2NkM2LjYyNSA3LjAxMTkxIDYuNzkzNzUgNy4xODA2NiA3IDcuMTgwNjZDNy4yMDYyNSA3LjE4MDY2IDcuMzc1IDcuMDExOTEgNy4zNzUgNi44MDU2NlY0LjkzMDY2SDkuMjVDOS40NTYyNSA0LjkzMDY2IDkuNjI1IDQuNzYxOTEgOS42MjUgNC41NTU2NkM5LjYyNSA0LjM0OTQxIDkuNDU2MjUgNC4xODA2NiA5LjI1IDQuMTgwNjZINy4zNzVWMi4zMDU2NkM3LjM3NSAyLjA5OTQxIDcuMjA2MjUgMS45MzA2NiA3IDEuOTMwNjZDNi43OTM3NSAxLjkzMDY2IDYuNjI1IDIuMDk5NDEgNi42MjUgMi4zMDU2NlY0LjE4MDY2SDQuNzVDNC41NDM3NSA0LjE4MDY2IDQuMzc1IDQuMzQ5NDEgNC4zNzUgNC41NTU2NkM0LjM3NSA0Ljc2MTkxIDQuNTQzNzUgNC45MzA2NiA0Ljc1IDQuOTMwNjZaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC43Ii8+CjwvZz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTExLjUgOS41VjExLjVMMi41IDExLjVWOS41TDExLjUgOS41Wk0xMiA4QzEyLjU1MjMgOCAxMyA4LjQ0NzcyIDEzIDlWMTJDMTMgMTIuNTUyMyAxMi41NTIzIDEzIDEyIDEzTDIgMTNDMS40NDc3MiAxMyAxIDEyLjU1MjMgMSAxMlY5QzEgOC40NDc3MiAxLjQ0NzcxIDggMiA4TDEyIDhaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5MiI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KC0xIDAgMCAxIDEwIDEuNTU1NjYpIi8+CjwvY2xpcFBhdGg+CjwvZGVmcz4KPC9zdmc+Cg==);
  --jp-icon-add-below: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5OCkiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik05LjI1IDEwLjA2OTNMNy4zNzUgMTAuMDY5M0w3LjM3NSA4LjE5NDM0QzcuMzc1IDcuOTg4MDkgNy4yMDYyNSA3LjgxOTM0IDcgNy44MTkzNEM2Ljc5Mzc1IDcuODE5MzQgNi42MjUgNy45ODgwOSA2LjYyNSA4LjE5NDM0TDYuNjI1IDEwLjA2OTNMNC43NSAxMC4wNjkzQzQuNTQzNzUgMTAuMDY5MyA0LjM3NSAxMC4yMzgxIDQuMzc1IDEwLjQ0NDNDNC4zNzUgMTAuNjUwNiA0LjU0Mzc1IDEwLjgxOTMgNC43NSAxMC44MTkzTDYuNjI1IDEwLjgxOTNMNi42MjUgMTIuNjk0M0M2LjYyNSAxMi45MDA2IDYuNzkzNzUgMTMuMDY5MyA3IDEzLjA2OTNDNy4yMDYyNSAxMy4wNjkzIDcuMzc1IDEyLjkwMDYgNy4zNzUgMTIuNjk0M0w3LjM3NSAxMC44MTkzTDkuMjUgMTAuODE5M0M5LjQ1NjI1IDEwLjgxOTMgOS42MjUgMTAuNjUwNiA5LjYyNSAxMC40NDQzQzkuNjI1IDEwLjIzODEgOS40NTYyNSAxMC4wNjkzIDkuMjUgMTAuMDY5M1oiIGZpbGw9IiM2MTYxNjEiIHN0cm9rZT0iIzYxNjE2MSIgc3Ryb2tlLXdpZHRoPSIwLjciLz4KPC9nPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMi41IDUuNUwyLjUgMy41TDExLjUgMy41TDExLjUgNS41TDIuNSA1LjVaTTIgN0MxLjQ0NzcyIDcgMSA2LjU1MjI4IDEgNkwxIDNDMSAyLjQ0NzcyIDEuNDQ3NzIgMiAyIDJMMTIgMkMxMi41NTIzIDIgMTMgMi40NDc3MiAxMyAzTDEzIDZDMTMgNi41NTIyOSAxMi41NTIzIDcgMTIgN0wyIDdaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5OCI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KDEgMS43NDg0NmUtMDcgMS43NDg0NmUtMDcgLTEgNCAxMy40NDQzKSIvPgo8L2NsaXBQYXRoPgo8L2RlZnM+Cjwvc3ZnPgo=);
  --jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bell: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE2IDE2IiB2ZXJzaW9uPSIxLjEiPgogICA8cGF0aCBjbGFzcz0ianAtaWNvbjIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMzMzMzMzIgogICAgICBkPSJtOCAwLjI5Yy0xLjQgMC0yLjcgMC43My0zLjYgMS44LTEuMiAxLjUtMS40IDMuNC0xLjUgNS4yLTAuMTggMi4yLTAuNDQgNC0yLjMgNS4zbDAuMjggMS4zaDVjMC4wMjYgMC42NiAwLjMyIDEuMSAwLjcxIDEuNSAwLjg0IDAuNjEgMiAwLjYxIDIuOCAwIDAuNTItMC40IDAuNi0xIDAuNzEtMS41aDVsMC4yOC0xLjNjLTEuOS0wLjk3LTIuMi0zLjMtMi4zLTUuMy0wLjEzLTEuOC0wLjI2LTMuNy0xLjUtNS4yLTAuODUtMS0yLjItMS44LTMuNi0xLjh6bTAgMS40YzAuODggMCAxLjkgMC41NSAyLjUgMS4zIDAuODggMS4xIDEuMSAyLjcgMS4yIDQuNCAwLjEzIDEuNyAwLjIzIDMuNiAxLjMgNS4yaC0xMGMxLjEtMS42IDEuMi0zLjQgMS4zLTUuMiAwLjEzLTEuNyAwLjMtMy4zIDEuMi00LjQgMC41OS0wLjcyIDEuNi0xLjMgMi41LTEuM3ptLTAuNzQgMTJoMS41Yy0wLjAwMTUgMC4yOCAwLjAxNSAwLjc5LTAuNzQgMC43OS0wLjczIDAuMDAxNi0wLjcyLTAuNTMtMC43NC0wLjc5eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-bug-dot: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiPgogICAgICAgIDxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMTcuMTkgOEgyMFYxMEgxNy45MUMxNy45NiAxMC4zMyAxOCAxMC42NiAxOCAxMVYxMkgyMFYxNEgxOC41SDE4VjE0LjAyNzVDMTUuNzUgMTQuMjc2MiAxNCAxNi4xODM3IDE0IDE4LjVDMTQgMTkuMjA4IDE0LjE2MzUgMTkuODc3OSAxNC40NTQ5IDIwLjQ3MzlDMTMuNzA2MyAyMC44MTE3IDEyLjg3NTcgMjEgMTIgMjFDOS43OCAyMSA3Ljg1IDE5Ljc5IDYuODEgMThINFYxNkg2LjA5QzYuMDQgMTUuNjcgNiAxNS4zNCA2IDE1VjE0SDRWMTJINlYxMUM2IDEwLjY2IDYuMDQgMTAuMzMgNi4wOSAxMEg0VjhINi44MUM3LjI2IDcuMjIgNy44OCA2LjU1IDguNjIgNi4wNEw3IDQuNDFMOC40MSAzTDEwLjU5IDUuMTdDMTEuMDQgNS4wNiAxMS41MSA1IDEyIDVDMTIuNDkgNSAxMi45NiA1LjA2IDEzLjQyIDUuMTdMMTUuNTkgM0wxNyA0LjQxTDE1LjM3IDYuMDRDMTYuMTIgNi41NSAxNi43NCA3LjIyIDE3LjE5IDhaTTEwIDE2SDE0VjE0SDEwVjE2Wk0xMCAxMkgxNFYxMEgxMFYxMloiIGZpbGw9IiM2MTYxNjEiLz4KICAgICAgICA8cGF0aCBkPSJNMjIgMTguNUMyMiAyMC40MzMgMjAuNDMzIDIyIDE4LjUgMjJDMTYuNTY3IDIyIDE1IDIwLjQzMyAxNSAxOC41QzE1IDE2LjU2NyAxNi41NjcgMTUgMTguNSAxNUMyMC40MzMgMTUgMjIgMTYuNTY3IDIyIDE4LjVaIiBmaWxsPSIjNjE2MTYxIi8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yMCA4aC0yLjgxYy0uNDUtLjc4LTEuMDctMS40NS0xLjgyLTEuOTZMMTcgNC40MSAxNS41OSAzbC0yLjE3IDIuMTdDMTIuOTYgNS4wNiAxMi40OSA1IDEyIDVjLS40OSAwLS45Ni4wNi0xLjQxLjE3TDguNDEgMyA3IDQuNDFsMS42MiAxLjYzQzcuODggNi41NSA3LjI2IDcuMjIgNi44MSA4SDR2MmgyLjA5Yy0uMDUuMzMtLjA5LjY2LS4wOSAxdjFINHYyaDJ2MWMwIC4zNC4wNC42Ny4wOSAxSDR2MmgyLjgxYzEuMDQgMS43OSAyLjk3IDMgNS4xOSAzczQuMTUtMS4yMSA1LjE5LTNIMjB2LTJoLTIuMDljLjA1LS4zMy4wOS0uNjYuMDktMXYtMWgydi0yaC0ydi0xYzAtLjM0LS4wNC0uNjctLjA5LTFIMjBWOHptLTYgOGgtNHYtMmg0djJ6bTAtNGgtNHYtMmg0djJ6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-build: url(data:image/svg+xml;base64,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);
  --jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik05IDE2LjE3TDQuODMgMTJsLTEuNDIgMS40MUw5IDE5IDIxIDdsLTEuNDEtMS40MXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBzaGFwZS1yZW5kZXJpbmc9Imdlb21ldHJpY1ByZWNpc2lvbiI+CiAgICA8cGF0aCBkPSJNNi41OSwzLjQxTDIsOEw2LjU5LDEyLjZMOCwxMS4xOEw0LjgyLDhMOCw0LjgyTDYuNTksMy40MU0xMi40MSwzLjQxTDExLDQuODJMMTQuMTgsOEwxMSwxMS4xOEwxMi40MSwxMi42TDE3LDhMMTIuNDEsMy40MU0yMS41OSwxMS41OUwxMy41LDE5LjY4TDkuODMsMTZMOC40MiwxNy40MUwxMy41LDIyLjVMMjMsMTNMMjEuNTksMTEuNTlaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTExLjQgMTguNkw2LjggMTRMMTEuNCA5LjRMMTAgOEw0IDE0TDEwIDIwTDExLjQgMTguNlpNMTYuNiAxOC42TDIxLjIgMTRMMTYuNiA5LjRMMTggOEwyNCAxNEwxOCAyMEwxNi42IDE4LjZWMTguNloiLz4KCTwvZz4KPC9zdmc+Cg==);
  --jp-icon-collapse-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNNiAxM3YyaDh2LTJ6IiAvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1jb25zb2xlLWljb24tYmFja2dyb3VuZC1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtY29uc29sZS1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIj4KICAgIDxwYXRoIGQ9Ik0xMDUgMTI3LjNoNDB2MTIuOGgtNDB6TTUxLjEgNzdMNzQgOTkuOWwtMjMuMyAyMy4zIDEwLjUgMTAuNSAyMy4zLTIzLjNMOTUgOTkuOSA4NC41IDg5LjQgNjEuNiA2Ni41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copyright: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGVuYWJsZS1iYWNrZ3JvdW5kPSJuZXcgMCAwIDI0IDI0IiBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCI+CiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0xMS44OCw5LjE0YzEuMjgsMC4wNiwxLjYxLDEuMTUsMS42MywxLjY2aDEuNzljLTAuMDgtMS45OC0xLjQ5LTMuMTktMy40NS0zLjE5QzkuNjQsNy42MSw4LDksOCwxMi4xNCBjMCwxLjk0LDAuOTMsNC4yNCwzLjg0LDQuMjRjMi4yMiwwLDMuNDEtMS42NSwzLjQ0LTIuOTVoLTEuNzljLTAuMDMsMC41OS0wLjQ1LDEuMzgtMS42MywxLjQ0QzEwLjU1LDE0LjgzLDEwLDEzLjgxLDEwLDEyLjE0IEMxMCw5LjI1LDExLjI4LDkuMTYsMTEuODgsOS4xNHogTTEyLDJDNi40OCwyLDIsNi40OCwyLDEyczQuNDgsMTAsMTAsMTBzMTAtNC40OCwxMC0xMFMxNy41MiwyLDEyLDJ6IE0xMiwyMGMtNC40MSwwLTgtMy41OS04LTggczMuNTktOCw4LThzOCwzLjU5LDgsOFMxNi40MSwyMCwxMiwyMHoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-delete: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2cHgiIGhlaWdodD0iMTZweCI+CiAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIiAvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjI2MjYyIiBkPSJNNiAxOWMwIDEuMS45IDIgMiAyaDhjMS4xIDAgMi0uOSAyLTJWN0g2djEyek0xOSA0aC0zLjVsLTEtMWgtNWwtMSAxSDV2MmgxNFY0eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-duplicate: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTIuNzk5OTggMC44NzVIOC44OTU4MkM5LjIwMDYxIDAuODc1IDkuNDQ5OTggMS4xMzkxNCA5LjQ0OTk4IDEuNDYxOThDOS40NDk5OCAxLjc4NDgyIDkuMjAwNjEgMi4wNDg5NiA4Ljg5NTgyIDIuMDQ4OTZIMy4zNTQxNUMzLjA0OTM2IDIuMDQ4OTYgMi43OTk5OCAyLjMxMzEgMi43OTk5OCAyLjYzNTk0VjkuNjc5NjlDMi43OTk5OCAxMC4wMDI1IDIuNTUwNjEgMTAuMjY2NyAyLjI0NTgyIDEwLjI2NjdDMS45NDEwMyAxMC4yNjY3IDEuNjkxNjUgMTAuMDAyNSAxLjY5MTY1IDkuNjc5NjlWMi4wNDg5NkMxLjY5MTY1IDEuNDAzMjggMi4xOTA0IDAuODc1IDIuNzk5OTggMC44NzVaTTUuMzY2NjUgMTEuOVY0LjU1SDExLjA4MzNWMTEuOUg1LjM2NjY1Wk00LjE0MTY1IDQuMTQxNjdDNC4xNDE2NSAzLjY5MDYzIDQuNTA3MjggMy4zMjUgNC45NTgzMiAzLjMyNUgxMS40OTE3QzExLjk0MjcgMy4zMjUgMTIuMzA4MyAzLjY5MDYzIDEyLjMwODMgNC4xNDE2N1YxMi4zMDgzQzEyLjMwODMgMTIuNzU5NCAxMS45NDI3IDEzLjEyNSAxMS40OTE3IDEzLjEyNUg0Ljk1ODMyQzQuNTA3MjggMTMuMTI1IDQuMTQxNjUgMTIuNzU5NCA0LjE0MTY1IDEyLjMwODNWNC4xNDE2N1oiIGZpbGw9IiM2MTYxNjEiLz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNOS40MzU3NCA4LjI2NTA3SDguMzY0MzFWOS4zMzY1QzguMzY0MzEgOS40NTQzNSA4LjI2Nzg4IDkuNTUwNzggOC4xNTAwMiA5LjU1MDc4QzguMDMyMTcgOS41NTA3OCA3LjkzNTc0IDkuNDU0MzUgNy45MzU3NCA5LjMzNjVWOC4yNjUwN0g2Ljg2NDMxQzYuNzQ2NDUgOC4yNjUwNyA2LjY1MDAyIDguMTY4NjQgNi42NTAwMiA4LjA1MDc4QzYuNjUwMDIgNy45MzI5MiA2Ljc0NjQ1IDcuODM2NSA2Ljg2NDMxIDcuODM2NUg3LjkzNTc0VjYuNzY1MDdDNy45MzU3NCA2LjY0NzIxIDguMDMyMTcgNi41NTA3OCA4LjE1MDAyIDYuNTUwNzhDOC4yNjc4OCA2LjU1MDc4IDguMzY0MzEgNi42NDcyMSA4LjM2NDMxIDYuNzY1MDdWNy44MzY1SDkuNDM1NzRDOS41NTM2IDcuODM2NSA5LjY1MDAyIDcuOTMyOTIgOS42NTAwMiA4LjA1MDc4QzkuNjUwMDIgOC4xNjg2NCA5LjU1MzYgOC4yNjUwNyA5LjQzNTc0IDguMjY1MDdaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC41Ii8+Cjwvc3ZnPgo=);
  --jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-error: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj48Y2lyY2xlIGN4PSIxMiIgY3k9IjE5IiByPSIyIi8+PHBhdGggZD0iTTEwIDNoNHYxMmgtNHoiLz48L2c+CjxwYXRoIGZpbGw9Im5vbmUiIGQ9Ik0wIDBoMjR2MjRIMHoiLz4KPC9zdmc+Cg==);
  --jp-icon-expand-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNMTEgMTBIOXYzSDZ2MmgzdjNoMnYtM2gzdi0yaC0zeiIgLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-filter-dot: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWRvdCIgZmlsbD0iI0ZGRiI+CiAgICA8Y2lyY2xlIGN4PSIxOCIgY3k9IjE3IiByPSIzIj48L2NpcmNsZT4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-filter: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-folder-favorite: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgwVjB6IiBmaWxsPSJub25lIi8+PHBhdGggY2xhc3M9ImpwLWljb24zIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzYxNjE2MSIgZD0iTTIwIDZoLThsLTItMkg0Yy0xLjEgMC0yIC45LTIgMnYxMmMwIDEuMS45IDIgMiAyaDE2YzEuMSAwIDItLjkgMi0yVjhjMC0xLjEtLjktMi0yLTJ6bS0yLjA2IDExTDE1IDE1LjI4IDEyLjA2IDE3bC43OC0zLjMzLTIuNTktMi4yNCAzLjQxLS4yOUwxNSA4bDEuMzQgMy4xNCAzLjQxLjI5LTIuNTkgMi4yNC43OCAzLjMzeiIvPgo8L3N2Zz4K);
  --jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-home: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPjxwYXRoIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xMCAyMHYtNmg0djZoNXYtOGgzTDEyIDMgMiAxMmgzdjh6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==);
  --jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-info: url(data:image/svg+xml;base64,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);
  --jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaW5zcGVjdG9yLWljb24tY29sb3IganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K);
  --jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtanNvbi1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0Y5QTgyNSI+CiAgICA8cGF0aCBkPSJNMjAuMiAxMS44Yy0xLjYgMC0xLjcuNS0xLjcgMSAwIC40LjEuOS4xIDEuMy4xLjUuMS45LjEgMS4zIDAgMS43LTEuNCAyLjMtMy41IDIuM2gtLjl2LTEuOWguNWMxLjEgMCAxLjQgMCAxLjQtLjggMC0uMyAwLS42LS4xLTEgMC0uNC0uMS0uOC0uMS0xLjIgMC0xLjMgMC0xLjggMS4zLTItMS4zLS4yLTEuMy0uNy0xLjMtMiAwLS40LjEtLjguMS0xLjIuMS0uNC4xLS43LjEtMSAwLS44LS40LS43LTEuNC0uOGgtLjVWNC4xaC45YzIuMiAwIDMuNS43IDMuNSAyLjMgMCAuNC0uMS45LS4xIDEuMy0uMS41LS4xLjktLjEgMS4zIDAgLjUuMiAxIDEuNyAxdjEuOHpNMS44IDEwLjFjMS42IDAgMS43LS41IDEuNy0xIDAtLjQtLjEtLjktLjEtMS4zLS4xLS41LS4xLS45LS4xLTEuMyAwLTEuNiAxLjQtMi4zIDMuNS0yLjNoLjl2MS45aC0uNWMtMSAwLTEuNCAwLTEuNC44IDAgLjMgMCAuNi4xIDEgMCAuMi4xLjYuMSAxIDAgMS4zIDAgMS44LTEuMyAyQzYgMTEuMiA2IDExLjcgNiAxM2MwIC40LS4xLjgtLjEgMS4yLS4xLjMtLjEuNy0uMSAxIDAgLjguMy44IDEuNC44aC41djEuOWgtLjljLTIuMSAwLTMuNS0uNi0zLjUtMi4zIDAtLjQuMS0uOS4xLTEuMy4xLS41LjEtLjkuMS0xLjMgMC0uNS0uMi0xLTEuNy0xdi0xLjl6Ii8+CiAgICA8Y2lyY2xlIGN4PSIxMSIgY3k9IjEzLjgiIHI9IjIuMSIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSI4LjIiIHI9IjIuMSIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-julia: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDMyNSAzMDAiPgogIDxnIGNsYXNzPSJqcC1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjY2IzYzMzIj4KICAgIDxwYXRoIGQ9Ik0gMTUwLjg5ODQzOCAyMjUgQyAxNTAuODk4NDM4IDI2Ni40MjE4NzUgMTE3LjMyMDMxMiAzMDAgNzUuODk4NDM4IDMwMCBDIDM0LjQ3NjU2MiAzMDAgMC44OTg0MzggMjY2LjQyMTg3NSAwLjg5ODQzOCAyMjUgQyAwLjg5ODQzOCAxODMuNTc4MTI1IDM0LjQ3NjU2MiAxNTAgNzUuODk4NDM4IDE1MCBDIDExNy4zMjAzMTIgMTUwIDE1MC44OTg0MzggMTgzLjU3ODEyNSAxNTAuODk4NDM4IDIyNSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzM4OTgyNiI+CiAgICA8cGF0aCBkPSJNIDIzNy41IDc1IEMgMjM3LjUgMTE2LjQyMTg3NSAyMDMuOTIxODc1IDE1MCAxNjIuNSAxNTAgQyAxMjEuMDc4MTI1IDE1MCA4Ny41IDExNi40MjE4NzUgODcuNSA3NSBDIDg3LjUgMzMuNTc4MTI1IDEyMS4wNzgxMjUgMCAxNjIuNSAwIEMgMjAzLjkyMTg3NSAwIDIzNy41IDMzLjU3ODEyNSAyMzcuNSA3NSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzk1NThiMiI+CiAgICA8cGF0aCBkPSJNIDMyNC4xMDE1NjIgMjI1IEMgMzI0LjEwMTU2MiAyNjYuNDIxODc1IDI5MC41MjM0MzggMzAwIDI0OS4xMDE1NjIgMzAwIEMgMjA3LjY3OTY4OCAzMDAgMTc0LjEwMTU2MiAyNjYuNDIxODc1IDE3NC4xMDE1NjIgMjI1IEMgMTc0LjEwMTU2MiAxODMuNTc4MTI1IDIwNy42Nzk2ODggMTUwIDI0OS4xMDE1NjIgMTUwIEMgMjkwLjUyMzQzOCAxNTAgMzI0LjEwMTU2MiAxODMuNTc4MTI1IDMyNC4xMDE1NjIgMjI1Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgPGcgY2xhc3M9ImpwLWp1cHl0ZXItaWNvbi1jb2xvciIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-jupyter: url(data:image/svg+xml;base64,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);
  --jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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);
  --jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K);
  --jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==);
  --jp-icon-launch: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMzIgMzIiIHdpZHRoPSIzMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yNiwyOEg2YTIuMDAyNywyLjAwMjcsMCwwLDEtMi0yVjZBMi4wMDI3LDIuMDAyNywwLDAsMSw2LDRIMTZWNkg2VjI2SDI2VjE2aDJWMjZBMi4wMDI3LDIuMDAyNywwLDAsMSwyNiwyOFoiLz4KICAgIDxwb2x5Z29uIHBvaW50cz0iMjAgMiAyMCA0IDI2LjU4NiA0IDE4IDEyLjU4NiAxOS40MTQgMTQgMjggNS40MTQgMjggMTIgMzAgMTIgMzAgMiAyMCAyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==);
  --jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4K);
  --jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-move-down: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMTIuNDcxIDcuNTI4OTlDMTIuNzYzMiA3LjIzNjg0IDEyLjc2MzIgNi43NjMxNiAxMi40NzEgNi40NzEwMVY2LjQ3MTAxQzEyLjE3OSA2LjE3OTA1IDExLjcwNTcgNi4xNzg4NCAxMS40MTM1IDYuNDcwNTRMNy43NSAxMC4xMjc1VjEuNzVDNy43NSAxLjMzNTc5IDcuNDE0MjEgMSA3IDFWMUM2LjU4NTc5IDEgNi4yNSAxLjMzNTc5IDYuMjUgMS43NVYxMC4xMjc1TDIuNTk3MjYgNi40NjgyMkMyLjMwMzM4IDYuMTczODEgMS44MjY0MSA2LjE3MzU5IDEuNTMyMjYgNi40Njc3NFY2LjQ2Nzc0QzEuMjM4MyA2Ljc2MTcgMS4yMzgzIDcuMjM4MyAxLjUzMjI2IDcuNTMyMjZMNi4yOTI4OSAxMi4yOTI5QzYuNjgzNDIgMTIuNjgzNCA3LjMxNjU4IDEyLjY4MzQgNy43MDcxMSAxMi4yOTI5TDEyLjQ3MSA3LjUyODk5WiIgZmlsbD0iIzYxNjE2MSIvPgo8L3N2Zz4K);
  --jp-icon-move-up: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMS41Mjg5OSA2LjQ3MTAxQzEuMjM2ODQgNi43NjMxNiAxLjIzNjg0IDcuMjM2ODQgMS41Mjg5OSA3LjUyODk5VjcuNTI4OTlDMS44MjA5NSA3LjgyMDk1IDIuMjk0MjYgNy44MjExNiAyLjU4NjQ5IDcuNTI5NDZMNi4yNSAzLjg3MjVWMTIuMjVDNi4yNSAxMi42NjQyIDYuNTg1NzkgMTMgNyAxM1YxM0M3LjQxNDIxIDEzIDcuNzUgMTIuNjY0MiA3Ljc1IDEyLjI1VjMuODcyNUwxMS40MDI3IDcuNTMxNzhDMTEuNjk2NiA3LjgyNjE5IDEyLjE3MzYgNy44MjY0MSAxMi40Njc3IDcuNTMyMjZWNy41MzIyNkMxMi43NjE3IDcuMjM4MyAxMi43NjE3IDYuNzYxNyAxMi40Njc3IDYuNDY3NzRMNy43MDcxMSAxLjcwNzExQzcuMzE2NTggMS4zMTY1OCA2LjY4MzQyIDEuMzE2NTggNi4yOTI4OSAxLjcwNzExTDEuNTI4OTkgNi40NzEwMVoiIGZpbGw9IiM2MTYxNjEiLz4KPC9zdmc+Cg==);
  --jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=);
  --jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtbm90ZWJvb2staWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-numbering: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTQgMTlINlYxOS41SDVWMjAuNUg2VjIxSDRWMjJIN1YxOEg0VjE5Wk01IDEwSDZWNkg0VjdINVYxMFpNNCAxM0g1LjhMNCAxNS4xVjE2SDdWMTVINS4yTDcgMTIuOVYxMkg0VjEzWk05IDdWOUgyM1Y3SDlaTTkgMjFIMjNWMTlIOVYyMVpNOSAxNUgyM1YxM0g5VjE1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-offline-bolt: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDIuMDJjLTUuNTEgMC05Ljk4IDQuNDctOS45OCA5Ljk4czQuNDcgOS45OCA5Ljk4IDkuOTggOS45OC00LjQ3IDkuOTgtOS45OFMxNy41MSAyLjAyIDEyIDIuMDJ6TTExLjQ4IDIwdi02LjI2SDhMMTMgNHY2LjI2aDMuMzVMMTEuNDggMjB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-pdf: url(data:image/svg+xml;base64,PHN2ZwogICB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyMiAyMiIgd2lkdGg9IjE2Ij4KICAgIDxwYXRoIHRyYW5zZm9ybT0icm90YXRlKDQ1KSIgY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0ZGMkEyQSIKICAgICAgIGQ9Im0gMjIuMzQ0MzY5LC0zLjAxNjM2NDIgaCA1LjYzODYwNCB2IDEuNTc5MjQzMyBoIC0zLjU0OTIyNyB2IDEuNTA4NjkyOTkgaCAzLjMzNzU3NiBWIDEuNjUwODE1NCBoIC0zLjMzNzU3NiB2IDMuNDM1MjYxMyBoIC0yLjA4OTM3NyB6IG0gLTcuMTM2NDQ0LDEuNTc5MjQzMyB2IDQuOTQzOTU0MyBoIDAuNzQ4OTIgcSAxLjI4MDc2MSwwIDEuOTUzNzAzLC0wLjYzNDk1MzUgMC42NzgzNjksLTAuNjM0OTUzNSAwLjY3ODM2OSwtMS44NDUxNjQxIDAsLTEuMjA0NzgzNTUgLTAuNjcyOTQyLC0xLjgzNDMxMDExIC0wLjY3Mjk0MiwtMC42Mjk1MjY1OSAtMS45NTkxMywtMC42Mjk1MjY1OSB6IG0gLTIuMDg5Mzc3LC0xLjU3OTI0MzMgaCAyLjIwMzM0MyBxIDEuODQ1MTY0LDAgMi43NDYwMzksMC4yNjU5MjA3IDAuOTA2MzAxLDAuMjYwNDkzNyAxLjU1MjEwOCwwLjg5MDAyMDMgMC41Njk4MywwLjU0ODEyMjMgMC44NDY2MDUsMS4yNjQ0ODAwNiAwLjI3Njc3NCwwLjcxNjM1NzgxIDAuMjc2Nzc0LDEuNjIyNjU4OTQgMCwwLjkxNzE1NTEgLTAuMjc2Nzc0LDEuNjM4OTM5OSAtMC4yNzY3NzUsMC43MTYzNTc4IC0wLjg0NjYwNSwxLjI2NDQ4IC0wLjY1MTIzNCwwLjYyOTUyNjYgLTEuNTYyOTYyLDAuODk1NDQ3MyAtMC45MTE3MjgsMC4yNjA0OTM3IC0yLjczNTE4NSwwLjI2MDQ5MzcgaCAtMi4yMDMzNDMgeiBtIC04LjE0NTg1NjUsMCBoIDMuNDY3ODIzIHEgMS41NDY2ODE2LDAgMi4zNzE1Nzg1LDAuNjg5MjIzIDAuODMwMzI0LDAuNjgzNzk2MSAwLjgzMDMyNCwxLjk1MzcwMzE0IDAsMS4yNzUzMzM5NyAtMC44MzAzMjQsMS45NjQ1NTcwNiBRIDkuOTg3MTk2MSwyLjI3NDkxNSA4LjQ0MDUxNDUsMi4yNzQ5MTUgSCA3LjA2MjA2ODQgViA1LjA4NjA3NjcgSCA0Ljk3MjY5MTUgWiBtIDIuMDg5Mzc2OSwxLjUxNDExOTkgdiAyLjI2MzAzOTQzIGggMS4xNTU5NDEgcSAwLjYwNzgxODgsMCAwLjkzODg2MjksLTAuMjkzMDU1NDcgMC4zMzEwNDQxLC0wLjI5ODQ4MjQxIDAuMzMxMDQ0MSwtMC44NDExNzc3MiAwLC0wLjU0MjY5NTMxIC0wLjMzMTA0NDEsLTAuODM1NzUwNzQgLTAuMzMxMDQ0MSwtMC4yOTMwNTU1IC0wLjkzODg2MjksLTAuMjkzMDU1NSB6IgovPgo8L3N2Zz4K);
  --jp-icon-python: url(data:image/svg+xml;base64,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);
  --jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==);
  --jp-icon-react: url(data:image/svg+xml;base64,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);
  --jp-icon-redo: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZD0iTTE4LjQgMTAuNkMxNi41NSA4Ljk5IDE0LjE1IDggMTEuNSA4Yy00LjY1IDAtOC41OCAzLjAzLTkuOTYgNy4yMkwzLjkgMTZjMS4wNS0zLjE5IDQuMDUtNS41IDcuNi01LjUgMS45NSAwIDMuNzMuNzIgNS4xMiAxLjg4TDEzIDE2aDlWN2wtMy42IDMuNnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==);
  --jp-icon-share: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTSAxOCAyIEMgMTYuMzU0OTkgMiAxNSAzLjM1NDk5MDQgMTUgNSBDIDE1IDUuMTkwOTUyOSAxNS4wMjE3OTEgNS4zNzcxMjI0IDE1LjA1NjY0MSA1LjU1ODU5MzggTCA3LjkyMTg3NSA5LjcyMDcwMzEgQyA3LjM5ODUzOTkgOS4yNzc4NTM5IDYuNzMyMDc3MSA5IDYgOSBDIDQuMzU0OTkwNCA5IDMgMTAuMzU0OTkgMyAxMiBDIDMgMTMuNjQ1MDEgNC4zNTQ5OTA0IDE1IDYgMTUgQyA2LjczMjA3NzEgMTUgNy4zOTg1Mzk5IDE0LjcyMjE0NiA3LjkyMTg3NSAxNC4yNzkyOTcgTCAxNS4wNTY2NDEgMTguNDM5NDUzIEMgMTUuMDIxNTU1IDE4LjYyMTUxNCAxNSAxOC44MDgzODYgMTUgMTkgQyAxNSAyMC42NDUwMSAxNi4zNTQ5OSAyMiAxOCAyMiBDIDE5LjY0NTAxIDIyIDIxIDIwLjY0NTAxIDIxIDE5IEMgMjEgMTcuMzU0OTkgMTkuNjQ1MDEgMTYgMTggMTYgQyAxNy4yNjc0OCAxNiAxNi42MDE1OTMgMTYuMjc5MzI4IDE2LjA3ODEyNSAxNi43MjI2NTYgTCA4Ljk0MzM1OTQgMTIuNTU4NTk0IEMgOC45NzgyMDk1IDEyLjM3NzEyMiA5IDEyLjE5MDk1MyA5IDEyIEMgOSAxMS44MDkwNDcgOC45NzgyMDk1IDExLjYyMjg3OCA4Ljk0MzM1OTQgMTEuNDQxNDA2IEwgMTYuMDc4MTI1IDcuMjc5Mjk2OSBDIDE2LjYwMTQ2IDcuNzIyMTQ2MSAxNy4yNjc5MjMgOCAxOCA4IEMgMTkuNjQ1MDEgOCAyMSA2LjY0NTAwOTYgMjEgNSBDIDIxIDMuMzU0OTkwNCAxOS42NDUwMSAyIDE4IDIgeiBNIDE4IDQgQyAxOC41NjQxMjkgNCAxOSA0LjQzNTg3MDYgMTkgNSBDIDE5IDUuNTY0MTI5NCAxOC41NjQxMjkgNiAxOCA2IEMgMTcuNDM1ODcxIDYgMTcgNS41NjQxMjk0IDE3IDUgQyAxNyA0LjQzNTg3MDYgMTcuNDM1ODcxIDQgMTggNCB6IE0gNiAxMSBDIDYuNTY0MTI5NCAxMSA3IDExLjQzNTg3MSA3IDEyIEMgNyAxMi41NjQxMjkgNi41NjQxMjk0IDEzIDYgMTMgQyA1LjQzNTg3MDYgMTMgNSAxMi41NjQxMjkgNSAxMiBDIDUgMTEuNDM1ODcxIDUuNDM1ODcwNiAxMSA2IDExIHogTSAxOCAxOCBDIDE4LjU2NDEyOSAxOCAxOSAxOC40MzU4NzEgMTkgMTkgQyAxOSAxOS41NjQxMjkgMTguNTY0MTI5IDIwIDE4IDIwIEMgMTcuNDM1ODcxIDIwIDE3IDE5LjU2NDEyOSAxNyAxOSBDIDE3IDE4LjQzNTg3MSAxNy40MzU4NzEgMTggMTggMTggeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-table-rows: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMSw4SDNWNGgxOFY4eiBNMjEsMTBIM3Y0aDE4VjEweiBNMjEsMTZIM3Y0aDE4VjE2eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-tag: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjgiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCA0MyAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTI4LjgzMzIgMTIuMzM0TDMyLjk5OTggMTYuNTAwN0wzNy4xNjY1IDEyLjMzNEgyOC44MzMyWiIvPgoJCTxwYXRoIGQ9Ik0xNi4yMDk1IDIxLjYxMDRDMTUuNjg3MyAyMi4xMjk5IDE0Ljg0NDMgMjIuMTI5OSAxNC4zMjQ4IDIxLjYxMDRMNi45ODI5IDE0LjcyNDVDNi41NzI0IDE0LjMzOTQgNi4wODMxMyAxMy42MDk4IDYuMDQ3ODYgMTMuMDQ4MkM1Ljk1MzQ3IDExLjUyODggNi4wMjAwMiA4LjYxOTQ0IDYuMDY2MjEgNy4wNzY5NUM2LjA4MjgxIDYuNTE0NzcgNi41NTU0OCA2LjA0MzQ3IDcuMTE4MDQgNi4wMzA1NUM5LjA4ODYzIDUuOTg0NzMgMTMuMjYzOCA1LjkzNTc5IDEzLjY1MTggNi4zMjQyNUwyMS43MzY5IDEzLjYzOUMyMi4yNTYgMTQuMTU4NSAyMS43ODUxIDE1LjQ3MjQgMjEuMjYyIDE1Ljk5NDZMMTYuMjA5NSAyMS42MTA0Wk05Ljc3NTg1IDguMjY1QzkuMzM1NTEgNy44MjU2NiA4LjYyMzUxIDcuODI1NjYgOC4xODI4IDguMjY1QzcuNzQzNDYgOC43MDU3MSA3Ljc0MzQ2IDkuNDE3MzMgOC4xODI4IDkuODU2NjdDOC42MjM4MiAxMC4yOTY0IDkuMzM1ODIgMTAuMjk2NCA5Ljc3NTg1IDkuODU2NjdDMTAuMjE1NiA5LjQxNzMzIDEwLjIxNTYgOC43MDUzMyA5Ljc3NTg1IDguMjY1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-terminal: url(data:image/svg+xml;base64,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);
  --jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtdGV4dC1lZGl0b3ItaWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xNSAxNUgzdjJoMTJ2LTJ6bTAtOEgzdjJoMTJWN3pNMyAxM2gxOHYtMkgzdjJ6bTAgOGgxOHYtMkgzdjJ6TTMgM3YyaDE4VjNIM3oiLz4KPC9zdmc+Cg==);
  --jp-icon-toc: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik03LDVIMjFWN0g3VjVNNywxM1YxMUgyMVYxM0g3TTQsNC41QTEuNSwxLjUgMCAwLDEgNS41LDZBMS41LDEuNSAwIDAsMSA0LDcuNUExLjUsMS41IDAgMCwxIDIuNSw2QTEuNSwxLjUgMCAwLDEgNCw0LjVNNCwxMC41QTEuNSwxLjUgMCAwLDEgNS41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMy41QTEuNSwxLjUgMCAwLDEgMi41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMC41TTcsMTlWMTdIMjFWMTlIN000LDE2LjVBMS41LDEuNSAwIDAsMSA1LjUsMThBMS41LDEuNSAwIDAsMSA0LDE5LjVBMS41LDEuNSAwIDAsMSAyLjUsMThBMS41LDEuNSAwIDAsMSA0LDE2LjVaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tree-view: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMiAxMVYzaC03djNIOVYzSDJ2OGg3VjhoMnYxMGg0djNoN3YtOGgtN3YzaC0yVjhoMnYzeiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K);
  --jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-user: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE2IDdhNCA0IDAgMTEtOCAwIDQgNCAwIDAxOCAwek0xMiAxNGE3IDcgMCAwMC03IDdoMTRhNyA3IDAgMDAtNy03eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-users: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZlcnNpb249IjEuMSIgdmlld0JveD0iMCAwIDM2IDI0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogPGcgY2xhc3M9ImpwLWljb24zIiB0cmFuc2Zvcm09Im1hdHJpeCgxLjczMjcgMCAwIDEuNzMyNyAtMy42MjgyIC4wOTk1NzcpIiBmaWxsPSIjNjE2MTYxIj4KICA8cGF0aCB0cmFuc2Zvcm09Im1hdHJpeCgxLjUsMCwwLDEuNSwwLC02KSIgZD0ibTEyLjE4NiA3LjUwOThjLTEuMDUzNSAwLTEuOTc1NyAwLjU2NjUtMi40Nzg1IDEuNDEwMiAwLjc1MDYxIDAuMzEyNzcgMS4zOTc0IDAuODI2NDggMS44NzMgMS40NzI3aDMuNDg2M2MwLTEuNTkyLTEuMjg4OS0yLjg4MjgtMi44ODA5LTIuODgyOHoiLz4KICA8cGF0aCBkPSJtMjAuNDY1IDIuMzg5NWEyLjE4ODUgMi4xODg1IDAgMCAxLTIuMTg4NCAyLjE4ODUgMi4xODg1IDIuMTg4NSAwIDAgMS0yLjE4ODUtMi4xODg1IDIuMTg4NSAyLjE4ODUgMCAwIDEgMi4xODg1LTIuMTg4NSAyLjE4ODUgMi4xODg1IDAgMCAxIDIuMTg4NCAyLjE4ODV6Ii8+CiAgPHBhdGggdHJhbnNmb3JtPSJtYXRyaXgoMS41LDAsMCwxLjUsMCwtNikiIGQ9Im0zLjU4OTggOC40MjE5Yy0xLjExMjYgMC0yLjAxMzcgMC45MDExMS0yLjAxMzcgMi4wMTM3aDIuODE0NWMwLjI2Nzk3LTAuMzczMDkgMC41OTA3LTAuNzA0MzUgMC45NTg5OC0wLjk3ODUyLTAuMzQ0MzMtMC42MTY4OC0xLjAwMzEtMS4wMzUyLTEuNzU5OC0xLjAzNTJ6Ii8+CiAgPHBhdGggZD0ibTYuOTE1NCA0LjYyM2ExLjUyOTQgMS41Mjk0IDAgMCAxLTEuNTI5NCAxLjUyOTQgMS41Mjk0IDEuNTI5NCAwIDAgMS0xLjUyOTQtMS41Mjk0IDEuNTI5NCAxLjUyOTQgMCAwIDEgMS41Mjk0LTEuNTI5NCAxLjUyOTQgMS41Mjk0IDAgMCAxIDEuNTI5NCAxLjUyOTR6Ii8+CiAgPHBhdGggZD0ibTYuMTM1IDEzLjUzNWMwLTMuMjM5MiAyLjYyNTktNS44NjUgNS44NjUtNS44NjUgMy4yMzkyIDAgNS44NjUgMi42MjU5IDUuODY1IDUuODY1eiIvPgogIDxjaXJjbGUgY3g9IjEyIiBjeT0iMy43Njg1IiByPSIyLjk2ODUiLz4KIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-word: url(data:image/svg+xml;base64,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);
  --jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K);
}

/* Icon CSS class declarations */

.jp-AddAboveIcon {
  background-image: var(--jp-icon-add-above);
}

.jp-AddBelowIcon {
  background-image: var(--jp-icon-add-below);
}

.jp-AddIcon {
  background-image: var(--jp-icon-add);
}

.jp-BellIcon {
  background-image: var(--jp-icon-bell);
}

.jp-BugDotIcon {
  background-image: var(--jp-icon-bug-dot);
}

.jp-BugIcon {
  background-image: var(--jp-icon-bug);
}

.jp-BuildIcon {
  background-image: var(--jp-icon-build);
}

.jp-CaretDownEmptyIcon {
  background-image: var(--jp-icon-caret-down-empty);
}

.jp-CaretDownEmptyThinIcon {
  background-image: var(--jp-icon-caret-down-empty-thin);
}

.jp-CaretDownIcon {
  background-image: var(--jp-icon-caret-down);
}

.jp-CaretLeftIcon {
  background-image: var(--jp-icon-caret-left);
}

.jp-CaretRightIcon {
  background-image: var(--jp-icon-caret-right);
}

.jp-CaretUpEmptyThinIcon {
  background-image: var(--jp-icon-caret-up-empty-thin);
}

.jp-CaretUpIcon {
  background-image: var(--jp-icon-caret-up);
}

.jp-CaseSensitiveIcon {
  background-image: var(--jp-icon-case-sensitive);
}

.jp-CheckIcon {
  background-image: var(--jp-icon-check);
}

.jp-CircleEmptyIcon {
  background-image: var(--jp-icon-circle-empty);
}

.jp-CircleIcon {
  background-image: var(--jp-icon-circle);
}

.jp-ClearIcon {
  background-image: var(--jp-icon-clear);
}

.jp-CloseIcon {
  background-image: var(--jp-icon-close);
}

.jp-CodeCheckIcon {
  background-image: var(--jp-icon-code-check);
}

.jp-CodeIcon {
  background-image: var(--jp-icon-code);
}

.jp-CollapseAllIcon {
  background-image: var(--jp-icon-collapse-all);
}

.jp-ConsoleIcon {
  background-image: var(--jp-icon-console);
}

.jp-CopyIcon {
  background-image: var(--jp-icon-copy);
}

.jp-CopyrightIcon {
  background-image: var(--jp-icon-copyright);
}

.jp-CutIcon {
  background-image: var(--jp-icon-cut);
}

.jp-DeleteIcon {
  background-image: var(--jp-icon-delete);
}

.jp-DownloadIcon {
  background-image: var(--jp-icon-download);
}

.jp-DuplicateIcon {
  background-image: var(--jp-icon-duplicate);
}

.jp-EditIcon {
  background-image: var(--jp-icon-edit);
}

.jp-EllipsesIcon {
  background-image: var(--jp-icon-ellipses);
}

.jp-ErrorIcon {
  background-image: var(--jp-icon-error);
}

.jp-ExpandAllIcon {
  background-image: var(--jp-icon-expand-all);
}

.jp-ExtensionIcon {
  background-image: var(--jp-icon-extension);
}

.jp-FastForwardIcon {
  background-image: var(--jp-icon-fast-forward);
}

.jp-FileIcon {
  background-image: var(--jp-icon-file);
}

.jp-FileUploadIcon {
  background-image: var(--jp-icon-file-upload);
}

.jp-FilterDotIcon {
  background-image: var(--jp-icon-filter-dot);
}

.jp-FilterIcon {
  background-image: var(--jp-icon-filter);
}

.jp-FilterListIcon {
  background-image: var(--jp-icon-filter-list);
}

.jp-FolderFavoriteIcon {
  background-image: var(--jp-icon-folder-favorite);
}

.jp-FolderIcon {
  background-image: var(--jp-icon-folder);
}

.jp-HomeIcon {
  background-image: var(--jp-icon-home);
}

.jp-Html5Icon {
  background-image: var(--jp-icon-html5);
}

.jp-ImageIcon {
  background-image: var(--jp-icon-image);
}

.jp-InfoIcon {
  background-image: var(--jp-icon-info);
}

.jp-InspectorIcon {
  background-image: var(--jp-icon-inspector);
}

.jp-JsonIcon {
  background-image: var(--jp-icon-json);
}

.jp-JuliaIcon {
  background-image: var(--jp-icon-julia);
}

.jp-JupyterFaviconIcon {
  background-image: var(--jp-icon-jupyter-favicon);
}

.jp-JupyterIcon {
  background-image: var(--jp-icon-jupyter);
}

.jp-JupyterlabWordmarkIcon {
  background-image: var(--jp-icon-jupyterlab-wordmark);
}

.jp-KernelIcon {
  background-image: var(--jp-icon-kernel);
}

.jp-KeyboardIcon {
  background-image: var(--jp-icon-keyboard);
}

.jp-LaunchIcon {
  background-image: var(--jp-icon-launch);
}

.jp-LauncherIcon {
  background-image: var(--jp-icon-launcher);
}

.jp-LineFormIcon {
  background-image: var(--jp-icon-line-form);
}

.jp-LinkIcon {
  background-image: var(--jp-icon-link);
}

.jp-ListIcon {
  background-image: var(--jp-icon-list);
}

.jp-MarkdownIcon {
  background-image: var(--jp-icon-markdown);
}

.jp-MoveDownIcon {
  background-image: var(--jp-icon-move-down);
}

.jp-MoveUpIcon {
  background-image: var(--jp-icon-move-up);
}

.jp-NewFolderIcon {
  background-image: var(--jp-icon-new-folder);
}

.jp-NotTrustedIcon {
  background-image: var(--jp-icon-not-trusted);
}

.jp-NotebookIcon {
  background-image: var(--jp-icon-notebook);
}

.jp-NumberingIcon {
  background-image: var(--jp-icon-numbering);
}

.jp-OfflineBoltIcon {
  background-image: var(--jp-icon-offline-bolt);
}

.jp-PaletteIcon {
  background-image: var(--jp-icon-palette);
}

.jp-PasteIcon {
  background-image: var(--jp-icon-paste);
}

.jp-PdfIcon {
  background-image: var(--jp-icon-pdf);
}

.jp-PythonIcon {
  background-image: var(--jp-icon-python);
}

.jp-RKernelIcon {
  background-image: var(--jp-icon-r-kernel);
}

.jp-ReactIcon {
  background-image: var(--jp-icon-react);
}

.jp-RedoIcon {
  background-image: var(--jp-icon-redo);
}

.jp-RefreshIcon {
  background-image: var(--jp-icon-refresh);
}

.jp-RegexIcon {
  background-image: var(--jp-icon-regex);
}

.jp-RunIcon {
  background-image: var(--jp-icon-run);
}

.jp-RunningIcon {
  background-image: var(--jp-icon-running);
}

.jp-SaveIcon {
  background-image: var(--jp-icon-save);
}

.jp-SearchIcon {
  background-image: var(--jp-icon-search);
}

.jp-SettingsIcon {
  background-image: var(--jp-icon-settings);
}

.jp-ShareIcon {
  background-image: var(--jp-icon-share);
}

.jp-SpreadsheetIcon {
  background-image: var(--jp-icon-spreadsheet);
}

.jp-StopIcon {
  background-image: var(--jp-icon-stop);
}

.jp-TabIcon {
  background-image: var(--jp-icon-tab);
}

.jp-TableRowsIcon {
  background-image: var(--jp-icon-table-rows);
}

.jp-TagIcon {
  background-image: var(--jp-icon-tag);
}

.jp-TerminalIcon {
  background-image: var(--jp-icon-terminal);
}

.jp-TextEditorIcon {
  background-image: var(--jp-icon-text-editor);
}

.jp-TocIcon {
  background-image: var(--jp-icon-toc);
}

.jp-TreeViewIcon {
  background-image: var(--jp-icon-tree-view);
}

.jp-TrustedIcon {
  background-image: var(--jp-icon-trusted);
}

.jp-UndoIcon {
  background-image: var(--jp-icon-undo);
}

.jp-UserIcon {
  background-image: var(--jp-icon-user);
}

.jp-UsersIcon {
  background-image: var(--jp-icon-users);
}

.jp-VegaIcon {
  background-image: var(--jp-icon-vega);
}

.jp-WordIcon {
  background-image: var(--jp-icon-word);
}

.jp-YamlIcon {
  background-image: var(--jp-icon-yaml);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

.jp-Icon,
.jp-MaterialIcon {
  background-position: center;
  background-repeat: no-repeat;
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-cover {
  background-position: center;
  background-repeat: no-repeat;
  background-size: cover;
}

/**
 * (DEPRECATED) Support for specific CSS icon sizes
 */

.jp-Icon-16 {
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-18 {
  background-size: 18px;
  min-width: 18px;
  min-height: 18px;
}

.jp-Icon-20 {
  background-size: 20px;
  min-width: 20px;
  min-height: 20px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.lm-TabBar .lm-TabBar-addButton {
  align-items: center;
  display: flex;
  padding: 4px;
  padding-bottom: 5px;
  margin-right: 1px;
  background-color: var(--jp-layout-color2);
}

.lm-TabBar .lm-TabBar-addButton:hover {
  background-color: var(--jp-layout-color1);
}

.lm-DockPanel-tabBar .lm-TabBar-tab {
  width: var(--jp-private-horizontal-tab-width);
}

.lm-DockPanel-tabBar .lm-TabBar-content {
  flex: unset;
}

.lm-DockPanel-tabBar[data-orientation='horizontal'] {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for icons as inline SVG HTMLElements
 */

/* recolor the primary elements of an icon */
.jp-icon0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-accent0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-accent1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-accent2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-accent3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-accent4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-accent0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-accent1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-accent2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-accent3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-accent4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-none[fill] {
  fill: none;
}

.jp-icon-none[stroke] {
  stroke: none;
}

/* brand icon colors. Same for light and dark */
.jp-icon-brand0[fill] {
  fill: var(--jp-brand-color0);
}

.jp-icon-brand1[fill] {
  fill: var(--jp-brand-color1);
}

.jp-icon-brand2[fill] {
  fill: var(--jp-brand-color2);
}

.jp-icon-brand3[fill] {
  fill: var(--jp-brand-color3);
}

.jp-icon-brand4[fill] {
  fill: var(--jp-brand-color4);
}

.jp-icon-brand0[stroke] {
  stroke: var(--jp-brand-color0);
}

.jp-icon-brand1[stroke] {
  stroke: var(--jp-brand-color1);
}

.jp-icon-brand2[stroke] {
  stroke: var(--jp-brand-color2);
}

.jp-icon-brand3[stroke] {
  stroke: var(--jp-brand-color3);
}

.jp-icon-brand4[stroke] {
  stroke: var(--jp-brand-color4);
}

/* warn icon colors. Same for light and dark */
.jp-icon-warn0[fill] {
  fill: var(--jp-warn-color0);
}

.jp-icon-warn1[fill] {
  fill: var(--jp-warn-color1);
}

.jp-icon-warn2[fill] {
  fill: var(--jp-warn-color2);
}

.jp-icon-warn3[fill] {
  fill: var(--jp-warn-color3);
}

.jp-icon-warn0[stroke] {
  stroke: var(--jp-warn-color0);
}

.jp-icon-warn1[stroke] {
  stroke: var(--jp-warn-color1);
}

.jp-icon-warn2[stroke] {
  stroke: var(--jp-warn-color2);
}

.jp-icon-warn3[stroke] {
  stroke: var(--jp-warn-color3);
}

/* icon colors that contrast well with each other and most backgrounds */
.jp-icon-contrast0[fill] {
  fill: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[fill] {
  fill: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[fill] {
  fill: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[fill] {
  fill: var(--jp-icon-contrast-color3);
}

.jp-icon-contrast0[stroke] {
  stroke: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[stroke] {
  stroke: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[stroke] {
  stroke: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[stroke] {
  stroke: var(--jp-icon-contrast-color3);
}

.jp-icon-dot[fill] {
  fill: var(--jp-warn-color0);
}

.jp-jupyter-icon-color[fill] {
  fill: var(--jp-jupyter-icon-color, var(--jp-warn-color0));
}

.jp-notebook-icon-color[fill] {
  fill: var(--jp-notebook-icon-color, var(--jp-warn-color0));
}

.jp-json-icon-color[fill] {
  fill: var(--jp-json-icon-color, var(--jp-warn-color1));
}

.jp-console-icon-color[fill] {
  fill: var(--jp-console-icon-color, white);
}

.jp-console-icon-background-color[fill] {
  fill: var(--jp-console-icon-background-color, var(--jp-brand-color1));
}

.jp-terminal-icon-color[fill] {
  fill: var(--jp-terminal-icon-color, var(--jp-layout-color2));
}

.jp-terminal-icon-background-color[fill] {
  fill: var(
    --jp-terminal-icon-background-color,
    var(--jp-inverse-layout-color2)
  );
}

.jp-text-editor-icon-color[fill] {
  fill: var(--jp-text-editor-icon-color, var(--jp-inverse-layout-color3));
}

.jp-inspector-icon-color[fill] {
  fill: var(--jp-inspector-icon-color, var(--jp-inverse-layout-color3));
}

/* CSS for icons in selected filebrowser listing items */
.jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

.jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* stylelint-disable selector-max-class, selector-max-compound-selectors */

/**
* TODO: come up with non css-hack solution for showing the busy icon on top
*  of the close icon
* CSS for complex behavior of close icon of tabs in the main area tabbar
*/
.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon3[fill] {
  fill: none;
}

.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: var(--jp-inverse-layout-color3);
}

/* stylelint-enable selector-max-class, selector-max-compound-selectors */

/* CSS for icons in status bar */
#jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

#jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* special handling for splash icon CSS. While the theme CSS reloads during
   splash, the splash icon can loose theming. To prevent that, we set a
   default for its color variable */
:root {
  --jp-warn-color0: var(--md-orange-700);
}

/* not sure what to do with this one, used in filebrowser listing */
.jp-DragIcon {
  margin-right: 4px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for alt colors for icons as inline SVG HTMLElements
 */

/* alt recolor the primary elements of an icon */
.jp-icon-alt .jp-icon0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-alt .jp-icon0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* alt recolor the accent elements of an icon */
.jp-icon-alt .jp-icon-accent0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-alt .jp-icon-accent0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-icon-hoverShow:not(:hover) .jp-icon-hoverShow-content {
  display: none !important;
}

/**
 * Support for hover colors for icons as inline SVG HTMLElements
 */

/**
 * regular colors
 */

/* recolor the primary elements of an icon */
.jp-icon-hover :hover .jp-icon0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-hover :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-hover :hover .jp-icon-none-hover[fill] {
  fill: none;
}

.jp-icon-hover :hover .jp-icon-none-hover[stroke] {
  stroke: none;
}

/**
 * inverse colors
 */

/* inverse recolor the primary elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* inverse recolor the accent elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-IFrame {
  width: 100%;
  height: 100%;
}

.jp-IFrame > iframe {
  border: none;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-IFrame {
  position: relative;
}

body.lm-mod-override-cursor .jp-IFrame::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-HoverBox {
  position: fixed;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FormGroup-content fieldset {
  border: none;
  padding: 0;
  min-width: 0;
  width: 100%;
}

/* stylelint-disable selector-max-type */

.jp-FormGroup-content fieldset .jp-inputFieldWrapper input,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper select,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper textarea {
  font-size: var(--jp-content-font-size2);
  border-color: var(--jp-input-border-color);
  border-style: solid;
  border-radius: var(--jp-border-radius);
  border-width: 1px;
  padding: 6px 8px;
  background: none;
  color: var(--jp-ui-font-color0);
  height: inherit;
}

.jp-FormGroup-content fieldset input[type='checkbox'] {
  position: relative;
  top: 2px;
  margin-left: 0;
}

.jp-FormGroup-content button.jp-mod-styled {
  cursor: pointer;
}

.jp-FormGroup-content .checkbox label {
  cursor: pointer;
  font-size: var(--jp-content-font-size1);
}

.jp-FormGroup-content .jp-root > fieldset > legend {
  display: none;
}

.jp-FormGroup-content .jp-root > fieldset > p {
  display: none;
}

/** copy of `input.jp-mod-styled:focus` style */
.jp-FormGroup-content fieldset input:focus,
.jp-FormGroup-content fieldset select:focus {
  -moz-outline-radius: unset;
  outline: var(--jp-border-width) solid var(--md-blue-500);
  outline-offset: -1px;
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-FormGroup-content fieldset input:hover:not(:focus),
.jp-FormGroup-content fieldset select:hover:not(:focus) {
  background-color: var(--jp-border-color2);
}

/* stylelint-enable selector-max-type */

.jp-FormGroup-content .checkbox .field-description {
  /* Disable default description field for checkbox:
   because other widgets do not have description fields,
   we add descriptions to each widget on the field level.
  */
  display: none;
}

.jp-FormGroup-content #root__description {
  display: none;
}

.jp-FormGroup-content .jp-modifiedIndicator {
  width: 5px;
  background-color: var(--jp-brand-color2);
  margin-top: 0;
  margin-left: calc(var(--jp-private-settingeditor-modifier-indent) * -1);
  flex-shrink: 0;
}

.jp-FormGroup-content .jp-modifiedIndicator.jp-errorIndicator {
  background-color: var(--jp-error-color0);
  margin-right: 0.5em;
}

/* RJSF ARRAY style */

.jp-arrayFieldWrapper legend {
  font-size: var(--jp-content-font-size2);
  color: var(--jp-ui-font-color0);
  flex-basis: 100%;
  padding: 4px 0;
  font-weight: var(--jp-content-heading-font-weight);
  border-bottom: 1px solid var(--jp-border-color2);
}

.jp-arrayFieldWrapper .field-description {
  padding: 4px 0;
  white-space: pre-wrap;
}

.jp-arrayFieldWrapper .array-item {
  width: 100%;
  border: 1px solid var(--jp-border-color2);
  border-radius: 4px;
  margin: 4px;
}

.jp-ArrayOperations {
  display: flex;
  margin-left: 8px;
}

.jp-ArrayOperationsButton {
  margin: 2px;
}

.jp-ArrayOperationsButton .jp-icon3[fill] {
  fill: var(--jp-ui-font-color0);
}

button.jp-ArrayOperationsButton.jp-mod-styled:disabled {
  cursor: not-allowed;
  opacity: 0.5;
}

/* RJSF form validation error */

.jp-FormGroup-content .validationErrors {
  color: var(--jp-error-color0);
}

/* Hide panel level error as duplicated the field level error */
.jp-FormGroup-content .panel.errors {
  display: none;
}

/* RJSF normal content (settings-editor) */

.jp-FormGroup-contentNormal {
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-FormGroup-contentItem {
  margin-left: 7px;
  color: var(--jp-ui-font-color0);
}

.jp-FormGroup-contentNormal .jp-FormGroup-description {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-default {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-fieldLabel {
  font-size: var(--jp-content-font-size1);
  font-weight: normal;
  min-width: 120px;
}

.jp-FormGroup-contentNormal fieldset:not(:first-child) {
  margin-left: 7px;
}

.jp-FormGroup-contentNormal .field-array-of-string .array-item {
  /* Display `jp-ArrayOperations` buttons side-by-side with content except
    for small screens where flex-wrap will place them one below the other.
  */
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-objectFieldWrapper .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

/* RJSF compact content (metadata-form) */

.jp-FormGroup-content.jp-FormGroup-contentCompact {
  width: 100%;
}

.jp-FormGroup-contentCompact .form-group {
  display: flex;
  padding: 0.5em 0.2em 0.5em 0;
}

.jp-FormGroup-contentCompact
  .jp-FormGroup-compactTitle
  .jp-FormGroup-description {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color2);
}

.jp-FormGroup-contentCompact .jp-FormGroup-fieldLabel {
  padding-bottom: 0.3em;
}

.jp-FormGroup-contentCompact .jp-inputFieldWrapper .form-control {
  width: 100%;
  box-sizing: border-box;
}

.jp-FormGroup-contentCompact .jp-arrayFieldWrapper .jp-FormGroup-compactTitle {
  padding-bottom: 7px;
}

.jp-FormGroup-contentCompact
  .jp-objectFieldWrapper
  .jp-objectFieldWrapper
  .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

.jp-FormGroup-contentCompact ul.error-detail {
  margin-block-start: 0.5em;
  margin-block-end: 0.5em;
  padding-inline-start: 1em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-SidePanel {
  display: flex;
  flex-direction: column;
  min-width: var(--jp-sidebar-min-width);
  overflow-y: auto;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  font-size: var(--jp-ui-font-size1);
}

.jp-SidePanel-header {
  flex: 0 0 auto;
  display: flex;
  border-bottom: var(--jp-border-width) solid var(--jp-border-color2);
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin: 0;
  padding: 2px;
  text-transform: uppercase;
}

.jp-SidePanel-toolbar {
  flex: 0 0 auto;
}

.jp-SidePanel-content {
  flex: 1 1 auto;
}

.jp-SidePanel-toolbar,
.jp-AccordionPanel-toolbar {
  height: var(--jp-private-toolbar-height);
}

.jp-SidePanel-toolbar.jp-Toolbar-micro {
  display: none;
}

.lm-AccordionPanel .jp-AccordionPanel-title {
  box-sizing: border-box;
  line-height: 25px;
  margin: 0;
  display: flex;
  align-items: center;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  font-size: var(--jp-ui-font-size0);
}

.jp-AccordionPanel-title {
  cursor: pointer;
  user-select: none;
  -moz-user-select: none;
  -webkit-user-select: none;
  text-transform: uppercase;
}

.lm-AccordionPanel[data-orientation='horizontal'] > .jp-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleLabel {
  user-select: none;
  text-overflow: ellipsis;
  white-space: nowrap;
  overflow: hidden;
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleCollapser {
  transform: rotate(-90deg);
  margin: auto 0;
  height: 16px;
}

.jp-AccordionPanel-title.lm-mod-expanded .lm-AccordionPanel-titleCollapser {
  transform: rotate(0deg);
}

.lm-AccordionPanel .jp-AccordionPanel-toolbar {
  background: none;
  box-shadow: none;
  border: none;
  margin-left: auto;
}

.lm-AccordionPanel .lm-SplitPanel-handle:hover {
  background: var(--jp-layout-color3);
}

.jp-text-truncated {
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Spinner {
  position: absolute;
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 10;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-layout-color0);
  outline: none;
}

.jp-SpinnerContent {
  font-size: 10px;
  margin: 50px auto;
  text-indent: -9999em;
  width: 3em;
  height: 3em;
  border-radius: 50%;
  background: var(--jp-brand-color3);
  background: linear-gradient(
    to right,
    #f37626 10%,
    rgba(255, 255, 255, 0) 42%
  );
  position: relative;
  animation: load3 1s infinite linear, fadeIn 1s;
}

.jp-SpinnerContent::before {
  width: 50%;
  height: 50%;
  background: #f37626;
  border-radius: 100% 0 0;
  position: absolute;
  top: 0;
  left: 0;
  content: '';
}

.jp-SpinnerContent::after {
  background: var(--jp-layout-color0);
  width: 75%;
  height: 75%;
  border-radius: 50%;
  content: '';
  margin: auto;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
}

@keyframes fadeIn {
  0% {
    opacity: 0;
  }

  100% {
    opacity: 1;
  }
}

@keyframes load3 {
  0% {
    transform: rotate(0deg);
  }

  100% {
    transform: rotate(360deg);
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

button.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: none;
  box-sizing: border-box;
  text-align: center;
  line-height: 32px;
  height: 32px;
  padding: 0 12px;
  letter-spacing: 0.8px;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input.jp-mod-styled {
  background: var(--jp-input-background);
  height: 28px;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color1);
  padding-left: 7px;
  padding-right: 7px;
  font-size: var(--jp-ui-font-size2);
  color: var(--jp-ui-font-color0);
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input[type='checkbox'].jp-mod-styled {
  appearance: checkbox;
  -webkit-appearance: checkbox;
  -moz-appearance: checkbox;
  height: auto;
}

input.jp-mod-styled:focus {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-select-wrapper {
  display: flex;
  position: relative;
  flex-direction: column;
  padding: 1px;
  background-color: var(--jp-layout-color1);
  box-sizing: border-box;
  margin-bottom: 12px;
}

.jp-select-wrapper:not(.multiple) {
  height: 28px;
}

.jp-select-wrapper.jp-mod-focused select.jp-mod-styled {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-input-active-background);
}

select.jp-mod-styled:hover {
  cursor: pointer;
  color: var(--jp-ui-font-color0);
  background-color: var(--jp-input-hover-background);
  box-shadow: inset 0 0 1px rgba(0, 0, 0, 0.5);
}

select.jp-mod-styled {
  flex: 1 1 auto;
  width: 100%;
  font-size: var(--jp-ui-font-size2);
  background: var(--jp-input-background);
  color: var(--jp-ui-font-color0);
  padding: 0 25px 0 8px;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

select.jp-mod-styled:not([multiple]) {
  height: 32px;
}

select.jp-mod-styled[multiple] {
  max-height: 200px;
  overflow-y: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-switch {
  display: flex;
  align-items: center;
  padding-left: 4px;
  padding-right: 4px;
  font-size: var(--jp-ui-font-size1);
  background-color: transparent;
  color: var(--jp-ui-font-color1);
  border: none;
  height: 20px;
}

.jp-switch:hover {
  background-color: var(--jp-layout-color2);
}

.jp-switch-label {
  margin-right: 5px;
  font-family: var(--jp-ui-font-family);
}

.jp-switch-track {
  cursor: pointer;
  background-color: var(--jp-switch-color, var(--jp-border-color1));
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 34px;
  height: 16px;
  width: 35px;
  position: relative;
}

.jp-switch-track::before {
  content: '';
  position: absolute;
  height: 10px;
  width: 10px;
  margin: 3px;
  left: 0;
  background-color: var(--jp-ui-inverse-font-color1);
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 50%;
}

.jp-switch[aria-checked='true'] .jp-switch-track {
  background-color: var(--jp-switch-true-position-color, var(--jp-warn-color0));
}

.jp-switch[aria-checked='true'] .jp-switch-track::before {
  /* track width (35) - margins (3 + 3) - thumb width (10) */
  left: 19px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toolbar-height: calc(
    28px + var(--jp-border-width)
  ); /* leave 28px for content */
}

.jp-Toolbar {
  color: var(--jp-ui-font-color1);
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: 2px;
  z-index: 8;
  overflow-x: hidden;
}

/* Toolbar items */

.jp-Toolbar > .jp-Toolbar-item.jp-Toolbar-spacer {
  flex-grow: 1;
  flex-shrink: 1;
}

.jp-Toolbar-item.jp-Toolbar-kernelStatus {
  display: inline-block;
  width: 32px;
  background-repeat: no-repeat;
  background-position: center;
  background-size: 16px;
}

.jp-Toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  display: flex;
  padding-left: 1px;
  padding-right: 1px;
  font-size: var(--jp-ui-font-size1);
  line-height: var(--jp-private-toolbar-height);
  height: 100%;
}

/* Toolbar buttons */

/* This is the div we use to wrap the react component into a Widget */
div.jp-ToolbarButton {
  color: transparent;
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0;
  margin: 0;
}

button.jp-ToolbarButtonComponent {
  background: var(--jp-layout-color1);
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0 6px;
  margin: 0;
  height: 24px;
  border-radius: var(--jp-border-radius);
  display: flex;
  align-items: center;
  text-align: center;
  font-size: 14px;
  min-width: unset;
  min-height: unset;
}

button.jp-ToolbarButtonComponent:disabled {
  opacity: 0.4;
}

button.jp-ToolbarButtonComponent > span {
  padding: 0;
  flex: 0 0 auto;
}

button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label {
  font-size: var(--jp-ui-font-size1);
  line-height: 100%;
  padding-left: 2px;
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar.jp-Toolbar-micro {
  padding: 0;
  min-height: 0;
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar {
  border: none;
  box-shadow: none;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-WindowedPanel-outer {
  position: relative;
  overflow-y: auto;
}

.jp-WindowedPanel-inner {
  position: relative;
}

.jp-WindowedPanel-window {
  position: absolute;
  left: 0;
  right: 0;
  overflow: visible;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* Sibling imports */

body {
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
}

/* Disable native link decoration styles everywhere outside of dialog boxes */
a {
  text-decoration: unset;
  color: unset;
}

a:hover {
  text-decoration: unset;
  color: unset;
}

/* Accessibility for links inside dialog box text */
.jp-Dialog-content a {
  text-decoration: revert;
  color: var(--jp-content-link-color);
}

.jp-Dialog-content a:hover {
  text-decoration: revert;
}

/* Styles for ui-components */
.jp-Button {
  color: var(--jp-ui-font-color2);
  border-radius: var(--jp-border-radius);
  padding: 0 12px;
  font-size: var(--jp-ui-font-size1);

  /* Copy from blueprint 3 */
  display: inline-flex;
  flex-direction: row;
  border: none;
  cursor: pointer;
  align-items: center;
  justify-content: center;
  text-align: left;
  vertical-align: middle;
  min-height: 30px;
  min-width: 30px;
}

.jp-Button:disabled {
  cursor: not-allowed;
}

.jp-Button:empty {
  padding: 0 !important;
}

.jp-Button.jp-mod-small {
  min-height: 24px;
  min-width: 24px;
  font-size: 12px;
  padding: 0 7px;
}

/* Use our own theme for hover styles */
.jp-Button.jp-mod-minimal:hover {
  background-color: var(--jp-layout-color2);
}

.jp-Button.jp-mod-minimal {
  background: none;
}

.jp-InputGroup {
  display: block;
  position: relative;
}

.jp-InputGroup input {
  box-sizing: border-box;
  border: none;
  border-radius: 0;
  background-color: transparent;
  color: var(--jp-ui-font-color0);
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
  padding-bottom: 0;
  padding-top: 0;
  padding-left: 10px;
  padding-right: 28px;
  position: relative;
  width: 100%;
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  font-size: 14px;
  font-weight: 400;
  height: 30px;
  line-height: 30px;
  outline: none;
  vertical-align: middle;
}

.jp-InputGroup input:focus {
  box-shadow: inset 0 0 0 var(--jp-border-width)
      var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-InputGroup input:disabled {
  cursor: not-allowed;
  resize: block;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input:disabled ~ span {
  cursor: not-allowed;
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input::placeholder,
input::placeholder {
  color: var(--jp-ui-font-color2);
}

.jp-InputGroupAction {
  position: absolute;
  bottom: 1px;
  right: 0;
  padding: 6px;
}

.jp-HTMLSelect.jp-DefaultStyle select {
  background-color: initial;
  border: none;
  border-radius: 0;
  box-shadow: none;
  color: var(--jp-ui-font-color0);
  display: block;
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  height: 24px;
  line-height: 14px;
  padding: 0 25px 0 10px;
  text-align: left;
  -moz-appearance: none;
  -webkit-appearance: none;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
  cursor: not-allowed;
  resize: block;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled ~ span {
  cursor: not-allowed;
}

/* Use our own theme for hover and option styles */
/* stylelint-disable-next-line selector-max-type */
.jp-HTMLSelect.jp-DefaultStyle select:hover,
.jp-HTMLSelect.jp-DefaultStyle select > option {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color0);
}

select {
  box-sizing: border-box;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-StatusBar-Widget {
  display: flex;
  align-items: center;
  background: var(--jp-layout-color2);
  min-height: var(--jp-statusbar-height);
  justify-content: space-between;
  padding: 0 10px;
}

.jp-StatusBar-Left {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-StatusBar-Middle {
  display: flex;
  align-items: center;
}

.jp-StatusBar-Right {
  display: flex;
  align-items: center;
  flex-direction: row-reverse;
}

.jp-StatusBar-Item {
  max-height: var(--jp-statusbar-height);
  margin: 0 2px;
  height: var(--jp-statusbar-height);
  white-space: nowrap;
  text-overflow: ellipsis;
  color: var(--jp-ui-font-color1);
  padding: 0 6px;
}

.jp-mod-highlighted:hover {
  background-color: var(--jp-layout-color3);
}

.jp-mod-clicked {
  background-color: var(--jp-brand-color1);
}

.jp-mod-clicked:hover {
  background-color: var(--jp-brand-color0);
}

.jp-mod-clicked .jp-StatusBar-TextItem {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-StatusBar-HoverItem {
  box-shadow: '0px 4px 4px rgba(0, 0, 0, 0.25)';
}

.jp-StatusBar-TextItem {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  line-height: 24px;
  color: var(--jp-ui-font-color1);
}

.jp-StatusBar-GroupItem {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-Statusbar-ProgressCircle svg {
  display: block;
  margin: 0 auto;
  width: 16px;
  height: 24px;
  align-self: normal;
}

.jp-Statusbar-ProgressCircle path {
  fill: var(--jp-inverse-layout-color3);
}

.jp-Statusbar-ProgressBar-progress-bar {
  height: 10px;
  width: 100px;
  border: solid 0.25px var(--jp-brand-color2);
  border-radius: 3px;
  overflow: hidden;
  align-self: center;
}

.jp-Statusbar-ProgressBar-progress-bar > div {
  background-color: var(--jp-brand-color2);
  background-image: linear-gradient(
    -45deg,
    rgba(255, 255, 255, 0.2) 25%,
    transparent 25%,
    transparent 50%,
    rgba(255, 255, 255, 0.2) 50%,
    rgba(255, 255, 255, 0.2) 75%,
    transparent 75%,
    transparent
  );
  background-size: 40px 40px;
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 14px;
  color: #fff;
  text-align: center;
  animation: jp-Statusbar-ExecutionTime-progress-bar 2s linear infinite;
}

.jp-Statusbar-ProgressBar-progress-bar p {
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
  font-size: var(--jp-ui-font-size1);
  line-height: 10px;
  width: 100px;
}

@keyframes jp-Statusbar-ExecutionTime-progress-bar {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 40px 40px;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-commandpalette-search-height: 28px;
}

/*-----------------------------------------------------------------------------
| Overall styles
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  padding-bottom: 0;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Modal variant
|----------------------------------------------------------------------------*/

.jp-ModalCommandPalette {
  position: absolute;
  z-index: 10000;
  top: 38px;
  left: 30%;
  margin: 0;
  padding: 4px;
  width: 40%;
  box-shadow: var(--jp-elevation-z4);
  border-radius: 4px;
  background: var(--jp-layout-color0);
}

.jp-ModalCommandPalette .lm-CommandPalette {
  max-height: 40vh;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-close-icon::after {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-header {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-item {
  margin-left: 4px;
  margin-right: 4px;
}

.jp-ModalCommandPalette
  .lm-CommandPalette
  .lm-CommandPalette-item.lm-mod-disabled {
  display: none;
}

/*-----------------------------------------------------------------------------
| Search
|----------------------------------------------------------------------------*/

.lm-CommandPalette-search {
  padding: 4px;
  background-color: var(--jp-layout-color1);
  z-index: 2;
}

.lm-CommandPalette-wrapper {
  overflow: overlay;
  padding: 0 9px;
  background-color: var(--jp-input-active-background);
  height: 30px;
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
}

.lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper {
  box-shadow: inset 0 0 0 1px var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-SearchIconGroup {
  color: white;
  background-color: var(--jp-brand-color1);
  position: absolute;
  top: 4px;
  right: 4px;
  padding: 5px 5px 1px;
}

.jp-SearchIconGroup svg {
  height: 20px;
  width: 20px;
}

.jp-SearchIconGroup .jp-icon3[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-input {
  background: transparent;
  width: calc(100% - 18px);
  float: left;
  border: none;
  outline: none;
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  line-height: var(--jp-private-commandpalette-search-height);
}

.lm-CommandPalette-input::-webkit-input-placeholder,
.lm-CommandPalette-input::-moz-placeholder,
.lm-CommandPalette-input:-ms-input-placeholder {
  color: var(--jp-ui-font-color2);
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Results
|----------------------------------------------------------------------------*/

.lm-CommandPalette-header:first-child {
  margin-top: 0;
}

.lm-CommandPalette-header {
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin-top: 8px;
  padding: 8px 0 8px 12px;
  text-transform: uppercase;
}

.lm-CommandPalette-header.lm-mod-active {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-header > mark {
  background-color: transparent;
  font-weight: bold;
  color: var(--jp-ui-font-color1);
}

.lm-CommandPalette-item {
  padding: 4px 12px 4px 4px;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  font-weight: 400;
  display: flex;
}

.lm-CommandPalette-item.lm-mod-disabled {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item.lm-mod-active {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item.lm-mod-active .lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-inverse-font-color0);
}

.lm-CommandPalette-item.lm-mod-active .jp-icon-selectable[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled) {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled) {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-itemContent {
  overflow: hidden;
}

.lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.lm-CommandPalette-item.lm-mod-disabled mark {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item .lm-CommandPalette-itemIcon {
  margin: 0 4px 0 0;
  position: relative;
  width: 16px;
  top: 2px;
  flex: 0 0 auto;
}

.lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon {
  opacity: 0.6;
}

.lm-CommandPalette-item .lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemCaption {
  display: none;
}

.lm-CommandPalette-content {
  background-color: var(--jp-layout-color1);
}

.lm-CommandPalette-content:empty::after {
  content: 'No results';
  margin: auto;
  margin-top: 20px;
  width: 100px;
  display: block;
  font-size: var(--jp-ui-font-size2);
  font-family: var(--jp-ui-font-family);
  font-weight: lighter;
}

.lm-CommandPalette-emptyMessage {
  text-align: center;
  margin-top: 24px;
  line-height: 1.32;
  padding: 0 8px;
  color: var(--jp-content-font-color3);
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Dialog {
  position: absolute;
  z-index: 10000;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  top: 0;
  left: 0;
  margin: 0;
  padding: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-dialog-background);
}

.jp-Dialog-content {
  display: flex;
  flex-direction: column;
  margin-left: auto;
  margin-right: auto;
  background: var(--jp-layout-color1);
  padding: 24px 24px 12px;
  min-width: 300px;
  min-height: 150px;
  max-width: 1000px;
  max-height: 500px;
  box-sizing: border-box;
  box-shadow: var(--jp-elevation-z20);
  word-wrap: break-word;
  border-radius: var(--jp-border-radius);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color1);
  resize: both;
}

.jp-Dialog-content.jp-Dialog-content-small {
  max-width: 500px;
}

.jp-Dialog-button {
  overflow: visible;
}

button.jp-Dialog-button:focus {
  outline: 1px solid var(--jp-brand-color1);
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button:focus::-moz-focus-inner {
  border: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus {
  outline: 1px solid var(--jp-accept-color-normal, var(--jp-brand-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus {
  outline: 1px solid var(--jp-warn-color-normal, var(--jp-error-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline: 1px solid var(--jp-reject-color-normal, var(--md-grey-600));
}

button.jp-Dialog-close-button {
  padding: 0;
  height: 100%;
  min-width: unset;
  min-height: unset;
}

.jp-Dialog-header {
  display: flex;
  justify-content: space-between;
  flex: 0 0 auto;
  padding-bottom: 12px;
  font-size: var(--jp-ui-font-size3);
  font-weight: 400;
  color: var(--jp-ui-font-color1);
}

.jp-Dialog-body {
  display: flex;
  flex-direction: column;
  flex: 1 1 auto;
  font-size: var(--jp-ui-font-size1);
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

.jp-Dialog-footer {
  display: flex;
  flex-direction: row;
  justify-content: flex-end;
  align-items: center;
  flex: 0 0 auto;
  margin-left: -12px;
  margin-right: -12px;
  padding: 12px;
}

.jp-Dialog-checkbox {
  padding-right: 5px;
}

.jp-Dialog-checkbox > input:focus-visible {
  outline: 1px solid var(--jp-input-active-border-color);
  outline-offset: 1px;
}

.jp-Dialog-spacer {
  flex: 1 1 auto;
}

.jp-Dialog-title {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.jp-Dialog-body > .jp-select-wrapper {
  width: 100%;
}

.jp-Dialog-body > button {
  padding: 0 16px;
}

.jp-Dialog-body > label {
  line-height: 1.4;
  color: var(--jp-ui-font-color0);
}

.jp-Dialog-button.jp-mod-styled:not(:last-child) {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Input-Boolean-Dialog {
  flex-direction: row-reverse;
  align-items: end;
  width: 100%;
}

.jp-Input-Boolean-Dialog > label {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MainAreaWidget > :focus {
  outline: none;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error {
  padding: 6px;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error > pre {
  width: auto;
  padding: 10px;
  background: var(--jp-error-color3);
  border: var(--jp-border-width) solid var(--jp-error-color1);
  border-radius: var(--jp-border-radius);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  white-space: pre-wrap;
  word-wrap: break-word;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/**
 * google-material-color v1.2.6
 * https://github.com/danlevan/google-material-color
 */
:root {
  --md-red-50: #ffebee;
  --md-red-100: #ffcdd2;
  --md-red-200: #ef9a9a;
  --md-red-300: #e57373;
  --md-red-400: #ef5350;
  --md-red-500: #f44336;
  --md-red-600: #e53935;
  --md-red-700: #d32f2f;
  --md-red-800: #c62828;
  --md-red-900: #b71c1c;
  --md-red-A100: #ff8a80;
  --md-red-A200: #ff5252;
  --md-red-A400: #ff1744;
  --md-red-A700: #d50000;
  --md-pink-50: #fce4ec;
  --md-pink-100: #f8bbd0;
  --md-pink-200: #f48fb1;
  --md-pink-300: #f06292;
  --md-pink-400: #ec407a;
  --md-pink-500: #e91e63;
  --md-pink-600: #d81b60;
  --md-pink-700: #c2185b;
  --md-pink-800: #ad1457;
  --md-pink-900: #880e4f;
  --md-pink-A100: #ff80ab;
  --md-pink-A200: #ff4081;
  --md-pink-A400: #f50057;
  --md-pink-A700: #c51162;
  --md-purple-50: #f3e5f5;
  --md-purple-100: #e1bee7;
  --md-purple-200: #ce93d8;
  --md-purple-300: #ba68c8;
  --md-purple-400: #ab47bc;
  --md-purple-500: #9c27b0;
  --md-purple-600: #8e24aa;
  --md-purple-700: #7b1fa2;
  --md-purple-800: #6a1b9a;
  --md-purple-900: #4a148c;
  --md-purple-A100: #ea80fc;
  --md-purple-A200: #e040fb;
  --md-purple-A400: #d500f9;
  --md-purple-A700: #a0f;
  --md-deep-purple-50: #ede7f6;
  --md-deep-purple-100: #d1c4e9;
  --md-deep-purple-200: #b39ddb;
  --md-deep-purple-300: #9575cd;
  --md-deep-purple-400: #7e57c2;
  --md-deep-purple-500: #673ab7;
  --md-deep-purple-600: #5e35b1;
  --md-deep-purple-700: #512da8;
  --md-deep-purple-800: #4527a0;
  --md-deep-purple-900: #311b92;
  --md-deep-purple-A100: #b388ff;
  --md-deep-purple-A200: #7c4dff;
  --md-deep-purple-A400: #651fff;
  --md-deep-purple-A700: #6200ea;
  --md-indigo-50: #e8eaf6;
  --md-indigo-100: #c5cae9;
  --md-indigo-200: #9fa8da;
  --md-indigo-300: #7986cb;
  --md-indigo-400: #5c6bc0;
  --md-indigo-500: #3f51b5;
  --md-indigo-600: #3949ab;
  --md-indigo-700: #303f9f;
  --md-indigo-800: #283593;
  --md-indigo-900: #1a237e;
  --md-indigo-A100: #8c9eff;
  --md-indigo-A200: #536dfe;
  --md-indigo-A400: #3d5afe;
  --md-indigo-A700: #304ffe;
  --md-blue-50: #e3f2fd;
  --md-blue-100: #bbdefb;
  --md-blue-200: #90caf9;
  --md-blue-300: #64b5f6;
  --md-blue-400: #42a5f5;
  --md-blue-500: #2196f3;
  --md-blue-600: #1e88e5;
  --md-blue-700: #1976d2;
  --md-blue-800: #1565c0;
  --md-blue-900: #0d47a1;
  --md-blue-A100: #82b1ff;
  --md-blue-A200: #448aff;
  --md-blue-A400: #2979ff;
  --md-blue-A700: #2962ff;
  --md-light-blue-50: #e1f5fe;
  --md-light-blue-100: #b3e5fc;
  --md-light-blue-200: #81d4fa;
  --md-light-blue-300: #4fc3f7;
  --md-light-blue-400: #29b6f6;
  --md-light-blue-500: #03a9f4;
  --md-light-blue-600: #039be5;
  --md-light-blue-700: #0288d1;
  --md-light-blue-800: #0277bd;
  --md-light-blue-900: #01579b;
  --md-light-blue-A100: #80d8ff;
  --md-light-blue-A200: #40c4ff;
  --md-light-blue-A400: #00b0ff;
  --md-light-blue-A700: #0091ea;
  --md-cyan-50: #e0f7fa;
  --md-cyan-100: #b2ebf2;
  --md-cyan-200: #80deea;
  --md-cyan-300: #4dd0e1;
  --md-cyan-400: #26c6da;
  --md-cyan-500: #00bcd4;
  --md-cyan-600: #00acc1;
  --md-cyan-700: #0097a7;
  --md-cyan-800: #00838f;
  --md-cyan-900: #006064;
  --md-cyan-A100: #84ffff;
  --md-cyan-A200: #18ffff;
  --md-cyan-A400: #00e5ff;
  --md-cyan-A700: #00b8d4;
  --md-teal-50: #e0f2f1;
  --md-teal-100: #b2dfdb;
  --md-teal-200: #80cbc4;
  --md-teal-300: #4db6ac;
  --md-teal-400: #26a69a;
  --md-teal-500: #009688;
  --md-teal-600: #00897b;
  --md-teal-700: #00796b;
  --md-teal-800: #00695c;
  --md-teal-900: #004d40;
  --md-teal-A100: #a7ffeb;
  --md-teal-A200: #64ffda;
  --md-teal-A400: #1de9b6;
  --md-teal-A700: #00bfa5;
  --md-green-50: #e8f5e9;
  --md-green-100: #c8e6c9;
  --md-green-200: #a5d6a7;
  --md-green-300: #81c784;
  --md-green-400: #66bb6a;
  --md-green-500: #4caf50;
  --md-green-600: #43a047;
  --md-green-700: #388e3c;
  --md-green-800: #2e7d32;
  --md-green-900: #1b5e20;
  --md-green-A100: #b9f6ca;
  --md-green-A200: #69f0ae;
  --md-green-A400: #00e676;
  --md-green-A700: #00c853;
  --md-light-green-50: #f1f8e9;
  --md-light-green-100: #dcedc8;
  --md-light-green-200: #c5e1a5;
  --md-light-green-300: #aed581;
  --md-light-green-400: #9ccc65;
  --md-light-green-500: #8bc34a;
  --md-light-green-600: #7cb342;
  --md-light-green-700: #689f38;
  --md-light-green-800: #558b2f;
  --md-light-green-900: #33691e;
  --md-light-green-A100: #ccff90;
  --md-light-green-A200: #b2ff59;
  --md-light-green-A400: #76ff03;
  --md-light-green-A700: #64dd17;
  --md-lime-50: #f9fbe7;
  --md-lime-100: #f0f4c3;
  --md-lime-200: #e6ee9c;
  --md-lime-300: #dce775;
  --md-lime-400: #d4e157;
  --md-lime-500: #cddc39;
  --md-lime-600: #c0ca33;
  --md-lime-700: #afb42b;
  --md-lime-800: #9e9d24;
  --md-lime-900: #827717;
  --md-lime-A100: #f4ff81;
  --md-lime-A200: #eeff41;
  --md-lime-A400: #c6ff00;
  --md-lime-A700: #aeea00;
  --md-yellow-50: #fffde7;
  --md-yellow-100: #fff9c4;
  --md-yellow-200: #fff59d;
  --md-yellow-300: #fff176;
  --md-yellow-400: #ffee58;
  --md-yellow-500: #ffeb3b;
  --md-yellow-600: #fdd835;
  --md-yellow-700: #fbc02d;
  --md-yellow-800: #f9a825;
  --md-yellow-900: #f57f17;
  --md-yellow-A100: #ffff8d;
  --md-yellow-A200: #ff0;
  --md-yellow-A400: #ffea00;
  --md-yellow-A700: #ffd600;
  --md-amber-50: #fff8e1;
  --md-amber-100: #ffecb3;
  --md-amber-200: #ffe082;
  --md-amber-300: #ffd54f;
  --md-amber-400: #ffca28;
  --md-amber-500: #ffc107;
  --md-amber-600: #ffb300;
  --md-amber-700: #ffa000;
  --md-amber-800: #ff8f00;
  --md-amber-900: #ff6f00;
  --md-amber-A100: #ffe57f;
  --md-amber-A200: #ffd740;
  --md-amber-A400: #ffc400;
  --md-amber-A700: #ffab00;
  --md-orange-50: #fff3e0;
  --md-orange-100: #ffe0b2;
  --md-orange-200: #ffcc80;
  --md-orange-300: #ffb74d;
  --md-orange-400: #ffa726;
  --md-orange-500: #ff9800;
  --md-orange-600: #fb8c00;
  --md-orange-700: #f57c00;
  --md-orange-800: #ef6c00;
  --md-orange-900: #e65100;
  --md-orange-A100: #ffd180;
  --md-orange-A200: #ffab40;
  --md-orange-A400: #ff9100;
  --md-orange-A700: #ff6d00;
  --md-deep-orange-50: #fbe9e7;
  --md-deep-orange-100: #ffccbc;
  --md-deep-orange-200: #ffab91;
  --md-deep-orange-300: #ff8a65;
  --md-deep-orange-400: #ff7043;
  --md-deep-orange-500: #ff5722;
  --md-deep-orange-600: #f4511e;
  --md-deep-orange-700: #e64a19;
  --md-deep-orange-800: #d84315;
  --md-deep-orange-900: #bf360c;
  --md-deep-orange-A100: #ff9e80;
  --md-deep-orange-A200: #ff6e40;
  --md-deep-orange-A400: #ff3d00;
  --md-deep-orange-A700: #dd2c00;
  --md-brown-50: #efebe9;
  --md-brown-100: #d7ccc8;
  --md-brown-200: #bcaaa4;
  --md-brown-300: #a1887f;
  --md-brown-400: #8d6e63;
  --md-brown-500: #795548;
  --md-brown-600: #6d4c41;
  --md-brown-700: #5d4037;
  --md-brown-800: #4e342e;
  --md-brown-900: #3e2723;
  --md-grey-50: #fafafa;
  --md-grey-100: #f5f5f5;
  --md-grey-200: #eee;
  --md-grey-300: #e0e0e0;
  --md-grey-400: #bdbdbd;
  --md-grey-500: #9e9e9e;
  --md-grey-600: #757575;
  --md-grey-700: #616161;
  --md-grey-800: #424242;
  --md-grey-900: #212121;
  --md-blue-grey-50: #eceff1;
  --md-blue-grey-100: #cfd8dc;
  --md-blue-grey-200: #b0bec5;
  --md-blue-grey-300: #90a4ae;
  --md-blue-grey-400: #78909c;
  --md-blue-grey-500: #607d8b;
  --md-blue-grey-600: #546e7a;
  --md-blue-grey-700: #455a64;
  --md-blue-grey-800: #37474f;
  --md-blue-grey-900: #263238;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| RenderedText
|----------------------------------------------------------------------------*/

:root {
  /* This is the padding value to fill the gaps between lines containing spans with background color. */
  --jp-private-code-span-padding: calc(
    (var(--jp-code-line-height) - 1) * var(--jp-code-font-size) / 2
  );
}

.jp-RenderedText {
  text-align: left;
  padding-left: var(--jp-code-padding);
  line-height: var(--jp-code-line-height);
  font-family: var(--jp-code-font-family);
}

.jp-RenderedText pre,
.jp-RenderedJavaScript pre,
.jp-RenderedHTMLCommon pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
  border: none;
  margin: 0;
  padding: 0;
}

.jp-RenderedText pre a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* console foregrounds and backgrounds */
.jp-RenderedText pre .ansi-black-fg {
  color: #3e424d;
}

.jp-RenderedText pre .ansi-red-fg {
  color: #e75c58;
}

.jp-RenderedText pre .ansi-green-fg {
  color: #00a250;
}

.jp-RenderedText pre .ansi-yellow-fg {
  color: #ddb62b;
}

.jp-RenderedText pre .ansi-blue-fg {
  color: #208ffb;
}

.jp-RenderedText pre .ansi-magenta-fg {
  color: #d160c4;
}

.jp-RenderedText pre .ansi-cyan-fg {
  color: #60c6c8;
}

.jp-RenderedText pre .ansi-white-fg {
  color: #c5c1b4;
}

.jp-RenderedText pre .ansi-black-bg {
  background-color: #3e424d;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-bg {
  background-color: #e75c58;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-bg {
  background-color: #00a250;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-bg {
  background-color: #ddb62b;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-bg {
  background-color: #208ffb;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-bg {
  background-color: #d160c4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-bg {
  background-color: #60c6c8;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-bg {
  background-color: #c5c1b4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-black-intense-fg {
  color: #282c36;
}

.jp-RenderedText pre .ansi-red-intense-fg {
  color: #b22b31;
}

.jp-RenderedText pre .ansi-green-intense-fg {
  color: #007427;
}

.jp-RenderedText pre .ansi-yellow-intense-fg {
  color: #b27d12;
}

.jp-RenderedText pre .ansi-blue-intense-fg {
  color: #0065ca;
}

.jp-RenderedText pre .ansi-magenta-intense-fg {
  color: #a03196;
}

.jp-RenderedText pre .ansi-cyan-intense-fg {
  color: #258f8f;
}

.jp-RenderedText pre .ansi-white-intense-fg {
  color: #a1a6b2;
}

.jp-RenderedText pre .ansi-black-intense-bg {
  background-color: #282c36;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-intense-bg {
  background-color: #b22b31;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-intense-bg {
  background-color: #007427;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-intense-bg {
  background-color: #b27d12;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-intense-bg {
  background-color: #0065ca;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-intense-bg {
  background-color: #a03196;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-intense-bg {
  background-color: #258f8f;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-intense-bg {
  background-color: #a1a6b2;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-default-inverse-fg {
  color: var(--jp-ui-inverse-font-color0);
}

.jp-RenderedText pre .ansi-default-inverse-bg {
  background-color: var(--jp-inverse-layout-color0);
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-bold {
  font-weight: bold;
}

.jp-RenderedText pre .ansi-underline {
  text-decoration: underline;
}

.jp-RenderedText[data-mime-type='application/vnd.jupyter.stderr'] {
  background: var(--jp-rendermime-error-background);
  padding-top: var(--jp-code-padding);
}

/*-----------------------------------------------------------------------------
| RenderedLatex
|----------------------------------------------------------------------------*/

.jp-RenderedLatex {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);
}

/* Left-justify outputs.*/
.jp-OutputArea-output.jp-RenderedLatex {
  padding: var(--jp-code-padding);
  text-align: left;
}

/*-----------------------------------------------------------------------------
| RenderedHTML
|----------------------------------------------------------------------------*/

.jp-RenderedHTMLCommon {
  color: var(--jp-content-font-color1);
  font-family: var(--jp-content-font-family);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);

  /* Give a bit more R padding on Markdown text to keep line lengths reasonable */
  padding-right: 20px;
}

.jp-RenderedHTMLCommon em {
  font-style: italic;
}

.jp-RenderedHTMLCommon strong {
  font-weight: bold;
}

.jp-RenderedHTMLCommon u {
  text-decoration: underline;
}

.jp-RenderedHTMLCommon a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* Headings */

.jp-RenderedHTMLCommon h1,
.jp-RenderedHTMLCommon h2,
.jp-RenderedHTMLCommon h3,
.jp-RenderedHTMLCommon h4,
.jp-RenderedHTMLCommon h5,
.jp-RenderedHTMLCommon h6 {
  line-height: var(--jp-content-heading-line-height);
  font-weight: var(--jp-content-heading-font-weight);
  font-style: normal;
  margin: var(--jp-content-heading-margin-top) 0
    var(--jp-content-heading-margin-bottom) 0;
}

.jp-RenderedHTMLCommon h1:first-child,
.jp-RenderedHTMLCommon h2:first-child,
.jp-RenderedHTMLCommon h3:first-child,
.jp-RenderedHTMLCommon h4:first-child,
.jp-RenderedHTMLCommon h5:first-child,
.jp-RenderedHTMLCommon h6:first-child {
  margin-top: calc(0.5 * var(--jp-content-heading-margin-top));
}

.jp-RenderedHTMLCommon h1:last-child,
.jp-RenderedHTMLCommon h2:last-child,
.jp-RenderedHTMLCommon h3:last-child,
.jp-RenderedHTMLCommon h4:last-child,
.jp-RenderedHTMLCommon h5:last-child,
.jp-RenderedHTMLCommon h6:last-child {
  margin-bottom: calc(0.5 * var(--jp-content-heading-margin-bottom));
}

.jp-RenderedHTMLCommon h1 {
  font-size: var(--jp-content-font-size5);
}

.jp-RenderedHTMLCommon h2 {
  font-size: var(--jp-content-font-size4);
}

.jp-RenderedHTMLCommon h3 {
  font-size: var(--jp-content-font-size3);
}

.jp-RenderedHTMLCommon h4 {
  font-size: var(--jp-content-font-size2);
}

.jp-RenderedHTMLCommon h5 {
  font-size: var(--jp-content-font-size1);
}

.jp-RenderedHTMLCommon h6 {
  font-size: var(--jp-content-font-size0);
}

/* Lists */

/* stylelint-disable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon ul:not(.list-inline),
.jp-RenderedHTMLCommon ol:not(.list-inline) {
  padding-left: 2em;
}

.jp-RenderedHTMLCommon ul {
  list-style: disc;
}

.jp-RenderedHTMLCommon ul ul {
  list-style: square;
}

.jp-RenderedHTMLCommon ul ul ul {
  list-style: circle;
}

.jp-RenderedHTMLCommon ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol ol {
  list-style: upper-alpha;
}

.jp-RenderedHTMLCommon ol ol ol {
  list-style: lower-alpha;
}

.jp-RenderedHTMLCommon ol ol ol ol {
  list-style: lower-roman;
}

.jp-RenderedHTMLCommon ol ol ol ol ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol,
.jp-RenderedHTMLCommon ul {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon ul ul,
.jp-RenderedHTMLCommon ul ol,
.jp-RenderedHTMLCommon ol ul,
.jp-RenderedHTMLCommon ol ol {
  margin-bottom: 0;
}

/* stylelint-enable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon hr {
  color: var(--jp-border-color2);
  background-color: var(--jp-border-color1);
  margin-top: 1em;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon > pre {
  margin: 1.5em 2em;
}

.jp-RenderedHTMLCommon pre,
.jp-RenderedHTMLCommon code {
  border: 0;
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  line-height: var(--jp-code-line-height);
  padding: 0;
  white-space: pre-wrap;
}

.jp-RenderedHTMLCommon :not(pre) > code {
  background-color: var(--jp-layout-color2);
  padding: 1px 5px;
}

/* Tables */

.jp-RenderedHTMLCommon table {
  border-collapse: collapse;
  border-spacing: 0;
  border: none;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  table-layout: fixed;
  margin-left: auto;
  margin-bottom: 1em;
  margin-right: auto;
}

.jp-RenderedHTMLCommon thead {
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  vertical-align: bottom;
}

.jp-RenderedHTMLCommon td,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon tr {
  vertical-align: middle;
  padding: 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}

.jp-RenderedMarkdown.jp-RenderedHTMLCommon td,
.jp-RenderedMarkdown.jp-RenderedHTMLCommon th {
  max-width: none;
}

:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon td,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon th,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon tr {
  text-align: right;
}

.jp-RenderedHTMLCommon th {
  font-weight: bold;
}

.jp-RenderedHTMLCommon tbody tr:nth-child(odd) {
  background: var(--jp-layout-color0);
}

.jp-RenderedHTMLCommon tbody tr:nth-child(even) {
  background: var(--jp-rendermime-table-row-background);
}

.jp-RenderedHTMLCommon tbody tr:hover {
  background: var(--jp-rendermime-table-row-hover-background);
}

.jp-RenderedHTMLCommon p {
  text-align: left;
  margin: 0;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon img {
  -moz-force-broken-image-icon: 1;
}

/* Restrict to direct children as other images could be nested in other content. */
.jp-RenderedHTMLCommon > img {
  display: block;
  margin-left: 0;
  margin-right: 0;
  margin-bottom: 1em;
}

/* Change color behind transparent images if they need it... */
[data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-light-background {
  background-color: var(--jp-inverse-layout-color1);
}

[data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-dark-background {
  background-color: var(--jp-inverse-layout-color1);
}

.jp-RenderedHTMLCommon img,
.jp-RenderedImage img,
.jp-RenderedHTMLCommon svg,
.jp-RenderedSVG svg {
  max-width: 100%;
  height: auto;
}

.jp-RenderedHTMLCommon img.jp-mod-unconfined,
.jp-RenderedImage img.jp-mod-unconfined,
.jp-RenderedHTMLCommon svg.jp-mod-unconfined,
.jp-RenderedSVG svg.jp-mod-unconfined {
  max-width: none;
}

.jp-RenderedHTMLCommon .alert {
  padding: var(--jp-notebook-padding);
  border: var(--jp-border-width) solid transparent;
  border-radius: var(--jp-border-radius);
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon .alert-info {
  color: var(--jp-info-color0);
  background-color: var(--jp-info-color3);
  border-color: var(--jp-info-color2);
}

.jp-RenderedHTMLCommon .alert-info hr {
  border-color: var(--jp-info-color3);
}

.jp-RenderedHTMLCommon .alert-info > p:last-child,
.jp-RenderedHTMLCommon .alert-info > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-warning {
  color: var(--jp-warn-color0);
  background-color: var(--jp-warn-color3);
  border-color: var(--jp-warn-color2);
}

.jp-RenderedHTMLCommon .alert-warning hr {
  border-color: var(--jp-warn-color3);
}

.jp-RenderedHTMLCommon .alert-warning > p:last-child,
.jp-RenderedHTMLCommon .alert-warning > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-success {
  color: var(--jp-success-color0);
  background-color: var(--jp-success-color3);
  border-color: var(--jp-success-color2);
}

.jp-RenderedHTMLCommon .alert-success hr {
  border-color: var(--jp-success-color3);
}

.jp-RenderedHTMLCommon .alert-success > p:last-child,
.jp-RenderedHTMLCommon .alert-success > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-danger {
  color: var(--jp-error-color0);
  background-color: var(--jp-error-color3);
  border-color: var(--jp-error-color2);
}

.jp-RenderedHTMLCommon .alert-danger hr {
  border-color: var(--jp-error-color3);
}

.jp-RenderedHTMLCommon .alert-danger > p:last-child,
.jp-RenderedHTMLCommon .alert-danger > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon blockquote {
  margin: 1em 2em;
  padding: 0 1em;
  border-left: 5px solid var(--jp-border-color2);
}

a.jp-InternalAnchorLink {
  visibility: hidden;
  margin-left: 8px;
  color: var(--md-blue-800);
}

h1:hover .jp-InternalAnchorLink,
h2:hover .jp-InternalAnchorLink,
h3:hover .jp-InternalAnchorLink,
h4:hover .jp-InternalAnchorLink,
h5:hover .jp-InternalAnchorLink,
h6:hover .jp-InternalAnchorLink {
  visibility: visible;
}

.jp-RenderedHTMLCommon kbd {
  background-color: var(--jp-rendermime-table-row-background);
  border: 1px solid var(--jp-border-color0);
  border-bottom-color: var(--jp-border-color2);
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
  display: inline-block;
  font-size: var(--jp-ui-font-size0);
  line-height: 1em;
  padding: 0.2em 0.5em;
}

/* Most direct children of .jp-RenderedHTMLCommon have a margin-bottom of 1.0.
 * At the bottom of cells this is a bit too much as there is also spacing
 * between cells. Going all the way to 0 gets too tight between markdown and
 * code cells.
 */
.jp-RenderedHTMLCommon > *:last-child {
  margin-bottom: 0.5em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-cursor-backdrop {
  position: fixed;
  width: 200px;
  height: 200px;
  margin-top: -100px;
  margin-left: -100px;
  will-change: transform;
  z-index: 100;
}

.lm-mod-drag-image {
  will-change: transform;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-lineFormSearch {
  padding: 4px 12px;
  background-color: var(--jp-layout-color2);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
  font-size: var(--jp-ui-font-size1);
}

.jp-lineFormCaption {
  font-size: var(--jp-ui-font-size0);
  line-height: var(--jp-ui-font-size1);
  margin-top: 4px;
  color: var(--jp-ui-font-color0);
}

.jp-baseLineForm {
  border: none;
  border-radius: 0;
  position: absolute;
  background-size: 16px;
  background-repeat: no-repeat;
  background-position: center;
  outline: none;
}

.jp-lineFormButtonContainer {
  top: 4px;
  right: 8px;
  height: 24px;
  padding: 0 12px;
  width: 12px;
}

.jp-lineFormButtonIcon {
  top: 0;
  right: 0;
  background-color: var(--jp-brand-color1);
  height: 100%;
  width: 100%;
  box-sizing: border-box;
  padding: 4px 6px;
}

.jp-lineFormButton {
  top: 0;
  right: 0;
  background-color: transparent;
  height: 100%;
  width: 100%;
  box-sizing: border-box;
}

.jp-lineFormWrapper {
  overflow: hidden;
  padding: 0 8px;
  border: 1px solid var(--jp-border-color0);
  background-color: var(--jp-input-active-background);
  height: 22px;
}

.jp-lineFormWrapperFocusWithin {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-lineFormInput {
  background: transparent;
  width: 200px;
  height: 100%;
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  line-height: 28px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-JSONEditor {
  display: flex;
  flex-direction: column;
  width: 100%;
}

.jp-JSONEditor-host {
  flex: 1 1 auto;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  background: var(--jp-layout-color0);
  min-height: 50px;
  padding: 1px;
}

.jp-JSONEditor.jp-mod-error .jp-JSONEditor-host {
  border-color: red;
  outline-color: red;
}

.jp-JSONEditor-header {
  display: flex;
  flex: 1 0 auto;
  padding: 0 0 0 12px;
}

.jp-JSONEditor-header label {
  flex: 0 0 auto;
}

.jp-JSONEditor-commitButton {
  height: 16px;
  width: 16px;
  background-size: 18px;
  background-repeat: no-repeat;
  background-position: center;
}

.jp-JSONEditor-host.jp-mod-focused {
  background-color: var(--jp-input-active-background);
  border: 1px solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

.jp-Editor.jp-mod-dropTarget {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/
.jp-DocumentSearch-input {
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  font-size: var(--jp-ui-font-size1);
  background-color: var(--jp-layout-color0);
  font-family: var(--jp-ui-font-family);
  padding: 2px 1px;
  resize: none;
}

.jp-DocumentSearch-overlay {
  position: absolute;
  background-color: var(--jp-toolbar-background);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  border-left: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  top: 0;
  right: 0;
  z-index: 7;
  min-width: 405px;
  padding: 2px;
  font-size: var(--jp-ui-font-size1);

  --jp-private-document-search-button-height: 20px;
}

.jp-DocumentSearch-overlay button {
  background-color: var(--jp-toolbar-background);
  outline: 0;
}

.jp-DocumentSearch-overlay button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-overlay button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-overlay-row {
  display: flex;
  align-items: center;
  margin-bottom: 2px;
}

.jp-DocumentSearch-button-content {
  display: inline-block;
  cursor: pointer;
  box-sizing: border-box;
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-button-content svg {
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-input-wrapper {
  border: var(--jp-border-width) solid var(--jp-border-color0);
  display: flex;
  background-color: var(--jp-layout-color0);
  margin: 2px;
}

.jp-DocumentSearch-input-wrapper:focus-within {
  border-color: var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper {
  all: initial;
  overflow: hidden;
  display: inline-block;
  border: none;
  box-sizing: border-box;
}

.jp-DocumentSearch-toggle-wrapper {
  width: 14px;
  height: 14px;
}

.jp-DocumentSearch-button-wrapper {
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
}

.jp-DocumentSearch-toggle-wrapper:focus,
.jp-DocumentSearch-button-wrapper:focus {
  outline: var(--jp-border-width) solid
    var(--jp-cell-editor-active-border-color);
  outline-offset: -1px;
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper,
.jp-DocumentSearch-button-content:focus {
  outline: none;
}

.jp-DocumentSearch-toggle-placeholder {
  width: 5px;
}

.jp-DocumentSearch-input-button::before {
  display: block;
  padding-top: 100%;
}

.jp-DocumentSearch-input-button-off {
  opacity: var(--jp-search-toggle-off-opacity);
}

.jp-DocumentSearch-input-button-off:hover {
  opacity: var(--jp-search-toggle-hover-opacity);
}

.jp-DocumentSearch-input-button-on {
  opacity: var(--jp-search-toggle-on-opacity);
}

.jp-DocumentSearch-index-counter {
  padding-left: 10px;
  padding-right: 10px;
  user-select: none;
  min-width: 35px;
  display: inline-block;
}

.jp-DocumentSearch-up-down-wrapper {
  display: inline-block;
  padding-right: 2px;
  margin-left: auto;
  white-space: nowrap;
}

.jp-DocumentSearch-spacer {
  margin-left: auto;
}

.jp-DocumentSearch-up-down-wrapper button {
  outline: 0;
  border: none;
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
  vertical-align: middle;
  margin: 1px 5px 2px;
}

.jp-DocumentSearch-up-down-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-up-down-button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-filter-button {
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-filter-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled:hover {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-search-options {
  padding: 0 8px;
  margin-left: 3px;
  width: 100%;
  display: grid;
  justify-content: start;
  grid-template-columns: 1fr 1fr;
  align-items: center;
  justify-items: stretch;
}

.jp-DocumentSearch-search-filter-disabled {
  color: var(--jp-ui-font-color2);
}

.jp-DocumentSearch-search-filter {
  display: flex;
  align-items: center;
  user-select: none;
}

.jp-DocumentSearch-regex-error {
  color: var(--jp-error-color0);
}

.jp-DocumentSearch-replace-button-wrapper {
  overflow: hidden;
  display: inline-block;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color0);
  margin: auto 2px;
  padding: 1px 4px;
  height: calc(var(--jp-private-document-search-button-height) + 2px);
}

.jp-DocumentSearch-replace-button-wrapper:focus {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-replace-button {
  display: inline-block;
  text-align: center;
  cursor: pointer;
  box-sizing: border-box;
  color: var(--jp-ui-font-color1);

  /* height - 2 * (padding of wrapper) */
  line-height: calc(var(--jp-private-document-search-button-height) - 2px);
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-replace-button:focus {
  outline: none;
}

.jp-DocumentSearch-replace-wrapper-class {
  margin-left: 14px;
  display: flex;
}

.jp-DocumentSearch-replace-toggle {
  border: none;
  background-color: var(--jp-toolbar-background);
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-replace-toggle:hover {
  background-color: var(--jp-layout-color2);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.cm-editor {
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  border: 0;
  border-radius: 0;
  height: auto;

  /* Changed to auto to autogrow */
}

.cm-editor pre {
  padding: 0 var(--jp-code-padding);
}

.jp-CodeMirrorEditor[data-type='inline'] .cm-dialog {
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
}

.jp-CodeMirrorEditor {
  cursor: text;
}

/* When zoomed out 67% and 33% on a screen of 1440 width x 900 height */
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width1) solid
      var(--jp-editor-cursor-color);
  }
}

/* When zoomed out less than 33% */
@media screen and (min-width: 4320px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width2) solid
      var(--jp-editor-cursor-color);
  }
}

.cm-editor.jp-mod-readOnly .cm-cursor {
  display: none;
}

.jp-CollaboratorCursor {
  border-left: 5px solid transparent;
  border-right: 5px solid transparent;
  border-top: none;
  border-bottom: 3px solid;
  background-clip: content-box;
  margin-left: -5px;
  margin-right: -5px;
}

.cm-searching,
.cm-searching span {
  /* `.cm-searching span`: we need to override syntax highlighting */
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.cm-searching::selection,
.cm-searching span::selection {
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.jp-current-match > .cm-searching,
.jp-current-match > .cm-searching span,
.cm-searching > .jp-current-match,
.cm-searching > .jp-current-match span {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.jp-current-match > .cm-searching::selection,
.cm-searching > .jp-current-match::selection,
.jp-current-match > .cm-searching span::selection {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.cm-trailingspace {
  background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAYAAAB4ka1VAAAAsElEQVQIHQGlAFr/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7+r3zKmT0/+pk9P/7+r3zAAAAAAAAAAABAAAAAAAAAAA6OPzM+/q9wAAAAAA6OPzMwAAAAAAAAAAAgAAAAAAAAAAGR8NiRQaCgAZIA0AGR8NiQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQyoYJ/SY80UAAAAASUVORK5CYII=);
  background-position: center left;
  background-repeat: repeat-x;
}

.jp-CollaboratorCursor-hover {
  position: absolute;
  z-index: 1;
  transform: translateX(-50%);
  color: white;
  border-radius: 3px;
  padding-left: 4px;
  padding-right: 4px;
  padding-top: 1px;
  padding-bottom: 1px;
  text-align: center;
  font-size: var(--jp-ui-font-size1);
  white-space: nowrap;
}

.jp-CodeMirror-ruler {
  border-left: 1px dashed var(--jp-border-color2);
}

/* Styles for shared cursors (remote cursor locations and selected ranges) */
.jp-CodeMirrorEditor .cm-ySelectionCaret {
  position: relative;
  border-left: 1px solid black;
  margin-left: -1px;
  margin-right: -1px;
  box-sizing: border-box;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret > .cm-ySelectionInfo {
  white-space: nowrap;
  position: absolute;
  top: -1.15em;
  padding-bottom: 0.05em;
  left: -1px;
  font-size: 0.95em;
  font-family: var(--jp-ui-font-family);
  font-weight: bold;
  line-height: normal;
  user-select: none;
  color: white;
  padding-left: 2px;
  padding-right: 2px;
  z-index: 101;
  transition: opacity 0.3s ease-in-out;
}

.jp-CodeMirrorEditor .cm-ySelectionInfo {
  transition-delay: 0.7s;
  opacity: 0;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret:hover > .cm-ySelectionInfo {
  opacity: 1;
  transition-delay: 0s;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MimeDocument {
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-filebrowser-button-height: 28px;
  --jp-private-filebrowser-button-width: 48px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FileBrowser .jp-SidePanel-content {
  display: flex;
  flex-direction: column;
}

.jp-FileBrowser-toolbar.jp-Toolbar {
  flex-wrap: wrap;
  row-gap: 12px;
  border-bottom: none;
  height: auto;
  margin: 8px 12px 0;
  box-shadow: none;
  padding: 0;
  justify-content: flex-start;
}

.jp-FileBrowser-Panel {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
}

.jp-BreadCrumbs {
  flex: 0 0 auto;
  margin: 8px 12px;
}

.jp-BreadCrumbs-item {
  margin: 0 2px;
  padding: 0 2px;
  border-radius: var(--jp-border-radius);
  cursor: pointer;
}

.jp-BreadCrumbs-item:hover {
  background-color: var(--jp-layout-color2);
}

.jp-BreadCrumbs-item:first-child {
  margin-left: 0;
}

.jp-BreadCrumbs-item.jp-mod-dropTarget {
  background-color: var(--jp-brand-color2);
  opacity: 0.7;
}

/*-----------------------------------------------------------------------------
| Buttons
|----------------------------------------------------------------------------*/

.jp-FileBrowser-toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  padding-left: 0;
  padding-right: 2px;
  align-items: center;
  height: unset;
}

.jp-FileBrowser-toolbar > .jp-Toolbar-item .jp-ToolbarButtonComponent {
  width: 40px;
}

/*-----------------------------------------------------------------------------
| Other styles
|----------------------------------------------------------------------------*/

.jp-FileDialog.jp-mod-conflict input {
  color: var(--jp-error-color1);
}

.jp-FileDialog .jp-new-name-title {
  margin-top: 12px;
}

.jp-LastModified-hidden {
  display: none;
}

.jp-FileSize-hidden {
  display: none;
}

.jp-FileBrowser .lm-AccordionPanel > h3:first-child {
  display: none;
}

/*-----------------------------------------------------------------------------
| DirListing
|----------------------------------------------------------------------------*/

.jp-DirListing {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
  outline: 0;
}

.jp-DirListing-header {
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  align-items: center;
  overflow: hidden;
  border-top: var(--jp-border-width) solid var(--jp-border-color2);
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
}

.jp-DirListing-headerItem {
  padding: 4px 12px 2px;
  font-weight: 500;
}

.jp-DirListing-headerItem:hover {
  background: var(--jp-layout-color2);
}

.jp-DirListing-headerItem.jp-id-name {
  flex: 1 0 84px;
}

.jp-DirListing-headerItem.jp-id-modified {
  flex: 0 0 112px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-DirListing-headerItem.jp-id-filesize {
  flex: 0 0 75px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-id-narrow {
  display: none;
  flex: 0 0 5px;
  padding: 4px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
  color: var(--jp-border-color2);
}

.jp-DirListing-narrow .jp-id-narrow {
  display: block;
}

.jp-DirListing-narrow .jp-id-modified,
.jp-DirListing-narrow .jp-DirListing-itemModified {
  display: none;
}

.jp-DirListing-headerItem.jp-mod-selected {
  font-weight: 600;
}

/* increase specificity to override bundled default */
.jp-DirListing-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  list-style-type: none;
  overflow: auto;
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-content mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.jp-DirListing-content .jp-DirListing-item.jp-mod-selected mark {
  color: var(--jp-ui-inverse-font-color0);
}

/* Style the directory listing content when a user drops a file to upload */
.jp-DirListing.jp-mod-native-drop .jp-DirListing-content {
  outline: 5px dashed rgba(128, 128, 128, 0.5);
  outline-offset: -10px;
  cursor: copy;
}

.jp-DirListing-item {
  display: flex;
  flex-direction: row;
  align-items: center;
  padding: 4px 12px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-DirListing-checkboxWrapper {
  /* Increases hit area of checkbox. */
  padding: 4px;
}

.jp-DirListing-header
  .jp-DirListing-checkboxWrapper
  + .jp-DirListing-headerItem {
  padding-left: 4px;
}

.jp-DirListing-content .jp-DirListing-checkboxWrapper {
  position: relative;
  left: -4px;
  margin: -4px 0 -4px -8px;
}

.jp-DirListing-checkboxWrapper.jp-mod-visible {
  visibility: visible;
}

/* For devices that support hovering, hide checkboxes until hovered, selected...
*/
@media (hover: hover) {
  .jp-DirListing-checkboxWrapper {
    visibility: hidden;
  }

  .jp-DirListing-item:hover .jp-DirListing-checkboxWrapper,
  .jp-DirListing-item.jp-mod-selected .jp-DirListing-checkboxWrapper {
    visibility: visible;
  }
}

.jp-DirListing-item[data-is-dot] {
  opacity: 75%;
}

.jp-DirListing-item.jp-mod-selected {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.jp-DirListing-item.jp-mod-dropTarget {
  background: var(--jp-brand-color3);
}

.jp-DirListing-item:hover:not(.jp-mod-selected) {
  background: var(--jp-layout-color2);
}

.jp-DirListing-itemIcon {
  flex: 0 0 20px;
  margin-right: 4px;
}

.jp-DirListing-itemText {
  flex: 1 0 64px;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  user-select: none;
}

.jp-DirListing-itemText:focus {
  outline-width: 2px;
  outline-color: var(--jp-inverse-layout-color1);
  outline-style: solid;
  outline-offset: 1px;
}

.jp-DirListing-item.jp-mod-selected .jp-DirListing-itemText:focus {
  outline-color: var(--jp-layout-color1);
}

.jp-DirListing-itemModified {
  flex: 0 0 125px;
  text-align: right;
}

.jp-DirListing-itemFileSize {
  flex: 0 0 90px;
  text-align: right;
}

.jp-DirListing-editor {
  flex: 1 0 64px;
  outline: none;
  border: none;
  color: var(--jp-ui-font-color1);
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon::before {
  color: var(--jp-success-color1);
  content: '\25CF';
  font-size: 8px;
  position: absolute;
  left: -8px;
}

.jp-DirListing-item.jp-mod-running.jp-mod-selected
  .jp-DirListing-itemIcon::before {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-DirListing-item.lm-mod-drag-image,
.jp-DirListing-item.jp-mod-selected.lm-mod-drag-image {
  font-size: var(--jp-ui-font-size1);
  padding-left: 4px;
  margin-left: 4px;
  width: 160px;
  background-color: var(--jp-ui-inverse-font-color2);
  box-shadow: var(--jp-elevation-z2);
  border-radius: 0;
  color: var(--jp-ui-font-color1);
  transform: translateX(-40%) translateY(-58%);
}

.jp-Document {
  min-width: 120px;
  min-height: 120px;
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Main OutputArea
| OutputArea has a list of Outputs
|----------------------------------------------------------------------------*/

.jp-OutputArea {
  overflow-y: auto;
}

.jp-OutputArea-child {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-OutputPrompt {
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-outprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
  opacity: var(--jp-cell-prompt-opacity);

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-OutputArea-prompt {
  display: table-cell;
  vertical-align: top;
}

.jp-OutputArea-output {
  display: table-cell;
  width: 100%;
  height: auto;
  overflow: auto;
  user-select: text;
  -moz-user-select: text;
  -webkit-user-select: text;
  -ms-user-select: text;
}

.jp-OutputArea .jp-RenderedText {
  padding-left: 1ch;
}

/**
 * Prompt overlay.
 */

.jp-OutputArea-promptOverlay {
  position: absolute;
  top: 0;
  width: var(--jp-cell-prompt-width);
  height: 100%;
  opacity: 0.5;
}

.jp-OutputArea-promptOverlay:hover {
  background: var(--jp-layout-color2);
  box-shadow: inset 0 0 1px var(--jp-inverse-layout-color0);
  cursor: zoom-out;
}

.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay:hover {
  cursor: zoom-in;
}

/**
 * Isolated output.
 */
.jp-OutputArea-output.jp-mod-isolated {
  width: 100%;
  display: block;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated {
  position: relative;
}

body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/* pre */

.jp-OutputArea-output pre {
  border: none;
  margin: 0;
  padding: 0;
  overflow-x: auto;
  overflow-y: auto;
  word-break: break-all;
  word-wrap: break-word;
  white-space: pre-wrap;
}

/* tables */

.jp-OutputArea-output.jp-RenderedHTMLCommon table {
  margin-left: 0;
  margin-right: 0;
}

/* description lists */

.jp-OutputArea-output dl,
.jp-OutputArea-output dt,
.jp-OutputArea-output dd {
  display: block;
}

.jp-OutputArea-output dl {
  width: 100%;
  overflow: hidden;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dt {
  font-weight: bold;
  float: left;
  width: 20%;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dd {
  float: left;
  width: 80%;
  padding: 0;
  margin: 0;
}

.jp-TrimmedOutputs pre {
  background: var(--jp-layout-color3);
  font-size: calc(var(--jp-code-font-size) * 1.4);
  text-align: center;
  text-transform: uppercase;
}

/* Hide the gutter in case of
 *  - nested output areas (e.g. in the case of output widgets)
 *  - mirrored output areas
 */
.jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt {
  display: none;
}

/* Hide empty lines in the output area, for instance due to cleared widgets */
.jp-OutputArea-prompt:empty {
  padding: 0;
  border: 0;
}

/*-----------------------------------------------------------------------------
| executeResult is added to any Output-result for the display of the object
| returned by a cell
|----------------------------------------------------------------------------*/

.jp-OutputArea-output.jp-OutputArea-executeResult {
  margin-left: 0;
  width: 100%;
}

/* Text output with the Out[] prompt needs a top padding to match the
 * alignment of the Out[] prompt itself.
 */
.jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output {
  padding-top: var(--jp-code-padding);
  border-top: var(--jp-border-width) solid transparent;
}

/*-----------------------------------------------------------------------------
| The Stdin output
|----------------------------------------------------------------------------*/

.jp-Stdin-prompt {
  color: var(--jp-content-font-color0);
  padding-right: var(--jp-code-padding);
  vertical-align: baseline;
  flex: 0 0 auto;
}

.jp-Stdin-input {
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  color: inherit;
  background-color: inherit;
  width: 42%;
  min-width: 200px;

  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;

  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0 0.25em;
  margin: 0 0.25em;
  flex: 0 0 70%;
}

.jp-Stdin-input::placeholder {
  opacity: 0;
}

.jp-Stdin-input:focus {
  box-shadow: none;
}

.jp-Stdin-input:focus::placeholder {
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Output Area View
|----------------------------------------------------------------------------*/

.jp-LinkedOutputView .jp-OutputArea {
  height: 100%;
  display: block;
}

.jp-LinkedOutputView .jp-OutputArea-output:only-child {
  height: 100%;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

@media print {
  .jp-OutputArea-child {
    break-inside: avoid-page;
  }
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-OutputPrompt {
    display: table-row;
    text-align: left;
  }

  .jp-OutputArea-child .jp-OutputArea-output {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }
}

/* Trimmed outputs warning */
.jp-TrimmedOutputs > a {
  margin: 10px;
  text-decoration: none;
  cursor: pointer;
}

.jp-TrimmedOutputs > a:hover {
  text-decoration: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Table of Contents
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toc-active-width: 4px;
}

.jp-TableOfContents {
  display: flex;
  flex-direction: column;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  height: 100%;
}

.jp-TableOfContents-placeholder {
  text-align: center;
}

.jp-TableOfContents-placeholderContent {
  color: var(--jp-content-font-color2);
  padding: 8px;
}

.jp-TableOfContents-placeholderContent > h3 {
  margin-bottom: var(--jp-content-heading-margin-bottom);
}

.jp-TableOfContents .jp-SidePanel-content {
  overflow-y: auto;
}

.jp-TableOfContents-tree {
  margin: 4px;
}

.jp-TableOfContents ol {
  list-style-type: none;
}

/* stylelint-disable-next-line selector-max-type */
.jp-TableOfContents li > ol {
  /* Align left border with triangle icon center */
  padding-left: 11px;
}

.jp-TableOfContents-content {
  /* left margin for the active heading indicator */
  margin: 0 0 0 var(--jp-private-toc-active-width);
  padding: 0;
  background-color: var(--jp-layout-color1);
}

.jp-tocItem {
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-tocItem-heading {
  display: flex;
  cursor: pointer;
}

.jp-tocItem-heading:hover {
  background-color: var(--jp-layout-color2);
}

.jp-tocItem-content {
  display: block;
  padding: 4px 0;
  white-space: nowrap;
  text-overflow: ellipsis;
  overflow-x: hidden;
}

.jp-tocItem-collapser {
  height: 20px;
  margin: 2px 2px 0;
  padding: 0;
  background: none;
  border: none;
  cursor: pointer;
}

.jp-tocItem-collapser:hover {
  background-color: var(--jp-layout-color3);
}

/* Active heading indicator */

.jp-tocItem-heading::before {
  content: ' ';
  background: transparent;
  width: var(--jp-private-toc-active-width);
  height: 24px;
  position: absolute;
  left: 0;
  border-radius: var(--jp-border-radius);
}

.jp-tocItem-heading.jp-tocItem-active::before {
  background-color: var(--jp-brand-color1);
}

.jp-tocItem-heading:hover.jp-tocItem-active::before {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapser {
  flex: 0 0 var(--jp-cell-collapser-width);
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
  border-radius: var(--jp-border-radius);
  opacity: 1;
}

.jp-Collapser-child {
  display: block;
  width: 100%;
  box-sizing: border-box;

  /* height: 100% doesn't work because the height of its parent is computed from content */
  position: absolute;
  top: 0;
  bottom: 0;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Hiding collapsers in print mode.

Note: input and output wrappers have "display: block" propery in print mode.
*/

@media print {
  .jp-Collapser {
    display: none;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Header/Footer
|----------------------------------------------------------------------------*/

/* Hidden by zero height by default */
.jp-CellHeader,
.jp-CellFooter {
  height: 0;
  width: 100%;
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Input
|----------------------------------------------------------------------------*/

/* All input areas */
.jp-InputArea {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-InputArea-editor {
  display: table-cell;
  overflow: hidden;
  vertical-align: top;

  /* This is the non-active, default styling */
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  background: var(--jp-cell-editor-background);
}

.jp-InputPrompt {
  display: table-cell;
  vertical-align: top;
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-inprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  opacity: var(--jp-cell-prompt-opacity);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-InputArea-editor {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }

  .jp-InputPrompt {
    display: table-row;
    text-align: left;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Placeholder {
  display: table;
  table-layout: fixed;
  width: 100%;
}

.jp-Placeholder-prompt {
  display: table-cell;
  box-sizing: border-box;
}

.jp-Placeholder-content {
  display: table-cell;
  padding: 4px 6px;
  border: 1px solid transparent;
  border-radius: 0;
  background: none;
  box-sizing: border-box;
  cursor: pointer;
}

.jp-Placeholder-contentContainer {
  display: flex;
}

.jp-Placeholder-content:hover,
.jp-InputPlaceholder > .jp-Placeholder-content:hover {
  border-color: var(--jp-layout-color3);
}

.jp-Placeholder-content .jp-MoreHorizIcon {
  width: 32px;
  height: 16px;
  border: 1px solid transparent;
  border-radius: var(--jp-border-radius);
}

.jp-Placeholder-content .jp-MoreHorizIcon:hover {
  border: 1px solid var(--jp-border-color1);
  box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.25);
  background-color: var(--jp-layout-color0);
}

.jp-PlaceholderText {
  white-space: nowrap;
  overflow-x: hidden;
  color: var(--jp-inverse-layout-color3);
  font-family: var(--jp-code-font-family);
}

.jp-InputPlaceholder > .jp-Placeholder-content {
  border-color: var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-cell-scrolling-output-offset: 5px;
}

/*-----------------------------------------------------------------------------
| Cell
|----------------------------------------------------------------------------*/

.jp-Cell {
  padding: var(--jp-cell-padding);
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Common input/output
|----------------------------------------------------------------------------*/

.jp-Cell-inputWrapper,
.jp-Cell-outputWrapper {
  display: flex;
  flex-direction: row;
  padding: 0;
  margin: 0;

  /* Added to reveal the box-shadow on the input and output collapsers. */
  overflow: visible;
}

/* Only input/output areas inside cells */
.jp-Cell-inputArea,
.jp-Cell-outputArea {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Collapser
|----------------------------------------------------------------------------*/

/* Make the output collapser disappear when there is not output, but do so
 * in a manner that leaves it in the layout and preserves its width.
 */
.jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser {
  border: none !important;
  background: transparent !important;
}

.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser {
  min-height: var(--jp-cell-collapser-min-height);
}

/*-----------------------------------------------------------------------------
| Output
|----------------------------------------------------------------------------*/

/* Put a space between input and output when there IS output */
.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper {
  margin-top: 5px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea {
  overflow-y: auto;
  max-height: 24em;
  margin-left: var(--jp-private-cell-scrolling-output-offset);
  resize: vertical;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea[style*='height'] {
  max-height: unset;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea::after {
  content: ' ';
  box-shadow: inset 0 0 6px 2px rgb(0 0 0 / 30%);
  width: 100%;
  height: 100%;
  position: sticky;
  bottom: 0;
  top: 0;
  margin-top: -50%;
  float: left;
  display: block;
  pointer-events: none;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-child {
  padding-top: 6px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt {
  width: calc(
    var(--jp-cell-prompt-width) - var(--jp-private-cell-scrolling-output-offset)
  );
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay {
  left: calc(-1 * var(--jp-private-cell-scrolling-output-offset));
}

/*-----------------------------------------------------------------------------
| CodeCell
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| MarkdownCell
|----------------------------------------------------------------------------*/

.jp-MarkdownOutput {
  display: table-cell;
  width: 100%;
  margin-top: 0;
  margin-bottom: 0;
  padding-left: var(--jp-code-padding);
}

.jp-MarkdownOutput.jp-RenderedHTMLCommon {
  overflow: auto;
}

/* collapseHeadingButton (show always if hiddenCellsButton is _not_ shown) */
.jp-collapseHeadingButton {
  display: flex;
  min-height: var(--jp-cell-collapser-min-height);
  font-size: var(--jp-code-font-size);
  position: absolute;
  background-color: transparent;
  background-size: 25px;
  background-repeat: no-repeat;
  background-position-x: center;
  background-position-y: top;
  background-image: var(--jp-icon-caret-down);
  right: 0;
  top: 0;
  bottom: 0;
}

.jp-collapseHeadingButton.jp-mod-collapsed {
  background-image: var(--jp-icon-caret-right);
}

/*
 set the container font size to match that of content
 so that the nested collapse buttons have the right size
*/
.jp-MarkdownCell .jp-InputPrompt {
  font-size: var(--jp-content-font-size1);
}

/*
  Align collapseHeadingButton with cell top header
  The font sizes are identical to the ones in packages/rendermime/style/base.css
*/
.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='1'] {
  font-size: var(--jp-content-font-size5);
  background-position-y: calc(0.3 * var(--jp-content-font-size5));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='2'] {
  font-size: var(--jp-content-font-size4);
  background-position-y: calc(0.3 * var(--jp-content-font-size4));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='3'] {
  font-size: var(--jp-content-font-size3);
  background-position-y: calc(0.3 * var(--jp-content-font-size3));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='4'] {
  font-size: var(--jp-content-font-size2);
  background-position-y: calc(0.3 * var(--jp-content-font-size2));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='5'] {
  font-size: var(--jp-content-font-size1);
  background-position-y: top;
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='6'] {
  font-size: var(--jp-content-font-size0);
  background-position-y: top;
}

/* collapseHeadingButton (show only on (hover,active) if hiddenCellsButton is shown) */
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-collapseHeadingButton {
  display: none;
}

.jp-Notebook.jp-mod-showHiddenCellsButton
  :is(.jp-MarkdownCell:hover, .jp-mod-active)
  .jp-collapseHeadingButton {
  display: flex;
}

/* showHiddenCellsButton (only show if jp-mod-showHiddenCellsButton is set, which
is a consequence of the showHiddenCellsButton option in Notebook Settings)*/
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton {
  margin-left: calc(var(--jp-cell-prompt-width) + 2 * var(--jp-code-padding));
  margin-top: var(--jp-code-padding);
  border: 1px solid var(--jp-border-color2);
  background-color: var(--jp-border-color3) !important;
  color: var(--jp-content-font-color0) !important;
  display: flex;
}

.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton:hover {
  background-color: var(--jp-border-color2) !important;
}

.jp-showHiddenCellsButton {
  display: none;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Using block instead of flex to allow the use of the break-inside CSS property for
cell outputs.
*/

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-notebook-toolbar-padding: 2px 5px 2px 2px;
}

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-NotebookPanel-toolbar {
  padding: var(--jp-notebook-toolbar-padding);

  /* disable paint containment from lumino 2.0 default strict CSS containment */
  contain: style size !important;
}

.jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused {
  border: none;
  box-shadow: none;
}

.jp-Notebook-toolbarCellTypeDropdown select {
  height: 24px;
  font-size: var(--jp-ui-font-size1);
  line-height: 14px;
  border-radius: 0;
  display: block;
}

.jp-Notebook-toolbarCellTypeDropdown span {
  top: 5px !important;
}

.jp-Toolbar-responsive-popup {
  position: absolute;
  height: fit-content;
  display: flex;
  flex-direction: row;
  flex-wrap: wrap;
  justify-content: flex-end;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: var(--jp-notebook-toolbar-padding);
  z-index: 1;
  right: 0;
  top: 0;
}

.jp-Toolbar > .jp-Toolbar-responsive-opener {
  margin-left: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-Notebook-ExecutionIndicator {
  position: relative;
  display: inline-block;
  height: 100%;
  z-index: 9997;
}

.jp-Notebook-ExecutionIndicator-tooltip {
  visibility: hidden;
  height: auto;
  width: max-content;
  width: -moz-max-content;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color1);
  text-align: justify;
  border-radius: 6px;
  padding: 0 5px;
  position: fixed;
  display: table;
}

.jp-Notebook-ExecutionIndicator-tooltip.up {
  transform: translateX(-50%) translateY(-100%) translateY(-32px);
}

.jp-Notebook-ExecutionIndicator-tooltip.down {
  transform: translateX(calc(-100% + 16px)) translateY(5px);
}

.jp-Notebook-ExecutionIndicator-tooltip.hidden {
  display: none;
}

.jp-Notebook-ExecutionIndicator:hover .jp-Notebook-ExecutionIndicator-tooltip {
  visibility: visible;
}

.jp-Notebook-ExecutionIndicator span {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  color: var(--jp-ui-font-color1);
  line-height: 24px;
  display: block;
}

.jp-Notebook-ExecutionIndicator-progress-bar {
  display: flex;
  justify-content: center;
  height: 100%;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*
 * Execution indicator
 */
.jp-tocItem-content::after {
  content: '';

  /* Must be identical to form a circle */
  width: 12px;
  height: 12px;
  background: none;
  border: none;
  position: absolute;
  right: 0;
}

.jp-tocItem-content[data-running='0']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background: none;
}

.jp-tocItem-content[data-running='1']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background-color: var(--jp-inverse-layout-color3);
}

.jp-tocItem-content[data-running='0'],
.jp-tocItem-content[data-running='1'] {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Notebook-footer {
  height: 27px;
  margin-left: calc(
    var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
      var(--jp-cell-padding)
  );
  width: calc(
    100% -
      (
        var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
          var(--jp-cell-padding) + var(--jp-cell-padding)
      )
  );
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  color: var(--jp-ui-font-color3);
  margin-top: 6px;
  background: none;
  cursor: pointer;
}

.jp-Notebook-footer:focus {
  border-color: var(--jp-cell-editor-active-border-color);
}

/* For devices that support hovering, hide footer until hover */
@media (hover: hover) {
  .jp-Notebook-footer {
    opacity: 0;
  }

  .jp-Notebook-footer:focus,
  .jp-Notebook-footer:hover {
    opacity: 1;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Imports
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-side-by-side-output-size: 1fr;
  --jp-side-by-side-resized-cell: var(--jp-side-by-side-output-size);
  --jp-private-notebook-dragImage-width: 304px;
  --jp-private-notebook-dragImage-height: 36px;
  --jp-private-notebook-selected-color: var(--md-blue-400);
  --jp-private-notebook-active-color: var(--md-green-400);
}

/*-----------------------------------------------------------------------------
| Notebook
|----------------------------------------------------------------------------*/

/* stylelint-disable selector-max-class */

.jp-NotebookPanel {
  display: block;
  height: 100%;
}

.jp-NotebookPanel.jp-Document {
  min-width: 240px;
  min-height: 120px;
}

.jp-Notebook {
  padding: var(--jp-notebook-padding);
  outline: none;
  overflow: auto;
  background: var(--jp-layout-color0);
}

.jp-Notebook.jp-mod-scrollPastEnd::after {
  display: block;
  content: '';
  min-height: var(--jp-notebook-scroll-padding);
}

.jp-MainAreaWidget-ContainStrict .jp-Notebook * {
  contain: strict;
}

.jp-Notebook .jp-Cell {
  overflow: visible;
}

.jp-Notebook .jp-Cell .jp-InputPrompt {
  cursor: move;
}

/*-----------------------------------------------------------------------------
| Notebook state related styling
|
| The notebook and cells each have states, here are the possibilities:
|
| - Notebook
|   - Command
|   - Edit
| - Cell
|   - None
|   - Active (only one can be active)
|   - Selected (the cells actions are applied to)
|   - Multiselected (when multiple selected, the cursor)
|   - No outputs
|----------------------------------------------------------------------------*/

/* Command or edit modes */

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

/* cell is active */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser {
  background: var(--jp-brand-color1);
}

/* cell is dirty */
.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt {
  color: var(--jp-warn-color1);
}

.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt::before {
  color: var(--jp-warn-color1);
  content: '•';
}

.jp-Notebook .jp-Cell.jp-mod-active.jp-mod-dirty .jp-Collapser {
  background: var(--jp-warn-color1);
}

/* collapser is hovered */
.jp-Notebook .jp-Cell .jp-Collapser:hover {
  box-shadow: var(--jp-elevation-z2);
  background: var(--jp-brand-color1);
  opacity: var(--jp-cell-collapser-not-active-hover-opacity);
}

/* cell is active and collapser is hovered */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/* Command mode */

.jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected {
  background: var(--jp-notebook-multiselected-color);
}

.jp-Notebook.jp-mod-commandMode
  .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected) {
  background: transparent;
}

/* Edit mode */

.jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-cell-editor-active-background);
}

/*-----------------------------------------------------------------------------
| Notebook drag and drop
|----------------------------------------------------------------------------*/

.jp-Notebook-cell.jp-mod-dropSource {
  opacity: 0.5;
}

.jp-Notebook-cell.jp-mod-dropTarget,
.jp-Notebook.jp-mod-commandMode
  .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget {
  border-top-color: var(--jp-private-notebook-selected-color);
  border-top-style: solid;
  border-top-width: 2px;
}

.jp-dragImage {
  display: block;
  flex-direction: row;
  width: var(--jp-private-notebook-dragImage-width);
  height: var(--jp-private-notebook-dragImage-height);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
  overflow: visible;
}

.jp-dragImage-singlePrompt {
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

.jp-dragImage .jp-dragImage-content {
  flex: 1 1 auto;
  z-index: 2;
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  line-height: var(--jp-code-line-height);
  padding: var(--jp-code-padding);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background-color);
  color: var(--jp-content-font-color3);
  text-align: left;
  margin: 4px 4px 4px 0;
}

.jp-dragImage .jp-dragImage-prompt {
  flex: 0 0 auto;
  min-width: 36px;
  color: var(--jp-cell-inprompt-font-color);
  padding: var(--jp-code-padding);
  padding-left: 12px;
  font-family: var(--jp-cell-prompt-font-family);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: 1.9;
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
}

.jp-dragImage-multipleBack {
  z-index: -1;
  position: absolute;
  height: 32px;
  width: 300px;
  top: 8px;
  left: 8px;
  background: var(--jp-layout-color2);
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

/*-----------------------------------------------------------------------------
| Cell toolbar
|----------------------------------------------------------------------------*/

.jp-NotebookTools {
  display: block;
  min-width: var(--jp-sidebar-min-width);
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
    * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  overflow: auto;
}

.jp-ActiveCellTool {
  padding: 12px 0;
  display: flex;
}

.jp-ActiveCellTool-Content {
  flex: 1 1 auto;
}

.jp-ActiveCellTool .jp-ActiveCellTool-CellContent {
  background: var(--jp-cell-editor-background);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  min-height: 29px;
}

.jp-ActiveCellTool .jp-InputPrompt {
  min-width: calc(var(--jp-cell-prompt-width) * 0.75);
}

.jp-ActiveCellTool-CellContent > pre {
  padding: 5px 4px;
  margin: 0;
  white-space: normal;
}

.jp-MetadataEditorTool {
  flex-direction: column;
  padding: 12px 0;
}

.jp-RankedPanel > :not(:first-child) {
  margin-top: 12px;
}

.jp-KeySelector select.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: var(--jp-border-width) solid var(--jp-border-color1);
}

.jp-KeySelector label,
.jp-MetadataEditorTool label,
.jp-NumberSetter label {
  line-height: 1.4;
}

.jp-NotebookTools .jp-select-wrapper {
  margin-top: 4px;
  margin-bottom: 0;
}

.jp-NumberSetter input {
  width: 100%;
  margin-top: 4px;
}

.jp-NotebookTools .jp-Collapse {
  margin-top: 16px;
}

/*-----------------------------------------------------------------------------
| Presentation Mode (.jp-mod-presentationMode)
|----------------------------------------------------------------------------*/

.jp-mod-presentationMode .jp-Notebook {
  --jp-content-font-size1: var(--jp-content-presentation-font-size1);
  --jp-code-font-size: var(--jp-code-presentation-font-size);
}

.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt,
.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt {
  flex: 0 0 110px;
}

/*-----------------------------------------------------------------------------
| Side-by-side Mode (.jp-mod-sideBySide)
|----------------------------------------------------------------------------*/
.jp-mod-sideBySide.jp-Notebook .jp-Notebook-cell {
  margin-top: 3em;
  margin-bottom: 3em;
  margin-left: 5%;
  margin-right: 5%;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell {
  display: grid;
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-output-size)
    );
  grid-template-rows: auto minmax(0, 1fr) auto;
  grid-template-areas:
    'header header header'
    'input handle output'
    'footer footer footer';
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell.jp-mod-resizedCell {
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-resized-cell)
    );
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellHeader {
  grid-area: header;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-inputWrapper {
  grid-area: input;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-outputWrapper {
  /* overwrite the default margin (no vertical separation needed in side by side move */
  margin-top: 0;
  grid-area: output;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellFooter {
  grid-area: footer;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle {
  grid-area: handle;
  user-select: none;
  display: block;
  height: 100%;
  cursor: ew-resize;
  padding: 0 var(--jp-cell-padding);
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle::after {
  content: '';
  display: block;
  background: var(--jp-border-color2);
  height: 100%;
  width: 5px;
}

.jp-mod-sideBySide.jp-Notebook
  .jp-CodeCell.jp-mod-resizedCell
  .jp-CellResizeHandle::after {
  background: var(--jp-border-color0);
}

.jp-CellResizeHandle {
  display: none;
}

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Cell-Placeholder {
  padding-left: 55px;
}

.jp-Cell-Placeholder-wrapper {
  background: #fff;
  border: 1px solid;
  border-color: #e5e6e9 #dfe0e4 #d0d1d5;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  margin: 10px 15px;
}

.jp-Cell-Placeholder-wrapper-inner {
  padding: 15px;
  position: relative;
}

.jp-Cell-Placeholder-wrapper-body {
  background-repeat: repeat;
  background-size: 50% auto;
}

.jp-Cell-Placeholder-wrapper-body div {
  background: #f6f7f8;
  background-image: -webkit-linear-gradient(
    left,
    #f6f7f8 0%,
    #edeef1 20%,
    #f6f7f8 40%,
    #f6f7f8 100%
  );
  background-repeat: no-repeat;
  background-size: 800px 104px;
  height: 104px;
  position: absolute;
  right: 15px;
  left: 15px;
  top: 15px;
}

div.jp-Cell-Placeholder-h1 {
  top: 20px;
  height: 20px;
  left: 15px;
  width: 150px;
}

div.jp-Cell-Placeholder-h2 {
  left: 15px;
  top: 50px;
  height: 10px;
  width: 100px;
}

div.jp-Cell-Placeholder-content-1,
div.jp-Cell-Placeholder-content-2,
div.jp-Cell-Placeholder-content-3 {
  left: 15px;
  right: 15px;
  height: 10px;
}

div.jp-Cell-Placeholder-content-1 {
  top: 100px;
}

div.jp-Cell-Placeholder-content-2 {
  top: 120px;
}

div.jp-Cell-Placeholder-content-3 {
  top: 140px;
}

</style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
The following CSS variables define the main, public API for styling JupyterLab.
These variables should be used by all plugins wherever possible. In other
words, plugins should not define custom colors, sizes, etc unless absolutely
necessary. This enables users to change the visual theme of JupyterLab
by changing these variables.

Many variables appear in an ordered sequence (0,1,2,3). These sequences
are designed to work well together, so for example, `--jp-border-color1` should
be used with `--jp-layout-color1`. The numbers have the following meanings:

* 0: super-primary, reserved for special emphasis
* 1: primary, most important under normal situations
* 2: secondary, next most important under normal situations
* 3: tertiary, next most important under normal situations

Throughout JupyterLab, we are mostly following principles from Google's
Material Design when selecting colors. We are not, however, following
all of MD as it is not optimized for dense, information rich UIs.
*/

:root {
  /* Elevation
   *
   * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here:
   *
   * https://github.com/material-components/material-components-web
   * https://material-components-web.appspot.com/elevation.html
   */

  --jp-shadow-base-lightness: 0;
  --jp-shadow-umbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.2
  );
  --jp-shadow-penumbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.14
  );
  --jp-shadow-ambient-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.12
  );
  --jp-elevation-z0: none;
  --jp-elevation-z1: 0 2px 1px -1px var(--jp-shadow-umbra-color),
    0 1px 1px 0 var(--jp-shadow-penumbra-color),
    0 1px 3px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z2: 0 3px 1px -2px var(--jp-shadow-umbra-color),
    0 2px 2px 0 var(--jp-shadow-penumbra-color),
    0 1px 5px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z4: 0 2px 4px -1px var(--jp-shadow-umbra-color),
    0 4px 5px 0 var(--jp-shadow-penumbra-color),
    0 1px 10px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z6: 0 3px 5px -1px var(--jp-shadow-umbra-color),
    0 6px 10px 0 var(--jp-shadow-penumbra-color),
    0 1px 18px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z8: 0 5px 5px -3px var(--jp-shadow-umbra-color),
    0 8px 10px 1px var(--jp-shadow-penumbra-color),
    0 3px 14px 2px var(--jp-shadow-ambient-color);
  --jp-elevation-z12: 0 7px 8px -4px var(--jp-shadow-umbra-color),
    0 12px 17px 2px var(--jp-shadow-penumbra-color),
    0 5px 22px 4px var(--jp-shadow-ambient-color);
  --jp-elevation-z16: 0 8px 10px -5px var(--jp-shadow-umbra-color),
    0 16px 24px 2px var(--jp-shadow-penumbra-color),
    0 6px 30px 5px var(--jp-shadow-ambient-color);
  --jp-elevation-z20: 0 10px 13px -6px var(--jp-shadow-umbra-color),
    0 20px 31px 3px var(--jp-shadow-penumbra-color),
    0 8px 38px 7px var(--jp-shadow-ambient-color);
  --jp-elevation-z24: 0 11px 15px -7px var(--jp-shadow-umbra-color),
    0 24px 38px 3px var(--jp-shadow-penumbra-color),
    0 9px 46px 8px var(--jp-shadow-ambient-color);

  /* Borders
   *
   * The following variables, specify the visual styling of borders in JupyterLab.
   */

  --jp-border-width: 1px;
  --jp-border-color0: var(--md-grey-400);
  --jp-border-color1: var(--md-grey-400);
  --jp-border-color2: var(--md-grey-300);
  --jp-border-color3: var(--md-grey-200);
  --jp-inverse-border-color: var(--md-grey-600);
  --jp-border-radius: 2px;

  /* UI Fonts
   *
   * The UI font CSS variables are used for the typography all of the JupyterLab
   * user interface elements that are not directly user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-ui-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-ui-font-scale-factor: 1.2;
  --jp-ui-font-size0: 0.83333em;
  --jp-ui-font-size1: 13px; /* Base font size */
  --jp-ui-font-size2: 1.2em;
  --jp-ui-font-size3: 1.44em;
  --jp-ui-font-family: system-ui, -apple-system, blinkmacsystemfont, 'Segoe UI',
    helvetica, arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji',
    'Segoe UI Symbol';

  /*
   * Use these font colors against the corresponding main layout colors.
   * In a light theme, these go from dark to light.
   */

  /* Defaults use Material Design specification */
  --jp-ui-font-color0: rgba(0, 0, 0, 1);
  --jp-ui-font-color1: rgba(0, 0, 0, 0.87);
  --jp-ui-font-color2: rgba(0, 0, 0, 0.54);
  --jp-ui-font-color3: rgba(0, 0, 0, 0.38);

  /*
   * Use these against the brand/accent/warn/error colors.
   * These will typically go from light to darker, in both a dark and light theme.
   */

  --jp-ui-inverse-font-color0: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color1: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color2: rgba(255, 255, 255, 0.7);
  --jp-ui-inverse-font-color3: rgba(255, 255, 255, 0.5);

  /* Content Fonts
   *
   * Content font variables are used for typography of user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-content-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-content-line-height: 1.6;
  --jp-content-font-scale-factor: 1.2;
  --jp-content-font-size0: 0.83333em;
  --jp-content-font-size1: 14px; /* Base font size */
  --jp-content-font-size2: 1.2em;
  --jp-content-font-size3: 1.44em;
  --jp-content-font-size4: 1.728em;
  --jp-content-font-size5: 2.0736em;

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-content-presentation-font-size1: 17px;
  --jp-content-heading-line-height: 1;
  --jp-content-heading-margin-top: 1.2em;
  --jp-content-heading-margin-bottom: 0.8em;
  --jp-content-heading-font-weight: 500;

  /* Defaults use Material Design specification */
  --jp-content-font-color0: rgba(0, 0, 0, 1);
  --jp-content-font-color1: rgba(0, 0, 0, 0.87);
  --jp-content-font-color2: rgba(0, 0, 0, 0.54);
  --jp-content-font-color3: rgba(0, 0, 0, 0.38);
  --jp-content-link-color: var(--md-blue-900);
  --jp-content-font-family: system-ui, -apple-system, blinkmacsystemfont,
    'Segoe UI', helvetica, arial, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol';

  /*
   * Code Fonts
   *
   * Code font variables are used for typography of code and other monospaces content.
   */

  --jp-code-font-size: 13px;
  --jp-code-line-height: 1.3077; /* 17px for 13px base */
  --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */
  --jp-code-font-family-default: menlo, consolas, 'DejaVu Sans Mono', monospace;
  --jp-code-font-family: var(--jp-code-font-family-default);

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-code-presentation-font-size: 16px;

  /* may need to tweak cursor width if you change font size */
  --jp-code-cursor-width0: 1.4px;
  --jp-code-cursor-width1: 2px;
  --jp-code-cursor-width2: 4px;

  /* Layout
   *
   * The following are the main layout colors use in JupyterLab. In a light
   * theme these would go from light to dark.
   */

  --jp-layout-color0: white;
  --jp-layout-color1: white;
  --jp-layout-color2: var(--md-grey-200);
  --jp-layout-color3: var(--md-grey-400);
  --jp-layout-color4: var(--md-grey-600);

  /* Inverse Layout
   *
   * The following are the inverse layout colors use in JupyterLab. In a light
   * theme these would go from dark to light.
   */

  --jp-inverse-layout-color0: #111;
  --jp-inverse-layout-color1: var(--md-grey-900);
  --jp-inverse-layout-color2: var(--md-grey-800);
  --jp-inverse-layout-color3: var(--md-grey-700);
  --jp-inverse-layout-color4: var(--md-grey-600);

  /* Brand/accent */

  --jp-brand-color0: var(--md-blue-900);
  --jp-brand-color1: var(--md-blue-700);
  --jp-brand-color2: var(--md-blue-300);
  --jp-brand-color3: var(--md-blue-100);
  --jp-brand-color4: var(--md-blue-50);
  --jp-accent-color0: var(--md-green-900);
  --jp-accent-color1: var(--md-green-700);
  --jp-accent-color2: var(--md-green-300);
  --jp-accent-color3: var(--md-green-100);

  /* State colors (warn, error, success, info) */

  --jp-warn-color0: var(--md-orange-900);
  --jp-warn-color1: var(--md-orange-700);
  --jp-warn-color2: var(--md-orange-300);
  --jp-warn-color3: var(--md-orange-100);
  --jp-error-color0: var(--md-red-900);
  --jp-error-color1: var(--md-red-700);
  --jp-error-color2: var(--md-red-300);
  --jp-error-color3: var(--md-red-100);
  --jp-success-color0: var(--md-green-900);
  --jp-success-color1: var(--md-green-700);
  --jp-success-color2: var(--md-green-300);
  --jp-success-color3: var(--md-green-100);
  --jp-info-color0: var(--md-cyan-900);
  --jp-info-color1: var(--md-cyan-700);
  --jp-info-color2: var(--md-cyan-300);
  --jp-info-color3: var(--md-cyan-100);

  /* Cell specific styles */

  --jp-cell-padding: 5px;
  --jp-cell-collapser-width: 8px;
  --jp-cell-collapser-min-height: 20px;
  --jp-cell-collapser-not-active-hover-opacity: 0.6;
  --jp-cell-editor-background: var(--md-grey-100);
  --jp-cell-editor-border-color: var(--md-grey-300);
  --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-cell-editor-active-background: var(--jp-layout-color0);
  --jp-cell-editor-active-border-color: var(--jp-brand-color1);
  --jp-cell-prompt-width: 64px;
  --jp-cell-prompt-font-family: var(--jp-code-font-family-default);
  --jp-cell-prompt-letter-spacing: 0;
  --jp-cell-prompt-opacity: 1;
  --jp-cell-prompt-not-active-opacity: 0.5;
  --jp-cell-prompt-not-active-font-color: var(--md-grey-700);

  /* A custom blend of MD grey and blue 600
   * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */
  --jp-cell-inprompt-font-color: #307fc1;

  /* A custom blend of MD grey and orange 600
   * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */
  --jp-cell-outprompt-font-color: #bf5b3d;

  /* Notebook specific styles */

  --jp-notebook-padding: 10px;
  --jp-notebook-select-background: var(--jp-layout-color1);
  --jp-notebook-multiselected-color: var(--md-blue-50);

  /* The scroll padding is calculated to fill enough space at the bottom of the
  notebook to show one single-line cell (with appropriate padding) at the top
  when the notebook is scrolled all the way to the bottom. We also subtract one
  pixel so that no scrollbar appears if we have just one single-line cell in the
  notebook. This padding is to enable a 'scroll past end' feature in a notebook.
  */
  --jp-notebook-scroll-padding: calc(
    100% - var(--jp-code-font-size) * var(--jp-code-line-height) -
      var(--jp-code-padding) - var(--jp-cell-padding) - 1px
  );

  /* Rendermime styles */

  --jp-rendermime-error-background: #fdd;
  --jp-rendermime-table-row-background: var(--md-grey-100);
  --jp-rendermime-table-row-hover-background: var(--md-light-blue-50);

  /* Dialog specific styles */

  --jp-dialog-background: rgba(0, 0, 0, 0.25);

  /* Console specific styles */

  --jp-console-padding: 10px;

  /* Toolbar specific styles */

  --jp-toolbar-border-color: var(--jp-border-color1);
  --jp-toolbar-micro-height: 8px;
  --jp-toolbar-background: var(--jp-layout-color1);
  --jp-toolbar-box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.24);
  --jp-toolbar-header-margin: 4px 4px 0 4px;
  --jp-toolbar-active-background: var(--md-grey-300);

  /* Statusbar specific styles */

  --jp-statusbar-height: 24px;

  /* Input field styles */

  --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-input-active-background: var(--jp-layout-color1);
  --jp-input-hover-background: var(--jp-layout-color1);
  --jp-input-background: var(--md-grey-100);
  --jp-input-border-color: var(--jp-inverse-border-color);
  --jp-input-active-border-color: var(--jp-brand-color1);
  --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3);

  /* General editor styles */

  --jp-editor-selected-background: #d9d9d9;
  --jp-editor-selected-focused-background: #d7d4f0;
  --jp-editor-cursor-color: var(--jp-ui-font-color0);

  /* Code mirror specific styles */

  --jp-mirror-editor-keyword-color: #008000;
  --jp-mirror-editor-atom-color: #88f;
  --jp-mirror-editor-number-color: #080;
  --jp-mirror-editor-def-color: #00f;
  --jp-mirror-editor-variable-color: var(--md-grey-900);
  --jp-mirror-editor-variable-2-color: rgb(0, 54, 109);
  --jp-mirror-editor-variable-3-color: #085;
  --jp-mirror-editor-punctuation-color: #05a;
  --jp-mirror-editor-property-color: #05a;
  --jp-mirror-editor-operator-color: #a2f;
  --jp-mirror-editor-comment-color: #408080;
  --jp-mirror-editor-string-color: #ba2121;
  --jp-mirror-editor-string-2-color: #708;
  --jp-mirror-editor-meta-color: #a2f;
  --jp-mirror-editor-qualifier-color: #555;
  --jp-mirror-editor-builtin-color: #008000;
  --jp-mirror-editor-bracket-color: #997;
  --jp-mirror-editor-tag-color: #170;
  --jp-mirror-editor-attribute-color: #00c;
  --jp-mirror-editor-header-color: blue;
  --jp-mirror-editor-quote-color: #090;
  --jp-mirror-editor-link-color: #00c;
  --jp-mirror-editor-error-color: #f00;
  --jp-mirror-editor-hr-color: #999;

  /*
    RTC user specific colors.
    These colors are used for the cursor, username in the editor,
    and the icon of the user.
  */

  --jp-collaborator-color1: #ffad8e;
  --jp-collaborator-color2: #dac83d;
  --jp-collaborator-color3: #72dd76;
  --jp-collaborator-color4: #00e4d0;
  --jp-collaborator-color5: #45d4ff;
  --jp-collaborator-color6: #e2b1ff;
  --jp-collaborator-color7: #ff9de6;

  /* Vega extension styles */

  --jp-vega-background: white;

  /* Sidebar-related styles */

  --jp-sidebar-min-width: 250px;

  /* Search-related styles */

  --jp-search-toggle-off-opacity: 0.5;
  --jp-search-toggle-hover-opacity: 0.8;
  --jp-search-toggle-on-opacity: 1;
  --jp-search-selected-match-background-color: rgb(245, 200, 0);
  --jp-search-selected-match-color: black;
  --jp-search-unselected-match-background-color: var(
    --jp-inverse-layout-color0
  );
  --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0);

  /* Icon colors that work well with light or dark backgrounds */
  --jp-icon-contrast-color0: var(--md-purple-600);
  --jp-icon-contrast-color1: var(--md-green-600);
  --jp-icon-contrast-color2: var(--md-pink-600);
  --jp-icon-contrast-color3: var(--md-blue-600);

  /* Button colors */
  --jp-accept-color-normal: var(--md-blue-700);
  --jp-accept-color-hover: var(--md-blue-800);
  --jp-accept-color-active: var(--md-blue-900);
  --jp-warn-color-normal: var(--md-red-700);
  --jp-warn-color-hover: var(--md-red-800);
  --jp-warn-color-active: var(--md-red-900);
  --jp-reject-color-normal: var(--md-grey-600);
  --jp-reject-color-hover: var(--md-grey-700);
  --jp-reject-color-active: var(--md-grey-800);

  /* File or activity icons and switch semantic variables */
  --jp-jupyter-icon-color: #f37626;
  --jp-notebook-icon-color: #f37626;
  --jp-json-icon-color: var(--md-orange-700);
  --jp-console-icon-background-color: var(--md-blue-700);
  --jp-console-icon-color: white;
  --jp-terminal-icon-background-color: var(--md-grey-800);
  --jp-terminal-icon-color: var(--md-grey-200);
  --jp-text-editor-icon-color: var(--md-grey-700);
  --jp-inspector-icon-color: var(--md-grey-700);
  --jp-switch-color: var(--md-grey-400);
  --jp-switch-true-position-color: var(--md-orange-900);
}
</style>
<style type="text/css">
/* Force rendering true colors when outputing to pdf */
* {
  -webkit-print-color-adjust: exact;
}

/* Misc */
a.anchor-link {
  display: none;
}

/* Input area styling */
.jp-InputArea {
  overflow: hidden;
}

.jp-InputArea-editor {
  overflow: hidden;
}

.cm-editor.cm-s-jupyter .highlight pre {
/* weird, but --jp-code-padding defined to be 5px but 4px horizontal padding is hardcoded for pre.cm-line */
  padding: var(--jp-code-padding) 4px;
  margin: 0;

  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
  color: inherit;

}

.jp-OutputArea-output pre {
  line-height: inherit;
  font-family: inherit;
}

.jp-RenderedText pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
}

/* Hiding the collapser by default */
.jp-Collapser {
  display: none;
}

@page {
    margin: 0.5in; /* Margin for each printed piece of paper */
}

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}
</style>
<!-- Load mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML-full,Safe"> </script>
<!-- MathJax configuration -->
<script type="text/x-mathjax-config">
    init_mathjax = function() {
        if (window.MathJax) {
        // MathJax loaded
            MathJax.Hub.Config({
                TeX: {
                    equationNumbers: {
                    autoNumber: "AMS",
                    useLabelIds: true
                    }
                },
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true,
                    processEnvironments: true
                },
                displayAlign: 'center',
                CommonHTML: {
                    linebreaks: {
                    automatic: true
                    }
                }
            });

            MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
        }
    }
    init_mathjax();
    </script>
<!-- End of mathjax configuration --><script type="module">
  document.addEventListener("DOMContentLoaded", async () => {
    const diagrams = document.querySelectorAll(".jp-Mermaid > pre.mermaid");
    // do not load mermaidjs if not needed
    if (!diagrams.length) {
      return;
    }
    const mermaid = (await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs")).default;
    const parser = new DOMParser();

    mermaid.initialize({
      maxTextSize: 100000,
      maxEdges: 100000,
      startOnLoad: false,
      fontFamily: window
        .getComputedStyle(document.body)
        .getPropertyValue("--jp-ui-font-family"),
      theme: document.querySelector("body[data-jp-theme-light='true']")
        ? "default"
        : "dark",
    });

    let _nextMermaidId = 0;

    function makeMermaidImage(svg) {
      const img = document.createElement("img");
      const doc = parser.parseFromString(svg, "image/svg+xml");
      const svgEl = doc.querySelector("svg");
      const { maxWidth } = svgEl?.style || {};
      const firstTitle = doc.querySelector("title");
      const firstDesc = doc.querySelector("desc");

      img.setAttribute("src", `data:image/svg+xml,${encodeURIComponent(svg)}`);
      if (maxWidth) {
        img.width = parseInt(maxWidth);
      }
      if (firstTitle) {
        img.setAttribute("alt", firstTitle.textContent);
      }
      if (firstDesc) {
        const caption = document.createElement("figcaption");
        caption.className = "sr-only";
        caption.textContent = firstDesc.textContent;
        return [img, caption];
      }
      return [img];
    }

    async function makeMermaidError(text) {
      let errorMessage = "";
      try {
        await mermaid.parse(text);
      } catch (err) {
        errorMessage = `${err}`;
      }

      const result = document.createElement("details");
      result.className = 'jp-RenderedMermaid-Details';
      const summary = document.createElement("summary");
      summary.className = 'jp-RenderedMermaid-Summary';
      const pre = document.createElement("pre");
      const code = document.createElement("code");
      code.innerText = text;
      pre.appendChild(code);
      summary.appendChild(pre);
      result.appendChild(summary);

      const warning = document.createElement("pre");
      warning.innerText = errorMessage;
      result.appendChild(warning);
      return [result];
    }

    async function renderOneMarmaid(src) {
      const id = `jp-mermaid-${_nextMermaidId++}`;
      const parent = src.parentNode;
      let raw = src.textContent.trim();
      const el = document.createElement("div");
      el.style.visibility = "hidden";
      document.body.appendChild(el);
      let results = null;
      let output = null;
      try {
        let { svg } = await mermaid.render(id, raw, el);
        svg = cleanMermaidSvg(svg);
        results = makeMermaidImage(svg);
        output = document.createElement("figure");
        results.map(output.appendChild, output);
      } catch (err) {
        parent.classList.add("jp-mod-warning");
        results = await makeMermaidError(raw);
        output = results[0];
      } finally {
        el.remove();
      }
      parent.classList.add("jp-RenderedMermaid");
      parent.appendChild(output);
    }


    /**
     * Post-process to ensure mermaid diagrams contain only valid SVG and XHTML.
     */
    function cleanMermaidSvg(svg) {
      return svg.replace(RE_VOID_ELEMENT, replaceVoidElement);
    }


    /**
     * A regular expression for all void elements, which may include attributes and
     * a slash.
     *
     * @see https://developer.mozilla.org/en-US/docs/Glossary/Void_element
     *
     * Of these, only `<br>` is generated by Mermaid in place of `\n`,
     * but _any_ "malformed" tag will break the SVG rendering entirely.
     */
    const RE_VOID_ELEMENT =
      /<\s*(area|base|br|col|embed|hr|img|input|link|meta|param|source|track|wbr)\s*([^>]*?)\s*>/gi;

    /**
     * Ensure a void element is closed with a slash, preserving any attributes.
     */
    function replaceVoidElement(match, tag, rest) {
      rest = rest.trim();
      if (!rest.endsWith('/')) {
        rest = `${rest} /`;
      }
      return `<${tag} ${rest}>`;
    }

    void Promise.all([...diagrams].map(renderOneMarmaid));
  });
</script>
<style>
  .jp-Mermaid:not(.jp-RenderedMermaid) {
    display: none;
  }

  .jp-RenderedMermaid {
    overflow: auto;
    display: flex;
  }

  .jp-RenderedMermaid.jp-mod-warning {
    width: auto;
    padding: 0.5em;
    margin-top: 0.5em;
    border: var(--jp-border-width) solid var(--jp-warn-color2);
    border-radius: var(--jp-border-radius);
    color: var(--jp-ui-font-color1);
    font-size: var(--jp-ui-font-size1);
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  .jp-RenderedMermaid figure {
    margin: 0;
    overflow: auto;
    max-width: 100%;
  }

  .jp-RenderedMermaid img {
    max-width: 100%;
  }

  .jp-RenderedMermaid-Details > pre {
    margin-top: 1em;
  }

  .jp-RenderedMermaid-Summary {
    color: var(--jp-warn-color2);
  }

  .jp-RenderedMermaid:not(.jp-mod-warning) pre {
    display: none;
  }

  .jp-RenderedMermaid-Summary > pre {
    display: inline-block;
    white-space: normal;
  }
</style>
<!-- End of mermaid configuration --></head>
<body class="jp-Notebook" data-jp-theme-light="true" data-jp-theme-name="JupyterLab Light">
<main>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=73c2d2c9-4a4b-48ca-90a3-1ccd120ca08b">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h1 id="Fine-tuning-using-Mistral-7B-v0.1">Fine tuning using Mistral 7B v0.1<a class="anchor-link" href="#Fine-tuning-using-Mistral-7B-v0.1">¶</a></h1>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=5b5bef3d-89b9-42ed-b158-a39bd61f6a31">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Base-Model-and-Quantization">Base Model and Quantization<a class="anchor-link" href="#Base-Model-and-Quantization">¶</a></h3>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=938123ed-15b0-45ee-b622-d9bbe5fe3a48">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>4-bit quantization alongside specific computational optimizations</p>
<p>load_in_4bit: This option likely enables loading and storing tensors in 4-bit precision. This can significantly reduce memory usage at the cost of precision. Enabling this suggests that you're optimizing for memory efficiency, potentially to fit larger models or datasets in memory.</p>
<p>bnb_4bit_use_double_quant: This indicates the use of a double quantization process for 4-bit representation. Double quantization might be used to improve the precision of the 4-bit quantized values, potentially mitigating some of the precision loss associated with low-bit quantization.</p>
<p>bnb_4bit_quant_type="nf4": This specifies the quantization type or algorithm used for converting tensors to 4-bit representations. "nf4" might refer to a specific quantization scheme optimized for neural network weights and activations. The exact nature of "nf4" would depend on the documentation of the BitsAndBytes library, but it suggests an approach tailored to maintain as much information as possible within the 4-bit limitation.</p>
<p>bnb_4bit_compute_dtype=torch.bfloat16: This sets the data type for computations using 4-bit quantized tensors to torch.bfloat16, which is a 16-bit floating-point representation that offers a good balance between precision and memory usage. By performing computations in bfloat16, the configuration aims to maintain computational accuracy and efficiency, particularly on hardware that supports bfloat16 operations natively.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=dfe5f014-527c-4a83-863b-4b6330c72ed5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [1]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># GPU 0: NVIDIA GeForce RTX 4090</span>
<span class="c1"># GPU 1: NVIDIA GeForce RTX 4090</span>
<span class="c1"># GPU 2: NVIDIA GeForce RTX 4090</span>
<span class="c1"># GPU 3: NVIDIA GeForce RTX 3090 Ti</span>
<span class="c1"># GPU 4: NVIDIA GeForce RTX 3090 Ti</span>
<span class="c1"># GPU 5: NVIDIA GeForce RTX 3090</span>
<span class="c1"># GPU 6: NVIDIA GeForce RTX 3090</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"CUDA_VISIBLE_DEVICES"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"2"</span>  <span class="c1"># ""makes all visible, "0" GPU 0 visible</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=53ffaa11-3936-40a0-8f2a-3d083ff2afef">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Supress-warnings">Supress warnings<a class="anchor-link" href="#Supress-warnings">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=e77f7e3d-3af3-4dc8-9571-d13398c29ee9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [2]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">UserWarning</span><span class="p">)</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">FutureWarning</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=ef535c7a-848e-4c6e-a692-208f98610d82">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Inspect-the-base-model">Inspect the base model<a class="anchor-link" href="#Inspect-the-base-model">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=8bfef1a8-c05a-4e14-af0e-358bfb04352a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [3]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">access_token</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HF_TOKEN"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=89cce100-9b2e-4675-bfda-f029e7c4056e">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [4]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">login</span>
<span class="n">login</span><span class="p">(</span><span class="n">access_token</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>VBox(children=(HTML(value='&lt;center&gt; &lt;img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=15ec782b-0776-4354-ad08-66f1e9e50187">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [3]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s2">"Mistral-7B-v0.1"</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"mistralai/"</span><span class="o">+</span><span class="n">model_name</span>

<span class="n">bnb_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span>
    <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="s2">"nf4"</span><span class="p">,</span>
    <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=1b873230-c602-4aa1-801e-a58d63de5c48">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">bnb_config</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">token</span><span class="o">=</span><span class="n">access_token</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=83f74939-3ab3-4011-90cb-8e90c22ea162">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [6]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torchinfo</span> <span class="kn">import</span> <span class="n">summary</span>
<span class="n">summary</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[6]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>================================================================================
Layer (type:depth-idx)                                  Param #
================================================================================
MistralForSequenceClassification                        --
├─MistralModel: 1-1                                     --
│    └─Embedding: 2-1                                   131,072,000
│    └─ModuleList: 2-2                                  --
│    │    └─MistralDecoderLayer: 3-1                    109,060,096
│    │    └─MistralDecoderLayer: 3-2                    109,060,096
│    │    └─MistralDecoderLayer: 3-3                    109,060,096
│    │    └─MistralDecoderLayer: 3-4                    109,060,096
│    │    └─MistralDecoderLayer: 3-5                    109,060,096
│    │    └─MistralDecoderLayer: 3-6                    109,060,096
│    │    └─MistralDecoderLayer: 3-7                    109,060,096
│    │    └─MistralDecoderLayer: 3-8                    109,060,096
│    │    └─MistralDecoderLayer: 3-9                    109,060,096
│    │    └─MistralDecoderLayer: 3-10                   109,060,096
│    │    └─MistralDecoderLayer: 3-11                   109,060,096
│    │    └─MistralDecoderLayer: 3-12                   109,060,096
│    │    └─MistralDecoderLayer: 3-13                   109,060,096
│    │    └─MistralDecoderLayer: 3-14                   109,060,096
│    │    └─MistralDecoderLayer: 3-15                   109,060,096
│    │    └─MistralDecoderLayer: 3-16                   109,060,096
│    │    └─MistralDecoderLayer: 3-17                   109,060,096
│    │    └─MistralDecoderLayer: 3-18                   109,060,096
│    │    └─MistralDecoderLayer: 3-19                   109,060,096
│    │    └─MistralDecoderLayer: 3-20                   109,060,096
│    │    └─MistralDecoderLayer: 3-21                   109,060,096
│    │    └─MistralDecoderLayer: 3-22                   109,060,096
│    │    └─MistralDecoderLayer: 3-23                   109,060,096
│    │    └─MistralDecoderLayer: 3-24                   109,060,096
│    │    └─MistralDecoderLayer: 3-25                   109,060,096
│    │    └─MistralDecoderLayer: 3-26                   109,060,096
│    │    └─MistralDecoderLayer: 3-27                   109,060,096
│    │    └─MistralDecoderLayer: 3-28                   109,060,096
│    │    └─MistralDecoderLayer: 3-29                   109,060,096
│    │    └─MistralDecoderLayer: 3-30                   109,060,096
│    │    └─MistralDecoderLayer: 3-31                   109,060,096
│    │    └─MistralDecoderLayer: 3-32                   109,060,096
│    └─MistralRMSNorm: 2-3                              4,096
├─Linear: 1-2                                           8,192
================================================================================
Total params: 3,621,007,360
Trainable params: 131,346,432
Non-trainable params: 3,489,660,928
================================================================================</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=49bbe47c-4430-4f4a-90d8-1113bd320a18">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [7]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[7]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>MistralForSequenceClassification(
  (model): MistralModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x MistralDecoderLayer(
        (self_attn): MistralSdpaAttention(
          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): MistralRotaryEmbedding()
        )
        (mlp): MistralMLP(
          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): MistralRMSNorm()
        (post_attention_layernorm): MistralRMSNorm()
      )
    )
    (norm): MistralRMSNorm()
  )
  (score): Linear(in_features=4096, out_features=2, bias=False)
)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=6627b1da-57d6-4a17-8ea3-746b5cb1f3d9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Load-the-news-dataset-from-pickle-file">Load the news dataset from pickle file<a class="anchor-link" href="#Load-the-news-dataset-from-pickle-file">¶</a></h3><p>If any of the check_files don't exist then load the pickle file</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=1cc372cc-0cae-4b49-a2d7-8e57f58244a7">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [4]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pickle</span>

<span class="n">base_path</span> <span class="o">=</span> <span class="s1">'./data/'</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">base_path</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">file_name</span> <span class="o">=</span> <span class="s1">'news_small_dataset.pkl'</span>
<span class="n">file_path</span> <span class="o">=</span> <span class="n">base_path</span><span class="o">+</span><span class="n">file_name</span>

<span class="k">def</span> <span class="nf">pickle_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">file_path</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="s1">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
        <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">file</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Dataset has been pickled to: </span><span class="si">{</span><span class="n">file_path</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">load_pickle_dataset</span><span class="p">(</span><span class="n">file_path</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="s1">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Dataset has been loaded from: </span><span class="si">{</span><span class="n">file_path</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dataset</span>

<span class="k">def</span> <span class="nf">check_files_exists</span><span class="p">(</span><span class="n">file_names</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">file_names</span><span class="p">:</span>
        <span class="n">file_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">base_path</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">file_path</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="kc">False</span>

<span class="c1"># if these files exist we do not want to load the news_dataset.pkl to tokenize and make these files</span>
<span class="n">check_files</span> <span class="o">=</span> <span class="p">[</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_train_ds.pkl'</span><span class="p">,</span> <span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_eval_ds.pkl'</span><span class="p">,</span> <span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_test_ds.pkl'</span><span class="p">]</span>

<span class="k">if</span> <span class="n">check_files_exists</span><span class="p">(</span><span class="n">check_files</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"At least one of the specified files already exists. Not loading new dataset."</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">news_split_ds</span> <span class="o">=</span> <span class="n">load_pickle_dataset</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">news_split_ds</span><span class="p">)</span>
    <span class="n">total_rows</span> <span class="o">=</span> <span class="p">(</span><span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'train'</span><span class="p">]</span><span class="o">.</span><span class="n">num_rows</span> <span class="o">+</span>
              <span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'eval'</span><span class="p">]</span><span class="o">.</span><span class="n">num_rows</span> <span class="o">+</span>
              <span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'test'</span><span class="p">]</span><span class="o">.</span><span class="n">num_rows</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Total number of rows:"</span><span class="p">,</span> <span class="n">total_rows</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Dataset loaded successfully."</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>At least one of the specified files already exists. Not loading new dataset.
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=8ad450a2-6cef-432e-9e63-7ff985b4726e">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Tokenization-of-data">Tokenization of data<a class="anchor-link" href="#Tokenization-of-data">¶</a></h3>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=186f093c-7487-44ae-ad95-9ee8d24f04c7">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>
return_tensors="pt": This argument configures the tokenizer to output PyTorch ("pt") tensors. If you're working with TensorFlow, you'd use "tf" instead, and for NumPy arrays, you could omit this argument or set return_tensors to None.
</p>
<p>
Direct Model Input: By converting the tokenized input into tensors, the output can be directly used as input to a PyTorch model, fitting seamlessly into the data processing pipeline for model training or inference.

<p>Handling of Batch Inputs: This approach also supports batch inputs. If you pass a list of texts to the tokenizer with return_tensors="pt", it will automatically pad the sequences to the maximum length in the batch, returning a tensor where the first dimension is the batch size.</p>
<p>Padding and Truncation: The padding=True and truncation=True arguments ensure that all sequences are padded to the same length (up to max_length) and are truncated if they exceed this length, which is important for processing sequences in batches.</p>
</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=88c3a38c-1d24-4a1e-8d96-1b7c2ce162c7">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [9]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">token</span><span class="o">=</span><span class="n">access_token</span><span class="p">)</span>

<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">tokenize_fn</span><span class="p">(</span><span class="n">news</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">news</span><span class="p">[</span><span class="s1">'article'</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=02efc4f0-742a-4838-887d-5556a26ae15f">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Tokenize-train,-evaluation,-and-test-datasets">Tokenize train, evaluation, and test datasets<a class="anchor-link" href="#Tokenize-train,-evaluation,-and-test-datasets">¶</a></h3><p>If any of the check files exist then don't run tokenization and save some time.
Else load the pickle files that already exist.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=0f3e3101-df30-4af2-9976-49ba0cf44d62">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [5]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">check_files_exists</span><span class="p">(</span><span class="n">check_files</span><span class="p">):</span>
    <span class="n">tokenized_train_ds</span> <span class="o">=</span> <span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'train'</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_fn</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">tokenized_eval_ds</span> <span class="o">=</span> <span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'eval'</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_fn</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">tokenized_test_ds</span> <span class="o">=</span> <span class="n">news_split_ds</span><span class="p">[</span><span class="s1">'test'</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_fn</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">tokenized_train_ds</span><span class="o">.</span><span class="n">features</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">tokenized_eval_ds</span><span class="o">.</span><span class="n">features</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">tokenized_test_ds</span><span class="o">.</span><span class="n">features</span><span class="p">)</span>
    
    <span class="n">pickle_dataset</span><span class="p">(</span><span class="n">tokenized_train_ds</span><span class="p">,</span> <span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_train_ds.pkl'</span><span class="p">)</span>
    <span class="n">pickle_dataset</span><span class="p">(</span><span class="n">tokenized_eval_ds</span><span class="p">,</span> <span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_eval_ds.pkl'</span><span class="p">)</span>
    <span class="n">pickle_dataset</span><span class="p">(</span><span class="n">tokenized_test_ds</span><span class="p">,</span> <span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_test_ds.pkl'</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Files already exist, so load datasets"</span><span class="p">)</span>
    <span class="n">tokenized_train_ds</span> <span class="o">=</span> <span class="n">load_pickle_dataset</span><span class="p">(</span><span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_train_ds.pkl'</span><span class="p">)</span>
    <span class="n">tokenized_eval_ds</span> <span class="o">=</span> <span class="n">load_pickle_dataset</span><span class="p">(</span><span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_eval_ds.pkl'</span><span class="p">)</span>
    <span class="n">tokenized_test_ds</span> <span class="o">=</span> <span class="n">load_pickle_dataset</span><span class="p">(</span><span class="n">base_path</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s1">'-small_tokenized_test_ds.pkl'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Files already exist, so load datasets
Dataset has been loaded from: ./data/Mistral-7B-v0.1-small_tokenized_train_ds.pkl
Dataset has been loaded from: ./data/Mistral-7B-v0.1-small_tokenized_eval_ds.pkl
Dataset has been loaded from: ./data/Mistral-7B-v0.1-small_tokenized_test_ds.pkl
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=307cc4fb-9766-4b0f-943b-4a1c90053e09">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Look-at-the-tokenized-data">Look at the tokenized data<a class="anchor-link" href="#Look-at-the-tokenized-data">¶</a></h3><p>Notice what the actual data looks like, and then the tokenized data which is a bunch of numbers, and then the attention mask at the end.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=44321182-e1b9-4570-bd24-7a5cf42ba504">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [11]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">count_train_records</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenized_train_ds</span><span class="p">)</span>
<span class="n">count_eval_records</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenized_eval_ds</span><span class="p">)</span>
<span class="n">count_test_records</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenized_test_ds</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of records in training dataset: </span><span class="si">{</span><span class="n">count_train_records</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of records in evaluation dataset: </span><span class="si">{</span><span class="n">count_eval_records</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of records in test dataset: </span><span class="si">{</span><span class="n">count_test_records</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">count_total_records</span> <span class="o">=</span> <span class="n">count_train_records</span> <span class="o">+</span> <span class="n">count_eval_records</span> <span class="o">+</span> <span class="n">count_test_records</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Total number of records: </span><span class="si">{</span><span class="n">count_total_records</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Number of records in training dataset: 33611
Number of records in evaluation dataset: 7203
Number of records in test dataset: 7203
Total number of records: 48017
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=64be678b-d2a8-403e-bcc8-ef9e7eabb0cf">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [12]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">first_record</span> <span class="o">=</span> <span class="n">tokenized_train_ds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">first_record</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>{'article': "In a year where homicides, rapes and robberies increased slightly, New York City still saw serious crime drop 1.7 percent in 2015, continuing an overall decline that began in the 1990s, NYPD Commissioner William Bratton said Monday.\nAt a news conference with Mayor Bill de Blasio, Bratton touted last year’s crime statistics, which he said, when combined with an even larger decline in 2014, put to rest the fear that substantial decreases couldn’t continue under the new administration at City Hall.\n“While we have had some fluctuation, some increases in certain categories, the overall trend in all our crime categories continues to go down,” Bratton told reporters. “It was a very good year for us, 2015.\nHomicides increased by 4.5 percent in 2015, rising to 350 from 333 in the prior year, which was the lowest since 1994, said Deputy Commissioner Dermot Shea. Rapes increased 6 percent and robberies rose 2 percent, said Shea, who is in charge of data collection and operations for the NYPD.\nThe lower overall crime statistics came about due to what Shea called “targeted enforcement,” where cops make quality arrests even though the overall number of apprehensions was the lowest in the city since 2003.\nTwo boroughs — Manhattan and the Bronx — actually saw serious crimes increase by 3 percent and 4 percent, respectively, Shea said. Manhattan’s increase was driven by more robberies, while the Bronx, although seeing an overall crime increase, had what he said was a “phenomenal” reduction in shootings. Citywide, shootings were down in 2015 about 3 percent, to 1,103 from 1,172 in 2014.\nShea largely attributed the 2015 increase in rapes to victims coming forward with complaints about attacks from years past.\nSign up to get the latest updates Get Newsday's Breaking News alerts in your inbox. By clicking Sign up, you agree to our privacy policy.\n“Twenty percent of these rapes didn’t happen in 2015,” he said.\nThe NYPD has seen an increase in rapes involving single women who, after a night of drinking, get into cabs of all kinds and are attacked, Shea said.\n“They get driven, and passing out and waking up in a desolate area, and they get sexually attacked. This is something, really, that people need to be exceptionally aware of, and like any case in New York City, the buddy system works,” said Shea, referring to the need for people to travel in pairs when taking a cab at night.\nBratton and police brass hope to build upon the continuing drop in overall crime by using technology such as ShotSpotter and a newly minted GPS system for police cars.\nJessica Tisch, NYPD deputy commissioner for technology, said ShotSpotter, an acoustical system that detects gunfire, identified gunshots in 1,672 cases, mostly in Brooklyn. Of those alerts, 74 percent didn’t have any 911 calls from the public associated with them.\nTisch said ShotSpotter helped police recover ballistic evidence in 19 percent of the gunfire alerts. In 22 percent of those cases, Tisch said, cops were able to make positive matches of bullets with those from guns used in earlier shootings.\nTisch also highlighted a special GPS system being tried in about 5,000 patrol cars that allows the NYPD to see where its vehicles are and to track their movements over a 24-hour period, as well as gather information about the officers’ driving.\n", 'label': 0, 'input_ids': [1, 560, 264, 879, 970, 3153, 294, 1926, 28725, 5017, 274, 304, 7006, 537, 497, 7483, 7191, 28725, 1450, 2726, 3805, 1309, 2672, 4592, 9311, 6088, 28705, 28740, 28723, 28787, 4153, 297, 28705, 28750, 28734, 28740, 28782, 28725, 15317, 396, 7544, 17468, 369, 3125, 297, 272, 28705, 28740, 28774, 28774, 28734, 28713, 28725, 11800, 8268, 9238, 263, 4246, 1896, 1061, 266, 773, 9262, 28723, 13, 3167, 264, 4231, 9887, 395, 20671, 6502, 340, 2025, 293, 691, 28725, 1896, 1061, 266, 8466, 286, 1432, 879, 28809, 28713, 9311, 13110, 28725, 690, 400, 773, 28725, 739, 9837, 395, 396, 1019, 6084, 17468, 297, 28705, 28750, 28734, 28740, 28781, 28725, 1658, 298, 1846, 272, 4813, 369, 15045, 8512, 2018, 3481, 28809, 28707, 3688, 916, 272, 633, 10298, 438, 3805, 6756, 28723, 13, 28835, 23475, 478, 506, 553, 741, 19363, 10223, 28725, 741, 12095, 297, 2552, 13187, 28725, 272, 7544, 9156, 297, 544, 813, 9311, 13187, 10352, 298, 576, 1060, 3372, 1896, 1061, 266, 2240, 25875, 28723, 981, 1313, 403, 264, 1215, 1179, 879, 354, 592, 28725, 28705, 28750, 28734, 28740, 28782, 28723, 13, 28769, 7412, 1926, 7483, 486, 28705, 28781, 28723, 28782, 4153, 297, 28705, 28750, 28734, 28740, 28782, 28725, 11862, 298, 28705, 28770, 28782, 28734, 477, 28705, 28770, 28770, 28770, 297, 272, 4681, 879, 28725, 690, 403, 272, 15341, 1854, 28705, 28740, 28774, 28774, 28781, 28725, 773, 27694, 9238, 263, 384, 858, 322, 985, 28708, 28723, 399, 7722, 7483, 28705, 28784, 4153, 304, 7006, 537, 497, 8536, 28705, 28750, 4153, 28725, 773, 985, 28708, 28725, 693, 349, 297, 5685, 302, 1178, 5442, 304, 6933, 354, 272, 11800, 8268, 28723, 13, 1014, 3889, 7544, 9311, 13110, 1988, 684, 2940, 298, 767, 985, 28708, 1987, 981, 3731, 286, 19046, 3372, 970, 19407, 1038, 4045, 8431, 28713, 1019, 2070, 272, 7544, 1474, 302, 954, 267, 7533, 594, 403, 272, 15341, 297, 272, 2990, 1854, 28705, 28750, 28734, 28734, 28770, 28723, 13, 13849, 287, 11376, 28713, 1040, 21638, 304, 272, 8303, 28744, 1040, 2590, 2672, 4592, 17440, 5247, 486, 28705, 28770, 4153, 304, 28705, 28781, 4153, 28725, 8628, 28725, 985, 28708, 773, 28723, 21638, 28809, 28713, 5247, 403, 12215, 486, 680, 7006, 537, 497, 28725, 1312, 272, 8303, 28744, 28725, 5432, 6252, 396, 7544, 9311, 5247, 28725, 553, 767, 400, 773, 403, 264, 981, 28720, 540, 4361, 282, 28838, 13388, 297, 6041, 742, 28723, 3805, 5734, 28725, 6041, 742, 654, 1060, 297, 28705, 28750, 28734, 28740, 28782, 684, 28705, 28770, 4153, 28725, 298, 28705, 28740, 28725, 28740, 28734, 28770, 477, 28705, 28740, 28725, 28740, 28787, 28750, 297, 28705, 28750, 28734, 28740, 28781, 28723, 13, 4853, 28708, 12282, 26133, 272, 28705, 28750, 28734, 28740, 28782, 5247, 297, 5017, 274, 298, 13980, 3524, 3814, 395, 23430, 684, 10813, 477, 1267, 2609, 28723, 13, 7384, 582, 298, 625, 272, 7345, 11756, 2483, 7615, 1466, 28742, 28713, 15102, 288, 7615, 389, 9916, 297, 574, 297, 2858, 28723, 2463, 24675, 9315, 582, 28725, 368, 5861, 298, 813, 12917, 4920, 28723, 13, 28835, 21173, 3743, 4153, 302, 1167, 5017, 274, 1539, 28809, 28707, 4804, 297, 28705, 28750, 28734, 28740, 28782, 3372, 400, 773, 28723], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=b2d7f4f8-9eb3-4feb-86e7-fb0dad88753f">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Turn-on-accelerate">Turn on accelerate<a class="anchor-link" href="#Turn-on-accelerate">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=f3eab29e-fa47-4a13-abc4-d6425ae741b6">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [6]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">accelerate</span> <span class="kn">import</span> <span class="n">FullyShardedDataParallelPlugin</span><span class="p">,</span> <span class="n">Accelerator</span>
<span class="kn">from</span> <span class="nn">torch.distributed.fsdp.fully_sharded_data_parallel</span> <span class="kn">import</span> <span class="n">FullOptimStateDictConfig</span><span class="p">,</span> <span class="n">FullStateDictConfig</span>

<span class="n">fsdp_plugin</span> <span class="o">=</span> <span class="n">FullyShardedDataParallelPlugin</span><span class="p">(</span>
    <span class="n">state_dict_config</span><span class="o">=</span><span class="n">FullStateDictConfig</span><span class="p">(</span><span class="n">offload_to_cpu</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rank0_only</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="n">optim_state_dict_config</span><span class="o">=</span><span class="n">FullOptimStateDictConfig</span><span class="p">(</span><span class="n">offload_to_cpu</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rank0_only</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
<span class="p">)</span>

<span class="n">accelerator</span> <span class="o">=</span> <span class="n">Accelerator</span><span class="p">(</span><span class="n">fsdp_plugin</span><span class="o">=</span><span class="n">fsdp_plugin</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=22e040f1-e596-476d-9f26-ffa6f8a4a548">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="LoRA---Low-Rank-Adaptation">LoRA - Low-Rank Adaptation<a class="anchor-link" href="#LoRA---Low-Rank-Adaptation">¶</a></h3>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=bc8b8091-6179-4df0-b5bd-b29ec2dde4d5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>LoRA, short for Low-Rank Adaptation, is a technique designed to efficiently fine-tune large pre-trained models without the need to update all the model parameters, significantly reducing the computational and memory overhead typically associated with training. LoRA targets the challenge of adapting massive models, particularly in natural language processing (NLP) and computer vision, to specialized tasks while keeping the resource requirements manageable.</p>
<p>
Gradient checkpointing for the model. Gradient checkpointing is a technique used to reduce memory usage during the training of deep neural networks by trading compute for memory. It works by storing a minimal set of intermediate activations during the forward pass and then recomputing the others during the backward pass. This is particularly useful for training large models or using larger batch sizes.
</p>
<p>
The forward pass is the process where the input data is passed through the network from the input layer to the output layer. During this pass, the network performs a series of computations at each layer, applying weights to the inputs, adding biases (if applicable), and passing the result through an activation function. The final output of the forward pass is the prediction made by the network. The main goal of the forward pass is to compute the output given the current state of the model's parameters (weights and biases). This output is then used to calculate the loss, which quantifies how well the model's predictions match the actual labels.
</p>
<p>
The backward pass, or backpropagation, is the process of computing the gradient of the loss function with respect to each parameter in the network. This involves applying the chain rule of calculus to take derivatives step-by-step from the output layer back to the input layer. Essentially, it calculates how much each parameter contributed to the error in the prediction. The purpose of the backward pass is to update the model's parameters in a way that minimally reduces the loss, improving the model's predictions. The gradients calculated during this pass indicate the direction in which each parameter should be adjusted to decrease the error. Using an optimization algorithm (e.g., Stochastic Gradient Descent), these gradients are then used to update the weights to minimize the loss.
</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=41956597-0441-4bc6-aefe-fc4b0d5349c5">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>
r: This parameter specifies the rank of the low-rank matrices that are introduced by LoRA. A smaller rank means fewer parameters to train, leading to a more memory-efficient fine-tuning process.
</p>
<p>
lora_alpha: This multiplier adjusts the scale of the LoRA parameters. A higher value increases the capacity of the LoRA adjustments to the original model weights.
</p>
<p>
target_modules: Lists the specific parts of the model to which LoRA will be applied. These typically correspond to components within transformer blocks, such as the query, key, value, and output projections in attention mechanisms, as well as any additional modules relevant to the model architecture.
</p>
<p>
bias: Specifies how biases are treated in the adaptation process. In this case, biases are not adjusted ("none").
</p>
<p>
lora_dropout: Sets the dropout rate for the LoRA parameters, helping to prevent overfitting during fine-tuning. The dropout rate is a hyperparameter used in the training of neural networks, representing the probability that a given neuron (or unit) is temporarily "dropped" from the network during a specific iteration of training. This means that the neuron will not participate in the forward pass and its contribution to the backward pass (gradient computation) is also ignored during that iteration. Dropout is applied randomly to a subset of neurons in the network at each training step.
</p>
<p>
task_type: Indicates the type of task for which the model is being fine-tuned. The example uses TaskType.SEQ_CLS, 
suggesting a sequence classification task, such as sentiment analysis or document classification. In my case a binary classification of machine versus human generated text.
</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=0532bdc6-4317-474b-859c-4e5d2fe6f1bd">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [14]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">prepare_model_for_kbit_training</span><span class="p">,</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">get_peft_model</span><span class="p">,</span> <span class="n">TaskType</span>

<span class="n">model</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">prepare_model_for_kbit_training</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span>
    <span class="n">r</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span>
        <span class="s2">"q_proj"</span><span class="p">,</span>
        <span class="s2">"k_proj"</span><span class="p">,</span>
        <span class="s2">"v_proj"</span><span class="p">,</span>
        <span class="s2">"o_proj"</span><span class="p">,</span>
        <span class="s2">"gate_proj"</span><span class="p">,</span>
        <span class="s2">"up_proj"</span><span class="p">,</span>
        <span class="s2">"down_proj"</span><span class="p">,</span>
        <span class="s2">"lm_head"</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="n">bias</span><span class="o">=</span><span class="s2">"none"</span><span class="p">,</span>
    <span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
    <span class="n">task_type</span><span class="o">=</span><span class="n">TaskType</span><span class="o">.</span><span class="n">SEQ_CLS</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">accelerator</span><span class="o">.</span><span class="n">prepare_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=1cd29494-4453-4c01-b878-ef2f4533d34d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Inspect-the-model">Inspect the model<a class="anchor-link" href="#Inspect-the-model">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=d5058f6d-185d-45af-83b9-bb7f61d4bee3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [15]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">print_trainable_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">trainable_params</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">all_param</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
        <span class="n">all_param</span> <span class="o">+=</span> <span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="n">trainable_params</span> <span class="o">+=</span> <span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"trainable params: </span><span class="si">{</span><span class="n">trainable_params</span><span class="si">}</span><span class="s2"> || all params: </span><span class="si">{</span><span class="n">all_param</span><span class="si">}</span><span class="s2"> || trainable%: </span><span class="si">{</span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">trainable_params</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">all_param</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=b14fe8f2-eccd-4cfe-9d20-ef48bc178842">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [16]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">print_trainable_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">model</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>trainable params: 20979712 || all params: 3641987072 || trainable%: 0.5760512485421585
</pre>
</div>
</div>
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[16]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>PeftModelForSequenceClassification(
  (base_model): LoraModel(
    (model): MistralForSequenceClassification(
      (model): MistralModel(
        (embed_tokens): Embedding(32000, 4096)
        (layers): ModuleList(
          (0-31): 32 x MistralDecoderLayer(
            (self_attn): MistralSdpaAttention(
              (q_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (v_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): MistralRotaryEmbedding()
            )
            (mlp): MistralMLP(
              (gate_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=14336, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (up_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=14336, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (down_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=14336, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): MistralRMSNorm()
            (post_attention_layernorm): MistralRMSNorm()
          )
        )
        (norm): MistralRMSNorm()
      )
      (score): ModulesToSaveWrapper(
        (original_module): Linear(in_features=4096, out_features=2, bias=False)
        (modules_to_save): ModuleDict(
          (default): Linear(in_features=4096, out_features=2, bias=False)
        )
      )
    )
  )
)</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=e17b4782-03f8-4335-bfda-2a65794bff2c">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Look-at-hardware">Look at hardware<a class="anchor-link" href="#Look-at-hardware">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=c27c3328-1926-4adc-8696-b3b343ec4afd">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [17]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Available GPUs: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"GPU </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_name</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Available GPUs: 1
GPU 0: NVIDIA GeForce RTX 4090
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=bdf68d0e-8786-489d-a4b8-b7478513efea">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [18]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">!</span>nvidia-smi
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Thu May 23 04:44:22 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 3090        Off |   00000000:01:00.0 Off |                  N/A |
| 83%   83C    P2            391W /  420W |    2430MiB /  24576MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA GeForce RTX 4090        Off |   00000000:02:00.0 Off |                  Off |
| 31%   56C    P2            346W /  450W |   10353MiB /  24564MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA GeForce RTX 3090 Ti     Off |   00000000:2A:00.0 Off |                  Off |
| 94%   89C    P2            401W /  450W |    1698MiB /  24564MiB |     96%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA GeForce RTX 3090        Off |   00000000:41:00.0 Off |                  N/A |
|  0%   83C    P2            391W /  420W |    3462MiB /  24576MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA GeForce RTX 4090        Off |   00000000:42:00.0 Off |                  Off |
| 35%   68C    P2            337W /  450W |   15979MiB /  24564MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA GeForce RTX 4090        Off |   00000000:61:00.0 Off |                  Off |
|  0%   52C    P2             33W /  450W |    5520MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA GeForce RTX 3090 Ti     Off |   00000000:62:00.0 Off |                  Off |
| 77%   75C    P2            396W /  450W |    4790MiB /  24564MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A      2561      G   /usr/bin/gnome-shell                            4MiB |
|    0   N/A  N/A     12702      C   /usr/bin/python3                             2416MiB |
|    1   N/A  N/A      2561      G   /usr/bin/gnome-shell                           95MiB |
|    1   N/A  N/A     13002      C   /usr/bin/python3                            10248MiB |
|    2   N/A  N/A      2561      G   /usr/bin/gnome-shell                            4MiB |
|    2   N/A  N/A     12510      C   /usr/bin/python3                             1684MiB |
|    3   N/A  N/A      2561      G   /usr/bin/gnome-shell                            4MiB |
|    3   N/A  N/A     12816      C   /usr/bin/python3                             3448MiB |
|    4   N/A  N/A      2561      G   /usr/bin/gnome-shell                            6MiB |
|    4   N/A  N/A     15333      C   /usr/bin/python3                            15834MiB |
|    5   N/A  N/A      2561      G   /usr/bin/gnome-shell                            6MiB |
|    5   N/A  N/A     16085      C   /usr/bin/python3                              386MiB |
|    5   N/A  N/A     16917      C   /usr/bin/python3                             5114MiB |
|    6   N/A  N/A      2561      G   /usr/bin/gnome-shell                            4MiB |
|    6   N/A  N/A     12625      C   /usr/bin/python3                             4776MiB |
+-----------------------------------------------------------------------------------------+
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=2340f2d1-1cc7-454e-bad6-bf4ac2c46fc9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [7]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">model</span><span class="o">.</span><span class="n">is_parallelizable</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">model</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=1199841a-6519-4422-8259-2fdbb114e466">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [8]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span><span class="p">,</span> <span class="n">accuracy_score</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">logits_and_labels</span><span class="p">):</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">logits_and_labels</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
    <span class="n">f1</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">'macro'</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">'accuracy'</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span> <span class="s1">'f1'</span><span class="p">:</span> <span class="n">f1</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=b8c7f3ee-6997-4ed7-b28e-5d574831fb08">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [9]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">project_name</span> <span class="o">=</span> <span class="s2">"praxis-"</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s2">"-small-finetune"</span>
<span class="n">output_dir_path</span> <span class="o">=</span> <span class="s2">"./"</span> <span class="o">+</span> <span class="n">project_name</span>
<span class="n">output_dir_path</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[9]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>'./praxis-Mistral-7B-v0.1-small-finetune'</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=e7a69bde-1d50-46a0-9838-00812afbdb16">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [22]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=ca48a7aa-41d7-4c57-bfec-093d22bf4b54">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p><strong>output_dir</strong> (<code>output_dir_path</code>): This specifies the directory where outputs such as model checkpoints and logs will be saved. It's important for organizing the outputs of your training sessions.</p>
<p><strong>warmup_steps</strong> (<code>5</code>): This parameter sets the number of steps during which the learning rate will gradually increase from zero to the initially set learning rate. This warmup phase helps stabilize the model's training early on, preventing the model from diverging due to high gradient values at the start.</p>
<p><strong>per_device_train_batch_size</strong> (<code>32</code>): This sets the number of training examples to process on each device (like a GPU) during training. A higher batch size can speed up training but may require more memory.</p>
<p><strong>per_device_eval_batch_size</strong> (<code>32</code>): Similar to the training batch size, this is the number of examples to process on each device during evaluation. It determines how quickly the model can process the evaluation data.</p>
<p><strong>num_train_epochs</strong> (<code>10</code>): This defines the total number of times the training process should iterate over the entire dataset. More epochs can lead to better learning but also risk overfitting if too high.</p>
<p><strong>gradient_checkpointing</strong> (<code>False</code>): If set to <code>True</code>, this would enable gradient checkpointing to reduce memory usage at the cost of longer training time. It's useful for very deep models that otherwise would not fit into GPU memory.</p>
<p><strong>gradient_accumulation_steps</strong> (<code>2</code>): This setting allows you to accumulate gradients over multiple steps before performing an update on the model's weights. It's a way to effectively increase the batch size without increasing the memory load, which can be helpful when dealing with hardware constraints.</p>
<p><strong>max_steps</strong> (<code>500</code>): This is the maximum number of training steps to execute, regardless of how many epochs are set. Training will stop when this number of steps is reached.</p>
<p><strong>learning_rate</strong> (<code>2.5e-5</code>): This is the step size at which the optimizer updates the model’s weights. A smaller learning rate might lead to better fine-tuning but slower convergence, and vice versa.</p>
<p><strong>logging_steps</strong> (<code>200</code>): Specifies how often to log training information. The setting determines after how many steps new logs should be created, which might include loss and other metrics. More frequent logging provides finer-grained visibility into the training progress but can add computational overhead.</p>
<p><strong>bf16</strong> (<code>True</code>): This would enable training using bfloat16 precision, which is a mixed precision format with fewer bits than the standard single-precision floating point (fp32). Like fp16, it can reduce memory usage and potentially speed up training if supported by the hardware. It's particularly useful on TPUs and newer GPUs that support this format. (4090)</p>
<p><strong>fp16</strong> (<code>True</code>): This enables half-precision floating point (16-bit) training. It reduces memory usage and can speed up training, provided the hardware (like modern GPUs) supports it. (3090 and 4090)</p>
<p><strong>optim</strong> (<code>"paged_adamw_8bit"</code>): Specifies the optimizer to use. "paged_adamw_8bit" might refer to a variation of the AdamW optimizer that is optimized for lower precision and memory bandwidth, enhancing training speed and efficiency.</p>
<p><strong>logging_dir</strong> (<code>"./logs"</code>): This specifies the directory where training logs should be saved. It's used to store logs if you are using a logging framework or callback that writes out logs to files. Organizing logs in a specific directory is helpful for post-training analysis and for monitoring the training process through tools like TensorBoard.</p>
<p><strong>save_strategy</strong> (<code>"epoch"</code>): This determines how often to save model checkpoints. Setting it to <code>"epoch"</code> means the model will save checkpoints at the end of each epoch.</p>
<p><strong>save_steps</strong> (<code>50</code>): This is closely related to the <code>save_strategy</code> when set to "steps". It defines how often to save the model, specifically after how many training steps. A lower number means more frequent saves, which increases disk I/O but provides more restore points for training.</p>
<p><strong>evaluation_strategy</strong> (<code>"epoch"</code>): This configures when the model should be evaluated against the evaluation dataset. Like <code>save_strategy</code>, setting this to <code>"epoch"</code> triggers evaluations at the end of each epoch, providing feedback on model performance after it has seen the entire training dataset.</p>
<p><strong>eval_steps</strong> (<code>50</code>): This parameter determines how often to evaluate the model if the <code>evaluation_strategy</code> is set to "steps". Similar to <code>logging_steps</code>, setting this affects how frequently the model's performance is assessed on the evaluation dataset during the training process. More frequent evaluations provide a closer look at the model's performance but at the cost of increased computational overhead.</p>
<p><strong>do_eval</strong> (<code>True</code>): This flag enables the evaluation of the model on the evaluation dataset. If <code>True</code>, it will use the evaluation dataset to assess model performance based on metrics</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=d8f3cd41-fd17-4fd8-83b8-54f464df79ff">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Training">Training<a class="anchor-link" href="#Training">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=2df4e2a5-2a08-434b-983a-f6cd42f33da3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [23]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">transformers</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="n">transformers</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">777</span><span class="p">)</span>

<span class="k">if</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_train_ds</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">tokenized_eval_ds</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">transformers</span><span class="o">.</span><span class="n">TrainingArguments</span><span class="p">(</span>
        <span class="n">output_dir</span><span class="o">=</span><span class="n">output_dir_path</span><span class="p">,</span>
        <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">logging_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">logging_dir</span><span class="o">=</span><span class="n">output_dir_path</span><span class="o">+</span><span class="s2">"/logs"</span><span class="p">,</span>
        <span class="n">evaluation_strategy</span><span class="o">=</span><span class="s1">'epoch'</span><span class="p">,</span>
        <span class="n">save_strategy</span><span class="o">=</span><span class="s1">'epoch'</span><span class="p">,</span>
        <span class="n">bf16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">optim</span><span class="o">=</span><span class="s2">"paged_adamw_8bit"</span><span class="p">,</span>
        <span class="n">do_eval</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>
    <span class="n">data_collator</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">DataCollatorWithPadding</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">),</span>
<span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">resume_from_checkpoint</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># Turn to True if power goes out...</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>2024-05-23 04:44:23.330661: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-23 04:44:24.062880: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
<span class="ansi-blue-intense-fg ansi-bold">wandb</span>: Currently logged in as: <span class="ansi-yellow-fg">nispoe</span>. Use <span class="ansi-bold">`wandb login --relogin`</span> to force relogin
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
Tracking run with wandb version 0.17.0
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
Run data is saved locally in <code>/home/nispoe/kuk/Praxis/wandb/run-20240523_044425-rbaznync</code>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
Syncing run <strong><a href="https://wandb.ai/nispoe/huggingface/runs/rbaznync" target="_blank">driven-shadow-334</a></strong> to <a href="https://wandb.ai/nispoe/huggingface" target="_blank">Weights &amp; Biases</a> (<a href="https://wandb.me/run" target="_blank">docs</a>)<br/>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
 View project at <a href="https://wandb.ai/nispoe/huggingface" target="_blank">https://wandb.ai/nispoe/huggingface</a>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
 View run at <a href="https://wandb.ai/nispoe/huggingface/runs/rbaznync" target="_blank">https://wandb.ai/nispoe/huggingface/runs/rbaznync</a>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output" data-mime-type="text/html" tabindex="0">
<div>
<progress max="42020" style="width:300px; height:20px; vertical-align: middle;" value="42020"></progress>
      [42020/42020 34:26:46, Epoch 10/10]
    </div>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: left;">
<th>Epoch</th>
<th>Training Loss</th>
<th>Validation Loss</th>
<th>Accuracy</th>
<th>F1</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0.255700</td>
<td>0.051499</td>
<td>0.990976</td>
<td>0.990459</td>
</tr>
<tr>
<td>2</td>
<td>0.009700</td>
<td>0.044348</td>
<td>0.993336</td>
<td>0.992976</td>
</tr>
<tr>
<td>3</td>
<td>0.000000</td>
<td>0.048642</td>
<td>0.993753</td>
<td>0.993403</td>
</tr>
<tr>
<td>4</td>
<td>0.000000</td>
<td>0.048971</td>
<td>0.994447</td>
<td>0.994128</td>
</tr>
<tr>
<td>5</td>
<td>0.000000</td>
<td>0.035751</td>
<td>0.994308</td>
<td>0.993994</td>
</tr>
<tr>
<td>6</td>
<td>0.000000</td>
<td>0.060425</td>
<td>0.994308</td>
<td>0.993977</td>
</tr>
<tr>
<td>7</td>
<td>0.000000</td>
<td>0.035533</td>
<td>0.996113</td>
<td>0.995893</td>
</tr>
<tr>
<td>8</td>
<td>0.000000</td>
<td>0.038748</td>
<td>0.995419</td>
<td>0.995159</td>
</tr>
<tr>
<td>9</td>
<td>0.000000</td>
<td>0.038357</td>
<td>0.996390</td>
<td>0.996187</td>
</tr>
<tr>
<td>10</td>
<td>0.000000</td>
<td>0.038947</td>
<td>0.996390</td>
<td>0.996187</td>
</tr>
</tbody>
</table><p>
</p></div>
</div>
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[23]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>TrainOutput(global_step=42020, training_loss=0.027812564423845325, metrics={'train_runtime': 124010.886, 'train_samples_per_second': 2.71, 'train_steps_per_second': 0.339, 'total_flos': 7.22830417723392e+18, 'train_loss': 0.027812564423845325, 'epoch': 10.0})</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=0d13982d-53cb-4c88-bd32-a98d4a5a9874">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Determine-best-checkpoint">Determine best checkpoint<a class="anchor-link" href="#Determine-best-checkpoint">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=8bd570c7-8feb-4b69-bda7-9c678bfd92c3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [24]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">!</span>ls<span class="w"> </span>-ltr<span class="w"> </span><span class="o">{</span>output_dir_path<span class="o">}</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>total 44
drwxr-xr-x 2 nispoe nispoe 4096 May 23 04:44 logs
drwxrwxr-x 2 nispoe nispoe 4096 May 23 08:11 checkpoint-4202
drwxrwxr-x 2 nispoe nispoe 4096 May 23 11:37 checkpoint-8404
drwxrwxr-x 2 nispoe nispoe 4096 May 23 15:04 checkpoint-12606
drwxrwxr-x 2 nispoe nispoe 4096 May 23 18:31 checkpoint-16808
drwxrwxr-x 2 nispoe nispoe 4096 May 23 21:58 checkpoint-21010
drwxrwxr-x 2 nispoe nispoe 4096 May 24 01:24 checkpoint-25212
drwxrwxr-x 2 nispoe nispoe 4096 May 24 04:51 checkpoint-29414
drwxrwxr-x 2 nispoe nispoe 4096 May 24 08:17 checkpoint-33616
drwxrwxr-x 2 nispoe nispoe 4096 May 24 11:44 checkpoint-37818
drwxrwxr-x 2 nispoe nispoe 4096 May 24 15:11 checkpoint-42020
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=97771269-d77c-4c1b-b264-34e082db6cbc">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [11]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.python.summary.summary_iterator</span> <span class="kn">import</span> <span class="n">summary_iterator</span>
<span class="kn">import</span> <span class="nn">glob</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># Construct the logs directory path</span>
<span class="n">logs_directory</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">'./'</span><span class="p">,</span> <span class="n">project_name</span><span class="p">,</span> <span class="s1">'logs'</span><span class="p">)</span>
<span class="n">file_pattern</span> <span class="o">=</span> <span class="s1">'events.out.tfevents.*'</span>

<span class="c1"># Retrieve all event files matching the pattern</span>
<span class="n">event_files</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">logs_directory</span><span class="p">,</span> <span class="n">file_pattern</span><span class="p">))</span>

<span class="c1"># Function to print out TensorBoard event logs</span>
<span class="k">def</span> <span class="nf">print_events_from_file</span><span class="p">(</span><span class="n">event_files</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">event_file</span> <span class="ow">in</span> <span class="n">event_files</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Reading events from file: </span><span class="si">{</span><span class="n">event_file</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">summary_iterator</span><span class="p">(</span><span class="n">event_file</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">e</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">v</span><span class="o">.</span><span class="n">HasField</span><span class="p">(</span><span class="s1">'simple_value'</span><span class="p">):</span>
                        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Step: </span><span class="si">{</span><span class="n">e</span><span class="o">.</span><span class="n">step</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">v</span><span class="o">.</span><span class="n">simple_value</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>  <span class="c1"># Just in case the event file is not readable</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Failed to read </span><span class="si">{</span><span class="n">event_file</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">print_events_from_file</span><span class="p">(</span><span class="n">event_files</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Reading events from file: ./praxis-Mistral-7B-v0.1-small-finetune/logs/events.out.tfevents.1716457464.hephaestus.16917.0
Step: 10, train/loss: 2.920799970626831
Step: 10, train/grad_norm: 67.82101440429688
Step: 10, train/learning_rate: 4.998810254619457e-05
Step: 10, train/epoch: 0.0023798190522938967
Step: 20, train/loss: 2.7014000415802
Step: 20, train/grad_norm: 130.58973693847656
Step: 20, train/learning_rate: 4.997620271751657e-05
Step: 20, train/epoch: 0.004759638104587793
Step: 30, train/loss: 2.5153000354766846
Step: 30, train/grad_norm: 45.847476959228516
Step: 30, train/learning_rate: 4.9964302888838574e-05
Step: 30, train/epoch: 0.007139457389712334
Step: 40, train/loss: 1.361199975013733
Step: 40, train/grad_norm: 250.22775268554688
Step: 40, train/learning_rate: 4.995240306016058e-05
Step: 40, train/epoch: 0.009519276209175587
Step: 50, train/loss: 0.25780001282691956
Step: 50, train/grad_norm: 4.738203642773442e-05
Step: 50, train/learning_rate: 4.994050323148258e-05
Step: 50, train/epoch: 0.011899095959961414
Step: 60, train/loss: 1.6883000135421753
Step: 60, train/grad_norm: 5.35391279754549e-07
Step: 60, train/learning_rate: 4.992860704078339e-05
Step: 60, train/epoch: 0.014278914779424667
Step: 70, train/loss: 0.0
Step: 70, train/grad_norm: 0.03094426915049553
Step: 70, train/learning_rate: 4.9916707212105393e-05
Step: 70, train/epoch: 0.016658734530210495
Step: 80, train/loss: 0.008299999870359898
Step: 80, train/grad_norm: 0.0003406752075534314
Step: 80, train/learning_rate: 4.9904807383427396e-05
Step: 80, train/epoch: 0.019038552418351173
Step: 90, train/loss: 0.011800000444054604
Step: 90, train/grad_norm: 0.00025308612384833395
Step: 90, train/learning_rate: 4.98929075547494e-05
Step: 90, train/epoch: 0.021418372169137
Step: 100, train/loss: 9.999999747378752e-05
Step: 100, train/grad_norm: 6.687654402630946e-16
Step: 100, train/learning_rate: 4.98810077260714e-05
Step: 100, train/epoch: 0.02379819191992283
Step: 110, train/loss: 0.05550000071525574
Step: 110, train/grad_norm: 0.0006123321945779026
Step: 110, train/learning_rate: 4.986911153537221e-05
Step: 110, train/epoch: 0.026178009808063507
Step: 120, train/loss: 1.042799949645996
Step: 120, train/grad_norm: 0.2703879177570343
Step: 120, train/learning_rate: 4.9857211706694216e-05
Step: 120, train/epoch: 0.028557829558849335
Step: 130, train/loss: 0.4244999885559082
Step: 130, train/grad_norm: 0.00011057042138418183
Step: 130, train/learning_rate: 4.984531187801622e-05
Step: 130, train/epoch: 0.030937649309635162
Step: 140, train/loss: 0.8537999987602234
Step: 140, train/grad_norm: 351.4767150878906
Step: 140, train/learning_rate: 4.983341204933822e-05
Step: 140, train/epoch: 0.03331746906042099
Step: 150, train/loss: 0.6474999785423279
Step: 150, train/grad_norm: 0.243855819106102
Step: 150, train/learning_rate: 4.9821512220660225e-05
Step: 150, train/epoch: 0.03569728881120682
Step: 160, train/loss: 0.5544000267982483
Step: 160, train/grad_norm: 2.6058738231658936
Step: 160, train/learning_rate: 4.9809616029961035e-05
Step: 160, train/epoch: 0.03807710483670235
Step: 170, train/loss: 0.5037000179290771
Step: 170, train/grad_norm: 1.6341216574344841e-12
Step: 170, train/learning_rate: 4.979771620128304e-05
Step: 170, train/epoch: 0.040456924587488174
Step: 180, train/loss: 0.6193000078201294
Step: 180, train/grad_norm: 2.2882201022866866e-08
Step: 180, train/learning_rate: 4.978581637260504e-05
Step: 180, train/epoch: 0.042836744338274
Step: 190, train/loss: 0.006000000052154064
Step: 190, train/grad_norm: 0.035682640969753265
Step: 190, train/learning_rate: 4.9773916543927044e-05
Step: 190, train/epoch: 0.04521656408905983
Step: 200, train/loss: 0.32269999384880066
Step: 200, train/grad_norm: 0.0009501790045760572
Step: 200, train/learning_rate: 4.976201671524905e-05
Step: 200, train/epoch: 0.04759638383984566
Step: 210, train/loss: 0.7674999833106995
Step: 210, train/grad_norm: 5.023004789883601e-10
Step: 210, train/learning_rate: 4.975012052454986e-05
Step: 210, train/epoch: 0.049976203590631485
Step: 220, train/loss: 0.09390000253915787
Step: 220, train/grad_norm: 94.61456298828125
Step: 220, train/learning_rate: 4.973822069587186e-05
Step: 220, train/epoch: 0.052356019616127014
Step: 230, train/loss: 0.35850000381469727
Step: 230, train/grad_norm: 0.00534230237826705
Step: 230, train/learning_rate: 4.972632086719386e-05
Step: 230, train/epoch: 0.05473583936691284
Step: 240, train/loss: 0.5257999897003174
Step: 240, train/grad_norm: 7.915934219226983e-08
Step: 240, train/learning_rate: 4.9714421038515866e-05
Step: 240, train/epoch: 0.05711565911769867
Step: 250, train/loss: 0.18520000576972961
Step: 250, train/grad_norm: 1.5423640936003684e-15
Step: 250, train/learning_rate: 4.970252120983787e-05
Step: 250, train/epoch: 0.0594954788684845
Step: 260, train/loss: 1.9595999717712402
Step: 260, train/grad_norm: 141.60610961914062
Step: 260, train/learning_rate: 4.969062501913868e-05
Step: 260, train/epoch: 0.061875298619270325
Step: 270, train/loss: 0.21330000460147858
Step: 270, train/grad_norm: 0.07488052546977997
Step: 270, train/learning_rate: 4.967872519046068e-05
Step: 270, train/epoch: 0.06425511837005615
Step: 280, train/loss: 0.08309999853372574
Step: 280, train/grad_norm: 4.680804897329305e-13
Step: 280, train/learning_rate: 4.9666825361782685e-05
Step: 280, train/epoch: 0.06663493812084198
Step: 290, train/loss: 0.1459999978542328
Step: 290, train/grad_norm: 1.9015074315120728e-07
Step: 290, train/learning_rate: 4.965492553310469e-05
Step: 290, train/epoch: 0.06901475787162781
Step: 300, train/loss: 0.04259999841451645
Step: 300, train/grad_norm: 0.0006908020004630089
Step: 300, train/learning_rate: 4.964302570442669e-05
Step: 300, train/epoch: 0.07139457762241364
Step: 310, train/loss: 0.9189000129699707
Step: 310, train/grad_norm: 3.953599758688142e-08
Step: 310, train/learning_rate: 4.96311295137275e-05
Step: 310, train/epoch: 0.07377438992261887
Step: 320, train/loss: 0.21639999747276306
Step: 320, train/grad_norm: 136.07098388671875
Step: 320, train/learning_rate: 4.9619229685049504e-05
Step: 320, train/epoch: 0.0761542096734047
Step: 330, train/loss: 0.5174000263214111
Step: 330, train/grad_norm: 249.29661560058594
Step: 330, train/learning_rate: 4.960732985637151e-05
Step: 330, train/epoch: 0.07853402942419052
Step: 340, train/loss: 0.37929999828338623
Step: 340, train/grad_norm: 4.989474389205384e-10
Step: 340, train/learning_rate: 4.959543002769351e-05
Step: 340, train/epoch: 0.08091384917497635
Step: 350, train/loss: 0.0
Step: 350, train/grad_norm: 0.0011707150842994452
Step: 350, train/learning_rate: 4.958353019901551e-05
Step: 350, train/epoch: 0.08329366892576218
Step: 360, train/loss: 0.000699999975040555
Step: 360, train/grad_norm: 3.222736408791359e-14
Step: 360, train/learning_rate: 4.957163400831632e-05
Step: 360, train/epoch: 0.085673488676548
Step: 370, train/loss: 0.0
Step: 370, train/grad_norm: 0.06801413744688034
Step: 370, train/learning_rate: 4.9559734179638326e-05
Step: 370, train/epoch: 0.08805330842733383
Step: 380, train/loss: 0.26080000400543213
Step: 380, train/grad_norm: 225.99586486816406
Step: 380, train/learning_rate: 4.954783435096033e-05
Step: 380, train/epoch: 0.09043312817811966
Step: 390, train/loss: 0.12110000103712082
Step: 390, train/grad_norm: 290.8619384765625
Step: 390, train/learning_rate: 4.953593452228233e-05
Step: 390, train/epoch: 0.09281294792890549
Step: 400, train/loss: 0.3312999904155731
Step: 400, train/grad_norm: 9.859038527793018e-07
Step: 400, train/learning_rate: 4.9524034693604335e-05
Step: 400, train/epoch: 0.09519276767969131
Step: 410, train/loss: 0.005100000184029341
Step: 410, train/grad_norm: 47.85874938964844
Step: 410, train/learning_rate: 4.9512138502905145e-05
Step: 410, train/epoch: 0.09757258743047714
Step: 420, train/loss: 0.1242000013589859
Step: 420, train/grad_norm: 1.4968016159855324e-07
Step: 420, train/learning_rate: 4.950023867422715e-05
Step: 420, train/epoch: 0.09995240718126297
Step: 430, train/loss: 0.0
Step: 430, train/grad_norm: 7.72917161384612e-08
Step: 430, train/learning_rate: 4.948833884554915e-05
Step: 430, train/epoch: 0.1023322194814682
Step: 440, train/loss: 0.20810000598430634
Step: 440, train/grad_norm: 3.1100066966160966e-10
Step: 440, train/learning_rate: 4.9476439016871154e-05
Step: 440, train/epoch: 0.10471203923225403
Step: 450, train/loss: 0.23360000550746918
Step: 450, train/grad_norm: 3.068344653911481e-07
Step: 450, train/learning_rate: 4.946453918819316e-05
Step: 450, train/epoch: 0.10709185898303986
Step: 460, train/loss: 0.9624999761581421
Step: 460, train/grad_norm: 9.056794203488039e-11
Step: 460, train/learning_rate: 4.945264299749397e-05
Step: 460, train/epoch: 0.10947167873382568
Step: 470, train/loss: 0.07419999688863754
Step: 470, train/grad_norm: 1.3455919543048367e-06
Step: 470, train/learning_rate: 4.944074316881597e-05
Step: 470, train/epoch: 0.11185149848461151
Step: 480, train/loss: 0.06549999862909317
Step: 480, train/grad_norm: 1.86885573683071e-09
Step: 480, train/learning_rate: 4.9428843340137973e-05
Step: 480, train/epoch: 0.11423131823539734
Step: 490, train/loss: 0.0
Step: 490, train/grad_norm: 3.6309474871210057e-10
Step: 490, train/learning_rate: 4.9416943511459976e-05
Step: 490, train/epoch: 0.11661113798618317
Step: 500, train/loss: 0.2312999963760376
Step: 500, train/grad_norm: 5.449727474848842e-11
Step: 500, train/learning_rate: 4.940504368278198e-05
Step: 500, train/epoch: 0.118990957736969
Step: 510, train/loss: 0.015399999916553497
Step: 510, train/grad_norm: 166.3809814453125
Step: 510, train/learning_rate: 4.939314749208279e-05
Step: 510, train/epoch: 0.12137077748775482
Step: 520, train/loss: 0.27730000019073486
Step: 520, train/grad_norm: 7.988517616505653e-13
Step: 520, train/learning_rate: 4.938124766340479e-05
Step: 520, train/epoch: 0.12375059723854065
Step: 530, train/loss: 0.18619999289512634
Step: 530, train/grad_norm: 1.5173834953287335e-13
Step: 530, train/learning_rate: 4.9369347834726796e-05
Step: 530, train/epoch: 0.12613041698932648
Step: 540, train/loss: 0.000699999975040555
Step: 540, train/grad_norm: 0.0007564482511952519
Step: 540, train/learning_rate: 4.93574480060488e-05
Step: 540, train/epoch: 0.1285102367401123
Step: 550, train/loss: 0.2574999928474426
Step: 550, train/grad_norm: 0.015599772334098816
Step: 550, train/learning_rate: 4.93455481773708e-05
Step: 550, train/epoch: 0.13089005649089813
Step: 560, train/loss: 0.44369998574256897
Step: 560, train/grad_norm: 1.664342136331598e-10
Step: 560, train/learning_rate: 4.933365198667161e-05
Step: 560, train/epoch: 0.13326987624168396
Step: 570, train/loss: 0.04820000007748604
Step: 570, train/grad_norm: 7.739084685454145e-05
Step: 570, train/learning_rate: 4.9321752157993615e-05
Step: 570, train/epoch: 0.1356496959924698
Step: 580, train/loss: 0.0
Step: 580, train/grad_norm: 1.1153106486135007e-16
Step: 580, train/learning_rate: 4.930985232931562e-05
Step: 580, train/epoch: 0.13802951574325562
Step: 590, train/loss: 0.13199999928474426
Step: 590, train/grad_norm: 398.5223388671875
Step: 590, train/learning_rate: 4.929795250063762e-05
Step: 590, train/epoch: 0.14040933549404144
Step: 600, train/loss: 0.45080000162124634
Step: 600, train/grad_norm: 7.305895888265468e-10
Step: 600, train/learning_rate: 4.9286052671959624e-05
Step: 600, train/epoch: 0.14278915524482727
Step: 610, train/loss: 0.0
Step: 610, train/grad_norm: 8.06824496536046e-13
Step: 610, train/learning_rate: 4.9274156481260434e-05
Step: 610, train/epoch: 0.1451689600944519
Step: 620, train/loss: 0.7609000205993652
Step: 620, train/grad_norm: 273.31475830078125
Step: 620, train/learning_rate: 4.926225665258244e-05
Step: 620, train/epoch: 0.14754877984523773
Step: 630, train/loss: 0.02879999950528145
Step: 630, train/grad_norm: 1.9549157892527827e-14
Step: 630, train/learning_rate: 4.925035682390444e-05
Step: 630, train/epoch: 0.14992859959602356
Step: 640, train/loss: 0.0013000000035390258
Step: 640, train/grad_norm: 1.0130936718602112e-14
Step: 640, train/learning_rate: 4.923845699522644e-05
Step: 640, train/epoch: 0.1523084193468094
Step: 650, train/loss: 0.09189999848604202
Step: 650, train/grad_norm: 348.73724365234375
Step: 650, train/learning_rate: 4.9226557166548446e-05
Step: 650, train/epoch: 0.15468823909759521
Step: 660, train/loss: 0.24379999935626984
Step: 660, train/grad_norm: 0.8023598194122314
Step: 660, train/learning_rate: 4.9214660975849256e-05
Step: 660, train/epoch: 0.15706805884838104
Step: 670, train/loss: 0.0026000000070780516
Step: 670, train/grad_norm: 4.627299077242242e-08
Step: 670, train/learning_rate: 4.920276114717126e-05
Step: 670, train/epoch: 0.15944787859916687
Step: 680, train/loss: 0.13740000128746033
Step: 680, train/grad_norm: 0.3275112211704254
Step: 680, train/learning_rate: 4.919086131849326e-05
Step: 680, train/epoch: 0.1618276983499527
Step: 690, train/loss: 0.1273999959230423
Step: 690, train/grad_norm: 8.211887170261889e-09
Step: 690, train/learning_rate: 4.9178961489815265e-05
Step: 690, train/epoch: 0.16420751810073853
Step: 700, train/loss: 0.0
Step: 700, train/grad_norm: 0.001075768144801259
Step: 700, train/learning_rate: 4.916706166113727e-05
Step: 700, train/epoch: 0.16658733785152435
Step: 710, train/loss: 1.0734000205993652
Step: 710, train/grad_norm: 2.301116519554114e-10
Step: 710, train/learning_rate: 4.915516547043808e-05
Step: 710, train/epoch: 0.16896715760231018
Step: 720, train/loss: 0.08320000022649765
Step: 720, train/grad_norm: 2.99667917147417e-08
Step: 720, train/learning_rate: 4.914326564176008e-05
Step: 720, train/epoch: 0.171346977353096
Step: 730, train/loss: 0.0005000000237487257
Step: 730, train/grad_norm: 3.426266562134295e-11
Step: 730, train/learning_rate: 4.9131365813082084e-05
Step: 730, train/epoch: 0.17372679710388184
Step: 740, train/loss: 0.31220000982284546
Step: 740, train/grad_norm: 2.956837086287578e-08
Step: 740, train/learning_rate: 4.911946598440409e-05
Step: 740, train/epoch: 0.17610661685466766
Step: 750, train/loss: 0.12250000238418579
Step: 750, train/grad_norm: 1.765993356704712
Step: 750, train/learning_rate: 4.910756615572609e-05
Step: 750, train/epoch: 0.1784864366054535
Step: 760, train/loss: 0.19300000369548798
Step: 760, train/grad_norm: 2.392623654756676e-09
Step: 760, train/learning_rate: 4.90956699650269e-05
Step: 760, train/epoch: 0.18086625635623932
Step: 770, train/loss: 0.0957999974489212
Step: 770, train/grad_norm: 1.0037987558135161e-13
Step: 770, train/learning_rate: 4.90837701363489e-05
Step: 770, train/epoch: 0.18324607610702515
Step: 780, train/loss: 0.5436999797821045
Step: 780, train/grad_norm: 3.845804317365031e-12
Step: 780, train/learning_rate: 4.9071870307670906e-05
Step: 780, train/epoch: 0.18562589585781097
Step: 790, train/loss: 0.0003000000142492354
Step: 790, train/grad_norm: 4.473929038795177e-06
Step: 790, train/learning_rate: 4.905997047899291e-05
Step: 790, train/epoch: 0.1880057156085968
Step: 800, train/loss: 0.0006000000284984708
Step: 800, train/grad_norm: 1.3672364455874497e-11
Step: 800, train/learning_rate: 4.904807065031491e-05
Step: 800, train/epoch: 0.19038553535938263
Step: 810, train/loss: 0.26420000195503235
Step: 810, train/grad_norm: 0.4404687285423279
Step: 810, train/learning_rate: 4.903617445961572e-05
Step: 810, train/epoch: 0.19276535511016846
Step: 820, train/loss: 0.5543000102043152
Step: 820, train/grad_norm: 1.6766164194839672e-12
Step: 820, train/learning_rate: 4.9024274630937725e-05
Step: 820, train/epoch: 0.19514517486095428
Step: 830, train/loss: 0.0003000000142492354
Step: 830, train/grad_norm: 4.822183097275001e-09
Step: 830, train/learning_rate: 4.901237480225973e-05
Step: 830, train/epoch: 0.1975249946117401
Step: 840, train/loss: 0.09730000048875809
Step: 840, train/grad_norm: 0.364268958568573
Step: 840, train/learning_rate: 4.900047497358173e-05
Step: 840, train/epoch: 0.19990481436252594
Step: 850, train/loss: 0.001500000013038516
Step: 850, train/grad_norm: 8.349752638435459e-13
Step: 850, train/learning_rate: 4.8988575144903734e-05
Step: 850, train/epoch: 0.20228461921215057
Step: 860, train/loss: 0.0
Step: 860, train/grad_norm: 9.343501915282104e-06
Step: 860, train/learning_rate: 4.8976678954204544e-05
Step: 860, train/epoch: 0.2046644389629364
Step: 870, train/loss: 0.0
Step: 870, train/grad_norm: 1.0709374332435947e-13
Step: 870, train/learning_rate: 4.896477912552655e-05
Step: 870, train/epoch: 0.20704425871372223
Step: 880, train/loss: 0.34380000829696655
Step: 880, train/grad_norm: 1.3517745856006513e-07
Step: 880, train/learning_rate: 4.895287929684855e-05
Step: 880, train/epoch: 0.20942407846450806
Step: 890, train/loss: 0.5375000238418579
Step: 890, train/grad_norm: 4.7895337047521025e-06
Step: 890, train/learning_rate: 4.8940979468170553e-05
Step: 890, train/epoch: 0.21180389821529388
Step: 900, train/loss: 0.06960000097751617
Step: 900, train/grad_norm: 1.6176011810413449e-13
Step: 900, train/learning_rate: 4.8929079639492556e-05
Step: 900, train/epoch: 0.2141837179660797
Step: 910, train/loss: 0.09139999747276306
Step: 910, train/grad_norm: 3.0823464669538225e-08
Step: 910, train/learning_rate: 4.8917183448793367e-05
Step: 910, train/epoch: 0.21656353771686554
Step: 920, train/loss: 0.0010000000474974513
Step: 920, train/grad_norm: 0.0016280378913506866
Step: 920, train/learning_rate: 4.890528362011537e-05
Step: 920, train/epoch: 0.21894335746765137
Step: 930, train/loss: 0.25
Step: 930, train/grad_norm: 1.3122902937923175e-11
Step: 930, train/learning_rate: 4.889338379143737e-05
Step: 930, train/epoch: 0.2213231772184372
Step: 940, train/loss: 1.3144999742507935
Step: 940, train/grad_norm: 1.0509143555381684e-10
Step: 940, train/learning_rate: 4.8881483962759376e-05
Step: 940, train/epoch: 0.22370299696922302
Step: 950, train/loss: 0.0
Step: 950, train/grad_norm: 7.182295291841001e-08
Step: 950, train/learning_rate: 4.886958413408138e-05
Step: 950, train/epoch: 0.22608281672000885
Step: 960, train/loss: 1.2026000022888184
Step: 960, train/grad_norm: 2.4121900423779152e-06
Step: 960, train/learning_rate: 4.885768794338219e-05
Step: 960, train/epoch: 0.22846263647079468
Step: 970, train/loss: 0.04089999943971634
Step: 970, train/grad_norm: 77.8721694946289
Step: 970, train/learning_rate: 4.884578811470419e-05
Step: 970, train/epoch: 0.2308424562215805
Step: 980, train/loss: 0.19670000672340393
Step: 980, train/grad_norm: 3.9219396619216695e-09
Step: 980, train/learning_rate: 4.8833888286026195e-05
Step: 980, train/epoch: 0.23322227597236633
Step: 990, train/loss: 0.6234999895095825
Step: 990, train/grad_norm: 1.9358756162546342e-06
Step: 990, train/learning_rate: 4.88219884573482e-05
Step: 990, train/epoch: 0.23560209572315216
Step: 1000, train/loss: 0.6632999777793884
Step: 1000, train/grad_norm: 7.804797606814073e-12
Step: 1000, train/learning_rate: 4.88100886286702e-05
Step: 1000, train/epoch: 0.237981915473938
Step: 1010, train/loss: 0.48010000586509705
Step: 1010, train/grad_norm: 0.0003501976025290787
Step: 1010, train/learning_rate: 4.879819243797101e-05
Step: 1010, train/epoch: 0.24036173522472382
Step: 1020, train/loss: 0.08380000293254852
Step: 1020, train/grad_norm: 34.45262908935547
Step: 1020, train/learning_rate: 4.8786292609293014e-05
Step: 1020, train/epoch: 0.24274155497550964
Step: 1030, train/loss: 0.5065000057220459
Step: 1030, train/grad_norm: 8.415590855292976e-07
Step: 1030, train/learning_rate: 4.877439278061502e-05
Step: 1030, train/epoch: 0.24512137472629547
Step: 1040, train/loss: 0.9164999723434448
Step: 1040, train/grad_norm: 3.666277916636318e-05
Step: 1040, train/learning_rate: 4.876249295193702e-05
Step: 1040, train/epoch: 0.2475011944770813
Step: 1050, train/loss: 0.7504000067710876
Step: 1050, train/grad_norm: 2.7430074212020372e-08
Step: 1050, train/learning_rate: 4.875059676123783e-05
Step: 1050, train/epoch: 0.24988101422786713
Step: 1060, train/loss: 0.42809998989105225
Step: 1060, train/grad_norm: 5.682931600858865e-07
Step: 1060, train/learning_rate: 4.873869693255983e-05
Step: 1060, train/epoch: 0.25226083397865295
Step: 1070, train/loss: 0.21250000596046448
Step: 1070, train/grad_norm: 1.3231770280147659e-12
Step: 1070, train/learning_rate: 4.8726797103881836e-05
Step: 1070, train/epoch: 0.2546406388282776
Step: 1080, train/loss: 0.0
Step: 1080, train/grad_norm: 1.285935468331445e-06
Step: 1080, train/learning_rate: 4.871489727520384e-05
Step: 1080, train/epoch: 0.2570204734802246
Step: 1090, train/loss: 0.0
Step: 1090, train/grad_norm: 3.246248105395755e-12
Step: 1090, train/learning_rate: 4.870299744652584e-05
Step: 1090, train/epoch: 0.25940027832984924
Step: 1100, train/loss: 0.0005000000237487257
Step: 1100, train/grad_norm: 4.147829286662479e-15
Step: 1100, train/learning_rate: 4.869110125582665e-05
Step: 1100, train/epoch: 0.26178011298179626
Step: 1110, train/loss: 0.13439999520778656
Step: 1110, train/grad_norm: 9.008896406648148e-11
Step: 1110, train/learning_rate: 4.8679201427148655e-05
Step: 1110, train/epoch: 0.2641599178314209
Step: 1120, train/loss: 0.0071000000461936
Step: 1120, train/grad_norm: 3.274596566127386e-11
Step: 1120, train/learning_rate: 4.866730159847066e-05
Step: 1120, train/epoch: 0.2665397524833679
Step: 1130, train/loss: 0.30390000343322754
Step: 1130, train/grad_norm: 2.6469432548581473e-11
Step: 1130, train/learning_rate: 4.865540176979266e-05
Step: 1130, train/epoch: 0.26891955733299255
Step: 1140, train/loss: 0.41110000014305115
Step: 1140, train/grad_norm: 2.1110602119733812e-06
Step: 1140, train/learning_rate: 4.8643501941114664e-05
Step: 1140, train/epoch: 0.2712993919849396
Step: 1150, train/loss: 0.10949999839067459
Step: 1150, train/grad_norm: 3.4895094813691685e-07
Step: 1150, train/learning_rate: 4.8631605750415474e-05
Step: 1150, train/epoch: 0.2736791968345642
Step: 1160, train/loss: 0.00039999998989515007
Step: 1160, train/grad_norm: 1.0591660737991333
Step: 1160, train/learning_rate: 4.861970592173748e-05
Step: 1160, train/epoch: 0.27605903148651123
Step: 1170, train/loss: 0.3257000148296356
Step: 1170, train/grad_norm: 265.8230895996094
Step: 1170, train/learning_rate: 4.860780609305948e-05
Step: 1170, train/epoch: 0.27843883633613586
Step: 1180, train/loss: 0.0013000000035390258
Step: 1180, train/grad_norm: 1.158803206635639e-05
Step: 1180, train/learning_rate: 4.859590626438148e-05
Step: 1180, train/epoch: 0.2808186709880829
Step: 1190, train/loss: 0.0
Step: 1190, train/grad_norm: 0.00015508654178120196
Step: 1190, train/learning_rate: 4.8584006435703486e-05
Step: 1190, train/epoch: 0.2831984758377075
Step: 1200, train/loss: 0.07769999653100967
Step: 1200, train/grad_norm: 0.3562203347682953
Step: 1200, train/learning_rate: 4.8572110245004296e-05
Step: 1200, train/epoch: 0.28557831048965454
Step: 1210, train/loss: 0.10890000313520432
Step: 1210, train/grad_norm: 0.0010136689525097609
Step: 1210, train/learning_rate: 4.85602104163263e-05
Step: 1210, train/epoch: 0.2879581153392792
Step: 1220, train/loss: 0.24779999256134033
Step: 1220, train/grad_norm: 2.0134334022259281e-07
Step: 1220, train/learning_rate: 4.85483105876483e-05
Step: 1220, train/epoch: 0.2903379201889038
Step: 1230, train/loss: 0.0
Step: 1230, train/grad_norm: 0.3935771882534027
Step: 1230, train/learning_rate: 4.8536410758970305e-05
Step: 1230, train/epoch: 0.29271775484085083
Step: 1240, train/loss: 0.4828000068664551
Step: 1240, train/grad_norm: 0.013769404031336308
Step: 1240, train/learning_rate: 4.852451093029231e-05
Step: 1240, train/epoch: 0.29509755969047546
Step: 1250, train/loss: 0.6844000220298767
Step: 1250, train/grad_norm: 3.150245902361348e-05
Step: 1250, train/learning_rate: 4.851261473959312e-05
Step: 1250, train/epoch: 0.2974773943424225
Step: 1260, train/loss: 0.011300000362098217
Step: 1260, train/grad_norm: 68.7501220703125
Step: 1260, train/learning_rate: 4.850071491091512e-05
Step: 1260, train/epoch: 0.2998571991920471
Step: 1270, train/loss: 0.22859999537467957
Step: 1270, train/grad_norm: 146.28231811523438
Step: 1270, train/learning_rate: 4.8488815082237124e-05
Step: 1270, train/epoch: 0.30223703384399414
Step: 1280, train/loss: 0.0010999999940395355
Step: 1280, train/grad_norm: 0.1193021759390831
Step: 1280, train/learning_rate: 4.847691525355913e-05
Step: 1280, train/epoch: 0.3046168386936188
Step: 1290, train/loss: 0.00019999999494757503
Step: 1290, train/grad_norm: 1.2755242174977699e-11
Step: 1290, train/learning_rate: 4.846501542488113e-05
Step: 1290, train/epoch: 0.3069966733455658
Step: 1300, train/loss: 0.00800000037997961
Step: 1300, train/grad_norm: 0.0026477884966880083
Step: 1300, train/learning_rate: 4.845311923418194e-05
Step: 1300, train/epoch: 0.30937647819519043
Step: 1310, train/loss: 0.04650000110268593
Step: 1310, train/grad_norm: 2.72688509994623e-07
Step: 1310, train/learning_rate: 4.8441219405503944e-05
Step: 1310, train/epoch: 0.31175631284713745
Step: 1320, train/loss: 0.0006000000284984708
Step: 1320, train/grad_norm: 2.705079042708558e-11
Step: 1320, train/learning_rate: 4.8429319576825947e-05
Step: 1320, train/epoch: 0.3141361176967621
Step: 1330, train/loss: 0.11029999703168869
Step: 1330, train/grad_norm: 0.32331976294517517
Step: 1330, train/learning_rate: 4.841741974814795e-05
Step: 1330, train/epoch: 0.3165159523487091
Step: 1340, train/loss: 0.2743000090122223
Step: 1340, train/grad_norm: 6.2784675236571275e-09
Step: 1340, train/learning_rate: 4.840551991946995e-05
Step: 1340, train/epoch: 0.31889575719833374
Step: 1350, train/loss: 0.17679999768733978
Step: 1350, train/grad_norm: 5.3951098379911855e-05
Step: 1350, train/learning_rate: 4.839362372877076e-05
Step: 1350, train/epoch: 0.32127559185028076
Step: 1360, train/loss: 0.032499998807907104
Step: 1360, train/grad_norm: 1.163022195910246e-11
Step: 1360, train/learning_rate: 4.8381723900092766e-05
Step: 1360, train/epoch: 0.3236553966999054
Step: 1370, train/loss: 0.043699998408555984
Step: 1370, train/grad_norm: 1.7452492784286733e-07
Step: 1370, train/learning_rate: 4.836982407141477e-05
Step: 1370, train/epoch: 0.3260352313518524
Step: 1380, train/loss: 0.27230000495910645
Step: 1380, train/grad_norm: 0.0001444634108338505
Step: 1380, train/learning_rate: 4.835792424273677e-05
Step: 1380, train/epoch: 0.32841503620147705
Step: 1390, train/loss: 9.999999747378752e-05
Step: 1390, train/grad_norm: 0.00037502514896914363
Step: 1390, train/learning_rate: 4.8346024414058775e-05
Step: 1390, train/epoch: 0.3307948708534241
Step: 1400, train/loss: 0.09889999777078629
Step: 1400, train/grad_norm: 8.544770935259294e-07
Step: 1400, train/learning_rate: 4.8334128223359585e-05
Step: 1400, train/epoch: 0.3331746757030487
Step: 1410, train/loss: 0.31029999256134033
Step: 1410, train/grad_norm: 1.0669092276316405e-08
Step: 1410, train/learning_rate: 4.832222839468159e-05
Step: 1410, train/epoch: 0.3355545103549957
Step: 1420, train/loss: 0.0
Step: 1420, train/grad_norm: 6.689981684945834e-13
Step: 1420, train/learning_rate: 4.831032856600359e-05
Step: 1420, train/epoch: 0.33793431520462036
Step: 1430, train/loss: 0.18170000612735748
Step: 1430, train/grad_norm: 0.027571000158786774
Step: 1430, train/learning_rate: 4.8298428737325594e-05
Step: 1430, train/epoch: 0.3403141498565674
Step: 1440, train/loss: 0.5874999761581421
Step: 1440, train/grad_norm: 157.68698120117188
Step: 1440, train/learning_rate: 4.82865289086476e-05
Step: 1440, train/epoch: 0.342693954706192
Step: 1450, train/loss: 0.5210999846458435
Step: 1450, train/grad_norm: 0.34122300148010254
Step: 1450, train/learning_rate: 4.827463271794841e-05
Step: 1450, train/epoch: 0.34507375955581665
Step: 1460, train/loss: 0.28369998931884766
Step: 1460, train/grad_norm: 0.0012770602479577065
Step: 1460, train/learning_rate: 4.826273288927041e-05
Step: 1460, train/epoch: 0.34745359420776367
Step: 1470, train/loss: 0.06629999727010727
Step: 1470, train/grad_norm: 6.407880448477954e-09
Step: 1470, train/learning_rate: 4.825083306059241e-05
Step: 1470, train/epoch: 0.3498333990573883
Step: 1480, train/loss: 0.0406000018119812
Step: 1480, train/grad_norm: 2.007537913684132e-09
Step: 1480, train/learning_rate: 4.8238933231914416e-05
Step: 1480, train/epoch: 0.3522132337093353
Step: 1490, train/loss: 0.0
Step: 1490, train/grad_norm: 0.002907775342464447
Step: 1490, train/learning_rate: 4.822703340323642e-05
Step: 1490, train/epoch: 0.35459303855895996
Step: 1500, train/loss: 0.13670000433921814
Step: 1500, train/grad_norm: 0.22341643273830414
Step: 1500, train/learning_rate: 4.821513721253723e-05
Step: 1500, train/epoch: 0.356972873210907
Step: 1510, train/loss: 0.0
Step: 1510, train/grad_norm: 1.0979013921955105e-12
Step: 1510, train/learning_rate: 4.820323738385923e-05
Step: 1510, train/epoch: 0.3593526780605316
Step: 1520, train/loss: 0.015599999576807022
Step: 1520, train/grad_norm: 3.8832054138183594
Step: 1520, train/learning_rate: 4.8191337555181235e-05
Step: 1520, train/epoch: 0.36173251271247864
Step: 1530, train/loss: 0.0
Step: 1530, train/grad_norm: 1.3671953347227372e-08
Step: 1530, train/learning_rate: 4.817943772650324e-05
Step: 1530, train/epoch: 0.36411231756210327
Step: 1540, train/loss: 0.46000000834465027
Step: 1540, train/grad_norm: 0.006526790093630552
Step: 1540, train/learning_rate: 4.816753789782524e-05
Step: 1540, train/epoch: 0.3664921522140503
Step: 1550, train/loss: 0.16359999775886536
Step: 1550, train/grad_norm: 2.608862587294425e-06
Step: 1550, train/learning_rate: 4.815564170712605e-05
Step: 1550, train/epoch: 0.3688719570636749
Step: 1560, train/loss: 0.09539999812841415
Step: 1560, train/grad_norm: 4.48872015113011e-05
Step: 1560, train/learning_rate: 4.8143741878448054e-05
Step: 1560, train/epoch: 0.37125179171562195
Step: 1570, train/loss: 0.5511000156402588
Step: 1570, train/grad_norm: 295.28057861328125
Step: 1570, train/learning_rate: 4.813184204977006e-05
Step: 1570, train/epoch: 0.3736315965652466
Step: 1580, train/loss: 0.4544999897480011
Step: 1580, train/grad_norm: 265.0040283203125
Step: 1580, train/learning_rate: 4.811994222109206e-05
Step: 1580, train/epoch: 0.3760114312171936
Step: 1590, train/loss: 0.4007999897003174
Step: 1590, train/grad_norm: 8.878409971657675e-06
Step: 1590, train/learning_rate: 4.810804239241406e-05
Step: 1590, train/epoch: 0.37839123606681824
Step: 1600, train/loss: 0.07180000096559525
Step: 1600, train/grad_norm: 27.941089630126953
Step: 1600, train/learning_rate: 4.809614620171487e-05
Step: 1600, train/epoch: 0.38077107071876526
Step: 1610, train/loss: 0.19220000505447388
Step: 1610, train/grad_norm: 3.0467013857560232e-05
Step: 1610, train/learning_rate: 4.8084246373036876e-05
Step: 1610, train/epoch: 0.3831508755683899
Step: 1620, train/loss: 0.22190000116825104
Step: 1620, train/grad_norm: 1.79509606823558e-05
Step: 1620, train/learning_rate: 4.807234654435888e-05
Step: 1620, train/epoch: 0.3855307102203369
Step: 1630, train/loss: 0.1542000025510788
Step: 1630, train/grad_norm: 167.4661865234375
Step: 1630, train/learning_rate: 4.806044671568088e-05
Step: 1630, train/epoch: 0.38791051506996155
Step: 1640, train/loss: 0.8141000270843506
Step: 1640, train/grad_norm: 0.007439859677106142
Step: 1640, train/learning_rate: 4.8048546887002885e-05
Step: 1640, train/epoch: 0.39029034972190857
Step: 1650, train/loss: 0.11670000106096268
Step: 1650, train/grad_norm: 127.88681030273438
Step: 1650, train/learning_rate: 4.8036650696303695e-05
Step: 1650, train/epoch: 0.3926701545715332
Step: 1660, train/loss: 0.1476999968290329
Step: 1660, train/grad_norm: 5.723722829031885e-09
Step: 1660, train/learning_rate: 4.80247508676257e-05
Step: 1660, train/epoch: 0.3950499892234802
Step: 1670, train/loss: 0.15469999611377716
Step: 1670, train/grad_norm: 0.004975256975740194
Step: 1670, train/learning_rate: 4.80128510389477e-05
Step: 1670, train/epoch: 0.39742979407310486
Step: 1680, train/loss: 0.023900000378489494
Step: 1680, train/grad_norm: 346.4637451171875
Step: 1680, train/learning_rate: 4.8000951210269704e-05
Step: 1680, train/epoch: 0.3998096287250519
Step: 1690, train/loss: 0.0
Step: 1690, train/grad_norm: 3.4775188396451995e-05
Step: 1690, train/learning_rate: 4.798905138159171e-05
Step: 1690, train/epoch: 0.4021894335746765
Step: 1700, train/loss: 0.004800000227987766
Step: 1700, train/grad_norm: 3.083623232669197e-05
Step: 1700, train/learning_rate: 4.797715519089252e-05
Step: 1700, train/epoch: 0.40456923842430115
Step: 1710, train/loss: 0.1599999964237213
Step: 1710, train/grad_norm: 0.002864471636712551
Step: 1710, train/learning_rate: 4.796525536221452e-05
Step: 1710, train/epoch: 0.40694907307624817
Step: 1720, train/loss: 0.44839999079704285
Step: 1720, train/grad_norm: 2.1629155064206707e-11
Step: 1720, train/learning_rate: 4.7953355533536524e-05
Step: 1720, train/epoch: 0.4093288779258728
Step: 1730, train/loss: 0.1453000009059906
Step: 1730, train/grad_norm: 1.1972738889198808e-07
Step: 1730, train/learning_rate: 4.7941455704858527e-05
Step: 1730, train/epoch: 0.4117087125778198
Step: 1740, train/loss: 0.323199987411499
Step: 1740, train/grad_norm: 0.00015423502190969884
Step: 1740, train/learning_rate: 4.792955587618053e-05
Step: 1740, train/epoch: 0.41408851742744446
Step: 1750, train/loss: 0.2312999963760376
Step: 1750, train/grad_norm: 107.21028137207031
Step: 1750, train/learning_rate: 4.791765968548134e-05
Step: 1750, train/epoch: 0.4164683520793915
Step: 1760, train/loss: 0.0
Step: 1760, train/grad_norm: 0.05913267284631729
Step: 1760, train/learning_rate: 4.790575985680334e-05
Step: 1760, train/epoch: 0.4188481569290161
Step: 1770, train/loss: 0.010499999858438969
Step: 1770, train/grad_norm: 49.619415283203125
Step: 1770, train/learning_rate: 4.7893860028125346e-05
Step: 1770, train/epoch: 0.42122799158096313
Step: 1780, train/loss: 0.0
Step: 1780, train/grad_norm: 0.1439886838197708
Step: 1780, train/learning_rate: 4.788196019944735e-05
Step: 1780, train/epoch: 0.42360779643058777
Step: 1790, train/loss: 0.24629999697208405
Step: 1790, train/grad_norm: 1.1557999002320685e-09
Step: 1790, train/learning_rate: 4.787006037076935e-05
Step: 1790, train/epoch: 0.4259876310825348
Step: 1800, train/loss: 0.2632000148296356
Step: 1800, train/grad_norm: 411.3612060546875
Step: 1800, train/learning_rate: 4.785816418007016e-05
Step: 1800, train/epoch: 0.4283674359321594
Step: 1810, train/loss: 0.0044999998062849045
Step: 1810, train/grad_norm: 0.07064050436019897
Step: 1810, train/learning_rate: 4.7846264351392165e-05
Step: 1810, train/epoch: 0.43074727058410645
Step: 1820, train/loss: 0.06539999693632126
Step: 1820, train/grad_norm: 0.06190303713083267
Step: 1820, train/learning_rate: 4.783436452271417e-05
Step: 1820, train/epoch: 0.4331270754337311
Step: 1830, train/loss: 0.0003000000142492354
Step: 1830, train/grad_norm: 0.0008766379323787987
Step: 1830, train/learning_rate: 4.782246469403617e-05
Step: 1830, train/epoch: 0.4355069100856781
Step: 1840, train/loss: 0.17980000376701355
Step: 1840, train/grad_norm: 47.970741271972656
Step: 1840, train/learning_rate: 4.7810564865358174e-05
Step: 1840, train/epoch: 0.43788671493530273
Step: 1850, train/loss: 0.03229999914765358
Step: 1850, train/grad_norm: 0.0363934263586998
Step: 1850, train/learning_rate: 4.7798668674658984e-05
Step: 1850, train/epoch: 0.44026654958724976
Step: 1860, train/loss: 0.18569999933242798
Step: 1860, train/grad_norm: 3.745793583220802e-05
Step: 1860, train/learning_rate: 4.778676884598099e-05
Step: 1860, train/epoch: 0.4426463544368744
Step: 1870, train/loss: 0.04230000078678131
Step: 1870, train/grad_norm: 0.0006074656266719103
Step: 1870, train/learning_rate: 4.777486901730299e-05
Step: 1870, train/epoch: 0.4450261890888214
Step: 1880, train/loss: 0.003100000089034438
Step: 1880, train/grad_norm: 4.600560714607127e-07
Step: 1880, train/learning_rate: 4.776296918862499e-05
Step: 1880, train/epoch: 0.44740599393844604
Step: 1890, train/loss: 0.0
Step: 1890, train/grad_norm: 8.8955985688699e-08
Step: 1890, train/learning_rate: 4.7751069359946996e-05
Step: 1890, train/epoch: 0.44978582859039307
Step: 1900, train/loss: 0.12729999423027039
Step: 1900, train/grad_norm: 0.00018937699496746063
Step: 1900, train/learning_rate: 4.7739173169247806e-05
Step: 1900, train/epoch: 0.4521656334400177
Step: 1910, train/loss: 0.0966000035405159
Step: 1910, train/grad_norm: 1.2133497540034455e-09
Step: 1910, train/learning_rate: 4.772727334056981e-05
Step: 1910, train/epoch: 0.4545454680919647
Step: 1920, train/loss: 0.00419999985024333
Step: 1920, train/grad_norm: 1.1433651252445998e-06
Step: 1920, train/learning_rate: 4.771537351189181e-05
Step: 1920, train/epoch: 0.45692527294158936
Step: 1930, train/loss: 0.03750000149011612
Step: 1930, train/grad_norm: 2.207780198659748e-05
Step: 1930, train/learning_rate: 4.7703473683213815e-05
Step: 1930, train/epoch: 0.4593051075935364
Step: 1940, train/loss: 0.15639999508857727
Step: 1940, train/grad_norm: 1.6625930854274884e-08
Step: 1940, train/learning_rate: 4.769157385453582e-05
Step: 1940, train/epoch: 0.461684912443161
Step: 1950, train/loss: 0.0
Step: 1950, train/grad_norm: 4.500711803245849e-09
Step: 1950, train/learning_rate: 4.767967766383663e-05
Step: 1950, train/epoch: 0.46406471729278564
Step: 1960, train/loss: 0.0
Step: 1960, train/grad_norm: 0.014427891001105309
Step: 1960, train/learning_rate: 4.766777783515863e-05
Step: 1960, train/epoch: 0.46644455194473267
Step: 1970, train/loss: 0.25049999356269836
Step: 1970, train/grad_norm: 6.088661166359088e-07
Step: 1970, train/learning_rate: 4.7655878006480634e-05
Step: 1970, train/epoch: 0.4688243567943573
Step: 1980, train/loss: 0.16459999978542328
Step: 1980, train/grad_norm: 3.307290796783491e-08
Step: 1980, train/learning_rate: 4.764397817780264e-05
Step: 1980, train/epoch: 0.4712041914463043
Step: 1990, train/loss: 0.0
Step: 1990, train/grad_norm: 0.0012831490021198988
Step: 1990, train/learning_rate: 4.763207834912464e-05
Step: 1990, train/epoch: 0.47358399629592896
Step: 2000, train/loss: 0.21469999849796295
Step: 2000, train/grad_norm: 5.11254059920585e-12
Step: 2000, train/learning_rate: 4.762018215842545e-05
Step: 2000, train/epoch: 0.475963830947876
Step: 2010, train/loss: 0.4968999922275543
Step: 2010, train/grad_norm: 0.0008893138146959245
Step: 2010, train/learning_rate: 4.760828232974745e-05
Step: 2010, train/epoch: 0.4783436357975006
Step: 2020, train/loss: 0.0
Step: 2020, train/grad_norm: 3.614942397689447e-05
Step: 2020, train/learning_rate: 4.7596382501069456e-05
Step: 2020, train/epoch: 0.48072347044944763
Step: 2030, train/loss: 0.07509999722242355
Step: 2030, train/grad_norm: 2.0604118411426953e-09
Step: 2030, train/learning_rate: 4.758448267239146e-05
Step: 2030, train/epoch: 0.48310327529907227
Step: 2040, train/loss: 0.013100000098347664
Step: 2040, train/grad_norm: 3.137911619432998e-08
Step: 2040, train/learning_rate: 4.757258284371346e-05
Step: 2040, train/epoch: 0.4854831099510193
Step: 2050, train/loss: 0.042500000447034836
Step: 2050, train/grad_norm: 1.5529036318184808e-05
Step: 2050, train/learning_rate: 4.756068665301427e-05
Step: 2050, train/epoch: 0.4878629148006439
Step: 2060, train/loss: 0.00019999999494757503
Step: 2060, train/grad_norm: 0.0004067025729455054
Step: 2060, train/learning_rate: 4.7548786824336275e-05
Step: 2060, train/epoch: 0.49024274945259094
Step: 2070, train/loss: 0.003100000089034438
Step: 2070, train/grad_norm: 0.034173283725976944
Step: 2070, train/learning_rate: 4.753688699565828e-05
Step: 2070, train/epoch: 0.4926225543022156
Step: 2080, train/loss: 0.0
Step: 2080, train/grad_norm: 9.419176905112181e-08
Step: 2080, train/learning_rate: 4.752498716698028e-05
Step: 2080, train/epoch: 0.4950023889541626
Step: 2090, train/loss: 0.01640000008046627
Step: 2090, train/grad_norm: 254.7471160888672
Step: 2090, train/learning_rate: 4.7513087338302284e-05
Step: 2090, train/epoch: 0.49738219380378723
Step: 2100, train/loss: 0.5648000240325928
Step: 2100, train/grad_norm: 4.089793947059661e-05
Step: 2100, train/learning_rate: 4.7501191147603095e-05
Step: 2100, train/epoch: 0.49976202845573425
Step: 2110, train/loss: 0.0003000000142492354
Step: 2110, train/grad_norm: 5.954452930723164e-08
Step: 2110, train/learning_rate: 4.74892913189251e-05
Step: 2110, train/epoch: 0.5021418333053589
Step: 2120, train/loss: 0.4090999960899353
Step: 2120, train/grad_norm: 0.0017828536219894886
Step: 2120, train/learning_rate: 4.74773914902471e-05
Step: 2120, train/epoch: 0.5045216679573059
Step: 2130, train/loss: 0.014700000174343586
Step: 2130, train/grad_norm: 4.279387866290563e-08
Step: 2130, train/learning_rate: 4.7465491661569104e-05
Step: 2130, train/epoch: 0.5069015026092529
Step: 2140, train/loss: 0.18029999732971191
Step: 2140, train/grad_norm: 8.687555350661569e-07
Step: 2140, train/learning_rate: 4.7453591832891107e-05
Step: 2140, train/epoch: 0.5092812776565552
Step: 2150, train/loss: 0.0032999999821186066
Step: 2150, train/grad_norm: 5.4477182764856025e-09
Step: 2150, train/learning_rate: 4.744169564219192e-05
Step: 2150, train/epoch: 0.5116611123085022
Step: 2160, train/loss: 0.08579999953508377
Step: 2160, train/grad_norm: 0.015369986183941364
Step: 2160, train/learning_rate: 4.742979581351392e-05
Step: 2160, train/epoch: 0.5140409469604492
Step: 2170, train/loss: 0.0019000000320374966
Step: 2170, train/grad_norm: 2.377024410638029e-10
Step: 2170, train/learning_rate: 4.741789598483592e-05
Step: 2170, train/epoch: 0.5164207816123962
Step: 2180, train/loss: 0.0
Step: 2180, train/grad_norm: 3.672784032460186e-08
Step: 2180, train/learning_rate: 4.7405996156157926e-05
Step: 2180, train/epoch: 0.5188005566596985
Step: 2190, train/loss: 0.0
Step: 2190, train/grad_norm: 1.0425928564927744e-07
Step: 2190, train/learning_rate: 4.739409632747993e-05
Step: 2190, train/epoch: 0.5211803913116455
Step: 2200, train/loss: 0.24690000712871552
Step: 2200, train/grad_norm: 1.1756599249679311e-08
Step: 2200, train/learning_rate: 4.738220013678074e-05
Step: 2200, train/epoch: 0.5235602259635925
Step: 2210, train/loss: 0.06369999796152115
Step: 2210, train/grad_norm: 0.0353035107254982
Step: 2210, train/learning_rate: 4.737030030810274e-05
Step: 2210, train/epoch: 0.5259400010108948
Step: 2220, train/loss: 0.2198999971151352
Step: 2220, train/grad_norm: 1.3255440762804938e-06
Step: 2220, train/learning_rate: 4.7358400479424745e-05
Step: 2220, train/epoch: 0.5283198356628418
Step: 2230, train/loss: 0.3319000005722046
Step: 2230, train/grad_norm: 5.618847370147705
Step: 2230, train/learning_rate: 4.734650065074675e-05
Step: 2230, train/epoch: 0.5306996703147888
Step: 2240, train/loss: 0.0
Step: 2240, train/grad_norm: 1.9725572201423347e-05
Step: 2240, train/learning_rate: 4.733460082206875e-05
Step: 2240, train/epoch: 0.5330795049667358
Step: 2250, train/loss: 0.14790000021457672
Step: 2250, train/grad_norm: 69.19400024414062
Step: 2250, train/learning_rate: 4.732270463136956e-05
Step: 2250, train/epoch: 0.5354592800140381
Step: 2260, train/loss: 0.05009999871253967
Step: 2260, train/grad_norm: 2.698447474358545e-07
Step: 2260, train/learning_rate: 4.7310804802691564e-05
Step: 2260, train/epoch: 0.5378391146659851
Step: 2270, train/loss: 0.006500000134110451
Step: 2270, train/grad_norm: 4.074011667398736e-05
Step: 2270, train/learning_rate: 4.729890497401357e-05
Step: 2270, train/epoch: 0.5402189493179321
Step: 2280, train/loss: 0.09459999948740005
Step: 2280, train/grad_norm: 3.440371187934943e-08
Step: 2280, train/learning_rate: 4.728700514533557e-05
Step: 2280, train/epoch: 0.5425987839698792
Step: 2290, train/loss: 0.19339999556541443
Step: 2290, train/grad_norm: 0.02040787599980831
Step: 2290, train/learning_rate: 4.727510531665757e-05
Step: 2290, train/epoch: 0.5449785590171814
Step: 2300, train/loss: 0.00019999999494757503
Step: 2300, train/grad_norm: 3.6629366206852865e-09
Step: 2300, train/learning_rate: 4.726320912595838e-05
Step: 2300, train/epoch: 0.5473583936691284
Step: 2310, train/loss: 0.10080000013113022
Step: 2310, train/grad_norm: 1.401282861479558e-05
Step: 2310, train/learning_rate: 4.7251309297280386e-05
Step: 2310, train/epoch: 0.5497382283210754
Step: 2320, train/loss: 0.2766000032424927
Step: 2320, train/grad_norm: 0.00019257876556366682
Step: 2320, train/learning_rate: 4.723940946860239e-05
Step: 2320, train/epoch: 0.5521180629730225
Step: 2330, train/loss: 0.00019999999494757503
Step: 2330, train/grad_norm: 3.98023939851555e-06
Step: 2330, train/learning_rate: 4.722750963992439e-05
Step: 2330, train/epoch: 0.5544978380203247
Step: 2340, train/loss: 9.999999747378752e-05
Step: 2340, train/grad_norm: 5.047848026151769e-05
Step: 2340, train/learning_rate: 4.7215609811246395e-05
Step: 2340, train/epoch: 0.5568776726722717
Step: 2350, train/loss: 0.0008999999845400453
Step: 2350, train/grad_norm: 3.3851381431304617e-06
Step: 2350, train/learning_rate: 4.7203713620547205e-05
Step: 2350, train/epoch: 0.5592575073242188
Step: 2360, train/loss: 0.03880000114440918
Step: 2360, train/grad_norm: 203.77978515625
Step: 2360, train/learning_rate: 4.719181379186921e-05
Step: 2360, train/epoch: 0.5616373419761658
Step: 2370, train/loss: 0.02290000021457672
Step: 2370, train/grad_norm: 1.0563459085233262e-07
Step: 2370, train/learning_rate: 4.717991396319121e-05
Step: 2370, train/epoch: 0.564017117023468
Step: 2380, train/loss: 0.26600000262260437
Step: 2380, train/grad_norm: 2.4635497197778022e-08
Step: 2380, train/learning_rate: 4.7168014134513214e-05
Step: 2380, train/epoch: 0.566396951675415
Step: 2390, train/loss: 0.0
Step: 2390, train/grad_norm: 1.3329540596296319e-11
Step: 2390, train/learning_rate: 4.7156117943814024e-05
Step: 2390, train/epoch: 0.5687767863273621
Step: 2400, train/loss: 0.25589999556541443
Step: 2400, train/grad_norm: 3.026744082035293e-07
Step: 2400, train/learning_rate: 4.714421811513603e-05
Step: 2400, train/epoch: 0.5711566209793091
Step: 2410, train/loss: 0.15850000083446503
Step: 2410, train/grad_norm: 8.37321323388096e-08
Step: 2410, train/learning_rate: 4.713231828645803e-05
Step: 2410, train/epoch: 0.5735363960266113
Step: 2420, train/loss: 0.0007999999797903001
Step: 2420, train/grad_norm: 0.25880005955696106
Step: 2420, train/learning_rate: 4.712041845778003e-05
Step: 2420, train/epoch: 0.5759162306785583
Step: 2430, train/loss: 0.0005000000237487257
Step: 2430, train/grad_norm: 0.0003496887511573732
Step: 2430, train/learning_rate: 4.7108518629102036e-05
Step: 2430, train/epoch: 0.5782960653305054
Step: 2440, train/loss: 9.999999747378752e-05
Step: 2440, train/grad_norm: 0.00027259462513029575
Step: 2440, train/learning_rate: 4.7096622438402846e-05
Step: 2440, train/epoch: 0.5806758403778076
Step: 2450, train/loss: 9.999999747378752e-05
Step: 2450, train/grad_norm: 2.4383544428019377e-07
Step: 2450, train/learning_rate: 4.708472260972485e-05
Step: 2450, train/epoch: 0.5830556750297546
Step: 2460, train/loss: 0.0
Step: 2460, train/grad_norm: 0.013839134946465492
Step: 2460, train/learning_rate: 4.707282278104685e-05
Step: 2460, train/epoch: 0.5854355096817017
Step: 2470, train/loss: 0.13910000026226044
Step: 2470, train/grad_norm: 7.996140629984438e-05
Step: 2470, train/learning_rate: 4.7060922952368855e-05
Step: 2470, train/epoch: 0.5878153443336487
Step: 2480, train/loss: 0.0
Step: 2480, train/grad_norm: 8.16410761217412e-09
Step: 2480, train/learning_rate: 4.704902312369086e-05
Step: 2480, train/epoch: 0.5901951193809509
Step: 2490, train/loss: 0.12280000001192093
Step: 2490, train/grad_norm: 185.5548858642578
Step: 2490, train/learning_rate: 4.703712693299167e-05
Step: 2490, train/epoch: 0.592574954032898
Step: 2500, train/loss: 0.19380000233650208
Step: 2500, train/grad_norm: 2.43553859036183e-06
Step: 2500, train/learning_rate: 4.702522710431367e-05
Step: 2500, train/epoch: 0.594954788684845
Step: 2510, train/loss: 0.00860000029206276
Step: 2510, train/grad_norm: 0.0054975589737296104
Step: 2510, train/learning_rate: 4.7013327275635675e-05
Step: 2510, train/epoch: 0.597334623336792
Step: 2520, train/loss: 0.00800000037997961
Step: 2520, train/grad_norm: 2.3292663172469474e-05
Step: 2520, train/learning_rate: 4.700142744695768e-05
Step: 2520, train/epoch: 0.5997143983840942
Step: 2530, train/loss: 0.0
Step: 2530, train/grad_norm: 4.966376536685857e-07
Step: 2530, train/learning_rate: 4.698952761827968e-05
Step: 2530, train/epoch: 0.6020942330360413
Step: 2540, train/loss: 0.03790000081062317
Step: 2540, train/grad_norm: 8.017807704163715e-05
Step: 2540, train/learning_rate: 4.697763142758049e-05
Step: 2540, train/epoch: 0.6044740676879883
Step: 2550, train/loss: 0.017500000074505806
Step: 2550, train/grad_norm: 5.365739343687892e-05
Step: 2550, train/learning_rate: 4.6965731598902494e-05
Step: 2550, train/epoch: 0.6068539023399353
Step: 2560, train/loss: 0.1754000037908554
Step: 2560, train/grad_norm: 216.6422119140625
Step: 2560, train/learning_rate: 4.69538317702245e-05
Step: 2560, train/epoch: 0.6092336773872375
Step: 2570, train/loss: 0.03610000014305115
Step: 2570, train/grad_norm: 22.677675247192383
Step: 2570, train/learning_rate: 4.69419319415465e-05
Step: 2570, train/epoch: 0.6116135120391846
Step: 2580, train/loss: 0.010300000198185444
Step: 2580, train/grad_norm: 205.9258575439453
Step: 2580, train/learning_rate: 4.69300321128685e-05
Step: 2580, train/epoch: 0.6139933466911316
Step: 2590, train/loss: 0.0
Step: 2590, train/grad_norm: 5.0964972615474835e-05
Step: 2590, train/learning_rate: 4.691813592216931e-05
Step: 2590, train/epoch: 0.6163731813430786
Step: 2600, train/loss: 9.999999747378752e-05
Step: 2600, train/grad_norm: 4.2873531114207e-11
Step: 2600, train/learning_rate: 4.6906236093491316e-05
Step: 2600, train/epoch: 0.6187529563903809
Step: 2610, train/loss: 0.0421999990940094
Step: 2610, train/grad_norm: 3.9204118706948066e-07
Step: 2610, train/learning_rate: 4.689433626481332e-05
Step: 2610, train/epoch: 0.6211327910423279
Step: 2620, train/loss: 0.00039999998989515007
Step: 2620, train/grad_norm: 7.150436886149691e-06
Step: 2620, train/learning_rate: 4.688243643613532e-05
Step: 2620, train/epoch: 0.6235126256942749
Step: 2630, train/loss: 0.22310000658035278
Step: 2630, train/grad_norm: 0.0006248407298699021
Step: 2630, train/learning_rate: 4.6870536607457325e-05
Step: 2630, train/epoch: 0.6258924603462219
Step: 2640, train/loss: 0.0006000000284984708
Step: 2640, train/grad_norm: 5.5085688899225715e-08
Step: 2640, train/learning_rate: 4.6858640416758135e-05
Step: 2640, train/epoch: 0.6282722353935242
Step: 2650, train/loss: 0.02800000086426735
Step: 2650, train/grad_norm: 1.3423193934158917e-07
Step: 2650, train/learning_rate: 4.684674058808014e-05
Step: 2650, train/epoch: 0.6306520700454712
Step: 2660, train/loss: 0.1527000069618225
Step: 2660, train/grad_norm: 8.517502259053344e-10
Step: 2660, train/learning_rate: 4.683484075940214e-05
Step: 2660, train/epoch: 0.6330319046974182
Step: 2670, train/loss: 0.0
Step: 2670, train/grad_norm: 2.9264866086009533e-09
Step: 2670, train/learning_rate: 4.6822940930724144e-05
Step: 2670, train/epoch: 0.6354116797447205
Step: 2680, train/loss: 0.00989999994635582
Step: 2680, train/grad_norm: 5.749790332743032e-08
Step: 2680, train/learning_rate: 4.681104110204615e-05
Step: 2680, train/epoch: 0.6377915143966675
Step: 2690, train/loss: 0.0
Step: 2690, train/grad_norm: 9.530048714623263e-07
Step: 2690, train/learning_rate: 4.679914491134696e-05
Step: 2690, train/epoch: 0.6401713490486145
Step: 2700, train/loss: 0.027499999850988388
Step: 2700, train/grad_norm: 1.9050141872867243e-06
Step: 2700, train/learning_rate: 4.678724508266896e-05
Step: 2700, train/epoch: 0.6425511837005615
Step: 2710, train/loss: 0.5
Step: 2710, train/grad_norm: 1.9648261684324098e-07
Step: 2710, train/learning_rate: 4.677534525399096e-05
Step: 2710, train/epoch: 0.6449309587478638
Step: 2720, train/loss: 0.011500000022351742
Step: 2720, train/grad_norm: 5.506249362952076e-05
Step: 2720, train/learning_rate: 4.6763445425312966e-05
Step: 2720, train/epoch: 0.6473107933998108
Step: 2730, train/loss: 0.0
Step: 2730, train/grad_norm: 0.0007681488641537726
Step: 2730, train/learning_rate: 4.675154559663497e-05
Step: 2730, train/epoch: 0.6496906280517578
Step: 2740, train/loss: 0.0035000001080334187
Step: 2740, train/grad_norm: 1.6254800527804036e-07
Step: 2740, train/learning_rate: 4.673964940593578e-05
Step: 2740, train/epoch: 0.6520704627037048
Step: 2750, train/loss: 0.16599999368190765
Step: 2750, train/grad_norm: 0.0015835738740861416
Step: 2750, train/learning_rate: 4.672774957725778e-05
Step: 2750, train/epoch: 0.6544502377510071
Step: 2760, train/loss: 0.0003000000142492354
Step: 2760, train/grad_norm: 6.933339022907603e-07
Step: 2760, train/learning_rate: 4.6715849748579785e-05
Step: 2760, train/epoch: 0.6568300724029541
Step: 2770, train/loss: 0.09650000184774399
Step: 2770, train/grad_norm: 0.00021477714471984655
Step: 2770, train/learning_rate: 4.670394991990179e-05
Step: 2770, train/epoch: 0.6592099070549011
Step: 2780, train/loss: 0.4101000130176544
Step: 2780, train/grad_norm: 5.774366854893742e-06
Step: 2780, train/learning_rate: 4.669205009122379e-05
Step: 2780, train/epoch: 0.6615897417068481
Step: 2790, train/loss: 0.052000001072883606
Step: 2790, train/grad_norm: 4.01718380782512e-11
Step: 2790, train/learning_rate: 4.66801539005246e-05
Step: 2790, train/epoch: 0.6639695167541504
Step: 2800, train/loss: 0.10840000212192535
Step: 2800, train/grad_norm: 161.5518035888672
Step: 2800, train/learning_rate: 4.6668254071846604e-05
Step: 2800, train/epoch: 0.6663493514060974
Step: 2810, train/loss: 0.0
Step: 2810, train/grad_norm: 3.949373308387294e-07
Step: 2810, train/learning_rate: 4.665635424316861e-05
Step: 2810, train/epoch: 0.6687291860580444
Step: 2820, train/loss: 0.2766999900341034
Step: 2820, train/grad_norm: 129.98159790039062
Step: 2820, train/learning_rate: 4.664445441449061e-05
Step: 2820, train/epoch: 0.6711090207099915
Step: 2830, train/loss: 0.00039999998989515007
Step: 2830, train/grad_norm: 0.00012365708244033158
Step: 2830, train/learning_rate: 4.663255458581261e-05
Step: 2830, train/epoch: 0.6734887957572937
Step: 2840, train/loss: 0.0
Step: 2840, train/grad_norm: 8.431333409220088e-10
Step: 2840, train/learning_rate: 4.6620658395113423e-05
Step: 2840, train/epoch: 0.6758686304092407
Step: 2850, train/loss: 0.0026000000070780516
Step: 2850, train/grad_norm: 8.08455509297961e-15
Step: 2850, train/learning_rate: 4.6608758566435426e-05
Step: 2850, train/epoch: 0.6782484650611877
Step: 2860, train/loss: 0.1445000022649765
Step: 2860, train/grad_norm: 0.9332243800163269
Step: 2860, train/learning_rate: 4.659685873775743e-05
Step: 2860, train/epoch: 0.6806282997131348
Step: 2870, train/loss: 0.2867000102996826
Step: 2870, train/grad_norm: 1.2164919098722748e-06
Step: 2870, train/learning_rate: 4.658495890907943e-05
Step: 2870, train/epoch: 0.683008074760437
Step: 2880, train/loss: 0.13050000369548798
Step: 2880, train/grad_norm: 0.10742152482271194
Step: 2880, train/learning_rate: 4.6573059080401435e-05
Step: 2880, train/epoch: 0.685387909412384
Step: 2890, train/loss: 0.034299999475479126
Step: 2890, train/grad_norm: 1.145264931778911e-07
Step: 2890, train/learning_rate: 4.6561162889702246e-05
Step: 2890, train/epoch: 0.687767744064331
Step: 2900, train/loss: 0.003100000089034438
Step: 2900, train/grad_norm: 2.393522208876675e-06
Step: 2900, train/learning_rate: 4.654926306102425e-05
Step: 2900, train/epoch: 0.6901475191116333
Step: 2910, train/loss: 0.17030000686645508
Step: 2910, train/grad_norm: 0.0055805835872888565
Step: 2910, train/learning_rate: 4.653736323234625e-05
Step: 2910, train/epoch: 0.6925273537635803
Step: 2920, train/loss: 0.013299999758601189
Step: 2920, train/grad_norm: 5.833465654969672e-11
Step: 2920, train/learning_rate: 4.6525463403668255e-05
Step: 2920, train/epoch: 0.6949071884155273
Step: 2930, train/loss: 0.0
Step: 2930, train/grad_norm: 4.519418155268795e-07
Step: 2930, train/learning_rate: 4.651356357499026e-05
Step: 2930, train/epoch: 0.6972870230674744
Step: 2940, train/loss: 0.0
Step: 2940, train/grad_norm: 2.8644171479186298e-08
Step: 2940, train/learning_rate: 4.650166738429107e-05
Step: 2940, train/epoch: 0.6996667981147766
Step: 2950, train/loss: 0.0
Step: 2950, train/grad_norm: 3.380426642252132e-05
Step: 2950, train/learning_rate: 4.648976755561307e-05
Step: 2950, train/epoch: 0.7020466327667236
Step: 2960, train/loss: 0.0348999984562397
Step: 2960, train/grad_norm: 5.018610527152134e-11
Step: 2960, train/learning_rate: 4.6477867726935074e-05
Step: 2960, train/epoch: 0.7044264674186707
Step: 2970, train/loss: 0.09319999814033508
Step: 2970, train/grad_norm: 0.010582411661744118
Step: 2970, train/learning_rate: 4.646596789825708e-05
Step: 2970, train/epoch: 0.7068063020706177
Step: 2980, train/loss: 0.21819999814033508
Step: 2980, train/grad_norm: 5.3518098866334185e-05
Step: 2980, train/learning_rate: 4.645406806957908e-05
Step: 2980, train/epoch: 0.7091860771179199
Step: 2990, train/loss: 0.00039999998989515007
Step: 2990, train/grad_norm: 2.2490211915027203e-09
Step: 2990, train/learning_rate: 4.644217187887989e-05
Step: 2990, train/epoch: 0.7115659117698669
Step: 3000, train/loss: 0.0007999999797903001
Step: 3000, train/grad_norm: 2.5059563313334365e-07
Step: 3000, train/learning_rate: 4.643027205020189e-05
Step: 3000, train/epoch: 0.713945746421814
Step: 3010, train/loss: 0.23600000143051147
Step: 3010, train/grad_norm: 0.015994522720575333
Step: 3010, train/learning_rate: 4.6418372221523896e-05
Step: 3010, train/epoch: 0.716325581073761
Step: 3020, train/loss: 0.3407999873161316
Step: 3020, train/grad_norm: 3.9062970245140605e-06
Step: 3020, train/learning_rate: 4.64064723928459e-05
Step: 3020, train/epoch: 0.7187053561210632
Step: 3030, train/loss: 0.05570000037550926
Step: 3030, train/grad_norm: 4.216257920575117e-08
Step: 3030, train/learning_rate: 4.63945725641679e-05
Step: 3030, train/epoch: 0.7210851907730103
Step: 3040, train/loss: 0.06719999760389328
Step: 3040, train/grad_norm: 5.590105160990788e-07
Step: 3040, train/learning_rate: 4.638267637346871e-05
Step: 3040, train/epoch: 0.7234650254249573
Step: 3050, train/loss: 0.0
Step: 3050, train/grad_norm: 0.007330758962780237
Step: 3050, train/learning_rate: 4.6370776544790715e-05
Step: 3050, train/epoch: 0.7258448600769043
Step: 3060, train/loss: 0.2728999853134155
Step: 3060, train/grad_norm: 4.408453264659329e-07
Step: 3060, train/learning_rate: 4.635887671611272e-05
Step: 3060, train/epoch: 0.7282246351242065
Step: 3070, train/loss: 0.39250001311302185
Step: 3070, train/grad_norm: 130.78175354003906
Step: 3070, train/learning_rate: 4.634697688743472e-05
Step: 3070, train/epoch: 0.7306044697761536
Step: 3080, train/loss: 0.025599999353289604
Step: 3080, train/grad_norm: 97.83173370361328
Step: 3080, train/learning_rate: 4.6335077058756724e-05
Step: 3080, train/epoch: 0.7329843044281006
Step: 3090, train/loss: 0.005400000140070915
Step: 3090, train/grad_norm: 3.8171183405211195e-06
Step: 3090, train/learning_rate: 4.6323180868057534e-05
Step: 3090, train/epoch: 0.7353641390800476
Step: 3100, train/loss: 0.11959999799728394
Step: 3100, train/grad_norm: 0.0011488848831504583
Step: 3100, train/learning_rate: 4.631128103937954e-05
Step: 3100, train/epoch: 0.7377439141273499
Step: 3110, train/loss: 0.550000011920929
Step: 3110, train/grad_norm: 0.002599060535430908
Step: 3110, train/learning_rate: 4.629938121070154e-05
Step: 3110, train/epoch: 0.7401237487792969
Step: 3120, train/loss: 0.05510000139474869
Step: 3120, train/grad_norm: 0.008308765478432178
Step: 3120, train/learning_rate: 4.628748138202354e-05
Step: 3120, train/epoch: 0.7425035834312439
Step: 3130, train/loss: 0.0027000000700354576
Step: 3130, train/grad_norm: 3.803566869464703e-05
Step: 3130, train/learning_rate: 4.6275581553345546e-05
Step: 3130, train/epoch: 0.7448834180831909
Step: 3140, train/loss: 0.11410000175237656
Step: 3140, train/grad_norm: 58.96150588989258
Step: 3140, train/learning_rate: 4.6263685362646356e-05
Step: 3140, train/epoch: 0.7472631931304932
Step: 3150, train/loss: 0.0008999999845400453
Step: 3150, train/grad_norm: 8.325058297486976e-05
Step: 3150, train/learning_rate: 4.625178553396836e-05
Step: 3150, train/epoch: 0.7496430277824402
Step: 3160, train/loss: 0.0026000000070780516
Step: 3160, train/grad_norm: 2.596756530692801e-05
Step: 3160, train/learning_rate: 4.623988570529036e-05
Step: 3160, train/epoch: 0.7520228624343872
Step: 3170, train/loss: 0.0
Step: 3170, train/grad_norm: 2.2519039703183807e-05
Step: 3170, train/learning_rate: 4.6227985876612365e-05
Step: 3170, train/epoch: 0.7544026374816895
Step: 3180, train/loss: 0.009600000455975533
Step: 3180, train/grad_norm: 1.2595876796694938e-05
Step: 3180, train/learning_rate: 4.621608604793437e-05
Step: 3180, train/epoch: 0.7567824721336365
Step: 3190, train/loss: 0.03790000081062317
Step: 3190, train/grad_norm: 5.62858949706424e-05
Step: 3190, train/learning_rate: 4.620418985723518e-05
Step: 3190, train/epoch: 0.7591623067855835
Step: 3200, train/loss: 0.026399999856948853
Step: 3200, train/grad_norm: 7.603647986798023e-07
Step: 3200, train/learning_rate: 4.619229002855718e-05
Step: 3200, train/epoch: 0.7615421414375305
Step: 3210, train/loss: 0.045899998396635056
Step: 3210, train/grad_norm: 3.6652882045018487e-06
Step: 3210, train/learning_rate: 4.6180390199879184e-05
Step: 3210, train/epoch: 0.7639219164848328
Step: 3220, train/loss: 0.24079999327659607
Step: 3220, train/grad_norm: 6.349722752929665e-06
Step: 3220, train/learning_rate: 4.616849037120119e-05
Step: 3220, train/epoch: 0.7663017511367798
Step: 3230, train/loss: 0.1462000012397766
Step: 3230, train/grad_norm: 1.9255436711773655e-07
Step: 3230, train/learning_rate: 4.615659054252319e-05
Step: 3230, train/epoch: 0.7686815857887268
Step: 3240, train/loss: 0.0044999998062849045
Step: 3240, train/grad_norm: 0.0011999346315860748
Step: 3240, train/learning_rate: 4.6144694351824e-05
Step: 3240, train/epoch: 0.7710614204406738
Step: 3250, train/loss: 0.00019999999494757503
Step: 3250, train/grad_norm: 0.001376852742396295
Step: 3250, train/learning_rate: 4.6132794523146003e-05
Step: 3250, train/epoch: 0.7734411954879761
Step: 3260, train/loss: 0.00019999999494757503
Step: 3260, train/grad_norm: 0.00044073647586628795
Step: 3260, train/learning_rate: 4.6120894694468006e-05
Step: 3260, train/epoch: 0.7758210301399231
Step: 3270, train/loss: 0.1062999963760376
Step: 3270, train/grad_norm: 0.0014928453601896763
Step: 3270, train/learning_rate: 4.610899486579001e-05
Step: 3270, train/epoch: 0.7782008647918701
Step: 3280, train/loss: 0.0
Step: 3280, train/grad_norm: 0.0001789296220522374
Step: 3280, train/learning_rate: 4.609709503711201e-05
Step: 3280, train/epoch: 0.7805806994438171
Step: 3290, train/loss: 0.0
Step: 3290, train/grad_norm: 7.52231289880001e-06
Step: 3290, train/learning_rate: 4.608519884641282e-05
Step: 3290, train/epoch: 0.7829604744911194
Step: 3300, train/loss: 0.0010999999940395355
Step: 3300, train/grad_norm: 6.298479320321348e-07
Step: 3300, train/learning_rate: 4.6073299017734826e-05
Step: 3300, train/epoch: 0.7853403091430664
Step: 3310, train/loss: 9.999999747378752e-05
Step: 3310, train/grad_norm: 5.545104375670462e-09
Step: 3310, train/learning_rate: 4.606139918905683e-05
Step: 3310, train/epoch: 0.7877201437950134
Step: 3320, train/loss: 0.0
Step: 3320, train/grad_norm: 1.0250292916680337e-06
Step: 3320, train/learning_rate: 4.604949936037883e-05
Step: 3320, train/epoch: 0.7900999784469604
Step: 3330, train/loss: 0.0
Step: 3330, train/grad_norm: 1.2813876310246997e-05
Step: 3330, train/learning_rate: 4.6037599531700835e-05
Step: 3330, train/epoch: 0.7924797534942627
Step: 3340, train/loss: 0.0
Step: 3340, train/grad_norm: 0.0003514822747092694
Step: 3340, train/learning_rate: 4.6025703341001645e-05
Step: 3340, train/epoch: 0.7948595881462097
Step: 3350, train/loss: 0.05469999834895134
Step: 3350, train/grad_norm: 5.3572475735563785e-05
Step: 3350, train/learning_rate: 4.601380351232365e-05
Step: 3350, train/epoch: 0.7972394227981567
Step: 3360, train/loss: 0.0
Step: 3360, train/grad_norm: 2.5093598310377274e-07
Step: 3360, train/learning_rate: 4.600190368364565e-05
Step: 3360, train/epoch: 0.7996192574501038
Step: 3370, train/loss: 0.0
Step: 3370, train/grad_norm: 4.184500994597329e-07
Step: 3370, train/learning_rate: 4.5990003854967654e-05
Step: 3370, train/epoch: 0.801999032497406
Step: 3380, train/loss: 0.0
Step: 3380, train/grad_norm: 9.584553026797948e-07
Step: 3380, train/learning_rate: 4.597810402628966e-05
Step: 3380, train/epoch: 0.804378867149353
Step: 3390, train/loss: 0.0
Step: 3390, train/grad_norm: 1.3156345630704891e-05
Step: 3390, train/learning_rate: 4.596620783559047e-05
Step: 3390, train/epoch: 0.8067587018013
Step: 3400, train/loss: 9.999999747378752e-05
Step: 3400, train/grad_norm: 4.188298407825641e-05
Step: 3400, train/learning_rate: 4.595430800691247e-05
Step: 3400, train/epoch: 0.8091384768486023
Step: 3410, train/loss: 9.999999747378752e-05
Step: 3410, train/grad_norm: 0.0007157915970310569
Step: 3410, train/learning_rate: 4.594240817823447e-05
Step: 3410, train/epoch: 0.8115183115005493
Step: 3420, train/loss: 0.1046999990940094
Step: 3420, train/grad_norm: 3.391333507352101e-07
Step: 3420, train/learning_rate: 4.5930508349556476e-05
Step: 3420, train/epoch: 0.8138981461524963
Step: 3430, train/loss: 0.0
Step: 3430, train/grad_norm: 1.1049851309508085e-06
Step: 3430, train/learning_rate: 4.591860852087848e-05
Step: 3430, train/epoch: 0.8162779808044434
Step: 3440, train/loss: 0.2969000041484833
Step: 3440, train/grad_norm: 5.3211966587696224e-05
Step: 3440, train/learning_rate: 4.590671233017929e-05
Step: 3440, train/epoch: 0.8186577558517456
Step: 3450, train/loss: 0.06210000067949295
Step: 3450, train/grad_norm: 0.0005046309670433402
Step: 3450, train/learning_rate: 4.589481250150129e-05
Step: 3450, train/epoch: 0.8210375905036926
Step: 3460, train/loss: 0.08950000256299973
Step: 3460, train/grad_norm: 0.0060195173136889935
Step: 3460, train/learning_rate: 4.5882912672823295e-05
Step: 3460, train/epoch: 0.8234174251556396
Step: 3470, train/loss: 0.0681999996304512
Step: 3470, train/grad_norm: 0.06948769837617874
Step: 3470, train/learning_rate: 4.58710128441453e-05
Step: 3470, train/epoch: 0.8257972598075867
Step: 3480, train/loss: 0.05079999938607216
Step: 3480, train/grad_norm: 0.00014221043966244906
Step: 3480, train/learning_rate: 4.58591130154673e-05
Step: 3480, train/epoch: 0.8281770348548889
Step: 3490, train/loss: 0.0015999999595806003
Step: 3490, train/grad_norm: 0.006656244862824678
Step: 3490, train/learning_rate: 4.584721682476811e-05
Step: 3490, train/epoch: 0.8305568695068359
Step: 3500, train/loss: 0.00800000037997961
Step: 3500, train/grad_norm: 2.044461143668741e-05
Step: 3500, train/learning_rate: 4.5835316996090114e-05
Step: 3500, train/epoch: 0.832936704158783
Step: 3510, train/loss: 0.15469999611377716
Step: 3510, train/grad_norm: 0.001888537430204451
Step: 3510, train/learning_rate: 4.582341716741212e-05
Step: 3510, train/epoch: 0.83531653881073
Step: 3520, train/loss: 0.01850000023841858
Step: 3520, train/grad_norm: 0.00024313872563652694
Step: 3520, train/learning_rate: 4.581151733873412e-05
Step: 3520, train/epoch: 0.8376963138580322
Step: 3530, train/loss: 0.007000000216066837
Step: 3530, train/grad_norm: 0.0027163424529135227
Step: 3530, train/learning_rate: 4.579961751005612e-05
Step: 3530, train/epoch: 0.8400761485099792
Step: 3540, train/loss: 0.05350000038743019
Step: 3540, train/grad_norm: 4.6060781642154325e-06
Step: 3540, train/learning_rate: 4.578772131935693e-05
Step: 3540, train/epoch: 0.8424559831619263
Step: 3550, train/loss: 0.0
Step: 3550, train/grad_norm: 6.544925417983904e-05
Step: 3550, train/learning_rate: 4.5775821490678936e-05
Step: 3550, train/epoch: 0.8448358178138733
Step: 3560, train/loss: 0.3693000078201294
Step: 3560, train/grad_norm: 455.0527038574219
Step: 3560, train/learning_rate: 4.576392166200094e-05
Step: 3560, train/epoch: 0.8472155928611755
Step: 3570, train/loss: 0.24079999327659607
Step: 3570, train/grad_norm: 0.00015446489851456136
Step: 3570, train/learning_rate: 4.575202183332294e-05
Step: 3570, train/epoch: 0.8495954275131226
Step: 3580, train/loss: 0.017100000753998756
Step: 3580, train/grad_norm: 86.46739196777344
Step: 3580, train/learning_rate: 4.5740122004644945e-05
Step: 3580, train/epoch: 0.8519752621650696
Step: 3590, train/loss: 0.0006000000284984708
Step: 3590, train/grad_norm: 0.00037685592542402446
Step: 3590, train/learning_rate: 4.5728225813945755e-05
Step: 3590, train/epoch: 0.8543550968170166
Step: 3600, train/loss: 0.05900000035762787
Step: 3600, train/grad_norm: 0.00035685361945070326
Step: 3600, train/learning_rate: 4.571632598526776e-05
Step: 3600, train/epoch: 0.8567348718643188
Step: 3610, train/loss: 0.0
Step: 3610, train/grad_norm: 0.03662421554327011
Step: 3610, train/learning_rate: 4.570442615658976e-05
Step: 3610, train/epoch: 0.8591147065162659
Step: 3620, train/loss: 0.0044999998062849045
Step: 3620, train/grad_norm: 1.7049850953299028e-07
Step: 3620, train/learning_rate: 4.5692526327911764e-05
Step: 3620, train/epoch: 0.8614945411682129
Step: 3630, train/loss: 0.22339999675750732
Step: 3630, train/grad_norm: 3.785690205404535e-05
Step: 3630, train/learning_rate: 4.568062649923377e-05
Step: 3630, train/epoch: 0.8638743162155151
Step: 3640, train/loss: 0.00019999999494757503
Step: 3640, train/grad_norm: 0.029221251606941223
Step: 3640, train/learning_rate: 4.566873030853458e-05
Step: 3640, train/epoch: 0.8662541508674622
Step: 3650, train/loss: 0.2117999941110611
Step: 3650, train/grad_norm: 67.28306579589844
Step: 3650, train/learning_rate: 4.565683047985658e-05
Step: 3650, train/epoch: 0.8686339855194092
Step: 3660, train/loss: 0.10909999907016754
Step: 3660, train/grad_norm: 0.00029560094117186964
Step: 3660, train/learning_rate: 4.5644930651178584e-05
Step: 3660, train/epoch: 0.8710138201713562
Step: 3670, train/loss: 0.06650000065565109
Step: 3670, train/grad_norm: 0.23718886077404022
Step: 3670, train/learning_rate: 4.5633030822500587e-05
Step: 3670, train/epoch: 0.8733935952186584
Step: 3680, train/loss: 0.0013000000035390258
Step: 3680, train/grad_norm: 0.0006760582327842712
Step: 3680, train/learning_rate: 4.562113099382259e-05
Step: 3680, train/epoch: 0.8757734298706055
Step: 3690, train/loss: 0.060600001364946365
Step: 3690, train/grad_norm: 0.0005967481411062181
Step: 3690, train/learning_rate: 4.56092348031234e-05
Step: 3690, train/epoch: 0.8781532645225525
Step: 3700, train/loss: 0.0017000000225380063
Step: 3700, train/grad_norm: 25.252487182617188
Step: 3700, train/learning_rate: 4.55973349744454e-05
Step: 3700, train/epoch: 0.8805330991744995
Step: 3710, train/loss: 0.34060001373291016
Step: 3710, train/grad_norm: 2.0701230823760852e-05
Step: 3710, train/learning_rate: 4.5585435145767406e-05
Step: 3710, train/epoch: 0.8829128742218018
Step: 3720, train/loss: 0.17919999361038208
Step: 3720, train/grad_norm: 0.008813126012682915
Step: 3720, train/learning_rate: 4.557353531708941e-05
Step: 3720, train/epoch: 0.8852927088737488
Step: 3730, train/loss: 0.0
Step: 3730, train/grad_norm: 1.2601317678218038e-07
Step: 3730, train/learning_rate: 4.556163912639022e-05
Step: 3730, train/epoch: 0.8876725435256958
Step: 3740, train/loss: 0.00019999999494757503
Step: 3740, train/grad_norm: 0.017696889117360115
Step: 3740, train/learning_rate: 4.554973929771222e-05
Step: 3740, train/epoch: 0.8900523781776428
Step: 3750, train/loss: 0.0
Step: 3750, train/grad_norm: 2.574992947756982e-07
Step: 3750, train/learning_rate: 4.5537839469034225e-05
Step: 3750, train/epoch: 0.8924321532249451
Step: 3760, train/loss: 0.0
Step: 3760, train/grad_norm: 8.275956275838325e-08
Step: 3760, train/learning_rate: 4.552593964035623e-05
Step: 3760, train/epoch: 0.8948119878768921
Step: 3770, train/loss: 0.22020000219345093
Step: 3770, train/grad_norm: 9.297829137722147e-07
Step: 3770, train/learning_rate: 4.551403981167823e-05
Step: 3770, train/epoch: 0.8971918225288391
Step: 3780, train/loss: 0.00430000014603138
Step: 3780, train/grad_norm: 0.004203688818961382
Step: 3780, train/learning_rate: 4.550214362097904e-05
Step: 3780, train/epoch: 0.8995716571807861
Step: 3790, train/loss: 0.12970000505447388
Step: 3790, train/grad_norm: 0.10054143518209457
Step: 3790, train/learning_rate: 4.5490243792301044e-05
Step: 3790, train/epoch: 0.9019514322280884
Step: 3800, train/loss: 9.999999747378752e-05
Step: 3800, train/grad_norm: 0.00020371326536405832
Step: 3800, train/learning_rate: 4.547834396362305e-05
Step: 3800, train/epoch: 0.9043312668800354
Step: 3810, train/loss: 0.0008999999845400453
Step: 3810, train/grad_norm: 1.5575882196426392
Step: 3810, train/learning_rate: 4.546644413494505e-05
Step: 3810, train/epoch: 0.9067111015319824
Step: 3820, train/loss: 0.3343999981880188
Step: 3820, train/grad_norm: 0.26161816716194153
Step: 3820, train/learning_rate: 4.545454430626705e-05
Step: 3820, train/epoch: 0.9090909361839294
Step: 3830, train/loss: 9.999999747378752e-05
Step: 3830, train/grad_norm: 1.64013428616272e-07
Step: 3830, train/learning_rate: 4.544264811556786e-05
Step: 3830, train/epoch: 0.9114707112312317
Step: 3840, train/loss: 0.07729999721050262
Step: 3840, train/grad_norm: 1.1086075573985e-05
Step: 3840, train/learning_rate: 4.5430748286889866e-05
Step: 3840, train/epoch: 0.9138505458831787
Step: 3850, train/loss: 0.0003000000142492354
Step: 3850, train/grad_norm: 0.011056467890739441
Step: 3850, train/learning_rate: 4.541884845821187e-05
Step: 3850, train/epoch: 0.9162303805351257
Step: 3860, train/loss: 0.0006000000284984708
Step: 3860, train/grad_norm: 0.005404396913945675
Step: 3860, train/learning_rate: 4.540694862953387e-05
Step: 3860, train/epoch: 0.9186102151870728
Step: 3870, train/loss: 0.0
Step: 3870, train/grad_norm: 3.891165931690921e-07
Step: 3870, train/learning_rate: 4.5395048800855875e-05
Step: 3870, train/epoch: 0.920989990234375
Step: 3880, train/loss: 0.12950000166893005
Step: 3880, train/grad_norm: 4.024968347948743e-06
Step: 3880, train/learning_rate: 4.5383152610156685e-05
Step: 3880, train/epoch: 0.923369824886322
Step: 3890, train/loss: 0.28760001063346863
Step: 3890, train/grad_norm: 7.035897851892514e-06
Step: 3890, train/learning_rate: 4.537125278147869e-05
Step: 3890, train/epoch: 0.925749659538269
Step: 3900, train/loss: 0.003700000001117587
Step: 3900, train/grad_norm: 6.421982106985524e-05
Step: 3900, train/learning_rate: 4.535935295280069e-05
Step: 3900, train/epoch: 0.9281294345855713
Step: 3910, train/loss: 0.0
Step: 3910, train/grad_norm: 2.1516059860005043e-05
Step: 3910, train/learning_rate: 4.5347453124122694e-05
Step: 3910, train/epoch: 0.9305092692375183
Step: 3920, train/loss: 0.0007999999797903001
Step: 3920, train/grad_norm: 6.697161438751209e-07
Step: 3920, train/learning_rate: 4.53355532954447e-05
Step: 3920, train/epoch: 0.9328891038894653
Step: 3930, train/loss: 0.07169999927282333
Step: 3930, train/grad_norm: 4.876355524174869e-05
Step: 3930, train/learning_rate: 4.532365710474551e-05
Step: 3930, train/epoch: 0.9352689385414124
Step: 3940, train/loss: 0.4632999897003174
Step: 3940, train/grad_norm: 4.7899928176775575e-05
Step: 3940, train/learning_rate: 4.531175727606751e-05
Step: 3940, train/epoch: 0.9376487135887146
Step: 3950, train/loss: 0.12680000066757202
Step: 3950, train/grad_norm: 0.2212265431880951
Step: 3950, train/learning_rate: 4.529985744738951e-05
Step: 3950, train/epoch: 0.9400285482406616
Step: 3960, train/loss: 0.0008999999845400453
Step: 3960, train/grad_norm: 0.05681564658880234
Step: 3960, train/learning_rate: 4.5287957618711516e-05
Step: 3960, train/epoch: 0.9424083828926086
Step: 3970, train/loss: 9.999999747378752e-05
Step: 3970, train/grad_norm: 0.002949492074549198
Step: 3970, train/learning_rate: 4.527605779003352e-05
Step: 3970, train/epoch: 0.9447882175445557
Step: 3980, train/loss: 0.00019999999494757503
Step: 3980, train/grad_norm: 0.00012332772894296795
Step: 3980, train/learning_rate: 4.526416159933433e-05
Step: 3980, train/epoch: 0.9471679925918579
Step: 3990, train/loss: 0.00019999999494757503
Step: 3990, train/grad_norm: 4.070142267664778e-08
Step: 3990, train/learning_rate: 4.525226177065633e-05
Step: 3990, train/epoch: 0.9495478272438049
Step: 4000, train/loss: 0.11169999837875366
Step: 4000, train/grad_norm: 1.1931997789815796e-07
Step: 4000, train/learning_rate: 4.5240361941978335e-05
Step: 4000, train/epoch: 0.951927661895752
Step: 4010, train/loss: 0.025200000032782555
Step: 4010, train/grad_norm: 5.436981518869288e-05
Step: 4010, train/learning_rate: 4.522846211330034e-05
Step: 4010, train/epoch: 0.954307496547699
Step: 4020, train/loss: 0.15000000596046448
Step: 4020, train/grad_norm: 5.0885464588645846e-06
Step: 4020, train/learning_rate: 4.521656228462234e-05
Step: 4020, train/epoch: 0.9566872715950012
Step: 4030, train/loss: 0.024299999698996544
Step: 4030, train/grad_norm: 7.709838246228173e-05
Step: 4030, train/learning_rate: 4.520466609392315e-05
Step: 4030, train/epoch: 0.9590671062469482
Step: 4040, train/loss: 9.999999747378752e-05
Step: 4040, train/grad_norm: 0.006081714760512114
Step: 4040, train/learning_rate: 4.5192766265245155e-05
Step: 4040, train/epoch: 0.9614469408988953
Step: 4050, train/loss: 0.028200000524520874
Step: 4050, train/grad_norm: 5.110652168127672e-08
Step: 4050, train/learning_rate: 4.518086643656716e-05
Step: 4050, train/epoch: 0.9638267755508423
Step: 4060, train/loss: 0.08969999849796295
Step: 4060, train/grad_norm: 195.52865600585938
Step: 4060, train/learning_rate: 4.516896660788916e-05
Step: 4060, train/epoch: 0.9662065505981445
Step: 4070, train/loss: 0.03590000048279762
Step: 4070, train/grad_norm: 3.3050007914425805e-05
Step: 4070, train/learning_rate: 4.5157066779211164e-05
Step: 4070, train/epoch: 0.9685863852500916
Step: 4080, train/loss: 0.017799999564886093
Step: 4080, train/grad_norm: 0.5873934030532837
Step: 4080, train/learning_rate: 4.5145170588511974e-05
Step: 4080, train/epoch: 0.9709662199020386
Step: 4090, train/loss: 0.34060001373291016
Step: 4090, train/grad_norm: 2.0612422304111533e-05
Step: 4090, train/learning_rate: 4.513327075983398e-05
Step: 4090, train/epoch: 0.9733460545539856
Step: 4100, train/loss: 0.1687999963760376
Step: 4100, train/grad_norm: 1.051772414939478e-05
Step: 4100, train/learning_rate: 4.512137093115598e-05
Step: 4100, train/epoch: 0.9757258296012878
Step: 4110, train/loss: 0.0
Step: 4110, train/grad_norm: 1.1591909014896373e-06
Step: 4110, train/learning_rate: 4.510947110247798e-05
Step: 4110, train/epoch: 0.9781056642532349
Step: 4120, train/loss: 0.2078000009059906
Step: 4120, train/grad_norm: 1.5350229887189926e-07
Step: 4120, train/learning_rate: 4.5097571273799986e-05
Step: 4120, train/epoch: 0.9804854989051819
Step: 4130, train/loss: 9.999999747378752e-05
Step: 4130, train/grad_norm: 0.0001520012301625684
Step: 4130, train/learning_rate: 4.5085675083100796e-05
Step: 4130, train/epoch: 0.9828652739524841
Step: 4140, train/loss: 0.0
Step: 4140, train/grad_norm: 5.751921889896039e-07
Step: 4140, train/learning_rate: 4.50737752544228e-05
Step: 4140, train/epoch: 0.9852451086044312
Step: 4150, train/loss: 0.11659999936819077
Step: 4150, train/grad_norm: 1.1185845778527437e-06
Step: 4150, train/learning_rate: 4.50618754257448e-05
Step: 4150, train/epoch: 0.9876249432563782
Step: 4160, train/loss: 0.07509999722242355
Step: 4160, train/grad_norm: 0.5218692421913147
Step: 4160, train/learning_rate: 4.5049975597066805e-05
Step: 4160, train/epoch: 0.9900047779083252
Step: 4170, train/loss: 0.0
Step: 4170, train/grad_norm: 8.473055004287744e-07
Step: 4170, train/learning_rate: 4.503807576838881e-05
Step: 4170, train/epoch: 0.9923845529556274
Step: 4180, train/loss: 0.00279999990016222
Step: 4180, train/grad_norm: 0.008593134582042694
Step: 4180, train/learning_rate: 4.502617957768962e-05
Step: 4180, train/epoch: 0.9947643876075745
Step: 4190, train/loss: 0.36980000138282776
Step: 4190, train/grad_norm: 0.00020115860388614237
Step: 4190, train/learning_rate: 4.501427974901162e-05
Step: 4190, train/epoch: 0.9971442222595215
Step: 4200, train/loss: 0.2556999921798706
Step: 4200, train/grad_norm: 0.0005599778378382325
Step: 4200, train/learning_rate: 4.5002379920333624e-05
Step: 4200, train/epoch: 0.9995240569114685
Step: 4202, eval/loss: 0.051498834043741226
Step: 4202, eval/accuracy: 0.9909759759902954
Step: 4202, eval/f1: 0.9904594421386719
Step: 4202, eval/runtime: 734.13232421875
Step: 4202, eval/samples_per_second: 9.812000274658203
Step: 4202, eval/steps_per_second: 1.2269999980926514
Step: 4202, train/epoch: 1.0
Step: 4210, train/loss: 0.0
Step: 4210, train/grad_norm: 0.00023603421868756413
Step: 4210, train/learning_rate: 4.499048009165563e-05
Step: 4210, train/epoch: 1.0019038915634155
Step: 4220, train/loss: 0.027000000700354576
Step: 4220, train/grad_norm: 0.023570844903588295
Step: 4220, train/learning_rate: 4.497858026297763e-05
Step: 4220, train/epoch: 1.0042836666107178
Step: 4230, train/loss: 0.07989999651908875
Step: 4230, train/grad_norm: 266.0047302246094
Step: 4230, train/learning_rate: 4.496668407227844e-05
Step: 4230, train/epoch: 1.00666344165802
Step: 4240, train/loss: 0.16419999301433563
Step: 4240, train/grad_norm: 0.0023411193396896124
Step: 4240, train/learning_rate: 4.495478424360044e-05
Step: 4240, train/epoch: 1.0090433359146118
Step: 4250, train/loss: 0.2506999969482422
Step: 4250, train/grad_norm: 0.13139225542545319
Step: 4250, train/learning_rate: 4.4942884414922446e-05
Step: 4250, train/epoch: 1.011423110961914
Step: 4260, train/loss: 0.0006000000284984708
Step: 4260, train/grad_norm: 0.0012384020956233144
Step: 4260, train/learning_rate: 4.493098458624445e-05
Step: 4260, train/epoch: 1.0138030052185059
Step: 4270, train/loss: 0.06520000100135803
Step: 4270, train/grad_norm: 0.00033075400278903544
Step: 4270, train/learning_rate: 4.491908475756645e-05
Step: 4270, train/epoch: 1.016182780265808
Step: 4280, train/loss: 0.0
Step: 4280, train/grad_norm: 0.000344206637237221
Step: 4280, train/learning_rate: 4.490718856686726e-05
Step: 4280, train/epoch: 1.0185625553131104
Step: 4290, train/loss: 0.149399995803833
Step: 4290, train/grad_norm: 0.00010690982162486762
Step: 4290, train/learning_rate: 4.4895288738189265e-05
Step: 4290, train/epoch: 1.0209424495697021
Step: 4300, train/loss: 0.00039999998989515007
Step: 4300, train/grad_norm: 8.63602508616168e-06
Step: 4300, train/learning_rate: 4.488338890951127e-05
Step: 4300, train/epoch: 1.0233222246170044
Step: 4310, train/loss: 0.021199999377131462
Step: 4310, train/grad_norm: 0.000697195588145405
Step: 4310, train/learning_rate: 4.487148908083327e-05
Step: 4310, train/epoch: 1.0257019996643066
Step: 4320, train/loss: 0.0
Step: 4320, train/grad_norm: 0.44396260380744934
Step: 4320, train/learning_rate: 4.4859589252155274e-05
Step: 4320, train/epoch: 1.0280818939208984
Step: 4330, train/loss: 0.0
Step: 4330, train/grad_norm: 1.087754071704694e-06
Step: 4330, train/learning_rate: 4.4847693061456084e-05
Step: 4330, train/epoch: 1.0304616689682007
Step: 4340, train/loss: 0.0
Step: 4340, train/grad_norm: 0.00024082463642116636
Step: 4340, train/learning_rate: 4.483579323277809e-05
Step: 4340, train/epoch: 1.0328415632247925
Step: 4350, train/loss: 0.004999999888241291
Step: 4350, train/grad_norm: 3.9723050576867536e-05
Step: 4350, train/learning_rate: 4.482389340410009e-05
Step: 4350, train/epoch: 1.0352213382720947
Step: 4360, train/loss: 0.0
Step: 4360, train/grad_norm: 3.549254188328632e-06
Step: 4360, train/learning_rate: 4.481199357542209e-05
Step: 4360, train/epoch: 1.037601113319397
Step: 4370, train/loss: 0.0
Step: 4370, train/grad_norm: 0.004736511968076229
Step: 4370, train/learning_rate: 4.4800093746744096e-05
Step: 4370, train/epoch: 1.0399810075759888
Step: 4380, train/loss: 0.07819999754428864
Step: 4380, train/grad_norm: 0.000224979012273252
Step: 4380, train/learning_rate: 4.4788197556044906e-05
Step: 4380, train/epoch: 1.042360782623291
Step: 4390, train/loss: 0.07699999958276749
Step: 4390, train/grad_norm: 6.269917363077582e-10
Step: 4390, train/learning_rate: 4.477629772736691e-05
Step: 4390, train/epoch: 1.0447405576705933
Step: 4400, train/loss: 0.015799999237060547
Step: 4400, train/grad_norm: 0.0009134938009083271
Step: 4400, train/learning_rate: 4.476439789868891e-05
Step: 4400, train/epoch: 1.047120451927185
Step: 4410, train/loss: 0.003599999938160181
Step: 4410, train/grad_norm: 18.68290901184082
Step: 4410, train/learning_rate: 4.4752498070010915e-05
Step: 4410, train/epoch: 1.0495002269744873
Step: 4420, train/loss: 0.13750000298023224
Step: 4420, train/grad_norm: 0.20406672358512878
Step: 4420, train/learning_rate: 4.474059824133292e-05
Step: 4420, train/epoch: 1.0518800020217896
Step: 4430, train/loss: 0.0
Step: 4430, train/grad_norm: 8.067674571066163e-06
Step: 4430, train/learning_rate: 4.472870205063373e-05
Step: 4430, train/epoch: 1.0542598962783813
Step: 4440, train/loss: 0.006800000090152025
Step: 4440, train/grad_norm: 9.335749382444192e-06
Step: 4440, train/learning_rate: 4.471680222195573e-05
Step: 4440, train/epoch: 1.0566396713256836
Step: 4450, train/loss: 0.0
Step: 4450, train/grad_norm: 5.555696588999126e-06
Step: 4450, train/learning_rate: 4.4704902393277735e-05
Step: 4450, train/epoch: 1.0590195655822754
Step: 4460, train/loss: 0.002099999925121665
Step: 4460, train/grad_norm: 0.003571570385247469
Step: 4460, train/learning_rate: 4.469300256459974e-05
Step: 4460, train/epoch: 1.0613993406295776
Step: 4470, train/loss: 0.0
Step: 4470, train/grad_norm: 7.352354714385001e-06
Step: 4470, train/learning_rate: 4.468110273592174e-05
Step: 4470, train/epoch: 1.0637791156768799
Step: 4480, train/loss: 0.13910000026226044
Step: 4480, train/grad_norm: 3.1370731012891895e-10
Step: 4480, train/learning_rate: 4.466920654522255e-05
Step: 4480, train/epoch: 1.0661590099334717
Step: 4490, train/loss: 0.05590000003576279
Step: 4490, train/grad_norm: 1.820723127821111e-07
Step: 4490, train/learning_rate: 4.4657306716544554e-05
Step: 4490, train/epoch: 1.068538784980774
Step: 4500, train/loss: 0.632099986076355
Step: 4500, train/grad_norm: 143.68772888183594
Step: 4500, train/learning_rate: 4.464540688786656e-05
Step: 4500, train/epoch: 1.0709185600280762
Step: 4510, train/loss: 0.000699999975040555
Step: 4510, train/grad_norm: 0.0002844766422640532
Step: 4510, train/learning_rate: 4.463350705918856e-05
Step: 4510, train/epoch: 1.073298454284668
Step: 4520, train/loss: 0.013000000268220901
Step: 4520, train/grad_norm: 0.00023957951634656638
Step: 4520, train/learning_rate: 4.462160723051056e-05
Step: 4520, train/epoch: 1.0756782293319702
Step: 4530, train/loss: 0.09290000051259995
Step: 4530, train/grad_norm: 2.504382848739624
Step: 4530, train/learning_rate: 4.460971103981137e-05
Step: 4530, train/epoch: 1.078058123588562
Step: 4540, train/loss: 0.0
Step: 4540, train/grad_norm: 0.014058815315365791
Step: 4540, train/learning_rate: 4.4597811211133376e-05
Step: 4540, train/epoch: 1.0804378986358643
Step: 4550, train/loss: 0.0737999975681305
Step: 4550, train/grad_norm: 2.87147258859477e-07
Step: 4550, train/learning_rate: 4.458591138245538e-05
Step: 4550, train/epoch: 1.0828176736831665
Step: 4560, train/loss: 0.04490000009536743
Step: 4560, train/grad_norm: 1.9379906746053166e-07
Step: 4560, train/learning_rate: 4.457401155377738e-05
Step: 4560, train/epoch: 1.0851975679397583
Step: 4570, train/loss: 0.0
Step: 4570, train/grad_norm: 0.00014277032460086048
Step: 4570, train/learning_rate: 4.4562111725099385e-05
Step: 4570, train/epoch: 1.0875773429870605
Step: 4580, train/loss: 0.03180000185966492
Step: 4580, train/grad_norm: 0.0004638823738787323
Step: 4580, train/learning_rate: 4.4550215534400195e-05
Step: 4580, train/epoch: 1.0899571180343628
Step: 4590, train/loss: 0.1054999977350235
Step: 4590, train/grad_norm: 0.009257308207452297
Step: 4590, train/learning_rate: 4.45383157057222e-05
Step: 4590, train/epoch: 1.0923370122909546
Step: 4600, train/loss: 9.999999747378752e-05
Step: 4600, train/grad_norm: 4.698717384599149e-05
Step: 4600, train/learning_rate: 4.45264158770442e-05
Step: 4600, train/epoch: 1.0947167873382568
Step: 4610, train/loss: 0.0
Step: 4610, train/grad_norm: 3.169134572544863e-07
Step: 4610, train/learning_rate: 4.4514516048366204e-05
Step: 4610, train/epoch: 1.097096562385559
Step: 4620, train/loss: 0.06639999896287918
Step: 4620, train/grad_norm: 6.686908022857097e-07
Step: 4620, train/learning_rate: 4.450261621968821e-05
Step: 4620, train/epoch: 1.0994764566421509
Step: 4630, train/loss: 0.004699999932199717
Step: 4630, train/grad_norm: 1.725244280770255e-09
Step: 4630, train/learning_rate: 4.449072002898902e-05
Step: 4630, train/epoch: 1.1018562316894531
Step: 4640, train/loss: 0.024800000712275505
Step: 4640, train/grad_norm: 1.413276550010778e-06
Step: 4640, train/learning_rate: 4.447882020031102e-05
Step: 4640, train/epoch: 1.104236125946045
Step: 4650, train/loss: 0.0003000000142492354
Step: 4650, train/grad_norm: 8.588381561480674e-09
Step: 4650, train/learning_rate: 4.446692037163302e-05
Step: 4650, train/epoch: 1.1066159009933472
Step: 4660, train/loss: 9.999999747378752e-05
Step: 4660, train/grad_norm: 2.262619733810425
Step: 4660, train/learning_rate: 4.4455020542955026e-05
Step: 4660, train/epoch: 1.1089956760406494
Step: 4670, train/loss: 0.0
Step: 4670, train/grad_norm: 6.411939312833681e-10
Step: 4670, train/learning_rate: 4.444312071427703e-05
Step: 4670, train/epoch: 1.1113755702972412
Step: 4680, train/loss: 0.0
Step: 4680, train/grad_norm: 1.9276443288807066e-11
Step: 4680, train/learning_rate: 4.443122452357784e-05
Step: 4680, train/epoch: 1.1137553453445435
Step: 4690, train/loss: 0.10459999740123749
Step: 4690, train/grad_norm: 1.8723135042364447e-08
Step: 4690, train/learning_rate: 4.441932469489984e-05
Step: 4690, train/epoch: 1.1161351203918457
Step: 4700, train/loss: 0.23749999701976776
Step: 4700, train/grad_norm: 3.252672090003905e-11
Step: 4700, train/learning_rate: 4.4407424866221845e-05
Step: 4700, train/epoch: 1.1185150146484375
Step: 4710, train/loss: 0.04439999908208847
Step: 4710, train/grad_norm: 22.3184757232666
Step: 4710, train/learning_rate: 4.439552503754385e-05
Step: 4710, train/epoch: 1.1208947896957397
Step: 4720, train/loss: 0.0008999999845400453
Step: 4720, train/grad_norm: 1.4887829138388042e-06
Step: 4720, train/learning_rate: 4.438362520886585e-05
Step: 4720, train/epoch: 1.1232746839523315
Step: 4730, train/loss: 0.0
Step: 4730, train/grad_norm: 0.00026073804474435747
Step: 4730, train/learning_rate: 4.437172901816666e-05
Step: 4730, train/epoch: 1.1256544589996338
Step: 4740, train/loss: 0.0
Step: 4740, train/grad_norm: 3.312216056983175e-09
Step: 4740, train/learning_rate: 4.4359829189488664e-05
Step: 4740, train/epoch: 1.128034234046936
Step: 4750, train/loss: 0.0
Step: 4750, train/grad_norm: 0.0007904425729066133
Step: 4750, train/learning_rate: 4.434792936081067e-05
Step: 4750, train/epoch: 1.1304141283035278
Step: 4760, train/loss: 0.0015999999595806003
Step: 4760, train/grad_norm: 1.6192141629289836e-05
Step: 4760, train/learning_rate: 4.433602953213267e-05
Step: 4760, train/epoch: 1.13279390335083
Step: 4770, train/loss: 0.13830000162124634
Step: 4770, train/grad_norm: 2.1511795011974755e-07
Step: 4770, train/learning_rate: 4.432412970345467e-05
Step: 4770, train/epoch: 1.1351736783981323
Step: 4780, train/loss: 0.0
Step: 4780, train/grad_norm: 0.005378117319196463
Step: 4780, train/learning_rate: 4.4312233512755483e-05
Step: 4780, train/epoch: 1.1375535726547241
Step: 4790, train/loss: 0.0
Step: 4790, train/grad_norm: 1.224154349936668e-12
Step: 4790, train/learning_rate: 4.4300333684077486e-05
Step: 4790, train/epoch: 1.1399333477020264
Step: 4800, train/loss: 0.0
Step: 4800, train/grad_norm: 1.5900454286565946e-07
Step: 4800, train/learning_rate: 4.428843385539949e-05
Step: 4800, train/epoch: 1.1423132419586182
Step: 4810, train/loss: 9.999999747378752e-05
Step: 4810, train/grad_norm: 0.00010090718569699675
Step: 4810, train/learning_rate: 4.427653402672149e-05
Step: 4810, train/epoch: 1.1446930170059204
Step: 4820, train/loss: 0.0
Step: 4820, train/grad_norm: 1.745286863297224e-05
Step: 4820, train/learning_rate: 4.4264634198043495e-05
Step: 4820, train/epoch: 1.1470727920532227
Step: 4830, train/loss: 0.0
Step: 4830, train/grad_norm: 1.268752464200773e-10
Step: 4830, train/learning_rate: 4.4252738007344306e-05
Step: 4830, train/epoch: 1.1494526863098145
Step: 4840, train/loss: 0.0
Step: 4840, train/grad_norm: 0.0033984521869570017
Step: 4840, train/learning_rate: 4.424083817866631e-05
Step: 4840, train/epoch: 1.1518324613571167
Step: 4850, train/loss: 0.0
Step: 4850, train/grad_norm: 6.845741881988943e-05
Step: 4850, train/learning_rate: 4.422893834998831e-05
Step: 4850, train/epoch: 1.154212236404419
Step: 4860, train/loss: 0.014600000344216824
Step: 4860, train/grad_norm: 0.0004358684818726033
Step: 4860, train/learning_rate: 4.4217038521310315e-05
Step: 4860, train/epoch: 1.1565921306610107
Step: 4870, train/loss: 0.06989999860525131
Step: 4870, train/grad_norm: 600.2888793945312
Step: 4870, train/learning_rate: 4.420513869263232e-05
Step: 4870, train/epoch: 1.158971905708313
Step: 4880, train/loss: 0.0
Step: 4880, train/grad_norm: 3.30743714584969e-05
Step: 4880, train/learning_rate: 4.419324250193313e-05
Step: 4880, train/epoch: 1.1613516807556152
Step: 4890, train/loss: 0.0
Step: 4890, train/grad_norm: 1.41359279837161e-07
Step: 4890, train/learning_rate: 4.418134267325513e-05
Step: 4890, train/epoch: 1.163731575012207
Step: 4900, train/loss: 0.0
Step: 4900, train/grad_norm: 7.626363367307931e-05
Step: 4900, train/learning_rate: 4.4169442844577134e-05
Step: 4900, train/epoch: 1.1661113500595093
Step: 4910, train/loss: 0.0
Step: 4910, train/grad_norm: 6.558290624525398e-05
Step: 4910, train/learning_rate: 4.415754301589914e-05
Step: 4910, train/epoch: 1.168491244316101
Step: 4920, train/loss: 0.12189999967813492
Step: 4920, train/grad_norm: 0.0003217628109268844
Step: 4920, train/learning_rate: 4.414564318722114e-05
Step: 4920, train/epoch: 1.1708710193634033
Step: 4930, train/loss: 0.0012000000569969416
Step: 4930, train/grad_norm: 6.107292392698582e-06
Step: 4930, train/learning_rate: 4.413374699652195e-05
Step: 4930, train/epoch: 1.1732507944107056
Step: 4940, train/loss: 0.0
Step: 4940, train/grad_norm: 5.086238843432511e-07
Step: 4940, train/learning_rate: 4.412184716784395e-05
Step: 4940, train/epoch: 1.1756306886672974
Step: 4950, train/loss: 0.00570000009611249
Step: 4950, train/grad_norm: 1.5434277467196722e-10
Step: 4950, train/learning_rate: 4.4109947339165956e-05
Step: 4950, train/epoch: 1.1780104637145996
Step: 4960, train/loss: 0.08789999783039093
Step: 4960, train/grad_norm: 0.00019323504238855094
Step: 4960, train/learning_rate: 4.409804751048796e-05
Step: 4960, train/epoch: 1.1803902387619019
Step: 4970, train/loss: 0.0
Step: 4970, train/grad_norm: 2.292183125973679e-05
Step: 4970, train/learning_rate: 4.408614768180996e-05
Step: 4970, train/epoch: 1.1827701330184937
Step: 4980, train/loss: 0.07660000026226044
Step: 4980, train/grad_norm: 8.090241671032672e-11
Step: 4980, train/learning_rate: 4.407425149111077e-05
Step: 4980, train/epoch: 1.185149908065796
Step: 4990, train/loss: 0.0
Step: 4990, train/grad_norm: 9.393695821202641e-10
Step: 4990, train/learning_rate: 4.4062351662432775e-05
Step: 4990, train/epoch: 1.1875298023223877
Step: 5000, train/loss: 0.0
Step: 5000, train/grad_norm: 0.00017200756701640785
Step: 5000, train/learning_rate: 4.405045183375478e-05
Step: 5000, train/epoch: 1.18990957736969
Step: 5010, train/loss: 0.0
Step: 5010, train/grad_norm: 0.14363911747932434
Step: 5010, train/learning_rate: 4.403855200507678e-05
Step: 5010, train/epoch: 1.1922893524169922
Step: 5020, train/loss: 0.0
Step: 5020, train/grad_norm: 2.069089305223315e-06
Step: 5020, train/learning_rate: 4.4026652176398784e-05
Step: 5020, train/epoch: 1.194669246673584
Step: 5030, train/loss: 0.0
Step: 5030, train/grad_norm: 5.686871645593783e-06
Step: 5030, train/learning_rate: 4.4014755985699594e-05
Step: 5030, train/epoch: 1.1970490217208862
Step: 5040, train/loss: 0.0
Step: 5040, train/grad_norm: 1.1935188393152885e-09
Step: 5040, train/learning_rate: 4.40028561570216e-05
Step: 5040, train/epoch: 1.1994287967681885
Step: 5050, train/loss: 0.03400000184774399
Step: 5050, train/grad_norm: 1.5215395610113092e-09
Step: 5050, train/learning_rate: 4.39909563283436e-05
Step: 5050, train/epoch: 1.2018086910247803
Step: 5060, train/loss: 0.0
Step: 5060, train/grad_norm: 0.0002114517701556906
Step: 5060, train/learning_rate: 4.39790564996656e-05
Step: 5060, train/epoch: 1.2041884660720825
Step: 5070, train/loss: 0.0
Step: 5070, train/grad_norm: 6.6679333166064225e-09
Step: 5070, train/learning_rate: 4.396716030896641e-05
Step: 5070, train/epoch: 1.2065683603286743
Step: 5080, train/loss: 0.0
Step: 5080, train/grad_norm: 1.1912915987011274e-08
Step: 5080, train/learning_rate: 4.3955260480288416e-05
Step: 5080, train/epoch: 1.2089481353759766
Step: 5090, train/loss: 0.07339999824762344
Step: 5090, train/grad_norm: 386.2928466796875
Step: 5090, train/learning_rate: 4.394336065161042e-05
Step: 5090, train/epoch: 1.2113279104232788
Step: 5100, train/loss: 0.0
Step: 5100, train/grad_norm: 9.370731390845322e-08
Step: 5100, train/learning_rate: 4.393146082293242e-05
Step: 5100, train/epoch: 1.2137078046798706
Step: 5110, train/loss: 0.0
Step: 5110, train/grad_norm: 7.0636576587901345e-09
Step: 5110, train/learning_rate: 4.3919560994254425e-05
Step: 5110, train/epoch: 1.2160875797271729
Step: 5120, train/loss: 0.0
Step: 5120, train/grad_norm: 1.1480818102427293e-05
Step: 5120, train/learning_rate: 4.3907664803555235e-05
Step: 5120, train/epoch: 1.218467354774475
Step: 5130, train/loss: 0.0
Step: 5130, train/grad_norm: 4.1469311895525607e-07
Step: 5130, train/learning_rate: 4.389576497487724e-05
Step: 5130, train/epoch: 1.220847249031067
Step: 5140, train/loss: 0.0
Step: 5140, train/grad_norm: 3.306424911642125e-09
Step: 5140, train/learning_rate: 4.388386514619924e-05
Step: 5140, train/epoch: 1.2232270240783691
Step: 5150, train/loss: 0.0
Step: 5150, train/grad_norm: 1.6601064999122173e-05
Step: 5150, train/learning_rate: 4.3871965317521244e-05
Step: 5150, train/epoch: 1.2256067991256714
Step: 5160, train/loss: 0.11330000311136246
Step: 5160, train/grad_norm: 5.21278877840814e-07
Step: 5160, train/learning_rate: 4.386006548884325e-05
Step: 5160, train/epoch: 1.2279866933822632
Step: 5170, train/loss: 0.0
Step: 5170, train/grad_norm: 1.7471935507273884e-06
Step: 5170, train/learning_rate: 4.384816929814406e-05
Step: 5170, train/epoch: 1.2303664684295654
Step: 5180, train/loss: 0.010200000368058681
Step: 5180, train/grad_norm: 1.0751814016884964e-07
Step: 5180, train/learning_rate: 4.383626946946606e-05
Step: 5180, train/epoch: 1.2327463626861572
Step: 5190, train/loss: 0.0
Step: 5190, train/grad_norm: 2.6737710868474096e-05
Step: 5190, train/learning_rate: 4.3824369640788063e-05
Step: 5190, train/epoch: 1.2351261377334595
Step: 5200, train/loss: 0.5436999797821045
Step: 5200, train/grad_norm: 8.694100870343391e-06
Step: 5200, train/learning_rate: 4.3812469812110066e-05
Step: 5200, train/epoch: 1.2375059127807617
Step: 5210, train/loss: 0.0
Step: 5210, train/grad_norm: 4.42329735506064e-07
Step: 5210, train/learning_rate: 4.380056998343207e-05
Step: 5210, train/epoch: 1.2398858070373535
Step: 5220, train/loss: 9.999999747378752e-05
Step: 5220, train/grad_norm: 5.885250953241439e-09
Step: 5220, train/learning_rate: 4.378867379273288e-05
Step: 5220, train/epoch: 1.2422655820846558
Step: 5230, train/loss: 0.1477999985218048
Step: 5230, train/grad_norm: 0.0021070947404950857
Step: 5230, train/learning_rate: 4.377677396405488e-05
Step: 5230, train/epoch: 1.244645357131958
Step: 5240, train/loss: 9.999999747378752e-05
Step: 5240, train/grad_norm: 9.632518049329519e-05
Step: 5240, train/learning_rate: 4.3764874135376886e-05
Step: 5240, train/epoch: 1.2470252513885498
Step: 5250, train/loss: 0.0
Step: 5250, train/grad_norm: 7.454842965159969e-09
Step: 5250, train/learning_rate: 4.375297430669889e-05
Step: 5250, train/epoch: 1.249405026435852
Step: 5260, train/loss: 0.00019999999494757503
Step: 5260, train/grad_norm: 3.7444522149598924e-06
Step: 5260, train/learning_rate: 4.374107447802089e-05
Step: 5260, train/epoch: 1.2517849206924438
Step: 5270, train/loss: 0.0
Step: 5270, train/grad_norm: 1.663898185899626e-10
Step: 5270, train/learning_rate: 4.37291782873217e-05
Step: 5270, train/epoch: 1.254164695739746
Step: 5280, train/loss: 0.0
Step: 5280, train/grad_norm: 6.646154182554653e-10
Step: 5280, train/learning_rate: 4.3717278458643705e-05
Step: 5280, train/epoch: 1.2565444707870483
Step: 5290, train/loss: 0.0
Step: 5290, train/grad_norm: 2.2211136752048333e-07
Step: 5290, train/learning_rate: 4.370537862996571e-05
Step: 5290, train/epoch: 1.2589243650436401
Step: 5300, train/loss: 0.0
Step: 5300, train/grad_norm: 2.032903090309901e-08
Step: 5300, train/learning_rate: 4.369347880128771e-05
Step: 5300, train/epoch: 1.2613041400909424
Step: 5310, train/loss: 0.0
Step: 5310, train/grad_norm: 1.3475373350502196e-07
Step: 5310, train/learning_rate: 4.3681578972609714e-05
Step: 5310, train/epoch: 1.2636839151382446
Step: 5320, train/loss: 0.0
Step: 5320, train/grad_norm: 2.540733456957156e-10
Step: 5320, train/learning_rate: 4.3669682781910524e-05
Step: 5320, train/epoch: 1.2660638093948364
Step: 5330, train/loss: 0.043699998408555984
Step: 5330, train/grad_norm: 1.277528554055607e-06
Step: 5330, train/learning_rate: 4.365778295323253e-05
Step: 5330, train/epoch: 1.2684435844421387
Step: 5340, train/loss: 0.17710000276565552
Step: 5340, train/grad_norm: 8.592840572418936e-07
Step: 5340, train/learning_rate: 4.364588312455453e-05
Step: 5340, train/epoch: 1.270823359489441
Step: 5350, train/loss: 0.0
Step: 5350, train/grad_norm: 9.586190348465085e-13
Step: 5350, train/learning_rate: 4.363398329587653e-05
Step: 5350, train/epoch: 1.2732032537460327
Step: 5360, train/loss: 0.08479999750852585
Step: 5360, train/grad_norm: 6.877034497421164e-10
Step: 5360, train/learning_rate: 4.3622083467198536e-05
Step: 5360, train/epoch: 1.275583028793335
Step: 5370, train/loss: 9.999999747378752e-05
Step: 5370, train/grad_norm: 5.088324872559724e-09
Step: 5370, train/learning_rate: 4.3610187276499346e-05
Step: 5370, train/epoch: 1.2779629230499268
Step: 5380, train/loss: 0.0
Step: 5380, train/grad_norm: 1.284907513721123e-09
Step: 5380, train/learning_rate: 4.359828744782135e-05
Step: 5380, train/epoch: 1.280342698097229
Step: 5390, train/loss: 0.49219998717308044
Step: 5390, train/grad_norm: 8.17110333173332e-07
Step: 5390, train/learning_rate: 4.358638761914335e-05
Step: 5390, train/epoch: 1.2827224731445312
Step: 5400, train/loss: 0.1453000009059906
Step: 5400, train/grad_norm: 1.532356321831685e-07
Step: 5400, train/learning_rate: 4.3574487790465355e-05
Step: 5400, train/epoch: 1.285102367401123
Step: 5410, train/loss: 0.0
Step: 5410, train/grad_norm: 0.00121961603872478
Step: 5410, train/learning_rate: 4.356258796178736e-05
Step: 5410, train/epoch: 1.2874821424484253
Step: 5420, train/loss: 0.09960000216960907
Step: 5420, train/grad_norm: 1.2035837244184222e-05
Step: 5420, train/learning_rate: 4.355069177108817e-05
Step: 5420, train/epoch: 1.2898619174957275
Step: 5430, train/loss: 0.10130000114440918
Step: 5430, train/grad_norm: 7.059019117150456e-05
Step: 5430, train/learning_rate: 4.353879194241017e-05
Step: 5430, train/epoch: 1.2922418117523193
Step: 5440, train/loss: 0.0
Step: 5440, train/grad_norm: 0.004040543921291828
Step: 5440, train/learning_rate: 4.3526892113732174e-05
Step: 5440, train/epoch: 1.2946215867996216
Step: 5450, train/loss: 0.020899999886751175
Step: 5450, train/grad_norm: 8.694571442902088e-05
Step: 5450, train/learning_rate: 4.351499228505418e-05
Step: 5450, train/epoch: 1.2970014810562134
Step: 5460, train/loss: 0.019300000742077827
Step: 5460, train/grad_norm: 0.00015221476496662945
Step: 5460, train/learning_rate: 4.350309245637618e-05
Step: 5460, train/epoch: 1.2993812561035156
Step: 5470, train/loss: 0.06639999896287918
Step: 5470, train/grad_norm: 0.0005343368975445628
Step: 5470, train/learning_rate: 4.349119626567699e-05
Step: 5470, train/epoch: 1.3017610311508179
Step: 5480, train/loss: 0.0003000000142492354
Step: 5480, train/grad_norm: 0.0038276577834039927
Step: 5480, train/learning_rate: 4.347929643699899e-05
Step: 5480, train/epoch: 1.3041409254074097
Step: 5490, train/loss: 0.0013000000035390258
Step: 5490, train/grad_norm: 0.004577383864670992
Step: 5490, train/learning_rate: 4.3467396608320996e-05
Step: 5490, train/epoch: 1.306520700454712
Step: 5500, train/loss: 0.0
Step: 5500, train/grad_norm: 0.00016376843268517405
Step: 5500, train/learning_rate: 4.3455496779643e-05
Step: 5500, train/epoch: 1.3089004755020142
Step: 5510, train/loss: 0.1282999962568283
Step: 5510, train/grad_norm: 131.55313110351562
Step: 5510, train/learning_rate: 4.3443596950965e-05
Step: 5510, train/epoch: 1.311280369758606
Step: 5520, train/loss: 0.0
Step: 5520, train/grad_norm: 1.8086943498474284e-07
Step: 5520, train/learning_rate: 4.343170076026581e-05
Step: 5520, train/epoch: 1.3136601448059082
Step: 5530, train/loss: 0.8640999794006348
Step: 5530, train/grad_norm: 4.255096791894175e-05
Step: 5530, train/learning_rate: 4.3419800931587815e-05
Step: 5530, train/epoch: 1.3160400390625
Step: 5540, train/loss: 0.0
Step: 5540, train/grad_norm: 4.176321817794815e-05
Step: 5540, train/learning_rate: 4.340790110290982e-05
Step: 5540, train/epoch: 1.3184198141098022
Step: 5550, train/loss: 0.125
Step: 5550, train/grad_norm: 0.057206083089113235
Step: 5550, train/learning_rate: 4.339600127423182e-05
Step: 5550, train/epoch: 1.3207995891571045
Step: 5560, train/loss: 0.10859999805688858
Step: 5560, train/grad_norm: 0.00035667416523210704
Step: 5560, train/learning_rate: 4.3384101445553824e-05
Step: 5560, train/epoch: 1.3231794834136963
Step: 5570, train/loss: 9.999999747378752e-05
Step: 5570, train/grad_norm: 6.010377546772361e-05
Step: 5570, train/learning_rate: 4.3372205254854634e-05
Step: 5570, train/epoch: 1.3255592584609985
Step: 5580, train/loss: 9.999999747378752e-05
Step: 5580, train/grad_norm: 0.004853495396673679
Step: 5580, train/learning_rate: 4.336030542617664e-05
Step: 5580, train/epoch: 1.3279390335083008
Step: 5590, train/loss: 0.02759999968111515
Step: 5590, train/grad_norm: 0.00046859876601956785
Step: 5590, train/learning_rate: 4.334840559749864e-05
Step: 5590, train/epoch: 1.3303189277648926
Step: 5600, train/loss: 0.0007999999797903001
Step: 5600, train/grad_norm: 4.390074082039064e-06
Step: 5600, train/learning_rate: 4.3336505768820643e-05
Step: 5600, train/epoch: 1.3326987028121948
Step: 5610, train/loss: 0.0
Step: 5610, train/grad_norm: 5.866407377652649e-07
Step: 5610, train/learning_rate: 4.3324605940142646e-05
Step: 5610, train/epoch: 1.335078477859497
Step: 5620, train/loss: 0.0
Step: 5620, train/grad_norm: 0.00026558066019788384
Step: 5620, train/learning_rate: 4.3312709749443457e-05
Step: 5620, train/epoch: 1.3374583721160889
Step: 5630, train/loss: 0.0
Step: 5630, train/grad_norm: 1.0636250991069574e-08
Step: 5630, train/learning_rate: 4.330080992076546e-05
Step: 5630, train/epoch: 1.3398381471633911
Step: 5640, train/loss: 0.0
Step: 5640, train/grad_norm: 1.1194754279131303e-07
Step: 5640, train/learning_rate: 4.328891009208746e-05
Step: 5640, train/epoch: 1.342218041419983
Step: 5650, train/loss: 0.0
Step: 5650, train/grad_norm: 1.0570193609282796e-07
Step: 5650, train/learning_rate: 4.3277010263409466e-05
Step: 5650, train/epoch: 1.3445978164672852
Step: 5660, train/loss: 0.0
Step: 5660, train/grad_norm: 2.8947317787242355e-07
Step: 5660, train/learning_rate: 4.326511043473147e-05
Step: 5660, train/epoch: 1.3469775915145874
Step: 5670, train/loss: 0.00039999998989515007
Step: 5670, train/grad_norm: 41.20400619506836
Step: 5670, train/learning_rate: 4.325321424403228e-05
Step: 5670, train/epoch: 1.3493574857711792
Step: 5680, train/loss: 0.11330000311136246
Step: 5680, train/grad_norm: 7.959775416566117e-08
Step: 5680, train/learning_rate: 4.324131441535428e-05
Step: 5680, train/epoch: 1.3517372608184814
Step: 5690, train/loss: 0.0
Step: 5690, train/grad_norm: 7.130009294087358e-08
Step: 5690, train/learning_rate: 4.3229414586676285e-05
Step: 5690, train/epoch: 1.3541170358657837
Step: 5700, train/loss: 0.0
Step: 5700, train/grad_norm: 3.7372901995347263e-10
Step: 5700, train/learning_rate: 4.321751475799829e-05
Step: 5700, train/epoch: 1.3564969301223755
Step: 5710, train/loss: 0.0
Step: 5710, train/grad_norm: 1.2876233768110978e-06
Step: 5710, train/learning_rate: 4.320561492932029e-05
Step: 5710, train/epoch: 1.3588767051696777
Step: 5720, train/loss: 0.0
Step: 5720, train/grad_norm: 4.8702350596840915e-08
Step: 5720, train/learning_rate: 4.31937187386211e-05
Step: 5720, train/epoch: 1.3612565994262695
Step: 5730, train/loss: 0.0
Step: 5730, train/grad_norm: 1.9679314391396474e-06
Step: 5730, train/learning_rate: 4.3181818909943104e-05
Step: 5730, train/epoch: 1.3636363744735718
Step: 5740, train/loss: 0.07109999656677246
Step: 5740, train/grad_norm: 4.9038668237244565e-08
Step: 5740, train/learning_rate: 4.316991908126511e-05
Step: 5740, train/epoch: 1.366016149520874
Step: 5750, train/loss: 0.1080000028014183
Step: 5750, train/grad_norm: 6.095158937569067e-07
Step: 5750, train/learning_rate: 4.315801925258711e-05
Step: 5750, train/epoch: 1.3683960437774658
Step: 5760, train/loss: 0.1770000010728836
Step: 5760, train/grad_norm: 308.1015319824219
Step: 5760, train/learning_rate: 4.314611942390911e-05
Step: 5760, train/epoch: 1.370775818824768
Step: 5770, train/loss: 0.005100000184029341
Step: 5770, train/grad_norm: 0.005141571629792452
Step: 5770, train/learning_rate: 4.313422323320992e-05
Step: 5770, train/epoch: 1.3731555938720703
Step: 5780, train/loss: 0.053599998354911804
Step: 5780, train/grad_norm: 0.0006188864936120808
Step: 5780, train/learning_rate: 4.3122323404531926e-05
Step: 5780, train/epoch: 1.375535488128662
Step: 5790, train/loss: 0.0006000000284984708
Step: 5790, train/grad_norm: 8.050580024719238
Step: 5790, train/learning_rate: 4.311042357585393e-05
Step: 5790, train/epoch: 1.3779152631759644
Step: 5800, train/loss: 0.07270000129938126
Step: 5800, train/grad_norm: 8.827241254039109e-05
Step: 5800, train/learning_rate: 4.309852374717593e-05
Step: 5800, train/epoch: 1.3802950382232666
Step: 5810, train/loss: 0.0
Step: 5810, train/grad_norm: 6.8644032580778e-05
Step: 5810, train/learning_rate: 4.3086623918497935e-05
Step: 5810, train/epoch: 1.3826749324798584
Step: 5820, train/loss: 9.999999747378752e-05
Step: 5820, train/grad_norm: 5.2515362767735496e-05
Step: 5820, train/learning_rate: 4.3074727727798745e-05
Step: 5820, train/epoch: 1.3850547075271606
Step: 5830, train/loss: 0.008700000122189522
Step: 5830, train/grad_norm: 1.4330900739878416e-05
Step: 5830, train/learning_rate: 4.306282789912075e-05
Step: 5830, train/epoch: 1.3874346017837524
Step: 5840, train/loss: 0.0003000000142492354
Step: 5840, train/grad_norm: 2.8227643156242266e-07
Step: 5840, train/learning_rate: 4.305092807044275e-05
Step: 5840, train/epoch: 1.3898143768310547
Step: 5850, train/loss: 0.0
Step: 5850, train/grad_norm: 7.771604941808619e-06
Step: 5850, train/learning_rate: 4.3039028241764754e-05
Step: 5850, train/epoch: 1.392194151878357
Step: 5860, train/loss: 0.0
Step: 5860, train/grad_norm: 1.6645423102090717e-06
Step: 5860, train/learning_rate: 4.302712841308676e-05
Step: 5860, train/epoch: 1.3945740461349487
Step: 5870, train/loss: 0.0
Step: 5870, train/grad_norm: 1.3946271337772487e-07
Step: 5870, train/learning_rate: 4.301523222238757e-05
Step: 5870, train/epoch: 1.396953821182251
Step: 5880, train/loss: 0.0
Step: 5880, train/grad_norm: 6.125939307821682e-06
Step: 5880, train/learning_rate: 4.300333239370957e-05
Step: 5880, train/epoch: 1.3993335962295532
Step: 5890, train/loss: 0.0
Step: 5890, train/grad_norm: 6.048231171007501e-06
Step: 5890, train/learning_rate: 4.299143256503157e-05
Step: 5890, train/epoch: 1.401713490486145
Step: 5900, train/loss: 0.16019999980926514
Step: 5900, train/grad_norm: 9.979309822938376e-09
Step: 5900, train/learning_rate: 4.2979532736353576e-05
Step: 5900, train/epoch: 1.4040932655334473
Step: 5910, train/loss: 0.0
Step: 5910, train/grad_norm: 1.4040213436317117e-09
Step: 5910, train/learning_rate: 4.296763290767558e-05
Step: 5910, train/epoch: 1.406473159790039
Step: 5920, train/loss: 0.0
Step: 5920, train/grad_norm: 5.613221105704724e-07
Step: 5920, train/learning_rate: 4.295573671697639e-05
Step: 5920, train/epoch: 1.4088529348373413
Step: 5930, train/loss: 0.0
Step: 5930, train/grad_norm: 3.4999050324557857e-09
Step: 5930, train/learning_rate: 4.294383688829839e-05
Step: 5930, train/epoch: 1.4112327098846436
Step: 5940, train/loss: 0.2190999984741211
Step: 5940, train/grad_norm: 1.3609071174869314e-05
Step: 5940, train/learning_rate: 4.2931937059620395e-05
Step: 5940, train/epoch: 1.4136126041412354
Step: 5950, train/loss: 9.999999747378752e-05
Step: 5950, train/grad_norm: 3.607551336288452
Step: 5950, train/learning_rate: 4.29200372309424e-05
Step: 5950, train/epoch: 1.4159923791885376
Step: 5960, train/loss: 0.0
Step: 5960, train/grad_norm: 0.0007185607100836933
Step: 5960, train/learning_rate: 4.29081374022644e-05
Step: 5960, train/epoch: 1.4183721542358398
Step: 5970, train/loss: 9.999999747378752e-05
Step: 5970, train/grad_norm: 1.5571438780170865e-05
Step: 5970, train/learning_rate: 4.289624121156521e-05
Step: 5970, train/epoch: 1.4207520484924316
Step: 5980, train/loss: 0.0
Step: 5980, train/grad_norm: 7.981811359059066e-05
Step: 5980, train/learning_rate: 4.2884341382887214e-05
Step: 5980, train/epoch: 1.4231318235397339
Step: 5990, train/loss: 0.004699999932199717
Step: 5990, train/grad_norm: 238.91575622558594
Step: 5990, train/learning_rate: 4.287244155420922e-05
Step: 5990, train/epoch: 1.4255117177963257
Step: 6000, train/loss: 0.0
Step: 6000, train/grad_norm: 1.717437839943159e-06
Step: 6000, train/learning_rate: 4.286054172553122e-05
Step: 6000, train/epoch: 1.427891492843628
Step: 6010, train/loss: 0.11249999701976776
Step: 6010, train/grad_norm: 8.934114157455042e-05
Step: 6010, train/learning_rate: 4.2848641896853223e-05
Step: 6010, train/epoch: 1.4302712678909302
Step: 6020, train/loss: 0.0
Step: 6020, train/grad_norm: 0.00014092140190768987
Step: 6020, train/learning_rate: 4.2836745706154034e-05
Step: 6020, train/epoch: 1.432651162147522
Step: 6030, train/loss: 0.5
Step: 6030, train/grad_norm: 94.53659057617188
Step: 6030, train/learning_rate: 4.2824845877476037e-05
Step: 6030, train/epoch: 1.4350309371948242
Step: 6040, train/loss: 9.999999747378752e-05
Step: 6040, train/grad_norm: 0.017761508002877235
Step: 6040, train/learning_rate: 4.281294604879804e-05
Step: 6040, train/epoch: 1.4374107122421265
Step: 6050, train/loss: 0.002899999963119626
Step: 6050, train/grad_norm: 0.010315781459212303
Step: 6050, train/learning_rate: 4.280104622012004e-05
Step: 6050, train/epoch: 1.4397906064987183
Step: 6060, train/loss: 0.048900000751018524
Step: 6060, train/grad_norm: 0.09732837975025177
Step: 6060, train/learning_rate: 4.2789146391442046e-05
Step: 6060, train/epoch: 1.4421703815460205
Step: 6070, train/loss: 0.0005000000237487257
Step: 6070, train/grad_norm: 0.0006561885820701718
Step: 6070, train/learning_rate: 4.2777250200742856e-05
Step: 6070, train/epoch: 1.4445501565933228
Step: 6080, train/loss: 0.0
Step: 6080, train/grad_norm: 0.0001123625916079618
Step: 6080, train/learning_rate: 4.276535037206486e-05
Step: 6080, train/epoch: 1.4469300508499146
Step: 6090, train/loss: 0.0
Step: 6090, train/grad_norm: 3.1166600820142776e-05
Step: 6090, train/learning_rate: 4.275345054338686e-05
Step: 6090, train/epoch: 1.4493098258972168
Step: 6100, train/loss: 0.0
Step: 6100, train/grad_norm: 5.1030317990807816e-05
Step: 6100, train/learning_rate: 4.2741550714708865e-05
Step: 6100, train/epoch: 1.4516897201538086
Step: 6110, train/loss: 0.0
Step: 6110, train/grad_norm: 0.0005017632502131164
Step: 6110, train/learning_rate: 4.272965088603087e-05
Step: 6110, train/epoch: 1.4540694952011108
Step: 6120, train/loss: 0.0
Step: 6120, train/grad_norm: 1.878756665973924e-05
Step: 6120, train/learning_rate: 4.271775469533168e-05
Step: 6120, train/epoch: 1.456449270248413
Step: 6130, train/loss: 0.0
Step: 6130, train/grad_norm: 1.6592195606790483e-05
Step: 6130, train/learning_rate: 4.270585486665368e-05
Step: 6130, train/epoch: 1.4588291645050049
Step: 6140, train/loss: 0.0
Step: 6140, train/grad_norm: 2.0674608094850555e-05
Step: 6140, train/learning_rate: 4.2693955037975684e-05
Step: 6140, train/epoch: 1.4612089395523071
Step: 6150, train/loss: 0.27000001072883606
Step: 6150, train/grad_norm: 13.822877883911133
Step: 6150, train/learning_rate: 4.268205520929769e-05
Step: 6150, train/epoch: 1.4635887145996094
Step: 6160, train/loss: 0.0005000000237487257
Step: 6160, train/grad_norm: 0.0013128940481692553
Step: 6160, train/learning_rate: 4.267015538061969e-05
Step: 6160, train/epoch: 1.4659686088562012
Step: 6170, train/loss: 0.1657000035047531
Step: 6170, train/grad_norm: 0.5158429145812988
Step: 6170, train/learning_rate: 4.26582591899205e-05
Step: 6170, train/epoch: 1.4683483839035034
Step: 6180, train/loss: 0.030899999663233757
Step: 6180, train/grad_norm: 0.025119230151176453
Step: 6180, train/learning_rate: 4.26463593612425e-05
Step: 6180, train/epoch: 1.4707282781600952
Step: 6190, train/loss: 0.17579999566078186
Step: 6190, train/grad_norm: 0.0008584739989601076
Step: 6190, train/learning_rate: 4.2634459532564506e-05
Step: 6190, train/epoch: 1.4731080532073975
Step: 6200, train/loss: 0.0066999997943639755
Step: 6200, train/grad_norm: 0.044710174202919006
Step: 6200, train/learning_rate: 4.262255970388651e-05
Step: 6200, train/epoch: 1.4754878282546997
Step: 6210, train/loss: 0.07079999893903732
Step: 6210, train/grad_norm: 0.0008614470134489238
Step: 6210, train/learning_rate: 4.261065987520851e-05
Step: 6210, train/epoch: 1.4778677225112915
Step: 6220, train/loss: 0.13609999418258667
Step: 6220, train/grad_norm: 0.0012802882120013237
Step: 6220, train/learning_rate: 4.259876368450932e-05
Step: 6220, train/epoch: 1.4802474975585938
Step: 6230, train/loss: 0.002199999988079071
Step: 6230, train/grad_norm: 0.07645127177238464
Step: 6230, train/learning_rate: 4.2586863855831325e-05
Step: 6230, train/epoch: 1.482627272605896
Step: 6240, train/loss: 0.00039999998989515007
Step: 6240, train/grad_norm: 0.00024131003010552377
Step: 6240, train/learning_rate: 4.257496402715333e-05
Step: 6240, train/epoch: 1.4850071668624878
Step: 6250, train/loss: 0.002400000113993883
Step: 6250, train/grad_norm: 0.0007104355026967824
Step: 6250, train/learning_rate: 4.256306419847533e-05
Step: 6250, train/epoch: 1.48738694190979
Step: 6260, train/loss: 0.1031000018119812
Step: 6260, train/grad_norm: 81.1593017578125
Step: 6260, train/learning_rate: 4.2551164369797334e-05
Step: 6260, train/epoch: 1.4897668361663818
Step: 6270, train/loss: 0.009800000116229057
Step: 6270, train/grad_norm: 37.14509582519531
Step: 6270, train/learning_rate: 4.2539268179098144e-05
Step: 6270, train/epoch: 1.492146611213684
Step: 6280, train/loss: 9.999999747378752e-05
Step: 6280, train/grad_norm: 0.0004638239915948361
Step: 6280, train/learning_rate: 4.252736835042015e-05
Step: 6280, train/epoch: 1.4945263862609863
Step: 6290, train/loss: 0.328000009059906
Step: 6290, train/grad_norm: 0.03407131880521774
Step: 6290, train/learning_rate: 4.251546852174215e-05
Step: 6290, train/epoch: 1.4969062805175781
Step: 6300, train/loss: 0.0031999999191612005
Step: 6300, train/grad_norm: 0.0008347384282387793
Step: 6300, train/learning_rate: 4.250356869306415e-05
Step: 6300, train/epoch: 1.4992860555648804
Step: 6310, train/loss: 0.0
Step: 6310, train/grad_norm: 0.0045906552113592625
Step: 6310, train/learning_rate: 4.2491668864386156e-05
Step: 6310, train/epoch: 1.5016658306121826
Step: 6320, train/loss: 0.0
Step: 6320, train/grad_norm: 1.0127276084404002e-07
Step: 6320, train/learning_rate: 4.2479772673686966e-05
Step: 6320, train/epoch: 1.5040457248687744
Step: 6330, train/loss: 0.0017000000225380063
Step: 6330, train/grad_norm: 1.6508642147528008e-05
Step: 6330, train/learning_rate: 4.246787284500897e-05
Step: 6330, train/epoch: 1.5064254999160767
Step: 6340, train/loss: 0.0
Step: 6340, train/grad_norm: 1.288473231397802e-05
Step: 6340, train/learning_rate: 4.245597301633097e-05
Step: 6340, train/epoch: 1.508805274963379
Step: 6350, train/loss: 0.0024999999441206455
Step: 6350, train/grad_norm: 6.322749435128117e-09
Step: 6350, train/learning_rate: 4.2444073187652975e-05
Step: 6350, train/epoch: 1.5111851692199707
Step: 6360, train/loss: 0.0
Step: 6360, train/grad_norm: 1.416990436098331e-08
Step: 6360, train/learning_rate: 4.243217335897498e-05
Step: 6360, train/epoch: 1.513564944267273
Step: 6370, train/loss: 0.0
Step: 6370, train/grad_norm: 5.393967512645759e-07
Step: 6370, train/learning_rate: 4.242027716827579e-05
Step: 6370, train/epoch: 1.5159448385238647
Step: 6380, train/loss: 0.0
Step: 6380, train/grad_norm: 6.780662253902392e-09
Step: 6380, train/learning_rate: 4.240837733959779e-05
Step: 6380, train/epoch: 1.518324613571167
Step: 6390, train/loss: 0.0
Step: 6390, train/grad_norm: 3.266244675614871e-05
Step: 6390, train/learning_rate: 4.2396477510919794e-05
Step: 6390, train/epoch: 1.5207043886184692
Step: 6400, train/loss: 0.09139999747276306
Step: 6400, train/grad_norm: 327.71551513671875
Step: 6400, train/learning_rate: 4.23845776822418e-05
Step: 6400, train/epoch: 1.523084282875061
Step: 6410, train/loss: 0.0
Step: 6410, train/grad_norm: 6.715520157740684e-06
Step: 6410, train/learning_rate: 4.237268149154261e-05
Step: 6410, train/epoch: 1.5254640579223633
Step: 6420, train/loss: 9.999999747378752e-05
Step: 6420, train/grad_norm: 3.2079159328901596e-09
Step: 6420, train/learning_rate: 4.236078166286461e-05
Step: 6420, train/epoch: 1.5278438329696655
Step: 6430, train/loss: 0.0
Step: 6430, train/grad_norm: 7.177290317628149e-09
Step: 6430, train/learning_rate: 4.2348881834186614e-05
Step: 6430, train/epoch: 1.5302237272262573
Step: 6440, train/loss: 0.0
Step: 6440, train/grad_norm: 6.831935017714841e-10
Step: 6440, train/learning_rate: 4.2336982005508617e-05
Step: 6440, train/epoch: 1.5326035022735596
Step: 6450, train/loss: 0.0
Step: 6450, train/grad_norm: 5.203981134904723e-10
Step: 6450, train/learning_rate: 4.232508217683062e-05
Step: 6450, train/epoch: 1.5349833965301514
Step: 6460, train/loss: 0.26350000500679016
Step: 6460, train/grad_norm: 6.742994173691841e-06
Step: 6460, train/learning_rate: 4.231318598613143e-05
Step: 6460, train/epoch: 1.5373631715774536
Step: 6470, train/loss: 0.24699999392032623
Step: 6470, train/grad_norm: 24.548311233520508
Step: 6470, train/learning_rate: 4.230128615745343e-05
Step: 6470, train/epoch: 1.5397429466247559
Step: 6480, train/loss: 0.2021999955177307
Step: 6480, train/grad_norm: 333.9661865234375
Step: 6480, train/learning_rate: 4.2289386328775436e-05
Step: 6480, train/epoch: 1.5421228408813477
Step: 6490, train/loss: 0.11840000003576279
Step: 6490, train/grad_norm: 0.00010154510528082028
Step: 6490, train/learning_rate: 4.227748650009744e-05
Step: 6490, train/epoch: 1.54450261592865
Step: 6500, train/loss: 0.10939999669790268
Step: 6500, train/grad_norm: 50.913116455078125
Step: 6500, train/learning_rate: 4.226558667141944e-05
Step: 6500, train/epoch: 1.5468823909759521
Step: 6510, train/loss: 0.0032999999821186066
Step: 6510, train/grad_norm: 0.004620672203600407
Step: 6510, train/learning_rate: 4.225369048072025e-05
Step: 6510, train/epoch: 1.549262285232544
Step: 6520, train/loss: 0.012500000186264515
Step: 6520, train/grad_norm: 0.0024088898207992315
Step: 6520, train/learning_rate: 4.2241790652042255e-05
Step: 6520, train/epoch: 1.5516420602798462
Step: 6530, train/loss: 0.0
Step: 6530, train/grad_norm: 0.002699327189475298
Step: 6530, train/learning_rate: 4.222989082336426e-05
Step: 6530, train/epoch: 1.5540218353271484
Step: 6540, train/loss: 0.0
Step: 6540, train/grad_norm: 3.447822746238671e-05
Step: 6540, train/learning_rate: 4.221799099468626e-05
Step: 6540, train/epoch: 1.5564017295837402
Step: 6550, train/loss: 0.0027000000700354576
Step: 6550, train/grad_norm: 0.0011964516015723348
Step: 6550, train/learning_rate: 4.2206091166008264e-05
Step: 6550, train/epoch: 1.5587815046310425
Step: 6560, train/loss: 0.003700000001117587
Step: 6560, train/grad_norm: 7.91115013498711e-08
Step: 6560, train/learning_rate: 4.2194194975309074e-05
Step: 6560, train/epoch: 1.5611613988876343
Step: 6570, train/loss: 0.0008999999845400453
Step: 6570, train/grad_norm: 0.009822933934628963
Step: 6570, train/learning_rate: 4.218229514663108e-05
Step: 6570, train/epoch: 1.5635411739349365
Step: 6580, train/loss: 0.04479999840259552
Step: 6580, train/grad_norm: 0.004731999710202217
Step: 6580, train/learning_rate: 4.217039531795308e-05
Step: 6580, train/epoch: 1.5659209489822388
Step: 6590, train/loss: 0.1412999927997589
Step: 6590, train/grad_norm: 4.124201495869784e-06
Step: 6590, train/learning_rate: 4.215849548927508e-05
Step: 6590, train/epoch: 1.5683008432388306
Step: 6600, train/loss: 0.09809999912977219
Step: 6600, train/grad_norm: 0.0004054904857184738
Step: 6600, train/learning_rate: 4.2146595660597086e-05
Step: 6600, train/epoch: 1.5706806182861328
Step: 6610, train/loss: 0.05869999900460243
Step: 6610, train/grad_norm: 0.0003288299485575408
Step: 6610, train/learning_rate: 4.2134699469897896e-05
Step: 6610, train/epoch: 1.573060393333435
Step: 6620, train/loss: 0.0
Step: 6620, train/grad_norm: 0.0030098827555775642
Step: 6620, train/learning_rate: 4.21227996412199e-05
Step: 6620, train/epoch: 1.5754402875900269
Step: 6630, train/loss: 0.0
Step: 6630, train/grad_norm: 4.75766995577942e-07
Step: 6630, train/learning_rate: 4.21108998125419e-05
Step: 6630, train/epoch: 1.577820062637329
Step: 6640, train/loss: 0.00019999999494757503
Step: 6640, train/grad_norm: 3.855616796499817e-06
Step: 6640, train/learning_rate: 4.2098999983863905e-05
Step: 6640, train/epoch: 1.580199956893921
Step: 6650, train/loss: 0.0
Step: 6650, train/grad_norm: 6.901879601173277e-07
Step: 6650, train/learning_rate: 4.208710015518591e-05
Step: 6650, train/epoch: 1.5825797319412231
Step: 6660, train/loss: 0.0
Step: 6660, train/grad_norm: 0.002556823194026947
Step: 6660, train/learning_rate: 4.207520396448672e-05
Step: 6660, train/epoch: 1.5849595069885254
Step: 6670, train/loss: 0.0
Step: 6670, train/grad_norm: 6.945061272745079e-07
Step: 6670, train/learning_rate: 4.206330413580872e-05
Step: 6670, train/epoch: 1.5873394012451172
Step: 6680, train/loss: 0.036400001496076584
Step: 6680, train/grad_norm: 181.88095092773438
Step: 6680, train/learning_rate: 4.2051404307130724e-05
Step: 6680, train/epoch: 1.5897191762924194
Step: 6690, train/loss: 0.11150000244379044
Step: 6690, train/grad_norm: 1.9382945026791276e-07
Step: 6690, train/learning_rate: 4.203950447845273e-05
Step: 6690, train/epoch: 1.5920989513397217
Step: 6700, train/loss: 0.0
Step: 6700, train/grad_norm: 5.8140129112871364e-05
Step: 6700, train/learning_rate: 4.202760464977473e-05
Step: 6700, train/epoch: 1.5944788455963135
Step: 6710, train/loss: 0.0
Step: 6710, train/grad_norm: 0.0020318320021033287
Step: 6710, train/learning_rate: 4.201570845907554e-05
Step: 6710, train/epoch: 1.5968586206436157
Step: 6720, train/loss: 0.005200000014156103
Step: 6720, train/grad_norm: 16.20887565612793
Step: 6720, train/learning_rate: 4.200380863039754e-05
Step: 6720, train/epoch: 1.5992385149002075
Step: 6730, train/loss: 0.0
Step: 6730, train/grad_norm: 9.627864301364752e-07
Step: 6730, train/learning_rate: 4.1991908801719546e-05
Step: 6730, train/epoch: 1.6016182899475098
Step: 6740, train/loss: 0.11869999766349792
Step: 6740, train/grad_norm: 308.680419921875
Step: 6740, train/learning_rate: 4.198000897304155e-05
Step: 6740, train/epoch: 1.603998064994812
Step: 6750, train/loss: 0.09880000352859497
Step: 6750, train/grad_norm: 0.0019829163793474436
Step: 6750, train/learning_rate: 4.196810914436355e-05
Step: 6750, train/epoch: 1.6063779592514038
Step: 6760, train/loss: 0.1354999989271164
Step: 6760, train/grad_norm: 0.0012208737898617983
Step: 6760, train/learning_rate: 4.195621295366436e-05
Step: 6760, train/epoch: 1.608757734298706
Step: 6770, train/loss: 0.10819999873638153
Step: 6770, train/grad_norm: 0.00984371267259121
Step: 6770, train/learning_rate: 4.1944313124986365e-05
Step: 6770, train/epoch: 1.6111375093460083
Step: 6780, train/loss: 0.13750000298023224
Step: 6780, train/grad_norm: 6.100680911913514e-05
Step: 6780, train/learning_rate: 4.193241329630837e-05
Step: 6780, train/epoch: 1.6135174036026
Step: 6790, train/loss: 0.10159999877214432
Step: 6790, train/grad_norm: 3.0494158636429347e-05
Step: 6790, train/learning_rate: 4.192051346763037e-05
Step: 6790, train/epoch: 1.6158971786499023
Step: 6800, train/loss: 0.0
Step: 6800, train/grad_norm: 0.022324375808238983
Step: 6800, train/learning_rate: 4.1908613638952374e-05
Step: 6800, train/epoch: 1.6182769536972046
Step: 6810, train/loss: 0.21170000731945038
Step: 6810, train/grad_norm: 0.014598826877772808
Step: 6810, train/learning_rate: 4.1896717448253185e-05
Step: 6810, train/epoch: 1.6206568479537964
Step: 6820, train/loss: 0.0019000000320374966
Step: 6820, train/grad_norm: 0.0034219452645629644
Step: 6820, train/learning_rate: 4.188481761957519e-05
Step: 6820, train/epoch: 1.6230366230010986
Step: 6830, train/loss: 9.999999747378752e-05
Step: 6830, train/grad_norm: 5.2747556765098125e-05
Step: 6830, train/learning_rate: 4.187291779089719e-05
Step: 6830, train/epoch: 1.6254165172576904
Step: 6840, train/loss: 0.16689999401569366
Step: 6840, train/grad_norm: 0.00048093817895278335
Step: 6840, train/learning_rate: 4.1861017962219194e-05
Step: 6840, train/epoch: 1.6277962923049927
Step: 6850, train/loss: 0.0
Step: 6850, train/grad_norm: 0.006333420053124428
Step: 6850, train/learning_rate: 4.1849118133541197e-05
Step: 6850, train/epoch: 1.630176067352295
Step: 6860, train/loss: 0.05920000001788139
Step: 6860, train/grad_norm: 0.00014025461860001087
Step: 6860, train/learning_rate: 4.183722194284201e-05
Step: 6860, train/epoch: 1.6325559616088867
Step: 6870, train/loss: 0.013199999928474426
Step: 6870, train/grad_norm: 0.2684842646121979
Step: 6870, train/learning_rate: 4.182532211416401e-05
Step: 6870, train/epoch: 1.634935736656189
Step: 6880, train/loss: 0.0005000000237487257
Step: 6880, train/grad_norm: 11.696074485778809
Step: 6880, train/learning_rate: 4.181342228548601e-05
Step: 6880, train/epoch: 1.6373155117034912
Step: 6890, train/loss: 0.0
Step: 6890, train/grad_norm: 2.9964716929953283e-08
Step: 6890, train/learning_rate: 4.1801522456808016e-05
Step: 6890, train/epoch: 1.639695405960083
Step: 6900, train/loss: 0.0
Step: 6900, train/grad_norm: 0.004026752896606922
Step: 6900, train/learning_rate: 4.178962262813002e-05
Step: 6900, train/epoch: 1.6420751810073853
Step: 6910, train/loss: 0.0
Step: 6910, train/grad_norm: 5.28779964952264e-05
Step: 6910, train/learning_rate: 4.177772643743083e-05
Step: 6910, train/epoch: 1.644455075263977
Step: 6920, train/loss: 0.13439999520778656
Step: 6920, train/grad_norm: 4.129114586248761e-06
Step: 6920, train/learning_rate: 4.176582660875283e-05
Step: 6920, train/epoch: 1.6468348503112793
Step: 6930, train/loss: 9.999999747378752e-05
Step: 6930, train/grad_norm: 1.4354457562149037e-05
Step: 6930, train/learning_rate: 4.1753926780074835e-05
Step: 6930, train/epoch: 1.6492146253585815
Step: 6940, train/loss: 9.999999747378752e-05
Step: 6940, train/grad_norm: 2.0710107492050156e-05
Step: 6940, train/learning_rate: 4.174202695139684e-05
Step: 6940, train/epoch: 1.6515945196151733
Step: 6950, train/loss: 0.0
Step: 6950, train/grad_norm: 1.3510081942058605e-07
Step: 6950, train/learning_rate: 4.173012712271884e-05
Step: 6950, train/epoch: 1.6539742946624756
Step: 6960, train/loss: 0.020600000396370888
Step: 6960, train/grad_norm: 1.2222786608617753e-05
Step: 6960, train/learning_rate: 4.171823093201965e-05
Step: 6960, train/epoch: 1.6563540697097778
Step: 6970, train/loss: 0.13359999656677246
Step: 6970, train/grad_norm: 0.0002941309357993305
Step: 6970, train/learning_rate: 4.1706331103341654e-05
Step: 6970, train/epoch: 1.6587339639663696
Step: 6980, train/loss: 9.999999747378752e-05
Step: 6980, train/grad_norm: 0.00486775441095233
Step: 6980, train/learning_rate: 4.169443127466366e-05
Step: 6980, train/epoch: 1.6611137390136719
Step: 6990, train/loss: 0.003800000064074993
Step: 6990, train/grad_norm: 3.363902578712441e-05
Step: 6990, train/learning_rate: 4.168253144598566e-05
Step: 6990, train/epoch: 1.6634936332702637
Step: 7000, train/loss: 0.2628999948501587
Step: 7000, train/grad_norm: 3.988342541560996e-06
Step: 7000, train/learning_rate: 4.167063161730766e-05
Step: 7000, train/epoch: 1.665873408317566
Step: 7010, train/loss: 0.0
Step: 7010, train/grad_norm: 0.00040509930113330483
Step: 7010, train/learning_rate: 4.165873542660847e-05
Step: 7010, train/epoch: 1.6682531833648682
Step: 7020, train/loss: 0.0003000000142492354
Step: 7020, train/grad_norm: 5.26763415109599e-06
Step: 7020, train/learning_rate: 4.1646835597930476e-05
Step: 7020, train/epoch: 1.67063307762146
Step: 7030, train/loss: 0.0
Step: 7030, train/grad_norm: 1.9386704934731824e-06
Step: 7030, train/learning_rate: 4.163493576925248e-05
Step: 7030, train/epoch: 1.6730128526687622
Step: 7040, train/loss: 0.1671999990940094
Step: 7040, train/grad_norm: 176.28306579589844
Step: 7040, train/learning_rate: 4.162303594057448e-05
Step: 7040, train/epoch: 1.6753926277160645
Step: 7050, train/loss: 9.999999747378752e-05
Step: 7050, train/grad_norm: 0.9905807375907898
Step: 7050, train/learning_rate: 4.1611136111896485e-05
Step: 7050, train/epoch: 1.6777725219726562
Step: 7060, train/loss: 0.0
Step: 7060, train/grad_norm: 0.00044267799239605665
Step: 7060, train/learning_rate: 4.1599239921197295e-05
Step: 7060, train/epoch: 1.6801522970199585
Step: 7070, train/loss: 0.0
Step: 7070, train/grad_norm: 2.3785615610449895e-07
Step: 7070, train/learning_rate: 4.15873400925193e-05
Step: 7070, train/epoch: 1.6825320720672607
Step: 7080, train/loss: 0.003000000026077032
Step: 7080, train/grad_norm: 8.51723541472893e-07
Step: 7080, train/learning_rate: 4.15754402638413e-05
Step: 7080, train/epoch: 1.6849119663238525
Step: 7090, train/loss: 0.0
Step: 7090, train/grad_norm: 0.00013428262900561094
Step: 7090, train/learning_rate: 4.1563540435163304e-05
Step: 7090, train/epoch: 1.6872917413711548
Step: 7100, train/loss: 0.0
Step: 7100, train/grad_norm: 3.337116822876851e-06
Step: 7100, train/learning_rate: 4.155164060648531e-05
Step: 7100, train/epoch: 1.6896716356277466
Step: 7110, train/loss: 0.0
Step: 7110, train/grad_norm: 2.4803887299640337e-07
Step: 7110, train/learning_rate: 4.153974441578612e-05
Step: 7110, train/epoch: 1.6920514106750488
Step: 7120, train/loss: 0.0
Step: 7120, train/grad_norm: 1.4386265320354141e-05
Step: 7120, train/learning_rate: 4.152784458710812e-05
Step: 7120, train/epoch: 1.694431185722351
Step: 7130, train/loss: 0.0
Step: 7130, train/grad_norm: 0.0002443246776238084
Step: 7130, train/learning_rate: 4.151594475843012e-05
Step: 7130, train/epoch: 1.6968110799789429
Step: 7140, train/loss: 0.0
Step: 7140, train/grad_norm: 5.369465270632645e-06
Step: 7140, train/learning_rate: 4.1504044929752126e-05
Step: 7140, train/epoch: 1.6991908550262451
Step: 7150, train/loss: 0.0
Step: 7150, train/grad_norm: 9.689962098491378e-06
Step: 7150, train/learning_rate: 4.149214510107413e-05
Step: 7150, train/epoch: 1.7015706300735474
Step: 7160, train/loss: 0.0
Step: 7160, train/grad_norm: 0.00013487806427292526
Step: 7160, train/learning_rate: 4.148024891037494e-05
Step: 7160, train/epoch: 1.7039505243301392
Step: 7170, train/loss: 0.002300000051036477
Step: 7170, train/grad_norm: 0.0006892385426908731
Step: 7170, train/learning_rate: 4.146834908169694e-05
Step: 7170, train/epoch: 1.7063302993774414
Step: 7180, train/loss: 0.0
Step: 7180, train/grad_norm: 6.520597526105121e-05
Step: 7180, train/learning_rate: 4.1456449253018945e-05
Step: 7180, train/epoch: 1.7087101936340332
Step: 7190, train/loss: 0.05270000174641609
Step: 7190, train/grad_norm: 0.0006945347413420677
Step: 7190, train/learning_rate: 4.144454942434095e-05
Step: 7190, train/epoch: 1.7110899686813354
Step: 7200, train/loss: 0.035100001841783524
Step: 7200, train/grad_norm: 0.001024464494548738
Step: 7200, train/learning_rate: 4.143264959566295e-05
Step: 7200, train/epoch: 1.7134697437286377
Step: 7210, train/loss: 0.17229999601840973
Step: 7210, train/grad_norm: 207.90023803710938
Step: 7210, train/learning_rate: 4.142075340496376e-05
Step: 7210, train/epoch: 1.7158496379852295
Step: 7220, train/loss: 0.029500000178813934
Step: 7220, train/grad_norm: 6.948390483856201
Step: 7220, train/learning_rate: 4.1408853576285765e-05
Step: 7220, train/epoch: 1.7182294130325317
Step: 7230, train/loss: 0.0012000000569969416
Step: 7230, train/grad_norm: 1.571682696521748e-05
Step: 7230, train/learning_rate: 4.139695374760777e-05
Step: 7230, train/epoch: 1.720609188079834
Step: 7240, train/loss: 0.0
Step: 7240, train/grad_norm: 1.0371138392528678e-09
Step: 7240, train/learning_rate: 4.138505391892977e-05
Step: 7240, train/epoch: 1.7229890823364258
Step: 7250, train/loss: 0.0
Step: 7250, train/grad_norm: 6.908777550052037e-07
Step: 7250, train/learning_rate: 4.1373154090251774e-05
Step: 7250, train/epoch: 1.725368857383728
Step: 7260, train/loss: 0.0
Step: 7260, train/grad_norm: 7.393869623228966e-07
Step: 7260, train/learning_rate: 4.1361257899552584e-05
Step: 7260, train/epoch: 1.7277486324310303
Step: 7270, train/loss: 0.07339999824762344
Step: 7270, train/grad_norm: 6.1035052567604e-08
Step: 7270, train/learning_rate: 4.134935807087459e-05
Step: 7270, train/epoch: 1.730128526687622
Step: 7280, train/loss: 0.6176999807357788
Step: 7280, train/grad_norm: 102.07009887695312
Step: 7280, train/learning_rate: 4.133745824219659e-05
Step: 7280, train/epoch: 1.7325083017349243
Step: 7290, train/loss: 0.0
Step: 7290, train/grad_norm: 0.0032543025445193052
Step: 7290, train/learning_rate: 4.132555841351859e-05
Step: 7290, train/epoch: 1.7348881959915161
Step: 7300, train/loss: 9.999999747378752e-05
Step: 7300, train/grad_norm: 0.007048971951007843
Step: 7300, train/learning_rate: 4.1313658584840596e-05
Step: 7300, train/epoch: 1.7372679710388184
Step: 7310, train/loss: 0.06610000133514404
Step: 7310, train/grad_norm: 0.01167281810194254
Step: 7310, train/learning_rate: 4.1301762394141406e-05
Step: 7310, train/epoch: 1.7396477460861206
Step: 7320, train/loss: 0.0005000000237487257
Step: 7320, train/grad_norm: 0.06649711728096008
Step: 7320, train/learning_rate: 4.128986256546341e-05
Step: 7320, train/epoch: 1.7420276403427124
Step: 7330, train/loss: 0.1379999965429306
Step: 7330, train/grad_norm: 0.08481699973344803
Step: 7330, train/learning_rate: 4.127796273678541e-05
Step: 7330, train/epoch: 1.7444074153900146
Step: 7340, train/loss: 9.999999747378752e-05
Step: 7340, train/grad_norm: 0.00036078941775485873
Step: 7340, train/learning_rate: 4.1266062908107415e-05
Step: 7340, train/epoch: 1.746787190437317
Step: 7350, train/loss: 0.0
Step: 7350, train/grad_norm: 0.0031992027070373297
Step: 7350, train/learning_rate: 4.125416307942942e-05
Step: 7350, train/epoch: 1.7491670846939087
Step: 7360, train/loss: 0.08869999647140503
Step: 7360, train/grad_norm: 0.0001311372616328299
Step: 7360, train/learning_rate: 4.124226688873023e-05
Step: 7360, train/epoch: 1.751546859741211
Step: 7370, train/loss: 0.2134999930858612
Step: 7370, train/grad_norm: 6.436606054194272e-05
Step: 7370, train/learning_rate: 4.123036706005223e-05
Step: 7370, train/epoch: 1.7539267539978027
Step: 7380, train/loss: 0.0
Step: 7380, train/grad_norm: 0.005121107213199139
Step: 7380, train/learning_rate: 4.1218467231374234e-05
Step: 7380, train/epoch: 1.756306529045105
Step: 7390, train/loss: 0.0012000000569969416
Step: 7390, train/grad_norm: 0.00025914437719620764
Step: 7390, train/learning_rate: 4.120656740269624e-05
Step: 7390, train/epoch: 1.7586863040924072
Step: 7400, train/loss: 0.0
Step: 7400, train/grad_norm: 0.0002912254713010043
Step: 7400, train/learning_rate: 4.119466757401824e-05
Step: 7400, train/epoch: 1.761066198348999
Step: 7410, train/loss: 0.0
Step: 7410, train/grad_norm: 0.004319475032389164
Step: 7410, train/learning_rate: 4.118277138331905e-05
Step: 7410, train/epoch: 1.7634459733963013
Step: 7420, train/loss: 9.999999747378752e-05
Step: 7420, train/grad_norm: 0.0005826048436574638
Step: 7420, train/learning_rate: 4.117087155464105e-05
Step: 7420, train/epoch: 1.7658257484436035
Step: 7430, train/loss: 0.0
Step: 7430, train/grad_norm: 0.0016639187233522534
Step: 7430, train/learning_rate: 4.1158971725963056e-05
Step: 7430, train/epoch: 1.7682056427001953
Step: 7440, train/loss: 9.999999747378752e-05
Step: 7440, train/grad_norm: 4.02185796701815e-05
Step: 7440, train/learning_rate: 4.114707189728506e-05
Step: 7440, train/epoch: 1.7705854177474976
Step: 7450, train/loss: 0.0
Step: 7450, train/grad_norm: 0.21283145248889923
Step: 7450, train/learning_rate: 4.113517206860706e-05
Step: 7450, train/epoch: 1.7729653120040894
Step: 7460, train/loss: 0.0
Step: 7460, train/grad_norm: 6.155752998893149e-06
Step: 7460, train/learning_rate: 4.112327587790787e-05
Step: 7460, train/epoch: 1.7753450870513916
Step: 7470, train/loss: 0.0
Step: 7470, train/grad_norm: 9.680124639999121e-05
Step: 7470, train/learning_rate: 4.1111376049229875e-05
Step: 7470, train/epoch: 1.7777248620986938
Step: 7480, train/loss: 0.09690000116825104
Step: 7480, train/grad_norm: 0.38757726550102234
Step: 7480, train/learning_rate: 4.109947622055188e-05
Step: 7480, train/epoch: 1.7801047563552856
Step: 7490, train/loss: 9.999999747378752e-05
Step: 7490, train/grad_norm: 0.5347416996955872
Step: 7490, train/learning_rate: 4.108757639187388e-05
Step: 7490, train/epoch: 1.782484531402588
Step: 7500, train/loss: 0.0
Step: 7500, train/grad_norm: 3.2874913813429885e-07
Step: 7500, train/learning_rate: 4.1075676563195884e-05
Step: 7500, train/epoch: 1.7848643064498901
Step: 7510, train/loss: 0.0
Step: 7510, train/grad_norm: 0.01332042645663023
Step: 7510, train/learning_rate: 4.1063780372496694e-05
Step: 7510, train/epoch: 1.787244200706482
Step: 7520, train/loss: 0.0
Step: 7520, train/grad_norm: 0.0007527768611907959
Step: 7520, train/learning_rate: 4.10518805438187e-05
Step: 7520, train/epoch: 1.7896239757537842
Step: 7530, train/loss: 0.09570000320672989
Step: 7530, train/grad_norm: 0.009057371877133846
Step: 7530, train/learning_rate: 4.10399807151407e-05
Step: 7530, train/epoch: 1.7920037508010864
Step: 7540, train/loss: 0.0
Step: 7540, train/grad_norm: 0.0007029457483440638
Step: 7540, train/learning_rate: 4.10280808864627e-05
Step: 7540, train/epoch: 1.7943836450576782
Step: 7550, train/loss: 0.2578999996185303
Step: 7550, train/grad_norm: 73.66737365722656
Step: 7550, train/learning_rate: 4.1016181057784706e-05
Step: 7550, train/epoch: 1.7967634201049805
Step: 7560, train/loss: 9.999999747378752e-05
Step: 7560, train/grad_norm: 5.675188003806397e-05
Step: 7560, train/learning_rate: 4.1004284867085516e-05
Step: 7560, train/epoch: 1.7991433143615723
Step: 7570, train/loss: 0.040699999779462814
Step: 7570, train/grad_norm: 0.004342671949416399
Step: 7570, train/learning_rate: 4.099238503840752e-05
Step: 7570, train/epoch: 1.8015230894088745
Step: 7580, train/loss: 9.999999747378752e-05
Step: 7580, train/grad_norm: 8.352098666364327e-05
Step: 7580, train/learning_rate: 4.098048520972952e-05
Step: 7580, train/epoch: 1.8039028644561768
Step: 7590, train/loss: 0.0
Step: 7590, train/grad_norm: 4.271457498816744e-07
Step: 7590, train/learning_rate: 4.0968585381051525e-05
Step: 7590, train/epoch: 1.8062827587127686
Step: 7600, train/loss: 0.0
Step: 7600, train/grad_norm: 1.8108954463968985e-05
Step: 7600, train/learning_rate: 4.095668555237353e-05
Step: 7600, train/epoch: 1.8086625337600708
Step: 7610, train/loss: 0.0003000000142492354
Step: 7610, train/grad_norm: 5.519473688764265e-06
Step: 7610, train/learning_rate: 4.094478936167434e-05
Step: 7610, train/epoch: 1.811042308807373
Step: 7620, train/loss: 0.008200000040233135
Step: 7620, train/grad_norm: 188.5049591064453
Step: 7620, train/learning_rate: 4.093288953299634e-05
Step: 7620, train/epoch: 1.8134222030639648
Step: 7630, train/loss: 0.0
Step: 7630, train/grad_norm: 2.82211534141652e-08
Step: 7630, train/learning_rate: 4.0920989704318345e-05
Step: 7630, train/epoch: 1.815801978111267
Step: 7640, train/loss: 0.0
Step: 7640, train/grad_norm: 3.657457341432746e-07
Step: 7640, train/learning_rate: 4.090908987564035e-05
Step: 7640, train/epoch: 1.8181818723678589
Step: 7650, train/loss: 0.0
Step: 7650, train/grad_norm: 5.444596808956703e-06
Step: 7650, train/learning_rate: 4.089719004696235e-05
Step: 7650, train/epoch: 1.8205616474151611
Step: 7660, train/loss: 0.0
Step: 7660, train/grad_norm: 3.750899324472812e-08
Step: 7660, train/learning_rate: 4.088529385626316e-05
Step: 7660, train/epoch: 1.8229414224624634
Step: 7670, train/loss: 0.0
Step: 7670, train/grad_norm: 4.348394000430744e-08
Step: 7670, train/learning_rate: 4.0873394027585164e-05
Step: 7670, train/epoch: 1.8253213167190552
Step: 7680, train/loss: 0.0
Step: 7680, train/grad_norm: 1.2089214578736573e-05
Step: 7680, train/learning_rate: 4.086149419890717e-05
Step: 7680, train/epoch: 1.8277010917663574
Step: 7690, train/loss: 0.0
Step: 7690, train/grad_norm: 7.183342631833511e-08
Step: 7690, train/learning_rate: 4.084959437022917e-05
Step: 7690, train/epoch: 1.8300808668136597
Step: 7700, train/loss: 0.11089999973773956
Step: 7700, train/grad_norm: 0.002223515883088112
Step: 7700, train/learning_rate: 4.083769454155117e-05
Step: 7700, train/epoch: 1.8324607610702515
Step: 7710, train/loss: 0.0
Step: 7710, train/grad_norm: 0.0020393808372318745
Step: 7710, train/learning_rate: 4.082579835085198e-05
Step: 7710, train/epoch: 1.8348405361175537
Step: 7720, train/loss: 9.999999747378752e-05
Step: 7720, train/grad_norm: 1.223706505015798e-07
Step: 7720, train/learning_rate: 4.0813898522173986e-05
Step: 7720, train/epoch: 1.8372204303741455
Step: 7730, train/loss: 0.0
Step: 7730, train/grad_norm: 4.134534719923977e-06
Step: 7730, train/learning_rate: 4.080199869349599e-05
Step: 7730, train/epoch: 1.8396002054214478
Step: 7740, train/loss: 0.0
Step: 7740, train/grad_norm: 7.121996532077901e-06
Step: 7740, train/learning_rate: 4.079009886481799e-05
Step: 7740, train/epoch: 1.84197998046875
Step: 7750, train/loss: 0.0414000004529953
Step: 7750, train/grad_norm: 148.8906707763672
Step: 7750, train/learning_rate: 4.07782026741188e-05
Step: 7750, train/epoch: 1.8443598747253418
Step: 7760, train/loss: 0.4790000021457672
Step: 7760, train/grad_norm: 0.029270028695464134
Step: 7760, train/learning_rate: 4.0766302845440805e-05
Step: 7760, train/epoch: 1.846739649772644
Step: 7770, train/loss: 0.0005000000237487257
Step: 7770, train/grad_norm: 0.006905327085405588
Step: 7770, train/learning_rate: 4.075440301676281e-05
Step: 7770, train/epoch: 1.8491194248199463
Step: 7780, train/loss: 0.09459999948740005
Step: 7780, train/grad_norm: 0.00015682486991863698
Step: 7780, train/learning_rate: 4.074250318808481e-05
Step: 7780, train/epoch: 1.851499319076538
Step: 7790, train/loss: 0.0
Step: 7790, train/grad_norm: 0.1162848174571991
Step: 7790, train/learning_rate: 4.0730603359406814e-05
Step: 7790, train/epoch: 1.8538790941238403
Step: 7800, train/loss: 0.003000000026077032
Step: 7800, train/grad_norm: 0.00016993861936498433
Step: 7800, train/learning_rate: 4.0718707168707624e-05
Step: 7800, train/epoch: 1.8562588691711426
Step: 7810, train/loss: 0.13670000433921814
Step: 7810, train/grad_norm: 65.89888763427734
Step: 7810, train/learning_rate: 4.070680734002963e-05
Step: 7810, train/epoch: 1.8586387634277344
Step: 7820, train/loss: 0.02070000022649765
Step: 7820, train/grad_norm: 0.0005478961393237114
Step: 7820, train/learning_rate: 4.069490751135163e-05
Step: 7820, train/epoch: 1.8610185384750366
Step: 7830, train/loss: 0.0
Step: 7830, train/grad_norm: 0.003816956654191017
Step: 7830, train/learning_rate: 4.068300768267363e-05
Step: 7830, train/epoch: 1.8633984327316284
Step: 7840, train/loss: 0.11029999703168869
Step: 7840, train/grad_norm: 0.0012605816591531038
Step: 7840, train/learning_rate: 4.0671107853995636e-05
Step: 7840, train/epoch: 1.8657782077789307
Step: 7850, train/loss: 0.00019999999494757503
Step: 7850, train/grad_norm: 0.0014785518869757652
Step: 7850, train/learning_rate: 4.0659211663296446e-05
Step: 7850, train/epoch: 1.868157982826233
Step: 7860, train/loss: 0.0
Step: 7860, train/grad_norm: 9.516345016891137e-05
Step: 7860, train/learning_rate: 4.064731183461845e-05
Step: 7860, train/epoch: 1.8705378770828247
Step: 7870, train/loss: 9.999999747378752e-05
Step: 7870, train/grad_norm: 0.00025520139024592936
Step: 7870, train/learning_rate: 4.063541200594045e-05
Step: 7870, train/epoch: 1.872917652130127
Step: 7880, train/loss: 0.0
Step: 7880, train/grad_norm: 0.0003877085691783577
Step: 7880, train/learning_rate: 4.0623512177262455e-05
Step: 7880, train/epoch: 1.8752974271774292
Step: 7890, train/loss: 0.21250000596046448
Step: 7890, train/grad_norm: 0.0003355596272740513
Step: 7890, train/learning_rate: 4.061161234858446e-05
Step: 7890, train/epoch: 1.877677321434021
Step: 7900, train/loss: 0.002199999988079071
Step: 7900, train/grad_norm: 0.0003167543327435851
Step: 7900, train/learning_rate: 4.059971615788527e-05
Step: 7900, train/epoch: 1.8800570964813232
Step: 7910, train/loss: 0.00019999999494757503
Step: 7910, train/grad_norm: 0.15303795039653778
Step: 7910, train/learning_rate: 4.058781632920727e-05
Step: 7910, train/epoch: 1.882436990737915
Step: 7920, train/loss: 9.999999747378752e-05
Step: 7920, train/grad_norm: 4.6921948523959145e-05
Step: 7920, train/learning_rate: 4.0575916500529274e-05
Step: 7920, train/epoch: 1.8848167657852173
Step: 7930, train/loss: 0.0012000000569969416
Step: 7930, train/grad_norm: 1.1875357586177415e-06
Step: 7930, train/learning_rate: 4.056401667185128e-05
Step: 7930, train/epoch: 1.8871965408325195
Step: 7940, train/loss: 0.0
Step: 7940, train/grad_norm: 0.0008645812049508095
Step: 7940, train/learning_rate: 4.055211684317328e-05
Step: 7940, train/epoch: 1.8895764350891113
Step: 7950, train/loss: 0.0
Step: 7950, train/grad_norm: 1.206052638735855e-05
Step: 7950, train/learning_rate: 4.054022065247409e-05
Step: 7950, train/epoch: 1.8919562101364136
Step: 7960, train/loss: 0.0
Step: 7960, train/grad_norm: 7.103658845153404e-06
Step: 7960, train/learning_rate: 4.0528320823796093e-05
Step: 7960, train/epoch: 1.8943359851837158
Step: 7970, train/loss: 0.0
Step: 7970, train/grad_norm: 9.84736175269063e-07
Step: 7970, train/learning_rate: 4.0516420995118096e-05
Step: 7970, train/epoch: 1.8967158794403076
Step: 7980, train/loss: 0.0
Step: 7980, train/grad_norm: 9.417748287887662e-07
Step: 7980, train/learning_rate: 4.05045211664401e-05
Step: 7980, train/epoch: 1.8990956544876099
Step: 7990, train/loss: 0.0
Step: 7990, train/grad_norm: 0.0007379595190286636
Step: 7990, train/learning_rate: 4.04926213377621e-05
Step: 7990, train/epoch: 1.901475429534912
Step: 8000, train/loss: 0.0
Step: 8000, train/grad_norm: 0.0037528681568801403
Step: 8000, train/learning_rate: 4.048072514706291e-05
Step: 8000, train/epoch: 1.903855323791504
Step: 8010, train/loss: 0.0
Step: 8010, train/grad_norm: 2.8532325814012438e-05
Step: 8010, train/learning_rate: 4.0468825318384916e-05
Step: 8010, train/epoch: 1.9062350988388062
Step: 8020, train/loss: 0.0
Step: 8020, train/grad_norm: 3.433675829001004e-06
Step: 8020, train/learning_rate: 4.045692548970692e-05
Step: 8020, train/epoch: 1.908614993095398
Step: 8030, train/loss: 0.0
Step: 8030, train/grad_norm: 2.06183049158426e-05
Step: 8030, train/learning_rate: 4.044502566102892e-05
Step: 8030, train/epoch: 1.9109947681427002
Step: 8040, train/loss: 0.0
Step: 8040, train/grad_norm: 0.0007573034963570535
Step: 8040, train/learning_rate: 4.0433125832350925e-05
Step: 8040, train/epoch: 1.9133745431900024
Step: 8050, train/loss: 0.017500000074505806
Step: 8050, train/grad_norm: 9.110592031902343e-07
Step: 8050, train/learning_rate: 4.0421229641651735e-05
Step: 8050, train/epoch: 1.9157544374465942
Step: 8060, train/loss: 0.021900000050663948
Step: 8060, train/grad_norm: 4.5154574763728306e-05
Step: 8060, train/learning_rate: 4.040932981297374e-05
Step: 8060, train/epoch: 1.9181342124938965
Step: 8070, train/loss: 0.0
Step: 8070, train/grad_norm: 3.933394054911332e-06
Step: 8070, train/learning_rate: 4.039742998429574e-05
Step: 8070, train/epoch: 1.9205139875411987
Step: 8080, train/loss: 0.0
Step: 8080, train/grad_norm: 1.7538715837872587e-05
Step: 8080, train/learning_rate: 4.0385530155617744e-05
Step: 8080, train/epoch: 1.9228938817977905
Step: 8090, train/loss: 0.0
Step: 8090, train/grad_norm: 0.003489614697173238
Step: 8090, train/learning_rate: 4.037363032693975e-05
Step: 8090, train/epoch: 1.9252736568450928
Step: 8100, train/loss: 0.0
Step: 8100, train/grad_norm: 9.008536835608538e-06
Step: 8100, train/learning_rate: 4.036173413624056e-05
Step: 8100, train/epoch: 1.9276535511016846
Step: 8110, train/loss: 9.999999747378752e-05
Step: 8110, train/grad_norm: 7.29897237761179e-07
Step: 8110, train/learning_rate: 4.034983430756256e-05
Step: 8110, train/epoch: 1.9300333261489868
Step: 8120, train/loss: 0.13359999656677246
Step: 8120, train/grad_norm: 3.5142140404786915e-05
Step: 8120, train/learning_rate: 4.033793447888456e-05
Step: 8120, train/epoch: 1.932413101196289
Step: 8130, train/loss: 9.999999747378752e-05
Step: 8130, train/grad_norm: 5.323926961864345e-05
Step: 8130, train/learning_rate: 4.0326034650206566e-05
Step: 8130, train/epoch: 1.9347929954528809
Step: 8140, train/loss: 0.0024999999441206455
Step: 8140, train/grad_norm: 0.011124810203909874
Step: 8140, train/learning_rate: 4.031413482152857e-05
Step: 8140, train/epoch: 1.937172770500183
Step: 8150, train/loss: 0.0
Step: 8150, train/grad_norm: 0.10583095997571945
Step: 8150, train/learning_rate: 4.030223863082938e-05
Step: 8150, train/epoch: 1.9395525455474854
Step: 8160, train/loss: 0.12939999997615814
Step: 8160, train/grad_norm: 0.0008744417573325336
Step: 8160, train/learning_rate: 4.029033880215138e-05
Step: 8160, train/epoch: 1.9419324398040771
Step: 8170, train/loss: 0.057500001043081284
Step: 8170, train/grad_norm: 0.028037533164024353
Step: 8170, train/learning_rate: 4.0278438973473385e-05
Step: 8170, train/epoch: 1.9443122148513794
Step: 8180, train/loss: 0.17980000376701355
Step: 8180, train/grad_norm: 0.002813625382259488
Step: 8180, train/learning_rate: 4.026653914479539e-05
Step: 8180, train/epoch: 1.9466921091079712
Step: 8190, train/loss: 9.999999747378752e-05
Step: 8190, train/grad_norm: 0.0024946616031229496
Step: 8190, train/learning_rate: 4.025463931611739e-05
Step: 8190, train/epoch: 1.9490718841552734
Step: 8200, train/loss: 0.0
Step: 8200, train/grad_norm: 0.0030426979064941406
Step: 8200, train/learning_rate: 4.02427431254182e-05
Step: 8200, train/epoch: 1.9514516592025757
Step: 8210, train/loss: 0.06840000301599503
Step: 8210, train/grad_norm: 5.8626788813853636e-05
Step: 8210, train/learning_rate: 4.0230843296740204e-05
Step: 8210, train/epoch: 1.9538315534591675
Step: 8220, train/loss: 0.0
Step: 8220, train/grad_norm: 0.010043760761618614
Step: 8220, train/learning_rate: 4.021894346806221e-05
Step: 8220, train/epoch: 1.9562113285064697
Step: 8230, train/loss: 0.20739999413490295
Step: 8230, train/grad_norm: 0.0015800956171005964
Step: 8230, train/learning_rate: 4.020704363938421e-05
Step: 8230, train/epoch: 1.958591103553772
Step: 8240, train/loss: 0.015699999406933784
Step: 8240, train/grad_norm: 0.01470792107284069
Step: 8240, train/learning_rate: 4.019514381070621e-05
Step: 8240, train/epoch: 1.9609709978103638
Step: 8250, train/loss: 0.0006000000284984708
Step: 8250, train/grad_norm: 0.0012574167922139168
Step: 8250, train/learning_rate: 4.018324762000702e-05
Step: 8250, train/epoch: 1.963350772857666
Step: 8260, train/loss: 9.999999747378752e-05
Step: 8260, train/grad_norm: 0.056191593408584595
Step: 8260, train/learning_rate: 4.0171347791329026e-05
Step: 8260, train/epoch: 1.9657305479049683
Step: 8270, train/loss: 0.01600000075995922
Step: 8270, train/grad_norm: 0.00035888832644559443
Step: 8270, train/learning_rate: 4.015944796265103e-05
Step: 8270, train/epoch: 1.96811044216156
Step: 8280, train/loss: 0.0
Step: 8280, train/grad_norm: 0.00021752799511887133
Step: 8280, train/learning_rate: 4.014754813397303e-05
Step: 8280, train/epoch: 1.9704902172088623
Step: 8290, train/loss: 0.125
Step: 8290, train/grad_norm: 3.0647834137198515e-06
Step: 8290, train/learning_rate: 4.0135648305295035e-05
Step: 8290, train/epoch: 1.972870111465454
Step: 8300, train/loss: 0.0
Step: 8300, train/grad_norm: 3.429603748372756e-05
Step: 8300, train/learning_rate: 4.0123752114595845e-05
Step: 8300, train/epoch: 1.9752498865127563
Step: 8310, train/loss: 0.10679999738931656
Step: 8310, train/grad_norm: 0.00019797380082309246
Step: 8310, train/learning_rate: 4.011185228591785e-05
Step: 8310, train/epoch: 1.9776296615600586
Step: 8320, train/loss: 0.0
Step: 8320, train/grad_norm: 9.116172441281378e-05
Step: 8320, train/learning_rate: 4.009995245723985e-05
Step: 8320, train/epoch: 1.9800095558166504
Step: 8330, train/loss: 0.0
Step: 8330, train/grad_norm: 8.868667646311224e-05
Step: 8330, train/learning_rate: 4.0088052628561854e-05
Step: 8330, train/epoch: 1.9823893308639526
Step: 8340, train/loss: 0.0
Step: 8340, train/grad_norm: 0.0002886733564082533
Step: 8340, train/learning_rate: 4.007615279988386e-05
Step: 8340, train/epoch: 1.9847691059112549
Step: 8350, train/loss: 0.0
Step: 8350, train/grad_norm: 0.00020219211000949144
Step: 8350, train/learning_rate: 4.006425660918467e-05
Step: 8350, train/epoch: 1.9871490001678467
Step: 8360, train/loss: 0.0027000000700354576
Step: 8360, train/grad_norm: 0.000671595276799053
Step: 8360, train/learning_rate: 4.005235678050667e-05
Step: 8360, train/epoch: 1.989528775215149
Step: 8370, train/loss: 0.0
Step: 8370, train/grad_norm: 0.00017254143313039094
Step: 8370, train/learning_rate: 4.0040456951828673e-05
Step: 8370, train/epoch: 1.9919086694717407
Step: 8380, train/loss: 0.0
Step: 8380, train/grad_norm: 9.699244401417673e-05
Step: 8380, train/learning_rate: 4.0028557123150676e-05
Step: 8380, train/epoch: 1.994288444519043
Step: 8390, train/loss: 0.0
Step: 8390, train/grad_norm: 2.6584413717500865e-05
Step: 8390, train/learning_rate: 4.001665729447268e-05
Step: 8390, train/epoch: 1.9966682195663452
Step: 8400, train/loss: 0.009700000286102295
Step: 8400, train/grad_norm: 3.336484223837033e-05
Step: 8400, train/learning_rate: 4.000476110377349e-05
Step: 8400, train/epoch: 1.999048113822937
Step: 8404, eval/loss: 0.04434797540307045
Step: 8404, eval/accuracy: 0.9933360815048218
Step: 8404, eval/f1: 0.9929759502410889
Step: 8404, eval/runtime: 734.8837890625
Step: 8404, eval/samples_per_second: 9.802000045776367
Step: 8404, eval/steps_per_second: 1.2259999513626099
Step: 8404, train/epoch: 2.0
Step: 8410, train/loss: 0.125
Step: 8410, train/grad_norm: 7.83321593189612e-05
Step: 8410, train/learning_rate: 3.999286127509549e-05
Step: 8410, train/epoch: 2.0014278888702393
Step: 8420, train/loss: 0.0
Step: 8420, train/grad_norm: 0.0002194785192841664
Step: 8420, train/learning_rate: 3.9980961446417496e-05
Step: 8420, train/epoch: 2.003807783126831
Step: 8430, train/loss: 0.030500000342726707
Step: 8430, train/grad_norm: 0.00017709145322442055
Step: 8430, train/learning_rate: 3.99690616177395e-05
Step: 8430, train/epoch: 2.0061874389648438
Step: 8440, train/loss: 0.0015999999595806003
Step: 8440, train/grad_norm: 45.4721565246582
Step: 8440, train/learning_rate: 3.99571617890615e-05
Step: 8440, train/epoch: 2.0085673332214355
Step: 8450, train/loss: 0.0
Step: 8450, train/grad_norm: 0.0002556424879003316
Step: 8450, train/learning_rate: 3.994526559836231e-05
Step: 8450, train/epoch: 2.0109472274780273
Step: 8460, train/loss: 9.999999747378752e-05
Step: 8460, train/grad_norm: 1.3865103028365411e-05
Step: 8460, train/learning_rate: 3.9933365769684315e-05
Step: 8460, train/epoch: 2.01332688331604
Step: 8470, train/loss: 0.0
Step: 8470, train/grad_norm: 4.3282278056722134e-05
Step: 8470, train/learning_rate: 3.992146594100632e-05
Step: 8470, train/epoch: 2.015706777572632
Step: 8480, train/loss: 0.0
Step: 8480, train/grad_norm: 0.001316532609052956
Step: 8480, train/learning_rate: 3.990956611232832e-05
Step: 8480, train/epoch: 2.0180866718292236
Step: 8490, train/loss: 0.0
Step: 8490, train/grad_norm: 0.011290404945611954
Step: 8490, train/learning_rate: 3.9897666283650324e-05
Step: 8490, train/epoch: 2.0204663276672363
Step: 8500, train/loss: 0.0003000000142492354
Step: 8500, train/grad_norm: 4.185057207450882e-07
Step: 8500, train/learning_rate: 3.9885770092951134e-05
Step: 8500, train/epoch: 2.022846221923828
Step: 8510, train/loss: 0.0
Step: 8510, train/grad_norm: 0.0001423585053998977
Step: 8510, train/learning_rate: 3.987387026427314e-05
Step: 8510, train/epoch: 2.02522611618042
Step: 8520, train/loss: 0.0
Step: 8520, train/grad_norm: 8.219586220548081e-07
Step: 8520, train/learning_rate: 3.986197043559514e-05
Step: 8520, train/epoch: 2.0276060104370117
Step: 8530, train/loss: 0.0
Step: 8530, train/grad_norm: 0.0003661109658423811
Step: 8530, train/learning_rate: 3.985007060691714e-05
Step: 8530, train/epoch: 2.0299856662750244
Step: 8540, train/loss: 0.0
Step: 8540, train/grad_norm: 2.8368514904286712e-05
Step: 8540, train/learning_rate: 3.9838170778239146e-05
Step: 8540, train/epoch: 2.032365560531616
Step: 8550, train/loss: 0.0
Step: 8550, train/grad_norm: 3.50902701029554e-05
Step: 8550, train/learning_rate: 3.9826274587539956e-05
Step: 8550, train/epoch: 2.034745454788208
Step: 8560, train/loss: 0.0
Step: 8560, train/grad_norm: 1.1864062798849773e-05
Step: 8560, train/learning_rate: 3.981437475886196e-05
Step: 8560, train/epoch: 2.0371251106262207
Step: 8570, train/loss: 0.11410000175237656
Step: 8570, train/grad_norm: 6.367341848090291e-05
Step: 8570, train/learning_rate: 3.980247493018396e-05
Step: 8570, train/epoch: 2.0395050048828125
Step: 8580, train/loss: 0.0
Step: 8580, train/grad_norm: 0.0001143685876741074
Step: 8580, train/learning_rate: 3.9790575101505965e-05
Step: 8580, train/epoch: 2.0418848991394043
Step: 8590, train/loss: 0.18440000712871552
Step: 8590, train/grad_norm: 0.00046906279749237
Step: 8590, train/learning_rate: 3.977867527282797e-05
Step: 8590, train/epoch: 2.044264554977417
Step: 8600, train/loss: 0.0003000000142492354
Step: 8600, train/grad_norm: 0.12008544057607651
Step: 8600, train/learning_rate: 3.976677908212878e-05
Step: 8600, train/epoch: 2.046644449234009
Step: 8610, train/loss: 0.0007999999797903001
Step: 8610, train/grad_norm: 0.0002187766513088718
Step: 8610, train/learning_rate: 3.975487925345078e-05
Step: 8610, train/epoch: 2.0490243434906006
Step: 8620, train/loss: 0.0
Step: 8620, train/grad_norm: 0.00024464516900479794
Step: 8620, train/learning_rate: 3.9742979424772784e-05
Step: 8620, train/epoch: 2.0514039993286133
Step: 8630, train/loss: 0.0
Step: 8630, train/grad_norm: 0.00030381439137272537
Step: 8630, train/learning_rate: 3.973107959609479e-05
Step: 8630, train/epoch: 2.053783893585205
Step: 8640, train/loss: 0.0
Step: 8640, train/grad_norm: 2.2472357159131207e-05
Step: 8640, train/learning_rate: 3.971917976741679e-05
Step: 8640, train/epoch: 2.056163787841797
Step: 8650, train/loss: 0.0
Step: 8650, train/grad_norm: 0.00016754143871366978
Step: 8650, train/learning_rate: 3.97072835767176e-05
Step: 8650, train/epoch: 2.0585434436798096
Step: 8660, train/loss: 0.0
Step: 8660, train/grad_norm: 0.010858460329473019
Step: 8660, train/learning_rate: 3.96953837480396e-05
Step: 8660, train/epoch: 2.0609233379364014
Step: 8670, train/loss: 0.0
Step: 8670, train/grad_norm: 0.00016752383089624345
Step: 8670, train/learning_rate: 3.9683483919361606e-05
Step: 8670, train/epoch: 2.063303232192993
Step: 8680, train/loss: 0.0
Step: 8680, train/grad_norm: 0.0005492153577506542
Step: 8680, train/learning_rate: 3.967158409068361e-05
Step: 8680, train/epoch: 2.065683126449585
Step: 8690, train/loss: 0.0
Step: 8690, train/grad_norm: 0.21865476667881012
Step: 8690, train/learning_rate: 3.965968426200561e-05
Step: 8690, train/epoch: 2.0680627822875977
Step: 8700, train/loss: 0.0
Step: 8700, train/grad_norm: 3.865237886202522e-06
Step: 8700, train/learning_rate: 3.964778807130642e-05
Step: 8700, train/epoch: 2.0704426765441895
Step: 8710, train/loss: 0.0
Step: 8710, train/grad_norm: 0.00034478423185646534
Step: 8710, train/learning_rate: 3.9635888242628425e-05
Step: 8710, train/epoch: 2.0728225708007812
Step: 8720, train/loss: 0.0
Step: 8720, train/grad_norm: 6.6356888055452146e-06
Step: 8720, train/learning_rate: 3.962398841395043e-05
Step: 8720, train/epoch: 2.075202226638794
Step: 8730, train/loss: 0.0007999999797903001
Step: 8730, train/grad_norm: 1.5724755940027535e-05
Step: 8730, train/learning_rate: 3.961208858527243e-05
Step: 8730, train/epoch: 2.0775821208953857
Step: 8740, train/loss: 0.0015999999595806003
Step: 8740, train/grad_norm: 1.641486960579641e-05
Step: 8740, train/learning_rate: 3.9600188756594434e-05
Step: 8740, train/epoch: 2.0799620151519775
Step: 8750, train/loss: 0.13989999890327454
Step: 8750, train/grad_norm: 1.8926526536233723e-05
Step: 8750, train/learning_rate: 3.9588292565895244e-05
Step: 8750, train/epoch: 2.0823416709899902
Step: 8760, train/loss: 0.00039999998989515007
Step: 8760, train/grad_norm: 0.0018415007507428527
Step: 8760, train/learning_rate: 3.957639273721725e-05
Step: 8760, train/epoch: 2.084721565246582
Step: 8770, train/loss: 0.0003000000142492354
Step: 8770, train/grad_norm: 6.59155411995016e-05
Step: 8770, train/learning_rate: 3.956449290853925e-05
Step: 8770, train/epoch: 2.087101459503174
Step: 8780, train/loss: 0.0
Step: 8780, train/grad_norm: 0.0001033438093145378
Step: 8780, train/learning_rate: 3.9552593079861253e-05
Step: 8780, train/epoch: 2.0894811153411865
Step: 8790, train/loss: 0.0
Step: 8790, train/grad_norm: 0.00019595630874391645
Step: 8790, train/learning_rate: 3.9540693251183257e-05
Step: 8790, train/epoch: 2.0918610095977783
Step: 8800, train/loss: 0.0
Step: 8800, train/grad_norm: 2.9381897093117004e-06
Step: 8800, train/learning_rate: 3.9528797060484067e-05
Step: 8800, train/epoch: 2.09424090385437
Step: 8810, train/loss: 0.0
Step: 8810, train/grad_norm: 9.991475963033736e-06
Step: 8810, train/learning_rate: 3.951689723180607e-05
Step: 8810, train/epoch: 2.096620559692383
Step: 8820, train/loss: 0.0
Step: 8820, train/grad_norm: 2.684280843823217e-05
Step: 8820, train/learning_rate: 3.950499740312807e-05
Step: 8820, train/epoch: 2.0990004539489746
Step: 8830, train/loss: 0.0
Step: 8830, train/grad_norm: 1.4046327123651281e-05
Step: 8830, train/learning_rate: 3.9493097574450076e-05
Step: 8830, train/epoch: 2.1013803482055664
Step: 8840, train/loss: 0.12269999831914902
Step: 8840, train/grad_norm: 1.0442389793752227e-05
Step: 8840, train/learning_rate: 3.948119774577208e-05
Step: 8840, train/epoch: 2.103760004043579
Step: 8850, train/loss: 0.0
Step: 8850, train/grad_norm: 0.0002778734196908772
Step: 8850, train/learning_rate: 3.946930155507289e-05
Step: 8850, train/epoch: 2.106139898300171
Step: 8860, train/loss: 0.0
Step: 8860, train/grad_norm: 0.0008302477654069662
Step: 8860, train/learning_rate: 3.945740172639489e-05
Step: 8860, train/epoch: 2.1085197925567627
Step: 8870, train/loss: 0.0
Step: 8870, train/grad_norm: 0.0013300986029207706
Step: 8870, train/learning_rate: 3.9445501897716895e-05
Step: 8870, train/epoch: 2.1108996868133545
Step: 8880, train/loss: 0.03909999877214432
Step: 8880, train/grad_norm: 0.05319682136178017
Step: 8880, train/learning_rate: 3.94336020690389e-05
Step: 8880, train/epoch: 2.113279342651367
Step: 8890, train/loss: 0.0
Step: 8890, train/grad_norm: 0.00029907620046287775
Step: 8890, train/learning_rate: 3.94217022403609e-05
Step: 8890, train/epoch: 2.115659236907959
Step: 8900, train/loss: 0.0
Step: 8900, train/grad_norm: 0.000256245955824852
Step: 8900, train/learning_rate: 3.940980604966171e-05
Step: 8900, train/epoch: 2.118039131164551
Step: 8910, train/loss: 9.999999747378752e-05
Step: 8910, train/grad_norm: 6.605345697607845e-05
Step: 8910, train/learning_rate: 3.9397906220983714e-05
Step: 8910, train/epoch: 2.1204187870025635
Step: 8920, train/loss: 9.999999747378752e-05
Step: 8920, train/grad_norm: 9.006965910884901e-07
Step: 8920, train/learning_rate: 3.938600639230572e-05
Step: 8920, train/epoch: 2.1227986812591553
Step: 8930, train/loss: 0.0
Step: 8930, train/grad_norm: 4.6473312977468595e-05
Step: 8930, train/learning_rate: 3.937410656362772e-05
Step: 8930, train/epoch: 2.125178575515747
Step: 8940, train/loss: 0.0786999985575676
Step: 8940, train/grad_norm: 0.002621609019115567
Step: 8940, train/learning_rate: 3.936220673494972e-05
Step: 8940, train/epoch: 2.1275582313537598
Step: 8950, train/loss: 0.0
Step: 8950, train/grad_norm: 0.0018326223362237215
Step: 8950, train/learning_rate: 3.935031054425053e-05
Step: 8950, train/epoch: 2.1299381256103516
Step: 8960, train/loss: 0.0003000000142492354
Step: 8960, train/grad_norm: 1.027738790071453e-06
Step: 8960, train/learning_rate: 3.9338410715572536e-05
Step: 8960, train/epoch: 2.1323180198669434
Step: 8970, train/loss: 0.0
Step: 8970, train/grad_norm: 3.5474415199132636e-05
Step: 8970, train/learning_rate: 3.932651088689454e-05
Step: 8970, train/epoch: 2.134697675704956
Step: 8980, train/loss: 0.08590000122785568
Step: 8980, train/grad_norm: 588.69580078125
Step: 8980, train/learning_rate: 3.931461105821654e-05
Step: 8980, train/epoch: 2.137077569961548
Step: 8990, train/loss: 0.33160001039505005
Step: 8990, train/grad_norm: 5.868551511412079e-07
Step: 8990, train/learning_rate: 3.9302711229538545e-05
Step: 8990, train/epoch: 2.1394574642181396
Step: 9000, train/loss: 0.0
Step: 9000, train/grad_norm: 0.00035855741589330137
Step: 9000, train/learning_rate: 3.9290815038839355e-05
Step: 9000, train/epoch: 2.1418371200561523
Step: 9010, train/loss: 0.0
Step: 9010, train/grad_norm: 1.1023339538951404e-05
Step: 9010, train/learning_rate: 3.927891521016136e-05
Step: 9010, train/epoch: 2.144217014312744
Step: 9020, train/loss: 0.09149999916553497
Step: 9020, train/grad_norm: 0.0012108662631362677
Step: 9020, train/learning_rate: 3.926701538148336e-05
Step: 9020, train/epoch: 2.146596908569336
Step: 9030, train/loss: 0.0
Step: 9030, train/grad_norm: 0.05352136865258217
Step: 9030, train/learning_rate: 3.9255115552805364e-05
Step: 9030, train/epoch: 2.1489765644073486
Step: 9040, train/loss: 0.0
Step: 9040, train/grad_norm: 2.72855868388433e-05
Step: 9040, train/learning_rate: 3.924321572412737e-05
Step: 9040, train/epoch: 2.1513564586639404
Step: 9050, train/loss: 0.0
Step: 9050, train/grad_norm: 0.0019381785532459617
Step: 9050, train/learning_rate: 3.923131953342818e-05
Step: 9050, train/epoch: 2.1537363529205322
Step: 9060, train/loss: 0.0027000000700354576
Step: 9060, train/grad_norm: 0.0002555768587626517
Step: 9060, train/learning_rate: 3.921941970475018e-05
Step: 9060, train/epoch: 2.156116247177124
Step: 9070, train/loss: 0.0
Step: 9070, train/grad_norm: 0.0008866595453582704
Step: 9070, train/learning_rate: 3.920751987607218e-05
Step: 9070, train/epoch: 2.1584959030151367
Step: 9080, train/loss: 0.0
Step: 9080, train/grad_norm: 1.4862705029372592e-05
Step: 9080, train/learning_rate: 3.9195620047394186e-05
Step: 9080, train/epoch: 2.1608757972717285
Step: 9090, train/loss: 0.0
Step: 9090, train/grad_norm: 0.00013282315921969712
Step: 9090, train/learning_rate: 3.918372021871619e-05
Step: 9090, train/epoch: 2.1632556915283203
Step: 9100, train/loss: 0.0
Step: 9100, train/grad_norm: 0.0012033276725560427
Step: 9100, train/learning_rate: 3.9171824028017e-05
Step: 9100, train/epoch: 2.165635347366333
Step: 9110, train/loss: 0.0
Step: 9110, train/grad_norm: 0.002832448575645685
Step: 9110, train/learning_rate: 3.9159924199339e-05
Step: 9110, train/epoch: 2.168015241622925
Step: 9120, train/loss: 0.0
Step: 9120, train/grad_norm: 0.00048392097232863307
Step: 9120, train/learning_rate: 3.9148024370661005e-05
Step: 9120, train/epoch: 2.1703951358795166
Step: 9130, train/loss: 0.0
Step: 9130, train/grad_norm: 2.439573290757835e-05
Step: 9130, train/learning_rate: 3.913612454198301e-05
Step: 9130, train/epoch: 2.1727747917175293
Step: 9140, train/loss: 0.0
Step: 9140, train/grad_norm: 1.1598093806242105e-05
Step: 9140, train/learning_rate: 3.912422835128382e-05
Step: 9140, train/epoch: 2.175154685974121
Step: 9150, train/loss: 0.0
Step: 9150, train/grad_norm: 1.1641089940894744e-06
Step: 9150, train/learning_rate: 3.911232852260582e-05
Step: 9150, train/epoch: 2.177534580230713
Step: 9160, train/loss: 0.0
Step: 9160, train/grad_norm: 7.472065044566989e-05
Step: 9160, train/learning_rate: 3.9100428693927824e-05
Step: 9160, train/epoch: 2.1799142360687256
Step: 9170, train/loss: 0.0
Step: 9170, train/grad_norm: 1.8830396584235132e-05
Step: 9170, train/learning_rate: 3.908852886524983e-05
Step: 9170, train/epoch: 2.1822941303253174
Step: 9180, train/loss: 0.0
Step: 9180, train/grad_norm: 2.8739912067976547e-06
Step: 9180, train/learning_rate: 3.907662903657183e-05
Step: 9180, train/epoch: 2.184674024581909
Step: 9190, train/loss: 0.0
Step: 9190, train/grad_norm: 0.0002115231181960553
Step: 9190, train/learning_rate: 3.906473284587264e-05
Step: 9190, train/epoch: 2.187053680419922
Step: 9200, train/loss: 0.0
Step: 9200, train/grad_norm: 6.826321623520926e-06
Step: 9200, train/learning_rate: 3.9052833017194644e-05
Step: 9200, train/epoch: 2.1894335746765137
Step: 9210, train/loss: 0.0
Step: 9210, train/grad_norm: 5.289485216053436e-06
Step: 9210, train/learning_rate: 3.904093318851665e-05
Step: 9210, train/epoch: 2.1918134689331055
Step: 9220, train/loss: 0.0
Step: 9220, train/grad_norm: 2.2461166736320592e-05
Step: 9220, train/learning_rate: 3.902903335983865e-05
Step: 9220, train/epoch: 2.194193124771118
Step: 9230, train/loss: 0.13050000369548798
Step: 9230, train/grad_norm: 667.2916259765625
Step: 9230, train/learning_rate: 3.901713353116065e-05
Step: 9230, train/epoch: 2.19657301902771
Step: 9240, train/loss: 0.1679999977350235
Step: 9240, train/grad_norm: 0.02529362589120865
Step: 9240, train/learning_rate: 3.900523734046146e-05
Step: 9240, train/epoch: 2.1989529132843018
Step: 9250, train/loss: 0.0
Step: 9250, train/grad_norm: 0.017471570521593094
Step: 9250, train/learning_rate: 3.8993337511783466e-05
Step: 9250, train/epoch: 2.2013328075408936
Step: 9260, train/loss: 0.0003000000142492354
Step: 9260, train/grad_norm: 0.0031025109346956015
Step: 9260, train/learning_rate: 3.898143768310547e-05
Step: 9260, train/epoch: 2.2037124633789062
Step: 9270, train/loss: 0.0
Step: 9270, train/grad_norm: 4.83433686895296e-05
Step: 9270, train/learning_rate: 3.896953785442747e-05
Step: 9270, train/epoch: 2.206092357635498
Step: 9280, train/loss: 0.00039999998989515007
Step: 9280, train/grad_norm: 0.00023980234982445836
Step: 9280, train/learning_rate: 3.8957638025749475e-05
Step: 9280, train/epoch: 2.20847225189209
Step: 9290, train/loss: 0.20630000531673431
Step: 9290, train/grad_norm: 0.00012633467849809676
Step: 9290, train/learning_rate: 3.8945741835050285e-05
Step: 9290, train/epoch: 2.2108519077301025
Step: 9300, train/loss: 0.0
Step: 9300, train/grad_norm: 0.002083432162180543
Step: 9300, train/learning_rate: 3.893384200637229e-05
Step: 9300, train/epoch: 2.2132318019866943
Step: 9310, train/loss: 0.0
Step: 9310, train/grad_norm: 0.0003126082010567188
Step: 9310, train/learning_rate: 3.892194217769429e-05
Step: 9310, train/epoch: 2.215611696243286
Step: 9320, train/loss: 0.0
Step: 9320, train/grad_norm: 1.0849154023162555e-05
Step: 9320, train/learning_rate: 3.8910042349016294e-05
Step: 9320, train/epoch: 2.217991352081299
Step: 9330, train/loss: 0.0
Step: 9330, train/grad_norm: 0.0019815133418887854
Step: 9330, train/learning_rate: 3.88981425203383e-05
Step: 9330, train/epoch: 2.2203712463378906
Step: 9340, train/loss: 0.0
Step: 9340, train/grad_norm: 0.00018650757556315511
Step: 9340, train/learning_rate: 3.888624632963911e-05
Step: 9340, train/epoch: 2.2227511405944824
Step: 9350, train/loss: 0.0
Step: 9350, train/grad_norm: 0.00011392357555450872
Step: 9350, train/learning_rate: 3.887434650096111e-05
Step: 9350, train/epoch: 2.225130796432495
Step: 9360, train/loss: 0.0
Step: 9360, train/grad_norm: 0.00012562039773911238
Step: 9360, train/learning_rate: 3.886244667228311e-05
Step: 9360, train/epoch: 2.227510690689087
Step: 9370, train/loss: 0.0
Step: 9370, train/grad_norm: 6.376417786668753e-06
Step: 9370, train/learning_rate: 3.8850546843605116e-05
Step: 9370, train/epoch: 2.2298905849456787
Step: 9380, train/loss: 0.0027000000700354576
Step: 9380, train/grad_norm: 8.103331492748111e-05
Step: 9380, train/learning_rate: 3.883864701492712e-05
Step: 9380, train/epoch: 2.2322702407836914
Step: 9390, train/loss: 0.0
Step: 9390, train/grad_norm: 0.0036879724357277155
Step: 9390, train/learning_rate: 3.882675082422793e-05
Step: 9390, train/epoch: 2.234650135040283
Step: 9400, train/loss: 0.0
Step: 9400, train/grad_norm: 0.0002240782487206161
Step: 9400, train/learning_rate: 3.881485099554993e-05
Step: 9400, train/epoch: 2.237030029296875
Step: 9410, train/loss: 0.14229999482631683
Step: 9410, train/grad_norm: 0.03521233797073364
Step: 9410, train/learning_rate: 3.8802951166871935e-05
Step: 9410, train/epoch: 2.239409923553467
Step: 9420, train/loss: 0.11540000140666962
Step: 9420, train/grad_norm: 0.00013807369396090508
Step: 9420, train/learning_rate: 3.879105133819394e-05
Step: 9420, train/epoch: 2.2417895793914795
Step: 9430, train/loss: 0.0
Step: 9430, train/grad_norm: 0.00016716467507649213
Step: 9430, train/learning_rate: 3.877915150951594e-05
Step: 9430, train/epoch: 2.2441694736480713
Step: 9440, train/loss: 0.0
Step: 9440, train/grad_norm: 3.835927782347426e-05
Step: 9440, train/learning_rate: 3.876725531881675e-05
Step: 9440, train/epoch: 2.246549367904663
Step: 9450, train/loss: 0.08550000190734863
Step: 9450, train/grad_norm: 0.006875837221741676
Step: 9450, train/learning_rate: 3.8755355490138754e-05
Step: 9450, train/epoch: 2.248929023742676
Step: 9460, train/loss: 0.0
Step: 9460, train/grad_norm: 0.00018706938135437667
Step: 9460, train/learning_rate: 3.874345566146076e-05
Step: 9460, train/epoch: 2.2513089179992676
Step: 9470, train/loss: 0.0333000011742115
Step: 9470, train/grad_norm: 229.4890594482422
Step: 9470, train/learning_rate: 3.873155583278276e-05
Step: 9470, train/epoch: 2.2536888122558594
Step: 9480, train/loss: 0.0
Step: 9480, train/grad_norm: 0.0018816444789990783
Step: 9480, train/learning_rate: 3.871965600410476e-05
Step: 9480, train/epoch: 2.256068468093872
Step: 9490, train/loss: 0.0
Step: 9490, train/grad_norm: 0.00011774701124522835
Step: 9490, train/learning_rate: 3.870775981340557e-05
Step: 9490, train/epoch: 2.258448362350464
Step: 9500, train/loss: 0.0
Step: 9500, train/grad_norm: 0.0001358461449854076
Step: 9500, train/learning_rate: 3.8695859984727576e-05
Step: 9500, train/epoch: 2.2608282566070557
Step: 9510, train/loss: 0.0
Step: 9510, train/grad_norm: 0.00019424883066676557
Step: 9510, train/learning_rate: 3.868396015604958e-05
Step: 9510, train/epoch: 2.2632079124450684
Step: 9520, train/loss: 0.0
Step: 9520, train/grad_norm: 4.9662347009871155e-05
Step: 9520, train/learning_rate: 3.867206032737158e-05
Step: 9520, train/epoch: 2.26558780670166
Step: 9530, train/loss: 0.0
Step: 9530, train/grad_norm: 0.0008772997534833848
Step: 9530, train/learning_rate: 3.8660160498693585e-05
Step: 9530, train/epoch: 2.267967700958252
Step: 9540, train/loss: 0.0
Step: 9540, train/grad_norm: 0.000759829708840698
Step: 9540, train/learning_rate: 3.8648264307994395e-05
Step: 9540, train/epoch: 2.2703473567962646
Step: 9550, train/loss: 9.999999747378752e-05
Step: 9550, train/grad_norm: 7.740795990685001e-05
Step: 9550, train/learning_rate: 3.86363644793164e-05
Step: 9550, train/epoch: 2.2727272510528564
Step: 9560, train/loss: 0.03610000014305115
Step: 9560, train/grad_norm: 0.00407874071970582
Step: 9560, train/learning_rate: 3.86244646506384e-05
Step: 9560, train/epoch: 2.2751071453094482
Step: 9570, train/loss: 9.999999747378752e-05
Step: 9570, train/grad_norm: 0.0011212899116799235
Step: 9570, train/learning_rate: 3.8612564821960405e-05
Step: 9570, train/epoch: 2.277486801147461
Step: 9580, train/loss: 0.0
Step: 9580, train/grad_norm: 1.030602788887336e-06
Step: 9580, train/learning_rate: 3.860066499328241e-05
Step: 9580, train/epoch: 2.2798666954040527
Step: 9590, train/loss: 0.0
Step: 9590, train/grad_norm: 3.9232162407643045e-07
Step: 9590, train/learning_rate: 3.858876880258322e-05
Step: 9590, train/epoch: 2.2822465896606445
Step: 9600, train/loss: 0.0
Step: 9600, train/grad_norm: 3.969962278915773e-07
Step: 9600, train/learning_rate: 3.857686897390522e-05
Step: 9600, train/epoch: 2.2846264839172363
Step: 9610, train/loss: 0.0044999998062849045
Step: 9610, train/grad_norm: 0.0021962958853691816
Step: 9610, train/learning_rate: 3.8564969145227224e-05
Step: 9610, train/epoch: 2.287006139755249
Step: 9620, train/loss: 0.0
Step: 9620, train/grad_norm: 0.00036625986103899777
Step: 9620, train/learning_rate: 3.855306931654923e-05
Step: 9620, train/epoch: 2.289386034011841
Step: 9630, train/loss: 0.049400001764297485
Step: 9630, train/grad_norm: 8.423235997589984e-10
Step: 9630, train/learning_rate: 3.854116948787123e-05
Step: 9630, train/epoch: 2.2917659282684326
Step: 9640, train/loss: 0.0
Step: 9640, train/grad_norm: 1.5224912885969388e-08
Step: 9640, train/learning_rate: 3.852927329717204e-05
Step: 9640, train/epoch: 2.2941455841064453
Step: 9650, train/loss: 0.00039999998989515007
Step: 9650, train/grad_norm: 1.9469260337245942e-07
Step: 9650, train/learning_rate: 3.851737346849404e-05
Step: 9650, train/epoch: 2.296525478363037
Step: 9660, train/loss: 0.0
Step: 9660, train/grad_norm: 8.277699706238373e-12
Step: 9660, train/learning_rate: 3.8505473639816046e-05
Step: 9660, train/epoch: 2.298905372619629
Step: 9670, train/loss: 0.0
Step: 9670, train/grad_norm: 5.0665036610553216e-09
Step: 9670, train/learning_rate: 3.849357381113805e-05
Step: 9670, train/epoch: 2.3012850284576416
Step: 9680, train/loss: 0.0
Step: 9680, train/grad_norm: 1.8075446917009685e-08
Step: 9680, train/learning_rate: 3.848167398246005e-05
Step: 9680, train/epoch: 2.3036649227142334
Step: 9690, train/loss: 0.12729999423027039
Step: 9690, train/grad_norm: 874.9014282226562
Step: 9690, train/learning_rate: 3.846977779176086e-05
Step: 9690, train/epoch: 2.306044816970825
Step: 9700, train/loss: 0.0
Step: 9700, train/grad_norm: 2.8404743716237135e-05
Step: 9700, train/learning_rate: 3.8457877963082865e-05
Step: 9700, train/epoch: 2.308424472808838
Step: 9710, train/loss: 0.0
Step: 9710, train/grad_norm: 2.01376360031702e-10
Step: 9710, train/learning_rate: 3.844597813440487e-05
Step: 9710, train/epoch: 2.3108043670654297
Step: 9720, train/loss: 0.0
Step: 9720, train/grad_norm: 2.791771924748332e-09
Step: 9720, train/learning_rate: 3.843407830572687e-05
Step: 9720, train/epoch: 2.3131842613220215
Step: 9730, train/loss: 0.0
Step: 9730, train/grad_norm: 7.670834634154744e-08
Step: 9730, train/learning_rate: 3.8422178477048874e-05
Step: 9730, train/epoch: 2.315563917160034
Step: 9740, train/loss: 0.0
Step: 9740, train/grad_norm: 9.437950421187224e-12
Step: 9740, train/learning_rate: 3.8410282286349684e-05
Step: 9740, train/epoch: 2.317943811416626
Step: 9750, train/loss: 0.0
Step: 9750, train/grad_norm: 3.0524563044309616e-05
Step: 9750, train/learning_rate: 3.839838245767169e-05
Step: 9750, train/epoch: 2.3203237056732178
Step: 9760, train/loss: 0.0
Step: 9760, train/grad_norm: 7.381161792885393e-10
Step: 9760, train/learning_rate: 3.838648262899369e-05
Step: 9760, train/epoch: 2.3227033615112305
Step: 9770, train/loss: 0.0
Step: 9770, train/grad_norm: 1.5156617627631874e-10
Step: 9770, train/learning_rate: 3.837458280031569e-05
Step: 9770, train/epoch: 2.3250832557678223
Step: 9780, train/loss: 0.0
Step: 9780, train/grad_norm: 1.353556733363348e-08
Step: 9780, train/learning_rate: 3.8362682971637696e-05
Step: 9780, train/epoch: 2.327463150024414
Step: 9790, train/loss: 0.023900000378489494
Step: 9790, train/grad_norm: 1.2070255920790512e-14
Step: 9790, train/learning_rate: 3.8350786780938506e-05
Step: 9790, train/epoch: 2.329843044281006
Step: 9800, train/loss: 0.0
Step: 9800, train/grad_norm: 7.152941572385885e-10
Step: 9800, train/learning_rate: 3.833888695226051e-05
Step: 9800, train/epoch: 2.3322227001190186
Step: 9810, train/loss: 0.0
Step: 9810, train/grad_norm: 1.1706574696290772e-06
Step: 9810, train/learning_rate: 3.832698712358251e-05
Step: 9810, train/epoch: 2.3346025943756104
Step: 9820, train/loss: 0.0
Step: 9820, train/grad_norm: 2.899934337108334e-08
Step: 9820, train/learning_rate: 3.8315087294904515e-05
Step: 9820, train/epoch: 2.336982488632202
Step: 9830, train/loss: 0.0
Step: 9830, train/grad_norm: 2.0347233013057187e-12
Step: 9830, train/learning_rate: 3.830318746622652e-05
Step: 9830, train/epoch: 2.339362144470215
Step: 9840, train/loss: 0.0
Step: 9840, train/grad_norm: 0.00017465429846197367
Step: 9840, train/learning_rate: 3.829129127552733e-05
Step: 9840, train/epoch: 2.3417420387268066
Step: 9850, train/loss: 0.020600000396370888
Step: 9850, train/grad_norm: 0.2235191911458969
Step: 9850, train/learning_rate: 3.827939144684933e-05
Step: 9850, train/epoch: 2.3441219329833984
Step: 9860, train/loss: 0.2793999910354614
Step: 9860, train/grad_norm: 8.17461568658473e-06
Step: 9860, train/learning_rate: 3.8267491618171334e-05
Step: 9860, train/epoch: 2.346501588821411
Step: 9870, train/loss: 0.052400000393390656
Step: 9870, train/grad_norm: 6.544968300659093e-07
Step: 9870, train/learning_rate: 3.825559178949334e-05
Step: 9870, train/epoch: 2.348881483078003
Step: 9880, train/loss: 0.00019999999494757503
Step: 9880, train/grad_norm: 0.0010540458606556058
Step: 9880, train/learning_rate: 3.824369196081534e-05
Step: 9880, train/epoch: 2.3512613773345947
Step: 9890, train/loss: 0.0
Step: 9890, train/grad_norm: 0.0001769695954862982
Step: 9890, train/learning_rate: 3.823179577011615e-05
Step: 9890, train/epoch: 2.3536410331726074
Step: 9900, train/loss: 0.00019999999494757503
Step: 9900, train/grad_norm: 1.3766479867172166e-07
Step: 9900, train/learning_rate: 3.821989594143815e-05
Step: 9900, train/epoch: 2.356020927429199
Step: 9910, train/loss: 0.03669999912381172
Step: 9910, train/grad_norm: 1.295984919380544e-08
Step: 9910, train/learning_rate: 3.8207996112760156e-05
Step: 9910, train/epoch: 2.358400821685791
Step: 9920, train/loss: 0.0
Step: 9920, train/grad_norm: 2.3563938356119252e-08
Step: 9920, train/learning_rate: 3.819609628408216e-05
Step: 9920, train/epoch: 2.3607804775238037
Step: 9930, train/loss: 0.0
Step: 9930, train/grad_norm: 8.876382935341098e-07
Step: 9930, train/learning_rate: 3.818419645540416e-05
Step: 9930, train/epoch: 2.3631603717803955
Step: 9940, train/loss: 0.0
Step: 9940, train/grad_norm: 3.02218232661744e-08
Step: 9940, train/learning_rate: 3.817230026470497e-05
Step: 9940, train/epoch: 2.3655402660369873
Step: 9950, train/loss: 0.09459999948740005
Step: 9950, train/grad_norm: 431.91943359375
Step: 9950, train/learning_rate: 3.8160400436026976e-05
Step: 9950, train/epoch: 2.367919921875
Step: 9960, train/loss: 0.0
Step: 9960, train/grad_norm: 0.001441356842406094
Step: 9960, train/learning_rate: 3.814850060734898e-05
Step: 9960, train/epoch: 2.370299816131592
Step: 9970, train/loss: 0.18440000712871552
Step: 9970, train/grad_norm: 1.5216245685678587e-07
Step: 9970, train/learning_rate: 3.813660077867098e-05
Step: 9970, train/epoch: 2.3726797103881836
Step: 9980, train/loss: 0.002899999963119626
Step: 9980, train/grad_norm: 23.705352783203125
Step: 9980, train/learning_rate: 3.8124700949992985e-05
Step: 9980, train/epoch: 2.3750596046447754
Step: 9990, train/loss: 0.2567000091075897
Step: 9990, train/grad_norm: 7.694319356232882e-05
Step: 9990, train/learning_rate: 3.8112804759293795e-05
Step: 9990, train/epoch: 2.377439260482788
Step: 10000, train/loss: 0.0
Step: 10000, train/grad_norm: 0.04190335050225258
Step: 10000, train/learning_rate: 3.81009049306158e-05
Step: 10000, train/epoch: 2.37981915473938
Step: 10010, train/loss: 0.414900004863739
Step: 10010, train/grad_norm: 0.00032300985185429454
Step: 10010, train/learning_rate: 3.80890051019378e-05
Step: 10010, train/epoch: 2.3821990489959717
Step: 10020, train/loss: 0.0010000000474974513
Step: 10020, train/grad_norm: 0.006575240753591061
Step: 10020, train/learning_rate: 3.8077105273259804e-05
Step: 10020, train/epoch: 2.3845787048339844
Step: 10030, train/loss: 0.19709999859333038
Step: 10030, train/grad_norm: 0.057821910828351974
Step: 10030, train/learning_rate: 3.806520544458181e-05
Step: 10030, train/epoch: 2.386958599090576
Step: 10040, train/loss: 0.00989999994635582
Step: 10040, train/grad_norm: 0.0013704474549740553
Step: 10040, train/learning_rate: 3.805330925388262e-05
Step: 10040, train/epoch: 2.389338493347168
Step: 10050, train/loss: 0.004399999976158142
Step: 10050, train/grad_norm: 0.0001454266457585618
Step: 10050, train/learning_rate: 3.804140942520462e-05
Step: 10050, train/epoch: 2.3917181491851807
Step: 10060, train/loss: 0.0
Step: 10060, train/grad_norm: 0.000134549816721119
Step: 10060, train/learning_rate: 3.802950959652662e-05
Step: 10060, train/epoch: 2.3940980434417725
Step: 10070, train/loss: 9.999999747378752e-05
Step: 10070, train/grad_norm: 0.0004835338331758976
Step: 10070, train/learning_rate: 3.8017609767848626e-05
Step: 10070, train/epoch: 2.3964779376983643
Step: 10080, train/loss: 0.0052999998442828655
Step: 10080, train/grad_norm: 4.3398737034294754e-05
Step: 10080, train/learning_rate: 3.800570993917063e-05
Step: 10080, train/epoch: 2.398857593536377
Step: 10090, train/loss: 0.0
Step: 10090, train/grad_norm: 0.009876100346446037
Step: 10090, train/learning_rate: 3.799381374847144e-05
Step: 10090, train/epoch: 2.4012374877929688
Step: 10100, train/loss: 0.1062999963760376
Step: 10100, train/grad_norm: 0.0035358748864382505
Step: 10100, train/learning_rate: 3.798191391979344e-05
Step: 10100, train/epoch: 2.4036173820495605
Step: 10110, train/loss: 0.0003000000142492354
Step: 10110, train/grad_norm: 3.2029527119448176e-07
Step: 10110, train/learning_rate: 3.7970014091115445e-05
Step: 10110, train/epoch: 2.4059970378875732
Step: 10120, train/loss: 0.0
Step: 10120, train/grad_norm: 1.3621388461615425e-06
Step: 10120, train/learning_rate: 3.795811426243745e-05
Step: 10120, train/epoch: 2.408376932144165
Step: 10130, train/loss: 0.0
Step: 10130, train/grad_norm: 0.00012817507376894355
Step: 10130, train/learning_rate: 3.794621443375945e-05
Step: 10130, train/epoch: 2.410756826400757
Step: 10140, train/loss: 0.0
Step: 10140, train/grad_norm: 4.75452088721795e-06
Step: 10140, train/learning_rate: 3.793431824306026e-05
Step: 10140, train/epoch: 2.4131367206573486
Step: 10150, train/loss: 0.0
Step: 10150, train/grad_norm: 0.00028751877835020423
Step: 10150, train/learning_rate: 3.7922418414382264e-05
Step: 10150, train/epoch: 2.4155163764953613
Step: 10160, train/loss: 9.999999747378752e-05
Step: 10160, train/grad_norm: 1.7357913293380989e-06
Step: 10160, train/learning_rate: 3.791051858570427e-05
Step: 10160, train/epoch: 2.417896270751953
Step: 10170, train/loss: 0.017400000244379044
Step: 10170, train/grad_norm: 255.2430877685547
Step: 10170, train/learning_rate: 3.789861875702627e-05
Step: 10170, train/epoch: 2.420276165008545
Step: 10180, train/loss: 0.0015999999595806003
Step: 10180, train/grad_norm: 4.06398781294115e-09
Step: 10180, train/learning_rate: 3.788671892834827e-05
Step: 10180, train/epoch: 2.4226558208465576
Step: 10190, train/loss: 0.0
Step: 10190, train/grad_norm: 7.241012644954026e-05
Step: 10190, train/learning_rate: 3.787482273764908e-05
Step: 10190, train/epoch: 2.4250357151031494
Step: 10200, train/loss: 0.08709999918937683
Step: 10200, train/grad_norm: 4.540245299722301e-07
Step: 10200, train/learning_rate: 3.7862922908971086e-05
Step: 10200, train/epoch: 2.427415609359741
Step: 10210, train/loss: 0.0005000000237487257
Step: 10210, train/grad_norm: 1.1750387329811929e-07
Step: 10210, train/learning_rate: 3.785102308029309e-05
Step: 10210, train/epoch: 2.429795265197754
Step: 10220, train/loss: 0.08479999750852585
Step: 10220, train/grad_norm: 8.248159429058433e-05
Step: 10220, train/learning_rate: 3.783912325161509e-05
Step: 10220, train/epoch: 2.4321751594543457
Step: 10230, train/loss: 0.0
Step: 10230, train/grad_norm: 0.0001185888031614013
Step: 10230, train/learning_rate: 3.7827223422937095e-05
Step: 10230, train/epoch: 2.4345550537109375
Step: 10240, train/loss: 0.0
Step: 10240, train/grad_norm: 5.416203748609405e-06
Step: 10240, train/learning_rate: 3.7815327232237905e-05
Step: 10240, train/epoch: 2.43693470954895
Step: 10250, train/loss: 0.0
Step: 10250, train/grad_norm: 0.01301341038197279
Step: 10250, train/learning_rate: 3.780342740355991e-05
Step: 10250, train/epoch: 2.439314603805542
Step: 10260, train/loss: 0.0
Step: 10260, train/grad_norm: 0.15690957009792328
Step: 10260, train/learning_rate: 3.779152757488191e-05
Step: 10260, train/epoch: 2.441694498062134
Step: 10270, train/loss: 0.007699999958276749
Step: 10270, train/grad_norm: 223.25843811035156
Step: 10270, train/learning_rate: 3.7779627746203914e-05
Step: 10270, train/epoch: 2.4440741539001465
Step: 10280, train/loss: 0.003599999938160181
Step: 10280, train/grad_norm: 9.205585627114488e-08
Step: 10280, train/learning_rate: 3.776772791752592e-05
Step: 10280, train/epoch: 2.4464540481567383
Step: 10290, train/loss: 0.0006000000284984708
Step: 10290, train/grad_norm: 2.0999245009534206e-08
Step: 10290, train/learning_rate: 3.775583172682673e-05
Step: 10290, train/epoch: 2.44883394241333
Step: 10300, train/loss: 0.0
Step: 10300, train/grad_norm: 2.6758552849059924e-05
Step: 10300, train/learning_rate: 3.774393189814873e-05
Step: 10300, train/epoch: 2.4512135982513428
Step: 10310, train/loss: 0.15559999644756317
Step: 10310, train/grad_norm: 7.246400400617858e-06
Step: 10310, train/learning_rate: 3.7732032069470733e-05
Step: 10310, train/epoch: 2.4535934925079346
Step: 10320, train/loss: 0.0
Step: 10320, train/grad_norm: 0.003601135453209281
Step: 10320, train/learning_rate: 3.7720132240792736e-05
Step: 10320, train/epoch: 2.4559733867645264
Step: 10330, train/loss: 0.0
Step: 10330, train/grad_norm: 0.03737605735659599
Step: 10330, train/learning_rate: 3.770823241211474e-05
Step: 10330, train/epoch: 2.458353281021118
Step: 10340, train/loss: 0.0
Step: 10340, train/grad_norm: 1.1495243512626985e-07
Step: 10340, train/learning_rate: 3.769633622141555e-05
Step: 10340, train/epoch: 2.460732936859131
Step: 10350, train/loss: 0.0
Step: 10350, train/grad_norm: 3.37157653120812e-05
Step: 10350, train/learning_rate: 3.768443639273755e-05
Step: 10350, train/epoch: 2.4631128311157227
Step: 10360, train/loss: 0.0
Step: 10360, train/grad_norm: 1.491998204983247e-07
Step: 10360, train/learning_rate: 3.7672536564059556e-05
Step: 10360, train/epoch: 2.4654927253723145
Step: 10370, train/loss: 0.11339999735355377
Step: 10370, train/grad_norm: 50.206520080566406
Step: 10370, train/learning_rate: 3.766063673538156e-05
Step: 10370, train/epoch: 2.467872381210327
Step: 10380, train/loss: 0.0
Step: 10380, train/grad_norm: 3.5088027061647153e-07
Step: 10380, train/learning_rate: 3.764873690670356e-05
Step: 10380, train/epoch: 2.470252275466919
Step: 10390, train/loss: 0.0010000000474974513
Step: 10390, train/grad_norm: 0.00041231175418943167
Step: 10390, train/learning_rate: 3.763684071600437e-05
Step: 10390, train/epoch: 2.4726321697235107
Step: 10400, train/loss: 0.18359999358654022
Step: 10400, train/grad_norm: 271.71783447265625
Step: 10400, train/learning_rate: 3.7624940887326375e-05
Step: 10400, train/epoch: 2.4750118255615234
Step: 10410, train/loss: 0.0
Step: 10410, train/grad_norm: 0.022391991689801216
Step: 10410, train/learning_rate: 3.761304105864838e-05
Step: 10410, train/epoch: 2.4773917198181152
Step: 10420, train/loss: 0.0
Step: 10420, train/grad_norm: 3.072176957275019e-09
Step: 10420, train/learning_rate: 3.760114122997038e-05
Step: 10420, train/epoch: 2.479771614074707
Step: 10430, train/loss: 0.0
Step: 10430, train/grad_norm: 0.002952669747173786
Step: 10430, train/learning_rate: 3.7589241401292384e-05
Step: 10430, train/epoch: 2.4821512699127197
Step: 10440, train/loss: 0.0
Step: 10440, train/grad_norm: 4.840519068238791e-06
Step: 10440, train/learning_rate: 3.7577345210593194e-05
Step: 10440, train/epoch: 2.4845311641693115
Step: 10450, train/loss: 0.0
Step: 10450, train/grad_norm: 5.330586532181769e-07
Step: 10450, train/learning_rate: 3.75654453819152e-05
Step: 10450, train/epoch: 2.4869110584259033
Step: 10460, train/loss: 0.0
Step: 10460, train/grad_norm: 4.9957992587224e-07
Step: 10460, train/learning_rate: 3.75535455532372e-05
Step: 10460, train/epoch: 2.489290714263916
Step: 10470, train/loss: 0.0
Step: 10470, train/grad_norm: 0.00035329171805642545
Step: 10470, train/learning_rate: 3.75416457245592e-05
Step: 10470, train/epoch: 2.491670608520508
Step: 10480, train/loss: 0.06880000233650208
Step: 10480, train/grad_norm: 9.228720188048101e-08
Step: 10480, train/learning_rate: 3.752974953386001e-05
Step: 10480, train/epoch: 2.4940505027770996
Step: 10490, train/loss: 0.0
Step: 10490, train/grad_norm: 2.587085248251242e-07
Step: 10490, train/learning_rate: 3.7517849705182016e-05
Step: 10490, train/epoch: 2.4964301586151123
Step: 10500, train/loss: 0.1589999943971634
Step: 10500, train/grad_norm: 0.002055220305919647
Step: 10500, train/learning_rate: 3.750594987650402e-05
Step: 10500, train/epoch: 2.498810052871704
Step: 10510, train/loss: 0.027799999341368675
Step: 10510, train/grad_norm: 0.02893793024122715
Step: 10510, train/learning_rate: 3.749405004782602e-05
Step: 10510, train/epoch: 2.501189947128296
Step: 10520, train/loss: 9.999999747378752e-05
Step: 10520, train/grad_norm: 0.00010220969852525741
Step: 10520, train/learning_rate: 3.7482150219148025e-05
Step: 10520, train/epoch: 2.5035698413848877
Step: 10530, train/loss: 0.0
Step: 10530, train/grad_norm: 4.144762897340115e-06
Step: 10530, train/learning_rate: 3.7470254028448835e-05
Step: 10530, train/epoch: 2.5059494972229004
Step: 10540, train/loss: 0.049800001084804535
Step: 10540, train/grad_norm: 0.01936373859643936
Step: 10540, train/learning_rate: 3.745835419977084e-05
Step: 10540, train/epoch: 2.508329391479492
Step: 10550, train/loss: 0.0
Step: 10550, train/grad_norm: 3.46653068845626e-05
Step: 10550, train/learning_rate: 3.744645437109284e-05
Step: 10550, train/epoch: 2.510709285736084
Step: 10560, train/loss: 9.999999747378752e-05
Step: 10560, train/grad_norm: 2.9405146051431075e-05
Step: 10560, train/learning_rate: 3.7434554542414844e-05
Step: 10560, train/epoch: 2.5130889415740967
Step: 10570, train/loss: 0.0
Step: 10570, train/grad_norm: 0.010330308228731155
Step: 10570, train/learning_rate: 3.742265471373685e-05
Step: 10570, train/epoch: 2.5154688358306885
Step: 10580, train/loss: 0.0
Step: 10580, train/grad_norm: 1.9891916963388212e-05
Step: 10580, train/learning_rate: 3.741075852303766e-05
Step: 10580, train/epoch: 2.5178487300872803
Step: 10590, train/loss: 0.0
Step: 10590, train/grad_norm: 7.668011676287279e-06
Step: 10590, train/learning_rate: 3.739885869435966e-05
Step: 10590, train/epoch: 2.520228385925293
Step: 10600, train/loss: 0.11259999871253967
Step: 10600, train/grad_norm: 8.464283018838614e-05
Step: 10600, train/learning_rate: 3.738695886568166e-05
Step: 10600, train/epoch: 2.5226082801818848
Step: 10610, train/loss: 0.0869000032544136
Step: 10610, train/grad_norm: 0.00010690910858102143
Step: 10610, train/learning_rate: 3.7375059037003666e-05
Step: 10610, train/epoch: 2.5249881744384766
Step: 10620, train/loss: 0.015300000086426735
Step: 10620, train/grad_norm: 0.0015085701597854495
Step: 10620, train/learning_rate: 3.736315920832567e-05
Step: 10620, train/epoch: 2.5273678302764893
Step: 10630, train/loss: 0.0
Step: 10630, train/grad_norm: 7.028906111372635e-06
Step: 10630, train/learning_rate: 3.735126301762648e-05
Step: 10630, train/epoch: 2.529747724533081
Step: 10640, train/loss: 0.0
Step: 10640, train/grad_norm: 4.147558684053365e-06
Step: 10640, train/learning_rate: 3.733936318894848e-05
Step: 10640, train/epoch: 2.532127618789673
Step: 10650, train/loss: 0.0
Step: 10650, train/grad_norm: 0.00013871326518710703
Step: 10650, train/learning_rate: 3.7327463360270485e-05
Step: 10650, train/epoch: 2.5345072746276855
Step: 10660, train/loss: 0.0
Step: 10660, train/grad_norm: 1.2047944437654223e-06
Step: 10660, train/learning_rate: 3.731556353159249e-05
Step: 10660, train/epoch: 2.5368871688842773
Step: 10670, train/loss: 0.0
Step: 10670, train/grad_norm: 2.1210338672972284e-05
Step: 10670, train/learning_rate: 3.730366370291449e-05
Step: 10670, train/epoch: 2.539267063140869
Step: 10680, train/loss: 0.0
Step: 10680, train/grad_norm: 1.1220521628274582e-05
Step: 10680, train/learning_rate: 3.72917675122153e-05
Step: 10680, train/epoch: 2.541646718978882
Step: 10690, train/loss: 0.0
Step: 10690, train/grad_norm: 4.540437203104375e-06
Step: 10690, train/learning_rate: 3.7279867683537304e-05
Step: 10690, train/epoch: 2.5440266132354736
Step: 10700, train/loss: 0.0
Step: 10700, train/grad_norm: 6.802491952839773e-06
Step: 10700, train/learning_rate: 3.726796785485931e-05
Step: 10700, train/epoch: 2.5464065074920654
Step: 10710, train/loss: 0.0
Step: 10710, train/grad_norm: 1.711544427962508e-05
Step: 10710, train/learning_rate: 3.725606802618131e-05
Step: 10710, train/epoch: 2.5487864017486572
Step: 10720, train/loss: 0.0
Step: 10720, train/grad_norm: 1.5462400142496335e-06
Step: 10720, train/learning_rate: 3.7244168197503313e-05
Step: 10720, train/epoch: 2.55116605758667
Step: 10730, train/loss: 0.0
Step: 10730, train/grad_norm: 4.6728962388442596e-07
Step: 10730, train/learning_rate: 3.7232272006804124e-05
Step: 10730, train/epoch: 2.5535459518432617
Step: 10740, train/loss: 0.05900000035762787
Step: 10740, train/grad_norm: 0.003603365970775485
Step: 10740, train/learning_rate: 3.7220372178126127e-05
Step: 10740, train/epoch: 2.5559258460998535
Step: 10750, train/loss: 0.22499999403953552
Step: 10750, train/grad_norm: 1.0099391829498927e-06
Step: 10750, train/learning_rate: 3.720847234944813e-05
Step: 10750, train/epoch: 2.558305501937866
Step: 10760, train/loss: 0.0
Step: 10760, train/grad_norm: 0.0007473922451026738
Step: 10760, train/learning_rate: 3.719657252077013e-05
Step: 10760, train/epoch: 2.560685396194458
Step: 10770, train/loss: 0.0
Step: 10770, train/grad_norm: 0.01788860373198986
Step: 10770, train/learning_rate: 3.7184672692092136e-05
Step: 10770, train/epoch: 2.56306529045105
Step: 10780, train/loss: 0.0
Step: 10780, train/grad_norm: 2.0583987861755304e-05
Step: 10780, train/learning_rate: 3.7172776501392946e-05
Step: 10780, train/epoch: 2.5654449462890625
Step: 10790, train/loss: 0.0007999999797903001
Step: 10790, train/grad_norm: 3.905014818883501e-05
Step: 10790, train/learning_rate: 3.716087667271495e-05
Step: 10790, train/epoch: 2.5678248405456543
Step: 10800, train/loss: 0.004900000058114529
Step: 10800, train/grad_norm: 0.00013853018754161894
Step: 10800, train/learning_rate: 3.714897684403695e-05
Step: 10800, train/epoch: 2.570204734802246
Step: 10810, train/loss: 9.999999747378752e-05
Step: 10810, train/grad_norm: 0.003525816835463047
Step: 10810, train/learning_rate: 3.7137077015358955e-05
Step: 10810, train/epoch: 2.572584390640259
Step: 10820, train/loss: 0.0
Step: 10820, train/grad_norm: 0.00107288034632802
Step: 10820, train/learning_rate: 3.712517718668096e-05
Step: 10820, train/epoch: 2.5749642848968506
Step: 10830, train/loss: 0.0
Step: 10830, train/grad_norm: 0.00014430092414841056
Step: 10830, train/learning_rate: 3.711328099598177e-05
Step: 10830, train/epoch: 2.5773441791534424
Step: 10840, train/loss: 0.0
Step: 10840, train/grad_norm: 0.0002467320009600371
Step: 10840, train/learning_rate: 3.710138116730377e-05
Step: 10840, train/epoch: 2.579723834991455
Step: 10850, train/loss: 0.0
Step: 10850, train/grad_norm: 5.151583081897115e-06
Step: 10850, train/learning_rate: 3.7089481338625774e-05
Step: 10850, train/epoch: 2.582103729248047
Step: 10860, train/loss: 0.06679999828338623
Step: 10860, train/grad_norm: 0.0001216179080074653
Step: 10860, train/learning_rate: 3.707758150994778e-05
Step: 10860, train/epoch: 2.5844836235046387
Step: 10870, train/loss: 0.0
Step: 10870, train/grad_norm: 0.0008567136828787625
Step: 10870, train/learning_rate: 3.706568168126978e-05
Step: 10870, train/epoch: 2.5868632793426514
Step: 10880, train/loss: 0.20000000298023224
Step: 10880, train/grad_norm: 4.586471550283022e-05
Step: 10880, train/learning_rate: 3.705378549057059e-05
Step: 10880, train/epoch: 2.589243173599243
Step: 10890, train/loss: 0.0
Step: 10890, train/grad_norm: 0.00036484963493421674
Step: 10890, train/learning_rate: 3.704188566189259e-05
Step: 10890, train/epoch: 2.591623067855835
Step: 10900, train/loss: 0.0
Step: 10900, train/grad_norm: 0.006075431127101183
Step: 10900, train/learning_rate: 3.7029985833214596e-05
Step: 10900, train/epoch: 2.5940029621124268
Step: 10910, train/loss: 0.0
Step: 10910, train/grad_norm: 0.010830539278686047
Step: 10910, train/learning_rate: 3.70180860045366e-05
Step: 10910, train/epoch: 2.5963826179504395
Step: 10920, train/loss: 0.0
Step: 10920, train/grad_norm: 0.00016791748930700123
Step: 10920, train/learning_rate: 3.70061861758586e-05
Step: 10920, train/epoch: 2.5987625122070312
Step: 10930, train/loss: 0.0
Step: 10930, train/grad_norm: 0.0006705792620778084
Step: 10930, train/learning_rate: 3.699428998515941e-05
Step: 10930, train/epoch: 2.601142406463623
Step: 10940, train/loss: 0.0
Step: 10940, train/grad_norm: 0.0010399629827588797
Step: 10940, train/learning_rate: 3.6982390156481415e-05
Step: 10940, train/epoch: 2.6035220623016357
Step: 10950, train/loss: 0.0
Step: 10950, train/grad_norm: 4.320867810747586e-05
Step: 10950, train/learning_rate: 3.697049032780342e-05
Step: 10950, train/epoch: 2.6059019565582275
Step: 10960, train/loss: 0.0
Step: 10960, train/grad_norm: 0.012468703091144562
Step: 10960, train/learning_rate: 3.695859049912542e-05
Step: 10960, train/epoch: 2.6082818508148193
Step: 10970, train/loss: 0.0
Step: 10970, train/grad_norm: 2.5008681404870003e-05
Step: 10970, train/learning_rate: 3.6946690670447424e-05
Step: 10970, train/epoch: 2.610661506652832
Step: 10980, train/loss: 0.0
Step: 10980, train/grad_norm: 0.00022227592126000673
Step: 10980, train/learning_rate: 3.6934794479748234e-05
Step: 10980, train/epoch: 2.613041400909424
Step: 10990, train/loss: 0.0
Step: 10990, train/grad_norm: 0.004190586507320404
Step: 10990, train/learning_rate: 3.692289465107024e-05
Step: 10990, train/epoch: 2.6154212951660156
Step: 11000, train/loss: 0.005799999926239252
Step: 11000, train/grad_norm: 78.73281860351562
Step: 11000, train/learning_rate: 3.691099482239224e-05
Step: 11000, train/epoch: 2.6178009510040283
Step: 11010, train/loss: 0.0
Step: 11010, train/grad_norm: 0.00016325761680491269
Step: 11010, train/learning_rate: 3.689909499371424e-05
Step: 11010, train/epoch: 2.62018084526062
Step: 11020, train/loss: 9.999999747378752e-05
Step: 11020, train/grad_norm: 5.077562946098624e-06
Step: 11020, train/learning_rate: 3.6887195165036246e-05
Step: 11020, train/epoch: 2.622560739517212
Step: 11030, train/loss: 0.0
Step: 11030, train/grad_norm: 2.4419257897534408e-05
Step: 11030, train/learning_rate: 3.6875298974337056e-05
Step: 11030, train/epoch: 2.6249403953552246
Step: 11040, train/loss: 0.0
Step: 11040, train/grad_norm: 0.00025077848113141954
Step: 11040, train/learning_rate: 3.686339914565906e-05
Step: 11040, train/epoch: 2.6273202896118164
Step: 11050, train/loss: 0.0015999999595806003
Step: 11050, train/grad_norm: 5.399991778176627e-07
Step: 11050, train/learning_rate: 3.685149931698106e-05
Step: 11050, train/epoch: 2.629700183868408
Step: 11060, train/loss: 0.09690000116825104
Step: 11060, train/grad_norm: 0.0003799355181399733
Step: 11060, train/learning_rate: 3.6839599488303065e-05
Step: 11060, train/epoch: 2.632080078125
Step: 11070, train/loss: 0.0
Step: 11070, train/grad_norm: 0.0031256580259650946
Step: 11070, train/learning_rate: 3.682769965962507e-05
Step: 11070, train/epoch: 2.6344597339630127
Step: 11080, train/loss: 0.0
Step: 11080, train/grad_norm: 7.268556510098279e-05
Step: 11080, train/learning_rate: 3.681580346892588e-05
Step: 11080, train/epoch: 2.6368396282196045
Step: 11090, train/loss: 0.0
Step: 11090, train/grad_norm: 4.402644844958559e-05
Step: 11090, train/learning_rate: 3.680390364024788e-05
Step: 11090, train/epoch: 2.6392195224761963
Step: 11100, train/loss: 0.0
Step: 11100, train/grad_norm: 0.00039346140692941844
Step: 11100, train/learning_rate: 3.6792003811569884e-05
Step: 11100, train/epoch: 2.641599178314209
Step: 11110, train/loss: 0.0
Step: 11110, train/grad_norm: 1.9365872105936432e-07
Step: 11110, train/learning_rate: 3.678010398289189e-05
Step: 11110, train/epoch: 2.643979072570801
Step: 11120, train/loss: 0.0
Step: 11120, train/grad_norm: 1.9852674085996114e-05
Step: 11120, train/learning_rate: 3.676820415421389e-05
Step: 11120, train/epoch: 2.6463589668273926
Step: 11130, train/loss: 0.1800999939441681
Step: 11130, train/grad_norm: 186.6616668701172
Step: 11130, train/learning_rate: 3.67563079635147e-05
Step: 11130, train/epoch: 2.6487386226654053
Step: 11140, train/loss: 0.0
Step: 11140, train/grad_norm: 0.0013530866708606482
Step: 11140, train/learning_rate: 3.6744408134836704e-05
Step: 11140, train/epoch: 2.651118516921997
Step: 11150, train/loss: 0.0003000000142492354
Step: 11150, train/grad_norm: 1.0940697393380105e-05
Step: 11150, train/learning_rate: 3.6732508306158707e-05
Step: 11150, train/epoch: 2.653498411178589
Step: 11160, train/loss: 0.0
Step: 11160, train/grad_norm: 9.355467045679688e-05
Step: 11160, train/learning_rate: 3.672060847748071e-05
Step: 11160, train/epoch: 2.6558780670166016
Step: 11170, train/loss: 0.0
Step: 11170, train/grad_norm: 7.909500709502026e-06
Step: 11170, train/learning_rate: 3.670870864880271e-05
Step: 11170, train/epoch: 2.6582579612731934
Step: 11180, train/loss: 0.018400000408291817
Step: 11180, train/grad_norm: 0.0037445472553372383
Step: 11180, train/learning_rate: 3.669681245810352e-05
Step: 11180, train/epoch: 2.660637855529785
Step: 11190, train/loss: 0.00019999999494757503
Step: 11190, train/grad_norm: 2.6121556758880615
Step: 11190, train/learning_rate: 3.6684912629425526e-05
Step: 11190, train/epoch: 2.663017511367798
Step: 11200, train/loss: 0.0
Step: 11200, train/grad_norm: 8.667509973747656e-05
Step: 11200, train/learning_rate: 3.667301280074753e-05
Step: 11200, train/epoch: 2.6653974056243896
Step: 11210, train/loss: 0.0013000000035390258
Step: 11210, train/grad_norm: 1.4056354302738328e-05
Step: 11210, train/learning_rate: 3.666111297206953e-05
Step: 11210, train/epoch: 2.6677772998809814
Step: 11220, train/loss: 0.0
Step: 11220, train/grad_norm: 1.1562030977074755e-06
Step: 11220, train/learning_rate: 3.6649213143391535e-05
Step: 11220, train/epoch: 2.670156955718994
Step: 11230, train/loss: 0.0
Step: 11230, train/grad_norm: 1.1709974387486e-05
Step: 11230, train/learning_rate: 3.6637316952692345e-05
Step: 11230, train/epoch: 2.672536849975586
Step: 11240, train/loss: 0.0
Step: 11240, train/grad_norm: 0.00014644752081949264
Step: 11240, train/learning_rate: 3.662541712401435e-05
Step: 11240, train/epoch: 2.6749167442321777
Step: 11250, train/loss: 0.04650000110268593
Step: 11250, train/grad_norm: 423.7769470214844
Step: 11250, train/learning_rate: 3.661351729533635e-05
Step: 11250, train/epoch: 2.6772966384887695
Step: 11260, train/loss: 0.0
Step: 11260, train/grad_norm: 7.120161171769723e-05
Step: 11260, train/learning_rate: 3.6601617466658354e-05
Step: 11260, train/epoch: 2.6796762943267822
Step: 11270, train/loss: 0.0
Step: 11270, train/grad_norm: 1.4196301023616797e-08
Step: 11270, train/learning_rate: 3.658971763798036e-05
Step: 11270, train/epoch: 2.682056188583374
Step: 11280, train/loss: 0.0010000000474974513
Step: 11280, train/grad_norm: 0.00030041043646633625
Step: 11280, train/learning_rate: 3.657782144728117e-05
Step: 11280, train/epoch: 2.684436082839966
Step: 11290, train/loss: 0.10159999877214432
Step: 11290, train/grad_norm: 3.722899782587774e-05
Step: 11290, train/learning_rate: 3.656592161860317e-05
Step: 11290, train/epoch: 2.6868157386779785
Step: 11300, train/loss: 9.999999747378752e-05
Step: 11300, train/grad_norm: 0.00016697611135896295
Step: 11300, train/learning_rate: 3.655402178992517e-05
Step: 11300, train/epoch: 2.6891956329345703
Step: 11310, train/loss: 0.0031999999191612005
Step: 11310, train/grad_norm: 5.619633157039061e-06
Step: 11310, train/learning_rate: 3.6542121961247176e-05
Step: 11310, train/epoch: 2.691575527191162
Step: 11320, train/loss: 0.1428000032901764
Step: 11320, train/grad_norm: 4.012740828329697e-05
Step: 11320, train/learning_rate: 3.653022213256918e-05
Step: 11320, train/epoch: 2.693955183029175
Step: 11330, train/loss: 0.0
Step: 11330, train/grad_norm: 5.0040343921864405e-06
Step: 11330, train/learning_rate: 3.651832594186999e-05
Step: 11330, train/epoch: 2.6963350772857666
Step: 11340, train/loss: 0.0
Step: 11340, train/grad_norm: 0.00019790510123129934
Step: 11340, train/learning_rate: 3.650642611319199e-05
Step: 11340, train/epoch: 2.6987149715423584
Step: 11350, train/loss: 0.042399998754262924
Step: 11350, train/grad_norm: 4.8963207518681884e-05
Step: 11350, train/learning_rate: 3.6494526284513995e-05
Step: 11350, train/epoch: 2.701094627380371
Step: 11360, train/loss: 0.0
Step: 11360, train/grad_norm: 0.00010276792454533279
Step: 11360, train/learning_rate: 3.6482626455836e-05
Step: 11360, train/epoch: 2.703474521636963
Step: 11370, train/loss: 0.0
Step: 11370, train/grad_norm: 2.6018578864750452e-05
Step: 11370, train/learning_rate: 3.6470726627158e-05
Step: 11370, train/epoch: 2.7058544158935547
Step: 11380, train/loss: 0.0
Step: 11380, train/grad_norm: 1.4377458683156874e-05
Step: 11380, train/learning_rate: 3.645883043645881e-05
Step: 11380, train/epoch: 2.7082340717315674
Step: 11390, train/loss: 0.052299998700618744
Step: 11390, train/grad_norm: 6.052577373338863e-05
Step: 11390, train/learning_rate: 3.6446930607780814e-05
Step: 11390, train/epoch: 2.710613965988159
Step: 11400, train/loss: 0.125
Step: 11400, train/grad_norm: 0.0003972326230723411
Step: 11400, train/learning_rate: 3.643503077910282e-05
Step: 11400, train/epoch: 2.712993860244751
Step: 11410, train/loss: 9.999999747378752e-05
Step: 11410, train/grad_norm: 5.102500654174946e-05
Step: 11410, train/learning_rate: 3.642313095042482e-05
Step: 11410, train/epoch: 2.7153735160827637
Step: 11420, train/loss: 0.0
Step: 11420, train/grad_norm: 0.0026655630208551884
Step: 11420, train/learning_rate: 3.641123112174682e-05
Step: 11420, train/epoch: 2.7177534103393555
Step: 11430, train/loss: 0.011699999682605267
Step: 11430, train/grad_norm: 1.6401676475652494e-05
Step: 11430, train/learning_rate: 3.639933493104763e-05
Step: 11430, train/epoch: 2.7201333045959473
Step: 11440, train/loss: 0.0
Step: 11440, train/grad_norm: 0.00014149703201837838
Step: 11440, train/learning_rate: 3.6387435102369636e-05
Step: 11440, train/epoch: 2.722513198852539
Step: 11450, train/loss: 0.0608999989926815
Step: 11450, train/grad_norm: 0.0001036000030580908
Step: 11450, train/learning_rate: 3.637553527369164e-05
Step: 11450, train/epoch: 2.7248928546905518
Step: 11460, train/loss: 0.0
Step: 11460, train/grad_norm: 0.00036885266308672726
Step: 11460, train/learning_rate: 3.636363544501364e-05
Step: 11460, train/epoch: 2.7272727489471436
Step: 11470, train/loss: 0.00139999995008111
Step: 11470, train/grad_norm: 3.380496127647348e-05
Step: 11470, train/learning_rate: 3.6351735616335645e-05
Step: 11470, train/epoch: 2.7296526432037354
Step: 11480, train/loss: 0.0
Step: 11480, train/grad_norm: 0.10778164863586426
Step: 11480, train/learning_rate: 3.6339839425636455e-05
Step: 11480, train/epoch: 2.732032299041748
Step: 11490, train/loss: 0.07989999651908875
Step: 11490, train/grad_norm: 4.021822678623721e-05
Step: 11490, train/learning_rate: 3.632793959695846e-05
Step: 11490, train/epoch: 2.73441219329834
Step: 11500, train/loss: 0.003000000026077032
Step: 11500, train/grad_norm: 4.252117378200637e-06
Step: 11500, train/learning_rate: 3.631603976828046e-05
Step: 11500, train/epoch: 2.7367920875549316
Step: 11510, train/loss: 0.0
Step: 11510, train/grad_norm: 2.6345451260567643e-06
Step: 11510, train/learning_rate: 3.6304139939602464e-05
Step: 11510, train/epoch: 2.7391717433929443
Step: 11520, train/loss: 0.21170000731945038
Step: 11520, train/grad_norm: 4.972148235538043e-05
Step: 11520, train/learning_rate: 3.629224011092447e-05
Step: 11520, train/epoch: 2.741551637649536
Step: 11530, train/loss: 9.999999747378752e-05
Step: 11530, train/grad_norm: 2.572145422163885e-05
Step: 11530, train/learning_rate: 3.628034392022528e-05
Step: 11530, train/epoch: 2.743931531906128
Step: 11540, train/loss: 0.0017000000225380063
Step: 11540, train/grad_norm: 34.28252029418945
Step: 11540, train/learning_rate: 3.626844409154728e-05
Step: 11540, train/epoch: 2.7463111877441406
Step: 11550, train/loss: 0.013799999840557575
Step: 11550, train/grad_norm: 1.0994164767907932e-05
Step: 11550, train/learning_rate: 3.6256544262869284e-05
Step: 11550, train/epoch: 2.7486910820007324
Step: 11560, train/loss: 0.13359999656677246
Step: 11560, train/grad_norm: 0.04868706315755844
Step: 11560, train/learning_rate: 3.6244644434191287e-05
Step: 11560, train/epoch: 2.751070976257324
Step: 11570, train/loss: 0.0
Step: 11570, train/grad_norm: 9.114218119066209e-05
Step: 11570, train/learning_rate: 3.623274460551329e-05
Step: 11570, train/epoch: 2.753450632095337
Step: 11580, train/loss: 0.0
Step: 11580, train/grad_norm: 0.00022859977616462857
Step: 11580, train/learning_rate: 3.62208484148141e-05
Step: 11580, train/epoch: 2.7558305263519287
Step: 11590, train/loss: 0.0
Step: 11590, train/grad_norm: 4.229478508932516e-05
Step: 11590, train/learning_rate: 3.62089485861361e-05
Step: 11590, train/epoch: 2.7582104206085205
Step: 11600, train/loss: 0.0
Step: 11600, train/grad_norm: 0.0010954826138913631
Step: 11600, train/learning_rate: 3.6197048757458106e-05
Step: 11600, train/epoch: 2.760590076446533
Step: 11610, train/loss: 0.0
Step: 11610, train/grad_norm: 7.563791587017477e-05
Step: 11610, train/learning_rate: 3.618514892878011e-05
Step: 11610, train/epoch: 2.762969970703125
Step: 11620, train/loss: 0.0
Step: 11620, train/grad_norm: 4.253576844348572e-05
Step: 11620, train/learning_rate: 3.617324910010211e-05
Step: 11620, train/epoch: 2.765349864959717
Step: 11630, train/loss: 0.18729999661445618
Step: 11630, train/grad_norm: 83.1962661743164
Step: 11630, train/learning_rate: 3.616135290940292e-05
Step: 11630, train/epoch: 2.7677297592163086
Step: 11640, train/loss: 0.007600000128149986
Step: 11640, train/grad_norm: 0.00033690687268972397
Step: 11640, train/learning_rate: 3.6149453080724925e-05
Step: 11640, train/epoch: 2.7701094150543213
Step: 11650, train/loss: 0.0957999974489212
Step: 11650, train/grad_norm: 0.0006424206076189876
Step: 11650, train/learning_rate: 3.613755325204693e-05
Step: 11650, train/epoch: 2.772489309310913
Step: 11660, train/loss: 9.999999747378752e-05
Step: 11660, train/grad_norm: 0.0035394385922700167
Step: 11660, train/learning_rate: 3.612565342336893e-05
Step: 11660, train/epoch: 2.774869203567505
Step: 11670, train/loss: 0.0
Step: 11670, train/grad_norm: 0.0001220217818627134
Step: 11670, train/learning_rate: 3.6113753594690934e-05
Step: 11670, train/epoch: 2.7772488594055176
Step: 11680, train/loss: 0.0
Step: 11680, train/grad_norm: 0.00012166388478362933
Step: 11680, train/learning_rate: 3.6101857403991744e-05
Step: 11680, train/epoch: 2.7796287536621094
Step: 11690, train/loss: 0.0
Step: 11690, train/grad_norm: 5.070577753940597e-05
Step: 11690, train/learning_rate: 3.608995757531375e-05
Step: 11690, train/epoch: 2.782008647918701
Step: 11700, train/loss: 0.0
Step: 11700, train/grad_norm: 9.857115219347179e-05
Step: 11700, train/learning_rate: 3.607805774663575e-05
Step: 11700, train/epoch: 2.784388303756714
Step: 11710, train/loss: 0.0
Step: 11710, train/grad_norm: 0.00011044136772397906
Step: 11710, train/learning_rate: 3.606615791795775e-05
Step: 11710, train/epoch: 2.7867681980133057
Step: 11720, train/loss: 0.0
Step: 11720, train/grad_norm: 0.0008330015698447824
Step: 11720, train/learning_rate: 3.6054258089279756e-05
Step: 11720, train/epoch: 2.7891480922698975
Step: 11730, train/loss: 0.02539999969303608
Step: 11730, train/grad_norm: 9.798513929126784e-05
Step: 11730, train/learning_rate: 3.6042361898580566e-05
Step: 11730, train/epoch: 2.79152774810791
Step: 11740, train/loss: 0.0
Step: 11740, train/grad_norm: 0.013981628231704235
Step: 11740, train/learning_rate: 3.603046206990257e-05
Step: 11740, train/epoch: 2.793907642364502
Step: 11750, train/loss: 0.1039000004529953
Step: 11750, train/grad_norm: 129.2662353515625
Step: 11750, train/learning_rate: 3.601856224122457e-05
Step: 11750, train/epoch: 2.7962875366210938
Step: 11760, train/loss: 0.0
Step: 11760, train/grad_norm: 0.0002060717815766111
Step: 11760, train/learning_rate: 3.6006662412546575e-05
Step: 11760, train/epoch: 2.7986671924591064
Step: 11770, train/loss: 0.0024999999441206455
Step: 11770, train/grad_norm: 0.03163784369826317
Step: 11770, train/learning_rate: 3.599476258386858e-05
Step: 11770, train/epoch: 2.8010470867156982
Step: 11780, train/loss: 0.0
Step: 11780, train/grad_norm: 6.38424296539597e-07
Step: 11780, train/learning_rate: 3.598286639316939e-05
Step: 11780, train/epoch: 2.80342698097229
Step: 11790, train/loss: 0.01269999984651804
Step: 11790, train/grad_norm: 0.09905245900154114
Step: 11790, train/learning_rate: 3.597096656449139e-05
Step: 11790, train/epoch: 2.805806875228882
Step: 11800, train/loss: 0.0
Step: 11800, train/grad_norm: 9.17547513381578e-05
Step: 11800, train/learning_rate: 3.5959066735813394e-05
Step: 11800, train/epoch: 2.8081865310668945
Step: 11810, train/loss: 0.01730000041425228
Step: 11810, train/grad_norm: 0.000125562641187571
Step: 11810, train/learning_rate: 3.59471669071354e-05
Step: 11810, train/epoch: 2.8105664253234863
Step: 11820, train/loss: 0.02710000053048134
Step: 11820, train/grad_norm: 2.3577702449983917e-05
Step: 11820, train/learning_rate: 3.593527071643621e-05
Step: 11820, train/epoch: 2.812946319580078
Step: 11830, train/loss: 0.00019999999494757503
Step: 11830, train/grad_norm: 5.910047548240982e-05
Step: 11830, train/learning_rate: 3.592337088775821e-05
Step: 11830, train/epoch: 2.815325975418091
Step: 11840, train/loss: 0.002300000051036477
Step: 11840, train/grad_norm: 0.00012603979848790914
Step: 11840, train/learning_rate: 3.591147105908021e-05
Step: 11840, train/epoch: 2.8177058696746826
Step: 11850, train/loss: 0.00559999980032444
Step: 11850, train/grad_norm: 0.006614745128899813
Step: 11850, train/learning_rate: 3.5899571230402216e-05
Step: 11850, train/epoch: 2.8200857639312744
Step: 11860, train/loss: 0.0
Step: 11860, train/grad_norm: 7.721258725723601e-07
Step: 11860, train/learning_rate: 3.588767140172422e-05
Step: 11860, train/epoch: 2.822465419769287
Step: 11870, train/loss: 0.014299999922513962
Step: 11870, train/grad_norm: 2.229427991551347e-06
Step: 11870, train/learning_rate: 3.587577521102503e-05
Step: 11870, train/epoch: 2.824845314025879
Step: 11880, train/loss: 0.0
Step: 11880, train/grad_norm: 3.1460625905310735e-05
Step: 11880, train/learning_rate: 3.586387538234703e-05
Step: 11880, train/epoch: 2.8272252082824707
Step: 11890, train/loss: 0.0
Step: 11890, train/grad_norm: 3.6881747433881173e-08
Step: 11890, train/learning_rate: 3.5851975553669035e-05
Step: 11890, train/epoch: 2.8296048641204834
Step: 11900, train/loss: 0.0
Step: 11900, train/grad_norm: 2.425903767289128e-05
Step: 11900, train/learning_rate: 3.584007572499104e-05
Step: 11900, train/epoch: 2.831984758377075
Step: 11910, train/loss: 0.0
Step: 11910, train/grad_norm: 2.758171467576176e-05
Step: 11910, train/learning_rate: 3.582817589631304e-05
Step: 11910, train/epoch: 2.834364652633667
Step: 11920, train/loss: 0.0
Step: 11920, train/grad_norm: 5.1122301556461025e-06
Step: 11920, train/learning_rate: 3.581627970561385e-05
Step: 11920, train/epoch: 2.8367443084716797
Step: 11930, train/loss: 0.0
Step: 11930, train/grad_norm: 1.1176102816534694e-05
Step: 11930, train/learning_rate: 3.5804379876935855e-05
Step: 11930, train/epoch: 2.8391242027282715
Step: 11940, train/loss: 0.0
Step: 11940, train/grad_norm: 5.879472837477806e-07
Step: 11940, train/learning_rate: 3.579248004825786e-05
Step: 11940, train/epoch: 2.8415040969848633
Step: 11950, train/loss: 0.0
Step: 11950, train/grad_norm: 1.4653720427304506e-05
Step: 11950, train/learning_rate: 3.578058021957986e-05
Step: 11950, train/epoch: 2.843883752822876
Step: 11960, train/loss: 0.0
Step: 11960, train/grad_norm: 5.634595368064765e-07
Step: 11960, train/learning_rate: 3.5768680390901864e-05
Step: 11960, train/epoch: 2.8462636470794678
Step: 11970, train/loss: 0.0
Step: 11970, train/grad_norm: 2.7193302685191156e-06
Step: 11970, train/learning_rate: 3.5756784200202674e-05
Step: 11970, train/epoch: 2.8486435413360596
Step: 11980, train/loss: 0.0
Step: 11980, train/grad_norm: 4.67831114292494e-07
Step: 11980, train/learning_rate: 3.574488437152468e-05
Step: 11980, train/epoch: 2.8510234355926514
Step: 11990, train/loss: 9.999999747378752e-05
Step: 11990, train/grad_norm: 7.05843149262364e-07
Step: 11990, train/learning_rate: 3.573298454284668e-05
Step: 11990, train/epoch: 2.853403091430664
Step: 12000, train/loss: 0.0
Step: 12000, train/grad_norm: 1.2895549843960907e-05
Step: 12000, train/learning_rate: 3.572108471416868e-05
Step: 12000, train/epoch: 2.855782985687256
Step: 12010, train/loss: 0.0
Step: 12010, train/grad_norm: 0.0001070394937414676
Step: 12010, train/learning_rate: 3.5709184885490686e-05
Step: 12010, train/epoch: 2.8581628799438477
Step: 12020, train/loss: 0.1679999977350235
Step: 12020, train/grad_norm: 1.0085218491440173e-06
Step: 12020, train/learning_rate: 3.5697288694791496e-05
Step: 12020, train/epoch: 2.8605425357818604
Step: 12030, train/loss: 0.0
Step: 12030, train/grad_norm: 3.6120397908234736e-06
Step: 12030, train/learning_rate: 3.56853888661135e-05
Step: 12030, train/epoch: 2.862922430038452
Step: 12040, train/loss: 0.0
Step: 12040, train/grad_norm: 4.101951708435081e-05
Step: 12040, train/learning_rate: 3.56734890374355e-05
Step: 12040, train/epoch: 2.865302324295044
Step: 12050, train/loss: 0.0
Step: 12050, train/grad_norm: 2.1460158677655272e-05
Step: 12050, train/learning_rate: 3.5661589208757505e-05
Step: 12050, train/epoch: 2.8676819801330566
Step: 12060, train/loss: 0.0
Step: 12060, train/grad_norm: 8.720741107026697e-07
Step: 12060, train/learning_rate: 3.564968938007951e-05
Step: 12060, train/epoch: 2.8700618743896484
Step: 12070, train/loss: 0.0
Step: 12070, train/grad_norm: 5.355226676329039e-06
Step: 12070, train/learning_rate: 3.563779318938032e-05
Step: 12070, train/epoch: 2.8724417686462402
Step: 12080, train/loss: 0.17579999566078186
Step: 12080, train/grad_norm: 0.0010776378912851214
Step: 12080, train/learning_rate: 3.562589336070232e-05
Step: 12080, train/epoch: 2.874821424484253
Step: 12090, train/loss: 0.0005000000237487257
Step: 12090, train/grad_norm: 0.006139740347862244
Step: 12090, train/learning_rate: 3.5613993532024324e-05
Step: 12090, train/epoch: 2.8772013187408447
Step: 12100, train/loss: 0.0005000000237487257
Step: 12100, train/grad_norm: 7.890168190002441
Step: 12100, train/learning_rate: 3.560209370334633e-05
Step: 12100, train/epoch: 2.8795812129974365
Step: 12110, train/loss: 0.0
Step: 12110, train/grad_norm: 0.00024056517577264458
Step: 12110, train/learning_rate: 3.559019387466833e-05
Step: 12110, train/epoch: 2.881960868835449
Step: 12120, train/loss: 0.0
Step: 12120, train/grad_norm: 0.0017760255141183734
Step: 12120, train/learning_rate: 3.557829768396914e-05
Step: 12120, train/epoch: 2.884340763092041
Step: 12130, train/loss: 0.0
Step: 12130, train/grad_norm: 0.00657940749078989
Step: 12130, train/learning_rate: 3.556639785529114e-05
Step: 12130, train/epoch: 2.886720657348633
Step: 12140, train/loss: 0.2312999963760376
Step: 12140, train/grad_norm: 0.00015933101531118155
Step: 12140, train/learning_rate: 3.5554498026613146e-05
Step: 12140, train/epoch: 2.8891003131866455
Step: 12150, train/loss: 0.0
Step: 12150, train/grad_norm: 0.04175253212451935
Step: 12150, train/learning_rate: 3.554259819793515e-05
Step: 12150, train/epoch: 2.8914802074432373
Step: 12160, train/loss: 0.0
Step: 12160, train/grad_norm: 0.00011386202095309272
Step: 12160, train/learning_rate: 3.553069836925715e-05
Step: 12160, train/epoch: 2.893860101699829
Step: 12170, train/loss: 9.999999747378752e-05
Step: 12170, train/grad_norm: 0.0001314445398747921
Step: 12170, train/learning_rate: 3.551880217855796e-05
Step: 12170, train/epoch: 2.896239995956421
Step: 12180, train/loss: 9.999999747378752e-05
Step: 12180, train/grad_norm: 0.0016608803998678923
Step: 12180, train/learning_rate: 3.5506902349879965e-05
Step: 12180, train/epoch: 2.8986196517944336
Step: 12190, train/loss: 0.0
Step: 12190, train/grad_norm: 0.010407098568975925
Step: 12190, train/learning_rate: 3.549500252120197e-05
Step: 12190, train/epoch: 2.9009995460510254
Step: 12200, train/loss: 0.0
Step: 12200, train/grad_norm: 0.0008184625185094774
Step: 12200, train/learning_rate: 3.548310269252397e-05
Step: 12200, train/epoch: 2.903379440307617
Step: 12210, train/loss: 0.008700000122189522
Step: 12210, train/grad_norm: 0.000605411478318274
Step: 12210, train/learning_rate: 3.5471202863845974e-05
Step: 12210, train/epoch: 2.90575909614563
Step: 12220, train/loss: 0.13940000534057617
Step: 12220, train/grad_norm: 0.0001712085650069639
Step: 12220, train/learning_rate: 3.5459306673146784e-05
Step: 12220, train/epoch: 2.9081389904022217
Step: 12230, train/loss: 0.002300000051036477
Step: 12230, train/grad_norm: 37.80760192871094
Step: 12230, train/learning_rate: 3.544740684446879e-05
Step: 12230, train/epoch: 2.9105188846588135
Step: 12240, train/loss: 0.00019999999494757503
Step: 12240, train/grad_norm: 0.004303756635636091
Step: 12240, train/learning_rate: 3.543550701579079e-05
Step: 12240, train/epoch: 2.912898540496826
Step: 12250, train/loss: 0.0
Step: 12250, train/grad_norm: 0.0001077453198377043
Step: 12250, train/learning_rate: 3.542360718711279e-05
Step: 12250, train/epoch: 2.915278434753418
Step: 12260, train/loss: 0.0
Step: 12260, train/grad_norm: 0.0011049738386645913
Step: 12260, train/learning_rate: 3.5411707358434796e-05
Step: 12260, train/epoch: 2.9176583290100098
Step: 12270, train/loss: 0.0
Step: 12270, train/grad_norm: 9.61054229264846e-06
Step: 12270, train/learning_rate: 3.5399811167735606e-05
Step: 12270, train/epoch: 2.9200379848480225
Step: 12280, train/loss: 0.0
Step: 12280, train/grad_norm: 0.09259514510631561
Step: 12280, train/learning_rate: 3.538791133905761e-05
Step: 12280, train/epoch: 2.9224178791046143
Step: 12290, train/loss: 0.0
Step: 12290, train/grad_norm: 1.580719867888547e-06
Step: 12290, train/learning_rate: 3.537601151037961e-05
Step: 12290, train/epoch: 2.924797773361206
Step: 12300, train/loss: 0.0
Step: 12300, train/grad_norm: 4.2463136196602136e-05
Step: 12300, train/learning_rate: 3.5364111681701615e-05
Step: 12300, train/epoch: 2.9271774291992188
Step: 12310, train/loss: 0.0
Step: 12310, train/grad_norm: 3.340748207847355e-06
Step: 12310, train/learning_rate: 3.535221185302362e-05
Step: 12310, train/epoch: 2.9295573234558105
Step: 12320, train/loss: 0.0
Step: 12320, train/grad_norm: 0.18555904924869537
Step: 12320, train/learning_rate: 3.534031566232443e-05
Step: 12320, train/epoch: 2.9319372177124023
Step: 12330, train/loss: 0.0
Step: 12330, train/grad_norm: 7.232386997202411e-05
Step: 12330, train/learning_rate: 3.532841583364643e-05
Step: 12330, train/epoch: 2.934316873550415
Step: 12340, train/loss: 0.0
Step: 12340, train/grad_norm: 3.5540072076400975e-06
Step: 12340, train/learning_rate: 3.5316516004968435e-05
Step: 12340, train/epoch: 2.936696767807007
Step: 12350, train/loss: 0.0
Step: 12350, train/grad_norm: 8.726732630748302e-06
Step: 12350, train/learning_rate: 3.530461617629044e-05
Step: 12350, train/epoch: 2.9390766620635986
Step: 12360, train/loss: 0.0
Step: 12360, train/grad_norm: 2.1010556849887507e-07
Step: 12360, train/learning_rate: 3.529271634761244e-05
Step: 12360, train/epoch: 2.9414565563201904
Step: 12370, train/loss: 0.0
Step: 12370, train/grad_norm: 2.7407568836679275e-07
Step: 12370, train/learning_rate: 3.528082015691325e-05
Step: 12370, train/epoch: 2.943836212158203
Step: 12380, train/loss: 0.0
Step: 12380, train/grad_norm: 0.00022875268768984824
Step: 12380, train/learning_rate: 3.5268920328235254e-05
Step: 12380, train/epoch: 2.946216106414795
Step: 12390, train/loss: 0.0
Step: 12390, train/grad_norm: 3.8736452552257106e-06
Step: 12390, train/learning_rate: 3.525702049955726e-05
Step: 12390, train/epoch: 2.9485960006713867
Step: 12400, train/loss: 0.0
Step: 12400, train/grad_norm: 1.1068590538343415e-05
Step: 12400, train/learning_rate: 3.524512067087926e-05
Step: 12400, train/epoch: 2.9509756565093994
Step: 12410, train/loss: 0.0
Step: 12410, train/grad_norm: 0.0009888357017189264
Step: 12410, train/learning_rate: 3.523322084220126e-05
Step: 12410, train/epoch: 2.953355550765991
Step: 12420, train/loss: 0.00019999999494757503
Step: 12420, train/grad_norm: 6.323685646057129
Step: 12420, train/learning_rate: 3.522132465150207e-05
Step: 12420, train/epoch: 2.955735445022583
Step: 12430, train/loss: 0.0
Step: 12430, train/grad_norm: 0.002233276842162013
Step: 12430, train/learning_rate: 3.5209424822824076e-05
Step: 12430, train/epoch: 2.9581151008605957
Step: 12440, train/loss: 0.025599999353289604
Step: 12440, train/grad_norm: 4.563168204185786e-06
Step: 12440, train/learning_rate: 3.519752499414608e-05
Step: 12440, train/epoch: 2.9604949951171875
Step: 12450, train/loss: 0.0
Step: 12450, train/grad_norm: 8.479050302412361e-05
Step: 12450, train/learning_rate: 3.518562516546808e-05
Step: 12450, train/epoch: 2.9628748893737793
Step: 12460, train/loss: 0.32339999079704285
Step: 12460, train/grad_norm: 1.364924628433073e-05
Step: 12460, train/learning_rate: 3.5173725336790085e-05
Step: 12460, train/epoch: 2.965254545211792
Step: 12470, train/loss: 0.0
Step: 12470, train/grad_norm: 9.286072599934414e-05
Step: 12470, train/learning_rate: 3.5161829146090895e-05
Step: 12470, train/epoch: 2.967634439468384
Step: 12480, train/loss: 0.0
Step: 12480, train/grad_norm: 3.842559635813814e-06
Step: 12480, train/learning_rate: 3.51499293174129e-05
Step: 12480, train/epoch: 2.9700143337249756
Step: 12490, train/loss: 0.0
Step: 12490, train/grad_norm: 0.0004770785744767636
Step: 12490, train/learning_rate: 3.51380294887349e-05
Step: 12490, train/epoch: 2.9723939895629883
Step: 12500, train/loss: 0.03359999880194664
Step: 12500, train/grad_norm: 1.0016054147854447e-05
Step: 12500, train/learning_rate: 3.5126129660056904e-05
Step: 12500, train/epoch: 2.97477388381958
Step: 12510, train/loss: 0.0
Step: 12510, train/grad_norm: 3.819298217422329e-05
Step: 12510, train/learning_rate: 3.511422983137891e-05
Step: 12510, train/epoch: 2.977153778076172
Step: 12520, train/loss: 0.11089999973773956
Step: 12520, train/grad_norm: 0.0004066282417625189
Step: 12520, train/learning_rate: 3.510233364067972e-05
Step: 12520, train/epoch: 2.9795336723327637
Step: 12530, train/loss: 0.0
Step: 12530, train/grad_norm: 0.00015009113121777773
Step: 12530, train/learning_rate: 3.509043381200172e-05
Step: 12530, train/epoch: 2.9819133281707764
Step: 12540, train/loss: 0.032999999821186066
Step: 12540, train/grad_norm: 0.0007008595275692642
Step: 12540, train/learning_rate: 3.507853398332372e-05
Step: 12540, train/epoch: 2.984293222427368
Step: 12550, train/loss: 0.0
Step: 12550, train/grad_norm: 0.0002924679429270327
Step: 12550, train/learning_rate: 3.5066634154645726e-05
Step: 12550, train/epoch: 2.98667311668396
Step: 12560, train/loss: 0.06949999928474426
Step: 12560, train/grad_norm: 0.0017390650464221835
Step: 12560, train/learning_rate: 3.505473432596773e-05
Step: 12560, train/epoch: 2.9890527725219727
Step: 12570, train/loss: 0.00019999999494757503
Step: 12570, train/grad_norm: 0.0042710755951702595
Step: 12570, train/learning_rate: 3.504283813526854e-05
Step: 12570, train/epoch: 2.9914326667785645
Step: 12580, train/loss: 0.0
Step: 12580, train/grad_norm: 0.00025706214364618063
Step: 12580, train/learning_rate: 3.503093830659054e-05
Step: 12580, train/epoch: 2.9938125610351562
Step: 12590, train/loss: 0.0625
Step: 12590, train/grad_norm: 0.00086189154535532
Step: 12590, train/learning_rate: 3.5019038477912545e-05
Step: 12590, train/epoch: 2.996192216873169
Step: 12600, train/loss: 0.0
Step: 12600, train/grad_norm: 0.000561007356736809
Step: 12600, train/learning_rate: 3.500713864923455e-05
Step: 12600, train/epoch: 2.9985721111297607
Step: 12606, eval/loss: 0.048641636967659
Step: 12606, eval/accuracy: 0.9937525987625122
Step: 12606, eval/f1: 0.9934030771255493
Step: 12606, eval/runtime: 734.7880249023438
Step: 12606, eval/samples_per_second: 9.803000450134277
Step: 12606, eval/steps_per_second: 1.2259999513626099
Step: 12606, train/epoch: 3.0
Step: 12610, train/loss: 0.0
Step: 12610, train/grad_norm: 0.001895821769721806
Step: 12610, train/learning_rate: 3.499523882055655e-05
Step: 12610, train/epoch: 3.0009520053863525
Step: 12620, train/loss: 0.0
Step: 12620, train/grad_norm: 0.002114142058417201
Step: 12620, train/learning_rate: 3.498334262985736e-05
Step: 12620, train/epoch: 3.0033316612243652
Step: 12630, train/loss: 0.0
Step: 12630, train/grad_norm: 0.0002929540059994906
Step: 12630, train/learning_rate: 3.4971442801179364e-05
Step: 12630, train/epoch: 3.005711555480957
Step: 12640, train/loss: 0.0
Step: 12640, train/grad_norm: 0.001356509979814291
Step: 12640, train/learning_rate: 3.495954297250137e-05
Step: 12640, train/epoch: 3.008091449737549
Step: 12650, train/loss: 9.999999747378752e-05
Step: 12650, train/grad_norm: 2.1723797090089647e-06
Step: 12650, train/learning_rate: 3.494764314382337e-05
Step: 12650, train/epoch: 3.0104711055755615
Step: 12660, train/loss: 0.0
Step: 12660, train/grad_norm: 4.9266687710769475e-05
Step: 12660, train/learning_rate: 3.493574331514537e-05
Step: 12660, train/epoch: 3.0128509998321533
Step: 12670, train/loss: 9.999999747378752e-05
Step: 12670, train/grad_norm: 0.00016243087884504348
Step: 12670, train/learning_rate: 3.4923847124446183e-05
Step: 12670, train/epoch: 3.015230894088745
Step: 12680, train/loss: 0.0
Step: 12680, train/grad_norm: 5.023082485422492e-05
Step: 12680, train/learning_rate: 3.4911947295768186e-05
Step: 12680, train/epoch: 3.017610549926758
Step: 12690, train/loss: 9.999999747378752e-05
Step: 12690, train/grad_norm: 1.812223854358308e-05
Step: 12690, train/learning_rate: 3.490004746709019e-05
Step: 12690, train/epoch: 3.0199904441833496
Step: 12700, train/loss: 0.0
Step: 12700, train/grad_norm: 1.9715515009011142e-05
Step: 12700, train/learning_rate: 3.488814763841219e-05
Step: 12700, train/epoch: 3.0223703384399414
Step: 12710, train/loss: 0.0
Step: 12710, train/grad_norm: 6.490455234597903e-06
Step: 12710, train/learning_rate: 3.4876247809734195e-05
Step: 12710, train/epoch: 3.024750232696533
Step: 12720, train/loss: 0.0
Step: 12720, train/grad_norm: 4.4367418183810514e-08
Step: 12720, train/learning_rate: 3.4864351619035006e-05
Step: 12720, train/epoch: 3.027129888534546
Step: 12730, train/loss: 0.0
Step: 12730, train/grad_norm: 7.951725820021238e-06
Step: 12730, train/learning_rate: 3.485245179035701e-05
Step: 12730, train/epoch: 3.0295097827911377
Step: 12740, train/loss: 0.0
Step: 12740, train/grad_norm: 1.1394758985261433e-06
Step: 12740, train/learning_rate: 3.484055196167901e-05
Step: 12740, train/epoch: 3.0318896770477295
Step: 12750, train/loss: 0.09730000048875809
Step: 12750, train/grad_norm: 5.514973963727243e-06
Step: 12750, train/learning_rate: 3.4828652133001015e-05
Step: 12750, train/epoch: 3.034269332885742
Step: 12760, train/loss: 0.0
Step: 12760, train/grad_norm: 4.240730959281791e-06
Step: 12760, train/learning_rate: 3.481675230432302e-05
Step: 12760, train/epoch: 3.036649227142334
Step: 12770, train/loss: 0.0
Step: 12770, train/grad_norm: 8.394205330830573e-10
Step: 12770, train/learning_rate: 3.480485611362383e-05
Step: 12770, train/epoch: 3.039029121398926
Step: 12780, train/loss: 0.0
Step: 12780, train/grad_norm: 0.00021313012985046953
Step: 12780, train/learning_rate: 3.479295628494583e-05
Step: 12780, train/epoch: 3.0414087772369385
Step: 12790, train/loss: 0.05979999899864197
Step: 12790, train/grad_norm: 7.888583786552772e-05
Step: 12790, train/learning_rate: 3.4781056456267834e-05
Step: 12790, train/epoch: 3.0437886714935303
Step: 12800, train/loss: 0.0
Step: 12800, train/grad_norm: 7.594618409711984e-07
Step: 12800, train/learning_rate: 3.476915662758984e-05
Step: 12800, train/epoch: 3.046168565750122
Step: 12810, train/loss: 0.0
Step: 12810, train/grad_norm: 2.189198866631159e-08
Step: 12810, train/learning_rate: 3.475725679891184e-05
Step: 12810, train/epoch: 3.0485482215881348
Step: 12820, train/loss: 0.1753000020980835
Step: 12820, train/grad_norm: 3.3415958569094073e-06
Step: 12820, train/learning_rate: 3.474536060821265e-05
Step: 12820, train/epoch: 3.0509281158447266
Step: 12830, train/loss: 0.009800000116229057
Step: 12830, train/grad_norm: 1.5160069779085461e-05
Step: 12830, train/learning_rate: 3.473346077953465e-05
Step: 12830, train/epoch: 3.0533080101013184
Step: 12840, train/loss: 0.0
Step: 12840, train/grad_norm: 7.28832162621984e-07
Step: 12840, train/learning_rate: 3.4721560950856656e-05
Step: 12840, train/epoch: 3.055687665939331
Step: 12850, train/loss: 0.0
Step: 12850, train/grad_norm: 0.05783652141690254
Step: 12850, train/learning_rate: 3.470966112217866e-05
Step: 12850, train/epoch: 3.058067560195923
Step: 12860, train/loss: 0.0
Step: 12860, train/grad_norm: 7.430771802319214e-07
Step: 12860, train/learning_rate: 3.469776129350066e-05
Step: 12860, train/epoch: 3.0604474544525146
Step: 12870, train/loss: 0.0
Step: 12870, train/grad_norm: 4.3207674025325105e-06
Step: 12870, train/learning_rate: 3.468586510280147e-05
Step: 12870, train/epoch: 3.0628271102905273
Step: 12880, train/loss: 0.06679999828338623
Step: 12880, train/grad_norm: 4.333393462729873e-06
Step: 12880, train/learning_rate: 3.4673965274123475e-05
Step: 12880, train/epoch: 3.065207004547119
Step: 12890, train/loss: 0.0
Step: 12890, train/grad_norm: 6.02143899186558e-08
Step: 12890, train/learning_rate: 3.466206544544548e-05
Step: 12890, train/epoch: 3.067586898803711
Step: 12900, train/loss: 0.0
Step: 12900, train/grad_norm: 2.257579154729683e-07
Step: 12900, train/learning_rate: 3.465016561676748e-05
Step: 12900, train/epoch: 3.0699667930603027
Step: 12910, train/loss: 0.0
Step: 12910, train/grad_norm: 8.578413002169327e-08
Step: 12910, train/learning_rate: 3.4638265788089484e-05
Step: 12910, train/epoch: 3.0723464488983154
Step: 12920, train/loss: 0.0
Step: 12920, train/grad_norm: 3.982822249781748e-08
Step: 12920, train/learning_rate: 3.4626369597390294e-05
Step: 12920, train/epoch: 3.0747263431549072
Step: 12930, train/loss: 0.0
Step: 12930, train/grad_norm: 2.977859310249187e-07
Step: 12930, train/learning_rate: 3.46144697687123e-05
Step: 12930, train/epoch: 3.077106237411499
Step: 12940, train/loss: 9.999999747378752e-05
Step: 12940, train/grad_norm: 9.843901125350385e-07
Step: 12940, train/learning_rate: 3.46025699400343e-05
Step: 12940, train/epoch: 3.0794858932495117
Step: 12950, train/loss: 0.06759999692440033
Step: 12950, train/grad_norm: 4.980950507160742e-06
Step: 12950, train/learning_rate: 3.45906701113563e-05
Step: 12950, train/epoch: 3.0818657875061035
Step: 12960, train/loss: 0.0
Step: 12960, train/grad_norm: 0.029921256005764008
Step: 12960, train/learning_rate: 3.4578770282678306e-05
Step: 12960, train/epoch: 3.0842456817626953
Step: 12970, train/loss: 0.0
Step: 12970, train/grad_norm: 1.1235853207836044e-06
Step: 12970, train/learning_rate: 3.4566874091979116e-05
Step: 12970, train/epoch: 3.086625337600708
Step: 12980, train/loss: 0.0
Step: 12980, train/grad_norm: 0.0019990187138319016
Step: 12980, train/learning_rate: 3.455497426330112e-05
Step: 12980, train/epoch: 3.0890052318573
Step: 12990, train/loss: 0.0
Step: 12990, train/grad_norm: 6.483482906105564e-08
Step: 12990, train/learning_rate: 3.454307443462312e-05
Step: 12990, train/epoch: 3.0913851261138916
Step: 13000, train/loss: 0.0
Step: 13000, train/grad_norm: 2.3126906967263494e-07
Step: 13000, train/learning_rate: 3.4531174605945125e-05
Step: 13000, train/epoch: 3.0937647819519043
Step: 13010, train/loss: 0.0
Step: 13010, train/grad_norm: 8.587629736211966e-07
Step: 13010, train/learning_rate: 3.451927477726713e-05
Step: 13010, train/epoch: 3.096144676208496
Step: 13020, train/loss: 0.09839999675750732
Step: 13020, train/grad_norm: 0.00010200657561654225
Step: 13020, train/learning_rate: 3.450737858656794e-05
Step: 13020, train/epoch: 3.098524570465088
Step: 13030, train/loss: 0.0
Step: 13030, train/grad_norm: 0.010626493953168392
Step: 13030, train/learning_rate: 3.449547875788994e-05
Step: 13030, train/epoch: 3.1009042263031006
Step: 13040, train/loss: 0.014700000174343586
Step: 13040, train/grad_norm: 236.3371124267578
Step: 13040, train/learning_rate: 3.4483578929211944e-05
Step: 13040, train/epoch: 3.1032841205596924
Step: 13050, train/loss: 0.0
Step: 13050, train/grad_norm: 0.0007662936113774776
Step: 13050, train/learning_rate: 3.447167910053395e-05
Step: 13050, train/epoch: 3.105664014816284
Step: 13060, train/loss: 0.0
Step: 13060, train/grad_norm: 1.4761292277398752e-07
Step: 13060, train/learning_rate: 3.445977927185595e-05
Step: 13060, train/epoch: 3.108043670654297
Step: 13070, train/loss: 0.0
Step: 13070, train/grad_norm: 1.3710949531287042e-07
Step: 13070, train/learning_rate: 3.444788308115676e-05
Step: 13070, train/epoch: 3.1104235649108887
Step: 13080, train/loss: 0.12620000541210175
Step: 13080, train/grad_norm: 6.582394689758075e-06
Step: 13080, train/learning_rate: 3.4435983252478763e-05
Step: 13080, train/epoch: 3.1128034591674805
Step: 13090, train/loss: 0.0
Step: 13090, train/grad_norm: 4.801311206392711e-06
Step: 13090, train/learning_rate: 3.4424083423800766e-05
Step: 13090, train/epoch: 3.1151833534240723
Step: 13100, train/loss: 0.0
Step: 13100, train/grad_norm: 0.002776277018710971
Step: 13100, train/learning_rate: 3.441218359512277e-05
Step: 13100, train/epoch: 3.117563009262085
Step: 13110, train/loss: 0.16169999539852142
Step: 13110, train/grad_norm: 8.932373020797968e-06
Step: 13110, train/learning_rate: 3.440028376644477e-05
Step: 13110, train/epoch: 3.1199429035186768
Step: 13120, train/loss: 0.0
Step: 13120, train/grad_norm: 0.00020730795222334564
Step: 13120, train/learning_rate: 3.438838757574558e-05
Step: 13120, train/epoch: 3.1223227977752686
Step: 13130, train/loss: 0.0
Step: 13130, train/grad_norm: 0.0004454069712664932
Step: 13130, train/learning_rate: 3.4376487747067586e-05
Step: 13130, train/epoch: 3.1247024536132812
Step: 13140, train/loss: 0.0
Step: 13140, train/grad_norm: 1.2742871149384882e-05
Step: 13140, train/learning_rate: 3.436458791838959e-05
Step: 13140, train/epoch: 3.127082347869873
Step: 13150, train/loss: 0.0005000000237487257
Step: 13150, train/grad_norm: 2.3261911337613128e-05
Step: 13150, train/learning_rate: 3.435268808971159e-05
Step: 13150, train/epoch: 3.129462242126465
Step: 13160, train/loss: 0.0
Step: 13160, train/grad_norm: 0.008809246122837067
Step: 13160, train/learning_rate: 3.43407918990124e-05
Step: 13160, train/epoch: 3.1318418979644775
Step: 13170, train/loss: 0.0
Step: 13170, train/grad_norm: 5.5094174058467615e-06
Step: 13170, train/learning_rate: 3.4328892070334405e-05
Step: 13170, train/epoch: 3.1342217922210693
Step: 13180, train/loss: 0.0006000000284984708
Step: 13180, train/grad_norm: 5.076197794551263e-06
Step: 13180, train/learning_rate: 3.431699224165641e-05
Step: 13180, train/epoch: 3.136601686477661
Step: 13190, train/loss: 0.0
Step: 13190, train/grad_norm: 2.360226062592119e-05
Step: 13190, train/learning_rate: 3.430509241297841e-05
Step: 13190, train/epoch: 3.138981342315674
Step: 13200, train/loss: 0.0
Step: 13200, train/grad_norm: 0.00023048229923006147
Step: 13200, train/learning_rate: 3.4293192584300414e-05
Step: 13200, train/epoch: 3.1413612365722656
Step: 13210, train/loss: 0.09799999743700027
Step: 13210, train/grad_norm: 6.0304810176603496e-05
Step: 13210, train/learning_rate: 3.4281296393601224e-05
Step: 13210, train/epoch: 3.1437411308288574
Step: 13220, train/loss: 0.0
Step: 13220, train/grad_norm: 8.147558219206985e-06
Step: 13220, train/learning_rate: 3.426939656492323e-05
Step: 13220, train/epoch: 3.14612078666687
Step: 13230, train/loss: 0.0
Step: 13230, train/grad_norm: 0.00012703654647339135
Step: 13230, train/learning_rate: 3.425749673624523e-05
Step: 13230, train/epoch: 3.148500680923462
Step: 13240, train/loss: 0.0
Step: 13240, train/grad_norm: 0.0002304910885868594
Step: 13240, train/learning_rate: 3.424559690756723e-05
Step: 13240, train/epoch: 3.1508805751800537
Step: 13250, train/loss: 0.00839999970048666
Step: 13250, train/grad_norm: 100.74597930908203
Step: 13250, train/learning_rate: 3.4233697078889236e-05
Step: 13250, train/epoch: 3.1532604694366455
Step: 13260, train/loss: 0.14630000293254852
Step: 13260, train/grad_norm: 39.28628921508789
Step: 13260, train/learning_rate: 3.4221800888190046e-05
Step: 13260, train/epoch: 3.155640125274658
Step: 13270, train/loss: 0.0215000007301569
Step: 13270, train/grad_norm: 0.01774146594107151
Step: 13270, train/learning_rate: 3.420990105951205e-05
Step: 13270, train/epoch: 3.15802001953125
Step: 13280, train/loss: 0.0024999999441206455
Step: 13280, train/grad_norm: 5.4322215873980895e-05
Step: 13280, train/learning_rate: 3.419800123083405e-05
Step: 13280, train/epoch: 3.160399913787842
Step: 13290, train/loss: 0.0
Step: 13290, train/grad_norm: 2.0344910808489658e-05
Step: 13290, train/learning_rate: 3.4186101402156055e-05
Step: 13290, train/epoch: 3.1627795696258545
Step: 13300, train/loss: 0.0
Step: 13300, train/grad_norm: 0.00016338196292053908
Step: 13300, train/learning_rate: 3.417420157347806e-05
Step: 13300, train/epoch: 3.1651594638824463
Step: 13310, train/loss: 0.0005000000237487257
Step: 13310, train/grad_norm: 19.296405792236328
Step: 13310, train/learning_rate: 3.416230538277887e-05
Step: 13310, train/epoch: 3.167539358139038
Step: 13320, train/loss: 0.0
Step: 13320, train/grad_norm: 1.8417132707782002e-07
Step: 13320, train/learning_rate: 3.415040555410087e-05
Step: 13320, train/epoch: 3.169919013977051
Step: 13330, train/loss: 0.0
Step: 13330, train/grad_norm: 5.166031602499288e-09
Step: 13330, train/learning_rate: 3.4138505725422874e-05
Step: 13330, train/epoch: 3.1722989082336426
Step: 13340, train/loss: 0.0
Step: 13340, train/grad_norm: 9.05911790027858e-09
Step: 13340, train/learning_rate: 3.412660589674488e-05
Step: 13340, train/epoch: 3.1746788024902344
Step: 13350, train/loss: 0.0
Step: 13350, train/grad_norm: 8.585142552419711e-08
Step: 13350, train/learning_rate: 3.411470606806688e-05
Step: 13350, train/epoch: 3.177058458328247
Step: 13360, train/loss: 0.0
Step: 13360, train/grad_norm: 1.44609338392776e-10
Step: 13360, train/learning_rate: 3.410280987736769e-05
Step: 13360, train/epoch: 3.179438352584839
Step: 13370, train/loss: 0.0
Step: 13370, train/grad_norm: 0.0001025789460982196
Step: 13370, train/learning_rate: 3.409091004868969e-05
Step: 13370, train/epoch: 3.1818182468414307
Step: 13380, train/loss: 0.0
Step: 13380, train/grad_norm: 1.220509489030519e-07
Step: 13380, train/learning_rate: 3.4079010220011696e-05
Step: 13380, train/epoch: 3.1841979026794434
Step: 13390, train/loss: 0.0
Step: 13390, train/grad_norm: 7.659846090746214e-09
Step: 13390, train/learning_rate: 3.40671103913337e-05
Step: 13390, train/epoch: 3.186577796936035
Step: 13400, train/loss: 0.0
Step: 13400, train/grad_norm: 4.550156972982222e-06
Step: 13400, train/learning_rate: 3.40552105626557e-05
Step: 13400, train/epoch: 3.188957691192627
Step: 13410, train/loss: 0.0
Step: 13410, train/grad_norm: 1.3276057336808478e-11
Step: 13410, train/learning_rate: 3.404331437195651e-05
Step: 13410, train/epoch: 3.1913373470306396
Step: 13420, train/loss: 0.0
Step: 13420, train/grad_norm: 1.1446985581642366e-06
Step: 13420, train/learning_rate: 3.4031414543278515e-05
Step: 13420, train/epoch: 3.1937172412872314
Step: 13430, train/loss: 0.07729999721050262
Step: 13430, train/grad_norm: 1.9985185728543797e-10
Step: 13430, train/learning_rate: 3.401951471460052e-05
Step: 13430, train/epoch: 3.1960971355438232
Step: 13440, train/loss: 0.0
Step: 13440, train/grad_norm: 6.50674722435518e-11
Step: 13440, train/learning_rate: 3.400761488592252e-05
Step: 13440, train/epoch: 3.198477029800415
Step: 13450, train/loss: 0.0
Step: 13450, train/grad_norm: 1.2212281035317574e-05
Step: 13450, train/learning_rate: 3.3995715057244524e-05
Step: 13450, train/epoch: 3.2008566856384277
Step: 13460, train/loss: 0.00039999998989515007
Step: 13460, train/grad_norm: 2.0768263553350153e-08
Step: 13460, train/learning_rate: 3.3983818866545334e-05
Step: 13460, train/epoch: 3.2032365798950195
Step: 13470, train/loss: 0.0
Step: 13470, train/grad_norm: 1.397039461892291e-08
Step: 13470, train/learning_rate: 3.397191903786734e-05
Step: 13470, train/epoch: 3.2056164741516113
Step: 13480, train/loss: 0.0
Step: 13480, train/grad_norm: 5.121853519085562e-07
Step: 13480, train/learning_rate: 3.396001920918934e-05
Step: 13480, train/epoch: 3.207996129989624
Step: 13490, train/loss: 0.0
Step: 13490, train/grad_norm: 4.1785286157391965e-05
Step: 13490, train/learning_rate: 3.3948119380511343e-05
Step: 13490, train/epoch: 3.210376024246216
Step: 13500, train/loss: 0.0
Step: 13500, train/grad_norm: 6.344010206049688e-09
Step: 13500, train/learning_rate: 3.3936219551833346e-05
Step: 13500, train/epoch: 3.2127559185028076
Step: 13510, train/loss: 0.0
Step: 13510, train/grad_norm: 1.0899933045038779e-07
Step: 13510, train/learning_rate: 3.3924323361134157e-05
Step: 13510, train/epoch: 3.2151355743408203
Step: 13520, train/loss: 0.1941000074148178
Step: 13520, train/grad_norm: 165.91641235351562
Step: 13520, train/learning_rate: 3.391242353245616e-05
Step: 13520, train/epoch: 3.217515468597412
Step: 13530, train/loss: 0.0
Step: 13530, train/grad_norm: 1.1355936294421554e-05
Step: 13530, train/learning_rate: 3.390052370377816e-05
Step: 13530, train/epoch: 3.219895362854004
Step: 13540, train/loss: 0.023499999195337296
Step: 13540, train/grad_norm: 7.085928643846273e-08
Step: 13540, train/learning_rate: 3.3888623875100166e-05
Step: 13540, train/epoch: 3.2222750186920166
Step: 13550, train/loss: 0.0
Step: 13550, train/grad_norm: 5.167102244740818e-06
Step: 13550, train/learning_rate: 3.387672404642217e-05
Step: 13550, train/epoch: 3.2246549129486084
Step: 13560, train/loss: 0.0
Step: 13560, train/grad_norm: 8.119423569041828e-07
Step: 13560, train/learning_rate: 3.386482785572298e-05
Step: 13560, train/epoch: 3.2270348072052
Step: 13570, train/loss: 0.0
Step: 13570, train/grad_norm: 1.1451774639681389e-07
Step: 13570, train/learning_rate: 3.385292802704498e-05
Step: 13570, train/epoch: 3.229414463043213
Step: 13580, train/loss: 0.0
Step: 13580, train/grad_norm: 1.2166221363685281e-09
Step: 13580, train/learning_rate: 3.3841028198366985e-05
Step: 13580, train/epoch: 3.2317943572998047
Step: 13590, train/loss: 0.0
Step: 13590, train/grad_norm: 1.3196457757658209e-06
Step: 13590, train/learning_rate: 3.382912836968899e-05
Step: 13590, train/epoch: 3.2341742515563965
Step: 13600, train/loss: 0.0
Step: 13600, train/grad_norm: 1.8612723806654685e-07
Step: 13600, train/learning_rate: 3.381722854101099e-05
Step: 13600, train/epoch: 3.236553907394409
Step: 13610, train/loss: 0.0
Step: 13610, train/grad_norm: 1.2375088772387244e-05
Step: 13610, train/learning_rate: 3.38053323503118e-05
Step: 13610, train/epoch: 3.238933801651001
Step: 13620, train/loss: 0.0
Step: 13620, train/grad_norm: 1.3483836269756466e-08
Step: 13620, train/learning_rate: 3.3793432521633804e-05
Step: 13620, train/epoch: 3.2413136959075928
Step: 13630, train/loss: 0.0
Step: 13630, train/grad_norm: 1.2249867609170906e-07
Step: 13630, train/learning_rate: 3.378153269295581e-05
Step: 13630, train/epoch: 3.2436935901641846
Step: 13640, train/loss: 0.0
Step: 13640, train/grad_norm: 1.1657200957415625e-05
Step: 13640, train/learning_rate: 3.376963286427781e-05
Step: 13640, train/epoch: 3.2460732460021973
Step: 13650, train/loss: 0.003599999938160181
Step: 13650, train/grad_norm: 7.184008609328885e-06
Step: 13650, train/learning_rate: 3.375773303559981e-05
Step: 13650, train/epoch: 3.248453140258789
Step: 13660, train/loss: 0.0
Step: 13660, train/grad_norm: 3.766685097161826e-07
Step: 13660, train/learning_rate: 3.374583684490062e-05
Step: 13660, train/epoch: 3.250833034515381
Step: 13670, train/loss: 0.0
Step: 13670, train/grad_norm: 8.83919284433432e-08
Step: 13670, train/learning_rate: 3.3733937016222626e-05
Step: 13670, train/epoch: 3.2532126903533936
Step: 13680, train/loss: 0.0
Step: 13680, train/grad_norm: 4.3073158928486066e-10
Step: 13680, train/learning_rate: 3.372203718754463e-05
Step: 13680, train/epoch: 3.2555925846099854
Step: 13690, train/loss: 9.999999747378752e-05
Step: 13690, train/grad_norm: 2.510750274353768e-09
Step: 13690, train/learning_rate: 3.371013735886663e-05
Step: 13690, train/epoch: 3.257972478866577
Step: 13700, train/loss: 0.0
Step: 13700, train/grad_norm: 7.420991043893821e-10
Step: 13700, train/learning_rate: 3.3698237530188635e-05
Step: 13700, train/epoch: 3.26035213470459
Step: 13710, train/loss: 0.009600000455975533
Step: 13710, train/grad_norm: 5.469563170434899e-12
Step: 13710, train/learning_rate: 3.3686341339489445e-05
Step: 13710, train/epoch: 3.2627320289611816
Step: 13720, train/loss: 0.17419999837875366
Step: 13720, train/grad_norm: 2.2182122847880237e-05
Step: 13720, train/learning_rate: 3.367444151081145e-05
Step: 13720, train/epoch: 3.2651119232177734
Step: 13730, train/loss: 0.0
Step: 13730, train/grad_norm: 5.711311678169295e-05
Step: 13730, train/learning_rate: 3.366254168213345e-05
Step: 13730, train/epoch: 3.267491579055786
Step: 13740, train/loss: 0.2653999924659729
Step: 13740, train/grad_norm: 374.2924499511719
Step: 13740, train/learning_rate: 3.3650641853455454e-05
Step: 13740, train/epoch: 3.269871473312378
Step: 13750, train/loss: 0.08789999783039093
Step: 13750, train/grad_norm: 7.568294222437544e-07
Step: 13750, train/learning_rate: 3.363874202477746e-05
Step: 13750, train/epoch: 3.2722513675689697
Step: 13760, train/loss: 0.0
Step: 13760, train/grad_norm: 2.3310353469696565e-07
Step: 13760, train/learning_rate: 3.362684583407827e-05
Step: 13760, train/epoch: 3.2746310234069824
Step: 13770, train/loss: 0.26989999413490295
Step: 13770, train/grad_norm: 0.015416773967444897
Step: 13770, train/learning_rate: 3.361494600540027e-05
Step: 13770, train/epoch: 3.277010917663574
Step: 13780, train/loss: 0.0013000000035390258
Step: 13780, train/grad_norm: 0.0006533238920383155
Step: 13780, train/learning_rate: 3.360304617672227e-05
Step: 13780, train/epoch: 3.279390811920166
Step: 13790, train/loss: 0.0
Step: 13790, train/grad_norm: 0.0018802284030243754
Step: 13790, train/learning_rate: 3.3591146348044276e-05
Step: 13790, train/epoch: 3.2817704677581787
Step: 13800, train/loss: 0.0
Step: 13800, train/grad_norm: 3.020826989086345e-05
Step: 13800, train/learning_rate: 3.357924651936628e-05
Step: 13800, train/epoch: 3.2841503620147705
Step: 13810, train/loss: 0.0
Step: 13810, train/grad_norm: 0.0011430340819060802
Step: 13810, train/learning_rate: 3.356735032866709e-05
Step: 13810, train/epoch: 3.2865302562713623
Step: 13820, train/loss: 0.0
Step: 13820, train/grad_norm: 1.6847287042764947e-05
Step: 13820, train/learning_rate: 3.355545049998909e-05
Step: 13820, train/epoch: 3.288910150527954
Step: 13830, train/loss: 0.0
Step: 13830, train/grad_norm: 6.4680784817028325e-06
Step: 13830, train/learning_rate: 3.3543550671311095e-05
Step: 13830, train/epoch: 3.291289806365967
Step: 13840, train/loss: 0.0
Step: 13840, train/grad_norm: 1.4867738173052203e-05
Step: 13840, train/learning_rate: 3.35316508426331e-05
Step: 13840, train/epoch: 3.2936697006225586
Step: 13850, train/loss: 0.0
Step: 13850, train/grad_norm: 8.166082261595875e-05
Step: 13850, train/learning_rate: 3.35197510139551e-05
Step: 13850, train/epoch: 3.2960495948791504
Step: 13860, train/loss: 0.0
Step: 13860, train/grad_norm: 0.0004634954093489796
Step: 13860, train/learning_rate: 3.350785482325591e-05
Step: 13860, train/epoch: 3.298429250717163
Step: 13870, train/loss: 0.0
Step: 13870, train/grad_norm: 2.4799561288091354e-05
Step: 13870, train/learning_rate: 3.3495954994577914e-05
Step: 13870, train/epoch: 3.300809144973755
Step: 13880, train/loss: 0.002199999988079071
Step: 13880, train/grad_norm: 3.7054967833682895e-05
Step: 13880, train/learning_rate: 3.348405516589992e-05
Step: 13880, train/epoch: 3.3031890392303467
Step: 13890, train/loss: 0.0
Step: 13890, train/grad_norm: 0.0004435455775819719
Step: 13890, train/learning_rate: 3.347215533722192e-05
Step: 13890, train/epoch: 3.3055686950683594
Step: 13900, train/loss: 0.0
Step: 13900, train/grad_norm: 0.0011753729777410626
Step: 13900, train/learning_rate: 3.3460255508543923e-05
Step: 13900, train/epoch: 3.307948589324951
Step: 13910, train/loss: 0.12809999287128448
Step: 13910, train/grad_norm: 8.858045475790277e-06
Step: 13910, train/learning_rate: 3.3448359317844734e-05
Step: 13910, train/epoch: 3.310328483581543
Step: 13920, train/loss: 0.0006000000284984708
Step: 13920, train/grad_norm: 0.00010041521454695612
Step: 13920, train/learning_rate: 3.3436459489166737e-05
Step: 13920, train/epoch: 3.3127081394195557
Step: 13930, train/loss: 0.0
Step: 13930, train/grad_norm: 4.441811142896768e-06
Step: 13930, train/learning_rate: 3.342455966048874e-05
Step: 13930, train/epoch: 3.3150880336761475
Step: 13940, train/loss: 0.0
Step: 13940, train/grad_norm: 3.756120747766545e-07
Step: 13940, train/learning_rate: 3.341265983181074e-05
Step: 13940, train/epoch: 3.3174679279327393
Step: 13950, train/loss: 0.1445000022649765
Step: 13950, train/grad_norm: 6.261913222260773e-05
Step: 13950, train/learning_rate: 3.3400760003132746e-05
Step: 13950, train/epoch: 3.319847583770752
Step: 13960, train/loss: 0.006399999838322401
Step: 13960, train/grad_norm: 6.309837044682354e-05
Step: 13960, train/learning_rate: 3.3388863812433556e-05
Step: 13960, train/epoch: 3.3222274780273438
Step: 13970, train/loss: 0.0
Step: 13970, train/grad_norm: 0.00021896715043112636
Step: 13970, train/learning_rate: 3.337696398375556e-05
Step: 13970, train/epoch: 3.3246073722839355
Step: 13980, train/loss: 0.007499999832361937
Step: 13980, train/grad_norm: 2.025179855991155e-05
Step: 13980, train/learning_rate: 3.336506415507756e-05
Step: 13980, train/epoch: 3.3269872665405273
Step: 13990, train/loss: 0.0
Step: 13990, train/grad_norm: 0.0003654166648630053
Step: 13990, train/learning_rate: 3.3353164326399565e-05
Step: 13990, train/epoch: 3.32936692237854
Step: 14000, train/loss: 0.0
Step: 14000, train/grad_norm: 0.0001220530248247087
Step: 14000, train/learning_rate: 3.334126449772157e-05
Step: 14000, train/epoch: 3.331746816635132
Step: 14010, train/loss: 0.0
Step: 14010, train/grad_norm: 5.8547307162371e-06
Step: 14010, train/learning_rate: 3.332936830702238e-05
Step: 14010, train/epoch: 3.3341267108917236
Step: 14020, train/loss: 0.10589999705553055
Step: 14020, train/grad_norm: 4.977777052772581e-07
Step: 14020, train/learning_rate: 3.331746847834438e-05
Step: 14020, train/epoch: 3.3365063667297363
Step: 14030, train/loss: 0.0
Step: 14030, train/grad_norm: 8.658034857944585e-07
Step: 14030, train/learning_rate: 3.3305568649666384e-05
Step: 14030, train/epoch: 3.338886260986328
Step: 14040, train/loss: 0.0
Step: 14040, train/grad_norm: 5.333981607691385e-05
Step: 14040, train/learning_rate: 3.329366882098839e-05
Step: 14040, train/epoch: 3.34126615524292
Step: 14050, train/loss: 0.0007999999797903001
Step: 14050, train/grad_norm: 5.948882972006686e-05
Step: 14050, train/learning_rate: 3.328176899231039e-05
Step: 14050, train/epoch: 3.3436458110809326
Step: 14060, train/loss: 0.0
Step: 14060, train/grad_norm: 1.2524658814072609e-05
Step: 14060, train/learning_rate: 3.32698728016112e-05
Step: 14060, train/epoch: 3.3460257053375244
Step: 14070, train/loss: 0.13050000369548798
Step: 14070, train/grad_norm: 3.806785025517456e-05
Step: 14070, train/learning_rate: 3.32579729729332e-05
Step: 14070, train/epoch: 3.348405599594116
Step: 14080, train/loss: 9.999999747378752e-05
Step: 14080, train/grad_norm: 0.0017569276969879866
Step: 14080, train/learning_rate: 3.3246073144255206e-05
Step: 14080, train/epoch: 3.350785255432129
Step: 14090, train/loss: 0.00019999999494757503
Step: 14090, train/grad_norm: 2.2810458176536486e-05
Step: 14090, train/learning_rate: 3.323417331557721e-05
Step: 14090, train/epoch: 3.3531651496887207
Step: 14100, train/loss: 0.09799999743700027
Step: 14100, train/grad_norm: 0.0001250441127922386
Step: 14100, train/learning_rate: 3.322227348689921e-05
Step: 14100, train/epoch: 3.3555450439453125
Step: 14110, train/loss: 0.00019999999494757503
Step: 14110, train/grad_norm: 0.0002213883853983134
Step: 14110, train/learning_rate: 3.321037729620002e-05
Step: 14110, train/epoch: 3.357924699783325
Step: 14120, train/loss: 0.033799998462200165
Step: 14120, train/grad_norm: 0.0019447730155661702
Step: 14120, train/learning_rate: 3.3198477467522025e-05
Step: 14120, train/epoch: 3.360304594039917
Step: 14130, train/loss: 0.25279998779296875
Step: 14130, train/grad_norm: 0.00032333072158508003
Step: 14130, train/learning_rate: 3.318657763884403e-05
Step: 14130, train/epoch: 3.362684488296509
Step: 14140, train/loss: 0.004800000227987766
Step: 14140, train/grad_norm: 1.3107414815749507e-05
Step: 14140, train/learning_rate: 3.317467781016603e-05
Step: 14140, train/epoch: 3.3650641441345215
Step: 14150, train/loss: 0.0
Step: 14150, train/grad_norm: 1.7264561392948963e-05
Step: 14150, train/learning_rate: 3.3162777981488034e-05
Step: 14150, train/epoch: 3.3674440383911133
Step: 14160, train/loss: 0.0015999999595806003
Step: 14160, train/grad_norm: 0.0004241373098921031
Step: 14160, train/learning_rate: 3.3150881790788844e-05
Step: 14160, train/epoch: 3.369823932647705
Step: 14170, train/loss: 0.0
Step: 14170, train/grad_norm: 0.00016848029918037355
Step: 14170, train/learning_rate: 3.313898196211085e-05
Step: 14170, train/epoch: 3.372203826904297
Step: 14180, train/loss: 0.0
Step: 14180, train/grad_norm: 0.004111321177333593
Step: 14180, train/learning_rate: 3.312708213343285e-05
Step: 14180, train/epoch: 3.3745834827423096
Step: 14190, train/loss: 0.0
Step: 14190, train/grad_norm: 0.03711499646306038
Step: 14190, train/learning_rate: 3.311518230475485e-05
Step: 14190, train/epoch: 3.3769633769989014
Step: 14200, train/loss: 0.0
Step: 14200, train/grad_norm: 4.373848241812084e-06
Step: 14200, train/learning_rate: 3.3103282476076856e-05
Step: 14200, train/epoch: 3.379343271255493
Step: 14210, train/loss: 0.0
Step: 14210, train/grad_norm: 0.0005491349147632718
Step: 14210, train/learning_rate: 3.3091386285377666e-05
Step: 14210, train/epoch: 3.381722927093506
Step: 14220, train/loss: 0.0
Step: 14220, train/grad_norm: 0.00012882515147794038
Step: 14220, train/learning_rate: 3.307948645669967e-05
Step: 14220, train/epoch: 3.3841028213500977
Step: 14230, train/loss: 0.002300000051036477
Step: 14230, train/grad_norm: 0.00024093779211398214
Step: 14230, train/learning_rate: 3.306758662802167e-05
Step: 14230, train/epoch: 3.3864827156066895
Step: 14240, train/loss: 0.16259999573230743
Step: 14240, train/grad_norm: 0.0036710971035063267
Step: 14240, train/learning_rate: 3.3055686799343675e-05
Step: 14240, train/epoch: 3.388862371444702
Step: 14250, train/loss: 0.0
Step: 14250, train/grad_norm: 0.0025992325972765684
Step: 14250, train/learning_rate: 3.304378697066568e-05
Step: 14250, train/epoch: 3.391242265701294
Step: 14260, train/loss: 0.0
Step: 14260, train/grad_norm: 0.002409809036180377
Step: 14260, train/learning_rate: 3.303189077996649e-05
Step: 14260, train/epoch: 3.3936221599578857
Step: 14270, train/loss: 0.0
Step: 14270, train/grad_norm: 0.0005259672761894763
Step: 14270, train/learning_rate: 3.301999095128849e-05
Step: 14270, train/epoch: 3.3960018157958984
Step: 14280, train/loss: 0.0
Step: 14280, train/grad_norm: 0.00019239779794588685
Step: 14280, train/learning_rate: 3.3008091122610494e-05
Step: 14280, train/epoch: 3.3983817100524902
Step: 14290, train/loss: 0.0
Step: 14290, train/grad_norm: 0.0010554325999692082
Step: 14290, train/learning_rate: 3.29961912939325e-05
Step: 14290, train/epoch: 3.400761604309082
Step: 14300, train/loss: 0.0
Step: 14300, train/grad_norm: 0.00013648424646817148
Step: 14300, train/learning_rate: 3.29842914652545e-05
Step: 14300, train/epoch: 3.4031412601470947
Step: 14310, train/loss: 0.0
Step: 14310, train/grad_norm: 6.691225280519575e-06
Step: 14310, train/learning_rate: 3.297239527455531e-05
Step: 14310, train/epoch: 3.4055211544036865
Step: 14320, train/loss: 0.0
Step: 14320, train/grad_norm: 0.00010312780068488792
Step: 14320, train/learning_rate: 3.2960495445877314e-05
Step: 14320, train/epoch: 3.4079010486602783
Step: 14330, train/loss: 0.0
Step: 14330, train/grad_norm: 3.945908247260377e-05
Step: 14330, train/learning_rate: 3.294859561719932e-05
Step: 14330, train/epoch: 3.410280704498291
Step: 14340, train/loss: 0.00019999999494757503
Step: 14340, train/grad_norm: 0.003149065189063549
Step: 14340, train/learning_rate: 3.293669578852132e-05
Step: 14340, train/epoch: 3.412660598754883
Step: 14350, train/loss: 0.00039999998989515007
Step: 14350, train/grad_norm: 1.2582320778165013e-05
Step: 14350, train/learning_rate: 3.292479595984332e-05
Step: 14350, train/epoch: 3.4150404930114746
Step: 14360, train/loss: 0.0
Step: 14360, train/grad_norm: 1.9329552742419764e-05
Step: 14360, train/learning_rate: 3.291289976914413e-05
Step: 14360, train/epoch: 3.4174203872680664
Step: 14370, train/loss: 0.0
Step: 14370, train/grad_norm: 2.2461568732978776e-05
Step: 14370, train/learning_rate: 3.2900999940466136e-05
Step: 14370, train/epoch: 3.419800043106079
Step: 14380, train/loss: 0.0
Step: 14380, train/grad_norm: 0.0009915457339957356
Step: 14380, train/learning_rate: 3.288910011178814e-05
Step: 14380, train/epoch: 3.422179937362671
Step: 14390, train/loss: 0.0
Step: 14390, train/grad_norm: 1.6930282072280534e-05
Step: 14390, train/learning_rate: 3.287720028311014e-05
Step: 14390, train/epoch: 3.4245598316192627
Step: 14400, train/loss: 0.0
Step: 14400, train/grad_norm: 0.00018393289064988494
Step: 14400, train/learning_rate: 3.2865300454432145e-05
Step: 14400, train/epoch: 3.4269394874572754
Step: 14410, train/loss: 0.0
Step: 14410, train/grad_norm: 3.446237315074541e-05
Step: 14410, train/learning_rate: 3.2853404263732955e-05
Step: 14410, train/epoch: 3.429319381713867
Step: 14420, train/loss: 0.0
Step: 14420, train/grad_norm: 8.083306966000237e-06
Step: 14420, train/learning_rate: 3.284150443505496e-05
Step: 14420, train/epoch: 3.431699275970459
Step: 14430, train/loss: 0.0
Step: 14430, train/grad_norm: 0.21265390515327454
Step: 14430, train/learning_rate: 3.282960460637696e-05
Step: 14430, train/epoch: 3.4340789318084717
Step: 14440, train/loss: 0.0
Step: 14440, train/grad_norm: 5.42253146704752e-05
Step: 14440, train/learning_rate: 3.2817704777698964e-05
Step: 14440, train/epoch: 3.4364588260650635
Step: 14450, train/loss: 0.0
Step: 14450, train/grad_norm: 1.6356407286366448e-05
Step: 14450, train/learning_rate: 3.280580494902097e-05
Step: 14450, train/epoch: 3.4388387203216553
Step: 14460, train/loss: 0.0
Step: 14460, train/grad_norm: 0.0010963566601276398
Step: 14460, train/learning_rate: 3.279390875832178e-05
Step: 14460, train/epoch: 3.441218376159668
Step: 14470, train/loss: 0.0
Step: 14470, train/grad_norm: 4.3166497221136524e-07
Step: 14470, train/learning_rate: 3.278200892964378e-05
Step: 14470, train/epoch: 3.4435982704162598
Step: 14480, train/loss: 0.0
Step: 14480, train/grad_norm: 9.222334483638406e-06
Step: 14480, train/learning_rate: 3.277010910096578e-05
Step: 14480, train/epoch: 3.4459781646728516
Step: 14490, train/loss: 0.0
Step: 14490, train/grad_norm: 2.0387471522553824e-05
Step: 14490, train/learning_rate: 3.2758209272287786e-05
Step: 14490, train/epoch: 3.4483578205108643
Step: 14500, train/loss: 0.0
Step: 14500, train/grad_norm: 1.2289650896946114e-07
Step: 14500, train/learning_rate: 3.2746313081588596e-05
Step: 14500, train/epoch: 3.450737714767456
Step: 14510, train/loss: 0.0
Step: 14510, train/grad_norm: 3.048172203534705e-08
Step: 14510, train/learning_rate: 3.27344132529106e-05
Step: 14510, train/epoch: 3.453117609024048
Step: 14520, train/loss: 0.0
Step: 14520, train/grad_norm: 3.961019501730334e-06
Step: 14520, train/learning_rate: 3.27225134242326e-05
Step: 14520, train/epoch: 3.4554972648620605
Step: 14530, train/loss: 0.0
Step: 14530, train/grad_norm: 0.004239409230649471
Step: 14530, train/learning_rate: 3.2710613595554605e-05
Step: 14530, train/epoch: 3.4578771591186523
Step: 14540, train/loss: 0.0
Step: 14540, train/grad_norm: 3.156214916089084e-06
Step: 14540, train/learning_rate: 3.269871376687661e-05
Step: 14540, train/epoch: 3.460257053375244
Step: 14550, train/loss: 0.0
Step: 14550, train/grad_norm: 2.066392517008353e-05
Step: 14550, train/learning_rate: 3.268681757617742e-05
Step: 14550, train/epoch: 3.462636947631836
Step: 14560, train/loss: 0.0
Step: 14560, train/grad_norm: 8.640012310934253e-06
Step: 14560, train/learning_rate: 3.267491774749942e-05
Step: 14560, train/epoch: 3.4650166034698486
Step: 14570, train/loss: 0.0
Step: 14570, train/grad_norm: 0.00010801255848491564
Step: 14570, train/learning_rate: 3.2663017918821424e-05
Step: 14570, train/epoch: 3.4673964977264404
Step: 14580, train/loss: 0.15469999611377716
Step: 14580, train/grad_norm: 4.2621290958777536e-06
Step: 14580, train/learning_rate: 3.265111809014343e-05
Step: 14580, train/epoch: 3.4697763919830322
Step: 14590, train/loss: 0.0
Step: 14590, train/grad_norm: 0.000580072752200067
Step: 14590, train/learning_rate: 3.263921826146543e-05
Step: 14590, train/epoch: 3.472156047821045
Step: 14600, train/loss: 0.0957999974489212
Step: 14600, train/grad_norm: 0.0004929477581754327
Step: 14600, train/learning_rate: 3.262732207076624e-05
Step: 14600, train/epoch: 3.4745359420776367
Step: 14610, train/loss: 0.02669999934732914
Step: 14610, train/grad_norm: 317.475341796875
Step: 14610, train/learning_rate: 3.261542224208824e-05
Step: 14610, train/epoch: 3.4769158363342285
Step: 14620, train/loss: 0.0
Step: 14620, train/grad_norm: 0.0005594347603619099
Step: 14620, train/learning_rate: 3.2603522413410246e-05
Step: 14620, train/epoch: 3.479295492172241
Step: 14630, train/loss: 9.999999747378752e-05
Step: 14630, train/grad_norm: 1.1590439498831984e-05
Step: 14630, train/learning_rate: 3.259162258473225e-05
Step: 14630, train/epoch: 3.481675386428833
Step: 14640, train/loss: 0.0
Step: 14640, train/grad_norm: 5.0211770030728076e-06
Step: 14640, train/learning_rate: 3.257972275605425e-05
Step: 14640, train/epoch: 3.484055280685425
Step: 14650, train/loss: 0.0
Step: 14650, train/grad_norm: 0.00010413197742309421
Step: 14650, train/learning_rate: 3.256782656535506e-05
Step: 14650, train/epoch: 3.4864349365234375
Step: 14660, train/loss: 0.0
Step: 14660, train/grad_norm: 2.658275889189099e-06
Step: 14660, train/learning_rate: 3.2555926736677065e-05
Step: 14660, train/epoch: 3.4888148307800293
Step: 14670, train/loss: 0.0
Step: 14670, train/grad_norm: 1.7129416107763973e-07
Step: 14670, train/learning_rate: 3.254402690799907e-05
Step: 14670, train/epoch: 3.491194725036621
Step: 14680, train/loss: 0.0
Step: 14680, train/grad_norm: 8.127619366860017e-05
Step: 14680, train/learning_rate: 3.253212707932107e-05
Step: 14680, train/epoch: 3.493574380874634
Step: 14690, train/loss: 0.0
Step: 14690, train/grad_norm: 0.0001344722113572061
Step: 14690, train/learning_rate: 3.2520227250643075e-05
Step: 14690, train/epoch: 3.4959542751312256
Step: 14700, train/loss: 0.0
Step: 14700, train/grad_norm: 1.3133779930285527e-06
Step: 14700, train/learning_rate: 3.2508331059943885e-05
Step: 14700, train/epoch: 3.4983341693878174
Step: 14710, train/loss: 0.0
Step: 14710, train/grad_norm: 1.349483136436902e-05
Step: 14710, train/learning_rate: 3.249643123126589e-05
Step: 14710, train/epoch: 3.500714063644409
Step: 14720, train/loss: 0.0
Step: 14720, train/grad_norm: 4.008928044640925e-06
Step: 14720, train/learning_rate: 3.248453140258789e-05
Step: 14720, train/epoch: 3.503093719482422
Step: 14730, train/loss: 0.07660000026226044
Step: 14730, train/grad_norm: 0.005273119080811739
Step: 14730, train/learning_rate: 3.2472631573909894e-05
Step: 14730, train/epoch: 3.5054736137390137
Step: 14740, train/loss: 0.0
Step: 14740, train/grad_norm: 5.660376336891204e-05
Step: 14740, train/learning_rate: 3.24607317452319e-05
Step: 14740, train/epoch: 3.5078535079956055
Step: 14750, train/loss: 0.0
Step: 14750, train/grad_norm: 0.02409915067255497
Step: 14750, train/learning_rate: 3.244883555453271e-05
Step: 14750, train/epoch: 3.510233163833618
Step: 14760, train/loss: 0.00039999998989515007
Step: 14760, train/grad_norm: 2.216823850176297e-05
Step: 14760, train/learning_rate: 3.243693572585471e-05
Step: 14760, train/epoch: 3.51261305809021
Step: 14770, train/loss: 0.0
Step: 14770, train/grad_norm: 5.780083483841736e-06
Step: 14770, train/learning_rate: 3.242503589717671e-05
Step: 14770, train/epoch: 3.5149929523468018
Step: 14780, train/loss: 0.0
Step: 14780, train/grad_norm: 0.0010218479437753558
Step: 14780, train/learning_rate: 3.2413136068498716e-05
Step: 14780, train/epoch: 3.5173726081848145
Step: 14790, train/loss: 0.0
Step: 14790, train/grad_norm: 4.540827092114341e-07
Step: 14790, train/learning_rate: 3.240123623982072e-05
Step: 14790, train/epoch: 3.5197525024414062
Step: 14800, train/loss: 0.15780000388622284
Step: 14800, train/grad_norm: 66.7514419555664
Step: 14800, train/learning_rate: 3.238934004912153e-05
Step: 14800, train/epoch: 3.522132396697998
Step: 14810, train/loss: 9.999999747378752e-05
Step: 14810, train/grad_norm: 0.0013796222629025578
Step: 14810, train/learning_rate: 3.237744022044353e-05
Step: 14810, train/epoch: 3.5245120525360107
Step: 14820, train/loss: 0.164900004863739
Step: 14820, train/grad_norm: 0.004035863094031811
Step: 14820, train/learning_rate: 3.2365540391765535e-05
Step: 14820, train/epoch: 3.5268919467926025
Step: 14830, train/loss: 0.0
Step: 14830, train/grad_norm: 1.4768946130061522e-05
Step: 14830, train/learning_rate: 3.235364056308754e-05
Step: 14830, train/epoch: 3.5292718410491943
Step: 14840, train/loss: 9.999999747378752e-05
Step: 14840, train/grad_norm: 0.0001310335792368278
Step: 14840, train/learning_rate: 3.234174073440954e-05
Step: 14840, train/epoch: 3.531651496887207
Step: 14850, train/loss: 0.01850000023841858
Step: 14850, train/grad_norm: 0.001705451519228518
Step: 14850, train/learning_rate: 3.232984454371035e-05
Step: 14850, train/epoch: 3.534031391143799
Step: 14860, train/loss: 0.0
Step: 14860, train/grad_norm: 0.00030553940450772643
Step: 14860, train/learning_rate: 3.2317944715032354e-05
Step: 14860, train/epoch: 3.5364112854003906
Step: 14870, train/loss: 0.0
Step: 14870, train/grad_norm: 0.0006824563024565578
Step: 14870, train/learning_rate: 3.230604488635436e-05
Step: 14870, train/epoch: 3.5387909412384033
Step: 14880, train/loss: 0.0
Step: 14880, train/grad_norm: 0.0029231959488242865
Step: 14880, train/learning_rate: 3.229414505767636e-05
Step: 14880, train/epoch: 3.541170835494995
Step: 14890, train/loss: 0.15700000524520874
Step: 14890, train/grad_norm: 0.0002277054009027779
Step: 14890, train/learning_rate: 3.228224522899836e-05
Step: 14890, train/epoch: 3.543550729751587
Step: 14900, train/loss: 0.04490000009536743
Step: 14900, train/grad_norm: 9.514290468359832e-06
Step: 14900, train/learning_rate: 3.227034903829917e-05
Step: 14900, train/epoch: 3.5459306240081787
Step: 14910, train/loss: 0.0
Step: 14910, train/grad_norm: 4.5918139221612364e-05
Step: 14910, train/learning_rate: 3.2258449209621176e-05
Step: 14910, train/epoch: 3.5483102798461914
Step: 14920, train/loss: 0.0
Step: 14920, train/grad_norm: 0.0002165488840546459
Step: 14920, train/learning_rate: 3.224654938094318e-05
Step: 14920, train/epoch: 3.550690174102783
Step: 14930, train/loss: 0.0012000000569969416
Step: 14930, train/grad_norm: 0.00042301262146793306
Step: 14930, train/learning_rate: 3.223464955226518e-05
Step: 14930, train/epoch: 3.553070068359375
Step: 14940, train/loss: 0.07580000162124634
Step: 14940, train/grad_norm: 0.001071589533239603
Step: 14940, train/learning_rate: 3.2222749723587185e-05
Step: 14940, train/epoch: 3.5554497241973877
Step: 14950, train/loss: 9.999999747378752e-05
Step: 14950, train/grad_norm: 8.615116530563682e-05
Step: 14950, train/learning_rate: 3.2210853532887995e-05
Step: 14950, train/epoch: 3.5578296184539795
Step: 14960, train/loss: 0.0
Step: 14960, train/grad_norm: 0.008779131807386875
Step: 14960, train/learning_rate: 3.219895370421e-05
Step: 14960, train/epoch: 3.5602095127105713
Step: 14970, train/loss: 0.0
Step: 14970, train/grad_norm: 0.00014376537001226097
Step: 14970, train/learning_rate: 3.2187053875532e-05
Step: 14970, train/epoch: 3.562589168548584
Step: 14980, train/loss: 0.0
Step: 14980, train/grad_norm: 8.887972217053175e-05
Step: 14980, train/learning_rate: 3.2175154046854004e-05
Step: 14980, train/epoch: 3.564969062805176
Step: 14990, train/loss: 0.0
Step: 14990, train/grad_norm: 3.9227095840033144e-05
Step: 14990, train/learning_rate: 3.216325421817601e-05
Step: 14990, train/epoch: 3.5673489570617676
Step: 15000, train/loss: 0.0
Step: 15000, train/grad_norm: 0.0001029138729791157
Step: 15000, train/learning_rate: 3.215135802747682e-05
Step: 15000, train/epoch: 3.5697286128997803
Step: 15010, train/loss: 0.0
Step: 15010, train/grad_norm: 3.932280742446892e-05
Step: 15010, train/learning_rate: 3.213945819879882e-05
Step: 15010, train/epoch: 3.572108507156372
Step: 15020, train/loss: 0.0005000000237487257
Step: 15020, train/grad_norm: 4.5424731069942936e-05
Step: 15020, train/learning_rate: 3.212755837012082e-05
Step: 15020, train/epoch: 3.574488401412964
Step: 15030, train/loss: 0.0015999999595806003
Step: 15030, train/grad_norm: 0.0005743459332734346
Step: 15030, train/learning_rate: 3.2115658541442826e-05
Step: 15030, train/epoch: 3.5768680572509766
Step: 15040, train/loss: 0.026900000870227814
Step: 15040, train/grad_norm: 2.5693681891425513e-05
Step: 15040, train/learning_rate: 3.210375871276483e-05
Step: 15040, train/epoch: 3.5792479515075684
Step: 15050, train/loss: 9.999999747378752e-05
Step: 15050, train/grad_norm: 2.3695931304246187e-05
Step: 15050, train/learning_rate: 3.209186252206564e-05
Step: 15050, train/epoch: 3.58162784576416
Step: 15060, train/loss: 0.0
Step: 15060, train/grad_norm: 4.31138460044167e-06
Step: 15060, train/learning_rate: 3.207996269338764e-05
Step: 15060, train/epoch: 3.584007501602173
Step: 15070, train/loss: 0.0
Step: 15070, train/grad_norm: 1.354212872684002e-05
Step: 15070, train/learning_rate: 3.2068062864709646e-05
Step: 15070, train/epoch: 3.5863873958587646
Step: 15080, train/loss: 0.04879999905824661
Step: 15080, train/grad_norm: 6.249390935408883e-06
Step: 15080, train/learning_rate: 3.205616303603165e-05
Step: 15080, train/epoch: 3.5887672901153564
Step: 15090, train/loss: 0.0
Step: 15090, train/grad_norm: 0.0003701868699863553
Step: 15090, train/learning_rate: 3.204426320735365e-05
Step: 15090, train/epoch: 3.5911471843719482
Step: 15100, train/loss: 0.0
Step: 15100, train/grad_norm: 6.148251122795045e-05
Step: 15100, train/learning_rate: 3.203236701665446e-05
Step: 15100, train/epoch: 3.593526840209961
Step: 15110, train/loss: 0.0
Step: 15110, train/grad_norm: 4.220913979224861e-06
Step: 15110, train/learning_rate: 3.2020467187976465e-05
Step: 15110, train/epoch: 3.5959067344665527
Step: 15120, train/loss: 0.10080000013113022
Step: 15120, train/grad_norm: 8.385300316149369e-05
Step: 15120, train/learning_rate: 3.200856735929847e-05
Step: 15120, train/epoch: 3.5982866287231445
Step: 15130, train/loss: 0.10159999877214432
Step: 15130, train/grad_norm: 3.938162080885377e-06
Step: 15130, train/learning_rate: 3.199666753062047e-05
Step: 15130, train/epoch: 3.6006662845611572
Step: 15140, train/loss: 0.0
Step: 15140, train/grad_norm: 0.0002564194437582046
Step: 15140, train/learning_rate: 3.1984767701942474e-05
Step: 15140, train/epoch: 3.603046178817749
Step: 15150, train/loss: 9.999999747378752e-05
Step: 15150, train/grad_norm: 0.00026791548589244485
Step: 15150, train/learning_rate: 3.1972871511243284e-05
Step: 15150, train/epoch: 3.605426073074341
Step: 15160, train/loss: 0.0
Step: 15160, train/grad_norm: 0.0004476220055948943
Step: 15160, train/learning_rate: 3.196097168256529e-05
Step: 15160, train/epoch: 3.6078057289123535
Step: 15170, train/loss: 0.11949999630451202
Step: 15170, train/grad_norm: 0.00047388047096319497
Step: 15170, train/learning_rate: 3.194907185388729e-05
Step: 15170, train/epoch: 3.6101856231689453
Step: 15180, train/loss: 0.0
Step: 15180, train/grad_norm: 0.0004724557511508465
Step: 15180, train/learning_rate: 3.193717202520929e-05
Step: 15180, train/epoch: 3.612565517425537
Step: 15190, train/loss: 0.0
Step: 15190, train/grad_norm: 8.938361861510202e-05
Step: 15190, train/learning_rate: 3.1925272196531296e-05
Step: 15190, train/epoch: 3.61494517326355
Step: 15200, train/loss: 0.0
Step: 15200, train/grad_norm: 5.846855492563918e-05
Step: 15200, train/learning_rate: 3.1913376005832106e-05
Step: 15200, train/epoch: 3.6173250675201416
Step: 15210, train/loss: 0.0
Step: 15210, train/grad_norm: 0.0009217459592036903
Step: 15210, train/learning_rate: 3.190147617715411e-05
Step: 15210, train/epoch: 3.6197049617767334
Step: 15220, train/loss: 0.0
Step: 15220, train/grad_norm: 0.00026826810790225863
Step: 15220, train/learning_rate: 3.188957634847611e-05
Step: 15220, train/epoch: 3.622084617614746
Step: 15230, train/loss: 0.0
Step: 15230, train/grad_norm: 0.0026437791530042887
Step: 15230, train/learning_rate: 3.1877676519798115e-05
Step: 15230, train/epoch: 3.624464511871338
Step: 15240, train/loss: 0.0
Step: 15240, train/grad_norm: 1.2141621482442133e-05
Step: 15240, train/learning_rate: 3.186577669112012e-05
Step: 15240, train/epoch: 3.6268444061279297
Step: 15250, train/loss: 0.0
Step: 15250, train/grad_norm: 0.0005106859025545418
Step: 15250, train/learning_rate: 3.185388050042093e-05
Step: 15250, train/epoch: 3.6292240619659424
Step: 15260, train/loss: 0.0
Step: 15260, train/grad_norm: 0.0011573386145755649
Step: 15260, train/learning_rate: 3.184198067174293e-05
Step: 15260, train/epoch: 3.631603956222534
Step: 15270, train/loss: 0.0
Step: 15270, train/grad_norm: 0.5317063927650452
Step: 15270, train/learning_rate: 3.1830080843064934e-05
Step: 15270, train/epoch: 3.633983850479126
Step: 15280, train/loss: 0.0
Step: 15280, train/grad_norm: 0.0003327605954837054
Step: 15280, train/learning_rate: 3.181818101438694e-05
Step: 15280, train/epoch: 3.6363637447357178
Step: 15290, train/loss: 0.14229999482631683
Step: 15290, train/grad_norm: 2.248221426270902e-05
Step: 15290, train/learning_rate: 3.180628118570894e-05
Step: 15290, train/epoch: 3.6387434005737305
Step: 15300, train/loss: 0.0
Step: 15300, train/grad_norm: 0.0022878972813487053
Step: 15300, train/learning_rate: 3.179438499500975e-05
Step: 15300, train/epoch: 3.6411232948303223
Step: 15310, train/loss: 0.16179999709129333
Step: 15310, train/grad_norm: 0.006005636416375637
Step: 15310, train/learning_rate: 3.178248516633175e-05
Step: 15310, train/epoch: 3.643503189086914
Step: 15320, train/loss: 9.999999747378752e-05
Step: 15320, train/grad_norm: 0.10587786138057709
Step: 15320, train/learning_rate: 3.1770585337653756e-05
Step: 15320, train/epoch: 3.6458828449249268
Step: 15330, train/loss: 9.999999747378752e-05
Step: 15330, train/grad_norm: 0.0027663756627589464
Step: 15330, train/learning_rate: 3.175868550897576e-05
Step: 15330, train/epoch: 3.6482627391815186
Step: 15340, train/loss: 0.0
Step: 15340, train/grad_norm: 0.00016819890879560262
Step: 15340, train/learning_rate: 3.174678568029776e-05
Step: 15340, train/epoch: 3.6506426334381104
Step: 15350, train/loss: 0.00019999999494757503
Step: 15350, train/grad_norm: 0.0001529236906208098
Step: 15350, train/learning_rate: 3.173488948959857e-05
Step: 15350, train/epoch: 3.653022289276123
Step: 15360, train/loss: 0.00139999995008111
Step: 15360, train/grad_norm: 4.711272049462423e-05
Step: 15360, train/learning_rate: 3.1722989660920575e-05
Step: 15360, train/epoch: 3.655402183532715
Step: 15370, train/loss: 0.0
Step: 15370, train/grad_norm: 0.0002734116278588772
Step: 15370, train/learning_rate: 3.171108983224258e-05
Step: 15370, train/epoch: 3.6577820777893066
Step: 15380, train/loss: 0.0
Step: 15380, train/grad_norm: 0.0008391299052163959
Step: 15380, train/learning_rate: 3.169919000356458e-05
Step: 15380, train/epoch: 3.6601617336273193
Step: 15390, train/loss: 0.0
Step: 15390, train/grad_norm: 6.699692676193081e-06
Step: 15390, train/learning_rate: 3.1687290174886584e-05
Step: 15390, train/epoch: 3.662541627883911
Step: 15400, train/loss: 0.0
Step: 15400, train/grad_norm: 7.75411244831048e-05
Step: 15400, train/learning_rate: 3.1675393984187394e-05
Step: 15400, train/epoch: 3.664921522140503
Step: 15410, train/loss: 0.0
Step: 15410, train/grad_norm: 0.00015613689902238548
Step: 15410, train/learning_rate: 3.16634941555094e-05
Step: 15410, train/epoch: 3.6673011779785156
Step: 15420, train/loss: 0.0
Step: 15420, train/grad_norm: 1.7480779206380248e-05
Step: 15420, train/learning_rate: 3.16515943268314e-05
Step: 15420, train/epoch: 3.6696810722351074
Step: 15430, train/loss: 0.0
Step: 15430, train/grad_norm: 1.5440763263541157e-06
Step: 15430, train/learning_rate: 3.1639694498153403e-05
Step: 15430, train/epoch: 3.672060966491699
Step: 15440, train/loss: 0.0
Step: 15440, train/grad_norm: 2.091956412186846e-05
Step: 15440, train/learning_rate: 3.1627794669475406e-05
Step: 15440, train/epoch: 3.674440860748291
Step: 15450, train/loss: 0.0
Step: 15450, train/grad_norm: 2.7592568585532717e-06
Step: 15450, train/learning_rate: 3.1615898478776217e-05
Step: 15450, train/epoch: 3.6768205165863037
Step: 15460, train/loss: 0.11020000278949738
Step: 15460, train/grad_norm: 49.40927505493164
Step: 15460, train/learning_rate: 3.160399865009822e-05
Step: 15460, train/epoch: 3.6792004108428955
Step: 15470, train/loss: 0.00039999998989515007
Step: 15470, train/grad_norm: 0.5580385327339172
Step: 15470, train/learning_rate: 3.159209882142022e-05
Step: 15470, train/epoch: 3.6815803050994873
Step: 15480, train/loss: 0.0
Step: 15480, train/grad_norm: 0.0001711004733806476
Step: 15480, train/learning_rate: 3.1580198992742226e-05
Step: 15480, train/epoch: 3.6839599609375
Step: 15490, train/loss: 0.09730000048875809
Step: 15490, train/grad_norm: 66.57588958740234
Step: 15490, train/learning_rate: 3.156829916406423e-05
Step: 15490, train/epoch: 3.686339855194092
Step: 15500, train/loss: 0.0
Step: 15500, train/grad_norm: 8.4870116552338e-05
Step: 15500, train/learning_rate: 3.155640297336504e-05
Step: 15500, train/epoch: 3.6887197494506836
Step: 15510, train/loss: 0.15549999475479126
Step: 15510, train/grad_norm: 9.11777387955226e-05
Step: 15510, train/learning_rate: 3.154450314468704e-05
Step: 15510, train/epoch: 3.6910994052886963
Step: 15520, train/loss: 0.0
Step: 15520, train/grad_norm: 0.0005530135240405798
Step: 15520, train/learning_rate: 3.1532603316009045e-05
Step: 15520, train/epoch: 3.693479299545288
Step: 15530, train/loss: 0.0005000000237487257
Step: 15530, train/grad_norm: 0.00025292200734838843
Step: 15530, train/learning_rate: 3.152070348733105e-05
Step: 15530, train/epoch: 3.69585919380188
Step: 15540, train/loss: 0.0
Step: 15540, train/grad_norm: 0.0001737069251248613
Step: 15540, train/learning_rate: 3.150880365865305e-05
Step: 15540, train/epoch: 3.6982388496398926
Step: 15550, train/loss: 0.0005000000237487257
Step: 15550, train/grad_norm: 0.00013743115414399654
Step: 15550, train/learning_rate: 3.149690746795386e-05
Step: 15550, train/epoch: 3.7006187438964844
Step: 15560, train/loss: 0.0
Step: 15560, train/grad_norm: 0.0005449726013466716
Step: 15560, train/learning_rate: 3.1485007639275864e-05
Step: 15560, train/epoch: 3.702998638153076
Step: 15570, train/loss: 0.0
Step: 15570, train/grad_norm: 2.9360289772739634e-05
Step: 15570, train/learning_rate: 3.147310781059787e-05
Step: 15570, train/epoch: 3.705378293991089
Step: 15580, train/loss: 0.0
Step: 15580, train/grad_norm: 0.0003253139730077237
Step: 15580, train/learning_rate: 3.146120798191987e-05
Step: 15580, train/epoch: 3.7077581882476807
Step: 15590, train/loss: 0.0
Step: 15590, train/grad_norm: 0.00014916261716280133
Step: 15590, train/learning_rate: 3.144930815324187e-05
Step: 15590, train/epoch: 3.7101380825042725
Step: 15600, train/loss: 0.0
Step: 15600, train/grad_norm: 0.003889518091455102
Step: 15600, train/learning_rate: 3.143741196254268e-05
Step: 15600, train/epoch: 3.712517738342285
Step: 15610, train/loss: 0.0
Step: 15610, train/grad_norm: 0.0095966262742877
Step: 15610, train/learning_rate: 3.1425512133864686e-05
Step: 15610, train/epoch: 3.714897632598877
Step: 15620, train/loss: 0.0
Step: 15620, train/grad_norm: 6.038556239218451e-05
Step: 15620, train/learning_rate: 3.141361230518669e-05
Step: 15620, train/epoch: 3.7172775268554688
Step: 15630, train/loss: 0.22460000216960907
Step: 15630, train/grad_norm: 0.000247589050559327
Step: 15630, train/learning_rate: 3.140171247650869e-05
Step: 15630, train/epoch: 3.7196574211120605
Step: 15640, train/loss: 0.0005000000237487257
Step: 15640, train/grad_norm: 19.88344383239746
Step: 15640, train/learning_rate: 3.1389812647830695e-05
Step: 15640, train/epoch: 3.7220370769500732
Step: 15650, train/loss: 9.999999747378752e-05
Step: 15650, train/grad_norm: 2.7775561809539795
Step: 15650, train/learning_rate: 3.1377916457131505e-05
Step: 15650, train/epoch: 3.724416971206665
Step: 15660, train/loss: 0.0
Step: 15660, train/grad_norm: 0.00012240125215612352
Step: 15660, train/learning_rate: 3.136601662845351e-05
Step: 15660, train/epoch: 3.726796865463257
Step: 15670, train/loss: 0.06920000165700912
Step: 15670, train/grad_norm: 8.125108433887362e-05
Step: 15670, train/learning_rate: 3.135411679977551e-05
Step: 15670, train/epoch: 3.7291765213012695
Step: 15680, train/loss: 0.0
Step: 15680, train/grad_norm: 0.0007751384982839227
Step: 15680, train/learning_rate: 3.1342216971097514e-05
Step: 15680, train/epoch: 3.7315564155578613
Step: 15690, train/loss: 0.0
Step: 15690, train/grad_norm: 2.2152531528263353e-05
Step: 15690, train/learning_rate: 3.133031714241952e-05
Step: 15690, train/epoch: 3.733936309814453
Step: 15700, train/loss: 0.0
Step: 15700, train/grad_norm: 0.003450451884418726
Step: 15700, train/learning_rate: 3.131842095172033e-05
Step: 15700, train/epoch: 3.736315965652466
Step: 15710, train/loss: 0.0
Step: 15710, train/grad_norm: 0.004436439834535122
Step: 15710, train/learning_rate: 3.130652112304233e-05
Step: 15710, train/epoch: 3.7386958599090576
Step: 15720, train/loss: 0.0
Step: 15720, train/grad_norm: 7.563695589851704e-07
Step: 15720, train/learning_rate: 3.129462129436433e-05
Step: 15720, train/epoch: 3.7410757541656494
Step: 15730, train/loss: 0.0
Step: 15730, train/grad_norm: 1.058164798450889e-05
Step: 15730, train/learning_rate: 3.1282721465686336e-05
Step: 15730, train/epoch: 3.743455410003662
Step: 15740, train/loss: 0.0
Step: 15740, train/grad_norm: 0.0001964226976269856
Step: 15740, train/learning_rate: 3.127082163700834e-05
Step: 15740, train/epoch: 3.745835304260254
Step: 15750, train/loss: 0.0
Step: 15750, train/grad_norm: 0.00021866834140382707
Step: 15750, train/learning_rate: 3.125892544630915e-05
Step: 15750, train/epoch: 3.7482151985168457
Step: 15760, train/loss: 0.0
Step: 15760, train/grad_norm: 2.7187925297766924e-05
Step: 15760, train/learning_rate: 3.124702561763115e-05
Step: 15760, train/epoch: 3.7505948543548584
Step: 15770, train/loss: 0.0
Step: 15770, train/grad_norm: 0.00013795532868243754
Step: 15770, train/learning_rate: 3.1235125788953155e-05
Step: 15770, train/epoch: 3.75297474861145
Step: 15780, train/loss: 0.0
Step: 15780, train/grad_norm: 0.00035278021823614836
Step: 15780, train/learning_rate: 3.122322596027516e-05
Step: 15780, train/epoch: 3.755354642868042
Step: 15790, train/loss: 0.0
Step: 15790, train/grad_norm: 0.0003513292467687279
Step: 15790, train/learning_rate: 3.121132613159716e-05
Step: 15790, train/epoch: 3.7577342987060547
Step: 15800, train/loss: 0.0
Step: 15800, train/grad_norm: 0.0008798061753623188
Step: 15800, train/learning_rate: 3.119942994089797e-05
Step: 15800, train/epoch: 3.7601141929626465
Step: 15810, train/loss: 0.0
Step: 15810, train/grad_norm: 4.185606667306274e-05
Step: 15810, train/learning_rate: 3.1187530112219974e-05
Step: 15810, train/epoch: 3.7624940872192383
Step: 15820, train/loss: 0.0
Step: 15820, train/grad_norm: 5.5504802730865777e-05
Step: 15820, train/learning_rate: 3.117563028354198e-05
Step: 15820, train/epoch: 3.76487398147583
Step: 15830, train/loss: 0.0
Step: 15830, train/grad_norm: 2.16702687794168e-06
Step: 15830, train/learning_rate: 3.116373045486398e-05
Step: 15830, train/epoch: 3.7672536373138428
Step: 15840, train/loss: 0.05739999935030937
Step: 15840, train/grad_norm: 2.9872870072722435e-05
Step: 15840, train/learning_rate: 3.115183426416479e-05
Step: 15840, train/epoch: 3.7696335315704346
Step: 15850, train/loss: 0.0
Step: 15850, train/grad_norm: 1.0761558542071725e-06
Step: 15850, train/learning_rate: 3.1139934435486794e-05
Step: 15850, train/epoch: 3.7720134258270264
Step: 15860, train/loss: 0.0
Step: 15860, train/grad_norm: 0.00012313648767303675
Step: 15860, train/learning_rate: 3.1128034606808797e-05
Step: 15860, train/epoch: 3.774393081665039
Step: 15870, train/loss: 0.0
Step: 15870, train/grad_norm: 0.00017274630954489112
Step: 15870, train/learning_rate: 3.11161347781308e-05
Step: 15870, train/epoch: 3.776772975921631
Step: 15880, train/loss: 9.999999747378752e-05
Step: 15880, train/grad_norm: 1.1732133771147346e-06
Step: 15880, train/learning_rate: 3.11042349494528e-05
Step: 15880, train/epoch: 3.7791528701782227
Step: 15890, train/loss: 9.999999747378752e-05
Step: 15890, train/grad_norm: 4.5862936531193554e-05
Step: 15890, train/learning_rate: 3.109233875875361e-05
Step: 15890, train/epoch: 3.7815325260162354
Step: 15900, train/loss: 0.0003000000142492354
Step: 15900, train/grad_norm: 3.985837338404963e-06
Step: 15900, train/learning_rate: 3.1080438930075616e-05
Step: 15900, train/epoch: 3.783912420272827
Step: 15910, train/loss: 0.0
Step: 15910, train/grad_norm: 0.0014241987373679876
Step: 15910, train/learning_rate: 3.106853910139762e-05
Step: 15910, train/epoch: 3.786292314529419
Step: 15920, train/loss: 0.0005000000237487257
Step: 15920, train/grad_norm: 2.061505801975727e-05
Step: 15920, train/learning_rate: 3.105663927271962e-05
Step: 15920, train/epoch: 3.7886719703674316
Step: 15930, train/loss: 0.0
Step: 15930, train/grad_norm: 4.844015165872406e-06
Step: 15930, train/learning_rate: 3.1044739444041625e-05
Step: 15930, train/epoch: 3.7910518646240234
Step: 15940, train/loss: 0.23909999430179596
Step: 15940, train/grad_norm: 0.0009418540284968913
Step: 15940, train/learning_rate: 3.1032843253342435e-05
Step: 15940, train/epoch: 3.7934317588806152
Step: 15950, train/loss: 0.0
Step: 15950, train/grad_norm: 0.011007903143763542
Step: 15950, train/learning_rate: 3.102094342466444e-05
Step: 15950, train/epoch: 3.795811414718628
Step: 15960, train/loss: 0.000699999975040555
Step: 15960, train/grad_norm: 0.013126246631145477
Step: 15960, train/learning_rate: 3.100904359598644e-05
Step: 15960, train/epoch: 3.7981913089752197
Step: 15970, train/loss: 0.0
Step: 15970, train/grad_norm: 0.00734906317666173
Step: 15970, train/learning_rate: 3.0997143767308444e-05
Step: 15970, train/epoch: 3.8005712032318115
Step: 15980, train/loss: 0.00019999999494757503
Step: 15980, train/grad_norm: 4.973757768311771e-06
Step: 15980, train/learning_rate: 3.098524393863045e-05
Step: 15980, train/epoch: 3.802950859069824
Step: 15990, train/loss: 0.013799999840557575
Step: 15990, train/grad_norm: 1.3050083907728549e-05
Step: 15990, train/learning_rate: 3.097334774793126e-05
Step: 15990, train/epoch: 3.805330753326416
Step: 16000, train/loss: 0.0
Step: 16000, train/grad_norm: 1.794008130673319e-05
Step: 16000, train/learning_rate: 3.096144791925326e-05
Step: 16000, train/epoch: 3.807710647583008
Step: 16010, train/loss: 0.0
Step: 16010, train/grad_norm: 2.2054266082705e-05
Step: 16010, train/learning_rate: 3.094954809057526e-05
Step: 16010, train/epoch: 3.8100905418395996
Step: 16020, train/loss: 0.0
Step: 16020, train/grad_norm: 0.00017517022206448019
Step: 16020, train/learning_rate: 3.0937648261897266e-05
Step: 16020, train/epoch: 3.8124701976776123
Step: 16030, train/loss: 0.0
Step: 16030, train/grad_norm: 4.5740418386230886e-07
Step: 16030, train/learning_rate: 3.092574843321927e-05
Step: 16030, train/epoch: 3.814850091934204
Step: 16040, train/loss: 0.0
Step: 16040, train/grad_norm: 4.320805047086651e-08
Step: 16040, train/learning_rate: 3.091385224252008e-05
Step: 16040, train/epoch: 3.817229986190796
Step: 16050, train/loss: 0.14920000731945038
Step: 16050, train/grad_norm: 94.83793640136719
Step: 16050, train/learning_rate: 3.090195241384208e-05
Step: 16050, train/epoch: 3.8196096420288086
Step: 16060, train/loss: 0.0
Step: 16060, train/grad_norm: 8.037851512199268e-05
Step: 16060, train/learning_rate: 3.0890052585164085e-05
Step: 16060, train/epoch: 3.8219895362854004
Step: 16070, train/loss: 0.0
Step: 16070, train/grad_norm: 0.00017016891797538847
Step: 16070, train/learning_rate: 3.087815275648609e-05
Step: 16070, train/epoch: 3.824369430541992
Step: 16080, train/loss: 0.0003000000142492354
Step: 16080, train/grad_norm: 1.6142053604125977
Step: 16080, train/learning_rate: 3.086625292780809e-05
Step: 16080, train/epoch: 3.826749086380005
Step: 16090, train/loss: 0.0
Step: 16090, train/grad_norm: 2.630739072628785e-05
Step: 16090, train/learning_rate: 3.08543567371089e-05
Step: 16090, train/epoch: 3.8291289806365967
Step: 16100, train/loss: 0.0
Step: 16100, train/grad_norm: 0.0034556223545223475
Step: 16100, train/learning_rate: 3.0842456908430904e-05
Step: 16100, train/epoch: 3.8315088748931885
Step: 16110, train/loss: 0.0
Step: 16110, train/grad_norm: 3.952224233216839e-06
Step: 16110, train/learning_rate: 3.083055707975291e-05
Step: 16110, train/epoch: 3.833888530731201
Step: 16120, train/loss: 0.0
Step: 16120, train/grad_norm: 6.004311580909416e-05
Step: 16120, train/learning_rate: 3.081865725107491e-05
Step: 16120, train/epoch: 3.836268424987793
Step: 16130, train/loss: 0.0
Step: 16130, train/grad_norm: 1.3454719010042027e-05
Step: 16130, train/learning_rate: 3.080675742239691e-05
Step: 16130, train/epoch: 3.8386483192443848
Step: 16140, train/loss: 0.0
Step: 16140, train/grad_norm: 0.0003485189808998257
Step: 16140, train/learning_rate: 3.079486123169772e-05
Step: 16140, train/epoch: 3.8410279750823975
Step: 16150, train/loss: 0.0
Step: 16150, train/grad_norm: 4.638222435460193e-06
Step: 16150, train/learning_rate: 3.0782961403019726e-05
Step: 16150, train/epoch: 3.8434078693389893
Step: 16160, train/loss: 0.0
Step: 16160, train/grad_norm: 0.00288577564060688
Step: 16160, train/learning_rate: 3.077106157434173e-05
Step: 16160, train/epoch: 3.845787763595581
Step: 16170, train/loss: 0.0
Step: 16170, train/grad_norm: 1.1288561836408917e-06
Step: 16170, train/learning_rate: 3.075916174566373e-05
Step: 16170, train/epoch: 3.848167657852173
Step: 16180, train/loss: 0.0
Step: 16180, train/grad_norm: 9.822771971812472e-06
Step: 16180, train/learning_rate: 3.0747261916985735e-05
Step: 16180, train/epoch: 3.8505473136901855
Step: 16190, train/loss: 0.000699999975040555
Step: 16190, train/grad_norm: 8.855061395252051e-08
Step: 16190, train/learning_rate: 3.0735365726286545e-05
Step: 16190, train/epoch: 3.8529272079467773
Step: 16200, train/loss: 0.0
Step: 16200, train/grad_norm: 3.927851253138215e-07
Step: 16200, train/learning_rate: 3.072346589760855e-05
Step: 16200, train/epoch: 3.855307102203369
Step: 16210, train/loss: 0.1046999990940094
Step: 16210, train/grad_norm: 0.0028569826390594244
Step: 16210, train/learning_rate: 3.071156606893055e-05
Step: 16210, train/epoch: 3.857686758041382
Step: 16220, train/loss: 0.0
Step: 16220, train/grad_norm: 0.007663989905267954
Step: 16220, train/learning_rate: 3.0699666240252554e-05
Step: 16220, train/epoch: 3.8600666522979736
Step: 16230, train/loss: 0.0
Step: 16230, train/grad_norm: 0.02892431430518627
Step: 16230, train/learning_rate: 3.068776641157456e-05
Step: 16230, train/epoch: 3.8624465465545654
Step: 16240, train/loss: 0.0
Step: 16240, train/grad_norm: 0.0004939469508826733
Step: 16240, train/learning_rate: 3.067587022087537e-05
Step: 16240, train/epoch: 3.864826202392578
Step: 16250, train/loss: 0.00019999999494757503
Step: 16250, train/grad_norm: 1.081818754755659e-05
Step: 16250, train/learning_rate: 3.066397039219737e-05
Step: 16250, train/epoch: 3.86720609664917
Step: 16260, train/loss: 0.0
Step: 16260, train/grad_norm: 3.34361160980734e-09
Step: 16260, train/learning_rate: 3.0652070563519374e-05
Step: 16260, train/epoch: 3.8695859909057617
Step: 16270, train/loss: 0.0
Step: 16270, train/grad_norm: 4.236321728967596e-06
Step: 16270, train/learning_rate: 3.0640170734841377e-05
Step: 16270, train/epoch: 3.8719656467437744
Step: 16280, train/loss: 0.0
Step: 16280, train/grad_norm: 1.5890432791820785e-07
Step: 16280, train/learning_rate: 3.062827090616338e-05
Step: 16280, train/epoch: 3.874345541000366
Step: 16290, train/loss: 9.999999747378752e-05
Step: 16290, train/grad_norm: 1.9508560100689465e-08
Step: 16290, train/learning_rate: 3.061637471546419e-05
Step: 16290, train/epoch: 3.876725435256958
Step: 16300, train/loss: 0.0
Step: 16300, train/grad_norm: 1.3507501783749376e-09
Step: 16300, train/learning_rate: 3.060447488678619e-05
Step: 16300, train/epoch: 3.8791050910949707
Step: 16310, train/loss: 0.0
Step: 16310, train/grad_norm: 0.0009090492385439575
Step: 16310, train/learning_rate: 3.0592575058108196e-05
Step: 16310, train/epoch: 3.8814849853515625
Step: 16320, train/loss: 0.0
Step: 16320, train/grad_norm: 6.539595087673433e-09
Step: 16320, train/learning_rate: 3.05806752294302e-05
Step: 16320, train/epoch: 3.8838648796081543
Step: 16330, train/loss: 0.0
Step: 16330, train/grad_norm: 1.5356259552845586e-07
Step: 16330, train/learning_rate: 3.05687754007522e-05
Step: 16330, train/epoch: 3.886244535446167
Step: 16340, train/loss: 0.08709999918937683
Step: 16340, train/grad_norm: 118.46875
Step: 16340, train/learning_rate: 3.055687921005301e-05
Step: 16340, train/epoch: 3.888624429702759
Step: 16350, train/loss: 0.0
Step: 16350, train/grad_norm: 0.0005297885509207845
Step: 16350, train/learning_rate: 3.0544979381375015e-05
Step: 16350, train/epoch: 3.8910043239593506
Step: 16360, train/loss: 0.0
Step: 16360, train/grad_norm: 0.00017736236623022705
Step: 16360, train/learning_rate: 3.053307955269702e-05
Step: 16360, train/epoch: 3.8933842182159424
Step: 16370, train/loss: 9.999999747378752e-05
Step: 16370, train/grad_norm: 2.338084215125491e-07
Step: 16370, train/learning_rate: 3.052117972401902e-05
Step: 16370, train/epoch: 3.895763874053955
Step: 16380, train/loss: 0.0
Step: 16380, train/grad_norm: 2.513845629437128e-07
Step: 16380, train/learning_rate: 3.0509281714330427e-05
Step: 16380, train/epoch: 3.898143768310547
Step: 16390, train/loss: 0.0
Step: 16390, train/grad_norm: 3.617738570937945e-07
Step: 16390, train/learning_rate: 3.049738188565243e-05
Step: 16390, train/epoch: 3.9005236625671387
Step: 16400, train/loss: 0.0
Step: 16400, train/grad_norm: 0.00022227798763196915
Step: 16400, train/learning_rate: 3.0485483875963837e-05
Step: 16400, train/epoch: 3.9029033184051514
Step: 16410, train/loss: 0.0
Step: 16410, train/grad_norm: 3.682916371872125e-08
Step: 16410, train/learning_rate: 3.047358404728584e-05
Step: 16410, train/epoch: 3.905283212661743
Step: 16420, train/loss: 0.0
Step: 16420, train/grad_norm: 1.9702254405729036e-07
Step: 16420, train/learning_rate: 3.0461684218607843e-05
Step: 16420, train/epoch: 3.907663106918335
Step: 16430, train/loss: 0.0
Step: 16430, train/grad_norm: 1.241614278058023e-08
Step: 16430, train/learning_rate: 3.044978620891925e-05
Step: 16430, train/epoch: 3.9100427627563477
Step: 16440, train/loss: 0.0
Step: 16440, train/grad_norm: 9.84622161581683e-09
Step: 16440, train/learning_rate: 3.0437886380241252e-05
Step: 16440, train/epoch: 3.9124226570129395
Step: 16450, train/loss: 0.0
Step: 16450, train/grad_norm: 6.98373767704652e-08
Step: 16450, train/learning_rate: 3.042598837055266e-05
Step: 16450, train/epoch: 3.9148025512695312
Step: 16460, train/loss: 0.0
Step: 16460, train/grad_norm: 3.6795077562601364e-07
Step: 16460, train/learning_rate: 3.0414088541874662e-05
Step: 16460, train/epoch: 3.917182207107544
Step: 16470, train/loss: 0.01810000091791153
Step: 16470, train/grad_norm: 7.889490234447294e-07
Step: 16470, train/learning_rate: 3.0402188713196665e-05
Step: 16470, train/epoch: 3.9195621013641357
Step: 16480, train/loss: 0.0
Step: 16480, train/grad_norm: 8.41829660203075e-06
Step: 16480, train/learning_rate: 3.039029070350807e-05
Step: 16480, train/epoch: 3.9219419956207275
Step: 16490, train/loss: 0.0
Step: 16490, train/grad_norm: 8.00991547293961e-05
Step: 16490, train/learning_rate: 3.0378390874830075e-05
Step: 16490, train/epoch: 3.9243216514587402
Step: 16500, train/loss: 0.2687999904155731
Step: 16500, train/grad_norm: 0.00018630239355843514
Step: 16500, train/learning_rate: 3.036649286514148e-05
Step: 16500, train/epoch: 3.926701545715332
Step: 16510, train/loss: 0.0
Step: 16510, train/grad_norm: 0.003073820611461997
Step: 16510, train/learning_rate: 3.0354593036463484e-05
Step: 16510, train/epoch: 3.929081439971924
Step: 16520, train/loss: 9.999999747378752e-05
Step: 16520, train/grad_norm: 0.0077211190946400166
Step: 16520, train/learning_rate: 3.0342693207785487e-05
Step: 16520, train/epoch: 3.9314610958099365
Step: 16530, train/loss: 0.0
Step: 16530, train/grad_norm: 0.0003828493063338101
Step: 16530, train/learning_rate: 3.0330795198096894e-05
Step: 16530, train/epoch: 3.9338409900665283
Step: 16540, train/loss: 0.0
Step: 16540, train/grad_norm: 0.0007700492278672755
Step: 16540, train/learning_rate: 3.0318895369418897e-05
Step: 16540, train/epoch: 3.93622088432312
Step: 16550, train/loss: 0.0
Step: 16550, train/grad_norm: 2.3319898900808766e-05
Step: 16550, train/learning_rate: 3.0306997359730303e-05
Step: 16550, train/epoch: 3.938600778579712
Step: 16560, train/loss: 0.0
Step: 16560, train/grad_norm: 0.00019661943952087313
Step: 16560, train/learning_rate: 3.0295097531052306e-05
Step: 16560, train/epoch: 3.9409804344177246
Step: 16570, train/loss: 9.999999747378752e-05
Step: 16570, train/grad_norm: 0.00032215312239713967
Step: 16570, train/learning_rate: 3.028319770237431e-05
Step: 16570, train/epoch: 3.9433603286743164
Step: 16580, train/loss: 0.0
Step: 16580, train/grad_norm: 0.0003654595639090985
Step: 16580, train/learning_rate: 3.0271299692685716e-05
Step: 16580, train/epoch: 3.945740222930908
Step: 16590, train/loss: 0.0
Step: 16590, train/grad_norm: 0.039056893438100815
Step: 16590, train/learning_rate: 3.025939986400772e-05
Step: 16590, train/epoch: 3.948119878768921
Step: 16600, train/loss: 0.09740000218153
Step: 16600, train/grad_norm: 0.19840918481349945
Step: 16600, train/learning_rate: 3.0247501854319125e-05
Step: 16600, train/epoch: 3.9504997730255127
Step: 16610, train/loss: 0.02449999935925007
Step: 16610, train/grad_norm: 0.0006089072558097541
Step: 16610, train/learning_rate: 3.023560202564113e-05
Step: 16610, train/epoch: 3.9528796672821045
Step: 16620, train/loss: 0.0
Step: 16620, train/grad_norm: 0.0016685976879671216
Step: 16620, train/learning_rate: 3.022370219696313e-05
Step: 16620, train/epoch: 3.955259323120117
Step: 16630, train/loss: 0.0
Step: 16630, train/grad_norm: 0.0039101880975067616
Step: 16630, train/learning_rate: 3.0211804187274538e-05
Step: 16630, train/epoch: 3.957639217376709
Step: 16640, train/loss: 0.0
Step: 16640, train/grad_norm: 6.98378644301556e-05
Step: 16640, train/learning_rate: 3.019990435859654e-05
Step: 16640, train/epoch: 3.960019111633301
Step: 16650, train/loss: 0.0
Step: 16650, train/grad_norm: 0.0017139341216534376
Step: 16650, train/learning_rate: 3.0188006348907948e-05
Step: 16650, train/epoch: 3.9623987674713135
Step: 16660, train/loss: 0.0
Step: 16660, train/grad_norm: 7.478810584871098e-05
Step: 16660, train/learning_rate: 3.017610652022995e-05
Step: 16660, train/epoch: 3.9647786617279053
Step: 16670, train/loss: 0.0
Step: 16670, train/grad_norm: 7.947266567498446e-05
Step: 16670, train/learning_rate: 3.0164206691551954e-05
Step: 16670, train/epoch: 3.967158555984497
Step: 16680, train/loss: 0.0
Step: 16680, train/grad_norm: 1.9125809558317997e-05
Step: 16680, train/learning_rate: 3.015230868186336e-05
Step: 16680, train/epoch: 3.9695382118225098
Step: 16690, train/loss: 0.0
Step: 16690, train/grad_norm: 1.3337024938664399e-05
Step: 16690, train/learning_rate: 3.0140408853185363e-05
Step: 16690, train/epoch: 3.9719181060791016
Step: 16700, train/loss: 0.0
Step: 16700, train/grad_norm: 0.0001579881936777383
Step: 16700, train/learning_rate: 3.012851084349677e-05
Step: 16700, train/epoch: 3.9742980003356934
Step: 16710, train/loss: 0.0
Step: 16710, train/grad_norm: 5.0822007324313745e-05
Step: 16710, train/learning_rate: 3.0116611014818773e-05
Step: 16710, train/epoch: 3.976677656173706
Step: 16720, train/loss: 0.0
Step: 16720, train/grad_norm: 3.926147110178135e-05
Step: 16720, train/learning_rate: 3.0104711186140776e-05
Step: 16720, train/epoch: 3.979057550430298
Step: 16730, train/loss: 0.0
Step: 16730, train/grad_norm: 1.2577468169183703e-06
Step: 16730, train/learning_rate: 3.0092813176452182e-05
Step: 16730, train/epoch: 3.9814374446868896
Step: 16740, train/loss: 0.0
Step: 16740, train/grad_norm: 5.113179781801591e-07
Step: 16740, train/learning_rate: 3.0080913347774185e-05
Step: 16740, train/epoch: 3.9838173389434814
Step: 16750, train/loss: 0.0
Step: 16750, train/grad_norm: 1.223933941219002e-05
Step: 16750, train/learning_rate: 3.0069015338085592e-05
Step: 16750, train/epoch: 3.986196994781494
Step: 16760, train/loss: 0.0
Step: 16760, train/grad_norm: 6.820815542596392e-06
Step: 16760, train/learning_rate: 3.0057115509407595e-05
Step: 16760, train/epoch: 3.988576889038086
Step: 16770, train/loss: 0.0
Step: 16770, train/grad_norm: 9.97968436422525e-06
Step: 16770, train/learning_rate: 3.0045215680729598e-05
Step: 16770, train/epoch: 3.9909567832946777
Step: 16780, train/loss: 0.0
Step: 16780, train/grad_norm: 5.863967089680955e-05
Step: 16780, train/learning_rate: 3.0033317671041004e-05
Step: 16780, train/epoch: 3.9933364391326904
Step: 16790, train/loss: 0.0
Step: 16790, train/grad_norm: 1.4625614994656644e-06
Step: 16790, train/learning_rate: 3.0021417842363007e-05
Step: 16790, train/epoch: 3.9957163333892822
Step: 16800, train/loss: 0.0
Step: 16800, train/grad_norm: 1.2487517778936308e-05
Step: 16800, train/learning_rate: 3.0009519832674414e-05
Step: 16800, train/epoch: 3.998096227645874
Step: 16808, eval/loss: 0.04897090420126915
Step: 16808, eval/accuracy: 0.9944467544555664
Step: 16808, eval/f1: 0.9941284656524658
Step: 16808, eval/runtime: 734.7747192382812
Step: 16808, eval/samples_per_second: 9.803000450134277
Step: 16808, eval/steps_per_second: 1.2259999513626099
Step: 16808, train/epoch: 4.0
Step: 16810, train/loss: 0.0
Step: 16810, train/grad_norm: 6.051292871234182e-07
Step: 16810, train/learning_rate: 2.9997620003996417e-05
Step: 16810, train/epoch: 4.000475883483887
Step: 16820, train/loss: 0.0
Step: 16820, train/grad_norm: 1.0677483714971459e-06
Step: 16820, train/learning_rate: 2.9985721994307823e-05
Step: 16820, train/epoch: 4.0028557777404785
Step: 16830, train/loss: 0.0
Step: 16830, train/grad_norm: 8.540179806004744e-06
Step: 16830, train/learning_rate: 2.9973822165629826e-05
Step: 16830, train/epoch: 4.00523567199707
Step: 16840, train/loss: 0.0
Step: 16840, train/grad_norm: 4.469958003028296e-05
Step: 16840, train/learning_rate: 2.996192233695183e-05
Step: 16840, train/epoch: 4.007615566253662
Step: 16850, train/loss: 0.0
Step: 16850, train/grad_norm: 1.7841774024418555e-05
Step: 16850, train/learning_rate: 2.9950024327263236e-05
Step: 16850, train/epoch: 4.009995460510254
Step: 16860, train/loss: 0.0
Step: 16860, train/grad_norm: 8.792010339675471e-05
Step: 16860, train/learning_rate: 2.993812449858524e-05
Step: 16860, train/epoch: 4.0123748779296875
Step: 16870, train/loss: 0.0
Step: 16870, train/grad_norm: 6.096016022638651e-06
Step: 16870, train/learning_rate: 2.9926226488896646e-05
Step: 16870, train/epoch: 4.014754772186279
Step: 16880, train/loss: 0.0
Step: 16880, train/grad_norm: 1.2273356333025731e-05
Step: 16880, train/learning_rate: 2.991432666021865e-05
Step: 16880, train/epoch: 4.017134666442871
Step: 16890, train/loss: 0.0
Step: 16890, train/grad_norm: 1.4145603017823305e-06
Step: 16890, train/learning_rate: 2.990242683154065e-05
Step: 16890, train/epoch: 4.019514560699463
Step: 16900, train/loss: 0.0
Step: 16900, train/grad_norm: 5.787987333860656e-07
Step: 16900, train/learning_rate: 2.9890528821852058e-05
Step: 16900, train/epoch: 4.021894454956055
Step: 16910, train/loss: 0.0
Step: 16910, train/grad_norm: 5.670783139066771e-06
Step: 16910, train/learning_rate: 2.987862899317406e-05
Step: 16910, train/epoch: 4.0242743492126465
Step: 16920, train/loss: 0.0
Step: 16920, train/grad_norm: 2.815018524415791e-05
Step: 16920, train/learning_rate: 2.9866730983485468e-05
Step: 16920, train/epoch: 4.02665376663208
Step: 16930, train/loss: 0.0
Step: 16930, train/grad_norm: 5.55671431357041e-06
Step: 16930, train/learning_rate: 2.985483115480747e-05
Step: 16930, train/epoch: 4.029033660888672
Step: 16940, train/loss: 0.0
Step: 16940, train/grad_norm: 6.73575550536043e-06
Step: 16940, train/learning_rate: 2.9842931326129474e-05
Step: 16940, train/epoch: 4.031413555145264
Step: 16950, train/loss: 0.0
Step: 16950, train/grad_norm: 1.6810196029837243e-05
Step: 16950, train/learning_rate: 2.983103331644088e-05
Step: 16950, train/epoch: 4.0337934494018555
Step: 16960, train/loss: 0.0
Step: 16960, train/grad_norm: 4.32278829975985e-05
Step: 16960, train/learning_rate: 2.9819133487762883e-05
Step: 16960, train/epoch: 4.036173343658447
Step: 16970, train/loss: 0.0
Step: 16970, train/grad_norm: 7.931057552923448e-06
Step: 16970, train/learning_rate: 2.980723547807429e-05
Step: 16970, train/epoch: 4.038553237915039
Step: 16980, train/loss: 0.0
Step: 16980, train/grad_norm: 4.2317795305280015e-05
Step: 16980, train/learning_rate: 2.9795335649396293e-05
Step: 16980, train/epoch: 4.040932655334473
Step: 16990, train/loss: 0.0
Step: 16990, train/grad_norm: 3.4890160804934567e-06
Step: 16990, train/learning_rate: 2.9783435820718296e-05
Step: 16990, train/epoch: 4.0433125495910645
Step: 17000, train/loss: 0.0
Step: 17000, train/grad_norm: 3.01415497006019e-07
Step: 17000, train/learning_rate: 2.9771537811029702e-05
Step: 17000, train/epoch: 4.045692443847656
Step: 17010, train/loss: 0.0
Step: 17010, train/grad_norm: 4.326462294557132e-05
Step: 17010, train/learning_rate: 2.9759637982351705e-05
Step: 17010, train/epoch: 4.048072338104248
Step: 17020, train/loss: 0.0
Step: 17020, train/grad_norm: 9.767439041752368e-05
Step: 17020, train/learning_rate: 2.9747739972663112e-05
Step: 17020, train/epoch: 4.05045223236084
Step: 17030, train/loss: 0.0
Step: 17030, train/grad_norm: 6.568923708982766e-05
Step: 17030, train/learning_rate: 2.9735840143985115e-05
Step: 17030, train/epoch: 4.052832126617432
Step: 17040, train/loss: 0.0
Step: 17040, train/grad_norm: 1.6346561096725054e-05
Step: 17040, train/learning_rate: 2.9723940315307118e-05
Step: 17040, train/epoch: 4.055212020874023
Step: 17050, train/loss: 0.0
Step: 17050, train/grad_norm: 3.554218346835114e-05
Step: 17050, train/learning_rate: 2.9712042305618525e-05
Step: 17050, train/epoch: 4.057591438293457
Step: 17060, train/loss: 0.0
Step: 17060, train/grad_norm: 1.0819303497555666e-05
Step: 17060, train/learning_rate: 2.9700142476940528e-05
Step: 17060, train/epoch: 4.059971332550049
Step: 17070, train/loss: 0.0
Step: 17070, train/grad_norm: 7.99517295035912e-07
Step: 17070, train/learning_rate: 2.9688244467251934e-05
Step: 17070, train/epoch: 4.062351226806641
Step: 17080, train/loss: 0.0
Step: 17080, train/grad_norm: 5.47963736607926e-06
Step: 17080, train/learning_rate: 2.9676344638573937e-05
Step: 17080, train/epoch: 4.064731121063232
Step: 17090, train/loss: 0.0
Step: 17090, train/grad_norm: 3.4984370813617716e-06
Step: 17090, train/learning_rate: 2.966444480989594e-05
Step: 17090, train/epoch: 4.067111015319824
Step: 17100, train/loss: 0.0
Step: 17100, train/grad_norm: 2.957192691610544e-06
Step: 17100, train/learning_rate: 2.9652546800207347e-05
Step: 17100, train/epoch: 4.069490909576416
Step: 17110, train/loss: 0.0
Step: 17110, train/grad_norm: 0.0059277331456542015
Step: 17110, train/learning_rate: 2.964064697152935e-05
Step: 17110, train/epoch: 4.07187032699585
Step: 17120, train/loss: 0.0
Step: 17120, train/grad_norm: 3.674172830869793e-06
Step: 17120, train/learning_rate: 2.9628748961840756e-05
Step: 17120, train/epoch: 4.074250221252441
Step: 17130, train/loss: 0.0
Step: 17130, train/grad_norm: 4.775190245709382e-05
Step: 17130, train/learning_rate: 2.961684913316276e-05
Step: 17130, train/epoch: 4.076630115509033
Step: 17140, train/loss: 9.999999747378752e-05
Step: 17140, train/grad_norm: 0.48767006397247314
Step: 17140, train/learning_rate: 2.9604949304484762e-05
Step: 17140, train/epoch: 4.079010009765625
Step: 17150, train/loss: 0.0
Step: 17150, train/grad_norm: 4.1421324681323313e-07
Step: 17150, train/learning_rate: 2.959305129479617e-05
Step: 17150, train/epoch: 4.081389904022217
Step: 17160, train/loss: 0.0
Step: 17160, train/grad_norm: 9.558673852438915e-09
Step: 17160, train/learning_rate: 2.9581151466118172e-05
Step: 17160, train/epoch: 4.083769798278809
Step: 17170, train/loss: 0.0
Step: 17170, train/grad_norm: 3.4465956755269644e-09
Step: 17170, train/learning_rate: 2.956925345642958e-05
Step: 17170, train/epoch: 4.086149215698242
Step: 17180, train/loss: 0.014399999752640724
Step: 17180, train/grad_norm: 5.463355591928121e-06
Step: 17180, train/learning_rate: 2.955735362775158e-05
Step: 17180, train/epoch: 4.088529109954834
Step: 17190, train/loss: 0.0
Step: 17190, train/grad_norm: 4.044670376401882e-09
Step: 17190, train/learning_rate: 2.9545453799073584e-05
Step: 17190, train/epoch: 4.090909004211426
Step: 17200, train/loss: 0.0
Step: 17200, train/grad_norm: 2.7143983970745467e-05
Step: 17200, train/learning_rate: 2.953355578938499e-05
Step: 17200, train/epoch: 4.093288898468018
Step: 17210, train/loss: 0.0
Step: 17210, train/grad_norm: 1.9165580233959645e-09
Step: 17210, train/learning_rate: 2.9521655960706994e-05
Step: 17210, train/epoch: 4.095668792724609
Step: 17220, train/loss: 0.0
Step: 17220, train/grad_norm: 1.5821105536772961e-09
Step: 17220, train/learning_rate: 2.95097579510184e-05
Step: 17220, train/epoch: 4.098048686981201
Step: 17230, train/loss: 0.0
Step: 17230, train/grad_norm: 6.826940679438565e-10
Step: 17230, train/learning_rate: 2.9497858122340403e-05
Step: 17230, train/epoch: 4.100428581237793
Step: 17240, train/loss: 0.17970000207424164
Step: 17240, train/grad_norm: 6.650882369285682e-06
Step: 17240, train/learning_rate: 2.9485958293662407e-05
Step: 17240, train/epoch: 4.102807998657227
Step: 17250, train/loss: 0.0
Step: 17250, train/grad_norm: 1.5080712728376966e-05
Step: 17250, train/learning_rate: 2.9474060283973813e-05
Step: 17250, train/epoch: 4.105187892913818
Step: 17260, train/loss: 9.999999747378752e-05
Step: 17260, train/grad_norm: 3.1468212000618223e-06
Step: 17260, train/learning_rate: 2.9462160455295816e-05
Step: 17260, train/epoch: 4.10756778717041
Step: 17270, train/loss: 0.0
Step: 17270, train/grad_norm: 5.370185931496962e-07
Step: 17270, train/learning_rate: 2.9450262445607223e-05
Step: 17270, train/epoch: 4.109947681427002
Step: 17280, train/loss: 0.0
Step: 17280, train/grad_norm: 3.2217195666817133e-07
Step: 17280, train/learning_rate: 2.9438362616929226e-05
Step: 17280, train/epoch: 4.112327575683594
Step: 17290, train/loss: 0.0
Step: 17290, train/grad_norm: 3.2045434636529535e-05
Step: 17290, train/learning_rate: 2.942646278825123e-05
Step: 17290, train/epoch: 4.1147074699401855
Step: 17300, train/loss: 0.0
Step: 17300, train/grad_norm: 5.48372099729022e-07
Step: 17300, train/learning_rate: 2.9414564778562635e-05
Step: 17300, train/epoch: 4.117086887359619
Step: 17310, train/loss: 0.0
Step: 17310, train/grad_norm: 3.1754620977153536e-06
Step: 17310, train/learning_rate: 2.9402664949884638e-05
Step: 17310, train/epoch: 4.119466781616211
Step: 17320, train/loss: 0.0
Step: 17320, train/grad_norm: 2.679524868653971e-07
Step: 17320, train/learning_rate: 2.9390766940196045e-05
Step: 17320, train/epoch: 4.121846675872803
Step: 17330, train/loss: 0.0
Step: 17330, train/grad_norm: 3.2889795420487644e-06
Step: 17330, train/learning_rate: 2.9378867111518048e-05
Step: 17330, train/epoch: 4.1242265701293945
Step: 17340, train/loss: 0.0
Step: 17340, train/grad_norm: 2.108231598185739e-07
Step: 17340, train/learning_rate: 2.936696728284005e-05
Step: 17340, train/epoch: 4.126606464385986
Step: 17350, train/loss: 0.0
Step: 17350, train/grad_norm: 7.625388661836041e-06
Step: 17350, train/learning_rate: 2.9355069273151457e-05
Step: 17350, train/epoch: 4.128986358642578
Step: 17360, train/loss: 0.0
Step: 17360, train/grad_norm: 5.000584337722103e-07
Step: 17360, train/learning_rate: 2.934316944447346e-05
Step: 17360, train/epoch: 4.13136625289917
Step: 17370, train/loss: 0.0
Step: 17370, train/grad_norm: 3.464309372702701e-08
Step: 17370, train/learning_rate: 2.9331271434784867e-05
Step: 17370, train/epoch: 4.1337456703186035
Step: 17380, train/loss: 0.0
Step: 17380, train/grad_norm: 4.808564995073539e-07
Step: 17380, train/learning_rate: 2.931937160610687e-05
Step: 17380, train/epoch: 4.136125564575195
Step: 17390, train/loss: 0.0
Step: 17390, train/grad_norm: 5.660652732331073e-06
Step: 17390, train/learning_rate: 2.9307471777428873e-05
Step: 17390, train/epoch: 4.138505458831787
Step: 17400, train/loss: 0.0
Step: 17400, train/grad_norm: 0.0003647335688583553
Step: 17400, train/learning_rate: 2.929557376774028e-05
Step: 17400, train/epoch: 4.140885353088379
Step: 17410, train/loss: 0.0
Step: 17410, train/grad_norm: 3.601095386329689e-06
Step: 17410, train/learning_rate: 2.9283673939062282e-05
Step: 17410, train/epoch: 4.143265247344971
Step: 17420, train/loss: 0.0
Step: 17420, train/grad_norm: 4.579324013320729e-07
Step: 17420, train/learning_rate: 2.927177592937369e-05
Step: 17420, train/epoch: 4.1456451416015625
Step: 17430, train/loss: 0.0
Step: 17430, train/grad_norm: 1.6089686596387764e-06
Step: 17430, train/learning_rate: 2.9259876100695692e-05
Step: 17430, train/epoch: 4.148024559020996
Step: 17440, train/loss: 0.23559999465942383
Step: 17440, train/grad_norm: 0.00010746567568276078
Step: 17440, train/learning_rate: 2.9247976272017695e-05
Step: 17440, train/epoch: 4.150404453277588
Step: 17450, train/loss: 0.0
Step: 17450, train/grad_norm: 3.874828053085366e-06
Step: 17450, train/learning_rate: 2.92360782623291e-05
Step: 17450, train/epoch: 4.15278434753418
Step: 17460, train/loss: 0.0
Step: 17460, train/grad_norm: 5.331065767677501e-05
Step: 17460, train/learning_rate: 2.9224178433651105e-05
Step: 17460, train/epoch: 4.1551642417907715
Step: 17470, train/loss: 0.0
Step: 17470, train/grad_norm: 8.27720359666273e-05
Step: 17470, train/learning_rate: 2.921228042396251e-05
Step: 17470, train/epoch: 4.157544136047363
Step: 17480, train/loss: 0.0
Step: 17480, train/grad_norm: 7.638333772774786e-05
Step: 17480, train/learning_rate: 2.9200380595284514e-05
Step: 17480, train/epoch: 4.159924030303955
Step: 17490, train/loss: 0.0
Step: 17490, train/grad_norm: 7.215071946120588e-06
Step: 17490, train/learning_rate: 2.9188480766606517e-05
Step: 17490, train/epoch: 4.162303447723389
Step: 17500, train/loss: 0.0
Step: 17500, train/grad_norm: 0.00017313040734734386
Step: 17500, train/learning_rate: 2.9176582756917924e-05
Step: 17500, train/epoch: 4.1646833419799805
Step: 17510, train/loss: 0.0
Step: 17510, train/grad_norm: 1.0164864761463832e-05
Step: 17510, train/learning_rate: 2.9164682928239927e-05
Step: 17510, train/epoch: 4.167063236236572
Step: 17520, train/loss: 0.0
Step: 17520, train/grad_norm: 4.687352884502616e-06
Step: 17520, train/learning_rate: 2.9152784918551333e-05
Step: 17520, train/epoch: 4.169443130493164
Step: 17530, train/loss: 0.0
Step: 17530, train/grad_norm: 0.00012973778939340264
Step: 17530, train/learning_rate: 2.9140885089873336e-05
Step: 17530, train/epoch: 4.171823024749756
Step: 17540, train/loss: 0.0
Step: 17540, train/grad_norm: 4.8643487389199436e-05
Step: 17540, train/learning_rate: 2.9128987080184743e-05
Step: 17540, train/epoch: 4.174202919006348
Step: 17550, train/loss: 0.0
Step: 17550, train/grad_norm: 3.3646958854660625e-06
Step: 17550, train/learning_rate: 2.9117087251506746e-05
Step: 17550, train/epoch: 4.1765828132629395
Step: 17560, train/loss: 0.0
Step: 17560, train/grad_norm: 5.240828249952756e-05
Step: 17560, train/learning_rate: 2.910518742282875e-05
Step: 17560, train/epoch: 4.178962230682373
Step: 17570, train/loss: 0.0
Step: 17570, train/grad_norm: 4.106921551283449e-06
Step: 17570, train/learning_rate: 2.9093289413140155e-05
Step: 17570, train/epoch: 4.181342124938965
Step: 17580, train/loss: 0.0
Step: 17580, train/grad_norm: 0.0007912568980827928
Step: 17580, train/learning_rate: 2.908138958446216e-05
Step: 17580, train/epoch: 4.183722019195557
Step: 17590, train/loss: 0.0003000000142492354
Step: 17590, train/grad_norm: 5.3243144066073e-05
Step: 17590, train/learning_rate: 2.9069491574773565e-05
Step: 17590, train/epoch: 4.186101913452148
Step: 17600, train/loss: 0.16130000352859497
Step: 17600, train/grad_norm: 287.1019287109375
Step: 17600, train/learning_rate: 2.9057591746095568e-05
Step: 17600, train/epoch: 4.18848180770874
Step: 17610, train/loss: 0.0003000000142492354
Step: 17610, train/grad_norm: 0.0002879707608371973
Step: 17610, train/learning_rate: 2.904569191741757e-05
Step: 17610, train/epoch: 4.190861701965332
Step: 17620, train/loss: 0.12269999831914902
Step: 17620, train/grad_norm: 0.0002403801481705159
Step: 17620, train/learning_rate: 2.9033793907728978e-05
Step: 17620, train/epoch: 4.193241119384766
Step: 17630, train/loss: 0.0
Step: 17630, train/grad_norm: 2.971712456201203e-05
Step: 17630, train/learning_rate: 2.902189407905098e-05
Step: 17630, train/epoch: 4.195621013641357
Step: 17640, train/loss: 0.0
Step: 17640, train/grad_norm: 0.0003089410893153399
Step: 17640, train/learning_rate: 2.9009996069362387e-05
Step: 17640, train/epoch: 4.198000907897949
Step: 17650, train/loss: 0.0
Step: 17650, train/grad_norm: 4.373447518446483e-05
Step: 17650, train/learning_rate: 2.899809624068439e-05
Step: 17650, train/epoch: 4.200380802154541
Step: 17660, train/loss: 0.0
Step: 17660, train/grad_norm: 3.9417729567503557e-05
Step: 17660, train/learning_rate: 2.8986196412006393e-05
Step: 17660, train/epoch: 4.202760696411133
Step: 17670, train/loss: 0.0
Step: 17670, train/grad_norm: 2.7258389309281483e-05
Step: 17670, train/learning_rate: 2.89742984023178e-05
Step: 17670, train/epoch: 4.205140590667725
Step: 17680, train/loss: 0.0
Step: 17680, train/grad_norm: 8.0565614553052e-06
Step: 17680, train/learning_rate: 2.8962398573639803e-05
Step: 17680, train/epoch: 4.207520008087158
Step: 17690, train/loss: 0.0
Step: 17690, train/grad_norm: 0.0006403748993761837
Step: 17690, train/learning_rate: 2.895050056395121e-05
Step: 17690, train/epoch: 4.20989990234375
Step: 17700, train/loss: 0.09290000051259995
Step: 17700, train/grad_norm: 0.00598652521148324
Step: 17700, train/learning_rate: 2.8938600735273212e-05
Step: 17700, train/epoch: 4.212279796600342
Step: 17710, train/loss: 0.0
Step: 17710, train/grad_norm: 0.0009348368621431291
Step: 17710, train/learning_rate: 2.8926700906595215e-05
Step: 17710, train/epoch: 4.214659690856934
Step: 17720, train/loss: 0.0
Step: 17720, train/grad_norm: 0.008092077448964119
Step: 17720, train/learning_rate: 2.8914802896906622e-05
Step: 17720, train/epoch: 4.217039585113525
Step: 17730, train/loss: 0.0
Step: 17730, train/grad_norm: 3.336422741995193e-05
Step: 17730, train/learning_rate: 2.8902903068228625e-05
Step: 17730, train/epoch: 4.219419479370117
Step: 17740, train/loss: 0.0
Step: 17740, train/grad_norm: 5.1382525271037593e-05
Step: 17740, train/learning_rate: 2.889100505854003e-05
Step: 17740, train/epoch: 4.221799373626709
Step: 17750, train/loss: 0.00860000029206276
Step: 17750, train/grad_norm: 0.0009154611616395414
Step: 17750, train/learning_rate: 2.8879105229862034e-05
Step: 17750, train/epoch: 4.224178791046143
Step: 17760, train/loss: 9.999999747378752e-05
Step: 17760, train/grad_norm: 3.0238274121074937e-05
Step: 17760, train/learning_rate: 2.8867205401184037e-05
Step: 17760, train/epoch: 4.226558685302734
Step: 17770, train/loss: 0.0
Step: 17770, train/grad_norm: 3.31889750668779e-05
Step: 17770, train/learning_rate: 2.8855307391495444e-05
Step: 17770, train/epoch: 4.228938579559326
Step: 17780, train/loss: 0.17749999463558197
Step: 17780, train/grad_norm: 0.07208152115345001
Step: 17780, train/learning_rate: 2.8843407562817447e-05
Step: 17780, train/epoch: 4.231318473815918
Step: 17790, train/loss: 9.999999747378752e-05
Step: 17790, train/grad_norm: 7.106178236426786e-05
Step: 17790, train/learning_rate: 2.8831509553128853e-05
Step: 17790, train/epoch: 4.23369836807251
Step: 17800, train/loss: 0.0
Step: 17800, train/grad_norm: 4.031057443398822e-08
Step: 17800, train/learning_rate: 2.8819609724450856e-05
Step: 17800, train/epoch: 4.236078262329102
Step: 17810, train/loss: 0.2304999977350235
Step: 17810, train/grad_norm: 8.584082934248727e-06
Step: 17810, train/learning_rate: 2.880770989577286e-05
Step: 17810, train/epoch: 4.238457679748535
Step: 17820, train/loss: 0.04129999876022339
Step: 17820, train/grad_norm: 4.562436206612119e-09
Step: 17820, train/learning_rate: 2.8795811886084266e-05
Step: 17820, train/epoch: 4.240837574005127
Step: 17830, train/loss: 0.05730000138282776
Step: 17830, train/grad_norm: 0.00021999831369612366
Step: 17830, train/learning_rate: 2.878391205740627e-05
Step: 17830, train/epoch: 4.243217468261719
Step: 17840, train/loss: 9.999999747378752e-05
Step: 17840, train/grad_norm: 1.1597036291277618e-06
Step: 17840, train/learning_rate: 2.8772014047717676e-05
Step: 17840, train/epoch: 4.2455973625183105
Step: 17850, train/loss: 0.0
Step: 17850, train/grad_norm: 3.5871048353897095e-09
Step: 17850, train/learning_rate: 2.876011421903968e-05
Step: 17850, train/epoch: 4.247977256774902
Step: 17860, train/loss: 0.0
Step: 17860, train/grad_norm: 2.999503090350686e-09
Step: 17860, train/learning_rate: 2.874821439036168e-05
Step: 17860, train/epoch: 4.250357151031494
Step: 17870, train/loss: 0.0
Step: 17870, train/grad_norm: 3.169345896836262e-10
Step: 17870, train/learning_rate: 2.8736316380673088e-05
Step: 17870, train/epoch: 4.252736568450928
Step: 17880, train/loss: 0.0
Step: 17880, train/grad_norm: 3.232918288631481e-07
Step: 17880, train/learning_rate: 2.872441655199509e-05
Step: 17880, train/epoch: 4.2551164627075195
Step: 17890, train/loss: 0.0
Step: 17890, train/grad_norm: 3.9894697323461514e-08
Step: 17890, train/learning_rate: 2.8712518542306498e-05
Step: 17890, train/epoch: 4.257496356964111
Step: 17900, train/loss: 0.0
Step: 17900, train/grad_norm: 2.570549328950733e-09
Step: 17900, train/learning_rate: 2.87006187136285e-05
Step: 17900, train/epoch: 4.259876251220703
Step: 17910, train/loss: 0.0
Step: 17910, train/grad_norm: 4.1094614289249876e-07
Step: 17910, train/learning_rate: 2.8688718884950504e-05
Step: 17910, train/epoch: 4.262256145477295
Step: 17920, train/loss: 0.0
Step: 17920, train/grad_norm: 2.7221506115893135e-07
Step: 17920, train/learning_rate: 2.867682087526191e-05
Step: 17920, train/epoch: 4.264636039733887
Step: 17930, train/loss: 0.0
Step: 17930, train/grad_norm: 8.14564984352728e-08
Step: 17930, train/learning_rate: 2.8664921046583913e-05
Step: 17930, train/epoch: 4.2670159339904785
Step: 17940, train/loss: 0.0
Step: 17940, train/grad_norm: 1.4640007262034516e-10
Step: 17940, train/learning_rate: 2.865302303689532e-05
Step: 17940, train/epoch: 4.269395351409912
Step: 17950, train/loss: 0.0
Step: 17950, train/grad_norm: 2.4306487933500875e-09
Step: 17950, train/learning_rate: 2.8641123208217323e-05
Step: 17950, train/epoch: 4.271775245666504
Step: 17960, train/loss: 0.0
Step: 17960, train/grad_norm: 0.009808325208723545
Step: 17960, train/learning_rate: 2.8629223379539326e-05
Step: 17960, train/epoch: 4.274155139923096
Step: 17970, train/loss: 0.0
Step: 17970, train/grad_norm: 5.515993772853278e-10
Step: 17970, train/learning_rate: 2.8617325369850732e-05
Step: 17970, train/epoch: 4.2765350341796875
Step: 17980, train/loss: 0.0
Step: 17980, train/grad_norm: 2.61103139109764e-07
Step: 17980, train/learning_rate: 2.8605425541172735e-05
Step: 17980, train/epoch: 4.278914928436279
Step: 17990, train/loss: 0.0
Step: 17990, train/grad_norm: 3.119937919127125e-10
Step: 17990, train/learning_rate: 2.8593527531484142e-05
Step: 17990, train/epoch: 4.281294822692871
Step: 18000, train/loss: 0.0
Step: 18000, train/grad_norm: 5.104503486563772e-08
Step: 18000, train/learning_rate: 2.8581627702806145e-05
Step: 18000, train/epoch: 4.283674240112305
Step: 18010, train/loss: 0.0
Step: 18010, train/grad_norm: 6.0215497796889395e-05
Step: 18010, train/learning_rate: 2.8569727874128148e-05
Step: 18010, train/epoch: 4.2860541343688965
Step: 18020, train/loss: 0.0
Step: 18020, train/grad_norm: 1.0986923593137021e-09
Step: 18020, train/learning_rate: 2.8557829864439555e-05
Step: 18020, train/epoch: 4.288434028625488
Step: 18030, train/loss: 0.0
Step: 18030, train/grad_norm: 0.0003466519992798567
Step: 18030, train/learning_rate: 2.8545930035761558e-05
Step: 18030, train/epoch: 4.29081392288208
Step: 18040, train/loss: 0.0
Step: 18040, train/grad_norm: 1.0406739470170123e-08
Step: 18040, train/learning_rate: 2.8534032026072964e-05
Step: 18040, train/epoch: 4.293193817138672
Step: 18050, train/loss: 0.0
Step: 18050, train/grad_norm: 4.627522400824091e-07
Step: 18050, train/learning_rate: 2.8522132197394967e-05
Step: 18050, train/epoch: 4.295573711395264
Step: 18060, train/loss: 0.0
Step: 18060, train/grad_norm: 2.02538918969708e-09
Step: 18060, train/learning_rate: 2.851023236871697e-05
Step: 18060, train/epoch: 4.297953128814697
Step: 18070, train/loss: 0.0
Step: 18070, train/grad_norm: 1.9667713291315891e-10
Step: 18070, train/learning_rate: 2.8498334359028377e-05
Step: 18070, train/epoch: 4.300333023071289
Step: 18080, train/loss: 0.0
Step: 18080, train/grad_norm: 1.7034037114171952e-07
Step: 18080, train/learning_rate: 2.848643453035038e-05
Step: 18080, train/epoch: 4.302712917327881
Step: 18090, train/loss: 0.0
Step: 18090, train/grad_norm: 1.846687069928521e-08
Step: 18090, train/learning_rate: 2.8474536520661786e-05
Step: 18090, train/epoch: 4.305092811584473
Step: 18100, train/loss: 0.0
Step: 18100, train/grad_norm: 3.602302900418408e-10
Step: 18100, train/learning_rate: 2.846263669198379e-05
Step: 18100, train/epoch: 4.3074727058410645
Step: 18110, train/loss: 0.0
Step: 18110, train/grad_norm: 4.3914045733117746e-09
Step: 18110, train/learning_rate: 2.8450736863305792e-05
Step: 18110, train/epoch: 4.309852600097656
Step: 18120, train/loss: 0.0
Step: 18120, train/grad_norm: 2.2275798983173445e-08
Step: 18120, train/learning_rate: 2.84388388536172e-05
Step: 18120, train/epoch: 4.312232494354248
Step: 18130, train/loss: 0.06289999932050705
Step: 18130, train/grad_norm: 1.1664748056361418e-09
Step: 18130, train/learning_rate: 2.8426939024939202e-05
Step: 18130, train/epoch: 4.314611911773682
Step: 18140, train/loss: 0.0
Step: 18140, train/grad_norm: 9.368278597321478e-09
Step: 18140, train/learning_rate: 2.841504101525061e-05
Step: 18140, train/epoch: 4.316991806030273
Step: 18150, train/loss: 0.0
Step: 18150, train/grad_norm: 6.839608046593781e-11
Step: 18150, train/learning_rate: 2.840314118657261e-05
Step: 18150, train/epoch: 4.319371700286865
Step: 18160, train/loss: 0.00019999999494757503
Step: 18160, train/grad_norm: 2.2596405813146703e-07
Step: 18160, train/learning_rate: 2.8391241357894614e-05
Step: 18160, train/epoch: 4.321751594543457
Step: 18170, train/loss: 0.0
Step: 18170, train/grad_norm: 4.982249279805728e-09
Step: 18170, train/learning_rate: 2.837934334820602e-05
Step: 18170, train/epoch: 4.324131488800049
Step: 18180, train/loss: 0.0
Step: 18180, train/grad_norm: 1.753683170901965e-10
Step: 18180, train/learning_rate: 2.8367443519528024e-05
Step: 18180, train/epoch: 4.326511383056641
Step: 18190, train/loss: 0.0
Step: 18190, train/grad_norm: 1.3052979649843394e-13
Step: 18190, train/learning_rate: 2.835554550983943e-05
Step: 18190, train/epoch: 4.328890800476074
Step: 18200, train/loss: 0.24690000712871552
Step: 18200, train/grad_norm: 4.378762241685763e-07
Step: 18200, train/learning_rate: 2.8343645681161433e-05
Step: 18200, train/epoch: 4.331270694732666
Step: 18210, train/loss: 0.0
Step: 18210, train/grad_norm: 0.0004505740071181208
Step: 18210, train/learning_rate: 2.833174767147284e-05
Step: 18210, train/epoch: 4.333650588989258
Step: 18220, train/loss: 0.0
Step: 18220, train/grad_norm: 0.07735997438430786
Step: 18220, train/learning_rate: 2.8319847842794843e-05
Step: 18220, train/epoch: 4.33603048324585
Step: 18230, train/loss: 0.0
Step: 18230, train/grad_norm: 0.1101444661617279
Step: 18230, train/learning_rate: 2.8307948014116846e-05
Step: 18230, train/epoch: 4.338410377502441
Step: 18240, train/loss: 0.0
Step: 18240, train/grad_norm: 8.272024570032954e-05
Step: 18240, train/learning_rate: 2.8296050004428253e-05
Step: 18240, train/epoch: 4.340790271759033
Step: 18250, train/loss: 0.0
Step: 18250, train/grad_norm: 3.6847479350399226e-05
Step: 18250, train/learning_rate: 2.8284150175750256e-05
Step: 18250, train/epoch: 4.343169689178467
Step: 18260, train/loss: 0.07190000265836716
Step: 18260, train/grad_norm: 8.016862011572812e-06
Step: 18260, train/learning_rate: 2.8272252166061662e-05
Step: 18260, train/epoch: 4.345549583435059
Step: 18270, train/loss: 0.0
Step: 18270, train/grad_norm: 1.9735846308321925e-06
Step: 18270, train/learning_rate: 2.8260352337383665e-05
Step: 18270, train/epoch: 4.34792947769165
Step: 18280, train/loss: 0.05739999935030937
Step: 18280, train/grad_norm: 8.44873211462982e-05
Step: 18280, train/learning_rate: 2.8248452508705668e-05
Step: 18280, train/epoch: 4.350309371948242
Step: 18290, train/loss: 0.0
Step: 18290, train/grad_norm: 0.00013243047578725964
Step: 18290, train/learning_rate: 2.8236554499017075e-05
Step: 18290, train/epoch: 4.352689266204834
Step: 18300, train/loss: 0.03280000016093254
Step: 18300, train/grad_norm: 0.07731910794973373
Step: 18300, train/learning_rate: 2.8224654670339078e-05
Step: 18300, train/epoch: 4.355069160461426
Step: 18310, train/loss: 9.999999747378752e-05
Step: 18310, train/grad_norm: 6.932327232789248e-05
Step: 18310, train/learning_rate: 2.8212756660650484e-05
Step: 18310, train/epoch: 4.357449054718018
Step: 18320, train/loss: 0.0
Step: 18320, train/grad_norm: 0.0007621470140293241
Step: 18320, train/learning_rate: 2.8200856831972487e-05
Step: 18320, train/epoch: 4.359828472137451
Step: 18330, train/loss: 0.0
Step: 18330, train/grad_norm: 0.00018906993500422686
Step: 18330, train/learning_rate: 2.818895700329449e-05
Step: 18330, train/epoch: 4.362208366394043
Step: 18340, train/loss: 0.0
Step: 18340, train/grad_norm: 8.500538388034329e-05
Step: 18340, train/learning_rate: 2.8177058993605897e-05
Step: 18340, train/epoch: 4.364588260650635
Step: 18350, train/loss: 0.0
Step: 18350, train/grad_norm: 0.0007692311774007976
Step: 18350, train/learning_rate: 2.81651591649279e-05
Step: 18350, train/epoch: 4.366968154907227
Step: 18360, train/loss: 0.0
Step: 18360, train/grad_norm: 0.0008232488762587309
Step: 18360, train/learning_rate: 2.8153261155239306e-05
Step: 18360, train/epoch: 4.369348049163818
Step: 18370, train/loss: 0.06560000032186508
Step: 18370, train/grad_norm: 4.131334208068438e-05
Step: 18370, train/learning_rate: 2.814136132656131e-05
Step: 18370, train/epoch: 4.37172794342041
Step: 18380, train/loss: 0.0
Step: 18380, train/grad_norm: 1.2388470167934429e-05
Step: 18380, train/learning_rate: 2.8129461497883312e-05
Step: 18380, train/epoch: 4.374107360839844
Step: 18390, train/loss: 0.05429999902844429
Step: 18390, train/grad_norm: 7.785381967551075e-06
Step: 18390, train/learning_rate: 2.811756348819472e-05
Step: 18390, train/epoch: 4.3764872550964355
Step: 18400, train/loss: 0.0
Step: 18400, train/grad_norm: 0.00016875793517101556
Step: 18400, train/learning_rate: 2.8105663659516722e-05
Step: 18400, train/epoch: 4.378867149353027
Step: 18410, train/loss: 0.0
Step: 18410, train/grad_norm: 0.00018301879754289985
Step: 18410, train/learning_rate: 2.809376564982813e-05
Step: 18410, train/epoch: 4.381247043609619
Step: 18420, train/loss: 0.0
Step: 18420, train/grad_norm: 1.558213625685312e-05
Step: 18420, train/learning_rate: 2.808186582115013e-05
Step: 18420, train/epoch: 4.383626937866211
Step: 18430, train/loss: 0.0
Step: 18430, train/grad_norm: 5.052062988397665e-05
Step: 18430, train/learning_rate: 2.8069965992472135e-05
Step: 18430, train/epoch: 4.386006832122803
Step: 18440, train/loss: 0.0
Step: 18440, train/grad_norm: 1.2374041034490801e-05
Step: 18440, train/learning_rate: 2.805806798278354e-05
Step: 18440, train/epoch: 4.388386249542236
Step: 18450, train/loss: 0.0
Step: 18450, train/grad_norm: 0.0003029881336260587
Step: 18450, train/learning_rate: 2.8046168154105544e-05
Step: 18450, train/epoch: 4.390766143798828
Step: 18460, train/loss: 0.0
Step: 18460, train/grad_norm: 4.1892275476129726e-05
Step: 18460, train/learning_rate: 2.803427014441695e-05
Step: 18460, train/epoch: 4.39314603805542
Step: 18470, train/loss: 0.0
Step: 18470, train/grad_norm: 2.029805000347551e-05
Step: 18470, train/learning_rate: 2.8022370315738954e-05
Step: 18470, train/epoch: 4.395525932312012
Step: 18480, train/loss: 0.0010000000474974513
Step: 18480, train/grad_norm: 29.328224182128906
Step: 18480, train/learning_rate: 2.8010470487060957e-05
Step: 18480, train/epoch: 4.3979058265686035
Step: 18490, train/loss: 0.16169999539852142
Step: 18490, train/grad_norm: 0.00019408657681196928
Step: 18490, train/learning_rate: 2.7998572477372363e-05
Step: 18490, train/epoch: 4.400285720825195
Step: 18500, train/loss: 0.0
Step: 18500, train/grad_norm: 0.00026713116676546633
Step: 18500, train/learning_rate: 2.7986672648694366e-05
Step: 18500, train/epoch: 4.402665615081787
Step: 18510, train/loss: 0.00019999999494757503
Step: 18510, train/grad_norm: 5.224882988841273e-05
Step: 18510, train/learning_rate: 2.7974774639005773e-05
Step: 18510, train/epoch: 4.405045032501221
Step: 18520, train/loss: 0.00019999999494757503
Step: 18520, train/grad_norm: 3.4696156944846734e-05
Step: 18520, train/learning_rate: 2.7962874810327776e-05
Step: 18520, train/epoch: 4.4074249267578125
Step: 18530, train/loss: 0.0
Step: 18530, train/grad_norm: 5.1906263252021745e-06
Step: 18530, train/learning_rate: 2.795097498164978e-05
Step: 18530, train/epoch: 4.409804821014404
Step: 18540, train/loss: 0.00019999999494757503
Step: 18540, train/grad_norm: 9.196353912353516
Step: 18540, train/learning_rate: 2.7939076971961185e-05
Step: 18540, train/epoch: 4.412184715270996
Step: 18550, train/loss: 0.06710000336170197
Step: 18550, train/grad_norm: 0.00011972781794611365
Step: 18550, train/learning_rate: 2.792717714328319e-05
Step: 18550, train/epoch: 4.414564609527588
Step: 18560, train/loss: 0.0
Step: 18560, train/grad_norm: 4.832485501538031e-05
Step: 18560, train/learning_rate: 2.7915279133594595e-05
Step: 18560, train/epoch: 4.41694450378418
Step: 18570, train/loss: 0.07840000092983246
Step: 18570, train/grad_norm: 0.00015982118202373385
Step: 18570, train/learning_rate: 2.7903379304916598e-05
Step: 18570, train/epoch: 4.419323921203613
Step: 18580, train/loss: 0.24779999256134033
Step: 18580, train/grad_norm: 0.03496340662240982
Step: 18580, train/learning_rate: 2.78914794762386e-05
Step: 18580, train/epoch: 4.421703815460205
Step: 18590, train/loss: 0.0006000000284984708
Step: 18590, train/grad_norm: 0.010538918897509575
Step: 18590, train/learning_rate: 2.7879581466550007e-05
Step: 18590, train/epoch: 4.424083709716797
Step: 18600, train/loss: 9.999999747378752e-05
Step: 18600, train/grad_norm: 1.1612647771835327
Step: 18600, train/learning_rate: 2.786768163787201e-05
Step: 18600, train/epoch: 4.426463603973389
Step: 18610, train/loss: 0.031700000166893005
Step: 18610, train/grad_norm: 0.07799060642719269
Step: 18610, train/learning_rate: 2.7855783628183417e-05
Step: 18610, train/epoch: 4.4288434982299805
Step: 18620, train/loss: 9.999999747378752e-05
Step: 18620, train/grad_norm: 2.6153875296586193e-05
Step: 18620, train/learning_rate: 2.784388379950542e-05
Step: 18620, train/epoch: 4.431223392486572
Step: 18630, train/loss: 0.0
Step: 18630, train/grad_norm: 2.2118392735137604e-05
Step: 18630, train/learning_rate: 2.7831983970827423e-05
Step: 18630, train/epoch: 4.433602809906006
Step: 18640, train/loss: 0.0
Step: 18640, train/grad_norm: 3.4511947433202295e-07
Step: 18640, train/learning_rate: 2.782008596113883e-05
Step: 18640, train/epoch: 4.435982704162598
Step: 18650, train/loss: 0.07620000094175339
Step: 18650, train/grad_norm: 0.0024958793073892593
Step: 18650, train/learning_rate: 2.7808186132460833e-05
Step: 18650, train/epoch: 4.4383625984191895
Step: 18660, train/loss: 0.0006000000284984708
Step: 18660, train/grad_norm: 5.48252064618282e-05
Step: 18660, train/learning_rate: 2.779628812277224e-05
Step: 18660, train/epoch: 4.440742492675781
Step: 18670, train/loss: 9.999999747378752e-05
Step: 18670, train/grad_norm: 2.9002583687542938e-05
Step: 18670, train/learning_rate: 2.7784388294094242e-05
Step: 18670, train/epoch: 4.443122386932373
Step: 18680, train/loss: 0.0
Step: 18680, train/grad_norm: 0.0007395200082100928
Step: 18680, train/learning_rate: 2.7772488465416245e-05
Step: 18680, train/epoch: 4.445502281188965
Step: 18690, train/loss: 0.0
Step: 18690, train/grad_norm: 0.00014564259618055075
Step: 18690, train/learning_rate: 2.7760590455727652e-05
Step: 18690, train/epoch: 4.447882175445557
Step: 18700, train/loss: 0.0
Step: 18700, train/grad_norm: 3.935153654310852e-05
Step: 18700, train/learning_rate: 2.7748690627049655e-05
Step: 18700, train/epoch: 4.45026159286499
Step: 18710, train/loss: 9.999999747378752e-05
Step: 18710, train/grad_norm: 3.1720389870315557e-07
Step: 18710, train/learning_rate: 2.773679261736106e-05
Step: 18710, train/epoch: 4.452641487121582
Step: 18720, train/loss: 0.08550000190734863
Step: 18720, train/grad_norm: 0.000111931367428042
Step: 18720, train/learning_rate: 2.7724892788683064e-05
Step: 18720, train/epoch: 4.455021381378174
Step: 18730, train/loss: 0.0
Step: 18730, train/grad_norm: 4.381445251055993e-08
Step: 18730, train/learning_rate: 2.7712992960005067e-05
Step: 18730, train/epoch: 4.457401275634766
Step: 18740, train/loss: 0.0
Step: 18740, train/grad_norm: 0.0003370093763805926
Step: 18740, train/learning_rate: 2.7701094950316474e-05
Step: 18740, train/epoch: 4.459781169891357
Step: 18750, train/loss: 0.028300000354647636
Step: 18750, train/grad_norm: 1.2254260582267307e-05
Step: 18750, train/learning_rate: 2.7689195121638477e-05
Step: 18750, train/epoch: 4.462161064147949
Step: 18760, train/loss: 0.0
Step: 18760, train/grad_norm: 2.831701237937523e-07
Step: 18760, train/learning_rate: 2.7677297111949883e-05
Step: 18760, train/epoch: 4.464540481567383
Step: 18770, train/loss: 0.0
Step: 18770, train/grad_norm: 1.6496412627020618e-06
Step: 18770, train/learning_rate: 2.7665397283271886e-05
Step: 18770, train/epoch: 4.466920375823975
Step: 18780, train/loss: 0.0
Step: 18780, train/grad_norm: 5.836023774463683e-06
Step: 18780, train/learning_rate: 2.765349745459389e-05
Step: 18780, train/epoch: 4.469300270080566
Step: 18790, train/loss: 0.0
Step: 18790, train/grad_norm: 0.00015404116129502654
Step: 18790, train/learning_rate: 2.7641599444905296e-05
Step: 18790, train/epoch: 4.471680164337158
Step: 18800, train/loss: 0.0
Step: 18800, train/grad_norm: 3.776937546717818e-06
Step: 18800, train/learning_rate: 2.76296996162273e-05
Step: 18800, train/epoch: 4.47406005859375
Step: 18810, train/loss: 0.0
Step: 18810, train/grad_norm: 0.0015637731412425637
Step: 18810, train/learning_rate: 2.7617801606538706e-05
Step: 18810, train/epoch: 4.476439952850342
Step: 18820, train/loss: 0.0
Step: 18820, train/grad_norm: 6.914200412211358e-08
Step: 18820, train/learning_rate: 2.760590177786071e-05
Step: 18820, train/epoch: 4.478819847106934
Step: 18830, train/loss: 0.0
Step: 18830, train/grad_norm: 1.321474428550573e-06
Step: 18830, train/learning_rate: 2.759400194918271e-05
Step: 18830, train/epoch: 4.481199264526367
Step: 18840, train/loss: 0.0
Step: 18840, train/grad_norm: 0.00020532184862531722
Step: 18840, train/learning_rate: 2.7582103939494118e-05
Step: 18840, train/epoch: 4.483579158782959
Step: 18850, train/loss: 0.0
Step: 18850, train/grad_norm: 6.374725813884652e-08
Step: 18850, train/learning_rate: 2.757020411081612e-05
Step: 18850, train/epoch: 4.485959053039551
Step: 18860, train/loss: 0.0
Step: 18860, train/grad_norm: 1.0096006917592604e-05
Step: 18860, train/learning_rate: 2.7558306101127528e-05
Step: 18860, train/epoch: 4.488338947296143
Step: 18870, train/loss: 0.0
Step: 18870, train/grad_norm: 2.648081078859832e-07
Step: 18870, train/learning_rate: 2.754640627244953e-05
Step: 18870, train/epoch: 4.490718841552734
Step: 18880, train/loss: 0.0
Step: 18880, train/grad_norm: 1.3036260497756302e-05
Step: 18880, train/learning_rate: 2.7534508262760937e-05
Step: 18880, train/epoch: 4.493098735809326
Step: 18890, train/loss: 0.0
Step: 18890, train/grad_norm: 6.969498645048589e-05
Step: 18890, train/learning_rate: 2.752260843408294e-05
Step: 18890, train/epoch: 4.49547815322876
Step: 18900, train/loss: 0.0
Step: 18900, train/grad_norm: 5.357661336802266e-08
Step: 18900, train/learning_rate: 2.7510708605404943e-05
Step: 18900, train/epoch: 4.497858047485352
Step: 18910, train/loss: 0.0
Step: 18910, train/grad_norm: 0.00037043989868834615
Step: 18910, train/learning_rate: 2.749881059571635e-05
Step: 18910, train/epoch: 4.500237941741943
Step: 18920, train/loss: 0.0
Step: 18920, train/grad_norm: 2.4519373255316168e-05
Step: 18920, train/learning_rate: 2.7486910767038353e-05
Step: 18920, train/epoch: 4.502617835998535
Step: 18930, train/loss: 0.0
Step: 18930, train/grad_norm: 1.471747737014084e-07
Step: 18930, train/learning_rate: 2.747501275734976e-05
Step: 18930, train/epoch: 4.504997730255127
Step: 18940, train/loss: 0.0
Step: 18940, train/grad_norm: 4.013687515680431e-08
Step: 18940, train/learning_rate: 2.7463112928671762e-05
Step: 18940, train/epoch: 4.507377624511719
Step: 18950, train/loss: 0.0
Step: 18950, train/grad_norm: 0.00040832001832313836
Step: 18950, train/learning_rate: 2.7451213099993765e-05
Step: 18950, train/epoch: 4.509757041931152
Step: 18960, train/loss: 0.0
Step: 18960, train/grad_norm: 7.936812096431822e-08
Step: 18960, train/learning_rate: 2.7439315090305172e-05
Step: 18960, train/epoch: 4.512136936187744
Step: 18970, train/loss: 0.0
Step: 18970, train/grad_norm: 3.074777623623959e-07
Step: 18970, train/learning_rate: 2.7427415261627175e-05
Step: 18970, train/epoch: 4.514516830444336
Step: 18980, train/loss: 0.060499999672174454
Step: 18980, train/grad_norm: 0.002340918406844139
Step: 18980, train/learning_rate: 2.741551725193858e-05
Step: 18980, train/epoch: 4.516896724700928
Step: 18990, train/loss: 0.0
Step: 18990, train/grad_norm: 4.400317629915662e-05
Step: 18990, train/learning_rate: 2.7403617423260584e-05
Step: 18990, train/epoch: 4.5192766189575195
Step: 19000, train/loss: 0.00019999999494757503
Step: 19000, train/grad_norm: 13.535542488098145
Step: 19000, train/learning_rate: 2.7391717594582587e-05
Step: 19000, train/epoch: 4.521656513214111
Step: 19010, train/loss: 0.0
Step: 19010, train/grad_norm: 1.6629280708002625e-07
Step: 19010, train/learning_rate: 2.7379819584893994e-05
Step: 19010, train/epoch: 4.524036407470703
Step: 19020, train/loss: 0.10779999941587448
Step: 19020, train/grad_norm: 1.1311113894407754e-06
Step: 19020, train/learning_rate: 2.7367919756215997e-05
Step: 19020, train/epoch: 4.526415824890137
Step: 19030, train/loss: 0.0
Step: 19030, train/grad_norm: 1.4061110050533898e-05
Step: 19030, train/learning_rate: 2.7356021746527404e-05
Step: 19030, train/epoch: 4.5287957191467285
Step: 19040, train/loss: 0.0
Step: 19040, train/grad_norm: 4.262987118863748e-08
Step: 19040, train/learning_rate: 2.7344121917849407e-05
Step: 19040, train/epoch: 4.53117561340332
Step: 19050, train/loss: 0.08320000022649765
Step: 19050, train/grad_norm: 0.00023650552611798048
Step: 19050, train/learning_rate: 2.733222208917141e-05
Step: 19050, train/epoch: 4.533555507659912
Step: 19060, train/loss: 0.0
Step: 19060, train/grad_norm: 6.931467396498192e-06
Step: 19060, train/learning_rate: 2.7320324079482816e-05
Step: 19060, train/epoch: 4.535935401916504
Step: 19070, train/loss: 0.0
Step: 19070, train/grad_norm: 1.8296944404028181e-07
Step: 19070, train/learning_rate: 2.730842425080482e-05
Step: 19070, train/epoch: 4.538315296173096
Step: 19080, train/loss: 0.0
Step: 19080, train/grad_norm: 0.0024470651987940073
Step: 19080, train/learning_rate: 2.7296526241116226e-05
Step: 19080, train/epoch: 4.540694713592529
Step: 19090, train/loss: 0.0
Step: 19090, train/grad_norm: 4.65080887579461e-07
Step: 19090, train/learning_rate: 2.728462641243823e-05
Step: 19090, train/epoch: 4.543074607849121
Step: 19100, train/loss: 0.0
Step: 19100, train/grad_norm: 3.6621597701014252e-06
Step: 19100, train/learning_rate: 2.7272726583760232e-05
Step: 19100, train/epoch: 4.545454502105713
Step: 19110, train/loss: 0.08129999786615372
Step: 19110, train/grad_norm: 8.46075181470951e-06
Step: 19110, train/learning_rate: 2.7260828574071638e-05
Step: 19110, train/epoch: 4.547834396362305
Step: 19120, train/loss: 0.0
Step: 19120, train/grad_norm: 0.0024734430480748415
Step: 19120, train/learning_rate: 2.724892874539364e-05
Step: 19120, train/epoch: 4.5502142906188965
Step: 19130, train/loss: 0.0
Step: 19130, train/grad_norm: 0.0001659937115618959
Step: 19130, train/learning_rate: 2.7237030735705048e-05
Step: 19130, train/epoch: 4.552594184875488
Step: 19140, train/loss: 0.05990000069141388
Step: 19140, train/grad_norm: 9.860816589934984e-07
Step: 19140, train/learning_rate: 2.722513090702705e-05
Step: 19140, train/epoch: 4.554973602294922
Step: 19150, train/loss: 0.0
Step: 19150, train/grad_norm: 6.919952284079045e-06
Step: 19150, train/learning_rate: 2.7213231078349054e-05
Step: 19150, train/epoch: 4.557353496551514
Step: 19160, train/loss: 0.0
Step: 19160, train/grad_norm: 6.613159712287597e-06
Step: 19160, train/learning_rate: 2.720133306866046e-05
Step: 19160, train/epoch: 4.5597333908081055
Step: 19170, train/loss: 0.10970000177621841
Step: 19170, train/grad_norm: 0.0006178734474815428
Step: 19170, train/learning_rate: 2.7189433239982463e-05
Step: 19170, train/epoch: 4.562113285064697
Step: 19180, train/loss: 0.0
Step: 19180, train/grad_norm: 0.0004744653415400535
Step: 19180, train/learning_rate: 2.717753523029387e-05
Step: 19180, train/epoch: 4.564493179321289
Step: 19190, train/loss: 0.0
Step: 19190, train/grad_norm: 0.0010010979603976011
Step: 19190, train/learning_rate: 2.7165635401615873e-05
Step: 19190, train/epoch: 4.566873073577881
Step: 19200, train/loss: 9.999999747378752e-05
Step: 19200, train/grad_norm: 1.0120439583261032e-05
Step: 19200, train/learning_rate: 2.7153735572937876e-05
Step: 19200, train/epoch: 4.569252967834473
Step: 19210, train/loss: 0.09260000288486481
Step: 19210, train/grad_norm: 0.01173423696309328
Step: 19210, train/learning_rate: 2.7141837563249283e-05
Step: 19210, train/epoch: 4.571632385253906
Step: 19220, train/loss: 9.999999747378752e-05
Step: 19220, train/grad_norm: 3.3759092730178963e-06
Step: 19220, train/learning_rate: 2.7129937734571286e-05
Step: 19220, train/epoch: 4.574012279510498
Step: 19230, train/loss: 0.0
Step: 19230, train/grad_norm: 8.09478933661012e-06
Step: 19230, train/learning_rate: 2.7118039724882692e-05
Step: 19230, train/epoch: 4.57639217376709
Step: 19240, train/loss: 0.0
Step: 19240, train/grad_norm: 8.002721187949646e-06
Step: 19240, train/learning_rate: 2.7106139896204695e-05
Step: 19240, train/epoch: 4.578772068023682
Step: 19250, train/loss: 0.0
Step: 19250, train/grad_norm: 0.00019882380729541183
Step: 19250, train/learning_rate: 2.7094240067526698e-05
Step: 19250, train/epoch: 4.581151962280273
Step: 19260, train/loss: 0.11599999666213989
Step: 19260, train/grad_norm: 1.9047389287152328e-05
Step: 19260, train/learning_rate: 2.7082342057838105e-05
Step: 19260, train/epoch: 4.583531856536865
Step: 19270, train/loss: 0.0
Step: 19270, train/grad_norm: 0.00027457784744910896
Step: 19270, train/learning_rate: 2.7070442229160108e-05
Step: 19270, train/epoch: 4.585911273956299
Step: 19280, train/loss: 0.0
Step: 19280, train/grad_norm: 3.8093533021310577e-06
Step: 19280, train/learning_rate: 2.7058544219471514e-05
Step: 19280, train/epoch: 4.588291168212891
Step: 19290, train/loss: 0.0
Step: 19290, train/grad_norm: 0.001012571738101542
Step: 19290, train/learning_rate: 2.7046644390793517e-05
Step: 19290, train/epoch: 4.590671062469482
Step: 19300, train/loss: 0.0
Step: 19300, train/grad_norm: 0.000603088759817183
Step: 19300, train/learning_rate: 2.703474456211552e-05
Step: 19300, train/epoch: 4.593050956726074
Step: 19310, train/loss: 9.999999747378752e-05
Step: 19310, train/grad_norm: 1.1125748642371036e-06
Step: 19310, train/learning_rate: 2.7022846552426927e-05
Step: 19310, train/epoch: 4.595430850982666
Step: 19320, train/loss: 0.0
Step: 19320, train/grad_norm: 0.00013726233737543225
Step: 19320, train/learning_rate: 2.701094672374893e-05
Step: 19320, train/epoch: 4.597810745239258
Step: 19330, train/loss: 0.0
Step: 19330, train/grad_norm: 0.0011486499570310116
Step: 19330, train/learning_rate: 2.6999048714060336e-05
Step: 19330, train/epoch: 4.600190162658691
Step: 19340, train/loss: 0.0
Step: 19340, train/grad_norm: 0.00015281631203833967
Step: 19340, train/learning_rate: 2.698714888538234e-05
Step: 19340, train/epoch: 4.602570056915283
Step: 19350, train/loss: 0.0
Step: 19350, train/grad_norm: 4.114374405617127e-06
Step: 19350, train/learning_rate: 2.6975249056704342e-05
Step: 19350, train/epoch: 4.604949951171875
Step: 19360, train/loss: 0.09790000319480896
Step: 19360, train/grad_norm: 0.057280536741018295
Step: 19360, train/learning_rate: 2.696335104701575e-05
Step: 19360, train/epoch: 4.607329845428467
Step: 19370, train/loss: 0.15549999475479126
Step: 19370, train/grad_norm: 1.3228159332356881e-05
Step: 19370, train/learning_rate: 2.6951451218337752e-05
Step: 19370, train/epoch: 4.609709739685059
Step: 19380, train/loss: 9.999999747378752e-05
Step: 19380, train/grad_norm: 0.00010383488552179188
Step: 19380, train/learning_rate: 2.693955320864916e-05
Step: 19380, train/epoch: 4.61208963394165
Step: 19390, train/loss: 0.0
Step: 19390, train/grad_norm: 1.3812374163535424e-05
Step: 19390, train/learning_rate: 2.692765337997116e-05
Step: 19390, train/epoch: 4.614469528198242
Step: 19400, train/loss: 9.999999747378752e-05
Step: 19400, train/grad_norm: 7.640169314981904e-06
Step: 19400, train/learning_rate: 2.6915753551293164e-05
Step: 19400, train/epoch: 4.616848945617676
Step: 19410, train/loss: 0.0
Step: 19410, train/grad_norm: 0.00010169821325689554
Step: 19410, train/learning_rate: 2.690385554160457e-05
Step: 19410, train/epoch: 4.619228839874268
Step: 19420, train/loss: 0.0
Step: 19420, train/grad_norm: 1.5544392226729542e-05
Step: 19420, train/learning_rate: 2.6891955712926574e-05
Step: 19420, train/epoch: 4.621608734130859
Step: 19430, train/loss: 0.0
Step: 19430, train/grad_norm: 0.0009748924640007317
Step: 19430, train/learning_rate: 2.688005770323798e-05
Step: 19430, train/epoch: 4.623988628387451
Step: 19440, train/loss: 0.0
Step: 19440, train/grad_norm: 4.795150744030252e-05
Step: 19440, train/learning_rate: 2.6868157874559984e-05
Step: 19440, train/epoch: 4.626368522644043
Step: 19450, train/loss: 0.0
Step: 19450, train/grad_norm: 4.54035138375275e-08
Step: 19450, train/learning_rate: 2.6856258045881987e-05
Step: 19450, train/epoch: 4.628748416900635
Step: 19460, train/loss: 0.003599999938160181
Step: 19460, train/grad_norm: 7.448330165971129e-07
Step: 19460, train/learning_rate: 2.6844360036193393e-05
Step: 19460, train/epoch: 4.631127834320068
Step: 19470, train/loss: 0.0
Step: 19470, train/grad_norm: 2.4263892228759687e-08
Step: 19470, train/learning_rate: 2.6832460207515396e-05
Step: 19470, train/epoch: 4.63350772857666
Step: 19480, train/loss: 0.0
Step: 19480, train/grad_norm: 0.0004713370290119201
Step: 19480, train/learning_rate: 2.6820562197826803e-05
Step: 19480, train/epoch: 4.635887622833252
Step: 19490, train/loss: 0.0
Step: 19490, train/grad_norm: 0.00033906567841768265
Step: 19490, train/learning_rate: 2.6808662369148806e-05
Step: 19490, train/epoch: 4.638267517089844
Step: 19500, train/loss: 0.0
Step: 19500, train/grad_norm: 0.0014006089186295867
Step: 19500, train/learning_rate: 2.679676254047081e-05
Step: 19500, train/epoch: 4.6406474113464355
Step: 19510, train/loss: 0.1234000027179718
Step: 19510, train/grad_norm: 0.001368007273413241
Step: 19510, train/learning_rate: 2.6784864530782215e-05
Step: 19510, train/epoch: 4.643027305603027
Step: 19520, train/loss: 0.0
Step: 19520, train/grad_norm: 0.00011893771443283185
Step: 19520, train/learning_rate: 2.6772964702104218e-05
Step: 19520, train/epoch: 4.645406723022461
Step: 19530, train/loss: 0.0
Step: 19530, train/grad_norm: 9.625050552131142e-07
Step: 19530, train/learning_rate: 2.6761066692415625e-05
Step: 19530, train/epoch: 4.647786617279053
Step: 19540, train/loss: 0.0
Step: 19540, train/grad_norm: 1.2172605238447431e-05
Step: 19540, train/learning_rate: 2.6749166863737628e-05
Step: 19540, train/epoch: 4.6501665115356445
Step: 19550, train/loss: 0.0
Step: 19550, train/grad_norm: 2.094837873301003e-05
Step: 19550, train/learning_rate: 2.6737268854049034e-05
Step: 19550, train/epoch: 4.652546405792236
Step: 19560, train/loss: 0.0
Step: 19560, train/grad_norm: 3.5211205613450147e-06
Step: 19560, train/learning_rate: 2.6725369025371037e-05
Step: 19560, train/epoch: 4.654926300048828
Step: 19570, train/loss: 0.0
Step: 19570, train/grad_norm: 0.10695986449718475
Step: 19570, train/learning_rate: 2.671346919669304e-05
Step: 19570, train/epoch: 4.65730619430542
Step: 19580, train/loss: 0.0
Step: 19580, train/grad_norm: 2.0164743546047248e-05
Step: 19580, train/learning_rate: 2.6701571187004447e-05
Step: 19580, train/epoch: 4.659686088562012
Step: 19590, train/loss: 0.0020000000949949026
Step: 19590, train/grad_norm: 7.000913683441468e-06
Step: 19590, train/learning_rate: 2.668967135832645e-05
Step: 19590, train/epoch: 4.662065505981445
Step: 19600, train/loss: 0.0
Step: 19600, train/grad_norm: 2.51026534670018e-07
Step: 19600, train/learning_rate: 2.6677773348637857e-05
Step: 19600, train/epoch: 4.664445400238037
Step: 19610, train/loss: 0.0
Step: 19610, train/grad_norm: 6.216288511495804e-06
Step: 19610, train/learning_rate: 2.666587351995986e-05
Step: 19610, train/epoch: 4.666825294494629
Step: 19620, train/loss: 0.0
Step: 19620, train/grad_norm: 1.2980248698113428e-07
Step: 19620, train/learning_rate: 2.6653973691281863e-05
Step: 19620, train/epoch: 4.669205188751221
Step: 19630, train/loss: 0.13600000739097595
Step: 19630, train/grad_norm: 0.25129783153533936
Step: 19630, train/learning_rate: 2.664207568159327e-05
Step: 19630, train/epoch: 4.6715850830078125
Step: 19640, train/loss: 0.0
Step: 19640, train/grad_norm: 0.0002884801069740206
Step: 19640, train/learning_rate: 2.6630175852915272e-05
Step: 19640, train/epoch: 4.673964977264404
Step: 19650, train/loss: 9.999999747378752e-05
Step: 19650, train/grad_norm: 5.376318586058915e-05
Step: 19650, train/learning_rate: 2.661827784322668e-05
Step: 19650, train/epoch: 4.676344394683838
Step: 19660, train/loss: 0.0010000000474974513
Step: 19660, train/grad_norm: 0.00038143902202136815
Step: 19660, train/learning_rate: 2.660637801454868e-05
Step: 19660, train/epoch: 4.67872428894043
Step: 19670, train/loss: 0.0
Step: 19670, train/grad_norm: 1.4786267456656788e-05
Step: 19670, train/learning_rate: 2.6594478185870685e-05
Step: 19670, train/epoch: 4.6811041831970215
Step: 19680, train/loss: 0.0
Step: 19680, train/grad_norm: 0.0013059221673756838
Step: 19680, train/learning_rate: 2.658258017618209e-05
Step: 19680, train/epoch: 4.683484077453613
Step: 19690, train/loss: 0.0
Step: 19690, train/grad_norm: 3.825102339760633e-06
Step: 19690, train/learning_rate: 2.6570680347504094e-05
Step: 19690, train/epoch: 4.685863971710205
Step: 19700, train/loss: 0.0
Step: 19700, train/grad_norm: 1.064050775312353e-05
Step: 19700, train/learning_rate: 2.65587823378155e-05
Step: 19700, train/epoch: 4.688243865966797
Step: 19710, train/loss: 0.0
Step: 19710, train/grad_norm: 1.2741655154968612e-05
Step: 19710, train/learning_rate: 2.6546882509137504e-05
Step: 19710, train/epoch: 4.6906232833862305
Step: 19720, train/loss: 0.0
Step: 19720, train/grad_norm: 1.110231778511661e-06
Step: 19720, train/learning_rate: 2.6534982680459507e-05
Step: 19720, train/epoch: 4.693003177642822
Step: 19730, train/loss: 0.0
Step: 19730, train/grad_norm: 3.27998964166909e-06
Step: 19730, train/learning_rate: 2.6523084670770913e-05
Step: 19730, train/epoch: 4.695383071899414
Step: 19740, train/loss: 0.07109999656677246
Step: 19740, train/grad_norm: 0.0006832665530964732
Step: 19740, train/learning_rate: 2.6511184842092916e-05
Step: 19740, train/epoch: 4.697762966156006
Step: 19750, train/loss: 0.0
Step: 19750, train/grad_norm: 3.1830066291149706e-05
Step: 19750, train/learning_rate: 2.6499286832404323e-05
Step: 19750, train/epoch: 4.700142860412598
Step: 19760, train/loss: 0.0
Step: 19760, train/grad_norm: 0.0029900120571255684
Step: 19760, train/learning_rate: 2.6487387003726326e-05
Step: 19760, train/epoch: 4.7025227546691895
Step: 19770, train/loss: 0.00019999999494757503
Step: 19770, train/grad_norm: 0.010504942387342453
Step: 19770, train/learning_rate: 2.647548717504833e-05
Step: 19770, train/epoch: 4.704902648925781
Step: 19780, train/loss: 0.0
Step: 19780, train/grad_norm: 2.327253605471924e-05
Step: 19780, train/learning_rate: 2.6463589165359735e-05
Step: 19780, train/epoch: 4.707282066345215
Step: 19790, train/loss: 0.0
Step: 19790, train/grad_norm: 0.00016101173241622746
Step: 19790, train/learning_rate: 2.645168933668174e-05
Step: 19790, train/epoch: 4.709661960601807
Step: 19800, train/loss: 0.0
Step: 19800, train/grad_norm: 5.789875558548374e-06
Step: 19800, train/learning_rate: 2.6439791326993145e-05
Step: 19800, train/epoch: 4.712041854858398
Step: 19810, train/loss: 0.0
Step: 19810, train/grad_norm: 8.453984628431499e-05
Step: 19810, train/learning_rate: 2.6427891498315148e-05
Step: 19810, train/epoch: 4.71442174911499
Step: 19820, train/loss: 0.0
Step: 19820, train/grad_norm: 9.705588126962539e-06
Step: 19820, train/learning_rate: 2.641599166963715e-05
Step: 19820, train/epoch: 4.716801643371582
Step: 19830, train/loss: 0.0
Step: 19830, train/grad_norm: 4.131336027057841e-06
Step: 19830, train/learning_rate: 2.6404093659948558e-05
Step: 19830, train/epoch: 4.719181537628174
Step: 19840, train/loss: 0.0
Step: 19840, train/grad_norm: 1.712461562419776e-05
Step: 19840, train/learning_rate: 2.639219383127056e-05
Step: 19840, train/epoch: 4.721560955047607
Step: 19850, train/loss: 0.0
Step: 19850, train/grad_norm: 7.454006549778569e-07
Step: 19850, train/learning_rate: 2.6380295821581967e-05
Step: 19850, train/epoch: 4.723940849304199
Step: 19860, train/loss: 0.0
Step: 19860, train/grad_norm: 2.0215779272803047e-07
Step: 19860, train/learning_rate: 2.636839599290397e-05
Step: 19860, train/epoch: 4.726320743560791
Step: 19870, train/loss: 0.0
Step: 19870, train/grad_norm: 4.222998200020811e-07
Step: 19870, train/learning_rate: 2.6356496164225973e-05
Step: 19870, train/epoch: 4.728700637817383
Step: 19880, train/loss: 0.0
Step: 19880, train/grad_norm: 1.1822080381307387e-07
Step: 19880, train/learning_rate: 2.634459815453738e-05
Step: 19880, train/epoch: 4.731080532073975
Step: 19890, train/loss: 0.00039999998989515007
Step: 19890, train/grad_norm: 5.144753932952881
Step: 19890, train/learning_rate: 2.6332698325859383e-05
Step: 19890, train/epoch: 4.733460426330566
Step: 19900, train/loss: 0.0020000000949949026
Step: 19900, train/grad_norm: 66.59371185302734
Step: 19900, train/learning_rate: 2.632080031617079e-05
Step: 19900, train/epoch: 4.73583984375
Step: 19910, train/loss: 0.0
Step: 19910, train/grad_norm: 2.8082385483685357e-07
Step: 19910, train/learning_rate: 2.6308900487492792e-05
Step: 19910, train/epoch: 4.738219738006592
Step: 19920, train/loss: 0.0
Step: 19920, train/grad_norm: 4.688977242039982e-07
Step: 19920, train/learning_rate: 2.6297000658814795e-05
Step: 19920, train/epoch: 4.740599632263184
Step: 19930, train/loss: 0.0
Step: 19930, train/grad_norm: 5.06706157921144e-07
Step: 19930, train/learning_rate: 2.6285102649126202e-05
Step: 19930, train/epoch: 4.742979526519775
Step: 19940, train/loss: 0.0
Step: 19940, train/grad_norm: 0.0002926211163867265
Step: 19940, train/learning_rate: 2.6273202820448205e-05
Step: 19940, train/epoch: 4.745359420776367
Step: 19950, train/loss: 0.0
Step: 19950, train/grad_norm: 4.199820523353992e-06
Step: 19950, train/learning_rate: 2.626130481075961e-05
Step: 19950, train/epoch: 4.747739315032959
Step: 19960, train/loss: 0.11020000278949738
Step: 19960, train/grad_norm: 3.801798084168695e-05
Step: 19960, train/learning_rate: 2.6249404982081614e-05
Step: 19960, train/epoch: 4.750119209289551
Step: 19970, train/loss: 0.08879999816417694
Step: 19970, train/grad_norm: 0.06036025285720825
Step: 19970, train/learning_rate: 2.6237505153403617e-05
Step: 19970, train/epoch: 4.752498626708984
Step: 19980, train/loss: 0.005799999926239252
Step: 19980, train/grad_norm: 4.4485856051323935e-05
Step: 19980, train/learning_rate: 2.6225607143715024e-05
Step: 19980, train/epoch: 4.754878520965576
Step: 19990, train/loss: 0.0
Step: 19990, train/grad_norm: 3.9055783418007195e-05
Step: 19990, train/learning_rate: 2.6213707315037027e-05
Step: 19990, train/epoch: 4.757258415222168
Step: 20000, train/loss: 0.0
Step: 20000, train/grad_norm: 7.193823421403067e-06
Step: 20000, train/learning_rate: 2.6201809305348434e-05
Step: 20000, train/epoch: 4.75963830947876
Step: 20010, train/loss: 0.0
Step: 20010, train/grad_norm: 0.00044981716200709343
Step: 20010, train/learning_rate: 2.6189909476670437e-05
Step: 20010, train/epoch: 4.762018203735352
Step: 20020, train/loss: 0.0
Step: 20020, train/grad_norm: 3.7212978440948064e-06
Step: 20020, train/learning_rate: 2.617800964799244e-05
Step: 20020, train/epoch: 4.764398097991943
Step: 20030, train/loss: 0.0
Step: 20030, train/grad_norm: 7.14626048647915e-06
Step: 20030, train/learning_rate: 2.6166111638303846e-05
Step: 20030, train/epoch: 4.766777515411377
Step: 20040, train/loss: 0.0
Step: 20040, train/grad_norm: 9.781317089618824e-08
Step: 20040, train/learning_rate: 2.615421180962585e-05
Step: 20040, train/epoch: 4.769157409667969
Step: 20050, train/loss: 0.0
Step: 20050, train/grad_norm: 1.4209603250492364e-05
Step: 20050, train/learning_rate: 2.6142313799937256e-05
Step: 20050, train/epoch: 4.7715373039245605
Step: 20060, train/loss: 0.0
Step: 20060, train/grad_norm: 4.1625171434134245e-05
Step: 20060, train/learning_rate: 2.613041397125926e-05
Step: 20060, train/epoch: 4.773917198181152
Step: 20070, train/loss: 0.0
Step: 20070, train/grad_norm: 3.230780566809699e-05
Step: 20070, train/learning_rate: 2.6118514142581262e-05
Step: 20070, train/epoch: 4.776297092437744
Step: 20080, train/loss: 0.0
Step: 20080, train/grad_norm: 4.3736254156101495e-06
Step: 20080, train/learning_rate: 2.6106616132892668e-05
Step: 20080, train/epoch: 4.778676986694336
Step: 20090, train/loss: 0.0
Step: 20090, train/grad_norm: 3.3534557587699965e-05
Step: 20090, train/learning_rate: 2.609471630421467e-05
Step: 20090, train/epoch: 4.7810564041137695
Step: 20100, train/loss: 0.0
Step: 20100, train/grad_norm: 0.000503264251165092
Step: 20100, train/learning_rate: 2.6082818294526078e-05
Step: 20100, train/epoch: 4.783436298370361
Step: 20110, train/loss: 0.0
Step: 20110, train/grad_norm: 5.040329824623768e-07
Step: 20110, train/learning_rate: 2.607091846584808e-05
Step: 20110, train/epoch: 4.785816192626953
Step: 20120, train/loss: 0.0
Step: 20120, train/grad_norm: 6.289811153692426e-06
Step: 20120, train/learning_rate: 2.6059018637170084e-05
Step: 20120, train/epoch: 4.788196086883545
Step: 20130, train/loss: 0.0
Step: 20130, train/grad_norm: 1.1812091571528072e-07
Step: 20130, train/learning_rate: 2.604712062748149e-05
Step: 20130, train/epoch: 4.790575981140137
Step: 20140, train/loss: 0.0
Step: 20140, train/grad_norm: 7.302171070477925e-06
Step: 20140, train/learning_rate: 2.6035220798803493e-05
Step: 20140, train/epoch: 4.7929558753967285
Step: 20150, train/loss: 0.0
Step: 20150, train/grad_norm: 8.170843557309126e-09
Step: 20150, train/learning_rate: 2.60233227891149e-05
Step: 20150, train/epoch: 4.79533576965332
Step: 20160, train/loss: 0.0
Step: 20160, train/grad_norm: 2.326897430293684e-07
Step: 20160, train/learning_rate: 2.6011422960436903e-05
Step: 20160, train/epoch: 4.797715187072754
Step: 20170, train/loss: 0.0
Step: 20170, train/grad_norm: 4.52000620043691e-07
Step: 20170, train/learning_rate: 2.5999523131758906e-05
Step: 20170, train/epoch: 4.800095081329346
Step: 20180, train/loss: 0.0
Step: 20180, train/grad_norm: 2.2090186746481777e-07
Step: 20180, train/learning_rate: 2.5987625122070312e-05
Step: 20180, train/epoch: 4.8024749755859375
Step: 20190, train/loss: 0.0
Step: 20190, train/grad_norm: 3.2025121754486463e-07
Step: 20190, train/learning_rate: 2.5975725293392316e-05
Step: 20190, train/epoch: 4.804854869842529
Step: 20200, train/loss: 0.0
Step: 20200, train/grad_norm: 9.864806997939013e-07
Step: 20200, train/learning_rate: 2.5963827283703722e-05
Step: 20200, train/epoch: 4.807234764099121
Step: 20210, train/loss: 0.0
Step: 20210, train/grad_norm: 3.6164417451800546e-06
Step: 20210, train/learning_rate: 2.5951927455025725e-05
Step: 20210, train/epoch: 4.809614658355713
Step: 20220, train/loss: 0.0
Step: 20220, train/grad_norm: 1.3394771514185777e-08
Step: 20220, train/learning_rate: 2.594002944533713e-05
Step: 20220, train/epoch: 4.8119940757751465
Step: 20230, train/loss: 0.0
Step: 20230, train/grad_norm: 1.8231648413191692e-09
Step: 20230, train/learning_rate: 2.5928129616659135e-05
Step: 20230, train/epoch: 4.814373970031738
Step: 20240, train/loss: 0.0
Step: 20240, train/grad_norm: 4.952206040798046e-07
Step: 20240, train/learning_rate: 2.5916229787981138e-05
Step: 20240, train/epoch: 4.81675386428833
Step: 20250, train/loss: 0.0
Step: 20250, train/grad_norm: 1.5892318572241493e-09
Step: 20250, train/learning_rate: 2.5904331778292544e-05
Step: 20250, train/epoch: 4.819133758544922
Step: 20260, train/loss: 0.0
Step: 20260, train/grad_norm: 4.355518740339903e-06
Step: 20260, train/learning_rate: 2.5892431949614547e-05
Step: 20260, train/epoch: 4.821513652801514
Step: 20270, train/loss: 0.0
Step: 20270, train/grad_norm: 1.2716034802906506e-07
Step: 20270, train/learning_rate: 2.5880533939925954e-05
Step: 20270, train/epoch: 4.8238935470581055
Step: 20280, train/loss: 0.0
Step: 20280, train/grad_norm: 0.0050562648102641106
Step: 20280, train/learning_rate: 2.5868634111247957e-05
Step: 20280, train/epoch: 4.826273441314697
Step: 20290, train/loss: 0.0
Step: 20290, train/grad_norm: 8.382065885825796e-08
Step: 20290, train/learning_rate: 2.585673428256996e-05
Step: 20290, train/epoch: 4.828652858734131
Step: 20300, train/loss: 0.0
Step: 20300, train/grad_norm: 2.5040193918357545e-07
Step: 20300, train/learning_rate: 2.5844836272881366e-05
Step: 20300, train/epoch: 4.831032752990723
Step: 20310, train/loss: 0.0
Step: 20310, train/grad_norm: 8.057603650968304e-08
Step: 20310, train/learning_rate: 2.583293644420337e-05
Step: 20310, train/epoch: 4.8334126472473145
Step: 20320, train/loss: 0.0
Step: 20320, train/grad_norm: 9.787948584971673e-08
Step: 20320, train/learning_rate: 2.5821038434514776e-05
Step: 20320, train/epoch: 4.835792541503906
Step: 20330, train/loss: 0.0
Step: 20330, train/grad_norm: 1.964680507171579e-08
Step: 20330, train/learning_rate: 2.580913860583678e-05
Step: 20330, train/epoch: 4.838172435760498
Step: 20340, train/loss: 0.0
Step: 20340, train/grad_norm: 4.6886859195183206e-07
Step: 20340, train/learning_rate: 2.5797238777158782e-05
Step: 20340, train/epoch: 4.84055233001709
Step: 20350, train/loss: 9.999999747378752e-05
Step: 20350, train/grad_norm: 5.866336323379073e-07
Step: 20350, train/learning_rate: 2.578534076747019e-05
Step: 20350, train/epoch: 4.842931747436523
Step: 20360, train/loss: 0.0
Step: 20360, train/grad_norm: 4.0236327549791895e-06
Step: 20360, train/learning_rate: 2.577344093879219e-05
Step: 20360, train/epoch: 4.845311641693115
Step: 20370, train/loss: 0.0
Step: 20370, train/grad_norm: 8.665073636393572e-08
Step: 20370, train/learning_rate: 2.5761542929103598e-05
Step: 20370, train/epoch: 4.847691535949707
Step: 20380, train/loss: 0.0
Step: 20380, train/grad_norm: 7.615074991917936e-06
Step: 20380, train/learning_rate: 2.57496431004256e-05
Step: 20380, train/epoch: 4.850071430206299
Step: 20390, train/loss: 0.0
Step: 20390, train/grad_norm: 1.6402531173298485e-06
Step: 20390, train/learning_rate: 2.5737743271747604e-05
Step: 20390, train/epoch: 4.852451324462891
Step: 20400, train/loss: 0.0
Step: 20400, train/grad_norm: 0.0006701861275359988
Step: 20400, train/learning_rate: 2.572584526205901e-05
Step: 20400, train/epoch: 4.854831218719482
Step: 20410, train/loss: 0.0
Step: 20410, train/grad_norm: 7.37680311431177e-05
Step: 20410, train/learning_rate: 2.5713945433381014e-05
Step: 20410, train/epoch: 4.857210636138916
Step: 20420, train/loss: 0.0
Step: 20420, train/grad_norm: 8.388116725654982e-07
Step: 20420, train/learning_rate: 2.570204742369242e-05
Step: 20420, train/epoch: 4.859590530395508
Step: 20430, train/loss: 0.0
Step: 20430, train/grad_norm: 1.7537635130793205e-06
Step: 20430, train/learning_rate: 2.5690147595014423e-05
Step: 20430, train/epoch: 4.8619704246521
Step: 20440, train/loss: 0.0
Step: 20440, train/grad_norm: 2.2289427761279512e-06
Step: 20440, train/learning_rate: 2.5678247766336426e-05
Step: 20440, train/epoch: 4.864350318908691
Step: 20450, train/loss: 0.0
Step: 20450, train/grad_norm: 5.521029038391134e-07
Step: 20450, train/learning_rate: 2.5666349756647833e-05
Step: 20450, train/epoch: 4.866730213165283
Step: 20460, train/loss: 0.0
Step: 20460, train/grad_norm: 1.3945802493253723e-05
Step: 20460, train/learning_rate: 2.5654449927969836e-05
Step: 20460, train/epoch: 4.869110107421875
Step: 20470, train/loss: 0.0
Step: 20470, train/grad_norm: 6.199715426191688e-05
Step: 20470, train/learning_rate: 2.5642551918281242e-05
Step: 20470, train/epoch: 4.871490001678467
Step: 20480, train/loss: 0.0
Step: 20480, train/grad_norm: 5.4856799103220055e-09
Step: 20480, train/learning_rate: 2.5630652089603245e-05
Step: 20480, train/epoch: 4.8738694190979
Step: 20490, train/loss: 0.0
Step: 20490, train/grad_norm: 1.134800586832796e-09
Step: 20490, train/learning_rate: 2.5618752260925248e-05
Step: 20490, train/epoch: 4.876249313354492
Step: 20500, train/loss: 0.0
Step: 20500, train/grad_norm: 4.5806626047806276e-08
Step: 20500, train/learning_rate: 2.5606854251236655e-05
Step: 20500, train/epoch: 4.878629207611084
Step: 20510, train/loss: 0.0
Step: 20510, train/grad_norm: 4.196368585951404e-09
Step: 20510, train/learning_rate: 2.5594954422558658e-05
Step: 20510, train/epoch: 4.881009101867676
Step: 20520, train/loss: 0.0
Step: 20520, train/grad_norm: 1.2560466555733285e-10
Step: 20520, train/learning_rate: 2.5583056412870064e-05
Step: 20520, train/epoch: 4.883388996124268
Step: 20530, train/loss: 0.0
Step: 20530, train/grad_norm: 1.5131530744838528e-05
Step: 20530, train/learning_rate: 2.5571156584192067e-05
Step: 20530, train/epoch: 4.885768890380859
Step: 20540, train/loss: 0.0
Step: 20540, train/grad_norm: 0.0004548989236354828
Step: 20540, train/learning_rate: 2.555925675551407e-05
Step: 20540, train/epoch: 4.888148307800293
Step: 20550, train/loss: 0.0
Step: 20550, train/grad_norm: 6.121647544432562e-08
Step: 20550, train/learning_rate: 2.5547358745825477e-05
Step: 20550, train/epoch: 4.890528202056885
Step: 20560, train/loss: 0.0003000000142492354
Step: 20560, train/grad_norm: 1.1911173380951823e-08
Step: 20560, train/learning_rate: 2.553545891714748e-05
Step: 20560, train/epoch: 4.892908096313477
Step: 20570, train/loss: 0.0
Step: 20570, train/grad_norm: 1.7150816233879596e-07
Step: 20570, train/learning_rate: 2.5523560907458887e-05
Step: 20570, train/epoch: 4.895287990570068
Step: 20580, train/loss: 0.0
Step: 20580, train/grad_norm: 0.0002018812665482983
Step: 20580, train/learning_rate: 2.551166107878089e-05
Step: 20580, train/epoch: 4.89766788482666
Step: 20590, train/loss: 0.15549999475479126
Step: 20590, train/grad_norm: 3.844707862299401e-06
Step: 20590, train/learning_rate: 2.5499761250102893e-05
Step: 20590, train/epoch: 4.900047779083252
Step: 20600, train/loss: 0.0
Step: 20600, train/grad_norm: 8.445242372090433e-08
Step: 20600, train/learning_rate: 2.54878632404143e-05
Step: 20600, train/epoch: 4.9024271965026855
Step: 20610, train/loss: 0.0
Step: 20610, train/grad_norm: 5.221945684752427e-07
Step: 20610, train/learning_rate: 2.5475963411736302e-05
Step: 20610, train/epoch: 4.904807090759277
Step: 20620, train/loss: 0.0
Step: 20620, train/grad_norm: 4.81207003133477e-08
Step: 20620, train/learning_rate: 2.546406540204771e-05
Step: 20620, train/epoch: 4.907186985015869
Step: 20630, train/loss: 0.0
Step: 20630, train/grad_norm: 0.006957176607102156
Step: 20630, train/learning_rate: 2.545216557336971e-05
Step: 20630, train/epoch: 4.909566879272461
Step: 20640, train/loss: 0.0
Step: 20640, train/grad_norm: 3.471167886459625e-08
Step: 20640, train/learning_rate: 2.5440265744691715e-05
Step: 20640, train/epoch: 4.911946773529053
Step: 20650, train/loss: 0.0
Step: 20650, train/grad_norm: 8.512529916515632e-07
Step: 20650, train/learning_rate: 2.542836773500312e-05
Step: 20650, train/epoch: 4.9143266677856445
Step: 20660, train/loss: 0.00019999999494757503
Step: 20660, train/grad_norm: 1.1456683779442756e-07
Step: 20660, train/learning_rate: 2.5416467906325124e-05
Step: 20660, train/epoch: 4.916706562042236
Step: 20670, train/loss: 0.0
Step: 20670, train/grad_norm: 1.0604074304865208e-05
Step: 20670, train/learning_rate: 2.540456989663653e-05
Step: 20670, train/epoch: 4.91908597946167
Step: 20680, train/loss: 0.0
Step: 20680, train/grad_norm: 2.4007638899092854e-07
Step: 20680, train/learning_rate: 2.5392670067958534e-05
Step: 20680, train/epoch: 4.921465873718262
Step: 20690, train/loss: 0.09300000220537186
Step: 20690, train/grad_norm: 5.776538819191046e-05
Step: 20690, train/learning_rate: 2.5380770239280537e-05
Step: 20690, train/epoch: 4.9238457679748535
Step: 20700, train/loss: 0.0
Step: 20700, train/grad_norm: 0.00483928807079792
Step: 20700, train/learning_rate: 2.5368872229591943e-05
Step: 20700, train/epoch: 4.926225662231445
Step: 20710, train/loss: 0.0
Step: 20710, train/grad_norm: 0.02071557566523552
Step: 20710, train/learning_rate: 2.5356972400913946e-05
Step: 20710, train/epoch: 4.928605556488037
Step: 20720, train/loss: 0.0
Step: 20720, train/grad_norm: 1.3388638762990013e-05
Step: 20720, train/learning_rate: 2.5345074391225353e-05
Step: 20720, train/epoch: 4.930985450744629
Step: 20730, train/loss: 0.0
Step: 20730, train/grad_norm: 0.0006821567076258361
Step: 20730, train/learning_rate: 2.5333174562547356e-05
Step: 20730, train/epoch: 4.9333648681640625
Step: 20740, train/loss: 0.0
Step: 20740, train/grad_norm: 6.238076366571477e-08
Step: 20740, train/learning_rate: 2.532127473386936e-05
Step: 20740, train/epoch: 4.935744762420654
Step: 20750, train/loss: 0.0
Step: 20750, train/grad_norm: 1.7572394028775307e-07
Step: 20750, train/learning_rate: 2.5309376724180765e-05
Step: 20750, train/epoch: 4.938124656677246
Step: 20760, train/loss: 0.0
Step: 20760, train/grad_norm: 0.000799881701823324
Step: 20760, train/learning_rate: 2.529747689550277e-05
Step: 20760, train/epoch: 4.940504550933838
Step: 20770, train/loss: 0.0
Step: 20770, train/grad_norm: 2.3234193236021383e-07
Step: 20770, train/learning_rate: 2.5285578885814175e-05
Step: 20770, train/epoch: 4.94288444519043
Step: 20780, train/loss: 0.0
Step: 20780, train/grad_norm: 2.8506499802460894e-05
Step: 20780, train/learning_rate: 2.5273679057136178e-05
Step: 20780, train/epoch: 4.9452643394470215
Step: 20790, train/loss: 0.0
Step: 20790, train/grad_norm: 4.048690414037992e-07
Step: 20790, train/learning_rate: 2.526177922845818e-05
Step: 20790, train/epoch: 4.947643756866455
Step: 20800, train/loss: 0.0
Step: 20800, train/grad_norm: 5.072067565947691e-08
Step: 20800, train/learning_rate: 2.5249881218769588e-05
Step: 20800, train/epoch: 4.950023651123047
Step: 20810, train/loss: 0.0
Step: 20810, train/grad_norm: 1.1538357966855983e-06
Step: 20810, train/learning_rate: 2.523798139009159e-05
Step: 20810, train/epoch: 4.952403545379639
Step: 20820, train/loss: 0.0
Step: 20820, train/grad_norm: 4.64201832528488e-07
Step: 20820, train/learning_rate: 2.5226083380402997e-05
Step: 20820, train/epoch: 4.9547834396362305
Step: 20830, train/loss: 0.0
Step: 20830, train/grad_norm: 1.5365454686389057e-08
Step: 20830, train/learning_rate: 2.5214183551725e-05
Step: 20830, train/epoch: 4.957163333892822
Step: 20840, train/loss: 0.0
Step: 20840, train/grad_norm: 8.146990694513079e-06
Step: 20840, train/learning_rate: 2.5202283723047003e-05
Step: 20840, train/epoch: 4.959543228149414
Step: 20850, train/loss: 0.0
Step: 20850, train/grad_norm: 1.0929872587439604e-05
Step: 20850, train/learning_rate: 2.519038571335841e-05
Step: 20850, train/epoch: 4.961923122406006
Step: 20860, train/loss: 0.0
Step: 20860, train/grad_norm: 7.762466225358367e-07
Step: 20860, train/learning_rate: 2.5178485884680413e-05
Step: 20860, train/epoch: 4.9643025398254395
Step: 20870, train/loss: 0.0
Step: 20870, train/grad_norm: 4.517994966590777e-06
Step: 20870, train/learning_rate: 2.516658787499182e-05
Step: 20870, train/epoch: 4.966682434082031
Step: 20880, train/loss: 0.0
Step: 20880, train/grad_norm: 1.589160518733479e-07
Step: 20880, train/learning_rate: 2.5154688046313822e-05
Step: 20880, train/epoch: 4.969062328338623
Step: 20890, train/loss: 0.0
Step: 20890, train/grad_norm: 1.4802399164182134e-06
Step: 20890, train/learning_rate: 2.514279003662523e-05
Step: 20890, train/epoch: 4.971442222595215
Step: 20900, train/loss: 0.0
Step: 20900, train/grad_norm: 4.4817329580837395e-07
Step: 20900, train/learning_rate: 2.5130890207947232e-05
Step: 20900, train/epoch: 4.973822116851807
Step: 20910, train/loss: 0.0
Step: 20910, train/grad_norm: 6.0076246882090345e-05
Step: 20910, train/learning_rate: 2.5118990379269235e-05
Step: 20910, train/epoch: 4.976202011108398
Step: 20920, train/loss: 0.0
Step: 20920, train/grad_norm: 8.426667363892193e-08
Step: 20920, train/learning_rate: 2.510709236958064e-05
Step: 20920, train/epoch: 4.978581428527832
Step: 20930, train/loss: 0.0
Step: 20930, train/grad_norm: 8.758700460020918e-06
Step: 20930, train/learning_rate: 2.5095192540902644e-05
Step: 20930, train/epoch: 4.980961322784424
Step: 20940, train/loss: 0.1679999977350235
Step: 20940, train/grad_norm: 0.02997446246445179
Step: 20940, train/learning_rate: 2.508329453121405e-05
Step: 20940, train/epoch: 4.983341217041016
Step: 20950, train/loss: 0.08129999786615372
Step: 20950, train/grad_norm: 0.00016603968106210232
Step: 20950, train/learning_rate: 2.5071394702536054e-05
Step: 20950, train/epoch: 4.985721111297607
Step: 20960, train/loss: 0.0
Step: 20960, train/grad_norm: 0.00010037552419817075
Step: 20960, train/learning_rate: 2.5059494873858057e-05
Step: 20960, train/epoch: 4.988101005554199
Step: 20970, train/loss: 0.0
Step: 20970, train/grad_norm: 2.2862834157422185e-05
Step: 20970, train/learning_rate: 2.5047596864169464e-05
Step: 20970, train/epoch: 4.990480899810791
Step: 20980, train/loss: 0.0
Step: 20980, train/grad_norm: 0.00014602832379750907
Step: 20980, train/learning_rate: 2.5035697035491467e-05
Step: 20980, train/epoch: 4.992860317230225
Step: 20990, train/loss: 0.0
Step: 20990, train/grad_norm: 0.00019736353715416044
Step: 20990, train/learning_rate: 2.5023799025802873e-05
Step: 20990, train/epoch: 4.995240211486816
Step: 21000, train/loss: 0.00019999999494757503
Step: 21000, train/grad_norm: 0.00014557133545167744
Step: 21000, train/learning_rate: 2.5011899197124876e-05
Step: 21000, train/epoch: 4.997620105743408
Step: 21010, train/loss: 0.0
Step: 21010, train/grad_norm: 2.701416406125645e-06
Step: 21010, train/learning_rate: 2.499999936844688e-05
Step: 21010, train/epoch: 5.0
Step: 21010, eval/loss: 0.03575105965137482
Step: 21010, eval/accuracy: 0.9943079352378845
Step: 21010, eval/f1: 0.9939942955970764
Step: 21010, eval/runtime: 733.9583740234375
Step: 21010, eval/samples_per_second: 9.814000129699707
Step: 21010, eval/steps_per_second: 1.2280000448226929
Step: 21010, train/epoch: 5.0
Step: 21020, train/loss: 0.0
Step: 21020, train/grad_norm: 4.050140341860242e-05
Step: 21020, train/learning_rate: 2.4988101358758286e-05
Step: 21020, train/epoch: 5.002379894256592
Step: 21030, train/loss: 0.0
Step: 21030, train/grad_norm: 7.119266228983179e-05
Step: 21030, train/learning_rate: 2.497620153008029e-05
Step: 21030, train/epoch: 5.004759788513184
Step: 21040, train/loss: 0.0
Step: 21040, train/grad_norm: 7.2468005782866385e-06
Step: 21040, train/learning_rate: 2.4964303520391695e-05
Step: 21040, train/epoch: 5.007139682769775
Step: 21050, train/loss: 0.0
Step: 21050, train/grad_norm: 1.3730824321100954e-05
Step: 21050, train/learning_rate: 2.4952403691713698e-05
Step: 21050, train/epoch: 5.009519100189209
Step: 21060, train/loss: 0.0
Step: 21060, train/grad_norm: 5.727119969378691e-07
Step: 21060, train/learning_rate: 2.49405038630357e-05
Step: 21060, train/epoch: 5.011898994445801
Step: 21070, train/loss: 0.0
Step: 21070, train/grad_norm: 2.5214038032572716e-05
Step: 21070, train/learning_rate: 2.4928605853347108e-05
Step: 21070, train/epoch: 5.014278888702393
Step: 21080, train/loss: 0.0
Step: 21080, train/grad_norm: 0.04298447072505951
Step: 21080, train/learning_rate: 2.491670602466911e-05
Step: 21080, train/epoch: 5.016658782958984
Step: 21090, train/loss: 0.0
Step: 21090, train/grad_norm: 4.9600121201365255e-06
Step: 21090, train/learning_rate: 2.4904808014980517e-05
Step: 21090, train/epoch: 5.019038677215576
Step: 21100, train/loss: 0.0
Step: 21100, train/grad_norm: 3.551026020431891e-05
Step: 21100, train/learning_rate: 2.489290818630252e-05
Step: 21100, train/epoch: 5.021418571472168
Step: 21110, train/loss: 0.0
Step: 21110, train/grad_norm: 3.911268322553951e-06
Step: 21110, train/learning_rate: 2.4881008357624523e-05
Step: 21110, train/epoch: 5.023797988891602
Step: 21120, train/loss: 0.0
Step: 21120, train/grad_norm: 3.358874067771467e-08
Step: 21120, train/learning_rate: 2.486911034793593e-05
Step: 21120, train/epoch: 5.026177883148193
Step: 21130, train/loss: 0.0
Step: 21130, train/grad_norm: 5.3466610552277416e-05
Step: 21130, train/learning_rate: 2.4857210519257933e-05
Step: 21130, train/epoch: 5.028557777404785
Step: 21140, train/loss: 0.0
Step: 21140, train/grad_norm: 1.1908587111975066e-05
Step: 21140, train/learning_rate: 2.484531250956934e-05
Step: 21140, train/epoch: 5.030937671661377
Step: 21150, train/loss: 0.0
Step: 21150, train/grad_norm: 0.0008502649143338203
Step: 21150, train/learning_rate: 2.4833412680891342e-05
Step: 21150, train/epoch: 5.033317565917969
Step: 21160, train/loss: 0.0
Step: 21160, train/grad_norm: 8.931250050636663e-08
Step: 21160, train/learning_rate: 2.4821512852213345e-05
Step: 21160, train/epoch: 5.0356974601745605
Step: 21170, train/loss: 0.0
Step: 21170, train/grad_norm: 1.0062822184409015e-05
Step: 21170, train/learning_rate: 2.4809614842524752e-05
Step: 21170, train/epoch: 5.038076877593994
Step: 21180, train/loss: 0.0
Step: 21180, train/grad_norm: 0.004946663044393063
Step: 21180, train/learning_rate: 2.4797715013846755e-05
Step: 21180, train/epoch: 5.040456771850586
Step: 21190, train/loss: 0.008100000210106373
Step: 21190, train/grad_norm: 3.174521054916113e-07
Step: 21190, train/learning_rate: 2.478581700415816e-05
Step: 21190, train/epoch: 5.042836666107178
Step: 21200, train/loss: 0.0
Step: 21200, train/grad_norm: 6.958037829463137e-06
Step: 21200, train/learning_rate: 2.4773917175480165e-05
Step: 21200, train/epoch: 5.0452165603637695
Step: 21210, train/loss: 0.0
Step: 21210, train/grad_norm: 0.000438829098129645
Step: 21210, train/learning_rate: 2.4762017346802168e-05
Step: 21210, train/epoch: 5.047596454620361
Step: 21220, train/loss: 0.0
Step: 21220, train/grad_norm: 9.752504865900846e-07
Step: 21220, train/learning_rate: 2.4750119337113574e-05
Step: 21220, train/epoch: 5.049976348876953
Step: 21230, train/loss: 0.0
Step: 21230, train/grad_norm: 1.3116017782976996e-07
Step: 21230, train/learning_rate: 2.4738219508435577e-05
Step: 21230, train/epoch: 5.052356243133545
Step: 21240, train/loss: 0.0
Step: 21240, train/grad_norm: 1.126810843743442e-06
Step: 21240, train/learning_rate: 2.4726321498746984e-05
Step: 21240, train/epoch: 5.0547356605529785
Step: 21250, train/loss: 0.0
Step: 21250, train/grad_norm: 1.4376237231772393e-05
Step: 21250, train/learning_rate: 2.4714421670068987e-05
Step: 21250, train/epoch: 5.05711555480957
Step: 21260, train/loss: 0.0
Step: 21260, train/grad_norm: 2.2577469280804507e-05
Step: 21260, train/learning_rate: 2.470252184139099e-05
Step: 21260, train/epoch: 5.059495449066162
Step: 21270, train/loss: 0.0
Step: 21270, train/grad_norm: 4.6754050231356814e-07
Step: 21270, train/learning_rate: 2.4690623831702396e-05
Step: 21270, train/epoch: 5.061875343322754
Step: 21280, train/loss: 0.0
Step: 21280, train/grad_norm: 1.7481204849900678e-05
Step: 21280, train/learning_rate: 2.46787240030244e-05
Step: 21280, train/epoch: 5.064255237579346
Step: 21290, train/loss: 0.0
Step: 21290, train/grad_norm: 1.017704198602587e-05
Step: 21290, train/learning_rate: 2.4666825993335806e-05
Step: 21290, train/epoch: 5.0666351318359375
Step: 21300, train/loss: 0.0
Step: 21300, train/grad_norm: 3.070173875130422e-07
Step: 21300, train/learning_rate: 2.465492616465781e-05
Step: 21300, train/epoch: 5.069014549255371
Step: 21310, train/loss: 0.0
Step: 21310, train/grad_norm: 5.236495326244039e-07
Step: 21310, train/learning_rate: 2.4643026335979812e-05
Step: 21310, train/epoch: 5.071394443511963
Step: 21320, train/loss: 0.0
Step: 21320, train/grad_norm: 1.0914587988963831e-07
Step: 21320, train/learning_rate: 2.463112832629122e-05
Step: 21320, train/epoch: 5.073774337768555
Step: 21330, train/loss: 0.0
Step: 21330, train/grad_norm: 1.2121778354412527e-06
Step: 21330, train/learning_rate: 2.461922849761322e-05
Step: 21330, train/epoch: 5.0761542320251465
Step: 21340, train/loss: 0.0
Step: 21340, train/grad_norm: 3.111925082066591e-07
Step: 21340, train/learning_rate: 2.4607330487924628e-05
Step: 21340, train/epoch: 5.078534126281738
Step: 21350, train/loss: 0.0
Step: 21350, train/grad_norm: 1.4480499430646887e-06
Step: 21350, train/learning_rate: 2.459543065924663e-05
Step: 21350, train/epoch: 5.08091402053833
Step: 21360, train/loss: 0.0
Step: 21360, train/grad_norm: 4.551090114546241e-06
Step: 21360, train/learning_rate: 2.4583530830568634e-05
Step: 21360, train/epoch: 5.083293437957764
Step: 21370, train/loss: 0.0
Step: 21370, train/grad_norm: 2.9747849339401e-07
Step: 21370, train/learning_rate: 2.457163282088004e-05
Step: 21370, train/epoch: 5.0856733322143555
Step: 21380, train/loss: 0.14300000667572021
Step: 21380, train/grad_norm: 0.00047931974404491484
Step: 21380, train/learning_rate: 2.4559732992202044e-05
Step: 21380, train/epoch: 5.088053226470947
Step: 21390, train/loss: 9.999999747378752e-05
Step: 21390, train/grad_norm: 0.0006242418312467635
Step: 21390, train/learning_rate: 2.454783498251345e-05
Step: 21390, train/epoch: 5.090433120727539
Step: 21400, train/loss: 0.0
Step: 21400, train/grad_norm: 0.0005615352420136333
Step: 21400, train/learning_rate: 2.4535935153835453e-05
Step: 21400, train/epoch: 5.092813014984131
Step: 21410, train/loss: 0.0
Step: 21410, train/grad_norm: 3.17881531373132e-05
Step: 21410, train/learning_rate: 2.4524035325157456e-05
Step: 21410, train/epoch: 5.095192909240723
Step: 21420, train/loss: 0.0
Step: 21420, train/grad_norm: 2.6236328267259523e-05
Step: 21420, train/learning_rate: 2.4512137315468863e-05
Step: 21420, train/epoch: 5.0975728034973145
Step: 21430, train/loss: 0.0
Step: 21430, train/grad_norm: 0.15943515300750732
Step: 21430, train/learning_rate: 2.4500237486790866e-05
Step: 21430, train/epoch: 5.099952220916748
Step: 21440, train/loss: 0.0
Step: 21440, train/grad_norm: 1.8479063612630853e-07
Step: 21440, train/learning_rate: 2.4488339477102272e-05
Step: 21440, train/epoch: 5.10233211517334
Step: 21450, train/loss: 0.0
Step: 21450, train/grad_norm: 4.326798807596788e-05
Step: 21450, train/learning_rate: 2.4476439648424275e-05
Step: 21450, train/epoch: 5.104712009429932
Step: 21460, train/loss: 0.0
Step: 21460, train/grad_norm: 0.0004990511806681752
Step: 21460, train/learning_rate: 2.4464539819746278e-05
Step: 21460, train/epoch: 5.107091903686523
Step: 21470, train/loss: 0.0
Step: 21470, train/grad_norm: 0.002058543497696519
Step: 21470, train/learning_rate: 2.4452641810057685e-05
Step: 21470, train/epoch: 5.109471797943115
Step: 21480, train/loss: 0.0
Step: 21480, train/grad_norm: 2.1397179139626132e-08
Step: 21480, train/learning_rate: 2.4440741981379688e-05
Step: 21480, train/epoch: 5.111851692199707
Step: 21490, train/loss: 0.0
Step: 21490, train/grad_norm: 0.005898150149732828
Step: 21490, train/learning_rate: 2.4428843971691094e-05
Step: 21490, train/epoch: 5.114231109619141
Step: 21500, train/loss: 0.0
Step: 21500, train/grad_norm: 9.84859216401901e-09
Step: 21500, train/learning_rate: 2.4416944143013097e-05
Step: 21500, train/epoch: 5.116611003875732
Step: 21510, train/loss: 0.0
Step: 21510, train/grad_norm: 1.0298332853153624e-07
Step: 21510, train/learning_rate: 2.44050443143351e-05
Step: 21510, train/epoch: 5.118990898132324
Step: 21520, train/loss: 0.0
Step: 21520, train/grad_norm: 2.7525292267682744e-08
Step: 21520, train/learning_rate: 2.4393146304646507e-05
Step: 21520, train/epoch: 5.121370792388916
Step: 21530, train/loss: 0.0
Step: 21530, train/grad_norm: 3.421211545173719e-07
Step: 21530, train/learning_rate: 2.438124647596851e-05
Step: 21530, train/epoch: 5.123750686645508
Step: 21540, train/loss: 0.0
Step: 21540, train/grad_norm: 1.928578718946028e-08
Step: 21540, train/learning_rate: 2.4369348466279916e-05
Step: 21540, train/epoch: 5.1261305809021
Step: 21550, train/loss: 0.0
Step: 21550, train/grad_norm: 4.6519369334419025e-07
Step: 21550, train/learning_rate: 2.435744863760192e-05
Step: 21550, train/epoch: 5.128509998321533
Step: 21560, train/loss: 0.0
Step: 21560, train/grad_norm: 1.1495030776131898e-06
Step: 21560, train/learning_rate: 2.4345550627913326e-05
Step: 21560, train/epoch: 5.130889892578125
Step: 21570, train/loss: 0.0
Step: 21570, train/grad_norm: 4.231892307871021e-05
Step: 21570, train/learning_rate: 2.433365079923533e-05
Step: 21570, train/epoch: 5.133269786834717
Step: 21580, train/loss: 0.0
Step: 21580, train/grad_norm: 8.644457921036519e-06
Step: 21580, train/learning_rate: 2.4321750970557332e-05
Step: 21580, train/epoch: 5.135649681091309
Step: 21590, train/loss: 0.0
Step: 21590, train/grad_norm: 1.5774993755712785e-07
Step: 21590, train/learning_rate: 2.430985296086874e-05
Step: 21590, train/epoch: 5.1380295753479
Step: 21600, train/loss: 9.999999747378752e-05
Step: 21600, train/grad_norm: 4.61642315485733e-07
Step: 21600, train/learning_rate: 2.429795313219074e-05
Step: 21600, train/epoch: 5.140409469604492
Step: 21610, train/loss: 0.11720000207424164
Step: 21610, train/grad_norm: 1.1089286289234224e-07
Step: 21610, train/learning_rate: 2.4286055122502148e-05
Step: 21610, train/epoch: 5.142789363861084
Step: 21620, train/loss: 0.00019999999494757503
Step: 21620, train/grad_norm: 5.871689268133196e-07
Step: 21620, train/learning_rate: 2.427415529382415e-05
Step: 21620, train/epoch: 5.145168781280518
Step: 21630, train/loss: 0.0
Step: 21630, train/grad_norm: 2.4018376279855147e-05
Step: 21630, train/learning_rate: 2.4262255465146154e-05
Step: 21630, train/epoch: 5.147548675537109
Step: 21640, train/loss: 0.0
Step: 21640, train/grad_norm: 5.556419213625929e-10
Step: 21640, train/learning_rate: 2.425035745545756e-05
Step: 21640, train/epoch: 5.149928569793701
Step: 21650, train/loss: 0.05389999970793724
Step: 21650, train/grad_norm: 1.3190918934924412e-06
Step: 21650, train/learning_rate: 2.4238457626779564e-05
Step: 21650, train/epoch: 5.152308464050293
Step: 21660, train/loss: 0.0
Step: 21660, train/grad_norm: 2.608878979515339e-08
Step: 21660, train/learning_rate: 2.422655961709097e-05
Step: 21660, train/epoch: 5.154688358306885
Step: 21670, train/loss: 0.0
Step: 21670, train/grad_norm: 7.937877626318368e-07
Step: 21670, train/learning_rate: 2.4214659788412973e-05
Step: 21670, train/epoch: 5.157068252563477
Step: 21680, train/loss: 0.0
Step: 21680, train/grad_norm: 5.577449968541259e-08
Step: 21680, train/learning_rate: 2.4202759959734976e-05
Step: 21680, train/epoch: 5.15944766998291
Step: 21690, train/loss: 0.0
Step: 21690, train/grad_norm: 0.33009371161460876
Step: 21690, train/learning_rate: 2.4190861950046383e-05
Step: 21690, train/epoch: 5.161827564239502
Step: 21700, train/loss: 9.999999747378752e-05
Step: 21700, train/grad_norm: 2.8785953531951236e-07
Step: 21700, train/learning_rate: 2.4178962121368386e-05
Step: 21700, train/epoch: 5.164207458496094
Step: 21710, train/loss: 0.0
Step: 21710, train/grad_norm: 4.471349711820949e-06
Step: 21710, train/learning_rate: 2.4167064111679792e-05
Step: 21710, train/epoch: 5.1665873527526855
Step: 21720, train/loss: 0.0
Step: 21720, train/grad_norm: 7.954356533446116e-07
Step: 21720, train/learning_rate: 2.4155164283001795e-05
Step: 21720, train/epoch: 5.168967247009277
Step: 21730, train/loss: 0.0
Step: 21730, train/grad_norm: 2.484793810708652e-07
Step: 21730, train/learning_rate: 2.41432644543238e-05
Step: 21730, train/epoch: 5.171347141265869
Step: 21740, train/loss: 0.0
Step: 21740, train/grad_norm: 0.001809507142752409
Step: 21740, train/learning_rate: 2.4131366444635205e-05
Step: 21740, train/epoch: 5.173726558685303
Step: 21750, train/loss: 0.08749999850988388
Step: 21750, train/grad_norm: 2.5659004677436315e-05
Step: 21750, train/learning_rate: 2.4119466615957208e-05
Step: 21750, train/epoch: 5.1761064529418945
Step: 21760, train/loss: 0.0
Step: 21760, train/grad_norm: 6.804204844002015e-08
Step: 21760, train/learning_rate: 2.4107568606268615e-05
Step: 21760, train/epoch: 5.178486347198486
Step: 21770, train/loss: 0.0
Step: 21770, train/grad_norm: 3.739246778877714e-08
Step: 21770, train/learning_rate: 2.4095668777590618e-05
Step: 21770, train/epoch: 5.180866241455078
Step: 21780, train/loss: 0.0
Step: 21780, train/grad_norm: 1.7790165429687477e-07
Step: 21780, train/learning_rate: 2.408376894891262e-05
Step: 21780, train/epoch: 5.18324613571167
Step: 21790, train/loss: 0.0
Step: 21790, train/grad_norm: 1.612016058061272e-05
Step: 21790, train/learning_rate: 2.4071870939224027e-05
Step: 21790, train/epoch: 5.185626029968262
Step: 21800, train/loss: 0.0
Step: 21800, train/grad_norm: 8.577811740906327e-07
Step: 21800, train/learning_rate: 2.405997111054603e-05
Step: 21800, train/epoch: 5.1880059242248535
Step: 21810, train/loss: 0.0
Step: 21810, train/grad_norm: 4.5370225620899873e-07
Step: 21810, train/learning_rate: 2.4048073100857437e-05
Step: 21810, train/epoch: 5.190385341644287
Step: 21820, train/loss: 0.0
Step: 21820, train/grad_norm: 2.1020454369136132e-05
Step: 21820, train/learning_rate: 2.403617327217944e-05
Step: 21820, train/epoch: 5.192765235900879
Step: 21830, train/loss: 0.0
Step: 21830, train/grad_norm: 6.778986403332965e-07
Step: 21830, train/learning_rate: 2.4024273443501443e-05
Step: 21830, train/epoch: 5.195145130157471
Step: 21840, train/loss: 0.0
Step: 21840, train/grad_norm: 2.6864594474318437e-05
Step: 21840, train/learning_rate: 2.401237543381285e-05
Step: 21840, train/epoch: 5.1975250244140625
Step: 21850, train/loss: 0.0
Step: 21850, train/grad_norm: 1.376988166157389e-06
Step: 21850, train/learning_rate: 2.4000475605134852e-05
Step: 21850, train/epoch: 5.199904918670654
Step: 21860, train/loss: 0.0
Step: 21860, train/grad_norm: 8.102153969957726e-07
Step: 21860, train/learning_rate: 2.398857759544626e-05
Step: 21860, train/epoch: 5.202284812927246
Step: 21870, train/loss: 0.0
Step: 21870, train/grad_norm: 3.478723738226108e-05
Step: 21870, train/learning_rate: 2.3976677766768262e-05
Step: 21870, train/epoch: 5.20466423034668
Step: 21880, train/loss: 0.0
Step: 21880, train/grad_norm: 1.2969559293196653e-07
Step: 21880, train/learning_rate: 2.3964777938090265e-05
Step: 21880, train/epoch: 5.2070441246032715
Step: 21890, train/loss: 0.0
Step: 21890, train/grad_norm: 7.553011869276816e-07
Step: 21890, train/learning_rate: 2.395287992840167e-05
Step: 21890, train/epoch: 5.209424018859863
Step: 21900, train/loss: 0.0
Step: 21900, train/grad_norm: 1.044891178025864e-06
Step: 21900, train/learning_rate: 2.3940980099723674e-05
Step: 21900, train/epoch: 5.211803913116455
Step: 21910, train/loss: 0.0
Step: 21910, train/grad_norm: 1.1559280643780312e-08
Step: 21910, train/learning_rate: 2.392908209003508e-05
Step: 21910, train/epoch: 5.214183807373047
Step: 21920, train/loss: 0.0
Step: 21920, train/grad_norm: 5.483195309352595e-06
Step: 21920, train/learning_rate: 2.3917182261357084e-05
Step: 21920, train/epoch: 5.216563701629639
Step: 21930, train/loss: 0.0
Step: 21930, train/grad_norm: 1.1135688509966712e-05
Step: 21930, train/learning_rate: 2.3905282432679087e-05
Step: 21930, train/epoch: 5.2189435958862305
Step: 21940, train/loss: 0.0
Step: 21940, train/grad_norm: 1.4218063881799026e-07
Step: 21940, train/learning_rate: 2.3893384422990493e-05
Step: 21940, train/epoch: 5.221323013305664
Step: 21950, train/loss: 0.0
Step: 21950, train/grad_norm: 0.00011129970516776666
Step: 21950, train/learning_rate: 2.3881484594312496e-05
Step: 21950, train/epoch: 5.223702907562256
Step: 21960, train/loss: 0.0
Step: 21960, train/grad_norm: 1.1765827139242901e-06
Step: 21960, train/learning_rate: 2.3869586584623903e-05
Step: 21960, train/epoch: 5.226082801818848
Step: 21970, train/loss: 0.0
Step: 21970, train/grad_norm: 2.590303120086901e-05
Step: 21970, train/learning_rate: 2.3857686755945906e-05
Step: 21970, train/epoch: 5.2284626960754395
Step: 21980, train/loss: 0.0
Step: 21980, train/grad_norm: 2.6156088850370907e-08
Step: 21980, train/learning_rate: 2.384578692726791e-05
Step: 21980, train/epoch: 5.230842590332031
Step: 21990, train/loss: 0.0
Step: 21990, train/grad_norm: 4.4916671981809486e-07
Step: 21990, train/learning_rate: 2.3833888917579316e-05
Step: 21990, train/epoch: 5.233222484588623
Step: 22000, train/loss: 0.0
Step: 22000, train/grad_norm: 4.532533992573917e-08
Step: 22000, train/learning_rate: 2.382198908890132e-05
Step: 22000, train/epoch: 5.235601902008057
Step: 22010, train/loss: 0.0
Step: 22010, train/grad_norm: 8.536013046978042e-05
Step: 22010, train/learning_rate: 2.3810091079212725e-05
Step: 22010, train/epoch: 5.237981796264648
Step: 22020, train/loss: 0.0
Step: 22020, train/grad_norm: 1.9424199138029508e-08
Step: 22020, train/learning_rate: 2.3798191250534728e-05
Step: 22020, train/epoch: 5.24036169052124
Step: 22030, train/loss: 0.0
Step: 22030, train/grad_norm: 1.523868320418842e-08
Step: 22030, train/learning_rate: 2.378629142185673e-05
Step: 22030, train/epoch: 5.242741584777832
Step: 22040, train/loss: 0.0
Step: 22040, train/grad_norm: 4.728793555841548e-06
Step: 22040, train/learning_rate: 2.3774393412168138e-05
Step: 22040, train/epoch: 5.245121479034424
Step: 22050, train/loss: 0.0
Step: 22050, train/grad_norm: 7.835478932349815e-09
Step: 22050, train/learning_rate: 2.376249358349014e-05
Step: 22050, train/epoch: 5.247501373291016
Step: 22060, train/loss: 0.0
Step: 22060, train/grad_norm: 1.0234119329766145e-08
Step: 22060, train/learning_rate: 2.3750595573801547e-05
Step: 22060, train/epoch: 5.249880790710449
Step: 22070, train/loss: 0.0
Step: 22070, train/grad_norm: 2.736343596154711e-09
Step: 22070, train/learning_rate: 2.373869574512355e-05
Step: 22070, train/epoch: 5.252260684967041
Step: 22080, train/loss: 0.0
Step: 22080, train/grad_norm: 3.2336894946638495e-05
Step: 22080, train/learning_rate: 2.3726795916445553e-05
Step: 22080, train/epoch: 5.254640579223633
Step: 22090, train/loss: 0.0
Step: 22090, train/grad_norm: 8.957627869676799e-06
Step: 22090, train/learning_rate: 2.371489790675696e-05
Step: 22090, train/epoch: 5.257020473480225
Step: 22100, train/loss: 0.0
Step: 22100, train/grad_norm: 1.6534914548671509e-09
Step: 22100, train/learning_rate: 2.3702998078078963e-05
Step: 22100, train/epoch: 5.259400367736816
Step: 22110, train/loss: 0.0
Step: 22110, train/grad_norm: 1.0566257060418138e-06
Step: 22110, train/learning_rate: 2.369110006839037e-05
Step: 22110, train/epoch: 5.261780261993408
Step: 22120, train/loss: 0.0
Step: 22120, train/grad_norm: 1.2857613640449017e-08
Step: 22120, train/learning_rate: 2.3679200239712372e-05
Step: 22120, train/epoch: 5.26416015625
Step: 22130, train/loss: 0.14839999377727509
Step: 22130, train/grad_norm: 144.26234436035156
Step: 22130, train/learning_rate: 2.3667300411034375e-05
Step: 22130, train/epoch: 5.266539573669434
Step: 22140, train/loss: 0.0
Step: 22140, train/grad_norm: 7.020292105153203e-05
Step: 22140, train/learning_rate: 2.3655402401345782e-05
Step: 22140, train/epoch: 5.268919467926025
Step: 22150, train/loss: 0.0
Step: 22150, train/grad_norm: 0.0021098745055496693
Step: 22150, train/learning_rate: 2.3643502572667785e-05
Step: 22150, train/epoch: 5.271299362182617
Step: 22160, train/loss: 0.008700000122189522
Step: 22160, train/grad_norm: 0.0002282027853652835
Step: 22160, train/learning_rate: 2.363160456297919e-05
Step: 22160, train/epoch: 5.273679256439209
Step: 22170, train/loss: 0.0
Step: 22170, train/grad_norm: 2.8133765226812102e-05
Step: 22170, train/learning_rate: 2.3619704734301195e-05
Step: 22170, train/epoch: 5.276059150695801
Step: 22180, train/loss: 0.0
Step: 22180, train/grad_norm: 9.247475674101224e-08
Step: 22180, train/learning_rate: 2.3607804905623198e-05
Step: 22180, train/epoch: 5.278439044952393
Step: 22190, train/loss: 0.0
Step: 22190, train/grad_norm: 9.343987585452851e-06
Step: 22190, train/learning_rate: 2.3595906895934604e-05
Step: 22190, train/epoch: 5.280818462371826
Step: 22200, train/loss: 0.1664000004529953
Step: 22200, train/grad_norm: 137.8472900390625
Step: 22200, train/learning_rate: 2.3584007067256607e-05
Step: 22200, train/epoch: 5.283198356628418
Step: 22210, train/loss: 0.0
Step: 22210, train/grad_norm: 0.0011816034093499184
Step: 22210, train/learning_rate: 2.3572109057568014e-05
Step: 22210, train/epoch: 5.28557825088501
Step: 22220, train/loss: 9.999999747378752e-05
Step: 22220, train/grad_norm: 0.024905720725655556
Step: 22220, train/learning_rate: 2.3560209228890017e-05
Step: 22220, train/epoch: 5.287958145141602
Step: 22230, train/loss: 0.0
Step: 22230, train/grad_norm: 0.00011221653403481469
Step: 22230, train/learning_rate: 2.3548311219201423e-05
Step: 22230, train/epoch: 5.290338039398193
Step: 22240, train/loss: 9.999999747378752e-05
Step: 22240, train/grad_norm: 4.221759445499629e-05
Step: 22240, train/learning_rate: 2.3536411390523426e-05
Step: 22240, train/epoch: 5.292717933654785
Step: 22250, train/loss: 0.0
Step: 22250, train/grad_norm: 4.213347710901871e-05
Step: 22250, train/learning_rate: 2.352451156184543e-05
Step: 22250, train/epoch: 5.295097351074219
Step: 22260, train/loss: 0.008700000122189522
Step: 22260, train/grad_norm: 124.62593841552734
Step: 22260, train/learning_rate: 2.3512613552156836e-05
Step: 22260, train/epoch: 5.2974772453308105
Step: 22270, train/loss: 0.03440000116825104
Step: 22270, train/grad_norm: 530.5431518554688
Step: 22270, train/learning_rate: 2.350071372347884e-05
Step: 22270, train/epoch: 5.299857139587402
Step: 22280, train/loss: 0.0
Step: 22280, train/grad_norm: 1.007803831498677e-07
Step: 22280, train/learning_rate: 2.3488815713790245e-05
Step: 22280, train/epoch: 5.302237033843994
Step: 22290, train/loss: 0.0013000000035390258
Step: 22290, train/grad_norm: 3.805609139817534e-06
Step: 22290, train/learning_rate: 2.347691588511225e-05
Step: 22290, train/epoch: 5.304616928100586
Step: 22300, train/loss: 0.0
Step: 22300, train/grad_norm: 0.0008279191679321229
Step: 22300, train/learning_rate: 2.346501605643425e-05
Step: 22300, train/epoch: 5.306996822357178
Step: 22310, train/loss: 0.0
Step: 22310, train/grad_norm: 0.0026163654401898384
Step: 22310, train/learning_rate: 2.3453118046745658e-05
Step: 22310, train/epoch: 5.3093767166137695
Step: 22320, train/loss: 0.0
Step: 22320, train/grad_norm: 2.5307555915787816e-05
Step: 22320, train/learning_rate: 2.344121821806766e-05
Step: 22320, train/epoch: 5.311756134033203
Step: 22330, train/loss: 0.0
Step: 22330, train/grad_norm: 3.399666093173437e-05
Step: 22330, train/learning_rate: 2.3429320208379067e-05
Step: 22330, train/epoch: 5.314136028289795
Step: 22340, train/loss: 0.0
Step: 22340, train/grad_norm: 5.10206655235379e-06
Step: 22340, train/learning_rate: 2.341742037970107e-05
Step: 22340, train/epoch: 5.316515922546387
Step: 22350, train/loss: 0.0
Step: 22350, train/grad_norm: 0.00017209918587468565
Step: 22350, train/learning_rate: 2.3405520551023073e-05
Step: 22350, train/epoch: 5.3188958168029785
Step: 22360, train/loss: 0.0
Step: 22360, train/grad_norm: 7.109234957169974e-06
Step: 22360, train/learning_rate: 2.339362254133448e-05
Step: 22360, train/epoch: 5.32127571105957
Step: 22370, train/loss: 0.0
Step: 22370, train/grad_norm: 0.048871010541915894
Step: 22370, train/learning_rate: 2.3381722712656483e-05
Step: 22370, train/epoch: 5.323655605316162
Step: 22380, train/loss: 0.0
Step: 22380, train/grad_norm: 2.288022187713068e-05
Step: 22380, train/learning_rate: 2.336982470296789e-05
Step: 22380, train/epoch: 5.326035022735596
Step: 22390, train/loss: 0.0
Step: 22390, train/grad_norm: 4.5243690692586824e-05
Step: 22390, train/learning_rate: 2.3357924874289893e-05
Step: 22390, train/epoch: 5.3284149169921875
Step: 22400, train/loss: 0.0
Step: 22400, train/grad_norm: 0.0016561511438339949
Step: 22400, train/learning_rate: 2.3346025045611896e-05
Step: 22400, train/epoch: 5.330794811248779
Step: 22410, train/loss: 0.0
Step: 22410, train/grad_norm: 2.9729873858741485e-05
Step: 22410, train/learning_rate: 2.3334127035923302e-05
Step: 22410, train/epoch: 5.333174705505371
Step: 22420, train/loss: 0.0
Step: 22420, train/grad_norm: 6.748837677150732e-06
Step: 22420, train/learning_rate: 2.3322227207245305e-05
Step: 22420, train/epoch: 5.335554599761963
Step: 22430, train/loss: 0.0
Step: 22430, train/grad_norm: 2.523283001210075e-05
Step: 22430, train/learning_rate: 2.3310329197556712e-05
Step: 22430, train/epoch: 5.337934494018555
Step: 22440, train/loss: 0.0
Step: 22440, train/grad_norm: 1.1335791896271985e-05
Step: 22440, train/learning_rate: 2.3298429368878715e-05
Step: 22440, train/epoch: 5.340313911437988
Step: 22450, train/loss: 0.0
Step: 22450, train/grad_norm: 0.00016062892973423004
Step: 22450, train/learning_rate: 2.3286529540200718e-05
Step: 22450, train/epoch: 5.34269380569458
Step: 22460, train/loss: 0.0
Step: 22460, train/grad_norm: 1.733078170218505e-05
Step: 22460, train/learning_rate: 2.3274631530512124e-05
Step: 22460, train/epoch: 5.345073699951172
Step: 22470, train/loss: 0.0
Step: 22470, train/grad_norm: 0.00018286814156454057
Step: 22470, train/learning_rate: 2.3262731701834127e-05
Step: 22470, train/epoch: 5.347453594207764
Step: 22480, train/loss: 0.0
Step: 22480, train/grad_norm: 2.618037979118526e-05
Step: 22480, train/learning_rate: 2.3250833692145534e-05
Step: 22480, train/epoch: 5.3498334884643555
Step: 22490, train/loss: 0.0
Step: 22490, train/grad_norm: 0.00013170699821785092
Step: 22490, train/learning_rate: 2.3238933863467537e-05
Step: 22490, train/epoch: 5.352213382720947
Step: 22500, train/loss: 0.0
Step: 22500, train/grad_norm: 3.301197648397647e-05
Step: 22500, train/learning_rate: 2.322703403478954e-05
Step: 22500, train/epoch: 5.354593276977539
Step: 22510, train/loss: 0.0
Step: 22510, train/grad_norm: 3.475940684438683e-05
Step: 22510, train/learning_rate: 2.3215136025100946e-05
Step: 22510, train/epoch: 5.356972694396973
Step: 22520, train/loss: 0.0
Step: 22520, train/grad_norm: 2.9492488465621136e-05
Step: 22520, train/learning_rate: 2.320323619642295e-05
Step: 22520, train/epoch: 5.3593525886535645
Step: 22530, train/loss: 0.0
Step: 22530, train/grad_norm: 6.385673623299226e-05
Step: 22530, train/learning_rate: 2.3191338186734356e-05
Step: 22530, train/epoch: 5.361732482910156
Step: 22540, train/loss: 9.999999747378752e-05
Step: 22540, train/grad_norm: 9.807890455704182e-05
Step: 22540, train/learning_rate: 2.317943835805636e-05
Step: 22540, train/epoch: 5.364112377166748
Step: 22550, train/loss: 0.0
Step: 22550, train/grad_norm: 7.758805622870568e-06
Step: 22550, train/learning_rate: 2.3167538529378362e-05
Step: 22550, train/epoch: 5.36649227142334
Step: 22560, train/loss: 0.0
Step: 22560, train/grad_norm: 0.0004161162069067359
Step: 22560, train/learning_rate: 2.315564051968977e-05
Step: 22560, train/epoch: 5.368872165679932
Step: 22570, train/loss: 0.0
Step: 22570, train/grad_norm: 1.9652550690807402e-05
Step: 22570, train/learning_rate: 2.314374069101177e-05
Step: 22570, train/epoch: 5.371251583099365
Step: 22580, train/loss: 0.11640000343322754
Step: 22580, train/grad_norm: 0.01106896623969078
Step: 22580, train/learning_rate: 2.3131842681323178e-05
Step: 22580, train/epoch: 5.373631477355957
Step: 22590, train/loss: 0.0
Step: 22590, train/grad_norm: 9.584755389369093e-06
Step: 22590, train/learning_rate: 2.311994285264518e-05
Step: 22590, train/epoch: 5.376011371612549
Step: 22600, train/loss: 0.0
Step: 22600, train/grad_norm: 3.4078788303304464e-05
Step: 22600, train/learning_rate: 2.3108043023967184e-05
Step: 22600, train/epoch: 5.378391265869141
Step: 22610, train/loss: 0.0
Step: 22610, train/grad_norm: 8.204230539377022e-07
Step: 22610, train/learning_rate: 2.309614501427859e-05
Step: 22610, train/epoch: 5.380771160125732
Step: 22620, train/loss: 0.0
Step: 22620, train/grad_norm: 6.811161711084424e-06
Step: 22620, train/learning_rate: 2.3084245185600594e-05
Step: 22620, train/epoch: 5.383151054382324
Step: 22630, train/loss: 0.0
Step: 22630, train/grad_norm: 1.350573711533798e-06
Step: 22630, train/learning_rate: 2.3072347175912e-05
Step: 22630, train/epoch: 5.385530471801758
Step: 22640, train/loss: 0.0
Step: 22640, train/grad_norm: 0.00440430361777544
Step: 22640, train/learning_rate: 2.3060447347234003e-05
Step: 22640, train/epoch: 5.38791036605835
Step: 22650, train/loss: 0.0
Step: 22650, train/grad_norm: 1.5215746316243894e-06
Step: 22650, train/learning_rate: 2.3048547518556006e-05
Step: 22650, train/epoch: 5.390290260314941
Step: 22660, train/loss: 0.0
Step: 22660, train/grad_norm: 3.5480803717291565e-07
Step: 22660, train/learning_rate: 2.3036649508867413e-05
Step: 22660, train/epoch: 5.392670154571533
Step: 22670, train/loss: 0.0
Step: 22670, train/grad_norm: 0.008923746645450592
Step: 22670, train/learning_rate: 2.3024749680189416e-05
Step: 22670, train/epoch: 5.395050048828125
Step: 22680, train/loss: 0.0
Step: 22680, train/grad_norm: 4.9307072913507e-07
Step: 22680, train/learning_rate: 2.3012851670500822e-05
Step: 22680, train/epoch: 5.397429943084717
Step: 22690, train/loss: 0.0
Step: 22690, train/grad_norm: 0.00010111511073773727
Step: 22690, train/learning_rate: 2.3000951841822825e-05
Step: 22690, train/epoch: 5.399809837341309
Step: 22700, train/loss: 0.0
Step: 22700, train/grad_norm: 1.1081296179327182e-05
Step: 22700, train/learning_rate: 2.298905201314483e-05
Step: 22700, train/epoch: 5.402189254760742
Step: 22710, train/loss: 0.0
Step: 22710, train/grad_norm: 1.8385206203674898e-05
Step: 22710, train/learning_rate: 2.2977154003456235e-05
Step: 22710, train/epoch: 5.404569149017334
Step: 22720, train/loss: 0.0
Step: 22720, train/grad_norm: 7.03865589457564e-05
Step: 22720, train/learning_rate: 2.2965254174778238e-05
Step: 22720, train/epoch: 5.406949043273926
Step: 22730, train/loss: 0.0
Step: 22730, train/grad_norm: 0.0014212849782779813
Step: 22730, train/learning_rate: 2.2953356165089644e-05
Step: 22730, train/epoch: 5.409328937530518
Step: 22740, train/loss: 0.0
Step: 22740, train/grad_norm: 2.9535876819863915e-05
Step: 22740, train/learning_rate: 2.2941456336411647e-05
Step: 22740, train/epoch: 5.411708831787109
Step: 22750, train/loss: 0.0
Step: 22750, train/grad_norm: 2.4200568077503704e-05
Step: 22750, train/learning_rate: 2.292955650773365e-05
Step: 22750, train/epoch: 5.414088726043701
Step: 22760, train/loss: 0.0
Step: 22760, train/grad_norm: 5.2777428209083155e-05
Step: 22760, train/learning_rate: 2.2917658498045057e-05
Step: 22760, train/epoch: 5.416468143463135
Step: 22770, train/loss: 0.18140000104904175
Step: 22770, train/grad_norm: 1.6406870599894319e-06
Step: 22770, train/learning_rate: 2.290575866936706e-05
Step: 22770, train/epoch: 5.418848037719727
Step: 22780, train/loss: 0.0
Step: 22780, train/grad_norm: 1.2443830200936645e-05
Step: 22780, train/learning_rate: 2.2893860659678467e-05
Step: 22780, train/epoch: 5.421227931976318
Step: 22790, train/loss: 0.0
Step: 22790, train/grad_norm: 0.0007273972150869668
Step: 22790, train/learning_rate: 2.288196083100047e-05
Step: 22790, train/epoch: 5.42360782623291
Step: 22800, train/loss: 9.999999747378752e-05
Step: 22800, train/grad_norm: 2.6089721814059885e-06
Step: 22800, train/learning_rate: 2.2870061002322473e-05
Step: 22800, train/epoch: 5.425987720489502
Step: 22810, train/loss: 0.0
Step: 22810, train/grad_norm: 0.0027257075998932123
Step: 22810, train/learning_rate: 2.285816299263388e-05
Step: 22810, train/epoch: 5.428367614746094
Step: 22820, train/loss: 0.0019000000320374966
Step: 22820, train/grad_norm: 3.179229224770097e-06
Step: 22820, train/learning_rate: 2.2846263163955882e-05
Step: 22820, train/epoch: 5.430747032165527
Step: 22830, train/loss: 0.0
Step: 22830, train/grad_norm: 6.070914423617069e-06
Step: 22830, train/learning_rate: 2.283436515426729e-05
Step: 22830, train/epoch: 5.433126926422119
Step: 22840, train/loss: 0.0
Step: 22840, train/grad_norm: 5.2182549552526325e-06
Step: 22840, train/learning_rate: 2.2822465325589292e-05
Step: 22840, train/epoch: 5.435506820678711
Step: 22850, train/loss: 0.0
Step: 22850, train/grad_norm: 0.00011329593689879403
Step: 22850, train/learning_rate: 2.2810565496911295e-05
Step: 22850, train/epoch: 5.437886714935303
Step: 22860, train/loss: 0.0
Step: 22860, train/grad_norm: 1.8337979781790636e-05
Step: 22860, train/learning_rate: 2.27986674872227e-05
Step: 22860, train/epoch: 5.4402666091918945
Step: 22870, train/loss: 0.0
Step: 22870, train/grad_norm: 0.0006199792842380702
Step: 22870, train/learning_rate: 2.2786767658544704e-05
Step: 22870, train/epoch: 5.442646503448486
Step: 22880, train/loss: 0.0019000000320374966
Step: 22880, train/grad_norm: 5.750295031248243e-07
Step: 22880, train/learning_rate: 2.277486964885611e-05
Step: 22880, train/epoch: 5.445026397705078
Step: 22890, train/loss: 0.0
Step: 22890, train/grad_norm: 4.3715479591810436e-07
Step: 22890, train/learning_rate: 2.2762969820178114e-05
Step: 22890, train/epoch: 5.447405815124512
Step: 22900, train/loss: 0.0
Step: 22900, train/grad_norm: 4.339883162174374e-06
Step: 22900, train/learning_rate: 2.275107181048952e-05
Step: 22900, train/epoch: 5.4497857093811035
Step: 22910, train/loss: 0.0
Step: 22910, train/grad_norm: 2.816641199387959e-06
Step: 22910, train/learning_rate: 2.2739171981811523e-05
Step: 22910, train/epoch: 5.452165603637695
Step: 22920, train/loss: 0.0
Step: 22920, train/grad_norm: 2.4914901587180793e-05
Step: 22920, train/learning_rate: 2.2727272153133526e-05
Step: 22920, train/epoch: 5.454545497894287
Step: 22930, train/loss: 0.0
Step: 22930, train/grad_norm: 0.0002617604623083025
Step: 22930, train/learning_rate: 2.2715374143444933e-05
Step: 22930, train/epoch: 5.456925392150879
Step: 22940, train/loss: 0.0
Step: 22940, train/grad_norm: 7.353382898145355e-06
Step: 22940, train/learning_rate: 2.2703474314766936e-05
Step: 22940, train/epoch: 5.459305286407471
Step: 22950, train/loss: 0.0
Step: 22950, train/grad_norm: 6.45235923002474e-06
Step: 22950, train/learning_rate: 2.2691576305078343e-05
Step: 22950, train/epoch: 5.461684703826904
Step: 22960, train/loss: 0.0
Step: 22960, train/grad_norm: 3.016071161709988e-07
Step: 22960, train/learning_rate: 2.2679676476400346e-05
Step: 22960, train/epoch: 5.464064598083496
Step: 22970, train/loss: 0.0
Step: 22970, train/grad_norm: 2.774267522909213e-05
Step: 22970, train/learning_rate: 2.266777664772235e-05
Step: 22970, train/epoch: 5.466444492340088
Step: 22980, train/loss: 0.0
Step: 22980, train/grad_norm: 3.5923541872762144e-05
Step: 22980, train/learning_rate: 2.2655878638033755e-05
Step: 22980, train/epoch: 5.46882438659668
Step: 22990, train/loss: 0.149399995803833
Step: 22990, train/grad_norm: 1.5573712062177947e-06
Step: 22990, train/learning_rate: 2.2643978809355758e-05
Step: 22990, train/epoch: 5.4712042808532715
Step: 23000, train/loss: 0.0
Step: 23000, train/grad_norm: 0.0005010762251913548
Step: 23000, train/learning_rate: 2.2632080799667165e-05
Step: 23000, train/epoch: 5.473584175109863
Step: 23010, train/loss: 0.0
Step: 23010, train/grad_norm: 0.0012277261121198535
Step: 23010, train/learning_rate: 2.2620180970989168e-05
Step: 23010, train/epoch: 5.475963592529297
Step: 23020, train/loss: 0.0
Step: 23020, train/grad_norm: 0.005923441611230373
Step: 23020, train/learning_rate: 2.260828114231117e-05
Step: 23020, train/epoch: 5.478343486785889
Step: 23030, train/loss: 0.0
Step: 23030, train/grad_norm: 2.2809113033872563e-06
Step: 23030, train/learning_rate: 2.2596383132622577e-05
Step: 23030, train/epoch: 5.4807233810424805
Step: 23040, train/loss: 0.0
Step: 23040, train/grad_norm: 5.085384600533871e-06
Step: 23040, train/learning_rate: 2.258448330394458e-05
Step: 23040, train/epoch: 5.483103275299072
Step: 23050, train/loss: 0.0
Step: 23050, train/grad_norm: 5.942973439232446e-05
Step: 23050, train/learning_rate: 2.2572585294255987e-05
Step: 23050, train/epoch: 5.485483169555664
Step: 23060, train/loss: 0.0
Step: 23060, train/grad_norm: 2.3768627954723343e-07
Step: 23060, train/learning_rate: 2.256068546557799e-05
Step: 23060, train/epoch: 5.487863063812256
Step: 23070, train/loss: 0.07890000194311142
Step: 23070, train/grad_norm: 2.179030343540944e-05
Step: 23070, train/learning_rate: 2.2548785636899993e-05
Step: 23070, train/epoch: 5.490242958068848
Step: 23080, train/loss: 0.0
Step: 23080, train/grad_norm: 2.4653103537275456e-05
Step: 23080, train/learning_rate: 2.25368876272114e-05
Step: 23080, train/epoch: 5.492622375488281
Step: 23090, train/loss: 0.0
Step: 23090, train/grad_norm: 9.097722795559093e-06
Step: 23090, train/learning_rate: 2.2524987798533402e-05
Step: 23090, train/epoch: 5.495002269744873
Step: 23100, train/loss: 0.06599999964237213
Step: 23100, train/grad_norm: 0.00010543299140408635
Step: 23100, train/learning_rate: 2.251308978884481e-05
Step: 23100, train/epoch: 5.497382164001465
Step: 23110, train/loss: 0.0
Step: 23110, train/grad_norm: 5.205055913393153e-06
Step: 23110, train/learning_rate: 2.2501189960166812e-05
Step: 23110, train/epoch: 5.499762058258057
Step: 23120, train/loss: 0.0
Step: 23120, train/grad_norm: 1.072384839062579e-05
Step: 23120, train/learning_rate: 2.2489290131488815e-05
Step: 23120, train/epoch: 5.502141952514648
Step: 23130, train/loss: 0.0
Step: 23130, train/grad_norm: 9.500918167759664e-06
Step: 23130, train/learning_rate: 2.247739212180022e-05
Step: 23130, train/epoch: 5.50452184677124
Step: 23140, train/loss: 0.0
Step: 23140, train/grad_norm: 6.54738032608293e-05
Step: 23140, train/learning_rate: 2.2465492293122225e-05
Step: 23140, train/epoch: 5.506901264190674
Step: 23150, train/loss: 0.0
Step: 23150, train/grad_norm: 6.633572411374189e-07
Step: 23150, train/learning_rate: 2.245359428343363e-05
Step: 23150, train/epoch: 5.509281158447266
Step: 23160, train/loss: 0.0
Step: 23160, train/grad_norm: 5.23262372098543e-07
Step: 23160, train/learning_rate: 2.2441694454755634e-05
Step: 23160, train/epoch: 5.511661052703857
Step: 23170, train/loss: 0.0
Step: 23170, train/grad_norm: 4.475117293623043e-06
Step: 23170, train/learning_rate: 2.2429794626077637e-05
Step: 23170, train/epoch: 5.514040946960449
Step: 23180, train/loss: 0.0
Step: 23180, train/grad_norm: 0.00017831241711974144
Step: 23180, train/learning_rate: 2.2417896616389044e-05
Step: 23180, train/epoch: 5.516420841217041
Step: 23190, train/loss: 0.0
Step: 23190, train/grad_norm: 7.109510988811962e-07
Step: 23190, train/learning_rate: 2.2405996787711047e-05
Step: 23190, train/epoch: 5.518800735473633
Step: 23200, train/loss: 9.999999747378752e-05
Step: 23200, train/grad_norm: 6.20309720034129e-07
Step: 23200, train/learning_rate: 2.2394098778022453e-05
Step: 23200, train/epoch: 5.521180152893066
Step: 23210, train/loss: 0.0
Step: 23210, train/grad_norm: 0.16983862221240997
Step: 23210, train/learning_rate: 2.2382198949344456e-05
Step: 23210, train/epoch: 5.523560047149658
Step: 23220, train/loss: 0.0
Step: 23220, train/grad_norm: 3.423071802899358e-06
Step: 23220, train/learning_rate: 2.237029912066646e-05
Step: 23220, train/epoch: 5.52593994140625
Step: 23230, train/loss: 0.0
Step: 23230, train/grad_norm: 8.712068932936745e-08
Step: 23230, train/learning_rate: 2.2358401110977866e-05
Step: 23230, train/epoch: 5.528319835662842
Step: 23240, train/loss: 0.0
Step: 23240, train/grad_norm: 2.2604486730415374e-05
Step: 23240, train/learning_rate: 2.234650128229987e-05
Step: 23240, train/epoch: 5.530699729919434
Step: 23250, train/loss: 0.06719999760389328
Step: 23250, train/grad_norm: 382.7406005859375
Step: 23250, train/learning_rate: 2.2334603272611275e-05
Step: 23250, train/epoch: 5.533079624176025
Step: 23260, train/loss: 0.0
Step: 23260, train/grad_norm: 8.841204544296488e-05
Step: 23260, train/learning_rate: 2.232270344393328e-05
Step: 23260, train/epoch: 5.535459518432617
Step: 23270, train/loss: 0.0
Step: 23270, train/grad_norm: 1.8744802332548716e-07
Step: 23270, train/learning_rate: 2.231080361525528e-05
Step: 23270, train/epoch: 5.537838935852051
Step: 23280, train/loss: 0.0
Step: 23280, train/grad_norm: 6.128257723503339e-07
Step: 23280, train/learning_rate: 2.2298905605566688e-05
Step: 23280, train/epoch: 5.540218830108643
Step: 23290, train/loss: 0.0
Step: 23290, train/grad_norm: 0.0005721125635318458
Step: 23290, train/learning_rate: 2.228700577688869e-05
Step: 23290, train/epoch: 5.542598724365234
Step: 23300, train/loss: 0.0
Step: 23300, train/grad_norm: 8.338871992918939e-08
Step: 23300, train/learning_rate: 2.2275107767200097e-05
Step: 23300, train/epoch: 5.544978618621826
Step: 23310, train/loss: 0.0
Step: 23310, train/grad_norm: 8.73799672262976e-06
Step: 23310, train/learning_rate: 2.22632079385221e-05
Step: 23310, train/epoch: 5.547358512878418
Step: 23320, train/loss: 0.0
Step: 23320, train/grad_norm: 0.00017820080392993987
Step: 23320, train/learning_rate: 2.2251308109844103e-05
Step: 23320, train/epoch: 5.54973840713501
Step: 23330, train/loss: 0.0
Step: 23330, train/grad_norm: 1.3857007843398605e-06
Step: 23330, train/learning_rate: 2.223941010015551e-05
Step: 23330, train/epoch: 5.552117824554443
Step: 23340, train/loss: 0.0
Step: 23340, train/grad_norm: 6.910011052241316e-07
Step: 23340, train/learning_rate: 2.2227510271477513e-05
Step: 23340, train/epoch: 5.554497718811035
Step: 23350, train/loss: 0.0
Step: 23350, train/grad_norm: 5.177348157303641e-06
Step: 23350, train/learning_rate: 2.221561226178892e-05
Step: 23350, train/epoch: 5.556877613067627
Step: 23360, train/loss: 0.0010000000474974513
Step: 23360, train/grad_norm: 1.0272639201502898e-06
Step: 23360, train/learning_rate: 2.2203712433110923e-05
Step: 23360, train/epoch: 5.559257507324219
Step: 23370, train/loss: 0.06369999796152115
Step: 23370, train/grad_norm: 250.7777099609375
Step: 23370, train/learning_rate: 2.2191812604432926e-05
Step: 23370, train/epoch: 5.5616374015808105
Step: 23380, train/loss: 0.0
Step: 23380, train/grad_norm: 1.6014438415368204e-06
Step: 23380, train/learning_rate: 2.2179914594744332e-05
Step: 23380, train/epoch: 5.564017295837402
Step: 23390, train/loss: 0.0
Step: 23390, train/grad_norm: 6.581962952623144e-05
Step: 23390, train/learning_rate: 2.2168014766066335e-05
Step: 23390, train/epoch: 5.566397190093994
Step: 23400, train/loss: 0.17030000686645508
Step: 23400, train/grad_norm: 2.0942559331160737e-07
Step: 23400, train/learning_rate: 2.2156116756377742e-05
Step: 23400, train/epoch: 5.568776607513428
Step: 23410, train/loss: 0.0
Step: 23410, train/grad_norm: 0.002616648096591234
Step: 23410, train/learning_rate: 2.2144216927699745e-05
Step: 23410, train/epoch: 5.5711565017700195
Step: 23420, train/loss: 9.999999747378752e-05
Step: 23420, train/grad_norm: 0.027871543541550636
Step: 23420, train/learning_rate: 2.2132317099021748e-05
Step: 23420, train/epoch: 5.573536396026611
Step: 23430, train/loss: 0.0003000000142492354
Step: 23430, train/grad_norm: 0.00010874612053157762
Step: 23430, train/learning_rate: 2.2120419089333154e-05
Step: 23430, train/epoch: 5.575916290283203
Step: 23440, train/loss: 0.0
Step: 23440, train/grad_norm: 8.866900316206738e-05
Step: 23440, train/learning_rate: 2.2108519260655157e-05
Step: 23440, train/epoch: 5.578296184539795
Step: 23450, train/loss: 0.0
Step: 23450, train/grad_norm: 0.00023126018641050905
Step: 23450, train/learning_rate: 2.2096621250966564e-05
Step: 23450, train/epoch: 5.580676078796387
Step: 23460, train/loss: 0.1436000019311905
Step: 23460, train/grad_norm: 0.0004646146553568542
Step: 23460, train/learning_rate: 2.2084721422288567e-05
Step: 23460, train/epoch: 5.58305549621582
Step: 23470, train/loss: 0.1363999992609024
Step: 23470, train/grad_norm: 2.209821468568407e-05
Step: 23470, train/learning_rate: 2.207282159361057e-05
Step: 23470, train/epoch: 5.585435390472412
Step: 23480, train/loss: 0.0
Step: 23480, train/grad_norm: 1.2683879504038487e-05
Step: 23480, train/learning_rate: 2.2060923583921976e-05
Step: 23480, train/epoch: 5.587815284729004
Step: 23490, train/loss: 0.0010000000474974513
Step: 23490, train/grad_norm: 15.260395050048828
Step: 23490, train/learning_rate: 2.204902375524398e-05
Step: 23490, train/epoch: 5.590195178985596
Step: 23500, train/loss: 0.0
Step: 23500, train/grad_norm: 0.00014410166477318853
Step: 23500, train/learning_rate: 2.2037125745555386e-05
Step: 23500, train/epoch: 5.5925750732421875
Step: 23510, train/loss: 0.0
Step: 23510, train/grad_norm: 7.073112556099659e-06
Step: 23510, train/learning_rate: 2.202522591687739e-05
Step: 23510, train/epoch: 5.594954967498779
Step: 23520, train/loss: 0.0
Step: 23520, train/grad_norm: 0.0007956954068504274
Step: 23520, train/learning_rate: 2.2013326088199392e-05
Step: 23520, train/epoch: 5.597334384918213
Step: 23530, train/loss: 0.0
Step: 23530, train/grad_norm: 0.00027488148771226406
Step: 23530, train/learning_rate: 2.20014280785108e-05
Step: 23530, train/epoch: 5.599714279174805
Step: 23540, train/loss: 0.15230000019073486
Step: 23540, train/grad_norm: 0.0004548803553916514
Step: 23540, train/learning_rate: 2.19895282498328e-05
Step: 23540, train/epoch: 5.6020941734313965
Step: 23550, train/loss: 0.0
Step: 23550, train/grad_norm: 0.009127656929194927
Step: 23550, train/learning_rate: 2.1977630240144208e-05
Step: 23550, train/epoch: 5.604474067687988
Step: 23560, train/loss: 0.00019999999494757503
Step: 23560, train/grad_norm: 0.00616007624194026
Step: 23560, train/learning_rate: 2.196573041146621e-05
Step: 23560, train/epoch: 5.60685396194458
Step: 23570, train/loss: 9.999999747378752e-05
Step: 23570, train/grad_norm: 0.0014647507341578603
Step: 23570, train/learning_rate: 2.1953832401777618e-05
Step: 23570, train/epoch: 5.609233856201172
Step: 23580, train/loss: 0.0
Step: 23580, train/grad_norm: 0.006969723850488663
Step: 23580, train/learning_rate: 2.194193257309962e-05
Step: 23580, train/epoch: 5.611613750457764
Step: 23590, train/loss: 0.09380000084638596
Step: 23590, train/grad_norm: 0.0001974642655113712
Step: 23590, train/learning_rate: 2.1930032744421624e-05
Step: 23590, train/epoch: 5.613993167877197
Step: 23600, train/loss: 0.0
Step: 23600, train/grad_norm: 0.003592300694435835
Step: 23600, train/learning_rate: 2.191813473473303e-05
Step: 23600, train/epoch: 5.616373062133789
Step: 23610, train/loss: 0.0
Step: 23610, train/grad_norm: 0.0006872971425764263
Step: 23610, train/learning_rate: 2.1906234906055033e-05
Step: 23610, train/epoch: 5.618752956390381
Step: 23620, train/loss: 0.0
Step: 23620, train/grad_norm: 0.013378890231251717
Step: 23620, train/learning_rate: 2.189433689636644e-05
Step: 23620, train/epoch: 5.621132850646973
Step: 23630, train/loss: 0.0
Step: 23630, train/grad_norm: 0.0002495010267011821
Step: 23630, train/learning_rate: 2.1882437067688443e-05
Step: 23630, train/epoch: 5.6235127449035645
Step: 23640, train/loss: 0.0
Step: 23640, train/grad_norm: 0.00012849661288782954
Step: 23640, train/learning_rate: 2.1870537239010446e-05
Step: 23640, train/epoch: 5.625892639160156
Step: 23650, train/loss: 0.0
Step: 23650, train/grad_norm: 0.0004104807158000767
Step: 23650, train/learning_rate: 2.1858639229321852e-05
Step: 23650, train/epoch: 5.62827205657959
Step: 23660, train/loss: 0.06520000100135803
Step: 23660, train/grad_norm: 6.0694133026117925e-06
Step: 23660, train/learning_rate: 2.1846739400643855e-05
Step: 23660, train/epoch: 5.630651950836182
Step: 23670, train/loss: 0.0
Step: 23670, train/grad_norm: 0.00041468782001174986
Step: 23670, train/learning_rate: 2.1834841390955262e-05
Step: 23670, train/epoch: 5.633031845092773
Step: 23680, train/loss: 0.0
Step: 23680, train/grad_norm: 0.0033102782908827066
Step: 23680, train/learning_rate: 2.1822941562277265e-05
Step: 23680, train/epoch: 5.635411739349365
Step: 23690, train/loss: 0.0
Step: 23690, train/grad_norm: 8.512094063917175e-05
Step: 23690, train/learning_rate: 2.1811041733599268e-05
Step: 23690, train/epoch: 5.637791633605957
Step: 23700, train/loss: 0.0
Step: 23700, train/grad_norm: 3.356100569362752e-05
Step: 23700, train/learning_rate: 2.1799143723910674e-05
Step: 23700, train/epoch: 5.640171527862549
Step: 23710, train/loss: 0.0
Step: 23710, train/grad_norm: 5.693759158020839e-05
Step: 23710, train/learning_rate: 2.1787243895232677e-05
Step: 23710, train/epoch: 5.642550945281982
Step: 23720, train/loss: 0.0
Step: 23720, train/grad_norm: 1.4846801832391066e-06
Step: 23720, train/learning_rate: 2.1775345885544084e-05
Step: 23720, train/epoch: 5.644930839538574
Step: 23730, train/loss: 0.002400000113993883
Step: 23730, train/grad_norm: 5.113591760164127e-05
Step: 23730, train/learning_rate: 2.1763446056866087e-05
Step: 23730, train/epoch: 5.647310733795166
Step: 23740, train/loss: 0.0
Step: 23740, train/grad_norm: 2.0579345800797455e-05
Step: 23740, train/learning_rate: 2.175154622818809e-05
Step: 23740, train/epoch: 5.649690628051758
Step: 23750, train/loss: 0.0
Step: 23750, train/grad_norm: 1.1563369298528414e-05
Step: 23750, train/learning_rate: 2.1739648218499497e-05
Step: 23750, train/epoch: 5.65207052230835
Step: 23760, train/loss: 0.0
Step: 23760, train/grad_norm: 2.8464035040087765e-07
Step: 23760, train/learning_rate: 2.17277483898215e-05
Step: 23760, train/epoch: 5.654450416564941
Step: 23770, train/loss: 0.10779999941587448
Step: 23770, train/grad_norm: 0.00015676535258535296
Step: 23770, train/learning_rate: 2.1715850380132906e-05
Step: 23770, train/epoch: 5.656830310821533
Step: 23780, train/loss: 0.0
Step: 23780, train/grad_norm: 0.00011061125405831262
Step: 23780, train/learning_rate: 2.170395055145491e-05
Step: 23780, train/epoch: 5.659209728240967
Step: 23790, train/loss: 9.999999747378752e-05
Step: 23790, train/grad_norm: 0.014832192100584507
Step: 23790, train/learning_rate: 2.1692050722776912e-05
Step: 23790, train/epoch: 5.661589622497559
Step: 23800, train/loss: 0.0
Step: 23800, train/grad_norm: 0.00016795439296402037
Step: 23800, train/learning_rate: 2.168015271308832e-05
Step: 23800, train/epoch: 5.66396951675415
Step: 23810, train/loss: 0.0
Step: 23810, train/grad_norm: 0.001383146271109581
Step: 23810, train/learning_rate: 2.1668252884410322e-05
Step: 23810, train/epoch: 5.666349411010742
Step: 23820, train/loss: 0.0
Step: 23820, train/grad_norm: 0.0008125145104713738
Step: 23820, train/learning_rate: 2.1656354874721728e-05
Step: 23820, train/epoch: 5.668729305267334
Step: 23830, train/loss: 0.0
Step: 23830, train/grad_norm: 4.311911834520288e-05
Step: 23830, train/learning_rate: 2.164445504604373e-05
Step: 23830, train/epoch: 5.671109199523926
Step: 23840, train/loss: 0.0
Step: 23840, train/grad_norm: 5.074494993095868e-07
Step: 23840, train/learning_rate: 2.1632555217365734e-05
Step: 23840, train/epoch: 5.673488616943359
Step: 23850, train/loss: 0.0
Step: 23850, train/grad_norm: 0.000572126533370465
Step: 23850, train/learning_rate: 2.162065720767714e-05
Step: 23850, train/epoch: 5.675868511199951
Step: 23860, train/loss: 0.0
Step: 23860, train/grad_norm: 0.0002660057507455349
Step: 23860, train/learning_rate: 2.1608757378999144e-05
Step: 23860, train/epoch: 5.678248405456543
Step: 23870, train/loss: 0.0
Step: 23870, train/grad_norm: 3.9151645978563465e-06
Step: 23870, train/learning_rate: 2.159685936931055e-05
Step: 23870, train/epoch: 5.680628299713135
Step: 23880, train/loss: 0.0
Step: 23880, train/grad_norm: 9.993030289479066e-06
Step: 23880, train/learning_rate: 2.1584959540632553e-05
Step: 23880, train/epoch: 5.683008193969727
Step: 23890, train/loss: 0.0
Step: 23890, train/grad_norm: 3.142403102174285e-06
Step: 23890, train/learning_rate: 2.1573059711954556e-05
Step: 23890, train/epoch: 5.685388088226318
Step: 23900, train/loss: 0.0
Step: 23900, train/grad_norm: 0.017870517447590828
Step: 23900, train/learning_rate: 2.1561161702265963e-05
Step: 23900, train/epoch: 5.687767505645752
Step: 23910, train/loss: 0.0
Step: 23910, train/grad_norm: 3.096049795203726e-06
Step: 23910, train/learning_rate: 2.1549261873587966e-05
Step: 23910, train/epoch: 5.690147399902344
Step: 23920, train/loss: 0.0038999998942017555
Step: 23920, train/grad_norm: 0.0006454329704865813
Step: 23920, train/learning_rate: 2.1537363863899373e-05
Step: 23920, train/epoch: 5.6925272941589355
Step: 23930, train/loss: 9.999999747378752e-05
Step: 23930, train/grad_norm: 3.4016879908449482e-06
Step: 23930, train/learning_rate: 2.1525464035221376e-05
Step: 23930, train/epoch: 5.694907188415527
Step: 23940, train/loss: 0.0
Step: 23940, train/grad_norm: 1.094926665246021e-05
Step: 23940, train/learning_rate: 2.151356420654338e-05
Step: 23940, train/epoch: 5.697287082672119
Step: 23950, train/loss: 0.0
Step: 23950, train/grad_norm: 9.62952799454797e-06
Step: 23950, train/learning_rate: 2.1501666196854785e-05
Step: 23950, train/epoch: 5.699666976928711
Step: 23960, train/loss: 0.15000000596046448
Step: 23960, train/grad_norm: 9.368501196149737e-06
Step: 23960, train/learning_rate: 2.1489766368176788e-05
Step: 23960, train/epoch: 5.702046871185303
Step: 23970, train/loss: 0.0
Step: 23970, train/grad_norm: 0.0005240357713773847
Step: 23970, train/learning_rate: 2.1477868358488195e-05
Step: 23970, train/epoch: 5.704426288604736
Step: 23980, train/loss: 0.0
Step: 23980, train/grad_norm: 3.871923581755254e-06
Step: 23980, train/learning_rate: 2.1465968529810198e-05
Step: 23980, train/epoch: 5.706806182861328
Step: 23990, train/loss: 0.0
Step: 23990, train/grad_norm: 0.0002419108641333878
Step: 23990, train/learning_rate: 2.14540687011322e-05
Step: 23990, train/epoch: 5.70918607711792
Step: 24000, train/loss: 0.0
Step: 24000, train/grad_norm: 5.7098426623269916e-05
Step: 24000, train/learning_rate: 2.1442170691443607e-05
Step: 24000, train/epoch: 5.711565971374512
Step: 24010, train/loss: 0.0
Step: 24010, train/grad_norm: 3.941398972528987e-06
Step: 24010, train/learning_rate: 2.143027086276561e-05
Step: 24010, train/epoch: 5.7139458656311035
Step: 24020, train/loss: 0.0
Step: 24020, train/grad_norm: 5.421988134912681e-06
Step: 24020, train/learning_rate: 2.1418372853077017e-05
Step: 24020, train/epoch: 5.716325759887695
Step: 24030, train/loss: 0.0
Step: 24030, train/grad_norm: 1.1719626513695403e-07
Step: 24030, train/learning_rate: 2.140647302439902e-05
Step: 24030, train/epoch: 5.718705177307129
Step: 24040, train/loss: 0.0
Step: 24040, train/grad_norm: 4.2068796801686403e-07
Step: 24040, train/learning_rate: 2.1394573195721023e-05
Step: 24040, train/epoch: 5.721085071563721
Step: 24050, train/loss: 0.0
Step: 24050, train/grad_norm: 5.19475361215882e-06
Step: 24050, train/learning_rate: 2.138267518603243e-05
Step: 24050, train/epoch: 5.7234649658203125
Step: 24060, train/loss: 0.0
Step: 24060, train/grad_norm: 8.49989810376428e-05
Step: 24060, train/learning_rate: 2.1370775357354432e-05
Step: 24060, train/epoch: 5.725844860076904
Step: 24070, train/loss: 0.0
Step: 24070, train/grad_norm: 1.3561121704697143e-05
Step: 24070, train/learning_rate: 2.135887734766584e-05
Step: 24070, train/epoch: 5.728224754333496
Step: 24080, train/loss: 0.0
Step: 24080, train/grad_norm: 4.728186922875466e-06
Step: 24080, train/learning_rate: 2.1346977518987842e-05
Step: 24080, train/epoch: 5.730604648590088
Step: 24090, train/loss: 0.0
Step: 24090, train/grad_norm: 2.1641782950609922e-05
Step: 24090, train/learning_rate: 2.1335077690309845e-05
Step: 24090, train/epoch: 5.7329840660095215
Step: 24100, train/loss: 0.0
Step: 24100, train/grad_norm: 5.368836355046369e-06
Step: 24100, train/learning_rate: 2.132317968062125e-05
Step: 24100, train/epoch: 5.735363960266113
Step: 24110, train/loss: 0.09529999643564224
Step: 24110, train/grad_norm: 8.16898827906698e-05
Step: 24110, train/learning_rate: 2.1311279851943254e-05
Step: 24110, train/epoch: 5.737743854522705
Step: 24120, train/loss: 0.0
Step: 24120, train/grad_norm: 1.9968867491115816e-05
Step: 24120, train/learning_rate: 2.129938184225466e-05
Step: 24120, train/epoch: 5.740123748779297
Step: 24130, train/loss: 0.0
Step: 24130, train/grad_norm: 0.0023489880841225386
Step: 24130, train/learning_rate: 2.1287482013576664e-05
Step: 24130, train/epoch: 5.742503643035889
Step: 24140, train/loss: 9.999999747378752e-05
Step: 24140, train/grad_norm: 0.006312441546469927
Step: 24140, train/learning_rate: 2.1275582184898667e-05
Step: 24140, train/epoch: 5.7448835372924805
Step: 24150, train/loss: 0.0
Step: 24150, train/grad_norm: 0.0003955891006626189
Step: 24150, train/learning_rate: 2.1263684175210074e-05
Step: 24150, train/epoch: 5.747263431549072
Step: 24160, train/loss: 0.0
Step: 24160, train/grad_norm: 1.982427784241736e-05
Step: 24160, train/learning_rate: 2.1251784346532077e-05
Step: 24160, train/epoch: 5.749642848968506
Step: 24170, train/loss: 0.0
Step: 24170, train/grad_norm: 0.001956071238964796
Step: 24170, train/learning_rate: 2.1239886336843483e-05
Step: 24170, train/epoch: 5.752022743225098
Step: 24180, train/loss: 0.0
Step: 24180, train/grad_norm: 3.4680460885283537e-06
Step: 24180, train/learning_rate: 2.1227986508165486e-05
Step: 24180, train/epoch: 5.7544026374816895
Step: 24190, train/loss: 0.0
Step: 24190, train/grad_norm: 0.0006182665238156915
Step: 24190, train/learning_rate: 2.121608667948749e-05
Step: 24190, train/epoch: 5.756782531738281
Step: 24200, train/loss: 0.0
Step: 24200, train/grad_norm: 0.0009387003374285996
Step: 24200, train/learning_rate: 2.1204188669798896e-05
Step: 24200, train/epoch: 5.759162425994873
Step: 24210, train/loss: 0.0
Step: 24210, train/grad_norm: 3.064038764932775e-06
Step: 24210, train/learning_rate: 2.11922888411209e-05
Step: 24210, train/epoch: 5.761542320251465
Step: 24220, train/loss: 0.0
Step: 24220, train/grad_norm: 2.4191796910599805e-05
Step: 24220, train/learning_rate: 2.1180390831432305e-05
Step: 24220, train/epoch: 5.763921737670898
Step: 24230, train/loss: 0.0
Step: 24230, train/grad_norm: 4.80360256460699e-07
Step: 24230, train/learning_rate: 2.1168491002754308e-05
Step: 24230, train/epoch: 5.76630163192749
Step: 24240, train/loss: 0.004800000227987766
Step: 24240, train/grad_norm: 1.0340667358832434e-05
Step: 24240, train/learning_rate: 2.1156592993065715e-05
Step: 24240, train/epoch: 5.768681526184082
Step: 24250, train/loss: 0.0
Step: 24250, train/grad_norm: 3.459670188021846e-05
Step: 24250, train/learning_rate: 2.1144693164387718e-05
Step: 24250, train/epoch: 5.771061420440674
Step: 24260, train/loss: 0.0
Step: 24260, train/grad_norm: 1.7076065716992161e-07
Step: 24260, train/learning_rate: 2.113279333570972e-05
Step: 24260, train/epoch: 5.773441314697266
Step: 24270, train/loss: 0.0
Step: 24270, train/grad_norm: 0.002700868993997574
Step: 24270, train/learning_rate: 2.1120895326021127e-05
Step: 24270, train/epoch: 5.775821208953857
Step: 24280, train/loss: 0.07729999721050262
Step: 24280, train/grad_norm: 0.0004305018810555339
Step: 24280, train/learning_rate: 2.110899549734313e-05
Step: 24280, train/epoch: 5.778200626373291
Step: 24290, train/loss: 0.0
Step: 24290, train/grad_norm: 0.001227963948622346
Step: 24290, train/learning_rate: 2.1097097487654537e-05
Step: 24290, train/epoch: 5.780580520629883
Step: 24300, train/loss: 0.0
Step: 24300, train/grad_norm: 1.960835618319834e-07
Step: 24300, train/learning_rate: 2.108519765897654e-05
Step: 24300, train/epoch: 5.782960414886475
Step: 24310, train/loss: 0.0
Step: 24310, train/grad_norm: 0.0001216669988934882
Step: 24310, train/learning_rate: 2.1073297830298543e-05
Step: 24310, train/epoch: 5.785340309143066
Step: 24320, train/loss: 0.0
Step: 24320, train/grad_norm: 2.2405836261896184e-06
Step: 24320, train/learning_rate: 2.106139982060995e-05
Step: 24320, train/epoch: 5.787720203399658
Step: 24330, train/loss: 0.0
Step: 24330, train/grad_norm: 8.213327964767814e-05
Step: 24330, train/learning_rate: 2.1049499991931953e-05
Step: 24330, train/epoch: 5.79010009765625
Step: 24340, train/loss: 0.0
Step: 24340, train/grad_norm: 3.1172094168141484e-05
Step: 24340, train/learning_rate: 2.103760198224336e-05
Step: 24340, train/epoch: 5.792479991912842
Step: 24350, train/loss: 0.0
Step: 24350, train/grad_norm: 1.2123147143938695e-06
Step: 24350, train/learning_rate: 2.1025702153565362e-05
Step: 24350, train/epoch: 5.794859409332275
Step: 24360, train/loss: 0.0
Step: 24360, train/grad_norm: 3.544047649484128e-05
Step: 24360, train/learning_rate: 2.1013802324887365e-05
Step: 24360, train/epoch: 5.797239303588867
Step: 24370, train/loss: 0.0
Step: 24370, train/grad_norm: 4.244063893565908e-05
Step: 24370, train/learning_rate: 2.100190431519877e-05
Step: 24370, train/epoch: 5.799619197845459
Step: 24380, train/loss: 0.0
Step: 24380, train/grad_norm: 3.974194129341413e-08
Step: 24380, train/learning_rate: 2.0990004486520775e-05
Step: 24380, train/epoch: 5.801999092102051
Step: 24390, train/loss: 0.0
Step: 24390, train/grad_norm: 0.00010854674474103376
Step: 24390, train/learning_rate: 2.097810647683218e-05
Step: 24390, train/epoch: 5.804378986358643
Step: 24400, train/loss: 0.0
Step: 24400, train/grad_norm: 1.5529498114119633e-06
Step: 24400, train/learning_rate: 2.0966206648154184e-05
Step: 24400, train/epoch: 5.806758880615234
Step: 24410, train/loss: 0.0
Step: 24410, train/grad_norm: 2.5510637442494044e-06
Step: 24410, train/learning_rate: 2.0954306819476187e-05
Step: 24410, train/epoch: 5.809138298034668
Step: 24420, train/loss: 0.0
Step: 24420, train/grad_norm: 8.780768325777899e-07
Step: 24420, train/learning_rate: 2.0942408809787594e-05
Step: 24420, train/epoch: 5.81151819229126
Step: 24430, train/loss: 0.0
Step: 24430, train/grad_norm: 1.7233787730219774e-05
Step: 24430, train/learning_rate: 2.0930508981109597e-05
Step: 24430, train/epoch: 5.813898086547852
Step: 24440, train/loss: 0.0
Step: 24440, train/grad_norm: 0.001493568648584187
Step: 24440, train/learning_rate: 2.0918610971421003e-05
Step: 24440, train/epoch: 5.816277980804443
Step: 24450, train/loss: 0.0
Step: 24450, train/grad_norm: 9.213071280100849e-06
Step: 24450, train/learning_rate: 2.0906711142743006e-05
Step: 24450, train/epoch: 5.818657875061035
Step: 24460, train/loss: 0.0
Step: 24460, train/grad_norm: 0.002144381869584322
Step: 24460, train/learning_rate: 2.089481131406501e-05
Step: 24460, train/epoch: 5.821037769317627
Step: 24470, train/loss: 0.0
Step: 24470, train/grad_norm: 3.3143162454507546e-06
Step: 24470, train/learning_rate: 2.0882913304376416e-05
Step: 24470, train/epoch: 5.8234171867370605
Step: 24480, train/loss: 0.0
Step: 24480, train/grad_norm: 1.9558518715712125e-07
Step: 24480, train/learning_rate: 2.087101347569842e-05
Step: 24480, train/epoch: 5.825797080993652
Step: 24490, train/loss: 0.0
Step: 24490, train/grad_norm: 4.8049732868094e-05
Step: 24490, train/learning_rate: 2.0859115466009825e-05
Step: 24490, train/epoch: 5.828176975250244
Step: 24500, train/loss: 0.0
Step: 24500, train/grad_norm: 0.00014081761764828116
Step: 24500, train/learning_rate: 2.084721563733183e-05
Step: 24500, train/epoch: 5.830556869506836
Step: 24510, train/loss: 0.0
Step: 24510, train/grad_norm: 2.5181510210359193e-09
Step: 24510, train/learning_rate: 2.083531580865383e-05
Step: 24510, train/epoch: 5.832936763763428
Step: 24520, train/loss: 0.0
Step: 24520, train/grad_norm: 6.116519557508582e-07
Step: 24520, train/learning_rate: 2.0823417798965238e-05
Step: 24520, train/epoch: 5.8353166580200195
Step: 24530, train/loss: 0.0
Step: 24530, train/grad_norm: 2.711459637794178e-05
Step: 24530, train/learning_rate: 2.081151797028724e-05
Step: 24530, train/epoch: 5.837696552276611
Step: 24540, train/loss: 0.0
Step: 24540, train/grad_norm: 2.356196455366444e-05
Step: 24540, train/learning_rate: 2.0799619960598648e-05
Step: 24540, train/epoch: 5.840075969696045
Step: 24550, train/loss: 0.0
Step: 24550, train/grad_norm: 2.587888161542651e-07
Step: 24550, train/learning_rate: 2.078772013192065e-05
Step: 24550, train/epoch: 5.842455863952637
Step: 24560, train/loss: 0.0
Step: 24560, train/grad_norm: 1.4176706827129237e-05
Step: 24560, train/learning_rate: 2.0775820303242654e-05
Step: 24560, train/epoch: 5.8448357582092285
Step: 24570, train/loss: 0.0
Step: 24570, train/grad_norm: 8.224460907513276e-05
Step: 24570, train/learning_rate: 2.076392229355406e-05
Step: 24570, train/epoch: 5.84721565246582
Step: 24580, train/loss: 0.0
Step: 24580, train/grad_norm: 1.1515473943291e-06
Step: 24580, train/learning_rate: 2.0752022464876063e-05
Step: 24580, train/epoch: 5.849595546722412
Step: 24590, train/loss: 0.0
Step: 24590, train/grad_norm: 4.6397857659030706e-05
Step: 24590, train/learning_rate: 2.074012445518747e-05
Step: 24590, train/epoch: 5.851975440979004
Step: 24600, train/loss: 0.0
Step: 24600, train/grad_norm: 2.324083681060074e-07
Step: 24600, train/learning_rate: 2.0728224626509473e-05
Step: 24600, train/epoch: 5.8543548583984375
Step: 24610, train/loss: 0.0
Step: 24610, train/grad_norm: 1.2120025530748535e-05
Step: 24610, train/learning_rate: 2.0716324797831476e-05
Step: 24610, train/epoch: 5.856734752655029
Step: 24620, train/loss: 0.0
Step: 24620, train/grad_norm: 0.00019405326747801155
Step: 24620, train/learning_rate: 2.0704426788142882e-05
Step: 24620, train/epoch: 5.859114646911621
Step: 24630, train/loss: 0.0
Step: 24630, train/grad_norm: 9.535001481708605e-06
Step: 24630, train/learning_rate: 2.0692526959464885e-05
Step: 24630, train/epoch: 5.861494541168213
Step: 24640, train/loss: 0.0
Step: 24640, train/grad_norm: 0.0065298681147396564
Step: 24640, train/learning_rate: 2.0680628949776292e-05
Step: 24640, train/epoch: 5.863874435424805
Step: 24650, train/loss: 0.0
Step: 24650, train/grad_norm: 0.0004809828824363649
Step: 24650, train/learning_rate: 2.0668729121098295e-05
Step: 24650, train/epoch: 5.8662543296813965
Step: 24660, train/loss: 0.0
Step: 24660, train/grad_norm: 1.3440411748888437e-05
Step: 24660, train/learning_rate: 2.0656829292420298e-05
Step: 24660, train/epoch: 5.86863374710083
Step: 24670, train/loss: 0.0
Step: 24670, train/grad_norm: 0.00024111647508107126
Step: 24670, train/learning_rate: 2.0644931282731704e-05
Step: 24670, train/epoch: 5.871013641357422
Step: 24680, train/loss: 0.0
Step: 24680, train/grad_norm: 7.0458618210977875e-06
Step: 24680, train/learning_rate: 2.0633031454053707e-05
Step: 24680, train/epoch: 5.873393535614014
Step: 24690, train/loss: 0.0
Step: 24690, train/grad_norm: 5.7987643231172115e-05
Step: 24690, train/learning_rate: 2.0621133444365114e-05
Step: 24690, train/epoch: 5.8757734298706055
Step: 24700, train/loss: 0.0
Step: 24700, train/grad_norm: 7.547863060608506e-05
Step: 24700, train/learning_rate: 2.0609233615687117e-05
Step: 24700, train/epoch: 5.878153324127197
Step: 24710, train/loss: 0.0
Step: 24710, train/grad_norm: 4.267833901394624e-06
Step: 24710, train/learning_rate: 2.059733378700912e-05
Step: 24710, train/epoch: 5.880533218383789
Step: 24720, train/loss: 0.0
Step: 24720, train/grad_norm: 2.4317921543115517e-07
Step: 24720, train/learning_rate: 2.0585435777320527e-05
Step: 24720, train/epoch: 5.882913112640381
Step: 24730, train/loss: 0.0
Step: 24730, train/grad_norm: 1.066540544769623e-08
Step: 24730, train/learning_rate: 2.057353594864253e-05
Step: 24730, train/epoch: 5.8852925300598145
Step: 24740, train/loss: 0.0
Step: 24740, train/grad_norm: 4.457018803805113e-06
Step: 24740, train/learning_rate: 2.0561637938953936e-05
Step: 24740, train/epoch: 5.887672424316406
Step: 24750, train/loss: 0.0
Step: 24750, train/grad_norm: 4.655120847019134e-06
Step: 24750, train/learning_rate: 2.054973811027594e-05
Step: 24750, train/epoch: 5.890052318572998
Step: 24760, train/loss: 0.0
Step: 24760, train/grad_norm: 1.2883625686299638e-06
Step: 24760, train/learning_rate: 2.0537838281597942e-05
Step: 24760, train/epoch: 5.89243221282959
Step: 24770, train/loss: 0.0
Step: 24770, train/grad_norm: 4.461666094357497e-07
Step: 24770, train/learning_rate: 2.052594027190935e-05
Step: 24770, train/epoch: 5.894812107086182
Step: 24780, train/loss: 0.0
Step: 24780, train/grad_norm: 5.950622039563314e-07
Step: 24780, train/learning_rate: 2.051404044323135e-05
Step: 24780, train/epoch: 5.897192001342773
Step: 24790, train/loss: 0.05270000174641609
Step: 24790, train/grad_norm: 9.132220839092042e-06
Step: 24790, train/learning_rate: 2.0502142433542758e-05
Step: 24790, train/epoch: 5.899571418762207
Step: 24800, train/loss: 0.1265999972820282
Step: 24800, train/grad_norm: 0.00014892943727318197
Step: 24800, train/learning_rate: 2.049024260486476e-05
Step: 24800, train/epoch: 5.901951313018799
Step: 24810, train/loss: 0.0
Step: 24810, train/grad_norm: 0.01860800012946129
Step: 24810, train/learning_rate: 2.0478342776186764e-05
Step: 24810, train/epoch: 5.904331207275391
Step: 24820, train/loss: 0.0
Step: 24820, train/grad_norm: 0.0036175143904983997
Step: 24820, train/learning_rate: 2.046644476649817e-05
Step: 24820, train/epoch: 5.906711101531982
Step: 24830, train/loss: 0.0
Step: 24830, train/grad_norm: 0.0006294691702350974
Step: 24830, train/learning_rate: 2.0454544937820174e-05
Step: 24830, train/epoch: 5.909090995788574
Step: 24840, train/loss: 0.0
Step: 24840, train/grad_norm: 6.87334468238987e-05
Step: 24840, train/learning_rate: 2.044264692813158e-05
Step: 24840, train/epoch: 5.911470890045166
Step: 24850, train/loss: 0.0
Step: 24850, train/grad_norm: 0.0024209043476730585
Step: 24850, train/learning_rate: 2.0430747099453583e-05
Step: 24850, train/epoch: 5.913850784301758
Step: 24860, train/loss: 0.0
Step: 24860, train/grad_norm: 0.00034415346453897655
Step: 24860, train/learning_rate: 2.0418847270775586e-05
Step: 24860, train/epoch: 5.916230201721191
Step: 24870, train/loss: 0.0
Step: 24870, train/grad_norm: 0.0006063284236006439
Step: 24870, train/learning_rate: 2.0406949261086993e-05
Step: 24870, train/epoch: 5.918610095977783
Step: 24880, train/loss: 0.0
Step: 24880, train/grad_norm: 0.0009813698707148433
Step: 24880, train/learning_rate: 2.0395049432408996e-05
Step: 24880, train/epoch: 5.920989990234375
Step: 24890, train/loss: 0.0
Step: 24890, train/grad_norm: 6.418272823793814e-05
Step: 24890, train/learning_rate: 2.0383151422720402e-05
Step: 24890, train/epoch: 5.923369884490967
Step: 24900, train/loss: 0.0
Step: 24900, train/grad_norm: 6.69354676574585e-07
Step: 24900, train/learning_rate: 2.0371251594042405e-05
Step: 24900, train/epoch: 5.925749778747559
Step: 24910, train/loss: 0.08749999850988388
Step: 24910, train/grad_norm: 0.0015180143527686596
Step: 24910, train/learning_rate: 2.0359353584353812e-05
Step: 24910, train/epoch: 5.92812967300415
Step: 24920, train/loss: 0.0
Step: 24920, train/grad_norm: 0.00019564836111385375
Step: 24920, train/learning_rate: 2.0347453755675815e-05
Step: 24920, train/epoch: 5.930509090423584
Step: 24930, train/loss: 0.0005000000237487257
Step: 24930, train/grad_norm: 0.001620173454284668
Step: 24930, train/learning_rate: 2.0335553926997818e-05
Step: 24930, train/epoch: 5.932888984680176
Step: 24940, train/loss: 0.0
Step: 24940, train/grad_norm: 3.6553708469000412e-06
Step: 24940, train/learning_rate: 2.0323655917309225e-05
Step: 24940, train/epoch: 5.935268878936768
Step: 24950, train/loss: 0.0
Step: 24950, train/grad_norm: 0.0007973914034664631
Step: 24950, train/learning_rate: 2.0311756088631228e-05
Step: 24950, train/epoch: 5.937648773193359
Step: 24960, train/loss: 0.0
Step: 24960, train/grad_norm: 3.9899390685604885e-05
Step: 24960, train/learning_rate: 2.0299858078942634e-05
Step: 24960, train/epoch: 5.940028667449951
Step: 24970, train/loss: 0.0
Step: 24970, train/grad_norm: 2.252843387395842e-07
Step: 24970, train/learning_rate: 2.0287958250264637e-05
Step: 24970, train/epoch: 5.942408561706543
Step: 24980, train/loss: 0.0
Step: 24980, train/grad_norm: 7.83332870923914e-05
Step: 24980, train/learning_rate: 2.027605842158664e-05
Step: 24980, train/epoch: 5.944787979125977
Step: 24990, train/loss: 0.0
Step: 24990, train/grad_norm: 9.467266499996185e-05
Step: 24990, train/learning_rate: 2.0264160411898047e-05
Step: 24990, train/epoch: 5.947167873382568
Step: 25000, train/loss: 0.0
Step: 25000, train/grad_norm: 0.0014147881884127855
Step: 25000, train/learning_rate: 2.025226058322005e-05
Step: 25000, train/epoch: 5.94954776763916
Step: 25010, train/loss: 0.0
Step: 25010, train/grad_norm: 8.640448868391104e-06
Step: 25010, train/learning_rate: 2.0240362573531456e-05
Step: 25010, train/epoch: 5.951927661895752
Step: 25020, train/loss: 0.0
Step: 25020, train/grad_norm: 1.1189983695203409e-07
Step: 25020, train/learning_rate: 2.022846274485346e-05
Step: 25020, train/epoch: 5.954307556152344
Step: 25030, train/loss: 0.0731000006198883
Step: 25030, train/grad_norm: 0.0018155707512050867
Step: 25030, train/learning_rate: 2.0216562916175462e-05
Step: 25030, train/epoch: 5.9566874504089355
Step: 25040, train/loss: 0.0
Step: 25040, train/grad_norm: 0.00011552321666385978
Step: 25040, train/learning_rate: 2.020466490648687e-05
Step: 25040, train/epoch: 5.959067344665527
Step: 25050, train/loss: 0.0
Step: 25050, train/grad_norm: 0.054914914071559906
Step: 25050, train/learning_rate: 2.0192765077808872e-05
Step: 25050, train/epoch: 5.961446762084961
Step: 25060, train/loss: 9.999999747378752e-05
Step: 25060, train/grad_norm: 0.210634246468544
Step: 25060, train/learning_rate: 2.018086706812028e-05
Step: 25060, train/epoch: 5.963826656341553
Step: 25070, train/loss: 0.0
Step: 25070, train/grad_norm: 3.517280902087805e-06
Step: 25070, train/learning_rate: 2.016896723944228e-05
Step: 25070, train/epoch: 5.9662065505981445
Step: 25080, train/loss: 0.0
Step: 25080, train/grad_norm: 2.756708454398904e-05
Step: 25080, train/learning_rate: 2.0157067410764284e-05
Step: 25080, train/epoch: 5.968586444854736
Step: 25090, train/loss: 0.0
Step: 25090, train/grad_norm: 8.124005944409873e-06
Step: 25090, train/learning_rate: 2.014516940107569e-05
Step: 25090, train/epoch: 5.970966339111328
Step: 25100, train/loss: 0.0
Step: 25100, train/grad_norm: 0.00028853421099483967
Step: 25100, train/learning_rate: 2.0133269572397694e-05
Step: 25100, train/epoch: 5.97334623336792
Step: 25110, train/loss: 0.0
Step: 25110, train/grad_norm: 1.5997818991309032e-05
Step: 25110, train/learning_rate: 2.01213715627091e-05
Step: 25110, train/epoch: 5.9757256507873535
Step: 25120, train/loss: 9.999999747378752e-05
Step: 25120, train/grad_norm: 3.1592331652063876e-05
Step: 25120, train/learning_rate: 2.0109471734031104e-05
Step: 25120, train/epoch: 5.978105545043945
Step: 25130, train/loss: 0.0
Step: 25130, train/grad_norm: 1.4396353435586207e-05
Step: 25130, train/learning_rate: 2.0097571905353107e-05
Step: 25130, train/epoch: 5.980485439300537
Step: 25140, train/loss: 9.999999747378752e-05
Step: 25140, train/grad_norm: 6.886801884320448e-07
Step: 25140, train/learning_rate: 2.0085673895664513e-05
Step: 25140, train/epoch: 5.982865333557129
Step: 25150, train/loss: 0.0
Step: 25150, train/grad_norm: 2.9401132195516766e-08
Step: 25150, train/learning_rate: 2.0073774066986516e-05
Step: 25150, train/epoch: 5.985245227813721
Step: 25160, train/loss: 0.0
Step: 25160, train/grad_norm: 6.64513720494142e-07
Step: 25160, train/learning_rate: 2.0061876057297923e-05
Step: 25160, train/epoch: 5.9876251220703125
Step: 25170, train/loss: 0.0
Step: 25170, train/grad_norm: 6.744139824377271e-08
Step: 25170, train/learning_rate: 2.0049976228619926e-05
Step: 25170, train/epoch: 5.990004539489746
Step: 25180, train/loss: 0.0
Step: 25180, train/grad_norm: 8.295719311490757e-08
Step: 25180, train/learning_rate: 2.003807639994193e-05
Step: 25180, train/epoch: 5.992384433746338
Step: 25190, train/loss: 0.0
Step: 25190, train/grad_norm: 1.8079113317526208e-08
Step: 25190, train/learning_rate: 2.0026178390253335e-05
Step: 25190, train/epoch: 5.99476432800293
Step: 25200, train/loss: 0.0
Step: 25200, train/grad_norm: 5.987061513224035e-08
Step: 25200, train/learning_rate: 2.0014278561575338e-05
Step: 25200, train/epoch: 5.9971442222595215
Step: 25210, train/loss: 0.0
Step: 25210, train/grad_norm: 3.702551111928187e-05
Step: 25210, train/learning_rate: 2.0002380551886745e-05
Step: 25210, train/epoch: 5.999524116516113
Step: 25212, eval/loss: 0.060425177216529846
Step: 25212, eval/accuracy: 0.9943079352378845
Step: 25212, eval/f1: 0.9939771890640259
Step: 25212, eval/runtime: 734.4677124023438
Step: 25212, eval/samples_per_second: 9.807000160217285
Step: 25212, eval/steps_per_second: 1.2269999980926514
Step: 25212, train/epoch: 6.0
Step: 25220, train/loss: 0.0
Step: 25220, train/grad_norm: 0.0005572998197749257
Step: 25220, train/learning_rate: 1.9990480723208748e-05
Step: 25220, train/epoch: 6.001904010772705
Step: 25230, train/loss: 0.0
Step: 25230, train/grad_norm: 2.147642703675956e-07
Step: 25230, train/learning_rate: 1.997858089453075e-05
Step: 25230, train/epoch: 6.004283905029297
Step: 25240, train/loss: 0.0
Step: 25240, train/grad_norm: 0.00012382215936668217
Step: 25240, train/learning_rate: 1.9966682884842157e-05
Step: 25240, train/epoch: 6.0066633224487305
Step: 25250, train/loss: 0.0
Step: 25250, train/grad_norm: 5.2157038226141594e-06
Step: 25250, train/learning_rate: 1.995478305616416e-05
Step: 25250, train/epoch: 6.009043216705322
Step: 25260, train/loss: 0.0
Step: 25260, train/grad_norm: 9.226571819453966e-06
Step: 25260, train/learning_rate: 1.9942885046475567e-05
Step: 25260, train/epoch: 6.011423110961914
Step: 25270, train/loss: 0.0
Step: 25270, train/grad_norm: 8.90711326917426e-09
Step: 25270, train/learning_rate: 1.993098521779757e-05
Step: 25270, train/epoch: 6.013803005218506
Step: 25280, train/loss: 0.0
Step: 25280, train/grad_norm: 4.223043248430258e-08
Step: 25280, train/learning_rate: 1.9919085389119573e-05
Step: 25280, train/epoch: 6.016182899475098
Step: 25290, train/loss: 0.0
Step: 25290, train/grad_norm: 7.605931386933662e-06
Step: 25290, train/learning_rate: 1.990718737943098e-05
Step: 25290, train/epoch: 6.0185627937316895
Step: 25300, train/loss: 0.0
Step: 25300, train/grad_norm: 0.00024717897758819163
Step: 25300, train/learning_rate: 1.9895287550752982e-05
Step: 25300, train/epoch: 6.020942211151123
Step: 25310, train/loss: 0.0
Step: 25310, train/grad_norm: 3.363244331922033e-07
Step: 25310, train/learning_rate: 1.988338954106439e-05
Step: 25310, train/epoch: 6.023322105407715
Step: 25320, train/loss: 0.0
Step: 25320, train/grad_norm: 1.7561076674610376e-05
Step: 25320, train/learning_rate: 1.9871489712386392e-05
Step: 25320, train/epoch: 6.025701999664307
Step: 25330, train/loss: 0.0
Step: 25330, train/grad_norm: 3.5716217098524794e-05
Step: 25330, train/learning_rate: 1.9859589883708395e-05
Step: 25330, train/epoch: 6.028081893920898
Step: 25340, train/loss: 0.0
Step: 25340, train/grad_norm: 3.905141056748107e-05
Step: 25340, train/learning_rate: 1.98476918740198e-05
Step: 25340, train/epoch: 6.03046178817749
Step: 25350, train/loss: 0.0
Step: 25350, train/grad_norm: 3.125564163042327e-08
Step: 25350, train/learning_rate: 1.9835792045341805e-05
Step: 25350, train/epoch: 6.032841682434082
Step: 25360, train/loss: 0.0
Step: 25360, train/grad_norm: 7.566318345197942e-06
Step: 25360, train/learning_rate: 1.982389403565321e-05
Step: 25360, train/epoch: 6.035221099853516
Step: 25370, train/loss: 0.0
Step: 25370, train/grad_norm: 4.900196017842973e-07
Step: 25370, train/learning_rate: 1.9811994206975214e-05
Step: 25370, train/epoch: 6.037600994110107
Step: 25380, train/loss: 0.0
Step: 25380, train/grad_norm: 8.957488830674265e-07
Step: 25380, train/learning_rate: 1.9800094378297217e-05
Step: 25380, train/epoch: 6.039980888366699
Step: 25390, train/loss: 0.0
Step: 25390, train/grad_norm: 0.017239250242710114
Step: 25390, train/learning_rate: 1.9788196368608624e-05
Step: 25390, train/epoch: 6.042360782623291
Step: 25400, train/loss: 0.0
Step: 25400, train/grad_norm: 3.520497893205743e-09
Step: 25400, train/learning_rate: 1.9776296539930627e-05
Step: 25400, train/epoch: 6.044740676879883
Step: 25410, train/loss: 0.0
Step: 25410, train/grad_norm: 2.8078229519223896e-08
Step: 25410, train/learning_rate: 1.9764398530242033e-05
Step: 25410, train/epoch: 6.047120571136475
Step: 25420, train/loss: 0.0
Step: 25420, train/grad_norm: 1.0033973012468778e-05
Step: 25420, train/learning_rate: 1.9752498701564036e-05
Step: 25420, train/epoch: 6.049500465393066
Step: 25430, train/loss: 0.0
Step: 25430, train/grad_norm: 1.5439546530160442e-07
Step: 25430, train/learning_rate: 1.974059887288604e-05
Step: 25430, train/epoch: 6.0518798828125
Step: 25440, train/loss: 0.0
Step: 25440, train/grad_norm: 4.547620483208448e-05
Step: 25440, train/learning_rate: 1.9728700863197446e-05
Step: 25440, train/epoch: 6.054259777069092
Step: 25450, train/loss: 0.0
Step: 25450, train/grad_norm: 1.5338410719323292e-07
Step: 25450, train/learning_rate: 1.971680103451945e-05
Step: 25450, train/epoch: 6.056639671325684
Step: 25460, train/loss: 0.0
Step: 25460, train/grad_norm: 3.069177409997792e-07
Step: 25460, train/learning_rate: 1.9704903024830855e-05
Step: 25460, train/epoch: 6.059019565582275
Step: 25470, train/loss: 0.0
Step: 25470, train/grad_norm: 1.7117533843702404e-06
Step: 25470, train/learning_rate: 1.969300319615286e-05
Step: 25470, train/epoch: 6.061399459838867
Step: 25480, train/loss: 0.0
Step: 25480, train/grad_norm: 6.50069296170841e-07
Step: 25480, train/learning_rate: 1.968110336747486e-05
Step: 25480, train/epoch: 6.063779354095459
Step: 25490, train/loss: 0.0
Step: 25490, train/grad_norm: 4.875728905062715e-07
Step: 25490, train/learning_rate: 1.9669205357786268e-05
Step: 25490, train/epoch: 6.066158771514893
Step: 25500, train/loss: 0.0
Step: 25500, train/grad_norm: 1.5873838776769844e-07
Step: 25500, train/learning_rate: 1.965730552910827e-05
Step: 25500, train/epoch: 6.068538665771484
Step: 25510, train/loss: 0.0
Step: 25510, train/grad_norm: 1.40986941232768e-07
Step: 25510, train/learning_rate: 1.9645407519419678e-05
Step: 25510, train/epoch: 6.070918560028076
Step: 25520, train/loss: 0.0
Step: 25520, train/grad_norm: 4.143248943933031e-08
Step: 25520, train/learning_rate: 1.963350769074168e-05
Step: 25520, train/epoch: 6.073298454284668
Step: 25530, train/loss: 0.0
Step: 25530, train/grad_norm: 1.5113624840523698e-07
Step: 25530, train/learning_rate: 1.9621607862063684e-05
Step: 25530, train/epoch: 6.07567834854126
Step: 25540, train/loss: 0.0
Step: 25540, train/grad_norm: 4.5073147703078575e-06
Step: 25540, train/learning_rate: 1.960970985237509e-05
Step: 25540, train/epoch: 6.078058242797852
Step: 25550, train/loss: 0.0
Step: 25550, train/grad_norm: 0.00027687480906024575
Step: 25550, train/learning_rate: 1.9597810023697093e-05
Step: 25550, train/epoch: 6.080437660217285
Step: 25560, train/loss: 0.0
Step: 25560, train/grad_norm: 0.00012619192420970649
Step: 25560, train/learning_rate: 1.95859120140085e-05
Step: 25560, train/epoch: 6.082817554473877
Step: 25570, train/loss: 0.0
Step: 25570, train/grad_norm: 2.016936605286901e-06
Step: 25570, train/learning_rate: 1.9574012185330503e-05
Step: 25570, train/epoch: 6.085197448730469
Step: 25580, train/loss: 0.0
Step: 25580, train/grad_norm: 0.000396242510760203
Step: 25580, train/learning_rate: 1.956211417564191e-05
Step: 25580, train/epoch: 6.0875773429870605
Step: 25590, train/loss: 0.0
Step: 25590, train/grad_norm: 1.9377683457832973e-08
Step: 25590, train/learning_rate: 1.9550214346963912e-05
Step: 25590, train/epoch: 6.089957237243652
Step: 25600, train/loss: 0.0
Step: 25600, train/grad_norm: 2.9050315930589932e-08
Step: 25600, train/learning_rate: 1.9538314518285915e-05
Step: 25600, train/epoch: 6.092337131500244
Step: 25610, train/loss: 0.0
Step: 25610, train/grad_norm: 4.496454130276106e-05
Step: 25610, train/learning_rate: 1.9526416508597322e-05
Step: 25610, train/epoch: 6.094717025756836
Step: 25620, train/loss: 0.0
Step: 25620, train/grad_norm: 5.437613914693884e-09
Step: 25620, train/learning_rate: 1.9514516679919325e-05
Step: 25620, train/epoch: 6.0970964431762695
Step: 25630, train/loss: 0.0
Step: 25630, train/grad_norm: 1.173294990053364e-07
Step: 25630, train/learning_rate: 1.950261867023073e-05
Step: 25630, train/epoch: 6.099476337432861
Step: 25640, train/loss: 0.0
Step: 25640, train/grad_norm: 0.0023593439254909754
Step: 25640, train/learning_rate: 1.9490718841552734e-05
Step: 25640, train/epoch: 6.101856231689453
Step: 25650, train/loss: 0.0
Step: 25650, train/grad_norm: 4.199855084152659e-06
Step: 25650, train/learning_rate: 1.9478819012874737e-05
Step: 25650, train/epoch: 6.104236125946045
Step: 25660, train/loss: 0.0
Step: 25660, train/grad_norm: 8.010239582745271e-08
Step: 25660, train/learning_rate: 1.9466921003186144e-05
Step: 25660, train/epoch: 6.106616020202637
Step: 25670, train/loss: 0.0
Step: 25670, train/grad_norm: 3.912722146282022e-09
Step: 25670, train/learning_rate: 1.9455021174508147e-05
Step: 25670, train/epoch: 6.1089959144592285
Step: 25680, train/loss: 0.0
Step: 25680, train/grad_norm: 4.773630823962094e-09
Step: 25680, train/learning_rate: 1.9443123164819553e-05
Step: 25680, train/epoch: 6.111375331878662
Step: 25690, train/loss: 0.0
Step: 25690, train/grad_norm: 1.8611735868034884e-06
Step: 25690, train/learning_rate: 1.9431223336141557e-05
Step: 25690, train/epoch: 6.113755226135254
Step: 25700, train/loss: 0.0
Step: 25700, train/grad_norm: 0.00016715933452360332
Step: 25700, train/learning_rate: 1.941932350746356e-05
Step: 25700, train/epoch: 6.116135120391846
Step: 25710, train/loss: 0.0
Step: 25710, train/grad_norm: 8.681625196516052e-09
Step: 25710, train/learning_rate: 1.9407425497774966e-05
Step: 25710, train/epoch: 6.1185150146484375
Step: 25720, train/loss: 0.0
Step: 25720, train/grad_norm: 1.8415681424244212e-08
Step: 25720, train/learning_rate: 1.939552566909697e-05
Step: 25720, train/epoch: 6.120894908905029
Step: 25730, train/loss: 0.0
Step: 25730, train/grad_norm: 4.813795385416597e-05
Step: 25730, train/learning_rate: 1.9383627659408376e-05
Step: 25730, train/epoch: 6.123274803161621
Step: 25740, train/loss: 0.0
Step: 25740, train/grad_norm: 3.3035236768341747e-09
Step: 25740, train/learning_rate: 1.937172783073038e-05
Step: 25740, train/epoch: 6.125654220581055
Step: 25750, train/loss: 0.0
Step: 25750, train/grad_norm: 0.0002136354160029441
Step: 25750, train/learning_rate: 1.935982800205238e-05
Step: 25750, train/epoch: 6.1280341148376465
Step: 25760, train/loss: 0.0
Step: 25760, train/grad_norm: 2.9431966197535075e-08
Step: 25760, train/learning_rate: 1.9347929992363788e-05
Step: 25760, train/epoch: 6.130414009094238
Step: 25770, train/loss: 0.0
Step: 25770, train/grad_norm: 3.77897002579175e-09
Step: 25770, train/learning_rate: 1.933603016368579e-05
Step: 25770, train/epoch: 6.13279390335083
Step: 25780, train/loss: 0.0
Step: 25780, train/grad_norm: 6.654735784650256e-07
Step: 25780, train/learning_rate: 1.9324132153997198e-05
Step: 25780, train/epoch: 6.135173797607422
Step: 25790, train/loss: 0.0
Step: 25790, train/grad_norm: 6.279338435888349e-07
Step: 25790, train/learning_rate: 1.93122323253192e-05
Step: 25790, train/epoch: 6.137553691864014
Step: 25800, train/loss: 0.0
Step: 25800, train/grad_norm: 5.326254722604062e-06
Step: 25800, train/learning_rate: 1.9300332496641204e-05
Step: 25800, train/epoch: 6.1399335861206055
Step: 25810, train/loss: 0.0
Step: 25810, train/grad_norm: 7.367177090600308e-07
Step: 25810, train/learning_rate: 1.928843448695261e-05
Step: 25810, train/epoch: 6.142313003540039
Step: 25820, train/loss: 0.0
Step: 25820, train/grad_norm: 0.6744741201400757
Step: 25820, train/learning_rate: 1.9276534658274613e-05
Step: 25820, train/epoch: 6.144692897796631
Step: 25830, train/loss: 0.0
Step: 25830, train/grad_norm: 6.049273792996246e-08
Step: 25830, train/learning_rate: 1.926463664858602e-05
Step: 25830, train/epoch: 6.147072792053223
Step: 25840, train/loss: 0.0026000000070780516
Step: 25840, train/grad_norm: 1.7970066323869105e-07
Step: 25840, train/learning_rate: 1.9252736819908023e-05
Step: 25840, train/epoch: 6.1494526863098145
Step: 25850, train/loss: 0.0
Step: 25850, train/grad_norm: 2.9129803014171785e-09
Step: 25850, train/learning_rate: 1.9240836991230026e-05
Step: 25850, train/epoch: 6.151832580566406
Step: 25860, train/loss: 0.0
Step: 25860, train/grad_norm: 6.567257742062793e-07
Step: 25860, train/learning_rate: 1.9228938981541432e-05
Step: 25860, train/epoch: 6.154212474822998
Step: 25870, train/loss: 0.0
Step: 25870, train/grad_norm: 6.290410681231151e-08
Step: 25870, train/learning_rate: 1.9217039152863435e-05
Step: 25870, train/epoch: 6.156591892242432
Step: 25880, train/loss: 0.0
Step: 25880, train/grad_norm: 0.00032107214792631567
Step: 25880, train/learning_rate: 1.9205141143174842e-05
Step: 25880, train/epoch: 6.158971786499023
Step: 25890, train/loss: 0.0
Step: 25890, train/grad_norm: 1.025146101341079e-08
Step: 25890, train/learning_rate: 1.9193241314496845e-05
Step: 25890, train/epoch: 6.161351680755615
Step: 25900, train/loss: 0.0
Step: 25900, train/grad_norm: 4.5168974338594126e-07
Step: 25900, train/learning_rate: 1.9181341485818848e-05
Step: 25900, train/epoch: 6.163731575012207
Step: 25910, train/loss: 0.0
Step: 25910, train/grad_norm: 9.4525658411726e-09
Step: 25910, train/learning_rate: 1.9169443476130255e-05
Step: 25910, train/epoch: 6.166111469268799
Step: 25920, train/loss: 0.09610000252723694
Step: 25920, train/grad_norm: 3.4386284930576494e-09
Step: 25920, train/learning_rate: 1.9157543647452258e-05
Step: 25920, train/epoch: 6.168491363525391
Step: 25930, train/loss: 0.09529999643564224
Step: 25930, train/grad_norm: 2.1966333463296905e-07
Step: 25930, train/learning_rate: 1.9145645637763664e-05
Step: 25930, train/epoch: 6.170870780944824
Step: 25940, train/loss: 0.0
Step: 25940, train/grad_norm: 9.609086859541094e-09
Step: 25940, train/learning_rate: 1.9133745809085667e-05
Step: 25940, train/epoch: 6.173250675201416
Step: 25950, train/loss: 0.0
Step: 25950, train/grad_norm: 5.368427446228452e-05
Step: 25950, train/learning_rate: 1.912184598040767e-05
Step: 25950, train/epoch: 6.175630569458008
Step: 25960, train/loss: 0.0
Step: 25960, train/grad_norm: 1.9117310046112834e-07
Step: 25960, train/learning_rate: 1.9109947970719077e-05
Step: 25960, train/epoch: 6.1780104637146
Step: 25970, train/loss: 0.0
Step: 25970, train/grad_norm: 6.143679911474464e-07
Step: 25970, train/learning_rate: 1.909804814204108e-05
Step: 25970, train/epoch: 6.180390357971191
Step: 25980, train/loss: 0.0
Step: 25980, train/grad_norm: 2.832404391028831e-07
Step: 25980, train/learning_rate: 1.9086150132352486e-05
Step: 25980, train/epoch: 6.182770252227783
Step: 25990, train/loss: 0.0
Step: 25990, train/grad_norm: 5.814533506054431e-05
Step: 25990, train/learning_rate: 1.907425030367449e-05
Step: 25990, train/epoch: 6.185150146484375
Step: 26000, train/loss: 0.0
Step: 26000, train/grad_norm: 8.304437506012619e-05
Step: 26000, train/learning_rate: 1.9062350474996492e-05
Step: 26000, train/epoch: 6.187529563903809
Step: 26010, train/loss: 0.0
Step: 26010, train/grad_norm: 2.19457178900484e-05
Step: 26010, train/learning_rate: 1.90504524653079e-05
Step: 26010, train/epoch: 6.1899094581604
Step: 26020, train/loss: 0.0
Step: 26020, train/grad_norm: 0.00010065889364341274
Step: 26020, train/learning_rate: 1.9038552636629902e-05
Step: 26020, train/epoch: 6.192289352416992
Step: 26030, train/loss: 0.0
Step: 26030, train/grad_norm: 8.999686684774133e-08
Step: 26030, train/learning_rate: 1.902665462694131e-05
Step: 26030, train/epoch: 6.194669246673584
Step: 26040, train/loss: 0.0
Step: 26040, train/grad_norm: 3.375288315510261e-07
Step: 26040, train/learning_rate: 1.901475479826331e-05
Step: 26040, train/epoch: 6.197049140930176
Step: 26050, train/loss: 0.0
Step: 26050, train/grad_norm: 0.0013852925039827824
Step: 26050, train/learning_rate: 1.9002854969585314e-05
Step: 26050, train/epoch: 6.199429035186768
Step: 26060, train/loss: 0.0
Step: 26060, train/grad_norm: 2.442213364872714e-08
Step: 26060, train/learning_rate: 1.899095695989672e-05
Step: 26060, train/epoch: 6.201808452606201
Step: 26070, train/loss: 0.0
Step: 26070, train/grad_norm: 0.0002723150246310979
Step: 26070, train/learning_rate: 1.8979057131218724e-05
Step: 26070, train/epoch: 6.204188346862793
Step: 26080, train/loss: 0.0
Step: 26080, train/grad_norm: 3.3255837195156346e-08
Step: 26080, train/learning_rate: 1.896715912153013e-05
Step: 26080, train/epoch: 6.206568241119385
Step: 26090, train/loss: 0.0
Step: 26090, train/grad_norm: 1.1886412210060371e-07
Step: 26090, train/learning_rate: 1.8955259292852134e-05
Step: 26090, train/epoch: 6.208948135375977
Step: 26100, train/loss: 0.0
Step: 26100, train/grad_norm: 9.315246352059603e-09
Step: 26100, train/learning_rate: 1.8943359464174137e-05
Step: 26100, train/epoch: 6.211328029632568
Step: 26110, train/loss: 0.0
Step: 26110, train/grad_norm: 0.03655097633600235
Step: 26110, train/learning_rate: 1.8931461454485543e-05
Step: 26110, train/epoch: 6.21370792388916
Step: 26120, train/loss: 0.0
Step: 26120, train/grad_norm: 5.908037201152183e-05
Step: 26120, train/learning_rate: 1.8919561625807546e-05
Step: 26120, train/epoch: 6.216087341308594
Step: 26130, train/loss: 0.0
Step: 26130, train/grad_norm: 1.2168919738542172e-06
Step: 26130, train/learning_rate: 1.8907663616118953e-05
Step: 26130, train/epoch: 6.2184672355651855
Step: 26140, train/loss: 0.0
Step: 26140, train/grad_norm: 1.1011809419869678e-06
Step: 26140, train/learning_rate: 1.8895763787440956e-05
Step: 26140, train/epoch: 6.220847129821777
Step: 26150, train/loss: 0.0
Step: 26150, train/grad_norm: 0.0007218308164738119
Step: 26150, train/learning_rate: 1.888386395876296e-05
Step: 26150, train/epoch: 6.223227024078369
Step: 26160, train/loss: 0.0
Step: 26160, train/grad_norm: 3.4305432450310036e-07
Step: 26160, train/learning_rate: 1.8871965949074365e-05
Step: 26160, train/epoch: 6.225606918334961
Step: 26170, train/loss: 0.0
Step: 26170, train/grad_norm: 5.668153590931979e-08
Step: 26170, train/learning_rate: 1.8860066120396368e-05
Step: 26170, train/epoch: 6.227986812591553
Step: 26180, train/loss: 0.0
Step: 26180, train/grad_norm: 1.0486795787301162e-07
Step: 26180, train/learning_rate: 1.8848168110707775e-05
Step: 26180, train/epoch: 6.2303667068481445
Step: 26190, train/loss: 0.0
Step: 26190, train/grad_norm: 0.0005642790347337723
Step: 26190, train/learning_rate: 1.8836268282029778e-05
Step: 26190, train/epoch: 6.232746124267578
Step: 26200, train/loss: 0.0
Step: 26200, train/grad_norm: 2.4056793336058035e-05
Step: 26200, train/learning_rate: 1.882436845335178e-05
Step: 26200, train/epoch: 6.23512601852417
Step: 26210, train/loss: 0.0
Step: 26210, train/grad_norm: 0.007778164930641651
Step: 26210, train/learning_rate: 1.8812470443663187e-05
Step: 26210, train/epoch: 6.237505912780762
Step: 26220, train/loss: 0.0
Step: 26220, train/grad_norm: 7.79037168108232e-10
Step: 26220, train/learning_rate: 1.880057061498519e-05
Step: 26220, train/epoch: 6.2398858070373535
Step: 26230, train/loss: 0.0
Step: 26230, train/grad_norm: 5.081563722342253e-05
Step: 26230, train/learning_rate: 1.8788672605296597e-05
Step: 26230, train/epoch: 6.242265701293945
Step: 26240, train/loss: 0.0
Step: 26240, train/grad_norm: 1.502012381138229e-08
Step: 26240, train/learning_rate: 1.87767727766186e-05
Step: 26240, train/epoch: 6.244645595550537
Step: 26250, train/loss: 0.0
Step: 26250, train/grad_norm: 0.3014366626739502
Step: 26250, train/learning_rate: 1.8764874766930006e-05
Step: 26250, train/epoch: 6.247025012969971
Step: 26260, train/loss: 0.0052999998442828655
Step: 26260, train/grad_norm: 9.342608791484963e-06
Step: 26260, train/learning_rate: 1.875297493825201e-05
Step: 26260, train/epoch: 6.2494049072265625
Step: 26270, train/loss: 0.002400000113993883
Step: 26270, train/grad_norm: 0.0006045958143658936
Step: 26270, train/learning_rate: 1.8741075109574012e-05
Step: 26270, train/epoch: 6.251784801483154
Step: 26280, train/loss: 0.0
Step: 26280, train/grad_norm: 4.903942567580089e-07
Step: 26280, train/learning_rate: 1.872917709988542e-05
Step: 26280, train/epoch: 6.254164695739746
Step: 26290, train/loss: 0.05860000103712082
Step: 26290, train/grad_norm: 7.660146366106346e-06
Step: 26290, train/learning_rate: 1.8717277271207422e-05
Step: 26290, train/epoch: 6.256544589996338
Step: 26300, train/loss: 0.0
Step: 26300, train/grad_norm: 1.8278779378988474e-08
Step: 26300, train/learning_rate: 1.870537926151883e-05
Step: 26300, train/epoch: 6.25892448425293
Step: 26310, train/loss: 0.0
Step: 26310, train/grad_norm: 4.936294317303691e-06
Step: 26310, train/learning_rate: 1.869347943284083e-05
Step: 26310, train/epoch: 6.2613043785095215
Step: 26320, train/loss: 0.0
Step: 26320, train/grad_norm: 0.0007962214876897633
Step: 26320, train/learning_rate: 1.8681579604162835e-05
Step: 26320, train/epoch: 6.263683795928955
Step: 26330, train/loss: 0.0
Step: 26330, train/grad_norm: 6.265425156470883e-08
Step: 26330, train/learning_rate: 1.866968159447424e-05
Step: 26330, train/epoch: 6.266063690185547
Step: 26340, train/loss: 0.0
Step: 26340, train/grad_norm: 9.085419350185475e-08
Step: 26340, train/learning_rate: 1.8657781765796244e-05
Step: 26340, train/epoch: 6.268443584442139
Step: 26350, train/loss: 0.0
Step: 26350, train/grad_norm: 8.93828899783955e-11
Step: 26350, train/learning_rate: 1.864588375610765e-05
Step: 26350, train/epoch: 6.2708234786987305
Step: 26360, train/loss: 0.0
Step: 26360, train/grad_norm: 5.116796728543704e-07
Step: 26360, train/learning_rate: 1.8633983927429654e-05
Step: 26360, train/epoch: 6.273203372955322
Step: 26370, train/loss: 0.0
Step: 26370, train/grad_norm: 4.777938329425524e-07
Step: 26370, train/learning_rate: 1.8622084098751657e-05
Step: 26370, train/epoch: 6.275583267211914
Step: 26380, train/loss: 0.0
Step: 26380, train/grad_norm: 3.1063938088493614e-09
Step: 26380, train/learning_rate: 1.8610186089063063e-05
Step: 26380, train/epoch: 6.277962684631348
Step: 26390, train/loss: 0.0
Step: 26390, train/grad_norm: 7.253315170707708e-10
Step: 26390, train/learning_rate: 1.8598286260385066e-05
Step: 26390, train/epoch: 6.2803425788879395
Step: 26400, train/loss: 0.0
Step: 26400, train/grad_norm: 0.02596856653690338
Step: 26400, train/learning_rate: 1.8586388250696473e-05
Step: 26400, train/epoch: 6.282722473144531
Step: 26410, train/loss: 0.0
Step: 26410, train/grad_norm: 1.017625208987738e-06
Step: 26410, train/learning_rate: 1.8574488422018476e-05
Step: 26410, train/epoch: 6.285102367401123
Step: 26420, train/loss: 0.0
Step: 26420, train/grad_norm: 0.00495326379314065
Step: 26420, train/learning_rate: 1.856258859334048e-05
Step: 26420, train/epoch: 6.287482261657715
Step: 26430, train/loss: 0.0
Step: 26430, train/grad_norm: 3.153974859060327e-08
Step: 26430, train/learning_rate: 1.8550690583651885e-05
Step: 26430, train/epoch: 6.289862155914307
Step: 26440, train/loss: 0.0
Step: 26440, train/grad_norm: 1.0628533345879987e-05
Step: 26440, train/learning_rate: 1.853879075497389e-05
Step: 26440, train/epoch: 6.29224157333374
Step: 26450, train/loss: 0.0
Step: 26450, train/grad_norm: 2.0938716716045747e-07
Step: 26450, train/learning_rate: 1.8526892745285295e-05
Step: 26450, train/epoch: 6.294621467590332
Step: 26460, train/loss: 0.0
Step: 26460, train/grad_norm: 1.800681204144894e-08
Step: 26460, train/learning_rate: 1.8514992916607298e-05
Step: 26460, train/epoch: 6.297001361846924
Step: 26470, train/loss: 0.0
Step: 26470, train/grad_norm: 3.788852040997881e-07
Step: 26470, train/learning_rate: 1.85030930879293e-05
Step: 26470, train/epoch: 6.299381256103516
Step: 26480, train/loss: 0.0
Step: 26480, train/grad_norm: 3.0879044061293826e-05
Step: 26480, train/learning_rate: 1.8491195078240708e-05
Step: 26480, train/epoch: 6.301761150360107
Step: 26490, train/loss: 0.0
Step: 26490, train/grad_norm: 6.7810339565710365e-09
Step: 26490, train/learning_rate: 1.847929524956271e-05
Step: 26490, train/epoch: 6.304141044616699
Step: 26500, train/loss: 0.0
Step: 26500, train/grad_norm: 2.6294118882219664e-08
Step: 26500, train/learning_rate: 1.8467397239874117e-05
Step: 26500, train/epoch: 6.306520938873291
Step: 26510, train/loss: 0.0
Step: 26510, train/grad_norm: 2.172029084368532e-10
Step: 26510, train/learning_rate: 1.845549741119612e-05
Step: 26510, train/epoch: 6.308900356292725
Step: 26520, train/loss: 0.0
Step: 26520, train/grad_norm: 0.26685380935668945
Step: 26520, train/learning_rate: 1.8443597582518123e-05
Step: 26520, train/epoch: 6.311280250549316
Step: 26530, train/loss: 0.0
Step: 26530, train/grad_norm: 4.643451401165066e-09
Step: 26530, train/learning_rate: 1.843169957282953e-05
Step: 26530, train/epoch: 6.313660144805908
Step: 26540, train/loss: 0.0
Step: 26540, train/grad_norm: 0.003347157733514905
Step: 26540, train/learning_rate: 1.8419799744151533e-05
Step: 26540, train/epoch: 6.3160400390625
Step: 26550, train/loss: 0.003599999938160181
Step: 26550, train/grad_norm: 2.2110549835474558e-08
Step: 26550, train/learning_rate: 1.840790173446294e-05
Step: 26550, train/epoch: 6.318419933319092
Step: 26560, train/loss: 0.0
Step: 26560, train/grad_norm: 3.591257836887962e-06
Step: 26560, train/learning_rate: 1.8396001905784942e-05
Step: 26560, train/epoch: 6.320799827575684
Step: 26570, train/loss: 0.0
Step: 26570, train/grad_norm: 1.675512430665549e-05
Step: 26570, train/learning_rate: 1.8384102077106945e-05
Step: 26570, train/epoch: 6.323179244995117
Step: 26580, train/loss: 0.0
Step: 26580, train/grad_norm: 1.0830045766851981e-06
Step: 26580, train/learning_rate: 1.8372204067418352e-05
Step: 26580, train/epoch: 6.325559139251709
Step: 26590, train/loss: 0.0
Step: 26590, train/grad_norm: 3.772085144237991e-10
Step: 26590, train/learning_rate: 1.8360304238740355e-05
Step: 26590, train/epoch: 6.327939033508301
Step: 26600, train/loss: 0.0
Step: 26600, train/grad_norm: 9.785494897024405e-13
Step: 26600, train/learning_rate: 1.834840622905176e-05
Step: 26600, train/epoch: 6.330318927764893
Step: 26610, train/loss: 0.0
Step: 26610, train/grad_norm: 2.129100229453229e-10
Step: 26610, train/learning_rate: 1.8336506400373764e-05
Step: 26610, train/epoch: 6.332698822021484
Step: 26620, train/loss: 0.0
Step: 26620, train/grad_norm: 4.4217613511671194e-13
Step: 26620, train/learning_rate: 1.8324606571695767e-05
Step: 26620, train/epoch: 6.335078716278076
Step: 26630, train/loss: 0.007000000216066837
Step: 26630, train/grad_norm: 5.630057184347059e-11
Step: 26630, train/learning_rate: 1.8312708562007174e-05
Step: 26630, train/epoch: 6.33745813369751
Step: 26640, train/loss: 0.0
Step: 26640, train/grad_norm: 1.5431990130210238e-09
Step: 26640, train/learning_rate: 1.8300808733329177e-05
Step: 26640, train/epoch: 6.339838027954102
Step: 26650, train/loss: 0.0
Step: 26650, train/grad_norm: 1.5052750712563068e-11
Step: 26650, train/learning_rate: 1.8288910723640583e-05
Step: 26650, train/epoch: 6.342217922210693
Step: 26660, train/loss: 0.0
Step: 26660, train/grad_norm: 1.617746647752938e-08
Step: 26660, train/learning_rate: 1.8277010894962586e-05
Step: 26660, train/epoch: 6.344597816467285
Step: 26670, train/loss: 0.0
Step: 26670, train/grad_norm: 1.2767563698987128e-12
Step: 26670, train/learning_rate: 1.826511106628459e-05
Step: 26670, train/epoch: 6.346977710723877
Step: 26680, train/loss: 0.0
Step: 26680, train/grad_norm: 3.2591030568740464e-11
Step: 26680, train/learning_rate: 1.8253213056595996e-05
Step: 26680, train/epoch: 6.349357604980469
Step: 26690, train/loss: 0.0
Step: 26690, train/grad_norm: 1.1608772889368582e-14
Step: 26690, train/learning_rate: 1.8241313227918e-05
Step: 26690, train/epoch: 6.3517374992370605
Step: 26700, train/loss: 0.0
Step: 26700, train/grad_norm: 5.826050498156446e-14
Step: 26700, train/learning_rate: 1.8229415218229406e-05
Step: 26700, train/epoch: 6.354116916656494
Step: 26710, train/loss: 0.0
Step: 26710, train/grad_norm: 1.1483397344136392e-07
Step: 26710, train/learning_rate: 1.821751538955141e-05
Step: 26710, train/epoch: 6.356496810913086
Step: 26720, train/loss: 0.0
Step: 26720, train/grad_norm: 8.617398634669371e-07
Step: 26720, train/learning_rate: 1.820561556087341e-05
Step: 26720, train/epoch: 6.358876705169678
Step: 26730, train/loss: 0.0
Step: 26730, train/grad_norm: 1.1930094275426684e-17
Step: 26730, train/learning_rate: 1.8193717551184818e-05
Step: 26730, train/epoch: 6.3612565994262695
Step: 26740, train/loss: 0.0
Step: 26740, train/grad_norm: 4.71285843683944e-10
Step: 26740, train/learning_rate: 1.818181772250682e-05
Step: 26740, train/epoch: 6.363636493682861
Step: 26750, train/loss: 0.0
Step: 26750, train/grad_norm: 3.718591461230464e-13
Step: 26750, train/learning_rate: 1.8169919712818228e-05
Step: 26750, train/epoch: 6.366016387939453
Step: 26760, train/loss: 0.0
Step: 26760, train/grad_norm: 7.734441559437144e-11
Step: 26760, train/learning_rate: 1.815801988414023e-05
Step: 26760, train/epoch: 6.368395805358887
Step: 26770, train/loss: 0.0
Step: 26770, train/grad_norm: 2.163311058067663e-10
Step: 26770, train/learning_rate: 1.8146120055462234e-05
Step: 26770, train/epoch: 6.3707756996154785
Step: 26780, train/loss: 0.0
Step: 26780, train/grad_norm: 2.0203352879555325e-12
Step: 26780, train/learning_rate: 1.813422204577364e-05
Step: 26780, train/epoch: 6.37315559387207
Step: 26790, train/loss: 0.0
Step: 26790, train/grad_norm: 1.6118390178121444e-08
Step: 26790, train/learning_rate: 1.8122322217095643e-05
Step: 26790, train/epoch: 6.375535488128662
Step: 26800, train/loss: 0.0
Step: 26800, train/grad_norm: 2.1075062761610752e-07
Step: 26800, train/learning_rate: 1.811042420740705e-05
Step: 26800, train/epoch: 6.377915382385254
Step: 26810, train/loss: 0.0
Step: 26810, train/grad_norm: 6.979944508662961e-14
Step: 26810, train/learning_rate: 1.8098524378729053e-05
Step: 26810, train/epoch: 6.380295276641846
Step: 26820, train/loss: 0.0
Step: 26820, train/grad_norm: 3.606079126169774e-14
Step: 26820, train/learning_rate: 1.8086624550051056e-05
Step: 26820, train/epoch: 6.382674694061279
Step: 26830, train/loss: 0.0
Step: 26830, train/grad_norm: 3.882252693962734e-15
Step: 26830, train/learning_rate: 1.8074726540362462e-05
Step: 26830, train/epoch: 6.385054588317871
Step: 26840, train/loss: 0.0
Step: 26840, train/grad_norm: 2.4453872260465914e-09
Step: 26840, train/learning_rate: 1.8062826711684465e-05
Step: 26840, train/epoch: 6.387434482574463
Step: 26850, train/loss: 0.0
Step: 26850, train/grad_norm: 1.680726136044086e-08
Step: 26850, train/learning_rate: 1.8050928701995872e-05
Step: 26850, train/epoch: 6.389814376831055
Step: 26860, train/loss: 0.0
Step: 26860, train/grad_norm: 6.8130349723406064e-15
Step: 26860, train/learning_rate: 1.8039028873317875e-05
Step: 26860, train/epoch: 6.3921942710876465
Step: 26870, train/loss: 0.0
Step: 26870, train/grad_norm: 1.0380145500738999e-13
Step: 26870, train/learning_rate: 1.8027129044639878e-05
Step: 26870, train/epoch: 6.394574165344238
Step: 26880, train/loss: 0.0
Step: 26880, train/grad_norm: 4.750466686687105e-10
Step: 26880, train/learning_rate: 1.8015231034951285e-05
Step: 26880, train/epoch: 6.39695405960083
Step: 26890, train/loss: 0.0
Step: 26890, train/grad_norm: 2.230738438413482e-08
Step: 26890, train/learning_rate: 1.8003331206273288e-05
Step: 26890, train/epoch: 6.399333477020264
Step: 26900, train/loss: 0.0
Step: 26900, train/grad_norm: 3.6624425714393283e-09
Step: 26900, train/learning_rate: 1.7991433196584694e-05
Step: 26900, train/epoch: 6.4017133712768555
Step: 26910, train/loss: 0.0
Step: 26910, train/grad_norm: 2.8828878520514856e-11
Step: 26910, train/learning_rate: 1.7979533367906697e-05
Step: 26910, train/epoch: 6.404093265533447
Step: 26920, train/loss: 0.0
Step: 26920, train/grad_norm: 3.406259097005204e-08
Step: 26920, train/learning_rate: 1.7967635358218104e-05
Step: 26920, train/epoch: 6.406473159790039
Step: 26930, train/loss: 0.0
Step: 26930, train/grad_norm: 6.7287083023448435e-12
Step: 26930, train/learning_rate: 1.7955735529540107e-05
Step: 26930, train/epoch: 6.408853054046631
Step: 26940, train/loss: 0.0
Step: 26940, train/grad_norm: 2.7606221128517738e-11
Step: 26940, train/learning_rate: 1.794383570086211e-05
Step: 26940, train/epoch: 6.411232948303223
Step: 26950, train/loss: 0.0
Step: 26950, train/grad_norm: 2.1697730417935546e-12
Step: 26950, train/learning_rate: 1.7931937691173516e-05
Step: 26950, train/epoch: 6.413612365722656
Step: 26960, train/loss: 0.0
Step: 26960, train/grad_norm: 9.631026011425181e-12
Step: 26960, train/learning_rate: 1.792003786249552e-05
Step: 26960, train/epoch: 6.415992259979248
Step: 26970, train/loss: 0.0
Step: 26970, train/grad_norm: 1.1107865387043059e-13
Step: 26970, train/learning_rate: 1.7908139852806926e-05
Step: 26970, train/epoch: 6.41837215423584
Step: 26980, train/loss: 0.0
Step: 26980, train/grad_norm: 3.087013609147371e-13
Step: 26980, train/learning_rate: 1.789624002412893e-05
Step: 26980, train/epoch: 6.420752048492432
Step: 26990, train/loss: 0.0
Step: 26990, train/grad_norm: 1.172188603959512e-07
Step: 26990, train/learning_rate: 1.7884340195450932e-05
Step: 26990, train/epoch: 6.423131942749023
Step: 27000, train/loss: 0.0
Step: 27000, train/grad_norm: 4.641597978367884e-13
Step: 27000, train/learning_rate: 1.787244218576234e-05
Step: 27000, train/epoch: 6.425511837005615
Step: 27010, train/loss: 0.0
Step: 27010, train/grad_norm: 6.720610125268006e-14
Step: 27010, train/learning_rate: 1.786054235708434e-05
Step: 27010, train/epoch: 6.427891254425049
Step: 27020, train/loss: 0.0
Step: 27020, train/grad_norm: 3.7278695213215085e-13
Step: 27020, train/learning_rate: 1.7848644347395748e-05
Step: 27020, train/epoch: 6.430271148681641
Step: 27030, train/loss: 0.0
Step: 27030, train/grad_norm: 2.0710124153805465e-12
Step: 27030, train/learning_rate: 1.783674451871775e-05
Step: 27030, train/epoch: 6.432651042938232
Step: 27040, train/loss: 0.0
Step: 27040, train/grad_norm: 2.7167765259683563e-10
Step: 27040, train/learning_rate: 1.7824844690039754e-05
Step: 27040, train/epoch: 6.435030937194824
Step: 27050, train/loss: 0.0
Step: 27050, train/grad_norm: 5.746295366620968e-13
Step: 27050, train/learning_rate: 1.781294668035116e-05
Step: 27050, train/epoch: 6.437410831451416
Step: 27060, train/loss: 0.0
Step: 27060, train/grad_norm: 0.00021358189405873418
Step: 27060, train/learning_rate: 1.7801046851673163e-05
Step: 27060, train/epoch: 6.439790725708008
Step: 27070, train/loss: 0.0
Step: 27070, train/grad_norm: 1.4381916990657828e-08
Step: 27070, train/learning_rate: 1.778914884198457e-05
Step: 27070, train/epoch: 6.4421706199646
Step: 27080, train/loss: 0.0
Step: 27080, train/grad_norm: 4.5379873470086096e-11
Step: 27080, train/learning_rate: 1.7777249013306573e-05
Step: 27080, train/epoch: 6.444550037384033
Step: 27090, train/loss: 0.0
Step: 27090, train/grad_norm: 7.989852668366382e-10
Step: 27090, train/learning_rate: 1.7765349184628576e-05
Step: 27090, train/epoch: 6.446929931640625
Step: 27100, train/loss: 0.0
Step: 27100, train/grad_norm: 2.2552351097715473e-09
Step: 27100, train/learning_rate: 1.7753451174939983e-05
Step: 27100, train/epoch: 6.449309825897217
Step: 27110, train/loss: 0.013799999840557575
Step: 27110, train/grad_norm: 9.740362162669047e-12
Step: 27110, train/learning_rate: 1.7741551346261986e-05
Step: 27110, train/epoch: 6.451689720153809
Step: 27120, train/loss: 0.0
Step: 27120, train/grad_norm: 5.3188005215421e-12
Step: 27120, train/learning_rate: 1.7729653336573392e-05
Step: 27120, train/epoch: 6.4540696144104
Step: 27130, train/loss: 0.0
Step: 27130, train/grad_norm: 5.1769111220068e-10
Step: 27130, train/learning_rate: 1.7717753507895395e-05
Step: 27130, train/epoch: 6.456449508666992
Step: 27140, train/loss: 0.037300001829862595
Step: 27140, train/grad_norm: 0.050377216190099716
Step: 27140, train/learning_rate: 1.7705853679217398e-05
Step: 27140, train/epoch: 6.458828926086426
Step: 27150, train/loss: 0.0
Step: 27150, train/grad_norm: 5.229193789091369e-08
Step: 27150, train/learning_rate: 1.7693955669528805e-05
Step: 27150, train/epoch: 6.461208820343018
Step: 27160, train/loss: 0.0
Step: 27160, train/grad_norm: 4.4251210334911306e-11
Step: 27160, train/learning_rate: 1.7682055840850808e-05
Step: 27160, train/epoch: 6.463588714599609
Step: 27170, train/loss: 0.0
Step: 27170, train/grad_norm: 9.705241943436249e-09
Step: 27170, train/learning_rate: 1.7670157831162214e-05
Step: 27170, train/epoch: 6.465968608856201
Step: 27180, train/loss: 0.0
Step: 27180, train/grad_norm: 2.2713260092177556e-11
Step: 27180, train/learning_rate: 1.7658258002484217e-05
Step: 27180, train/epoch: 6.468348503112793
Step: 27190, train/loss: 0.0
Step: 27190, train/grad_norm: 2.6240957323730996e-11
Step: 27190, train/learning_rate: 1.764635817380622e-05
Step: 27190, train/epoch: 6.470728397369385
Step: 27200, train/loss: 0.0
Step: 27200, train/grad_norm: 8.248390685750007e-12
Step: 27200, train/learning_rate: 1.7634460164117627e-05
Step: 27200, train/epoch: 6.473107814788818
Step: 27210, train/loss: 0.0
Step: 27210, train/grad_norm: 1.2943361937800546e-09
Step: 27210, train/learning_rate: 1.762256033543963e-05
Step: 27210, train/epoch: 6.47548770904541
Step: 27220, train/loss: 0.0
Step: 27220, train/grad_norm: 1.7993377621206008e-11
Step: 27220, train/learning_rate: 1.7610662325751036e-05
Step: 27220, train/epoch: 6.477867603302002
Step: 27230, train/loss: 0.0
Step: 27230, train/grad_norm: 1.7061998391909583e-07
Step: 27230, train/learning_rate: 1.759876249707304e-05
Step: 27230, train/epoch: 6.480247497558594
Step: 27240, train/loss: 0.3188000023365021
Step: 27240, train/grad_norm: 7.45999173545897e-08
Step: 27240, train/learning_rate: 1.7586862668395042e-05
Step: 27240, train/epoch: 6.4826273918151855
Step: 27250, train/loss: 0.0
Step: 27250, train/grad_norm: 3.146143399135326e-06
Step: 27250, train/learning_rate: 1.757496465870645e-05
Step: 27250, train/epoch: 6.485007286071777
Step: 27260, train/loss: 0.0
Step: 27260, train/grad_norm: 3.729221873527422e-07
Step: 27260, train/learning_rate: 1.7563064830028452e-05
Step: 27260, train/epoch: 6.487387180328369
Step: 27270, train/loss: 0.0
Step: 27270, train/grad_norm: 6.94989284966141e-06
Step: 27270, train/learning_rate: 1.755116682033986e-05
Step: 27270, train/epoch: 6.489766597747803
Step: 27280, train/loss: 0.0
Step: 27280, train/grad_norm: 1.9787479686783627e-05
Step: 27280, train/learning_rate: 1.753926699166186e-05
Step: 27280, train/epoch: 6.4921464920043945
Step: 27290, train/loss: 0.0
Step: 27290, train/grad_norm: 6.445739586524724e-08
Step: 27290, train/learning_rate: 1.7527367162983865e-05
Step: 27290, train/epoch: 6.494526386260986
Step: 27300, train/loss: 0.0
Step: 27300, train/grad_norm: 3.4006698115263134e-05
Step: 27300, train/learning_rate: 1.751546915329527e-05
Step: 27300, train/epoch: 6.496906280517578
Step: 27310, train/loss: 0.0
Step: 27310, train/grad_norm: 0.0023141978308558464
Step: 27310, train/learning_rate: 1.7503569324617274e-05
Step: 27310, train/epoch: 6.49928617477417
Step: 27320, train/loss: 0.0
Step: 27320, train/grad_norm: 2.0745821529999375e-05
Step: 27320, train/learning_rate: 1.749167131492868e-05
Step: 27320, train/epoch: 6.501666069030762
Step: 27330, train/loss: 0.0
Step: 27330, train/grad_norm: 1.1732320359669757e-07
Step: 27330, train/learning_rate: 1.7479771486250684e-05
Step: 27330, train/epoch: 6.504045486450195
Step: 27340, train/loss: 0.0003000000142492354
Step: 27340, train/grad_norm: 5.537042966352601e-07
Step: 27340, train/learning_rate: 1.7467871657572687e-05
Step: 27340, train/epoch: 6.506425380706787
Step: 27350, train/loss: 0.0
Step: 27350, train/grad_norm: 1.355066672203975e-07
Step: 27350, train/learning_rate: 1.7455973647884093e-05
Step: 27350, train/epoch: 6.508805274963379
Step: 27360, train/loss: 0.0
Step: 27360, train/grad_norm: 1.0879839251742851e-08
Step: 27360, train/learning_rate: 1.7444073819206096e-05
Step: 27360, train/epoch: 6.511185169219971
Step: 27370, train/loss: 0.11169999837875366
Step: 27370, train/grad_norm: 1.2216229805517287e-08
Step: 27370, train/learning_rate: 1.7432175809517503e-05
Step: 27370, train/epoch: 6.5135650634765625
Step: 27380, train/loss: 0.0
Step: 27380, train/grad_norm: 3.74417339799038e-07
Step: 27380, train/learning_rate: 1.7420275980839506e-05
Step: 27380, train/epoch: 6.515944957733154
Step: 27390, train/loss: 0.0
Step: 27390, train/grad_norm: 1.0507951841987051e-08
Step: 27390, train/learning_rate: 1.740837615216151e-05
Step: 27390, train/epoch: 6.518324375152588
Step: 27400, train/loss: 0.0
Step: 27400, train/grad_norm: 1.8589057049211988e-07
Step: 27400, train/learning_rate: 1.7396478142472915e-05
Step: 27400, train/epoch: 6.52070426940918
Step: 27410, train/loss: 0.0
Step: 27410, train/grad_norm: 5.13641737143189e-07
Step: 27410, train/learning_rate: 1.738457831379492e-05
Step: 27410, train/epoch: 6.5230841636657715
Step: 27420, train/loss: 0.0
Step: 27420, train/grad_norm: 8.249593719256154e-08
Step: 27420, train/learning_rate: 1.7372680304106325e-05
Step: 27420, train/epoch: 6.525464057922363
Step: 27430, train/loss: 0.0
Step: 27430, train/grad_norm: 3.776174395397902e-08
Step: 27430, train/learning_rate: 1.7360780475428328e-05
Step: 27430, train/epoch: 6.527843952178955
Step: 27440, train/loss: 0.0
Step: 27440, train/grad_norm: 3.525918401692252e-08
Step: 27440, train/learning_rate: 1.734888064675033e-05
Step: 27440, train/epoch: 6.530223846435547
Step: 27450, train/loss: 0.0
Step: 27450, train/grad_norm: 1.537106015803147e-07
Step: 27450, train/learning_rate: 1.7336982637061737e-05
Step: 27450, train/epoch: 6.532603740692139
Step: 27460, train/loss: 0.0
Step: 27460, train/grad_norm: 5.88640546084207e-07
Step: 27460, train/learning_rate: 1.732508280838374e-05
Step: 27460, train/epoch: 6.534983158111572
Step: 27470, train/loss: 0.0
Step: 27470, train/grad_norm: 4.777043649539792e-09
Step: 27470, train/learning_rate: 1.7313184798695147e-05
Step: 27470, train/epoch: 6.537363052368164
Step: 27480, train/loss: 0.0
Step: 27480, train/grad_norm: 0.0003582515346352011
Step: 27480, train/learning_rate: 1.730128497001715e-05
Step: 27480, train/epoch: 6.539742946624756
Step: 27490, train/loss: 0.0
Step: 27490, train/grad_norm: 2.735137286435929e-06
Step: 27490, train/learning_rate: 1.7289385141339153e-05
Step: 27490, train/epoch: 6.542122840881348
Step: 27500, train/loss: 0.0
Step: 27500, train/grad_norm: 6.388403335222392e-07
Step: 27500, train/learning_rate: 1.727748713165056e-05
Step: 27500, train/epoch: 6.5445027351379395
Step: 27510, train/loss: 0.0
Step: 27510, train/grad_norm: 6.205340241649537e-07
Step: 27510, train/learning_rate: 1.7265587302972563e-05
Step: 27510, train/epoch: 6.546882629394531
Step: 27520, train/loss: 0.0
Step: 27520, train/grad_norm: 5.8537228397881336e-08
Step: 27520, train/learning_rate: 1.725368929328397e-05
Step: 27520, train/epoch: 6.549262046813965
Step: 27530, train/loss: 0.0
Step: 27530, train/grad_norm: 2.1921927384482842e-08
Step: 27530, train/learning_rate: 1.7241789464605972e-05
Step: 27530, train/epoch: 6.551641941070557
Step: 27540, train/loss: 0.0
Step: 27540, train/grad_norm: 4.0300078829602626e-09
Step: 27540, train/learning_rate: 1.7229889635927975e-05
Step: 27540, train/epoch: 6.554021835327148
Step: 27550, train/loss: 0.0
Step: 27550, train/grad_norm: 5.047161799609512e-09
Step: 27550, train/learning_rate: 1.7217991626239382e-05
Step: 27550, train/epoch: 6.55640172958374
Step: 27560, train/loss: 0.0
Step: 27560, train/grad_norm: 2.3481370590161532e-06
Step: 27560, train/learning_rate: 1.7206091797561385e-05
Step: 27560, train/epoch: 6.558781623840332
Step: 27570, train/loss: 0.0
Step: 27570, train/grad_norm: 5.1707482739971056e-09
Step: 27570, train/learning_rate: 1.719419378787279e-05
Step: 27570, train/epoch: 6.561161518096924
Step: 27580, train/loss: 0.0
Step: 27580, train/grad_norm: 6.240655547884444e-09
Step: 27580, train/learning_rate: 1.7182293959194794e-05
Step: 27580, train/epoch: 6.563540935516357
Step: 27590, train/loss: 0.0
Step: 27590, train/grad_norm: 3.0539973328025383e-10
Step: 27590, train/learning_rate: 1.71703959495062e-05
Step: 27590, train/epoch: 6.565920829772949
Step: 27600, train/loss: 0.375
Step: 27600, train/grad_norm: 7.49452766513059e-08
Step: 27600, train/learning_rate: 1.7158496120828204e-05
Step: 27600, train/epoch: 6.568300724029541
Step: 27610, train/loss: 0.04259999841451645
Step: 27610, train/grad_norm: 0.00026795416488312185
Step: 27610, train/learning_rate: 1.7146596292150207e-05
Step: 27610, train/epoch: 6.570680618286133
Step: 27620, train/loss: 0.0
Step: 27620, train/grad_norm: 0.015997543931007385
Step: 27620, train/learning_rate: 1.7134698282461613e-05
Step: 27620, train/epoch: 6.573060512542725
Step: 27630, train/loss: 0.0
Step: 27630, train/grad_norm: 6.051200398360379e-05
Step: 27630, train/learning_rate: 1.7122798453783616e-05
Step: 27630, train/epoch: 6.575440406799316
Step: 27640, train/loss: 0.0
Step: 27640, train/grad_norm: 0.004872606601566076
Step: 27640, train/learning_rate: 1.7110900444095023e-05
Step: 27640, train/epoch: 6.577820301055908
Step: 27650, train/loss: 0.0
Step: 27650, train/grad_norm: 2.0230268571452825e-09
Step: 27650, train/learning_rate: 1.7099000615417026e-05
Step: 27650, train/epoch: 6.580199718475342
Step: 27660, train/loss: 0.0
Step: 27660, train/grad_norm: 5.042795514498266e-09
Step: 27660, train/learning_rate: 1.708710078673903e-05
Step: 27660, train/epoch: 6.582579612731934
Step: 27670, train/loss: 0.0
Step: 27670, train/grad_norm: 5.121593253665235e-13
Step: 27670, train/learning_rate: 1.7075202777050436e-05
Step: 27670, train/epoch: 6.584959506988525
Step: 27680, train/loss: 0.0
Step: 27680, train/grad_norm: 4.820053163712146e-06
Step: 27680, train/learning_rate: 1.706330294837244e-05
Step: 27680, train/epoch: 6.587339401245117
Step: 27690, train/loss: 0.0
Step: 27690, train/grad_norm: 0.0006580461631529033
Step: 27690, train/learning_rate: 1.7051404938683845e-05
Step: 27690, train/epoch: 6.589719295501709
Step: 27700, train/loss: 0.0
Step: 27700, train/grad_norm: 4.415502644405933e-06
Step: 27700, train/learning_rate: 1.7039505110005848e-05
Step: 27700, train/epoch: 6.592099189758301
Step: 27710, train/loss: 0.0
Step: 27710, train/grad_norm: 6.361898954310163e-07
Step: 27710, train/learning_rate: 1.702760528132785e-05
Step: 27710, train/epoch: 6.594478607177734
Step: 27720, train/loss: 0.0
Step: 27720, train/grad_norm: 1.975134637177689e-06
Step: 27720, train/learning_rate: 1.7015707271639258e-05
Step: 27720, train/epoch: 6.596858501434326
Step: 27730, train/loss: 0.0
Step: 27730, train/grad_norm: 0.002299990737810731
Step: 27730, train/learning_rate: 1.700380744296126e-05
Step: 27730, train/epoch: 6.599238395690918
Step: 27740, train/loss: 0.0
Step: 27740, train/grad_norm: 2.4862324607966e-06
Step: 27740, train/learning_rate: 1.6991909433272667e-05
Step: 27740, train/epoch: 6.60161828994751
Step: 27750, train/loss: 0.0
Step: 27750, train/grad_norm: 2.903495499584352e-11
Step: 27750, train/learning_rate: 1.698000960459467e-05
Step: 27750, train/epoch: 6.603998184204102
Step: 27760, train/loss: 0.0
Step: 27760, train/grad_norm: 5.21237609518721e-07
Step: 27760, train/learning_rate: 1.6968109775916673e-05
Step: 27760, train/epoch: 6.606378078460693
Step: 27770, train/loss: 0.0
Step: 27770, train/grad_norm: 2.027112941505038e-06
Step: 27770, train/learning_rate: 1.695621176622808e-05
Step: 27770, train/epoch: 6.608757972717285
Step: 27780, train/loss: 0.0
Step: 27780, train/grad_norm: 3.976121547566436e-07
Step: 27780, train/learning_rate: 1.6944311937550083e-05
Step: 27780, train/epoch: 6.611137390136719
Step: 27790, train/loss: 0.0
Step: 27790, train/grad_norm: 1.4978022077824504e-12
Step: 27790, train/learning_rate: 1.693241392786149e-05
Step: 27790, train/epoch: 6.6135172843933105
Step: 27800, train/loss: 0.0
Step: 27800, train/grad_norm: 1.3190693607612047e-05
Step: 27800, train/learning_rate: 1.6920514099183492e-05
Step: 27800, train/epoch: 6.615897178649902
Step: 27810, train/loss: 0.0
Step: 27810, train/grad_norm: 8.856846420712827e-07
Step: 27810, train/learning_rate: 1.6908614270505495e-05
Step: 27810, train/epoch: 6.618277072906494
Step: 27820, train/loss: 0.0
Step: 27820, train/grad_norm: 1.0952206821457366e-06
Step: 27820, train/learning_rate: 1.6896716260816902e-05
Step: 27820, train/epoch: 6.620656967163086
Step: 27830, train/loss: 0.0
Step: 27830, train/grad_norm: 9.378281333738414e-07
Step: 27830, train/learning_rate: 1.6884816432138905e-05
Step: 27830, train/epoch: 6.623036861419678
Step: 27840, train/loss: 0.0
Step: 27840, train/grad_norm: 7.396602086373605e-08
Step: 27840, train/learning_rate: 1.687291842245031e-05
Step: 27840, train/epoch: 6.625416278839111
Step: 27850, train/loss: 0.0
Step: 27850, train/grad_norm: 4.822571497697936e-08
Step: 27850, train/learning_rate: 1.6861018593772314e-05
Step: 27850, train/epoch: 6.627796173095703
Step: 27860, train/loss: 0.0
Step: 27860, train/grad_norm: 4.958443605573848e-05
Step: 27860, train/learning_rate: 1.6849118765094317e-05
Step: 27860, train/epoch: 6.630176067352295
Step: 27870, train/loss: 0.0
Step: 27870, train/grad_norm: 1.735659438395487e-08
Step: 27870, train/learning_rate: 1.6837220755405724e-05
Step: 27870, train/epoch: 6.632555961608887
Step: 27880, train/loss: 0.0
Step: 27880, train/grad_norm: 4.2181565618193417e-07
Step: 27880, train/learning_rate: 1.6825320926727727e-05
Step: 27880, train/epoch: 6.6349358558654785
Step: 27890, train/loss: 0.13439999520778656
Step: 27890, train/grad_norm: 2.5397319404873997e-05
Step: 27890, train/learning_rate: 1.6813422917039134e-05
Step: 27890, train/epoch: 6.63731575012207
Step: 27900, train/loss: 0.0
Step: 27900, train/grad_norm: 4.8276291408910765e-08
Step: 27900, train/learning_rate: 1.6801523088361137e-05
Step: 27900, train/epoch: 6.639695167541504
Step: 27910, train/loss: 0.0
Step: 27910, train/grad_norm: 6.839152799642534e-09
Step: 27910, train/learning_rate: 1.678962325968314e-05
Step: 27910, train/epoch: 6.642075061798096
Step: 27920, train/loss: 9.999999747378752e-05
Step: 27920, train/grad_norm: 0.24224905669689178
Step: 27920, train/learning_rate: 1.6777725249994546e-05
Step: 27920, train/epoch: 6.6444549560546875
Step: 27930, train/loss: 0.0
Step: 27930, train/grad_norm: 3.160557389492169e-05
Step: 27930, train/learning_rate: 1.676582542131655e-05
Step: 27930, train/epoch: 6.646834850311279
Step: 27940, train/loss: 0.0
Step: 27940, train/grad_norm: 0.001767194946296513
Step: 27940, train/learning_rate: 1.6753927411627956e-05
Step: 27940, train/epoch: 6.649214744567871
Step: 27950, train/loss: 0.0
Step: 27950, train/grad_norm: 4.239736597355659e-07
Step: 27950, train/learning_rate: 1.674202758294996e-05
Step: 27950, train/epoch: 6.651594638824463
Step: 27960, train/loss: 0.0
Step: 27960, train/grad_norm: 1.9430713038559588e-08
Step: 27960, train/learning_rate: 1.6730127754271962e-05
Step: 27960, train/epoch: 6.653974533081055
Step: 27970, train/loss: 0.0
Step: 27970, train/grad_norm: 1.2414818684192142e-07
Step: 27970, train/learning_rate: 1.6718229744583368e-05
Step: 27970, train/epoch: 6.656353950500488
Step: 27980, train/loss: 0.0
Step: 27980, train/grad_norm: 2.0503239284153096e-05
Step: 27980, train/learning_rate: 1.670632991590537e-05
Step: 27980, train/epoch: 6.65873384475708
Step: 27990, train/loss: 0.0
Step: 27990, train/grad_norm: 6.889253967301556e-08
Step: 27990, train/learning_rate: 1.6694431906216778e-05
Step: 27990, train/epoch: 6.661113739013672
Step: 28000, train/loss: 0.0
Step: 28000, train/grad_norm: 1.1551224865513632e-08
Step: 28000, train/learning_rate: 1.668253207753878e-05
Step: 28000, train/epoch: 6.663493633270264
Step: 28010, train/loss: 0.0
Step: 28010, train/grad_norm: 0.00016227811283897609
Step: 28010, train/learning_rate: 1.6670632248860784e-05
Step: 28010, train/epoch: 6.6658735275268555
Step: 28020, train/loss: 0.0
Step: 28020, train/grad_norm: 1.4255789437811472e-06
Step: 28020, train/learning_rate: 1.665873423917219e-05
Step: 28020, train/epoch: 6.668253421783447
Step: 28030, train/loss: 0.0
Step: 28030, train/grad_norm: 1.1122657639361933e-07
Step: 28030, train/learning_rate: 1.6646834410494193e-05
Step: 28030, train/epoch: 6.670632839202881
Step: 28040, train/loss: 0.0
Step: 28040, train/grad_norm: 9.09986556507647e-06
Step: 28040, train/learning_rate: 1.66349364008056e-05
Step: 28040, train/epoch: 6.673012733459473
Step: 28050, train/loss: 0.0
Step: 28050, train/grad_norm: 5.178553692530841e-06
Step: 28050, train/learning_rate: 1.6623036572127603e-05
Step: 28050, train/epoch: 6.6753926277160645
Step: 28060, train/loss: 0.0
Step: 28060, train/grad_norm: 2.563076009209908e-07
Step: 28060, train/learning_rate: 1.6611136743449606e-05
Step: 28060, train/epoch: 6.677772521972656
Step: 28070, train/loss: 0.0
Step: 28070, train/grad_norm: 7.668220547429883e-08
Step: 28070, train/learning_rate: 1.6599238733761013e-05
Step: 28070, train/epoch: 6.680152416229248
Step: 28080, train/loss: 0.0
Step: 28080, train/grad_norm: 5.764719257506101e-10
Step: 28080, train/learning_rate: 1.6587338905083016e-05
Step: 28080, train/epoch: 6.68253231048584
Step: 28090, train/loss: 0.0
Step: 28090, train/grad_norm: 2.761972552889347e-07
Step: 28090, train/learning_rate: 1.6575440895394422e-05
Step: 28090, train/epoch: 6.684911727905273
Step: 28100, train/loss: 0.0
Step: 28100, train/grad_norm: 7.771038326609414e-06
Step: 28100, train/learning_rate: 1.6563541066716425e-05
Step: 28100, train/epoch: 6.687291622161865
Step: 28110, train/loss: 0.0
Step: 28110, train/grad_norm: 1.4275748583258974e-07
Step: 28110, train/learning_rate: 1.6551641238038428e-05
Step: 28110, train/epoch: 6.689671516418457
Step: 28120, train/loss: 0.0
Step: 28120, train/grad_norm: 7.495584242178666e-08
Step: 28120, train/learning_rate: 1.6539743228349835e-05
Step: 28120, train/epoch: 6.692051410675049
Step: 28130, train/loss: 0.0
Step: 28130, train/grad_norm: 7.285890291086616e-08
Step: 28130, train/learning_rate: 1.6527843399671838e-05
Step: 28130, train/epoch: 6.694431304931641
Step: 28140, train/loss: 0.0
Step: 28140, train/grad_norm: 1.6101110134059127e-07
Step: 28140, train/learning_rate: 1.6515945389983244e-05
Step: 28140, train/epoch: 6.696811199188232
Step: 28150, train/loss: 0.0
Step: 28150, train/grad_norm: 4.9460076922969165e-08
Step: 28150, train/learning_rate: 1.6504045561305247e-05
Step: 28150, train/epoch: 6.699191093444824
Step: 28160, train/loss: 0.0
Step: 28160, train/grad_norm: 2.328069683699141e-07
Step: 28160, train/learning_rate: 1.649214573262725e-05
Step: 28160, train/epoch: 6.701570510864258
Step: 28170, train/loss: 0.0
Step: 28170, train/grad_norm: 9.653652877084085e-11
Step: 28170, train/learning_rate: 1.6480247722938657e-05
Step: 28170, train/epoch: 6.70395040512085
Step: 28180, train/loss: 0.0
Step: 28180, train/grad_norm: 3.3532619454490487e-06
Step: 28180, train/learning_rate: 1.646834789426066e-05
Step: 28180, train/epoch: 6.706330299377441
Step: 28190, train/loss: 0.0
Step: 28190, train/grad_norm: 2.2188029902281414e-07
Step: 28190, train/learning_rate: 1.6456449884572066e-05
Step: 28190, train/epoch: 6.708710193634033
Step: 28200, train/loss: 0.0
Step: 28200, train/grad_norm: 3.77729520550929e-05
Step: 28200, train/learning_rate: 1.644455005589407e-05
Step: 28200, train/epoch: 6.711090087890625
Step: 28210, train/loss: 0.0
Step: 28210, train/grad_norm: 6.057934598402426e-08
Step: 28210, train/learning_rate: 1.6432650227216072e-05
Step: 28210, train/epoch: 6.713469982147217
Step: 28220, train/loss: 0.0
Step: 28220, train/grad_norm: 2.0306414327819766e-09
Step: 28220, train/learning_rate: 1.642075221752748e-05
Step: 28220, train/epoch: 6.71584939956665
Step: 28230, train/loss: 0.0
Step: 28230, train/grad_norm: 2.182726348110009e-05
Step: 28230, train/learning_rate: 1.6408852388849482e-05
Step: 28230, train/epoch: 6.718229293823242
Step: 28240, train/loss: 0.028300000354647636
Step: 28240, train/grad_norm: 5.397274294161036e-10
Step: 28240, train/learning_rate: 1.639695437916089e-05
Step: 28240, train/epoch: 6.720609188079834
Step: 28250, train/loss: 0.0
Step: 28250, train/grad_norm: 1.624235483177472e-05
Step: 28250, train/learning_rate: 1.638505455048289e-05
Step: 28250, train/epoch: 6.722989082336426
Step: 28260, train/loss: 0.13439999520778656
Step: 28260, train/grad_norm: 120.40311431884766
Step: 28260, train/learning_rate: 1.6373156540794298e-05
Step: 28260, train/epoch: 6.725368976593018
Step: 28270, train/loss: 0.0
Step: 28270, train/grad_norm: 6.584033144463319e-06
Step: 28270, train/learning_rate: 1.63612567121163e-05
Step: 28270, train/epoch: 6.727748870849609
Step: 28280, train/loss: 0.0
Step: 28280, train/grad_norm: 2.612709124605317e-07
Step: 28280, train/learning_rate: 1.6349356883438304e-05
Step: 28280, train/epoch: 6.730128288269043
Step: 28290, train/loss: 0.0
Step: 28290, train/grad_norm: 0.0001116754428949207
Step: 28290, train/learning_rate: 1.633745887374971e-05
Step: 28290, train/epoch: 6.732508182525635
Step: 28300, train/loss: 0.0
Step: 28300, train/grad_norm: 1.507891170149378e-06
Step: 28300, train/learning_rate: 1.6325559045071714e-05
Step: 28300, train/epoch: 6.734888076782227
Step: 28310, train/loss: 0.0
Step: 28310, train/grad_norm: 3.392242736710571e-11
Step: 28310, train/learning_rate: 1.631366103538312e-05
Step: 28310, train/epoch: 6.737267971038818
Step: 28320, train/loss: 0.0
Step: 28320, train/grad_norm: 6.163968646433204e-05
Step: 28320, train/learning_rate: 1.6301761206705123e-05
Step: 28320, train/epoch: 6.73964786529541
Step: 28330, train/loss: 0.0
Step: 28330, train/grad_norm: 0.001455096760764718
Step: 28330, train/learning_rate: 1.6289861378027126e-05
Step: 28330, train/epoch: 6.742027759552002
Step: 28340, train/loss: 0.0
Step: 28340, train/grad_norm: 4.8983732995111495e-05
Step: 28340, train/learning_rate: 1.6277963368338533e-05
Step: 28340, train/epoch: 6.744407653808594
Step: 28350, train/loss: 0.0
Step: 28350, train/grad_norm: 1.473622660341789e-06
Step: 28350, train/learning_rate: 1.6266063539660536e-05
Step: 28350, train/epoch: 6.746787071228027
Step: 28360, train/loss: 0.0
Step: 28360, train/grad_norm: 1.1409686521801632e-05
Step: 28360, train/learning_rate: 1.6254165529971942e-05
Step: 28360, train/epoch: 6.749166965484619
Step: 28370, train/loss: 0.0
Step: 28370, train/grad_norm: 1.4701365103064745e-07
Step: 28370, train/learning_rate: 1.6242265701293945e-05
Step: 28370, train/epoch: 6.751546859741211
Step: 28380, train/loss: 0.0
Step: 28380, train/grad_norm: 2.6971342936832343e-09
Step: 28380, train/learning_rate: 1.623036587261595e-05
Step: 28380, train/epoch: 6.753926753997803
Step: 28390, train/loss: 0.0
Step: 28390, train/grad_norm: 6.027476956660394e-06
Step: 28390, train/learning_rate: 1.6218467862927355e-05
Step: 28390, train/epoch: 6.7563066482543945
Step: 28400, train/loss: 0.0
Step: 28400, train/grad_norm: 5.471093572850805e-06
Step: 28400, train/learning_rate: 1.6206568034249358e-05
Step: 28400, train/epoch: 6.758686542510986
Step: 28410, train/loss: 0.0
Step: 28410, train/grad_norm: 1.8994461470356327e-07
Step: 28410, train/learning_rate: 1.6194670024560764e-05
Step: 28410, train/epoch: 6.76106595993042
Step: 28420, train/loss: 0.0
Step: 28420, train/grad_norm: 6.073050826671533e-06
Step: 28420, train/learning_rate: 1.6182770195882767e-05
Step: 28420, train/epoch: 6.763445854187012
Step: 28430, train/loss: 0.0
Step: 28430, train/grad_norm: 4.6294604771901504e-07
Step: 28430, train/learning_rate: 1.617087036720477e-05
Step: 28430, train/epoch: 6.7658257484436035
Step: 28440, train/loss: 0.0
Step: 28440, train/grad_norm: 1.2119676284783054e-07
Step: 28440, train/learning_rate: 1.6158972357516177e-05
Step: 28440, train/epoch: 6.768205642700195
Step: 28450, train/loss: 0.0
Step: 28450, train/grad_norm: 0.0001527548156445846
Step: 28450, train/learning_rate: 1.614707252883818e-05
Step: 28450, train/epoch: 6.770585536956787
Step: 28460, train/loss: 0.0
Step: 28460, train/grad_norm: 4.4626222717170094e-08
Step: 28460, train/learning_rate: 1.6135174519149587e-05
Step: 28460, train/epoch: 6.772965431213379
Step: 28470, train/loss: 0.00019999999494757503
Step: 28470, train/grad_norm: 0.000161983713041991
Step: 28470, train/learning_rate: 1.612327469047159e-05
Step: 28470, train/epoch: 6.7753448486328125
Step: 28480, train/loss: 0.0
Step: 28480, train/grad_norm: 1.6474614312755875e-06
Step: 28480, train/learning_rate: 1.6111374861793593e-05
Step: 28480, train/epoch: 6.777724742889404
Step: 28490, train/loss: 0.0
Step: 28490, train/grad_norm: 6.381218554452062e-05
Step: 28490, train/learning_rate: 1.6099476852105e-05
Step: 28490, train/epoch: 6.780104637145996
Step: 28500, train/loss: 0.0
Step: 28500, train/grad_norm: 1.7168506616371815e-08
Step: 28500, train/learning_rate: 1.6087577023427002e-05
Step: 28500, train/epoch: 6.782484531402588
Step: 28510, train/loss: 0.0
Step: 28510, train/grad_norm: 6.656341042798886e-07
Step: 28510, train/learning_rate: 1.607567901373841e-05
Step: 28510, train/epoch: 6.78486442565918
Step: 28520, train/loss: 0.14139999449253082
Step: 28520, train/grad_norm: 8.352769509656355e-05
Step: 28520, train/learning_rate: 1.606377918506041e-05
Step: 28520, train/epoch: 6.7872443199157715
Step: 28530, train/loss: 0.0
Step: 28530, train/grad_norm: 0.00013050256529822946
Step: 28530, train/learning_rate: 1.6051879356382415e-05
Step: 28530, train/epoch: 6.789624214172363
Step: 28540, train/loss: 0.0
Step: 28540, train/grad_norm: 0.0018765537533909082
Step: 28540, train/learning_rate: 1.603998134669382e-05
Step: 28540, train/epoch: 6.792003631591797
Step: 28550, train/loss: 0.0
Step: 28550, train/grad_norm: 8.917599188862368e-05
Step: 28550, train/learning_rate: 1.6028081518015824e-05
Step: 28550, train/epoch: 6.794383525848389
Step: 28560, train/loss: 0.0
Step: 28560, train/grad_norm: 6.903044413775206e-05
Step: 28560, train/learning_rate: 1.601618350832723e-05
Step: 28560, train/epoch: 6.7967634201049805
Step: 28570, train/loss: 0.0
Step: 28570, train/grad_norm: 5.591857643594267e-08
Step: 28570, train/learning_rate: 1.6004283679649234e-05
Step: 28570, train/epoch: 6.799143314361572
Step: 28580, train/loss: 0.0
Step: 28580, train/grad_norm: 1.8823806385626085e-05
Step: 28580, train/learning_rate: 1.5992383850971237e-05
Step: 28580, train/epoch: 6.801523208618164
Step: 28590, train/loss: 0.0
Step: 28590, train/grad_norm: 0.0007459642947651446
Step: 28590, train/learning_rate: 1.5980485841282643e-05
Step: 28590, train/epoch: 6.803903102874756
Step: 28600, train/loss: 0.00019999999494757503
Step: 28600, train/grad_norm: 0.0548182874917984
Step: 28600, train/learning_rate: 1.5968586012604646e-05
Step: 28600, train/epoch: 6.8062825202941895
Step: 28610, train/loss: 0.0
Step: 28610, train/grad_norm: 2.570223659859039e-05
Step: 28610, train/learning_rate: 1.5956688002916053e-05
Step: 28610, train/epoch: 6.808662414550781
Step: 28620, train/loss: 0.0
Step: 28620, train/grad_norm: 0.0032871034927666187
Step: 28620, train/learning_rate: 1.5944788174238056e-05
Step: 28620, train/epoch: 6.811042308807373
Step: 28630, train/loss: 0.0
Step: 28630, train/grad_norm: 0.00012001051072729751
Step: 28630, train/learning_rate: 1.593288834556006e-05
Step: 28630, train/epoch: 6.813422203063965
Step: 28640, train/loss: 9.999999747378752e-05
Step: 28640, train/grad_norm: 3.041597843170166
Step: 28640, train/learning_rate: 1.5920990335871466e-05
Step: 28640, train/epoch: 6.815802097320557
Step: 28650, train/loss: 0.0
Step: 28650, train/grad_norm: 6.781623937968106e-07
Step: 28650, train/learning_rate: 1.590909050719347e-05
Step: 28650, train/epoch: 6.818181991577148
Step: 28660, train/loss: 0.08479999750852585
Step: 28660, train/grad_norm: 4.486211935272877e-08
Step: 28660, train/learning_rate: 1.5897192497504875e-05
Step: 28660, train/epoch: 6.820561408996582
Step: 28670, train/loss: 0.0
Step: 28670, train/grad_norm: 7.729321396254818e-07
Step: 28670, train/learning_rate: 1.5885292668826878e-05
Step: 28670, train/epoch: 6.822941303253174
Step: 28680, train/loss: 9.999999747378752e-05
Step: 28680, train/grad_norm: 0.025531301274895668
Step: 28680, train/learning_rate: 1.587339284014888e-05
Step: 28680, train/epoch: 6.825321197509766
Step: 28690, train/loss: 0.0
Step: 28690, train/grad_norm: 2.0719149063097575e-08
Step: 28690, train/learning_rate: 1.5861494830460288e-05
Step: 28690, train/epoch: 6.827701091766357
Step: 28700, train/loss: 0.0
Step: 28700, train/grad_norm: 6.617967301281169e-06
Step: 28700, train/learning_rate: 1.584959500178229e-05
Step: 28700, train/epoch: 6.830080986022949
Step: 28710, train/loss: 0.0
Step: 28710, train/grad_norm: 0.000241545814787969
Step: 28710, train/learning_rate: 1.5837696992093697e-05
Step: 28710, train/epoch: 6.832460880279541
Step: 28720, train/loss: 0.03460000082850456
Step: 28720, train/grad_norm: 1.0560395224956665e-07
Step: 28720, train/learning_rate: 1.58257971634157e-05
Step: 28720, train/epoch: 6.834840774536133
Step: 28730, train/loss: 0.0
Step: 28730, train/grad_norm: 3.112676694172478e-08
Step: 28730, train/learning_rate: 1.5813897334737703e-05
Step: 28730, train/epoch: 6.837220191955566
Step: 28740, train/loss: 0.0
Step: 28740, train/grad_norm: 1.1758882578760677e-07
Step: 28740, train/learning_rate: 1.580199932504911e-05
Step: 28740, train/epoch: 6.839600086212158
Step: 28750, train/loss: 0.0
Step: 28750, train/grad_norm: 1.730917392706033e-05
Step: 28750, train/learning_rate: 1.5790099496371113e-05
Step: 28750, train/epoch: 6.84197998046875
Step: 28760, train/loss: 0.0005000000237487257
Step: 28760, train/grad_norm: 2.586821956640506e-08
Step: 28760, train/learning_rate: 1.577820148668252e-05
Step: 28760, train/epoch: 6.844359874725342
Step: 28770, train/loss: 0.0
Step: 28770, train/grad_norm: 6.579153932761983e-07
Step: 28770, train/learning_rate: 1.5766301658004522e-05
Step: 28770, train/epoch: 6.846739768981934
Step: 28780, train/loss: 0.0
Step: 28780, train/grad_norm: 0.0002610513474792242
Step: 28780, train/learning_rate: 1.5754401829326525e-05
Step: 28780, train/epoch: 6.849119663238525
Step: 28790, train/loss: 0.0
Step: 28790, train/grad_norm: 2.6148669348913245e-05
Step: 28790, train/learning_rate: 1.5742503819637932e-05
Step: 28790, train/epoch: 6.851499080657959
Step: 28800, train/loss: 0.0
Step: 28800, train/grad_norm: 9.61031332735729e-07
Step: 28800, train/learning_rate: 1.5730603990959935e-05
Step: 28800, train/epoch: 6.853878974914551
Step: 28810, train/loss: 0.0
Step: 28810, train/grad_norm: 0.0018744512926787138
Step: 28810, train/learning_rate: 1.571870598127134e-05
Step: 28810, train/epoch: 6.856258869171143
Step: 28820, train/loss: 0.0
Step: 28820, train/grad_norm: 2.6975718355970457e-05
Step: 28820, train/learning_rate: 1.5706806152593344e-05
Step: 28820, train/epoch: 6.858638763427734
Step: 28830, train/loss: 0.0
Step: 28830, train/grad_norm: 4.7080186504899757e-07
Step: 28830, train/learning_rate: 1.5694906323915347e-05
Step: 28830, train/epoch: 6.861018657684326
Step: 28840, train/loss: 0.0
Step: 28840, train/grad_norm: 1.4123273217592214e-08
Step: 28840, train/learning_rate: 1.5683008314226754e-05
Step: 28840, train/epoch: 6.863398551940918
Step: 28850, train/loss: 0.0
Step: 28850, train/grad_norm: 1.5150061699387152e-05
Step: 28850, train/learning_rate: 1.5671108485548757e-05
Step: 28850, train/epoch: 6.865777969360352
Step: 28860, train/loss: 0.0
Step: 28860, train/grad_norm: 8.484485647386464e-08
Step: 28860, train/learning_rate: 1.5659210475860164e-05
Step: 28860, train/epoch: 6.868157863616943
Step: 28870, train/loss: 0.0
Step: 28870, train/grad_norm: 2.7105863864562707e-07
Step: 28870, train/learning_rate: 1.5647310647182167e-05
Step: 28870, train/epoch: 6.870537757873535
Step: 28880, train/loss: 0.0
Step: 28880, train/grad_norm: 3.776732260263316e-09
Step: 28880, train/learning_rate: 1.563541081850417e-05
Step: 28880, train/epoch: 6.872917652130127
Step: 28890, train/loss: 0.0
Step: 28890, train/grad_norm: 5.6234315707115456e-05
Step: 28890, train/learning_rate: 1.5623512808815576e-05
Step: 28890, train/epoch: 6.875297546386719
Step: 28900, train/loss: 0.0
Step: 28900, train/grad_norm: 2.4442472934538273e-08
Step: 28900, train/learning_rate: 1.561161298013758e-05
Step: 28900, train/epoch: 6.8776774406433105
Step: 28910, train/loss: 0.0
Step: 28910, train/grad_norm: 7.969149919517804e-06
Step: 28910, train/learning_rate: 1.5599714970448986e-05
Step: 28910, train/epoch: 6.880057334899902
Step: 28920, train/loss: 0.0
Step: 28920, train/grad_norm: 2.4550462285333197e-07
Step: 28920, train/learning_rate: 1.558781514177099e-05
Step: 28920, train/epoch: 6.882436752319336
Step: 28930, train/loss: 0.0
Step: 28930, train/grad_norm: 6.82251810601997e-09
Step: 28930, train/learning_rate: 1.5575917132082395e-05
Step: 28930, train/epoch: 6.884816646575928
Step: 28940, train/loss: 0.0
Step: 28940, train/grad_norm: 3.295557959859252e-08
Step: 28940, train/learning_rate: 1.5564017303404398e-05
Step: 28940, train/epoch: 6.8871965408325195
Step: 28950, train/loss: 0.05429999902844429
Step: 28950, train/grad_norm: 3.28484794920314e-09
Step: 28950, train/learning_rate: 1.55521174747264e-05
Step: 28950, train/epoch: 6.889576435089111
Step: 28960, train/loss: 0.0
Step: 28960, train/grad_norm: 3.0237510145525448e-05
Step: 28960, train/learning_rate: 1.5540219465037808e-05
Step: 28960, train/epoch: 6.891956329345703
Step: 28970, train/loss: 0.0
Step: 28970, train/grad_norm: 5.01941554986729e-09
Step: 28970, train/learning_rate: 1.552831963635981e-05
Step: 28970, train/epoch: 6.894336223602295
Step: 28980, train/loss: 0.0
Step: 28980, train/grad_norm: 5.634726463199513e-09
Step: 28980, train/learning_rate: 1.5516421626671217e-05
Step: 28980, train/epoch: 6.8967156410217285
Step: 28990, train/loss: 0.0
Step: 28990, train/grad_norm: 5.904687895963434e-06
Step: 28990, train/learning_rate: 1.550452179799322e-05
Step: 28990, train/epoch: 6.89909553527832
Step: 29000, train/loss: 0.0
Step: 29000, train/grad_norm: 6.772675078536849e-07
Step: 29000, train/learning_rate: 1.5492621969315223e-05
Step: 29000, train/epoch: 6.901475429534912
Step: 29010, train/loss: 0.1234000027179718
Step: 29010, train/grad_norm: 7.12178079993464e-05
Step: 29010, train/learning_rate: 1.548072395962663e-05
Step: 29010, train/epoch: 6.903855323791504
Step: 29020, train/loss: 0.0
Step: 29020, train/grad_norm: 1.3608728295366745e-05
Step: 29020, train/learning_rate: 1.5468824130948633e-05
Step: 29020, train/epoch: 6.906235218048096
Step: 29030, train/loss: 0.0
Step: 29030, train/grad_norm: 3.032665881619323e-05
Step: 29030, train/learning_rate: 1.545692612126004e-05
Step: 29030, train/epoch: 6.9086151123046875
Step: 29040, train/loss: 0.0
Step: 29040, train/grad_norm: 1.9605684428825043e-05
Step: 29040, train/learning_rate: 1.5445026292582043e-05
Step: 29040, train/epoch: 6.910994529724121
Step: 29050, train/loss: 0.0
Step: 29050, train/grad_norm: 9.064200276043266e-05
Step: 29050, train/learning_rate: 1.5433126463904046e-05
Step: 29050, train/epoch: 6.913374423980713
Step: 29060, train/loss: 0.0
Step: 29060, train/grad_norm: 5.643680651701288e-07
Step: 29060, train/learning_rate: 1.5421228454215452e-05
Step: 29060, train/epoch: 6.915754318237305
Step: 29070, train/loss: 0.0
Step: 29070, train/grad_norm: 7.205825340861338e-07
Step: 29070, train/learning_rate: 1.5409328625537455e-05
Step: 29070, train/epoch: 6.9181342124938965
Step: 29080, train/loss: 0.0
Step: 29080, train/grad_norm: 8.584841865166837e-11
Step: 29080, train/learning_rate: 1.539743061584886e-05
Step: 29080, train/epoch: 6.920514106750488
Step: 29090, train/loss: 0.0
Step: 29090, train/grad_norm: 6.241494787673219e-08
Step: 29090, train/learning_rate: 1.5385530787170865e-05
Step: 29090, train/epoch: 6.92289400100708
Step: 29100, train/loss: 0.0
Step: 29100, train/grad_norm: 1.519418105999648e-06
Step: 29100, train/learning_rate: 1.5373630958492868e-05
Step: 29100, train/epoch: 6.925273895263672
Step: 29110, train/loss: 0.0
Step: 29110, train/grad_norm: 3.239705256419256e-05
Step: 29110, train/learning_rate: 1.5361732948804274e-05
Step: 29110, train/epoch: 6.9276533126831055
Step: 29120, train/loss: 0.0
Step: 29120, train/grad_norm: 3.209231408618507e-06
Step: 29120, train/learning_rate: 1.5349833120126277e-05
Step: 29120, train/epoch: 6.930033206939697
Step: 29130, train/loss: 0.0
Step: 29130, train/grad_norm: 3.8936123019084334e-05
Step: 29130, train/learning_rate: 1.5337935110437684e-05
Step: 29130, train/epoch: 6.932413101196289
Step: 29140, train/loss: 0.0
Step: 29140, train/grad_norm: 1.626338175242381e-08
Step: 29140, train/learning_rate: 1.5326035281759687e-05
Step: 29140, train/epoch: 6.934792995452881
Step: 29150, train/loss: 0.0
Step: 29150, train/grad_norm: 2.0825535102630965e-05
Step: 29150, train/learning_rate: 1.531413545308169e-05
Step: 29150, train/epoch: 6.937172889709473
Step: 29160, train/loss: 0.0
Step: 29160, train/grad_norm: 9.67879429936147e-08
Step: 29160, train/learning_rate: 1.5302237443393096e-05
Step: 29160, train/epoch: 6.9395527839660645
Step: 29170, train/loss: 0.0
Step: 29170, train/grad_norm: 1.1379175703041255e-05
Step: 29170, train/learning_rate: 1.52903376147151e-05
Step: 29170, train/epoch: 6.941932201385498
Step: 29180, train/loss: 0.0
Step: 29180, train/grad_norm: 5.740251651786821e-08
Step: 29180, train/learning_rate: 1.5278439605026506e-05
Step: 29180, train/epoch: 6.94431209564209
Step: 29190, train/loss: 0.0
Step: 29190, train/grad_norm: 5.214829343458405e-06
Step: 29190, train/learning_rate: 1.526653977634851e-05
Step: 29190, train/epoch: 6.946691989898682
Step: 29200, train/loss: 0.0
Step: 29200, train/grad_norm: 1.979690932785161e-05
Step: 29200, train/learning_rate: 1.5254640857165214e-05
Step: 29200, train/epoch: 6.949071884155273
Step: 29210, train/loss: 0.0
Step: 29210, train/grad_norm: 5.427829137261142e-07
Step: 29210, train/learning_rate: 1.5242741937981918e-05
Step: 29210, train/epoch: 6.951451778411865
Step: 29220, train/loss: 0.0
Step: 29220, train/grad_norm: 9.006031905300915e-05
Step: 29220, train/learning_rate: 1.5230842109303921e-05
Step: 29220, train/epoch: 6.953831672668457
Step: 29230, train/loss: 0.0
Step: 29230, train/grad_norm: 5.376982414873055e-09
Step: 29230, train/learning_rate: 1.5218943190120626e-05
Step: 29230, train/epoch: 6.956211090087891
Step: 29240, train/loss: 0.0
Step: 29240, train/grad_norm: 0.0005319886840879917
Step: 29240, train/learning_rate: 1.5207044270937331e-05
Step: 29240, train/epoch: 6.958590984344482
Step: 29250, train/loss: 0.0
Step: 29250, train/grad_norm: 7.760153494018596e-06
Step: 29250, train/learning_rate: 1.5195145351754036e-05
Step: 29250, train/epoch: 6.960970878601074
Step: 29260, train/loss: 0.0
Step: 29260, train/grad_norm: 5.031125169807638e-07
Step: 29260, train/learning_rate: 1.518324643257074e-05
Step: 29260, train/epoch: 6.963350772857666
Step: 29270, train/loss: 0.0
Step: 29270, train/grad_norm: 0.00020911620231345296
Step: 29270, train/learning_rate: 1.5171346603892744e-05
Step: 29270, train/epoch: 6.965730667114258
Step: 29280, train/loss: 0.0
Step: 29280, train/grad_norm: 8.426291331353752e-10
Step: 29280, train/learning_rate: 1.5159447684709448e-05
Step: 29280, train/epoch: 6.96811056137085
Step: 29290, train/loss: 0.0
Step: 29290, train/grad_norm: 0.00013636315998155624
Step: 29290, train/learning_rate: 1.5147548765526153e-05
Step: 29290, train/epoch: 6.970490455627441
Step: 29300, train/loss: 0.0
Step: 29300, train/grad_norm: 2.308496186742559e-05
Step: 29300, train/learning_rate: 1.5135649846342858e-05
Step: 29300, train/epoch: 6.972869873046875
Step: 29310, train/loss: 0.0
Step: 29310, train/grad_norm: 8.1977923116483e-08
Step: 29310, train/learning_rate: 1.5123750927159563e-05
Step: 29310, train/epoch: 6.975249767303467
Step: 29320, train/loss: 0.0
Step: 29320, train/grad_norm: 7.396472323506487e-09
Step: 29320, train/learning_rate: 1.5111851098481566e-05
Step: 29320, train/epoch: 6.977629661560059
Step: 29330, train/loss: 0.0
Step: 29330, train/grad_norm: 2.5807494239415973e-05
Step: 29330, train/learning_rate: 1.509995217929827e-05
Step: 29330, train/epoch: 6.98000955581665
Step: 29340, train/loss: 0.0
Step: 29340, train/grad_norm: 2.2864188320426138e-10
Step: 29340, train/learning_rate: 1.5088053260114975e-05
Step: 29340, train/epoch: 6.982389450073242
Step: 29350, train/loss: 0.0
Step: 29350, train/grad_norm: 7.624352292623371e-06
Step: 29350, train/learning_rate: 1.507615434093168e-05
Step: 29350, train/epoch: 6.984769344329834
Step: 29360, train/loss: 0.0
Step: 29360, train/grad_norm: 1.7487720072040247e-07
Step: 29360, train/learning_rate: 1.5064255421748385e-05
Step: 29360, train/epoch: 6.987148761749268
Step: 29370, train/loss: 0.0
Step: 29370, train/grad_norm: 1.613793574506417e-05
Step: 29370, train/learning_rate: 1.5052355593070388e-05
Step: 29370, train/epoch: 6.989528656005859
Step: 29380, train/loss: 0.0
Step: 29380, train/grad_norm: 3.4631734706636053e-06
Step: 29380, train/learning_rate: 1.5040456673887093e-05
Step: 29380, train/epoch: 6.991908550262451
Step: 29390, train/loss: 0.0
Step: 29390, train/grad_norm: 1.4623226185506155e-08
Step: 29390, train/learning_rate: 1.5028557754703797e-05
Step: 29390, train/epoch: 6.994288444519043
Step: 29400, train/loss: 0.0
Step: 29400, train/grad_norm: 1.0428372121396023e-07
Step: 29400, train/learning_rate: 1.5016658835520502e-05
Step: 29400, train/epoch: 6.996668338775635
Step: 29410, train/loss: 0.0
Step: 29410, train/grad_norm: 4.9327980377711356e-05
Step: 29410, train/learning_rate: 1.5004759916337207e-05
Step: 29410, train/epoch: 6.999048233032227
Step: 29414, eval/loss: 0.03553291782736778
Step: 29414, eval/accuracy: 0.9961127042770386
Step: 29414, eval/f1: 0.9958932995796204
Step: 29414, eval/runtime: 733.8991088867188
Step: 29414, eval/samples_per_second: 9.8149995803833
Step: 29414, eval/steps_per_second: 1.2280000448226929
Step: 29414, train/epoch: 7.0
Step: 29420, train/loss: 0.0
Step: 29420, train/grad_norm: 8.483583542329143e-07
Step: 29420, train/learning_rate: 1.4992860997153912e-05
Step: 29420, train/epoch: 7.001428127288818
Step: 29430, train/loss: 0.0
Step: 29430, train/grad_norm: 1.6084345588751603e-07
Step: 29430, train/learning_rate: 1.4980961168475915e-05
Step: 29430, train/epoch: 7.003807544708252
Step: 29440, train/loss: 0.0
Step: 29440, train/grad_norm: 5.053330554005697e-08
Step: 29440, train/learning_rate: 1.496906224929262e-05
Step: 29440, train/epoch: 7.006187438964844
Step: 29450, train/loss: 0.0
Step: 29450, train/grad_norm: 1.0268640693311681e-07
Step: 29450, train/learning_rate: 1.4957163330109324e-05
Step: 29450, train/epoch: 7.0085673332214355
Step: 29460, train/loss: 0.0
Step: 29460, train/grad_norm: 1.9576374654661777e-07
Step: 29460, train/learning_rate: 1.4945264410926029e-05
Step: 29460, train/epoch: 7.010947227478027
Step: 29470, train/loss: 0.0
Step: 29470, train/grad_norm: 2.7138314635521965e-06
Step: 29470, train/learning_rate: 1.4933365491742734e-05
Step: 29470, train/epoch: 7.013327121734619
Step: 29480, train/loss: 0.0
Step: 29480, train/grad_norm: 1.1310589798085857e-07
Step: 29480, train/learning_rate: 1.4921465663064737e-05
Step: 29480, train/epoch: 7.015707015991211
Step: 29490, train/loss: 0.0
Step: 29490, train/grad_norm: 5.869682517811725e-09
Step: 29490, train/learning_rate: 1.4909566743881442e-05
Step: 29490, train/epoch: 7.0180864334106445
Step: 29500, train/loss: 0.0
Step: 29500, train/grad_norm: 4.113142182404772e-08
Step: 29500, train/learning_rate: 1.4897667824698146e-05
Step: 29500, train/epoch: 7.020466327667236
Step: 29510, train/loss: 0.0
Step: 29510, train/grad_norm: 0.001501578721217811
Step: 29510, train/learning_rate: 1.4885768905514851e-05
Step: 29510, train/epoch: 7.022846221923828
Step: 29520, train/loss: 0.0
Step: 29520, train/grad_norm: 1.080337483472249e-06
Step: 29520, train/learning_rate: 1.4873869986331556e-05
Step: 29520, train/epoch: 7.02522611618042
Step: 29530, train/loss: 0.0
Step: 29530, train/grad_norm: 1.7151949123217491e-06
Step: 29530, train/learning_rate: 1.4861970157653559e-05
Step: 29530, train/epoch: 7.027606010437012
Step: 29540, train/loss: 0.0
Step: 29540, train/grad_norm: 1.7280184749779437e-07
Step: 29540, train/learning_rate: 1.4850071238470264e-05
Step: 29540, train/epoch: 7.0299859046936035
Step: 29550, train/loss: 0.0
Step: 29550, train/grad_norm: 3.654380634543486e-05
Step: 29550, train/learning_rate: 1.4838172319286969e-05
Step: 29550, train/epoch: 7.032365322113037
Step: 29560, train/loss: 0.0
Step: 29560, train/grad_norm: 5.399123779170623e-07
Step: 29560, train/learning_rate: 1.4826273400103673e-05
Step: 29560, train/epoch: 7.034745216369629
Step: 29570, train/loss: 0.0
Step: 29570, train/grad_norm: 7.81794951763004e-05
Step: 29570, train/learning_rate: 1.4814374480920378e-05
Step: 29570, train/epoch: 7.037125110626221
Step: 29580, train/loss: 0.0
Step: 29580, train/grad_norm: 1.271603089492146e-08
Step: 29580, train/learning_rate: 1.4802474652242381e-05
Step: 29580, train/epoch: 7.0395050048828125
Step: 29590, train/loss: 0.0
Step: 29590, train/grad_norm: 1.3164751635486027e-06
Step: 29590, train/learning_rate: 1.4790575733059086e-05
Step: 29590, train/epoch: 7.041884899139404
Step: 29600, train/loss: 0.0
Step: 29600, train/grad_norm: 1.0713941955842188e-09
Step: 29600, train/learning_rate: 1.477867681387579e-05
Step: 29600, train/epoch: 7.044264793395996
Step: 29610, train/loss: 0.0
Step: 29610, train/grad_norm: 0.0004305536567699164
Step: 29610, train/learning_rate: 1.4766777894692495e-05
Step: 29610, train/epoch: 7.046644687652588
Step: 29620, train/loss: 0.0
Step: 29620, train/grad_norm: 0.015911750495433807
Step: 29620, train/learning_rate: 1.47548789755092e-05
Step: 29620, train/epoch: 7.0490241050720215
Step: 29630, train/loss: 0.0
Step: 29630, train/grad_norm: 5.513435826287605e-05
Step: 29630, train/learning_rate: 1.4742979146831203e-05
Step: 29630, train/epoch: 7.051403999328613
Step: 29640, train/loss: 0.0
Step: 29640, train/grad_norm: 6.0412901348172454e-09
Step: 29640, train/learning_rate: 1.4731080227647908e-05
Step: 29640, train/epoch: 7.053783893585205
Step: 29650, train/loss: 0.0
Step: 29650, train/grad_norm: 0.00011486311996122822
Step: 29650, train/learning_rate: 1.4719181308464613e-05
Step: 29650, train/epoch: 7.056163787841797
Step: 29660, train/loss: 0.0
Step: 29660, train/grad_norm: 6.033526915416587e-06
Step: 29660, train/learning_rate: 1.4707282389281318e-05
Step: 29660, train/epoch: 7.058543682098389
Step: 29670, train/loss: 0.0
Step: 29670, train/grad_norm: 1.89743639111839e-06
Step: 29670, train/learning_rate: 1.4695383470098022e-05
Step: 29670, train/epoch: 7.0609235763549805
Step: 29680, train/loss: 0.0
Step: 29680, train/grad_norm: 8.804193818434669e-09
Step: 29680, train/learning_rate: 1.4683483641420025e-05
Step: 29680, train/epoch: 7.063302993774414
Step: 29690, train/loss: 0.0
Step: 29690, train/grad_norm: 5.381101786383624e-09
Step: 29690, train/learning_rate: 1.467158472223673e-05
Step: 29690, train/epoch: 7.065682888031006
Step: 29700, train/loss: 0.0
Step: 29700, train/grad_norm: 5.716960004065186e-06
Step: 29700, train/learning_rate: 1.4659685803053435e-05
Step: 29700, train/epoch: 7.068062782287598
Step: 29710, train/loss: 0.0
Step: 29710, train/grad_norm: 1.1458142807896365e-06
Step: 29710, train/learning_rate: 1.464778688387014e-05
Step: 29710, train/epoch: 7.0704426765441895
Step: 29720, train/loss: 0.0
Step: 29720, train/grad_norm: 1.0002986527979374e-06
Step: 29720, train/learning_rate: 1.4635887964686844e-05
Step: 29720, train/epoch: 7.072822570800781
Step: 29730, train/loss: 0.0
Step: 29730, train/grad_norm: 0.000489183934405446
Step: 29730, train/learning_rate: 1.4623988136008848e-05
Step: 29730, train/epoch: 7.075202465057373
Step: 29740, train/loss: 0.0
Step: 29740, train/grad_norm: 1.7855820999557181e-07
Step: 29740, train/learning_rate: 1.4612089216825552e-05
Step: 29740, train/epoch: 7.077581882476807
Step: 29750, train/loss: 0.0
Step: 29750, train/grad_norm: 7.82030929258326e-06
Step: 29750, train/learning_rate: 1.4600190297642257e-05
Step: 29750, train/epoch: 7.079961776733398
Step: 29760, train/loss: 0.0
Step: 29760, train/grad_norm: 2.439524582342756e-09
Step: 29760, train/learning_rate: 1.4588291378458962e-05
Step: 29760, train/epoch: 7.08234167098999
Step: 29770, train/loss: 0.0
Step: 29770, train/grad_norm: 4.233024952782216e-08
Step: 29770, train/learning_rate: 1.4576392459275667e-05
Step: 29770, train/epoch: 7.084721565246582
Step: 29780, train/loss: 0.0
Step: 29780, train/grad_norm: 9.823749591575393e-10
Step: 29780, train/learning_rate: 1.4564493540092371e-05
Step: 29780, train/epoch: 7.087101459503174
Step: 29790, train/loss: 0.0
Step: 29790, train/grad_norm: 1.488825489559531e-07
Step: 29790, train/learning_rate: 1.4552593711414374e-05
Step: 29790, train/epoch: 7.089481353759766
Step: 29800, train/loss: 0.0
Step: 29800, train/grad_norm: 3.092803672188893e-05
Step: 29800, train/learning_rate: 1.454069479223108e-05
Step: 29800, train/epoch: 7.091861248016357
Step: 29810, train/loss: 0.0
Step: 29810, train/grad_norm: 3.929633701771351e-11
Step: 29810, train/learning_rate: 1.4528795873047784e-05
Step: 29810, train/epoch: 7.094240665435791
Step: 29820, train/loss: 0.0
Step: 29820, train/grad_norm: 0.0001328373036812991
Step: 29820, train/learning_rate: 1.4516896953864489e-05
Step: 29820, train/epoch: 7.096620559692383
Step: 29830, train/loss: 0.0
Step: 29830, train/grad_norm: 2.341280058715256e-09
Step: 29830, train/learning_rate: 1.4504998034681194e-05
Step: 29830, train/epoch: 7.099000453948975
Step: 29840, train/loss: 0.0
Step: 29840, train/grad_norm: 1.3947339283504334e-08
Step: 29840, train/learning_rate: 1.4493098206003197e-05
Step: 29840, train/epoch: 7.101380348205566
Step: 29850, train/loss: 0.0
Step: 29850, train/grad_norm: 2.2271230193382507e-08
Step: 29850, train/learning_rate: 1.4481199286819901e-05
Step: 29850, train/epoch: 7.103760242462158
Step: 29860, train/loss: 0.0
Step: 29860, train/grad_norm: 1.648599301518061e-08
Step: 29860, train/learning_rate: 1.4469300367636606e-05
Step: 29860, train/epoch: 7.10614013671875
Step: 29870, train/loss: 0.0
Step: 29870, train/grad_norm: 1.636887816403032e-07
Step: 29870, train/learning_rate: 1.4457401448453311e-05
Step: 29870, train/epoch: 7.108519554138184
Step: 29880, train/loss: 0.0
Step: 29880, train/grad_norm: 6.061793556000339e-06
Step: 29880, train/learning_rate: 1.4445502529270016e-05
Step: 29880, train/epoch: 7.110899448394775
Step: 29890, train/loss: 0.0
Step: 29890, train/grad_norm: 0.00010590424062684178
Step: 29890, train/learning_rate: 1.4433602700592019e-05
Step: 29890, train/epoch: 7.113279342651367
Step: 29900, train/loss: 0.0
Step: 29900, train/grad_norm: 4.7776349674677476e-05
Step: 29900, train/learning_rate: 1.4421703781408723e-05
Step: 29900, train/epoch: 7.115659236907959
Step: 29910, train/loss: 0.0
Step: 29910, train/grad_norm: 2.127854799027773e-07
Step: 29910, train/learning_rate: 1.4409804862225428e-05
Step: 29910, train/epoch: 7.118039131164551
Step: 29920, train/loss: 0.0
Step: 29920, train/grad_norm: 5.40057044418063e-06
Step: 29920, train/learning_rate: 1.4397905943042133e-05
Step: 29920, train/epoch: 7.120419025421143
Step: 29930, train/loss: 0.0
Step: 29930, train/grad_norm: 1.3287132787809242e-05
Step: 29930, train/learning_rate: 1.4386007023858838e-05
Step: 29930, train/epoch: 7.122798442840576
Step: 29940, train/loss: 0.0
Step: 29940, train/grad_norm: 6.148884779122454e-08
Step: 29940, train/learning_rate: 1.437410719518084e-05
Step: 29940, train/epoch: 7.125178337097168
Step: 29950, train/loss: 0.0
Step: 29950, train/grad_norm: 0.003120168810710311
Step: 29950, train/learning_rate: 1.4362208275997546e-05
Step: 29950, train/epoch: 7.12755823135376
Step: 29960, train/loss: 0.0
Step: 29960, train/grad_norm: 1.4366648315444763e-07
Step: 29960, train/learning_rate: 1.435030935681425e-05
Step: 29960, train/epoch: 7.129938125610352
Step: 29970, train/loss: 0.0
Step: 29970, train/grad_norm: 2.176158275801754e-08
Step: 29970, train/learning_rate: 1.4338410437630955e-05
Step: 29970, train/epoch: 7.132318019866943
Step: 29980, train/loss: 0.0
Step: 29980, train/grad_norm: 1.0813241715368349e-05
Step: 29980, train/learning_rate: 1.432651151844766e-05
Step: 29980, train/epoch: 7.134697914123535
Step: 29990, train/loss: 0.0
Step: 29990, train/grad_norm: 9.445994209045239e-09
Step: 29990, train/learning_rate: 1.4314611689769663e-05
Step: 29990, train/epoch: 7.137077808380127
Step: 30000, train/loss: 0.0
Step: 30000, train/grad_norm: 7.101749361027032e-06
Step: 30000, train/learning_rate: 1.4302712770586368e-05
Step: 30000, train/epoch: 7.1394572257995605
Step: 30010, train/loss: 0.0
Step: 30010, train/grad_norm: 0.0001022589422063902
Step: 30010, train/learning_rate: 1.4290813851403072e-05
Step: 30010, train/epoch: 7.141837120056152
Step: 30020, train/loss: 0.0
Step: 30020, train/grad_norm: 6.024908998369938e-06
Step: 30020, train/learning_rate: 1.4278914932219777e-05
Step: 30020, train/epoch: 7.144217014312744
Step: 30030, train/loss: 0.0
Step: 30030, train/grad_norm: 7.905971415311797e-07
Step: 30030, train/learning_rate: 1.4267016013036482e-05
Step: 30030, train/epoch: 7.146596908569336
Step: 30040, train/loss: 0.0
Step: 30040, train/grad_norm: 1.812139949208813e-08
Step: 30040, train/learning_rate: 1.4255116184358485e-05
Step: 30040, train/epoch: 7.148976802825928
Step: 30050, train/loss: 0.0
Step: 30050, train/grad_norm: 1.2983502273300473e-08
Step: 30050, train/learning_rate: 1.424321726517519e-05
Step: 30050, train/epoch: 7.1513566970825195
Step: 30060, train/loss: 0.0
Step: 30060, train/grad_norm: 2.0893618057016283e-05
Step: 30060, train/learning_rate: 1.4231318345991895e-05
Step: 30060, train/epoch: 7.153736114501953
Step: 30070, train/loss: 0.0
Step: 30070, train/grad_norm: 5.2364804048465885e-08
Step: 30070, train/learning_rate: 1.42194194268086e-05
Step: 30070, train/epoch: 7.156116008758545
Step: 30080, train/loss: 0.0
Step: 30080, train/grad_norm: 4.911430373510939e-09
Step: 30080, train/learning_rate: 1.4207520507625304e-05
Step: 30080, train/epoch: 7.158495903015137
Step: 30090, train/loss: 0.0
Step: 30090, train/grad_norm: 5.238882105906839e-10
Step: 30090, train/learning_rate: 1.4195620678947307e-05
Step: 30090, train/epoch: 7.1608757972717285
Step: 30100, train/loss: 0.0
Step: 30100, train/grad_norm: 6.548351638713257e-09
Step: 30100, train/learning_rate: 1.4183721759764012e-05
Step: 30100, train/epoch: 7.16325569152832
Step: 30110, train/loss: 0.0
Step: 30110, train/grad_norm: 0.00016929322737269104
Step: 30110, train/learning_rate: 1.4171822840580717e-05
Step: 30110, train/epoch: 7.165635585784912
Step: 30120, train/loss: 0.0
Step: 30120, train/grad_norm: 1.2612429323155538e-08
Step: 30120, train/learning_rate: 1.4159923921397422e-05
Step: 30120, train/epoch: 7.168015003204346
Step: 30130, train/loss: 0.0
Step: 30130, train/grad_norm: 3.36900370712101e-07
Step: 30130, train/learning_rate: 1.4148025002214126e-05
Step: 30130, train/epoch: 7.1703948974609375
Step: 30140, train/loss: 0.0
Step: 30140, train/grad_norm: 1.0480550871605487e-09
Step: 30140, train/learning_rate: 1.4136126083030831e-05
Step: 30140, train/epoch: 7.172774791717529
Step: 30150, train/loss: 0.0
Step: 30150, train/grad_norm: 5.9397198270971785e-09
Step: 30150, train/learning_rate: 1.4124226254352834e-05
Step: 30150, train/epoch: 7.175154685974121
Step: 30160, train/loss: 0.0
Step: 30160, train/grad_norm: 4.9877457719560425e-09
Step: 30160, train/learning_rate: 1.4112327335169539e-05
Step: 30160, train/epoch: 7.177534580230713
Step: 30170, train/loss: 0.0
Step: 30170, train/grad_norm: 6.808327679408421e-09
Step: 30170, train/learning_rate: 1.4100428415986244e-05
Step: 30170, train/epoch: 7.179914474487305
Step: 30180, train/loss: 0.0
Step: 30180, train/grad_norm: 2.6346290837864217e-07
Step: 30180, train/learning_rate: 1.4088529496802948e-05
Step: 30180, train/epoch: 7.1822943687438965
Step: 30190, train/loss: 0.0
Step: 30190, train/grad_norm: 5.367355893071135e-09
Step: 30190, train/learning_rate: 1.4076630577619653e-05
Step: 30190, train/epoch: 7.18467378616333
Step: 30200, train/loss: 0.0
Step: 30200, train/grad_norm: 1.0172092288485146e-06
Step: 30200, train/learning_rate: 1.4064730748941656e-05
Step: 30200, train/epoch: 7.187053680419922
Step: 30210, train/loss: 0.0
Step: 30210, train/grad_norm: 1.8472329443852686e-08
Step: 30210, train/learning_rate: 1.4052831829758361e-05
Step: 30210, train/epoch: 7.189433574676514
Step: 30220, train/loss: 0.0
Step: 30220, train/grad_norm: 2.974163970659305e-10
Step: 30220, train/learning_rate: 1.4040932910575066e-05
Step: 30220, train/epoch: 7.1918134689331055
Step: 30230, train/loss: 0.0
Step: 30230, train/grad_norm: 1.272809839747424e-07
Step: 30230, train/learning_rate: 1.402903399139177e-05
Step: 30230, train/epoch: 7.194193363189697
Step: 30240, train/loss: 0.0
Step: 30240, train/grad_norm: 4.794147230313683e-07
Step: 30240, train/learning_rate: 1.4017135072208475e-05
Step: 30240, train/epoch: 7.196573257446289
Step: 30250, train/loss: 0.0
Step: 30250, train/grad_norm: 6.506502892023036e-09
Step: 30250, train/learning_rate: 1.4005235243530478e-05
Step: 30250, train/epoch: 7.198952674865723
Step: 30260, train/loss: 0.0
Step: 30260, train/grad_norm: 1.8265088996827217e-08
Step: 30260, train/learning_rate: 1.3993336324347183e-05
Step: 30260, train/epoch: 7.2013325691223145
Step: 30270, train/loss: 0.0
Step: 30270, train/grad_norm: 3.153724614790576e-09
Step: 30270, train/learning_rate: 1.3981437405163888e-05
Step: 30270, train/epoch: 7.203712463378906
Step: 30280, train/loss: 0.0
Step: 30280, train/grad_norm: 8.827994861348998e-06
Step: 30280, train/learning_rate: 1.3969538485980593e-05
Step: 30280, train/epoch: 7.206092357635498
Step: 30290, train/loss: 0.0
Step: 30290, train/grad_norm: 2.0526611024251906e-06
Step: 30290, train/learning_rate: 1.3957639566797297e-05
Step: 30290, train/epoch: 7.20847225189209
Step: 30300, train/loss: 0.0
Step: 30300, train/grad_norm: 1.537575755605758e-08
Step: 30300, train/learning_rate: 1.39457397381193e-05
Step: 30300, train/epoch: 7.210852146148682
Step: 30310, train/loss: 0.0
Step: 30310, train/grad_norm: 0.00011205481132492423
Step: 30310, train/learning_rate: 1.3933840818936005e-05
Step: 30310, train/epoch: 7.213231563568115
Step: 30320, train/loss: 0.0
Step: 30320, train/grad_norm: 1.489004120003301e-07
Step: 30320, train/learning_rate: 1.392194189975271e-05
Step: 30320, train/epoch: 7.215611457824707
Step: 30330, train/loss: 0.0
Step: 30330, train/grad_norm: 0.000265809940174222
Step: 30330, train/learning_rate: 1.3910042980569415e-05
Step: 30330, train/epoch: 7.217991352081299
Step: 30340, train/loss: 0.0
Step: 30340, train/grad_norm: 1.6006265202150871e-09
Step: 30340, train/learning_rate: 1.389814406138612e-05
Step: 30340, train/epoch: 7.220371246337891
Step: 30350, train/loss: 0.0
Step: 30350, train/grad_norm: 2.447585190079593e-10
Step: 30350, train/learning_rate: 1.3886244232708123e-05
Step: 30350, train/epoch: 7.222751140594482
Step: 30360, train/loss: 0.0
Step: 30360, train/grad_norm: 2.1971526820152576e-08
Step: 30360, train/learning_rate: 1.3874345313524827e-05
Step: 30360, train/epoch: 7.225131034851074
Step: 30370, train/loss: 0.0
Step: 30370, train/grad_norm: 3.931878467255956e-08
Step: 30370, train/learning_rate: 1.3862446394341532e-05
Step: 30370, train/epoch: 7.227510929107666
Step: 30380, train/loss: 0.0
Step: 30380, train/grad_norm: 2.4819833299716265e-08
Step: 30380, train/learning_rate: 1.3850547475158237e-05
Step: 30380, train/epoch: 7.2298903465271
Step: 30390, train/loss: 0.0
Step: 30390, train/grad_norm: 1.972368934843871e-09
Step: 30390, train/learning_rate: 1.3838648555974942e-05
Step: 30390, train/epoch: 7.232270240783691
Step: 30400, train/loss: 0.0
Step: 30400, train/grad_norm: 2.095841944438348e-09
Step: 30400, train/learning_rate: 1.3826748727296945e-05
Step: 30400, train/epoch: 7.234650135040283
Step: 30410, train/loss: 0.0
Step: 30410, train/grad_norm: 6.491159729193896e-05
Step: 30410, train/learning_rate: 1.381484980811365e-05
Step: 30410, train/epoch: 7.237030029296875
Step: 30420, train/loss: 0.0
Step: 30420, train/grad_norm: 8.966398127085995e-06
Step: 30420, train/learning_rate: 1.3802950888930354e-05
Step: 30420, train/epoch: 7.239409923553467
Step: 30430, train/loss: 0.0
Step: 30430, train/grad_norm: 2.2252811504586134e-07
Step: 30430, train/learning_rate: 1.3791051969747059e-05
Step: 30430, train/epoch: 7.241789817810059
Step: 30440, train/loss: 0.0
Step: 30440, train/grad_norm: 1.08741247117905e-08
Step: 30440, train/learning_rate: 1.3779153050563764e-05
Step: 30440, train/epoch: 7.244169235229492
Step: 30450, train/loss: 0.0
Step: 30450, train/grad_norm: 1.5416503629239742e-09
Step: 30450, train/learning_rate: 1.3767254131380469e-05
Step: 30450, train/epoch: 7.246549129486084
Step: 30460, train/loss: 0.0
Step: 30460, train/grad_norm: 1.2864938447876284e-08
Step: 30460, train/learning_rate: 1.3755354302702472e-05
Step: 30460, train/epoch: 7.248929023742676
Step: 30470, train/loss: 0.0017000000225380063
Step: 30470, train/grad_norm: 0.00043089292012155056
Step: 30470, train/learning_rate: 1.3743455383519176e-05
Step: 30470, train/epoch: 7.251308917999268
Step: 30480, train/loss: 0.032999999821186066
Step: 30480, train/grad_norm: 6.716668998762998e-09
Step: 30480, train/learning_rate: 1.3731556464335881e-05
Step: 30480, train/epoch: 7.253688812255859
Step: 30490, train/loss: 0.0
Step: 30490, train/grad_norm: 0.00029310776153579354
Step: 30490, train/learning_rate: 1.3719657545152586e-05
Step: 30490, train/epoch: 7.256068706512451
Step: 30500, train/loss: 0.0
Step: 30500, train/grad_norm: 8.561886716051959e-06
Step: 30500, train/learning_rate: 1.370775862596929e-05
Step: 30500, train/epoch: 7.258448123931885
Step: 30510, train/loss: 0.0
Step: 30510, train/grad_norm: 1.0076010248383227e-08
Step: 30510, train/learning_rate: 1.3695858797291294e-05
Step: 30510, train/epoch: 7.260828018188477
Step: 30520, train/loss: 0.0
Step: 30520, train/grad_norm: 1.3147839126759209e-05
Step: 30520, train/learning_rate: 1.3683959878107999e-05
Step: 30520, train/epoch: 7.263207912445068
Step: 30530, train/loss: 0.0
Step: 30530, train/grad_norm: 7.854487762415374e-07
Step: 30530, train/learning_rate: 1.3672060958924703e-05
Step: 30530, train/epoch: 7.26558780670166
Step: 30540, train/loss: 0.0
Step: 30540, train/grad_norm: 3.4252944924872963e-10
Step: 30540, train/learning_rate: 1.3660162039741408e-05
Step: 30540, train/epoch: 7.267967700958252
Step: 30550, train/loss: 0.0
Step: 30550, train/grad_norm: 0.002144260797649622
Step: 30550, train/learning_rate: 1.3648263120558113e-05
Step: 30550, train/epoch: 7.270347595214844
Step: 30560, train/loss: 0.0
Step: 30560, train/grad_norm: 2.391978171090159e-08
Step: 30560, train/learning_rate: 1.3636363291880116e-05
Step: 30560, train/epoch: 7.2727274894714355
Step: 30570, train/loss: 0.0
Step: 30570, train/grad_norm: 4.362522668088786e-05
Step: 30570, train/learning_rate: 1.362446437269682e-05
Step: 30570, train/epoch: 7.275106906890869
Step: 30580, train/loss: 0.4375
Step: 30580, train/grad_norm: 9.736102947499603e-05
Step: 30580, train/learning_rate: 1.3612565453513525e-05
Step: 30580, train/epoch: 7.277486801147461
Step: 30590, train/loss: 0.0
Step: 30590, train/grad_norm: 0.0005073188804090023
Step: 30590, train/learning_rate: 1.360066653433023e-05
Step: 30590, train/epoch: 7.279866695404053
Step: 30600, train/loss: 0.0
Step: 30600, train/grad_norm: 0.0009810531046241522
Step: 30600, train/learning_rate: 1.3588767615146935e-05
Step: 30600, train/epoch: 7.2822465896606445
Step: 30610, train/loss: 0.0
Step: 30610, train/grad_norm: 0.00021202093921601772
Step: 30610, train/learning_rate: 1.3576867786468938e-05
Step: 30610, train/epoch: 7.284626483917236
Step: 30620, train/loss: 0.0
Step: 30620, train/grad_norm: 0.0006878377171233296
Step: 30620, train/learning_rate: 1.3564968867285643e-05
Step: 30620, train/epoch: 7.287006378173828
Step: 30630, train/loss: 0.0
Step: 30630, train/grad_norm: 0.0001557175855850801
Step: 30630, train/learning_rate: 1.3553069948102348e-05
Step: 30630, train/epoch: 7.289385795593262
Step: 30640, train/loss: 0.0
Step: 30640, train/grad_norm: 0.00017001737433020025
Step: 30640, train/learning_rate: 1.3541171028919052e-05
Step: 30640, train/epoch: 7.2917656898498535
Step: 30650, train/loss: 0.0
Step: 30650, train/grad_norm: 8.858622459229082e-05
Step: 30650, train/learning_rate: 1.3529272109735757e-05
Step: 30650, train/epoch: 7.294145584106445
Step: 30660, train/loss: 0.0
Step: 30660, train/grad_norm: 0.0003258034703321755
Step: 30660, train/learning_rate: 1.351737228105776e-05
Step: 30660, train/epoch: 7.296525478363037
Step: 30670, train/loss: 0.0
Step: 30670, train/grad_norm: 1.409179276379291e-05
Step: 30670, train/learning_rate: 1.3505473361874465e-05
Step: 30670, train/epoch: 7.298905372619629
Step: 30680, train/loss: 0.0
Step: 30680, train/grad_norm: 3.5382101486902684e-05
Step: 30680, train/learning_rate: 1.349357444269117e-05
Step: 30680, train/epoch: 7.301285266876221
Step: 30690, train/loss: 0.0
Step: 30690, train/grad_norm: 7.332609675358981e-05
Step: 30690, train/learning_rate: 1.3481675523507874e-05
Step: 30690, train/epoch: 7.303664684295654
Step: 30700, train/loss: 0.0
Step: 30700, train/grad_norm: 0.0001174982899101451
Step: 30700, train/learning_rate: 1.346977660432458e-05
Step: 30700, train/epoch: 7.306044578552246
Step: 30710, train/loss: 0.0
Step: 30710, train/grad_norm: 8.47546380100539e-06
Step: 30710, train/learning_rate: 1.3457876775646582e-05
Step: 30710, train/epoch: 7.308424472808838
Step: 30720, train/loss: 0.0
Step: 30720, train/grad_norm: 1.9048502508667298e-05
Step: 30720, train/learning_rate: 1.3445977856463287e-05
Step: 30720, train/epoch: 7.31080436706543
Step: 30730, train/loss: 0.0
Step: 30730, train/grad_norm: 1.1359164091118146e-05
Step: 30730, train/learning_rate: 1.3434078937279992e-05
Step: 30730, train/epoch: 7.3131842613220215
Step: 30740, train/loss: 0.0
Step: 30740, train/grad_norm: 7.891643690527417e-06
Step: 30740, train/learning_rate: 1.3422180018096697e-05
Step: 30740, train/epoch: 7.315564155578613
Step: 30750, train/loss: 0.0
Step: 30750, train/grad_norm: 2.9232760425657034e-05
Step: 30750, train/learning_rate: 1.3410281098913401e-05
Step: 30750, train/epoch: 7.317944049835205
Step: 30760, train/loss: 0.0
Step: 30760, train/grad_norm: 0.00017940982070285827
Step: 30760, train/learning_rate: 1.3398381270235404e-05
Step: 30760, train/epoch: 7.320323467254639
Step: 30770, train/loss: 0.0
Step: 30770, train/grad_norm: 0.00015785888535901904
Step: 30770, train/learning_rate: 1.3386482351052109e-05
Step: 30770, train/epoch: 7.3227033615112305
Step: 30780, train/loss: 0.0
Step: 30780, train/grad_norm: 3.923933763871901e-05
Step: 30780, train/learning_rate: 1.3374583431868814e-05
Step: 30780, train/epoch: 7.325083255767822
Step: 30790, train/loss: 0.0
Step: 30790, train/grad_norm: 8.196871931431815e-06
Step: 30790, train/learning_rate: 1.3362684512685519e-05
Step: 30790, train/epoch: 7.327463150024414
Step: 30800, train/loss: 0.0
Step: 30800, train/grad_norm: 9.28298777580494e-06
Step: 30800, train/learning_rate: 1.3350785593502223e-05
Step: 30800, train/epoch: 7.329843044281006
Step: 30810, train/loss: 0.0
Step: 30810, train/grad_norm: 8.647626600577496e-06
Step: 30810, train/learning_rate: 1.3338886674318928e-05
Step: 30810, train/epoch: 7.332222938537598
Step: 30820, train/loss: 0.0
Step: 30820, train/grad_norm: 2.0240886442479677e-05
Step: 30820, train/learning_rate: 1.3326986845640931e-05
Step: 30820, train/epoch: 7.334602355957031
Step: 30830, train/loss: 0.0
Step: 30830, train/grad_norm: 0.0002635372511576861
Step: 30830, train/learning_rate: 1.3315087926457636e-05
Step: 30830, train/epoch: 7.336982250213623
Step: 30840, train/loss: 0.0
Step: 30840, train/grad_norm: 9.649876301409677e-05
Step: 30840, train/learning_rate: 1.330318900727434e-05
Step: 30840, train/epoch: 7.339362144470215
Step: 30850, train/loss: 0.0
Step: 30850, train/grad_norm: 7.849604116927367e-06
Step: 30850, train/learning_rate: 1.3291290088091046e-05
Step: 30850, train/epoch: 7.341742038726807
Step: 30860, train/loss: 0.0
Step: 30860, train/grad_norm: 2.5285373340011574e-05
Step: 30860, train/learning_rate: 1.327939116890775e-05
Step: 30860, train/epoch: 7.344121932983398
Step: 30870, train/loss: 0.0
Step: 30870, train/grad_norm: 2.084620609821286e-05
Step: 30870, train/learning_rate: 1.3267491340229753e-05
Step: 30870, train/epoch: 7.34650182723999
Step: 30880, train/loss: 0.0
Step: 30880, train/grad_norm: 0.0002847268187906593
Step: 30880, train/learning_rate: 1.3255592421046458e-05
Step: 30880, train/epoch: 7.348881721496582
Step: 30890, train/loss: 0.0
Step: 30890, train/grad_norm: 6.305625447566854e-06
Step: 30890, train/learning_rate: 1.3243693501863163e-05
Step: 30890, train/epoch: 7.351261138916016
Step: 30900, train/loss: 0.0
Step: 30900, train/grad_norm: 4.635243385564536e-05
Step: 30900, train/learning_rate: 1.3231794582679868e-05
Step: 30900, train/epoch: 7.353641033172607
Step: 30910, train/loss: 0.0
Step: 30910, train/grad_norm: 1.3269732335174922e-05
Step: 30910, train/learning_rate: 1.3219895663496573e-05
Step: 30910, train/epoch: 7.356020927429199
Step: 30920, train/loss: 0.03460000082850456
Step: 30920, train/grad_norm: 0.00020505243446677923
Step: 30920, train/learning_rate: 1.3207995834818576e-05
Step: 30920, train/epoch: 7.358400821685791
Step: 30930, train/loss: 0.0
Step: 30930, train/grad_norm: 7.349465249717468e-07
Step: 30930, train/learning_rate: 1.319609691563528e-05
Step: 30930, train/epoch: 7.360780715942383
Step: 30940, train/loss: 9.999999747378752e-05
Step: 30940, train/grad_norm: 0.005542648956179619
Step: 30940, train/learning_rate: 1.3184197996451985e-05
Step: 30940, train/epoch: 7.363160610198975
Step: 30950, train/loss: 0.0
Step: 30950, train/grad_norm: 4.667376742872875e-07
Step: 30950, train/learning_rate: 1.317229907726869e-05
Step: 30950, train/epoch: 7.365540027618408
Step: 30960, train/loss: 0.0
Step: 30960, train/grad_norm: 2.6886538762482814e-05
Step: 30960, train/learning_rate: 1.3160400158085395e-05
Step: 30960, train/epoch: 7.367919921875
Step: 30970, train/loss: 0.0
Step: 30970, train/grad_norm: 1.2145859727752395e-05
Step: 30970, train/learning_rate: 1.3148500329407398e-05
Step: 30970, train/epoch: 7.370299816131592
Step: 30980, train/loss: 0.0
Step: 30980, train/grad_norm: 0.00019258231623098254
Step: 30980, train/learning_rate: 1.3136601410224102e-05
Step: 30980, train/epoch: 7.372679710388184
Step: 30990, train/loss: 0.0
Step: 30990, train/grad_norm: 4.62858188257087e-05
Step: 30990, train/learning_rate: 1.3124702491040807e-05
Step: 30990, train/epoch: 7.375059604644775
Step: 31000, train/loss: 0.0
Step: 31000, train/grad_norm: 1.1715142136381473e-06
Step: 31000, train/learning_rate: 1.3112803571857512e-05
Step: 31000, train/epoch: 7.377439498901367
Step: 31010, train/loss: 0.0
Step: 31010, train/grad_norm: 5.080727532913443e-06
Step: 31010, train/learning_rate: 1.3100904652674217e-05
Step: 31010, train/epoch: 7.379818916320801
Step: 31020, train/loss: 0.0
Step: 31020, train/grad_norm: 5.971537029836327e-05
Step: 31020, train/learning_rate: 1.308900482399622e-05
Step: 31020, train/epoch: 7.382198810577393
Step: 31030, train/loss: 0.0
Step: 31030, train/grad_norm: 2.832044629030861e-06
Step: 31030, train/learning_rate: 1.3077105904812925e-05
Step: 31030, train/epoch: 7.384578704833984
Step: 31040, train/loss: 0.0
Step: 31040, train/grad_norm: 0.00028213736368343234
Step: 31040, train/learning_rate: 1.306520698562963e-05
Step: 31040, train/epoch: 7.386958599090576
Step: 31050, train/loss: 0.0
Step: 31050, train/grad_norm: 6.216023962224426e-07
Step: 31050, train/learning_rate: 1.3053308066446334e-05
Step: 31050, train/epoch: 7.389338493347168
Step: 31060, train/loss: 0.0
Step: 31060, train/grad_norm: 1.0753718697742443e-06
Step: 31060, train/learning_rate: 1.3041409147263039e-05
Step: 31060, train/epoch: 7.39171838760376
Step: 31070, train/loss: 0.0
Step: 31070, train/grad_norm: 3.974615765400813e-07
Step: 31070, train/learning_rate: 1.3029509318585042e-05
Step: 31070, train/epoch: 7.394098281860352
Step: 31080, train/loss: 0.0
Step: 31080, train/grad_norm: 2.706849500100361e-06
Step: 31080, train/learning_rate: 1.3017610399401747e-05
Step: 31080, train/epoch: 7.396477699279785
Step: 31090, train/loss: 0.0
Step: 31090, train/grad_norm: 5.975933618174167e-06
Step: 31090, train/learning_rate: 1.3005711480218451e-05
Step: 31090, train/epoch: 7.398857593536377
Step: 31100, train/loss: 0.0
Step: 31100, train/grad_norm: 1.9523678929544985e-05
Step: 31100, train/learning_rate: 1.2993812561035156e-05
Step: 31100, train/epoch: 7.401237487792969
Step: 31110, train/loss: 0.0
Step: 31110, train/grad_norm: 7.207037242551451e-07
Step: 31110, train/learning_rate: 1.2981913641851861e-05
Step: 31110, train/epoch: 7.4036173820495605
Step: 31120, train/loss: 0.0
Step: 31120, train/grad_norm: 0.08684214949607849
Step: 31120, train/learning_rate: 1.2970014722668566e-05
Step: 31120, train/epoch: 7.405997276306152
Step: 31130, train/loss: 0.0
Step: 31130, train/grad_norm: 3.0637563668278744e-06
Step: 31130, train/learning_rate: 1.2958114893990569e-05
Step: 31130, train/epoch: 7.408377170562744
Step: 31140, train/loss: 0.0
Step: 31140, train/grad_norm: 1.5116135728021618e-05
Step: 31140, train/learning_rate: 1.2946215974807274e-05
Step: 31140, train/epoch: 7.410756587982178
Step: 31150, train/loss: 0.0
Step: 31150, train/grad_norm: 3.68712949239125e-06
Step: 31150, train/learning_rate: 1.2934317055623978e-05
Step: 31150, train/epoch: 7.4131364822387695
Step: 31160, train/loss: 0.0
Step: 31160, train/grad_norm: 1.9396953575778753e-05
Step: 31160, train/learning_rate: 1.2922418136440683e-05
Step: 31160, train/epoch: 7.415516376495361
Step: 31170, train/loss: 0.0
Step: 31170, train/grad_norm: 5.143106682226062e-05
Step: 31170, train/learning_rate: 1.2910519217257388e-05
Step: 31170, train/epoch: 7.417896270751953
Step: 31180, train/loss: 0.0
Step: 31180, train/grad_norm: 7.22229560778942e-06
Step: 31180, train/learning_rate: 1.2898619388579391e-05
Step: 31180, train/epoch: 7.420276165008545
Step: 31190, train/loss: 0.0
Step: 31190, train/grad_norm: 4.26695942223887e-06
Step: 31190, train/learning_rate: 1.2886720469396096e-05
Step: 31190, train/epoch: 7.422656059265137
Step: 31200, train/loss: 0.0
Step: 31200, train/grad_norm: 5.637867616314907e-06
Step: 31200, train/learning_rate: 1.28748215502128e-05
Step: 31200, train/epoch: 7.42503547668457
Step: 31210, train/loss: 0.0
Step: 31210, train/grad_norm: 1.4202879356162157e-05
Step: 31210, train/learning_rate: 1.2862922631029505e-05
Step: 31210, train/epoch: 7.427415370941162
Step: 31220, train/loss: 0.0
Step: 31220, train/grad_norm: 7.427371747326106e-05
Step: 31220, train/learning_rate: 1.285102371184621e-05
Step: 31220, train/epoch: 7.429795265197754
Step: 31230, train/loss: 0.0
Step: 31230, train/grad_norm: 3.3359679036948364e-06
Step: 31230, train/learning_rate: 1.2839123883168213e-05
Step: 31230, train/epoch: 7.432175159454346
Step: 31240, train/loss: 0.0
Step: 31240, train/grad_norm: 0.0017709346720948815
Step: 31240, train/learning_rate: 1.2827224963984918e-05
Step: 31240, train/epoch: 7.4345550537109375
Step: 31250, train/loss: 0.0
Step: 31250, train/grad_norm: 3.2251050470222253e-06
Step: 31250, train/learning_rate: 1.2815326044801623e-05
Step: 31250, train/epoch: 7.436934947967529
Step: 31260, train/loss: 0.0
Step: 31260, train/grad_norm: 8.139170404319884e-07
Step: 31260, train/learning_rate: 1.2803427125618327e-05
Step: 31260, train/epoch: 7.439314842224121
Step: 31270, train/loss: 0.0
Step: 31270, train/grad_norm: 4.0787588773127936e-07
Step: 31270, train/learning_rate: 1.2791528206435032e-05
Step: 31270, train/epoch: 7.441694259643555
Step: 31280, train/loss: 0.0
Step: 31280, train/grad_norm: 1.2303800076551852e-06
Step: 31280, train/learning_rate: 1.2779628377757035e-05
Step: 31280, train/epoch: 7.4440741539001465
Step: 31290, train/loss: 0.0
Step: 31290, train/grad_norm: 4.4609589622268686e-07
Step: 31290, train/learning_rate: 1.276772945857374e-05
Step: 31290, train/epoch: 7.446454048156738
Step: 31300, train/loss: 0.0
Step: 31300, train/grad_norm: 1.2620374434391124e-07
Step: 31300, train/learning_rate: 1.2755830539390445e-05
Step: 31300, train/epoch: 7.44883394241333
Step: 31310, train/loss: 0.0
Step: 31310, train/grad_norm: 2.7487260467751184e-06
Step: 31310, train/learning_rate: 1.274393162020715e-05
Step: 31310, train/epoch: 7.451213836669922
Step: 31320, train/loss: 0.0
Step: 31320, train/grad_norm: 8.753383553994354e-06
Step: 31320, train/learning_rate: 1.2732032701023854e-05
Step: 31320, train/epoch: 7.453593730926514
Step: 31330, train/loss: 0.0
Step: 31330, train/grad_norm: 6.720286819472676e-06
Step: 31330, train/learning_rate: 1.2720132872345857e-05
Step: 31330, train/epoch: 7.455973148345947
Step: 31340, train/loss: 0.0
Step: 31340, train/grad_norm: 7.550145710411016e-06
Step: 31340, train/learning_rate: 1.2708233953162562e-05
Step: 31340, train/epoch: 7.458353042602539
Step: 31350, train/loss: 0.0
Step: 31350, train/grad_norm: 1.233213879459072e-06
Step: 31350, train/learning_rate: 1.2696335033979267e-05
Step: 31350, train/epoch: 7.460732936859131
Step: 31360, train/loss: 0.0
Step: 31360, train/grad_norm: 7.944291428430006e-05
Step: 31360, train/learning_rate: 1.2684436114795972e-05
Step: 31360, train/epoch: 7.463112831115723
Step: 31370, train/loss: 0.0
Step: 31370, train/grad_norm: 7.67180836191983e-07
Step: 31370, train/learning_rate: 1.2672537195612676e-05
Step: 31370, train/epoch: 7.4654927253723145
Step: 31380, train/loss: 0.0
Step: 31380, train/grad_norm: 4.39366813225206e-05
Step: 31380, train/learning_rate: 1.266063736693468e-05
Step: 31380, train/epoch: 7.467872619628906
Step: 31390, train/loss: 0.0
Step: 31390, train/grad_norm: 4.39372479377198e-06
Step: 31390, train/learning_rate: 1.2648738447751384e-05
Step: 31390, train/epoch: 7.47025203704834
Step: 31400, train/loss: 0.0
Step: 31400, train/grad_norm: 5.825946800541715e-07
Step: 31400, train/learning_rate: 1.2636839528568089e-05
Step: 31400, train/epoch: 7.472631931304932
Step: 31410, train/loss: 0.0
Step: 31410, train/grad_norm: 8.339750820596237e-06
Step: 31410, train/learning_rate: 1.2624940609384794e-05
Step: 31410, train/epoch: 7.475011825561523
Step: 31420, train/loss: 0.0
Step: 31420, train/grad_norm: 5.1558522500272375e-06
Step: 31420, train/learning_rate: 1.2613041690201499e-05
Step: 31420, train/epoch: 7.477391719818115
Step: 31430, train/loss: 0.0
Step: 31430, train/grad_norm: 2.146635415556375e-05
Step: 31430, train/learning_rate: 1.2601141861523502e-05
Step: 31430, train/epoch: 7.479771614074707
Step: 31440, train/loss: 0.0
Step: 31440, train/grad_norm: 3.902122443832923e-06
Step: 31440, train/learning_rate: 1.2589242942340206e-05
Step: 31440, train/epoch: 7.482151508331299
Step: 31450, train/loss: 0.0
Step: 31450, train/grad_norm: 0.000328887312207371
Step: 31450, train/learning_rate: 1.2577344023156911e-05
Step: 31450, train/epoch: 7.484531402587891
Step: 31460, train/loss: 0.0
Step: 31460, train/grad_norm: 3.8919040434848284e-07
Step: 31460, train/learning_rate: 1.2565445103973616e-05
Step: 31460, train/epoch: 7.486910820007324
Step: 31470, train/loss: 0.0
Step: 31470, train/grad_norm: 7.1138119892566465e-06
Step: 31470, train/learning_rate: 1.255354618479032e-05
Step: 31470, train/epoch: 7.489290714263916
Step: 31480, train/loss: 0.0
Step: 31480, train/grad_norm: 5.5840046115918085e-06
Step: 31480, train/learning_rate: 1.2541647265607025e-05
Step: 31480, train/epoch: 7.491670608520508
Step: 31490, train/loss: 0.0
Step: 31490, train/grad_norm: 3.023882527486421e-06
Step: 31490, train/learning_rate: 1.2529747436929028e-05
Step: 31490, train/epoch: 7.4940505027771
Step: 31500, train/loss: 0.0
Step: 31500, train/grad_norm: 1.5317431461880915e-05
Step: 31500, train/learning_rate: 1.2517848517745733e-05
Step: 31500, train/epoch: 7.496430397033691
Step: 31510, train/loss: 0.0
Step: 31510, train/grad_norm: 3.3606925171625335e-06
Step: 31510, train/learning_rate: 1.2505949598562438e-05
Step: 31510, train/epoch: 7.498810291290283
Step: 31520, train/loss: 0.0
Step: 31520, train/grad_norm: 7.155610092013376e-06
Step: 31520, train/learning_rate: 1.2494050679379143e-05
Step: 31520, train/epoch: 7.501189708709717
Step: 31530, train/loss: 0.0
Step: 31530, train/grad_norm: 4.308588756885001e-07
Step: 31530, train/learning_rate: 1.2482151760195848e-05
Step: 31530, train/epoch: 7.503569602966309
Step: 31540, train/loss: 0.0
Step: 31540, train/grad_norm: 5.003373644285602e-07
Step: 31540, train/learning_rate: 1.247025193151785e-05
Step: 31540, train/epoch: 7.5059494972229
Step: 31550, train/loss: 0.0
Step: 31550, train/grad_norm: 1.8014949091593735e-05
Step: 31550, train/learning_rate: 1.2458353012334555e-05
Step: 31550, train/epoch: 7.508329391479492
Step: 31560, train/loss: 0.0
Step: 31560, train/grad_norm: 5.866049264113826e-07
Step: 31560, train/learning_rate: 1.244645409315126e-05
Step: 31560, train/epoch: 7.510709285736084
Step: 31570, train/loss: 0.0
Step: 31570, train/grad_norm: 4.284541773813544e-06
Step: 31570, train/learning_rate: 1.2434555173967965e-05
Step: 31570, train/epoch: 7.513089179992676
Step: 31580, train/loss: 0.0
Step: 31580, train/grad_norm: 9.808812961864533e-08
Step: 31580, train/learning_rate: 1.242265625478467e-05
Step: 31580, train/epoch: 7.515468597412109
Step: 31590, train/loss: 0.0
Step: 31590, train/grad_norm: 8.518759386788588e-07
Step: 31590, train/learning_rate: 1.2410756426106673e-05
Step: 31590, train/epoch: 7.517848491668701
Step: 31600, train/loss: 0.0
Step: 31600, train/grad_norm: 1.2103518201911356e-05
Step: 31600, train/learning_rate: 1.2398857506923378e-05
Step: 31600, train/epoch: 7.520228385925293
Step: 31610, train/loss: 0.0
Step: 31610, train/grad_norm: 3.872595470966189e-07
Step: 31610, train/learning_rate: 1.2386958587740082e-05
Step: 31610, train/epoch: 7.522608280181885
Step: 31620, train/loss: 0.0
Step: 31620, train/grad_norm: 8.870258625393035e-07
Step: 31620, train/learning_rate: 1.2375059668556787e-05
Step: 31620, train/epoch: 7.524988174438477
Step: 31630, train/loss: 0.0
Step: 31630, train/grad_norm: 9.402644707279251e-08
Step: 31630, train/learning_rate: 1.2363160749373492e-05
Step: 31630, train/epoch: 7.527368068695068
Step: 31640, train/loss: 0.0
Step: 31640, train/grad_norm: 5.181986466595845e-07
Step: 31640, train/learning_rate: 1.2351260920695495e-05
Step: 31640, train/epoch: 7.52974796295166
Step: 31650, train/loss: 0.0
Step: 31650, train/grad_norm: 2.503661789887701e-06
Step: 31650, train/learning_rate: 1.23393620015122e-05
Step: 31650, train/epoch: 7.532127380371094
Step: 31660, train/loss: 0.0
Step: 31660, train/grad_norm: 3.1138642953010276e-07
Step: 31660, train/learning_rate: 1.2327463082328904e-05
Step: 31660, train/epoch: 7.5345072746276855
Step: 31670, train/loss: 0.0
Step: 31670, train/grad_norm: 1.780392864247915e-07
Step: 31670, train/learning_rate: 1.231556416314561e-05
Step: 31670, train/epoch: 7.536887168884277
Step: 31680, train/loss: 0.0
Step: 31680, train/grad_norm: 1.0687665053410456e-05
Step: 31680, train/learning_rate: 1.2303665243962314e-05
Step: 31680, train/epoch: 7.539267063140869
Step: 31690, train/loss: 0.0
Step: 31690, train/grad_norm: 2.7587941531237448e-06
Step: 31690, train/learning_rate: 1.2291765415284317e-05
Step: 31690, train/epoch: 7.541646957397461
Step: 31700, train/loss: 0.0
Step: 31700, train/grad_norm: 3.296043473710597e-07
Step: 31700, train/learning_rate: 1.2279866496101022e-05
Step: 31700, train/epoch: 7.544026851654053
Step: 31710, train/loss: 0.0
Step: 31710, train/grad_norm: 1.5900430980764213e-06
Step: 31710, train/learning_rate: 1.2267967576917727e-05
Step: 31710, train/epoch: 7.546406269073486
Step: 31720, train/loss: 0.0
Step: 31720, train/grad_norm: 5.87772301514633e-05
Step: 31720, train/learning_rate: 1.2256068657734431e-05
Step: 31720, train/epoch: 7.548786163330078
Step: 31730, train/loss: 0.0
Step: 31730, train/grad_norm: 2.8862746148661245e-06
Step: 31730, train/learning_rate: 1.2244169738551136e-05
Step: 31730, train/epoch: 7.55116605758667
Step: 31740, train/loss: 0.0
Step: 31740, train/grad_norm: 1.2342771697149146e-05
Step: 31740, train/learning_rate: 1.2232269909873139e-05
Step: 31740, train/epoch: 7.553545951843262
Step: 31750, train/loss: 0.0
Step: 31750, train/grad_norm: 4.2426113395777065e-06
Step: 31750, train/learning_rate: 1.2220370990689844e-05
Step: 31750, train/epoch: 7.5559258460998535
Step: 31760, train/loss: 0.0
Step: 31760, train/grad_norm: 2.1193180145928636e-05
Step: 31760, train/learning_rate: 1.2208472071506549e-05
Step: 31760, train/epoch: 7.558305740356445
Step: 31770, train/loss: 0.0
Step: 31770, train/grad_norm: 3.0819051062280778e-06
Step: 31770, train/learning_rate: 1.2196573152323253e-05
Step: 31770, train/epoch: 7.560685157775879
Step: 31780, train/loss: 0.0
Step: 31780, train/grad_norm: 2.898926538819069e-07
Step: 31780, train/learning_rate: 1.2184674233139958e-05
Step: 31780, train/epoch: 7.563065052032471
Step: 31790, train/loss: 0.0
Step: 31790, train/grad_norm: 1.0664763976819813e-05
Step: 31790, train/learning_rate: 1.2172775313956663e-05
Step: 31790, train/epoch: 7.5654449462890625
Step: 31800, train/loss: 0.0
Step: 31800, train/grad_norm: 8.381180123251397e-07
Step: 31800, train/learning_rate: 1.2160875485278666e-05
Step: 31800, train/epoch: 7.567824840545654
Step: 31810, train/loss: 0.0
Step: 31810, train/grad_norm: 1.3112965007167077e-06
Step: 31810, train/learning_rate: 1.214897656609537e-05
Step: 31810, train/epoch: 7.570204734802246
Step: 31820, train/loss: 0.0
Step: 31820, train/grad_norm: 7.85491408805683e-07
Step: 31820, train/learning_rate: 1.2137077646912076e-05
Step: 31820, train/epoch: 7.572584629058838
Step: 31830, train/loss: 0.0
Step: 31830, train/grad_norm: 4.0888198782340623e-07
Step: 31830, train/learning_rate: 1.212517872772878e-05
Step: 31830, train/epoch: 7.57496452331543
Step: 31840, train/loss: 0.0
Step: 31840, train/grad_norm: 9.985375072574243e-06
Step: 31840, train/learning_rate: 1.2113279808545485e-05
Step: 31840, train/epoch: 7.577343940734863
Step: 31850, train/loss: 0.0
Step: 31850, train/grad_norm: 1.258232987311203e-05
Step: 31850, train/learning_rate: 1.2101379979867488e-05
Step: 31850, train/epoch: 7.579723834991455
Step: 31860, train/loss: 0.0
Step: 31860, train/grad_norm: 1.248880653292872e-05
Step: 31860, train/learning_rate: 1.2089481060684193e-05
Step: 31860, train/epoch: 7.582103729248047
Step: 31870, train/loss: 0.0
Step: 31870, train/grad_norm: 0.0009198752813972533
Step: 31870, train/learning_rate: 1.2077582141500898e-05
Step: 31870, train/epoch: 7.584483623504639
Step: 31880, train/loss: 0.0
Step: 31880, train/grad_norm: 1.4447912235482363e-06
Step: 31880, train/learning_rate: 1.2065683222317602e-05
Step: 31880, train/epoch: 7.5868635177612305
Step: 31890, train/loss: 0.0
Step: 31890, train/grad_norm: 7.985367119545117e-06
Step: 31890, train/learning_rate: 1.2053784303134307e-05
Step: 31890, train/epoch: 7.589243412017822
Step: 31900, train/loss: 0.0
Step: 31900, train/grad_norm: 4.815059924112575e-07
Step: 31900, train/learning_rate: 1.204188447445631e-05
Step: 31900, train/epoch: 7.591622829437256
Step: 31910, train/loss: 0.0
Step: 31910, train/grad_norm: 4.899729333374125e-07
Step: 31910, train/learning_rate: 1.2029985555273015e-05
Step: 31910, train/epoch: 7.594002723693848
Step: 31920, train/loss: 0.0
Step: 31920, train/grad_norm: 8.855879968905356e-06
Step: 31920, train/learning_rate: 1.201808663608972e-05
Step: 31920, train/epoch: 7.5963826179504395
Step: 31930, train/loss: 0.0
Step: 31930, train/grad_norm: 3.307596671220381e-06
Step: 31930, train/learning_rate: 1.2006187716906425e-05
Step: 31930, train/epoch: 7.598762512207031
Step: 31940, train/loss: 0.0
Step: 31940, train/grad_norm: 1.5162589761530398e-07
Step: 31940, train/learning_rate: 1.199428879772313e-05
Step: 31940, train/epoch: 7.601142406463623
Step: 31950, train/loss: 0.0
Step: 31950, train/grad_norm: 7.70897486290778e-07
Step: 31950, train/learning_rate: 1.1982388969045132e-05
Step: 31950, train/epoch: 7.603522300720215
Step: 31960, train/loss: 0.0
Step: 31960, train/grad_norm: 1.3126978046784643e-06
Step: 31960, train/learning_rate: 1.1970490049861837e-05
Step: 31960, train/epoch: 7.605901718139648
Step: 31970, train/loss: 0.0
Step: 31970, train/grad_norm: 2.509265755179513e-07
Step: 31970, train/learning_rate: 1.1958591130678542e-05
Step: 31970, train/epoch: 7.60828161239624
Step: 31980, train/loss: 0.0
Step: 31980, train/grad_norm: 1.213965219903912e-06
Step: 31980, train/learning_rate: 1.1946692211495247e-05
Step: 31980, train/epoch: 7.610661506652832
Step: 31990, train/loss: 0.0
Step: 31990, train/grad_norm: 1.805453990755268e-07
Step: 31990, train/learning_rate: 1.1934793292311952e-05
Step: 31990, train/epoch: 7.613041400909424
Step: 32000, train/loss: 0.0
Step: 32000, train/grad_norm: 1.854547804214235e-07
Step: 32000, train/learning_rate: 1.1922893463633955e-05
Step: 32000, train/epoch: 7.615421295166016
Step: 32010, train/loss: 0.0
Step: 32010, train/grad_norm: 5.438993866846431e-06
Step: 32010, train/learning_rate: 1.191099454445066e-05
Step: 32010, train/epoch: 7.617801189422607
Step: 32020, train/loss: 0.0
Step: 32020, train/grad_norm: 2.991204723912233e-07
Step: 32020, train/learning_rate: 1.1899095625267364e-05
Step: 32020, train/epoch: 7.620181083679199
Step: 32030, train/loss: 0.0
Step: 32030, train/grad_norm: 7.236987471515022e-07
Step: 32030, train/learning_rate: 1.1887196706084069e-05
Step: 32030, train/epoch: 7.622560501098633
Step: 32040, train/loss: 0.0
Step: 32040, train/grad_norm: 5.79593290694902e-07
Step: 32040, train/learning_rate: 1.1875297786900774e-05
Step: 32040, train/epoch: 7.624940395355225
Step: 32050, train/loss: 0.0
Step: 32050, train/grad_norm: 4.7404564611497335e-06
Step: 32050, train/learning_rate: 1.1863397958222777e-05
Step: 32050, train/epoch: 7.627320289611816
Step: 32060, train/loss: 0.0
Step: 32060, train/grad_norm: 5.569643235503463e-07
Step: 32060, train/learning_rate: 1.1851499039039481e-05
Step: 32060, train/epoch: 7.629700183868408
Step: 32070, train/loss: 0.0
Step: 32070, train/grad_norm: 4.616253136191517e-06
Step: 32070, train/learning_rate: 1.1839600119856186e-05
Step: 32070, train/epoch: 7.632080078125
Step: 32080, train/loss: 0.0
Step: 32080, train/grad_norm: 8.953999781624589e-07
Step: 32080, train/learning_rate: 1.1827701200672891e-05
Step: 32080, train/epoch: 7.634459972381592
Step: 32090, train/loss: 0.0
Step: 32090, train/grad_norm: 3.035488816749421e-06
Step: 32090, train/learning_rate: 1.1815802281489596e-05
Step: 32090, train/epoch: 7.636839389801025
Step: 32100, train/loss: 0.05119999870657921
Step: 32100, train/grad_norm: 1.1532806638570037e-06
Step: 32100, train/learning_rate: 1.1803902452811599e-05
Step: 32100, train/epoch: 7.639219284057617
Step: 32110, train/loss: 0.0
Step: 32110, train/grad_norm: 6.54150153422961e-06
Step: 32110, train/learning_rate: 1.1792003533628304e-05
Step: 32110, train/epoch: 7.641599178314209
Step: 32120, train/loss: 0.0
Step: 32120, train/grad_norm: 0.0044176578521728516
Step: 32120, train/learning_rate: 1.1780104614445008e-05
Step: 32120, train/epoch: 7.643979072570801
Step: 32130, train/loss: 0.0
Step: 32130, train/grad_norm: 1.6460693359476863e-06
Step: 32130, train/learning_rate: 1.1768205695261713e-05
Step: 32130, train/epoch: 7.646358966827393
Step: 32140, train/loss: 0.0
Step: 32140, train/grad_norm: 4.1623067659202206e-07
Step: 32140, train/learning_rate: 1.1756306776078418e-05
Step: 32140, train/epoch: 7.648738861083984
Step: 32150, train/loss: 0.002199999988079071
Step: 32150, train/grad_norm: 1.5374989743577316e-05
Step: 32150, train/learning_rate: 1.1744407856895123e-05
Step: 32150, train/epoch: 7.651118278503418
Step: 32160, train/loss: 0.0
Step: 32160, train/grad_norm: 3.2853342872840585e-06
Step: 32160, train/learning_rate: 1.1732508028217126e-05
Step: 32160, train/epoch: 7.65349817276001
Step: 32170, train/loss: 0.0
Step: 32170, train/grad_norm: 7.18546798452735e-05
Step: 32170, train/learning_rate: 1.172060910903383e-05
Step: 32170, train/epoch: 7.655878067016602
Step: 32180, train/loss: 0.0
Step: 32180, train/grad_norm: 4.20616515839356e-06
Step: 32180, train/learning_rate: 1.1708710189850535e-05
Step: 32180, train/epoch: 7.658257961273193
Step: 32190, train/loss: 0.0006000000284984708
Step: 32190, train/grad_norm: 9.081599046112387e-07
Step: 32190, train/learning_rate: 1.169681127066724e-05
Step: 32190, train/epoch: 7.660637855529785
Step: 32200, train/loss: 0.09179999679327011
Step: 32200, train/grad_norm: 1.176214027509559e-06
Step: 32200, train/learning_rate: 1.1684912351483945e-05
Step: 32200, train/epoch: 7.663017749786377
Step: 32210, train/loss: 0.0
Step: 32210, train/grad_norm: 1.0745688996394165e-06
Step: 32210, train/learning_rate: 1.1673012522805948e-05
Step: 32210, train/epoch: 7.665397644042969
Step: 32220, train/loss: 0.0
Step: 32220, train/grad_norm: 8.220735594477446e-07
Step: 32220, train/learning_rate: 1.1661113603622653e-05
Step: 32220, train/epoch: 7.667777061462402
Step: 32230, train/loss: 0.0
Step: 32230, train/grad_norm: 1.1210856882826192e-06
Step: 32230, train/learning_rate: 1.1649214684439357e-05
Step: 32230, train/epoch: 7.670156955718994
Step: 32240, train/loss: 0.0
Step: 32240, train/grad_norm: 1.4459081967288512e-06
Step: 32240, train/learning_rate: 1.1637315765256062e-05
Step: 32240, train/epoch: 7.672536849975586
Step: 32250, train/loss: 0.0
Step: 32250, train/grad_norm: 1.700235998214339e-06
Step: 32250, train/learning_rate: 1.1625416846072767e-05
Step: 32250, train/epoch: 7.674916744232178
Step: 32260, train/loss: 0.0
Step: 32260, train/grad_norm: 4.1149320395561517e-07
Step: 32260, train/learning_rate: 1.161351701739477e-05
Step: 32260, train/epoch: 7.6772966384887695
Step: 32270, train/loss: 0.0
Step: 32270, train/grad_norm: 3.449584653480997e-07
Step: 32270, train/learning_rate: 1.1601618098211475e-05
Step: 32270, train/epoch: 7.679676532745361
Step: 32280, train/loss: 0.0
Step: 32280, train/grad_norm: 3.181694694376347e-07
Step: 32280, train/learning_rate: 1.158971917902818e-05
Step: 32280, train/epoch: 7.682055950164795
Step: 32290, train/loss: 0.0
Step: 32290, train/grad_norm: 2.8192407626193017e-06
Step: 32290, train/learning_rate: 1.1577820259844884e-05
Step: 32290, train/epoch: 7.684435844421387
Step: 32300, train/loss: 0.06129999831318855
Step: 32300, train/grad_norm: 2.4797753894745256e-07
Step: 32300, train/learning_rate: 1.1565921340661589e-05
Step: 32300, train/epoch: 7.6868157386779785
Step: 32310, train/loss: 0.0
Step: 32310, train/grad_norm: 1.0462721320436685e-06
Step: 32310, train/learning_rate: 1.1554021511983592e-05
Step: 32310, train/epoch: 7.68919563293457
Step: 32320, train/loss: 0.0
Step: 32320, train/grad_norm: 0.029659362509846687
Step: 32320, train/learning_rate: 1.1542122592800297e-05
Step: 32320, train/epoch: 7.691575527191162
Step: 32330, train/loss: 0.0
Step: 32330, train/grad_norm: 4.4929115006198117e-07
Step: 32330, train/learning_rate: 1.1530223673617002e-05
Step: 32330, train/epoch: 7.693955421447754
Step: 32340, train/loss: 9.999999747378752e-05
Step: 32340, train/grad_norm: 6.59985175843758e-07
Step: 32340, train/learning_rate: 1.1518324754433706e-05
Step: 32340, train/epoch: 7.696335315704346
Step: 32350, train/loss: 0.0
Step: 32350, train/grad_norm: 1.06213963135815e-06
Step: 32350, train/learning_rate: 1.1506425835250411e-05
Step: 32350, train/epoch: 7.698714733123779
Step: 32360, train/loss: 0.0
Step: 32360, train/grad_norm: 7.781963518027624e-08
Step: 32360, train/learning_rate: 1.1494526006572414e-05
Step: 32360, train/epoch: 7.701094627380371
Step: 32370, train/loss: 0.0
Step: 32370, train/grad_norm: 1.695162268333661e-07
Step: 32370, train/learning_rate: 1.1482627087389119e-05
Step: 32370, train/epoch: 7.703474521636963
Step: 32380, train/loss: 0.0
Step: 32380, train/grad_norm: 3.4356546052549675e-07
Step: 32380, train/learning_rate: 1.1470728168205824e-05
Step: 32380, train/epoch: 7.705854415893555
Step: 32390, train/loss: 0.0
Step: 32390, train/grad_norm: 2.3369312884824467e-07
Step: 32390, train/learning_rate: 1.1458829249022529e-05
Step: 32390, train/epoch: 7.7082343101501465
Step: 32400, train/loss: 0.0
Step: 32400, train/grad_norm: 4.4592911763174925e-06
Step: 32400, train/learning_rate: 1.1446930329839233e-05
Step: 32400, train/epoch: 7.710614204406738
Step: 32410, train/loss: 0.0
Step: 32410, train/grad_norm: 7.732081940048374e-06
Step: 32410, train/learning_rate: 1.1435030501161236e-05
Step: 32410, train/epoch: 7.712993621826172
Step: 32420, train/loss: 0.0
Step: 32420, train/grad_norm: 9.64232640399132e-07
Step: 32420, train/learning_rate: 1.1423131581977941e-05
Step: 32420, train/epoch: 7.715373516082764
Step: 32430, train/loss: 0.0
Step: 32430, train/grad_norm: 2.121021367429421e-07
Step: 32430, train/learning_rate: 1.1411232662794646e-05
Step: 32430, train/epoch: 7.7177534103393555
Step: 32440, train/loss: 0.0
Step: 32440, train/grad_norm: 9.656533705992842e-08
Step: 32440, train/learning_rate: 1.139933374361135e-05
Step: 32440, train/epoch: 7.720133304595947
Step: 32450, train/loss: 0.0
Step: 32450, train/grad_norm: 2.8289986175877857e-07
Step: 32450, train/learning_rate: 1.1387434824428055e-05
Step: 32450, train/epoch: 7.722513198852539
Step: 32460, train/loss: 0.0
Step: 32460, train/grad_norm: 9.851046911535377e-08
Step: 32460, train/learning_rate: 1.137553590524476e-05
Step: 32460, train/epoch: 7.724893093109131
Step: 32470, train/loss: 0.0
Step: 32470, train/grad_norm: 6.165925583445642e-07
Step: 32470, train/learning_rate: 1.1363636076566763e-05
Step: 32470, train/epoch: 7.7272725105285645
Step: 32480, train/loss: 0.0
Step: 32480, train/grad_norm: 3.1881036193226464e-06
Step: 32480, train/learning_rate: 1.1351737157383468e-05
Step: 32480, train/epoch: 7.729652404785156
Step: 32490, train/loss: 0.0
Step: 32490, train/grad_norm: 3.225044338250882e-06
Step: 32490, train/learning_rate: 1.1339838238200173e-05
Step: 32490, train/epoch: 7.732032299041748
Step: 32500, train/loss: 0.0
Step: 32500, train/grad_norm: 0.0001681281573837623
Step: 32500, train/learning_rate: 1.1327939319016878e-05
Step: 32500, train/epoch: 7.73441219329834
Step: 32510, train/loss: 0.0
Step: 32510, train/grad_norm: 7.980695954756811e-05
Step: 32510, train/learning_rate: 1.1316040399833582e-05
Step: 32510, train/epoch: 7.736792087554932
Step: 32520, train/loss: 0.0
Step: 32520, train/grad_norm: 4.16296097682789e-06
Step: 32520, train/learning_rate: 1.1304140571155585e-05
Step: 32520, train/epoch: 7.739171981811523
Step: 32530, train/loss: 0.0
Step: 32530, train/grad_norm: 2.3884069833002286e-07
Step: 32530, train/learning_rate: 1.129224165197229e-05
Step: 32530, train/epoch: 7.741551876068115
Step: 32540, train/loss: 0.0
Step: 32540, train/grad_norm: 8.911032637115568e-06
Step: 32540, train/learning_rate: 1.1280342732788995e-05
Step: 32540, train/epoch: 7.743931293487549
Step: 32550, train/loss: 0.0
Step: 32550, train/grad_norm: 4.399900035423343e-07
Step: 32550, train/learning_rate: 1.12684438136057e-05
Step: 32550, train/epoch: 7.746311187744141
Step: 32560, train/loss: 0.0
Step: 32560, train/grad_norm: 1.0217102897058794e-07
Step: 32560, train/learning_rate: 1.1256544894422404e-05
Step: 32560, train/epoch: 7.748691082000732
Step: 32570, train/loss: 0.0
Step: 32570, train/grad_norm: 9.630301747165504e-07
Step: 32570, train/learning_rate: 1.1244645065744407e-05
Step: 32570, train/epoch: 7.751070976257324
Step: 32580, train/loss: 0.0
Step: 32580, train/grad_norm: 1.5769144738442264e-05
Step: 32580, train/learning_rate: 1.1232746146561112e-05
Step: 32580, train/epoch: 7.753450870513916
Step: 32590, train/loss: 0.0
Step: 32590, train/grad_norm: 2.95978509257111e-07
Step: 32590, train/learning_rate: 1.1220847227377817e-05
Step: 32590, train/epoch: 7.755830764770508
Step: 32600, train/loss: 0.0
Step: 32600, train/grad_norm: 3.5647002505356795e-07
Step: 32600, train/learning_rate: 1.1208948308194522e-05
Step: 32600, train/epoch: 7.758210182189941
Step: 32610, train/loss: 0.0
Step: 32610, train/grad_norm: 3.1924520271786605e-07
Step: 32610, train/learning_rate: 1.1197049389011227e-05
Step: 32610, train/epoch: 7.760590076446533
Step: 32620, train/loss: 0.0
Step: 32620, train/grad_norm: 1.9362845193882094e-07
Step: 32620, train/learning_rate: 1.118514956033323e-05
Step: 32620, train/epoch: 7.762969970703125
Step: 32630, train/loss: 0.0
Step: 32630, train/grad_norm: 3.522811994116637e-06
Step: 32630, train/learning_rate: 1.1173250641149934e-05
Step: 32630, train/epoch: 7.765349864959717
Step: 32640, train/loss: 0.0
Step: 32640, train/grad_norm: 3.860854036474848e-08
Step: 32640, train/learning_rate: 1.116135172196664e-05
Step: 32640, train/epoch: 7.767729759216309
Step: 32650, train/loss: 0.0
Step: 32650, train/grad_norm: 1.8471866258806813e-09
Step: 32650, train/learning_rate: 1.1149452802783344e-05
Step: 32650, train/epoch: 7.7701096534729
Step: 32660, train/loss: 0.0
Step: 32660, train/grad_norm: 6.69091164695601e-08
Step: 32660, train/learning_rate: 1.1137553883600049e-05
Step: 32660, train/epoch: 7.772489070892334
Step: 32670, train/loss: 0.0
Step: 32670, train/grad_norm: 1.8720618299994385e-07
Step: 32670, train/learning_rate: 1.1125654054922052e-05
Step: 32670, train/epoch: 7.774868965148926
Step: 32680, train/loss: 0.0
Step: 32680, train/grad_norm: 8.40433713733546e-08
Step: 32680, train/learning_rate: 1.1113755135738757e-05
Step: 32680, train/epoch: 7.777248859405518
Step: 32690, train/loss: 0.0
Step: 32690, train/grad_norm: 1.0194959543241566e-07
Step: 32690, train/learning_rate: 1.1101856216555461e-05
Step: 32690, train/epoch: 7.779628753662109
Step: 32700, train/loss: 0.0
Step: 32700, train/grad_norm: 2.967718728541513e-06
Step: 32700, train/learning_rate: 1.1089957297372166e-05
Step: 32700, train/epoch: 7.782008647918701
Step: 32710, train/loss: 0.0
Step: 32710, train/grad_norm: 3.412837713767658e-06
Step: 32710, train/learning_rate: 1.1078058378188871e-05
Step: 32710, train/epoch: 7.784388542175293
Step: 32720, train/loss: 0.0
Step: 32720, train/grad_norm: 2.6475686354388017e-07
Step: 32720, train/learning_rate: 1.1066158549510874e-05
Step: 32720, train/epoch: 7.786768436431885
Step: 32730, train/loss: 0.0
Step: 32730, train/grad_norm: 2.788580104606808e-07
Step: 32730, train/learning_rate: 1.1054259630327579e-05
Step: 32730, train/epoch: 7.789147853851318
Step: 32740, train/loss: 0.0
Step: 32740, train/grad_norm: 2.844631410425791e-07
Step: 32740, train/learning_rate: 1.1042360711144283e-05
Step: 32740, train/epoch: 7.79152774810791
Step: 32750, train/loss: 0.0
Step: 32750, train/grad_norm: 1.961378757187049e-07
Step: 32750, train/learning_rate: 1.1030461791960988e-05
Step: 32750, train/epoch: 7.793907642364502
Step: 32760, train/loss: 0.0
Step: 32760, train/grad_norm: 1.7735362689563772e-06
Step: 32760, train/learning_rate: 1.1018562872777693e-05
Step: 32760, train/epoch: 7.796287536621094
Step: 32770, train/loss: 0.0
Step: 32770, train/grad_norm: 4.545497176877689e-06
Step: 32770, train/learning_rate: 1.1006663044099696e-05
Step: 32770, train/epoch: 7.7986674308776855
Step: 32780, train/loss: 0.0
Step: 32780, train/grad_norm: 2.523343880511675e-07
Step: 32780, train/learning_rate: 1.09947641249164e-05
Step: 32780, train/epoch: 7.801047325134277
Step: 32790, train/loss: 0.0
Step: 32790, train/grad_norm: 4.4275904656387866e-05
Step: 32790, train/learning_rate: 1.0982865205733106e-05
Step: 32790, train/epoch: 7.803426742553711
Step: 32800, train/loss: 0.0
Step: 32800, train/grad_norm: 1.155691370513523e-05
Step: 32800, train/learning_rate: 1.097096628654981e-05
Step: 32800, train/epoch: 7.805806636810303
Step: 32810, train/loss: 0.0
Step: 32810, train/grad_norm: 1.7327090972685255e-05
Step: 32810, train/learning_rate: 1.0959067367366515e-05
Step: 32810, train/epoch: 7.8081865310668945
Step: 32820, train/loss: 0.0
Step: 32820, train/grad_norm: 9.426616998098325e-06
Step: 32820, train/learning_rate: 1.094716844818322e-05
Step: 32820, train/epoch: 7.810566425323486
Step: 32830, train/loss: 0.0
Step: 32830, train/grad_norm: 1.1799687626989908e-06
Step: 32830, train/learning_rate: 1.0935268619505223e-05
Step: 32830, train/epoch: 7.812946319580078
Step: 32840, train/loss: 0.0
Step: 32840, train/grad_norm: 3.9574092625116464e-07
Step: 32840, train/learning_rate: 1.0923369700321928e-05
Step: 32840, train/epoch: 7.81532621383667
Step: 32850, train/loss: 0.0
Step: 32850, train/grad_norm: 2.9115471988916397e-07
Step: 32850, train/learning_rate: 1.0911470781138632e-05
Step: 32850, train/epoch: 7.8177056312561035
Step: 32860, train/loss: 0.0
Step: 32860, train/grad_norm: 2.7532610147318337e-06
Step: 32860, train/learning_rate: 1.0899571861955337e-05
Step: 32860, train/epoch: 7.820085525512695
Step: 32870, train/loss: 0.0
Step: 32870, train/grad_norm: 9.608525033399928e-06
Step: 32870, train/learning_rate: 1.0887672942772042e-05
Step: 32870, train/epoch: 7.822465419769287
Step: 32880, train/loss: 0.0
Step: 32880, train/grad_norm: 7.713131111586335e-08
Step: 32880, train/learning_rate: 1.0875773114094045e-05
Step: 32880, train/epoch: 7.824845314025879
Step: 32890, train/loss: 0.0
Step: 32890, train/grad_norm: 6.9347579483292066e-06
Step: 32890, train/learning_rate: 1.086387419491075e-05
Step: 32890, train/epoch: 7.827225208282471
Step: 32900, train/loss: 0.0
Step: 32900, train/grad_norm: 3.256361367220961e-07
Step: 32900, train/learning_rate: 1.0851975275727455e-05
Step: 32900, train/epoch: 7.8296051025390625
Step: 32910, train/loss: 0.0
Step: 32910, train/grad_norm: 5.052560481999535e-06
Step: 32910, train/learning_rate: 1.084007635654416e-05
Step: 32910, train/epoch: 7.831984996795654
Step: 32920, train/loss: 0.0
Step: 32920, train/grad_norm: 2.0636753106373362e-07
Step: 32920, train/learning_rate: 1.0828177437360864e-05
Step: 32920, train/epoch: 7.834364414215088
Step: 32930, train/loss: 0.0
Step: 32930, train/grad_norm: 5.348634317670076e-07
Step: 32930, train/learning_rate: 1.0816277608682867e-05
Step: 32930, train/epoch: 7.83674430847168
Step: 32940, train/loss: 0.0
Step: 32940, train/grad_norm: 9.912683935908717e-07
Step: 32940, train/learning_rate: 1.0804378689499572e-05
Step: 32940, train/epoch: 7.8391242027282715
Step: 32950, train/loss: 0.0
Step: 32950, train/grad_norm: 2.2614012777921744e-06
Step: 32950, train/learning_rate: 1.0792479770316277e-05
Step: 32950, train/epoch: 7.841504096984863
Step: 32960, train/loss: 0.0
Step: 32960, train/grad_norm: 6.269115715440421e-08
Step: 32960, train/learning_rate: 1.0780580851132981e-05
Step: 32960, train/epoch: 7.843883991241455
Step: 32970, train/loss: 0.0
Step: 32970, train/grad_norm: 1.0759833912743488e-06
Step: 32970, train/learning_rate: 1.0768681931949686e-05
Step: 32970, train/epoch: 7.846263885498047
Step: 32980, train/loss: 0.0
Step: 32980, train/grad_norm: 3.5976978551843786e-07
Step: 32980, train/learning_rate: 1.075678210327169e-05
Step: 32980, train/epoch: 7.8486433029174805
Step: 32990, train/loss: 0.0
Step: 32990, train/grad_norm: 4.409450866660336e-07
Step: 32990, train/learning_rate: 1.0744883184088394e-05
Step: 32990, train/epoch: 7.851023197174072
Step: 33000, train/loss: 0.0
Step: 33000, train/grad_norm: 7.594736217697573e-08
Step: 33000, train/learning_rate: 1.0732984264905099e-05
Step: 33000, train/epoch: 7.853403091430664
Step: 33010, train/loss: 0.0
Step: 33010, train/grad_norm: 1.196642926970526e-07
Step: 33010, train/learning_rate: 1.0721085345721804e-05
Step: 33010, train/epoch: 7.855782985687256
Step: 33020, train/loss: 0.0
Step: 33020, train/grad_norm: 5.283718564896844e-07
Step: 33020, train/learning_rate: 1.0709186426538508e-05
Step: 33020, train/epoch: 7.858162879943848
Step: 33030, train/loss: 0.0
Step: 33030, train/grad_norm: 3.0957392027630704e-07
Step: 33030, train/learning_rate: 1.0697286597860511e-05
Step: 33030, train/epoch: 7.8605427742004395
Step: 33040, train/loss: 0.0
Step: 33040, train/grad_norm: 5.016642603550281e-07
Step: 33040, train/learning_rate: 1.0685387678677216e-05
Step: 33040, train/epoch: 7.862922191619873
Step: 33050, train/loss: 0.0
Step: 33050, train/grad_norm: 6.677933015453164e-07
Step: 33050, train/learning_rate: 1.0673488759493921e-05
Step: 33050, train/epoch: 7.865302085876465
Step: 33060, train/loss: 0.0
Step: 33060, train/grad_norm: 2.870839637125755e-07
Step: 33060, train/learning_rate: 1.0661589840310626e-05
Step: 33060, train/epoch: 7.867681980133057
Step: 33070, train/loss: 0.0
Step: 33070, train/grad_norm: 1.7200377442350145e-06
Step: 33070, train/learning_rate: 1.064969092112733e-05
Step: 33070, train/epoch: 7.870061874389648
Step: 33080, train/loss: 0.0
Step: 33080, train/grad_norm: 1.2655783621084993e-06
Step: 33080, train/learning_rate: 1.0637791092449334e-05
Step: 33080, train/epoch: 7.87244176864624
Step: 33090, train/loss: 0.0
Step: 33090, train/grad_norm: 8.27757276056218e-07
Step: 33090, train/learning_rate: 1.0625892173266038e-05
Step: 33090, train/epoch: 7.874821662902832
Step: 33100, train/loss: 0.0
Step: 33100, train/grad_norm: 6.720422902617429e-07
Step: 33100, train/learning_rate: 1.0613993254082743e-05
Step: 33100, train/epoch: 7.877201557159424
Step: 33110, train/loss: 0.0
Step: 33110, train/grad_norm: 4.108646578515618e-07
Step: 33110, train/learning_rate: 1.0602094334899448e-05
Step: 33110, train/epoch: 7.879580974578857
Step: 33120, train/loss: 0.0
Step: 33120, train/grad_norm: 8.31675066592652e-08
Step: 33120, train/learning_rate: 1.0590195415716153e-05
Step: 33120, train/epoch: 7.881960868835449
Step: 33130, train/loss: 0.0
Step: 33130, train/grad_norm: 4.470459771255264e-06
Step: 33130, train/learning_rate: 1.0578296496532857e-05
Step: 33130, train/epoch: 7.884340763092041
Step: 33140, train/loss: 0.0
Step: 33140, train/grad_norm: 1.086041720554931e-06
Step: 33140, train/learning_rate: 1.056639666785486e-05
Step: 33140, train/epoch: 7.886720657348633
Step: 33150, train/loss: 0.0
Step: 33150, train/grad_norm: 5.373104272621276e-07
Step: 33150, train/learning_rate: 1.0554497748671565e-05
Step: 33150, train/epoch: 7.889100551605225
Step: 33160, train/loss: 0.0
Step: 33160, train/grad_norm: 6.067865797376726e-06
Step: 33160, train/learning_rate: 1.054259882948827e-05
Step: 33160, train/epoch: 7.891480445861816
Step: 33170, train/loss: 0.0
Step: 33170, train/grad_norm: 6.173970064082823e-07
Step: 33170, train/learning_rate: 1.0530699910304975e-05
Step: 33170, train/epoch: 7.89385986328125
Step: 33180, train/loss: 0.0
Step: 33180, train/grad_norm: 0.0003455311234574765
Step: 33180, train/learning_rate: 1.051880099112168e-05
Step: 33180, train/epoch: 7.896239757537842
Step: 33190, train/loss: 0.0
Step: 33190, train/grad_norm: 7.780256083833592e-08
Step: 33190, train/learning_rate: 1.0506901162443683e-05
Step: 33190, train/epoch: 7.898619651794434
Step: 33200, train/loss: 0.0
Step: 33200, train/grad_norm: 4.072935269050504e-07
Step: 33200, train/learning_rate: 1.0495002243260387e-05
Step: 33200, train/epoch: 7.900999546051025
Step: 33210, train/loss: 0.0
Step: 33210, train/grad_norm: 3.867295461645881e-08
Step: 33210, train/learning_rate: 1.0483103324077092e-05
Step: 33210, train/epoch: 7.903379440307617
Step: 33220, train/loss: 0.0
Step: 33220, train/grad_norm: 1.3998452175201237e-07
Step: 33220, train/learning_rate: 1.0471204404893797e-05
Step: 33220, train/epoch: 7.905759334564209
Step: 33230, train/loss: 0.0
Step: 33230, train/grad_norm: 1.4453652852353116e-07
Step: 33230, train/learning_rate: 1.0459305485710502e-05
Step: 33230, train/epoch: 7.908138751983643
Step: 33240, train/loss: 0.0
Step: 33240, train/grad_norm: 1.954342224053107e-06
Step: 33240, train/learning_rate: 1.0447405657032505e-05
Step: 33240, train/epoch: 7.910518646240234
Step: 33250, train/loss: 0.0
Step: 33250, train/grad_norm: 3.79288735530281e-07
Step: 33250, train/learning_rate: 1.043550673784921e-05
Step: 33250, train/epoch: 7.912898540496826
Step: 33260, train/loss: 0.0
Step: 33260, train/grad_norm: 1.0320312867406756e-05
Step: 33260, train/learning_rate: 1.0423607818665914e-05
Step: 33260, train/epoch: 7.915278434753418
Step: 33270, train/loss: 0.0
Step: 33270, train/grad_norm: 2.7396558834880125e-06
Step: 33270, train/learning_rate: 1.0411708899482619e-05
Step: 33270, train/epoch: 7.91765832901001
Step: 33280, train/loss: 0.0
Step: 33280, train/grad_norm: 3.311420186946634e-07
Step: 33280, train/learning_rate: 1.0399809980299324e-05
Step: 33280, train/epoch: 7.920038223266602
Step: 33290, train/loss: 0.0
Step: 33290, train/grad_norm: 7.4578251769708e-06
Step: 33290, train/learning_rate: 1.0387910151621327e-05
Step: 33290, train/epoch: 7.922418117523193
Step: 33300, train/loss: 0.0
Step: 33300, train/grad_norm: 4.809540996575379e-07
Step: 33300, train/learning_rate: 1.0376011232438032e-05
Step: 33300, train/epoch: 7.924797534942627
Step: 33310, train/loss: 0.0
Step: 33310, train/grad_norm: 1.1106163100293998e-07
Step: 33310, train/learning_rate: 1.0364112313254736e-05
Step: 33310, train/epoch: 7.927177429199219
Step: 33320, train/loss: 0.0
Step: 33320, train/grad_norm: 8.425144187640399e-06
Step: 33320, train/learning_rate: 1.0352213394071441e-05
Step: 33320, train/epoch: 7.9295573234558105
Step: 33330, train/loss: 0.0
Step: 33330, train/grad_norm: 2.747048256424023e-06
Step: 33330, train/learning_rate: 1.0340314474888146e-05
Step: 33330, train/epoch: 7.931937217712402
Step: 33340, train/loss: 0.0
Step: 33340, train/grad_norm: 2.9910354442108655e-06
Step: 33340, train/learning_rate: 1.0328414646210149e-05
Step: 33340, train/epoch: 7.934317111968994
Step: 33350, train/loss: 0.0
Step: 33350, train/grad_norm: 6.61803412072004e-08
Step: 33350, train/learning_rate: 1.0316515727026854e-05
Step: 33350, train/epoch: 7.936697006225586
Step: 33360, train/loss: 0.0
Step: 33360, train/grad_norm: 1.9312675192395545e-07
Step: 33360, train/learning_rate: 1.0304616807843558e-05
Step: 33360, train/epoch: 7.9390764236450195
Step: 33370, train/loss: 0.0
Step: 33370, train/grad_norm: 1.6433697965112515e-05
Step: 33370, train/learning_rate: 1.0292717888660263e-05
Step: 33370, train/epoch: 7.941456317901611
Step: 33380, train/loss: 0.0
Step: 33380, train/grad_norm: 2.7242472810939944e-07
Step: 33380, train/learning_rate: 1.0280818969476968e-05
Step: 33380, train/epoch: 7.943836212158203
Step: 33390, train/loss: 0.0
Step: 33390, train/grad_norm: 1.1044751602184988e-07
Step: 33390, train/learning_rate: 1.0268919140798971e-05
Step: 33390, train/epoch: 7.946216106414795
Step: 33400, train/loss: 0.0
Step: 33400, train/grad_norm: 3.177807741394645e-07
Step: 33400, train/learning_rate: 1.0257020221615676e-05
Step: 33400, train/epoch: 7.948596000671387
Step: 33410, train/loss: 9.999999747378752e-05
Step: 33410, train/grad_norm: 5.8493984766982976e-08
Step: 33410, train/learning_rate: 1.024512130243238e-05
Step: 33410, train/epoch: 7.9509758949279785
Step: 33420, train/loss: 0.19059999287128448
Step: 33420, train/grad_norm: 1.9804427211056463e-05
Step: 33420, train/learning_rate: 1.0233222383249085e-05
Step: 33420, train/epoch: 7.953355312347412
Step: 33430, train/loss: 0.0
Step: 33430, train/grad_norm: 3.192543226759881e-05
Step: 33430, train/learning_rate: 1.022132346406579e-05
Step: 33430, train/epoch: 7.955735206604004
Step: 33440, train/loss: 0.0
Step: 33440, train/grad_norm: 2.720374868658837e-05
Step: 33440, train/learning_rate: 1.0209423635387793e-05
Step: 33440, train/epoch: 7.958115100860596
Step: 33450, train/loss: 0.0
Step: 33450, train/grad_norm: 1.902864642033819e-05
Step: 33450, train/learning_rate: 1.0197524716204498e-05
Step: 33450, train/epoch: 7.9604949951171875
Step: 33460, train/loss: 0.0
Step: 33460, train/grad_norm: 3.312182889203541e-05
Step: 33460, train/learning_rate: 1.0185625797021203e-05
Step: 33460, train/epoch: 7.962874889373779
Step: 33470, train/loss: 0.0
Step: 33470, train/grad_norm: 0.0005635513225570321
Step: 33470, train/learning_rate: 1.0173726877837908e-05
Step: 33470, train/epoch: 7.965254783630371
Step: 33480, train/loss: 0.0
Step: 33480, train/grad_norm: 1.2024959687551018e-05
Step: 33480, train/learning_rate: 1.0161827958654612e-05
Step: 33480, train/epoch: 7.967634677886963
Step: 33490, train/loss: 0.0
Step: 33490, train/grad_norm: 4.796187567990273e-05
Step: 33490, train/learning_rate: 1.0149929039471317e-05
Step: 33490, train/epoch: 7.9700140953063965
Step: 33500, train/loss: 0.0
Step: 33500, train/grad_norm: 0.00016311892250087112
Step: 33500, train/learning_rate: 1.013802921079332e-05
Step: 33500, train/epoch: 7.972393989562988
Step: 33510, train/loss: 0.0
Step: 33510, train/grad_norm: 0.00035754640703089535
Step: 33510, train/learning_rate: 1.0126130291610025e-05
Step: 33510, train/epoch: 7.97477388381958
Step: 33520, train/loss: 0.0
Step: 33520, train/grad_norm: 0.00045794248580932617
Step: 33520, train/learning_rate: 1.011423137242673e-05
Step: 33520, train/epoch: 7.977153778076172
Step: 33530, train/loss: 0.0
Step: 33530, train/grad_norm: 7.07464714650996e-05
Step: 33530, train/learning_rate: 1.0102332453243434e-05
Step: 33530, train/epoch: 7.979533672332764
Step: 33540, train/loss: 0.0
Step: 33540, train/grad_norm: 1.647169847274199e-05
Step: 33540, train/learning_rate: 1.009043353406014e-05
Step: 33540, train/epoch: 7.9819135665893555
Step: 33550, train/loss: 0.0
Step: 33550, train/grad_norm: 1.1698814887495246e-05
Step: 33550, train/learning_rate: 1.0078533705382142e-05
Step: 33550, train/epoch: 7.984292984008789
Step: 33560, train/loss: 0.0
Step: 33560, train/grad_norm: 0.0001592903136042878
Step: 33560, train/learning_rate: 1.0066634786198847e-05
Step: 33560, train/epoch: 7.986672878265381
Step: 33570, train/loss: 0.0
Step: 33570, train/grad_norm: 3.8641697756247595e-05
Step: 33570, train/learning_rate: 1.0054735867015552e-05
Step: 33570, train/epoch: 7.989052772521973
Step: 33580, train/loss: 0.0
Step: 33580, train/grad_norm: 2.329681592527777e-05
Step: 33580, train/learning_rate: 1.0042836947832257e-05
Step: 33580, train/epoch: 7.9914326667785645
Step: 33590, train/loss: 0.0
Step: 33590, train/grad_norm: 2.9968203307362273e-05
Step: 33590, train/learning_rate: 1.0030938028648961e-05
Step: 33590, train/epoch: 7.993812561035156
Step: 33600, train/loss: 0.0
Step: 33600, train/grad_norm: 0.00010153219773201272
Step: 33600, train/learning_rate: 1.0019038199970964e-05
Step: 33600, train/epoch: 7.996192455291748
Step: 33610, train/loss: 0.0
Step: 33610, train/grad_norm: 1.1235531928832643e-05
Step: 33610, train/learning_rate: 1.0007139280787669e-05
Step: 33610, train/epoch: 7.998571872711182
Step: 33616, eval/loss: 0.03874758630990982
Step: 33616, eval/accuracy: 0.9954185485839844
Step: 33616, eval/f1: 0.9951589703559875
Step: 33616, eval/runtime: 733.8856201171875
Step: 33616, eval/samples_per_second: 9.8149995803833
Step: 33616, eval/steps_per_second: 1.2280000448226929
Step: 33616, train/epoch: 8.0
Step: 33620, train/loss: 0.0
Step: 33620, train/grad_norm: 1.0205623766523786e-05
Step: 33620, train/learning_rate: 9.995240361604374e-06
Step: 33620, train/epoch: 8.000951766967773
Step: 33630, train/loss: 0.0
Step: 33630, train/grad_norm: 1.3588783076556865e-05
Step: 33630, train/learning_rate: 9.983341442421079e-06
Step: 33630, train/epoch: 8.003332138061523
Step: 33640, train/loss: 0.0
Step: 33640, train/grad_norm: 1.567999424878508e-05
Step: 33640, train/learning_rate: 9.971442523237783e-06
Step: 33640, train/epoch: 8.005711555480957
Step: 33650, train/loss: 0.0
Step: 33650, train/grad_norm: 1.7819704225985333e-05
Step: 33650, train/learning_rate: 9.959542694559786e-06
Step: 33650, train/epoch: 8.00809097290039
Step: 33660, train/loss: 0.08869999647140503
Step: 33660, train/grad_norm: 2.096442403853871e-05
Step: 33660, train/learning_rate: 9.947643775376491e-06
Step: 33660, train/epoch: 8.01047134399414
Step: 33670, train/loss: 0.0
Step: 33670, train/grad_norm: 0.00011633881513262168
Step: 33670, train/learning_rate: 9.935744856193196e-06
Step: 33670, train/epoch: 8.012850761413574
Step: 33680, train/loss: 0.0
Step: 33680, train/grad_norm: 3.4716404115897603e-06
Step: 33680, train/learning_rate: 9.9238459370099e-06
Step: 33680, train/epoch: 8.015231132507324
Step: 33690, train/loss: 0.0
Step: 33690, train/grad_norm: 6.548886631208006e-06
Step: 33690, train/learning_rate: 9.911947017826606e-06
Step: 33690, train/epoch: 8.017610549926758
Step: 33700, train/loss: 0.0
Step: 33700, train/grad_norm: 4.9039022997021675e-05
Step: 33700, train/learning_rate: 9.900047189148609e-06
Step: 33700, train/epoch: 8.019990921020508
Step: 33710, train/loss: 0.0
Step: 33710, train/grad_norm: 1.621580304345116e-05
Step: 33710, train/learning_rate: 9.888148269965313e-06
Step: 33710, train/epoch: 8.022370338439941
Step: 33720, train/loss: 0.0
Step: 33720, train/grad_norm: 1.1110133527836297e-05
Step: 33720, train/learning_rate: 9.876249350782018e-06
Step: 33720, train/epoch: 8.024749755859375
Step: 33730, train/loss: 0.0
Step: 33730, train/grad_norm: 5.565903484239243e-05
Step: 33730, train/learning_rate: 9.864350431598723e-06
Step: 33730, train/epoch: 8.027130126953125
Step: 33740, train/loss: 0.0
Step: 33740, train/grad_norm: 2.241969013994094e-05
Step: 33740, train/learning_rate: 9.852451512415428e-06
Step: 33740, train/epoch: 8.029509544372559
Step: 33750, train/loss: 0.0
Step: 33750, train/grad_norm: 9.711910934129264e-06
Step: 33750, train/learning_rate: 9.84055168373743e-06
Step: 33750, train/epoch: 8.031889915466309
Step: 33760, train/loss: 0.0
Step: 33760, train/grad_norm: 1.6374739061575383e-05
Step: 33760, train/learning_rate: 9.828652764554136e-06
Step: 33760, train/epoch: 8.034269332885742
Step: 33770, train/loss: 0.0
Step: 33770, train/grad_norm: 1.8568831364973448e-05
Step: 33770, train/learning_rate: 9.81675384537084e-06
Step: 33770, train/epoch: 8.036648750305176
Step: 33780, train/loss: 0.0
Step: 33780, train/grad_norm: 1.1731670383596793e-05
Step: 33780, train/learning_rate: 9.804854926187545e-06
Step: 33780, train/epoch: 8.039029121398926
Step: 33790, train/loss: 0.0
Step: 33790, train/grad_norm: 3.150896736769937e-05
Step: 33790, train/learning_rate: 9.79295600700425e-06
Step: 33790, train/epoch: 8.04140853881836
Step: 33800, train/loss: 0.0
Step: 33800, train/grad_norm: 0.0001024203302222304
Step: 33800, train/learning_rate: 9.781057087820955e-06
Step: 33800, train/epoch: 8.04378890991211
Step: 33810, train/loss: 0.0
Step: 33810, train/grad_norm: 1.9644046915345825e-05
Step: 33810, train/learning_rate: 9.769157259142958e-06
Step: 33810, train/epoch: 8.046168327331543
Step: 33820, train/loss: 0.0
Step: 33820, train/grad_norm: 9.781811968423426e-06
Step: 33820, train/learning_rate: 9.757258339959662e-06
Step: 33820, train/epoch: 8.048548698425293
Step: 33830, train/loss: 0.0
Step: 33830, train/grad_norm: 2.89055060420651e-06
Step: 33830, train/learning_rate: 9.745359420776367e-06
Step: 33830, train/epoch: 8.050928115844727
Step: 33840, train/loss: 0.0
Step: 33840, train/grad_norm: 6.367606692947447e-05
Step: 33840, train/learning_rate: 9.733460501593072e-06
Step: 33840, train/epoch: 8.05330753326416
Step: 33850, train/loss: 0.0
Step: 33850, train/grad_norm: 3.8294921978376806e-05
Step: 33850, train/learning_rate: 9.721561582409777e-06
Step: 33850, train/epoch: 8.05568790435791
Step: 33860, train/loss: 0.0
Step: 33860, train/grad_norm: 8.645311595500971e-07
Step: 33860, train/learning_rate: 9.70966175373178e-06
Step: 33860, train/epoch: 8.058067321777344
Step: 33870, train/loss: 0.0
Step: 33870, train/grad_norm: 2.2955784515943378e-05
Step: 33870, train/learning_rate: 9.697762834548485e-06
Step: 33870, train/epoch: 8.060447692871094
Step: 33880, train/loss: 0.0
Step: 33880, train/grad_norm: 2.7321631932863966e-05
Step: 33880, train/learning_rate: 9.68586391536519e-06
Step: 33880, train/epoch: 8.062827110290527
Step: 33890, train/loss: 0.0
Step: 33890, train/grad_norm: 1.7129887055489235e-05
Step: 33890, train/learning_rate: 9.673964996181894e-06
Step: 33890, train/epoch: 8.065207481384277
Step: 33900, train/loss: 0.0
Step: 33900, train/grad_norm: 4.239500412950292e-05
Step: 33900, train/learning_rate: 9.662066076998599e-06
Step: 33900, train/epoch: 8.067586898803711
Step: 33910, train/loss: 0.0
Step: 33910, train/grad_norm: 1.1297315722913481e-06
Step: 33910, train/learning_rate: 9.650166248320602e-06
Step: 33910, train/epoch: 8.069966316223145
Step: 33920, train/loss: 0.0
Step: 33920, train/grad_norm: 0.00010418446618132293
Step: 33920, train/learning_rate: 9.638267329137307e-06
Step: 33920, train/epoch: 8.072346687316895
Step: 33930, train/loss: 0.0
Step: 33930, train/grad_norm: 5.788769612991018e-06
Step: 33930, train/learning_rate: 9.626368409954011e-06
Step: 33930, train/epoch: 8.074726104736328
Step: 33940, train/loss: 0.0
Step: 33940, train/grad_norm: 3.5048683457716834e-06
Step: 33940, train/learning_rate: 9.614469490770716e-06
Step: 33940, train/epoch: 8.077106475830078
Step: 33950, train/loss: 0.0
Step: 33950, train/grad_norm: 8.262291885330342e-06
Step: 33950, train/learning_rate: 9.602570571587421e-06
Step: 33950, train/epoch: 8.079485893249512
Step: 33960, train/loss: 0.0
Step: 33960, train/grad_norm: 2.2055459339753725e-05
Step: 33960, train/learning_rate: 9.590670742909424e-06
Step: 33960, train/epoch: 8.081865310668945
Step: 33970, train/loss: 0.0
Step: 33970, train/grad_norm: 3.2076557545224205e-05
Step: 33970, train/learning_rate: 9.578771823726129e-06
Step: 33970, train/epoch: 8.084245681762695
Step: 33980, train/loss: 0.0
Step: 33980, train/grad_norm: 0.0002802321396302432
Step: 33980, train/learning_rate: 9.566872904542834e-06
Step: 33980, train/epoch: 8.086625099182129
Step: 33990, train/loss: 0.0
Step: 33990, train/grad_norm: 2.9621942303492688e-05
Step: 33990, train/learning_rate: 9.554973985359538e-06
Step: 33990, train/epoch: 8.089005470275879
Step: 34000, train/loss: 0.0
Step: 34000, train/grad_norm: 8.827807323541492e-05
Step: 34000, train/learning_rate: 9.543075066176243e-06
Step: 34000, train/epoch: 8.091384887695312
Step: 34010, train/loss: 0.0
Step: 34010, train/grad_norm: 3.986448064097203e-06
Step: 34010, train/learning_rate: 9.531175237498246e-06
Step: 34010, train/epoch: 8.093765258789062
Step: 34020, train/loss: 0.0
Step: 34020, train/grad_norm: 2.280014450661838e-05
Step: 34020, train/learning_rate: 9.519276318314951e-06
Step: 34020, train/epoch: 8.096144676208496
Step: 34030, train/loss: 0.0
Step: 34030, train/grad_norm: 0.00010527426638873294
Step: 34030, train/learning_rate: 9.507377399131656e-06
Step: 34030, train/epoch: 8.09852409362793
Step: 34040, train/loss: 0.0
Step: 34040, train/grad_norm: 9.748884622240439e-06
Step: 34040, train/learning_rate: 9.49547847994836e-06
Step: 34040, train/epoch: 8.10090446472168
Step: 34050, train/loss: 0.0
Step: 34050, train/grad_norm: 8.261017683253158e-06
Step: 34050, train/learning_rate: 9.483579560765065e-06
Step: 34050, train/epoch: 8.103283882141113
Step: 34060, train/loss: 0.0
Step: 34060, train/grad_norm: 2.6780228381539928e-06
Step: 34060, train/learning_rate: 9.471679732087068e-06
Step: 34060, train/epoch: 8.105664253234863
Step: 34070, train/loss: 0.0
Step: 34070, train/grad_norm: 9.06215916529618e-07
Step: 34070, train/learning_rate: 9.459780812903773e-06
Step: 34070, train/epoch: 8.108043670654297
Step: 34080, train/loss: 0.0
Step: 34080, train/grad_norm: 1.0469542530699982e-06
Step: 34080, train/learning_rate: 9.447881893720478e-06
Step: 34080, train/epoch: 8.110424041748047
Step: 34090, train/loss: 0.0
Step: 34090, train/grad_norm: 2.059966209344566e-05
Step: 34090, train/learning_rate: 9.435982974537183e-06
Step: 34090, train/epoch: 8.11280345916748
Step: 34100, train/loss: 0.0
Step: 34100, train/grad_norm: 2.4177119485102594e-05
Step: 34100, train/learning_rate: 9.424084055353887e-06
Step: 34100, train/epoch: 8.115182876586914
Step: 34110, train/loss: 0.0
Step: 34110, train/grad_norm: 5.38318499820889e-06
Step: 34110, train/learning_rate: 9.41218422667589e-06
Step: 34110, train/epoch: 8.117563247680664
Step: 34120, train/loss: 0.0
Step: 34120, train/grad_norm: 6.160993507364765e-05
Step: 34120, train/learning_rate: 9.400285307492595e-06
Step: 34120, train/epoch: 8.119942665100098
Step: 34130, train/loss: 0.0
Step: 34130, train/grad_norm: 9.18547302717343e-05
Step: 34130, train/learning_rate: 9.3883863883093e-06
Step: 34130, train/epoch: 8.122323036193848
Step: 34140, train/loss: 0.0
Step: 34140, train/grad_norm: 1.4939908396627288e-05
Step: 34140, train/learning_rate: 9.376487469126005e-06
Step: 34140, train/epoch: 8.124702453613281
Step: 34150, train/loss: 0.0
Step: 34150, train/grad_norm: 6.621216016355902e-05
Step: 34150, train/learning_rate: 9.36458854994271e-06
Step: 34150, train/epoch: 8.127081871032715
Step: 34160, train/loss: 0.0
Step: 34160, train/grad_norm: 8.86961497599259e-05
Step: 34160, train/learning_rate: 9.352689630759414e-06
Step: 34160, train/epoch: 8.129462242126465
Step: 34170, train/loss: 0.0
Step: 34170, train/grad_norm: 5.347299884306267e-05
Step: 34170, train/learning_rate: 9.340789802081417e-06
Step: 34170, train/epoch: 8.131841659545898
Step: 34180, train/loss: 0.0
Step: 34180, train/grad_norm: 7.671684443266713e-07
Step: 34180, train/learning_rate: 9.328890882898122e-06
Step: 34180, train/epoch: 8.134222030639648
Step: 34190, train/loss: 0.0
Step: 34190, train/grad_norm: 8.341400825884193e-05
Step: 34190, train/learning_rate: 9.316991963714827e-06
Step: 34190, train/epoch: 8.136601448059082
Step: 34200, train/loss: 0.0
Step: 34200, train/grad_norm: 1.1543968412297545e-06
Step: 34200, train/learning_rate: 9.305093044531532e-06
Step: 34200, train/epoch: 8.138981819152832
Step: 34210, train/loss: 0.0
Step: 34210, train/grad_norm: 2.8463136914069764e-06
Step: 34210, train/learning_rate: 9.293194125348236e-06
Step: 34210, train/epoch: 8.141361236572266
Step: 34220, train/loss: 0.0
Step: 34220, train/grad_norm: 5.127010808791965e-06
Step: 34220, train/learning_rate: 9.28129429667024e-06
Step: 34220, train/epoch: 8.1437406539917
Step: 34230, train/loss: 0.0
Step: 34230, train/grad_norm: 3.0487597541650757e-05
Step: 34230, train/learning_rate: 9.269395377486944e-06
Step: 34230, train/epoch: 8.14612102508545
Step: 34240, train/loss: 0.0
Step: 34240, train/grad_norm: 1.2588504432642367e-05
Step: 34240, train/learning_rate: 9.257496458303649e-06
Step: 34240, train/epoch: 8.148500442504883
Step: 34250, train/loss: 0.0
Step: 34250, train/grad_norm: 7.201505241027917e-07
Step: 34250, train/learning_rate: 9.245597539120354e-06
Step: 34250, train/epoch: 8.150880813598633
Step: 34260, train/loss: 0.0
Step: 34260, train/grad_norm: 3.165468297083862e-05
Step: 34260, train/learning_rate: 9.233698619937059e-06
Step: 34260, train/epoch: 8.153260231018066
Step: 34270, train/loss: 0.0
Step: 34270, train/grad_norm: 3.843052763841115e-05
Step: 34270, train/learning_rate: 9.221798791259062e-06
Step: 34270, train/epoch: 8.155640602111816
Step: 34280, train/loss: 0.0
Step: 34280, train/grad_norm: 8.372503361897543e-05
Step: 34280, train/learning_rate: 9.209899872075766e-06
Step: 34280, train/epoch: 8.15802001953125
Step: 34290, train/loss: 0.0
Step: 34290, train/grad_norm: 5.579104254138656e-06
Step: 34290, train/learning_rate: 9.198000952892471e-06
Step: 34290, train/epoch: 8.160399436950684
Step: 34300, train/loss: 0.0
Step: 34300, train/grad_norm: 1.37885081130662e-05
Step: 34300, train/learning_rate: 9.186102033709176e-06
Step: 34300, train/epoch: 8.162779808044434
Step: 34310, train/loss: 0.0
Step: 34310, train/grad_norm: 4.836474545300007e-07
Step: 34310, train/learning_rate: 9.17420311452588e-06
Step: 34310, train/epoch: 8.165159225463867
Step: 34320, train/loss: 0.0
Step: 34320, train/grad_norm: 2.1475934772752225e-05
Step: 34320, train/learning_rate: 9.162303285847884e-06
Step: 34320, train/epoch: 8.167539596557617
Step: 34330, train/loss: 0.0
Step: 34330, train/grad_norm: 2.4926701371441595e-05
Step: 34330, train/learning_rate: 9.150404366664588e-06
Step: 34330, train/epoch: 8.16991901397705
Step: 34340, train/loss: 0.0
Step: 34340, train/grad_norm: 2.4682296498212963e-05
Step: 34340, train/learning_rate: 9.138505447481293e-06
Step: 34340, train/epoch: 8.172298431396484
Step: 34350, train/loss: 0.0
Step: 34350, train/grad_norm: 1.352009621768957e-05
Step: 34350, train/learning_rate: 9.126606528297998e-06
Step: 34350, train/epoch: 8.174678802490234
Step: 34360, train/loss: 0.0
Step: 34360, train/grad_norm: 2.3539889298263006e-05
Step: 34360, train/learning_rate: 9.114707609114703e-06
Step: 34360, train/epoch: 8.177058219909668
Step: 34370, train/loss: 0.0
Step: 34370, train/grad_norm: 0.00020456043421290815
Step: 34370, train/learning_rate: 9.102807780436706e-06
Step: 34370, train/epoch: 8.179438591003418
Step: 34380, train/loss: 0.0
Step: 34380, train/grad_norm: 3.470859883236699e-05
Step: 34380, train/learning_rate: 9.09090886125341e-06
Step: 34380, train/epoch: 8.181818008422852
Step: 34390, train/loss: 0.0
Step: 34390, train/grad_norm: 7.745062475805753e-07
Step: 34390, train/learning_rate: 9.079009942070115e-06
Step: 34390, train/epoch: 8.184198379516602
Step: 34400, train/loss: 0.0
Step: 34400, train/grad_norm: 8.856060594553128e-05
Step: 34400, train/learning_rate: 9.06711102288682e-06
Step: 34400, train/epoch: 8.186577796936035
Step: 34410, train/loss: 0.0
Step: 34410, train/grad_norm: 5.861512022420357e-07
Step: 34410, train/learning_rate: 9.055212103703525e-06
Step: 34410, train/epoch: 8.188957214355469
Step: 34420, train/loss: 0.0
Step: 34420, train/grad_norm: 1.2240645446581766e-05
Step: 34420, train/learning_rate: 9.043312275025528e-06
Step: 34420, train/epoch: 8.191337585449219
Step: 34430, train/loss: 0.0
Step: 34430, train/grad_norm: 6.172150278871413e-06
Step: 34430, train/learning_rate: 9.031413355842233e-06
Step: 34430, train/epoch: 8.193717002868652
Step: 34440, train/loss: 0.0
Step: 34440, train/grad_norm: 6.3903753471095115e-06
Step: 34440, train/learning_rate: 9.019514436658937e-06
Step: 34440, train/epoch: 8.196097373962402
Step: 34450, train/loss: 0.0
Step: 34450, train/grad_norm: 3.925526641523902e-07
Step: 34450, train/learning_rate: 9.007615517475642e-06
Step: 34450, train/epoch: 8.198476791381836
Step: 34460, train/loss: 0.0
Step: 34460, train/grad_norm: 5.171265911485534e-06
Step: 34460, train/learning_rate: 8.995716598292347e-06
Step: 34460, train/epoch: 8.200857162475586
Step: 34470, train/loss: 0.0
Step: 34470, train/grad_norm: 8.182524311450834e-07
Step: 34470, train/learning_rate: 8.983817679109052e-06
Step: 34470, train/epoch: 8.20323657989502
Step: 34480, train/loss: 0.0
Step: 34480, train/grad_norm: 9.017406227940228e-06
Step: 34480, train/learning_rate: 8.971917850431055e-06
Step: 34480, train/epoch: 8.205615997314453
Step: 34490, train/loss: 0.0
Step: 34490, train/grad_norm: 3.476217534625903e-05
Step: 34490, train/learning_rate: 8.96001893124776e-06
Step: 34490, train/epoch: 8.207996368408203
Step: 34500, train/loss: 0.0
Step: 34500, train/grad_norm: 1.5284267647075467e-05
Step: 34500, train/learning_rate: 8.948120012064464e-06
Step: 34500, train/epoch: 8.210375785827637
Step: 34510, train/loss: 0.0
Step: 34510, train/grad_norm: 2.1313539377842972e-07
Step: 34510, train/learning_rate: 8.93622109288117e-06
Step: 34510, train/epoch: 8.212756156921387
Step: 34520, train/loss: 0.0
Step: 34520, train/grad_norm: 0.00607218686491251
Step: 34520, train/learning_rate: 8.924322173697874e-06
Step: 34520, train/epoch: 8.21513557434082
Step: 34530, train/loss: 0.0
Step: 34530, train/grad_norm: 9.049193749888218e-08
Step: 34530, train/learning_rate: 8.912422345019877e-06
Step: 34530, train/epoch: 8.21751594543457
Step: 34540, train/loss: 0.0
Step: 34540, train/grad_norm: 7.509073384426301e-06
Step: 34540, train/learning_rate: 8.900523425836582e-06
Step: 34540, train/epoch: 8.219895362854004
Step: 34550, train/loss: 0.0
Step: 34550, train/grad_norm: 6.95965638897178e-07
Step: 34550, train/learning_rate: 8.888624506653287e-06
Step: 34550, train/epoch: 8.222274780273438
Step: 34560, train/loss: 0.0
Step: 34560, train/grad_norm: 1.0553036418059492e-06
Step: 34560, train/learning_rate: 8.876725587469991e-06
Step: 34560, train/epoch: 8.224655151367188
Step: 34570, train/loss: 0.0
Step: 34570, train/grad_norm: 2.8729025416396325e-06
Step: 34570, train/learning_rate: 8.864826668286696e-06
Step: 34570, train/epoch: 8.227034568786621
Step: 34580, train/loss: 0.0
Step: 34580, train/grad_norm: 2.7272790248389356e-05
Step: 34580, train/learning_rate: 8.852926839608699e-06
Step: 34580, train/epoch: 8.229414939880371
Step: 34590, train/loss: 0.0
Step: 34590, train/grad_norm: 3.169102274114266e-05
Step: 34590, train/learning_rate: 8.841027920425404e-06
Step: 34590, train/epoch: 8.231794357299805
Step: 34600, train/loss: 0.0
Step: 34600, train/grad_norm: 1.0093110631714808e-06
Step: 34600, train/learning_rate: 8.829129001242109e-06
Step: 34600, train/epoch: 8.234173774719238
Step: 34610, train/loss: 0.0
Step: 34610, train/grad_norm: 1.361500721941411e-06
Step: 34610, train/learning_rate: 8.817230082058813e-06
Step: 34610, train/epoch: 8.236554145812988
Step: 34620, train/loss: 0.0
Step: 34620, train/grad_norm: 5.2477760618785396e-05
Step: 34620, train/learning_rate: 8.805331162875518e-06
Step: 34620, train/epoch: 8.238933563232422
Step: 34630, train/loss: 0.0
Step: 34630, train/grad_norm: 4.921495246890117e-07
Step: 34630, train/learning_rate: 8.793431334197521e-06
Step: 34630, train/epoch: 8.241313934326172
Step: 34640, train/loss: 0.0
Step: 34640, train/grad_norm: 0.0002129797503584996
Step: 34640, train/learning_rate: 8.781532415014226e-06
Step: 34640, train/epoch: 8.243693351745605
Step: 34650, train/loss: 0.0
Step: 34650, train/grad_norm: 2.9589759265036264e-07
Step: 34650, train/learning_rate: 8.76963349583093e-06
Step: 34650, train/epoch: 8.246073722839355
Step: 34660, train/loss: 0.0
Step: 34660, train/grad_norm: 1.4730740986124147e-05
Step: 34660, train/learning_rate: 8.757734576647636e-06
Step: 34660, train/epoch: 8.248453140258789
Step: 34670, train/loss: 0.0
Step: 34670, train/grad_norm: 1.4913279301254079e-05
Step: 34670, train/learning_rate: 8.74583565746434e-06
Step: 34670, train/epoch: 8.250832557678223
Step: 34680, train/loss: 0.0
Step: 34680, train/grad_norm: 7.716358595644124e-06
Step: 34680, train/learning_rate: 8.733935828786343e-06
Step: 34680, train/epoch: 8.253212928771973
Step: 34690, train/loss: 0.0
Step: 34690, train/grad_norm: 4.630610987987893e-07
Step: 34690, train/learning_rate: 8.722036909603048e-06
Step: 34690, train/epoch: 8.255592346191406
Step: 34700, train/loss: 0.0
Step: 34700, train/grad_norm: 6.285426366048341e-07
Step: 34700, train/learning_rate: 8.710137990419753e-06
Step: 34700, train/epoch: 8.257972717285156
Step: 34710, train/loss: 0.0
Step: 34710, train/grad_norm: 4.455299404071411e-06
Step: 34710, train/learning_rate: 8.698239071236458e-06
Step: 34710, train/epoch: 8.26035213470459
Step: 34720, train/loss: 0.0
Step: 34720, train/grad_norm: 3.442434172029607e-05
Step: 34720, train/learning_rate: 8.686340152053162e-06
Step: 34720, train/epoch: 8.26273250579834
Step: 34730, train/loss: 0.0
Step: 34730, train/grad_norm: 1.1251445357629564e-06
Step: 34730, train/learning_rate: 8.674440323375165e-06
Step: 34730, train/epoch: 8.265111923217773
Step: 34740, train/loss: 0.0
Step: 34740, train/grad_norm: 4.8203921323874965e-05
Step: 34740, train/learning_rate: 8.66254140419187e-06
Step: 34740, train/epoch: 8.267491340637207
Step: 34750, train/loss: 0.0
Step: 34750, train/grad_norm: 3.057788126170635e-05
Step: 34750, train/learning_rate: 8.650642485008575e-06
Step: 34750, train/epoch: 8.269871711730957
Step: 34760, train/loss: 0.0
Step: 34760, train/grad_norm: 2.650453780006501e-06
Step: 34760, train/learning_rate: 8.63874356582528e-06
Step: 34760, train/epoch: 8.27225112915039
Step: 34770, train/loss: 0.0
Step: 34770, train/grad_norm: 8.803301057014323e-07
Step: 34770, train/learning_rate: 8.626844646641985e-06
Step: 34770, train/epoch: 8.27463150024414
Step: 34780, train/loss: 0.0
Step: 34780, train/grad_norm: 6.283150469243992e-06
Step: 34780, train/learning_rate: 8.614944817963988e-06
Step: 34780, train/epoch: 8.277010917663574
Step: 34790, train/loss: 0.0
Step: 34790, train/grad_norm: 1.1584266985664726e-06
Step: 34790, train/learning_rate: 8.603045898780692e-06
Step: 34790, train/epoch: 8.279390335083008
Step: 34800, train/loss: 0.0
Step: 34800, train/grad_norm: 3.438997009652667e-05
Step: 34800, train/learning_rate: 8.591146979597397e-06
Step: 34800, train/epoch: 8.281770706176758
Step: 34810, train/loss: 0.0
Step: 34810, train/grad_norm: 1.1079977184635936e-06
Step: 34810, train/learning_rate: 8.579248060414102e-06
Step: 34810, train/epoch: 8.284150123596191
Step: 34820, train/loss: 0.0
Step: 34820, train/grad_norm: 1.3754738574789371e-06
Step: 34820, train/learning_rate: 8.567349141230807e-06
Step: 34820, train/epoch: 8.286530494689941
Step: 34830, train/loss: 0.0
Step: 34830, train/grad_norm: 0.00010674391523934901
Step: 34830, train/learning_rate: 8.555450222047511e-06
Step: 34830, train/epoch: 8.288909912109375
Step: 34840, train/loss: 0.0
Step: 34840, train/grad_norm: 5.275254011394281e-07
Step: 34840, train/learning_rate: 8.543550393369514e-06
Step: 34840, train/epoch: 8.291290283203125
Step: 34850, train/loss: 0.0
Step: 34850, train/grad_norm: 8.31603415463178e-07
Step: 34850, train/learning_rate: 8.53165147418622e-06
Step: 34850, train/epoch: 8.293669700622559
Step: 34860, train/loss: 0.0
Step: 34860, train/grad_norm: 2.5922149688994978e-06
Step: 34860, train/learning_rate: 8.519752555002924e-06
Step: 34860, train/epoch: 8.296049118041992
Step: 34870, train/loss: 0.0
Step: 34870, train/grad_norm: 7.002690381341381e-06
Step: 34870, train/learning_rate: 8.507853635819629e-06
Step: 34870, train/epoch: 8.298429489135742
Step: 34880, train/loss: 0.0
Step: 34880, train/grad_norm: 3.384273838946683e-07
Step: 34880, train/learning_rate: 8.495954716636334e-06
Step: 34880, train/epoch: 8.300808906555176
Step: 34890, train/loss: 0.0
Step: 34890, train/grad_norm: 7.610761258547427e-06
Step: 34890, train/learning_rate: 8.484054887958337e-06
Step: 34890, train/epoch: 8.303189277648926
Step: 34900, train/loss: 0.0
Step: 34900, train/grad_norm: 9.229947863786947e-06
Step: 34900, train/learning_rate: 8.472155968775041e-06
Step: 34900, train/epoch: 8.30556869506836
Step: 34910, train/loss: 0.0
Step: 34910, train/grad_norm: 6.949085786800424e-07
Step: 34910, train/learning_rate: 8.460257049591746e-06
Step: 34910, train/epoch: 8.30794906616211
Step: 34920, train/loss: 0.0
Step: 34920, train/grad_norm: 1.8999027133759228e-06
Step: 34920, train/learning_rate: 8.448358130408451e-06
Step: 34920, train/epoch: 8.310328483581543
Step: 34930, train/loss: 0.0
Step: 34930, train/grad_norm: 6.539583864650922e-06
Step: 34930, train/learning_rate: 8.436459211225156e-06
Step: 34930, train/epoch: 8.312707901000977
Step: 34940, train/loss: 0.0
Step: 34940, train/grad_norm: 3.943107458326267e-06
Step: 34940, train/learning_rate: 8.424559382547159e-06
Step: 34940, train/epoch: 8.315088272094727
Step: 34950, train/loss: 0.0
Step: 34950, train/grad_norm: 1.751242598402314e-05
Step: 34950, train/learning_rate: 8.412660463363864e-06
Step: 34950, train/epoch: 8.31746768951416
Step: 34960, train/loss: 0.0
Step: 34960, train/grad_norm: 0.00011931932385778055
Step: 34960, train/learning_rate: 8.400761544180568e-06
Step: 34960, train/epoch: 8.31984806060791
Step: 34970, train/loss: 0.0
Step: 34970, train/grad_norm: 6.18954391029547e-06
Step: 34970, train/learning_rate: 8.388862624997273e-06
Step: 34970, train/epoch: 8.322227478027344
Step: 34980, train/loss: 0.0
Step: 34980, train/grad_norm: 8.674979312672804e-08
Step: 34980, train/learning_rate: 8.376963705813978e-06
Step: 34980, train/epoch: 8.324606895446777
Step: 34990, train/loss: 0.0
Step: 34990, train/grad_norm: 7.702245966356713e-06
Step: 34990, train/learning_rate: 8.365063877135981e-06
Step: 34990, train/epoch: 8.326987266540527
Step: 35000, train/loss: 0.0
Step: 35000, train/grad_norm: 1.306475587625755e-05
Step: 35000, train/learning_rate: 8.353164957952686e-06
Step: 35000, train/epoch: 8.329366683959961
Step: 35010, train/loss: 0.0
Step: 35010, train/grad_norm: 0.00010146340355277061
Step: 35010, train/learning_rate: 8.34126603876939e-06
Step: 35010, train/epoch: 8.331747055053711
Step: 35020, train/loss: 0.0
Step: 35020, train/grad_norm: 3.915676279575564e-05
Step: 35020, train/learning_rate: 8.329367119586095e-06
Step: 35020, train/epoch: 8.334126472473145
Step: 35030, train/loss: 0.0
Step: 35030, train/grad_norm: 1.4695066965941805e-05
Step: 35030, train/learning_rate: 8.3174682004028e-06
Step: 35030, train/epoch: 8.336506843566895
Step: 35040, train/loss: 0.0
Step: 35040, train/grad_norm: 8.518137519786251e-07
Step: 35040, train/learning_rate: 8.305568371724803e-06
Step: 35040, train/epoch: 8.338886260986328
Step: 35050, train/loss: 0.0
Step: 35050, train/grad_norm: 1.5828970845177537e-06
Step: 35050, train/learning_rate: 8.293669452541508e-06
Step: 35050, train/epoch: 8.341265678405762
Step: 35060, train/loss: 0.0
Step: 35060, train/grad_norm: 5.439110736915609e-06
Step: 35060, train/learning_rate: 8.281770533358213e-06
Step: 35060, train/epoch: 8.343646049499512
Step: 35070, train/loss: 0.0
Step: 35070, train/grad_norm: 3.0253179374994943e-06
Step: 35070, train/learning_rate: 8.269871614174917e-06
Step: 35070, train/epoch: 8.346025466918945
Step: 35080, train/loss: 0.0
Step: 35080, train/grad_norm: 1.7747834135661833e-05
Step: 35080, train/learning_rate: 8.257972694991622e-06
Step: 35080, train/epoch: 8.348405838012695
Step: 35090, train/loss: 0.0
Step: 35090, train/grad_norm: 1.692296365263246e-07
Step: 35090, train/learning_rate: 8.246072866313625e-06
Step: 35090, train/epoch: 8.350785255432129
Step: 35100, train/loss: 0.0
Step: 35100, train/grad_norm: 2.0653271349146962e-05
Step: 35100, train/learning_rate: 8.23417394713033e-06
Step: 35100, train/epoch: 8.353165626525879
Step: 35110, train/loss: 0.0
Step: 35110, train/grad_norm: 7.71329553117539e-07
Step: 35110, train/learning_rate: 8.222275027947035e-06
Step: 35110, train/epoch: 8.355545043945312
Step: 35120, train/loss: 0.0
Step: 35120, train/grad_norm: 4.036828613607213e-06
Step: 35120, train/learning_rate: 8.21037610876374e-06
Step: 35120, train/epoch: 8.357924461364746
Step: 35130, train/loss: 0.0
Step: 35130, train/grad_norm: 3.159733978463919e-06
Step: 35130, train/learning_rate: 8.198477189580444e-06
Step: 35130, train/epoch: 8.360304832458496
Step: 35140, train/loss: 0.0
Step: 35140, train/grad_norm: 8.221751158998813e-06
Step: 35140, train/learning_rate: 8.186578270397149e-06
Step: 35140, train/epoch: 8.36268424987793
Step: 35150, train/loss: 0.0
Step: 35150, train/grad_norm: 9.367449251840299e-07
Step: 35150, train/learning_rate: 8.174678441719152e-06
Step: 35150, train/epoch: 8.36506462097168
Step: 35160, train/loss: 0.0
Step: 35160, train/grad_norm: 9.627761983210803e-07
Step: 35160, train/learning_rate: 8.162779522535857e-06
Step: 35160, train/epoch: 8.367444038391113
Step: 35170, train/loss: 0.0
Step: 35170, train/grad_norm: 5.339052222552709e-07
Step: 35170, train/learning_rate: 8.150880603352562e-06
Step: 35170, train/epoch: 8.369823455810547
Step: 35180, train/loss: 0.0
Step: 35180, train/grad_norm: 2.7935824618907645e-05
Step: 35180, train/learning_rate: 8.138981684169266e-06
Step: 35180, train/epoch: 8.372203826904297
Step: 35190, train/loss: 0.012199999764561653
Step: 35190, train/grad_norm: 1.550746128486935e-05
Step: 35190, train/learning_rate: 8.127082764985971e-06
Step: 35190, train/epoch: 8.37458324432373
Step: 35200, train/loss: 0.10939999669790268
Step: 35200, train/grad_norm: 9.000931640912313e-06
Step: 35200, train/learning_rate: 8.115182936307974e-06
Step: 35200, train/epoch: 8.37696361541748
Step: 35210, train/loss: 0.0
Step: 35210, train/grad_norm: 9.65118124440778e-06
Step: 35210, train/learning_rate: 8.103284017124679e-06
Step: 35210, train/epoch: 8.379343032836914
Step: 35220, train/loss: 0.0
Step: 35220, train/grad_norm: 7.7924742072355e-06
Step: 35220, train/learning_rate: 8.091385097941384e-06
Step: 35220, train/epoch: 8.381723403930664
Step: 35230, train/loss: 0.0
Step: 35230, train/grad_norm: 1.0573046438366873e-06
Step: 35230, train/learning_rate: 8.079486178758088e-06
Step: 35230, train/epoch: 8.384102821350098
Step: 35240, train/loss: 0.0
Step: 35240, train/grad_norm: 0.0005929386825300753
Step: 35240, train/learning_rate: 8.067587259574793e-06
Step: 35240, train/epoch: 8.386482238769531
Step: 35250, train/loss: 0.0
Step: 35250, train/grad_norm: 2.089800119620122e-07
Step: 35250, train/learning_rate: 8.055687430896796e-06
Step: 35250, train/epoch: 8.388862609863281
Step: 35260, train/loss: 0.0
Step: 35260, train/grad_norm: 0.0002682922058738768
Step: 35260, train/learning_rate: 8.043788511713501e-06
Step: 35260, train/epoch: 8.391242027282715
Step: 35270, train/loss: 0.0
Step: 35270, train/grad_norm: 7.878273027017713e-05
Step: 35270, train/learning_rate: 8.031889592530206e-06
Step: 35270, train/epoch: 8.393622398376465
Step: 35280, train/loss: 0.0
Step: 35280, train/grad_norm: 1.4354502127389424e-05
Step: 35280, train/learning_rate: 8.01999067334691e-06
Step: 35280, train/epoch: 8.396001815795898
Step: 35290, train/loss: 0.0
Step: 35290, train/grad_norm: 3.6017521779285744e-05
Step: 35290, train/learning_rate: 8.008091754163615e-06
Step: 35290, train/epoch: 8.398382186889648
Step: 35300, train/loss: 0.0
Step: 35300, train/grad_norm: 0.0007343765464611351
Step: 35300, train/learning_rate: 7.996191925485618e-06
Step: 35300, train/epoch: 8.400761604309082
Step: 35310, train/loss: 0.0
Step: 35310, train/grad_norm: 1.059096393873915e-05
Step: 35310, train/learning_rate: 7.984293006302323e-06
Step: 35310, train/epoch: 8.403141021728516
Step: 35320, train/loss: 0.0
Step: 35320, train/grad_norm: 6.561042482644552e-06
Step: 35320, train/learning_rate: 7.972394087119028e-06
Step: 35320, train/epoch: 8.405521392822266
Step: 35330, train/loss: 0.0
Step: 35330, train/grad_norm: 1.4672118595626671e-05
Step: 35330, train/learning_rate: 7.960495167935733e-06
Step: 35330, train/epoch: 8.4079008102417
Step: 35340, train/loss: 0.0
Step: 35340, train/grad_norm: 1.0907003343163524e-05
Step: 35340, train/learning_rate: 7.948596248752438e-06
Step: 35340, train/epoch: 8.41028118133545
Step: 35350, train/loss: 0.0
Step: 35350, train/grad_norm: 1.666337630013004e-05
Step: 35350, train/learning_rate: 7.93669642007444e-06
Step: 35350, train/epoch: 8.412660598754883
Step: 35360, train/loss: 0.0
Step: 35360, train/grad_norm: 5.2249612053856254e-05
Step: 35360, train/learning_rate: 7.924797500891145e-06
Step: 35360, train/epoch: 8.415040016174316
Step: 35370, train/loss: 0.0
Step: 35370, train/grad_norm: 3.436006409174297e-06
Step: 35370, train/learning_rate: 7.91289858170785e-06
Step: 35370, train/epoch: 8.417420387268066
Step: 35380, train/loss: 0.0
Step: 35380, train/grad_norm: 7.8628771007061e-05
Step: 35380, train/learning_rate: 7.900999662524555e-06
Step: 35380, train/epoch: 8.4197998046875
Step: 35390, train/loss: 0.0
Step: 35390, train/grad_norm: 2.3065051209414378e-05
Step: 35390, train/learning_rate: 7.88910074334126e-06
Step: 35390, train/epoch: 8.42218017578125
Step: 35400, train/loss: 0.0
Step: 35400, train/grad_norm: 9.249437425751239e-06
Step: 35400, train/learning_rate: 7.877200914663263e-06
Step: 35400, train/epoch: 8.424559593200684
Step: 35410, train/loss: 0.0
Step: 35410, train/grad_norm: 1.0183051927015185e-05
Step: 35410, train/learning_rate: 7.865301995479967e-06
Step: 35410, train/epoch: 8.426939964294434
Step: 35420, train/loss: 0.0
Step: 35420, train/grad_norm: 5.568129836319713e-06
Step: 35420, train/learning_rate: 7.853403076296672e-06
Step: 35420, train/epoch: 8.429319381713867
Step: 35430, train/loss: 0.0
Step: 35430, train/grad_norm: 6.754032801836729e-05
Step: 35430, train/learning_rate: 7.841504157113377e-06
Step: 35430, train/epoch: 8.4316987991333
Step: 35440, train/loss: 0.0
Step: 35440, train/grad_norm: 9.734503692016006e-06
Step: 35440, train/learning_rate: 7.829605237930082e-06
Step: 35440, train/epoch: 8.43407917022705
Step: 35450, train/loss: 0.0
Step: 35450, train/grad_norm: 3.511340082695824e-06
Step: 35450, train/learning_rate: 7.817705409252085e-06
Step: 35450, train/epoch: 8.436458587646484
Step: 35460, train/loss: 0.0
Step: 35460, train/grad_norm: 2.9171467758715153e-05
Step: 35460, train/learning_rate: 7.80580649006879e-06
Step: 35460, train/epoch: 8.438838958740234
Step: 35470, train/loss: 0.0
Step: 35470, train/grad_norm: 3.138201691399445e-06
Step: 35470, train/learning_rate: 7.793907570885494e-06
Step: 35470, train/epoch: 8.441218376159668
Step: 35480, train/loss: 0.0
Step: 35480, train/grad_norm: 5.3725660109193996e-05
Step: 35480, train/learning_rate: 7.782008651702199e-06
Step: 35480, train/epoch: 8.443598747253418
Step: 35490, train/loss: 0.0
Step: 35490, train/grad_norm: 1.3469359146256465e-05
Step: 35490, train/learning_rate: 7.770109732518904e-06
Step: 35490, train/epoch: 8.445978164672852
Step: 35500, train/loss: 0.0
Step: 35500, train/grad_norm: 0.0001856525195762515
Step: 35500, train/learning_rate: 7.758210813335609e-06
Step: 35500, train/epoch: 8.448357582092285
Step: 35510, train/loss: 0.0
Step: 35510, train/grad_norm: 3.7925954643469595e-07
Step: 35510, train/learning_rate: 7.746310984657612e-06
Step: 35510, train/epoch: 8.450737953186035
Step: 35520, train/loss: 0.0
Step: 35520, train/grad_norm: 5.882662662770599e-06
Step: 35520, train/learning_rate: 7.734412065474316e-06
Step: 35520, train/epoch: 8.453117370605469
Step: 35530, train/loss: 0.0
Step: 35530, train/grad_norm: 6.241751634661341e-06
Step: 35530, train/learning_rate: 7.722513146291021e-06
Step: 35530, train/epoch: 8.455497741699219
Step: 35540, train/loss: 0.0
Step: 35540, train/grad_norm: 4.0027956856647506e-05
Step: 35540, train/learning_rate: 7.710614227107726e-06
Step: 35540, train/epoch: 8.457877159118652
Step: 35550, train/loss: 0.0
Step: 35550, train/grad_norm: 1.0332730198570061e-05
Step: 35550, train/learning_rate: 7.69871530792443e-06
Step: 35550, train/epoch: 8.460256576538086
Step: 35560, train/loss: 0.0
Step: 35560, train/grad_norm: 3.640083741629496e-05
Step: 35560, train/learning_rate: 7.686815479246434e-06
Step: 35560, train/epoch: 8.462636947631836
Step: 35570, train/loss: 0.0
Step: 35570, train/grad_norm: 1.8132028344552964e-05
Step: 35570, train/learning_rate: 7.674916560063139e-06
Step: 35570, train/epoch: 8.46501636505127
Step: 35580, train/loss: 0.0
Step: 35580, train/grad_norm: 9.715128726384137e-06
Step: 35580, train/learning_rate: 7.663017640879843e-06
Step: 35580, train/epoch: 8.46739673614502
Step: 35590, train/loss: 0.0
Step: 35590, train/grad_norm: 6.426478194043739e-07
Step: 35590, train/learning_rate: 7.651118721696548e-06
Step: 35590, train/epoch: 8.469776153564453
Step: 35600, train/loss: 0.0
Step: 35600, train/grad_norm: 1.0954458957712632e-05
Step: 35600, train/learning_rate: 7.639219802513253e-06
Step: 35600, train/epoch: 8.472156524658203
Step: 35610, train/loss: 0.0
Step: 35610, train/grad_norm: 7.345616904785857e-05
Step: 35610, train/learning_rate: 7.627320428582607e-06
Step: 35610, train/epoch: 8.474535942077637
Step: 35620, train/loss: 0.0
Step: 35620, train/grad_norm: 0.00015231013821903616
Step: 35620, train/learning_rate: 7.615421054651961e-06
Step: 35620, train/epoch: 8.47691535949707
Step: 35630, train/loss: 0.0
Step: 35630, train/grad_norm: 7.088665824994678e-06
Step: 35630, train/learning_rate: 7.6035221354686655e-06
Step: 35630, train/epoch: 8.47929573059082
Step: 35640, train/loss: 0.0
Step: 35640, train/grad_norm: 8.249274287663866e-06
Step: 35640, train/learning_rate: 7.59162321628537e-06
Step: 35640, train/epoch: 8.481675148010254
Step: 35650, train/loss: 0.0
Step: 35650, train/grad_norm: 1.1599043318710756e-05
Step: 35650, train/learning_rate: 7.579723842354724e-06
Step: 35650, train/epoch: 8.484055519104004
Step: 35660, train/loss: 0.0
Step: 35660, train/grad_norm: 1.5858824554015882e-05
Step: 35660, train/learning_rate: 7.567824923171429e-06
Step: 35660, train/epoch: 8.486434936523438
Step: 35670, train/loss: 0.0
Step: 35670, train/grad_norm: 9.358732313557994e-06
Step: 35670, train/learning_rate: 7.555925549240783e-06
Step: 35670, train/epoch: 8.488815307617188
Step: 35680, train/loss: 0.0
Step: 35680, train/grad_norm: 3.600211857701652e-05
Step: 35680, train/learning_rate: 7.544026630057488e-06
Step: 35680, train/epoch: 8.491194725036621
Step: 35690, train/loss: 0.0
Step: 35690, train/grad_norm: 1.9816287021967582e-05
Step: 35690, train/learning_rate: 7.532127710874192e-06
Step: 35690, train/epoch: 8.493574142456055
Step: 35700, train/loss: 0.0
Step: 35700, train/grad_norm: 7.975556400197092e-06
Step: 35700, train/learning_rate: 7.520228336943546e-06
Step: 35700, train/epoch: 8.495954513549805
Step: 35710, train/loss: 0.0
Step: 35710, train/grad_norm: 2.319180202903226e-05
Step: 35710, train/learning_rate: 7.508329417760251e-06
Step: 35710, train/epoch: 8.498333930969238
Step: 35720, train/loss: 0.0
Step: 35720, train/grad_norm: 4.138135864195647e-06
Step: 35720, train/learning_rate: 7.496430498576956e-06
Step: 35720, train/epoch: 8.500714302062988
Step: 35730, train/loss: 0.0
Step: 35730, train/grad_norm: 0.0001636842207517475
Step: 35730, train/learning_rate: 7.48453112464631e-06
Step: 35730, train/epoch: 8.503093719482422
Step: 35740, train/loss: 0.0
Step: 35740, train/grad_norm: 1.2363299219941837e-06
Step: 35740, train/learning_rate: 7.4726322054630145e-06
Step: 35740, train/epoch: 8.505473136901855
Step: 35750, train/loss: 0.0
Step: 35750, train/grad_norm: 3.3949884254980134e-06
Step: 35750, train/learning_rate: 7.4607328315323684e-06
Step: 35750, train/epoch: 8.507853507995605
Step: 35760, train/loss: 0.0
Step: 35760, train/grad_norm: 1.5108070329006296e-05
Step: 35760, train/learning_rate: 7.448833912349073e-06
Step: 35760, train/epoch: 8.510232925415039
Step: 35770, train/loss: 0.0
Step: 35770, train/grad_norm: 1.1678383089019917e-05
Step: 35770, train/learning_rate: 7.436934993165778e-06
Step: 35770, train/epoch: 8.512613296508789
Step: 35780, train/loss: 0.0
Step: 35780, train/grad_norm: 8.906754942472617e-07
Step: 35780, train/learning_rate: 7.425035619235132e-06
Step: 35780, train/epoch: 8.514992713928223
Step: 35790, train/loss: 0.0
Step: 35790, train/grad_norm: 1.7313534044660628e-05
Step: 35790, train/learning_rate: 7.413136700051837e-06
Step: 35790, train/epoch: 8.517373085021973
Step: 35800, train/loss: 0.0
Step: 35800, train/grad_norm: 9.579484867572319e-07
Step: 35800, train/learning_rate: 7.4012373261211906e-06
Step: 35800, train/epoch: 8.519752502441406
Step: 35810, train/loss: 0.0
Step: 35810, train/grad_norm: 1.8472135252523003e-06
Step: 35810, train/learning_rate: 7.389338406937895e-06
Step: 35810, train/epoch: 8.52213191986084
Step: 35820, train/loss: 0.0
Step: 35820, train/grad_norm: 3.7229116855996836e-07
Step: 35820, train/learning_rate: 7.3774394877546e-06
Step: 35820, train/epoch: 8.52451229095459
Step: 35830, train/loss: 0.0
Step: 35830, train/grad_norm: 2.1080461010569707e-05
Step: 35830, train/learning_rate: 7.365540113823954e-06
Step: 35830, train/epoch: 8.526891708374023
Step: 35840, train/loss: 0.0
Step: 35840, train/grad_norm: 7.950005169732322e-07
Step: 35840, train/learning_rate: 7.353641194640659e-06
Step: 35840, train/epoch: 8.529272079467773
Step: 35850, train/loss: 0.0
Step: 35850, train/grad_norm: 7.9726360127097e-06
Step: 35850, train/learning_rate: 7.341741820710013e-06
Step: 35850, train/epoch: 8.531651496887207
Step: 35860, train/loss: 0.0
Step: 35860, train/grad_norm: 0.0003689791774377227
Step: 35860, train/learning_rate: 7.3298429015267175e-06
Step: 35860, train/epoch: 8.534031867980957
Step: 35870, train/loss: 0.0
Step: 35870, train/grad_norm: 3.804675543506164e-06
Step: 35870, train/learning_rate: 7.317943982343422e-06
Step: 35870, train/epoch: 8.53641128540039
Step: 35880, train/loss: 0.0
Step: 35880, train/grad_norm: 1.1194713351869723e-06
Step: 35880, train/learning_rate: 7.306044608412776e-06
Step: 35880, train/epoch: 8.538790702819824
Step: 35890, train/loss: 0.0
Step: 35890, train/grad_norm: 3.0960691219661385e-05
Step: 35890, train/learning_rate: 7.294145689229481e-06
Step: 35890, train/epoch: 8.541171073913574
Step: 35900, train/loss: 0.0
Step: 35900, train/grad_norm: 0.0008241812465712428
Step: 35900, train/learning_rate: 7.282246770046186e-06
Step: 35900, train/epoch: 8.543550491333008
Step: 35910, train/loss: 0.0
Step: 35910, train/grad_norm: 8.175080665751011e-07
Step: 35910, train/learning_rate: 7.27034739611554e-06
Step: 35910, train/epoch: 8.545930862426758
Step: 35920, train/loss: 0.0
Step: 35920, train/grad_norm: 2.396734998910688e-06
Step: 35920, train/learning_rate: 7.258448476932244e-06
Step: 35920, train/epoch: 8.548310279846191
Step: 35930, train/loss: 0.0
Step: 35930, train/grad_norm: 6.664900865871459e-05
Step: 35930, train/learning_rate: 7.246549103001598e-06
Step: 35930, train/epoch: 8.550689697265625
Step: 35940, train/loss: 0.0
Step: 35940, train/grad_norm: 5.63001549380715e-07
Step: 35940, train/learning_rate: 7.234650183818303e-06
Step: 35940, train/epoch: 8.553070068359375
Step: 35950, train/loss: 0.0
Step: 35950, train/grad_norm: 3.230740048820735e-06
Step: 35950, train/learning_rate: 7.222751264635008e-06
Step: 35950, train/epoch: 8.555449485778809
Step: 35960, train/loss: 0.0
Step: 35960, train/grad_norm: 1.1039491027986514e-06
Step: 35960, train/learning_rate: 7.210851890704362e-06
Step: 35960, train/epoch: 8.557829856872559
Step: 35970, train/loss: 0.0
Step: 35970, train/grad_norm: 3.619380004238337e-05
Step: 35970, train/learning_rate: 7.1989529715210665e-06
Step: 35970, train/epoch: 8.560209274291992
Step: 35980, train/loss: 0.0
Step: 35980, train/grad_norm: 3.0234739369916497e-06
Step: 35980, train/learning_rate: 7.18705359759042e-06
Step: 35980, train/epoch: 8.562589645385742
Step: 35990, train/loss: 0.0
Step: 35990, train/grad_norm: 7.850795782360365e-07
Step: 35990, train/learning_rate: 7.175154678407125e-06
Step: 35990, train/epoch: 8.564969062805176
Step: 36000, train/loss: 0.0
Step: 36000, train/grad_norm: 2.7108269932796247e-05
Step: 36000, train/learning_rate: 7.16325575922383e-06
Step: 36000, train/epoch: 8.56734848022461
Step: 36010, train/loss: 0.0
Step: 36010, train/grad_norm: 5.380608854466118e-05
Step: 36010, train/learning_rate: 7.151356385293184e-06
Step: 36010, train/epoch: 8.56972885131836
Step: 36020, train/loss: 0.0
Step: 36020, train/grad_norm: 2.7548464913706994e-06
Step: 36020, train/learning_rate: 7.139457466109889e-06
Step: 36020, train/epoch: 8.572108268737793
Step: 36030, train/loss: 0.0
Step: 36030, train/grad_norm: 3.878801635437412e-06
Step: 36030, train/learning_rate: 7.1275580921792425e-06
Step: 36030, train/epoch: 8.574488639831543
Step: 36040, train/loss: 0.0
Step: 36040, train/grad_norm: 5.683734343620017e-06
Step: 36040, train/learning_rate: 7.115659172995947e-06
Step: 36040, train/epoch: 8.576868057250977
Step: 36050, train/loss: 0.0
Step: 36050, train/grad_norm: 1.4139816812530626e-05
Step: 36050, train/learning_rate: 7.103760253812652e-06
Step: 36050, train/epoch: 8.579248428344727
Step: 36060, train/loss: 0.0
Step: 36060, train/grad_norm: 6.140536470411462e-07
Step: 36060, train/learning_rate: 7.091860879882006e-06
Step: 36060, train/epoch: 8.58162784576416
Step: 36070, train/loss: 0.0
Step: 36070, train/grad_norm: 8.06516240459132e-08
Step: 36070, train/learning_rate: 7.079961960698711e-06
Step: 36070, train/epoch: 8.584007263183594
Step: 36080, train/loss: 0.0
Step: 36080, train/grad_norm: 4.055128556501586e-06
Step: 36080, train/learning_rate: 7.0680630415154155e-06
Step: 36080, train/epoch: 8.586387634277344
Step: 36090, train/loss: 0.0
Step: 36090, train/grad_norm: 3.901298896380467e-06
Step: 36090, train/learning_rate: 7.0561636675847694e-06
Step: 36090, train/epoch: 8.588767051696777
Step: 36100, train/loss: 0.0
Step: 36100, train/grad_norm: 1.7857527154774289e-06
Step: 36100, train/learning_rate: 7.044264748401474e-06
Step: 36100, train/epoch: 8.591147422790527
Step: 36110, train/loss: 0.0
Step: 36110, train/grad_norm: 2.2610143446399888e-07
Step: 36110, train/learning_rate: 7.032365374470828e-06
Step: 36110, train/epoch: 8.593526840209961
Step: 36120, train/loss: 0.0
Step: 36120, train/grad_norm: 1.0059126225314685e-06
Step: 36120, train/learning_rate: 7.020466455287533e-06
Step: 36120, train/epoch: 8.595906257629395
Step: 36130, train/loss: 0.0
Step: 36130, train/grad_norm: 0.00010532014857744798
Step: 36130, train/learning_rate: 7.008567536104238e-06
Step: 36130, train/epoch: 8.598286628723145
Step: 36140, train/loss: 0.0
Step: 36140, train/grad_norm: 4.0376676224695984e-06
Step: 36140, train/learning_rate: 6.9966681621735916e-06
Step: 36140, train/epoch: 8.600666046142578
Step: 36150, train/loss: 0.0
Step: 36150, train/grad_norm: 1.912867810460739e-05
Step: 36150, train/learning_rate: 6.984769242990296e-06
Step: 36150, train/epoch: 8.603046417236328
Step: 36160, train/loss: 0.0
Step: 36160, train/grad_norm: 5.340593816072214e-06
Step: 36160, train/learning_rate: 6.97286986905965e-06
Step: 36160, train/epoch: 8.605425834655762
Step: 36170, train/loss: 0.0
Step: 36170, train/grad_norm: 1.2171683465567185e-06
Step: 36170, train/learning_rate: 6.960970949876355e-06
Step: 36170, train/epoch: 8.607806205749512
Step: 36180, train/loss: 0.0
Step: 36180, train/grad_norm: 1.8014655722708994e-07
Step: 36180, train/learning_rate: 6.94907203069306e-06
Step: 36180, train/epoch: 8.610185623168945
Step: 36190, train/loss: 0.0
Step: 36190, train/grad_norm: 3.349342932779109e-06
Step: 36190, train/learning_rate: 6.937172656762414e-06
Step: 36190, train/epoch: 8.612565040588379
Step: 36200, train/loss: 0.0
Step: 36200, train/grad_norm: 1.713383426249493e-05
Step: 36200, train/learning_rate: 6.9252737375791185e-06
Step: 36200, train/epoch: 8.614945411682129
Step: 36210, train/loss: 0.0
Step: 36210, train/grad_norm: 1.3041252486800659e-06
Step: 36210, train/learning_rate: 6.913374363648472e-06
Step: 36210, train/epoch: 8.617324829101562
Step: 36220, train/loss: 0.0
Step: 36220, train/grad_norm: 3.2245511647488456e-06
Step: 36220, train/learning_rate: 6.901475444465177e-06
Step: 36220, train/epoch: 8.619705200195312
Step: 36230, train/loss: 0.0
Step: 36230, train/grad_norm: 1.898755641605021e-07
Step: 36230, train/learning_rate: 6.889576525281882e-06
Step: 36230, train/epoch: 8.622084617614746
Step: 36240, train/loss: 0.0
Step: 36240, train/grad_norm: 2.6201189484709175e-07
Step: 36240, train/learning_rate: 6.877677151351236e-06
Step: 36240, train/epoch: 8.624464988708496
Step: 36250, train/loss: 0.0
Step: 36250, train/grad_norm: 3.490067911116057e-06
Step: 36250, train/learning_rate: 6.865778232167941e-06
Step: 36250, train/epoch: 8.62684440612793
Step: 36260, train/loss: 0.0
Step: 36260, train/grad_norm: 4.510944927460514e-05
Step: 36260, train/learning_rate: 6.853879312984645e-06
Step: 36260, train/epoch: 8.629223823547363
Step: 36270, train/loss: 0.0
Step: 36270, train/grad_norm: 2.8987429686821997e-05
Step: 36270, train/learning_rate: 6.841979939053999e-06
Step: 36270, train/epoch: 8.631604194641113
Step: 36280, train/loss: 0.0
Step: 36280, train/grad_norm: 3.9540822172057233e-07
Step: 36280, train/learning_rate: 6.830081019870704e-06
Step: 36280, train/epoch: 8.633983612060547
Step: 36290, train/loss: 0.0
Step: 36290, train/grad_norm: 7.288735105248634e-06
Step: 36290, train/learning_rate: 6.818181645940058e-06
Step: 36290, train/epoch: 8.636363983154297
Step: 36300, train/loss: 0.0
Step: 36300, train/grad_norm: 9.813425094762351e-07
Step: 36300, train/learning_rate: 6.806282726756763e-06
Step: 36300, train/epoch: 8.63874340057373
Step: 36310, train/loss: 0.0
Step: 36310, train/grad_norm: 1.389114686389803e-06
Step: 36310, train/learning_rate: 6.7943838075734675e-06
Step: 36310, train/epoch: 8.641122817993164
Step: 36320, train/loss: 0.0
Step: 36320, train/grad_norm: 3.709688826347701e-05
Step: 36320, train/learning_rate: 6.782484433642821e-06
Step: 36320, train/epoch: 8.643503189086914
Step: 36330, train/loss: 0.0
Step: 36330, train/grad_norm: 1.0019107321568299e-07
Step: 36330, train/learning_rate: 6.770585514459526e-06
Step: 36330, train/epoch: 8.645882606506348
Step: 36340, train/loss: 0.0
Step: 36340, train/grad_norm: 2.5276129235862754e-05
Step: 36340, train/learning_rate: 6.75868614052888e-06
Step: 36340, train/epoch: 8.648262977600098
Step: 36350, train/loss: 0.0
Step: 36350, train/grad_norm: 1.340395556326257e-05
Step: 36350, train/learning_rate: 6.746787221345585e-06
Step: 36350, train/epoch: 8.650642395019531
Step: 36360, train/loss: 0.0
Step: 36360, train/grad_norm: 1.705323143141868e-06
Step: 36360, train/learning_rate: 6.73488830216229e-06
Step: 36360, train/epoch: 8.653022766113281
Step: 36370, train/loss: 0.0
Step: 36370, train/grad_norm: 2.0650629721785663e-06
Step: 36370, train/learning_rate: 6.7229889282316435e-06
Step: 36370, train/epoch: 8.655402183532715
Step: 36380, train/loss: 0.0
Step: 36380, train/grad_norm: 3.5922207075600454e-07
Step: 36380, train/learning_rate: 6.711090009048348e-06
Step: 36380, train/epoch: 8.657781600952148
Step: 36390, train/loss: 0.0
Step: 36390, train/grad_norm: 0.00028488365933299065
Step: 36390, train/learning_rate: 6.699190635117702e-06
Step: 36390, train/epoch: 8.660161972045898
Step: 36400, train/loss: 0.0
Step: 36400, train/grad_norm: 5.090455488243606e-07
Step: 36400, train/learning_rate: 6.687291715934407e-06
Step: 36400, train/epoch: 8.662541389465332
Step: 36410, train/loss: 0.0
Step: 36410, train/grad_norm: 3.714306149049662e-05
Step: 36410, train/learning_rate: 6.675392796751112e-06
Step: 36410, train/epoch: 8.664921760559082
Step: 36420, train/loss: 0.0
Step: 36420, train/grad_norm: 6.663412023044657e-06
Step: 36420, train/learning_rate: 6.663493422820466e-06
Step: 36420, train/epoch: 8.667301177978516
Step: 36430, train/loss: 0.002400000113993883
Step: 36430, train/grad_norm: 85.01039123535156
Step: 36430, train/learning_rate: 6.65159450363717e-06
Step: 36430, train/epoch: 8.669681549072266
Step: 36440, train/loss: 0.0
Step: 36440, train/grad_norm: 5.611615506495582e-06
Step: 36440, train/learning_rate: 6.639695584453875e-06
Step: 36440, train/epoch: 8.6720609664917
Step: 36450, train/loss: 0.0
Step: 36450, train/grad_norm: 4.0815975808072835e-06
Step: 36450, train/learning_rate: 6.627796210523229e-06
Step: 36450, train/epoch: 8.674440383911133
Step: 36460, train/loss: 0.008999999612569809
Step: 36460, train/grad_norm: 8.093813903542468e-07
Step: 36460, train/learning_rate: 6.615897291339934e-06
Step: 36460, train/epoch: 8.676820755004883
Step: 36470, train/loss: 0.0
Step: 36470, train/grad_norm: 1.4915810879756464e-06
Step: 36470, train/learning_rate: 6.603997917409288e-06
Step: 36470, train/epoch: 8.679200172424316
Step: 36480, train/loss: 0.0
Step: 36480, train/grad_norm: 1.546359499116079e-06
Step: 36480, train/learning_rate: 6.5920989982259925e-06
Step: 36480, train/epoch: 8.681580543518066
Step: 36490, train/loss: 0.0
Step: 36490, train/grad_norm: 1.6218589735217392e-05
Step: 36490, train/learning_rate: 6.580200079042697e-06
Step: 36490, train/epoch: 8.6839599609375
Step: 36500, train/loss: 0.0
Step: 36500, train/grad_norm: 1.1362887164523272e-07
Step: 36500, train/learning_rate: 6.568300705112051e-06
Step: 36500, train/epoch: 8.686339378356934
Step: 36510, train/loss: 0.0
Step: 36510, train/grad_norm: 2.189723545598099e-06
Step: 36510, train/learning_rate: 6.556401785928756e-06
Step: 36510, train/epoch: 8.688719749450684
Step: 36520, train/loss: 0.0
Step: 36520, train/grad_norm: 1.0899619837800856e-06
Step: 36520, train/learning_rate: 6.54450241199811e-06
Step: 36520, train/epoch: 8.691099166870117
Step: 36530, train/loss: 0.0
Step: 36530, train/grad_norm: 6.482623120973585e-06
Step: 36530, train/learning_rate: 6.532603492814815e-06
Step: 36530, train/epoch: 8.693479537963867
Step: 36540, train/loss: 0.0
Step: 36540, train/grad_norm: 5.945060479461972e-07
Step: 36540, train/learning_rate: 6.5207045736315195e-06
Step: 36540, train/epoch: 8.6958589553833
Step: 36550, train/loss: 0.0
Step: 36550, train/grad_norm: 9.387645150127355e-07
Step: 36550, train/learning_rate: 6.508805199700873e-06
Step: 36550, train/epoch: 8.69823932647705
Step: 36560, train/loss: 0.0
Step: 36560, train/grad_norm: 1.740481536671723e-07
Step: 36560, train/learning_rate: 6.496906280517578e-06
Step: 36560, train/epoch: 8.700618743896484
Step: 36570, train/loss: 0.0
Step: 36570, train/grad_norm: 7.680744715798937e-07
Step: 36570, train/learning_rate: 6.485007361334283e-06
Step: 36570, train/epoch: 8.702998161315918
Step: 36580, train/loss: 0.0
Step: 36580, train/grad_norm: 4.320934294810286e-06
Step: 36580, train/learning_rate: 6.473107987403637e-06
Step: 36580, train/epoch: 8.705378532409668
Step: 36590, train/loss: 0.0
Step: 36590, train/grad_norm: 5.2110248361714184e-05
Step: 36590, train/learning_rate: 6.461209068220342e-06
Step: 36590, train/epoch: 8.707757949829102
Step: 36600, train/loss: 0.0
Step: 36600, train/grad_norm: 3.3434187685088546e-07
Step: 36600, train/learning_rate: 6.4493096942896955e-06
Step: 36600, train/epoch: 8.710138320922852
Step: 36610, train/loss: 0.0
Step: 36610, train/grad_norm: 1.2782155863533262e-05
Step: 36610, train/learning_rate: 6.4374107751064e-06
Step: 36610, train/epoch: 8.712517738342285
Step: 36620, train/loss: 0.0
Step: 36620, train/grad_norm: 2.8007484615955036e-06
Step: 36620, train/learning_rate: 6.425511855923105e-06
Step: 36620, train/epoch: 8.714898109436035
Step: 36630, train/loss: 0.0
Step: 36630, train/grad_norm: 7.125751153580495e-07
Step: 36630, train/learning_rate: 6.413612481992459e-06
Step: 36630, train/epoch: 8.717277526855469
Step: 36640, train/loss: 0.0
Step: 36640, train/grad_norm: 1.1795863201768952e-06
Step: 36640, train/learning_rate: 6.401713562809164e-06
Step: 36640, train/epoch: 8.719656944274902
Step: 36650, train/loss: 0.0
Step: 36650, train/grad_norm: 1.706525836198125e-07
Step: 36650, train/learning_rate: 6.389814188878518e-06
Step: 36650, train/epoch: 8.722037315368652
Step: 36660, train/loss: 0.0
Step: 36660, train/grad_norm: 6.90887020482478e-07
Step: 36660, train/learning_rate: 6.377915269695222e-06
Step: 36660, train/epoch: 8.724416732788086
Step: 36670, train/loss: 0.0
Step: 36670, train/grad_norm: 3.7877008196574025e-08
Step: 36670, train/learning_rate: 6.366016350511927e-06
Step: 36670, train/epoch: 8.726797103881836
Step: 36680, train/loss: 0.0
Step: 36680, train/grad_norm: 4.211724444758147e-06
Step: 36680, train/learning_rate: 6.354116976581281e-06
Step: 36680, train/epoch: 8.72917652130127
Step: 36690, train/loss: 0.0
Step: 36690, train/grad_norm: 4.6714404788872343e-07
Step: 36690, train/learning_rate: 6.342218057397986e-06
Step: 36690, train/epoch: 8.731555938720703
Step: 36700, train/loss: 0.0
Step: 36700, train/grad_norm: 1.1863680526857934e-07
Step: 36700, train/learning_rate: 6.33031868346734e-06
Step: 36700, train/epoch: 8.733936309814453
Step: 36710, train/loss: 0.0
Step: 36710, train/grad_norm: 2.274181269967812e-07
Step: 36710, train/learning_rate: 6.3184197642840445e-06
Step: 36710, train/epoch: 8.736315727233887
Step: 36720, train/loss: 0.0
Step: 36720, train/grad_norm: 7.199127480816969e-07
Step: 36720, train/learning_rate: 6.306520845100749e-06
Step: 36720, train/epoch: 8.738696098327637
Step: 36730, train/loss: 0.0
Step: 36730, train/grad_norm: 1.2758951584146416e-07
Step: 36730, train/learning_rate: 6.294621471170103e-06
Step: 36730, train/epoch: 8.74107551574707
Step: 36740, train/loss: 0.0
Step: 36740, train/grad_norm: 4.147704657953e-06
Step: 36740, train/learning_rate: 6.282722551986808e-06
Step: 36740, train/epoch: 8.74345588684082
Step: 36750, train/loss: 0.0
Step: 36750, train/grad_norm: 2.9196642117312877e-06
Step: 36750, train/learning_rate: 6.270823632803513e-06
Step: 36750, train/epoch: 8.745835304260254
Step: 36760, train/loss: 0.0
Step: 36760, train/grad_norm: 4.140059672863572e-07
Step: 36760, train/learning_rate: 6.258924258872867e-06
Step: 36760, train/epoch: 8.748214721679688
Step: 36770, train/loss: 0.0
Step: 36770, train/grad_norm: 4.3264137161713734e-07
Step: 36770, train/learning_rate: 6.247025339689571e-06
Step: 36770, train/epoch: 8.750595092773438
Step: 36780, train/loss: 0.0
Step: 36780, train/grad_norm: 8.920581421989482e-06
Step: 36780, train/learning_rate: 6.235125965758925e-06
Step: 36780, train/epoch: 8.752974510192871
Step: 36790, train/loss: 0.0
Step: 36790, train/grad_norm: 1.0821586471365663e-07
Step: 36790, train/learning_rate: 6.22322704657563e-06
Step: 36790, train/epoch: 8.755354881286621
Step: 36800, train/loss: 0.0
Step: 36800, train/grad_norm: 1.2191704001907055e-07
Step: 36800, train/learning_rate: 6.211328127392335e-06
Step: 36800, train/epoch: 8.757734298706055
Step: 36810, train/loss: 0.0
Step: 36810, train/grad_norm: 7.734367386547092e-07
Step: 36810, train/learning_rate: 6.199428753461689e-06
Step: 36810, train/epoch: 8.760114669799805
Step: 36820, train/loss: 0.0
Step: 36820, train/grad_norm: 6.310377216323104e-07
Step: 36820, train/learning_rate: 6.1875298342783935e-06
Step: 36820, train/epoch: 8.762494087219238
Step: 36830, train/loss: 0.0
Step: 36830, train/grad_norm: 7.548489975306438e-07
Step: 36830, train/learning_rate: 6.1756304603477474e-06
Step: 36830, train/epoch: 8.764873504638672
Step: 36840, train/loss: 0.0
Step: 36840, train/grad_norm: 1.1124716365884524e-06
Step: 36840, train/learning_rate: 6.163731541164452e-06
Step: 36840, train/epoch: 8.767253875732422
Step: 36850, train/loss: 0.0
Step: 36850, train/grad_norm: 3.0373601589417376e-07
Step: 36850, train/learning_rate: 6.151832621981157e-06
Step: 36850, train/epoch: 8.769633293151855
Step: 36860, train/loss: 0.1242000013589859
Step: 36860, train/grad_norm: 3.393976612642291e-06
Step: 36860, train/learning_rate: 6.139933248050511e-06
Step: 36860, train/epoch: 8.772013664245605
Step: 36870, train/loss: 0.0
Step: 36870, train/grad_norm: 4.362389154266566e-05
Step: 36870, train/learning_rate: 6.128034328867216e-06
Step: 36870, train/epoch: 8.774393081665039
Step: 36880, train/loss: 0.0
Step: 36880, train/grad_norm: 0.0006434028618969023
Step: 36880, train/learning_rate: 6.1161349549365696e-06
Step: 36880, train/epoch: 8.776772499084473
Step: 36890, train/loss: 0.0
Step: 36890, train/grad_norm: 7.00186938047409e-05
Step: 36890, train/learning_rate: 6.104236035753274e-06
Step: 36890, train/epoch: 8.779152870178223
Step: 36900, train/loss: 0.0
Step: 36900, train/grad_norm: 3.767579983104952e-05
Step: 36900, train/learning_rate: 6.092337116569979e-06
Step: 36900, train/epoch: 8.781532287597656
Step: 36910, train/loss: 0.0
Step: 36910, train/grad_norm: 5.969626727164723e-05
Step: 36910, train/learning_rate: 6.080437742639333e-06
Step: 36910, train/epoch: 8.783912658691406
Step: 36920, train/loss: 0.0
Step: 36920, train/grad_norm: 3.9402857510140166e-06
Step: 36920, train/learning_rate: 6.068538823456038e-06
Step: 36920, train/epoch: 8.78629207611084
Step: 36930, train/loss: 0.0
Step: 36930, train/grad_norm: 6.3525253608531784e-06
Step: 36930, train/learning_rate: 6.0566399042727426e-06
Step: 36930, train/epoch: 8.78867244720459
Step: 36940, train/loss: 0.0
Step: 36940, train/grad_norm: 6.651051080552861e-05
Step: 36940, train/learning_rate: 6.0447405303420965e-06
Step: 36940, train/epoch: 8.791051864624023
Step: 36950, train/loss: 0.0
Step: 36950, train/grad_norm: 0.00010481841309228912
Step: 36950, train/learning_rate: 6.032841611158801e-06
Step: 36950, train/epoch: 8.793431282043457
Step: 36960, train/loss: 0.0
Step: 36960, train/grad_norm: 0.0001786959037417546
Step: 36960, train/learning_rate: 6.020942237228155e-06
Step: 36960, train/epoch: 8.795811653137207
Step: 36970, train/loss: 0.0
Step: 36970, train/grad_norm: 1.1056414223276079e-05
Step: 36970, train/learning_rate: 6.00904331804486e-06
Step: 36970, train/epoch: 8.79819107055664
Step: 36980, train/loss: 0.0
Step: 36980, train/grad_norm: 3.0204291761037894e-05
Step: 36980, train/learning_rate: 5.997144398861565e-06
Step: 36980, train/epoch: 8.80057144165039
Step: 36990, train/loss: 0.0
Step: 36990, train/grad_norm: 0.0003288408333901316
Step: 36990, train/learning_rate: 5.985245024930919e-06
Step: 36990, train/epoch: 8.802950859069824
Step: 37000, train/loss: 0.0
Step: 37000, train/grad_norm: 3.0116641937638633e-05
Step: 37000, train/learning_rate: 5.973346105747623e-06
Step: 37000, train/epoch: 8.805331230163574
Step: 37010, train/loss: 0.0
Step: 37010, train/grad_norm: 1.8017717593465932e-05
Step: 37010, train/learning_rate: 5.961446731816977e-06
Step: 37010, train/epoch: 8.807710647583008
Step: 37020, train/loss: 0.0
Step: 37020, train/grad_norm: 2.8949858460691758e-05
Step: 37020, train/learning_rate: 5.949547812633682e-06
Step: 37020, train/epoch: 8.810090065002441
Step: 37030, train/loss: 0.0
Step: 37030, train/grad_norm: 5.031318505643867e-05
Step: 37030, train/learning_rate: 5.937648893450387e-06
Step: 37030, train/epoch: 8.812470436096191
Step: 37040, train/loss: 0.0
Step: 37040, train/grad_norm: 7.657858986931387e-06
Step: 37040, train/learning_rate: 5.925749519519741e-06
Step: 37040, train/epoch: 8.814849853515625
Step: 37050, train/loss: 0.0
Step: 37050, train/grad_norm: 2.3110997062758543e-05
Step: 37050, train/learning_rate: 5.9138506003364455e-06
Step: 37050, train/epoch: 8.817230224609375
Step: 37060, train/loss: 0.0
Step: 37060, train/grad_norm: 0.0006892826058901846
Step: 37060, train/learning_rate: 5.901951226405799e-06
Step: 37060, train/epoch: 8.819609642028809
Step: 37070, train/loss: 0.0
Step: 37070, train/grad_norm: 0.00021836870291735977
Step: 37070, train/learning_rate: 5.890052307222504e-06
Step: 37070, train/epoch: 8.821989059448242
Step: 37080, train/loss: 0.0
Step: 37080, train/grad_norm: 3.118213498964906e-05
Step: 37080, train/learning_rate: 5.878153388039209e-06
Step: 37080, train/epoch: 8.824369430541992
Step: 37090, train/loss: 0.0
Step: 37090, train/grad_norm: 1.1176069165230729e-05
Step: 37090, train/learning_rate: 5.866254014108563e-06
Step: 37090, train/epoch: 8.826748847961426
Step: 37100, train/loss: 0.0
Step: 37100, train/grad_norm: 1.064731077349279e-05
Step: 37100, train/learning_rate: 5.854355094925268e-06
Step: 37100, train/epoch: 8.829129219055176
Step: 37110, train/loss: 0.0
Step: 37110, train/grad_norm: 9.44503062783042e-06
Step: 37110, train/learning_rate: 5.842456175741972e-06
Step: 37110, train/epoch: 8.83150863647461
Step: 37120, train/loss: 0.0
Step: 37120, train/grad_norm: 0.0005006470601074398
Step: 37120, train/learning_rate: 5.830556801811326e-06
Step: 37120, train/epoch: 8.83388900756836
Step: 37130, train/loss: 0.0
Step: 37130, train/grad_norm: 4.1854796108964365e-06
Step: 37130, train/learning_rate: 5.818657882628031e-06
Step: 37130, train/epoch: 8.836268424987793
Step: 37140, train/loss: 0.0
Step: 37140, train/grad_norm: 5.886857252335176e-06
Step: 37140, train/learning_rate: 5.806758508697385e-06
Step: 37140, train/epoch: 8.838647842407227
Step: 37150, train/loss: 0.0
Step: 37150, train/grad_norm: 7.126851414795965e-06
Step: 37150, train/learning_rate: 5.79485958951409e-06
Step: 37150, train/epoch: 8.841028213500977
Step: 37160, train/loss: 0.0
Step: 37160, train/grad_norm: 6.597190804313868e-05
Step: 37160, train/learning_rate: 5.7829606703307945e-06
Step: 37160, train/epoch: 8.84340763092041
Step: 37170, train/loss: 0.0
Step: 37170, train/grad_norm: 5.125176176079549e-05
Step: 37170, train/learning_rate: 5.771061296400148e-06
Step: 37170, train/epoch: 8.84578800201416
Step: 37180, train/loss: 0.0
Step: 37180, train/grad_norm: 0.00010041170753538609
Step: 37180, train/learning_rate: 5.759162377216853e-06
Step: 37180, train/epoch: 8.848167419433594
Step: 37190, train/loss: 0.0
Step: 37190, train/grad_norm: 0.00021488338825292885
Step: 37190, train/learning_rate: 5.747263003286207e-06
Step: 37190, train/epoch: 8.850547790527344
Step: 37200, train/loss: 0.0
Step: 37200, train/grad_norm: 7.75478474679403e-06
Step: 37200, train/learning_rate: 5.735364084102912e-06
Step: 37200, train/epoch: 8.852927207946777
Step: 37210, train/loss: 0.0
Step: 37210, train/grad_norm: 6.545187261508545e-06
Step: 37210, train/learning_rate: 5.723465164919617e-06
Step: 37210, train/epoch: 8.855306625366211
Step: 37220, train/loss: 0.0
Step: 37220, train/grad_norm: 3.950784503103932e-06
Step: 37220, train/learning_rate: 5.7115657909889705e-06
Step: 37220, train/epoch: 8.857686996459961
Step: 37230, train/loss: 0.0
Step: 37230, train/grad_norm: 0.0007230272749438882
Step: 37230, train/learning_rate: 5.699666871805675e-06
Step: 37230, train/epoch: 8.860066413879395
Step: 37240, train/loss: 0.0
Step: 37240, train/grad_norm: 0.000635040458291769
Step: 37240, train/learning_rate: 5.68776795262238e-06
Step: 37240, train/epoch: 8.862446784973145
Step: 37250, train/loss: 0.0
Step: 37250, train/grad_norm: 7.466164970537648e-05
Step: 37250, train/learning_rate: 5.675868578691734e-06
Step: 37250, train/epoch: 8.864826202392578
Step: 37260, train/loss: 0.0
Step: 37260, train/grad_norm: 4.9492799007566646e-05
Step: 37260, train/learning_rate: 5.663969659508439e-06
Step: 37260, train/epoch: 8.867205619812012
Step: 37270, train/loss: 0.0
Step: 37270, train/grad_norm: 0.00014395487960428
Step: 37270, train/learning_rate: 5.652070285577793e-06
Step: 37270, train/epoch: 8.869585990905762
Step: 37280, train/loss: 0.0026000000070780516
Step: 37280, train/grad_norm: 9.546717592456844e-06
Step: 37280, train/learning_rate: 5.6401713663944975e-06
Step: 37280, train/epoch: 8.871965408325195
Step: 37290, train/loss: 0.0
Step: 37290, train/grad_norm: 2.7538067115528975e-07
Step: 37290, train/learning_rate: 5.628272447211202e-06
Step: 37290, train/epoch: 8.874345779418945
Step: 37300, train/loss: 0.0
Step: 37300, train/grad_norm: 6.264724652282894e-05
Step: 37300, train/learning_rate: 5.616373073280556e-06
Step: 37300, train/epoch: 8.876725196838379
Step: 37310, train/loss: 0.0
Step: 37310, train/grad_norm: 0.00022773932141717523
Step: 37310, train/learning_rate: 5.604474154097261e-06
Step: 37310, train/epoch: 8.879105567932129
Step: 37320, train/loss: 0.0
Step: 37320, train/grad_norm: 1.7156362446257845e-05
Step: 37320, train/learning_rate: 5.592574780166615e-06
Step: 37320, train/epoch: 8.881484985351562
Step: 37330, train/loss: 0.0
Step: 37330, train/grad_norm: 0.0014085604343563318
Step: 37330, train/learning_rate: 5.58067586098332e-06
Step: 37330, train/epoch: 8.883864402770996
Step: 37340, train/loss: 0.0
Step: 37340, train/grad_norm: 7.64245487516746e-06
Step: 37340, train/learning_rate: 5.568776941800024e-06
Step: 37340, train/epoch: 8.886244773864746
Step: 37350, train/loss: 0.0
Step: 37350, train/grad_norm: 6.794522050768137e-05
Step: 37350, train/learning_rate: 5.556877567869378e-06
Step: 37350, train/epoch: 8.88862419128418
Step: 37360, train/loss: 0.0010000000474974513
Step: 37360, train/grad_norm: 2.670101275725756e-05
Step: 37360, train/learning_rate: 5.544978648686083e-06
Step: 37360, train/epoch: 8.89100456237793
Step: 37370, train/loss: 0.0
Step: 37370, train/grad_norm: 1.0245514658890897e-06
Step: 37370, train/learning_rate: 5.533079274755437e-06
Step: 37370, train/epoch: 8.893383979797363
Step: 37380, train/loss: 0.0
Step: 37380, train/grad_norm: 2.074655327533037e-07
Step: 37380, train/learning_rate: 5.521180355572142e-06
Step: 37380, train/epoch: 8.895764350891113
Step: 37390, train/loss: 0.0
Step: 37390, train/grad_norm: 6.970954927965067e-06
Step: 37390, train/learning_rate: 5.5092814363888465e-06
Step: 37390, train/epoch: 8.898143768310547
Step: 37400, train/loss: 0.0
Step: 37400, train/grad_norm: 5.672624752151023e-07
Step: 37400, train/learning_rate: 5.4973820624582e-06
Step: 37400, train/epoch: 8.90052318572998
Step: 37410, train/loss: 0.0
Step: 37410, train/grad_norm: 2.2141384761198424e-05
Step: 37410, train/learning_rate: 5.485483143274905e-06
Step: 37410, train/epoch: 8.90290355682373
Step: 37420, train/loss: 0.0
Step: 37420, train/grad_norm: 1.2579081385410973e-06
Step: 37420, train/learning_rate: 5.47358422409161e-06
Step: 37420, train/epoch: 8.905282974243164
Step: 37430, train/loss: 0.0
Step: 37430, train/grad_norm: 1.670235064921144e-06
Step: 37430, train/learning_rate: 5.461684850160964e-06
Step: 37430, train/epoch: 8.907663345336914
Step: 37440, train/loss: 0.0
Step: 37440, train/grad_norm: 8.822308882372454e-05
Step: 37440, train/learning_rate: 5.449785930977669e-06
Step: 37440, train/epoch: 8.910042762756348
Step: 37450, train/loss: 0.0
Step: 37450, train/grad_norm: 1.636072738619987e-05
Step: 37450, train/learning_rate: 5.4378865570470225e-06
Step: 37450, train/epoch: 8.912422180175781
Step: 37460, train/loss: 0.0
Step: 37460, train/grad_norm: 1.8553595282355673e-07
Step: 37460, train/learning_rate: 5.425987637863727e-06
Step: 37460, train/epoch: 8.914802551269531
Step: 37470, train/loss: 0.0
Step: 37470, train/grad_norm: 2.464860699546989e-05
Step: 37470, train/learning_rate: 5.414088718680432e-06
Step: 37470, train/epoch: 8.917181968688965
Step: 37480, train/loss: 0.0
Step: 37480, train/grad_norm: 3.856635885313153e-06
Step: 37480, train/learning_rate: 5.402189344749786e-06
Step: 37480, train/epoch: 8.919562339782715
Step: 37490, train/loss: 0.0
Step: 37490, train/grad_norm: 1.753432172790781e-07
Step: 37490, train/learning_rate: 5.390290425566491e-06
Step: 37490, train/epoch: 8.921941757202148
Step: 37500, train/loss: 0.0
Step: 37500, train/grad_norm: 4.533632818493061e-06
Step: 37500, train/learning_rate: 5.378391051635845e-06
Step: 37500, train/epoch: 8.924322128295898
Step: 37510, train/loss: 0.0
Step: 37510, train/grad_norm: 2.412379558336397e-07
Step: 37510, train/learning_rate: 5.366492132452549e-06
Step: 37510, train/epoch: 8.926701545715332
Step: 37520, train/loss: 0.0
Step: 37520, train/grad_norm: 2.627672984090168e-05
Step: 37520, train/learning_rate: 5.354593213269254e-06
Step: 37520, train/epoch: 8.929080963134766
Step: 37530, train/loss: 0.0
Step: 37530, train/grad_norm: 6.315587143035373e-06
Step: 37530, train/learning_rate: 5.342693839338608e-06
Step: 37530, train/epoch: 8.931461334228516
Step: 37540, train/loss: 0.0
Step: 37540, train/grad_norm: 1.2490259905462153e-05
Step: 37540, train/learning_rate: 5.330794920155313e-06
Step: 37540, train/epoch: 8.93384075164795
Step: 37550, train/loss: 0.0
Step: 37550, train/grad_norm: 0.0001619658141862601
Step: 37550, train/learning_rate: 5.318895546224667e-06
Step: 37550, train/epoch: 8.9362211227417
Step: 37560, train/loss: 0.0
Step: 37560, train/grad_norm: 7.084739627316594e-05
Step: 37560, train/learning_rate: 5.3069966270413715e-06
Step: 37560, train/epoch: 8.938600540161133
Step: 37570, train/loss: 0.0
Step: 37570, train/grad_norm: 3.9562473830301315e-06
Step: 37570, train/learning_rate: 5.295097707858076e-06
Step: 37570, train/epoch: 8.940980911254883
Step: 37580, train/loss: 0.0
Step: 37580, train/grad_norm: 0.000387766573112458
Step: 37580, train/learning_rate: 5.28319833392743e-06
Step: 37580, train/epoch: 8.943360328674316
Step: 37590, train/loss: 0.0
Step: 37590, train/grad_norm: 3.5026450859731995e-06
Step: 37590, train/learning_rate: 5.271299414744135e-06
Step: 37590, train/epoch: 8.94573974609375
Step: 37600, train/loss: 0.0
Step: 37600, train/grad_norm: 5.444420094136149e-05
Step: 37600, train/learning_rate: 5.25940049556084e-06
Step: 37600, train/epoch: 8.9481201171875
Step: 37610, train/loss: 0.0
Step: 37610, train/grad_norm: 1.5022941397546674e-06
Step: 37610, train/learning_rate: 5.247501121630194e-06
Step: 37610, train/epoch: 8.950499534606934
Step: 37620, train/loss: 0.0
Step: 37620, train/grad_norm: 2.029912820944446e-06
Step: 37620, train/learning_rate: 5.2356022024468984e-06
Step: 37620, train/epoch: 8.952879905700684
Step: 37630, train/loss: 0.0
Step: 37630, train/grad_norm: 3.581757709980593e-06
Step: 37630, train/learning_rate: 5.223702828516252e-06
Step: 37630, train/epoch: 8.955259323120117
Step: 37640, train/loss: 0.0
Step: 37640, train/grad_norm: 5.860110832145438e-06
Step: 37640, train/learning_rate: 5.211803909332957e-06
Step: 37640, train/epoch: 8.957639694213867
Step: 37650, train/loss: 0.0
Step: 37650, train/grad_norm: 7.107785495463759e-05
Step: 37650, train/learning_rate: 5.199904990149662e-06
Step: 37650, train/epoch: 8.9600191116333
Step: 37660, train/loss: 0.0
Step: 37660, train/grad_norm: 1.829055804591917e-06
Step: 37660, train/learning_rate: 5.188005616219016e-06
Step: 37660, train/epoch: 8.962398529052734
Step: 37670, train/loss: 0.0
Step: 37670, train/grad_norm: 0.009281689301133156
Step: 37670, train/learning_rate: 5.1761066970357206e-06
Step: 37670, train/epoch: 8.964778900146484
Step: 37680, train/loss: 0.0
Step: 37680, train/grad_norm: 0.00013970582222100347
Step: 37680, train/learning_rate: 5.1642073231050745e-06
Step: 37680, train/epoch: 8.967158317565918
Step: 37690, train/loss: 0.0
Step: 37690, train/grad_norm: 3.987394848081749e-06
Step: 37690, train/learning_rate: 5.152308403921779e-06
Step: 37690, train/epoch: 8.969538688659668
Step: 37700, train/loss: 0.0
Step: 37700, train/grad_norm: 3.420326208924962e-07
Step: 37700, train/learning_rate: 5.140409484738484e-06
Step: 37700, train/epoch: 8.971918106079102
Step: 37710, train/loss: 0.0
Step: 37710, train/grad_norm: 3.5613061299955007e-06
Step: 37710, train/learning_rate: 5.128510110807838e-06
Step: 37710, train/epoch: 8.974297523498535
Step: 37720, train/loss: 0.0
Step: 37720, train/grad_norm: 0.00031770486384630203
Step: 37720, train/learning_rate: 5.116611191624543e-06
Step: 37720, train/epoch: 8.976677894592285
Step: 37730, train/loss: 0.0
Step: 37730, train/grad_norm: 3.9451128941436764e-07
Step: 37730, train/learning_rate: 5.104711817693897e-06
Step: 37730, train/epoch: 8.979057312011719
Step: 37740, train/loss: 0.0
Step: 37740, train/grad_norm: 2.0690299606940243e-07
Step: 37740, train/learning_rate: 5.092812898510601e-06
Step: 37740, train/epoch: 8.981437683105469
Step: 37750, train/loss: 0.0
Step: 37750, train/grad_norm: 7.472966899513267e-06
Step: 37750, train/learning_rate: 5.080913979327306e-06
Step: 37750, train/epoch: 8.983817100524902
Step: 37760, train/loss: 0.0
Step: 37760, train/grad_norm: 3.4227762171212817e-06
Step: 37760, train/learning_rate: 5.06901460539666e-06
Step: 37760, train/epoch: 8.986197471618652
Step: 37770, train/loss: 0.0
Step: 37770, train/grad_norm: 7.578200893476605e-06
Step: 37770, train/learning_rate: 5.057115686213365e-06
Step: 37770, train/epoch: 8.988576889038086
Step: 37780, train/loss: 0.0
Step: 37780, train/grad_norm: 3.1869890904090425e-07
Step: 37780, train/learning_rate: 5.04521676703007e-06
Step: 37780, train/epoch: 8.99095630645752
Step: 37790, train/loss: 0.0
Step: 37790, train/grad_norm: 7.201833795988932e-07
Step: 37790, train/learning_rate: 5.0333173930994235e-06
Step: 37790, train/epoch: 8.99333667755127
Step: 37800, train/loss: 0.0
Step: 37800, train/grad_norm: 2.9673125823137525e-07
Step: 37800, train/learning_rate: 5.021418473916128e-06
Step: 37800, train/epoch: 8.995716094970703
Step: 37810, train/loss: 0.0
Step: 37810, train/grad_norm: 2.5559807909303345e-05
Step: 37810, train/learning_rate: 5.009519099985482e-06
Step: 37810, train/epoch: 8.998096466064453
Step: 37818, eval/loss: 0.0383572019636631
Step: 37818, eval/accuracy: 0.9963904023170471
Step: 37818, eval/f1: 0.996187150478363
Step: 37818, eval/runtime: 734.5687866210938
Step: 37818, eval/samples_per_second: 9.805999755859375
Step: 37818, eval/steps_per_second: 1.2269999980926514
Step: 37818, train/epoch: 9.0
Step: 37820, train/loss: 0.0
Step: 37820, train/grad_norm: 3.628587001003325e-05
Step: 37820, train/learning_rate: 4.997620180802187e-06
Step: 37820, train/epoch: 9.000475883483887
Step: 37830, train/loss: 0.0
Step: 37830, train/grad_norm: 1.1580109458009247e-06
Step: 37830, train/learning_rate: 4.985721261618892e-06
Step: 37830, train/epoch: 9.002856254577637
Step: 37840, train/loss: 0.0
Step: 37840, train/grad_norm: 1.3213801821621018e-06
Step: 37840, train/learning_rate: 4.973821887688246e-06
Step: 37840, train/epoch: 9.00523567199707
Step: 37850, train/loss: 0.0
Step: 37850, train/grad_norm: 7.933757615319337e-07
Step: 37850, train/learning_rate: 4.96192296850495e-06
Step: 37850, train/epoch: 9.007615089416504
Step: 37860, train/loss: 0.0
Step: 37860, train/grad_norm: 1.9963745501172525e-07
Step: 37860, train/learning_rate: 4.950023594574304e-06
Step: 37860, train/epoch: 9.009995460510254
Step: 37870, train/loss: 0.0
Step: 37870, train/grad_norm: 1.0901583664235659e-05
Step: 37870, train/learning_rate: 4.938124675391009e-06
Step: 37870, train/epoch: 9.012374877929688
Step: 37880, train/loss: 0.0
Step: 37880, train/grad_norm: 2.8843385734944604e-05
Step: 37880, train/learning_rate: 4.926225756207714e-06
Step: 37880, train/epoch: 9.014755249023438
Step: 37890, train/loss: 0.0
Step: 37890, train/grad_norm: 5.436118954094127e-06
Step: 37890, train/learning_rate: 4.914326382277068e-06
Step: 37890, train/epoch: 9.017134666442871
Step: 37900, train/loss: 0.0
Step: 37900, train/grad_norm: 1.8903817817772506e-06
Step: 37900, train/learning_rate: 4.9024274630937725e-06
Step: 37900, train/epoch: 9.019514083862305
Step: 37910, train/loss: 0.0
Step: 37910, train/grad_norm: 6.3055549617274664e-06
Step: 37910, train/learning_rate: 4.890528543910477e-06
Step: 37910, train/epoch: 9.021894454956055
Step: 37920, train/loss: 0.0
Step: 37920, train/grad_norm: 3.289648304871662e-07
Step: 37920, train/learning_rate: 4.878629169979831e-06
Step: 37920, train/epoch: 9.024273872375488
Step: 37930, train/loss: 0.0
Step: 37930, train/grad_norm: 8.828124919091351e-06
Step: 37930, train/learning_rate: 4.866730250796536e-06
Step: 37930, train/epoch: 9.026654243469238
Step: 37940, train/loss: 0.0
Step: 37940, train/grad_norm: 1.3864137144992128e-06
Step: 37940, train/learning_rate: 4.85483087686589e-06
Step: 37940, train/epoch: 9.029033660888672
Step: 37950, train/loss: 0.0
Step: 37950, train/grad_norm: 5.985628376947716e-06
Step: 37950, train/learning_rate: 4.842931957682595e-06
Step: 37950, train/epoch: 9.031414031982422
Step: 37960, train/loss: 0.0
Step: 37960, train/grad_norm: 1.2678111716013518e-06
Step: 37960, train/learning_rate: 4.8310330384992994e-06
Step: 37960, train/epoch: 9.033793449401855
Step: 37970, train/loss: 0.0
Step: 37970, train/grad_norm: 1.7396629914401274e-08
Step: 37970, train/learning_rate: 4.819133664568653e-06
Step: 37970, train/epoch: 9.036172866821289
Step: 37980, train/loss: 0.0
Step: 37980, train/grad_norm: 2.2349237838170666e-07
Step: 37980, train/learning_rate: 4.807234745385358e-06
Step: 37980, train/epoch: 9.038553237915039
Step: 37990, train/loss: 0.0
Step: 37990, train/grad_norm: 1.68371670383749e-07
Step: 37990, train/learning_rate: 4.795335371454712e-06
Step: 37990, train/epoch: 9.040932655334473
Step: 38000, train/loss: 0.0
Step: 38000, train/grad_norm: 9.622664265407366e-07
Step: 38000, train/learning_rate: 4.783436452271417e-06
Step: 38000, train/epoch: 9.043313026428223
Step: 38010, train/loss: 0.0
Step: 38010, train/grad_norm: 2.542436595831532e-05
Step: 38010, train/learning_rate: 4.7715375330881216e-06
Step: 38010, train/epoch: 9.045692443847656
Step: 38020, train/loss: 0.0
Step: 38020, train/grad_norm: 6.180606760608498e-06
Step: 38020, train/learning_rate: 4.7596381591574755e-06
Step: 38020, train/epoch: 9.048072814941406
Step: 38030, train/loss: 0.0
Step: 38030, train/grad_norm: 4.703792910731863e-06
Step: 38030, train/learning_rate: 4.74773923997418e-06
Step: 38030, train/epoch: 9.05045223236084
Step: 38040, train/loss: 0.0
Step: 38040, train/grad_norm: 5.263062121230178e-06
Step: 38040, train/learning_rate: 4.735839866043534e-06
Step: 38040, train/epoch: 9.052831649780273
Step: 38050, train/loss: 0.0
Step: 38050, train/grad_norm: 2.3996377422008663e-05
Step: 38050, train/learning_rate: 4.723940946860239e-06
Step: 38050, train/epoch: 9.055212020874023
Step: 38060, train/loss: 0.0
Step: 38060, train/grad_norm: 5.795136075903429e-06
Step: 38060, train/learning_rate: 4.712042027676944e-06
Step: 38060, train/epoch: 9.057591438293457
Step: 38070, train/loss: 0.0
Step: 38070, train/grad_norm: 3.2334637580788694e-06
Step: 38070, train/learning_rate: 4.700142653746298e-06
Step: 38070, train/epoch: 9.059971809387207
Step: 38080, train/loss: 0.0
Step: 38080, train/grad_norm: 1.2656856824833085e-06
Step: 38080, train/learning_rate: 4.688243734563002e-06
Step: 38080, train/epoch: 9.06235122680664
Step: 38090, train/loss: 0.0
Step: 38090, train/grad_norm: 1.2682095075433608e-05
Step: 38090, train/learning_rate: 4.676344815379707e-06
Step: 38090, train/epoch: 9.064730644226074
Step: 38100, train/loss: 0.0
Step: 38100, train/grad_norm: 2.470474385063426e-07
Step: 38100, train/learning_rate: 4.664445441449061e-06
Step: 38100, train/epoch: 9.067111015319824
Step: 38110, train/loss: 0.0
Step: 38110, train/grad_norm: 4.977222829438688e-07
Step: 38110, train/learning_rate: 4.652546522265766e-06
Step: 38110, train/epoch: 9.069490432739258
Step: 38120, train/loss: 0.0
Step: 38120, train/grad_norm: 0.00011205345799680799
Step: 38120, train/learning_rate: 4.64064714833512e-06
Step: 38120, train/epoch: 9.071870803833008
Step: 38130, train/loss: 0.0
Step: 38130, train/grad_norm: 5.529799091164023e-05
Step: 38130, train/learning_rate: 4.6287482291518245e-06
Step: 38130, train/epoch: 9.074250221252441
Step: 38140, train/loss: 0.0
Step: 38140, train/grad_norm: 3.5684381145983934e-05
Step: 38140, train/learning_rate: 4.616849309968529e-06
Step: 38140, train/epoch: 9.076630592346191
Step: 38150, train/loss: 0.0
Step: 38150, train/grad_norm: 3.2301542773893743e-07
Step: 38150, train/learning_rate: 4.604949936037883e-06
Step: 38150, train/epoch: 9.079010009765625
Step: 38160, train/loss: 0.0
Step: 38160, train/grad_norm: 1.2816552725780639e-06
Step: 38160, train/learning_rate: 4.593051016854588e-06
Step: 38160, train/epoch: 9.081389427185059
Step: 38170, train/loss: 0.0
Step: 38170, train/grad_norm: 2.4905295504140668e-05
Step: 38170, train/learning_rate: 4.581151642923942e-06
Step: 38170, train/epoch: 9.083769798278809
Step: 38180, train/loss: 0.0
Step: 38180, train/grad_norm: 8.324861937580863e-07
Step: 38180, train/learning_rate: 4.569252723740647e-06
Step: 38180, train/epoch: 9.086149215698242
Step: 38190, train/loss: 0.0
Step: 38190, train/grad_norm: 4.161111974099185e-06
Step: 38190, train/learning_rate: 4.557353804557351e-06
Step: 38190, train/epoch: 9.088529586791992
Step: 38200, train/loss: 0.0
Step: 38200, train/grad_norm: 0.00011851979070343077
Step: 38200, train/learning_rate: 4.545454430626705e-06
Step: 38200, train/epoch: 9.090909004211426
Step: 38210, train/loss: 0.0
Step: 38210, train/grad_norm: 4.1068801692745183e-07
Step: 38210, train/learning_rate: 4.53355551144341e-06
Step: 38210, train/epoch: 9.093289375305176
Step: 38220, train/loss: 0.0
Step: 38220, train/grad_norm: 0.07165242731571198
Step: 38220, train/learning_rate: 4.521656137512764e-06
Step: 38220, train/epoch: 9.09566879272461
Step: 38230, train/loss: 0.0
Step: 38230, train/grad_norm: 4.5637429479938874e-07
Step: 38230, train/learning_rate: 4.509757218329469e-06
Step: 38230, train/epoch: 9.098048210144043
Step: 38240, train/loss: 0.0
Step: 38240, train/grad_norm: 3.8299072002700996e-07
Step: 38240, train/learning_rate: 4.4978582991461735e-06
Step: 38240, train/epoch: 9.100428581237793
Step: 38250, train/loss: 0.0
Step: 38250, train/grad_norm: 1.5001884889898065e-07
Step: 38250, train/learning_rate: 4.485958925215527e-06
Step: 38250, train/epoch: 9.102807998657227
Step: 38260, train/loss: 0.0
Step: 38260, train/grad_norm: 7.362952146650059e-06
Step: 38260, train/learning_rate: 4.474060006032232e-06
Step: 38260, train/epoch: 9.105188369750977
Step: 38270, train/loss: 0.0
Step: 38270, train/grad_norm: 1.0080775609822012e-06
Step: 38270, train/learning_rate: 4.462161086848937e-06
Step: 38270, train/epoch: 9.10756778717041
Step: 38280, train/loss: 0.0
Step: 38280, train/grad_norm: 3.460321522652521e-06
Step: 38280, train/learning_rate: 4.450261712918291e-06
Step: 38280, train/epoch: 9.109947204589844
Step: 38290, train/loss: 0.0
Step: 38290, train/grad_norm: 2.5689703875286796e-07
Step: 38290, train/learning_rate: 4.438362793734996e-06
Step: 38290, train/epoch: 9.112327575683594
Step: 38300, train/loss: 0.0
Step: 38300, train/grad_norm: 1.9742871870676026e-07
Step: 38300, train/learning_rate: 4.4264634198043495e-06
Step: 38300, train/epoch: 9.114706993103027
Step: 38310, train/loss: 0.0
Step: 38310, train/grad_norm: 3.9990169170778245e-05
Step: 38310, train/learning_rate: 4.414564500621054e-06
Step: 38310, train/epoch: 9.117087364196777
Step: 38320, train/loss: 0.0
Step: 38320, train/grad_norm: 5.049200808571186e-07
Step: 38320, train/learning_rate: 4.402665581437759e-06
Step: 38320, train/epoch: 9.119466781616211
Step: 38330, train/loss: 0.0
Step: 38330, train/grad_norm: 1.1626124063468524e-07
Step: 38330, train/learning_rate: 4.390766207507113e-06
Step: 38330, train/epoch: 9.121847152709961
Step: 38340, train/loss: 0.1679999977350235
Step: 38340, train/grad_norm: 1.088619910660782e-06
Step: 38340, train/learning_rate: 4.378867288323818e-06
Step: 38340, train/epoch: 9.124226570129395
Step: 38350, train/loss: 0.0
Step: 38350, train/grad_norm: 1.9709883417817764e-05
Step: 38350, train/learning_rate: 4.366967914393172e-06
Step: 38350, train/epoch: 9.126605987548828
Step: 38360, train/loss: 0.0
Step: 38360, train/grad_norm: 8.279797043542203e-07
Step: 38360, train/learning_rate: 4.3550689952098764e-06
Step: 38360, train/epoch: 9.128986358642578
Step: 38370, train/loss: 0.0
Step: 38370, train/grad_norm: 1.8117065337719396e-05
Step: 38370, train/learning_rate: 4.343170076026581e-06
Step: 38370, train/epoch: 9.131365776062012
Step: 38380, train/loss: 0.0
Step: 38380, train/grad_norm: 0.00034224966657347977
Step: 38380, train/learning_rate: 4.331270702095935e-06
Step: 38380, train/epoch: 9.133746147155762
Step: 38390, train/loss: 0.0
Step: 38390, train/grad_norm: 2.6290363166481256e-05
Step: 38390, train/learning_rate: 4.31937178291264e-06
Step: 38390, train/epoch: 9.136125564575195
Step: 38400, train/loss: 0.0
Step: 38400, train/grad_norm: 0.0050489287823438644
Step: 38400, train/learning_rate: 4.307472408981994e-06
Step: 38400, train/epoch: 9.138505935668945
Step: 38410, train/loss: 0.0
Step: 38410, train/grad_norm: 0.00022811941744294018
Step: 38410, train/learning_rate: 4.2955734897986986e-06
Step: 38410, train/epoch: 9.140885353088379
Step: 38420, train/loss: 0.0
Step: 38420, train/grad_norm: 2.0229035726515576e-05
Step: 38420, train/learning_rate: 4.283674570615403e-06
Step: 38420, train/epoch: 9.143264770507812
Step: 38430, train/loss: 0.0
Step: 38430, train/grad_norm: 4.565052222460508e-06
Step: 38430, train/learning_rate: 4.271775196684757e-06
Step: 38430, train/epoch: 9.145645141601562
Step: 38440, train/loss: 0.0
Step: 38440, train/grad_norm: 1.6726220565033145e-05
Step: 38440, train/learning_rate: 4.259876277501462e-06
Step: 38440, train/epoch: 9.148024559020996
Step: 38450, train/loss: 0.0
Step: 38450, train/grad_norm: 1.906204124679789e-05
Step: 38450, train/learning_rate: 4.247977358318167e-06
Step: 38450, train/epoch: 9.150404930114746
Step: 38460, train/loss: 0.0
Step: 38460, train/grad_norm: 9.079255505639594e-06
Step: 38460, train/learning_rate: 4.236077984387521e-06
Step: 38460, train/epoch: 9.15278434753418
Step: 38470, train/loss: 0.0
Step: 38470, train/grad_norm: 4.9465588745079e-06
Step: 38470, train/learning_rate: 4.2241790652042255e-06
Step: 38470, train/epoch: 9.155163764953613
Step: 38480, train/loss: 0.0
Step: 38480, train/grad_norm: 3.5623618259705836e-06
Step: 38480, train/learning_rate: 4.212279691273579e-06
Step: 38480, train/epoch: 9.157544136047363
Step: 38490, train/loss: 0.0
Step: 38490, train/grad_norm: 7.457923402398592e-06
Step: 38490, train/learning_rate: 4.200380772090284e-06
Step: 38490, train/epoch: 9.159923553466797
Step: 38500, train/loss: 0.0
Step: 38500, train/grad_norm: 9.834597221924923e-06
Step: 38500, train/learning_rate: 4.188481852906989e-06
Step: 38500, train/epoch: 9.162303924560547
Step: 38510, train/loss: 0.0
Step: 38510, train/grad_norm: 1.1938946045120247e-06
Step: 38510, train/learning_rate: 4.176582478976343e-06
Step: 38510, train/epoch: 9.16468334197998
Step: 38520, train/loss: 0.0
Step: 38520, train/grad_norm: 2.999080834342749e-06
Step: 38520, train/learning_rate: 4.164683559793048e-06
Step: 38520, train/epoch: 9.16706371307373
Step: 38530, train/loss: 0.0
Step: 38530, train/grad_norm: 3.351009127072757e-06
Step: 38530, train/learning_rate: 4.1527841858624015e-06
Step: 38530, train/epoch: 9.169443130493164
Step: 38540, train/loss: 0.0
Step: 38540, train/grad_norm: 4.870349584962241e-06
Step: 38540, train/learning_rate: 4.140885266679106e-06
Step: 38540, train/epoch: 9.171822547912598
Step: 38550, train/loss: 0.0
Step: 38550, train/grad_norm: 1.875929956440814e-05
Step: 38550, train/learning_rate: 4.128986347495811e-06
Step: 38550, train/epoch: 9.174202919006348
Step: 38560, train/loss: 0.0
Step: 38560, train/grad_norm: 7.205638326013286e-07
Step: 38560, train/learning_rate: 4.117086973565165e-06
Step: 38560, train/epoch: 9.176582336425781
Step: 38570, train/loss: 0.0
Step: 38570, train/grad_norm: 9.469585347687826e-05
Step: 38570, train/learning_rate: 4.10518805438187e-06
Step: 38570, train/epoch: 9.178962707519531
Step: 38580, train/loss: 0.0
Step: 38580, train/grad_norm: 5.5877576414786745e-06
Step: 38580, train/learning_rate: 4.0932891351985745e-06
Step: 38580, train/epoch: 9.181342124938965
Step: 38590, train/loss: 0.0
Step: 38590, train/grad_norm: 1.4288013971963665e-06
Step: 38590, train/learning_rate: 4.081389761267928e-06
Step: 38590, train/epoch: 9.183722496032715
Step: 38600, train/loss: 0.0
Step: 38600, train/grad_norm: 1.9662991689983755e-05
Step: 38600, train/learning_rate: 4.069490842084633e-06
Step: 38600, train/epoch: 9.186101913452148
Step: 38610, train/loss: 0.0
Step: 38610, train/grad_norm: 8.474533387925476e-06
Step: 38610, train/learning_rate: 4.057591468153987e-06
Step: 38610, train/epoch: 9.188481330871582
Step: 38620, train/loss: 0.0
Step: 38620, train/grad_norm: 2.4120322450471576e-07
Step: 38620, train/learning_rate: 4.045692548970692e-06
Step: 38620, train/epoch: 9.190861701965332
Step: 38630, train/loss: 0.0
Step: 38630, train/grad_norm: 7.405594715237385e-06
Step: 38630, train/learning_rate: 4.033793629787397e-06
Step: 38630, train/epoch: 9.193241119384766
Step: 38640, train/loss: 0.0
Step: 38640, train/grad_norm: 3.737007864401676e-05
Step: 38640, train/learning_rate: 4.0218942558567505e-06
Step: 38640, train/epoch: 9.195621490478516
Step: 38650, train/loss: 0.0
Step: 38650, train/grad_norm: 1.1837011015813914e-06
Step: 38650, train/learning_rate: 4.009995336673455e-06
Step: 38650, train/epoch: 9.19800090789795
Step: 38660, train/loss: 0.0
Step: 38660, train/grad_norm: 3.544761284501874e-06
Step: 38660, train/learning_rate: 3.998095962742809e-06
Step: 38660, train/epoch: 9.200380325317383
Step: 38670, train/loss: 0.0
Step: 38670, train/grad_norm: 1.1864775842695963e-05
Step: 38670, train/learning_rate: 3.986197043559514e-06
Step: 38670, train/epoch: 9.202760696411133
Step: 38680, train/loss: 0.0
Step: 38680, train/grad_norm: 1.3389721971179824e-06
Step: 38680, train/learning_rate: 3.974298124376219e-06
Step: 38680, train/epoch: 9.205140113830566
Step: 38690, train/loss: 0.0
Step: 38690, train/grad_norm: 1.0659138752089348e-05
Step: 38690, train/learning_rate: 3.962398750445573e-06
Step: 38690, train/epoch: 9.207520484924316
Step: 38700, train/loss: 0.0
Step: 38700, train/grad_norm: 1.153697098743578e-06
Step: 38700, train/learning_rate: 3.9504998312622774e-06
Step: 38700, train/epoch: 9.20989990234375
Step: 38710, train/loss: 0.0
Step: 38710, train/grad_norm: 3.767962698475458e-05
Step: 38710, train/learning_rate: 3.938600457331631e-06
Step: 38710, train/epoch: 9.2122802734375
Step: 38720, train/loss: 0.0
Step: 38720, train/grad_norm: 9.894773029373027e-06
Step: 38720, train/learning_rate: 3.926701538148336e-06
Step: 38720, train/epoch: 9.214659690856934
Step: 38730, train/loss: 0.0
Step: 38730, train/grad_norm: 2.574411337263882e-05
Step: 38730, train/learning_rate: 3.914802618965041e-06
Step: 38730, train/epoch: 9.217039108276367
Step: 38740, train/loss: 0.0
Step: 38740, train/grad_norm: 5.949667638560641e-07
Step: 38740, train/learning_rate: 3.902903245034395e-06
Step: 38740, train/epoch: 9.219419479370117
Step: 38750, train/loss: 0.0
Step: 38750, train/grad_norm: 6.949397146627234e-08
Step: 38750, train/learning_rate: 3.8910043258510996e-06
Step: 38750, train/epoch: 9.22179889678955
Step: 38760, train/loss: 0.0
Step: 38760, train/grad_norm: 1.3595840755442623e-06
Step: 38760, train/learning_rate: 3.879105406667804e-06
Step: 38760, train/epoch: 9.2241792678833
Step: 38770, train/loss: 0.0
Step: 38770, train/grad_norm: 1.8513017607801885e-07
Step: 38770, train/learning_rate: 3.867206032737158e-06
Step: 38770, train/epoch: 9.226558685302734
Step: 38780, train/loss: 0.0
Step: 38780, train/grad_norm: 3.4133925055357395e-06
Step: 38780, train/learning_rate: 3.855307113553863e-06
Step: 38780, train/epoch: 9.228939056396484
Step: 38790, train/loss: 0.0
Step: 38790, train/grad_norm: 1.4709116840094794e-05
Step: 38790, train/learning_rate: 3.843407739623217e-06
Step: 38790, train/epoch: 9.231318473815918
Step: 38800, train/loss: 0.0
Step: 38800, train/grad_norm: 8.409295446654141e-07
Step: 38800, train/learning_rate: 3.831508820439922e-06
Step: 38800, train/epoch: 9.233697891235352
Step: 38810, train/loss: 0.0
Step: 38810, train/grad_norm: 3.845559149340261e-06
Step: 38810, train/learning_rate: 3.8196099012566265e-06
Step: 38810, train/epoch: 9.236078262329102
Step: 38820, train/loss: 0.0
Step: 38820, train/grad_norm: 3.3182004699483514e-06
Step: 38820, train/learning_rate: 3.8077105273259804e-06
Step: 38820, train/epoch: 9.238457679748535
Step: 38830, train/loss: 0.0
Step: 38830, train/grad_norm: 8.569518104195595e-05
Step: 38830, train/learning_rate: 3.795811608142685e-06
Step: 38830, train/epoch: 9.240838050842285
Step: 38840, train/loss: 0.0
Step: 38840, train/grad_norm: 4.948517471348168e-06
Step: 38840, train/learning_rate: 3.7839124615857145e-06
Step: 38840, train/epoch: 9.243217468261719
Step: 38850, train/loss: 0.0
Step: 38850, train/grad_norm: 8.249082839029143e-07
Step: 38850, train/learning_rate: 3.772013315028744e-06
Step: 38850, train/epoch: 9.245596885681152
Step: 38860, train/loss: 0.0
Step: 38860, train/grad_norm: 8.505041932949098e-07
Step: 38860, train/learning_rate: 3.760114168471773e-06
Step: 38860, train/epoch: 9.247977256774902
Step: 38870, train/loss: 0.0
Step: 38870, train/grad_norm: 6.0746115195797756e-05
Step: 38870, train/learning_rate: 3.748215249288478e-06
Step: 38870, train/epoch: 9.250356674194336
Step: 38880, train/loss: 0.0
Step: 38880, train/grad_norm: 1.3950118955108337e-05
Step: 38880, train/learning_rate: 3.7363161027315073e-06
Step: 38880, train/epoch: 9.252737045288086
Step: 38890, train/loss: 0.0
Step: 38890, train/grad_norm: 1.8424900360969332e-07
Step: 38890, train/learning_rate: 3.7244169561745366e-06
Step: 38890, train/epoch: 9.25511646270752
Step: 38900, train/loss: 0.0
Step: 38900, train/grad_norm: 3.825073690677527e-06
Step: 38900, train/learning_rate: 3.712517809617566e-06
Step: 38900, train/epoch: 9.25749683380127
Step: 38910, train/loss: 0.0
Step: 38910, train/grad_norm: 0.0006497016292996705
Step: 38910, train/learning_rate: 3.7006186630605953e-06
Step: 38910, train/epoch: 9.259876251220703
Step: 38920, train/loss: 0.0
Step: 38920, train/grad_norm: 2.0624544561087532e-07
Step: 38920, train/learning_rate: 3.6887197438773e-06
Step: 38920, train/epoch: 9.262255668640137
Step: 38930, train/loss: 0.0
Step: 38930, train/grad_norm: 0.00010671542986528948
Step: 38930, train/learning_rate: 3.6768205973203294e-06
Step: 38930, train/epoch: 9.264636039733887
Step: 38940, train/loss: 0.0
Step: 38940, train/grad_norm: 2.0574693735397886e-06
Step: 38940, train/learning_rate: 3.6649214507633587e-06
Step: 38940, train/epoch: 9.26701545715332
Step: 38950, train/loss: 0.0
Step: 38950, train/grad_norm: 3.62505687689918e-07
Step: 38950, train/learning_rate: 3.653022304206388e-06
Step: 38950, train/epoch: 9.26939582824707
Step: 38960, train/loss: 0.0
Step: 38960, train/grad_norm: 3.13022923137396e-07
Step: 38960, train/learning_rate: 3.641123385023093e-06
Step: 38960, train/epoch: 9.271775245666504
Step: 38970, train/loss: 0.0
Step: 38970, train/grad_norm: 9.656617976361304e-07
Step: 38970, train/learning_rate: 3.629224238466122e-06
Step: 38970, train/epoch: 9.274155616760254
Step: 38980, train/loss: 0.0
Step: 38980, train/grad_norm: 3.730129947143723e-07
Step: 38980, train/learning_rate: 3.6173250919091515e-06
Step: 38980, train/epoch: 9.276535034179688
Step: 38990, train/loss: 0.0
Step: 38990, train/grad_norm: 1.3751462120126234e-06
Step: 38990, train/learning_rate: 3.605425945352181e-06
Step: 38990, train/epoch: 9.278914451599121
Step: 39000, train/loss: 0.0
Step: 39000, train/grad_norm: 0.0001727885683067143
Step: 39000, train/learning_rate: 3.59352679879521e-06
Step: 39000, train/epoch: 9.281294822692871
Step: 39010, train/loss: 0.0
Step: 39010, train/grad_norm: 9.643648809287697e-06
Step: 39010, train/learning_rate: 3.581627879611915e-06
Step: 39010, train/epoch: 9.283674240112305
Step: 39020, train/loss: 0.0
Step: 39020, train/grad_norm: 6.4587661654513795e-06
Step: 39020, train/learning_rate: 3.5697287330549443e-06
Step: 39020, train/epoch: 9.286054611206055
Step: 39030, train/loss: 0.0
Step: 39030, train/grad_norm: 2.4913049401220633e-07
Step: 39030, train/learning_rate: 3.5578295864979737e-06
Step: 39030, train/epoch: 9.288434028625488
Step: 39040, train/loss: 0.0
Step: 39040, train/grad_norm: 3.2226751045527635e-06
Step: 39040, train/learning_rate: 3.545930439941003e-06
Step: 39040, train/epoch: 9.290813446044922
Step: 39050, train/loss: 0.0
Step: 39050, train/grad_norm: 7.511498097301228e-06
Step: 39050, train/learning_rate: 3.5340315207577078e-06
Step: 39050, train/epoch: 9.293193817138672
Step: 39060, train/loss: 0.0
Step: 39060, train/grad_norm: 3.9185974856081884e-06
Step: 39060, train/learning_rate: 3.522132374200737e-06
Step: 39060, train/epoch: 9.295573234558105
Step: 39070, train/loss: 0.0
Step: 39070, train/grad_norm: 9.823367690842133e-06
Step: 39070, train/learning_rate: 3.5102332276437664e-06
Step: 39070, train/epoch: 9.297953605651855
Step: 39080, train/loss: 0.0
Step: 39080, train/grad_norm: 3.618620667111827e-06
Step: 39080, train/learning_rate: 3.4983340810867958e-06
Step: 39080, train/epoch: 9.300333023071289
Step: 39090, train/loss: 0.0
Step: 39090, train/grad_norm: 6.732447218382731e-06
Step: 39090, train/learning_rate: 3.486434934529825e-06
Step: 39090, train/epoch: 9.302713394165039
Step: 39100, train/loss: 0.0
Step: 39100, train/grad_norm: 3.292858536951826e-06
Step: 39100, train/learning_rate: 3.47453601534653e-06
Step: 39100, train/epoch: 9.305092811584473
Step: 39110, train/loss: 0.0
Step: 39110, train/grad_norm: 1.224741595251544e-06
Step: 39110, train/learning_rate: 3.4626368687895592e-06
Step: 39110, train/epoch: 9.307472229003906
Step: 39120, train/loss: 0.0
Step: 39120, train/grad_norm: 9.551533366902731e-06
Step: 39120, train/learning_rate: 3.4507377222325886e-06
Step: 39120, train/epoch: 9.309852600097656
Step: 39130, train/loss: 0.0
Step: 39130, train/grad_norm: 7.688694313401356e-05
Step: 39130, train/learning_rate: 3.438838575675618e-06
Step: 39130, train/epoch: 9.31223201751709
Step: 39140, train/loss: 0.0
Step: 39140, train/grad_norm: 9.313433793067816e-07
Step: 39140, train/learning_rate: 3.4269396564923227e-06
Step: 39140, train/epoch: 9.31461238861084
Step: 39150, train/loss: 0.0
Step: 39150, train/grad_norm: 7.429207926179515e-06
Step: 39150, train/learning_rate: 3.415040509935352e-06
Step: 39150, train/epoch: 9.316991806030273
Step: 39160, train/loss: 0.0
Step: 39160, train/grad_norm: 3.75142508346471e-06
Step: 39160, train/learning_rate: 3.4031413633783814e-06
Step: 39160, train/epoch: 9.319372177124023
Step: 39170, train/loss: 0.0
Step: 39170, train/grad_norm: 1.6140700154210208e-07
Step: 39170, train/learning_rate: 3.3912422168214107e-06
Step: 39170, train/epoch: 9.321751594543457
Step: 39180, train/loss: 0.0
Step: 39180, train/grad_norm: 2.3774151713951142e-07
Step: 39180, train/learning_rate: 3.37934307026444e-06
Step: 39180, train/epoch: 9.32413101196289
Step: 39190, train/loss: 0.0
Step: 39190, train/grad_norm: 9.291870242122968e-07
Step: 39190, train/learning_rate: 3.367444151081145e-06
Step: 39190, train/epoch: 9.32651138305664
Step: 39200, train/loss: 0.0
Step: 39200, train/grad_norm: 8.502690434397664e-06
Step: 39200, train/learning_rate: 3.355545004524174e-06
Step: 39200, train/epoch: 9.328890800476074
Step: 39210, train/loss: 0.0
Step: 39210, train/grad_norm: 0.000440409203292802
Step: 39210, train/learning_rate: 3.3436458579672035e-06
Step: 39210, train/epoch: 9.331271171569824
Step: 39220, train/loss: 0.0
Step: 39220, train/grad_norm: 8.175848620339821e-07
Step: 39220, train/learning_rate: 3.331746711410233e-06
Step: 39220, train/epoch: 9.333650588989258
Step: 39230, train/loss: 0.0
Step: 39230, train/grad_norm: 6.980213129281765e-06
Step: 39230, train/learning_rate: 3.3198477922269376e-06
Step: 39230, train/epoch: 9.336030006408691
Step: 39240, train/loss: 0.0
Step: 39240, train/grad_norm: 9.027510532177985e-05
Step: 39240, train/learning_rate: 3.307948645669967e-06
Step: 39240, train/epoch: 9.338410377502441
Step: 39250, train/loss: 0.0
Step: 39250, train/grad_norm: 2.7097599740955047e-05
Step: 39250, train/learning_rate: 3.2960494991129963e-06
Step: 39250, train/epoch: 9.340789794921875
Step: 39260, train/loss: 0.0
Step: 39260, train/grad_norm: 2.7409023459767923e-06
Step: 39260, train/learning_rate: 3.2841503525560256e-06
Step: 39260, train/epoch: 9.343170166015625
Step: 39270, train/loss: 0.0
Step: 39270, train/grad_norm: 1.392558442603331e-05
Step: 39270, train/learning_rate: 3.272251205999055e-06
Step: 39270, train/epoch: 9.345549583435059
Step: 39280, train/loss: 0.0
Step: 39280, train/grad_norm: 0.00014169503992889076
Step: 39280, train/learning_rate: 3.2603522868157597e-06
Step: 39280, train/epoch: 9.347929954528809
Step: 39290, train/loss: 0.0
Step: 39290, train/grad_norm: 2.7987266548734624e-07
Step: 39290, train/learning_rate: 3.248453140258789e-06
Step: 39290, train/epoch: 9.350309371948242
Step: 39300, train/loss: 0.0
Step: 39300, train/grad_norm: 7.222377575999417e-07
Step: 39300, train/learning_rate: 3.2365539937018184e-06
Step: 39300, train/epoch: 9.352688789367676
Step: 39310, train/loss: 0.0
Step: 39310, train/grad_norm: 3.276789129813551e-06
Step: 39310, train/learning_rate: 3.2246548471448477e-06
Step: 39310, train/epoch: 9.355069160461426
Step: 39320, train/loss: 0.0
Step: 39320, train/grad_norm: 7.759217623970471e-06
Step: 39320, train/learning_rate: 3.2127559279615525e-06
Step: 39320, train/epoch: 9.35744857788086
Step: 39330, train/loss: 0.0
Step: 39330, train/grad_norm: 3.007946361321956e-06
Step: 39330, train/learning_rate: 3.200856781404582e-06
Step: 39330, train/epoch: 9.35982894897461
Step: 39340, train/loss: 0.0
Step: 39340, train/grad_norm: 3.852330792142311e-06
Step: 39340, train/learning_rate: 3.188957634847611e-06
Step: 39340, train/epoch: 9.362208366394043
Step: 39350, train/loss: 0.0
Step: 39350, train/grad_norm: 1.505475211160956e-05
Step: 39350, train/learning_rate: 3.1770584882906405e-06
Step: 39350, train/epoch: 9.364588737487793
Step: 39360, train/loss: 0.0
Step: 39360, train/grad_norm: 2.854744423075317e-07
Step: 39360, train/learning_rate: 3.16515934173367e-06
Step: 39360, train/epoch: 9.366968154907227
Step: 39370, train/loss: 0.0
Step: 39370, train/grad_norm: 1.7708913446767838e-06
Step: 39370, train/learning_rate: 3.1532604225503746e-06
Step: 39370, train/epoch: 9.36934757232666
Step: 39380, train/loss: 0.0
Step: 39380, train/grad_norm: 1.762409374350682e-05
Step: 39380, train/learning_rate: 3.141361275993404e-06
Step: 39380, train/epoch: 9.37172794342041
Step: 39390, train/loss: 0.0
Step: 39390, train/grad_norm: 1.7254239992325893e-07
Step: 39390, train/learning_rate: 3.1294621294364333e-06
Step: 39390, train/epoch: 9.374107360839844
Step: 39400, train/loss: 0.0
Step: 39400, train/grad_norm: 1.7795210283111373e-07
Step: 39400, train/learning_rate: 3.1175629828794627e-06
Step: 39400, train/epoch: 9.376487731933594
Step: 39410, train/loss: 0.0
Step: 39410, train/grad_norm: 2.062039857264608e-05
Step: 39410, train/learning_rate: 3.1056640636961674e-06
Step: 39410, train/epoch: 9.378867149353027
Step: 39420, train/loss: 0.0
Step: 39420, train/grad_norm: 4.992501771994284e-07
Step: 39420, train/learning_rate: 3.0937649171391968e-06
Step: 39420, train/epoch: 9.381246566772461
Step: 39430, train/loss: 0.0
Step: 39430, train/grad_norm: 3.921850293409079e-06
Step: 39430, train/learning_rate: 3.081865770582226e-06
Step: 39430, train/epoch: 9.383626937866211
Step: 39440, train/loss: 0.0
Step: 39440, train/grad_norm: 1.0131341241503833e-06
Step: 39440, train/learning_rate: 3.0699666240252554e-06
Step: 39440, train/epoch: 9.386006355285645
Step: 39450, train/loss: 0.0
Step: 39450, train/grad_norm: 5.03253716033214e-07
Step: 39450, train/learning_rate: 3.0580674774682848e-06
Step: 39450, train/epoch: 9.388386726379395
Step: 39460, train/loss: 0.0
Step: 39460, train/grad_norm: 9.388293165102368e-08
Step: 39460, train/learning_rate: 3.0461685582849896e-06
Step: 39460, train/epoch: 9.390766143798828
Step: 39470, train/loss: 0.0
Step: 39470, train/grad_norm: 9.281089887736016e-07
Step: 39470, train/learning_rate: 3.034269411728019e-06
Step: 39470, train/epoch: 9.393146514892578
Step: 39480, train/loss: 0.0
Step: 39480, train/grad_norm: 3.1544018384010997e-06
Step: 39480, train/learning_rate: 3.0223702651710482e-06
Step: 39480, train/epoch: 9.395525932312012
Step: 39490, train/loss: 0.0
Step: 39490, train/grad_norm: 7.82572328716924e-07
Step: 39490, train/learning_rate: 3.0104711186140776e-06
Step: 39490, train/epoch: 9.397905349731445
Step: 39500, train/loss: 0.0
Step: 39500, train/grad_norm: 0.00016208074521273375
Step: 39500, train/learning_rate: 2.9985721994307823e-06
Step: 39500, train/epoch: 9.400285720825195
Step: 39510, train/loss: 0.0
Step: 39510, train/grad_norm: 8.114502270473167e-05
Step: 39510, train/learning_rate: 2.9866730528738117e-06
Step: 39510, train/epoch: 9.402665138244629
Step: 39520, train/loss: 0.0
Step: 39520, train/grad_norm: 9.512706924397207e-07
Step: 39520, train/learning_rate: 2.974773906316841e-06
Step: 39520, train/epoch: 9.405045509338379
Step: 39530, train/loss: 0.0
Step: 39530, train/grad_norm: 0.0002760142378974706
Step: 39530, train/learning_rate: 2.9628747597598704e-06
Step: 39530, train/epoch: 9.407424926757812
Step: 39540, train/loss: 0.0
Step: 39540, train/grad_norm: 4.582922701956704e-05
Step: 39540, train/learning_rate: 2.9509756132028997e-06
Step: 39540, train/epoch: 9.409805297851562
Step: 39550, train/loss: 0.0
Step: 39550, train/grad_norm: 6.307962507889897e-07
Step: 39550, train/learning_rate: 2.9390766940196045e-06
Step: 39550, train/epoch: 9.412184715270996
Step: 39560, train/loss: 0.0
Step: 39560, train/grad_norm: 4.5205688365967944e-06
Step: 39560, train/learning_rate: 2.927177547462634e-06
Step: 39560, train/epoch: 9.41456413269043
Step: 39570, train/loss: 0.0
Step: 39570, train/grad_norm: 2.6139541660086252e-05
Step: 39570, train/learning_rate: 2.915278400905663e-06
Step: 39570, train/epoch: 9.41694450378418
Step: 39580, train/loss: 0.0
Step: 39580, train/grad_norm: 1.1491453051348799e-06
Step: 39580, train/learning_rate: 2.9033792543486925e-06
Step: 39580, train/epoch: 9.419323921203613
Step: 39590, train/loss: 0.0
Step: 39590, train/grad_norm: 3.252512760809623e-05
Step: 39590, train/learning_rate: 2.8914803351653973e-06
Step: 39590, train/epoch: 9.421704292297363
Step: 39600, train/loss: 0.0
Step: 39600, train/grad_norm: 2.1254652438074118e-07
Step: 39600, train/learning_rate: 2.8795811886084266e-06
Step: 39600, train/epoch: 9.424083709716797
Step: 39610, train/loss: 0.0
Step: 39610, train/grad_norm: 2.0889591212380765e-07
Step: 39610, train/learning_rate: 2.867682042051456e-06
Step: 39610, train/epoch: 9.42646312713623
Step: 39620, train/loss: 0.0
Step: 39620, train/grad_norm: 7.433606015183614e-07
Step: 39620, train/learning_rate: 2.8557828954944853e-06
Step: 39620, train/epoch: 9.42884349822998
Step: 39630, train/loss: 0.0
Step: 39630, train/grad_norm: 3.354450745973736e-05
Step: 39630, train/learning_rate: 2.84388397631119e-06
Step: 39630, train/epoch: 9.431222915649414
Step: 39640, train/loss: 0.0
Step: 39640, train/grad_norm: 6.699578534608008e-06
Step: 39640, train/learning_rate: 2.8319848297542194e-06
Step: 39640, train/epoch: 9.433603286743164
Step: 39650, train/loss: 0.0
Step: 39650, train/grad_norm: 2.862852852558717e-07
Step: 39650, train/learning_rate: 2.8200856831972487e-06
Step: 39650, train/epoch: 9.435982704162598
Step: 39660, train/loss: 0.0
Step: 39660, train/grad_norm: 9.224397103935189e-07
Step: 39660, train/learning_rate: 2.808186536640278e-06
Step: 39660, train/epoch: 9.438363075256348
Step: 39670, train/loss: 0.0
Step: 39670, train/grad_norm: 3.9917239291753503e-07
Step: 39670, train/learning_rate: 2.7962873900833074e-06
Step: 39670, train/epoch: 9.440742492675781
Step: 39680, train/loss: 0.0
Step: 39680, train/grad_norm: 1.3190252502681687e-05
Step: 39680, train/learning_rate: 2.784388470900012e-06
Step: 39680, train/epoch: 9.443121910095215
Step: 39690, train/loss: 0.0
Step: 39690, train/grad_norm: 1.106746253753954e-06
Step: 39690, train/learning_rate: 2.7724893243430415e-06
Step: 39690, train/epoch: 9.445502281188965
Step: 39700, train/loss: 0.0
Step: 39700, train/grad_norm: 3.5940825426905576e-08
Step: 39700, train/learning_rate: 2.760590177786071e-06
Step: 39700, train/epoch: 9.447881698608398
Step: 39710, train/loss: 0.0
Step: 39710, train/grad_norm: 1.0420246326248161e-05
Step: 39710, train/learning_rate: 2.7486910312291e-06
Step: 39710, train/epoch: 9.450262069702148
Step: 39720, train/loss: 0.0
Step: 39720, train/grad_norm: 1.2664649773341807e-07
Step: 39720, train/learning_rate: 2.736792112045805e-06
Step: 39720, train/epoch: 9.452641487121582
Step: 39730, train/loss: 0.0
Step: 39730, train/grad_norm: 9.212513396050781e-05
Step: 39730, train/learning_rate: 2.7248929654888343e-06
Step: 39730, train/epoch: 9.455021858215332
Step: 39740, train/loss: 0.0
Step: 39740, train/grad_norm: 1.72335967363324e-05
Step: 39740, train/learning_rate: 2.7129938189318636e-06
Step: 39740, train/epoch: 9.457401275634766
Step: 39750, train/loss: 0.0
Step: 39750, train/grad_norm: 4.062158041051589e-06
Step: 39750, train/learning_rate: 2.701094672374893e-06
Step: 39750, train/epoch: 9.4597806930542
Step: 39760, train/loss: 0.0
Step: 39760, train/grad_norm: 1.5749594695080305e-07
Step: 39760, train/learning_rate: 2.6891955258179223e-06
Step: 39760, train/epoch: 9.46216106414795
Step: 39770, train/loss: 0.0
Step: 39770, train/grad_norm: 5.00599890074227e-06
Step: 39770, train/learning_rate: 2.677296606634627e-06
Step: 39770, train/epoch: 9.464540481567383
Step: 39780, train/loss: 0.0
Step: 39780, train/grad_norm: 2.9963457564008422e-06
Step: 39780, train/learning_rate: 2.6653974600776564e-06
Step: 39780, train/epoch: 9.466920852661133
Step: 39790, train/loss: 0.0
Step: 39790, train/grad_norm: 3.844694219878875e-06
Step: 39790, train/learning_rate: 2.6534983135206858e-06
Step: 39790, train/epoch: 9.469300270080566
Step: 39800, train/loss: 0.0
Step: 39800, train/grad_norm: 1.5456087567145005e-05
Step: 39800, train/learning_rate: 2.641599166963715e-06
Step: 39800, train/epoch: 9.4716796875
Step: 39810, train/loss: 0.0
Step: 39810, train/grad_norm: 1.4360772411237122e-06
Step: 39810, train/learning_rate: 2.62970024778042e-06
Step: 39810, train/epoch: 9.47406005859375
Step: 39820, train/loss: 0.0
Step: 39820, train/grad_norm: 2.4843595383572392e-05
Step: 39820, train/learning_rate: 2.6178011012234492e-06
Step: 39820, train/epoch: 9.476439476013184
Step: 39830, train/loss: 0.0
Step: 39830, train/grad_norm: 2.127906191162765e-05
Step: 39830, train/learning_rate: 2.6059019546664786e-06
Step: 39830, train/epoch: 9.478819847106934
Step: 39840, train/loss: 0.0
Step: 39840, train/grad_norm: 2.63033719249961e-08
Step: 39840, train/learning_rate: 2.594002808109508e-06
Step: 39840, train/epoch: 9.481199264526367
Step: 39850, train/loss: 0.0
Step: 39850, train/grad_norm: 2.9870088269490225e-07
Step: 39850, train/learning_rate: 2.5821036615525372e-06
Step: 39850, train/epoch: 9.483579635620117
Step: 39860, train/loss: 0.0
Step: 39860, train/grad_norm: 7.010276021901518e-05
Step: 39860, train/learning_rate: 2.570204742369242e-06
Step: 39860, train/epoch: 9.48595905303955
Step: 39870, train/loss: 0.0
Step: 39870, train/grad_norm: 7.692245418411403e-08
Step: 39870, train/learning_rate: 2.5583055958122713e-06
Step: 39870, train/epoch: 9.488338470458984
Step: 39880, train/loss: 0.0
Step: 39880, train/grad_norm: 5.558958491747035e-06
Step: 39880, train/learning_rate: 2.5464064492553007e-06
Step: 39880, train/epoch: 9.490718841552734
Step: 39890, train/loss: 0.0
Step: 39890, train/grad_norm: 1.0831121471710503e-05
Step: 39890, train/learning_rate: 2.53450730269833e-06
Step: 39890, train/epoch: 9.493098258972168
Step: 39900, train/loss: 0.0
Step: 39900, train/grad_norm: 7.114766162885644e-07
Step: 39900, train/learning_rate: 2.522608383515035e-06
Step: 39900, train/epoch: 9.495478630065918
Step: 39910, train/loss: 0.0
Step: 39910, train/grad_norm: 2.0851921078701707e-07
Step: 39910, train/learning_rate: 2.510709236958064e-06
Step: 39910, train/epoch: 9.497858047485352
Step: 39920, train/loss: 0.0
Step: 39920, train/grad_norm: 1.7875692719826475e-05
Step: 39920, train/learning_rate: 2.4988100904010935e-06
Step: 39920, train/epoch: 9.500238418579102
Step: 39930, train/loss: 0.0
Step: 39930, train/grad_norm: 1.894604082508522e-07
Step: 39930, train/learning_rate: 2.486910943844123e-06
Step: 39930, train/epoch: 9.502617835998535
Step: 39940, train/loss: 0.0
Step: 39940, train/grad_norm: 1.453248501093185e-06
Step: 39940, train/learning_rate: 2.475011797287152e-06
Step: 39940, train/epoch: 9.504997253417969
Step: 39950, train/loss: 0.0
Step: 39950, train/grad_norm: 1.7764195945346728e-05
Step: 39950, train/learning_rate: 2.463112878103857e-06
Step: 39950, train/epoch: 9.507377624511719
Step: 39960, train/loss: 0.0
Step: 39960, train/grad_norm: 1.2591286235874577e-07
Step: 39960, train/learning_rate: 2.4512137315468863e-06
Step: 39960, train/epoch: 9.509757041931152
Step: 39970, train/loss: 0.0
Step: 39970, train/grad_norm: 1.570081803947687e-05
Step: 39970, train/learning_rate: 2.4393145849899156e-06
Step: 39970, train/epoch: 9.512137413024902
Step: 39980, train/loss: 0.0
Step: 39980, train/grad_norm: 1.0842409636779848e-07
Step: 39980, train/learning_rate: 2.427415438432945e-06
Step: 39980, train/epoch: 9.514516830444336
Step: 39990, train/loss: 0.0
Step: 39990, train/grad_norm: 2.2022474510663415e-08
Step: 39990, train/learning_rate: 2.4155165192496497e-06
Step: 39990, train/epoch: 9.51689624786377
Step: 40000, train/loss: 0.0
Step: 40000, train/grad_norm: 1.49891491219023e-07
Step: 40000, train/learning_rate: 2.403617372692679e-06
Step: 40000, train/epoch: 9.51927661895752
Step: 40010, train/loss: 0.0
Step: 40010, train/grad_norm: 1.2919952496304177e-05
Step: 40010, train/learning_rate: 2.3917182261357084e-06
Step: 40010, train/epoch: 9.521656036376953
Step: 40020, train/loss: 0.0
Step: 40020, train/grad_norm: 1.979077023861464e-05
Step: 40020, train/learning_rate: 2.3798190795787377e-06
Step: 40020, train/epoch: 9.524036407470703
Step: 40030, train/loss: 0.0
Step: 40030, train/grad_norm: 8.377638778256369e-07
Step: 40030, train/learning_rate: 2.367919933021767e-06
Step: 40030, train/epoch: 9.526415824890137
Step: 40040, train/loss: 0.0
Step: 40040, train/grad_norm: 1.0770804692583624e-07
Step: 40040, train/learning_rate: 2.356021013838472e-06
Step: 40040, train/epoch: 9.528796195983887
Step: 40050, train/loss: 0.0
Step: 40050, train/grad_norm: 2.4720556268675864e-08
Step: 40050, train/learning_rate: 2.344121867281501e-06
Step: 40050, train/epoch: 9.53117561340332
Step: 40060, train/loss: 0.0
Step: 40060, train/grad_norm: 1.3065598977846093e-05
Step: 40060, train/learning_rate: 2.3322227207245305e-06
Step: 40060, train/epoch: 9.533555030822754
Step: 40070, train/loss: 0.0
Step: 40070, train/grad_norm: 5.932379281148314e-07
Step: 40070, train/learning_rate: 2.32032357416756e-06
Step: 40070, train/epoch: 9.535935401916504
Step: 40080, train/loss: 0.0
Step: 40080, train/grad_norm: 2.1931836613475753e-07
Step: 40080, train/learning_rate: 2.3084246549842646e-06
Step: 40080, train/epoch: 9.538314819335938
Step: 40090, train/loss: 0.0
Step: 40090, train/grad_norm: 3.151494411213207e-06
Step: 40090, train/learning_rate: 2.296525508427294e-06
Step: 40090, train/epoch: 9.540695190429688
Step: 40100, train/loss: 0.0
Step: 40100, train/grad_norm: 5.20592379871232e-07
Step: 40100, train/learning_rate: 2.2846263618703233e-06
Step: 40100, train/epoch: 9.543074607849121
Step: 40110, train/loss: 0.0
Step: 40110, train/grad_norm: 6.54936798127892e-07
Step: 40110, train/learning_rate: 2.2727272153133526e-06
Step: 40110, train/epoch: 9.545454978942871
Step: 40120, train/loss: 0.0
Step: 40120, train/grad_norm: 3.342317143051332e-07
Step: 40120, train/learning_rate: 2.260828068756382e-06
Step: 40120, train/epoch: 9.547834396362305
Step: 40130, train/loss: 0.0
Step: 40130, train/grad_norm: 6.927122626620985e-07
Step: 40130, train/learning_rate: 2.2489291495730868e-06
Step: 40130, train/epoch: 9.550213813781738
Step: 40140, train/loss: 0.0
Step: 40140, train/grad_norm: 7.01909539202461e-06
Step: 40140, train/learning_rate: 2.237030003016116e-06
Step: 40140, train/epoch: 9.552594184875488
Step: 40150, train/loss: 0.0
Step: 40150, train/grad_norm: 1.2276483175810426e-05
Step: 40150, train/learning_rate: 2.2251308564591454e-06
Step: 40150, train/epoch: 9.554973602294922
Step: 40160, train/loss: 0.0
Step: 40160, train/grad_norm: 1.542183235869743e-05
Step: 40160, train/learning_rate: 2.2132317099021748e-06
Step: 40160, train/epoch: 9.557353973388672
Step: 40170, train/loss: 0.0
Step: 40170, train/grad_norm: 2.930557307934123e-08
Step: 40170, train/learning_rate: 2.2013327907188796e-06
Step: 40170, train/epoch: 9.559733390808105
Step: 40180, train/loss: 0.0
Step: 40180, train/grad_norm: 5.193106517253909e-06
Step: 40180, train/learning_rate: 2.189433644161909e-06
Step: 40180, train/epoch: 9.562112808227539
Step: 40190, train/loss: 0.0
Step: 40190, train/grad_norm: 5.62533557513234e-07
Step: 40190, train/learning_rate: 2.1775344976049382e-06
Step: 40190, train/epoch: 9.564493179321289
Step: 40200, train/loss: 0.0
Step: 40200, train/grad_norm: 4.349150458438089e-06
Step: 40200, train/learning_rate: 2.1656353510479676e-06
Step: 40200, train/epoch: 9.566872596740723
Step: 40210, train/loss: 0.0
Step: 40210, train/grad_norm: 5.926116841692419e-07
Step: 40210, train/learning_rate: 2.153736204490997e-06
Step: 40210, train/epoch: 9.569252967834473
Step: 40220, train/loss: 0.0
Step: 40220, train/grad_norm: 0.00013787411444354802
Step: 40220, train/learning_rate: 2.1418372853077017e-06
Step: 40220, train/epoch: 9.571632385253906
Step: 40230, train/loss: 0.0
Step: 40230, train/grad_norm: 3.8681089790770784e-05
Step: 40230, train/learning_rate: 2.129938138750731e-06
Step: 40230, train/epoch: 9.574012756347656
Step: 40240, train/loss: 0.0
Step: 40240, train/grad_norm: 2.8163608476461377e-06
Step: 40240, train/learning_rate: 2.1180389921937604e-06
Step: 40240, train/epoch: 9.57639217376709
Step: 40250, train/loss: 0.0
Step: 40250, train/grad_norm: 6.190543899720069e-06
Step: 40250, train/learning_rate: 2.1061398456367897e-06
Step: 40250, train/epoch: 9.578771591186523
Step: 40260, train/loss: 0.0
Step: 40260, train/grad_norm: 2.8195438517286675e-07
Step: 40260, train/learning_rate: 2.0942409264534945e-06
Step: 40260, train/epoch: 9.581151962280273
Step: 40270, train/loss: 0.0
Step: 40270, train/grad_norm: 5.373152589527308e-07
Step: 40270, train/learning_rate: 2.082341779896524e-06
Step: 40270, train/epoch: 9.583531379699707
Step: 40280, train/loss: 0.0
Step: 40280, train/grad_norm: 8.273761409327562e-07
Step: 40280, train/learning_rate: 2.070442633339553e-06
Step: 40280, train/epoch: 9.585911750793457
Step: 40290, train/loss: 0.0
Step: 40290, train/grad_norm: 5.474011643968879e-08
Step: 40290, train/learning_rate: 2.0585434867825825e-06
Step: 40290, train/epoch: 9.58829116821289
Step: 40300, train/loss: 0.0
Step: 40300, train/grad_norm: 3.6711905977426795e-06
Step: 40300, train/learning_rate: 2.0466445675992873e-06
Step: 40300, train/epoch: 9.59067153930664
Step: 40310, train/loss: 0.0
Step: 40310, train/grad_norm: 4.2544050415926904e-07
Step: 40310, train/learning_rate: 2.0347454210423166e-06
Step: 40310, train/epoch: 9.593050956726074
Step: 40320, train/loss: 0.0
Step: 40320, train/grad_norm: 1.1318092418832748e-07
Step: 40320, train/learning_rate: 2.022846274485346e-06
Step: 40320, train/epoch: 9.595430374145508
Step: 40330, train/loss: 0.0
Step: 40330, train/grad_norm: 1.685082366975621e-07
Step: 40330, train/learning_rate: 2.0109471279283753e-06
Step: 40330, train/epoch: 9.597810745239258
Step: 40340, train/loss: 0.0
Step: 40340, train/grad_norm: 1.992543303686034e-07
Step: 40340, train/learning_rate: 1.9990479813714046e-06
Step: 40340, train/epoch: 9.600190162658691
Step: 40350, train/loss: 0.0
Step: 40350, train/grad_norm: 3.6706131822938914e-07
Step: 40350, train/learning_rate: 1.9871490621881094e-06
Step: 40350, train/epoch: 9.602570533752441
Step: 40360, train/loss: 0.0
Step: 40360, train/grad_norm: 9.743351256474853e-05
Step: 40360, train/learning_rate: 1.9752499156311387e-06
Step: 40360, train/epoch: 9.604949951171875
Step: 40370, train/loss: 0.0
Step: 40370, train/grad_norm: 1.4452998584602028e-06
Step: 40370, train/learning_rate: 1.963350769074168e-06
Step: 40370, train/epoch: 9.607329368591309
Step: 40380, train/loss: 0.0
Step: 40380, train/grad_norm: 5.61810260535367e-08
Step: 40380, train/learning_rate: 1.9514516225171974e-06
Step: 40380, train/epoch: 9.609709739685059
Step: 40390, train/loss: 0.0
Step: 40390, train/grad_norm: 4.988641535419447e-07
Step: 40390, train/learning_rate: 1.939552703333902e-06
Step: 40390, train/epoch: 9.612089157104492
Step: 40400, train/loss: 0.0
Step: 40400, train/grad_norm: 6.529458573822922e-07
Step: 40400, train/learning_rate: 1.9276535567769315e-06
Step: 40400, train/epoch: 9.614469528198242
Step: 40410, train/loss: 0.0
Step: 40410, train/grad_norm: 2.3339504551245227e-08
Step: 40410, train/learning_rate: 1.915754410219961e-06
Step: 40410, train/epoch: 9.616848945617676
Step: 40420, train/loss: 0.0
Step: 40420, train/grad_norm: 3.5226009913458256e-06
Step: 40420, train/learning_rate: 1.9038552636629902e-06
Step: 40420, train/epoch: 9.619229316711426
Step: 40430, train/loss: 0.0
Step: 40430, train/grad_norm: 5.124760704688924e-08
Step: 40430, train/learning_rate: 1.8919562307928572e-06
Step: 40430, train/epoch: 9.62160873413086
Step: 40440, train/loss: 0.0
Step: 40440, train/grad_norm: 2.409291823823878e-07
Step: 40440, train/learning_rate: 1.8800570842358866e-06
Step: 40440, train/epoch: 9.623988151550293
Step: 40450, train/loss: 0.0
Step: 40450, train/grad_norm: 5.106905405227735e-07
Step: 40450, train/learning_rate: 1.8681580513657536e-06
Step: 40450, train/epoch: 9.626368522644043
Step: 40460, train/loss: 0.0
Step: 40460, train/grad_norm: 1.0838533626156277e-06
Step: 40460, train/learning_rate: 1.856258904808783e-06
Step: 40460, train/epoch: 9.628747940063477
Step: 40470, train/loss: 0.0
Step: 40470, train/grad_norm: 1.196444827655796e-06
Step: 40470, train/learning_rate: 1.84435987193865e-06
Step: 40470, train/epoch: 9.631128311157227
Step: 40480, train/loss: 0.0
Step: 40480, train/grad_norm: 3.7104217653904925e-07
Step: 40480, train/learning_rate: 1.8324607253816794e-06
Step: 40480, train/epoch: 9.63350772857666
Step: 40490, train/loss: 0.0
Step: 40490, train/grad_norm: 1.1392897249606904e-05
Step: 40490, train/learning_rate: 1.8205616925115464e-06
Step: 40490, train/epoch: 9.63588809967041
Step: 40500, train/loss: 0.0
Step: 40500, train/grad_norm: 8.195385817089118e-06
Step: 40500, train/learning_rate: 1.8086625459545758e-06
Step: 40500, train/epoch: 9.638267517089844
Step: 40510, train/loss: 0.0
Step: 40510, train/grad_norm: 7.373139396804618e-07
Step: 40510, train/learning_rate: 1.796763399397605e-06
Step: 40510, train/epoch: 9.640646934509277
Step: 40520, train/loss: 0.0
Step: 40520, train/grad_norm: 4.085912223672494e-06
Step: 40520, train/learning_rate: 1.7848643665274722e-06
Step: 40520, train/epoch: 9.643027305603027
Step: 40530, train/loss: 0.0
Step: 40530, train/grad_norm: 2.6611681391841557e-07
Step: 40530, train/learning_rate: 1.7729652199705015e-06
Step: 40530, train/epoch: 9.645406723022461
Step: 40540, train/loss: 0.0
Step: 40540, train/grad_norm: 1.4169826556553744e-07
Step: 40540, train/learning_rate: 1.7610661871003686e-06
Step: 40540, train/epoch: 9.647787094116211
Step: 40550, train/loss: 0.0
Step: 40550, train/grad_norm: 8.956795682024676e-06
Step: 40550, train/learning_rate: 1.7491670405433979e-06
Step: 40550, train/epoch: 9.650166511535645
Step: 40560, train/loss: 0.0
Step: 40560, train/grad_norm: 2.8829512643824273e-07
Step: 40560, train/learning_rate: 1.737268007673265e-06
Step: 40560, train/epoch: 9.652546882629395
Step: 40570, train/loss: 0.0
Step: 40570, train/grad_norm: 5.056315330875805e-07
Step: 40570, train/learning_rate: 1.7253688611162943e-06
Step: 40570, train/epoch: 9.654926300048828
Step: 40580, train/loss: 0.0
Step: 40580, train/grad_norm: 5.726534072891809e-05
Step: 40580, train/learning_rate: 1.7134698282461613e-06
Step: 40580, train/epoch: 9.657305717468262
Step: 40590, train/loss: 0.0
Step: 40590, train/grad_norm: 4.632069703802699e-06
Step: 40590, train/learning_rate: 1.7015706816891907e-06
Step: 40590, train/epoch: 9.659686088562012
Step: 40600, train/loss: 0.0
Step: 40600, train/grad_norm: 1.3163781886760262e-07
Step: 40600, train/learning_rate: 1.68967153513222e-06
Step: 40600, train/epoch: 9.662065505981445
Step: 40610, train/loss: 0.0
Step: 40610, train/grad_norm: 4.088180958206067e-07
Step: 40610, train/learning_rate: 1.677772502262087e-06
Step: 40610, train/epoch: 9.664445877075195
Step: 40620, train/loss: 0.0
Step: 40620, train/grad_norm: 9.065489052773046e-07
Step: 40620, train/learning_rate: 1.6658733557051164e-06
Step: 40620, train/epoch: 9.666825294494629
Step: 40630, train/loss: 0.0
Step: 40630, train/grad_norm: 2.2310220515464607e-07
Step: 40630, train/learning_rate: 1.6539743228349835e-06
Step: 40630, train/epoch: 9.669204711914062
Step: 40640, train/loss: 0.0
Step: 40640, train/grad_norm: 6.324177093119943e-07
Step: 40640, train/learning_rate: 1.6420751762780128e-06
Step: 40640, train/epoch: 9.671585083007812
Step: 40650, train/loss: 0.0
Step: 40650, train/grad_norm: 1.5532057773270935e-07
Step: 40650, train/learning_rate: 1.6301761434078799e-06
Step: 40650, train/epoch: 9.673964500427246
Step: 40660, train/loss: 0.0
Step: 40660, train/grad_norm: 3.351993029809819e-07
Step: 40660, train/learning_rate: 1.6182769968509092e-06
Step: 40660, train/epoch: 9.676344871520996
Step: 40670, train/loss: 0.0
Step: 40670, train/grad_norm: 9.484527254244313e-06
Step: 40670, train/learning_rate: 1.6063779639807763e-06
Step: 40670, train/epoch: 9.67872428894043
Step: 40680, train/loss: 0.0
Step: 40680, train/grad_norm: 9.083515806196374e-08
Step: 40680, train/learning_rate: 1.5944788174238056e-06
Step: 40680, train/epoch: 9.68110466003418
Step: 40690, train/loss: 0.0
Step: 40690, train/grad_norm: 1.2820881238440052e-05
Step: 40690, train/learning_rate: 1.582579670866835e-06
Step: 40690, train/epoch: 9.683484077453613
Step: 40700, train/loss: 0.0
Step: 40700, train/grad_norm: 6.428319920814829e-07
Step: 40700, train/learning_rate: 1.570680637996702e-06
Step: 40700, train/epoch: 9.685863494873047
Step: 40710, train/loss: 0.0
Step: 40710, train/grad_norm: 3.9964626807886816e-07
Step: 40710, train/learning_rate: 1.5587814914397313e-06
Step: 40710, train/epoch: 9.688243865966797
Step: 40720, train/loss: 0.0
Step: 40720, train/grad_norm: 1.9752374100789893e-06
Step: 40720, train/learning_rate: 1.5468824585695984e-06
Step: 40720, train/epoch: 9.69062328338623
Step: 40730, train/loss: 0.0
Step: 40730, train/grad_norm: 1.9597698042161937e-07
Step: 40730, train/learning_rate: 1.5349833120126277e-06
Step: 40730, train/epoch: 9.69300365447998
Step: 40740, train/loss: 0.0
Step: 40740, train/grad_norm: 9.317745934822597e-06
Step: 40740, train/learning_rate: 1.5230842791424948e-06
Step: 40740, train/epoch: 9.695383071899414
Step: 40750, train/loss: 0.0
Step: 40750, train/grad_norm: 3.007663096354918e-08
Step: 40750, train/learning_rate: 1.5111851325855241e-06
Step: 40750, train/epoch: 9.697763442993164
Step: 40760, train/loss: 0.0
Step: 40760, train/grad_norm: 4.346109562902711e-06
Step: 40760, train/learning_rate: 1.4992860997153912e-06
Step: 40760, train/epoch: 9.700142860412598
Step: 40770, train/loss: 0.0
Step: 40770, train/grad_norm: 9.837934157985728e-06
Step: 40770, train/learning_rate: 1.4873869531584205e-06
Step: 40770, train/epoch: 9.702522277832031
Step: 40780, train/loss: 0.0
Step: 40780, train/grad_norm: 7.330944384875693e-08
Step: 40780, train/learning_rate: 1.4754878066014498e-06
Step: 40780, train/epoch: 9.704902648925781
Step: 40790, train/loss: 0.0
Step: 40790, train/grad_norm: 1.5540977983619086e-05
Step: 40790, train/learning_rate: 1.463588773731317e-06
Step: 40790, train/epoch: 9.707282066345215
Step: 40800, train/loss: 0.0
Step: 40800, train/grad_norm: 2.7911849542761047e-07
Step: 40800, train/learning_rate: 1.4516896271743462e-06
Step: 40800, train/epoch: 9.709662437438965
Step: 40810, train/loss: 0.0
Step: 40810, train/grad_norm: 0.0005494889919646084
Step: 40810, train/learning_rate: 1.4397905943042133e-06
Step: 40810, train/epoch: 9.712041854858398
Step: 40820, train/loss: 0.0
Step: 40820, train/grad_norm: 5.622201570076868e-06
Step: 40820, train/learning_rate: 1.4278914477472426e-06
Step: 40820, train/epoch: 9.714421272277832
Step: 40830, train/loss: 0.0
Step: 40830, train/grad_norm: 3.482476586214034e-06
Step: 40830, train/learning_rate: 1.4159924148771097e-06
Step: 40830, train/epoch: 9.716801643371582
Step: 40840, train/loss: 0.0
Step: 40840, train/grad_norm: 7.330701805585704e-07
Step: 40840, train/learning_rate: 1.404093268320139e-06
Step: 40840, train/epoch: 9.719181060791016
Step: 40850, train/loss: 0.0
Step: 40850, train/grad_norm: 9.658082854002714e-05
Step: 40850, train/learning_rate: 1.392194235450006e-06
Step: 40850, train/epoch: 9.721561431884766
Step: 40860, train/loss: 0.0
Step: 40860, train/grad_norm: 1.184226050554571e-08
Step: 40860, train/learning_rate: 1.3802950888930354e-06
Step: 40860, train/epoch: 9.7239408493042
Step: 40870, train/loss: 0.0
Step: 40870, train/grad_norm: 1.989525117096491e-06
Step: 40870, train/learning_rate: 1.3683960560229025e-06
Step: 40870, train/epoch: 9.72632122039795
Step: 40880, train/loss: 0.0
Step: 40880, train/grad_norm: 1.0561697649791313e-07
Step: 40880, train/learning_rate: 1.3564969094659318e-06
Step: 40880, train/epoch: 9.728700637817383
Step: 40890, train/loss: 0.0
Step: 40890, train/grad_norm: 1.0909752035104248e-07
Step: 40890, train/learning_rate: 1.3445977629089612e-06
Step: 40890, train/epoch: 9.731080055236816
Step: 40900, train/loss: 0.0
Step: 40900, train/grad_norm: 7.0221703936113045e-06
Step: 40900, train/learning_rate: 1.3326987300388282e-06
Step: 40900, train/epoch: 9.733460426330566
Step: 40910, train/loss: 0.0
Step: 40910, train/grad_norm: 4.5193436903900874e-07
Step: 40910, train/learning_rate: 1.3207995834818576e-06
Step: 40910, train/epoch: 9.73583984375
Step: 40920, train/loss: 0.0
Step: 40920, train/grad_norm: 4.552254904410802e-05
Step: 40920, train/learning_rate: 1.3089005506117246e-06
Step: 40920, train/epoch: 9.73822021484375
Step: 40930, train/loss: 0.0
Step: 40930, train/grad_norm: 4.606768470694078e-06
Step: 40930, train/learning_rate: 1.297001404054754e-06
Step: 40930, train/epoch: 9.740599632263184
Step: 40940, train/loss: 0.0
Step: 40940, train/grad_norm: 1.196531002278789e-06
Step: 40940, train/learning_rate: 1.285102371184621e-06
Step: 40940, train/epoch: 9.742980003356934
Step: 40950, train/loss: 0.0
Step: 40950, train/grad_norm: 2.677225268143957e-07
Step: 40950, train/learning_rate: 1.2732032246276503e-06
Step: 40950, train/epoch: 9.745359420776367
Step: 40960, train/loss: 0.0
Step: 40960, train/grad_norm: 1.1046975600947917e-07
Step: 40960, train/learning_rate: 1.2613041917575174e-06
Step: 40960, train/epoch: 9.7477388381958
Step: 40970, train/loss: 0.0
Step: 40970, train/grad_norm: 2.2359348150757796e-08
Step: 40970, train/learning_rate: 1.2494050452005467e-06
Step: 40970, train/epoch: 9.75011920928955
Step: 40980, train/loss: 0.0
Step: 40980, train/grad_norm: 1.139503353897453e-07
Step: 40980, train/learning_rate: 1.237505898643576e-06
Step: 40980, train/epoch: 9.752498626708984
Step: 40990, train/loss: 0.0
Step: 40990, train/grad_norm: 4.1172509668285784e-07
Step: 40990, train/learning_rate: 1.2256068657734431e-06
Step: 40990, train/epoch: 9.754878997802734
Step: 41000, train/loss: 0.0
Step: 41000, train/grad_norm: 9.817081263463479e-07
Step: 41000, train/learning_rate: 1.2137077192164725e-06
Step: 41000, train/epoch: 9.757258415222168
Step: 41010, train/loss: 0.0
Step: 41010, train/grad_norm: 1.1955820809816942e-05
Step: 41010, train/learning_rate: 1.2018086863463395e-06
Step: 41010, train/epoch: 9.759637832641602
Step: 41020, train/loss: 0.0
Step: 41020, train/grad_norm: 1.3781574637050653e-07
Step: 41020, train/learning_rate: 1.1899095397893689e-06
Step: 41020, train/epoch: 9.762018203735352
Step: 41030, train/loss: 0.0
Step: 41030, train/grad_norm: 4.6302727696456714e-07
Step: 41030, train/learning_rate: 1.178010506919236e-06
Step: 41030, train/epoch: 9.764397621154785
Step: 41040, train/loss: 0.0
Step: 41040, train/grad_norm: 0.0005609028739854693
Step: 41040, train/learning_rate: 1.1661113603622653e-06
Step: 41040, train/epoch: 9.766777992248535
Step: 41050, train/loss: 0.0
Step: 41050, train/grad_norm: 5.151229288458126e-06
Step: 41050, train/learning_rate: 1.1542123274921323e-06
Step: 41050, train/epoch: 9.769157409667969
Step: 41060, train/loss: 0.0
Step: 41060, train/grad_norm: 7.093285034898145e-07
Step: 41060, train/learning_rate: 1.1423131809351617e-06
Step: 41060, train/epoch: 9.771537780761719
Step: 41070, train/loss: 0.0
Step: 41070, train/grad_norm: 8.195139571398613e-07
Step: 41070, train/learning_rate: 1.130414034378191e-06
Step: 41070, train/epoch: 9.773917198181152
Step: 41080, train/loss: 0.0
Step: 41080, train/grad_norm: 2.6538004021858796e-05
Step: 41080, train/learning_rate: 1.118515001508058e-06
Step: 41080, train/epoch: 9.776296615600586
Step: 41090, train/loss: 0.0
Step: 41090, train/grad_norm: 7.373370181085193e-08
Step: 41090, train/learning_rate: 1.1066158549510874e-06
Step: 41090, train/epoch: 9.778676986694336
Step: 41100, train/loss: 0.0
Step: 41100, train/grad_norm: 1.0166170341108227e-06
Step: 41100, train/learning_rate: 1.0947168220809544e-06
Step: 41100, train/epoch: 9.78105640411377
Step: 41110, train/loss: 0.0
Step: 41110, train/grad_norm: 3.4764973406709032e-06
Step: 41110, train/learning_rate: 1.0828176755239838e-06
Step: 41110, train/epoch: 9.78343677520752
Step: 41120, train/loss: 0.0
Step: 41120, train/grad_norm: 2.0401157598826103e-06
Step: 41120, train/learning_rate: 1.0709186426538508e-06
Step: 41120, train/epoch: 9.785816192626953
Step: 41130, train/loss: 0.0
Step: 41130, train/grad_norm: 1.0861763257707935e-06
Step: 41130, train/learning_rate: 1.0590194960968802e-06
Step: 41130, train/epoch: 9.788196563720703
Step: 41140, train/loss: 0.0
Step: 41140, train/grad_norm: 1.942160633916501e-05
Step: 41140, train/learning_rate: 1.0471204632267472e-06
Step: 41140, train/epoch: 9.790575981140137
Step: 41150, train/loss: 0.0
Step: 41150, train/grad_norm: 5.264047285891138e-05
Step: 41150, train/learning_rate: 1.0352213166697766e-06
Step: 41150, train/epoch: 9.79295539855957
Step: 41160, train/loss: 0.0
Step: 41160, train/grad_norm: 8.25046811314678e-07
Step: 41160, train/learning_rate: 1.0233222837996436e-06
Step: 41160, train/epoch: 9.79533576965332
Step: 41170, train/loss: 0.0
Step: 41170, train/grad_norm: 3.289204687462188e-05
Step: 41170, train/learning_rate: 1.011423137242673e-06
Step: 41170, train/epoch: 9.797715187072754
Step: 41180, train/loss: 0.0
Step: 41180, train/grad_norm: 1.083586539607495e-06
Step: 41180, train/learning_rate: 9.995239906857023e-07
Step: 41180, train/epoch: 9.800095558166504
Step: 41190, train/loss: 0.0
Step: 41190, train/grad_norm: 4.6138695324771106e-05
Step: 41190, train/learning_rate: 9.876249578155694e-07
Step: 41190, train/epoch: 9.802474975585938
Step: 41200, train/loss: 0.0
Step: 41200, train/grad_norm: 3.1480837492381397e-07
Step: 41200, train/learning_rate: 9.757258112585987e-07
Step: 41200, train/epoch: 9.804854393005371
Step: 41210, train/loss: 0.0
Step: 41210, train/grad_norm: 5.083905563196822e-08
Step: 41210, train/learning_rate: 9.638267783884658e-07
Step: 41210, train/epoch: 9.807234764099121
Step: 41220, train/loss: 0.0
Step: 41220, train/grad_norm: 6.330531618914392e-07
Step: 41220, train/learning_rate: 9.519276318314951e-07
Step: 41220, train/epoch: 9.809614181518555
Step: 41230, train/loss: 0.0
Step: 41230, train/grad_norm: 4.237505493165372e-07
Step: 41230, train/learning_rate: 9.400285421179433e-07
Step: 41230, train/epoch: 9.811994552612305
Step: 41240, train/loss: 0.0
Step: 41240, train/grad_norm: 3.0159569064380776e-07
Step: 41240, train/learning_rate: 9.281294524043915e-07
Step: 41240, train/epoch: 9.814373970031738
Step: 41250, train/loss: 0.0
Step: 41250, train/grad_norm: 3.213634499843465e-06
Step: 41250, train/learning_rate: 9.162303626908397e-07
Step: 41250, train/epoch: 9.816754341125488
Step: 41260, train/loss: 0.0
Step: 41260, train/grad_norm: 1.5544998177574598e-07
Step: 41260, train/learning_rate: 9.043312729772879e-07
Step: 41260, train/epoch: 9.819133758544922
Step: 41270, train/loss: 0.0
Step: 41270, train/grad_norm: 4.4926396185474005e-06
Step: 41270, train/learning_rate: 8.924321832637361e-07
Step: 41270, train/epoch: 9.821513175964355
Step: 41280, train/loss: 0.0
Step: 41280, train/grad_norm: 1.5479056969525118e-07
Step: 41280, train/learning_rate: 8.805330935501843e-07
Step: 41280, train/epoch: 9.823893547058105
Step: 41290, train/loss: 0.0
Step: 41290, train/grad_norm: 5.113116117172467e-07
Step: 41290, train/learning_rate: 8.686340038366325e-07
Step: 41290, train/epoch: 9.826272964477539
Step: 41300, train/loss: 0.0
Step: 41300, train/grad_norm: 1.738148256436034e-07
Step: 41300, train/learning_rate: 8.567349141230807e-07
Step: 41300, train/epoch: 9.828653335571289
Step: 41310, train/loss: 0.0
Step: 41310, train/grad_norm: 3.818471304839477e-05
Step: 41310, train/learning_rate: 8.4483576756611e-07
Step: 41310, train/epoch: 9.831032752990723
Step: 41320, train/loss: 0.0
Step: 41320, train/grad_norm: 1.7169901411762112e-07
Step: 41320, train/learning_rate: 8.329366778525582e-07
Step: 41320, train/epoch: 9.833413124084473
Step: 41330, train/loss: 0.0
Step: 41330, train/grad_norm: 4.402416834636824e-06
Step: 41330, train/learning_rate: 8.210375881390064e-07
Step: 41330, train/epoch: 9.835792541503906
Step: 41340, train/loss: 0.0
Step: 41340, train/grad_norm: 4.044677552883513e-06
Step: 41340, train/learning_rate: 8.091384984254546e-07
Step: 41340, train/epoch: 9.83817195892334
Step: 41350, train/loss: 0.0
Step: 41350, train/grad_norm: 5.210360427554406e-07
Step: 41350, train/learning_rate: 7.972394087119028e-07
Step: 41350, train/epoch: 9.84055233001709
Step: 41360, train/loss: 0.0
Step: 41360, train/grad_norm: 1.9689176042447798e-05
Step: 41360, train/learning_rate: 7.85340318998351e-07
Step: 41360, train/epoch: 9.842931747436523
Step: 41370, train/loss: 0.0
Step: 41370, train/grad_norm: 5.433460614767682e-07
Step: 41370, train/learning_rate: 7.734412292847992e-07
Step: 41370, train/epoch: 9.845312118530273
Step: 41380, train/loss: 0.0
Step: 41380, train/grad_norm: 3.799664796133584e-07
Step: 41380, train/learning_rate: 7.615421395712474e-07
Step: 41380, train/epoch: 9.847691535949707
Step: 41390, train/loss: 0.0
Step: 41390, train/grad_norm: 1.4536730930103658e-07
Step: 41390, train/learning_rate: 7.496430498576956e-07
Step: 41390, train/epoch: 9.85007095336914
Step: 41400, train/loss: 0.0
Step: 41400, train/grad_norm: 4.079910468135495e-06
Step: 41400, train/learning_rate: 7.377439033007249e-07
Step: 41400, train/epoch: 9.85245132446289
Step: 41410, train/loss: 0.0
Step: 41410, train/grad_norm: 4.049467960953734e-08
Step: 41410, train/learning_rate: 7.258448135871731e-07
Step: 41410, train/epoch: 9.854830741882324
Step: 41420, train/loss: 0.0
Step: 41420, train/grad_norm: 3.4769081480590103e-07
Step: 41420, train/learning_rate: 7.139457238736213e-07
Step: 41420, train/epoch: 9.857211112976074
Step: 41430, train/loss: 0.0
Step: 41430, train/grad_norm: 4.716934654425131e-06
Step: 41430, train/learning_rate: 7.020466341600695e-07
Step: 41430, train/epoch: 9.859590530395508
Step: 41440, train/loss: 0.0
Step: 41440, train/grad_norm: 1.2693110875261482e-05
Step: 41440, train/learning_rate: 6.901475444465177e-07
Step: 41440, train/epoch: 9.861970901489258
Step: 41450, train/loss: 0.0
Step: 41450, train/grad_norm: 1.2628936119085665e-08
Step: 41450, train/learning_rate: 6.782484547329659e-07
Step: 41450, train/epoch: 9.864350318908691
Step: 41460, train/loss: 0.0
Step: 41460, train/grad_norm: 7.158952212193981e-05
Step: 41460, train/learning_rate: 6.663493650194141e-07
Step: 41460, train/epoch: 9.866729736328125
Step: 41470, train/loss: 0.0
Step: 41470, train/grad_norm: 2.1393151428128476e-07
Step: 41470, train/learning_rate: 6.544502753058623e-07
Step: 41470, train/epoch: 9.869110107421875
Step: 41480, train/loss: 0.0
Step: 41480, train/grad_norm: 4.210350198263768e-06
Step: 41480, train/learning_rate: 6.425511855923105e-07
Step: 41480, train/epoch: 9.871489524841309
Step: 41490, train/loss: 0.0
Step: 41490, train/grad_norm: 1.7749516700860113e-07
Step: 41490, train/learning_rate: 6.306520958787587e-07
Step: 41490, train/epoch: 9.873869895935059
Step: 41500, train/loss: 0.0
Step: 41500, train/grad_norm: 7.83134310040623e-05
Step: 41500, train/learning_rate: 6.18752949321788e-07
Step: 41500, train/epoch: 9.876249313354492
Step: 41510, train/loss: 0.0
Step: 41510, train/grad_norm: 1.589538101143262e-07
Step: 41510, train/learning_rate: 6.068538596082362e-07
Step: 41510, train/epoch: 9.878629684448242
Step: 41520, train/loss: 0.0
Step: 41520, train/grad_norm: 2.2392026949091814e-05
Step: 41520, train/learning_rate: 5.949547698946844e-07
Step: 41520, train/epoch: 9.881009101867676
Step: 41530, train/loss: 0.0
Step: 41530, train/grad_norm: 2.2001174215802166e-08
Step: 41530, train/learning_rate: 5.830556801811326e-07
Step: 41530, train/epoch: 9.88338851928711
Step: 41540, train/loss: 0.0
Step: 41540, train/grad_norm: 8.804342542134691e-06
Step: 41540, train/learning_rate: 5.711565904675808e-07
Step: 41540, train/epoch: 9.88576889038086
Step: 41550, train/loss: 0.0
Step: 41550, train/grad_norm: 5.331205829861574e-05
Step: 41550, train/learning_rate: 5.59257500754029e-07
Step: 41550, train/epoch: 9.888148307800293
Step: 41560, train/loss: 0.0
Step: 41560, train/grad_norm: 1.2730305343211512e-06
Step: 41560, train/learning_rate: 5.473584110404772e-07
Step: 41560, train/epoch: 9.890528678894043
Step: 41570, train/loss: 0.0
Step: 41570, train/grad_norm: 3.9431456571037415e-06
Step: 41570, train/learning_rate: 5.354593213269254e-07
Step: 41570, train/epoch: 9.892908096313477
Step: 41580, train/loss: 0.0
Step: 41580, train/grad_norm: 7.020487942099862e-07
Step: 41580, train/learning_rate: 5.235602316133736e-07
Step: 41580, train/epoch: 9.89528751373291
Step: 41590, train/loss: 0.0
Step: 41590, train/grad_norm: 2.077341036965663e-07
Step: 41590, train/learning_rate: 5.116611418998218e-07
Step: 41590, train/epoch: 9.89766788482666
Step: 41600, train/loss: 0.0
Step: 41600, train/grad_norm: 8.278291716123931e-06
Step: 41600, train/learning_rate: 4.997619953428512e-07
Step: 41600, train/epoch: 9.900047302246094
Step: 41610, train/loss: 0.0
Step: 41610, train/grad_norm: 1.039824837789638e-06
Step: 41610, train/learning_rate: 4.878629056292993e-07
Step: 41610, train/epoch: 9.902427673339844
Step: 41620, train/loss: 0.0
Step: 41620, train/grad_norm: 6.999268293839123e-07
Step: 41620, train/learning_rate: 4.7596381591574755e-07
Step: 41620, train/epoch: 9.904807090759277
Step: 41630, train/loss: 0.0
Step: 41630, train/grad_norm: 7.396586454433418e-08
Step: 41630, train/learning_rate: 4.6406472620219574e-07
Step: 41630, train/epoch: 9.907187461853027
Step: 41640, train/loss: 0.0
Step: 41640, train/grad_norm: 2.18530713027576e-05
Step: 41640, train/learning_rate: 4.5216563648864394e-07
Step: 41640, train/epoch: 9.909566879272461
Step: 41650, train/loss: 0.0
Step: 41650, train/grad_norm: 7.760439984849654e-06
Step: 41650, train/learning_rate: 4.4026654677509214e-07
Step: 41650, train/epoch: 9.911946296691895
Step: 41660, train/loss: 0.0
Step: 41660, train/grad_norm: 5.100126827528584e-07
Step: 41660, train/learning_rate: 4.2836745706154034e-07
Step: 41660, train/epoch: 9.914326667785645
Step: 41670, train/loss: 0.0
Step: 41670, train/grad_norm: 1.8889879527250741e-07
Step: 41670, train/learning_rate: 4.164683389262791e-07
Step: 41670, train/epoch: 9.916706085205078
Step: 41680, train/loss: 0.0
Step: 41680, train/grad_norm: 5.5671143854851834e-06
Step: 41680, train/learning_rate: 4.045692492127273e-07
Step: 41680, train/epoch: 9.919086456298828
Step: 41690, train/loss: 0.0
Step: 41690, train/grad_norm: 6.261749604163924e-06
Step: 41690, train/learning_rate: 3.926701594991755e-07
Step: 41690, train/epoch: 9.921465873718262
Step: 41700, train/loss: 0.0
Step: 41700, train/grad_norm: 3.015993206645362e-05
Step: 41700, train/learning_rate: 3.807710697856237e-07
Step: 41700, train/epoch: 9.923846244812012
Step: 41710, train/loss: 0.0
Step: 41710, train/grad_norm: 7.526752483499877e-07
Step: 41710, train/learning_rate: 3.6887195165036246e-07
Step: 41710, train/epoch: 9.926225662231445
Step: 41720, train/loss: 0.0
Step: 41720, train/grad_norm: 2.2709060942815995e-07
Step: 41720, train/learning_rate: 3.5697286193681066e-07
Step: 41720, train/epoch: 9.928605079650879
Step: 41730, train/loss: 0.0
Step: 41730, train/grad_norm: 1.5012791720891983e-07
Step: 41730, train/learning_rate: 3.4507377222325886e-07
Step: 41730, train/epoch: 9.930985450744629
Step: 41740, train/loss: 0.0
Step: 41740, train/grad_norm: 5.474763020174578e-05
Step: 41740, train/learning_rate: 3.3317468250970705e-07
Step: 41740, train/epoch: 9.933364868164062
Step: 41750, train/loss: 0.0
Step: 41750, train/grad_norm: 4.2982939874036674e-08
Step: 41750, train/learning_rate: 3.2127559279615525e-07
Step: 41750, train/epoch: 9.935745239257812
Step: 41760, train/loss: 0.0
Step: 41760, train/grad_norm: 5.823842457175488e-07
Step: 41760, train/learning_rate: 3.09376474660894e-07
Step: 41760, train/epoch: 9.938124656677246
Step: 41770, train/loss: 0.0
Step: 41770, train/grad_norm: 1.1326174664816335e-08
Step: 41770, train/learning_rate: 2.974773849473422e-07
Step: 41770, train/epoch: 9.94050407409668
Step: 41780, train/loss: 0.0
Step: 41780, train/grad_norm: 1.3468205679600942e-06
Step: 41780, train/learning_rate: 2.855782952337904e-07
Step: 41780, train/epoch: 9.94288444519043
Step: 41790, train/loss: 0.0
Step: 41790, train/grad_norm: 5.657271984205181e-08
Step: 41790, train/learning_rate: 2.736792055202386e-07
Step: 41790, train/epoch: 9.945263862609863
Step: 41800, train/loss: 0.0
Step: 41800, train/grad_norm: 4.339576875622697e-08
Step: 41800, train/learning_rate: 2.617801158066868e-07
Step: 41800, train/epoch: 9.947644233703613
Step: 41810, train/loss: 0.0
Step: 41810, train/grad_norm: 2.982579303534294e-07
Step: 41810, train/learning_rate: 2.498809976714256e-07
Step: 41810, train/epoch: 9.950023651123047
Step: 41820, train/loss: 0.0
Step: 41820, train/grad_norm: 1.0739658762304316e-07
Step: 41820, train/learning_rate: 2.3798190795787377e-07
Step: 41820, train/epoch: 9.952404022216797
Step: 41830, train/loss: 0.0
Step: 41830, train/grad_norm: 1.7336733435513452e-06
Step: 41830, train/learning_rate: 2.2608281824432197e-07
Step: 41830, train/epoch: 9.95478343963623
Step: 41840, train/loss: 0.0
Step: 41840, train/grad_norm: 5.2008054751695454e-08
Step: 41840, train/learning_rate: 2.1418372853077017e-07
Step: 41840, train/epoch: 9.957162857055664
Step: 41850, train/loss: 0.0
Step: 41850, train/grad_norm: 6.955706055578048e-08
Step: 41850, train/learning_rate: 2.0228462460636365e-07
Step: 41850, train/epoch: 9.959543228149414
Step: 41860, train/loss: 0.0
Step: 41860, train/grad_norm: 1.7091549580072751e-06
Step: 41860, train/learning_rate: 1.9038553489281185e-07
Step: 41860, train/epoch: 9.961922645568848
Step: 41870, train/loss: 0.0
Step: 41870, train/grad_norm: 5.247220087767346e-06
Step: 41870, train/learning_rate: 1.7848643096840533e-07
Step: 41870, train/epoch: 9.964303016662598
Step: 41880, train/loss: 0.0
Step: 41880, train/grad_norm: 1.9072298584887903e-07
Step: 41880, train/learning_rate: 1.6658734125485353e-07
Step: 41880, train/epoch: 9.966682434082031
Step: 41890, train/loss: 0.0
Step: 41890, train/grad_norm: 8.04541073762266e-08
Step: 41890, train/learning_rate: 1.54688237330447e-07
Step: 41890, train/epoch: 9.969062805175781
Step: 41900, train/loss: 0.0
Step: 41900, train/grad_norm: 4.902811383544758e-07
Step: 41900, train/learning_rate: 1.427891476168952e-07
Step: 41900, train/epoch: 9.971442222595215
Step: 41910, train/loss: 0.0
Step: 41910, train/grad_norm: 3.483912587398663e-05
Step: 41910, train/learning_rate: 1.308900579033434e-07
Step: 41910, train/epoch: 9.973821640014648
Step: 41920, train/loss: 0.0
Step: 41920, train/grad_norm: 2.161059882155314e-07
Step: 41920, train/learning_rate: 1.1899095397893689e-07
Step: 41920, train/epoch: 9.976202011108398
Step: 41930, train/loss: 0.0
Step: 41930, train/grad_norm: 6.78302740197978e-07
Step: 41930, train/learning_rate: 1.0709186426538508e-07
Step: 41930, train/epoch: 9.978581428527832
Step: 41940, train/loss: 0.0
Step: 41940, train/grad_norm: 1.9834686781905475e-07
Step: 41940, train/learning_rate: 9.519276744640592e-08
Step: 41940, train/epoch: 9.980961799621582
Step: 41950, train/loss: 0.0
Step: 41950, train/grad_norm: 1.0470724021161004e-07
Step: 41950, train/learning_rate: 8.329367062742676e-08
Step: 41950, train/epoch: 9.983341217041016
Step: 41960, train/loss: 0.0
Step: 41960, train/grad_norm: 7.638366383844186e-08
Step: 41960, train/learning_rate: 7.13945738084476e-08
Step: 41960, train/epoch: 9.98572063446045
Step: 41970, train/loss: 0.0
Step: 41970, train/grad_norm: 1.690967292233836e-05
Step: 41970, train/learning_rate: 5.949547698946844e-08
Step: 41970, train/epoch: 9.9881010055542
Step: 41980, train/loss: 0.0
Step: 41980, train/grad_norm: 4.7478074804985226e-08
Step: 41980, train/learning_rate: 4.759638372320296e-08
Step: 41980, train/epoch: 9.990480422973633
Step: 41990, train/loss: 0.0
Step: 41990, train/grad_norm: 0.000633968913462013
Step: 41990, train/learning_rate: 3.56972869042238e-08
Step: 41990, train/epoch: 9.992860794067383
Step: 42000, train/loss: 0.0
Step: 42000, train/grad_norm: 5.969491212454159e-07
Step: 42000, train/learning_rate: 2.379819186160148e-08
Step: 42000, train/epoch: 9.995240211486816
Step: 42010, train/loss: 0.0
Step: 42010, train/grad_norm: 4.2603559791132284e-07
Step: 42010, train/learning_rate: 1.189909593080074e-08
Step: 42010, train/epoch: 9.997620582580566
Step: 42020, train/loss: 0.0
Step: 42020, train/grad_norm: 1.5665616501792101e-06
Step: 42020, train/learning_rate: 0.0
Step: 42020, train/epoch: 10.0
Step: 42020, eval/loss: 0.0389472097158432
Step: 42020, eval/accuracy: 0.9963904023170471
Step: 42020, eval/f1: 0.9961866140365601
Step: 42020, eval/runtime: 734.0211181640625
Step: 42020, eval/samples_per_second: 9.812999725341797
Step: 42020, eval/steps_per_second: 1.2269999980926514
Step: 42020, train/epoch: 10.0
Step: 42020, train/train_runtime: 124010.8828125
Step: 42020, train/train_samples_per_second: 2.7100000381469727
Step: 42020, train/train_steps_per_second: 0.33899998664855957
Step: 42020, train/total_flos: 7.228304339964527e+18
Step: 42020, train/train_loss: 0.027812564745545387
Step: 42020, train/epoch: 10.0
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=6ffd0164-0f41-4bbe-b905-7dda371dfd20">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [12]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.summary.summary_iterator</span> <span class="kn">import</span> <span class="n">summary_iterator</span>

<span class="n">logs_directory</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">'./'</span><span class="p">,</span> <span class="n">project_name</span><span class="p">,</span> <span class="s1">'logs'</span><span class="p">)</span>
<span class="n">file_pattern</span> <span class="o">=</span> <span class="s1">'events.out.tfevents.*'</span>

<span class="n">event_files</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">logs_directory</span><span class="p">,</span> <span class="n">file_pattern</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">extract_metrics</span><span class="p">(</span><span class="n">event_files</span><span class="p">):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">last_train_loss</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">for</span> <span class="n">event_file</span> <span class="ow">in</span> <span class="n">event_files</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">summary_iterator</span><span class="p">(</span><span class="n">event_file</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">e</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">v</span><span class="o">.</span><span class="n">HasField</span><span class="p">(</span><span class="s1">'simple_value'</span><span class="p">):</span>
                    <span class="n">step</span> <span class="o">=</span> <span class="n">e</span><span class="o">.</span><span class="n">step</span>
                    <span class="n">metric_name</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">'/'</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                    <span class="n">metric_value</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">simple_value</span>

                    <span class="n">formatted_value</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">metric_value</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">"</span>

                    <span class="k">if</span> <span class="s1">'train/loss'</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="p">:</span>
                        <span class="n">last_train_loss</span> <span class="o">=</span> <span class="n">formatted_value</span>

                    <span class="k">if</span> <span class="s1">'eval'</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="p">:</span>
                        <span class="n">entry</span> <span class="o">=</span> <span class="nb">next</span><span class="p">((</span><span class="n">item</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">data</span> <span class="k">if</span> <span class="n">item</span><span class="p">[</span><span class="s1">'Step'</span><span class="p">]</span> <span class="o">==</span> <span class="n">step</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="n">entry</span><span class="p">:</span>
                            <span class="n">entry</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'Step'</span><span class="p">:</span> <span class="n">step</span><span class="p">,</span> <span class="s1">'Train Loss'</span><span class="p">:</span> <span class="n">last_train_loss</span><span class="p">,</span> <span class="s1">'Eval Loss'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">'Accuracy'</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">'F1'</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>
                            <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">entry</span><span class="p">)</span>
                        <span class="k">if</span> <span class="s1">'loss'</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="p">:</span>
                            <span class="n">entry</span><span class="p">[</span><span class="s1">'Eval Loss'</span><span class="p">]</span> <span class="o">=</span> <span class="n">formatted_value</span>
                        <span class="k">elif</span> <span class="s1">'accuracy'</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="p">:</span>
                            <span class="n">entry</span><span class="p">[</span><span class="s1">'Accuracy'</span><span class="p">]</span> <span class="o">=</span> <span class="n">formatted_value</span>
                        <span class="k">elif</span> <span class="s1">'f1'</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">tag</span><span class="p">:</span>
                            <span class="n">entry</span><span class="p">[</span><span class="s1">'F1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">formatted_value</span>

    <span class="k">return</span> <span class="n">data</span>

<span class="n">metrics_data</span> <span class="o">=</span> <span class="n">extract_metrics</span><span class="p">(</span><span class="n">event_files</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">metrics_data</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">'Step'</span><span class="p">)</span>

<span class="n">file_path</span> <span class="o">=</span> <span class="s2">"./images/"</span><span class="o">+</span><span class="n">model_name</span><span class="o">+</span><span class="s2">"_Checkpoint_Data.csv"</span>
<span class="n">df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>    Step Train Loss Eval Loss  Accuracy        F1
0   4202   0.255700  0.051499  0.990976  0.990459
1   8404   0.009700  0.044348  0.993336  0.992976
2  12606   0.000000  0.048642  0.993753  0.993403
3  16808   0.000000  0.048971  0.994447  0.994128
4  21010   0.000000  0.035751  0.994308  0.993994
5  25212   0.000000  0.060425  0.994308  0.993977
6  29414   0.000000  0.035533  0.996113  0.995893
7  33616   0.000000  0.038748  0.995419  0.995159
8  37818   0.000000  0.038357  0.996390  0.996187
9  42020   0.000000  0.038947  0.996390  0.996187
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=37cc5f92-d47f-4356-8fad-d9b64b6f5361">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [27]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">fillna</span><span class="p">({</span>
    <span class="s1">'Eval Loss'</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="s1">'inf'</span><span class="p">),</span>
    <span class="s1">'Accuracy'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">'F1'</span><span class="p">:</span> <span class="mi">0</span>
<span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">df</span><span class="p">[</span><span class="s1">'Eval Loss'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Eval Loss'</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'Accuracy'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Accuracy'</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'F1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'F1'</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>

<span class="n">df</span><span class="p">[</span><span class="s1">'Eval Loss Rank'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Eval Loss'</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'Accuracy Rank'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Accuracy'</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'F1 Rank'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'F1'</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">'min'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">df</span><span class="p">[</span><span class="s1">'Rank Sum'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Eval Loss Rank'</span><span class="p">]</span> <span class="o">+</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Accuracy Rank'</span><span class="p">]</span> <span class="o">+</span> <span class="n">df</span><span class="p">[</span><span class="s1">'F1 Rank'</span><span class="p">]</span>

<span class="n">best_checkpoint</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">'Rank Sum'</span><span class="p">]</span><span class="o">.</span><span class="n">idxmin</span><span class="p">()]</span>

<span class="n">checkpoint_folder_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"checkpoint-</span><span class="si">{</span><span class="n">best_checkpoint</span><span class="p">[</span><span class="s1">'Step'</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Best Checkpoint Step: </span><span class="si">{</span><span class="n">checkpoint_folder_name</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">best_checkpoint</span><span class="p">[[</span><span class="s1">'Step'</span><span class="p">,</span> <span class="s1">'Train Loss'</span><span class="p">,</span> <span class="s1">'Eval Loss'</span><span class="p">,</span> <span class="s1">'Accuracy'</span><span class="p">,</span> <span class="s1">'F1'</span><span class="p">,</span> <span class="s1">'Rank Sum'</span><span class="p">]])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Best Checkpoint Step: checkpoint-37818
Step             37818
Train Loss    0.000000
Eval Loss     0.038357
Accuracy       0.99639
F1            0.996187
Rank Sum           5.0
Name: 8, dtype: object
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=5c3c3999-8c32-4a25-aaca-2ec9036b69ed">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Run-TensorBoard">Run TensorBoard<a class="anchor-link" href="#Run-TensorBoard">¶</a></h3><p>tensorboard --logdir=~/kuk/Praxis/praxis-Llama-2-7b-hf-small-finetune/logs --host=0.0.0.0</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=f8f36022-dc73-492f-998c-43e5ed1a6f46">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="PAUSE-SCRIPT">PAUSE SCRIPT<a class="anchor-link" href="#PAUSE-SCRIPT">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=444ae65b-241c-47ef-bbc2-81f3a2ce50e9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [28]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># My flag to pause the script, set to True to pause</span>
<span class="n">pause_script</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=c8be3672-68e0-44f8-b8b1-31290665658d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [29]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">StopExecution</span><span class="p">(</span><span class="ne">Exception</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">_render_traceback_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Script Paused"</span><span class="p">)</span>
        <span class="k">pass</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=eb3b5ed2-5320-49c0-9e61-90d6f0942f7a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [30]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="n">pause_script</span><span class="p">:</span>
    <span class="k">raise</span> <span class="n">StopExecution</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=19be6e26-d888-4da0-b3f8-836d68ac2051">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Testing">Testing<a class="anchor-link" href="#Testing">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=b5111194-5eeb-4104-8df0-f977a6b716e0">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [31]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>

<span class="n">bnb_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span>
    <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="s2">"nf4"</span><span class="p">,</span>
    <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
<span class="p">)</span>

<span class="n">base_model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">checkpoint</span><span class="p">,</span>
    <span class="n">quantization_config</span><span class="o">=</span><span class="n">bnb_config</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span>
    <span class="n">num_labels</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">token</span><span class="o">=</span><span class="n">access_token</span><span class="p">,</span>
    <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">eval_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">checkpoint</span><span class="p">,</span>
    <span class="n">token</span><span class="o">=</span><span class="n">access_token</span><span class="p">,</span>
    <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Loading checkpoint shards:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=530fa55f-c8c4-485f-a093-09bf2175d586">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [32]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">PeftModel</span>
<span class="n">test_checkpoint_name</span> <span class="o">=</span> <span class="n">checkpoint_folder_name</span>
<span class="n">ft_model</span> <span class="o">=</span> <span class="n">PeftModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model</span><span class="p">,</span> <span class="n">project_name</span><span class="o">+</span><span class="s1">'/'</span><span class="o">+</span><span class="n">test_checkpoint_name</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=27354c4f-95cc-4d1f-ac64-c5e6b4a842ae">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [33]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Using"</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">(),</span> <span class="s2">"GPUs!"</span><span class="p">)</span>
    <span class="n">ft_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">ft_model</span><span class="p">)</span>

<span class="n">ft_model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[33]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>PeftModelForSequenceClassification(
  (base_model): LoraModel(
    (model): MistralForSequenceClassification(
      (model): MistralModel(
        (embed_tokens): Embedding(32000, 4096)
        (layers): ModuleList(
          (0-31): 32 x MistralDecoderLayer(
            (self_attn): MistralSdpaAttention(
              (q_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (v_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): MistralRotaryEmbedding()
            )
            (mlp): MistralMLP(
              (gate_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=14336, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (up_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=14336, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (down_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=14336, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): MistralRMSNorm()
            (post_attention_layernorm): MistralRMSNorm()
          )
        )
        (norm): MistralRMSNorm()
      )
      (score): ModulesToSaveWrapper(
        (original_module): Linear(in_features=4096, out_features=2, bias=False)
        (modules_to_save): ModuleDict(
          (default): Linear(in_features=4096, out_features=2, bias=False)
        )
      )
    )
  )
)</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=eee9ab6f-a7d5-4dd5-9436-2f6f6e2bffaf">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [34]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tqdm.auto</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="n">processed_predictions</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">tokenized_test_ds</span><span class="p">):</span>

        <span class="n">eval_prompt</span> <span class="o">=</span> <span class="n">record</span><span class="p">[</span><span class="s1">'article'</span><span class="p">]</span>
        <span class="n">model_input</span> <span class="o">=</span> <span class="n">tokenize_fn</span><span class="p">({</span><span class="s1">'article'</span><span class="p">:</span> <span class="n">eval_prompt</span><span class="p">})</span>

        <span class="c1"># model_input = {k: v.to('cuda') for k, v in model_input.items()}</span>
        
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">ft_model</span><span class="p">(</span><span class="o">**</span><span class="n">model_input</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>
        
        <span class="n">prediction</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>  <span class="c1"># Use .item() to get a Python number</span>
        <span class="n">processed_predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">prediction</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>  0%|          | 0/7203 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=80f06a2c-5ded-4da0-8232-145383967c9d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Accuracy-and-F1">Accuracy and F1<a class="anchor-link" href="#Accuracy-and-F1">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=6e891bc5-55e0-4c43-a035-0edc37dcb6e3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [35]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">true_articles</span> <span class="o">=</span> <span class="n">tokenized_test_ds</span><span class="p">[</span><span class="s1">'label'</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=6bf33824-d7cc-4d22-af4d-7d44e569c2b9">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [36]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span><span class="p">,</span> <span class="n">accuracy_score</span>

<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">true_articles</span><span class="p">,</span> <span class="n">processed_predictions</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"accuracy:"</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>accuracy: 0.9970845481049563
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=9b37dc24-3bc0-48a0-ace9-a360bae91169">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [37]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">true_articles</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=a29ebf68-632f-49d1-b903-407ff05b5569">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [38]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">processed_predictions</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=fb4d8350-a8d1-4853-a9f8-74ae9ea36bde">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [39]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">f1_score</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">true_articles</span><span class="p">,</span> <span class="n">processed_predictions</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">'macro'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"f1_score:"</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>f1_score: 0.9969235043490792
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell" id="cell-id=2dcaac14-2b83-4c24-b83a-f6d0e73a2d2a">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Confusion-Matrix">Confusion Matrix<a class="anchor-link" href="#Confusion-Matrix">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=57e64afb-e3ee-4c79-8228-b2d79806229e">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [40]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">ConfusionMatrixDisplay</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">tokenized_test_ds</span><span class="p">[</span><span class="s1">'label'</span><span class="p">],</span> <span class="n">processed_predictions</span><span class="p">)</span>

<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'human'</span><span class="p">,</span> <span class="s1">'machine'</span><span class="p">]</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="o">=</span><span class="n">cm</span><span class="p">,</span> <span class="n">display_labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Blues</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=af7d6f55-fb4f-4195-ac3f-93852ecd4b65">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [43]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">file_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">project_name</span><span class="si">}</span><span class="s2">-v12.ipynb"</span>
<span class="n">html_file_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">file_name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">'.ipynb'</span><span class="p">,</span><span class="w"> </span><span class="s1">'.html'</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span>

<span class="n">command</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"jupyter nbconvert '</span><span class="si">{</span><span class="n">file_name</span><span class="si">}</span><span class="s2">' --to html --output-dir './html' --output '</span><span class="si">{</span><span class="n">html_file_name</span><span class="si">}</span><span class="s2">'"</span>
<span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">system</span><span class="p">(</span><span class="n">command</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>[NbConvertApp] Converting notebook praxis-Mistral-7B-v0.1-small-finetune-v11.ipynb to html
[NbConvertApp] WARNING | Alternative text is missing on 1 image(s).
[NbConvertApp] Writing 1278736 bytes to html/praxis-Mistral-7B-v0.1-small-finetune-v11.html
</pre>
</div>
</div>
</div>
</div>
</div>
</main>
</body>
</html>
