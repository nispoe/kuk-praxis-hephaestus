{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c063b655-f0c1-4558-ae88-055063b7a9e6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73c2d2c9-4a4b-48ca-90a3-1ccd120ca08b",
   "metadata": {},
   "source": [
    "# Fine tuning using BERT Large Cased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0f2da2-54e7-42b7-b44e-f6b7e8a48787",
   "metadata": {},
   "source": [
    "### Supress warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "362a364e-20f5-4d16-af44-62aada774f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d23075-e94c-4aa9-969f-ba66e1278d5e",
   "metadata": {},
   "source": [
    "### Inspect the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7cedfd8-5efe-47e5-84df-f1c00cd7cbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"bert-large-cased\"\n",
    "checkpoint = \"google-bert/\"+model_name\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84b0595d-6453-48e4-982f-0608c272cb91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "BertForSequenceClassification                           --\n",
       "├─BertModel: 1-1                                        --\n",
       "│    └─BertEmbeddings: 2-1                              --\n",
       "│    │    └─Embedding: 3-1                              29,691,904\n",
       "│    │    └─Embedding: 3-2                              524,288\n",
       "│    │    └─Embedding: 3-3                              2,048\n",
       "│    │    └─LayerNorm: 3-4                              2,048\n",
       "│    │    └─Dropout: 3-5                                --\n",
       "│    └─BertEncoder: 2-2                                 --\n",
       "│    │    └─ModuleList: 3-6                             302,309,376\n",
       "│    └─BertPooler: 2-3                                  --\n",
       "│    │    └─Linear: 3-7                                 1,049,600\n",
       "│    │    └─Tanh: 3-8                                   --\n",
       "├─Dropout: 1-2                                          --\n",
       "├─Linear: 1-3                                           2,050\n",
       "================================================================================\n",
       "Total params: 333,581,314\n",
       "Trainable params: 333,581,314\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09a29b9b-fd31-4bb9-ada1-e7264f8a85a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601449e5-fe1f-4194-b061-4951fc8bb86f",
   "metadata": {},
   "source": [
    "### Load the news dataset from pickle file\n",
    "If any of the check_files don't exist then load the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5365d1de-318f-46e6-b91b-609f9fd8cbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one of the specified files already exists. Not loading new dataset.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "base_path = './data/'\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "file_name = 'news_small_dataset.pkl'\n",
    "file_path = base_path+file_name\n",
    "\n",
    "def pickle_dataset(dataset, file_path):\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(dataset, file)\n",
    "        print(f\"Dataset has been pickled to: {file_path}\")\n",
    "\n",
    "def load_pickle_dataset(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        dataset = pickle.load(file)\n",
    "        print(f\"Dataset has been loaded from: {file_path}\")\n",
    "    return dataset\n",
    "\n",
    "def check_files_exists(file_names):\n",
    "    for name in file_names:\n",
    "        file_path = os.path.join(base_path, name)\n",
    "        if os.path.exists(file_path):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# if these files exist we do not want to load the news_dataset.pkl to tokenize and make these files\n",
    "check_files = [model_name+'-small_tokenized_train_ds.pkl', model_name+'-small_tokenized_eval_ds.pkl', model_name+'-small_tokenized_test_ds.pkl']\n",
    "\n",
    "if check_files_exists(check_files):\n",
    "    print(\"At least one of the specified files already exists. Not loading new dataset.\")\n",
    "else:\n",
    "    news_split_ds = load_pickle_dataset(file_path)\n",
    "    print(news_split_ds)\n",
    "    total_rows = (news_split_ds['train'].num_rows +\n",
    "              news_split_ds['eval'].num_rows +\n",
    "              news_split_ds['test'].num_rows)\n",
    "    print(\"Total number of rows:\", total_rows)\n",
    "    print(\"Dataset loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "395adbb2-9498-49cc-baf1-f09e3fcfb39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_fn(news):\n",
    "  return tokenizer(news['article'], padding=True, truncation=True, return_tensors=\"pt\", max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6234e6-1e6b-4f41-b3a2-5bb0adfae5a4",
   "metadata": {},
   "source": [
    "### Tokenize train, evaluation, and test datasets\n",
    "If any of the check files exist then don't run tokenization and save some time.\n",
    "Else load the pickle files that already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0b27288-1ea3-4230-9f9d-aeab3cd76b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already exist, so load datasets\n",
      "Dataset has been loaded from: ./data/bert-large-cased-small_tokenized_train_ds.pkl\n",
      "Dataset has been loaded from: ./data/bert-large-cased-small_tokenized_eval_ds.pkl\n",
      "Dataset has been loaded from: ./data/bert-large-cased-small_tokenized_test_ds.pkl\n"
     ]
    }
   ],
   "source": [
    "if not check_files_exists(check_files):\n",
    "    tokenized_train_ds = news_split_ds['train'].map(tokenize_fn, batched=True)\n",
    "    tokenized_eval_ds = news_split_ds['eval'].map(tokenize_fn, batched=True)\n",
    "    tokenized_test_ds = news_split_ds['test'].map(tokenize_fn, batched=True)\n",
    "\n",
    "    print(tokenized_train_ds.features)\n",
    "    print(tokenized_eval_ds.features)\n",
    "    print(tokenized_test_ds.features)\n",
    "    \n",
    "    pickle_dataset(tokenized_train_ds, base_path+model_name+'-small_tokenized_train_ds.pkl')\n",
    "    pickle_dataset(tokenized_eval_ds, base_path+model_name+'-small_tokenized_eval_ds.pkl')\n",
    "    pickle_dataset(tokenized_test_ds, base_path+model_name+'-small_tokenized_test_ds.pkl')\n",
    "else:\n",
    "    print(\"Files already exist, so load datasets\")\n",
    "    tokenized_train_ds = load_pickle_dataset(base_path+model_name+'-small_tokenized_train_ds.pkl')\n",
    "    tokenized_eval_ds = load_pickle_dataset(base_path+model_name+'-small_tokenized_eval_ds.pkl')\n",
    "    tokenized_test_ds = load_pickle_dataset(base_path+model_name+'-small_tokenized_test_ds.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828b2ac6-7634-40c9-9ed8-222634448ce1",
   "metadata": {},
   "source": [
    "### Look at the tokenized data\n",
    "Notice what the actual data looks like, and then the tokenized data which is a bunch of numbers, and then the attention mask at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ca4f524-e97e-4cee-b28a-74f204f669b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in training dataset: 33611\n",
      "Number of records in evaluation dataset: 7203\n",
      "Number of records in test dataset: 7203\n",
      "Total number of records: 48017\n"
     ]
    }
   ],
   "source": [
    "count_train_records = len(tokenized_train_ds)\n",
    "count_eval_records = len(tokenized_eval_ds)\n",
    "count_test_records = len(tokenized_test_ds)\n",
    "print(f\"Number of records in training dataset: {count_train_records}\")\n",
    "print(f\"Number of records in evaluation dataset: {count_eval_records}\")\n",
    "print(f\"Number of records in test dataset: {count_test_records}\")\n",
    "count_total_records = count_train_records + count_eval_records + count_test_records\n",
    "print(f\"Total number of records: {count_total_records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab4e094a-6fc0-4e11-8ab2-8e394b7a07a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'article': \"In a year where homicides, rapes and robberies increased slightly, New York City still saw serious crime drop 1.7 percent in 2015, continuing an overall decline that began in the 1990s, NYPD Commissioner William Bratton said Monday.\\nAt a news conference with Mayor Bill de Blasio, Bratton touted last year’s crime statistics, which he said, when combined with an even larger decline in 2014, put to rest the fear that substantial decreases couldn’t continue under the new administration at City Hall.\\n“While we have had some fluctuation, some increases in certain categories, the overall trend in all our crime categories continues to go down,” Bratton told reporters. “It was a very good year for us, 2015.\\nHomicides increased by 4.5 percent in 2015, rising to 350 from 333 in the prior year, which was the lowest since 1994, said Deputy Commissioner Dermot Shea. Rapes increased 6 percent and robberies rose 2 percent, said Shea, who is in charge of data collection and operations for the NYPD.\\nThe lower overall crime statistics came about due to what Shea called “targeted enforcement,” where cops make quality arrests even though the overall number of apprehensions was the lowest in the city since 2003.\\nTwo boroughs — Manhattan and the Bronx — actually saw serious crimes increase by 3 percent and 4 percent, respectively, Shea said. Manhattan’s increase was driven by more robberies, while the Bronx, although seeing an overall crime increase, had what he said was a “phenomenal” reduction in shootings. Citywide, shootings were down in 2015 about 3 percent, to 1,103 from 1,172 in 2014.\\nShea largely attributed the 2015 increase in rapes to victims coming forward with complaints about attacks from years past.\\nSign up to get the latest updates Get Newsday's Breaking News alerts in your inbox. By clicking Sign up, you agree to our privacy policy.\\n“Twenty percent of these rapes didn’t happen in 2015,” he said.\\nThe NYPD has seen an increase in rapes involving single women who, after a night of drinking, get into cabs of all kinds and are attacked, Shea said.\\n“They get driven, and passing out and waking up in a desolate area, and they get sexually attacked. This is something, really, that people need to be exceptionally aware of, and like any case in New York City, the buddy system works,” said Shea, referring to the need for people to travel in pairs when taking a cab at night.\\nBratton and police brass hope to build upon the continuing drop in overall crime by using technology such as ShotSpotter and a newly minted GPS system for police cars.\\nJessica Tisch, NYPD deputy commissioner for technology, said ShotSpotter, an acoustical system that detects gunfire, identified gunshots in 1,672 cases, mostly in Brooklyn. Of those alerts, 74 percent didn’t have any 911 calls from the public associated with them.\\nTisch said ShotSpotter helped police recover ballistic evidence in 19 percent of the gunfire alerts. In 22 percent of those cases, Tisch said, cops were able to make positive matches of bullets with those from guns used in earlier shootings.\\nTisch also highlighted a special GPS system being tried in about 5,000 patrol cars that allows the NYPD to see where its vehicles are and to track their movements over a 24-hour period, as well as gather information about the officers’ driving.\\n\", 'label': 0, 'input_ids': [101, 1130, 170, 1214, 1187, 27029, 1116, 117, 9372, 1116, 1105, 187, 12809, 3169, 1905, 2569, 2776, 117, 1203, 1365, 1392, 1253, 1486, 3021, 3755, 3968, 122, 119, 128, 3029, 1107, 1410, 117, 5542, 1126, 2905, 6246, 1115, 1310, 1107, 1103, 3281, 117, 5883, 15481, 6520, 1613, 139, 7625, 1633, 1163, 6356, 119, 1335, 170, 2371, 3511, 1114, 4643, 2617, 1260, 139, 7580, 2660, 117, 139, 7625, 1633, 1106, 18527, 1314, 1214, 787, 188, 3755, 9161, 117, 1134, 1119, 1163, 117, 1165, 3490, 1114, 1126, 1256, 2610, 6246, 1107, 1387, 117, 1508, 1106, 1832, 1103, 2945, 1115, 6432, 19377, 1577, 787, 189, 2760, 1223, 1103, 1207, 3469, 1120, 1392, 1944, 119, 789, 1799, 1195, 1138, 1125, 1199, 23896, 5822, 10255, 117, 1199, 6986, 1107, 2218, 6788, 117, 1103, 2905, 10209, 1107, 1155, 1412, 3755, 6788, 3430, 1106, 1301, 1205, 117, 790, 139, 7625, 1633, 1500, 13509, 119, 789, 1135, 1108, 170, 1304, 1363, 1214, 1111, 1366, 117, 1410, 119, 9800, 7257, 8959, 2569, 1118, 125, 119, 126, 3029, 1107, 1410, 117, 4703, 1106, 8301, 1121, 23335, 1107, 1103, 2988, 1214, 117, 1134, 1108, 1103, 6905, 1290, 1898, 117, 1163, 4831, 6520, 9682, 21277, 18352, 119, 21196, 1279, 2569, 127, 3029, 1105, 187, 12809, 3169, 1905, 3152, 123, 3029, 117, 1163, 18352, 117, 1150, 1110, 1107, 2965, 1104, 2233, 2436, 1105, 2500, 1111, 1103, 5883, 15481, 119, 1109, 2211, 2905, 3755, 9161, 1338, 1164, 1496, 1106, 1184, 18352, 1270, 789, 9271, 7742, 117, 790, 1187, 11396, 1294, 3068, 19189, 1256, 1463, 1103, 2905, 1295, 1104, 12647, 17121, 1116, 1108, 1103, 6905, 1107, 1103, 1331, 1290, 1581, 119, 1960, 27126, 783, 6545, 1105, 1103, 15115, 783, 2140, 1486, 3021, 6969, 2773, 1118, 124, 3029, 1105, 125, 3029, 117, 3569, 117, 18352, 1163, 119, 6545, 787, 188, 2773, 1108, 4940, 1118, 1167, 187, 12809, 3169, 1905, 117, 1229, 1103, 15115, 117, 1780, 3195, 1126, 2905, 3755, 2773, 117, 1125, 1184, 1119, 1163, 1108, 170, 789, 14343, 1233, 790, 7234, 1107, 4598, 1116, 119, 1392, 15665, 117, 4598, 1116, 1127, 1205, 1107, 1410, 1164, 124, 3029, 117, 1106, 122, 117, 9550, 1121, 122, 117, 19639, 1107, 1387, 119, 18352, 3494, 6547, 1103, 1410, 2773, 1107, 9372, 1116, 1106, 5256, 1909, 1977, 1114, 11344, 1164, 3690, 1121, 1201, 1763, 119, 20979, 1146, 1106, 1243, 1103, 6270, 15549, 3949, 3128, 6194, 112, 188, 20048, 3128, 10427, 1116, 1107, 1240, 1107, 8757, 119, 1650, 25600, 20979, 1146, 117, 1128, 5340, 1106, 1412, 9909, 2818, 119, 789, 7714, 3029, 1104, 1292, 9372, 1116, 1238, 787, 189, 3333, 1107, 1410, 117, 790, 1119, 1163, 119, 1109, 5883, 15481, 1144, 1562, 1126, 2773, 1107, 9372, 1116, 5336, 1423, 1535, 1150, 117, 1170, 170, 1480, 1104, 5464, 117, 1243, 1154, 10347, 1116, 1104, 1155, 7553, 1105, 1132, 3623, 117, 18352, 1163, 119, 789, 1220, 1243, 4940, 117, 1105, 3744, 1149, 1105, 14047, 1146, 1107, 170, 3532, 14995, 1298, 117, 1105, 1152, 1243, 13014, 3623, 119, 1188, 1110, 1380, 117, 1541, 117, 1115, 1234, 1444, 1106, 1129, 18635, 4484, 1104, 117, 1105, 1176, 1251, 1692, 1107, 1203, 1365, 1392, 117, 1103, 16723, 1449, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "first_record = tokenized_train_ds[0]\n",
    "print(first_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a991f2e-0532-4424-90d4-bda2758ec289",
   "metadata": {},
   "source": [
    "### Turn on accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17623651-b9c0-4dbe-bbd1-07bb51eda8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef357cf6-2074-4ad9-892d-f2ce93221e16",
   "metadata": {},
   "source": [
    "### Look at hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9848d5f-f539-456c-a110-72ac08530f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 7\n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "GPU 1: NVIDIA GeForce RTX 4090\n",
      "GPU 2: NVIDIA GeForce RTX 4090\n",
      "GPU 3: NVIDIA GeForce RTX 3090 Ti\n",
      "GPU 4: NVIDIA GeForce RTX 3090 Ti\n",
      "GPU 5: NVIDIA GeForce RTX 3090\n",
      "GPU 6: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b847246a-83e8-4982-a6c8-ba549f8e38a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 29 10:58:09 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off |   00000000:01:00.0 Off |                  N/A |\n",
      "| 41%   42C    P8             43W /  420W |      13MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        Off |   00000000:02:00.0  On |                  Off |\n",
      "| 30%   33C    P8             22W /  450W |     476MiB /  24564MiB |      5%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 3090 Ti     Off |   00000000:2B:00.0 Off |                  Off |\n",
      "| 44%   44C    P8             27W /  450W |      13MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 3090        Off |   00000000:41:00.0 Off |                  N/A |\n",
      "| 41%   43C    P8             35W /  420W |      13MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA GeForce RTX 4090        Off |   00000000:42:00.0 Off |                  Off |\n",
      "| 31%   42C    P8             23W /  450W |      14MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA GeForce RTX 4090        Off |   00000000:61:00.0 Off |                  Off |\n",
      "| 30%   38C    P8             18W /  450W |      14MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA GeForce RTX 3090 Ti     Off |   00000000:62:00.0 Off |                  Off |\n",
      "| 37%   39C    P8             26W /  450W |      13MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      4756      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    1   N/A  N/A      4756      G   /usr/lib/xorg/Xorg                            163MiB |\n",
      "|    1   N/A  N/A      4947      G   /usr/bin/gnome-shell                           75MiB |\n",
      "|    1   N/A  N/A      5519      G   ...irefox/4259/usr/lib/firefox/firefox        218MiB |\n",
      "|    2   N/A  N/A      4756      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    3   N/A  N/A      4756      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    4   N/A  N/A      4756      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    5   N/A  N/A      4756      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    6   N/A  N/A      4756      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3fabb987-30a6-47bf-8048-b76477a8b070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(logits_and_labels):\n",
    "    logits, labels = logits_and_labels\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='macro')\n",
    "    return {'accuracy': acc, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69b144bb-2c85-429c-8643-3d98a423613d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./praxis-bert-large-cased-small-finetune'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_name = \"praxis-\"+model_name+\"-small-finetune\"\n",
    "output_dir_path = \"./\" + project_name\n",
    "output_dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4b7cb69-8dbe-457e-9449-1e81323374ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a71a13f-2748-4216-b0aa-8200d7fc45d7",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "306b4000-44a5-4b1c-956e-8f86ee3f280d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 10:58:10.315529: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-29 10:58:10.979693: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnispoe\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nispoe/data/kuk/Praxis/wandb/run-20240529_105812-rabthjxg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nispoe/huggingface/runs/rabthjxg' target=\"_blank\">lemon-glade-353</a></strong> to <a href='https://wandb.ai/nispoe/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nispoe/huggingface' target=\"_blank\">https://wandb.ai/nispoe/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nispoe/huggingface/runs/rabthjxg' target=\"_blank\">https://wandb.ai/nispoe/huggingface/runs/rabthjxg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6010' max='6010' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6010/6010 2:36:26, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.320300</td>\n",
       "      <td>0.306954</td>\n",
       "      <td>0.869638</td>\n",
       "      <td>0.862995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.215000</td>\n",
       "      <td>0.359650</td>\n",
       "      <td>0.858670</td>\n",
       "      <td>0.840971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.091400</td>\n",
       "      <td>0.342167</td>\n",
       "      <td>0.889074</td>\n",
       "      <td>0.880647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.030100</td>\n",
       "      <td>0.536876</td>\n",
       "      <td>0.881299</td>\n",
       "      <td>0.876329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.025700</td>\n",
       "      <td>0.963678</td>\n",
       "      <td>0.838817</td>\n",
       "      <td>0.812513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>0.761493</td>\n",
       "      <td>0.887130</td>\n",
       "      <td>0.877392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>0.661285</td>\n",
       "      <td>0.894072</td>\n",
       "      <td>0.886275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>0.820575</td>\n",
       "      <td>0.885048</td>\n",
       "      <td>0.873770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.780710</td>\n",
       "      <td>0.896154</td>\n",
       "      <td>0.887489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.753427</td>\n",
       "      <td>0.902957</td>\n",
       "      <td>0.896661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6010, training_loss=0.1008431031354351, metrics={'train_runtime': 9391.8904, 'train_samples_per_second': 35.787, 'train_steps_per_second': 0.64, 'total_flos': 3.132314505281741e+17, 'train_loss': 0.1008431031354351, 'epoch': 10.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "transformers.set_seed(777)\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_ds,\n",
    "    eval_dataset=tokenized_eval_ds,\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir_path,\n",
    "        num_train_epochs=10,\n",
    "        logging_steps=10,\n",
    "        logging_dir=output_dir_path+\"/logs\",\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        do_eval=True,\n",
    "    ),\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=None),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model = accelerator.prepare_model(model)\n",
    "trainer.train(resume_from_checkpoint=False)  # Turn to True if power goes out..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9247c1-3c27-4cb0-b4d6-602dcb02488e",
   "metadata": {},
   "source": [
    "### Determine best checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f00a0d4e-adfc-4877-af62-4d84992590dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 44\n",
      "drwxr-xr-x 2 nispoe nispoe 4096 May 29 10:58 logs\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 29 11:13 checkpoint-601\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 29 11:29 checkpoint-1202\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 29 11:45 checkpoint-1803\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 29 12:00 checkpoint-2404\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 29 12:16 checkpoint-3005\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 29 12:32 checkpoint-3606\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 29 12:47 checkpoint-4207\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 29 13:03 checkpoint-4808\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 29 13:19 checkpoint-5409\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 29 13:34 checkpoint-6010\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr {output_dir_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf241b2f-b4ac-47a7-941a-9b66b9f3e440",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading events from file: ./praxis-bert-large-cased-small-finetune/logs/events.out.tfevents.1716998292.hephaestus.40626.0\n",
      "Step: 10, train/loss: 0.6650999784469604\n",
      "Step: 10, train/grad_norm: 7.379927635192871\n",
      "Step: 10, train/learning_rate: 4.9916805437533185e-05\n",
      "Step: 10, train/epoch: 0.01663893461227417\n",
      "Step: 20, train/loss: 0.6646999716758728\n",
      "Step: 20, train/grad_norm: 1.286886215209961\n",
      "Step: 20, train/learning_rate: 4.983361213817261e-05\n",
      "Step: 20, train/epoch: 0.03327786922454834\n",
      "Step: 30, train/loss: 0.6434000134468079\n",
      "Step: 30, train/grad_norm: 3.8686728477478027\n",
      "Step: 30, train/learning_rate: 4.975041520083323e-05\n",
      "Step: 30, train/epoch: 0.04991680383682251\n",
      "Step: 40, train/loss: 0.652899980545044\n",
      "Step: 40, train/grad_norm: 7.759418487548828\n",
      "Step: 40, train/learning_rate: 4.966722190147266e-05\n",
      "Step: 40, train/epoch: 0.06655573844909668\n",
      "Step: 50, train/loss: 0.6455000042915344\n",
      "Step: 50, train/grad_norm: 5.228432655334473\n",
      "Step: 50, train/learning_rate: 4.958402496413328e-05\n",
      "Step: 50, train/epoch: 0.08319467306137085\n",
      "Step: 60, train/loss: 0.6456000208854675\n",
      "Step: 60, train/grad_norm: 6.745361804962158\n",
      "Step: 60, train/learning_rate: 4.9500831664772704e-05\n",
      "Step: 60, train/epoch: 0.09983360767364502\n",
      "Step: 70, train/loss: 0.6291999816894531\n",
      "Step: 70, train/grad_norm: 6.824819564819336\n",
      "Step: 70, train/learning_rate: 4.941763836541213e-05\n",
      "Step: 70, train/epoch: 0.11647254228591919\n",
      "Step: 80, train/loss: 0.6047000288963318\n",
      "Step: 80, train/grad_norm: 7.026662349700928\n",
      "Step: 80, train/learning_rate: 4.933444142807275e-05\n",
      "Step: 80, train/epoch: 0.13311147689819336\n",
      "Step: 90, train/loss: 0.6072999835014343\n",
      "Step: 90, train/grad_norm: 3.4369773864746094\n",
      "Step: 90, train/learning_rate: 4.925124812871218e-05\n",
      "Step: 90, train/epoch: 0.14975041151046753\n",
      "Step: 100, train/loss: 0.6258999705314636\n",
      "Step: 100, train/grad_norm: 5.188878059387207\n",
      "Step: 100, train/learning_rate: 4.9168054829351604e-05\n",
      "Step: 100, train/epoch: 0.1663893461227417\n",
      "Step: 110, train/loss: 0.6044999957084656\n",
      "Step: 110, train/grad_norm: 4.786876678466797\n",
      "Step: 110, train/learning_rate: 4.9084857892012224e-05\n",
      "Step: 110, train/epoch: 0.18302828073501587\n",
      "Step: 120, train/loss: 0.5842999815940857\n",
      "Step: 120, train/grad_norm: 2.784226655960083\n",
      "Step: 120, train/learning_rate: 4.900166459265165e-05\n",
      "Step: 120, train/epoch: 0.19966721534729004\n",
      "Step: 130, train/loss: 0.6194000244140625\n",
      "Step: 130, train/grad_norm: 6.181437969207764\n",
      "Step: 130, train/learning_rate: 4.891846765531227e-05\n",
      "Step: 130, train/epoch: 0.2163061499595642\n",
      "Step: 140, train/loss: 0.5371999740600586\n",
      "Step: 140, train/grad_norm: 2.96254301071167\n",
      "Step: 140, train/learning_rate: 4.88352743559517e-05\n",
      "Step: 140, train/epoch: 0.23294508457183838\n",
      "Step: 150, train/loss: 0.5982000231742859\n",
      "Step: 150, train/grad_norm: 12.252120971679688\n",
      "Step: 150, train/learning_rate: 4.875208105659112e-05\n",
      "Step: 150, train/epoch: 0.24958401918411255\n",
      "Step: 160, train/loss: 0.6740000247955322\n",
      "Step: 160, train/grad_norm: 4.8721842765808105\n",
      "Step: 160, train/learning_rate: 4.866888411925174e-05\n",
      "Step: 160, train/epoch: 0.2662229537963867\n",
      "Step: 170, train/loss: 0.6366999745368958\n",
      "Step: 170, train/grad_norm: 2.536864995956421\n",
      "Step: 170, train/learning_rate: 4.858569081989117e-05\n",
      "Step: 170, train/epoch: 0.2828618884086609\n",
      "Step: 180, train/loss: 0.593999981880188\n",
      "Step: 180, train/grad_norm: 4.543741226196289\n",
      "Step: 180, train/learning_rate: 4.8502497520530596e-05\n",
      "Step: 180, train/epoch: 0.29950082302093506\n",
      "Step: 190, train/loss: 0.5482000112533569\n",
      "Step: 190, train/grad_norm: 4.019906997680664\n",
      "Step: 190, train/learning_rate: 4.8419300583191216e-05\n",
      "Step: 190, train/epoch: 0.31613975763320923\n",
      "Step: 200, train/loss: 0.5090000033378601\n",
      "Step: 200, train/grad_norm: 4.355790615081787\n",
      "Step: 200, train/learning_rate: 4.833610728383064e-05\n",
      "Step: 200, train/epoch: 0.3327786922454834\n",
      "Step: 210, train/loss: 0.5224999785423279\n",
      "Step: 210, train/grad_norm: 4.853490829467773\n",
      "Step: 210, train/learning_rate: 4.825291034649126e-05\n",
      "Step: 210, train/epoch: 0.34941762685775757\n",
      "Step: 220, train/loss: 0.5131999850273132\n",
      "Step: 220, train/grad_norm: 11.556658744812012\n",
      "Step: 220, train/learning_rate: 4.816971704713069e-05\n",
      "Step: 220, train/epoch: 0.36605656147003174\n",
      "Step: 230, train/loss: 0.5979999899864197\n",
      "Step: 230, train/grad_norm: 5.770701885223389\n",
      "Step: 230, train/learning_rate: 4.8086523747770116e-05\n",
      "Step: 230, train/epoch: 0.3826954960823059\n",
      "Step: 240, train/loss: 0.5360999703407288\n",
      "Step: 240, train/grad_norm: 2.2215681076049805\n",
      "Step: 240, train/learning_rate: 4.8003326810430735e-05\n",
      "Step: 240, train/epoch: 0.3993344306945801\n",
      "Step: 250, train/loss: 0.5254999995231628\n",
      "Step: 250, train/grad_norm: 7.734715461730957\n",
      "Step: 250, train/learning_rate: 4.792013351107016e-05\n",
      "Step: 250, train/epoch: 0.41597336530685425\n",
      "Step: 260, train/loss: 0.4781000018119812\n",
      "Step: 260, train/grad_norm: 3.7666351795196533\n",
      "Step: 260, train/learning_rate: 4.783694021170959e-05\n",
      "Step: 260, train/epoch: 0.4326122999191284\n",
      "Step: 270, train/loss: 0.48559999465942383\n",
      "Step: 270, train/grad_norm: 4.142789840698242\n",
      "Step: 270, train/learning_rate: 4.775374327437021e-05\n",
      "Step: 270, train/epoch: 0.4492512345314026\n",
      "Step: 280, train/loss: 0.4706999957561493\n",
      "Step: 280, train/grad_norm: 4.515623092651367\n",
      "Step: 280, train/learning_rate: 4.7670549975009635e-05\n",
      "Step: 280, train/epoch: 0.46589016914367676\n",
      "Step: 290, train/loss: 0.4627000093460083\n",
      "Step: 290, train/grad_norm: 7.341929912567139\n",
      "Step: 290, train/learning_rate: 4.7587353037670255e-05\n",
      "Step: 290, train/epoch: 0.4825291037559509\n",
      "Step: 300, train/loss: 0.38920000195503235\n",
      "Step: 300, train/grad_norm: 9.941044807434082\n",
      "Step: 300, train/learning_rate: 4.750415973830968e-05\n",
      "Step: 300, train/epoch: 0.4991680383682251\n",
      "Step: 310, train/loss: 0.47940000891685486\n",
      "Step: 310, train/grad_norm: 7.152563571929932\n",
      "Step: 310, train/learning_rate: 4.742096643894911e-05\n",
      "Step: 310, train/epoch: 0.5158069729804993\n",
      "Step: 320, train/loss: 0.5112000107765198\n",
      "Step: 320, train/grad_norm: 8.148675918579102\n",
      "Step: 320, train/learning_rate: 4.733776950160973e-05\n",
      "Step: 320, train/epoch: 0.5324459075927734\n",
      "Step: 330, train/loss: 0.4724999964237213\n",
      "Step: 330, train/grad_norm: 9.778786659240723\n",
      "Step: 330, train/learning_rate: 4.7254576202249154e-05\n",
      "Step: 330, train/epoch: 0.5490848422050476\n",
      "Step: 340, train/loss: 0.621999979019165\n",
      "Step: 340, train/grad_norm: 18.10699462890625\n",
      "Step: 340, train/learning_rate: 4.7171379264909774e-05\n",
      "Step: 340, train/epoch: 0.5657237768173218\n",
      "Step: 350, train/loss: 0.5335000157356262\n",
      "Step: 350, train/grad_norm: 3.2250826358795166\n",
      "Step: 350, train/learning_rate: 4.70881859655492e-05\n",
      "Step: 350, train/epoch: 0.582362711429596\n",
      "Step: 360, train/loss: 0.447299987077713\n",
      "Step: 360, train/grad_norm: 5.13907527923584\n",
      "Step: 360, train/learning_rate: 4.700499266618863e-05\n",
      "Step: 360, train/epoch: 0.5990016460418701\n",
      "Step: 370, train/loss: 0.4772000014781952\n",
      "Step: 370, train/grad_norm: 13.262104034423828\n",
      "Step: 370, train/learning_rate: 4.692179572884925e-05\n",
      "Step: 370, train/epoch: 0.6156405806541443\n",
      "Step: 380, train/loss: 0.49050000309944153\n",
      "Step: 380, train/grad_norm: 3.437772035598755\n",
      "Step: 380, train/learning_rate: 4.6838602429488674e-05\n",
      "Step: 380, train/epoch: 0.6322795152664185\n",
      "Step: 390, train/loss: 0.43540000915527344\n",
      "Step: 390, train/grad_norm: 12.934346199035645\n",
      "Step: 390, train/learning_rate: 4.67554091301281e-05\n",
      "Step: 390, train/epoch: 0.6489184498786926\n",
      "Step: 400, train/loss: 0.45739999413490295\n",
      "Step: 400, train/grad_norm: 9.638089179992676\n",
      "Step: 400, train/learning_rate: 4.667221219278872e-05\n",
      "Step: 400, train/epoch: 0.6655573844909668\n",
      "Step: 410, train/loss: 0.426800012588501\n",
      "Step: 410, train/grad_norm: 4.0092644691467285\n",
      "Step: 410, train/learning_rate: 4.658901889342815e-05\n",
      "Step: 410, train/epoch: 0.682196319103241\n",
      "Step: 420, train/loss: 0.4074999988079071\n",
      "Step: 420, train/grad_norm: 11.53060245513916\n",
      "Step: 420, train/learning_rate: 4.6505821956088766e-05\n",
      "Step: 420, train/epoch: 0.6988352537155151\n",
      "Step: 430, train/loss: 0.4683000147342682\n",
      "Step: 430, train/grad_norm: 7.020390510559082\n",
      "Step: 430, train/learning_rate: 4.642262865672819e-05\n",
      "Step: 430, train/epoch: 0.7154741883277893\n",
      "Step: 440, train/loss: 0.3993000090122223\n",
      "Step: 440, train/grad_norm: 5.496994495391846\n",
      "Step: 440, train/learning_rate: 4.633943535736762e-05\n",
      "Step: 440, train/epoch: 0.7321131229400635\n",
      "Step: 450, train/loss: 0.4302000105381012\n",
      "Step: 450, train/grad_norm: 7.127499103546143\n",
      "Step: 450, train/learning_rate: 4.625623842002824e-05\n",
      "Step: 450, train/epoch: 0.7487520575523376\n",
      "Step: 460, train/loss: 0.3986999988555908\n",
      "Step: 460, train/grad_norm: 15.396225929260254\n",
      "Step: 460, train/learning_rate: 4.6173045120667666e-05\n",
      "Step: 460, train/epoch: 0.7653909921646118\n",
      "Step: 470, train/loss: 0.38040000200271606\n",
      "Step: 470, train/grad_norm: 5.808687210083008\n",
      "Step: 470, train/learning_rate: 4.608985182130709e-05\n",
      "Step: 470, train/epoch: 0.782029926776886\n",
      "Step: 480, train/loss: 0.36719998717308044\n",
      "Step: 480, train/grad_norm: 17.565017700195312\n",
      "Step: 480, train/learning_rate: 4.600665488396771e-05\n",
      "Step: 480, train/epoch: 0.7986688613891602\n",
      "Step: 490, train/loss: 0.4332999885082245\n",
      "Step: 490, train/grad_norm: 7.671588897705078\n",
      "Step: 490, train/learning_rate: 4.592346158460714e-05\n",
      "Step: 490, train/epoch: 0.8153077960014343\n",
      "Step: 500, train/loss: 0.4683000147342682\n",
      "Step: 500, train/grad_norm: 6.345863342285156\n",
      "Step: 500, train/learning_rate: 4.584026464726776e-05\n",
      "Step: 500, train/epoch: 0.8319467306137085\n",
      "Step: 510, train/loss: 0.44359999895095825\n",
      "Step: 510, train/grad_norm: 15.940984725952148\n",
      "Step: 510, train/learning_rate: 4.5757071347907186e-05\n",
      "Step: 510, train/epoch: 0.8485856652259827\n",
      "Step: 520, train/loss: 0.3905999958515167\n",
      "Step: 520, train/grad_norm: 4.151467323303223\n",
      "Step: 520, train/learning_rate: 4.567387804854661e-05\n",
      "Step: 520, train/epoch: 0.8652245998382568\n",
      "Step: 530, train/loss: 0.42399999499320984\n",
      "Step: 530, train/grad_norm: 8.86231803894043\n",
      "Step: 530, train/learning_rate: 4.559068111120723e-05\n",
      "Step: 530, train/epoch: 0.881863534450531\n",
      "Step: 540, train/loss: 0.38670000433921814\n",
      "Step: 540, train/grad_norm: 5.747830867767334\n",
      "Step: 540, train/learning_rate: 4.550748781184666e-05\n",
      "Step: 540, train/epoch: 0.8985024690628052\n",
      "Step: 550, train/loss: 0.38600000739097595\n",
      "Step: 550, train/grad_norm: 4.733415126800537\n",
      "Step: 550, train/learning_rate: 4.5424294512486085e-05\n",
      "Step: 550, train/epoch: 0.9151414036750793\n",
      "Step: 560, train/loss: 0.33709999918937683\n",
      "Step: 560, train/grad_norm: 4.240734577178955\n",
      "Step: 560, train/learning_rate: 4.5341097575146705e-05\n",
      "Step: 560, train/epoch: 0.9317803382873535\n",
      "Step: 570, train/loss: 0.47029998898506165\n",
      "Step: 570, train/grad_norm: 18.157974243164062\n",
      "Step: 570, train/learning_rate: 4.525790427578613e-05\n",
      "Step: 570, train/epoch: 0.9484192728996277\n",
      "Step: 580, train/loss: 0.4287000000476837\n",
      "Step: 580, train/grad_norm: 14.636628150939941\n",
      "Step: 580, train/learning_rate: 4.517470733844675e-05\n",
      "Step: 580, train/epoch: 0.9650582075119019\n",
      "Step: 590, train/loss: 0.3953000009059906\n",
      "Step: 590, train/grad_norm: 5.208237171173096\n",
      "Step: 590, train/learning_rate: 4.509151403908618e-05\n",
      "Step: 590, train/epoch: 0.981697142124176\n",
      "Step: 600, train/loss: 0.32030001282691956\n",
      "Step: 600, train/grad_norm: 7.7438063621521\n",
      "Step: 600, train/learning_rate: 4.5008320739725605e-05\n",
      "Step: 600, train/epoch: 0.9983360767364502\n",
      "Step: 601, eval/loss: 0.30695414543151855\n",
      "Step: 601, eval/accuracy: 0.869637668132782\n",
      "Step: 601, eval/f1: 0.8629953861236572\n",
      "Step: 601, eval/runtime: 81.06269836425781\n",
      "Step: 601, eval/samples_per_second: 88.85700225830078\n",
      "Step: 601, eval/steps_per_second: 1.590999960899353\n",
      "Step: 601, train/epoch: 1.0\n",
      "Step: 610, train/loss: 0.2547000050544739\n",
      "Step: 610, train/grad_norm: 15.0462646484375\n",
      "Step: 610, train/learning_rate: 4.4925123802386224e-05\n",
      "Step: 610, train/epoch: 1.0149750709533691\n",
      "Step: 620, train/loss: 0.28540000319480896\n",
      "Step: 620, train/grad_norm: 5.719680309295654\n",
      "Step: 620, train/learning_rate: 4.484193050302565e-05\n",
      "Step: 620, train/epoch: 1.0316139459609985\n",
      "Step: 630, train/loss: 0.23759999871253967\n",
      "Step: 630, train/grad_norm: 4.401866436004639\n",
      "Step: 630, train/learning_rate: 4.475873720366508e-05\n",
      "Step: 630, train/epoch: 1.0482529401779175\n",
      "Step: 640, train/loss: 0.20509999990463257\n",
      "Step: 640, train/grad_norm: 8.836006164550781\n",
      "Step: 640, train/learning_rate: 4.46755402663257e-05\n",
      "Step: 640, train/epoch: 1.0648918151855469\n",
      "Step: 650, train/loss: 0.28529998660087585\n",
      "Step: 650, train/grad_norm: 6.6452555656433105\n",
      "Step: 650, train/learning_rate: 4.4592346966965124e-05\n",
      "Step: 650, train/epoch: 1.0815308094024658\n",
      "Step: 660, train/loss: 0.2402999997138977\n",
      "Step: 660, train/grad_norm: 16.249557495117188\n",
      "Step: 660, train/learning_rate: 4.4509150029625744e-05\n",
      "Step: 660, train/epoch: 1.0981696844100952\n",
      "Step: 670, train/loss: 0.27619999647140503\n",
      "Step: 670, train/grad_norm: 6.004464149475098\n",
      "Step: 670, train/learning_rate: 4.442595673026517e-05\n",
      "Step: 670, train/epoch: 1.1148086786270142\n",
      "Step: 680, train/loss: 0.3082999885082245\n",
      "Step: 680, train/grad_norm: 5.181602954864502\n",
      "Step: 680, train/learning_rate: 4.43427634309046e-05\n",
      "Step: 680, train/epoch: 1.1314475536346436\n",
      "Step: 690, train/loss: 0.29739999771118164\n",
      "Step: 690, train/grad_norm: 15.422213554382324\n",
      "Step: 690, train/learning_rate: 4.425956649356522e-05\n",
      "Step: 690, train/epoch: 1.1480865478515625\n",
      "Step: 700, train/loss: 0.2892000079154968\n",
      "Step: 700, train/grad_norm: 9.457697868347168\n",
      "Step: 700, train/learning_rate: 4.417637319420464e-05\n",
      "Step: 700, train/epoch: 1.164725422859192\n",
      "Step: 710, train/loss: 0.26330000162124634\n",
      "Step: 710, train/grad_norm: 5.8354620933532715\n",
      "Step: 710, train/learning_rate: 4.409317625686526e-05\n",
      "Step: 710, train/epoch: 1.1813644170761108\n",
      "Step: 720, train/loss: 0.265500009059906\n",
      "Step: 720, train/grad_norm: 13.612055778503418\n",
      "Step: 720, train/learning_rate: 4.400998295750469e-05\n",
      "Step: 720, train/epoch: 1.1980032920837402\n",
      "Step: 730, train/loss: 0.22529999911785126\n",
      "Step: 730, train/grad_norm: 3.1723642349243164\n",
      "Step: 730, train/learning_rate: 4.3926789658144116e-05\n",
      "Step: 730, train/epoch: 1.2146422863006592\n",
      "Step: 740, train/loss: 0.21240000426769257\n",
      "Step: 740, train/grad_norm: 7.779797077178955\n",
      "Step: 740, train/learning_rate: 4.3843592720804736e-05\n",
      "Step: 740, train/epoch: 1.2312811613082886\n",
      "Step: 750, train/loss: 0.23970000445842743\n",
      "Step: 750, train/grad_norm: 6.085953712463379\n",
      "Step: 750, train/learning_rate: 4.376039942144416e-05\n",
      "Step: 750, train/epoch: 1.2479201555252075\n",
      "Step: 760, train/loss: 0.21140000224113464\n",
      "Step: 760, train/grad_norm: 18.5372314453125\n",
      "Step: 760, train/learning_rate: 4.367720612208359e-05\n",
      "Step: 760, train/epoch: 1.264559030532837\n",
      "Step: 770, train/loss: 0.31700000166893005\n",
      "Step: 770, train/grad_norm: 8.859749794006348\n",
      "Step: 770, train/learning_rate: 4.359400918474421e-05\n",
      "Step: 770, train/epoch: 1.2811980247497559\n",
      "Step: 780, train/loss: 0.23569999635219574\n",
      "Step: 780, train/grad_norm: 3.8643972873687744\n",
      "Step: 780, train/learning_rate: 4.3510815885383636e-05\n",
      "Step: 780, train/epoch: 1.2978368997573853\n",
      "Step: 790, train/loss: 0.17970000207424164\n",
      "Step: 790, train/grad_norm: 6.866947650909424\n",
      "Step: 790, train/learning_rate: 4.3427618948044255e-05\n",
      "Step: 790, train/epoch: 1.3144758939743042\n",
      "Step: 800, train/loss: 0.24199999868869781\n",
      "Step: 800, train/grad_norm: 12.740652084350586\n",
      "Step: 800, train/learning_rate: 4.334442564868368e-05\n",
      "Step: 800, train/epoch: 1.3311147689819336\n",
      "Step: 810, train/loss: 0.20909999310970306\n",
      "Step: 810, train/grad_norm: 2.816776990890503\n",
      "Step: 810, train/learning_rate: 4.326123234932311e-05\n",
      "Step: 810, train/epoch: 1.3477537631988525\n",
      "Step: 820, train/loss: 0.22990000247955322\n",
      "Step: 820, train/grad_norm: 11.178464889526367\n",
      "Step: 820, train/learning_rate: 4.317803541198373e-05\n",
      "Step: 820, train/epoch: 1.364392638206482\n",
      "Step: 830, train/loss: 0.27079999446868896\n",
      "Step: 830, train/grad_norm: 5.8031158447265625\n",
      "Step: 830, train/learning_rate: 4.3094842112623155e-05\n",
      "Step: 830, train/epoch: 1.3810316324234009\n",
      "Step: 840, train/loss: 0.21480000019073486\n",
      "Step: 840, train/grad_norm: 7.422558307647705\n",
      "Step: 840, train/learning_rate: 4.301164881326258e-05\n",
      "Step: 840, train/epoch: 1.3976705074310303\n",
      "Step: 850, train/loss: 0.18770000338554382\n",
      "Step: 850, train/grad_norm: 5.016116142272949\n",
      "Step: 850, train/learning_rate: 4.29284518759232e-05\n",
      "Step: 850, train/epoch: 1.4143095016479492\n",
      "Step: 860, train/loss: 0.2793000042438507\n",
      "Step: 860, train/grad_norm: 11.61337661743164\n",
      "Step: 860, train/learning_rate: 4.284525857656263e-05\n",
      "Step: 860, train/epoch: 1.4309483766555786\n",
      "Step: 870, train/loss: 0.24089999496936798\n",
      "Step: 870, train/grad_norm: 4.819259166717529\n",
      "Step: 870, train/learning_rate: 4.276206163922325e-05\n",
      "Step: 870, train/epoch: 1.4475873708724976\n",
      "Step: 880, train/loss: 0.24480000138282776\n",
      "Step: 880, train/grad_norm: 15.488348960876465\n",
      "Step: 880, train/learning_rate: 4.2678868339862674e-05\n",
      "Step: 880, train/epoch: 1.464226245880127\n",
      "Step: 890, train/loss: 0.2558000087738037\n",
      "Step: 890, train/grad_norm: 16.711782455444336\n",
      "Step: 890, train/learning_rate: 4.25956750405021e-05\n",
      "Step: 890, train/epoch: 1.480865240097046\n",
      "Step: 900, train/loss: 0.29159998893737793\n",
      "Step: 900, train/grad_norm: 7.856405735015869\n",
      "Step: 900, train/learning_rate: 4.251247810316272e-05\n",
      "Step: 900, train/epoch: 1.4975041151046753\n",
      "Step: 910, train/loss: 0.26589998602867126\n",
      "Step: 910, train/grad_norm: 11.860725402832031\n",
      "Step: 910, train/learning_rate: 4.242928480380215e-05\n",
      "Step: 910, train/epoch: 1.5141431093215942\n",
      "Step: 920, train/loss: 0.21660000085830688\n",
      "Step: 920, train/grad_norm: 8.398576736450195\n",
      "Step: 920, train/learning_rate: 4.2346091504441574e-05\n",
      "Step: 920, train/epoch: 1.5307819843292236\n",
      "Step: 930, train/loss: 0.25110000371932983\n",
      "Step: 930, train/grad_norm: 5.566257476806641\n",
      "Step: 930, train/learning_rate: 4.2262894567102194e-05\n",
      "Step: 930, train/epoch: 1.5474209785461426\n",
      "Step: 940, train/loss: 0.2013999968767166\n",
      "Step: 940, train/grad_norm: 12.434866905212402\n",
      "Step: 940, train/learning_rate: 4.217970126774162e-05\n",
      "Step: 940, train/epoch: 1.564059853553772\n",
      "Step: 950, train/loss: 0.22499999403953552\n",
      "Step: 950, train/grad_norm: 6.166606426239014\n",
      "Step: 950, train/learning_rate: 4.209650433040224e-05\n",
      "Step: 950, train/epoch: 1.580698847770691\n",
      "Step: 960, train/loss: 0.25519999861717224\n",
      "Step: 960, train/grad_norm: 15.631294250488281\n",
      "Step: 960, train/learning_rate: 4.201331103104167e-05\n",
      "Step: 960, train/epoch: 1.5973377227783203\n",
      "Step: 970, train/loss: 0.2944999933242798\n",
      "Step: 970, train/grad_norm: 9.884221076965332\n",
      "Step: 970, train/learning_rate: 4.1930117731681094e-05\n",
      "Step: 970, train/epoch: 1.6139767169952393\n",
      "Step: 980, train/loss: 0.2904999852180481\n",
      "Step: 980, train/grad_norm: 7.072795391082764\n",
      "Step: 980, train/learning_rate: 4.184692079434171e-05\n",
      "Step: 980, train/epoch: 1.6306155920028687\n",
      "Step: 990, train/loss: 0.24490000307559967\n",
      "Step: 990, train/grad_norm: 15.719935417175293\n",
      "Step: 990, train/learning_rate: 4.176372749498114e-05\n",
      "Step: 990, train/epoch: 1.6472545862197876\n",
      "Step: 1000, train/loss: 0.22020000219345093\n",
      "Step: 1000, train/grad_norm: 7.39123010635376\n",
      "Step: 1000, train/learning_rate: 4.1680534195620567e-05\n",
      "Step: 1000, train/epoch: 1.663893461227417\n",
      "Step: 1010, train/loss: 0.2312999963760376\n",
      "Step: 1010, train/grad_norm: 5.4389262199401855\n",
      "Step: 1010, train/learning_rate: 4.1597337258281186e-05\n",
      "Step: 1010, train/epoch: 1.680532455444336\n",
      "Step: 1020, train/loss: 0.21930000185966492\n",
      "Step: 1020, train/grad_norm: 5.343178749084473\n",
      "Step: 1020, train/learning_rate: 4.151414395892061e-05\n",
      "Step: 1020, train/epoch: 1.6971713304519653\n",
      "Step: 1030, train/loss: 0.19359999895095825\n",
      "Step: 1030, train/grad_norm: 4.48672342300415\n",
      "Step: 1030, train/learning_rate: 4.143094702158123e-05\n",
      "Step: 1030, train/epoch: 1.7138103246688843\n",
      "Step: 1040, train/loss: 0.23100000619888306\n",
      "Step: 1040, train/grad_norm: 13.000986099243164\n",
      "Step: 1040, train/learning_rate: 4.134775372222066e-05\n",
      "Step: 1040, train/epoch: 1.7304491996765137\n",
      "Step: 1050, train/loss: 0.22519999742507935\n",
      "Step: 1050, train/grad_norm: 8.908065795898438\n",
      "Step: 1050, train/learning_rate: 4.1264560422860086e-05\n",
      "Step: 1050, train/epoch: 1.7470881938934326\n",
      "Step: 1060, train/loss: 0.20640000700950623\n",
      "Step: 1060, train/grad_norm: 7.233611106872559\n",
      "Step: 1060, train/learning_rate: 4.1181363485520706e-05\n",
      "Step: 1060, train/epoch: 1.763727068901062\n",
      "Step: 1070, train/loss: 0.20890000462532043\n",
      "Step: 1070, train/grad_norm: 8.384458541870117\n",
      "Step: 1070, train/learning_rate: 4.109817018616013e-05\n",
      "Step: 1070, train/epoch: 1.780366063117981\n",
      "Step: 1080, train/loss: 0.22869999706745148\n",
      "Step: 1080, train/grad_norm: 6.600235462188721\n",
      "Step: 1080, train/learning_rate: 4.101497324882075e-05\n",
      "Step: 1080, train/epoch: 1.7970049381256104\n",
      "Step: 1090, train/loss: 0.22010000050067902\n",
      "Step: 1090, train/grad_norm: 5.64536714553833\n",
      "Step: 1090, train/learning_rate: 4.093177994946018e-05\n",
      "Step: 1090, train/epoch: 1.8136439323425293\n",
      "Step: 1100, train/loss: 0.16910000145435333\n",
      "Step: 1100, train/grad_norm: 10.902040481567383\n",
      "Step: 1100, train/learning_rate: 4.0848586650099605e-05\n",
      "Step: 1100, train/epoch: 1.8302828073501587\n",
      "Step: 1110, train/loss: 0.1981000006198883\n",
      "Step: 1110, train/grad_norm: 13.382040977478027\n",
      "Step: 1110, train/learning_rate: 4.0765389712760225e-05\n",
      "Step: 1110, train/epoch: 1.8469218015670776\n",
      "Step: 1120, train/loss: 0.28369998931884766\n",
      "Step: 1120, train/grad_norm: 9.344864845275879\n",
      "Step: 1120, train/learning_rate: 4.068219641339965e-05\n",
      "Step: 1120, train/epoch: 1.863560676574707\n",
      "Step: 1130, train/loss: 0.2874000072479248\n",
      "Step: 1130, train/grad_norm: 7.412860870361328\n",
      "Step: 1130, train/learning_rate: 4.059900311403908e-05\n",
      "Step: 1130, train/epoch: 1.880199670791626\n",
      "Step: 1140, train/loss: 0.24230000376701355\n",
      "Step: 1140, train/grad_norm: 7.926681041717529\n",
      "Step: 1140, train/learning_rate: 4.05158061766997e-05\n",
      "Step: 1140, train/epoch: 1.8968385457992554\n",
      "Step: 1150, train/loss: 0.20669999718666077\n",
      "Step: 1150, train/grad_norm: 8.032573699951172\n",
      "Step: 1150, train/learning_rate: 4.0432612877339125e-05\n",
      "Step: 1150, train/epoch: 1.9134775400161743\n",
      "Step: 1160, train/loss: 0.1671999990940094\n",
      "Step: 1160, train/grad_norm: 5.1958842277526855\n",
      "Step: 1160, train/learning_rate: 4.0349415939999744e-05\n",
      "Step: 1160, train/epoch: 1.9301164150238037\n",
      "Step: 1170, train/loss: 0.22949999570846558\n",
      "Step: 1170, train/grad_norm: 5.932378768920898\n",
      "Step: 1170, train/learning_rate: 4.026622264063917e-05\n",
      "Step: 1170, train/epoch: 1.9467554092407227\n",
      "Step: 1180, train/loss: 0.24469999969005585\n",
      "Step: 1180, train/grad_norm: 5.25864839553833\n",
      "Step: 1180, train/learning_rate: 4.01830293412786e-05\n",
      "Step: 1180, train/epoch: 1.963394284248352\n",
      "Step: 1190, train/loss: 0.24120000004768372\n",
      "Step: 1190, train/grad_norm: 3.08305025100708\n",
      "Step: 1190, train/learning_rate: 4.009983240393922e-05\n",
      "Step: 1190, train/epoch: 1.980033278465271\n",
      "Step: 1200, train/loss: 0.2150000035762787\n",
      "Step: 1200, train/grad_norm: 5.102991104125977\n",
      "Step: 1200, train/learning_rate: 4.0016639104578644e-05\n",
      "Step: 1200, train/epoch: 1.9966721534729004\n",
      "Step: 1202, eval/loss: 0.35965049266815186\n",
      "Step: 1202, eval/accuracy: 0.8586699962615967\n",
      "Step: 1202, eval/f1: 0.8409706354141235\n",
      "Step: 1202, eval/runtime: 81.02980041503906\n",
      "Step: 1202, eval/samples_per_second: 88.89299774169922\n",
      "Step: 1202, eval/steps_per_second: 1.5920000076293945\n",
      "Step: 1202, train/epoch: 2.0\n",
      "Step: 1210, train/loss: 0.1039000004529953\n",
      "Step: 1210, train/grad_norm: 3.8285012245178223\n",
      "Step: 1210, train/learning_rate: 3.993344580521807e-05\n",
      "Step: 1210, train/epoch: 2.0133111476898193\n",
      "Step: 1220, train/loss: 0.11219999939203262\n",
      "Step: 1220, train/grad_norm: 25.277910232543945\n",
      "Step: 1220, train/learning_rate: 3.985024886787869e-05\n",
      "Step: 1220, train/epoch: 2.0299501419067383\n",
      "Step: 1230, train/loss: 0.21850000321865082\n",
      "Step: 1230, train/grad_norm: 22.73598861694336\n",
      "Step: 1230, train/learning_rate: 3.976705556851812e-05\n",
      "Step: 1230, train/epoch: 2.0465891361236572\n",
      "Step: 1240, train/loss: 0.13009999692440033\n",
      "Step: 1240, train/grad_norm: 2.9584758281707764\n",
      "Step: 1240, train/learning_rate: 3.968385863117874e-05\n",
      "Step: 1240, train/epoch: 2.063227891921997\n",
      "Step: 1250, train/loss: 0.06729999929666519\n",
      "Step: 1250, train/grad_norm: 7.855915546417236\n",
      "Step: 1250, train/learning_rate: 3.9600665331818163e-05\n",
      "Step: 1250, train/epoch: 2.079866886138916\n",
      "Step: 1260, train/loss: 0.06520000100135803\n",
      "Step: 1260, train/grad_norm: 8.434605598449707\n",
      "Step: 1260, train/learning_rate: 3.951747203245759e-05\n",
      "Step: 1260, train/epoch: 2.096505880355835\n",
      "Step: 1270, train/loss: 0.08079999685287476\n",
      "Step: 1270, train/grad_norm: 8.29426097869873\n",
      "Step: 1270, train/learning_rate: 3.943427509511821e-05\n",
      "Step: 1270, train/epoch: 2.113144874572754\n",
      "Step: 1280, train/loss: 0.054499998688697815\n",
      "Step: 1280, train/grad_norm: 5.6500372886657715\n",
      "Step: 1280, train/learning_rate: 3.9351081795757636e-05\n",
      "Step: 1280, train/epoch: 2.1297836303710938\n",
      "Step: 1290, train/loss: 0.07940000295639038\n",
      "Step: 1290, train/grad_norm: 9.495299339294434\n",
      "Step: 1290, train/learning_rate: 3.926788849639706e-05\n",
      "Step: 1290, train/epoch: 2.1464226245880127\n",
      "Step: 1300, train/loss: 0.04600000008940697\n",
      "Step: 1300, train/grad_norm: 1.3131067752838135\n",
      "Step: 1300, train/learning_rate: 3.918469155905768e-05\n",
      "Step: 1300, train/epoch: 2.1630616188049316\n",
      "Step: 1310, train/loss: 0.06909999996423721\n",
      "Step: 1310, train/grad_norm: 7.466380596160889\n",
      "Step: 1310, train/learning_rate: 3.910149825969711e-05\n",
      "Step: 1310, train/epoch: 2.1797006130218506\n",
      "Step: 1320, train/loss: 0.1590999960899353\n",
      "Step: 1320, train/grad_norm: 8.547000885009766\n",
      "Step: 1320, train/learning_rate: 3.901830132235773e-05\n",
      "Step: 1320, train/epoch: 2.1963393688201904\n",
      "Step: 1330, train/loss: 0.09619999676942825\n",
      "Step: 1330, train/grad_norm: 7.704184055328369\n",
      "Step: 1330, train/learning_rate: 3.8935108022997156e-05\n",
      "Step: 1330, train/epoch: 2.2129783630371094\n",
      "Step: 1340, train/loss: 0.09640000015497208\n",
      "Step: 1340, train/grad_norm: 4.896998405456543\n",
      "Step: 1340, train/learning_rate: 3.885191472363658e-05\n",
      "Step: 1340, train/epoch: 2.2296173572540283\n",
      "Step: 1350, train/loss: 0.10000000149011612\n",
      "Step: 1350, train/grad_norm: 7.114982604980469\n",
      "Step: 1350, train/learning_rate: 3.87687177862972e-05\n",
      "Step: 1350, train/epoch: 2.2462563514709473\n",
      "Step: 1360, train/loss: 0.0714000016450882\n",
      "Step: 1360, train/grad_norm: 10.497421264648438\n",
      "Step: 1360, train/learning_rate: 3.868552448693663e-05\n",
      "Step: 1360, train/epoch: 2.262895107269287\n",
      "Step: 1370, train/loss: 0.09629999846220016\n",
      "Step: 1370, train/grad_norm: 5.845889568328857\n",
      "Step: 1370, train/learning_rate: 3.8602331187576056e-05\n",
      "Step: 1370, train/epoch: 2.279534101486206\n",
      "Step: 1380, train/loss: 0.11420000344514847\n",
      "Step: 1380, train/grad_norm: 11.11335563659668\n",
      "Step: 1380, train/learning_rate: 3.8519134250236675e-05\n",
      "Step: 1380, train/epoch: 2.296173095703125\n",
      "Step: 1390, train/loss: 0.09210000187158585\n",
      "Step: 1390, train/grad_norm: 9.664042472839355\n",
      "Step: 1390, train/learning_rate: 3.84359409508761e-05\n",
      "Step: 1390, train/epoch: 2.312812089920044\n",
      "Step: 1400, train/loss: 0.10440000146627426\n",
      "Step: 1400, train/grad_norm: 7.4072265625\n",
      "Step: 1400, train/learning_rate: 3.835274401353672e-05\n",
      "Step: 1400, train/epoch: 2.329450845718384\n",
      "Step: 1410, train/loss: 0.10100000351667404\n",
      "Step: 1410, train/grad_norm: 0.7590458393096924\n",
      "Step: 1410, train/learning_rate: 3.826955071417615e-05\n",
      "Step: 1410, train/epoch: 2.3460898399353027\n",
      "Step: 1420, train/loss: 0.11469999700784683\n",
      "Step: 1420, train/grad_norm: 15.696939468383789\n",
      "Step: 1420, train/learning_rate: 3.8186357414815575e-05\n",
      "Step: 1420, train/epoch: 2.3627288341522217\n",
      "Step: 1430, train/loss: 0.06260000169277191\n",
      "Step: 1430, train/grad_norm: 16.864971160888672\n",
      "Step: 1430, train/learning_rate: 3.8103160477476195e-05\n",
      "Step: 1430, train/epoch: 2.3793678283691406\n",
      "Step: 1440, train/loss: 0.08940000087022781\n",
      "Step: 1440, train/grad_norm: 3.739851951599121\n",
      "Step: 1440, train/learning_rate: 3.801996717811562e-05\n",
      "Step: 1440, train/epoch: 2.3960065841674805\n",
      "Step: 1450, train/loss: 0.1039000004529953\n",
      "Step: 1450, train/grad_norm: 16.05160903930664\n",
      "Step: 1450, train/learning_rate: 3.793677024077624e-05\n",
      "Step: 1450, train/epoch: 2.4126455783843994\n",
      "Step: 1460, train/loss: 0.06970000267028809\n",
      "Step: 1460, train/grad_norm: 3.170542001724243\n",
      "Step: 1460, train/learning_rate: 3.785357694141567e-05\n",
      "Step: 1460, train/epoch: 2.4292845726013184\n",
      "Step: 1470, train/loss: 0.11649999767541885\n",
      "Step: 1470, train/grad_norm: 10.343746185302734\n",
      "Step: 1470, train/learning_rate: 3.7770383642055094e-05\n",
      "Step: 1470, train/epoch: 2.4459235668182373\n",
      "Step: 1480, train/loss: 0.15680000185966492\n",
      "Step: 1480, train/grad_norm: 5.3386030197143555\n",
      "Step: 1480, train/learning_rate: 3.7687186704715714e-05\n",
      "Step: 1480, train/epoch: 2.462562322616577\n",
      "Step: 1490, train/loss: 0.18549999594688416\n",
      "Step: 1490, train/grad_norm: 19.059362411499023\n",
      "Step: 1490, train/learning_rate: 3.760399340535514e-05\n",
      "Step: 1490, train/epoch: 2.479201316833496\n",
      "Step: 1500, train/loss: 0.15320000052452087\n",
      "Step: 1500, train/grad_norm: 7.418020725250244\n",
      "Step: 1500, train/learning_rate: 3.752080010599457e-05\n",
      "Step: 1500, train/epoch: 2.495840311050415\n",
      "Step: 1510, train/loss: 0.11789999902248383\n",
      "Step: 1510, train/grad_norm: 4.328559398651123\n",
      "Step: 1510, train/learning_rate: 3.743760316865519e-05\n",
      "Step: 1510, train/epoch: 2.512479305267334\n",
      "Step: 1520, train/loss: 0.07859999686479568\n",
      "Step: 1520, train/grad_norm: 1.0580400228500366\n",
      "Step: 1520, train/learning_rate: 3.7354409869294614e-05\n",
      "Step: 1520, train/epoch: 2.529118061065674\n",
      "Step: 1530, train/loss: 0.05490000173449516\n",
      "Step: 1530, train/grad_norm: 1.3123377561569214\n",
      "Step: 1530, train/learning_rate: 3.727121293195523e-05\n",
      "Step: 1530, train/epoch: 2.5457570552825928\n",
      "Step: 1540, train/loss: 0.093299999833107\n",
      "Step: 1540, train/grad_norm: 3.578989028930664\n",
      "Step: 1540, train/learning_rate: 3.718801963259466e-05\n",
      "Step: 1540, train/epoch: 2.5623960494995117\n",
      "Step: 1550, train/loss: 0.11640000343322754\n",
      "Step: 1550, train/grad_norm: 20.201793670654297\n",
      "Step: 1550, train/learning_rate: 3.710482633323409e-05\n",
      "Step: 1550, train/epoch: 2.5790350437164307\n",
      "Step: 1560, train/loss: 0.11299999803304672\n",
      "Step: 1560, train/grad_norm: 1.3932878971099854\n",
      "Step: 1560, train/learning_rate: 3.7021629395894706e-05\n",
      "Step: 1560, train/epoch: 2.5956737995147705\n",
      "Step: 1570, train/loss: 0.12110000103712082\n",
      "Step: 1570, train/grad_norm: 17.904216766357422\n",
      "Step: 1570, train/learning_rate: 3.693843609653413e-05\n",
      "Step: 1570, train/epoch: 2.6123127937316895\n",
      "Step: 1580, train/loss: 0.14830000698566437\n",
      "Step: 1580, train/grad_norm: 23.242755889892578\n",
      "Step: 1580, train/learning_rate: 3.685524279717356e-05\n",
      "Step: 1580, train/epoch: 2.6289517879486084\n",
      "Step: 1590, train/loss: 0.29910001158714294\n",
      "Step: 1590, train/grad_norm: 25.216632843017578\n",
      "Step: 1590, train/learning_rate: 3.677204585983418e-05\n",
      "Step: 1590, train/epoch: 2.6455907821655273\n",
      "Step: 1600, train/loss: 0.21140000224113464\n",
      "Step: 1600, train/grad_norm: 14.146870613098145\n",
      "Step: 1600, train/learning_rate: 3.6688852560473606e-05\n",
      "Step: 1600, train/epoch: 2.662229537963867\n",
      "Step: 1610, train/loss: 0.11240000277757645\n",
      "Step: 1610, train/grad_norm: 5.489464282989502\n",
      "Step: 1610, train/learning_rate: 3.6605655623134226e-05\n",
      "Step: 1610, train/epoch: 2.678868532180786\n",
      "Step: 1620, train/loss: 0.14190000295639038\n",
      "Step: 1620, train/grad_norm: 5.556247711181641\n",
      "Step: 1620, train/learning_rate: 3.652246232377365e-05\n",
      "Step: 1620, train/epoch: 2.695507526397705\n",
      "Step: 1630, train/loss: 0.04259999841451645\n",
      "Step: 1630, train/grad_norm: 12.555967330932617\n",
      "Step: 1630, train/learning_rate: 3.643926902441308e-05\n",
      "Step: 1630, train/epoch: 2.712146520614624\n",
      "Step: 1640, train/loss: 0.1080000028014183\n",
      "Step: 1640, train/grad_norm: 3.965730667114258\n",
      "Step: 1640, train/learning_rate: 3.63560720870737e-05\n",
      "Step: 1640, train/epoch: 2.728785276412964\n",
      "Step: 1650, train/loss: 0.14949999749660492\n",
      "Step: 1650, train/grad_norm: 4.766308307647705\n",
      "Step: 1650, train/learning_rate: 3.6272878787713125e-05\n",
      "Step: 1650, train/epoch: 2.745424270629883\n",
      "Step: 1660, train/loss: 0.06769999861717224\n",
      "Step: 1660, train/grad_norm: 3.3826780319213867\n",
      "Step: 1660, train/learning_rate: 3.618968548835255e-05\n",
      "Step: 1660, train/epoch: 2.7620632648468018\n",
      "Step: 1670, train/loss: 0.09989999979734421\n",
      "Step: 1670, train/grad_norm: 9.229239463806152\n",
      "Step: 1670, train/learning_rate: 3.610648855101317e-05\n",
      "Step: 1670, train/epoch: 2.7787022590637207\n",
      "Step: 1680, train/loss: 0.09380000084638596\n",
      "Step: 1680, train/grad_norm: 7.114170551300049\n",
      "Step: 1680, train/learning_rate: 3.60232952516526e-05\n",
      "Step: 1680, train/epoch: 2.7953410148620605\n",
      "Step: 1690, train/loss: 0.09059999883174896\n",
      "Step: 1690, train/grad_norm: 17.731178283691406\n",
      "Step: 1690, train/learning_rate: 3.594009831431322e-05\n",
      "Step: 1690, train/epoch: 2.8119800090789795\n",
      "Step: 1700, train/loss: 0.11490000039339066\n",
      "Step: 1700, train/grad_norm: 5.109368324279785\n",
      "Step: 1700, train/learning_rate: 3.5856905014952645e-05\n",
      "Step: 1700, train/epoch: 2.8286190032958984\n",
      "Step: 1710, train/loss: 0.09070000052452087\n",
      "Step: 1710, train/grad_norm: 6.854075908660889\n",
      "Step: 1710, train/learning_rate: 3.577371171559207e-05\n",
      "Step: 1710, train/epoch: 2.8452579975128174\n",
      "Step: 1720, train/loss: 0.06729999929666519\n",
      "Step: 1720, train/grad_norm: 0.7393420338630676\n",
      "Step: 1720, train/learning_rate: 3.569051477825269e-05\n",
      "Step: 1720, train/epoch: 2.8618967533111572\n",
      "Step: 1730, train/loss: 0.08869999647140503\n",
      "Step: 1730, train/grad_norm: 1.9623090028762817\n",
      "Step: 1730, train/learning_rate: 3.560732147889212e-05\n",
      "Step: 1730, train/epoch: 2.878535747528076\n",
      "Step: 1740, train/loss: 0.11680000275373459\n",
      "Step: 1740, train/grad_norm: 6.107633590698242\n",
      "Step: 1740, train/learning_rate: 3.5524128179531544e-05\n",
      "Step: 1740, train/epoch: 2.895174741744995\n",
      "Step: 1750, train/loss: 0.12129999697208405\n",
      "Step: 1750, train/grad_norm: 11.396027565002441\n",
      "Step: 1750, train/learning_rate: 3.5440931242192164e-05\n",
      "Step: 1750, train/epoch: 2.911813735961914\n",
      "Step: 1760, train/loss: 0.12800000607967377\n",
      "Step: 1760, train/grad_norm: 3.7579526901245117\n",
      "Step: 1760, train/learning_rate: 3.535773794283159e-05\n",
      "Step: 1760, train/epoch: 2.928452491760254\n",
      "Step: 1770, train/loss: 0.09600000083446503\n",
      "Step: 1770, train/grad_norm: 9.360591888427734\n",
      "Step: 1770, train/learning_rate: 3.527454100549221e-05\n",
      "Step: 1770, train/epoch: 2.945091485977173\n",
      "Step: 1780, train/loss: 0.10279999673366547\n",
      "Step: 1780, train/grad_norm: 4.386389255523682\n",
      "Step: 1780, train/learning_rate: 3.519134770613164e-05\n",
      "Step: 1780, train/epoch: 2.961730480194092\n",
      "Step: 1790, train/loss: 0.08669999986886978\n",
      "Step: 1790, train/grad_norm: 6.229263782501221\n",
      "Step: 1790, train/learning_rate: 3.5108154406771064e-05\n",
      "Step: 1790, train/epoch: 2.9783694744110107\n",
      "Step: 1800, train/loss: 0.09139999747276306\n",
      "Step: 1800, train/grad_norm: 4.381775379180908\n",
      "Step: 1800, train/learning_rate: 3.5024957469431683e-05\n",
      "Step: 1800, train/epoch: 2.9950082302093506\n",
      "Step: 1803, eval/loss: 0.3421666622161865\n",
      "Step: 1803, eval/accuracy: 0.8890739679336548\n",
      "Step: 1803, eval/f1: 0.8806466460227966\n",
      "Step: 1803, eval/runtime: 80.9738998413086\n",
      "Step: 1803, eval/samples_per_second: 88.95500183105469\n",
      "Step: 1803, eval/steps_per_second: 1.593000054359436\n",
      "Step: 1803, train/epoch: 3.0\n",
      "Step: 1810, train/loss: 0.09889999777078629\n",
      "Step: 1810, train/grad_norm: 6.016899585723877\n",
      "Step: 1810, train/learning_rate: 3.494176417007111e-05\n",
      "Step: 1810, train/epoch: 3.0116472244262695\n",
      "Step: 1820, train/loss: 0.036400001496076584\n",
      "Step: 1820, train/grad_norm: 5.9477033615112305\n",
      "Step: 1820, train/learning_rate: 3.485856723273173e-05\n",
      "Step: 1820, train/epoch: 3.0282862186431885\n",
      "Step: 1830, train/loss: 0.08380000293254852\n",
      "Step: 1830, train/grad_norm: 7.687243461608887\n",
      "Step: 1830, train/learning_rate: 3.4775373933371156e-05\n",
      "Step: 1830, train/epoch: 3.0449252128601074\n",
      "Step: 1840, train/loss: 0.12210000306367874\n",
      "Step: 1840, train/grad_norm: 4.2209086418151855\n",
      "Step: 1840, train/learning_rate: 3.469218063401058e-05\n",
      "Step: 1840, train/epoch: 3.0615639686584473\n",
      "Step: 1850, train/loss: 0.0406000018119812\n",
      "Step: 1850, train/grad_norm: 15.77047061920166\n",
      "Step: 1850, train/learning_rate: 3.46089836966712e-05\n",
      "Step: 1850, train/epoch: 3.078202962875366\n",
      "Step: 1860, train/loss: 0.04390000179409981\n",
      "Step: 1860, train/grad_norm: 10.06063461303711\n",
      "Step: 1860, train/learning_rate: 3.452579039731063e-05\n",
      "Step: 1860, train/epoch: 3.094841957092285\n",
      "Step: 1870, train/loss: 0.04100000113248825\n",
      "Step: 1870, train/grad_norm: 5.723227024078369\n",
      "Step: 1870, train/learning_rate: 3.4442597097950056e-05\n",
      "Step: 1870, train/epoch: 3.111480951309204\n",
      "Step: 1880, train/loss: 0.0771000012755394\n",
      "Step: 1880, train/grad_norm: 7.898998737335205\n",
      "Step: 1880, train/learning_rate: 3.4359400160610676e-05\n",
      "Step: 1880, train/epoch: 3.128119707107544\n",
      "Step: 1890, train/loss: 0.025800000876188278\n",
      "Step: 1890, train/grad_norm: 8.577921867370605\n",
      "Step: 1890, train/learning_rate: 3.42762068612501e-05\n",
      "Step: 1890, train/epoch: 3.144758701324463\n",
      "Step: 1900, train/loss: 0.10530000180006027\n",
      "Step: 1900, train/grad_norm: 20.188709259033203\n",
      "Step: 1900, train/learning_rate: 3.419300992391072e-05\n",
      "Step: 1900, train/epoch: 3.161397695541382\n",
      "Step: 1910, train/loss: 0.05900000035762787\n",
      "Step: 1910, train/grad_norm: 5.515536785125732\n",
      "Step: 1910, train/learning_rate: 3.410981662455015e-05\n",
      "Step: 1910, train/epoch: 3.178036689758301\n",
      "Step: 1920, train/loss: 0.04639999940991402\n",
      "Step: 1920, train/grad_norm: 1.3021445274353027\n",
      "Step: 1920, train/learning_rate: 3.4026623325189576e-05\n",
      "Step: 1920, train/epoch: 3.1946754455566406\n",
      "Step: 1930, train/loss: 0.0333000011742115\n",
      "Step: 1930, train/grad_norm: 5.4112420082092285\n",
      "Step: 1930, train/learning_rate: 3.3943426387850195e-05\n",
      "Step: 1930, train/epoch: 3.2113144397735596\n",
      "Step: 1940, train/loss: 0.03359999880194664\n",
      "Step: 1940, train/grad_norm: 13.206639289855957\n",
      "Step: 1940, train/learning_rate: 3.386023308848962e-05\n",
      "Step: 1940, train/epoch: 3.2279534339904785\n",
      "Step: 1950, train/loss: 0.031199999153614044\n",
      "Step: 1950, train/grad_norm: 14.935193061828613\n",
      "Step: 1950, train/learning_rate: 3.377703978912905e-05\n",
      "Step: 1950, train/epoch: 3.2445924282073975\n",
      "Step: 1960, train/loss: 0.06560000032186508\n",
      "Step: 1960, train/grad_norm: 0.39047902822494507\n",
      "Step: 1960, train/learning_rate: 3.369384285178967e-05\n",
      "Step: 1960, train/epoch: 3.2612311840057373\n",
      "Step: 1970, train/loss: 0.1459999978542328\n",
      "Step: 1970, train/grad_norm: 6.8906474113464355\n",
      "Step: 1970, train/learning_rate: 3.3610649552429095e-05\n",
      "Step: 1970, train/epoch: 3.2778701782226562\n",
      "Step: 1980, train/loss: 0.09139999747276306\n",
      "Step: 1980, train/grad_norm: 9.57711410522461\n",
      "Step: 1980, train/learning_rate: 3.3527452615089715e-05\n",
      "Step: 1980, train/epoch: 3.294509172439575\n",
      "Step: 1990, train/loss: 0.07900000363588333\n",
      "Step: 1990, train/grad_norm: 6.065140724182129\n",
      "Step: 1990, train/learning_rate: 3.344425931572914e-05\n",
      "Step: 1990, train/epoch: 3.311148166656494\n",
      "Step: 2000, train/loss: 0.04340000078082085\n",
      "Step: 2000, train/grad_norm: 8.166923522949219\n",
      "Step: 2000, train/learning_rate: 3.336106601636857e-05\n",
      "Step: 2000, train/epoch: 3.327786922454834\n",
      "Step: 2010, train/loss: 0.02419999986886978\n",
      "Step: 2010, train/grad_norm: 9.210704803466797\n",
      "Step: 2010, train/learning_rate: 3.327786907902919e-05\n",
      "Step: 2010, train/epoch: 3.344425916671753\n",
      "Step: 2020, train/loss: 0.059300001710653305\n",
      "Step: 2020, train/grad_norm: 19.087488174438477\n",
      "Step: 2020, train/learning_rate: 3.3194675779668614e-05\n",
      "Step: 2020, train/epoch: 3.361064910888672\n",
      "Step: 2030, train/loss: 0.06199999898672104\n",
      "Step: 2030, train/grad_norm: 11.547325134277344\n",
      "Step: 2030, train/learning_rate: 3.311148248030804e-05\n",
      "Step: 2030, train/epoch: 3.377703905105591\n",
      "Step: 2040, train/loss: 0.11749999970197678\n",
      "Step: 2040, train/grad_norm: 10.436565399169922\n",
      "Step: 2040, train/learning_rate: 3.302828554296866e-05\n",
      "Step: 2040, train/epoch: 3.3943426609039307\n",
      "Step: 2050, train/loss: 0.04360000044107437\n",
      "Step: 2050, train/grad_norm: 2.3637306690216064\n",
      "Step: 2050, train/learning_rate: 3.294509224360809e-05\n",
      "Step: 2050, train/epoch: 3.4109816551208496\n",
      "Step: 2060, train/loss: 0.05389999970793724\n",
      "Step: 2060, train/grad_norm: 13.828747749328613\n",
      "Step: 2060, train/learning_rate: 3.286189530626871e-05\n",
      "Step: 2060, train/epoch: 3.4276206493377686\n",
      "Step: 2070, train/loss: 0.055799998342990875\n",
      "Step: 2070, train/grad_norm: 0.9243711233139038\n",
      "Step: 2070, train/learning_rate: 3.2778702006908134e-05\n",
      "Step: 2070, train/epoch: 3.4442596435546875\n",
      "Step: 2080, train/loss: 0.039000000804662704\n",
      "Step: 2080, train/grad_norm: 5.878324031829834\n",
      "Step: 2080, train/learning_rate: 3.269550870754756e-05\n",
      "Step: 2080, train/epoch: 3.4608983993530273\n",
      "Step: 2090, train/loss: 0.07150000333786011\n",
      "Step: 2090, train/grad_norm: 1.1281424760818481\n",
      "Step: 2090, train/learning_rate: 3.261231177020818e-05\n",
      "Step: 2090, train/epoch: 3.4775373935699463\n",
      "Step: 2100, train/loss: 0.04100000113248825\n",
      "Step: 2100, train/grad_norm: 1.068257451057434\n",
      "Step: 2100, train/learning_rate: 3.252911847084761e-05\n",
      "Step: 2100, train/epoch: 3.4941763877868652\n",
      "Step: 2110, train/loss: 0.0357000008225441\n",
      "Step: 2110, train/grad_norm: 0.6815605759620667\n",
      "Step: 2110, train/learning_rate: 3.244592517148703e-05\n",
      "Step: 2110, train/epoch: 3.510815382003784\n",
      "Step: 2120, train/loss: 0.04670000076293945\n",
      "Step: 2120, train/grad_norm: 0.08489129692316055\n",
      "Step: 2120, train/learning_rate: 3.236272823414765e-05\n",
      "Step: 2120, train/epoch: 3.527454137802124\n",
      "Step: 2130, train/loss: 0.05909999832510948\n",
      "Step: 2130, train/grad_norm: 20.02013397216797\n",
      "Step: 2130, train/learning_rate: 3.227953493478708e-05\n",
      "Step: 2130, train/epoch: 3.544093132019043\n",
      "Step: 2140, train/loss: 0.0729999989271164\n",
      "Step: 2140, train/grad_norm: 12.016074180603027\n",
      "Step: 2140, train/learning_rate: 3.21963379974477e-05\n",
      "Step: 2140, train/epoch: 3.560732126235962\n",
      "Step: 2150, train/loss: 0.05429999902844429\n",
      "Step: 2150, train/grad_norm: 6.999797344207764\n",
      "Step: 2150, train/learning_rate: 3.2113144698087126e-05\n",
      "Step: 2150, train/epoch: 3.577371120452881\n",
      "Step: 2160, train/loss: 0.039799999445676804\n",
      "Step: 2160, train/grad_norm: 6.165569305419922\n",
      "Step: 2160, train/learning_rate: 3.202995139872655e-05\n",
      "Step: 2160, train/epoch: 3.5940098762512207\n",
      "Step: 2170, train/loss: 0.09040000289678574\n",
      "Step: 2170, train/grad_norm: 11.207161903381348\n",
      "Step: 2170, train/learning_rate: 3.194675446138717e-05\n",
      "Step: 2170, train/epoch: 3.6106488704681396\n",
      "Step: 2180, train/loss: 0.0625\n",
      "Step: 2180, train/grad_norm: 6.602334499359131\n",
      "Step: 2180, train/learning_rate: 3.18635611620266e-05\n",
      "Step: 2180, train/epoch: 3.6272878646850586\n",
      "Step: 2190, train/loss: 0.06750000268220901\n",
      "Step: 2190, train/grad_norm: 7.360368251800537\n",
      "Step: 2190, train/learning_rate: 3.1780367862666026e-05\n",
      "Step: 2190, train/epoch: 3.6439268589019775\n",
      "Step: 2200, train/loss: 0.0828000009059906\n",
      "Step: 2200, train/grad_norm: 14.29983139038086\n",
      "Step: 2200, train/learning_rate: 3.1697170925326645e-05\n",
      "Step: 2200, train/epoch: 3.6605656147003174\n",
      "Step: 2210, train/loss: 0.06759999692440033\n",
      "Step: 2210, train/grad_norm: 10.042001724243164\n",
      "Step: 2210, train/learning_rate: 3.161397762596607e-05\n",
      "Step: 2210, train/epoch: 3.6772046089172363\n",
      "Step: 2220, train/loss: 0.06639999896287918\n",
      "Step: 2220, train/grad_norm: 9.730768203735352\n",
      "Step: 2220, train/learning_rate: 3.153078068862669e-05\n",
      "Step: 2220, train/epoch: 3.6938436031341553\n",
      "Step: 2230, train/loss: 0.03840000182390213\n",
      "Step: 2230, train/grad_norm: 13.787581443786621\n",
      "Step: 2230, train/learning_rate: 3.144758738926612e-05\n",
      "Step: 2230, train/epoch: 3.710482597351074\n",
      "Step: 2240, train/loss: 0.06509999930858612\n",
      "Step: 2240, train/grad_norm: 13.831450462341309\n",
      "Step: 2240, train/learning_rate: 3.1364394089905545e-05\n",
      "Step: 2240, train/epoch: 3.727121353149414\n",
      "Step: 2250, train/loss: 0.12330000102519989\n",
      "Step: 2250, train/grad_norm: 12.166550636291504\n",
      "Step: 2250, train/learning_rate: 3.1281197152566165e-05\n",
      "Step: 2250, train/epoch: 3.743760347366333\n",
      "Step: 2260, train/loss: 0.05829999968409538\n",
      "Step: 2260, train/grad_norm: 4.636735439300537\n",
      "Step: 2260, train/learning_rate: 3.119800385320559e-05\n",
      "Step: 2260, train/epoch: 3.760399341583252\n",
      "Step: 2270, train/loss: 0.066600002348423\n",
      "Step: 2270, train/grad_norm: 9.448049545288086\n",
      "Step: 2270, train/learning_rate: 3.111480691586621e-05\n",
      "Step: 2270, train/epoch: 3.777038335800171\n",
      "Step: 2280, train/loss: 0.07680000364780426\n",
      "Step: 2280, train/grad_norm: 8.790578842163086\n",
      "Step: 2280, train/learning_rate: 3.103161361650564e-05\n",
      "Step: 2280, train/epoch: 3.7936770915985107\n",
      "Step: 2290, train/loss: 0.04430000111460686\n",
      "Step: 2290, train/grad_norm: 6.543406963348389\n",
      "Step: 2290, train/learning_rate: 3.0948420317145064e-05\n",
      "Step: 2290, train/epoch: 3.8103160858154297\n",
      "Step: 2300, train/loss: 0.09139999747276306\n",
      "Step: 2300, train/grad_norm: 0.6336570978164673\n",
      "Step: 2300, train/learning_rate: 3.0865223379805684e-05\n",
      "Step: 2300, train/epoch: 3.8269550800323486\n",
      "Step: 2310, train/loss: 0.05209999904036522\n",
      "Step: 2310, train/grad_norm: 10.280624389648438\n",
      "Step: 2310, train/learning_rate: 3.078203008044511e-05\n",
      "Step: 2310, train/epoch: 3.8435940742492676\n",
      "Step: 2320, train/loss: 0.031700000166893005\n",
      "Step: 2320, train/grad_norm: 2.1632895469665527\n",
      "Step: 2320, train/learning_rate: 3.069883678108454e-05\n",
      "Step: 2320, train/epoch: 3.8602328300476074\n",
      "Step: 2330, train/loss: 0.02979999966919422\n",
      "Step: 2330, train/grad_norm: 0.03548111766576767\n",
      "Step: 2330, train/learning_rate: 3.061563984374516e-05\n",
      "Step: 2330, train/epoch: 3.8768718242645264\n",
      "Step: 2340, train/loss: 0.05490000173449516\n",
      "Step: 2340, train/grad_norm: 0.21265415847301483\n",
      "Step: 2340, train/learning_rate: 3.0532446544384584e-05\n",
      "Step: 2340, train/epoch: 3.8935108184814453\n",
      "Step: 2350, train/loss: 0.05130000039935112\n",
      "Step: 2350, train/grad_norm: 0.08509968221187592\n",
      "Step: 2350, train/learning_rate: 3.0449251426034607e-05\n",
      "Step: 2350, train/epoch: 3.9101498126983643\n",
      "Step: 2360, train/loss: 0.02810000069439411\n",
      "Step: 2360, train/grad_norm: 15.517621994018555\n",
      "Step: 2360, train/learning_rate: 3.036605630768463e-05\n",
      "Step: 2360, train/epoch: 3.926788568496704\n",
      "Step: 2370, train/loss: 0.11410000175237656\n",
      "Step: 2370, train/grad_norm: 14.607611656188965\n",
      "Step: 2370, train/learning_rate: 3.0282861189334653e-05\n",
      "Step: 2370, train/epoch: 3.943427562713623\n",
      "Step: 2380, train/loss: 0.21250000596046448\n",
      "Step: 2380, train/grad_norm: 1.5537161827087402\n",
      "Step: 2380, train/learning_rate: 3.019966788997408e-05\n",
      "Step: 2380, train/epoch: 3.960066556930542\n",
      "Step: 2390, train/loss: 0.04969999939203262\n",
      "Step: 2390, train/grad_norm: 8.886703491210938\n",
      "Step: 2390, train/learning_rate: 3.0116472771624103e-05\n",
      "Step: 2390, train/epoch: 3.976705551147461\n",
      "Step: 2400, train/loss: 0.03009999915957451\n",
      "Step: 2400, train/grad_norm: 0.7822450995445251\n",
      "Step: 2400, train/learning_rate: 3.0033277653274126e-05\n",
      "Step: 2400, train/epoch: 3.993344306945801\n",
      "Step: 2404, eval/loss: 0.5368757247924805\n",
      "Step: 2404, eval/accuracy: 0.8812994360923767\n",
      "Step: 2404, eval/f1: 0.876329243183136\n",
      "Step: 2404, eval/runtime: 80.9739990234375\n",
      "Step: 2404, eval/samples_per_second: 88.9540023803711\n",
      "Step: 2404, eval/steps_per_second: 1.593000054359436\n",
      "Step: 2404, train/epoch: 4.0\n",
      "Step: 2410, train/loss: 0.025100000202655792\n",
      "Step: 2410, train/grad_norm: 0.39962220191955566\n",
      "Step: 2410, train/learning_rate: 2.995008253492415e-05\n",
      "Step: 2410, train/epoch: 4.009983539581299\n",
      "Step: 2420, train/loss: 0.08049999922513962\n",
      "Step: 2420, train/grad_norm: 9.439085960388184\n",
      "Step: 2420, train/learning_rate: 2.9866889235563576e-05\n",
      "Step: 2420, train/epoch: 4.026622295379639\n",
      "Step: 2430, train/loss: 0.027300000190734863\n",
      "Step: 2430, train/grad_norm: 0.5106600522994995\n",
      "Step: 2430, train/learning_rate: 2.97836941172136e-05\n",
      "Step: 2430, train/epoch: 4.0432610511779785\n",
      "Step: 2440, train/loss: 0.029600000008940697\n",
      "Step: 2440, train/grad_norm: 0.30137187242507935\n",
      "Step: 2440, train/learning_rate: 2.9700498998863623e-05\n",
      "Step: 2440, train/epoch: 4.059900283813477\n",
      "Step: 2450, train/loss: 0.039400000125169754\n",
      "Step: 2450, train/grad_norm: 0.13104544579982758\n",
      "Step: 2450, train/learning_rate: 2.9617303880513646e-05\n",
      "Step: 2450, train/epoch: 4.076539039611816\n",
      "Step: 2460, train/loss: 0.01360000018030405\n",
      "Step: 2460, train/grad_norm: 0.948762059211731\n",
      "Step: 2460, train/learning_rate: 2.9534110581153072e-05\n",
      "Step: 2460, train/epoch: 4.0931782722473145\n",
      "Step: 2470, train/loss: 0.054099999368190765\n",
      "Step: 2470, train/grad_norm: 6.634372711181641\n",
      "Step: 2470, train/learning_rate: 2.9450915462803096e-05\n",
      "Step: 2470, train/epoch: 4.109817028045654\n",
      "Step: 2480, train/loss: 0.039900001138448715\n",
      "Step: 2480, train/grad_norm: 0.10155005753040314\n",
      "Step: 2480, train/learning_rate: 2.936772034445312e-05\n",
      "Step: 2480, train/epoch: 4.126455783843994\n",
      "Step: 2490, train/loss: 0.03500000014901161\n",
      "Step: 2490, train/grad_norm: 0.029327765107154846\n",
      "Step: 2490, train/learning_rate: 2.9284525226103142e-05\n",
      "Step: 2490, train/epoch: 4.143095016479492\n",
      "Step: 2500, train/loss: 0.04540000110864639\n",
      "Step: 2500, train/grad_norm: 2.477949619293213\n",
      "Step: 2500, train/learning_rate: 2.920133192674257e-05\n",
      "Step: 2500, train/epoch: 4.159733772277832\n",
      "Step: 2510, train/loss: 0.07660000026226044\n",
      "Step: 2510, train/grad_norm: 9.099358558654785\n",
      "Step: 2510, train/learning_rate: 2.9118136808392592e-05\n",
      "Step: 2510, train/epoch: 4.176372528076172\n",
      "Step: 2520, train/loss: 0.030700000002980232\n",
      "Step: 2520, train/grad_norm: 0.06178414449095726\n",
      "Step: 2520, train/learning_rate: 2.9034941690042615e-05\n",
      "Step: 2520, train/epoch: 4.19301176071167\n",
      "Step: 2530, train/loss: 0.04149999842047691\n",
      "Step: 2530, train/grad_norm: 0.343082070350647\n",
      "Step: 2530, train/learning_rate: 2.8951746571692638e-05\n",
      "Step: 2530, train/epoch: 4.20965051651001\n",
      "Step: 2540, train/loss: 0.021900000050663948\n",
      "Step: 2540, train/grad_norm: 11.989400863647461\n",
      "Step: 2540, train/learning_rate: 2.8868553272332065e-05\n",
      "Step: 2540, train/epoch: 4.226289749145508\n",
      "Step: 2550, train/loss: 0.027799999341368675\n",
      "Step: 2550, train/grad_norm: 7.3339385986328125\n",
      "Step: 2550, train/learning_rate: 2.8785358153982088e-05\n",
      "Step: 2550, train/epoch: 4.242928504943848\n",
      "Step: 2560, train/loss: 0.02199999988079071\n",
      "Step: 2560, train/grad_norm: 7.0583415031433105\n",
      "Step: 2560, train/learning_rate: 2.870216303563211e-05\n",
      "Step: 2560, train/epoch: 4.2595672607421875\n",
      "Step: 2570, train/loss: 0.05660000070929527\n",
      "Step: 2570, train/grad_norm: 11.60353946685791\n",
      "Step: 2570, train/learning_rate: 2.8618967917282134e-05\n",
      "Step: 2570, train/epoch: 4.2762064933776855\n",
      "Step: 2580, train/loss: 0.04399999976158142\n",
      "Step: 2580, train/grad_norm: 6.059980869293213\n",
      "Step: 2580, train/learning_rate: 2.853577461792156e-05\n",
      "Step: 2580, train/epoch: 4.292845249176025\n",
      "Step: 2590, train/loss: 0.039400000125169754\n",
      "Step: 2590, train/grad_norm: 2.0871505737304688\n",
      "Step: 2590, train/learning_rate: 2.8452579499571584e-05\n",
      "Step: 2590, train/epoch: 4.309484004974365\n",
      "Step: 2600, train/loss: 0.05260000005364418\n",
      "Step: 2600, train/grad_norm: 4.719142913818359\n",
      "Step: 2600, train/learning_rate: 2.8369384381221607e-05\n",
      "Step: 2600, train/epoch: 4.326123237609863\n",
      "Step: 2610, train/loss: 0.026499999687075615\n",
      "Step: 2610, train/grad_norm: 3.661092519760132\n",
      "Step: 2610, train/learning_rate: 2.828618926287163e-05\n",
      "Step: 2610, train/epoch: 4.342761993408203\n",
      "Step: 2620, train/loss: 0.03189999982714653\n",
      "Step: 2620, train/grad_norm: 1.4103132486343384\n",
      "Step: 2620, train/learning_rate: 2.8202994144521654e-05\n",
      "Step: 2620, train/epoch: 4.359401226043701\n",
      "Step: 2630, train/loss: 0.01119999960064888\n",
      "Step: 2630, train/grad_norm: 20.36663246154785\n",
      "Step: 2630, train/learning_rate: 2.811980084516108e-05\n",
      "Step: 2630, train/epoch: 4.376039981842041\n",
      "Step: 2640, train/loss: 0.0142000000923872\n",
      "Step: 2640, train/grad_norm: 0.1665094792842865\n",
      "Step: 2640, train/learning_rate: 2.8036605726811104e-05\n",
      "Step: 2640, train/epoch: 4.392678737640381\n",
      "Step: 2650, train/loss: 0.08240000158548355\n",
      "Step: 2650, train/grad_norm: 5.083676338195801\n",
      "Step: 2650, train/learning_rate: 2.7953410608461127e-05\n",
      "Step: 2650, train/epoch: 4.409317970275879\n",
      "Step: 2660, train/loss: 0.03440000116825104\n",
      "Step: 2660, train/grad_norm: 8.45683479309082\n",
      "Step: 2660, train/learning_rate: 2.787021549011115e-05\n",
      "Step: 2660, train/epoch: 4.425956726074219\n",
      "Step: 2670, train/loss: 0.013899999670684338\n",
      "Step: 2670, train/grad_norm: 0.029507359489798546\n",
      "Step: 2670, train/learning_rate: 2.7787022190750577e-05\n",
      "Step: 2670, train/epoch: 4.442595481872559\n",
      "Step: 2680, train/loss: 0.010400000028312206\n",
      "Step: 2680, train/grad_norm: 0.20183013379573822\n",
      "Step: 2680, train/learning_rate: 2.77038270724006e-05\n",
      "Step: 2680, train/epoch: 4.459234714508057\n",
      "Step: 2690, train/loss: 0.02019999921321869\n",
      "Step: 2690, train/grad_norm: 21.434980392456055\n",
      "Step: 2690, train/learning_rate: 2.7620631954050623e-05\n",
      "Step: 2690, train/epoch: 4.4758734703063965\n",
      "Step: 2700, train/loss: 0.027799999341368675\n",
      "Step: 2700, train/grad_norm: 0.7410769462585449\n",
      "Step: 2700, train/learning_rate: 2.7537436835700646e-05\n",
      "Step: 2700, train/epoch: 4.4925127029418945\n",
      "Step: 2710, train/loss: 0.03319999948143959\n",
      "Step: 2710, train/grad_norm: 0.03604386746883392\n",
      "Step: 2710, train/learning_rate: 2.7454243536340073e-05\n",
      "Step: 2710, train/epoch: 4.509151458740234\n",
      "Step: 2720, train/loss: 0.011099999770522118\n",
      "Step: 2720, train/grad_norm: 1.6255216598510742\n",
      "Step: 2720, train/learning_rate: 2.7371048417990096e-05\n",
      "Step: 2720, train/epoch: 4.525790214538574\n",
      "Step: 2730, train/loss: 0.019999999552965164\n",
      "Step: 2730, train/grad_norm: 0.6235983371734619\n",
      "Step: 2730, train/learning_rate: 2.728785329964012e-05\n",
      "Step: 2730, train/epoch: 4.542429447174072\n",
      "Step: 2740, train/loss: 0.0031999999191612005\n",
      "Step: 2740, train/grad_norm: 0.33208924531936646\n",
      "Step: 2740, train/learning_rate: 2.7204658181290142e-05\n",
      "Step: 2740, train/epoch: 4.559068202972412\n",
      "Step: 2750, train/loss: 0.10429999977350235\n",
      "Step: 2750, train/grad_norm: 0.07474042475223541\n",
      "Step: 2750, train/learning_rate: 2.712146488192957e-05\n",
      "Step: 2750, train/epoch: 4.575706958770752\n",
      "Step: 2760, train/loss: 0.03680000081658363\n",
      "Step: 2760, train/grad_norm: 8.664207458496094\n",
      "Step: 2760, train/learning_rate: 2.7038269763579592e-05\n",
      "Step: 2760, train/epoch: 4.59234619140625\n",
      "Step: 2770, train/loss: 0.06870000064373016\n",
      "Step: 2770, train/grad_norm: 0.9974080324172974\n",
      "Step: 2770, train/learning_rate: 2.6955074645229615e-05\n",
      "Step: 2770, train/epoch: 4.60898494720459\n",
      "Step: 2780, train/loss: 0.04859999939799309\n",
      "Step: 2780, train/grad_norm: 0.27995389699935913\n",
      "Step: 2780, train/learning_rate: 2.687187952687964e-05\n",
      "Step: 2780, train/epoch: 4.625624179840088\n",
      "Step: 2790, train/loss: 0.021800000220537186\n",
      "Step: 2790, train/grad_norm: 0.12486632913351059\n",
      "Step: 2790, train/learning_rate: 2.6788686227519065e-05\n",
      "Step: 2790, train/epoch: 4.642262935638428\n",
      "Step: 2800, train/loss: 0.02250000089406967\n",
      "Step: 2800, train/grad_norm: 0.5034139156341553\n",
      "Step: 2800, train/learning_rate: 2.6705491109169088e-05\n",
      "Step: 2800, train/epoch: 4.658901691436768\n",
      "Step: 2810, train/loss: 0.03269999846816063\n",
      "Step: 2810, train/grad_norm: 17.919893264770508\n",
      "Step: 2810, train/learning_rate: 2.662229599081911e-05\n",
      "Step: 2810, train/epoch: 4.675540924072266\n",
      "Step: 2820, train/loss: 0.07020000368356705\n",
      "Step: 2820, train/grad_norm: 4.067751407623291\n",
      "Step: 2820, train/learning_rate: 2.6539100872469135e-05\n",
      "Step: 2820, train/epoch: 4.6921796798706055\n",
      "Step: 2830, train/loss: 0.027799999341368675\n",
      "Step: 2830, train/grad_norm: 8.11676025390625\n",
      "Step: 2830, train/learning_rate: 2.645590757310856e-05\n",
      "Step: 2830, train/epoch: 4.708818435668945\n",
      "Step: 2840, train/loss: 0.07400000095367432\n",
      "Step: 2840, train/grad_norm: 28.39150619506836\n",
      "Step: 2840, train/learning_rate: 2.6372712454758584e-05\n",
      "Step: 2840, train/epoch: 4.725457668304443\n",
      "Step: 2850, train/loss: 0.020400000736117363\n",
      "Step: 2850, train/grad_norm: 1.339153528213501\n",
      "Step: 2850, train/learning_rate: 2.6289517336408608e-05\n",
      "Step: 2850, train/epoch: 4.742096424102783\n",
      "Step: 2860, train/loss: 0.021800000220537186\n",
      "Step: 2860, train/grad_norm: 0.046490322798490524\n",
      "Step: 2860, train/learning_rate: 2.620632221805863e-05\n",
      "Step: 2860, train/epoch: 4.758735656738281\n",
      "Step: 2870, train/loss: 0.02070000022649765\n",
      "Step: 2870, train/grad_norm: 0.19811032712459564\n",
      "Step: 2870, train/learning_rate: 2.6123128918698058e-05\n",
      "Step: 2870, train/epoch: 4.775374412536621\n",
      "Step: 2880, train/loss: 0.030500000342726707\n",
      "Step: 2880, train/grad_norm: 0.2856137156486511\n",
      "Step: 2880, train/learning_rate: 2.603993380034808e-05\n",
      "Step: 2880, train/epoch: 4.792013168334961\n",
      "Step: 2890, train/loss: 0.062300000339746475\n",
      "Step: 2890, train/grad_norm: 6.161964416503906\n",
      "Step: 2890, train/learning_rate: 2.5956738681998104e-05\n",
      "Step: 2890, train/epoch: 4.808652400970459\n",
      "Step: 2900, train/loss: 0.03579999879002571\n",
      "Step: 2900, train/grad_norm: 28.106369018554688\n",
      "Step: 2900, train/learning_rate: 2.5873543563648127e-05\n",
      "Step: 2900, train/epoch: 4.825291156768799\n",
      "Step: 2910, train/loss: 0.02539999969303608\n",
      "Step: 2910, train/grad_norm: 3.52901291847229\n",
      "Step: 2910, train/learning_rate: 2.5790350264287554e-05\n",
      "Step: 2910, train/epoch: 4.841929912567139\n",
      "Step: 2920, train/loss: 0.02879999950528145\n",
      "Step: 2920, train/grad_norm: 0.28236469626426697\n",
      "Step: 2920, train/learning_rate: 2.5707155145937577e-05\n",
      "Step: 2920, train/epoch: 4.858569145202637\n",
      "Step: 2930, train/loss: 0.06499999761581421\n",
      "Step: 2930, train/grad_norm: 8.355181694030762\n",
      "Step: 2930, train/learning_rate: 2.56239600275876e-05\n",
      "Step: 2930, train/epoch: 4.875207901000977\n",
      "Step: 2940, train/loss: 0.02319999970495701\n",
      "Step: 2940, train/grad_norm: 1.2648417949676514\n",
      "Step: 2940, train/learning_rate: 2.5540764909237623e-05\n",
      "Step: 2940, train/epoch: 4.891847133636475\n",
      "Step: 2950, train/loss: 0.05490000173449516\n",
      "Step: 2950, train/grad_norm: 11.116598129272461\n",
      "Step: 2950, train/learning_rate: 2.545757160987705e-05\n",
      "Step: 2950, train/epoch: 4.9084858894348145\n",
      "Step: 2960, train/loss: 0.03590000048279762\n",
      "Step: 2960, train/grad_norm: 0.07278187572956085\n",
      "Step: 2960, train/learning_rate: 2.5374376491527073e-05\n",
      "Step: 2960, train/epoch: 4.925124645233154\n",
      "Step: 2970, train/loss: 0.02669999934732914\n",
      "Step: 2970, train/grad_norm: 0.2648467421531677\n",
      "Step: 2970, train/learning_rate: 2.5291181373177096e-05\n",
      "Step: 2970, train/epoch: 4.941763877868652\n",
      "Step: 2980, train/loss: 0.03370000049471855\n",
      "Step: 2980, train/grad_norm: 0.034052807837724686\n",
      "Step: 2980, train/learning_rate: 2.520798625482712e-05\n",
      "Step: 2980, train/epoch: 4.958402633666992\n",
      "Step: 2990, train/loss: 0.03550000116229057\n",
      "Step: 2990, train/grad_norm: 1.0716551542282104\n",
      "Step: 2990, train/learning_rate: 2.5124791136477143e-05\n",
      "Step: 2990, train/epoch: 4.975041389465332\n",
      "Step: 3000, train/loss: 0.025699999183416367\n",
      "Step: 3000, train/grad_norm: 10.15709400177002\n",
      "Step: 3000, train/learning_rate: 2.504159783711657e-05\n",
      "Step: 3000, train/epoch: 4.99168062210083\n",
      "Step: 3005, eval/loss: 0.9636783003807068\n",
      "Step: 3005, eval/accuracy: 0.8388171792030334\n",
      "Step: 3005, eval/f1: 0.8125127553939819\n",
      "Step: 3005, eval/runtime: 80.9292984008789\n",
      "Step: 3005, eval/samples_per_second: 89.00399780273438\n",
      "Step: 3005, eval/steps_per_second: 1.593999981880188\n",
      "Step: 3005, train/epoch: 5.0\n",
      "Step: 3010, train/loss: 0.16140000522136688\n",
      "Step: 3010, train/grad_norm: 1.622541904449463\n",
      "Step: 3010, train/learning_rate: 2.4958402718766592e-05\n",
      "Step: 3010, train/epoch: 5.00831937789917\n",
      "Step: 3020, train/loss: 0.019500000402331352\n",
      "Step: 3020, train/grad_norm: 1.0872955322265625\n",
      "Step: 3020, train/learning_rate: 2.4875207600416616e-05\n",
      "Step: 3020, train/epoch: 5.024958610534668\n",
      "Step: 3030, train/loss: 0.03660000115633011\n",
      "Step: 3030, train/grad_norm: 0.10720779746770859\n",
      "Step: 3030, train/learning_rate: 2.479201248206664e-05\n",
      "Step: 3030, train/epoch: 5.041597366333008\n",
      "Step: 3040, train/loss: 0.003700000001117587\n",
      "Step: 3040, train/grad_norm: 1.2607592344284058\n",
      "Step: 3040, train/learning_rate: 2.4708819182706065e-05\n",
      "Step: 3040, train/epoch: 5.058236122131348\n",
      "Step: 3050, train/loss: 0.026399999856948853\n",
      "Step: 3050, train/grad_norm: 15.823887825012207\n",
      "Step: 3050, train/learning_rate: 2.462562406435609e-05\n",
      "Step: 3050, train/epoch: 5.074875354766846\n",
      "Step: 3060, train/loss: 0.020800000056624413\n",
      "Step: 3060, train/grad_norm: 0.042160578072071075\n",
      "Step: 3060, train/learning_rate: 2.4542428946006112e-05\n",
      "Step: 3060, train/epoch: 5.0915141105651855\n",
      "Step: 3070, train/loss: 0.07249999791383743\n",
      "Step: 3070, train/grad_norm: 20.791763305664062\n",
      "Step: 3070, train/learning_rate: 2.4459233827656135e-05\n",
      "Step: 3070, train/epoch: 5.108152866363525\n",
      "Step: 3080, train/loss: 0.05180000141263008\n",
      "Step: 3080, train/grad_norm: 14.2481107711792\n",
      "Step: 3080, train/learning_rate: 2.437604052829556e-05\n",
      "Step: 3080, train/epoch: 5.124792098999023\n",
      "Step: 3090, train/loss: 0.02630000002682209\n",
      "Step: 3090, train/grad_norm: 12.355966567993164\n",
      "Step: 3090, train/learning_rate: 2.4292845409945585e-05\n",
      "Step: 3090, train/epoch: 5.141430854797363\n",
      "Step: 3100, train/loss: 0.09109999984502792\n",
      "Step: 3100, train/grad_norm: 27.27370262145996\n",
      "Step: 3100, train/learning_rate: 2.4209650291595608e-05\n",
      "Step: 3100, train/epoch: 5.158070087432861\n",
      "Step: 3110, train/loss: 0.0421999990940094\n",
      "Step: 3110, train/grad_norm: 8.817068099975586\n",
      "Step: 3110, train/learning_rate: 2.412645517324563e-05\n",
      "Step: 3110, train/epoch: 5.174708843231201\n",
      "Step: 3120, train/loss: 0.022199999541044235\n",
      "Step: 3120, train/grad_norm: 10.755829811096191\n",
      "Step: 3120, train/learning_rate: 2.4043261873885058e-05\n",
      "Step: 3120, train/epoch: 5.191347599029541\n",
      "Step: 3130, train/loss: 0.020500000566244125\n",
      "Step: 3130, train/grad_norm: 0.9016442894935608\n",
      "Step: 3130, train/learning_rate: 2.396006675553508e-05\n",
      "Step: 3130, train/epoch: 5.207986831665039\n",
      "Step: 3140, train/loss: 0.01769999973475933\n",
      "Step: 3140, train/grad_norm: 0.3834715187549591\n",
      "Step: 3140, train/learning_rate: 2.3876871637185104e-05\n",
      "Step: 3140, train/epoch: 5.224625587463379\n",
      "Step: 3150, train/loss: 0.025599999353289604\n",
      "Step: 3150, train/grad_norm: 1.0055166482925415\n",
      "Step: 3150, train/learning_rate: 2.3793676518835127e-05\n",
      "Step: 3150, train/epoch: 5.241264343261719\n",
      "Step: 3160, train/loss: 0.04439999908208847\n",
      "Step: 3160, train/grad_norm: 0.7569411993026733\n",
      "Step: 3160, train/learning_rate: 2.3710483219474554e-05\n",
      "Step: 3160, train/epoch: 5.257903575897217\n",
      "Step: 3170, train/loss: 0.022600000724196434\n",
      "Step: 3170, train/grad_norm: 12.26883316040039\n",
      "Step: 3170, train/learning_rate: 2.3627288101124577e-05\n",
      "Step: 3170, train/epoch: 5.274542331695557\n",
      "Step: 3180, train/loss: 0.013000000268220901\n",
      "Step: 3180, train/grad_norm: 0.016452137380838394\n",
      "Step: 3180, train/learning_rate: 2.35440929827746e-05\n",
      "Step: 3180, train/epoch: 5.291181564331055\n",
      "Step: 3190, train/loss: 0.00989999994635582\n",
      "Step: 3190, train/grad_norm: 0.07464400678873062\n",
      "Step: 3190, train/learning_rate: 2.3460897864424624e-05\n",
      "Step: 3190, train/epoch: 5.3078203201293945\n",
      "Step: 3200, train/loss: 0.043699998408555984\n",
      "Step: 3200, train/grad_norm: 13.549458503723145\n",
      "Step: 3200, train/learning_rate: 2.337770456506405e-05\n",
      "Step: 3200, train/epoch: 5.324459075927734\n",
      "Step: 3210, train/loss: 0.02459999918937683\n",
      "Step: 3210, train/grad_norm: 0.04871544614434242\n",
      "Step: 3210, train/learning_rate: 2.3294509446714073e-05\n",
      "Step: 3210, train/epoch: 5.341098308563232\n",
      "Step: 3220, train/loss: 0.02850000001490116\n",
      "Step: 3220, train/grad_norm: 12.979305267333984\n",
      "Step: 3220, train/learning_rate: 2.3211314328364097e-05\n",
      "Step: 3220, train/epoch: 5.357737064361572\n",
      "Step: 3230, train/loss: 0.052799999713897705\n",
      "Step: 3230, train/grad_norm: 0.022556083276867867\n",
      "Step: 3230, train/learning_rate: 2.312811921001412e-05\n",
      "Step: 3230, train/epoch: 5.374375820159912\n",
      "Step: 3240, train/loss: 0.03579999879002571\n",
      "Step: 3240, train/grad_norm: 10.945629119873047\n",
      "Step: 3240, train/learning_rate: 2.3044925910653546e-05\n",
      "Step: 3240, train/epoch: 5.39101505279541\n",
      "Step: 3250, train/loss: 0.014100000262260437\n",
      "Step: 3250, train/grad_norm: 0.0497625507414341\n",
      "Step: 3250, train/learning_rate: 2.296173079230357e-05\n",
      "Step: 3250, train/epoch: 5.40765380859375\n",
      "Step: 3260, train/loss: 0.0008999999845400453\n",
      "Step: 3260, train/grad_norm: 0.17727713286876678\n",
      "Step: 3260, train/learning_rate: 2.2878535673953593e-05\n",
      "Step: 3260, train/epoch: 5.424293041229248\n",
      "Step: 3270, train/loss: 0.03200000151991844\n",
      "Step: 3270, train/grad_norm: 0.0830669179558754\n",
      "Step: 3270, train/learning_rate: 2.2795340555603616e-05\n",
      "Step: 3270, train/epoch: 5.440931797027588\n",
      "Step: 3280, train/loss: 0.015799999237060547\n",
      "Step: 3280, train/grad_norm: 0.7180947661399841\n",
      "Step: 3280, train/learning_rate: 2.2712147256243043e-05\n",
      "Step: 3280, train/epoch: 5.457570552825928\n",
      "Step: 3290, train/loss: 0.04050000011920929\n",
      "Step: 3290, train/grad_norm: 0.18038007616996765\n",
      "Step: 3290, train/learning_rate: 2.2628952137893066e-05\n",
      "Step: 3290, train/epoch: 5.474209785461426\n",
      "Step: 3300, train/loss: 0.010200000368058681\n",
      "Step: 3300, train/grad_norm: 0.05358428880572319\n",
      "Step: 3300, train/learning_rate: 2.254575701954309e-05\n",
      "Step: 3300, train/epoch: 5.490848541259766\n",
      "Step: 3310, train/loss: 0.006099999882280827\n",
      "Step: 3310, train/grad_norm: 0.04704916849732399\n",
      "Step: 3310, train/learning_rate: 2.2462561901193112e-05\n",
      "Step: 3310, train/epoch: 5.5074872970581055\n",
      "Step: 3320, train/loss: 0.007000000216066837\n",
      "Step: 3320, train/grad_norm: 0.00826254952698946\n",
      "Step: 3320, train/learning_rate: 2.237936860183254e-05\n",
      "Step: 3320, train/epoch: 5.5241265296936035\n",
      "Step: 3330, train/loss: 0.039500001817941666\n",
      "Step: 3330, train/grad_norm: 0.013554884120821953\n",
      "Step: 3330, train/learning_rate: 2.2296173483482562e-05\n",
      "Step: 3330, train/epoch: 5.540765285491943\n",
      "Step: 3340, train/loss: 0.035100001841783524\n",
      "Step: 3340, train/grad_norm: 26.654029846191406\n",
      "Step: 3340, train/learning_rate: 2.2212978365132585e-05\n",
      "Step: 3340, train/epoch: 5.557404518127441\n",
      "Step: 3350, train/loss: 0.006800000090152025\n",
      "Step: 3350, train/grad_norm: 0.21943268179893494\n",
      "Step: 3350, train/learning_rate: 2.212978324678261e-05\n",
      "Step: 3350, train/epoch: 5.574043273925781\n",
      "Step: 3360, train/loss: 0.023000000044703484\n",
      "Step: 3360, train/grad_norm: 0.3388117551803589\n",
      "Step: 3360, train/learning_rate: 2.204658812843263e-05\n",
      "Step: 3360, train/epoch: 5.590682029724121\n",
      "Step: 3370, train/loss: 0.011800000444054604\n",
      "Step: 3370, train/grad_norm: 0.007293786387890577\n",
      "Step: 3370, train/learning_rate: 2.1963394829072058e-05\n",
      "Step: 3370, train/epoch: 5.607321262359619\n",
      "Step: 3380, train/loss: 0.027300000190734863\n",
      "Step: 3380, train/grad_norm: 0.012802631594240665\n",
      "Step: 3380, train/learning_rate: 2.188019971072208e-05\n",
      "Step: 3380, train/epoch: 5.623960018157959\n",
      "Step: 3390, train/loss: 0.001500000013038516\n",
      "Step: 3390, train/grad_norm: 0.4829845130443573\n",
      "Step: 3390, train/learning_rate: 2.1797004592372105e-05\n",
      "Step: 3390, train/epoch: 5.640598773956299\n",
      "Step: 3400, train/loss: 0.0071000000461936\n",
      "Step: 3400, train/grad_norm: 0.13434132933616638\n",
      "Step: 3400, train/learning_rate: 2.1713809474022128e-05\n",
      "Step: 3400, train/epoch: 5.657238006591797\n",
      "Step: 3410, train/loss: 0.0008999999845400453\n",
      "Step: 3410, train/grad_norm: 0.010117864236235619\n",
      "Step: 3410, train/learning_rate: 2.1630616174661554e-05\n",
      "Step: 3410, train/epoch: 5.673876762390137\n",
      "Step: 3420, train/loss: 0.020099999383091927\n",
      "Step: 3420, train/grad_norm: 0.3204759657382965\n",
      "Step: 3420, train/learning_rate: 2.1547421056311578e-05\n",
      "Step: 3420, train/epoch: 5.690515995025635\n",
      "Step: 3430, train/loss: 0.06909999996423721\n",
      "Step: 3430, train/grad_norm: 8.211695671081543\n",
      "Step: 3430, train/learning_rate: 2.14642259379616e-05\n",
      "Step: 3430, train/epoch: 5.707154750823975\n",
      "Step: 3440, train/loss: 0.010300000198185444\n",
      "Step: 3440, train/grad_norm: 0.28671762347221375\n",
      "Step: 3440, train/learning_rate: 2.1381030819611624e-05\n",
      "Step: 3440, train/epoch: 5.7237935066223145\n",
      "Step: 3450, train/loss: 0.03610000014305115\n",
      "Step: 3450, train/grad_norm: 0.07495754957199097\n",
      "Step: 3450, train/learning_rate: 2.129783752025105e-05\n",
      "Step: 3450, train/epoch: 5.7404327392578125\n",
      "Step: 3460, train/loss: 0.017999999225139618\n",
      "Step: 3460, train/grad_norm: 16.191173553466797\n",
      "Step: 3460, train/learning_rate: 2.1214642401901074e-05\n",
      "Step: 3460, train/epoch: 5.757071495056152\n",
      "Step: 3470, train/loss: 0.01769999973475933\n",
      "Step: 3470, train/grad_norm: 12.310417175292969\n",
      "Step: 3470, train/learning_rate: 2.1131447283551097e-05\n",
      "Step: 3470, train/epoch: 5.773710250854492\n",
      "Step: 3480, train/loss: 0.018300000578165054\n",
      "Step: 3480, train/grad_norm: 0.008262217044830322\n",
      "Step: 3480, train/learning_rate: 2.104825216520112e-05\n",
      "Step: 3480, train/epoch: 5.79034948348999\n",
      "Step: 3490, train/loss: 0.021900000050663948\n",
      "Step: 3490, train/grad_norm: 0.09837721288204193\n",
      "Step: 3490, train/learning_rate: 2.0965058865840547e-05\n",
      "Step: 3490, train/epoch: 5.80698823928833\n",
      "Step: 3500, train/loss: 0.019200000911951065\n",
      "Step: 3500, train/grad_norm: 0.10267393290996552\n",
      "Step: 3500, train/learning_rate: 2.088186374749057e-05\n",
      "Step: 3500, train/epoch: 5.823627471923828\n",
      "Step: 3510, train/loss: 0.017999999225139618\n",
      "Step: 3510, train/grad_norm: 0.013066254556179047\n",
      "Step: 3510, train/learning_rate: 2.0798668629140593e-05\n",
      "Step: 3510, train/epoch: 5.840266227722168\n",
      "Step: 3520, train/loss: 0.015300000086426735\n",
      "Step: 3520, train/grad_norm: 18.84300422668457\n",
      "Step: 3520, train/learning_rate: 2.0715473510790616e-05\n",
      "Step: 3520, train/epoch: 5.856904983520508\n",
      "Step: 3530, train/loss: 0.020500000566244125\n",
      "Step: 3530, train/grad_norm: 18.501970291137695\n",
      "Step: 3530, train/learning_rate: 2.0632280211430043e-05\n",
      "Step: 3530, train/epoch: 5.873544216156006\n",
      "Step: 3540, train/loss: 0.025499999523162842\n",
      "Step: 3540, train/grad_norm: 9.178881645202637\n",
      "Step: 3540, train/learning_rate: 2.0549085093080066e-05\n",
      "Step: 3540, train/epoch: 5.890182971954346\n",
      "Step: 3550, train/loss: 0.004800000227987766\n",
      "Step: 3550, train/grad_norm: 0.001367163029499352\n",
      "Step: 3550, train/learning_rate: 2.046588997473009e-05\n",
      "Step: 3550, train/epoch: 5.9068217277526855\n",
      "Step: 3560, train/loss: 0.011099999770522118\n",
      "Step: 3560, train/grad_norm: 0.3925434648990631\n",
      "Step: 3560, train/learning_rate: 2.0382694856380112e-05\n",
      "Step: 3560, train/epoch: 5.923460960388184\n",
      "Step: 3570, train/loss: 0.014499999582767487\n",
      "Step: 3570, train/grad_norm: 0.1599249541759491\n",
      "Step: 3570, train/learning_rate: 2.029950155701954e-05\n",
      "Step: 3570, train/epoch: 5.940099716186523\n",
      "Step: 3580, train/loss: 0.0005000000237487257\n",
      "Step: 3580, train/grad_norm: 0.007888536900281906\n",
      "Step: 3580, train/learning_rate: 2.0216306438669562e-05\n",
      "Step: 3580, train/epoch: 5.9567389488220215\n",
      "Step: 3590, train/loss: 0.005400000140070915\n",
      "Step: 3590, train/grad_norm: 0.030155770480632782\n",
      "Step: 3590, train/learning_rate: 2.0133111320319586e-05\n",
      "Step: 3590, train/epoch: 5.973377704620361\n",
      "Step: 3600, train/loss: 0.02319999970495701\n",
      "Step: 3600, train/grad_norm: 0.21351559460163116\n",
      "Step: 3600, train/learning_rate: 2.004991620196961e-05\n",
      "Step: 3600, train/epoch: 5.990016460418701\n",
      "Step: 3606, eval/loss: 0.7614932060241699\n",
      "Step: 3606, eval/accuracy: 0.8871303796768188\n",
      "Step: 3606, eval/f1: 0.877392053604126\n",
      "Step: 3606, eval/runtime: 81.12539672851562\n",
      "Step: 3606, eval/samples_per_second: 88.78800201416016\n",
      "Step: 3606, eval/steps_per_second: 1.590000033378601\n",
      "Step: 3606, train/epoch: 6.0\n",
      "Step: 3610, train/loss: 0.00019999999494757503\n",
      "Step: 3610, train/grad_norm: 1.6622898578643799\n",
      "Step: 3610, train/learning_rate: 1.9966722902609035e-05\n",
      "Step: 3610, train/epoch: 6.006655693054199\n",
      "Step: 3620, train/loss: 0.0010000000474974513\n",
      "Step: 3620, train/grad_norm: 8.954622268676758\n",
      "Step: 3620, train/learning_rate: 1.988352778425906e-05\n",
      "Step: 3620, train/epoch: 6.023294448852539\n",
      "Step: 3630, train/loss: 0.03310000151395798\n",
      "Step: 3630, train/grad_norm: 0.037791792303323746\n",
      "Step: 3630, train/learning_rate: 1.9800332665909082e-05\n",
      "Step: 3630, train/epoch: 6.039933681488037\n",
      "Step: 3640, train/loss: 0.007799999788403511\n",
      "Step: 3640, train/grad_norm: 0.02959926798939705\n",
      "Step: 3640, train/learning_rate: 1.9717137547559105e-05\n",
      "Step: 3640, train/epoch: 6.056572437286377\n",
      "Step: 3650, train/loss: 0.018799999728798866\n",
      "Step: 3650, train/grad_norm: 15.459480285644531\n",
      "Step: 3650, train/learning_rate: 1.963394424819853e-05\n",
      "Step: 3650, train/epoch: 6.073211193084717\n",
      "Step: 3660, train/loss: 0.00430000014603138\n",
      "Step: 3660, train/grad_norm: 1.2712490558624268\n",
      "Step: 3660, train/learning_rate: 1.9550749129848555e-05\n",
      "Step: 3660, train/epoch: 6.089850425720215\n",
      "Step: 3670, train/loss: 0.017799999564886093\n",
      "Step: 3670, train/grad_norm: 0.010559126734733582\n",
      "Step: 3670, train/learning_rate: 1.9467554011498578e-05\n",
      "Step: 3670, train/epoch: 6.106489181518555\n",
      "Step: 3680, train/loss: 0.010900000110268593\n",
      "Step: 3680, train/grad_norm: 23.855579376220703\n",
      "Step: 3680, train/learning_rate: 1.93843588931486e-05\n",
      "Step: 3680, train/epoch: 6.1231279373168945\n",
      "Step: 3690, train/loss: 0.007899999618530273\n",
      "Step: 3690, train/grad_norm: 0.20952056348323822\n",
      "Step: 3690, train/learning_rate: 1.9301165593788028e-05\n",
      "Step: 3690, train/epoch: 6.139767169952393\n",
      "Step: 3700, train/loss: 0.007899999618530273\n",
      "Step: 3700, train/grad_norm: 0.011631473898887634\n",
      "Step: 3700, train/learning_rate: 1.921797047543805e-05\n",
      "Step: 3700, train/epoch: 6.156405925750732\n",
      "Step: 3710, train/loss: 0.005400000140070915\n",
      "Step: 3710, train/grad_norm: 0.0019308924674987793\n",
      "Step: 3710, train/learning_rate: 1.9134775357088074e-05\n",
      "Step: 3710, train/epoch: 6.1730451583862305\n",
      "Step: 3720, train/loss: 0.005499999970197678\n",
      "Step: 3720, train/grad_norm: 0.0006610799464397132\n",
      "Step: 3720, train/learning_rate: 1.9051580238738097e-05\n",
      "Step: 3720, train/epoch: 6.18968391418457\n",
      "Step: 3730, train/loss: 0.019200000911951065\n",
      "Step: 3730, train/grad_norm: 0.6703833937644958\n",
      "Step: 3730, train/learning_rate: 1.896838512038812e-05\n",
      "Step: 3730, train/epoch: 6.20632266998291\n",
      "Step: 3740, train/loss: 0.004000000189989805\n",
      "Step: 3740, train/grad_norm: 0.0013540933141484857\n",
      "Step: 3740, train/learning_rate: 1.8885191821027547e-05\n",
      "Step: 3740, train/epoch: 6.222961902618408\n",
      "Step: 3750, train/loss: 0.0010999999940395355\n",
      "Step: 3750, train/grad_norm: 21.3061580657959\n",
      "Step: 3750, train/learning_rate: 1.880199670267757e-05\n",
      "Step: 3750, train/epoch: 6.239600658416748\n",
      "Step: 3760, train/loss: 0.012799999676644802\n",
      "Step: 3760, train/grad_norm: 0.2843455970287323\n",
      "Step: 3760, train/learning_rate: 1.8718801584327593e-05\n",
      "Step: 3760, train/epoch: 6.256239414215088\n",
      "Step: 3770, train/loss: 0.03929999843239784\n",
      "Step: 3770, train/grad_norm: 0.026719670742750168\n",
      "Step: 3770, train/learning_rate: 1.8635606465977617e-05\n",
      "Step: 3770, train/epoch: 6.272878646850586\n",
      "Step: 3780, train/loss: 0.03370000049471855\n",
      "Step: 3780, train/grad_norm: 0.7187400460243225\n",
      "Step: 3780, train/learning_rate: 1.8552413166617043e-05\n",
      "Step: 3780, train/epoch: 6.289517402648926\n",
      "Step: 3790, train/loss: 0.025499999523162842\n",
      "Step: 3790, train/grad_norm: 0.4570990800857544\n",
      "Step: 3790, train/learning_rate: 1.8469218048267066e-05\n",
      "Step: 3790, train/epoch: 6.306156635284424\n",
      "Step: 3800, train/loss: 0.008799999952316284\n",
      "Step: 3800, train/grad_norm: 0.08785100281238556\n",
      "Step: 3800, train/learning_rate: 1.838602292991709e-05\n",
      "Step: 3800, train/epoch: 6.322795391082764\n",
      "Step: 3810, train/loss: 0.00039999998989515007\n",
      "Step: 3810, train/grad_norm: 0.006764447316527367\n",
      "Step: 3810, train/learning_rate: 1.8302827811567113e-05\n",
      "Step: 3810, train/epoch: 6.3394341468811035\n",
      "Step: 3820, train/loss: 0.0013000000035390258\n",
      "Step: 3820, train/grad_norm: 0.006368349771946669\n",
      "Step: 3820, train/learning_rate: 1.821963451220654e-05\n",
      "Step: 3820, train/epoch: 6.356073379516602\n",
      "Step: 3830, train/loss: 0.0003000000142492354\n",
      "Step: 3830, train/grad_norm: 1.2786762714385986\n",
      "Step: 3830, train/learning_rate: 1.8136439393856563e-05\n",
      "Step: 3830, train/epoch: 6.372712135314941\n",
      "Step: 3840, train/loss: 0.0005000000237487257\n",
      "Step: 3840, train/grad_norm: 0.15480510890483856\n",
      "Step: 3840, train/learning_rate: 1.8053244275506586e-05\n",
      "Step: 3840, train/epoch: 6.389350891113281\n",
      "Step: 3850, train/loss: 0.00570000009611249\n",
      "Step: 3850, train/grad_norm: 0.015988685190677643\n",
      "Step: 3850, train/learning_rate: 1.797004915715661e-05\n",
      "Step: 3850, train/epoch: 6.405990123748779\n",
      "Step: 3860, train/loss: 0.01549999974668026\n",
      "Step: 3860, train/grad_norm: 0.4192192852497101\n",
      "Step: 3860, train/learning_rate: 1.7886855857796036e-05\n",
      "Step: 3860, train/epoch: 6.422628879547119\n",
      "Step: 3870, train/loss: 0.013100000098347664\n",
      "Step: 3870, train/grad_norm: 0.019433321431279182\n",
      "Step: 3870, train/learning_rate: 1.780366073944606e-05\n",
      "Step: 3870, train/epoch: 6.439268112182617\n",
      "Step: 3880, train/loss: 0.02710000053048134\n",
      "Step: 3880, train/grad_norm: 0.003752671414986253\n",
      "Step: 3880, train/learning_rate: 1.7720465621096082e-05\n",
      "Step: 3880, train/epoch: 6.455906867980957\n",
      "Step: 3890, train/loss: 0.009999999776482582\n",
      "Step: 3890, train/grad_norm: 0.011628449894487858\n",
      "Step: 3890, train/learning_rate: 1.7637270502746105e-05\n",
      "Step: 3890, train/epoch: 6.472545623779297\n",
      "Step: 3900, train/loss: 0.0071000000461936\n",
      "Step: 3900, train/grad_norm: 0.0031205792911350727\n",
      "Step: 3900, train/learning_rate: 1.7554077203385532e-05\n",
      "Step: 3900, train/epoch: 6.489184856414795\n",
      "Step: 3910, train/loss: 0.00559999980032444\n",
      "Step: 3910, train/grad_norm: 0.006662706378847361\n",
      "Step: 3910, train/learning_rate: 1.7470882085035555e-05\n",
      "Step: 3910, train/epoch: 6.505823612213135\n",
      "Step: 3920, train/loss: 0.03220000118017197\n",
      "Step: 3920, train/grad_norm: 0.0017887529684230685\n",
      "Step: 3920, train/learning_rate: 1.7387686966685578e-05\n",
      "Step: 3920, train/epoch: 6.522462368011475\n",
      "Step: 3930, train/loss: 0.04839999973773956\n",
      "Step: 3930, train/grad_norm: 0.009794878773391247\n",
      "Step: 3930, train/learning_rate: 1.73044918483356e-05\n",
      "Step: 3930, train/epoch: 6.539101600646973\n",
      "Step: 3940, train/loss: 0.0019000000320374966\n",
      "Step: 3940, train/grad_norm: 0.0141519233584404\n",
      "Step: 3940, train/learning_rate: 1.7221298548975028e-05\n",
      "Step: 3940, train/epoch: 6.5557403564453125\n",
      "Step: 3950, train/loss: 0.02930000051856041\n",
      "Step: 3950, train/grad_norm: 0.05983831360936165\n",
      "Step: 3950, train/learning_rate: 1.713810343062505e-05\n",
      "Step: 3950, train/epoch: 6.5723795890808105\n",
      "Step: 3960, train/loss: 0.00930000003427267\n",
      "Step: 3960, train/grad_norm: 0.11989559978246689\n",
      "Step: 3960, train/learning_rate: 1.7054908312275074e-05\n",
      "Step: 3960, train/epoch: 6.58901834487915\n",
      "Step: 3970, train/loss: 0.03180000185966492\n",
      "Step: 3970, train/grad_norm: 14.402530670166016\n",
      "Step: 3970, train/learning_rate: 1.6971713193925098e-05\n",
      "Step: 3970, train/epoch: 6.60565710067749\n",
      "Step: 3980, train/loss: 0.07569999992847443\n",
      "Step: 3980, train/grad_norm: 0.2820259630680084\n",
      "Step: 3980, train/learning_rate: 1.6888519894564524e-05\n",
      "Step: 3980, train/epoch: 6.622296333312988\n",
      "Step: 3990, train/loss: 0.007600000128149986\n",
      "Step: 3990, train/grad_norm: 0.017688274383544922\n",
      "Step: 3990, train/learning_rate: 1.6805324776214547e-05\n",
      "Step: 3990, train/epoch: 6.638935089111328\n",
      "Step: 4000, train/loss: 0.005900000222027302\n",
      "Step: 4000, train/grad_norm: 0.05742119997739792\n",
      "Step: 4000, train/learning_rate: 1.672212965786457e-05\n",
      "Step: 4000, train/epoch: 6.655573844909668\n",
      "Step: 4010, train/loss: 0.046799998730421066\n",
      "Step: 4010, train/grad_norm: 0.019646452739834785\n",
      "Step: 4010, train/learning_rate: 1.6638934539514594e-05\n",
      "Step: 4010, train/epoch: 6.672213077545166\n",
      "Step: 4020, train/loss: 0.05139999836683273\n",
      "Step: 4020, train/grad_norm: 16.010717391967773\n",
      "Step: 4020, train/learning_rate: 1.655574124015402e-05\n",
      "Step: 4020, train/epoch: 6.688851833343506\n",
      "Step: 4030, train/loss: 0.04729999974370003\n",
      "Step: 4030, train/grad_norm: 7.5811543464660645\n",
      "Step: 4030, train/learning_rate: 1.6472546121804044e-05\n",
      "Step: 4030, train/epoch: 6.705491065979004\n",
      "Step: 4040, train/loss: 0.024700000882148743\n",
      "Step: 4040, train/grad_norm: 11.639124870300293\n",
      "Step: 4040, train/learning_rate: 1.6389351003454067e-05\n",
      "Step: 4040, train/epoch: 6.722129821777344\n",
      "Step: 4050, train/loss: 0.008299999870359898\n",
      "Step: 4050, train/grad_norm: 0.03987877443432808\n",
      "Step: 4050, train/learning_rate: 1.630615588510409e-05\n",
      "Step: 4050, train/epoch: 6.738768577575684\n",
      "Step: 4060, train/loss: 0.006000000052154064\n",
      "Step: 4060, train/grad_norm: 0.21223118901252747\n",
      "Step: 4060, train/learning_rate: 1.6222962585743517e-05\n",
      "Step: 4060, train/epoch: 6.755407810211182\n",
      "Step: 4070, train/loss: 0.01590000092983246\n",
      "Step: 4070, train/grad_norm: 2.929969310760498\n",
      "Step: 4070, train/learning_rate: 1.613976746739354e-05\n",
      "Step: 4070, train/epoch: 6.7720465660095215\n",
      "Step: 4080, train/loss: 0.0005000000237487257\n",
      "Step: 4080, train/grad_norm: 0.019801992923021317\n",
      "Step: 4080, train/learning_rate: 1.6056572349043563e-05\n",
      "Step: 4080, train/epoch: 6.788685321807861\n",
      "Step: 4090, train/loss: 0.00019999999494757503\n",
      "Step: 4090, train/grad_norm: 0.007694380823522806\n",
      "Step: 4090, train/learning_rate: 1.5973377230693586e-05\n",
      "Step: 4090, train/epoch: 6.805324554443359\n",
      "Step: 4100, train/loss: 0.031599998474121094\n",
      "Step: 4100, train/grad_norm: 19.122411727905273\n",
      "Step: 4100, train/learning_rate: 1.5890183931333013e-05\n",
      "Step: 4100, train/epoch: 6.821963310241699\n",
      "Step: 4110, train/loss: 0.010400000028312206\n",
      "Step: 4110, train/grad_norm: 0.0039468081668019295\n",
      "Step: 4110, train/learning_rate: 1.5806988812983036e-05\n",
      "Step: 4110, train/epoch: 6.838602542877197\n",
      "Step: 4120, train/loss: 0.014000000432133675\n",
      "Step: 4120, train/grad_norm: 0.5546874403953552\n",
      "Step: 4120, train/learning_rate: 1.572379369463306e-05\n",
      "Step: 4120, train/epoch: 6.855241298675537\n",
      "Step: 4130, train/loss: 0.02199999988079071\n",
      "Step: 4130, train/grad_norm: 11.902568817138672\n",
      "Step: 4130, train/learning_rate: 1.5640598576283082e-05\n",
      "Step: 4130, train/epoch: 6.871880054473877\n",
      "Step: 4140, train/loss: 0.0035000001080334187\n",
      "Step: 4140, train/grad_norm: 0.026944352313876152\n",
      "Step: 4140, train/learning_rate: 1.5557403457933106e-05\n",
      "Step: 4140, train/epoch: 6.888519287109375\n",
      "Step: 4150, train/loss: 0.01209999993443489\n",
      "Step: 4150, train/grad_norm: 14.36258316040039\n",
      "Step: 4150, train/learning_rate: 1.5474210158572532e-05\n",
      "Step: 4150, train/epoch: 6.905158042907715\n",
      "Step: 4160, train/loss: 0.014700000174343586\n",
      "Step: 4160, train/grad_norm: 0.018140392377972603\n",
      "Step: 4160, train/learning_rate: 1.5391015040222555e-05\n",
      "Step: 4160, train/epoch: 6.921796798706055\n",
      "Step: 4170, train/loss: 0.0003000000142492354\n",
      "Step: 4170, train/grad_norm: 0.011942432262003422\n",
      "Step: 4170, train/learning_rate: 1.530781992187258e-05\n",
      "Step: 4170, train/epoch: 6.938436031341553\n",
      "Step: 4180, train/loss: 0.0024999999441206455\n",
      "Step: 4180, train/grad_norm: 0.11077415943145752\n",
      "Step: 4180, train/learning_rate: 1.5224625713017303e-05\n",
      "Step: 4180, train/epoch: 6.955074787139893\n",
      "Step: 4190, train/loss: 0.012199999764561653\n",
      "Step: 4190, train/grad_norm: 0.22780875861644745\n",
      "Step: 4190, train/learning_rate: 1.5141430594667327e-05\n",
      "Step: 4190, train/epoch: 6.971714019775391\n",
      "Step: 4200, train/loss: 0.006200000178068876\n",
      "Step: 4200, train/grad_norm: 0.16444896161556244\n",
      "Step: 4200, train/learning_rate: 1.5058236385812052e-05\n",
      "Step: 4200, train/epoch: 6.9883527755737305\n",
      "Step: 4207, eval/loss: 0.6612853407859802\n",
      "Step: 4207, eval/accuracy: 0.8940719366073608\n",
      "Step: 4207, eval/f1: 0.8862751722335815\n",
      "Step: 4207, eval/runtime: 80.94640350341797\n",
      "Step: 4207, eval/samples_per_second: 88.98500061035156\n",
      "Step: 4207, eval/steps_per_second: 1.593999981880188\n",
      "Step: 4207, train/epoch: 7.0\n",
      "Step: 4210, train/loss: 0.000699999975040555\n",
      "Step: 4210, train/grad_norm: 0.0023968794848769903\n",
      "Step: 4210, train/learning_rate: 1.4975041267462075e-05\n",
      "Step: 4210, train/epoch: 7.00499153137207\n",
      "Step: 4220, train/loss: 0.006800000090152025\n",
      "Step: 4220, train/grad_norm: 0.0052306619472801685\n",
      "Step: 4220, train/learning_rate: 1.48918470586068e-05\n",
      "Step: 4220, train/epoch: 7.021630764007568\n",
      "Step: 4230, train/loss: 0.00039999998989515007\n",
      "Step: 4230, train/grad_norm: 0.11341997236013412\n",
      "Step: 4230, train/learning_rate: 1.4808651940256823e-05\n",
      "Step: 4230, train/epoch: 7.038269519805908\n",
      "Step: 4240, train/loss: 0.010099999606609344\n",
      "Step: 4240, train/grad_norm: 0.012986968271434307\n",
      "Step: 4240, train/learning_rate: 1.4725457731401548e-05\n",
      "Step: 4240, train/epoch: 7.054908275604248\n",
      "Step: 4250, train/loss: 0.012500000186264515\n",
      "Step: 4250, train/grad_norm: 0.1236957535147667\n",
      "Step: 4250, train/learning_rate: 1.4642262613051571e-05\n",
      "Step: 4250, train/epoch: 7.071547508239746\n",
      "Step: 4260, train/loss: 0.010099999606609344\n",
      "Step: 4260, train/grad_norm: 0.021851612254977226\n",
      "Step: 4260, train/learning_rate: 1.4559068404196296e-05\n",
      "Step: 4260, train/epoch: 7.088186264038086\n",
      "Step: 4270, train/loss: 0.0010000000474974513\n",
      "Step: 4270, train/grad_norm: 0.002221910050138831\n",
      "Step: 4270, train/learning_rate: 1.4475873285846319e-05\n",
      "Step: 4270, train/epoch: 7.104825496673584\n",
      "Step: 4280, train/loss: 0.0005000000237487257\n",
      "Step: 4280, train/grad_norm: 0.08578064292669296\n",
      "Step: 4280, train/learning_rate: 1.4392679076991044e-05\n",
      "Step: 4280, train/epoch: 7.121464252471924\n",
      "Step: 4290, train/loss: 0.00019999999494757503\n",
      "Step: 4290, train/grad_norm: 0.0017999415285885334\n",
      "Step: 4290, train/learning_rate: 1.4309483958641067e-05\n",
      "Step: 4290, train/epoch: 7.138103008270264\n",
      "Step: 4300, train/loss: 9.999999747378752e-05\n",
      "Step: 4300, train/grad_norm: 0.04550759494304657\n",
      "Step: 4300, train/learning_rate: 1.4226289749785792e-05\n",
      "Step: 4300, train/epoch: 7.154742240905762\n",
      "Step: 4310, train/loss: 0.019999999552965164\n",
      "Step: 4310, train/grad_norm: 0.0055239517241716385\n",
      "Step: 4310, train/learning_rate: 1.4143094631435815e-05\n",
      "Step: 4310, train/epoch: 7.171380996704102\n",
      "Step: 4320, train/loss: 0.015399999916553497\n",
      "Step: 4320, train/grad_norm: 12.271554946899414\n",
      "Step: 4320, train/learning_rate: 1.405990042258054e-05\n",
      "Step: 4320, train/epoch: 7.188019752502441\n",
      "Step: 4330, train/loss: 0.0017000000225380063\n",
      "Step: 4330, train/grad_norm: 4.480959892272949\n",
      "Step: 4330, train/learning_rate: 1.3976705304230563e-05\n",
      "Step: 4330, train/epoch: 7.2046589851379395\n",
      "Step: 4340, train/loss: 0.024900000542402267\n",
      "Step: 4340, train/grad_norm: 19.89503288269043\n",
      "Step: 4340, train/learning_rate: 1.3893511095375288e-05\n",
      "Step: 4340, train/epoch: 7.221297740936279\n",
      "Step: 4350, train/loss: 0.0017000000225380063\n",
      "Step: 4350, train/grad_norm: 0.015090561471879482\n",
      "Step: 4350, train/learning_rate: 1.3810315977025311e-05\n",
      "Step: 4350, train/epoch: 7.237936973571777\n",
      "Step: 4360, train/loss: 0.0348999984562397\n",
      "Step: 4360, train/grad_norm: 17.300004959106445\n",
      "Step: 4360, train/learning_rate: 1.3727121768170036e-05\n",
      "Step: 4360, train/epoch: 7.254575729370117\n",
      "Step: 4370, train/loss: 0.02410000003874302\n",
      "Step: 4370, train/grad_norm: 0.002285287482663989\n",
      "Step: 4370, train/learning_rate: 1.364392664982006e-05\n",
      "Step: 4370, train/epoch: 7.271214485168457\n",
      "Step: 4380, train/loss: 0.014000000432133675\n",
      "Step: 4380, train/grad_norm: 0.04582372307777405\n",
      "Step: 4380, train/learning_rate: 1.3560732440964784e-05\n",
      "Step: 4380, train/epoch: 7.287853717803955\n",
      "Step: 4390, train/loss: 0.00860000029206276\n",
      "Step: 4390, train/grad_norm: 0.02979191206395626\n",
      "Step: 4390, train/learning_rate: 1.3477537322614808e-05\n",
      "Step: 4390, train/epoch: 7.304492473602295\n",
      "Step: 4400, train/loss: 0.008899999782443047\n",
      "Step: 4400, train/grad_norm: 0.02045493572950363\n",
      "Step: 4400, train/learning_rate: 1.3394343113759533e-05\n",
      "Step: 4400, train/epoch: 7.321131229400635\n",
      "Step: 4410, train/loss: 0.010200000368058681\n",
      "Step: 4410, train/grad_norm: 0.021721534430980682\n",
      "Step: 4410, train/learning_rate: 1.3311147995409556e-05\n",
      "Step: 4410, train/epoch: 7.337770462036133\n",
      "Step: 4420, train/loss: 0.00019999999494757503\n",
      "Step: 4420, train/grad_norm: 0.027428239583969116\n",
      "Step: 4420, train/learning_rate: 1.322795378655428e-05\n",
      "Step: 4420, train/epoch: 7.354409217834473\n",
      "Step: 4430, train/loss: 0.003599999938160181\n",
      "Step: 4430, train/grad_norm: 19.804410934448242\n",
      "Step: 4430, train/learning_rate: 1.3144758668204304e-05\n",
      "Step: 4430, train/epoch: 7.371048450469971\n",
      "Step: 4440, train/loss: 9.999999747378752e-05\n",
      "Step: 4440, train/grad_norm: 0.06700112670660019\n",
      "Step: 4440, train/learning_rate: 1.3061564459349029e-05\n",
      "Step: 4440, train/epoch: 7.3876872062683105\n",
      "Step: 4450, train/loss: 0.005100000184029341\n",
      "Step: 4450, train/grad_norm: 0.004190264735370874\n",
      "Step: 4450, train/learning_rate: 1.2978369340999052e-05\n",
      "Step: 4450, train/epoch: 7.40432596206665\n",
      "Step: 4460, train/loss: 9.999999747378752e-05\n",
      "Step: 4460, train/grad_norm: 0.0019355458207428455\n",
      "Step: 4460, train/learning_rate: 1.2895175132143777e-05\n",
      "Step: 4460, train/epoch: 7.420965194702148\n",
      "Step: 4470, train/loss: 0.0038999998942017555\n",
      "Step: 4470, train/grad_norm: 0.3602466285228729\n",
      "Step: 4470, train/learning_rate: 1.28119800137938e-05\n",
      "Step: 4470, train/epoch: 7.437603950500488\n",
      "Step: 4480, train/loss: 9.999999747378752e-05\n",
      "Step: 4480, train/grad_norm: 0.001036735367961228\n",
      "Step: 4480, train/learning_rate: 1.2728785804938525e-05\n",
      "Step: 4480, train/epoch: 7.454242706298828\n",
      "Step: 4490, train/loss: 9.999999747378752e-05\n",
      "Step: 4490, train/grad_norm: 0.00404978496953845\n",
      "Step: 4490, train/learning_rate: 1.2645590686588548e-05\n",
      "Step: 4490, train/epoch: 7.470881938934326\n",
      "Step: 4500, train/loss: 0.007499999832361937\n",
      "Step: 4500, train/grad_norm: 0.21923211216926575\n",
      "Step: 4500, train/learning_rate: 1.2562395568238571e-05\n",
      "Step: 4500, train/epoch: 7.487520694732666\n",
      "Step: 4510, train/loss: 0.00839999970048666\n",
      "Step: 4510, train/grad_norm: 0.02831721305847168\n",
      "Step: 4510, train/learning_rate: 1.2479201359383296e-05\n",
      "Step: 4510, train/epoch: 7.504159927368164\n",
      "Step: 4520, train/loss: 0.0027000000700354576\n",
      "Step: 4520, train/grad_norm: 0.0015441622817888856\n",
      "Step: 4520, train/learning_rate: 1.239600624103332e-05\n",
      "Step: 4520, train/epoch: 7.520798683166504\n",
      "Step: 4530, train/loss: 0.02979999966919422\n",
      "Step: 4530, train/grad_norm: 0.0599154569208622\n",
      "Step: 4530, train/learning_rate: 1.2312812032178044e-05\n",
      "Step: 4530, train/epoch: 7.537437438964844\n",
      "Step: 4540, train/loss: 0.0031999999191612005\n",
      "Step: 4540, train/grad_norm: 0.002454683417454362\n",
      "Step: 4540, train/learning_rate: 1.2229616913828067e-05\n",
      "Step: 4540, train/epoch: 7.554076671600342\n",
      "Step: 4550, train/loss: 0.0066999997943639755\n",
      "Step: 4550, train/grad_norm: 0.007617061026394367\n",
      "Step: 4550, train/learning_rate: 1.2146422704972792e-05\n",
      "Step: 4550, train/epoch: 7.570715427398682\n",
      "Step: 4560, train/loss: 0.032099999487400055\n",
      "Step: 4560, train/grad_norm: 14.078611373901367\n",
      "Step: 4560, train/learning_rate: 1.2063227586622816e-05\n",
      "Step: 4560, train/epoch: 7.5873541831970215\n",
      "Step: 4570, train/loss: 0.007799999788403511\n",
      "Step: 4570, train/grad_norm: 9.24219799041748\n",
      "Step: 4570, train/learning_rate: 1.198003337776754e-05\n",
      "Step: 4570, train/epoch: 7.6039934158325195\n",
      "Step: 4580, train/loss: 0.00019999999494757503\n",
      "Step: 4580, train/grad_norm: 0.08814805001020432\n",
      "Step: 4580, train/learning_rate: 1.1896838259417564e-05\n",
      "Step: 4580, train/epoch: 7.620632171630859\n",
      "Step: 4590, train/loss: 0.004399999976158142\n",
      "Step: 4590, train/grad_norm: 0.006935791112482548\n",
      "Step: 4590, train/learning_rate: 1.1813644050562289e-05\n",
      "Step: 4590, train/epoch: 7.637271404266357\n",
      "Step: 4600, train/loss: 0.003000000026077032\n",
      "Step: 4600, train/grad_norm: 3.9059090614318848\n",
      "Step: 4600, train/learning_rate: 1.1730448932212312e-05\n",
      "Step: 4600, train/epoch: 7.653910160064697\n",
      "Step: 4610, train/loss: 0.014700000174343586\n",
      "Step: 4610, train/grad_norm: 0.0028248389717191458\n",
      "Step: 4610, train/learning_rate: 1.1647254723357037e-05\n",
      "Step: 4610, train/epoch: 7.670548915863037\n",
      "Step: 4620, train/loss: 0.012799999676644802\n",
      "Step: 4620, train/grad_norm: 7.761115074157715\n",
      "Step: 4620, train/learning_rate: 1.156405960500706e-05\n",
      "Step: 4620, train/epoch: 7.687188148498535\n",
      "Step: 4630, train/loss: 0.0012000000569969416\n",
      "Step: 4630, train/grad_norm: 21.777402877807617\n",
      "Step: 4630, train/learning_rate: 1.1480865396151785e-05\n",
      "Step: 4630, train/epoch: 7.703826904296875\n",
      "Step: 4640, train/loss: 0.015399999916553497\n",
      "Step: 4640, train/grad_norm: 42.067230224609375\n",
      "Step: 4640, train/learning_rate: 1.1397670277801808e-05\n",
      "Step: 4640, train/epoch: 7.720465660095215\n",
      "Step: 4650, train/loss: 0.007600000128149986\n",
      "Step: 4650, train/grad_norm: 0.0006406084867194295\n",
      "Step: 4650, train/learning_rate: 1.1314476068946533e-05\n",
      "Step: 4650, train/epoch: 7.737104892730713\n",
      "Step: 4660, train/loss: 0.0006000000284984708\n",
      "Step: 4660, train/grad_norm: 0.10855291038751602\n",
      "Step: 4660, train/learning_rate: 1.1231280950596556e-05\n",
      "Step: 4660, train/epoch: 7.753743648529053\n",
      "Step: 4670, train/loss: 0.017999999225139618\n",
      "Step: 4670, train/grad_norm: 0.0009347275481559336\n",
      "Step: 4670, train/learning_rate: 1.1148086741741281e-05\n",
      "Step: 4670, train/epoch: 7.770382881164551\n",
      "Step: 4680, train/loss: 0.02930000051856041\n",
      "Step: 4680, train/grad_norm: 0.0009983604541048408\n",
      "Step: 4680, train/learning_rate: 1.1064891623391304e-05\n",
      "Step: 4680, train/epoch: 7.787021636962891\n",
      "Step: 4690, train/loss: 0.021400000900030136\n",
      "Step: 4690, train/grad_norm: 11.106881141662598\n",
      "Step: 4690, train/learning_rate: 1.0981697414536029e-05\n",
      "Step: 4690, train/epoch: 7.8036603927612305\n",
      "Step: 4700, train/loss: 0.007799999788403511\n",
      "Step: 4700, train/grad_norm: 0.009443010203540325\n",
      "Step: 4700, train/learning_rate: 1.0898502296186052e-05\n",
      "Step: 4700, train/epoch: 7.8202996253967285\n",
      "Step: 4710, train/loss: 0.019099999219179153\n",
      "Step: 4710, train/grad_norm: 0.010231172665953636\n",
      "Step: 4710, train/learning_rate: 1.0815308087330777e-05\n",
      "Step: 4710, train/epoch: 7.836938381195068\n",
      "Step: 4720, train/loss: 0.01720000058412552\n",
      "Step: 4720, train/grad_norm: 0.007262092083692551\n",
      "Step: 4720, train/learning_rate: 1.07321129689808e-05\n",
      "Step: 4720, train/epoch: 7.853577136993408\n",
      "Step: 4730, train/loss: 0.00430000014603138\n",
      "Step: 4730, train/grad_norm: 0.037078648805618286\n",
      "Step: 4730, train/learning_rate: 1.0648918760125525e-05\n",
      "Step: 4730, train/epoch: 7.870216369628906\n",
      "Step: 4740, train/loss: 0.005799999926239252\n",
      "Step: 4740, train/grad_norm: 10.838316917419434\n",
      "Step: 4740, train/learning_rate: 1.0565723641775548e-05\n",
      "Step: 4740, train/epoch: 7.886855125427246\n",
      "Step: 4750, train/loss: 0.008799999952316284\n",
      "Step: 4750, train/grad_norm: 0.018246853724122047\n",
      "Step: 4750, train/learning_rate: 1.0482529432920273e-05\n",
      "Step: 4750, train/epoch: 7.903494358062744\n",
      "Step: 4760, train/loss: 0.002099999925121665\n",
      "Step: 4760, train/grad_norm: 0.010813702829182148\n",
      "Step: 4760, train/learning_rate: 1.0399334314570297e-05\n",
      "Step: 4760, train/epoch: 7.920133113861084\n",
      "Step: 4770, train/loss: 9.999999747378752e-05\n",
      "Step: 4770, train/grad_norm: 0.0009137290180660784\n",
      "Step: 4770, train/learning_rate: 1.0316140105715021e-05\n",
      "Step: 4770, train/epoch: 7.936771869659424\n",
      "Step: 4780, train/loss: 9.999999747378752e-05\n",
      "Step: 4780, train/grad_norm: 0.005046097561717033\n",
      "Step: 4780, train/learning_rate: 1.0232944987365045e-05\n",
      "Step: 4780, train/epoch: 7.953411102294922\n",
      "Step: 4790, train/loss: 0.00039999998989515007\n",
      "Step: 4790, train/grad_norm: 4.900938034057617\n",
      "Step: 4790, train/learning_rate: 1.014975077850977e-05\n",
      "Step: 4790, train/epoch: 7.970049858093262\n",
      "Step: 4800, train/loss: 0.015399999916553497\n",
      "Step: 4800, train/grad_norm: 0.022846629843115807\n",
      "Step: 4800, train/learning_rate: 1.0066555660159793e-05\n",
      "Step: 4800, train/epoch: 7.986688613891602\n",
      "Step: 4808, eval/loss: 0.8205753564834595\n",
      "Step: 4808, eval/accuracy: 0.8850479125976562\n",
      "Step: 4808, eval/f1: 0.8737701177597046\n",
      "Step: 4808, eval/runtime: 80.9186019897461\n",
      "Step: 4808, eval/samples_per_second: 89.01499938964844\n",
      "Step: 4808, eval/steps_per_second: 1.593999981880188\n",
      "Step: 4808, train/epoch: 8.0\n",
      "Step: 4810, train/loss: 0.00039999998989515007\n",
      "Step: 4810, train/grad_norm: 3.7829530239105225\n",
      "Step: 4810, train/learning_rate: 9.983361451304518e-06\n",
      "Step: 4810, train/epoch: 8.003327369689941\n",
      "Step: 4820, train/loss: 0.01269999984651804\n",
      "Step: 4820, train/grad_norm: 0.05273885279893875\n",
      "Step: 4820, train/learning_rate: 9.900166332954541e-06\n",
      "Step: 4820, train/epoch: 8.019967079162598\n",
      "Step: 4830, train/loss: 0.014700000174343586\n",
      "Step: 4830, train/grad_norm: 0.0008335012826137245\n",
      "Step: 4830, train/learning_rate: 9.816972124099266e-06\n",
      "Step: 4830, train/epoch: 8.036605834960938\n",
      "Step: 4840, train/loss: 0.005799999926239252\n",
      "Step: 4840, train/grad_norm: 0.004663687199354172\n",
      "Step: 4840, train/learning_rate: 9.733777005749289e-06\n",
      "Step: 4840, train/epoch: 8.053244590759277\n",
      "Step: 4850, train/loss: 0.00019999999494757503\n",
      "Step: 4850, train/grad_norm: 0.000646020460408181\n",
      "Step: 4850, train/learning_rate: 9.650582796894014e-06\n",
      "Step: 4850, train/epoch: 8.069883346557617\n",
      "Step: 4860, train/loss: 9.999999747378752e-05\n",
      "Step: 4860, train/grad_norm: 0.014409000054001808\n",
      "Step: 4860, train/learning_rate: 9.567387678544037e-06\n",
      "Step: 4860, train/epoch: 8.086522102355957\n",
      "Step: 4870, train/loss: 0.018200000748038292\n",
      "Step: 4870, train/grad_norm: 0.0004957150085829198\n",
      "Step: 4870, train/learning_rate: 9.48419256019406e-06\n",
      "Step: 4870, train/epoch: 8.103161811828613\n",
      "Step: 4880, train/loss: 0.023499999195337296\n",
      "Step: 4880, train/grad_norm: 0.003847949206829071\n",
      "Step: 4880, train/learning_rate: 9.400998351338785e-06\n",
      "Step: 4880, train/epoch: 8.119800567626953\n",
      "Step: 4890, train/loss: 0.013899999670684338\n",
      "Step: 4890, train/grad_norm: 0.07798745483160019\n",
      "Step: 4890, train/learning_rate: 9.317803232988808e-06\n",
      "Step: 4890, train/epoch: 8.136439323425293\n",
      "Step: 4900, train/loss: 0.008999999612569809\n",
      "Step: 4900, train/grad_norm: 4.479844093322754\n",
      "Step: 4900, train/learning_rate: 9.234609024133533e-06\n",
      "Step: 4900, train/epoch: 8.153078079223633\n",
      "Step: 4910, train/loss: 0.029100000858306885\n",
      "Step: 4910, train/grad_norm: 21.16271209716797\n",
      "Step: 4910, train/learning_rate: 9.151413905783556e-06\n",
      "Step: 4910, train/epoch: 8.169716835021973\n",
      "Step: 4920, train/loss: 0.0034000000450760126\n",
      "Step: 4920, train/grad_norm: 0.02794240415096283\n",
      "Step: 4920, train/learning_rate: 9.068219696928281e-06\n",
      "Step: 4920, train/epoch: 8.186356544494629\n",
      "Step: 4930, train/loss: 9.999999747378752e-05\n",
      "Step: 4930, train/grad_norm: 0.17829817533493042\n",
      "Step: 4930, train/learning_rate: 8.985024578578304e-06\n",
      "Step: 4930, train/epoch: 8.202995300292969\n",
      "Step: 4940, train/loss: 0.004399999976158142\n",
      "Step: 4940, train/grad_norm: 0.0008688584784977138\n",
      "Step: 4940, train/learning_rate: 8.90183036972303e-06\n",
      "Step: 4940, train/epoch: 8.219634056091309\n",
      "Step: 4950, train/loss: 0.007699999958276749\n",
      "Step: 4950, train/grad_norm: 14.093936920166016\n",
      "Step: 4950, train/learning_rate: 8.818635251373053e-06\n",
      "Step: 4950, train/epoch: 8.236272811889648\n",
      "Step: 4960, train/loss: 0.005499999970197678\n",
      "Step: 4960, train/grad_norm: 0.009275692515075207\n",
      "Step: 4960, train/learning_rate: 8.735441042517778e-06\n",
      "Step: 4960, train/epoch: 8.252911567687988\n",
      "Step: 4970, train/loss: 0.0\n",
      "Step: 4970, train/grad_norm: 0.013558149337768555\n",
      "Step: 4970, train/learning_rate: 8.6522459241678e-06\n",
      "Step: 4970, train/epoch: 8.269550323486328\n",
      "Step: 4980, train/loss: 9.999999747378752e-05\n",
      "Step: 4980, train/grad_norm: 0.0017318419413641095\n",
      "Step: 4980, train/learning_rate: 8.569051715312526e-06\n",
      "Step: 4980, train/epoch: 8.286190032958984\n",
      "Step: 4990, train/loss: 0.0\n",
      "Step: 4990, train/grad_norm: 0.0018796606454998255\n",
      "Step: 4990, train/learning_rate: 8.485856596962549e-06\n",
      "Step: 4990, train/epoch: 8.302828788757324\n",
      "Step: 5000, train/loss: 0.004100000020116568\n",
      "Step: 5000, train/grad_norm: 0.019527707248926163\n",
      "Step: 5000, train/learning_rate: 8.402662388107274e-06\n",
      "Step: 5000, train/epoch: 8.319467544555664\n",
      "Step: 5010, train/loss: 9.999999747378752e-05\n",
      "Step: 5010, train/grad_norm: 0.0012054108083248138\n",
      "Step: 5010, train/learning_rate: 8.319467269757297e-06\n",
      "Step: 5010, train/epoch: 8.336106300354004\n",
      "Step: 5020, train/loss: 0.014100000262260437\n",
      "Step: 5020, train/grad_norm: 0.003433455480262637\n",
      "Step: 5020, train/learning_rate: 8.236273060902022e-06\n",
      "Step: 5020, train/epoch: 8.352745056152344\n",
      "Step: 5030, train/loss: 0.0\n",
      "Step: 5030, train/grad_norm: 0.040278058499097824\n",
      "Step: 5030, train/learning_rate: 8.153077942552045e-06\n",
      "Step: 5030, train/epoch: 8.369384765625\n",
      "Step: 5040, train/loss: 0.0012000000569969416\n",
      "Step: 5040, train/grad_norm: 0.013159656897187233\n",
      "Step: 5040, train/learning_rate: 8.06988373369677e-06\n",
      "Step: 5040, train/epoch: 8.38602352142334\n",
      "Step: 5050, train/loss: 0.0\n",
      "Step: 5050, train/grad_norm: 0.0005327864200808108\n",
      "Step: 5050, train/learning_rate: 7.986688615346793e-06\n",
      "Step: 5050, train/epoch: 8.40266227722168\n",
      "Step: 5060, train/loss: 0.0005000000237487257\n",
      "Step: 5060, train/grad_norm: 0.0008077782113105059\n",
      "Step: 5060, train/learning_rate: 7.903494406491518e-06\n",
      "Step: 5060, train/epoch: 8.41930103302002\n",
      "Step: 5070, train/loss: 0.0005000000237487257\n",
      "Step: 5070, train/grad_norm: 0.0025086584500968456\n",
      "Step: 5070, train/learning_rate: 7.820299288141541e-06\n",
      "Step: 5070, train/epoch: 8.43593978881836\n",
      "Step: 5080, train/loss: 0.0\n",
      "Step: 5080, train/grad_norm: 0.0008108507026918232\n",
      "Step: 5080, train/learning_rate: 7.737105079286266e-06\n",
      "Step: 5080, train/epoch: 8.452579498291016\n",
      "Step: 5090, train/loss: 9.999999747378752e-05\n",
      "Step: 5090, train/grad_norm: 0.05273452773690224\n",
      "Step: 5090, train/learning_rate: 7.65390996093629e-06\n",
      "Step: 5090, train/epoch: 8.469218254089355\n",
      "Step: 5100, train/loss: 9.999999747378752e-05\n",
      "Step: 5100, train/grad_norm: 0.001463445951230824\n",
      "Step: 5100, train/learning_rate: 7.570715297333663e-06\n",
      "Step: 5100, train/epoch: 8.485857009887695\n",
      "Step: 5110, train/loss: 0.005499999970197678\n",
      "Step: 5110, train/grad_norm: 0.32461676001548767\n",
      "Step: 5110, train/learning_rate: 7.487520633731037e-06\n",
      "Step: 5110, train/epoch: 8.502495765686035\n",
      "Step: 5120, train/loss: 0.02979999966919422\n",
      "Step: 5120, train/grad_norm: 0.0008804670069366693\n",
      "Step: 5120, train/learning_rate: 7.4043259701284114e-06\n",
      "Step: 5120, train/epoch: 8.519134521484375\n",
      "Step: 5130, train/loss: 0.003599999938160181\n",
      "Step: 5130, train/grad_norm: 22.449987411499023\n",
      "Step: 5130, train/learning_rate: 7.3211313065257855e-06\n",
      "Step: 5130, train/epoch: 8.535773277282715\n",
      "Step: 5140, train/loss: 0.000699999975040555\n",
      "Step: 5140, train/grad_norm: 1.4169235229492188\n",
      "Step: 5140, train/learning_rate: 7.2379366429231595e-06\n",
      "Step: 5140, train/epoch: 8.552412986755371\n",
      "Step: 5150, train/loss: 0.008100000210106373\n",
      "Step: 5150, train/grad_norm: 0.001821876154281199\n",
      "Step: 5150, train/learning_rate: 7.1547419793205336e-06\n",
      "Step: 5150, train/epoch: 8.569051742553711\n",
      "Step: 5160, train/loss: 9.999999747378752e-05\n",
      "Step: 5160, train/grad_norm: 0.00047173656639643013\n",
      "Step: 5160, train/learning_rate: 7.071547315717908e-06\n",
      "Step: 5160, train/epoch: 8.58569049835205\n",
      "Step: 5170, train/loss: 0.000699999975040555\n",
      "Step: 5170, train/grad_norm: 0.05947134643793106\n",
      "Step: 5170, train/learning_rate: 6.988352652115282e-06\n",
      "Step: 5170, train/epoch: 8.60232925415039\n",
      "Step: 5180, train/loss: 9.999999747378752e-05\n",
      "Step: 5180, train/grad_norm: 0.008470514789223671\n",
      "Step: 5180, train/learning_rate: 6.905157988512656e-06\n",
      "Step: 5180, train/epoch: 8.61896800994873\n",
      "Step: 5190, train/loss: 0.008100000210106373\n",
      "Step: 5190, train/grad_norm: 0.07845903187990189\n",
      "Step: 5190, train/learning_rate: 6.82196332491003e-06\n",
      "Step: 5190, train/epoch: 8.635607719421387\n",
      "Step: 5200, train/loss: 9.999999747378752e-05\n",
      "Step: 5200, train/grad_norm: 0.2132355272769928\n",
      "Step: 5200, train/learning_rate: 6.738768661307404e-06\n",
      "Step: 5200, train/epoch: 8.652246475219727\n",
      "Step: 5210, train/loss: 0.0\n",
      "Step: 5210, train/grad_norm: 0.0007676962995901704\n",
      "Step: 5210, train/learning_rate: 6.655573997704778e-06\n",
      "Step: 5210, train/epoch: 8.668885231018066\n",
      "Step: 5220, train/loss: 0.004699999932199717\n",
      "Step: 5220, train/grad_norm: 0.0786362811923027\n",
      "Step: 5220, train/learning_rate: 6.572379334102152e-06\n",
      "Step: 5220, train/epoch: 8.685523986816406\n",
      "Step: 5230, train/loss: 0.013000000268220901\n",
      "Step: 5230, train/grad_norm: 0.2826753258705139\n",
      "Step: 5230, train/learning_rate: 6.489184670499526e-06\n",
      "Step: 5230, train/epoch: 8.702162742614746\n",
      "Step: 5240, train/loss: 0.00019999999494757503\n",
      "Step: 5240, train/grad_norm: 0.0007739370339550078\n",
      "Step: 5240, train/learning_rate: 6.4059900068969e-06\n",
      "Step: 5240, train/epoch: 8.718802452087402\n",
      "Step: 5250, train/loss: 0.0\n",
      "Step: 5250, train/grad_norm: 0.00046046709758229554\n",
      "Step: 5250, train/learning_rate: 6.322795343294274e-06\n",
      "Step: 5250, train/epoch: 8.735441207885742\n",
      "Step: 5260, train/loss: 0.002400000113993883\n",
      "Step: 5260, train/grad_norm: 0.002531785285100341\n",
      "Step: 5260, train/learning_rate: 6.239600679691648e-06\n",
      "Step: 5260, train/epoch: 8.752079963684082\n",
      "Step: 5270, train/loss: 0.00019999999494757503\n",
      "Step: 5270, train/grad_norm: 0.0006896028644405305\n",
      "Step: 5270, train/learning_rate: 6.156406016089022e-06\n",
      "Step: 5270, train/epoch: 8.768718719482422\n",
      "Step: 5280, train/loss: 0.0\n",
      "Step: 5280, train/grad_norm: 0.00038863709778524935\n",
      "Step: 5280, train/learning_rate: 6.073211352486396e-06\n",
      "Step: 5280, train/epoch: 8.785357475280762\n",
      "Step: 5290, train/loss: 9.999999747378752e-05\n",
      "Step: 5290, train/grad_norm: 0.0003710906021296978\n",
      "Step: 5290, train/learning_rate: 5.99001668888377e-06\n",
      "Step: 5290, train/epoch: 8.801996231079102\n",
      "Step: 5300, train/loss: 0.0\n",
      "Step: 5300, train/grad_norm: 0.0035801909398287535\n",
      "Step: 5300, train/learning_rate: 5.906822025281144e-06\n",
      "Step: 5300, train/epoch: 8.818635940551758\n",
      "Step: 5310, train/loss: 9.999999747378752e-05\n",
      "Step: 5310, train/grad_norm: 0.003037575865164399\n",
      "Step: 5310, train/learning_rate: 5.823627361678518e-06\n",
      "Step: 5310, train/epoch: 8.835274696350098\n",
      "Step: 5320, train/loss: 9.999999747378752e-05\n",
      "Step: 5320, train/grad_norm: 0.001992327393963933\n",
      "Step: 5320, train/learning_rate: 5.740432698075892e-06\n",
      "Step: 5320, train/epoch: 8.851913452148438\n",
      "Step: 5330, train/loss: 0.0\n",
      "Step: 5330, train/grad_norm: 0.0003853419329971075\n",
      "Step: 5330, train/learning_rate: 5.6572380344732665e-06\n",
      "Step: 5330, train/epoch: 8.868552207946777\n",
      "Step: 5340, train/loss: 0.0\n",
      "Step: 5340, train/grad_norm: 0.0007368588703684509\n",
      "Step: 5340, train/learning_rate: 5.5740433708706405e-06\n",
      "Step: 5340, train/epoch: 8.885190963745117\n",
      "Step: 5350, train/loss: 0.0\n",
      "Step: 5350, train/grad_norm: 0.008331201039254665\n",
      "Step: 5350, train/learning_rate: 5.4908487072680146e-06\n",
      "Step: 5350, train/epoch: 8.901830673217773\n",
      "Step: 5360, train/loss: 0.0142000000923872\n",
      "Step: 5360, train/grad_norm: 22.035802841186523\n",
      "Step: 5360, train/learning_rate: 5.407654043665389e-06\n",
      "Step: 5360, train/epoch: 8.918469429016113\n",
      "Step: 5370, train/loss: 9.999999747378752e-05\n",
      "Step: 5370, train/grad_norm: 0.0027819566894322634\n",
      "Step: 5370, train/learning_rate: 5.324459380062763e-06\n",
      "Step: 5370, train/epoch: 8.935108184814453\n",
      "Step: 5380, train/loss: 0.0\n",
      "Step: 5380, train/grad_norm: 0.0021369706373661757\n",
      "Step: 5380, train/learning_rate: 5.241264716460137e-06\n",
      "Step: 5380, train/epoch: 8.951746940612793\n",
      "Step: 5390, train/loss: 9.999999747378752e-05\n",
      "Step: 5390, train/grad_norm: 0.0007932715816423297\n",
      "Step: 5390, train/learning_rate: 5.158070052857511e-06\n",
      "Step: 5390, train/epoch: 8.968385696411133\n",
      "Step: 5400, train/loss: 0.0006000000284984708\n",
      "Step: 5400, train/grad_norm: 0.0005010966560803354\n",
      "Step: 5400, train/learning_rate: 5.074875389254885e-06\n",
      "Step: 5400, train/epoch: 8.985025405883789\n",
      "Step: 5409, eval/loss: 0.7807104587554932\n",
      "Step: 5409, eval/accuracy: 0.8961544036865234\n",
      "Step: 5409, eval/f1: 0.887488842010498\n",
      "Step: 5409, eval/runtime: 81.05960083007812\n",
      "Step: 5409, eval/samples_per_second: 88.86100006103516\n",
      "Step: 5409, eval/steps_per_second: 1.590999960899353\n",
      "Step: 5409, train/epoch: 9.0\n",
      "Step: 5410, train/loss: 0.009999999776482582\n",
      "Step: 5410, train/grad_norm: 0.0012826987076550722\n",
      "Step: 5410, train/learning_rate: 4.991680725652259e-06\n",
      "Step: 5410, train/epoch: 9.001664161682129\n",
      "Step: 5420, train/loss: 0.0026000000070780516\n",
      "Step: 5420, train/grad_norm: 0.000388908403692767\n",
      "Step: 5420, train/learning_rate: 4.908486062049633e-06\n",
      "Step: 5420, train/epoch: 9.018302917480469\n",
      "Step: 5430, train/loss: 0.0\n",
      "Step: 5430, train/grad_norm: 0.007046644575893879\n",
      "Step: 5430, train/learning_rate: 4.825291398447007e-06\n",
      "Step: 5430, train/epoch: 9.034941673278809\n",
      "Step: 5440, train/loss: 0.0\n",
      "Step: 5440, train/grad_norm: 0.0005747099057771266\n",
      "Step: 5440, train/learning_rate: 4.74209628009703e-06\n",
      "Step: 5440, train/epoch: 9.051580429077148\n",
      "Step: 5450, train/loss: 0.0\n",
      "Step: 5450, train/grad_norm: 0.0004818399902433157\n",
      "Step: 5450, train/learning_rate: 4.658901616494404e-06\n",
      "Step: 5450, train/epoch: 9.068219184875488\n",
      "Step: 5460, train/loss: 0.0\n",
      "Step: 5460, train/grad_norm: 0.0006722650723531842\n",
      "Step: 5460, train/learning_rate: 4.575706952891778e-06\n",
      "Step: 5460, train/epoch: 9.084858894348145\n",
      "Step: 5470, train/loss: 9.999999747378752e-05\n",
      "Step: 5470, train/grad_norm: 0.008873350918293\n",
      "Step: 5470, train/learning_rate: 4.492512289289152e-06\n",
      "Step: 5470, train/epoch: 9.101497650146484\n",
      "Step: 5480, train/loss: 0.00019999999494757503\n",
      "Step: 5480, train/grad_norm: 0.006439928896725178\n",
      "Step: 5480, train/learning_rate: 4.409317625686526e-06\n",
      "Step: 5480, train/epoch: 9.118136405944824\n",
      "Step: 5490, train/loss: 0.0\n",
      "Step: 5490, train/grad_norm: 0.017334740608930588\n",
      "Step: 5490, train/learning_rate: 4.3261229620839e-06\n",
      "Step: 5490, train/epoch: 9.134775161743164\n",
      "Step: 5500, train/loss: 0.00019999999494757503\n",
      "Step: 5500, train/grad_norm: 0.00036593180266208947\n",
      "Step: 5500, train/learning_rate: 4.242928298481274e-06\n",
      "Step: 5500, train/epoch: 9.151413917541504\n",
      "Step: 5510, train/loss: 0.0\n",
      "Step: 5510, train/grad_norm: 0.0030986221972852945\n",
      "Step: 5510, train/learning_rate: 4.1597336348786484e-06\n",
      "Step: 5510, train/epoch: 9.16805362701416\n",
      "Step: 5520, train/loss: 0.0\n",
      "Step: 5520, train/grad_norm: 0.0010162257822230458\n",
      "Step: 5520, train/learning_rate: 4.0765389712760225e-06\n",
      "Step: 5520, train/epoch: 9.1846923828125\n",
      "Step: 5530, train/loss: 0.0\n",
      "Step: 5530, train/grad_norm: 0.00045302064972929657\n",
      "Step: 5530, train/learning_rate: 3.9933443076733965e-06\n",
      "Step: 5530, train/epoch: 9.20133113861084\n",
      "Step: 5540, train/loss: 0.0\n",
      "Step: 5540, train/grad_norm: 0.0015629427507519722\n",
      "Step: 5540, train/learning_rate: 3.910149644070771e-06\n",
      "Step: 5540, train/epoch: 9.21796989440918\n",
      "Step: 5550, train/loss: 0.0\n",
      "Step: 5550, train/grad_norm: 0.0012072036042809486\n",
      "Step: 5550, train/learning_rate: 3.826954980468145e-06\n",
      "Step: 5550, train/epoch: 9.23460865020752\n",
      "Step: 5560, train/loss: 9.999999747378752e-05\n",
      "Step: 5560, train/grad_norm: 0.033601488918066025\n",
      "Step: 5560, train/learning_rate: 3.7437603168655187e-06\n",
      "Step: 5560, train/epoch: 9.251248359680176\n",
      "Step: 5570, train/loss: 0.00019999999494757503\n",
      "Step: 5570, train/grad_norm: 0.0019083821680396795\n",
      "Step: 5570, train/learning_rate: 3.6605656532628927e-06\n",
      "Step: 5570, train/epoch: 9.267887115478516\n",
      "Step: 5580, train/loss: 0.0\n",
      "Step: 5580, train/grad_norm: 0.005200145300477743\n",
      "Step: 5580, train/learning_rate: 3.5773709896602668e-06\n",
      "Step: 5580, train/epoch: 9.284525871276855\n",
      "Step: 5590, train/loss: 0.0\n",
      "Step: 5590, train/grad_norm: 0.00027167564257979393\n",
      "Step: 5590, train/learning_rate: 3.494176326057641e-06\n",
      "Step: 5590, train/epoch: 9.301164627075195\n",
      "Step: 5600, train/loss: 0.0\n",
      "Step: 5600, train/grad_norm: 0.0002708563406486064\n",
      "Step: 5600, train/learning_rate: 3.410981662455015e-06\n",
      "Step: 5600, train/epoch: 9.317803382873535\n",
      "Step: 5610, train/loss: 0.0\n",
      "Step: 5610, train/grad_norm: 0.0035021936055272818\n",
      "Step: 5610, train/learning_rate: 3.327786998852389e-06\n",
      "Step: 5610, train/epoch: 9.334442138671875\n",
      "Step: 5620, train/loss: 0.0\n",
      "Step: 5620, train/grad_norm: 0.001028434606269002\n",
      "Step: 5620, train/learning_rate: 3.244592335249763e-06\n",
      "Step: 5620, train/epoch: 9.351081848144531\n",
      "Step: 5630, train/loss: 0.00570000009611249\n",
      "Step: 5630, train/grad_norm: 0.0021857344545423985\n",
      "Step: 5630, train/learning_rate: 3.161397671647137e-06\n",
      "Step: 5630, train/epoch: 9.367720603942871\n",
      "Step: 5640, train/loss: 0.0\n",
      "Step: 5640, train/grad_norm: 0.07322469353675842\n",
      "Step: 5640, train/learning_rate: 3.078203008044511e-06\n",
      "Step: 5640, train/epoch: 9.384359359741211\n",
      "Step: 5650, train/loss: 0.00039999998989515007\n",
      "Step: 5650, train/grad_norm: 0.00048555355169810355\n",
      "Step: 5650, train/learning_rate: 2.995008344441885e-06\n",
      "Step: 5650, train/epoch: 9.40099811553955\n",
      "Step: 5660, train/loss: 0.008200000040233135\n",
      "Step: 5660, train/grad_norm: 0.000346620159689337\n",
      "Step: 5660, train/learning_rate: 2.911813680839259e-06\n",
      "Step: 5660, train/epoch: 9.41763687133789\n",
      "Step: 5670, train/loss: 0.0\n",
      "Step: 5670, train/grad_norm: 0.002893709344789386\n",
      "Step: 5670, train/learning_rate: 2.8286190172366332e-06\n",
      "Step: 5670, train/epoch: 9.434276580810547\n",
      "Step: 5680, train/loss: 9.999999747378752e-05\n",
      "Step: 5680, train/grad_norm: 0.00027123678592033684\n",
      "Step: 5680, train/learning_rate: 2.7454243536340073e-06\n",
      "Step: 5680, train/epoch: 9.450915336608887\n",
      "Step: 5690, train/loss: 0.0\n",
      "Step: 5690, train/grad_norm: 0.0031994632445275784\n",
      "Step: 5690, train/learning_rate: 2.6622296900313813e-06\n",
      "Step: 5690, train/epoch: 9.467554092407227\n",
      "Step: 5700, train/loss: 9.999999747378752e-05\n",
      "Step: 5700, train/grad_norm: 0.00029697243007831275\n",
      "Step: 5700, train/learning_rate: 2.5790350264287554e-06\n",
      "Step: 5700, train/epoch: 9.484192848205566\n",
      "Step: 5710, train/loss: 0.0\n",
      "Step: 5710, train/grad_norm: 0.0003000526630785316\n",
      "Step: 5710, train/learning_rate: 2.4958403628261294e-06\n",
      "Step: 5710, train/epoch: 9.500831604003906\n",
      "Step: 5720, train/loss: 9.999999747378752e-05\n",
      "Step: 5720, train/grad_norm: 0.04446140304207802\n",
      "Step: 5720, train/learning_rate: 2.4126456992235035e-06\n",
      "Step: 5720, train/epoch: 9.517471313476562\n",
      "Step: 5730, train/loss: 0.0\n",
      "Step: 5730, train/grad_norm: 0.0518430694937706\n",
      "Step: 5730, train/learning_rate: 2.329450808247202e-06\n",
      "Step: 5730, train/epoch: 9.534110069274902\n",
      "Step: 5740, train/loss: 0.0\n",
      "Step: 5740, train/grad_norm: 0.0009460836881771684\n",
      "Step: 5740, train/learning_rate: 2.246256144644576e-06\n",
      "Step: 5740, train/epoch: 9.550748825073242\n",
      "Step: 5750, train/loss: 0.0\n",
      "Step: 5750, train/grad_norm: 0.00030385248828679323\n",
      "Step: 5750, train/learning_rate: 2.16306148104195e-06\n",
      "Step: 5750, train/epoch: 9.567387580871582\n",
      "Step: 5760, train/loss: 0.011900000274181366\n",
      "Step: 5760, train/grad_norm: 0.0008704187930561602\n",
      "Step: 5760, train/learning_rate: 2.0798668174393242e-06\n",
      "Step: 5760, train/epoch: 9.584026336669922\n",
      "Step: 5770, train/loss: 0.0\n",
      "Step: 5770, train/grad_norm: 0.00028282107086852193\n",
      "Step: 5770, train/learning_rate: 1.9966721538366983e-06\n",
      "Step: 5770, train/epoch: 9.600665092468262\n",
      "Step: 5780, train/loss: 0.0\n",
      "Step: 5780, train/grad_norm: 0.0003305691061541438\n",
      "Step: 5780, train/learning_rate: 1.9134774902340723e-06\n",
      "Step: 5780, train/epoch: 9.617304801940918\n",
      "Step: 5790, train/loss: 0.0\n",
      "Step: 5790, train/grad_norm: 0.0006107558729127049\n",
      "Step: 5790, train/learning_rate: 1.8302828266314464e-06\n",
      "Step: 5790, train/epoch: 9.633943557739258\n",
      "Step: 5800, train/loss: 0.0\n",
      "Step: 5800, train/grad_norm: 0.00045466225128620863\n",
      "Step: 5800, train/learning_rate: 1.7470881630288204e-06\n",
      "Step: 5800, train/epoch: 9.650582313537598\n",
      "Step: 5810, train/loss: 0.0031999999191612005\n",
      "Step: 5810, train/grad_norm: 34.072792053222656\n",
      "Step: 5810, train/learning_rate: 1.6638934994261945e-06\n",
      "Step: 5810, train/epoch: 9.667221069335938\n",
      "Step: 5820, train/loss: 9.999999747378752e-05\n",
      "Step: 5820, train/grad_norm: 0.0003403422888368368\n",
      "Step: 5820, train/learning_rate: 1.5806988358235685e-06\n",
      "Step: 5820, train/epoch: 9.683859825134277\n",
      "Step: 5830, train/loss: 0.0\n",
      "Step: 5830, train/grad_norm: 0.000840534339658916\n",
      "Step: 5830, train/learning_rate: 1.4975041722209426e-06\n",
      "Step: 5830, train/epoch: 9.700499534606934\n",
      "Step: 5840, train/loss: 0.0\n",
      "Step: 5840, train/grad_norm: 0.00028071997803635895\n",
      "Step: 5840, train/learning_rate: 1.4143095086183166e-06\n",
      "Step: 5840, train/epoch: 9.717138290405273\n",
      "Step: 5850, train/loss: 0.0044999998062849045\n",
      "Step: 5850, train/grad_norm: 63.262306213378906\n",
      "Step: 5850, train/learning_rate: 1.3311148450156907e-06\n",
      "Step: 5850, train/epoch: 9.733777046203613\n",
      "Step: 5860, train/loss: 0.0\n",
      "Step: 5860, train/grad_norm: 0.0075220451690256596\n",
      "Step: 5860, train/learning_rate: 1.2479201814130647e-06\n",
      "Step: 5860, train/epoch: 9.750415802001953\n",
      "Step: 5870, train/loss: 0.0008999999845400453\n",
      "Step: 5870, train/grad_norm: 0.00029876743792556226\n",
      "Step: 5870, train/learning_rate: 1.164725404123601e-06\n",
      "Step: 5870, train/epoch: 9.767054557800293\n",
      "Step: 5880, train/loss: 9.999999747378752e-05\n",
      "Step: 5880, train/grad_norm: 2.5793659687042236\n",
      "Step: 5880, train/learning_rate: 1.081530740520975e-06\n",
      "Step: 5880, train/epoch: 9.78369426727295\n",
      "Step: 5890, train/loss: 0.0003000000142492354\n",
      "Step: 5890, train/grad_norm: 0.05648842826485634\n",
      "Step: 5890, train/learning_rate: 9.983360769183491e-07\n",
      "Step: 5890, train/epoch: 9.800333023071289\n",
      "Step: 5900, train/loss: 0.0\n",
      "Step: 5900, train/grad_norm: 0.010483689606189728\n",
      "Step: 5900, train/learning_rate: 9.151414133157232e-07\n",
      "Step: 5900, train/epoch: 9.816971778869629\n",
      "Step: 5910, train/loss: 0.0\n",
      "Step: 5910, train/grad_norm: 0.003737509483471513\n",
      "Step: 5910, train/learning_rate: 8.319467497130972e-07\n",
      "Step: 5910, train/epoch: 9.833610534667969\n",
      "Step: 5920, train/loss: 9.999999747378752e-05\n",
      "Step: 5920, train/grad_norm: 0.591089129447937\n",
      "Step: 5920, train/learning_rate: 7.487520861104713e-07\n",
      "Step: 5920, train/epoch: 9.850249290466309\n",
      "Step: 5930, train/loss: 0.0\n",
      "Step: 5930, train/grad_norm: 0.0006332180928438902\n",
      "Step: 5930, train/learning_rate: 6.655574225078453e-07\n",
      "Step: 5930, train/epoch: 9.866888046264648\n",
      "Step: 5940, train/loss: 0.0\n",
      "Step: 5940, train/grad_norm: 0.0022520171478390694\n",
      "Step: 5940, train/learning_rate: 5.823627020618005e-07\n",
      "Step: 5940, train/epoch: 9.883527755737305\n",
      "Step: 5950, train/loss: 0.0\n",
      "Step: 5950, train/grad_norm: 0.0016951907891780138\n",
      "Step: 5950, train/learning_rate: 4.991680384591746e-07\n",
      "Step: 5950, train/epoch: 9.900166511535645\n",
      "Step: 5960, train/loss: 0.0\n",
      "Step: 5960, train/grad_norm: 0.0010415074648335576\n",
      "Step: 5960, train/learning_rate: 4.159733748565486e-07\n",
      "Step: 5960, train/epoch: 9.916805267333984\n",
      "Step: 5970, train/loss: 0.00019999999494757503\n",
      "Step: 5970, train/grad_norm: 0.4321092367172241\n",
      "Step: 5970, train/learning_rate: 3.3277871125392267e-07\n",
      "Step: 5970, train/epoch: 9.933444023132324\n",
      "Step: 5980, train/loss: 9.999999747378752e-05\n",
      "Step: 5980, train/grad_norm: 0.0003310820902697742\n",
      "Step: 5980, train/learning_rate: 2.495840192295873e-07\n",
      "Step: 5980, train/epoch: 9.950082778930664\n",
      "Step: 5990, train/loss: 0.0\n",
      "Step: 5990, train/grad_norm: 0.018530724570155144\n",
      "Step: 5990, train/learning_rate: 1.6638935562696133e-07\n",
      "Step: 5990, train/epoch: 9.96672248840332\n",
      "Step: 6000, train/loss: 0.0\n",
      "Step: 6000, train/grad_norm: 0.0003715738421306014\n",
      "Step: 6000, train/learning_rate: 8.319467781348067e-08\n",
      "Step: 6000, train/epoch: 9.98336124420166\n",
      "Step: 6010, train/loss: 0.0\n",
      "Step: 6010, train/grad_norm: 0.00018000579439103603\n",
      "Step: 6010, train/learning_rate: 0.0\n",
      "Step: 6010, train/epoch: 10.0\n",
      "Step: 6010, eval/loss: 0.7534269690513611\n",
      "Step: 6010, eval/accuracy: 0.9029570817947388\n",
      "Step: 6010, eval/f1: 0.896660566329956\n",
      "Step: 6010, eval/runtime: 81.02619934082031\n",
      "Step: 6010, eval/samples_per_second: 88.89700317382812\n",
      "Step: 6010, eval/steps_per_second: 1.5920000076293945\n",
      "Step: 6010, train/epoch: 10.0\n",
      "Step: 6010, train/train_runtime: 9391.890625\n",
      "Step: 6010, train/train_samples_per_second: 35.7869987487793\n",
      "Step: 6010, train/train_steps_per_second: 0.6399999856948853\n",
      "Step: 6010, train/total_flos: 3.132314495012045e+17\n",
      "Step: 6010, train/train_loss: 0.10084310173988342\n",
      "Step: 6010, train/epoch: 10.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.summary.summary_iterator import summary_iterator\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Construct the logs directory path\n",
    "logs_directory = os.path.join('./', project_name, 'logs')\n",
    "file_pattern = 'events.out.tfevents.*'\n",
    "\n",
    "# Retrieve all event files matching the pattern\n",
    "event_files = glob.glob(os.path.join(logs_directory, file_pattern))\n",
    "\n",
    "# Function to print out TensorBoard event logs\n",
    "def print_events_from_file(event_files):\n",
    "    for event_file in event_files:\n",
    "        print(f\"Reading events from file: {event_file}\")\n",
    "        try:\n",
    "            for e in summary_iterator(event_file):\n",
    "                for v in e.summary.value:\n",
    "                    if v.HasField('simple_value'):\n",
    "                        print(f\"Step: {e.step}, {v.tag}: {v.simple_value}\")\n",
    "        except Exception as e:  # Just in case the event file is not readable\n",
    "            print(f\"Failed to read {event_file}: {e}\")\n",
    "\n",
    "print_events_from_file(event_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f8e0ccc-f7c4-4a02-a1e6-5e3012717125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Step Train Loss Eval Loss  Accuracy        F1\n",
      "0   601   0.320300  0.306954  0.869638  0.862995\n",
      "1  1202   0.215000  0.359650  0.858670  0.840971\n",
      "2  1803   0.091400  0.342167  0.889074  0.880647\n",
      "3  2404   0.030100  0.536876  0.881299  0.876329\n",
      "4  3005   0.025700  0.963678  0.838817  0.812513\n",
      "5  3606   0.023200  0.761493  0.887130  0.877392\n",
      "6  4207   0.006200  0.661285  0.894072  0.886275\n",
      "7  4808   0.015400  0.820575  0.885048  0.873770\n",
      "8  5409   0.000600  0.780710  0.896154  0.887489\n",
      "9  6010   0.000000  0.753427  0.902957  0.896661\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.python.summary.summary_iterator import summary_iterator\n",
    "\n",
    "logs_directory = os.path.join('./', project_name, 'logs')\n",
    "file_pattern = 'events.out.tfevents.*'\n",
    "\n",
    "event_files = glob.glob(os.path.join(logs_directory, file_pattern))\n",
    "\n",
    "def extract_metrics(event_files):\n",
    "    data = []\n",
    "    last_train_loss = None\n",
    "\n",
    "    for event_file in event_files:\n",
    "        for e in summary_iterator(event_file):\n",
    "            for v in e.summary.value:\n",
    "                if v.HasField('simple_value'):\n",
    "                    step = e.step\n",
    "                    metric_name = v.tag.split('/')[-1]\n",
    "                    metric_value = v.simple_value\n",
    "\n",
    "                    formatted_value = f\"{metric_value:.6f}\"\n",
    "\n",
    "                    if 'train/loss' in v.tag:\n",
    "                        last_train_loss = formatted_value\n",
    "\n",
    "                    if 'eval' in v.tag:\n",
    "                        entry = next((item for item in data if item['Step'] == step), None)\n",
    "                        if not entry:\n",
    "                            entry = {'Step': step, 'Train Loss': last_train_loss, 'Eval Loss': None, 'Accuracy': None, 'F1': None}\n",
    "                            data.append(entry)\n",
    "                        if 'loss' in v.tag:\n",
    "                            entry['Eval Loss'] = formatted_value\n",
    "                        elif 'accuracy' in v.tag:\n",
    "                            entry['Accuracy'] = formatted_value\n",
    "                        elif 'f1' in v.tag:\n",
    "                            entry['F1'] = formatted_value\n",
    "    return data\n",
    "\n",
    "metrics_data = extract_metrics(event_files)\n",
    "\n",
    "df = pd.DataFrame(metrics_data)\n",
    "df = df.sort_values(by='Step')\n",
    "\n",
    "file_path = \"../images/\"+model_name+\"_Checkpoint_Data.csv\"\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52633cb6-5847-4d51-86c5-c6153e72cc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Checkpoint Step: checkpoint-6010\n",
      "Step              6010\n",
      "Train Loss    0.000000\n",
      "Eval Loss     0.753427\n",
      "Accuracy      0.902957\n",
      "F1            0.896661\n",
      "Rank Sum           8.0\n",
      "Name: 9, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df.fillna({\n",
    "    'Eval Loss': float('inf'),\n",
    "    'Accuracy': 0,\n",
    "    'F1': 0\n",
    "}, inplace=True)\n",
    "\n",
    "df['Eval Loss'] = df['Eval Loss'].astype(float)\n",
    "df['Accuracy'] = df['Accuracy'].astype(float)\n",
    "df['F1'] = df['F1'].astype(float)\n",
    "\n",
    "df['Eval Loss Rank'] = df['Eval Loss'].rank(method='min', ascending=True)\n",
    "df['Accuracy Rank'] = df['Accuracy'].rank(method='min', ascending=False)\n",
    "df['F1 Rank'] = df['F1'].rank(method='min', ascending=False)\n",
    "\n",
    "df['Rank Sum'] = df['Eval Loss Rank'] + df['Accuracy Rank'] + df['F1 Rank']\n",
    "\n",
    "best_checkpoint = df.loc[df['Rank Sum'].idxmin()]\n",
    "\n",
    "checkpoint_folder_name = f\"checkpoint-{best_checkpoint['Step']}\"\n",
    "print(f\"Best Checkpoint Step: {checkpoint_folder_name}\")\n",
    "print(best_checkpoint[['Step', 'Train Loss', 'Eval Loss', 'Accuracy', 'F1', 'Rank Sum']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d4044b-4ffc-409e-ac1a-8b4c58e72499",
   "metadata": {},
   "source": [
    "### Run TensorBoard\n",
    "tensorboard --logdir=~/kuk/Praxis/praxis-Llama-2-7b-hf-small-finetune/logs --host=0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e5a88a-7620-4280-970b-ff171568aebb",
   "metadata": {},
   "source": [
    "### PAUSE SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57fc853d-d293-402d-87d0-e27c83915c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My flag to pause the script, set to True to pause\n",
    "pause_script = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53887034-7922-4ed5-bd9a-8c80d6596ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        print(\"Script Paused\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd8248c6-2c95-4d82-a7f0-2e0ac37ab320",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pause_script:\n",
    "    raise StopExecution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19be6e26-d888-4da0-b3f8-836d68ac2051",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b140136-4f0d-44e4-9955-d95a342f250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import pipeline\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "test_checkpoint_name = checkpoint_folder_name\n",
    "savedmodel = pipeline('text-classification', model=output_dir_path + \"/\" + test_checkpoint_name, device=accelerator.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e0c22fa-0da7-4a30-8d27-128cab0bdfc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['article', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 7203\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50f2ad84-10b4-4f10-bbea-25571bfcda78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf124a825ff4c49891be1a5992117f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "max_tokens = 512\n",
    "truncated_articles = []\n",
    "\n",
    "for article in tqdm(tokenized_test_ds['article']):\n",
    "    encoded_article = tokenizer.encode(\n",
    "        article,\n",
    "        max_length=max_tokens,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    truncated_article = tokenizer.decode(encoded_article, skip_special_tokens=True)\n",
    "    truncated_articles.append(truncated_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d687d739-aafa-4ae2-b938-e8465856746e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdfb9f4b1c7a437d8d30ca2225a493c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifying articles:   0%|          | 0/7203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "310fbe318e2743c08b90aaca412357e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Labeling articles:   0%|          | 0/7203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Create a dataset from the truncated articles\n",
    "data = {\"text\": truncated_articles}\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Define a function to make predictions\n",
    "def classify_batch(batch):\n",
    "    return savedmodel(batch[\"text\"])\n",
    "\n",
    "# Process the dataset with batching\n",
    "test_predictions = []\n",
    "for batch in tqdm(dataset.to_dict()[\"text\"], desc=\"Classifying articles\"):\n",
    "    predictions = classify_batch({\"text\": batch})\n",
    "    test_predictions.extend(predictions)\n",
    "\n",
    "test_predictions_labels = []\n",
    "for prediction in tqdm(test_predictions, desc=\"Labeling articles\"):\n",
    "    label = 0 if prediction['label'] == 'LABEL_0' else 1\n",
    "    test_predictions_labels.append(label)\n",
    "\n",
    "print(test_predictions_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26d6eead-d836-4e80-8ff6-b86b8599b245",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "true_articles = tokenized_test_ds['label']\n",
    "print(true_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10e33442-ac7f-49d3-a6ad-5eb4b892eedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(prediction):\n",
    "  return int(prediction['label'].split('_')[1])\n",
    "\n",
    "processed_predictions = [get_label(prediction) for prediction in test_predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f06a2c-5ded-4da0-8232-145383967c9d",
   "metadata": {},
   "source": [
    "<h1>Accuracy and F1</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb4d8350-a8d1-4853-a9f8-74ae9ea36bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9010134666111342\n"
     ]
    }
   ],
   "source": [
    "true_articles = tokenized_test_ds['label']\n",
    "accuracy = accuracy_score(true_articles, test_predictions_labels)\n",
    "print(\"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40780784-e77d-4b26-9dae-572137e591bf",
   "metadata": {},
   "source": [
    "<p>precision (how many of the items identified as positive are actually positive) and the recall (how many of the actual positives were identified correctly)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30465934-1b77-4a2c-8c52-34e239e37abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.8947738786563556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(true_articles, test_predictions_labels, average='macro')\n",
    "print(\"f1_score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcaac14-2b83-4c24-b83a-f6d0e73a2d2a",
   "metadata": {},
   "source": [
    "<h1>Confusion Matrix</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57e64afb-e3ee-4c79-8228-b2d79806229e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(tokenized_test_ds['label'], processed_predictions)\n",
    "\n",
    "labels = ['human', 'machine']\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c87380b0-553d-4642-9697-26c76c98dde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook praxis-bert-large-cased-small-finetune-v7.ipynb to html\n",
      "[NbConvertApp] WARNING | Alternative text is missing on 1 image(s).\n",
      "[NbConvertApp] Writing 575296 bytes to html/praxis-bert-large-cased-small-finetune-v7.html\n"
     ]
    }
   ],
   "source": [
    "file_name = f\"{project_name}.ipynb\"\n",
    "html_file_name = f\"{file_name.replace('.ipynb', '.html')}\"\n",
    "\n",
    "command = f\"jupyter nbconvert '{file_name}' --to html --output-dir './html' --output '{html_file_name}'\"\n",
    "get_ipython().system(command)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
