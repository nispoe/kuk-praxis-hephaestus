{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c2d2c9-4a4b-48ca-90a3-1ccd120ca08b",
   "metadata": {},
   "source": [
    "# Fine tuning using BERT Large Uncased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0f2da2-54e7-42b7-b44e-f6b7e8a48787",
   "metadata": {},
   "source": [
    "### Supress warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "362a364e-20f5-4d16-af44-62aada774f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d23075-e94c-4aa9-969f-ba66e1278d5e",
   "metadata": {},
   "source": [
    "### Inspect the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7cedfd8-5efe-47e5-84df-f1c00cd7cbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"bert-large-uncased\"\n",
    "checkpoint = \"google-bert/\"+model_name\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84b0595d-6453-48e4-982f-0608c272cb91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "BertForSequenceClassification                           --\n",
       "├─BertModel: 1-1                                        --\n",
       "│    └─BertEmbeddings: 2-1                              --\n",
       "│    │    └─Embedding: 3-1                              31,254,528\n",
       "│    │    └─Embedding: 3-2                              524,288\n",
       "│    │    └─Embedding: 3-3                              2,048\n",
       "│    │    └─LayerNorm: 3-4                              2,048\n",
       "│    │    └─Dropout: 3-5                                --\n",
       "│    └─BertEncoder: 2-2                                 --\n",
       "│    │    └─ModuleList: 3-6                             302,309,376\n",
       "│    └─BertPooler: 2-3                                  --\n",
       "│    │    └─Linear: 3-7                                 1,049,600\n",
       "│    │    └─Tanh: 3-8                                   --\n",
       "├─Dropout: 1-2                                          --\n",
       "├─Linear: 1-3                                           2,050\n",
       "================================================================================\n",
       "Total params: 335,143,938\n",
       "Trainable params: 335,143,938\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09a29b9b-fd31-4bb9-ada1-e7264f8a85a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601449e5-fe1f-4194-b061-4951fc8bb86f",
   "metadata": {},
   "source": [
    "### Load the news dataset from pickle file\n",
    "If any of the check_files don't exist then load the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5365d1de-318f-46e6-b91b-609f9fd8cbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one of the specified files already exists. Not loading new dataset.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "base_path = './data/'\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "file_name = 'news_small_dataset.pkl'\n",
    "file_path = base_path+file_name\n",
    "\n",
    "def pickle_dataset(dataset, file_path):\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(dataset, file)\n",
    "        print(f\"Dataset has been pickled to: {file_path}\")\n",
    "\n",
    "def load_pickle_dataset(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        dataset = pickle.load(file)\n",
    "        print(f\"Dataset has been loaded from: {file_path}\")\n",
    "    return dataset\n",
    "\n",
    "def check_files_exists(file_names):\n",
    "    for name in file_names:\n",
    "        file_path = os.path.join(base_path, name)\n",
    "        if os.path.exists(file_path):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# if these files exist we do not want to load the news_dataset.pkl to tokenize and make these files\n",
    "check_files = [model_name+'-small_tokenized_train_ds.pkl', model_name+'-small_tokenized_eval_ds.pkl', model_name+'-small_tokenized_test_ds.pkl']\n",
    "\n",
    "if check_files_exists(check_files):\n",
    "    print(\"At least one of the specified files already exists. Not loading new dataset.\")\n",
    "else:\n",
    "    news_split_ds = load_pickle_dataset(file_path)\n",
    "    print(news_split_ds)\n",
    "    total_rows = (news_split_ds['train'].num_rows +\n",
    "              news_split_ds['eval'].num_rows +\n",
    "              news_split_ds['test'].num_rows)\n",
    "    print(\"Total number of rows:\", total_rows)\n",
    "    print(\"Dataset loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "395adbb2-9498-49cc-baf1-f09e3fcfb39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_fn(news):\n",
    "  return tokenizer(news['article'], padding=True, truncation=True, return_tensors=\"pt\", max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6234e6-1e6b-4f41-b3a2-5bb0adfae5a4",
   "metadata": {},
   "source": [
    "### Tokenize train, evaluation, and test datasets\n",
    "If any of the check files exist then don't run tokenization and save some time.\n",
    "Else load the pickle files that already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0b27288-1ea3-4230-9f9d-aeab3cd76b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already exist, so load datasets\n",
      "Dataset has been loaded from: ./data/bert-large-uncased-small_tokenized_train_ds.pkl\n",
      "Dataset has been loaded from: ./data/bert-large-uncased-small_tokenized_eval_ds.pkl\n",
      "Dataset has been loaded from: ./data/bert-large-uncased-small_tokenized_test_ds.pkl\n"
     ]
    }
   ],
   "source": [
    "if not check_files_exists(check_files):\n",
    "    tokenized_train_ds = news_split_ds['train'].map(tokenize_fn, batched=True)\n",
    "    tokenized_eval_ds = news_split_ds['eval'].map(tokenize_fn, batched=True)\n",
    "    tokenized_test_ds = news_split_ds['test'].map(tokenize_fn, batched=True)\n",
    "\n",
    "    print(tokenized_train_ds.features)\n",
    "    print(tokenized_eval_ds.features)\n",
    "    print(tokenized_test_ds.features)\n",
    "    \n",
    "    pickle_dataset(tokenized_train_ds, base_path+model_name+'-small_tokenized_train_ds.pkl')\n",
    "    pickle_dataset(tokenized_eval_ds, base_path+model_name+'-small_tokenized_eval_ds.pkl')\n",
    "    pickle_dataset(tokenized_test_ds, base_path+model_name+'-small_tokenized_test_ds.pkl')\n",
    "else:\n",
    "    print(\"Files already exist, so load datasets\")\n",
    "    tokenized_train_ds = load_pickle_dataset(base_path+model_name+'-small_tokenized_train_ds.pkl')\n",
    "    tokenized_eval_ds = load_pickle_dataset(base_path+model_name+'-small_tokenized_eval_ds.pkl')\n",
    "    tokenized_test_ds = load_pickle_dataset(base_path+model_name+'-small_tokenized_test_ds.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828b2ac6-7634-40c9-9ed8-222634448ce1",
   "metadata": {},
   "source": [
    "### Look at the tokenized data\n",
    "Notice what the actual data looks like, and then the tokenized data which is a bunch of numbers, and then the attention mask at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ca4f524-e97e-4cee-b28a-74f204f669b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in training dataset: 33611\n",
      "Number of records in evaluation dataset: 7203\n",
      "Number of records in test dataset: 7203\n",
      "Total number of records: 48017\n"
     ]
    }
   ],
   "source": [
    "count_train_records = len(tokenized_train_ds)\n",
    "count_eval_records = len(tokenized_eval_ds)\n",
    "count_test_records = len(tokenized_test_ds)\n",
    "print(f\"Number of records in training dataset: {count_train_records}\")\n",
    "print(f\"Number of records in evaluation dataset: {count_eval_records}\")\n",
    "print(f\"Number of records in test dataset: {count_test_records}\")\n",
    "count_total_records = count_train_records + count_eval_records + count_test_records\n",
    "print(f\"Total number of records: {count_total_records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab4e094a-6fc0-4e11-8ab2-8e394b7a07a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'article': \"In a year where homicides, rapes and robberies increased slightly, New York City still saw serious crime drop 1.7 percent in 2015, continuing an overall decline that began in the 1990s, NYPD Commissioner William Bratton said Monday.\\nAt a news conference with Mayor Bill de Blasio, Bratton touted last year’s crime statistics, which he said, when combined with an even larger decline in 2014, put to rest the fear that substantial decreases couldn’t continue under the new administration at City Hall.\\n“While we have had some fluctuation, some increases in certain categories, the overall trend in all our crime categories continues to go down,” Bratton told reporters. “It was a very good year for us, 2015.\\nHomicides increased by 4.5 percent in 2015, rising to 350 from 333 in the prior year, which was the lowest since 1994, said Deputy Commissioner Dermot Shea. Rapes increased 6 percent and robberies rose 2 percent, said Shea, who is in charge of data collection and operations for the NYPD.\\nThe lower overall crime statistics came about due to what Shea called “targeted enforcement,” where cops make quality arrests even though the overall number of apprehensions was the lowest in the city since 2003.\\nTwo boroughs — Manhattan and the Bronx — actually saw serious crimes increase by 3 percent and 4 percent, respectively, Shea said. Manhattan’s increase was driven by more robberies, while the Bronx, although seeing an overall crime increase, had what he said was a “phenomenal” reduction in shootings. Citywide, shootings were down in 2015 about 3 percent, to 1,103 from 1,172 in 2014.\\nShea largely attributed the 2015 increase in rapes to victims coming forward with complaints about attacks from years past.\\nSign up to get the latest updates Get Newsday's Breaking News alerts in your inbox. By clicking Sign up, you agree to our privacy policy.\\n“Twenty percent of these rapes didn’t happen in 2015,” he said.\\nThe NYPD has seen an increase in rapes involving single women who, after a night of drinking, get into cabs of all kinds and are attacked, Shea said.\\n“They get driven, and passing out and waking up in a desolate area, and they get sexually attacked. This is something, really, that people need to be exceptionally aware of, and like any case in New York City, the buddy system works,” said Shea, referring to the need for people to travel in pairs when taking a cab at night.\\nBratton and police brass hope to build upon the continuing drop in overall crime by using technology such as ShotSpotter and a newly minted GPS system for police cars.\\nJessica Tisch, NYPD deputy commissioner for technology, said ShotSpotter, an acoustical system that detects gunfire, identified gunshots in 1,672 cases, mostly in Brooklyn. Of those alerts, 74 percent didn’t have any 911 calls from the public associated with them.\\nTisch said ShotSpotter helped police recover ballistic evidence in 19 percent of the gunfire alerts. In 22 percent of those cases, Tisch said, cops were able to make positive matches of bullets with those from guns used in earlier shootings.\\nTisch also highlighted a special GPS system being tried in about 5,000 patrol cars that allows the NYPD to see where its vehicles are and to track their movements over a 24-hour period, as well as gather information about the officers’ driving.\\n\", 'label': 0, 'input_ids': [101, 1999, 1037, 2095, 2073, 18268, 2015, 1010, 9040, 2015, 1998, 27307, 3111, 3445, 3621, 1010, 2047, 2259, 2103, 2145, 2387, 3809, 4126, 4530, 1015, 1012, 1021, 3867, 1999, 2325, 1010, 5719, 2019, 3452, 6689, 2008, 2211, 1999, 1996, 4134, 1010, 6396, 17299, 5849, 2520, 28557, 2669, 2056, 6928, 1012, 2012, 1037, 2739, 3034, 2007, 3664, 3021, 2139, 1038, 8523, 3695, 1010, 28557, 2669, 2000, 12926, 2197, 2095, 1521, 1055, 4126, 6747, 1010, 2029, 2002, 2056, 1010, 2043, 4117, 2007, 2019, 2130, 3469, 6689, 1999, 2297, 1010, 2404, 2000, 2717, 1996, 3571, 2008, 6937, 17913, 2481, 1521, 1056, 3613, 2104, 1996, 2047, 3447, 2012, 2103, 2534, 1012, 1523, 2096, 2057, 2031, 2018, 2070, 19857, 6593, 14505, 1010, 2070, 7457, 1999, 3056, 7236, 1010, 1996, 3452, 9874, 1999, 2035, 2256, 4126, 7236, 4247, 2000, 2175, 2091, 1010, 1524, 28557, 2669, 2409, 12060, 1012, 1523, 2009, 2001, 1037, 2200, 2204, 2095, 2005, 2149, 1010, 2325, 1012, 18268, 2015, 3445, 2011, 1018, 1012, 1019, 3867, 1999, 2325, 1010, 4803, 2000, 8698, 2013, 21211, 1999, 1996, 3188, 2095, 1010, 2029, 2001, 1996, 7290, 2144, 2807, 1010, 2056, 4112, 5849, 4315, 18938, 16994, 1012, 9040, 2015, 3445, 1020, 3867, 1998, 27307, 3111, 3123, 1016, 3867, 1010, 2056, 16994, 1010, 2040, 2003, 1999, 3715, 1997, 2951, 3074, 1998, 3136, 2005, 1996, 6396, 17299, 1012, 1996, 2896, 3452, 4126, 6747, 2234, 2055, 2349, 2000, 2054, 16994, 2170, 1523, 9416, 7285, 1010, 1524, 2073, 10558, 2191, 3737, 17615, 2130, 2295, 1996, 3452, 2193, 1997, 25809, 2015, 2001, 1996, 7290, 1999, 1996, 2103, 2144, 2494, 1012, 2048, 21413, 1517, 7128, 1998, 1996, 14487, 1517, 2941, 2387, 3809, 6997, 3623, 2011, 1017, 3867, 1998, 1018, 3867, 1010, 4414, 1010, 16994, 2056, 1012, 7128, 1521, 1055, 3623, 2001, 5533, 2011, 2062, 27307, 3111, 1010, 2096, 1996, 14487, 1010, 2348, 3773, 2019, 3452, 4126, 3623, 1010, 2018, 2054, 2002, 2056, 2001, 1037, 1523, 13352, 2140, 1524, 7312, 1999, 5008, 2015, 1012, 2103, 22517, 1010, 5008, 2015, 2020, 2091, 1999, 2325, 2055, 1017, 3867, 1010, 2000, 1015, 1010, 9800, 2013, 1015, 1010, 18253, 1999, 2297, 1012, 16994, 4321, 7108, 1996, 2325, 3623, 1999, 9040, 2015, 2000, 5694, 2746, 2830, 2007, 10821, 2055, 4491, 2013, 2086, 2627, 1012, 3696, 2039, 2000, 2131, 1996, 6745, 14409, 2131, 2739, 10259, 1005, 1055, 4911, 2739, 9499, 2015, 1999, 2115, 1999, 8758, 1012, 2011, 22042, 3696, 2039, 1010, 2017, 5993, 2000, 2256, 9394, 3343, 1012, 1523, 3174, 3867, 1997, 2122, 9040, 2015, 2134, 1521, 1056, 4148, 1999, 2325, 1010, 1524, 2002, 2056, 1012, 1996, 6396, 17299, 2038, 2464, 2019, 3623, 1999, 9040, 2015, 5994, 2309, 2308, 2040, 1010, 2044, 1037, 2305, 1997, 5948, 1010, 2131, 2046, 9298, 2015, 1997, 2035, 7957, 1998, 2024, 4457, 1010, 16994, 2056, 1012, 1523, 2027, 2131, 5533, 1010, 1998, 4458, 2041, 1998, 12447, 2039, 1999, 1037, 4078, 19425, 2181, 1010, 1998, 2027, 2131, 12581, 4457, 1012, 2023, 2003, 2242, 1010, 2428, 1010, 2008, 2111, 2342, 2000, 2022, 17077, 5204, 1997, 1010, 1998, 2066, 2151, 2553, 1999, 2047, 2259, 2103, 1010, 1996, 8937, 2291, 2573, 1010, 1524, 2056, 16994, 1010, 7727, 2000, 1996, 2342, 2005, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "first_record = tokenized_train_ds[0]\n",
    "print(first_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a991f2e-0532-4424-90d4-bda2758ec289",
   "metadata": {},
   "source": [
    "### Turn on accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17623651-b9c0-4dbe-bbd1-07bb51eda8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef357cf6-2074-4ad9-892d-f2ce93221e16",
   "metadata": {},
   "source": [
    "### Look at hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9848d5f-f539-456c-a110-72ac08530f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 7\n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "GPU 1: NVIDIA GeForce RTX 4090\n",
      "GPU 2: NVIDIA GeForce RTX 4090\n",
      "GPU 3: NVIDIA GeForce RTX 3090 Ti\n",
      "GPU 4: NVIDIA GeForce RTX 3090 Ti\n",
      "GPU 5: NVIDIA GeForce RTX 3090\n",
      "GPU 6: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b847246a-83e8-4982-a6c8-ba549f8e38a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 22 09:55:59 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off |   00000000:01:00.0 Off |                  N/A |\n",
      "| 31%   34C    P8             36W /  420W |      13MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        Off |   00000000:02:00.0 Off |                  Off |\n",
      "|  0%   38C    P8             21W /  450W |     112MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 3090 Ti     Off |   00000000:2A:00.0 Off |                  Off |\n",
      "| 34%   39C    P8             25W /  450W |      13MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 3090        Off |   00000000:41:00.0 Off |                  N/A |\n",
      "| 32%   34C    P8             28W /  420W |      13MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA GeForce RTX 4090        Off |   00000000:42:00.0 Off |                  Off |\n",
      "|  0%   42C    P8             21W /  450W |      14MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA GeForce RTX 4090        Off |   00000000:61:00.0 Off |                  Off |\n",
      "|  0%   40C    P8             11W /  450W |      14MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA GeForce RTX 3090 Ti     Off |   00000000:62:00.0 Off |                  Off |\n",
      "| 30%   35C    P8             25W /  450W |      13MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   1026070      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    1   N/A  N/A   1026070      G   /usr/lib/xorg/Xorg                             86MiB |\n",
      "|    1   N/A  N/A   1026178      G   /usr/bin/gnome-shell                           13MiB |\n",
      "|    2   N/A  N/A   1026070      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    3   N/A  N/A   1026070      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    4   N/A  N/A   1026070      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    5   N/A  N/A   1026070      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    6   N/A  N/A   1026070      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fabb987-30a6-47bf-8048-b76477a8b070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(logits_and_labels):\n",
    "    logits, labels = logits_and_labels\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='macro')\n",
    "    return {'accuracy': acc, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69b144bb-2c85-429c-8643-3d98a423613d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./praxis-bert-large-uncased-small-finetune'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_name = \"praxis-\"+model_name+\"-small-finetune\"\n",
    "output_dir_path = \"./\" + project_name\n",
    "output_dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4b7cb69-8dbe-457e-9449-1e81323374ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a71a13f-2748-4216-b0aa-8200d7fc45d7",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "306b4000-44a5-4b1c-956e-8f86ee3f280d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 09:56:00.328107: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-22 09:56:00.864924: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnispoe\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nispoe/kuk/Praxis/wandb/run-20240522_095602-0pgmtnq8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nispoe/huggingface/runs/0pgmtnq8' target=\"_blank\">likely-donkey-296</a></strong> to <a href='https://wandb.ai/nispoe/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nispoe/huggingface' target=\"_blank\">https://wandb.ai/nispoe/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nispoe/huggingface/runs/0pgmtnq8' target=\"_blank\">https://wandb.ai/nispoe/huggingface/runs/0pgmtnq8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6010' max='6010' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6010/6010 2:37:01, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.393100</td>\n",
       "      <td>0.360599</td>\n",
       "      <td>0.843815</td>\n",
       "      <td>0.832579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.289400</td>\n",
       "      <td>0.298993</td>\n",
       "      <td>0.874636</td>\n",
       "      <td>0.864003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.133400</td>\n",
       "      <td>0.354863</td>\n",
       "      <td>0.871859</td>\n",
       "      <td>0.868087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.097600</td>\n",
       "      <td>0.389370</td>\n",
       "      <td>0.879495</td>\n",
       "      <td>0.875366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.022500</td>\n",
       "      <td>0.572214</td>\n",
       "      <td>0.880050</td>\n",
       "      <td>0.873018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.024900</td>\n",
       "      <td>0.563661</td>\n",
       "      <td>0.889768</td>\n",
       "      <td>0.883994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.055900</td>\n",
       "      <td>0.674912</td>\n",
       "      <td>0.890462</td>\n",
       "      <td>0.883184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.788489</td>\n",
       "      <td>0.890462</td>\n",
       "      <td>0.885028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.017400</td>\n",
       "      <td>0.831780</td>\n",
       "      <td>0.895183</td>\n",
       "      <td>0.890066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.814673</td>\n",
       "      <td>0.896016</td>\n",
       "      <td>0.889461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6010, training_loss=0.12112291483948481, metrics={'train_runtime': 9428.0936, 'train_samples_per_second': 35.65, 'train_steps_per_second': 0.637, 'total_flos': 3.132314505281741e+17, 'train_loss': 0.12112291483948481, 'epoch': 10.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "transformers.set_seed(777)\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_ds,\n",
    "    eval_dataset=tokenized_eval_ds,\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir_path,\n",
    "        num_train_epochs=10,\n",
    "        logging_steps=10,\n",
    "        logging_dir=output_dir_path+\"/logs\",\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        do_eval=True,\n",
    "    ),\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=None),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model = accelerator.prepare_model(model)\n",
    "trainer.train(resume_from_checkpoint=False)  # Turn to True if power goes out..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9247c1-3c27-4cb0-b4d6-602dcb02488e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### Determine best checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f00a0d4e-adfc-4877-af62-4d84992590dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 44\n",
      "drwxr-xr-x 2 nispoe nispoe 4096 May 22 09:56 logs\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 22 10:11 checkpoint-601\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 22 10:27 checkpoint-1202\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 22 10:43 checkpoint-1803\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 22 10:58 checkpoint-2404\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 22 11:14 checkpoint-3005\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 22 11:30 checkpoint-3606\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 22 11:46 checkpoint-4207\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 22 12:01 checkpoint-4808\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 22 12:17 checkpoint-5409\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 22 12:33 checkpoint-6010\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr {output_dir_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf241b2f-b4ac-47a7-941a-9b66b9f3e440",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading events from file: ./praxis-bert-large-uncased-small-finetune/logs/events.out.tfevents.1716389762.hephaestus.2595688.0\n",
      "Step: 10, train/loss: 0.6819000244140625\n",
      "Step: 10, train/grad_norm: 3.657686710357666\n",
      "Step: 10, train/learning_rate: 4.9916805437533185e-05\n",
      "Step: 10, train/epoch: 0.01663893461227417\n",
      "Step: 20, train/loss: 0.6589999794960022\n",
      "Step: 20, train/grad_norm: 2.169076681137085\n",
      "Step: 20, train/learning_rate: 4.983361213817261e-05\n",
      "Step: 20, train/epoch: 0.03327786922454834\n",
      "Step: 30, train/loss: 0.6484000086784363\n",
      "Step: 30, train/grad_norm: 3.6420397758483887\n",
      "Step: 30, train/learning_rate: 4.975041520083323e-05\n",
      "Step: 30, train/epoch: 0.04991680383682251\n",
      "Step: 40, train/loss: 0.6299999952316284\n",
      "Step: 40, train/grad_norm: 4.6052656173706055\n",
      "Step: 40, train/learning_rate: 4.966722190147266e-05\n",
      "Step: 40, train/epoch: 0.06655573844909668\n",
      "Step: 50, train/loss: 0.641700029373169\n",
      "Step: 50, train/grad_norm: 6.3665618896484375\n",
      "Step: 50, train/learning_rate: 4.958402496413328e-05\n",
      "Step: 50, train/epoch: 0.08319467306137085\n",
      "Step: 60, train/loss: 0.6222000122070312\n",
      "Step: 60, train/grad_norm: 11.7671480178833\n",
      "Step: 60, train/learning_rate: 4.9500831664772704e-05\n",
      "Step: 60, train/epoch: 0.09983360767364502\n",
      "Step: 70, train/loss: 0.650600016117096\n",
      "Step: 70, train/grad_norm: 8.749176979064941\n",
      "Step: 70, train/learning_rate: 4.941763836541213e-05\n",
      "Step: 70, train/epoch: 0.11647254228591919\n",
      "Step: 80, train/loss: 0.6360999941825867\n",
      "Step: 80, train/grad_norm: 4.537679672241211\n",
      "Step: 80, train/learning_rate: 4.933444142807275e-05\n",
      "Step: 80, train/epoch: 0.13311147689819336\n",
      "Step: 90, train/loss: 0.6269999742507935\n",
      "Step: 90, train/grad_norm: 2.690973997116089\n",
      "Step: 90, train/learning_rate: 4.925124812871218e-05\n",
      "Step: 90, train/epoch: 0.14975041151046753\n",
      "Step: 100, train/loss: 0.6292999982833862\n",
      "Step: 100, train/grad_norm: 3.264220714569092\n",
      "Step: 100, train/learning_rate: 4.9168054829351604e-05\n",
      "Step: 100, train/epoch: 0.1663893461227417\n",
      "Step: 110, train/loss: 0.6302000284194946\n",
      "Step: 110, train/grad_norm: 3.6637439727783203\n",
      "Step: 110, train/learning_rate: 4.9084857892012224e-05\n",
      "Step: 110, train/epoch: 0.18302828073501587\n",
      "Step: 120, train/loss: 0.6171000003814697\n",
      "Step: 120, train/grad_norm: 6.9460039138793945\n",
      "Step: 120, train/learning_rate: 4.900166459265165e-05\n",
      "Step: 120, train/epoch: 0.19966721534729004\n",
      "Step: 130, train/loss: 0.5800999999046326\n",
      "Step: 130, train/grad_norm: 5.89971923828125\n",
      "Step: 130, train/learning_rate: 4.891846765531227e-05\n",
      "Step: 130, train/epoch: 0.2163061499595642\n",
      "Step: 140, train/loss: 0.5471000075340271\n",
      "Step: 140, train/grad_norm: 26.219844818115234\n",
      "Step: 140, train/learning_rate: 4.88352743559517e-05\n",
      "Step: 140, train/epoch: 0.23294508457183838\n",
      "Step: 150, train/loss: 0.5475999712944031\n",
      "Step: 150, train/grad_norm: 80.8899154663086\n",
      "Step: 150, train/learning_rate: 4.875208105659112e-05\n",
      "Step: 150, train/epoch: 0.24958401918411255\n",
      "Step: 160, train/loss: 0.6456999778747559\n",
      "Step: 160, train/grad_norm: 4.638545513153076\n",
      "Step: 160, train/learning_rate: 4.866888411925174e-05\n",
      "Step: 160, train/epoch: 0.2662229537963867\n",
      "Step: 170, train/loss: 0.6119999885559082\n",
      "Step: 170, train/grad_norm: 2.786513328552246\n",
      "Step: 170, train/learning_rate: 4.858569081989117e-05\n",
      "Step: 170, train/epoch: 0.2828618884086609\n",
      "Step: 180, train/loss: 0.5932000279426575\n",
      "Step: 180, train/grad_norm: 3.500417470932007\n",
      "Step: 180, train/learning_rate: 4.8502497520530596e-05\n",
      "Step: 180, train/epoch: 0.29950082302093506\n",
      "Step: 190, train/loss: 0.576200008392334\n",
      "Step: 190, train/grad_norm: 6.750767707824707\n",
      "Step: 190, train/learning_rate: 4.8419300583191216e-05\n",
      "Step: 190, train/epoch: 0.31613975763320923\n",
      "Step: 200, train/loss: 0.5598000288009644\n",
      "Step: 200, train/grad_norm: 5.947140693664551\n",
      "Step: 200, train/learning_rate: 4.833610728383064e-05\n",
      "Step: 200, train/epoch: 0.3327786922454834\n",
      "Step: 210, train/loss: 0.501800000667572\n",
      "Step: 210, train/grad_norm: 4.598869800567627\n",
      "Step: 210, train/learning_rate: 4.825291034649126e-05\n",
      "Step: 210, train/epoch: 0.34941762685775757\n",
      "Step: 220, train/loss: 0.5388000011444092\n",
      "Step: 220, train/grad_norm: 6.736586570739746\n",
      "Step: 220, train/learning_rate: 4.816971704713069e-05\n",
      "Step: 220, train/epoch: 0.36605656147003174\n",
      "Step: 230, train/loss: 0.5210999846458435\n",
      "Step: 230, train/grad_norm: 6.4862470626831055\n",
      "Step: 230, train/learning_rate: 4.8086523747770116e-05\n",
      "Step: 230, train/epoch: 0.3826954960823059\n",
      "Step: 240, train/loss: 0.49160000681877136\n",
      "Step: 240, train/grad_norm: 5.892024040222168\n",
      "Step: 240, train/learning_rate: 4.8003326810430735e-05\n",
      "Step: 240, train/epoch: 0.3993344306945801\n",
      "Step: 250, train/loss: 0.4871000051498413\n",
      "Step: 250, train/grad_norm: 14.77529525756836\n",
      "Step: 250, train/learning_rate: 4.792013351107016e-05\n",
      "Step: 250, train/epoch: 0.41597336530685425\n",
      "Step: 260, train/loss: 0.49959999322891235\n",
      "Step: 260, train/grad_norm: 4.900386333465576\n",
      "Step: 260, train/learning_rate: 4.783694021170959e-05\n",
      "Step: 260, train/epoch: 0.4326122999191284\n",
      "Step: 270, train/loss: 0.5734000205993652\n",
      "Step: 270, train/grad_norm: 6.212315559387207\n",
      "Step: 270, train/learning_rate: 4.775374327437021e-05\n",
      "Step: 270, train/epoch: 0.4492512345314026\n",
      "Step: 280, train/loss: 0.5584999918937683\n",
      "Step: 280, train/grad_norm: 3.7486259937286377\n",
      "Step: 280, train/learning_rate: 4.7670549975009635e-05\n",
      "Step: 280, train/epoch: 0.46589016914367676\n",
      "Step: 290, train/loss: 0.6019999980926514\n",
      "Step: 290, train/grad_norm: 4.0343122482299805\n",
      "Step: 290, train/learning_rate: 4.7587353037670255e-05\n",
      "Step: 290, train/epoch: 0.4825291037559509\n",
      "Step: 300, train/loss: 0.5475000143051147\n",
      "Step: 300, train/grad_norm: 6.415895938873291\n",
      "Step: 300, train/learning_rate: 4.750415973830968e-05\n",
      "Step: 300, train/epoch: 0.4991680383682251\n",
      "Step: 310, train/loss: 0.5263000130653381\n",
      "Step: 310, train/grad_norm: 4.782790660858154\n",
      "Step: 310, train/learning_rate: 4.742096643894911e-05\n",
      "Step: 310, train/epoch: 0.5158069729804993\n",
      "Step: 320, train/loss: 0.5277000069618225\n",
      "Step: 320, train/grad_norm: 5.045601844787598\n",
      "Step: 320, train/learning_rate: 4.733776950160973e-05\n",
      "Step: 320, train/epoch: 0.5324459075927734\n",
      "Step: 330, train/loss: 0.5181999802589417\n",
      "Step: 330, train/grad_norm: 4.512552261352539\n",
      "Step: 330, train/learning_rate: 4.7254576202249154e-05\n",
      "Step: 330, train/epoch: 0.5490848422050476\n",
      "Step: 340, train/loss: 0.47690001130104065\n",
      "Step: 340, train/grad_norm: 5.035177707672119\n",
      "Step: 340, train/learning_rate: 4.7171379264909774e-05\n",
      "Step: 340, train/epoch: 0.5657237768173218\n",
      "Step: 350, train/loss: 0.4697999954223633\n",
      "Step: 350, train/grad_norm: 4.453613758087158\n",
      "Step: 350, train/learning_rate: 4.70881859655492e-05\n",
      "Step: 350, train/epoch: 0.582362711429596\n",
      "Step: 360, train/loss: 0.5220000147819519\n",
      "Step: 360, train/grad_norm: 6.040460109710693\n",
      "Step: 360, train/learning_rate: 4.700499266618863e-05\n",
      "Step: 360, train/epoch: 0.5990016460418701\n",
      "Step: 370, train/loss: 0.4803999960422516\n",
      "Step: 370, train/grad_norm: 6.804906368255615\n",
      "Step: 370, train/learning_rate: 4.692179572884925e-05\n",
      "Step: 370, train/epoch: 0.6156405806541443\n",
      "Step: 380, train/loss: 0.5253999829292297\n",
      "Step: 380, train/grad_norm: 9.032817840576172\n",
      "Step: 380, train/learning_rate: 4.6838602429488674e-05\n",
      "Step: 380, train/epoch: 0.6322795152664185\n",
      "Step: 390, train/loss: 0.5619000196456909\n",
      "Step: 390, train/grad_norm: 3.8427183628082275\n",
      "Step: 390, train/learning_rate: 4.67554091301281e-05\n",
      "Step: 390, train/epoch: 0.6489184498786926\n",
      "Step: 400, train/loss: 0.5407999753952026\n",
      "Step: 400, train/grad_norm: 9.957931518554688\n",
      "Step: 400, train/learning_rate: 4.667221219278872e-05\n",
      "Step: 400, train/epoch: 0.6655573844909668\n",
      "Step: 410, train/loss: 0.5022000074386597\n",
      "Step: 410, train/grad_norm: 6.888420104980469\n",
      "Step: 410, train/learning_rate: 4.658901889342815e-05\n",
      "Step: 410, train/epoch: 0.682196319103241\n",
      "Step: 420, train/loss: 0.5102999806404114\n",
      "Step: 420, train/grad_norm: 7.45243501663208\n",
      "Step: 420, train/learning_rate: 4.6505821956088766e-05\n",
      "Step: 420, train/epoch: 0.6988352537155151\n",
      "Step: 430, train/loss: 0.5210000276565552\n",
      "Step: 430, train/grad_norm: 4.286772727966309\n",
      "Step: 430, train/learning_rate: 4.642262865672819e-05\n",
      "Step: 430, train/epoch: 0.7154741883277893\n",
      "Step: 440, train/loss: 0.414000004529953\n",
      "Step: 440, train/grad_norm: 8.351259231567383\n",
      "Step: 440, train/learning_rate: 4.633943535736762e-05\n",
      "Step: 440, train/epoch: 0.7321131229400635\n",
      "Step: 450, train/loss: 0.4796000123023987\n",
      "Step: 450, train/grad_norm: 5.528547763824463\n",
      "Step: 450, train/learning_rate: 4.625623842002824e-05\n",
      "Step: 450, train/epoch: 0.7487520575523376\n",
      "Step: 460, train/loss: 0.4934999942779541\n",
      "Step: 460, train/grad_norm: 4.931757926940918\n",
      "Step: 460, train/learning_rate: 4.6173045120667666e-05\n",
      "Step: 460, train/epoch: 0.7653909921646118\n",
      "Step: 470, train/loss: 0.38040000200271606\n",
      "Step: 470, train/grad_norm: 4.6560492515563965\n",
      "Step: 470, train/learning_rate: 4.608985182130709e-05\n",
      "Step: 470, train/epoch: 0.782029926776886\n",
      "Step: 480, train/loss: 0.43470001220703125\n",
      "Step: 480, train/grad_norm: 8.717528343200684\n",
      "Step: 480, train/learning_rate: 4.600665488396771e-05\n",
      "Step: 480, train/epoch: 0.7986688613891602\n",
      "Step: 490, train/loss: 0.4903999865055084\n",
      "Step: 490, train/grad_norm: 3.263373851776123\n",
      "Step: 490, train/learning_rate: 4.592346158460714e-05\n",
      "Step: 490, train/epoch: 0.8153077960014343\n",
      "Step: 500, train/loss: 0.45570001006126404\n",
      "Step: 500, train/grad_norm: 4.523529529571533\n",
      "Step: 500, train/learning_rate: 4.584026464726776e-05\n",
      "Step: 500, train/epoch: 0.8319467306137085\n",
      "Step: 510, train/loss: 0.4629000127315521\n",
      "Step: 510, train/grad_norm: 7.75285005569458\n",
      "Step: 510, train/learning_rate: 4.5757071347907186e-05\n",
      "Step: 510, train/epoch: 0.8485856652259827\n",
      "Step: 520, train/loss: 0.4174000024795532\n",
      "Step: 520, train/grad_norm: 4.87214994430542\n",
      "Step: 520, train/learning_rate: 4.567387804854661e-05\n",
      "Step: 520, train/epoch: 0.8652245998382568\n",
      "Step: 530, train/loss: 0.4431000053882599\n",
      "Step: 530, train/grad_norm: 6.986176490783691\n",
      "Step: 530, train/learning_rate: 4.559068111120723e-05\n",
      "Step: 530, train/epoch: 0.881863534450531\n",
      "Step: 540, train/loss: 0.43380001187324524\n",
      "Step: 540, train/grad_norm: 5.545015811920166\n",
      "Step: 540, train/learning_rate: 4.550748781184666e-05\n",
      "Step: 540, train/epoch: 0.8985024690628052\n",
      "Step: 550, train/loss: 0.44350001215934753\n",
      "Step: 550, train/grad_norm: 3.6709489822387695\n",
      "Step: 550, train/learning_rate: 4.5424294512486085e-05\n",
      "Step: 550, train/epoch: 0.9151414036750793\n",
      "Step: 560, train/loss: 0.4546999931335449\n",
      "Step: 560, train/grad_norm: 12.960838317871094\n",
      "Step: 560, train/learning_rate: 4.5341097575146705e-05\n",
      "Step: 560, train/epoch: 0.9317803382873535\n",
      "Step: 570, train/loss: 0.45680001378059387\n",
      "Step: 570, train/grad_norm: 10.963479042053223\n",
      "Step: 570, train/learning_rate: 4.525790427578613e-05\n",
      "Step: 570, train/epoch: 0.9484192728996277\n",
      "Step: 580, train/loss: 0.4415999948978424\n",
      "Step: 580, train/grad_norm: 9.563138008117676\n",
      "Step: 580, train/learning_rate: 4.517470733844675e-05\n",
      "Step: 580, train/epoch: 0.9650582075119019\n",
      "Step: 590, train/loss: 0.40799999237060547\n",
      "Step: 590, train/grad_norm: 6.184965133666992\n",
      "Step: 590, train/learning_rate: 4.509151403908618e-05\n",
      "Step: 590, train/epoch: 0.981697142124176\n",
      "Step: 600, train/loss: 0.39309999346733093\n",
      "Step: 600, train/grad_norm: 8.54326343536377\n",
      "Step: 600, train/learning_rate: 4.5008320739725605e-05\n",
      "Step: 600, train/epoch: 0.9983360767364502\n",
      "Step: 601, eval/loss: 0.36059924960136414\n",
      "Step: 601, eval/accuracy: 0.8438150882720947\n",
      "Step: 601, eval/f1: 0.8325792551040649\n",
      "Step: 601, eval/runtime: 81.10440063476562\n",
      "Step: 601, eval/samples_per_second: 88.81099700927734\n",
      "Step: 601, eval/steps_per_second: 1.590999960899353\n",
      "Step: 601, train/epoch: 1.0\n",
      "Step: 610, train/loss: 0.34689998626708984\n",
      "Step: 610, train/grad_norm: 7.719303607940674\n",
      "Step: 610, train/learning_rate: 4.4925123802386224e-05\n",
      "Step: 610, train/epoch: 1.0149750709533691\n",
      "Step: 620, train/loss: 0.34850001335144043\n",
      "Step: 620, train/grad_norm: 9.029298782348633\n",
      "Step: 620, train/learning_rate: 4.484193050302565e-05\n",
      "Step: 620, train/epoch: 1.0316139459609985\n",
      "Step: 630, train/loss: 0.35530000925064087\n",
      "Step: 630, train/grad_norm: 4.965867042541504\n",
      "Step: 630, train/learning_rate: 4.475873720366508e-05\n",
      "Step: 630, train/epoch: 1.0482529401779175\n",
      "Step: 640, train/loss: 0.2754000127315521\n",
      "Step: 640, train/grad_norm: 12.91421127319336\n",
      "Step: 640, train/learning_rate: 4.46755402663257e-05\n",
      "Step: 640, train/epoch: 1.0648918151855469\n",
      "Step: 650, train/loss: 0.33000001311302185\n",
      "Step: 650, train/grad_norm: 10.323365211486816\n",
      "Step: 650, train/learning_rate: 4.4592346966965124e-05\n",
      "Step: 650, train/epoch: 1.0815308094024658\n",
      "Step: 660, train/loss: 0.35249999165534973\n",
      "Step: 660, train/grad_norm: 7.754454135894775\n",
      "Step: 660, train/learning_rate: 4.4509150029625744e-05\n",
      "Step: 660, train/epoch: 1.0981696844100952\n",
      "Step: 670, train/loss: 0.31790000200271606\n",
      "Step: 670, train/grad_norm: 4.826776027679443\n",
      "Step: 670, train/learning_rate: 4.442595673026517e-05\n",
      "Step: 670, train/epoch: 1.1148086786270142\n",
      "Step: 680, train/loss: 0.3005000054836273\n",
      "Step: 680, train/grad_norm: 6.902236461639404\n",
      "Step: 680, train/learning_rate: 4.43427634309046e-05\n",
      "Step: 680, train/epoch: 1.1314475536346436\n",
      "Step: 690, train/loss: 0.3465000092983246\n",
      "Step: 690, train/grad_norm: 15.339977264404297\n",
      "Step: 690, train/learning_rate: 4.425956649356522e-05\n",
      "Step: 690, train/epoch: 1.1480865478515625\n",
      "Step: 700, train/loss: 0.3402000069618225\n",
      "Step: 700, train/grad_norm: 7.544649600982666\n",
      "Step: 700, train/learning_rate: 4.417637319420464e-05\n",
      "Step: 700, train/epoch: 1.164725422859192\n",
      "Step: 710, train/loss: 0.4083999991416931\n",
      "Step: 710, train/grad_norm: 5.331587314605713\n",
      "Step: 710, train/learning_rate: 4.409317625686526e-05\n",
      "Step: 710, train/epoch: 1.1813644170761108\n",
      "Step: 720, train/loss: 0.29330000281333923\n",
      "Step: 720, train/grad_norm: 7.895542144775391\n",
      "Step: 720, train/learning_rate: 4.400998295750469e-05\n",
      "Step: 720, train/epoch: 1.1980032920837402\n",
      "Step: 730, train/loss: 0.3361999988555908\n",
      "Step: 730, train/grad_norm: 7.067652702331543\n",
      "Step: 730, train/learning_rate: 4.3926789658144116e-05\n",
      "Step: 730, train/epoch: 1.2146422863006592\n",
      "Step: 740, train/loss: 0.2759999930858612\n",
      "Step: 740, train/grad_norm: 10.54889965057373\n",
      "Step: 740, train/learning_rate: 4.3843592720804736e-05\n",
      "Step: 740, train/epoch: 1.2312811613082886\n",
      "Step: 750, train/loss: 0.2948000133037567\n",
      "Step: 750, train/grad_norm: 6.142396450042725\n",
      "Step: 750, train/learning_rate: 4.376039942144416e-05\n",
      "Step: 750, train/epoch: 1.2479201555252075\n",
      "Step: 760, train/loss: 0.31690001487731934\n",
      "Step: 760, train/grad_norm: 13.175126075744629\n",
      "Step: 760, train/learning_rate: 4.367720612208359e-05\n",
      "Step: 760, train/epoch: 1.264559030532837\n",
      "Step: 770, train/loss: 0.30640000104904175\n",
      "Step: 770, train/grad_norm: 5.926136016845703\n",
      "Step: 770, train/learning_rate: 4.359400918474421e-05\n",
      "Step: 770, train/epoch: 1.2811980247497559\n",
      "Step: 780, train/loss: 0.27549999952316284\n",
      "Step: 780, train/grad_norm: 5.178319454193115\n",
      "Step: 780, train/learning_rate: 4.3510815885383636e-05\n",
      "Step: 780, train/epoch: 1.2978368997573853\n",
      "Step: 790, train/loss: 0.2637999951839447\n",
      "Step: 790, train/grad_norm: 7.383257865905762\n",
      "Step: 790, train/learning_rate: 4.3427618948044255e-05\n",
      "Step: 790, train/epoch: 1.3144758939743042\n",
      "Step: 800, train/loss: 0.42320001125335693\n",
      "Step: 800, train/grad_norm: 7.496824741363525\n",
      "Step: 800, train/learning_rate: 4.334442564868368e-05\n",
      "Step: 800, train/epoch: 1.3311147689819336\n",
      "Step: 810, train/loss: 0.30820000171661377\n",
      "Step: 810, train/grad_norm: 5.795135974884033\n",
      "Step: 810, train/learning_rate: 4.326123234932311e-05\n",
      "Step: 810, train/epoch: 1.3477537631988525\n",
      "Step: 820, train/loss: 0.27889999747276306\n",
      "Step: 820, train/grad_norm: 5.5813493728637695\n",
      "Step: 820, train/learning_rate: 4.317803541198373e-05\n",
      "Step: 820, train/epoch: 1.364392638206482\n",
      "Step: 830, train/loss: 0.3262999951839447\n",
      "Step: 830, train/grad_norm: 6.686099052429199\n",
      "Step: 830, train/learning_rate: 4.3094842112623155e-05\n",
      "Step: 830, train/epoch: 1.3810316324234009\n",
      "Step: 840, train/loss: 0.33660000562667847\n",
      "Step: 840, train/grad_norm: 7.21431827545166\n",
      "Step: 840, train/learning_rate: 4.301164881326258e-05\n",
      "Step: 840, train/epoch: 1.3976705074310303\n",
      "Step: 850, train/loss: 0.28380000591278076\n",
      "Step: 850, train/grad_norm: 5.278636932373047\n",
      "Step: 850, train/learning_rate: 4.29284518759232e-05\n",
      "Step: 850, train/epoch: 1.4143095016479492\n",
      "Step: 860, train/loss: 0.27379998564720154\n",
      "Step: 860, train/grad_norm: 8.341675758361816\n",
      "Step: 860, train/learning_rate: 4.284525857656263e-05\n",
      "Step: 860, train/epoch: 1.4309483766555786\n",
      "Step: 870, train/loss: 0.30709999799728394\n",
      "Step: 870, train/grad_norm: 5.742964267730713\n",
      "Step: 870, train/learning_rate: 4.276206163922325e-05\n",
      "Step: 870, train/epoch: 1.4475873708724976\n",
      "Step: 880, train/loss: 0.2799000144004822\n",
      "Step: 880, train/grad_norm: 9.538690567016602\n",
      "Step: 880, train/learning_rate: 4.2678868339862674e-05\n",
      "Step: 880, train/epoch: 1.464226245880127\n",
      "Step: 890, train/loss: 0.2799000144004822\n",
      "Step: 890, train/grad_norm: 8.279274940490723\n",
      "Step: 890, train/learning_rate: 4.25956750405021e-05\n",
      "Step: 890, train/epoch: 1.480865240097046\n",
      "Step: 900, train/loss: 0.3188000023365021\n",
      "Step: 900, train/grad_norm: 14.414512634277344\n",
      "Step: 900, train/learning_rate: 4.251247810316272e-05\n",
      "Step: 900, train/epoch: 1.4975041151046753\n",
      "Step: 910, train/loss: 0.3215999901294708\n",
      "Step: 910, train/grad_norm: 9.898310661315918\n",
      "Step: 910, train/learning_rate: 4.242928480380215e-05\n",
      "Step: 910, train/epoch: 1.5141431093215942\n",
      "Step: 920, train/loss: 0.2867000102996826\n",
      "Step: 920, train/grad_norm: 12.816717147827148\n",
      "Step: 920, train/learning_rate: 4.2346091504441574e-05\n",
      "Step: 920, train/epoch: 1.5307819843292236\n",
      "Step: 930, train/loss: 0.32089999318122864\n",
      "Step: 930, train/grad_norm: 8.799272537231445\n",
      "Step: 930, train/learning_rate: 4.2262894567102194e-05\n",
      "Step: 930, train/epoch: 1.5474209785461426\n",
      "Step: 940, train/loss: 0.3441999852657318\n",
      "Step: 940, train/grad_norm: 9.031394958496094\n",
      "Step: 940, train/learning_rate: 4.217970126774162e-05\n",
      "Step: 940, train/epoch: 1.564059853553772\n",
      "Step: 950, train/loss: 0.289000004529953\n",
      "Step: 950, train/grad_norm: 6.869452953338623\n",
      "Step: 950, train/learning_rate: 4.209650433040224e-05\n",
      "Step: 950, train/epoch: 1.580698847770691\n",
      "Step: 960, train/loss: 0.2849999964237213\n",
      "Step: 960, train/grad_norm: 9.43094539642334\n",
      "Step: 960, train/learning_rate: 4.201331103104167e-05\n",
      "Step: 960, train/epoch: 1.5973377227783203\n",
      "Step: 970, train/loss: 0.3156000077724457\n",
      "Step: 970, train/grad_norm: 6.665925025939941\n",
      "Step: 970, train/learning_rate: 4.1930117731681094e-05\n",
      "Step: 970, train/epoch: 1.6139767169952393\n",
      "Step: 980, train/loss: 0.3659999966621399\n",
      "Step: 980, train/grad_norm: 120.88938903808594\n",
      "Step: 980, train/learning_rate: 4.184692079434171e-05\n",
      "Step: 980, train/epoch: 1.6306155920028687\n",
      "Step: 990, train/loss: 0.45730000734329224\n",
      "Step: 990, train/grad_norm: 37.41096496582031\n",
      "Step: 990, train/learning_rate: 4.176372749498114e-05\n",
      "Step: 990, train/epoch: 1.6472545862197876\n",
      "Step: 1000, train/loss: 0.3521000146865845\n",
      "Step: 1000, train/grad_norm: 7.380383491516113\n",
      "Step: 1000, train/learning_rate: 4.1680534195620567e-05\n",
      "Step: 1000, train/epoch: 1.663893461227417\n",
      "Step: 1010, train/loss: 0.3142000138759613\n",
      "Step: 1010, train/grad_norm: 22.83286476135254\n",
      "Step: 1010, train/learning_rate: 4.1597337258281186e-05\n",
      "Step: 1010, train/epoch: 1.680532455444336\n",
      "Step: 1020, train/loss: 0.26840001344680786\n",
      "Step: 1020, train/grad_norm: 8.26580810546875\n",
      "Step: 1020, train/learning_rate: 4.151414395892061e-05\n",
      "Step: 1020, train/epoch: 1.6971713304519653\n",
      "Step: 1030, train/loss: 0.3176000118255615\n",
      "Step: 1030, train/grad_norm: 6.613798141479492\n",
      "Step: 1030, train/learning_rate: 4.143094702158123e-05\n",
      "Step: 1030, train/epoch: 1.7138103246688843\n",
      "Step: 1040, train/loss: 0.29190000891685486\n",
      "Step: 1040, train/grad_norm: 11.150859832763672\n",
      "Step: 1040, train/learning_rate: 4.134775372222066e-05\n",
      "Step: 1040, train/epoch: 1.7304491996765137\n",
      "Step: 1050, train/loss: 0.32589998841285706\n",
      "Step: 1050, train/grad_norm: 12.4976167678833\n",
      "Step: 1050, train/learning_rate: 4.1264560422860086e-05\n",
      "Step: 1050, train/epoch: 1.7470881938934326\n",
      "Step: 1060, train/loss: 0.3668999969959259\n",
      "Step: 1060, train/grad_norm: 7.902416706085205\n",
      "Step: 1060, train/learning_rate: 4.1181363485520706e-05\n",
      "Step: 1060, train/epoch: 1.763727068901062\n",
      "Step: 1070, train/loss: 0.31529998779296875\n",
      "Step: 1070, train/grad_norm: 6.13801383972168\n",
      "Step: 1070, train/learning_rate: 4.109817018616013e-05\n",
      "Step: 1070, train/epoch: 1.780366063117981\n",
      "Step: 1080, train/loss: 0.3172000050544739\n",
      "Step: 1080, train/grad_norm: 8.793351173400879\n",
      "Step: 1080, train/learning_rate: 4.101497324882075e-05\n",
      "Step: 1080, train/epoch: 1.7970049381256104\n",
      "Step: 1090, train/loss: 0.26030001044273376\n",
      "Step: 1090, train/grad_norm: 8.911458969116211\n",
      "Step: 1090, train/learning_rate: 4.093177994946018e-05\n",
      "Step: 1090, train/epoch: 1.8136439323425293\n",
      "Step: 1100, train/loss: 0.28380000591278076\n",
      "Step: 1100, train/grad_norm: 8.478662490844727\n",
      "Step: 1100, train/learning_rate: 4.0848586650099605e-05\n",
      "Step: 1100, train/epoch: 1.8302828073501587\n",
      "Step: 1110, train/loss: 0.23019999265670776\n",
      "Step: 1110, train/grad_norm: 15.695149421691895\n",
      "Step: 1110, train/learning_rate: 4.0765389712760225e-05\n",
      "Step: 1110, train/epoch: 1.8469218015670776\n",
      "Step: 1120, train/loss: 0.3010999858379364\n",
      "Step: 1120, train/grad_norm: 7.303323745727539\n",
      "Step: 1120, train/learning_rate: 4.068219641339965e-05\n",
      "Step: 1120, train/epoch: 1.863560676574707\n",
      "Step: 1130, train/loss: 0.27320000529289246\n",
      "Step: 1130, train/grad_norm: 8.382616996765137\n",
      "Step: 1130, train/learning_rate: 4.059900311403908e-05\n",
      "Step: 1130, train/epoch: 1.880199670791626\n",
      "Step: 1140, train/loss: 0.2973000109195709\n",
      "Step: 1140, train/grad_norm: 6.350660800933838\n",
      "Step: 1140, train/learning_rate: 4.05158061766997e-05\n",
      "Step: 1140, train/epoch: 1.8968385457992554\n",
      "Step: 1150, train/loss: 0.3028999865055084\n",
      "Step: 1150, train/grad_norm: 7.819075107574463\n",
      "Step: 1150, train/learning_rate: 4.0432612877339125e-05\n",
      "Step: 1150, train/epoch: 1.9134775400161743\n",
      "Step: 1160, train/loss: 0.34450000524520874\n",
      "Step: 1160, train/grad_norm: 10.252517700195312\n",
      "Step: 1160, train/learning_rate: 4.0349415939999744e-05\n",
      "Step: 1160, train/epoch: 1.9301164150238037\n",
      "Step: 1170, train/loss: 0.26159998774528503\n",
      "Step: 1170, train/grad_norm: 5.703171253204346\n",
      "Step: 1170, train/learning_rate: 4.026622264063917e-05\n",
      "Step: 1170, train/epoch: 1.9467554092407227\n",
      "Step: 1180, train/loss: 0.25110000371932983\n",
      "Step: 1180, train/grad_norm: 11.041226387023926\n",
      "Step: 1180, train/learning_rate: 4.01830293412786e-05\n",
      "Step: 1180, train/epoch: 1.963394284248352\n",
      "Step: 1190, train/loss: 0.28040000796318054\n",
      "Step: 1190, train/grad_norm: 6.087123870849609\n",
      "Step: 1190, train/learning_rate: 4.009983240393922e-05\n",
      "Step: 1190, train/epoch: 1.980033278465271\n",
      "Step: 1200, train/loss: 0.28940001130104065\n",
      "Step: 1200, train/grad_norm: 3.7796847820281982\n",
      "Step: 1200, train/learning_rate: 4.0016639104578644e-05\n",
      "Step: 1200, train/epoch: 1.9966721534729004\n",
      "Step: 1202, eval/loss: 0.298993319272995\n",
      "Step: 1202, eval/accuracy: 0.8746355772018433\n",
      "Step: 1202, eval/f1: 0.8640034198760986\n",
      "Step: 1202, eval/runtime: 81.15859985351562\n",
      "Step: 1202, eval/samples_per_second: 88.75199890136719\n",
      "Step: 1202, eval/steps_per_second: 1.5889999866485596\n",
      "Step: 1202, train/epoch: 2.0\n",
      "Step: 1210, train/loss: 0.22110000252723694\n",
      "Step: 1210, train/grad_norm: 6.69949197769165\n",
      "Step: 1210, train/learning_rate: 3.993344580521807e-05\n",
      "Step: 1210, train/epoch: 2.0133111476898193\n",
      "Step: 1220, train/loss: 0.13940000534057617\n",
      "Step: 1220, train/grad_norm: 13.478719711303711\n",
      "Step: 1220, train/learning_rate: 3.985024886787869e-05\n",
      "Step: 1220, train/epoch: 2.0299501419067383\n",
      "Step: 1230, train/loss: 0.2296999990940094\n",
      "Step: 1230, train/grad_norm: 29.544902801513672\n",
      "Step: 1230, train/learning_rate: 3.976705556851812e-05\n",
      "Step: 1230, train/epoch: 2.0465891361236572\n",
      "Step: 1240, train/loss: 0.15109999477863312\n",
      "Step: 1240, train/grad_norm: 4.747902870178223\n",
      "Step: 1240, train/learning_rate: 3.968385863117874e-05\n",
      "Step: 1240, train/epoch: 2.063227891921997\n",
      "Step: 1250, train/loss: 0.12710000574588776\n",
      "Step: 1250, train/grad_norm: 5.7203474044799805\n",
      "Step: 1250, train/learning_rate: 3.9600665331818163e-05\n",
      "Step: 1250, train/epoch: 2.079866886138916\n",
      "Step: 1260, train/loss: 0.13760000467300415\n",
      "Step: 1260, train/grad_norm: 14.402242660522461\n",
      "Step: 1260, train/learning_rate: 3.951747203245759e-05\n",
      "Step: 1260, train/epoch: 2.096505880355835\n",
      "Step: 1270, train/loss: 0.09730000048875809\n",
      "Step: 1270, train/grad_norm: 6.054759502410889\n",
      "Step: 1270, train/learning_rate: 3.943427509511821e-05\n",
      "Step: 1270, train/epoch: 2.113144874572754\n",
      "Step: 1280, train/loss: 0.11710000038146973\n",
      "Step: 1280, train/grad_norm: 7.886241436004639\n",
      "Step: 1280, train/learning_rate: 3.9351081795757636e-05\n",
      "Step: 1280, train/epoch: 2.1297836303710938\n",
      "Step: 1290, train/loss: 0.1817999929189682\n",
      "Step: 1290, train/grad_norm: 59.83172607421875\n",
      "Step: 1290, train/learning_rate: 3.926788849639706e-05\n",
      "Step: 1290, train/epoch: 2.1464226245880127\n",
      "Step: 1300, train/loss: 0.22849999368190765\n",
      "Step: 1300, train/grad_norm: 4.513936519622803\n",
      "Step: 1300, train/learning_rate: 3.918469155905768e-05\n",
      "Step: 1300, train/epoch: 2.1630616188049316\n",
      "Step: 1310, train/loss: 0.2184000015258789\n",
      "Step: 1310, train/grad_norm: 7.93461799621582\n",
      "Step: 1310, train/learning_rate: 3.910149825969711e-05\n",
      "Step: 1310, train/epoch: 2.1797006130218506\n",
      "Step: 1320, train/loss: 0.15399999916553497\n",
      "Step: 1320, train/grad_norm: 4.7274169921875\n",
      "Step: 1320, train/learning_rate: 3.901830132235773e-05\n",
      "Step: 1320, train/epoch: 2.1963393688201904\n",
      "Step: 1330, train/loss: 0.2484000027179718\n",
      "Step: 1330, train/grad_norm: 15.167468070983887\n",
      "Step: 1330, train/learning_rate: 3.8935108022997156e-05\n",
      "Step: 1330, train/epoch: 2.2129783630371094\n",
      "Step: 1340, train/loss: 0.15139999985694885\n",
      "Step: 1340, train/grad_norm: 5.1264543533325195\n",
      "Step: 1340, train/learning_rate: 3.885191472363658e-05\n",
      "Step: 1340, train/epoch: 2.2296173572540283\n",
      "Step: 1350, train/loss: 0.18780000507831573\n",
      "Step: 1350, train/grad_norm: 9.326313972473145\n",
      "Step: 1350, train/learning_rate: 3.87687177862972e-05\n",
      "Step: 1350, train/epoch: 2.2462563514709473\n",
      "Step: 1360, train/loss: 0.19089999794960022\n",
      "Step: 1360, train/grad_norm: 12.14399528503418\n",
      "Step: 1360, train/learning_rate: 3.868552448693663e-05\n",
      "Step: 1360, train/epoch: 2.262895107269287\n",
      "Step: 1370, train/loss: 0.17949999868869781\n",
      "Step: 1370, train/grad_norm: 9.23416519165039\n",
      "Step: 1370, train/learning_rate: 3.8602331187576056e-05\n",
      "Step: 1370, train/epoch: 2.279534101486206\n",
      "Step: 1380, train/loss: 0.1298999935388565\n",
      "Step: 1380, train/grad_norm: 6.769871711730957\n",
      "Step: 1380, train/learning_rate: 3.8519134250236675e-05\n",
      "Step: 1380, train/epoch: 2.296173095703125\n",
      "Step: 1390, train/loss: 0.1315000057220459\n",
      "Step: 1390, train/grad_norm: 14.823349952697754\n",
      "Step: 1390, train/learning_rate: 3.84359409508761e-05\n",
      "Step: 1390, train/epoch: 2.312812089920044\n",
      "Step: 1400, train/loss: 0.21729999780654907\n",
      "Step: 1400, train/grad_norm: 7.531352519989014\n",
      "Step: 1400, train/learning_rate: 3.835274401353672e-05\n",
      "Step: 1400, train/epoch: 2.329450845718384\n",
      "Step: 1410, train/loss: 0.13050000369548798\n",
      "Step: 1410, train/grad_norm: 2.694657325744629\n",
      "Step: 1410, train/learning_rate: 3.826955071417615e-05\n",
      "Step: 1410, train/epoch: 2.3460898399353027\n",
      "Step: 1420, train/loss: 0.16760000586509705\n",
      "Step: 1420, train/grad_norm: 11.986554145812988\n",
      "Step: 1420, train/learning_rate: 3.8186357414815575e-05\n",
      "Step: 1420, train/epoch: 2.3627288341522217\n",
      "Step: 1430, train/loss: 0.12790000438690186\n",
      "Step: 1430, train/grad_norm: 10.51446533203125\n",
      "Step: 1430, train/learning_rate: 3.8103160477476195e-05\n",
      "Step: 1430, train/epoch: 2.3793678283691406\n",
      "Step: 1440, train/loss: 0.1632000058889389\n",
      "Step: 1440, train/grad_norm: 15.301133155822754\n",
      "Step: 1440, train/learning_rate: 3.801996717811562e-05\n",
      "Step: 1440, train/epoch: 2.3960065841674805\n",
      "Step: 1450, train/loss: 0.10610000044107437\n",
      "Step: 1450, train/grad_norm: 7.5099687576293945\n",
      "Step: 1450, train/learning_rate: 3.793677024077624e-05\n",
      "Step: 1450, train/epoch: 2.4126455783843994\n",
      "Step: 1460, train/loss: 0.1428000032901764\n",
      "Step: 1460, train/grad_norm: 12.823010444641113\n",
      "Step: 1460, train/learning_rate: 3.785357694141567e-05\n",
      "Step: 1460, train/epoch: 2.4292845726013184\n",
      "Step: 1470, train/loss: 0.16500000655651093\n",
      "Step: 1470, train/grad_norm: 11.92817211151123\n",
      "Step: 1470, train/learning_rate: 3.7770383642055094e-05\n",
      "Step: 1470, train/epoch: 2.4459235668182373\n",
      "Step: 1480, train/loss: 0.17560000717639923\n",
      "Step: 1480, train/grad_norm: 14.655537605285645\n",
      "Step: 1480, train/learning_rate: 3.7687186704715714e-05\n",
      "Step: 1480, train/epoch: 2.462562322616577\n",
      "Step: 1490, train/loss: 0.23929999768733978\n",
      "Step: 1490, train/grad_norm: 22.907060623168945\n",
      "Step: 1490, train/learning_rate: 3.760399340535514e-05\n",
      "Step: 1490, train/epoch: 2.479201316833496\n",
      "Step: 1500, train/loss: 0.15369999408721924\n",
      "Step: 1500, train/grad_norm: 6.981569766998291\n",
      "Step: 1500, train/learning_rate: 3.752080010599457e-05\n",
      "Step: 1500, train/epoch: 2.495840311050415\n",
      "Step: 1510, train/loss: 0.155799999833107\n",
      "Step: 1510, train/grad_norm: 7.32360315322876\n",
      "Step: 1510, train/learning_rate: 3.743760316865519e-05\n",
      "Step: 1510, train/epoch: 2.512479305267334\n",
      "Step: 1520, train/loss: 0.15929999947547913\n",
      "Step: 1520, train/grad_norm: 10.259540557861328\n",
      "Step: 1520, train/learning_rate: 3.7354409869294614e-05\n",
      "Step: 1520, train/epoch: 2.529118061065674\n",
      "Step: 1530, train/loss: 0.15109999477863312\n",
      "Step: 1530, train/grad_norm: 7.907167911529541\n",
      "Step: 1530, train/learning_rate: 3.727121293195523e-05\n",
      "Step: 1530, train/epoch: 2.5457570552825928\n",
      "Step: 1540, train/loss: 0.17399999499320984\n",
      "Step: 1540, train/grad_norm: 8.131012916564941\n",
      "Step: 1540, train/learning_rate: 3.718801963259466e-05\n",
      "Step: 1540, train/epoch: 2.5623960494995117\n",
      "Step: 1550, train/loss: 0.19750000536441803\n",
      "Step: 1550, train/grad_norm: 4.248681545257568\n",
      "Step: 1550, train/learning_rate: 3.710482633323409e-05\n",
      "Step: 1550, train/epoch: 2.5790350437164307\n",
      "Step: 1560, train/loss: 0.20069999992847443\n",
      "Step: 1560, train/grad_norm: 5.465976238250732\n",
      "Step: 1560, train/learning_rate: 3.7021629395894706e-05\n",
      "Step: 1560, train/epoch: 2.5956737995147705\n",
      "Step: 1570, train/loss: 0.19699999690055847\n",
      "Step: 1570, train/grad_norm: 6.826797962188721\n",
      "Step: 1570, train/learning_rate: 3.693843609653413e-05\n",
      "Step: 1570, train/epoch: 2.6123127937316895\n",
      "Step: 1580, train/loss: 0.16099999845027924\n",
      "Step: 1580, train/grad_norm: 7.1105194091796875\n",
      "Step: 1580, train/learning_rate: 3.685524279717356e-05\n",
      "Step: 1580, train/epoch: 2.6289517879486084\n",
      "Step: 1590, train/loss: 0.1437000036239624\n",
      "Step: 1590, train/grad_norm: 7.9957685470581055\n",
      "Step: 1590, train/learning_rate: 3.677204585983418e-05\n",
      "Step: 1590, train/epoch: 2.6455907821655273\n",
      "Step: 1600, train/loss: 0.10090000182390213\n",
      "Step: 1600, train/grad_norm: 8.354726791381836\n",
      "Step: 1600, train/learning_rate: 3.6688852560473606e-05\n",
      "Step: 1600, train/epoch: 2.662229537963867\n",
      "Step: 1610, train/loss: 0.14059999585151672\n",
      "Step: 1610, train/grad_norm: 6.95693826675415\n",
      "Step: 1610, train/learning_rate: 3.6605655623134226e-05\n",
      "Step: 1610, train/epoch: 2.678868532180786\n",
      "Step: 1620, train/loss: 0.1746000051498413\n",
      "Step: 1620, train/grad_norm: 6.106720924377441\n",
      "Step: 1620, train/learning_rate: 3.652246232377365e-05\n",
      "Step: 1620, train/epoch: 2.695507526397705\n",
      "Step: 1630, train/loss: 0.13830000162124634\n",
      "Step: 1630, train/grad_norm: 10.331401824951172\n",
      "Step: 1630, train/learning_rate: 3.643926902441308e-05\n",
      "Step: 1630, train/epoch: 2.712146520614624\n",
      "Step: 1640, train/loss: 0.12409999966621399\n",
      "Step: 1640, train/grad_norm: 6.9906182289123535\n",
      "Step: 1640, train/learning_rate: 3.63560720870737e-05\n",
      "Step: 1640, train/epoch: 2.728785276412964\n",
      "Step: 1650, train/loss: 0.13050000369548798\n",
      "Step: 1650, train/grad_norm: 10.959559440612793\n",
      "Step: 1650, train/learning_rate: 3.6272878787713125e-05\n",
      "Step: 1650, train/epoch: 2.745424270629883\n",
      "Step: 1660, train/loss: 0.16349999606609344\n",
      "Step: 1660, train/grad_norm: 8.541830062866211\n",
      "Step: 1660, train/learning_rate: 3.618968548835255e-05\n",
      "Step: 1660, train/epoch: 2.7620632648468018\n",
      "Step: 1670, train/loss: 0.1573999971151352\n",
      "Step: 1670, train/grad_norm: 6.944091320037842\n",
      "Step: 1670, train/learning_rate: 3.610648855101317e-05\n",
      "Step: 1670, train/epoch: 2.7787022590637207\n",
      "Step: 1680, train/loss: 0.12600000202655792\n",
      "Step: 1680, train/grad_norm: 6.960290908813477\n",
      "Step: 1680, train/learning_rate: 3.60232952516526e-05\n",
      "Step: 1680, train/epoch: 2.7953410148620605\n",
      "Step: 1690, train/loss: 0.11029999703168869\n",
      "Step: 1690, train/grad_norm: 5.327929496765137\n",
      "Step: 1690, train/learning_rate: 3.594009831431322e-05\n",
      "Step: 1690, train/epoch: 2.8119800090789795\n",
      "Step: 1700, train/loss: 0.14110000431537628\n",
      "Step: 1700, train/grad_norm: 7.436893463134766\n",
      "Step: 1700, train/learning_rate: 3.5856905014952645e-05\n",
      "Step: 1700, train/epoch: 2.8286190032958984\n",
      "Step: 1710, train/loss: 0.13339999318122864\n",
      "Step: 1710, train/grad_norm: 7.1290998458862305\n",
      "Step: 1710, train/learning_rate: 3.577371171559207e-05\n",
      "Step: 1710, train/epoch: 2.8452579975128174\n",
      "Step: 1720, train/loss: 0.11840000003576279\n",
      "Step: 1720, train/grad_norm: 9.098706245422363\n",
      "Step: 1720, train/learning_rate: 3.569051477825269e-05\n",
      "Step: 1720, train/epoch: 2.8618967533111572\n",
      "Step: 1730, train/loss: 0.11379999667406082\n",
      "Step: 1730, train/grad_norm: 5.16826868057251\n",
      "Step: 1730, train/learning_rate: 3.560732147889212e-05\n",
      "Step: 1730, train/epoch: 2.878535747528076\n",
      "Step: 1740, train/loss: 0.1265999972820282\n",
      "Step: 1740, train/grad_norm: 5.753211498260498\n",
      "Step: 1740, train/learning_rate: 3.5524128179531544e-05\n",
      "Step: 1740, train/epoch: 2.895174741744995\n",
      "Step: 1750, train/loss: 0.20029999315738678\n",
      "Step: 1750, train/grad_norm: 12.571109771728516\n",
      "Step: 1750, train/learning_rate: 3.5440931242192164e-05\n",
      "Step: 1750, train/epoch: 2.911813735961914\n",
      "Step: 1760, train/loss: 0.16220000386238098\n",
      "Step: 1760, train/grad_norm: 4.087470054626465\n",
      "Step: 1760, train/learning_rate: 3.535773794283159e-05\n",
      "Step: 1760, train/epoch: 2.928452491760254\n",
      "Step: 1770, train/loss: 0.13490000367164612\n",
      "Step: 1770, train/grad_norm: 8.1248140335083\n",
      "Step: 1770, train/learning_rate: 3.527454100549221e-05\n",
      "Step: 1770, train/epoch: 2.945091485977173\n",
      "Step: 1780, train/loss: 0.16410000622272491\n",
      "Step: 1780, train/grad_norm: 16.443233489990234\n",
      "Step: 1780, train/learning_rate: 3.519134770613164e-05\n",
      "Step: 1780, train/epoch: 2.961730480194092\n",
      "Step: 1790, train/loss: 0.13600000739097595\n",
      "Step: 1790, train/grad_norm: 4.518067359924316\n",
      "Step: 1790, train/learning_rate: 3.5108154406771064e-05\n",
      "Step: 1790, train/epoch: 2.9783694744110107\n",
      "Step: 1800, train/loss: 0.13339999318122864\n",
      "Step: 1800, train/grad_norm: 6.33749532699585\n",
      "Step: 1800, train/learning_rate: 3.5024957469431683e-05\n",
      "Step: 1800, train/epoch: 2.9950082302093506\n",
      "Step: 1803, eval/loss: 0.3548634350299835\n",
      "Step: 1803, eval/accuracy: 0.8718589544296265\n",
      "Step: 1803, eval/f1: 0.868087112903595\n",
      "Step: 1803, eval/runtime: 81.17990112304688\n",
      "Step: 1803, eval/samples_per_second: 88.72899627685547\n",
      "Step: 1803, eval/steps_per_second: 1.5889999866485596\n",
      "Step: 1803, train/epoch: 3.0\n",
      "Step: 1810, train/loss: 0.1535000056028366\n",
      "Step: 1810, train/grad_norm: 2.437950611114502\n",
      "Step: 1810, train/learning_rate: 3.494176417007111e-05\n",
      "Step: 1810, train/epoch: 3.0116472244262695\n",
      "Step: 1820, train/loss: 0.09610000252723694\n",
      "Step: 1820, train/grad_norm: 0.4850626587867737\n",
      "Step: 1820, train/learning_rate: 3.485856723273173e-05\n",
      "Step: 1820, train/epoch: 3.0282862186431885\n",
      "Step: 1830, train/loss: 0.05999999865889549\n",
      "Step: 1830, train/grad_norm: 9.282732009887695\n",
      "Step: 1830, train/learning_rate: 3.4775373933371156e-05\n",
      "Step: 1830, train/epoch: 3.0449252128601074\n",
      "Step: 1840, train/loss: 0.09030000120401382\n",
      "Step: 1840, train/grad_norm: 14.667524337768555\n",
      "Step: 1840, train/learning_rate: 3.469218063401058e-05\n",
      "Step: 1840, train/epoch: 3.0615639686584473\n",
      "Step: 1850, train/loss: 0.10249999910593033\n",
      "Step: 1850, train/grad_norm: 1.9992427825927734\n",
      "Step: 1850, train/learning_rate: 3.46089836966712e-05\n",
      "Step: 1850, train/epoch: 3.078202962875366\n",
      "Step: 1860, train/loss: 0.05299999937415123\n",
      "Step: 1860, train/grad_norm: 0.4270165264606476\n",
      "Step: 1860, train/learning_rate: 3.452579039731063e-05\n",
      "Step: 1860, train/epoch: 3.094841957092285\n",
      "Step: 1870, train/loss: 0.07909999787807465\n",
      "Step: 1870, train/grad_norm: 3.518293619155884\n",
      "Step: 1870, train/learning_rate: 3.4442597097950056e-05\n",
      "Step: 1870, train/epoch: 3.111480951309204\n",
      "Step: 1880, train/loss: 0.09539999812841415\n",
      "Step: 1880, train/grad_norm: 9.539806365966797\n",
      "Step: 1880, train/learning_rate: 3.4359400160610676e-05\n",
      "Step: 1880, train/epoch: 3.128119707107544\n",
      "Step: 1890, train/loss: 0.09260000288486481\n",
      "Step: 1890, train/grad_norm: 5.164310455322266\n",
      "Step: 1890, train/learning_rate: 3.42762068612501e-05\n",
      "Step: 1890, train/epoch: 3.144758701324463\n",
      "Step: 1900, train/loss: 0.08959999680519104\n",
      "Step: 1900, train/grad_norm: 11.824299812316895\n",
      "Step: 1900, train/learning_rate: 3.419300992391072e-05\n",
      "Step: 1900, train/epoch: 3.161397695541382\n",
      "Step: 1910, train/loss: 0.0835999995470047\n",
      "Step: 1910, train/grad_norm: 8.798039436340332\n",
      "Step: 1910, train/learning_rate: 3.410981662455015e-05\n",
      "Step: 1910, train/epoch: 3.178036689758301\n",
      "Step: 1920, train/loss: 0.07760000228881836\n",
      "Step: 1920, train/grad_norm: 10.696188926696777\n",
      "Step: 1920, train/learning_rate: 3.4026623325189576e-05\n",
      "Step: 1920, train/epoch: 3.1946754455566406\n",
      "Step: 1930, train/loss: 0.11420000344514847\n",
      "Step: 1930, train/grad_norm: 13.026788711547852\n",
      "Step: 1930, train/learning_rate: 3.3943426387850195e-05\n",
      "Step: 1930, train/epoch: 3.2113144397735596\n",
      "Step: 1940, train/loss: 0.052000001072883606\n",
      "Step: 1940, train/grad_norm: 4.6063032150268555\n",
      "Step: 1940, train/learning_rate: 3.386023308848962e-05\n",
      "Step: 1940, train/epoch: 3.2279534339904785\n",
      "Step: 1950, train/loss: 0.09430000185966492\n",
      "Step: 1950, train/grad_norm: 5.64780330657959\n",
      "Step: 1950, train/learning_rate: 3.377703978912905e-05\n",
      "Step: 1950, train/epoch: 3.2445924282073975\n",
      "Step: 1960, train/loss: 0.06830000132322311\n",
      "Step: 1960, train/grad_norm: 16.35171127319336\n",
      "Step: 1960, train/learning_rate: 3.369384285178967e-05\n",
      "Step: 1960, train/epoch: 3.2612311840057373\n",
      "Step: 1970, train/loss: 0.052000001072883606\n",
      "Step: 1970, train/grad_norm: 5.391345024108887\n",
      "Step: 1970, train/learning_rate: 3.3610649552429095e-05\n",
      "Step: 1970, train/epoch: 3.2778701782226562\n",
      "Step: 1980, train/loss: 0.10170000046491623\n",
      "Step: 1980, train/grad_norm: 12.882933616638184\n",
      "Step: 1980, train/learning_rate: 3.3527452615089715e-05\n",
      "Step: 1980, train/epoch: 3.294509172439575\n",
      "Step: 1990, train/loss: 0.09459999948740005\n",
      "Step: 1990, train/grad_norm: 8.32656478881836\n",
      "Step: 1990, train/learning_rate: 3.344425931572914e-05\n",
      "Step: 1990, train/epoch: 3.311148166656494\n",
      "Step: 2000, train/loss: 0.07609999924898148\n",
      "Step: 2000, train/grad_norm: 6.176665306091309\n",
      "Step: 2000, train/learning_rate: 3.336106601636857e-05\n",
      "Step: 2000, train/epoch: 3.327786922454834\n",
      "Step: 2010, train/loss: 0.0746999979019165\n",
      "Step: 2010, train/grad_norm: 5.063423156738281\n",
      "Step: 2010, train/learning_rate: 3.327786907902919e-05\n",
      "Step: 2010, train/epoch: 3.344425916671753\n",
      "Step: 2020, train/loss: 0.07609999924898148\n",
      "Step: 2020, train/grad_norm: 0.7490927577018738\n",
      "Step: 2020, train/learning_rate: 3.3194675779668614e-05\n",
      "Step: 2020, train/epoch: 3.361064910888672\n",
      "Step: 2030, train/loss: 0.06880000233650208\n",
      "Step: 2030, train/grad_norm: 0.5798059701919556\n",
      "Step: 2030, train/learning_rate: 3.311148248030804e-05\n",
      "Step: 2030, train/epoch: 3.377703905105591\n",
      "Step: 2040, train/loss: 0.08030000329017639\n",
      "Step: 2040, train/grad_norm: 6.765964508056641\n",
      "Step: 2040, train/learning_rate: 3.302828554296866e-05\n",
      "Step: 2040, train/epoch: 3.3943426609039307\n",
      "Step: 2050, train/loss: 0.0560000017285347\n",
      "Step: 2050, train/grad_norm: 4.660996913909912\n",
      "Step: 2050, train/learning_rate: 3.294509224360809e-05\n",
      "Step: 2050, train/epoch: 3.4109816551208496\n",
      "Step: 2060, train/loss: 0.07339999824762344\n",
      "Step: 2060, train/grad_norm: 1.2082328796386719\n",
      "Step: 2060, train/learning_rate: 3.286189530626871e-05\n",
      "Step: 2060, train/epoch: 3.4276206493377686\n",
      "Step: 2070, train/loss: 0.06440000236034393\n",
      "Step: 2070, train/grad_norm: 12.066240310668945\n",
      "Step: 2070, train/learning_rate: 3.2778702006908134e-05\n",
      "Step: 2070, train/epoch: 3.4442596435546875\n",
      "Step: 2080, train/loss: 0.044199999421834946\n",
      "Step: 2080, train/grad_norm: 8.076934814453125\n",
      "Step: 2080, train/learning_rate: 3.269550870754756e-05\n",
      "Step: 2080, train/epoch: 3.4608983993530273\n",
      "Step: 2090, train/loss: 0.08380000293254852\n",
      "Step: 2090, train/grad_norm: 19.301860809326172\n",
      "Step: 2090, train/learning_rate: 3.261231177020818e-05\n",
      "Step: 2090, train/epoch: 3.4775373935699463\n",
      "Step: 2100, train/loss: 0.05810000002384186\n",
      "Step: 2100, train/grad_norm: 9.127069473266602\n",
      "Step: 2100, train/learning_rate: 3.252911847084761e-05\n",
      "Step: 2100, train/epoch: 3.4941763877868652\n",
      "Step: 2110, train/loss: 0.14710000157356262\n",
      "Step: 2110, train/grad_norm: 9.0209321975708\n",
      "Step: 2110, train/learning_rate: 3.244592517148703e-05\n",
      "Step: 2110, train/epoch: 3.510815382003784\n",
      "Step: 2120, train/loss: 0.16179999709129333\n",
      "Step: 2120, train/grad_norm: 11.702500343322754\n",
      "Step: 2120, train/learning_rate: 3.236272823414765e-05\n",
      "Step: 2120, train/epoch: 3.527454137802124\n",
      "Step: 2130, train/loss: 0.08980000019073486\n",
      "Step: 2130, train/grad_norm: 8.04411506652832\n",
      "Step: 2130, train/learning_rate: 3.227953493478708e-05\n",
      "Step: 2130, train/epoch: 3.544093132019043\n",
      "Step: 2140, train/loss: 0.05260000005364418\n",
      "Step: 2140, train/grad_norm: 12.265417098999023\n",
      "Step: 2140, train/learning_rate: 3.21963379974477e-05\n",
      "Step: 2140, train/epoch: 3.560732126235962\n",
      "Step: 2150, train/loss: 0.08460000157356262\n",
      "Step: 2150, train/grad_norm: 14.28449535369873\n",
      "Step: 2150, train/learning_rate: 3.2113144698087126e-05\n",
      "Step: 2150, train/epoch: 3.577371120452881\n",
      "Step: 2160, train/loss: 0.052400000393390656\n",
      "Step: 2160, train/grad_norm: 24.353830337524414\n",
      "Step: 2160, train/learning_rate: 3.202995139872655e-05\n",
      "Step: 2160, train/epoch: 3.5940098762512207\n",
      "Step: 2170, train/loss: 0.06840000301599503\n",
      "Step: 2170, train/grad_norm: 5.243220806121826\n",
      "Step: 2170, train/learning_rate: 3.194675446138717e-05\n",
      "Step: 2170, train/epoch: 3.6106488704681396\n",
      "Step: 2180, train/loss: 0.11020000278949738\n",
      "Step: 2180, train/grad_norm: 18.67694854736328\n",
      "Step: 2180, train/learning_rate: 3.18635611620266e-05\n",
      "Step: 2180, train/epoch: 3.6272878646850586\n",
      "Step: 2190, train/loss: 0.0851999968290329\n",
      "Step: 2190, train/grad_norm: 9.500800132751465\n",
      "Step: 2190, train/learning_rate: 3.1780367862666026e-05\n",
      "Step: 2190, train/epoch: 3.6439268589019775\n",
      "Step: 2200, train/loss: 0.10740000009536743\n",
      "Step: 2200, train/grad_norm: 11.787144660949707\n",
      "Step: 2200, train/learning_rate: 3.1697170925326645e-05\n",
      "Step: 2200, train/epoch: 3.6605656147003174\n",
      "Step: 2210, train/loss: 0.093299999833107\n",
      "Step: 2210, train/grad_norm: 17.04511070251465\n",
      "Step: 2210, train/learning_rate: 3.161397762596607e-05\n",
      "Step: 2210, train/epoch: 3.6772046089172363\n",
      "Step: 2220, train/loss: 0.09470000118017197\n",
      "Step: 2220, train/grad_norm: 12.392898559570312\n",
      "Step: 2220, train/learning_rate: 3.153078068862669e-05\n",
      "Step: 2220, train/epoch: 3.6938436031341553\n",
      "Step: 2230, train/loss: 0.1273999959230423\n",
      "Step: 2230, train/grad_norm: 11.474607467651367\n",
      "Step: 2230, train/learning_rate: 3.144758738926612e-05\n",
      "Step: 2230, train/epoch: 3.710482597351074\n",
      "Step: 2240, train/loss: 0.09709999710321426\n",
      "Step: 2240, train/grad_norm: 15.326083183288574\n",
      "Step: 2240, train/learning_rate: 3.1364394089905545e-05\n",
      "Step: 2240, train/epoch: 3.727121353149414\n",
      "Step: 2250, train/loss: 0.11140000075101852\n",
      "Step: 2250, train/grad_norm: 7.772609710693359\n",
      "Step: 2250, train/learning_rate: 3.1281197152566165e-05\n",
      "Step: 2250, train/epoch: 3.743760347366333\n",
      "Step: 2260, train/loss: 0.08160000294446945\n",
      "Step: 2260, train/grad_norm: 1.786596655845642\n",
      "Step: 2260, train/learning_rate: 3.119800385320559e-05\n",
      "Step: 2260, train/epoch: 3.760399341583252\n",
      "Step: 2270, train/loss: 0.10170000046491623\n",
      "Step: 2270, train/grad_norm: 5.63909387588501\n",
      "Step: 2270, train/learning_rate: 3.111480691586621e-05\n",
      "Step: 2270, train/epoch: 3.777038335800171\n",
      "Step: 2280, train/loss: 0.08250000327825546\n",
      "Step: 2280, train/grad_norm: 8.828958511352539\n",
      "Step: 2280, train/learning_rate: 3.103161361650564e-05\n",
      "Step: 2280, train/epoch: 3.7936770915985107\n",
      "Step: 2290, train/loss: 0.07930000126361847\n",
      "Step: 2290, train/grad_norm: 9.954520225524902\n",
      "Step: 2290, train/learning_rate: 3.0948420317145064e-05\n",
      "Step: 2290, train/epoch: 3.8103160858154297\n",
      "Step: 2300, train/loss: 0.07129999995231628\n",
      "Step: 2300, train/grad_norm: 6.844237327575684\n",
      "Step: 2300, train/learning_rate: 3.0865223379805684e-05\n",
      "Step: 2300, train/epoch: 3.8269550800323486\n",
      "Step: 2310, train/loss: 0.06710000336170197\n",
      "Step: 2310, train/grad_norm: 5.420355796813965\n",
      "Step: 2310, train/learning_rate: 3.078203008044511e-05\n",
      "Step: 2310, train/epoch: 3.8435940742492676\n",
      "Step: 2320, train/loss: 0.06520000100135803\n",
      "Step: 2320, train/grad_norm: 20.67892837524414\n",
      "Step: 2320, train/learning_rate: 3.069883678108454e-05\n",
      "Step: 2320, train/epoch: 3.8602328300476074\n",
      "Step: 2330, train/loss: 0.07199999690055847\n",
      "Step: 2330, train/grad_norm: 3.7581329345703125\n",
      "Step: 2330, train/learning_rate: 3.061563984374516e-05\n",
      "Step: 2330, train/epoch: 3.8768718242645264\n",
      "Step: 2340, train/loss: 0.0771000012755394\n",
      "Step: 2340, train/grad_norm: 12.292685508728027\n",
      "Step: 2340, train/learning_rate: 3.0532446544384584e-05\n",
      "Step: 2340, train/epoch: 3.8935108184814453\n",
      "Step: 2350, train/loss: 0.10289999842643738\n",
      "Step: 2350, train/grad_norm: 10.936493873596191\n",
      "Step: 2350, train/learning_rate: 3.0449251426034607e-05\n",
      "Step: 2350, train/epoch: 3.9101498126983643\n",
      "Step: 2360, train/loss: 0.08150000125169754\n",
      "Step: 2360, train/grad_norm: 4.115225791931152\n",
      "Step: 2360, train/learning_rate: 3.036605630768463e-05\n",
      "Step: 2360, train/epoch: 3.926788568496704\n",
      "Step: 2370, train/loss: 0.05640000104904175\n",
      "Step: 2370, train/grad_norm: 12.37936019897461\n",
      "Step: 2370, train/learning_rate: 3.0282861189334653e-05\n",
      "Step: 2370, train/epoch: 3.943427562713623\n",
      "Step: 2380, train/loss: 0.06499999761581421\n",
      "Step: 2380, train/grad_norm: 14.459243774414062\n",
      "Step: 2380, train/learning_rate: 3.019966788997408e-05\n",
      "Step: 2380, train/epoch: 3.960066556930542\n",
      "Step: 2390, train/loss: 0.12630000710487366\n",
      "Step: 2390, train/grad_norm: 10.676506996154785\n",
      "Step: 2390, train/learning_rate: 3.0116472771624103e-05\n",
      "Step: 2390, train/epoch: 3.976705551147461\n",
      "Step: 2400, train/loss: 0.09759999811649323\n",
      "Step: 2400, train/grad_norm: 14.242445945739746\n",
      "Step: 2400, train/learning_rate: 3.0033277653274126e-05\n",
      "Step: 2400, train/epoch: 3.993344306945801\n",
      "Step: 2404, eval/loss: 0.38937002420425415\n",
      "Step: 2404, eval/accuracy: 0.8794946670532227\n",
      "Step: 2404, eval/f1: 0.8753659129142761\n",
      "Step: 2404, eval/runtime: 81.06739807128906\n",
      "Step: 2404, eval/samples_per_second: 88.85199737548828\n",
      "Step: 2404, eval/steps_per_second: 1.590999960899353\n",
      "Step: 2404, train/epoch: 4.0\n",
      "Step: 2410, train/loss: 0.05130000039935112\n",
      "Step: 2410, train/grad_norm: 3.447517156600952\n",
      "Step: 2410, train/learning_rate: 2.995008253492415e-05\n",
      "Step: 2410, train/epoch: 4.009983539581299\n",
      "Step: 2420, train/loss: 0.058400001376867294\n",
      "Step: 2420, train/grad_norm: 9.276147842407227\n",
      "Step: 2420, train/learning_rate: 2.9866889235563576e-05\n",
      "Step: 2420, train/epoch: 4.026622295379639\n",
      "Step: 2430, train/loss: 0.07159999758005142\n",
      "Step: 2430, train/grad_norm: 1.2179757356643677\n",
      "Step: 2430, train/learning_rate: 2.97836941172136e-05\n",
      "Step: 2430, train/epoch: 4.0432610511779785\n",
      "Step: 2440, train/loss: 0.017100000753998756\n",
      "Step: 2440, train/grad_norm: 0.2078753262758255\n",
      "Step: 2440, train/learning_rate: 2.9700498998863623e-05\n",
      "Step: 2440, train/epoch: 4.059900283813477\n",
      "Step: 2450, train/loss: 0.01679999940097332\n",
      "Step: 2450, train/grad_norm: 0.5040892958641052\n",
      "Step: 2450, train/learning_rate: 2.9617303880513646e-05\n",
      "Step: 2450, train/epoch: 4.076539039611816\n",
      "Step: 2460, train/loss: 0.03480000048875809\n",
      "Step: 2460, train/grad_norm: 0.3844872713088989\n",
      "Step: 2460, train/learning_rate: 2.9534110581153072e-05\n",
      "Step: 2460, train/epoch: 4.0931782722473145\n",
      "Step: 2470, train/loss: 0.07980000227689743\n",
      "Step: 2470, train/grad_norm: 3.9254488945007324\n",
      "Step: 2470, train/learning_rate: 2.9450915462803096e-05\n",
      "Step: 2470, train/epoch: 4.109817028045654\n",
      "Step: 2480, train/loss: 0.017899999395012856\n",
      "Step: 2480, train/grad_norm: 7.026580333709717\n",
      "Step: 2480, train/learning_rate: 2.936772034445312e-05\n",
      "Step: 2480, train/epoch: 4.126455783843994\n",
      "Step: 2490, train/loss: 0.0284000001847744\n",
      "Step: 2490, train/grad_norm: 4.939671993255615\n",
      "Step: 2490, train/learning_rate: 2.9284525226103142e-05\n",
      "Step: 2490, train/epoch: 4.143095016479492\n",
      "Step: 2500, train/loss: 0.06970000267028809\n",
      "Step: 2500, train/grad_norm: 0.18734504282474518\n",
      "Step: 2500, train/learning_rate: 2.920133192674257e-05\n",
      "Step: 2500, train/epoch: 4.159733772277832\n",
      "Step: 2510, train/loss: 0.03929999843239784\n",
      "Step: 2510, train/grad_norm: 16.953025817871094\n",
      "Step: 2510, train/learning_rate: 2.9118136808392592e-05\n",
      "Step: 2510, train/epoch: 4.176372528076172\n",
      "Step: 2520, train/loss: 0.0722000002861023\n",
      "Step: 2520, train/grad_norm: 17.568195343017578\n",
      "Step: 2520, train/learning_rate: 2.9034941690042615e-05\n",
      "Step: 2520, train/epoch: 4.19301176071167\n",
      "Step: 2530, train/loss: 0.179299995303154\n",
      "Step: 2530, train/grad_norm: 0.52370685338974\n",
      "Step: 2530, train/learning_rate: 2.8951746571692638e-05\n",
      "Step: 2530, train/epoch: 4.20965051651001\n",
      "Step: 2540, train/loss: 0.0794999971985817\n",
      "Step: 2540, train/grad_norm: 5.43560791015625\n",
      "Step: 2540, train/learning_rate: 2.8868553272332065e-05\n",
      "Step: 2540, train/epoch: 4.226289749145508\n",
      "Step: 2550, train/loss: 0.050700001418590546\n",
      "Step: 2550, train/grad_norm: 8.15192985534668\n",
      "Step: 2550, train/learning_rate: 2.8785358153982088e-05\n",
      "Step: 2550, train/epoch: 4.242928504943848\n",
      "Step: 2560, train/loss: 0.043299999088048935\n",
      "Step: 2560, train/grad_norm: 5.92882776260376\n",
      "Step: 2560, train/learning_rate: 2.870216303563211e-05\n",
      "Step: 2560, train/epoch: 4.2595672607421875\n",
      "Step: 2570, train/loss: 0.031300000846385956\n",
      "Step: 2570, train/grad_norm: 0.2699040472507477\n",
      "Step: 2570, train/learning_rate: 2.8618967917282134e-05\n",
      "Step: 2570, train/epoch: 4.2762064933776855\n",
      "Step: 2580, train/loss: 0.041600000113248825\n",
      "Step: 2580, train/grad_norm: 4.741888046264648\n",
      "Step: 2580, train/learning_rate: 2.853577461792156e-05\n",
      "Step: 2580, train/epoch: 4.292845249176025\n",
      "Step: 2590, train/loss: 0.12720000743865967\n",
      "Step: 2590, train/grad_norm: 2.81003999710083\n",
      "Step: 2590, train/learning_rate: 2.8452579499571584e-05\n",
      "Step: 2590, train/epoch: 4.309484004974365\n",
      "Step: 2600, train/loss: 0.08380000293254852\n",
      "Step: 2600, train/grad_norm: 9.724011421203613\n",
      "Step: 2600, train/learning_rate: 2.8369384381221607e-05\n",
      "Step: 2600, train/epoch: 4.326123237609863\n",
      "Step: 2610, train/loss: 0.03970000147819519\n",
      "Step: 2610, train/grad_norm: 22.477413177490234\n",
      "Step: 2610, train/learning_rate: 2.828618926287163e-05\n",
      "Step: 2610, train/epoch: 4.342761993408203\n",
      "Step: 2620, train/loss: 0.03869999945163727\n",
      "Step: 2620, train/grad_norm: 0.16098010540008545\n",
      "Step: 2620, train/learning_rate: 2.8202994144521654e-05\n",
      "Step: 2620, train/epoch: 4.359401226043701\n",
      "Step: 2630, train/loss: 0.07320000231266022\n",
      "Step: 2630, train/grad_norm: 4.345547199249268\n",
      "Step: 2630, train/learning_rate: 2.811980084516108e-05\n",
      "Step: 2630, train/epoch: 4.376039981842041\n",
      "Step: 2640, train/loss: 0.0989999994635582\n",
      "Step: 2640, train/grad_norm: 15.4393892288208\n",
      "Step: 2640, train/learning_rate: 2.8036605726811104e-05\n",
      "Step: 2640, train/epoch: 4.392678737640381\n",
      "Step: 2650, train/loss: 0.04830000177025795\n",
      "Step: 2650, train/grad_norm: 45.8855094909668\n",
      "Step: 2650, train/learning_rate: 2.7953410608461127e-05\n",
      "Step: 2650, train/epoch: 4.409317970275879\n",
      "Step: 2660, train/loss: 0.045099999755620956\n",
      "Step: 2660, train/grad_norm: 15.299935340881348\n",
      "Step: 2660, train/learning_rate: 2.787021549011115e-05\n",
      "Step: 2660, train/epoch: 4.425956726074219\n",
      "Step: 2670, train/loss: 0.030899999663233757\n",
      "Step: 2670, train/grad_norm: 4.167966842651367\n",
      "Step: 2670, train/learning_rate: 2.7787022190750577e-05\n",
      "Step: 2670, train/epoch: 4.442595481872559\n",
      "Step: 2680, train/loss: 0.06530000269412994\n",
      "Step: 2680, train/grad_norm: 17.26728630065918\n",
      "Step: 2680, train/learning_rate: 2.77038270724006e-05\n",
      "Step: 2680, train/epoch: 4.459234714508057\n",
      "Step: 2690, train/loss: 0.04899999871850014\n",
      "Step: 2690, train/grad_norm: 7.513868808746338\n",
      "Step: 2690, train/learning_rate: 2.7620631954050623e-05\n",
      "Step: 2690, train/epoch: 4.4758734703063965\n",
      "Step: 2700, train/loss: 0.07540000230073929\n",
      "Step: 2700, train/grad_norm: 14.563141822814941\n",
      "Step: 2700, train/learning_rate: 2.7537436835700646e-05\n",
      "Step: 2700, train/epoch: 4.4925127029418945\n",
      "Step: 2710, train/loss: 0.06129999831318855\n",
      "Step: 2710, train/grad_norm: 5.4800004959106445\n",
      "Step: 2710, train/learning_rate: 2.7454243536340073e-05\n",
      "Step: 2710, train/epoch: 4.509151458740234\n",
      "Step: 2720, train/loss: 0.03400000184774399\n",
      "Step: 2720, train/grad_norm: 6.628849029541016\n",
      "Step: 2720, train/learning_rate: 2.7371048417990096e-05\n",
      "Step: 2720, train/epoch: 4.525790214538574\n",
      "Step: 2730, train/loss: 0.06369999796152115\n",
      "Step: 2730, train/grad_norm: 5.869144916534424\n",
      "Step: 2730, train/learning_rate: 2.728785329964012e-05\n",
      "Step: 2730, train/epoch: 4.542429447174072\n",
      "Step: 2740, train/loss: 0.024700000882148743\n",
      "Step: 2740, train/grad_norm: 18.579139709472656\n",
      "Step: 2740, train/learning_rate: 2.7204658181290142e-05\n",
      "Step: 2740, train/epoch: 4.559068202972412\n",
      "Step: 2750, train/loss: 0.08070000261068344\n",
      "Step: 2750, train/grad_norm: 23.66309928894043\n",
      "Step: 2750, train/learning_rate: 2.712146488192957e-05\n",
      "Step: 2750, train/epoch: 4.575706958770752\n",
      "Step: 2760, train/loss: 0.08229999989271164\n",
      "Step: 2760, train/grad_norm: 9.747356414794922\n",
      "Step: 2760, train/learning_rate: 2.7038269763579592e-05\n",
      "Step: 2760, train/epoch: 4.59234619140625\n",
      "Step: 2770, train/loss: 0.06279999762773514\n",
      "Step: 2770, train/grad_norm: 0.4199482500553131\n",
      "Step: 2770, train/learning_rate: 2.6955074645229615e-05\n",
      "Step: 2770, train/epoch: 4.60898494720459\n",
      "Step: 2780, train/loss: 0.037300001829862595\n",
      "Step: 2780, train/grad_norm: 1.0121383666992188\n",
      "Step: 2780, train/learning_rate: 2.687187952687964e-05\n",
      "Step: 2780, train/epoch: 4.625624179840088\n",
      "Step: 2790, train/loss: 0.05429999902844429\n",
      "Step: 2790, train/grad_norm: 29.24915313720703\n",
      "Step: 2790, train/learning_rate: 2.6788686227519065e-05\n",
      "Step: 2790, train/epoch: 4.642262935638428\n",
      "Step: 2800, train/loss: 0.03819999843835831\n",
      "Step: 2800, train/grad_norm: 1.1602522134780884\n",
      "Step: 2800, train/learning_rate: 2.6705491109169088e-05\n",
      "Step: 2800, train/epoch: 4.658901691436768\n",
      "Step: 2810, train/loss: 0.03350000083446503\n",
      "Step: 2810, train/grad_norm: 1.9550139904022217\n",
      "Step: 2810, train/learning_rate: 2.662229599081911e-05\n",
      "Step: 2810, train/epoch: 4.675540924072266\n",
      "Step: 2820, train/loss: 0.09130000323057175\n",
      "Step: 2820, train/grad_norm: 7.00846529006958\n",
      "Step: 2820, train/learning_rate: 2.6539100872469135e-05\n",
      "Step: 2820, train/epoch: 4.6921796798706055\n",
      "Step: 2830, train/loss: 0.012600000016391277\n",
      "Step: 2830, train/grad_norm: 12.717360496520996\n",
      "Step: 2830, train/learning_rate: 2.645590757310856e-05\n",
      "Step: 2830, train/epoch: 4.708818435668945\n",
      "Step: 2840, train/loss: 0.12380000203847885\n",
      "Step: 2840, train/grad_norm: 16.52157974243164\n",
      "Step: 2840, train/learning_rate: 2.6372712454758584e-05\n",
      "Step: 2840, train/epoch: 4.725457668304443\n",
      "Step: 2850, train/loss: 0.1720000058412552\n",
      "Step: 2850, train/grad_norm: 33.69301986694336\n",
      "Step: 2850, train/learning_rate: 2.6289517336408608e-05\n",
      "Step: 2850, train/epoch: 4.742096424102783\n",
      "Step: 2860, train/loss: 0.04450000077486038\n",
      "Step: 2860, train/grad_norm: 10.643756866455078\n",
      "Step: 2860, train/learning_rate: 2.620632221805863e-05\n",
      "Step: 2860, train/epoch: 4.758735656738281\n",
      "Step: 2870, train/loss: 0.008299999870359898\n",
      "Step: 2870, train/grad_norm: 0.09228731691837311\n",
      "Step: 2870, train/learning_rate: 2.6123128918698058e-05\n",
      "Step: 2870, train/epoch: 4.775374412536621\n",
      "Step: 2880, train/loss: 0.03929999843239784\n",
      "Step: 2880, train/grad_norm: 10.616089820861816\n",
      "Step: 2880, train/learning_rate: 2.603993380034808e-05\n",
      "Step: 2880, train/epoch: 4.792013168334961\n",
      "Step: 2890, train/loss: 0.038600001484155655\n",
      "Step: 2890, train/grad_norm: 12.726758003234863\n",
      "Step: 2890, train/learning_rate: 2.5956738681998104e-05\n",
      "Step: 2890, train/epoch: 4.808652400970459\n",
      "Step: 2900, train/loss: 0.05939999967813492\n",
      "Step: 2900, train/grad_norm: 0.42228618264198303\n",
      "Step: 2900, train/learning_rate: 2.5873543563648127e-05\n",
      "Step: 2900, train/epoch: 4.825291156768799\n",
      "Step: 2910, train/loss: 0.061500001698732376\n",
      "Step: 2910, train/grad_norm: 0.14031106233596802\n",
      "Step: 2910, train/learning_rate: 2.5790350264287554e-05\n",
      "Step: 2910, train/epoch: 4.841929912567139\n",
      "Step: 2920, train/loss: 0.061500001698732376\n",
      "Step: 2920, train/grad_norm: 2.0670042037963867\n",
      "Step: 2920, train/learning_rate: 2.5707155145937577e-05\n",
      "Step: 2920, train/epoch: 4.858569145202637\n",
      "Step: 2930, train/loss: 0.07670000195503235\n",
      "Step: 2930, train/grad_norm: 17.30023956298828\n",
      "Step: 2930, train/learning_rate: 2.56239600275876e-05\n",
      "Step: 2930, train/epoch: 4.875207901000977\n",
      "Step: 2940, train/loss: 0.04879999905824661\n",
      "Step: 2940, train/grad_norm: 5.364220142364502\n",
      "Step: 2940, train/learning_rate: 2.5540764909237623e-05\n",
      "Step: 2940, train/epoch: 4.891847133636475\n",
      "Step: 2950, train/loss: 0.09730000048875809\n",
      "Step: 2950, train/grad_norm: 8.647360801696777\n",
      "Step: 2950, train/learning_rate: 2.545757160987705e-05\n",
      "Step: 2950, train/epoch: 4.9084858894348145\n",
      "Step: 2960, train/loss: 0.06430000066757202\n",
      "Step: 2960, train/grad_norm: 9.330982208251953\n",
      "Step: 2960, train/learning_rate: 2.5374376491527073e-05\n",
      "Step: 2960, train/epoch: 4.925124645233154\n",
      "Step: 2970, train/loss: 0.039400000125169754\n",
      "Step: 2970, train/grad_norm: 1.12380051612854\n",
      "Step: 2970, train/learning_rate: 2.5291181373177096e-05\n",
      "Step: 2970, train/epoch: 4.941763877868652\n",
      "Step: 2980, train/loss: 0.05620000138878822\n",
      "Step: 2980, train/grad_norm: 2.1921820640563965\n",
      "Step: 2980, train/learning_rate: 2.520798625482712e-05\n",
      "Step: 2980, train/epoch: 4.958402633666992\n",
      "Step: 2990, train/loss: 0.06239999830722809\n",
      "Step: 2990, train/grad_norm: 0.38474273681640625\n",
      "Step: 2990, train/learning_rate: 2.5124791136477143e-05\n",
      "Step: 2990, train/epoch: 4.975041389465332\n",
      "Step: 3000, train/loss: 0.02250000089406967\n",
      "Step: 3000, train/grad_norm: 0.8806288838386536\n",
      "Step: 3000, train/learning_rate: 2.504159783711657e-05\n",
      "Step: 3000, train/epoch: 4.99168062210083\n",
      "Step: 3005, eval/loss: 0.5722144842147827\n",
      "Step: 3005, eval/accuracy: 0.880050003528595\n",
      "Step: 3005, eval/f1: 0.8730183839797974\n",
      "Step: 3005, eval/runtime: 81.10649871826172\n",
      "Step: 3005, eval/samples_per_second: 88.80899810791016\n",
      "Step: 3005, eval/steps_per_second: 1.590999960899353\n",
      "Step: 3005, train/epoch: 5.0\n",
      "Step: 3010, train/loss: 0.055399999022483826\n",
      "Step: 3010, train/grad_norm: 7.092804908752441\n",
      "Step: 3010, train/learning_rate: 2.4958402718766592e-05\n",
      "Step: 3010, train/epoch: 5.00831937789917\n",
      "Step: 3020, train/loss: 0.037700001150369644\n",
      "Step: 3020, train/grad_norm: 17.183557510375977\n",
      "Step: 3020, train/learning_rate: 2.4875207600416616e-05\n",
      "Step: 3020, train/epoch: 5.024958610534668\n",
      "Step: 3030, train/loss: 0.005499999970197678\n",
      "Step: 3030, train/grad_norm: 1.8032000064849854\n",
      "Step: 3030, train/learning_rate: 2.479201248206664e-05\n",
      "Step: 3030, train/epoch: 5.041597366333008\n",
      "Step: 3040, train/loss: 0.012900000438094139\n",
      "Step: 3040, train/grad_norm: 0.16924835741519928\n",
      "Step: 3040, train/learning_rate: 2.4708819182706065e-05\n",
      "Step: 3040, train/epoch: 5.058236122131348\n",
      "Step: 3050, train/loss: 0.061000000685453415\n",
      "Step: 3050, train/grad_norm: 7.366444110870361\n",
      "Step: 3050, train/learning_rate: 2.462562406435609e-05\n",
      "Step: 3050, train/epoch: 5.074875354766846\n",
      "Step: 3060, train/loss: 0.032600000500679016\n",
      "Step: 3060, train/grad_norm: 0.03429386019706726\n",
      "Step: 3060, train/learning_rate: 2.4542428946006112e-05\n",
      "Step: 3060, train/epoch: 5.0915141105651855\n",
      "Step: 3070, train/loss: 0.05779999867081642\n",
      "Step: 3070, train/grad_norm: 36.17290496826172\n",
      "Step: 3070, train/learning_rate: 2.4459233827656135e-05\n",
      "Step: 3070, train/epoch: 5.108152866363525\n",
      "Step: 3080, train/loss: 0.03790000081062317\n",
      "Step: 3080, train/grad_norm: 0.3543195426464081\n",
      "Step: 3080, train/learning_rate: 2.437604052829556e-05\n",
      "Step: 3080, train/epoch: 5.124792098999023\n",
      "Step: 3090, train/loss: 0.009399999864399433\n",
      "Step: 3090, train/grad_norm: 0.14652599394321442\n",
      "Step: 3090, train/learning_rate: 2.4292845409945585e-05\n",
      "Step: 3090, train/epoch: 5.141430854797363\n",
      "Step: 3100, train/loss: 0.02710000053048134\n",
      "Step: 3100, train/grad_norm: 0.09012027084827423\n",
      "Step: 3100, train/learning_rate: 2.4209650291595608e-05\n",
      "Step: 3100, train/epoch: 5.158070087432861\n",
      "Step: 3110, train/loss: 0.05429999902844429\n",
      "Step: 3110, train/grad_norm: 2.1605064868927\n",
      "Step: 3110, train/learning_rate: 2.412645517324563e-05\n",
      "Step: 3110, train/epoch: 5.174708843231201\n",
      "Step: 3120, train/loss: 0.04830000177025795\n",
      "Step: 3120, train/grad_norm: 12.763079643249512\n",
      "Step: 3120, train/learning_rate: 2.4043261873885058e-05\n",
      "Step: 3120, train/epoch: 5.191347599029541\n",
      "Step: 3130, train/loss: 0.029400000348687172\n",
      "Step: 3130, train/grad_norm: 17.73719596862793\n",
      "Step: 3130, train/learning_rate: 2.396006675553508e-05\n",
      "Step: 3130, train/epoch: 5.207986831665039\n",
      "Step: 3140, train/loss: 0.020099999383091927\n",
      "Step: 3140, train/grad_norm: 0.24835804104804993\n",
      "Step: 3140, train/learning_rate: 2.3876871637185104e-05\n",
      "Step: 3140, train/epoch: 5.224625587463379\n",
      "Step: 3150, train/loss: 0.05920000001788139\n",
      "Step: 3150, train/grad_norm: 1.1416692733764648\n",
      "Step: 3150, train/learning_rate: 2.3793676518835127e-05\n",
      "Step: 3150, train/epoch: 5.241264343261719\n",
      "Step: 3160, train/loss: 0.021800000220537186\n",
      "Step: 3160, train/grad_norm: 0.4185327887535095\n",
      "Step: 3160, train/learning_rate: 2.3710483219474554e-05\n",
      "Step: 3160, train/epoch: 5.257903575897217\n",
      "Step: 3170, train/loss: 0.03720000013709068\n",
      "Step: 3170, train/grad_norm: 0.005192562006413937\n",
      "Step: 3170, train/learning_rate: 2.3627288101124577e-05\n",
      "Step: 3170, train/epoch: 5.274542331695557\n",
      "Step: 3180, train/loss: 0.03400000184774399\n",
      "Step: 3180, train/grad_norm: 2.649718999862671\n",
      "Step: 3180, train/learning_rate: 2.35440929827746e-05\n",
      "Step: 3180, train/epoch: 5.291181564331055\n",
      "Step: 3190, train/loss: 0.031099999323487282\n",
      "Step: 3190, train/grad_norm: 0.4385373294353485\n",
      "Step: 3190, train/learning_rate: 2.3460897864424624e-05\n",
      "Step: 3190, train/epoch: 5.3078203201293945\n",
      "Step: 3200, train/loss: 0.016300000250339508\n",
      "Step: 3200, train/grad_norm: 7.741015434265137\n",
      "Step: 3200, train/learning_rate: 2.337770456506405e-05\n",
      "Step: 3200, train/epoch: 5.324459075927734\n",
      "Step: 3210, train/loss: 0.03629999980330467\n",
      "Step: 3210, train/grad_norm: 45.70699691772461\n",
      "Step: 3210, train/learning_rate: 2.3294509446714073e-05\n",
      "Step: 3210, train/epoch: 5.341098308563232\n",
      "Step: 3220, train/loss: 0.0722000002861023\n",
      "Step: 3220, train/grad_norm: 2.3938729763031006\n",
      "Step: 3220, train/learning_rate: 2.3211314328364097e-05\n",
      "Step: 3220, train/epoch: 5.357737064361572\n",
      "Step: 3230, train/loss: 0.015599999576807022\n",
      "Step: 3230, train/grad_norm: 9.136673927307129\n",
      "Step: 3230, train/learning_rate: 2.312811921001412e-05\n",
      "Step: 3230, train/epoch: 5.374375820159912\n",
      "Step: 3240, train/loss: 0.024800000712275505\n",
      "Step: 3240, train/grad_norm: 0.5807300209999084\n",
      "Step: 3240, train/learning_rate: 2.3044925910653546e-05\n",
      "Step: 3240, train/epoch: 5.39101505279541\n",
      "Step: 3250, train/loss: 0.023900000378489494\n",
      "Step: 3250, train/grad_norm: 0.6831191778182983\n",
      "Step: 3250, train/learning_rate: 2.296173079230357e-05\n",
      "Step: 3250, train/epoch: 5.40765380859375\n",
      "Step: 3260, train/loss: 0.04529999941587448\n",
      "Step: 3260, train/grad_norm: 1.4530069828033447\n",
      "Step: 3260, train/learning_rate: 2.2878535673953593e-05\n",
      "Step: 3260, train/epoch: 5.424293041229248\n",
      "Step: 3270, train/loss: 0.030400000512599945\n",
      "Step: 3270, train/grad_norm: 15.548925399780273\n",
      "Step: 3270, train/learning_rate: 2.2795340555603616e-05\n",
      "Step: 3270, train/epoch: 5.440931797027588\n",
      "Step: 3280, train/loss: 0.058400001376867294\n",
      "Step: 3280, train/grad_norm: 1.520599365234375\n",
      "Step: 3280, train/learning_rate: 2.2712147256243043e-05\n",
      "Step: 3280, train/epoch: 5.457570552825928\n",
      "Step: 3290, train/loss: 0.0333000011742115\n",
      "Step: 3290, train/grad_norm: 10.386768341064453\n",
      "Step: 3290, train/learning_rate: 2.2628952137893066e-05\n",
      "Step: 3290, train/epoch: 5.474209785461426\n",
      "Step: 3300, train/loss: 0.03739999979734421\n",
      "Step: 3300, train/grad_norm: 12.099453926086426\n",
      "Step: 3300, train/learning_rate: 2.254575701954309e-05\n",
      "Step: 3300, train/epoch: 5.490848541259766\n",
      "Step: 3310, train/loss: 0.02539999969303608\n",
      "Step: 3310, train/grad_norm: 0.21891571581363678\n",
      "Step: 3310, train/learning_rate: 2.2462561901193112e-05\n",
      "Step: 3310, train/epoch: 5.5074872970581055\n",
      "Step: 3320, train/loss: 0.0333000011742115\n",
      "Step: 3320, train/grad_norm: 0.3476734459400177\n",
      "Step: 3320, train/learning_rate: 2.237936860183254e-05\n",
      "Step: 3320, train/epoch: 5.5241265296936035\n",
      "Step: 3330, train/loss: 0.07620000094175339\n",
      "Step: 3330, train/grad_norm: 1.9191508293151855\n",
      "Step: 3330, train/learning_rate: 2.2296173483482562e-05\n",
      "Step: 3330, train/epoch: 5.540765285491943\n",
      "Step: 3340, train/loss: 0.04430000111460686\n",
      "Step: 3340, train/grad_norm: 1.4123905897140503\n",
      "Step: 3340, train/learning_rate: 2.2212978365132585e-05\n",
      "Step: 3340, train/epoch: 5.557404518127441\n",
      "Step: 3350, train/loss: 0.07159999758005142\n",
      "Step: 3350, train/grad_norm: 13.405801773071289\n",
      "Step: 3350, train/learning_rate: 2.212978324678261e-05\n",
      "Step: 3350, train/epoch: 5.574043273925781\n",
      "Step: 3360, train/loss: 0.02319999970495701\n",
      "Step: 3360, train/grad_norm: 0.16086892783641815\n",
      "Step: 3360, train/learning_rate: 2.204658812843263e-05\n",
      "Step: 3360, train/epoch: 5.590682029724121\n",
      "Step: 3370, train/loss: 0.025599999353289604\n",
      "Step: 3370, train/grad_norm: 0.09366577118635178\n",
      "Step: 3370, train/learning_rate: 2.1963394829072058e-05\n",
      "Step: 3370, train/epoch: 5.607321262359619\n",
      "Step: 3380, train/loss: 0.01209999993443489\n",
      "Step: 3380, train/grad_norm: 0.2854655981063843\n",
      "Step: 3380, train/learning_rate: 2.188019971072208e-05\n",
      "Step: 3380, train/epoch: 5.623960018157959\n",
      "Step: 3390, train/loss: 0.02630000002682209\n",
      "Step: 3390, train/grad_norm: 0.37782642245292664\n",
      "Step: 3390, train/learning_rate: 2.1797004592372105e-05\n",
      "Step: 3390, train/epoch: 5.640598773956299\n",
      "Step: 3400, train/loss: 0.017100000753998756\n",
      "Step: 3400, train/grad_norm: 0.02474047802388668\n",
      "Step: 3400, train/learning_rate: 2.1713809474022128e-05\n",
      "Step: 3400, train/epoch: 5.657238006591797\n",
      "Step: 3410, train/loss: 0.050700001418590546\n",
      "Step: 3410, train/grad_norm: 0.038173239678144455\n",
      "Step: 3410, train/learning_rate: 2.1630616174661554e-05\n",
      "Step: 3410, train/epoch: 5.673876762390137\n",
      "Step: 3420, train/loss: 0.037700001150369644\n",
      "Step: 3420, train/grad_norm: 7.849737644195557\n",
      "Step: 3420, train/learning_rate: 2.1547421056311578e-05\n",
      "Step: 3420, train/epoch: 5.690515995025635\n",
      "Step: 3430, train/loss: 0.02459999918937683\n",
      "Step: 3430, train/grad_norm: 0.9175323247909546\n",
      "Step: 3430, train/learning_rate: 2.14642259379616e-05\n",
      "Step: 3430, train/epoch: 5.707154750823975\n",
      "Step: 3440, train/loss: 0.08060000091791153\n",
      "Step: 3440, train/grad_norm: 0.058921508491039276\n",
      "Step: 3440, train/learning_rate: 2.1381030819611624e-05\n",
      "Step: 3440, train/epoch: 5.7237935066223145\n",
      "Step: 3450, train/loss: 0.02590000070631504\n",
      "Step: 3450, train/grad_norm: 1.0085270404815674\n",
      "Step: 3450, train/learning_rate: 2.129783752025105e-05\n",
      "Step: 3450, train/epoch: 5.7404327392578125\n",
      "Step: 3460, train/loss: 0.03819999843835831\n",
      "Step: 3460, train/grad_norm: 16.730295181274414\n",
      "Step: 3460, train/learning_rate: 2.1214642401901074e-05\n",
      "Step: 3460, train/epoch: 5.757071495056152\n",
      "Step: 3470, train/loss: 0.019200000911951065\n",
      "Step: 3470, train/grad_norm: 0.510898768901825\n",
      "Step: 3470, train/learning_rate: 2.1131447283551097e-05\n",
      "Step: 3470, train/epoch: 5.773710250854492\n",
      "Step: 3480, train/loss: 0.011900000274181366\n",
      "Step: 3480, train/grad_norm: 15.622051239013672\n",
      "Step: 3480, train/learning_rate: 2.104825216520112e-05\n",
      "Step: 3480, train/epoch: 5.79034948348999\n",
      "Step: 3490, train/loss: 0.01080000028014183\n",
      "Step: 3490, train/grad_norm: 0.11466550081968307\n",
      "Step: 3490, train/learning_rate: 2.0965058865840547e-05\n",
      "Step: 3490, train/epoch: 5.80698823928833\n",
      "Step: 3500, train/loss: 0.045899998396635056\n",
      "Step: 3500, train/grad_norm: 14.622668266296387\n",
      "Step: 3500, train/learning_rate: 2.088186374749057e-05\n",
      "Step: 3500, train/epoch: 5.823627471923828\n",
      "Step: 3510, train/loss: 0.02160000056028366\n",
      "Step: 3510, train/grad_norm: 5.639919281005859\n",
      "Step: 3510, train/learning_rate: 2.0798668629140593e-05\n",
      "Step: 3510, train/epoch: 5.840266227722168\n",
      "Step: 3520, train/loss: 0.008200000040233135\n",
      "Step: 3520, train/grad_norm: 0.008753311820328236\n",
      "Step: 3520, train/learning_rate: 2.0715473510790616e-05\n",
      "Step: 3520, train/epoch: 5.856904983520508\n",
      "Step: 3530, train/loss: 0.01850000023841858\n",
      "Step: 3530, train/grad_norm: 13.677420616149902\n",
      "Step: 3530, train/learning_rate: 2.0632280211430043e-05\n",
      "Step: 3530, train/epoch: 5.873544216156006\n",
      "Step: 3540, train/loss: 0.03310000151395798\n",
      "Step: 3540, train/grad_norm: 32.220787048339844\n",
      "Step: 3540, train/learning_rate: 2.0549085093080066e-05\n",
      "Step: 3540, train/epoch: 5.890182971954346\n",
      "Step: 3550, train/loss: 0.029200000688433647\n",
      "Step: 3550, train/grad_norm: 0.5600741505622864\n",
      "Step: 3550, train/learning_rate: 2.046588997473009e-05\n",
      "Step: 3550, train/epoch: 5.9068217277526855\n",
      "Step: 3560, train/loss: 0.015200000256299973\n",
      "Step: 3560, train/grad_norm: 0.0493905209004879\n",
      "Step: 3560, train/learning_rate: 2.0382694856380112e-05\n",
      "Step: 3560, train/epoch: 5.923460960388184\n",
      "Step: 3570, train/loss: 0.041200000792741776\n",
      "Step: 3570, train/grad_norm: 0.2663241922855377\n",
      "Step: 3570, train/learning_rate: 2.029950155701954e-05\n",
      "Step: 3570, train/epoch: 5.940099716186523\n",
      "Step: 3580, train/loss: 0.03460000082850456\n",
      "Step: 3580, train/grad_norm: 0.03438340872526169\n",
      "Step: 3580, train/learning_rate: 2.0216306438669562e-05\n",
      "Step: 3580, train/epoch: 5.9567389488220215\n",
      "Step: 3590, train/loss: 0.039400000125169754\n",
      "Step: 3590, train/grad_norm: 0.27493566274642944\n",
      "Step: 3590, train/learning_rate: 2.0133111320319586e-05\n",
      "Step: 3590, train/epoch: 5.973377704620361\n",
      "Step: 3600, train/loss: 0.024900000542402267\n",
      "Step: 3600, train/grad_norm: 0.10963759571313858\n",
      "Step: 3600, train/learning_rate: 2.004991620196961e-05\n",
      "Step: 3600, train/epoch: 5.990016460418701\n",
      "Step: 3606, eval/loss: 0.5636608600616455\n",
      "Step: 3606, eval/accuracy: 0.889768123626709\n",
      "Step: 3606, eval/f1: 0.8839940428733826\n",
      "Step: 3606, eval/runtime: 81.11009979248047\n",
      "Step: 3606, eval/samples_per_second: 88.80500030517578\n",
      "Step: 3606, eval/steps_per_second: 1.590000033378601\n",
      "Step: 3606, train/epoch: 6.0\n",
      "Step: 3610, train/loss: 0.02500000037252903\n",
      "Step: 3610, train/grad_norm: 0.04549916461110115\n",
      "Step: 3610, train/learning_rate: 1.9966722902609035e-05\n",
      "Step: 3610, train/epoch: 6.006655693054199\n",
      "Step: 3620, train/loss: 0.009399999864399433\n",
      "Step: 3620, train/grad_norm: 0.7672474980354309\n",
      "Step: 3620, train/learning_rate: 1.988352778425906e-05\n",
      "Step: 3620, train/epoch: 6.023294448852539\n",
      "Step: 3630, train/loss: 0.01860000006854534\n",
      "Step: 3630, train/grad_norm: 0.025286510586738586\n",
      "Step: 3630, train/learning_rate: 1.9800332665909082e-05\n",
      "Step: 3630, train/epoch: 6.039933681488037\n",
      "Step: 3640, train/loss: 0.01489999983459711\n",
      "Step: 3640, train/grad_norm: 7.867234230041504\n",
      "Step: 3640, train/learning_rate: 1.9717137547559105e-05\n",
      "Step: 3640, train/epoch: 6.056572437286377\n",
      "Step: 3650, train/loss: 0.013899999670684338\n",
      "Step: 3650, train/grad_norm: 0.00726315239444375\n",
      "Step: 3650, train/learning_rate: 1.963394424819853e-05\n",
      "Step: 3650, train/epoch: 6.073211193084717\n",
      "Step: 3660, train/loss: 0.006099999882280827\n",
      "Step: 3660, train/grad_norm: 0.006600275635719299\n",
      "Step: 3660, train/learning_rate: 1.9550749129848555e-05\n",
      "Step: 3660, train/epoch: 6.089850425720215\n",
      "Step: 3670, train/loss: 0.008899999782443047\n",
      "Step: 3670, train/grad_norm: 0.23885692656040192\n",
      "Step: 3670, train/learning_rate: 1.9467554011498578e-05\n",
      "Step: 3670, train/epoch: 6.106489181518555\n",
      "Step: 3680, train/loss: 0.02160000056028366\n",
      "Step: 3680, train/grad_norm: 0.0593266524374485\n",
      "Step: 3680, train/learning_rate: 1.93843588931486e-05\n",
      "Step: 3680, train/epoch: 6.1231279373168945\n",
      "Step: 3690, train/loss: 0.020400000736117363\n",
      "Step: 3690, train/grad_norm: 0.2411082237958908\n",
      "Step: 3690, train/learning_rate: 1.9301165593788028e-05\n",
      "Step: 3690, train/epoch: 6.139767169952393\n",
      "Step: 3700, train/loss: 0.014399999752640724\n",
      "Step: 3700, train/grad_norm: 9.871169090270996\n",
      "Step: 3700, train/learning_rate: 1.921797047543805e-05\n",
      "Step: 3700, train/epoch: 6.156405925750732\n",
      "Step: 3710, train/loss: 0.009600000455975533\n",
      "Step: 3710, train/grad_norm: 0.0473724864423275\n",
      "Step: 3710, train/learning_rate: 1.9134775357088074e-05\n",
      "Step: 3710, train/epoch: 6.1730451583862305\n",
      "Step: 3720, train/loss: 0.014999999664723873\n",
      "Step: 3720, train/grad_norm: 0.2824658155441284\n",
      "Step: 3720, train/learning_rate: 1.9051580238738097e-05\n",
      "Step: 3720, train/epoch: 6.18968391418457\n",
      "Step: 3730, train/loss: 0.007600000128149986\n",
      "Step: 3730, train/grad_norm: 0.01041296124458313\n",
      "Step: 3730, train/learning_rate: 1.896838512038812e-05\n",
      "Step: 3730, train/epoch: 6.20632266998291\n",
      "Step: 3740, train/loss: 0.026200000196695328\n",
      "Step: 3740, train/grad_norm: 25.21955680847168\n",
      "Step: 3740, train/learning_rate: 1.8885191821027547e-05\n",
      "Step: 3740, train/epoch: 6.222961902618408\n",
      "Step: 3750, train/loss: 0.017799999564886093\n",
      "Step: 3750, train/grad_norm: 0.23162393271923065\n",
      "Step: 3750, train/learning_rate: 1.880199670267757e-05\n",
      "Step: 3750, train/epoch: 6.239600658416748\n",
      "Step: 3760, train/loss: 0.034299999475479126\n",
      "Step: 3760, train/grad_norm: 0.006765777710825205\n",
      "Step: 3760, train/learning_rate: 1.8718801584327593e-05\n",
      "Step: 3760, train/epoch: 6.256239414215088\n",
      "Step: 3770, train/loss: 0.01549999974668026\n",
      "Step: 3770, train/grad_norm: 0.16579775512218475\n",
      "Step: 3770, train/learning_rate: 1.8635606465977617e-05\n",
      "Step: 3770, train/epoch: 6.272878646850586\n",
      "Step: 3780, train/loss: 0.017899999395012856\n",
      "Step: 3780, train/grad_norm: 0.2396245002746582\n",
      "Step: 3780, train/learning_rate: 1.8552413166617043e-05\n",
      "Step: 3780, train/epoch: 6.289517402648926\n",
      "Step: 3790, train/loss: 0.026100000366568565\n",
      "Step: 3790, train/grad_norm: 1.0884408950805664\n",
      "Step: 3790, train/learning_rate: 1.8469218048267066e-05\n",
      "Step: 3790, train/epoch: 6.306156635284424\n",
      "Step: 3800, train/loss: 0.03680000081658363\n",
      "Step: 3800, train/grad_norm: 17.962003707885742\n",
      "Step: 3800, train/learning_rate: 1.838602292991709e-05\n",
      "Step: 3800, train/epoch: 6.322795391082764\n",
      "Step: 3810, train/loss: 0.051500000059604645\n",
      "Step: 3810, train/grad_norm: 0.40587466955184937\n",
      "Step: 3810, train/learning_rate: 1.8302827811567113e-05\n",
      "Step: 3810, train/epoch: 6.3394341468811035\n",
      "Step: 3820, train/loss: 0.01590000092983246\n",
      "Step: 3820, train/grad_norm: 0.3623347580432892\n",
      "Step: 3820, train/learning_rate: 1.821963451220654e-05\n",
      "Step: 3820, train/epoch: 6.356073379516602\n",
      "Step: 3830, train/loss: 0.03240000084042549\n",
      "Step: 3830, train/grad_norm: 0.8119471073150635\n",
      "Step: 3830, train/learning_rate: 1.8136439393856563e-05\n",
      "Step: 3830, train/epoch: 6.372712135314941\n",
      "Step: 3840, train/loss: 0.004399999976158142\n",
      "Step: 3840, train/grad_norm: 0.4976383447647095\n",
      "Step: 3840, train/learning_rate: 1.8053244275506586e-05\n",
      "Step: 3840, train/epoch: 6.389350891113281\n",
      "Step: 3850, train/loss: 0.03779999911785126\n",
      "Step: 3850, train/grad_norm: 0.03094319999217987\n",
      "Step: 3850, train/learning_rate: 1.797004915715661e-05\n",
      "Step: 3850, train/epoch: 6.405990123748779\n",
      "Step: 3860, train/loss: 0.03689999878406525\n",
      "Step: 3860, train/grad_norm: 16.716588973999023\n",
      "Step: 3860, train/learning_rate: 1.7886855857796036e-05\n",
      "Step: 3860, train/epoch: 6.422628879547119\n",
      "Step: 3870, train/loss: 0.01360000018030405\n",
      "Step: 3870, train/grad_norm: 0.28307658433914185\n",
      "Step: 3870, train/learning_rate: 1.780366073944606e-05\n",
      "Step: 3870, train/epoch: 6.439268112182617\n",
      "Step: 3880, train/loss: 0.014399999752640724\n",
      "Step: 3880, train/grad_norm: 0.40310534834861755\n",
      "Step: 3880, train/learning_rate: 1.7720465621096082e-05\n",
      "Step: 3880, train/epoch: 6.455906867980957\n",
      "Step: 3890, train/loss: 0.000699999975040555\n",
      "Step: 3890, train/grad_norm: 0.14114414155483246\n",
      "Step: 3890, train/learning_rate: 1.7637270502746105e-05\n",
      "Step: 3890, train/epoch: 6.472545623779297\n",
      "Step: 3900, train/loss: 0.023399999365210533\n",
      "Step: 3900, train/grad_norm: 0.4324508309364319\n",
      "Step: 3900, train/learning_rate: 1.7554077203385532e-05\n",
      "Step: 3900, train/epoch: 6.489184856414795\n",
      "Step: 3910, train/loss: 0.000699999975040555\n",
      "Step: 3910, train/grad_norm: 0.13410954177379608\n",
      "Step: 3910, train/learning_rate: 1.7470882085035555e-05\n",
      "Step: 3910, train/epoch: 6.505823612213135\n",
      "Step: 3920, train/loss: 0.029500000178813934\n",
      "Step: 3920, train/grad_norm: 0.005586875136941671\n",
      "Step: 3920, train/learning_rate: 1.7387686966685578e-05\n",
      "Step: 3920, train/epoch: 6.522462368011475\n",
      "Step: 3930, train/loss: 0.014700000174343586\n",
      "Step: 3930, train/grad_norm: 3.2246153354644775\n",
      "Step: 3930, train/learning_rate: 1.73044918483356e-05\n",
      "Step: 3930, train/epoch: 6.539101600646973\n",
      "Step: 3940, train/loss: 0.009499999694526196\n",
      "Step: 3940, train/grad_norm: 0.05539514124393463\n",
      "Step: 3940, train/learning_rate: 1.7221298548975028e-05\n",
      "Step: 3940, train/epoch: 6.5557403564453125\n",
      "Step: 3950, train/loss: 0.010599999688565731\n",
      "Step: 3950, train/grad_norm: 0.13379265367984772\n",
      "Step: 3950, train/learning_rate: 1.713810343062505e-05\n",
      "Step: 3950, train/epoch: 6.5723795890808105\n",
      "Step: 3960, train/loss: 0.009800000116229057\n",
      "Step: 3960, train/grad_norm: 0.023755202069878578\n",
      "Step: 3960, train/learning_rate: 1.7054908312275074e-05\n",
      "Step: 3960, train/epoch: 6.58901834487915\n",
      "Step: 3970, train/loss: 0.007600000128149986\n",
      "Step: 3970, train/grad_norm: 0.12464047968387604\n",
      "Step: 3970, train/learning_rate: 1.6971713193925098e-05\n",
      "Step: 3970, train/epoch: 6.60565710067749\n",
      "Step: 3980, train/loss: 0.010300000198185444\n",
      "Step: 3980, train/grad_norm: 0.027351539582014084\n",
      "Step: 3980, train/learning_rate: 1.6888519894564524e-05\n",
      "Step: 3980, train/epoch: 6.622296333312988\n",
      "Step: 3990, train/loss: 0.017000000923871994\n",
      "Step: 3990, train/grad_norm: 6.316439628601074\n",
      "Step: 3990, train/learning_rate: 1.6805324776214547e-05\n",
      "Step: 3990, train/epoch: 6.638935089111328\n",
      "Step: 4000, train/loss: 0.021700000390410423\n",
      "Step: 4000, train/grad_norm: 0.01431546825915575\n",
      "Step: 4000, train/learning_rate: 1.672212965786457e-05\n",
      "Step: 4000, train/epoch: 6.655573844909668\n",
      "Step: 4010, train/loss: 0.02419999986886978\n",
      "Step: 4010, train/grad_norm: 0.025266008451581\n",
      "Step: 4010, train/learning_rate: 1.6638934539514594e-05\n",
      "Step: 4010, train/epoch: 6.672213077545166\n",
      "Step: 4020, train/loss: 0.005900000222027302\n",
      "Step: 4020, train/grad_norm: 0.03029206581413746\n",
      "Step: 4020, train/learning_rate: 1.655574124015402e-05\n",
      "Step: 4020, train/epoch: 6.688851833343506\n",
      "Step: 4030, train/loss: 0.008299999870359898\n",
      "Step: 4030, train/grad_norm: 0.30096548795700073\n",
      "Step: 4030, train/learning_rate: 1.6472546121804044e-05\n",
      "Step: 4030, train/epoch: 6.705491065979004\n",
      "Step: 4040, train/loss: 0.0013000000035390258\n",
      "Step: 4040, train/grad_norm: 0.01711384393274784\n",
      "Step: 4040, train/learning_rate: 1.6389351003454067e-05\n",
      "Step: 4040, train/epoch: 6.722129821777344\n",
      "Step: 4050, train/loss: 0.007300000172108412\n",
      "Step: 4050, train/grad_norm: 0.05010835826396942\n",
      "Step: 4050, train/learning_rate: 1.630615588510409e-05\n",
      "Step: 4050, train/epoch: 6.738768577575684\n",
      "Step: 4060, train/loss: 0.029999999329447746\n",
      "Step: 4060, train/grad_norm: 1.092182993888855\n",
      "Step: 4060, train/learning_rate: 1.6222962585743517e-05\n",
      "Step: 4060, train/epoch: 6.755407810211182\n",
      "Step: 4070, train/loss: 0.01269999984651804\n",
      "Step: 4070, train/grad_norm: 0.19274142384529114\n",
      "Step: 4070, train/learning_rate: 1.613976746739354e-05\n",
      "Step: 4070, train/epoch: 6.7720465660095215\n",
      "Step: 4080, train/loss: 0.00039999998989515007\n",
      "Step: 4080, train/grad_norm: 0.02807457745075226\n",
      "Step: 4080, train/learning_rate: 1.6056572349043563e-05\n",
      "Step: 4080, train/epoch: 6.788685321807861\n",
      "Step: 4090, train/loss: 0.0006000000284984708\n",
      "Step: 4090, train/grad_norm: 5.649767875671387\n",
      "Step: 4090, train/learning_rate: 1.5973377230693586e-05\n",
      "Step: 4090, train/epoch: 6.805324554443359\n",
      "Step: 4100, train/loss: 0.00019999999494757503\n",
      "Step: 4100, train/grad_norm: 0.07267521321773529\n",
      "Step: 4100, train/learning_rate: 1.5890183931333013e-05\n",
      "Step: 4100, train/epoch: 6.821963310241699\n",
      "Step: 4110, train/loss: 0.00039999998989515007\n",
      "Step: 4110, train/grad_norm: 0.041070617735385895\n",
      "Step: 4110, train/learning_rate: 1.5806988812983036e-05\n",
      "Step: 4110, train/epoch: 6.838602542877197\n",
      "Step: 4120, train/loss: 0.0012000000569969416\n",
      "Step: 4120, train/grad_norm: 0.09609202295541763\n",
      "Step: 4120, train/learning_rate: 1.572379369463306e-05\n",
      "Step: 4120, train/epoch: 6.855241298675537\n",
      "Step: 4130, train/loss: 0.010300000198185444\n",
      "Step: 4130, train/grad_norm: 0.049761999398469925\n",
      "Step: 4130, train/learning_rate: 1.5640598576283082e-05\n",
      "Step: 4130, train/epoch: 6.871880054473877\n",
      "Step: 4140, train/loss: 0.014000000432133675\n",
      "Step: 4140, train/grad_norm: 0.0375966913998127\n",
      "Step: 4140, train/learning_rate: 1.5557403457933106e-05\n",
      "Step: 4140, train/epoch: 6.888519287109375\n",
      "Step: 4150, train/loss: 0.06400000303983688\n",
      "Step: 4150, train/grad_norm: 0.004011110868304968\n",
      "Step: 4150, train/learning_rate: 1.5474210158572532e-05\n",
      "Step: 4150, train/epoch: 6.905158042907715\n",
      "Step: 4160, train/loss: 0.010400000028312206\n",
      "Step: 4160, train/grad_norm: 0.023825882002711296\n",
      "Step: 4160, train/learning_rate: 1.5391015040222555e-05\n",
      "Step: 4160, train/epoch: 6.921796798706055\n",
      "Step: 4170, train/loss: 0.03460000082850456\n",
      "Step: 4170, train/grad_norm: 0.014390223659574986\n",
      "Step: 4170, train/learning_rate: 1.530781992187258e-05\n",
      "Step: 4170, train/epoch: 6.938436031341553\n",
      "Step: 4180, train/loss: 0.03819999843835831\n",
      "Step: 4180, train/grad_norm: 26.332195281982422\n",
      "Step: 4180, train/learning_rate: 1.5224625713017303e-05\n",
      "Step: 4180, train/epoch: 6.955074787139893\n",
      "Step: 4190, train/loss: 0.05339999869465828\n",
      "Step: 4190, train/grad_norm: 0.12326736003160477\n",
      "Step: 4190, train/learning_rate: 1.5141430594667327e-05\n",
      "Step: 4190, train/epoch: 6.971714019775391\n",
      "Step: 4200, train/loss: 0.05590000003576279\n",
      "Step: 4200, train/grad_norm: 0.07224670052528381\n",
      "Step: 4200, train/learning_rate: 1.5058236385812052e-05\n",
      "Step: 4200, train/epoch: 6.9883527755737305\n",
      "Step: 4207, eval/loss: 0.67491215467453\n",
      "Step: 4207, eval/accuracy: 0.8904622793197632\n",
      "Step: 4207, eval/f1: 0.8831835389137268\n",
      "Step: 4207, eval/runtime: 81.10880279541016\n",
      "Step: 4207, eval/samples_per_second: 88.80699920654297\n",
      "Step: 4207, eval/steps_per_second: 1.590000033378601\n",
      "Step: 4207, train/epoch: 7.0\n",
      "Step: 4210, train/loss: 0.008200000040233135\n",
      "Step: 4210, train/grad_norm: 0.35335418581962585\n",
      "Step: 4210, train/learning_rate: 1.4975041267462075e-05\n",
      "Step: 4210, train/epoch: 7.00499153137207\n",
      "Step: 4220, train/loss: 0.032099999487400055\n",
      "Step: 4220, train/grad_norm: 0.06534174829721451\n",
      "Step: 4220, train/learning_rate: 1.48918470586068e-05\n",
      "Step: 4220, train/epoch: 7.021630764007568\n",
      "Step: 4230, train/loss: 0.003599999938160181\n",
      "Step: 4230, train/grad_norm: 0.023954931646585464\n",
      "Step: 4230, train/learning_rate: 1.4808651940256823e-05\n",
      "Step: 4230, train/epoch: 7.038269519805908\n",
      "Step: 4240, train/loss: 0.007000000216066837\n",
      "Step: 4240, train/grad_norm: 2.27545428276062\n",
      "Step: 4240, train/learning_rate: 1.4725457731401548e-05\n",
      "Step: 4240, train/epoch: 7.054908275604248\n",
      "Step: 4250, train/loss: 0.0005000000237487257\n",
      "Step: 4250, train/grad_norm: 0.030296266078948975\n",
      "Step: 4250, train/learning_rate: 1.4642262613051571e-05\n",
      "Step: 4250, train/epoch: 7.071547508239746\n",
      "Step: 4260, train/loss: 0.016899999231100082\n",
      "Step: 4260, train/grad_norm: 4.904112815856934\n",
      "Step: 4260, train/learning_rate: 1.4559068404196296e-05\n",
      "Step: 4260, train/epoch: 7.088186264038086\n",
      "Step: 4270, train/loss: 0.008299999870359898\n",
      "Step: 4270, train/grad_norm: 0.03604399412870407\n",
      "Step: 4270, train/learning_rate: 1.4475873285846319e-05\n",
      "Step: 4270, train/epoch: 7.104825496673584\n",
      "Step: 4280, train/loss: 0.0010999999940395355\n",
      "Step: 4280, train/grad_norm: 0.007683989591896534\n",
      "Step: 4280, train/learning_rate: 1.4392679076991044e-05\n",
      "Step: 4280, train/epoch: 7.121464252471924\n",
      "Step: 4290, train/loss: 0.010700000450015068\n",
      "Step: 4290, train/grad_norm: 0.0644146129488945\n",
      "Step: 4290, train/learning_rate: 1.4309483958641067e-05\n",
      "Step: 4290, train/epoch: 7.138103008270264\n",
      "Step: 4300, train/loss: 0.02590000070631504\n",
      "Step: 4300, train/grad_norm: 1.0300192832946777\n",
      "Step: 4300, train/learning_rate: 1.4226289749785792e-05\n",
      "Step: 4300, train/epoch: 7.154742240905762\n",
      "Step: 4310, train/loss: 0.0066999997943639755\n",
      "Step: 4310, train/grad_norm: 0.1487082540988922\n",
      "Step: 4310, train/learning_rate: 1.4143094631435815e-05\n",
      "Step: 4310, train/epoch: 7.171380996704102\n",
      "Step: 4320, train/loss: 0.0006000000284984708\n",
      "Step: 4320, train/grad_norm: 0.027013109996914864\n",
      "Step: 4320, train/learning_rate: 1.405990042258054e-05\n",
      "Step: 4320, train/epoch: 7.188019752502441\n",
      "Step: 4330, train/loss: 0.0031999999191612005\n",
      "Step: 4330, train/grad_norm: 29.188852310180664\n",
      "Step: 4330, train/learning_rate: 1.3976705304230563e-05\n",
      "Step: 4330, train/epoch: 7.2046589851379395\n",
      "Step: 4340, train/loss: 0.00039999998989515007\n",
      "Step: 4340, train/grad_norm: 0.01435524970293045\n",
      "Step: 4340, train/learning_rate: 1.3893511095375288e-05\n",
      "Step: 4340, train/epoch: 7.221297740936279\n",
      "Step: 4350, train/loss: 0.0003000000142492354\n",
      "Step: 4350, train/grad_norm: 0.013759803026914597\n",
      "Step: 4350, train/learning_rate: 1.3810315977025311e-05\n",
      "Step: 4350, train/epoch: 7.237936973571777\n",
      "Step: 4360, train/loss: 0.0005000000237487257\n",
      "Step: 4360, train/grad_norm: 0.9617889523506165\n",
      "Step: 4360, train/learning_rate: 1.3727121768170036e-05\n",
      "Step: 4360, train/epoch: 7.254575729370117\n",
      "Step: 4370, train/loss: 0.03869999945163727\n",
      "Step: 4370, train/grad_norm: 0.009158224798738956\n",
      "Step: 4370, train/learning_rate: 1.364392664982006e-05\n",
      "Step: 4370, train/epoch: 7.271214485168457\n",
      "Step: 4380, train/loss: 0.01850000023841858\n",
      "Step: 4380, train/grad_norm: 0.01976369507610798\n",
      "Step: 4380, train/learning_rate: 1.3560732440964784e-05\n",
      "Step: 4380, train/epoch: 7.287853717803955\n",
      "Step: 4390, train/loss: 0.026000000536441803\n",
      "Step: 4390, train/grad_norm: 0.008468523621559143\n",
      "Step: 4390, train/learning_rate: 1.3477537322614808e-05\n",
      "Step: 4390, train/epoch: 7.304492473602295\n",
      "Step: 4400, train/loss: 0.0032999999821186066\n",
      "Step: 4400, train/grad_norm: 0.05240088701248169\n",
      "Step: 4400, train/learning_rate: 1.3394343113759533e-05\n",
      "Step: 4400, train/epoch: 7.321131229400635\n",
      "Step: 4410, train/loss: 0.0066999997943639755\n",
      "Step: 4410, train/grad_norm: 0.0417531356215477\n",
      "Step: 4410, train/learning_rate: 1.3311147995409556e-05\n",
      "Step: 4410, train/epoch: 7.337770462036133\n",
      "Step: 4420, train/loss: 0.01590000092983246\n",
      "Step: 4420, train/grad_norm: 0.14617440104484558\n",
      "Step: 4420, train/learning_rate: 1.322795378655428e-05\n",
      "Step: 4420, train/epoch: 7.354409217834473\n",
      "Step: 4430, train/loss: 0.0003000000142492354\n",
      "Step: 4430, train/grad_norm: 2.9411017894744873\n",
      "Step: 4430, train/learning_rate: 1.3144758668204304e-05\n",
      "Step: 4430, train/epoch: 7.371048450469971\n",
      "Step: 4440, train/loss: 0.006200000178068876\n",
      "Step: 4440, train/grad_norm: 0.0187876857817173\n",
      "Step: 4440, train/learning_rate: 1.3061564459349029e-05\n",
      "Step: 4440, train/epoch: 7.3876872062683105\n",
      "Step: 4450, train/loss: 0.0003000000142492354\n",
      "Step: 4450, train/grad_norm: 0.02908666431903839\n",
      "Step: 4450, train/learning_rate: 1.2978369340999052e-05\n",
      "Step: 4450, train/epoch: 7.40432596206665\n",
      "Step: 4460, train/loss: 0.003599999938160181\n",
      "Step: 4460, train/grad_norm: 55.13603210449219\n",
      "Step: 4460, train/learning_rate: 1.2895175132143777e-05\n",
      "Step: 4460, train/epoch: 7.420965194702148\n",
      "Step: 4470, train/loss: 0.0038999998942017555\n",
      "Step: 4470, train/grad_norm: 0.011950223706662655\n",
      "Step: 4470, train/learning_rate: 1.28119800137938e-05\n",
      "Step: 4470, train/epoch: 7.437603950500488\n",
      "Step: 4480, train/loss: 0.00039999998989515007\n",
      "Step: 4480, train/grad_norm: 0.00854661874473095\n",
      "Step: 4480, train/learning_rate: 1.2728785804938525e-05\n",
      "Step: 4480, train/epoch: 7.454242706298828\n",
      "Step: 4490, train/loss: 0.0071000000461936\n",
      "Step: 4490, train/grad_norm: 0.04813959449529648\n",
      "Step: 4490, train/learning_rate: 1.2645590686588548e-05\n",
      "Step: 4490, train/epoch: 7.470881938934326\n",
      "Step: 4500, train/loss: 0.0005000000237487257\n",
      "Step: 4500, train/grad_norm: 2.329789400100708\n",
      "Step: 4500, train/learning_rate: 1.2562395568238571e-05\n",
      "Step: 4500, train/epoch: 7.487520694732666\n",
      "Step: 4510, train/loss: 0.00019999999494757503\n",
      "Step: 4510, train/grad_norm: 0.34994009137153625\n",
      "Step: 4510, train/learning_rate: 1.2479201359383296e-05\n",
      "Step: 4510, train/epoch: 7.504159927368164\n",
      "Step: 4520, train/loss: 0.00019999999494757503\n",
      "Step: 4520, train/grad_norm: 0.004202873446047306\n",
      "Step: 4520, train/learning_rate: 1.239600624103332e-05\n",
      "Step: 4520, train/epoch: 7.520798683166504\n",
      "Step: 4530, train/loss: 0.0024999999441206455\n",
      "Step: 4530, train/grad_norm: 26.26763916015625\n",
      "Step: 4530, train/learning_rate: 1.2312812032178044e-05\n",
      "Step: 4530, train/epoch: 7.537437438964844\n",
      "Step: 4540, train/loss: 0.00019999999494757503\n",
      "Step: 4540, train/grad_norm: 0.0046302941627800465\n",
      "Step: 4540, train/learning_rate: 1.2229616913828067e-05\n",
      "Step: 4540, train/epoch: 7.554076671600342\n",
      "Step: 4550, train/loss: 0.009600000455975533\n",
      "Step: 4550, train/grad_norm: 0.0021538431756198406\n",
      "Step: 4550, train/learning_rate: 1.2146422704972792e-05\n",
      "Step: 4550, train/epoch: 7.570715427398682\n",
      "Step: 4560, train/loss: 0.005799999926239252\n",
      "Step: 4560, train/grad_norm: 0.007003961130976677\n",
      "Step: 4560, train/learning_rate: 1.2063227586622816e-05\n",
      "Step: 4560, train/epoch: 7.5873541831970215\n",
      "Step: 4570, train/loss: 0.011500000022351742\n",
      "Step: 4570, train/grad_norm: 0.007804005406796932\n",
      "Step: 4570, train/learning_rate: 1.198003337776754e-05\n",
      "Step: 4570, train/epoch: 7.6039934158325195\n",
      "Step: 4580, train/loss: 0.014999999664723873\n",
      "Step: 4580, train/grad_norm: 0.006172650493681431\n",
      "Step: 4580, train/learning_rate: 1.1896838259417564e-05\n",
      "Step: 4580, train/epoch: 7.620632171630859\n",
      "Step: 4590, train/loss: 0.003800000064074993\n",
      "Step: 4590, train/grad_norm: 0.5494760870933533\n",
      "Step: 4590, train/learning_rate: 1.1813644050562289e-05\n",
      "Step: 4590, train/epoch: 7.637271404266357\n",
      "Step: 4600, train/loss: 0.0284000001847744\n",
      "Step: 4600, train/grad_norm: 0.004645978100597858\n",
      "Step: 4600, train/learning_rate: 1.1730448932212312e-05\n",
      "Step: 4600, train/epoch: 7.653910160064697\n",
      "Step: 4610, train/loss: 0.00019999999494757503\n",
      "Step: 4610, train/grad_norm: 0.01151699386537075\n",
      "Step: 4610, train/learning_rate: 1.1647254723357037e-05\n",
      "Step: 4610, train/epoch: 7.670548915863037\n",
      "Step: 4620, train/loss: 0.007899999618530273\n",
      "Step: 4620, train/grad_norm: 0.039025478065013885\n",
      "Step: 4620, train/learning_rate: 1.156405960500706e-05\n",
      "Step: 4620, train/epoch: 7.687188148498535\n",
      "Step: 4630, train/loss: 0.017400000244379044\n",
      "Step: 4630, train/grad_norm: 0.2153169810771942\n",
      "Step: 4630, train/learning_rate: 1.1480865396151785e-05\n",
      "Step: 4630, train/epoch: 7.703826904296875\n",
      "Step: 4640, train/loss: 0.01140000019222498\n",
      "Step: 4640, train/grad_norm: 0.02975740283727646\n",
      "Step: 4640, train/learning_rate: 1.1397670277801808e-05\n",
      "Step: 4640, train/epoch: 7.720465660095215\n",
      "Step: 4650, train/loss: 0.0010999999940395355\n",
      "Step: 4650, train/grad_norm: 0.004442573990672827\n",
      "Step: 4650, train/learning_rate: 1.1314476068946533e-05\n",
      "Step: 4650, train/epoch: 7.737104892730713\n",
      "Step: 4660, train/loss: 9.999999747378752e-05\n",
      "Step: 4660, train/grad_norm: 0.002284660702571273\n",
      "Step: 4660, train/learning_rate: 1.1231280950596556e-05\n",
      "Step: 4660, train/epoch: 7.753743648529053\n",
      "Step: 4670, train/loss: 0.0015999999595806003\n",
      "Step: 4670, train/grad_norm: 14.258423805236816\n",
      "Step: 4670, train/learning_rate: 1.1148086741741281e-05\n",
      "Step: 4670, train/epoch: 7.770382881164551\n",
      "Step: 4680, train/loss: 0.014100000262260437\n",
      "Step: 4680, train/grad_norm: 0.08325791358947754\n",
      "Step: 4680, train/learning_rate: 1.1064891623391304e-05\n",
      "Step: 4680, train/epoch: 7.787021636962891\n",
      "Step: 4690, train/loss: 0.012299999594688416\n",
      "Step: 4690, train/grad_norm: 13.767468452453613\n",
      "Step: 4690, train/learning_rate: 1.0981697414536029e-05\n",
      "Step: 4690, train/epoch: 7.8036603927612305\n",
      "Step: 4700, train/loss: 0.012500000186264515\n",
      "Step: 4700, train/grad_norm: 0.13132759928703308\n",
      "Step: 4700, train/learning_rate: 1.0898502296186052e-05\n",
      "Step: 4700, train/epoch: 7.8202996253967285\n",
      "Step: 4710, train/loss: 0.01850000023841858\n",
      "Step: 4710, train/grad_norm: 0.02989860624074936\n",
      "Step: 4710, train/learning_rate: 1.0815308087330777e-05\n",
      "Step: 4710, train/epoch: 7.836938381195068\n",
      "Step: 4720, train/loss: 0.02539999969303608\n",
      "Step: 4720, train/grad_norm: 0.011695828288793564\n",
      "Step: 4720, train/learning_rate: 1.07321129689808e-05\n",
      "Step: 4720, train/epoch: 7.853577136993408\n",
      "Step: 4730, train/loss: 0.007799999788403511\n",
      "Step: 4730, train/grad_norm: 26.66992950439453\n",
      "Step: 4730, train/learning_rate: 1.0648918760125525e-05\n",
      "Step: 4730, train/epoch: 7.870216369628906\n",
      "Step: 4740, train/loss: 9.999999747378752e-05\n",
      "Step: 4740, train/grad_norm: 0.2163870483636856\n",
      "Step: 4740, train/learning_rate: 1.0565723641775548e-05\n",
      "Step: 4740, train/epoch: 7.886855125427246\n",
      "Step: 4750, train/loss: 0.007600000128149986\n",
      "Step: 4750, train/grad_norm: 0.002608742332085967\n",
      "Step: 4750, train/learning_rate: 1.0482529432920273e-05\n",
      "Step: 4750, train/epoch: 7.903494358062744\n",
      "Step: 4760, train/loss: 0.008100000210106373\n",
      "Step: 4760, train/grad_norm: 0.004402095451951027\n",
      "Step: 4760, train/learning_rate: 1.0399334314570297e-05\n",
      "Step: 4760, train/epoch: 7.920133113861084\n",
      "Step: 4770, train/loss: 0.007499999832361937\n",
      "Step: 4770, train/grad_norm: 7.138491630554199\n",
      "Step: 4770, train/learning_rate: 1.0316140105715021e-05\n",
      "Step: 4770, train/epoch: 7.936771869659424\n",
      "Step: 4780, train/loss: 0.029400000348687172\n",
      "Step: 4780, train/grad_norm: 0.014473005197942257\n",
      "Step: 4780, train/learning_rate: 1.0232944987365045e-05\n",
      "Step: 4780, train/epoch: 7.953411102294922\n",
      "Step: 4790, train/loss: 0.00019999999494757503\n",
      "Step: 4790, train/grad_norm: 0.0030915706884115934\n",
      "Step: 4790, train/learning_rate: 1.014975077850977e-05\n",
      "Step: 4790, train/epoch: 7.970049858093262\n",
      "Step: 4800, train/loss: 0.006899999920278788\n",
      "Step: 4800, train/grad_norm: 20.481393814086914\n",
      "Step: 4800, train/learning_rate: 1.0066555660159793e-05\n",
      "Step: 4800, train/epoch: 7.986688613891602\n",
      "Step: 4808, eval/loss: 0.7884886264801025\n",
      "Step: 4808, eval/accuracy: 0.8904622793197632\n",
      "Step: 4808, eval/f1: 0.8850279450416565\n",
      "Step: 4808, eval/runtime: 81.08799743652344\n",
      "Step: 4808, eval/samples_per_second: 88.8290023803711\n",
      "Step: 4808, eval/steps_per_second: 1.590999960899353\n",
      "Step: 4808, train/epoch: 8.0\n",
      "Step: 4810, train/loss: 0.032999999821186066\n",
      "Step: 4810, train/grad_norm: 0.004983577877283096\n",
      "Step: 4810, train/learning_rate: 9.983361451304518e-06\n",
      "Step: 4810, train/epoch: 8.003327369689941\n",
      "Step: 4820, train/loss: 0.0006000000284984708\n",
      "Step: 4820, train/grad_norm: 0.006941887084394693\n",
      "Step: 4820, train/learning_rate: 9.900166332954541e-06\n",
      "Step: 4820, train/epoch: 8.019967079162598\n",
      "Step: 4830, train/loss: 9.999999747378752e-05\n",
      "Step: 4830, train/grad_norm: 0.005334556568413973\n",
      "Step: 4830, train/learning_rate: 9.816972124099266e-06\n",
      "Step: 4830, train/epoch: 8.036605834960938\n",
      "Step: 4840, train/loss: 0.0017999999690800905\n",
      "Step: 4840, train/grad_norm: 34.300601959228516\n",
      "Step: 4840, train/learning_rate: 9.733777005749289e-06\n",
      "Step: 4840, train/epoch: 8.053244590759277\n",
      "Step: 4850, train/loss: 9.999999747378752e-05\n",
      "Step: 4850, train/grad_norm: 0.0016253840876743197\n",
      "Step: 4850, train/learning_rate: 9.650582796894014e-06\n",
      "Step: 4850, train/epoch: 8.069883346557617\n",
      "Step: 4860, train/loss: 0.005100000184029341\n",
      "Step: 4860, train/grad_norm: 0.01183775532990694\n",
      "Step: 4860, train/learning_rate: 9.567387678544037e-06\n",
      "Step: 4860, train/epoch: 8.086522102355957\n",
      "Step: 4870, train/loss: 9.999999747378752e-05\n",
      "Step: 4870, train/grad_norm: 0.003363472642377019\n",
      "Step: 4870, train/learning_rate: 9.48419256019406e-06\n",
      "Step: 4870, train/epoch: 8.103161811828613\n",
      "Step: 4880, train/loss: 0.007300000172108412\n",
      "Step: 4880, train/grad_norm: 0.006457794923335314\n",
      "Step: 4880, train/learning_rate: 9.400998351338785e-06\n",
      "Step: 4880, train/epoch: 8.119800567626953\n",
      "Step: 4890, train/loss: 9.999999747378752e-05\n",
      "Step: 4890, train/grad_norm: 0.39144039154052734\n",
      "Step: 4890, train/learning_rate: 9.317803232988808e-06\n",
      "Step: 4890, train/epoch: 8.136439323425293\n",
      "Step: 4900, train/loss: 9.999999747378752e-05\n",
      "Step: 4900, train/grad_norm: 0.006889830809086561\n",
      "Step: 4900, train/learning_rate: 9.234609024133533e-06\n",
      "Step: 4900, train/epoch: 8.153078079223633\n",
      "Step: 4910, train/loss: 0.01720000058412552\n",
      "Step: 4910, train/grad_norm: 6.008610248565674\n",
      "Step: 4910, train/learning_rate: 9.151413905783556e-06\n",
      "Step: 4910, train/epoch: 8.169716835021973\n",
      "Step: 4920, train/loss: 9.999999747378752e-05\n",
      "Step: 4920, train/grad_norm: 0.010919100604951382\n",
      "Step: 4920, train/learning_rate: 9.068219696928281e-06\n",
      "Step: 4920, train/epoch: 8.186356544494629\n",
      "Step: 4930, train/loss: 0.006800000090152025\n",
      "Step: 4930, train/grad_norm: 0.006357646081596613\n",
      "Step: 4930, train/learning_rate: 8.985024578578304e-06\n",
      "Step: 4930, train/epoch: 8.202995300292969\n",
      "Step: 4940, train/loss: 0.0044999998062849045\n",
      "Step: 4940, train/grad_norm: 0.015692882239818573\n",
      "Step: 4940, train/learning_rate: 8.90183036972303e-06\n",
      "Step: 4940, train/epoch: 8.219634056091309\n",
      "Step: 4950, train/loss: 9.999999747378752e-05\n",
      "Step: 4950, train/grad_norm: 0.034562379121780396\n",
      "Step: 4950, train/learning_rate: 8.818635251373053e-06\n",
      "Step: 4950, train/epoch: 8.236272811889648\n",
      "Step: 4960, train/loss: 0.002300000051036477\n",
      "Step: 4960, train/grad_norm: 43.647098541259766\n",
      "Step: 4960, train/learning_rate: 8.735441042517778e-06\n",
      "Step: 4960, train/epoch: 8.252911567687988\n",
      "Step: 4970, train/loss: 0.013399999588727951\n",
      "Step: 4970, train/grad_norm: 0.0015661827055737376\n",
      "Step: 4970, train/learning_rate: 8.6522459241678e-06\n",
      "Step: 4970, train/epoch: 8.269550323486328\n",
      "Step: 4980, train/loss: 9.999999747378752e-05\n",
      "Step: 4980, train/grad_norm: 0.022821661084890366\n",
      "Step: 4980, train/learning_rate: 8.569051715312526e-06\n",
      "Step: 4980, train/epoch: 8.286190032958984\n",
      "Step: 4990, train/loss: 0.01140000019222498\n",
      "Step: 4990, train/grad_norm: 17.501148223876953\n",
      "Step: 4990, train/learning_rate: 8.485856596962549e-06\n",
      "Step: 4990, train/epoch: 8.302828788757324\n",
      "Step: 5000, train/loss: 0.009600000455975533\n",
      "Step: 5000, train/grad_norm: 0.031149690970778465\n",
      "Step: 5000, train/learning_rate: 8.402662388107274e-06\n",
      "Step: 5000, train/epoch: 8.319467544555664\n",
      "Step: 5010, train/loss: 9.999999747378752e-05\n",
      "Step: 5010, train/grad_norm: 0.6947500109672546\n",
      "Step: 5010, train/learning_rate: 8.319467269757297e-06\n",
      "Step: 5010, train/epoch: 8.336106300354004\n",
      "Step: 5020, train/loss: 0.003000000026077032\n",
      "Step: 5020, train/grad_norm: 0.002711820648983121\n",
      "Step: 5020, train/learning_rate: 8.236273060902022e-06\n",
      "Step: 5020, train/epoch: 8.352745056152344\n",
      "Step: 5030, train/loss: 0.010300000198185444\n",
      "Step: 5030, train/grad_norm: 0.009295633062720299\n",
      "Step: 5030, train/learning_rate: 8.153077942552045e-06\n",
      "Step: 5030, train/epoch: 8.369384765625\n",
      "Step: 5040, train/loss: 0.010200000368058681\n",
      "Step: 5040, train/grad_norm: 21.85283088684082\n",
      "Step: 5040, train/learning_rate: 8.06988373369677e-06\n",
      "Step: 5040, train/epoch: 8.38602352142334\n",
      "Step: 5050, train/loss: 0.004699999932199717\n",
      "Step: 5050, train/grad_norm: 0.005739618558436632\n",
      "Step: 5050, train/learning_rate: 7.986688615346793e-06\n",
      "Step: 5050, train/epoch: 8.40266227722168\n",
      "Step: 5060, train/loss: 0.013100000098347664\n",
      "Step: 5060, train/grad_norm: 0.002221830189228058\n",
      "Step: 5060, train/learning_rate: 7.903494406491518e-06\n",
      "Step: 5060, train/epoch: 8.41930103302002\n",
      "Step: 5070, train/loss: 0.0\n",
      "Step: 5070, train/grad_norm: 0.003249318804591894\n",
      "Step: 5070, train/learning_rate: 7.820299288141541e-06\n",
      "Step: 5070, train/epoch: 8.43593978881836\n",
      "Step: 5080, train/loss: 9.999999747378752e-05\n",
      "Step: 5080, train/grad_norm: 0.002048980910331011\n",
      "Step: 5080, train/learning_rate: 7.737105079286266e-06\n",
      "Step: 5080, train/epoch: 8.452579498291016\n",
      "Step: 5090, train/loss: 9.999999747378752e-05\n",
      "Step: 5090, train/grad_norm: 0.015033834613859653\n",
      "Step: 5090, train/learning_rate: 7.65390996093629e-06\n",
      "Step: 5090, train/epoch: 8.469218254089355\n",
      "Step: 5100, train/loss: 0.012400000356137753\n",
      "Step: 5100, train/grad_norm: 0.0551857128739357\n",
      "Step: 5100, train/learning_rate: 7.570715297333663e-06\n",
      "Step: 5100, train/epoch: 8.485857009887695\n",
      "Step: 5110, train/loss: 0.011599999852478504\n",
      "Step: 5110, train/grad_norm: 0.011287190020084381\n",
      "Step: 5110, train/learning_rate: 7.487520633731037e-06\n",
      "Step: 5110, train/epoch: 8.502495765686035\n",
      "Step: 5120, train/loss: 0.0\n",
      "Step: 5120, train/grad_norm: 0.01756363734602928\n",
      "Step: 5120, train/learning_rate: 7.4043259701284114e-06\n",
      "Step: 5120, train/epoch: 8.519134521484375\n",
      "Step: 5130, train/loss: 9.999999747378752e-05\n",
      "Step: 5130, train/grad_norm: 0.04946424812078476\n",
      "Step: 5130, train/learning_rate: 7.3211313065257855e-06\n",
      "Step: 5130, train/epoch: 8.535773277282715\n",
      "Step: 5140, train/loss: 9.999999747378752e-05\n",
      "Step: 5140, train/grad_norm: 0.08523938059806824\n",
      "Step: 5140, train/learning_rate: 7.2379366429231595e-06\n",
      "Step: 5140, train/epoch: 8.552412986755371\n",
      "Step: 5150, train/loss: 0.017999999225139618\n",
      "Step: 5150, train/grad_norm: 11.631803512573242\n",
      "Step: 5150, train/learning_rate: 7.1547419793205336e-06\n",
      "Step: 5150, train/epoch: 8.569051742553711\n",
      "Step: 5160, train/loss: 9.999999747378752e-05\n",
      "Step: 5160, train/grad_norm: 0.0014791913563385606\n",
      "Step: 5160, train/learning_rate: 7.071547315717908e-06\n",
      "Step: 5160, train/epoch: 8.58569049835205\n",
      "Step: 5170, train/loss: 0.00019999999494757503\n",
      "Step: 5170, train/grad_norm: 2.130305051803589\n",
      "Step: 5170, train/learning_rate: 6.988352652115282e-06\n",
      "Step: 5170, train/epoch: 8.60232925415039\n",
      "Step: 5180, train/loss: 9.999999747378752e-05\n",
      "Step: 5180, train/grad_norm: 0.02087624929845333\n",
      "Step: 5180, train/learning_rate: 6.905157988512656e-06\n",
      "Step: 5180, train/epoch: 8.61896800994873\n",
      "Step: 5190, train/loss: 0.00930000003427267\n",
      "Step: 5190, train/grad_norm: 0.0008900229586288333\n",
      "Step: 5190, train/learning_rate: 6.82196332491003e-06\n",
      "Step: 5190, train/epoch: 8.635607719421387\n",
      "Step: 5200, train/loss: 0.011900000274181366\n",
      "Step: 5200, train/grad_norm: 0.0010235325898975134\n",
      "Step: 5200, train/learning_rate: 6.738768661307404e-06\n",
      "Step: 5200, train/epoch: 8.652246475219727\n",
      "Step: 5210, train/loss: 9.999999747378752e-05\n",
      "Step: 5210, train/grad_norm: 0.0015965088969096541\n",
      "Step: 5210, train/learning_rate: 6.655573997704778e-06\n",
      "Step: 5210, train/epoch: 8.668885231018066\n",
      "Step: 5220, train/loss: 0.01140000019222498\n",
      "Step: 5220, train/grad_norm: 0.1550074964761734\n",
      "Step: 5220, train/learning_rate: 6.572379334102152e-06\n",
      "Step: 5220, train/epoch: 8.685523986816406\n",
      "Step: 5230, train/loss: 0.00279999990016222\n",
      "Step: 5230, train/grad_norm: 0.0810571014881134\n",
      "Step: 5230, train/learning_rate: 6.489184670499526e-06\n",
      "Step: 5230, train/epoch: 8.702162742614746\n",
      "Step: 5240, train/loss: 0.006099999882280827\n",
      "Step: 5240, train/grad_norm: 0.007162159308791161\n",
      "Step: 5240, train/learning_rate: 6.4059900068969e-06\n",
      "Step: 5240, train/epoch: 8.718802452087402\n",
      "Step: 5250, train/loss: 0.01860000006854534\n",
      "Step: 5250, train/grad_norm: 0.005167597904801369\n",
      "Step: 5250, train/learning_rate: 6.322795343294274e-06\n",
      "Step: 5250, train/epoch: 8.735441207885742\n",
      "Step: 5260, train/loss: 0.005499999970197678\n",
      "Step: 5260, train/grad_norm: 0.003357907757163048\n",
      "Step: 5260, train/learning_rate: 6.239600679691648e-06\n",
      "Step: 5260, train/epoch: 8.752079963684082\n",
      "Step: 5270, train/loss: 0.0020000000949949026\n",
      "Step: 5270, train/grad_norm: 4.629648685455322\n",
      "Step: 5270, train/learning_rate: 6.156406016089022e-06\n",
      "Step: 5270, train/epoch: 8.768718719482422\n",
      "Step: 5280, train/loss: 9.999999747378752e-05\n",
      "Step: 5280, train/grad_norm: 0.001285541569814086\n",
      "Step: 5280, train/learning_rate: 6.073211352486396e-06\n",
      "Step: 5280, train/epoch: 8.785357475280762\n",
      "Step: 5290, train/loss: 0.0052999998442828655\n",
      "Step: 5290, train/grad_norm: 18.984973907470703\n",
      "Step: 5290, train/learning_rate: 5.99001668888377e-06\n",
      "Step: 5290, train/epoch: 8.801996231079102\n",
      "Step: 5300, train/loss: 0.010900000110268593\n",
      "Step: 5300, train/grad_norm: 0.04049037769436836\n",
      "Step: 5300, train/learning_rate: 5.906822025281144e-06\n",
      "Step: 5300, train/epoch: 8.818635940551758\n",
      "Step: 5310, train/loss: 0.010400000028312206\n",
      "Step: 5310, train/grad_norm: 0.01553933322429657\n",
      "Step: 5310, train/learning_rate: 5.823627361678518e-06\n",
      "Step: 5310, train/epoch: 8.835274696350098\n",
      "Step: 5320, train/loss: 0.0038999998942017555\n",
      "Step: 5320, train/grad_norm: 0.00121418631169945\n",
      "Step: 5320, train/learning_rate: 5.740432698075892e-06\n",
      "Step: 5320, train/epoch: 8.851913452148438\n",
      "Step: 5330, train/loss: 9.999999747378752e-05\n",
      "Step: 5330, train/grad_norm: 0.07772713154554367\n",
      "Step: 5330, train/learning_rate: 5.6572380344732665e-06\n",
      "Step: 5330, train/epoch: 8.868552207946777\n",
      "Step: 5340, train/loss: 0.0012000000569969416\n",
      "Step: 5340, train/grad_norm: 0.031004713848233223\n",
      "Step: 5340, train/learning_rate: 5.5740433708706405e-06\n",
      "Step: 5340, train/epoch: 8.885190963745117\n",
      "Step: 5350, train/loss: 9.999999747378752e-05\n",
      "Step: 5350, train/grad_norm: 0.07210826873779297\n",
      "Step: 5350, train/learning_rate: 5.4908487072680146e-06\n",
      "Step: 5350, train/epoch: 8.901830673217773\n",
      "Step: 5360, train/loss: 9.999999747378752e-05\n",
      "Step: 5360, train/grad_norm: 0.2438552975654602\n",
      "Step: 5360, train/learning_rate: 5.407654043665389e-06\n",
      "Step: 5360, train/epoch: 8.918469429016113\n",
      "Step: 5370, train/loss: 9.999999747378752e-05\n",
      "Step: 5370, train/grad_norm: 0.03380156680941582\n",
      "Step: 5370, train/learning_rate: 5.324459380062763e-06\n",
      "Step: 5370, train/epoch: 8.935108184814453\n",
      "Step: 5380, train/loss: 9.999999747378752e-05\n",
      "Step: 5380, train/grad_norm: 0.14224061369895935\n",
      "Step: 5380, train/learning_rate: 5.241264716460137e-06\n",
      "Step: 5380, train/epoch: 8.951746940612793\n",
      "Step: 5390, train/loss: 0.0027000000700354576\n",
      "Step: 5390, train/grad_norm: 6.808193206787109\n",
      "Step: 5390, train/learning_rate: 5.158070052857511e-06\n",
      "Step: 5390, train/epoch: 8.968385696411133\n",
      "Step: 5400, train/loss: 0.017400000244379044\n",
      "Step: 5400, train/grad_norm: 0.0014689938398078084\n",
      "Step: 5400, train/learning_rate: 5.074875389254885e-06\n",
      "Step: 5400, train/epoch: 8.985025405883789\n",
      "Step: 5409, eval/loss: 0.8317801356315613\n",
      "Step: 5409, eval/accuracy: 0.8951825499534607\n",
      "Step: 5409, eval/f1: 0.8900656700134277\n",
      "Step: 5409, eval/runtime: 81.15499877929688\n",
      "Step: 5409, eval/samples_per_second: 88.75599670410156\n",
      "Step: 5409, eval/steps_per_second: 1.590000033378601\n",
      "Step: 5409, train/epoch: 9.0\n",
      "Step: 5410, train/loss: 9.999999747378752e-05\n",
      "Step: 5410, train/grad_norm: 0.0009256581543013453\n",
      "Step: 5410, train/learning_rate: 4.991680725652259e-06\n",
      "Step: 5410, train/epoch: 9.001664161682129\n",
      "Step: 5420, train/loss: 0.00019999999494757503\n",
      "Step: 5420, train/grad_norm: 0.0008397145429626107\n",
      "Step: 5420, train/learning_rate: 4.908486062049633e-06\n",
      "Step: 5420, train/epoch: 9.018302917480469\n",
      "Step: 5430, train/loss: 9.999999747378752e-05\n",
      "Step: 5430, train/grad_norm: 0.006687409244477749\n",
      "Step: 5430, train/learning_rate: 4.825291398447007e-06\n",
      "Step: 5430, train/epoch: 9.034941673278809\n",
      "Step: 5440, train/loss: 0.0\n",
      "Step: 5440, train/grad_norm: 0.0008102335268631577\n",
      "Step: 5440, train/learning_rate: 4.74209628009703e-06\n",
      "Step: 5440, train/epoch: 9.051580429077148\n",
      "Step: 5450, train/loss: 0.00039999998989515007\n",
      "Step: 5450, train/grad_norm: 0.19532310962677002\n",
      "Step: 5450, train/learning_rate: 4.658901616494404e-06\n",
      "Step: 5450, train/epoch: 9.068219184875488\n",
      "Step: 5460, train/loss: 0.0\n",
      "Step: 5460, train/grad_norm: 0.0018512331880629063\n",
      "Step: 5460, train/learning_rate: 4.575706952891778e-06\n",
      "Step: 5460, train/epoch: 9.084858894348145\n",
      "Step: 5470, train/loss: 0.0\n",
      "Step: 5470, train/grad_norm: 0.0061291176825761795\n",
      "Step: 5470, train/learning_rate: 4.492512289289152e-06\n",
      "Step: 5470, train/epoch: 9.101497650146484\n",
      "Step: 5480, train/loss: 0.00570000009611249\n",
      "Step: 5480, train/grad_norm: 0.009489716030657291\n",
      "Step: 5480, train/learning_rate: 4.409317625686526e-06\n",
      "Step: 5480, train/epoch: 9.118136405944824\n",
      "Step: 5490, train/loss: 0.0010999999940395355\n",
      "Step: 5490, train/grad_norm: 0.0007403256604447961\n",
      "Step: 5490, train/learning_rate: 4.3261229620839e-06\n",
      "Step: 5490, train/epoch: 9.134775161743164\n",
      "Step: 5500, train/loss: 9.999999747378752e-05\n",
      "Step: 5500, train/grad_norm: 0.0007683925796300173\n",
      "Step: 5500, train/learning_rate: 4.242928298481274e-06\n",
      "Step: 5500, train/epoch: 9.151413917541504\n",
      "Step: 5510, train/loss: 0.0\n",
      "Step: 5510, train/grad_norm: 0.0009733690530993044\n",
      "Step: 5510, train/learning_rate: 4.1597336348786484e-06\n",
      "Step: 5510, train/epoch: 9.16805362701416\n",
      "Step: 5520, train/loss: 0.0\n",
      "Step: 5520, train/grad_norm: 0.0010703294537961483\n",
      "Step: 5520, train/learning_rate: 4.0765389712760225e-06\n",
      "Step: 5520, train/epoch: 9.1846923828125\n",
      "Step: 5530, train/loss: 9.999999747378752e-05\n",
      "Step: 5530, train/grad_norm: 0.01509647723287344\n",
      "Step: 5530, train/learning_rate: 3.9933443076733965e-06\n",
      "Step: 5530, train/epoch: 9.20133113861084\n",
      "Step: 5540, train/loss: 0.0\n",
      "Step: 5540, train/grad_norm: 0.001845940831117332\n",
      "Step: 5540, train/learning_rate: 3.910149644070771e-06\n",
      "Step: 5540, train/epoch: 9.21796989440918\n",
      "Step: 5550, train/loss: 9.999999747378752e-05\n",
      "Step: 5550, train/grad_norm: 0.014500843361020088\n",
      "Step: 5550, train/learning_rate: 3.826954980468145e-06\n",
      "Step: 5550, train/epoch: 9.23460865020752\n",
      "Step: 5560, train/loss: 0.0\n",
      "Step: 5560, train/grad_norm: 0.000909297086764127\n",
      "Step: 5560, train/learning_rate: 3.7437603168655187e-06\n",
      "Step: 5560, train/epoch: 9.251248359680176\n",
      "Step: 5570, train/loss: 9.999999747378752e-05\n",
      "Step: 5570, train/grad_norm: 0.01267150230705738\n",
      "Step: 5570, train/learning_rate: 3.6605656532628927e-06\n",
      "Step: 5570, train/epoch: 9.267887115478516\n",
      "Step: 5580, train/loss: 9.999999747378752e-05\n",
      "Step: 5580, train/grad_norm: 0.039610605686903\n",
      "Step: 5580, train/learning_rate: 3.5773709896602668e-06\n",
      "Step: 5580, train/epoch: 9.284525871276855\n",
      "Step: 5590, train/loss: 0.004100000020116568\n",
      "Step: 5590, train/grad_norm: 0.01053216215223074\n",
      "Step: 5590, train/learning_rate: 3.494176326057641e-06\n",
      "Step: 5590, train/epoch: 9.301164627075195\n",
      "Step: 5600, train/loss: 0.004699999932199717\n",
      "Step: 5600, train/grad_norm: 0.009948835708200932\n",
      "Step: 5600, train/learning_rate: 3.410981662455015e-06\n",
      "Step: 5600, train/epoch: 9.317803382873535\n",
      "Step: 5610, train/loss: 0.0\n",
      "Step: 5610, train/grad_norm: 0.09421354532241821\n",
      "Step: 5610, train/learning_rate: 3.327786998852389e-06\n",
      "Step: 5610, train/epoch: 9.334442138671875\n",
      "Step: 5620, train/loss: 9.999999747378752e-05\n",
      "Step: 5620, train/grad_norm: 0.004804892465472221\n",
      "Step: 5620, train/learning_rate: 3.244592335249763e-06\n",
      "Step: 5620, train/epoch: 9.351081848144531\n",
      "Step: 5630, train/loss: 0.004800000227987766\n",
      "Step: 5630, train/grad_norm: 0.0009078113944269717\n",
      "Step: 5630, train/learning_rate: 3.161397671647137e-06\n",
      "Step: 5630, train/epoch: 9.367720603942871\n",
      "Step: 5640, train/loss: 9.999999747378752e-05\n",
      "Step: 5640, train/grad_norm: 0.0006904243491590023\n",
      "Step: 5640, train/learning_rate: 3.078203008044511e-06\n",
      "Step: 5640, train/epoch: 9.384359359741211\n",
      "Step: 5650, train/loss: 0.0\n",
      "Step: 5650, train/grad_norm: 0.0009546789224259555\n",
      "Step: 5650, train/learning_rate: 2.995008344441885e-06\n",
      "Step: 5650, train/epoch: 9.40099811553955\n",
      "Step: 5660, train/loss: 9.999999747378752e-05\n",
      "Step: 5660, train/grad_norm: 0.021339748054742813\n",
      "Step: 5660, train/learning_rate: 2.911813680839259e-06\n",
      "Step: 5660, train/epoch: 9.41763687133789\n",
      "Step: 5670, train/loss: 0.0\n",
      "Step: 5670, train/grad_norm: 0.0027915232349187136\n",
      "Step: 5670, train/learning_rate: 2.8286190172366332e-06\n",
      "Step: 5670, train/epoch: 9.434276580810547\n",
      "Step: 5680, train/loss: 0.0\n",
      "Step: 5680, train/grad_norm: 0.0006890760851092637\n",
      "Step: 5680, train/learning_rate: 2.7454243536340073e-06\n",
      "Step: 5680, train/epoch: 9.450915336608887\n",
      "Step: 5690, train/loss: 0.0\n",
      "Step: 5690, train/grad_norm: 0.0007631756016053259\n",
      "Step: 5690, train/learning_rate: 2.6622296900313813e-06\n",
      "Step: 5690, train/epoch: 9.467554092407227\n",
      "Step: 5700, train/loss: 0.0\n",
      "Step: 5700, train/grad_norm: 0.0008199650910682976\n",
      "Step: 5700, train/learning_rate: 2.5790350264287554e-06\n",
      "Step: 5700, train/epoch: 9.484192848205566\n",
      "Step: 5710, train/loss: 9.999999747378752e-05\n",
      "Step: 5710, train/grad_norm: 0.0005975058302283287\n",
      "Step: 5710, train/learning_rate: 2.4958403628261294e-06\n",
      "Step: 5710, train/epoch: 9.500831604003906\n",
      "Step: 5720, train/loss: 0.0\n",
      "Step: 5720, train/grad_norm: 0.0006983137573115528\n",
      "Step: 5720, train/learning_rate: 2.4126456992235035e-06\n",
      "Step: 5720, train/epoch: 9.517471313476562\n",
      "Step: 5730, train/loss: 0.0003000000142492354\n",
      "Step: 5730, train/grad_norm: 3.245041847229004\n",
      "Step: 5730, train/learning_rate: 2.329450808247202e-06\n",
      "Step: 5730, train/epoch: 9.534110069274902\n",
      "Step: 5740, train/loss: 0.0034000000450760126\n",
      "Step: 5740, train/grad_norm: 35.304656982421875\n",
      "Step: 5740, train/learning_rate: 2.246256144644576e-06\n",
      "Step: 5740, train/epoch: 9.550748825073242\n",
      "Step: 5750, train/loss: 0.0\n",
      "Step: 5750, train/grad_norm: 0.0006812733481638134\n",
      "Step: 5750, train/learning_rate: 2.16306148104195e-06\n",
      "Step: 5750, train/epoch: 9.567387580871582\n",
      "Step: 5760, train/loss: 0.0027000000700354576\n",
      "Step: 5760, train/grad_norm: 119.13070678710938\n",
      "Step: 5760, train/learning_rate: 2.0798668174393242e-06\n",
      "Step: 5760, train/epoch: 9.584026336669922\n",
      "Step: 5770, train/loss: 0.0\n",
      "Step: 5770, train/grad_norm: 0.004107257351279259\n",
      "Step: 5770, train/learning_rate: 1.9966721538366983e-06\n",
      "Step: 5770, train/epoch: 9.600665092468262\n",
      "Step: 5780, train/loss: 0.0\n",
      "Step: 5780, train/grad_norm: 0.0006924148183315992\n",
      "Step: 5780, train/learning_rate: 1.9134774902340723e-06\n",
      "Step: 5780, train/epoch: 9.617304801940918\n",
      "Step: 5790, train/loss: 0.0\n",
      "Step: 5790, train/grad_norm: 0.0007580621168017387\n",
      "Step: 5790, train/learning_rate: 1.8302828266314464e-06\n",
      "Step: 5790, train/epoch: 9.633943557739258\n",
      "Step: 5800, train/loss: 0.01140000019222498\n",
      "Step: 5800, train/grad_norm: 0.0006467153434641659\n",
      "Step: 5800, train/learning_rate: 1.7470881630288204e-06\n",
      "Step: 5800, train/epoch: 9.650582313537598\n",
      "Step: 5810, train/loss: 0.0\n",
      "Step: 5810, train/grad_norm: 0.0007281529833562672\n",
      "Step: 5810, train/learning_rate: 1.6638934994261945e-06\n",
      "Step: 5810, train/epoch: 9.667221069335938\n",
      "Step: 5820, train/loss: 0.0\n",
      "Step: 5820, train/grad_norm: 0.017978733405470848\n",
      "Step: 5820, train/learning_rate: 1.5806988358235685e-06\n",
      "Step: 5820, train/epoch: 9.683859825134277\n",
      "Step: 5830, train/loss: 0.027000000700354576\n",
      "Step: 5830, train/grad_norm: 0.0011872239410877228\n",
      "Step: 5830, train/learning_rate: 1.4975041722209426e-06\n",
      "Step: 5830, train/epoch: 9.700499534606934\n",
      "Step: 5840, train/loss: 0.019700000062584877\n",
      "Step: 5840, train/grad_norm: 0.0034680909011512995\n",
      "Step: 5840, train/learning_rate: 1.4143095086183166e-06\n",
      "Step: 5840, train/epoch: 9.717138290405273\n",
      "Step: 5850, train/loss: 0.0\n",
      "Step: 5850, train/grad_norm: 0.0037482648622244596\n",
      "Step: 5850, train/learning_rate: 1.3311148450156907e-06\n",
      "Step: 5850, train/epoch: 9.733777046203613\n",
      "Step: 5860, train/loss: 0.013199999928474426\n",
      "Step: 5860, train/grad_norm: 0.004546121694147587\n",
      "Step: 5860, train/learning_rate: 1.2479201814130647e-06\n",
      "Step: 5860, train/epoch: 9.750415802001953\n",
      "Step: 5870, train/loss: 0.0\n",
      "Step: 5870, train/grad_norm: 0.00213228864595294\n",
      "Step: 5870, train/learning_rate: 1.164725404123601e-06\n",
      "Step: 5870, train/epoch: 9.767054557800293\n",
      "Step: 5880, train/loss: 0.0\n",
      "Step: 5880, train/grad_norm: 0.000943948864005506\n",
      "Step: 5880, train/learning_rate: 1.081530740520975e-06\n",
      "Step: 5880, train/epoch: 9.78369426727295\n",
      "Step: 5890, train/loss: 0.0\n",
      "Step: 5890, train/grad_norm: 0.0017348916735500097\n",
      "Step: 5890, train/learning_rate: 9.983360769183491e-07\n",
      "Step: 5890, train/epoch: 9.800333023071289\n",
      "Step: 5900, train/loss: 9.999999747378752e-05\n",
      "Step: 5900, train/grad_norm: 0.0007862095953896642\n",
      "Step: 5900, train/learning_rate: 9.151414133157232e-07\n",
      "Step: 5900, train/epoch: 9.816971778869629\n",
      "Step: 5910, train/loss: 0.0003000000142492354\n",
      "Step: 5910, train/grad_norm: 0.008978498168289661\n",
      "Step: 5910, train/learning_rate: 8.319467497130972e-07\n",
      "Step: 5910, train/epoch: 9.833610534667969\n",
      "Step: 5920, train/loss: 9.999999747378752e-05\n",
      "Step: 5920, train/grad_norm: 0.1335178017616272\n",
      "Step: 5920, train/learning_rate: 7.487520861104713e-07\n",
      "Step: 5920, train/epoch: 9.850249290466309\n",
      "Step: 5930, train/loss: 0.006899999920278788\n",
      "Step: 5930, train/grad_norm: 0.0030600049067288637\n",
      "Step: 5930, train/learning_rate: 6.655574225078453e-07\n",
      "Step: 5930, train/epoch: 9.866888046264648\n",
      "Step: 5940, train/loss: 0.010400000028312206\n",
      "Step: 5940, train/grad_norm: 0.004254491068422794\n",
      "Step: 5940, train/learning_rate: 5.823627020618005e-07\n",
      "Step: 5940, train/epoch: 9.883527755737305\n",
      "Step: 5950, train/loss: 0.01489999983459711\n",
      "Step: 5950, train/grad_norm: 0.005220625549554825\n",
      "Step: 5950, train/learning_rate: 4.991680384591746e-07\n",
      "Step: 5950, train/epoch: 9.900166511535645\n",
      "Step: 5960, train/loss: 9.999999747378752e-05\n",
      "Step: 5960, train/grad_norm: 0.000846231821924448\n",
      "Step: 5960, train/learning_rate: 4.159733748565486e-07\n",
      "Step: 5960, train/epoch: 9.916805267333984\n",
      "Step: 5970, train/loss: 0.0\n",
      "Step: 5970, train/grad_norm: 0.0007463709334842861\n",
      "Step: 5970, train/learning_rate: 3.3277871125392267e-07\n",
      "Step: 5970, train/epoch: 9.933444023132324\n",
      "Step: 5980, train/loss: 0.0\n",
      "Step: 5980, train/grad_norm: 0.0011836184421554208\n",
      "Step: 5980, train/learning_rate: 2.495840192295873e-07\n",
      "Step: 5980, train/epoch: 9.950082778930664\n",
      "Step: 5990, train/loss: 0.0003000000142492354\n",
      "Step: 5990, train/grad_norm: 0.00265654269605875\n",
      "Step: 5990, train/learning_rate: 1.6638935562696133e-07\n",
      "Step: 5990, train/epoch: 9.96672248840332\n",
      "Step: 6000, train/loss: 0.003000000026077032\n",
      "Step: 6000, train/grad_norm: 43.055328369140625\n",
      "Step: 6000, train/learning_rate: 8.319467781348067e-08\n",
      "Step: 6000, train/epoch: 9.98336124420166\n",
      "Step: 6010, train/loss: 0.0\n",
      "Step: 6010, train/grad_norm: 0.0009477089042775333\n",
      "Step: 6010, train/learning_rate: 0.0\n",
      "Step: 6010, train/epoch: 10.0\n",
      "Step: 6010, eval/loss: 0.8146726489067078\n",
      "Step: 6010, eval/accuracy: 0.8960155248641968\n",
      "Step: 6010, eval/f1: 0.8894609212875366\n",
      "Step: 6010, eval/runtime: 81.06120300292969\n",
      "Step: 6010, eval/samples_per_second: 88.85900115966797\n",
      "Step: 6010, eval/steps_per_second: 1.590999960899353\n",
      "Step: 6010, train/epoch: 10.0\n",
      "Step: 6010, train/train_runtime: 9428.09375\n",
      "Step: 6010, train/train_samples_per_second: 35.650001525878906\n",
      "Step: 6010, train/train_steps_per_second: 0.6370000243186951\n",
      "Step: 6010, train/total_flos: 3.132314495012045e+17\n",
      "Step: 6010, train/train_loss: 0.12112291157245636\n",
      "Step: 6010, train/epoch: 10.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.summary.summary_iterator import summary_iterator\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Construct the logs directory path\n",
    "logs_directory = os.path.join('./', project_name, 'logs')\n",
    "file_pattern = 'events.out.tfevents.*'\n",
    "\n",
    "# Retrieve all event files matching the pattern\n",
    "event_files = glob.glob(os.path.join(logs_directory, file_pattern))\n",
    "\n",
    "# Function to print out TensorBoard event logs\n",
    "def print_events_from_file(event_files):\n",
    "    for event_file in event_files:\n",
    "        print(f\"Reading events from file: {event_file}\")\n",
    "        try:\n",
    "            for e in summary_iterator(event_file):\n",
    "                for v in e.summary.value:\n",
    "                    if v.HasField('simple_value'):\n",
    "                        print(f\"Step: {e.step}, {v.tag}: {v.simple_value}\")\n",
    "        except Exception as e:  # Just in case the event file is not readable\n",
    "            print(f\"Failed to read {event_file}: {e}\")\n",
    "\n",
    "print_events_from_file(event_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f8e0ccc-f7c4-4a02-a1e6-5e3012717125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Step Train Loss Eval Loss  Accuracy        F1\n",
      "0   601   0.393100  0.360599  0.843815  0.832579\n",
      "1  1202   0.289400  0.298993  0.874636  0.864003\n",
      "2  1803   0.133400  0.354863  0.871859  0.868087\n",
      "3  2404   0.097600  0.389370  0.879495  0.875366\n",
      "4  3005   0.022500  0.572214  0.880050  0.873018\n",
      "5  3606   0.024900  0.563661  0.889768  0.883994\n",
      "6  4207   0.055900  0.674912  0.890462  0.883184\n",
      "7  4808   0.006900  0.788489  0.890462  0.885028\n",
      "8  5409   0.017400  0.831780  0.895183  0.890066\n",
      "9  6010   0.000000  0.814673  0.896016  0.889461\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.python.summary.summary_iterator import summary_iterator\n",
    "\n",
    "logs_directory = os.path.join('./', project_name, 'logs')\n",
    "file_pattern = 'events.out.tfevents.*'\n",
    "\n",
    "event_files = glob.glob(os.path.join(logs_directory, file_pattern))\n",
    "\n",
    "def extract_metrics(event_files):\n",
    "    data = []\n",
    "    last_train_loss = None\n",
    "\n",
    "    for event_file in event_files:\n",
    "        for e in summary_iterator(event_file):\n",
    "            for v in e.summary.value:\n",
    "                if v.HasField('simple_value'):\n",
    "                    step = e.step\n",
    "                    metric_name = v.tag.split('/')[-1]\n",
    "                    metric_value = v.simple_value\n",
    "\n",
    "                    formatted_value = f\"{metric_value:.6f}\"\n",
    "\n",
    "                    if 'train/loss' in v.tag:\n",
    "                        last_train_loss = formatted_value\n",
    "\n",
    "                    if 'eval' in v.tag:\n",
    "                        entry = next((item for item in data if item['Step'] == step), None)\n",
    "                        if not entry:\n",
    "                            entry = {'Step': step, 'Train Loss': last_train_loss, 'Eval Loss': None, 'Accuracy': None, 'F1': None}\n",
    "                            data.append(entry)\n",
    "                        if 'loss' in v.tag:\n",
    "                            entry['Eval Loss'] = formatted_value\n",
    "                        elif 'accuracy' in v.tag:\n",
    "                            entry['Accuracy'] = formatted_value\n",
    "                        elif 'f1' in v.tag:\n",
    "                            entry['F1'] = formatted_value\n",
    "    return data\n",
    "\n",
    "metrics_data = extract_metrics(event_files)\n",
    "\n",
    "df = pd.DataFrame(metrics_data)\n",
    "df = df.sort_values(by='Step')\n",
    "\n",
    "file_path = \"../images/\"+model_name+\"_Checkpoint_Data.csv\"\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52633cb6-5847-4d51-86c5-c6153e72cc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Checkpoint Step: checkpoint-6010\n",
      "Step              6010\n",
      "Train Loss    0.000000\n",
      "Eval Loss     0.814673\n",
      "Accuracy      0.896016\n",
      "F1            0.889461\n",
      "Rank Sum          12.0\n",
      "Name: 9, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df.fillna({\n",
    "    'Eval Loss': float('inf'),\n",
    "    'Accuracy': 0,\n",
    "    'F1': 0\n",
    "}, inplace=True)\n",
    "\n",
    "df['Eval Loss'] = df['Eval Loss'].astype(float)\n",
    "df['Accuracy'] = df['Accuracy'].astype(float)\n",
    "df['F1'] = df['F1'].astype(float)\n",
    "\n",
    "df['Eval Loss Rank'] = df['Eval Loss'].rank(method='min', ascending=True)\n",
    "df['Accuracy Rank'] = df['Accuracy'].rank(method='min', ascending=False)\n",
    "df['F1 Rank'] = df['F1'].rank(method='min', ascending=False)\n",
    "\n",
    "df['Rank Sum'] = df['Eval Loss Rank'] + df['Accuracy Rank'] + df['F1 Rank']\n",
    "\n",
    "best_checkpoint = df.loc[df['Rank Sum'].idxmin()]\n",
    "\n",
    "checkpoint_folder_name = f\"checkpoint-{best_checkpoint['Step']}\"\n",
    "print(f\"Best Checkpoint Step: {checkpoint_folder_name}\")\n",
    "print(best_checkpoint[['Step', 'Train Loss', 'Eval Loss', 'Accuracy', 'F1', 'Rank Sum']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d4044b-4ffc-409e-ac1a-8b4c58e72499",
   "metadata": {},
   "source": [
    "### Run TensorBoard\n",
    "tensorboard --logdir=~/kuk/Praxis/praxis-Llama-2-7b-hf-small-finetune/logs --host=0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e5a88a-7620-4280-970b-ff171568aebb",
   "metadata": {},
   "source": [
    "### PAUSE SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57fc853d-d293-402d-87d0-e27c83915c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My flag to pause the script, set to True to pause\n",
    "pause_script = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53887034-7922-4ed5-bd9a-8c80d6596ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        print(\"Script Paused\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd8248c6-2c95-4d82-a7f0-2e0ac37ab320",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pause_script:\n",
    "    raise StopExecution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19be6e26-d888-4da0-b3f8-836d68ac2051",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b140136-4f0d-44e4-9955-d95a342f250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import pipeline\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "test_checkpoint_name = checkpoint_folder_name\n",
    "savedmodel = pipeline('text-classification', model=output_dir_path + \"/\" + test_checkpoint_name, device=accelerator.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e0c22fa-0da7-4a30-8d27-128cab0bdfc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['article', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 7203\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50f2ad84-10b4-4f10-bbea-25571bfcda78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d40befc8172b41fba80a9a6acdb95b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "max_tokens = 512\n",
    "truncated_articles = []\n",
    "\n",
    "for article in tqdm(tokenized_test_ds['article']):\n",
    "    encoded_article = tokenizer.encode(\n",
    "        article,\n",
    "        max_length=max_tokens,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    truncated_article = tokenizer.decode(encoded_article, skip_special_tokens=True)\n",
    "    truncated_articles.append(truncated_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d687d739-aafa-4ae2-b938-e8465856746e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10cbe2132f4d4c49baccb3532f0c88f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifying articles:   0%|          | 0/7203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a96d376192645e582b5d9ddf596f15c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Labeling articles:   0%|          | 0/7203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Create a dataset from the truncated articles\n",
    "data = {\"text\": truncated_articles}\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Define a function to make predictions\n",
    "def classify_batch(batch):\n",
    "    return savedmodel(batch[\"text\"])\n",
    "\n",
    "# Process the dataset with batching\n",
    "test_predictions = []\n",
    "for batch in tqdm(dataset.to_dict()[\"text\"], desc=\"Classifying articles\"):\n",
    "    predictions = classify_batch({\"text\": batch})\n",
    "    test_predictions.extend(predictions)\n",
    "\n",
    "test_predictions_labels = []\n",
    "for prediction in tqdm(test_predictions, desc=\"Labeling articles\"):\n",
    "    label = 0 if prediction['label'] == 'LABEL_0' else 1\n",
    "    test_predictions_labels.append(label)\n",
    "\n",
    "print(test_predictions_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26d6eead-d836-4e80-8ff6-b86b8599b245",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "true_articles = tokenized_test_ds['label']\n",
    "print(true_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10e33442-ac7f-49d3-a6ad-5eb4b892eedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(prediction):\n",
    "  return int(prediction['label'].split('_')[1])\n",
    "\n",
    "processed_predictions = [get_label(prediction) for prediction in test_predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f06a2c-5ded-4da0-8232-145383967c9d",
   "metadata": {},
   "source": [
    "<h1>Accuracy and F1</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb4d8350-a8d1-4853-a9f8-74ae9ea36bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8994863251423019\n"
     ]
    }
   ],
   "source": [
    "true_articles = tokenized_test_ds['label']\n",
    "accuracy = accuracy_score(true_articles, test_predictions_labels)\n",
    "print(\"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40780784-e77d-4b26-9dae-572137e591bf",
   "metadata": {},
   "source": [
    "<p>precision (how many of the items identified as positive are actually positive) and the recall (how many of the actual positives were identified correctly)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30465934-1b77-4a2c-8c52-34e239e37abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.8931581513700004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(true_articles, test_predictions_labels, average='macro')\n",
    "print(\"f1_score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcaac14-2b83-4c24-b83a-f6d0e73a2d2a",
   "metadata": {},
   "source": [
    "<h1>Confusion Matrix</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57e64afb-e3ee-4c79-8228-b2d79806229e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(tokenized_test_ds['label'], processed_predictions)\n",
    "\n",
    "labels = ['human', 'machine']\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c87380b0-553d-4642-9697-26c76c98dde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook praxis-bert-large-uncased-small-finetune-v7.ipynb to html\n",
      "[NbConvertApp] WARNING | Alternative text is missing on 1 image(s).\n",
      "[NbConvertApp] Writing 575666 bytes to html/praxis-bert-large-uncased-small-finetune-v7.html\n"
     ]
    }
   ],
   "source": [
    "file_name = f\"{project_name}.ipynb\"\n",
    "html_file_name = f\"{file_name.replace('.ipynb', '.html')}\"\n",
    "\n",
    "command = f\"jupyter nbconvert '{file_name}' --to html --output-dir './html' --output '{html_file_name}'\"\n",
    "get_ipython().system(command)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
