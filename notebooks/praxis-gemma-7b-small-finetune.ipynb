{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c2d2c9-4a4b-48ca-90a3-1ccd120ca08b",
   "metadata": {},
   "source": [
    "# Fine tuning using Gemma 7b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5bef3d-89b9-42ed-b158-a39bd61f6a31",
   "metadata": {},
   "source": [
    "### Base Model and Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfe5f014-527c-4a83-863b-4b6330c72ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 0: NVIDIA GeForce RTX 4090\n",
    "# GPU 1: NVIDIA GeForce RTX 4090\n",
    "# GPU 2: NVIDIA GeForce RTX 4090\n",
    "# GPU 3: NVIDIA GeForce RTX 3090 Ti\n",
    "# GPU 4: NVIDIA GeForce RTX 3090 Ti\n",
    "# GPU 5: NVIDIA GeForce RTX 3090\n",
    "# GPU 6: NVIDIA GeForce RTX 3090\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # \"\"makes all visible, \"0\" GPU 0 visible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ffaa11-3936-40a0-8f2a-3d083ff2afef",
   "metadata": {},
   "source": [
    "### Supress warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e77f7e3d-3af3-4dc8-9571-d13398c29ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef535c7a-848e-4c6e-a692-208f98610d82",
   "metadata": {},
   "source": [
    "### Inspect the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bfef1a8-c05a-4e14-af0e-358bfb04352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89cce100-9b2e-4675-bfda-f029e7c4056e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0be9fcfc1b4ecfbf167968377056d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15ec782b-0776-4354-ad08-66f1e9e50187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"gemma-7b\"\n",
    "checkpoint = \"google/\"+model_name\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "285dcf58-91f8-442f-a208-9c0c064ceeb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d88f0ed4d249428de7100ed9bd6b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GemmaForSequenceClassification were not initialized from the model checkpoint at google/gemma-7b and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, quantization_config=bnb_config, device_map=\"auto\", num_labels=2, token=access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83f74939-3ab3-4011-90cb-8e90c22ea162",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "GemmaForSequenceClassification                          --\n",
       "├─GemmaModel: 1-1                                       --\n",
       "│    └─Embedding: 2-1                                   786,432,000\n",
       "│    └─ModuleList: 2-2                                  --\n",
       "│    │    └─GemmaDecoderLayer: 3-1                      138,418,176\n",
       "│    │    └─GemmaDecoderLayer: 3-2                      138,418,176\n",
       "│    │    └─GemmaDecoderLayer: 3-3                      138,418,176\n",
       "│    │    └─GemmaDecoderLayer: 3-4                      138,418,176\n",
       "│    │    └─GemmaDecoderLayer: 3-5                      138,418,176\n",
       "│    │    └─GemmaDecoderLayer: 3-6                      138,418,176\n",
       "│    │    └─GemmaDecoderLayer: 3-7                      138,418,176\n",
       "│    │    └─GemmaDecoderLayer: 3-8                      138,418,176\n",
       "│    │    └─GemmaDecoderLayer: 3-9                      138,418,176\n",
       "│    │    └─GemmaDecoderLayer: 3-10                     138,418,176\n",
       "│    │    └─GemmaDecoderLayer: 3-11                     138,418,176\n",
       "│    │    └─GemmaDecoderLayer: 3-12                     138,418,176\n",
       "│    │    └─GemmaDecoderLayer: 3-13                     138,418,176\n",
       "│    │    └─GemmaDecoderLayer: 3-14                     138,418,176\n",
       "│    │    └─GemmaDecoderLayer: 3-15                     138,418,176\n",
       "│    │    └─GemmaDecoderLayer: 3-16                     138,418,176\n",
       "│    │    └─GemmaDecoderLayer: 3-17                     138,418,176\n",
       "│    │    └─GemmaDecoderLayer: 3-18                     138,418,176\n",
       "│    │    └─GemmaDecoderLayer: 3-19                     138,418,176\n",
       "│    │    └─GemmaDecoderLayer: 3-20                     138,418,176\n",
       "│    │    └─GemmaDecoderLayer: 3-21                     138,418,176\n",
       "│    │    └─GemmaDecoderLayer: 3-22                     138,418,176\n",
       "│    │    └─GemmaDecoderLayer: 3-23                     138,418,176\n",
       "│    │    └─GemmaDecoderLayer: 3-24                     138,418,176\n",
       "│    │    └─GemmaDecoderLayer: 3-25                     138,418,176\n",
       "│    │    └─GemmaDecoderLayer: 3-26                     138,418,176\n",
       "│    │    └─GemmaDecoderLayer: 3-27                     138,418,176\n",
       "│    │    └─GemmaDecoderLayer: 3-28                     138,418,176\n",
       "│    └─GemmaRMSNorm: 2-3                                3,072\n",
       "├─Linear: 1-2                                           6,144\n",
       "================================================================================\n",
       "Total params: 4,662,150,144\n",
       "Trainable params: 786,613,248\n",
       "Non-trainable params: 3,875,536,896\n",
       "================================================================================"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49bbe47c-4430-4f4a-90d8-1113bd320a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForSequenceClassification(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 3072, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=3072, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=3072, out_features=24576, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3072, out_features=24576, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=24576, out_features=3072, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (score): Linear(in_features=3072, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6627b1da-57d6-4a17-8ea3-746b5cb1f3d9",
   "metadata": {},
   "source": [
    "### Load the news dataset from pickle file\n",
    "If any of the check_files don't exist then load the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cc372cc-0cae-4b49-a2d7-8e57f58244a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one of the specified files already exists. Not loading new dataset.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "base_path = './data/'\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "file_name = 'news_small_dataset.pkl'\n",
    "file_path = base_path+file_name\n",
    "\n",
    "def pickle_dataset(dataset, file_path):\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(dataset, file)\n",
    "        print(f\"Dataset has been pickled to: {file_path}\")\n",
    "\n",
    "def load_pickle_dataset(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        dataset = pickle.load(file)\n",
    "        print(f\"Dataset has been loaded from: {file_path}\")\n",
    "    return dataset\n",
    "\n",
    "def check_files_exists(file_names):\n",
    "    for name in file_names:\n",
    "        file_path = os.path.join(base_path, name)\n",
    "        if os.path.exists(file_path):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# if these files exist we do not want to load the news_dataset.pkl to tokenize and make these files\n",
    "check_files = [model_name+'-small_tokenized_train_ds.pkl', model_name+'-small_tokenized_eval_ds.pkl', model_name+'-small_tokenized_test_ds.pkl']\n",
    "\n",
    "if check_files_exists(check_files):\n",
    "    print(\"At least one of the specified files already exists. Not loading new dataset.\")\n",
    "else:\n",
    "    news_split_ds = load_pickle_dataset(file_path)\n",
    "    print(news_split_ds)\n",
    "    total_rows = (news_split_ds['train'].num_rows +\n",
    "              news_split_ds['eval'].num_rows +\n",
    "              news_split_ds['test'].num_rows)\n",
    "    print(\"Total number of rows:\", total_rows)\n",
    "    print(\"Dataset loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad450a2-6cef-432e-9e63-7ff985b4726e",
   "metadata": {},
   "source": [
    "### Tokenization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88c3a38c-1d24-4a1e-8d96-1b7c2ce162c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, token=access_token)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "\n",
    "def tokenize_fn(news):\n",
    "    return tokenizer(news['article'], padding=True, truncation=True, max_length=512, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02efc4f0-742a-4838-887d-5556a26ae15f",
   "metadata": {},
   "source": [
    "### Tokenize train, evaluation, and test datasets\n",
    "If any of the check files exist then don't run tokenization and save some time.\n",
    "Else load the pickle files that already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f3e3101-df30-4af2-9976-49ba0cf44d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already exist, so load datasets\n",
      "Dataset has been loaded from: ./data/gemma-7b-small_tokenized_train_ds.pkl\n",
      "Dataset has been loaded from: ./data/gemma-7b-small_tokenized_eval_ds.pkl\n",
      "Dataset has been loaded from: ./data/gemma-7b-small_tokenized_test_ds.pkl\n"
     ]
    }
   ],
   "source": [
    "if not check_files_exists(check_files):\n",
    "    tokenized_train_ds = news_split_ds['train'].map(tokenize_fn, batched=True)\n",
    "    tokenized_eval_ds = news_split_ds['eval'].map(tokenize_fn, batched=True)\n",
    "    tokenized_test_ds = news_split_ds['test'].map(tokenize_fn, batched=True)\n",
    "\n",
    "    print(tokenized_train_ds.features)\n",
    "    print(tokenized_eval_ds.features)\n",
    "    print(tokenized_test_ds.features)\n",
    "    \n",
    "    pickle_dataset(tokenized_train_ds, base_path+model_name+'-small_tokenized_train_ds.pkl')\n",
    "    pickle_dataset(tokenized_eval_ds, base_path+model_name+'-small_tokenized_eval_ds.pkl')\n",
    "    pickle_dataset(tokenized_test_ds, base_path+model_name+'-small_tokenized_test_ds.pkl')\n",
    "else:\n",
    "    print(\"Files already exist, so load datasets\")\n",
    "    tokenized_train_ds = load_pickle_dataset(base_path+model_name+'-small_tokenized_train_ds.pkl')\n",
    "    tokenized_eval_ds = load_pickle_dataset(base_path+model_name+'-small_tokenized_eval_ds.pkl')\n",
    "    tokenized_test_ds = load_pickle_dataset(base_path+model_name+'-small_tokenized_test_ds.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307cc4fb-9766-4b0f-943b-4a1c90053e09",
   "metadata": {},
   "source": [
    "### Look at the tokenized data\n",
    "Notice what the actual data looks like, and then the tokenized data which is a bunch of numbers, and then the attention mask at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44321182-e1b9-4570-bd24-7a5cf42ba504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in training dataset: 33611\n",
      "Number of records in evaluation dataset: 7203\n",
      "Number of records in test dataset: 7203\n",
      "Total number of records: 48017\n"
     ]
    }
   ],
   "source": [
    "count_train_records = len(tokenized_train_ds)\n",
    "count_eval_records = len(tokenized_eval_ds)\n",
    "count_test_records = len(tokenized_test_ds)\n",
    "print(f\"Number of records in training dataset: {count_train_records}\")\n",
    "print(f\"Number of records in evaluation dataset: {count_eval_records}\")\n",
    "print(f\"Number of records in test dataset: {count_test_records}\")\n",
    "count_total_records = count_train_records + count_eval_records + count_test_records\n",
    "print(f\"Total number of records: {count_total_records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64be678b-d2a8-403e-bcc8-ef9e7eabb0cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'article': \"In a year where homicides, rapes and robberies increased slightly, New York City still saw serious crime drop 1.7 percent in 2015, continuing an overall decline that began in the 1990s, NYPD Commissioner William Bratton said Monday.\\nAt a news conference with Mayor Bill de Blasio, Bratton touted last year’s crime statistics, which he said, when combined with an even larger decline in 2014, put to rest the fear that substantial decreases couldn’t continue under the new administration at City Hall.\\n“While we have had some fluctuation, some increases in certain categories, the overall trend in all our crime categories continues to go down,” Bratton told reporters. “It was a very good year for us, 2015.\\nHomicides increased by 4.5 percent in 2015, rising to 350 from 333 in the prior year, which was the lowest since 1994, said Deputy Commissioner Dermot Shea. Rapes increased 6 percent and robberies rose 2 percent, said Shea, who is in charge of data collection and operations for the NYPD.\\nThe lower overall crime statistics came about due to what Shea called “targeted enforcement,” where cops make quality arrests even though the overall number of apprehensions was the lowest in the city since 2003.\\nTwo boroughs — Manhattan and the Bronx — actually saw serious crimes increase by 3 percent and 4 percent, respectively, Shea said. Manhattan’s increase was driven by more robberies, while the Bronx, although seeing an overall crime increase, had what he said was a “phenomenal” reduction in shootings. Citywide, shootings were down in 2015 about 3 percent, to 1,103 from 1,172 in 2014.\\nShea largely attributed the 2015 increase in rapes to victims coming forward with complaints about attacks from years past.\\nSign up to get the latest updates Get Newsday's Breaking News alerts in your inbox. By clicking Sign up, you agree to our privacy policy.\\n“Twenty percent of these rapes didn’t happen in 2015,” he said.\\nThe NYPD has seen an increase in rapes involving single women who, after a night of drinking, get into cabs of all kinds and are attacked, Shea said.\\n“They get driven, and passing out and waking up in a desolate area, and they get sexually attacked. This is something, really, that people need to be exceptionally aware of, and like any case in New York City, the buddy system works,” said Shea, referring to the need for people to travel in pairs when taking a cab at night.\\nBratton and police brass hope to build upon the continuing drop in overall crime by using technology such as ShotSpotter and a newly minted GPS system for police cars.\\nJessica Tisch, NYPD deputy commissioner for technology, said ShotSpotter, an acoustical system that detects gunfire, identified gunshots in 1,672 cases, mostly in Brooklyn. Of those alerts, 74 percent didn’t have any 911 calls from the public associated with them.\\nTisch said ShotSpotter helped police recover ballistic evidence in 19 percent of the gunfire alerts. In 22 percent of those cases, Tisch said, cops were able to make positive matches of bullets with those from guns used in earlier shootings.\\nTisch also highlighted a special GPS system being tried in about 5,000 patrol cars that allows the NYPD to see where its vehicles are and to track their movements over a 24-hour period, as well as gather information about the officers’ driving.\\n\", 'label': 0, 'input_ids': [2, 886, 476, 1162, 1570, 92493, 3368, 235269, 1051, 3462, 578, 189409, 6629, 9677, 235269, 1622, 3459, 3922, 2076, 4818, 6449, 12476, 7605, 235248, 235274, 235265, 235324, 5243, 575, 235248, 235284, 235276, 235274, 235308, 235269, 14995, 671, 8691, 19944, 674, 6343, 575, 573, 235248, 235274, 235315, 235315, 235276, 235256, 235269, 223960, 24229, 7130, 86048, 1166, 1180, 8778, 235265, 108, 2761, 476, 3905, 11982, 675, 24268, 8352, 581, 64248, 536, 235269, 86048, 1166, 166871, 2001, 1162, 235349, 235256, 12476, 15617, 235269, 948, 693, 1180, 235269, 1185, 11943, 675, 671, 1693, 8107, 19944, 575, 235248, 235284, 235276, 235274, 235310, 235269, 2507, 577, 2066, 573, 7440, 674, 18525, 34550, 8961, 235349, 235251, 5374, 1362, 573, 888, 11588, 696, 3922, 8095, 235265, 108, 235366, 10811, 783, 791, 1093, 1009, 103284, 235269, 1009, 12062, 575, 3383, 14486, 235269, 573, 8691, 8493, 575, 832, 1167, 12476, 14486, 11614, 577, 871, 1706, 2316, 86048, 1166, 4203, 39124, 235265, 1080, 1718, 729, 476, 1508, 1426, 1162, 604, 917, 235269, 235248, 235284, 235276, 235274, 235308, 235265, 108, 34594, 139089, 6629, 731, 235248, 235310, 235265, 235308, 5243, 575, 235248, 235284, 235276, 235274, 235308, 235269, 17066, 577, 235248, 235304, 235308, 235276, 774, 235248, 235304, 235304, 235304, 575, 573, 5556, 1162, 235269, 948, 729, 573, 18558, 2754, 235248, 235274, 235315, 235315, 235310, 235269, 1180, 27206, 24229, 5991, 28753, 102394, 235265, 16738, 484, 6629, 235248, 235318, 5243, 578, 189409, 8270, 235248, 235284, 5243, 235269, 1180, 102394, 235269, 1064, 603, 575, 6256, 576, 1423, 5488, 578, 8582, 604, 573, 223960, 235265, 108, 651, 4581, 8691, 12476, 15617, 3392, 1105, 3402, 577, 1212, 102394, 3151, 1080, 170040, 22600, 2316, 1570, 54528, 1501, 3614, 67648, 1693, 2862, 573, 8691, 1758, 576, 234270, 729, 573, 18558, 575, 573, 3413, 2754, 235248, 235284, 235276, 235276, 235304, 235265, 108, 9069, 194335, 2062, 39375, 578, 573, 83996, 2062, 4247, 4818, 6449, 25552, 4740, 731, 235248, 235304, 5243, 578, 235248, 235310, 5243, 235269, 11235, 235269, 102394, 1180, 235265, 39375, 235349, 235256, 4740, 729, 17779, 731, 978, 189409, 235269, 2183, 573, 83996, 235269, 7546, 9601, 671, 8691, 12476, 4740, 235269, 1093, 1212, 693, 1180, 729, 476, 1080, 201275, 492, 235369, 12025, 575, 116651, 235265, 3922, 7584, 235269, 116651, 1049, 1706, 575, 235248, 235284, 235276, 235274, 235308, 1105, 235248, 235304, 5243, 235269, 577, 235248, 235274, 235269, 235274, 235276, 235304, 774, 235248, 235274, 235269, 235274, 235324, 235284, 575, 235248, 235284, 235276, 235274, 235310, 235265, 108, 137056, 15863, 30665, 573, 235248, 235284, 235276, 235274, 235308, 4740, 575, 1051, 3462, 577, 20516, 5163, 4830, 675, 25109, 1105, 15502, 774, 1658, 3433, 235265, 108, 6736, 908, 577, 947, 573, 7776, 12921, 3512, 4890, 1311, 235303, 235256, 69499, 4890, 55588, 575, 861, 53207, 235265, 3339, 24212, 7555, 908, 235269, 692, 7147, 577, 1167, 16961, 5075, 235265, 108, 235366, 74843, 5243, 576, 1450, 1051, 3462, 3528, 235349, 235251, 7895, 575, 235248, 235284, 235276, 235274, 235308, 2316, 693, 1180, 235265, 108, 651, 223960, 919, 3624, 671, 4740, 575, 1051, 3462, 18416, 3821, 3576, 1064, 235269, 1452, 476, 3354, 576, 16036, 235269, 947, 1280, 188553, 576, 832, 13049, 578, 708, 25900, 235269, 102394, 1180, 235265, 108, 235366, 4508, 947, 17779, 235269, 578, 13230], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "first_record = tokenized_train_ds[0]\n",
    "print(first_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d7f4f8-9eb3-4feb-86e7-fb0dad88753f",
   "metadata": {},
   "source": [
    "### Turn on accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3eab29e-fa47-4a13-abc4-d6425ae741b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e040f1-e596-476d-9f26-ffa6f8a4a548",
   "metadata": {},
   "source": [
    "### LoRA - Low-Rank Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0532bdc6-4317-474b-859c-4e5d2fe6f1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd29494-4453-4c01-b878-ef2f4533d34d",
   "metadata": {},
   "source": [
    "### Inspect the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5058f6d-185d-45af-83b9-bb7f61d4bee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b14fe8f2-eccd-4cfe-9d20-ef48bc178842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 25008128 || all params: 4687158272 || trainable%: 0.5335456271957526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GemmaForSequenceClassification(\n",
       "      (model): GemmaModel(\n",
       "        (embed_tokens): Embedding(256000, 3072, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x GemmaDecoderLayer(\n",
       "            (self_attn): GemmaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): GemmaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): GemmaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=24576, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=24576, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=24576, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=24576, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=24576, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=24576, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): PytorchGELUTanh()\n",
       "            )\n",
       "            (input_layernorm): GemmaRMSNorm()\n",
       "            (post_attention_layernorm): GemmaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): GemmaRMSNorm()\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=3072, out_features=2, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=3072, out_features=2, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_trainable_parameters(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17b4782-03f8-4335-bfda-2a65794bff2c",
   "metadata": {},
   "source": [
    "### Look at hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c27c3328-1926-4adc-8696-b3b343ec4afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 1\n",
      "GPU 0: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdf68d0e-8786-489d-a4b8-b7478513efea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 27 09:12:27 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off |   00000000:01:00.0 Off |                  N/A |\n",
      "| 31%   29C    P8             27W /  420W |      10MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        Off |   00000000:02:00.0  On |                  Off |\n",
      "|  0%   47C    P2             72W /  450W |    6240MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 3090 Ti     Off |   00000000:2B:00.0 Off |                  Off |\n",
      "| 30%   35C    P8             24W /  450W |      10MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 3090        Off |   00000000:41:00.0 Off |                  N/A |\n",
      "| 32%   29C    P8             19W /  420W |      10MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA GeForce RTX 4090        Off |   00000000:42:00.0 Off |                  Off |\n",
      "|  0%   50C    P2             58W /  450W |    7429MiB /  24564MiB |     14%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA GeForce RTX 4090        Off |   00000000:61:00.0 Off |                  Off |\n",
      "|  0%   41C    P8             11W /  450W |      11MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA GeForce RTX 3090 Ti     Off |   00000000:62:00.0 Off |                  Off |\n",
      "| 30%   32C    P8             16W /  450W |      10MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      2513      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    1   N/A  N/A      2513      G   /usr/lib/xorg/Xorg                             54MiB |\n",
      "|    1   N/A  N/A      2596      G   /usr/bin/gnome-shell                           13MiB |\n",
      "|    1   N/A  N/A     16287      C   /usr/bin/python3                             6156MiB |\n",
      "|    2   N/A  N/A      2513      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    3   N/A  N/A      2513      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    4   N/A  N/A      2513      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    4   N/A  N/A     16301      C   /usr/bin/python3                             7412MiB |\n",
      "|    5   N/A  N/A      2513      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    6   N/A  N/A      2513      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2340f2d1-1cc7-454e-bad6-bf4ac2c46fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1199841a-6519-4422-8259-2fdbb114e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(logits_and_labels):\n",
    "    logits, labels = logits_and_labels\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='macro')\n",
    "    return {'accuracy': acc, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8c7f3ee-6997-4ed7-b28e-5d574831fb08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./praxis-gemma-7b-small-finetune'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_name = \"praxis-\"+model_name+\"-small-finetune\"\n",
    "output_dir_path = \"./\" + project_name\n",
    "output_dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7a69bde-1d50-46a0-9838-00812afbdb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f3cd41-fd17-4fd8-83b8-54f464df79ff",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2df4e2a5-2a08-434b-983a-f6cd42f33da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 09:12:27.925703: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-27 09:12:28.471743: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnispoe\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nispoe/kuk/Praxis/wandb/run-20240527_091230-nstbcjf4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nispoe/huggingface/runs/nstbcjf4' target=\"_blank\">silver-bird-345</a></strong> to <a href='https://wandb.ai/nispoe/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nispoe/huggingface' target=\"_blank\">https://wandb.ai/nispoe/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nispoe/huggingface/runs/nstbcjf4' target=\"_blank\">https://wandb.ai/nispoe/huggingface/runs/nstbcjf4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42020' max='42020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42020/42020 11:58:14, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015864</td>\n",
       "      <td>0.996946</td>\n",
       "      <td>0.996776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016234</td>\n",
       "      <td>0.997779</td>\n",
       "      <td>0.997653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016317</td>\n",
       "      <td>0.997640</td>\n",
       "      <td>0.997506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=42020, training_loss=0.0009709231199656848, metrics={'train_runtime': 43098.9199, 'train_samples_per_second': 7.799, 'train_steps_per_second': 0.975, 'total_flos': 8.029224386753987e+18, 'train_loss': 0.0009709231199656848, 'epoch': 10.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from pathlib import Path\n",
    "\n",
    "transformers.set_seed(777)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_ds,\n",
    "    eval_dataset=tokenized_eval_ds,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir_path,\n",
    "        num_train_epochs=10,\n",
    "        logging_steps=10,\n",
    "        logging_dir=output_dir_path+\"/logs\",\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        do_eval=True,\n",
    "    ),\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "trainer.train(resume_from_checkpoint=True)  # Turn to True if power goes out..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d13982d-53cb-4c88-bd32-a98d4a5a9874",
   "metadata": {},
   "source": [
    "### Determine best checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bd570c7-8feb-4b69-bda7-9c678bfd92c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 44\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 26 05:05 checkpoint-4202\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 26 09:04 checkpoint-8404\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 26 13:04 checkpoint-12606\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 26 17:04 checkpoint-16808\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 26 21:03 checkpoint-21010\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 27 01:03 checkpoint-25212\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 27 05:03 checkpoint-29414\n",
      "drwxr-xr-x 2 nispoe nispoe 4096 May 27 09:12 logs\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 27 13:12 checkpoint-33616\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 27 17:11 checkpoint-37818\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 27 21:10 checkpoint-42020\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr {output_dir_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97771269-d77c-4c1b-b264-34e082db6cbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading events from file: ./praxis-gemma-7b-small-finetune/logs/events.out.tfevents.1716819149.hephaestus.16301.0\n",
      "Step: 29420, train/loss: 0.0\n",
      "Step: 29420, train/grad_norm: 0.002455816837027669\n",
      "Step: 29420, train/learning_rate: 1.4992860997153912e-05\n",
      "Step: 29420, train/epoch: 7.001428127288818\n",
      "Step: 29430, train/loss: 0.0\n",
      "Step: 29430, train/grad_norm: 0.0018349956953898072\n",
      "Step: 29430, train/learning_rate: 1.4980961168475915e-05\n",
      "Step: 29430, train/epoch: 7.003807544708252\n",
      "Step: 29440, train/loss: 0.0\n",
      "Step: 29440, train/grad_norm: 0.0007117743953131139\n",
      "Step: 29440, train/learning_rate: 1.496906224929262e-05\n",
      "Step: 29440, train/epoch: 7.006187438964844\n",
      "Step: 29450, train/loss: 0.0\n",
      "Step: 29450, train/grad_norm: 0.000821841589640826\n",
      "Step: 29450, train/learning_rate: 1.4957163330109324e-05\n",
      "Step: 29450, train/epoch: 7.0085673332214355\n",
      "Step: 29460, train/loss: 0.0\n",
      "Step: 29460, train/grad_norm: 0.0014752702554687858\n",
      "Step: 29460, train/learning_rate: 1.4945264410926029e-05\n",
      "Step: 29460, train/epoch: 7.010947227478027\n",
      "Step: 29470, train/loss: 0.0\n",
      "Step: 29470, train/grad_norm: 0.0010123541578650475\n",
      "Step: 29470, train/learning_rate: 1.4933365491742734e-05\n",
      "Step: 29470, train/epoch: 7.013327121734619\n",
      "Step: 29480, train/loss: 0.0\n",
      "Step: 29480, train/grad_norm: 0.0023002787493169308\n",
      "Step: 29480, train/learning_rate: 1.4921465663064737e-05\n",
      "Step: 29480, train/epoch: 7.015707015991211\n",
      "Step: 29490, train/loss: 0.0\n",
      "Step: 29490, train/grad_norm: 0.0008340652566403151\n",
      "Step: 29490, train/learning_rate: 1.4909566743881442e-05\n",
      "Step: 29490, train/epoch: 7.0180864334106445\n",
      "Step: 29500, train/loss: 0.0\n",
      "Step: 29500, train/grad_norm: 0.001129317213781178\n",
      "Step: 29500, train/learning_rate: 1.4897667824698146e-05\n",
      "Step: 29500, train/epoch: 7.020466327667236\n",
      "Step: 29510, train/loss: 0.0\n",
      "Step: 29510, train/grad_norm: 0.0018486621556803584\n",
      "Step: 29510, train/learning_rate: 1.4885768905514851e-05\n",
      "Step: 29510, train/epoch: 7.022846221923828\n",
      "Step: 29520, train/loss: 0.0\n",
      "Step: 29520, train/grad_norm: 0.000836100138258189\n",
      "Step: 29520, train/learning_rate: 1.4873869986331556e-05\n",
      "Step: 29520, train/epoch: 7.02522611618042\n",
      "Step: 29530, train/loss: 0.0\n",
      "Step: 29530, train/grad_norm: 0.009369799867272377\n",
      "Step: 29530, train/learning_rate: 1.4861970157653559e-05\n",
      "Step: 29530, train/epoch: 7.027606010437012\n",
      "Step: 29540, train/loss: 0.0\n",
      "Step: 29540, train/grad_norm: 0.0005707202944904566\n",
      "Step: 29540, train/learning_rate: 1.4850071238470264e-05\n",
      "Step: 29540, train/epoch: 7.0299859046936035\n",
      "Step: 29550, train/loss: 0.0\n",
      "Step: 29550, train/grad_norm: 0.0003660855581983924\n",
      "Step: 29550, train/learning_rate: 1.4838172319286969e-05\n",
      "Step: 29550, train/epoch: 7.032365322113037\n",
      "Step: 29560, train/loss: 0.0\n",
      "Step: 29560, train/grad_norm: 0.00037436833372339606\n",
      "Step: 29560, train/learning_rate: 1.4826273400103673e-05\n",
      "Step: 29560, train/epoch: 7.034745216369629\n",
      "Step: 29570, train/loss: 0.006200000178068876\n",
      "Step: 29570, train/grad_norm: 0.0005805850960314274\n",
      "Step: 29570, train/learning_rate: 1.4814374480920378e-05\n",
      "Step: 29570, train/epoch: 7.037125110626221\n",
      "Step: 29580, train/loss: 0.0\n",
      "Step: 29580, train/grad_norm: 0.000598711019847542\n",
      "Step: 29580, train/learning_rate: 1.4802474652242381e-05\n",
      "Step: 29580, train/epoch: 7.0395050048828125\n",
      "Step: 29590, train/loss: 0.0\n",
      "Step: 29590, train/grad_norm: 0.0009074448607861996\n",
      "Step: 29590, train/learning_rate: 1.4790575733059086e-05\n",
      "Step: 29590, train/epoch: 7.041884899139404\n",
      "Step: 29600, train/loss: 0.0\n",
      "Step: 29600, train/grad_norm: 0.0013830374227836728\n",
      "Step: 29600, train/learning_rate: 1.477867681387579e-05\n",
      "Step: 29600, train/epoch: 7.044264793395996\n",
      "Step: 29610, train/loss: 0.0\n",
      "Step: 29610, train/grad_norm: 0.0011029054876416922\n",
      "Step: 29610, train/learning_rate: 1.4766777894692495e-05\n",
      "Step: 29610, train/epoch: 7.046644687652588\n",
      "Step: 29620, train/loss: 0.0\n",
      "Step: 29620, train/grad_norm: 0.0005675259162671864\n",
      "Step: 29620, train/learning_rate: 1.47548789755092e-05\n",
      "Step: 29620, train/epoch: 7.0490241050720215\n",
      "Step: 29630, train/loss: 0.0\n",
      "Step: 29630, train/grad_norm: 0.00044451860594563186\n",
      "Step: 29630, train/learning_rate: 1.4742979146831203e-05\n",
      "Step: 29630, train/epoch: 7.051403999328613\n",
      "Step: 29640, train/loss: 0.0\n",
      "Step: 29640, train/grad_norm: 0.0007064309320412576\n",
      "Step: 29640, train/learning_rate: 1.4731080227647908e-05\n",
      "Step: 29640, train/epoch: 7.053783893585205\n",
      "Step: 29650, train/loss: 0.0\n",
      "Step: 29650, train/grad_norm: 0.00035657623084262013\n",
      "Step: 29650, train/learning_rate: 1.4719181308464613e-05\n",
      "Step: 29650, train/epoch: 7.056163787841797\n",
      "Step: 29660, train/loss: 0.0\n",
      "Step: 29660, train/grad_norm: 0.000786792254075408\n",
      "Step: 29660, train/learning_rate: 1.4707282389281318e-05\n",
      "Step: 29660, train/epoch: 7.058543682098389\n",
      "Step: 29670, train/loss: 0.0\n",
      "Step: 29670, train/grad_norm: 0.00040038666338659823\n",
      "Step: 29670, train/learning_rate: 1.4695383470098022e-05\n",
      "Step: 29670, train/epoch: 7.0609235763549805\n",
      "Step: 29680, train/loss: 0.0\n",
      "Step: 29680, train/grad_norm: 0.0014831958105787635\n",
      "Step: 29680, train/learning_rate: 1.4683483641420025e-05\n",
      "Step: 29680, train/epoch: 7.063302993774414\n",
      "Step: 29690, train/loss: 0.0\n",
      "Step: 29690, train/grad_norm: 0.00041576381772756577\n",
      "Step: 29690, train/learning_rate: 1.467158472223673e-05\n",
      "Step: 29690, train/epoch: 7.065682888031006\n",
      "Step: 29700, train/loss: 0.0\n",
      "Step: 29700, train/grad_norm: 0.00021364516578614712\n",
      "Step: 29700, train/learning_rate: 1.4659685803053435e-05\n",
      "Step: 29700, train/epoch: 7.068062782287598\n",
      "Step: 29710, train/loss: 0.0\n",
      "Step: 29710, train/grad_norm: 0.00041870129643939435\n",
      "Step: 29710, train/learning_rate: 1.464778688387014e-05\n",
      "Step: 29710, train/epoch: 7.0704426765441895\n",
      "Step: 29720, train/loss: 0.0\n",
      "Step: 29720, train/grad_norm: 0.0001107064017560333\n",
      "Step: 29720, train/learning_rate: 1.4635887964686844e-05\n",
      "Step: 29720, train/epoch: 7.072822570800781\n",
      "Step: 29730, train/loss: 0.0\n",
      "Step: 29730, train/grad_norm: 0.00022350931249093264\n",
      "Step: 29730, train/learning_rate: 1.4623988136008848e-05\n",
      "Step: 29730, train/epoch: 7.075202465057373\n",
      "Step: 29740, train/loss: 0.0\n",
      "Step: 29740, train/grad_norm: 0.003911924082785845\n",
      "Step: 29740, train/learning_rate: 1.4612089216825552e-05\n",
      "Step: 29740, train/epoch: 7.077581882476807\n",
      "Step: 29750, train/loss: 0.0\n",
      "Step: 29750, train/grad_norm: 0.00033060761052183807\n",
      "Step: 29750, train/learning_rate: 1.4600190297642257e-05\n",
      "Step: 29750, train/epoch: 7.079961776733398\n",
      "Step: 29760, train/loss: 0.0\n",
      "Step: 29760, train/grad_norm: 0.0002645348431542516\n",
      "Step: 29760, train/learning_rate: 1.4588291378458962e-05\n",
      "Step: 29760, train/epoch: 7.08234167098999\n",
      "Step: 29770, train/loss: 0.08829999715089798\n",
      "Step: 29770, train/grad_norm: 0.0004953212337568402\n",
      "Step: 29770, train/learning_rate: 1.4576392459275667e-05\n",
      "Step: 29770, train/epoch: 7.084721565246582\n",
      "Step: 29780, train/loss: 9.999999747378752e-05\n",
      "Step: 29780, train/grad_norm: 0.00022046989761292934\n",
      "Step: 29780, train/learning_rate: 1.4564493540092371e-05\n",
      "Step: 29780, train/epoch: 7.087101459503174\n",
      "Step: 29790, train/loss: 0.0\n",
      "Step: 29790, train/grad_norm: 0.0038203608710318804\n",
      "Step: 29790, train/learning_rate: 1.4552593711414374e-05\n",
      "Step: 29790, train/epoch: 7.089481353759766\n",
      "Step: 29800, train/loss: 0.0\n",
      "Step: 29800, train/grad_norm: 0.000362729886546731\n",
      "Step: 29800, train/learning_rate: 1.454069479223108e-05\n",
      "Step: 29800, train/epoch: 7.091861248016357\n",
      "Step: 29810, train/loss: 0.0\n",
      "Step: 29810, train/grad_norm: 6.998465687502176e-05\n",
      "Step: 29810, train/learning_rate: 1.4528795873047784e-05\n",
      "Step: 29810, train/epoch: 7.094240665435791\n",
      "Step: 29820, train/loss: 0.0\n",
      "Step: 29820, train/grad_norm: 0.0003201723739039153\n",
      "Step: 29820, train/learning_rate: 1.4516896953864489e-05\n",
      "Step: 29820, train/epoch: 7.096620559692383\n",
      "Step: 29830, train/loss: 0.0\n",
      "Step: 29830, train/grad_norm: 0.00026951992185786366\n",
      "Step: 29830, train/learning_rate: 1.4504998034681194e-05\n",
      "Step: 29830, train/epoch: 7.099000453948975\n",
      "Step: 29840, train/loss: 0.0\n",
      "Step: 29840, train/grad_norm: 0.0001633977226447314\n",
      "Step: 29840, train/learning_rate: 1.4493098206003197e-05\n",
      "Step: 29840, train/epoch: 7.101380348205566\n",
      "Step: 29850, train/loss: 0.0\n",
      "Step: 29850, train/grad_norm: 0.0003745620488189161\n",
      "Step: 29850, train/learning_rate: 1.4481199286819901e-05\n",
      "Step: 29850, train/epoch: 7.103760242462158\n",
      "Step: 29860, train/loss: 0.0\n",
      "Step: 29860, train/grad_norm: 0.00015566486399620771\n",
      "Step: 29860, train/learning_rate: 1.4469300367636606e-05\n",
      "Step: 29860, train/epoch: 7.10614013671875\n",
      "Step: 29870, train/loss: 0.0\n",
      "Step: 29870, train/grad_norm: 0.0001898630434880033\n",
      "Step: 29870, train/learning_rate: 1.4457401448453311e-05\n",
      "Step: 29870, train/epoch: 7.108519554138184\n",
      "Step: 29880, train/loss: 0.0\n",
      "Step: 29880, train/grad_norm: 0.000745426572393626\n",
      "Step: 29880, train/learning_rate: 1.4445502529270016e-05\n",
      "Step: 29880, train/epoch: 7.110899448394775\n",
      "Step: 29890, train/loss: 0.0\n",
      "Step: 29890, train/grad_norm: 0.00035450627910904586\n",
      "Step: 29890, train/learning_rate: 1.4433602700592019e-05\n",
      "Step: 29890, train/epoch: 7.113279342651367\n",
      "Step: 29900, train/loss: 0.0\n",
      "Step: 29900, train/grad_norm: 0.00018548607476986945\n",
      "Step: 29900, train/learning_rate: 1.4421703781408723e-05\n",
      "Step: 29900, train/epoch: 7.115659236907959\n",
      "Step: 29910, train/loss: 0.0\n",
      "Step: 29910, train/grad_norm: 0.0011771629797294736\n",
      "Step: 29910, train/learning_rate: 1.4409804862225428e-05\n",
      "Step: 29910, train/epoch: 7.118039131164551\n",
      "Step: 29920, train/loss: 0.0\n",
      "Step: 29920, train/grad_norm: 0.0001395927247358486\n",
      "Step: 29920, train/learning_rate: 1.4397905943042133e-05\n",
      "Step: 29920, train/epoch: 7.120419025421143\n",
      "Step: 29930, train/loss: 0.0\n",
      "Step: 29930, train/grad_norm: 3.211578950867988e-05\n",
      "Step: 29930, train/learning_rate: 1.4386007023858838e-05\n",
      "Step: 29930, train/epoch: 7.122798442840576\n",
      "Step: 29940, train/loss: 0.0\n",
      "Step: 29940, train/grad_norm: 0.00011238074512220919\n",
      "Step: 29940, train/learning_rate: 1.437410719518084e-05\n",
      "Step: 29940, train/epoch: 7.125178337097168\n",
      "Step: 29950, train/loss: 0.0\n",
      "Step: 29950, train/grad_norm: 0.00015794490172993392\n",
      "Step: 29950, train/learning_rate: 1.4362208275997546e-05\n",
      "Step: 29950, train/epoch: 7.12755823135376\n",
      "Step: 29960, train/loss: 0.17659999430179596\n",
      "Step: 29960, train/grad_norm: 0.0004292743979021907\n",
      "Step: 29960, train/learning_rate: 1.435030935681425e-05\n",
      "Step: 29960, train/epoch: 7.129938125610352\n",
      "Step: 29970, train/loss: 0.0\n",
      "Step: 29970, train/grad_norm: 0.003648512763902545\n",
      "Step: 29970, train/learning_rate: 1.4338410437630955e-05\n",
      "Step: 29970, train/epoch: 7.132318019866943\n",
      "Step: 29980, train/loss: 0.0\n",
      "Step: 29980, train/grad_norm: 0.01654389128088951\n",
      "Step: 29980, train/learning_rate: 1.432651151844766e-05\n",
      "Step: 29980, train/epoch: 7.134697914123535\n",
      "Step: 29990, train/loss: 0.0\n",
      "Step: 29990, train/grad_norm: 0.005047524347901344\n",
      "Step: 29990, train/learning_rate: 1.4314611689769663e-05\n",
      "Step: 29990, train/epoch: 7.137077808380127\n",
      "Step: 30000, train/loss: 0.0\n",
      "Step: 30000, train/grad_norm: 0.002706053201109171\n",
      "Step: 30000, train/learning_rate: 1.4302712770586368e-05\n",
      "Step: 30000, train/epoch: 7.1394572257995605\n",
      "Step: 30010, train/loss: 0.0\n",
      "Step: 30010, train/grad_norm: 0.0022532129660248756\n",
      "Step: 30010, train/learning_rate: 1.4290813851403072e-05\n",
      "Step: 30010, train/epoch: 7.141837120056152\n",
      "Step: 30020, train/loss: 9.999999747378752e-05\n",
      "Step: 30020, train/grad_norm: 0.0011583724990487099\n",
      "Step: 30020, train/learning_rate: 1.4278914932219777e-05\n",
      "Step: 30020, train/epoch: 7.144217014312744\n",
      "Step: 30030, train/loss: 0.0\n",
      "Step: 30030, train/grad_norm: 0.0020272203255444765\n",
      "Step: 30030, train/learning_rate: 1.4267016013036482e-05\n",
      "Step: 30030, train/epoch: 7.146596908569336\n",
      "Step: 30040, train/loss: 0.0\n",
      "Step: 30040, train/grad_norm: 0.00028694194043055177\n",
      "Step: 30040, train/learning_rate: 1.4255116184358485e-05\n",
      "Step: 30040, train/epoch: 7.148976802825928\n",
      "Step: 30050, train/loss: 0.0\n",
      "Step: 30050, train/grad_norm: 0.0003461109008640051\n",
      "Step: 30050, train/learning_rate: 1.424321726517519e-05\n",
      "Step: 30050, train/epoch: 7.1513566970825195\n",
      "Step: 30060, train/loss: 0.0\n",
      "Step: 30060, train/grad_norm: 0.0007834018906578422\n",
      "Step: 30060, train/learning_rate: 1.4231318345991895e-05\n",
      "Step: 30060, train/epoch: 7.153736114501953\n",
      "Step: 30070, train/loss: 0.0\n",
      "Step: 30070, train/grad_norm: 0.000209409961826168\n",
      "Step: 30070, train/learning_rate: 1.42194194268086e-05\n",
      "Step: 30070, train/epoch: 7.156116008758545\n",
      "Step: 30080, train/loss: 0.0\n",
      "Step: 30080, train/grad_norm: 0.0002669597161002457\n",
      "Step: 30080, train/learning_rate: 1.4207520507625304e-05\n",
      "Step: 30080, train/epoch: 7.158495903015137\n",
      "Step: 30090, train/loss: 0.15389999747276306\n",
      "Step: 30090, train/grad_norm: 0.003498868318274617\n",
      "Step: 30090, train/learning_rate: 1.4195620678947307e-05\n",
      "Step: 30090, train/epoch: 7.1608757972717285\n",
      "Step: 30100, train/loss: 0.0\n",
      "Step: 30100, train/grad_norm: 0.0056426334194839\n",
      "Step: 30100, train/learning_rate: 1.4183721759764012e-05\n",
      "Step: 30100, train/epoch: 7.16325569152832\n",
      "Step: 30110, train/loss: 0.0\n",
      "Step: 30110, train/grad_norm: 0.14666035771369934\n",
      "Step: 30110, train/learning_rate: 1.4171822840580717e-05\n",
      "Step: 30110, train/epoch: 7.165635585784912\n",
      "Step: 30120, train/loss: 0.0\n",
      "Step: 30120, train/grad_norm: 0.0015814913203939795\n",
      "Step: 30120, train/learning_rate: 1.4159923921397422e-05\n",
      "Step: 30120, train/epoch: 7.168015003204346\n",
      "Step: 30130, train/loss: 0.0\n",
      "Step: 30130, train/grad_norm: 0.0032275356352329254\n",
      "Step: 30130, train/learning_rate: 1.4148025002214126e-05\n",
      "Step: 30130, train/epoch: 7.1703948974609375\n",
      "Step: 30140, train/loss: 0.0\n",
      "Step: 30140, train/grad_norm: 0.001530432840809226\n",
      "Step: 30140, train/learning_rate: 1.4136126083030831e-05\n",
      "Step: 30140, train/epoch: 7.172774791717529\n",
      "Step: 30150, train/loss: 0.0\n",
      "Step: 30150, train/grad_norm: 0.003659742185845971\n",
      "Step: 30150, train/learning_rate: 1.4124226254352834e-05\n",
      "Step: 30150, train/epoch: 7.175154685974121\n",
      "Step: 30160, train/loss: 0.0\n",
      "Step: 30160, train/grad_norm: 0.0013133945176377892\n",
      "Step: 30160, train/learning_rate: 1.4112327335169539e-05\n",
      "Step: 30160, train/epoch: 7.177534580230713\n",
      "Step: 30170, train/loss: 0.0\n",
      "Step: 30170, train/grad_norm: 0.0006771092885173857\n",
      "Step: 30170, train/learning_rate: 1.4100428415986244e-05\n",
      "Step: 30170, train/epoch: 7.179914474487305\n",
      "Step: 30180, train/loss: 0.0\n",
      "Step: 30180, train/grad_norm: 0.0036010844632983208\n",
      "Step: 30180, train/learning_rate: 1.4088529496802948e-05\n",
      "Step: 30180, train/epoch: 7.1822943687438965\n",
      "Step: 30190, train/loss: 0.0\n",
      "Step: 30190, train/grad_norm: 0.001388555159792304\n",
      "Step: 30190, train/learning_rate: 1.4076630577619653e-05\n",
      "Step: 30190, train/epoch: 7.18467378616333\n",
      "Step: 30200, train/loss: 0.0\n",
      "Step: 30200, train/grad_norm: 0.005648019257932901\n",
      "Step: 30200, train/learning_rate: 1.4064730748941656e-05\n",
      "Step: 30200, train/epoch: 7.187053680419922\n",
      "Step: 30210, train/loss: 0.0\n",
      "Step: 30210, train/grad_norm: 0.002262349473312497\n",
      "Step: 30210, train/learning_rate: 1.4052831829758361e-05\n",
      "Step: 30210, train/epoch: 7.189433574676514\n",
      "Step: 30220, train/loss: 9.999999747378752e-05\n",
      "Step: 30220, train/grad_norm: 0.0016268654726445675\n",
      "Step: 30220, train/learning_rate: 1.4040932910575066e-05\n",
      "Step: 30220, train/epoch: 7.1918134689331055\n",
      "Step: 30230, train/loss: 0.0\n",
      "Step: 30230, train/grad_norm: 0.02847922407090664\n",
      "Step: 30230, train/learning_rate: 1.402903399139177e-05\n",
      "Step: 30230, train/epoch: 7.194193363189697\n",
      "Step: 30240, train/loss: 0.0\n",
      "Step: 30240, train/grad_norm: 0.0007804649067111313\n",
      "Step: 30240, train/learning_rate: 1.4017135072208475e-05\n",
      "Step: 30240, train/epoch: 7.196573257446289\n",
      "Step: 30250, train/loss: 0.0003000000142492354\n",
      "Step: 30250, train/grad_norm: 0.0008365276153199375\n",
      "Step: 30250, train/learning_rate: 1.4005235243530478e-05\n",
      "Step: 30250, train/epoch: 7.198952674865723\n",
      "Step: 30260, train/loss: 0.13130000233650208\n",
      "Step: 30260, train/grad_norm: 0.0006198838818818331\n",
      "Step: 30260, train/learning_rate: 1.3993336324347183e-05\n",
      "Step: 30260, train/epoch: 7.2013325691223145\n",
      "Step: 30270, train/loss: 9.999999747378752e-05\n",
      "Step: 30270, train/grad_norm: 0.0018410866614431143\n",
      "Step: 30270, train/learning_rate: 1.3981437405163888e-05\n",
      "Step: 30270, train/epoch: 7.203712463378906\n",
      "Step: 30280, train/loss: 0.0\n",
      "Step: 30280, train/grad_norm: 0.0023783037904649973\n",
      "Step: 30280, train/learning_rate: 1.3969538485980593e-05\n",
      "Step: 30280, train/epoch: 7.206092357635498\n",
      "Step: 30290, train/loss: 0.0\n",
      "Step: 30290, train/grad_norm: 0.0012971102260053158\n",
      "Step: 30290, train/learning_rate: 1.3957639566797297e-05\n",
      "Step: 30290, train/epoch: 7.20847225189209\n",
      "Step: 30300, train/loss: 0.0\n",
      "Step: 30300, train/grad_norm: 0.0007991088205017149\n",
      "Step: 30300, train/learning_rate: 1.39457397381193e-05\n",
      "Step: 30300, train/epoch: 7.210852146148682\n",
      "Step: 30310, train/loss: 0.0\n",
      "Step: 30310, train/grad_norm: 0.0020025500562042\n",
      "Step: 30310, train/learning_rate: 1.3933840818936005e-05\n",
      "Step: 30310, train/epoch: 7.213231563568115\n",
      "Step: 30320, train/loss: 0.0\n",
      "Step: 30320, train/grad_norm: 0.0016128995921462774\n",
      "Step: 30320, train/learning_rate: 1.392194189975271e-05\n",
      "Step: 30320, train/epoch: 7.215611457824707\n",
      "Step: 30330, train/loss: 0.0\n",
      "Step: 30330, train/grad_norm: 0.0008770498679950833\n",
      "Step: 30330, train/learning_rate: 1.3910042980569415e-05\n",
      "Step: 30330, train/epoch: 7.217991352081299\n",
      "Step: 30340, train/loss: 0.0\n",
      "Step: 30340, train/grad_norm: 0.004607460927218199\n",
      "Step: 30340, train/learning_rate: 1.389814406138612e-05\n",
      "Step: 30340, train/epoch: 7.220371246337891\n",
      "Step: 30350, train/loss: 0.0\n",
      "Step: 30350, train/grad_norm: 0.0010785381309688091\n",
      "Step: 30350, train/learning_rate: 1.3886244232708123e-05\n",
      "Step: 30350, train/epoch: 7.222751140594482\n",
      "Step: 30360, train/loss: 0.0\n",
      "Step: 30360, train/grad_norm: 0.0008774719317443669\n",
      "Step: 30360, train/learning_rate: 1.3874345313524827e-05\n",
      "Step: 30360, train/epoch: 7.225131034851074\n",
      "Step: 30370, train/loss: 0.0\n",
      "Step: 30370, train/grad_norm: 0.010827133432030678\n",
      "Step: 30370, train/learning_rate: 1.3862446394341532e-05\n",
      "Step: 30370, train/epoch: 7.227510929107666\n",
      "Step: 30380, train/loss: 0.0\n",
      "Step: 30380, train/grad_norm: 0.0014336982276290655\n",
      "Step: 30380, train/learning_rate: 1.3850547475158237e-05\n",
      "Step: 30380, train/epoch: 7.2298903465271\n",
      "Step: 30390, train/loss: 0.0\n",
      "Step: 30390, train/grad_norm: 0.0006598694017156959\n",
      "Step: 30390, train/learning_rate: 1.3838648555974942e-05\n",
      "Step: 30390, train/epoch: 7.232270240783691\n",
      "Step: 30400, train/loss: 0.0\n",
      "Step: 30400, train/grad_norm: 0.0007643919088877738\n",
      "Step: 30400, train/learning_rate: 1.3826748727296945e-05\n",
      "Step: 30400, train/epoch: 7.234650135040283\n",
      "Step: 30410, train/loss: 0.0\n",
      "Step: 30410, train/grad_norm: 0.0017059248639270663\n",
      "Step: 30410, train/learning_rate: 1.381484980811365e-05\n",
      "Step: 30410, train/epoch: 7.237030029296875\n",
      "Step: 30420, train/loss: 0.04039999842643738\n",
      "Step: 30420, train/grad_norm: 0.0006180445197969675\n",
      "Step: 30420, train/learning_rate: 1.3802950888930354e-05\n",
      "Step: 30420, train/epoch: 7.239409923553467\n",
      "Step: 30430, train/loss: 0.0\n",
      "Step: 30430, train/grad_norm: 0.0006760538090020418\n",
      "Step: 30430, train/learning_rate: 1.3791051969747059e-05\n",
      "Step: 30430, train/epoch: 7.241789817810059\n",
      "Step: 30440, train/loss: 0.0\n",
      "Step: 30440, train/grad_norm: 0.0007561814854852855\n",
      "Step: 30440, train/learning_rate: 1.3779153050563764e-05\n",
      "Step: 30440, train/epoch: 7.244169235229492\n",
      "Step: 30450, train/loss: 0.0\n",
      "Step: 30450, train/grad_norm: 0.0017481762915849686\n",
      "Step: 30450, train/learning_rate: 1.3767254131380469e-05\n",
      "Step: 30450, train/epoch: 7.246549129486084\n",
      "Step: 30460, train/loss: 0.0\n",
      "Step: 30460, train/grad_norm: 0.0011961414711549878\n",
      "Step: 30460, train/learning_rate: 1.3755354302702472e-05\n",
      "Step: 30460, train/epoch: 7.248929023742676\n",
      "Step: 30470, train/loss: 0.0\n",
      "Step: 30470, train/grad_norm: 0.0004233159124851227\n",
      "Step: 30470, train/learning_rate: 1.3743455383519176e-05\n",
      "Step: 30470, train/epoch: 7.251308917999268\n",
      "Step: 30480, train/loss: 9.999999747378752e-05\n",
      "Step: 30480, train/grad_norm: 0.0003918973670806736\n",
      "Step: 30480, train/learning_rate: 1.3731556464335881e-05\n",
      "Step: 30480, train/epoch: 7.253688812255859\n",
      "Step: 30490, train/loss: 0.0\n",
      "Step: 30490, train/grad_norm: 0.005272515583783388\n",
      "Step: 30490, train/learning_rate: 1.3719657545152586e-05\n",
      "Step: 30490, train/epoch: 7.256068706512451\n",
      "Step: 30500, train/loss: 0.0\n",
      "Step: 30500, train/grad_norm: 0.0008265984361059964\n",
      "Step: 30500, train/learning_rate: 1.370775862596929e-05\n",
      "Step: 30500, train/epoch: 7.258448123931885\n",
      "Step: 30510, train/loss: 0.0\n",
      "Step: 30510, train/grad_norm: 0.00023371017596218735\n",
      "Step: 30510, train/learning_rate: 1.3695858797291294e-05\n",
      "Step: 30510, train/epoch: 7.260828018188477\n",
      "Step: 30520, train/loss: 0.0\n",
      "Step: 30520, train/grad_norm: 0.00047685502795502543\n",
      "Step: 30520, train/learning_rate: 1.3683959878107999e-05\n",
      "Step: 30520, train/epoch: 7.263207912445068\n",
      "Step: 30530, train/loss: 0.0\n",
      "Step: 30530, train/grad_norm: 0.0004173823108430952\n",
      "Step: 30530, train/learning_rate: 1.3672060958924703e-05\n",
      "Step: 30530, train/epoch: 7.26558780670166\n",
      "Step: 30540, train/loss: 0.0\n",
      "Step: 30540, train/grad_norm: 0.0002091998903779313\n",
      "Step: 30540, train/learning_rate: 1.3660162039741408e-05\n",
      "Step: 30540, train/epoch: 7.267967700958252\n",
      "Step: 30550, train/loss: 0.16019999980926514\n",
      "Step: 30550, train/grad_norm: 0.0007852786220610142\n",
      "Step: 30550, train/learning_rate: 1.3648263120558113e-05\n",
      "Step: 30550, train/epoch: 7.270347595214844\n",
      "Step: 30560, train/loss: 0.0\n",
      "Step: 30560, train/grad_norm: 0.0006267228163778782\n",
      "Step: 30560, train/learning_rate: 1.3636363291880116e-05\n",
      "Step: 30560, train/epoch: 7.2727274894714355\n",
      "Step: 30570, train/loss: 0.0\n",
      "Step: 30570, train/grad_norm: 0.006449514999985695\n",
      "Step: 30570, train/learning_rate: 1.362446437269682e-05\n",
      "Step: 30570, train/epoch: 7.275106906890869\n",
      "Step: 30580, train/loss: 0.0\n",
      "Step: 30580, train/grad_norm: 0.015003512613475323\n",
      "Step: 30580, train/learning_rate: 1.3612565453513525e-05\n",
      "Step: 30580, train/epoch: 7.277486801147461\n",
      "Step: 30590, train/loss: 0.0\n",
      "Step: 30590, train/grad_norm: 0.0009213443845510483\n",
      "Step: 30590, train/learning_rate: 1.360066653433023e-05\n",
      "Step: 30590, train/epoch: 7.279866695404053\n",
      "Step: 30600, train/loss: 0.0\n",
      "Step: 30600, train/grad_norm: 0.0011973276268690825\n",
      "Step: 30600, train/learning_rate: 1.3588767615146935e-05\n",
      "Step: 30600, train/epoch: 7.2822465896606445\n",
      "Step: 30610, train/loss: 0.0\n",
      "Step: 30610, train/grad_norm: 0.002159549854695797\n",
      "Step: 30610, train/learning_rate: 1.3576867786468938e-05\n",
      "Step: 30610, train/epoch: 7.284626483917236\n",
      "Step: 30620, train/loss: 0.0\n",
      "Step: 30620, train/grad_norm: 0.005058066453784704\n",
      "Step: 30620, train/learning_rate: 1.3564968867285643e-05\n",
      "Step: 30620, train/epoch: 7.287006378173828\n",
      "Step: 30630, train/loss: 0.06129999831318855\n",
      "Step: 30630, train/grad_norm: 0.010903244838118553\n",
      "Step: 30630, train/learning_rate: 1.3553069948102348e-05\n",
      "Step: 30630, train/epoch: 7.289385795593262\n",
      "Step: 30640, train/loss: 0.0\n",
      "Step: 30640, train/grad_norm: 0.018417418003082275\n",
      "Step: 30640, train/learning_rate: 1.3541171028919052e-05\n",
      "Step: 30640, train/epoch: 7.2917656898498535\n",
      "Step: 30650, train/loss: 0.0\n",
      "Step: 30650, train/grad_norm: 0.09367484599351883\n",
      "Step: 30650, train/learning_rate: 1.3529272109735757e-05\n",
      "Step: 30650, train/epoch: 7.294145584106445\n",
      "Step: 30660, train/loss: 0.0\n",
      "Step: 30660, train/grad_norm: 0.03302156180143356\n",
      "Step: 30660, train/learning_rate: 1.351737228105776e-05\n",
      "Step: 30660, train/epoch: 7.296525478363037\n",
      "Step: 30670, train/loss: 0.0\n",
      "Step: 30670, train/grad_norm: 0.003979514352977276\n",
      "Step: 30670, train/learning_rate: 1.3505473361874465e-05\n",
      "Step: 30670, train/epoch: 7.298905372619629\n",
      "Step: 30680, train/loss: 0.0\n",
      "Step: 30680, train/grad_norm: 0.002046271227300167\n",
      "Step: 30680, train/learning_rate: 1.349357444269117e-05\n",
      "Step: 30680, train/epoch: 7.301285266876221\n",
      "Step: 30690, train/loss: 0.0\n",
      "Step: 30690, train/grad_norm: 0.0003611542342696339\n",
      "Step: 30690, train/learning_rate: 1.3481675523507874e-05\n",
      "Step: 30690, train/epoch: 7.303664684295654\n",
      "Step: 30700, train/loss: 0.0\n",
      "Step: 30700, train/grad_norm: 0.002684173407033086\n",
      "Step: 30700, train/learning_rate: 1.346977660432458e-05\n",
      "Step: 30700, train/epoch: 7.306044578552246\n",
      "Step: 30710, train/loss: 0.0\n",
      "Step: 30710, train/grad_norm: 0.00029964937129989266\n",
      "Step: 30710, train/learning_rate: 1.3457876775646582e-05\n",
      "Step: 30710, train/epoch: 7.308424472808838\n",
      "Step: 30720, train/loss: 0.11479999870061874\n",
      "Step: 30720, train/grad_norm: 0.008851789869368076\n",
      "Step: 30720, train/learning_rate: 1.3445977856463287e-05\n",
      "Step: 30720, train/epoch: 7.31080436706543\n",
      "Step: 30730, train/loss: 0.0\n",
      "Step: 30730, train/grad_norm: 0.005312711000442505\n",
      "Step: 30730, train/learning_rate: 1.3434078937279992e-05\n",
      "Step: 30730, train/epoch: 7.3131842613220215\n",
      "Step: 30740, train/loss: 0.0\n",
      "Step: 30740, train/grad_norm: 0.0020053114276379347\n",
      "Step: 30740, train/learning_rate: 1.3422180018096697e-05\n",
      "Step: 30740, train/epoch: 7.315564155578613\n",
      "Step: 30750, train/loss: 0.0\n",
      "Step: 30750, train/grad_norm: 0.0034014054108411074\n",
      "Step: 30750, train/learning_rate: 1.3410281098913401e-05\n",
      "Step: 30750, train/epoch: 7.317944049835205\n",
      "Step: 30760, train/loss: 0.0\n",
      "Step: 30760, train/grad_norm: 0.002354431664571166\n",
      "Step: 30760, train/learning_rate: 1.3398381270235404e-05\n",
      "Step: 30760, train/epoch: 7.320323467254639\n",
      "Step: 30770, train/loss: 0.0\n",
      "Step: 30770, train/grad_norm: 0.0016771849477663636\n",
      "Step: 30770, train/learning_rate: 1.3386482351052109e-05\n",
      "Step: 30770, train/epoch: 7.3227033615112305\n",
      "Step: 30780, train/loss: 0.0\n",
      "Step: 30780, train/grad_norm: 0.0018932936945930123\n",
      "Step: 30780, train/learning_rate: 1.3374583431868814e-05\n",
      "Step: 30780, train/epoch: 7.325083255767822\n",
      "Step: 30790, train/loss: 0.0\n",
      "Step: 30790, train/grad_norm: 0.0018297008937224746\n",
      "Step: 30790, train/learning_rate: 1.3362684512685519e-05\n",
      "Step: 30790, train/epoch: 7.327463150024414\n",
      "Step: 30800, train/loss: 0.0\n",
      "Step: 30800, train/grad_norm: 0.00045086132013238966\n",
      "Step: 30800, train/learning_rate: 1.3350785593502223e-05\n",
      "Step: 30800, train/epoch: 7.329843044281006\n",
      "Step: 30810, train/loss: 0.0\n",
      "Step: 30810, train/grad_norm: 0.0019023003987967968\n",
      "Step: 30810, train/learning_rate: 1.3338886674318928e-05\n",
      "Step: 30810, train/epoch: 7.332222938537598\n",
      "Step: 30820, train/loss: 0.0\n",
      "Step: 30820, train/grad_norm: 0.0007884370279498398\n",
      "Step: 30820, train/learning_rate: 1.3326986845640931e-05\n",
      "Step: 30820, train/epoch: 7.334602355957031\n",
      "Step: 30830, train/loss: 0.0\n",
      "Step: 30830, train/grad_norm: 0.0004214484943076968\n",
      "Step: 30830, train/learning_rate: 1.3315087926457636e-05\n",
      "Step: 30830, train/epoch: 7.336982250213623\n",
      "Step: 30840, train/loss: 0.0\n",
      "Step: 30840, train/grad_norm: 0.000894534052349627\n",
      "Step: 30840, train/learning_rate: 1.330318900727434e-05\n",
      "Step: 30840, train/epoch: 7.339362144470215\n",
      "Step: 30850, train/loss: 0.0\n",
      "Step: 30850, train/grad_norm: 0.0011448996374383569\n",
      "Step: 30850, train/learning_rate: 1.3291290088091046e-05\n",
      "Step: 30850, train/epoch: 7.341742038726807\n",
      "Step: 30860, train/loss: 0.0\n",
      "Step: 30860, train/grad_norm: 0.0010538182687014341\n",
      "Step: 30860, train/learning_rate: 1.327939116890775e-05\n",
      "Step: 30860, train/epoch: 7.344121932983398\n",
      "Step: 30870, train/loss: 0.0\n",
      "Step: 30870, train/grad_norm: 0.0009586006053723395\n",
      "Step: 30870, train/learning_rate: 1.3267491340229753e-05\n",
      "Step: 30870, train/epoch: 7.34650182723999\n",
      "Step: 30880, train/loss: 0.0\n",
      "Step: 30880, train/grad_norm: 0.0009142650524154305\n",
      "Step: 30880, train/learning_rate: 1.3255592421046458e-05\n",
      "Step: 30880, train/epoch: 7.348881721496582\n",
      "Step: 30890, train/loss: 0.0\n",
      "Step: 30890, train/grad_norm: 0.000569972035009414\n",
      "Step: 30890, train/learning_rate: 1.3243693501863163e-05\n",
      "Step: 30890, train/epoch: 7.351261138916016\n",
      "Step: 30900, train/loss: 0.0\n",
      "Step: 30900, train/grad_norm: 0.0004891882999800146\n",
      "Step: 30900, train/learning_rate: 1.3231794582679868e-05\n",
      "Step: 30900, train/epoch: 7.353641033172607\n",
      "Step: 30910, train/loss: 0.0\n",
      "Step: 30910, train/grad_norm: 0.0005873282789252698\n",
      "Step: 30910, train/learning_rate: 1.3219895663496573e-05\n",
      "Step: 30910, train/epoch: 7.356020927429199\n",
      "Step: 30920, train/loss: 0.0\n",
      "Step: 30920, train/grad_norm: 0.0006172285648062825\n",
      "Step: 30920, train/learning_rate: 1.3207995834818576e-05\n",
      "Step: 30920, train/epoch: 7.358400821685791\n",
      "Step: 30930, train/loss: 0.0\n",
      "Step: 30930, train/grad_norm: 0.0005220616003498435\n",
      "Step: 30930, train/learning_rate: 1.319609691563528e-05\n",
      "Step: 30930, train/epoch: 7.360780715942383\n",
      "Step: 30940, train/loss: 0.0\n",
      "Step: 30940, train/grad_norm: 0.0019523799419403076\n",
      "Step: 30940, train/learning_rate: 1.3184197996451985e-05\n",
      "Step: 30940, train/epoch: 7.363160610198975\n",
      "Step: 30950, train/loss: 0.0\n",
      "Step: 30950, train/grad_norm: 0.0004901003558188677\n",
      "Step: 30950, train/learning_rate: 1.317229907726869e-05\n",
      "Step: 30950, train/epoch: 7.365540027618408\n",
      "Step: 30960, train/loss: 0.0\n",
      "Step: 30960, train/grad_norm: 0.0008686946239322424\n",
      "Step: 30960, train/learning_rate: 1.3160400158085395e-05\n",
      "Step: 30960, train/epoch: 7.367919921875\n",
      "Step: 30970, train/loss: 0.12110000103712082\n",
      "Step: 30970, train/grad_norm: 0.001167686190456152\n",
      "Step: 30970, train/learning_rate: 1.3148500329407398e-05\n",
      "Step: 30970, train/epoch: 7.370299816131592\n",
      "Step: 30980, train/loss: 9.999999747378752e-05\n",
      "Step: 30980, train/grad_norm: 0.1011781096458435\n",
      "Step: 30980, train/learning_rate: 1.3136601410224102e-05\n",
      "Step: 30980, train/epoch: 7.372679710388184\n",
      "Step: 30990, train/loss: 0.0\n",
      "Step: 30990, train/grad_norm: 0.004869232419878244\n",
      "Step: 30990, train/learning_rate: 1.3124702491040807e-05\n",
      "Step: 30990, train/epoch: 7.375059604644775\n",
      "Step: 31000, train/loss: 0.0\n",
      "Step: 31000, train/grad_norm: 0.010006624273955822\n",
      "Step: 31000, train/learning_rate: 1.3112803571857512e-05\n",
      "Step: 31000, train/epoch: 7.377439498901367\n",
      "Step: 31010, train/loss: 0.0\n",
      "Step: 31010, train/grad_norm: 0.002175053581595421\n",
      "Step: 31010, train/learning_rate: 1.3100904652674217e-05\n",
      "Step: 31010, train/epoch: 7.379818916320801\n",
      "Step: 31020, train/loss: 0.0\n",
      "Step: 31020, train/grad_norm: 0.0011907039443030953\n",
      "Step: 31020, train/learning_rate: 1.308900482399622e-05\n",
      "Step: 31020, train/epoch: 7.382198810577393\n",
      "Step: 31030, train/loss: 0.0\n",
      "Step: 31030, train/grad_norm: 0.0012812556233257055\n",
      "Step: 31030, train/learning_rate: 1.3077105904812925e-05\n",
      "Step: 31030, train/epoch: 7.384578704833984\n",
      "Step: 31040, train/loss: 0.0\n",
      "Step: 31040, train/grad_norm: 0.0031783757731318474\n",
      "Step: 31040, train/learning_rate: 1.306520698562963e-05\n",
      "Step: 31040, train/epoch: 7.386958599090576\n",
      "Step: 31050, train/loss: 0.0\n",
      "Step: 31050, train/grad_norm: 0.0009943107143044472\n",
      "Step: 31050, train/learning_rate: 1.3053308066446334e-05\n",
      "Step: 31050, train/epoch: 7.389338493347168\n",
      "Step: 31060, train/loss: 0.0\n",
      "Step: 31060, train/grad_norm: 0.0008324136142618954\n",
      "Step: 31060, train/learning_rate: 1.3041409147263039e-05\n",
      "Step: 31060, train/epoch: 7.39171838760376\n",
      "Step: 31070, train/loss: 0.0\n",
      "Step: 31070, train/grad_norm: 0.0004913193988613784\n",
      "Step: 31070, train/learning_rate: 1.3029509318585042e-05\n",
      "Step: 31070, train/epoch: 7.394098281860352\n",
      "Step: 31080, train/loss: 0.0\n",
      "Step: 31080, train/grad_norm: 0.0004854426661040634\n",
      "Step: 31080, train/learning_rate: 1.3017610399401747e-05\n",
      "Step: 31080, train/epoch: 7.396477699279785\n",
      "Step: 31090, train/loss: 0.0\n",
      "Step: 31090, train/grad_norm: 0.0008442034013569355\n",
      "Step: 31090, train/learning_rate: 1.3005711480218451e-05\n",
      "Step: 31090, train/epoch: 7.398857593536377\n",
      "Step: 31100, train/loss: 0.0\n",
      "Step: 31100, train/grad_norm: 0.001230244874022901\n",
      "Step: 31100, train/learning_rate: 1.2993812561035156e-05\n",
      "Step: 31100, train/epoch: 7.401237487792969\n",
      "Step: 31110, train/loss: 0.0\n",
      "Step: 31110, train/grad_norm: 0.0022604886908084154\n",
      "Step: 31110, train/learning_rate: 1.2981913641851861e-05\n",
      "Step: 31110, train/epoch: 7.4036173820495605\n",
      "Step: 31120, train/loss: 0.0\n",
      "Step: 31120, train/grad_norm: 0.0010603269329294562\n",
      "Step: 31120, train/learning_rate: 1.2970014722668566e-05\n",
      "Step: 31120, train/epoch: 7.405997276306152\n",
      "Step: 31130, train/loss: 0.0\n",
      "Step: 31130, train/grad_norm: 0.001988583244383335\n",
      "Step: 31130, train/learning_rate: 1.2958114893990569e-05\n",
      "Step: 31130, train/epoch: 7.408377170562744\n",
      "Step: 31140, train/loss: 0.0\n",
      "Step: 31140, train/grad_norm: 0.0007733763777650893\n",
      "Step: 31140, train/learning_rate: 1.2946215974807274e-05\n",
      "Step: 31140, train/epoch: 7.410756587982178\n",
      "Step: 31150, train/loss: 0.0\n",
      "Step: 31150, train/grad_norm: 0.0003738780796993524\n",
      "Step: 31150, train/learning_rate: 1.2934317055623978e-05\n",
      "Step: 31150, train/epoch: 7.4131364822387695\n",
      "Step: 31160, train/loss: 0.0\n",
      "Step: 31160, train/grad_norm: 0.0005071110790595412\n",
      "Step: 31160, train/learning_rate: 1.2922418136440683e-05\n",
      "Step: 31160, train/epoch: 7.415516376495361\n",
      "Step: 31170, train/loss: 0.0\n",
      "Step: 31170, train/grad_norm: 0.0002752564905676991\n",
      "Step: 31170, train/learning_rate: 1.2910519217257388e-05\n",
      "Step: 31170, train/epoch: 7.417896270751953\n",
      "Step: 31180, train/loss: 0.0\n",
      "Step: 31180, train/grad_norm: 0.0002024795685429126\n",
      "Step: 31180, train/learning_rate: 1.2898619388579391e-05\n",
      "Step: 31180, train/epoch: 7.420276165008545\n",
      "Step: 31190, train/loss: 0.0\n",
      "Step: 31190, train/grad_norm: 0.0016381216701120138\n",
      "Step: 31190, train/learning_rate: 1.2886720469396096e-05\n",
      "Step: 31190, train/epoch: 7.422656059265137\n",
      "Step: 31200, train/loss: 0.0\n",
      "Step: 31200, train/grad_norm: 0.000250770099228248\n",
      "Step: 31200, train/learning_rate: 1.28748215502128e-05\n",
      "Step: 31200, train/epoch: 7.42503547668457\n",
      "Step: 31210, train/loss: 0.0\n",
      "Step: 31210, train/grad_norm: 0.0006341824191622436\n",
      "Step: 31210, train/learning_rate: 1.2862922631029505e-05\n",
      "Step: 31210, train/epoch: 7.427415370941162\n",
      "Step: 31220, train/loss: 0.0\n",
      "Step: 31220, train/grad_norm: 0.00015621686179656535\n",
      "Step: 31220, train/learning_rate: 1.285102371184621e-05\n",
      "Step: 31220, train/epoch: 7.429795265197754\n",
      "Step: 31230, train/loss: 0.0\n",
      "Step: 31230, train/grad_norm: 0.0002555303799454123\n",
      "Step: 31230, train/learning_rate: 1.2839123883168213e-05\n",
      "Step: 31230, train/epoch: 7.432175159454346\n",
      "Step: 31240, train/loss: 0.0\n",
      "Step: 31240, train/grad_norm: 0.00688701868057251\n",
      "Step: 31240, train/learning_rate: 1.2827224963984918e-05\n",
      "Step: 31240, train/epoch: 7.4345550537109375\n",
      "Step: 31250, train/loss: 0.0\n",
      "Step: 31250, train/grad_norm: 0.0006556394509971142\n",
      "Step: 31250, train/learning_rate: 1.2815326044801623e-05\n",
      "Step: 31250, train/epoch: 7.436934947967529\n",
      "Step: 31260, train/loss: 0.0\n",
      "Step: 31260, train/grad_norm: 0.00014457835641223937\n",
      "Step: 31260, train/learning_rate: 1.2803427125618327e-05\n",
      "Step: 31260, train/epoch: 7.439314842224121\n",
      "Step: 31270, train/loss: 0.0\n",
      "Step: 31270, train/grad_norm: 0.000440897885710001\n",
      "Step: 31270, train/learning_rate: 1.2791528206435032e-05\n",
      "Step: 31270, train/epoch: 7.441694259643555\n",
      "Step: 31280, train/loss: 0.0\n",
      "Step: 31280, train/grad_norm: 0.0013142118696123362\n",
      "Step: 31280, train/learning_rate: 1.2779628377757035e-05\n",
      "Step: 31280, train/epoch: 7.4440741539001465\n",
      "Step: 31290, train/loss: 0.0\n",
      "Step: 31290, train/grad_norm: 0.00020945013966411352\n",
      "Step: 31290, train/learning_rate: 1.276772945857374e-05\n",
      "Step: 31290, train/epoch: 7.446454048156738\n",
      "Step: 31300, train/loss: 0.0\n",
      "Step: 31300, train/grad_norm: 0.0001700339635135606\n",
      "Step: 31300, train/learning_rate: 1.2755830539390445e-05\n",
      "Step: 31300, train/epoch: 7.44883394241333\n",
      "Step: 31310, train/loss: 0.0\n",
      "Step: 31310, train/grad_norm: 0.00041257572593167424\n",
      "Step: 31310, train/learning_rate: 1.274393162020715e-05\n",
      "Step: 31310, train/epoch: 7.451213836669922\n",
      "Step: 31320, train/loss: 0.00559999980032444\n",
      "Step: 31320, train/grad_norm: 0.0003773922217078507\n",
      "Step: 31320, train/learning_rate: 1.2732032701023854e-05\n",
      "Step: 31320, train/epoch: 7.453593730926514\n",
      "Step: 31330, train/loss: 0.0\n",
      "Step: 31330, train/grad_norm: 0.0024124193005263805\n",
      "Step: 31330, train/learning_rate: 1.2720132872345857e-05\n",
      "Step: 31330, train/epoch: 7.455973148345947\n",
      "Step: 31340, train/loss: 0.0\n",
      "Step: 31340, train/grad_norm: 0.0028807411435991526\n",
      "Step: 31340, train/learning_rate: 1.2708233953162562e-05\n",
      "Step: 31340, train/epoch: 7.458353042602539\n",
      "Step: 31350, train/loss: 0.0\n",
      "Step: 31350, train/grad_norm: 0.001742217456921935\n",
      "Step: 31350, train/learning_rate: 1.2696335033979267e-05\n",
      "Step: 31350, train/epoch: 7.460732936859131\n",
      "Step: 31360, train/loss: 0.13130000233650208\n",
      "Step: 31360, train/grad_norm: 0.0003776574449148029\n",
      "Step: 31360, train/learning_rate: 1.2684436114795972e-05\n",
      "Step: 31360, train/epoch: 7.463112831115723\n",
      "Step: 31370, train/loss: 0.0\n",
      "Step: 31370, train/grad_norm: 0.0018091966630890965\n",
      "Step: 31370, train/learning_rate: 1.2672537195612676e-05\n",
      "Step: 31370, train/epoch: 7.4654927253723145\n",
      "Step: 31380, train/loss: 0.0\n",
      "Step: 31380, train/grad_norm: 0.0005776152247563004\n",
      "Step: 31380, train/learning_rate: 1.266063736693468e-05\n",
      "Step: 31380, train/epoch: 7.467872619628906\n",
      "Step: 31390, train/loss: 0.08829999715089798\n",
      "Step: 31390, train/grad_norm: 0.002008718904107809\n",
      "Step: 31390, train/learning_rate: 1.2648738447751384e-05\n",
      "Step: 31390, train/epoch: 7.47025203704834\n",
      "Step: 31400, train/loss: 0.0\n",
      "Step: 31400, train/grad_norm: 0.001419247011654079\n",
      "Step: 31400, train/learning_rate: 1.2636839528568089e-05\n",
      "Step: 31400, train/epoch: 7.472631931304932\n",
      "Step: 31410, train/loss: 0.0\n",
      "Step: 31410, train/grad_norm: 0.0029311818070709705\n",
      "Step: 31410, train/learning_rate: 1.2624940609384794e-05\n",
      "Step: 31410, train/epoch: 7.475011825561523\n",
      "Step: 31420, train/loss: 0.0\n",
      "Step: 31420, train/grad_norm: 0.05654856562614441\n",
      "Step: 31420, train/learning_rate: 1.2613041690201499e-05\n",
      "Step: 31420, train/epoch: 7.477391719818115\n",
      "Step: 31430, train/loss: 0.0\n",
      "Step: 31430, train/grad_norm: 0.0021770114544779062\n",
      "Step: 31430, train/learning_rate: 1.2601141861523502e-05\n",
      "Step: 31430, train/epoch: 7.479771614074707\n",
      "Step: 31440, train/loss: 0.0\n",
      "Step: 31440, train/grad_norm: 0.0004806443757843226\n",
      "Step: 31440, train/learning_rate: 1.2589242942340206e-05\n",
      "Step: 31440, train/epoch: 7.482151508331299\n",
      "Step: 31450, train/loss: 0.0\n",
      "Step: 31450, train/grad_norm: 0.0008330577984452248\n",
      "Step: 31450, train/learning_rate: 1.2577344023156911e-05\n",
      "Step: 31450, train/epoch: 7.484531402587891\n",
      "Step: 31460, train/loss: 0.0\n",
      "Step: 31460, train/grad_norm: 0.0004257466644048691\n",
      "Step: 31460, train/learning_rate: 1.2565445103973616e-05\n",
      "Step: 31460, train/epoch: 7.486910820007324\n",
      "Step: 31470, train/loss: 0.0\n",
      "Step: 31470, train/grad_norm: 0.0015768540324643254\n",
      "Step: 31470, train/learning_rate: 1.255354618479032e-05\n",
      "Step: 31470, train/epoch: 7.489290714263916\n",
      "Step: 31480, train/loss: 0.0\n",
      "Step: 31480, train/grad_norm: 0.0004206117009744048\n",
      "Step: 31480, train/learning_rate: 1.2541647265607025e-05\n",
      "Step: 31480, train/epoch: 7.491670608520508\n",
      "Step: 31490, train/loss: 0.0\n",
      "Step: 31490, train/grad_norm: 0.0005671869730576873\n",
      "Step: 31490, train/learning_rate: 1.2529747436929028e-05\n",
      "Step: 31490, train/epoch: 7.4940505027771\n",
      "Step: 31500, train/loss: 0.0\n",
      "Step: 31500, train/grad_norm: 0.0017708188388496637\n",
      "Step: 31500, train/learning_rate: 1.2517848517745733e-05\n",
      "Step: 31500, train/epoch: 7.496430397033691\n",
      "Step: 31510, train/loss: 0.0\n",
      "Step: 31510, train/grad_norm: 0.00014746416127309203\n",
      "Step: 31510, train/learning_rate: 1.2505949598562438e-05\n",
      "Step: 31510, train/epoch: 7.498810291290283\n",
      "Step: 31520, train/loss: 0.0\n",
      "Step: 31520, train/grad_norm: 0.00010969454160658643\n",
      "Step: 31520, train/learning_rate: 1.2494050679379143e-05\n",
      "Step: 31520, train/epoch: 7.501189708709717\n",
      "Step: 31530, train/loss: 0.0\n",
      "Step: 31530, train/grad_norm: 0.0012503439793363214\n",
      "Step: 31530, train/learning_rate: 1.2482151760195848e-05\n",
      "Step: 31530, train/epoch: 7.503569602966309\n",
      "Step: 31540, train/loss: 0.0\n",
      "Step: 31540, train/grad_norm: 0.0005681951297447085\n",
      "Step: 31540, train/learning_rate: 1.247025193151785e-05\n",
      "Step: 31540, train/epoch: 7.5059494972229\n",
      "Step: 31550, train/loss: 0.0\n",
      "Step: 31550, train/grad_norm: 0.0018575573340058327\n",
      "Step: 31550, train/learning_rate: 1.2458353012334555e-05\n",
      "Step: 31550, train/epoch: 7.508329391479492\n",
      "Step: 31560, train/loss: 0.0\n",
      "Step: 31560, train/grad_norm: 0.0002632582909427583\n",
      "Step: 31560, train/learning_rate: 1.244645409315126e-05\n",
      "Step: 31560, train/epoch: 7.510709285736084\n",
      "Step: 31570, train/loss: 0.0\n",
      "Step: 31570, train/grad_norm: 0.000447124766651541\n",
      "Step: 31570, train/learning_rate: 1.2434555173967965e-05\n",
      "Step: 31570, train/epoch: 7.513089179992676\n",
      "Step: 31580, train/loss: 0.0\n",
      "Step: 31580, train/grad_norm: 0.0025507225655019283\n",
      "Step: 31580, train/learning_rate: 1.242265625478467e-05\n",
      "Step: 31580, train/epoch: 7.515468597412109\n",
      "Step: 31590, train/loss: 0.0\n",
      "Step: 31590, train/grad_norm: 0.005159479100257158\n",
      "Step: 31590, train/learning_rate: 1.2410756426106673e-05\n",
      "Step: 31590, train/epoch: 7.517848491668701\n",
      "Step: 31600, train/loss: 0.0\n",
      "Step: 31600, train/grad_norm: 0.0003169558185618371\n",
      "Step: 31600, train/learning_rate: 1.2398857506923378e-05\n",
      "Step: 31600, train/epoch: 7.520228385925293\n",
      "Step: 31610, train/loss: 0.0\n",
      "Step: 31610, train/grad_norm: 0.0006060886662453413\n",
      "Step: 31610, train/learning_rate: 1.2386958587740082e-05\n",
      "Step: 31610, train/epoch: 7.522608280181885\n",
      "Step: 31620, train/loss: 0.0\n",
      "Step: 31620, train/grad_norm: 0.0001370714307995513\n",
      "Step: 31620, train/learning_rate: 1.2375059668556787e-05\n",
      "Step: 31620, train/epoch: 7.524988174438477\n",
      "Step: 31630, train/loss: 0.0\n",
      "Step: 31630, train/grad_norm: 0.00024344478151760995\n",
      "Step: 31630, train/learning_rate: 1.2363160749373492e-05\n",
      "Step: 31630, train/epoch: 7.527368068695068\n",
      "Step: 31640, train/loss: 0.0\n",
      "Step: 31640, train/grad_norm: 7.057871698634699e-05\n",
      "Step: 31640, train/learning_rate: 1.2351260920695495e-05\n",
      "Step: 31640, train/epoch: 7.52974796295166\n",
      "Step: 31650, train/loss: 0.0\n",
      "Step: 31650, train/grad_norm: 0.00035655993269756436\n",
      "Step: 31650, train/learning_rate: 1.23393620015122e-05\n",
      "Step: 31650, train/epoch: 7.532127380371094\n",
      "Step: 31660, train/loss: 0.0\n",
      "Step: 31660, train/grad_norm: 0.00017529520846437663\n",
      "Step: 31660, train/learning_rate: 1.2327463082328904e-05\n",
      "Step: 31660, train/epoch: 7.5345072746276855\n",
      "Step: 31670, train/loss: 0.0\n",
      "Step: 31670, train/grad_norm: 0.00012749490269925445\n",
      "Step: 31670, train/learning_rate: 1.231556416314561e-05\n",
      "Step: 31670, train/epoch: 7.536887168884277\n",
      "Step: 31680, train/loss: 0.0\n",
      "Step: 31680, train/grad_norm: 9.304697596235201e-05\n",
      "Step: 31680, train/learning_rate: 1.2303665243962314e-05\n",
      "Step: 31680, train/epoch: 7.539267063140869\n",
      "Step: 31690, train/loss: 0.0\n",
      "Step: 31690, train/grad_norm: 0.00019006738148164004\n",
      "Step: 31690, train/learning_rate: 1.2291765415284317e-05\n",
      "Step: 31690, train/epoch: 7.541646957397461\n",
      "Step: 31700, train/loss: 0.0\n",
      "Step: 31700, train/grad_norm: 8.099486149149016e-05\n",
      "Step: 31700, train/learning_rate: 1.2279866496101022e-05\n",
      "Step: 31700, train/epoch: 7.544026851654053\n",
      "Step: 31710, train/loss: 0.0\n",
      "Step: 31710, train/grad_norm: 0.0003938687441404909\n",
      "Step: 31710, train/learning_rate: 1.2267967576917727e-05\n",
      "Step: 31710, train/epoch: 7.546406269073486\n",
      "Step: 31720, train/loss: 0.0\n",
      "Step: 31720, train/grad_norm: 8.020499080885202e-05\n",
      "Step: 31720, train/learning_rate: 1.2256068657734431e-05\n",
      "Step: 31720, train/epoch: 7.548786163330078\n",
      "Step: 31730, train/loss: 0.0\n",
      "Step: 31730, train/grad_norm: 6.408377521438524e-05\n",
      "Step: 31730, train/learning_rate: 1.2244169738551136e-05\n",
      "Step: 31730, train/epoch: 7.55116605758667\n",
      "Step: 31740, train/loss: 0.0\n",
      "Step: 31740, train/grad_norm: 0.00012965498899575323\n",
      "Step: 31740, train/learning_rate: 1.2232269909873139e-05\n",
      "Step: 31740, train/epoch: 7.553545951843262\n",
      "Step: 31750, train/loss: 0.0\n",
      "Step: 31750, train/grad_norm: 0.00028842122992500663\n",
      "Step: 31750, train/learning_rate: 1.2220370990689844e-05\n",
      "Step: 31750, train/epoch: 7.5559258460998535\n",
      "Step: 31760, train/loss: 0.0\n",
      "Step: 31760, train/grad_norm: 0.00010378517617937177\n",
      "Step: 31760, train/learning_rate: 1.2208472071506549e-05\n",
      "Step: 31760, train/epoch: 7.558305740356445\n",
      "Step: 31770, train/loss: 0.0\n",
      "Step: 31770, train/grad_norm: 0.0006167448591440916\n",
      "Step: 31770, train/learning_rate: 1.2196573152323253e-05\n",
      "Step: 31770, train/epoch: 7.560685157775879\n",
      "Step: 31780, train/loss: 0.0\n",
      "Step: 31780, train/grad_norm: 0.0001634513319004327\n",
      "Step: 31780, train/learning_rate: 1.2184674233139958e-05\n",
      "Step: 31780, train/epoch: 7.563065052032471\n",
      "Step: 31790, train/loss: 0.0\n",
      "Step: 31790, train/grad_norm: 7.898019975982606e-05\n",
      "Step: 31790, train/learning_rate: 1.2172775313956663e-05\n",
      "Step: 31790, train/epoch: 7.5654449462890625\n",
      "Step: 31800, train/loss: 0.19609999656677246\n",
      "Step: 31800, train/grad_norm: 0.0017440328374505043\n",
      "Step: 31800, train/learning_rate: 1.2160875485278666e-05\n",
      "Step: 31800, train/epoch: 7.567824840545654\n",
      "Step: 31810, train/loss: 0.0\n",
      "Step: 31810, train/grad_norm: 0.016090383753180504\n",
      "Step: 31810, train/learning_rate: 1.214897656609537e-05\n",
      "Step: 31810, train/epoch: 7.570204734802246\n",
      "Step: 31820, train/loss: 0.0\n",
      "Step: 31820, train/grad_norm: 0.006029265001416206\n",
      "Step: 31820, train/learning_rate: 1.2137077646912076e-05\n",
      "Step: 31820, train/epoch: 7.572584629058838\n",
      "Step: 31830, train/loss: 0.0\n",
      "Step: 31830, train/grad_norm: 0.008750764653086662\n",
      "Step: 31830, train/learning_rate: 1.212517872772878e-05\n",
      "Step: 31830, train/epoch: 7.57496452331543\n",
      "Step: 31840, train/loss: 0.0\n",
      "Step: 31840, train/grad_norm: 0.00431375578045845\n",
      "Step: 31840, train/learning_rate: 1.2113279808545485e-05\n",
      "Step: 31840, train/epoch: 7.577343940734863\n",
      "Step: 31850, train/loss: 0.0\n",
      "Step: 31850, train/grad_norm: 0.006069655530154705\n",
      "Step: 31850, train/learning_rate: 1.2101379979867488e-05\n",
      "Step: 31850, train/epoch: 7.579723834991455\n",
      "Step: 31860, train/loss: 0.0\n",
      "Step: 31860, train/grad_norm: 0.004824609495699406\n",
      "Step: 31860, train/learning_rate: 1.2089481060684193e-05\n",
      "Step: 31860, train/epoch: 7.582103729248047\n",
      "Step: 31870, train/loss: 0.0\n",
      "Step: 31870, train/grad_norm: 0.0036997124552726746\n",
      "Step: 31870, train/learning_rate: 1.2077582141500898e-05\n",
      "Step: 31870, train/epoch: 7.584483623504639\n",
      "Step: 31880, train/loss: 0.0\n",
      "Step: 31880, train/grad_norm: 0.0011362958466634154\n",
      "Step: 31880, train/learning_rate: 1.2065683222317602e-05\n",
      "Step: 31880, train/epoch: 7.5868635177612305\n",
      "Step: 31890, train/loss: 0.0\n",
      "Step: 31890, train/grad_norm: 0.0017089203465729952\n",
      "Step: 31890, train/learning_rate: 1.2053784303134307e-05\n",
      "Step: 31890, train/epoch: 7.589243412017822\n",
      "Step: 31900, train/loss: 0.0\n",
      "Step: 31900, train/grad_norm: 0.0007440126501023769\n",
      "Step: 31900, train/learning_rate: 1.204188447445631e-05\n",
      "Step: 31900, train/epoch: 7.591622829437256\n",
      "Step: 31910, train/loss: 0.08950000256299973\n",
      "Step: 31910, train/grad_norm: 0.0010481859790161252\n",
      "Step: 31910, train/learning_rate: 1.2029985555273015e-05\n",
      "Step: 31910, train/epoch: 7.594002723693848\n",
      "Step: 31920, train/loss: 0.0\n",
      "Step: 31920, train/grad_norm: 0.0030737658962607384\n",
      "Step: 31920, train/learning_rate: 1.201808663608972e-05\n",
      "Step: 31920, train/epoch: 7.5963826179504395\n",
      "Step: 31930, train/loss: 0.0\n",
      "Step: 31930, train/grad_norm: 0.00625901622697711\n",
      "Step: 31930, train/learning_rate: 1.2006187716906425e-05\n",
      "Step: 31930, train/epoch: 7.598762512207031\n",
      "Step: 31940, train/loss: 0.0\n",
      "Step: 31940, train/grad_norm: 0.0024982343893498182\n",
      "Step: 31940, train/learning_rate: 1.199428879772313e-05\n",
      "Step: 31940, train/epoch: 7.601142406463623\n",
      "Step: 31950, train/loss: 0.0\n",
      "Step: 31950, train/grad_norm: 0.0032168093603104353\n",
      "Step: 31950, train/learning_rate: 1.1982388969045132e-05\n",
      "Step: 31950, train/epoch: 7.603522300720215\n",
      "Step: 31960, train/loss: 0.0\n",
      "Step: 31960, train/grad_norm: 0.0056729610078036785\n",
      "Step: 31960, train/learning_rate: 1.1970490049861837e-05\n",
      "Step: 31960, train/epoch: 7.605901718139648\n",
      "Step: 31970, train/loss: 0.0\n",
      "Step: 31970, train/grad_norm: 0.0008391599985770881\n",
      "Step: 31970, train/learning_rate: 1.1958591130678542e-05\n",
      "Step: 31970, train/epoch: 7.60828161239624\n",
      "Step: 31980, train/loss: 0.0\n",
      "Step: 31980, train/grad_norm: 0.0015346171567216516\n",
      "Step: 31980, train/learning_rate: 1.1946692211495247e-05\n",
      "Step: 31980, train/epoch: 7.610661506652832\n",
      "Step: 31990, train/loss: 0.0\n",
      "Step: 31990, train/grad_norm: 0.0008468547603115439\n",
      "Step: 31990, train/learning_rate: 1.1934793292311952e-05\n",
      "Step: 31990, train/epoch: 7.613041400909424\n",
      "Step: 32000, train/loss: 0.0\n",
      "Step: 32000, train/grad_norm: 0.0009976314613595605\n",
      "Step: 32000, train/learning_rate: 1.1922893463633955e-05\n",
      "Step: 32000, train/epoch: 7.615421295166016\n",
      "Step: 32010, train/loss: 0.0\n",
      "Step: 32010, train/grad_norm: 0.002280502114444971\n",
      "Step: 32010, train/learning_rate: 1.191099454445066e-05\n",
      "Step: 32010, train/epoch: 7.617801189422607\n",
      "Step: 32020, train/loss: 0.0\n",
      "Step: 32020, train/grad_norm: 0.000482819537864998\n",
      "Step: 32020, train/learning_rate: 1.1899095625267364e-05\n",
      "Step: 32020, train/epoch: 7.620181083679199\n",
      "Step: 32030, train/loss: 0.0\n",
      "Step: 32030, train/grad_norm: 0.0012379607651382685\n",
      "Step: 32030, train/learning_rate: 1.1887196706084069e-05\n",
      "Step: 32030, train/epoch: 7.622560501098633\n",
      "Step: 32040, train/loss: 0.0\n",
      "Step: 32040, train/grad_norm: 0.0009473806712776423\n",
      "Step: 32040, train/learning_rate: 1.1875297786900774e-05\n",
      "Step: 32040, train/epoch: 7.624940395355225\n",
      "Step: 32050, train/loss: 0.0\n",
      "Step: 32050, train/grad_norm: 0.0005026239668950438\n",
      "Step: 32050, train/learning_rate: 1.1863397958222777e-05\n",
      "Step: 32050, train/epoch: 7.627320289611816\n",
      "Step: 32060, train/loss: 0.0\n",
      "Step: 32060, train/grad_norm: 0.0020167422480881214\n",
      "Step: 32060, train/learning_rate: 1.1851499039039481e-05\n",
      "Step: 32060, train/epoch: 7.629700183868408\n",
      "Step: 32070, train/loss: 0.0\n",
      "Step: 32070, train/grad_norm: 0.0006579232285730541\n",
      "Step: 32070, train/learning_rate: 1.1839600119856186e-05\n",
      "Step: 32070, train/epoch: 7.632080078125\n",
      "Step: 32080, train/loss: 0.06560000032186508\n",
      "Step: 32080, train/grad_norm: 0.0011207099305465817\n",
      "Step: 32080, train/learning_rate: 1.1827701200672891e-05\n",
      "Step: 32080, train/epoch: 7.634459972381592\n",
      "Step: 32090, train/loss: 0.0\n",
      "Step: 32090, train/grad_norm: 0.00983339175581932\n",
      "Step: 32090, train/learning_rate: 1.1815802281489596e-05\n",
      "Step: 32090, train/epoch: 7.636839389801025\n",
      "Step: 32100, train/loss: 9.999999747378752e-05\n",
      "Step: 32100, train/grad_norm: 0.0005635272245854139\n",
      "Step: 32100, train/learning_rate: 1.1803902452811599e-05\n",
      "Step: 32100, train/epoch: 7.639219284057617\n",
      "Step: 32110, train/loss: 0.0\n",
      "Step: 32110, train/grad_norm: 0.0011263025226071477\n",
      "Step: 32110, train/learning_rate: 1.1792003533628304e-05\n",
      "Step: 32110, train/epoch: 7.641599178314209\n",
      "Step: 32120, train/loss: 0.0\n",
      "Step: 32120, train/grad_norm: 0.0003424794413149357\n",
      "Step: 32120, train/learning_rate: 1.1780104614445008e-05\n",
      "Step: 32120, train/epoch: 7.643979072570801\n",
      "Step: 32130, train/loss: 0.0\n",
      "Step: 32130, train/grad_norm: 0.0002085213054670021\n",
      "Step: 32130, train/learning_rate: 1.1768205695261713e-05\n",
      "Step: 32130, train/epoch: 7.646358966827393\n",
      "Step: 32140, train/loss: 0.1062999963760376\n",
      "Step: 32140, train/grad_norm: 0.0007583819096907973\n",
      "Step: 32140, train/learning_rate: 1.1756306776078418e-05\n",
      "Step: 32140, train/epoch: 7.648738861083984\n",
      "Step: 32150, train/loss: 9.999999747378752e-05\n",
      "Step: 32150, train/grad_norm: 0.0012730981688946486\n",
      "Step: 32150, train/learning_rate: 1.1744407856895123e-05\n",
      "Step: 32150, train/epoch: 7.651118278503418\n",
      "Step: 32160, train/loss: 0.06019999831914902\n",
      "Step: 32160, train/grad_norm: 0.007350562140345573\n",
      "Step: 32160, train/learning_rate: 1.1732508028217126e-05\n",
      "Step: 32160, train/epoch: 7.65349817276001\n",
      "Step: 32170, train/loss: 0.03970000147819519\n",
      "Step: 32170, train/grad_norm: 0.008224567398428917\n",
      "Step: 32170, train/learning_rate: 1.172060910903383e-05\n",
      "Step: 32170, train/epoch: 7.655878067016602\n",
      "Step: 32180, train/loss: 9.999999747378752e-05\n",
      "Step: 32180, train/grad_norm: 0.033655691891908646\n",
      "Step: 32180, train/learning_rate: 1.1708710189850535e-05\n",
      "Step: 32180, train/epoch: 7.658257961273193\n",
      "Step: 32190, train/loss: 0.0\n",
      "Step: 32190, train/grad_norm: 0.01054274383932352\n",
      "Step: 32190, train/learning_rate: 1.169681127066724e-05\n",
      "Step: 32190, train/epoch: 7.660637855529785\n",
      "Step: 32200, train/loss: 0.0\n",
      "Step: 32200, train/grad_norm: 0.0038607900496572256\n",
      "Step: 32200, train/learning_rate: 1.1684912351483945e-05\n",
      "Step: 32200, train/epoch: 7.663017749786377\n",
      "Step: 32210, train/loss: 0.0\n",
      "Step: 32210, train/grad_norm: 0.003615474794059992\n",
      "Step: 32210, train/learning_rate: 1.1673012522805948e-05\n",
      "Step: 32210, train/epoch: 7.665397644042969\n",
      "Step: 32220, train/loss: 0.0\n",
      "Step: 32220, train/grad_norm: 0.0027932447846978903\n",
      "Step: 32220, train/learning_rate: 1.1661113603622653e-05\n",
      "Step: 32220, train/epoch: 7.667777061462402\n",
      "Step: 32230, train/loss: 0.0\n",
      "Step: 32230, train/grad_norm: 0.0020062148105353117\n",
      "Step: 32230, train/learning_rate: 1.1649214684439357e-05\n",
      "Step: 32230, train/epoch: 7.670156955718994\n",
      "Step: 32240, train/loss: 0.0\n",
      "Step: 32240, train/grad_norm: 0.0022144941613078117\n",
      "Step: 32240, train/learning_rate: 1.1637315765256062e-05\n",
      "Step: 32240, train/epoch: 7.672536849975586\n",
      "Step: 32250, train/loss: 0.0\n",
      "Step: 32250, train/grad_norm: 0.002174701541662216\n",
      "Step: 32250, train/learning_rate: 1.1625416846072767e-05\n",
      "Step: 32250, train/epoch: 7.674916744232178\n",
      "Step: 32260, train/loss: 0.0\n",
      "Step: 32260, train/grad_norm: 0.0011007419088855386\n",
      "Step: 32260, train/learning_rate: 1.161351701739477e-05\n",
      "Step: 32260, train/epoch: 7.6772966384887695\n",
      "Step: 32270, train/loss: 0.0\n",
      "Step: 32270, train/grad_norm: 0.002311215503141284\n",
      "Step: 32270, train/learning_rate: 1.1601618098211475e-05\n",
      "Step: 32270, train/epoch: 7.679676532745361\n",
      "Step: 32280, train/loss: 0.0\n",
      "Step: 32280, train/grad_norm: 0.00025426267529837787\n",
      "Step: 32280, train/learning_rate: 1.158971917902818e-05\n",
      "Step: 32280, train/epoch: 7.682055950164795\n",
      "Step: 32290, train/loss: 0.0\n",
      "Step: 32290, train/grad_norm: 0.0007385186618193984\n",
      "Step: 32290, train/learning_rate: 1.1577820259844884e-05\n",
      "Step: 32290, train/epoch: 7.684435844421387\n",
      "Step: 32300, train/loss: 0.0\n",
      "Step: 32300, train/grad_norm: 0.00020796613534912467\n",
      "Step: 32300, train/learning_rate: 1.1565921340661589e-05\n",
      "Step: 32300, train/epoch: 7.6868157386779785\n",
      "Step: 32310, train/loss: 0.0\n",
      "Step: 32310, train/grad_norm: 0.00033135261037386954\n",
      "Step: 32310, train/learning_rate: 1.1554021511983592e-05\n",
      "Step: 32310, train/epoch: 7.68919563293457\n",
      "Step: 32320, train/loss: 9.999999747378752e-05\n",
      "Step: 32320, train/grad_norm: 0.0005034995847381651\n",
      "Step: 32320, train/learning_rate: 1.1542122592800297e-05\n",
      "Step: 32320, train/epoch: 7.691575527191162\n",
      "Step: 32330, train/loss: 0.0\n",
      "Step: 32330, train/grad_norm: 7.02857505530119e-05\n",
      "Step: 32330, train/learning_rate: 1.1530223673617002e-05\n",
      "Step: 32330, train/epoch: 7.693955421447754\n",
      "Step: 32340, train/loss: 0.0\n",
      "Step: 32340, train/grad_norm: 6.140328332548961e-05\n",
      "Step: 32340, train/learning_rate: 1.1518324754433706e-05\n",
      "Step: 32340, train/epoch: 7.696335315704346\n",
      "Step: 32350, train/loss: 0.026599999517202377\n",
      "Step: 32350, train/grad_norm: 0.0001676107494859025\n",
      "Step: 32350, train/learning_rate: 1.1506425835250411e-05\n",
      "Step: 32350, train/epoch: 7.698714733123779\n",
      "Step: 32360, train/loss: 0.0\n",
      "Step: 32360, train/grad_norm: 6.625890819123015e-05\n",
      "Step: 32360, train/learning_rate: 1.1494526006572414e-05\n",
      "Step: 32360, train/epoch: 7.701094627380371\n",
      "Step: 32370, train/loss: 0.0\n",
      "Step: 32370, train/grad_norm: 7.162654947023839e-05\n",
      "Step: 32370, train/learning_rate: 1.1482627087389119e-05\n",
      "Step: 32370, train/epoch: 7.703474521636963\n",
      "Step: 32380, train/loss: 0.0\n",
      "Step: 32380, train/grad_norm: 0.0004422995843924582\n",
      "Step: 32380, train/learning_rate: 1.1470728168205824e-05\n",
      "Step: 32380, train/epoch: 7.705854415893555\n",
      "Step: 32390, train/loss: 0.0\n",
      "Step: 32390, train/grad_norm: 0.00017922116967383772\n",
      "Step: 32390, train/learning_rate: 1.1458829249022529e-05\n",
      "Step: 32390, train/epoch: 7.7082343101501465\n",
      "Step: 32400, train/loss: 0.0\n",
      "Step: 32400, train/grad_norm: 0.0002457824011798948\n",
      "Step: 32400, train/learning_rate: 1.1446930329839233e-05\n",
      "Step: 32400, train/epoch: 7.710614204406738\n",
      "Step: 32410, train/loss: 0.0\n",
      "Step: 32410, train/grad_norm: 0.008311060257256031\n",
      "Step: 32410, train/learning_rate: 1.1435030501161236e-05\n",
      "Step: 32410, train/epoch: 7.712993621826172\n",
      "Step: 32420, train/loss: 0.0\n",
      "Step: 32420, train/grad_norm: 0.00029173694201745093\n",
      "Step: 32420, train/learning_rate: 1.1423131581977941e-05\n",
      "Step: 32420, train/epoch: 7.715373516082764\n",
      "Step: 32430, train/loss: 9.999999747378752e-05\n",
      "Step: 32430, train/grad_norm: 8.157521369867027e-05\n",
      "Step: 32430, train/learning_rate: 1.1411232662794646e-05\n",
      "Step: 32430, train/epoch: 7.7177534103393555\n",
      "Step: 32440, train/loss: 0.0\n",
      "Step: 32440, train/grad_norm: 3.353130159666762e-05\n",
      "Step: 32440, train/learning_rate: 1.139933374361135e-05\n",
      "Step: 32440, train/epoch: 7.720133304595947\n",
      "Step: 32450, train/loss: 0.0\n",
      "Step: 32450, train/grad_norm: 2.1243175069685094e-05\n",
      "Step: 32450, train/learning_rate: 1.1387434824428055e-05\n",
      "Step: 32450, train/epoch: 7.722513198852539\n",
      "Step: 32460, train/loss: 0.0\n",
      "Step: 32460, train/grad_norm: 0.0009924782207235694\n",
      "Step: 32460, train/learning_rate: 1.137553590524476e-05\n",
      "Step: 32460, train/epoch: 7.724893093109131\n",
      "Step: 32470, train/loss: 0.0\n",
      "Step: 32470, train/grad_norm: 0.0018489034846425056\n",
      "Step: 32470, train/learning_rate: 1.1363636076566763e-05\n",
      "Step: 32470, train/epoch: 7.7272725105285645\n",
      "Step: 32480, train/loss: 0.0\n",
      "Step: 32480, train/grad_norm: 1.700960456219036e-05\n",
      "Step: 32480, train/learning_rate: 1.1351737157383468e-05\n",
      "Step: 32480, train/epoch: 7.729652404785156\n",
      "Step: 32490, train/loss: 0.0\n",
      "Step: 32490, train/grad_norm: 4.066774999955669e-05\n",
      "Step: 32490, train/learning_rate: 1.1339838238200173e-05\n",
      "Step: 32490, train/epoch: 7.732032299041748\n",
      "Step: 32500, train/loss: 0.0\n",
      "Step: 32500, train/grad_norm: 4.2857020162045956e-05\n",
      "Step: 32500, train/learning_rate: 1.1327939319016878e-05\n",
      "Step: 32500, train/epoch: 7.73441219329834\n",
      "Step: 32510, train/loss: 0.0\n",
      "Step: 32510, train/grad_norm: 0.001601721509359777\n",
      "Step: 32510, train/learning_rate: 1.1316040399833582e-05\n",
      "Step: 32510, train/epoch: 7.736792087554932\n",
      "Step: 32520, train/loss: 0.0\n",
      "Step: 32520, train/grad_norm: 4.466291647986509e-05\n",
      "Step: 32520, train/learning_rate: 1.1304140571155585e-05\n",
      "Step: 32520, train/epoch: 7.739171981811523\n",
      "Step: 32530, train/loss: 0.0\n",
      "Step: 32530, train/grad_norm: 2.0406529074534774e-05\n",
      "Step: 32530, train/learning_rate: 1.129224165197229e-05\n",
      "Step: 32530, train/epoch: 7.741551876068115\n",
      "Step: 32540, train/loss: 0.0\n",
      "Step: 32540, train/grad_norm: 7.026707316981629e-05\n",
      "Step: 32540, train/learning_rate: 1.1280342732788995e-05\n",
      "Step: 32540, train/epoch: 7.743931293487549\n",
      "Step: 32550, train/loss: 0.0\n",
      "Step: 32550, train/grad_norm: 2.136743023584131e-05\n",
      "Step: 32550, train/learning_rate: 1.12684438136057e-05\n",
      "Step: 32550, train/epoch: 7.746311187744141\n",
      "Step: 32560, train/loss: 0.0\n",
      "Step: 32560, train/grad_norm: 2.3072216208674945e-05\n",
      "Step: 32560, train/learning_rate: 1.1256544894422404e-05\n",
      "Step: 32560, train/epoch: 7.748691082000732\n",
      "Step: 32570, train/loss: 0.0\n",
      "Step: 32570, train/grad_norm: 3.3403688576072454e-05\n",
      "Step: 32570, train/learning_rate: 1.1244645065744407e-05\n",
      "Step: 32570, train/epoch: 7.751070976257324\n",
      "Step: 32580, train/loss: 0.1265999972820282\n",
      "Step: 32580, train/grad_norm: 2.8726459277095273e-05\n",
      "Step: 32580, train/learning_rate: 1.1232746146561112e-05\n",
      "Step: 32580, train/epoch: 7.753450870513916\n",
      "Step: 32590, train/loss: 0.0\n",
      "Step: 32590, train/grad_norm: 0.0008393996395170689\n",
      "Step: 32590, train/learning_rate: 1.1220847227377817e-05\n",
      "Step: 32590, train/epoch: 7.755830764770508\n",
      "Step: 32600, train/loss: 0.0\n",
      "Step: 32600, train/grad_norm: 0.00149821350350976\n",
      "Step: 32600, train/learning_rate: 1.1208948308194522e-05\n",
      "Step: 32600, train/epoch: 7.758210182189941\n",
      "Step: 32610, train/loss: 0.0\n",
      "Step: 32610, train/grad_norm: 0.005766610149294138\n",
      "Step: 32610, train/learning_rate: 1.1197049389011227e-05\n",
      "Step: 32610, train/epoch: 7.760590076446533\n",
      "Step: 32620, train/loss: 0.00019999999494757503\n",
      "Step: 32620, train/grad_norm: 0.0023629043716937304\n",
      "Step: 32620, train/learning_rate: 1.118514956033323e-05\n",
      "Step: 32620, train/epoch: 7.762969970703125\n",
      "Step: 32630, train/loss: 0.0\n",
      "Step: 32630, train/grad_norm: 0.00038057987694628537\n",
      "Step: 32630, train/learning_rate: 1.1173250641149934e-05\n",
      "Step: 32630, train/epoch: 7.765349864959717\n",
      "Step: 32640, train/loss: 0.0\n",
      "Step: 32640, train/grad_norm: 0.002231247490271926\n",
      "Step: 32640, train/learning_rate: 1.116135172196664e-05\n",
      "Step: 32640, train/epoch: 7.767729759216309\n",
      "Step: 32650, train/loss: 0.05350000038743019\n",
      "Step: 32650, train/grad_norm: 0.08396506309509277\n",
      "Step: 32650, train/learning_rate: 1.1149452802783344e-05\n",
      "Step: 32650, train/epoch: 7.7701096534729\n",
      "Step: 32660, train/loss: 0.0\n",
      "Step: 32660, train/grad_norm: 0.013809055089950562\n",
      "Step: 32660, train/learning_rate: 1.1137553883600049e-05\n",
      "Step: 32660, train/epoch: 7.772489070892334\n",
      "Step: 32670, train/loss: 0.060600001364946365\n",
      "Step: 32670, train/grad_norm: 0.000955713156145066\n",
      "Step: 32670, train/learning_rate: 1.1125654054922052e-05\n",
      "Step: 32670, train/epoch: 7.774868965148926\n",
      "Step: 32680, train/loss: 9.999999747378752e-05\n",
      "Step: 32680, train/grad_norm: 0.051382869482040405\n",
      "Step: 32680, train/learning_rate: 1.1113755135738757e-05\n",
      "Step: 32680, train/epoch: 7.777248859405518\n",
      "Step: 32690, train/loss: 0.07810000330209732\n",
      "Step: 32690, train/grad_norm: 0.00019626303401309997\n",
      "Step: 32690, train/learning_rate: 1.1101856216555461e-05\n",
      "Step: 32690, train/epoch: 7.779628753662109\n",
      "Step: 32700, train/loss: 0.0\n",
      "Step: 32700, train/grad_norm: 0.00022276710660662502\n",
      "Step: 32700, train/learning_rate: 1.1089957297372166e-05\n",
      "Step: 32700, train/epoch: 7.782008647918701\n",
      "Step: 32710, train/loss: 0.0\n",
      "Step: 32710, train/grad_norm: 0.00038640250568278134\n",
      "Step: 32710, train/learning_rate: 1.1078058378188871e-05\n",
      "Step: 32710, train/epoch: 7.784388542175293\n",
      "Step: 32720, train/loss: 0.0\n",
      "Step: 32720, train/grad_norm: 0.006240066606551409\n",
      "Step: 32720, train/learning_rate: 1.1066158549510874e-05\n",
      "Step: 32720, train/epoch: 7.786768436431885\n",
      "Step: 32730, train/loss: 0.0\n",
      "Step: 32730, train/grad_norm: 0.00023931238683871925\n",
      "Step: 32730, train/learning_rate: 1.1054259630327579e-05\n",
      "Step: 32730, train/epoch: 7.789147853851318\n",
      "Step: 32740, train/loss: 0.0\n",
      "Step: 32740, train/grad_norm: 0.00033569333027116954\n",
      "Step: 32740, train/learning_rate: 1.1042360711144283e-05\n",
      "Step: 32740, train/epoch: 7.79152774810791\n",
      "Step: 32750, train/loss: 0.0\n",
      "Step: 32750, train/grad_norm: 0.0008410902228206396\n",
      "Step: 32750, train/learning_rate: 1.1030461791960988e-05\n",
      "Step: 32750, train/epoch: 7.793907642364502\n",
      "Step: 32760, train/loss: 0.0\n",
      "Step: 32760, train/grad_norm: 0.000534721533767879\n",
      "Step: 32760, train/learning_rate: 1.1018562872777693e-05\n",
      "Step: 32760, train/epoch: 7.796287536621094\n",
      "Step: 32770, train/loss: 0.0\n",
      "Step: 32770, train/grad_norm: 0.000605485460255295\n",
      "Step: 32770, train/learning_rate: 1.1006663044099696e-05\n",
      "Step: 32770, train/epoch: 7.7986674308776855\n",
      "Step: 32780, train/loss: 0.0\n",
      "Step: 32780, train/grad_norm: 0.0012129287933930755\n",
      "Step: 32780, train/learning_rate: 1.09947641249164e-05\n",
      "Step: 32780, train/epoch: 7.801047325134277\n",
      "Step: 32790, train/loss: 0.11020000278949738\n",
      "Step: 32790, train/grad_norm: 0.014623069204390049\n",
      "Step: 32790, train/learning_rate: 1.0982865205733106e-05\n",
      "Step: 32790, train/epoch: 7.803426742553711\n",
      "Step: 32800, train/loss: 9.999999747378752e-05\n",
      "Step: 32800, train/grad_norm: 0.0030687060207128525\n",
      "Step: 32800, train/learning_rate: 1.097096628654981e-05\n",
      "Step: 32800, train/epoch: 7.805806636810303\n",
      "Step: 32810, train/loss: 0.0\n",
      "Step: 32810, train/grad_norm: 0.0010168531443923712\n",
      "Step: 32810, train/learning_rate: 1.0959067367366515e-05\n",
      "Step: 32810, train/epoch: 7.8081865310668945\n",
      "Step: 32820, train/loss: 9.999999747378752e-05\n",
      "Step: 32820, train/grad_norm: 0.000286838854663074\n",
      "Step: 32820, train/learning_rate: 1.094716844818322e-05\n",
      "Step: 32820, train/epoch: 7.810566425323486\n",
      "Step: 32830, train/loss: 0.0\n",
      "Step: 32830, train/grad_norm: 0.00390853825956583\n",
      "Step: 32830, train/learning_rate: 1.0935268619505223e-05\n",
      "Step: 32830, train/epoch: 7.812946319580078\n",
      "Step: 32840, train/loss: 0.0\n",
      "Step: 32840, train/grad_norm: 0.0014441157691180706\n",
      "Step: 32840, train/learning_rate: 1.0923369700321928e-05\n",
      "Step: 32840, train/epoch: 7.81532621383667\n",
      "Step: 32850, train/loss: 0.0\n",
      "Step: 32850, train/grad_norm: 0.002552584046497941\n",
      "Step: 32850, train/learning_rate: 1.0911470781138632e-05\n",
      "Step: 32850, train/epoch: 7.8177056312561035\n",
      "Step: 32860, train/loss: 0.0\n",
      "Step: 32860, train/grad_norm: 0.00023879697255324572\n",
      "Step: 32860, train/learning_rate: 1.0899571861955337e-05\n",
      "Step: 32860, train/epoch: 7.820085525512695\n",
      "Step: 32870, train/loss: 0.0\n",
      "Step: 32870, train/grad_norm: 0.00019728582992684096\n",
      "Step: 32870, train/learning_rate: 1.0887672942772042e-05\n",
      "Step: 32870, train/epoch: 7.822465419769287\n",
      "Step: 32880, train/loss: 0.0\n",
      "Step: 32880, train/grad_norm: 0.0001611426123417914\n",
      "Step: 32880, train/learning_rate: 1.0875773114094045e-05\n",
      "Step: 32880, train/epoch: 7.824845314025879\n",
      "Step: 32890, train/loss: 0.0\n",
      "Step: 32890, train/grad_norm: 0.0006741267861798406\n",
      "Step: 32890, train/learning_rate: 1.086387419491075e-05\n",
      "Step: 32890, train/epoch: 7.827225208282471\n",
      "Step: 32900, train/loss: 0.0\n",
      "Step: 32900, train/grad_norm: 0.00025675882352516055\n",
      "Step: 32900, train/learning_rate: 1.0851975275727455e-05\n",
      "Step: 32900, train/epoch: 7.8296051025390625\n",
      "Step: 32910, train/loss: 0.0\n",
      "Step: 32910, train/grad_norm: 0.00014624714094679803\n",
      "Step: 32910, train/learning_rate: 1.084007635654416e-05\n",
      "Step: 32910, train/epoch: 7.831984996795654\n",
      "Step: 32920, train/loss: 0.0\n",
      "Step: 32920, train/grad_norm: 0.0018490190850570798\n",
      "Step: 32920, train/learning_rate: 1.0828177437360864e-05\n",
      "Step: 32920, train/epoch: 7.834364414215088\n",
      "Step: 32930, train/loss: 0.0\n",
      "Step: 32930, train/grad_norm: 0.00041260995203629136\n",
      "Step: 32930, train/learning_rate: 1.0816277608682867e-05\n",
      "Step: 32930, train/epoch: 7.83674430847168\n",
      "Step: 32940, train/loss: 0.0\n",
      "Step: 32940, train/grad_norm: 0.032777395099401474\n",
      "Step: 32940, train/learning_rate: 1.0804378689499572e-05\n",
      "Step: 32940, train/epoch: 7.8391242027282715\n",
      "Step: 32950, train/loss: 0.0\n",
      "Step: 32950, train/grad_norm: 0.0038253306411206722\n",
      "Step: 32950, train/learning_rate: 1.0792479770316277e-05\n",
      "Step: 32950, train/epoch: 7.841504096984863\n",
      "Step: 32960, train/loss: 0.0\n",
      "Step: 32960, train/grad_norm: 8.867765427567065e-05\n",
      "Step: 32960, train/learning_rate: 1.0780580851132981e-05\n",
      "Step: 32960, train/epoch: 7.843883991241455\n",
      "Step: 32970, train/loss: 0.0\n",
      "Step: 32970, train/grad_norm: 0.00024862957070581615\n",
      "Step: 32970, train/learning_rate: 1.0768681931949686e-05\n",
      "Step: 32970, train/epoch: 7.846263885498047\n",
      "Step: 32980, train/loss: 0.0\n",
      "Step: 32980, train/grad_norm: 0.0016037346795201302\n",
      "Step: 32980, train/learning_rate: 1.075678210327169e-05\n",
      "Step: 32980, train/epoch: 7.8486433029174805\n",
      "Step: 32990, train/loss: 0.0\n",
      "Step: 32990, train/grad_norm: 0.00014823862875346094\n",
      "Step: 32990, train/learning_rate: 1.0744883184088394e-05\n",
      "Step: 32990, train/epoch: 7.851023197174072\n",
      "Step: 33000, train/loss: 0.0\n",
      "Step: 33000, train/grad_norm: 0.02654900774359703\n",
      "Step: 33000, train/learning_rate: 1.0732984264905099e-05\n",
      "Step: 33000, train/epoch: 7.853403091430664\n",
      "Step: 33010, train/loss: 0.0\n",
      "Step: 33010, train/grad_norm: 0.00011035453644581139\n",
      "Step: 33010, train/learning_rate: 1.0721085345721804e-05\n",
      "Step: 33010, train/epoch: 7.855782985687256\n",
      "Step: 33020, train/loss: 0.0\n",
      "Step: 33020, train/grad_norm: 7.314758840948343e-05\n",
      "Step: 33020, train/learning_rate: 1.0709186426538508e-05\n",
      "Step: 33020, train/epoch: 7.858162879943848\n",
      "Step: 33030, train/loss: 0.0\n",
      "Step: 33030, train/grad_norm: 0.00015709217404946685\n",
      "Step: 33030, train/learning_rate: 1.0697286597860511e-05\n",
      "Step: 33030, train/epoch: 7.8605427742004395\n",
      "Step: 33040, train/loss: 0.0\n",
      "Step: 33040, train/grad_norm: 9.522468462819234e-05\n",
      "Step: 33040, train/learning_rate: 1.0685387678677216e-05\n",
      "Step: 33040, train/epoch: 7.862922191619873\n",
      "Step: 33050, train/loss: 0.0\n",
      "Step: 33050, train/grad_norm: 8.598736167186871e-05\n",
      "Step: 33050, train/learning_rate: 1.0673488759493921e-05\n",
      "Step: 33050, train/epoch: 7.865302085876465\n",
      "Step: 33060, train/loss: 0.0\n",
      "Step: 33060, train/grad_norm: 0.0010775068076327443\n",
      "Step: 33060, train/learning_rate: 1.0661589840310626e-05\n",
      "Step: 33060, train/epoch: 7.867681980133057\n",
      "Step: 33070, train/loss: 0.0\n",
      "Step: 33070, train/grad_norm: 0.00016114150639623404\n",
      "Step: 33070, train/learning_rate: 1.064969092112733e-05\n",
      "Step: 33070, train/epoch: 7.870061874389648\n",
      "Step: 33080, train/loss: 0.0\n",
      "Step: 33080, train/grad_norm: 0.00012423442967701703\n",
      "Step: 33080, train/learning_rate: 1.0637791092449334e-05\n",
      "Step: 33080, train/epoch: 7.87244176864624\n",
      "Step: 33090, train/loss: 0.0\n",
      "Step: 33090, train/grad_norm: 0.0010432183044031262\n",
      "Step: 33090, train/learning_rate: 1.0625892173266038e-05\n",
      "Step: 33090, train/epoch: 7.874821662902832\n",
      "Step: 33100, train/loss: 0.0\n",
      "Step: 33100, train/grad_norm: 0.0014854531036689878\n",
      "Step: 33100, train/learning_rate: 1.0613993254082743e-05\n",
      "Step: 33100, train/epoch: 7.877201557159424\n",
      "Step: 33110, train/loss: 0.0\n",
      "Step: 33110, train/grad_norm: 0.0001175205543404445\n",
      "Step: 33110, train/learning_rate: 1.0602094334899448e-05\n",
      "Step: 33110, train/epoch: 7.879580974578857\n",
      "Step: 33120, train/loss: 0.0\n",
      "Step: 33120, train/grad_norm: 3.900072624674067e-05\n",
      "Step: 33120, train/learning_rate: 1.0590195415716153e-05\n",
      "Step: 33120, train/epoch: 7.881960868835449\n",
      "Step: 33130, train/loss: 0.0\n",
      "Step: 33130, train/grad_norm: 0.00031086537637747824\n",
      "Step: 33130, train/learning_rate: 1.0578296496532857e-05\n",
      "Step: 33130, train/epoch: 7.884340763092041\n",
      "Step: 33140, train/loss: 0.0\n",
      "Step: 33140, train/grad_norm: 6.630909774685279e-05\n",
      "Step: 33140, train/learning_rate: 1.056639666785486e-05\n",
      "Step: 33140, train/epoch: 7.886720657348633\n",
      "Step: 33150, train/loss: 0.0\n",
      "Step: 33150, train/grad_norm: 0.00013874158321414143\n",
      "Step: 33150, train/learning_rate: 1.0554497748671565e-05\n",
      "Step: 33150, train/epoch: 7.889100551605225\n",
      "Step: 33160, train/loss: 0.0\n",
      "Step: 33160, train/grad_norm: 0.0001482913357904181\n",
      "Step: 33160, train/learning_rate: 1.054259882948827e-05\n",
      "Step: 33160, train/epoch: 7.891480445861816\n",
      "Step: 33170, train/loss: 0.0\n",
      "Step: 33170, train/grad_norm: 3.793369251070544e-05\n",
      "Step: 33170, train/learning_rate: 1.0530699910304975e-05\n",
      "Step: 33170, train/epoch: 7.89385986328125\n",
      "Step: 33180, train/loss: 0.0\n",
      "Step: 33180, train/grad_norm: 8.24731687316671e-05\n",
      "Step: 33180, train/learning_rate: 1.051880099112168e-05\n",
      "Step: 33180, train/epoch: 7.896239757537842\n",
      "Step: 33190, train/loss: 0.0\n",
      "Step: 33190, train/grad_norm: 4.03953054046724e-05\n",
      "Step: 33190, train/learning_rate: 1.0506901162443683e-05\n",
      "Step: 33190, train/epoch: 7.898619651794434\n",
      "Step: 33200, train/loss: 0.0\n",
      "Step: 33200, train/grad_norm: 0.0009380977135151625\n",
      "Step: 33200, train/learning_rate: 1.0495002243260387e-05\n",
      "Step: 33200, train/epoch: 7.900999546051025\n",
      "Step: 33210, train/loss: 0.0\n",
      "Step: 33210, train/grad_norm: 0.0011090215994045138\n",
      "Step: 33210, train/learning_rate: 1.0483103324077092e-05\n",
      "Step: 33210, train/epoch: 7.903379440307617\n",
      "Step: 33220, train/loss: 0.0\n",
      "Step: 33220, train/grad_norm: 6.200478674145415e-05\n",
      "Step: 33220, train/learning_rate: 1.0471204404893797e-05\n",
      "Step: 33220, train/epoch: 7.905759334564209\n",
      "Step: 33230, train/loss: 0.0\n",
      "Step: 33230, train/grad_norm: 0.0020865171682089567\n",
      "Step: 33230, train/learning_rate: 1.0459305485710502e-05\n",
      "Step: 33230, train/epoch: 7.908138751983643\n",
      "Step: 33240, train/loss: 0.0\n",
      "Step: 33240, train/grad_norm: 0.00013882324856240302\n",
      "Step: 33240, train/learning_rate: 1.0447405657032505e-05\n",
      "Step: 33240, train/epoch: 7.910518646240234\n",
      "Step: 33250, train/loss: 0.0\n",
      "Step: 33250, train/grad_norm: 0.0012830104678869247\n",
      "Step: 33250, train/learning_rate: 1.043550673784921e-05\n",
      "Step: 33250, train/epoch: 7.912898540496826\n",
      "Step: 33260, train/loss: 0.0\n",
      "Step: 33260, train/grad_norm: 0.0015711482847109437\n",
      "Step: 33260, train/learning_rate: 1.0423607818665914e-05\n",
      "Step: 33260, train/epoch: 7.915278434753418\n",
      "Step: 33270, train/loss: 0.0\n",
      "Step: 33270, train/grad_norm: 0.000179988332092762\n",
      "Step: 33270, train/learning_rate: 1.0411708899482619e-05\n",
      "Step: 33270, train/epoch: 7.91765832901001\n",
      "Step: 33280, train/loss: 0.0\n",
      "Step: 33280, train/grad_norm: 0.0001512025046395138\n",
      "Step: 33280, train/learning_rate: 1.0399809980299324e-05\n",
      "Step: 33280, train/epoch: 7.920038223266602\n",
      "Step: 33290, train/loss: 0.0\n",
      "Step: 33290, train/grad_norm: 0.0008181327139027417\n",
      "Step: 33290, train/learning_rate: 1.0387910151621327e-05\n",
      "Step: 33290, train/epoch: 7.922418117523193\n",
      "Step: 33300, train/loss: 0.0\n",
      "Step: 33300, train/grad_norm: 3.72588328900747e-05\n",
      "Step: 33300, train/learning_rate: 1.0376011232438032e-05\n",
      "Step: 33300, train/epoch: 7.924797534942627\n",
      "Step: 33310, train/loss: 0.0\n",
      "Step: 33310, train/grad_norm: 5.668683661497198e-05\n",
      "Step: 33310, train/learning_rate: 1.0364112313254736e-05\n",
      "Step: 33310, train/epoch: 7.927177429199219\n",
      "Step: 33320, train/loss: 0.0\n",
      "Step: 33320, train/grad_norm: 0.00022359826834872365\n",
      "Step: 33320, train/learning_rate: 1.0352213394071441e-05\n",
      "Step: 33320, train/epoch: 7.9295573234558105\n",
      "Step: 33330, train/loss: 0.010099999606609344\n",
      "Step: 33330, train/grad_norm: 0.001845352933742106\n",
      "Step: 33330, train/learning_rate: 1.0340314474888146e-05\n",
      "Step: 33330, train/epoch: 7.931937217712402\n",
      "Step: 33340, train/loss: 0.0\n",
      "Step: 33340, train/grad_norm: 0.00014598850975744426\n",
      "Step: 33340, train/learning_rate: 1.0328414646210149e-05\n",
      "Step: 33340, train/epoch: 7.934317111968994\n",
      "Step: 33350, train/loss: 0.0\n",
      "Step: 33350, train/grad_norm: 0.00013254421355668455\n",
      "Step: 33350, train/learning_rate: 1.0316515727026854e-05\n",
      "Step: 33350, train/epoch: 7.936697006225586\n",
      "Step: 33360, train/loss: 0.0\n",
      "Step: 33360, train/grad_norm: 0.0011077445233240724\n",
      "Step: 33360, train/learning_rate: 1.0304616807843558e-05\n",
      "Step: 33360, train/epoch: 7.9390764236450195\n",
      "Step: 33370, train/loss: 0.0\n",
      "Step: 33370, train/grad_norm: 0.005624331068247557\n",
      "Step: 33370, train/learning_rate: 1.0292717888660263e-05\n",
      "Step: 33370, train/epoch: 7.941456317901611\n",
      "Step: 33380, train/loss: 0.0\n",
      "Step: 33380, train/grad_norm: 0.0003900217416230589\n",
      "Step: 33380, train/learning_rate: 1.0280818969476968e-05\n",
      "Step: 33380, train/epoch: 7.943836212158203\n",
      "Step: 33390, train/loss: 0.0\n",
      "Step: 33390, train/grad_norm: 0.0002452085027471185\n",
      "Step: 33390, train/learning_rate: 1.0268919140798971e-05\n",
      "Step: 33390, train/epoch: 7.946216106414795\n",
      "Step: 33400, train/loss: 0.0\n",
      "Step: 33400, train/grad_norm: 5.243356281425804e-05\n",
      "Step: 33400, train/learning_rate: 1.0257020221615676e-05\n",
      "Step: 33400, train/epoch: 7.948596000671387\n",
      "Step: 33410, train/loss: 0.0\n",
      "Step: 33410, train/grad_norm: 2.4627557650092058e-05\n",
      "Step: 33410, train/learning_rate: 1.024512130243238e-05\n",
      "Step: 33410, train/epoch: 7.9509758949279785\n",
      "Step: 33420, train/loss: 0.0\n",
      "Step: 33420, train/grad_norm: 9.099871931539383e-06\n",
      "Step: 33420, train/learning_rate: 1.0233222383249085e-05\n",
      "Step: 33420, train/epoch: 7.953355312347412\n",
      "Step: 33430, train/loss: 0.0\n",
      "Step: 33430, train/grad_norm: 6.700268568238243e-05\n",
      "Step: 33430, train/learning_rate: 1.022132346406579e-05\n",
      "Step: 33430, train/epoch: 7.955735206604004\n",
      "Step: 33440, train/loss: 0.0\n",
      "Step: 33440, train/grad_norm: 1.4311525774246547e-05\n",
      "Step: 33440, train/learning_rate: 1.0209423635387793e-05\n",
      "Step: 33440, train/epoch: 7.958115100860596\n",
      "Step: 33450, train/loss: 0.0\n",
      "Step: 33450, train/grad_norm: 3.178335464326665e-05\n",
      "Step: 33450, train/learning_rate: 1.0197524716204498e-05\n",
      "Step: 33450, train/epoch: 7.9604949951171875\n",
      "Step: 33460, train/loss: 0.0\n",
      "Step: 33460, train/grad_norm: 2.5418587028980255e-05\n",
      "Step: 33460, train/learning_rate: 1.0185625797021203e-05\n",
      "Step: 33460, train/epoch: 7.962874889373779\n",
      "Step: 33470, train/loss: 0.01269999984651804\n",
      "Step: 33470, train/grad_norm: 0.00015084703045431525\n",
      "Step: 33470, train/learning_rate: 1.0173726877837908e-05\n",
      "Step: 33470, train/epoch: 7.965254783630371\n",
      "Step: 33480, train/loss: 0.15780000388622284\n",
      "Step: 33480, train/grad_norm: 160.2928466796875\n",
      "Step: 33480, train/learning_rate: 1.0161827958654612e-05\n",
      "Step: 33480, train/epoch: 7.967634677886963\n",
      "Step: 33490, train/loss: 0.0\n",
      "Step: 33490, train/grad_norm: 0.001625190838240087\n",
      "Step: 33490, train/learning_rate: 1.0149929039471317e-05\n",
      "Step: 33490, train/epoch: 7.9700140953063965\n",
      "Step: 33500, train/loss: 0.0\n",
      "Step: 33500, train/grad_norm: 0.0018313537584617734\n",
      "Step: 33500, train/learning_rate: 1.013802921079332e-05\n",
      "Step: 33500, train/epoch: 7.972393989562988\n",
      "Step: 33510, train/loss: 0.0\n",
      "Step: 33510, train/grad_norm: 0.3328682780265808\n",
      "Step: 33510, train/learning_rate: 1.0126130291610025e-05\n",
      "Step: 33510, train/epoch: 7.97477388381958\n",
      "Step: 33520, train/loss: 0.0\n",
      "Step: 33520, train/grad_norm: 0.002098886761814356\n",
      "Step: 33520, train/learning_rate: 1.011423137242673e-05\n",
      "Step: 33520, train/epoch: 7.977153778076172\n",
      "Step: 33530, train/loss: 0.0\n",
      "Step: 33530, train/grad_norm: 0.0004806684737559408\n",
      "Step: 33530, train/learning_rate: 1.0102332453243434e-05\n",
      "Step: 33530, train/epoch: 7.979533672332764\n",
      "Step: 33540, train/loss: 0.0\n",
      "Step: 33540, train/grad_norm: 0.00024794923956505954\n",
      "Step: 33540, train/learning_rate: 1.009043353406014e-05\n",
      "Step: 33540, train/epoch: 7.9819135665893555\n",
      "Step: 33550, train/loss: 0.0\n",
      "Step: 33550, train/grad_norm: 0.001234242576174438\n",
      "Step: 33550, train/learning_rate: 1.0078533705382142e-05\n",
      "Step: 33550, train/epoch: 7.984292984008789\n",
      "Step: 33560, train/loss: 0.0\n",
      "Step: 33560, train/grad_norm: 0.0014183416496962309\n",
      "Step: 33560, train/learning_rate: 1.0066634786198847e-05\n",
      "Step: 33560, train/epoch: 7.986672878265381\n",
      "Step: 33570, train/loss: 0.0\n",
      "Step: 33570, train/grad_norm: 0.0005043962155468762\n",
      "Step: 33570, train/learning_rate: 1.0054735867015552e-05\n",
      "Step: 33570, train/epoch: 7.989052772521973\n",
      "Step: 33580, train/loss: 0.004100000020116568\n",
      "Step: 33580, train/grad_norm: 0.008329402655363083\n",
      "Step: 33580, train/learning_rate: 1.0042836947832257e-05\n",
      "Step: 33580, train/epoch: 7.9914326667785645\n",
      "Step: 33590, train/loss: 0.0\n",
      "Step: 33590, train/grad_norm: 0.0013955151662230492\n",
      "Step: 33590, train/learning_rate: 1.0030938028648961e-05\n",
      "Step: 33590, train/epoch: 7.993812561035156\n",
      "Step: 33600, train/loss: 0.13979999721050262\n",
      "Step: 33600, train/grad_norm: 76.4635238647461\n",
      "Step: 33600, train/learning_rate: 1.0019038199970964e-05\n",
      "Step: 33600, train/epoch: 7.996192455291748\n",
      "Step: 33610, train/loss: 0.0\n",
      "Step: 33610, train/grad_norm: 0.0008195582777261734\n",
      "Step: 33610, train/learning_rate: 1.0007139280787669e-05\n",
      "Step: 33610, train/epoch: 7.998571872711182\n",
      "Step: 33616, eval/loss: 0.015864448621869087\n",
      "Step: 33616, eval/accuracy: 0.9969457387924194\n",
      "Step: 33616, eval/f1: 0.9967763423919678\n",
      "Step: 33616, eval/runtime: 857.0449829101562\n",
      "Step: 33616, eval/samples_per_second: 8.404000282287598\n",
      "Step: 33616, eval/steps_per_second: 1.0509999990463257\n",
      "Step: 33616, train/epoch: 8.0\n",
      "Step: 33620, train/loss: 0.0\n",
      "Step: 33620, train/grad_norm: 0.05011816695332527\n",
      "Step: 33620, train/learning_rate: 9.995240361604374e-06\n",
      "Step: 33620, train/epoch: 8.000951766967773\n",
      "Step: 33630, train/loss: 0.0\n",
      "Step: 33630, train/grad_norm: 0.010568664409220219\n",
      "Step: 33630, train/learning_rate: 9.983341442421079e-06\n",
      "Step: 33630, train/epoch: 8.003332138061523\n",
      "Step: 33640, train/loss: 0.0\n",
      "Step: 33640, train/grad_norm: 0.005443404894322157\n",
      "Step: 33640, train/learning_rate: 9.971442523237783e-06\n",
      "Step: 33640, train/epoch: 8.005711555480957\n",
      "Step: 33650, train/loss: 0.0\n",
      "Step: 33650, train/grad_norm: 0.0008704017964191735\n",
      "Step: 33650, train/learning_rate: 9.959542694559786e-06\n",
      "Step: 33650, train/epoch: 8.00809097290039\n",
      "Step: 33660, train/loss: 0.0\n",
      "Step: 33660, train/grad_norm: 0.003881054697558284\n",
      "Step: 33660, train/learning_rate: 9.947643775376491e-06\n",
      "Step: 33660, train/epoch: 8.01047134399414\n",
      "Step: 33670, train/loss: 0.0\n",
      "Step: 33670, train/grad_norm: 0.0003623605880420655\n",
      "Step: 33670, train/learning_rate: 9.935744856193196e-06\n",
      "Step: 33670, train/epoch: 8.012850761413574\n",
      "Step: 33680, train/loss: 0.0\n",
      "Step: 33680, train/grad_norm: 0.0006089427042752504\n",
      "Step: 33680, train/learning_rate: 9.9238459370099e-06\n",
      "Step: 33680, train/epoch: 8.015231132507324\n",
      "Step: 33690, train/loss: 0.0\n",
      "Step: 33690, train/grad_norm: 0.028515784069895744\n",
      "Step: 33690, train/learning_rate: 9.911947017826606e-06\n",
      "Step: 33690, train/epoch: 8.017610549926758\n",
      "Step: 33700, train/loss: 0.0\n",
      "Step: 33700, train/grad_norm: 0.03940701112151146\n",
      "Step: 33700, train/learning_rate: 9.900047189148609e-06\n",
      "Step: 33700, train/epoch: 8.019990921020508\n",
      "Step: 33710, train/loss: 0.0\n",
      "Step: 33710, train/grad_norm: 0.0003005920734722167\n",
      "Step: 33710, train/learning_rate: 9.888148269965313e-06\n",
      "Step: 33710, train/epoch: 8.022370338439941\n",
      "Step: 33720, train/loss: 0.0\n",
      "Step: 33720, train/grad_norm: 0.00020876104827038944\n",
      "Step: 33720, train/learning_rate: 9.876249350782018e-06\n",
      "Step: 33720, train/epoch: 8.024749755859375\n",
      "Step: 33730, train/loss: 0.0\n",
      "Step: 33730, train/grad_norm: 0.0025739152915775776\n",
      "Step: 33730, train/learning_rate: 9.864350431598723e-06\n",
      "Step: 33730, train/epoch: 8.027130126953125\n",
      "Step: 33740, train/loss: 0.0\n",
      "Step: 33740, train/grad_norm: 0.0006328527233563364\n",
      "Step: 33740, train/learning_rate: 9.852451512415428e-06\n",
      "Step: 33740, train/epoch: 8.029509544372559\n",
      "Step: 33750, train/loss: 0.0\n",
      "Step: 33750, train/grad_norm: 0.0012629301054403186\n",
      "Step: 33750, train/learning_rate: 9.84055168373743e-06\n",
      "Step: 33750, train/epoch: 8.031889915466309\n",
      "Step: 33760, train/loss: 0.0\n",
      "Step: 33760, train/grad_norm: 0.0008304599905386567\n",
      "Step: 33760, train/learning_rate: 9.828652764554136e-06\n",
      "Step: 33760, train/epoch: 8.034269332885742\n",
      "Step: 33770, train/loss: 0.0\n",
      "Step: 33770, train/grad_norm: 0.004059846047312021\n",
      "Step: 33770, train/learning_rate: 9.81675384537084e-06\n",
      "Step: 33770, train/epoch: 8.036648750305176\n",
      "Step: 33780, train/loss: 0.0\n",
      "Step: 33780, train/grad_norm: 0.0002891651529353112\n",
      "Step: 33780, train/learning_rate: 9.804854926187545e-06\n",
      "Step: 33780, train/epoch: 8.039029121398926\n",
      "Step: 33790, train/loss: 0.0\n",
      "Step: 33790, train/grad_norm: 0.0008636931306682527\n",
      "Step: 33790, train/learning_rate: 9.79295600700425e-06\n",
      "Step: 33790, train/epoch: 8.04140853881836\n",
      "Step: 33800, train/loss: 0.0\n",
      "Step: 33800, train/grad_norm: 0.0001351884420728311\n",
      "Step: 33800, train/learning_rate: 9.781057087820955e-06\n",
      "Step: 33800, train/epoch: 8.04378890991211\n",
      "Step: 33810, train/loss: 0.0\n",
      "Step: 33810, train/grad_norm: 0.00026159980916418135\n",
      "Step: 33810, train/learning_rate: 9.769157259142958e-06\n",
      "Step: 33810, train/epoch: 8.046168327331543\n",
      "Step: 33820, train/loss: 0.0\n",
      "Step: 33820, train/grad_norm: 0.0001755111588863656\n",
      "Step: 33820, train/learning_rate: 9.757258339959662e-06\n",
      "Step: 33820, train/epoch: 8.048548698425293\n",
      "Step: 33830, train/loss: 0.0\n",
      "Step: 33830, train/grad_norm: 0.0008208636427298188\n",
      "Step: 33830, train/learning_rate: 9.745359420776367e-06\n",
      "Step: 33830, train/epoch: 8.050928115844727\n",
      "Step: 33840, train/loss: 0.0\n",
      "Step: 33840, train/grad_norm: 0.000713926914613694\n",
      "Step: 33840, train/learning_rate: 9.733460501593072e-06\n",
      "Step: 33840, train/epoch: 8.05330753326416\n",
      "Step: 33850, train/loss: 0.0\n",
      "Step: 33850, train/grad_norm: 0.0002510594204068184\n",
      "Step: 33850, train/learning_rate: 9.721561582409777e-06\n",
      "Step: 33850, train/epoch: 8.05568790435791\n",
      "Step: 33860, train/loss: 0.0\n",
      "Step: 33860, train/grad_norm: 0.04074440523982048\n",
      "Step: 33860, train/learning_rate: 9.70966175373178e-06\n",
      "Step: 33860, train/epoch: 8.058067321777344\n",
      "Step: 33870, train/loss: 0.0\n",
      "Step: 33870, train/grad_norm: 0.00035196682438254356\n",
      "Step: 33870, train/learning_rate: 9.697762834548485e-06\n",
      "Step: 33870, train/epoch: 8.060447692871094\n",
      "Step: 33880, train/loss: 0.0\n",
      "Step: 33880, train/grad_norm: 0.00014590550563298166\n",
      "Step: 33880, train/learning_rate: 9.68586391536519e-06\n",
      "Step: 33880, train/epoch: 8.062827110290527\n",
      "Step: 33890, train/loss: 0.0\n",
      "Step: 33890, train/grad_norm: 0.0007036175229586661\n",
      "Step: 33890, train/learning_rate: 9.673964996181894e-06\n",
      "Step: 33890, train/epoch: 8.065207481384277\n",
      "Step: 33900, train/loss: 0.0\n",
      "Step: 33900, train/grad_norm: 0.0004280333232600242\n",
      "Step: 33900, train/learning_rate: 9.662066076998599e-06\n",
      "Step: 33900, train/epoch: 8.067586898803711\n",
      "Step: 33910, train/loss: 0.0\n",
      "Step: 33910, train/grad_norm: 0.00018902364536188543\n",
      "Step: 33910, train/learning_rate: 9.650166248320602e-06\n",
      "Step: 33910, train/epoch: 8.069966316223145\n",
      "Step: 33920, train/loss: 0.0\n",
      "Step: 33920, train/grad_norm: 0.00011144586460432038\n",
      "Step: 33920, train/learning_rate: 9.638267329137307e-06\n",
      "Step: 33920, train/epoch: 8.072346687316895\n",
      "Step: 33930, train/loss: 0.03889999911189079\n",
      "Step: 33930, train/grad_norm: 0.00014990181080065668\n",
      "Step: 33930, train/learning_rate: 9.626368409954011e-06\n",
      "Step: 33930, train/epoch: 8.074726104736328\n",
      "Step: 33940, train/loss: 0.0010999999940395355\n",
      "Step: 33940, train/grad_norm: 2.0043567928951234e-05\n",
      "Step: 33940, train/learning_rate: 9.614469490770716e-06\n",
      "Step: 33940, train/epoch: 8.077106475830078\n",
      "Step: 33950, train/loss: 0.04610000178217888\n",
      "Step: 33950, train/grad_norm: 0.00012618083565030247\n",
      "Step: 33950, train/learning_rate: 9.602570571587421e-06\n",
      "Step: 33950, train/epoch: 8.079485893249512\n",
      "Step: 33960, train/loss: 0.0\n",
      "Step: 33960, train/grad_norm: 0.0010274728992953897\n",
      "Step: 33960, train/learning_rate: 9.590670742909424e-06\n",
      "Step: 33960, train/epoch: 8.081865310668945\n",
      "Step: 33970, train/loss: 0.0\n",
      "Step: 33970, train/grad_norm: 0.06634681671857834\n",
      "Step: 33970, train/learning_rate: 9.578771823726129e-06\n",
      "Step: 33970, train/epoch: 8.084245681762695\n",
      "Step: 33980, train/loss: 0.0\n",
      "Step: 33980, train/grad_norm: 0.0006747245788574219\n",
      "Step: 33980, train/learning_rate: 9.566872904542834e-06\n",
      "Step: 33980, train/epoch: 8.086625099182129\n",
      "Step: 33990, train/loss: 0.0\n",
      "Step: 33990, train/grad_norm: 0.00037503347266465425\n",
      "Step: 33990, train/learning_rate: 9.554973985359538e-06\n",
      "Step: 33990, train/epoch: 8.089005470275879\n",
      "Step: 34000, train/loss: 0.0\n",
      "Step: 34000, train/grad_norm: 0.00011954951332882047\n",
      "Step: 34000, train/learning_rate: 9.543075066176243e-06\n",
      "Step: 34000, train/epoch: 8.091384887695312\n",
      "Step: 34010, train/loss: 0.0\n",
      "Step: 34010, train/grad_norm: 0.00016651942860335112\n",
      "Step: 34010, train/learning_rate: 9.531175237498246e-06\n",
      "Step: 34010, train/epoch: 8.093765258789062\n",
      "Step: 34020, train/loss: 0.0\n",
      "Step: 34020, train/grad_norm: 0.00021948468929622322\n",
      "Step: 34020, train/learning_rate: 9.519276318314951e-06\n",
      "Step: 34020, train/epoch: 8.096144676208496\n",
      "Step: 34030, train/loss: 0.0\n",
      "Step: 34030, train/grad_norm: 0.002858932362869382\n",
      "Step: 34030, train/learning_rate: 9.507377399131656e-06\n",
      "Step: 34030, train/epoch: 8.09852409362793\n",
      "Step: 34040, train/loss: 0.0\n",
      "Step: 34040, train/grad_norm: 0.00017585964815225452\n",
      "Step: 34040, train/learning_rate: 9.49547847994836e-06\n",
      "Step: 34040, train/epoch: 8.10090446472168\n",
      "Step: 34050, train/loss: 0.0\n",
      "Step: 34050, train/grad_norm: 0.0002511452476028353\n",
      "Step: 34050, train/learning_rate: 9.483579560765065e-06\n",
      "Step: 34050, train/epoch: 8.103283882141113\n",
      "Step: 34060, train/loss: 0.0\n",
      "Step: 34060, train/grad_norm: 8.403061656281352e-05\n",
      "Step: 34060, train/learning_rate: 9.471679732087068e-06\n",
      "Step: 34060, train/epoch: 8.105664253234863\n",
      "Step: 34070, train/loss: 0.0\n",
      "Step: 34070, train/grad_norm: 2.8583915991475806e-05\n",
      "Step: 34070, train/learning_rate: 9.459780812903773e-06\n",
      "Step: 34070, train/epoch: 8.108043670654297\n",
      "Step: 34080, train/loss: 0.0\n",
      "Step: 34080, train/grad_norm: 0.0017732256092131138\n",
      "Step: 34080, train/learning_rate: 9.447881893720478e-06\n",
      "Step: 34080, train/epoch: 8.110424041748047\n",
      "Step: 34090, train/loss: 0.0\n",
      "Step: 34090, train/grad_norm: 9.637013863539323e-05\n",
      "Step: 34090, train/learning_rate: 9.435982974537183e-06\n",
      "Step: 34090, train/epoch: 8.11280345916748\n",
      "Step: 34100, train/loss: 0.0\n",
      "Step: 34100, train/grad_norm: 0.00012033469101879746\n",
      "Step: 34100, train/learning_rate: 9.424084055353887e-06\n",
      "Step: 34100, train/epoch: 8.115182876586914\n",
      "Step: 34110, train/loss: 0.0\n",
      "Step: 34110, train/grad_norm: 0.00011236681893933564\n",
      "Step: 34110, train/learning_rate: 9.41218422667589e-06\n",
      "Step: 34110, train/epoch: 8.117563247680664\n",
      "Step: 34120, train/loss: 0.0\n",
      "Step: 34120, train/grad_norm: 9.441380097996444e-05\n",
      "Step: 34120, train/learning_rate: 9.400285307492595e-06\n",
      "Step: 34120, train/epoch: 8.119942665100098\n",
      "Step: 34130, train/loss: 0.0005000000237487257\n",
      "Step: 34130, train/grad_norm: 0.00614793598651886\n",
      "Step: 34130, train/learning_rate: 9.3883863883093e-06\n",
      "Step: 34130, train/epoch: 8.122323036193848\n",
      "Step: 34140, train/loss: 0.0\n",
      "Step: 34140, train/grad_norm: 3.750082032638602e-05\n",
      "Step: 34140, train/learning_rate: 9.376487469126005e-06\n",
      "Step: 34140, train/epoch: 8.124702453613281\n",
      "Step: 34150, train/loss: 0.0\n",
      "Step: 34150, train/grad_norm: 1.6210553440032527e-05\n",
      "Step: 34150, train/learning_rate: 9.36458854994271e-06\n",
      "Step: 34150, train/epoch: 8.127081871032715\n",
      "Step: 34160, train/loss: 0.0\n",
      "Step: 34160, train/grad_norm: 9.199540363624692e-05\n",
      "Step: 34160, train/learning_rate: 9.352689630759414e-06\n",
      "Step: 34160, train/epoch: 8.129462242126465\n",
      "Step: 34170, train/loss: 0.0\n",
      "Step: 34170, train/grad_norm: 3.7876583519391716e-05\n",
      "Step: 34170, train/learning_rate: 9.340789802081417e-06\n",
      "Step: 34170, train/epoch: 8.131841659545898\n",
      "Step: 34180, train/loss: 0.0\n",
      "Step: 34180, train/grad_norm: 0.00023784612130839378\n",
      "Step: 34180, train/learning_rate: 9.328890882898122e-06\n",
      "Step: 34180, train/epoch: 8.134222030639648\n",
      "Step: 34190, train/loss: 0.0\n",
      "Step: 34190, train/grad_norm: 2.106118517986033e-05\n",
      "Step: 34190, train/learning_rate: 9.316991963714827e-06\n",
      "Step: 34190, train/epoch: 8.136601448059082\n",
      "Step: 34200, train/loss: 0.07109999656677246\n",
      "Step: 34200, train/grad_norm: 0.0001803347549866885\n",
      "Step: 34200, train/learning_rate: 9.305093044531532e-06\n",
      "Step: 34200, train/epoch: 8.138981819152832\n",
      "Step: 34210, train/loss: 0.0\n",
      "Step: 34210, train/grad_norm: 0.00040005682967603207\n",
      "Step: 34210, train/learning_rate: 9.293194125348236e-06\n",
      "Step: 34210, train/epoch: 8.141361236572266\n",
      "Step: 34220, train/loss: 0.0\n",
      "Step: 34220, train/grad_norm: 0.0004573914338834584\n",
      "Step: 34220, train/learning_rate: 9.28129429667024e-06\n",
      "Step: 34220, train/epoch: 8.1437406539917\n",
      "Step: 34230, train/loss: 0.13750000298023224\n",
      "Step: 34230, train/grad_norm: 0.010594739578664303\n",
      "Step: 34230, train/learning_rate: 9.269395377486944e-06\n",
      "Step: 34230, train/epoch: 8.14612102508545\n",
      "Step: 34240, train/loss: 9.999999747378752e-05\n",
      "Step: 34240, train/grad_norm: 0.0033458955585956573\n",
      "Step: 34240, train/learning_rate: 9.257496458303649e-06\n",
      "Step: 34240, train/epoch: 8.148500442504883\n",
      "Step: 34250, train/loss: 0.0\n",
      "Step: 34250, train/grad_norm: 0.03292270004749298\n",
      "Step: 34250, train/learning_rate: 9.245597539120354e-06\n",
      "Step: 34250, train/epoch: 8.150880813598633\n",
      "Step: 34260, train/loss: 0.0\n",
      "Step: 34260, train/grad_norm: 0.0011234093690291047\n",
      "Step: 34260, train/learning_rate: 9.233698619937059e-06\n",
      "Step: 34260, train/epoch: 8.153260231018066\n",
      "Step: 34270, train/loss: 0.0\n",
      "Step: 34270, train/grad_norm: 0.003562723286449909\n",
      "Step: 34270, train/learning_rate: 9.221798791259062e-06\n",
      "Step: 34270, train/epoch: 8.155640602111816\n",
      "Step: 34280, train/loss: 0.0\n",
      "Step: 34280, train/grad_norm: 6.296560604823753e-05\n",
      "Step: 34280, train/learning_rate: 9.209899872075766e-06\n",
      "Step: 34280, train/epoch: 8.15802001953125\n",
      "Step: 34290, train/loss: 0.0\n",
      "Step: 34290, train/grad_norm: 0.000304000626783818\n",
      "Step: 34290, train/learning_rate: 9.198000952892471e-06\n",
      "Step: 34290, train/epoch: 8.160399436950684\n",
      "Step: 34300, train/loss: 0.0\n",
      "Step: 34300, train/grad_norm: 7.303900201804936e-05\n",
      "Step: 34300, train/learning_rate: 9.186102033709176e-06\n",
      "Step: 34300, train/epoch: 8.162779808044434\n",
      "Step: 34310, train/loss: 0.0\n",
      "Step: 34310, train/grad_norm: 0.0003471183590590954\n",
      "Step: 34310, train/learning_rate: 9.17420311452588e-06\n",
      "Step: 34310, train/epoch: 8.165159225463867\n",
      "Step: 34320, train/loss: 0.0\n",
      "Step: 34320, train/grad_norm: 0.0004180142714176327\n",
      "Step: 34320, train/learning_rate: 9.162303285847884e-06\n",
      "Step: 34320, train/epoch: 8.167539596557617\n",
      "Step: 34330, train/loss: 0.0\n",
      "Step: 34330, train/grad_norm: 0.00023347280512098223\n",
      "Step: 34330, train/learning_rate: 9.150404366664588e-06\n",
      "Step: 34330, train/epoch: 8.16991901397705\n",
      "Step: 34340, train/loss: 0.0\n",
      "Step: 34340, train/grad_norm: 0.0004000925400760025\n",
      "Step: 34340, train/learning_rate: 9.138505447481293e-06\n",
      "Step: 34340, train/epoch: 8.172298431396484\n",
      "Step: 34350, train/loss: 0.0\n",
      "Step: 34350, train/grad_norm: 0.0014250061940401793\n",
      "Step: 34350, train/learning_rate: 9.126606528297998e-06\n",
      "Step: 34350, train/epoch: 8.174678802490234\n",
      "Step: 34360, train/loss: 0.0\n",
      "Step: 34360, train/grad_norm: 0.0018887044861912727\n",
      "Step: 34360, train/learning_rate: 9.114707609114703e-06\n",
      "Step: 34360, train/epoch: 8.177058219909668\n",
      "Step: 34370, train/loss: 0.0\n",
      "Step: 34370, train/grad_norm: 0.00045941901043988764\n",
      "Step: 34370, train/learning_rate: 9.102807780436706e-06\n",
      "Step: 34370, train/epoch: 8.179438591003418\n",
      "Step: 34380, train/loss: 0.0\n",
      "Step: 34380, train/grad_norm: 0.0007205202709883451\n",
      "Step: 34380, train/learning_rate: 9.09090886125341e-06\n",
      "Step: 34380, train/epoch: 8.181818008422852\n",
      "Step: 34390, train/loss: 0.0\n",
      "Step: 34390, train/grad_norm: 4.984156112186611e-05\n",
      "Step: 34390, train/learning_rate: 9.079009942070115e-06\n",
      "Step: 34390, train/epoch: 8.184198379516602\n",
      "Step: 34400, train/loss: 0.0\n",
      "Step: 34400, train/grad_norm: 0.0018602119525894523\n",
      "Step: 34400, train/learning_rate: 9.06711102288682e-06\n",
      "Step: 34400, train/epoch: 8.186577796936035\n",
      "Step: 34410, train/loss: 0.0\n",
      "Step: 34410, train/grad_norm: 0.0005911999032832682\n",
      "Step: 34410, train/learning_rate: 9.055212103703525e-06\n",
      "Step: 34410, train/epoch: 8.188957214355469\n",
      "Step: 34420, train/loss: 0.0\n",
      "Step: 34420, train/grad_norm: 2.4909064450184815e-05\n",
      "Step: 34420, train/learning_rate: 9.043312275025528e-06\n",
      "Step: 34420, train/epoch: 8.191337585449219\n",
      "Step: 34430, train/loss: 0.0\n",
      "Step: 34430, train/grad_norm: 5.012991823605262e-05\n",
      "Step: 34430, train/learning_rate: 9.031413355842233e-06\n",
      "Step: 34430, train/epoch: 8.193717002868652\n",
      "Step: 34440, train/loss: 0.0\n",
      "Step: 34440, train/grad_norm: 3.6317287595011294e-05\n",
      "Step: 34440, train/learning_rate: 9.019514436658937e-06\n",
      "Step: 34440, train/epoch: 8.196097373962402\n",
      "Step: 34450, train/loss: 0.0\n",
      "Step: 34450, train/grad_norm: 0.0040966845117509365\n",
      "Step: 34450, train/learning_rate: 9.007615517475642e-06\n",
      "Step: 34450, train/epoch: 8.198476791381836\n",
      "Step: 34460, train/loss: 0.0\n",
      "Step: 34460, train/grad_norm: 0.00027584636700339615\n",
      "Step: 34460, train/learning_rate: 8.995716598292347e-06\n",
      "Step: 34460, train/epoch: 8.200857162475586\n",
      "Step: 34470, train/loss: 0.0\n",
      "Step: 34470, train/grad_norm: 0.0010747724445536733\n",
      "Step: 34470, train/learning_rate: 8.983817679109052e-06\n",
      "Step: 34470, train/epoch: 8.20323657989502\n",
      "Step: 34480, train/loss: 0.0\n",
      "Step: 34480, train/grad_norm: 0.00013283758016768843\n",
      "Step: 34480, train/learning_rate: 8.971917850431055e-06\n",
      "Step: 34480, train/epoch: 8.205615997314453\n",
      "Step: 34490, train/loss: 0.0\n",
      "Step: 34490, train/grad_norm: 0.00034723704447969794\n",
      "Step: 34490, train/learning_rate: 8.96001893124776e-06\n",
      "Step: 34490, train/epoch: 8.207996368408203\n",
      "Step: 34500, train/loss: 0.0\n",
      "Step: 34500, train/grad_norm: 0.0001825485087465495\n",
      "Step: 34500, train/learning_rate: 8.948120012064464e-06\n",
      "Step: 34500, train/epoch: 8.210375785827637\n",
      "Step: 34510, train/loss: 0.0\n",
      "Step: 34510, train/grad_norm: 0.0002965530438814312\n",
      "Step: 34510, train/learning_rate: 8.93622109288117e-06\n",
      "Step: 34510, train/epoch: 8.212756156921387\n",
      "Step: 34520, train/loss: 0.0\n",
      "Step: 34520, train/grad_norm: 0.0002147662453353405\n",
      "Step: 34520, train/learning_rate: 8.924322173697874e-06\n",
      "Step: 34520, train/epoch: 8.21513557434082\n",
      "Step: 34530, train/loss: 0.0\n",
      "Step: 34530, train/grad_norm: 0.0008971949573606253\n",
      "Step: 34530, train/learning_rate: 8.912422345019877e-06\n",
      "Step: 34530, train/epoch: 8.21751594543457\n",
      "Step: 34540, train/loss: 0.0\n",
      "Step: 34540, train/grad_norm: 9.805463923839852e-05\n",
      "Step: 34540, train/learning_rate: 8.900523425836582e-06\n",
      "Step: 34540, train/epoch: 8.219895362854004\n",
      "Step: 34550, train/loss: 0.0\n",
      "Step: 34550, train/grad_norm: 3.632662264863029e-05\n",
      "Step: 34550, train/learning_rate: 8.888624506653287e-06\n",
      "Step: 34550, train/epoch: 8.222274780273438\n",
      "Step: 34560, train/loss: 0.06880000233650208\n",
      "Step: 34560, train/grad_norm: 0.0005326309474185109\n",
      "Step: 34560, train/learning_rate: 8.876725587469991e-06\n",
      "Step: 34560, train/epoch: 8.224655151367188\n",
      "Step: 34570, train/loss: 0.0\n",
      "Step: 34570, train/grad_norm: 0.0005783461383543909\n",
      "Step: 34570, train/learning_rate: 8.864826668286696e-06\n",
      "Step: 34570, train/epoch: 8.227034568786621\n",
      "Step: 34580, train/loss: 0.0\n",
      "Step: 34580, train/grad_norm: 0.000958767079282552\n",
      "Step: 34580, train/learning_rate: 8.852926839608699e-06\n",
      "Step: 34580, train/epoch: 8.229414939880371\n",
      "Step: 34590, train/loss: 0.0\n",
      "Step: 34590, train/grad_norm: 0.0017521425615996122\n",
      "Step: 34590, train/learning_rate: 8.841027920425404e-06\n",
      "Step: 34590, train/epoch: 8.231794357299805\n",
      "Step: 34600, train/loss: 0.05389999970793724\n",
      "Step: 34600, train/grad_norm: 0.0027147093787789345\n",
      "Step: 34600, train/learning_rate: 8.829129001242109e-06\n",
      "Step: 34600, train/epoch: 8.234173774719238\n",
      "Step: 34610, train/loss: 0.0\n",
      "Step: 34610, train/grad_norm: 0.00047308235662057996\n",
      "Step: 34610, train/learning_rate: 8.817230082058813e-06\n",
      "Step: 34610, train/epoch: 8.236554145812988\n",
      "Step: 34620, train/loss: 0.0\n",
      "Step: 34620, train/grad_norm: 0.00118282251060009\n",
      "Step: 34620, train/learning_rate: 8.805331162875518e-06\n",
      "Step: 34620, train/epoch: 8.238933563232422\n",
      "Step: 34630, train/loss: 9.999999747378752e-05\n",
      "Step: 34630, train/grad_norm: 0.0006767752347514033\n",
      "Step: 34630, train/learning_rate: 8.793431334197521e-06\n",
      "Step: 34630, train/epoch: 8.241313934326172\n",
      "Step: 34640, train/loss: 0.0\n",
      "Step: 34640, train/grad_norm: 0.000313021766487509\n",
      "Step: 34640, train/learning_rate: 8.781532415014226e-06\n",
      "Step: 34640, train/epoch: 8.243693351745605\n",
      "Step: 34650, train/loss: 0.0\n",
      "Step: 34650, train/grad_norm: 0.00032132462365552783\n",
      "Step: 34650, train/learning_rate: 8.76963349583093e-06\n",
      "Step: 34650, train/epoch: 8.246073722839355\n",
      "Step: 34660, train/loss: 0.0\n",
      "Step: 34660, train/grad_norm: 0.000523814873304218\n",
      "Step: 34660, train/learning_rate: 8.757734576647636e-06\n",
      "Step: 34660, train/epoch: 8.248453140258789\n",
      "Step: 34670, train/loss: 0.0\n",
      "Step: 34670, train/grad_norm: 0.0002920945407822728\n",
      "Step: 34670, train/learning_rate: 8.74583565746434e-06\n",
      "Step: 34670, train/epoch: 8.250832557678223\n",
      "Step: 34680, train/loss: 0.0\n",
      "Step: 34680, train/grad_norm: 0.0005350146093405783\n",
      "Step: 34680, train/learning_rate: 8.733935828786343e-06\n",
      "Step: 34680, train/epoch: 8.253212928771973\n",
      "Step: 34690, train/loss: 0.0\n",
      "Step: 34690, train/grad_norm: 0.002711367094889283\n",
      "Step: 34690, train/learning_rate: 8.722036909603048e-06\n",
      "Step: 34690, train/epoch: 8.255592346191406\n",
      "Step: 34700, train/loss: 0.0\n",
      "Step: 34700, train/grad_norm: 0.00014813306916039437\n",
      "Step: 34700, train/learning_rate: 8.710137990419753e-06\n",
      "Step: 34700, train/epoch: 8.257972717285156\n",
      "Step: 34710, train/loss: 0.0\n",
      "Step: 34710, train/grad_norm: 0.0016124367248266935\n",
      "Step: 34710, train/learning_rate: 8.698239071236458e-06\n",
      "Step: 34710, train/epoch: 8.26035213470459\n",
      "Step: 34720, train/loss: 0.01759999990463257\n",
      "Step: 34720, train/grad_norm: 0.0040466985665261745\n",
      "Step: 34720, train/learning_rate: 8.686340152053162e-06\n",
      "Step: 34720, train/epoch: 8.26273250579834\n",
      "Step: 34730, train/loss: 0.0\n",
      "Step: 34730, train/grad_norm: 0.0015806227456778288\n",
      "Step: 34730, train/learning_rate: 8.674440323375165e-06\n",
      "Step: 34730, train/epoch: 8.265111923217773\n",
      "Step: 34740, train/loss: 0.0\n",
      "Step: 34740, train/grad_norm: 0.005256405100226402\n",
      "Step: 34740, train/learning_rate: 8.66254140419187e-06\n",
      "Step: 34740, train/epoch: 8.267491340637207\n",
      "Step: 34750, train/loss: 9.999999747378752e-05\n",
      "Step: 34750, train/grad_norm: 0.2895757257938385\n",
      "Step: 34750, train/learning_rate: 8.650642485008575e-06\n",
      "Step: 34750, train/epoch: 8.269871711730957\n",
      "Step: 34760, train/loss: 0.0\n",
      "Step: 34760, train/grad_norm: 0.0016859389143064618\n",
      "Step: 34760, train/learning_rate: 8.63874356582528e-06\n",
      "Step: 34760, train/epoch: 8.27225112915039\n",
      "Step: 34770, train/loss: 0.0\n",
      "Step: 34770, train/grad_norm: 0.0011500846594572067\n",
      "Step: 34770, train/learning_rate: 8.626844646641985e-06\n",
      "Step: 34770, train/epoch: 8.27463150024414\n",
      "Step: 34780, train/loss: 0.0\n",
      "Step: 34780, train/grad_norm: 8.70471922098659e-05\n",
      "Step: 34780, train/learning_rate: 8.614944817963988e-06\n",
      "Step: 34780, train/epoch: 8.277010917663574\n",
      "Step: 34790, train/loss: 0.0\n",
      "Step: 34790, train/grad_norm: 4.797561268787831e-05\n",
      "Step: 34790, train/learning_rate: 8.603045898780692e-06\n",
      "Step: 34790, train/epoch: 8.279390335083008\n",
      "Step: 34800, train/loss: 0.05040000006556511\n",
      "Step: 34800, train/grad_norm: 0.003188069211319089\n",
      "Step: 34800, train/learning_rate: 8.591146979597397e-06\n",
      "Step: 34800, train/epoch: 8.281770706176758\n",
      "Step: 34810, train/loss: 0.0\n",
      "Step: 34810, train/grad_norm: 0.0011991430073976517\n",
      "Step: 34810, train/learning_rate: 8.579248060414102e-06\n",
      "Step: 34810, train/epoch: 8.284150123596191\n",
      "Step: 34820, train/loss: 0.19449999928474426\n",
      "Step: 34820, train/grad_norm: 0.0017407909035682678\n",
      "Step: 34820, train/learning_rate: 8.567349141230807e-06\n",
      "Step: 34820, train/epoch: 8.286530494689941\n",
      "Step: 34830, train/loss: 0.0\n",
      "Step: 34830, train/grad_norm: 0.007771201431751251\n",
      "Step: 34830, train/learning_rate: 8.555450222047511e-06\n",
      "Step: 34830, train/epoch: 8.288909912109375\n",
      "Step: 34840, train/loss: 0.0\n",
      "Step: 34840, train/grad_norm: 0.0013132200110703707\n",
      "Step: 34840, train/learning_rate: 8.543550393369514e-06\n",
      "Step: 34840, train/epoch: 8.291290283203125\n",
      "Step: 34850, train/loss: 0.0\n",
      "Step: 34850, train/grad_norm: 0.0037629420403391123\n",
      "Step: 34850, train/learning_rate: 8.53165147418622e-06\n",
      "Step: 34850, train/epoch: 8.293669700622559\n",
      "Step: 34860, train/loss: 0.0\n",
      "Step: 34860, train/grad_norm: 0.003728679148480296\n",
      "Step: 34860, train/learning_rate: 8.519752555002924e-06\n",
      "Step: 34860, train/epoch: 8.296049118041992\n",
      "Step: 34870, train/loss: 0.0\n",
      "Step: 34870, train/grad_norm: 0.005119862500578165\n",
      "Step: 34870, train/learning_rate: 8.507853635819629e-06\n",
      "Step: 34870, train/epoch: 8.298429489135742\n",
      "Step: 34880, train/loss: 0.0\n",
      "Step: 34880, train/grad_norm: 0.009247723035514355\n",
      "Step: 34880, train/learning_rate: 8.495954716636334e-06\n",
      "Step: 34880, train/epoch: 8.300808906555176\n",
      "Step: 34890, train/loss: 0.0\n",
      "Step: 34890, train/grad_norm: 0.003103917231783271\n",
      "Step: 34890, train/learning_rate: 8.484054887958337e-06\n",
      "Step: 34890, train/epoch: 8.303189277648926\n",
      "Step: 34900, train/loss: 0.0\n",
      "Step: 34900, train/grad_norm: 0.0407867506146431\n",
      "Step: 34900, train/learning_rate: 8.472155968775041e-06\n",
      "Step: 34900, train/epoch: 8.30556869506836\n",
      "Step: 34910, train/loss: 0.0\n",
      "Step: 34910, train/grad_norm: 0.0013031854759901762\n",
      "Step: 34910, train/learning_rate: 8.460257049591746e-06\n",
      "Step: 34910, train/epoch: 8.30794906616211\n",
      "Step: 34920, train/loss: 0.0\n",
      "Step: 34920, train/grad_norm: 0.0015427132602781057\n",
      "Step: 34920, train/learning_rate: 8.448358130408451e-06\n",
      "Step: 34920, train/epoch: 8.310328483581543\n",
      "Step: 34930, train/loss: 0.0\n",
      "Step: 34930, train/grad_norm: 0.007488768082112074\n",
      "Step: 34930, train/learning_rate: 8.436459211225156e-06\n",
      "Step: 34930, train/epoch: 8.312707901000977\n",
      "Step: 34940, train/loss: 0.0\n",
      "Step: 34940, train/grad_norm: 0.0036097834818065166\n",
      "Step: 34940, train/learning_rate: 8.424559382547159e-06\n",
      "Step: 34940, train/epoch: 8.315088272094727\n",
      "Step: 34950, train/loss: 0.0\n",
      "Step: 34950, train/grad_norm: 0.0008492287597618997\n",
      "Step: 34950, train/learning_rate: 8.412660463363864e-06\n",
      "Step: 34950, train/epoch: 8.31746768951416\n",
      "Step: 34960, train/loss: 0.0\n",
      "Step: 34960, train/grad_norm: 0.0004211466293781996\n",
      "Step: 34960, train/learning_rate: 8.400761544180568e-06\n",
      "Step: 34960, train/epoch: 8.31984806060791\n",
      "Step: 34970, train/loss: 0.0\n",
      "Step: 34970, train/grad_norm: 0.0005279454053379595\n",
      "Step: 34970, train/learning_rate: 8.388862624997273e-06\n",
      "Step: 34970, train/epoch: 8.322227478027344\n",
      "Step: 34980, train/loss: 0.0\n",
      "Step: 34980, train/grad_norm: 0.00036094317329116166\n",
      "Step: 34980, train/learning_rate: 8.376963705813978e-06\n",
      "Step: 34980, train/epoch: 8.324606895446777\n",
      "Step: 34990, train/loss: 0.0\n",
      "Step: 34990, train/grad_norm: 0.0008153262315317988\n",
      "Step: 34990, train/learning_rate: 8.365063877135981e-06\n",
      "Step: 34990, train/epoch: 8.326987266540527\n",
      "Step: 35000, train/loss: 0.0\n",
      "Step: 35000, train/grad_norm: 0.0031025251373648643\n",
      "Step: 35000, train/learning_rate: 8.353164957952686e-06\n",
      "Step: 35000, train/epoch: 8.329366683959961\n",
      "Step: 35010, train/loss: 0.0\n",
      "Step: 35010, train/grad_norm: 0.0018716313643381\n",
      "Step: 35010, train/learning_rate: 8.34126603876939e-06\n",
      "Step: 35010, train/epoch: 8.331747055053711\n",
      "Step: 35020, train/loss: 0.0\n",
      "Step: 35020, train/grad_norm: 0.0006380521808750927\n",
      "Step: 35020, train/learning_rate: 8.329367119586095e-06\n",
      "Step: 35020, train/epoch: 8.334126472473145\n",
      "Step: 35030, train/loss: 0.0\n",
      "Step: 35030, train/grad_norm: 0.0011131683131679893\n",
      "Step: 35030, train/learning_rate: 8.3174682004028e-06\n",
      "Step: 35030, train/epoch: 8.336506843566895\n",
      "Step: 35040, train/loss: 0.0\n",
      "Step: 35040, train/grad_norm: 0.00019165330741088837\n",
      "Step: 35040, train/learning_rate: 8.305568371724803e-06\n",
      "Step: 35040, train/epoch: 8.338886260986328\n",
      "Step: 35050, train/loss: 0.0\n",
      "Step: 35050, train/grad_norm: 0.0005338597693480551\n",
      "Step: 35050, train/learning_rate: 8.293669452541508e-06\n",
      "Step: 35050, train/epoch: 8.341265678405762\n",
      "Step: 35060, train/loss: 0.0\n",
      "Step: 35060, train/grad_norm: 0.0008770546410232782\n",
      "Step: 35060, train/learning_rate: 8.281770533358213e-06\n",
      "Step: 35060, train/epoch: 8.343646049499512\n",
      "Step: 35070, train/loss: 0.0\n",
      "Step: 35070, train/grad_norm: 0.0035191895440220833\n",
      "Step: 35070, train/learning_rate: 8.269871614174917e-06\n",
      "Step: 35070, train/epoch: 8.346025466918945\n",
      "Step: 35080, train/loss: 0.0\n",
      "Step: 35080, train/grad_norm: 0.002062685787677765\n",
      "Step: 35080, train/learning_rate: 8.257972694991622e-06\n",
      "Step: 35080, train/epoch: 8.348405838012695\n",
      "Step: 35090, train/loss: 0.0\n",
      "Step: 35090, train/grad_norm: 0.00020307107479311526\n",
      "Step: 35090, train/learning_rate: 8.246072866313625e-06\n",
      "Step: 35090, train/epoch: 8.350785255432129\n",
      "Step: 35100, train/loss: 0.0\n",
      "Step: 35100, train/grad_norm: 0.00032311133691109717\n",
      "Step: 35100, train/learning_rate: 8.23417394713033e-06\n",
      "Step: 35100, train/epoch: 8.353165626525879\n",
      "Step: 35110, train/loss: 0.0\n",
      "Step: 35110, train/grad_norm: 0.00026762846391648054\n",
      "Step: 35110, train/learning_rate: 8.222275027947035e-06\n",
      "Step: 35110, train/epoch: 8.355545043945312\n",
      "Step: 35120, train/loss: 0.0\n",
      "Step: 35120, train/grad_norm: 0.00030316083575598896\n",
      "Step: 35120, train/learning_rate: 8.21037610876374e-06\n",
      "Step: 35120, train/epoch: 8.357924461364746\n",
      "Step: 35130, train/loss: 0.0\n",
      "Step: 35130, train/grad_norm: 0.00014797951735090464\n",
      "Step: 35130, train/learning_rate: 8.198477189580444e-06\n",
      "Step: 35130, train/epoch: 8.360304832458496\n",
      "Step: 35140, train/loss: 0.0\n",
      "Step: 35140, train/grad_norm: 0.000303106993669644\n",
      "Step: 35140, train/learning_rate: 8.186578270397149e-06\n",
      "Step: 35140, train/epoch: 8.36268424987793\n",
      "Step: 35150, train/loss: 0.0\n",
      "Step: 35150, train/grad_norm: 0.000205486299819313\n",
      "Step: 35150, train/learning_rate: 8.174678441719152e-06\n",
      "Step: 35150, train/epoch: 8.36506462097168\n",
      "Step: 35160, train/loss: 0.0\n",
      "Step: 35160, train/grad_norm: 0.0003002643643412739\n",
      "Step: 35160, train/learning_rate: 8.162779522535857e-06\n",
      "Step: 35160, train/epoch: 8.367444038391113\n",
      "Step: 35170, train/loss: 0.0\n",
      "Step: 35170, train/grad_norm: 0.00012505968334153295\n",
      "Step: 35170, train/learning_rate: 8.150880603352562e-06\n",
      "Step: 35170, train/epoch: 8.369823455810547\n",
      "Step: 35180, train/loss: 0.08240000158548355\n",
      "Step: 35180, train/grad_norm: 0.0007374398992396891\n",
      "Step: 35180, train/learning_rate: 8.138981684169266e-06\n",
      "Step: 35180, train/epoch: 8.372203826904297\n",
      "Step: 35190, train/loss: 0.0\n",
      "Step: 35190, train/grad_norm: 0.0010742411250248551\n",
      "Step: 35190, train/learning_rate: 8.127082764985971e-06\n",
      "Step: 35190, train/epoch: 8.37458324432373\n",
      "Step: 35200, train/loss: 0.0\n",
      "Step: 35200, train/grad_norm: 0.0012901038862764835\n",
      "Step: 35200, train/learning_rate: 8.115182936307974e-06\n",
      "Step: 35200, train/epoch: 8.37696361541748\n",
      "Step: 35210, train/loss: 0.0\n",
      "Step: 35210, train/grad_norm: 0.003691543824970722\n",
      "Step: 35210, train/learning_rate: 8.103284017124679e-06\n",
      "Step: 35210, train/epoch: 8.379343032836914\n",
      "Step: 35220, train/loss: 0.0\n",
      "Step: 35220, train/grad_norm: 0.024485843256115913\n",
      "Step: 35220, train/learning_rate: 8.091385097941384e-06\n",
      "Step: 35220, train/epoch: 8.381723403930664\n",
      "Step: 35230, train/loss: 0.0\n",
      "Step: 35230, train/grad_norm: 0.0004137535870540887\n",
      "Step: 35230, train/learning_rate: 8.079486178758088e-06\n",
      "Step: 35230, train/epoch: 8.384102821350098\n",
      "Step: 35240, train/loss: 0.0\n",
      "Step: 35240, train/grad_norm: 0.0018757140496745706\n",
      "Step: 35240, train/learning_rate: 8.067587259574793e-06\n",
      "Step: 35240, train/epoch: 8.386482238769531\n",
      "Step: 35250, train/loss: 0.0\n",
      "Step: 35250, train/grad_norm: 0.0015540681779384613\n",
      "Step: 35250, train/learning_rate: 8.055687430896796e-06\n",
      "Step: 35250, train/epoch: 8.388862609863281\n",
      "Step: 35260, train/loss: 0.0\n",
      "Step: 35260, train/grad_norm: 0.010493440553545952\n",
      "Step: 35260, train/learning_rate: 8.043788511713501e-06\n",
      "Step: 35260, train/epoch: 8.391242027282715\n",
      "Step: 35270, train/loss: 0.0\n",
      "Step: 35270, train/grad_norm: 0.0003509337839204818\n",
      "Step: 35270, train/learning_rate: 8.031889592530206e-06\n",
      "Step: 35270, train/epoch: 8.393622398376465\n",
      "Step: 35280, train/loss: 0.0\n",
      "Step: 35280, train/grad_norm: 0.0005733895814046264\n",
      "Step: 35280, train/learning_rate: 8.01999067334691e-06\n",
      "Step: 35280, train/epoch: 8.396001815795898\n",
      "Step: 35290, train/loss: 0.0\n",
      "Step: 35290, train/grad_norm: 0.0009346671868115664\n",
      "Step: 35290, train/learning_rate: 8.008091754163615e-06\n",
      "Step: 35290, train/epoch: 8.398382186889648\n",
      "Step: 35300, train/loss: 0.0\n",
      "Step: 35300, train/grad_norm: 0.0026882467791438103\n",
      "Step: 35300, train/learning_rate: 7.996191925485618e-06\n",
      "Step: 35300, train/epoch: 8.400761604309082\n",
      "Step: 35310, train/loss: 0.0\n",
      "Step: 35310, train/grad_norm: 0.0023771142587065697\n",
      "Step: 35310, train/learning_rate: 7.984293006302323e-06\n",
      "Step: 35310, train/epoch: 8.403141021728516\n",
      "Step: 35320, train/loss: 0.0006000000284984708\n",
      "Step: 35320, train/grad_norm: 0.0001997153158299625\n",
      "Step: 35320, train/learning_rate: 7.972394087119028e-06\n",
      "Step: 35320, train/epoch: 8.405521392822266\n",
      "Step: 35330, train/loss: 0.0\n",
      "Step: 35330, train/grad_norm: 0.00011601171718211845\n",
      "Step: 35330, train/learning_rate: 7.960495167935733e-06\n",
      "Step: 35330, train/epoch: 8.4079008102417\n",
      "Step: 35340, train/loss: 0.0\n",
      "Step: 35340, train/grad_norm: 0.00016816095740068704\n",
      "Step: 35340, train/learning_rate: 7.948596248752438e-06\n",
      "Step: 35340, train/epoch: 8.41028118133545\n",
      "Step: 35350, train/loss: 0.0\n",
      "Step: 35350, train/grad_norm: 0.00015700161748100072\n",
      "Step: 35350, train/learning_rate: 7.93669642007444e-06\n",
      "Step: 35350, train/epoch: 8.412660598754883\n",
      "Step: 35360, train/loss: 0.0\n",
      "Step: 35360, train/grad_norm: 0.0006643205415457487\n",
      "Step: 35360, train/learning_rate: 7.924797500891145e-06\n",
      "Step: 35360, train/epoch: 8.415040016174316\n",
      "Step: 35370, train/loss: 0.0\n",
      "Step: 35370, train/grad_norm: 0.00011044454731745645\n",
      "Step: 35370, train/learning_rate: 7.91289858170785e-06\n",
      "Step: 35370, train/epoch: 8.417420387268066\n",
      "Step: 35380, train/loss: 0.0\n",
      "Step: 35380, train/grad_norm: 0.0003295156639069319\n",
      "Step: 35380, train/learning_rate: 7.900999662524555e-06\n",
      "Step: 35380, train/epoch: 8.4197998046875\n",
      "Step: 35390, train/loss: 0.0\n",
      "Step: 35390, train/grad_norm: 0.00019314682867843658\n",
      "Step: 35390, train/learning_rate: 7.88910074334126e-06\n",
      "Step: 35390, train/epoch: 8.42218017578125\n",
      "Step: 35400, train/loss: 0.0\n",
      "Step: 35400, train/grad_norm: 0.014349112287163734\n",
      "Step: 35400, train/learning_rate: 7.877200914663263e-06\n",
      "Step: 35400, train/epoch: 8.424559593200684\n",
      "Step: 35410, train/loss: 0.0\n",
      "Step: 35410, train/grad_norm: 0.0003034863038919866\n",
      "Step: 35410, train/learning_rate: 7.865301995479967e-06\n",
      "Step: 35410, train/epoch: 8.426939964294434\n",
      "Step: 35420, train/loss: 0.0\n",
      "Step: 35420, train/grad_norm: 0.0005748455296270549\n",
      "Step: 35420, train/learning_rate: 7.853403076296672e-06\n",
      "Step: 35420, train/epoch: 8.429319381713867\n",
      "Step: 35430, train/loss: 0.0\n",
      "Step: 35430, train/grad_norm: 8.966097084339708e-05\n",
      "Step: 35430, train/learning_rate: 7.841504157113377e-06\n",
      "Step: 35430, train/epoch: 8.4316987991333\n",
      "Step: 35440, train/loss: 0.0\n",
      "Step: 35440, train/grad_norm: 5.450996832223609e-05\n",
      "Step: 35440, train/learning_rate: 7.829605237930082e-06\n",
      "Step: 35440, train/epoch: 8.43407917022705\n",
      "Step: 35450, train/loss: 0.0\n",
      "Step: 35450, train/grad_norm: 8.685525972396135e-05\n",
      "Step: 35450, train/learning_rate: 7.817705409252085e-06\n",
      "Step: 35450, train/epoch: 8.436458587646484\n",
      "Step: 35460, train/loss: 0.0\n",
      "Step: 35460, train/grad_norm: 6.538978050230071e-05\n",
      "Step: 35460, train/learning_rate: 7.80580649006879e-06\n",
      "Step: 35460, train/epoch: 8.438838958740234\n",
      "Step: 35470, train/loss: 0.0\n",
      "Step: 35470, train/grad_norm: 3.1004507036414e-05\n",
      "Step: 35470, train/learning_rate: 7.793907570885494e-06\n",
      "Step: 35470, train/epoch: 8.441218376159668\n",
      "Step: 35480, train/loss: 0.0\n",
      "Step: 35480, train/grad_norm: 0.00014842893870081753\n",
      "Step: 35480, train/learning_rate: 7.782008651702199e-06\n",
      "Step: 35480, train/epoch: 8.443598747253418\n",
      "Step: 35490, train/loss: 0.0\n",
      "Step: 35490, train/grad_norm: 8.217457798309624e-05\n",
      "Step: 35490, train/learning_rate: 7.770109732518904e-06\n",
      "Step: 35490, train/epoch: 8.445978164672852\n",
      "Step: 35500, train/loss: 0.0\n",
      "Step: 35500, train/grad_norm: 0.00038472749292850494\n",
      "Step: 35500, train/learning_rate: 7.758210813335609e-06\n",
      "Step: 35500, train/epoch: 8.448357582092285\n",
      "Step: 35510, train/loss: 0.0\n",
      "Step: 35510, train/grad_norm: 0.0001988687872653827\n",
      "Step: 35510, train/learning_rate: 7.746310984657612e-06\n",
      "Step: 35510, train/epoch: 8.450737953186035\n",
      "Step: 35520, train/loss: 0.0\n",
      "Step: 35520, train/grad_norm: 0.0002556605904828757\n",
      "Step: 35520, train/learning_rate: 7.734412065474316e-06\n",
      "Step: 35520, train/epoch: 8.453117370605469\n",
      "Step: 35530, train/loss: 0.0\n",
      "Step: 35530, train/grad_norm: 0.00014389971329364926\n",
      "Step: 35530, train/learning_rate: 7.722513146291021e-06\n",
      "Step: 35530, train/epoch: 8.455497741699219\n",
      "Step: 35540, train/loss: 0.0\n",
      "Step: 35540, train/grad_norm: 0.00011531530617503449\n",
      "Step: 35540, train/learning_rate: 7.710614227107726e-06\n",
      "Step: 35540, train/epoch: 8.457877159118652\n",
      "Step: 35550, train/loss: 0.0\n",
      "Step: 35550, train/grad_norm: 4.474649904295802e-05\n",
      "Step: 35550, train/learning_rate: 7.69871530792443e-06\n",
      "Step: 35550, train/epoch: 8.460256576538086\n",
      "Step: 35560, train/loss: 0.0\n",
      "Step: 35560, train/grad_norm: 0.00011927226296393201\n",
      "Step: 35560, train/learning_rate: 7.686815479246434e-06\n",
      "Step: 35560, train/epoch: 8.462636947631836\n",
      "Step: 35570, train/loss: 0.0\n",
      "Step: 35570, train/grad_norm: 0.00017968119936995208\n",
      "Step: 35570, train/learning_rate: 7.674916560063139e-06\n",
      "Step: 35570, train/epoch: 8.46501636505127\n",
      "Step: 35580, train/loss: 0.0\n",
      "Step: 35580, train/grad_norm: 0.0023848998825997114\n",
      "Step: 35580, train/learning_rate: 7.663017640879843e-06\n",
      "Step: 35580, train/epoch: 8.46739673614502\n",
      "Step: 35590, train/loss: 0.0\n",
      "Step: 35590, train/grad_norm: 5.195472840568982e-05\n",
      "Step: 35590, train/learning_rate: 7.651118721696548e-06\n",
      "Step: 35590, train/epoch: 8.469776153564453\n",
      "Step: 35600, train/loss: 0.0\n",
      "Step: 35600, train/grad_norm: 9.888974454952404e-05\n",
      "Step: 35600, train/learning_rate: 7.639219802513253e-06\n",
      "Step: 35600, train/epoch: 8.472156524658203\n",
      "Step: 35610, train/loss: 0.0\n",
      "Step: 35610, train/grad_norm: 2.9585380616481416e-05\n",
      "Step: 35610, train/learning_rate: 7.627320428582607e-06\n",
      "Step: 35610, train/epoch: 8.474535942077637\n",
      "Step: 35620, train/loss: 0.0\n",
      "Step: 35620, train/grad_norm: 2.358263918722514e-05\n",
      "Step: 35620, train/learning_rate: 7.615421054651961e-06\n",
      "Step: 35620, train/epoch: 8.47691535949707\n",
      "Step: 35630, train/loss: 0.0\n",
      "Step: 35630, train/grad_norm: 7.917187758721411e-05\n",
      "Step: 35630, train/learning_rate: 7.6035221354686655e-06\n",
      "Step: 35630, train/epoch: 8.47929573059082\n",
      "Step: 35640, train/loss: 0.0\n",
      "Step: 35640, train/grad_norm: 1.1667853868857492e-05\n",
      "Step: 35640, train/learning_rate: 7.59162321628537e-06\n",
      "Step: 35640, train/epoch: 8.481675148010254\n",
      "Step: 35650, train/loss: 0.0\n",
      "Step: 35650, train/grad_norm: 0.00012084075569873676\n",
      "Step: 35650, train/learning_rate: 7.579723842354724e-06\n",
      "Step: 35650, train/epoch: 8.484055519104004\n",
      "Step: 35660, train/loss: 0.0\n",
      "Step: 35660, train/grad_norm: 7.307016494451091e-05\n",
      "Step: 35660, train/learning_rate: 7.567824923171429e-06\n",
      "Step: 35660, train/epoch: 8.486434936523438\n",
      "Step: 35670, train/loss: 0.0\n",
      "Step: 35670, train/grad_norm: 7.334112888202071e-05\n",
      "Step: 35670, train/learning_rate: 7.555925549240783e-06\n",
      "Step: 35670, train/epoch: 8.488815307617188\n",
      "Step: 35680, train/loss: 0.0\n",
      "Step: 35680, train/grad_norm: 9.434561798116192e-05\n",
      "Step: 35680, train/learning_rate: 7.544026630057488e-06\n",
      "Step: 35680, train/epoch: 8.491194725036621\n",
      "Step: 35690, train/loss: 0.0\n",
      "Step: 35690, train/grad_norm: 5.484150824486278e-05\n",
      "Step: 35690, train/learning_rate: 7.532127710874192e-06\n",
      "Step: 35690, train/epoch: 8.493574142456055\n",
      "Step: 35700, train/loss: 0.0\n",
      "Step: 35700, train/grad_norm: 0.0006484490586444736\n",
      "Step: 35700, train/learning_rate: 7.520228336943546e-06\n",
      "Step: 35700, train/epoch: 8.495954513549805\n",
      "Step: 35710, train/loss: 0.0\n",
      "Step: 35710, train/grad_norm: 6.330603355308995e-05\n",
      "Step: 35710, train/learning_rate: 7.508329417760251e-06\n",
      "Step: 35710, train/epoch: 8.498333930969238\n",
      "Step: 35720, train/loss: 0.0\n",
      "Step: 35720, train/grad_norm: 6.946548819541931e-05\n",
      "Step: 35720, train/learning_rate: 7.496430498576956e-06\n",
      "Step: 35720, train/epoch: 8.500714302062988\n",
      "Step: 35730, train/loss: 0.0\n",
      "Step: 35730, train/grad_norm: 8.362471271539107e-05\n",
      "Step: 35730, train/learning_rate: 7.48453112464631e-06\n",
      "Step: 35730, train/epoch: 8.503093719482422\n",
      "Step: 35740, train/loss: 0.0\n",
      "Step: 35740, train/grad_norm: 8.741688361624256e-05\n",
      "Step: 35740, train/learning_rate: 7.4726322054630145e-06\n",
      "Step: 35740, train/epoch: 8.505473136901855\n",
      "Step: 35750, train/loss: 0.0\n",
      "Step: 35750, train/grad_norm: 0.0008537686662748456\n",
      "Step: 35750, train/learning_rate: 7.4607328315323684e-06\n",
      "Step: 35750, train/epoch: 8.507853507995605\n",
      "Step: 35760, train/loss: 0.0\n",
      "Step: 35760, train/grad_norm: 0.00026991189224645495\n",
      "Step: 35760, train/learning_rate: 7.448833912349073e-06\n",
      "Step: 35760, train/epoch: 8.510232925415039\n",
      "Step: 35770, train/loss: 0.0\n",
      "Step: 35770, train/grad_norm: 7.086378172971308e-05\n",
      "Step: 35770, train/learning_rate: 7.436934993165778e-06\n",
      "Step: 35770, train/epoch: 8.512613296508789\n",
      "Step: 35780, train/loss: 0.0\n",
      "Step: 35780, train/grad_norm: 0.00011299097968731076\n",
      "Step: 35780, train/learning_rate: 7.425035619235132e-06\n",
      "Step: 35780, train/epoch: 8.514992713928223\n",
      "Step: 35790, train/loss: 0.0\n",
      "Step: 35790, train/grad_norm: 0.00025021738838404417\n",
      "Step: 35790, train/learning_rate: 7.413136700051837e-06\n",
      "Step: 35790, train/epoch: 8.517373085021973\n",
      "Step: 35800, train/loss: 0.0\n",
      "Step: 35800, train/grad_norm: 5.146010880707763e-05\n",
      "Step: 35800, train/learning_rate: 7.4012373261211906e-06\n",
      "Step: 35800, train/epoch: 8.519752502441406\n",
      "Step: 35810, train/loss: 0.0\n",
      "Step: 35810, train/grad_norm: 0.00019724534649867564\n",
      "Step: 35810, train/learning_rate: 7.389338406937895e-06\n",
      "Step: 35810, train/epoch: 8.52213191986084\n",
      "Step: 35820, train/loss: 0.0\n",
      "Step: 35820, train/grad_norm: 1.4175125215842854e-05\n",
      "Step: 35820, train/learning_rate: 7.3774394877546e-06\n",
      "Step: 35820, train/epoch: 8.52451229095459\n",
      "Step: 35830, train/loss: 0.0\n",
      "Step: 35830, train/grad_norm: 6.46843109279871e-05\n",
      "Step: 35830, train/learning_rate: 7.365540113823954e-06\n",
      "Step: 35830, train/epoch: 8.526891708374023\n",
      "Step: 35840, train/loss: 0.0\n",
      "Step: 35840, train/grad_norm: 0.00017041242972481996\n",
      "Step: 35840, train/learning_rate: 7.353641194640659e-06\n",
      "Step: 35840, train/epoch: 8.529272079467773\n",
      "Step: 35850, train/loss: 0.0\n",
      "Step: 35850, train/grad_norm: 4.391679976833984e-05\n",
      "Step: 35850, train/learning_rate: 7.341741820710013e-06\n",
      "Step: 35850, train/epoch: 8.531651496887207\n",
      "Step: 35860, train/loss: 0.0\n",
      "Step: 35860, train/grad_norm: 0.00027125937049277127\n",
      "Step: 35860, train/learning_rate: 7.3298429015267175e-06\n",
      "Step: 35860, train/epoch: 8.534031867980957\n",
      "Step: 35870, train/loss: 0.16089999675750732\n",
      "Step: 35870, train/grad_norm: 0.021380122750997543\n",
      "Step: 35870, train/learning_rate: 7.317943982343422e-06\n",
      "Step: 35870, train/epoch: 8.53641128540039\n",
      "Step: 35880, train/loss: 0.0\n",
      "Step: 35880, train/grad_norm: 0.00061561050824821\n",
      "Step: 35880, train/learning_rate: 7.306044608412776e-06\n",
      "Step: 35880, train/epoch: 8.538790702819824\n",
      "Step: 35890, train/loss: 0.0\n",
      "Step: 35890, train/grad_norm: 0.0007102296804077923\n",
      "Step: 35890, train/learning_rate: 7.294145689229481e-06\n",
      "Step: 35890, train/epoch: 8.541171073913574\n",
      "Step: 35900, train/loss: 0.0\n",
      "Step: 35900, train/grad_norm: 0.05358089134097099\n",
      "Step: 35900, train/learning_rate: 7.282246770046186e-06\n",
      "Step: 35900, train/epoch: 8.543550491333008\n",
      "Step: 35910, train/loss: 0.0\n",
      "Step: 35910, train/grad_norm: 0.01429690234363079\n",
      "Step: 35910, train/learning_rate: 7.27034739611554e-06\n",
      "Step: 35910, train/epoch: 8.545930862426758\n",
      "Step: 35920, train/loss: 0.0\n",
      "Step: 35920, train/grad_norm: 0.0005448502488434315\n",
      "Step: 35920, train/learning_rate: 7.258448476932244e-06\n",
      "Step: 35920, train/epoch: 8.548310279846191\n",
      "Step: 35930, train/loss: 0.0\n",
      "Step: 35930, train/grad_norm: 0.00032662611920386553\n",
      "Step: 35930, train/learning_rate: 7.246549103001598e-06\n",
      "Step: 35930, train/epoch: 8.550689697265625\n",
      "Step: 35940, train/loss: 0.0\n",
      "Step: 35940, train/grad_norm: 0.0024578573647886515\n",
      "Step: 35940, train/learning_rate: 7.234650183818303e-06\n",
      "Step: 35940, train/epoch: 8.553070068359375\n",
      "Step: 35950, train/loss: 0.0\n",
      "Step: 35950, train/grad_norm: 0.0057202838361263275\n",
      "Step: 35950, train/learning_rate: 7.222751264635008e-06\n",
      "Step: 35950, train/epoch: 8.555449485778809\n",
      "Step: 35960, train/loss: 0.00019999999494757503\n",
      "Step: 35960, train/grad_norm: 0.00011330747656757012\n",
      "Step: 35960, train/learning_rate: 7.210851890704362e-06\n",
      "Step: 35960, train/epoch: 8.557829856872559\n",
      "Step: 35970, train/loss: 0.0\n",
      "Step: 35970, train/grad_norm: 0.0002632616669870913\n",
      "Step: 35970, train/learning_rate: 7.1989529715210665e-06\n",
      "Step: 35970, train/epoch: 8.560209274291992\n",
      "Step: 35980, train/loss: 0.0\n",
      "Step: 35980, train/grad_norm: 9.376424713991582e-05\n",
      "Step: 35980, train/learning_rate: 7.18705359759042e-06\n",
      "Step: 35980, train/epoch: 8.562589645385742\n",
      "Step: 35990, train/loss: 0.0\n",
      "Step: 35990, train/grad_norm: 0.00020424346439540386\n",
      "Step: 35990, train/learning_rate: 7.175154678407125e-06\n",
      "Step: 35990, train/epoch: 8.564969062805176\n",
      "Step: 36000, train/loss: 0.0\n",
      "Step: 36000, train/grad_norm: 0.006095510441809893\n",
      "Step: 36000, train/learning_rate: 7.16325575922383e-06\n",
      "Step: 36000, train/epoch: 8.56734848022461\n",
      "Step: 36010, train/loss: 0.0\n",
      "Step: 36010, train/grad_norm: 0.005458059720695019\n",
      "Step: 36010, train/learning_rate: 7.151356385293184e-06\n",
      "Step: 36010, train/epoch: 8.56972885131836\n",
      "Step: 36020, train/loss: 0.0\n",
      "Step: 36020, train/grad_norm: 0.00015537685249000788\n",
      "Step: 36020, train/learning_rate: 7.139457466109889e-06\n",
      "Step: 36020, train/epoch: 8.572108268737793\n",
      "Step: 36030, train/loss: 0.0\n",
      "Step: 36030, train/grad_norm: 0.0004275177780073136\n",
      "Step: 36030, train/learning_rate: 7.1275580921792425e-06\n",
      "Step: 36030, train/epoch: 8.574488639831543\n",
      "Step: 36040, train/loss: 0.0\n",
      "Step: 36040, train/grad_norm: 0.00038762070471420884\n",
      "Step: 36040, train/learning_rate: 7.115659172995947e-06\n",
      "Step: 36040, train/epoch: 8.576868057250977\n",
      "Step: 36050, train/loss: 0.0\n",
      "Step: 36050, train/grad_norm: 0.0001432788121746853\n",
      "Step: 36050, train/learning_rate: 7.103760253812652e-06\n",
      "Step: 36050, train/epoch: 8.579248428344727\n",
      "Step: 36060, train/loss: 0.0\n",
      "Step: 36060, train/grad_norm: 0.00029632210498675704\n",
      "Step: 36060, train/learning_rate: 7.091860879882006e-06\n",
      "Step: 36060, train/epoch: 8.58162784576416\n",
      "Step: 36070, train/loss: 0.0\n",
      "Step: 36070, train/grad_norm: 0.00015923733008094132\n",
      "Step: 36070, train/learning_rate: 7.079961960698711e-06\n",
      "Step: 36070, train/epoch: 8.584007263183594\n",
      "Step: 36080, train/loss: 0.0\n",
      "Step: 36080, train/grad_norm: 0.0005509542534127831\n",
      "Step: 36080, train/learning_rate: 7.0680630415154155e-06\n",
      "Step: 36080, train/epoch: 8.586387634277344\n",
      "Step: 36090, train/loss: 0.0\n",
      "Step: 36090, train/grad_norm: 0.0004083807289134711\n",
      "Step: 36090, train/learning_rate: 7.0561636675847694e-06\n",
      "Step: 36090, train/epoch: 8.588767051696777\n",
      "Step: 36100, train/loss: 0.0\n",
      "Step: 36100, train/grad_norm: 0.00012349820462986827\n",
      "Step: 36100, train/learning_rate: 7.044264748401474e-06\n",
      "Step: 36100, train/epoch: 8.591147422790527\n",
      "Step: 36110, train/loss: 0.0\n",
      "Step: 36110, train/grad_norm: 8.60318323248066e-05\n",
      "Step: 36110, train/learning_rate: 7.032365374470828e-06\n",
      "Step: 36110, train/epoch: 8.593526840209961\n",
      "Step: 36120, train/loss: 0.0\n",
      "Step: 36120, train/grad_norm: 0.0006723058759234846\n",
      "Step: 36120, train/learning_rate: 7.020466455287533e-06\n",
      "Step: 36120, train/epoch: 8.595906257629395\n",
      "Step: 36130, train/loss: 0.0\n",
      "Step: 36130, train/grad_norm: 0.0005966444150544703\n",
      "Step: 36130, train/learning_rate: 7.008567536104238e-06\n",
      "Step: 36130, train/epoch: 8.598286628723145\n",
      "Step: 36140, train/loss: 0.0\n",
      "Step: 36140, train/grad_norm: 0.00027638766914606094\n",
      "Step: 36140, train/learning_rate: 6.9966681621735916e-06\n",
      "Step: 36140, train/epoch: 8.600666046142578\n",
      "Step: 36150, train/loss: 0.0\n",
      "Step: 36150, train/grad_norm: 0.00026820358471013606\n",
      "Step: 36150, train/learning_rate: 6.984769242990296e-06\n",
      "Step: 36150, train/epoch: 8.603046417236328\n",
      "Step: 36160, train/loss: 0.0\n",
      "Step: 36160, train/grad_norm: 8.50114956847392e-05\n",
      "Step: 36160, train/learning_rate: 6.97286986905965e-06\n",
      "Step: 36160, train/epoch: 8.605425834655762\n",
      "Step: 36170, train/loss: 0.0\n",
      "Step: 36170, train/grad_norm: 7.550976442871615e-05\n",
      "Step: 36170, train/learning_rate: 6.960970949876355e-06\n",
      "Step: 36170, train/epoch: 8.607806205749512\n",
      "Step: 36180, train/loss: 0.0\n",
      "Step: 36180, train/grad_norm: 4.2894011130556464e-05\n",
      "Step: 36180, train/learning_rate: 6.94907203069306e-06\n",
      "Step: 36180, train/epoch: 8.610185623168945\n",
      "Step: 36190, train/loss: 0.0\n",
      "Step: 36190, train/grad_norm: 0.0001756461279001087\n",
      "Step: 36190, train/learning_rate: 6.937172656762414e-06\n",
      "Step: 36190, train/epoch: 8.612565040588379\n",
      "Step: 36200, train/loss: 0.0\n",
      "Step: 36200, train/grad_norm: 0.00022737070685252547\n",
      "Step: 36200, train/learning_rate: 6.9252737375791185e-06\n",
      "Step: 36200, train/epoch: 8.614945411682129\n",
      "Step: 36210, train/loss: 0.0\n",
      "Step: 36210, train/grad_norm: 0.00018640786584001034\n",
      "Step: 36210, train/learning_rate: 6.913374363648472e-06\n",
      "Step: 36210, train/epoch: 8.617324829101562\n",
      "Step: 36220, train/loss: 0.0\n",
      "Step: 36220, train/grad_norm: 7.218019891297445e-05\n",
      "Step: 36220, train/learning_rate: 6.901475444465177e-06\n",
      "Step: 36220, train/epoch: 8.619705200195312\n",
      "Step: 36230, train/loss: 0.0\n",
      "Step: 36230, train/grad_norm: 9.693031461210921e-05\n",
      "Step: 36230, train/learning_rate: 6.889576525281882e-06\n",
      "Step: 36230, train/epoch: 8.622084617614746\n",
      "Step: 36240, train/loss: 0.0\n",
      "Step: 36240, train/grad_norm: 0.00016942614456638694\n",
      "Step: 36240, train/learning_rate: 6.877677151351236e-06\n",
      "Step: 36240, train/epoch: 8.624464988708496\n",
      "Step: 36250, train/loss: 0.0\n",
      "Step: 36250, train/grad_norm: 0.0001020498457364738\n",
      "Step: 36250, train/learning_rate: 6.865778232167941e-06\n",
      "Step: 36250, train/epoch: 8.62684440612793\n",
      "Step: 36260, train/loss: 0.0\n",
      "Step: 36260, train/grad_norm: 0.00014559482224285603\n",
      "Step: 36260, train/learning_rate: 6.853879312984645e-06\n",
      "Step: 36260, train/epoch: 8.629223823547363\n",
      "Step: 36270, train/loss: 0.0\n",
      "Step: 36270, train/grad_norm: 0.0002821747330017388\n",
      "Step: 36270, train/learning_rate: 6.841979939053999e-06\n",
      "Step: 36270, train/epoch: 8.631604194641113\n",
      "Step: 36280, train/loss: 0.0\n",
      "Step: 36280, train/grad_norm: 0.001746499678120017\n",
      "Step: 36280, train/learning_rate: 6.830081019870704e-06\n",
      "Step: 36280, train/epoch: 8.633983612060547\n",
      "Step: 36290, train/loss: 0.0\n",
      "Step: 36290, train/grad_norm: 0.00015633367002010345\n",
      "Step: 36290, train/learning_rate: 6.818181645940058e-06\n",
      "Step: 36290, train/epoch: 8.636363983154297\n",
      "Step: 36300, train/loss: 0.0\n",
      "Step: 36300, train/grad_norm: 0.00011399193317629397\n",
      "Step: 36300, train/learning_rate: 6.806282726756763e-06\n",
      "Step: 36300, train/epoch: 8.63874340057373\n",
      "Step: 36310, train/loss: 0.0\n",
      "Step: 36310, train/grad_norm: 0.0001161814943770878\n",
      "Step: 36310, train/learning_rate: 6.7943838075734675e-06\n",
      "Step: 36310, train/epoch: 8.641122817993164\n",
      "Step: 36320, train/loss: 0.0\n",
      "Step: 36320, train/grad_norm: 0.0008245447534136474\n",
      "Step: 36320, train/learning_rate: 6.782484433642821e-06\n",
      "Step: 36320, train/epoch: 8.643503189086914\n",
      "Step: 36330, train/loss: 0.0\n",
      "Step: 36330, train/grad_norm: 0.0001999762316700071\n",
      "Step: 36330, train/learning_rate: 6.770585514459526e-06\n",
      "Step: 36330, train/epoch: 8.645882606506348\n",
      "Step: 36340, train/loss: 0.0\n",
      "Step: 36340, train/grad_norm: 9.867901826510206e-05\n",
      "Step: 36340, train/learning_rate: 6.75868614052888e-06\n",
      "Step: 36340, train/epoch: 8.648262977600098\n",
      "Step: 36350, train/loss: 0.0\n",
      "Step: 36350, train/grad_norm: 0.00013651173503603786\n",
      "Step: 36350, train/learning_rate: 6.746787221345585e-06\n",
      "Step: 36350, train/epoch: 8.650642395019531\n",
      "Step: 36360, train/loss: 0.0\n",
      "Step: 36360, train/grad_norm: 7.479491614503786e-05\n",
      "Step: 36360, train/learning_rate: 6.73488830216229e-06\n",
      "Step: 36360, train/epoch: 8.653022766113281\n",
      "Step: 36370, train/loss: 0.0\n",
      "Step: 36370, train/grad_norm: 0.03019951842725277\n",
      "Step: 36370, train/learning_rate: 6.7229889282316435e-06\n",
      "Step: 36370, train/epoch: 8.655402183532715\n",
      "Step: 36380, train/loss: 0.0\n",
      "Step: 36380, train/grad_norm: 2.6746758521767333e-05\n",
      "Step: 36380, train/learning_rate: 6.711090009048348e-06\n",
      "Step: 36380, train/epoch: 8.657781600952148\n",
      "Step: 36390, train/loss: 0.0\n",
      "Step: 36390, train/grad_norm: 0.00017063808627426624\n",
      "Step: 36390, train/learning_rate: 6.699190635117702e-06\n",
      "Step: 36390, train/epoch: 8.660161972045898\n",
      "Step: 36400, train/loss: 0.0\n",
      "Step: 36400, train/grad_norm: 7.797605212545022e-05\n",
      "Step: 36400, train/learning_rate: 6.687291715934407e-06\n",
      "Step: 36400, train/epoch: 8.662541389465332\n",
      "Step: 36410, train/loss: 0.0\n",
      "Step: 36410, train/grad_norm: 0.0003219830396119505\n",
      "Step: 36410, train/learning_rate: 6.675392796751112e-06\n",
      "Step: 36410, train/epoch: 8.664921760559082\n",
      "Step: 36420, train/loss: 0.0\n",
      "Step: 36420, train/grad_norm: 9.48859888012521e-05\n",
      "Step: 36420, train/learning_rate: 6.663493422820466e-06\n",
      "Step: 36420, train/epoch: 8.667301177978516\n",
      "Step: 36430, train/loss: 0.0\n",
      "Step: 36430, train/grad_norm: 0.00022220071696210653\n",
      "Step: 36430, train/learning_rate: 6.65159450363717e-06\n",
      "Step: 36430, train/epoch: 8.669681549072266\n",
      "Step: 36440, train/loss: 0.0\n",
      "Step: 36440, train/grad_norm: 7.906745304353535e-05\n",
      "Step: 36440, train/learning_rate: 6.639695584453875e-06\n",
      "Step: 36440, train/epoch: 8.6720609664917\n",
      "Step: 36450, train/loss: 0.0\n",
      "Step: 36450, train/grad_norm: 2.7988704459858127e-05\n",
      "Step: 36450, train/learning_rate: 6.627796210523229e-06\n",
      "Step: 36450, train/epoch: 8.674440383911133\n",
      "Step: 36460, train/loss: 0.0\n",
      "Step: 36460, train/grad_norm: 7.836859003873542e-05\n",
      "Step: 36460, train/learning_rate: 6.615897291339934e-06\n",
      "Step: 36460, train/epoch: 8.676820755004883\n",
      "Step: 36470, train/loss: 0.0\n",
      "Step: 36470, train/grad_norm: 0.0001269542844966054\n",
      "Step: 36470, train/learning_rate: 6.603997917409288e-06\n",
      "Step: 36470, train/epoch: 8.679200172424316\n",
      "Step: 36480, train/loss: 0.0\n",
      "Step: 36480, train/grad_norm: 0.00010520649084355682\n",
      "Step: 36480, train/learning_rate: 6.5920989982259925e-06\n",
      "Step: 36480, train/epoch: 8.681580543518066\n",
      "Step: 36490, train/loss: 0.0\n",
      "Step: 36490, train/grad_norm: 0.00014701140753459185\n",
      "Step: 36490, train/learning_rate: 6.580200079042697e-06\n",
      "Step: 36490, train/epoch: 8.6839599609375\n",
      "Step: 36500, train/loss: 0.0\n",
      "Step: 36500, train/grad_norm: 8.288783283205703e-05\n",
      "Step: 36500, train/learning_rate: 6.568300705112051e-06\n",
      "Step: 36500, train/epoch: 8.686339378356934\n",
      "Step: 36510, train/loss: 0.0\n",
      "Step: 36510, train/grad_norm: 0.0023348734248429537\n",
      "Step: 36510, train/learning_rate: 6.556401785928756e-06\n",
      "Step: 36510, train/epoch: 8.688719749450684\n",
      "Step: 36520, train/loss: 0.0\n",
      "Step: 36520, train/grad_norm: 0.0001407168310834095\n",
      "Step: 36520, train/learning_rate: 6.54450241199811e-06\n",
      "Step: 36520, train/epoch: 8.691099166870117\n",
      "Step: 36530, train/loss: 0.0\n",
      "Step: 36530, train/grad_norm: 0.00015125142817851156\n",
      "Step: 36530, train/learning_rate: 6.532603492814815e-06\n",
      "Step: 36530, train/epoch: 8.693479537963867\n",
      "Step: 36540, train/loss: 0.0\n",
      "Step: 36540, train/grad_norm: 7.389779057120904e-05\n",
      "Step: 36540, train/learning_rate: 6.5207045736315195e-06\n",
      "Step: 36540, train/epoch: 8.6958589553833\n",
      "Step: 36550, train/loss: 0.0\n",
      "Step: 36550, train/grad_norm: 0.0001454749726690352\n",
      "Step: 36550, train/learning_rate: 6.508805199700873e-06\n",
      "Step: 36550, train/epoch: 8.69823932647705\n",
      "Step: 36560, train/loss: 0.0\n",
      "Step: 36560, train/grad_norm: 7.101781375240535e-05\n",
      "Step: 36560, train/learning_rate: 6.496906280517578e-06\n",
      "Step: 36560, train/epoch: 8.700618743896484\n",
      "Step: 36570, train/loss: 0.0\n",
      "Step: 36570, train/grad_norm: 6.783347635064274e-05\n",
      "Step: 36570, train/learning_rate: 6.485007361334283e-06\n",
      "Step: 36570, train/epoch: 8.702998161315918\n",
      "Step: 36580, train/loss: 0.0\n",
      "Step: 36580, train/grad_norm: 0.00010343715257477015\n",
      "Step: 36580, train/learning_rate: 6.473107987403637e-06\n",
      "Step: 36580, train/epoch: 8.705378532409668\n",
      "Step: 36590, train/loss: 0.0\n",
      "Step: 36590, train/grad_norm: 8.527119643986225e-05\n",
      "Step: 36590, train/learning_rate: 6.461209068220342e-06\n",
      "Step: 36590, train/epoch: 8.707757949829102\n",
      "Step: 36600, train/loss: 0.0\n",
      "Step: 36600, train/grad_norm: 3.213393938494846e-05\n",
      "Step: 36600, train/learning_rate: 6.4493096942896955e-06\n",
      "Step: 36600, train/epoch: 8.710138320922852\n",
      "Step: 36610, train/loss: 0.0\n",
      "Step: 36610, train/grad_norm: 7.53693821025081e-05\n",
      "Step: 36610, train/learning_rate: 6.4374107751064e-06\n",
      "Step: 36610, train/epoch: 8.712517738342285\n",
      "Step: 36620, train/loss: 0.0\n",
      "Step: 36620, train/grad_norm: 4.176527727395296e-05\n",
      "Step: 36620, train/learning_rate: 6.425511855923105e-06\n",
      "Step: 36620, train/epoch: 8.714898109436035\n",
      "Step: 36630, train/loss: 0.0\n",
      "Step: 36630, train/grad_norm: 6.738639058312401e-05\n",
      "Step: 36630, train/learning_rate: 6.413612481992459e-06\n",
      "Step: 36630, train/epoch: 8.717277526855469\n",
      "Step: 36640, train/loss: 0.0\n",
      "Step: 36640, train/grad_norm: 5.880377648281865e-05\n",
      "Step: 36640, train/learning_rate: 6.401713562809164e-06\n",
      "Step: 36640, train/epoch: 8.719656944274902\n",
      "Step: 36650, train/loss: 0.0\n",
      "Step: 36650, train/grad_norm: 9.082514588953927e-05\n",
      "Step: 36650, train/learning_rate: 6.389814188878518e-06\n",
      "Step: 36650, train/epoch: 8.722037315368652\n",
      "Step: 36660, train/loss: 0.0\n",
      "Step: 36660, train/grad_norm: 0.0001589450694154948\n",
      "Step: 36660, train/learning_rate: 6.377915269695222e-06\n",
      "Step: 36660, train/epoch: 8.724416732788086\n",
      "Step: 36670, train/loss: 0.0\n",
      "Step: 36670, train/grad_norm: 1.360892019874882e-05\n",
      "Step: 36670, train/learning_rate: 6.366016350511927e-06\n",
      "Step: 36670, train/epoch: 8.726797103881836\n",
      "Step: 36680, train/loss: 0.0\n",
      "Step: 36680, train/grad_norm: 0.0001389076205668971\n",
      "Step: 36680, train/learning_rate: 6.354116976581281e-06\n",
      "Step: 36680, train/epoch: 8.72917652130127\n",
      "Step: 36690, train/loss: 0.0\n",
      "Step: 36690, train/grad_norm: 7.495869067497551e-05\n",
      "Step: 36690, train/learning_rate: 6.342218057397986e-06\n",
      "Step: 36690, train/epoch: 8.731555938720703\n",
      "Step: 36700, train/loss: 0.0\n",
      "Step: 36700, train/grad_norm: 0.0013576053315773606\n",
      "Step: 36700, train/learning_rate: 6.33031868346734e-06\n",
      "Step: 36700, train/epoch: 8.733936309814453\n",
      "Step: 36710, train/loss: 0.0\n",
      "Step: 36710, train/grad_norm: 2.1247293261694722e-05\n",
      "Step: 36710, train/learning_rate: 6.3184197642840445e-06\n",
      "Step: 36710, train/epoch: 8.736315727233887\n",
      "Step: 36720, train/loss: 0.0\n",
      "Step: 36720, train/grad_norm: 0.00017694364942144603\n",
      "Step: 36720, train/learning_rate: 6.306520845100749e-06\n",
      "Step: 36720, train/epoch: 8.738696098327637\n",
      "Step: 36730, train/loss: 0.0\n",
      "Step: 36730, train/grad_norm: 8.233672997448593e-05\n",
      "Step: 36730, train/learning_rate: 6.294621471170103e-06\n",
      "Step: 36730, train/epoch: 8.74107551574707\n",
      "Step: 36740, train/loss: 0.0\n",
      "Step: 36740, train/grad_norm: 5.288119791657664e-05\n",
      "Step: 36740, train/learning_rate: 6.282722551986808e-06\n",
      "Step: 36740, train/epoch: 8.74345588684082\n",
      "Step: 36750, train/loss: 0.0\n",
      "Step: 36750, train/grad_norm: 7.164196631492814e-06\n",
      "Step: 36750, train/learning_rate: 6.270823632803513e-06\n",
      "Step: 36750, train/epoch: 8.745835304260254\n",
      "Step: 36760, train/loss: 0.0\n",
      "Step: 36760, train/grad_norm: 0.0012012558290734887\n",
      "Step: 36760, train/learning_rate: 6.258924258872867e-06\n",
      "Step: 36760, train/epoch: 8.748214721679688\n",
      "Step: 36770, train/loss: 0.0\n",
      "Step: 36770, train/grad_norm: 0.00011796881881309673\n",
      "Step: 36770, train/learning_rate: 6.247025339689571e-06\n",
      "Step: 36770, train/epoch: 8.750595092773438\n",
      "Step: 36780, train/loss: 0.0\n",
      "Step: 36780, train/grad_norm: 6.685490370728076e-05\n",
      "Step: 36780, train/learning_rate: 6.235125965758925e-06\n",
      "Step: 36780, train/epoch: 8.752974510192871\n",
      "Step: 36790, train/loss: 0.0\n",
      "Step: 36790, train/grad_norm: 7.77413442847319e-05\n",
      "Step: 36790, train/learning_rate: 6.22322704657563e-06\n",
      "Step: 36790, train/epoch: 8.755354881286621\n",
      "Step: 36800, train/loss: 0.0\n",
      "Step: 36800, train/grad_norm: 2.7108739232062362e-05\n",
      "Step: 36800, train/learning_rate: 6.211328127392335e-06\n",
      "Step: 36800, train/epoch: 8.757734298706055\n",
      "Step: 36810, train/loss: 0.0\n",
      "Step: 36810, train/grad_norm: 4.877452738583088e-05\n",
      "Step: 36810, train/learning_rate: 6.199428753461689e-06\n",
      "Step: 36810, train/epoch: 8.760114669799805\n",
      "Step: 36820, train/loss: 0.0\n",
      "Step: 36820, train/grad_norm: 5.500180122908205e-05\n",
      "Step: 36820, train/learning_rate: 6.1875298342783935e-06\n",
      "Step: 36820, train/epoch: 8.762494087219238\n",
      "Step: 36830, train/loss: 0.0\n",
      "Step: 36830, train/grad_norm: 5.6344972108490765e-05\n",
      "Step: 36830, train/learning_rate: 6.1756304603477474e-06\n",
      "Step: 36830, train/epoch: 8.764873504638672\n",
      "Step: 36840, train/loss: 0.0\n",
      "Step: 36840, train/grad_norm: 0.00014327597455121577\n",
      "Step: 36840, train/learning_rate: 6.163731541164452e-06\n",
      "Step: 36840, train/epoch: 8.767253875732422\n",
      "Step: 36850, train/loss: 0.0\n",
      "Step: 36850, train/grad_norm: 5.113512816024013e-05\n",
      "Step: 36850, train/learning_rate: 6.151832621981157e-06\n",
      "Step: 36850, train/epoch: 8.769633293151855\n",
      "Step: 36860, train/loss: 0.0\n",
      "Step: 36860, train/grad_norm: 2.9978216844028793e-05\n",
      "Step: 36860, train/learning_rate: 6.139933248050511e-06\n",
      "Step: 36860, train/epoch: 8.772013664245605\n",
      "Step: 36870, train/loss: 0.0\n",
      "Step: 36870, train/grad_norm: 0.00014384911628440022\n",
      "Step: 36870, train/learning_rate: 6.128034328867216e-06\n",
      "Step: 36870, train/epoch: 8.774393081665039\n",
      "Step: 36880, train/loss: 0.0\n",
      "Step: 36880, train/grad_norm: 0.0021427220199257135\n",
      "Step: 36880, train/learning_rate: 6.1161349549365696e-06\n",
      "Step: 36880, train/epoch: 8.776772499084473\n",
      "Step: 36890, train/loss: 0.0\n",
      "Step: 36890, train/grad_norm: 3.679850124171935e-05\n",
      "Step: 36890, train/learning_rate: 6.104236035753274e-06\n",
      "Step: 36890, train/epoch: 8.779152870178223\n",
      "Step: 36900, train/loss: 0.0\n",
      "Step: 36900, train/grad_norm: 4.275464016245678e-05\n",
      "Step: 36900, train/learning_rate: 6.092337116569979e-06\n",
      "Step: 36900, train/epoch: 8.781532287597656\n",
      "Step: 36910, train/loss: 0.0\n",
      "Step: 36910, train/grad_norm: 0.00010698790720198303\n",
      "Step: 36910, train/learning_rate: 6.080437742639333e-06\n",
      "Step: 36910, train/epoch: 8.783912658691406\n",
      "Step: 36920, train/loss: 0.0\n",
      "Step: 36920, train/grad_norm: 7.072695734677836e-05\n",
      "Step: 36920, train/learning_rate: 6.068538823456038e-06\n",
      "Step: 36920, train/epoch: 8.78629207611084\n",
      "Step: 36930, train/loss: 0.0\n",
      "Step: 36930, train/grad_norm: 0.00011413582251407206\n",
      "Step: 36930, train/learning_rate: 6.0566399042727426e-06\n",
      "Step: 36930, train/epoch: 8.78867244720459\n",
      "Step: 36940, train/loss: 0.0\n",
      "Step: 36940, train/grad_norm: 4.37710878031794e-05\n",
      "Step: 36940, train/learning_rate: 6.0447405303420965e-06\n",
      "Step: 36940, train/epoch: 8.791051864624023\n",
      "Step: 36950, train/loss: 0.0\n",
      "Step: 36950, train/grad_norm: 3.288263178546913e-05\n",
      "Step: 36950, train/learning_rate: 6.032841611158801e-06\n",
      "Step: 36950, train/epoch: 8.793431282043457\n",
      "Step: 36960, train/loss: 0.0\n",
      "Step: 36960, train/grad_norm: 7.429888501064852e-05\n",
      "Step: 36960, train/learning_rate: 6.020942237228155e-06\n",
      "Step: 36960, train/epoch: 8.795811653137207\n",
      "Step: 36970, train/loss: 0.0\n",
      "Step: 36970, train/grad_norm: 0.025704581290483475\n",
      "Step: 36970, train/learning_rate: 6.00904331804486e-06\n",
      "Step: 36970, train/epoch: 8.79819107055664\n",
      "Step: 36980, train/loss: 0.0\n",
      "Step: 36980, train/grad_norm: 9.49770983424969e-05\n",
      "Step: 36980, train/learning_rate: 5.997144398861565e-06\n",
      "Step: 36980, train/epoch: 8.80057144165039\n",
      "Step: 36990, train/loss: 0.052299998700618744\n",
      "Step: 36990, train/grad_norm: 6.771938205929473e-05\n",
      "Step: 36990, train/learning_rate: 5.985245024930919e-06\n",
      "Step: 36990, train/epoch: 8.802950859069824\n",
      "Step: 37000, train/loss: 0.0\n",
      "Step: 37000, train/grad_norm: 0.0008839956717565656\n",
      "Step: 37000, train/learning_rate: 5.973346105747623e-06\n",
      "Step: 37000, train/epoch: 8.805331230163574\n",
      "Step: 37010, train/loss: 0.0\n",
      "Step: 37010, train/grad_norm: 0.0010411980329081416\n",
      "Step: 37010, train/learning_rate: 5.961446731816977e-06\n",
      "Step: 37010, train/epoch: 8.807710647583008\n",
      "Step: 37020, train/loss: 0.0\n",
      "Step: 37020, train/grad_norm: 0.0023561089765280485\n",
      "Step: 37020, train/learning_rate: 5.949547812633682e-06\n",
      "Step: 37020, train/epoch: 8.810090065002441\n",
      "Step: 37030, train/loss: 0.0\n",
      "Step: 37030, train/grad_norm: 0.0003838886914309114\n",
      "Step: 37030, train/learning_rate: 5.937648893450387e-06\n",
      "Step: 37030, train/epoch: 8.812470436096191\n",
      "Step: 37040, train/loss: 0.0\n",
      "Step: 37040, train/grad_norm: 0.00024417508393526077\n",
      "Step: 37040, train/learning_rate: 5.925749519519741e-06\n",
      "Step: 37040, train/epoch: 8.814849853515625\n",
      "Step: 37050, train/loss: 0.0\n",
      "Step: 37050, train/grad_norm: 0.00013565810513682663\n",
      "Step: 37050, train/learning_rate: 5.9138506003364455e-06\n",
      "Step: 37050, train/epoch: 8.817230224609375\n",
      "Step: 37060, train/loss: 0.0\n",
      "Step: 37060, train/grad_norm: 0.0012105151545256376\n",
      "Step: 37060, train/learning_rate: 5.901951226405799e-06\n",
      "Step: 37060, train/epoch: 8.819609642028809\n",
      "Step: 37070, train/loss: 0.0\n",
      "Step: 37070, train/grad_norm: 0.00034159497590735555\n",
      "Step: 37070, train/learning_rate: 5.890052307222504e-06\n",
      "Step: 37070, train/epoch: 8.821989059448242\n",
      "Step: 37080, train/loss: 0.0\n",
      "Step: 37080, train/grad_norm: 0.00045004882849752903\n",
      "Step: 37080, train/learning_rate: 5.878153388039209e-06\n",
      "Step: 37080, train/epoch: 8.824369430541992\n",
      "Step: 37090, train/loss: 0.0\n",
      "Step: 37090, train/grad_norm: 0.0006960175232961774\n",
      "Step: 37090, train/learning_rate: 5.866254014108563e-06\n",
      "Step: 37090, train/epoch: 8.826748847961426\n",
      "Step: 37100, train/loss: 0.0\n",
      "Step: 37100, train/grad_norm: 0.00020337154273875058\n",
      "Step: 37100, train/learning_rate: 5.854355094925268e-06\n",
      "Step: 37100, train/epoch: 8.829129219055176\n",
      "Step: 37110, train/loss: 0.0\n",
      "Step: 37110, train/grad_norm: 0.0001666086754994467\n",
      "Step: 37110, train/learning_rate: 5.842456175741972e-06\n",
      "Step: 37110, train/epoch: 8.83150863647461\n",
      "Step: 37120, train/loss: 0.0\n",
      "Step: 37120, train/grad_norm: 0.00015137658920139074\n",
      "Step: 37120, train/learning_rate: 5.830556801811326e-06\n",
      "Step: 37120, train/epoch: 8.83388900756836\n",
      "Step: 37130, train/loss: 0.0\n",
      "Step: 37130, train/grad_norm: 0.0002740366035141051\n",
      "Step: 37130, train/learning_rate: 5.818657882628031e-06\n",
      "Step: 37130, train/epoch: 8.836268424987793\n",
      "Step: 37140, train/loss: 0.0\n",
      "Step: 37140, train/grad_norm: 0.0007171487086452544\n",
      "Step: 37140, train/learning_rate: 5.806758508697385e-06\n",
      "Step: 37140, train/epoch: 8.838647842407227\n",
      "Step: 37150, train/loss: 0.0\n",
      "Step: 37150, train/grad_norm: 0.0001831254194257781\n",
      "Step: 37150, train/learning_rate: 5.79485958951409e-06\n",
      "Step: 37150, train/epoch: 8.841028213500977\n",
      "Step: 37160, train/loss: 0.0\n",
      "Step: 37160, train/grad_norm: 0.0021761751268059015\n",
      "Step: 37160, train/learning_rate: 5.7829606703307945e-06\n",
      "Step: 37160, train/epoch: 8.84340763092041\n",
      "Step: 37170, train/loss: 0.0\n",
      "Step: 37170, train/grad_norm: 0.00019942346261814237\n",
      "Step: 37170, train/learning_rate: 5.771061296400148e-06\n",
      "Step: 37170, train/epoch: 8.84578800201416\n",
      "Step: 37180, train/loss: 0.0\n",
      "Step: 37180, train/grad_norm: 0.00020556439994834363\n",
      "Step: 37180, train/learning_rate: 5.759162377216853e-06\n",
      "Step: 37180, train/epoch: 8.848167419433594\n",
      "Step: 37190, train/loss: 0.0\n",
      "Step: 37190, train/grad_norm: 0.0006696468917652965\n",
      "Step: 37190, train/learning_rate: 5.747263003286207e-06\n",
      "Step: 37190, train/epoch: 8.850547790527344\n",
      "Step: 37200, train/loss: 9.999999747378752e-05\n",
      "Step: 37200, train/grad_norm: 0.5823274254798889\n",
      "Step: 37200, train/learning_rate: 5.735364084102912e-06\n",
      "Step: 37200, train/epoch: 8.852927207946777\n",
      "Step: 37210, train/loss: 0.0\n",
      "Step: 37210, train/grad_norm: 6.02656182309147e-05\n",
      "Step: 37210, train/learning_rate: 5.723465164919617e-06\n",
      "Step: 37210, train/epoch: 8.855306625366211\n",
      "Step: 37220, train/loss: 0.0\n",
      "Step: 37220, train/grad_norm: 7.470297714462504e-05\n",
      "Step: 37220, train/learning_rate: 5.7115657909889705e-06\n",
      "Step: 37220, train/epoch: 8.857686996459961\n",
      "Step: 37230, train/loss: 0.0\n",
      "Step: 37230, train/grad_norm: 7.020658813416958e-05\n",
      "Step: 37230, train/learning_rate: 5.699666871805675e-06\n",
      "Step: 37230, train/epoch: 8.860066413879395\n",
      "Step: 37240, train/loss: 0.0\n",
      "Step: 37240, train/grad_norm: 5.6799563026288524e-05\n",
      "Step: 37240, train/learning_rate: 5.68776795262238e-06\n",
      "Step: 37240, train/epoch: 8.862446784973145\n",
      "Step: 37250, train/loss: 0.0\n",
      "Step: 37250, train/grad_norm: 4.8221521865343675e-05\n",
      "Step: 37250, train/learning_rate: 5.675868578691734e-06\n",
      "Step: 37250, train/epoch: 8.864826202392578\n",
      "Step: 37260, train/loss: 0.0\n",
      "Step: 37260, train/grad_norm: 0.00018673522572498769\n",
      "Step: 37260, train/learning_rate: 5.663969659508439e-06\n",
      "Step: 37260, train/epoch: 8.867205619812012\n",
      "Step: 37270, train/loss: 0.0\n",
      "Step: 37270, train/grad_norm: 6.72274109092541e-05\n",
      "Step: 37270, train/learning_rate: 5.652070285577793e-06\n",
      "Step: 37270, train/epoch: 8.869585990905762\n",
      "Step: 37280, train/loss: 0.0\n",
      "Step: 37280, train/grad_norm: 0.00010282370203640312\n",
      "Step: 37280, train/learning_rate: 5.6401713663944975e-06\n",
      "Step: 37280, train/epoch: 8.871965408325195\n",
      "Step: 37290, train/loss: 0.0\n",
      "Step: 37290, train/grad_norm: 3.7205249100225046e-05\n",
      "Step: 37290, train/learning_rate: 5.628272447211202e-06\n",
      "Step: 37290, train/epoch: 8.874345779418945\n",
      "Step: 37300, train/loss: 0.0\n",
      "Step: 37300, train/grad_norm: 0.00011998958507319912\n",
      "Step: 37300, train/learning_rate: 5.616373073280556e-06\n",
      "Step: 37300, train/epoch: 8.876725196838379\n",
      "Step: 37310, train/loss: 0.0\n",
      "Step: 37310, train/grad_norm: 2.406473140581511e-05\n",
      "Step: 37310, train/learning_rate: 5.604474154097261e-06\n",
      "Step: 37310, train/epoch: 8.879105567932129\n",
      "Step: 37320, train/loss: 0.0\n",
      "Step: 37320, train/grad_norm: 5.0658851250773296e-05\n",
      "Step: 37320, train/learning_rate: 5.592574780166615e-06\n",
      "Step: 37320, train/epoch: 8.881484985351562\n",
      "Step: 37330, train/loss: 0.0\n",
      "Step: 37330, train/grad_norm: 6.791565101593733e-05\n",
      "Step: 37330, train/learning_rate: 5.58067586098332e-06\n",
      "Step: 37330, train/epoch: 8.883864402770996\n",
      "Step: 37340, train/loss: 0.0\n",
      "Step: 37340, train/grad_norm: 8.987267210613936e-05\n",
      "Step: 37340, train/learning_rate: 5.568776941800024e-06\n",
      "Step: 37340, train/epoch: 8.886244773864746\n",
      "Step: 37350, train/loss: 0.0\n",
      "Step: 37350, train/grad_norm: 0.000484471587697044\n",
      "Step: 37350, train/learning_rate: 5.556877567869378e-06\n",
      "Step: 37350, train/epoch: 8.88862419128418\n",
      "Step: 37360, train/loss: 0.0\n",
      "Step: 37360, train/grad_norm: 0.0001008615072350949\n",
      "Step: 37360, train/learning_rate: 5.544978648686083e-06\n",
      "Step: 37360, train/epoch: 8.89100456237793\n",
      "Step: 37370, train/loss: 0.0\n",
      "Step: 37370, train/grad_norm: 1.9210297978133895e-05\n",
      "Step: 37370, train/learning_rate: 5.533079274755437e-06\n",
      "Step: 37370, train/epoch: 8.893383979797363\n",
      "Step: 37380, train/loss: 0.0\n",
      "Step: 37380, train/grad_norm: 2.2423313566832803e-05\n",
      "Step: 37380, train/learning_rate: 5.521180355572142e-06\n",
      "Step: 37380, train/epoch: 8.895764350891113\n",
      "Step: 37390, train/loss: 0.0\n",
      "Step: 37390, train/grad_norm: 0.0005949020851403475\n",
      "Step: 37390, train/learning_rate: 5.5092814363888465e-06\n",
      "Step: 37390, train/epoch: 8.898143768310547\n",
      "Step: 37400, train/loss: 0.0\n",
      "Step: 37400, train/grad_norm: 1.40449519676622e-05\n",
      "Step: 37400, train/learning_rate: 5.4973820624582e-06\n",
      "Step: 37400, train/epoch: 8.90052318572998\n",
      "Step: 37410, train/loss: 0.0\n",
      "Step: 37410, train/grad_norm: 0.00010626709263306111\n",
      "Step: 37410, train/learning_rate: 5.485483143274905e-06\n",
      "Step: 37410, train/epoch: 8.90290355682373\n",
      "Step: 37420, train/loss: 0.0\n",
      "Step: 37420, train/grad_norm: 1.3746512195211835e-05\n",
      "Step: 37420, train/learning_rate: 5.47358422409161e-06\n",
      "Step: 37420, train/epoch: 8.905282974243164\n",
      "Step: 37430, train/loss: 0.0\n",
      "Step: 37430, train/grad_norm: 4.8378733481513336e-05\n",
      "Step: 37430, train/learning_rate: 5.461684850160964e-06\n",
      "Step: 37430, train/epoch: 8.907663345336914\n",
      "Step: 37440, train/loss: 0.0\n",
      "Step: 37440, train/grad_norm: 1.0326274605176877e-05\n",
      "Step: 37440, train/learning_rate: 5.449785930977669e-06\n",
      "Step: 37440, train/epoch: 8.910042762756348\n",
      "Step: 37450, train/loss: 0.0\n",
      "Step: 37450, train/grad_norm: 0.00018063289462588727\n",
      "Step: 37450, train/learning_rate: 5.4378865570470225e-06\n",
      "Step: 37450, train/epoch: 8.912422180175781\n",
      "Step: 37460, train/loss: 0.0\n",
      "Step: 37460, train/grad_norm: 0.0005768327973783016\n",
      "Step: 37460, train/learning_rate: 5.425987637863727e-06\n",
      "Step: 37460, train/epoch: 8.914802551269531\n",
      "Step: 37470, train/loss: 0.0\n",
      "Step: 37470, train/grad_norm: 1.2054282706230879e-05\n",
      "Step: 37470, train/learning_rate: 5.414088718680432e-06\n",
      "Step: 37470, train/epoch: 8.917181968688965\n",
      "Step: 37480, train/loss: 0.0\n",
      "Step: 37480, train/grad_norm: 1.588128725416027e-05\n",
      "Step: 37480, train/learning_rate: 5.402189344749786e-06\n",
      "Step: 37480, train/epoch: 8.919562339782715\n",
      "Step: 37490, train/loss: 0.0\n",
      "Step: 37490, train/grad_norm: 3.1008825317258015e-05\n",
      "Step: 37490, train/learning_rate: 5.390290425566491e-06\n",
      "Step: 37490, train/epoch: 8.921941757202148\n",
      "Step: 37500, train/loss: 0.0\n",
      "Step: 37500, train/grad_norm: 4.9734811909729615e-05\n",
      "Step: 37500, train/learning_rate: 5.378391051635845e-06\n",
      "Step: 37500, train/epoch: 8.924322128295898\n",
      "Step: 37510, train/loss: 0.0\n",
      "Step: 37510, train/grad_norm: 4.3499247112777084e-05\n",
      "Step: 37510, train/learning_rate: 5.366492132452549e-06\n",
      "Step: 37510, train/epoch: 8.926701545715332\n",
      "Step: 37520, train/loss: 0.0\n",
      "Step: 37520, train/grad_norm: 6.378346006385982e-05\n",
      "Step: 37520, train/learning_rate: 5.354593213269254e-06\n",
      "Step: 37520, train/epoch: 8.929080963134766\n",
      "Step: 37530, train/loss: 0.0\n",
      "Step: 37530, train/grad_norm: 0.00021080659644212574\n",
      "Step: 37530, train/learning_rate: 5.342693839338608e-06\n",
      "Step: 37530, train/epoch: 8.931461334228516\n",
      "Step: 37540, train/loss: 0.0\n",
      "Step: 37540, train/grad_norm: 1.3980399671709165e-05\n",
      "Step: 37540, train/learning_rate: 5.330794920155313e-06\n",
      "Step: 37540, train/epoch: 8.93384075164795\n",
      "Step: 37550, train/loss: 0.0\n",
      "Step: 37550, train/grad_norm: 0.00011610700312303379\n",
      "Step: 37550, train/learning_rate: 5.318895546224667e-06\n",
      "Step: 37550, train/epoch: 8.9362211227417\n",
      "Step: 37560, train/loss: 0.0\n",
      "Step: 37560, train/grad_norm: 2.7983012842014432e-05\n",
      "Step: 37560, train/learning_rate: 5.3069966270413715e-06\n",
      "Step: 37560, train/epoch: 8.938600540161133\n",
      "Step: 37570, train/loss: 0.0\n",
      "Step: 37570, train/grad_norm: 1.5195812011370435e-05\n",
      "Step: 37570, train/learning_rate: 5.295097707858076e-06\n",
      "Step: 37570, train/epoch: 8.940980911254883\n",
      "Step: 37580, train/loss: 0.0\n",
      "Step: 37580, train/grad_norm: 0.0002982663281727582\n",
      "Step: 37580, train/learning_rate: 5.28319833392743e-06\n",
      "Step: 37580, train/epoch: 8.943360328674316\n",
      "Step: 37590, train/loss: 0.0\n",
      "Step: 37590, train/grad_norm: 2.512041828595102e-05\n",
      "Step: 37590, train/learning_rate: 5.271299414744135e-06\n",
      "Step: 37590, train/epoch: 8.94573974609375\n",
      "Step: 37600, train/loss: 0.0\n",
      "Step: 37600, train/grad_norm: 3.742118497029878e-05\n",
      "Step: 37600, train/learning_rate: 5.25940049556084e-06\n",
      "Step: 37600, train/epoch: 8.9481201171875\n",
      "Step: 37610, train/loss: 0.0\n",
      "Step: 37610, train/grad_norm: 3.058631409658119e-05\n",
      "Step: 37610, train/learning_rate: 5.247501121630194e-06\n",
      "Step: 37610, train/epoch: 8.950499534606934\n",
      "Step: 37620, train/loss: 0.0\n",
      "Step: 37620, train/grad_norm: 0.00013906735694035888\n",
      "Step: 37620, train/learning_rate: 5.2356022024468984e-06\n",
      "Step: 37620, train/epoch: 8.952879905700684\n",
      "Step: 37630, train/loss: 0.0\n",
      "Step: 37630, train/grad_norm: 2.519572080927901e-05\n",
      "Step: 37630, train/learning_rate: 5.223702828516252e-06\n",
      "Step: 37630, train/epoch: 8.955259323120117\n",
      "Step: 37640, train/loss: 0.0\n",
      "Step: 37640, train/grad_norm: 7.588327207486145e-06\n",
      "Step: 37640, train/learning_rate: 5.211803909332957e-06\n",
      "Step: 37640, train/epoch: 8.957639694213867\n",
      "Step: 37650, train/loss: 0.0\n",
      "Step: 37650, train/grad_norm: 0.013920813798904419\n",
      "Step: 37650, train/learning_rate: 5.199904990149662e-06\n",
      "Step: 37650, train/epoch: 8.9600191116333\n",
      "Step: 37660, train/loss: 0.0\n",
      "Step: 37660, train/grad_norm: 0.0001413837308064103\n",
      "Step: 37660, train/learning_rate: 5.188005616219016e-06\n",
      "Step: 37660, train/epoch: 8.962398529052734\n",
      "Step: 37670, train/loss: 0.0\n",
      "Step: 37670, train/grad_norm: 4.1545812564436346e-05\n",
      "Step: 37670, train/learning_rate: 5.1761066970357206e-06\n",
      "Step: 37670, train/epoch: 8.964778900146484\n",
      "Step: 37680, train/loss: 0.0\n",
      "Step: 37680, train/grad_norm: 3.882938835886307e-05\n",
      "Step: 37680, train/learning_rate: 5.1642073231050745e-06\n",
      "Step: 37680, train/epoch: 8.967158317565918\n",
      "Step: 37690, train/loss: 0.0\n",
      "Step: 37690, train/grad_norm: 4.75357701361645e-05\n",
      "Step: 37690, train/learning_rate: 5.152308403921779e-06\n",
      "Step: 37690, train/epoch: 8.969538688659668\n",
      "Step: 37700, train/loss: 0.0\n",
      "Step: 37700, train/grad_norm: 4.67855716124177e-05\n",
      "Step: 37700, train/learning_rate: 5.140409484738484e-06\n",
      "Step: 37700, train/epoch: 8.971918106079102\n",
      "Step: 37710, train/loss: 0.0\n",
      "Step: 37710, train/grad_norm: 1.361223985441029e-05\n",
      "Step: 37710, train/learning_rate: 5.128510110807838e-06\n",
      "Step: 37710, train/epoch: 8.974297523498535\n",
      "Step: 37720, train/loss: 0.0\n",
      "Step: 37720, train/grad_norm: 3.1069055694388226e-05\n",
      "Step: 37720, train/learning_rate: 5.116611191624543e-06\n",
      "Step: 37720, train/epoch: 8.976677894592285\n",
      "Step: 37730, train/loss: 0.0\n",
      "Step: 37730, train/grad_norm: 4.742719102068804e-05\n",
      "Step: 37730, train/learning_rate: 5.104711817693897e-06\n",
      "Step: 37730, train/epoch: 8.979057312011719\n",
      "Step: 37740, train/loss: 0.0\n",
      "Step: 37740, train/grad_norm: 9.549266542308033e-05\n",
      "Step: 37740, train/learning_rate: 5.092812898510601e-06\n",
      "Step: 37740, train/epoch: 8.981437683105469\n",
      "Step: 37750, train/loss: 0.0\n",
      "Step: 37750, train/grad_norm: 7.464795635314658e-05\n",
      "Step: 37750, train/learning_rate: 5.080913979327306e-06\n",
      "Step: 37750, train/epoch: 8.983817100524902\n",
      "Step: 37760, train/loss: 0.0\n",
      "Step: 37760, train/grad_norm: 2.7916388717130758e-05\n",
      "Step: 37760, train/learning_rate: 5.06901460539666e-06\n",
      "Step: 37760, train/epoch: 8.986197471618652\n",
      "Step: 37770, train/loss: 0.0\n",
      "Step: 37770, train/grad_norm: 0.00013315406977199018\n",
      "Step: 37770, train/learning_rate: 5.057115686213365e-06\n",
      "Step: 37770, train/epoch: 8.988576889038086\n",
      "Step: 37780, train/loss: 0.0\n",
      "Step: 37780, train/grad_norm: 2.6030375011032447e-05\n",
      "Step: 37780, train/learning_rate: 5.04521676703007e-06\n",
      "Step: 37780, train/epoch: 8.99095630645752\n",
      "Step: 37790, train/loss: 0.0\n",
      "Step: 37790, train/grad_norm: 5.566029722103849e-05\n",
      "Step: 37790, train/learning_rate: 5.0333173930994235e-06\n",
      "Step: 37790, train/epoch: 8.99333667755127\n",
      "Step: 37800, train/loss: 0.0\n",
      "Step: 37800, train/grad_norm: 9.851932554738596e-05\n",
      "Step: 37800, train/learning_rate: 5.021418473916128e-06\n",
      "Step: 37800, train/epoch: 8.995716094970703\n",
      "Step: 37810, train/loss: 0.0\n",
      "Step: 37810, train/grad_norm: 1.3386764294409659e-05\n",
      "Step: 37810, train/learning_rate: 5.009519099985482e-06\n",
      "Step: 37810, train/epoch: 8.998096466064453\n",
      "Step: 37818, eval/loss: 0.016234127804636955\n",
      "Step: 37818, eval/accuracy: 0.9977787137031555\n",
      "Step: 37818, eval/f1: 0.9976526498794556\n",
      "Step: 37818, eval/runtime: 856.205078125\n",
      "Step: 37818, eval/samples_per_second: 8.413000106811523\n",
      "Step: 37818, eval/steps_per_second: 1.0520000457763672\n",
      "Step: 37818, train/epoch: 9.0\n",
      "Step: 37820, train/loss: 0.0\n",
      "Step: 37820, train/grad_norm: 7.554670901299687e-06\n",
      "Step: 37820, train/learning_rate: 4.997620180802187e-06\n",
      "Step: 37820, train/epoch: 9.000475883483887\n",
      "Step: 37830, train/loss: 0.0\n",
      "Step: 37830, train/grad_norm: 1.7296286387136206e-05\n",
      "Step: 37830, train/learning_rate: 4.985721261618892e-06\n",
      "Step: 37830, train/epoch: 9.002856254577637\n",
      "Step: 37840, train/loss: 0.0\n",
      "Step: 37840, train/grad_norm: 5.503744614543393e-05\n",
      "Step: 37840, train/learning_rate: 4.973821887688246e-06\n",
      "Step: 37840, train/epoch: 9.00523567199707\n",
      "Step: 37850, train/loss: 0.0\n",
      "Step: 37850, train/grad_norm: 2.054470678558573e-05\n",
      "Step: 37850, train/learning_rate: 4.96192296850495e-06\n",
      "Step: 37850, train/epoch: 9.007615089416504\n",
      "Step: 37860, train/loss: 0.0\n",
      "Step: 37860, train/grad_norm: 8.092287316685542e-06\n",
      "Step: 37860, train/learning_rate: 4.950023594574304e-06\n",
      "Step: 37860, train/epoch: 9.009995460510254\n",
      "Step: 37870, train/loss: 0.0\n",
      "Step: 37870, train/grad_norm: 9.789648902369663e-05\n",
      "Step: 37870, train/learning_rate: 4.938124675391009e-06\n",
      "Step: 37870, train/epoch: 9.012374877929688\n",
      "Step: 37880, train/loss: 0.0\n",
      "Step: 37880, train/grad_norm: 3.114017454208806e-05\n",
      "Step: 37880, train/learning_rate: 4.926225756207714e-06\n",
      "Step: 37880, train/epoch: 9.014755249023438\n",
      "Step: 37890, train/loss: 0.0\n",
      "Step: 37890, train/grad_norm: 2.1181227566557936e-05\n",
      "Step: 37890, train/learning_rate: 4.914326382277068e-06\n",
      "Step: 37890, train/epoch: 9.017134666442871\n",
      "Step: 37900, train/loss: 0.0\n",
      "Step: 37900, train/grad_norm: 5.539020276046358e-05\n",
      "Step: 37900, train/learning_rate: 4.9024274630937725e-06\n",
      "Step: 37900, train/epoch: 9.019514083862305\n",
      "Step: 37910, train/loss: 0.0\n",
      "Step: 37910, train/grad_norm: 1.3144437616574578e-05\n",
      "Step: 37910, train/learning_rate: 4.890528543910477e-06\n",
      "Step: 37910, train/epoch: 9.021894454956055\n",
      "Step: 37920, train/loss: 0.0\n",
      "Step: 37920, train/grad_norm: 3.911997555405833e-05\n",
      "Step: 37920, train/learning_rate: 4.878629169979831e-06\n",
      "Step: 37920, train/epoch: 9.024273872375488\n",
      "Step: 37930, train/loss: 0.0\n",
      "Step: 37930, train/grad_norm: 8.028478987398557e-06\n",
      "Step: 37930, train/learning_rate: 4.866730250796536e-06\n",
      "Step: 37930, train/epoch: 9.026654243469238\n",
      "Step: 37940, train/loss: 0.0\n",
      "Step: 37940, train/grad_norm: 6.370804840116762e-06\n",
      "Step: 37940, train/learning_rate: 4.85483087686589e-06\n",
      "Step: 37940, train/epoch: 9.029033660888672\n",
      "Step: 37950, train/loss: 0.0\n",
      "Step: 37950, train/grad_norm: 2.7444435545476153e-05\n",
      "Step: 37950, train/learning_rate: 4.842931957682595e-06\n",
      "Step: 37950, train/epoch: 9.031414031982422\n",
      "Step: 37960, train/loss: 0.0\n",
      "Step: 37960, train/grad_norm: 1.3075249626126606e-05\n",
      "Step: 37960, train/learning_rate: 4.8310330384992994e-06\n",
      "Step: 37960, train/epoch: 9.033793449401855\n",
      "Step: 37970, train/loss: 0.0\n",
      "Step: 37970, train/grad_norm: 5.3859934268984944e-05\n",
      "Step: 37970, train/learning_rate: 4.819133664568653e-06\n",
      "Step: 37970, train/epoch: 9.036172866821289\n",
      "Step: 37980, train/loss: 0.0\n",
      "Step: 37980, train/grad_norm: 3.478717871985282e-06\n",
      "Step: 37980, train/learning_rate: 4.807234745385358e-06\n",
      "Step: 37980, train/epoch: 9.038553237915039\n",
      "Step: 37990, train/loss: 0.0\n",
      "Step: 37990, train/grad_norm: 2.1951584130874835e-05\n",
      "Step: 37990, train/learning_rate: 4.795335371454712e-06\n",
      "Step: 37990, train/epoch: 9.040932655334473\n",
      "Step: 38000, train/loss: 0.0\n",
      "Step: 38000, train/grad_norm: 1.8995966456714086e-05\n",
      "Step: 38000, train/learning_rate: 4.783436452271417e-06\n",
      "Step: 38000, train/epoch: 9.043313026428223\n",
      "Step: 38010, train/loss: 0.0\n",
      "Step: 38010, train/grad_norm: 2.331107134523336e-05\n",
      "Step: 38010, train/learning_rate: 4.7715375330881216e-06\n",
      "Step: 38010, train/epoch: 9.045692443847656\n",
      "Step: 38020, train/loss: 0.0\n",
      "Step: 38020, train/grad_norm: 2.2419271772378124e-05\n",
      "Step: 38020, train/learning_rate: 4.7596381591574755e-06\n",
      "Step: 38020, train/epoch: 9.048072814941406\n",
      "Step: 38030, train/loss: 0.0\n",
      "Step: 38030, train/grad_norm: 2.7838501409860328e-05\n",
      "Step: 38030, train/learning_rate: 4.74773923997418e-06\n",
      "Step: 38030, train/epoch: 9.05045223236084\n",
      "Step: 38040, train/loss: 0.0\n",
      "Step: 38040, train/grad_norm: 1.2370996046229266e-05\n",
      "Step: 38040, train/learning_rate: 4.735839866043534e-06\n",
      "Step: 38040, train/epoch: 9.052831649780273\n",
      "Step: 38050, train/loss: 0.0\n",
      "Step: 38050, train/grad_norm: 1.8335720596951433e-05\n",
      "Step: 38050, train/learning_rate: 4.723940946860239e-06\n",
      "Step: 38050, train/epoch: 9.055212020874023\n",
      "Step: 38060, train/loss: 0.0\n",
      "Step: 38060, train/grad_norm: 2.2552578229806386e-05\n",
      "Step: 38060, train/learning_rate: 4.712042027676944e-06\n",
      "Step: 38060, train/epoch: 9.057591438293457\n",
      "Step: 38070, train/loss: 0.0\n",
      "Step: 38070, train/grad_norm: 1.7396625480614603e-05\n",
      "Step: 38070, train/learning_rate: 4.700142653746298e-06\n",
      "Step: 38070, train/epoch: 9.059971809387207\n",
      "Step: 38080, train/loss: 0.0\n",
      "Step: 38080, train/grad_norm: 0.00025498613831587136\n",
      "Step: 38080, train/learning_rate: 4.688243734563002e-06\n",
      "Step: 38080, train/epoch: 9.06235122680664\n",
      "Step: 38090, train/loss: 0.0\n",
      "Step: 38090, train/grad_norm: 0.00022973738668952137\n",
      "Step: 38090, train/learning_rate: 4.676344815379707e-06\n",
      "Step: 38090, train/epoch: 9.064730644226074\n",
      "Step: 38100, train/loss: 0.0\n",
      "Step: 38100, train/grad_norm: 2.8120870410930365e-05\n",
      "Step: 38100, train/learning_rate: 4.664445441449061e-06\n",
      "Step: 38100, train/epoch: 9.067111015319824\n",
      "Step: 38110, train/loss: 0.0\n",
      "Step: 38110, train/grad_norm: 1.2187403626739979e-05\n",
      "Step: 38110, train/learning_rate: 4.652546522265766e-06\n",
      "Step: 38110, train/epoch: 9.069490432739258\n",
      "Step: 38120, train/loss: 0.0\n",
      "Step: 38120, train/grad_norm: 0.000227294338401407\n",
      "Step: 38120, train/learning_rate: 4.64064714833512e-06\n",
      "Step: 38120, train/epoch: 9.071870803833008\n",
      "Step: 38130, train/loss: 0.0\n",
      "Step: 38130, train/grad_norm: 4.463279037736356e-05\n",
      "Step: 38130, train/learning_rate: 4.6287482291518245e-06\n",
      "Step: 38130, train/epoch: 9.074250221252441\n",
      "Step: 38140, train/loss: 0.0\n",
      "Step: 38140, train/grad_norm: 0.00023354844597633928\n",
      "Step: 38140, train/learning_rate: 4.616849309968529e-06\n",
      "Step: 38140, train/epoch: 9.076630592346191\n",
      "Step: 38150, train/loss: 0.0\n",
      "Step: 38150, train/grad_norm: 2.7510297513799742e-05\n",
      "Step: 38150, train/learning_rate: 4.604949936037883e-06\n",
      "Step: 38150, train/epoch: 9.079010009765625\n",
      "Step: 38160, train/loss: 0.0\n",
      "Step: 38160, train/grad_norm: 0.003270405577495694\n",
      "Step: 38160, train/learning_rate: 4.593051016854588e-06\n",
      "Step: 38160, train/epoch: 9.081389427185059\n",
      "Step: 38170, train/loss: 0.0\n",
      "Step: 38170, train/grad_norm: 8.921530024963431e-06\n",
      "Step: 38170, train/learning_rate: 4.581151642923942e-06\n",
      "Step: 38170, train/epoch: 9.083769798278809\n",
      "Step: 38180, train/loss: 0.0\n",
      "Step: 38180, train/grad_norm: 2.1524136172956787e-05\n",
      "Step: 38180, train/learning_rate: 4.569252723740647e-06\n",
      "Step: 38180, train/epoch: 9.086149215698242\n",
      "Step: 38190, train/loss: 0.0\n",
      "Step: 38190, train/grad_norm: 3.0807343136984855e-05\n",
      "Step: 38190, train/learning_rate: 4.557353804557351e-06\n",
      "Step: 38190, train/epoch: 9.088529586791992\n",
      "Step: 38200, train/loss: 0.0\n",
      "Step: 38200, train/grad_norm: 1.7447893696953543e-05\n",
      "Step: 38200, train/learning_rate: 4.545454430626705e-06\n",
      "Step: 38200, train/epoch: 9.090909004211426\n",
      "Step: 38210, train/loss: 0.0\n",
      "Step: 38210, train/grad_norm: 1.1039035598514602e-05\n",
      "Step: 38210, train/learning_rate: 4.53355551144341e-06\n",
      "Step: 38210, train/epoch: 9.093289375305176\n",
      "Step: 38220, train/loss: 0.0\n",
      "Step: 38220, train/grad_norm: 1.5957981304381974e-05\n",
      "Step: 38220, train/learning_rate: 4.521656137512764e-06\n",
      "Step: 38220, train/epoch: 9.09566879272461\n",
      "Step: 38230, train/loss: 0.0\n",
      "Step: 38230, train/grad_norm: 8.531527782906778e-06\n",
      "Step: 38230, train/learning_rate: 4.509757218329469e-06\n",
      "Step: 38230, train/epoch: 9.098048210144043\n",
      "Step: 38240, train/loss: 0.0\n",
      "Step: 38240, train/grad_norm: 9.01107705431059e-06\n",
      "Step: 38240, train/learning_rate: 4.4978582991461735e-06\n",
      "Step: 38240, train/epoch: 9.100428581237793\n",
      "Step: 38250, train/loss: 0.0\n",
      "Step: 38250, train/grad_norm: 2.3926941139507107e-05\n",
      "Step: 38250, train/learning_rate: 4.485958925215527e-06\n",
      "Step: 38250, train/epoch: 9.102807998657227\n",
      "Step: 38260, train/loss: 0.0\n",
      "Step: 38260, train/grad_norm: 1.7426738850190304e-05\n",
      "Step: 38260, train/learning_rate: 4.474060006032232e-06\n",
      "Step: 38260, train/epoch: 9.105188369750977\n",
      "Step: 38270, train/loss: 0.0\n",
      "Step: 38270, train/grad_norm: 2.643324296514038e-05\n",
      "Step: 38270, train/learning_rate: 4.462161086848937e-06\n",
      "Step: 38270, train/epoch: 9.10756778717041\n",
      "Step: 38280, train/loss: 0.0\n",
      "Step: 38280, train/grad_norm: 1.3249705261841882e-05\n",
      "Step: 38280, train/learning_rate: 4.450261712918291e-06\n",
      "Step: 38280, train/epoch: 9.109947204589844\n",
      "Step: 38290, train/loss: 0.0\n",
      "Step: 38290, train/grad_norm: 4.153078407398425e-06\n",
      "Step: 38290, train/learning_rate: 4.438362793734996e-06\n",
      "Step: 38290, train/epoch: 9.112327575683594\n",
      "Step: 38300, train/loss: 0.0\n",
      "Step: 38300, train/grad_norm: 4.895105303148739e-05\n",
      "Step: 38300, train/learning_rate: 4.4264634198043495e-06\n",
      "Step: 38300, train/epoch: 9.114706993103027\n",
      "Step: 38310, train/loss: 0.0\n",
      "Step: 38310, train/grad_norm: 0.00028163669048808515\n",
      "Step: 38310, train/learning_rate: 4.414564500621054e-06\n",
      "Step: 38310, train/epoch: 9.117087364196777\n",
      "Step: 38320, train/loss: 0.0\n",
      "Step: 38320, train/grad_norm: 2.1365365682868287e-05\n",
      "Step: 38320, train/learning_rate: 4.402665581437759e-06\n",
      "Step: 38320, train/epoch: 9.119466781616211\n",
      "Step: 38330, train/loss: 0.0\n",
      "Step: 38330, train/grad_norm: 1.0587768883851822e-05\n",
      "Step: 38330, train/learning_rate: 4.390766207507113e-06\n",
      "Step: 38330, train/epoch: 9.121847152709961\n",
      "Step: 38340, train/loss: 0.0\n",
      "Step: 38340, train/grad_norm: 6.496263085864484e-05\n",
      "Step: 38340, train/learning_rate: 4.378867288323818e-06\n",
      "Step: 38340, train/epoch: 9.124226570129395\n",
      "Step: 38350, train/loss: 0.0\n",
      "Step: 38350, train/grad_norm: 4.712157078756718e-06\n",
      "Step: 38350, train/learning_rate: 4.366967914393172e-06\n",
      "Step: 38350, train/epoch: 9.126605987548828\n",
      "Step: 38360, train/loss: 0.0\n",
      "Step: 38360, train/grad_norm: 7.0659043558407575e-06\n",
      "Step: 38360, train/learning_rate: 4.3550689952098764e-06\n",
      "Step: 38360, train/epoch: 9.128986358642578\n",
      "Step: 38370, train/loss: 0.0\n",
      "Step: 38370, train/grad_norm: 2.7653732104226947e-05\n",
      "Step: 38370, train/learning_rate: 4.343170076026581e-06\n",
      "Step: 38370, train/epoch: 9.131365776062012\n",
      "Step: 38380, train/loss: 0.0\n",
      "Step: 38380, train/grad_norm: 0.0014272506814450026\n",
      "Step: 38380, train/learning_rate: 4.331270702095935e-06\n",
      "Step: 38380, train/epoch: 9.133746147155762\n",
      "Step: 38390, train/loss: 0.0\n",
      "Step: 38390, train/grad_norm: 7.611018190800678e-06\n",
      "Step: 38390, train/learning_rate: 4.31937178291264e-06\n",
      "Step: 38390, train/epoch: 9.136125564575195\n",
      "Step: 38400, train/loss: 0.0\n",
      "Step: 38400, train/grad_norm: 0.00010769559594336897\n",
      "Step: 38400, train/learning_rate: 4.307472408981994e-06\n",
      "Step: 38400, train/epoch: 9.138505935668945\n",
      "Step: 38410, train/loss: 0.0\n",
      "Step: 38410, train/grad_norm: 6.031128577888012e-06\n",
      "Step: 38410, train/learning_rate: 4.2955734897986986e-06\n",
      "Step: 38410, train/epoch: 9.140885353088379\n",
      "Step: 38420, train/loss: 0.0\n",
      "Step: 38420, train/grad_norm: 4.7107358113862574e-05\n",
      "Step: 38420, train/learning_rate: 4.283674570615403e-06\n",
      "Step: 38420, train/epoch: 9.143264770507812\n",
      "Step: 38430, train/loss: 0.0\n",
      "Step: 38430, train/grad_norm: 9.084336488740519e-05\n",
      "Step: 38430, train/learning_rate: 4.271775196684757e-06\n",
      "Step: 38430, train/epoch: 9.145645141601562\n",
      "Step: 38440, train/loss: 0.0\n",
      "Step: 38440, train/grad_norm: 0.00012809126928914338\n",
      "Step: 38440, train/learning_rate: 4.259876277501462e-06\n",
      "Step: 38440, train/epoch: 9.148024559020996\n",
      "Step: 38450, train/loss: 0.0\n",
      "Step: 38450, train/grad_norm: 0.00015713369066361338\n",
      "Step: 38450, train/learning_rate: 4.247977358318167e-06\n",
      "Step: 38450, train/epoch: 9.150404930114746\n",
      "Step: 38460, train/loss: 0.0\n",
      "Step: 38460, train/grad_norm: 1.0915788152487949e-05\n",
      "Step: 38460, train/learning_rate: 4.236077984387521e-06\n",
      "Step: 38460, train/epoch: 9.15278434753418\n",
      "Step: 38470, train/loss: 0.0\n",
      "Step: 38470, train/grad_norm: 1.2531496395240538e-05\n",
      "Step: 38470, train/learning_rate: 4.2241790652042255e-06\n",
      "Step: 38470, train/epoch: 9.155163764953613\n",
      "Step: 38480, train/loss: 0.0\n",
      "Step: 38480, train/grad_norm: 9.181403584079817e-06\n",
      "Step: 38480, train/learning_rate: 4.212279691273579e-06\n",
      "Step: 38480, train/epoch: 9.157544136047363\n",
      "Step: 38490, train/loss: 0.0\n",
      "Step: 38490, train/grad_norm: 2.6893783797277138e-05\n",
      "Step: 38490, train/learning_rate: 4.200380772090284e-06\n",
      "Step: 38490, train/epoch: 9.159923553466797\n",
      "Step: 38500, train/loss: 0.0\n",
      "Step: 38500, train/grad_norm: 1.5241961591527797e-05\n",
      "Step: 38500, train/learning_rate: 4.188481852906989e-06\n",
      "Step: 38500, train/epoch: 9.162303924560547\n",
      "Step: 38510, train/loss: 0.0\n",
      "Step: 38510, train/grad_norm: 1.2468473869375885e-05\n",
      "Step: 38510, train/learning_rate: 4.176582478976343e-06\n",
      "Step: 38510, train/epoch: 9.16468334197998\n",
      "Step: 38520, train/loss: 0.0\n",
      "Step: 38520, train/grad_norm: 1.4128351722320076e-05\n",
      "Step: 38520, train/learning_rate: 4.164683559793048e-06\n",
      "Step: 38520, train/epoch: 9.16706371307373\n",
      "Step: 38530, train/loss: 0.0\n",
      "Step: 38530, train/grad_norm: 8.380582585232332e-06\n",
      "Step: 38530, train/learning_rate: 4.1527841858624015e-06\n",
      "Step: 38530, train/epoch: 9.169443130493164\n",
      "Step: 38540, train/loss: 0.0\n",
      "Step: 38540, train/grad_norm: 1.564212107041385e-05\n",
      "Step: 38540, train/learning_rate: 4.140885266679106e-06\n",
      "Step: 38540, train/epoch: 9.171822547912598\n",
      "Step: 38550, train/loss: 0.0\n",
      "Step: 38550, train/grad_norm: 6.859117092972156e-06\n",
      "Step: 38550, train/learning_rate: 4.128986347495811e-06\n",
      "Step: 38550, train/epoch: 9.174202919006348\n",
      "Step: 38560, train/loss: 0.0\n",
      "Step: 38560, train/grad_norm: 2.651176146173384e-05\n",
      "Step: 38560, train/learning_rate: 4.117086973565165e-06\n",
      "Step: 38560, train/epoch: 9.176582336425781\n",
      "Step: 38570, train/loss: 0.0\n",
      "Step: 38570, train/grad_norm: 5.0328526413068175e-05\n",
      "Step: 38570, train/learning_rate: 4.10518805438187e-06\n",
      "Step: 38570, train/epoch: 9.178962707519531\n",
      "Step: 38580, train/loss: 0.0\n",
      "Step: 38580, train/grad_norm: 1.2372886885714252e-05\n",
      "Step: 38580, train/learning_rate: 4.0932891351985745e-06\n",
      "Step: 38580, train/epoch: 9.181342124938965\n",
      "Step: 38590, train/loss: 0.0\n",
      "Step: 38590, train/grad_norm: 3.824161467491649e-06\n",
      "Step: 38590, train/learning_rate: 4.081389761267928e-06\n",
      "Step: 38590, train/epoch: 9.183722496032715\n",
      "Step: 38600, train/loss: 0.0\n",
      "Step: 38600, train/grad_norm: 7.4661998041847255e-06\n",
      "Step: 38600, train/learning_rate: 4.069490842084633e-06\n",
      "Step: 38600, train/epoch: 9.186101913452148\n",
      "Step: 38610, train/loss: 0.0\n",
      "Step: 38610, train/grad_norm: 1.859991971286945e-05\n",
      "Step: 38610, train/learning_rate: 4.057591468153987e-06\n",
      "Step: 38610, train/epoch: 9.188481330871582\n",
      "Step: 38620, train/loss: 0.0\n",
      "Step: 38620, train/grad_norm: 1.4223556718206964e-05\n",
      "Step: 38620, train/learning_rate: 4.045692548970692e-06\n",
      "Step: 38620, train/epoch: 9.190861701965332\n",
      "Step: 38630, train/loss: 0.0\n",
      "Step: 38630, train/grad_norm: 9.42983115237439e-06\n",
      "Step: 38630, train/learning_rate: 4.033793629787397e-06\n",
      "Step: 38630, train/epoch: 9.193241119384766\n",
      "Step: 38640, train/loss: 0.0\n",
      "Step: 38640, train/grad_norm: 1.1271782568655908e-05\n",
      "Step: 38640, train/learning_rate: 4.0218942558567505e-06\n",
      "Step: 38640, train/epoch: 9.195621490478516\n",
      "Step: 38650, train/loss: 0.0\n",
      "Step: 38650, train/grad_norm: 5.500832685356727e-06\n",
      "Step: 38650, train/learning_rate: 4.009995336673455e-06\n",
      "Step: 38650, train/epoch: 9.19800090789795\n",
      "Step: 38660, train/loss: 0.0\n",
      "Step: 38660, train/grad_norm: 3.253575778217055e-05\n",
      "Step: 38660, train/learning_rate: 3.998095962742809e-06\n",
      "Step: 38660, train/epoch: 9.200380325317383\n",
      "Step: 38670, train/loss: 0.0\n",
      "Step: 38670, train/grad_norm: 6.4703735915827565e-06\n",
      "Step: 38670, train/learning_rate: 3.986197043559514e-06\n",
      "Step: 38670, train/epoch: 9.202760696411133\n",
      "Step: 38680, train/loss: 0.0\n",
      "Step: 38680, train/grad_norm: 8.84053315530764e-06\n",
      "Step: 38680, train/learning_rate: 3.974298124376219e-06\n",
      "Step: 38680, train/epoch: 9.205140113830566\n",
      "Step: 38690, train/loss: 0.0\n",
      "Step: 38690, train/grad_norm: 1.729000359773636e-05\n",
      "Step: 38690, train/learning_rate: 3.962398750445573e-06\n",
      "Step: 38690, train/epoch: 9.207520484924316\n",
      "Step: 38700, train/loss: 0.0\n",
      "Step: 38700, train/grad_norm: 3.9181373722385615e-05\n",
      "Step: 38700, train/learning_rate: 3.9504998312622774e-06\n",
      "Step: 38700, train/epoch: 9.20989990234375\n",
      "Step: 38710, train/loss: 0.0\n",
      "Step: 38710, train/grad_norm: 8.50545347930165e-06\n",
      "Step: 38710, train/learning_rate: 3.938600457331631e-06\n",
      "Step: 38710, train/epoch: 9.2122802734375\n",
      "Step: 38720, train/loss: 0.0\n",
      "Step: 38720, train/grad_norm: 1.285349117097212e-05\n",
      "Step: 38720, train/learning_rate: 3.926701538148336e-06\n",
      "Step: 38720, train/epoch: 9.214659690856934\n",
      "Step: 38730, train/loss: 0.0\n",
      "Step: 38730, train/grad_norm: 5.430839337350335e-06\n",
      "Step: 38730, train/learning_rate: 3.914802618965041e-06\n",
      "Step: 38730, train/epoch: 9.217039108276367\n",
      "Step: 38740, train/loss: 0.049400001764297485\n",
      "Step: 38740, train/grad_norm: 127.15312194824219\n",
      "Step: 38740, train/learning_rate: 3.902903245034395e-06\n",
      "Step: 38740, train/epoch: 9.219419479370117\n",
      "Step: 38750, train/loss: 0.0\n",
      "Step: 38750, train/grad_norm: 0.000475528824608773\n",
      "Step: 38750, train/learning_rate: 3.8910043258510996e-06\n",
      "Step: 38750, train/epoch: 9.22179889678955\n",
      "Step: 38760, train/loss: 0.0\n",
      "Step: 38760, train/grad_norm: 3.5071825550403446e-05\n",
      "Step: 38760, train/learning_rate: 3.879105406667804e-06\n",
      "Step: 38760, train/epoch: 9.2241792678833\n",
      "Step: 38770, train/loss: 0.0\n",
      "Step: 38770, train/grad_norm: 8.169990178430453e-05\n",
      "Step: 38770, train/learning_rate: 3.867206032737158e-06\n",
      "Step: 38770, train/epoch: 9.226558685302734\n",
      "Step: 38780, train/loss: 0.1031000018119812\n",
      "Step: 38780, train/grad_norm: 5.0088205171050504e-05\n",
      "Step: 38780, train/learning_rate: 3.855307113553863e-06\n",
      "Step: 38780, train/epoch: 9.228939056396484\n",
      "Step: 38790, train/loss: 0.0\n",
      "Step: 38790, train/grad_norm: 0.00021984719205647707\n",
      "Step: 38790, train/learning_rate: 3.843407739623217e-06\n",
      "Step: 38790, train/epoch: 9.231318473815918\n",
      "Step: 38800, train/loss: 0.0\n",
      "Step: 38800, train/grad_norm: 0.004051556345075369\n",
      "Step: 38800, train/learning_rate: 3.831508820439922e-06\n",
      "Step: 38800, train/epoch: 9.233697891235352\n",
      "Step: 38810, train/loss: 0.0\n",
      "Step: 38810, train/grad_norm: 0.00012466451153159142\n",
      "Step: 38810, train/learning_rate: 3.8196099012566265e-06\n",
      "Step: 38810, train/epoch: 9.236078262329102\n",
      "Step: 38820, train/loss: 0.0\n",
      "Step: 38820, train/grad_norm: 0.0007142347749322653\n",
      "Step: 38820, train/learning_rate: 3.8077105273259804e-06\n",
      "Step: 38820, train/epoch: 9.238457679748535\n",
      "Step: 38830, train/loss: 0.0\n",
      "Step: 38830, train/grad_norm: 0.0002789724967442453\n",
      "Step: 38830, train/learning_rate: 3.795811608142685e-06\n",
      "Step: 38830, train/epoch: 9.240838050842285\n",
      "Step: 38840, train/loss: 0.0\n",
      "Step: 38840, train/grad_norm: 0.00022837892174720764\n",
      "Step: 38840, train/learning_rate: 3.7839124615857145e-06\n",
      "Step: 38840, train/epoch: 9.243217468261719\n",
      "Step: 38850, train/loss: 0.0\n",
      "Step: 38850, train/grad_norm: 0.0004609807801898569\n",
      "Step: 38850, train/learning_rate: 3.772013315028744e-06\n",
      "Step: 38850, train/epoch: 9.245596885681152\n",
      "Step: 38860, train/loss: 0.0\n",
      "Step: 38860, train/grad_norm: 0.00023869297001510859\n",
      "Step: 38860, train/learning_rate: 3.760114168471773e-06\n",
      "Step: 38860, train/epoch: 9.247977256774902\n",
      "Step: 38870, train/loss: 0.0\n",
      "Step: 38870, train/grad_norm: 0.010780963115394115\n",
      "Step: 38870, train/learning_rate: 3.748215249288478e-06\n",
      "Step: 38870, train/epoch: 9.250356674194336\n",
      "Step: 38880, train/loss: 0.0\n",
      "Step: 38880, train/grad_norm: 3.666741758934222e-05\n",
      "Step: 38880, train/learning_rate: 3.7363161027315073e-06\n",
      "Step: 38880, train/epoch: 9.252737045288086\n",
      "Step: 38890, train/loss: 0.0\n",
      "Step: 38890, train/grad_norm: 7.053610897855833e-05\n",
      "Step: 38890, train/learning_rate: 3.7244169561745366e-06\n",
      "Step: 38890, train/epoch: 9.25511646270752\n",
      "Step: 38900, train/loss: 0.0\n",
      "Step: 38900, train/grad_norm: 0.0002948715118691325\n",
      "Step: 38900, train/learning_rate: 3.712517809617566e-06\n",
      "Step: 38900, train/epoch: 9.25749683380127\n",
      "Step: 38910, train/loss: 0.0\n",
      "Step: 38910, train/grad_norm: 0.0388939306139946\n",
      "Step: 38910, train/learning_rate: 3.7006186630605953e-06\n",
      "Step: 38910, train/epoch: 9.259876251220703\n",
      "Step: 38920, train/loss: 0.0\n",
      "Step: 38920, train/grad_norm: 0.004587850067764521\n",
      "Step: 38920, train/learning_rate: 3.6887197438773e-06\n",
      "Step: 38920, train/epoch: 9.262255668640137\n",
      "Step: 38930, train/loss: 0.0\n",
      "Step: 38930, train/grad_norm: 0.00018799352983478457\n",
      "Step: 38930, train/learning_rate: 3.6768205973203294e-06\n",
      "Step: 38930, train/epoch: 9.264636039733887\n",
      "Step: 38940, train/loss: 0.0\n",
      "Step: 38940, train/grad_norm: 7.15219575795345e-05\n",
      "Step: 38940, train/learning_rate: 3.6649214507633587e-06\n",
      "Step: 38940, train/epoch: 9.26701545715332\n",
      "Step: 38950, train/loss: 0.0\n",
      "Step: 38950, train/grad_norm: 8.856904605636373e-05\n",
      "Step: 38950, train/learning_rate: 3.653022304206388e-06\n",
      "Step: 38950, train/epoch: 9.26939582824707\n",
      "Step: 38960, train/loss: 0.0\n",
      "Step: 38960, train/grad_norm: 0.00018635836022440344\n",
      "Step: 38960, train/learning_rate: 3.641123385023093e-06\n",
      "Step: 38960, train/epoch: 9.271775245666504\n",
      "Step: 38970, train/loss: 0.0\n",
      "Step: 38970, train/grad_norm: 0.0010655381483957171\n",
      "Step: 38970, train/learning_rate: 3.629224238466122e-06\n",
      "Step: 38970, train/epoch: 9.274155616760254\n",
      "Step: 38980, train/loss: 0.0\n",
      "Step: 38980, train/grad_norm: 0.0001467708352720365\n",
      "Step: 38980, train/learning_rate: 3.6173250919091515e-06\n",
      "Step: 38980, train/epoch: 9.276535034179688\n",
      "Step: 38990, train/loss: 0.0\n",
      "Step: 38990, train/grad_norm: 0.00034929788671433926\n",
      "Step: 38990, train/learning_rate: 3.605425945352181e-06\n",
      "Step: 38990, train/epoch: 9.278914451599121\n",
      "Step: 39000, train/loss: 0.0\n",
      "Step: 39000, train/grad_norm: 0.00023620520369149745\n",
      "Step: 39000, train/learning_rate: 3.59352679879521e-06\n",
      "Step: 39000, train/epoch: 9.281294822692871\n",
      "Step: 39010, train/loss: 0.0\n",
      "Step: 39010, train/grad_norm: 0.0011819960782304406\n",
      "Step: 39010, train/learning_rate: 3.581627879611915e-06\n",
      "Step: 39010, train/epoch: 9.283674240112305\n",
      "Step: 39020, train/loss: 0.0\n",
      "Step: 39020, train/grad_norm: 0.0005841598031111062\n",
      "Step: 39020, train/learning_rate: 3.5697287330549443e-06\n",
      "Step: 39020, train/epoch: 9.286054611206055\n",
      "Step: 39030, train/loss: 0.0\n",
      "Step: 39030, train/grad_norm: 0.00012355631042737514\n",
      "Step: 39030, train/learning_rate: 3.5578295864979737e-06\n",
      "Step: 39030, train/epoch: 9.288434028625488\n",
      "Step: 39040, train/loss: 0.0\n",
      "Step: 39040, train/grad_norm: 0.0016710656927898526\n",
      "Step: 39040, train/learning_rate: 3.545930439941003e-06\n",
      "Step: 39040, train/epoch: 9.290813446044922\n",
      "Step: 39050, train/loss: 0.0\n",
      "Step: 39050, train/grad_norm: 4.2603525798767805e-05\n",
      "Step: 39050, train/learning_rate: 3.5340315207577078e-06\n",
      "Step: 39050, train/epoch: 9.293193817138672\n",
      "Step: 39060, train/loss: 0.0\n",
      "Step: 39060, train/grad_norm: 9.924334153765813e-05\n",
      "Step: 39060, train/learning_rate: 3.522132374200737e-06\n",
      "Step: 39060, train/epoch: 9.295573234558105\n",
      "Step: 39070, train/loss: 0.0\n",
      "Step: 39070, train/grad_norm: 6.814674998167902e-05\n",
      "Step: 39070, train/learning_rate: 3.5102332276437664e-06\n",
      "Step: 39070, train/epoch: 9.297953605651855\n",
      "Step: 39080, train/loss: 0.0\n",
      "Step: 39080, train/grad_norm: 2.8220887543284334e-05\n",
      "Step: 39080, train/learning_rate: 3.4983340810867958e-06\n",
      "Step: 39080, train/epoch: 9.300333023071289\n",
      "Step: 39090, train/loss: 0.0\n",
      "Step: 39090, train/grad_norm: 0.00012973365664947778\n",
      "Step: 39090, train/learning_rate: 3.486434934529825e-06\n",
      "Step: 39090, train/epoch: 9.302713394165039\n",
      "Step: 39100, train/loss: 0.0\n",
      "Step: 39100, train/grad_norm: 2.8711961931549013e-05\n",
      "Step: 39100, train/learning_rate: 3.47453601534653e-06\n",
      "Step: 39100, train/epoch: 9.305092811584473\n",
      "Step: 39110, train/loss: 0.0\n",
      "Step: 39110, train/grad_norm: 0.0005069577600806952\n",
      "Step: 39110, train/learning_rate: 3.4626368687895592e-06\n",
      "Step: 39110, train/epoch: 9.307472229003906\n",
      "Step: 39120, train/loss: 0.0\n",
      "Step: 39120, train/grad_norm: 0.00041527324356138706\n",
      "Step: 39120, train/learning_rate: 3.4507377222325886e-06\n",
      "Step: 39120, train/epoch: 9.309852600097656\n",
      "Step: 39130, train/loss: 0.0\n",
      "Step: 39130, train/grad_norm: 8.034903294174e-05\n",
      "Step: 39130, train/learning_rate: 3.438838575675618e-06\n",
      "Step: 39130, train/epoch: 9.31223201751709\n",
      "Step: 39140, train/loss: 0.0\n",
      "Step: 39140, train/grad_norm: 0.0008862133836373687\n",
      "Step: 39140, train/learning_rate: 3.4269396564923227e-06\n",
      "Step: 39140, train/epoch: 9.31461238861084\n",
      "Step: 39150, train/loss: 0.0\n",
      "Step: 39150, train/grad_norm: 0.00011021578393410891\n",
      "Step: 39150, train/learning_rate: 3.415040509935352e-06\n",
      "Step: 39150, train/epoch: 9.316991806030273\n",
      "Step: 39160, train/loss: 0.0\n",
      "Step: 39160, train/grad_norm: 0.00019202572002541274\n",
      "Step: 39160, train/learning_rate: 3.4031413633783814e-06\n",
      "Step: 39160, train/epoch: 9.319372177124023\n",
      "Step: 39170, train/loss: 0.0\n",
      "Step: 39170, train/grad_norm: 0.00010763136378955096\n",
      "Step: 39170, train/learning_rate: 3.3912422168214107e-06\n",
      "Step: 39170, train/epoch: 9.321751594543457\n",
      "Step: 39180, train/loss: 0.0\n",
      "Step: 39180, train/grad_norm: 3.7015688576502725e-05\n",
      "Step: 39180, train/learning_rate: 3.37934307026444e-06\n",
      "Step: 39180, train/epoch: 9.32413101196289\n",
      "Step: 39190, train/loss: 0.0\n",
      "Step: 39190, train/grad_norm: 2.1269974240567535e-05\n",
      "Step: 39190, train/learning_rate: 3.367444151081145e-06\n",
      "Step: 39190, train/epoch: 9.32651138305664\n",
      "Step: 39200, train/loss: 0.0\n",
      "Step: 39200, train/grad_norm: 5.08859011461027e-05\n",
      "Step: 39200, train/learning_rate: 3.355545004524174e-06\n",
      "Step: 39200, train/epoch: 9.328890800476074\n",
      "Step: 39210, train/loss: 0.0\n",
      "Step: 39210, train/grad_norm: 0.00023181905271485448\n",
      "Step: 39210, train/learning_rate: 3.3436458579672035e-06\n",
      "Step: 39210, train/epoch: 9.331271171569824\n",
      "Step: 39220, train/loss: 0.0\n",
      "Step: 39220, train/grad_norm: 2.7108821086585522e-05\n",
      "Step: 39220, train/learning_rate: 3.331746711410233e-06\n",
      "Step: 39220, train/epoch: 9.333650588989258\n",
      "Step: 39230, train/loss: 0.0\n",
      "Step: 39230, train/grad_norm: 0.00018524834013078362\n",
      "Step: 39230, train/learning_rate: 3.3198477922269376e-06\n",
      "Step: 39230, train/epoch: 9.336030006408691\n",
      "Step: 39240, train/loss: 0.0\n",
      "Step: 39240, train/grad_norm: 0.0005585765466094017\n",
      "Step: 39240, train/learning_rate: 3.307948645669967e-06\n",
      "Step: 39240, train/epoch: 9.338410377502441\n",
      "Step: 39250, train/loss: 0.0\n",
      "Step: 39250, train/grad_norm: 0.0002853984187822789\n",
      "Step: 39250, train/learning_rate: 3.2960494991129963e-06\n",
      "Step: 39250, train/epoch: 9.340789794921875\n",
      "Step: 39260, train/loss: 0.0\n",
      "Step: 39260, train/grad_norm: 0.0007258543046191335\n",
      "Step: 39260, train/learning_rate: 3.2841503525560256e-06\n",
      "Step: 39260, train/epoch: 9.343170166015625\n",
      "Step: 39270, train/loss: 0.0\n",
      "Step: 39270, train/grad_norm: 3.855794057017192e-05\n",
      "Step: 39270, train/learning_rate: 3.272251205999055e-06\n",
      "Step: 39270, train/epoch: 9.345549583435059\n",
      "Step: 39280, train/loss: 0.0\n",
      "Step: 39280, train/grad_norm: 4.129785520490259e-05\n",
      "Step: 39280, train/learning_rate: 3.2603522868157597e-06\n",
      "Step: 39280, train/epoch: 9.347929954528809\n",
      "Step: 39290, train/loss: 0.0\n",
      "Step: 39290, train/grad_norm: 2.3999611585168168e-05\n",
      "Step: 39290, train/learning_rate: 3.248453140258789e-06\n",
      "Step: 39290, train/epoch: 9.350309371948242\n",
      "Step: 39300, train/loss: 0.06369999796152115\n",
      "Step: 39300, train/grad_norm: 175.14361572265625\n",
      "Step: 39300, train/learning_rate: 3.2365539937018184e-06\n",
      "Step: 39300, train/epoch: 9.352688789367676\n",
      "Step: 39310, train/loss: 0.0\n",
      "Step: 39310, train/grad_norm: 4.191002153675072e-05\n",
      "Step: 39310, train/learning_rate: 3.2246548471448477e-06\n",
      "Step: 39310, train/epoch: 9.355069160461426\n",
      "Step: 39320, train/loss: 0.0\n",
      "Step: 39320, train/grad_norm: 9.241533552994952e-05\n",
      "Step: 39320, train/learning_rate: 3.2127559279615525e-06\n",
      "Step: 39320, train/epoch: 9.35744857788086\n",
      "Step: 39330, train/loss: 0.0\n",
      "Step: 39330, train/grad_norm: 4.413953502080403e-05\n",
      "Step: 39330, train/learning_rate: 3.200856781404582e-06\n",
      "Step: 39330, train/epoch: 9.35982894897461\n",
      "Step: 39340, train/loss: 0.0\n",
      "Step: 39340, train/grad_norm: 0.0020327032543718815\n",
      "Step: 39340, train/learning_rate: 3.188957634847611e-06\n",
      "Step: 39340, train/epoch: 9.362208366394043\n",
      "Step: 39350, train/loss: 0.0\n",
      "Step: 39350, train/grad_norm: 0.0013024278450757265\n",
      "Step: 39350, train/learning_rate: 3.1770584882906405e-06\n",
      "Step: 39350, train/epoch: 9.364588737487793\n",
      "Step: 39360, train/loss: 0.0\n",
      "Step: 39360, train/grad_norm: 0.0011033308692276478\n",
      "Step: 39360, train/learning_rate: 3.16515934173367e-06\n",
      "Step: 39360, train/epoch: 9.366968154907227\n",
      "Step: 39370, train/loss: 0.0\n",
      "Step: 39370, train/grad_norm: 0.0001449221308575943\n",
      "Step: 39370, train/learning_rate: 3.1532604225503746e-06\n",
      "Step: 39370, train/epoch: 9.36934757232666\n",
      "Step: 39380, train/loss: 0.0\n",
      "Step: 39380, train/grad_norm: 9.103701449930668e-05\n",
      "Step: 39380, train/learning_rate: 3.141361275993404e-06\n",
      "Step: 39380, train/epoch: 9.37172794342041\n",
      "Step: 39390, train/loss: 0.0\n",
      "Step: 39390, train/grad_norm: 0.00039150446536950767\n",
      "Step: 39390, train/learning_rate: 3.1294621294364333e-06\n",
      "Step: 39390, train/epoch: 9.374107360839844\n",
      "Step: 39400, train/loss: 0.0\n",
      "Step: 39400, train/grad_norm: 0.00031176971970126033\n",
      "Step: 39400, train/learning_rate: 3.1175629828794627e-06\n",
      "Step: 39400, train/epoch: 9.376487731933594\n",
      "Step: 39410, train/loss: 0.0\n",
      "Step: 39410, train/grad_norm: 9.770756878424436e-05\n",
      "Step: 39410, train/learning_rate: 3.1056640636961674e-06\n",
      "Step: 39410, train/epoch: 9.378867149353027\n",
      "Step: 39420, train/loss: 0.0\n",
      "Step: 39420, train/grad_norm: 5.30564684595447e-05\n",
      "Step: 39420, train/learning_rate: 3.0937649171391968e-06\n",
      "Step: 39420, train/epoch: 9.381246566772461\n",
      "Step: 39430, train/loss: 0.0\n",
      "Step: 39430, train/grad_norm: 2.8522556021925993e-05\n",
      "Step: 39430, train/learning_rate: 3.081865770582226e-06\n",
      "Step: 39430, train/epoch: 9.383626937866211\n",
      "Step: 39440, train/loss: 0.0\n",
      "Step: 39440, train/grad_norm: 0.0022407416254281998\n",
      "Step: 39440, train/learning_rate: 3.0699666240252554e-06\n",
      "Step: 39440, train/epoch: 9.386006355285645\n",
      "Step: 39450, train/loss: 9.999999747378752e-05\n",
      "Step: 39450, train/grad_norm: 0.00022613150940742344\n",
      "Step: 39450, train/learning_rate: 3.0580674774682848e-06\n",
      "Step: 39450, train/epoch: 9.388386726379395\n",
      "Step: 39460, train/loss: 0.0\n",
      "Step: 39460, train/grad_norm: 3.4243068967043655e-06\n",
      "Step: 39460, train/learning_rate: 3.0461685582849896e-06\n",
      "Step: 39460, train/epoch: 9.390766143798828\n",
      "Step: 39470, train/loss: 0.0\n",
      "Step: 39470, train/grad_norm: 9.935586604115088e-06\n",
      "Step: 39470, train/learning_rate: 3.034269411728019e-06\n",
      "Step: 39470, train/epoch: 9.393146514892578\n",
      "Step: 39480, train/loss: 0.0\n",
      "Step: 39480, train/grad_norm: 2.0867302737315185e-05\n",
      "Step: 39480, train/learning_rate: 3.0223702651710482e-06\n",
      "Step: 39480, train/epoch: 9.395525932312012\n",
      "Step: 39490, train/loss: 0.0\n",
      "Step: 39490, train/grad_norm: 3.911355452146381e-05\n",
      "Step: 39490, train/learning_rate: 3.0104711186140776e-06\n",
      "Step: 39490, train/epoch: 9.397905349731445\n",
      "Step: 39500, train/loss: 0.0\n",
      "Step: 39500, train/grad_norm: 2.126580693584401e-05\n",
      "Step: 39500, train/learning_rate: 2.9985721994307823e-06\n",
      "Step: 39500, train/epoch: 9.400285720825195\n",
      "Step: 39510, train/loss: 0.0\n",
      "Step: 39510, train/grad_norm: 0.00030471148784272373\n",
      "Step: 39510, train/learning_rate: 2.9866730528738117e-06\n",
      "Step: 39510, train/epoch: 9.402665138244629\n",
      "Step: 39520, train/loss: 0.0\n",
      "Step: 39520, train/grad_norm: 0.0011391743319109082\n",
      "Step: 39520, train/learning_rate: 2.974773906316841e-06\n",
      "Step: 39520, train/epoch: 9.405045509338379\n",
      "Step: 39530, train/loss: 0.0\n",
      "Step: 39530, train/grad_norm: 0.00011095745139755309\n",
      "Step: 39530, train/learning_rate: 2.9628747597598704e-06\n",
      "Step: 39530, train/epoch: 9.407424926757812\n",
      "Step: 39540, train/loss: 0.0\n",
      "Step: 39540, train/grad_norm: 7.795712917868514e-06\n",
      "Step: 39540, train/learning_rate: 2.9509756132028997e-06\n",
      "Step: 39540, train/epoch: 9.409805297851562\n",
      "Step: 39550, train/loss: 0.0\n",
      "Step: 39550, train/grad_norm: 0.0001436096936231479\n",
      "Step: 39550, train/learning_rate: 2.9390766940196045e-06\n",
      "Step: 39550, train/epoch: 9.412184715270996\n",
      "Step: 39560, train/loss: 0.0\n",
      "Step: 39560, train/grad_norm: 3.788638423429802e-05\n",
      "Step: 39560, train/learning_rate: 2.927177547462634e-06\n",
      "Step: 39560, train/epoch: 9.41456413269043\n",
      "Step: 39570, train/loss: 0.0\n",
      "Step: 39570, train/grad_norm: 3.9438371459254995e-05\n",
      "Step: 39570, train/learning_rate: 2.915278400905663e-06\n",
      "Step: 39570, train/epoch: 9.41694450378418\n",
      "Step: 39580, train/loss: 0.0\n",
      "Step: 39580, train/grad_norm: 8.733740105526522e-05\n",
      "Step: 39580, train/learning_rate: 2.9033792543486925e-06\n",
      "Step: 39580, train/epoch: 9.419323921203613\n",
      "Step: 39590, train/loss: 0.0\n",
      "Step: 39590, train/grad_norm: 1.9216766304452904e-05\n",
      "Step: 39590, train/learning_rate: 2.8914803351653973e-06\n",
      "Step: 39590, train/epoch: 9.421704292297363\n",
      "Step: 39600, train/loss: 0.0\n",
      "Step: 39600, train/grad_norm: 1.203081410494633e-05\n",
      "Step: 39600, train/learning_rate: 2.8795811886084266e-06\n",
      "Step: 39600, train/epoch: 9.424083709716797\n",
      "Step: 39610, train/loss: 0.0\n",
      "Step: 39610, train/grad_norm: 8.12778525869362e-05\n",
      "Step: 39610, train/learning_rate: 2.867682042051456e-06\n",
      "Step: 39610, train/epoch: 9.42646312713623\n",
      "Step: 39620, train/loss: 0.0\n",
      "Step: 39620, train/grad_norm: 7.189959433162585e-05\n",
      "Step: 39620, train/learning_rate: 2.8557828954944853e-06\n",
      "Step: 39620, train/epoch: 9.42884349822998\n",
      "Step: 39630, train/loss: 0.0\n",
      "Step: 39630, train/grad_norm: 0.00010733547969721258\n",
      "Step: 39630, train/learning_rate: 2.84388397631119e-06\n",
      "Step: 39630, train/epoch: 9.431222915649414\n",
      "Step: 39640, train/loss: 0.0\n",
      "Step: 39640, train/grad_norm: 0.00013248574396129698\n",
      "Step: 39640, train/learning_rate: 2.8319848297542194e-06\n",
      "Step: 39640, train/epoch: 9.433603286743164\n",
      "Step: 39650, train/loss: 0.0\n",
      "Step: 39650, train/grad_norm: 7.2316925070481375e-06\n",
      "Step: 39650, train/learning_rate: 2.8200856831972487e-06\n",
      "Step: 39650, train/epoch: 9.435982704162598\n",
      "Step: 39660, train/loss: 0.0\n",
      "Step: 39660, train/grad_norm: 0.014829061925411224\n",
      "Step: 39660, train/learning_rate: 2.808186536640278e-06\n",
      "Step: 39660, train/epoch: 9.438363075256348\n",
      "Step: 39670, train/loss: 0.0\n",
      "Step: 39670, train/grad_norm: 0.0004955295007675886\n",
      "Step: 39670, train/learning_rate: 2.7962873900833074e-06\n",
      "Step: 39670, train/epoch: 9.440742492675781\n",
      "Step: 39680, train/loss: 0.0\n",
      "Step: 39680, train/grad_norm: 0.00022518570767715573\n",
      "Step: 39680, train/learning_rate: 2.784388470900012e-06\n",
      "Step: 39680, train/epoch: 9.443121910095215\n",
      "Step: 39690, train/loss: 0.0\n",
      "Step: 39690, train/grad_norm: 8.817508387437556e-06\n",
      "Step: 39690, train/learning_rate: 2.7724893243430415e-06\n",
      "Step: 39690, train/epoch: 9.445502281188965\n",
      "Step: 39700, train/loss: 0.0\n",
      "Step: 39700, train/grad_norm: 3.5916293654736364e-06\n",
      "Step: 39700, train/learning_rate: 2.760590177786071e-06\n",
      "Step: 39700, train/epoch: 9.447881698608398\n",
      "Step: 39710, train/loss: 0.0\n",
      "Step: 39710, train/grad_norm: 6.8032240960747e-05\n",
      "Step: 39710, train/learning_rate: 2.7486910312291e-06\n",
      "Step: 39710, train/epoch: 9.450262069702148\n",
      "Step: 39720, train/loss: 0.0\n",
      "Step: 39720, train/grad_norm: 2.8726397431455553e-05\n",
      "Step: 39720, train/learning_rate: 2.736792112045805e-06\n",
      "Step: 39720, train/epoch: 9.452641487121582\n",
      "Step: 39730, train/loss: 0.0\n",
      "Step: 39730, train/grad_norm: 0.00010715700773289427\n",
      "Step: 39730, train/learning_rate: 2.7248929654888343e-06\n",
      "Step: 39730, train/epoch: 9.455021858215332\n",
      "Step: 39740, train/loss: 0.0\n",
      "Step: 39740, train/grad_norm: 1.420927310391562e-05\n",
      "Step: 39740, train/learning_rate: 2.7129938189318636e-06\n",
      "Step: 39740, train/epoch: 9.457401275634766\n",
      "Step: 39750, train/loss: 0.0\n",
      "Step: 39750, train/grad_norm: 4.509155587584246e-06\n",
      "Step: 39750, train/learning_rate: 2.701094672374893e-06\n",
      "Step: 39750, train/epoch: 9.4597806930542\n",
      "Step: 39760, train/loss: 0.0\n",
      "Step: 39760, train/grad_norm: 1.5716004782007076e-05\n",
      "Step: 39760, train/learning_rate: 2.6891955258179223e-06\n",
      "Step: 39760, train/epoch: 9.46216106414795\n",
      "Step: 39770, train/loss: 0.0\n",
      "Step: 39770, train/grad_norm: 9.81604080152465e-06\n",
      "Step: 39770, train/learning_rate: 2.677296606634627e-06\n",
      "Step: 39770, train/epoch: 9.464540481567383\n",
      "Step: 39780, train/loss: 0.0\n",
      "Step: 39780, train/grad_norm: 2.26009597099619e-05\n",
      "Step: 39780, train/learning_rate: 2.6653974600776564e-06\n",
      "Step: 39780, train/epoch: 9.466920852661133\n",
      "Step: 39790, train/loss: 0.0\n",
      "Step: 39790, train/grad_norm: 1.112711015593959e-05\n",
      "Step: 39790, train/learning_rate: 2.6534983135206858e-06\n",
      "Step: 39790, train/epoch: 9.469300270080566\n",
      "Step: 39800, train/loss: 0.0\n",
      "Step: 39800, train/grad_norm: 5.497639358509332e-05\n",
      "Step: 39800, train/learning_rate: 2.641599166963715e-06\n",
      "Step: 39800, train/epoch: 9.4716796875\n",
      "Step: 39810, train/loss: 0.0\n",
      "Step: 39810, train/grad_norm: 3.752883458219003e-06\n",
      "Step: 39810, train/learning_rate: 2.62970024778042e-06\n",
      "Step: 39810, train/epoch: 9.47406005859375\n",
      "Step: 39820, train/loss: 0.0\n",
      "Step: 39820, train/grad_norm: 2.1003783331252635e-05\n",
      "Step: 39820, train/learning_rate: 2.6178011012234492e-06\n",
      "Step: 39820, train/epoch: 9.476439476013184\n",
      "Step: 39830, train/loss: 0.0\n",
      "Step: 39830, train/grad_norm: 1.0225640835415106e-05\n",
      "Step: 39830, train/learning_rate: 2.6059019546664786e-06\n",
      "Step: 39830, train/epoch: 9.478819847106934\n",
      "Step: 39840, train/loss: 0.0\n",
      "Step: 39840, train/grad_norm: 3.855161412502639e-05\n",
      "Step: 39840, train/learning_rate: 2.594002808109508e-06\n",
      "Step: 39840, train/epoch: 9.481199264526367\n",
      "Step: 39850, train/loss: 0.0\n",
      "Step: 39850, train/grad_norm: 5.628401140711503e-06\n",
      "Step: 39850, train/learning_rate: 2.5821036615525372e-06\n",
      "Step: 39850, train/epoch: 9.483579635620117\n",
      "Step: 39860, train/loss: 0.0\n",
      "Step: 39860, train/grad_norm: 1.3916659554524813e-05\n",
      "Step: 39860, train/learning_rate: 2.570204742369242e-06\n",
      "Step: 39860, train/epoch: 9.48595905303955\n",
      "Step: 39870, train/loss: 0.0\n",
      "Step: 39870, train/grad_norm: 9.343588317278773e-06\n",
      "Step: 39870, train/learning_rate: 2.5583055958122713e-06\n",
      "Step: 39870, train/epoch: 9.488338470458984\n",
      "Step: 39880, train/loss: 0.0\n",
      "Step: 39880, train/grad_norm: 1.3155338820070028e-05\n",
      "Step: 39880, train/learning_rate: 2.5464064492553007e-06\n",
      "Step: 39880, train/epoch: 9.490718841552734\n",
      "Step: 39890, train/loss: 0.0\n",
      "Step: 39890, train/grad_norm: 3.740928150364198e-05\n",
      "Step: 39890, train/learning_rate: 2.53450730269833e-06\n",
      "Step: 39890, train/epoch: 9.493098258972168\n",
      "Step: 39900, train/loss: 0.0\n",
      "Step: 39900, train/grad_norm: 4.334244295023382e-06\n",
      "Step: 39900, train/learning_rate: 2.522608383515035e-06\n",
      "Step: 39900, train/epoch: 9.495478630065918\n",
      "Step: 39910, train/loss: 0.0\n",
      "Step: 39910, train/grad_norm: 4.041422653244808e-05\n",
      "Step: 39910, train/learning_rate: 2.510709236958064e-06\n",
      "Step: 39910, train/epoch: 9.497858047485352\n",
      "Step: 39920, train/loss: 0.0\n",
      "Step: 39920, train/grad_norm: 1.9123330275760964e-05\n",
      "Step: 39920, train/learning_rate: 2.4988100904010935e-06\n",
      "Step: 39920, train/epoch: 9.500238418579102\n",
      "Step: 39930, train/loss: 0.0\n",
      "Step: 39930, train/grad_norm: 0.00010418235615361482\n",
      "Step: 39930, train/learning_rate: 2.486910943844123e-06\n",
      "Step: 39930, train/epoch: 9.502617835998535\n",
      "Step: 39940, train/loss: 0.0\n",
      "Step: 39940, train/grad_norm: 3.9856298826634884e-05\n",
      "Step: 39940, train/learning_rate: 2.475011797287152e-06\n",
      "Step: 39940, train/epoch: 9.504997253417969\n",
      "Step: 39950, train/loss: 0.0\n",
      "Step: 39950, train/grad_norm: 4.892160723102279e-05\n",
      "Step: 39950, train/learning_rate: 2.463112878103857e-06\n",
      "Step: 39950, train/epoch: 9.507377624511719\n",
      "Step: 39960, train/loss: 0.0\n",
      "Step: 39960, train/grad_norm: 4.724540758616058e-06\n",
      "Step: 39960, train/learning_rate: 2.4512137315468863e-06\n",
      "Step: 39960, train/epoch: 9.509757041931152\n",
      "Step: 39970, train/loss: 0.0\n",
      "Step: 39970, train/grad_norm: 1.8754461052594706e-05\n",
      "Step: 39970, train/learning_rate: 2.4393145849899156e-06\n",
      "Step: 39970, train/epoch: 9.512137413024902\n",
      "Step: 39980, train/loss: 0.0\n",
      "Step: 39980, train/grad_norm: 4.074966909684008e-06\n",
      "Step: 39980, train/learning_rate: 2.427415438432945e-06\n",
      "Step: 39980, train/epoch: 9.514516830444336\n",
      "Step: 39990, train/loss: 0.0\n",
      "Step: 39990, train/grad_norm: 9.420483547728509e-05\n",
      "Step: 39990, train/learning_rate: 2.4155165192496497e-06\n",
      "Step: 39990, train/epoch: 9.51689624786377\n",
      "Step: 40000, train/loss: 0.0\n",
      "Step: 40000, train/grad_norm: 6.465706519520609e-06\n",
      "Step: 40000, train/learning_rate: 2.403617372692679e-06\n",
      "Step: 40000, train/epoch: 9.51927661895752\n",
      "Step: 40010, train/loss: 0.0\n",
      "Step: 40010, train/grad_norm: 2.6665018594940193e-05\n",
      "Step: 40010, train/learning_rate: 2.3917182261357084e-06\n",
      "Step: 40010, train/epoch: 9.521656036376953\n",
      "Step: 40020, train/loss: 0.0\n",
      "Step: 40020, train/grad_norm: 0.00020142503490205854\n",
      "Step: 40020, train/learning_rate: 2.3798190795787377e-06\n",
      "Step: 40020, train/epoch: 9.524036407470703\n",
      "Step: 40030, train/loss: 0.0\n",
      "Step: 40030, train/grad_norm: 8.314879232784733e-05\n",
      "Step: 40030, train/learning_rate: 2.367919933021767e-06\n",
      "Step: 40030, train/epoch: 9.526415824890137\n",
      "Step: 40040, train/loss: 0.0\n",
      "Step: 40040, train/grad_norm: 4.4207445171196014e-05\n",
      "Step: 40040, train/learning_rate: 2.356021013838472e-06\n",
      "Step: 40040, train/epoch: 9.528796195983887\n",
      "Step: 40050, train/loss: 0.0\n",
      "Step: 40050, train/grad_norm: 0.0001800095196813345\n",
      "Step: 40050, train/learning_rate: 2.344121867281501e-06\n",
      "Step: 40050, train/epoch: 9.53117561340332\n",
      "Step: 40060, train/loss: 0.0\n",
      "Step: 40060, train/grad_norm: 4.054999408253934e-06\n",
      "Step: 40060, train/learning_rate: 2.3322227207245305e-06\n",
      "Step: 40060, train/epoch: 9.533555030822754\n",
      "Step: 40070, train/loss: 0.0\n",
      "Step: 40070, train/grad_norm: 4.761586296808673e-06\n",
      "Step: 40070, train/learning_rate: 2.32032357416756e-06\n",
      "Step: 40070, train/epoch: 9.535935401916504\n",
      "Step: 40080, train/loss: 0.0\n",
      "Step: 40080, train/grad_norm: 8.886293471732643e-06\n",
      "Step: 40080, train/learning_rate: 2.3084246549842646e-06\n",
      "Step: 40080, train/epoch: 9.538314819335938\n",
      "Step: 40090, train/loss: 0.0\n",
      "Step: 40090, train/grad_norm: 8.473413799947593e-06\n",
      "Step: 40090, train/learning_rate: 2.296525508427294e-06\n",
      "Step: 40090, train/epoch: 9.540695190429688\n",
      "Step: 40100, train/loss: 0.0\n",
      "Step: 40100, train/grad_norm: 2.9570617698482238e-05\n",
      "Step: 40100, train/learning_rate: 2.2846263618703233e-06\n",
      "Step: 40100, train/epoch: 9.543074607849121\n",
      "Step: 40110, train/loss: 0.0\n",
      "Step: 40110, train/grad_norm: 1.4876841305522248e-05\n",
      "Step: 40110, train/learning_rate: 2.2727272153133526e-06\n",
      "Step: 40110, train/epoch: 9.545454978942871\n",
      "Step: 40120, train/loss: 0.0\n",
      "Step: 40120, train/grad_norm: 1.6675445294822566e-05\n",
      "Step: 40120, train/learning_rate: 2.260828068756382e-06\n",
      "Step: 40120, train/epoch: 9.547834396362305\n",
      "Step: 40130, train/loss: 0.0\n",
      "Step: 40130, train/grad_norm: 1.1225190974073485e-05\n",
      "Step: 40130, train/learning_rate: 2.2489291495730868e-06\n",
      "Step: 40130, train/epoch: 9.550213813781738\n",
      "Step: 40140, train/loss: 0.0\n",
      "Step: 40140, train/grad_norm: 6.594781734747812e-05\n",
      "Step: 40140, train/learning_rate: 2.237030003016116e-06\n",
      "Step: 40140, train/epoch: 9.552594184875488\n",
      "Step: 40150, train/loss: 0.0\n",
      "Step: 40150, train/grad_norm: 5.295614300848683e-06\n",
      "Step: 40150, train/learning_rate: 2.2251308564591454e-06\n",
      "Step: 40150, train/epoch: 9.554973602294922\n",
      "Step: 40160, train/loss: 0.0\n",
      "Step: 40160, train/grad_norm: 3.750507312361151e-05\n",
      "Step: 40160, train/learning_rate: 2.2132317099021748e-06\n",
      "Step: 40160, train/epoch: 9.557353973388672\n",
      "Step: 40170, train/loss: 0.0\n",
      "Step: 40170, train/grad_norm: 4.801472641702276e-06\n",
      "Step: 40170, train/learning_rate: 2.2013327907188796e-06\n",
      "Step: 40170, train/epoch: 9.559733390808105\n",
      "Step: 40180, train/loss: 0.0\n",
      "Step: 40180, train/grad_norm: 2.795758337015286e-05\n",
      "Step: 40180, train/learning_rate: 2.189433644161909e-06\n",
      "Step: 40180, train/epoch: 9.562112808227539\n",
      "Step: 40190, train/loss: 0.0\n",
      "Step: 40190, train/grad_norm: 6.491154636023566e-06\n",
      "Step: 40190, train/learning_rate: 2.1775344976049382e-06\n",
      "Step: 40190, train/epoch: 9.564493179321289\n",
      "Step: 40200, train/loss: 0.0\n",
      "Step: 40200, train/grad_norm: 4.519955382420449e-06\n",
      "Step: 40200, train/learning_rate: 2.1656353510479676e-06\n",
      "Step: 40200, train/epoch: 9.566872596740723\n",
      "Step: 40210, train/loss: 0.0\n",
      "Step: 40210, train/grad_norm: 9.202418368658982e-06\n",
      "Step: 40210, train/learning_rate: 2.153736204490997e-06\n",
      "Step: 40210, train/epoch: 9.569252967834473\n",
      "Step: 40220, train/loss: 0.0\n",
      "Step: 40220, train/grad_norm: 0.000539795495569706\n",
      "Step: 40220, train/learning_rate: 2.1418372853077017e-06\n",
      "Step: 40220, train/epoch: 9.571632385253906\n",
      "Step: 40230, train/loss: 0.0\n",
      "Step: 40230, train/grad_norm: 1.212097231473308e-05\n",
      "Step: 40230, train/learning_rate: 2.129938138750731e-06\n",
      "Step: 40230, train/epoch: 9.574012756347656\n",
      "Step: 40240, train/loss: 0.0\n",
      "Step: 40240, train/grad_norm: 5.102268005430233e-06\n",
      "Step: 40240, train/learning_rate: 2.1180389921937604e-06\n",
      "Step: 40240, train/epoch: 9.57639217376709\n",
      "Step: 40250, train/loss: 0.0\n",
      "Step: 40250, train/grad_norm: 9.425981261301786e-06\n",
      "Step: 40250, train/learning_rate: 2.1061398456367897e-06\n",
      "Step: 40250, train/epoch: 9.578771591186523\n",
      "Step: 40260, train/loss: 0.0\n",
      "Step: 40260, train/grad_norm: 1.5468916899408214e-05\n",
      "Step: 40260, train/learning_rate: 2.0942409264534945e-06\n",
      "Step: 40260, train/epoch: 9.581151962280273\n",
      "Step: 40270, train/loss: 0.0\n",
      "Step: 40270, train/grad_norm: 3.7038594200566877e-06\n",
      "Step: 40270, train/learning_rate: 2.082341779896524e-06\n",
      "Step: 40270, train/epoch: 9.583531379699707\n",
      "Step: 40280, train/loss: 0.0\n",
      "Step: 40280, train/grad_norm: 2.8036438379785977e-05\n",
      "Step: 40280, train/learning_rate: 2.070442633339553e-06\n",
      "Step: 40280, train/epoch: 9.585911750793457\n",
      "Step: 40290, train/loss: 0.0\n",
      "Step: 40290, train/grad_norm: 1.9692801288329065e-05\n",
      "Step: 40290, train/learning_rate: 2.0585434867825825e-06\n",
      "Step: 40290, train/epoch: 9.58829116821289\n",
      "Step: 40300, train/loss: 0.0\n",
      "Step: 40300, train/grad_norm: 5.3888175898464397e-05\n",
      "Step: 40300, train/learning_rate: 2.0466445675992873e-06\n",
      "Step: 40300, train/epoch: 9.59067153930664\n",
      "Step: 40310, train/loss: 0.0\n",
      "Step: 40310, train/grad_norm: 3.1231631965056295e-06\n",
      "Step: 40310, train/learning_rate: 2.0347454210423166e-06\n",
      "Step: 40310, train/epoch: 9.593050956726074\n",
      "Step: 40320, train/loss: 0.0\n",
      "Step: 40320, train/grad_norm: 1.0959273822663818e-05\n",
      "Step: 40320, train/learning_rate: 2.022846274485346e-06\n",
      "Step: 40320, train/epoch: 9.595430374145508\n",
      "Step: 40330, train/loss: 0.0\n",
      "Step: 40330, train/grad_norm: 4.969784185959725e-06\n",
      "Step: 40330, train/learning_rate: 2.0109471279283753e-06\n",
      "Step: 40330, train/epoch: 9.597810745239258\n",
      "Step: 40340, train/loss: 0.0\n",
      "Step: 40340, train/grad_norm: 2.075005113510997e-06\n",
      "Step: 40340, train/learning_rate: 1.9990479813714046e-06\n",
      "Step: 40340, train/epoch: 9.600190162658691\n",
      "Step: 40350, train/loss: 0.0\n",
      "Step: 40350, train/grad_norm: 3.570333319657948e-06\n",
      "Step: 40350, train/learning_rate: 1.9871490621881094e-06\n",
      "Step: 40350, train/epoch: 9.602570533752441\n",
      "Step: 40360, train/loss: 0.0\n",
      "Step: 40360, train/grad_norm: 2.1694151655538008e-05\n",
      "Step: 40360, train/learning_rate: 1.9752499156311387e-06\n",
      "Step: 40360, train/epoch: 9.604949951171875\n",
      "Step: 40370, train/loss: 0.0\n",
      "Step: 40370, train/grad_norm: 4.6278233639895916e-05\n",
      "Step: 40370, train/learning_rate: 1.963350769074168e-06\n",
      "Step: 40370, train/epoch: 9.607329368591309\n",
      "Step: 40380, train/loss: 0.0\n",
      "Step: 40380, train/grad_norm: 1.8833324020306463e-06\n",
      "Step: 40380, train/learning_rate: 1.9514516225171974e-06\n",
      "Step: 40380, train/epoch: 9.609709739685059\n",
      "Step: 40390, train/loss: 0.0\n",
      "Step: 40390, train/grad_norm: 5.064112428954104e-06\n",
      "Step: 40390, train/learning_rate: 1.939552703333902e-06\n",
      "Step: 40390, train/epoch: 9.612089157104492\n",
      "Step: 40400, train/loss: 0.0\n",
      "Step: 40400, train/grad_norm: 1.0418174497317523e-05\n",
      "Step: 40400, train/learning_rate: 1.9276535567769315e-06\n",
      "Step: 40400, train/epoch: 9.614469528198242\n",
      "Step: 40410, train/loss: 0.0\n",
      "Step: 40410, train/grad_norm: 7.607387715324876e-07\n",
      "Step: 40410, train/learning_rate: 1.915754410219961e-06\n",
      "Step: 40410, train/epoch: 9.616848945617676\n",
      "Step: 40420, train/loss: 0.0\n",
      "Step: 40420, train/grad_norm: 0.005896180402487516\n",
      "Step: 40420, train/learning_rate: 1.9038552636629902e-06\n",
      "Step: 40420, train/epoch: 9.619229316711426\n",
      "Step: 40430, train/loss: 0.0\n",
      "Step: 40430, train/grad_norm: 2.037899548668065e-06\n",
      "Step: 40430, train/learning_rate: 1.8919562307928572e-06\n",
      "Step: 40430, train/epoch: 9.62160873413086\n",
      "Step: 40440, train/loss: 0.0\n",
      "Step: 40440, train/grad_norm: 4.518924015428638e-06\n",
      "Step: 40440, train/learning_rate: 1.8800570842358866e-06\n",
      "Step: 40440, train/epoch: 9.623988151550293\n",
      "Step: 40450, train/loss: 0.0\n",
      "Step: 40450, train/grad_norm: 1.0922838555416092e-05\n",
      "Step: 40450, train/learning_rate: 1.8681580513657536e-06\n",
      "Step: 40450, train/epoch: 9.626368522644043\n",
      "Step: 40460, train/loss: 0.0\n",
      "Step: 40460, train/grad_norm: 4.49910839961376e-06\n",
      "Step: 40460, train/learning_rate: 1.856258904808783e-06\n",
      "Step: 40460, train/epoch: 9.628747940063477\n",
      "Step: 40470, train/loss: 0.0\n",
      "Step: 40470, train/grad_norm: 2.7584278541326057e-06\n",
      "Step: 40470, train/learning_rate: 1.84435987193865e-06\n",
      "Step: 40470, train/epoch: 9.631128311157227\n",
      "Step: 40480, train/loss: 0.0\n",
      "Step: 40480, train/grad_norm: 4.2466408558539115e-06\n",
      "Step: 40480, train/learning_rate: 1.8324607253816794e-06\n",
      "Step: 40480, train/epoch: 9.63350772857666\n",
      "Step: 40490, train/loss: 0.0\n",
      "Step: 40490, train/grad_norm: 7.507261034334078e-05\n",
      "Step: 40490, train/learning_rate: 1.8205616925115464e-06\n",
      "Step: 40490, train/epoch: 9.63588809967041\n",
      "Step: 40500, train/loss: 0.0\n",
      "Step: 40500, train/grad_norm: 0.0003094244166277349\n",
      "Step: 40500, train/learning_rate: 1.8086625459545758e-06\n",
      "Step: 40500, train/epoch: 9.638267517089844\n",
      "Step: 40510, train/loss: 0.0\n",
      "Step: 40510, train/grad_norm: 7.250618364196271e-05\n",
      "Step: 40510, train/learning_rate: 1.796763399397605e-06\n",
      "Step: 40510, train/epoch: 9.640646934509277\n",
      "Step: 40520, train/loss: 0.0\n",
      "Step: 40520, train/grad_norm: 2.9628142783622025e-06\n",
      "Step: 40520, train/learning_rate: 1.7848643665274722e-06\n",
      "Step: 40520, train/epoch: 9.643027305603027\n",
      "Step: 40530, train/loss: 0.0\n",
      "Step: 40530, train/grad_norm: 2.3080758182913996e-05\n",
      "Step: 40530, train/learning_rate: 1.7729652199705015e-06\n",
      "Step: 40530, train/epoch: 9.645406723022461\n",
      "Step: 40540, train/loss: 0.0\n",
      "Step: 40540, train/grad_norm: 7.46718724258244e-05\n",
      "Step: 40540, train/learning_rate: 1.7610661871003686e-06\n",
      "Step: 40540, train/epoch: 9.647787094116211\n",
      "Step: 40550, train/loss: 0.0\n",
      "Step: 40550, train/grad_norm: 6.155361916171387e-05\n",
      "Step: 40550, train/learning_rate: 1.7491670405433979e-06\n",
      "Step: 40550, train/epoch: 9.650166511535645\n",
      "Step: 40560, train/loss: 0.0\n",
      "Step: 40560, train/grad_norm: 4.5761062210658565e-05\n",
      "Step: 40560, train/learning_rate: 1.737268007673265e-06\n",
      "Step: 40560, train/epoch: 9.652546882629395\n",
      "Step: 40570, train/loss: 0.0\n",
      "Step: 40570, train/grad_norm: 5.532679278985597e-05\n",
      "Step: 40570, train/learning_rate: 1.7253688611162943e-06\n",
      "Step: 40570, train/epoch: 9.654926300048828\n",
      "Step: 40580, train/loss: 0.0\n",
      "Step: 40580, train/grad_norm: 7.829797687008977e-05\n",
      "Step: 40580, train/learning_rate: 1.7134698282461613e-06\n",
      "Step: 40580, train/epoch: 9.657305717468262\n",
      "Step: 40590, train/loss: 0.0\n",
      "Step: 40590, train/grad_norm: 6.2196163526095916e-06\n",
      "Step: 40590, train/learning_rate: 1.7015706816891907e-06\n",
      "Step: 40590, train/epoch: 9.659686088562012\n",
      "Step: 40600, train/loss: 0.0\n",
      "Step: 40600, train/grad_norm: 8.916305887396447e-06\n",
      "Step: 40600, train/learning_rate: 1.68967153513222e-06\n",
      "Step: 40600, train/epoch: 9.662065505981445\n",
      "Step: 40610, train/loss: 0.0\n",
      "Step: 40610, train/grad_norm: 1.0092797310790047e-05\n",
      "Step: 40610, train/learning_rate: 1.677772502262087e-06\n",
      "Step: 40610, train/epoch: 9.664445877075195\n",
      "Step: 40620, train/loss: 0.0\n",
      "Step: 40620, train/grad_norm: 4.172681292402558e-05\n",
      "Step: 40620, train/learning_rate: 1.6658733557051164e-06\n",
      "Step: 40620, train/epoch: 9.666825294494629\n",
      "Step: 40630, train/loss: 0.0\n",
      "Step: 40630, train/grad_norm: 3.584463047445752e-05\n",
      "Step: 40630, train/learning_rate: 1.6539743228349835e-06\n",
      "Step: 40630, train/epoch: 9.669204711914062\n",
      "Step: 40640, train/loss: 0.18440000712871552\n",
      "Step: 40640, train/grad_norm: 5.645956116495654e-05\n",
      "Step: 40640, train/learning_rate: 1.6420751762780128e-06\n",
      "Step: 40640, train/epoch: 9.671585083007812\n",
      "Step: 40650, train/loss: 0.0\n",
      "Step: 40650, train/grad_norm: 0.00012629131379071623\n",
      "Step: 40650, train/learning_rate: 1.6301761434078799e-06\n",
      "Step: 40650, train/epoch: 9.673964500427246\n",
      "Step: 40660, train/loss: 0.0\n",
      "Step: 40660, train/grad_norm: 5.851293099112809e-05\n",
      "Step: 40660, train/learning_rate: 1.6182769968509092e-06\n",
      "Step: 40660, train/epoch: 9.676344871520996\n",
      "Step: 40670, train/loss: 0.0\n",
      "Step: 40670, train/grad_norm: 0.00029547311714850366\n",
      "Step: 40670, train/learning_rate: 1.6063779639807763e-06\n",
      "Step: 40670, train/epoch: 9.67872428894043\n",
      "Step: 40680, train/loss: 0.0\n",
      "Step: 40680, train/grad_norm: 3.965653741033748e-05\n",
      "Step: 40680, train/learning_rate: 1.5944788174238056e-06\n",
      "Step: 40680, train/epoch: 9.68110466003418\n",
      "Step: 40690, train/loss: 0.0\n",
      "Step: 40690, train/grad_norm: 1.319508646702161e-05\n",
      "Step: 40690, train/learning_rate: 1.582579670866835e-06\n",
      "Step: 40690, train/epoch: 9.683484077453613\n",
      "Step: 40700, train/loss: 0.0\n",
      "Step: 40700, train/grad_norm: 9.78276802925393e-05\n",
      "Step: 40700, train/learning_rate: 1.570680637996702e-06\n",
      "Step: 40700, train/epoch: 9.685863494873047\n",
      "Step: 40710, train/loss: 0.0\n",
      "Step: 40710, train/grad_norm: 3.599525734898634e-05\n",
      "Step: 40710, train/learning_rate: 1.5587814914397313e-06\n",
      "Step: 40710, train/epoch: 9.688243865966797\n",
      "Step: 40720, train/loss: 0.0\n",
      "Step: 40720, train/grad_norm: 3.648824349511415e-05\n",
      "Step: 40720, train/learning_rate: 1.5468824585695984e-06\n",
      "Step: 40720, train/epoch: 9.69062328338623\n",
      "Step: 40730, train/loss: 0.0\n",
      "Step: 40730, train/grad_norm: 0.0005650492967106402\n",
      "Step: 40730, train/learning_rate: 1.5349833120126277e-06\n",
      "Step: 40730, train/epoch: 9.69300365447998\n",
      "Step: 40740, train/loss: 0.0\n",
      "Step: 40740, train/grad_norm: 0.000145482161315158\n",
      "Step: 40740, train/learning_rate: 1.5230842791424948e-06\n",
      "Step: 40740, train/epoch: 9.695383071899414\n",
      "Step: 40750, train/loss: 0.0\n",
      "Step: 40750, train/grad_norm: 0.0003982593188993633\n",
      "Step: 40750, train/learning_rate: 1.5111851325855241e-06\n",
      "Step: 40750, train/epoch: 9.697763442993164\n",
      "Step: 40760, train/loss: 0.0\n",
      "Step: 40760, train/grad_norm: 0.0005772510194219649\n",
      "Step: 40760, train/learning_rate: 1.4992860997153912e-06\n",
      "Step: 40760, train/epoch: 9.700142860412598\n",
      "Step: 40770, train/loss: 0.0\n",
      "Step: 40770, train/grad_norm: 3.9506656321464106e-05\n",
      "Step: 40770, train/learning_rate: 1.4873869531584205e-06\n",
      "Step: 40770, train/epoch: 9.702522277832031\n",
      "Step: 40780, train/loss: 0.0\n",
      "Step: 40780, train/grad_norm: 2.0414954633451998e-05\n",
      "Step: 40780, train/learning_rate: 1.4754878066014498e-06\n",
      "Step: 40780, train/epoch: 9.704902648925781\n",
      "Step: 40790, train/loss: 0.0\n",
      "Step: 40790, train/grad_norm: 8.121313294395804e-05\n",
      "Step: 40790, train/learning_rate: 1.463588773731317e-06\n",
      "Step: 40790, train/epoch: 9.707282066345215\n",
      "Step: 40800, train/loss: 0.0\n",
      "Step: 40800, train/grad_norm: 4.7529712901450694e-05\n",
      "Step: 40800, train/learning_rate: 1.4516896271743462e-06\n",
      "Step: 40800, train/epoch: 9.709662437438965\n",
      "Step: 40810, train/loss: 0.0\n",
      "Step: 40810, train/grad_norm: 3.8107049476820976e-05\n",
      "Step: 40810, train/learning_rate: 1.4397905943042133e-06\n",
      "Step: 40810, train/epoch: 9.712041854858398\n",
      "Step: 40820, train/loss: 0.0\n",
      "Step: 40820, train/grad_norm: 4.273581362213008e-05\n",
      "Step: 40820, train/learning_rate: 1.4278914477472426e-06\n",
      "Step: 40820, train/epoch: 9.714421272277832\n",
      "Step: 40830, train/loss: 0.0\n",
      "Step: 40830, train/grad_norm: 2.8664562705671415e-05\n",
      "Step: 40830, train/learning_rate: 1.4159924148771097e-06\n",
      "Step: 40830, train/epoch: 9.716801643371582\n",
      "Step: 40840, train/loss: 0.0\n",
      "Step: 40840, train/grad_norm: 3.8387101085390896e-05\n",
      "Step: 40840, train/learning_rate: 1.404093268320139e-06\n",
      "Step: 40840, train/epoch: 9.719181060791016\n",
      "Step: 40850, train/loss: 0.0\n",
      "Step: 40850, train/grad_norm: 0.0001237022370332852\n",
      "Step: 40850, train/learning_rate: 1.392194235450006e-06\n",
      "Step: 40850, train/epoch: 9.721561431884766\n",
      "Step: 40860, train/loss: 0.0\n",
      "Step: 40860, train/grad_norm: 1.59216233441839e-05\n",
      "Step: 40860, train/learning_rate: 1.3802950888930354e-06\n",
      "Step: 40860, train/epoch: 9.7239408493042\n",
      "Step: 40870, train/loss: 0.0\n",
      "Step: 40870, train/grad_norm: 0.0008283105562441051\n",
      "Step: 40870, train/learning_rate: 1.3683960560229025e-06\n",
      "Step: 40870, train/epoch: 9.72632122039795\n",
      "Step: 40880, train/loss: 0.0\n",
      "Step: 40880, train/grad_norm: 2.9005284886807203e-05\n",
      "Step: 40880, train/learning_rate: 1.3564969094659318e-06\n",
      "Step: 40880, train/epoch: 9.728700637817383\n",
      "Step: 40890, train/loss: 0.0\n",
      "Step: 40890, train/grad_norm: 4.789759259438142e-05\n",
      "Step: 40890, train/learning_rate: 1.3445977629089612e-06\n",
      "Step: 40890, train/epoch: 9.731080055236816\n",
      "Step: 40900, train/loss: 0.0\n",
      "Step: 40900, train/grad_norm: 0.00011930157052120194\n",
      "Step: 40900, train/learning_rate: 1.3326987300388282e-06\n",
      "Step: 40900, train/epoch: 9.733460426330566\n",
      "Step: 40910, train/loss: 0.0\n",
      "Step: 40910, train/grad_norm: 0.00010370611562393606\n",
      "Step: 40910, train/learning_rate: 1.3207995834818576e-06\n",
      "Step: 40910, train/epoch: 9.73583984375\n",
      "Step: 40920, train/loss: 0.0\n",
      "Step: 40920, train/grad_norm: 0.00021084383479319513\n",
      "Step: 40920, train/learning_rate: 1.3089005506117246e-06\n",
      "Step: 40920, train/epoch: 9.73822021484375\n",
      "Step: 40930, train/loss: 0.07699999958276749\n",
      "Step: 40930, train/grad_norm: 7.451437704730779e-05\n",
      "Step: 40930, train/learning_rate: 1.297001404054754e-06\n",
      "Step: 40930, train/epoch: 9.740599632263184\n",
      "Step: 40940, train/loss: 0.0\n",
      "Step: 40940, train/grad_norm: 7.367588113993406e-05\n",
      "Step: 40940, train/learning_rate: 1.285102371184621e-06\n",
      "Step: 40940, train/epoch: 9.742980003356934\n",
      "Step: 40950, train/loss: 0.0\n",
      "Step: 40950, train/grad_norm: 0.00010358933650422841\n",
      "Step: 40950, train/learning_rate: 1.2732032246276503e-06\n",
      "Step: 40950, train/epoch: 9.745359420776367\n",
      "Step: 40960, train/loss: 0.0\n",
      "Step: 40960, train/grad_norm: 6.470215157605708e-05\n",
      "Step: 40960, train/learning_rate: 1.2613041917575174e-06\n",
      "Step: 40960, train/epoch: 9.7477388381958\n",
      "Step: 40970, train/loss: 0.0\n",
      "Step: 40970, train/grad_norm: 3.140729677397758e-05\n",
      "Step: 40970, train/learning_rate: 1.2494050452005467e-06\n",
      "Step: 40970, train/epoch: 9.75011920928955\n",
      "Step: 40980, train/loss: 0.0\n",
      "Step: 40980, train/grad_norm: 7.055669993860647e-05\n",
      "Step: 40980, train/learning_rate: 1.237505898643576e-06\n",
      "Step: 40980, train/epoch: 9.752498626708984\n",
      "Step: 40990, train/loss: 0.0\n",
      "Step: 40990, train/grad_norm: 0.0005064120632596314\n",
      "Step: 40990, train/learning_rate: 1.2256068657734431e-06\n",
      "Step: 40990, train/epoch: 9.754878997802734\n",
      "Step: 41000, train/loss: 0.0\n",
      "Step: 41000, train/grad_norm: 7.884905062383041e-05\n",
      "Step: 41000, train/learning_rate: 1.2137077192164725e-06\n",
      "Step: 41000, train/epoch: 9.757258415222168\n",
      "Step: 41010, train/loss: 0.0\n",
      "Step: 41010, train/grad_norm: 4.437803363543935e-05\n",
      "Step: 41010, train/learning_rate: 1.2018086863463395e-06\n",
      "Step: 41010, train/epoch: 9.759637832641602\n",
      "Step: 41020, train/loss: 0.0\n",
      "Step: 41020, train/grad_norm: 8.26573304948397e-05\n",
      "Step: 41020, train/learning_rate: 1.1899095397893689e-06\n",
      "Step: 41020, train/epoch: 9.762018203735352\n",
      "Step: 41030, train/loss: 0.0\n",
      "Step: 41030, train/grad_norm: 4.4582215195987374e-05\n",
      "Step: 41030, train/learning_rate: 1.178010506919236e-06\n",
      "Step: 41030, train/epoch: 9.764397621154785\n",
      "Step: 41040, train/loss: 0.0\n",
      "Step: 41040, train/grad_norm: 9.264733671443537e-05\n",
      "Step: 41040, train/learning_rate: 1.1661113603622653e-06\n",
      "Step: 41040, train/epoch: 9.766777992248535\n",
      "Step: 41050, train/loss: 0.0\n",
      "Step: 41050, train/grad_norm: 9.419703565072268e-05\n",
      "Step: 41050, train/learning_rate: 1.1542123274921323e-06\n",
      "Step: 41050, train/epoch: 9.769157409667969\n",
      "Step: 41060, train/loss: 0.0\n",
      "Step: 41060, train/grad_norm: 5.3698429837822914e-05\n",
      "Step: 41060, train/learning_rate: 1.1423131809351617e-06\n",
      "Step: 41060, train/epoch: 9.771537780761719\n",
      "Step: 41070, train/loss: 0.0\n",
      "Step: 41070, train/grad_norm: 5.08804660057649e-05\n",
      "Step: 41070, train/learning_rate: 1.130414034378191e-06\n",
      "Step: 41070, train/epoch: 9.773917198181152\n",
      "Step: 41080, train/loss: 0.0\n",
      "Step: 41080, train/grad_norm: 1.9967574189649895e-05\n",
      "Step: 41080, train/learning_rate: 1.118515001508058e-06\n",
      "Step: 41080, train/epoch: 9.776296615600586\n",
      "Step: 41090, train/loss: 0.0\n",
      "Step: 41090, train/grad_norm: 2.5735978852026165e-05\n",
      "Step: 41090, train/learning_rate: 1.1066158549510874e-06\n",
      "Step: 41090, train/epoch: 9.778676986694336\n",
      "Step: 41100, train/loss: 0.0\n",
      "Step: 41100, train/grad_norm: 0.00010533393651712686\n",
      "Step: 41100, train/learning_rate: 1.0947168220809544e-06\n",
      "Step: 41100, train/epoch: 9.78105640411377\n",
      "Step: 41110, train/loss: 0.0\n",
      "Step: 41110, train/grad_norm: 0.00028797838604077697\n",
      "Step: 41110, train/learning_rate: 1.0828176755239838e-06\n",
      "Step: 41110, train/epoch: 9.78343677520752\n",
      "Step: 41120, train/loss: 0.0\n",
      "Step: 41120, train/grad_norm: 0.00028093118453398347\n",
      "Step: 41120, train/learning_rate: 1.0709186426538508e-06\n",
      "Step: 41120, train/epoch: 9.785816192626953\n",
      "Step: 41130, train/loss: 0.0\n",
      "Step: 41130, train/grad_norm: 7.938564522191882e-05\n",
      "Step: 41130, train/learning_rate: 1.0590194960968802e-06\n",
      "Step: 41130, train/epoch: 9.788196563720703\n",
      "Step: 41140, train/loss: 0.0\n",
      "Step: 41140, train/grad_norm: 2.7424735890235752e-05\n",
      "Step: 41140, train/learning_rate: 1.0471204632267472e-06\n",
      "Step: 41140, train/epoch: 9.790575981140137\n",
      "Step: 41150, train/loss: 0.0\n",
      "Step: 41150, train/grad_norm: 5.1816710765706375e-05\n",
      "Step: 41150, train/learning_rate: 1.0352213166697766e-06\n",
      "Step: 41150, train/epoch: 9.79295539855957\n",
      "Step: 41160, train/loss: 0.0\n",
      "Step: 41160, train/grad_norm: 5.3955925977788866e-05\n",
      "Step: 41160, train/learning_rate: 1.0233222837996436e-06\n",
      "Step: 41160, train/epoch: 9.79533576965332\n",
      "Step: 41170, train/loss: 0.0\n",
      "Step: 41170, train/grad_norm: 0.00011003202962456271\n",
      "Step: 41170, train/learning_rate: 1.011423137242673e-06\n",
      "Step: 41170, train/epoch: 9.797715187072754\n",
      "Step: 41180, train/loss: 0.0\n",
      "Step: 41180, train/grad_norm: 3.5791093978332356e-05\n",
      "Step: 41180, train/learning_rate: 9.995239906857023e-07\n",
      "Step: 41180, train/epoch: 9.800095558166504\n",
      "Step: 41190, train/loss: 0.0\n",
      "Step: 41190, train/grad_norm: 2.750853673205711e-05\n",
      "Step: 41190, train/learning_rate: 9.876249578155694e-07\n",
      "Step: 41190, train/epoch: 9.802474975585938\n",
      "Step: 41200, train/loss: 0.0\n",
      "Step: 41200, train/grad_norm: 5.163880996406078e-05\n",
      "Step: 41200, train/learning_rate: 9.757258112585987e-07\n",
      "Step: 41200, train/epoch: 9.804854393005371\n",
      "Step: 41210, train/loss: 0.0\n",
      "Step: 41210, train/grad_norm: 4.3702915718313307e-05\n",
      "Step: 41210, train/learning_rate: 9.638267783884658e-07\n",
      "Step: 41210, train/epoch: 9.807234764099121\n",
      "Step: 41220, train/loss: 0.0\n",
      "Step: 41220, train/grad_norm: 4.234324296703562e-05\n",
      "Step: 41220, train/learning_rate: 9.519276318314951e-07\n",
      "Step: 41220, train/epoch: 9.809614181518555\n",
      "Step: 41230, train/loss: 0.0\n",
      "Step: 41230, train/grad_norm: 3.2835239835549146e-05\n",
      "Step: 41230, train/learning_rate: 9.400285421179433e-07\n",
      "Step: 41230, train/epoch: 9.811994552612305\n",
      "Step: 41240, train/loss: 0.0\n",
      "Step: 41240, train/grad_norm: 0.00022925567463971674\n",
      "Step: 41240, train/learning_rate: 9.281294524043915e-07\n",
      "Step: 41240, train/epoch: 9.814373970031738\n",
      "Step: 41250, train/loss: 0.0\n",
      "Step: 41250, train/grad_norm: 0.00010711324284784496\n",
      "Step: 41250, train/learning_rate: 9.162303626908397e-07\n",
      "Step: 41250, train/epoch: 9.816754341125488\n",
      "Step: 41260, train/loss: 0.0\n",
      "Step: 41260, train/grad_norm: 5.893899651709944e-05\n",
      "Step: 41260, train/learning_rate: 9.043312729772879e-07\n",
      "Step: 41260, train/epoch: 9.819133758544922\n",
      "Step: 41270, train/loss: 0.0\n",
      "Step: 41270, train/grad_norm: 4.235423693899065e-05\n",
      "Step: 41270, train/learning_rate: 8.924321832637361e-07\n",
      "Step: 41270, train/epoch: 9.821513175964355\n",
      "Step: 41280, train/loss: 0.003800000064074993\n",
      "Step: 41280, train/grad_norm: 5.5772623454686254e-05\n",
      "Step: 41280, train/learning_rate: 8.805330935501843e-07\n",
      "Step: 41280, train/epoch: 9.823893547058105\n",
      "Step: 41290, train/loss: 0.0\n",
      "Step: 41290, train/grad_norm: 4.478861592360772e-05\n",
      "Step: 41290, train/learning_rate: 8.686340038366325e-07\n",
      "Step: 41290, train/epoch: 9.826272964477539\n",
      "Step: 41300, train/loss: 0.0\n",
      "Step: 41300, train/grad_norm: 5.645105375151616e-06\n",
      "Step: 41300, train/learning_rate: 8.567349141230807e-07\n",
      "Step: 41300, train/epoch: 9.828653335571289\n",
      "Step: 41310, train/loss: 0.0\n",
      "Step: 41310, train/grad_norm: 0.0014693498378619552\n",
      "Step: 41310, train/learning_rate: 8.4483576756611e-07\n",
      "Step: 41310, train/epoch: 9.831032752990723\n",
      "Step: 41320, train/loss: 0.0\n",
      "Step: 41320, train/grad_norm: 6.500887684524059e-05\n",
      "Step: 41320, train/learning_rate: 8.329366778525582e-07\n",
      "Step: 41320, train/epoch: 9.833413124084473\n",
      "Step: 41330, train/loss: 0.0\n",
      "Step: 41330, train/grad_norm: 0.00010991646558977664\n",
      "Step: 41330, train/learning_rate: 8.210375881390064e-07\n",
      "Step: 41330, train/epoch: 9.835792541503906\n",
      "Step: 41340, train/loss: 0.0\n",
      "Step: 41340, train/grad_norm: 2.218604458903428e-05\n",
      "Step: 41340, train/learning_rate: 8.091384984254546e-07\n",
      "Step: 41340, train/epoch: 9.83817195892334\n",
      "Step: 41350, train/loss: 0.0\n",
      "Step: 41350, train/grad_norm: 1.5337951481342316e-05\n",
      "Step: 41350, train/learning_rate: 7.972394087119028e-07\n",
      "Step: 41350, train/epoch: 9.84055233001709\n",
      "Step: 41360, train/loss: 0.0\n",
      "Step: 41360, train/grad_norm: 1.0772971108963247e-05\n",
      "Step: 41360, train/learning_rate: 7.85340318998351e-07\n",
      "Step: 41360, train/epoch: 9.842931747436523\n",
      "Step: 41370, train/loss: 0.0\n",
      "Step: 41370, train/grad_norm: 3.6060388083569705e-05\n",
      "Step: 41370, train/learning_rate: 7.734412292847992e-07\n",
      "Step: 41370, train/epoch: 9.845312118530273\n",
      "Step: 41380, train/loss: 0.0\n",
      "Step: 41380, train/grad_norm: 1.3559419130615424e-05\n",
      "Step: 41380, train/learning_rate: 7.615421395712474e-07\n",
      "Step: 41380, train/epoch: 9.847691535949707\n",
      "Step: 41390, train/loss: 0.0\n",
      "Step: 41390, train/grad_norm: 2.8658052542596124e-05\n",
      "Step: 41390, train/learning_rate: 7.496430498576956e-07\n",
      "Step: 41390, train/epoch: 9.85007095336914\n",
      "Step: 41400, train/loss: 0.0\n",
      "Step: 41400, train/grad_norm: 1.555692324473057e-05\n",
      "Step: 41400, train/learning_rate: 7.377439033007249e-07\n",
      "Step: 41400, train/epoch: 9.85245132446289\n",
      "Step: 41410, train/loss: 0.0\n",
      "Step: 41410, train/grad_norm: 1.0788313375087455e-05\n",
      "Step: 41410, train/learning_rate: 7.258448135871731e-07\n",
      "Step: 41410, train/epoch: 9.854830741882324\n",
      "Step: 41420, train/loss: 0.0\n",
      "Step: 41420, train/grad_norm: 3.41372870025225e-05\n",
      "Step: 41420, train/learning_rate: 7.139457238736213e-07\n",
      "Step: 41420, train/epoch: 9.857211112976074\n",
      "Step: 41430, train/loss: 0.0\n",
      "Step: 41430, train/grad_norm: 1.714839345368091e-05\n",
      "Step: 41430, train/learning_rate: 7.020466341600695e-07\n",
      "Step: 41430, train/epoch: 9.859590530395508\n",
      "Step: 41440, train/loss: 0.0\n",
      "Step: 41440, train/grad_norm: 8.522396819898859e-05\n",
      "Step: 41440, train/learning_rate: 6.901475444465177e-07\n",
      "Step: 41440, train/epoch: 9.861970901489258\n",
      "Step: 41450, train/loss: 0.0\n",
      "Step: 41450, train/grad_norm: 1.1245463610976003e-05\n",
      "Step: 41450, train/learning_rate: 6.782484547329659e-07\n",
      "Step: 41450, train/epoch: 9.864350318908691\n",
      "Step: 41460, train/loss: 0.0\n",
      "Step: 41460, train/grad_norm: 3.500757156871259e-05\n",
      "Step: 41460, train/learning_rate: 6.663493650194141e-07\n",
      "Step: 41460, train/epoch: 9.866729736328125\n",
      "Step: 41470, train/loss: 0.0\n",
      "Step: 41470, train/grad_norm: 2.5317311155959032e-05\n",
      "Step: 41470, train/learning_rate: 6.544502753058623e-07\n",
      "Step: 41470, train/epoch: 9.869110107421875\n",
      "Step: 41480, train/loss: 0.0\n",
      "Step: 41480, train/grad_norm: 5.546694956137799e-05\n",
      "Step: 41480, train/learning_rate: 6.425511855923105e-07\n",
      "Step: 41480, train/epoch: 9.871489524841309\n",
      "Step: 41490, train/loss: 0.0\n",
      "Step: 41490, train/grad_norm: 4.6221706725191325e-05\n",
      "Step: 41490, train/learning_rate: 6.306520958787587e-07\n",
      "Step: 41490, train/epoch: 9.873869895935059\n",
      "Step: 41500, train/loss: 0.0\n",
      "Step: 41500, train/grad_norm: 7.442486094078049e-05\n",
      "Step: 41500, train/learning_rate: 6.18752949321788e-07\n",
      "Step: 41500, train/epoch: 9.876249313354492\n",
      "Step: 41510, train/loss: 0.0\n",
      "Step: 41510, train/grad_norm: 4.336560596129857e-05\n",
      "Step: 41510, train/learning_rate: 6.068538596082362e-07\n",
      "Step: 41510, train/epoch: 9.878629684448242\n",
      "Step: 41520, train/loss: 0.0\n",
      "Step: 41520, train/grad_norm: 6.15097233094275e-05\n",
      "Step: 41520, train/learning_rate: 5.949547698946844e-07\n",
      "Step: 41520, train/epoch: 9.881009101867676\n",
      "Step: 41530, train/loss: 0.0\n",
      "Step: 41530, train/grad_norm: 6.36963932265644e-06\n",
      "Step: 41530, train/learning_rate: 5.830556801811326e-07\n",
      "Step: 41530, train/epoch: 9.88338851928711\n",
      "Step: 41540, train/loss: 0.0\n",
      "Step: 41540, train/grad_norm: 2.390218469372485e-05\n",
      "Step: 41540, train/learning_rate: 5.711565904675808e-07\n",
      "Step: 41540, train/epoch: 9.88576889038086\n",
      "Step: 41550, train/loss: 0.0\n",
      "Step: 41550, train/grad_norm: 0.00011347651161486283\n",
      "Step: 41550, train/learning_rate: 5.59257500754029e-07\n",
      "Step: 41550, train/epoch: 9.888148307800293\n",
      "Step: 41560, train/loss: 0.0\n",
      "Step: 41560, train/grad_norm: 4.1918847273336723e-05\n",
      "Step: 41560, train/learning_rate: 5.473584110404772e-07\n",
      "Step: 41560, train/epoch: 9.890528678894043\n",
      "Step: 41570, train/loss: 0.0\n",
      "Step: 41570, train/grad_norm: 0.00016433378914371133\n",
      "Step: 41570, train/learning_rate: 5.354593213269254e-07\n",
      "Step: 41570, train/epoch: 9.892908096313477\n",
      "Step: 41580, train/loss: 0.0\n",
      "Step: 41580, train/grad_norm: 7.460336928488687e-05\n",
      "Step: 41580, train/learning_rate: 5.235602316133736e-07\n",
      "Step: 41580, train/epoch: 9.89528751373291\n",
      "Step: 41590, train/loss: 0.0\n",
      "Step: 41590, train/grad_norm: 1.615719338587951e-05\n",
      "Step: 41590, train/learning_rate: 5.116611418998218e-07\n",
      "Step: 41590, train/epoch: 9.89766788482666\n",
      "Step: 41600, train/loss: 0.0\n",
      "Step: 41600, train/grad_norm: 0.00017180867143906653\n",
      "Step: 41600, train/learning_rate: 4.997619953428512e-07\n",
      "Step: 41600, train/epoch: 9.900047302246094\n",
      "Step: 41610, train/loss: 0.0\n",
      "Step: 41610, train/grad_norm: 1.2922683708893601e-05\n",
      "Step: 41610, train/learning_rate: 4.878629056292993e-07\n",
      "Step: 41610, train/epoch: 9.902427673339844\n",
      "Step: 41620, train/loss: 0.0\n",
      "Step: 41620, train/grad_norm: 3.546091465977952e-05\n",
      "Step: 41620, train/learning_rate: 4.7596381591574755e-07\n",
      "Step: 41620, train/epoch: 9.904807090759277\n",
      "Step: 41630, train/loss: 0.0\n",
      "Step: 41630, train/grad_norm: 3.0649018299300224e-05\n",
      "Step: 41630, train/learning_rate: 4.6406472620219574e-07\n",
      "Step: 41630, train/epoch: 9.907187461853027\n",
      "Step: 41640, train/loss: 0.0\n",
      "Step: 41640, train/grad_norm: 3.397943510208279e-05\n",
      "Step: 41640, train/learning_rate: 4.5216563648864394e-07\n",
      "Step: 41640, train/epoch: 9.909566879272461\n",
      "Step: 41650, train/loss: 0.0\n",
      "Step: 41650, train/grad_norm: 0.00020195607794448733\n",
      "Step: 41650, train/learning_rate: 4.4026654677509214e-07\n",
      "Step: 41650, train/epoch: 9.911946296691895\n",
      "Step: 41660, train/loss: 0.0\n",
      "Step: 41660, train/grad_norm: 4.432939022080973e-05\n",
      "Step: 41660, train/learning_rate: 4.2836745706154034e-07\n",
      "Step: 41660, train/epoch: 9.914326667785645\n",
      "Step: 41670, train/loss: 0.0\n",
      "Step: 41670, train/grad_norm: 4.035249003209174e-05\n",
      "Step: 41670, train/learning_rate: 4.164683389262791e-07\n",
      "Step: 41670, train/epoch: 9.916706085205078\n",
      "Step: 41680, train/loss: 0.0\n",
      "Step: 41680, train/grad_norm: 3.349843609612435e-05\n",
      "Step: 41680, train/learning_rate: 4.045692492127273e-07\n",
      "Step: 41680, train/epoch: 9.919086456298828\n",
      "Step: 41690, train/loss: 0.0\n",
      "Step: 41690, train/grad_norm: 2.3667202185606584e-05\n",
      "Step: 41690, train/learning_rate: 3.926701594991755e-07\n",
      "Step: 41690, train/epoch: 9.921465873718262\n",
      "Step: 41700, train/loss: 0.0\n",
      "Step: 41700, train/grad_norm: 4.2772611777763814e-05\n",
      "Step: 41700, train/learning_rate: 3.807710697856237e-07\n",
      "Step: 41700, train/epoch: 9.923846244812012\n",
      "Step: 41710, train/loss: 0.0\n",
      "Step: 41710, train/grad_norm: 0.00016873294953256845\n",
      "Step: 41710, train/learning_rate: 3.6887195165036246e-07\n",
      "Step: 41710, train/epoch: 9.926225662231445\n",
      "Step: 41720, train/loss: 0.0\n",
      "Step: 41720, train/grad_norm: 4.6166893298504874e-05\n",
      "Step: 41720, train/learning_rate: 3.5697286193681066e-07\n",
      "Step: 41720, train/epoch: 9.928605079650879\n",
      "Step: 41730, train/loss: 0.0\n",
      "Step: 41730, train/grad_norm: 3.2891617593122646e-05\n",
      "Step: 41730, train/learning_rate: 3.4507377222325886e-07\n",
      "Step: 41730, train/epoch: 9.930985450744629\n",
      "Step: 41740, train/loss: 0.0\n",
      "Step: 41740, train/grad_norm: 9.579135803505778e-05\n",
      "Step: 41740, train/learning_rate: 3.3317468250970705e-07\n",
      "Step: 41740, train/epoch: 9.933364868164062\n",
      "Step: 41750, train/loss: 0.0\n",
      "Step: 41750, train/grad_norm: 4.6767498133704066e-05\n",
      "Step: 41750, train/learning_rate: 3.2127559279615525e-07\n",
      "Step: 41750, train/epoch: 9.935745239257812\n",
      "Step: 41760, train/loss: 0.0\n",
      "Step: 41760, train/grad_norm: 5.362898809835315e-05\n",
      "Step: 41760, train/learning_rate: 3.09376474660894e-07\n",
      "Step: 41760, train/epoch: 9.938124656677246\n",
      "Step: 41770, train/loss: 0.0\n",
      "Step: 41770, train/grad_norm: 7.566205113107571e-06\n",
      "Step: 41770, train/learning_rate: 2.974773849473422e-07\n",
      "Step: 41770, train/epoch: 9.94050407409668\n",
      "Step: 41780, train/loss: 0.0\n",
      "Step: 41780, train/grad_norm: 0.00014576141256839037\n",
      "Step: 41780, train/learning_rate: 2.855782952337904e-07\n",
      "Step: 41780, train/epoch: 9.94288444519043\n",
      "Step: 41790, train/loss: 0.0\n",
      "Step: 41790, train/grad_norm: 1.84048039955087e-05\n",
      "Step: 41790, train/learning_rate: 2.736792055202386e-07\n",
      "Step: 41790, train/epoch: 9.945263862609863\n",
      "Step: 41800, train/loss: 0.0\n",
      "Step: 41800, train/grad_norm: 1.2699822036665864e-05\n",
      "Step: 41800, train/learning_rate: 2.617801158066868e-07\n",
      "Step: 41800, train/epoch: 9.947644233703613\n",
      "Step: 41810, train/loss: 0.0\n",
      "Step: 41810, train/grad_norm: 1.6115525795612484e-05\n",
      "Step: 41810, train/learning_rate: 2.498809976714256e-07\n",
      "Step: 41810, train/epoch: 9.950023651123047\n",
      "Step: 41820, train/loss: 0.0\n",
      "Step: 41820, train/grad_norm: 3.897862188750878e-05\n",
      "Step: 41820, train/learning_rate: 2.3798190795787377e-07\n",
      "Step: 41820, train/epoch: 9.952404022216797\n",
      "Step: 41830, train/loss: 0.0\n",
      "Step: 41830, train/grad_norm: 5.289971886668354e-05\n",
      "Step: 41830, train/learning_rate: 2.2608281824432197e-07\n",
      "Step: 41830, train/epoch: 9.95478343963623\n",
      "Step: 41840, train/loss: 0.0\n",
      "Step: 41840, train/grad_norm: 5.004682680009864e-05\n",
      "Step: 41840, train/learning_rate: 2.1418372853077017e-07\n",
      "Step: 41840, train/epoch: 9.957162857055664\n",
      "Step: 41850, train/loss: 0.0\n",
      "Step: 41850, train/grad_norm: 4.899222039966844e-05\n",
      "Step: 41850, train/learning_rate: 2.0228462460636365e-07\n",
      "Step: 41850, train/epoch: 9.959543228149414\n",
      "Step: 41860, train/loss: 0.0\n",
      "Step: 41860, train/grad_norm: 4.420896220835857e-05\n",
      "Step: 41860, train/learning_rate: 1.9038553489281185e-07\n",
      "Step: 41860, train/epoch: 9.961922645568848\n",
      "Step: 41870, train/loss: 0.0\n",
      "Step: 41870, train/grad_norm: 1.598550625203643e-05\n",
      "Step: 41870, train/learning_rate: 1.7848643096840533e-07\n",
      "Step: 41870, train/epoch: 9.964303016662598\n",
      "Step: 41880, train/loss: 0.0\n",
      "Step: 41880, train/grad_norm: 7.70797396398848e-06\n",
      "Step: 41880, train/learning_rate: 1.6658734125485353e-07\n",
      "Step: 41880, train/epoch: 9.966682434082031\n",
      "Step: 41890, train/loss: 0.0\n",
      "Step: 41890, train/grad_norm: 4.206150697427802e-05\n",
      "Step: 41890, train/learning_rate: 1.54688237330447e-07\n",
      "Step: 41890, train/epoch: 9.969062805175781\n",
      "Step: 41900, train/loss: 0.0\n",
      "Step: 41900, train/grad_norm: 1.2550091923912987e-05\n",
      "Step: 41900, train/learning_rate: 1.427891476168952e-07\n",
      "Step: 41900, train/epoch: 9.971442222595215\n",
      "Step: 41910, train/loss: 0.0\n",
      "Step: 41910, train/grad_norm: 3.2178893889067695e-05\n",
      "Step: 41910, train/learning_rate: 1.308900579033434e-07\n",
      "Step: 41910, train/epoch: 9.973821640014648\n",
      "Step: 41920, train/loss: 0.0\n",
      "Step: 41920, train/grad_norm: 1.1593685485422611e-05\n",
      "Step: 41920, train/learning_rate: 1.1899095397893689e-07\n",
      "Step: 41920, train/epoch: 9.976202011108398\n",
      "Step: 41930, train/loss: 0.0\n",
      "Step: 41930, train/grad_norm: 0.0001706641778582707\n",
      "Step: 41930, train/learning_rate: 1.0709186426538508e-07\n",
      "Step: 41930, train/epoch: 9.978581428527832\n",
      "Step: 41940, train/loss: 0.0\n",
      "Step: 41940, train/grad_norm: 4.440985867404379e-05\n",
      "Step: 41940, train/learning_rate: 9.519276744640592e-08\n",
      "Step: 41940, train/epoch: 9.980961799621582\n",
      "Step: 41950, train/loss: 0.0\n",
      "Step: 41950, train/grad_norm: 4.016704406240024e-05\n",
      "Step: 41950, train/learning_rate: 8.329367062742676e-08\n",
      "Step: 41950, train/epoch: 9.983341217041016\n",
      "Step: 41960, train/loss: 0.0\n",
      "Step: 41960, train/grad_norm: 2.673869312275201e-05\n",
      "Step: 41960, train/learning_rate: 7.13945738084476e-08\n",
      "Step: 41960, train/epoch: 9.98572063446045\n",
      "Step: 41970, train/loss: 0.0\n",
      "Step: 41970, train/grad_norm: 2.188368671340868e-05\n",
      "Step: 41970, train/learning_rate: 5.949547698946844e-08\n",
      "Step: 41970, train/epoch: 9.9881010055542\n",
      "Step: 41980, train/loss: 0.0\n",
      "Step: 41980, train/grad_norm: 1.4407061826204881e-05\n",
      "Step: 41980, train/learning_rate: 4.759638372320296e-08\n",
      "Step: 41980, train/epoch: 9.990480422973633\n",
      "Step: 41990, train/loss: 0.0\n",
      "Step: 41990, train/grad_norm: 5.501668783836067e-05\n",
      "Step: 41990, train/learning_rate: 3.56972869042238e-08\n",
      "Step: 41990, train/epoch: 9.992860794067383\n",
      "Step: 42000, train/loss: 0.0\n",
      "Step: 42000, train/grad_norm: 1.5288331269402988e-05\n",
      "Step: 42000, train/learning_rate: 2.379819186160148e-08\n",
      "Step: 42000, train/epoch: 9.995240211486816\n",
      "Step: 42010, train/loss: 0.0\n",
      "Step: 42010, train/grad_norm: 1.6838745068525895e-05\n",
      "Step: 42010, train/learning_rate: 1.189909593080074e-08\n",
      "Step: 42010, train/epoch: 9.997620582580566\n",
      "Step: 42020, train/loss: 0.0\n",
      "Step: 42020, train/grad_norm: 0.0007635365473106503\n",
      "Step: 42020, train/learning_rate: 0.0\n",
      "Step: 42020, train/epoch: 10.0\n",
      "Step: 42020, eval/loss: 0.01631724275648594\n",
      "Step: 42020, eval/accuracy: 0.9976398944854736\n",
      "Step: 42020, eval/f1: 0.9975061416625977\n",
      "Step: 42020, eval/runtime: 855.8834838867188\n",
      "Step: 42020, eval/samples_per_second: 8.416000366210938\n",
      "Step: 42020, eval/steps_per_second: 1.0529999732971191\n",
      "Step: 42020, train/epoch: 10.0\n",
      "Step: 42020, train/train_runtime: 43098.91796875\n",
      "Step: 42020, train/train_samples_per_second: 7.798999786376953\n",
      "Step: 42020, train/train_steps_per_second: 0.9750000238418579\n",
      "Step: 42020, train/total_flos: 8.029224343764468e+18\n",
      "Step: 42020, train/train_loss: 0.0009709231089800596\n",
      "Step: 42020, train/epoch: 10.0\n",
      "Reading events from file: ./praxis-gemma-7b-small-finetune/logs/events.out.tfevents.1716687041.hephaestus.7747.0\n",
      "Step: 10, train/loss: 5.183899879455566\n",
      "Step: 10, train/grad_norm: 224.91546630859375\n",
      "Step: 10, train/learning_rate: 4.998810254619457e-05\n",
      "Step: 10, train/epoch: 0.0023798190522938967\n",
      "Step: 20, train/loss: 4.873899936676025\n",
      "Step: 20, train/grad_norm: 188.18556213378906\n",
      "Step: 20, train/learning_rate: 4.997620271751657e-05\n",
      "Step: 20, train/epoch: 0.004759638104587793\n",
      "Step: 30, train/loss: 3.473299980163574\n",
      "Step: 30, train/grad_norm: 195.29371643066406\n",
      "Step: 30, train/learning_rate: 4.9964302888838574e-05\n",
      "Step: 30, train/epoch: 0.007139457389712334\n",
      "Step: 40, train/loss: 3.3427000045776367\n",
      "Step: 40, train/grad_norm: 427.759521484375\n",
      "Step: 40, train/learning_rate: 4.995240306016058e-05\n",
      "Step: 40, train/epoch: 0.009519276209175587\n",
      "Step: 50, train/loss: 3.040299892425537\n",
      "Step: 50, train/grad_norm: 236.1119384765625\n",
      "Step: 50, train/learning_rate: 4.994050323148258e-05\n",
      "Step: 50, train/epoch: 0.011899095959961414\n",
      "Step: 60, train/loss: 2.4370999336242676\n",
      "Step: 60, train/grad_norm: 188.3197479248047\n",
      "Step: 60, train/learning_rate: 4.992860704078339e-05\n",
      "Step: 60, train/epoch: 0.014278914779424667\n",
      "Step: 70, train/loss: 1.3693000078201294\n",
      "Step: 70, train/grad_norm: 263.4351501464844\n",
      "Step: 70, train/learning_rate: 4.9916707212105393e-05\n",
      "Step: 70, train/epoch: 0.016658734530210495\n",
      "Step: 80, train/loss: 0.8568000197410583\n",
      "Step: 80, train/grad_norm: 144.5196990966797\n",
      "Step: 80, train/learning_rate: 4.9904807383427396e-05\n",
      "Step: 80, train/epoch: 0.019038552418351173\n",
      "Step: 90, train/loss: 0.5439000129699707\n",
      "Step: 90, train/grad_norm: 4.928252220153809\n",
      "Step: 90, train/learning_rate: 4.98929075547494e-05\n",
      "Step: 90, train/epoch: 0.021418372169137\n",
      "Step: 100, train/loss: 0.45500001311302185\n",
      "Step: 100, train/grad_norm: 35.44502639770508\n",
      "Step: 100, train/learning_rate: 4.98810077260714e-05\n",
      "Step: 100, train/epoch: 0.02379819191992283\n",
      "Step: 110, train/loss: 1.1402000188827515\n",
      "Step: 110, train/grad_norm: 268.6637878417969\n",
      "Step: 110, train/learning_rate: 4.986911153537221e-05\n",
      "Step: 110, train/epoch: 0.026178009808063507\n",
      "Step: 120, train/loss: 0.6644999980926514\n",
      "Step: 120, train/grad_norm: 8.264488220214844\n",
      "Step: 120, train/learning_rate: 4.9857211706694216e-05\n",
      "Step: 120, train/epoch: 0.028557829558849335\n",
      "Step: 130, train/loss: 0.48570001125335693\n",
      "Step: 130, train/grad_norm: 46.9017333984375\n",
      "Step: 130, train/learning_rate: 4.984531187801622e-05\n",
      "Step: 130, train/epoch: 0.030937649309635162\n",
      "Step: 140, train/loss: 0.5799000263214111\n",
      "Step: 140, train/grad_norm: 179.37034606933594\n",
      "Step: 140, train/learning_rate: 4.983341204933822e-05\n",
      "Step: 140, train/epoch: 0.03331746906042099\n",
      "Step: 150, train/loss: 0.37720000743865967\n",
      "Step: 150, train/grad_norm: 110.18096923828125\n",
      "Step: 150, train/learning_rate: 4.9821512220660225e-05\n",
      "Step: 150, train/epoch: 0.03569728881120682\n",
      "Step: 160, train/loss: 0.2526000142097473\n",
      "Step: 160, train/grad_norm: 26.753677368164062\n",
      "Step: 160, train/learning_rate: 4.9809616029961035e-05\n",
      "Step: 160, train/epoch: 0.03807710483670235\n",
      "Step: 170, train/loss: 0.1080000028014183\n",
      "Step: 170, train/grad_norm: 1.4438934385907487e-06\n",
      "Step: 170, train/learning_rate: 4.979771620128304e-05\n",
      "Step: 170, train/epoch: 0.040456924587488174\n",
      "Step: 180, train/loss: 0.5806000232696533\n",
      "Step: 180, train/grad_norm: 0.1685301810503006\n",
      "Step: 180, train/learning_rate: 4.978581637260504e-05\n",
      "Step: 180, train/epoch: 0.042836744338274\n",
      "Step: 190, train/loss: 0.24079999327659607\n",
      "Step: 190, train/grad_norm: 52.871482849121094\n",
      "Step: 190, train/learning_rate: 4.9773916543927044e-05\n",
      "Step: 190, train/epoch: 0.04521656408905983\n",
      "Step: 200, train/loss: 9.999999747378752e-05\n",
      "Step: 200, train/grad_norm: 0.012540677562355995\n",
      "Step: 200, train/learning_rate: 4.976201671524905e-05\n",
      "Step: 200, train/epoch: 0.04759638383984566\n",
      "Step: 210, train/loss: 0.09679999947547913\n",
      "Step: 210, train/grad_norm: 6.861411065983702e-07\n",
      "Step: 210, train/learning_rate: 4.975012052454986e-05\n",
      "Step: 210, train/epoch: 0.049976203590631485\n",
      "Step: 220, train/loss: 0.07540000230073929\n",
      "Step: 220, train/grad_norm: 3.2511332392459735e-05\n",
      "Step: 220, train/learning_rate: 4.973822069587186e-05\n",
      "Step: 220, train/epoch: 0.052356019616127014\n",
      "Step: 230, train/loss: 0.00039999998989515007\n",
      "Step: 230, train/grad_norm: 0.0003009059582836926\n",
      "Step: 230, train/learning_rate: 4.972632086719386e-05\n",
      "Step: 230, train/epoch: 0.05473583936691284\n",
      "Step: 240, train/loss: 0.12150000035762787\n",
      "Step: 240, train/grad_norm: 8.95252960617654e-05\n",
      "Step: 240, train/learning_rate: 4.9714421038515866e-05\n",
      "Step: 240, train/epoch: 0.05711565911769867\n",
      "Step: 250, train/loss: 0.0\n",
      "Step: 250, train/grad_norm: 9.418966510565951e-05\n",
      "Step: 250, train/learning_rate: 4.970252120983787e-05\n",
      "Step: 250, train/epoch: 0.0594954788684845\n",
      "Step: 260, train/loss: 0.26249998807907104\n",
      "Step: 260, train/grad_norm: 5.022326945436362e-07\n",
      "Step: 260, train/learning_rate: 4.969062501913868e-05\n",
      "Step: 260, train/epoch: 0.061875298619270325\n",
      "Step: 270, train/loss: 0.0\n",
      "Step: 270, train/grad_norm: 3.6025184613208694e-07\n",
      "Step: 270, train/learning_rate: 4.967872519046068e-05\n",
      "Step: 270, train/epoch: 0.06425511837005615\n",
      "Step: 280, train/loss: 0.13830000162124634\n",
      "Step: 280, train/grad_norm: 0.00029625455499626696\n",
      "Step: 280, train/learning_rate: 4.9666825361782685e-05\n",
      "Step: 280, train/epoch: 0.06663493812084198\n",
      "Step: 290, train/loss: 0.5126000046730042\n",
      "Step: 290, train/grad_norm: 129.4424285888672\n",
      "Step: 290, train/learning_rate: 4.965492553310469e-05\n",
      "Step: 290, train/epoch: 0.06901475787162781\n",
      "Step: 300, train/loss: 0.2305999994277954\n",
      "Step: 300, train/grad_norm: 194.92994689941406\n",
      "Step: 300, train/learning_rate: 4.964302570442669e-05\n",
      "Step: 300, train/epoch: 0.07139457762241364\n",
      "Step: 310, train/loss: 0.2290000021457672\n",
      "Step: 310, train/grad_norm: 0.010039448738098145\n",
      "Step: 310, train/learning_rate: 4.96311295137275e-05\n",
      "Step: 310, train/epoch: 0.07377438992261887\n",
      "Step: 320, train/loss: 0.00019999999494757503\n",
      "Step: 320, train/grad_norm: 1.1505130714795087e-05\n",
      "Step: 320, train/learning_rate: 4.9619229685049504e-05\n",
      "Step: 320, train/epoch: 0.0761542096734047\n",
      "Step: 330, train/loss: 0.000699999975040555\n",
      "Step: 330, train/grad_norm: 5.002834320068359\n",
      "Step: 330, train/learning_rate: 4.960732985637151e-05\n",
      "Step: 330, train/epoch: 0.07853402942419052\n",
      "Step: 340, train/loss: 0.4440000057220459\n",
      "Step: 340, train/grad_norm: 0.49404454231262207\n",
      "Step: 340, train/learning_rate: 4.959543002769351e-05\n",
      "Step: 340, train/epoch: 0.08091384917497635\n",
      "Step: 350, train/loss: 0.06530000269412994\n",
      "Step: 350, train/grad_norm: 9.079634111230916e-08\n",
      "Step: 350, train/learning_rate: 4.958353019901551e-05\n",
      "Step: 350, train/epoch: 0.08329366892576218\n",
      "Step: 360, train/loss: 0.01640000008046627\n",
      "Step: 360, train/grad_norm: 1.6178430087165907e-05\n",
      "Step: 360, train/learning_rate: 4.957163400831632e-05\n",
      "Step: 360, train/epoch: 0.085673488676548\n",
      "Step: 370, train/loss: 0.17890000343322754\n",
      "Step: 370, train/grad_norm: 1.1030554560420569e-05\n",
      "Step: 370, train/learning_rate: 4.9559734179638326e-05\n",
      "Step: 370, train/epoch: 0.08805330842733383\n",
      "Step: 380, train/loss: 0.11089999973773956\n",
      "Step: 380, train/grad_norm: 0.00032583935535512865\n",
      "Step: 380, train/learning_rate: 4.954783435096033e-05\n",
      "Step: 380, train/epoch: 0.09043312817811966\n",
      "Step: 390, train/loss: 0.17839999496936798\n",
      "Step: 390, train/grad_norm: 243.61512756347656\n",
      "Step: 390, train/learning_rate: 4.953593452228233e-05\n",
      "Step: 390, train/epoch: 0.09281294792890549\n",
      "Step: 400, train/loss: 0.3856000006198883\n",
      "Step: 400, train/grad_norm: 0.0322592630982399\n",
      "Step: 400, train/learning_rate: 4.9524034693604335e-05\n",
      "Step: 400, train/epoch: 0.09519276767969131\n",
      "Step: 410, train/loss: 0.16089999675750732\n",
      "Step: 410, train/grad_norm: 0.006474161520600319\n",
      "Step: 410, train/learning_rate: 4.9512138502905145e-05\n",
      "Step: 410, train/epoch: 0.09757258743047714\n",
      "Step: 420, train/loss: 0.00800000037997961\n",
      "Step: 420, train/grad_norm: 0.0006295029888860881\n",
      "Step: 420, train/learning_rate: 4.950023867422715e-05\n",
      "Step: 420, train/epoch: 0.09995240718126297\n",
      "Step: 430, train/loss: 0.4668000042438507\n",
      "Step: 430, train/grad_norm: 6.91661528051668e-09\n",
      "Step: 430, train/learning_rate: 4.948833884554915e-05\n",
      "Step: 430, train/epoch: 0.1023322194814682\n",
      "Step: 440, train/loss: 0.07020000368356705\n",
      "Step: 440, train/grad_norm: 6.717237321929304e-10\n",
      "Step: 440, train/learning_rate: 4.9476439016871154e-05\n",
      "Step: 440, train/epoch: 0.10471203923225403\n",
      "Step: 450, train/loss: 0.1054999977350235\n",
      "Step: 450, train/grad_norm: 4.493317717901846e-09\n",
      "Step: 450, train/learning_rate: 4.946453918819316e-05\n",
      "Step: 450, train/epoch: 0.10709185898303986\n",
      "Step: 460, train/loss: 0.0\n",
      "Step: 460, train/grad_norm: 4.602851186064072e-05\n",
      "Step: 460, train/learning_rate: 4.945264299749397e-05\n",
      "Step: 460, train/epoch: 0.10947167873382568\n",
      "Step: 470, train/loss: 0.06800000369548798\n",
      "Step: 470, train/grad_norm: 6.758679091944941e-07\n",
      "Step: 470, train/learning_rate: 4.944074316881597e-05\n",
      "Step: 470, train/epoch: 0.11185149848461151\n",
      "Step: 480, train/loss: 0.3707999885082245\n",
      "Step: 480, train/grad_norm: 4.176439324510284e-05\n",
      "Step: 480, train/learning_rate: 4.9428843340137973e-05\n",
      "Step: 480, train/epoch: 0.11423131823539734\n",
      "Step: 490, train/loss: 0.3578000068664551\n",
      "Step: 490, train/grad_norm: 0.005865380633622408\n",
      "Step: 490, train/learning_rate: 4.9416943511459976e-05\n",
      "Step: 490, train/epoch: 0.11661113798618317\n",
      "Step: 500, train/loss: 0.094200000166893\n",
      "Step: 500, train/grad_norm: 2.8921123885083944e-05\n",
      "Step: 500, train/learning_rate: 4.940504368278198e-05\n",
      "Step: 500, train/epoch: 0.118990957736969\n",
      "Step: 510, train/loss: 0.366100013256073\n",
      "Step: 510, train/grad_norm: 1.3530144315154757e-05\n",
      "Step: 510, train/learning_rate: 4.939314749208279e-05\n",
      "Step: 510, train/epoch: 0.12137077748775482\n",
      "Step: 520, train/loss: 0.16259999573230743\n",
      "Step: 520, train/grad_norm: 2.6463405447429977e-05\n",
      "Step: 520, train/learning_rate: 4.938124766340479e-05\n",
      "Step: 520, train/epoch: 0.12375059723854065\n",
      "Step: 530, train/loss: 0.8514999747276306\n",
      "Step: 530, train/grad_norm: 0.2646079361438751\n",
      "Step: 530, train/learning_rate: 4.9369347834726796e-05\n",
      "Step: 530, train/epoch: 0.12613041698932648\n",
      "Step: 540, train/loss: 0.0027000000700354576\n",
      "Step: 540, train/grad_norm: 0.04816755652427673\n",
      "Step: 540, train/learning_rate: 4.93574480060488e-05\n",
      "Step: 540, train/epoch: 0.1285102367401123\n",
      "Step: 550, train/loss: 0.1088000014424324\n",
      "Step: 550, train/grad_norm: 0.06665410846471786\n",
      "Step: 550, train/learning_rate: 4.93455481773708e-05\n",
      "Step: 550, train/epoch: 0.13089005649089813\n",
      "Step: 560, train/loss: 0.03689999878406525\n",
      "Step: 560, train/grad_norm: 0.00018120120512321591\n",
      "Step: 560, train/learning_rate: 4.933365198667161e-05\n",
      "Step: 560, train/epoch: 0.13326987624168396\n",
      "Step: 570, train/loss: 0.5601000189781189\n",
      "Step: 570, train/grad_norm: 0.0005102724535390735\n",
      "Step: 570, train/learning_rate: 4.9321752157993615e-05\n",
      "Step: 570, train/epoch: 0.1356496959924698\n",
      "Step: 580, train/loss: 0.6391000151634216\n",
      "Step: 580, train/grad_norm: 3.792131053614867e-08\n",
      "Step: 580, train/learning_rate: 4.930985232931562e-05\n",
      "Step: 580, train/epoch: 0.13802951574325562\n",
      "Step: 590, train/loss: 0.006300000008195639\n",
      "Step: 590, train/grad_norm: 43.20380783081055\n",
      "Step: 590, train/learning_rate: 4.929795250063762e-05\n",
      "Step: 590, train/epoch: 0.14040933549404144\n",
      "Step: 600, train/loss: 0.3903999924659729\n",
      "Step: 600, train/grad_norm: 0.17626574635505676\n",
      "Step: 600, train/learning_rate: 4.9286052671959624e-05\n",
      "Step: 600, train/epoch: 0.14278915524482727\n",
      "Step: 610, train/loss: 0.00019999999494757503\n",
      "Step: 610, train/grad_norm: 5.445428178063594e-05\n",
      "Step: 610, train/learning_rate: 4.9274156481260434e-05\n",
      "Step: 610, train/epoch: 0.1451689600944519\n",
      "Step: 620, train/loss: 0.1444000005722046\n",
      "Step: 620, train/grad_norm: 0.00020067908917553723\n",
      "Step: 620, train/learning_rate: 4.926225665258244e-05\n",
      "Step: 620, train/epoch: 0.14754877984523773\n",
      "Step: 630, train/loss: 0.03669999912381172\n",
      "Step: 630, train/grad_norm: 2.2930349587113596e-05\n",
      "Step: 630, train/learning_rate: 4.925035682390444e-05\n",
      "Step: 630, train/epoch: 0.14992859959602356\n",
      "Step: 640, train/loss: 0.01640000008046627\n",
      "Step: 640, train/grad_norm: 1.2631997670098372e-08\n",
      "Step: 640, train/learning_rate: 4.923845699522644e-05\n",
      "Step: 640, train/epoch: 0.1523084193468094\n",
      "Step: 650, train/loss: 0.006399999838322401\n",
      "Step: 650, train/grad_norm: 9.1788297140738e-07\n",
      "Step: 650, train/learning_rate: 4.9226557166548446e-05\n",
      "Step: 650, train/epoch: 0.15468823909759521\n",
      "Step: 660, train/loss: 0.09260000288486481\n",
      "Step: 660, train/grad_norm: 353.0547180175781\n",
      "Step: 660, train/learning_rate: 4.9214660975849256e-05\n",
      "Step: 660, train/epoch: 0.15706805884838104\n",
      "Step: 670, train/loss: 0.18279999494552612\n",
      "Step: 670, train/grad_norm: 3.9683431651837964e-08\n",
      "Step: 670, train/learning_rate: 4.920276114717126e-05\n",
      "Step: 670, train/epoch: 0.15944787859916687\n",
      "Step: 680, train/loss: 0.26589998602867126\n",
      "Step: 680, train/grad_norm: 0.00878579169511795\n",
      "Step: 680, train/learning_rate: 4.919086131849326e-05\n",
      "Step: 680, train/epoch: 0.1618276983499527\n",
      "Step: 690, train/loss: 0.2451000064611435\n",
      "Step: 690, train/grad_norm: 0.0022641560062766075\n",
      "Step: 690, train/learning_rate: 4.9178961489815265e-05\n",
      "Step: 690, train/epoch: 0.16420751810073853\n",
      "Step: 700, train/loss: 0.10999999940395355\n",
      "Step: 700, train/grad_norm: 2.299980640411377\n",
      "Step: 700, train/learning_rate: 4.916706166113727e-05\n",
      "Step: 700, train/epoch: 0.16658733785152435\n",
      "Step: 710, train/loss: 0.2840000092983246\n",
      "Step: 710, train/grad_norm: 9.695700100564864e-06\n",
      "Step: 710, train/learning_rate: 4.915516547043808e-05\n",
      "Step: 710, train/epoch: 0.16896715760231018\n",
      "Step: 720, train/loss: 9.999999747378752e-05\n",
      "Step: 720, train/grad_norm: 1.8424145764583955e-07\n",
      "Step: 720, train/learning_rate: 4.914326564176008e-05\n",
      "Step: 720, train/epoch: 0.171346977353096\n",
      "Step: 730, train/loss: 0.0\n",
      "Step: 730, train/grad_norm: 1.758651251293486e-07\n",
      "Step: 730, train/learning_rate: 4.9131365813082084e-05\n",
      "Step: 730, train/epoch: 0.17372679710388184\n",
      "Step: 740, train/loss: 0.5099999904632568\n",
      "Step: 740, train/grad_norm: 4.981043821317144e-05\n",
      "Step: 740, train/learning_rate: 4.911946598440409e-05\n",
      "Step: 740, train/epoch: 0.17610661685466766\n",
      "Step: 750, train/loss: 0.09070000052452087\n",
      "Step: 750, train/grad_norm: 0.03397323563694954\n",
      "Step: 750, train/learning_rate: 4.910756615572609e-05\n",
      "Step: 750, train/epoch: 0.1784864366054535\n",
      "Step: 760, train/loss: 0.061799999326467514\n",
      "Step: 760, train/grad_norm: 0.10909995436668396\n",
      "Step: 760, train/learning_rate: 4.90956699650269e-05\n",
      "Step: 760, train/epoch: 0.18086625635623932\n",
      "Step: 770, train/loss: 0.14139999449253082\n",
      "Step: 770, train/grad_norm: 4.8394620534963906e-05\n",
      "Step: 770, train/learning_rate: 4.90837701363489e-05\n",
      "Step: 770, train/epoch: 0.18324607610702515\n",
      "Step: 780, train/loss: 0.0\n",
      "Step: 780, train/grad_norm: 0.0030991705134510994\n",
      "Step: 780, train/learning_rate: 4.9071870307670906e-05\n",
      "Step: 780, train/epoch: 0.18562589585781097\n",
      "Step: 790, train/loss: 0.004399999976158142\n",
      "Step: 790, train/grad_norm: 2.380103978794068e-05\n",
      "Step: 790, train/learning_rate: 4.905997047899291e-05\n",
      "Step: 790, train/epoch: 0.1880057156085968\n",
      "Step: 800, train/loss: 0.0\n",
      "Step: 800, train/grad_norm: 4.6328389657901425e-07\n",
      "Step: 800, train/learning_rate: 4.904807065031491e-05\n",
      "Step: 800, train/epoch: 0.19038553535938263\n",
      "Step: 810, train/loss: 0.4153999984264374\n",
      "Step: 810, train/grad_norm: 8.83610664459411e-06\n",
      "Step: 810, train/learning_rate: 4.903617445961572e-05\n",
      "Step: 810, train/epoch: 0.19276535511016846\n",
      "Step: 820, train/loss: 0.40459999442100525\n",
      "Step: 820, train/grad_norm: 2.7919024887523847e-06\n",
      "Step: 820, train/learning_rate: 4.9024274630937725e-05\n",
      "Step: 820, train/epoch: 0.19514517486095428\n",
      "Step: 830, train/loss: 0.0007999999797903001\n",
      "Step: 830, train/grad_norm: 3.1924307677400066e-06\n",
      "Step: 830, train/learning_rate: 4.901237480225973e-05\n",
      "Step: 830, train/epoch: 0.1975249946117401\n",
      "Step: 840, train/loss: 0.0706000030040741\n",
      "Step: 840, train/grad_norm: 0.0006240749498829246\n",
      "Step: 840, train/learning_rate: 4.900047497358173e-05\n",
      "Step: 840, train/epoch: 0.19990481436252594\n",
      "Step: 850, train/loss: 0.0027000000700354576\n",
      "Step: 850, train/grad_norm: 1.1061475561291445e-05\n",
      "Step: 850, train/learning_rate: 4.8988575144903734e-05\n",
      "Step: 850, train/epoch: 0.20228461921215057\n",
      "Step: 860, train/loss: 0.0\n",
      "Step: 860, train/grad_norm: 0.00017400925571564585\n",
      "Step: 860, train/learning_rate: 4.8976678954204544e-05\n",
      "Step: 860, train/epoch: 0.2046644389629364\n",
      "Step: 870, train/loss: 0.09740000218153\n",
      "Step: 870, train/grad_norm: 2.1467890292115044e-06\n",
      "Step: 870, train/learning_rate: 4.896477912552655e-05\n",
      "Step: 870, train/epoch: 0.20704425871372223\n",
      "Step: 880, train/loss: 0.09960000216960907\n",
      "Step: 880, train/grad_norm: 0.0031391645316034555\n",
      "Step: 880, train/learning_rate: 4.895287929684855e-05\n",
      "Step: 880, train/epoch: 0.20942407846450806\n",
      "Step: 890, train/loss: 0.2687000036239624\n",
      "Step: 890, train/grad_norm: 2.4896407921914943e-05\n",
      "Step: 890, train/learning_rate: 4.8940979468170553e-05\n",
      "Step: 890, train/epoch: 0.21180389821529388\n",
      "Step: 900, train/loss: 0.005200000014156103\n",
      "Step: 900, train/grad_norm: 2.825355841196142e-05\n",
      "Step: 900, train/learning_rate: 4.8929079639492556e-05\n",
      "Step: 900, train/epoch: 0.2141837179660797\n",
      "Step: 910, train/loss: 0.005100000184029341\n",
      "Step: 910, train/grad_norm: 7.319022552110255e-05\n",
      "Step: 910, train/learning_rate: 4.8917183448793367e-05\n",
      "Step: 910, train/epoch: 0.21656353771686554\n",
      "Step: 920, train/loss: 0.17949999868869781\n",
      "Step: 920, train/grad_norm: 8.691689146189674e-09\n",
      "Step: 920, train/learning_rate: 4.890528362011537e-05\n",
      "Step: 920, train/epoch: 0.21894335746765137\n",
      "Step: 930, train/loss: 0.01119999960064888\n",
      "Step: 930, train/grad_norm: 0.18687033653259277\n",
      "Step: 930, train/learning_rate: 4.889338379143737e-05\n",
      "Step: 930, train/epoch: 0.2213231772184372\n",
      "Step: 940, train/loss: 0.40560001134872437\n",
      "Step: 940, train/grad_norm: 0.28657621145248413\n",
      "Step: 940, train/learning_rate: 4.8881483962759376e-05\n",
      "Step: 940, train/epoch: 0.22370299696922302\n",
      "Step: 950, train/loss: 0.0\n",
      "Step: 950, train/grad_norm: 0.00010325561743229628\n",
      "Step: 950, train/learning_rate: 4.886958413408138e-05\n",
      "Step: 950, train/epoch: 0.22608281672000885\n",
      "Step: 960, train/loss: 0.3077999949455261\n",
      "Step: 960, train/grad_norm: 0.0005306541570462286\n",
      "Step: 960, train/learning_rate: 4.885768794338219e-05\n",
      "Step: 960, train/epoch: 0.22846263647079468\n",
      "Step: 970, train/loss: 0.15320000052452087\n",
      "Step: 970, train/grad_norm: 0.00434858538210392\n",
      "Step: 970, train/learning_rate: 4.884578811470419e-05\n",
      "Step: 970, train/epoch: 0.2308424562215805\n",
      "Step: 980, train/loss: 0.01889999955892563\n",
      "Step: 980, train/grad_norm: 0.00013404690253082663\n",
      "Step: 980, train/learning_rate: 4.8833888286026195e-05\n",
      "Step: 980, train/epoch: 0.23322227597236633\n",
      "Step: 990, train/loss: 0.0019000000320374966\n",
      "Step: 990, train/grad_norm: 0.009085254743695259\n",
      "Step: 990, train/learning_rate: 4.88219884573482e-05\n",
      "Step: 990, train/epoch: 0.23560209572315216\n",
      "Step: 1000, train/loss: 1.0090999603271484\n",
      "Step: 1000, train/grad_norm: 0.021529020741581917\n",
      "Step: 1000, train/learning_rate: 4.88100886286702e-05\n",
      "Step: 1000, train/epoch: 0.237981915473938\n",
      "Step: 1010, train/loss: 0.023900000378489494\n",
      "Step: 1010, train/grad_norm: 0.004635886754840612\n",
      "Step: 1010, train/learning_rate: 4.879819243797101e-05\n",
      "Step: 1010, train/epoch: 0.24036173522472382\n",
      "Step: 1020, train/loss: 0.005900000222027302\n",
      "Step: 1020, train/grad_norm: 0.4522283971309662\n",
      "Step: 1020, train/learning_rate: 4.8786292609293014e-05\n",
      "Step: 1020, train/epoch: 0.24274155497550964\n",
      "Step: 1030, train/loss: 0.39079999923706055\n",
      "Step: 1030, train/grad_norm: 0.002305084140971303\n",
      "Step: 1030, train/learning_rate: 4.877439278061502e-05\n",
      "Step: 1030, train/epoch: 0.24512137472629547\n",
      "Step: 1040, train/loss: 0.492900013923645\n",
      "Step: 1040, train/grad_norm: 0.015521544963121414\n",
      "Step: 1040, train/learning_rate: 4.876249295193702e-05\n",
      "Step: 1040, train/epoch: 0.2475011944770813\n",
      "Step: 1050, train/loss: 0.0008999999845400453\n",
      "Step: 1050, train/grad_norm: 0.03505837917327881\n",
      "Step: 1050, train/learning_rate: 4.875059676123783e-05\n",
      "Step: 1050, train/epoch: 0.24988101422786713\n",
      "Step: 1060, train/loss: 0.0\n",
      "Step: 1060, train/grad_norm: 0.0015528786461800337\n",
      "Step: 1060, train/learning_rate: 4.873869693255983e-05\n",
      "Step: 1060, train/epoch: 0.25226083397865295\n",
      "Step: 1070, train/loss: 0.1054999977350235\n",
      "Step: 1070, train/grad_norm: 0.13045424222946167\n",
      "Step: 1070, train/learning_rate: 4.8726797103881836e-05\n",
      "Step: 1070, train/epoch: 0.2546406388282776\n",
      "Step: 1080, train/loss: 0.1785999983549118\n",
      "Step: 1080, train/grad_norm: 0.2306462824344635\n",
      "Step: 1080, train/learning_rate: 4.871489727520384e-05\n",
      "Step: 1080, train/epoch: 0.2570204734802246\n",
      "Step: 1090, train/loss: 0.012900000438094139\n",
      "Step: 1090, train/grad_norm: 0.012537647038698196\n",
      "Step: 1090, train/learning_rate: 4.870299744652584e-05\n",
      "Step: 1090, train/epoch: 0.25940027832984924\n",
      "Step: 1100, train/loss: 0.00139999995008111\n",
      "Step: 1100, train/grad_norm: 1.7121201381087303e-05\n",
      "Step: 1100, train/learning_rate: 4.869110125582665e-05\n",
      "Step: 1100, train/epoch: 0.26178011298179626\n",
      "Step: 1110, train/loss: 0.17759999632835388\n",
      "Step: 1110, train/grad_norm: 9.305154890171252e-06\n",
      "Step: 1110, train/learning_rate: 4.8679201427148655e-05\n",
      "Step: 1110, train/epoch: 0.2641599178314209\n",
      "Step: 1120, train/loss: 0.2281000018119812\n",
      "Step: 1120, train/grad_norm: 5.632634929497726e-05\n",
      "Step: 1120, train/learning_rate: 4.866730159847066e-05\n",
      "Step: 1120, train/epoch: 0.2665397524833679\n",
      "Step: 1130, train/loss: 0.0020000000949949026\n",
      "Step: 1130, train/grad_norm: 0.007694181986153126\n",
      "Step: 1130, train/learning_rate: 4.865540176979266e-05\n",
      "Step: 1130, train/epoch: 0.26891955733299255\n",
      "Step: 1140, train/loss: 0.13429999351501465\n",
      "Step: 1140, train/grad_norm: 4.882257940153067e-07\n",
      "Step: 1140, train/learning_rate: 4.8643501941114664e-05\n",
      "Step: 1140, train/epoch: 0.2712993919849396\n",
      "Step: 1150, train/loss: 0.004399999976158142\n",
      "Step: 1150, train/grad_norm: 0.0003489663649816066\n",
      "Step: 1150, train/learning_rate: 4.8631605750415474e-05\n",
      "Step: 1150, train/epoch: 0.2736791968345642\n",
      "Step: 1160, train/loss: 0.0\n",
      "Step: 1160, train/grad_norm: 3.070959428441711e-05\n",
      "Step: 1160, train/learning_rate: 4.861970592173748e-05\n",
      "Step: 1160, train/epoch: 0.27605903148651123\n",
      "Step: 1170, train/loss: 0.23909999430179596\n",
      "Step: 1170, train/grad_norm: 1.5927621126174927\n",
      "Step: 1170, train/learning_rate: 4.860780609305948e-05\n",
      "Step: 1170, train/epoch: 0.27843883633613586\n",
      "Step: 1180, train/loss: 0.06639999896287918\n",
      "Step: 1180, train/grad_norm: 3.686495256260969e-05\n",
      "Step: 1180, train/learning_rate: 4.859590626438148e-05\n",
      "Step: 1180, train/epoch: 0.2808186709880829\n",
      "Step: 1190, train/loss: 9.999999747378752e-05\n",
      "Step: 1190, train/grad_norm: 6.185427992022596e-07\n",
      "Step: 1190, train/learning_rate: 4.8584006435703486e-05\n",
      "Step: 1190, train/epoch: 0.2831984758377075\n",
      "Step: 1200, train/loss: 0.0215000007301569\n",
      "Step: 1200, train/grad_norm: 4.700048521044664e-06\n",
      "Step: 1200, train/learning_rate: 4.8572110245004296e-05\n",
      "Step: 1200, train/epoch: 0.28557831048965454\n",
      "Step: 1210, train/loss: 0.2361000031232834\n",
      "Step: 1210, train/grad_norm: 8.948063623392954e-05\n",
      "Step: 1210, train/learning_rate: 4.85602104163263e-05\n",
      "Step: 1210, train/epoch: 0.2879581153392792\n",
      "Step: 1220, train/loss: 0.2078000009059906\n",
      "Step: 1220, train/grad_norm: 1.4939391803636681e-05\n",
      "Step: 1220, train/learning_rate: 4.85483105876483e-05\n",
      "Step: 1220, train/epoch: 0.2903379201889038\n",
      "Step: 1230, train/loss: 0.16519999504089355\n",
      "Step: 1230, train/grad_norm: 0.0019849278032779694\n",
      "Step: 1230, train/learning_rate: 4.8536410758970305e-05\n",
      "Step: 1230, train/epoch: 0.29271775484085083\n",
      "Step: 1240, train/loss: 0.06549999862909317\n",
      "Step: 1240, train/grad_norm: 2.7876332751475275e-05\n",
      "Step: 1240, train/learning_rate: 4.852451093029231e-05\n",
      "Step: 1240, train/epoch: 0.29509755969047546\n",
      "Step: 1250, train/loss: 0.0\n",
      "Step: 1250, train/grad_norm: 6.848624616395682e-05\n",
      "Step: 1250, train/learning_rate: 4.851261473959312e-05\n",
      "Step: 1250, train/epoch: 0.2974773943424225\n",
      "Step: 1260, train/loss: 0.41679999232292175\n",
      "Step: 1260, train/grad_norm: 73.78580474853516\n",
      "Step: 1260, train/learning_rate: 4.850071491091512e-05\n",
      "Step: 1260, train/epoch: 0.2998571991920471\n",
      "Step: 1270, train/loss: 0.0\n",
      "Step: 1270, train/grad_norm: 0.007628883235156536\n",
      "Step: 1270, train/learning_rate: 4.8488815082237124e-05\n",
      "Step: 1270, train/epoch: 0.30223703384399414\n",
      "Step: 1280, train/loss: 1.6503000259399414\n",
      "Step: 1280, train/grad_norm: 0.0014028942678123713\n",
      "Step: 1280, train/learning_rate: 4.847691525355913e-05\n",
      "Step: 1280, train/epoch: 0.3046168386936188\n",
      "Step: 1290, train/loss: 0.0\n",
      "Step: 1290, train/grad_norm: 0.00020754402794409543\n",
      "Step: 1290, train/learning_rate: 4.846501542488113e-05\n",
      "Step: 1290, train/epoch: 0.3069966733455658\n",
      "Step: 1300, train/loss: 0.12890000641345978\n",
      "Step: 1300, train/grad_norm: 0.014008371159434319\n",
      "Step: 1300, train/learning_rate: 4.845311923418194e-05\n",
      "Step: 1300, train/epoch: 0.30937647819519043\n",
      "Step: 1310, train/loss: 0.0\n",
      "Step: 1310, train/grad_norm: 0.006178523413836956\n",
      "Step: 1310, train/learning_rate: 4.8441219405503944e-05\n",
      "Step: 1310, train/epoch: 0.31175631284713745\n",
      "Step: 1320, train/loss: 0.17589999735355377\n",
      "Step: 1320, train/grad_norm: 8.604441745774238e-07\n",
      "Step: 1320, train/learning_rate: 4.8429319576825947e-05\n",
      "Step: 1320, train/epoch: 0.3141361176967621\n",
      "Step: 1330, train/loss: 0.0\n",
      "Step: 1330, train/grad_norm: 9.08525034901686e-05\n",
      "Step: 1330, train/learning_rate: 4.841741974814795e-05\n",
      "Step: 1330, train/epoch: 0.3165159523487091\n",
      "Step: 1340, train/loss: 0.2093999981880188\n",
      "Step: 1340, train/grad_norm: 0.00042089371709153056\n",
      "Step: 1340, train/learning_rate: 4.840551991946995e-05\n",
      "Step: 1340, train/epoch: 0.31889575719833374\n",
      "Step: 1350, train/loss: 0.06350000202655792\n",
      "Step: 1350, train/grad_norm: 0.00027761925593949854\n",
      "Step: 1350, train/learning_rate: 4.839362372877076e-05\n",
      "Step: 1350, train/epoch: 0.32127559185028076\n",
      "Step: 1360, train/loss: 0.22579999268054962\n",
      "Step: 1360, train/grad_norm: 1.607634658284951e-05\n",
      "Step: 1360, train/learning_rate: 4.8381723900092766e-05\n",
      "Step: 1360, train/epoch: 0.3236553966999054\n",
      "Step: 1370, train/loss: 0.005900000222027302\n",
      "Step: 1370, train/grad_norm: 4.056545367348008e-05\n",
      "Step: 1370, train/learning_rate: 4.836982407141477e-05\n",
      "Step: 1370, train/epoch: 0.3260352313518524\n",
      "Step: 1380, train/loss: 0.0\n",
      "Step: 1380, train/grad_norm: 2.371729806327494e-06\n",
      "Step: 1380, train/learning_rate: 4.835792424273677e-05\n",
      "Step: 1380, train/epoch: 0.32841503620147705\n",
      "Reading events from file: ./praxis-gemma-7b-small-finetune/logs/events.out.tfevents.1716703546.hephaestus.3674.0\n",
      "Step: 10, train/loss: 6.215400218963623\n",
      "Step: 10, train/grad_norm: 186.15252685546875\n",
      "Step: 10, train/learning_rate: 4.998810254619457e-05\n",
      "Step: 10, train/epoch: 0.0023798190522938967\n",
      "Step: 20, train/loss: 6.783599853515625\n",
      "Step: 20, train/grad_norm: 172.22877502441406\n",
      "Step: 20, train/learning_rate: 4.997620271751657e-05\n",
      "Step: 20, train/epoch: 0.004759638104587793\n",
      "Step: 30, train/loss: 5.689199924468994\n",
      "Step: 30, train/grad_norm: 395.4697265625\n",
      "Step: 30, train/learning_rate: 4.9964302888838574e-05\n",
      "Step: 30, train/epoch: 0.007139457389712334\n",
      "Step: 40, train/loss: 2.878200054168701\n",
      "Step: 40, train/grad_norm: 378.0408630371094\n",
      "Step: 40, train/learning_rate: 4.995240306016058e-05\n",
      "Step: 40, train/epoch: 0.009519276209175587\n",
      "Step: 50, train/loss: 3.275399923324585\n",
      "Step: 50, train/grad_norm: 179.95233154296875\n",
      "Step: 50, train/learning_rate: 4.994050323148258e-05\n",
      "Step: 50, train/epoch: 0.011899095959961414\n",
      "Step: 60, train/loss: 1.113700032234192\n",
      "Step: 60, train/grad_norm: 167.3417205810547\n",
      "Step: 60, train/learning_rate: 4.992860704078339e-05\n",
      "Step: 60, train/epoch: 0.014278914779424667\n",
      "Step: 70, train/loss: 2.1928999423980713\n",
      "Step: 70, train/grad_norm: 123.14849090576172\n",
      "Step: 70, train/learning_rate: 4.9916707212105393e-05\n",
      "Step: 70, train/epoch: 0.016658734530210495\n",
      "Step: 80, train/loss: 0.21529999375343323\n",
      "Step: 80, train/grad_norm: 11.299012184143066\n",
      "Step: 80, train/learning_rate: 4.9904807383427396e-05\n",
      "Step: 80, train/epoch: 0.019038552418351173\n",
      "Step: 90, train/loss: 0.3273000121116638\n",
      "Step: 90, train/grad_norm: 0.6285173892974854\n",
      "Step: 90, train/learning_rate: 4.98929075547494e-05\n",
      "Step: 90, train/epoch: 0.021418372169137\n",
      "Step: 100, train/loss: 2.512500047683716\n",
      "Step: 100, train/grad_norm: 0.3079034388065338\n",
      "Step: 100, train/learning_rate: 4.98810077260714e-05\n",
      "Step: 100, train/epoch: 0.02379819191992283\n",
      "Step: 110, train/loss: 2.77810001373291\n",
      "Step: 110, train/grad_norm: 181.45936584472656\n",
      "Step: 110, train/learning_rate: 4.986911153537221e-05\n",
      "Step: 110, train/epoch: 0.026178009808063507\n",
      "Step: 120, train/loss: 3.677799940109253\n",
      "Step: 120, train/grad_norm: 5605.34619140625\n",
      "Step: 120, train/learning_rate: 4.9857211706694216e-05\n",
      "Step: 120, train/epoch: 0.028557829558849335\n",
      "Step: 130, train/loss: 3.9077999591827393\n",
      "Step: 130, train/grad_norm: 297.99810791015625\n",
      "Step: 130, train/learning_rate: 4.984531187801622e-05\n",
      "Step: 130, train/epoch: 0.030937649309635162\n",
      "Step: 140, train/loss: 1.1194000244140625\n",
      "Step: 140, train/grad_norm: 151.96424865722656\n",
      "Step: 140, train/learning_rate: 4.983341204933822e-05\n",
      "Step: 140, train/epoch: 0.03331746906042099\n",
      "Step: 150, train/loss: 0.11879999935626984\n",
      "Step: 150, train/grad_norm: 48.526580810546875\n",
      "Step: 150, train/learning_rate: 4.9821512220660225e-05\n",
      "Step: 150, train/epoch: 0.03569728881120682\n",
      "Step: 160, train/loss: 0.11230000108480453\n",
      "Step: 160, train/grad_norm: 4.238967812852934e-05\n",
      "Step: 160, train/learning_rate: 4.9809616029961035e-05\n",
      "Step: 160, train/epoch: 0.03807710483670235\n",
      "Step: 170, train/loss: 0.4490000009536743\n",
      "Step: 170, train/grad_norm: 1.8706100490817335e-06\n",
      "Step: 170, train/learning_rate: 4.979771620128304e-05\n",
      "Step: 170, train/epoch: 0.040456924587488174\n",
      "Step: 180, train/loss: 0.34950000047683716\n",
      "Step: 180, train/grad_norm: 4.107353210449219\n",
      "Step: 180, train/learning_rate: 4.978581637260504e-05\n",
      "Step: 180, train/epoch: 0.042836744338274\n",
      "Step: 190, train/loss: 0.0008999999845400453\n",
      "Step: 190, train/grad_norm: 0.0002998504787683487\n",
      "Step: 190, train/learning_rate: 4.9773916543927044e-05\n",
      "Step: 190, train/epoch: 0.04521656408905983\n",
      "Step: 200, train/loss: 0.0\n",
      "Step: 200, train/grad_norm: 8.093897108096826e-09\n",
      "Step: 200, train/learning_rate: 4.976201671524905e-05\n",
      "Step: 200, train/epoch: 0.04759638383984566\n",
      "Step: 210, train/loss: 0.0\n",
      "Step: 210, train/grad_norm: 1.5382041965494864e-05\n",
      "Step: 210, train/learning_rate: 4.975012052454986e-05\n",
      "Step: 210, train/epoch: 0.049976203590631485\n",
      "Step: 220, train/loss: 0.006099999882280827\n",
      "Step: 220, train/grad_norm: 191.76361083984375\n",
      "Step: 220, train/learning_rate: 4.973822069587186e-05\n",
      "Step: 220, train/epoch: 0.052356019616127014\n",
      "Step: 230, train/loss: 1.8632999658584595\n",
      "Step: 230, train/grad_norm: 3.4420245356159285e-05\n",
      "Step: 230, train/learning_rate: 4.972632086719386e-05\n",
      "Step: 230, train/epoch: 0.05473583936691284\n",
      "Step: 240, train/loss: 0.7519000172615051\n",
      "Step: 240, train/grad_norm: 0.006733279675245285\n",
      "Step: 240, train/learning_rate: 4.9714421038515866e-05\n",
      "Step: 240, train/epoch: 0.05711565911769867\n",
      "Step: 250, train/loss: 0.0012000000569969416\n",
      "Step: 250, train/grad_norm: 0.016240084543824196\n",
      "Step: 250, train/learning_rate: 4.970252120983787e-05\n",
      "Step: 250, train/epoch: 0.0594954788684845\n",
      "Step: 260, train/loss: 0.055399999022483826\n",
      "Step: 260, train/grad_norm: 8.300638910441194e-06\n",
      "Step: 260, train/learning_rate: 4.969062501913868e-05\n",
      "Step: 260, train/epoch: 0.061875298619270325\n",
      "Step: 270, train/loss: 0.1242000013589859\n",
      "Step: 270, train/grad_norm: 1.5212458492896985e-06\n",
      "Step: 270, train/learning_rate: 4.967872519046068e-05\n",
      "Step: 270, train/epoch: 0.06425511837005615\n",
      "Step: 280, train/loss: 0.07150000333786011\n",
      "Step: 280, train/grad_norm: 1.3037994222031557e-06\n",
      "Step: 280, train/learning_rate: 4.9666825361782685e-05\n",
      "Step: 280, train/epoch: 0.06663493812084198\n",
      "Step: 290, train/loss: 0.18799999356269836\n",
      "Step: 290, train/grad_norm: 0.0008597264531999826\n",
      "Step: 290, train/learning_rate: 4.965492553310469e-05\n",
      "Step: 290, train/epoch: 0.06901475787162781\n",
      "Step: 300, train/loss: 0.14069999754428864\n",
      "Step: 300, train/grad_norm: 34.8262825012207\n",
      "Step: 300, train/learning_rate: 4.964302570442669e-05\n",
      "Step: 300, train/epoch: 0.07139457762241364\n",
      "Step: 310, train/loss: 0.0008999999845400453\n",
      "Step: 310, train/grad_norm: 2.1713643945986405e-05\n",
      "Step: 310, train/learning_rate: 4.96311295137275e-05\n",
      "Step: 310, train/epoch: 0.07377438992261887\n",
      "Step: 320, train/loss: 0.003700000001117587\n",
      "Step: 320, train/grad_norm: 3.477209986613161e-08\n",
      "Step: 320, train/learning_rate: 4.9619229685049504e-05\n",
      "Step: 320, train/epoch: 0.0761542096734047\n",
      "Step: 330, train/loss: 0.005900000222027302\n",
      "Step: 330, train/grad_norm: 2.4634977080495446e-07\n",
      "Step: 330, train/learning_rate: 4.960732985637151e-05\n",
      "Step: 330, train/epoch: 0.07853402942419052\n",
      "Step: 340, train/loss: 0.5687999725341797\n",
      "Step: 340, train/grad_norm: 3.6951038850929763e-07\n",
      "Step: 340, train/learning_rate: 4.959543002769351e-05\n",
      "Step: 340, train/epoch: 0.08091384917497635\n",
      "Step: 350, train/loss: 0.0\n",
      "Step: 350, train/grad_norm: 2.285561458847951e-05\n",
      "Step: 350, train/learning_rate: 4.958353019901551e-05\n",
      "Step: 350, train/epoch: 0.08329366892576218\n",
      "Step: 360, train/loss: 0.0\n",
      "Step: 360, train/grad_norm: 3.1215607876333706e-10\n",
      "Step: 360, train/learning_rate: 4.957163400831632e-05\n",
      "Step: 360, train/epoch: 0.085673488676548\n",
      "Step: 370, train/loss: 0.0\n",
      "Step: 370, train/grad_norm: 9.928356803357019e-08\n",
      "Step: 370, train/learning_rate: 4.9559734179638326e-05\n",
      "Step: 370, train/epoch: 0.08805330842733383\n",
      "Step: 380, train/loss: 0.18050000071525574\n",
      "Step: 380, train/grad_norm: 251.71861267089844\n",
      "Step: 380, train/learning_rate: 4.954783435096033e-05\n",
      "Step: 380, train/epoch: 0.09043312817811966\n",
      "Step: 390, train/loss: 0.24779999256134033\n",
      "Step: 390, train/grad_norm: 384.20257568359375\n",
      "Step: 390, train/learning_rate: 4.953593452228233e-05\n",
      "Step: 390, train/epoch: 0.09281294792890549\n",
      "Step: 400, train/loss: 0.018799999728798866\n",
      "Step: 400, train/grad_norm: 1.6592950971561216e-11\n",
      "Step: 400, train/learning_rate: 4.9524034693604335e-05\n",
      "Step: 400, train/epoch: 0.09519276767969131\n",
      "Step: 410, train/loss: 0.011900000274181366\n",
      "Step: 410, train/grad_norm: 1171.960693359375\n",
      "Step: 410, train/learning_rate: 4.9512138502905145e-05\n",
      "Step: 410, train/epoch: 0.09757258743047714\n",
      "Step: 420, train/loss: 0.0\n",
      "Step: 420, train/grad_norm: 3.4745757382381726e-09\n",
      "Step: 420, train/learning_rate: 4.950023867422715e-05\n",
      "Step: 420, train/epoch: 0.09995240718126297\n",
      "Step: 430, train/loss: 0.32249999046325684\n",
      "Step: 430, train/grad_norm: 4.6458167868301814e-10\n",
      "Step: 430, train/learning_rate: 4.948833884554915e-05\n",
      "Step: 430, train/epoch: 0.1023322194814682\n",
      "Step: 440, train/loss: 0.13359999656677246\n",
      "Step: 440, train/grad_norm: 1.0187427079699773e-07\n",
      "Step: 440, train/learning_rate: 4.9476439016871154e-05\n",
      "Step: 440, train/epoch: 0.10471203923225403\n",
      "Step: 450, train/loss: 0.03819999843835831\n",
      "Step: 450, train/grad_norm: 1.6036235450656022e-08\n",
      "Step: 450, train/learning_rate: 4.946453918819316e-05\n",
      "Step: 450, train/epoch: 0.10709185898303986\n",
      "Step: 460, train/loss: 0.006000000052154064\n",
      "Step: 460, train/grad_norm: 0.2797936499118805\n",
      "Step: 460, train/learning_rate: 4.945264299749397e-05\n",
      "Step: 460, train/epoch: 0.10947167873382568\n",
      "Step: 470, train/loss: 0.0008999999845400453\n",
      "Step: 470, train/grad_norm: 0.0002659879974089563\n",
      "Step: 470, train/learning_rate: 4.944074316881597e-05\n",
      "Step: 470, train/epoch: 0.11185149848461151\n",
      "Step: 480, train/loss: 0.7125999927520752\n",
      "Step: 480, train/grad_norm: 1.0184325605067102e-09\n",
      "Step: 480, train/learning_rate: 4.9428843340137973e-05\n",
      "Step: 480, train/epoch: 0.11423131823539734\n",
      "Step: 490, train/loss: 0.5453000068664551\n",
      "Step: 490, train/grad_norm: 0.00017047228175215423\n",
      "Step: 490, train/learning_rate: 4.9416943511459976e-05\n",
      "Step: 490, train/epoch: 0.11661113798618317\n",
      "Step: 500, train/loss: 0.0\n",
      "Step: 500, train/grad_norm: 1.662520480749663e-05\n",
      "Step: 500, train/learning_rate: 4.940504368278198e-05\n",
      "Step: 500, train/epoch: 0.118990957736969\n",
      "Step: 510, train/loss: 0.2750000059604645\n",
      "Step: 510, train/grad_norm: 0.0007643171702511609\n",
      "Step: 510, train/learning_rate: 4.939314749208279e-05\n",
      "Step: 510, train/epoch: 0.12137077748775482\n",
      "Step: 520, train/loss: 0.04569999873638153\n",
      "Step: 520, train/grad_norm: 1.663946022745222e-05\n",
      "Step: 520, train/learning_rate: 4.938124766340479e-05\n",
      "Step: 520, train/epoch: 0.12375059723854065\n",
      "Step: 530, train/loss: 0.0\n",
      "Step: 530, train/grad_norm: 2.824791044986341e-05\n",
      "Step: 530, train/learning_rate: 4.9369347834726796e-05\n",
      "Step: 530, train/epoch: 0.12613041698932648\n",
      "Step: 540, train/loss: 0.10000000149011612\n",
      "Step: 540, train/grad_norm: 8.97610803463067e-08\n",
      "Step: 540, train/learning_rate: 4.93574480060488e-05\n",
      "Step: 540, train/epoch: 0.1285102367401123\n",
      "Step: 550, train/loss: 0.0\n",
      "Step: 550, train/grad_norm: 2.1862449628429204e-09\n",
      "Step: 550, train/learning_rate: 4.93455481773708e-05\n",
      "Step: 550, train/epoch: 0.13089005649089813\n",
      "Step: 560, train/loss: 0.0\n",
      "Step: 560, train/grad_norm: 1.624510387321365e-10\n",
      "Step: 560, train/learning_rate: 4.933365198667161e-05\n",
      "Step: 560, train/epoch: 0.13326987624168396\n",
      "Step: 570, train/loss: 0.0\n",
      "Step: 570, train/grad_norm: 1.2551101463031955e-05\n",
      "Step: 570, train/learning_rate: 4.9321752157993615e-05\n",
      "Step: 570, train/epoch: 0.1356496959924698\n",
      "Step: 580, train/loss: 0.0\n",
      "Step: 580, train/grad_norm: 6.57853160834776e-10\n",
      "Step: 580, train/learning_rate: 4.930985232931562e-05\n",
      "Step: 580, train/epoch: 0.13802951574325562\n",
      "Step: 590, train/loss: 0.23280000686645508\n",
      "Step: 590, train/grad_norm: 3.0528966021847737e-08\n",
      "Step: 590, train/learning_rate: 4.929795250063762e-05\n",
      "Step: 590, train/epoch: 0.14040933549404144\n",
      "Step: 600, train/loss: 0.21060000360012054\n",
      "Step: 600, train/grad_norm: 44.84658432006836\n",
      "Step: 600, train/learning_rate: 4.9286052671959624e-05\n",
      "Step: 600, train/epoch: 0.14278915524482727\n",
      "Step: 610, train/loss: 0.0\n",
      "Step: 610, train/grad_norm: 1.2766219697368797e-05\n",
      "Step: 610, train/learning_rate: 4.9274156481260434e-05\n",
      "Step: 610, train/epoch: 0.1451689600944519\n",
      "Step: 620, train/loss: 0.58160001039505\n",
      "Step: 620, train/grad_norm: 2.7143397331237793\n",
      "Step: 620, train/learning_rate: 4.926225665258244e-05\n",
      "Step: 620, train/epoch: 0.14754877984523773\n",
      "Step: 630, train/loss: 0.1981000006198883\n",
      "Step: 630, train/grad_norm: 0.04544607922434807\n",
      "Step: 630, train/learning_rate: 4.925035682390444e-05\n",
      "Step: 630, train/epoch: 0.14992859959602356\n",
      "Step: 640, train/loss: 0.04580000042915344\n",
      "Step: 640, train/grad_norm: 0.0022576935589313507\n",
      "Step: 640, train/learning_rate: 4.923845699522644e-05\n",
      "Step: 640, train/epoch: 0.1523084193468094\n",
      "Step: 650, train/loss: 0.27730000019073486\n",
      "Step: 650, train/grad_norm: 402.31988525390625\n",
      "Step: 650, train/learning_rate: 4.9226557166548446e-05\n",
      "Step: 650, train/epoch: 0.15468823909759521\n",
      "Step: 660, train/loss: 0.2718999981880188\n",
      "Step: 660, train/grad_norm: 6.965517241042107e-05\n",
      "Step: 660, train/learning_rate: 4.9214660975849256e-05\n",
      "Step: 660, train/epoch: 0.15706805884838104\n",
      "Step: 670, train/loss: 0.22190000116825104\n",
      "Step: 670, train/grad_norm: 6.10893766861409e-05\n",
      "Step: 670, train/learning_rate: 4.920276114717126e-05\n",
      "Step: 670, train/epoch: 0.15944787859916687\n",
      "Step: 680, train/loss: 0.12970000505447388\n",
      "Step: 680, train/grad_norm: 0.00010527849372010678\n",
      "Step: 680, train/learning_rate: 4.919086131849326e-05\n",
      "Step: 680, train/epoch: 0.1618276983499527\n",
      "Step: 690, train/loss: 0.09809999912977219\n",
      "Step: 690, train/grad_norm: 6.273493636399508e-05\n",
      "Step: 690, train/learning_rate: 4.9178961489815265e-05\n",
      "Step: 690, train/epoch: 0.16420751810073853\n",
      "Step: 700, train/loss: 0.1808999925851822\n",
      "Step: 700, train/grad_norm: 0.0008071415941230953\n",
      "Step: 700, train/learning_rate: 4.916706166113727e-05\n",
      "Step: 700, train/epoch: 0.16658733785152435\n",
      "Step: 710, train/loss: 0.7799999713897705\n",
      "Step: 710, train/grad_norm: 0.027952060103416443\n",
      "Step: 710, train/learning_rate: 4.915516547043808e-05\n",
      "Step: 710, train/epoch: 0.16896715760231018\n",
      "Step: 720, train/loss: 0.07720000296831131\n",
      "Step: 720, train/grad_norm: 0.07119424641132355\n",
      "Step: 720, train/learning_rate: 4.914326564176008e-05\n",
      "Step: 720, train/epoch: 0.171346977353096\n",
      "Step: 730, train/loss: 0.003100000089034438\n",
      "Step: 730, train/grad_norm: 1.2296859495108947e-05\n",
      "Step: 730, train/learning_rate: 4.9131365813082084e-05\n",
      "Step: 730, train/epoch: 0.17372679710388184\n",
      "Step: 740, train/loss: 0.5130000114440918\n",
      "Step: 740, train/grad_norm: 123.2873306274414\n",
      "Step: 740, train/learning_rate: 4.911946598440409e-05\n",
      "Step: 740, train/epoch: 0.17610661685466766\n",
      "Step: 750, train/loss: 0.6582000255584717\n",
      "Step: 750, train/grad_norm: 0.8512892723083496\n",
      "Step: 750, train/learning_rate: 4.910756615572609e-05\n",
      "Step: 750, train/epoch: 0.1784864366054535\n",
      "Step: 760, train/loss: 0.003000000026077032\n",
      "Step: 760, train/grad_norm: 0.01666216365993023\n",
      "Step: 760, train/learning_rate: 4.90956699650269e-05\n",
      "Step: 760, train/epoch: 0.18086625635623932\n",
      "Step: 770, train/loss: 0.13130000233650208\n",
      "Step: 770, train/grad_norm: 0.0003945354255847633\n",
      "Step: 770, train/learning_rate: 4.90837701363489e-05\n",
      "Step: 770, train/epoch: 0.18324607610702515\n",
      "Step: 780, train/loss: 0.06239999830722809\n",
      "Step: 780, train/grad_norm: 0.17841874063014984\n",
      "Step: 780, train/learning_rate: 4.9071870307670906e-05\n",
      "Step: 780, train/epoch: 0.18562589585781097\n",
      "Step: 790, train/loss: 0.1257999986410141\n",
      "Step: 790, train/grad_norm: 0.0028723047580569983\n",
      "Step: 790, train/learning_rate: 4.905997047899291e-05\n",
      "Step: 790, train/epoch: 0.1880057156085968\n",
      "Step: 800, train/loss: 0.00019999999494757503\n",
      "Step: 800, train/grad_norm: 0.0010370088275521994\n",
      "Step: 800, train/learning_rate: 4.904807065031491e-05\n",
      "Step: 800, train/epoch: 0.19038553535938263\n",
      "Step: 810, train/loss: 0.1657000035047531\n",
      "Step: 810, train/grad_norm: 0.009013342671096325\n",
      "Step: 810, train/learning_rate: 4.903617445961572e-05\n",
      "Step: 810, train/epoch: 0.19276535511016846\n",
      "Step: 820, train/loss: 0.40880000591278076\n",
      "Step: 820, train/grad_norm: 0.001498634461313486\n",
      "Step: 820, train/learning_rate: 4.9024274630937725e-05\n",
      "Step: 820, train/epoch: 0.19514517486095428\n",
      "Step: 830, train/loss: 0.01140000019222498\n",
      "Step: 830, train/grad_norm: 1.689565896987915\n",
      "Step: 830, train/learning_rate: 4.901237480225973e-05\n",
      "Step: 830, train/epoch: 0.1975249946117401\n",
      "Step: 840, train/loss: 9.999999747378752e-05\n",
      "Step: 840, train/grad_norm: 0.026153670623898506\n",
      "Step: 840, train/learning_rate: 4.900047497358173e-05\n",
      "Step: 840, train/epoch: 0.19990481436252594\n",
      "Step: 850, train/loss: 0.0\n",
      "Step: 850, train/grad_norm: 0.0012393207289278507\n",
      "Step: 850, train/learning_rate: 4.8988575144903734e-05\n",
      "Step: 850, train/epoch: 0.20228461921215057\n",
      "Step: 860, train/loss: 0.260699987411499\n",
      "Step: 860, train/grad_norm: 0.11609677225351334\n",
      "Step: 860, train/learning_rate: 4.8976678954204544e-05\n",
      "Step: 860, train/epoch: 0.2046644389629364\n",
      "Step: 870, train/loss: 0.3149000108242035\n",
      "Step: 870, train/grad_norm: 0.012193911708891392\n",
      "Step: 870, train/learning_rate: 4.896477912552655e-05\n",
      "Step: 870, train/epoch: 0.20704425871372223\n",
      "Step: 880, train/loss: 0.06199999898672104\n",
      "Step: 880, train/grad_norm: 0.010572971776127815\n",
      "Step: 880, train/learning_rate: 4.895287929684855e-05\n",
      "Step: 880, train/epoch: 0.20942407846450806\n",
      "Step: 890, train/loss: 0.026000000536441803\n",
      "Step: 890, train/grad_norm: 0.013312891125679016\n",
      "Step: 890, train/learning_rate: 4.8940979468170553e-05\n",
      "Step: 890, train/epoch: 0.21180389821529388\n",
      "Step: 900, train/loss: 0.0027000000700354576\n",
      "Step: 900, train/grad_norm: 5.582961080108362e-07\n",
      "Step: 900, train/learning_rate: 4.8929079639492556e-05\n",
      "Step: 900, train/epoch: 0.2141837179660797\n",
      "Step: 910, train/loss: 0.00039999998989515007\n",
      "Step: 910, train/grad_norm: 0.0001469279231969267\n",
      "Step: 910, train/learning_rate: 4.8917183448793367e-05\n",
      "Step: 910, train/epoch: 0.21656353771686554\n",
      "Step: 920, train/loss: 0.23280000686645508\n",
      "Step: 920, train/grad_norm: 2.163300472091123e-08\n",
      "Step: 920, train/learning_rate: 4.890528362011537e-05\n",
      "Step: 920, train/epoch: 0.21894335746765137\n",
      "Step: 930, train/loss: 0.23800000548362732\n",
      "Step: 930, train/grad_norm: 0.716806948184967\n",
      "Step: 930, train/learning_rate: 4.889338379143737e-05\n",
      "Step: 930, train/epoch: 0.2213231772184372\n",
      "Step: 940, train/loss: 0.14159999787807465\n",
      "Step: 940, train/grad_norm: 92.19339752197266\n",
      "Step: 940, train/learning_rate: 4.8881483962759376e-05\n",
      "Step: 940, train/epoch: 0.22370299696922302\n",
      "Step: 950, train/loss: 0.0\n",
      "Step: 950, train/grad_norm: 0.0035830228589475155\n",
      "Step: 950, train/learning_rate: 4.886958413408138e-05\n",
      "Step: 950, train/epoch: 0.22608281672000885\n",
      "Step: 960, train/loss: 0.06430000066757202\n",
      "Step: 960, train/grad_norm: 0.0010514783207327127\n",
      "Step: 960, train/learning_rate: 4.885768794338219e-05\n",
      "Step: 960, train/epoch: 0.22846263647079468\n",
      "Step: 970, train/loss: 0.4991999864578247\n",
      "Step: 970, train/grad_norm: 4.662140054279007e-05\n",
      "Step: 970, train/learning_rate: 4.884578811470419e-05\n",
      "Step: 970, train/epoch: 0.2308424562215805\n",
      "Step: 980, train/loss: 0.2345999926328659\n",
      "Step: 980, train/grad_norm: 0.00028528954135254025\n",
      "Step: 980, train/learning_rate: 4.8833888286026195e-05\n",
      "Step: 980, train/epoch: 0.23322227597236633\n",
      "Step: 990, train/loss: 0.0632999986410141\n",
      "Step: 990, train/grad_norm: 4.141749858856201\n",
      "Step: 990, train/learning_rate: 4.88219884573482e-05\n",
      "Step: 990, train/epoch: 0.23560209572315216\n",
      "Step: 1000, train/loss: 0.15780000388622284\n",
      "Step: 1000, train/grad_norm: 5.620106047210527e-10\n",
      "Step: 1000, train/learning_rate: 4.88100886286702e-05\n",
      "Step: 1000, train/epoch: 0.237981915473938\n",
      "Step: 1010, train/loss: 0.03280000016093254\n",
      "Step: 1010, train/grad_norm: 8.211696695070714e-06\n",
      "Step: 1010, train/learning_rate: 4.879819243797101e-05\n",
      "Step: 1010, train/epoch: 0.24036173522472382\n",
      "Step: 1020, train/loss: 0.0003000000142492354\n",
      "Step: 1020, train/grad_norm: 0.0002497167733963579\n",
      "Step: 1020, train/learning_rate: 4.8786292609293014e-05\n",
      "Step: 1020, train/epoch: 0.24274155497550964\n",
      "Step: 1030, train/loss: 0.8187999725341797\n",
      "Step: 1030, train/grad_norm: 7.796541745541674e-10\n",
      "Step: 1030, train/learning_rate: 4.877439278061502e-05\n",
      "Step: 1030, train/epoch: 0.24512137472629547\n",
      "Step: 1040, train/loss: 0.0005000000237487257\n",
      "Step: 1040, train/grad_norm: 1.6581481759203598e-05\n",
      "Step: 1040, train/learning_rate: 4.876249295193702e-05\n",
      "Step: 1040, train/epoch: 0.2475011944770813\n",
      "Step: 1050, train/loss: 0.13519999384880066\n",
      "Step: 1050, train/grad_norm: 3.8160200347192585e-05\n",
      "Step: 1050, train/learning_rate: 4.875059676123783e-05\n",
      "Step: 1050, train/epoch: 0.24988101422786713\n",
      "Step: 1060, train/loss: 0.02969999983906746\n",
      "Step: 1060, train/grad_norm: 2.0682952595052484e-07\n",
      "Step: 1060, train/learning_rate: 4.873869693255983e-05\n",
      "Step: 1060, train/epoch: 0.25226083397865295\n",
      "Step: 1070, train/loss: 0.004900000058114529\n",
      "Step: 1070, train/grad_norm: 8.765770758145663e-07\n",
      "Step: 1070, train/learning_rate: 4.8726797103881836e-05\n",
      "Step: 1070, train/epoch: 0.2546406388282776\n",
      "Step: 1080, train/loss: 0.0\n",
      "Step: 1080, train/grad_norm: 0.0005557678523473442\n",
      "Step: 1080, train/learning_rate: 4.871489727520384e-05\n",
      "Step: 1080, train/epoch: 0.2570204734802246\n",
      "Step: 1090, train/loss: 0.26489999890327454\n",
      "Step: 1090, train/grad_norm: 196.70335388183594\n",
      "Step: 1090, train/learning_rate: 4.870299744652584e-05\n",
      "Step: 1090, train/epoch: 0.25940027832984924\n",
      "Step: 1100, train/loss: 0.28439998626708984\n",
      "Step: 1100, train/grad_norm: 7.945519797658562e-09\n",
      "Step: 1100, train/learning_rate: 4.869110125582665e-05\n",
      "Step: 1100, train/epoch: 0.26178011298179626\n",
      "Step: 1110, train/loss: 0.392300009727478\n",
      "Step: 1110, train/grad_norm: 1.5766147498652572e-06\n",
      "Step: 1110, train/learning_rate: 4.8679201427148655e-05\n",
      "Step: 1110, train/epoch: 0.2641599178314209\n",
      "Step: 1120, train/loss: 0.11640000343322754\n",
      "Step: 1120, train/grad_norm: 0.004304376896470785\n",
      "Step: 1120, train/learning_rate: 4.866730159847066e-05\n",
      "Step: 1120, train/epoch: 0.2665397524833679\n",
      "Step: 1130, train/loss: 0.09440000355243683\n",
      "Step: 1130, train/grad_norm: 0.10113132745027542\n",
      "Step: 1130, train/learning_rate: 4.865540176979266e-05\n",
      "Step: 1130, train/epoch: 0.26891955733299255\n",
      "Step: 1140, train/loss: 0.21070000529289246\n",
      "Step: 1140, train/grad_norm: 0.0006760901305824518\n",
      "Step: 1140, train/learning_rate: 4.8643501941114664e-05\n",
      "Step: 1140, train/epoch: 0.2712993919849396\n",
      "Step: 1150, train/loss: 0.00019999999494757503\n",
      "Step: 1150, train/grad_norm: 0.0005598641000688076\n",
      "Step: 1150, train/learning_rate: 4.8631605750415474e-05\n",
      "Step: 1150, train/epoch: 0.2736791968345642\n",
      "Step: 1160, train/loss: 0.08070000261068344\n",
      "Step: 1160, train/grad_norm: 1.2257948583282996e-05\n",
      "Step: 1160, train/learning_rate: 4.861970592173748e-05\n",
      "Step: 1160, train/epoch: 0.27605903148651123\n",
      "Step: 1170, train/loss: 0.1703999936580658\n",
      "Step: 1170, train/grad_norm: 104.99011993408203\n",
      "Step: 1170, train/learning_rate: 4.860780609305948e-05\n",
      "Step: 1170, train/epoch: 0.27843883633613586\n",
      "Step: 1180, train/loss: 0.33469998836517334\n",
      "Step: 1180, train/grad_norm: 7.922475560917519e-06\n",
      "Step: 1180, train/learning_rate: 4.859590626438148e-05\n",
      "Step: 1180, train/epoch: 0.2808186709880829\n",
      "Step: 1190, train/loss: 0.0\n",
      "Step: 1190, train/grad_norm: 2.610794126667315e-06\n",
      "Step: 1190, train/learning_rate: 4.8584006435703486e-05\n",
      "Step: 1190, train/epoch: 0.2831984758377075\n",
      "Step: 1200, train/loss: 0.06120000034570694\n",
      "Step: 1200, train/grad_norm: 8.915823855204508e-05\n",
      "Step: 1200, train/learning_rate: 4.8572110245004296e-05\n",
      "Step: 1200, train/epoch: 0.28557831048965454\n",
      "Step: 1210, train/loss: 0.0003000000142492354\n",
      "Step: 1210, train/grad_norm: 0.0003209046262782067\n",
      "Step: 1210, train/learning_rate: 4.85602104163263e-05\n",
      "Step: 1210, train/epoch: 0.2879581153392792\n",
      "Step: 1220, train/loss: 0.10779999941587448\n",
      "Step: 1220, train/grad_norm: 9.07477515283972e-05\n",
      "Step: 1220, train/learning_rate: 4.85483105876483e-05\n",
      "Step: 1220, train/epoch: 0.2903379201889038\n",
      "Step: 1230, train/loss: 0.11949999630451202\n",
      "Step: 1230, train/grad_norm: 237.02334594726562\n",
      "Step: 1230, train/learning_rate: 4.8536410758970305e-05\n",
      "Step: 1230, train/epoch: 0.29271775484085083\n",
      "Step: 1240, train/loss: 0.0\n",
      "Step: 1240, train/grad_norm: 2.2879326024849433e-06\n",
      "Step: 1240, train/learning_rate: 4.852451093029231e-05\n",
      "Step: 1240, train/epoch: 0.29509755969047546\n",
      "Step: 1250, train/loss: 0.001500000013038516\n",
      "Step: 1250, train/grad_norm: 1.0099634550897463e-07\n",
      "Step: 1250, train/learning_rate: 4.851261473959312e-05\n",
      "Step: 1250, train/epoch: 0.2974773943424225\n",
      "Step: 1260, train/loss: 0.20819999277591705\n",
      "Step: 1260, train/grad_norm: 0.37292003631591797\n",
      "Step: 1260, train/learning_rate: 4.850071491091512e-05\n",
      "Step: 1260, train/epoch: 0.2998571991920471\n",
      "Step: 1270, train/loss: 0.0012000000569969416\n",
      "Step: 1270, train/grad_norm: 0.0011382668744772673\n",
      "Step: 1270, train/learning_rate: 4.8488815082237124e-05\n",
      "Step: 1270, train/epoch: 0.30223703384399414\n",
      "Step: 1280, train/loss: 0.0003000000142492354\n",
      "Step: 1280, train/grad_norm: 0.025101523846387863\n",
      "Step: 1280, train/learning_rate: 4.847691525355913e-05\n",
      "Step: 1280, train/epoch: 0.3046168386936188\n",
      "Step: 1290, train/loss: 0.0\n",
      "Step: 1290, train/grad_norm: 8.09691480530006e-11\n",
      "Step: 1290, train/learning_rate: 4.846501542488113e-05\n",
      "Step: 1290, train/epoch: 0.3069966733455658\n",
      "Step: 1300, train/loss: 0.002199999988079071\n",
      "Step: 1300, train/grad_norm: 6.48054774501361e-05\n",
      "Step: 1300, train/learning_rate: 4.845311923418194e-05\n",
      "Step: 1300, train/epoch: 0.30937647819519043\n",
      "Step: 1310, train/loss: 0.00019999999494757503\n",
      "Step: 1310, train/grad_norm: 1.3405764320850722e-07\n",
      "Step: 1310, train/learning_rate: 4.8441219405503944e-05\n",
      "Step: 1310, train/epoch: 0.31175631284713745\n",
      "Step: 1320, train/loss: 0.17339999973773956\n",
      "Step: 1320, train/grad_norm: 2.932341374162206e-07\n",
      "Step: 1320, train/learning_rate: 4.8429319576825947e-05\n",
      "Step: 1320, train/epoch: 0.3141361176967621\n",
      "Step: 1330, train/loss: 0.08910000324249268\n",
      "Step: 1330, train/grad_norm: 3.988227490481222e-07\n",
      "Step: 1330, train/learning_rate: 4.841741974814795e-05\n",
      "Step: 1330, train/epoch: 0.3165159523487091\n",
      "Step: 1340, train/loss: 0.0\n",
      "Step: 1340, train/grad_norm: 4.0849237848306075e-05\n",
      "Step: 1340, train/learning_rate: 4.840551991946995e-05\n",
      "Step: 1340, train/epoch: 0.31889575719833374\n",
      "Step: 1350, train/loss: 0.7718999981880188\n",
      "Step: 1350, train/grad_norm: 0.00011556889512576163\n",
      "Step: 1350, train/learning_rate: 4.839362372877076e-05\n",
      "Step: 1350, train/epoch: 0.32127559185028076\n",
      "Step: 1360, train/loss: 9.999999747378752e-05\n",
      "Step: 1360, train/grad_norm: 0.05827301740646362\n",
      "Step: 1360, train/learning_rate: 4.8381723900092766e-05\n",
      "Step: 1360, train/epoch: 0.3236553966999054\n",
      "Step: 1370, train/loss: 9.999999747378752e-05\n",
      "Step: 1370, train/grad_norm: 0.007858175784349442\n",
      "Step: 1370, train/learning_rate: 4.836982407141477e-05\n",
      "Step: 1370, train/epoch: 0.3260352313518524\n",
      "Step: 1380, train/loss: 0.12129999697208405\n",
      "Step: 1380, train/grad_norm: 1.5161439180374146\n",
      "Step: 1380, train/learning_rate: 4.835792424273677e-05\n",
      "Step: 1380, train/epoch: 0.32841503620147705\n",
      "Step: 1390, train/loss: 0.00019999999494757503\n",
      "Step: 1390, train/grad_norm: 0.00019748523482121527\n",
      "Step: 1390, train/learning_rate: 4.8346024414058775e-05\n",
      "Step: 1390, train/epoch: 0.3307948708534241\n",
      "Step: 1400, train/loss: 0.0\n",
      "Step: 1400, train/grad_norm: 8.453259397356305e-06\n",
      "Step: 1400, train/learning_rate: 4.8334128223359585e-05\n",
      "Step: 1400, train/epoch: 0.3331746757030487\n",
      "Step: 1410, train/loss: 0.0\n",
      "Step: 1410, train/grad_norm: 6.278272394411033e-06\n",
      "Step: 1410, train/learning_rate: 4.832222839468159e-05\n",
      "Step: 1410, train/epoch: 0.3355545103549957\n",
      "Step: 1420, train/loss: 0.00019999999494757503\n",
      "Step: 1420, train/grad_norm: 1.53653752477112e-06\n",
      "Step: 1420, train/learning_rate: 4.831032856600359e-05\n",
      "Step: 1420, train/epoch: 0.33793431520462036\n",
      "Step: 1430, train/loss: 0.0017000000225380063\n",
      "Step: 1430, train/grad_norm: 17.230514526367188\n",
      "Step: 1430, train/learning_rate: 4.8298428737325594e-05\n",
      "Step: 1430, train/epoch: 0.3403141498565674\n",
      "Step: 1440, train/loss: 0.09799999743700027\n",
      "Step: 1440, train/grad_norm: 1.5808971909336833e-07\n",
      "Step: 1440, train/learning_rate: 4.82865289086476e-05\n",
      "Step: 1440, train/epoch: 0.342693954706192\n",
      "Step: 1450, train/loss: 0.3416000008583069\n",
      "Step: 1450, train/grad_norm: 4.702990554505959e-05\n",
      "Step: 1450, train/learning_rate: 4.827463271794841e-05\n",
      "Step: 1450, train/epoch: 0.34507375955581665\n",
      "Step: 1460, train/loss: 0.22059999406337738\n",
      "Step: 1460, train/grad_norm: 88.78849792480469\n",
      "Step: 1460, train/learning_rate: 4.826273288927041e-05\n",
      "Step: 1460, train/epoch: 0.34745359420776367\n",
      "Step: 1470, train/loss: 0.22390000522136688\n",
      "Step: 1470, train/grad_norm: 52.69709396362305\n",
      "Step: 1470, train/learning_rate: 4.825083306059241e-05\n",
      "Step: 1470, train/epoch: 0.3498333990573883\n",
      "Step: 1480, train/loss: 0.009200000204145908\n",
      "Step: 1480, train/grad_norm: 0.00029746448853984475\n",
      "Step: 1480, train/learning_rate: 4.8238933231914416e-05\n",
      "Step: 1480, train/epoch: 0.3522132337093353\n",
      "Step: 1490, train/loss: 0.20309999585151672\n",
      "Step: 1490, train/grad_norm: 6.043624694029859e-07\n",
      "Step: 1490, train/learning_rate: 4.822703340323642e-05\n",
      "Step: 1490, train/epoch: 0.35459303855895996\n",
      "Step: 1500, train/loss: 0.01850000023841858\n",
      "Step: 1500, train/grad_norm: 0.0006943957996554673\n",
      "Step: 1500, train/learning_rate: 4.821513721253723e-05\n",
      "Step: 1500, train/epoch: 0.356972873210907\n",
      "Step: 1510, train/loss: 0.0\n",
      "Step: 1510, train/grad_norm: 1.5069423042746166e-08\n",
      "Step: 1510, train/learning_rate: 4.820323738385923e-05\n",
      "Step: 1510, train/epoch: 0.3593526780605316\n",
      "Step: 1520, train/loss: 0.1200999990105629\n",
      "Step: 1520, train/grad_norm: 2.3251516819000244\n",
      "Step: 1520, train/learning_rate: 4.8191337555181235e-05\n",
      "Step: 1520, train/epoch: 0.36173251271247864\n",
      "Step: 1530, train/loss: 0.0357000008225441\n",
      "Step: 1530, train/grad_norm: 0.5104982852935791\n",
      "Step: 1530, train/learning_rate: 4.817943772650324e-05\n",
      "Step: 1530, train/epoch: 0.36411231756210327\n",
      "Step: 1540, train/loss: 0.24469999969005585\n",
      "Step: 1540, train/grad_norm: 1.6687984043528559e-06\n",
      "Step: 1540, train/learning_rate: 4.816753789782524e-05\n",
      "Step: 1540, train/epoch: 0.3664921522140503\n",
      "Step: 1550, train/loss: 0.11909999698400497\n",
      "Step: 1550, train/grad_norm: 2.487656831741333\n",
      "Step: 1550, train/learning_rate: 4.815564170712605e-05\n",
      "Step: 1550, train/epoch: 0.3688719570636749\n",
      "Step: 1560, train/loss: 0.17839999496936798\n",
      "Step: 1560, train/grad_norm: 1.2962897244506166e-06\n",
      "Step: 1560, train/learning_rate: 4.8143741878448054e-05\n",
      "Step: 1560, train/epoch: 0.37125179171562195\n",
      "Step: 1570, train/loss: 0.15479999780654907\n",
      "Step: 1570, train/grad_norm: 0.5245331525802612\n",
      "Step: 1570, train/learning_rate: 4.813184204977006e-05\n",
      "Step: 1570, train/epoch: 0.3736315965652466\n",
      "Step: 1580, train/loss: 0.31940001249313354\n",
      "Step: 1580, train/grad_norm: 0.0002618816797621548\n",
      "Step: 1580, train/learning_rate: 4.811994222109206e-05\n",
      "Step: 1580, train/epoch: 0.3760114312171936\n",
      "Step: 1590, train/loss: 0.09679999947547913\n",
      "Step: 1590, train/grad_norm: 0.0013815892161801457\n",
      "Step: 1590, train/learning_rate: 4.810804239241406e-05\n",
      "Step: 1590, train/epoch: 0.37839123606681824\n",
      "Step: 1600, train/loss: 0.0\n",
      "Step: 1600, train/grad_norm: 2.2785721739637665e-05\n",
      "Step: 1600, train/learning_rate: 4.809614620171487e-05\n",
      "Step: 1600, train/epoch: 0.38077107071876526\n",
      "Step: 1610, train/loss: 0.0005000000237487257\n",
      "Step: 1610, train/grad_norm: 3.017271637872909e-06\n",
      "Step: 1610, train/learning_rate: 4.8084246373036876e-05\n",
      "Step: 1610, train/epoch: 0.3831508755683899\n",
      "Step: 1620, train/loss: 0.010700000450015068\n",
      "Step: 1620, train/grad_norm: 0.00031443306943401694\n",
      "Step: 1620, train/learning_rate: 4.807234654435888e-05\n",
      "Step: 1620, train/epoch: 0.3855307102203369\n",
      "Step: 1630, train/loss: 0.39089998602867126\n",
      "Step: 1630, train/grad_norm: 0.0442517027258873\n",
      "Step: 1630, train/learning_rate: 4.806044671568088e-05\n",
      "Step: 1630, train/epoch: 0.38791051506996155\n",
      "Step: 1640, train/loss: 0.11890000104904175\n",
      "Step: 1640, train/grad_norm: 0.056241609156131744\n",
      "Step: 1640, train/learning_rate: 4.8048546887002885e-05\n",
      "Step: 1640, train/epoch: 0.39029034972190857\n",
      "Step: 1650, train/loss: 0.12479999661445618\n",
      "Step: 1650, train/grad_norm: 11.740157127380371\n",
      "Step: 1650, train/learning_rate: 4.8036650696303695e-05\n",
      "Step: 1650, train/epoch: 0.3926701545715332\n",
      "Step: 1660, train/loss: 0.15160000324249268\n",
      "Step: 1660, train/grad_norm: 5.356360912322998\n",
      "Step: 1660, train/learning_rate: 4.80247508676257e-05\n",
      "Step: 1660, train/epoch: 0.3950499892234802\n",
      "Step: 1670, train/loss: 9.999999747378752e-05\n",
      "Step: 1670, train/grad_norm: 3.5965135793958325e-06\n",
      "Step: 1670, train/learning_rate: 4.80128510389477e-05\n",
      "Step: 1670, train/epoch: 0.39742979407310486\n",
      "Step: 1680, train/loss: 0.0\n",
      "Step: 1680, train/grad_norm: 0.049268390983343124\n",
      "Step: 1680, train/learning_rate: 4.8000951210269704e-05\n",
      "Step: 1680, train/epoch: 0.3998096287250519\n",
      "Step: 1690, train/loss: 0.0\n",
      "Step: 1690, train/grad_norm: 4.62694238478889e-09\n",
      "Step: 1690, train/learning_rate: 4.798905138159171e-05\n",
      "Step: 1690, train/epoch: 0.4021894335746765\n",
      "Step: 1700, train/loss: 0.002099999925121665\n",
      "Step: 1700, train/grad_norm: 0.0001004851219477132\n",
      "Step: 1700, train/learning_rate: 4.797715519089252e-05\n",
      "Step: 1700, train/epoch: 0.40456923842430115\n",
      "Step: 1710, train/loss: 0.0\n",
      "Step: 1710, train/grad_norm: 8.075794539763592e-06\n",
      "Step: 1710, train/learning_rate: 4.796525536221452e-05\n",
      "Step: 1710, train/epoch: 0.40694907307624817\n",
      "Step: 1720, train/loss: 0.0\n",
      "Step: 1720, train/grad_norm: 9.004865205497481e-06\n",
      "Step: 1720, train/learning_rate: 4.7953355533536524e-05\n",
      "Step: 1720, train/epoch: 0.4093288779258728\n",
      "Step: 1730, train/loss: 0.17110000550746918\n",
      "Step: 1730, train/grad_norm: 0.00043560564517974854\n",
      "Step: 1730, train/learning_rate: 4.7941455704858527e-05\n",
      "Step: 1730, train/epoch: 0.4117087125778198\n",
      "Step: 1740, train/loss: 0.14000000059604645\n",
      "Step: 1740, train/grad_norm: 0.2656925618648529\n",
      "Step: 1740, train/learning_rate: 4.792955587618053e-05\n",
      "Step: 1740, train/epoch: 0.41408851742744446\n",
      "Step: 1750, train/loss: 0.0031999999191612005\n",
      "Step: 1750, train/grad_norm: 49.1696662902832\n",
      "Step: 1750, train/learning_rate: 4.791765968548134e-05\n",
      "Step: 1750, train/epoch: 0.4164683520793915\n",
      "Step: 1760, train/loss: 0.04859999939799309\n",
      "Step: 1760, train/grad_norm: 9.409422574435666e-08\n",
      "Step: 1760, train/learning_rate: 4.790575985680334e-05\n",
      "Step: 1760, train/epoch: 0.4188481569290161\n",
      "Step: 1770, train/loss: 0.09380000084638596\n",
      "Step: 1770, train/grad_norm: 3.066922727157362e-05\n",
      "Step: 1770, train/learning_rate: 4.7893860028125346e-05\n",
      "Step: 1770, train/epoch: 0.42122799158096313\n",
      "Step: 1780, train/loss: 0.0\n",
      "Step: 1780, train/grad_norm: 0.0002482755808159709\n",
      "Step: 1780, train/learning_rate: 4.788196019944735e-05\n",
      "Step: 1780, train/epoch: 0.42360779643058777\n",
      "Step: 1790, train/loss: 0.0\n",
      "Step: 1790, train/grad_norm: 8.591150901793299e-09\n",
      "Step: 1790, train/learning_rate: 4.787006037076935e-05\n",
      "Step: 1790, train/epoch: 0.4259876310825348\n",
      "Step: 1800, train/loss: 0.018400000408291817\n",
      "Step: 1800, train/grad_norm: 0.1626979261636734\n",
      "Step: 1800, train/learning_rate: 4.785816418007016e-05\n",
      "Step: 1800, train/epoch: 0.4283674359321594\n",
      "Step: 1810, train/loss: 0.000699999975040555\n",
      "Step: 1810, train/grad_norm: 3.923099939129315e-06\n",
      "Step: 1810, train/learning_rate: 4.7846264351392165e-05\n",
      "Step: 1810, train/epoch: 0.43074727058410645\n",
      "Step: 1820, train/loss: 0.0\n",
      "Step: 1820, train/grad_norm: 8.642792863611248e-07\n",
      "Step: 1820, train/learning_rate: 4.783436452271417e-05\n",
      "Step: 1820, train/epoch: 0.4331270754337311\n",
      "Step: 1830, train/loss: 0.3294999897480011\n",
      "Step: 1830, train/grad_norm: 257.8587951660156\n",
      "Step: 1830, train/learning_rate: 4.782246469403617e-05\n",
      "Step: 1830, train/epoch: 0.4355069100856781\n",
      "Step: 1840, train/loss: 0.21879999339580536\n",
      "Step: 1840, train/grad_norm: 242.52040100097656\n",
      "Step: 1840, train/learning_rate: 4.7810564865358174e-05\n",
      "Step: 1840, train/epoch: 0.43788671493530273\n",
      "Step: 1850, train/loss: 0.0006000000284984708\n",
      "Step: 1850, train/grad_norm: 2.1444461345672607\n",
      "Step: 1850, train/learning_rate: 4.7798668674658984e-05\n",
      "Step: 1850, train/epoch: 0.44026654958724976\n",
      "Step: 1860, train/loss: 0.49070000648498535\n",
      "Step: 1860, train/grad_norm: 6.511731953651179e-07\n",
      "Step: 1860, train/learning_rate: 4.778676884598099e-05\n",
      "Step: 1860, train/epoch: 0.4426463544368744\n",
      "Step: 1870, train/loss: 0.0\n",
      "Step: 1870, train/grad_norm: 0.026486054062843323\n",
      "Step: 1870, train/learning_rate: 4.777486901730299e-05\n",
      "Step: 1870, train/epoch: 0.4450261890888214\n",
      "Step: 1880, train/loss: 0.06620000302791595\n",
      "Step: 1880, train/grad_norm: 0.036733146756887436\n",
      "Step: 1880, train/learning_rate: 4.776296918862499e-05\n",
      "Step: 1880, train/epoch: 0.44740599393844604\n",
      "Step: 1890, train/loss: 0.13189999759197235\n",
      "Step: 1890, train/grad_norm: 0.0008550683269277215\n",
      "Step: 1890, train/learning_rate: 4.7751069359946996e-05\n",
      "Step: 1890, train/epoch: 0.44978582859039307\n",
      "Step: 1900, train/loss: 0.1306000053882599\n",
      "Step: 1900, train/grad_norm: 1.2315856110944878e-05\n",
      "Step: 1900, train/learning_rate: 4.7739173169247806e-05\n",
      "Step: 1900, train/epoch: 0.4521656334400177\n",
      "Step: 1910, train/loss: 0.22310000658035278\n",
      "Step: 1910, train/grad_norm: 0.0005467575392685831\n",
      "Step: 1910, train/learning_rate: 4.772727334056981e-05\n",
      "Step: 1910, train/epoch: 0.4545454680919647\n",
      "Step: 1920, train/loss: 0.0\n",
      "Step: 1920, train/grad_norm: 0.02767881006002426\n",
      "Step: 1920, train/learning_rate: 4.771537351189181e-05\n",
      "Step: 1920, train/epoch: 0.45692527294158936\n",
      "Step: 1930, train/loss: 0.05270000174641609\n",
      "Step: 1930, train/grad_norm: 0.002623816952109337\n",
      "Step: 1930, train/learning_rate: 4.7703473683213815e-05\n",
      "Step: 1930, train/epoch: 0.4593051075935364\n",
      "Step: 1940, train/loss: 0.0\n",
      "Step: 1940, train/grad_norm: 5.669738129654434e-06\n",
      "Step: 1940, train/learning_rate: 4.769157385453582e-05\n",
      "Step: 1940, train/epoch: 0.461684912443161\n",
      "Step: 1950, train/loss: 0.0\n",
      "Step: 1950, train/grad_norm: 3.3130506835732376e-06\n",
      "Step: 1950, train/learning_rate: 4.767967766383663e-05\n",
      "Step: 1950, train/epoch: 0.46406471729278564\n",
      "Step: 1960, train/loss: 0.004000000189989805\n",
      "Step: 1960, train/grad_norm: 0.0002437699440633878\n",
      "Step: 1960, train/learning_rate: 4.766777783515863e-05\n",
      "Step: 1960, train/epoch: 0.46644455194473267\n",
      "Step: 1970, train/loss: 0.0\n",
      "Step: 1970, train/grad_norm: 8.354782421449158e-10\n",
      "Step: 1970, train/learning_rate: 4.7655878006480634e-05\n",
      "Step: 1970, train/epoch: 0.4688243567943573\n",
      "Step: 1980, train/loss: 0.0\n",
      "Step: 1980, train/grad_norm: 1.62272626766935e-05\n",
      "Step: 1980, train/learning_rate: 4.764397817780264e-05\n",
      "Step: 1980, train/epoch: 0.4712041914463043\n",
      "Step: 1990, train/loss: 0.0\n",
      "Step: 1990, train/grad_norm: 8.61794160300633e-06\n",
      "Step: 1990, train/learning_rate: 4.763207834912464e-05\n",
      "Step: 1990, train/epoch: 0.47358399629592896\n",
      "Step: 2000, train/loss: 0.0\n",
      "Step: 2000, train/grad_norm: 4.300060041373399e-08\n",
      "Step: 2000, train/learning_rate: 4.762018215842545e-05\n",
      "Step: 2000, train/epoch: 0.475963830947876\n",
      "Step: 2010, train/loss: 0.20309999585151672\n",
      "Step: 2010, train/grad_norm: 5.427721407613717e-05\n",
      "Step: 2010, train/learning_rate: 4.760828232974745e-05\n",
      "Step: 2010, train/epoch: 0.4783436357975006\n",
      "Step: 2020, train/loss: 9.999999747378752e-05\n",
      "Step: 2020, train/grad_norm: 0.003791902679949999\n",
      "Step: 2020, train/learning_rate: 4.7596382501069456e-05\n",
      "Step: 2020, train/epoch: 0.48072347044944763\n",
      "Step: 2030, train/loss: 0.00019999999494757503\n",
      "Step: 2030, train/grad_norm: 3.493050471092829e-08\n",
      "Step: 2030, train/learning_rate: 4.758448267239146e-05\n",
      "Step: 2030, train/epoch: 0.48310327529907227\n",
      "Step: 2040, train/loss: 0.22499999403953552\n",
      "Step: 2040, train/grad_norm: 1.2578469466006936e-07\n",
      "Step: 2040, train/learning_rate: 4.757258284371346e-05\n",
      "Step: 2040, train/epoch: 0.4854831099510193\n",
      "Step: 2050, train/loss: 0.10859999805688858\n",
      "Step: 2050, train/grad_norm: 0.0012561329640448093\n",
      "Step: 2050, train/learning_rate: 4.756068665301427e-05\n",
      "Step: 2050, train/epoch: 0.4878629148006439\n",
      "Step: 2060, train/loss: 0.0\n",
      "Step: 2060, train/grad_norm: 1.5650772184017114e-05\n",
      "Step: 2060, train/learning_rate: 4.7548786824336275e-05\n",
      "Step: 2060, train/epoch: 0.49024274945259094\n",
      "Step: 2070, train/loss: 0.00419999985024333\n",
      "Step: 2070, train/grad_norm: 5.9289231300354\n",
      "Step: 2070, train/learning_rate: 4.753688699565828e-05\n",
      "Step: 2070, train/epoch: 0.4926225543022156\n",
      "Step: 2080, train/loss: 0.0\n",
      "Step: 2080, train/grad_norm: 7.36435424641968e-07\n",
      "Step: 2080, train/learning_rate: 4.752498716698028e-05\n",
      "Step: 2080, train/epoch: 0.4950023889541626\n",
      "Step: 2090, train/loss: 0.0\n",
      "Step: 2090, train/grad_norm: 1.5229736050059728e-07\n",
      "Step: 2090, train/learning_rate: 4.7513087338302284e-05\n",
      "Step: 2090, train/epoch: 0.49738219380378723\n",
      "Step: 2100, train/loss: 0.046300001442432404\n",
      "Step: 2100, train/grad_norm: 4.499063652474433e-05\n",
      "Step: 2100, train/learning_rate: 4.7501191147603095e-05\n",
      "Step: 2100, train/epoch: 0.49976202845573425\n",
      "Step: 2110, train/loss: 0.0\n",
      "Step: 2110, train/grad_norm: 0.0012714299373328686\n",
      "Step: 2110, train/learning_rate: 4.74892913189251e-05\n",
      "Step: 2110, train/epoch: 0.5021418333053589\n",
      "Step: 2120, train/loss: 0.14489999413490295\n",
      "Step: 2120, train/grad_norm: 0.00013462449715007097\n",
      "Step: 2120, train/learning_rate: 4.74773914902471e-05\n",
      "Step: 2120, train/epoch: 0.5045216679573059\n",
      "Step: 2130, train/loss: 0.08540000021457672\n",
      "Step: 2130, train/grad_norm: 0.0001379954192088917\n",
      "Step: 2130, train/learning_rate: 4.7465491661569104e-05\n",
      "Step: 2130, train/epoch: 0.5069015026092529\n",
      "Step: 2140, train/loss: 0.0\n",
      "Step: 2140, train/grad_norm: 1.0560149199534408e-08\n",
      "Step: 2140, train/learning_rate: 4.7453591832891107e-05\n",
      "Step: 2140, train/epoch: 0.5092812776565552\n",
      "Step: 2150, train/loss: 0.0\n",
      "Step: 2150, train/grad_norm: 8.593299571657553e-05\n",
      "Step: 2150, train/learning_rate: 4.744169564219192e-05\n",
      "Step: 2150, train/epoch: 0.5116611123085022\n",
      "Step: 2160, train/loss: 0.005900000222027302\n",
      "Step: 2160, train/grad_norm: 6.514721917483257e-07\n",
      "Step: 2160, train/learning_rate: 4.742979581351392e-05\n",
      "Step: 2160, train/epoch: 0.5140409469604492\n",
      "Step: 2170, train/loss: 0.03280000016093254\n",
      "Step: 2170, train/grad_norm: 0.0311204195022583\n",
      "Step: 2170, train/learning_rate: 4.741789598483592e-05\n",
      "Step: 2170, train/epoch: 0.5164207816123962\n",
      "Step: 2180, train/loss: 0.09769999980926514\n",
      "Step: 2180, train/grad_norm: 1.095586867450038e-06\n",
      "Step: 2180, train/learning_rate: 4.7405996156157926e-05\n",
      "Step: 2180, train/epoch: 0.5188005566596985\n",
      "Step: 2190, train/loss: 0.06480000168085098\n",
      "Step: 2190, train/grad_norm: 137.50819396972656\n",
      "Step: 2190, train/learning_rate: 4.739409632747993e-05\n",
      "Step: 2190, train/epoch: 0.5211803913116455\n",
      "Step: 2200, train/loss: 0.42730000615119934\n",
      "Step: 2200, train/grad_norm: 314.4190368652344\n",
      "Step: 2200, train/learning_rate: 4.738220013678074e-05\n",
      "Step: 2200, train/epoch: 0.5235602259635925\n",
      "Step: 2210, train/loss: 9.999999747378752e-05\n",
      "Step: 2210, train/grad_norm: 4.236533186485758e-07\n",
      "Step: 2210, train/learning_rate: 4.737030030810274e-05\n",
      "Step: 2210, train/epoch: 0.5259400010108948\n",
      "Step: 2220, train/loss: 0.18359999358654022\n",
      "Step: 2220, train/grad_norm: 1.287755253542855e-06\n",
      "Step: 2220, train/learning_rate: 4.7358400479424745e-05\n",
      "Step: 2220, train/epoch: 0.5283198356628418\n",
      "Step: 2230, train/loss: 0.18060000240802765\n",
      "Step: 2230, train/grad_norm: 9.405477612745017e-06\n",
      "Step: 2230, train/learning_rate: 4.734650065074675e-05\n",
      "Step: 2230, train/epoch: 0.5306996703147888\n",
      "Step: 2240, train/loss: 0.15489999949932098\n",
      "Step: 2240, train/grad_norm: 0.0004872564459219575\n",
      "Step: 2240, train/learning_rate: 4.733460082206875e-05\n",
      "Step: 2240, train/epoch: 0.5330795049667358\n",
      "Step: 2250, train/loss: 0.0005000000237487257\n",
      "Step: 2250, train/grad_norm: 0.000250423006946221\n",
      "Step: 2250, train/learning_rate: 4.732270463136956e-05\n",
      "Step: 2250, train/epoch: 0.5354592800140381\n",
      "Step: 2260, train/loss: 0.031599998474121094\n",
      "Step: 2260, train/grad_norm: 2.2636846551904455e-05\n",
      "Step: 2260, train/learning_rate: 4.7310804802691564e-05\n",
      "Step: 2260, train/epoch: 0.5378391146659851\n",
      "Step: 2270, train/loss: 0.0\n",
      "Step: 2270, train/grad_norm: 1.1545795132406056e-05\n",
      "Step: 2270, train/learning_rate: 4.729890497401357e-05\n",
      "Step: 2270, train/epoch: 0.5402189493179321\n",
      "Step: 2280, train/loss: 0.0\n",
      "Step: 2280, train/grad_norm: 2.5617966912250267e-06\n",
      "Step: 2280, train/learning_rate: 4.728700514533557e-05\n",
      "Step: 2280, train/epoch: 0.5425987839698792\n",
      "Step: 2290, train/loss: 0.0\n",
      "Step: 2290, train/grad_norm: 0.00027178856544196606\n",
      "Step: 2290, train/learning_rate: 4.727510531665757e-05\n",
      "Step: 2290, train/epoch: 0.5449785590171814\n",
      "Step: 2300, train/loss: 0.0997999981045723\n",
      "Step: 2300, train/grad_norm: 2.435993792460067e-06\n",
      "Step: 2300, train/learning_rate: 4.726320912595838e-05\n",
      "Step: 2300, train/epoch: 0.5473583936691284\n",
      "Step: 2310, train/loss: 0.000699999975040555\n",
      "Step: 2310, train/grad_norm: 5.297970346873626e-05\n",
      "Step: 2310, train/learning_rate: 4.7251309297280386e-05\n",
      "Step: 2310, train/epoch: 0.5497382283210754\n",
      "Step: 2320, train/loss: 0.18289999663829803\n",
      "Step: 2320, train/grad_norm: 1.7295222282409668\n",
      "Step: 2320, train/learning_rate: 4.723940946860239e-05\n",
      "Step: 2320, train/epoch: 0.5521180629730225\n",
      "Step: 2330, train/loss: 9.999999747378752e-05\n",
      "Step: 2330, train/grad_norm: 0.00024893699446693063\n",
      "Step: 2330, train/learning_rate: 4.722750963992439e-05\n",
      "Step: 2330, train/epoch: 0.5544978380203247\n",
      "Step: 2340, train/loss: 0.07460000365972519\n",
      "Step: 2340, train/grad_norm: 396.4615783691406\n",
      "Step: 2340, train/learning_rate: 4.7215609811246395e-05\n",
      "Step: 2340, train/epoch: 0.5568776726722717\n",
      "Step: 2350, train/loss: 0.0\n",
      "Step: 2350, train/grad_norm: 0.00038294741534627974\n",
      "Step: 2350, train/learning_rate: 4.7203713620547205e-05\n",
      "Step: 2350, train/epoch: 0.5592575073242188\n",
      "Step: 2360, train/loss: 0.0\n",
      "Step: 2360, train/grad_norm: 1.2769716306593182e-07\n",
      "Step: 2360, train/learning_rate: 4.719181379186921e-05\n",
      "Step: 2360, train/epoch: 0.5616373419761658\n",
      "Step: 2370, train/loss: 0.0015999999595806003\n",
      "Step: 2370, train/grad_norm: 1.0474548339800549e-08\n",
      "Step: 2370, train/learning_rate: 4.717991396319121e-05\n",
      "Step: 2370, train/epoch: 0.564017117023468\n",
      "Step: 2380, train/loss: 0.14110000431537628\n",
      "Step: 2380, train/grad_norm: 2.299002987982135e-09\n",
      "Step: 2380, train/learning_rate: 4.7168014134513214e-05\n",
      "Step: 2380, train/epoch: 0.566396951675415\n",
      "Step: 2390, train/loss: 0.0\n",
      "Step: 2390, train/grad_norm: 8.291432429530232e-09\n",
      "Step: 2390, train/learning_rate: 4.7156117943814024e-05\n",
      "Step: 2390, train/epoch: 0.5687767863273621\n",
      "Step: 2400, train/loss: 0.13259999454021454\n",
      "Step: 2400, train/grad_norm: 0.0007700802525505424\n",
      "Step: 2400, train/learning_rate: 4.714421811513603e-05\n",
      "Step: 2400, train/epoch: 0.5711566209793091\n",
      "Step: 2410, train/loss: 0.14569999277591705\n",
      "Step: 2410, train/grad_norm: 1.6160909126483602e-06\n",
      "Step: 2410, train/learning_rate: 4.713231828645803e-05\n",
      "Step: 2410, train/epoch: 0.5735363960266113\n",
      "Step: 2420, train/loss: 0.1720999926328659\n",
      "Step: 2420, train/grad_norm: 0.011429628357291222\n",
      "Step: 2420, train/learning_rate: 4.712041845778003e-05\n",
      "Step: 2420, train/epoch: 0.5759162306785583\n",
      "Step: 2430, train/loss: 0.051600001752376556\n",
      "Step: 2430, train/grad_norm: 1.353979496343527e-05\n",
      "Step: 2430, train/learning_rate: 4.7108518629102036e-05\n",
      "Step: 2430, train/epoch: 0.5782960653305054\n",
      "Step: 2440, train/loss: 0.26409998536109924\n",
      "Step: 2440, train/grad_norm: 0.000512841681484133\n",
      "Step: 2440, train/learning_rate: 4.7096622438402846e-05\n",
      "Step: 2440, train/epoch: 0.5806758403778076\n",
      "Step: 2450, train/loss: 9.999999747378752e-05\n",
      "Step: 2450, train/grad_norm: 3.635609857610689e-07\n",
      "Step: 2450, train/learning_rate: 4.708472260972485e-05\n",
      "Step: 2450, train/epoch: 0.5830556750297546\n",
      "Step: 2460, train/loss: 0.0\n",
      "Step: 2460, train/grad_norm: 2.328407866514226e-08\n",
      "Step: 2460, train/learning_rate: 4.707282278104685e-05\n",
      "Step: 2460, train/epoch: 0.5854355096817017\n",
      "Step: 2470, train/loss: 0.0\n",
      "Step: 2470, train/grad_norm: 0.0012511734385043383\n",
      "Step: 2470, train/learning_rate: 4.7060922952368855e-05\n",
      "Step: 2470, train/epoch: 0.5878153443336487\n",
      "Step: 2480, train/loss: 0.18279999494552612\n",
      "Step: 2480, train/grad_norm: 2.243083230268894e-07\n",
      "Step: 2480, train/learning_rate: 4.704902312369086e-05\n",
      "Step: 2480, train/epoch: 0.5901951193809509\n",
      "Step: 2490, train/loss: 0.3695000112056732\n",
      "Step: 2490, train/grad_norm: 219.30482482910156\n",
      "Step: 2490, train/learning_rate: 4.703712693299167e-05\n",
      "Step: 2490, train/epoch: 0.592574954032898\n",
      "Step: 2500, train/loss: 0.2084999978542328\n",
      "Step: 2500, train/grad_norm: 0.003229014575481415\n",
      "Step: 2500, train/learning_rate: 4.702522710431367e-05\n",
      "Step: 2500, train/epoch: 0.594954788684845\n",
      "Step: 2510, train/loss: 0.05719999969005585\n",
      "Step: 2510, train/grad_norm: 2.9647117116837762e-05\n",
      "Step: 2510, train/learning_rate: 4.7013327275635675e-05\n",
      "Step: 2510, train/epoch: 0.597334623336792\n",
      "Step: 2520, train/loss: 0.008700000122189522\n",
      "Step: 2520, train/grad_norm: 0.018951665610074997\n",
      "Step: 2520, train/learning_rate: 4.700142744695768e-05\n",
      "Step: 2520, train/epoch: 0.5997143983840942\n",
      "Step: 2530, train/loss: 0.010300000198185444\n",
      "Step: 2530, train/grad_norm: 7.069289858918637e-05\n",
      "Step: 2530, train/learning_rate: 4.698952761827968e-05\n",
      "Step: 2530, train/epoch: 0.6020942330360413\n",
      "Step: 2540, train/loss: 0.0017000000225380063\n",
      "Step: 2540, train/grad_norm: 5.667657751473598e-05\n",
      "Step: 2540, train/learning_rate: 4.697763142758049e-05\n",
      "Step: 2540, train/epoch: 0.6044740676879883\n",
      "Step: 2550, train/loss: 0.0\n",
      "Step: 2550, train/grad_norm: 0.0003539716708473861\n",
      "Step: 2550, train/learning_rate: 4.6965731598902494e-05\n",
      "Step: 2550, train/epoch: 0.6068539023399353\n",
      "Step: 2560, train/loss: 0.0\n",
      "Step: 2560, train/grad_norm: 0.02765861712396145\n",
      "Step: 2560, train/learning_rate: 4.69538317702245e-05\n",
      "Step: 2560, train/epoch: 0.6092336773872375\n",
      "Step: 2570, train/loss: 0.08839999884366989\n",
      "Step: 2570, train/grad_norm: 1.6662343114148825e-06\n",
      "Step: 2570, train/learning_rate: 4.69419319415465e-05\n",
      "Step: 2570, train/epoch: 0.6116135120391846\n",
      "Step: 2580, train/loss: 0.21940000355243683\n",
      "Step: 2580, train/grad_norm: 348.4084777832031\n",
      "Step: 2580, train/learning_rate: 4.69300321128685e-05\n",
      "Step: 2580, train/epoch: 0.6139933466911316\n",
      "Step: 2590, train/loss: 0.43849998712539673\n",
      "Step: 2590, train/grad_norm: 0.0182020366191864\n",
      "Step: 2590, train/learning_rate: 4.691813592216931e-05\n",
      "Step: 2590, train/epoch: 0.6163731813430786\n",
      "Step: 2600, train/loss: 0.1111999973654747\n",
      "Step: 2600, train/grad_norm: 0.03643376752734184\n",
      "Step: 2600, train/learning_rate: 4.6906236093491316e-05\n",
      "Step: 2600, train/epoch: 0.6187529563903809\n",
      "Step: 2610, train/loss: 0.11330000311136246\n",
      "Step: 2610, train/grad_norm: 0.18815426528453827\n",
      "Step: 2610, train/learning_rate: 4.689433626481332e-05\n",
      "Step: 2610, train/epoch: 0.6211327910423279\n",
      "Step: 2620, train/loss: 0.1200999990105629\n",
      "Step: 2620, train/grad_norm: 0.0009362242417410016\n",
      "Step: 2620, train/learning_rate: 4.688243643613532e-05\n",
      "Step: 2620, train/epoch: 0.6235126256942749\n",
      "Step: 2630, train/loss: 0.4781000018119812\n",
      "Step: 2630, train/grad_norm: 0.014226465485990047\n",
      "Step: 2630, train/learning_rate: 4.6870536607457325e-05\n",
      "Step: 2630, train/epoch: 0.6258924603462219\n",
      "Step: 2640, train/loss: 0.002300000051036477\n",
      "Step: 2640, train/grad_norm: 6.985537766013294e-05\n",
      "Step: 2640, train/learning_rate: 4.6858640416758135e-05\n",
      "Step: 2640, train/epoch: 0.6282722353935242\n",
      "Step: 2650, train/loss: 0.14399999380111694\n",
      "Step: 2650, train/grad_norm: 0.01698465645313263\n",
      "Step: 2650, train/learning_rate: 4.684674058808014e-05\n",
      "Step: 2650, train/epoch: 0.6306520700454712\n",
      "Step: 2660, train/loss: 0.3375000059604645\n",
      "Step: 2660, train/grad_norm: 1.6581373074586736e-06\n",
      "Step: 2660, train/learning_rate: 4.683484075940214e-05\n",
      "Step: 2660, train/epoch: 0.6330319046974182\n",
      "Step: 2670, train/loss: 0.00039999998989515007\n",
      "Step: 2670, train/grad_norm: 0.0003748348099179566\n",
      "Step: 2670, train/learning_rate: 4.6822940930724144e-05\n",
      "Step: 2670, train/epoch: 0.6354116797447205\n",
      "Step: 2680, train/loss: 0.0020000000949949026\n",
      "Step: 2680, train/grad_norm: 1.431019285291768e-07\n",
      "Step: 2680, train/learning_rate: 4.681104110204615e-05\n",
      "Step: 2680, train/epoch: 0.6377915143966675\n",
      "Step: 2690, train/loss: 9.999999747378752e-05\n",
      "Step: 2690, train/grad_norm: 0.8330507874488831\n",
      "Step: 2690, train/learning_rate: 4.679914491134696e-05\n",
      "Step: 2690, train/epoch: 0.6401713490486145\n",
      "Step: 2700, train/loss: 0.0\n",
      "Step: 2700, train/grad_norm: 2.949217048353603e-08\n",
      "Step: 2700, train/learning_rate: 4.678724508266896e-05\n",
      "Step: 2700, train/epoch: 0.6425511837005615\n",
      "Step: 2710, train/loss: 0.14219999313354492\n",
      "Step: 2710, train/grad_norm: 1.7568986630106531e-12\n",
      "Step: 2710, train/learning_rate: 4.677534525399096e-05\n",
      "Step: 2710, train/epoch: 0.6449309587478638\n",
      "Step: 2720, train/loss: 0.15309999883174896\n",
      "Step: 2720, train/grad_norm: 5.142864467622132e-10\n",
      "Step: 2720, train/learning_rate: 4.6763445425312966e-05\n",
      "Step: 2720, train/epoch: 0.6473107933998108\n",
      "Step: 2730, train/loss: 0.0\n",
      "Step: 2730, train/grad_norm: 2.6172563138970872e-06\n",
      "Step: 2730, train/learning_rate: 4.675154559663497e-05\n",
      "Step: 2730, train/epoch: 0.6496906280517578\n",
      "Step: 2740, train/loss: 0.0\n",
      "Step: 2740, train/grad_norm: 1.1282842251603142e-06\n",
      "Step: 2740, train/learning_rate: 4.673964940593578e-05\n",
      "Step: 2740, train/epoch: 0.6520704627037048\n",
      "Step: 2750, train/loss: 0.2858999967575073\n",
      "Step: 2750, train/grad_norm: 1.9241186237195507e-05\n",
      "Step: 2750, train/learning_rate: 4.672774957725778e-05\n",
      "Step: 2750, train/epoch: 0.6544502377510071\n",
      "Step: 2760, train/loss: 0.0\n",
      "Step: 2760, train/grad_norm: 0.0013023209758102894\n",
      "Step: 2760, train/learning_rate: 4.6715849748579785e-05\n",
      "Step: 2760, train/epoch: 0.6568300724029541\n",
      "Step: 2770, train/loss: 0.0\n",
      "Step: 2770, train/grad_norm: 3.051872772630304e-05\n",
      "Step: 2770, train/learning_rate: 4.670394991990179e-05\n",
      "Step: 2770, train/epoch: 0.6592099070549011\n",
      "Step: 2780, train/loss: 9.999999747378752e-05\n",
      "Step: 2780, train/grad_norm: 0.0003541675105225295\n",
      "Step: 2780, train/learning_rate: 4.669205009122379e-05\n",
      "Step: 2780, train/epoch: 0.6615897417068481\n",
      "Step: 2790, train/loss: 9.999999747378752e-05\n",
      "Step: 2790, train/grad_norm: 2.936564305855427e-05\n",
      "Step: 2790, train/learning_rate: 4.66801539005246e-05\n",
      "Step: 2790, train/epoch: 0.6639695167541504\n",
      "Step: 2800, train/loss: 0.01720000058412552\n",
      "Step: 2800, train/grad_norm: 200.93902587890625\n",
      "Step: 2800, train/learning_rate: 4.6668254071846604e-05\n",
      "Step: 2800, train/epoch: 0.6663493514060974\n",
      "Step: 2810, train/loss: 0.18809999525547028\n",
      "Step: 2810, train/grad_norm: 1.877413865258859e-06\n",
      "Step: 2810, train/learning_rate: 4.665635424316861e-05\n",
      "Step: 2810, train/epoch: 0.6687291860580444\n",
      "Step: 2820, train/loss: 0.0\n",
      "Step: 2820, train/grad_norm: 1.0743556231318507e-06\n",
      "Step: 2820, train/learning_rate: 4.664445441449061e-05\n",
      "Step: 2820, train/epoch: 0.6711090207099915\n",
      "Step: 2830, train/loss: 0.05429999902844429\n",
      "Step: 2830, train/grad_norm: 2.068559074031029e-10\n",
      "Step: 2830, train/learning_rate: 4.663255458581261e-05\n",
      "Step: 2830, train/epoch: 0.6734887957572937\n",
      "Step: 2840, train/loss: 0.03709999844431877\n",
      "Step: 2840, train/grad_norm: 0.00019783749303314835\n",
      "Step: 2840, train/learning_rate: 4.6620658395113423e-05\n",
      "Step: 2840, train/epoch: 0.6758686304092407\n",
      "Step: 2850, train/loss: 0.0\n",
      "Step: 2850, train/grad_norm: 2.40086084346558e-09\n",
      "Step: 2850, train/learning_rate: 4.6608758566435426e-05\n",
      "Step: 2850, train/epoch: 0.6782484650611877\n",
      "Step: 2860, train/loss: 0.2563000023365021\n",
      "Step: 2860, train/grad_norm: 1.0634897989802994e-05\n",
      "Step: 2860, train/learning_rate: 4.659685873775743e-05\n",
      "Step: 2860, train/epoch: 0.6806282997131348\n",
      "Step: 2870, train/loss: 0.09109999984502792\n",
      "Step: 2870, train/grad_norm: 0.0006833116058260202\n",
      "Step: 2870, train/learning_rate: 4.658495890907943e-05\n",
      "Step: 2870, train/epoch: 0.683008074760437\n",
      "Step: 2880, train/loss: 0.058800000697374344\n",
      "Step: 2880, train/grad_norm: 0.06758857518434525\n",
      "Step: 2880, train/learning_rate: 4.6573059080401435e-05\n",
      "Step: 2880, train/epoch: 0.685387909412384\n",
      "Step: 2890, train/loss: 0.0\n",
      "Step: 2890, train/grad_norm: 0.03731638565659523\n",
      "Step: 2890, train/learning_rate: 4.6561162889702246e-05\n",
      "Step: 2890, train/epoch: 0.687767744064331\n",
      "Step: 2900, train/loss: 0.004900000058114529\n",
      "Step: 2900, train/grad_norm: 3.7189332147136156e-07\n",
      "Step: 2900, train/learning_rate: 4.654926306102425e-05\n",
      "Step: 2900, train/epoch: 0.6901475191116333\n",
      "Step: 2910, train/loss: 9.999999747378752e-05\n",
      "Step: 2910, train/grad_norm: 2.447724727971945e-05\n",
      "Step: 2910, train/learning_rate: 4.653736323234625e-05\n",
      "Step: 2910, train/epoch: 0.6925273537635803\n",
      "Step: 2920, train/loss: 0.1875\n",
      "Step: 2920, train/grad_norm: 2.9298413210199215e-05\n",
      "Step: 2920, train/learning_rate: 4.6525463403668255e-05\n",
      "Step: 2920, train/epoch: 0.6949071884155273\n",
      "Step: 2930, train/loss: 0.11720000207424164\n",
      "Step: 2930, train/grad_norm: 0.0009720152011141181\n",
      "Step: 2930, train/learning_rate: 4.651356357499026e-05\n",
      "Step: 2930, train/epoch: 0.6972870230674744\n",
      "Step: 2940, train/loss: 0.0\n",
      "Step: 2940, train/grad_norm: 2.450427200528793e-05\n",
      "Step: 2940, train/learning_rate: 4.650166738429107e-05\n",
      "Step: 2940, train/epoch: 0.6996667981147766\n",
      "Step: 2950, train/loss: 0.004600000102072954\n",
      "Step: 2950, train/grad_norm: 52.290287017822266\n",
      "Step: 2950, train/learning_rate: 4.648976755561307e-05\n",
      "Step: 2950, train/epoch: 0.7020466327667236\n",
      "Step: 2960, train/loss: 0.20319999754428864\n",
      "Step: 2960, train/grad_norm: 0.0011432492174208164\n",
      "Step: 2960, train/learning_rate: 4.6477867726935074e-05\n",
      "Step: 2960, train/epoch: 0.7044264674186707\n",
      "Step: 2970, train/loss: 0.05909999832510948\n",
      "Step: 2970, train/grad_norm: 0.001483245985582471\n",
      "Step: 2970, train/learning_rate: 4.646596789825708e-05\n",
      "Step: 2970, train/epoch: 0.7068063020706177\n",
      "Step: 2980, train/loss: 0.0\n",
      "Step: 2980, train/grad_norm: 0.011342700570821762\n",
      "Step: 2980, train/learning_rate: 4.645406806957908e-05\n",
      "Step: 2980, train/epoch: 0.7091860771179199\n",
      "Step: 2990, train/loss: 0.0\n",
      "Step: 2990, train/grad_norm: 0.00016622486873529851\n",
      "Step: 2990, train/learning_rate: 4.644217187887989e-05\n",
      "Step: 2990, train/epoch: 0.7115659117698669\n",
      "Step: 3000, train/loss: 0.0\n",
      "Step: 3000, train/grad_norm: 0.0023060233797878027\n",
      "Step: 3000, train/learning_rate: 4.643027205020189e-05\n",
      "Step: 3000, train/epoch: 0.713945746421814\n",
      "Step: 3010, train/loss: 0.026200000196695328\n",
      "Step: 3010, train/grad_norm: 0.11339187622070312\n",
      "Step: 3010, train/learning_rate: 4.6418372221523896e-05\n",
      "Step: 3010, train/epoch: 0.716325581073761\n",
      "Step: 3020, train/loss: 0.00039999998989515007\n",
      "Step: 3020, train/grad_norm: 1.4225189685821533\n",
      "Step: 3020, train/learning_rate: 4.64064723928459e-05\n",
      "Step: 3020, train/epoch: 0.7187053561210632\n",
      "Step: 3030, train/loss: 0.0\n",
      "Step: 3030, train/grad_norm: 6.914960977155715e-07\n",
      "Step: 3030, train/learning_rate: 4.63945725641679e-05\n",
      "Step: 3030, train/epoch: 0.7210851907730103\n",
      "Step: 3040, train/loss: 0.00019999999494757503\n",
      "Step: 3040, train/grad_norm: 7.657671163840973e-10\n",
      "Step: 3040, train/learning_rate: 4.638267637346871e-05\n",
      "Step: 3040, train/epoch: 0.7234650254249573\n",
      "Step: 3050, train/loss: 0.0\n",
      "Step: 3050, train/grad_norm: 4.197145742068642e-08\n",
      "Step: 3050, train/learning_rate: 4.6370776544790715e-05\n",
      "Step: 3050, train/epoch: 0.7258448600769043\n",
      "Step: 3060, train/loss: 0.2953999936580658\n",
      "Step: 3060, train/grad_norm: 3.6460910450841766e-06\n",
      "Step: 3060, train/learning_rate: 4.635887671611272e-05\n",
      "Step: 3060, train/epoch: 0.7282246351242065\n",
      "Step: 3070, train/loss: 0.08569999784231186\n",
      "Step: 3070, train/grad_norm: 2.8750357627868652\n",
      "Step: 3070, train/learning_rate: 4.634697688743472e-05\n",
      "Step: 3070, train/epoch: 0.7306044697761536\n",
      "Step: 3080, train/loss: 0.023099999874830246\n",
      "Step: 3080, train/grad_norm: 264.58367919921875\n",
      "Step: 3080, train/learning_rate: 4.6335077058756724e-05\n",
      "Step: 3080, train/epoch: 0.7329843044281006\n",
      "Step: 3090, train/loss: 0.0\n",
      "Step: 3090, train/grad_norm: 4.8635720304446295e-06\n",
      "Step: 3090, train/learning_rate: 4.6323180868057534e-05\n",
      "Step: 3090, train/epoch: 0.7353641390800476\n",
      "Step: 3100, train/loss: 0.0\n",
      "Step: 3100, train/grad_norm: 0.002869431162253022\n",
      "Step: 3100, train/learning_rate: 4.631128103937954e-05\n",
      "Step: 3100, train/epoch: 0.7377439141273499\n",
      "Step: 3110, train/loss: 0.0035000001080334187\n",
      "Step: 3110, train/grad_norm: 0.00016507916734553874\n",
      "Step: 3110, train/learning_rate: 4.629938121070154e-05\n",
      "Step: 3110, train/epoch: 0.7401237487792969\n",
      "Step: 3120, train/loss: 0.1615999937057495\n",
      "Step: 3120, train/grad_norm: 318.3347473144531\n",
      "Step: 3120, train/learning_rate: 4.628748138202354e-05\n",
      "Step: 3120, train/epoch: 0.7425035834312439\n",
      "Step: 3130, train/loss: 0.4375\n",
      "Step: 3130, train/grad_norm: 2.1068044588901103e-05\n",
      "Step: 3130, train/learning_rate: 4.6275581553345546e-05\n",
      "Step: 3130, train/epoch: 0.7448834180831909\n",
      "Step: 3140, train/loss: 0.17110000550746918\n",
      "Step: 3140, train/grad_norm: 1.0902311259997077e-05\n",
      "Step: 3140, train/learning_rate: 4.6263685362646356e-05\n",
      "Step: 3140, train/epoch: 0.7472631931304932\n",
      "Step: 3150, train/loss: 0.0032999999821186066\n",
      "Step: 3150, train/grad_norm: 36.168216705322266\n",
      "Step: 3150, train/learning_rate: 4.625178553396836e-05\n",
      "Step: 3150, train/epoch: 0.7496430277824402\n",
      "Step: 3160, train/loss: 0.00019999999494757503\n",
      "Step: 3160, train/grad_norm: 0.0033522392623126507\n",
      "Step: 3160, train/learning_rate: 4.623988570529036e-05\n",
      "Step: 3160, train/epoch: 0.7520228624343872\n",
      "Step: 3170, train/loss: 0.06620000302791595\n",
      "Step: 3170, train/grad_norm: 0.33242565393447876\n",
      "Step: 3170, train/learning_rate: 4.6227985876612365e-05\n",
      "Step: 3170, train/epoch: 0.7544026374816895\n",
      "Step: 3180, train/loss: 0.0\n",
      "Step: 3180, train/grad_norm: 2.026870106419665e-06\n",
      "Step: 3180, train/learning_rate: 4.621608604793437e-05\n",
      "Step: 3180, train/epoch: 0.7567824721336365\n",
      "Step: 3190, train/loss: 0.2143000066280365\n",
      "Step: 3190, train/grad_norm: 0.00046223809476941824\n",
      "Step: 3190, train/learning_rate: 4.620418985723518e-05\n",
      "Step: 3190, train/epoch: 0.7591623067855835\n",
      "Step: 3200, train/loss: 0.30079999566078186\n",
      "Step: 3200, train/grad_norm: 0.0002555073006078601\n",
      "Step: 3200, train/learning_rate: 4.619229002855718e-05\n",
      "Step: 3200, train/epoch: 0.7615421414375305\n",
      "Step: 3210, train/loss: 0.0\n",
      "Step: 3210, train/grad_norm: 0.0004397118173073977\n",
      "Step: 3210, train/learning_rate: 4.6180390199879184e-05\n",
      "Step: 3210, train/epoch: 0.7639219164848328\n",
      "Step: 3220, train/loss: 0.06419999897480011\n",
      "Step: 3220, train/grad_norm: 153.4367218017578\n",
      "Step: 3220, train/learning_rate: 4.616849037120119e-05\n",
      "Step: 3220, train/epoch: 0.7663017511367798\n",
      "Step: 3230, train/loss: 0.1266999989748001\n",
      "Step: 3230, train/grad_norm: 0.0010096709011122584\n",
      "Step: 3230, train/learning_rate: 4.615659054252319e-05\n",
      "Step: 3230, train/epoch: 0.7686815857887268\n",
      "Step: 3240, train/loss: 9.999999747378752e-05\n",
      "Step: 3240, train/grad_norm: 0.00021722774545196444\n",
      "Step: 3240, train/learning_rate: 4.6144694351824e-05\n",
      "Step: 3240, train/epoch: 0.7710614204406738\n",
      "Step: 3250, train/loss: 0.0020000000949949026\n",
      "Step: 3250, train/grad_norm: 5.8581947087077424e-05\n",
      "Step: 3250, train/learning_rate: 4.6132794523146003e-05\n",
      "Step: 3250, train/epoch: 0.7734411954879761\n",
      "Step: 3260, train/loss: 0.0\n",
      "Step: 3260, train/grad_norm: 0.0017434294568374753\n",
      "Step: 3260, train/learning_rate: 4.6120894694468006e-05\n",
      "Step: 3260, train/epoch: 0.7758210301399231\n",
      "Step: 3270, train/loss: 0.17579999566078186\n",
      "Step: 3270, train/grad_norm: 8.723106475372333e-06\n",
      "Step: 3270, train/learning_rate: 4.610899486579001e-05\n",
      "Step: 3270, train/epoch: 0.7782008647918701\n",
      "Step: 3280, train/loss: 0.4000000059604645\n",
      "Step: 3280, train/grad_norm: 0.0008675093413330615\n",
      "Step: 3280, train/learning_rate: 4.609709503711201e-05\n",
      "Step: 3280, train/epoch: 0.7805806994438171\n",
      "Step: 3290, train/loss: 0.0\n",
      "Step: 3290, train/grad_norm: 0.000760353053919971\n",
      "Step: 3290, train/learning_rate: 4.608519884641282e-05\n",
      "Step: 3290, train/epoch: 0.7829604744911194\n",
      "Step: 3300, train/loss: 0.28369998931884766\n",
      "Step: 3300, train/grad_norm: 0.3243812322616577\n",
      "Step: 3300, train/learning_rate: 4.6073299017734826e-05\n",
      "Step: 3300, train/epoch: 0.7853403091430664\n",
      "Step: 3310, train/loss: 0.0005000000237487257\n",
      "Step: 3310, train/grad_norm: 0.00016346837219316512\n",
      "Step: 3310, train/learning_rate: 4.606139918905683e-05\n",
      "Step: 3310, train/epoch: 0.7877201437950134\n",
      "Step: 3320, train/loss: 0.36090001463890076\n",
      "Step: 3320, train/grad_norm: 0.00013220729306340218\n",
      "Step: 3320, train/learning_rate: 4.604949936037883e-05\n",
      "Step: 3320, train/epoch: 0.7900999784469604\n",
      "Step: 3330, train/loss: 0.0006000000284984708\n",
      "Step: 3330, train/grad_norm: 0.037941187620162964\n",
      "Step: 3330, train/learning_rate: 4.6037599531700835e-05\n",
      "Step: 3330, train/epoch: 0.7924797534942627\n",
      "Step: 3340, train/loss: 9.999999747378752e-05\n",
      "Step: 3340, train/grad_norm: 0.010155794210731983\n",
      "Step: 3340, train/learning_rate: 4.6025703341001645e-05\n",
      "Step: 3340, train/epoch: 0.7948595881462097\n",
      "Step: 3350, train/loss: 0.06520000100135803\n",
      "Step: 3350, train/grad_norm: 0.00026310025714337826\n",
      "Step: 3350, train/learning_rate: 4.601380351232365e-05\n",
      "Step: 3350, train/epoch: 0.7972394227981567\n",
      "Step: 3360, train/loss: 0.00039999998989515007\n",
      "Step: 3360, train/grad_norm: 1.075752606993774e-05\n",
      "Step: 3360, train/learning_rate: 4.600190368364565e-05\n",
      "Step: 3360, train/epoch: 0.7996192574501038\n",
      "Step: 3370, train/loss: 0.15940000116825104\n",
      "Step: 3370, train/grad_norm: 0.0007358857546932995\n",
      "Step: 3370, train/learning_rate: 4.5990003854967654e-05\n",
      "Step: 3370, train/epoch: 0.801999032497406\n",
      "Step: 3380, train/loss: 0.0\n",
      "Step: 3380, train/grad_norm: 0.0011550114722922444\n",
      "Step: 3380, train/learning_rate: 4.597810402628966e-05\n",
      "Step: 3380, train/epoch: 0.804378867149353\n",
      "Step: 3390, train/loss: 0.06650000065565109\n",
      "Step: 3390, train/grad_norm: 582.4976806640625\n",
      "Step: 3390, train/learning_rate: 4.596620783559047e-05\n",
      "Step: 3390, train/epoch: 0.8067587018013\n",
      "Step: 3400, train/loss: 0.00039999998989515007\n",
      "Step: 3400, train/grad_norm: 0.00474036019295454\n",
      "Step: 3400, train/learning_rate: 4.595430800691247e-05\n",
      "Step: 3400, train/epoch: 0.8091384768486023\n",
      "Step: 3410, train/loss: 0.002099999925121665\n",
      "Step: 3410, train/grad_norm: 1.0284727522957837e-06\n",
      "Step: 3410, train/learning_rate: 4.594240817823447e-05\n",
      "Step: 3410, train/epoch: 0.8115183115005493\n",
      "Step: 3420, train/loss: 0.0\n",
      "Step: 3420, train/grad_norm: 3.276489951531403e-05\n",
      "Step: 3420, train/learning_rate: 4.5930508349556476e-05\n",
      "Step: 3420, train/epoch: 0.8138981461524963\n",
      "Step: 3430, train/loss: 0.0\n",
      "Step: 3430, train/grad_norm: 8.294911140183103e-07\n",
      "Step: 3430, train/learning_rate: 4.591860852087848e-05\n",
      "Step: 3430, train/epoch: 0.8162779808044434\n",
      "Step: 3440, train/loss: 0.06790000200271606\n",
      "Step: 3440, train/grad_norm: 2.810422665788792e-05\n",
      "Step: 3440, train/learning_rate: 4.590671233017929e-05\n",
      "Step: 3440, train/epoch: 0.8186577558517456\n",
      "Step: 3450, train/loss: 0.0\n",
      "Step: 3450, train/grad_norm: 1.7064902522179182e-06\n",
      "Step: 3450, train/learning_rate: 4.589481250150129e-05\n",
      "Step: 3450, train/epoch: 0.8210375905036926\n",
      "Step: 3460, train/loss: 0.08500000089406967\n",
      "Step: 3460, train/grad_norm: 0.023406190797686577\n",
      "Step: 3460, train/learning_rate: 4.5882912672823295e-05\n",
      "Step: 3460, train/epoch: 0.8234174251556396\n",
      "Step: 3470, train/loss: 0.4526999890804291\n",
      "Step: 3470, train/grad_norm: 2.7320231311023235e-05\n",
      "Step: 3470, train/learning_rate: 4.58710128441453e-05\n",
      "Step: 3470, train/epoch: 0.8257972598075867\n",
      "Step: 3480, train/loss: 0.0\n",
      "Step: 3480, train/grad_norm: 0.00024015501549001783\n",
      "Step: 3480, train/learning_rate: 4.58591130154673e-05\n",
      "Step: 3480, train/epoch: 0.8281770348548889\n",
      "Step: 3490, train/loss: 0.008899999782443047\n",
      "Step: 3490, train/grad_norm: 5.934351065661758e-05\n",
      "Step: 3490, train/learning_rate: 4.584721682476811e-05\n",
      "Step: 3490, train/epoch: 0.8305568695068359\n",
      "Step: 3500, train/loss: 0.0\n",
      "Step: 3500, train/grad_norm: 7.038858740315845e-08\n",
      "Step: 3500, train/learning_rate: 4.5835316996090114e-05\n",
      "Step: 3500, train/epoch: 0.832936704158783\n",
      "Step: 3510, train/loss: 0.0\n",
      "Step: 3510, train/grad_norm: 1.7696624610152867e-08\n",
      "Step: 3510, train/learning_rate: 4.582341716741212e-05\n",
      "Step: 3510, train/epoch: 0.83531653881073\n",
      "Step: 3520, train/loss: 0.0003000000142492354\n",
      "Step: 3520, train/grad_norm: 1.3270323506731074e-06\n",
      "Step: 3520, train/learning_rate: 4.581151733873412e-05\n",
      "Step: 3520, train/epoch: 0.8376963138580322\n",
      "Step: 3530, train/loss: 0.0\n",
      "Step: 3530, train/grad_norm: 5.496999051501916e-07\n",
      "Step: 3530, train/learning_rate: 4.579961751005612e-05\n",
      "Step: 3530, train/epoch: 0.8400761485099792\n",
      "Step: 3540, train/loss: 0.1565999984741211\n",
      "Step: 3540, train/grad_norm: 3.337476300657727e-06\n",
      "Step: 3540, train/learning_rate: 4.578772131935693e-05\n",
      "Step: 3540, train/epoch: 0.8424559831619263\n",
      "Step: 3550, train/loss: 0.0\n",
      "Step: 3550, train/grad_norm: 1.0139178812096361e-05\n",
      "Step: 3550, train/learning_rate: 4.5775821490678936e-05\n",
      "Step: 3550, train/epoch: 0.8448358178138733\n",
      "Step: 3560, train/loss: 0.24060000479221344\n",
      "Step: 3560, train/grad_norm: 0.00021552063117269427\n",
      "Step: 3560, train/learning_rate: 4.576392166200094e-05\n",
      "Step: 3560, train/epoch: 0.8472155928611755\n",
      "Step: 3570, train/loss: 9.999999747378752e-05\n",
      "Step: 3570, train/grad_norm: 0.11918441206216812\n",
      "Step: 3570, train/learning_rate: 4.575202183332294e-05\n",
      "Step: 3570, train/epoch: 0.8495954275131226\n",
      "Step: 3580, train/loss: 9.999999747378752e-05\n",
      "Step: 3580, train/grad_norm: 0.0017372326692566276\n",
      "Step: 3580, train/learning_rate: 4.5740122004644945e-05\n",
      "Step: 3580, train/epoch: 0.8519752621650696\n",
      "Step: 3590, train/loss: 0.0\n",
      "Step: 3590, train/grad_norm: 0.001613233587704599\n",
      "Step: 3590, train/learning_rate: 4.5728225813945755e-05\n",
      "Step: 3590, train/epoch: 0.8543550968170166\n",
      "Step: 3600, train/loss: 0.0\n",
      "Step: 3600, train/grad_norm: 0.00022309996711555868\n",
      "Step: 3600, train/learning_rate: 4.571632598526776e-05\n",
      "Step: 3600, train/epoch: 0.8567348718643188\n",
      "Step: 3610, train/loss: 0.14839999377727509\n",
      "Step: 3610, train/grad_norm: 0.010502408258616924\n",
      "Step: 3610, train/learning_rate: 4.570442615658976e-05\n",
      "Step: 3610, train/epoch: 0.8591147065162659\n",
      "Step: 3620, train/loss: 9.999999747378752e-05\n",
      "Step: 3620, train/grad_norm: 0.0002046184818027541\n",
      "Step: 3620, train/learning_rate: 4.5692526327911764e-05\n",
      "Step: 3620, train/epoch: 0.8614945411682129\n",
      "Step: 3630, train/loss: 0.010300000198185444\n",
      "Step: 3630, train/grad_norm: 0.0021888602059334517\n",
      "Step: 3630, train/learning_rate: 4.568062649923377e-05\n",
      "Step: 3630, train/epoch: 0.8638743162155151\n",
      "Step: 3640, train/loss: 0.01810000091791153\n",
      "Step: 3640, train/grad_norm: 7.950767030706629e-05\n",
      "Step: 3640, train/learning_rate: 4.566873030853458e-05\n",
      "Step: 3640, train/epoch: 0.8662541508674622\n",
      "Step: 3650, train/loss: 0.0\n",
      "Step: 3650, train/grad_norm: 0.005216589197516441\n",
      "Step: 3650, train/learning_rate: 4.565683047985658e-05\n",
      "Step: 3650, train/epoch: 0.8686339855194092\n",
      "Step: 3660, train/loss: 0.0\n",
      "Step: 3660, train/grad_norm: 3.136040449142456\n",
      "Step: 3660, train/learning_rate: 4.5644930651178584e-05\n",
      "Step: 3660, train/epoch: 0.8710138201713562\n",
      "Step: 3670, train/loss: 0.2587999999523163\n",
      "Step: 3670, train/grad_norm: 0.14180319011211395\n",
      "Step: 3670, train/learning_rate: 4.5633030822500587e-05\n",
      "Step: 3670, train/epoch: 0.8733935952186584\n",
      "Step: 3680, train/loss: 0.12880000472068787\n",
      "Step: 3680, train/grad_norm: 0.00018479702703189105\n",
      "Step: 3680, train/learning_rate: 4.562113099382259e-05\n",
      "Step: 3680, train/epoch: 0.8757734298706055\n",
      "Step: 3690, train/loss: 0.010099999606609344\n",
      "Step: 3690, train/grad_norm: 8.34763704915531e-06\n",
      "Step: 3690, train/learning_rate: 4.56092348031234e-05\n",
      "Step: 3690, train/epoch: 0.8781532645225525\n",
      "Step: 3700, train/loss: 0.0\n",
      "Step: 3700, train/grad_norm: 0.00012526130012702197\n",
      "Step: 3700, train/learning_rate: 4.55973349744454e-05\n",
      "Step: 3700, train/epoch: 0.8805330991744995\n",
      "Step: 3710, train/loss: 0.35659998655319214\n",
      "Step: 3710, train/grad_norm: 4.8746886022854596e-05\n",
      "Step: 3710, train/learning_rate: 4.5585435145767406e-05\n",
      "Step: 3710, train/epoch: 0.8829128742218018\n",
      "Step: 3720, train/loss: 0.018200000748038292\n",
      "Step: 3720, train/grad_norm: 0.0005425740382634103\n",
      "Step: 3720, train/learning_rate: 4.557353531708941e-05\n",
      "Step: 3720, train/epoch: 0.8852927088737488\n",
      "Step: 3730, train/loss: 0.0\n",
      "Step: 3730, train/grad_norm: 0.001247188774868846\n",
      "Step: 3730, train/learning_rate: 4.556163912639022e-05\n",
      "Step: 3730, train/epoch: 0.8876725435256958\n",
      "Step: 3740, train/loss: 0.0\n",
      "Step: 3740, train/grad_norm: 1.5808721798293845e-07\n",
      "Step: 3740, train/learning_rate: 4.554973929771222e-05\n",
      "Step: 3740, train/epoch: 0.8900523781776428\n",
      "Step: 3750, train/loss: 0.0\n",
      "Step: 3750, train/grad_norm: 1.0574851216915704e-08\n",
      "Step: 3750, train/learning_rate: 4.5537839469034225e-05\n",
      "Step: 3750, train/epoch: 0.8924321532249451\n",
      "Step: 3760, train/loss: 0.0\n",
      "Step: 3760, train/grad_norm: 4.2299004121559847e-07\n",
      "Step: 3760, train/learning_rate: 4.552593964035623e-05\n",
      "Step: 3760, train/epoch: 0.8948119878768921\n",
      "Step: 3770, train/loss: 0.5\n",
      "Step: 3770, train/grad_norm: 8.774908577535712e-10\n",
      "Step: 3770, train/learning_rate: 4.551403981167823e-05\n",
      "Step: 3770, train/epoch: 0.8971918225288391\n",
      "Step: 3780, train/loss: 9.999999747378752e-05\n",
      "Step: 3780, train/grad_norm: 0.3567057251930237\n",
      "Step: 3780, train/learning_rate: 4.550214362097904e-05\n",
      "Step: 3780, train/epoch: 0.8995716571807861\n",
      "Step: 3790, train/loss: 0.0006000000284984708\n",
      "Step: 3790, train/grad_norm: 0.01143554411828518\n",
      "Step: 3790, train/learning_rate: 4.5490243792301044e-05\n",
      "Step: 3790, train/epoch: 0.9019514322280884\n",
      "Step: 3800, train/loss: 0.0\n",
      "Step: 3800, train/grad_norm: 0.0003407540498301387\n",
      "Step: 3800, train/learning_rate: 4.547834396362305e-05\n",
      "Step: 3800, train/epoch: 0.9043312668800354\n",
      "Step: 3810, train/loss: 0.0\n",
      "Step: 3810, train/grad_norm: 0.00012733765470329672\n",
      "Step: 3810, train/learning_rate: 4.546644413494505e-05\n",
      "Step: 3810, train/epoch: 0.9067111015319824\n",
      "Step: 3820, train/loss: 0.1500999927520752\n",
      "Step: 3820, train/grad_norm: 0.0018066799966618419\n",
      "Step: 3820, train/learning_rate: 4.545454430626705e-05\n",
      "Step: 3820, train/epoch: 0.9090909361839294\n",
      "Step: 3830, train/loss: 0.0\n",
      "Step: 3830, train/grad_norm: 0.014288639649748802\n",
      "Step: 3830, train/learning_rate: 4.544264811556786e-05\n",
      "Step: 3830, train/epoch: 0.9114707112312317\n",
      "Step: 3840, train/loss: 0.1834000051021576\n",
      "Step: 3840, train/grad_norm: 0.09453165531158447\n",
      "Step: 3840, train/learning_rate: 4.5430748286889866e-05\n",
      "Step: 3840, train/epoch: 0.9138505458831787\n",
      "Step: 3850, train/loss: 0.0\n",
      "Step: 3850, train/grad_norm: 0.006930147763341665\n",
      "Step: 3850, train/learning_rate: 4.541884845821187e-05\n",
      "Step: 3850, train/epoch: 0.9162303805351257\n",
      "Step: 3860, train/loss: 0.0\n",
      "Step: 3860, train/grad_norm: 0.03601817041635513\n",
      "Step: 3860, train/learning_rate: 4.540694862953387e-05\n",
      "Step: 3860, train/epoch: 0.9186102151870728\n",
      "Step: 3870, train/loss: 0.009200000204145908\n",
      "Step: 3870, train/grad_norm: 0.001602871110662818\n",
      "Step: 3870, train/learning_rate: 4.5395048800855875e-05\n",
      "Step: 3870, train/epoch: 0.920989990234375\n",
      "Step: 3880, train/loss: 0.17440000176429749\n",
      "Step: 3880, train/grad_norm: 0.0007653202628716826\n",
      "Step: 3880, train/learning_rate: 4.5383152610156685e-05\n",
      "Step: 3880, train/epoch: 0.923369824886322\n",
      "Step: 3890, train/loss: 0.09139999747276306\n",
      "Step: 3890, train/grad_norm: 0.0018988114316016436\n",
      "Step: 3890, train/learning_rate: 4.537125278147869e-05\n",
      "Step: 3890, train/epoch: 0.925749659538269\n",
      "Step: 3900, train/loss: 0.051500000059604645\n",
      "Step: 3900, train/grad_norm: 0.006567802745848894\n",
      "Step: 3900, train/learning_rate: 4.535935295280069e-05\n",
      "Step: 3900, train/epoch: 0.9281294345855713\n",
      "Step: 3910, train/loss: 0.0\n",
      "Step: 3910, train/grad_norm: 0.0075398217886686325\n",
      "Step: 3910, train/learning_rate: 4.5347453124122694e-05\n",
      "Step: 3910, train/epoch: 0.9305092692375183\n",
      "Step: 3920, train/loss: 0.0\n",
      "Step: 3920, train/grad_norm: 0.008041142486035824\n",
      "Step: 3920, train/learning_rate: 4.53355532954447e-05\n",
      "Step: 3920, train/epoch: 0.9328891038894653\n",
      "Step: 3930, train/loss: 0.001500000013038516\n",
      "Step: 3930, train/grad_norm: 1.033830165863037\n",
      "Step: 3930, train/learning_rate: 4.532365710474551e-05\n",
      "Step: 3930, train/epoch: 0.9352689385414124\n",
      "Step: 3940, train/loss: 0.0\n",
      "Step: 3940, train/grad_norm: 1.1454773812147323e-05\n",
      "Step: 3940, train/learning_rate: 4.531175727606751e-05\n",
      "Step: 3940, train/epoch: 0.9376487135887146\n",
      "Step: 3950, train/loss: 0.07699999958276749\n",
      "Step: 3950, train/grad_norm: 269.39349365234375\n",
      "Step: 3950, train/learning_rate: 4.529985744738951e-05\n",
      "Step: 3950, train/epoch: 0.9400285482406616\n",
      "Step: 3960, train/loss: 0.0\n",
      "Step: 3960, train/grad_norm: 3.442586603341624e-05\n",
      "Step: 3960, train/learning_rate: 4.5287957618711516e-05\n",
      "Step: 3960, train/epoch: 0.9424083828926086\n",
      "Step: 3970, train/loss: 0.13899999856948853\n",
      "Step: 3970, train/grad_norm: 11.091259002685547\n",
      "Step: 3970, train/learning_rate: 4.527605779003352e-05\n",
      "Step: 3970, train/epoch: 0.9447882175445557\n",
      "Step: 3980, train/loss: 0.10159999877214432\n",
      "Step: 3980, train/grad_norm: 3.014166395587381e-06\n",
      "Step: 3980, train/learning_rate: 4.526416159933433e-05\n",
      "Step: 3980, train/epoch: 0.9471679925918579\n",
      "Step: 3990, train/loss: 0.17659999430179596\n",
      "Step: 3990, train/grad_norm: 1.5486913980566896e-05\n",
      "Step: 3990, train/learning_rate: 4.525226177065633e-05\n",
      "Step: 3990, train/epoch: 0.9495478272438049\n",
      "Step: 4000, train/loss: 0.0\n",
      "Step: 4000, train/grad_norm: 4.905266541754827e-05\n",
      "Step: 4000, train/learning_rate: 4.5240361941978335e-05\n",
      "Step: 4000, train/epoch: 0.951927661895752\n",
      "Step: 4010, train/loss: 0.0\n",
      "Step: 4010, train/grad_norm: 0.00024145658244378865\n",
      "Step: 4010, train/learning_rate: 4.522846211330034e-05\n",
      "Step: 4010, train/epoch: 0.954307496547699\n",
      "Step: 4020, train/loss: 0.0\n",
      "Step: 4020, train/grad_norm: 4.1987157601397485e-05\n",
      "Step: 4020, train/learning_rate: 4.521656228462234e-05\n",
      "Step: 4020, train/epoch: 0.9566872715950012\n",
      "Step: 4030, train/loss: 0.125\n",
      "Step: 4030, train/grad_norm: 0.020794285461306572\n",
      "Step: 4030, train/learning_rate: 4.520466609392315e-05\n",
      "Step: 4030, train/epoch: 0.9590671062469482\n",
      "Step: 4040, train/loss: 9.999999747378752e-05\n",
      "Step: 4040, train/grad_norm: 0.001715456717647612\n",
      "Step: 4040, train/learning_rate: 4.5192766265245155e-05\n",
      "Step: 4040, train/epoch: 0.9614469408988953\n",
      "Step: 4050, train/loss: 9.999999747378752e-05\n",
      "Step: 4050, train/grad_norm: 1.0007875971496105e-05\n",
      "Step: 4050, train/learning_rate: 4.518086643656716e-05\n",
      "Step: 4050, train/epoch: 0.9638267755508423\n",
      "Step: 4060, train/loss: 0.10779999941587448\n",
      "Step: 4060, train/grad_norm: 0.00018278681091032922\n",
      "Step: 4060, train/learning_rate: 4.516896660788916e-05\n",
      "Step: 4060, train/epoch: 0.9662065505981445\n",
      "Step: 4070, train/loss: 0.2078000009059906\n",
      "Step: 4070, train/grad_norm: 2.5292123609688133e-05\n",
      "Step: 4070, train/learning_rate: 4.5157066779211164e-05\n",
      "Step: 4070, train/epoch: 0.9685863852500916\n",
      "Step: 4080, train/loss: 0.0\n",
      "Step: 4080, train/grad_norm: 0.0013962544035166502\n",
      "Step: 4080, train/learning_rate: 4.5145170588511974e-05\n",
      "Step: 4080, train/epoch: 0.9709662199020386\n",
      "Step: 4090, train/loss: 0.0008999999845400453\n",
      "Step: 4090, train/grad_norm: 7.85829615779221e-05\n",
      "Step: 4090, train/learning_rate: 4.513327075983398e-05\n",
      "Step: 4090, train/epoch: 0.9733460545539856\n",
      "Step: 4100, train/loss: 0.04749999940395355\n",
      "Step: 4100, train/grad_norm: 1.2068634532624856e-05\n",
      "Step: 4100, train/learning_rate: 4.512137093115598e-05\n",
      "Step: 4100, train/epoch: 0.9757258296012878\n",
      "Step: 4110, train/loss: 0.03359999880194664\n",
      "Step: 4110, train/grad_norm: 0.003133133752271533\n",
      "Step: 4110, train/learning_rate: 4.510947110247798e-05\n",
      "Step: 4110, train/epoch: 0.9781056642532349\n",
      "Step: 4120, train/loss: 9.999999747378752e-05\n",
      "Step: 4120, train/grad_norm: 1.2425141903804615e-05\n",
      "Step: 4120, train/learning_rate: 4.5097571273799986e-05\n",
      "Step: 4120, train/epoch: 0.9804854989051819\n",
      "Step: 4130, train/loss: 0.06639999896287918\n",
      "Step: 4130, train/grad_norm: 0.0005537715624086559\n",
      "Step: 4130, train/learning_rate: 4.5085675083100796e-05\n",
      "Step: 4130, train/epoch: 0.9828652739524841\n",
      "Step: 4140, train/loss: 0.0\n",
      "Step: 4140, train/grad_norm: 3.8727783248759806e-05\n",
      "Step: 4140, train/learning_rate: 4.50737752544228e-05\n",
      "Step: 4140, train/epoch: 0.9852451086044312\n",
      "Step: 4150, train/loss: 9.999999747378752e-05\n",
      "Step: 4150, train/grad_norm: 0.00025599580840207636\n",
      "Step: 4150, train/learning_rate: 4.50618754257448e-05\n",
      "Step: 4150, train/epoch: 0.9876249432563782\n",
      "Step: 4160, train/loss: 0.049400001764297485\n",
      "Step: 4160, train/grad_norm: 160.4295196533203\n",
      "Step: 4160, train/learning_rate: 4.5049975597066805e-05\n",
      "Step: 4160, train/epoch: 0.9900047779083252\n",
      "Step: 4170, train/loss: 9.999999747378752e-05\n",
      "Step: 4170, train/grad_norm: 5.481596758727392e-07\n",
      "Step: 4170, train/learning_rate: 4.503807576838881e-05\n",
      "Step: 4170, train/epoch: 0.9923845529556274\n",
      "Step: 4180, train/loss: 0.0\n",
      "Step: 4180, train/grad_norm: 8.51828076520178e-07\n",
      "Step: 4180, train/learning_rate: 4.502617957768962e-05\n",
      "Step: 4180, train/epoch: 0.9947643876075745\n",
      "Step: 4190, train/loss: 0.18240000307559967\n",
      "Step: 4190, train/grad_norm: 4.905977675662143e-06\n",
      "Step: 4190, train/learning_rate: 4.501427974901162e-05\n",
      "Step: 4190, train/epoch: 0.9971442222595215\n",
      "Step: 4200, train/loss: 0.4194999933242798\n",
      "Step: 4200, train/grad_norm: 0.000457312649814412\n",
      "Step: 4200, train/learning_rate: 4.5002379920333624e-05\n",
      "Step: 4200, train/epoch: 0.9995240569114685\n",
      "Step: 4202, eval/loss: 0.07811713963747025\n",
      "Step: 4202, eval/accuracy: 0.9894488453865051\n",
      "Step: 4202, eval/f1: 0.9888052940368652\n",
      "Step: 4202, eval/runtime: 856.7728881835938\n",
      "Step: 4202, eval/samples_per_second: 8.406999588012695\n",
      "Step: 4202, eval/steps_per_second: 1.0520000457763672\n",
      "Step: 4202, train/epoch: 1.0\n",
      "Step: 4210, train/loss: 0.03849999979138374\n",
      "Step: 4210, train/grad_norm: 0.03066447004675865\n",
      "Step: 4210, train/learning_rate: 4.499048009165563e-05\n",
      "Step: 4210, train/epoch: 1.0019038915634155\n",
      "Step: 4220, train/loss: 0.17110000550746918\n",
      "Step: 4220, train/grad_norm: 0.00038248029886744916\n",
      "Step: 4220, train/learning_rate: 4.497858026297763e-05\n",
      "Step: 4220, train/epoch: 1.0042836666107178\n",
      "Step: 4230, train/loss: 0.0\n",
      "Step: 4230, train/grad_norm: 3.888839273713529e-05\n",
      "Step: 4230, train/learning_rate: 4.496668407227844e-05\n",
      "Step: 4230, train/epoch: 1.00666344165802\n",
      "Step: 4240, train/loss: 0.0\n",
      "Step: 4240, train/grad_norm: 4.2547009797999635e-05\n",
      "Step: 4240, train/learning_rate: 4.495478424360044e-05\n",
      "Step: 4240, train/epoch: 1.0090433359146118\n",
      "Step: 4250, train/loss: 0.00139999995008111\n",
      "Step: 4250, train/grad_norm: 2.6021914436569205e-06\n",
      "Step: 4250, train/learning_rate: 4.4942884414922446e-05\n",
      "Step: 4250, train/epoch: 1.011423110961914\n",
      "Step: 4260, train/loss: 0.04729999974370003\n",
      "Step: 4260, train/grad_norm: 2.716428518295288\n",
      "Step: 4260, train/learning_rate: 4.493098458624445e-05\n",
      "Step: 4260, train/epoch: 1.0138030052185059\n",
      "Step: 4270, train/loss: 0.0\n",
      "Step: 4270, train/grad_norm: 0.016650618985295296\n",
      "Step: 4270, train/learning_rate: 4.491908475756645e-05\n",
      "Step: 4270, train/epoch: 1.016182780265808\n",
      "Step: 4280, train/loss: 0.529699981212616\n",
      "Step: 4280, train/grad_norm: 0.00016720037092454731\n",
      "Step: 4280, train/learning_rate: 4.490718856686726e-05\n",
      "Step: 4280, train/epoch: 1.0185625553131104\n",
      "Step: 4290, train/loss: 0.0\n",
      "Step: 4290, train/grad_norm: 0.005650630686432123\n",
      "Step: 4290, train/learning_rate: 4.4895288738189265e-05\n",
      "Step: 4290, train/epoch: 1.0209424495697021\n",
      "Step: 4300, train/loss: 0.0005000000237487257\n",
      "Step: 4300, train/grad_norm: 0.004930906929075718\n",
      "Step: 4300, train/learning_rate: 4.488338890951127e-05\n",
      "Step: 4300, train/epoch: 1.0233222246170044\n",
      "Step: 4310, train/loss: 0.0\n",
      "Step: 4310, train/grad_norm: 0.00018884339078795165\n",
      "Step: 4310, train/learning_rate: 4.487148908083327e-05\n",
      "Step: 4310, train/epoch: 1.0257019996643066\n",
      "Step: 4320, train/loss: 0.0\n",
      "Step: 4320, train/grad_norm: 6.605355883948505e-05\n",
      "Step: 4320, train/learning_rate: 4.4859589252155274e-05\n",
      "Step: 4320, train/epoch: 1.0280818939208984\n",
      "Step: 4330, train/loss: 0.0\n",
      "Step: 4330, train/grad_norm: 0.00019199380767531693\n",
      "Step: 4330, train/learning_rate: 4.4847693061456084e-05\n",
      "Step: 4330, train/epoch: 1.0304616689682007\n",
      "Step: 4340, train/loss: 0.13439999520778656\n",
      "Step: 4340, train/grad_norm: 0.0006806341698393226\n",
      "Step: 4340, train/learning_rate: 4.483579323277809e-05\n",
      "Step: 4340, train/epoch: 1.0328415632247925\n",
      "Step: 4350, train/loss: 0.0\n",
      "Step: 4350, train/grad_norm: 0.00027786626014858484\n",
      "Step: 4350, train/learning_rate: 4.482389340410009e-05\n",
      "Step: 4350, train/epoch: 1.0352213382720947\n",
      "Step: 4360, train/loss: 0.02539999969303608\n",
      "Step: 4360, train/grad_norm: 1.9146598788211122e-05\n",
      "Step: 4360, train/learning_rate: 4.481199357542209e-05\n",
      "Step: 4360, train/epoch: 1.037601113319397\n",
      "Step: 4370, train/loss: 0.0\n",
      "Step: 4370, train/grad_norm: 0.00019243612769059837\n",
      "Step: 4370, train/learning_rate: 4.4800093746744096e-05\n",
      "Step: 4370, train/epoch: 1.0399810075759888\n",
      "Step: 4380, train/loss: 0.0\n",
      "Step: 4380, train/grad_norm: 0.00015130701649468392\n",
      "Step: 4380, train/learning_rate: 4.4788197556044906e-05\n",
      "Step: 4380, train/epoch: 1.042360782623291\n",
      "Step: 4390, train/loss: 0.0007999999797903001\n",
      "Step: 4390, train/grad_norm: 6.172674329718575e-05\n",
      "Step: 4390, train/learning_rate: 4.477629772736691e-05\n",
      "Step: 4390, train/epoch: 1.0447405576705933\n",
      "Step: 4400, train/loss: 0.0\n",
      "Step: 4400, train/grad_norm: 0.29621046781539917\n",
      "Step: 4400, train/learning_rate: 4.476439789868891e-05\n",
      "Step: 4400, train/epoch: 1.047120451927185\n",
      "Step: 4410, train/loss: 0.006599999964237213\n",
      "Step: 4410, train/grad_norm: 104.14884948730469\n",
      "Step: 4410, train/learning_rate: 4.4752498070010915e-05\n",
      "Step: 4410, train/epoch: 1.0495002269744873\n",
      "Step: 4420, train/loss: 0.10819999873638153\n",
      "Step: 4420, train/grad_norm: 14.390249252319336\n",
      "Step: 4420, train/learning_rate: 4.474059824133292e-05\n",
      "Step: 4420, train/epoch: 1.0518800020217896\n",
      "Step: 4430, train/loss: 0.002099999925121665\n",
      "Step: 4430, train/grad_norm: 4.9075511924456805e-05\n",
      "Step: 4430, train/learning_rate: 4.472870205063373e-05\n",
      "Step: 4430, train/epoch: 1.0542598962783813\n",
      "Step: 4440, train/loss: 0.0\n",
      "Step: 4440, train/grad_norm: 6.965226930333301e-05\n",
      "Step: 4440, train/learning_rate: 4.471680222195573e-05\n",
      "Step: 4440, train/epoch: 1.0566396713256836\n",
      "Step: 4450, train/loss: 0.0\n",
      "Step: 4450, train/grad_norm: 7.823167834430933e-05\n",
      "Step: 4450, train/learning_rate: 4.4704902393277735e-05\n",
      "Step: 4450, train/epoch: 1.0590195655822754\n",
      "Step: 4460, train/loss: 0.06639999896287918\n",
      "Step: 4460, train/grad_norm: 301.1583251953125\n",
      "Step: 4460, train/learning_rate: 4.469300256459974e-05\n",
      "Step: 4460, train/epoch: 1.0613993406295776\n",
      "Step: 4470, train/loss: 0.0\n",
      "Step: 4470, train/grad_norm: 0.0676165521144867\n",
      "Step: 4470, train/learning_rate: 4.468110273592174e-05\n",
      "Step: 4470, train/epoch: 1.0637791156768799\n",
      "Step: 4480, train/loss: 0.006800000090152025\n",
      "Step: 4480, train/grad_norm: 3.8193360524019226e-05\n",
      "Step: 4480, train/learning_rate: 4.466920654522255e-05\n",
      "Step: 4480, train/epoch: 1.0661590099334717\n",
      "Step: 4490, train/loss: 9.999999747378752e-05\n",
      "Step: 4490, train/grad_norm: 0.00017875016783364117\n",
      "Step: 4490, train/learning_rate: 4.4657306716544554e-05\n",
      "Step: 4490, train/epoch: 1.068538784980774\n",
      "Step: 4500, train/loss: 0.0\n",
      "Step: 4500, train/grad_norm: 0.003853221656754613\n",
      "Step: 4500, train/learning_rate: 4.464540688786656e-05\n",
      "Step: 4500, train/epoch: 1.0709185600280762\n",
      "Step: 4510, train/loss: 0.0\n",
      "Step: 4510, train/grad_norm: 0.00015016880934126675\n",
      "Step: 4510, train/learning_rate: 4.463350705918856e-05\n",
      "Step: 4510, train/epoch: 1.073298454284668\n",
      "Step: 4520, train/loss: 0.12110000103712082\n",
      "Step: 4520, train/grad_norm: 0.004427692387253046\n",
      "Step: 4520, train/learning_rate: 4.462160723051056e-05\n",
      "Step: 4520, train/epoch: 1.0756782293319702\n",
      "Step: 4530, train/loss: 0.0\n",
      "Step: 4530, train/grad_norm: 9.020572906592861e-05\n",
      "Step: 4530, train/learning_rate: 4.460971103981137e-05\n",
      "Step: 4530, train/epoch: 1.078058123588562\n",
      "Step: 4540, train/loss: 0.0\n",
      "Step: 4540, train/grad_norm: 0.0002982672885991633\n",
      "Step: 4540, train/learning_rate: 4.4597811211133376e-05\n",
      "Step: 4540, train/epoch: 1.0804378986358643\n",
      "Step: 4550, train/loss: 0.0\n",
      "Step: 4550, train/grad_norm: 0.0028068043757230043\n",
      "Step: 4550, train/learning_rate: 4.458591138245538e-05\n",
      "Step: 4550, train/epoch: 1.0828176736831665\n",
      "Step: 4560, train/loss: 0.08829999715089798\n",
      "Step: 4560, train/grad_norm: 3.0292907467810437e-05\n",
      "Step: 4560, train/learning_rate: 4.457401155377738e-05\n",
      "Step: 4560, train/epoch: 1.0851975679397583\n",
      "Step: 4570, train/loss: 0.007600000128149986\n",
      "Step: 4570, train/grad_norm: 80.71331787109375\n",
      "Step: 4570, train/learning_rate: 4.4562111725099385e-05\n",
      "Step: 4570, train/epoch: 1.0875773429870605\n",
      "Step: 4580, train/loss: 0.09319999814033508\n",
      "Step: 4580, train/grad_norm: 0.00035716366255655885\n",
      "Step: 4580, train/learning_rate: 4.4550215534400195e-05\n",
      "Step: 4580, train/epoch: 1.0899571180343628\n",
      "Step: 4590, train/loss: 0.03280000016093254\n",
      "Step: 4590, train/grad_norm: 0.0015550575917586684\n",
      "Step: 4590, train/learning_rate: 4.45383157057222e-05\n",
      "Step: 4590, train/epoch: 1.0923370122909546\n",
      "Step: 4600, train/loss: 0.1200999990105629\n",
      "Step: 4600, train/grad_norm: 0.020727885887026787\n",
      "Step: 4600, train/learning_rate: 4.45264158770442e-05\n",
      "Step: 4600, train/epoch: 1.0947167873382568\n",
      "Step: 4610, train/loss: 0.0\n",
      "Step: 4610, train/grad_norm: 0.0001688793854555115\n",
      "Step: 4610, train/learning_rate: 4.4514516048366204e-05\n",
      "Step: 4610, train/epoch: 1.097096562385559\n",
      "Step: 4620, train/loss: 0.0\n",
      "Step: 4620, train/grad_norm: 2.6538398287812015e-06\n",
      "Step: 4620, train/learning_rate: 4.450261621968821e-05\n",
      "Step: 4620, train/epoch: 1.0994764566421509\n",
      "Step: 4630, train/loss: 0.06800000369548798\n",
      "Step: 4630, train/grad_norm: 6.979842055443441e-07\n",
      "Step: 4630, train/learning_rate: 4.449072002898902e-05\n",
      "Step: 4630, train/epoch: 1.1018562316894531\n",
      "Step: 4640, train/loss: 0.060499999672174454\n",
      "Step: 4640, train/grad_norm: 4.4335996790323406e-05\n",
      "Step: 4640, train/learning_rate: 4.447882020031102e-05\n",
      "Step: 4640, train/epoch: 1.104236125946045\n",
      "Step: 4650, train/loss: 0.0\n",
      "Step: 4650, train/grad_norm: 0.00026706731296144426\n",
      "Step: 4650, train/learning_rate: 4.446692037163302e-05\n",
      "Step: 4650, train/epoch: 1.1066159009933472\n",
      "Step: 4660, train/loss: 0.0\n",
      "Step: 4660, train/grad_norm: 0.001668054610490799\n",
      "Step: 4660, train/learning_rate: 4.4455020542955026e-05\n",
      "Step: 4660, train/epoch: 1.1089956760406494\n",
      "Step: 4670, train/loss: 0.015300000086426735\n",
      "Step: 4670, train/grad_norm: 7.146098505472764e-05\n",
      "Step: 4670, train/learning_rate: 4.444312071427703e-05\n",
      "Step: 4670, train/epoch: 1.1113755702972412\n",
      "Step: 4680, train/loss: 0.3698999881744385\n",
      "Step: 4680, train/grad_norm: 0.012962586246430874\n",
      "Step: 4680, train/learning_rate: 4.443122452357784e-05\n",
      "Step: 4680, train/epoch: 1.1137553453445435\n",
      "Step: 4690, train/loss: 9.999999747378752e-05\n",
      "Step: 4690, train/grad_norm: 0.10248050093650818\n",
      "Step: 4690, train/learning_rate: 4.441932469489984e-05\n",
      "Step: 4690, train/epoch: 1.1161351203918457\n",
      "Step: 4700, train/loss: 0.07490000128746033\n",
      "Step: 4700, train/grad_norm: 0.019445881247520447\n",
      "Step: 4700, train/learning_rate: 4.4407424866221845e-05\n",
      "Step: 4700, train/epoch: 1.1185150146484375\n",
      "Step: 4710, train/loss: 9.999999747378752e-05\n",
      "Step: 4710, train/grad_norm: 0.005871967878192663\n",
      "Step: 4710, train/learning_rate: 4.439552503754385e-05\n",
      "Step: 4710, train/epoch: 1.1208947896957397\n",
      "Step: 4720, train/loss: 9.999999747378752e-05\n",
      "Step: 4720, train/grad_norm: 0.00010054063750430942\n",
      "Step: 4720, train/learning_rate: 4.438362520886585e-05\n",
      "Step: 4720, train/epoch: 1.1232746839523315\n",
      "Step: 4730, train/loss: 0.0\n",
      "Step: 4730, train/grad_norm: 9.396618406753987e-05\n",
      "Step: 4730, train/learning_rate: 4.437172901816666e-05\n",
      "Step: 4730, train/epoch: 1.1256544589996338\n",
      "Step: 4740, train/loss: 0.0\n",
      "Step: 4740, train/grad_norm: 6.299492815742269e-05\n",
      "Step: 4740, train/learning_rate: 4.4359829189488664e-05\n",
      "Step: 4740, train/epoch: 1.128034234046936\n",
      "Step: 4750, train/loss: 0.10779999941587448\n",
      "Step: 4750, train/grad_norm: 3.7695642731705448e-06\n",
      "Step: 4750, train/learning_rate: 4.434792936081067e-05\n",
      "Step: 4750, train/epoch: 1.1304141283035278\n",
      "Step: 4760, train/loss: 0.0\n",
      "Step: 4760, train/grad_norm: 5.594351750914939e-05\n",
      "Step: 4760, train/learning_rate: 4.433602953213267e-05\n",
      "Step: 4760, train/epoch: 1.13279390335083\n",
      "Step: 4770, train/loss: 0.030300000682473183\n",
      "Step: 4770, train/grad_norm: 1.1956160051340703e-05\n",
      "Step: 4770, train/learning_rate: 4.432412970345467e-05\n",
      "Step: 4770, train/epoch: 1.1351736783981323\n",
      "Step: 4780, train/loss: 0.00019999999494757503\n",
      "Step: 4780, train/grad_norm: 3.631434202194214\n",
      "Step: 4780, train/learning_rate: 4.4312233512755483e-05\n",
      "Step: 4780, train/epoch: 1.1375535726547241\n",
      "Step: 4790, train/loss: 0.0\n",
      "Step: 4790, train/grad_norm: 3.1546001991955563e-05\n",
      "Step: 4790, train/learning_rate: 4.4300333684077486e-05\n",
      "Step: 4790, train/epoch: 1.1399333477020264\n",
      "Step: 4800, train/loss: 9.999999747378752e-05\n",
      "Step: 4800, train/grad_norm: 1.1856942364829592e-05\n",
      "Step: 4800, train/learning_rate: 4.428843385539949e-05\n",
      "Step: 4800, train/epoch: 1.1423132419586182\n",
      "Step: 4810, train/loss: 0.0\n",
      "Step: 4810, train/grad_norm: 0.00037461204919964075\n",
      "Step: 4810, train/learning_rate: 4.427653402672149e-05\n",
      "Step: 4810, train/epoch: 1.1446930170059204\n",
      "Step: 4820, train/loss: 0.0\n",
      "Step: 4820, train/grad_norm: 8.586291187384631e-06\n",
      "Step: 4820, train/learning_rate: 4.4264634198043495e-05\n",
      "Step: 4820, train/epoch: 1.1470727920532227\n",
      "Step: 4830, train/loss: 0.014399999752640724\n",
      "Step: 4830, train/grad_norm: 2.7190392302145483e-06\n",
      "Step: 4830, train/learning_rate: 4.4252738007344306e-05\n",
      "Step: 4830, train/epoch: 1.1494526863098145\n",
      "Step: 4840, train/loss: 0.0\n",
      "Step: 4840, train/grad_norm: 5.820382648380473e-05\n",
      "Step: 4840, train/learning_rate: 4.424083817866631e-05\n",
      "Step: 4840, train/epoch: 1.1518324613571167\n",
      "Step: 4850, train/loss: 0.0\n",
      "Step: 4850, train/grad_norm: 2.466544799517578e-07\n",
      "Step: 4850, train/learning_rate: 4.422893834998831e-05\n",
      "Step: 4850, train/epoch: 1.154212236404419\n",
      "Step: 4860, train/loss: 0.16949999332427979\n",
      "Step: 4860, train/grad_norm: 0.00017895735800266266\n",
      "Step: 4860, train/learning_rate: 4.4217038521310315e-05\n",
      "Step: 4860, train/epoch: 1.1565921306610107\n",
      "Step: 4870, train/loss: 0.0\n",
      "Step: 4870, train/grad_norm: 0.00826228130608797\n",
      "Step: 4870, train/learning_rate: 4.420513869263232e-05\n",
      "Step: 4870, train/epoch: 1.158971905708313\n",
      "Step: 4880, train/loss: 9.999999747378752e-05\n",
      "Step: 4880, train/grad_norm: 0.0020276177674531937\n",
      "Step: 4880, train/learning_rate: 4.419324250193313e-05\n",
      "Step: 4880, train/epoch: 1.1613516807556152\n",
      "Step: 4890, train/loss: 0.0\n",
      "Step: 4890, train/grad_norm: 0.0016120468499138951\n",
      "Step: 4890, train/learning_rate: 4.418134267325513e-05\n",
      "Step: 4890, train/epoch: 1.163731575012207\n",
      "Step: 4900, train/loss: 0.0\n",
      "Step: 4900, train/grad_norm: 0.00019175188208464533\n",
      "Step: 4900, train/learning_rate: 4.4169442844577134e-05\n",
      "Step: 4900, train/epoch: 1.1661113500595093\n",
      "Step: 4910, train/loss: 0.0\n",
      "Step: 4910, train/grad_norm: 0.004564099945127964\n",
      "Step: 4910, train/learning_rate: 4.415754301589914e-05\n",
      "Step: 4910, train/epoch: 1.168491244316101\n",
      "Step: 4920, train/loss: 0.0010999999940395355\n",
      "Step: 4920, train/grad_norm: 0.02659069001674652\n",
      "Step: 4920, train/learning_rate: 4.414564318722114e-05\n",
      "Step: 4920, train/epoch: 1.1708710193634033\n",
      "Step: 4930, train/loss: 0.022299999371170998\n",
      "Step: 4930, train/grad_norm: 1.3280594430398196e-05\n",
      "Step: 4930, train/learning_rate: 4.413374699652195e-05\n",
      "Step: 4930, train/epoch: 1.1732507944107056\n",
      "Step: 4940, train/loss: 0.0\n",
      "Step: 4940, train/grad_norm: 0.00010357294377172366\n",
      "Step: 4940, train/learning_rate: 4.412184716784395e-05\n",
      "Step: 4940, train/epoch: 1.1756306886672974\n",
      "Step: 4950, train/loss: 0.0\n",
      "Step: 4950, train/grad_norm: 6.975611086090794e-06\n",
      "Step: 4950, train/learning_rate: 4.4109947339165956e-05\n",
      "Step: 4950, train/epoch: 1.1780104637145996\n",
      "Step: 4960, train/loss: 0.07199999690055847\n",
      "Step: 4960, train/grad_norm: 1.9438859453657642e-05\n",
      "Step: 4960, train/learning_rate: 4.409804751048796e-05\n",
      "Step: 4960, train/epoch: 1.1803902387619019\n",
      "Step: 4970, train/loss: 0.38280001282691956\n",
      "Step: 4970, train/grad_norm: 0.0018652967410162091\n",
      "Step: 4970, train/learning_rate: 4.408614768180996e-05\n",
      "Step: 4970, train/epoch: 1.1827701330184937\n",
      "Step: 4980, train/loss: 9.999999747378752e-05\n",
      "Step: 4980, train/grad_norm: 0.006552183534950018\n",
      "Step: 4980, train/learning_rate: 4.407425149111077e-05\n",
      "Step: 4980, train/epoch: 1.185149908065796\n",
      "Step: 4990, train/loss: 0.016499999910593033\n",
      "Step: 4990, train/grad_norm: 0.0021547165233641863\n",
      "Step: 4990, train/learning_rate: 4.4062351662432775e-05\n",
      "Step: 4990, train/epoch: 1.1875298023223877\n",
      "Step: 5000, train/loss: 9.999999747378752e-05\n",
      "Step: 5000, train/grad_norm: 0.0019792811945080757\n",
      "Step: 5000, train/learning_rate: 4.405045183375478e-05\n",
      "Step: 5000, train/epoch: 1.18990957736969\n",
      "Step: 5010, train/loss: 0.04830000177025795\n",
      "Step: 5010, train/grad_norm: 0.003330845618620515\n",
      "Step: 5010, train/learning_rate: 4.403855200507678e-05\n",
      "Step: 5010, train/epoch: 1.1922893524169922\n",
      "Step: 5020, train/loss: 0.0\n",
      "Step: 5020, train/grad_norm: 0.0024923058226704597\n",
      "Step: 5020, train/learning_rate: 4.4026652176398784e-05\n",
      "Step: 5020, train/epoch: 1.194669246673584\n",
      "Step: 5030, train/loss: 0.004800000227987766\n",
      "Step: 5030, train/grad_norm: 0.08049246668815613\n",
      "Step: 5030, train/learning_rate: 4.4014755985699594e-05\n",
      "Step: 5030, train/epoch: 1.1970490217208862\n",
      "Step: 5040, train/loss: 0.0012000000569969416\n",
      "Step: 5040, train/grad_norm: 0.009514128789305687\n",
      "Step: 5040, train/learning_rate: 4.40028561570216e-05\n",
      "Step: 5040, train/epoch: 1.1994287967681885\n",
      "Step: 5050, train/loss: 0.2134000062942505\n",
      "Step: 5050, train/grad_norm: 0.00046463601756840944\n",
      "Step: 5050, train/learning_rate: 4.39909563283436e-05\n",
      "Step: 5050, train/epoch: 1.2018086910247803\n",
      "Step: 5060, train/loss: 0.0\n",
      "Step: 5060, train/grad_norm: 0.004094590898603201\n",
      "Step: 5060, train/learning_rate: 4.39790564996656e-05\n",
      "Step: 5060, train/epoch: 1.2041884660720825\n",
      "Step: 5070, train/loss: 0.0\n",
      "Step: 5070, train/grad_norm: 0.007700545713305473\n",
      "Step: 5070, train/learning_rate: 4.396716030896641e-05\n",
      "Step: 5070, train/epoch: 1.2065683603286743\n",
      "Step: 5080, train/loss: 0.0\n",
      "Step: 5080, train/grad_norm: 0.00033530572545714676\n",
      "Step: 5080, train/learning_rate: 4.3955260480288416e-05\n",
      "Step: 5080, train/epoch: 1.2089481353759766\n",
      "Step: 5090, train/loss: 0.20309999585151672\n",
      "Step: 5090, train/grad_norm: 160.0530242919922\n",
      "Step: 5090, train/learning_rate: 4.394336065161042e-05\n",
      "Step: 5090, train/epoch: 1.2113279104232788\n",
      "Step: 5100, train/loss: 0.12359999865293503\n",
      "Step: 5100, train/grad_norm: 0.018261834979057312\n",
      "Step: 5100, train/learning_rate: 4.393146082293242e-05\n",
      "Step: 5100, train/epoch: 1.2137078046798706\n",
      "Step: 5110, train/loss: 0.03200000151991844\n",
      "Step: 5110, train/grad_norm: 0.22046729922294617\n",
      "Step: 5110, train/learning_rate: 4.3919560994254425e-05\n",
      "Step: 5110, train/epoch: 1.2160875797271729\n",
      "Step: 5120, train/loss: 0.23100000619888306\n",
      "Step: 5120, train/grad_norm: 2.3599302768707275\n",
      "Step: 5120, train/learning_rate: 4.3907664803555235e-05\n",
      "Step: 5120, train/epoch: 1.218467354774475\n",
      "Step: 5130, train/loss: 0.0\n",
      "Step: 5130, train/grad_norm: 0.0006935329874977469\n",
      "Step: 5130, train/learning_rate: 4.389576497487724e-05\n",
      "Step: 5130, train/epoch: 1.220847249031067\n",
      "Step: 5140, train/loss: 0.0\n",
      "Step: 5140, train/grad_norm: 9.37157722091797e-07\n",
      "Step: 5140, train/learning_rate: 4.388386514619924e-05\n",
      "Step: 5140, train/epoch: 1.2232270240783691\n",
      "Step: 5150, train/loss: 0.2312999963760376\n",
      "Step: 5150, train/grad_norm: 0.0012050035875290632\n",
      "Step: 5150, train/learning_rate: 4.3871965317521244e-05\n",
      "Step: 5150, train/epoch: 1.2256067991256714\n",
      "Step: 5160, train/loss: 9.999999747378752e-05\n",
      "Step: 5160, train/grad_norm: 6.16179677308537e-05\n",
      "Step: 5160, train/learning_rate: 4.386006548884325e-05\n",
      "Step: 5160, train/epoch: 1.2279866933822632\n",
      "Step: 5170, train/loss: 0.0\n",
      "Step: 5170, train/grad_norm: 5.133726153871976e-05\n",
      "Step: 5170, train/learning_rate: 4.384816929814406e-05\n",
      "Step: 5170, train/epoch: 1.2303664684295654\n",
      "Step: 5180, train/loss: 0.0\n",
      "Step: 5180, train/grad_norm: 2.0026403944939375e-05\n",
      "Step: 5180, train/learning_rate: 4.383626946946606e-05\n",
      "Step: 5180, train/epoch: 1.2327463626861572\n",
      "Step: 5190, train/loss: 0.0\n",
      "Step: 5190, train/grad_norm: 0.0001317161659244448\n",
      "Step: 5190, train/learning_rate: 4.3824369640788063e-05\n",
      "Step: 5190, train/epoch: 1.2351261377334595\n",
      "Step: 5200, train/loss: 0.0\n",
      "Step: 5200, train/grad_norm: 0.00022645889839623123\n",
      "Step: 5200, train/learning_rate: 4.3812469812110066e-05\n",
      "Step: 5200, train/epoch: 1.2375059127807617\n",
      "Step: 5210, train/loss: 0.0\n",
      "Step: 5210, train/grad_norm: 0.00010161146201426163\n",
      "Step: 5210, train/learning_rate: 4.380056998343207e-05\n",
      "Step: 5210, train/epoch: 1.2398858070373535\n",
      "Step: 5220, train/loss: 0.0\n",
      "Step: 5220, train/grad_norm: 0.0001071184451575391\n",
      "Step: 5220, train/learning_rate: 4.378867379273288e-05\n",
      "Step: 5220, train/epoch: 1.2422655820846558\n",
      "Step: 5230, train/loss: 0.06369999796152115\n",
      "Step: 5230, train/grad_norm: 3.642939418568858e-06\n",
      "Step: 5230, train/learning_rate: 4.377677396405488e-05\n",
      "Step: 5230, train/epoch: 1.244645357131958\n",
      "Step: 5240, train/loss: 0.0\n",
      "Step: 5240, train/grad_norm: 0.0051925950683653355\n",
      "Step: 5240, train/learning_rate: 4.3764874135376886e-05\n",
      "Step: 5240, train/epoch: 1.2470252513885498\n",
      "Step: 5250, train/loss: 0.0\n",
      "Step: 5250, train/grad_norm: 5.9717462136177346e-05\n",
      "Step: 5250, train/learning_rate: 4.375297430669889e-05\n",
      "Step: 5250, train/epoch: 1.249405026435852\n",
      "Step: 5260, train/loss: 0.0\n",
      "Step: 5260, train/grad_norm: 1.8200364138465375e-05\n",
      "Step: 5260, train/learning_rate: 4.374107447802089e-05\n",
      "Step: 5260, train/epoch: 1.2517849206924438\n",
      "Step: 5270, train/loss: 0.0\n",
      "Step: 5270, train/grad_norm: 1.3045879313722253e-05\n",
      "Step: 5270, train/learning_rate: 4.37291782873217e-05\n",
      "Step: 5270, train/epoch: 1.254164695739746\n",
      "Step: 5280, train/loss: 0.0\n",
      "Step: 5280, train/grad_norm: 2.047500129265245e-05\n",
      "Step: 5280, train/learning_rate: 4.3717278458643705e-05\n",
      "Step: 5280, train/epoch: 1.2565444707870483\n",
      "Step: 5290, train/loss: 0.2969000041484833\n",
      "Step: 5290, train/grad_norm: 74.20464324951172\n",
      "Step: 5290, train/learning_rate: 4.370537862996571e-05\n",
      "Step: 5290, train/epoch: 1.2589243650436401\n",
      "Step: 5300, train/loss: 0.2605000138282776\n",
      "Step: 5300, train/grad_norm: 0.11638721823692322\n",
      "Step: 5300, train/learning_rate: 4.369347880128771e-05\n",
      "Step: 5300, train/epoch: 1.2613041400909424\n",
      "Step: 5310, train/loss: 0.1988999992609024\n",
      "Step: 5310, train/grad_norm: 0.003814121475443244\n",
      "Step: 5310, train/learning_rate: 4.3681578972609714e-05\n",
      "Step: 5310, train/epoch: 1.2636839151382446\n",
      "Step: 5320, train/loss: 0.08569999784231186\n",
      "Step: 5320, train/grad_norm: 0.004227305296808481\n",
      "Step: 5320, train/learning_rate: 4.3669682781910524e-05\n",
      "Step: 5320, train/epoch: 1.2660638093948364\n",
      "Step: 5330, train/loss: 0.060600001364946365\n",
      "Step: 5330, train/grad_norm: 0.02141692116856575\n",
      "Step: 5330, train/learning_rate: 4.365778295323253e-05\n",
      "Step: 5330, train/epoch: 1.2684435844421387\n",
      "Step: 5340, train/loss: 0.0471000000834465\n",
      "Step: 5340, train/grad_norm: 0.03770636394619942\n",
      "Step: 5340, train/learning_rate: 4.364588312455453e-05\n",
      "Step: 5340, train/epoch: 1.270823359489441\n",
      "Step: 5350, train/loss: 0.0\n",
      "Step: 5350, train/grad_norm: 0.00012625064118765295\n",
      "Step: 5350, train/learning_rate: 4.363398329587653e-05\n",
      "Step: 5350, train/epoch: 1.2732032537460327\n",
      "Step: 5360, train/loss: 0.13300000131130219\n",
      "Step: 5360, train/grad_norm: 6.839662091806531e-05\n",
      "Step: 5360, train/learning_rate: 4.3622083467198536e-05\n",
      "Step: 5360, train/epoch: 1.275583028793335\n",
      "Step: 5370, train/loss: 9.999999747378752e-05\n",
      "Step: 5370, train/grad_norm: 0.00035188719630241394\n",
      "Step: 5370, train/learning_rate: 4.3610187276499346e-05\n",
      "Step: 5370, train/epoch: 1.2779629230499268\n",
      "Step: 5380, train/loss: 9.999999747378752e-05\n",
      "Step: 5380, train/grad_norm: 0.0002222344628535211\n",
      "Step: 5380, train/learning_rate: 4.359828744782135e-05\n",
      "Step: 5380, train/epoch: 1.280342698097229\n",
      "Step: 5390, train/loss: 0.0\n",
      "Step: 5390, train/grad_norm: 8.382364467252046e-05\n",
      "Step: 5390, train/learning_rate: 4.358638761914335e-05\n",
      "Step: 5390, train/epoch: 1.2827224731445312\n",
      "Step: 5400, train/loss: 0.0\n",
      "Step: 5400, train/grad_norm: 1.7596776160644367e-05\n",
      "Step: 5400, train/learning_rate: 4.3574487790465355e-05\n",
      "Step: 5400, train/epoch: 1.285102367401123\n",
      "Step: 5410, train/loss: 0.1632000058889389\n",
      "Step: 5410, train/grad_norm: 336.615234375\n",
      "Step: 5410, train/learning_rate: 4.356258796178736e-05\n",
      "Step: 5410, train/epoch: 1.2874821424484253\n",
      "Step: 5420, train/loss: 0.049400001764297485\n",
      "Step: 5420, train/grad_norm: 0.015840010717511177\n",
      "Step: 5420, train/learning_rate: 4.355069177108817e-05\n",
      "Step: 5420, train/epoch: 1.2898619174957275\n",
      "Step: 5430, train/loss: 0.024000000208616257\n",
      "Step: 5430, train/grad_norm: 0.3013461232185364\n",
      "Step: 5430, train/learning_rate: 4.353879194241017e-05\n",
      "Step: 5430, train/epoch: 1.2922418117523193\n",
      "Step: 5440, train/loss: 0.0\n",
      "Step: 5440, train/grad_norm: 0.03893618285655975\n",
      "Step: 5440, train/learning_rate: 4.3526892113732174e-05\n",
      "Step: 5440, train/epoch: 1.2946215867996216\n",
      "Step: 5450, train/loss: 0.0019000000320374966\n",
      "Step: 5450, train/grad_norm: 0.00041992974001914263\n",
      "Step: 5450, train/learning_rate: 4.351499228505418e-05\n",
      "Step: 5450, train/epoch: 1.2970014810562134\n",
      "Step: 5460, train/loss: 0.0\n",
      "Step: 5460, train/grad_norm: 2.5873541176224535e-07\n",
      "Step: 5460, train/learning_rate: 4.350309245637618e-05\n",
      "Step: 5460, train/epoch: 1.2993812561035156\n",
      "Step: 5470, train/loss: 0.0\n",
      "Step: 5470, train/grad_norm: 7.161197572713718e-05\n",
      "Step: 5470, train/learning_rate: 4.349119626567699e-05\n",
      "Step: 5470, train/epoch: 1.3017610311508179\n",
      "Step: 5480, train/loss: 0.0\n",
      "Step: 5480, train/grad_norm: 2.6355404770583846e-05\n",
      "Step: 5480, train/learning_rate: 4.347929643699899e-05\n",
      "Step: 5480, train/epoch: 1.3041409254074097\n",
      "Step: 5490, train/loss: 0.0\n",
      "Step: 5490, train/grad_norm: 3.6851968161499826e-06\n",
      "Step: 5490, train/learning_rate: 4.3467396608320996e-05\n",
      "Step: 5490, train/epoch: 1.306520700454712\n",
      "Step: 5500, train/loss: 0.0\n",
      "Step: 5500, train/grad_norm: 9.150305231742095e-06\n",
      "Step: 5500, train/learning_rate: 4.3455496779643e-05\n",
      "Step: 5500, train/epoch: 1.3089004755020142\n",
      "Step: 5510, train/loss: 0.006500000134110451\n",
      "Step: 5510, train/grad_norm: 8.320256711158436e-06\n",
      "Step: 5510, train/learning_rate: 4.3443596950965e-05\n",
      "Step: 5510, train/epoch: 1.311280369758606\n",
      "Step: 5520, train/loss: 0.0\n",
      "Step: 5520, train/grad_norm: 9.941658873913184e-08\n",
      "Step: 5520, train/learning_rate: 4.343170076026581e-05\n",
      "Step: 5520, train/epoch: 1.3136601448059082\n",
      "Step: 5530, train/loss: 9.999999747378752e-05\n",
      "Step: 5530, train/grad_norm: 2.4758146537351422e-05\n",
      "Step: 5530, train/learning_rate: 4.3419800931587815e-05\n",
      "Step: 5530, train/epoch: 1.3160400390625\n",
      "Step: 5540, train/loss: 0.15700000524520874\n",
      "Step: 5540, train/grad_norm: 0.00022670315229333937\n",
      "Step: 5540, train/learning_rate: 4.340790110290982e-05\n",
      "Step: 5540, train/epoch: 1.3184198141098022\n",
      "Step: 5550, train/loss: 0.006500000134110451\n",
      "Step: 5550, train/grad_norm: 0.03948187455534935\n",
      "Step: 5550, train/learning_rate: 4.339600127423182e-05\n",
      "Step: 5550, train/epoch: 1.3207995891571045\n",
      "Step: 5560, train/loss: 0.1679999977350235\n",
      "Step: 5560, train/grad_norm: 0.020181845873594284\n",
      "Step: 5560, train/learning_rate: 4.3384101445553824e-05\n",
      "Step: 5560, train/epoch: 1.3231794834136963\n",
      "Step: 5570, train/loss: 0.11020000278949738\n",
      "Step: 5570, train/grad_norm: 0.02309572510421276\n",
      "Step: 5570, train/learning_rate: 4.3372205254854634e-05\n",
      "Step: 5570, train/epoch: 1.3255592584609985\n",
      "Step: 5580, train/loss: 0.09030000120401382\n",
      "Step: 5580, train/grad_norm: 0.0009061798336915672\n",
      "Step: 5580, train/learning_rate: 4.336030542617664e-05\n",
      "Step: 5580, train/epoch: 1.3279390335083008\n",
      "Step: 5590, train/loss: 0.11959999799728394\n",
      "Step: 5590, train/grad_norm: 97.86980438232422\n",
      "Step: 5590, train/learning_rate: 4.334840559749864e-05\n",
      "Step: 5590, train/epoch: 1.3303189277648926\n",
      "Step: 5600, train/loss: 0.002099999925121665\n",
      "Step: 5600, train/grad_norm: 0.02928469516336918\n",
      "Step: 5600, train/learning_rate: 4.3336505768820643e-05\n",
      "Step: 5600, train/epoch: 1.3326987028121948\n",
      "Step: 5610, train/loss: 9.999999747378752e-05\n",
      "Step: 5610, train/grad_norm: 0.007318519055843353\n",
      "Step: 5610, train/learning_rate: 4.3324605940142646e-05\n",
      "Step: 5610, train/epoch: 1.335078477859497\n",
      "Step: 5620, train/loss: 0.0\n",
      "Step: 5620, train/grad_norm: 0.0001282350713154301\n",
      "Step: 5620, train/learning_rate: 4.3312709749443457e-05\n",
      "Step: 5620, train/epoch: 1.3374583721160889\n",
      "Step: 5630, train/loss: 0.1468999981880188\n",
      "Step: 5630, train/grad_norm: 0.00011434008774813265\n",
      "Step: 5630, train/learning_rate: 4.330080992076546e-05\n",
      "Step: 5630, train/epoch: 1.3398381471633911\n",
      "Step: 5640, train/loss: 0.001500000013038516\n",
      "Step: 5640, train/grad_norm: 0.00025628431467339396\n",
      "Step: 5640, train/learning_rate: 4.328891009208746e-05\n",
      "Step: 5640, train/epoch: 1.342218041419983\n",
      "Step: 5650, train/loss: 0.0\n",
      "Step: 5650, train/grad_norm: 0.005013379734009504\n",
      "Step: 5650, train/learning_rate: 4.3277010263409466e-05\n",
      "Step: 5650, train/epoch: 1.3445978164672852\n",
      "Step: 5660, train/loss: 0.0\n",
      "Step: 5660, train/grad_norm: 0.0020209101494401693\n",
      "Step: 5660, train/learning_rate: 4.326511043473147e-05\n",
      "Step: 5660, train/epoch: 1.3469775915145874\n",
      "Step: 5670, train/loss: 0.2281000018119812\n",
      "Step: 5670, train/grad_norm: 0.004925235640257597\n",
      "Step: 5670, train/learning_rate: 4.325321424403228e-05\n",
      "Step: 5670, train/epoch: 1.3493574857711792\n",
      "Step: 5680, train/loss: 0.0008999999845400453\n",
      "Step: 5680, train/grad_norm: 0.016357949003577232\n",
      "Step: 5680, train/learning_rate: 4.324131441535428e-05\n",
      "Step: 5680, train/epoch: 1.3517372608184814\n",
      "Step: 5690, train/loss: 0.20000000298023224\n",
      "Step: 5690, train/grad_norm: 0.0010731070069596171\n",
      "Step: 5690, train/learning_rate: 4.3229414586676285e-05\n",
      "Step: 5690, train/epoch: 1.3541170358657837\n",
      "Step: 5700, train/loss: 0.0\n",
      "Step: 5700, train/grad_norm: 0.0009694389882497489\n",
      "Step: 5700, train/learning_rate: 4.321751475799829e-05\n",
      "Step: 5700, train/epoch: 1.3564969301223755\n",
      "Step: 5710, train/loss: 0.0\n",
      "Step: 5710, train/grad_norm: 0.0005129041965119541\n",
      "Step: 5710, train/learning_rate: 4.320561492932029e-05\n",
      "Step: 5710, train/epoch: 1.3588767051696777\n",
      "Step: 5720, train/loss: 0.0\n",
      "Step: 5720, train/grad_norm: 0.09393736720085144\n",
      "Step: 5720, train/learning_rate: 4.31937187386211e-05\n",
      "Step: 5720, train/epoch: 1.3612565994262695\n",
      "Step: 5730, train/loss: 0.2281000018119812\n",
      "Step: 5730, train/grad_norm: 0.005562688689678907\n",
      "Step: 5730, train/learning_rate: 4.3181818909943104e-05\n",
      "Step: 5730, train/epoch: 1.3636363744735718\n",
      "Step: 5740, train/loss: 0.17900000512599945\n",
      "Step: 5740, train/grad_norm: 72.62284088134766\n",
      "Step: 5740, train/learning_rate: 4.316991908126511e-05\n",
      "Step: 5740, train/epoch: 1.366016149520874\n",
      "Step: 5750, train/loss: 0.30820000171661377\n",
      "Step: 5750, train/grad_norm: 0.00040219203219749033\n",
      "Step: 5750, train/learning_rate: 4.315801925258711e-05\n",
      "Step: 5750, train/epoch: 1.3683960437774658\n",
      "Step: 5760, train/loss: 0.0\n",
      "Step: 5760, train/grad_norm: 6.641199812307264e-10\n",
      "Step: 5760, train/learning_rate: 4.314611942390911e-05\n",
      "Step: 5760, train/epoch: 1.370775818824768\n",
      "Step: 5770, train/loss: 0.03790000081062317\n",
      "Step: 5770, train/grad_norm: 1.5498591210416635e-06\n",
      "Step: 5770, train/learning_rate: 4.313422323320992e-05\n",
      "Step: 5770, train/epoch: 1.3731555938720703\n",
      "Step: 5780, train/loss: 0.025499999523162842\n",
      "Step: 5780, train/grad_norm: 3.283052865299396e-05\n",
      "Step: 5780, train/learning_rate: 4.3122323404531926e-05\n",
      "Step: 5780, train/epoch: 1.375535488128662\n",
      "Step: 5790, train/loss: 0.27970001101493835\n",
      "Step: 5790, train/grad_norm: 96.32846069335938\n",
      "Step: 5790, train/learning_rate: 4.311042357585393e-05\n",
      "Step: 5790, train/epoch: 1.3779152631759644\n",
      "Step: 5800, train/loss: 0.0003000000142492354\n",
      "Step: 5800, train/grad_norm: 5.899348138882488e-07\n",
      "Step: 5800, train/learning_rate: 4.309852374717593e-05\n",
      "Step: 5800, train/epoch: 1.3802950382232666\n",
      "Step: 5810, train/loss: 0.09449999779462814\n",
      "Step: 5810, train/grad_norm: 1.4959821328375256e-06\n",
      "Step: 5810, train/learning_rate: 4.3086623918497935e-05\n",
      "Step: 5810, train/epoch: 1.3826749324798584\n",
      "Step: 5820, train/loss: 0.0\n",
      "Step: 5820, train/grad_norm: 0.04549365118145943\n",
      "Step: 5820, train/learning_rate: 4.3074727727798745e-05\n",
      "Step: 5820, train/epoch: 1.3850547075271606\n",
      "Step: 5830, train/loss: 0.26350000500679016\n",
      "Step: 5830, train/grad_norm: 0.0014944947324693203\n",
      "Step: 5830, train/learning_rate: 4.306282789912075e-05\n",
      "Step: 5830, train/epoch: 1.3874346017837524\n",
      "Step: 5840, train/loss: 0.0003000000142492354\n",
      "Step: 5840, train/grad_norm: 2.200077772140503\n",
      "Step: 5840, train/learning_rate: 4.305092807044275e-05\n",
      "Step: 5840, train/epoch: 1.3898143768310547\n",
      "Step: 5850, train/loss: 0.0\n",
      "Step: 5850, train/grad_norm: 5.147818592377007e-05\n",
      "Step: 5850, train/learning_rate: 4.3039028241764754e-05\n",
      "Step: 5850, train/epoch: 1.392194151878357\n",
      "Step: 5860, train/loss: 0.0\n",
      "Step: 5860, train/grad_norm: 8.37860643514432e-05\n",
      "Step: 5860, train/learning_rate: 4.302712841308676e-05\n",
      "Step: 5860, train/epoch: 1.3945740461349487\n",
      "Step: 5870, train/loss: 0.14920000731945038\n",
      "Step: 5870, train/grad_norm: 0.03802506625652313\n",
      "Step: 5870, train/learning_rate: 4.301523222238757e-05\n",
      "Step: 5870, train/epoch: 1.396953821182251\n",
      "Step: 5880, train/loss: 0.00139999995008111\n",
      "Step: 5880, train/grad_norm: 24.89610481262207\n",
      "Step: 5880, train/learning_rate: 4.300333239370957e-05\n",
      "Step: 5880, train/epoch: 1.3993335962295532\n",
      "Step: 5890, train/loss: 0.09369999915361404\n",
      "Step: 5890, train/grad_norm: 32.813575744628906\n",
      "Step: 5890, train/learning_rate: 4.299143256503157e-05\n",
      "Step: 5890, train/epoch: 1.401713490486145\n",
      "Step: 5900, train/loss: 0.9702000021934509\n",
      "Step: 5900, train/grad_norm: 568.1401977539062\n",
      "Step: 5900, train/learning_rate: 4.2979532736353576e-05\n",
      "Step: 5900, train/epoch: 1.4040932655334473\n",
      "Step: 5910, train/loss: 0.0\n",
      "Step: 5910, train/grad_norm: 1.5133320985238896e-10\n",
      "Step: 5910, train/learning_rate: 4.296763290767558e-05\n",
      "Step: 5910, train/epoch: 1.406473159790039\n",
      "Step: 5920, train/loss: 0.053199999034404755\n",
      "Step: 5920, train/grad_norm: 3.836877837670727e-08\n",
      "Step: 5920, train/learning_rate: 4.295573671697639e-05\n",
      "Step: 5920, train/epoch: 1.4088529348373413\n",
      "Step: 5930, train/loss: 0.34689998626708984\n",
      "Step: 5930, train/grad_norm: 4.785063456580474e-09\n",
      "Step: 5930, train/learning_rate: 4.294383688829839e-05\n",
      "Step: 5930, train/epoch: 1.4112327098846436\n",
      "Step: 5940, train/loss: 0.0\n",
      "Step: 5940, train/grad_norm: 6.363084139593411e-07\n",
      "Step: 5940, train/learning_rate: 4.2931937059620395e-05\n",
      "Step: 5940, train/epoch: 1.4136126041412354\n",
      "Step: 5950, train/loss: 9.999999747378752e-05\n",
      "Step: 5950, train/grad_norm: 0.0016608586302027106\n",
      "Step: 5950, train/learning_rate: 4.29200372309424e-05\n",
      "Step: 5950, train/epoch: 1.4159923791885376\n",
      "Step: 5960, train/loss: 0.0\n",
      "Step: 5960, train/grad_norm: 0.027997395023703575\n",
      "Step: 5960, train/learning_rate: 4.29081374022644e-05\n",
      "Step: 5960, train/epoch: 1.4183721542358398\n",
      "Step: 5970, train/loss: 0.15620000660419464\n",
      "Step: 5970, train/grad_norm: 4.854614973903715e-12\n",
      "Step: 5970, train/learning_rate: 4.289624121156521e-05\n",
      "Step: 5970, train/epoch: 1.4207520484924316\n",
      "Step: 5980, train/loss: 0.30630001425743103\n",
      "Step: 5980, train/grad_norm: 205.0541229248047\n",
      "Step: 5980, train/learning_rate: 4.2884341382887214e-05\n",
      "Step: 5980, train/epoch: 1.4231318235397339\n",
      "Step: 5990, train/loss: 0.2079000025987625\n",
      "Step: 5990, train/grad_norm: 6.805726051330566\n",
      "Step: 5990, train/learning_rate: 4.287244155420922e-05\n",
      "Step: 5990, train/epoch: 1.4255117177963257\n",
      "Step: 6000, train/loss: 0.005900000222027302\n",
      "Step: 6000, train/grad_norm: 1.2274791515665129e-05\n",
      "Step: 6000, train/learning_rate: 4.286054172553122e-05\n",
      "Step: 6000, train/epoch: 1.427891492843628\n",
      "Step: 6010, train/loss: 0.1265999972820282\n",
      "Step: 6010, train/grad_norm: 1.9374184034859354e-07\n",
      "Step: 6010, train/learning_rate: 4.2848641896853223e-05\n",
      "Step: 6010, train/epoch: 1.4302712678909302\n",
      "Step: 6020, train/loss: 0.23280000686645508\n",
      "Step: 6020, train/grad_norm: 1.5693458976784314e-07\n",
      "Step: 6020, train/learning_rate: 4.2836745706154034e-05\n",
      "Step: 6020, train/epoch: 1.432651162147522\n",
      "Step: 6030, train/loss: 0.2094999998807907\n",
      "Step: 6030, train/grad_norm: 0.00023700798919890076\n",
      "Step: 6030, train/learning_rate: 4.2824845877476037e-05\n",
      "Step: 6030, train/epoch: 1.4350309371948242\n",
      "Step: 6040, train/loss: 0.11949999630451202\n",
      "Step: 6040, train/grad_norm: 0.0007618373492732644\n",
      "Step: 6040, train/learning_rate: 4.281294604879804e-05\n",
      "Step: 6040, train/epoch: 1.4374107122421265\n",
      "Step: 6050, train/loss: 0.0\n",
      "Step: 6050, train/grad_norm: 0.005118643399327993\n",
      "Step: 6050, train/learning_rate: 4.280104622012004e-05\n",
      "Step: 6050, train/epoch: 1.4397906064987183\n",
      "Step: 6060, train/loss: 0.019099999219179153\n",
      "Step: 6060, train/grad_norm: 0.015595409087836742\n",
      "Step: 6060, train/learning_rate: 4.2789146391442046e-05\n",
      "Step: 6060, train/epoch: 1.4421703815460205\n",
      "Step: 6070, train/loss: 0.031300000846385956\n",
      "Step: 6070, train/grad_norm: 0.0021927435882389545\n",
      "Step: 6070, train/learning_rate: 4.2777250200742856e-05\n",
      "Step: 6070, train/epoch: 1.4445501565933228\n",
      "Step: 6080, train/loss: 9.999999747378752e-05\n",
      "Step: 6080, train/grad_norm: 0.01450207270681858\n",
      "Step: 6080, train/learning_rate: 4.276535037206486e-05\n",
      "Step: 6080, train/epoch: 1.4469300508499146\n",
      "Step: 6090, train/loss: 0.12380000203847885\n",
      "Step: 6090, train/grad_norm: 194.5623321533203\n",
      "Step: 6090, train/learning_rate: 4.275345054338686e-05\n",
      "Step: 6090, train/epoch: 1.4493098258972168\n",
      "Step: 6100, train/loss: 0.0\n",
      "Step: 6100, train/grad_norm: 2.909321892730077e-06\n",
      "Step: 6100, train/learning_rate: 4.2741550714708865e-05\n",
      "Step: 6100, train/epoch: 1.4516897201538086\n",
      "Step: 6110, train/loss: 0.0\n",
      "Step: 6110, train/grad_norm: 9.27863766264636e-06\n",
      "Step: 6110, train/learning_rate: 4.272965088603087e-05\n",
      "Step: 6110, train/epoch: 1.4540694952011108\n",
      "Step: 6120, train/loss: 0.0\n",
      "Step: 6120, train/grad_norm: 3.8790392864029855e-05\n",
      "Step: 6120, train/learning_rate: 4.271775469533168e-05\n",
      "Step: 6120, train/epoch: 1.456449270248413\n",
      "Step: 6130, train/loss: 0.056299999356269836\n",
      "Step: 6130, train/grad_norm: 2.6491172320675105e-05\n",
      "Step: 6130, train/learning_rate: 4.270585486665368e-05\n",
      "Step: 6130, train/epoch: 1.4588291645050049\n",
      "Step: 6140, train/loss: 0.0\n",
      "Step: 6140, train/grad_norm: 3.172625383740524e-06\n",
      "Step: 6140, train/learning_rate: 4.2693955037975684e-05\n",
      "Step: 6140, train/epoch: 1.4612089395523071\n",
      "Step: 6150, train/loss: 0.0\n",
      "Step: 6150, train/grad_norm: 0.0005083055584691465\n",
      "Step: 6150, train/learning_rate: 4.268205520929769e-05\n",
      "Step: 6150, train/epoch: 1.4635887145996094\n",
      "Step: 6160, train/loss: 0.0\n",
      "Step: 6160, train/grad_norm: 0.002398950280621648\n",
      "Step: 6160, train/learning_rate: 4.267015538061969e-05\n",
      "Step: 6160, train/epoch: 1.4659686088562012\n",
      "Step: 6170, train/loss: 0.05979999899864197\n",
      "Step: 6170, train/grad_norm: 0.0010980656370520592\n",
      "Step: 6170, train/learning_rate: 4.26582591899205e-05\n",
      "Step: 6170, train/epoch: 1.4683483839035034\n",
      "Step: 6180, train/loss: 0.0\n",
      "Step: 6180, train/grad_norm: 0.0008036128710955381\n",
      "Step: 6180, train/learning_rate: 4.26463593612425e-05\n",
      "Step: 6180, train/epoch: 1.4707282781600952\n",
      "Step: 6190, train/loss: 0.0\n",
      "Step: 6190, train/grad_norm: 0.00012014510139124468\n",
      "Step: 6190, train/learning_rate: 4.2634459532564506e-05\n",
      "Step: 6190, train/epoch: 1.4731080532073975\n",
      "Step: 6200, train/loss: 0.03709999844431877\n",
      "Step: 6200, train/grad_norm: 4.4874428795083077e-07\n",
      "Step: 6200, train/learning_rate: 4.262255970388651e-05\n",
      "Step: 6200, train/epoch: 1.4754878282546997\n",
      "Step: 6210, train/loss: 0.0\n",
      "Step: 6210, train/grad_norm: 6.628621486015618e-05\n",
      "Step: 6210, train/learning_rate: 4.261065987520851e-05\n",
      "Step: 6210, train/epoch: 1.4778677225112915\n",
      "Step: 6220, train/loss: 0.0\n",
      "Step: 6220, train/grad_norm: 7.752147212158889e-06\n",
      "Step: 6220, train/learning_rate: 4.259876368450932e-05\n",
      "Step: 6220, train/epoch: 1.4802474975585938\n",
      "Step: 6230, train/loss: 0.0\n",
      "Step: 6230, train/grad_norm: 1.0184145139646716e-05\n",
      "Step: 6230, train/learning_rate: 4.2586863855831325e-05\n",
      "Step: 6230, train/epoch: 1.482627272605896\n",
      "Step: 6240, train/loss: 0.0\n",
      "Step: 6240, train/grad_norm: 0.00011193421232746914\n",
      "Step: 6240, train/learning_rate: 4.257496402715333e-05\n",
      "Step: 6240, train/epoch: 1.4850071668624878\n",
      "Step: 6250, train/loss: 0.1671999990940094\n",
      "Step: 6250, train/grad_norm: 0.0025323403533548117\n",
      "Step: 6250, train/learning_rate: 4.256306419847533e-05\n",
      "Step: 6250, train/epoch: 1.48738694190979\n",
      "Step: 6260, train/loss: 0.0\n",
      "Step: 6260, train/grad_norm: 0.012002803385257721\n",
      "Step: 6260, train/learning_rate: 4.2551164369797334e-05\n",
      "Step: 6260, train/epoch: 1.4897668361663818\n",
      "Step: 6270, train/loss: 0.0\n",
      "Step: 6270, train/grad_norm: 0.011792073957622051\n",
      "Step: 6270, train/learning_rate: 4.2539268179098144e-05\n",
      "Step: 6270, train/epoch: 1.492146611213684\n",
      "Step: 6280, train/loss: 0.23280000686645508\n",
      "Step: 6280, train/grad_norm: 0.001585805439390242\n",
      "Step: 6280, train/learning_rate: 4.252736835042015e-05\n",
      "Step: 6280, train/epoch: 1.4945263862609863\n",
      "Step: 6290, train/loss: 0.010900000110268593\n",
      "Step: 6290, train/grad_norm: 0.11649448424577713\n",
      "Step: 6290, train/learning_rate: 4.251546852174215e-05\n",
      "Step: 6290, train/epoch: 1.4969062805175781\n",
      "Step: 6300, train/loss: 0.0\n",
      "Step: 6300, train/grad_norm: 0.00014298726455308497\n",
      "Step: 6300, train/learning_rate: 4.250356869306415e-05\n",
      "Step: 6300, train/epoch: 1.4992860555648804\n",
      "Step: 6310, train/loss: 0.10830000042915344\n",
      "Step: 6310, train/grad_norm: 0.002563151065260172\n",
      "Step: 6310, train/learning_rate: 4.2491668864386156e-05\n",
      "Step: 6310, train/epoch: 1.5016658306121826\n",
      "Step: 6320, train/loss: 0.09529999643564224\n",
      "Step: 6320, train/grad_norm: 0.0004473291046451777\n",
      "Step: 6320, train/learning_rate: 4.2479772673686966e-05\n",
      "Step: 6320, train/epoch: 1.5040457248687744\n",
      "Step: 6330, train/loss: 0.01940000057220459\n",
      "Step: 6330, train/grad_norm: 0.00027976252022199333\n",
      "Step: 6330, train/learning_rate: 4.246787284500897e-05\n",
      "Step: 6330, train/epoch: 1.5064254999160767\n",
      "Step: 6340, train/loss: 0.0\n",
      "Step: 6340, train/grad_norm: 5.5412350775441155e-05\n",
      "Step: 6340, train/learning_rate: 4.245597301633097e-05\n",
      "Step: 6340, train/epoch: 1.508805274963379\n",
      "Step: 6350, train/loss: 0.0\n",
      "Step: 6350, train/grad_norm: 9.101852629100904e-05\n",
      "Step: 6350, train/learning_rate: 4.2444073187652975e-05\n",
      "Step: 6350, train/epoch: 1.5111851692199707\n",
      "Step: 6360, train/loss: 0.0\n",
      "Step: 6360, train/grad_norm: 0.0010204884456470609\n",
      "Step: 6360, train/learning_rate: 4.243217335897498e-05\n",
      "Step: 6360, train/epoch: 1.513564944267273\n",
      "Step: 6370, train/loss: 0.1265999972820282\n",
      "Step: 6370, train/grad_norm: 0.0011344091035425663\n",
      "Step: 6370, train/learning_rate: 4.242027716827579e-05\n",
      "Step: 6370, train/epoch: 1.5159448385238647\n",
      "Step: 6380, train/loss: 0.0\n",
      "Step: 6380, train/grad_norm: 0.0005599758587777615\n",
      "Step: 6380, train/learning_rate: 4.240837733959779e-05\n",
      "Step: 6380, train/epoch: 1.518324613571167\n",
      "Step: 6390, train/loss: 0.11339999735355377\n",
      "Step: 6390, train/grad_norm: 1.770568609237671\n",
      "Step: 6390, train/learning_rate: 4.2396477510919794e-05\n",
      "Step: 6390, train/epoch: 1.5207043886184692\n",
      "Step: 6400, train/loss: 9.999999747378752e-05\n",
      "Step: 6400, train/grad_norm: 0.014231228269636631\n",
      "Step: 6400, train/learning_rate: 4.23845776822418e-05\n",
      "Step: 6400, train/epoch: 1.523084282875061\n",
      "Step: 6410, train/loss: 0.0035000001080334187\n",
      "Step: 6410, train/grad_norm: 123.16385650634766\n",
      "Step: 6410, train/learning_rate: 4.237268149154261e-05\n",
      "Step: 6410, train/epoch: 1.5254640579223633\n",
      "Step: 6420, train/loss: 0.10779999941587448\n",
      "Step: 6420, train/grad_norm: 0.0012804475845769048\n",
      "Step: 6420, train/learning_rate: 4.236078166286461e-05\n",
      "Step: 6420, train/epoch: 1.5278438329696655\n",
      "Step: 6430, train/loss: 0.0\n",
      "Step: 6430, train/grad_norm: 0.00627873232588172\n",
      "Step: 6430, train/learning_rate: 4.2348881834186614e-05\n",
      "Step: 6430, train/epoch: 1.5302237272262573\n",
      "Step: 6440, train/loss: 9.999999747378752e-05\n",
      "Step: 6440, train/grad_norm: 0.002266263123601675\n",
      "Step: 6440, train/learning_rate: 4.2336982005508617e-05\n",
      "Step: 6440, train/epoch: 1.5326035022735596\n",
      "Step: 6450, train/loss: 0.0835999995470047\n",
      "Step: 6450, train/grad_norm: 0.003669304773211479\n",
      "Step: 6450, train/learning_rate: 4.232508217683062e-05\n",
      "Step: 6450, train/epoch: 1.5349833965301514\n",
      "Step: 6460, train/loss: 0.0\n",
      "Step: 6460, train/grad_norm: 0.009772923775017262\n",
      "Step: 6460, train/learning_rate: 4.231318598613143e-05\n",
      "Step: 6460, train/epoch: 1.5373631715774536\n",
      "Step: 6470, train/loss: 0.0\n",
      "Step: 6470, train/grad_norm: 0.022458216175436974\n",
      "Step: 6470, train/learning_rate: 4.230128615745343e-05\n",
      "Step: 6470, train/epoch: 1.5397429466247559\n",
      "Step: 6480, train/loss: 0.08749999850988388\n",
      "Step: 6480, train/grad_norm: 0.005794619210064411\n",
      "Step: 6480, train/learning_rate: 4.2289386328775436e-05\n",
      "Step: 6480, train/epoch: 1.5421228408813477\n",
      "Step: 6490, train/loss: 0.0010000000474974513\n",
      "Step: 6490, train/grad_norm: 0.0016369037330150604\n",
      "Step: 6490, train/learning_rate: 4.227748650009744e-05\n",
      "Step: 6490, train/epoch: 1.54450261592865\n",
      "Step: 6500, train/loss: 9.999999747378752e-05\n",
      "Step: 6500, train/grad_norm: 0.0026639122515916824\n",
      "Step: 6500, train/learning_rate: 4.226558667141944e-05\n",
      "Step: 6500, train/epoch: 1.5468823909759521\n",
      "Step: 6510, train/loss: 0.19850000739097595\n",
      "Step: 6510, train/grad_norm: 0.035377901047468185\n",
      "Step: 6510, train/learning_rate: 4.225369048072025e-05\n",
      "Step: 6510, train/epoch: 1.549262285232544\n",
      "Step: 6520, train/loss: 9.999999747378752e-05\n",
      "Step: 6520, train/grad_norm: 0.02662181295454502\n",
      "Step: 6520, train/learning_rate: 4.2241790652042255e-05\n",
      "Step: 6520, train/epoch: 1.5516420602798462\n",
      "Step: 6530, train/loss: 0.0\n",
      "Step: 6530, train/grad_norm: 0.01697954535484314\n",
      "Step: 6530, train/learning_rate: 4.222989082336426e-05\n",
      "Step: 6530, train/epoch: 1.5540218353271484\n",
      "Step: 6540, train/loss: 0.0\n",
      "Step: 6540, train/grad_norm: 0.0008887083386071026\n",
      "Step: 6540, train/learning_rate: 4.221799099468626e-05\n",
      "Step: 6540, train/epoch: 1.5564017295837402\n",
      "Step: 6550, train/loss: 0.10159999877214432\n",
      "Step: 6550, train/grad_norm: 0.06192880496382713\n",
      "Step: 6550, train/learning_rate: 4.2206091166008264e-05\n",
      "Step: 6550, train/epoch: 1.5587815046310425\n",
      "Step: 6560, train/loss: 0.00039999998989515007\n",
      "Step: 6560, train/grad_norm: 1.983980417251587\n",
      "Step: 6560, train/learning_rate: 4.2194194975309074e-05\n",
      "Step: 6560, train/epoch: 1.5611613988876343\n",
      "Step: 6570, train/loss: 0.14139999449253082\n",
      "Step: 6570, train/grad_norm: 0.023335326462984085\n",
      "Step: 6570, train/learning_rate: 4.218229514663108e-05\n",
      "Step: 6570, train/epoch: 1.5635411739349365\n",
      "Step: 6580, train/loss: 0.0\n",
      "Step: 6580, train/grad_norm: 0.013941523618996143\n",
      "Step: 6580, train/learning_rate: 4.217039531795308e-05\n",
      "Step: 6580, train/epoch: 1.5659209489822388\n",
      "Step: 6590, train/loss: 0.26739999651908875\n",
      "Step: 6590, train/grad_norm: 0.0005121802678331733\n",
      "Step: 6590, train/learning_rate: 4.215849548927508e-05\n",
      "Step: 6590, train/epoch: 1.5683008432388306\n",
      "Step: 6600, train/loss: 0.0\n",
      "Step: 6600, train/grad_norm: 0.06748775392770767\n",
      "Step: 6600, train/learning_rate: 4.2146595660597086e-05\n",
      "Step: 6600, train/epoch: 1.5706806182861328\n",
      "Step: 6610, train/loss: 0.08250000327825546\n",
      "Step: 6610, train/grad_norm: 0.07137534022331238\n",
      "Step: 6610, train/learning_rate: 4.2134699469897896e-05\n",
      "Step: 6610, train/epoch: 1.573060393333435\n",
      "Step: 6620, train/loss: 0.0\n",
      "Step: 6620, train/grad_norm: 0.004906742367893457\n",
      "Step: 6620, train/learning_rate: 4.21227996412199e-05\n",
      "Step: 6620, train/epoch: 1.5754402875900269\n",
      "Step: 6630, train/loss: 9.999999747378752e-05\n",
      "Step: 6630, train/grad_norm: 0.0013404308119788766\n",
      "Step: 6630, train/learning_rate: 4.21108998125419e-05\n",
      "Step: 6630, train/epoch: 1.577820062637329\n",
      "Step: 6640, train/loss: 0.16189999878406525\n",
      "Step: 6640, train/grad_norm: 0.0013098781928420067\n",
      "Step: 6640, train/learning_rate: 4.2098999983863905e-05\n",
      "Step: 6640, train/epoch: 1.580199956893921\n",
      "Step: 6650, train/loss: 0.041600000113248825\n",
      "Step: 6650, train/grad_norm: 0.004068767186254263\n",
      "Step: 6650, train/learning_rate: 4.208710015518591e-05\n",
      "Step: 6650, train/epoch: 1.5825797319412231\n",
      "Step: 6660, train/loss: 0.0\n",
      "Step: 6660, train/grad_norm: 0.002722205827012658\n",
      "Step: 6660, train/learning_rate: 4.207520396448672e-05\n",
      "Step: 6660, train/epoch: 1.5849595069885254\n",
      "Step: 6670, train/loss: 0.0003000000142492354\n",
      "Step: 6670, train/grad_norm: 0.00041123019764199853\n",
      "Step: 6670, train/learning_rate: 4.206330413580872e-05\n",
      "Step: 6670, train/epoch: 1.5873394012451172\n",
      "Step: 6680, train/loss: 0.04340000078082085\n",
      "Step: 6680, train/grad_norm: 0.01930972747504711\n",
      "Step: 6680, train/learning_rate: 4.2051404307130724e-05\n",
      "Step: 6680, train/epoch: 1.5897191762924194\n",
      "Step: 6690, train/loss: 0.20080000162124634\n",
      "Step: 6690, train/grad_norm: 0.0005079728434793651\n",
      "Step: 6690, train/learning_rate: 4.203950447845273e-05\n",
      "Step: 6690, train/epoch: 1.5920989513397217\n",
      "Step: 6700, train/loss: 0.0\n",
      "Step: 6700, train/grad_norm: 0.010550402104854584\n",
      "Step: 6700, train/learning_rate: 4.202760464977473e-05\n",
      "Step: 6700, train/epoch: 1.5944788455963135\n",
      "Step: 6710, train/loss: 0.0032999999821186066\n",
      "Step: 6710, train/grad_norm: 0.021608654409646988\n",
      "Step: 6710, train/learning_rate: 4.201570845907554e-05\n",
      "Step: 6710, train/epoch: 1.5968586206436157\n",
      "Step: 6720, train/loss: 0.00039999998989515007\n",
      "Step: 6720, train/grad_norm: 0.00017420480435248464\n",
      "Step: 6720, train/learning_rate: 4.200380863039754e-05\n",
      "Step: 6720, train/epoch: 1.5992385149002075\n",
      "Step: 6730, train/loss: 0.0\n",
      "Step: 6730, train/grad_norm: 1.4784696759306826e-05\n",
      "Step: 6730, train/learning_rate: 4.1991908801719546e-05\n",
      "Step: 6730, train/epoch: 1.6016182899475098\n",
      "Step: 6740, train/loss: 0.14219999313354492\n",
      "Step: 6740, train/grad_norm: 118.84046173095703\n",
      "Step: 6740, train/learning_rate: 4.198000897304155e-05\n",
      "Step: 6740, train/epoch: 1.603998064994812\n",
      "Step: 6750, train/loss: 0.17499999701976776\n",
      "Step: 6750, train/grad_norm: 405.0576171875\n",
      "Step: 6750, train/learning_rate: 4.196810914436355e-05\n",
      "Step: 6750, train/epoch: 1.6063779592514038\n",
      "Step: 6760, train/loss: 0.11249999701976776\n",
      "Step: 6760, train/grad_norm: 0.0006627176771871746\n",
      "Step: 6760, train/learning_rate: 4.195621295366436e-05\n",
      "Step: 6760, train/epoch: 1.608757734298706\n",
      "Step: 6770, train/loss: 0.0\n",
      "Step: 6770, train/grad_norm: 0.002284374088048935\n",
      "Step: 6770, train/learning_rate: 4.1944313124986365e-05\n",
      "Step: 6770, train/epoch: 1.6111375093460083\n",
      "Step: 6780, train/loss: 9.999999747378752e-05\n",
      "Step: 6780, train/grad_norm: 0.0016453851712867618\n",
      "Step: 6780, train/learning_rate: 4.193241329630837e-05\n",
      "Step: 6780, train/epoch: 1.6135174036026\n",
      "Step: 6790, train/loss: 0.0\n",
      "Step: 6790, train/grad_norm: 0.0010921949287876487\n",
      "Step: 6790, train/learning_rate: 4.192051346763037e-05\n",
      "Step: 6790, train/epoch: 1.6158971786499023\n",
      "Step: 6800, train/loss: 0.0\n",
      "Step: 6800, train/grad_norm: 0.0012192917056381702\n",
      "Step: 6800, train/learning_rate: 4.1908613638952374e-05\n",
      "Step: 6800, train/epoch: 1.6182769536972046\n",
      "Step: 6810, train/loss: 0.0\n",
      "Step: 6810, train/grad_norm: 0.0005765477544628084\n",
      "Step: 6810, train/learning_rate: 4.1896717448253185e-05\n",
      "Step: 6810, train/epoch: 1.6206568479537964\n",
      "Step: 6820, train/loss: 0.0\n",
      "Step: 6820, train/grad_norm: 0.0010361201129853725\n",
      "Step: 6820, train/learning_rate: 4.188481761957519e-05\n",
      "Step: 6820, train/epoch: 1.6230366230010986\n",
      "Step: 6830, train/loss: 9.999999747378752e-05\n",
      "Step: 6830, train/grad_norm: 0.00011478181113488972\n",
      "Step: 6830, train/learning_rate: 4.187291779089719e-05\n",
      "Step: 6830, train/epoch: 1.6254165172576904\n",
      "Step: 6840, train/loss: 0.0\n",
      "Step: 6840, train/grad_norm: 6.54404066153802e-05\n",
      "Step: 6840, train/learning_rate: 4.1861017962219194e-05\n",
      "Step: 6840, train/epoch: 1.6277962923049927\n",
      "Step: 6850, train/loss: 0.0\n",
      "Step: 6850, train/grad_norm: 3.906747224391438e-05\n",
      "Step: 6850, train/learning_rate: 4.1849118133541197e-05\n",
      "Step: 6850, train/epoch: 1.630176067352295\n",
      "Step: 6860, train/loss: 0.0\n",
      "Step: 6860, train/grad_norm: 0.00011832001473521814\n",
      "Step: 6860, train/learning_rate: 4.183722194284201e-05\n",
      "Step: 6860, train/epoch: 1.6325559616088867\n",
      "Step: 6870, train/loss: 0.0\n",
      "Step: 6870, train/grad_norm: 0.00013338561984710395\n",
      "Step: 6870, train/learning_rate: 4.182532211416401e-05\n",
      "Step: 6870, train/epoch: 1.634935736656189\n",
      "Step: 6880, train/loss: 0.1875\n",
      "Step: 6880, train/grad_norm: 0.00013107218546792865\n",
      "Step: 6880, train/learning_rate: 4.181342228548601e-05\n",
      "Step: 6880, train/epoch: 1.6373155117034912\n",
      "Step: 6890, train/loss: 0.0\n",
      "Step: 6890, train/grad_norm: 0.005548324901610613\n",
      "Step: 6890, train/learning_rate: 4.1801522456808016e-05\n",
      "Step: 6890, train/epoch: 1.639695405960083\n",
      "Step: 6900, train/loss: 0.0\n",
      "Step: 6900, train/grad_norm: 0.011471706442534924\n",
      "Step: 6900, train/learning_rate: 4.178962262813002e-05\n",
      "Step: 6900, train/epoch: 1.6420751810073853\n",
      "Step: 6910, train/loss: 0.06400000303983688\n",
      "Step: 6910, train/grad_norm: 0.02228834107518196\n",
      "Step: 6910, train/learning_rate: 4.177772643743083e-05\n",
      "Step: 6910, train/epoch: 1.644455075263977\n",
      "Step: 6920, train/loss: 0.04340000078082085\n",
      "Step: 6920, train/grad_norm: 0.009579678997397423\n",
      "Step: 6920, train/learning_rate: 4.176582660875283e-05\n",
      "Step: 6920, train/epoch: 1.6468348503112793\n",
      "Step: 6930, train/loss: 0.0\n",
      "Step: 6930, train/grad_norm: 0.004238332621753216\n",
      "Step: 6930, train/learning_rate: 4.1753926780074835e-05\n",
      "Step: 6930, train/epoch: 1.6492146253585815\n",
      "Step: 6940, train/loss: 9.999999747378752e-05\n",
      "Step: 6940, train/grad_norm: 0.004896380938589573\n",
      "Step: 6940, train/learning_rate: 4.174202695139684e-05\n",
      "Step: 6940, train/epoch: 1.6515945196151733\n",
      "Step: 6950, train/loss: 0.0\n",
      "Step: 6950, train/grad_norm: 0.0004315570113249123\n",
      "Step: 6950, train/learning_rate: 4.173012712271884e-05\n",
      "Step: 6950, train/epoch: 1.6539742946624756\n",
      "Step: 6960, train/loss: 0.0\n",
      "Step: 6960, train/grad_norm: 0.0017581356223672628\n",
      "Step: 6960, train/learning_rate: 4.171823093201965e-05\n",
      "Step: 6960, train/epoch: 1.6563540697097778\n",
      "Step: 6970, train/loss: 0.1445000022649765\n",
      "Step: 6970, train/grad_norm: 0.022542212158441544\n",
      "Step: 6970, train/learning_rate: 4.1706331103341654e-05\n",
      "Step: 6970, train/epoch: 1.6587339639663696\n",
      "Step: 6980, train/loss: 9.999999747378752e-05\n",
      "Step: 6980, train/grad_norm: 0.010942252352833748\n",
      "Step: 6980, train/learning_rate: 4.169443127466366e-05\n",
      "Step: 6980, train/epoch: 1.6611137390136719\n",
      "Step: 6990, train/loss: 9.999999747378752e-05\n",
      "Step: 6990, train/grad_norm: 0.0024863488506525755\n",
      "Step: 6990, train/learning_rate: 4.168253144598566e-05\n",
      "Step: 6990, train/epoch: 1.6634936332702637\n",
      "Step: 7000, train/loss: 0.0\n",
      "Step: 7000, train/grad_norm: 0.005835141055285931\n",
      "Step: 7000, train/learning_rate: 4.167063161730766e-05\n",
      "Step: 7000, train/epoch: 1.665873408317566\n",
      "Step: 7010, train/loss: 0.0\n",
      "Step: 7010, train/grad_norm: 0.019721293821930885\n",
      "Step: 7010, train/learning_rate: 4.165873542660847e-05\n",
      "Step: 7010, train/epoch: 1.6682531833648682\n",
      "Step: 7020, train/loss: 0.0723000019788742\n",
      "Step: 7020, train/grad_norm: 0.007628961931914091\n",
      "Step: 7020, train/learning_rate: 4.1646835597930476e-05\n",
      "Step: 7020, train/epoch: 1.67063307762146\n",
      "Step: 7030, train/loss: 0.07270000129938126\n",
      "Step: 7030, train/grad_norm: 0.005045111291110516\n",
      "Step: 7030, train/learning_rate: 4.163493576925248e-05\n",
      "Step: 7030, train/epoch: 1.6730128526687622\n",
      "Step: 7040, train/loss: 0.0\n",
      "Step: 7040, train/grad_norm: 0.0027005125302821398\n",
      "Step: 7040, train/learning_rate: 4.162303594057448e-05\n",
      "Step: 7040, train/epoch: 1.6753926277160645\n",
      "Step: 7050, train/loss: 0.0\n",
      "Step: 7050, train/grad_norm: 0.0021281775552779436\n",
      "Step: 7050, train/learning_rate: 4.1611136111896485e-05\n",
      "Step: 7050, train/epoch: 1.6777725219726562\n",
      "Step: 7060, train/loss: 0.0\n",
      "Step: 7060, train/grad_norm: 0.0013197704683989286\n",
      "Step: 7060, train/learning_rate: 4.1599239921197295e-05\n",
      "Step: 7060, train/epoch: 1.6801522970199585\n",
      "Step: 7070, train/loss: 0.08320000022649765\n",
      "Step: 7070, train/grad_norm: 0.018792161718010902\n",
      "Step: 7070, train/learning_rate: 4.15873400925193e-05\n",
      "Step: 7070, train/epoch: 1.6825320720672607\n",
      "Step: 7080, train/loss: 9.999999747378752e-05\n",
      "Step: 7080, train/grad_norm: 0.001409070799127221\n",
      "Step: 7080, train/learning_rate: 4.15754402638413e-05\n",
      "Step: 7080, train/epoch: 1.6849119663238525\n",
      "Step: 7090, train/loss: 0.0\n",
      "Step: 7090, train/grad_norm: 0.04054480791091919\n",
      "Step: 7090, train/learning_rate: 4.1563540435163304e-05\n",
      "Step: 7090, train/epoch: 1.6872917413711548\n",
      "Step: 7100, train/loss: 0.00019999999494757503\n",
      "Step: 7100, train/grad_norm: 0.00022377705317921937\n",
      "Step: 7100, train/learning_rate: 4.155164060648531e-05\n",
      "Step: 7100, train/epoch: 1.6896716356277466\n",
      "Step: 7110, train/loss: 0.0\n",
      "Step: 7110, train/grad_norm: 0.0002739988558460027\n",
      "Step: 7110, train/learning_rate: 4.153974441578612e-05\n",
      "Step: 7110, train/epoch: 1.6920514106750488\n",
      "Step: 7120, train/loss: 0.0\n",
      "Step: 7120, train/grad_norm: 0.012188201770186424\n",
      "Step: 7120, train/learning_rate: 4.152784458710812e-05\n",
      "Step: 7120, train/epoch: 1.694431185722351\n",
      "Step: 7130, train/loss: 0.0\n",
      "Step: 7130, train/grad_norm: 0.0006659395294263959\n",
      "Step: 7130, train/learning_rate: 4.151594475843012e-05\n",
      "Step: 7130, train/epoch: 1.6968110799789429\n",
      "Step: 7140, train/loss: 0.0\n",
      "Step: 7140, train/grad_norm: 0.00010660473344614729\n",
      "Step: 7140, train/learning_rate: 4.1504044929752126e-05\n",
      "Step: 7140, train/epoch: 1.6991908550262451\n",
      "Step: 7150, train/loss: 0.0\n",
      "Step: 7150, train/grad_norm: 5.0965172704309225e-05\n",
      "Step: 7150, train/learning_rate: 4.149214510107413e-05\n",
      "Step: 7150, train/epoch: 1.7015706300735474\n",
      "Step: 7160, train/loss: 0.1265999972820282\n",
      "Step: 7160, train/grad_norm: 0.021305132657289505\n",
      "Step: 7160, train/learning_rate: 4.148024891037494e-05\n",
      "Step: 7160, train/epoch: 1.7039505243301392\n",
      "Step: 7170, train/loss: 0.011500000022351742\n",
      "Step: 7170, train/grad_norm: 0.00033960651489906013\n",
      "Step: 7170, train/learning_rate: 4.146834908169694e-05\n",
      "Step: 7170, train/epoch: 1.7063302993774414\n",
      "Step: 7180, train/loss: 0.00989999994635582\n",
      "Step: 7180, train/grad_norm: 9.609897097107023e-05\n",
      "Step: 7180, train/learning_rate: 4.1456449253018945e-05\n",
      "Step: 7180, train/epoch: 1.7087101936340332\n",
      "Step: 7190, train/loss: 0.11810000240802765\n",
      "Step: 7190, train/grad_norm: 2.0116357803344727\n",
      "Step: 7190, train/learning_rate: 4.144454942434095e-05\n",
      "Step: 7190, train/epoch: 1.7110899686813354\n",
      "Step: 7200, train/loss: 0.0\n",
      "Step: 7200, train/grad_norm: 0.30701830983161926\n",
      "Step: 7200, train/learning_rate: 4.143264959566295e-05\n",
      "Step: 7200, train/epoch: 1.7134697437286377\n",
      "Step: 7210, train/loss: 0.09650000184774399\n",
      "Step: 7210, train/grad_norm: 81.5283203125\n",
      "Step: 7210, train/learning_rate: 4.142075340496376e-05\n",
      "Step: 7210, train/epoch: 1.7158496379852295\n",
      "Step: 7220, train/loss: 0.12729999423027039\n",
      "Step: 7220, train/grad_norm: 8.609030723571777\n",
      "Step: 7220, train/learning_rate: 4.1408853576285765e-05\n",
      "Step: 7220, train/epoch: 1.7182294130325317\n",
      "Step: 7230, train/loss: 9.999999747378752e-05\n",
      "Step: 7230, train/grad_norm: 0.0058725434355437756\n",
      "Step: 7230, train/learning_rate: 4.139695374760777e-05\n",
      "Step: 7230, train/epoch: 1.720609188079834\n",
      "Step: 7240, train/loss: 0.004699999932199717\n",
      "Step: 7240, train/grad_norm: 0.0018009089399129152\n",
      "Step: 7240, train/learning_rate: 4.138505391892977e-05\n",
      "Step: 7240, train/epoch: 1.7229890823364258\n",
      "Step: 7250, train/loss: 9.999999747378752e-05\n",
      "Step: 7250, train/grad_norm: 0.08787615597248077\n",
      "Step: 7250, train/learning_rate: 4.1373154090251774e-05\n",
      "Step: 7250, train/epoch: 1.725368857383728\n",
      "Step: 7260, train/loss: 0.22450000047683716\n",
      "Step: 7260, train/grad_norm: 8.185578917618841e-05\n",
      "Step: 7260, train/learning_rate: 4.1361257899552584e-05\n",
      "Step: 7260, train/epoch: 1.7277486324310303\n",
      "Step: 7270, train/loss: 0.1679999977350235\n",
      "Step: 7270, train/grad_norm: 0.005151680670678616\n",
      "Step: 7270, train/learning_rate: 4.134935807087459e-05\n",
      "Step: 7270, train/epoch: 1.730128526687622\n",
      "Step: 7280, train/loss: 0.2743000090122223\n",
      "Step: 7280, train/grad_norm: 570.2861938476562\n",
      "Step: 7280, train/learning_rate: 4.133745824219659e-05\n",
      "Step: 7280, train/epoch: 1.7325083017349243\n",
      "Step: 7290, train/loss: 0.0017000000225380063\n",
      "Step: 7290, train/grad_norm: 2.5221451593893107e-08\n",
      "Step: 7290, train/learning_rate: 4.132555841351859e-05\n",
      "Step: 7290, train/epoch: 1.7348881959915161\n",
      "Step: 7300, train/loss: 0.0\n",
      "Step: 7300, train/grad_norm: 1.1091414897634877e-08\n",
      "Step: 7300, train/learning_rate: 4.1313658584840596e-05\n",
      "Step: 7300, train/epoch: 1.7372679710388184\n",
      "Step: 7310, train/loss: 0.0\n",
      "Step: 7310, train/grad_norm: 1.3909449592119927e-07\n",
      "Step: 7310, train/learning_rate: 4.1301762394141406e-05\n",
      "Step: 7310, train/epoch: 1.7396477460861206\n",
      "Step: 7320, train/loss: 0.30309998989105225\n",
      "Step: 7320, train/grad_norm: 5.2966321106850955e-08\n",
      "Step: 7320, train/learning_rate: 4.128986256546341e-05\n",
      "Step: 7320, train/epoch: 1.7420276403427124\n",
      "Step: 7330, train/loss: 0.0\n",
      "Step: 7330, train/grad_norm: 0.0006773544591851532\n",
      "Step: 7330, train/learning_rate: 4.127796273678541e-05\n",
      "Step: 7330, train/epoch: 1.7444074153900146\n",
      "Step: 7340, train/loss: 9.999999747378752e-05\n",
      "Step: 7340, train/grad_norm: 0.002816243330016732\n",
      "Step: 7340, train/learning_rate: 4.1266062908107415e-05\n",
      "Step: 7340, train/epoch: 1.746787190437317\n",
      "Step: 7350, train/loss: 0.0\n",
      "Step: 7350, train/grad_norm: 0.0015147596132010221\n",
      "Step: 7350, train/learning_rate: 4.125416307942942e-05\n",
      "Step: 7350, train/epoch: 1.7491670846939087\n",
      "Step: 7360, train/loss: 0.11020000278949738\n",
      "Step: 7360, train/grad_norm: 0.04300044849514961\n",
      "Step: 7360, train/learning_rate: 4.124226688873023e-05\n",
      "Step: 7360, train/epoch: 1.751546859741211\n",
      "Step: 7370, train/loss: 9.999999747378752e-05\n",
      "Step: 7370, train/grad_norm: 0.0362691730260849\n",
      "Step: 7370, train/learning_rate: 4.123036706005223e-05\n",
      "Step: 7370, train/epoch: 1.7539267539978027\n",
      "Step: 7380, train/loss: 0.0\n",
      "Step: 7380, train/grad_norm: 0.0017607079353183508\n",
      "Step: 7380, train/learning_rate: 4.1218467231374234e-05\n",
      "Step: 7380, train/epoch: 1.756306529045105\n",
      "Step: 7390, train/loss: 0.0\n",
      "Step: 7390, train/grad_norm: 0.00025303615257143974\n",
      "Step: 7390, train/learning_rate: 4.120656740269624e-05\n",
      "Step: 7390, train/epoch: 1.7586863040924072\n",
      "Step: 7400, train/loss: 0.0\n",
      "Step: 7400, train/grad_norm: 0.0031362189911305904\n",
      "Step: 7400, train/learning_rate: 4.119466757401824e-05\n",
      "Step: 7400, train/epoch: 1.761066198348999\n",
      "Step: 7410, train/loss: 0.0\n",
      "Step: 7410, train/grad_norm: 0.0007945026154629886\n",
      "Step: 7410, train/learning_rate: 4.118277138331905e-05\n",
      "Step: 7410, train/epoch: 1.7634459733963013\n",
      "Step: 7420, train/loss: 0.0\n",
      "Step: 7420, train/grad_norm: 0.0007500617066398263\n",
      "Step: 7420, train/learning_rate: 4.117087155464105e-05\n",
      "Step: 7420, train/epoch: 1.7658257484436035\n",
      "Step: 7430, train/loss: 0.0\n",
      "Step: 7430, train/grad_norm: 0.00033553934190422297\n",
      "Step: 7430, train/learning_rate: 4.1158971725963056e-05\n",
      "Step: 7430, train/epoch: 1.7682056427001953\n",
      "Step: 7440, train/loss: 0.0\n",
      "Step: 7440, train/grad_norm: 0.00030508855707012117\n",
      "Step: 7440, train/learning_rate: 4.114707189728506e-05\n",
      "Step: 7440, train/epoch: 1.7705854177474976\n",
      "Step: 7450, train/loss: 0.0\n",
      "Step: 7450, train/grad_norm: 0.0008053338387981057\n",
      "Step: 7450, train/learning_rate: 4.113517206860706e-05\n",
      "Step: 7450, train/epoch: 1.7729653120040894\n",
      "Step: 7460, train/loss: 0.0\n",
      "Step: 7460, train/grad_norm: 0.0001897868060041219\n",
      "Step: 7460, train/learning_rate: 4.112327587790787e-05\n",
      "Step: 7460, train/epoch: 1.7753450870513916\n",
      "Step: 7470, train/loss: 0.0\n",
      "Step: 7470, train/grad_norm: 0.0021644181106239557\n",
      "Step: 7470, train/learning_rate: 4.1111376049229875e-05\n",
      "Step: 7470, train/epoch: 1.7777248620986938\n",
      "Step: 7480, train/loss: 0.0\n",
      "Step: 7480, train/grad_norm: 0.062169741839170456\n",
      "Step: 7480, train/learning_rate: 4.109947622055188e-05\n",
      "Step: 7480, train/epoch: 1.7801047563552856\n",
      "Step: 7490, train/loss: 0.0406000018119812\n",
      "Step: 7490, train/grad_norm: 0.0019882286433130503\n",
      "Step: 7490, train/learning_rate: 4.108757639187388e-05\n",
      "Step: 7490, train/epoch: 1.782484531402588\n",
      "Step: 7500, train/loss: 0.026000000536441803\n",
      "Step: 7500, train/grad_norm: 0.00017443935212213546\n",
      "Step: 7500, train/learning_rate: 4.1075676563195884e-05\n",
      "Step: 7500, train/epoch: 1.7848643064498901\n",
      "Step: 7510, train/loss: 0.0\n",
      "Step: 7510, train/grad_norm: 2.281543629578664e-06\n",
      "Step: 7510, train/learning_rate: 4.1063780372496694e-05\n",
      "Step: 7510, train/epoch: 1.787244200706482\n",
      "Step: 7520, train/loss: 0.08089999854564667\n",
      "Step: 7520, train/grad_norm: 1.8884025848819874e-05\n",
      "Step: 7520, train/learning_rate: 4.10518805438187e-05\n",
      "Step: 7520, train/epoch: 1.7896239757537842\n",
      "Step: 7530, train/loss: 0.0\n",
      "Step: 7530, train/grad_norm: 3.074280130022089e-06\n",
      "Step: 7530, train/learning_rate: 4.10399807151407e-05\n",
      "Step: 7530, train/epoch: 1.7920037508010864\n",
      "Step: 7540, train/loss: 0.0\n",
      "Step: 7540, train/grad_norm: 2.9565549652943446e-07\n",
      "Step: 7540, train/learning_rate: 4.10280808864627e-05\n",
      "Step: 7540, train/epoch: 1.7943836450576782\n",
      "Step: 7550, train/loss: 0.09769999980926514\n",
      "Step: 7550, train/grad_norm: 2.721307055253419e-06\n",
      "Step: 7550, train/learning_rate: 4.1016181057784706e-05\n",
      "Step: 7550, train/epoch: 1.7967634201049805\n",
      "Step: 7560, train/loss: 0.0\n",
      "Step: 7560, train/grad_norm: 0.0015063772443681955\n",
      "Step: 7560, train/learning_rate: 4.1004284867085516e-05\n",
      "Step: 7560, train/epoch: 1.7991433143615723\n",
      "Step: 7570, train/loss: 0.0\n",
      "Step: 7570, train/grad_norm: 9.469010819884716e-07\n",
      "Step: 7570, train/learning_rate: 4.099238503840752e-05\n",
      "Step: 7570, train/epoch: 1.8015230894088745\n",
      "Step: 7580, train/loss: 0.03750000149011612\n",
      "Step: 7580, train/grad_norm: 5.459136218632921e-07\n",
      "Step: 7580, train/learning_rate: 4.098048520972952e-05\n",
      "Step: 7580, train/epoch: 1.8039028644561768\n",
      "Step: 7590, train/loss: 0.10939999669790268\n",
      "Step: 7590, train/grad_norm: 5.283113205223344e-05\n",
      "Step: 7590, train/learning_rate: 4.0968585381051525e-05\n",
      "Step: 7590, train/epoch: 1.8062827587127686\n",
      "Step: 7600, train/loss: 0.03970000147819519\n",
      "Step: 7600, train/grad_norm: 1.0938864761556033e-05\n",
      "Step: 7600, train/learning_rate: 4.095668555237353e-05\n",
      "Step: 7600, train/epoch: 1.8086625337600708\n",
      "Step: 7610, train/loss: 0.0\n",
      "Step: 7610, train/grad_norm: 7.185150752775371e-05\n",
      "Step: 7610, train/learning_rate: 4.094478936167434e-05\n",
      "Step: 7610, train/epoch: 1.811042308807373\n",
      "Step: 7620, train/loss: 0.0\n",
      "Step: 7620, train/grad_norm: 1.792861985450145e-05\n",
      "Step: 7620, train/learning_rate: 4.093288953299634e-05\n",
      "Step: 7620, train/epoch: 1.8134222030639648\n",
      "Step: 7630, train/loss: 0.0\n",
      "Step: 7630, train/grad_norm: 3.066865974687971e-05\n",
      "Step: 7630, train/learning_rate: 4.0920989704318345e-05\n",
      "Step: 7630, train/epoch: 1.815801978111267\n",
      "Step: 7640, train/loss: 0.0\n",
      "Step: 7640, train/grad_norm: 4.227248427923769e-05\n",
      "Step: 7640, train/learning_rate: 4.090908987564035e-05\n",
      "Step: 7640, train/epoch: 1.8181818723678589\n",
      "Step: 7650, train/loss: 0.12269999831914902\n",
      "Step: 7650, train/grad_norm: 0.005305727943778038\n",
      "Step: 7650, train/learning_rate: 4.089719004696235e-05\n",
      "Step: 7650, train/epoch: 1.8205616474151611\n",
      "Step: 7660, train/loss: 0.0\n",
      "Step: 7660, train/grad_norm: 0.11797675490379333\n",
      "Step: 7660, train/learning_rate: 4.088529385626316e-05\n",
      "Step: 7660, train/epoch: 1.8229414224624634\n",
      "Step: 7670, train/loss: 9.999999747378752e-05\n",
      "Step: 7670, train/grad_norm: 0.0011769442353397608\n",
      "Step: 7670, train/learning_rate: 4.0873394027585164e-05\n",
      "Step: 7670, train/epoch: 1.8253213167190552\n",
      "Step: 7680, train/loss: 0.0\n",
      "Step: 7680, train/grad_norm: 0.0007696654065512121\n",
      "Step: 7680, train/learning_rate: 4.086149419890717e-05\n",
      "Step: 7680, train/epoch: 1.8277010917663574\n",
      "Step: 7690, train/loss: 0.0\n",
      "Step: 7690, train/grad_norm: 0.00013490786659531295\n",
      "Step: 7690, train/learning_rate: 4.084959437022917e-05\n",
      "Step: 7690, train/epoch: 1.8300808668136597\n",
      "Step: 7700, train/loss: 0.15000000596046448\n",
      "Step: 7700, train/grad_norm: 0.0002028497401624918\n",
      "Step: 7700, train/learning_rate: 4.083769454155117e-05\n",
      "Step: 7700, train/epoch: 1.8324607610702515\n",
      "Step: 7710, train/loss: 0.0003000000142492354\n",
      "Step: 7710, train/grad_norm: 0.01820393092930317\n",
      "Step: 7710, train/learning_rate: 4.082579835085198e-05\n",
      "Step: 7710, train/epoch: 1.8348405361175537\n",
      "Step: 7720, train/loss: 0.0\n",
      "Step: 7720, train/grad_norm: 0.0004295820835977793\n",
      "Step: 7720, train/learning_rate: 4.0813898522173986e-05\n",
      "Step: 7720, train/epoch: 1.8372204303741455\n",
      "Step: 7730, train/loss: 0.0\n",
      "Step: 7730, train/grad_norm: 0.0002519682457204908\n",
      "Step: 7730, train/learning_rate: 4.080199869349599e-05\n",
      "Step: 7730, train/epoch: 1.8396002054214478\n",
      "Step: 7740, train/loss: 0.025200000032782555\n",
      "Step: 7740, train/grad_norm: 0.0896904319524765\n",
      "Step: 7740, train/learning_rate: 4.079009886481799e-05\n",
      "Step: 7740, train/epoch: 1.84197998046875\n",
      "Step: 7750, train/loss: 0.002899999963119626\n",
      "Step: 7750, train/grad_norm: 0.027700817212462425\n",
      "Step: 7750, train/learning_rate: 4.07782026741188e-05\n",
      "Step: 7750, train/epoch: 1.8443598747253418\n",
      "Step: 7760, train/loss: 9.999999747378752e-05\n",
      "Step: 7760, train/grad_norm: 0.5892831087112427\n",
      "Step: 7760, train/learning_rate: 4.0766302845440805e-05\n",
      "Step: 7760, train/epoch: 1.846739649772644\n",
      "Step: 7770, train/loss: 0.0\n",
      "Step: 7770, train/grad_norm: 0.00495897838845849\n",
      "Step: 7770, train/learning_rate: 4.075440301676281e-05\n",
      "Step: 7770, train/epoch: 1.8491194248199463\n",
      "Step: 7780, train/loss: 0.06210000067949295\n",
      "Step: 7780, train/grad_norm: 0.0004920906503684819\n",
      "Step: 7780, train/learning_rate: 4.074250318808481e-05\n",
      "Step: 7780, train/epoch: 1.851499319076538\n",
      "Step: 7790, train/loss: 0.21490000188350677\n",
      "Step: 7790, train/grad_norm: 0.00014996895333752036\n",
      "Step: 7790, train/learning_rate: 4.0730603359406814e-05\n",
      "Step: 7790, train/epoch: 1.8538790941238403\n",
      "Step: 7800, train/loss: 0.007300000172108412\n",
      "Step: 7800, train/grad_norm: 0.0001259849377674982\n",
      "Step: 7800, train/learning_rate: 4.0718707168707624e-05\n",
      "Step: 7800, train/epoch: 1.8562588691711426\n",
      "Step: 7810, train/loss: 0.0006000000284984708\n",
      "Step: 7810, train/grad_norm: 11.830843925476074\n",
      "Step: 7810, train/learning_rate: 4.070680734002963e-05\n",
      "Step: 7810, train/epoch: 1.8586387634277344\n",
      "Step: 7820, train/loss: 0.04699999839067459\n",
      "Step: 7820, train/grad_norm: 4.510958149239741e-07\n",
      "Step: 7820, train/learning_rate: 4.069490751135163e-05\n",
      "Step: 7820, train/epoch: 1.8610185384750366\n",
      "Step: 7830, train/loss: 0.031099999323487282\n",
      "Step: 7830, train/grad_norm: 3.624894452514127e-05\n",
      "Step: 7830, train/learning_rate: 4.068300768267363e-05\n",
      "Step: 7830, train/epoch: 1.8633984327316284\n",
      "Step: 7840, train/loss: 0.0\n",
      "Step: 7840, train/grad_norm: 2.330265544969734e-07\n",
      "Step: 7840, train/learning_rate: 4.0671107853995636e-05\n",
      "Step: 7840, train/epoch: 1.8657782077789307\n",
      "Step: 7850, train/loss: 0.000699999975040555\n",
      "Step: 7850, train/grad_norm: 2.3610400603502057e-05\n",
      "Step: 7850, train/learning_rate: 4.0659211663296446e-05\n",
      "Step: 7850, train/epoch: 1.868157982826233\n",
      "Step: 7860, train/loss: 0.15230000019073486\n",
      "Step: 7860, train/grad_norm: 5.28043699432601e-07\n",
      "Step: 7860, train/learning_rate: 4.064731183461845e-05\n",
      "Step: 7860, train/epoch: 1.8705378770828247\n",
      "Step: 7870, train/loss: 0.2281000018119812\n",
      "Step: 7870, train/grad_norm: 1.7345644209854072e-06\n",
      "Step: 7870, train/learning_rate: 4.063541200594045e-05\n",
      "Step: 7870, train/epoch: 1.872917652130127\n",
      "Step: 7880, train/loss: 0.14339999854564667\n",
      "Step: 7880, train/grad_norm: 0.001193384057842195\n",
      "Step: 7880, train/learning_rate: 4.0623512177262455e-05\n",
      "Step: 7880, train/epoch: 1.8752974271774292\n",
      "Step: 7890, train/loss: 0.0006000000284984708\n",
      "Step: 7890, train/grad_norm: 0.029943739995360374\n",
      "Step: 7890, train/learning_rate: 4.061161234858446e-05\n",
      "Step: 7890, train/epoch: 1.877677321434021\n",
      "Step: 7900, train/loss: 0.0\n",
      "Step: 7900, train/grad_norm: 4.203483331366442e-05\n",
      "Step: 7900, train/learning_rate: 4.059971615788527e-05\n",
      "Step: 7900, train/epoch: 1.8800570964813232\n",
      "Step: 7910, train/loss: 0.19689999520778656\n",
      "Step: 7910, train/grad_norm: 0.007772657088935375\n",
      "Step: 7910, train/learning_rate: 4.058781632920727e-05\n",
      "Step: 7910, train/epoch: 1.882436990737915\n",
      "Step: 7920, train/loss: 0.0\n",
      "Step: 7920, train/grad_norm: 4.996843927074224e-05\n",
      "Step: 7920, train/learning_rate: 4.0575916500529274e-05\n",
      "Step: 7920, train/epoch: 1.8848167657852173\n",
      "Step: 7930, train/loss: 0.14169999957084656\n",
      "Step: 7930, train/grad_norm: 0.0005305205704644322\n",
      "Step: 7930, train/learning_rate: 4.056401667185128e-05\n",
      "Step: 7930, train/epoch: 1.8871965408325195\n",
      "Step: 7940, train/loss: 0.09539999812841415\n",
      "Step: 7940, train/grad_norm: 0.22898761928081512\n",
      "Step: 7940, train/learning_rate: 4.055211684317328e-05\n",
      "Step: 7940, train/epoch: 1.8895764350891113\n",
      "Step: 7950, train/loss: 0.0\n",
      "Step: 7950, train/grad_norm: 0.0010074920719489455\n",
      "Step: 7950, train/learning_rate: 4.054022065247409e-05\n",
      "Step: 7950, train/epoch: 1.8919562101364136\n",
      "Step: 7960, train/loss: 0.43130001425743103\n",
      "Step: 7960, train/grad_norm: 0.0005809857393614948\n",
      "Step: 7960, train/learning_rate: 4.0528320823796093e-05\n",
      "Step: 7960, train/epoch: 1.8943359851837158\n",
      "Step: 7970, train/loss: 0.06019999831914902\n",
      "Step: 7970, train/grad_norm: 0.0008475183858536184\n",
      "Step: 7970, train/learning_rate: 4.0516420995118096e-05\n",
      "Step: 7970, train/epoch: 1.8967158794403076\n",
      "Step: 7980, train/loss: 0.08990000188350677\n",
      "Step: 7980, train/grad_norm: 0.00484577938914299\n",
      "Step: 7980, train/learning_rate: 4.05045211664401e-05\n",
      "Step: 7980, train/epoch: 1.8990956544876099\n",
      "Step: 7990, train/loss: 0.00019999999494757503\n",
      "Step: 7990, train/grad_norm: 1.545715093612671\n",
      "Step: 7990, train/learning_rate: 4.04926213377621e-05\n",
      "Step: 7990, train/epoch: 1.901475429534912\n",
      "Step: 8000, train/loss: 0.0\n",
      "Step: 8000, train/grad_norm: 0.0011425473494455218\n",
      "Step: 8000, train/learning_rate: 4.048072514706291e-05\n",
      "Step: 8000, train/epoch: 1.903855323791504\n",
      "Step: 8010, train/loss: 0.358599990606308\n",
      "Step: 8010, train/grad_norm: 0.0016589959850534797\n",
      "Step: 8010, train/learning_rate: 4.0468825318384916e-05\n",
      "Step: 8010, train/epoch: 1.9062350988388062\n",
      "Step: 8020, train/loss: 0.0\n",
      "Step: 8020, train/grad_norm: 0.0009982771007344127\n",
      "Step: 8020, train/learning_rate: 4.045692548970692e-05\n",
      "Step: 8020, train/epoch: 1.908614993095398\n",
      "Step: 8030, train/loss: 0.0\n",
      "Step: 8030, train/grad_norm: 0.00026767945382744074\n",
      "Step: 8030, train/learning_rate: 4.044502566102892e-05\n",
      "Step: 8030, train/epoch: 1.9109947681427002\n",
      "Step: 8040, train/loss: 0.10080000013113022\n",
      "Step: 8040, train/grad_norm: 0.004049169365316629\n",
      "Step: 8040, train/learning_rate: 4.0433125832350925e-05\n",
      "Step: 8040, train/epoch: 1.9133745431900024\n",
      "Step: 8050, train/loss: 0.1664000004529953\n",
      "Step: 8050, train/grad_norm: 132.0300750732422\n",
      "Step: 8050, train/learning_rate: 4.0421229641651735e-05\n",
      "Step: 8050, train/epoch: 1.9157544374465942\n",
      "Step: 8060, train/loss: 0.11490000039339066\n",
      "Step: 8060, train/grad_norm: 0.12818759679794312\n",
      "Step: 8060, train/learning_rate: 4.040932981297374e-05\n",
      "Step: 8060, train/epoch: 1.9181342124938965\n",
      "Step: 8070, train/loss: 0.00019999999494757503\n",
      "Step: 8070, train/grad_norm: 0.00600746925920248\n",
      "Step: 8070, train/learning_rate: 4.039742998429574e-05\n",
      "Step: 8070, train/epoch: 1.9205139875411987\n",
      "Step: 8080, train/loss: 0.04989999905228615\n",
      "Step: 8080, train/grad_norm: 0.00033405417343601584\n",
      "Step: 8080, train/learning_rate: 4.0385530155617744e-05\n",
      "Step: 8080, train/epoch: 1.9228938817977905\n",
      "Step: 8090, train/loss: 0.0\n",
      "Step: 8090, train/grad_norm: 7.83955620136112e-05\n",
      "Step: 8090, train/learning_rate: 4.037363032693975e-05\n",
      "Step: 8090, train/epoch: 1.9252736568450928\n",
      "Step: 8100, train/loss: 0.0\n",
      "Step: 8100, train/grad_norm: 8.196609269361943e-05\n",
      "Step: 8100, train/learning_rate: 4.036173413624056e-05\n",
      "Step: 8100, train/epoch: 1.9276535511016846\n",
      "Step: 8110, train/loss: 0.1023000031709671\n",
      "Step: 8110, train/grad_norm: 0.0033169379457831383\n",
      "Step: 8110, train/learning_rate: 4.034983430756256e-05\n",
      "Step: 8110, train/epoch: 1.9300333261489868\n",
      "Step: 8120, train/loss: 9.999999747378752e-05\n",
      "Step: 8120, train/grad_norm: 0.00040977279422804713\n",
      "Step: 8120, train/learning_rate: 4.033793447888456e-05\n",
      "Step: 8120, train/epoch: 1.932413101196289\n",
      "Step: 8130, train/loss: 0.0\n",
      "Step: 8130, train/grad_norm: 0.0003090105892624706\n",
      "Step: 8130, train/learning_rate: 4.0326034650206566e-05\n",
      "Step: 8130, train/epoch: 1.9347929954528809\n",
      "Step: 8140, train/loss: 0.0\n",
      "Step: 8140, train/grad_norm: 0.00022643491683993489\n",
      "Step: 8140, train/learning_rate: 4.031413482152857e-05\n",
      "Step: 8140, train/epoch: 1.937172770500183\n",
      "Step: 8150, train/loss: 0.05790000036358833\n",
      "Step: 8150, train/grad_norm: 0.5370830297470093\n",
      "Step: 8150, train/learning_rate: 4.030223863082938e-05\n",
      "Step: 8150, train/epoch: 1.9395525455474854\n",
      "Step: 8160, train/loss: 0.0\n",
      "Step: 8160, train/grad_norm: 0.0011058601085096598\n",
      "Step: 8160, train/learning_rate: 4.029033880215138e-05\n",
      "Step: 8160, train/epoch: 1.9419324398040771\n",
      "Step: 8170, train/loss: 0.15680000185966492\n",
      "Step: 8170, train/grad_norm: 0.0009628756670281291\n",
      "Step: 8170, train/learning_rate: 4.0278438973473385e-05\n",
      "Step: 8170, train/epoch: 1.9443122148513794\n",
      "Step: 8180, train/loss: 0.002400000113993883\n",
      "Step: 8180, train/grad_norm: 12.988991737365723\n",
      "Step: 8180, train/learning_rate: 4.026653914479539e-05\n",
      "Step: 8180, train/epoch: 1.9466921091079712\n",
      "Step: 8190, train/loss: 0.00019999999494757503\n",
      "Step: 8190, train/grad_norm: 0.004682156257331371\n",
      "Step: 8190, train/learning_rate: 4.025463931611739e-05\n",
      "Step: 8190, train/epoch: 1.9490718841552734\n",
      "Step: 8200, train/loss: 0.0\n",
      "Step: 8200, train/grad_norm: 0.00055244768736884\n",
      "Step: 8200, train/learning_rate: 4.02427431254182e-05\n",
      "Step: 8200, train/epoch: 1.9514516592025757\n",
      "Step: 8210, train/loss: 0.17499999701976776\n",
      "Step: 8210, train/grad_norm: 0.0011166654294356704\n",
      "Step: 8210, train/learning_rate: 4.0230843296740204e-05\n",
      "Step: 8210, train/epoch: 1.9538315534591675\n",
      "Step: 8220, train/loss: 0.0\n",
      "Step: 8220, train/grad_norm: 0.002452516695484519\n",
      "Step: 8220, train/learning_rate: 4.021894346806221e-05\n",
      "Step: 8220, train/epoch: 1.9562113285064697\n",
      "Step: 8230, train/loss: 0.08709999918937683\n",
      "Step: 8230, train/grad_norm: 0.004276772029697895\n",
      "Step: 8230, train/learning_rate: 4.020704363938421e-05\n",
      "Step: 8230, train/epoch: 1.958591103553772\n",
      "Step: 8240, train/loss: 0.00019999999494757503\n",
      "Step: 8240, train/grad_norm: 0.0006483064498752356\n",
      "Step: 8240, train/learning_rate: 4.019514381070621e-05\n",
      "Step: 8240, train/epoch: 1.9609709978103638\n",
      "Step: 8250, train/loss: 0.09920000284910202\n",
      "Step: 8250, train/grad_norm: 0.00017197427223436534\n",
      "Step: 8250, train/learning_rate: 4.018324762000702e-05\n",
      "Step: 8250, train/epoch: 1.963350772857666\n",
      "Step: 8260, train/loss: 0.0\n",
      "Step: 8260, train/grad_norm: 0.0002821460657287389\n",
      "Step: 8260, train/learning_rate: 4.0171347791329026e-05\n",
      "Step: 8260, train/epoch: 1.9657305479049683\n",
      "Step: 8270, train/loss: 0.09920000284910202\n",
      "Step: 8270, train/grad_norm: 0.0028532417491078377\n",
      "Step: 8270, train/learning_rate: 4.015944796265103e-05\n",
      "Step: 8270, train/epoch: 1.96811044216156\n",
      "Step: 8280, train/loss: 0.00139999995008111\n",
      "Step: 8280, train/grad_norm: 0.00019720969430636615\n",
      "Step: 8280, train/learning_rate: 4.014754813397303e-05\n",
      "Step: 8280, train/epoch: 1.9704902172088623\n",
      "Step: 8290, train/loss: 0.13989999890327454\n",
      "Step: 8290, train/grad_norm: 0.01255633495748043\n",
      "Step: 8290, train/learning_rate: 4.0135648305295035e-05\n",
      "Step: 8290, train/epoch: 1.972870111465454\n",
      "Step: 8300, train/loss: 0.0\n",
      "Step: 8300, train/grad_norm: 0.0006158175528980792\n",
      "Step: 8300, train/learning_rate: 4.0123752114595845e-05\n",
      "Step: 8300, train/epoch: 1.9752498865127563\n",
      "Step: 8310, train/loss: 0.0\n",
      "Step: 8310, train/grad_norm: 7.479142368538305e-05\n",
      "Step: 8310, train/learning_rate: 4.011185228591785e-05\n",
      "Step: 8310, train/epoch: 1.9776296615600586\n",
      "Step: 8320, train/loss: 0.0\n",
      "Step: 8320, train/grad_norm: 0.00014636392006650567\n",
      "Step: 8320, train/learning_rate: 4.009995245723985e-05\n",
      "Step: 8320, train/epoch: 1.9800095558166504\n",
      "Step: 8330, train/loss: 0.0\n",
      "Step: 8330, train/grad_norm: 0.0003519393503665924\n",
      "Step: 8330, train/learning_rate: 4.0088052628561854e-05\n",
      "Step: 8330, train/epoch: 1.9823893308639526\n",
      "Step: 8340, train/loss: 0.0\n",
      "Step: 8340, train/grad_norm: 0.005048015154898167\n",
      "Step: 8340, train/learning_rate: 4.007615279988386e-05\n",
      "Step: 8340, train/epoch: 1.9847691059112549\n",
      "Step: 8350, train/loss: 0.0\n",
      "Step: 8350, train/grad_norm: 0.000131269043777138\n",
      "Step: 8350, train/learning_rate: 4.006425660918467e-05\n",
      "Step: 8350, train/epoch: 1.9871490001678467\n",
      "Step: 8360, train/loss: 9.999999747378752e-05\n",
      "Step: 8360, train/grad_norm: 1.2617055177688599\n",
      "Step: 8360, train/learning_rate: 4.005235678050667e-05\n",
      "Step: 8360, train/epoch: 1.989528775215149\n",
      "Step: 8370, train/loss: 0.13439999520778656\n",
      "Step: 8370, train/grad_norm: 86.53111267089844\n",
      "Step: 8370, train/learning_rate: 4.0040456951828673e-05\n",
      "Step: 8370, train/epoch: 1.9919086694717407\n",
      "Step: 8380, train/loss: 0.39239999651908875\n",
      "Step: 8380, train/grad_norm: 0.09600800275802612\n",
      "Step: 8380, train/learning_rate: 4.0028557123150676e-05\n",
      "Step: 8380, train/epoch: 1.994288444519043\n",
      "Step: 8390, train/loss: 0.001500000013038516\n",
      "Step: 8390, train/grad_norm: 0.0007498817867599428\n",
      "Step: 8390, train/learning_rate: 4.001665729447268e-05\n",
      "Step: 8390, train/epoch: 1.9966682195663452\n",
      "Step: 8400, train/loss: 0.006899999920278788\n",
      "Step: 8400, train/grad_norm: 0.00034334207884967327\n",
      "Step: 8400, train/learning_rate: 4.000476110377349e-05\n",
      "Step: 8400, train/epoch: 1.999048113822937\n",
      "Step: 8404, eval/loss: 0.09890208393335342\n",
      "Step: 8404, eval/accuracy: 0.9890323281288147\n",
      "Step: 8404, eval/f1: 0.9883756637573242\n",
      "Step: 8404, eval/runtime: 856.8402709960938\n",
      "Step: 8404, eval/samples_per_second: 8.406000137329102\n",
      "Step: 8404, eval/steps_per_second: 1.0520000457763672\n",
      "Step: 8404, train/epoch: 2.0\n",
      "Step: 8410, train/loss: 0.08669999986886978\n",
      "Step: 8410, train/grad_norm: 0.00011741433263523504\n",
      "Step: 8410, train/learning_rate: 3.999286127509549e-05\n",
      "Step: 8410, train/epoch: 2.0014278888702393\n",
      "Step: 8420, train/loss: 0.0\n",
      "Step: 8420, train/grad_norm: 0.0001490456925239414\n",
      "Step: 8420, train/learning_rate: 3.9980961446417496e-05\n",
      "Step: 8420, train/epoch: 2.003807783126831\n",
      "Step: 8430, train/loss: 0.02419999986886978\n",
      "Step: 8430, train/grad_norm: 0.001397659070789814\n",
      "Step: 8430, train/learning_rate: 3.99690616177395e-05\n",
      "Step: 8430, train/epoch: 2.0061874389648438\n",
      "Step: 8440, train/loss: 0.26249998807907104\n",
      "Step: 8440, train/grad_norm: 0.006421465426683426\n",
      "Step: 8440, train/learning_rate: 3.99571617890615e-05\n",
      "Step: 8440, train/epoch: 2.0085673332214355\n",
      "Step: 8450, train/loss: 0.12919999659061432\n",
      "Step: 8450, train/grad_norm: 277.25390625\n",
      "Step: 8450, train/learning_rate: 3.994526559836231e-05\n",
      "Step: 8450, train/epoch: 2.0109472274780273\n",
      "Step: 8460, train/loss: 0.024900000542402267\n",
      "Step: 8460, train/grad_norm: 0.0018484849715605378\n",
      "Step: 8460, train/learning_rate: 3.9933365769684315e-05\n",
      "Step: 8460, train/epoch: 2.01332688331604\n",
      "Step: 8470, train/loss: 0.0625\n",
      "Step: 8470, train/grad_norm: 0.0036425855942070484\n",
      "Step: 8470, train/learning_rate: 3.992146594100632e-05\n",
      "Step: 8470, train/epoch: 2.015706777572632\n",
      "Step: 8480, train/loss: 0.0\n",
      "Step: 8480, train/grad_norm: 0.0004466918471734971\n",
      "Step: 8480, train/learning_rate: 3.990956611232832e-05\n",
      "Step: 8480, train/epoch: 2.0180866718292236\n",
      "Step: 8490, train/loss: 0.0\n",
      "Step: 8490, train/grad_norm: 0.006199110299348831\n",
      "Step: 8490, train/learning_rate: 3.9897666283650324e-05\n",
      "Step: 8490, train/epoch: 2.0204663276672363\n",
      "Step: 8500, train/loss: 0.0\n",
      "Step: 8500, train/grad_norm: 0.0004844500799663365\n",
      "Step: 8500, train/learning_rate: 3.9885770092951134e-05\n",
      "Step: 8500, train/epoch: 2.022846221923828\n",
      "Step: 8510, train/loss: 0.0\n",
      "Step: 8510, train/grad_norm: 0.011918219737708569\n",
      "Step: 8510, train/learning_rate: 3.987387026427314e-05\n",
      "Step: 8510, train/epoch: 2.02522611618042\n",
      "Step: 8520, train/loss: 0.09809999912977219\n",
      "Step: 8520, train/grad_norm: 0.00022229342721402645\n",
      "Step: 8520, train/learning_rate: 3.986197043559514e-05\n",
      "Step: 8520, train/epoch: 2.0276060104370117\n",
      "Step: 8530, train/loss: 0.11949999630451202\n",
      "Step: 8530, train/grad_norm: 0.002852602396160364\n",
      "Step: 8530, train/learning_rate: 3.985007060691714e-05\n",
      "Step: 8530, train/epoch: 2.0299856662750244\n",
      "Step: 8540, train/loss: 0.0\n",
      "Step: 8540, train/grad_norm: 0.03862433880567551\n",
      "Step: 8540, train/learning_rate: 3.9838170778239146e-05\n",
      "Step: 8540, train/epoch: 2.032365560531616\n",
      "Step: 8550, train/loss: 9.999999747378752e-05\n",
      "Step: 8550, train/grad_norm: 0.022153183817863464\n",
      "Step: 8550, train/learning_rate: 3.9826274587539956e-05\n",
      "Step: 8550, train/epoch: 2.034745454788208\n",
      "Step: 8560, train/loss: 0.0\n",
      "Step: 8560, train/grad_norm: 0.0010448179673403502\n",
      "Step: 8560, train/learning_rate: 3.981437475886196e-05\n",
      "Step: 8560, train/epoch: 2.0371251106262207\n",
      "Step: 8570, train/loss: 0.0\n",
      "Step: 8570, train/grad_norm: 0.014652850106358528\n",
      "Step: 8570, train/learning_rate: 3.980247493018396e-05\n",
      "Step: 8570, train/epoch: 2.0395050048828125\n",
      "Step: 8580, train/loss: 0.0\n",
      "Step: 8580, train/grad_norm: 0.0008979105041362345\n",
      "Step: 8580, train/learning_rate: 3.9790575101505965e-05\n",
      "Step: 8580, train/epoch: 2.0418848991394043\n",
      "Step: 8590, train/loss: 0.2799000144004822\n",
      "Step: 8590, train/grad_norm: 0.04526554048061371\n",
      "Step: 8590, train/learning_rate: 3.977867527282797e-05\n",
      "Step: 8590, train/epoch: 2.044264554977417\n",
      "Step: 8600, train/loss: 9.999999747378752e-05\n",
      "Step: 8600, train/grad_norm: 0.012159336358308792\n",
      "Step: 8600, train/learning_rate: 3.976677908212878e-05\n",
      "Step: 8600, train/epoch: 2.046644449234009\n",
      "Step: 8610, train/loss: 0.0\n",
      "Step: 8610, train/grad_norm: 0.002283687936142087\n",
      "Step: 8610, train/learning_rate: 3.975487925345078e-05\n",
      "Step: 8610, train/epoch: 2.0490243434906006\n",
      "Step: 8620, train/loss: 0.12269999831914902\n",
      "Step: 8620, train/grad_norm: 93.77145385742188\n",
      "Step: 8620, train/learning_rate: 3.9742979424772784e-05\n",
      "Step: 8620, train/epoch: 2.0514039993286133\n",
      "Step: 8630, train/loss: 0.0027000000700354576\n",
      "Step: 8630, train/grad_norm: 110.75080871582031\n",
      "Step: 8630, train/learning_rate: 3.973107959609479e-05\n",
      "Step: 8630, train/epoch: 2.053783893585205\n",
      "Step: 8640, train/loss: 9.999999747378752e-05\n",
      "Step: 8640, train/grad_norm: 0.02006441541016102\n",
      "Step: 8640, train/learning_rate: 3.971917976741679e-05\n",
      "Step: 8640, train/epoch: 2.056163787841797\n",
      "Step: 8650, train/loss: 0.0\n",
      "Step: 8650, train/grad_norm: 0.00031773114460520446\n",
      "Step: 8650, train/learning_rate: 3.97072835767176e-05\n",
      "Step: 8650, train/epoch: 2.0585434436798096\n",
      "Step: 8660, train/loss: 0.27379998564720154\n",
      "Step: 8660, train/grad_norm: 0.001020403578877449\n",
      "Step: 8660, train/learning_rate: 3.96953837480396e-05\n",
      "Step: 8660, train/epoch: 2.0609233379364014\n",
      "Step: 8670, train/loss: 0.0\n",
      "Step: 8670, train/grad_norm: 0.00010478743206476793\n",
      "Step: 8670, train/learning_rate: 3.9683483919361606e-05\n",
      "Step: 8670, train/epoch: 2.063303232192993\n",
      "Step: 8680, train/loss: 0.0\n",
      "Step: 8680, train/grad_norm: 0.016537563875317574\n",
      "Step: 8680, train/learning_rate: 3.967158409068361e-05\n",
      "Step: 8680, train/epoch: 2.065683126449585\n",
      "Step: 8690, train/loss: 0.0\n",
      "Step: 8690, train/grad_norm: 0.010085755959153175\n",
      "Step: 8690, train/learning_rate: 3.965968426200561e-05\n",
      "Step: 8690, train/epoch: 2.0680627822875977\n",
      "Step: 8700, train/loss: 0.009200000204145908\n",
      "Step: 8700, train/grad_norm: 0.0010268185287714005\n",
      "Step: 8700, train/learning_rate: 3.964778807130642e-05\n",
      "Step: 8700, train/epoch: 2.0704426765441895\n",
      "Step: 8710, train/loss: 0.03189999982714653\n",
      "Step: 8710, train/grad_norm: 0.0017917986260727048\n",
      "Step: 8710, train/learning_rate: 3.9635888242628425e-05\n",
      "Step: 8710, train/epoch: 2.0728225708007812\n",
      "Step: 8720, train/loss: 0.0\n",
      "Step: 8720, train/grad_norm: 0.001876220921985805\n",
      "Step: 8720, train/learning_rate: 3.962398841395043e-05\n",
      "Step: 8720, train/epoch: 2.075202226638794\n",
      "Step: 8730, train/loss: 0.0\n",
      "Step: 8730, train/grad_norm: 0.000168893689988181\n",
      "Step: 8730, train/learning_rate: 3.961208858527243e-05\n",
      "Step: 8730, train/epoch: 2.0775821208953857\n",
      "Step: 8740, train/loss: 0.0\n",
      "Step: 8740, train/grad_norm: 0.1312616467475891\n",
      "Step: 8740, train/learning_rate: 3.9600188756594434e-05\n",
      "Step: 8740, train/epoch: 2.0799620151519775\n",
      "Step: 8750, train/loss: 0.0\n",
      "Step: 8750, train/grad_norm: 0.000764002907089889\n",
      "Step: 8750, train/learning_rate: 3.9588292565895244e-05\n",
      "Step: 8750, train/epoch: 2.0823416709899902\n",
      "Step: 8760, train/loss: 0.0\n",
      "Step: 8760, train/grad_norm: 0.011499143205583096\n",
      "Step: 8760, train/learning_rate: 3.957639273721725e-05\n",
      "Step: 8760, train/epoch: 2.084721565246582\n",
      "Step: 8770, train/loss: 0.0\n",
      "Step: 8770, train/grad_norm: 0.000512637896463275\n",
      "Step: 8770, train/learning_rate: 3.956449290853925e-05\n",
      "Step: 8770, train/epoch: 2.087101459503174\n",
      "Step: 8780, train/loss: 0.0\n",
      "Step: 8780, train/grad_norm: 0.00038286205381155014\n",
      "Step: 8780, train/learning_rate: 3.9552593079861253e-05\n",
      "Step: 8780, train/epoch: 2.0894811153411865\n",
      "Step: 8790, train/loss: 0.0\n",
      "Step: 8790, train/grad_norm: 0.0005241264589130878\n",
      "Step: 8790, train/learning_rate: 3.9540693251183257e-05\n",
      "Step: 8790, train/epoch: 2.0918610095977783\n",
      "Step: 8800, train/loss: 0.0\n",
      "Step: 8800, train/grad_norm: 5.54600810573902e-05\n",
      "Step: 8800, train/learning_rate: 3.9528797060484067e-05\n",
      "Step: 8800, train/epoch: 2.09424090385437\n",
      "Step: 8810, train/loss: 0.0\n",
      "Step: 8810, train/grad_norm: 9.852901712292805e-05\n",
      "Step: 8810, train/learning_rate: 3.951689723180607e-05\n",
      "Step: 8810, train/epoch: 2.096620559692383\n",
      "Step: 8820, train/loss: 0.0\n",
      "Step: 8820, train/grad_norm: 0.0007881937781348825\n",
      "Step: 8820, train/learning_rate: 3.950499740312807e-05\n",
      "Step: 8820, train/epoch: 2.0990004539489746\n",
      "Step: 8830, train/loss: 0.0\n",
      "Step: 8830, train/grad_norm: 0.0005638381117023528\n",
      "Step: 8830, train/learning_rate: 3.9493097574450076e-05\n",
      "Step: 8830, train/epoch: 2.1013803482055664\n",
      "Step: 8840, train/loss: 0.0\n",
      "Step: 8840, train/grad_norm: 0.00047157550579868257\n",
      "Step: 8840, train/learning_rate: 3.948119774577208e-05\n",
      "Step: 8840, train/epoch: 2.103760004043579\n",
      "Step: 8850, train/loss: 0.0\n",
      "Step: 8850, train/grad_norm: 2.140364631486591e-05\n",
      "Step: 8850, train/learning_rate: 3.946930155507289e-05\n",
      "Step: 8850, train/epoch: 2.106139898300171\n",
      "Step: 8860, train/loss: 0.0\n",
      "Step: 8860, train/grad_norm: 1.326610981777776e-05\n",
      "Step: 8860, train/learning_rate: 3.945740172639489e-05\n",
      "Step: 8860, train/epoch: 2.1085197925567627\n",
      "Step: 8870, train/loss: 0.0\n",
      "Step: 8870, train/grad_norm: 1.3789019249088597e-05\n",
      "Step: 8870, train/learning_rate: 3.9445501897716895e-05\n",
      "Step: 8870, train/epoch: 2.1108996868133545\n",
      "Step: 8880, train/loss: 0.0\n",
      "Step: 8880, train/grad_norm: 6.225089600775391e-05\n",
      "Step: 8880, train/learning_rate: 3.94336020690389e-05\n",
      "Step: 8880, train/epoch: 2.113279342651367\n",
      "Step: 8890, train/loss: 0.06560000032186508\n",
      "Step: 8890, train/grad_norm: 1.933771272888407e-05\n",
      "Step: 8890, train/learning_rate: 3.94217022403609e-05\n",
      "Step: 8890, train/epoch: 2.115659236907959\n",
      "Step: 8900, train/loss: 0.0\n",
      "Step: 8900, train/grad_norm: 0.00014865666162222624\n",
      "Step: 8900, train/learning_rate: 3.940980604966171e-05\n",
      "Step: 8900, train/epoch: 2.118039131164551\n",
      "Step: 8910, train/loss: 0.0\n",
      "Step: 8910, train/grad_norm: 1.5695140973548405e-05\n",
      "Step: 8910, train/learning_rate: 3.9397906220983714e-05\n",
      "Step: 8910, train/epoch: 2.1204187870025635\n",
      "Step: 8920, train/loss: 0.0\n",
      "Step: 8920, train/grad_norm: 3.122863972748746e-06\n",
      "Step: 8920, train/learning_rate: 3.938600639230572e-05\n",
      "Step: 8920, train/epoch: 2.1227986812591553\n",
      "Step: 8930, train/loss: 0.0\n",
      "Step: 8930, train/grad_norm: 5.986924224998802e-05\n",
      "Step: 8930, train/learning_rate: 3.937410656362772e-05\n",
      "Step: 8930, train/epoch: 2.125178575515747\n",
      "Step: 8940, train/loss: 0.0\n",
      "Step: 8940, train/grad_norm: 3.295638816780411e-05\n",
      "Step: 8940, train/learning_rate: 3.936220673494972e-05\n",
      "Step: 8940, train/epoch: 2.1275582313537598\n",
      "Step: 8950, train/loss: 0.14059999585151672\n",
      "Step: 8950, train/grad_norm: 82.24263763427734\n",
      "Step: 8950, train/learning_rate: 3.935031054425053e-05\n",
      "Step: 8950, train/epoch: 2.1299381256103516\n",
      "Step: 8960, train/loss: 0.0\n",
      "Step: 8960, train/grad_norm: 0.0007895021117292345\n",
      "Step: 8960, train/learning_rate: 3.9338410715572536e-05\n",
      "Step: 8960, train/epoch: 2.1323180198669434\n",
      "Step: 8970, train/loss: 0.0\n",
      "Step: 8970, train/grad_norm: 0.017793133854866028\n",
      "Step: 8970, train/learning_rate: 3.932651088689454e-05\n",
      "Step: 8970, train/epoch: 2.134697675704956\n",
      "Step: 8980, train/loss: 0.004800000227987766\n",
      "Step: 8980, train/grad_norm: 0.008854793384671211\n",
      "Step: 8980, train/learning_rate: 3.931461105821654e-05\n",
      "Step: 8980, train/epoch: 2.137077569961548\n",
      "Step: 8990, train/loss: 0.1679999977350235\n",
      "Step: 8990, train/grad_norm: 0.011092934757471085\n",
      "Step: 8990, train/learning_rate: 3.9302711229538545e-05\n",
      "Step: 8990, train/epoch: 2.1394574642181396\n",
      "Step: 9000, train/loss: 0.00800000037997961\n",
      "Step: 9000, train/grad_norm: 0.016261713579297066\n",
      "Step: 9000, train/learning_rate: 3.9290815038839355e-05\n",
      "Step: 9000, train/epoch: 2.1418371200561523\n",
      "Step: 9010, train/loss: 0.08129999786615372\n",
      "Step: 9010, train/grad_norm: 0.013445785269141197\n",
      "Step: 9010, train/learning_rate: 3.927891521016136e-05\n",
      "Step: 9010, train/epoch: 2.144217014312744\n",
      "Step: 9020, train/loss: 9.999999747378752e-05\n",
      "Step: 9020, train/grad_norm: 0.03904078155755997\n",
      "Step: 9020, train/learning_rate: 3.926701538148336e-05\n",
      "Step: 9020, train/epoch: 2.146596908569336\n",
      "Step: 9030, train/loss: 0.0\n",
      "Step: 9030, train/grad_norm: 0.0014910706086084247\n",
      "Step: 9030, train/learning_rate: 3.9255115552805364e-05\n",
      "Step: 9030, train/epoch: 2.1489765644073486\n",
      "Step: 9040, train/loss: 9.999999747378752e-05\n",
      "Step: 9040, train/grad_norm: 0.0007314329268410802\n",
      "Step: 9040, train/learning_rate: 3.924321572412737e-05\n",
      "Step: 9040, train/epoch: 2.1513564586639404\n",
      "Step: 9050, train/loss: 0.0\n",
      "Step: 9050, train/grad_norm: 0.0010513486340641975\n",
      "Step: 9050, train/learning_rate: 3.923131953342818e-05\n",
      "Step: 9050, train/epoch: 2.1537363529205322\n",
      "Step: 9060, train/loss: 0.0\n",
      "Step: 9060, train/grad_norm: 0.001262257108464837\n",
      "Step: 9060, train/learning_rate: 3.921941970475018e-05\n",
      "Step: 9060, train/epoch: 2.156116247177124\n",
      "Step: 9070, train/loss: 0.0\n",
      "Step: 9070, train/grad_norm: 0.000449283339548856\n",
      "Step: 9070, train/learning_rate: 3.920751987607218e-05\n",
      "Step: 9070, train/epoch: 2.1584959030151367\n",
      "Step: 9080, train/loss: 0.0\n",
      "Step: 9080, train/grad_norm: 8.583155431551859e-05\n",
      "Step: 9080, train/learning_rate: 3.9195620047394186e-05\n",
      "Step: 9080, train/epoch: 2.1608757972717285\n",
      "Step: 9090, train/loss: 0.0\n",
      "Step: 9090, train/grad_norm: 0.00022107642143964767\n",
      "Step: 9090, train/learning_rate: 3.918372021871619e-05\n",
      "Step: 9090, train/epoch: 2.1632556915283203\n",
      "Step: 9100, train/loss: 0.0\n",
      "Step: 9100, train/grad_norm: 0.00010723770537879318\n",
      "Step: 9100, train/learning_rate: 3.9171824028017e-05\n",
      "Step: 9100, train/epoch: 2.165635347366333\n",
      "Step: 9110, train/loss: 0.0640999972820282\n",
      "Step: 9110, train/grad_norm: 0.00029296017601154745\n",
      "Step: 9110, train/learning_rate: 3.9159924199339e-05\n",
      "Step: 9110, train/epoch: 2.168015241622925\n",
      "Step: 9120, train/loss: 0.0\n",
      "Step: 9120, train/grad_norm: 0.06837216019630432\n",
      "Step: 9120, train/learning_rate: 3.9148024370661005e-05\n",
      "Step: 9120, train/epoch: 2.1703951358795166\n",
      "Step: 9130, train/loss: 0.002300000051036477\n",
      "Step: 9130, train/grad_norm: 0.0008708556997589767\n",
      "Step: 9130, train/learning_rate: 3.913612454198301e-05\n",
      "Step: 9130, train/epoch: 2.1727747917175293\n",
      "Step: 9140, train/loss: 0.10080000013113022\n",
      "Step: 9140, train/grad_norm: 0.0008895422797650099\n",
      "Step: 9140, train/learning_rate: 3.912422835128382e-05\n",
      "Step: 9140, train/epoch: 2.175154685974121\n",
      "Step: 9150, train/loss: 0.13359999656677246\n",
      "Step: 9150, train/grad_norm: 0.0018532403046265244\n",
      "Step: 9150, train/learning_rate: 3.911232852260582e-05\n",
      "Step: 9150, train/epoch: 2.177534580230713\n",
      "Step: 9160, train/loss: 0.07699999958276749\n",
      "Step: 9160, train/grad_norm: 76.4990463256836\n",
      "Step: 9160, train/learning_rate: 3.9100428693927824e-05\n",
      "Step: 9160, train/epoch: 2.1799142360687256\n",
      "Step: 9170, train/loss: 0.1655000001192093\n",
      "Step: 9170, train/grad_norm: 16.68300437927246\n",
      "Step: 9170, train/learning_rate: 3.908852886524983e-05\n",
      "Step: 9170, train/epoch: 2.1822941303253174\n",
      "Step: 9180, train/loss: 0.0\n",
      "Step: 9180, train/grad_norm: 0.0022646829020231962\n",
      "Step: 9180, train/learning_rate: 3.907662903657183e-05\n",
      "Step: 9180, train/epoch: 2.184674024581909\n",
      "Step: 9190, train/loss: 9.999999747378752e-05\n",
      "Step: 9190, train/grad_norm: 0.003248650813475251\n",
      "Step: 9190, train/learning_rate: 3.906473284587264e-05\n",
      "Step: 9190, train/epoch: 2.187053680419922\n",
      "Step: 9200, train/loss: 0.0\n",
      "Step: 9200, train/grad_norm: 0.001127284369431436\n",
      "Step: 9200, train/learning_rate: 3.9052833017194644e-05\n",
      "Step: 9200, train/epoch: 2.1894335746765137\n",
      "Step: 9210, train/loss: 0.00039999998989515007\n",
      "Step: 9210, train/grad_norm: 0.00014411937445402145\n",
      "Step: 9210, train/learning_rate: 3.904093318851665e-05\n",
      "Step: 9210, train/epoch: 2.1918134689331055\n",
      "Step: 9220, train/loss: 0.34450000524520874\n",
      "Step: 9220, train/grad_norm: 88.76522827148438\n",
      "Step: 9220, train/learning_rate: 3.902903335983865e-05\n",
      "Step: 9220, train/epoch: 2.194193124771118\n",
      "Step: 9230, train/loss: 9.999999747378752e-05\n",
      "Step: 9230, train/grad_norm: 0.4408045709133148\n",
      "Step: 9230, train/learning_rate: 3.901713353116065e-05\n",
      "Step: 9230, train/epoch: 2.19657301902771\n",
      "Step: 9240, train/loss: 9.999999747378752e-05\n",
      "Step: 9240, train/grad_norm: 0.004774663597345352\n",
      "Step: 9240, train/learning_rate: 3.900523734046146e-05\n",
      "Step: 9240, train/epoch: 2.1989529132843018\n",
      "Step: 9250, train/loss: 0.0\n",
      "Step: 9250, train/grad_norm: 0.0005381929222494364\n",
      "Step: 9250, train/learning_rate: 3.8993337511783466e-05\n",
      "Step: 9250, train/epoch: 2.2013328075408936\n",
      "Step: 9260, train/loss: 0.34610000252723694\n",
      "Step: 9260, train/grad_norm: 0.004112855065613985\n",
      "Step: 9260, train/learning_rate: 3.898143768310547e-05\n",
      "Step: 9260, train/epoch: 2.2037124633789062\n",
      "Step: 9270, train/loss: 0.00019999999494757503\n",
      "Step: 9270, train/grad_norm: 0.03732966259121895\n",
      "Step: 9270, train/learning_rate: 3.896953785442747e-05\n",
      "Step: 9270, train/epoch: 2.206092357635498\n",
      "Step: 9280, train/loss: 9.999999747378752e-05\n",
      "Step: 9280, train/grad_norm: 0.010487187653779984\n",
      "Step: 9280, train/learning_rate: 3.8957638025749475e-05\n",
      "Step: 9280, train/epoch: 2.20847225189209\n",
      "Step: 9290, train/loss: 0.0\n",
      "Step: 9290, train/grad_norm: 0.0025094712618738413\n",
      "Step: 9290, train/learning_rate: 3.8945741835050285e-05\n",
      "Step: 9290, train/epoch: 2.2108519077301025\n",
      "Step: 9300, train/loss: 0.0\n",
      "Step: 9300, train/grad_norm: 0.00345230964012444\n",
      "Step: 9300, train/learning_rate: 3.893384200637229e-05\n",
      "Step: 9300, train/epoch: 2.2132318019866943\n",
      "Step: 9310, train/loss: 0.0\n",
      "Step: 9310, train/grad_norm: 0.0011413402389734983\n",
      "Step: 9310, train/learning_rate: 3.892194217769429e-05\n",
      "Step: 9310, train/epoch: 2.215611696243286\n",
      "Step: 9320, train/loss: 0.0\n",
      "Step: 9320, train/grad_norm: 0.00039563403697684407\n",
      "Step: 9320, train/learning_rate: 3.8910042349016294e-05\n",
      "Step: 9320, train/epoch: 2.217991352081299\n",
      "Step: 9330, train/loss: 0.0\n",
      "Step: 9330, train/grad_norm: 0.0015315673081204295\n",
      "Step: 9330, train/learning_rate: 3.88981425203383e-05\n",
      "Step: 9330, train/epoch: 2.2203712463378906\n",
      "Step: 9340, train/loss: 0.0\n",
      "Step: 9340, train/grad_norm: 0.0008320801425725222\n",
      "Step: 9340, train/learning_rate: 3.888624632963911e-05\n",
      "Step: 9340, train/epoch: 2.2227511405944824\n",
      "Step: 9350, train/loss: 0.0\n",
      "Step: 9350, train/grad_norm: 0.0012593988794833422\n",
      "Step: 9350, train/learning_rate: 3.887434650096111e-05\n",
      "Step: 9350, train/epoch: 2.225130796432495\n",
      "Step: 9360, train/loss: 0.011500000022351742\n",
      "Step: 9360, train/grad_norm: 0.00040760546107776463\n",
      "Step: 9360, train/learning_rate: 3.886244667228311e-05\n",
      "Step: 9360, train/epoch: 2.227510690689087\n",
      "Step: 9370, train/loss: 0.07580000162124634\n",
      "Step: 9370, train/grad_norm: 0.00034724900615401566\n",
      "Step: 9370, train/learning_rate: 3.8850546843605116e-05\n",
      "Step: 9370, train/epoch: 2.2298905849456787\n",
      "Step: 9380, train/loss: 0.0\n",
      "Step: 9380, train/grad_norm: 0.0005243748310022056\n",
      "Step: 9380, train/learning_rate: 3.883864701492712e-05\n",
      "Step: 9380, train/epoch: 2.2322702407836914\n",
      "Step: 9390, train/loss: 0.00019999999494757503\n",
      "Step: 9390, train/grad_norm: 0.0010238393442705274\n",
      "Step: 9390, train/learning_rate: 3.882675082422793e-05\n",
      "Step: 9390, train/epoch: 2.234650135040283\n",
      "Step: 9400, train/loss: 0.0\n",
      "Step: 9400, train/grad_norm: 0.0003029440122190863\n",
      "Step: 9400, train/learning_rate: 3.881485099554993e-05\n",
      "Step: 9400, train/epoch: 2.237030029296875\n",
      "Step: 9410, train/loss: 0.1242000013589859\n",
      "Step: 9410, train/grad_norm: 0.003370685037225485\n",
      "Step: 9410, train/learning_rate: 3.8802951166871935e-05\n",
      "Step: 9410, train/epoch: 2.239409923553467\n",
      "Step: 9420, train/loss: 9.999999747378752e-05\n",
      "Step: 9420, train/grad_norm: 0.01313717756420374\n",
      "Step: 9420, train/learning_rate: 3.879105133819394e-05\n",
      "Step: 9420, train/epoch: 2.2417895793914795\n",
      "Step: 9430, train/loss: 9.999999747378752e-05\n",
      "Step: 9430, train/grad_norm: 0.07943485677242279\n",
      "Step: 9430, train/learning_rate: 3.877915150951594e-05\n",
      "Step: 9430, train/epoch: 2.2441694736480713\n",
      "Step: 9440, train/loss: 0.0\n",
      "Step: 9440, train/grad_norm: 0.0071869902312755585\n",
      "Step: 9440, train/learning_rate: 3.876725531881675e-05\n",
      "Step: 9440, train/epoch: 2.246549367904663\n",
      "Step: 9450, train/loss: 0.12189999967813492\n",
      "Step: 9450, train/grad_norm: 90.78572082519531\n",
      "Step: 9450, train/learning_rate: 3.8755355490138754e-05\n",
      "Step: 9450, train/epoch: 2.248929023742676\n",
      "Step: 9460, train/loss: 0.0\n",
      "Step: 9460, train/grad_norm: 0.0030419055838137865\n",
      "Step: 9460, train/learning_rate: 3.874345566146076e-05\n",
      "Step: 9460, train/epoch: 2.2513089179992676\n",
      "Step: 9470, train/loss: 0.053599998354911804\n",
      "Step: 9470, train/grad_norm: 0.004906130023300648\n",
      "Step: 9470, train/learning_rate: 3.873155583278276e-05\n",
      "Step: 9470, train/epoch: 2.2536888122558594\n",
      "Step: 9480, train/loss: 0.10170000046491623\n",
      "Step: 9480, train/grad_norm: 89.03050994873047\n",
      "Step: 9480, train/learning_rate: 3.871965600410476e-05\n",
      "Step: 9480, train/epoch: 2.256068468093872\n",
      "Step: 9490, train/loss: 0.0\n",
      "Step: 9490, train/grad_norm: 0.030840598046779633\n",
      "Step: 9490, train/learning_rate: 3.870775981340557e-05\n",
      "Step: 9490, train/epoch: 2.258448362350464\n",
      "Step: 9500, train/loss: 9.999999747378752e-05\n",
      "Step: 9500, train/grad_norm: 0.013778170570731163\n",
      "Step: 9500, train/learning_rate: 3.8695859984727576e-05\n",
      "Step: 9500, train/epoch: 2.2608282566070557\n",
      "Step: 9510, train/loss: 0.20399999618530273\n",
      "Step: 9510, train/grad_norm: 0.11417793482542038\n",
      "Step: 9510, train/learning_rate: 3.868396015604958e-05\n",
      "Step: 9510, train/epoch: 2.2632079124450684\n",
      "Step: 9520, train/loss: 0.09809999912977219\n",
      "Step: 9520, train/grad_norm: 0.16684801876544952\n",
      "Step: 9520, train/learning_rate: 3.867206032737158e-05\n",
      "Step: 9520, train/epoch: 2.26558780670166\n",
      "Step: 9530, train/loss: 9.999999747378752e-05\n",
      "Step: 9530, train/grad_norm: 0.012554497458040714\n",
      "Step: 9530, train/learning_rate: 3.8660160498693585e-05\n",
      "Step: 9530, train/epoch: 2.267967700958252\n",
      "Step: 9540, train/loss: 0.0\n",
      "Step: 9540, train/grad_norm: 0.0028803639579564333\n",
      "Step: 9540, train/learning_rate: 3.8648264307994395e-05\n",
      "Step: 9540, train/epoch: 2.2703473567962646\n",
      "Step: 9550, train/loss: 0.09019999951124191\n",
      "Step: 9550, train/grad_norm: 0.0018792690243571997\n",
      "Step: 9550, train/learning_rate: 3.86363644793164e-05\n",
      "Step: 9550, train/epoch: 2.2727272510528564\n",
      "Step: 9560, train/loss: 0.30320000648498535\n",
      "Step: 9560, train/grad_norm: 0.09615912288427353\n",
      "Step: 9560, train/learning_rate: 3.86244646506384e-05\n",
      "Step: 9560, train/epoch: 2.2751071453094482\n",
      "Step: 9570, train/loss: 0.023900000378489494\n",
      "Step: 9570, train/grad_norm: 0.08429934084415436\n",
      "Step: 9570, train/learning_rate: 3.8612564821960405e-05\n",
      "Step: 9570, train/epoch: 2.277486801147461\n",
      "Step: 9580, train/loss: 0.1673000007867813\n",
      "Step: 9580, train/grad_norm: 0.004910001531243324\n",
      "Step: 9580, train/learning_rate: 3.860066499328241e-05\n",
      "Step: 9580, train/epoch: 2.2798666954040527\n",
      "Step: 9590, train/loss: 0.0\n",
      "Step: 9590, train/grad_norm: 0.005566542036831379\n",
      "Step: 9590, train/learning_rate: 3.858876880258322e-05\n",
      "Step: 9590, train/epoch: 2.2822465896606445\n",
      "Step: 9600, train/loss: 0.0\n",
      "Step: 9600, train/grad_norm: 0.0007088377024047077\n",
      "Step: 9600, train/learning_rate: 3.857686897390522e-05\n",
      "Step: 9600, train/epoch: 2.2846264839172363\n",
      "Step: 9610, train/loss: 0.13840000331401825\n",
      "Step: 9610, train/grad_norm: 0.04059334844350815\n",
      "Step: 9610, train/learning_rate: 3.8564969145227224e-05\n",
      "Step: 9610, train/epoch: 2.287006139755249\n",
      "Step: 9620, train/loss: 0.10189999639987946\n",
      "Step: 9620, train/grad_norm: 15.853170394897461\n",
      "Step: 9620, train/learning_rate: 3.855306931654923e-05\n",
      "Step: 9620, train/epoch: 2.289386034011841\n",
      "Step: 9630, train/loss: 0.09600000083446503\n",
      "Step: 9630, train/grad_norm: 0.03933662548661232\n",
      "Step: 9630, train/learning_rate: 3.854116948787123e-05\n",
      "Step: 9630, train/epoch: 2.2917659282684326\n",
      "Step: 9640, train/loss: 9.999999747378752e-05\n",
      "Step: 9640, train/grad_norm: 0.004240440670400858\n",
      "Step: 9640, train/learning_rate: 3.852927329717204e-05\n",
      "Step: 9640, train/epoch: 2.2941455841064453\n",
      "Step: 9650, train/loss: 0.0\n",
      "Step: 9650, train/grad_norm: 0.00032789784017950296\n",
      "Step: 9650, train/learning_rate: 3.851737346849404e-05\n",
      "Step: 9650, train/epoch: 2.296525478363037\n",
      "Step: 9660, train/loss: 0.0\n",
      "Step: 9660, train/grad_norm: 0.012879102490842342\n",
      "Step: 9660, train/learning_rate: 3.8505473639816046e-05\n",
      "Step: 9660, train/epoch: 2.298905372619629\n",
      "Step: 9670, train/loss: 0.1582999974489212\n",
      "Step: 9670, train/grad_norm: 0.0006036682170815766\n",
      "Step: 9670, train/learning_rate: 3.849357381113805e-05\n",
      "Step: 9670, train/epoch: 2.3012850284576416\n",
      "Step: 9680, train/loss: 0.0\n",
      "Step: 9680, train/grad_norm: 0.005542584229260683\n",
      "Step: 9680, train/learning_rate: 3.848167398246005e-05\n",
      "Step: 9680, train/epoch: 2.3036649227142334\n",
      "Step: 9690, train/loss: 0.012600000016391277\n",
      "Step: 9690, train/grad_norm: 89.70946502685547\n",
      "Step: 9690, train/learning_rate: 3.846977779176086e-05\n",
      "Step: 9690, train/epoch: 2.306044816970825\n",
      "Step: 9700, train/loss: 0.0\n",
      "Step: 9700, train/grad_norm: 0.007483946159482002\n",
      "Step: 9700, train/learning_rate: 3.8457877963082865e-05\n",
      "Step: 9700, train/epoch: 2.308424472808838\n",
      "Step: 9710, train/loss: 0.00800000037997961\n",
      "Step: 9710, train/grad_norm: 0.0008922603446990252\n",
      "Step: 9710, train/learning_rate: 3.844597813440487e-05\n",
      "Step: 9710, train/epoch: 2.3108043670654297\n",
      "Step: 9720, train/loss: 0.0\n",
      "Step: 9720, train/grad_norm: 0.002291254233568907\n",
      "Step: 9720, train/learning_rate: 3.843407830572687e-05\n",
      "Step: 9720, train/epoch: 2.3131842613220215\n",
      "Step: 9730, train/loss: 0.0\n",
      "Step: 9730, train/grad_norm: 0.002354217227548361\n",
      "Step: 9730, train/learning_rate: 3.8422178477048874e-05\n",
      "Step: 9730, train/epoch: 2.315563917160034\n",
      "Step: 9740, train/loss: 0.0\n",
      "Step: 9740, train/grad_norm: 0.0018797976663336158\n",
      "Step: 9740, train/learning_rate: 3.8410282286349684e-05\n",
      "Step: 9740, train/epoch: 2.317943811416626\n",
      "Step: 9750, train/loss: 0.23280000686645508\n",
      "Step: 9750, train/grad_norm: 290.7833557128906\n",
      "Step: 9750, train/learning_rate: 3.839838245767169e-05\n",
      "Step: 9750, train/epoch: 2.3203237056732178\n",
      "Step: 9760, train/loss: 0.09309999644756317\n",
      "Step: 9760, train/grad_norm: 0.06482841819524765\n",
      "Step: 9760, train/learning_rate: 3.838648262899369e-05\n",
      "Step: 9760, train/epoch: 2.3227033615112305\n",
      "Step: 9770, train/loss: 0.0544000007212162\n",
      "Step: 9770, train/grad_norm: 0.029923181980848312\n",
      "Step: 9770, train/learning_rate: 3.837458280031569e-05\n",
      "Step: 9770, train/epoch: 2.3250832557678223\n",
      "Step: 9780, train/loss: 9.999999747378752e-05\n",
      "Step: 9780, train/grad_norm: 0.08079539984464645\n",
      "Step: 9780, train/learning_rate: 3.8362682971637696e-05\n",
      "Step: 9780, train/epoch: 2.327463150024414\n",
      "Step: 9790, train/loss: 0.03750000149011612\n",
      "Step: 9790, train/grad_norm: 0.017344504594802856\n",
      "Step: 9790, train/learning_rate: 3.8350786780938506e-05\n",
      "Step: 9790, train/epoch: 2.329843044281006\n",
      "Step: 9800, train/loss: 0.0012000000569969416\n",
      "Step: 9800, train/grad_norm: 0.008654809556901455\n",
      "Step: 9800, train/learning_rate: 3.833888695226051e-05\n",
      "Step: 9800, train/epoch: 2.3322227001190186\n",
      "Step: 9810, train/loss: 0.0006000000284984708\n",
      "Step: 9810, train/grad_norm: 5.06464147567749\n",
      "Step: 9810, train/learning_rate: 3.832698712358251e-05\n",
      "Step: 9810, train/epoch: 2.3346025943756104\n",
      "Step: 9820, train/loss: 0.04899999871850014\n",
      "Step: 9820, train/grad_norm: 0.00026007043197751045\n",
      "Step: 9820, train/learning_rate: 3.8315087294904515e-05\n",
      "Step: 9820, train/epoch: 2.336982488632202\n",
      "Step: 9830, train/loss: 0.0\n",
      "Step: 9830, train/grad_norm: 0.00013304509047884494\n",
      "Step: 9830, train/learning_rate: 3.830318746622652e-05\n",
      "Step: 9830, train/epoch: 2.339362144470215\n",
      "Step: 9840, train/loss: 0.10819999873638153\n",
      "Step: 9840, train/grad_norm: 0.23368234932422638\n",
      "Step: 9840, train/learning_rate: 3.829129127552733e-05\n",
      "Step: 9840, train/epoch: 2.3417420387268066\n",
      "Step: 9850, train/loss: 0.0\n",
      "Step: 9850, train/grad_norm: 0.17291755974292755\n",
      "Step: 9850, train/learning_rate: 3.827939144684933e-05\n",
      "Step: 9850, train/epoch: 2.3441219329833984\n",
      "Step: 9860, train/loss: 0.1453000009059906\n",
      "Step: 9860, train/grad_norm: 86.34083557128906\n",
      "Step: 9860, train/learning_rate: 3.8267491618171334e-05\n",
      "Step: 9860, train/epoch: 2.346501588821411\n",
      "Step: 9870, train/loss: 0.14959999918937683\n",
      "Step: 9870, train/grad_norm: 0.11306490004062653\n",
      "Step: 9870, train/learning_rate: 3.825559178949334e-05\n",
      "Step: 9870, train/epoch: 2.348881483078003\n",
      "Step: 9880, train/loss: 0.00019999999494757503\n",
      "Step: 9880, train/grad_norm: 0.06354422122240067\n",
      "Step: 9880, train/learning_rate: 3.824369196081534e-05\n",
      "Step: 9880, train/epoch: 2.3512613773345947\n",
      "Step: 9890, train/loss: 0.037700001150369644\n",
      "Step: 9890, train/grad_norm: 0.14586050808429718\n",
      "Step: 9890, train/learning_rate: 3.823179577011615e-05\n",
      "Step: 9890, train/epoch: 2.3536410331726074\n",
      "Step: 9900, train/loss: 0.0640999972820282\n",
      "Step: 9900, train/grad_norm: 0.0023376073222607374\n",
      "Step: 9900, train/learning_rate: 3.821989594143815e-05\n",
      "Step: 9900, train/epoch: 2.356020927429199\n",
      "Step: 9910, train/loss: 0.15000000596046448\n",
      "Step: 9910, train/grad_norm: 355.1259765625\n",
      "Step: 9910, train/learning_rate: 3.8207996112760156e-05\n",
      "Step: 9910, train/epoch: 2.358400821685791\n",
      "Step: 9920, train/loss: 0.11490000039339066\n",
      "Step: 9920, train/grad_norm: 0.021229686215519905\n",
      "Step: 9920, train/learning_rate: 3.819609628408216e-05\n",
      "Step: 9920, train/epoch: 2.3607804775238037\n",
      "Step: 9930, train/loss: 0.0005000000237487257\n",
      "Step: 9930, train/grad_norm: 12.164843559265137\n",
      "Step: 9930, train/learning_rate: 3.818419645540416e-05\n",
      "Step: 9930, train/epoch: 2.3631603717803955\n",
      "Step: 9940, train/loss: 0.0\n",
      "Step: 9940, train/grad_norm: 0.004332272801548243\n",
      "Step: 9940, train/learning_rate: 3.817230026470497e-05\n",
      "Step: 9940, train/epoch: 2.3655402660369873\n",
      "Step: 9950, train/loss: 0.0\n",
      "Step: 9950, train/grad_norm: 0.0016950551653280854\n",
      "Step: 9950, train/learning_rate: 3.8160400436026976e-05\n",
      "Step: 9950, train/epoch: 2.367919921875\n",
      "Step: 9960, train/loss: 0.0\n",
      "Step: 9960, train/grad_norm: 0.0018119324231520295\n",
      "Step: 9960, train/learning_rate: 3.814850060734898e-05\n",
      "Step: 9960, train/epoch: 2.370299816131592\n",
      "Step: 9970, train/loss: 0.0\n",
      "Step: 9970, train/grad_norm: 0.001580918557010591\n",
      "Step: 9970, train/learning_rate: 3.813660077867098e-05\n",
      "Step: 9970, train/epoch: 2.3726797103881836\n",
      "Step: 9980, train/loss: 0.0\n",
      "Step: 9980, train/grad_norm: 0.0007511714939028025\n",
      "Step: 9980, train/learning_rate: 3.8124700949992985e-05\n",
      "Step: 9980, train/epoch: 2.3750596046447754\n",
      "Step: 9990, train/loss: 0.0\n",
      "Step: 9990, train/grad_norm: 0.0018212331924587488\n",
      "Step: 9990, train/learning_rate: 3.8112804759293795e-05\n",
      "Step: 9990, train/epoch: 2.377439260482788\n",
      "Step: 10000, train/loss: 0.0\n",
      "Step: 10000, train/grad_norm: 0.0013782217865809798\n",
      "Step: 10000, train/learning_rate: 3.81009049306158e-05\n",
      "Step: 10000, train/epoch: 2.37981915473938\n",
      "Step: 10010, train/loss: 0.0\n",
      "Step: 10010, train/grad_norm: 0.22960011661052704\n",
      "Step: 10010, train/learning_rate: 3.80890051019378e-05\n",
      "Step: 10010, train/epoch: 2.3821990489959717\n",
      "Step: 10020, train/loss: 0.0\n",
      "Step: 10020, train/grad_norm: 0.0004514103929977864\n",
      "Step: 10020, train/learning_rate: 3.8077105273259804e-05\n",
      "Step: 10020, train/epoch: 2.3845787048339844\n",
      "Step: 10030, train/loss: 0.0\n",
      "Step: 10030, train/grad_norm: 0.00014695840945933014\n",
      "Step: 10030, train/learning_rate: 3.806520544458181e-05\n",
      "Step: 10030, train/epoch: 2.386958599090576\n",
      "Step: 10040, train/loss: 0.13830000162124634\n",
      "Step: 10040, train/grad_norm: 85.60367584228516\n",
      "Step: 10040, train/learning_rate: 3.805330925388262e-05\n",
      "Step: 10040, train/epoch: 2.389338493347168\n",
      "Step: 10050, train/loss: 0.0\n",
      "Step: 10050, train/grad_norm: 0.0025846485514193773\n",
      "Step: 10050, train/learning_rate: 3.804140942520462e-05\n",
      "Step: 10050, train/epoch: 2.3917181491851807\n",
      "Step: 10060, train/loss: 0.0\n",
      "Step: 10060, train/grad_norm: 0.0032753648702055216\n",
      "Step: 10060, train/learning_rate: 3.802950959652662e-05\n",
      "Step: 10060, train/epoch: 2.3940980434417725\n",
      "Step: 10070, train/loss: 0.15360000729560852\n",
      "Step: 10070, train/grad_norm: 0.021310558542609215\n",
      "Step: 10070, train/learning_rate: 3.8017609767848626e-05\n",
      "Step: 10070, train/epoch: 2.3964779376983643\n",
      "Step: 10080, train/loss: 9.999999747378752e-05\n",
      "Step: 10080, train/grad_norm: 0.01741805672645569\n",
      "Step: 10080, train/learning_rate: 3.800570993917063e-05\n",
      "Step: 10080, train/epoch: 2.398857593536377\n",
      "Step: 10090, train/loss: 0.0\n",
      "Step: 10090, train/grad_norm: 0.004187227692455053\n",
      "Step: 10090, train/learning_rate: 3.799381374847144e-05\n",
      "Step: 10090, train/epoch: 2.4012374877929688\n",
      "Step: 10100, train/loss: 9.999999747378752e-05\n",
      "Step: 10100, train/grad_norm: 0.4197585880756378\n",
      "Step: 10100, train/learning_rate: 3.798191391979344e-05\n",
      "Step: 10100, train/epoch: 2.4036173820495605\n",
      "Step: 10110, train/loss: 0.0\n",
      "Step: 10110, train/grad_norm: 0.00026742610498331487\n",
      "Step: 10110, train/learning_rate: 3.7970014091115445e-05\n",
      "Step: 10110, train/epoch: 2.4059970378875732\n",
      "Step: 10120, train/loss: 0.0\n",
      "Step: 10120, train/grad_norm: 0.00038015071186237037\n",
      "Step: 10120, train/learning_rate: 3.795811426243745e-05\n",
      "Step: 10120, train/epoch: 2.408376932144165\n",
      "Step: 10130, train/loss: 0.0\n",
      "Step: 10130, train/grad_norm: 0.00010560564987827092\n",
      "Step: 10130, train/learning_rate: 3.794621443375945e-05\n",
      "Step: 10130, train/epoch: 2.410756826400757\n",
      "Step: 10140, train/loss: 0.0\n",
      "Step: 10140, train/grad_norm: 0.0005974280065856874\n",
      "Step: 10140, train/learning_rate: 3.793431824306026e-05\n",
      "Step: 10140, train/epoch: 2.4131367206573486\n",
      "Step: 10150, train/loss: 0.0\n",
      "Step: 10150, train/grad_norm: 0.00011239579907851294\n",
      "Step: 10150, train/learning_rate: 3.7922418414382264e-05\n",
      "Step: 10150, train/epoch: 2.4155163764953613\n",
      "Step: 10160, train/loss: 0.0\n",
      "Step: 10160, train/grad_norm: 0.0002706214436329901\n",
      "Step: 10160, train/learning_rate: 3.791051858570427e-05\n",
      "Step: 10160, train/epoch: 2.417896270751953\n",
      "Step: 10170, train/loss: 0.16949999332427979\n",
      "Step: 10170, train/grad_norm: 0.00015933267422951758\n",
      "Step: 10170, train/learning_rate: 3.789861875702627e-05\n",
      "Step: 10170, train/epoch: 2.420276165008545\n",
      "Step: 10180, train/loss: 0.0003000000142492354\n",
      "Step: 10180, train/grad_norm: 0.0020798142068088055\n",
      "Step: 10180, train/learning_rate: 3.788671892834827e-05\n",
      "Step: 10180, train/epoch: 2.4226558208465576\n",
      "Step: 10190, train/loss: 9.999999747378752e-05\n",
      "Step: 10190, train/grad_norm: 0.004433838184922934\n",
      "Step: 10190, train/learning_rate: 3.787482273764908e-05\n",
      "Step: 10190, train/epoch: 2.4250357151031494\n",
      "Step: 10200, train/loss: 0.0\n",
      "Step: 10200, train/grad_norm: 0.002771618077531457\n",
      "Step: 10200, train/learning_rate: 3.7862922908971086e-05\n",
      "Step: 10200, train/epoch: 2.427415609359741\n",
      "Step: 10210, train/loss: 0.0\n",
      "Step: 10210, train/grad_norm: 0.002799362177029252\n",
      "Step: 10210, train/learning_rate: 3.785102308029309e-05\n",
      "Step: 10210, train/epoch: 2.429795265197754\n",
      "Step: 10220, train/loss: 0.00019999999494757503\n",
      "Step: 10220, train/grad_norm: 0.002893444150686264\n",
      "Step: 10220, train/learning_rate: 3.783912325161509e-05\n",
      "Step: 10220, train/epoch: 2.4321751594543457\n",
      "Step: 10230, train/loss: 0.0\n",
      "Step: 10230, train/grad_norm: 0.003387540578842163\n",
      "Step: 10230, train/learning_rate: 3.7827223422937095e-05\n",
      "Step: 10230, train/epoch: 2.4345550537109375\n",
      "Step: 10240, train/loss: 0.0\n",
      "Step: 10240, train/grad_norm: 0.00211311224848032\n",
      "Step: 10240, train/learning_rate: 3.7815327232237905e-05\n",
      "Step: 10240, train/epoch: 2.43693470954895\n",
      "Step: 10250, train/loss: 0.0\n",
      "Step: 10250, train/grad_norm: 0.0012353021884337068\n",
      "Step: 10250, train/learning_rate: 3.780342740355991e-05\n",
      "Step: 10250, train/epoch: 2.439314603805542\n",
      "Step: 10260, train/loss: 0.0\n",
      "Step: 10260, train/grad_norm: 0.005087939091026783\n",
      "Step: 10260, train/learning_rate: 3.779152757488191e-05\n",
      "Step: 10260, train/epoch: 2.441694498062134\n",
      "Step: 10270, train/loss: 0.0031999999191612005\n",
      "Step: 10270, train/grad_norm: 42.33848190307617\n",
      "Step: 10270, train/learning_rate: 3.7779627746203914e-05\n",
      "Step: 10270, train/epoch: 2.4440741539001465\n",
      "Step: 10280, train/loss: 0.22030000388622284\n",
      "Step: 10280, train/grad_norm: 0.0022508478723466396\n",
      "Step: 10280, train/learning_rate: 3.776772791752592e-05\n",
      "Step: 10280, train/epoch: 2.4464540481567383\n",
      "Step: 10290, train/loss: 0.0\n",
      "Step: 10290, train/grad_norm: 0.003942381590604782\n",
      "Step: 10290, train/learning_rate: 3.775583172682673e-05\n",
      "Step: 10290, train/epoch: 2.44883394241333\n",
      "Step: 10300, train/loss: 0.0\n",
      "Step: 10300, train/grad_norm: 0.008487208746373653\n",
      "Step: 10300, train/learning_rate: 3.774393189814873e-05\n",
      "Step: 10300, train/epoch: 2.4512135982513428\n",
      "Step: 10310, train/loss: 9.999999747378752e-05\n",
      "Step: 10310, train/grad_norm: 0.02261933870613575\n",
      "Step: 10310, train/learning_rate: 3.7732032069470733e-05\n",
      "Step: 10310, train/epoch: 2.4535934925079346\n",
      "Step: 10320, train/loss: 0.0\n",
      "Step: 10320, train/grad_norm: 0.016532711684703827\n",
      "Step: 10320, train/learning_rate: 3.7720132240792736e-05\n",
      "Step: 10320, train/epoch: 2.4559733867645264\n",
      "Step: 10330, train/loss: 0.12269999831914902\n",
      "Step: 10330, train/grad_norm: 74.72026062011719\n",
      "Step: 10330, train/learning_rate: 3.770823241211474e-05\n",
      "Step: 10330, train/epoch: 2.458353281021118\n",
      "Step: 10340, train/loss: 0.0\n",
      "Step: 10340, train/grad_norm: 0.02574266865849495\n",
      "Step: 10340, train/learning_rate: 3.769633622141555e-05\n",
      "Step: 10340, train/epoch: 2.460732936859131\n",
      "Step: 10350, train/loss: 0.15479999780654907\n",
      "Step: 10350, train/grad_norm: 0.10729507356882095\n",
      "Step: 10350, train/learning_rate: 3.768443639273755e-05\n",
      "Step: 10350, train/epoch: 2.4631128311157227\n",
      "Step: 10360, train/loss: 0.05400000140070915\n",
      "Step: 10360, train/grad_norm: 0.032610516995191574\n",
      "Step: 10360, train/learning_rate: 3.7672536564059556e-05\n",
      "Step: 10360, train/epoch: 2.4654927253723145\n",
      "Step: 10370, train/loss: 0.004399999976158142\n",
      "Step: 10370, train/grad_norm: 0.004518302623182535\n",
      "Step: 10370, train/learning_rate: 3.766063673538156e-05\n",
      "Step: 10370, train/epoch: 2.467872381210327\n",
      "Step: 10380, train/loss: 0.0\n",
      "Step: 10380, train/grad_norm: 0.0006119694444350898\n",
      "Step: 10380, train/learning_rate: 3.764873690670356e-05\n",
      "Step: 10380, train/epoch: 2.470252275466919\n",
      "Step: 10390, train/loss: 0.0\n",
      "Step: 10390, train/grad_norm: 0.0014831910375505686\n",
      "Step: 10390, train/learning_rate: 3.763684071600437e-05\n",
      "Step: 10390, train/epoch: 2.4726321697235107\n",
      "Step: 10400, train/loss: 0.0\n",
      "Step: 10400, train/grad_norm: 0.0017100429395213723\n",
      "Step: 10400, train/learning_rate: 3.7624940887326375e-05\n",
      "Step: 10400, train/epoch: 2.4750118255615234\n",
      "Step: 10410, train/loss: 0.0\n",
      "Step: 10410, train/grad_norm: 0.0012726718559861183\n",
      "Step: 10410, train/learning_rate: 3.761304105864838e-05\n",
      "Step: 10410, train/epoch: 2.4773917198181152\n",
      "Step: 10420, train/loss: 0.09449999779462814\n",
      "Step: 10420, train/grad_norm: 0.00043354523950256407\n",
      "Step: 10420, train/learning_rate: 3.760114122997038e-05\n",
      "Step: 10420, train/epoch: 2.479771614074707\n",
      "Step: 10430, train/loss: 0.0\n",
      "Step: 10430, train/grad_norm: 0.0025388263165950775\n",
      "Step: 10430, train/learning_rate: 3.7589241401292384e-05\n",
      "Step: 10430, train/epoch: 2.4821512699127197\n",
      "Step: 10440, train/loss: 0.0\n",
      "Step: 10440, train/grad_norm: 0.01891152188181877\n",
      "Step: 10440, train/learning_rate: 3.7577345210593194e-05\n",
      "Step: 10440, train/epoch: 2.4845311641693115\n",
      "Step: 10450, train/loss: 0.0\n",
      "Step: 10450, train/grad_norm: 0.024369509890675545\n",
      "Step: 10450, train/learning_rate: 3.75654453819152e-05\n",
      "Step: 10450, train/epoch: 2.4869110584259033\n",
      "Step: 10460, train/loss: 9.999999747378752e-05\n",
      "Step: 10460, train/grad_norm: 0.005970177240669727\n",
      "Step: 10460, train/learning_rate: 3.75535455532372e-05\n",
      "Step: 10460, train/epoch: 2.489290714263916\n",
      "Step: 10470, train/loss: 0.0\n",
      "Step: 10470, train/grad_norm: 0.005661436356604099\n",
      "Step: 10470, train/learning_rate: 3.75416457245592e-05\n",
      "Step: 10470, train/epoch: 2.491670608520508\n",
      "Step: 10480, train/loss: 0.0\n",
      "Step: 10480, train/grad_norm: 0.0006758057861588895\n",
      "Step: 10480, train/learning_rate: 3.752974953386001e-05\n",
      "Step: 10480, train/epoch: 2.4940505027770996\n",
      "Step: 10490, train/loss: 0.0\n",
      "Step: 10490, train/grad_norm: 0.00020926246361341327\n",
      "Step: 10490, train/learning_rate: 3.7517849705182016e-05\n",
      "Step: 10490, train/epoch: 2.4964301586151123\n",
      "Step: 10500, train/loss: 0.0\n",
      "Step: 10500, train/grad_norm: 0.0005224441993050277\n",
      "Step: 10500, train/learning_rate: 3.750594987650402e-05\n",
      "Step: 10500, train/epoch: 2.498810052871704\n",
      "Step: 10510, train/loss: 0.0\n",
      "Step: 10510, train/grad_norm: 0.00021368863235693425\n",
      "Step: 10510, train/learning_rate: 3.749405004782602e-05\n",
      "Step: 10510, train/epoch: 2.501189947128296\n",
      "Step: 10520, train/loss: 0.0\n",
      "Step: 10520, train/grad_norm: 0.00031913057318888605\n",
      "Step: 10520, train/learning_rate: 3.7482150219148025e-05\n",
      "Step: 10520, train/epoch: 2.5035698413848877\n",
      "Step: 10530, train/loss: 0.0\n",
      "Step: 10530, train/grad_norm: 0.0001154476631199941\n",
      "Step: 10530, train/learning_rate: 3.7470254028448835e-05\n",
      "Step: 10530, train/epoch: 2.5059494972229004\n",
      "Step: 10540, train/loss: 0.0\n",
      "Step: 10540, train/grad_norm: 0.00014246047066990286\n",
      "Step: 10540, train/learning_rate: 3.745835419977084e-05\n",
      "Step: 10540, train/epoch: 2.508329391479492\n",
      "Step: 10550, train/loss: 0.0\n",
      "Step: 10550, train/grad_norm: 0.0001687991461949423\n",
      "Step: 10550, train/learning_rate: 3.744645437109284e-05\n",
      "Step: 10550, train/epoch: 2.510709285736084\n",
      "Step: 10560, train/loss: 0.0\n",
      "Step: 10560, train/grad_norm: 0.0005290114204399288\n",
      "Step: 10560, train/learning_rate: 3.7434554542414844e-05\n",
      "Step: 10560, train/epoch: 2.5130889415740967\n",
      "Step: 10570, train/loss: 0.0\n",
      "Step: 10570, train/grad_norm: 0.0003016434784512967\n",
      "Step: 10570, train/learning_rate: 3.742265471373685e-05\n",
      "Step: 10570, train/epoch: 2.5154688358306885\n",
      "Step: 10580, train/loss: 0.0\n",
      "Step: 10580, train/grad_norm: 9.102228068513796e-05\n",
      "Step: 10580, train/learning_rate: 3.741075852303766e-05\n",
      "Step: 10580, train/epoch: 2.5178487300872803\n",
      "Step: 10590, train/loss: 0.0\n",
      "Step: 10590, train/grad_norm: 0.0003799931437242776\n",
      "Step: 10590, train/learning_rate: 3.739885869435966e-05\n",
      "Step: 10590, train/epoch: 2.520228385925293\n",
      "Step: 10600, train/loss: 0.0\n",
      "Step: 10600, train/grad_norm: 0.01934223249554634\n",
      "Step: 10600, train/learning_rate: 3.738695886568166e-05\n",
      "Step: 10600, train/epoch: 2.5226082801818848\n",
      "Step: 10610, train/loss: 0.0\n",
      "Step: 10610, train/grad_norm: 0.0004557858919724822\n",
      "Step: 10610, train/learning_rate: 3.7375059037003666e-05\n",
      "Step: 10610, train/epoch: 2.5249881744384766\n",
      "Step: 10620, train/loss: 0.0\n",
      "Step: 10620, train/grad_norm: 7.373205880867317e-05\n",
      "Step: 10620, train/learning_rate: 3.736315920832567e-05\n",
      "Step: 10620, train/epoch: 2.5273678302764893\n",
      "Step: 10630, train/loss: 0.0\n",
      "Step: 10630, train/grad_norm: 0.000130775137222372\n",
      "Step: 10630, train/learning_rate: 3.735126301762648e-05\n",
      "Step: 10630, train/epoch: 2.529747724533081\n",
      "Step: 10640, train/loss: 0.0\n",
      "Step: 10640, train/grad_norm: 0.00011905177234439179\n",
      "Step: 10640, train/learning_rate: 3.733936318894848e-05\n",
      "Step: 10640, train/epoch: 2.532127618789673\n",
      "Step: 10650, train/loss: 0.0\n",
      "Step: 10650, train/grad_norm: 9.362411219626665e-05\n",
      "Step: 10650, train/learning_rate: 3.7327463360270485e-05\n",
      "Step: 10650, train/epoch: 2.5345072746276855\n",
      "Step: 10660, train/loss: 0.0\n",
      "Step: 10660, train/grad_norm: 6.557873712154105e-05\n",
      "Step: 10660, train/learning_rate: 3.731556353159249e-05\n",
      "Step: 10660, train/epoch: 2.5368871688842773\n",
      "Step: 10670, train/loss: 0.0\n",
      "Step: 10670, train/grad_norm: 0.00012175612937426195\n",
      "Step: 10670, train/learning_rate: 3.730366370291449e-05\n",
      "Step: 10670, train/epoch: 2.539267063140869\n",
      "Step: 10680, train/loss: 0.0\n",
      "Step: 10680, train/grad_norm: 0.00016051781130954623\n",
      "Step: 10680, train/learning_rate: 3.72917675122153e-05\n",
      "Step: 10680, train/epoch: 2.541646718978882\n",
      "Step: 10690, train/loss: 0.0\n",
      "Step: 10690, train/grad_norm: 0.00019228032033424824\n",
      "Step: 10690, train/learning_rate: 3.7279867683537304e-05\n",
      "Step: 10690, train/epoch: 2.5440266132354736\n",
      "Step: 10700, train/loss: 0.0\n",
      "Step: 10700, train/grad_norm: 9.65276631177403e-05\n",
      "Step: 10700, train/learning_rate: 3.726796785485931e-05\n",
      "Step: 10700, train/epoch: 2.5464065074920654\n",
      "Step: 10710, train/loss: 0.0\n",
      "Step: 10710, train/grad_norm: 0.00010587798897176981\n",
      "Step: 10710, train/learning_rate: 3.725606802618131e-05\n",
      "Step: 10710, train/epoch: 2.5487864017486572\n",
      "Step: 10720, train/loss: 0.0\n",
      "Step: 10720, train/grad_norm: 9.295577183365822e-05\n",
      "Step: 10720, train/learning_rate: 3.7244168197503313e-05\n",
      "Step: 10720, train/epoch: 2.55116605758667\n",
      "Step: 10730, train/loss: 0.0\n",
      "Step: 10730, train/grad_norm: 0.00016447543748654425\n",
      "Step: 10730, train/learning_rate: 3.7232272006804124e-05\n",
      "Step: 10730, train/epoch: 2.5535459518432617\n",
      "Step: 10740, train/loss: 0.18440000712871552\n",
      "Step: 10740, train/grad_norm: 518.4680786132812\n",
      "Step: 10740, train/learning_rate: 3.7220372178126127e-05\n",
      "Step: 10740, train/epoch: 2.5559258460998535\n",
      "Step: 10750, train/loss: 0.0\n",
      "Step: 10750, train/grad_norm: 0.0001972497848328203\n",
      "Step: 10750, train/learning_rate: 3.720847234944813e-05\n",
      "Step: 10750, train/epoch: 2.558305501937866\n",
      "Step: 10760, train/loss: 0.00019999999494757503\n",
      "Step: 10760, train/grad_norm: 0.00015307871217373759\n",
      "Step: 10760, train/learning_rate: 3.719657252077013e-05\n",
      "Step: 10760, train/epoch: 2.560685396194458\n",
      "Step: 10770, train/loss: 0.013100000098347664\n",
      "Step: 10770, train/grad_norm: 0.00020698393927887082\n",
      "Step: 10770, train/learning_rate: 3.7184672692092136e-05\n",
      "Step: 10770, train/epoch: 2.56306529045105\n",
      "Step: 10780, train/loss: 0.0\n",
      "Step: 10780, train/grad_norm: 9.778632374946028e-06\n",
      "Step: 10780, train/learning_rate: 3.7172776501392946e-05\n",
      "Step: 10780, train/epoch: 2.5654449462890625\n",
      "Step: 10790, train/loss: 0.4634999930858612\n",
      "Step: 10790, train/grad_norm: 13.91428279876709\n",
      "Step: 10790, train/learning_rate: 3.716087667271495e-05\n",
      "Step: 10790, train/epoch: 2.5678248405456543\n",
      "Step: 10800, train/loss: 0.0066999997943639755\n",
      "Step: 10800, train/grad_norm: 0.01490596029907465\n",
      "Step: 10800, train/learning_rate: 3.714897684403695e-05\n",
      "Step: 10800, train/epoch: 2.570204734802246\n",
      "Step: 10810, train/loss: 0.0\n",
      "Step: 10810, train/grad_norm: 4.214299042359926e-05\n",
      "Step: 10810, train/learning_rate: 3.7137077015358955e-05\n",
      "Step: 10810, train/epoch: 2.572584390640259\n",
      "Step: 10820, train/loss: 0.0\n",
      "Step: 10820, train/grad_norm: 1.0816714848260744e-06\n",
      "Step: 10820, train/learning_rate: 3.712517718668096e-05\n",
      "Step: 10820, train/epoch: 2.5749642848968506\n",
      "Step: 10830, train/loss: 0.0\n",
      "Step: 10830, train/grad_norm: 3.667162218334852e-07\n",
      "Step: 10830, train/learning_rate: 3.711328099598177e-05\n",
      "Step: 10830, train/epoch: 2.5773441791534424\n",
      "Step: 10840, train/loss: 0.0\n",
      "Step: 10840, train/grad_norm: 1.0841264952432539e-07\n",
      "Step: 10840, train/learning_rate: 3.710138116730377e-05\n",
      "Step: 10840, train/epoch: 2.579723834991455\n",
      "Step: 10850, train/loss: 0.0\n",
      "Step: 10850, train/grad_norm: 1.9860260636050953e-06\n",
      "Step: 10850, train/learning_rate: 3.7089481338625774e-05\n",
      "Step: 10850, train/epoch: 2.582103729248047\n",
      "Step: 10860, train/loss: 0.5625\n",
      "Step: 10860, train/grad_norm: 97.36094665527344\n",
      "Step: 10860, train/learning_rate: 3.707758150994778e-05\n",
      "Step: 10860, train/epoch: 2.5844836235046387\n",
      "Step: 10870, train/loss: 9.999999747378752e-05\n",
      "Step: 10870, train/grad_norm: 0.5770525336265564\n",
      "Step: 10870, train/learning_rate: 3.706568168126978e-05\n",
      "Step: 10870, train/epoch: 2.5868632793426514\n",
      "Step: 10880, train/loss: 0.0005000000237487257\n",
      "Step: 10880, train/grad_norm: 7.663210868835449\n",
      "Step: 10880, train/learning_rate: 3.705378549057059e-05\n",
      "Step: 10880, train/epoch: 2.589243173599243\n",
      "Step: 10890, train/loss: 0.0\n",
      "Step: 10890, train/grad_norm: 0.0014240927994251251\n",
      "Step: 10890, train/learning_rate: 3.704188566189259e-05\n",
      "Step: 10890, train/epoch: 2.591623067855835\n",
      "Step: 10900, train/loss: 0.00019999999494757503\n",
      "Step: 10900, train/grad_norm: 9.445496834814548e-05\n",
      "Step: 10900, train/learning_rate: 3.7029985833214596e-05\n",
      "Step: 10900, train/epoch: 2.5940029621124268\n",
      "Step: 10910, train/loss: 0.0\n",
      "Step: 10910, train/grad_norm: 0.0001310512307099998\n",
      "Step: 10910, train/learning_rate: 3.70180860045366e-05\n",
      "Step: 10910, train/epoch: 2.5963826179504395\n",
      "Step: 10920, train/loss: 0.0\n",
      "Step: 10920, train/grad_norm: 3.270572779001668e-05\n",
      "Step: 10920, train/learning_rate: 3.70061861758586e-05\n",
      "Step: 10920, train/epoch: 2.5987625122070312\n",
      "Step: 10930, train/loss: 0.20160000026226044\n",
      "Step: 10930, train/grad_norm: 0.04584110900759697\n",
      "Step: 10930, train/learning_rate: 3.699428998515941e-05\n",
      "Step: 10930, train/epoch: 2.601142406463623\n",
      "Step: 10940, train/loss: 9.999999747378752e-05\n",
      "Step: 10940, train/grad_norm: 0.011363129131495953\n",
      "Step: 10940, train/learning_rate: 3.6982390156481415e-05\n",
      "Step: 10940, train/epoch: 2.6035220623016357\n",
      "Step: 10950, train/loss: 0.15240000188350677\n",
      "Step: 10950, train/grad_norm: 0.0033584474585950375\n",
      "Step: 10950, train/learning_rate: 3.697049032780342e-05\n",
      "Step: 10950, train/epoch: 2.6059019565582275\n",
      "Step: 10960, train/loss: 0.0\n",
      "Step: 10960, train/grad_norm: 0.03226708248257637\n",
      "Step: 10960, train/learning_rate: 3.695859049912542e-05\n",
      "Step: 10960, train/epoch: 2.6082818508148193\n",
      "Step: 10970, train/loss: 0.09080000221729279\n",
      "Step: 10970, train/grad_norm: 0.1029215157032013\n",
      "Step: 10970, train/learning_rate: 3.6946690670447424e-05\n",
      "Step: 10970, train/epoch: 2.610661506652832\n",
      "Step: 10980, train/loss: 0.0\n",
      "Step: 10980, train/grad_norm: 0.002559415064752102\n",
      "Step: 10980, train/learning_rate: 3.6934794479748234e-05\n",
      "Step: 10980, train/epoch: 2.613041400909424\n",
      "Step: 10990, train/loss: 0.13279999792575836\n",
      "Step: 10990, train/grad_norm: 0.0012083940673619509\n",
      "Step: 10990, train/learning_rate: 3.692289465107024e-05\n",
      "Step: 10990, train/epoch: 2.6154212951660156\n",
      "Step: 11000, train/loss: 0.0\n",
      "Step: 11000, train/grad_norm: 0.017845435068011284\n",
      "Step: 11000, train/learning_rate: 3.691099482239224e-05\n",
      "Step: 11000, train/epoch: 2.6178009510040283\n",
      "Step: 11010, train/loss: 0.00019999999494757503\n",
      "Step: 11010, train/grad_norm: 0.00074679811950773\n",
      "Step: 11010, train/learning_rate: 3.689909499371424e-05\n",
      "Step: 11010, train/epoch: 2.62018084526062\n",
      "Step: 11020, train/loss: 0.0\n",
      "Step: 11020, train/grad_norm: 0.0004183500714134425\n",
      "Step: 11020, train/learning_rate: 3.6887195165036246e-05\n",
      "Step: 11020, train/epoch: 2.622560739517212\n",
      "Step: 11030, train/loss: 0.0\n",
      "Step: 11030, train/grad_norm: 0.00011350393469911069\n",
      "Step: 11030, train/learning_rate: 3.6875298974337056e-05\n",
      "Step: 11030, train/epoch: 2.6249403953552246\n",
      "Step: 11040, train/loss: 0.0\n",
      "Step: 11040, train/grad_norm: 0.00021294252655934542\n",
      "Step: 11040, train/learning_rate: 3.686339914565906e-05\n",
      "Step: 11040, train/epoch: 2.6273202896118164\n",
      "Step: 11050, train/loss: 0.0\n",
      "Step: 11050, train/grad_norm: 7.363707118202001e-05\n",
      "Step: 11050, train/learning_rate: 3.685149931698106e-05\n",
      "Step: 11050, train/epoch: 2.629700183868408\n",
      "Step: 11060, train/loss: 0.0\n",
      "Step: 11060, train/grad_norm: 0.00016642326954752207\n",
      "Step: 11060, train/learning_rate: 3.6839599488303065e-05\n",
      "Step: 11060, train/epoch: 2.632080078125\n",
      "Step: 11070, train/loss: 0.0\n",
      "Step: 11070, train/grad_norm: 0.0005019462550990283\n",
      "Step: 11070, train/learning_rate: 3.682769965962507e-05\n",
      "Step: 11070, train/epoch: 2.6344597339630127\n",
      "Step: 11080, train/loss: 0.15700000524520874\n",
      "Step: 11080, train/grad_norm: 0.0018653636798262596\n",
      "Step: 11080, train/learning_rate: 3.681580346892588e-05\n",
      "Step: 11080, train/epoch: 2.6368396282196045\n",
      "Step: 11090, train/loss: 0.0\n",
      "Step: 11090, train/grad_norm: 0.024170003831386566\n",
      "Step: 11090, train/learning_rate: 3.680390364024788e-05\n",
      "Step: 11090, train/epoch: 2.6392195224761963\n",
      "Step: 11100, train/loss: 9.999999747378752e-05\n",
      "Step: 11100, train/grad_norm: 0.023325316607952118\n",
      "Step: 11100, train/learning_rate: 3.6792003811569884e-05\n",
      "Step: 11100, train/epoch: 2.641599178314209\n",
      "Step: 11110, train/loss: 0.0\n",
      "Step: 11110, train/grad_norm: 0.0035528126172721386\n",
      "Step: 11110, train/learning_rate: 3.678010398289189e-05\n",
      "Step: 11110, train/epoch: 2.643979072570801\n",
      "Step: 11120, train/loss: 0.10320000350475311\n",
      "Step: 11120, train/grad_norm: 0.0016312077641487122\n",
      "Step: 11120, train/learning_rate: 3.676820415421389e-05\n",
      "Step: 11120, train/epoch: 2.6463589668273926\n",
      "Step: 11130, train/loss: 0.0\n",
      "Step: 11130, train/grad_norm: 0.026443131268024445\n",
      "Step: 11130, train/learning_rate: 3.67563079635147e-05\n",
      "Step: 11130, train/epoch: 2.6487386226654053\n",
      "Step: 11140, train/loss: 0.06639999896287918\n",
      "Step: 11140, train/grad_norm: 103.0541000366211\n",
      "Step: 11140, train/learning_rate: 3.6744408134836704e-05\n",
      "Step: 11140, train/epoch: 2.651118516921997\n",
      "Step: 11150, train/loss: 0.0\n",
      "Step: 11150, train/grad_norm: 0.002612246898934245\n",
      "Step: 11150, train/learning_rate: 3.6732508306158707e-05\n",
      "Step: 11150, train/epoch: 2.653498411178589\n",
      "Step: 11160, train/loss: 0.0\n",
      "Step: 11160, train/grad_norm: 0.004111118149012327\n",
      "Step: 11160, train/learning_rate: 3.672060847748071e-05\n",
      "Step: 11160, train/epoch: 2.6558780670166016\n",
      "Step: 11170, train/loss: 0.0\n",
      "Step: 11170, train/grad_norm: 0.0027147182263433933\n",
      "Step: 11170, train/learning_rate: 3.670870864880271e-05\n",
      "Step: 11170, train/epoch: 2.6582579612731934\n",
      "Step: 11180, train/loss: 9.999999747378752e-05\n",
      "Step: 11180, train/grad_norm: 0.000484654912725091\n",
      "Step: 11180, train/learning_rate: 3.669681245810352e-05\n",
      "Step: 11180, train/epoch: 2.660637855529785\n",
      "Step: 11190, train/loss: 0.1867000013589859\n",
      "Step: 11190, train/grad_norm: 0.026878628879785538\n",
      "Step: 11190, train/learning_rate: 3.6684912629425526e-05\n",
      "Step: 11190, train/epoch: 2.663017511367798\n",
      "Step: 11200, train/loss: 0.0\n",
      "Step: 11200, train/grad_norm: 0.017589807510375977\n",
      "Step: 11200, train/learning_rate: 3.667301280074753e-05\n",
      "Step: 11200, train/epoch: 2.6653974056243896\n",
      "Step: 11210, train/loss: 9.999999747378752e-05\n",
      "Step: 11210, train/grad_norm: 0.0045950752682983875\n",
      "Step: 11210, train/learning_rate: 3.666111297206953e-05\n",
      "Step: 11210, train/epoch: 2.6677772998809814\n",
      "Step: 11220, train/loss: 0.0\n",
      "Step: 11220, train/grad_norm: 0.002356689190492034\n",
      "Step: 11220, train/learning_rate: 3.6649213143391535e-05\n",
      "Step: 11220, train/epoch: 2.670156955718994\n",
      "Step: 11230, train/loss: 0.0\n",
      "Step: 11230, train/grad_norm: 0.00607936829328537\n",
      "Step: 11230, train/learning_rate: 3.6637316952692345e-05\n",
      "Step: 11230, train/epoch: 2.672536849975586\n",
      "Step: 11240, train/loss: 0.13830000162124634\n",
      "Step: 11240, train/grad_norm: 0.03598041459918022\n",
      "Step: 11240, train/learning_rate: 3.662541712401435e-05\n",
      "Step: 11240, train/epoch: 2.6749167442321777\n",
      "Step: 11250, train/loss: 0.0010000000474974513\n",
      "Step: 11250, train/grad_norm: 0.019982051104307175\n",
      "Step: 11250, train/learning_rate: 3.661351729533635e-05\n",
      "Step: 11250, train/epoch: 2.6772966384887695\n",
      "Step: 11260, train/loss: 0.10080000013113022\n",
      "Step: 11260, train/grad_norm: 0.011241085827350616\n",
      "Step: 11260, train/learning_rate: 3.6601617466658354e-05\n",
      "Step: 11260, train/epoch: 2.6796762943267822\n",
      "Step: 11270, train/loss: 0.00019999999494757503\n",
      "Step: 11270, train/grad_norm: 0.08158977329730988\n",
      "Step: 11270, train/learning_rate: 3.658971763798036e-05\n",
      "Step: 11270, train/epoch: 2.682056188583374\n",
      "Step: 11280, train/loss: 0.0006000000284984708\n",
      "Step: 11280, train/grad_norm: 0.03238007053732872\n",
      "Step: 11280, train/learning_rate: 3.657782144728117e-05\n",
      "Step: 11280, train/epoch: 2.684436082839966\n",
      "Step: 11290, train/loss: 9.999999747378752e-05\n",
      "Step: 11290, train/grad_norm: 0.33833378553390503\n",
      "Step: 11290, train/learning_rate: 3.656592161860317e-05\n",
      "Step: 11290, train/epoch: 2.6868157386779785\n",
      "Step: 11300, train/loss: 0.0\n",
      "Step: 11300, train/grad_norm: 0.0006568071548826993\n",
      "Step: 11300, train/learning_rate: 3.655402178992517e-05\n",
      "Step: 11300, train/epoch: 2.6891956329345703\n",
      "Step: 11310, train/loss: 0.0\n",
      "Step: 11310, train/grad_norm: 0.001988770207390189\n",
      "Step: 11310, train/learning_rate: 3.6542121961247176e-05\n",
      "Step: 11310, train/epoch: 2.691575527191162\n",
      "Step: 11320, train/loss: 0.0\n",
      "Step: 11320, train/grad_norm: 3.335823566885665e-05\n",
      "Step: 11320, train/learning_rate: 3.653022213256918e-05\n",
      "Step: 11320, train/epoch: 2.693955183029175\n",
      "Step: 11330, train/loss: 0.0\n",
      "Step: 11330, train/grad_norm: 0.00013829325325787067\n",
      "Step: 11330, train/learning_rate: 3.651832594186999e-05\n",
      "Step: 11330, train/epoch: 2.6963350772857666\n",
      "Step: 11340, train/loss: 0.11559999734163284\n",
      "Step: 11340, train/grad_norm: 3.184520755894482e-05\n",
      "Step: 11340, train/learning_rate: 3.650642611319199e-05\n",
      "Step: 11340, train/epoch: 2.6987149715423584\n",
      "Step: 11350, train/loss: 0.0\n",
      "Step: 11350, train/grad_norm: 5.394877734943293e-05\n",
      "Step: 11350, train/learning_rate: 3.6494526284513995e-05\n",
      "Step: 11350, train/epoch: 2.701094627380371\n",
      "Step: 11360, train/loss: 0.0\n",
      "Step: 11360, train/grad_norm: 9.454676910536364e-05\n",
      "Step: 11360, train/learning_rate: 3.6482626455836e-05\n",
      "Step: 11360, train/epoch: 2.703474521636963\n",
      "Step: 11370, train/loss: 0.12110000103712082\n",
      "Step: 11370, train/grad_norm: 0.0003682238457258791\n",
      "Step: 11370, train/learning_rate: 3.6470726627158e-05\n",
      "Step: 11370, train/epoch: 2.7058544158935547\n",
      "Step: 11380, train/loss: 0.0\n",
      "Step: 11380, train/grad_norm: 0.001221490791067481\n",
      "Step: 11380, train/learning_rate: 3.645883043645881e-05\n",
      "Step: 11380, train/epoch: 2.7082340717315674\n",
      "Step: 11390, train/loss: 0.0\n",
      "Step: 11390, train/grad_norm: 0.0061569539830088615\n",
      "Step: 11390, train/learning_rate: 3.6446930607780814e-05\n",
      "Step: 11390, train/epoch: 2.710613965988159\n",
      "Step: 11400, train/loss: 0.13830000162124634\n",
      "Step: 11400, train/grad_norm: 0.017478127032518387\n",
      "Step: 11400, train/learning_rate: 3.643503077910282e-05\n",
      "Step: 11400, train/epoch: 2.712993860244751\n",
      "Step: 11410, train/loss: 0.09889999777078629\n",
      "Step: 11410, train/grad_norm: 75.5351333618164\n",
      "Step: 11410, train/learning_rate: 3.642313095042482e-05\n",
      "Step: 11410, train/epoch: 2.7153735160827637\n",
      "Step: 11420, train/loss: 0.06889999657869339\n",
      "Step: 11420, train/grad_norm: 0.6237711310386658\n",
      "Step: 11420, train/learning_rate: 3.641123112174682e-05\n",
      "Step: 11420, train/epoch: 2.7177534103393555\n",
      "Step: 11430, train/loss: 9.999999747378752e-05\n",
      "Step: 11430, train/grad_norm: 0.0003248567518312484\n",
      "Step: 11430, train/learning_rate: 3.639933493104763e-05\n",
      "Step: 11430, train/epoch: 2.7201333045959473\n",
      "Step: 11440, train/loss: 0.0\n",
      "Step: 11440, train/grad_norm: 5.1877864279958885e-06\n",
      "Step: 11440, train/learning_rate: 3.6387435102369636e-05\n",
      "Step: 11440, train/epoch: 2.722513198852539\n",
      "Step: 11450, train/loss: 0.0\n",
      "Step: 11450, train/grad_norm: 3.0533058179571526e-06\n",
      "Step: 11450, train/learning_rate: 3.637553527369164e-05\n",
      "Step: 11450, train/epoch: 2.7248928546905518\n",
      "Step: 11460, train/loss: 0.0\n",
      "Step: 11460, train/grad_norm: 4.101403465028852e-05\n",
      "Step: 11460, train/learning_rate: 3.636363544501364e-05\n",
      "Step: 11460, train/epoch: 2.7272727489471436\n",
      "Step: 11470, train/loss: 0.041200000792741776\n",
      "Step: 11470, train/grad_norm: 2.06075837922981e-05\n",
      "Step: 11470, train/learning_rate: 3.6351735616335645e-05\n",
      "Step: 11470, train/epoch: 2.7296526432037354\n",
      "Step: 11480, train/loss: 0.05040000006556511\n",
      "Step: 11480, train/grad_norm: 484.28033447265625\n",
      "Step: 11480, train/learning_rate: 3.6339839425636455e-05\n",
      "Step: 11480, train/epoch: 2.732032299041748\n",
      "Step: 11490, train/loss: 0.0\n",
      "Step: 11490, train/grad_norm: 2.237425542261917e-05\n",
      "Step: 11490, train/learning_rate: 3.632793959695846e-05\n",
      "Step: 11490, train/epoch: 2.73441219329834\n",
      "Step: 11500, train/loss: 0.0\n",
      "Step: 11500, train/grad_norm: 6.32282399237738e-06\n",
      "Step: 11500, train/learning_rate: 3.631603976828046e-05\n",
      "Step: 11500, train/epoch: 2.7367920875549316\n",
      "Step: 11510, train/loss: 0.0\n",
      "Step: 11510, train/grad_norm: 8.129264460876584e-05\n",
      "Step: 11510, train/learning_rate: 3.6304139939602464e-05\n",
      "Step: 11510, train/epoch: 2.7391717433929443\n",
      "Step: 11520, train/loss: 0.19380000233650208\n",
      "Step: 11520, train/grad_norm: 0.0010712732328101993\n",
      "Step: 11520, train/learning_rate: 3.629224011092447e-05\n",
      "Step: 11520, train/epoch: 2.741551637649536\n",
      "Step: 11530, train/loss: 0.05590000003576279\n",
      "Step: 11530, train/grad_norm: 0.04927143454551697\n",
      "Step: 11530, train/learning_rate: 3.628034392022528e-05\n",
      "Step: 11530, train/epoch: 2.743931531906128\n",
      "Step: 11540, train/loss: 0.08959999680519104\n",
      "Step: 11540, train/grad_norm: 0.08227765560150146\n",
      "Step: 11540, train/learning_rate: 3.626844409154728e-05\n",
      "Step: 11540, train/epoch: 2.7463111877441406\n",
      "Step: 11550, train/loss: 0.03779999911785126\n",
      "Step: 11550, train/grad_norm: 0.035135433077812195\n",
      "Step: 11550, train/learning_rate: 3.6256544262869284e-05\n",
      "Step: 11550, train/epoch: 2.7486910820007324\n",
      "Step: 11560, train/loss: 0.08990000188350677\n",
      "Step: 11560, train/grad_norm: 0.03968367725610733\n",
      "Step: 11560, train/learning_rate: 3.6244644434191287e-05\n",
      "Step: 11560, train/epoch: 2.751070976257324\n",
      "Step: 11570, train/loss: 9.999999747378752e-05\n",
      "Step: 11570, train/grad_norm: 0.008958698250353336\n",
      "Step: 11570, train/learning_rate: 3.623274460551329e-05\n",
      "Step: 11570, train/epoch: 2.753450632095337\n",
      "Step: 11580, train/loss: 0.0\n",
      "Step: 11580, train/grad_norm: 0.0012408077018335462\n",
      "Step: 11580, train/learning_rate: 3.62208484148141e-05\n",
      "Step: 11580, train/epoch: 2.7558305263519287\n",
      "Step: 11590, train/loss: 0.021800000220537186\n",
      "Step: 11590, train/grad_norm: 0.0006673422758467495\n",
      "Step: 11590, train/learning_rate: 3.62089485861361e-05\n",
      "Step: 11590, train/epoch: 2.7582104206085205\n",
      "Step: 11600, train/loss: 0.1890999972820282\n",
      "Step: 11600, train/grad_norm: 0.006082544103264809\n",
      "Step: 11600, train/learning_rate: 3.6197048757458106e-05\n",
      "Step: 11600, train/epoch: 2.760590076446533\n",
      "Step: 11610, train/loss: 0.0\n",
      "Step: 11610, train/grad_norm: 0.009274162352085114\n",
      "Step: 11610, train/learning_rate: 3.618514892878011e-05\n",
      "Step: 11610, train/epoch: 2.762969970703125\n",
      "Step: 11620, train/loss: 0.18520000576972961\n",
      "Step: 11620, train/grad_norm: 0.017438240349292755\n",
      "Step: 11620, train/learning_rate: 3.617324910010211e-05\n",
      "Step: 11620, train/epoch: 2.765349864959717\n",
      "Step: 11630, train/loss: 0.00019999999494757503\n",
      "Step: 11630, train/grad_norm: 0.19824321568012238\n",
      "Step: 11630, train/learning_rate: 3.616135290940292e-05\n",
      "Step: 11630, train/epoch: 2.7677297592163086\n",
      "Step: 11640, train/loss: 9.999999747378752e-05\n",
      "Step: 11640, train/grad_norm: 0.00947300624102354\n",
      "Step: 11640, train/learning_rate: 3.6149453080724925e-05\n",
      "Step: 11640, train/epoch: 2.7701094150543213\n",
      "Step: 11650, train/loss: 0.0\n",
      "Step: 11650, train/grad_norm: 0.003969524521380663\n",
      "Step: 11650, train/learning_rate: 3.613755325204693e-05\n",
      "Step: 11650, train/epoch: 2.772489309310913\n",
      "Step: 11660, train/loss: 0.0\n",
      "Step: 11660, train/grad_norm: 0.0020610580686479807\n",
      "Step: 11660, train/learning_rate: 3.612565342336893e-05\n",
      "Step: 11660, train/epoch: 2.774869203567505\n",
      "Step: 11670, train/loss: 0.12189999967813492\n",
      "Step: 11670, train/grad_norm: 0.003203459782525897\n",
      "Step: 11670, train/learning_rate: 3.6113753594690934e-05\n",
      "Step: 11670, train/epoch: 2.7772488594055176\n",
      "Step: 11680, train/loss: 0.0\n",
      "Step: 11680, train/grad_norm: 0.03327561914920807\n",
      "Step: 11680, train/learning_rate: 3.6101857403991744e-05\n",
      "Step: 11680, train/epoch: 2.7796287536621094\n",
      "Step: 11690, train/loss: 0.09780000150203705\n",
      "Step: 11690, train/grad_norm: 0.1722683310508728\n",
      "Step: 11690, train/learning_rate: 3.608995757531375e-05\n",
      "Step: 11690, train/epoch: 2.782008647918701\n",
      "Step: 11700, train/loss: 9.999999747378752e-05\n",
      "Step: 11700, train/grad_norm: 0.010090767405927181\n",
      "Step: 11700, train/learning_rate: 3.607805774663575e-05\n",
      "Step: 11700, train/epoch: 2.784388303756714\n",
      "Step: 11710, train/loss: 0.0\n",
      "Step: 11710, train/grad_norm: 0.003185235196724534\n",
      "Step: 11710, train/learning_rate: 3.606615791795775e-05\n",
      "Step: 11710, train/epoch: 2.7867681980133057\n",
      "Step: 11720, train/loss: 0.0\n",
      "Step: 11720, train/grad_norm: 0.0011741704074665904\n",
      "Step: 11720, train/learning_rate: 3.6054258089279756e-05\n",
      "Step: 11720, train/epoch: 2.7891480922698975\n",
      "Step: 11730, train/loss: 0.0\n",
      "Step: 11730, train/grad_norm: 0.004312377423048019\n",
      "Step: 11730, train/learning_rate: 3.6042361898580566e-05\n",
      "Step: 11730, train/epoch: 2.79152774810791\n",
      "Step: 11740, train/loss: 0.0\n",
      "Step: 11740, train/grad_norm: 0.0017259623855352402\n",
      "Step: 11740, train/learning_rate: 3.603046206990257e-05\n",
      "Step: 11740, train/epoch: 2.793907642364502\n",
      "Step: 11750, train/loss: 0.11959999799728394\n",
      "Step: 11750, train/grad_norm: 0.001442690147086978\n",
      "Step: 11750, train/learning_rate: 3.601856224122457e-05\n",
      "Step: 11750, train/epoch: 2.7962875366210938\n",
      "Step: 11760, train/loss: 0.02539999969303608\n",
      "Step: 11760, train/grad_norm: 159.5052490234375\n",
      "Step: 11760, train/learning_rate: 3.6006662412546575e-05\n",
      "Step: 11760, train/epoch: 2.7986671924591064\n",
      "Step: 11770, train/loss: 0.0\n",
      "Step: 11770, train/grad_norm: 0.006369554903358221\n",
      "Step: 11770, train/learning_rate: 3.599476258386858e-05\n",
      "Step: 11770, train/epoch: 2.8010470867156982\n",
      "Step: 11780, train/loss: 0.0\n",
      "Step: 11780, train/grad_norm: 0.001726016984321177\n",
      "Step: 11780, train/learning_rate: 3.598286639316939e-05\n",
      "Step: 11780, train/epoch: 2.80342698097229\n",
      "Step: 11790, train/loss: 0.0\n",
      "Step: 11790, train/grad_norm: 0.03948405757546425\n",
      "Step: 11790, train/learning_rate: 3.597096656449139e-05\n",
      "Step: 11790, train/epoch: 2.805806875228882\n",
      "Step: 11800, train/loss: 0.003800000064074993\n",
      "Step: 11800, train/grad_norm: 0.004391196183860302\n",
      "Step: 11800, train/learning_rate: 3.5959066735813394e-05\n",
      "Step: 11800, train/epoch: 2.8081865310668945\n",
      "Step: 11810, train/loss: 9.999999747378752e-05\n",
      "Step: 11810, train/grad_norm: 0.0011191029334440827\n",
      "Step: 11810, train/learning_rate: 3.59471669071354e-05\n",
      "Step: 11810, train/epoch: 2.8105664253234863\n",
      "Step: 11820, train/loss: 0.0024999999441206455\n",
      "Step: 11820, train/grad_norm: 0.0006473236717283726\n",
      "Step: 11820, train/learning_rate: 3.593527071643621e-05\n",
      "Step: 11820, train/epoch: 2.812946319580078\n",
      "Step: 11830, train/loss: 0.15569999814033508\n",
      "Step: 11830, train/grad_norm: 22.363224029541016\n",
      "Step: 11830, train/learning_rate: 3.592337088775821e-05\n",
      "Step: 11830, train/epoch: 2.815325975418091\n",
      "Step: 11840, train/loss: 0.05469999834895134\n",
      "Step: 11840, train/grad_norm: 0.026344070211052895\n",
      "Step: 11840, train/learning_rate: 3.591147105908021e-05\n",
      "Step: 11840, train/epoch: 2.8177058696746826\n",
      "Step: 11850, train/loss: 9.999999747378752e-05\n",
      "Step: 11850, train/grad_norm: 0.1070752739906311\n",
      "Step: 11850, train/learning_rate: 3.5899571230402216e-05\n",
      "Step: 11850, train/epoch: 2.8200857639312744\n",
      "Step: 11860, train/loss: 0.0\n",
      "Step: 11860, train/grad_norm: 0.016232958063483238\n",
      "Step: 11860, train/learning_rate: 3.588767140172422e-05\n",
      "Step: 11860, train/epoch: 2.822465419769287\n",
      "Step: 11870, train/loss: 0.15719999372959137\n",
      "Step: 11870, train/grad_norm: 0.002569095930084586\n",
      "Step: 11870, train/learning_rate: 3.587577521102503e-05\n",
      "Step: 11870, train/epoch: 2.824845314025879\n",
      "Step: 11880, train/loss: 0.1251000016927719\n",
      "Step: 11880, train/grad_norm: 0.09341923892498016\n",
      "Step: 11880, train/learning_rate: 3.586387538234703e-05\n",
      "Step: 11880, train/epoch: 2.8272252082824707\n",
      "Step: 11890, train/loss: 0.000699999975040555\n",
      "Step: 11890, train/grad_norm: 0.00440630130469799\n",
      "Step: 11890, train/learning_rate: 3.5851975553669035e-05\n",
      "Step: 11890, train/epoch: 2.8296048641204834\n",
      "Step: 11900, train/loss: 0.0\n",
      "Step: 11900, train/grad_norm: 0.0018543433398008347\n",
      "Step: 11900, train/learning_rate: 3.584007572499104e-05\n",
      "Step: 11900, train/epoch: 2.831984758377075\n",
      "Step: 11910, train/loss: 0.0\n",
      "Step: 11910, train/grad_norm: 0.0018365244613960385\n",
      "Step: 11910, train/learning_rate: 3.582817589631304e-05\n",
      "Step: 11910, train/epoch: 2.834364652633667\n",
      "Step: 11920, train/loss: 0.0\n",
      "Step: 11920, train/grad_norm: 0.0014976352686062455\n",
      "Step: 11920, train/learning_rate: 3.581627970561385e-05\n",
      "Step: 11920, train/epoch: 2.8367443084716797\n",
      "Step: 11930, train/loss: 0.0\n",
      "Step: 11930, train/grad_norm: 0.0009183551883324981\n",
      "Step: 11930, train/learning_rate: 3.5804379876935855e-05\n",
      "Step: 11930, train/epoch: 2.8391242027282715\n",
      "Step: 11940, train/loss: 0.15629999339580536\n",
      "Step: 11940, train/grad_norm: 0.007124021649360657\n",
      "Step: 11940, train/learning_rate: 3.579248004825786e-05\n",
      "Step: 11940, train/epoch: 2.8415040969848633\n",
      "Step: 11950, train/loss: 0.0\n",
      "Step: 11950, train/grad_norm: 0.007963331416249275\n",
      "Step: 11950, train/learning_rate: 3.578058021957986e-05\n",
      "Step: 11950, train/epoch: 2.843883752822876\n",
      "Step: 11960, train/loss: 0.0\n",
      "Step: 11960, train/grad_norm: 0.006057588383555412\n",
      "Step: 11960, train/learning_rate: 3.5768680390901864e-05\n",
      "Step: 11960, train/epoch: 2.8462636470794678\n",
      "Step: 11970, train/loss: 0.0\n",
      "Step: 11970, train/grad_norm: 0.014177929610013962\n",
      "Step: 11970, train/learning_rate: 3.5756784200202674e-05\n",
      "Step: 11970, train/epoch: 2.8486435413360596\n",
      "Step: 11980, train/loss: 0.0\n",
      "Step: 11980, train/grad_norm: 0.007989294826984406\n",
      "Step: 11980, train/learning_rate: 3.574488437152468e-05\n",
      "Step: 11980, train/epoch: 2.8510234355926514\n",
      "Step: 11990, train/loss: 0.0\n",
      "Step: 11990, train/grad_norm: 0.0019160923548042774\n",
      "Step: 11990, train/learning_rate: 3.573298454284668e-05\n",
      "Step: 11990, train/epoch: 2.853403091430664\n",
      "Step: 12000, train/loss: 0.1468999981880188\n",
      "Step: 12000, train/grad_norm: 0.0030401507392525673\n",
      "Step: 12000, train/learning_rate: 3.572108471416868e-05\n",
      "Step: 12000, train/epoch: 2.855782985687256\n",
      "Step: 12010, train/loss: 0.04050000011920929\n",
      "Step: 12010, train/grad_norm: 0.027491481974720955\n",
      "Step: 12010, train/learning_rate: 3.5709184885490686e-05\n",
      "Step: 12010, train/epoch: 2.8581628799438477\n",
      "Step: 12020, train/loss: 9.999999747378752e-05\n",
      "Step: 12020, train/grad_norm: 0.01224584225565195\n",
      "Step: 12020, train/learning_rate: 3.5697288694791496e-05\n",
      "Step: 12020, train/epoch: 2.8605425357818604\n",
      "Step: 12030, train/loss: 0.0\n",
      "Step: 12030, train/grad_norm: 0.004071404226124287\n",
      "Step: 12030, train/learning_rate: 3.56853888661135e-05\n",
      "Step: 12030, train/epoch: 2.862922430038452\n",
      "Step: 12040, train/loss: 0.0\n",
      "Step: 12040, train/grad_norm: 0.00802321545779705\n",
      "Step: 12040, train/learning_rate: 3.56734890374355e-05\n",
      "Step: 12040, train/epoch: 2.865302324295044\n",
      "Step: 12050, train/loss: 0.0\n",
      "Step: 12050, train/grad_norm: 0.004961748141795397\n",
      "Step: 12050, train/learning_rate: 3.5661589208757505e-05\n",
      "Step: 12050, train/epoch: 2.8676819801330566\n",
      "Step: 12060, train/loss: 0.0\n",
      "Step: 12060, train/grad_norm: 0.003628932172432542\n",
      "Step: 12060, train/learning_rate: 3.564968938007951e-05\n",
      "Step: 12060, train/epoch: 2.8700618743896484\n",
      "Step: 12070, train/loss: 0.0\n",
      "Step: 12070, train/grad_norm: 0.0023393318988382816\n",
      "Step: 12070, train/learning_rate: 3.563779318938032e-05\n",
      "Step: 12070, train/epoch: 2.8724417686462402\n",
      "Step: 12080, train/loss: 0.07970000058412552\n",
      "Step: 12080, train/grad_norm: 0.011556542478501797\n",
      "Step: 12080, train/learning_rate: 3.562589336070232e-05\n",
      "Step: 12080, train/epoch: 2.874821424484253\n",
      "Step: 12090, train/loss: 0.0\n",
      "Step: 12090, train/grad_norm: 0.051560498774051666\n",
      "Step: 12090, train/learning_rate: 3.5613993532024324e-05\n",
      "Step: 12090, train/epoch: 2.8772013187408447\n",
      "Step: 12100, train/loss: 0.0\n",
      "Step: 12100, train/grad_norm: 0.006513245403766632\n",
      "Step: 12100, train/learning_rate: 3.560209370334633e-05\n",
      "Step: 12100, train/epoch: 2.8795812129974365\n",
      "Step: 12110, train/loss: 0.0\n",
      "Step: 12110, train/grad_norm: 0.0032948371954262257\n",
      "Step: 12110, train/learning_rate: 3.559019387466833e-05\n",
      "Step: 12110, train/epoch: 2.881960868835449\n",
      "Step: 12120, train/loss: 0.0\n",
      "Step: 12120, train/grad_norm: 0.0019245351431891322\n",
      "Step: 12120, train/learning_rate: 3.557829768396914e-05\n",
      "Step: 12120, train/epoch: 2.884340763092041\n",
      "Step: 12130, train/loss: 0.0\n",
      "Step: 12130, train/grad_norm: 0.0034031271934509277\n",
      "Step: 12130, train/learning_rate: 3.556639785529114e-05\n",
      "Step: 12130, train/epoch: 2.886720657348633\n",
      "Step: 12140, train/loss: 0.0\n",
      "Step: 12140, train/grad_norm: 0.000689295120537281\n",
      "Step: 12140, train/learning_rate: 3.5554498026613146e-05\n",
      "Step: 12140, train/epoch: 2.8891003131866455\n",
      "Step: 12150, train/loss: 0.0\n",
      "Step: 12150, train/grad_norm: 0.0013693178771063685\n",
      "Step: 12150, train/learning_rate: 3.554259819793515e-05\n",
      "Step: 12150, train/epoch: 2.8914802074432373\n",
      "Step: 12160, train/loss: 0.0\n",
      "Step: 12160, train/grad_norm: 0.0009594251750968397\n",
      "Step: 12160, train/learning_rate: 3.553069836925715e-05\n",
      "Step: 12160, train/epoch: 2.893860101699829\n",
      "Step: 12170, train/loss: 0.06289999932050705\n",
      "Step: 12170, train/grad_norm: 0.001774117350578308\n",
      "Step: 12170, train/learning_rate: 3.551880217855796e-05\n",
      "Step: 12170, train/epoch: 2.896239995956421\n",
      "Step: 12180, train/loss: 0.1242000013589859\n",
      "Step: 12180, train/grad_norm: 0.09421072155237198\n",
      "Step: 12180, train/learning_rate: 3.5506902349879965e-05\n",
      "Step: 12180, train/epoch: 2.8986196517944336\n",
      "Step: 12190, train/loss: 0.08479999750852585\n",
      "Step: 12190, train/grad_norm: 0.03019048646092415\n",
      "Step: 12190, train/learning_rate: 3.549500252120197e-05\n",
      "Step: 12190, train/epoch: 2.9009995460510254\n",
      "Step: 12200, train/loss: 0.0\n",
      "Step: 12200, train/grad_norm: 0.0013271353673189878\n",
      "Step: 12200, train/learning_rate: 3.548310269252397e-05\n",
      "Step: 12200, train/epoch: 2.903379440307617\n",
      "Step: 12210, train/loss: 0.0\n",
      "Step: 12210, train/grad_norm: 0.0010406880173832178\n",
      "Step: 12210, train/learning_rate: 3.5471202863845974e-05\n",
      "Step: 12210, train/epoch: 2.90575909614563\n",
      "Step: 12220, train/loss: 0.2867000102996826\n",
      "Step: 12220, train/grad_norm: 0.035353757441043854\n",
      "Step: 12220, train/learning_rate: 3.5459306673146784e-05\n",
      "Step: 12220, train/epoch: 2.9081389904022217\n",
      "Step: 12230, train/loss: 0.0003000000142492354\n",
      "Step: 12230, train/grad_norm: 0.19563360512256622\n",
      "Step: 12230, train/learning_rate: 3.544740684446879e-05\n",
      "Step: 12230, train/epoch: 2.9105188846588135\n",
      "Step: 12240, train/loss: 9.999999747378752e-05\n",
      "Step: 12240, train/grad_norm: 0.005863668397068977\n",
      "Step: 12240, train/learning_rate: 3.543550701579079e-05\n",
      "Step: 12240, train/epoch: 2.912898540496826\n",
      "Step: 12250, train/loss: 0.0\n",
      "Step: 12250, train/grad_norm: 0.002042466076090932\n",
      "Step: 12250, train/learning_rate: 3.542360718711279e-05\n",
      "Step: 12250, train/epoch: 2.915278434753418\n",
      "Step: 12260, train/loss: 0.0\n",
      "Step: 12260, train/grad_norm: 0.0009145625517703593\n",
      "Step: 12260, train/learning_rate: 3.5411707358434796e-05\n",
      "Step: 12260, train/epoch: 2.9176583290100098\n",
      "Step: 12270, train/loss: 0.0\n",
      "Step: 12270, train/grad_norm: 0.0013342456659302115\n",
      "Step: 12270, train/learning_rate: 3.5399811167735606e-05\n",
      "Step: 12270, train/epoch: 2.9200379848480225\n",
      "Step: 12280, train/loss: 0.12269999831914902\n",
      "Step: 12280, train/grad_norm: 80.24565887451172\n",
      "Step: 12280, train/learning_rate: 3.538791133905761e-05\n",
      "Step: 12280, train/epoch: 2.9224178791046143\n",
      "Step: 12290, train/loss: 0.0\n",
      "Step: 12290, train/grad_norm: 0.008301780559122562\n",
      "Step: 12290, train/learning_rate: 3.537601151037961e-05\n",
      "Step: 12290, train/epoch: 2.924797773361206\n",
      "Step: 12300, train/loss: 0.0\n",
      "Step: 12300, train/grad_norm: 0.010521619580686092\n",
      "Step: 12300, train/learning_rate: 3.5364111681701615e-05\n",
      "Step: 12300, train/epoch: 2.9271774291992188\n",
      "Step: 12310, train/loss: 0.0\n",
      "Step: 12310, train/grad_norm: 0.02250104397535324\n",
      "Step: 12310, train/learning_rate: 3.535221185302362e-05\n",
      "Step: 12310, train/epoch: 2.9295573234558105\n",
      "Step: 12320, train/loss: 0.0\n",
      "Step: 12320, train/grad_norm: 0.01667756587266922\n",
      "Step: 12320, train/learning_rate: 3.534031566232443e-05\n",
      "Step: 12320, train/epoch: 2.9319372177124023\n",
      "Step: 12330, train/loss: 9.999999747378752e-05\n",
      "Step: 12330, train/grad_norm: 0.0031403207685798407\n",
      "Step: 12330, train/learning_rate: 3.532841583364643e-05\n",
      "Step: 12330, train/epoch: 2.934316873550415\n",
      "Step: 12340, train/loss: 0.13279999792575836\n",
      "Step: 12340, train/grad_norm: 0.003772897645831108\n",
      "Step: 12340, train/learning_rate: 3.5316516004968435e-05\n",
      "Step: 12340, train/epoch: 2.936696767807007\n",
      "Step: 12350, train/loss: 0.0\n",
      "Step: 12350, train/grad_norm: 0.004608200863003731\n",
      "Step: 12350, train/learning_rate: 3.530461617629044e-05\n",
      "Step: 12350, train/epoch: 2.9390766620635986\n",
      "Step: 12360, train/loss: 0.0\n",
      "Step: 12360, train/grad_norm: 0.0068754032254219055\n",
      "Step: 12360, train/learning_rate: 3.529271634761244e-05\n",
      "Step: 12360, train/epoch: 2.9414565563201904\n",
      "Step: 12370, train/loss: 0.0\n",
      "Step: 12370, train/grad_norm: 0.006015855353325605\n",
      "Step: 12370, train/learning_rate: 3.528082015691325e-05\n",
      "Step: 12370, train/epoch: 2.943836212158203\n",
      "Step: 12380, train/loss: 0.0\n",
      "Step: 12380, train/grad_norm: 0.0018751999596133828\n",
      "Step: 12380, train/learning_rate: 3.5268920328235254e-05\n",
      "Step: 12380, train/epoch: 2.946216106414795\n",
      "Step: 12390, train/loss: 0.0\n",
      "Step: 12390, train/grad_norm: 0.0030980464071035385\n",
      "Step: 12390, train/learning_rate: 3.525702049955726e-05\n",
      "Step: 12390, train/epoch: 2.9485960006713867\n",
      "Step: 12400, train/loss: 0.0\n",
      "Step: 12400, train/grad_norm: 0.003923511598259211\n",
      "Step: 12400, train/learning_rate: 3.524512067087926e-05\n",
      "Step: 12400, train/epoch: 2.9509756565093994\n",
      "Step: 12410, train/loss: 0.0\n",
      "Step: 12410, train/grad_norm: 0.0021075215190649033\n",
      "Step: 12410, train/learning_rate: 3.523322084220126e-05\n",
      "Step: 12410, train/epoch: 2.953355550765991\n",
      "Step: 12420, train/loss: 0.0\n",
      "Step: 12420, train/grad_norm: 0.0020290305837988853\n",
      "Step: 12420, train/learning_rate: 3.522132465150207e-05\n",
      "Step: 12420, train/epoch: 2.955735445022583\n",
      "Step: 12430, train/loss: 0.0\n",
      "Step: 12430, train/grad_norm: 0.002869212767109275\n",
      "Step: 12430, train/learning_rate: 3.5209424822824076e-05\n",
      "Step: 12430, train/epoch: 2.9581151008605957\n",
      "Step: 12440, train/loss: 0.052000001072883606\n",
      "Step: 12440, train/grad_norm: 0.0006213366286829114\n",
      "Step: 12440, train/learning_rate: 3.519752499414608e-05\n",
      "Step: 12440, train/epoch: 2.9604949951171875\n",
      "Step: 12450, train/loss: 0.0\n",
      "Step: 12450, train/grad_norm: 0.004731562454253435\n",
      "Step: 12450, train/learning_rate: 3.518562516546808e-05\n",
      "Step: 12450, train/epoch: 2.9628748893737793\n",
      "Step: 12460, train/loss: 0.0\n",
      "Step: 12460, train/grad_norm: 0.0017442250391468406\n",
      "Step: 12460, train/learning_rate: 3.5173725336790085e-05\n",
      "Step: 12460, train/epoch: 2.965254545211792\n",
      "Step: 12470, train/loss: 0.0\n",
      "Step: 12470, train/grad_norm: 0.0015987628139555454\n",
      "Step: 12470, train/learning_rate: 3.5161829146090895e-05\n",
      "Step: 12470, train/epoch: 2.967634439468384\n",
      "Step: 12480, train/loss: 0.0\n",
      "Step: 12480, train/grad_norm: 0.09995851665735245\n",
      "Step: 12480, train/learning_rate: 3.51499293174129e-05\n",
      "Step: 12480, train/epoch: 2.9700143337249756\n",
      "Step: 12490, train/loss: 0.12189999967813492\n",
      "Step: 12490, train/grad_norm: 0.0005277263117022812\n",
      "Step: 12490, train/learning_rate: 3.51380294887349e-05\n",
      "Step: 12490, train/epoch: 2.9723939895629883\n",
      "Step: 12500, train/loss: 0.0\n",
      "Step: 12500, train/grad_norm: 0.001790188835002482\n",
      "Step: 12500, train/learning_rate: 3.5126129660056904e-05\n",
      "Step: 12500, train/epoch: 2.97477388381958\n",
      "Step: 12510, train/loss: 9.999999747378752e-05\n",
      "Step: 12510, train/grad_norm: 0.0023436234332621098\n",
      "Step: 12510, train/learning_rate: 3.511422983137891e-05\n",
      "Step: 12510, train/epoch: 2.977153778076172\n",
      "Step: 12520, train/loss: 0.0\n",
      "Step: 12520, train/grad_norm: 0.0015592509880661964\n",
      "Step: 12520, train/learning_rate: 3.510233364067972e-05\n",
      "Step: 12520, train/epoch: 2.9795336723327637\n",
      "Step: 12530, train/loss: 0.0\n",
      "Step: 12530, train/grad_norm: 0.001598311006091535\n",
      "Step: 12530, train/learning_rate: 3.509043381200172e-05\n",
      "Step: 12530, train/epoch: 2.9819133281707764\n",
      "Step: 12540, train/loss: 0.0\n",
      "Step: 12540, train/grad_norm: 0.0016628586454316974\n",
      "Step: 12540, train/learning_rate: 3.507853398332372e-05\n",
      "Step: 12540, train/epoch: 2.984293222427368\n",
      "Step: 12550, train/loss: 0.0\n",
      "Step: 12550, train/grad_norm: 0.0007540013757534325\n",
      "Step: 12550, train/learning_rate: 3.5066634154645726e-05\n",
      "Step: 12550, train/epoch: 2.98667311668396\n",
      "Step: 12560, train/loss: 0.0\n",
      "Step: 12560, train/grad_norm: 0.0007655865047127008\n",
      "Step: 12560, train/learning_rate: 3.505473432596773e-05\n",
      "Step: 12560, train/epoch: 2.9890527725219727\n",
      "Step: 12570, train/loss: 0.0\n",
      "Step: 12570, train/grad_norm: 0.001590487314388156\n",
      "Step: 12570, train/learning_rate: 3.504283813526854e-05\n",
      "Step: 12570, train/epoch: 2.9914326667785645\n",
      "Step: 12580, train/loss: 0.0\n",
      "Step: 12580, train/grad_norm: 0.0020346541423350573\n",
      "Step: 12580, train/learning_rate: 3.503093830659054e-05\n",
      "Step: 12580, train/epoch: 2.9938125610351562\n",
      "Step: 12590, train/loss: 0.1648000031709671\n",
      "Step: 12590, train/grad_norm: 0.0016451503615826368\n",
      "Step: 12590, train/learning_rate: 3.5019038477912545e-05\n",
      "Step: 12590, train/epoch: 2.996192216873169\n",
      "Step: 12600, train/loss: 0.0\n",
      "Step: 12600, train/grad_norm: 0.010630076751112938\n",
      "Step: 12600, train/learning_rate: 3.500713864923455e-05\n",
      "Step: 12600, train/epoch: 2.9985721111297607\n",
      "Step: 12606, eval/loss: 0.019650734961032867\n",
      "Step: 12606, eval/accuracy: 0.9966680407524109\n",
      "Step: 12606, eval/f1: 0.9964809417724609\n",
      "Step: 12606, eval/runtime: 857.1392211914062\n",
      "Step: 12606, eval/samples_per_second: 8.404000282287598\n",
      "Step: 12606, eval/steps_per_second: 1.0509999990463257\n",
      "Step: 12606, train/epoch: 3.0\n",
      "Step: 12610, train/loss: 9.999999747378752e-05\n",
      "Step: 12610, train/grad_norm: 0.025187183171510696\n",
      "Step: 12610, train/learning_rate: 3.499523882055655e-05\n",
      "Step: 12610, train/epoch: 3.0009520053863525\n",
      "Step: 12620, train/loss: 0.0\n",
      "Step: 12620, train/grad_norm: 0.0036113245878368616\n",
      "Step: 12620, train/learning_rate: 3.498334262985736e-05\n",
      "Step: 12620, train/epoch: 3.0033316612243652\n",
      "Step: 12630, train/loss: 0.0\n",
      "Step: 12630, train/grad_norm: 0.006779801100492477\n",
      "Step: 12630, train/learning_rate: 3.4971442801179364e-05\n",
      "Step: 12630, train/epoch: 3.005711555480957\n",
      "Step: 12640, train/loss: 0.0649000033736229\n",
      "Step: 12640, train/grad_norm: 0.00715935043990612\n",
      "Step: 12640, train/learning_rate: 3.495954297250137e-05\n",
      "Step: 12640, train/epoch: 3.008091449737549\n",
      "Step: 12650, train/loss: 9.999999747378752e-05\n",
      "Step: 12650, train/grad_norm: 0.022027969360351562\n",
      "Step: 12650, train/learning_rate: 3.494764314382337e-05\n",
      "Step: 12650, train/epoch: 3.0104711055755615\n",
      "Step: 12660, train/loss: 0.0\n",
      "Step: 12660, train/grad_norm: 0.001561282086186111\n",
      "Step: 12660, train/learning_rate: 3.493574331514537e-05\n",
      "Step: 12660, train/epoch: 3.0128509998321533\n",
      "Step: 12670, train/loss: 9.999999747378752e-05\n",
      "Step: 12670, train/grad_norm: 0.0011653788387775421\n",
      "Step: 12670, train/learning_rate: 3.4923847124446183e-05\n",
      "Step: 12670, train/epoch: 3.015230894088745\n",
      "Step: 12680, train/loss: 0.0\n",
      "Step: 12680, train/grad_norm: 0.0008800270152278244\n",
      "Step: 12680, train/learning_rate: 3.4911947295768186e-05\n",
      "Step: 12680, train/epoch: 3.017610549926758\n",
      "Step: 12690, train/loss: 0.0\n",
      "Step: 12690, train/grad_norm: 0.00027892194339074194\n",
      "Step: 12690, train/learning_rate: 3.490004746709019e-05\n",
      "Step: 12690, train/epoch: 3.0199904441833496\n",
      "Step: 12700, train/loss: 0.1046999990940094\n",
      "Step: 12700, train/grad_norm: 0.0008007352007552981\n",
      "Step: 12700, train/learning_rate: 3.488814763841219e-05\n",
      "Step: 12700, train/epoch: 3.0223703384399414\n",
      "Step: 12710, train/loss: 0.0005000000237487257\n",
      "Step: 12710, train/grad_norm: 0.0019769768696278334\n",
      "Step: 12710, train/learning_rate: 3.4876247809734195e-05\n",
      "Step: 12710, train/epoch: 3.024750232696533\n",
      "Step: 12720, train/loss: 0.10779999941587448\n",
      "Step: 12720, train/grad_norm: 0.005347078666090965\n",
      "Step: 12720, train/learning_rate: 3.4864351619035006e-05\n",
      "Step: 12720, train/epoch: 3.027129888534546\n",
      "Step: 12730, train/loss: 0.030700000002980232\n",
      "Step: 12730, train/grad_norm: 0.011071543209254742\n",
      "Step: 12730, train/learning_rate: 3.485245179035701e-05\n",
      "Step: 12730, train/epoch: 3.0295097827911377\n",
      "Step: 12740, train/loss: 0.0\n",
      "Step: 12740, train/grad_norm: 0.012873123399913311\n",
      "Step: 12740, train/learning_rate: 3.484055196167901e-05\n",
      "Step: 12740, train/epoch: 3.0318896770477295\n",
      "Step: 12750, train/loss: 0.0\n",
      "Step: 12750, train/grad_norm: 0.0033347727730870247\n",
      "Step: 12750, train/learning_rate: 3.4828652133001015e-05\n",
      "Step: 12750, train/epoch: 3.034269332885742\n",
      "Step: 12760, train/loss: 0.0\n",
      "Step: 12760, train/grad_norm: 0.0016368012875318527\n",
      "Step: 12760, train/learning_rate: 3.481675230432302e-05\n",
      "Step: 12760, train/epoch: 3.036649227142334\n",
      "Step: 12770, train/loss: 0.0\n",
      "Step: 12770, train/grad_norm: 0.0013455533189699054\n",
      "Step: 12770, train/learning_rate: 3.480485611362383e-05\n",
      "Step: 12770, train/epoch: 3.039029121398926\n",
      "Step: 12780, train/loss: 0.0\n",
      "Step: 12780, train/grad_norm: 0.0042481739073991776\n",
      "Step: 12780, train/learning_rate: 3.479295628494583e-05\n",
      "Step: 12780, train/epoch: 3.0414087772369385\n",
      "Step: 12790, train/loss: 0.0\n",
      "Step: 12790, train/grad_norm: 0.0009854983072727919\n",
      "Step: 12790, train/learning_rate: 3.4781056456267834e-05\n",
      "Step: 12790, train/epoch: 3.0437886714935303\n",
      "Step: 12800, train/loss: 0.04490000009536743\n",
      "Step: 12800, train/grad_norm: 0.001632276689633727\n",
      "Step: 12800, train/learning_rate: 3.476915662758984e-05\n",
      "Step: 12800, train/epoch: 3.046168565750122\n",
      "Step: 12810, train/loss: 0.008700000122189522\n",
      "Step: 12810, train/grad_norm: 0.0009945542551577091\n",
      "Step: 12810, train/learning_rate: 3.475725679891184e-05\n",
      "Step: 12810, train/epoch: 3.0485482215881348\n",
      "Step: 12820, train/loss: 0.0\n",
      "Step: 12820, train/grad_norm: 0.0012611720012500882\n",
      "Step: 12820, train/learning_rate: 3.474536060821265e-05\n",
      "Step: 12820, train/epoch: 3.0509281158447266\n",
      "Step: 12830, train/loss: 0.0\n",
      "Step: 12830, train/grad_norm: 0.001899134716950357\n",
      "Step: 12830, train/learning_rate: 3.473346077953465e-05\n",
      "Step: 12830, train/epoch: 3.0533080101013184\n",
      "Step: 12840, train/loss: 0.0\n",
      "Step: 12840, train/grad_norm: 0.0019441315671429038\n",
      "Step: 12840, train/learning_rate: 3.4721560950856656e-05\n",
      "Step: 12840, train/epoch: 3.055687665939331\n",
      "Step: 12850, train/loss: 0.0\n",
      "Step: 12850, train/grad_norm: 0.0013112822780385613\n",
      "Step: 12850, train/learning_rate: 3.470966112217866e-05\n",
      "Step: 12850, train/epoch: 3.058067560195923\n",
      "Step: 12860, train/loss: 0.0\n",
      "Step: 12860, train/grad_norm: 0.0005074770306237042\n",
      "Step: 12860, train/learning_rate: 3.469776129350066e-05\n",
      "Step: 12860, train/epoch: 3.0604474544525146\n",
      "Step: 12870, train/loss: 0.0\n",
      "Step: 12870, train/grad_norm: 0.0022408042568713427\n",
      "Step: 12870, train/learning_rate: 3.468586510280147e-05\n",
      "Step: 12870, train/epoch: 3.0628271102905273\n",
      "Step: 12880, train/loss: 0.0\n",
      "Step: 12880, train/grad_norm: 0.001272836932912469\n",
      "Step: 12880, train/learning_rate: 3.4673965274123475e-05\n",
      "Step: 12880, train/epoch: 3.065207004547119\n",
      "Step: 12890, train/loss: 0.0\n",
      "Step: 12890, train/grad_norm: 0.0003743221750482917\n",
      "Step: 12890, train/learning_rate: 3.466206544544548e-05\n",
      "Step: 12890, train/epoch: 3.067586898803711\n",
      "Step: 12900, train/loss: 0.0\n",
      "Step: 12900, train/grad_norm: 0.0007255850359797478\n",
      "Step: 12900, train/learning_rate: 3.465016561676748e-05\n",
      "Step: 12900, train/epoch: 3.0699667930603027\n",
      "Step: 12910, train/loss: 0.0\n",
      "Step: 12910, train/grad_norm: 0.0003832816146314144\n",
      "Step: 12910, train/learning_rate: 3.4638265788089484e-05\n",
      "Step: 12910, train/epoch: 3.0723464488983154\n",
      "Step: 12920, train/loss: 0.0\n",
      "Step: 12920, train/grad_norm: 0.00030267279362306\n",
      "Step: 12920, train/learning_rate: 3.4626369597390294e-05\n",
      "Step: 12920, train/epoch: 3.0747263431549072\n",
      "Step: 12930, train/loss: 0.0\n",
      "Step: 12930, train/grad_norm: 0.0006045066402293742\n",
      "Step: 12930, train/learning_rate: 3.46144697687123e-05\n",
      "Step: 12930, train/epoch: 3.077106237411499\n",
      "Step: 12940, train/loss: 0.0\n",
      "Step: 12940, train/grad_norm: 0.0010858029127120972\n",
      "Step: 12940, train/learning_rate: 3.46025699400343e-05\n",
      "Step: 12940, train/epoch: 3.0794858932495117\n",
      "Step: 12950, train/loss: 0.0\n",
      "Step: 12950, train/grad_norm: 0.0003249896690249443\n",
      "Step: 12950, train/learning_rate: 3.45906701113563e-05\n",
      "Step: 12950, train/epoch: 3.0818657875061035\n",
      "Step: 12960, train/loss: 0.0\n",
      "Step: 12960, train/grad_norm: 0.0005584271857514977\n",
      "Step: 12960, train/learning_rate: 3.4578770282678306e-05\n",
      "Step: 12960, train/epoch: 3.0842456817626953\n",
      "Step: 12970, train/loss: 0.0\n",
      "Step: 12970, train/grad_norm: 0.00017225780175067484\n",
      "Step: 12970, train/learning_rate: 3.4566874091979116e-05\n",
      "Step: 12970, train/epoch: 3.086625337600708\n",
      "Step: 12980, train/loss: 0.0\n",
      "Step: 12980, train/grad_norm: 0.00039625697536394\n",
      "Step: 12980, train/learning_rate: 3.455497426330112e-05\n",
      "Step: 12980, train/epoch: 3.0890052318573\n",
      "Step: 12990, train/loss: 0.15780000388622284\n",
      "Step: 12990, train/grad_norm: 0.007444620598107576\n",
      "Step: 12990, train/learning_rate: 3.454307443462312e-05\n",
      "Step: 12990, train/epoch: 3.0913851261138916\n",
      "Step: 13000, train/loss: 0.0\n",
      "Step: 13000, train/grad_norm: 0.015577775426208973\n",
      "Step: 13000, train/learning_rate: 3.4531174605945125e-05\n",
      "Step: 13000, train/epoch: 3.0937647819519043\n",
      "Step: 13010, train/loss: 0.0\n",
      "Step: 13010, train/grad_norm: 0.010744480416178703\n",
      "Step: 13010, train/learning_rate: 3.451927477726713e-05\n",
      "Step: 13010, train/epoch: 3.096144676208496\n",
      "Step: 13020, train/loss: 0.0\n",
      "Step: 13020, train/grad_norm: 0.0030742953531444073\n",
      "Step: 13020, train/learning_rate: 3.450737858656794e-05\n",
      "Step: 13020, train/epoch: 3.098524570465088\n",
      "Step: 13030, train/loss: 0.0\n",
      "Step: 13030, train/grad_norm: 0.002532427432015538\n",
      "Step: 13030, train/learning_rate: 3.449547875788994e-05\n",
      "Step: 13030, train/epoch: 3.1009042263031006\n",
      "Step: 13040, train/loss: 0.0\n",
      "Step: 13040, train/grad_norm: 0.007726816926151514\n",
      "Step: 13040, train/learning_rate: 3.4483578929211944e-05\n",
      "Step: 13040, train/epoch: 3.1032841205596924\n",
      "Step: 13050, train/loss: 0.0\n",
      "Step: 13050, train/grad_norm: 0.0008718189783394337\n",
      "Step: 13050, train/learning_rate: 3.447167910053395e-05\n",
      "Step: 13050, train/epoch: 3.105664014816284\n",
      "Step: 13060, train/loss: 0.0\n",
      "Step: 13060, train/grad_norm: 0.0010454669827595353\n",
      "Step: 13060, train/learning_rate: 3.445977927185595e-05\n",
      "Step: 13060, train/epoch: 3.108043670654297\n",
      "Step: 13070, train/loss: 0.0\n",
      "Step: 13070, train/grad_norm: 0.001517475233413279\n",
      "Step: 13070, train/learning_rate: 3.444788308115676e-05\n",
      "Step: 13070, train/epoch: 3.1104235649108887\n",
      "Step: 13080, train/loss: 0.0\n",
      "Step: 13080, train/grad_norm: 0.0015916324919089675\n",
      "Step: 13080, train/learning_rate: 3.4435983252478763e-05\n",
      "Step: 13080, train/epoch: 3.1128034591674805\n",
      "Step: 13090, train/loss: 0.0\n",
      "Step: 13090, train/grad_norm: 0.001547321560792625\n",
      "Step: 13090, train/learning_rate: 3.4424083423800766e-05\n",
      "Step: 13090, train/epoch: 3.1151833534240723\n",
      "Step: 13100, train/loss: 0.09139999747276306\n",
      "Step: 13100, train/grad_norm: 0.0017460969975218177\n",
      "Step: 13100, train/learning_rate: 3.441218359512277e-05\n",
      "Step: 13100, train/epoch: 3.117563009262085\n",
      "Step: 13110, train/loss: 0.0\n",
      "Step: 13110, train/grad_norm: 0.05878009274601936\n",
      "Step: 13110, train/learning_rate: 3.440028376644477e-05\n",
      "Step: 13110, train/epoch: 3.1199429035186768\n",
      "Step: 13120, train/loss: 0.1762000024318695\n",
      "Step: 13120, train/grad_norm: 19.726194381713867\n",
      "Step: 13120, train/learning_rate: 3.438838757574558e-05\n",
      "Step: 13120, train/epoch: 3.1223227977752686\n",
      "Step: 13130, train/loss: 0.0\n",
      "Step: 13130, train/grad_norm: 0.015810390934348106\n",
      "Step: 13130, train/learning_rate: 3.4376487747067586e-05\n",
      "Step: 13130, train/epoch: 3.1247024536132812\n",
      "Step: 13140, train/loss: 0.0\n",
      "Step: 13140, train/grad_norm: 0.007115204818546772\n",
      "Step: 13140, train/learning_rate: 3.436458791838959e-05\n",
      "Step: 13140, train/epoch: 3.127082347869873\n",
      "Step: 13150, train/loss: 0.0\n",
      "Step: 13150, train/grad_norm: 0.010479407384991646\n",
      "Step: 13150, train/learning_rate: 3.435268808971159e-05\n",
      "Step: 13150, train/epoch: 3.129462242126465\n",
      "Step: 13160, train/loss: 0.0\n",
      "Step: 13160, train/grad_norm: 0.0054925368167459965\n",
      "Step: 13160, train/learning_rate: 3.43407918990124e-05\n",
      "Step: 13160, train/epoch: 3.1318418979644775\n",
      "Step: 13170, train/loss: 0.0\n",
      "Step: 13170, train/grad_norm: 0.01609470508992672\n",
      "Step: 13170, train/learning_rate: 3.4328892070334405e-05\n",
      "Step: 13170, train/epoch: 3.1342217922210693\n",
      "Step: 13180, train/loss: 0.0\n",
      "Step: 13180, train/grad_norm: 0.0003478167054709047\n",
      "Step: 13180, train/learning_rate: 3.431699224165641e-05\n",
      "Step: 13180, train/epoch: 3.136601686477661\n",
      "Step: 13190, train/loss: 0.0\n",
      "Step: 13190, train/grad_norm: 0.0009552134433761239\n",
      "Step: 13190, train/learning_rate: 3.430509241297841e-05\n",
      "Step: 13190, train/epoch: 3.138981342315674\n",
      "Step: 13200, train/loss: 0.0\n",
      "Step: 13200, train/grad_norm: 0.0017865120898932219\n",
      "Step: 13200, train/learning_rate: 3.4293192584300414e-05\n",
      "Step: 13200, train/epoch: 3.1413612365722656\n",
      "Step: 13210, train/loss: 0.12030000239610672\n",
      "Step: 13210, train/grad_norm: 469.93695068359375\n",
      "Step: 13210, train/learning_rate: 3.4281296393601224e-05\n",
      "Step: 13210, train/epoch: 3.1437411308288574\n",
      "Step: 13220, train/loss: 0.0\n",
      "Step: 13220, train/grad_norm: 0.008935836143791676\n",
      "Step: 13220, train/learning_rate: 3.426939656492323e-05\n",
      "Step: 13220, train/epoch: 3.14612078666687\n",
      "Step: 13230, train/loss: 0.0\n",
      "Step: 13230, train/grad_norm: 0.008509761653840542\n",
      "Step: 13230, train/learning_rate: 3.425749673624523e-05\n",
      "Step: 13230, train/epoch: 3.148500680923462\n",
      "Step: 13240, train/loss: 0.0\n",
      "Step: 13240, train/grad_norm: 0.0041327825747430325\n",
      "Step: 13240, train/learning_rate: 3.424559690756723e-05\n",
      "Step: 13240, train/epoch: 3.1508805751800537\n",
      "Step: 13250, train/loss: 0.0\n",
      "Step: 13250, train/grad_norm: 0.060149382799863815\n",
      "Step: 13250, train/learning_rate: 3.4233697078889236e-05\n",
      "Step: 13250, train/epoch: 3.1532604694366455\n",
      "Step: 13260, train/loss: 0.14380000531673431\n",
      "Step: 13260, train/grad_norm: 0.02028617449104786\n",
      "Step: 13260, train/learning_rate: 3.4221800888190046e-05\n",
      "Step: 13260, train/epoch: 3.155640125274658\n",
      "Step: 13270, train/loss: 0.0\n",
      "Step: 13270, train/grad_norm: 0.019205663353204727\n",
      "Step: 13270, train/learning_rate: 3.420990105951205e-05\n",
      "Step: 13270, train/epoch: 3.15802001953125\n",
      "Step: 13280, train/loss: 9.999999747378752e-05\n",
      "Step: 13280, train/grad_norm: 0.02782353013753891\n",
      "Step: 13280, train/learning_rate: 3.419800123083405e-05\n",
      "Step: 13280, train/epoch: 3.160399913787842\n",
      "Step: 13290, train/loss: 0.0\n",
      "Step: 13290, train/grad_norm: 0.006061629392206669\n",
      "Step: 13290, train/learning_rate: 3.4186101402156055e-05\n",
      "Step: 13290, train/epoch: 3.1627795696258545\n",
      "Step: 13300, train/loss: 0.0\n",
      "Step: 13300, train/grad_norm: 0.0027760653756558895\n",
      "Step: 13300, train/learning_rate: 3.417420157347806e-05\n",
      "Step: 13300, train/epoch: 3.1651594638824463\n",
      "Step: 13310, train/loss: 0.0\n",
      "Step: 13310, train/grad_norm: 0.0017802513903006911\n",
      "Step: 13310, train/learning_rate: 3.416230538277887e-05\n",
      "Step: 13310, train/epoch: 3.167539358139038\n",
      "Step: 13320, train/loss: 0.1257999986410141\n",
      "Step: 13320, train/grad_norm: 0.027893731370568275\n",
      "Step: 13320, train/learning_rate: 3.415040555410087e-05\n",
      "Step: 13320, train/epoch: 3.169919013977051\n",
      "Step: 13330, train/loss: 9.999999747378752e-05\n",
      "Step: 13330, train/grad_norm: 0.12076263129711151\n",
      "Step: 13330, train/learning_rate: 3.4138505725422874e-05\n",
      "Step: 13330, train/epoch: 3.1722989082336426\n",
      "Step: 13340, train/loss: 9.999999747378752e-05\n",
      "Step: 13340, train/grad_norm: 0.0013041046913713217\n",
      "Step: 13340, train/learning_rate: 3.412660589674488e-05\n",
      "Step: 13340, train/epoch: 3.1746788024902344\n",
      "Step: 13350, train/loss: 0.0\n",
      "Step: 13350, train/grad_norm: 0.0003217563789803535\n",
      "Step: 13350, train/learning_rate: 3.411470606806688e-05\n",
      "Step: 13350, train/epoch: 3.177058458328247\n",
      "Step: 13360, train/loss: 0.0\n",
      "Step: 13360, train/grad_norm: 0.00025405926862731576\n",
      "Step: 13360, train/learning_rate: 3.410280987736769e-05\n",
      "Step: 13360, train/epoch: 3.179438352584839\n",
      "Step: 13370, train/loss: 0.0\n",
      "Step: 13370, train/grad_norm: 0.0008583471062593162\n",
      "Step: 13370, train/learning_rate: 3.409091004868969e-05\n",
      "Step: 13370, train/epoch: 3.1818182468414307\n",
      "Step: 13380, train/loss: 0.0006000000284984708\n",
      "Step: 13380, train/grad_norm: 16.04593276977539\n",
      "Step: 13380, train/learning_rate: 3.4079010220011696e-05\n",
      "Step: 13380, train/epoch: 3.1841979026794434\n",
      "Step: 13390, train/loss: 0.0\n",
      "Step: 13390, train/grad_norm: 0.00011251898104092106\n",
      "Step: 13390, train/learning_rate: 3.40671103913337e-05\n",
      "Step: 13390, train/epoch: 3.186577796936035\n",
      "Step: 13400, train/loss: 0.1736000031232834\n",
      "Step: 13400, train/grad_norm: 0.0037999050691723824\n",
      "Step: 13400, train/learning_rate: 3.40552105626557e-05\n",
      "Step: 13400, train/epoch: 3.188957691192627\n",
      "Step: 13410, train/loss: 0.186599999666214\n",
      "Step: 13410, train/grad_norm: 0.025066064670681953\n",
      "Step: 13410, train/learning_rate: 3.404331437195651e-05\n",
      "Step: 13410, train/epoch: 3.1913373470306396\n",
      "Step: 13420, train/loss: 9.999999747378752e-05\n",
      "Step: 13420, train/grad_norm: 0.023989945650100708\n",
      "Step: 13420, train/learning_rate: 3.4031414543278515e-05\n",
      "Step: 13420, train/epoch: 3.1937172412872314\n",
      "Step: 13430, train/loss: 0.020899999886751175\n",
      "Step: 13430, train/grad_norm: 0.00692451698705554\n",
      "Step: 13430, train/learning_rate: 3.401951471460052e-05\n",
      "Step: 13430, train/epoch: 3.1960971355438232\n",
      "Step: 13440, train/loss: 0.0\n",
      "Step: 13440, train/grad_norm: 0.00034973639412783086\n",
      "Step: 13440, train/learning_rate: 3.400761488592252e-05\n",
      "Step: 13440, train/epoch: 3.198477029800415\n",
      "Step: 13450, train/loss: 0.0\n",
      "Step: 13450, train/grad_norm: 0.0008660047315061092\n",
      "Step: 13450, train/learning_rate: 3.3995715057244524e-05\n",
      "Step: 13450, train/epoch: 3.2008566856384277\n",
      "Step: 13460, train/loss: 0.0\n",
      "Step: 13460, train/grad_norm: 0.00331898289732635\n",
      "Step: 13460, train/learning_rate: 3.3983818866545334e-05\n",
      "Step: 13460, train/epoch: 3.2032365798950195\n",
      "Step: 13470, train/loss: 0.17030000686645508\n",
      "Step: 13470, train/grad_norm: 0.0025593265891075134\n",
      "Step: 13470, train/learning_rate: 3.397191903786734e-05\n",
      "Step: 13470, train/epoch: 3.2056164741516113\n",
      "Step: 13480, train/loss: 0.05950000137090683\n",
      "Step: 13480, train/grad_norm: 0.15761542320251465\n",
      "Step: 13480, train/learning_rate: 3.396001920918934e-05\n",
      "Step: 13480, train/epoch: 3.207996129989624\n",
      "Step: 13490, train/loss: 0.2298000007867813\n",
      "Step: 13490, train/grad_norm: 0.05727556347846985\n",
      "Step: 13490, train/learning_rate: 3.3948119380511343e-05\n",
      "Step: 13490, train/epoch: 3.210376024246216\n",
      "Step: 13500, train/loss: 0.0\n",
      "Step: 13500, train/grad_norm: 0.0019730040803551674\n",
      "Step: 13500, train/learning_rate: 3.3936219551833346e-05\n",
      "Step: 13500, train/epoch: 3.2127559185028076\n",
      "Step: 13510, train/loss: 0.0\n",
      "Step: 13510, train/grad_norm: 0.0009879651479423046\n",
      "Step: 13510, train/learning_rate: 3.3924323361134157e-05\n",
      "Step: 13510, train/epoch: 3.2151355743408203\n",
      "Step: 13520, train/loss: 0.0\n",
      "Step: 13520, train/grad_norm: 0.00010872799612116069\n",
      "Step: 13520, train/learning_rate: 3.391242353245616e-05\n",
      "Step: 13520, train/epoch: 3.217515468597412\n",
      "Step: 13530, train/loss: 0.0\n",
      "Step: 13530, train/grad_norm: 0.00045693133142776787\n",
      "Step: 13530, train/learning_rate: 3.390052370377816e-05\n",
      "Step: 13530, train/epoch: 3.219895362854004\n",
      "Step: 13540, train/loss: 0.0\n",
      "Step: 13540, train/grad_norm: 0.00017052564362529665\n",
      "Step: 13540, train/learning_rate: 3.3888623875100166e-05\n",
      "Step: 13540, train/epoch: 3.2222750186920166\n",
      "Step: 13550, train/loss: 0.0\n",
      "Step: 13550, train/grad_norm: 0.00040192579035647213\n",
      "Step: 13550, train/learning_rate: 3.387672404642217e-05\n",
      "Step: 13550, train/epoch: 3.2246549129486084\n",
      "Step: 13560, train/loss: 0.0\n",
      "Step: 13560, train/grad_norm: 0.00029375270241871476\n",
      "Step: 13560, train/learning_rate: 3.386482785572298e-05\n",
      "Step: 13560, train/epoch: 3.2270348072052\n",
      "Step: 13570, train/loss: 0.0\n",
      "Step: 13570, train/grad_norm: 0.00021141856268513948\n",
      "Step: 13570, train/learning_rate: 3.385292802704498e-05\n",
      "Step: 13570, train/epoch: 3.229414463043213\n",
      "Step: 13580, train/loss: 0.0\n",
      "Step: 13580, train/grad_norm: 0.00011443888070061803\n",
      "Step: 13580, train/learning_rate: 3.3841028198366985e-05\n",
      "Step: 13580, train/epoch: 3.2317943572998047\n",
      "Step: 13590, train/loss: 0.0\n",
      "Step: 13590, train/grad_norm: 0.00014707230729982257\n",
      "Step: 13590, train/learning_rate: 3.382912836968899e-05\n",
      "Step: 13590, train/epoch: 3.2341742515563965\n",
      "Step: 13600, train/loss: 0.0\n",
      "Step: 13600, train/grad_norm: 8.973712101578712e-05\n",
      "Step: 13600, train/learning_rate: 3.381722854101099e-05\n",
      "Step: 13600, train/epoch: 3.236553907394409\n",
      "Step: 13610, train/loss: 0.1266999989748001\n",
      "Step: 13610, train/grad_norm: 0.00032950280001387\n",
      "Step: 13610, train/learning_rate: 3.38053323503118e-05\n",
      "Step: 13610, train/epoch: 3.238933801651001\n",
      "Step: 13620, train/loss: 0.0\n",
      "Step: 13620, train/grad_norm: 0.012429611757397652\n",
      "Step: 13620, train/learning_rate: 3.3793432521633804e-05\n",
      "Step: 13620, train/epoch: 3.2413136959075928\n",
      "Step: 13630, train/loss: 0.3260999917984009\n",
      "Step: 13630, train/grad_norm: 1.1870900392532349\n",
      "Step: 13630, train/learning_rate: 3.378153269295581e-05\n",
      "Step: 13630, train/epoch: 3.2436935901641846\n",
      "Step: 13640, train/loss: 0.11640000343322754\n",
      "Step: 13640, train/grad_norm: 0.0033341634552925825\n",
      "Step: 13640, train/learning_rate: 3.376963286427781e-05\n",
      "Step: 13640, train/epoch: 3.2460732460021973\n",
      "Step: 13650, train/loss: 0.13989999890327454\n",
      "Step: 13650, train/grad_norm: 0.01967402920126915\n",
      "Step: 13650, train/learning_rate: 3.375773303559981e-05\n",
      "Step: 13650, train/epoch: 3.248453140258789\n",
      "Step: 13660, train/loss: 0.00019999999494757503\n",
      "Step: 13660, train/grad_norm: 0.19681842625141144\n",
      "Step: 13660, train/learning_rate: 3.374583684490062e-05\n",
      "Step: 13660, train/epoch: 3.250833034515381\n",
      "Step: 13670, train/loss: 0.0\n",
      "Step: 13670, train/grad_norm: 0.002707212697714567\n",
      "Step: 13670, train/learning_rate: 3.3733937016222626e-05\n",
      "Step: 13670, train/epoch: 3.2532126903533936\n",
      "Step: 13680, train/loss: 0.0\n",
      "Step: 13680, train/grad_norm: 0.0007771685486659408\n",
      "Step: 13680, train/learning_rate: 3.372203718754463e-05\n",
      "Step: 13680, train/epoch: 3.2555925846099854\n",
      "Step: 13690, train/loss: 0.17030000686645508\n",
      "Step: 13690, train/grad_norm: 0.005569104105234146\n",
      "Step: 13690, train/learning_rate: 3.371013735886663e-05\n",
      "Step: 13690, train/epoch: 3.257972478866577\n",
      "Step: 13700, train/loss: 0.0\n",
      "Step: 13700, train/grad_norm: 0.030870836228132248\n",
      "Step: 13700, train/learning_rate: 3.3698237530188635e-05\n",
      "Step: 13700, train/epoch: 3.26035213470459\n",
      "Step: 13710, train/loss: 0.0\n",
      "Step: 13710, train/grad_norm: 0.02746529132127762\n",
      "Step: 13710, train/learning_rate: 3.3686341339489445e-05\n",
      "Step: 13710, train/epoch: 3.2627320289611816\n",
      "Step: 13720, train/loss: 0.0\n",
      "Step: 13720, train/grad_norm: 0.029612621292471886\n",
      "Step: 13720, train/learning_rate: 3.367444151081145e-05\n",
      "Step: 13720, train/epoch: 3.2651119232177734\n",
      "Step: 13730, train/loss: 0.0\n",
      "Step: 13730, train/grad_norm: 0.001852389075793326\n",
      "Step: 13730, train/learning_rate: 3.366254168213345e-05\n",
      "Step: 13730, train/epoch: 3.267491579055786\n",
      "Step: 13740, train/loss: 0.0\n",
      "Step: 13740, train/grad_norm: 0.03647838905453682\n",
      "Step: 13740, train/learning_rate: 3.3650641853455454e-05\n",
      "Step: 13740, train/epoch: 3.269871473312378\n",
      "Step: 13750, train/loss: 0.0\n",
      "Step: 13750, train/grad_norm: 0.0011552248615771532\n",
      "Step: 13750, train/learning_rate: 3.363874202477746e-05\n",
      "Step: 13750, train/epoch: 3.2722513675689697\n",
      "Step: 13760, train/loss: 0.0\n",
      "Step: 13760, train/grad_norm: 0.0012140049366280437\n",
      "Step: 13760, train/learning_rate: 3.362684583407827e-05\n",
      "Step: 13760, train/epoch: 3.2746310234069824\n",
      "Step: 13770, train/loss: 0.0\n",
      "Step: 13770, train/grad_norm: 0.0014288336969912052\n",
      "Step: 13770, train/learning_rate: 3.361494600540027e-05\n",
      "Step: 13770, train/epoch: 3.277010917663574\n",
      "Step: 13780, train/loss: 0.0\n",
      "Step: 13780, train/grad_norm: 0.0016916997265070677\n",
      "Step: 13780, train/learning_rate: 3.360304617672227e-05\n",
      "Step: 13780, train/epoch: 3.279390811920166\n",
      "Step: 13790, train/loss: 0.0\n",
      "Step: 13790, train/grad_norm: 0.0011554127559065819\n",
      "Step: 13790, train/learning_rate: 3.3591146348044276e-05\n",
      "Step: 13790, train/epoch: 3.2817704677581787\n",
      "Step: 13800, train/loss: 0.0\n",
      "Step: 13800, train/grad_norm: 0.001442550914362073\n",
      "Step: 13800, train/learning_rate: 3.357924651936628e-05\n",
      "Step: 13800, train/epoch: 3.2841503620147705\n",
      "Step: 13810, train/loss: 0.0\n",
      "Step: 13810, train/grad_norm: 0.0014675308484584093\n",
      "Step: 13810, train/learning_rate: 3.356735032866709e-05\n",
      "Step: 13810, train/epoch: 3.2865302562713623\n",
      "Step: 13820, train/loss: 0.0\n",
      "Step: 13820, train/grad_norm: 0.0005384533433243632\n",
      "Step: 13820, train/learning_rate: 3.355545049998909e-05\n",
      "Step: 13820, train/epoch: 3.288910150527954\n",
      "Step: 13830, train/loss: 0.0\n",
      "Step: 13830, train/grad_norm: 0.0004902505897916853\n",
      "Step: 13830, train/learning_rate: 3.3543550671311095e-05\n",
      "Step: 13830, train/epoch: 3.291289806365967\n",
      "Step: 13840, train/loss: 0.0\n",
      "Step: 13840, train/grad_norm: 0.0005340739735402167\n",
      "Step: 13840, train/learning_rate: 3.35316508426331e-05\n",
      "Step: 13840, train/epoch: 3.2936697006225586\n",
      "Step: 13850, train/loss: 0.0\n",
      "Step: 13850, train/grad_norm: 0.0006647867267020047\n",
      "Step: 13850, train/learning_rate: 3.35197510139551e-05\n",
      "Step: 13850, train/epoch: 3.2960495948791504\n",
      "Step: 13860, train/loss: 0.0\n",
      "Step: 13860, train/grad_norm: 0.002622389467433095\n",
      "Step: 13860, train/learning_rate: 3.350785482325591e-05\n",
      "Step: 13860, train/epoch: 3.298429250717163\n",
      "Step: 13870, train/loss: 0.0\n",
      "Step: 13870, train/grad_norm: 0.00042230344843119383\n",
      "Step: 13870, train/learning_rate: 3.3495954994577914e-05\n",
      "Step: 13870, train/epoch: 3.300809144973755\n",
      "Step: 13880, train/loss: 0.0\n",
      "Step: 13880, train/grad_norm: 0.00020708043302875012\n",
      "Step: 13880, train/learning_rate: 3.348405516589992e-05\n",
      "Step: 13880, train/epoch: 3.3031890392303467\n",
      "Step: 13890, train/loss: 0.0\n",
      "Step: 13890, train/grad_norm: 0.0003710006130859256\n",
      "Step: 13890, train/learning_rate: 3.347215533722192e-05\n",
      "Step: 13890, train/epoch: 3.3055686950683594\n",
      "Step: 13900, train/loss: 0.0\n",
      "Step: 13900, train/grad_norm: 0.00022082982468418777\n",
      "Step: 13900, train/learning_rate: 3.3460255508543923e-05\n",
      "Step: 13900, train/epoch: 3.307948589324951\n",
      "Step: 13910, train/loss: 0.0\n",
      "Step: 13910, train/grad_norm: 0.0002198340225731954\n",
      "Step: 13910, train/learning_rate: 3.3448359317844734e-05\n",
      "Step: 13910, train/epoch: 3.310328483581543\n",
      "Step: 13920, train/loss: 0.0\n",
      "Step: 13920, train/grad_norm: 0.0002504301373846829\n",
      "Step: 13920, train/learning_rate: 3.3436459489166737e-05\n",
      "Step: 13920, train/epoch: 3.3127081394195557\n",
      "Step: 13930, train/loss: 0.0\n",
      "Step: 13930, train/grad_norm: 0.00015021543367765844\n",
      "Step: 13930, train/learning_rate: 3.342455966048874e-05\n",
      "Step: 13930, train/epoch: 3.3150880336761475\n",
      "Step: 13940, train/loss: 0.13830000162124634\n",
      "Step: 13940, train/grad_norm: 0.026571206748485565\n",
      "Step: 13940, train/learning_rate: 3.341265983181074e-05\n",
      "Step: 13940, train/epoch: 3.3174679279327393\n",
      "Step: 13950, train/loss: 9.999999747378752e-05\n",
      "Step: 13950, train/grad_norm: 0.020423123612999916\n",
      "Step: 13950, train/learning_rate: 3.3400760003132746e-05\n",
      "Step: 13950, train/epoch: 3.319847583770752\n",
      "Step: 13960, train/loss: 0.0\n",
      "Step: 13960, train/grad_norm: 0.0023662985768169165\n",
      "Step: 13960, train/learning_rate: 3.3388863812433556e-05\n",
      "Step: 13960, train/epoch: 3.3222274780273438\n",
      "Step: 13970, train/loss: 0.0\n",
      "Step: 13970, train/grad_norm: 0.0026262581814080477\n",
      "Step: 13970, train/learning_rate: 3.337696398375556e-05\n",
      "Step: 13970, train/epoch: 3.3246073722839355\n",
      "Step: 13980, train/loss: 0.0\n",
      "Step: 13980, train/grad_norm: 0.0031220728997141123\n",
      "Step: 13980, train/learning_rate: 3.336506415507756e-05\n",
      "Step: 13980, train/epoch: 3.3269872665405273\n",
      "Step: 13990, train/loss: 0.0\n",
      "Step: 13990, train/grad_norm: 0.0010633605998009443\n",
      "Step: 13990, train/learning_rate: 3.3353164326399565e-05\n",
      "Step: 13990, train/epoch: 3.32936692237854\n",
      "Step: 14000, train/loss: 0.0\n",
      "Step: 14000, train/grad_norm: 0.001815027673728764\n",
      "Step: 14000, train/learning_rate: 3.334126449772157e-05\n",
      "Step: 14000, train/epoch: 3.331746816635132\n",
      "Step: 14010, train/loss: 0.0\n",
      "Step: 14010, train/grad_norm: 0.0024618401657789946\n",
      "Step: 14010, train/learning_rate: 3.332936830702238e-05\n",
      "Step: 14010, train/epoch: 3.3341267108917236\n",
      "Step: 14020, train/loss: 0.0\n",
      "Step: 14020, train/grad_norm: 0.0006876795087009668\n",
      "Step: 14020, train/learning_rate: 3.331746847834438e-05\n",
      "Step: 14020, train/epoch: 3.3365063667297363\n",
      "Step: 14030, train/loss: 0.0\n",
      "Step: 14030, train/grad_norm: 0.0019132388988509774\n",
      "Step: 14030, train/learning_rate: 3.3305568649666384e-05\n",
      "Step: 14030, train/epoch: 3.338886260986328\n",
      "Step: 14040, train/loss: 0.0\n",
      "Step: 14040, train/grad_norm: 0.0009490468655712903\n",
      "Step: 14040, train/learning_rate: 3.329366882098839e-05\n",
      "Step: 14040, train/epoch: 3.34126615524292\n",
      "Step: 14050, train/loss: 0.1664000004529953\n",
      "Step: 14050, train/grad_norm: 0.001478266902267933\n",
      "Step: 14050, train/learning_rate: 3.328176899231039e-05\n",
      "Step: 14050, train/epoch: 3.3436458110809326\n",
      "Step: 14060, train/loss: 0.0\n",
      "Step: 14060, train/grad_norm: 0.026632728055119514\n",
      "Step: 14060, train/learning_rate: 3.32698728016112e-05\n",
      "Step: 14060, train/epoch: 3.3460257053375244\n",
      "Step: 14070, train/loss: 0.0\n",
      "Step: 14070, train/grad_norm: 0.009316079318523407\n",
      "Step: 14070, train/learning_rate: 3.32579729729332e-05\n",
      "Step: 14070, train/epoch: 3.348405599594116\n",
      "Step: 14080, train/loss: 0.0\n",
      "Step: 14080, train/grad_norm: 0.011109459213912487\n",
      "Step: 14080, train/learning_rate: 3.3246073144255206e-05\n",
      "Step: 14080, train/epoch: 3.350785255432129\n",
      "Step: 14090, train/loss: 0.0\n",
      "Step: 14090, train/grad_norm: 0.0018794310744851828\n",
      "Step: 14090, train/learning_rate: 3.323417331557721e-05\n",
      "Step: 14090, train/epoch: 3.3531651496887207\n",
      "Step: 14100, train/loss: 9.999999747378752e-05\n",
      "Step: 14100, train/grad_norm: 0.0036280432250350714\n",
      "Step: 14100, train/learning_rate: 3.322227348689921e-05\n",
      "Step: 14100, train/epoch: 3.3555450439453125\n",
      "Step: 14110, train/loss: 0.0\n",
      "Step: 14110, train/grad_norm: 0.002943498780950904\n",
      "Step: 14110, train/learning_rate: 3.321037729620002e-05\n",
      "Step: 14110, train/epoch: 3.357924699783325\n",
      "Step: 14120, train/loss: 0.0\n",
      "Step: 14120, train/grad_norm: 0.0013103795936331153\n",
      "Step: 14120, train/learning_rate: 3.3198477467522025e-05\n",
      "Step: 14120, train/epoch: 3.360304594039917\n",
      "Step: 14130, train/loss: 0.1453000009059906\n",
      "Step: 14130, train/grad_norm: 0.0009613566217012703\n",
      "Step: 14130, train/learning_rate: 3.318657763884403e-05\n",
      "Step: 14130, train/epoch: 3.362684488296509\n",
      "Step: 14140, train/loss: 0.1257999986410141\n",
      "Step: 14140, train/grad_norm: 0.054215073585510254\n",
      "Step: 14140, train/learning_rate: 3.317467781016603e-05\n",
      "Step: 14140, train/epoch: 3.3650641441345215\n",
      "Step: 14150, train/loss: 0.00019999999494757503\n",
      "Step: 14150, train/grad_norm: 0.051717646420001984\n",
      "Step: 14150, train/learning_rate: 3.3162777981488034e-05\n",
      "Step: 14150, train/epoch: 3.3674440383911133\n",
      "Step: 14160, train/loss: 0.07349999994039536\n",
      "Step: 14160, train/grad_norm: 127.31128692626953\n",
      "Step: 14160, train/learning_rate: 3.3150881790788844e-05\n",
      "Step: 14160, train/epoch: 3.369823932647705\n",
      "Step: 14170, train/loss: 0.0\n",
      "Step: 14170, train/grad_norm: 0.00888415239751339\n",
      "Step: 14170, train/learning_rate: 3.313898196211085e-05\n",
      "Step: 14170, train/epoch: 3.372203826904297\n",
      "Step: 14180, train/loss: 0.13130000233650208\n",
      "Step: 14180, train/grad_norm: 0.03817388415336609\n",
      "Step: 14180, train/learning_rate: 3.312708213343285e-05\n",
      "Step: 14180, train/epoch: 3.3745834827423096\n",
      "Step: 14190, train/loss: 0.00019999999494757503\n",
      "Step: 14190, train/grad_norm: 0.06239640712738037\n",
      "Step: 14190, train/learning_rate: 3.311518230475485e-05\n",
      "Step: 14190, train/epoch: 3.3769633769989014\n",
      "Step: 14200, train/loss: 0.0\n",
      "Step: 14200, train/grad_norm: 0.0028495020233094692\n",
      "Step: 14200, train/learning_rate: 3.3103282476076856e-05\n",
      "Step: 14200, train/epoch: 3.379343271255493\n",
      "Step: 14210, train/loss: 0.12189999967813492\n",
      "Step: 14210, train/grad_norm: 0.006995639763772488\n",
      "Step: 14210, train/learning_rate: 3.3091386285377666e-05\n",
      "Step: 14210, train/epoch: 3.381722927093506\n",
      "Step: 14220, train/loss: 0.07150000333786011\n",
      "Step: 14220, train/grad_norm: 0.024217698723077774\n",
      "Step: 14220, train/learning_rate: 3.307948645669967e-05\n",
      "Step: 14220, train/epoch: 3.3841028213500977\n",
      "Step: 14230, train/loss: 0.0003000000142492354\n",
      "Step: 14230, train/grad_norm: 0.023187706246972084\n",
      "Step: 14230, train/learning_rate: 3.306758662802167e-05\n",
      "Step: 14230, train/epoch: 3.3864827156066895\n",
      "Step: 14240, train/loss: 0.09969999641180038\n",
      "Step: 14240, train/grad_norm: 0.015941407531499863\n",
      "Step: 14240, train/learning_rate: 3.3055686799343675e-05\n",
      "Step: 14240, train/epoch: 3.388862371444702\n",
      "Step: 14250, train/loss: 9.999999747378752e-05\n",
      "Step: 14250, train/grad_norm: 0.059735920280218124\n",
      "Step: 14250, train/learning_rate: 3.304378697066568e-05\n",
      "Step: 14250, train/epoch: 3.391242265701294\n",
      "Step: 14260, train/loss: 0.14920000731945038\n",
      "Step: 14260, train/grad_norm: 87.04608154296875\n",
      "Step: 14260, train/learning_rate: 3.303189077996649e-05\n",
      "Step: 14260, train/epoch: 3.3936221599578857\n",
      "Step: 14270, train/loss: 0.12200000137090683\n",
      "Step: 14270, train/grad_norm: 0.2177186757326126\n",
      "Step: 14270, train/learning_rate: 3.301999095128849e-05\n",
      "Step: 14270, train/epoch: 3.3960018157958984\n",
      "Step: 14280, train/loss: 9.999999747378752e-05\n",
      "Step: 14280, train/grad_norm: 0.0001690300414338708\n",
      "Step: 14280, train/learning_rate: 3.3008091122610494e-05\n",
      "Step: 14280, train/epoch: 3.3983817100524902\n",
      "Step: 14290, train/loss: 0.0\n",
      "Step: 14290, train/grad_norm: 6.308049341896549e-05\n",
      "Step: 14290, train/learning_rate: 3.29961912939325e-05\n",
      "Step: 14290, train/epoch: 3.400761604309082\n",
      "Step: 14300, train/loss: 0.14219999313354492\n",
      "Step: 14300, train/grad_norm: 0.00010875302541535348\n",
      "Step: 14300, train/learning_rate: 3.29842914652545e-05\n",
      "Step: 14300, train/epoch: 3.4031412601470947\n",
      "Step: 14310, train/loss: 9.999999747378752e-05\n",
      "Step: 14310, train/grad_norm: 0.00022744969464838505\n",
      "Step: 14310, train/learning_rate: 3.297239527455531e-05\n",
      "Step: 14310, train/epoch: 3.4055211544036865\n",
      "Step: 14320, train/loss: 0.0\n",
      "Step: 14320, train/grad_norm: 0.0005886699655093253\n",
      "Step: 14320, train/learning_rate: 3.2960495445877314e-05\n",
      "Step: 14320, train/epoch: 3.4079010486602783\n",
      "Step: 14330, train/loss: 0.13519999384880066\n",
      "Step: 14330, train/grad_norm: 81.07710266113281\n",
      "Step: 14330, train/learning_rate: 3.294859561719932e-05\n",
      "Step: 14330, train/epoch: 3.410280704498291\n",
      "Step: 14340, train/loss: 0.0\n",
      "Step: 14340, train/grad_norm: 0.10169340670108795\n",
      "Step: 14340, train/learning_rate: 3.293669578852132e-05\n",
      "Step: 14340, train/epoch: 3.412660598754883\n",
      "Step: 14350, train/loss: 0.0\n",
      "Step: 14350, train/grad_norm: 0.012772531248629093\n",
      "Step: 14350, train/learning_rate: 3.292479595984332e-05\n",
      "Step: 14350, train/epoch: 3.4150404930114746\n",
      "Step: 14360, train/loss: 0.09880000352859497\n",
      "Step: 14360, train/grad_norm: 0.011502522975206375\n",
      "Step: 14360, train/learning_rate: 3.291289976914413e-05\n",
      "Step: 14360, train/epoch: 3.4174203872680664\n",
      "Step: 14370, train/loss: 0.0003000000142492354\n",
      "Step: 14370, train/grad_norm: 0.030991418287158012\n",
      "Step: 14370, train/learning_rate: 3.2900999940466136e-05\n",
      "Step: 14370, train/epoch: 3.419800043106079\n",
      "Step: 14380, train/loss: 0.08829999715089798\n",
      "Step: 14380, train/grad_norm: 0.0009146667434833944\n",
      "Step: 14380, train/learning_rate: 3.288910011178814e-05\n",
      "Step: 14380, train/epoch: 3.422179937362671\n",
      "Step: 14390, train/loss: 0.0\n",
      "Step: 14390, train/grad_norm: 0.0007974935579113662\n",
      "Step: 14390, train/learning_rate: 3.287720028311014e-05\n",
      "Step: 14390, train/epoch: 3.4245598316192627\n",
      "Step: 14400, train/loss: 0.0\n",
      "Step: 14400, train/grad_norm: 0.0004081264778506011\n",
      "Step: 14400, train/learning_rate: 3.2865300454432145e-05\n",
      "Step: 14400, train/epoch: 3.4269394874572754\n",
      "Step: 14410, train/loss: 0.0\n",
      "Step: 14410, train/grad_norm: 0.002275685779750347\n",
      "Step: 14410, train/learning_rate: 3.2853404263732955e-05\n",
      "Step: 14410, train/epoch: 3.429319381713867\n",
      "Step: 14420, train/loss: 0.0\n",
      "Step: 14420, train/grad_norm: 8.438043005298823e-05\n",
      "Step: 14420, train/learning_rate: 3.284150443505496e-05\n",
      "Step: 14420, train/epoch: 3.431699275970459\n",
      "Step: 14430, train/loss: 0.0\n",
      "Step: 14430, train/grad_norm: 0.0863446295261383\n",
      "Step: 14430, train/learning_rate: 3.282960460637696e-05\n",
      "Step: 14430, train/epoch: 3.4340789318084717\n",
      "Step: 14440, train/loss: 0.09799999743700027\n",
      "Step: 14440, train/grad_norm: 0.00047813740093261003\n",
      "Step: 14440, train/learning_rate: 3.2817704777698964e-05\n",
      "Step: 14440, train/epoch: 3.4364588260650635\n",
      "Step: 14450, train/loss: 9.999999747378752e-05\n",
      "Step: 14450, train/grad_norm: 0.001857569208368659\n",
      "Step: 14450, train/learning_rate: 3.280580494902097e-05\n",
      "Step: 14450, train/epoch: 3.4388387203216553\n",
      "Step: 14460, train/loss: 0.0\n",
      "Step: 14460, train/grad_norm: 0.007902652025222778\n",
      "Step: 14460, train/learning_rate: 3.279390875832178e-05\n",
      "Step: 14460, train/epoch: 3.441218376159668\n",
      "Step: 14470, train/loss: 0.013399999588727951\n",
      "Step: 14470, train/grad_norm: 0.0005213991389609873\n",
      "Step: 14470, train/learning_rate: 3.278200892964378e-05\n",
      "Step: 14470, train/epoch: 3.4435982704162598\n",
      "Step: 14480, train/loss: 0.0\n",
      "Step: 14480, train/grad_norm: 0.0002808656427077949\n",
      "Step: 14480, train/learning_rate: 3.277010910096578e-05\n",
      "Step: 14480, train/epoch: 3.4459781646728516\n",
      "Step: 14490, train/loss: 0.0\n",
      "Step: 14490, train/grad_norm: 0.0002619015285745263\n",
      "Step: 14490, train/learning_rate: 3.2758209272287786e-05\n",
      "Step: 14490, train/epoch: 3.4483578205108643\n",
      "Step: 14500, train/loss: 0.0\n",
      "Step: 14500, train/grad_norm: 0.001004653051495552\n",
      "Step: 14500, train/learning_rate: 3.2746313081588596e-05\n",
      "Step: 14500, train/epoch: 3.450737714767456\n",
      "Step: 14510, train/loss: 0.0\n",
      "Step: 14510, train/grad_norm: 0.00011895694478880614\n",
      "Step: 14510, train/learning_rate: 3.27344132529106e-05\n",
      "Step: 14510, train/epoch: 3.453117609024048\n",
      "Step: 14520, train/loss: 0.0\n",
      "Step: 14520, train/grad_norm: 0.00025042201741598547\n",
      "Step: 14520, train/learning_rate: 3.27225134242326e-05\n",
      "Step: 14520, train/epoch: 3.4554972648620605\n",
      "Step: 14530, train/loss: 0.0\n",
      "Step: 14530, train/grad_norm: 8.081251144176349e-05\n",
      "Step: 14530, train/learning_rate: 3.2710613595554605e-05\n",
      "Step: 14530, train/epoch: 3.4578771591186523\n",
      "Step: 14540, train/loss: 0.0\n",
      "Step: 14540, train/grad_norm: 4.1793227865127847e-05\n",
      "Step: 14540, train/learning_rate: 3.269871376687661e-05\n",
      "Step: 14540, train/epoch: 3.460257053375244\n",
      "Step: 14550, train/loss: 0.0\n",
      "Step: 14550, train/grad_norm: 9.140319161815569e-05\n",
      "Step: 14550, train/learning_rate: 3.268681757617742e-05\n",
      "Step: 14550, train/epoch: 3.462636947631836\n",
      "Step: 14560, train/loss: 0.0471000000834465\n",
      "Step: 14560, train/grad_norm: 0.0004640851984731853\n",
      "Step: 14560, train/learning_rate: 3.267491774749942e-05\n",
      "Step: 14560, train/epoch: 3.4650166034698486\n",
      "Step: 14570, train/loss: 0.0\n",
      "Step: 14570, train/grad_norm: 0.0034882400650531054\n",
      "Step: 14570, train/learning_rate: 3.2663017918821424e-05\n",
      "Step: 14570, train/epoch: 3.4673964977264404\n",
      "Step: 14580, train/loss: 0.3695000112056732\n",
      "Step: 14580, train/grad_norm: 0.0055623832158744335\n",
      "Step: 14580, train/learning_rate: 3.265111809014343e-05\n",
      "Step: 14580, train/epoch: 3.4697763919830322\n",
      "Step: 14590, train/loss: 0.09560000151395798\n",
      "Step: 14590, train/grad_norm: 0.9009061455726624\n",
      "Step: 14590, train/learning_rate: 3.263921826146543e-05\n",
      "Step: 14590, train/epoch: 3.472156047821045\n",
      "Step: 14600, train/loss: 9.999999747378752e-05\n",
      "Step: 14600, train/grad_norm: 0.0011912854388356209\n",
      "Step: 14600, train/learning_rate: 3.262732207076624e-05\n",
      "Step: 14600, train/epoch: 3.4745359420776367\n",
      "Step: 14610, train/loss: 0.18129999935626984\n",
      "Step: 14610, train/grad_norm: 0.010135756805539131\n",
      "Step: 14610, train/learning_rate: 3.261542224208824e-05\n",
      "Step: 14610, train/epoch: 3.4769158363342285\n",
      "Step: 14620, train/loss: 0.053599998354911804\n",
      "Step: 14620, train/grad_norm: 650.3838500976562\n",
      "Step: 14620, train/learning_rate: 3.2603522413410246e-05\n",
      "Step: 14620, train/epoch: 3.479295492172241\n",
      "Step: 14630, train/loss: 0.14820000529289246\n",
      "Step: 14630, train/grad_norm: 0.032030437141656876\n",
      "Step: 14630, train/learning_rate: 3.259162258473225e-05\n",
      "Step: 14630, train/epoch: 3.481675386428833\n",
      "Step: 14640, train/loss: 0.0851999968290329\n",
      "Step: 14640, train/grad_norm: 0.03785347938537598\n",
      "Step: 14640, train/learning_rate: 3.257972275605425e-05\n",
      "Step: 14640, train/epoch: 3.484055280685425\n",
      "Step: 14650, train/loss: 0.00019999999494757503\n",
      "Step: 14650, train/grad_norm: 0.09819646924734116\n",
      "Step: 14650, train/learning_rate: 3.256782656535506e-05\n",
      "Step: 14650, train/epoch: 3.4864349365234375\n",
      "Step: 14660, train/loss: 9.999999747378752e-05\n",
      "Step: 14660, train/grad_norm: 0.0010855323635041714\n",
      "Step: 14660, train/learning_rate: 3.2555926736677065e-05\n",
      "Step: 14660, train/epoch: 3.4888148307800293\n",
      "Step: 14670, train/loss: 0.0\n",
      "Step: 14670, train/grad_norm: 0.0007021394558250904\n",
      "Step: 14670, train/learning_rate: 3.254402690799907e-05\n",
      "Step: 14670, train/epoch: 3.491194725036621\n",
      "Step: 14680, train/loss: 0.051600001752376556\n",
      "Step: 14680, train/grad_norm: 0.018626339733600616\n",
      "Step: 14680, train/learning_rate: 3.253212707932107e-05\n",
      "Step: 14680, train/epoch: 3.493574380874634\n",
      "Step: 14690, train/loss: 0.00019999999494757503\n",
      "Step: 14690, train/grad_norm: 1.1846874952316284\n",
      "Step: 14690, train/learning_rate: 3.2520227250643075e-05\n",
      "Step: 14690, train/epoch: 3.4959542751312256\n",
      "Step: 14700, train/loss: 0.13439999520778656\n",
      "Step: 14700, train/grad_norm: 0.00011585956235649064\n",
      "Step: 14700, train/learning_rate: 3.2508331059943885e-05\n",
      "Step: 14700, train/epoch: 3.4983341693878174\n",
      "Step: 14710, train/loss: 0.14219999313354492\n",
      "Step: 14710, train/grad_norm: 0.025173522531986237\n",
      "Step: 14710, train/learning_rate: 3.249643123126589e-05\n",
      "Step: 14710, train/epoch: 3.500714063644409\n",
      "Step: 14720, train/loss: 0.0\n",
      "Step: 14720, train/grad_norm: 0.008168303407728672\n",
      "Step: 14720, train/learning_rate: 3.248453140258789e-05\n",
      "Step: 14720, train/epoch: 3.503093719482422\n",
      "Step: 14730, train/loss: 0.0\n",
      "Step: 14730, train/grad_norm: 0.01008506491780281\n",
      "Step: 14730, train/learning_rate: 3.2472631573909894e-05\n",
      "Step: 14730, train/epoch: 3.5054736137390137\n",
      "Step: 14740, train/loss: 0.0007999999797903001\n",
      "Step: 14740, train/grad_norm: 0.03003607876598835\n",
      "Step: 14740, train/learning_rate: 3.24607317452319e-05\n",
      "Step: 14740, train/epoch: 3.5078535079956055\n",
      "Step: 14750, train/loss: 0.005400000140070915\n",
      "Step: 14750, train/grad_norm: 0.010767213068902493\n",
      "Step: 14750, train/learning_rate: 3.244883555453271e-05\n",
      "Step: 14750, train/epoch: 3.510233163833618\n",
      "Step: 14760, train/loss: 0.0\n",
      "Step: 14760, train/grad_norm: 0.0008649714291095734\n",
      "Step: 14760, train/learning_rate: 3.243693572585471e-05\n",
      "Step: 14760, train/epoch: 3.51261305809021\n",
      "Step: 14770, train/loss: 0.0\n",
      "Step: 14770, train/grad_norm: 0.0011779319029301405\n",
      "Step: 14770, train/learning_rate: 3.242503589717671e-05\n",
      "Step: 14770, train/epoch: 3.5149929523468018\n",
      "Step: 14780, train/loss: 0.0\n",
      "Step: 14780, train/grad_norm: 0.0013241126434877515\n",
      "Step: 14780, train/learning_rate: 3.2413136068498716e-05\n",
      "Step: 14780, train/epoch: 3.5173726081848145\n",
      "Step: 14790, train/loss: 0.0\n",
      "Step: 14790, train/grad_norm: 0.00018139359599445015\n",
      "Step: 14790, train/learning_rate: 3.240123623982072e-05\n",
      "Step: 14790, train/epoch: 3.5197525024414062\n",
      "Step: 14800, train/loss: 0.11089999973773956\n",
      "Step: 14800, train/grad_norm: 94.26274108886719\n",
      "Step: 14800, train/learning_rate: 3.238934004912153e-05\n",
      "Step: 14800, train/epoch: 3.522132396697998\n",
      "Step: 14810, train/loss: 0.0003000000142492354\n",
      "Step: 14810, train/grad_norm: 0.0009745007264427841\n",
      "Step: 14810, train/learning_rate: 3.237744022044353e-05\n",
      "Step: 14810, train/epoch: 3.5245120525360107\n",
      "Step: 14820, train/loss: 0.0\n",
      "Step: 14820, train/grad_norm: 0.0026815433520823717\n",
      "Step: 14820, train/learning_rate: 3.2365540391765535e-05\n",
      "Step: 14820, train/epoch: 3.5268919467926025\n",
      "Step: 14830, train/loss: 0.0\n",
      "Step: 14830, train/grad_norm: 0.00395573303103447\n",
      "Step: 14830, train/learning_rate: 3.235364056308754e-05\n",
      "Step: 14830, train/epoch: 3.5292718410491943\n",
      "Step: 14840, train/loss: 0.0\n",
      "Step: 14840, train/grad_norm: 0.0004288930504117161\n",
      "Step: 14840, train/learning_rate: 3.234174073440954e-05\n",
      "Step: 14840, train/epoch: 3.531651496887207\n",
      "Step: 14850, train/loss: 0.0\n",
      "Step: 14850, train/grad_norm: 0.0270786602050066\n",
      "Step: 14850, train/learning_rate: 3.232984454371035e-05\n",
      "Step: 14850, train/epoch: 3.534031391143799\n",
      "Step: 14860, train/loss: 0.0\n",
      "Step: 14860, train/grad_norm: 0.0005708459066227078\n",
      "Step: 14860, train/learning_rate: 3.2317944715032354e-05\n",
      "Step: 14860, train/epoch: 3.5364112854003906\n",
      "Step: 14870, train/loss: 0.0\n",
      "Step: 14870, train/grad_norm: 0.002440531738102436\n",
      "Step: 14870, train/learning_rate: 3.230604488635436e-05\n",
      "Step: 14870, train/epoch: 3.5387909412384033\n",
      "Step: 14880, train/loss: 0.0\n",
      "Step: 14880, train/grad_norm: 0.0006885316106490791\n",
      "Step: 14880, train/learning_rate: 3.229414505767636e-05\n",
      "Step: 14880, train/epoch: 3.541170835494995\n",
      "Step: 14890, train/loss: 0.0\n",
      "Step: 14890, train/grad_norm: 0.0005985083989799023\n",
      "Step: 14890, train/learning_rate: 3.228224522899836e-05\n",
      "Step: 14890, train/epoch: 3.543550729751587\n",
      "Step: 14900, train/loss: 0.0\n",
      "Step: 14900, train/grad_norm: 0.00029987739981152117\n",
      "Step: 14900, train/learning_rate: 3.227034903829917e-05\n",
      "Step: 14900, train/epoch: 3.5459306240081787\n",
      "Step: 14910, train/loss: 0.0\n",
      "Step: 14910, train/grad_norm: 0.0004178550443612039\n",
      "Step: 14910, train/learning_rate: 3.2258449209621176e-05\n",
      "Step: 14910, train/epoch: 3.5483102798461914\n",
      "Step: 14920, train/loss: 0.0\n",
      "Step: 14920, train/grad_norm: 0.00030513969250023365\n",
      "Step: 14920, train/learning_rate: 3.224654938094318e-05\n",
      "Step: 14920, train/epoch: 3.550690174102783\n",
      "Step: 14930, train/loss: 0.0\n",
      "Step: 14930, train/grad_norm: 0.001447677263058722\n",
      "Step: 14930, train/learning_rate: 3.223464955226518e-05\n",
      "Step: 14930, train/epoch: 3.553070068359375\n",
      "Step: 14940, train/loss: 0.0\n",
      "Step: 14940, train/grad_norm: 0.000662043341435492\n",
      "Step: 14940, train/learning_rate: 3.2222749723587185e-05\n",
      "Step: 14940, train/epoch: 3.5554497241973877\n",
      "Step: 14950, train/loss: 0.0934000015258789\n",
      "Step: 14950, train/grad_norm: 0.00016607267025392503\n",
      "Step: 14950, train/learning_rate: 3.2210853532887995e-05\n",
      "Step: 14950, train/epoch: 3.5578296184539795\n",
      "Step: 14960, train/loss: 0.0\n",
      "Step: 14960, train/grad_norm: 0.0010945540852844715\n",
      "Step: 14960, train/learning_rate: 3.219895370421e-05\n",
      "Step: 14960, train/epoch: 3.5602095127105713\n",
      "Step: 14970, train/loss: 0.0\n",
      "Step: 14970, train/grad_norm: 0.018574541434645653\n",
      "Step: 14970, train/learning_rate: 3.2187053875532e-05\n",
      "Step: 14970, train/epoch: 3.562589168548584\n",
      "Step: 14980, train/loss: 0.0\n",
      "Step: 14980, train/grad_norm: 0.0002738239709287882\n",
      "Step: 14980, train/learning_rate: 3.2175154046854004e-05\n",
      "Step: 14980, train/epoch: 3.564969062805176\n",
      "Step: 14990, train/loss: 0.0\n",
      "Step: 14990, train/grad_norm: 0.0004981897072866559\n",
      "Step: 14990, train/learning_rate: 3.216325421817601e-05\n",
      "Step: 14990, train/epoch: 3.5673489570617676\n",
      "Step: 15000, train/loss: 0.0\n",
      "Step: 15000, train/grad_norm: 0.00041332998080179095\n",
      "Step: 15000, train/learning_rate: 3.215135802747682e-05\n",
      "Step: 15000, train/epoch: 3.5697286128997803\n",
      "Step: 15010, train/loss: 0.0\n",
      "Step: 15010, train/grad_norm: 0.00041655689710751176\n",
      "Step: 15010, train/learning_rate: 3.213945819879882e-05\n",
      "Step: 15010, train/epoch: 3.572108507156372\n",
      "Step: 15020, train/loss: 0.2337000072002411\n",
      "Step: 15020, train/grad_norm: 0.24414339661598206\n",
      "Step: 15020, train/learning_rate: 3.212755837012082e-05\n",
      "Step: 15020, train/epoch: 3.574488401412964\n",
      "Step: 15030, train/loss: 9.999999747378752e-05\n",
      "Step: 15030, train/grad_norm: 0.032350968569517136\n",
      "Step: 15030, train/learning_rate: 3.2115658541442826e-05\n",
      "Step: 15030, train/epoch: 3.5768680572509766\n",
      "Step: 15040, train/loss: 9.999999747378752e-05\n",
      "Step: 15040, train/grad_norm: 0.0059746792539954185\n",
      "Step: 15040, train/learning_rate: 3.210375871276483e-05\n",
      "Step: 15040, train/epoch: 3.5792479515075684\n",
      "Step: 15050, train/loss: 0.0\n",
      "Step: 15050, train/grad_norm: 0.002797600580379367\n",
      "Step: 15050, train/learning_rate: 3.209186252206564e-05\n",
      "Step: 15050, train/epoch: 3.58162784576416\n",
      "Step: 15060, train/loss: 0.0\n",
      "Step: 15060, train/grad_norm: 0.0003466616617515683\n",
      "Step: 15060, train/learning_rate: 3.207996269338764e-05\n",
      "Step: 15060, train/epoch: 3.584007501602173\n",
      "Step: 15070, train/loss: 0.0\n",
      "Step: 15070, train/grad_norm: 9.929071529768407e-05\n",
      "Step: 15070, train/learning_rate: 3.2068062864709646e-05\n",
      "Step: 15070, train/epoch: 3.5863873958587646\n",
      "Step: 15080, train/loss: 0.0\n",
      "Step: 15080, train/grad_norm: 0.00012980845349375159\n",
      "Step: 15080, train/learning_rate: 3.205616303603165e-05\n",
      "Step: 15080, train/epoch: 3.5887672901153564\n",
      "Step: 15090, train/loss: 0.0\n",
      "Step: 15090, train/grad_norm: 9.562006744090468e-05\n",
      "Step: 15090, train/learning_rate: 3.204426320735365e-05\n",
      "Step: 15090, train/epoch: 3.5911471843719482\n",
      "Step: 15100, train/loss: 0.0\n",
      "Step: 15100, train/grad_norm: 2.986973231600132e-05\n",
      "Step: 15100, train/learning_rate: 3.203236701665446e-05\n",
      "Step: 15100, train/epoch: 3.593526840209961\n",
      "Step: 15110, train/loss: 0.0\n",
      "Step: 15110, train/grad_norm: 0.00012208722182549536\n",
      "Step: 15110, train/learning_rate: 3.2020467187976465e-05\n",
      "Step: 15110, train/epoch: 3.5959067344665527\n",
      "Step: 15120, train/loss: 0.0\n",
      "Step: 15120, train/grad_norm: 0.0003684542025439441\n",
      "Step: 15120, train/learning_rate: 3.200856735929847e-05\n",
      "Step: 15120, train/epoch: 3.5982866287231445\n",
      "Step: 15130, train/loss: 0.24940000474452972\n",
      "Step: 15130, train/grad_norm: 0.0007162215770222247\n",
      "Step: 15130, train/learning_rate: 3.199666753062047e-05\n",
      "Step: 15130, train/epoch: 3.6006662845611572\n",
      "Step: 15140, train/loss: 0.0\n",
      "Step: 15140, train/grad_norm: 0.010680007748305798\n",
      "Step: 15140, train/learning_rate: 3.1984767701942474e-05\n",
      "Step: 15140, train/epoch: 3.603046178817749\n",
      "Step: 15150, train/loss: 0.0\n",
      "Step: 15150, train/grad_norm: 0.0011957106180489063\n",
      "Step: 15150, train/learning_rate: 3.1972871511243284e-05\n",
      "Step: 15150, train/epoch: 3.605426073074341\n",
      "Step: 15160, train/loss: 0.0\n",
      "Step: 15160, train/grad_norm: 0.0019264372531324625\n",
      "Step: 15160, train/learning_rate: 3.196097168256529e-05\n",
      "Step: 15160, train/epoch: 3.6078057289123535\n",
      "Step: 15170, train/loss: 0.0\n",
      "Step: 15170, train/grad_norm: 0.0021385306026786566\n",
      "Step: 15170, train/learning_rate: 3.194907185388729e-05\n",
      "Step: 15170, train/epoch: 3.6101856231689453\n",
      "Step: 15180, train/loss: 0.013199999928474426\n",
      "Step: 15180, train/grad_norm: 0.0003906410711351782\n",
      "Step: 15180, train/learning_rate: 3.193717202520929e-05\n",
      "Step: 15180, train/epoch: 3.612565517425537\n",
      "Step: 15190, train/loss: 0.12030000239610672\n",
      "Step: 15190, train/grad_norm: 0.010984611697494984\n",
      "Step: 15190, train/learning_rate: 3.1925272196531296e-05\n",
      "Step: 15190, train/epoch: 3.61494517326355\n",
      "Step: 15200, train/loss: 0.0\n",
      "Step: 15200, train/grad_norm: 0.005831791087985039\n",
      "Step: 15200, train/learning_rate: 3.1913376005832106e-05\n",
      "Step: 15200, train/epoch: 3.6173250675201416\n",
      "Step: 15210, train/loss: 0.0\n",
      "Step: 15210, train/grad_norm: 0.004569271579384804\n",
      "Step: 15210, train/learning_rate: 3.190147617715411e-05\n",
      "Step: 15210, train/epoch: 3.6197049617767334\n",
      "Step: 15220, train/loss: 0.0\n",
      "Step: 15220, train/grad_norm: 0.004218373913317919\n",
      "Step: 15220, train/learning_rate: 3.188957634847611e-05\n",
      "Step: 15220, train/epoch: 3.622084617614746\n",
      "Step: 15230, train/loss: 0.0\n",
      "Step: 15230, train/grad_norm: 0.0036035256925970316\n",
      "Step: 15230, train/learning_rate: 3.1877676519798115e-05\n",
      "Step: 15230, train/epoch: 3.624464511871338\n",
      "Step: 15240, train/loss: 0.0\n",
      "Step: 15240, train/grad_norm: 0.0015809491742402315\n",
      "Step: 15240, train/learning_rate: 3.186577669112012e-05\n",
      "Step: 15240, train/epoch: 3.6268444061279297\n",
      "Step: 15250, train/loss: 0.0\n",
      "Step: 15250, train/grad_norm: 0.0012382033746689558\n",
      "Step: 15250, train/learning_rate: 3.185388050042093e-05\n",
      "Step: 15250, train/epoch: 3.6292240619659424\n",
      "Step: 15260, train/loss: 0.0\n",
      "Step: 15260, train/grad_norm: 0.0008755597518756986\n",
      "Step: 15260, train/learning_rate: 3.184198067174293e-05\n",
      "Step: 15260, train/epoch: 3.631603956222534\n",
      "Step: 15270, train/loss: 0.0\n",
      "Step: 15270, train/grad_norm: 0.0019454071298241615\n",
      "Step: 15270, train/learning_rate: 3.1830080843064934e-05\n",
      "Step: 15270, train/epoch: 3.633983850479126\n",
      "Step: 15280, train/loss: 0.0\n",
      "Step: 15280, train/grad_norm: 0.0005969423800706863\n",
      "Step: 15280, train/learning_rate: 3.181818101438694e-05\n",
      "Step: 15280, train/epoch: 3.6363637447357178\n",
      "Step: 15290, train/loss: 0.0\n",
      "Step: 15290, train/grad_norm: 0.0003763871791306883\n",
      "Step: 15290, train/learning_rate: 3.180628118570894e-05\n",
      "Step: 15290, train/epoch: 3.6387434005737305\n",
      "Step: 15300, train/loss: 0.06449999660253525\n",
      "Step: 15300, train/grad_norm: 0.0072756619192659855\n",
      "Step: 15300, train/learning_rate: 3.179438499500975e-05\n",
      "Step: 15300, train/epoch: 3.6411232948303223\n",
      "Step: 15310, train/loss: 0.049400001764297485\n",
      "Step: 15310, train/grad_norm: 0.002729637548327446\n",
      "Step: 15310, train/learning_rate: 3.178248516633175e-05\n",
      "Step: 15310, train/epoch: 3.643503189086914\n",
      "Step: 15320, train/loss: 0.0\n",
      "Step: 15320, train/grad_norm: 0.002586491173133254\n",
      "Step: 15320, train/learning_rate: 3.1770585337653756e-05\n",
      "Step: 15320, train/epoch: 3.6458828449249268\n",
      "Step: 15330, train/loss: 0.0\n",
      "Step: 15330, train/grad_norm: 0.0026512499898672104\n",
      "Step: 15330, train/learning_rate: 3.175868550897576e-05\n",
      "Step: 15330, train/epoch: 3.6482627391815186\n",
      "Step: 15340, train/loss: 0.0\n",
      "Step: 15340, train/grad_norm: 0.00032757496228441596\n",
      "Step: 15340, train/learning_rate: 3.174678568029776e-05\n",
      "Step: 15340, train/epoch: 3.6506426334381104\n",
      "Step: 15350, train/loss: 0.16290000081062317\n",
      "Step: 15350, train/grad_norm: 0.004841278772801161\n",
      "Step: 15350, train/learning_rate: 3.173488948959857e-05\n",
      "Step: 15350, train/epoch: 3.653022289276123\n",
      "Step: 15360, train/loss: 9.999999747378752e-05\n",
      "Step: 15360, train/grad_norm: 0.009210516698658466\n",
      "Step: 15360, train/learning_rate: 3.1722989660920575e-05\n",
      "Step: 15360, train/epoch: 3.655402183532715\n",
      "Step: 15370, train/loss: 0.0003000000142492354\n",
      "Step: 15370, train/grad_norm: 0.0021986891515552998\n",
      "Step: 15370, train/learning_rate: 3.171108983224258e-05\n",
      "Step: 15370, train/epoch: 3.6577820777893066\n",
      "Step: 15380, train/loss: 0.0\n",
      "Step: 15380, train/grad_norm: 0.000905150081962347\n",
      "Step: 15380, train/learning_rate: 3.169919000356458e-05\n",
      "Step: 15380, train/epoch: 3.6601617336273193\n",
      "Step: 15390, train/loss: 0.0\n",
      "Step: 15390, train/grad_norm: 0.0007837182492949069\n",
      "Step: 15390, train/learning_rate: 3.1687290174886584e-05\n",
      "Step: 15390, train/epoch: 3.662541627883911\n",
      "Step: 15400, train/loss: 0.2176000028848648\n",
      "Step: 15400, train/grad_norm: 0.0018894470995292068\n",
      "Step: 15400, train/learning_rate: 3.1675393984187394e-05\n",
      "Step: 15400, train/epoch: 3.664921522140503\n",
      "Step: 15410, train/loss: 9.999999747378752e-05\n",
      "Step: 15410, train/grad_norm: 0.18378405272960663\n",
      "Step: 15410, train/learning_rate: 3.16634941555094e-05\n",
      "Step: 15410, train/epoch: 3.6673011779785156\n",
      "Step: 15420, train/loss: 0.1281999945640564\n",
      "Step: 15420, train/grad_norm: 0.006491303909569979\n",
      "Step: 15420, train/learning_rate: 3.16515943268314e-05\n",
      "Step: 15420, train/epoch: 3.6696810722351074\n",
      "Step: 15430, train/loss: 9.999999747378752e-05\n",
      "Step: 15430, train/grad_norm: 0.053948432207107544\n",
      "Step: 15430, train/learning_rate: 3.1639694498153403e-05\n",
      "Step: 15430, train/epoch: 3.672060966491699\n",
      "Step: 15440, train/loss: 0.010900000110268593\n",
      "Step: 15440, train/grad_norm: 0.000975009344983846\n",
      "Step: 15440, train/learning_rate: 3.1627794669475406e-05\n",
      "Step: 15440, train/epoch: 3.674440860748291\n",
      "Step: 15450, train/loss: 0.0\n",
      "Step: 15450, train/grad_norm: 5.389611396822147e-05\n",
      "Step: 15450, train/learning_rate: 3.1615898478776217e-05\n",
      "Step: 15450, train/epoch: 3.6768205165863037\n",
      "Step: 15460, train/loss: 0.15230000019073486\n",
      "Step: 15460, train/grad_norm: 115.25747680664062\n",
      "Step: 15460, train/learning_rate: 3.160399865009822e-05\n",
      "Step: 15460, train/epoch: 3.6792004108428955\n",
      "Step: 15470, train/loss: 0.0\n",
      "Step: 15470, train/grad_norm: 0.0009084665798582137\n",
      "Step: 15470, train/learning_rate: 3.159209882142022e-05\n",
      "Step: 15470, train/epoch: 3.6815803050994873\n",
      "Step: 15480, train/loss: 0.11800000071525574\n",
      "Step: 15480, train/grad_norm: 77.32212829589844\n",
      "Step: 15480, train/learning_rate: 3.1580198992742226e-05\n",
      "Step: 15480, train/epoch: 3.6839599609375\n",
      "Step: 15490, train/loss: 9.999999747378752e-05\n",
      "Step: 15490, train/grad_norm: 0.09189353883266449\n",
      "Step: 15490, train/learning_rate: 3.156829916406423e-05\n",
      "Step: 15490, train/epoch: 3.686339855194092\n",
      "Step: 15500, train/loss: 0.13699999451637268\n",
      "Step: 15500, train/grad_norm: 0.1287275105714798\n",
      "Step: 15500, train/learning_rate: 3.155640297336504e-05\n",
      "Step: 15500, train/epoch: 3.6887197494506836\n",
      "Step: 15510, train/loss: 0.13619999587535858\n",
      "Step: 15510, train/grad_norm: 0.008870737627148628\n",
      "Step: 15510, train/learning_rate: 3.154450314468704e-05\n",
      "Step: 15510, train/epoch: 3.6910994052886963\n",
      "Step: 15520, train/loss: 9.999999747378752e-05\n",
      "Step: 15520, train/grad_norm: 0.0062423874624073505\n",
      "Step: 15520, train/learning_rate: 3.1532603316009045e-05\n",
      "Step: 15520, train/epoch: 3.693479299545288\n",
      "Step: 15530, train/loss: 0.0\n",
      "Step: 15530, train/grad_norm: 0.01031761895865202\n",
      "Step: 15530, train/learning_rate: 3.152070348733105e-05\n",
      "Step: 15530, train/epoch: 3.69585919380188\n",
      "Step: 15540, train/loss: 0.0\n",
      "Step: 15540, train/grad_norm: 0.009947448968887329\n",
      "Step: 15540, train/learning_rate: 3.150880365865305e-05\n",
      "Step: 15540, train/epoch: 3.6982388496398926\n",
      "Step: 15550, train/loss: 0.0\n",
      "Step: 15550, train/grad_norm: 0.007395007647573948\n",
      "Step: 15550, train/learning_rate: 3.149690746795386e-05\n",
      "Step: 15550, train/epoch: 3.7006187438964844\n",
      "Step: 15560, train/loss: 0.0\n",
      "Step: 15560, train/grad_norm: 0.0054767196998000145\n",
      "Step: 15560, train/learning_rate: 3.1485007639275864e-05\n",
      "Step: 15560, train/epoch: 3.702998638153076\n",
      "Step: 15570, train/loss: 0.0\n",
      "Step: 15570, train/grad_norm: 0.0032870578579604626\n",
      "Step: 15570, train/learning_rate: 3.147310781059787e-05\n",
      "Step: 15570, train/epoch: 3.705378293991089\n",
      "Step: 15580, train/loss: 0.0\n",
      "Step: 15580, train/grad_norm: 0.0019033369608223438\n",
      "Step: 15580, train/learning_rate: 3.146120798191987e-05\n",
      "Step: 15580, train/epoch: 3.7077581882476807\n",
      "Step: 15590, train/loss: 0.0\n",
      "Step: 15590, train/grad_norm: 0.04365653544664383\n",
      "Step: 15590, train/learning_rate: 3.144930815324187e-05\n",
      "Step: 15590, train/epoch: 3.7101380825042725\n",
      "Step: 15600, train/loss: 0.0\n",
      "Step: 15600, train/grad_norm: 0.0016817894065752625\n",
      "Step: 15600, train/learning_rate: 3.143741196254268e-05\n",
      "Step: 15600, train/epoch: 3.712517738342285\n",
      "Step: 15610, train/loss: 0.0\n",
      "Step: 15610, train/grad_norm: 0.001255636103451252\n",
      "Step: 15610, train/learning_rate: 3.1425512133864686e-05\n",
      "Step: 15610, train/epoch: 3.714897632598877\n",
      "Step: 15620, train/loss: 0.08320000022649765\n",
      "Step: 15620, train/grad_norm: 0.029287390410900116\n",
      "Step: 15620, train/learning_rate: 3.141361230518669e-05\n",
      "Step: 15620, train/epoch: 3.7172775268554688\n",
      "Step: 15630, train/loss: 0.14380000531673431\n",
      "Step: 15630, train/grad_norm: 77.35296630859375\n",
      "Step: 15630, train/learning_rate: 3.140171247650869e-05\n",
      "Step: 15630, train/epoch: 3.7196574211120605\n",
      "Step: 15640, train/loss: 0.009499999694526196\n",
      "Step: 15640, train/grad_norm: 0.007826062850654125\n",
      "Step: 15640, train/learning_rate: 3.1389812647830695e-05\n",
      "Step: 15640, train/epoch: 3.7220370769500732\n",
      "Step: 15650, train/loss: 9.999999747378752e-05\n",
      "Step: 15650, train/grad_norm: 0.011916452087461948\n",
      "Step: 15650, train/learning_rate: 3.1377916457131505e-05\n",
      "Step: 15650, train/epoch: 3.724416971206665\n",
      "Step: 15660, train/loss: 0.20880000293254852\n",
      "Step: 15660, train/grad_norm: 0.044124506413936615\n",
      "Step: 15660, train/learning_rate: 3.136601662845351e-05\n",
      "Step: 15660, train/epoch: 3.726796865463257\n",
      "Step: 15670, train/loss: 0.00019999999494757503\n",
      "Step: 15670, train/grad_norm: 0.01707620359957218\n",
      "Step: 15670, train/learning_rate: 3.135411679977551e-05\n",
      "Step: 15670, train/epoch: 3.7291765213012695\n",
      "Step: 15680, train/loss: 0.00860000029206276\n",
      "Step: 15680, train/grad_norm: 0.0005168917705304921\n",
      "Step: 15680, train/learning_rate: 3.1342216971097514e-05\n",
      "Step: 15680, train/epoch: 3.7315564155578613\n",
      "Step: 15690, train/loss: 0.0\n",
      "Step: 15690, train/grad_norm: 0.0001201359773403965\n",
      "Step: 15690, train/learning_rate: 3.133031714241952e-05\n",
      "Step: 15690, train/epoch: 3.733936309814453\n",
      "Step: 15700, train/loss: 0.11089999973773956\n",
      "Step: 15700, train/grad_norm: 0.0017164917662739754\n",
      "Step: 15700, train/learning_rate: 3.131842095172033e-05\n",
      "Step: 15700, train/epoch: 3.736315965652466\n",
      "Step: 15710, train/loss: 0.0\n",
      "Step: 15710, train/grad_norm: 0.0006958497688174248\n",
      "Step: 15710, train/learning_rate: 3.130652112304233e-05\n",
      "Step: 15710, train/epoch: 3.7386958599090576\n",
      "Step: 15720, train/loss: 0.0\n",
      "Step: 15720, train/grad_norm: 0.006447543855756521\n",
      "Step: 15720, train/learning_rate: 3.129462129436433e-05\n",
      "Step: 15720, train/epoch: 3.7410757541656494\n",
      "Step: 15730, train/loss: 0.0\n",
      "Step: 15730, train/grad_norm: 0.0014960166299715638\n",
      "Step: 15730, train/learning_rate: 3.1282721465686336e-05\n",
      "Step: 15730, train/epoch: 3.743455410003662\n",
      "Step: 15740, train/loss: 0.0\n",
      "Step: 15740, train/grad_norm: 0.0005136589170433581\n",
      "Step: 15740, train/learning_rate: 3.127082163700834e-05\n",
      "Step: 15740, train/epoch: 3.745835304260254\n",
      "Step: 15750, train/loss: 0.11640000343322754\n",
      "Step: 15750, train/grad_norm: 0.001797481789253652\n",
      "Step: 15750, train/learning_rate: 3.125892544630915e-05\n",
      "Step: 15750, train/epoch: 3.7482151985168457\n",
      "Step: 15760, train/loss: 0.10849999636411667\n",
      "Step: 15760, train/grad_norm: 0.018800048157572746\n",
      "Step: 15760, train/learning_rate: 3.124702561763115e-05\n",
      "Step: 15760, train/epoch: 3.7505948543548584\n",
      "Step: 15770, train/loss: 0.00019999999494757503\n",
      "Step: 15770, train/grad_norm: 0.013835717923939228\n",
      "Step: 15770, train/learning_rate: 3.1235125788953155e-05\n",
      "Step: 15770, train/epoch: 3.75297474861145\n",
      "Step: 15780, train/loss: 0.14300000667572021\n",
      "Step: 15780, train/grad_norm: 0.006211312022060156\n",
      "Step: 15780, train/learning_rate: 3.122322596027516e-05\n",
      "Step: 15780, train/epoch: 3.755354642868042\n",
      "Step: 15790, train/loss: 0.07069999724626541\n",
      "Step: 15790, train/grad_norm: 0.06375836580991745\n",
      "Step: 15790, train/learning_rate: 3.121132613159716e-05\n",
      "Step: 15790, train/epoch: 3.7577342987060547\n",
      "Step: 15800, train/loss: 0.00019999999494757503\n",
      "Step: 15800, train/grad_norm: 0.03574392944574356\n",
      "Step: 15800, train/learning_rate: 3.119942994089797e-05\n",
      "Step: 15800, train/epoch: 3.7601141929626465\n",
      "Step: 15810, train/loss: 0.00019999999494757503\n",
      "Step: 15810, train/grad_norm: 0.012120570056140423\n",
      "Step: 15810, train/learning_rate: 3.1187530112219974e-05\n",
      "Step: 15810, train/epoch: 3.7624940872192383\n",
      "Step: 15820, train/loss: 0.0\n",
      "Step: 15820, train/grad_norm: 0.0005968277691863477\n",
      "Step: 15820, train/learning_rate: 3.117563028354198e-05\n",
      "Step: 15820, train/epoch: 3.76487398147583\n",
      "Step: 15830, train/loss: 0.0\n",
      "Step: 15830, train/grad_norm: 0.0011346747633069754\n",
      "Step: 15830, train/learning_rate: 3.116373045486398e-05\n",
      "Step: 15830, train/epoch: 3.7672536373138428\n",
      "Step: 15840, train/loss: 0.0\n",
      "Step: 15840, train/grad_norm: 0.0002578219573479146\n",
      "Step: 15840, train/learning_rate: 3.115183426416479e-05\n",
      "Step: 15840, train/epoch: 3.7696335315704346\n",
      "Step: 15850, train/loss: 0.09769999980926514\n",
      "Step: 15850, train/grad_norm: 0.0002706622763071209\n",
      "Step: 15850, train/learning_rate: 3.1139934435486794e-05\n",
      "Step: 15850, train/epoch: 3.7720134258270264\n",
      "Step: 15860, train/loss: 0.0\n",
      "Step: 15860, train/grad_norm: 0.0008370328578166664\n",
      "Step: 15860, train/learning_rate: 3.1128034606808797e-05\n",
      "Step: 15860, train/epoch: 3.774393081665039\n",
      "Step: 15870, train/loss: 0.0\n",
      "Step: 15870, train/grad_norm: 0.008045184426009655\n",
      "Step: 15870, train/learning_rate: 3.11161347781308e-05\n",
      "Step: 15870, train/epoch: 3.776772975921631\n",
      "Step: 15880, train/loss: 0.0843999981880188\n",
      "Step: 15880, train/grad_norm: 0.0010579206282272935\n",
      "Step: 15880, train/learning_rate: 3.11042349494528e-05\n",
      "Step: 15880, train/epoch: 3.7791528701782227\n",
      "Step: 15890, train/loss: 0.08399999886751175\n",
      "Step: 15890, train/grad_norm: 0.19659563899040222\n",
      "Step: 15890, train/learning_rate: 3.109233875875361e-05\n",
      "Step: 15890, train/epoch: 3.7815325260162354\n",
      "Step: 15900, train/loss: 9.999999747378752e-05\n",
      "Step: 15900, train/grad_norm: 0.041595425456762314\n",
      "Step: 15900, train/learning_rate: 3.1080438930075616e-05\n",
      "Step: 15900, train/epoch: 3.783912420272827\n",
      "Step: 15910, train/loss: 9.999999747378752e-05\n",
      "Step: 15910, train/grad_norm: 0.2212478071451187\n",
      "Step: 15910, train/learning_rate: 3.106853910139762e-05\n",
      "Step: 15910, train/epoch: 3.786292314529419\n",
      "Step: 15920, train/loss: 0.12030000239610672\n",
      "Step: 15920, train/grad_norm: 0.007218663115054369\n",
      "Step: 15920, train/learning_rate: 3.105663927271962e-05\n",
      "Step: 15920, train/epoch: 3.7886719703674316\n",
      "Step: 15930, train/loss: 9.999999747378752e-05\n",
      "Step: 15930, train/grad_norm: 0.024509795010089874\n",
      "Step: 15930, train/learning_rate: 3.1044739444041625e-05\n",
      "Step: 15930, train/epoch: 3.7910518646240234\n",
      "Step: 15940, train/loss: 0.01979999989271164\n",
      "Step: 15940, train/grad_norm: 0.006174003705382347\n",
      "Step: 15940, train/learning_rate: 3.1032843253342435e-05\n",
      "Step: 15940, train/epoch: 3.7934317588806152\n",
      "Step: 15950, train/loss: 0.0\n",
      "Step: 15950, train/grad_norm: 0.005902113858610392\n",
      "Step: 15950, train/learning_rate: 3.102094342466444e-05\n",
      "Step: 15950, train/epoch: 3.795811414718628\n",
      "Step: 15960, train/loss: 0.0\n",
      "Step: 15960, train/grad_norm: 0.004438660107553005\n",
      "Step: 15960, train/learning_rate: 3.100904359598644e-05\n",
      "Step: 15960, train/epoch: 3.7981913089752197\n",
      "Step: 15970, train/loss: 0.0\n",
      "Step: 15970, train/grad_norm: 0.0028338294941931963\n",
      "Step: 15970, train/learning_rate: 3.0997143767308444e-05\n",
      "Step: 15970, train/epoch: 3.8005712032318115\n",
      "Step: 15980, train/loss: 0.0\n",
      "Step: 15980, train/grad_norm: 0.002356742974370718\n",
      "Step: 15980, train/learning_rate: 3.098524393863045e-05\n",
      "Step: 15980, train/epoch: 3.802950859069824\n",
      "Step: 15990, train/loss: 0.0\n",
      "Step: 15990, train/grad_norm: 0.061727482825517654\n",
      "Step: 15990, train/learning_rate: 3.097334774793126e-05\n",
      "Step: 15990, train/epoch: 3.805330753326416\n",
      "Step: 16000, train/loss: 0.0\n",
      "Step: 16000, train/grad_norm: 0.0006068358197808266\n",
      "Step: 16000, train/learning_rate: 3.096144791925326e-05\n",
      "Step: 16000, train/epoch: 3.807710647583008\n",
      "Step: 16010, train/loss: 0.0\n",
      "Step: 16010, train/grad_norm: 0.000535063270945102\n",
      "Step: 16010, train/learning_rate: 3.094954809057526e-05\n",
      "Step: 16010, train/epoch: 3.8100905418395996\n",
      "Step: 16020, train/loss: 0.0\n",
      "Step: 16020, train/grad_norm: 0.0009227258851751685\n",
      "Step: 16020, train/learning_rate: 3.0937648261897266e-05\n",
      "Step: 16020, train/epoch: 3.8124701976776123\n",
      "Step: 16030, train/loss: 0.0\n",
      "Step: 16030, train/grad_norm: 0.0003469064540695399\n",
      "Step: 16030, train/learning_rate: 3.092574843321927e-05\n",
      "Step: 16030, train/epoch: 3.814850091934204\n",
      "Step: 16040, train/loss: 0.07769999653100967\n",
      "Step: 16040, train/grad_norm: 0.0014509651809930801\n",
      "Step: 16040, train/learning_rate: 3.091385224252008e-05\n",
      "Step: 16040, train/epoch: 3.817229986190796\n",
      "Step: 16050, train/loss: 0.0\n",
      "Step: 16050, train/grad_norm: 0.007515801582485437\n",
      "Step: 16050, train/learning_rate: 3.090195241384208e-05\n",
      "Step: 16050, train/epoch: 3.8196096420288086\n",
      "Step: 16060, train/loss: 0.0\n",
      "Step: 16060, train/grad_norm: 0.005484830122441053\n",
      "Step: 16060, train/learning_rate: 3.0890052585164085e-05\n",
      "Step: 16060, train/epoch: 3.8219895362854004\n",
      "Step: 16070, train/loss: 0.0\n",
      "Step: 16070, train/grad_norm: 0.0032185015734285116\n",
      "Step: 16070, train/learning_rate: 3.087815275648609e-05\n",
      "Step: 16070, train/epoch: 3.824369430541992\n",
      "Step: 16080, train/loss: 0.0\n",
      "Step: 16080, train/grad_norm: 0.0016485077794641256\n",
      "Step: 16080, train/learning_rate: 3.086625292780809e-05\n",
      "Step: 16080, train/epoch: 3.826749086380005\n",
      "Step: 16090, train/loss: 0.1136000007390976\n",
      "Step: 16090, train/grad_norm: 0.000305352354189381\n",
      "Step: 16090, train/learning_rate: 3.08543567371089e-05\n",
      "Step: 16090, train/epoch: 3.8291289806365967\n",
      "Step: 16100, train/loss: 0.17270000278949738\n",
      "Step: 16100, train/grad_norm: 0.00013175533968023956\n",
      "Step: 16100, train/learning_rate: 3.0842456908430904e-05\n",
      "Step: 16100, train/epoch: 3.8315088748931885\n",
      "Step: 16110, train/loss: 0.056699998676776886\n",
      "Step: 16110, train/grad_norm: 0.004254888277500868\n",
      "Step: 16110, train/learning_rate: 3.083055707975291e-05\n",
      "Step: 16110, train/epoch: 3.833888530731201\n",
      "Step: 16120, train/loss: 0.0\n",
      "Step: 16120, train/grad_norm: 0.007258756086230278\n",
      "Step: 16120, train/learning_rate: 3.081865725107491e-05\n",
      "Step: 16120, train/epoch: 3.836268424987793\n",
      "Step: 16130, train/loss: 0.027400000020861626\n",
      "Step: 16130, train/grad_norm: 0.03552016243338585\n",
      "Step: 16130, train/learning_rate: 3.080675742239691e-05\n",
      "Step: 16130, train/epoch: 3.8386483192443848\n",
      "Step: 16140, train/loss: 9.999999747378752e-05\n",
      "Step: 16140, train/grad_norm: 0.011538030579686165\n",
      "Step: 16140, train/learning_rate: 3.079486123169772e-05\n",
      "Step: 16140, train/epoch: 3.8410279750823975\n",
      "Step: 16150, train/loss: 0.0\n",
      "Step: 16150, train/grad_norm: 0.007425054907798767\n",
      "Step: 16150, train/learning_rate: 3.0782961403019726e-05\n",
      "Step: 16150, train/epoch: 3.8434078693389893\n",
      "Step: 16160, train/loss: 0.0\n",
      "Step: 16160, train/grad_norm: 0.0007207884336821735\n",
      "Step: 16160, train/learning_rate: 3.077106157434173e-05\n",
      "Step: 16160, train/epoch: 3.845787763595581\n",
      "Step: 16170, train/loss: 0.21209999918937683\n",
      "Step: 16170, train/grad_norm: 0.0010158821241930127\n",
      "Step: 16170, train/learning_rate: 3.075916174566373e-05\n",
      "Step: 16170, train/epoch: 3.848167657852173\n",
      "Step: 16180, train/loss: 0.032499998807907104\n",
      "Step: 16180, train/grad_norm: 0.11289412528276443\n",
      "Step: 16180, train/learning_rate: 3.0747261916985735e-05\n",
      "Step: 16180, train/epoch: 3.8505473136901855\n",
      "Step: 16190, train/loss: 9.999999747378752e-05\n",
      "Step: 16190, train/grad_norm: 0.010924804024398327\n",
      "Step: 16190, train/learning_rate: 3.0735365726286545e-05\n",
      "Step: 16190, train/epoch: 3.8529272079467773\n",
      "Step: 16200, train/loss: 9.999999747378752e-05\n",
      "Step: 16200, train/grad_norm: 0.0018781741382554173\n",
      "Step: 16200, train/learning_rate: 3.072346589760855e-05\n",
      "Step: 16200, train/epoch: 3.855307102203369\n",
      "Step: 16210, train/loss: 0.0\n",
      "Step: 16210, train/grad_norm: 0.0017420094227418303\n",
      "Step: 16210, train/learning_rate: 3.071156606893055e-05\n",
      "Step: 16210, train/epoch: 3.857686758041382\n",
      "Step: 16220, train/loss: 0.0\n",
      "Step: 16220, train/grad_norm: 0.004815662279725075\n",
      "Step: 16220, train/learning_rate: 3.0699666240252554e-05\n",
      "Step: 16220, train/epoch: 3.8600666522979736\n",
      "Step: 16230, train/loss: 0.0\n",
      "Step: 16230, train/grad_norm: 0.0014821654185652733\n",
      "Step: 16230, train/learning_rate: 3.068776641157456e-05\n",
      "Step: 16230, train/epoch: 3.8624465465545654\n",
      "Step: 16240, train/loss: 0.0\n",
      "Step: 16240, train/grad_norm: 0.002588791772723198\n",
      "Step: 16240, train/learning_rate: 3.067587022087537e-05\n",
      "Step: 16240, train/epoch: 3.864826202392578\n",
      "Step: 16250, train/loss: 0.0010999999940395355\n",
      "Step: 16250, train/grad_norm: 0.0006330786272883415\n",
      "Step: 16250, train/learning_rate: 3.066397039219737e-05\n",
      "Step: 16250, train/epoch: 3.86720609664917\n",
      "Step: 16260, train/loss: 0.0\n",
      "Step: 16260, train/grad_norm: 0.00015432787768077105\n",
      "Step: 16260, train/learning_rate: 3.0652070563519374e-05\n",
      "Step: 16260, train/epoch: 3.8695859909057617\n",
      "Step: 16270, train/loss: 0.05389999970793724\n",
      "Step: 16270, train/grad_norm: 0.001513305469416082\n",
      "Step: 16270, train/learning_rate: 3.0640170734841377e-05\n",
      "Step: 16270, train/epoch: 3.8719656467437744\n",
      "Step: 16280, train/loss: 0.0\n",
      "Step: 16280, train/grad_norm: 0.0002831449673976749\n",
      "Step: 16280, train/learning_rate: 3.062827090616338e-05\n",
      "Step: 16280, train/epoch: 3.874345541000366\n",
      "Step: 16290, train/loss: 0.0\n",
      "Step: 16290, train/grad_norm: 0.0001817655429476872\n",
      "Step: 16290, train/learning_rate: 3.061637471546419e-05\n",
      "Step: 16290, train/epoch: 3.876725435256958\n",
      "Step: 16300, train/loss: 0.0\n",
      "Step: 16300, train/grad_norm: 0.00028285960434004664\n",
      "Step: 16300, train/learning_rate: 3.060447488678619e-05\n",
      "Step: 16300, train/epoch: 3.8791050910949707\n",
      "Step: 16310, train/loss: 0.0\n",
      "Step: 16310, train/grad_norm: 0.0004469848645385355\n",
      "Step: 16310, train/learning_rate: 3.0592575058108196e-05\n",
      "Step: 16310, train/epoch: 3.8814849853515625\n",
      "Step: 16320, train/loss: 0.0\n",
      "Step: 16320, train/grad_norm: 0.0008332124562002718\n",
      "Step: 16320, train/learning_rate: 3.05806752294302e-05\n",
      "Step: 16320, train/epoch: 3.8838648796081543\n",
      "Step: 16330, train/loss: 0.0\n",
      "Step: 16330, train/grad_norm: 0.0008224238990806043\n",
      "Step: 16330, train/learning_rate: 3.05687754007522e-05\n",
      "Step: 16330, train/epoch: 3.886244535446167\n",
      "Step: 16340, train/loss: 0.0\n",
      "Step: 16340, train/grad_norm: 0.00024644582299515605\n",
      "Step: 16340, train/learning_rate: 3.055687921005301e-05\n",
      "Step: 16340, train/epoch: 3.888624429702759\n",
      "Step: 16350, train/loss: 0.0\n",
      "Step: 16350, train/grad_norm: 0.0004467031394597143\n",
      "Step: 16350, train/learning_rate: 3.0544979381375015e-05\n",
      "Step: 16350, train/epoch: 3.8910043239593506\n",
      "Step: 16360, train/loss: 0.0\n",
      "Step: 16360, train/grad_norm: 0.0003213215677533299\n",
      "Step: 16360, train/learning_rate: 3.053307955269702e-05\n",
      "Step: 16360, train/epoch: 3.8933842182159424\n",
      "Step: 16370, train/loss: 0.0\n",
      "Step: 16370, train/grad_norm: 0.00036707890103571117\n",
      "Step: 16370, train/learning_rate: 3.052117972401902e-05\n",
      "Step: 16370, train/epoch: 3.895763874053955\n",
      "Step: 16380, train/loss: 0.0\n",
      "Step: 16380, train/grad_norm: 0.00033212918788194656\n",
      "Step: 16380, train/learning_rate: 3.0509281714330427e-05\n",
      "Step: 16380, train/epoch: 3.898143768310547\n",
      "Step: 16390, train/loss: 0.0\n",
      "Step: 16390, train/grad_norm: 0.0007122267852537334\n",
      "Step: 16390, train/learning_rate: 3.049738188565243e-05\n",
      "Step: 16390, train/epoch: 3.9005236625671387\n",
      "Step: 16400, train/loss: 0.0\n",
      "Step: 16400, train/grad_norm: 0.009745527990162373\n",
      "Step: 16400, train/learning_rate: 3.0485483875963837e-05\n",
      "Step: 16400, train/epoch: 3.9029033184051514\n",
      "Step: 16410, train/loss: 0.00039999998989515007\n",
      "Step: 16410, train/grad_norm: 0.006158279720693827\n",
      "Step: 16410, train/learning_rate: 3.047358404728584e-05\n",
      "Step: 16410, train/epoch: 3.905283212661743\n",
      "Step: 16420, train/loss: 0.0\n",
      "Step: 16420, train/grad_norm: 2.9892455131630413e-05\n",
      "Step: 16420, train/learning_rate: 3.0461684218607843e-05\n",
      "Step: 16420, train/epoch: 3.907663106918335\n",
      "Step: 16430, train/loss: 0.0\n",
      "Step: 16430, train/grad_norm: 2.8814531106036156e-05\n",
      "Step: 16430, train/learning_rate: 3.044978620891925e-05\n",
      "Step: 16430, train/epoch: 3.9100427627563477\n",
      "Step: 16440, train/loss: 0.0\n",
      "Step: 16440, train/grad_norm: 0.00018260722572449595\n",
      "Step: 16440, train/learning_rate: 3.0437886380241252e-05\n",
      "Step: 16440, train/epoch: 3.9124226570129395\n",
      "Step: 16450, train/loss: 0.0\n",
      "Step: 16450, train/grad_norm: 1.7254995327675715e-05\n",
      "Step: 16450, train/learning_rate: 3.042598837055266e-05\n",
      "Step: 16450, train/epoch: 3.9148025512695312\n",
      "Step: 16460, train/loss: 0.0\n",
      "Step: 16460, train/grad_norm: 5.022026380174793e-05\n",
      "Step: 16460, train/learning_rate: 3.0414088541874662e-05\n",
      "Step: 16460, train/epoch: 3.917182207107544\n",
      "Step: 16470, train/loss: 0.0\n",
      "Step: 16470, train/grad_norm: 6.671442679362372e-05\n",
      "Step: 16470, train/learning_rate: 3.0402188713196665e-05\n",
      "Step: 16470, train/epoch: 3.9195621013641357\n",
      "Step: 16480, train/loss: 0.0\n",
      "Step: 16480, train/grad_norm: 7.058308983687311e-05\n",
      "Step: 16480, train/learning_rate: 3.039029070350807e-05\n",
      "Step: 16480, train/epoch: 3.9219419956207275\n",
      "Step: 16490, train/loss: 0.0\n",
      "Step: 16490, train/grad_norm: 5.027018778491765e-05\n",
      "Step: 16490, train/learning_rate: 3.0378390874830075e-05\n",
      "Step: 16490, train/epoch: 3.9243216514587402\n",
      "Step: 16500, train/loss: 0.0\n",
      "Step: 16500, train/grad_norm: 1.4151957657304592e-05\n",
      "Step: 16500, train/learning_rate: 3.036649286514148e-05\n",
      "Step: 16500, train/epoch: 3.926701545715332\n",
      "Step: 16510, train/loss: 0.0\n",
      "Step: 16510, train/grad_norm: 8.243800948548596e-06\n",
      "Step: 16510, train/learning_rate: 3.0354593036463484e-05\n",
      "Step: 16510, train/epoch: 3.929081439971924\n",
      "Step: 16520, train/loss: 0.0\n",
      "Step: 16520, train/grad_norm: 0.0005130259087309241\n",
      "Step: 16520, train/learning_rate: 3.0342693207785487e-05\n",
      "Step: 16520, train/epoch: 3.9314610958099365\n",
      "Step: 16530, train/loss: 0.0\n",
      "Step: 16530, train/grad_norm: 4.131679452257231e-05\n",
      "Step: 16530, train/learning_rate: 3.0330795198096894e-05\n",
      "Step: 16530, train/epoch: 3.9338409900665283\n",
      "Step: 16540, train/loss: 0.0\n",
      "Step: 16540, train/grad_norm: 1.5191702004813123e-05\n",
      "Step: 16540, train/learning_rate: 3.0318895369418897e-05\n",
      "Step: 16540, train/epoch: 3.93622088432312\n",
      "Step: 16550, train/loss: 0.0\n",
      "Step: 16550, train/grad_norm: 3.667768760351464e-05\n",
      "Step: 16550, train/learning_rate: 3.0306997359730303e-05\n",
      "Step: 16550, train/epoch: 3.938600778579712\n",
      "Step: 16560, train/loss: 0.0\n",
      "Step: 16560, train/grad_norm: 2.2369951693690382e-05\n",
      "Step: 16560, train/learning_rate: 3.0295097531052306e-05\n",
      "Step: 16560, train/epoch: 3.9409804344177246\n",
      "Step: 16570, train/loss: 0.0\n",
      "Step: 16570, train/grad_norm: 4.3539723264984787e-05\n",
      "Step: 16570, train/learning_rate: 3.028319770237431e-05\n",
      "Step: 16570, train/epoch: 3.9433603286743164\n",
      "Step: 16580, train/loss: 0.0\n",
      "Step: 16580, train/grad_norm: 1.9963594240834936e-05\n",
      "Step: 16580, train/learning_rate: 3.0271299692685716e-05\n",
      "Step: 16580, train/epoch: 3.945740222930908\n",
      "Step: 16590, train/loss: 0.0\n",
      "Step: 16590, train/grad_norm: 9.598126780474558e-05\n",
      "Step: 16590, train/learning_rate: 3.025939986400772e-05\n",
      "Step: 16590, train/epoch: 3.948119878768921\n",
      "Step: 16600, train/loss: 0.0\n",
      "Step: 16600, train/grad_norm: 0.00024527020286768675\n",
      "Step: 16600, train/learning_rate: 3.0247501854319125e-05\n",
      "Step: 16600, train/epoch: 3.9504997730255127\n",
      "Step: 16610, train/loss: 0.0\n",
      "Step: 16610, train/grad_norm: 6.1153341448516585e-06\n",
      "Step: 16610, train/learning_rate: 3.023560202564113e-05\n",
      "Step: 16610, train/epoch: 3.9528796672821045\n",
      "Step: 16620, train/loss: 0.0\n",
      "Step: 16620, train/grad_norm: 0.00013045071682427078\n",
      "Step: 16620, train/learning_rate: 3.022370219696313e-05\n",
      "Step: 16620, train/epoch: 3.955259323120117\n",
      "Step: 16630, train/loss: 0.18359999358654022\n",
      "Step: 16630, train/grad_norm: 0.0013430764665827155\n",
      "Step: 16630, train/learning_rate: 3.0211804187274538e-05\n",
      "Step: 16630, train/epoch: 3.957639217376709\n",
      "Step: 16640, train/loss: 0.0\n",
      "Step: 16640, train/grad_norm: 0.009333574213087559\n",
      "Step: 16640, train/learning_rate: 3.019990435859654e-05\n",
      "Step: 16640, train/epoch: 3.960019111633301\n",
      "Step: 16650, train/loss: 0.14219999313354492\n",
      "Step: 16650, train/grad_norm: 72.88641357421875\n",
      "Step: 16650, train/learning_rate: 3.0188006348907948e-05\n",
      "Step: 16650, train/epoch: 3.9623987674713135\n",
      "Step: 16660, train/loss: 9.999999747378752e-05\n",
      "Step: 16660, train/grad_norm: 0.04200480505824089\n",
      "Step: 16660, train/learning_rate: 3.017610652022995e-05\n",
      "Step: 16660, train/epoch: 3.9647786617279053\n",
      "Step: 16670, train/loss: 9.999999747378752e-05\n",
      "Step: 16670, train/grad_norm: 0.024181989952921867\n",
      "Step: 16670, train/learning_rate: 3.0164206691551954e-05\n",
      "Step: 16670, train/epoch: 3.967158555984497\n",
      "Step: 16680, train/loss: 9.999999747378752e-05\n",
      "Step: 16680, train/grad_norm: 0.00853309128433466\n",
      "Step: 16680, train/learning_rate: 3.015230868186336e-05\n",
      "Step: 16680, train/epoch: 3.9695382118225098\n",
      "Step: 16690, train/loss: 0.0\n",
      "Step: 16690, train/grad_norm: 0.0018420162377879024\n",
      "Step: 16690, train/learning_rate: 3.0140408853185363e-05\n",
      "Step: 16690, train/epoch: 3.9719181060791016\n",
      "Step: 16700, train/loss: 0.0\n",
      "Step: 16700, train/grad_norm: 0.0014194082468748093\n",
      "Step: 16700, train/learning_rate: 3.012851084349677e-05\n",
      "Step: 16700, train/epoch: 3.9742980003356934\n",
      "Step: 16710, train/loss: 0.0\n",
      "Step: 16710, train/grad_norm: 0.0009018549462780356\n",
      "Step: 16710, train/learning_rate: 3.0116611014818773e-05\n",
      "Step: 16710, train/epoch: 3.976677656173706\n",
      "Step: 16720, train/loss: 0.0\n",
      "Step: 16720, train/grad_norm: 0.0038442518562078476\n",
      "Step: 16720, train/learning_rate: 3.0104711186140776e-05\n",
      "Step: 16720, train/epoch: 3.979057550430298\n",
      "Step: 16730, train/loss: 0.0\n",
      "Step: 16730, train/grad_norm: 0.0012294795596972108\n",
      "Step: 16730, train/learning_rate: 3.0092813176452182e-05\n",
      "Step: 16730, train/epoch: 3.9814374446868896\n",
      "Step: 16740, train/loss: 0.0\n",
      "Step: 16740, train/grad_norm: 0.0024984295014292\n",
      "Step: 16740, train/learning_rate: 3.0080913347774185e-05\n",
      "Step: 16740, train/epoch: 3.9838173389434814\n",
      "Step: 16750, train/loss: 0.0\n",
      "Step: 16750, train/grad_norm: 0.0004503216187004\n",
      "Step: 16750, train/learning_rate: 3.0069015338085592e-05\n",
      "Step: 16750, train/epoch: 3.986196994781494\n",
      "Step: 16760, train/loss: 0.0\n",
      "Step: 16760, train/grad_norm: 0.0003312404442112893\n",
      "Step: 16760, train/learning_rate: 3.0057115509407595e-05\n",
      "Step: 16760, train/epoch: 3.988576889038086\n",
      "Step: 16770, train/loss: 0.0\n",
      "Step: 16770, train/grad_norm: 0.00023403819068334997\n",
      "Step: 16770, train/learning_rate: 3.0045215680729598e-05\n",
      "Step: 16770, train/epoch: 3.9909567832946777\n",
      "Step: 16780, train/loss: 0.0\n",
      "Step: 16780, train/grad_norm: 0.0009049986256286502\n",
      "Step: 16780, train/learning_rate: 3.0033317671041004e-05\n",
      "Step: 16780, train/epoch: 3.9933364391326904\n",
      "Step: 16790, train/loss: 0.0\n",
      "Step: 16790, train/grad_norm: 0.00038886198308318853\n",
      "Step: 16790, train/learning_rate: 3.0021417842363007e-05\n",
      "Step: 16790, train/epoch: 3.9957163333892822\n",
      "Step: 16800, train/loss: 0.0\n",
      "Step: 16800, train/grad_norm: 0.0006940274615772069\n",
      "Step: 16800, train/learning_rate: 3.0009519832674414e-05\n",
      "Step: 16800, train/epoch: 3.998096227645874\n",
      "Step: 16808, eval/loss: 0.01837705262005329\n",
      "Step: 16808, eval/accuracy: 0.9977787137031555\n",
      "Step: 16808, eval/f1: 0.9976530075073242\n",
      "Step: 16808, eval/runtime: 856.807373046875\n",
      "Step: 16808, eval/samples_per_second: 8.406999588012695\n",
      "Step: 16808, eval/steps_per_second: 1.0520000457763672\n",
      "Step: 16808, train/epoch: 4.0\n",
      "Step: 16810, train/loss: 0.0\n",
      "Step: 16810, train/grad_norm: 0.00035635652602650225\n",
      "Step: 16810, train/learning_rate: 2.9997620003996417e-05\n",
      "Step: 16810, train/epoch: 4.000475883483887\n",
      "Step: 16820, train/loss: 0.0\n",
      "Step: 16820, train/grad_norm: 0.000442319898866117\n",
      "Step: 16820, train/learning_rate: 2.9985721994307823e-05\n",
      "Step: 16820, train/epoch: 4.0028557777404785\n",
      "Step: 16830, train/loss: 0.0\n",
      "Step: 16830, train/grad_norm: 0.0008240350289270282\n",
      "Step: 16830, train/learning_rate: 2.9973822165629826e-05\n",
      "Step: 16830, train/epoch: 4.00523567199707\n",
      "Step: 16840, train/loss: 0.0\n",
      "Step: 16840, train/grad_norm: 0.001443517510779202\n",
      "Step: 16840, train/learning_rate: 2.996192233695183e-05\n",
      "Step: 16840, train/epoch: 4.007615566253662\n",
      "Step: 16850, train/loss: 0.0\n",
      "Step: 16850, train/grad_norm: 0.0004820541071239859\n",
      "Step: 16850, train/learning_rate: 2.9950024327263236e-05\n",
      "Step: 16850, train/epoch: 4.009995460510254\n",
      "Step: 16860, train/loss: 0.0\n",
      "Step: 16860, train/grad_norm: 0.001704670488834381\n",
      "Step: 16860, train/learning_rate: 2.993812449858524e-05\n",
      "Step: 16860, train/epoch: 4.0123748779296875\n",
      "Step: 16870, train/loss: 0.0\n",
      "Step: 16870, train/grad_norm: 0.00023269426310434937\n",
      "Step: 16870, train/learning_rate: 2.9926226488896646e-05\n",
      "Step: 16870, train/epoch: 4.014754772186279\n",
      "Step: 16880, train/loss: 0.0\n",
      "Step: 16880, train/grad_norm: 0.00047870719572529197\n",
      "Step: 16880, train/learning_rate: 2.991432666021865e-05\n",
      "Step: 16880, train/epoch: 4.017134666442871\n",
      "Step: 16890, train/loss: 0.0\n",
      "Step: 16890, train/grad_norm: 0.00022974127205088735\n",
      "Step: 16890, train/learning_rate: 2.990242683154065e-05\n",
      "Step: 16890, train/epoch: 4.019514560699463\n",
      "Step: 16900, train/loss: 0.0\n",
      "Step: 16900, train/grad_norm: 0.00010327292693546042\n",
      "Step: 16900, train/learning_rate: 2.9890528821852058e-05\n",
      "Step: 16900, train/epoch: 4.021894454956055\n",
      "Step: 16910, train/loss: 0.1664000004529953\n",
      "Step: 16910, train/grad_norm: 0.000721812539268285\n",
      "Step: 16910, train/learning_rate: 2.987862899317406e-05\n",
      "Step: 16910, train/epoch: 4.0242743492126465\n",
      "Step: 16920, train/loss: 0.0\n",
      "Step: 16920, train/grad_norm: 0.05549902096390724\n",
      "Step: 16920, train/learning_rate: 2.9866730983485468e-05\n",
      "Step: 16920, train/epoch: 4.02665376663208\n",
      "Step: 16930, train/loss: 0.15880000591278076\n",
      "Step: 16930, train/grad_norm: 0.1179090291261673\n",
      "Step: 16930, train/learning_rate: 2.985483115480747e-05\n",
      "Step: 16930, train/epoch: 4.029033660888672\n",
      "Step: 16940, train/loss: 0.00039999998989515007\n",
      "Step: 16940, train/grad_norm: 0.04302048683166504\n",
      "Step: 16940, train/learning_rate: 2.9842931326129474e-05\n",
      "Step: 16940, train/epoch: 4.031413555145264\n",
      "Step: 16950, train/loss: 0.0\n",
      "Step: 16950, train/grad_norm: 0.0029830392450094223\n",
      "Step: 16950, train/learning_rate: 2.983103331644088e-05\n",
      "Step: 16950, train/epoch: 4.0337934494018555\n",
      "Step: 16960, train/loss: 0.0\n",
      "Step: 16960, train/grad_norm: 0.004022221080958843\n",
      "Step: 16960, train/learning_rate: 2.9819133487762883e-05\n",
      "Step: 16960, train/epoch: 4.036173343658447\n",
      "Step: 16970, train/loss: 0.0\n",
      "Step: 16970, train/grad_norm: 0.0011683892225846648\n",
      "Step: 16970, train/learning_rate: 2.980723547807429e-05\n",
      "Step: 16970, train/epoch: 4.038553237915039\n",
      "Step: 16980, train/loss: 0.0\n",
      "Step: 16980, train/grad_norm: 0.0026263296604156494\n",
      "Step: 16980, train/learning_rate: 2.9795335649396293e-05\n",
      "Step: 16980, train/epoch: 4.040932655334473\n",
      "Step: 16990, train/loss: 0.0\n",
      "Step: 16990, train/grad_norm: 0.0009184596128761768\n",
      "Step: 16990, train/learning_rate: 2.9783435820718296e-05\n",
      "Step: 16990, train/epoch: 4.0433125495910645\n",
      "Step: 17000, train/loss: 0.0\n",
      "Step: 17000, train/grad_norm: 0.000296676967991516\n",
      "Step: 17000, train/learning_rate: 2.9771537811029702e-05\n",
      "Step: 17000, train/epoch: 4.045692443847656\n",
      "Step: 17010, train/loss: 0.0\n",
      "Step: 17010, train/grad_norm: 0.00014022250252310187\n",
      "Step: 17010, train/learning_rate: 2.9759637982351705e-05\n",
      "Step: 17010, train/epoch: 4.048072338104248\n",
      "Step: 17020, train/loss: 0.0\n",
      "Step: 17020, train/grad_norm: 0.001136473729275167\n",
      "Step: 17020, train/learning_rate: 2.9747739972663112e-05\n",
      "Step: 17020, train/epoch: 4.05045223236084\n",
      "Step: 17030, train/loss: 0.0\n",
      "Step: 17030, train/grad_norm: 0.0003703499387484044\n",
      "Step: 17030, train/learning_rate: 2.9735840143985115e-05\n",
      "Step: 17030, train/epoch: 4.052832126617432\n",
      "Step: 17040, train/loss: 0.0\n",
      "Step: 17040, train/grad_norm: 0.0002239073655800894\n",
      "Step: 17040, train/learning_rate: 2.9723940315307118e-05\n",
      "Step: 17040, train/epoch: 4.055212020874023\n",
      "Step: 17050, train/loss: 0.0\n",
      "Step: 17050, train/grad_norm: 0.0006108882371336222\n",
      "Step: 17050, train/learning_rate: 2.9712042305618525e-05\n",
      "Step: 17050, train/epoch: 4.057591438293457\n",
      "Step: 17060, train/loss: 0.0\n",
      "Step: 17060, train/grad_norm: 0.0003154050500597805\n",
      "Step: 17060, train/learning_rate: 2.9700142476940528e-05\n",
      "Step: 17060, train/epoch: 4.059971332550049\n",
      "Step: 17070, train/loss: 0.0\n",
      "Step: 17070, train/grad_norm: 0.00021948020730633289\n",
      "Step: 17070, train/learning_rate: 2.9688244467251934e-05\n",
      "Step: 17070, train/epoch: 4.062351226806641\n",
      "Step: 17080, train/loss: 0.13830000162124634\n",
      "Step: 17080, train/grad_norm: 0.00862190779298544\n",
      "Step: 17080, train/learning_rate: 2.9676344638573937e-05\n",
      "Step: 17080, train/epoch: 4.064731121063232\n",
      "Step: 17090, train/loss: 0.0\n",
      "Step: 17090, train/grad_norm: 0.028145836666226387\n",
      "Step: 17090, train/learning_rate: 2.966444480989594e-05\n",
      "Step: 17090, train/epoch: 4.067111015319824\n",
      "Step: 17100, train/loss: 0.0\n",
      "Step: 17100, train/grad_norm: 0.042198024690151215\n",
      "Step: 17100, train/learning_rate: 2.9652546800207347e-05\n",
      "Step: 17100, train/epoch: 4.069490909576416\n",
      "Step: 17110, train/loss: 0.0\n",
      "Step: 17110, train/grad_norm: 0.00979614071547985\n",
      "Step: 17110, train/learning_rate: 2.964064697152935e-05\n",
      "Step: 17110, train/epoch: 4.07187032699585\n",
      "Step: 17120, train/loss: 0.0\n",
      "Step: 17120, train/grad_norm: 0.0017207989003509283\n",
      "Step: 17120, train/learning_rate: 2.9628748961840756e-05\n",
      "Step: 17120, train/epoch: 4.074250221252441\n",
      "Step: 17130, train/loss: 0.0\n",
      "Step: 17130, train/grad_norm: 0.0019182318355888128\n",
      "Step: 17130, train/learning_rate: 2.961684913316276e-05\n",
      "Step: 17130, train/epoch: 4.076630115509033\n",
      "Step: 17140, train/loss: 0.0\n",
      "Step: 17140, train/grad_norm: 0.0011400224175304174\n",
      "Step: 17140, train/learning_rate: 2.9604949304484762e-05\n",
      "Step: 17140, train/epoch: 4.079010009765625\n",
      "Step: 17150, train/loss: 0.0\n",
      "Step: 17150, train/grad_norm: 0.0011998129775747657\n",
      "Step: 17150, train/learning_rate: 2.959305129479617e-05\n",
      "Step: 17150, train/epoch: 4.081389904022217\n",
      "Step: 17160, train/loss: 0.1445000022649765\n",
      "Step: 17160, train/grad_norm: 84.94666290283203\n",
      "Step: 17160, train/learning_rate: 2.9581151466118172e-05\n",
      "Step: 17160, train/epoch: 4.083769798278809\n",
      "Step: 17170, train/loss: 0.0\n",
      "Step: 17170, train/grad_norm: 0.017412422224879265\n",
      "Step: 17170, train/learning_rate: 2.956925345642958e-05\n",
      "Step: 17170, train/epoch: 4.086149215698242\n",
      "Step: 17180, train/loss: 0.00039999998989515007\n",
      "Step: 17180, train/grad_norm: 0.016600653529167175\n",
      "Step: 17180, train/learning_rate: 2.955735362775158e-05\n",
      "Step: 17180, train/epoch: 4.088529109954834\n",
      "Step: 17190, train/loss: 0.0\n",
      "Step: 17190, train/grad_norm: 0.003511324990540743\n",
      "Step: 17190, train/learning_rate: 2.9545453799073584e-05\n",
      "Step: 17190, train/epoch: 4.090909004211426\n",
      "Step: 17200, train/loss: 0.0\n",
      "Step: 17200, train/grad_norm: 0.007000716868788004\n",
      "Step: 17200, train/learning_rate: 2.953355578938499e-05\n",
      "Step: 17200, train/epoch: 4.093288898468018\n",
      "Step: 17210, train/loss: 0.0\n",
      "Step: 17210, train/grad_norm: 0.0017634478863328695\n",
      "Step: 17210, train/learning_rate: 2.9521655960706994e-05\n",
      "Step: 17210, train/epoch: 4.095668792724609\n",
      "Step: 17220, train/loss: 0.0\n",
      "Step: 17220, train/grad_norm: 0.0008471017354167998\n",
      "Step: 17220, train/learning_rate: 2.95097579510184e-05\n",
      "Step: 17220, train/epoch: 4.098048686981201\n",
      "Step: 17230, train/loss: 0.0\n",
      "Step: 17230, train/grad_norm: 0.0011577645782381296\n",
      "Step: 17230, train/learning_rate: 2.9497858122340403e-05\n",
      "Step: 17230, train/epoch: 4.100428581237793\n",
      "Step: 17240, train/loss: 0.0\n",
      "Step: 17240, train/grad_norm: 0.00037649052683264017\n",
      "Step: 17240, train/learning_rate: 2.9485958293662407e-05\n",
      "Step: 17240, train/epoch: 4.102807998657227\n",
      "Step: 17250, train/loss: 0.0\n",
      "Step: 17250, train/grad_norm: 0.0011284955544397235\n",
      "Step: 17250, train/learning_rate: 2.9474060283973813e-05\n",
      "Step: 17250, train/epoch: 4.105187892913818\n",
      "Step: 17260, train/loss: 0.0\n",
      "Step: 17260, train/grad_norm: 0.002646972192451358\n",
      "Step: 17260, train/learning_rate: 2.9462160455295816e-05\n",
      "Step: 17260, train/epoch: 4.10756778717041\n",
      "Step: 17270, train/loss: 0.0\n",
      "Step: 17270, train/grad_norm: 0.000783299095928669\n",
      "Step: 17270, train/learning_rate: 2.9450262445607223e-05\n",
      "Step: 17270, train/epoch: 4.109947681427002\n",
      "Step: 17280, train/loss: 0.0\n",
      "Step: 17280, train/grad_norm: 0.0003809383779298514\n",
      "Step: 17280, train/learning_rate: 2.9438362616929226e-05\n",
      "Step: 17280, train/epoch: 4.112327575683594\n",
      "Step: 17290, train/loss: 0.0\n",
      "Step: 17290, train/grad_norm: 0.00036774229374714196\n",
      "Step: 17290, train/learning_rate: 2.942646278825123e-05\n",
      "Step: 17290, train/epoch: 4.1147074699401855\n",
      "Step: 17300, train/loss: 0.0\n",
      "Step: 17300, train/grad_norm: 0.0006322310073301196\n",
      "Step: 17300, train/learning_rate: 2.9414564778562635e-05\n",
      "Step: 17300, train/epoch: 4.117086887359619\n",
      "Step: 17310, train/loss: 0.0\n",
      "Step: 17310, train/grad_norm: 0.0016706886235624552\n",
      "Step: 17310, train/learning_rate: 2.9402664949884638e-05\n",
      "Step: 17310, train/epoch: 4.119466781616211\n",
      "Step: 17320, train/loss: 0.0\n",
      "Step: 17320, train/grad_norm: 0.0005714191356673837\n",
      "Step: 17320, train/learning_rate: 2.9390766940196045e-05\n",
      "Step: 17320, train/epoch: 4.121846675872803\n",
      "Step: 17330, train/loss: 0.0\n",
      "Step: 17330, train/grad_norm: 0.0004957820056006312\n",
      "Step: 17330, train/learning_rate: 2.9378867111518048e-05\n",
      "Step: 17330, train/epoch: 4.1242265701293945\n",
      "Step: 17340, train/loss: 0.0\n",
      "Step: 17340, train/grad_norm: 0.00011814587924163789\n",
      "Step: 17340, train/learning_rate: 2.936696728284005e-05\n",
      "Step: 17340, train/epoch: 4.126606464385986\n",
      "Step: 17350, train/loss: 0.0\n",
      "Step: 17350, train/grad_norm: 0.0001412674318999052\n",
      "Step: 17350, train/learning_rate: 2.9355069273151457e-05\n",
      "Step: 17350, train/epoch: 4.128986358642578\n",
      "Step: 17360, train/loss: 0.0\n",
      "Step: 17360, train/grad_norm: 0.00022807407367508858\n",
      "Step: 17360, train/learning_rate: 2.934316944447346e-05\n",
      "Step: 17360, train/epoch: 4.13136625289917\n",
      "Step: 17370, train/loss: 0.0\n",
      "Step: 17370, train/grad_norm: 0.00030887610046193004\n",
      "Step: 17370, train/learning_rate: 2.9331271434784867e-05\n",
      "Step: 17370, train/epoch: 4.1337456703186035\n",
      "Step: 17380, train/loss: 0.0\n",
      "Step: 17380, train/grad_norm: 0.0007821070030331612\n",
      "Step: 17380, train/learning_rate: 2.931937160610687e-05\n",
      "Step: 17380, train/epoch: 4.136125564575195\n",
      "Step: 17390, train/loss: 0.0\n",
      "Step: 17390, train/grad_norm: 0.0003232900344301015\n",
      "Step: 17390, train/learning_rate: 2.9307471777428873e-05\n",
      "Step: 17390, train/epoch: 4.138505458831787\n",
      "Step: 17400, train/loss: 0.0\n",
      "Step: 17400, train/grad_norm: 0.00020896995556540787\n",
      "Step: 17400, train/learning_rate: 2.929557376774028e-05\n",
      "Step: 17400, train/epoch: 4.140885353088379\n",
      "Step: 17410, train/loss: 0.0\n",
      "Step: 17410, train/grad_norm: 0.00024604660575278103\n",
      "Step: 17410, train/learning_rate: 2.9283673939062282e-05\n",
      "Step: 17410, train/epoch: 4.143265247344971\n",
      "Step: 17420, train/loss: 0.0\n",
      "Step: 17420, train/grad_norm: 0.0001366733922623098\n",
      "Step: 17420, train/learning_rate: 2.927177592937369e-05\n",
      "Step: 17420, train/epoch: 4.1456451416015625\n",
      "Step: 17430, train/loss: 0.0\n",
      "Step: 17430, train/grad_norm: 0.0003147879906464368\n",
      "Step: 17430, train/learning_rate: 2.9259876100695692e-05\n",
      "Step: 17430, train/epoch: 4.148024559020996\n",
      "Step: 17440, train/loss: 0.0\n",
      "Step: 17440, train/grad_norm: 0.0013887201203033328\n",
      "Step: 17440, train/learning_rate: 2.9247976272017695e-05\n",
      "Step: 17440, train/epoch: 4.150404453277588\n",
      "Step: 17450, train/loss: 0.0\n",
      "Step: 17450, train/grad_norm: 0.0006887416820973158\n",
      "Step: 17450, train/learning_rate: 2.92360782623291e-05\n",
      "Step: 17450, train/epoch: 4.15278434753418\n",
      "Step: 17460, train/loss: 0.0\n",
      "Step: 17460, train/grad_norm: 9.264620894100517e-05\n",
      "Step: 17460, train/learning_rate: 2.9224178433651105e-05\n",
      "Step: 17460, train/epoch: 4.1551642417907715\n",
      "Step: 17470, train/loss: 0.0\n",
      "Step: 17470, train/grad_norm: 0.00032005662797018886\n",
      "Step: 17470, train/learning_rate: 2.921228042396251e-05\n",
      "Step: 17470, train/epoch: 4.157544136047363\n",
      "Step: 17480, train/loss: 0.0\n",
      "Step: 17480, train/grad_norm: 0.00013716470857616514\n",
      "Step: 17480, train/learning_rate: 2.9200380595284514e-05\n",
      "Step: 17480, train/epoch: 4.159924030303955\n",
      "Step: 17490, train/loss: 0.0\n",
      "Step: 17490, train/grad_norm: 0.00033887807512655854\n",
      "Step: 17490, train/learning_rate: 2.9188480766606517e-05\n",
      "Step: 17490, train/epoch: 4.162303447723389\n",
      "Step: 17500, train/loss: 0.0\n",
      "Step: 17500, train/grad_norm: 0.0001033758744597435\n",
      "Step: 17500, train/learning_rate: 2.9176582756917924e-05\n",
      "Step: 17500, train/epoch: 4.1646833419799805\n",
      "Step: 17510, train/loss: 0.0\n",
      "Step: 17510, train/grad_norm: 0.00011248567170696333\n",
      "Step: 17510, train/learning_rate: 2.9164682928239927e-05\n",
      "Step: 17510, train/epoch: 4.167063236236572\n",
      "Step: 17520, train/loss: 0.0\n",
      "Step: 17520, train/grad_norm: 5.118099579703994e-05\n",
      "Step: 17520, train/learning_rate: 2.9152784918551333e-05\n",
      "Step: 17520, train/epoch: 4.169443130493164\n",
      "Step: 17530, train/loss: 0.0\n",
      "Step: 17530, train/grad_norm: 0.0001564694248372689\n",
      "Step: 17530, train/learning_rate: 2.9140885089873336e-05\n",
      "Step: 17530, train/epoch: 4.171823024749756\n",
      "Step: 17540, train/loss: 0.0\n",
      "Step: 17540, train/grad_norm: 5.904783756705001e-05\n",
      "Step: 17540, train/learning_rate: 2.9128987080184743e-05\n",
      "Step: 17540, train/epoch: 4.174202919006348\n",
      "Step: 17550, train/loss: 0.0\n",
      "Step: 17550, train/grad_norm: 0.0005743894726037979\n",
      "Step: 17550, train/learning_rate: 2.9117087251506746e-05\n",
      "Step: 17550, train/epoch: 4.1765828132629395\n",
      "Step: 17560, train/loss: 0.0\n",
      "Step: 17560, train/grad_norm: 0.0001311598316533491\n",
      "Step: 17560, train/learning_rate: 2.910518742282875e-05\n",
      "Step: 17560, train/epoch: 4.178962230682373\n",
      "Step: 17570, train/loss: 0.0\n",
      "Step: 17570, train/grad_norm: 0.0001245110179297626\n",
      "Step: 17570, train/learning_rate: 2.9093289413140155e-05\n",
      "Step: 17570, train/epoch: 4.181342124938965\n",
      "Step: 17580, train/loss: 0.0\n",
      "Step: 17580, train/grad_norm: 0.00011484221613500267\n",
      "Step: 17580, train/learning_rate: 2.908138958446216e-05\n",
      "Step: 17580, train/epoch: 4.183722019195557\n",
      "Step: 17590, train/loss: 0.0\n",
      "Step: 17590, train/grad_norm: 0.00020815468451473862\n",
      "Step: 17590, train/learning_rate: 2.9069491574773565e-05\n",
      "Step: 17590, train/epoch: 4.186101913452148\n",
      "Step: 17600, train/loss: 0.0\n",
      "Step: 17600, train/grad_norm: 0.00016968045383691788\n",
      "Step: 17600, train/learning_rate: 2.9057591746095568e-05\n",
      "Step: 17600, train/epoch: 4.18848180770874\n",
      "Step: 17610, train/loss: 0.0\n",
      "Step: 17610, train/grad_norm: 0.0001576645881868899\n",
      "Step: 17610, train/learning_rate: 2.904569191741757e-05\n",
      "Step: 17610, train/epoch: 4.190861701965332\n",
      "Step: 17620, train/loss: 0.09730000048875809\n",
      "Step: 17620, train/grad_norm: 0.0009124595671892166\n",
      "Step: 17620, train/learning_rate: 2.9033793907728978e-05\n",
      "Step: 17620, train/epoch: 4.193241119384766\n",
      "Step: 17630, train/loss: 0.18610000610351562\n",
      "Step: 17630, train/grad_norm: 0.0033919133711606264\n",
      "Step: 17630, train/learning_rate: 2.902189407905098e-05\n",
      "Step: 17630, train/epoch: 4.195621013641357\n",
      "Step: 17640, train/loss: 0.11410000175237656\n",
      "Step: 17640, train/grad_norm: 0.1086672991514206\n",
      "Step: 17640, train/learning_rate: 2.9009996069362387e-05\n",
      "Step: 17640, train/epoch: 4.198000907897949\n",
      "Step: 17650, train/loss: 0.00019999999494757503\n",
      "Step: 17650, train/grad_norm: 0.1536758989095688\n",
      "Step: 17650, train/learning_rate: 2.899809624068439e-05\n",
      "Step: 17650, train/epoch: 4.200380802154541\n",
      "Step: 17660, train/loss: 0.0\n",
      "Step: 17660, train/grad_norm: 0.01386964786797762\n",
      "Step: 17660, train/learning_rate: 2.8986196412006393e-05\n",
      "Step: 17660, train/epoch: 4.202760696411133\n",
      "Step: 17670, train/loss: 0.07100000232458115\n",
      "Step: 17670, train/grad_norm: 6.625440437346697e-05\n",
      "Step: 17670, train/learning_rate: 2.89742984023178e-05\n",
      "Step: 17670, train/epoch: 4.205140590667725\n",
      "Step: 17680, train/loss: 0.0\n",
      "Step: 17680, train/grad_norm: 3.817627293756232e-05\n",
      "Step: 17680, train/learning_rate: 2.8962398573639803e-05\n",
      "Step: 17680, train/epoch: 4.207520008087158\n",
      "Step: 17690, train/loss: 9.999999747378752e-05\n",
      "Step: 17690, train/grad_norm: 5.7750658015720546e-05\n",
      "Step: 17690, train/learning_rate: 2.895050056395121e-05\n",
      "Step: 17690, train/epoch: 4.20989990234375\n",
      "Step: 17700, train/loss: 0.3391000032424927\n",
      "Step: 17700, train/grad_norm: 0.0013534508179873228\n",
      "Step: 17700, train/learning_rate: 2.8938600735273212e-05\n",
      "Step: 17700, train/epoch: 4.212279796600342\n",
      "Step: 17710, train/loss: 0.06019999831914902\n",
      "Step: 17710, train/grad_norm: 0.0020501071121543646\n",
      "Step: 17710, train/learning_rate: 2.8926700906595215e-05\n",
      "Step: 17710, train/epoch: 4.214659690856934\n",
      "Step: 17720, train/loss: 0.0\n",
      "Step: 17720, train/grad_norm: 0.004961763508617878\n",
      "Step: 17720, train/learning_rate: 2.8914802896906622e-05\n",
      "Step: 17720, train/epoch: 4.217039585113525\n",
      "Step: 17730, train/loss: 0.031199999153614044\n",
      "Step: 17730, train/grad_norm: 484.53167724609375\n",
      "Step: 17730, train/learning_rate: 2.8902903068228625e-05\n",
      "Step: 17730, train/epoch: 4.219419479370117\n",
      "Step: 17740, train/loss: 0.0\n",
      "Step: 17740, train/grad_norm: 0.0006183354998938739\n",
      "Step: 17740, train/learning_rate: 2.889100505854003e-05\n",
      "Step: 17740, train/epoch: 4.221799373626709\n",
      "Step: 17750, train/loss: 0.5394999980926514\n",
      "Step: 17750, train/grad_norm: 0.2221335470676422\n",
      "Step: 17750, train/learning_rate: 2.8879105229862034e-05\n",
      "Step: 17750, train/epoch: 4.224178791046143\n",
      "Step: 17760, train/loss: 0.00019999999494757503\n",
      "Step: 17760, train/grad_norm: 0.20643681287765503\n",
      "Step: 17760, train/learning_rate: 2.8867205401184037e-05\n",
      "Step: 17760, train/epoch: 4.226558685302734\n",
      "Step: 17770, train/loss: 0.07940000295639038\n",
      "Step: 17770, train/grad_norm: 0.00899153295904398\n",
      "Step: 17770, train/learning_rate: 2.8855307391495444e-05\n",
      "Step: 17770, train/epoch: 4.228938579559326\n",
      "Step: 17780, train/loss: 9.999999747378752e-05\n",
      "Step: 17780, train/grad_norm: 0.00626266049221158\n",
      "Step: 17780, train/learning_rate: 2.8843407562817447e-05\n",
      "Step: 17780, train/epoch: 4.231318473815918\n",
      "Step: 17790, train/loss: 0.10170000046491623\n",
      "Step: 17790, train/grad_norm: 0.16731543838977814\n",
      "Step: 17790, train/learning_rate: 2.8831509553128853e-05\n",
      "Step: 17790, train/epoch: 4.23369836807251\n",
      "Step: 17800, train/loss: 0.0\n",
      "Step: 17800, train/grad_norm: 1.3863080312148668e-05\n",
      "Step: 17800, train/learning_rate: 2.8819609724450856e-05\n",
      "Step: 17800, train/epoch: 4.236078262329102\n",
      "Step: 17810, train/loss: 0.1031000018119812\n",
      "Step: 17810, train/grad_norm: 0.00029178927070461214\n",
      "Step: 17810, train/learning_rate: 2.880770989577286e-05\n",
      "Step: 17810, train/epoch: 4.238457679748535\n",
      "Step: 17820, train/loss: 0.0\n",
      "Step: 17820, train/grad_norm: 5.268320092000067e-05\n",
      "Step: 17820, train/learning_rate: 2.8795811886084266e-05\n",
      "Step: 17820, train/epoch: 4.240837574005127\n",
      "Step: 17830, train/loss: 0.00019999999494757503\n",
      "Step: 17830, train/grad_norm: 0.00031899192254059017\n",
      "Step: 17830, train/learning_rate: 2.878391205740627e-05\n",
      "Step: 17830, train/epoch: 4.243217468261719\n",
      "Step: 17840, train/loss: 0.0\n",
      "Step: 17840, train/grad_norm: 6.936889258213341e-05\n",
      "Step: 17840, train/learning_rate: 2.8772014047717676e-05\n",
      "Step: 17840, train/epoch: 4.2455973625183105\n",
      "Step: 17850, train/loss: 0.015200000256299973\n",
      "Step: 17850, train/grad_norm: 1.270545726583805e-05\n",
      "Step: 17850, train/learning_rate: 2.876011421903968e-05\n",
      "Step: 17850, train/epoch: 4.247977256774902\n",
      "Step: 17860, train/loss: 0.0\n",
      "Step: 17860, train/grad_norm: 7.902090874267742e-05\n",
      "Step: 17860, train/learning_rate: 2.874821439036168e-05\n",
      "Step: 17860, train/epoch: 4.250357151031494\n",
      "Step: 17870, train/loss: 0.1648000031709671\n",
      "Step: 17870, train/grad_norm: 9.948906517820433e-05\n",
      "Step: 17870, train/learning_rate: 2.8736316380673088e-05\n",
      "Step: 17870, train/epoch: 4.252736568450928\n",
      "Step: 17880, train/loss: 0.0\n",
      "Step: 17880, train/grad_norm: 0.01347109954804182\n",
      "Step: 17880, train/learning_rate: 2.872441655199509e-05\n",
      "Step: 17880, train/epoch: 4.2551164627075195\n",
      "Step: 17890, train/loss: 0.02160000056028366\n",
      "Step: 17890, train/grad_norm: 0.0012567746452987194\n",
      "Step: 17890, train/learning_rate: 2.8712518542306498e-05\n",
      "Step: 17890, train/epoch: 4.257496356964111\n",
      "Step: 17900, train/loss: 0.0\n",
      "Step: 17900, train/grad_norm: 0.00014724856009706855\n",
      "Step: 17900, train/learning_rate: 2.87006187136285e-05\n",
      "Step: 17900, train/epoch: 4.259876251220703\n",
      "Step: 17910, train/loss: 0.0\n",
      "Step: 17910, train/grad_norm: 0.000816940504591912\n",
      "Step: 17910, train/learning_rate: 2.8688718884950504e-05\n",
      "Step: 17910, train/epoch: 4.262256145477295\n",
      "Step: 17920, train/loss: 0.0763000026345253\n",
      "Step: 17920, train/grad_norm: 0.025415444746613503\n",
      "Step: 17920, train/learning_rate: 2.867682087526191e-05\n",
      "Step: 17920, train/epoch: 4.264636039733887\n",
      "Step: 17930, train/loss: 9.999999747378752e-05\n",
      "Step: 17930, train/grad_norm: 0.008654575794935226\n",
      "Step: 17930, train/learning_rate: 2.8664921046583913e-05\n",
      "Step: 17930, train/epoch: 4.2670159339904785\n",
      "Step: 17940, train/loss: 0.15629999339580536\n",
      "Step: 17940, train/grad_norm: 0.024170557036995888\n",
      "Step: 17940, train/learning_rate: 2.865302303689532e-05\n",
      "Step: 17940, train/epoch: 4.269395351409912\n",
      "Step: 17950, train/loss: 0.0\n",
      "Step: 17950, train/grad_norm: 0.0030134173575788736\n",
      "Step: 17950, train/learning_rate: 2.8641123208217323e-05\n",
      "Step: 17950, train/epoch: 4.271775245666504\n",
      "Step: 17960, train/loss: 0.0\n",
      "Step: 17960, train/grad_norm: 0.006220965646207333\n",
      "Step: 17960, train/learning_rate: 2.8629223379539326e-05\n",
      "Step: 17960, train/epoch: 4.274155139923096\n",
      "Step: 17970, train/loss: 0.0\n",
      "Step: 17970, train/grad_norm: 0.002177987713366747\n",
      "Step: 17970, train/learning_rate: 2.8617325369850732e-05\n",
      "Step: 17970, train/epoch: 4.2765350341796875\n",
      "Step: 17980, train/loss: 0.0020000000949949026\n",
      "Step: 17980, train/grad_norm: 33.97606658935547\n",
      "Step: 17980, train/learning_rate: 2.8605425541172735e-05\n",
      "Step: 17980, train/epoch: 4.278914928436279\n",
      "Step: 17990, train/loss: 0.0\n",
      "Step: 17990, train/grad_norm: 0.0031337577383965254\n",
      "Step: 17990, train/learning_rate: 2.8593527531484142e-05\n",
      "Step: 17990, train/epoch: 4.281294822692871\n",
      "Step: 18000, train/loss: 0.0940999984741211\n",
      "Step: 18000, train/grad_norm: 88.59400939941406\n",
      "Step: 18000, train/learning_rate: 2.8581627702806145e-05\n",
      "Step: 18000, train/epoch: 4.283674240112305\n",
      "Step: 18010, train/loss: 9.999999747378752e-05\n",
      "Step: 18010, train/grad_norm: 0.0005151967634446919\n",
      "Step: 18010, train/learning_rate: 2.8569727874128148e-05\n",
      "Step: 18010, train/epoch: 4.2860541343688965\n",
      "Step: 18020, train/loss: 0.0\n",
      "Step: 18020, train/grad_norm: 0.00013523624511435628\n",
      "Step: 18020, train/learning_rate: 2.8557829864439555e-05\n",
      "Step: 18020, train/epoch: 4.288434028625488\n",
      "Step: 18030, train/loss: 0.0\n",
      "Step: 18030, train/grad_norm: 0.002143464284017682\n",
      "Step: 18030, train/learning_rate: 2.8545930035761558e-05\n",
      "Step: 18030, train/epoch: 4.29081392288208\n",
      "Step: 18040, train/loss: 0.0\n",
      "Step: 18040, train/grad_norm: 0.0011683183256536722\n",
      "Step: 18040, train/learning_rate: 2.8534032026072964e-05\n",
      "Step: 18040, train/epoch: 4.293193817138672\n",
      "Step: 18050, train/loss: 0.0\n",
      "Step: 18050, train/grad_norm: 0.0001204155123559758\n",
      "Step: 18050, train/learning_rate: 2.8522132197394967e-05\n",
      "Step: 18050, train/epoch: 4.295573711395264\n",
      "Step: 18060, train/loss: 0.0\n",
      "Step: 18060, train/grad_norm: 0.005517879035323858\n",
      "Step: 18060, train/learning_rate: 2.851023236871697e-05\n",
      "Step: 18060, train/epoch: 4.297953128814697\n",
      "Step: 18070, train/loss: 0.0\n",
      "Step: 18070, train/grad_norm: 0.00016378110740333796\n",
      "Step: 18070, train/learning_rate: 2.8498334359028377e-05\n",
      "Step: 18070, train/epoch: 4.300333023071289\n",
      "Step: 18080, train/loss: 0.0\n",
      "Step: 18080, train/grad_norm: 0.000251150835538283\n",
      "Step: 18080, train/learning_rate: 2.848643453035038e-05\n",
      "Step: 18080, train/epoch: 4.302712917327881\n",
      "Step: 18090, train/loss: 0.0\n",
      "Step: 18090, train/grad_norm: 0.0011653222609311342\n",
      "Step: 18090, train/learning_rate: 2.8474536520661786e-05\n",
      "Step: 18090, train/epoch: 4.305092811584473\n",
      "Step: 18100, train/loss: 0.0\n",
      "Step: 18100, train/grad_norm: 0.0001481150247855112\n",
      "Step: 18100, train/learning_rate: 2.846263669198379e-05\n",
      "Step: 18100, train/epoch: 4.3074727058410645\n",
      "Step: 18110, train/loss: 0.0\n",
      "Step: 18110, train/grad_norm: 0.0003130995901301503\n",
      "Step: 18110, train/learning_rate: 2.8450736863305792e-05\n",
      "Step: 18110, train/epoch: 4.309852600097656\n",
      "Step: 18120, train/loss: 0.0\n",
      "Step: 18120, train/grad_norm: 0.00011675745918182656\n",
      "Step: 18120, train/learning_rate: 2.84388388536172e-05\n",
      "Step: 18120, train/epoch: 4.312232494354248\n",
      "Step: 18130, train/loss: 0.0\n",
      "Step: 18130, train/grad_norm: 1.9653645722428337e-05\n",
      "Step: 18130, train/learning_rate: 2.8426939024939202e-05\n",
      "Step: 18130, train/epoch: 4.314611911773682\n",
      "Step: 18140, train/loss: 0.0\n",
      "Step: 18140, train/grad_norm: 0.00020381610374897718\n",
      "Step: 18140, train/learning_rate: 2.841504101525061e-05\n",
      "Step: 18140, train/epoch: 4.316991806030273\n",
      "Step: 18150, train/loss: 0.0\n",
      "Step: 18150, train/grad_norm: 0.0008272909908555448\n",
      "Step: 18150, train/learning_rate: 2.840314118657261e-05\n",
      "Step: 18150, train/epoch: 4.319371700286865\n",
      "Step: 18160, train/loss: 0.0\n",
      "Step: 18160, train/grad_norm: 0.0005411346792243421\n",
      "Step: 18160, train/learning_rate: 2.8391241357894614e-05\n",
      "Step: 18160, train/epoch: 4.321751594543457\n",
      "Step: 18170, train/loss: 0.0\n",
      "Step: 18170, train/grad_norm: 0.0008607447380200028\n",
      "Step: 18170, train/learning_rate: 2.837934334820602e-05\n",
      "Step: 18170, train/epoch: 4.324131488800049\n",
      "Step: 18180, train/loss: 0.0\n",
      "Step: 18180, train/grad_norm: 0.00012019397399853915\n",
      "Step: 18180, train/learning_rate: 2.8367443519528024e-05\n",
      "Step: 18180, train/epoch: 4.326511383056641\n",
      "Step: 18190, train/loss: 0.0\n",
      "Step: 18190, train/grad_norm: 1.7114316506194882e-05\n",
      "Step: 18190, train/learning_rate: 2.835554550983943e-05\n",
      "Step: 18190, train/epoch: 4.328890800476074\n",
      "Step: 18200, train/loss: 0.0\n",
      "Step: 18200, train/grad_norm: 2.26752927119378e-05\n",
      "Step: 18200, train/learning_rate: 2.8343645681161433e-05\n",
      "Step: 18200, train/epoch: 4.331270694732666\n",
      "Step: 18210, train/loss: 0.0\n",
      "Step: 18210, train/grad_norm: 0.00027654317091219127\n",
      "Step: 18210, train/learning_rate: 2.833174767147284e-05\n",
      "Step: 18210, train/epoch: 4.333650588989258\n",
      "Step: 18220, train/loss: 0.0\n",
      "Step: 18220, train/grad_norm: 3.198865306330845e-05\n",
      "Step: 18220, train/learning_rate: 2.8319847842794843e-05\n",
      "Step: 18220, train/epoch: 4.33603048324585\n",
      "Step: 18230, train/loss: 0.0\n",
      "Step: 18230, train/grad_norm: 5.212936957832426e-05\n",
      "Step: 18230, train/learning_rate: 2.8307948014116846e-05\n",
      "Step: 18230, train/epoch: 4.338410377502441\n",
      "Step: 18240, train/loss: 0.0\n",
      "Step: 18240, train/grad_norm: 0.00028483077767305076\n",
      "Step: 18240, train/learning_rate: 2.8296050004428253e-05\n",
      "Step: 18240, train/epoch: 4.340790271759033\n",
      "Step: 18250, train/loss: 0.2078000009059906\n",
      "Step: 18250, train/grad_norm: 0.05159817636013031\n",
      "Step: 18250, train/learning_rate: 2.8284150175750256e-05\n",
      "Step: 18250, train/epoch: 4.343169689178467\n",
      "Step: 18260, train/loss: 0.0003000000142492354\n",
      "Step: 18260, train/grad_norm: 0.008155575022101402\n",
      "Step: 18260, train/learning_rate: 2.8272252166061662e-05\n",
      "Step: 18260, train/epoch: 4.345549583435059\n",
      "Step: 18270, train/loss: 0.0\n",
      "Step: 18270, train/grad_norm: 0.0006437856354750693\n",
      "Step: 18270, train/learning_rate: 2.8260352337383665e-05\n",
      "Step: 18270, train/epoch: 4.34792947769165\n",
      "Step: 18280, train/loss: 0.0\n",
      "Step: 18280, train/grad_norm: 0.0006821875576861203\n",
      "Step: 18280, train/learning_rate: 2.8248452508705668e-05\n",
      "Step: 18280, train/epoch: 4.350309371948242\n",
      "Step: 18290, train/loss: 0.0\n",
      "Step: 18290, train/grad_norm: 0.00025295375962741673\n",
      "Step: 18290, train/learning_rate: 2.8236554499017075e-05\n",
      "Step: 18290, train/epoch: 4.352689266204834\n",
      "Step: 18300, train/loss: 0.0\n",
      "Step: 18300, train/grad_norm: 0.00023397829500027\n",
      "Step: 18300, train/learning_rate: 2.8224654670339078e-05\n",
      "Step: 18300, train/epoch: 4.355069160461426\n",
      "Step: 18310, train/loss: 0.0\n",
      "Step: 18310, train/grad_norm: 0.00010752579692052677\n",
      "Step: 18310, train/learning_rate: 2.8212756660650484e-05\n",
      "Step: 18310, train/epoch: 4.357449054718018\n",
      "Step: 18320, train/loss: 0.0\n",
      "Step: 18320, train/grad_norm: 0.00015376984083559364\n",
      "Step: 18320, train/learning_rate: 2.8200856831972487e-05\n",
      "Step: 18320, train/epoch: 4.359828472137451\n",
      "Step: 18330, train/loss: 0.0\n",
      "Step: 18330, train/grad_norm: 0.0001709203643258661\n",
      "Step: 18330, train/learning_rate: 2.818895700329449e-05\n",
      "Step: 18330, train/epoch: 4.362208366394043\n",
      "Step: 18340, train/loss: 0.0\n",
      "Step: 18340, train/grad_norm: 0.00012385949958115816\n",
      "Step: 18340, train/learning_rate: 2.8177058993605897e-05\n",
      "Step: 18340, train/epoch: 4.364588260650635\n",
      "Step: 18350, train/loss: 0.0\n",
      "Step: 18350, train/grad_norm: 0.0003236261254642159\n",
      "Step: 18350, train/learning_rate: 2.81651591649279e-05\n",
      "Step: 18350, train/epoch: 4.366968154907227\n",
      "Step: 18360, train/loss: 0.0\n",
      "Step: 18360, train/grad_norm: 0.00015151598199736327\n",
      "Step: 18360, train/learning_rate: 2.8153261155239306e-05\n",
      "Step: 18360, train/epoch: 4.369348049163818\n",
      "Step: 18370, train/loss: 0.0\n",
      "Step: 18370, train/grad_norm: 0.00010390415991423652\n",
      "Step: 18370, train/learning_rate: 2.814136132656131e-05\n",
      "Step: 18370, train/epoch: 4.37172794342041\n",
      "Step: 18380, train/loss: 0.0\n",
      "Step: 18380, train/grad_norm: 4.744105535792187e-05\n",
      "Step: 18380, train/learning_rate: 2.8129461497883312e-05\n",
      "Step: 18380, train/epoch: 4.374107360839844\n",
      "Step: 18390, train/loss: 0.0\n",
      "Step: 18390, train/grad_norm: 3.1370585929835215e-05\n",
      "Step: 18390, train/learning_rate: 2.811756348819472e-05\n",
      "Step: 18390, train/epoch: 4.3764872550964355\n",
      "Step: 18400, train/loss: 0.0\n",
      "Step: 18400, train/grad_norm: 0.09974545240402222\n",
      "Step: 18400, train/learning_rate: 2.8105663659516722e-05\n",
      "Step: 18400, train/epoch: 4.378867149353027\n",
      "Step: 18410, train/loss: 0.0\n",
      "Step: 18410, train/grad_norm: 8.792655717115849e-05\n",
      "Step: 18410, train/learning_rate: 2.809376564982813e-05\n",
      "Step: 18410, train/epoch: 4.381247043609619\n",
      "Step: 18420, train/loss: 0.0\n",
      "Step: 18420, train/grad_norm: 0.00017827667761594057\n",
      "Step: 18420, train/learning_rate: 2.808186582115013e-05\n",
      "Step: 18420, train/epoch: 4.383626937866211\n",
      "Step: 18430, train/loss: 0.0\n",
      "Step: 18430, train/grad_norm: 0.00012128705566283315\n",
      "Step: 18430, train/learning_rate: 2.8069965992472135e-05\n",
      "Step: 18430, train/epoch: 4.386006832122803\n",
      "Step: 18440, train/loss: 0.13359999656677246\n",
      "Step: 18440, train/grad_norm: 0.001147614442743361\n",
      "Step: 18440, train/learning_rate: 2.805806798278354e-05\n",
      "Step: 18440, train/epoch: 4.388386249542236\n",
      "Step: 18450, train/loss: 0.0\n",
      "Step: 18450, train/grad_norm: 0.008634937927126884\n",
      "Step: 18450, train/learning_rate: 2.8046168154105544e-05\n",
      "Step: 18450, train/epoch: 4.390766143798828\n",
      "Step: 18460, train/loss: 0.0\n",
      "Step: 18460, train/grad_norm: 0.07394911348819733\n",
      "Step: 18460, train/learning_rate: 2.803427014441695e-05\n",
      "Step: 18460, train/epoch: 4.39314603805542\n",
      "Step: 18470, train/loss: 0.0\n",
      "Step: 18470, train/grad_norm: 0.0008097854442894459\n",
      "Step: 18470, train/learning_rate: 2.8022370315738954e-05\n",
      "Step: 18470, train/epoch: 4.395525932312012\n",
      "Step: 18480, train/loss: 0.1054999977350235\n",
      "Step: 18480, train/grad_norm: 0.008985180407762527\n",
      "Step: 18480, train/learning_rate: 2.8010470487060957e-05\n",
      "Step: 18480, train/epoch: 4.3979058265686035\n",
      "Step: 18490, train/loss: 0.11640000343322754\n",
      "Step: 18490, train/grad_norm: 97.29523468017578\n",
      "Step: 18490, train/learning_rate: 2.7998572477372363e-05\n",
      "Step: 18490, train/epoch: 4.400285720825195\n",
      "Step: 18500, train/loss: 9.999999747378752e-05\n",
      "Step: 18500, train/grad_norm: 0.010176006704568863\n",
      "Step: 18500, train/learning_rate: 2.7986672648694366e-05\n",
      "Step: 18500, train/epoch: 4.402665615081787\n",
      "Step: 18510, train/loss: 9.999999747378752e-05\n",
      "Step: 18510, train/grad_norm: 0.06615379452705383\n",
      "Step: 18510, train/learning_rate: 2.7974774639005773e-05\n",
      "Step: 18510, train/epoch: 4.405045032501221\n",
      "Step: 18520, train/loss: 0.0\n",
      "Step: 18520, train/grad_norm: 0.0031463149935007095\n",
      "Step: 18520, train/learning_rate: 2.7962874810327776e-05\n",
      "Step: 18520, train/epoch: 4.4074249267578125\n",
      "Step: 18530, train/loss: 0.0\n",
      "Step: 18530, train/grad_norm: 0.0034477035515010357\n",
      "Step: 18530, train/learning_rate: 2.795097498164978e-05\n",
      "Step: 18530, train/epoch: 4.409804821014404\n",
      "Step: 18540, train/loss: 0.13050000369548798\n",
      "Step: 18540, train/grad_norm: 72.69458770751953\n",
      "Step: 18540, train/learning_rate: 2.7939076971961185e-05\n",
      "Step: 18540, train/epoch: 4.412184715270996\n",
      "Step: 18550, train/loss: 0.0\n",
      "Step: 18550, train/grad_norm: 0.09118516743183136\n",
      "Step: 18550, train/learning_rate: 2.792717714328319e-05\n",
      "Step: 18550, train/epoch: 4.414564609527588\n",
      "Step: 18560, train/loss: 0.057999998331069946\n",
      "Step: 18560, train/grad_norm: 0.5793922543525696\n",
      "Step: 18560, train/learning_rate: 2.7915279133594595e-05\n",
      "Step: 18560, train/epoch: 4.41694450378418\n",
      "Step: 18570, train/loss: 9.999999747378752e-05\n",
      "Step: 18570, train/grad_norm: 0.10272612422704697\n",
      "Step: 18570, train/learning_rate: 2.7903379304916598e-05\n",
      "Step: 18570, train/epoch: 4.419323921203613\n",
      "Step: 18580, train/loss: 0.0\n",
      "Step: 18580, train/grad_norm: 0.00515682203695178\n",
      "Step: 18580, train/learning_rate: 2.78914794762386e-05\n",
      "Step: 18580, train/epoch: 4.421703815460205\n",
      "Step: 18590, train/loss: 0.0\n",
      "Step: 18590, train/grad_norm: 0.005609904881566763\n",
      "Step: 18590, train/learning_rate: 2.7879581466550007e-05\n",
      "Step: 18590, train/epoch: 4.424083709716797\n",
      "Step: 18600, train/loss: 0.0\n",
      "Step: 18600, train/grad_norm: 0.0009336621733382344\n",
      "Step: 18600, train/learning_rate: 2.786768163787201e-05\n",
      "Step: 18600, train/epoch: 4.426463603973389\n",
      "Step: 18610, train/loss: 0.0\n",
      "Step: 18610, train/grad_norm: 0.002880391199141741\n",
      "Step: 18610, train/learning_rate: 2.7855783628183417e-05\n",
      "Step: 18610, train/epoch: 4.4288434982299805\n",
      "Step: 18620, train/loss: 0.0\n",
      "Step: 18620, train/grad_norm: 0.0016696070088073611\n",
      "Step: 18620, train/learning_rate: 2.784388379950542e-05\n",
      "Step: 18620, train/epoch: 4.431223392486572\n",
      "Step: 18630, train/loss: 0.16949999332427979\n",
      "Step: 18630, train/grad_norm: 0.00023722491459921002\n",
      "Step: 18630, train/learning_rate: 2.7831983970827423e-05\n",
      "Step: 18630, train/epoch: 4.433602809906006\n",
      "Step: 18640, train/loss: 9.999999747378752e-05\n",
      "Step: 18640, train/grad_norm: 0.017815178260207176\n",
      "Step: 18640, train/learning_rate: 2.782008596113883e-05\n",
      "Step: 18640, train/epoch: 4.435982704162598\n",
      "Step: 18650, train/loss: 0.0\n",
      "Step: 18650, train/grad_norm: 0.0030990331433713436\n",
      "Step: 18650, train/learning_rate: 2.7808186132460833e-05\n",
      "Step: 18650, train/epoch: 4.4383625984191895\n",
      "Step: 18660, train/loss: 0.10700000077486038\n",
      "Step: 18660, train/grad_norm: 77.73705291748047\n",
      "Step: 18660, train/learning_rate: 2.779628812277224e-05\n",
      "Step: 18660, train/epoch: 4.440742492675781\n",
      "Step: 18670, train/loss: 0.0\n",
      "Step: 18670, train/grad_norm: 0.0078889150172472\n",
      "Step: 18670, train/learning_rate: 2.7784388294094242e-05\n",
      "Step: 18670, train/epoch: 4.443122386932373\n",
      "Step: 18680, train/loss: 0.022299999371170998\n",
      "Step: 18680, train/grad_norm: 0.01772048883140087\n",
      "Step: 18680, train/learning_rate: 2.7772488465416245e-05\n",
      "Step: 18680, train/epoch: 4.445502281188965\n",
      "Step: 18690, train/loss: 9.999999747378752e-05\n",
      "Step: 18690, train/grad_norm: 0.017228759825229645\n",
      "Step: 18690, train/learning_rate: 2.7760590455727652e-05\n",
      "Step: 18690, train/epoch: 4.447882175445557\n",
      "Step: 18700, train/loss: 0.021900000050663948\n",
      "Step: 18700, train/grad_norm: 0.0492442287504673\n",
      "Step: 18700, train/learning_rate: 2.7748690627049655e-05\n",
      "Step: 18700, train/epoch: 4.45026159286499\n",
      "Step: 18710, train/loss: 0.0\n",
      "Step: 18710, train/grad_norm: 0.002725912956520915\n",
      "Step: 18710, train/learning_rate: 2.773679261736106e-05\n",
      "Step: 18710, train/epoch: 4.452641487121582\n",
      "Step: 18720, train/loss: 0.0\n",
      "Step: 18720, train/grad_norm: 0.0008812537998892367\n",
      "Step: 18720, train/learning_rate: 2.7724892788683064e-05\n",
      "Step: 18720, train/epoch: 4.455021381378174\n",
      "Step: 18730, train/loss: 0.0\n",
      "Step: 18730, train/grad_norm: 0.0019181253155693412\n",
      "Step: 18730, train/learning_rate: 2.7712992960005067e-05\n",
      "Step: 18730, train/epoch: 4.457401275634766\n",
      "Step: 18740, train/loss: 0.0\n",
      "Step: 18740, train/grad_norm: 0.00022665096912533045\n",
      "Step: 18740, train/learning_rate: 2.7701094950316474e-05\n",
      "Step: 18740, train/epoch: 4.459781169891357\n",
      "Step: 18750, train/loss: 0.16410000622272491\n",
      "Step: 18750, train/grad_norm: 73.86197662353516\n",
      "Step: 18750, train/learning_rate: 2.7689195121638477e-05\n",
      "Step: 18750, train/epoch: 4.462161064147949\n",
      "Step: 18760, train/loss: 0.0\n",
      "Step: 18760, train/grad_norm: 0.009858012199401855\n",
      "Step: 18760, train/learning_rate: 2.7677297111949883e-05\n",
      "Step: 18760, train/epoch: 4.464540481567383\n",
      "Step: 18770, train/loss: 9.999999747378752e-05\n",
      "Step: 18770, train/grad_norm: 0.012669818475842476\n",
      "Step: 18770, train/learning_rate: 2.7665397283271886e-05\n",
      "Step: 18770, train/epoch: 4.466920375823975\n",
      "Step: 18780, train/loss: 0.0\n",
      "Step: 18780, train/grad_norm: 0.02280600182712078\n",
      "Step: 18780, train/learning_rate: 2.765349745459389e-05\n",
      "Step: 18780, train/epoch: 4.469300270080566\n",
      "Step: 18790, train/loss: 0.0\n",
      "Step: 18790, train/grad_norm: 0.0032629515044391155\n",
      "Step: 18790, train/learning_rate: 2.7641599444905296e-05\n",
      "Step: 18790, train/epoch: 4.471680164337158\n",
      "Step: 18800, train/loss: 9.999999747378752e-05\n",
      "Step: 18800, train/grad_norm: 0.0004996143979951739\n",
      "Step: 18800, train/learning_rate: 2.76296996162273e-05\n",
      "Step: 18800, train/epoch: 4.47406005859375\n",
      "Step: 18810, train/loss: 0.0\n",
      "Step: 18810, train/grad_norm: 0.0006326668662950397\n",
      "Step: 18810, train/learning_rate: 2.7617801606538706e-05\n",
      "Step: 18810, train/epoch: 4.476439952850342\n",
      "Step: 18820, train/loss: 0.0\n",
      "Step: 18820, train/grad_norm: 0.0008761974168010056\n",
      "Step: 18820, train/learning_rate: 2.760590177786071e-05\n",
      "Step: 18820, train/epoch: 4.478819847106934\n",
      "Step: 18830, train/loss: 0.0\n",
      "Step: 18830, train/grad_norm: 0.0005345205427147448\n",
      "Step: 18830, train/learning_rate: 2.759400194918271e-05\n",
      "Step: 18830, train/epoch: 4.481199264526367\n",
      "Step: 18840, train/loss: 0.0\n",
      "Step: 18840, train/grad_norm: 0.003726122435182333\n",
      "Step: 18840, train/learning_rate: 2.7582103939494118e-05\n",
      "Step: 18840, train/epoch: 4.483579158782959\n",
      "Step: 18850, train/loss: 0.0\n",
      "Step: 18850, train/grad_norm: 0.0014797921758145094\n",
      "Step: 18850, train/learning_rate: 2.757020411081612e-05\n",
      "Step: 18850, train/epoch: 4.485959053039551\n",
      "Step: 18860, train/loss: 0.0\n",
      "Step: 18860, train/grad_norm: 0.0005458319210447371\n",
      "Step: 18860, train/learning_rate: 2.7558306101127528e-05\n",
      "Step: 18860, train/epoch: 4.488338947296143\n",
      "Step: 18870, train/loss: 0.15940000116825104\n",
      "Step: 18870, train/grad_norm: 9.94827933027409e-05\n",
      "Step: 18870, train/learning_rate: 2.754640627244953e-05\n",
      "Step: 18870, train/epoch: 4.490718841552734\n",
      "Step: 18880, train/loss: 0.0\n",
      "Step: 18880, train/grad_norm: 0.03193720430135727\n",
      "Step: 18880, train/learning_rate: 2.7534508262760937e-05\n",
      "Step: 18880, train/epoch: 4.493098735809326\n",
      "Step: 18890, train/loss: 0.0\n",
      "Step: 18890, train/grad_norm: 0.04421159252524376\n",
      "Step: 18890, train/learning_rate: 2.752260843408294e-05\n",
      "Step: 18890, train/epoch: 4.49547815322876\n",
      "Step: 18900, train/loss: 0.17890000343322754\n",
      "Step: 18900, train/grad_norm: 0.019574671983718872\n",
      "Step: 18900, train/learning_rate: 2.7510708605404943e-05\n",
      "Step: 18900, train/epoch: 4.497858047485352\n",
      "Step: 18910, train/loss: 9.999999747378752e-05\n",
      "Step: 18910, train/grad_norm: 0.03970750793814659\n",
      "Step: 18910, train/learning_rate: 2.749881059571635e-05\n",
      "Step: 18910, train/epoch: 4.500237941741943\n",
      "Step: 18920, train/loss: 0.0\n",
      "Step: 18920, train/grad_norm: 0.022234242409467697\n",
      "Step: 18920, train/learning_rate: 2.7486910767038353e-05\n",
      "Step: 18920, train/epoch: 4.502617835998535\n",
      "Step: 18930, train/loss: 0.000699999975040555\n",
      "Step: 18930, train/grad_norm: 0.0033369555603712797\n",
      "Step: 18930, train/learning_rate: 2.747501275734976e-05\n",
      "Step: 18930, train/epoch: 4.504997730255127\n",
      "Step: 18940, train/loss: 0.0\n",
      "Step: 18940, train/grad_norm: 0.0035890734288841486\n",
      "Step: 18940, train/learning_rate: 2.7463112928671762e-05\n",
      "Step: 18940, train/epoch: 4.507377624511719\n",
      "Step: 18950, train/loss: 0.0\n",
      "Step: 18950, train/grad_norm: 0.0022495021112263203\n",
      "Step: 18950, train/learning_rate: 2.7451213099993765e-05\n",
      "Step: 18950, train/epoch: 4.509757041931152\n",
      "Step: 18960, train/loss: 0.13199999928474426\n",
      "Step: 18960, train/grad_norm: 0.007465756498277187\n",
      "Step: 18960, train/learning_rate: 2.7439315090305172e-05\n",
      "Step: 18960, train/epoch: 4.512136936187744\n",
      "Step: 18970, train/loss: 0.0\n",
      "Step: 18970, train/grad_norm: 0.011282617226243019\n",
      "Step: 18970, train/learning_rate: 2.7427415261627175e-05\n",
      "Step: 18970, train/epoch: 4.514516830444336\n",
      "Step: 18980, train/loss: 0.0\n",
      "Step: 18980, train/grad_norm: 0.011535663157701492\n",
      "Step: 18980, train/learning_rate: 2.741551725193858e-05\n",
      "Step: 18980, train/epoch: 4.516896724700928\n",
      "Step: 18990, train/loss: 0.0\n",
      "Step: 18990, train/grad_norm: 0.007691939361393452\n",
      "Step: 18990, train/learning_rate: 2.7403617423260584e-05\n",
      "Step: 18990, train/epoch: 4.5192766189575195\n",
      "Step: 19000, train/loss: 0.0\n",
      "Step: 19000, train/grad_norm: 0.010000115260481834\n",
      "Step: 19000, train/learning_rate: 2.7391717594582587e-05\n",
      "Step: 19000, train/epoch: 4.521656513214111\n",
      "Step: 19010, train/loss: 0.0\n",
      "Step: 19010, train/grad_norm: 0.003442842047661543\n",
      "Step: 19010, train/learning_rate: 2.7379819584893994e-05\n",
      "Step: 19010, train/epoch: 4.524036407470703\n",
      "Step: 19020, train/loss: 0.0\n",
      "Step: 19020, train/grad_norm: 0.002155960537493229\n",
      "Step: 19020, train/learning_rate: 2.7367919756215997e-05\n",
      "Step: 19020, train/epoch: 4.526415824890137\n",
      "Step: 19030, train/loss: 0.0\n",
      "Step: 19030, train/grad_norm: 0.002998953452333808\n",
      "Step: 19030, train/learning_rate: 2.7356021746527404e-05\n",
      "Step: 19030, train/epoch: 4.5287957191467285\n",
      "Step: 19040, train/loss: 0.0\n",
      "Step: 19040, train/grad_norm: 0.0025548043195158243\n",
      "Step: 19040, train/learning_rate: 2.7344121917849407e-05\n",
      "Step: 19040, train/epoch: 4.53117561340332\n",
      "Step: 19050, train/loss: 0.0\n",
      "Step: 19050, train/grad_norm: 0.0012547821970656514\n",
      "Step: 19050, train/learning_rate: 2.733222208917141e-05\n",
      "Step: 19050, train/epoch: 4.533555507659912\n",
      "Step: 19060, train/loss: 0.0\n",
      "Step: 19060, train/grad_norm: 0.0018182953353971243\n",
      "Step: 19060, train/learning_rate: 2.7320324079482816e-05\n",
      "Step: 19060, train/epoch: 4.535935401916504\n",
      "Step: 19070, train/loss: 0.0\n",
      "Step: 19070, train/grad_norm: 0.0021922935266047716\n",
      "Step: 19070, train/learning_rate: 2.730842425080482e-05\n",
      "Step: 19070, train/epoch: 4.538315296173096\n",
      "Step: 19080, train/loss: 0.0\n",
      "Step: 19080, train/grad_norm: 0.0008313586004078388\n",
      "Step: 19080, train/learning_rate: 2.7296526241116226e-05\n",
      "Step: 19080, train/epoch: 4.540694713592529\n",
      "Step: 19090, train/loss: 0.0\n",
      "Step: 19090, train/grad_norm: 0.0008803069940768182\n",
      "Step: 19090, train/learning_rate: 2.728462641243823e-05\n",
      "Step: 19090, train/epoch: 4.543074607849121\n",
      "Step: 19100, train/loss: 0.0\n",
      "Step: 19100, train/grad_norm: 0.0006788436439819634\n",
      "Step: 19100, train/learning_rate: 2.7272726583760232e-05\n",
      "Step: 19100, train/epoch: 4.545454502105713\n",
      "Step: 19110, train/loss: 0.0\n",
      "Step: 19110, train/grad_norm: 0.0013857557205483317\n",
      "Step: 19110, train/learning_rate: 2.7260828574071638e-05\n",
      "Step: 19110, train/epoch: 4.547834396362305\n",
      "Step: 19120, train/loss: 0.0\n",
      "Step: 19120, train/grad_norm: 0.0013566429261118174\n",
      "Step: 19120, train/learning_rate: 2.724892874539364e-05\n",
      "Step: 19120, train/epoch: 4.5502142906188965\n",
      "Step: 19130, train/loss: 0.0\n",
      "Step: 19130, train/grad_norm: 0.0007736791158095002\n",
      "Step: 19130, train/learning_rate: 2.7237030735705048e-05\n",
      "Step: 19130, train/epoch: 4.552594184875488\n",
      "Step: 19140, train/loss: 0.0\n",
      "Step: 19140, train/grad_norm: 0.00040254220948554575\n",
      "Step: 19140, train/learning_rate: 2.722513090702705e-05\n",
      "Step: 19140, train/epoch: 4.554973602294922\n",
      "Step: 19150, train/loss: 0.0\n",
      "Step: 19150, train/grad_norm: 0.0011552685173228383\n",
      "Step: 19150, train/learning_rate: 2.7213231078349054e-05\n",
      "Step: 19150, train/epoch: 4.557353496551514\n",
      "Step: 19160, train/loss: 0.0\n",
      "Step: 19160, train/grad_norm: 0.0015495643019676208\n",
      "Step: 19160, train/learning_rate: 2.720133306866046e-05\n",
      "Step: 19160, train/epoch: 4.5597333908081055\n",
      "Step: 19170, train/loss: 0.0\n",
      "Step: 19170, train/grad_norm: 0.0006055990816093981\n",
      "Step: 19170, train/learning_rate: 2.7189433239982463e-05\n",
      "Step: 19170, train/epoch: 4.562113285064697\n",
      "Step: 19180, train/loss: 0.0\n",
      "Step: 19180, train/grad_norm: 0.0010111980373039842\n",
      "Step: 19180, train/learning_rate: 2.717753523029387e-05\n",
      "Step: 19180, train/epoch: 4.564493179321289\n",
      "Step: 19190, train/loss: 0.0\n",
      "Step: 19190, train/grad_norm: 0.0006126176449470222\n",
      "Step: 19190, train/learning_rate: 2.7165635401615873e-05\n",
      "Step: 19190, train/epoch: 4.566873073577881\n",
      "Step: 19200, train/loss: 0.0\n",
      "Step: 19200, train/grad_norm: 0.00028503176872618496\n",
      "Step: 19200, train/learning_rate: 2.7153735572937876e-05\n",
      "Step: 19200, train/epoch: 4.569252967834473\n",
      "Step: 19210, train/loss: 0.0\n",
      "Step: 19210, train/grad_norm: 0.0005236021243035793\n",
      "Step: 19210, train/learning_rate: 2.7141837563249283e-05\n",
      "Step: 19210, train/epoch: 4.571632385253906\n",
      "Step: 19220, train/loss: 0.0\n",
      "Step: 19220, train/grad_norm: 0.0002999044954776764\n",
      "Step: 19220, train/learning_rate: 2.7129937734571286e-05\n",
      "Step: 19220, train/epoch: 4.574012279510498\n",
      "Step: 19230, train/loss: 0.0\n",
      "Step: 19230, train/grad_norm: 0.0002547931799199432\n",
      "Step: 19230, train/learning_rate: 2.7118039724882692e-05\n",
      "Step: 19230, train/epoch: 4.57639217376709\n",
      "Step: 19240, train/loss: 0.0\n",
      "Step: 19240, train/grad_norm: 0.00032132372143678367\n",
      "Step: 19240, train/learning_rate: 2.7106139896204695e-05\n",
      "Step: 19240, train/epoch: 4.578772068023682\n",
      "Step: 19250, train/loss: 0.0\n",
      "Step: 19250, train/grad_norm: 0.0014523259596899152\n",
      "Step: 19250, train/learning_rate: 2.7094240067526698e-05\n",
      "Step: 19250, train/epoch: 4.581151962280273\n",
      "Step: 19260, train/loss: 0.13910000026226044\n",
      "Step: 19260, train/grad_norm: 0.0013171430910006166\n",
      "Step: 19260, train/learning_rate: 2.7082342057838105e-05\n",
      "Step: 19260, train/epoch: 4.583531856536865\n",
      "Step: 19270, train/loss: 0.0\n",
      "Step: 19270, train/grad_norm: 0.015394346788525581\n",
      "Step: 19270, train/learning_rate: 2.7070442229160108e-05\n",
      "Step: 19270, train/epoch: 4.585911273956299\n",
      "Step: 19280, train/loss: 0.0860000029206276\n",
      "Step: 19280, train/grad_norm: 0.0363541804254055\n",
      "Step: 19280, train/learning_rate: 2.7058544219471514e-05\n",
      "Step: 19280, train/epoch: 4.588291168212891\n",
      "Step: 19290, train/loss: 9.999999747378752e-05\n",
      "Step: 19290, train/grad_norm: 0.02870755083858967\n",
      "Step: 19290, train/learning_rate: 2.7046644390793517e-05\n",
      "Step: 19290, train/epoch: 4.590671062469482\n",
      "Step: 19300, train/loss: 0.0\n",
      "Step: 19300, train/grad_norm: 0.005220290273427963\n",
      "Step: 19300, train/learning_rate: 2.703474456211552e-05\n",
      "Step: 19300, train/epoch: 4.593050956726074\n",
      "Step: 19310, train/loss: 0.0\n",
      "Step: 19310, train/grad_norm: 0.001828560372814536\n",
      "Step: 19310, train/learning_rate: 2.7022846552426927e-05\n",
      "Step: 19310, train/epoch: 4.595430850982666\n",
      "Step: 19320, train/loss: 0.0\n",
      "Step: 19320, train/grad_norm: 0.0032460836227983236\n",
      "Step: 19320, train/learning_rate: 2.701094672374893e-05\n",
      "Step: 19320, train/epoch: 4.597810745239258\n",
      "Step: 19330, train/loss: 0.0\n",
      "Step: 19330, train/grad_norm: 0.0016879476606845856\n",
      "Step: 19330, train/learning_rate: 2.6999048714060336e-05\n",
      "Step: 19330, train/epoch: 4.600190162658691\n",
      "Step: 19340, train/loss: 0.0\n",
      "Step: 19340, train/grad_norm: 0.002074766205623746\n",
      "Step: 19340, train/learning_rate: 2.698714888538234e-05\n",
      "Step: 19340, train/epoch: 4.602570056915283\n",
      "Step: 19350, train/loss: 0.0\n",
      "Step: 19350, train/grad_norm: 0.001208232599310577\n",
      "Step: 19350, train/learning_rate: 2.6975249056704342e-05\n",
      "Step: 19350, train/epoch: 4.604949951171875\n",
      "Step: 19360, train/loss: 0.0\n",
      "Step: 19360, train/grad_norm: 0.00281251291744411\n",
      "Step: 19360, train/learning_rate: 2.696335104701575e-05\n",
      "Step: 19360, train/epoch: 4.607329845428467\n",
      "Step: 19370, train/loss: 0.0\n",
      "Step: 19370, train/grad_norm: 0.0017042490653693676\n",
      "Step: 19370, train/learning_rate: 2.6951451218337752e-05\n",
      "Step: 19370, train/epoch: 4.609709739685059\n",
      "Step: 19380, train/loss: 0.0\n",
      "Step: 19380, train/grad_norm: 0.001353114959783852\n",
      "Step: 19380, train/learning_rate: 2.693955320864916e-05\n",
      "Step: 19380, train/epoch: 4.61208963394165\n",
      "Step: 19390, train/loss: 0.0\n",
      "Step: 19390, train/grad_norm: 0.0006850560894235969\n",
      "Step: 19390, train/learning_rate: 2.692765337997116e-05\n",
      "Step: 19390, train/epoch: 4.614469528198242\n",
      "Step: 19400, train/loss: 0.0\n",
      "Step: 19400, train/grad_norm: 0.0020431503653526306\n",
      "Step: 19400, train/learning_rate: 2.6915753551293164e-05\n",
      "Step: 19400, train/epoch: 4.616848945617676\n",
      "Step: 19410, train/loss: 0.0\n",
      "Step: 19410, train/grad_norm: 0.0033486387692391872\n",
      "Step: 19410, train/learning_rate: 2.690385554160457e-05\n",
      "Step: 19410, train/epoch: 4.619228839874268\n",
      "Step: 19420, train/loss: 0.0\n",
      "Step: 19420, train/grad_norm: 0.005054757930338383\n",
      "Step: 19420, train/learning_rate: 2.6891955712926574e-05\n",
      "Step: 19420, train/epoch: 4.621608734130859\n",
      "Step: 19430, train/loss: 0.0\n",
      "Step: 19430, train/grad_norm: 0.002625246299430728\n",
      "Step: 19430, train/learning_rate: 2.688005770323798e-05\n",
      "Step: 19430, train/epoch: 4.623988628387451\n",
      "Step: 19440, train/loss: 0.0\n",
      "Step: 19440, train/grad_norm: 0.0033942132722586393\n",
      "Step: 19440, train/learning_rate: 2.6868157874559984e-05\n",
      "Step: 19440, train/epoch: 4.626368522644043\n",
      "Step: 19450, train/loss: 0.0\n",
      "Step: 19450, train/grad_norm: 0.0008695707656443119\n",
      "Step: 19450, train/learning_rate: 2.6856258045881987e-05\n",
      "Step: 19450, train/epoch: 4.628748416900635\n",
      "Step: 19460, train/loss: 0.0\n",
      "Step: 19460, train/grad_norm: 0.009250268340110779\n",
      "Step: 19460, train/learning_rate: 2.6844360036193393e-05\n",
      "Step: 19460, train/epoch: 4.631127834320068\n",
      "Step: 19470, train/loss: 0.15860000252723694\n",
      "Step: 19470, train/grad_norm: 0.001658891444094479\n",
      "Step: 19470, train/learning_rate: 2.6832460207515396e-05\n",
      "Step: 19470, train/epoch: 4.63350772857666\n",
      "Step: 19480, train/loss: 0.0\n",
      "Step: 19480, train/grad_norm: 0.01952735334634781\n",
      "Step: 19480, train/learning_rate: 2.6820562197826803e-05\n",
      "Step: 19480, train/epoch: 4.635887622833252\n",
      "Step: 19490, train/loss: 0.0\n",
      "Step: 19490, train/grad_norm: 0.009468420408666134\n",
      "Step: 19490, train/learning_rate: 2.6808662369148806e-05\n",
      "Step: 19490, train/epoch: 4.638267517089844\n",
      "Step: 19500, train/loss: 0.0\n",
      "Step: 19500, train/grad_norm: 0.034529779106378555\n",
      "Step: 19500, train/learning_rate: 2.679676254047081e-05\n",
      "Step: 19500, train/epoch: 4.6406474113464355\n",
      "Step: 19510, train/loss: 0.0\n",
      "Step: 19510, train/grad_norm: 0.0026605334132909775\n",
      "Step: 19510, train/learning_rate: 2.6784864530782215e-05\n",
      "Step: 19510, train/epoch: 4.643027305603027\n",
      "Step: 19520, train/loss: 0.0\n",
      "Step: 19520, train/grad_norm: 0.003636664478108287\n",
      "Step: 19520, train/learning_rate: 2.6772964702104218e-05\n",
      "Step: 19520, train/epoch: 4.645406723022461\n",
      "Step: 19530, train/loss: 0.0\n",
      "Step: 19530, train/grad_norm: 0.0027338899672031403\n",
      "Step: 19530, train/learning_rate: 2.6761066692415625e-05\n",
      "Step: 19530, train/epoch: 4.647786617279053\n",
      "Step: 19540, train/loss: 0.0\n",
      "Step: 19540, train/grad_norm: 0.0005535303498618305\n",
      "Step: 19540, train/learning_rate: 2.6749166863737628e-05\n",
      "Step: 19540, train/epoch: 4.6501665115356445\n",
      "Step: 19550, train/loss: 0.0\n",
      "Step: 19550, train/grad_norm: 0.0033066256437450647\n",
      "Step: 19550, train/learning_rate: 2.6737268854049034e-05\n",
      "Step: 19550, train/epoch: 4.652546405792236\n",
      "Step: 19560, train/loss: 0.0\n",
      "Step: 19560, train/grad_norm: 0.0016490767011418939\n",
      "Step: 19560, train/learning_rate: 2.6725369025371037e-05\n",
      "Step: 19560, train/epoch: 4.654926300048828\n",
      "Step: 19570, train/loss: 0.0\n",
      "Step: 19570, train/grad_norm: 0.0022263885475695133\n",
      "Step: 19570, train/learning_rate: 2.671346919669304e-05\n",
      "Step: 19570, train/epoch: 4.65730619430542\n",
      "Step: 19580, train/loss: 0.0\n",
      "Step: 19580, train/grad_norm: 0.002357420977205038\n",
      "Step: 19580, train/learning_rate: 2.6701571187004447e-05\n",
      "Step: 19580, train/epoch: 4.659686088562012\n",
      "Step: 19590, train/loss: 0.0\n",
      "Step: 19590, train/grad_norm: 0.002061385428532958\n",
      "Step: 19590, train/learning_rate: 2.668967135832645e-05\n",
      "Step: 19590, train/epoch: 4.662065505981445\n",
      "Step: 19600, train/loss: 0.0\n",
      "Step: 19600, train/grad_norm: 0.0007031455170363188\n",
      "Step: 19600, train/learning_rate: 2.6677773348637857e-05\n",
      "Step: 19600, train/epoch: 4.664445400238037\n",
      "Step: 19610, train/loss: 0.0\n",
      "Step: 19610, train/grad_norm: 0.0011899055680260062\n",
      "Step: 19610, train/learning_rate: 2.666587351995986e-05\n",
      "Step: 19610, train/epoch: 4.666825294494629\n",
      "Step: 19620, train/loss: 0.0\n",
      "Step: 19620, train/grad_norm: 0.0006053833640180528\n",
      "Step: 19620, train/learning_rate: 2.6653973691281863e-05\n",
      "Step: 19620, train/epoch: 4.669205188751221\n",
      "Step: 19630, train/loss: 0.0\n",
      "Step: 19630, train/grad_norm: 0.0012776509393006563\n",
      "Step: 19630, train/learning_rate: 2.664207568159327e-05\n",
      "Step: 19630, train/epoch: 4.6715850830078125\n",
      "Step: 19640, train/loss: 0.0\n",
      "Step: 19640, train/grad_norm: 0.0008590110810473561\n",
      "Step: 19640, train/learning_rate: 2.6630175852915272e-05\n",
      "Step: 19640, train/epoch: 4.673964977264404\n",
      "Step: 19650, train/loss: 0.0\n",
      "Step: 19650, train/grad_norm: 0.00030909391352906823\n",
      "Step: 19650, train/learning_rate: 2.661827784322668e-05\n",
      "Step: 19650, train/epoch: 4.676344394683838\n",
      "Step: 19660, train/loss: 0.11089999973773956\n",
      "Step: 19660, train/grad_norm: 0.0006976569420658052\n",
      "Step: 19660, train/learning_rate: 2.660637801454868e-05\n",
      "Step: 19660, train/epoch: 4.67872428894043\n",
      "Step: 19670, train/loss: 0.021900000050663948\n",
      "Step: 19670, train/grad_norm: 0.0005584409227594733\n",
      "Step: 19670, train/learning_rate: 2.6594478185870685e-05\n",
      "Step: 19670, train/epoch: 4.6811041831970215\n",
      "Step: 19680, train/loss: 0.0\n",
      "Step: 19680, train/grad_norm: 0.0037040519528090954\n",
      "Step: 19680, train/learning_rate: 2.658258017618209e-05\n",
      "Step: 19680, train/epoch: 4.683484077453613\n",
      "Step: 19690, train/loss: 0.0\n",
      "Step: 19690, train/grad_norm: 0.0007828077068552375\n",
      "Step: 19690, train/learning_rate: 2.6570680347504094e-05\n",
      "Step: 19690, train/epoch: 4.685863971710205\n",
      "Step: 19700, train/loss: 0.0\n",
      "Step: 19700, train/grad_norm: 0.0007073223241604865\n",
      "Step: 19700, train/learning_rate: 2.65587823378155e-05\n",
      "Step: 19700, train/epoch: 4.688243865966797\n",
      "Step: 19710, train/loss: 0.0\n",
      "Step: 19710, train/grad_norm: 0.0010048256954178214\n",
      "Step: 19710, train/learning_rate: 2.6546882509137504e-05\n",
      "Step: 19710, train/epoch: 4.6906232833862305\n",
      "Step: 19720, train/loss: 0.1445000022649765\n",
      "Step: 19720, train/grad_norm: 0.007602545898407698\n",
      "Step: 19720, train/learning_rate: 2.6534982680459507e-05\n",
      "Step: 19720, train/epoch: 4.693003177642822\n",
      "Step: 19730, train/loss: 0.0\n",
      "Step: 19730, train/grad_norm: 0.004915244411677122\n",
      "Step: 19730, train/learning_rate: 2.6523084670770913e-05\n",
      "Step: 19730, train/epoch: 4.695383071899414\n",
      "Step: 19740, train/loss: 0.0\n",
      "Step: 19740, train/grad_norm: 0.006184978876262903\n",
      "Step: 19740, train/learning_rate: 2.6511184842092916e-05\n",
      "Step: 19740, train/epoch: 4.697762966156006\n",
      "Step: 19750, train/loss: 0.0\n",
      "Step: 19750, train/grad_norm: 0.00860199797898531\n",
      "Step: 19750, train/learning_rate: 2.6499286832404323e-05\n",
      "Step: 19750, train/epoch: 4.700142860412598\n",
      "Step: 19760, train/loss: 0.0\n",
      "Step: 19760, train/grad_norm: 0.015849746763706207\n",
      "Step: 19760, train/learning_rate: 2.6487387003726326e-05\n",
      "Step: 19760, train/epoch: 4.7025227546691895\n",
      "Step: 19770, train/loss: 0.0\n",
      "Step: 19770, train/grad_norm: 0.011773255653679371\n",
      "Step: 19770, train/learning_rate: 2.647548717504833e-05\n",
      "Step: 19770, train/epoch: 4.704902648925781\n",
      "Step: 19780, train/loss: 0.11249999701976776\n",
      "Step: 19780, train/grad_norm: 0.006819837726652622\n",
      "Step: 19780, train/learning_rate: 2.6463589165359735e-05\n",
      "Step: 19780, train/epoch: 4.707282066345215\n",
      "Step: 19790, train/loss: 0.0\n",
      "Step: 19790, train/grad_norm: 0.032987069338560104\n",
      "Step: 19790, train/learning_rate: 2.645168933668174e-05\n",
      "Step: 19790, train/epoch: 4.709661960601807\n",
      "Step: 19800, train/loss: 0.0\n",
      "Step: 19800, train/grad_norm: 0.009538696147501469\n",
      "Step: 19800, train/learning_rate: 2.6439791326993145e-05\n",
      "Step: 19800, train/epoch: 4.712041854858398\n",
      "Step: 19810, train/loss: 0.0\n",
      "Step: 19810, train/grad_norm: 0.07878129184246063\n",
      "Step: 19810, train/learning_rate: 2.6427891498315148e-05\n",
      "Step: 19810, train/epoch: 4.71442174911499\n",
      "Step: 19820, train/loss: 0.0\n",
      "Step: 19820, train/grad_norm: 0.02397845685482025\n",
      "Step: 19820, train/learning_rate: 2.641599166963715e-05\n",
      "Step: 19820, train/epoch: 4.716801643371582\n",
      "Step: 19830, train/loss: 0.0\n",
      "Step: 19830, train/grad_norm: 0.002423446625471115\n",
      "Step: 19830, train/learning_rate: 2.6404093659948558e-05\n",
      "Step: 19830, train/epoch: 4.719181537628174\n",
      "Step: 19840, train/loss: 0.0\n",
      "Step: 19840, train/grad_norm: 0.02265576645731926\n",
      "Step: 19840, train/learning_rate: 2.639219383127056e-05\n",
      "Step: 19840, train/epoch: 4.721560955047607\n",
      "Step: 19850, train/loss: 0.0\n",
      "Step: 19850, train/grad_norm: 0.002463614335283637\n",
      "Step: 19850, train/learning_rate: 2.6380295821581967e-05\n",
      "Step: 19850, train/epoch: 4.723940849304199\n",
      "Step: 19860, train/loss: 0.0\n",
      "Step: 19860, train/grad_norm: 0.0009705862612463534\n",
      "Step: 19860, train/learning_rate: 2.636839599290397e-05\n",
      "Step: 19860, train/epoch: 4.726320743560791\n",
      "Step: 19870, train/loss: 0.0\n",
      "Step: 19870, train/grad_norm: 0.0016691857017576694\n",
      "Step: 19870, train/learning_rate: 2.6356496164225973e-05\n",
      "Step: 19870, train/epoch: 4.728700637817383\n",
      "Step: 19880, train/loss: 0.0\n",
      "Step: 19880, train/grad_norm: 0.0010576435597613454\n",
      "Step: 19880, train/learning_rate: 2.634459815453738e-05\n",
      "Step: 19880, train/epoch: 4.731080532073975\n",
      "Step: 19890, train/loss: 0.12030000239610672\n",
      "Step: 19890, train/grad_norm: 0.16275368630886078\n",
      "Step: 19890, train/learning_rate: 2.6332698325859383e-05\n",
      "Step: 19890, train/epoch: 4.733460426330566\n",
      "Step: 19900, train/loss: 0.00019999999494757503\n",
      "Step: 19900, train/grad_norm: 0.0021654837764799595\n",
      "Step: 19900, train/learning_rate: 2.632080031617079e-05\n",
      "Step: 19900, train/epoch: 4.73583984375\n",
      "Step: 19910, train/loss: 0.08590000122785568\n",
      "Step: 19910, train/grad_norm: 0.0010029806289821863\n",
      "Step: 19910, train/learning_rate: 2.6308900487492792e-05\n",
      "Step: 19910, train/epoch: 4.738219738006592\n",
      "Step: 19920, train/loss: 0.0\n",
      "Step: 19920, train/grad_norm: 0.0017860031221061945\n",
      "Step: 19920, train/learning_rate: 2.6297000658814795e-05\n",
      "Step: 19920, train/epoch: 4.740599632263184\n",
      "Step: 19930, train/loss: 0.11490000039339066\n",
      "Step: 19930, train/grad_norm: 0.051864948123693466\n",
      "Step: 19930, train/learning_rate: 2.6285102649126202e-05\n",
      "Step: 19930, train/epoch: 4.742979526519775\n",
      "Step: 19940, train/loss: 0.00039999998989515007\n",
      "Step: 19940, train/grad_norm: 0.0008989290217868984\n",
      "Step: 19940, train/learning_rate: 2.6273202820448205e-05\n",
      "Step: 19940, train/epoch: 4.745359420776367\n",
      "Step: 19950, train/loss: 0.0\n",
      "Step: 19950, train/grad_norm: 0.0016006424557417631\n",
      "Step: 19950, train/learning_rate: 2.626130481075961e-05\n",
      "Step: 19950, train/epoch: 4.747739315032959\n",
      "Step: 19960, train/loss: 0.0\n",
      "Step: 19960, train/grad_norm: 0.006738692056387663\n",
      "Step: 19960, train/learning_rate: 2.6249404982081614e-05\n",
      "Step: 19960, train/epoch: 4.750119209289551\n",
      "Step: 19970, train/loss: 0.0\n",
      "Step: 19970, train/grad_norm: 0.0003427426563575864\n",
      "Step: 19970, train/learning_rate: 2.6237505153403617e-05\n",
      "Step: 19970, train/epoch: 4.752498626708984\n",
      "Step: 19980, train/loss: 0.1492999941110611\n",
      "Step: 19980, train/grad_norm: 0.02542043849825859\n",
      "Step: 19980, train/learning_rate: 2.6225607143715024e-05\n",
      "Step: 19980, train/epoch: 4.754878520965576\n",
      "Step: 19990, train/loss: 9.999999747378752e-05\n",
      "Step: 19990, train/grad_norm: 0.007047702558338642\n",
      "Step: 19990, train/learning_rate: 2.6213707315037027e-05\n",
      "Step: 19990, train/epoch: 4.757258415222168\n",
      "Step: 20000, train/loss: 0.0\n",
      "Step: 20000, train/grad_norm: 0.0006113450508564711\n",
      "Step: 20000, train/learning_rate: 2.6201809305348434e-05\n",
      "Step: 20000, train/epoch: 4.75963830947876\n",
      "Step: 20010, train/loss: 0.01269999984651804\n",
      "Step: 20010, train/grad_norm: 0.0037971902638673782\n",
      "Step: 20010, train/learning_rate: 2.6189909476670437e-05\n",
      "Step: 20010, train/epoch: 4.762018203735352\n",
      "Step: 20020, train/loss: 0.0\n",
      "Step: 20020, train/grad_norm: 0.0025312460493296385\n",
      "Step: 20020, train/learning_rate: 2.617800964799244e-05\n",
      "Step: 20020, train/epoch: 4.764398097991943\n",
      "Step: 20030, train/loss: 0.0\n",
      "Step: 20030, train/grad_norm: 0.0011793996673077345\n",
      "Step: 20030, train/learning_rate: 2.6166111638303846e-05\n",
      "Step: 20030, train/epoch: 4.766777515411377\n",
      "Step: 20040, train/loss: 0.0\n",
      "Step: 20040, train/grad_norm: 0.0008765324018895626\n",
      "Step: 20040, train/learning_rate: 2.615421180962585e-05\n",
      "Step: 20040, train/epoch: 4.769157409667969\n",
      "Step: 20050, train/loss: 0.0\n",
      "Step: 20050, train/grad_norm: 0.00034630426671355963\n",
      "Step: 20050, train/learning_rate: 2.6142313799937256e-05\n",
      "Step: 20050, train/epoch: 4.7715373039245605\n",
      "Step: 20060, train/loss: 0.0\n",
      "Step: 20060, train/grad_norm: 0.0052626715041697025\n",
      "Step: 20060, train/learning_rate: 2.613041397125926e-05\n",
      "Step: 20060, train/epoch: 4.773917198181152\n",
      "Step: 20070, train/loss: 0.0\n",
      "Step: 20070, train/grad_norm: 0.0004153485642746091\n",
      "Step: 20070, train/learning_rate: 2.6118514142581262e-05\n",
      "Step: 20070, train/epoch: 4.776297092437744\n",
      "Step: 20080, train/loss: 0.0\n",
      "Step: 20080, train/grad_norm: 0.0052429074421525\n",
      "Step: 20080, train/learning_rate: 2.6106616132892668e-05\n",
      "Step: 20080, train/epoch: 4.778676986694336\n",
      "Step: 20090, train/loss: 0.18129999935626984\n",
      "Step: 20090, train/grad_norm: 0.006353204138576984\n",
      "Step: 20090, train/learning_rate: 2.609471630421467e-05\n",
      "Step: 20090, train/epoch: 4.7810564041137695\n",
      "Step: 20100, train/loss: 0.0\n",
      "Step: 20100, train/grad_norm: 0.04275732487440109\n",
      "Step: 20100, train/learning_rate: 2.6082818294526078e-05\n",
      "Step: 20100, train/epoch: 4.783436298370361\n",
      "Step: 20110, train/loss: 0.0\n",
      "Step: 20110, train/grad_norm: 0.02241288684308529\n",
      "Step: 20110, train/learning_rate: 2.607091846584808e-05\n",
      "Step: 20110, train/epoch: 4.785816192626953\n",
      "Step: 20120, train/loss: 0.0\n",
      "Step: 20120, train/grad_norm: 0.00239848205819726\n",
      "Step: 20120, train/learning_rate: 2.6059018637170084e-05\n",
      "Step: 20120, train/epoch: 4.788196086883545\n",
      "Step: 20130, train/loss: 0.0\n",
      "Step: 20130, train/grad_norm: 0.0007761004962958395\n",
      "Step: 20130, train/learning_rate: 2.604712062748149e-05\n",
      "Step: 20130, train/epoch: 4.790575981140137\n",
      "Step: 20140, train/loss: 0.0\n",
      "Step: 20140, train/grad_norm: 0.0020058611407876015\n",
      "Step: 20140, train/learning_rate: 2.6035220798803493e-05\n",
      "Step: 20140, train/epoch: 4.7929558753967285\n",
      "Step: 20150, train/loss: 0.0\n",
      "Step: 20150, train/grad_norm: 0.0014218561118468642\n",
      "Step: 20150, train/learning_rate: 2.60233227891149e-05\n",
      "Step: 20150, train/epoch: 4.79533576965332\n",
      "Step: 20160, train/loss: 0.0\n",
      "Step: 20160, train/grad_norm: 0.0003835005045402795\n",
      "Step: 20160, train/learning_rate: 2.6011422960436903e-05\n",
      "Step: 20160, train/epoch: 4.797715187072754\n",
      "Step: 20170, train/loss: 0.0\n",
      "Step: 20170, train/grad_norm: 0.011141755618155003\n",
      "Step: 20170, train/learning_rate: 2.5999523131758906e-05\n",
      "Step: 20170, train/epoch: 4.800095081329346\n",
      "Step: 20180, train/loss: 0.0\n",
      "Step: 20180, train/grad_norm: 0.0006103025516495109\n",
      "Step: 20180, train/learning_rate: 2.5987625122070312e-05\n",
      "Step: 20180, train/epoch: 4.8024749755859375\n",
      "Step: 20190, train/loss: 0.0\n",
      "Step: 20190, train/grad_norm: 0.0006183551740832627\n",
      "Step: 20190, train/learning_rate: 2.5975725293392316e-05\n",
      "Step: 20190, train/epoch: 4.804854869842529\n",
      "Step: 20200, train/loss: 0.0\n",
      "Step: 20200, train/grad_norm: 0.0005547357141040266\n",
      "Step: 20200, train/learning_rate: 2.5963827283703722e-05\n",
      "Step: 20200, train/epoch: 4.807234764099121\n",
      "Step: 20210, train/loss: 0.0\n",
      "Step: 20210, train/grad_norm: 0.0003912274551112205\n",
      "Step: 20210, train/learning_rate: 2.5951927455025725e-05\n",
      "Step: 20210, train/epoch: 4.809614658355713\n",
      "Step: 20220, train/loss: 0.0\n",
      "Step: 20220, train/grad_norm: 0.0007428149692714214\n",
      "Step: 20220, train/learning_rate: 2.594002944533713e-05\n",
      "Step: 20220, train/epoch: 4.8119940757751465\n",
      "Step: 20230, train/loss: 0.0\n",
      "Step: 20230, train/grad_norm: 0.0003113495768047869\n",
      "Step: 20230, train/learning_rate: 2.5928129616659135e-05\n",
      "Step: 20230, train/epoch: 4.814373970031738\n",
      "Step: 20240, train/loss: 0.0\n",
      "Step: 20240, train/grad_norm: 0.0004140899982303381\n",
      "Step: 20240, train/learning_rate: 2.5916229787981138e-05\n",
      "Step: 20240, train/epoch: 4.81675386428833\n",
      "Step: 20250, train/loss: 0.0\n",
      "Step: 20250, train/grad_norm: 0.00027999983285553753\n",
      "Step: 20250, train/learning_rate: 2.5904331778292544e-05\n",
      "Step: 20250, train/epoch: 4.819133758544922\n",
      "Step: 20260, train/loss: 0.0\n",
      "Step: 20260, train/grad_norm: 0.0003352269995957613\n",
      "Step: 20260, train/learning_rate: 2.5892431949614547e-05\n",
      "Step: 20260, train/epoch: 4.821513652801514\n",
      "Step: 20270, train/loss: 0.0\n",
      "Step: 20270, train/grad_norm: 0.00024356995709240437\n",
      "Step: 20270, train/learning_rate: 2.5880533939925954e-05\n",
      "Step: 20270, train/epoch: 4.8238935470581055\n",
      "Step: 20280, train/loss: 0.0\n",
      "Step: 20280, train/grad_norm: 0.0026426182594150305\n",
      "Step: 20280, train/learning_rate: 2.5868634111247957e-05\n",
      "Step: 20280, train/epoch: 4.826273441314697\n",
      "Step: 20290, train/loss: 0.0\n",
      "Step: 20290, train/grad_norm: 0.0003097633889410645\n",
      "Step: 20290, train/learning_rate: 2.585673428256996e-05\n",
      "Step: 20290, train/epoch: 4.828652858734131\n",
      "Step: 20300, train/loss: 0.0\n",
      "Step: 20300, train/grad_norm: 0.00027928457711823285\n",
      "Step: 20300, train/learning_rate: 2.5844836272881366e-05\n",
      "Step: 20300, train/epoch: 4.831032752990723\n",
      "Step: 20310, train/loss: 0.0\n",
      "Step: 20310, train/grad_norm: 0.0010313461534678936\n",
      "Step: 20310, train/learning_rate: 2.583293644420337e-05\n",
      "Step: 20310, train/epoch: 4.8334126472473145\n",
      "Step: 20320, train/loss: 0.0\n",
      "Step: 20320, train/grad_norm: 0.00016550086729694158\n",
      "Step: 20320, train/learning_rate: 2.5821038434514776e-05\n",
      "Step: 20320, train/epoch: 4.835792541503906\n",
      "Step: 20330, train/loss: 0.0\n",
      "Step: 20330, train/grad_norm: 0.0008811591542325914\n",
      "Step: 20330, train/learning_rate: 2.580913860583678e-05\n",
      "Step: 20330, train/epoch: 4.838172435760498\n",
      "Step: 20340, train/loss: 0.0\n",
      "Step: 20340, train/grad_norm: 0.00017661243327893317\n",
      "Step: 20340, train/learning_rate: 2.5797238777158782e-05\n",
      "Step: 20340, train/epoch: 4.84055233001709\n",
      "Step: 20350, train/loss: 0.0\n",
      "Step: 20350, train/grad_norm: 0.0003453404933679849\n",
      "Step: 20350, train/learning_rate: 2.578534076747019e-05\n",
      "Step: 20350, train/epoch: 4.842931747436523\n",
      "Step: 20360, train/loss: 0.0\n",
      "Step: 20360, train/grad_norm: 0.0001704251189948991\n",
      "Step: 20360, train/learning_rate: 2.577344093879219e-05\n",
      "Step: 20360, train/epoch: 4.845311641693115\n",
      "Step: 20370, train/loss: 0.0\n",
      "Step: 20370, train/grad_norm: 0.0004217249806970358\n",
      "Step: 20370, train/learning_rate: 2.5761542929103598e-05\n",
      "Step: 20370, train/epoch: 4.847691535949707\n",
      "Step: 20380, train/loss: 0.0\n",
      "Step: 20380, train/grad_norm: 0.0002217115688836202\n",
      "Step: 20380, train/learning_rate: 2.57496431004256e-05\n",
      "Step: 20380, train/epoch: 4.850071430206299\n",
      "Step: 20390, train/loss: 0.0\n",
      "Step: 20390, train/grad_norm: 0.0009139023022726178\n",
      "Step: 20390, train/learning_rate: 2.5737743271747604e-05\n",
      "Step: 20390, train/epoch: 4.852451324462891\n",
      "Step: 20400, train/loss: 0.0\n",
      "Step: 20400, train/grad_norm: 0.00016823605983518064\n",
      "Step: 20400, train/learning_rate: 2.572584526205901e-05\n",
      "Step: 20400, train/epoch: 4.854831218719482\n",
      "Step: 20410, train/loss: 0.0\n",
      "Step: 20410, train/grad_norm: 0.0001355620042886585\n",
      "Step: 20410, train/learning_rate: 2.5713945433381014e-05\n",
      "Step: 20410, train/epoch: 4.857210636138916\n",
      "Step: 20420, train/loss: 0.0\n",
      "Step: 20420, train/grad_norm: 0.00038613073411397636\n",
      "Step: 20420, train/learning_rate: 2.570204742369242e-05\n",
      "Step: 20420, train/epoch: 4.859590530395508\n",
      "Step: 20430, train/loss: 0.0\n",
      "Step: 20430, train/grad_norm: 0.00029119194368831813\n",
      "Step: 20430, train/learning_rate: 2.5690147595014423e-05\n",
      "Step: 20430, train/epoch: 4.8619704246521\n",
      "Step: 20440, train/loss: 0.0\n",
      "Step: 20440, train/grad_norm: 0.00014632154488936067\n",
      "Step: 20440, train/learning_rate: 2.5678247766336426e-05\n",
      "Step: 20440, train/epoch: 4.864350318908691\n",
      "Step: 20450, train/loss: 0.0\n",
      "Step: 20450, train/grad_norm: 0.0002837145293597132\n",
      "Step: 20450, train/learning_rate: 2.5666349756647833e-05\n",
      "Step: 20450, train/epoch: 4.866730213165283\n",
      "Step: 20460, train/loss: 0.0\n",
      "Step: 20460, train/grad_norm: 0.0002911245683208108\n",
      "Step: 20460, train/learning_rate: 2.5654449927969836e-05\n",
      "Step: 20460, train/epoch: 4.869110107421875\n",
      "Step: 20470, train/loss: 0.0\n",
      "Step: 20470, train/grad_norm: 0.0001448151597287506\n",
      "Step: 20470, train/learning_rate: 2.5642551918281242e-05\n",
      "Step: 20470, train/epoch: 4.871490001678467\n",
      "Step: 20480, train/loss: 0.0\n",
      "Step: 20480, train/grad_norm: 5.09744968439918e-05\n",
      "Step: 20480, train/learning_rate: 2.5630652089603245e-05\n",
      "Step: 20480, train/epoch: 4.8738694190979\n",
      "Step: 20490, train/loss: 0.0\n",
      "Step: 20490, train/grad_norm: 0.0003318519738968462\n",
      "Step: 20490, train/learning_rate: 2.5618752260925248e-05\n",
      "Step: 20490, train/epoch: 4.876249313354492\n",
      "Step: 20500, train/loss: 0.0\n",
      "Step: 20500, train/grad_norm: 0.00013304523599799722\n",
      "Step: 20500, train/learning_rate: 2.5606854251236655e-05\n",
      "Step: 20500, train/epoch: 4.878629207611084\n",
      "Step: 20510, train/loss: 0.0\n",
      "Step: 20510, train/grad_norm: 0.00011392087617423385\n",
      "Step: 20510, train/learning_rate: 2.5594954422558658e-05\n",
      "Step: 20510, train/epoch: 4.881009101867676\n",
      "Step: 20520, train/loss: 0.0\n",
      "Step: 20520, train/grad_norm: 0.00012027124466840178\n",
      "Step: 20520, train/learning_rate: 2.5583056412870064e-05\n",
      "Step: 20520, train/epoch: 4.883388996124268\n",
      "Step: 20530, train/loss: 0.0\n",
      "Step: 20530, train/grad_norm: 0.0005091935745440423\n",
      "Step: 20530, train/learning_rate: 2.5571156584192067e-05\n",
      "Step: 20530, train/epoch: 4.885768890380859\n",
      "Step: 20540, train/loss: 0.0\n",
      "Step: 20540, train/grad_norm: 0.00018312617612536997\n",
      "Step: 20540, train/learning_rate: 2.555925675551407e-05\n",
      "Step: 20540, train/epoch: 4.888148307800293\n",
      "Step: 20550, train/loss: 0.0\n",
      "Step: 20550, train/grad_norm: 0.00019965128740295768\n",
      "Step: 20550, train/learning_rate: 2.5547358745825477e-05\n",
      "Step: 20550, train/epoch: 4.890528202056885\n",
      "Step: 20560, train/loss: 0.0\n",
      "Step: 20560, train/grad_norm: 0.0007718098349869251\n",
      "Step: 20560, train/learning_rate: 2.553545891714748e-05\n",
      "Step: 20560, train/epoch: 4.892908096313477\n",
      "Step: 20570, train/loss: 0.0\n",
      "Step: 20570, train/grad_norm: 9.806273737922311e-05\n",
      "Step: 20570, train/learning_rate: 2.5523560907458887e-05\n",
      "Step: 20570, train/epoch: 4.895287990570068\n",
      "Step: 20580, train/loss: 0.17730000615119934\n",
      "Step: 20580, train/grad_norm: 0.0008366380352526903\n",
      "Step: 20580, train/learning_rate: 2.551166107878089e-05\n",
      "Step: 20580, train/epoch: 4.89766788482666\n",
      "Step: 20590, train/loss: 0.0\n",
      "Step: 20590, train/grad_norm: 0.02919350191950798\n",
      "Step: 20590, train/learning_rate: 2.5499761250102893e-05\n",
      "Step: 20590, train/epoch: 4.900047779083252\n",
      "Step: 20600, train/loss: 9.999999747378752e-05\n",
      "Step: 20600, train/grad_norm: 0.011818134225904942\n",
      "Step: 20600, train/learning_rate: 2.54878632404143e-05\n",
      "Step: 20600, train/epoch: 4.9024271965026855\n",
      "Step: 20610, train/loss: 0.0\n",
      "Step: 20610, train/grad_norm: 0.004087124485522509\n",
      "Step: 20610, train/learning_rate: 2.5475963411736302e-05\n",
      "Step: 20610, train/epoch: 4.904807090759277\n",
      "Step: 20620, train/loss: 0.0\n",
      "Step: 20620, train/grad_norm: 0.0016687462339177728\n",
      "Step: 20620, train/learning_rate: 2.546406540204771e-05\n",
      "Step: 20620, train/epoch: 4.907186985015869\n",
      "Step: 20630, train/loss: 0.0\n",
      "Step: 20630, train/grad_norm: 0.003452551318332553\n",
      "Step: 20630, train/learning_rate: 2.545216557336971e-05\n",
      "Step: 20630, train/epoch: 4.909566879272461\n",
      "Step: 20640, train/loss: 0.0\n",
      "Step: 20640, train/grad_norm: 0.0010520482901483774\n",
      "Step: 20640, train/learning_rate: 2.5440265744691715e-05\n",
      "Step: 20640, train/epoch: 4.911946773529053\n",
      "Step: 20650, train/loss: 0.0\n",
      "Step: 20650, train/grad_norm: 0.0012805325677618384\n",
      "Step: 20650, train/learning_rate: 2.542836773500312e-05\n",
      "Step: 20650, train/epoch: 4.9143266677856445\n",
      "Step: 20660, train/loss: 0.0\n",
      "Step: 20660, train/grad_norm: 0.001504069077782333\n",
      "Step: 20660, train/learning_rate: 2.5416467906325124e-05\n",
      "Step: 20660, train/epoch: 4.916706562042236\n",
      "Step: 20670, train/loss: 0.0\n",
      "Step: 20670, train/grad_norm: 0.0015761752147227526\n",
      "Step: 20670, train/learning_rate: 2.540456989663653e-05\n",
      "Step: 20670, train/epoch: 4.91908597946167\n",
      "Step: 20680, train/loss: 0.0\n",
      "Step: 20680, train/grad_norm: 0.0009061300079338253\n",
      "Step: 20680, train/learning_rate: 2.5392670067958534e-05\n",
      "Step: 20680, train/epoch: 4.921465873718262\n",
      "Step: 20690, train/loss: 0.0\n",
      "Step: 20690, train/grad_norm: 0.0013168390141800046\n",
      "Step: 20690, train/learning_rate: 2.5380770239280537e-05\n",
      "Step: 20690, train/epoch: 4.9238457679748535\n",
      "Step: 20700, train/loss: 0.0\n",
      "Step: 20700, train/grad_norm: 0.001016460359096527\n",
      "Step: 20700, train/learning_rate: 2.5368872229591943e-05\n",
      "Step: 20700, train/epoch: 4.926225662231445\n",
      "Step: 20710, train/loss: 0.11949999630451202\n",
      "Step: 20710, train/grad_norm: 0.0035873050801455975\n",
      "Step: 20710, train/learning_rate: 2.5356972400913946e-05\n",
      "Step: 20710, train/epoch: 4.928605556488037\n",
      "Step: 20720, train/loss: 9.999999747378752e-05\n",
      "Step: 20720, train/grad_norm: 0.25708189606666565\n",
      "Step: 20720, train/learning_rate: 2.5345074391225353e-05\n",
      "Step: 20720, train/epoch: 4.930985450744629\n",
      "Step: 20730, train/loss: 9.999999747378752e-05\n",
      "Step: 20730, train/grad_norm: 0.005668969824910164\n",
      "Step: 20730, train/learning_rate: 2.5333174562547356e-05\n",
      "Step: 20730, train/epoch: 4.9333648681640625\n",
      "Step: 20740, train/loss: 0.0\n",
      "Step: 20740, train/grad_norm: 0.0048203240148723125\n",
      "Step: 20740, train/learning_rate: 2.532127473386936e-05\n",
      "Step: 20740, train/epoch: 4.935744762420654\n",
      "Step: 20750, train/loss: 0.0\n",
      "Step: 20750, train/grad_norm: 0.0013320420403033495\n",
      "Step: 20750, train/learning_rate: 2.5309376724180765e-05\n",
      "Step: 20750, train/epoch: 4.938124656677246\n",
      "Step: 20760, train/loss: 0.12890000641345978\n",
      "Step: 20760, train/grad_norm: 0.004004830494523048\n",
      "Step: 20760, train/learning_rate: 2.529747689550277e-05\n",
      "Step: 20760, train/epoch: 4.940504550933838\n",
      "Step: 20770, train/loss: 0.0\n",
      "Step: 20770, train/grad_norm: 0.011869307607412338\n",
      "Step: 20770, train/learning_rate: 2.5285578885814175e-05\n",
      "Step: 20770, train/epoch: 4.94288444519043\n",
      "Step: 20780, train/loss: 0.00019999999494757503\n",
      "Step: 20780, train/grad_norm: 0.0024333333130925894\n",
      "Step: 20780, train/learning_rate: 2.5273679057136178e-05\n",
      "Step: 20780, train/epoch: 4.9452643394470215\n",
      "Step: 20790, train/loss: 0.0\n",
      "Step: 20790, train/grad_norm: 0.0018077280838042498\n",
      "Step: 20790, train/learning_rate: 2.526177922845818e-05\n",
      "Step: 20790, train/epoch: 4.947643756866455\n",
      "Step: 20800, train/loss: 0.0\n",
      "Step: 20800, train/grad_norm: 0.00041631085332483053\n",
      "Step: 20800, train/learning_rate: 2.5249881218769588e-05\n",
      "Step: 20800, train/epoch: 4.950023651123047\n",
      "Step: 20810, train/loss: 0.11949999630451202\n",
      "Step: 20810, train/grad_norm: 0.002408315660431981\n",
      "Step: 20810, train/learning_rate: 2.523798139009159e-05\n",
      "Step: 20810, train/epoch: 4.952403545379639\n",
      "Step: 20820, train/loss: 0.0\n",
      "Step: 20820, train/grad_norm: 0.006510264705866575\n",
      "Step: 20820, train/learning_rate: 2.5226083380402997e-05\n",
      "Step: 20820, train/epoch: 4.9547834396362305\n",
      "Step: 20830, train/loss: 0.0\n",
      "Step: 20830, train/grad_norm: 0.002442676806822419\n",
      "Step: 20830, train/learning_rate: 2.5214183551725e-05\n",
      "Step: 20830, train/epoch: 4.957163333892822\n",
      "Step: 20840, train/loss: 0.0\n",
      "Step: 20840, train/grad_norm: 0.011361664161086082\n",
      "Step: 20840, train/learning_rate: 2.5202283723047003e-05\n",
      "Step: 20840, train/epoch: 4.959543228149414\n",
      "Step: 20850, train/loss: 0.0\n",
      "Step: 20850, train/grad_norm: 0.0024456516839563847\n",
      "Step: 20850, train/learning_rate: 2.519038571335841e-05\n",
      "Step: 20850, train/epoch: 4.961923122406006\n",
      "Step: 20860, train/loss: 0.13130000233650208\n",
      "Step: 20860, train/grad_norm: 0.0038089153822511435\n",
      "Step: 20860, train/learning_rate: 2.5178485884680413e-05\n",
      "Step: 20860, train/epoch: 4.9643025398254395\n",
      "Step: 20870, train/loss: 9.999999747378752e-05\n",
      "Step: 20870, train/grad_norm: 0.025726674124598503\n",
      "Step: 20870, train/learning_rate: 2.516658787499182e-05\n",
      "Step: 20870, train/epoch: 4.966682434082031\n",
      "Step: 20880, train/loss: 0.0\n",
      "Step: 20880, train/grad_norm: 0.0017347844550386071\n",
      "Step: 20880, train/learning_rate: 2.5154688046313822e-05\n",
      "Step: 20880, train/epoch: 4.969062328338623\n",
      "Step: 20890, train/loss: 0.0\n",
      "Step: 20890, train/grad_norm: 0.0027706644032150507\n",
      "Step: 20890, train/learning_rate: 2.514279003662523e-05\n",
      "Step: 20890, train/epoch: 4.971442222595215\n",
      "Step: 20900, train/loss: 0.0\n",
      "Step: 20900, train/grad_norm: 0.0003508978115860373\n",
      "Step: 20900, train/learning_rate: 2.5130890207947232e-05\n",
      "Step: 20900, train/epoch: 4.973822116851807\n",
      "Step: 20910, train/loss: 0.0\n",
      "Step: 20910, train/grad_norm: 0.0010057104518637061\n",
      "Step: 20910, train/learning_rate: 2.5118990379269235e-05\n",
      "Step: 20910, train/epoch: 4.976202011108398\n",
      "Step: 20920, train/loss: 0.0\n",
      "Step: 20920, train/grad_norm: 0.0015365062281489372\n",
      "Step: 20920, train/learning_rate: 2.510709236958064e-05\n",
      "Step: 20920, train/epoch: 4.978581428527832\n",
      "Step: 20930, train/loss: 0.0\n",
      "Step: 20930, train/grad_norm: 0.0016797182615846395\n",
      "Step: 20930, train/learning_rate: 2.5095192540902644e-05\n",
      "Step: 20930, train/epoch: 4.980961322784424\n",
      "Step: 20940, train/loss: 0.0\n",
      "Step: 20940, train/grad_norm: 0.0005941962008364499\n",
      "Step: 20940, train/learning_rate: 2.508329453121405e-05\n",
      "Step: 20940, train/epoch: 4.983341217041016\n",
      "Step: 20950, train/loss: 0.0\n",
      "Step: 20950, train/grad_norm: 0.0006628461414948106\n",
      "Step: 20950, train/learning_rate: 2.5071394702536054e-05\n",
      "Step: 20950, train/epoch: 4.985721111297607\n",
      "Step: 20960, train/loss: 0.0\n",
      "Step: 20960, train/grad_norm: 0.0011411590967327356\n",
      "Step: 20960, train/learning_rate: 2.5059494873858057e-05\n",
      "Step: 20960, train/epoch: 4.988101005554199\n",
      "Step: 20970, train/loss: 0.0\n",
      "Step: 20970, train/grad_norm: 0.0011642847675830126\n",
      "Step: 20970, train/learning_rate: 2.5047596864169464e-05\n",
      "Step: 20970, train/epoch: 4.990480899810791\n",
      "Step: 20980, train/loss: 0.0\n",
      "Step: 20980, train/grad_norm: 0.00029959421954117715\n",
      "Step: 20980, train/learning_rate: 2.5035697035491467e-05\n",
      "Step: 20980, train/epoch: 4.992860317230225\n",
      "Step: 20990, train/loss: 0.0\n",
      "Step: 20990, train/grad_norm: 0.0007893560105003417\n",
      "Step: 20990, train/learning_rate: 2.5023799025802873e-05\n",
      "Step: 20990, train/epoch: 4.995240211486816\n",
      "Step: 21000, train/loss: 0.0\n",
      "Step: 21000, train/grad_norm: 0.0013983966782689095\n",
      "Step: 21000, train/learning_rate: 2.5011899197124876e-05\n",
      "Step: 21000, train/epoch: 4.997620105743408\n",
      "Step: 21010, train/loss: 0.0\n",
      "Step: 21010, train/grad_norm: 0.00012998731108382344\n",
      "Step: 21010, train/learning_rate: 2.499999936844688e-05\n",
      "Step: 21010, train/epoch: 5.0\n",
      "Step: 21010, eval/loss: 0.028315206989645958\n",
      "Step: 21010, eval/accuracy: 0.9969457387924194\n",
      "Step: 21010, eval/f1: 0.9967746138572693\n",
      "Step: 21010, eval/runtime: 857.7152709960938\n",
      "Step: 21010, eval/samples_per_second: 8.39799976348877\n",
      "Step: 21010, eval/steps_per_second: 1.0499999523162842\n",
      "Step: 21010, train/epoch: 5.0\n",
      "Step: 21020, train/loss: 0.0\n",
      "Step: 21020, train/grad_norm: 0.0005413373000919819\n",
      "Step: 21020, train/learning_rate: 2.4988101358758286e-05\n",
      "Step: 21020, train/epoch: 5.002379894256592\n",
      "Step: 21030, train/loss: 0.0\n",
      "Step: 21030, train/grad_norm: 0.00021878421830479056\n",
      "Step: 21030, train/learning_rate: 2.497620153008029e-05\n",
      "Step: 21030, train/epoch: 5.004759788513184\n",
      "Step: 21040, train/loss: 0.0\n",
      "Step: 21040, train/grad_norm: 0.0003092670813202858\n",
      "Step: 21040, train/learning_rate: 2.4964303520391695e-05\n",
      "Step: 21040, train/epoch: 5.007139682769775\n",
      "Step: 21050, train/loss: 0.0\n",
      "Step: 21050, train/grad_norm: 0.00012083012552466244\n",
      "Step: 21050, train/learning_rate: 2.4952403691713698e-05\n",
      "Step: 21050, train/epoch: 5.009519100189209\n",
      "Step: 21060, train/loss: 0.0\n",
      "Step: 21060, train/grad_norm: 0.00014813506277278066\n",
      "Step: 21060, train/learning_rate: 2.49405038630357e-05\n",
      "Step: 21060, train/epoch: 5.011898994445801\n",
      "Step: 21070, train/loss: 0.0\n",
      "Step: 21070, train/grad_norm: 0.00023285204952117056\n",
      "Step: 21070, train/learning_rate: 2.4928605853347108e-05\n",
      "Step: 21070, train/epoch: 5.014278888702393\n",
      "Step: 21080, train/loss: 0.0\n",
      "Step: 21080, train/grad_norm: 0.0005742419743910432\n",
      "Step: 21080, train/learning_rate: 2.491670602466911e-05\n",
      "Step: 21080, train/epoch: 5.016658782958984\n",
      "Step: 21090, train/loss: 0.0\n",
      "Step: 21090, train/grad_norm: 0.00022966330288909376\n",
      "Step: 21090, train/learning_rate: 2.4904808014980517e-05\n",
      "Step: 21090, train/epoch: 5.019038677215576\n",
      "Step: 21100, train/loss: 0.0\n",
      "Step: 21100, train/grad_norm: 0.00020424777176231146\n",
      "Step: 21100, train/learning_rate: 2.489290818630252e-05\n",
      "Step: 21100, train/epoch: 5.021418571472168\n",
      "Step: 21110, train/loss: 0.0\n",
      "Step: 21110, train/grad_norm: 0.00030223646899685264\n",
      "Step: 21110, train/learning_rate: 2.4881008357624523e-05\n",
      "Step: 21110, train/epoch: 5.023797988891602\n",
      "Step: 21120, train/loss: 0.0\n",
      "Step: 21120, train/grad_norm: 7.582934631500393e-05\n",
      "Step: 21120, train/learning_rate: 2.486911034793593e-05\n",
      "Step: 21120, train/epoch: 5.026177883148193\n",
      "Step: 21130, train/loss: 0.0\n",
      "Step: 21130, train/grad_norm: 0.0015849733026698232\n",
      "Step: 21130, train/learning_rate: 2.4857210519257933e-05\n",
      "Step: 21130, train/epoch: 5.028557777404785\n",
      "Step: 21140, train/loss: 0.0\n",
      "Step: 21140, train/grad_norm: 0.0001353593688691035\n",
      "Step: 21140, train/learning_rate: 2.484531250956934e-05\n",
      "Step: 21140, train/epoch: 5.030937671661377\n",
      "Step: 21150, train/loss: 0.0\n",
      "Step: 21150, train/grad_norm: 0.0006568016833625734\n",
      "Step: 21150, train/learning_rate: 2.4833412680891342e-05\n",
      "Step: 21150, train/epoch: 5.033317565917969\n",
      "Step: 21160, train/loss: 0.0\n",
      "Step: 21160, train/grad_norm: 0.00018586331862024963\n",
      "Step: 21160, train/learning_rate: 2.4821512852213345e-05\n",
      "Step: 21160, train/epoch: 5.0356974601745605\n",
      "Step: 21170, train/loss: 0.0\n",
      "Step: 21170, train/grad_norm: 0.00013374179252423346\n",
      "Step: 21170, train/learning_rate: 2.4809614842524752e-05\n",
      "Step: 21170, train/epoch: 5.038076877593994\n",
      "Step: 21180, train/loss: 0.0\n",
      "Step: 21180, train/grad_norm: 0.0006012500962242484\n",
      "Step: 21180, train/learning_rate: 2.4797715013846755e-05\n",
      "Step: 21180, train/epoch: 5.040456771850586\n",
      "Step: 21190, train/loss: 0.0\n",
      "Step: 21190, train/grad_norm: 6.490616942755878e-05\n",
      "Step: 21190, train/learning_rate: 2.478581700415816e-05\n",
      "Step: 21190, train/epoch: 5.042836666107178\n",
      "Step: 21200, train/loss: 0.0\n",
      "Step: 21200, train/grad_norm: 0.00036447597085498273\n",
      "Step: 21200, train/learning_rate: 2.4773917175480165e-05\n",
      "Step: 21200, train/epoch: 5.0452165603637695\n",
      "Step: 21210, train/loss: 0.0\n",
      "Step: 21210, train/grad_norm: 0.0001919173082569614\n",
      "Step: 21210, train/learning_rate: 2.4762017346802168e-05\n",
      "Step: 21210, train/epoch: 5.047596454620361\n",
      "Step: 21220, train/loss: 0.0\n",
      "Step: 21220, train/grad_norm: 0.00021713382739108056\n",
      "Step: 21220, train/learning_rate: 2.4750119337113574e-05\n",
      "Step: 21220, train/epoch: 5.049976348876953\n",
      "Step: 21230, train/loss: 0.0\n",
      "Step: 21230, train/grad_norm: 0.0002771952422335744\n",
      "Step: 21230, train/learning_rate: 2.4738219508435577e-05\n",
      "Step: 21230, train/epoch: 5.052356243133545\n",
      "Step: 21240, train/loss: 0.0\n",
      "Step: 21240, train/grad_norm: 0.0002814267354551703\n",
      "Step: 21240, train/learning_rate: 2.4726321498746984e-05\n",
      "Step: 21240, train/epoch: 5.0547356605529785\n",
      "Step: 21250, train/loss: 0.0\n",
      "Step: 21250, train/grad_norm: 0.00027187305386178195\n",
      "Step: 21250, train/learning_rate: 2.4714421670068987e-05\n",
      "Step: 21250, train/epoch: 5.05711555480957\n",
      "Step: 21260, train/loss: 0.0\n",
      "Step: 21260, train/grad_norm: 0.00044800181058235466\n",
      "Step: 21260, train/learning_rate: 2.470252184139099e-05\n",
      "Step: 21260, train/epoch: 5.059495449066162\n",
      "Step: 21270, train/loss: 0.0\n",
      "Step: 21270, train/grad_norm: 0.00020160150597803295\n",
      "Step: 21270, train/learning_rate: 2.4690623831702396e-05\n",
      "Step: 21270, train/epoch: 5.061875343322754\n",
      "Step: 21280, train/loss: 0.0\n",
      "Step: 21280, train/grad_norm: 0.0004099791403859854\n",
      "Step: 21280, train/learning_rate: 2.46787240030244e-05\n",
      "Step: 21280, train/epoch: 5.064255237579346\n",
      "Step: 21290, train/loss: 0.0\n",
      "Step: 21290, train/grad_norm: 0.00021470044157467782\n",
      "Step: 21290, train/learning_rate: 2.4666825993335806e-05\n",
      "Step: 21290, train/epoch: 5.0666351318359375\n",
      "Step: 21300, train/loss: 0.0\n",
      "Step: 21300, train/grad_norm: 0.00019208734738640487\n",
      "Step: 21300, train/learning_rate: 2.465492616465781e-05\n",
      "Step: 21300, train/epoch: 5.069014549255371\n",
      "Step: 21310, train/loss: 0.0\n",
      "Step: 21310, train/grad_norm: 9.941903408616781e-05\n",
      "Step: 21310, train/learning_rate: 2.4643026335979812e-05\n",
      "Step: 21310, train/epoch: 5.071394443511963\n",
      "Step: 21320, train/loss: 0.0\n",
      "Step: 21320, train/grad_norm: 0.00015038427955005318\n",
      "Step: 21320, train/learning_rate: 2.463112832629122e-05\n",
      "Step: 21320, train/epoch: 5.073774337768555\n",
      "Step: 21330, train/loss: 0.0\n",
      "Step: 21330, train/grad_norm: 5.201605745241977e-05\n",
      "Step: 21330, train/learning_rate: 2.461922849761322e-05\n",
      "Step: 21330, train/epoch: 5.0761542320251465\n",
      "Step: 21340, train/loss: 0.0\n",
      "Step: 21340, train/grad_norm: 3.969833051087335e-05\n",
      "Step: 21340, train/learning_rate: 2.4607330487924628e-05\n",
      "Step: 21340, train/epoch: 5.078534126281738\n",
      "Step: 21350, train/loss: 0.0\n",
      "Step: 21350, train/grad_norm: 8.89854782144539e-05\n",
      "Step: 21350, train/learning_rate: 2.459543065924663e-05\n",
      "Step: 21350, train/epoch: 5.08091402053833\n",
      "Step: 21360, train/loss: 0.0\n",
      "Step: 21360, train/grad_norm: 8.320048073073849e-05\n",
      "Step: 21360, train/learning_rate: 2.4583530830568634e-05\n",
      "Step: 21360, train/epoch: 5.083293437957764\n",
      "Step: 21370, train/loss: 0.0\n",
      "Step: 21370, train/grad_norm: 0.0002628587244544178\n",
      "Step: 21370, train/learning_rate: 2.457163282088004e-05\n",
      "Step: 21370, train/epoch: 5.0856733322143555\n",
      "Step: 21380, train/loss: 0.0\n",
      "Step: 21380, train/grad_norm: 0.00034324530861340463\n",
      "Step: 21380, train/learning_rate: 2.4559732992202044e-05\n",
      "Step: 21380, train/epoch: 5.088053226470947\n",
      "Step: 21390, train/loss: 0.0\n",
      "Step: 21390, train/grad_norm: 0.0005012238398194313\n",
      "Step: 21390, train/learning_rate: 2.454783498251345e-05\n",
      "Step: 21390, train/epoch: 5.090433120727539\n",
      "Step: 21400, train/loss: 0.0\n",
      "Step: 21400, train/grad_norm: 5.884394340682775e-05\n",
      "Step: 21400, train/learning_rate: 2.4535935153835453e-05\n",
      "Step: 21400, train/epoch: 5.092813014984131\n",
      "Step: 21410, train/loss: 0.0\n",
      "Step: 21410, train/grad_norm: 0.0002393541653873399\n",
      "Step: 21410, train/learning_rate: 2.4524035325157456e-05\n",
      "Step: 21410, train/epoch: 5.095192909240723\n",
      "Step: 21420, train/loss: 0.0\n",
      "Step: 21420, train/grad_norm: 0.0001412163837812841\n",
      "Step: 21420, train/learning_rate: 2.4512137315468863e-05\n",
      "Step: 21420, train/epoch: 5.0975728034973145\n",
      "Step: 21430, train/loss: 0.17970000207424164\n",
      "Step: 21430, train/grad_norm: 0.001011930056847632\n",
      "Step: 21430, train/learning_rate: 2.4500237486790866e-05\n",
      "Step: 21430, train/epoch: 5.099952220916748\n",
      "Step: 21440, train/loss: 0.0\n",
      "Step: 21440, train/grad_norm: 0.04708597809076309\n",
      "Step: 21440, train/learning_rate: 2.4488339477102272e-05\n",
      "Step: 21440, train/epoch: 5.10233211517334\n",
      "Step: 21450, train/loss: 9.999999747378752e-05\n",
      "Step: 21450, train/grad_norm: 0.0371834896504879\n",
      "Step: 21450, train/learning_rate: 2.4476439648424275e-05\n",
      "Step: 21450, train/epoch: 5.104712009429932\n",
      "Step: 21460, train/loss: 0.10320000350475311\n",
      "Step: 21460, train/grad_norm: 0.012720935977995396\n",
      "Step: 21460, train/learning_rate: 2.4464539819746278e-05\n",
      "Step: 21460, train/epoch: 5.107091903686523\n",
      "Step: 21470, train/loss: 0.1720000058412552\n",
      "Step: 21470, train/grad_norm: 0.22992190718650818\n",
      "Step: 21470, train/learning_rate: 2.4452641810057685e-05\n",
      "Step: 21470, train/epoch: 5.109471797943115\n",
      "Step: 21480, train/loss: 0.060499999672174454\n",
      "Step: 21480, train/grad_norm: 0.00468559842556715\n",
      "Step: 21480, train/learning_rate: 2.4440741981379688e-05\n",
      "Step: 21480, train/epoch: 5.111851692199707\n",
      "Step: 21490, train/loss: 0.0\n",
      "Step: 21490, train/grad_norm: 0.0002499090041965246\n",
      "Step: 21490, train/learning_rate: 2.4428843971691094e-05\n",
      "Step: 21490, train/epoch: 5.114231109619141\n",
      "Step: 21500, train/loss: 0.0\n",
      "Step: 21500, train/grad_norm: 0.23296493291854858\n",
      "Step: 21500, train/learning_rate: 2.4416944143013097e-05\n",
      "Step: 21500, train/epoch: 5.116611003875732\n",
      "Step: 21510, train/loss: 0.0\n",
      "Step: 21510, train/grad_norm: 0.0001717823906801641\n",
      "Step: 21510, train/learning_rate: 2.44050443143351e-05\n",
      "Step: 21510, train/epoch: 5.118990898132324\n",
      "Step: 21520, train/loss: 0.0\n",
      "Step: 21520, train/grad_norm: 0.0005128730554133654\n",
      "Step: 21520, train/learning_rate: 2.4393146304646507e-05\n",
      "Step: 21520, train/epoch: 5.121370792388916\n",
      "Step: 21530, train/loss: 0.0\n",
      "Step: 21530, train/grad_norm: 4.4131600589025766e-05\n",
      "Step: 21530, train/learning_rate: 2.438124647596851e-05\n",
      "Step: 21530, train/epoch: 5.123750686645508\n",
      "Step: 21540, train/loss: 0.2703000009059906\n",
      "Step: 21540, train/grad_norm: 0.0003959361056331545\n",
      "Step: 21540, train/learning_rate: 2.4369348466279916e-05\n",
      "Step: 21540, train/epoch: 5.1261305809021\n",
      "Step: 21550, train/loss: 0.0\n",
      "Step: 21550, train/grad_norm: 0.08523909747600555\n",
      "Step: 21550, train/learning_rate: 2.435744863760192e-05\n",
      "Step: 21550, train/epoch: 5.128509998321533\n",
      "Step: 21560, train/loss: 0.00019999999494757503\n",
      "Step: 21560, train/grad_norm: 0.002200544811785221\n",
      "Step: 21560, train/learning_rate: 2.4345550627913326e-05\n",
      "Step: 21560, train/epoch: 5.130889892578125\n",
      "Step: 21570, train/loss: 0.00430000014603138\n",
      "Step: 21570, train/grad_norm: 0.004533548839390278\n",
      "Step: 21570, train/learning_rate: 2.433365079923533e-05\n",
      "Step: 21570, train/epoch: 5.133269786834717\n",
      "Step: 21580, train/loss: 0.09849999845027924\n",
      "Step: 21580, train/grad_norm: 75.02515411376953\n",
      "Step: 21580, train/learning_rate: 2.4321750970557332e-05\n",
      "Step: 21580, train/epoch: 5.135649681091309\n",
      "Step: 21590, train/loss: 0.04729999974370003\n",
      "Step: 21590, train/grad_norm: 0.3054063320159912\n",
      "Step: 21590, train/learning_rate: 2.430985296086874e-05\n",
      "Step: 21590, train/epoch: 5.1380295753479\n",
      "Step: 21600, train/loss: 0.20990000665187836\n",
      "Step: 21600, train/grad_norm: 86.82974243164062\n",
      "Step: 21600, train/learning_rate: 2.429795313219074e-05\n",
      "Step: 21600, train/epoch: 5.140409469604492\n",
      "Step: 21610, train/loss: 0.00139999995008111\n",
      "Step: 21610, train/grad_norm: 0.042553938925266266\n",
      "Step: 21610, train/learning_rate: 2.4286055122502148e-05\n",
      "Step: 21610, train/epoch: 5.142789363861084\n",
      "Step: 21620, train/loss: 0.1331000030040741\n",
      "Step: 21620, train/grad_norm: 0.0006532688275910914\n",
      "Step: 21620, train/learning_rate: 2.427415529382415e-05\n",
      "Step: 21620, train/epoch: 5.145168781280518\n",
      "Step: 21630, train/loss: 0.1031000018119812\n",
      "Step: 21630, train/grad_norm: 0.01677457056939602\n",
      "Step: 21630, train/learning_rate: 2.4262255465146154e-05\n",
      "Step: 21630, train/epoch: 5.147548675537109\n",
      "Step: 21640, train/loss: 0.03519999980926514\n",
      "Step: 21640, train/grad_norm: 0.00019925586821045727\n",
      "Step: 21640, train/learning_rate: 2.425035745545756e-05\n",
      "Step: 21640, train/epoch: 5.149928569793701\n",
      "Step: 21650, train/loss: 0.0\n",
      "Step: 21650, train/grad_norm: 0.2190321981906891\n",
      "Step: 21650, train/learning_rate: 2.4238457626779564e-05\n",
      "Step: 21650, train/epoch: 5.152308464050293\n",
      "Step: 21660, train/loss: 9.999999747378752e-05\n",
      "Step: 21660, train/grad_norm: 0.0003888389328494668\n",
      "Step: 21660, train/learning_rate: 2.422655961709097e-05\n",
      "Step: 21660, train/epoch: 5.154688358306885\n",
      "Step: 21670, train/loss: 0.0\n",
      "Step: 21670, train/grad_norm: 5.725232040276751e-05\n",
      "Step: 21670, train/learning_rate: 2.4214659788412973e-05\n",
      "Step: 21670, train/epoch: 5.157068252563477\n",
      "Step: 21680, train/loss: 0.0\n",
      "Step: 21680, train/grad_norm: 0.0002035947109106928\n",
      "Step: 21680, train/learning_rate: 2.4202759959734976e-05\n",
      "Step: 21680, train/epoch: 5.15944766998291\n",
      "Step: 21690, train/loss: 0.0\n",
      "Step: 21690, train/grad_norm: 4.980577796231955e-05\n",
      "Step: 21690, train/learning_rate: 2.4190861950046383e-05\n",
      "Step: 21690, train/epoch: 5.161827564239502\n",
      "Step: 21700, train/loss: 0.0\n",
      "Step: 21700, train/grad_norm: 0.00022705752053298056\n",
      "Step: 21700, train/learning_rate: 2.4178962121368386e-05\n",
      "Step: 21700, train/epoch: 5.164207458496094\n",
      "Step: 21710, train/loss: 0.0\n",
      "Step: 21710, train/grad_norm: 6.318750092759728e-05\n",
      "Step: 21710, train/learning_rate: 2.4167064111679792e-05\n",
      "Step: 21710, train/epoch: 5.1665873527526855\n",
      "Step: 21720, train/loss: 0.0\n",
      "Step: 21720, train/grad_norm: 7.363200711552054e-05\n",
      "Step: 21720, train/learning_rate: 2.4155164283001795e-05\n",
      "Step: 21720, train/epoch: 5.168967247009277\n",
      "Step: 21730, train/loss: 0.0\n",
      "Step: 21730, train/grad_norm: 0.0001239156408701092\n",
      "Step: 21730, train/learning_rate: 2.41432644543238e-05\n",
      "Step: 21730, train/epoch: 5.171347141265869\n",
      "Step: 21740, train/loss: 0.0\n",
      "Step: 21740, train/grad_norm: 1.865704689407721e-05\n",
      "Step: 21740, train/learning_rate: 2.4131366444635205e-05\n",
      "Step: 21740, train/epoch: 5.173726558685303\n",
      "Step: 21750, train/loss: 0.1265999972820282\n",
      "Step: 21750, train/grad_norm: 0.001310258638113737\n",
      "Step: 21750, train/learning_rate: 2.4119466615957208e-05\n",
      "Step: 21750, train/epoch: 5.1761064529418945\n",
      "Step: 21760, train/loss: 0.002300000051036477\n",
      "Step: 21760, train/grad_norm: 0.0023629162460565567\n",
      "Step: 21760, train/learning_rate: 2.4107568606268615e-05\n",
      "Step: 21760, train/epoch: 5.178486347198486\n",
      "Step: 21770, train/loss: 0.0\n",
      "Step: 21770, train/grad_norm: 0.0027016079984605312\n",
      "Step: 21770, train/learning_rate: 2.4095668777590618e-05\n",
      "Step: 21770, train/epoch: 5.180866241455078\n",
      "Step: 21780, train/loss: 0.0\n",
      "Step: 21780, train/grad_norm: 0.0011125538730993867\n",
      "Step: 21780, train/learning_rate: 2.408376894891262e-05\n",
      "Step: 21780, train/epoch: 5.18324613571167\n",
      "Step: 21790, train/loss: 0.0\n",
      "Step: 21790, train/grad_norm: 0.005088374018669128\n",
      "Step: 21790, train/learning_rate: 2.4071870939224027e-05\n",
      "Step: 21790, train/epoch: 5.185626029968262\n",
      "Step: 21800, train/loss: 0.17270000278949738\n",
      "Step: 21800, train/grad_norm: 0.006146375555545092\n",
      "Step: 21800, train/learning_rate: 2.405997111054603e-05\n",
      "Step: 21800, train/epoch: 5.1880059242248535\n",
      "Step: 21810, train/loss: 0.008299999870359898\n",
      "Step: 21810, train/grad_norm: 0.09928081184625626\n",
      "Step: 21810, train/learning_rate: 2.4048073100857437e-05\n",
      "Step: 21810, train/epoch: 5.190385341644287\n",
      "Step: 21820, train/loss: 0.0\n",
      "Step: 21820, train/grad_norm: 0.012141657061874866\n",
      "Step: 21820, train/learning_rate: 2.403617327217944e-05\n",
      "Step: 21820, train/epoch: 5.192765235900879\n",
      "Step: 21830, train/loss: 0.0\n",
      "Step: 21830, train/grad_norm: 0.00997625570744276\n",
      "Step: 21830, train/learning_rate: 2.4024273443501443e-05\n",
      "Step: 21830, train/epoch: 5.195145130157471\n",
      "Step: 21840, train/loss: 0.1062999963760376\n",
      "Step: 21840, train/grad_norm: 0.009753933176398277\n",
      "Step: 21840, train/learning_rate: 2.401237543381285e-05\n",
      "Step: 21840, train/epoch: 5.1975250244140625\n",
      "Step: 21850, train/loss: 0.0\n",
      "Step: 21850, train/grad_norm: 0.009773023426532745\n",
      "Step: 21850, train/learning_rate: 2.4000475605134852e-05\n",
      "Step: 21850, train/epoch: 5.199904918670654\n",
      "Step: 21860, train/loss: 0.0\n",
      "Step: 21860, train/grad_norm: 0.004714612383395433\n",
      "Step: 21860, train/learning_rate: 2.398857759544626e-05\n",
      "Step: 21860, train/epoch: 5.202284812927246\n",
      "Step: 21870, train/loss: 0.0\n",
      "Step: 21870, train/grad_norm: 0.012655269354581833\n",
      "Step: 21870, train/learning_rate: 2.3976677766768262e-05\n",
      "Step: 21870, train/epoch: 5.20466423034668\n",
      "Step: 21880, train/loss: 0.0\n",
      "Step: 21880, train/grad_norm: 0.0028060241602361202\n",
      "Step: 21880, train/learning_rate: 2.3964777938090265e-05\n",
      "Step: 21880, train/epoch: 5.2070441246032715\n",
      "Step: 21890, train/loss: 0.0\n",
      "Step: 21890, train/grad_norm: 0.003908422775566578\n",
      "Step: 21890, train/learning_rate: 2.395287992840167e-05\n",
      "Step: 21890, train/epoch: 5.209424018859863\n",
      "Step: 21900, train/loss: 0.0\n",
      "Step: 21900, train/grad_norm: 0.0005370665458030999\n",
      "Step: 21900, train/learning_rate: 2.3940980099723674e-05\n",
      "Step: 21900, train/epoch: 5.211803913116455\n",
      "Step: 21910, train/loss: 0.10080000013113022\n",
      "Step: 21910, train/grad_norm: 0.008885200135409832\n",
      "Step: 21910, train/learning_rate: 2.392908209003508e-05\n",
      "Step: 21910, train/epoch: 5.214183807373047\n",
      "Step: 21920, train/loss: 0.0\n",
      "Step: 21920, train/grad_norm: 0.028128456324338913\n",
      "Step: 21920, train/learning_rate: 2.3917182261357084e-05\n",
      "Step: 21920, train/epoch: 5.216563701629639\n",
      "Step: 21930, train/loss: 9.999999747378752e-05\n",
      "Step: 21930, train/grad_norm: 0.05945347994565964\n",
      "Step: 21930, train/learning_rate: 2.3905282432679087e-05\n",
      "Step: 21930, train/epoch: 5.2189435958862305\n",
      "Step: 21940, train/loss: 0.0\n",
      "Step: 21940, train/grad_norm: 0.002909788629040122\n",
      "Step: 21940, train/learning_rate: 2.3893384422990493e-05\n",
      "Step: 21940, train/epoch: 5.221323013305664\n",
      "Step: 21950, train/loss: 0.0\n",
      "Step: 21950, train/grad_norm: 0.004627371206879616\n",
      "Step: 21950, train/learning_rate: 2.3881484594312496e-05\n",
      "Step: 21950, train/epoch: 5.223702907562256\n",
      "Step: 21960, train/loss: 0.0\n",
      "Step: 21960, train/grad_norm: 0.0009990222752094269\n",
      "Step: 21960, train/learning_rate: 2.3869586584623903e-05\n",
      "Step: 21960, train/epoch: 5.226082801818848\n",
      "Step: 21970, train/loss: 0.0\n",
      "Step: 21970, train/grad_norm: 0.02060018666088581\n",
      "Step: 21970, train/learning_rate: 2.3857686755945906e-05\n",
      "Step: 21970, train/epoch: 5.2284626960754395\n",
      "Step: 21980, train/loss: 0.0\n",
      "Step: 21980, train/grad_norm: 0.0006530631799250841\n",
      "Step: 21980, train/learning_rate: 2.384578692726791e-05\n",
      "Step: 21980, train/epoch: 5.230842590332031\n",
      "Step: 21990, train/loss: 0.0\n",
      "Step: 21990, train/grad_norm: 0.002306594280526042\n",
      "Step: 21990, train/learning_rate: 2.3833888917579316e-05\n",
      "Step: 21990, train/epoch: 5.233222484588623\n",
      "Step: 22000, train/loss: 0.0\n",
      "Step: 22000, train/grad_norm: 0.000874039891641587\n",
      "Step: 22000, train/learning_rate: 2.382198908890132e-05\n",
      "Step: 22000, train/epoch: 5.235601902008057\n",
      "Step: 22010, train/loss: 0.0843999981880188\n",
      "Step: 22010, train/grad_norm: 0.007828066125512123\n",
      "Step: 22010, train/learning_rate: 2.3810091079212725e-05\n",
      "Step: 22010, train/epoch: 5.237981796264648\n",
      "Step: 22020, train/loss: 9.999999747378752e-05\n",
      "Step: 22020, train/grad_norm: 0.0015606082743033767\n",
      "Step: 22020, train/learning_rate: 2.3798191250534728e-05\n",
      "Step: 22020, train/epoch: 5.24036169052124\n",
      "Step: 22030, train/loss: 0.0\n",
      "Step: 22030, train/grad_norm: 0.0036841246765106916\n",
      "Step: 22030, train/learning_rate: 2.378629142185673e-05\n",
      "Step: 22030, train/epoch: 5.242741584777832\n",
      "Step: 22040, train/loss: 0.0\n",
      "Step: 22040, train/grad_norm: 0.0010986769339069724\n",
      "Step: 22040, train/learning_rate: 2.3774393412168138e-05\n",
      "Step: 22040, train/epoch: 5.245121479034424\n",
      "Step: 22050, train/loss: 0.0\n",
      "Step: 22050, train/grad_norm: 0.0007687322795391083\n",
      "Step: 22050, train/learning_rate: 2.376249358349014e-05\n",
      "Step: 22050, train/epoch: 5.247501373291016\n",
      "Step: 22060, train/loss: 0.0\n",
      "Step: 22060, train/grad_norm: 0.0005617821589112282\n",
      "Step: 22060, train/learning_rate: 2.3750595573801547e-05\n",
      "Step: 22060, train/epoch: 5.249880790710449\n",
      "Step: 22070, train/loss: 0.0\n",
      "Step: 22070, train/grad_norm: 0.001039857161231339\n",
      "Step: 22070, train/learning_rate: 2.373869574512355e-05\n",
      "Step: 22070, train/epoch: 5.252260684967041\n",
      "Step: 22080, train/loss: 0.0\n",
      "Step: 22080, train/grad_norm: 0.007642572745680809\n",
      "Step: 22080, train/learning_rate: 2.3726795916445553e-05\n",
      "Step: 22080, train/epoch: 5.254640579223633\n",
      "Step: 22090, train/loss: 0.0\n",
      "Step: 22090, train/grad_norm: 0.0002835836785379797\n",
      "Step: 22090, train/learning_rate: 2.371489790675696e-05\n",
      "Step: 22090, train/epoch: 5.257020473480225\n",
      "Step: 22100, train/loss: 0.0\n",
      "Step: 22100, train/grad_norm: 0.0007037892937660217\n",
      "Step: 22100, train/learning_rate: 2.3702998078078963e-05\n",
      "Step: 22100, train/epoch: 5.259400367736816\n",
      "Step: 22110, train/loss: 0.0\n",
      "Step: 22110, train/grad_norm: 0.0002998972195200622\n",
      "Step: 22110, train/learning_rate: 2.369110006839037e-05\n",
      "Step: 22110, train/epoch: 5.261780261993408\n",
      "Step: 22120, train/loss: 0.0\n",
      "Step: 22120, train/grad_norm: 0.0022237966768443584\n",
      "Step: 22120, train/learning_rate: 2.3679200239712372e-05\n",
      "Step: 22120, train/epoch: 5.26416015625\n",
      "Step: 22130, train/loss: 0.0\n",
      "Step: 22130, train/grad_norm: 0.0874922126531601\n",
      "Step: 22130, train/learning_rate: 2.3667300411034375e-05\n",
      "Step: 22130, train/epoch: 5.266539573669434\n",
      "Step: 22140, train/loss: 0.02160000056028366\n",
      "Step: 22140, train/grad_norm: 0.0002688113891053945\n",
      "Step: 22140, train/learning_rate: 2.3655402401345782e-05\n",
      "Step: 22140, train/epoch: 5.268919467926025\n",
      "Step: 22150, train/loss: 0.13199999928474426\n",
      "Step: 22150, train/grad_norm: 0.0012672027805820107\n",
      "Step: 22150, train/learning_rate: 2.3643502572667785e-05\n",
      "Step: 22150, train/epoch: 5.271299362182617\n",
      "Step: 22160, train/loss: 0.0\n",
      "Step: 22160, train/grad_norm: 0.0033111898228526115\n",
      "Step: 22160, train/learning_rate: 2.363160456297919e-05\n",
      "Step: 22160, train/epoch: 5.273679256439209\n",
      "Step: 22170, train/loss: 0.0\n",
      "Step: 22170, train/grad_norm: 0.0015085944905877113\n",
      "Step: 22170, train/learning_rate: 2.3619704734301195e-05\n",
      "Step: 22170, train/epoch: 5.276059150695801\n",
      "Step: 22180, train/loss: 0.0\n",
      "Step: 22180, train/grad_norm: 0.1411467045545578\n",
      "Step: 22180, train/learning_rate: 2.3607804905623198e-05\n",
      "Step: 22180, train/epoch: 5.278439044952393\n",
      "Step: 22190, train/loss: 0.0625\n",
      "Step: 22190, train/grad_norm: 0.004478923976421356\n",
      "Step: 22190, train/learning_rate: 2.3595906895934604e-05\n",
      "Step: 22190, train/epoch: 5.280818462371826\n",
      "Step: 22200, train/loss: 9.999999747378752e-05\n",
      "Step: 22200, train/grad_norm: 0.31417563557624817\n",
      "Step: 22200, train/learning_rate: 2.3584007067256607e-05\n",
      "Step: 22200, train/epoch: 5.283198356628418\n",
      "Step: 22210, train/loss: 0.0\n",
      "Step: 22210, train/grad_norm: 0.0016361901070922613\n",
      "Step: 22210, train/learning_rate: 2.3572109057568014e-05\n",
      "Step: 22210, train/epoch: 5.28557825088501\n",
      "Step: 22220, train/loss: 0.0\n",
      "Step: 22220, train/grad_norm: 0.0006683598621748388\n",
      "Step: 22220, train/learning_rate: 2.3560209228890017e-05\n",
      "Step: 22220, train/epoch: 5.287958145141602\n",
      "Step: 22230, train/loss: 0.0003000000142492354\n",
      "Step: 22230, train/grad_norm: 0.14978836476802826\n",
      "Step: 22230, train/learning_rate: 2.3548311219201423e-05\n",
      "Step: 22230, train/epoch: 5.290338039398193\n",
      "Step: 22240, train/loss: 0.0\n",
      "Step: 22240, train/grad_norm: 0.00013564607070293278\n",
      "Step: 22240, train/learning_rate: 2.3536411390523426e-05\n",
      "Step: 22240, train/epoch: 5.292717933654785\n",
      "Step: 22250, train/loss: 0.16899999976158142\n",
      "Step: 22250, train/grad_norm: 0.01468063984066248\n",
      "Step: 22250, train/learning_rate: 2.352451156184543e-05\n",
      "Step: 22250, train/epoch: 5.295097351074219\n",
      "Step: 22260, train/loss: 0.0\n",
      "Step: 22260, train/grad_norm: 0.0019404975464567542\n",
      "Step: 22260, train/learning_rate: 2.3512613552156836e-05\n",
      "Step: 22260, train/epoch: 5.2974772453308105\n",
      "Step: 22270, train/loss: 0.0\n",
      "Step: 22270, train/grad_norm: 0.1311023086309433\n",
      "Step: 22270, train/learning_rate: 2.350071372347884e-05\n",
      "Step: 22270, train/epoch: 5.299857139587402\n",
      "Step: 22280, train/loss: 0.09220000356435776\n",
      "Step: 22280, train/grad_norm: 0.004309782758355141\n",
      "Step: 22280, train/learning_rate: 2.3488815713790245e-05\n",
      "Step: 22280, train/epoch: 5.302237033843994\n",
      "Step: 22290, train/loss: 9.999999747378752e-05\n",
      "Step: 22290, train/grad_norm: 0.008991439826786518\n",
      "Step: 22290, train/learning_rate: 2.347691588511225e-05\n",
      "Step: 22290, train/epoch: 5.304616928100586\n",
      "Step: 22300, train/loss: 0.0\n",
      "Step: 22300, train/grad_norm: 0.011846166104078293\n",
      "Step: 22300, train/learning_rate: 2.346501605643425e-05\n",
      "Step: 22300, train/epoch: 5.306996822357178\n",
      "Step: 22310, train/loss: 0.0\n",
      "Step: 22310, train/grad_norm: 0.004891130141913891\n",
      "Step: 22310, train/learning_rate: 2.3453118046745658e-05\n",
      "Step: 22310, train/epoch: 5.3093767166137695\n",
      "Step: 22320, train/loss: 0.0\n",
      "Step: 22320, train/grad_norm: 0.013012828305363655\n",
      "Step: 22320, train/learning_rate: 2.344121821806766e-05\n",
      "Step: 22320, train/epoch: 5.311756134033203\n",
      "Step: 22330, train/loss: 0.0\n",
      "Step: 22330, train/grad_norm: 0.0012718124780803919\n",
      "Step: 22330, train/learning_rate: 2.3429320208379067e-05\n",
      "Step: 22330, train/epoch: 5.314136028289795\n",
      "Step: 22340, train/loss: 0.0\n",
      "Step: 22340, train/grad_norm: 0.0010411239927634597\n",
      "Step: 22340, train/learning_rate: 2.341742037970107e-05\n",
      "Step: 22340, train/epoch: 5.316515922546387\n",
      "Step: 22350, train/loss: 0.0\n",
      "Step: 22350, train/grad_norm: 0.001943671377375722\n",
      "Step: 22350, train/learning_rate: 2.3405520551023073e-05\n",
      "Step: 22350, train/epoch: 5.3188958168029785\n",
      "Step: 22360, train/loss: 0.06019999831914902\n",
      "Step: 22360, train/grad_norm: 122.38652801513672\n",
      "Step: 22360, train/learning_rate: 2.339362254133448e-05\n",
      "Step: 22360, train/epoch: 5.32127571105957\n",
      "Step: 22370, train/loss: 0.0\n",
      "Step: 22370, train/grad_norm: 0.006015157792717218\n",
      "Step: 22370, train/learning_rate: 2.3381722712656483e-05\n",
      "Step: 22370, train/epoch: 5.323655605316162\n",
      "Step: 22380, train/loss: 0.0\n",
      "Step: 22380, train/grad_norm: 0.01229467149823904\n",
      "Step: 22380, train/learning_rate: 2.336982470296789e-05\n",
      "Step: 22380, train/epoch: 5.326035022735596\n",
      "Step: 22390, train/loss: 0.0\n",
      "Step: 22390, train/grad_norm: 0.0013242402346804738\n",
      "Step: 22390, train/learning_rate: 2.3357924874289893e-05\n",
      "Step: 22390, train/epoch: 5.3284149169921875\n",
      "Step: 22400, train/loss: 0.0\n",
      "Step: 22400, train/grad_norm: 0.0007036429597064853\n",
      "Step: 22400, train/learning_rate: 2.3346025045611896e-05\n",
      "Step: 22400, train/epoch: 5.330794811248779\n",
      "Step: 22410, train/loss: 0.0\n",
      "Step: 22410, train/grad_norm: 0.004379813093692064\n",
      "Step: 22410, train/learning_rate: 2.3334127035923302e-05\n",
      "Step: 22410, train/epoch: 5.333174705505371\n",
      "Step: 22420, train/loss: 0.0\n",
      "Step: 22420, train/grad_norm: 0.001592499203979969\n",
      "Step: 22420, train/learning_rate: 2.3322227207245305e-05\n",
      "Step: 22420, train/epoch: 5.335554599761963\n",
      "Step: 22430, train/loss: 0.0\n",
      "Step: 22430, train/grad_norm: 0.0012647958938032389\n",
      "Step: 22430, train/learning_rate: 2.3310329197556712e-05\n",
      "Step: 22430, train/epoch: 5.337934494018555\n",
      "Step: 22440, train/loss: 0.0\n",
      "Step: 22440, train/grad_norm: 0.0009510508389212191\n",
      "Step: 22440, train/learning_rate: 2.3298429368878715e-05\n",
      "Step: 22440, train/epoch: 5.340313911437988\n",
      "Step: 22450, train/loss: 0.0\n",
      "Step: 22450, train/grad_norm: 0.0016604532720521092\n",
      "Step: 22450, train/learning_rate: 2.3286529540200718e-05\n",
      "Step: 22450, train/epoch: 5.34269380569458\n",
      "Step: 22460, train/loss: 0.0\n",
      "Step: 22460, train/grad_norm: 0.02814319357275963\n",
      "Step: 22460, train/learning_rate: 2.3274631530512124e-05\n",
      "Step: 22460, train/epoch: 5.345073699951172\n",
      "Step: 22470, train/loss: 0.0\n",
      "Step: 22470, train/grad_norm: 0.0024474156089127064\n",
      "Step: 22470, train/learning_rate: 2.3262731701834127e-05\n",
      "Step: 22470, train/epoch: 5.347453594207764\n",
      "Step: 22480, train/loss: 0.0\n",
      "Step: 22480, train/grad_norm: 0.000660622026771307\n",
      "Step: 22480, train/learning_rate: 2.3250833692145534e-05\n",
      "Step: 22480, train/epoch: 5.3498334884643555\n",
      "Step: 22490, train/loss: 0.0\n",
      "Step: 22490, train/grad_norm: 0.0008839105139486492\n",
      "Step: 22490, train/learning_rate: 2.3238933863467537e-05\n",
      "Step: 22490, train/epoch: 5.352213382720947\n",
      "Step: 22500, train/loss: 0.0\n",
      "Step: 22500, train/grad_norm: 8.396854536840692e-05\n",
      "Step: 22500, train/learning_rate: 2.322703403478954e-05\n",
      "Step: 22500, train/epoch: 5.354593276977539\n",
      "Step: 22510, train/loss: 0.0\n",
      "Step: 22510, train/grad_norm: 0.00013007615052629262\n",
      "Step: 22510, train/learning_rate: 2.3215136025100946e-05\n",
      "Step: 22510, train/epoch: 5.356972694396973\n",
      "Step: 22520, train/loss: 9.999999747378752e-05\n",
      "Step: 22520, train/grad_norm: 0.00018513735267333686\n",
      "Step: 22520, train/learning_rate: 2.320323619642295e-05\n",
      "Step: 22520, train/epoch: 5.3593525886535645\n",
      "Step: 22530, train/loss: 0.0\n",
      "Step: 22530, train/grad_norm: 0.00021387735614553094\n",
      "Step: 22530, train/learning_rate: 2.3191338186734356e-05\n",
      "Step: 22530, train/epoch: 5.361732482910156\n",
      "Step: 22540, train/loss: 0.0\n",
      "Step: 22540, train/grad_norm: 0.0002302865614183247\n",
      "Step: 22540, train/learning_rate: 2.317943835805636e-05\n",
      "Step: 22540, train/epoch: 5.364112377166748\n",
      "Step: 22550, train/loss: 0.0\n",
      "Step: 22550, train/grad_norm: 0.00020837091142311692\n",
      "Step: 22550, train/learning_rate: 2.3167538529378362e-05\n",
      "Step: 22550, train/epoch: 5.36649227142334\n",
      "Step: 22560, train/loss: 0.0\n",
      "Step: 22560, train/grad_norm: 7.19392264727503e-05\n",
      "Step: 22560, train/learning_rate: 2.315564051968977e-05\n",
      "Step: 22560, train/epoch: 5.368872165679932\n",
      "Step: 22570, train/loss: 0.0\n",
      "Step: 22570, train/grad_norm: 0.0003857461269944906\n",
      "Step: 22570, train/learning_rate: 2.314374069101177e-05\n",
      "Step: 22570, train/epoch: 5.371251583099365\n",
      "Step: 22580, train/loss: 0.0\n",
      "Step: 22580, train/grad_norm: 0.00010542618110775948\n",
      "Step: 22580, train/learning_rate: 2.3131842681323178e-05\n",
      "Step: 22580, train/epoch: 5.373631477355957\n",
      "Step: 22590, train/loss: 0.0\n",
      "Step: 22590, train/grad_norm: 0.00045916211092844605\n",
      "Step: 22590, train/learning_rate: 2.311994285264518e-05\n",
      "Step: 22590, train/epoch: 5.376011371612549\n",
      "Step: 22600, train/loss: 0.0\n",
      "Step: 22600, train/grad_norm: 0.00029938548686914146\n",
      "Step: 22600, train/learning_rate: 2.3108043023967184e-05\n",
      "Step: 22600, train/epoch: 5.378391265869141\n",
      "Step: 22610, train/loss: 0.0\n",
      "Step: 22610, train/grad_norm: 0.00015796195657458156\n",
      "Step: 22610, train/learning_rate: 2.309614501427859e-05\n",
      "Step: 22610, train/epoch: 5.380771160125732\n",
      "Step: 22620, train/loss: 0.0\n",
      "Step: 22620, train/grad_norm: 7.020294287940487e-05\n",
      "Step: 22620, train/learning_rate: 2.3084245185600594e-05\n",
      "Step: 22620, train/epoch: 5.383151054382324\n",
      "Step: 22630, train/loss: 0.0\n",
      "Step: 22630, train/grad_norm: 0.00011812165030278265\n",
      "Step: 22630, train/learning_rate: 2.3072347175912e-05\n",
      "Step: 22630, train/epoch: 5.385530471801758\n",
      "Step: 22640, train/loss: 0.0\n",
      "Step: 22640, train/grad_norm: 0.04804408550262451\n",
      "Step: 22640, train/learning_rate: 2.3060447347234003e-05\n",
      "Step: 22640, train/epoch: 5.38791036605835\n",
      "Step: 22650, train/loss: 0.0\n",
      "Step: 22650, train/grad_norm: 0.00020967113960068673\n",
      "Step: 22650, train/learning_rate: 2.3048547518556006e-05\n",
      "Step: 22650, train/epoch: 5.390290260314941\n",
      "Step: 22660, train/loss: 0.0\n",
      "Step: 22660, train/grad_norm: 0.00015163158241193742\n",
      "Step: 22660, train/learning_rate: 2.3036649508867413e-05\n",
      "Step: 22660, train/epoch: 5.392670154571533\n",
      "Step: 22670, train/loss: 0.0\n",
      "Step: 22670, train/grad_norm: 0.0001223115250468254\n",
      "Step: 22670, train/learning_rate: 2.3024749680189416e-05\n",
      "Step: 22670, train/epoch: 5.395050048828125\n",
      "Step: 22680, train/loss: 0.0\n",
      "Step: 22680, train/grad_norm: 9.254074393538758e-05\n",
      "Step: 22680, train/learning_rate: 2.3012851670500822e-05\n",
      "Step: 22680, train/epoch: 5.397429943084717\n",
      "Step: 22690, train/loss: 0.07729999721050262\n",
      "Step: 22690, train/grad_norm: 0.0002050731418421492\n",
      "Step: 22690, train/learning_rate: 2.3000951841822825e-05\n",
      "Step: 22690, train/epoch: 5.399809837341309\n",
      "Step: 22700, train/loss: 0.0\n",
      "Step: 22700, train/grad_norm: 0.00015628554683644325\n",
      "Step: 22700, train/learning_rate: 2.298905201314483e-05\n",
      "Step: 22700, train/epoch: 5.402189254760742\n",
      "Step: 22710, train/loss: 0.0\n",
      "Step: 22710, train/grad_norm: 0.00016550387954339385\n",
      "Step: 22710, train/learning_rate: 2.2977154003456235e-05\n",
      "Step: 22710, train/epoch: 5.404569149017334\n",
      "Step: 22720, train/loss: 0.0\n",
      "Step: 22720, train/grad_norm: 0.00012548569065984339\n",
      "Step: 22720, train/learning_rate: 2.2965254174778238e-05\n",
      "Step: 22720, train/epoch: 5.406949043273926\n",
      "Step: 22730, train/loss: 0.12809999287128448\n",
      "Step: 22730, train/grad_norm: 0.0004748203500639647\n",
      "Step: 22730, train/learning_rate: 2.2953356165089644e-05\n",
      "Step: 22730, train/epoch: 5.409328937530518\n",
      "Step: 22740, train/loss: 0.0\n",
      "Step: 22740, train/grad_norm: 0.0037444555200636387\n",
      "Step: 22740, train/learning_rate: 2.2941456336411647e-05\n",
      "Step: 22740, train/epoch: 5.411708831787109\n",
      "Step: 22750, train/loss: 0.0\n",
      "Step: 22750, train/grad_norm: 0.011974772438406944\n",
      "Step: 22750, train/learning_rate: 2.292955650773365e-05\n",
      "Step: 22750, train/epoch: 5.414088726043701\n",
      "Step: 22760, train/loss: 0.025499999523162842\n",
      "Step: 22760, train/grad_norm: 347.51104736328125\n",
      "Step: 22760, train/learning_rate: 2.2917658498045057e-05\n",
      "Step: 22760, train/epoch: 5.416468143463135\n",
      "Step: 22770, train/loss: 0.0\n",
      "Step: 22770, train/grad_norm: 0.012520728632807732\n",
      "Step: 22770, train/learning_rate: 2.290575866936706e-05\n",
      "Step: 22770, train/epoch: 5.418848037719727\n",
      "Step: 22780, train/loss: 0.0005000000237487257\n",
      "Step: 22780, train/grad_norm: 0.000923552259337157\n",
      "Step: 22780, train/learning_rate: 2.2893860659678467e-05\n",
      "Step: 22780, train/epoch: 5.421227931976318\n",
      "Step: 22790, train/loss: 0.0\n",
      "Step: 22790, train/grad_norm: 0.000382391968742013\n",
      "Step: 22790, train/learning_rate: 2.288196083100047e-05\n",
      "Step: 22790, train/epoch: 5.42360782623291\n",
      "Step: 22800, train/loss: 0.0\n",
      "Step: 22800, train/grad_norm: 0.0007856315351091325\n",
      "Step: 22800, train/learning_rate: 2.2870061002322473e-05\n",
      "Step: 22800, train/epoch: 5.425987720489502\n",
      "Step: 22810, train/loss: 0.16840000450611115\n",
      "Step: 22810, train/grad_norm: 0.00793696939945221\n",
      "Step: 22810, train/learning_rate: 2.285816299263388e-05\n",
      "Step: 22810, train/epoch: 5.428367614746094\n",
      "Step: 22820, train/loss: 9.999999747378752e-05\n",
      "Step: 22820, train/grad_norm: 0.015054558403789997\n",
      "Step: 22820, train/learning_rate: 2.2846263163955882e-05\n",
      "Step: 22820, train/epoch: 5.430747032165527\n",
      "Step: 22830, train/loss: 9.999999747378752e-05\n",
      "Step: 22830, train/grad_norm: 0.007540274877101183\n",
      "Step: 22830, train/learning_rate: 2.283436515426729e-05\n",
      "Step: 22830, train/epoch: 5.433126926422119\n",
      "Step: 22840, train/loss: 0.04809999838471413\n",
      "Step: 22840, train/grad_norm: 1563.3548583984375\n",
      "Step: 22840, train/learning_rate: 2.2822465325589292e-05\n",
      "Step: 22840, train/epoch: 5.435506820678711\n",
      "Step: 22850, train/loss: 0.0\n",
      "Step: 22850, train/grad_norm: 0.007699015084654093\n",
      "Step: 22850, train/learning_rate: 2.2810565496911295e-05\n",
      "Step: 22850, train/epoch: 5.437886714935303\n",
      "Step: 22860, train/loss: 0.0\n",
      "Step: 22860, train/grad_norm: 0.00042943464359268546\n",
      "Step: 22860, train/learning_rate: 2.27986674872227e-05\n",
      "Step: 22860, train/epoch: 5.4402666091918945\n",
      "Step: 22870, train/loss: 0.0\n",
      "Step: 22870, train/grad_norm: 0.0026334018912166357\n",
      "Step: 22870, train/learning_rate: 2.2786767658544704e-05\n",
      "Step: 22870, train/epoch: 5.442646503448486\n",
      "Step: 22880, train/loss: 0.0\n",
      "Step: 22880, train/grad_norm: 0.0005346438265405595\n",
      "Step: 22880, train/learning_rate: 2.277486964885611e-05\n",
      "Step: 22880, train/epoch: 5.445026397705078\n",
      "Step: 22890, train/loss: 0.06019999831914902\n",
      "Step: 22890, train/grad_norm: 0.026404518634080887\n",
      "Step: 22890, train/learning_rate: 2.2762969820178114e-05\n",
      "Step: 22890, train/epoch: 5.447405815124512\n",
      "Step: 22900, train/loss: 0.0\n",
      "Step: 22900, train/grad_norm: 0.0035963456612080336\n",
      "Step: 22900, train/learning_rate: 2.275107181048952e-05\n",
      "Step: 22900, train/epoch: 5.4497857093811035\n",
      "Step: 22910, train/loss: 0.0\n",
      "Step: 22910, train/grad_norm: 0.0010667426977306604\n",
      "Step: 22910, train/learning_rate: 2.2739171981811523e-05\n",
      "Step: 22910, train/epoch: 5.452165603637695\n",
      "Step: 22920, train/loss: 0.0\n",
      "Step: 22920, train/grad_norm: 0.0003435226099099964\n",
      "Step: 22920, train/learning_rate: 2.2727272153133526e-05\n",
      "Step: 22920, train/epoch: 5.454545497894287\n",
      "Step: 22930, train/loss: 0.0\n",
      "Step: 22930, train/grad_norm: 0.0019043118227273226\n",
      "Step: 22930, train/learning_rate: 2.2715374143444933e-05\n",
      "Step: 22930, train/epoch: 5.456925392150879\n",
      "Step: 22940, train/loss: 0.0\n",
      "Step: 22940, train/grad_norm: 0.0017793811857700348\n",
      "Step: 22940, train/learning_rate: 2.2703474314766936e-05\n",
      "Step: 22940, train/epoch: 5.459305286407471\n",
      "Step: 22950, train/loss: 0.0\n",
      "Step: 22950, train/grad_norm: 0.0006989416433498263\n",
      "Step: 22950, train/learning_rate: 2.2691576305078343e-05\n",
      "Step: 22950, train/epoch: 5.461684703826904\n",
      "Step: 22960, train/loss: 0.0\n",
      "Step: 22960, train/grad_norm: 0.0003217954945284873\n",
      "Step: 22960, train/learning_rate: 2.2679676476400346e-05\n",
      "Step: 22960, train/epoch: 5.464064598083496\n",
      "Step: 22970, train/loss: 0.0\n",
      "Step: 22970, train/grad_norm: 0.0006422956357710063\n",
      "Step: 22970, train/learning_rate: 2.266777664772235e-05\n",
      "Step: 22970, train/epoch: 5.466444492340088\n",
      "Step: 22980, train/loss: 0.0\n",
      "Step: 22980, train/grad_norm: 0.0009115908178500831\n",
      "Step: 22980, train/learning_rate: 2.2655878638033755e-05\n",
      "Step: 22980, train/epoch: 5.46882438659668\n",
      "Step: 22990, train/loss: 0.0\n",
      "Step: 22990, train/grad_norm: 0.00017287369701080024\n",
      "Step: 22990, train/learning_rate: 2.2643978809355758e-05\n",
      "Step: 22990, train/epoch: 5.4712042808532715\n",
      "Step: 23000, train/loss: 0.0\n",
      "Step: 23000, train/grad_norm: 0.0009442846057936549\n",
      "Step: 23000, train/learning_rate: 2.2632080799667165e-05\n",
      "Step: 23000, train/epoch: 5.473584175109863\n",
      "Step: 23010, train/loss: 0.0\n",
      "Step: 23010, train/grad_norm: 7.717280823271722e-05\n",
      "Step: 23010, train/learning_rate: 2.2620180970989168e-05\n",
      "Step: 23010, train/epoch: 5.475963592529297\n",
      "Step: 23020, train/loss: 0.0\n",
      "Step: 23020, train/grad_norm: 5.721212437492795e-05\n",
      "Step: 23020, train/learning_rate: 2.260828114231117e-05\n",
      "Step: 23020, train/epoch: 5.478343486785889\n",
      "Step: 23030, train/loss: 0.0\n",
      "Step: 23030, train/grad_norm: 0.00022255010844673961\n",
      "Step: 23030, train/learning_rate: 2.2596383132622577e-05\n",
      "Step: 23030, train/epoch: 5.4807233810424805\n",
      "Step: 23040, train/loss: 0.0\n",
      "Step: 23040, train/grad_norm: 6.67736676405184e-05\n",
      "Step: 23040, train/learning_rate: 2.258448330394458e-05\n",
      "Step: 23040, train/epoch: 5.483103275299072\n",
      "Step: 23050, train/loss: 0.0\n",
      "Step: 23050, train/grad_norm: 0.00014093934441916645\n",
      "Step: 23050, train/learning_rate: 2.2572585294255987e-05\n",
      "Step: 23050, train/epoch: 5.485483169555664\n",
      "Step: 23060, train/loss: 0.0\n",
      "Step: 23060, train/grad_norm: 0.00018252659356221557\n",
      "Step: 23060, train/learning_rate: 2.256068546557799e-05\n",
      "Step: 23060, train/epoch: 5.487863063812256\n",
      "Step: 23070, train/loss: 0.0\n",
      "Step: 23070, train/grad_norm: 4.9824488087324426e-05\n",
      "Step: 23070, train/learning_rate: 2.2548785636899993e-05\n",
      "Step: 23070, train/epoch: 5.490242958068848\n",
      "Step: 23080, train/loss: 0.0\n",
      "Step: 23080, train/grad_norm: 6.725250568706542e-05\n",
      "Step: 23080, train/learning_rate: 2.25368876272114e-05\n",
      "Step: 23080, train/epoch: 5.492622375488281\n",
      "Step: 23090, train/loss: 0.0\n",
      "Step: 23090, train/grad_norm: 0.0001236967509612441\n",
      "Step: 23090, train/learning_rate: 2.2524987798533402e-05\n",
      "Step: 23090, train/epoch: 5.495002269744873\n",
      "Step: 23100, train/loss: 0.0\n",
      "Step: 23100, train/grad_norm: 0.00018439267296344042\n",
      "Step: 23100, train/learning_rate: 2.251308978884481e-05\n",
      "Step: 23100, train/epoch: 5.497382164001465\n",
      "Step: 23110, train/loss: 0.0\n",
      "Step: 23110, train/grad_norm: 0.00035170180490240455\n",
      "Step: 23110, train/learning_rate: 2.2501189960166812e-05\n",
      "Step: 23110, train/epoch: 5.499762058258057\n",
      "Step: 23120, train/loss: 0.0\n",
      "Step: 23120, train/grad_norm: 0.0009043911704793572\n",
      "Step: 23120, train/learning_rate: 2.2489290131488815e-05\n",
      "Step: 23120, train/epoch: 5.502141952514648\n",
      "Step: 23130, train/loss: 0.0\n",
      "Step: 23130, train/grad_norm: 0.006846505682915449\n",
      "Step: 23130, train/learning_rate: 2.247739212180022e-05\n",
      "Step: 23130, train/epoch: 5.50452184677124\n",
      "Step: 23140, train/loss: 0.0\n",
      "Step: 23140, train/grad_norm: 8.192619134206325e-05\n",
      "Step: 23140, train/learning_rate: 2.2465492293122225e-05\n",
      "Step: 23140, train/epoch: 5.506901264190674\n",
      "Step: 23150, train/loss: 0.15080000460147858\n",
      "Step: 23150, train/grad_norm: 0.0019384102197363973\n",
      "Step: 23150, train/learning_rate: 2.245359428343363e-05\n",
      "Step: 23150, train/epoch: 5.509281158447266\n",
      "Step: 23160, train/loss: 9.999999747378752e-05\n",
      "Step: 23160, train/grad_norm: 0.004451294895261526\n",
      "Step: 23160, train/learning_rate: 2.2441694454755634e-05\n",
      "Step: 23160, train/epoch: 5.511661052703857\n",
      "Step: 23170, train/loss: 0.0\n",
      "Step: 23170, train/grad_norm: 0.001857399009168148\n",
      "Step: 23170, train/learning_rate: 2.2429794626077637e-05\n",
      "Step: 23170, train/epoch: 5.514040946960449\n",
      "Step: 23180, train/loss: 0.0\n",
      "Step: 23180, train/grad_norm: 0.0009334167698398232\n",
      "Step: 23180, train/learning_rate: 2.2417896616389044e-05\n",
      "Step: 23180, train/epoch: 5.516420841217041\n",
      "Step: 23190, train/loss: 0.0\n",
      "Step: 23190, train/grad_norm: 0.031686991453170776\n",
      "Step: 23190, train/learning_rate: 2.2405996787711047e-05\n",
      "Step: 23190, train/epoch: 5.518800735473633\n",
      "Step: 23200, train/loss: 0.0\n",
      "Step: 23200, train/grad_norm: 0.000829781754873693\n",
      "Step: 23200, train/learning_rate: 2.2394098778022453e-05\n",
      "Step: 23200, train/epoch: 5.521180152893066\n",
      "Step: 23210, train/loss: 0.0\n",
      "Step: 23210, train/grad_norm: 0.0004397905431687832\n",
      "Step: 23210, train/learning_rate: 2.2382198949344456e-05\n",
      "Step: 23210, train/epoch: 5.523560047149658\n",
      "Step: 23220, train/loss: 0.0\n",
      "Step: 23220, train/grad_norm: 0.0004487162223085761\n",
      "Step: 23220, train/learning_rate: 2.237029912066646e-05\n",
      "Step: 23220, train/epoch: 5.52593994140625\n",
      "Step: 23230, train/loss: 0.0\n",
      "Step: 23230, train/grad_norm: 0.0017853685421869159\n",
      "Step: 23230, train/learning_rate: 2.2358401110977866e-05\n",
      "Step: 23230, train/epoch: 5.528319835662842\n",
      "Step: 23240, train/loss: 0.0\n",
      "Step: 23240, train/grad_norm: 0.00047498458297923207\n",
      "Step: 23240, train/learning_rate: 2.234650128229987e-05\n",
      "Step: 23240, train/epoch: 5.530699729919434\n",
      "Step: 23250, train/loss: 0.0\n",
      "Step: 23250, train/grad_norm: 0.0007273082155734301\n",
      "Step: 23250, train/learning_rate: 2.2334603272611275e-05\n",
      "Step: 23250, train/epoch: 5.533079624176025\n",
      "Step: 23260, train/loss: 0.0\n",
      "Step: 23260, train/grad_norm: 0.00016287490143440664\n",
      "Step: 23260, train/learning_rate: 2.232270344393328e-05\n",
      "Step: 23260, train/epoch: 5.535459518432617\n",
      "Step: 23270, train/loss: 0.0\n",
      "Step: 23270, train/grad_norm: 0.00019639266247395426\n",
      "Step: 23270, train/learning_rate: 2.231080361525528e-05\n",
      "Step: 23270, train/epoch: 5.537838935852051\n",
      "Step: 23280, train/loss: 0.0\n",
      "Step: 23280, train/grad_norm: 0.00018343630654271692\n",
      "Step: 23280, train/learning_rate: 2.2298905605566688e-05\n",
      "Step: 23280, train/epoch: 5.540218830108643\n",
      "Step: 23290, train/loss: 0.0\n",
      "Step: 23290, train/grad_norm: 0.00057788728736341\n",
      "Step: 23290, train/learning_rate: 2.228700577688869e-05\n",
      "Step: 23290, train/epoch: 5.542598724365234\n",
      "Step: 23300, train/loss: 0.0\n",
      "Step: 23300, train/grad_norm: 0.00010123543324880302\n",
      "Step: 23300, train/learning_rate: 2.2275107767200097e-05\n",
      "Step: 23300, train/epoch: 5.544978618621826\n",
      "Step: 23310, train/loss: 0.0\n",
      "Step: 23310, train/grad_norm: 0.0002762050717137754\n",
      "Step: 23310, train/learning_rate: 2.22632079385221e-05\n",
      "Step: 23310, train/epoch: 5.547358512878418\n",
      "Step: 23320, train/loss: 0.0\n",
      "Step: 23320, train/grad_norm: 0.005674430634826422\n",
      "Step: 23320, train/learning_rate: 2.2251308109844103e-05\n",
      "Step: 23320, train/epoch: 5.54973840713501\n",
      "Step: 23330, train/loss: 0.0\n",
      "Step: 23330, train/grad_norm: 0.00018863036530092359\n",
      "Step: 23330, train/learning_rate: 2.223941010015551e-05\n",
      "Step: 23330, train/epoch: 5.552117824554443\n",
      "Step: 23340, train/loss: 0.0\n",
      "Step: 23340, train/grad_norm: 2.1321504391380586e-05\n",
      "Step: 23340, train/learning_rate: 2.2227510271477513e-05\n",
      "Step: 23340, train/epoch: 5.554497718811035\n",
      "Step: 23350, train/loss: 0.0\n",
      "Step: 23350, train/grad_norm: 0.00092046003555879\n",
      "Step: 23350, train/learning_rate: 2.221561226178892e-05\n",
      "Step: 23350, train/epoch: 5.556877613067627\n",
      "Step: 23360, train/loss: 0.0\n",
      "Step: 23360, train/grad_norm: 0.00019677441741805524\n",
      "Step: 23360, train/learning_rate: 2.2203712433110923e-05\n",
      "Step: 23360, train/epoch: 5.559257507324219\n",
      "Step: 23370, train/loss: 0.0\n",
      "Step: 23370, train/grad_norm: 0.00013738263805862516\n",
      "Step: 23370, train/learning_rate: 2.2191812604432926e-05\n",
      "Step: 23370, train/epoch: 5.5616374015808105\n",
      "Step: 23380, train/loss: 0.0\n",
      "Step: 23380, train/grad_norm: 0.00042122427839785814\n",
      "Step: 23380, train/learning_rate: 2.2179914594744332e-05\n",
      "Step: 23380, train/epoch: 5.564017295837402\n",
      "Step: 23390, train/loss: 0.0\n",
      "Step: 23390, train/grad_norm: 0.00029737697332166135\n",
      "Step: 23390, train/learning_rate: 2.2168014766066335e-05\n",
      "Step: 23390, train/epoch: 5.566397190093994\n",
      "Step: 23400, train/loss: 0.0\n",
      "Step: 23400, train/grad_norm: 0.00010127221321454272\n",
      "Step: 23400, train/learning_rate: 2.2156116756377742e-05\n",
      "Step: 23400, train/epoch: 5.568776607513428\n",
      "Step: 23410, train/loss: 0.0\n",
      "Step: 23410, train/grad_norm: 0.00013660751574207097\n",
      "Step: 23410, train/learning_rate: 2.2144216927699745e-05\n",
      "Step: 23410, train/epoch: 5.5711565017700195\n",
      "Step: 23420, train/loss: 0.0\n",
      "Step: 23420, train/grad_norm: 0.0002895308716688305\n",
      "Step: 23420, train/learning_rate: 2.2132317099021748e-05\n",
      "Step: 23420, train/epoch: 5.573536396026611\n",
      "Step: 23430, train/loss: 0.09920000284910202\n",
      "Step: 23430, train/grad_norm: 0.000336839904775843\n",
      "Step: 23430, train/learning_rate: 2.2120419089333154e-05\n",
      "Step: 23430, train/epoch: 5.575916290283203\n",
      "Step: 23440, train/loss: 0.0\n",
      "Step: 23440, train/grad_norm: 0.00603482173755765\n",
      "Step: 23440, train/learning_rate: 2.2108519260655157e-05\n",
      "Step: 23440, train/epoch: 5.578296184539795\n",
      "Step: 23450, train/loss: 0.0\n",
      "Step: 23450, train/grad_norm: 0.0010783864418044686\n",
      "Step: 23450, train/learning_rate: 2.2096621250966564e-05\n",
      "Step: 23450, train/epoch: 5.580676078796387\n",
      "Step: 23460, train/loss: 0.0\n",
      "Step: 23460, train/grad_norm: 0.0014073612401261926\n",
      "Step: 23460, train/learning_rate: 2.2084721422288567e-05\n",
      "Step: 23460, train/epoch: 5.58305549621582\n",
      "Step: 23470, train/loss: 0.0\n",
      "Step: 23470, train/grad_norm: 0.002120533725246787\n",
      "Step: 23470, train/learning_rate: 2.207282159361057e-05\n",
      "Step: 23470, train/epoch: 5.585435390472412\n",
      "Step: 23480, train/loss: 0.0\n",
      "Step: 23480, train/grad_norm: 0.000498926208820194\n",
      "Step: 23480, train/learning_rate: 2.2060923583921976e-05\n",
      "Step: 23480, train/epoch: 5.587815284729004\n",
      "Step: 23490, train/loss: 0.0\n",
      "Step: 23490, train/grad_norm: 0.0009813865181058645\n",
      "Step: 23490, train/learning_rate: 2.204902375524398e-05\n",
      "Step: 23490, train/epoch: 5.590195178985596\n",
      "Step: 23500, train/loss: 0.0\n",
      "Step: 23500, train/grad_norm: 0.000927031971514225\n",
      "Step: 23500, train/learning_rate: 2.2037125745555386e-05\n",
      "Step: 23500, train/epoch: 5.5925750732421875\n",
      "Step: 23510, train/loss: 0.04879999905824661\n",
      "Step: 23510, train/grad_norm: 0.000779943831730634\n",
      "Step: 23510, train/learning_rate: 2.202522591687739e-05\n",
      "Step: 23510, train/epoch: 5.594954967498779\n",
      "Step: 23520, train/loss: 0.0\n",
      "Step: 23520, train/grad_norm: 0.01784510724246502\n",
      "Step: 23520, train/learning_rate: 2.2013326088199392e-05\n",
      "Step: 23520, train/epoch: 5.597334384918213\n",
      "Step: 23530, train/loss: 0.22360000014305115\n",
      "Step: 23530, train/grad_norm: 0.04003432020545006\n",
      "Step: 23530, train/learning_rate: 2.20014280785108e-05\n",
      "Step: 23530, train/epoch: 5.599714279174805\n",
      "Step: 23540, train/loss: 9.999999747378752e-05\n",
      "Step: 23540, train/grad_norm: 0.05762734264135361\n",
      "Step: 23540, train/learning_rate: 2.19895282498328e-05\n",
      "Step: 23540, train/epoch: 5.6020941734313965\n",
      "Step: 23550, train/loss: 9.999999747378752e-05\n",
      "Step: 23550, train/grad_norm: 0.0035929600708186626\n",
      "Step: 23550, train/learning_rate: 2.1977630240144208e-05\n",
      "Step: 23550, train/epoch: 5.604474067687988\n",
      "Step: 23560, train/loss: 0.0\n",
      "Step: 23560, train/grad_norm: 0.0004874042351730168\n",
      "Step: 23560, train/learning_rate: 2.196573041146621e-05\n",
      "Step: 23560, train/epoch: 5.60685396194458\n",
      "Step: 23570, train/loss: 0.0\n",
      "Step: 23570, train/grad_norm: 0.00017554320220369846\n",
      "Step: 23570, train/learning_rate: 2.1953832401777618e-05\n",
      "Step: 23570, train/epoch: 5.609233856201172\n",
      "Step: 23580, train/loss: 0.0\n",
      "Step: 23580, train/grad_norm: 2.648956433404237e-05\n",
      "Step: 23580, train/learning_rate: 2.194193257309962e-05\n",
      "Step: 23580, train/epoch: 5.611613750457764\n",
      "Step: 23590, train/loss: 0.08399999886751175\n",
      "Step: 23590, train/grad_norm: 4.8238860472338274e-05\n",
      "Step: 23590, train/learning_rate: 2.1930032744421624e-05\n",
      "Step: 23590, train/epoch: 5.613993167877197\n",
      "Step: 23600, train/loss: 0.0\n",
      "Step: 23600, train/grad_norm: 0.00841198768466711\n",
      "Step: 23600, train/learning_rate: 2.191813473473303e-05\n",
      "Step: 23600, train/epoch: 5.616373062133789\n",
      "Step: 23610, train/loss: 0.002099999925121665\n",
      "Step: 23610, train/grad_norm: 0.00029489901498891413\n",
      "Step: 23610, train/learning_rate: 2.1906234906055033e-05\n",
      "Step: 23610, train/epoch: 5.618752956390381\n",
      "Step: 23620, train/loss: 0.1687999963760376\n",
      "Step: 23620, train/grad_norm: 0.07371008396148682\n",
      "Step: 23620, train/learning_rate: 2.189433689636644e-05\n",
      "Step: 23620, train/epoch: 5.621132850646973\n",
      "Step: 23630, train/loss: 9.999999747378752e-05\n",
      "Step: 23630, train/grad_norm: 0.02799607627093792\n",
      "Step: 23630, train/learning_rate: 2.1882437067688443e-05\n",
      "Step: 23630, train/epoch: 5.6235127449035645\n",
      "Step: 23640, train/loss: 9.999999747378752e-05\n",
      "Step: 23640, train/grad_norm: 0.005677867215126753\n",
      "Step: 23640, train/learning_rate: 2.1870537239010446e-05\n",
      "Step: 23640, train/epoch: 5.625892639160156\n",
      "Step: 23650, train/loss: 0.0\n",
      "Step: 23650, train/grad_norm: 0.0031277774833142757\n",
      "Step: 23650, train/learning_rate: 2.1858639229321852e-05\n",
      "Step: 23650, train/epoch: 5.62827205657959\n",
      "Step: 23660, train/loss: 0.0\n",
      "Step: 23660, train/grad_norm: 0.000407825515139848\n",
      "Step: 23660, train/learning_rate: 2.1846739400643855e-05\n",
      "Step: 23660, train/epoch: 5.630651950836182\n",
      "Step: 23670, train/loss: 0.0\n",
      "Step: 23670, train/grad_norm: 0.0010366712231189013\n",
      "Step: 23670, train/learning_rate: 2.1834841390955262e-05\n",
      "Step: 23670, train/epoch: 5.633031845092773\n",
      "Step: 23680, train/loss: 0.0\n",
      "Step: 23680, train/grad_norm: 0.001784487278200686\n",
      "Step: 23680, train/learning_rate: 2.1822941562277265e-05\n",
      "Step: 23680, train/epoch: 5.635411739349365\n",
      "Step: 23690, train/loss: 0.0\n",
      "Step: 23690, train/grad_norm: 0.0002760695351753384\n",
      "Step: 23690, train/learning_rate: 2.1811041733599268e-05\n",
      "Step: 23690, train/epoch: 5.637791633605957\n",
      "Step: 23700, train/loss: 0.0\n",
      "Step: 23700, train/grad_norm: 0.0017407237319275737\n",
      "Step: 23700, train/learning_rate: 2.1799143723910674e-05\n",
      "Step: 23700, train/epoch: 5.640171527862549\n",
      "Step: 23710, train/loss: 0.0\n",
      "Step: 23710, train/grad_norm: 0.0018452469957992435\n",
      "Step: 23710, train/learning_rate: 2.1787243895232677e-05\n",
      "Step: 23710, train/epoch: 5.642550945281982\n",
      "Step: 23720, train/loss: 0.0\n",
      "Step: 23720, train/grad_norm: 0.0009978108573704958\n",
      "Step: 23720, train/learning_rate: 2.1775345885544084e-05\n",
      "Step: 23720, train/epoch: 5.644930839538574\n",
      "Step: 23730, train/loss: 0.0\n",
      "Step: 23730, train/grad_norm: 0.0010793469846248627\n",
      "Step: 23730, train/learning_rate: 2.1763446056866087e-05\n",
      "Step: 23730, train/epoch: 5.647310733795166\n",
      "Step: 23740, train/loss: 0.0\n",
      "Step: 23740, train/grad_norm: 0.00034457878791727126\n",
      "Step: 23740, train/learning_rate: 2.175154622818809e-05\n",
      "Step: 23740, train/epoch: 5.649690628051758\n",
      "Step: 23750, train/loss: 0.0\n",
      "Step: 23750, train/grad_norm: 0.0007140111993066967\n",
      "Step: 23750, train/learning_rate: 2.1739648218499497e-05\n",
      "Step: 23750, train/epoch: 5.65207052230835\n",
      "Step: 23760, train/loss: 0.0\n",
      "Step: 23760, train/grad_norm: 0.003725421382114291\n",
      "Step: 23760, train/learning_rate: 2.17277483898215e-05\n",
      "Step: 23760, train/epoch: 5.654450416564941\n",
      "Step: 23770, train/loss: 0.0\n",
      "Step: 23770, train/grad_norm: 0.00035402202047407627\n",
      "Step: 23770, train/learning_rate: 2.1715850380132906e-05\n",
      "Step: 23770, train/epoch: 5.656830310821533\n",
      "Step: 23780, train/loss: 0.0\n",
      "Step: 23780, train/grad_norm: 0.0009116766741499305\n",
      "Step: 23780, train/learning_rate: 2.170395055145491e-05\n",
      "Step: 23780, train/epoch: 5.659209728240967\n",
      "Step: 23790, train/loss: 0.0\n",
      "Step: 23790, train/grad_norm: 0.0007068293052725494\n",
      "Step: 23790, train/learning_rate: 2.1692050722776912e-05\n",
      "Step: 23790, train/epoch: 5.661589622497559\n",
      "Step: 23800, train/loss: 0.0\n",
      "Step: 23800, train/grad_norm: 0.0006789418985135853\n",
      "Step: 23800, train/learning_rate: 2.168015271308832e-05\n",
      "Step: 23800, train/epoch: 5.66396951675415\n",
      "Step: 23810, train/loss: 0.0\n",
      "Step: 23810, train/grad_norm: 0.0017375430325046182\n",
      "Step: 23810, train/learning_rate: 2.1668252884410322e-05\n",
      "Step: 23810, train/epoch: 5.666349411010742\n",
      "Step: 23820, train/loss: 0.0\n",
      "Step: 23820, train/grad_norm: 0.001349722733721137\n",
      "Step: 23820, train/learning_rate: 2.1656354874721728e-05\n",
      "Step: 23820, train/epoch: 5.668729305267334\n",
      "Step: 23830, train/loss: 0.0\n",
      "Step: 23830, train/grad_norm: 0.0002238773595308885\n",
      "Step: 23830, train/learning_rate: 2.164445504604373e-05\n",
      "Step: 23830, train/epoch: 5.671109199523926\n",
      "Step: 23840, train/loss: 0.0\n",
      "Step: 23840, train/grad_norm: 0.0001975985651370138\n",
      "Step: 23840, train/learning_rate: 2.1632555217365734e-05\n",
      "Step: 23840, train/epoch: 5.673488616943359\n",
      "Step: 23850, train/loss: 0.0\n",
      "Step: 23850, train/grad_norm: 0.0006233840249478817\n",
      "Step: 23850, train/learning_rate: 2.162065720767714e-05\n",
      "Step: 23850, train/epoch: 5.675868511199951\n",
      "Step: 23860, train/loss: 0.0\n",
      "Step: 23860, train/grad_norm: 0.0009849370690062642\n",
      "Step: 23860, train/learning_rate: 2.1608757378999144e-05\n",
      "Step: 23860, train/epoch: 5.678248405456543\n",
      "Step: 23870, train/loss: 0.0\n",
      "Step: 23870, train/grad_norm: 0.000221681097173132\n",
      "Step: 23870, train/learning_rate: 2.159685936931055e-05\n",
      "Step: 23870, train/epoch: 5.680628299713135\n",
      "Step: 23880, train/loss: 0.0\n",
      "Step: 23880, train/grad_norm: 0.0004967714776284993\n",
      "Step: 23880, train/learning_rate: 2.1584959540632553e-05\n",
      "Step: 23880, train/epoch: 5.683008193969727\n",
      "Step: 23890, train/loss: 0.08299999684095383\n",
      "Step: 23890, train/grad_norm: 633.0752563476562\n",
      "Step: 23890, train/learning_rate: 2.1573059711954556e-05\n",
      "Step: 23890, train/epoch: 5.685388088226318\n",
      "Step: 23900, train/loss: 0.0005000000237487257\n",
      "Step: 23900, train/grad_norm: 0.011940235272049904\n",
      "Step: 23900, train/learning_rate: 2.1561161702265963e-05\n",
      "Step: 23900, train/epoch: 5.687767505645752\n",
      "Step: 23910, train/loss: 0.0\n",
      "Step: 23910, train/grad_norm: 0.00023607838375028223\n",
      "Step: 23910, train/learning_rate: 2.1549261873587966e-05\n",
      "Step: 23910, train/epoch: 5.690147399902344\n",
      "Step: 23920, train/loss: 0.0\n",
      "Step: 23920, train/grad_norm: 0.00016372959362342954\n",
      "Step: 23920, train/learning_rate: 2.1537363863899373e-05\n",
      "Step: 23920, train/epoch: 5.6925272941589355\n",
      "Step: 23930, train/loss: 0.08479999750852585\n",
      "Step: 23930, train/grad_norm: 0.021143201738595963\n",
      "Step: 23930, train/learning_rate: 2.1525464035221376e-05\n",
      "Step: 23930, train/epoch: 5.694907188415527\n",
      "Step: 23940, train/loss: 0.40389999747276306\n",
      "Step: 23940, train/grad_norm: 91.66707611083984\n",
      "Step: 23940, train/learning_rate: 2.151356420654338e-05\n",
      "Step: 23940, train/epoch: 5.697287082672119\n",
      "Step: 23950, train/loss: 0.000699999975040555\n",
      "Step: 23950, train/grad_norm: 0.043282728642225266\n",
      "Step: 23950, train/learning_rate: 2.1501666196854785e-05\n",
      "Step: 23950, train/epoch: 5.699666976928711\n",
      "Step: 23960, train/loss: 0.00019999999494757503\n",
      "Step: 23960, train/grad_norm: 2.0213022232055664\n",
      "Step: 23960, train/learning_rate: 2.1489766368176788e-05\n",
      "Step: 23960, train/epoch: 5.702046871185303\n",
      "Step: 23970, train/loss: 0.0\n",
      "Step: 23970, train/grad_norm: 2.2551763322553597e-05\n",
      "Step: 23970, train/learning_rate: 2.1477868358488195e-05\n",
      "Step: 23970, train/epoch: 5.704426288604736\n",
      "Step: 23980, train/loss: 0.0\n",
      "Step: 23980, train/grad_norm: 1.9298116967547685e-05\n",
      "Step: 23980, train/learning_rate: 2.1465968529810198e-05\n",
      "Step: 23980, train/epoch: 5.706806182861328\n",
      "Step: 23990, train/loss: 0.0\n",
      "Step: 23990, train/grad_norm: 1.8366150698057027e-06\n",
      "Step: 23990, train/learning_rate: 2.14540687011322e-05\n",
      "Step: 23990, train/epoch: 5.70918607711792\n",
      "Step: 24000, train/loss: 0.0\n",
      "Step: 24000, train/grad_norm: 3.44457225764927e-06\n",
      "Step: 24000, train/learning_rate: 2.1442170691443607e-05\n",
      "Step: 24000, train/epoch: 5.711565971374512\n",
      "Step: 24010, train/loss: 0.0\n",
      "Step: 24010, train/grad_norm: 1.2682232863880927e-06\n",
      "Step: 24010, train/learning_rate: 2.143027086276561e-05\n",
      "Step: 24010, train/epoch: 5.7139458656311035\n",
      "Step: 24020, train/loss: 0.005200000014156103\n",
      "Step: 24020, train/grad_norm: 3.675752850540448e-06\n",
      "Step: 24020, train/learning_rate: 2.1418372853077017e-05\n",
      "Step: 24020, train/epoch: 5.716325759887695\n",
      "Step: 24030, train/loss: 0.0\n",
      "Step: 24030, train/grad_norm: 8.801627018328873e-07\n",
      "Step: 24030, train/learning_rate: 2.140647302439902e-05\n",
      "Step: 24030, train/epoch: 5.718705177307129\n",
      "Step: 24040, train/loss: 0.0\n",
      "Step: 24040, train/grad_norm: 1.249332285624405e-06\n",
      "Step: 24040, train/learning_rate: 2.1394573195721023e-05\n",
      "Step: 24040, train/epoch: 5.721085071563721\n",
      "Step: 24050, train/loss: 0.0\n",
      "Step: 24050, train/grad_norm: 1.1130178791063372e-05\n",
      "Step: 24050, train/learning_rate: 2.138267518603243e-05\n",
      "Step: 24050, train/epoch: 5.7234649658203125\n",
      "Step: 24060, train/loss: 0.1469999998807907\n",
      "Step: 24060, train/grad_norm: 5.660221177095082e-06\n",
      "Step: 24060, train/learning_rate: 2.1370775357354432e-05\n",
      "Step: 24060, train/epoch: 5.725844860076904\n",
      "Step: 24070, train/loss: 0.0\n",
      "Step: 24070, train/grad_norm: 4.131364403292537e-05\n",
      "Step: 24070, train/learning_rate: 2.135887734766584e-05\n",
      "Step: 24070, train/epoch: 5.728224754333496\n",
      "Step: 24080, train/loss: 0.0\n",
      "Step: 24080, train/grad_norm: 8.702347258804366e-05\n",
      "Step: 24080, train/learning_rate: 2.1346977518987842e-05\n",
      "Step: 24080, train/epoch: 5.730604648590088\n",
      "Step: 24090, train/loss: 0.06520000100135803\n",
      "Step: 24090, train/grad_norm: 0.0006914255209267139\n",
      "Step: 24090, train/learning_rate: 2.1335077690309845e-05\n",
      "Step: 24090, train/epoch: 5.7329840660095215\n",
      "Step: 24100, train/loss: 0.0\n",
      "Step: 24100, train/grad_norm: 0.005420427303761244\n",
      "Step: 24100, train/learning_rate: 2.132317968062125e-05\n",
      "Step: 24100, train/epoch: 5.735363960266113\n",
      "Step: 24110, train/loss: 0.0\n",
      "Step: 24110, train/grad_norm: 0.0011552810901775956\n",
      "Step: 24110, train/learning_rate: 2.1311279851943254e-05\n",
      "Step: 24110, train/epoch: 5.737743854522705\n",
      "Step: 24120, train/loss: 0.0\n",
      "Step: 24120, train/grad_norm: 0.0006088929949328303\n",
      "Step: 24120, train/learning_rate: 2.129938184225466e-05\n",
      "Step: 24120, train/epoch: 5.740123748779297\n",
      "Step: 24130, train/loss: 0.0\n",
      "Step: 24130, train/grad_norm: 0.0003981835616286844\n",
      "Step: 24130, train/learning_rate: 2.1287482013576664e-05\n",
      "Step: 24130, train/epoch: 5.742503643035889\n",
      "Step: 24140, train/loss: 0.0\n",
      "Step: 24140, train/grad_norm: 0.0028220813255757093\n",
      "Step: 24140, train/learning_rate: 2.1275582184898667e-05\n",
      "Step: 24140, train/epoch: 5.7448835372924805\n",
      "Step: 24150, train/loss: 0.0003000000142492354\n",
      "Step: 24150, train/grad_norm: 3.432457685470581\n",
      "Step: 24150, train/learning_rate: 2.1263684175210074e-05\n",
      "Step: 24150, train/epoch: 5.747263431549072\n",
      "Step: 24160, train/loss: 0.0\n",
      "Step: 24160, train/grad_norm: 3.483791442704387e-05\n",
      "Step: 24160, train/learning_rate: 2.1251784346532077e-05\n",
      "Step: 24160, train/epoch: 5.749642848968506\n",
      "Step: 24170, train/loss: 0.16249999403953552\n",
      "Step: 24170, train/grad_norm: 0.00038301481981761754\n",
      "Step: 24170, train/learning_rate: 2.1239886336843483e-05\n",
      "Step: 24170, train/epoch: 5.752022743225098\n",
      "Step: 24180, train/loss: 0.0\n",
      "Step: 24180, train/grad_norm: 0.019494712352752686\n",
      "Step: 24180, train/learning_rate: 2.1227986508165486e-05\n",
      "Step: 24180, train/epoch: 5.7544026374816895\n",
      "Step: 24190, train/loss: 0.0\n",
      "Step: 24190, train/grad_norm: 0.02380051650106907\n",
      "Step: 24190, train/learning_rate: 2.121608667948749e-05\n",
      "Step: 24190, train/epoch: 5.756782531738281\n",
      "Step: 24200, train/loss: 0.0\n",
      "Step: 24200, train/grad_norm: 0.005074948538094759\n",
      "Step: 24200, train/learning_rate: 2.1204188669798896e-05\n",
      "Step: 24200, train/epoch: 5.759162425994873\n",
      "Step: 24210, train/loss: 0.0\n",
      "Step: 24210, train/grad_norm: 0.0020799152553081512\n",
      "Step: 24210, train/learning_rate: 2.11922888411209e-05\n",
      "Step: 24210, train/epoch: 5.761542320251465\n",
      "Step: 24220, train/loss: 0.0\n",
      "Step: 24220, train/grad_norm: 0.0018503705505281687\n",
      "Step: 24220, train/learning_rate: 2.1180390831432305e-05\n",
      "Step: 24220, train/epoch: 5.763921737670898\n",
      "Step: 24230, train/loss: 0.0\n",
      "Step: 24230, train/grad_norm: 0.00044374121353030205\n",
      "Step: 24230, train/learning_rate: 2.1168491002754308e-05\n",
      "Step: 24230, train/epoch: 5.76630163192749\n",
      "Step: 24240, train/loss: 0.0\n",
      "Step: 24240, train/grad_norm: 0.0005980387213639915\n",
      "Step: 24240, train/learning_rate: 2.1156592993065715e-05\n",
      "Step: 24240, train/epoch: 5.768681526184082\n",
      "Step: 24250, train/loss: 0.0\n",
      "Step: 24250, train/grad_norm: 0.0017084063729271293\n",
      "Step: 24250, train/learning_rate: 2.1144693164387718e-05\n",
      "Step: 24250, train/epoch: 5.771061420440674\n",
      "Step: 24260, train/loss: 0.0\n",
      "Step: 24260, train/grad_norm: 0.0008099600090645254\n",
      "Step: 24260, train/learning_rate: 2.113279333570972e-05\n",
      "Step: 24260, train/epoch: 5.773441314697266\n",
      "Step: 24270, train/loss: 0.0\n",
      "Step: 24270, train/grad_norm: 0.0005521640414372087\n",
      "Step: 24270, train/learning_rate: 2.1120895326021127e-05\n",
      "Step: 24270, train/epoch: 5.775821208953857\n",
      "Step: 24280, train/loss: 0.0\n",
      "Step: 24280, train/grad_norm: 0.012096325866878033\n",
      "Step: 24280, train/learning_rate: 2.110899549734313e-05\n",
      "Step: 24280, train/epoch: 5.778200626373291\n",
      "Step: 24290, train/loss: 0.0\n",
      "Step: 24290, train/grad_norm: 0.0005241733160801232\n",
      "Step: 24290, train/learning_rate: 2.1097097487654537e-05\n",
      "Step: 24290, train/epoch: 5.780580520629883\n",
      "Step: 24300, train/loss: 0.0\n",
      "Step: 24300, train/grad_norm: 0.0007086287369020283\n",
      "Step: 24300, train/learning_rate: 2.108519765897654e-05\n",
      "Step: 24300, train/epoch: 5.782960414886475\n",
      "Step: 24310, train/loss: 0.0\n",
      "Step: 24310, train/grad_norm: 0.000126624945551157\n",
      "Step: 24310, train/learning_rate: 2.1073297830298543e-05\n",
      "Step: 24310, train/epoch: 5.785340309143066\n",
      "Step: 24320, train/loss: 0.0\n",
      "Step: 24320, train/grad_norm: 1.0507222214073408e-05\n",
      "Step: 24320, train/learning_rate: 2.106139982060995e-05\n",
      "Step: 24320, train/epoch: 5.787720203399658\n",
      "Step: 24330, train/loss: 0.0\n",
      "Step: 24330, train/grad_norm: 0.009273436851799488\n",
      "Step: 24330, train/learning_rate: 2.1049499991931953e-05\n",
      "Step: 24330, train/epoch: 5.79010009765625\n",
      "Step: 24340, train/loss: 0.0\n",
      "Step: 24340, train/grad_norm: 0.0005817108321934938\n",
      "Step: 24340, train/learning_rate: 2.103760198224336e-05\n",
      "Step: 24340, train/epoch: 5.792479991912842\n",
      "Step: 24350, train/loss: 0.0\n",
      "Step: 24350, train/grad_norm: 0.00033994673867709935\n",
      "Step: 24350, train/learning_rate: 2.1025702153565362e-05\n",
      "Step: 24350, train/epoch: 5.794859409332275\n",
      "Step: 24360, train/loss: 0.0\n",
      "Step: 24360, train/grad_norm: 0.003654082305729389\n",
      "Step: 24360, train/learning_rate: 2.1013802324887365e-05\n",
      "Step: 24360, train/epoch: 5.797239303588867\n",
      "Step: 24370, train/loss: 0.0\n",
      "Step: 24370, train/grad_norm: 0.00041558139491826296\n",
      "Step: 24370, train/learning_rate: 2.100190431519877e-05\n",
      "Step: 24370, train/epoch: 5.799619197845459\n",
      "Step: 24380, train/loss: 0.05310000106692314\n",
      "Step: 24380, train/grad_norm: 0.0007260050624608994\n",
      "Step: 24380, train/learning_rate: 2.0990004486520775e-05\n",
      "Step: 24380, train/epoch: 5.801999092102051\n",
      "Step: 24390, train/loss: 0.0\n",
      "Step: 24390, train/grad_norm: 0.0009599387412890792\n",
      "Step: 24390, train/learning_rate: 2.097810647683218e-05\n",
      "Step: 24390, train/epoch: 5.804378986358643\n",
      "Step: 24400, train/loss: 0.0\n",
      "Step: 24400, train/grad_norm: 0.0394953191280365\n",
      "Step: 24400, train/learning_rate: 2.0966206648154184e-05\n",
      "Step: 24400, train/epoch: 5.806758880615234\n",
      "Step: 24410, train/loss: 0.0\n",
      "Step: 24410, train/grad_norm: 0.0007078038179315627\n",
      "Step: 24410, train/learning_rate: 2.0954306819476187e-05\n",
      "Step: 24410, train/epoch: 5.809138298034668\n",
      "Step: 24420, train/loss: 0.0\n",
      "Step: 24420, train/grad_norm: 0.0010295226238667965\n",
      "Step: 24420, train/learning_rate: 2.0942408809787594e-05\n",
      "Step: 24420, train/epoch: 5.81151819229126\n",
      "Step: 24430, train/loss: 0.0\n",
      "Step: 24430, train/grad_norm: 7.734609971521422e-05\n",
      "Step: 24430, train/learning_rate: 2.0930508981109597e-05\n",
      "Step: 24430, train/epoch: 5.813898086547852\n",
      "Step: 24440, train/loss: 0.0\n",
      "Step: 24440, train/grad_norm: 0.0018382705748081207\n",
      "Step: 24440, train/learning_rate: 2.0918610971421003e-05\n",
      "Step: 24440, train/epoch: 5.816277980804443\n",
      "Step: 24450, train/loss: 0.0\n",
      "Step: 24450, train/grad_norm: 0.00027953865355812013\n",
      "Step: 24450, train/learning_rate: 2.0906711142743006e-05\n",
      "Step: 24450, train/epoch: 5.818657875061035\n",
      "Step: 24460, train/loss: 0.0\n",
      "Step: 24460, train/grad_norm: 0.005270251538604498\n",
      "Step: 24460, train/learning_rate: 2.089481131406501e-05\n",
      "Step: 24460, train/epoch: 5.821037769317627\n",
      "Step: 24470, train/loss: 0.0\n",
      "Step: 24470, train/grad_norm: 0.001171320560388267\n",
      "Step: 24470, train/learning_rate: 2.0882913304376416e-05\n",
      "Step: 24470, train/epoch: 5.8234171867370605\n",
      "Step: 24480, train/loss: 0.0\n",
      "Step: 24480, train/grad_norm: 0.0005481144762597978\n",
      "Step: 24480, train/learning_rate: 2.087101347569842e-05\n",
      "Step: 24480, train/epoch: 5.825797080993652\n",
      "Step: 24490, train/loss: 0.0\n",
      "Step: 24490, train/grad_norm: 0.0005452784826047719\n",
      "Step: 24490, train/learning_rate: 2.0859115466009825e-05\n",
      "Step: 24490, train/epoch: 5.828176975250244\n",
      "Step: 24500, train/loss: 0.0\n",
      "Step: 24500, train/grad_norm: 0.00018203354557044804\n",
      "Step: 24500, train/learning_rate: 2.084721563733183e-05\n",
      "Step: 24500, train/epoch: 5.830556869506836\n",
      "Step: 24510, train/loss: 0.0\n",
      "Step: 24510, train/grad_norm: 0.0009950667154043913\n",
      "Step: 24510, train/learning_rate: 2.083531580865383e-05\n",
      "Step: 24510, train/epoch: 5.832936763763428\n",
      "Step: 24520, train/loss: 0.0\n",
      "Step: 24520, train/grad_norm: 0.0006182874785736203\n",
      "Step: 24520, train/learning_rate: 2.0823417798965238e-05\n",
      "Step: 24520, train/epoch: 5.8353166580200195\n",
      "Step: 24530, train/loss: 0.0\n",
      "Step: 24530, train/grad_norm: 0.00036326737608760595\n",
      "Step: 24530, train/learning_rate: 2.081151797028724e-05\n",
      "Step: 24530, train/epoch: 5.837696552276611\n",
      "Step: 24540, train/loss: 0.1257999986410141\n",
      "Step: 24540, train/grad_norm: 0.0009624980739317834\n",
      "Step: 24540, train/learning_rate: 2.0799619960598648e-05\n",
      "Step: 24540, train/epoch: 5.840075969696045\n",
      "Step: 24550, train/loss: 0.0\n",
      "Step: 24550, train/grad_norm: 0.0024454579688608646\n",
      "Step: 24550, train/learning_rate: 2.078772013192065e-05\n",
      "Step: 24550, train/epoch: 5.842455863952637\n",
      "Step: 24560, train/loss: 0.0\n",
      "Step: 24560, train/grad_norm: 0.0036543509922921658\n",
      "Step: 24560, train/learning_rate: 2.0775820303242654e-05\n",
      "Step: 24560, train/epoch: 5.8448357582092285\n",
      "Step: 24570, train/loss: 0.0\n",
      "Step: 24570, train/grad_norm: 0.0069315386936068535\n",
      "Step: 24570, train/learning_rate: 2.076392229355406e-05\n",
      "Step: 24570, train/epoch: 5.84721565246582\n",
      "Step: 24580, train/loss: 0.0\n",
      "Step: 24580, train/grad_norm: 0.002349199028685689\n",
      "Step: 24580, train/learning_rate: 2.0752022464876063e-05\n",
      "Step: 24580, train/epoch: 5.849595546722412\n",
      "Step: 24590, train/loss: 0.0\n",
      "Step: 24590, train/grad_norm: 0.0036071392241865396\n",
      "Step: 24590, train/learning_rate: 2.074012445518747e-05\n",
      "Step: 24590, train/epoch: 5.851975440979004\n",
      "Step: 24600, train/loss: 0.0\n",
      "Step: 24600, train/grad_norm: 0.003249132540076971\n",
      "Step: 24600, train/learning_rate: 2.0728224626509473e-05\n",
      "Step: 24600, train/epoch: 5.8543548583984375\n",
      "Step: 24610, train/loss: 0.0\n",
      "Step: 24610, train/grad_norm: 0.002069530775770545\n",
      "Step: 24610, train/learning_rate: 2.0716324797831476e-05\n",
      "Step: 24610, train/epoch: 5.856734752655029\n",
      "Step: 24620, train/loss: 0.0\n",
      "Step: 24620, train/grad_norm: 0.0009264462860301137\n",
      "Step: 24620, train/learning_rate: 2.0704426788142882e-05\n",
      "Step: 24620, train/epoch: 5.859114646911621\n",
      "Step: 24630, train/loss: 0.0\n",
      "Step: 24630, train/grad_norm: 0.0006102140177972615\n",
      "Step: 24630, train/learning_rate: 2.0692526959464885e-05\n",
      "Step: 24630, train/epoch: 5.861494541168213\n",
      "Step: 24640, train/loss: 9.999999747378752e-05\n",
      "Step: 24640, train/grad_norm: 0.0004244321899022907\n",
      "Step: 24640, train/learning_rate: 2.0680628949776292e-05\n",
      "Step: 24640, train/epoch: 5.863874435424805\n",
      "Step: 24650, train/loss: 0.0\n",
      "Step: 24650, train/grad_norm: 0.00025764357997104526\n",
      "Step: 24650, train/learning_rate: 2.0668729121098295e-05\n",
      "Step: 24650, train/epoch: 5.8662543296813965\n",
      "Step: 24660, train/loss: 9.999999747378752e-05\n",
      "Step: 24660, train/grad_norm: 0.06467138230800629\n",
      "Step: 24660, train/learning_rate: 2.0656829292420298e-05\n",
      "Step: 24660, train/epoch: 5.86863374710083\n",
      "Step: 24670, train/loss: 0.0\n",
      "Step: 24670, train/grad_norm: 7.414262654492632e-05\n",
      "Step: 24670, train/learning_rate: 2.0644931282731704e-05\n",
      "Step: 24670, train/epoch: 5.871013641357422\n",
      "Step: 24680, train/loss: 0.0\n",
      "Step: 24680, train/grad_norm: 5.9704590967157856e-05\n",
      "Step: 24680, train/learning_rate: 2.0633031454053707e-05\n",
      "Step: 24680, train/epoch: 5.873393535614014\n",
      "Step: 24690, train/loss: 0.0\n",
      "Step: 24690, train/grad_norm: 1.3397784641711041e-05\n",
      "Step: 24690, train/learning_rate: 2.0621133444365114e-05\n",
      "Step: 24690, train/epoch: 5.8757734298706055\n",
      "Step: 24700, train/loss: 0.0\n",
      "Step: 24700, train/grad_norm: 0.0001392393751302734\n",
      "Step: 24700, train/learning_rate: 2.0609233615687117e-05\n",
      "Step: 24700, train/epoch: 5.878153324127197\n",
      "Step: 24710, train/loss: 0.15549999475479126\n",
      "Step: 24710, train/grad_norm: 3.143269623251399e-06\n",
      "Step: 24710, train/learning_rate: 2.059733378700912e-05\n",
      "Step: 24710, train/epoch: 5.880533218383789\n",
      "Step: 24720, train/loss: 0.0\n",
      "Step: 24720, train/grad_norm: 0.0011630674125626683\n",
      "Step: 24720, train/learning_rate: 2.0585435777320527e-05\n",
      "Step: 24720, train/epoch: 5.882913112640381\n",
      "Step: 24730, train/loss: 0.0\n",
      "Step: 24730, train/grad_norm: 0.006354506127536297\n",
      "Step: 24730, train/learning_rate: 2.057353594864253e-05\n",
      "Step: 24730, train/epoch: 5.8852925300598145\n",
      "Step: 24740, train/loss: 0.0\n",
      "Step: 24740, train/grad_norm: 0.061747677624225616\n",
      "Step: 24740, train/learning_rate: 2.0561637938953936e-05\n",
      "Step: 24740, train/epoch: 5.887672424316406\n",
      "Step: 24750, train/loss: 0.0\n",
      "Step: 24750, train/grad_norm: 0.0024291439913213253\n",
      "Step: 24750, train/learning_rate: 2.054973811027594e-05\n",
      "Step: 24750, train/epoch: 5.890052318572998\n",
      "Step: 24760, train/loss: 0.0\n",
      "Step: 24760, train/grad_norm: 0.0014687867369502783\n",
      "Step: 24760, train/learning_rate: 2.0537838281597942e-05\n",
      "Step: 24760, train/epoch: 5.89243221282959\n",
      "Step: 24770, train/loss: 0.0\n",
      "Step: 24770, train/grad_norm: 0.0003772565396502614\n",
      "Step: 24770, train/learning_rate: 2.052594027190935e-05\n",
      "Step: 24770, train/epoch: 5.894812107086182\n",
      "Step: 24780, train/loss: 0.0\n",
      "Step: 24780, train/grad_norm: 0.00098532869014889\n",
      "Step: 24780, train/learning_rate: 2.051404044323135e-05\n",
      "Step: 24780, train/epoch: 5.897192001342773\n",
      "Step: 24790, train/loss: 0.0\n",
      "Step: 24790, train/grad_norm: 0.000434673682320863\n",
      "Step: 24790, train/learning_rate: 2.0502142433542758e-05\n",
      "Step: 24790, train/epoch: 5.899571418762207\n",
      "Step: 24800, train/loss: 0.11640000343322754\n",
      "Step: 24800, train/grad_norm: 0.03704558685421944\n",
      "Step: 24800, train/learning_rate: 2.049024260486476e-05\n",
      "Step: 24800, train/epoch: 5.901951313018799\n",
      "Step: 24810, train/loss: 0.0\n",
      "Step: 24810, train/grad_norm: 0.011038866825401783\n",
      "Step: 24810, train/learning_rate: 2.0478342776186764e-05\n",
      "Step: 24810, train/epoch: 5.904331207275391\n",
      "Step: 24820, train/loss: 0.0\n",
      "Step: 24820, train/grad_norm: 0.015176057815551758\n",
      "Step: 24820, train/learning_rate: 2.046644476649817e-05\n",
      "Step: 24820, train/epoch: 5.906711101531982\n",
      "Step: 24830, train/loss: 0.0\n",
      "Step: 24830, train/grad_norm: 0.007561434060335159\n",
      "Step: 24830, train/learning_rate: 2.0454544937820174e-05\n",
      "Step: 24830, train/epoch: 5.909090995788574\n",
      "Step: 24840, train/loss: 9.999999747378752e-05\n",
      "Step: 24840, train/grad_norm: 0.0005448759184218943\n",
      "Step: 24840, train/learning_rate: 2.044264692813158e-05\n",
      "Step: 24840, train/epoch: 5.911470890045166\n",
      "Step: 24850, train/loss: 0.0\n",
      "Step: 24850, train/grad_norm: 0.0004042872751597315\n",
      "Step: 24850, train/learning_rate: 2.0430747099453583e-05\n",
      "Step: 24850, train/epoch: 5.913850784301758\n",
      "Step: 24860, train/loss: 0.0\n",
      "Step: 24860, train/grad_norm: 0.00034900367609225214\n",
      "Step: 24860, train/learning_rate: 2.0418847270775586e-05\n",
      "Step: 24860, train/epoch: 5.916230201721191\n",
      "Step: 24870, train/loss: 0.0\n",
      "Step: 24870, train/grad_norm: 0.00043614927562884986\n",
      "Step: 24870, train/learning_rate: 2.0406949261086993e-05\n",
      "Step: 24870, train/epoch: 5.918610095977783\n",
      "Step: 24880, train/loss: 0.0\n",
      "Step: 24880, train/grad_norm: 0.00016009857063181698\n",
      "Step: 24880, train/learning_rate: 2.0395049432408996e-05\n",
      "Step: 24880, train/epoch: 5.920989990234375\n",
      "Step: 24890, train/loss: 0.0\n",
      "Step: 24890, train/grad_norm: 0.00019121973309665918\n",
      "Step: 24890, train/learning_rate: 2.0383151422720402e-05\n",
      "Step: 24890, train/epoch: 5.923369884490967\n",
      "Step: 24900, train/loss: 0.0\n",
      "Step: 24900, train/grad_norm: 0.00013493435108102858\n",
      "Step: 24900, train/learning_rate: 2.0371251594042405e-05\n",
      "Step: 24900, train/epoch: 5.925749778747559\n",
      "Step: 24910, train/loss: 0.0\n",
      "Step: 24910, train/grad_norm: 0.00018612371059134603\n",
      "Step: 24910, train/learning_rate: 2.0359353584353812e-05\n",
      "Step: 24910, train/epoch: 5.92812967300415\n",
      "Step: 24920, train/loss: 0.09139999747276306\n",
      "Step: 24920, train/grad_norm: 0.0004422571219038218\n",
      "Step: 24920, train/learning_rate: 2.0347453755675815e-05\n",
      "Step: 24920, train/epoch: 5.930509090423584\n",
      "Step: 24930, train/loss: 9.999999747378752e-05\n",
      "Step: 24930, train/grad_norm: 0.14125677943229675\n",
      "Step: 24930, train/learning_rate: 2.0335553926997818e-05\n",
      "Step: 24930, train/epoch: 5.932888984680176\n",
      "Step: 24940, train/loss: 0.0\n",
      "Step: 24940, train/grad_norm: 0.0006148314569145441\n",
      "Step: 24940, train/learning_rate: 2.0323655917309225e-05\n",
      "Step: 24940, train/epoch: 5.935268878936768\n",
      "Step: 24950, train/loss: 0.0\n",
      "Step: 24950, train/grad_norm: 0.0005659994203597307\n",
      "Step: 24950, train/learning_rate: 2.0311756088631228e-05\n",
      "Step: 24950, train/epoch: 5.937648773193359\n",
      "Step: 24960, train/loss: 0.0\n",
      "Step: 24960, train/grad_norm: 0.0013351384550333023\n",
      "Step: 24960, train/learning_rate: 2.0299858078942634e-05\n",
      "Step: 24960, train/epoch: 5.940028667449951\n",
      "Step: 24970, train/loss: 0.0\n",
      "Step: 24970, train/grad_norm: 0.00018501115846447647\n",
      "Step: 24970, train/learning_rate: 2.0287958250264637e-05\n",
      "Step: 24970, train/epoch: 5.942408561706543\n",
      "Step: 24980, train/loss: 0.1468999981880188\n",
      "Step: 24980, train/grad_norm: 0.001059740548953414\n",
      "Step: 24980, train/learning_rate: 2.027605842158664e-05\n",
      "Step: 24980, train/epoch: 5.944787979125977\n",
      "Step: 24990, train/loss: 0.0\n",
      "Step: 24990, train/grad_norm: 0.017697542905807495\n",
      "Step: 24990, train/learning_rate: 2.0264160411898047e-05\n",
      "Step: 24990, train/epoch: 5.947167873382568\n",
      "Step: 25000, train/loss: 9.999999747378752e-05\n",
      "Step: 25000, train/grad_norm: 0.016756460070610046\n",
      "Step: 25000, train/learning_rate: 2.025226058322005e-05\n",
      "Step: 25000, train/epoch: 5.94954776763916\n",
      "Step: 25010, train/loss: 9.999999747378752e-05\n",
      "Step: 25010, train/grad_norm: 0.004294075537472963\n",
      "Step: 25010, train/learning_rate: 2.0240362573531456e-05\n",
      "Step: 25010, train/epoch: 5.951927661895752\n",
      "Step: 25020, train/loss: 0.12809999287128448\n",
      "Step: 25020, train/grad_norm: 0.0021047538612037897\n",
      "Step: 25020, train/learning_rate: 2.022846274485346e-05\n",
      "Step: 25020, train/epoch: 5.954307556152344\n",
      "Step: 25030, train/loss: 0.0\n",
      "Step: 25030, train/grad_norm: 0.009784352034330368\n",
      "Step: 25030, train/learning_rate: 2.0216562916175462e-05\n",
      "Step: 25030, train/epoch: 5.9566874504089355\n",
      "Step: 25040, train/loss: 9.999999747378752e-05\n",
      "Step: 25040, train/grad_norm: 0.005507983732968569\n",
      "Step: 25040, train/learning_rate: 2.020466490648687e-05\n",
      "Step: 25040, train/epoch: 5.959067344665527\n",
      "Step: 25050, train/loss: 0.0\n",
      "Step: 25050, train/grad_norm: 0.01575782708823681\n",
      "Step: 25050, train/learning_rate: 2.0192765077808872e-05\n",
      "Step: 25050, train/epoch: 5.961446762084961\n",
      "Step: 25060, train/loss: 0.0\n",
      "Step: 25060, train/grad_norm: 0.02163437008857727\n",
      "Step: 25060, train/learning_rate: 2.018086706812028e-05\n",
      "Step: 25060, train/epoch: 5.963826656341553\n",
      "Step: 25070, train/loss: 0.0\n",
      "Step: 25070, train/grad_norm: 0.002412561560049653\n",
      "Step: 25070, train/learning_rate: 2.016896723944228e-05\n",
      "Step: 25070, train/epoch: 5.9662065505981445\n",
      "Step: 25080, train/loss: 0.0\n",
      "Step: 25080, train/grad_norm: 0.0035749112721532583\n",
      "Step: 25080, train/learning_rate: 2.0157067410764284e-05\n",
      "Step: 25080, train/epoch: 5.968586444854736\n",
      "Step: 25090, train/loss: 0.0\n",
      "Step: 25090, train/grad_norm: 0.0012534199049696326\n",
      "Step: 25090, train/learning_rate: 2.014516940107569e-05\n",
      "Step: 25090, train/epoch: 5.970966339111328\n",
      "Step: 25100, train/loss: 0.0\n",
      "Step: 25100, train/grad_norm: 0.0024157725274562836\n",
      "Step: 25100, train/learning_rate: 2.0133269572397694e-05\n",
      "Step: 25100, train/epoch: 5.97334623336792\n",
      "Step: 25110, train/loss: 0.0\n",
      "Step: 25110, train/grad_norm: 0.0010898290202021599\n",
      "Step: 25110, train/learning_rate: 2.01213715627091e-05\n",
      "Step: 25110, train/epoch: 5.9757256507873535\n",
      "Step: 25120, train/loss: 0.0\n",
      "Step: 25120, train/grad_norm: 0.0015835175290703773\n",
      "Step: 25120, train/learning_rate: 2.0109471734031104e-05\n",
      "Step: 25120, train/epoch: 5.978105545043945\n",
      "Step: 25130, train/loss: 0.0\n",
      "Step: 25130, train/grad_norm: 0.0006539120222441852\n",
      "Step: 25130, train/learning_rate: 2.0097571905353107e-05\n",
      "Step: 25130, train/epoch: 5.980485439300537\n",
      "Step: 25140, train/loss: 0.0\n",
      "Step: 25140, train/grad_norm: 0.0010630397591739893\n",
      "Step: 25140, train/learning_rate: 2.0085673895664513e-05\n",
      "Step: 25140, train/epoch: 5.982865333557129\n",
      "Step: 25150, train/loss: 0.0\n",
      "Step: 25150, train/grad_norm: 0.00029352493584156036\n",
      "Step: 25150, train/learning_rate: 2.0073774066986516e-05\n",
      "Step: 25150, train/epoch: 5.985245227813721\n",
      "Step: 25160, train/loss: 0.0\n",
      "Step: 25160, train/grad_norm: 3.190402276231907e-05\n",
      "Step: 25160, train/learning_rate: 2.0061876057297923e-05\n",
      "Step: 25160, train/epoch: 5.9876251220703125\n",
      "Step: 25170, train/loss: 0.06800000369548798\n",
      "Step: 25170, train/grad_norm: 0.0016488636611029506\n",
      "Step: 25170, train/learning_rate: 2.0049976228619926e-05\n",
      "Step: 25170, train/epoch: 5.990004539489746\n",
      "Step: 25180, train/loss: 0.0\n",
      "Step: 25180, train/grad_norm: 2.9042641472187825e-05\n",
      "Step: 25180, train/learning_rate: 2.003807639994193e-05\n",
      "Step: 25180, train/epoch: 5.992384433746338\n",
      "Step: 25190, train/loss: 0.0\n",
      "Step: 25190, train/grad_norm: 0.0003076812135986984\n",
      "Step: 25190, train/learning_rate: 2.0026178390253335e-05\n",
      "Step: 25190, train/epoch: 5.99476432800293\n",
      "Step: 25200, train/loss: 0.0\n",
      "Step: 25200, train/grad_norm: 0.0014673878904432058\n",
      "Step: 25200, train/learning_rate: 2.0014278561575338e-05\n",
      "Step: 25200, train/epoch: 5.9971442222595215\n",
      "Step: 25210, train/loss: 0.0\n",
      "Step: 25210, train/grad_norm: 0.005753015633672476\n",
      "Step: 25210, train/learning_rate: 2.0002380551886745e-05\n",
      "Step: 25210, train/epoch: 5.999524116516113\n",
      "Step: 25212, eval/loss: 0.07347122579813004\n",
      "Step: 25212, eval/accuracy: 0.9919477701187134\n",
      "Step: 25212, eval/f1: 0.9914686679840088\n",
      "Step: 25212, eval/runtime: 856.3023071289062\n",
      "Step: 25212, eval/samples_per_second: 8.411999702453613\n",
      "Step: 25212, eval/steps_per_second: 1.0520000457763672\n",
      "Step: 25212, train/epoch: 6.0\n",
      "Step: 25220, train/loss: 0.0\n",
      "Step: 25220, train/grad_norm: 0.0044021327048540115\n",
      "Step: 25220, train/learning_rate: 1.9990480723208748e-05\n",
      "Step: 25220, train/epoch: 6.001904010772705\n",
      "Step: 25230, train/loss: 0.0\n",
      "Step: 25230, train/grad_norm: 0.00020196464902255684\n",
      "Step: 25230, train/learning_rate: 1.997858089453075e-05\n",
      "Step: 25230, train/epoch: 6.004283905029297\n",
      "Step: 25240, train/loss: 0.0\n",
      "Step: 25240, train/grad_norm: 0.0015935507835820317\n",
      "Step: 25240, train/learning_rate: 1.9966682884842157e-05\n",
      "Step: 25240, train/epoch: 6.0066633224487305\n",
      "Step: 25250, train/loss: 0.0\n",
      "Step: 25250, train/grad_norm: 0.0005042975535616279\n",
      "Step: 25250, train/learning_rate: 1.995478305616416e-05\n",
      "Step: 25250, train/epoch: 6.009043216705322\n",
      "Step: 25260, train/loss: 0.0\n",
      "Step: 25260, train/grad_norm: 0.005071713123470545\n",
      "Step: 25260, train/learning_rate: 1.9942885046475567e-05\n",
      "Step: 25260, train/epoch: 6.011423110961914\n",
      "Step: 25270, train/loss: 0.0\n",
      "Step: 25270, train/grad_norm: 8.020136738196015e-05\n",
      "Step: 25270, train/learning_rate: 1.993098521779757e-05\n",
      "Step: 25270, train/epoch: 6.013803005218506\n",
      "Step: 25280, train/loss: 0.0\n",
      "Step: 25280, train/grad_norm: 0.000266556249698624\n",
      "Step: 25280, train/learning_rate: 1.9919085389119573e-05\n",
      "Step: 25280, train/epoch: 6.016182899475098\n",
      "Step: 25290, train/loss: 0.0\n",
      "Step: 25290, train/grad_norm: 0.0011654235422611237\n",
      "Step: 25290, train/learning_rate: 1.990718737943098e-05\n",
      "Step: 25290, train/epoch: 6.0185627937316895\n",
      "Step: 25300, train/loss: 0.0\n",
      "Step: 25300, train/grad_norm: 0.00024210315314121544\n",
      "Step: 25300, train/learning_rate: 1.9895287550752982e-05\n",
      "Step: 25300, train/epoch: 6.020942211151123\n",
      "Step: 25310, train/loss: 0.0\n",
      "Step: 25310, train/grad_norm: 0.0009857960976660252\n",
      "Step: 25310, train/learning_rate: 1.988338954106439e-05\n",
      "Step: 25310, train/epoch: 6.023322105407715\n",
      "Step: 25320, train/loss: 0.10779999941587448\n",
      "Step: 25320, train/grad_norm: 0.0021390835754573345\n",
      "Step: 25320, train/learning_rate: 1.9871489712386392e-05\n",
      "Step: 25320, train/epoch: 6.025701999664307\n",
      "Step: 25330, train/loss: 9.999999747378752e-05\n",
      "Step: 25330, train/grad_norm: 0.02669529989361763\n",
      "Step: 25330, train/learning_rate: 1.9859589883708395e-05\n",
      "Step: 25330, train/epoch: 6.028081893920898\n",
      "Step: 25340, train/loss: 0.0\n",
      "Step: 25340, train/grad_norm: 0.0023317423183470964\n",
      "Step: 25340, train/learning_rate: 1.98476918740198e-05\n",
      "Step: 25340, train/epoch: 6.03046178817749\n",
      "Step: 25350, train/loss: 0.0\n",
      "Step: 25350, train/grad_norm: 0.007733358070254326\n",
      "Step: 25350, train/learning_rate: 1.9835792045341805e-05\n",
      "Step: 25350, train/epoch: 6.032841682434082\n",
      "Step: 25360, train/loss: 0.0\n",
      "Step: 25360, train/grad_norm: 0.0006898261490277946\n",
      "Step: 25360, train/learning_rate: 1.982389403565321e-05\n",
      "Step: 25360, train/epoch: 6.035221099853516\n",
      "Step: 25370, train/loss: 0.0\n",
      "Step: 25370, train/grad_norm: 0.00231887586414814\n",
      "Step: 25370, train/learning_rate: 1.9811994206975214e-05\n",
      "Step: 25370, train/epoch: 6.037600994110107\n",
      "Step: 25380, train/loss: 0.0\n",
      "Step: 25380, train/grad_norm: 0.0021495497785508633\n",
      "Step: 25380, train/learning_rate: 1.9800094378297217e-05\n",
      "Step: 25380, train/epoch: 6.039980888366699\n",
      "Step: 25390, train/loss: 0.0\n",
      "Step: 25390, train/grad_norm: 0.0015962539473548532\n",
      "Step: 25390, train/learning_rate: 1.9788196368608624e-05\n",
      "Step: 25390, train/epoch: 6.042360782623291\n",
      "Step: 25400, train/loss: 0.0\n",
      "Step: 25400, train/grad_norm: 0.0010363730834797025\n",
      "Step: 25400, train/learning_rate: 1.9776296539930627e-05\n",
      "Step: 25400, train/epoch: 6.044740676879883\n",
      "Step: 25410, train/loss: 0.0\n",
      "Step: 25410, train/grad_norm: 0.0009883143939077854\n",
      "Step: 25410, train/learning_rate: 1.9764398530242033e-05\n",
      "Step: 25410, train/epoch: 6.047120571136475\n",
      "Step: 25420, train/loss: 0.0\n",
      "Step: 25420, train/grad_norm: 0.0023988746106624603\n",
      "Step: 25420, train/learning_rate: 1.9752498701564036e-05\n",
      "Step: 25420, train/epoch: 6.049500465393066\n",
      "Step: 25430, train/loss: 0.0\n",
      "Step: 25430, train/grad_norm: 0.002498640911653638\n",
      "Step: 25430, train/learning_rate: 1.974059887288604e-05\n",
      "Step: 25430, train/epoch: 6.0518798828125\n",
      "Step: 25440, train/loss: 0.0\n",
      "Step: 25440, train/grad_norm: 0.00047936852206476033\n",
      "Step: 25440, train/learning_rate: 1.9728700863197446e-05\n",
      "Step: 25440, train/epoch: 6.054259777069092\n",
      "Step: 25450, train/loss: 0.0\n",
      "Step: 25450, train/grad_norm: 0.0017203474417328835\n",
      "Step: 25450, train/learning_rate: 1.971680103451945e-05\n",
      "Step: 25450, train/epoch: 6.056639671325684\n",
      "Step: 25460, train/loss: 0.0\n",
      "Step: 25460, train/grad_norm: 0.0003470933297649026\n",
      "Step: 25460, train/learning_rate: 1.9704903024830855e-05\n",
      "Step: 25460, train/epoch: 6.059019565582275\n",
      "Step: 25470, train/loss: 0.0\n",
      "Step: 25470, train/grad_norm: 0.0005271364934742451\n",
      "Step: 25470, train/learning_rate: 1.969300319615286e-05\n",
      "Step: 25470, train/epoch: 6.061399459838867\n",
      "Step: 25480, train/loss: 0.0\n",
      "Step: 25480, train/grad_norm: 0.00033125668414868414\n",
      "Step: 25480, train/learning_rate: 1.968110336747486e-05\n",
      "Step: 25480, train/epoch: 6.063779354095459\n",
      "Step: 25490, train/loss: 0.0\n",
      "Step: 25490, train/grad_norm: 0.0015418804250657558\n",
      "Step: 25490, train/learning_rate: 1.9669205357786268e-05\n",
      "Step: 25490, train/epoch: 6.066158771514893\n",
      "Step: 25500, train/loss: 0.0\n",
      "Step: 25500, train/grad_norm: 0.0006061577005311847\n",
      "Step: 25500, train/learning_rate: 1.965730552910827e-05\n",
      "Step: 25500, train/epoch: 6.068538665771484\n",
      "Step: 25510, train/loss: 0.0\n",
      "Step: 25510, train/grad_norm: 0.0007853300776332617\n",
      "Step: 25510, train/learning_rate: 1.9645407519419678e-05\n",
      "Step: 25510, train/epoch: 6.070918560028076\n",
      "Step: 25520, train/loss: 0.0\n",
      "Step: 25520, train/grad_norm: 0.00028557213954627514\n",
      "Step: 25520, train/learning_rate: 1.963350769074168e-05\n",
      "Step: 25520, train/epoch: 6.073298454284668\n",
      "Step: 25530, train/loss: 0.0\n",
      "Step: 25530, train/grad_norm: 0.0020368394907563925\n",
      "Step: 25530, train/learning_rate: 1.9621607862063684e-05\n",
      "Step: 25530, train/epoch: 6.07567834854126\n",
      "Step: 25540, train/loss: 0.0\n",
      "Step: 25540, train/grad_norm: 0.0003298305964563042\n",
      "Step: 25540, train/learning_rate: 1.960970985237509e-05\n",
      "Step: 25540, train/epoch: 6.078058242797852\n",
      "Step: 25550, train/loss: 0.0\n",
      "Step: 25550, train/grad_norm: 0.00026300782337784767\n",
      "Step: 25550, train/learning_rate: 1.9597810023697093e-05\n",
      "Step: 25550, train/epoch: 6.080437660217285\n",
      "Step: 25560, train/loss: 0.0\n",
      "Step: 25560, train/grad_norm: 0.0015132963890209794\n",
      "Step: 25560, train/learning_rate: 1.95859120140085e-05\n",
      "Step: 25560, train/epoch: 6.082817554473877\n",
      "Step: 25570, train/loss: 0.0\n",
      "Step: 25570, train/grad_norm: 0.00018007372273132205\n",
      "Step: 25570, train/learning_rate: 1.9574012185330503e-05\n",
      "Step: 25570, train/epoch: 6.085197448730469\n",
      "Step: 25580, train/loss: 0.0\n",
      "Step: 25580, train/grad_norm: 0.0002661419566720724\n",
      "Step: 25580, train/learning_rate: 1.956211417564191e-05\n",
      "Step: 25580, train/epoch: 6.0875773429870605\n",
      "Step: 25590, train/loss: 0.0\n",
      "Step: 25590, train/grad_norm: 0.0003100119938608259\n",
      "Step: 25590, train/learning_rate: 1.9550214346963912e-05\n",
      "Step: 25590, train/epoch: 6.089957237243652\n",
      "Step: 25600, train/loss: 0.0\n",
      "Step: 25600, train/grad_norm: 0.00019564834656193852\n",
      "Step: 25600, train/learning_rate: 1.9538314518285915e-05\n",
      "Step: 25600, train/epoch: 6.092337131500244\n",
      "Step: 25610, train/loss: 0.0\n",
      "Step: 25610, train/grad_norm: 0.0010072935838252306\n",
      "Step: 25610, train/learning_rate: 1.9526416508597322e-05\n",
      "Step: 25610, train/epoch: 6.094717025756836\n",
      "Step: 25620, train/loss: 0.0\n",
      "Step: 25620, train/grad_norm: 0.00029520108364522457\n",
      "Step: 25620, train/learning_rate: 1.9514516679919325e-05\n",
      "Step: 25620, train/epoch: 6.0970964431762695\n",
      "Step: 25630, train/loss: 0.0\n",
      "Step: 25630, train/grad_norm: 0.00012032437371090055\n",
      "Step: 25630, train/learning_rate: 1.950261867023073e-05\n",
      "Step: 25630, train/epoch: 6.099476337432861\n",
      "Step: 25640, train/loss: 0.0\n",
      "Step: 25640, train/grad_norm: 0.001404745620675385\n",
      "Step: 25640, train/learning_rate: 1.9490718841552734e-05\n",
      "Step: 25640, train/epoch: 6.101856231689453\n",
      "Step: 25650, train/loss: 0.0\n",
      "Step: 25650, train/grad_norm: 0.00039837765507400036\n",
      "Step: 25650, train/learning_rate: 1.9478819012874737e-05\n",
      "Step: 25650, train/epoch: 6.104236125946045\n",
      "Step: 25660, train/loss: 0.0\n",
      "Step: 25660, train/grad_norm: 0.00022558672935701907\n",
      "Step: 25660, train/learning_rate: 1.9466921003186144e-05\n",
      "Step: 25660, train/epoch: 6.106616020202637\n",
      "Step: 25670, train/loss: 0.0\n",
      "Step: 25670, train/grad_norm: 0.00021956147975288332\n",
      "Step: 25670, train/learning_rate: 1.9455021174508147e-05\n",
      "Step: 25670, train/epoch: 6.1089959144592285\n",
      "Step: 25680, train/loss: 0.0\n",
      "Step: 25680, train/grad_norm: 7.045161328278482e-05\n",
      "Step: 25680, train/learning_rate: 1.9443123164819553e-05\n",
      "Step: 25680, train/epoch: 6.111375331878662\n",
      "Step: 25690, train/loss: 0.0\n",
      "Step: 25690, train/grad_norm: 0.00029058929067105055\n",
      "Step: 25690, train/learning_rate: 1.9431223336141557e-05\n",
      "Step: 25690, train/epoch: 6.113755226135254\n",
      "Step: 25700, train/loss: 0.0\n",
      "Step: 25700, train/grad_norm: 1.9768345737247728e-05\n",
      "Step: 25700, train/learning_rate: 1.941932350746356e-05\n",
      "Step: 25700, train/epoch: 6.116135120391846\n",
      "Step: 25710, train/loss: 0.0\n",
      "Step: 25710, train/grad_norm: 0.00010094133176608011\n",
      "Step: 25710, train/learning_rate: 1.9407425497774966e-05\n",
      "Step: 25710, train/epoch: 6.1185150146484375\n",
      "Step: 25720, train/loss: 0.0\n",
      "Step: 25720, train/grad_norm: 8.28590418677777e-05\n",
      "Step: 25720, train/learning_rate: 1.939552566909697e-05\n",
      "Step: 25720, train/epoch: 6.120894908905029\n",
      "Step: 25730, train/loss: 0.0\n",
      "Step: 25730, train/grad_norm: 2.1589068637695163e-05\n",
      "Step: 25730, train/learning_rate: 1.9383627659408376e-05\n",
      "Step: 25730, train/epoch: 6.123274803161621\n",
      "Step: 25740, train/loss: 0.0\n",
      "Step: 25740, train/grad_norm: 2.317598591616843e-05\n",
      "Step: 25740, train/learning_rate: 1.937172783073038e-05\n",
      "Step: 25740, train/epoch: 6.125654220581055\n",
      "Step: 25750, train/loss: 0.0\n",
      "Step: 25750, train/grad_norm: 0.0013785577611997724\n",
      "Step: 25750, train/learning_rate: 1.935982800205238e-05\n",
      "Step: 25750, train/epoch: 6.1280341148376465\n",
      "Step: 25760, train/loss: 0.0\n",
      "Step: 25760, train/grad_norm: 0.00012322665133979172\n",
      "Step: 25760, train/learning_rate: 1.9347929992363788e-05\n",
      "Step: 25760, train/epoch: 6.130414009094238\n",
      "Step: 25770, train/loss: 0.06560000032186508\n",
      "Step: 25770, train/grad_norm: 389.636474609375\n",
      "Step: 25770, train/learning_rate: 1.933603016368579e-05\n",
      "Step: 25770, train/epoch: 6.13279390335083\n",
      "Step: 25780, train/loss: 0.0\n",
      "Step: 25780, train/grad_norm: 0.0005794531898573041\n",
      "Step: 25780, train/learning_rate: 1.9324132153997198e-05\n",
      "Step: 25780, train/epoch: 6.135173797607422\n",
      "Step: 25790, train/loss: 0.0\n",
      "Step: 25790, train/grad_norm: 0.00018250402354169637\n",
      "Step: 25790, train/learning_rate: 1.93122323253192e-05\n",
      "Step: 25790, train/epoch: 6.137553691864014\n",
      "Step: 25800, train/loss: 0.0\n",
      "Step: 25800, train/grad_norm: 0.0004927539848722517\n",
      "Step: 25800, train/learning_rate: 1.9300332496641204e-05\n",
      "Step: 25800, train/epoch: 6.1399335861206055\n",
      "Step: 25810, train/loss: 0.0\n",
      "Step: 25810, train/grad_norm: 0.0002502004790585488\n",
      "Step: 25810, train/learning_rate: 1.928843448695261e-05\n",
      "Step: 25810, train/epoch: 6.142313003540039\n",
      "Step: 25820, train/loss: 0.0\n",
      "Step: 25820, train/grad_norm: 0.0009264685795642436\n",
      "Step: 25820, train/learning_rate: 1.9276534658274613e-05\n",
      "Step: 25820, train/epoch: 6.144692897796631\n",
      "Step: 25830, train/loss: 0.0\n",
      "Step: 25830, train/grad_norm: 2.7901942303287797e-05\n",
      "Step: 25830, train/learning_rate: 1.926463664858602e-05\n",
      "Step: 25830, train/epoch: 6.147072792053223\n",
      "Step: 25840, train/loss: 0.0\n",
      "Step: 25840, train/grad_norm: 0.00015922037709970027\n",
      "Step: 25840, train/learning_rate: 1.9252736819908023e-05\n",
      "Step: 25840, train/epoch: 6.1494526863098145\n",
      "Step: 25850, train/loss: 0.0\n",
      "Step: 25850, train/grad_norm: 0.000374418479623273\n",
      "Step: 25850, train/learning_rate: 1.9240836991230026e-05\n",
      "Step: 25850, train/epoch: 6.151832580566406\n",
      "Step: 25860, train/loss: 0.0\n",
      "Step: 25860, train/grad_norm: 0.0009793015196919441\n",
      "Step: 25860, train/learning_rate: 1.9228938981541432e-05\n",
      "Step: 25860, train/epoch: 6.154212474822998\n",
      "Step: 25870, train/loss: 0.0\n",
      "Step: 25870, train/grad_norm: 0.0001798749144654721\n",
      "Step: 25870, train/learning_rate: 1.9217039152863435e-05\n",
      "Step: 25870, train/epoch: 6.156591892242432\n",
      "Step: 25880, train/loss: 0.0\n",
      "Step: 25880, train/grad_norm: 8.594847895437852e-05\n",
      "Step: 25880, train/learning_rate: 1.9205141143174842e-05\n",
      "Step: 25880, train/epoch: 6.158971786499023\n",
      "Step: 25890, train/loss: 0.15780000388622284\n",
      "Step: 25890, train/grad_norm: 3.463694520178251e-05\n",
      "Step: 25890, train/learning_rate: 1.9193241314496845e-05\n",
      "Step: 25890, train/epoch: 6.161351680755615\n",
      "Step: 25900, train/loss: 0.0005000000237487257\n",
      "Step: 25900, train/grad_norm: 0.00027206112281419337\n",
      "Step: 25900, train/learning_rate: 1.9181341485818848e-05\n",
      "Step: 25900, train/epoch: 6.163731575012207\n",
      "Step: 25910, train/loss: 0.08980000019073486\n",
      "Step: 25910, train/grad_norm: 0.0002819533983711153\n",
      "Step: 25910, train/learning_rate: 1.9169443476130255e-05\n",
      "Step: 25910, train/epoch: 6.166111469268799\n",
      "Step: 25920, train/loss: 0.21799999475479126\n",
      "Step: 25920, train/grad_norm: 0.003461235435679555\n",
      "Step: 25920, train/learning_rate: 1.9157543647452258e-05\n",
      "Step: 25920, train/epoch: 6.168491363525391\n",
      "Step: 25930, train/loss: 0.08449999988079071\n",
      "Step: 25930, train/grad_norm: 0.06550299376249313\n",
      "Step: 25930, train/learning_rate: 1.9145645637763664e-05\n",
      "Step: 25930, train/epoch: 6.170870780944824\n",
      "Step: 25940, train/loss: 0.12210000306367874\n",
      "Step: 25940, train/grad_norm: 0.5615772604942322\n",
      "Step: 25940, train/learning_rate: 1.9133745809085667e-05\n",
      "Step: 25940, train/epoch: 6.173250675201416\n",
      "Step: 25950, train/loss: 0.007499999832361937\n",
      "Step: 25950, train/grad_norm: 0.006674968637526035\n",
      "Step: 25950, train/learning_rate: 1.912184598040767e-05\n",
      "Step: 25950, train/epoch: 6.175630569458008\n",
      "Step: 25960, train/loss: 0.0\n",
      "Step: 25960, train/grad_norm: 8.439078374067321e-05\n",
      "Step: 25960, train/learning_rate: 1.9109947970719077e-05\n",
      "Step: 25960, train/epoch: 6.1780104637146\n",
      "Step: 25970, train/loss: 0.0\n",
      "Step: 25970, train/grad_norm: 0.00012019690620945767\n",
      "Step: 25970, train/learning_rate: 1.909804814204108e-05\n",
      "Step: 25970, train/epoch: 6.180390357971191\n",
      "Step: 25980, train/loss: 0.0\n",
      "Step: 25980, train/grad_norm: 0.001029017847031355\n",
      "Step: 25980, train/learning_rate: 1.9086150132352486e-05\n",
      "Step: 25980, train/epoch: 6.182770252227783\n",
      "Step: 25990, train/loss: 0.0\n",
      "Step: 25990, train/grad_norm: 9.749180208018515e-06\n",
      "Step: 25990, train/learning_rate: 1.907425030367449e-05\n",
      "Step: 25990, train/epoch: 6.185150146484375\n",
      "Step: 26000, train/loss: 0.0\n",
      "Step: 26000, train/grad_norm: 0.0001457364414818585\n",
      "Step: 26000, train/learning_rate: 1.9062350474996492e-05\n",
      "Step: 26000, train/epoch: 6.187529563903809\n",
      "Step: 26010, train/loss: 0.0\n",
      "Step: 26010, train/grad_norm: 0.00019260904809925705\n",
      "Step: 26010, train/learning_rate: 1.90504524653079e-05\n",
      "Step: 26010, train/epoch: 6.1899094581604\n",
      "Step: 26020, train/loss: 0.0\n",
      "Step: 26020, train/grad_norm: 0.00010728211782407016\n",
      "Step: 26020, train/learning_rate: 1.9038552636629902e-05\n",
      "Step: 26020, train/epoch: 6.192289352416992\n",
      "Step: 26030, train/loss: 0.0\n",
      "Step: 26030, train/grad_norm: 0.0003683597024064511\n",
      "Step: 26030, train/learning_rate: 1.902665462694131e-05\n",
      "Step: 26030, train/epoch: 6.194669246673584\n",
      "Step: 26040, train/loss: 0.0\n",
      "Step: 26040, train/grad_norm: 5.9681573475245386e-05\n",
      "Step: 26040, train/learning_rate: 1.901475479826331e-05\n",
      "Step: 26040, train/epoch: 6.197049140930176\n",
      "Step: 26050, train/loss: 0.0\n",
      "Step: 26050, train/grad_norm: 4.049647759529762e-05\n",
      "Step: 26050, train/learning_rate: 1.9002854969585314e-05\n",
      "Step: 26050, train/epoch: 6.199429035186768\n",
      "Step: 26060, train/loss: 0.0\n",
      "Step: 26060, train/grad_norm: 8.231575338868424e-05\n",
      "Step: 26060, train/learning_rate: 1.899095695989672e-05\n",
      "Step: 26060, train/epoch: 6.201808452606201\n",
      "Step: 26070, train/loss: 0.0\n",
      "Step: 26070, train/grad_norm: 0.00034618782228790224\n",
      "Step: 26070, train/learning_rate: 1.8979057131218724e-05\n",
      "Step: 26070, train/epoch: 6.204188346862793\n",
      "Step: 26080, train/loss: 0.0\n",
      "Step: 26080, train/grad_norm: 6.282007234403864e-05\n",
      "Step: 26080, train/learning_rate: 1.896715912153013e-05\n",
      "Step: 26080, train/epoch: 6.206568241119385\n",
      "Step: 26090, train/loss: 0.0\n",
      "Step: 26090, train/grad_norm: 1.9450378658802947e-06\n",
      "Step: 26090, train/learning_rate: 1.8955259292852134e-05\n",
      "Step: 26090, train/epoch: 6.208948135375977\n",
      "Step: 26100, train/loss: 0.0\n",
      "Step: 26100, train/grad_norm: 0.00015802487905602902\n",
      "Step: 26100, train/learning_rate: 1.8943359464174137e-05\n",
      "Step: 26100, train/epoch: 6.211328029632568\n",
      "Step: 26110, train/loss: 0.008299999870359898\n",
      "Step: 26110, train/grad_norm: 3.439526699366979e-05\n",
      "Step: 26110, train/learning_rate: 1.8931461454485543e-05\n",
      "Step: 26110, train/epoch: 6.21370792388916\n",
      "Step: 26120, train/loss: 0.0\n",
      "Step: 26120, train/grad_norm: 0.0003037206770386547\n",
      "Step: 26120, train/learning_rate: 1.8919561625807546e-05\n",
      "Step: 26120, train/epoch: 6.216087341308594\n",
      "Step: 26130, train/loss: 0.0\n",
      "Step: 26130, train/grad_norm: 3.450529038673267e-05\n",
      "Step: 26130, train/learning_rate: 1.8907663616118953e-05\n",
      "Step: 26130, train/epoch: 6.2184672355651855\n",
      "Step: 26140, train/loss: 0.0\n",
      "Step: 26140, train/grad_norm: 0.0001266648614546284\n",
      "Step: 26140, train/learning_rate: 1.8895763787440956e-05\n",
      "Step: 26140, train/epoch: 6.220847129821777\n",
      "Step: 26150, train/loss: 0.0\n",
      "Step: 26150, train/grad_norm: 6.685911648673937e-05\n",
      "Step: 26150, train/learning_rate: 1.888386395876296e-05\n",
      "Step: 26150, train/epoch: 6.223227024078369\n",
      "Step: 26160, train/loss: 0.0\n",
      "Step: 26160, train/grad_norm: 2.5677760277176276e-05\n",
      "Step: 26160, train/learning_rate: 1.8871965949074365e-05\n",
      "Step: 26160, train/epoch: 6.225606918334961\n",
      "Step: 26170, train/loss: 0.11800000071525574\n",
      "Step: 26170, train/grad_norm: 1.2260931725904811e-05\n",
      "Step: 26170, train/learning_rate: 1.8860066120396368e-05\n",
      "Step: 26170, train/epoch: 6.227986812591553\n",
      "Step: 26180, train/loss: 0.01119999960064888\n",
      "Step: 26180, train/grad_norm: 0.00020953078637830913\n",
      "Step: 26180, train/learning_rate: 1.8848168110707775e-05\n",
      "Step: 26180, train/epoch: 6.2303667068481445\n",
      "Step: 26190, train/loss: 0.020800000056624413\n",
      "Step: 26190, train/grad_norm: 0.00019186193821951747\n",
      "Step: 26190, train/learning_rate: 1.8836268282029778e-05\n",
      "Step: 26190, train/epoch: 6.232746124267578\n",
      "Step: 26200, train/loss: 0.08910000324249268\n",
      "Step: 26200, train/grad_norm: 0.0007904378580860794\n",
      "Step: 26200, train/learning_rate: 1.882436845335178e-05\n",
      "Step: 26200, train/epoch: 6.23512601852417\n",
      "Step: 26210, train/loss: 0.07490000128746033\n",
      "Step: 26210, train/grad_norm: 82.63067626953125\n",
      "Step: 26210, train/learning_rate: 1.8812470443663187e-05\n",
      "Step: 26210, train/epoch: 6.237505912780762\n",
      "Step: 26220, train/loss: 0.0\n",
      "Step: 26220, train/grad_norm: 0.0005784088862128556\n",
      "Step: 26220, train/learning_rate: 1.880057061498519e-05\n",
      "Step: 26220, train/epoch: 6.2398858070373535\n",
      "Step: 26230, train/loss: 0.0\n",
      "Step: 26230, train/grad_norm: 0.0007593167829327285\n",
      "Step: 26230, train/learning_rate: 1.8788672605296597e-05\n",
      "Step: 26230, train/epoch: 6.242265701293945\n",
      "Step: 26240, train/loss: 0.0003000000142492354\n",
      "Step: 26240, train/grad_norm: 0.0012719581136479974\n",
      "Step: 26240, train/learning_rate: 1.87767727766186e-05\n",
      "Step: 26240, train/epoch: 6.244645595550537\n",
      "Step: 26250, train/loss: 0.052299998700618744\n",
      "Step: 26250, train/grad_norm: 0.00046701024984940886\n",
      "Step: 26250, train/learning_rate: 1.8764874766930006e-05\n",
      "Step: 26250, train/epoch: 6.247025012969971\n",
      "Step: 26260, train/loss: 0.0\n",
      "Step: 26260, train/grad_norm: 0.0005197966820560396\n",
      "Step: 26260, train/learning_rate: 1.875297493825201e-05\n",
      "Step: 26260, train/epoch: 6.2494049072265625\n",
      "Step: 26270, train/loss: 0.0\n",
      "Step: 26270, train/grad_norm: 0.00012857570254709572\n",
      "Step: 26270, train/learning_rate: 1.8741075109574012e-05\n",
      "Step: 26270, train/epoch: 6.251784801483154\n",
      "Step: 26280, train/loss: 0.0\n",
      "Step: 26280, train/grad_norm: 0.00027811917243525386\n",
      "Step: 26280, train/learning_rate: 1.872917709988542e-05\n",
      "Step: 26280, train/epoch: 6.254164695739746\n",
      "Step: 26290, train/loss: 0.0\n",
      "Step: 26290, train/grad_norm: 9.065114863915369e-05\n",
      "Step: 26290, train/learning_rate: 1.8717277271207422e-05\n",
      "Step: 26290, train/epoch: 6.256544589996338\n",
      "Step: 26300, train/loss: 9.999999747378752e-05\n",
      "Step: 26300, train/grad_norm: 0.000825712108053267\n",
      "Step: 26300, train/learning_rate: 1.870537926151883e-05\n",
      "Step: 26300, train/epoch: 6.25892448425293\n",
      "Step: 26310, train/loss: 0.0\n",
      "Step: 26310, train/grad_norm: 4.9825754103949293e-05\n",
      "Step: 26310, train/learning_rate: 1.869347943284083e-05\n",
      "Step: 26310, train/epoch: 6.2613043785095215\n",
      "Step: 26320, train/loss: 0.0\n",
      "Step: 26320, train/grad_norm: 6.879538705106825e-05\n",
      "Step: 26320, train/learning_rate: 1.8681579604162835e-05\n",
      "Step: 26320, train/epoch: 6.263683795928955\n",
      "Step: 26330, train/loss: 0.0\n",
      "Step: 26330, train/grad_norm: 2.3551650883746333e-05\n",
      "Step: 26330, train/learning_rate: 1.866968159447424e-05\n",
      "Step: 26330, train/epoch: 6.266063690185547\n",
      "Step: 26340, train/loss: 0.0\n",
      "Step: 26340, train/grad_norm: 9.854538802755997e-05\n",
      "Step: 26340, train/learning_rate: 1.8657781765796244e-05\n",
      "Step: 26340, train/epoch: 6.268443584442139\n",
      "Step: 26350, train/loss: 0.03629999980330467\n",
      "Step: 26350, train/grad_norm: 9.39477322390303e-05\n",
      "Step: 26350, train/learning_rate: 1.864588375610765e-05\n",
      "Step: 26350, train/epoch: 6.2708234786987305\n",
      "Step: 26360, train/loss: 0.0\n",
      "Step: 26360, train/grad_norm: 2.8563083105836995e-05\n",
      "Step: 26360, train/learning_rate: 1.8633983927429654e-05\n",
      "Step: 26360, train/epoch: 6.273203372955322\n",
      "Step: 26370, train/loss: 0.0005000000237487257\n",
      "Step: 26370, train/grad_norm: 12.328265190124512\n",
      "Step: 26370, train/learning_rate: 1.8622084098751657e-05\n",
      "Step: 26370, train/epoch: 6.275583267211914\n",
      "Step: 26380, train/loss: 0.0\n",
      "Step: 26380, train/grad_norm: 3.997232488472946e-05\n",
      "Step: 26380, train/learning_rate: 1.8610186089063063e-05\n",
      "Step: 26380, train/epoch: 6.277962684631348\n",
      "Step: 26390, train/loss: 0.0\n",
      "Step: 26390, train/grad_norm: 9.542487532598898e-06\n",
      "Step: 26390, train/learning_rate: 1.8598286260385066e-05\n",
      "Step: 26390, train/epoch: 6.2803425788879395\n",
      "Step: 26400, train/loss: 0.0\n",
      "Step: 26400, train/grad_norm: 1.9474866348900832e-05\n",
      "Step: 26400, train/learning_rate: 1.8586388250696473e-05\n",
      "Step: 26400, train/epoch: 6.282722473144531\n",
      "Step: 26410, train/loss: 0.0\n",
      "Step: 26410, train/grad_norm: 9.254804353986401e-06\n",
      "Step: 26410, train/learning_rate: 1.8574488422018476e-05\n",
      "Step: 26410, train/epoch: 6.285102367401123\n",
      "Step: 26420, train/loss: 0.0\n",
      "Step: 26420, train/grad_norm: 5.64297351957066e-06\n",
      "Step: 26420, train/learning_rate: 1.856258859334048e-05\n",
      "Step: 26420, train/epoch: 6.287482261657715\n",
      "Step: 26430, train/loss: 0.09650000184774399\n",
      "Step: 26430, train/grad_norm: 462.0657958984375\n",
      "Step: 26430, train/learning_rate: 1.8550690583651885e-05\n",
      "Step: 26430, train/epoch: 6.289862155914307\n",
      "Step: 26440, train/loss: 0.0\n",
      "Step: 26440, train/grad_norm: 1.3865195796824992e-05\n",
      "Step: 26440, train/learning_rate: 1.853879075497389e-05\n",
      "Step: 26440, train/epoch: 6.29224157333374\n",
      "Step: 26450, train/loss: 0.0\n",
      "Step: 26450, train/grad_norm: 2.844977097993251e-05\n",
      "Step: 26450, train/learning_rate: 1.8526892745285295e-05\n",
      "Step: 26450, train/epoch: 6.294621467590332\n",
      "Step: 26460, train/loss: 0.0\n",
      "Step: 26460, train/grad_norm: 5.003244586987421e-05\n",
      "Step: 26460, train/learning_rate: 1.8514992916607298e-05\n",
      "Step: 26460, train/epoch: 6.297001361846924\n",
      "Step: 26470, train/loss: 0.0\n",
      "Step: 26470, train/grad_norm: 4.7254448872990906e-05\n",
      "Step: 26470, train/learning_rate: 1.85030930879293e-05\n",
      "Step: 26470, train/epoch: 6.299381256103516\n",
      "Step: 26480, train/loss: 0.0\n",
      "Step: 26480, train/grad_norm: 1.4224799997464288e-05\n",
      "Step: 26480, train/learning_rate: 1.8491195078240708e-05\n",
      "Step: 26480, train/epoch: 6.301761150360107\n",
      "Step: 26490, train/loss: 0.0\n",
      "Step: 26490, train/grad_norm: 3.8761972973588854e-05\n",
      "Step: 26490, train/learning_rate: 1.847929524956271e-05\n",
      "Step: 26490, train/epoch: 6.304141044616699\n",
      "Step: 26500, train/loss: 0.0\n",
      "Step: 26500, train/grad_norm: 3.6503868159343256e-06\n",
      "Step: 26500, train/learning_rate: 1.8467397239874117e-05\n",
      "Step: 26500, train/epoch: 6.306520938873291\n",
      "Step: 26510, train/loss: 0.0\n",
      "Step: 26510, train/grad_norm: 1.028681708703516e-05\n",
      "Step: 26510, train/learning_rate: 1.845549741119612e-05\n",
      "Step: 26510, train/epoch: 6.308900356292725\n",
      "Step: 26520, train/loss: 0.0\n",
      "Step: 26520, train/grad_norm: 7.456883759004995e-05\n",
      "Step: 26520, train/learning_rate: 1.8443597582518123e-05\n",
      "Step: 26520, train/epoch: 6.311280250549316\n",
      "Step: 26530, train/loss: 0.0\n",
      "Step: 26530, train/grad_norm: 5.746465831180103e-05\n",
      "Step: 26530, train/learning_rate: 1.843169957282953e-05\n",
      "Step: 26530, train/epoch: 6.313660144805908\n",
      "Step: 26540, train/loss: 0.0\n",
      "Step: 26540, train/grad_norm: 9.611052519176155e-06\n",
      "Step: 26540, train/learning_rate: 1.8419799744151533e-05\n",
      "Step: 26540, train/epoch: 6.3160400390625\n",
      "Step: 26550, train/loss: 0.0\n",
      "Step: 26550, train/grad_norm: 1.137187337008072e-05\n",
      "Step: 26550, train/learning_rate: 1.840790173446294e-05\n",
      "Step: 26550, train/epoch: 6.318419933319092\n",
      "Step: 26560, train/loss: 0.0\n",
      "Step: 26560, train/grad_norm: 1.7013202523230575e-05\n",
      "Step: 26560, train/learning_rate: 1.8396001905784942e-05\n",
      "Step: 26560, train/epoch: 6.320799827575684\n",
      "Step: 26570, train/loss: 0.0\n",
      "Step: 26570, train/grad_norm: 8.36026665638201e-05\n",
      "Step: 26570, train/learning_rate: 1.8384102077106945e-05\n",
      "Step: 26570, train/epoch: 6.323179244995117\n",
      "Step: 26580, train/loss: 0.0\n",
      "Step: 26580, train/grad_norm: 3.0255760066211224e-05\n",
      "Step: 26580, train/learning_rate: 1.8372204067418352e-05\n",
      "Step: 26580, train/epoch: 6.325559139251709\n",
      "Step: 26590, train/loss: 0.0\n",
      "Step: 26590, train/grad_norm: 0.00018576595175545663\n",
      "Step: 26590, train/learning_rate: 1.8360304238740355e-05\n",
      "Step: 26590, train/epoch: 6.327939033508301\n",
      "Step: 26600, train/loss: 0.0\n",
      "Step: 26600, train/grad_norm: 1.6141546439030208e-05\n",
      "Step: 26600, train/learning_rate: 1.834840622905176e-05\n",
      "Step: 26600, train/epoch: 6.330318927764893\n",
      "Step: 26610, train/loss: 0.0\n",
      "Step: 26610, train/grad_norm: 1.0407729860162362e-05\n",
      "Step: 26610, train/learning_rate: 1.8336506400373764e-05\n",
      "Step: 26610, train/epoch: 6.332698822021484\n",
      "Step: 26620, train/loss: 0.0\n",
      "Step: 26620, train/grad_norm: 1.462916088712518e-06\n",
      "Step: 26620, train/learning_rate: 1.8324606571695767e-05\n",
      "Step: 26620, train/epoch: 6.335078716278076\n",
      "Step: 26630, train/loss: 0.0\n",
      "Step: 26630, train/grad_norm: 8.88358317752136e-06\n",
      "Step: 26630, train/learning_rate: 1.8312708562007174e-05\n",
      "Step: 26630, train/epoch: 6.33745813369751\n",
      "Step: 26640, train/loss: 0.0\n",
      "Step: 26640, train/grad_norm: 2.9053660455247154e-06\n",
      "Step: 26640, train/learning_rate: 1.8300808733329177e-05\n",
      "Step: 26640, train/epoch: 6.339838027954102\n",
      "Step: 26650, train/loss: 0.0\n",
      "Step: 26650, train/grad_norm: 3.554812428774312e-05\n",
      "Step: 26650, train/learning_rate: 1.8288910723640583e-05\n",
      "Step: 26650, train/epoch: 6.342217922210693\n",
      "Step: 26660, train/loss: 0.0\n",
      "Step: 26660, train/grad_norm: 4.544917828752659e-05\n",
      "Step: 26660, train/learning_rate: 1.8277010894962586e-05\n",
      "Step: 26660, train/epoch: 6.344597816467285\n",
      "Step: 26670, train/loss: 0.0\n",
      "Step: 26670, train/grad_norm: 1.935851832968183e-05\n",
      "Step: 26670, train/learning_rate: 1.826511106628459e-05\n",
      "Step: 26670, train/epoch: 6.346977710723877\n",
      "Step: 26680, train/loss: 0.0\n",
      "Step: 26680, train/grad_norm: 2.0273895643185824e-05\n",
      "Step: 26680, train/learning_rate: 1.8253213056595996e-05\n",
      "Step: 26680, train/epoch: 6.349357604980469\n",
      "Step: 26690, train/loss: 0.0\n",
      "Step: 26690, train/grad_norm: 9.229929105458723e-07\n",
      "Step: 26690, train/learning_rate: 1.8241313227918e-05\n",
      "Step: 26690, train/epoch: 6.3517374992370605\n",
      "Step: 26700, train/loss: 0.0\n",
      "Step: 26700, train/grad_norm: 8.97045811143471e-06\n",
      "Step: 26700, train/learning_rate: 1.8229415218229406e-05\n",
      "Step: 26700, train/epoch: 6.354116916656494\n",
      "Step: 26710, train/loss: 0.0\n",
      "Step: 26710, train/grad_norm: 3.3910779166035354e-05\n",
      "Step: 26710, train/learning_rate: 1.821751538955141e-05\n",
      "Step: 26710, train/epoch: 6.356496810913086\n",
      "Step: 26720, train/loss: 0.0\n",
      "Step: 26720, train/grad_norm: 1.264759248442715e-05\n",
      "Step: 26720, train/learning_rate: 1.820561556087341e-05\n",
      "Step: 26720, train/epoch: 6.358876705169678\n",
      "Step: 26730, train/loss: 0.0\n",
      "Step: 26730, train/grad_norm: 3.2788611292744463e-07\n",
      "Step: 26730, train/learning_rate: 1.8193717551184818e-05\n",
      "Step: 26730, train/epoch: 6.3612565994262695\n",
      "Step: 26740, train/loss: 0.1535000056028366\n",
      "Step: 26740, train/grad_norm: 3.0196239094948396e-05\n",
      "Step: 26740, train/learning_rate: 1.818181772250682e-05\n",
      "Step: 26740, train/epoch: 6.363636493682861\n",
      "Step: 26750, train/loss: 0.0\n",
      "Step: 26750, train/grad_norm: 2.969486013171263e-05\n",
      "Step: 26750, train/learning_rate: 1.8169919712818228e-05\n",
      "Step: 26750, train/epoch: 6.366016387939453\n",
      "Step: 26760, train/loss: 0.0\n",
      "Step: 26760, train/grad_norm: 3.384735100553371e-05\n",
      "Step: 26760, train/learning_rate: 1.815801988414023e-05\n",
      "Step: 26760, train/epoch: 6.368395805358887\n",
      "Step: 26770, train/loss: 0.0\n",
      "Step: 26770, train/grad_norm: 0.000426071957917884\n",
      "Step: 26770, train/learning_rate: 1.8146120055462234e-05\n",
      "Step: 26770, train/epoch: 6.3707756996154785\n",
      "Step: 26780, train/loss: 0.0\n",
      "Step: 26780, train/grad_norm: 1.552618050482124e-05\n",
      "Step: 26780, train/learning_rate: 1.813422204577364e-05\n",
      "Step: 26780, train/epoch: 6.37315559387207\n",
      "Step: 26790, train/loss: 0.0\n",
      "Step: 26790, train/grad_norm: 7.268758054124191e-05\n",
      "Step: 26790, train/learning_rate: 1.8122322217095643e-05\n",
      "Step: 26790, train/epoch: 6.375535488128662\n",
      "Step: 26800, train/loss: 0.0\n",
      "Step: 26800, train/grad_norm: 0.00044099893420934677\n",
      "Step: 26800, train/learning_rate: 1.811042420740705e-05\n",
      "Step: 26800, train/epoch: 6.377915382385254\n",
      "Step: 26810, train/loss: 0.0\n",
      "Step: 26810, train/grad_norm: 1.595075627847109e-05\n",
      "Step: 26810, train/learning_rate: 1.8098524378729053e-05\n",
      "Step: 26810, train/epoch: 6.380295276641846\n",
      "Step: 26820, train/loss: 0.0\n",
      "Step: 26820, train/grad_norm: 3.459647632553242e-05\n",
      "Step: 26820, train/learning_rate: 1.8086624550051056e-05\n",
      "Step: 26820, train/epoch: 6.382674694061279\n",
      "Step: 26830, train/loss: 0.0\n",
      "Step: 26830, train/grad_norm: 1.950189471244812e-05\n",
      "Step: 26830, train/learning_rate: 1.8074726540362462e-05\n",
      "Step: 26830, train/epoch: 6.385054588317871\n",
      "Step: 26840, train/loss: 0.0\n",
      "Step: 26840, train/grad_norm: 6.84192928019911e-05\n",
      "Step: 26840, train/learning_rate: 1.8062826711684465e-05\n",
      "Step: 26840, train/epoch: 6.387434482574463\n",
      "Step: 26850, train/loss: 0.0\n",
      "Step: 26850, train/grad_norm: 0.0003957111912313849\n",
      "Step: 26850, train/learning_rate: 1.8050928701995872e-05\n",
      "Step: 26850, train/epoch: 6.389814376831055\n",
      "Step: 26860, train/loss: 0.0\n",
      "Step: 26860, train/grad_norm: 8.105894085019827e-05\n",
      "Step: 26860, train/learning_rate: 1.8039028873317875e-05\n",
      "Step: 26860, train/epoch: 6.3921942710876465\n",
      "Step: 26870, train/loss: 0.09529999643564224\n",
      "Step: 26870, train/grad_norm: 8.120820712065324e-05\n",
      "Step: 26870, train/learning_rate: 1.8027129044639878e-05\n",
      "Step: 26870, train/epoch: 6.394574165344238\n",
      "Step: 26880, train/loss: 0.0\n",
      "Step: 26880, train/grad_norm: 0.0006246728007681668\n",
      "Step: 26880, train/learning_rate: 1.8015231034951285e-05\n",
      "Step: 26880, train/epoch: 6.39695405960083\n",
      "Step: 26890, train/loss: 0.0\n",
      "Step: 26890, train/grad_norm: 0.003482230007648468\n",
      "Step: 26890, train/learning_rate: 1.8003331206273288e-05\n",
      "Step: 26890, train/epoch: 6.399333477020264\n",
      "Step: 26900, train/loss: 0.0\n",
      "Step: 26900, train/grad_norm: 0.0010953792370855808\n",
      "Step: 26900, train/learning_rate: 1.7991433196584694e-05\n",
      "Step: 26900, train/epoch: 6.4017133712768555\n",
      "Step: 26910, train/loss: 0.0\n",
      "Step: 26910, train/grad_norm: 0.0006959461024962366\n",
      "Step: 26910, train/learning_rate: 1.7979533367906697e-05\n",
      "Step: 26910, train/epoch: 6.404093265533447\n",
      "Step: 26920, train/loss: 0.0\n",
      "Step: 26920, train/grad_norm: 3.596226451918483e-05\n",
      "Step: 26920, train/learning_rate: 1.7967635358218104e-05\n",
      "Step: 26920, train/epoch: 6.406473159790039\n",
      "Step: 26930, train/loss: 0.0\n",
      "Step: 26930, train/grad_norm: 0.0010316872503608465\n",
      "Step: 26930, train/learning_rate: 1.7955735529540107e-05\n",
      "Step: 26930, train/epoch: 6.408853054046631\n",
      "Step: 26940, train/loss: 0.0\n",
      "Step: 26940, train/grad_norm: 0.00045710327685810626\n",
      "Step: 26940, train/learning_rate: 1.794383570086211e-05\n",
      "Step: 26940, train/epoch: 6.411232948303223\n",
      "Step: 26950, train/loss: 0.00019999999494757503\n",
      "Step: 26950, train/grad_norm: 0.00040520698530599475\n",
      "Step: 26950, train/learning_rate: 1.7931937691173516e-05\n",
      "Step: 26950, train/epoch: 6.413612365722656\n",
      "Step: 26960, train/loss: 0.0\n",
      "Step: 26960, train/grad_norm: 1.6480509657412767e-05\n",
      "Step: 26960, train/learning_rate: 1.792003786249552e-05\n",
      "Step: 26960, train/epoch: 6.415992259979248\n",
      "Step: 26970, train/loss: 0.0\n",
      "Step: 26970, train/grad_norm: 5.0494520110078156e-05\n",
      "Step: 26970, train/learning_rate: 1.7908139852806926e-05\n",
      "Step: 26970, train/epoch: 6.41837215423584\n",
      "Step: 26980, train/loss: 0.0\n",
      "Step: 26980, train/grad_norm: 3.7144720408832654e-05\n",
      "Step: 26980, train/learning_rate: 1.789624002412893e-05\n",
      "Step: 26980, train/epoch: 6.420752048492432\n",
      "Step: 26990, train/loss: 0.0\n",
      "Step: 26990, train/grad_norm: 2.1318535800674e-05\n",
      "Step: 26990, train/learning_rate: 1.7884340195450932e-05\n",
      "Step: 26990, train/epoch: 6.423131942749023\n",
      "Step: 27000, train/loss: 0.0\n",
      "Step: 27000, train/grad_norm: 1.793138108041603e-05\n",
      "Step: 27000, train/learning_rate: 1.787244218576234e-05\n",
      "Step: 27000, train/epoch: 6.425511837005615\n",
      "Step: 27010, train/loss: 0.0\n",
      "Step: 27010, train/grad_norm: 0.0006784707657061517\n",
      "Step: 27010, train/learning_rate: 1.786054235708434e-05\n",
      "Step: 27010, train/epoch: 6.427891254425049\n",
      "Step: 27020, train/loss: 0.0\n",
      "Step: 27020, train/grad_norm: 0.00017524475697427988\n",
      "Step: 27020, train/learning_rate: 1.7848644347395748e-05\n",
      "Step: 27020, train/epoch: 6.430271148681641\n",
      "Step: 27030, train/loss: 0.0\n",
      "Step: 27030, train/grad_norm: 5.011756002204493e-05\n",
      "Step: 27030, train/learning_rate: 1.783674451871775e-05\n",
      "Step: 27030, train/epoch: 6.432651042938232\n",
      "Step: 27040, train/loss: 0.0\n",
      "Step: 27040, train/grad_norm: 1.1575879398151301e-05\n",
      "Step: 27040, train/learning_rate: 1.7824844690039754e-05\n",
      "Step: 27040, train/epoch: 6.435030937194824\n",
      "Step: 27050, train/loss: 0.0\n",
      "Step: 27050, train/grad_norm: 0.00012555789726320654\n",
      "Step: 27050, train/learning_rate: 1.781294668035116e-05\n",
      "Step: 27050, train/epoch: 6.437410831451416\n",
      "Step: 27060, train/loss: 0.0\n",
      "Step: 27060, train/grad_norm: 3.9236801967490464e-05\n",
      "Step: 27060, train/learning_rate: 1.7801046851673163e-05\n",
      "Step: 27060, train/epoch: 6.439790725708008\n",
      "Step: 27070, train/loss: 0.0\n",
      "Step: 27070, train/grad_norm: 0.00010876392479985952\n",
      "Step: 27070, train/learning_rate: 1.778914884198457e-05\n",
      "Step: 27070, train/epoch: 6.4421706199646\n",
      "Step: 27080, train/loss: 0.0\n",
      "Step: 27080, train/grad_norm: 5.804022293887101e-05\n",
      "Step: 27080, train/learning_rate: 1.7777249013306573e-05\n",
      "Step: 27080, train/epoch: 6.444550037384033\n",
      "Step: 27090, train/loss: 0.0\n",
      "Step: 27090, train/grad_norm: 2.417500400042627e-05\n",
      "Step: 27090, train/learning_rate: 1.7765349184628576e-05\n",
      "Step: 27090, train/epoch: 6.446929931640625\n",
      "Step: 27100, train/loss: 0.0\n",
      "Step: 27100, train/grad_norm: 3.705921335495077e-05\n",
      "Step: 27100, train/learning_rate: 1.7753451174939983e-05\n",
      "Step: 27100, train/epoch: 6.449309825897217\n",
      "Step: 27110, train/loss: 0.0\n",
      "Step: 27110, train/grad_norm: 3.2790932891657576e-05\n",
      "Step: 27110, train/learning_rate: 1.7741551346261986e-05\n",
      "Step: 27110, train/epoch: 6.451689720153809\n",
      "Step: 27120, train/loss: 0.05550000071525574\n",
      "Step: 27120, train/grad_norm: 72.7154541015625\n",
      "Step: 27120, train/learning_rate: 1.7729653336573392e-05\n",
      "Step: 27120, train/epoch: 6.4540696144104\n",
      "Step: 27130, train/loss: 0.0\n",
      "Step: 27130, train/grad_norm: 5.3412673878483474e-05\n",
      "Step: 27130, train/learning_rate: 1.7717753507895395e-05\n",
      "Step: 27130, train/epoch: 6.456449508666992\n",
      "Step: 27140, train/loss: 0.0\n",
      "Step: 27140, train/grad_norm: 0.010559075511991978\n",
      "Step: 27140, train/learning_rate: 1.7705853679217398e-05\n",
      "Step: 27140, train/epoch: 6.458828926086426\n",
      "Step: 27150, train/loss: 0.0\n",
      "Step: 27150, train/grad_norm: 0.00013208671589381993\n",
      "Step: 27150, train/learning_rate: 1.7693955669528805e-05\n",
      "Step: 27150, train/epoch: 6.461208820343018\n",
      "Step: 27160, train/loss: 0.0\n",
      "Step: 27160, train/grad_norm: 3.768092574318871e-05\n",
      "Step: 27160, train/learning_rate: 1.7682055840850808e-05\n",
      "Step: 27160, train/epoch: 6.463588714599609\n",
      "Step: 27170, train/loss: 0.026599999517202377\n",
      "Step: 27170, train/grad_norm: 0.00010224366997135803\n",
      "Step: 27170, train/learning_rate: 1.7670157831162214e-05\n",
      "Step: 27170, train/epoch: 6.465968608856201\n",
      "Step: 27180, train/loss: 0.0\n",
      "Step: 27180, train/grad_norm: 4.918579361401498e-05\n",
      "Step: 27180, train/learning_rate: 1.7658258002484217e-05\n",
      "Step: 27180, train/epoch: 6.468348503112793\n",
      "Step: 27190, train/loss: 0.0\n",
      "Step: 27190, train/grad_norm: 8.540107046428602e-06\n",
      "Step: 27190, train/learning_rate: 1.764635817380622e-05\n",
      "Step: 27190, train/epoch: 6.470728397369385\n",
      "Step: 27200, train/loss: 0.0\n",
      "Step: 27200, train/grad_norm: 8.016279025468975e-05\n",
      "Step: 27200, train/learning_rate: 1.7634460164117627e-05\n",
      "Step: 27200, train/epoch: 6.473107814788818\n",
      "Step: 27210, train/loss: 0.0\n",
      "Step: 27210, train/grad_norm: 9.339823736809194e-05\n",
      "Step: 27210, train/learning_rate: 1.762256033543963e-05\n",
      "Step: 27210, train/epoch: 6.47548770904541\n",
      "Step: 27220, train/loss: 0.0\n",
      "Step: 27220, train/grad_norm: 3.640380600700155e-05\n",
      "Step: 27220, train/learning_rate: 1.7610662325751036e-05\n",
      "Step: 27220, train/epoch: 6.477867603302002\n",
      "Step: 27230, train/loss: 0.0\n",
      "Step: 27230, train/grad_norm: 0.0028756314422935247\n",
      "Step: 27230, train/learning_rate: 1.759876249707304e-05\n",
      "Step: 27230, train/epoch: 6.480247497558594\n",
      "Step: 27240, train/loss: 0.0\n",
      "Step: 27240, train/grad_norm: 6.143741484265774e-05\n",
      "Step: 27240, train/learning_rate: 1.7586862668395042e-05\n",
      "Step: 27240, train/epoch: 6.4826273918151855\n",
      "Step: 27250, train/loss: 0.0\n",
      "Step: 27250, train/grad_norm: 2.8171667509013787e-05\n",
      "Step: 27250, train/learning_rate: 1.757496465870645e-05\n",
      "Step: 27250, train/epoch: 6.485007286071777\n",
      "Step: 27260, train/loss: 0.0\n",
      "Step: 27260, train/grad_norm: 2.4668461264809594e-05\n",
      "Step: 27260, train/learning_rate: 1.7563064830028452e-05\n",
      "Step: 27260, train/epoch: 6.487387180328369\n",
      "Step: 27270, train/loss: 0.0\n",
      "Step: 27270, train/grad_norm: 0.002134073758497834\n",
      "Step: 27270, train/learning_rate: 1.755116682033986e-05\n",
      "Step: 27270, train/epoch: 6.489766597747803\n",
      "Step: 27280, train/loss: 0.0\n",
      "Step: 27280, train/grad_norm: 0.0001710704091237858\n",
      "Step: 27280, train/learning_rate: 1.753926699166186e-05\n",
      "Step: 27280, train/epoch: 6.4921464920043945\n",
      "Step: 27290, train/loss: 0.0\n",
      "Step: 27290, train/grad_norm: 7.20903481123969e-05\n",
      "Step: 27290, train/learning_rate: 1.7527367162983865e-05\n",
      "Step: 27290, train/epoch: 6.494526386260986\n",
      "Step: 27300, train/loss: 0.0\n",
      "Step: 27300, train/grad_norm: 2.7559648515307344e-05\n",
      "Step: 27300, train/learning_rate: 1.751546915329527e-05\n",
      "Step: 27300, train/epoch: 6.496906280517578\n",
      "Step: 27310, train/loss: 0.025200000032782555\n",
      "Step: 27310, train/grad_norm: 7.383347838185728e-05\n",
      "Step: 27310, train/learning_rate: 1.7503569324617274e-05\n",
      "Step: 27310, train/epoch: 6.49928617477417\n",
      "Step: 27320, train/loss: 0.0\n",
      "Step: 27320, train/grad_norm: 0.00016418032464571297\n",
      "Step: 27320, train/learning_rate: 1.749167131492868e-05\n",
      "Step: 27320, train/epoch: 6.501666069030762\n",
      "Step: 27330, train/loss: 0.0027000000700354576\n",
      "Step: 27330, train/grad_norm: 0.0005018463707529008\n",
      "Step: 27330, train/learning_rate: 1.7479771486250684e-05\n",
      "Step: 27330, train/epoch: 6.504045486450195\n",
      "Step: 27340, train/loss: 0.0013000000035390258\n",
      "Step: 27340, train/grad_norm: 2.7247488105786033e-05\n",
      "Step: 27340, train/learning_rate: 1.7467871657572687e-05\n",
      "Step: 27340, train/epoch: 6.506425380706787\n",
      "Step: 27350, train/loss: 0.0\n",
      "Step: 27350, train/grad_norm: 9.928319195751101e-05\n",
      "Step: 27350, train/learning_rate: 1.7455973647884093e-05\n",
      "Step: 27350, train/epoch: 6.508805274963379\n",
      "Step: 27360, train/loss: 0.0\n",
      "Step: 27360, train/grad_norm: 1.5853127024456626e-06\n",
      "Step: 27360, train/learning_rate: 1.7444073819206096e-05\n",
      "Step: 27360, train/epoch: 6.511185169219971\n",
      "Step: 27370, train/loss: 0.0\n",
      "Step: 27370, train/grad_norm: 6.908224168000743e-05\n",
      "Step: 27370, train/learning_rate: 1.7432175809517503e-05\n",
      "Step: 27370, train/epoch: 6.5135650634765625\n",
      "Step: 27380, train/loss: 0.0\n",
      "Step: 27380, train/grad_norm: 1.2326735486567486e-05\n",
      "Step: 27380, train/learning_rate: 1.7420275980839506e-05\n",
      "Step: 27380, train/epoch: 6.515944957733154\n",
      "Step: 27390, train/loss: 0.0\n",
      "Step: 27390, train/grad_norm: 1.0627489245962352e-05\n",
      "Step: 27390, train/learning_rate: 1.740837615216151e-05\n",
      "Step: 27390, train/epoch: 6.518324375152588\n",
      "Step: 27400, train/loss: 0.0\n",
      "Step: 27400, train/grad_norm: 0.18322858214378357\n",
      "Step: 27400, train/learning_rate: 1.7396478142472915e-05\n",
      "Step: 27400, train/epoch: 6.52070426940918\n",
      "Step: 27410, train/loss: 0.0\n",
      "Step: 27410, train/grad_norm: 1.9641036487882957e-05\n",
      "Step: 27410, train/learning_rate: 1.738457831379492e-05\n",
      "Step: 27410, train/epoch: 6.5230841636657715\n",
      "Step: 27420, train/loss: 0.0\n",
      "Step: 27420, train/grad_norm: 6.473867415479617e-06\n",
      "Step: 27420, train/learning_rate: 1.7372680304106325e-05\n",
      "Step: 27420, train/epoch: 6.525464057922363\n",
      "Step: 27430, train/loss: 0.0\n",
      "Step: 27430, train/grad_norm: 3.475496441751602e-06\n",
      "Step: 27430, train/learning_rate: 1.7360780475428328e-05\n",
      "Step: 27430, train/epoch: 6.527843952178955\n",
      "Step: 27440, train/loss: 0.0\n",
      "Step: 27440, train/grad_norm: 8.350819371116813e-06\n",
      "Step: 27440, train/learning_rate: 1.734888064675033e-05\n",
      "Step: 27440, train/epoch: 6.530223846435547\n",
      "Step: 27450, train/loss: 0.0\n",
      "Step: 27450, train/grad_norm: 0.00011954380897805095\n",
      "Step: 27450, train/learning_rate: 1.7336982637061737e-05\n",
      "Step: 27450, train/epoch: 6.532603740692139\n",
      "Step: 27460, train/loss: 0.0\n",
      "Step: 27460, train/grad_norm: 4.902378350379877e-05\n",
      "Step: 27460, train/learning_rate: 1.732508280838374e-05\n",
      "Step: 27460, train/epoch: 6.534983158111572\n",
      "Step: 27470, train/loss: 0.0\n",
      "Step: 27470, train/grad_norm: 1.3113373825035524e-05\n",
      "Step: 27470, train/learning_rate: 1.7313184798695147e-05\n",
      "Step: 27470, train/epoch: 6.537363052368164\n",
      "Step: 27480, train/loss: 0.0\n",
      "Step: 27480, train/grad_norm: 1.3580486665887292e-05\n",
      "Step: 27480, train/learning_rate: 1.730128497001715e-05\n",
      "Step: 27480, train/epoch: 6.539742946624756\n",
      "Step: 27490, train/loss: 0.0\n",
      "Step: 27490, train/grad_norm: 5.659410817315802e-05\n",
      "Step: 27490, train/learning_rate: 1.7289385141339153e-05\n",
      "Step: 27490, train/epoch: 6.542122840881348\n",
      "Step: 27500, train/loss: 0.0\n",
      "Step: 27500, train/grad_norm: 1.0527039194130339e-05\n",
      "Step: 27500, train/learning_rate: 1.727748713165056e-05\n",
      "Step: 27500, train/epoch: 6.5445027351379395\n",
      "Step: 27510, train/loss: 0.0\n",
      "Step: 27510, train/grad_norm: 2.4580500394222327e-05\n",
      "Step: 27510, train/learning_rate: 1.7265587302972563e-05\n",
      "Step: 27510, train/epoch: 6.546882629394531\n",
      "Step: 27520, train/loss: 0.0\n",
      "Step: 27520, train/grad_norm: 3.823701263172552e-06\n",
      "Step: 27520, train/learning_rate: 1.725368929328397e-05\n",
      "Step: 27520, train/epoch: 6.549262046813965\n",
      "Step: 27530, train/loss: 0.0\n",
      "Step: 27530, train/grad_norm: 2.4756440325290896e-05\n",
      "Step: 27530, train/learning_rate: 1.7241789464605972e-05\n",
      "Step: 27530, train/epoch: 6.551641941070557\n",
      "Step: 27540, train/loss: 0.0\n",
      "Step: 27540, train/grad_norm: 0.0006960754981264472\n",
      "Step: 27540, train/learning_rate: 1.7229889635927975e-05\n",
      "Step: 27540, train/epoch: 6.554021835327148\n",
      "Step: 27550, train/loss: 0.0\n",
      "Step: 27550, train/grad_norm: 1.4926327821740415e-05\n",
      "Step: 27550, train/learning_rate: 1.7217991626239382e-05\n",
      "Step: 27550, train/epoch: 6.55640172958374\n",
      "Step: 27560, train/loss: 0.0\n",
      "Step: 27560, train/grad_norm: 2.9579403417301364e-05\n",
      "Step: 27560, train/learning_rate: 1.7206091797561385e-05\n",
      "Step: 27560, train/epoch: 6.558781623840332\n",
      "Step: 27570, train/loss: 0.0\n",
      "Step: 27570, train/grad_norm: 8.586443072999828e-06\n",
      "Step: 27570, train/learning_rate: 1.719419378787279e-05\n",
      "Step: 27570, train/epoch: 6.561161518096924\n",
      "Step: 27580, train/loss: 0.0\n",
      "Step: 27580, train/grad_norm: 1.8651770687938551e-06\n",
      "Step: 27580, train/learning_rate: 1.7182293959194794e-05\n",
      "Step: 27580, train/epoch: 6.563540935516357\n",
      "Step: 27590, train/loss: 0.0\n",
      "Step: 27590, train/grad_norm: 1.2129423794249306e-06\n",
      "Step: 27590, train/learning_rate: 1.71703959495062e-05\n",
      "Step: 27590, train/epoch: 6.565920829772949\n",
      "Step: 27600, train/loss: 0.0\n",
      "Step: 27600, train/grad_norm: 1.0013749488280155e-05\n",
      "Step: 27600, train/learning_rate: 1.7158496120828204e-05\n",
      "Step: 27600, train/epoch: 6.568300724029541\n",
      "Step: 27610, train/loss: 0.0\n",
      "Step: 27610, train/grad_norm: 1.6339001376763918e-05\n",
      "Step: 27610, train/learning_rate: 1.7146596292150207e-05\n",
      "Step: 27610, train/epoch: 6.570680618286133\n",
      "Step: 27620, train/loss: 0.0\n",
      "Step: 27620, train/grad_norm: 2.0072951883776113e-05\n",
      "Step: 27620, train/learning_rate: 1.7134698282461613e-05\n",
      "Step: 27620, train/epoch: 6.573060512542725\n",
      "Step: 27630, train/loss: 0.0\n",
      "Step: 27630, train/grad_norm: 4.995706331101246e-05\n",
      "Step: 27630, train/learning_rate: 1.7122798453783616e-05\n",
      "Step: 27630, train/epoch: 6.575440406799316\n",
      "Step: 27640, train/loss: 0.0\n",
      "Step: 27640, train/grad_norm: 1.0308258424629457e-05\n",
      "Step: 27640, train/learning_rate: 1.7110900444095023e-05\n",
      "Step: 27640, train/epoch: 6.577820301055908\n",
      "Step: 27650, train/loss: 0.0\n",
      "Step: 27650, train/grad_norm: 4.564686241792515e-05\n",
      "Step: 27650, train/learning_rate: 1.7099000615417026e-05\n",
      "Step: 27650, train/epoch: 6.580199718475342\n",
      "Step: 27660, train/loss: 0.0\n",
      "Step: 27660, train/grad_norm: 3.255674528190866e-05\n",
      "Step: 27660, train/learning_rate: 1.708710078673903e-05\n",
      "Step: 27660, train/epoch: 6.582579612731934\n",
      "Step: 27670, train/loss: 0.0\n",
      "Step: 27670, train/grad_norm: 1.7076232325052842e-05\n",
      "Step: 27670, train/learning_rate: 1.7075202777050436e-05\n",
      "Step: 27670, train/epoch: 6.584959506988525\n",
      "Step: 27680, train/loss: 0.0\n",
      "Step: 27680, train/grad_norm: 3.581962300813757e-05\n",
      "Step: 27680, train/learning_rate: 1.706330294837244e-05\n",
      "Step: 27680, train/epoch: 6.587339401245117\n",
      "Step: 27690, train/loss: 0.0\n",
      "Step: 27690, train/grad_norm: 2.263033366034506e-06\n",
      "Step: 27690, train/learning_rate: 1.7051404938683845e-05\n",
      "Step: 27690, train/epoch: 6.589719295501709\n",
      "Step: 27700, train/loss: 0.0\n",
      "Step: 27700, train/grad_norm: 6.067321578484552e-07\n",
      "Step: 27700, train/learning_rate: 1.7039505110005848e-05\n",
      "Step: 27700, train/epoch: 6.592099189758301\n",
      "Step: 27710, train/loss: 0.0\n",
      "Step: 27710, train/grad_norm: 9.982560413845931e-07\n",
      "Step: 27710, train/learning_rate: 1.702760528132785e-05\n",
      "Step: 27710, train/epoch: 6.594478607177734\n",
      "Step: 27720, train/loss: 0.0\n",
      "Step: 27720, train/grad_norm: 1.0393683623988181e-05\n",
      "Step: 27720, train/learning_rate: 1.7015707271639258e-05\n",
      "Step: 27720, train/epoch: 6.596858501434326\n",
      "Step: 27730, train/loss: 0.0\n",
      "Step: 27730, train/grad_norm: 8.636432085040724e-07\n",
      "Step: 27730, train/learning_rate: 1.700380744296126e-05\n",
      "Step: 27730, train/epoch: 6.599238395690918\n",
      "Step: 27740, train/loss: 0.0\n",
      "Step: 27740, train/grad_norm: 5.212347969063558e-05\n",
      "Step: 27740, train/learning_rate: 1.6991909433272667e-05\n",
      "Step: 27740, train/epoch: 6.60161828994751\n",
      "Step: 27750, train/loss: 0.0\n",
      "Step: 27750, train/grad_norm: 4.861201159656048e-05\n",
      "Step: 27750, train/learning_rate: 1.698000960459467e-05\n",
      "Step: 27750, train/epoch: 6.603998184204102\n",
      "Step: 27760, train/loss: 0.0\n",
      "Step: 27760, train/grad_norm: 9.723303810460493e-05\n",
      "Step: 27760, train/learning_rate: 1.6968109775916673e-05\n",
      "Step: 27760, train/epoch: 6.606378078460693\n",
      "Step: 27770, train/loss: 0.0\n",
      "Step: 27770, train/grad_norm: 2.137759793185978e-06\n",
      "Step: 27770, train/learning_rate: 1.695621176622808e-05\n",
      "Step: 27770, train/epoch: 6.608757972717285\n",
      "Step: 27780, train/loss: 0.0\n",
      "Step: 27780, train/grad_norm: 9.383262522533187e-07\n",
      "Step: 27780, train/learning_rate: 1.6944311937550083e-05\n",
      "Step: 27780, train/epoch: 6.611137390136719\n",
      "Step: 27790, train/loss: 0.0\n",
      "Step: 27790, train/grad_norm: 2.132976533175679e-06\n",
      "Step: 27790, train/learning_rate: 1.693241392786149e-05\n",
      "Step: 27790, train/epoch: 6.6135172843933105\n",
      "Step: 27800, train/loss: 9.999999747378752e-05\n",
      "Step: 27800, train/grad_norm: 1.0486087376193609e-05\n",
      "Step: 27800, train/learning_rate: 1.6920514099183492e-05\n",
      "Step: 27800, train/epoch: 6.615897178649902\n",
      "Step: 27810, train/loss: 0.0\n",
      "Step: 27810, train/grad_norm: 3.370182457729243e-05\n",
      "Step: 27810, train/learning_rate: 1.6908614270505495e-05\n",
      "Step: 27810, train/epoch: 6.618277072906494\n",
      "Step: 27820, train/loss: 0.10000000149011612\n",
      "Step: 27820, train/grad_norm: 1.7672390413281391e-06\n",
      "Step: 27820, train/learning_rate: 1.6896716260816902e-05\n",
      "Step: 27820, train/epoch: 6.620656967163086\n",
      "Step: 27830, train/loss: 0.0\n",
      "Step: 27830, train/grad_norm: 1.5658246411476284e-05\n",
      "Step: 27830, train/learning_rate: 1.6884816432138905e-05\n",
      "Step: 27830, train/epoch: 6.623036861419678\n",
      "Step: 27840, train/loss: 0.0\n",
      "Step: 27840, train/grad_norm: 3.193981319782324e-05\n",
      "Step: 27840, train/learning_rate: 1.687291842245031e-05\n",
      "Step: 27840, train/epoch: 6.625416278839111\n",
      "Step: 27850, train/loss: 0.0\n",
      "Step: 27850, train/grad_norm: 3.1802317153051263e-06\n",
      "Step: 27850, train/learning_rate: 1.6861018593772314e-05\n",
      "Step: 27850, train/epoch: 6.627796173095703\n",
      "Step: 27860, train/loss: 0.0\n",
      "Step: 27860, train/grad_norm: 2.633714393596165e-05\n",
      "Step: 27860, train/learning_rate: 1.6849118765094317e-05\n",
      "Step: 27860, train/epoch: 6.630176067352295\n",
      "Step: 27870, train/loss: 0.0\n",
      "Step: 27870, train/grad_norm: 8.848917332215933e-07\n",
      "Step: 27870, train/learning_rate: 1.6837220755405724e-05\n",
      "Step: 27870, train/epoch: 6.632555961608887\n",
      "Step: 27880, train/loss: 0.0\n",
      "Step: 27880, train/grad_norm: 0.0002033159980783239\n",
      "Step: 27880, train/learning_rate: 1.6825320926727727e-05\n",
      "Step: 27880, train/epoch: 6.6349358558654785\n",
      "Step: 27890, train/loss: 0.0\n",
      "Step: 27890, train/grad_norm: 1.7563703295309097e-05\n",
      "Step: 27890, train/learning_rate: 1.6813422917039134e-05\n",
      "Step: 27890, train/epoch: 6.63731575012207\n",
      "Step: 27900, train/loss: 0.0\n",
      "Step: 27900, train/grad_norm: 9.57237830334634e-07\n",
      "Step: 27900, train/learning_rate: 1.6801523088361137e-05\n",
      "Step: 27900, train/epoch: 6.639695167541504\n",
      "Step: 27910, train/loss: 0.0\n",
      "Step: 27910, train/grad_norm: 1.0955880043184152e-06\n",
      "Step: 27910, train/learning_rate: 1.678962325968314e-05\n",
      "Step: 27910, train/epoch: 6.642075061798096\n",
      "Step: 27920, train/loss: 0.0\n",
      "Step: 27920, train/grad_norm: 9.609483640815597e-06\n",
      "Step: 27920, train/learning_rate: 1.6777725249994546e-05\n",
      "Step: 27920, train/epoch: 6.6444549560546875\n",
      "Step: 27930, train/loss: 0.0\n",
      "Step: 27930, train/grad_norm: 1.7293101336690597e-05\n",
      "Step: 27930, train/learning_rate: 1.676582542131655e-05\n",
      "Step: 27930, train/epoch: 6.646834850311279\n",
      "Step: 27940, train/loss: 0.0\n",
      "Step: 27940, train/grad_norm: 3.915722572855884e-06\n",
      "Step: 27940, train/learning_rate: 1.6753927411627956e-05\n",
      "Step: 27940, train/epoch: 6.649214744567871\n",
      "Step: 27950, train/loss: 0.0\n",
      "Step: 27950, train/grad_norm: 5.339807529480822e-08\n",
      "Step: 27950, train/learning_rate: 1.674202758294996e-05\n",
      "Step: 27950, train/epoch: 6.651594638824463\n",
      "Step: 27960, train/loss: 0.0\n",
      "Step: 27960, train/grad_norm: 6.836064858362079e-05\n",
      "Step: 27960, train/learning_rate: 1.6730127754271962e-05\n",
      "Step: 27960, train/epoch: 6.653974533081055\n",
      "Step: 27970, train/loss: 0.0\n",
      "Step: 27970, train/grad_norm: 1.9959099518018775e-05\n",
      "Step: 27970, train/learning_rate: 1.6718229744583368e-05\n",
      "Step: 27970, train/epoch: 6.656353950500488\n",
      "Step: 27980, train/loss: 0.0\n",
      "Step: 27980, train/grad_norm: 3.5546119647733576e-07\n",
      "Step: 27980, train/learning_rate: 1.670632991590537e-05\n",
      "Step: 27980, train/epoch: 6.65873384475708\n",
      "Step: 27990, train/loss: 0.0\n",
      "Step: 27990, train/grad_norm: 1.1350126669640304e-06\n",
      "Step: 27990, train/learning_rate: 1.6694431906216778e-05\n",
      "Step: 27990, train/epoch: 6.661113739013672\n",
      "Step: 28000, train/loss: 0.0\n",
      "Step: 28000, train/grad_norm: 2.1210253180470318e-06\n",
      "Step: 28000, train/learning_rate: 1.668253207753878e-05\n",
      "Step: 28000, train/epoch: 6.663493633270264\n",
      "Step: 28010, train/loss: 0.1687999963760376\n",
      "Step: 28010, train/grad_norm: 0.00014044571435078979\n",
      "Step: 28010, train/learning_rate: 1.6670632248860784e-05\n",
      "Step: 28010, train/epoch: 6.6658735275268555\n",
      "Step: 28020, train/loss: 0.0\n",
      "Step: 28020, train/grad_norm: 0.0014630770310759544\n",
      "Step: 28020, train/learning_rate: 1.665873423917219e-05\n",
      "Step: 28020, train/epoch: 6.668253421783447\n",
      "Step: 28030, train/loss: 0.0\n",
      "Step: 28030, train/grad_norm: 0.0021263007074594498\n",
      "Step: 28030, train/learning_rate: 1.6646834410494193e-05\n",
      "Step: 28030, train/epoch: 6.670632839202881\n",
      "Step: 28040, train/loss: 0.0\n",
      "Step: 28040, train/grad_norm: 0.00045653912820853293\n",
      "Step: 28040, train/learning_rate: 1.66349364008056e-05\n",
      "Step: 28040, train/epoch: 6.673012733459473\n",
      "Step: 28050, train/loss: 0.0\n",
      "Step: 28050, train/grad_norm: 0.000775629305280745\n",
      "Step: 28050, train/learning_rate: 1.6623036572127603e-05\n",
      "Step: 28050, train/epoch: 6.6753926277160645\n",
      "Step: 28060, train/loss: 0.0\n",
      "Step: 28060, train/grad_norm: 0.0014373455196619034\n",
      "Step: 28060, train/learning_rate: 1.6611136743449606e-05\n",
      "Step: 28060, train/epoch: 6.677772521972656\n",
      "Step: 28070, train/loss: 0.0\n",
      "Step: 28070, train/grad_norm: 0.000596018391661346\n",
      "Step: 28070, train/learning_rate: 1.6599238733761013e-05\n",
      "Step: 28070, train/epoch: 6.680152416229248\n",
      "Step: 28080, train/loss: 0.0\n",
      "Step: 28080, train/grad_norm: 9.053597750607878e-05\n",
      "Step: 28080, train/learning_rate: 1.6587338905083016e-05\n",
      "Step: 28080, train/epoch: 6.68253231048584\n",
      "Step: 28090, train/loss: 0.0\n",
      "Step: 28090, train/grad_norm: 0.0029255400877445936\n",
      "Step: 28090, train/learning_rate: 1.6575440895394422e-05\n",
      "Step: 28090, train/epoch: 6.684911727905273\n",
      "Step: 28100, train/loss: 0.0\n",
      "Step: 28100, train/grad_norm: 0.000827261945232749\n",
      "Step: 28100, train/learning_rate: 1.6563541066716425e-05\n",
      "Step: 28100, train/epoch: 6.687291622161865\n",
      "Step: 28110, train/loss: 0.0\n",
      "Step: 28110, train/grad_norm: 0.0012638549087569118\n",
      "Step: 28110, train/learning_rate: 1.6551641238038428e-05\n",
      "Step: 28110, train/epoch: 6.689671516418457\n",
      "Step: 28120, train/loss: 0.2484000027179718\n",
      "Step: 28120, train/grad_norm: 72.3978500366211\n",
      "Step: 28120, train/learning_rate: 1.6539743228349835e-05\n",
      "Step: 28120, train/epoch: 6.692051410675049\n",
      "Step: 28130, train/loss: 0.00039999998989515007\n",
      "Step: 28130, train/grad_norm: 0.03428199514746666\n",
      "Step: 28130, train/learning_rate: 1.6527843399671838e-05\n",
      "Step: 28130, train/epoch: 6.694431304931641\n",
      "Step: 28140, train/loss: 0.0003000000142492354\n",
      "Step: 28140, train/grad_norm: 0.0190554391592741\n",
      "Step: 28140, train/learning_rate: 1.6515945389983244e-05\n",
      "Step: 28140, train/epoch: 6.696811199188232\n",
      "Step: 28150, train/loss: 0.1445000022649765\n",
      "Step: 28150, train/grad_norm: 2.914720244007185e-05\n",
      "Step: 28150, train/learning_rate: 1.6504045561305247e-05\n",
      "Step: 28150, train/epoch: 6.699191093444824\n",
      "Step: 28160, train/loss: 0.0\n",
      "Step: 28160, train/grad_norm: 0.007708822842687368\n",
      "Step: 28160, train/learning_rate: 1.649214573262725e-05\n",
      "Step: 28160, train/epoch: 6.701570510864258\n",
      "Step: 28170, train/loss: 0.0\n",
      "Step: 28170, train/grad_norm: 0.008630864322185516\n",
      "Step: 28170, train/learning_rate: 1.6480247722938657e-05\n",
      "Step: 28170, train/epoch: 6.70395040512085\n",
      "Step: 28180, train/loss: 0.0\n",
      "Step: 28180, train/grad_norm: 0.004320299252867699\n",
      "Step: 28180, train/learning_rate: 1.646834789426066e-05\n",
      "Step: 28180, train/epoch: 6.706330299377441\n",
      "Step: 28190, train/loss: 0.08399999886751175\n",
      "Step: 28190, train/grad_norm: 0.013196165673434734\n",
      "Step: 28190, train/learning_rate: 1.6456449884572066e-05\n",
      "Step: 28190, train/epoch: 6.708710193634033\n",
      "Step: 28200, train/loss: 0.0\n",
      "Step: 28200, train/grad_norm: 7.227634341688827e-05\n",
      "Step: 28200, train/learning_rate: 1.644455005589407e-05\n",
      "Step: 28200, train/epoch: 6.711090087890625\n",
      "Step: 28210, train/loss: 0.0\n",
      "Step: 28210, train/grad_norm: 0.03282683342695236\n",
      "Step: 28210, train/learning_rate: 1.6432650227216072e-05\n",
      "Step: 28210, train/epoch: 6.713469982147217\n",
      "Step: 28220, train/loss: 0.0\n",
      "Step: 28220, train/grad_norm: 0.012991220690310001\n",
      "Step: 28220, train/learning_rate: 1.642075221752748e-05\n",
      "Step: 28220, train/epoch: 6.71584939956665\n",
      "Step: 28230, train/loss: 0.0\n",
      "Step: 28230, train/grad_norm: 0.0016980778891593218\n",
      "Step: 28230, train/learning_rate: 1.6408852388849482e-05\n",
      "Step: 28230, train/epoch: 6.718229293823242\n",
      "Step: 28240, train/loss: 0.13699999451637268\n",
      "Step: 28240, train/grad_norm: 0.17263740301132202\n",
      "Step: 28240, train/learning_rate: 1.639695437916089e-05\n",
      "Step: 28240, train/epoch: 6.720609188079834\n",
      "Step: 28250, train/loss: 9.999999747378752e-05\n",
      "Step: 28250, train/grad_norm: 0.00759861059486866\n",
      "Step: 28250, train/learning_rate: 1.638505455048289e-05\n",
      "Step: 28250, train/epoch: 6.722989082336426\n",
      "Step: 28260, train/loss: 9.999999747378752e-05\n",
      "Step: 28260, train/grad_norm: 0.005833352915942669\n",
      "Step: 28260, train/learning_rate: 1.6373156540794298e-05\n",
      "Step: 28260, train/epoch: 6.725368976593018\n",
      "Step: 28270, train/loss: 0.0\n",
      "Step: 28270, train/grad_norm: 0.0015102995093911886\n",
      "Step: 28270, train/learning_rate: 1.63612567121163e-05\n",
      "Step: 28270, train/epoch: 6.727748870849609\n",
      "Step: 28280, train/loss: 0.0\n",
      "Step: 28280, train/grad_norm: 8.911400072975084e-05\n",
      "Step: 28280, train/learning_rate: 1.6349356883438304e-05\n",
      "Step: 28280, train/epoch: 6.730128288269043\n",
      "Step: 28290, train/loss: 0.0\n",
      "Step: 28290, train/grad_norm: 3.1488070817431435e-05\n",
      "Step: 28290, train/learning_rate: 1.633745887374971e-05\n",
      "Step: 28290, train/epoch: 6.732508182525635\n",
      "Step: 28300, train/loss: 0.0\n",
      "Step: 28300, train/grad_norm: 7.902042125351727e-05\n",
      "Step: 28300, train/learning_rate: 1.6325559045071714e-05\n",
      "Step: 28300, train/epoch: 6.734888076782227\n",
      "Step: 28310, train/loss: 0.0\n",
      "Step: 28310, train/grad_norm: 3.40547485393472e-05\n",
      "Step: 28310, train/learning_rate: 1.631366103538312e-05\n",
      "Step: 28310, train/epoch: 6.737267971038818\n",
      "Step: 28320, train/loss: 0.0\n",
      "Step: 28320, train/grad_norm: 0.0003013872483279556\n",
      "Step: 28320, train/learning_rate: 1.6301761206705123e-05\n",
      "Step: 28320, train/epoch: 6.73964786529541\n",
      "Step: 28330, train/loss: 0.0\n",
      "Step: 28330, train/grad_norm: 8.562442235415801e-05\n",
      "Step: 28330, train/learning_rate: 1.6289861378027126e-05\n",
      "Step: 28330, train/epoch: 6.742027759552002\n",
      "Step: 28340, train/loss: 0.0\n",
      "Step: 28340, train/grad_norm: 9.218029299518093e-05\n",
      "Step: 28340, train/learning_rate: 1.6277963368338533e-05\n",
      "Step: 28340, train/epoch: 6.744407653808594\n",
      "Step: 28350, train/loss: 0.0\n",
      "Step: 28350, train/grad_norm: 0.0006432154914364219\n",
      "Step: 28350, train/learning_rate: 1.6266063539660536e-05\n",
      "Step: 28350, train/epoch: 6.746787071228027\n",
      "Step: 28360, train/loss: 0.0\n",
      "Step: 28360, train/grad_norm: 2.720806332945358e-05\n",
      "Step: 28360, train/learning_rate: 1.6254165529971942e-05\n",
      "Step: 28360, train/epoch: 6.749166965484619\n",
      "Step: 28370, train/loss: 0.0\n",
      "Step: 28370, train/grad_norm: 4.4240168790565804e-05\n",
      "Step: 28370, train/learning_rate: 1.6242265701293945e-05\n",
      "Step: 28370, train/epoch: 6.751546859741211\n",
      "Step: 28380, train/loss: 0.0\n",
      "Step: 28380, train/grad_norm: 0.0003578256582841277\n",
      "Step: 28380, train/learning_rate: 1.623036587261595e-05\n",
      "Step: 28380, train/epoch: 6.753926753997803\n",
      "Step: 28390, train/loss: 0.0\n",
      "Step: 28390, train/grad_norm: 0.001347231213003397\n",
      "Step: 28390, train/learning_rate: 1.6218467862927355e-05\n",
      "Step: 28390, train/epoch: 6.7563066482543945\n",
      "Step: 28400, train/loss: 0.0\n",
      "Step: 28400, train/grad_norm: 1.2132019946875516e-05\n",
      "Step: 28400, train/learning_rate: 1.6206568034249358e-05\n",
      "Step: 28400, train/epoch: 6.758686542510986\n",
      "Step: 28410, train/loss: 0.0\n",
      "Step: 28410, train/grad_norm: 7.242157153086737e-06\n",
      "Step: 28410, train/learning_rate: 1.6194670024560764e-05\n",
      "Step: 28410, train/epoch: 6.76106595993042\n",
      "Step: 28420, train/loss: 0.0\n",
      "Step: 28420, train/grad_norm: 0.00031777223921380937\n",
      "Step: 28420, train/learning_rate: 1.6182770195882767e-05\n",
      "Step: 28420, train/epoch: 6.763445854187012\n",
      "Step: 28430, train/loss: 0.0\n",
      "Step: 28430, train/grad_norm: 8.570620593673084e-06\n",
      "Step: 28430, train/learning_rate: 1.617087036720477e-05\n",
      "Step: 28430, train/epoch: 6.7658257484436035\n",
      "Step: 28440, train/loss: 0.0\n",
      "Step: 28440, train/grad_norm: 0.0001958838547579944\n",
      "Step: 28440, train/learning_rate: 1.6158972357516177e-05\n",
      "Step: 28440, train/epoch: 6.768205642700195\n",
      "Step: 28450, train/loss: 0.0\n",
      "Step: 28450, train/grad_norm: 0.00024673796724528074\n",
      "Step: 28450, train/learning_rate: 1.614707252883818e-05\n",
      "Step: 28450, train/epoch: 6.770585536956787\n",
      "Step: 28460, train/loss: 0.0\n",
      "Step: 28460, train/grad_norm: 0.00015022026491351426\n",
      "Step: 28460, train/learning_rate: 1.6135174519149587e-05\n",
      "Step: 28460, train/epoch: 6.772965431213379\n",
      "Step: 28470, train/loss: 0.0\n",
      "Step: 28470, train/grad_norm: 0.0002679122262634337\n",
      "Step: 28470, train/learning_rate: 1.612327469047159e-05\n",
      "Step: 28470, train/epoch: 6.7753448486328125\n",
      "Step: 28480, train/loss: 0.0\n",
      "Step: 28480, train/grad_norm: 3.120749852314475e-06\n",
      "Step: 28480, train/learning_rate: 1.6111374861793593e-05\n",
      "Step: 28480, train/epoch: 6.777724742889404\n",
      "Step: 28490, train/loss: 0.0\n",
      "Step: 28490, train/grad_norm: 4.4443488150136545e-05\n",
      "Step: 28490, train/learning_rate: 1.6099476852105e-05\n",
      "Step: 28490, train/epoch: 6.780104637145996\n",
      "Step: 28500, train/loss: 0.0\n",
      "Step: 28500, train/grad_norm: 5.536177195608616e-05\n",
      "Step: 28500, train/learning_rate: 1.6087577023427002e-05\n",
      "Step: 28500, train/epoch: 6.782484531402588\n",
      "Step: 28510, train/loss: 0.0\n",
      "Step: 28510, train/grad_norm: 0.0006197746843099594\n",
      "Step: 28510, train/learning_rate: 1.607567901373841e-05\n",
      "Step: 28510, train/epoch: 6.78486442565918\n",
      "Step: 28520, train/loss: 0.0\n",
      "Step: 28520, train/grad_norm: 1.114225324272411e-05\n",
      "Step: 28520, train/learning_rate: 1.606377918506041e-05\n",
      "Step: 28520, train/epoch: 6.7872443199157715\n",
      "Step: 28530, train/loss: 0.0\n",
      "Step: 28530, train/grad_norm: 7.306160114239901e-05\n",
      "Step: 28530, train/learning_rate: 1.6051879356382415e-05\n",
      "Step: 28530, train/epoch: 6.789624214172363\n",
      "Step: 28540, train/loss: 0.0\n",
      "Step: 28540, train/grad_norm: 0.000139232914079912\n",
      "Step: 28540, train/learning_rate: 1.603998134669382e-05\n",
      "Step: 28540, train/epoch: 6.792003631591797\n",
      "Step: 28550, train/loss: 0.0\n",
      "Step: 28550, train/grad_norm: 5.769175186287612e-05\n",
      "Step: 28550, train/learning_rate: 1.6028081518015824e-05\n",
      "Step: 28550, train/epoch: 6.794383525848389\n",
      "Step: 28560, train/loss: 0.0\n",
      "Step: 28560, train/grad_norm: 0.00011228186485823244\n",
      "Step: 28560, train/learning_rate: 1.601618350832723e-05\n",
      "Step: 28560, train/epoch: 6.7967634201049805\n",
      "Step: 28570, train/loss: 0.0\n",
      "Step: 28570, train/grad_norm: 4.640412498702062e-06\n",
      "Step: 28570, train/learning_rate: 1.6004283679649234e-05\n",
      "Step: 28570, train/epoch: 6.799143314361572\n",
      "Step: 28580, train/loss: 0.0\n",
      "Step: 28580, train/grad_norm: 0.00018551545508671552\n",
      "Step: 28580, train/learning_rate: 1.5992383850971237e-05\n",
      "Step: 28580, train/epoch: 6.801523208618164\n",
      "Step: 28590, train/loss: 0.0\n",
      "Step: 28590, train/grad_norm: 0.0030023425351828337\n",
      "Step: 28590, train/learning_rate: 1.5980485841282643e-05\n",
      "Step: 28590, train/epoch: 6.803903102874756\n",
      "Step: 28600, train/loss: 0.0\n",
      "Step: 28600, train/grad_norm: 8.753957081353292e-05\n",
      "Step: 28600, train/learning_rate: 1.5968586012604646e-05\n",
      "Step: 28600, train/epoch: 6.8062825202941895\n",
      "Step: 28610, train/loss: 0.0\n",
      "Step: 28610, train/grad_norm: 0.000507128017488867\n",
      "Step: 28610, train/learning_rate: 1.5956688002916053e-05\n",
      "Step: 28610, train/epoch: 6.808662414550781\n",
      "Step: 28620, train/loss: 0.0\n",
      "Step: 28620, train/grad_norm: 9.859038982540369e-05\n",
      "Step: 28620, train/learning_rate: 1.5944788174238056e-05\n",
      "Step: 28620, train/epoch: 6.811042308807373\n",
      "Step: 28630, train/loss: 0.0\n",
      "Step: 28630, train/grad_norm: 9.957129077520221e-05\n",
      "Step: 28630, train/learning_rate: 1.593288834556006e-05\n",
      "Step: 28630, train/epoch: 6.813422203063965\n",
      "Step: 28640, train/loss: 0.07190000265836716\n",
      "Step: 28640, train/grad_norm: 1.5144547660383978e-06\n",
      "Step: 28640, train/learning_rate: 1.5920990335871466e-05\n",
      "Step: 28640, train/epoch: 6.815802097320557\n",
      "Step: 28650, train/loss: 0.0\n",
      "Step: 28650, train/grad_norm: 0.00017089713946916163\n",
      "Step: 28650, train/learning_rate: 1.590909050719347e-05\n",
      "Step: 28650, train/epoch: 6.818181991577148\n",
      "Step: 28660, train/loss: 0.0\n",
      "Step: 28660, train/grad_norm: 2.3298043743125163e-05\n",
      "Step: 28660, train/learning_rate: 1.5897192497504875e-05\n",
      "Step: 28660, train/epoch: 6.820561408996582\n",
      "Step: 28670, train/loss: 0.00039999998989515007\n",
      "Step: 28670, train/grad_norm: 9.671073913574219\n",
      "Step: 28670, train/learning_rate: 1.5885292668826878e-05\n",
      "Step: 28670, train/epoch: 6.822941303253174\n",
      "Step: 28680, train/loss: 0.0\n",
      "Step: 28680, train/grad_norm: 0.00025502368225716054\n",
      "Step: 28680, train/learning_rate: 1.587339284014888e-05\n",
      "Step: 28680, train/epoch: 6.825321197509766\n",
      "Step: 28690, train/loss: 0.0\n",
      "Step: 28690, train/grad_norm: 5.7915640354622155e-05\n",
      "Step: 28690, train/learning_rate: 1.5861494830460288e-05\n",
      "Step: 28690, train/epoch: 6.827701091766357\n",
      "Step: 28700, train/loss: 0.0\n",
      "Step: 28700, train/grad_norm: 1.015228554024361e-05\n",
      "Step: 28700, train/learning_rate: 1.584959500178229e-05\n",
      "Step: 28700, train/epoch: 6.830080986022949\n",
      "Step: 28710, train/loss: 0.0\n",
      "Step: 28710, train/grad_norm: 4.266901305527426e-05\n",
      "Step: 28710, train/learning_rate: 1.5837696992093697e-05\n",
      "Step: 28710, train/epoch: 6.832460880279541\n",
      "Step: 28720, train/loss: 0.0\n",
      "Step: 28720, train/grad_norm: 4.9708025471773e-05\n",
      "Step: 28720, train/learning_rate: 1.58257971634157e-05\n",
      "Step: 28720, train/epoch: 6.834840774536133\n",
      "Step: 28730, train/loss: 0.0\n",
      "Step: 28730, train/grad_norm: 0.001515859505161643\n",
      "Step: 28730, train/learning_rate: 1.5813897334737703e-05\n",
      "Step: 28730, train/epoch: 6.837220191955566\n",
      "Step: 28740, train/loss: 0.0\n",
      "Step: 28740, train/grad_norm: 2.3998189135454595e-05\n",
      "Step: 28740, train/learning_rate: 1.580199932504911e-05\n",
      "Step: 28740, train/epoch: 6.839600086212158\n",
      "Step: 28750, train/loss: 0.0\n",
      "Step: 28750, train/grad_norm: 2.13543426070828e-05\n",
      "Step: 28750, train/learning_rate: 1.5790099496371113e-05\n",
      "Step: 28750, train/epoch: 6.84197998046875\n",
      "Step: 28760, train/loss: 0.0\n",
      "Step: 28760, train/grad_norm: 2.699077640500036e-06\n",
      "Step: 28760, train/learning_rate: 1.577820148668252e-05\n",
      "Step: 28760, train/epoch: 6.844359874725342\n",
      "Step: 28770, train/loss: 0.0\n",
      "Step: 28770, train/grad_norm: 1.9603927285061218e-05\n",
      "Step: 28770, train/learning_rate: 1.5766301658004522e-05\n",
      "Step: 28770, train/epoch: 6.846739768981934\n",
      "Step: 28780, train/loss: 0.0\n",
      "Step: 28780, train/grad_norm: 2.792557825159747e-05\n",
      "Step: 28780, train/learning_rate: 1.5754401829326525e-05\n",
      "Step: 28780, train/epoch: 6.849119663238525\n",
      "Step: 28790, train/loss: 0.0\n",
      "Step: 28790, train/grad_norm: 0.00010681376443244517\n",
      "Step: 28790, train/learning_rate: 1.5742503819637932e-05\n",
      "Step: 28790, train/epoch: 6.851499080657959\n",
      "Step: 28800, train/loss: 0.0\n",
      "Step: 28800, train/grad_norm: 3.761172411032021e-05\n",
      "Step: 28800, train/learning_rate: 1.5730603990959935e-05\n",
      "Step: 28800, train/epoch: 6.853878974914551\n",
      "Step: 28810, train/loss: 0.0\n",
      "Step: 28810, train/grad_norm: 0.0003560338809620589\n",
      "Step: 28810, train/learning_rate: 1.571870598127134e-05\n",
      "Step: 28810, train/epoch: 6.856258869171143\n",
      "Step: 28820, train/loss: 0.0\n",
      "Step: 28820, train/grad_norm: 6.527169171022251e-05\n",
      "Step: 28820, train/learning_rate: 1.5706806152593344e-05\n",
      "Step: 28820, train/epoch: 6.858638763427734\n",
      "Step: 28830, train/loss: 0.0\n",
      "Step: 28830, train/grad_norm: 0.00014852796448394656\n",
      "Step: 28830, train/learning_rate: 1.5694906323915347e-05\n",
      "Step: 28830, train/epoch: 6.861018657684326\n",
      "Step: 28840, train/loss: 0.0\n",
      "Step: 28840, train/grad_norm: 1.4073086276766844e-05\n",
      "Step: 28840, train/learning_rate: 1.5683008314226754e-05\n",
      "Step: 28840, train/epoch: 6.863398551940918\n",
      "Step: 28850, train/loss: 0.0\n",
      "Step: 28850, train/grad_norm: 3.801899583777413e-05\n",
      "Step: 28850, train/learning_rate: 1.5671108485548757e-05\n",
      "Step: 28850, train/epoch: 6.865777969360352\n",
      "Step: 28860, train/loss: 0.0\n",
      "Step: 28860, train/grad_norm: 9.535923709336203e-06\n",
      "Step: 28860, train/learning_rate: 1.5659210475860164e-05\n",
      "Step: 28860, train/epoch: 6.868157863616943\n",
      "Step: 28870, train/loss: 0.10859999805688858\n",
      "Step: 28870, train/grad_norm: 0.00037623406387865543\n",
      "Step: 28870, train/learning_rate: 1.5647310647182167e-05\n",
      "Step: 28870, train/epoch: 6.870537757873535\n",
      "Step: 28880, train/loss: 0.0\n",
      "Step: 28880, train/grad_norm: 0.000993297784589231\n",
      "Step: 28880, train/learning_rate: 1.563541081850417e-05\n",
      "Step: 28880, train/epoch: 6.872917652130127\n",
      "Step: 28890, train/loss: 0.2078000009059906\n",
      "Step: 28890, train/grad_norm: 0.020649923011660576\n",
      "Step: 28890, train/learning_rate: 1.5623512808815576e-05\n",
      "Step: 28890, train/epoch: 6.875297546386719\n",
      "Step: 28900, train/loss: 0.03020000085234642\n",
      "Step: 28900, train/grad_norm: 0.029418906196951866\n",
      "Step: 28900, train/learning_rate: 1.561161298013758e-05\n",
      "Step: 28900, train/epoch: 6.8776774406433105\n",
      "Step: 28910, train/loss: 0.000699999975040555\n",
      "Step: 28910, train/grad_norm: 0.02191201224923134\n",
      "Step: 28910, train/learning_rate: 1.5599714970448986e-05\n",
      "Step: 28910, train/epoch: 6.880057334899902\n",
      "Step: 28920, train/loss: 0.0\n",
      "Step: 28920, train/grad_norm: 0.004925941117107868\n",
      "Step: 28920, train/learning_rate: 1.558781514177099e-05\n",
      "Step: 28920, train/epoch: 6.882436752319336\n",
      "Step: 28930, train/loss: 0.0\n",
      "Step: 28930, train/grad_norm: 0.0019406419014558196\n",
      "Step: 28930, train/learning_rate: 1.5575917132082395e-05\n",
      "Step: 28930, train/epoch: 6.884816646575928\n",
      "Step: 28940, train/loss: 0.0\n",
      "Step: 28940, train/grad_norm: 0.002244290430098772\n",
      "Step: 28940, train/learning_rate: 1.5564017303404398e-05\n",
      "Step: 28940, train/epoch: 6.8871965408325195\n",
      "Step: 28950, train/loss: 0.0\n",
      "Step: 28950, train/grad_norm: 0.001798337558284402\n",
      "Step: 28950, train/learning_rate: 1.55521174747264e-05\n",
      "Step: 28950, train/epoch: 6.889576435089111\n",
      "Step: 28960, train/loss: 0.0\n",
      "Step: 28960, train/grad_norm: 0.011372916400432587\n",
      "Step: 28960, train/learning_rate: 1.5540219465037808e-05\n",
      "Step: 28960, train/epoch: 6.891956329345703\n",
      "Step: 28970, train/loss: 0.0\n",
      "Step: 28970, train/grad_norm: 0.0008346219547092915\n",
      "Step: 28970, train/learning_rate: 1.552831963635981e-05\n",
      "Step: 28970, train/epoch: 6.894336223602295\n",
      "Step: 28980, train/loss: 0.0\n",
      "Step: 28980, train/grad_norm: 0.0026146401651203632\n",
      "Step: 28980, train/learning_rate: 1.5516421626671217e-05\n",
      "Step: 28980, train/epoch: 6.8967156410217285\n",
      "Step: 28990, train/loss: 0.0\n",
      "Step: 28990, train/grad_norm: 0.0017221445450559258\n",
      "Step: 28990, train/learning_rate: 1.550452179799322e-05\n",
      "Step: 28990, train/epoch: 6.89909553527832\n",
      "Step: 29000, train/loss: 0.0\n",
      "Step: 29000, train/grad_norm: 0.0009626329410821199\n",
      "Step: 29000, train/learning_rate: 1.5492621969315223e-05\n",
      "Step: 29000, train/epoch: 6.901475429534912\n",
      "Step: 29010, train/loss: 9.999999747378752e-05\n",
      "Step: 29010, train/grad_norm: 0.0007898386684246361\n",
      "Step: 29010, train/learning_rate: 1.548072395962663e-05\n",
      "Step: 29010, train/epoch: 6.903855323791504\n",
      "Step: 29020, train/loss: 0.09059999883174896\n",
      "Step: 29020, train/grad_norm: 0.0003819985140580684\n",
      "Step: 29020, train/learning_rate: 1.5468824130948633e-05\n",
      "Step: 29020, train/epoch: 6.906235218048096\n",
      "Step: 29030, train/loss: 0.0\n",
      "Step: 29030, train/grad_norm: 0.0005477308877743781\n",
      "Step: 29030, train/learning_rate: 1.545692612126004e-05\n",
      "Step: 29030, train/epoch: 6.9086151123046875\n",
      "Step: 29040, train/loss: 0.21559999883174896\n",
      "Step: 29040, train/grad_norm: 0.008331130258738995\n",
      "Step: 29040, train/learning_rate: 1.5445026292582043e-05\n",
      "Step: 29040, train/epoch: 6.910994529724121\n",
      "Step: 29050, train/loss: 0.0\n",
      "Step: 29050, train/grad_norm: 0.014681047759950161\n",
      "Step: 29050, train/learning_rate: 1.5433126463904046e-05\n",
      "Step: 29050, train/epoch: 6.913374423980713\n",
      "Step: 29060, train/loss: 0.0\n",
      "Step: 29060, train/grad_norm: 0.008339012041687965\n",
      "Step: 29060, train/learning_rate: 1.5421228454215452e-05\n",
      "Step: 29060, train/epoch: 6.915754318237305\n",
      "Step: 29070, train/loss: 0.0\n",
      "Step: 29070, train/grad_norm: 0.01059021521359682\n",
      "Step: 29070, train/learning_rate: 1.5409328625537455e-05\n",
      "Step: 29070, train/epoch: 6.9181342124938965\n",
      "Step: 29080, train/loss: 0.0\n",
      "Step: 29080, train/grad_norm: 0.000696392438840121\n",
      "Step: 29080, train/learning_rate: 1.539743061584886e-05\n",
      "Step: 29080, train/epoch: 6.920514106750488\n",
      "Step: 29090, train/loss: 0.0\n",
      "Step: 29090, train/grad_norm: 0.002009514719247818\n",
      "Step: 29090, train/learning_rate: 1.5385530787170865e-05\n",
      "Step: 29090, train/epoch: 6.92289400100708\n",
      "Step: 29100, train/loss: 0.0\n",
      "Step: 29100, train/grad_norm: 0.002238515065982938\n",
      "Step: 29100, train/learning_rate: 1.5373630958492868e-05\n",
      "Step: 29100, train/epoch: 6.925273895263672\n",
      "Step: 29110, train/loss: 0.0\n",
      "Step: 29110, train/grad_norm: 0.005643341224640608\n",
      "Step: 29110, train/learning_rate: 1.5361732948804274e-05\n",
      "Step: 29110, train/epoch: 6.9276533126831055\n",
      "Step: 29120, train/loss: 0.0\n",
      "Step: 29120, train/grad_norm: 0.0031427021604031324\n",
      "Step: 29120, train/learning_rate: 1.5349833120126277e-05\n",
      "Step: 29120, train/epoch: 6.930033206939697\n",
      "Step: 29130, train/loss: 0.0\n",
      "Step: 29130, train/grad_norm: 0.002587545895949006\n",
      "Step: 29130, train/learning_rate: 1.5337935110437684e-05\n",
      "Step: 29130, train/epoch: 6.932413101196289\n",
      "Step: 29140, train/loss: 0.0\n",
      "Step: 29140, train/grad_norm: 0.00040917244041338563\n",
      "Step: 29140, train/learning_rate: 1.5326035281759687e-05\n",
      "Step: 29140, train/epoch: 6.934792995452881\n",
      "Step: 29150, train/loss: 0.0\n",
      "Step: 29150, train/grad_norm: 0.0017318965401500463\n",
      "Step: 29150, train/learning_rate: 1.531413545308169e-05\n",
      "Step: 29150, train/epoch: 6.937172889709473\n",
      "Step: 29160, train/loss: 0.0\n",
      "Step: 29160, train/grad_norm: 0.0024277535267174244\n",
      "Step: 29160, train/learning_rate: 1.5302237443393096e-05\n",
      "Step: 29160, train/epoch: 6.9395527839660645\n",
      "Step: 29170, train/loss: 0.0\n",
      "Step: 29170, train/grad_norm: 0.002174721099436283\n",
      "Step: 29170, train/learning_rate: 1.52903376147151e-05\n",
      "Step: 29170, train/epoch: 6.941932201385498\n",
      "Step: 29180, train/loss: 0.0\n",
      "Step: 29180, train/grad_norm: 0.0006434950628317893\n",
      "Step: 29180, train/learning_rate: 1.5278439605026506e-05\n",
      "Step: 29180, train/epoch: 6.94431209564209\n",
      "Step: 29190, train/loss: 0.0\n",
      "Step: 29190, train/grad_norm: 0.0023978548124432564\n",
      "Step: 29190, train/learning_rate: 1.526653977634851e-05\n",
      "Step: 29190, train/epoch: 6.946691989898682\n",
      "Step: 29200, train/loss: 0.0\n",
      "Step: 29200, train/grad_norm: 0.001548672909848392\n",
      "Step: 29200, train/learning_rate: 1.5254640857165214e-05\n",
      "Step: 29200, train/epoch: 6.949071884155273\n",
      "Step: 29210, train/loss: 0.0\n",
      "Step: 29210, train/grad_norm: 0.0005281356279738247\n",
      "Step: 29210, train/learning_rate: 1.5242741937981918e-05\n",
      "Step: 29210, train/epoch: 6.951451778411865\n",
      "Step: 29220, train/loss: 0.0\n",
      "Step: 29220, train/grad_norm: 0.002883288776502013\n",
      "Step: 29220, train/learning_rate: 1.5230842109303921e-05\n",
      "Step: 29220, train/epoch: 6.953831672668457\n",
      "Step: 29230, train/loss: 0.0\n",
      "Step: 29230, train/grad_norm: 0.00037741829873993993\n",
      "Step: 29230, train/learning_rate: 1.5218943190120626e-05\n",
      "Step: 29230, train/epoch: 6.956211090087891\n",
      "Step: 29240, train/loss: 0.11089999973773956\n",
      "Step: 29240, train/grad_norm: 0.005386946722865105\n",
      "Step: 29240, train/learning_rate: 1.5207044270937331e-05\n",
      "Step: 29240, train/epoch: 6.958590984344482\n",
      "Step: 29250, train/loss: 9.999999747378752e-05\n",
      "Step: 29250, train/grad_norm: 0.0030231087002903223\n",
      "Step: 29250, train/learning_rate: 1.5195145351754036e-05\n",
      "Step: 29250, train/epoch: 6.960970878601074\n",
      "Step: 29260, train/loss: 0.0\n",
      "Step: 29260, train/grad_norm: 0.0035226258914917707\n",
      "Step: 29260, train/learning_rate: 1.518324643257074e-05\n",
      "Step: 29260, train/epoch: 6.963350772857666\n",
      "Step: 29270, train/loss: 0.0044999998062849045\n",
      "Step: 29270, train/grad_norm: 0.008516043424606323\n",
      "Step: 29270, train/learning_rate: 1.5171346603892744e-05\n",
      "Step: 29270, train/epoch: 6.965730667114258\n",
      "Step: 29280, train/loss: 0.0\n",
      "Step: 29280, train/grad_norm: 0.0007714377716183662\n",
      "Step: 29280, train/learning_rate: 1.5159447684709448e-05\n",
      "Step: 29280, train/epoch: 6.96811056137085\n",
      "Step: 29290, train/loss: 0.0\n",
      "Step: 29290, train/grad_norm: 0.04060736671090126\n",
      "Step: 29290, train/learning_rate: 1.5147548765526153e-05\n",
      "Step: 29290, train/epoch: 6.970490455627441\n",
      "Step: 29300, train/loss: 0.0\n",
      "Step: 29300, train/grad_norm: 0.001093576312996447\n",
      "Step: 29300, train/learning_rate: 1.5135649846342858e-05\n",
      "Step: 29300, train/epoch: 6.972869873046875\n",
      "Step: 29310, train/loss: 0.0\n",
      "Step: 29310, train/grad_norm: 0.0020114246290177107\n",
      "Step: 29310, train/learning_rate: 1.5123750927159563e-05\n",
      "Step: 29310, train/epoch: 6.975249767303467\n",
      "Step: 29320, train/loss: 0.09610000252723694\n",
      "Step: 29320, train/grad_norm: 0.001490834983997047\n",
      "Step: 29320, train/learning_rate: 1.5111851098481566e-05\n",
      "Step: 29320, train/epoch: 6.977629661560059\n",
      "Step: 29330, train/loss: 0.0\n",
      "Step: 29330, train/grad_norm: 0.0034131580032408237\n",
      "Step: 29330, train/learning_rate: 1.509995217929827e-05\n",
      "Step: 29330, train/epoch: 6.98000955581665\n",
      "Step: 29340, train/loss: 0.0\n",
      "Step: 29340, train/grad_norm: 0.0036161490716040134\n",
      "Step: 29340, train/learning_rate: 1.5088053260114975e-05\n",
      "Step: 29340, train/epoch: 6.982389450073242\n",
      "Step: 29350, train/loss: 0.0\n",
      "Step: 29350, train/grad_norm: 0.004804337862879038\n",
      "Step: 29350, train/learning_rate: 1.507615434093168e-05\n",
      "Step: 29350, train/epoch: 6.984769344329834\n",
      "Step: 29360, train/loss: 0.0\n",
      "Step: 29360, train/grad_norm: 0.0022659883834421635\n",
      "Step: 29360, train/learning_rate: 1.5064255421748385e-05\n",
      "Step: 29360, train/epoch: 6.987148761749268\n",
      "Step: 29370, train/loss: 0.12349999696016312\n",
      "Step: 29370, train/grad_norm: 0.007191483862698078\n",
      "Step: 29370, train/learning_rate: 1.5052355593070388e-05\n",
      "Step: 29370, train/epoch: 6.989528656005859\n",
      "Step: 29380, train/loss: 9.999999747378752e-05\n",
      "Step: 29380, train/grad_norm: 0.021572282537817955\n",
      "Step: 29380, train/learning_rate: 1.5040456673887093e-05\n",
      "Step: 29380, train/epoch: 6.991908550262451\n",
      "Step: 29390, train/loss: 0.0\n",
      "Step: 29390, train/grad_norm: 0.013414962217211723\n",
      "Step: 29390, train/learning_rate: 1.5028557754703797e-05\n",
      "Step: 29390, train/epoch: 6.994288444519043\n",
      "Step: 29400, train/loss: 0.0\n",
      "Step: 29400, train/grad_norm: 0.00787089392542839\n",
      "Step: 29400, train/learning_rate: 1.5016658835520502e-05\n",
      "Step: 29400, train/epoch: 6.996668338775635\n",
      "Step: 29410, train/loss: 0.0\n",
      "Step: 29410, train/grad_norm: 0.0038868566043674946\n",
      "Step: 29410, train/learning_rate: 1.5004759916337207e-05\n",
      "Step: 29410, train/epoch: 6.999048233032227\n",
      "Step: 29414, eval/loss: 0.01692274771630764\n",
      "Step: 29414, eval/accuracy: 0.997501015663147\n",
      "Step: 29414, eval/f1: 0.9973592758178711\n",
      "Step: 29414, eval/runtime: 857.0255126953125\n",
      "Step: 29414, eval/samples_per_second: 8.404999732971191\n",
      "Step: 29414, eval/steps_per_second: 1.0509999990463257\n",
      "Step: 29414, train/epoch: 7.0\n",
      "Step: 29420, train/loss: 0.0\n",
      "Step: 29420, train/grad_norm: 0.0024408535100519657\n",
      "Step: 29420, train/learning_rate: 1.4992860997153912e-05\n",
      "Step: 29420, train/epoch: 7.001428127288818\n",
      "Step: 29430, train/loss: 0.0\n",
      "Step: 29430, train/grad_norm: 0.0018219155026599765\n",
      "Step: 29430, train/learning_rate: 1.4980961168475915e-05\n",
      "Step: 29430, train/epoch: 7.003807544708252\n",
      "Step: 29440, train/loss: 0.0\n",
      "Step: 29440, train/grad_norm: 0.0006938840379007161\n",
      "Step: 29440, train/learning_rate: 1.496906224929262e-05\n",
      "Step: 29440, train/epoch: 7.006187438964844\n",
      "Step: 29450, train/loss: 0.0\n",
      "Step: 29450, train/grad_norm: 0.0008306802483275533\n",
      "Step: 29450, train/learning_rate: 1.4957163330109324e-05\n",
      "Step: 29450, train/epoch: 7.0085673332214355\n",
      "Step: 29460, train/loss: 0.0\n",
      "Step: 29460, train/grad_norm: 0.0014774135779589415\n",
      "Step: 29460, train/learning_rate: 1.4945264410926029e-05\n",
      "Step: 29460, train/epoch: 7.010947227478027\n",
      "Step: 29470, train/loss: 0.0\n",
      "Step: 29470, train/grad_norm: 0.0010103851091116667\n",
      "Step: 29470, train/learning_rate: 1.4933365491742734e-05\n",
      "Step: 29470, train/epoch: 7.013327121734619\n",
      "Step: 29480, train/loss: 0.0\n",
      "Step: 29480, train/grad_norm: 0.0023108625318855047\n",
      "Step: 29480, train/learning_rate: 1.4921465663064737e-05\n",
      "Step: 29480, train/epoch: 7.015707015991211\n",
      "Step: 29490, train/loss: 0.0\n",
      "Step: 29490, train/grad_norm: 0.0008727280655875802\n",
      "Step: 29490, train/learning_rate: 1.4909566743881442e-05\n",
      "Step: 29490, train/epoch: 7.0180864334106445\n",
      "Step: 29500, train/loss: 0.0\n",
      "Step: 29500, train/grad_norm: 0.0011507277376949787\n",
      "Step: 29500, train/learning_rate: 1.4897667824698146e-05\n",
      "Step: 29500, train/epoch: 7.020466327667236\n",
      "Step: 29510, train/loss: 0.0\n",
      "Step: 29510, train/grad_norm: 0.0019083800725638866\n",
      "Step: 29510, train/learning_rate: 1.4885768905514851e-05\n",
      "Step: 29510, train/epoch: 7.022846221923828\n",
      "Step: 29520, train/loss: 0.0\n",
      "Step: 29520, train/grad_norm: 0.0008377081830985844\n",
      "Step: 29520, train/learning_rate: 1.4873869986331556e-05\n",
      "Step: 29520, train/epoch: 7.02522611618042\n",
      "Step: 29530, train/loss: 0.0\n",
      "Step: 29530, train/grad_norm: 0.00905714649707079\n",
      "Step: 29530, train/learning_rate: 1.4861970157653559e-05\n",
      "Step: 29530, train/epoch: 7.027606010437012\n",
      "Step: 29540, train/loss: 0.0\n",
      "Step: 29540, train/grad_norm: 0.0005722157075069845\n",
      "Step: 29540, train/learning_rate: 1.4850071238470264e-05\n",
      "Step: 29540, train/epoch: 7.0299859046936035\n",
      "Step: 29550, train/loss: 0.0\n",
      "Step: 29550, train/grad_norm: 0.0003661439986899495\n",
      "Step: 29550, train/learning_rate: 1.4838172319286969e-05\n",
      "Step: 29550, train/epoch: 7.032365322113037\n",
      "Step: 29560, train/loss: 0.0\n",
      "Step: 29560, train/grad_norm: 0.00037430092925205827\n",
      "Step: 29560, train/learning_rate: 1.4826273400103673e-05\n",
      "Step: 29560, train/epoch: 7.034745216369629\n",
      "Step: 29570, train/loss: 0.01080000028014183\n",
      "Step: 29570, train/grad_norm: 0.0006434413371607661\n",
      "Step: 29570, train/learning_rate: 1.4814374480920378e-05\n",
      "Step: 29570, train/epoch: 7.037125110626221\n",
      "Step: 29580, train/loss: 0.0\n",
      "Step: 29580, train/grad_norm: 0.000624298641923815\n",
      "Step: 29580, train/learning_rate: 1.4802474652242381e-05\n",
      "Step: 29580, train/epoch: 7.0395050048828125\n",
      "Step: 29590, train/loss: 0.0\n",
      "Step: 29590, train/grad_norm: 0.0009448511991649866\n",
      "Step: 29590, train/learning_rate: 1.4790575733059086e-05\n",
      "Step: 29590, train/epoch: 7.041884899139404\n",
      "Step: 29600, train/loss: 0.0\n",
      "Step: 29600, train/grad_norm: 0.0014031727332621813\n",
      "Step: 29600, train/learning_rate: 1.477867681387579e-05\n",
      "Step: 29600, train/epoch: 7.044264793395996\n",
      "Step: 29610, train/loss: 0.0\n",
      "Step: 29610, train/grad_norm: 0.001136938575655222\n",
      "Step: 29610, train/learning_rate: 1.4766777894692495e-05\n",
      "Step: 29610, train/epoch: 7.046644687652588\n",
      "Step: 29620, train/loss: 0.0\n",
      "Step: 29620, train/grad_norm: 0.0005795668694190681\n",
      "Step: 29620, train/learning_rate: 1.47548789755092e-05\n",
      "Step: 29620, train/epoch: 7.0490241050720215\n",
      "Step: 29630, train/loss: 0.0\n",
      "Step: 29630, train/grad_norm: 0.00047602193080820143\n",
      "Step: 29630, train/learning_rate: 1.4742979146831203e-05\n",
      "Step: 29630, train/epoch: 7.051403999328613\n",
      "Step: 29640, train/loss: 0.0\n",
      "Step: 29640, train/grad_norm: 0.0007336054113693535\n",
      "Step: 29640, train/learning_rate: 1.4731080227647908e-05\n",
      "Step: 29640, train/epoch: 7.053783893585205\n",
      "Step: 29650, train/loss: 0.0\n",
      "Step: 29650, train/grad_norm: 0.0003602336801122874\n",
      "Step: 29650, train/learning_rate: 1.4719181308464613e-05\n",
      "Step: 29650, train/epoch: 7.056163787841797\n",
      "Step: 29660, train/loss: 0.0\n",
      "Step: 29660, train/grad_norm: 0.0007759274449199438\n",
      "Step: 29660, train/learning_rate: 1.4707282389281318e-05\n",
      "Step: 29660, train/epoch: 7.058543682098389\n",
      "Step: 29670, train/loss: 0.0\n",
      "Step: 29670, train/grad_norm: 0.0004232296487316489\n",
      "Step: 29670, train/learning_rate: 1.4695383470098022e-05\n",
      "Step: 29670, train/epoch: 7.0609235763549805\n",
      "Step: 29680, train/loss: 0.0\n",
      "Step: 29680, train/grad_norm: 0.0014861574163660407\n",
      "Step: 29680, train/learning_rate: 1.4683483641420025e-05\n",
      "Step: 29680, train/epoch: 7.063302993774414\n",
      "Step: 29690, train/loss: 0.0\n",
      "Step: 29690, train/grad_norm: 0.0004369573143776506\n",
      "Step: 29690, train/learning_rate: 1.467158472223673e-05\n",
      "Step: 29690, train/epoch: 7.065682888031006\n",
      "Step: 29700, train/loss: 0.0\n",
      "Step: 29700, train/grad_norm: 0.0002148583298549056\n",
      "Step: 29700, train/learning_rate: 1.4659685803053435e-05\n",
      "Step: 29700, train/epoch: 7.068062782287598\n",
      "Step: 29710, train/loss: 0.0\n",
      "Step: 29710, train/grad_norm: 0.00043524496140889823\n",
      "Step: 29710, train/learning_rate: 1.464778688387014e-05\n",
      "Step: 29710, train/epoch: 7.0704426765441895\n",
      "Step: 29720, train/loss: 0.0\n",
      "Step: 29720, train/grad_norm: 0.00011681322939693928\n",
      "Step: 29720, train/learning_rate: 1.4635887964686844e-05\n",
      "Step: 29720, train/epoch: 7.072822570800781\n",
      "Step: 29730, train/loss: 0.0\n",
      "Step: 29730, train/grad_norm: 0.00023130980844143778\n",
      "Step: 29730, train/learning_rate: 1.4623988136008848e-05\n",
      "Step: 29730, train/epoch: 7.075202465057373\n",
      "Step: 29740, train/loss: 0.0\n",
      "Step: 29740, train/grad_norm: 0.003883595112711191\n",
      "Step: 29740, train/learning_rate: 1.4612089216825552e-05\n",
      "Step: 29740, train/epoch: 7.077581882476807\n",
      "Step: 29750, train/loss: 0.0\n",
      "Step: 29750, train/grad_norm: 0.0003493432595860213\n",
      "Step: 29750, train/learning_rate: 1.4600190297642257e-05\n",
      "Step: 29750, train/epoch: 7.079961776733398\n",
      "Step: 29760, train/loss: 0.0\n",
      "Step: 29760, train/grad_norm: 0.00026381894713267684\n",
      "Step: 29760, train/learning_rate: 1.4588291378458962e-05\n",
      "Step: 29760, train/epoch: 7.08234167098999\n",
      "Step: 29770, train/loss: 0.09019999951124191\n",
      "Step: 29770, train/grad_norm: 0.0005038956878706813\n",
      "Step: 29770, train/learning_rate: 1.4576392459275667e-05\n",
      "Step: 29770, train/epoch: 7.084721565246582\n",
      "Step: 29780, train/loss: 9.999999747378752e-05\n",
      "Step: 29780, train/grad_norm: 0.00021919768187217414\n",
      "Step: 29780, train/learning_rate: 1.4564493540092371e-05\n",
      "Step: 29780, train/epoch: 7.087101459503174\n",
      "Step: 29790, train/loss: 0.0\n",
      "Step: 29790, train/grad_norm: 0.0039604054763913155\n",
      "Step: 29790, train/learning_rate: 1.4552593711414374e-05\n",
      "Step: 29790, train/epoch: 7.089481353759766\n",
      "Step: 29800, train/loss: 0.0\n",
      "Step: 29800, train/grad_norm: 0.000312777585349977\n",
      "Step: 29800, train/learning_rate: 1.454069479223108e-05\n",
      "Step: 29800, train/epoch: 7.091861248016357\n",
      "Step: 29810, train/loss: 0.0\n",
      "Step: 29810, train/grad_norm: 6.781420233892277e-05\n",
      "Step: 29810, train/learning_rate: 1.4528795873047784e-05\n",
      "Step: 29810, train/epoch: 7.094240665435791\n",
      "Step: 29820, train/loss: 0.0\n",
      "Step: 29820, train/grad_norm: 0.00030534534016624093\n",
      "Step: 29820, train/learning_rate: 1.4516896953864489e-05\n",
      "Step: 29820, train/epoch: 7.096620559692383\n",
      "Step: 29830, train/loss: 0.0\n",
      "Step: 29830, train/grad_norm: 0.000267393043031916\n",
      "Step: 29830, train/learning_rate: 1.4504998034681194e-05\n",
      "Step: 29830, train/epoch: 7.099000453948975\n",
      "Step: 29840, train/loss: 0.0\n",
      "Step: 29840, train/grad_norm: 0.0001547533756820485\n",
      "Step: 29840, train/learning_rate: 1.4493098206003197e-05\n",
      "Step: 29840, train/epoch: 7.101380348205566\n",
      "Step: 29850, train/loss: 0.0\n",
      "Step: 29850, train/grad_norm: 0.00036371112219057977\n",
      "Step: 29850, train/learning_rate: 1.4481199286819901e-05\n",
      "Step: 29850, train/epoch: 7.103760242462158\n",
      "Step: 29860, train/loss: 0.0\n",
      "Step: 29860, train/grad_norm: 0.0001538226642878726\n",
      "Step: 29860, train/learning_rate: 1.4469300367636606e-05\n",
      "Step: 29860, train/epoch: 7.10614013671875\n",
      "Step: 29870, train/loss: 0.0\n",
      "Step: 29870, train/grad_norm: 0.00018487611669115722\n",
      "Step: 29870, train/learning_rate: 1.4457401448453311e-05\n",
      "Step: 29870, train/epoch: 7.108519554138184\n",
      "Step: 29880, train/loss: 0.0\n",
      "Step: 29880, train/grad_norm: 0.0007326248451136053\n",
      "Step: 29880, train/learning_rate: 1.4445502529270016e-05\n",
      "Step: 29880, train/epoch: 7.110899448394775\n",
      "Step: 29890, train/loss: 0.0\n",
      "Step: 29890, train/grad_norm: 0.00035435124300420284\n",
      "Step: 29890, train/learning_rate: 1.4433602700592019e-05\n",
      "Step: 29890, train/epoch: 7.113279342651367\n",
      "Step: 29900, train/loss: 0.0\n",
      "Step: 29900, train/grad_norm: 0.00018504762556403875\n",
      "Step: 29900, train/learning_rate: 1.4421703781408723e-05\n",
      "Step: 29900, train/epoch: 7.115659236907959\n",
      "Step: 29910, train/loss: 0.0\n",
      "Step: 29910, train/grad_norm: 0.0017018836224451661\n",
      "Step: 29910, train/learning_rate: 1.4409804862225428e-05\n",
      "Step: 29910, train/epoch: 7.118039131164551\n",
      "Step: 29920, train/loss: 0.0\n",
      "Step: 29920, train/grad_norm: 0.00013583514373749495\n",
      "Step: 29920, train/learning_rate: 1.4397905943042133e-05\n",
      "Step: 29920, train/epoch: 7.120419025421143\n",
      "Step: 29930, train/loss: 0.0\n",
      "Step: 29930, train/grad_norm: 3.181509600835852e-05\n",
      "Step: 29930, train/learning_rate: 1.4386007023858838e-05\n",
      "Step: 29930, train/epoch: 7.122798442840576\n",
      "Step: 29940, train/loss: 0.0\n",
      "Step: 29940, train/grad_norm: 0.00011024237028323114\n",
      "Step: 29940, train/learning_rate: 1.437410719518084e-05\n",
      "Step: 29940, train/epoch: 7.125178337097168\n",
      "Step: 29950, train/loss: 0.0\n",
      "Step: 29950, train/grad_norm: 0.00016493117436766624\n",
      "Step: 29950, train/learning_rate: 1.4362208275997546e-05\n",
      "Step: 29950, train/epoch: 7.12755823135376\n",
      "Step: 29960, train/loss: 0.17810000479221344\n",
      "Step: 29960, train/grad_norm: 0.00041647913167253137\n",
      "Step: 29960, train/learning_rate: 1.435030935681425e-05\n",
      "Step: 29960, train/epoch: 7.129938125610352\n",
      "Step: 29970, train/loss: 0.0\n",
      "Step: 29970, train/grad_norm: 0.0035225411411374807\n",
      "Step: 29970, train/learning_rate: 1.4338410437630955e-05\n",
      "Step: 29970, train/epoch: 7.132318019866943\n",
      "Step: 29980, train/loss: 0.0\n",
      "Step: 29980, train/grad_norm: 0.026126621291041374\n",
      "Step: 29980, train/learning_rate: 1.432651151844766e-05\n",
      "Step: 29980, train/epoch: 7.134697914123535\n",
      "Step: 29990, train/loss: 0.0\n",
      "Step: 29990, train/grad_norm: 0.005421710666269064\n",
      "Step: 29990, train/learning_rate: 1.4314611689769663e-05\n",
      "Step: 29990, train/epoch: 7.137077808380127\n",
      "Step: 30000, train/loss: 0.0\n",
      "Step: 30000, train/grad_norm: 0.0027227038517594337\n",
      "Step: 30000, train/learning_rate: 1.4302712770586368e-05\n",
      "Step: 30000, train/epoch: 7.1394572257995605\n",
      "Step: 30010, train/loss: 0.0\n",
      "Step: 30010, train/grad_norm: 0.0022110561840236187\n",
      "Step: 30010, train/learning_rate: 1.4290813851403072e-05\n",
      "Step: 30010, train/epoch: 7.141837120056152\n",
      "Step: 30020, train/loss: 9.999999747378752e-05\n",
      "Step: 30020, train/grad_norm: 0.0011977178510278463\n",
      "Step: 30020, train/learning_rate: 1.4278914932219777e-05\n",
      "Step: 30020, train/epoch: 7.144217014312744\n",
      "Step: 30030, train/loss: 0.0\n",
      "Step: 30030, train/grad_norm: 0.0019217064836993814\n",
      "Step: 30030, train/learning_rate: 1.4267016013036482e-05\n",
      "Step: 30030, train/epoch: 7.146596908569336\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.summary.summary_iterator import summary_iterator\n",
    "import glob\n",
    "import os\n",
    "\n",
    "logs_directory = os.path.join('./', project_name, 'logs')\n",
    "file_pattern = 'events.out.tfevents.*'\n",
    "\n",
    "event_files = glob.glob(os.path.join(logs_directory, file_pattern))\n",
    "\n",
    "def print_events_from_file(event_files):\n",
    "    for event_file in event_files:\n",
    "        print(f\"Reading events from file: {event_file}\")\n",
    "        try:\n",
    "            for e in summary_iterator(event_file):\n",
    "                for v in e.summary.value:\n",
    "                    if v.HasField('simple_value'):\n",
    "                        print(f\"Step: {e.step}, {v.tag}: {v.simple_value}\")\n",
    "        except Exception as e:  # Just in case the event file is not readable\n",
    "            print(f\"Failed to read {event_file}: {e}\")\n",
    "\n",
    "print_events_from_file(event_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ffd0164-0f41-4bbe-b905-7dda371dfd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Step Train Loss Eval Loss  Accuracy        F1\n",
      "3   4202   0.419500  0.078117  0.989449  0.988805\n",
      "4   8404   0.006900  0.098902  0.989032  0.988376\n",
      "5  12606   0.000000  0.019651  0.996668  0.996481\n",
      "6  16808   0.000000  0.018377  0.997779  0.997653\n",
      "7  21010   0.000000  0.028315  0.996946  0.996775\n",
      "8  25212   0.000000  0.073471  0.991948  0.991469\n",
      "9  29414   0.000000  0.016923  0.997501  0.997359\n",
      "0  33616   0.000000  0.015864  0.996946  0.996776\n",
      "1  37818   0.000000  0.016234  0.997779  0.997653\n",
      "2  42020   0.000000  0.016317  0.997640  0.997506\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.python.summary.summary_iterator import summary_iterator\n",
    "\n",
    "logs_directory = os.path.join('./', project_name, 'logs')\n",
    "file_pattern = 'events.out.tfevents.*'\n",
    "\n",
    "event_files = glob.glob(os.path.join(logs_directory, file_pattern))\n",
    "\n",
    "def extract_metrics(event_files):\n",
    "    data = []\n",
    "    last_train_loss = None\n",
    "\n",
    "    for event_file in event_files:\n",
    "        for e in summary_iterator(event_file):\n",
    "            for v in e.summary.value:\n",
    "                if v.HasField('simple_value'):\n",
    "                    step = e.step\n",
    "                    metric_name = v.tag.split('/')[-1]\n",
    "                    metric_value = v.simple_value\n",
    "\n",
    "                    formatted_value = f\"{metric_value:.6f}\"\n",
    "\n",
    "                    if 'train/loss' in v.tag:\n",
    "                        last_train_loss = formatted_value\n",
    "\n",
    "                    if 'eval' in v.tag:\n",
    "                        entry = next((item for item in data if item['Step'] == step), None)\n",
    "                        if not entry:\n",
    "                            entry = {'Step': step, 'Train Loss': last_train_loss, 'Eval Loss': None, 'Accuracy': None, 'F1': None}\n",
    "                            data.append(entry)\n",
    "                        if 'loss' in v.tag:\n",
    "                            entry['Eval Loss'] = formatted_value\n",
    "                        elif 'accuracy' in v.tag:\n",
    "                            entry['Accuracy'] = formatted_value\n",
    "                        elif 'f1' in v.tag:\n",
    "                            entry['F1'] = formatted_value\n",
    "\n",
    "    return data\n",
    "\n",
    "metrics_data = extract_metrics(event_files)\n",
    "\n",
    "df = pd.DataFrame(metrics_data)\n",
    "df = df.sort_values(by='Step')\n",
    "\n",
    "file_path = \"./images/\"+model_name+\"_Checkpoint_Data.csv\"\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37cc5f92-d47f-4356-8fad-d9b64b6f5361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Checkpoint Step: checkpoint-37818\n",
      "Step             37818\n",
      "Train Loss    0.000000\n",
      "Eval Loss     0.016234\n",
      "Accuracy      0.997779\n",
      "F1            0.997653\n",
      "Rank Sum           4.0\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df.fillna({\n",
    "    'Eval Loss': float('inf'),\n",
    "    'Accuracy': 0,\n",
    "    'F1': 0\n",
    "}, inplace=True)\n",
    "\n",
    "df['Eval Loss'] = df['Eval Loss'].astype(float)\n",
    "df['Accuracy'] = df['Accuracy'].astype(float)\n",
    "df['F1'] = df['F1'].astype(float)\n",
    "\n",
    "df['Eval Loss Rank'] = df['Eval Loss'].rank(method='min', ascending=True)\n",
    "df['Accuracy Rank'] = df['Accuracy'].rank(method='min', ascending=False)\n",
    "df['F1 Rank'] = df['F1'].rank(method='min', ascending=False)\n",
    "\n",
    "df['Rank Sum'] = df['Eval Loss Rank'] + df['Accuracy Rank'] + df['F1 Rank']\n",
    "\n",
    "best_checkpoint = df.loc[df['Rank Sum'].idxmin()]\n",
    "\n",
    "checkpoint_folder_name = f\"checkpoint-{best_checkpoint['Step']}\"\n",
    "print(f\"Best Checkpoint Step: {checkpoint_folder_name}\")\n",
    "print(best_checkpoint[['Step', 'Train Loss', 'Eval Loss', 'Accuracy', 'F1', 'Rank Sum']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3c3999-8c32-4a25-aaca-2ec9036b69ed",
   "metadata": {},
   "source": [
    "### Run TensorBoard\n",
    "tensorboard --logdir=~/kuk/Praxis/praxis-Llama-2-7b-hf-small-finetune/logs --host=0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f36022-dc73-492f-998c-43e5ed1a6f46",
   "metadata": {},
   "source": [
    "### PAUSE SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "444ae65b-241c-47ef-bbc2-81f3a2ce50e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My flag to pause the script, set to True to pause\n",
    "pause_script = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8be3672-68e0-44f8-b8b1-31290665658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        print(\"Script Paused\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb3b5ed2-5320-49c0-9e61-90d6f0942f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pause_script:\n",
    "    raise StopExecution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19be6e26-d888-4da0-b3f8-836d68ac2051",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5111194-5eeb-4104-8df0-f977a6b716e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d4258edc6847a89ce5b4a8c2f4a1d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GemmaForSequenceClassification were not initialized from the model checkpoint at google/gemma-7b and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    num_labels=2,\n",
    "    token=access_token,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    checkpoint,\n",
    "    token=access_token,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "530fa55f-c8c4-485f-a093-09bf2175d586",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "test_checkpoint_name = checkpoint_folder_name\n",
    "ft_model = PeftModel.from_pretrained(base_model, project_name+'/'+test_checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27354c4f-95cc-4d1f-ac64-c5e6b4a842ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GemmaForSequenceClassification(\n",
       "      (model): GemmaModel(\n",
       "        (embed_tokens): Embedding(256000, 3072, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x GemmaDecoderLayer(\n",
       "            (self_attn): GemmaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): GemmaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): GemmaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=24576, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=24576, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=24576, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=24576, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=24576, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=24576, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): PytorchGELUTanh()\n",
       "            )\n",
       "            (input_layernorm): GemmaRMSNorm()\n",
       "            (post_attention_layernorm): GemmaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): GemmaRMSNorm()\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=3072, out_features=2, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=3072, out_features=2, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    ft_model = torch.nn.DataParallel(ft_model)\n",
    "\n",
    "ft_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eee9ab6f-a7d5-4dd5-9436-2f6f6e2bffaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5385962c84104849966a05cbaad464c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "processed_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for record in tqdm(tokenized_test_ds):\n",
    "\n",
    "        eval_prompt = record['article']\n",
    "        model_input = tokenize_fn({'article': eval_prompt})\n",
    "\n",
    "        # model_input = {k: v.to('cuda') for k, v in model_input.items()}\n",
    "        \n",
    "        outputs = ft_model(**model_input)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        prediction = logits[0].argmax(-1).item()  # Use .item() to get a Python number\n",
    "        processed_predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f06a2c-5ded-4da0-8232-145383967c9d",
   "metadata": {},
   "source": [
    "### Accuracy and F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e891bc5-55e0-4c43-a035-0edc37dcb6e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "true_articles = tokenized_test_ds['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6bf33824-d7cc-4d22-af4d-7d44e569c2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9979175343606831\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(true_articles, processed_predictions)\n",
    "print(\"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b37dc24-3bc0-48a0-ace9-a360bae91169",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(true_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a29ebf68-632f-49d1-b903-407ff05b5569",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(processed_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb4d8350-a8d1-4853-a9f8-74ae9ea36bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.9978013211120609\n"
     ]
    }
   ],
   "source": [
    "f1_score = f1_score(true_articles, processed_predictions, average='macro')\n",
    "print(\"f1_score:\", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcaac14-2b83-4c24-b83a-f6d0e73a2d2a",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57e64afb-e3ee-4c79-8228-b2d79806229e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(tokenized_test_ds['label'], processed_predictions)\n",
    "\n",
    "labels = ['human', 'machine']\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af7d6f55-fb4f-4195-ac3f-93852ecd4b65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook praxis-gemma-7b-small-finetune-v12.ipynb to html\n",
      "[NbConvertApp] WARNING | Alternative text is missing on 1 image(s).\n",
      "[NbConvertApp] Writing 1313044 bytes to html/praxis-gemma-7b-small-finetune-v12.html\n"
     ]
    }
   ],
   "source": [
    "file_name = f\"{project_name}.ipynb\"\n",
    "html_file_name = f\"{file_name.replace('.ipynb', '.html')}\"\n",
    "\n",
    "command = f\"jupyter nbconvert '{file_name}' --to html --output-dir './html' --output '{html_file_name}'\"\n",
    "get_ipython().system(command)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
