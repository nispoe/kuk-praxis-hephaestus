{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c2d2c9-4a4b-48ca-90a3-1ccd120ca08b",
   "metadata": {},
   "source": [
    "# Fine tuning using BERT Base Uncased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0f2da2-54e7-42b7-b44e-f6b7e8a48787",
   "metadata": {},
   "source": [
    "### Supress warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "362a364e-20f5-4d16-af44-62aada774f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d23075-e94c-4aa9-969f-ba66e1278d5e",
   "metadata": {},
   "source": [
    "### Inspect the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7cedfd8-5efe-47e5-84df-f1c00cd7cbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "checkpoint = \"google-bert/\"+model_name\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84b0595d-6453-48e4-982f-0608c272cb91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "BertForSequenceClassification                           --\n",
       "├─BertModel: 1-1                                        --\n",
       "│    └─BertEmbeddings: 2-1                              --\n",
       "│    │    └─Embedding: 3-1                              23,440,896\n",
       "│    │    └─Embedding: 3-2                              393,216\n",
       "│    │    └─Embedding: 3-3                              1,536\n",
       "│    │    └─LayerNorm: 3-4                              1,536\n",
       "│    │    └─Dropout: 3-5                                --\n",
       "│    └─BertEncoder: 2-2                                 --\n",
       "│    │    └─ModuleList: 3-6                             85,054,464\n",
       "│    └─BertPooler: 2-3                                  --\n",
       "│    │    └─Linear: 3-7                                 590,592\n",
       "│    │    └─Tanh: 3-8                                   --\n",
       "├─Dropout: 1-2                                          --\n",
       "├─Linear: 1-3                                           1,538\n",
       "================================================================================\n",
       "Total params: 109,483,778\n",
       "Trainable params: 109,483,778\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09a29b9b-fd31-4bb9-ada1-e7264f8a85a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601449e5-fe1f-4194-b061-4951fc8bb86f",
   "metadata": {},
   "source": [
    "### Load the news dataset from pickle file\n",
    "If any of the check_files don't exist then load the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5365d1de-318f-46e6-b91b-609f9fd8cbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one of the specified files already exists. Not loading new dataset.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "base_path = './data/'\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "file_name = 'news_small_dataset.pkl'\n",
    "file_path = base_path+file_name\n",
    "\n",
    "def pickle_dataset(dataset, file_path):\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(dataset, file)\n",
    "        print(f\"Dataset has been pickled to: {file_path}\")\n",
    "\n",
    "def load_pickle_dataset(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        dataset = pickle.load(file)\n",
    "        print(f\"Dataset has been loaded from: {file_path}\")\n",
    "    return dataset\n",
    "\n",
    "def check_files_exists(file_names):\n",
    "    for name in file_names:\n",
    "        file_path = os.path.join(base_path, name)\n",
    "        if os.path.exists(file_path):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# if these files exist we do not want to load the news_dataset.pkl to tokenize and make these files\n",
    "check_files = [model_name+'-small_tokenized_train_ds.pkl', model_name+'-small_tokenized_eval_ds.pkl', model_name+'-small_tokenized_test_ds.pkl']\n",
    "\n",
    "if check_files_exists(check_files):\n",
    "    print(\"At least one of the specified files already exists. Not loading new dataset.\")\n",
    "else:\n",
    "    news_split_ds = load_pickle_dataset(file_path)\n",
    "    print(news_split_ds)\n",
    "    total_rows = (news_split_ds['train'].num_rows +\n",
    "              news_split_ds['eval'].num_rows +\n",
    "              news_split_ds['test'].num_rows)\n",
    "    print(\"Total number of rows:\", total_rows)\n",
    "    print(\"Dataset loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "395adbb2-9498-49cc-baf1-f09e3fcfb39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1365a5fb369c40f889577c8baf74a298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8db8272e004d60ab4e30ec3b08783c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0bc9390acd94715b7736d8b9207f752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_fn(news):\n",
    "  return tokenizer(news['article'], padding=True, truncation=True, return_tensors=\"pt\", max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6234e6-1e6b-4f41-b3a2-5bb0adfae5a4",
   "metadata": {},
   "source": [
    "### Tokenize train, evaluation, and test datasets\n",
    "If any of the check files exist then don't run tokenization and save some time.\n",
    "Else load the pickle files that already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0b27288-1ea3-4230-9f9d-aeab3cd76b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already exist, so load datasets\n",
      "Dataset has been loaded from: ./data/bert-base-uncased-small_tokenized_train_ds.pkl\n",
      "Dataset has been loaded from: ./data/bert-base-uncased-small_tokenized_eval_ds.pkl\n",
      "Dataset has been loaded from: ./data/bert-base-uncased-small_tokenized_test_ds.pkl\n"
     ]
    }
   ],
   "source": [
    "if not check_files_exists(check_files):\n",
    "    tokenized_train_ds = news_split_ds['train'].map(tokenize_fn, batched=True)\n",
    "    tokenized_eval_ds = news_split_ds['eval'].map(tokenize_fn, batched=True)\n",
    "    tokenized_test_ds = news_split_ds['test'].map(tokenize_fn, batched=True)\n",
    "\n",
    "    print(tokenized_train_ds.features)\n",
    "    print(tokenized_eval_ds.features)\n",
    "    print(tokenized_test_ds.features)\n",
    "    \n",
    "    pickle_dataset(tokenized_train_ds, base_path+model_name+'-small_tokenized_train_ds.pkl')\n",
    "    pickle_dataset(tokenized_eval_ds, base_path+model_name+'-small_tokenized_eval_ds.pkl')\n",
    "    pickle_dataset(tokenized_test_ds, base_path+model_name+'-small_tokenized_test_ds.pkl')\n",
    "else:\n",
    "    print(\"Files already exist, so load datasets\")\n",
    "    tokenized_train_ds = load_pickle_dataset(base_path+model_name+'-small_tokenized_train_ds.pkl')\n",
    "    tokenized_eval_ds = load_pickle_dataset(base_path+model_name+'-small_tokenized_eval_ds.pkl')\n",
    "    tokenized_test_ds = load_pickle_dataset(base_path+model_name+'-small_tokenized_test_ds.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828b2ac6-7634-40c9-9ed8-222634448ce1",
   "metadata": {},
   "source": [
    "### Look at the tokenized data\n",
    "Notice what the actual data looks like, and then the tokenized data which is a bunch of numbers, and then the attention mask at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ca4f524-e97e-4cee-b28a-74f204f669b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in training dataset: 33611\n",
      "Number of records in evaluation dataset: 7203\n",
      "Number of records in test dataset: 7203\n",
      "Total number of records: 48017\n"
     ]
    }
   ],
   "source": [
    "count_train_records = len(tokenized_train_ds)\n",
    "count_eval_records = len(tokenized_eval_ds)\n",
    "count_test_records = len(tokenized_test_ds)\n",
    "print(f\"Number of records in training dataset: {count_train_records}\")\n",
    "print(f\"Number of records in evaluation dataset: {count_eval_records}\")\n",
    "print(f\"Number of records in test dataset: {count_test_records}\")\n",
    "count_total_records = count_train_records + count_eval_records + count_test_records\n",
    "print(f\"Total number of records: {count_total_records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab4e094a-6fc0-4e11-8ab2-8e394b7a07a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'article': \"In a year where homicides, rapes and robberies increased slightly, New York City still saw serious crime drop 1.7 percent in 2015, continuing an overall decline that began in the 1990s, NYPD Commissioner William Bratton said Monday.\\nAt a news conference with Mayor Bill de Blasio, Bratton touted last year’s crime statistics, which he said, when combined with an even larger decline in 2014, put to rest the fear that substantial decreases couldn’t continue under the new administration at City Hall.\\n“While we have had some fluctuation, some increases in certain categories, the overall trend in all our crime categories continues to go down,” Bratton told reporters. “It was a very good year for us, 2015.\\nHomicides increased by 4.5 percent in 2015, rising to 350 from 333 in the prior year, which was the lowest since 1994, said Deputy Commissioner Dermot Shea. Rapes increased 6 percent and robberies rose 2 percent, said Shea, who is in charge of data collection and operations for the NYPD.\\nThe lower overall crime statistics came about due to what Shea called “targeted enforcement,” where cops make quality arrests even though the overall number of apprehensions was the lowest in the city since 2003.\\nTwo boroughs — Manhattan and the Bronx — actually saw serious crimes increase by 3 percent and 4 percent, respectively, Shea said. Manhattan’s increase was driven by more robberies, while the Bronx, although seeing an overall crime increase, had what he said was a “phenomenal” reduction in shootings. Citywide, shootings were down in 2015 about 3 percent, to 1,103 from 1,172 in 2014.\\nShea largely attributed the 2015 increase in rapes to victims coming forward with complaints about attacks from years past.\\nSign up to get the latest updates Get Newsday's Breaking News alerts in your inbox. By clicking Sign up, you agree to our privacy policy.\\n“Twenty percent of these rapes didn’t happen in 2015,” he said.\\nThe NYPD has seen an increase in rapes involving single women who, after a night of drinking, get into cabs of all kinds and are attacked, Shea said.\\n“They get driven, and passing out and waking up in a desolate area, and they get sexually attacked. This is something, really, that people need to be exceptionally aware of, and like any case in New York City, the buddy system works,” said Shea, referring to the need for people to travel in pairs when taking a cab at night.\\nBratton and police brass hope to build upon the continuing drop in overall crime by using technology such as ShotSpotter and a newly minted GPS system for police cars.\\nJessica Tisch, NYPD deputy commissioner for technology, said ShotSpotter, an acoustical system that detects gunfire, identified gunshots in 1,672 cases, mostly in Brooklyn. Of those alerts, 74 percent didn’t have any 911 calls from the public associated with them.\\nTisch said ShotSpotter helped police recover ballistic evidence in 19 percent of the gunfire alerts. In 22 percent of those cases, Tisch said, cops were able to make positive matches of bullets with those from guns used in earlier shootings.\\nTisch also highlighted a special GPS system being tried in about 5,000 patrol cars that allows the NYPD to see where its vehicles are and to track their movements over a 24-hour period, as well as gather information about the officers’ driving.\\n\", 'label': 0, 'input_ids': [101, 1999, 1037, 2095, 2073, 18268, 2015, 1010, 9040, 2015, 1998, 27307, 3111, 3445, 3621, 1010, 2047, 2259, 2103, 2145, 2387, 3809, 4126, 4530, 1015, 1012, 1021, 3867, 1999, 2325, 1010, 5719, 2019, 3452, 6689, 2008, 2211, 1999, 1996, 4134, 1010, 6396, 17299, 5849, 2520, 28557, 2669, 2056, 6928, 1012, 2012, 1037, 2739, 3034, 2007, 3664, 3021, 2139, 1038, 8523, 3695, 1010, 28557, 2669, 2000, 12926, 2197, 2095, 1521, 1055, 4126, 6747, 1010, 2029, 2002, 2056, 1010, 2043, 4117, 2007, 2019, 2130, 3469, 6689, 1999, 2297, 1010, 2404, 2000, 2717, 1996, 3571, 2008, 6937, 17913, 2481, 1521, 1056, 3613, 2104, 1996, 2047, 3447, 2012, 2103, 2534, 1012, 1523, 2096, 2057, 2031, 2018, 2070, 19857, 6593, 14505, 1010, 2070, 7457, 1999, 3056, 7236, 1010, 1996, 3452, 9874, 1999, 2035, 2256, 4126, 7236, 4247, 2000, 2175, 2091, 1010, 1524, 28557, 2669, 2409, 12060, 1012, 1523, 2009, 2001, 1037, 2200, 2204, 2095, 2005, 2149, 1010, 2325, 1012, 18268, 2015, 3445, 2011, 1018, 1012, 1019, 3867, 1999, 2325, 1010, 4803, 2000, 8698, 2013, 21211, 1999, 1996, 3188, 2095, 1010, 2029, 2001, 1996, 7290, 2144, 2807, 1010, 2056, 4112, 5849, 4315, 18938, 16994, 1012, 9040, 2015, 3445, 1020, 3867, 1998, 27307, 3111, 3123, 1016, 3867, 1010, 2056, 16994, 1010, 2040, 2003, 1999, 3715, 1997, 2951, 3074, 1998, 3136, 2005, 1996, 6396, 17299, 1012, 1996, 2896, 3452, 4126, 6747, 2234, 2055, 2349, 2000, 2054, 16994, 2170, 1523, 9416, 7285, 1010, 1524, 2073, 10558, 2191, 3737, 17615, 2130, 2295, 1996, 3452, 2193, 1997, 25809, 2015, 2001, 1996, 7290, 1999, 1996, 2103, 2144, 2494, 1012, 2048, 21413, 1517, 7128, 1998, 1996, 14487, 1517, 2941, 2387, 3809, 6997, 3623, 2011, 1017, 3867, 1998, 1018, 3867, 1010, 4414, 1010, 16994, 2056, 1012, 7128, 1521, 1055, 3623, 2001, 5533, 2011, 2062, 27307, 3111, 1010, 2096, 1996, 14487, 1010, 2348, 3773, 2019, 3452, 4126, 3623, 1010, 2018, 2054, 2002, 2056, 2001, 1037, 1523, 13352, 2140, 1524, 7312, 1999, 5008, 2015, 1012, 2103, 22517, 1010, 5008, 2015, 2020, 2091, 1999, 2325, 2055, 1017, 3867, 1010, 2000, 1015, 1010, 9800, 2013, 1015, 1010, 18253, 1999, 2297, 1012, 16994, 4321, 7108, 1996, 2325, 3623, 1999, 9040, 2015, 2000, 5694, 2746, 2830, 2007, 10821, 2055, 4491, 2013, 2086, 2627, 1012, 3696, 2039, 2000, 2131, 1996, 6745, 14409, 2131, 2739, 10259, 1005, 1055, 4911, 2739, 9499, 2015, 1999, 2115, 1999, 8758, 1012, 2011, 22042, 3696, 2039, 1010, 2017, 5993, 2000, 2256, 9394, 3343, 1012, 1523, 3174, 3867, 1997, 2122, 9040, 2015, 2134, 1521, 1056, 4148, 1999, 2325, 1010, 1524, 2002, 2056, 1012, 1996, 6396, 17299, 2038, 2464, 2019, 3623, 1999, 9040, 2015, 5994, 2309, 2308, 2040, 1010, 2044, 1037, 2305, 1997, 5948, 1010, 2131, 2046, 9298, 2015, 1997, 2035, 7957, 1998, 2024, 4457, 1010, 16994, 2056, 1012, 1523, 2027, 2131, 5533, 1010, 1998, 4458, 2041, 1998, 12447, 2039, 1999, 1037, 4078, 19425, 2181, 1010, 1998, 2027, 2131, 12581, 4457, 1012, 2023, 2003, 2242, 1010, 2428, 1010, 2008, 2111, 2342, 2000, 2022, 17077, 5204, 1997, 1010, 1998, 2066, 2151, 2553, 1999, 2047, 2259, 2103, 1010, 1996, 8937, 2291, 2573, 1010, 1524, 2056, 16994, 1010, 7727, 2000, 1996, 2342, 2005, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "first_record = tokenized_train_ds[0]\n",
    "print(first_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a991f2e-0532-4424-90d4-bda2758ec289",
   "metadata": {},
   "source": [
    "### Turn on accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17623651-b9c0-4dbe-bbd1-07bb51eda8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef357cf6-2074-4ad9-892d-f2ce93221e16",
   "metadata": {},
   "source": [
    "### Look at hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9848d5f-f539-456c-a110-72ac08530f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 7\n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "GPU 1: NVIDIA GeForce RTX 4090\n",
      "GPU 2: NVIDIA GeForce RTX 4090\n",
      "GPU 3: NVIDIA GeForce RTX 3090 Ti\n",
      "GPU 4: NVIDIA GeForce RTX 3090 Ti\n",
      "GPU 5: NVIDIA GeForce RTX 3090\n",
      "GPU 6: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b847246a-83e8-4982-a6c8-ba549f8e38a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 22 01:47:43 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off |   00000000:01:00.0 Off |                  N/A |\n",
      "| 31%   33C    P8             35W /  420W |      13MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        Off |   00000000:02:00.0 Off |                  Off |\n",
      "|  0%   45C    P8             24W /  450W |     112MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 3090 Ti     Off |   00000000:2A:00.0 Off |                  Off |\n",
      "| 30%   37C    P8             24W /  450W |      13MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 3090        Off |   00000000:41:00.0 Off |                  N/A |\n",
      "| 32%   32C    P8             31W /  420W |      13MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA GeForce RTX 4090        Off |   00000000:42:00.0 Off |                  Off |\n",
      "|  0%   50C    P8             22W /  450W |      14MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA GeForce RTX 4090        Off |   00000000:61:00.0 Off |                  Off |\n",
      "|  0%   40C    P8             11W /  450W |      14MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA GeForce RTX 3090 Ti     Off |   00000000:62:00.0 Off |                  Off |\n",
      "| 30%   35C    P8             25W /  450W |      13MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   1026070      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    1   N/A  N/A   1026070      G   /usr/lib/xorg/Xorg                             86MiB |\n",
      "|    1   N/A  N/A   1026178      G   /usr/bin/gnome-shell                           13MiB |\n",
      "|    2   N/A  N/A   1026070      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    3   N/A  N/A   1026070      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    4   N/A  N/A   1026070      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    5   N/A  N/A   1026070      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    6   N/A  N/A   1026070      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3fabb987-30a6-47bf-8048-b76477a8b070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(logits_and_labels):\n",
    "    logits, labels = logits_and_labels\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='macro')\n",
    "    return {'accuracy': acc, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69b144bb-2c85-429c-8643-3d98a423613d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./praxis-bert-base-uncased-small-finetune'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_name = \"praxis-\"+model_name+\"-small-finetune\"\n",
    "output_dir_path = \"./\" + project_name\n",
    "output_dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4b7cb69-8dbe-457e-9449-1e81323374ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a71a13f-2748-4216-b0aa-8200d7fc45d7",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "306b4000-44a5-4b1c-956e-8f86ee3f280d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 01:47:44.332861: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-22 01:47:44.869342: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnispoe\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nispoe/kuk/Praxis/wandb/run-20240522_014746-644mz1f2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nispoe/huggingface/runs/644mz1f2' target=\"_blank\">playful-snowflake-291</a></strong> to <a href='https://wandb.ai/nispoe/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nispoe/huggingface' target=\"_blank\">https://wandb.ai/nispoe/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nispoe/huggingface/runs/644mz1f2' target=\"_blank\">https://wandb.ai/nispoe/huggingface/runs/644mz1f2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6010' max='6010' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6010/6010 53:52, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.441600</td>\n",
       "      <td>0.484352</td>\n",
       "      <td>0.765098</td>\n",
       "      <td>0.764558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.288600</td>\n",
       "      <td>0.383758</td>\n",
       "      <td>0.827572</td>\n",
       "      <td>0.824655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.177800</td>\n",
       "      <td>0.390909</td>\n",
       "      <td>0.855060</td>\n",
       "      <td>0.848444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.043400</td>\n",
       "      <td>0.618340</td>\n",
       "      <td>0.851728</td>\n",
       "      <td>0.843860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.037200</td>\n",
       "      <td>0.941856</td>\n",
       "      <td>0.800916</td>\n",
       "      <td>0.799464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>0.885973</td>\n",
       "      <td>0.842149</td>\n",
       "      <td>0.838166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.036500</td>\n",
       "      <td>1.663635</td>\n",
       "      <td>0.773844</td>\n",
       "      <td>0.773563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>1.017183</td>\n",
       "      <td>0.862696</td>\n",
       "      <td>0.857880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>1.073309</td>\n",
       "      <td>0.860614</td>\n",
       "      <td>0.855655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.087800</td>\n",
       "      <td>1.260292</td>\n",
       "      <td>0.845065</td>\n",
       "      <td>0.841668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6010, training_loss=0.10376425536040751, metrics={'train_runtime': 3237.7132, 'train_samples_per_second': 103.811, 'train_steps_per_second': 1.856, 'total_flos': 8.84342568170496e+16, 'train_loss': 0.10376425536040751, 'epoch': 10.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "transformers.set_seed(777)\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_ds,\n",
    "    eval_dataset=tokenized_eval_ds,\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir_path,\n",
    "        num_train_epochs=10,\n",
    "        logging_steps=10,\n",
    "        logging_dir=output_dir_path+\"/logs\",\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        do_eval=True,\n",
    "    ),\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=None),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model = accelerator.prepare_model(model)\n",
    "trainer.train(resume_from_checkpoint=False)  # Turn to True if power goes out..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9247c1-3c27-4cb0-b4d6-602dcb02488e",
   "metadata": {},
   "source": [
    "### Determine best checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f00a0d4e-adfc-4877-af62-4d84992590dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 44\n",
      "drwxr-xr-x 2 nispoe nispoe 4096 May 22 01:47 logs\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 22 01:53 checkpoint-601\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 22 01:58 checkpoint-1202\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 22 02:04 checkpoint-1803\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 22 02:09 checkpoint-2404\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 22 02:14 checkpoint-3005\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 22 02:20 checkpoint-3606\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 22 02:25 checkpoint-4207\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 22 02:30 checkpoint-4808\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 22 02:36 checkpoint-5409\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 22 02:41 checkpoint-6010\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr {output_dir_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf241b2f-b4ac-47a7-941a-9b66b9f3e440",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 20:21:36.365291: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-04 20:21:36.979274: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading events from file: ./praxis-bert-base-uncased-small-finetune/logs/events.out.tfevents.1716360466.hephaestus.2386955.0\n",
      "WARNING:tensorflow:From /home/nispoe/.local/lib/python3.10/site-packages/tensorflow/python/summary/summary_iterator.py:27: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "Step: 10, train/loss: 0.65829998254776\n",
      "Step: 10, train/grad_norm: 0.8190418481826782\n",
      "Step: 10, train/learning_rate: 4.9916805437533185e-05\n",
      "Step: 10, train/epoch: 0.01663893461227417\n",
      "Step: 20, train/loss: 0.6480000019073486\n",
      "Step: 20, train/grad_norm: 0.8293776512145996\n",
      "Step: 20, train/learning_rate: 4.983361213817261e-05\n",
      "Step: 20, train/epoch: 0.03327786922454834\n",
      "Step: 30, train/loss: 0.6549000144004822\n",
      "Step: 30, train/grad_norm: 1.3691949844360352\n",
      "Step: 30, train/learning_rate: 4.975041520083323e-05\n",
      "Step: 30, train/epoch: 0.04991680383682251\n",
      "Step: 40, train/loss: 0.6492999792098999\n",
      "Step: 40, train/grad_norm: 1.8871378898620605\n",
      "Step: 40, train/learning_rate: 4.966722190147266e-05\n",
      "Step: 40, train/epoch: 0.06655573844909668\n",
      "Step: 50, train/loss: 0.6794000267982483\n",
      "Step: 50, train/grad_norm: 4.625670909881592\n",
      "Step: 50, train/learning_rate: 4.958402496413328e-05\n",
      "Step: 50, train/epoch: 0.08319467306137085\n",
      "Step: 60, train/loss: 0.630299985408783\n",
      "Step: 60, train/grad_norm: 3.439519166946411\n",
      "Step: 60, train/learning_rate: 4.9500831664772704e-05\n",
      "Step: 60, train/epoch: 0.09983360767364502\n",
      "Step: 70, train/loss: 0.5828999876976013\n",
      "Step: 70, train/grad_norm: 2.5344181060791016\n",
      "Step: 70, train/learning_rate: 4.941763836541213e-05\n",
      "Step: 70, train/epoch: 0.11647254228591919\n",
      "Step: 80, train/loss: 0.5622000098228455\n",
      "Step: 80, train/grad_norm: 6.395686149597168\n",
      "Step: 80, train/learning_rate: 4.933444142807275e-05\n",
      "Step: 80, train/epoch: 0.13311147689819336\n",
      "Step: 90, train/loss: 0.6022999882698059\n",
      "Step: 90, train/grad_norm: 4.66585111618042\n",
      "Step: 90, train/learning_rate: 4.925124812871218e-05\n",
      "Step: 90, train/epoch: 0.14975041151046753\n",
      "Step: 100, train/loss: 0.5791000127792358\n",
      "Step: 100, train/grad_norm: 2.198995351791382\n",
      "Step: 100, train/learning_rate: 4.9168054829351604e-05\n",
      "Step: 100, train/epoch: 0.1663893461227417\n",
      "Step: 110, train/loss: 0.5669000148773193\n",
      "Step: 110, train/grad_norm: 2.903641939163208\n",
      "Step: 110, train/learning_rate: 4.9084857892012224e-05\n",
      "Step: 110, train/epoch: 0.18302828073501587\n",
      "Step: 120, train/loss: 0.5537999868392944\n",
      "Step: 120, train/grad_norm: 3.388364791870117\n",
      "Step: 120, train/learning_rate: 4.900166459265165e-05\n",
      "Step: 120, train/epoch: 0.19966721534729004\n",
      "Step: 130, train/loss: 0.5444999933242798\n",
      "Step: 130, train/grad_norm: 2.0415546894073486\n",
      "Step: 130, train/learning_rate: 4.891846765531227e-05\n",
      "Step: 130, train/epoch: 0.2163061499595642\n",
      "Step: 140, train/loss: 0.4691999852657318\n",
      "Step: 140, train/grad_norm: 4.702225208282471\n",
      "Step: 140, train/learning_rate: 4.88352743559517e-05\n",
      "Step: 140, train/epoch: 0.23294508457183838\n",
      "Step: 150, train/loss: 0.5784000158309937\n",
      "Step: 150, train/grad_norm: 2.7871739864349365\n",
      "Step: 150, train/learning_rate: 4.875208105659112e-05\n",
      "Step: 150, train/epoch: 0.24958401918411255\n",
      "Step: 160, train/loss: 0.5041999816894531\n",
      "Step: 160, train/grad_norm: 5.8950042724609375\n",
      "Step: 160, train/learning_rate: 4.866888411925174e-05\n",
      "Step: 160, train/epoch: 0.2662229537963867\n",
      "Step: 170, train/loss: 0.498199999332428\n",
      "Step: 170, train/grad_norm: 4.288083076477051\n",
      "Step: 170, train/learning_rate: 4.858569081989117e-05\n",
      "Step: 170, train/epoch: 0.2828618884086609\n",
      "Step: 180, train/loss: 0.5060999989509583\n",
      "Step: 180, train/grad_norm: 6.531476974487305\n",
      "Step: 180, train/learning_rate: 4.8502497520530596e-05\n",
      "Step: 180, train/epoch: 0.29950082302093506\n",
      "Step: 190, train/loss: 0.4296000003814697\n",
      "Step: 190, train/grad_norm: 4.290502071380615\n",
      "Step: 190, train/learning_rate: 4.8419300583191216e-05\n",
      "Step: 190, train/epoch: 0.31613975763320923\n",
      "Step: 200, train/loss: 0.4943000078201294\n",
      "Step: 200, train/grad_norm: 2.4411935806274414\n",
      "Step: 200, train/learning_rate: 4.833610728383064e-05\n",
      "Step: 200, train/epoch: 0.3327786922454834\n",
      "Step: 210, train/loss: 0.46810001134872437\n",
      "Step: 210, train/grad_norm: 2.9940192699432373\n",
      "Step: 210, train/learning_rate: 4.825291034649126e-05\n",
      "Step: 210, train/epoch: 0.34941762685775757\n",
      "Step: 220, train/loss: 0.5199000239372253\n",
      "Step: 220, train/grad_norm: 11.600845336914062\n",
      "Step: 220, train/learning_rate: 4.816971704713069e-05\n",
      "Step: 220, train/epoch: 0.36605656147003174\n",
      "Step: 230, train/loss: 0.5108000040054321\n",
      "Step: 230, train/grad_norm: 9.390843391418457\n",
      "Step: 230, train/learning_rate: 4.8086523747770116e-05\n",
      "Step: 230, train/epoch: 0.3826954960823059\n",
      "Step: 240, train/loss: 0.44769999384880066\n",
      "Step: 240, train/grad_norm: 3.704559803009033\n",
      "Step: 240, train/learning_rate: 4.8003326810430735e-05\n",
      "Step: 240, train/epoch: 0.3993344306945801\n",
      "Step: 250, train/loss: 0.4551999866962433\n",
      "Step: 250, train/grad_norm: 3.5692994594573975\n",
      "Step: 250, train/learning_rate: 4.792013351107016e-05\n",
      "Step: 250, train/epoch: 0.41597336530685425\n",
      "Step: 260, train/loss: 0.45669999718666077\n",
      "Step: 260, train/grad_norm: 3.9626522064208984\n",
      "Step: 260, train/learning_rate: 4.783694021170959e-05\n",
      "Step: 260, train/epoch: 0.4326122999191284\n",
      "Step: 270, train/loss: 0.47279998660087585\n",
      "Step: 270, train/grad_norm: 3.024169921875\n",
      "Step: 270, train/learning_rate: 4.775374327437021e-05\n",
      "Step: 270, train/epoch: 0.4492512345314026\n",
      "Step: 280, train/loss: 0.43149998784065247\n",
      "Step: 280, train/grad_norm: 8.34223461151123\n",
      "Step: 280, train/learning_rate: 4.7670549975009635e-05\n",
      "Step: 280, train/epoch: 0.46589016914367676\n",
      "Step: 290, train/loss: 0.4336000084877014\n",
      "Step: 290, train/grad_norm: 10.384108543395996\n",
      "Step: 290, train/learning_rate: 4.7587353037670255e-05\n",
      "Step: 290, train/epoch: 0.4825291037559509\n",
      "Step: 300, train/loss: 0.4291999936103821\n",
      "Step: 300, train/grad_norm: 4.4081573486328125\n",
      "Step: 300, train/learning_rate: 4.750415973830968e-05\n",
      "Step: 300, train/epoch: 0.4991680383682251\n",
      "Step: 310, train/loss: 0.44679999351501465\n",
      "Step: 310, train/grad_norm: 11.369922637939453\n",
      "Step: 310, train/learning_rate: 4.742096643894911e-05\n",
      "Step: 310, train/epoch: 0.5158069729804993\n",
      "Step: 320, train/loss: 0.4738999903202057\n",
      "Step: 320, train/grad_norm: 5.099761962890625\n",
      "Step: 320, train/learning_rate: 4.733776950160973e-05\n",
      "Step: 320, train/epoch: 0.5324459075927734\n",
      "Step: 330, train/loss: 0.39329999685287476\n",
      "Step: 330, train/grad_norm: 4.19200325012207\n",
      "Step: 330, train/learning_rate: 4.7254576202249154e-05\n",
      "Step: 330, train/epoch: 0.5490848422050476\n",
      "Step: 340, train/loss: 0.4244999885559082\n",
      "Step: 340, train/grad_norm: 7.565423011779785\n",
      "Step: 340, train/learning_rate: 4.7171379264909774e-05\n",
      "Step: 340, train/epoch: 0.5657237768173218\n",
      "Step: 350, train/loss: 0.4223000109195709\n",
      "Step: 350, train/grad_norm: 8.980240821838379\n",
      "Step: 350, train/learning_rate: 4.70881859655492e-05\n",
      "Step: 350, train/epoch: 0.582362711429596\n",
      "Step: 360, train/loss: 0.4361000061035156\n",
      "Step: 360, train/grad_norm: 6.5364508628845215\n",
      "Step: 360, train/learning_rate: 4.700499266618863e-05\n",
      "Step: 360, train/epoch: 0.5990016460418701\n",
      "Step: 370, train/loss: 0.4896000027656555\n",
      "Step: 370, train/grad_norm: 6.5464558601379395\n",
      "Step: 370, train/learning_rate: 4.692179572884925e-05\n",
      "Step: 370, train/epoch: 0.6156405806541443\n",
      "Step: 380, train/loss: 0.43630000948905945\n",
      "Step: 380, train/grad_norm: 8.148404121398926\n",
      "Step: 380, train/learning_rate: 4.6838602429488674e-05\n",
      "Step: 380, train/epoch: 0.6322795152664185\n",
      "Step: 390, train/loss: 0.4187999963760376\n",
      "Step: 390, train/grad_norm: 5.969727039337158\n",
      "Step: 390, train/learning_rate: 4.67554091301281e-05\n",
      "Step: 390, train/epoch: 0.6489184498786926\n",
      "Step: 400, train/loss: 0.4316999912261963\n",
      "Step: 400, train/grad_norm: 4.035233974456787\n",
      "Step: 400, train/learning_rate: 4.667221219278872e-05\n",
      "Step: 400, train/epoch: 0.6655573844909668\n",
      "Step: 410, train/loss: 0.43299999833106995\n",
      "Step: 410, train/grad_norm: 2.4523608684539795\n",
      "Step: 410, train/learning_rate: 4.658901889342815e-05\n",
      "Step: 410, train/epoch: 0.682196319103241\n",
      "Step: 420, train/loss: 0.4090999960899353\n",
      "Step: 420, train/grad_norm: 6.244604587554932\n",
      "Step: 420, train/learning_rate: 4.6505821956088766e-05\n",
      "Step: 420, train/epoch: 0.6988352537155151\n",
      "Step: 430, train/loss: 0.4715999960899353\n",
      "Step: 430, train/grad_norm: 6.928226470947266\n",
      "Step: 430, train/learning_rate: 4.642262865672819e-05\n",
      "Step: 430, train/epoch: 0.7154741883277893\n",
      "Step: 440, train/loss: 0.41110000014305115\n",
      "Step: 440, train/grad_norm: 8.935722351074219\n",
      "Step: 440, train/learning_rate: 4.633943535736762e-05\n",
      "Step: 440, train/epoch: 0.7321131229400635\n",
      "Step: 450, train/loss: 0.4961000084877014\n",
      "Step: 450, train/grad_norm: 3.9726243019104004\n",
      "Step: 450, train/learning_rate: 4.625623842002824e-05\n",
      "Step: 450, train/epoch: 0.7487520575523376\n",
      "Step: 460, train/loss: 0.45320001244544983\n",
      "Step: 460, train/grad_norm: 2.4679315090179443\n",
      "Step: 460, train/learning_rate: 4.6173045120667666e-05\n",
      "Step: 460, train/epoch: 0.7653909921646118\n",
      "Step: 470, train/loss: 0.4180999994277954\n",
      "Step: 470, train/grad_norm: 11.405202865600586\n",
      "Step: 470, train/learning_rate: 4.608985182130709e-05\n",
      "Step: 470, train/epoch: 0.782029926776886\n",
      "Step: 480, train/loss: 0.3749000132083893\n",
      "Step: 480, train/grad_norm: 12.755825996398926\n",
      "Step: 480, train/learning_rate: 4.600665488396771e-05\n",
      "Step: 480, train/epoch: 0.7986688613891602\n",
      "Step: 490, train/loss: 0.41929998993873596\n",
      "Step: 490, train/grad_norm: 3.6342809200286865\n",
      "Step: 490, train/learning_rate: 4.592346158460714e-05\n",
      "Step: 490, train/epoch: 0.8153077960014343\n",
      "Step: 500, train/loss: 0.453900009393692\n",
      "Step: 500, train/grad_norm: 9.357511520385742\n",
      "Step: 500, train/learning_rate: 4.584026464726776e-05\n",
      "Step: 500, train/epoch: 0.8319467306137085\n",
      "Step: 510, train/loss: 0.4553999900817871\n",
      "Step: 510, train/grad_norm: 4.152285099029541\n",
      "Step: 510, train/learning_rate: 4.5757071347907186e-05\n",
      "Step: 510, train/epoch: 0.8485856652259827\n",
      "Step: 520, train/loss: 0.3237000107765198\n",
      "Step: 520, train/grad_norm: 5.803722858428955\n",
      "Step: 520, train/learning_rate: 4.567387804854661e-05\n",
      "Step: 520, train/epoch: 0.8652245998382568\n",
      "Step: 530, train/loss: 0.40310001373291016\n",
      "Step: 530, train/grad_norm: 6.3072943687438965\n",
      "Step: 530, train/learning_rate: 4.559068111120723e-05\n",
      "Step: 530, train/epoch: 0.881863534450531\n",
      "Step: 540, train/loss: 0.40070000290870667\n",
      "Step: 540, train/grad_norm: 5.2784295082092285\n",
      "Step: 540, train/learning_rate: 4.550748781184666e-05\n",
      "Step: 540, train/epoch: 0.8985024690628052\n",
      "Step: 550, train/loss: 0.4652999937534332\n",
      "Step: 550, train/grad_norm: 6.43049955368042\n",
      "Step: 550, train/learning_rate: 4.5424294512486085e-05\n",
      "Step: 550, train/epoch: 0.9151414036750793\n",
      "Step: 560, train/loss: 0.45179998874664307\n",
      "Step: 560, train/grad_norm: 3.2263009548187256\n",
      "Step: 560, train/learning_rate: 4.5341097575146705e-05\n",
      "Step: 560, train/epoch: 0.9317803382873535\n",
      "Step: 570, train/loss: 0.4180000126361847\n",
      "Step: 570, train/grad_norm: 10.02290153503418\n",
      "Step: 570, train/learning_rate: 4.525790427578613e-05\n",
      "Step: 570, train/epoch: 0.9484192728996277\n",
      "Step: 580, train/loss: 0.42320001125335693\n",
      "Step: 580, train/grad_norm: 11.368558883666992\n",
      "Step: 580, train/learning_rate: 4.517470733844675e-05\n",
      "Step: 580, train/epoch: 0.9650582075119019\n",
      "Step: 590, train/loss: 0.43320000171661377\n",
      "Step: 590, train/grad_norm: 7.874508857727051\n",
      "Step: 590, train/learning_rate: 4.509151403908618e-05\n",
      "Step: 590, train/epoch: 0.981697142124176\n",
      "Step: 600, train/loss: 0.4415999948978424\n",
      "Step: 600, train/grad_norm: 4.985068321228027\n",
      "Step: 600, train/learning_rate: 4.5008320739725605e-05\n",
      "Step: 600, train/epoch: 0.9983360767364502\n",
      "Step: 601, eval/loss: 0.4843520224094391\n",
      "Step: 601, eval/accuracy: 0.7650978565216064\n",
      "Step: 601, eval/f1: 0.7645577788352966\n",
      "Step: 601, eval/runtime: 29.16830062866211\n",
      "Step: 601, eval/samples_per_second: 246.9459991455078\n",
      "Step: 601, eval/steps_per_second: 4.422999858856201\n",
      "Step: 601, train/epoch: 1.0\n",
      "Step: 610, train/loss: 0.3043000102043152\n",
      "Step: 610, train/grad_norm: 8.47378921508789\n",
      "Step: 610, train/learning_rate: 4.4925123802386224e-05\n",
      "Step: 610, train/epoch: 1.0149750709533691\n",
      "Step: 620, train/loss: 0.28439998626708984\n",
      "Step: 620, train/grad_norm: 5.401245594024658\n",
      "Step: 620, train/learning_rate: 4.484193050302565e-05\n",
      "Step: 620, train/epoch: 1.0316139459609985\n",
      "Step: 630, train/loss: 0.2401999980211258\n",
      "Step: 630, train/grad_norm: 5.658929824829102\n",
      "Step: 630, train/learning_rate: 4.475873720366508e-05\n",
      "Step: 630, train/epoch: 1.0482529401779175\n",
      "Step: 640, train/loss: 0.19020000100135803\n",
      "Step: 640, train/grad_norm: 6.249221324920654\n",
      "Step: 640, train/learning_rate: 4.46755402663257e-05\n",
      "Step: 640, train/epoch: 1.0648918151855469\n",
      "Step: 650, train/loss: 0.29760000109672546\n",
      "Step: 650, train/grad_norm: 7.286983489990234\n",
      "Step: 650, train/learning_rate: 4.4592346966965124e-05\n",
      "Step: 650, train/epoch: 1.0815308094024658\n",
      "Step: 660, train/loss: 0.27970001101493835\n",
      "Step: 660, train/grad_norm: 3.230241060256958\n",
      "Step: 660, train/learning_rate: 4.4509150029625744e-05\n",
      "Step: 660, train/epoch: 1.0981696844100952\n",
      "Step: 670, train/loss: 0.265500009059906\n",
      "Step: 670, train/grad_norm: 3.6349616050720215\n",
      "Step: 670, train/learning_rate: 4.442595673026517e-05\n",
      "Step: 670, train/epoch: 1.1148086786270142\n",
      "Step: 680, train/loss: 0.271699994802475\n",
      "Step: 680, train/grad_norm: 5.940262317657471\n",
      "Step: 680, train/learning_rate: 4.43427634309046e-05\n",
      "Step: 680, train/epoch: 1.1314475536346436\n",
      "Step: 690, train/loss: 0.2639999985694885\n",
      "Step: 690, train/grad_norm: 18.71757698059082\n",
      "Step: 690, train/learning_rate: 4.425956649356522e-05\n",
      "Step: 690, train/epoch: 1.1480865478515625\n",
      "Step: 700, train/loss: 0.2460000067949295\n",
      "Step: 700, train/grad_norm: 5.8361053466796875\n",
      "Step: 700, train/learning_rate: 4.417637319420464e-05\n",
      "Step: 700, train/epoch: 1.164725422859192\n",
      "Step: 710, train/loss: 0.2761000096797943\n",
      "Step: 710, train/grad_norm: 20.060483932495117\n",
      "Step: 710, train/learning_rate: 4.409317625686526e-05\n",
      "Step: 710, train/epoch: 1.1813644170761108\n",
      "Step: 720, train/loss: 0.25780001282691956\n",
      "Step: 720, train/grad_norm: 15.117937088012695\n",
      "Step: 720, train/learning_rate: 4.400998295750469e-05\n",
      "Step: 720, train/epoch: 1.1980032920837402\n",
      "Step: 730, train/loss: 0.2630999982357025\n",
      "Step: 730, train/grad_norm: 3.7613272666931152\n",
      "Step: 730, train/learning_rate: 4.3926789658144116e-05\n",
      "Step: 730, train/epoch: 1.2146422863006592\n",
      "Step: 740, train/loss: 0.23720000684261322\n",
      "Step: 740, train/grad_norm: 10.588419914245605\n",
      "Step: 740, train/learning_rate: 4.3843592720804736e-05\n",
      "Step: 740, train/epoch: 1.2312811613082886\n",
      "Step: 750, train/loss: 0.29339998960494995\n",
      "Step: 750, train/grad_norm: 6.913580894470215\n",
      "Step: 750, train/learning_rate: 4.376039942144416e-05\n",
      "Step: 750, train/epoch: 1.2479201555252075\n",
      "Step: 760, train/loss: 0.2680000066757202\n",
      "Step: 760, train/grad_norm: 11.98587703704834\n",
      "Step: 760, train/learning_rate: 4.367720612208359e-05\n",
      "Step: 760, train/epoch: 1.264559030532837\n",
      "Step: 770, train/loss: 0.25029999017715454\n",
      "Step: 770, train/grad_norm: 6.309391498565674\n",
      "Step: 770, train/learning_rate: 4.359400918474421e-05\n",
      "Step: 770, train/epoch: 1.2811980247497559\n",
      "Step: 780, train/loss: 0.2743000090122223\n",
      "Step: 780, train/grad_norm: 3.4880621433258057\n",
      "Step: 780, train/learning_rate: 4.3510815885383636e-05\n",
      "Step: 780, train/epoch: 1.2978368997573853\n",
      "Step: 790, train/loss: 0.23330000042915344\n",
      "Step: 790, train/grad_norm: 3.4442360401153564\n",
      "Step: 790, train/learning_rate: 4.3427618948044255e-05\n",
      "Step: 790, train/epoch: 1.3144758939743042\n",
      "Step: 800, train/loss: 0.29330000281333923\n",
      "Step: 800, train/grad_norm: 8.102633476257324\n",
      "Step: 800, train/learning_rate: 4.334442564868368e-05\n",
      "Step: 800, train/epoch: 1.3311147689819336\n",
      "Step: 810, train/loss: 0.24310000240802765\n",
      "Step: 810, train/grad_norm: 11.883820533752441\n",
      "Step: 810, train/learning_rate: 4.326123234932311e-05\n",
      "Step: 810, train/epoch: 1.3477537631988525\n",
      "Step: 820, train/loss: 0.24729999899864197\n",
      "Step: 820, train/grad_norm: 8.050459861755371\n",
      "Step: 820, train/learning_rate: 4.317803541198373e-05\n",
      "Step: 820, train/epoch: 1.364392638206482\n",
      "Step: 830, train/loss: 0.2621999979019165\n",
      "Step: 830, train/grad_norm: 6.911316871643066\n",
      "Step: 830, train/learning_rate: 4.3094842112623155e-05\n",
      "Step: 830, train/epoch: 1.3810316324234009\n",
      "Step: 840, train/loss: 0.2433999925851822\n",
      "Step: 840, train/grad_norm: 3.93637752532959\n",
      "Step: 840, train/learning_rate: 4.301164881326258e-05\n",
      "Step: 840, train/epoch: 1.3976705074310303\n",
      "Step: 850, train/loss: 0.2289000004529953\n",
      "Step: 850, train/grad_norm: 12.931926727294922\n",
      "Step: 850, train/learning_rate: 4.29284518759232e-05\n",
      "Step: 850, train/epoch: 1.4143095016479492\n",
      "Step: 860, train/loss: 0.2989000082015991\n",
      "Step: 860, train/grad_norm: 6.6558051109313965\n",
      "Step: 860, train/learning_rate: 4.284525857656263e-05\n",
      "Step: 860, train/epoch: 1.4309483766555786\n",
      "Step: 870, train/loss: 0.24529999494552612\n",
      "Step: 870, train/grad_norm: 11.347943305969238\n",
      "Step: 870, train/learning_rate: 4.276206163922325e-05\n",
      "Step: 870, train/epoch: 1.4475873708724976\n",
      "Step: 880, train/loss: 0.23549999296665192\n",
      "Step: 880, train/grad_norm: 11.598960876464844\n",
      "Step: 880, train/learning_rate: 4.2678868339862674e-05\n",
      "Step: 880, train/epoch: 1.464226245880127\n",
      "Step: 890, train/loss: 0.2298000007867813\n",
      "Step: 890, train/grad_norm: 9.348592758178711\n",
      "Step: 890, train/learning_rate: 4.25956750405021e-05\n",
      "Step: 890, train/epoch: 1.480865240097046\n",
      "Step: 900, train/loss: 0.2736000120639801\n",
      "Step: 900, train/grad_norm: 12.069742202758789\n",
      "Step: 900, train/learning_rate: 4.251247810316272e-05\n",
      "Step: 900, train/epoch: 1.4975041151046753\n",
      "Step: 910, train/loss: 0.2971999943256378\n",
      "Step: 910, train/grad_norm: 13.658381462097168\n",
      "Step: 910, train/learning_rate: 4.242928480380215e-05\n",
      "Step: 910, train/epoch: 1.5141431093215942\n",
      "Step: 920, train/loss: 0.28999999165534973\n",
      "Step: 920, train/grad_norm: 17.40813446044922\n",
      "Step: 920, train/learning_rate: 4.2346091504441574e-05\n",
      "Step: 920, train/epoch: 1.5307819843292236\n",
      "Step: 930, train/loss: 0.3264999985694885\n",
      "Step: 930, train/grad_norm: 8.276775360107422\n",
      "Step: 930, train/learning_rate: 4.2262894567102194e-05\n",
      "Step: 930, train/epoch: 1.5474209785461426\n",
      "Step: 940, train/loss: 0.26170000433921814\n",
      "Step: 940, train/grad_norm: 6.148312568664551\n",
      "Step: 940, train/learning_rate: 4.217970126774162e-05\n",
      "Step: 940, train/epoch: 1.564059853553772\n",
      "Step: 950, train/loss: 0.275299996137619\n",
      "Step: 950, train/grad_norm: 14.440340995788574\n",
      "Step: 950, train/learning_rate: 4.209650433040224e-05\n",
      "Step: 950, train/epoch: 1.580698847770691\n",
      "Step: 960, train/loss: 0.382099986076355\n",
      "Step: 960, train/grad_norm: 4.491091728210449\n",
      "Step: 960, train/learning_rate: 4.201331103104167e-05\n",
      "Step: 960, train/epoch: 1.5973377227783203\n",
      "Step: 970, train/loss: 0.3400999903678894\n",
      "Step: 970, train/grad_norm: 5.127945899963379\n",
      "Step: 970, train/learning_rate: 4.1930117731681094e-05\n",
      "Step: 970, train/epoch: 1.6139767169952393\n",
      "Step: 980, train/loss: 0.2295999974012375\n",
      "Step: 980, train/grad_norm: 12.177291870117188\n",
      "Step: 980, train/learning_rate: 4.184692079434171e-05\n",
      "Step: 980, train/epoch: 1.6306155920028687\n",
      "Step: 990, train/loss: 0.25440001487731934\n",
      "Step: 990, train/grad_norm: 6.096354961395264\n",
      "Step: 990, train/learning_rate: 4.176372749498114e-05\n",
      "Step: 990, train/epoch: 1.6472545862197876\n",
      "Step: 1000, train/loss: 0.23999999463558197\n",
      "Step: 1000, train/grad_norm: 3.788726568222046\n",
      "Step: 1000, train/learning_rate: 4.1680534195620567e-05\n",
      "Step: 1000, train/epoch: 1.663893461227417\n",
      "Step: 1010, train/loss: 0.27549999952316284\n",
      "Step: 1010, train/grad_norm: 14.219772338867188\n",
      "Step: 1010, train/learning_rate: 4.1597337258281186e-05\n",
      "Step: 1010, train/epoch: 1.680532455444336\n",
      "Step: 1020, train/loss: 0.26589998602867126\n",
      "Step: 1020, train/grad_norm: 4.7088398933410645\n",
      "Step: 1020, train/learning_rate: 4.151414395892061e-05\n",
      "Step: 1020, train/epoch: 1.6971713304519653\n",
      "Step: 1030, train/loss: 0.24369999766349792\n",
      "Step: 1030, train/grad_norm: 10.575819969177246\n",
      "Step: 1030, train/learning_rate: 4.143094702158123e-05\n",
      "Step: 1030, train/epoch: 1.7138103246688843\n",
      "Step: 1040, train/loss: 0.28439998626708984\n",
      "Step: 1040, train/grad_norm: 4.186656475067139\n",
      "Step: 1040, train/learning_rate: 4.134775372222066e-05\n",
      "Step: 1040, train/epoch: 1.7304491996765137\n",
      "Step: 1050, train/loss: 0.257099986076355\n",
      "Step: 1050, train/grad_norm: 13.874733924865723\n",
      "Step: 1050, train/learning_rate: 4.1264560422860086e-05\n",
      "Step: 1050, train/epoch: 1.7470881938934326\n",
      "Step: 1060, train/loss: 0.2709999978542328\n",
      "Step: 1060, train/grad_norm: 11.149019241333008\n",
      "Step: 1060, train/learning_rate: 4.1181363485520706e-05\n",
      "Step: 1060, train/epoch: 1.763727068901062\n",
      "Step: 1070, train/loss: 0.2703999876976013\n",
      "Step: 1070, train/grad_norm: 8.557164192199707\n",
      "Step: 1070, train/learning_rate: 4.109817018616013e-05\n",
      "Step: 1070, train/epoch: 1.780366063117981\n",
      "Step: 1080, train/loss: 0.30630001425743103\n",
      "Step: 1080, train/grad_norm: 5.420615196228027\n",
      "Step: 1080, train/learning_rate: 4.101497324882075e-05\n",
      "Step: 1080, train/epoch: 1.7970049381256104\n",
      "Step: 1090, train/loss: 0.2624000012874603\n",
      "Step: 1090, train/grad_norm: 6.703299045562744\n",
      "Step: 1090, train/learning_rate: 4.093177994946018e-05\n",
      "Step: 1090, train/epoch: 1.8136439323425293\n",
      "Step: 1100, train/loss: 0.2906000018119812\n",
      "Step: 1100, train/grad_norm: 6.747497081756592\n",
      "Step: 1100, train/learning_rate: 4.0848586650099605e-05\n",
      "Step: 1100, train/epoch: 1.8302828073501587\n",
      "Step: 1110, train/loss: 0.2296999990940094\n",
      "Step: 1110, train/grad_norm: 6.458169460296631\n",
      "Step: 1110, train/learning_rate: 4.0765389712760225e-05\n",
      "Step: 1110, train/epoch: 1.8469218015670776\n",
      "Step: 1120, train/loss: 0.2603999972343445\n",
      "Step: 1120, train/grad_norm: 6.2216267585754395\n",
      "Step: 1120, train/learning_rate: 4.068219641339965e-05\n",
      "Step: 1120, train/epoch: 1.863560676574707\n",
      "Step: 1130, train/loss: 0.30820000171661377\n",
      "Step: 1130, train/grad_norm: 10.871910095214844\n",
      "Step: 1130, train/learning_rate: 4.059900311403908e-05\n",
      "Step: 1130, train/epoch: 1.880199670791626\n",
      "Step: 1140, train/loss: 0.28540000319480896\n",
      "Step: 1140, train/grad_norm: 4.614161491394043\n",
      "Step: 1140, train/learning_rate: 4.05158061766997e-05\n",
      "Step: 1140, train/epoch: 1.8968385457992554\n",
      "Step: 1150, train/loss: 0.29649999737739563\n",
      "Step: 1150, train/grad_norm: 12.824382781982422\n",
      "Step: 1150, train/learning_rate: 4.0432612877339125e-05\n",
      "Step: 1150, train/epoch: 1.9134775400161743\n",
      "Step: 1160, train/loss: 0.21610000729560852\n",
      "Step: 1160, train/grad_norm: 5.182090759277344\n",
      "Step: 1160, train/learning_rate: 4.0349415939999744e-05\n",
      "Step: 1160, train/epoch: 1.9301164150238037\n",
      "Step: 1170, train/loss: 0.30970001220703125\n",
      "Step: 1170, train/grad_norm: 4.724658012390137\n",
      "Step: 1170, train/learning_rate: 4.026622264063917e-05\n",
      "Step: 1170, train/epoch: 1.9467554092407227\n",
      "Step: 1180, train/loss: 0.23579999804496765\n",
      "Step: 1180, train/grad_norm: 5.088613510131836\n",
      "Step: 1180, train/learning_rate: 4.01830293412786e-05\n",
      "Step: 1180, train/epoch: 1.963394284248352\n",
      "Step: 1190, train/loss: 0.25850000977516174\n",
      "Step: 1190, train/grad_norm: 3.448484182357788\n",
      "Step: 1190, train/learning_rate: 4.009983240393922e-05\n",
      "Step: 1190, train/epoch: 1.980033278465271\n",
      "Step: 1200, train/loss: 0.28859999775886536\n",
      "Step: 1200, train/grad_norm: 5.252344608306885\n",
      "Step: 1200, train/learning_rate: 4.0016639104578644e-05\n",
      "Step: 1200, train/epoch: 1.9966721534729004\n",
      "Step: 1202, eval/loss: 0.3837577998638153\n",
      "Step: 1202, eval/accuracy: 0.8275718688964844\n",
      "Step: 1202, eval/f1: 0.8246551156044006\n",
      "Step: 1202, eval/runtime: 29.065200805664062\n",
      "Step: 1202, eval/samples_per_second: 247.82200622558594\n",
      "Step: 1202, eval/steps_per_second: 4.438000202178955\n",
      "Step: 1202, train/epoch: 2.0\n",
      "Step: 1210, train/loss: 0.1956000030040741\n",
      "Step: 1210, train/grad_norm: 3.1013286113739014\n",
      "Step: 1210, train/learning_rate: 3.993344580521807e-05\n",
      "Step: 1210, train/epoch: 2.0133111476898193\n",
      "Step: 1220, train/loss: 0.07829999923706055\n",
      "Step: 1220, train/grad_norm: 11.027677536010742\n",
      "Step: 1220, train/learning_rate: 3.985024886787869e-05\n",
      "Step: 1220, train/epoch: 2.0299501419067383\n",
      "Step: 1230, train/loss: 0.1306000053882599\n",
      "Step: 1230, train/grad_norm: 27.377849578857422\n",
      "Step: 1230, train/learning_rate: 3.976705556851812e-05\n",
      "Step: 1230, train/epoch: 2.0465891361236572\n",
      "Step: 1240, train/loss: 0.20649999380111694\n",
      "Step: 1240, train/grad_norm: 6.9659271240234375\n",
      "Step: 1240, train/learning_rate: 3.968385863117874e-05\n",
      "Step: 1240, train/epoch: 2.063227891921997\n",
      "Step: 1250, train/loss: 0.13120000064373016\n",
      "Step: 1250, train/grad_norm: 8.108773231506348\n",
      "Step: 1250, train/learning_rate: 3.9600665331818163e-05\n",
      "Step: 1250, train/epoch: 2.079866886138916\n",
      "Step: 1260, train/loss: 0.12690000236034393\n",
      "Step: 1260, train/grad_norm: 6.681667804718018\n",
      "Step: 1260, train/learning_rate: 3.951747203245759e-05\n",
      "Step: 1260, train/epoch: 2.096505880355835\n",
      "Step: 1270, train/loss: 0.14830000698566437\n",
      "Step: 1270, train/grad_norm: 9.554360389709473\n",
      "Step: 1270, train/learning_rate: 3.943427509511821e-05\n",
      "Step: 1270, train/epoch: 2.113144874572754\n",
      "Step: 1280, train/loss: 0.1062999963760376\n",
      "Step: 1280, train/grad_norm: 5.04857063293457\n",
      "Step: 1280, train/learning_rate: 3.9351081795757636e-05\n",
      "Step: 1280, train/epoch: 2.1297836303710938\n",
      "Step: 1290, train/loss: 0.10159999877214432\n",
      "Step: 1290, train/grad_norm: 4.479731559753418\n",
      "Step: 1290, train/learning_rate: 3.926788849639706e-05\n",
      "Step: 1290, train/epoch: 2.1464226245880127\n",
      "Step: 1300, train/loss: 0.066600002348423\n",
      "Step: 1300, train/grad_norm: 1.6420552730560303\n",
      "Step: 1300, train/learning_rate: 3.918469155905768e-05\n",
      "Step: 1300, train/epoch: 2.1630616188049316\n",
      "Step: 1310, train/loss: 0.09769999980926514\n",
      "Step: 1310, train/grad_norm: 17.445348739624023\n",
      "Step: 1310, train/learning_rate: 3.910149825969711e-05\n",
      "Step: 1310, train/epoch: 2.1797006130218506\n",
      "Step: 1320, train/loss: 0.125\n",
      "Step: 1320, train/grad_norm: 10.603901863098145\n",
      "Step: 1320, train/learning_rate: 3.901830132235773e-05\n",
      "Step: 1320, train/epoch: 2.1963393688201904\n",
      "Step: 1330, train/loss: 0.12999999523162842\n",
      "Step: 1330, train/grad_norm: 9.092986106872559\n",
      "Step: 1330, train/learning_rate: 3.8935108022997156e-05\n",
      "Step: 1330, train/epoch: 2.2129783630371094\n",
      "Step: 1340, train/loss: 0.12540000677108765\n",
      "Step: 1340, train/grad_norm: 6.170156955718994\n",
      "Step: 1340, train/learning_rate: 3.885191472363658e-05\n",
      "Step: 1340, train/epoch: 2.2296173572540283\n",
      "Step: 1350, train/loss: 0.1420000046491623\n",
      "Step: 1350, train/grad_norm: 9.61954402923584\n",
      "Step: 1350, train/learning_rate: 3.87687177862972e-05\n",
      "Step: 1350, train/epoch: 2.2462563514709473\n",
      "Step: 1360, train/loss: 0.09929999709129333\n",
      "Step: 1360, train/grad_norm: 6.976644992828369\n",
      "Step: 1360, train/learning_rate: 3.868552448693663e-05\n",
      "Step: 1360, train/epoch: 2.262895107269287\n",
      "Step: 1370, train/loss: 0.1136000007390976\n",
      "Step: 1370, train/grad_norm: 8.294543266296387\n",
      "Step: 1370, train/learning_rate: 3.8602331187576056e-05\n",
      "Step: 1370, train/epoch: 2.279534101486206\n",
      "Step: 1380, train/loss: 0.1168999969959259\n",
      "Step: 1380, train/grad_norm: 5.037829399108887\n",
      "Step: 1380, train/learning_rate: 3.8519134250236675e-05\n",
      "Step: 1380, train/epoch: 2.296173095703125\n",
      "Step: 1390, train/loss: 0.12870000302791595\n",
      "Step: 1390, train/grad_norm: 12.695578575134277\n",
      "Step: 1390, train/learning_rate: 3.84359409508761e-05\n",
      "Step: 1390, train/epoch: 2.312812089920044\n",
      "Step: 1400, train/loss: 0.0957999974489212\n",
      "Step: 1400, train/grad_norm: 5.496462821960449\n",
      "Step: 1400, train/learning_rate: 3.835274401353672e-05\n",
      "Step: 1400, train/epoch: 2.329450845718384\n",
      "Step: 1410, train/loss: 0.13109999895095825\n",
      "Step: 1410, train/grad_norm: 7.4934000968933105\n",
      "Step: 1410, train/learning_rate: 3.826955071417615e-05\n",
      "Step: 1410, train/epoch: 2.3460898399353027\n",
      "Step: 1420, train/loss: 0.125\n",
      "Step: 1420, train/grad_norm: 5.4479851722717285\n",
      "Step: 1420, train/learning_rate: 3.8186357414815575e-05\n",
      "Step: 1420, train/epoch: 2.3627288341522217\n",
      "Step: 1430, train/loss: 0.12610000371932983\n",
      "Step: 1430, train/grad_norm: 16.19007110595703\n",
      "Step: 1430, train/learning_rate: 3.8103160477476195e-05\n",
      "Step: 1430, train/epoch: 2.3793678283691406\n",
      "Step: 1440, train/loss: 0.2313999980688095\n",
      "Step: 1440, train/grad_norm: 18.05260467529297\n",
      "Step: 1440, train/learning_rate: 3.801996717811562e-05\n",
      "Step: 1440, train/epoch: 2.3960065841674805\n",
      "Step: 1450, train/loss: 0.22100000083446503\n",
      "Step: 1450, train/grad_norm: 3.766860008239746\n",
      "Step: 1450, train/learning_rate: 3.793677024077624e-05\n",
      "Step: 1450, train/epoch: 2.4126455783843994\n",
      "Step: 1460, train/loss: 0.07559999823570251\n",
      "Step: 1460, train/grad_norm: 12.507867813110352\n",
      "Step: 1460, train/learning_rate: 3.785357694141567e-05\n",
      "Step: 1460, train/epoch: 2.4292845726013184\n",
      "Step: 1470, train/loss: 0.08810000121593475\n",
      "Step: 1470, train/grad_norm: 8.359009742736816\n",
      "Step: 1470, train/learning_rate: 3.7770383642055094e-05\n",
      "Step: 1470, train/epoch: 2.4459235668182373\n",
      "Step: 1480, train/loss: 0.1251000016927719\n",
      "Step: 1480, train/grad_norm: 2.1570613384246826\n",
      "Step: 1480, train/learning_rate: 3.7687186704715714e-05\n",
      "Step: 1480, train/epoch: 2.462562322616577\n",
      "Step: 1490, train/loss: 0.13030000030994415\n",
      "Step: 1490, train/grad_norm: 17.056814193725586\n",
      "Step: 1490, train/learning_rate: 3.760399340535514e-05\n",
      "Step: 1490, train/epoch: 2.479201316833496\n",
      "Step: 1500, train/loss: 0.12839999794960022\n",
      "Step: 1500, train/grad_norm: 2.1749696731567383\n",
      "Step: 1500, train/learning_rate: 3.752080010599457e-05\n",
      "Step: 1500, train/epoch: 2.495840311050415\n",
      "Step: 1510, train/loss: 0.08699999749660492\n",
      "Step: 1510, train/grad_norm: 7.07460355758667\n",
      "Step: 1510, train/learning_rate: 3.743760316865519e-05\n",
      "Step: 1510, train/epoch: 2.512479305267334\n",
      "Step: 1520, train/loss: 0.11599999666213989\n",
      "Step: 1520, train/grad_norm: 12.03420352935791\n",
      "Step: 1520, train/learning_rate: 3.7354409869294614e-05\n",
      "Step: 1520, train/epoch: 2.529118061065674\n",
      "Step: 1530, train/loss: 0.11400000005960464\n",
      "Step: 1530, train/grad_norm: 4.537534236907959\n",
      "Step: 1530, train/learning_rate: 3.727121293195523e-05\n",
      "Step: 1530, train/epoch: 2.5457570552825928\n",
      "Step: 1540, train/loss: 0.13950000703334808\n",
      "Step: 1540, train/grad_norm: 10.455254554748535\n",
      "Step: 1540, train/learning_rate: 3.718801963259466e-05\n",
      "Step: 1540, train/epoch: 2.5623960494995117\n",
      "Step: 1550, train/loss: 0.11349999904632568\n",
      "Step: 1550, train/grad_norm: 5.956282138824463\n",
      "Step: 1550, train/learning_rate: 3.710482633323409e-05\n",
      "Step: 1550, train/epoch: 2.5790350437164307\n",
      "Step: 1560, train/loss: 0.13009999692440033\n",
      "Step: 1560, train/grad_norm: 4.6672892570495605\n",
      "Step: 1560, train/learning_rate: 3.7021629395894706e-05\n",
      "Step: 1560, train/epoch: 2.5956737995147705\n",
      "Step: 1570, train/loss: 0.0869000032544136\n",
      "Step: 1570, train/grad_norm: 7.409707546234131\n",
      "Step: 1570, train/learning_rate: 3.693843609653413e-05\n",
      "Step: 1570, train/epoch: 2.6123127937316895\n",
      "Step: 1580, train/loss: 0.12680000066757202\n",
      "Step: 1580, train/grad_norm: 8.609509468078613\n",
      "Step: 1580, train/learning_rate: 3.685524279717356e-05\n",
      "Step: 1580, train/epoch: 2.6289517879486084\n",
      "Step: 1590, train/loss: 0.08609999716281891\n",
      "Step: 1590, train/grad_norm: 3.386357069015503\n",
      "Step: 1590, train/learning_rate: 3.677204585983418e-05\n",
      "Step: 1590, train/epoch: 2.6455907821655273\n",
      "Step: 1600, train/loss: 0.1031000018119812\n",
      "Step: 1600, train/grad_norm: 7.748258113861084\n",
      "Step: 1600, train/learning_rate: 3.6688852560473606e-05\n",
      "Step: 1600, train/epoch: 2.662229537963867\n",
      "Step: 1610, train/loss: 0.18629999458789825\n",
      "Step: 1610, train/grad_norm: 7.231173992156982\n",
      "Step: 1610, train/learning_rate: 3.6605655623134226e-05\n",
      "Step: 1610, train/epoch: 2.678868532180786\n",
      "Step: 1620, train/loss: 0.17030000686645508\n",
      "Step: 1620, train/grad_norm: 8.949335098266602\n",
      "Step: 1620, train/learning_rate: 3.652246232377365e-05\n",
      "Step: 1620, train/epoch: 2.695507526397705\n",
      "Step: 1630, train/loss: 0.09719999879598618\n",
      "Step: 1630, train/grad_norm: 9.187850952148438\n",
      "Step: 1630, train/learning_rate: 3.643926902441308e-05\n",
      "Step: 1630, train/epoch: 2.712146520614624\n",
      "Step: 1640, train/loss: 0.13089999556541443\n",
      "Step: 1640, train/grad_norm: 13.057866096496582\n",
      "Step: 1640, train/learning_rate: 3.63560720870737e-05\n",
      "Step: 1640, train/epoch: 2.728785276412964\n",
      "Step: 1650, train/loss: 0.1080000028014183\n",
      "Step: 1650, train/grad_norm: 5.245146751403809\n",
      "Step: 1650, train/learning_rate: 3.6272878787713125e-05\n",
      "Step: 1650, train/epoch: 2.745424270629883\n",
      "Step: 1660, train/loss: 0.16200000047683716\n",
      "Step: 1660, train/grad_norm: 6.163614273071289\n",
      "Step: 1660, train/learning_rate: 3.618968548835255e-05\n",
      "Step: 1660, train/epoch: 2.7620632648468018\n",
      "Step: 1670, train/loss: 0.10000000149011612\n",
      "Step: 1670, train/grad_norm: 2.358016014099121\n",
      "Step: 1670, train/learning_rate: 3.610648855101317e-05\n",
      "Step: 1670, train/epoch: 2.7787022590637207\n",
      "Step: 1680, train/loss: 0.08720000088214874\n",
      "Step: 1680, train/grad_norm: 3.1819279193878174\n",
      "Step: 1680, train/learning_rate: 3.60232952516526e-05\n",
      "Step: 1680, train/epoch: 2.7953410148620605\n",
      "Step: 1690, train/loss: 0.10090000182390213\n",
      "Step: 1690, train/grad_norm: 7.1672163009643555\n",
      "Step: 1690, train/learning_rate: 3.594009831431322e-05\n",
      "Step: 1690, train/epoch: 2.8119800090789795\n",
      "Step: 1700, train/loss: 0.14489999413490295\n",
      "Step: 1700, train/grad_norm: 7.389575481414795\n",
      "Step: 1700, train/learning_rate: 3.5856905014952645e-05\n",
      "Step: 1700, train/epoch: 2.8286190032958984\n",
      "Step: 1710, train/loss: 0.06780000030994415\n",
      "Step: 1710, train/grad_norm: 1.4314689636230469\n",
      "Step: 1710, train/learning_rate: 3.577371171559207e-05\n",
      "Step: 1710, train/epoch: 2.8452579975128174\n",
      "Step: 1720, train/loss: 0.11840000003576279\n",
      "Step: 1720, train/grad_norm: 8.507331848144531\n",
      "Step: 1720, train/learning_rate: 3.569051477825269e-05\n",
      "Step: 1720, train/epoch: 2.8618967533111572\n",
      "Step: 1730, train/loss: 0.13899999856948853\n",
      "Step: 1730, train/grad_norm: 6.176205635070801\n",
      "Step: 1730, train/learning_rate: 3.560732147889212e-05\n",
      "Step: 1730, train/epoch: 2.878535747528076\n",
      "Step: 1740, train/loss: 0.14480000734329224\n",
      "Step: 1740, train/grad_norm: 10.21414566040039\n",
      "Step: 1740, train/learning_rate: 3.5524128179531544e-05\n",
      "Step: 1740, train/epoch: 2.895174741744995\n",
      "Step: 1750, train/loss: 0.15870000422000885\n",
      "Step: 1750, train/grad_norm: 15.744009017944336\n",
      "Step: 1750, train/learning_rate: 3.5440931242192164e-05\n",
      "Step: 1750, train/epoch: 2.911813735961914\n",
      "Step: 1760, train/loss: 0.08739999681711197\n",
      "Step: 1760, train/grad_norm: 7.090940475463867\n",
      "Step: 1760, train/learning_rate: 3.535773794283159e-05\n",
      "Step: 1760, train/epoch: 2.928452491760254\n",
      "Step: 1770, train/loss: 0.10610000044107437\n",
      "Step: 1770, train/grad_norm: 18.02572250366211\n",
      "Step: 1770, train/learning_rate: 3.527454100549221e-05\n",
      "Step: 1770, train/epoch: 2.945091485977173\n",
      "Step: 1780, train/loss: 0.16609999537467957\n",
      "Step: 1780, train/grad_norm: 8.667293548583984\n",
      "Step: 1780, train/learning_rate: 3.519134770613164e-05\n",
      "Step: 1780, train/epoch: 2.961730480194092\n",
      "Step: 1790, train/loss: 0.13249999284744263\n",
      "Step: 1790, train/grad_norm: 7.491443157196045\n",
      "Step: 1790, train/learning_rate: 3.5108154406771064e-05\n",
      "Step: 1790, train/epoch: 2.9783694744110107\n",
      "Step: 1800, train/loss: 0.1777999997138977\n",
      "Step: 1800, train/grad_norm: 3.9248909950256348\n",
      "Step: 1800, train/learning_rate: 3.5024957469431683e-05\n",
      "Step: 1800, train/epoch: 2.9950082302093506\n",
      "Step: 1803, eval/loss: 0.3909089267253876\n",
      "Step: 1803, eval/accuracy: 0.8550603985786438\n",
      "Step: 1803, eval/f1: 0.8484440445899963\n",
      "Step: 1803, eval/runtime: 29.082799911499023\n",
      "Step: 1803, eval/samples_per_second: 247.67300415039062\n",
      "Step: 1803, eval/steps_per_second: 4.435999870300293\n",
      "Step: 1803, train/epoch: 3.0\n",
      "Step: 1810, train/loss: 0.17339999973773956\n",
      "Step: 1810, train/grad_norm: 7.626627445220947\n",
      "Step: 1810, train/learning_rate: 3.494176417007111e-05\n",
      "Step: 1810, train/epoch: 3.0116472244262695\n",
      "Step: 1820, train/loss: 0.04360000044107437\n",
      "Step: 1820, train/grad_norm: 6.4236626625061035\n",
      "Step: 1820, train/learning_rate: 3.485856723273173e-05\n",
      "Step: 1820, train/epoch: 3.0282862186431885\n",
      "Step: 1830, train/loss: 0.039400000125169754\n",
      "Step: 1830, train/grad_norm: 3.1772329807281494\n",
      "Step: 1830, train/learning_rate: 3.4775373933371156e-05\n",
      "Step: 1830, train/epoch: 3.0449252128601074\n",
      "Step: 1840, train/loss: 0.07000000029802322\n",
      "Step: 1840, train/grad_norm: 17.924345016479492\n",
      "Step: 1840, train/learning_rate: 3.469218063401058e-05\n",
      "Step: 1840, train/epoch: 3.0615639686584473\n",
      "Step: 1850, train/loss: 0.04830000177025795\n",
      "Step: 1850, train/grad_norm: 9.079641342163086\n",
      "Step: 1850, train/learning_rate: 3.46089836966712e-05\n",
      "Step: 1850, train/epoch: 3.078202962875366\n",
      "Step: 1860, train/loss: 0.04740000143647194\n",
      "Step: 1860, train/grad_norm: 1.1991353034973145\n",
      "Step: 1860, train/learning_rate: 3.452579039731063e-05\n",
      "Step: 1860, train/epoch: 3.094841957092285\n",
      "Step: 1870, train/loss: 0.03759999945759773\n",
      "Step: 1870, train/grad_norm: 0.4399283230304718\n",
      "Step: 1870, train/learning_rate: 3.4442597097950056e-05\n",
      "Step: 1870, train/epoch: 3.111480951309204\n",
      "Step: 1880, train/loss: 0.06769999861717224\n",
      "Step: 1880, train/grad_norm: 23.998565673828125\n",
      "Step: 1880, train/learning_rate: 3.4359400160610676e-05\n",
      "Step: 1880, train/epoch: 3.128119707107544\n",
      "Step: 1890, train/loss: 0.11180000007152557\n",
      "Step: 1890, train/grad_norm: 12.997265815734863\n",
      "Step: 1890, train/learning_rate: 3.42762068612501e-05\n",
      "Step: 1890, train/epoch: 3.144758701324463\n",
      "Step: 1900, train/loss: 0.07400000095367432\n",
      "Step: 1900, train/grad_norm: 14.180262565612793\n",
      "Step: 1900, train/learning_rate: 3.419300992391072e-05\n",
      "Step: 1900, train/epoch: 3.161397695541382\n",
      "Step: 1910, train/loss: 0.061400000005960464\n",
      "Step: 1910, train/grad_norm: 3.03905987739563\n",
      "Step: 1910, train/learning_rate: 3.410981662455015e-05\n",
      "Step: 1910, train/epoch: 3.178036689758301\n",
      "Step: 1920, train/loss: 0.07050000131130219\n",
      "Step: 1920, train/grad_norm: 19.570232391357422\n",
      "Step: 1920, train/learning_rate: 3.4026623325189576e-05\n",
      "Step: 1920, train/epoch: 3.1946754455566406\n",
      "Step: 1930, train/loss: 0.02539999969303608\n",
      "Step: 1930, train/grad_norm: 0.3881848156452179\n",
      "Step: 1930, train/learning_rate: 3.3943426387850195e-05\n",
      "Step: 1930, train/epoch: 3.2113144397735596\n",
      "Step: 1940, train/loss: 0.07109999656677246\n",
      "Step: 1940, train/grad_norm: 17.916309356689453\n",
      "Step: 1940, train/learning_rate: 3.386023308848962e-05\n",
      "Step: 1940, train/epoch: 3.2279534339904785\n",
      "Step: 1950, train/loss: 0.03539999946951866\n",
      "Step: 1950, train/grad_norm: 9.92866325378418\n",
      "Step: 1950, train/learning_rate: 3.377703978912905e-05\n",
      "Step: 1950, train/epoch: 3.2445924282073975\n",
      "Step: 1960, train/loss: 0.03200000151991844\n",
      "Step: 1960, train/grad_norm: 7.256804466247559\n",
      "Step: 1960, train/learning_rate: 3.369384285178967e-05\n",
      "Step: 1960, train/epoch: 3.2612311840057373\n",
      "Step: 1970, train/loss: 0.07280000299215317\n",
      "Step: 1970, train/grad_norm: 0.17144747078418732\n",
      "Step: 1970, train/learning_rate: 3.3610649552429095e-05\n",
      "Step: 1970, train/epoch: 3.2778701782226562\n",
      "Step: 1980, train/loss: 0.08969999849796295\n",
      "Step: 1980, train/grad_norm: 15.581927299499512\n",
      "Step: 1980, train/learning_rate: 3.3527452615089715e-05\n",
      "Step: 1980, train/epoch: 3.294509172439575\n",
      "Step: 1990, train/loss: 0.09749999642372131\n",
      "Step: 1990, train/grad_norm: 20.642873764038086\n",
      "Step: 1990, train/learning_rate: 3.344425931572914e-05\n",
      "Step: 1990, train/epoch: 3.311148166656494\n",
      "Step: 2000, train/loss: 0.04729999974370003\n",
      "Step: 2000, train/grad_norm: 9.092758178710938\n",
      "Step: 2000, train/learning_rate: 3.336106601636857e-05\n",
      "Step: 2000, train/epoch: 3.327786922454834\n",
      "Step: 2010, train/loss: 0.04569999873638153\n",
      "Step: 2010, train/grad_norm: 11.88144588470459\n",
      "Step: 2010, train/learning_rate: 3.327786907902919e-05\n",
      "Step: 2010, train/epoch: 3.344425916671753\n",
      "Step: 2020, train/loss: 0.13130000233650208\n",
      "Step: 2020, train/grad_norm: 6.160036087036133\n",
      "Step: 2020, train/learning_rate: 3.3194675779668614e-05\n",
      "Step: 2020, train/epoch: 3.361064910888672\n",
      "Step: 2030, train/loss: 0.04670000076293945\n",
      "Step: 2030, train/grad_norm: 5.594160079956055\n",
      "Step: 2030, train/learning_rate: 3.311148248030804e-05\n",
      "Step: 2030, train/epoch: 3.377703905105591\n",
      "Step: 2040, train/loss: 0.06589999794960022\n",
      "Step: 2040, train/grad_norm: 1.9165687561035156\n",
      "Step: 2040, train/learning_rate: 3.302828554296866e-05\n",
      "Step: 2040, train/epoch: 3.3943426609039307\n",
      "Step: 2050, train/loss: 0.06419999897480011\n",
      "Step: 2050, train/grad_norm: 5.674718856811523\n",
      "Step: 2050, train/learning_rate: 3.294509224360809e-05\n",
      "Step: 2050, train/epoch: 3.4109816551208496\n",
      "Step: 2060, train/loss: 0.032999999821186066\n",
      "Step: 2060, train/grad_norm: 4.916676044464111\n",
      "Step: 2060, train/learning_rate: 3.286189530626871e-05\n",
      "Step: 2060, train/epoch: 3.4276206493377686\n",
      "Step: 2070, train/loss: 0.03310000151395798\n",
      "Step: 2070, train/grad_norm: 10.338193893432617\n",
      "Step: 2070, train/learning_rate: 3.2778702006908134e-05\n",
      "Step: 2070, train/epoch: 3.4442596435546875\n",
      "Step: 2080, train/loss: 0.05889999866485596\n",
      "Step: 2080, train/grad_norm: 8.213831901550293\n",
      "Step: 2080, train/learning_rate: 3.269550870754756e-05\n",
      "Step: 2080, train/epoch: 3.4608983993530273\n",
      "Step: 2090, train/loss: 0.0551999993622303\n",
      "Step: 2090, train/grad_norm: 8.741455078125\n",
      "Step: 2090, train/learning_rate: 3.261231177020818e-05\n",
      "Step: 2090, train/epoch: 3.4775373935699463\n",
      "Step: 2100, train/loss: 0.053300000727176666\n",
      "Step: 2100, train/grad_norm: 2.6059682369232178\n",
      "Step: 2100, train/learning_rate: 3.252911847084761e-05\n",
      "Step: 2100, train/epoch: 3.4941763877868652\n",
      "Step: 2110, train/loss: 0.07450000196695328\n",
      "Step: 2110, train/grad_norm: 16.210220336914062\n",
      "Step: 2110, train/learning_rate: 3.244592517148703e-05\n",
      "Step: 2110, train/epoch: 3.510815382003784\n",
      "Step: 2120, train/loss: 0.06129999831318855\n",
      "Step: 2120, train/grad_norm: 8.837286949157715\n",
      "Step: 2120, train/learning_rate: 3.236272823414765e-05\n",
      "Step: 2120, train/epoch: 3.527454137802124\n",
      "Step: 2130, train/loss: 0.10109999775886536\n",
      "Step: 2130, train/grad_norm: 15.959219932556152\n",
      "Step: 2130, train/learning_rate: 3.227953493478708e-05\n",
      "Step: 2130, train/epoch: 3.544093132019043\n",
      "Step: 2140, train/loss: 0.14229999482631683\n",
      "Step: 2140, train/grad_norm: 6.594682693481445\n",
      "Step: 2140, train/learning_rate: 3.21963379974477e-05\n",
      "Step: 2140, train/epoch: 3.560732126235962\n",
      "Step: 2150, train/loss: 0.053300000727176666\n",
      "Step: 2150, train/grad_norm: 0.13462771475315094\n",
      "Step: 2150, train/learning_rate: 3.2113144698087126e-05\n",
      "Step: 2150, train/epoch: 3.577371120452881\n",
      "Step: 2160, train/loss: 0.039400000125169754\n",
      "Step: 2160, train/grad_norm: 6.786731243133545\n",
      "Step: 2160, train/learning_rate: 3.202995139872655e-05\n",
      "Step: 2160, train/epoch: 3.5940098762512207\n",
      "Step: 2170, train/loss: 0.08229999989271164\n",
      "Step: 2170, train/grad_norm: 22.20990562438965\n",
      "Step: 2170, train/learning_rate: 3.194675446138717e-05\n",
      "Step: 2170, train/epoch: 3.6106488704681396\n",
      "Step: 2180, train/loss: 0.08730000257492065\n",
      "Step: 2180, train/grad_norm: 7.2296061515808105\n",
      "Step: 2180, train/learning_rate: 3.18635611620266e-05\n",
      "Step: 2180, train/epoch: 3.6272878646850586\n",
      "Step: 2190, train/loss: 0.0794999971985817\n",
      "Step: 2190, train/grad_norm: 11.48222541809082\n",
      "Step: 2190, train/learning_rate: 3.1780367862666026e-05\n",
      "Step: 2190, train/epoch: 3.6439268589019775\n",
      "Step: 2200, train/loss: 0.07209999859333038\n",
      "Step: 2200, train/grad_norm: 8.808452606201172\n",
      "Step: 2200, train/learning_rate: 3.1697170925326645e-05\n",
      "Step: 2200, train/epoch: 3.6605656147003174\n",
      "Step: 2210, train/loss: 0.11060000211000443\n",
      "Step: 2210, train/grad_norm: 17.443422317504883\n",
      "Step: 2210, train/learning_rate: 3.161397762596607e-05\n",
      "Step: 2210, train/epoch: 3.6772046089172363\n",
      "Step: 2220, train/loss: 0.06759999692440033\n",
      "Step: 2220, train/grad_norm: 3.305938243865967\n",
      "Step: 2220, train/learning_rate: 3.153078068862669e-05\n",
      "Step: 2220, train/epoch: 3.6938436031341553\n",
      "Step: 2230, train/loss: 0.05950000137090683\n",
      "Step: 2230, train/grad_norm: 7.335351467132568\n",
      "Step: 2230, train/learning_rate: 3.144758738926612e-05\n",
      "Step: 2230, train/epoch: 3.710482597351074\n",
      "Step: 2240, train/loss: 0.0640999972820282\n",
      "Step: 2240, train/grad_norm: 9.967755317687988\n",
      "Step: 2240, train/learning_rate: 3.1364394089905545e-05\n",
      "Step: 2240, train/epoch: 3.727121353149414\n",
      "Step: 2250, train/loss: 0.06759999692440033\n",
      "Step: 2250, train/grad_norm: 11.529422760009766\n",
      "Step: 2250, train/learning_rate: 3.1281197152566165e-05\n",
      "Step: 2250, train/epoch: 3.743760347366333\n",
      "Step: 2260, train/loss: 0.049400001764297485\n",
      "Step: 2260, train/grad_norm: 5.538758277893066\n",
      "Step: 2260, train/learning_rate: 3.119800385320559e-05\n",
      "Step: 2260, train/epoch: 3.760399341583252\n",
      "Step: 2270, train/loss: 0.03889999911189079\n",
      "Step: 2270, train/grad_norm: 6.060009479522705\n",
      "Step: 2270, train/learning_rate: 3.111480691586621e-05\n",
      "Step: 2270, train/epoch: 3.777038335800171\n",
      "Step: 2280, train/loss: 0.11320000141859055\n",
      "Step: 2280, train/grad_norm: 12.267117500305176\n",
      "Step: 2280, train/learning_rate: 3.103161361650564e-05\n",
      "Step: 2280, train/epoch: 3.7936770915985107\n",
      "Step: 2290, train/loss: 0.09229999780654907\n",
      "Step: 2290, train/grad_norm: 4.435079097747803\n",
      "Step: 2290, train/learning_rate: 3.0948420317145064e-05\n",
      "Step: 2290, train/epoch: 3.8103160858154297\n",
      "Step: 2300, train/loss: 0.06539999693632126\n",
      "Step: 2300, train/grad_norm: 7.302435398101807\n",
      "Step: 2300, train/learning_rate: 3.0865223379805684e-05\n",
      "Step: 2300, train/epoch: 3.8269550800323486\n",
      "Step: 2310, train/loss: 0.03869999945163727\n",
      "Step: 2310, train/grad_norm: 5.452612400054932\n",
      "Step: 2310, train/learning_rate: 3.078203008044511e-05\n",
      "Step: 2310, train/epoch: 3.8435940742492676\n",
      "Step: 2320, train/loss: 0.09440000355243683\n",
      "Step: 2320, train/grad_norm: 4.060266971588135\n",
      "Step: 2320, train/learning_rate: 3.069883678108454e-05\n",
      "Step: 2320, train/epoch: 3.8602328300476074\n",
      "Step: 2330, train/loss: 0.0471000000834465\n",
      "Step: 2330, train/grad_norm: 5.112148284912109\n",
      "Step: 2330, train/learning_rate: 3.061563984374516e-05\n",
      "Step: 2330, train/epoch: 3.8768718242645264\n",
      "Step: 2340, train/loss: 0.04450000077486038\n",
      "Step: 2340, train/grad_norm: 6.418694019317627\n",
      "Step: 2340, train/learning_rate: 3.0532446544384584e-05\n",
      "Step: 2340, train/epoch: 3.8935108184814453\n",
      "Step: 2350, train/loss: 0.07729999721050262\n",
      "Step: 2350, train/grad_norm: 15.642815589904785\n",
      "Step: 2350, train/learning_rate: 3.0449251426034607e-05\n",
      "Step: 2350, train/epoch: 3.9101498126983643\n",
      "Step: 2360, train/loss: 0.07900000363588333\n",
      "Step: 2360, train/grad_norm: 8.061161994934082\n",
      "Step: 2360, train/learning_rate: 3.036605630768463e-05\n",
      "Step: 2360, train/epoch: 3.926788568496704\n",
      "Step: 2370, train/loss: 0.08569999784231186\n",
      "Step: 2370, train/grad_norm: 2.9498536586761475\n",
      "Step: 2370, train/learning_rate: 3.0282861189334653e-05\n",
      "Step: 2370, train/epoch: 3.943427562713623\n",
      "Step: 2380, train/loss: 0.04879999905824661\n",
      "Step: 2380, train/grad_norm: 3.382533550262451\n",
      "Step: 2380, train/learning_rate: 3.019966788997408e-05\n",
      "Step: 2380, train/epoch: 3.960066556930542\n",
      "Step: 2390, train/loss: 0.0494999997317791\n",
      "Step: 2390, train/grad_norm: 7.42744255065918\n",
      "Step: 2390, train/learning_rate: 3.0116472771624103e-05\n",
      "Step: 2390, train/epoch: 3.976705551147461\n",
      "Step: 2400, train/loss: 0.04340000078082085\n",
      "Step: 2400, train/grad_norm: 5.021377086639404\n",
      "Step: 2400, train/learning_rate: 3.0033277653274126e-05\n",
      "Step: 2400, train/epoch: 3.993344306945801\n",
      "Step: 2404, eval/loss: 0.618340015411377\n",
      "Step: 2404, eval/accuracy: 0.8517284393310547\n",
      "Step: 2404, eval/f1: 0.8438596725463867\n",
      "Step: 2404, eval/runtime: 28.990100860595703\n",
      "Step: 2404, eval/samples_per_second: 248.46400451660156\n",
      "Step: 2404, eval/steps_per_second: 4.449999809265137\n",
      "Step: 2404, train/epoch: 4.0\n",
      "Step: 2410, train/loss: 0.047200001776218414\n",
      "Step: 2410, train/grad_norm: 6.778086185455322\n",
      "Step: 2410, train/learning_rate: 2.995008253492415e-05\n",
      "Step: 2410, train/epoch: 4.009983539581299\n",
      "Step: 2420, train/loss: 0.026900000870227814\n",
      "Step: 2420, train/grad_norm: 1.129975438117981\n",
      "Step: 2420, train/learning_rate: 2.9866889235563576e-05\n",
      "Step: 2420, train/epoch: 4.026622295379639\n",
      "Step: 2430, train/loss: 0.054999999701976776\n",
      "Step: 2430, train/grad_norm: 0.12797923386096954\n",
      "Step: 2430, train/learning_rate: 2.97836941172136e-05\n",
      "Step: 2430, train/epoch: 4.0432610511779785\n",
      "Step: 2440, train/loss: 0.026499999687075615\n",
      "Step: 2440, train/grad_norm: 7.593715190887451\n",
      "Step: 2440, train/learning_rate: 2.9700498998863623e-05\n",
      "Step: 2440, train/epoch: 4.059900283813477\n",
      "Step: 2450, train/loss: 0.011500000022351742\n",
      "Step: 2450, train/grad_norm: 2.61326003074646\n",
      "Step: 2450, train/learning_rate: 2.9617303880513646e-05\n",
      "Step: 2450, train/epoch: 4.076539039611816\n",
      "Step: 2460, train/loss: 0.03790000081062317\n",
      "Step: 2460, train/grad_norm: 10.021653175354004\n",
      "Step: 2460, train/learning_rate: 2.9534110581153072e-05\n",
      "Step: 2460, train/epoch: 4.0931782722473145\n",
      "Step: 2470, train/loss: 0.04230000078678131\n",
      "Step: 2470, train/grad_norm: 7.035030841827393\n",
      "Step: 2470, train/learning_rate: 2.9450915462803096e-05\n",
      "Step: 2470, train/epoch: 4.109817028045654\n",
      "Step: 2480, train/loss: 0.10300000011920929\n",
      "Step: 2480, train/grad_norm: 21.427047729492188\n",
      "Step: 2480, train/learning_rate: 2.936772034445312e-05\n",
      "Step: 2480, train/epoch: 4.126455783843994\n",
      "Step: 2490, train/loss: 0.04910000041127205\n",
      "Step: 2490, train/grad_norm: 3.5806219577789307\n",
      "Step: 2490, train/learning_rate: 2.9284525226103142e-05\n",
      "Step: 2490, train/epoch: 4.143095016479492\n",
      "Step: 2500, train/loss: 0.018699999898672104\n",
      "Step: 2500, train/grad_norm: 6.673237323760986\n",
      "Step: 2500, train/learning_rate: 2.920133192674257e-05\n",
      "Step: 2500, train/epoch: 4.159733772277832\n",
      "Step: 2510, train/loss: 0.042899999767541885\n",
      "Step: 2510, train/grad_norm: 7.881353855133057\n",
      "Step: 2510, train/learning_rate: 2.9118136808392592e-05\n",
      "Step: 2510, train/epoch: 4.176372528076172\n",
      "Step: 2520, train/loss: 0.043800000101327896\n",
      "Step: 2520, train/grad_norm: 10.281780242919922\n",
      "Step: 2520, train/learning_rate: 2.9034941690042615e-05\n",
      "Step: 2520, train/epoch: 4.19301176071167\n",
      "Step: 2530, train/loss: 0.025599999353289604\n",
      "Step: 2530, train/grad_norm: 4.426957607269287\n",
      "Step: 2530, train/learning_rate: 2.8951746571692638e-05\n",
      "Step: 2530, train/epoch: 4.20965051651001\n",
      "Step: 2540, train/loss: 0.03319999948143959\n",
      "Step: 2540, train/grad_norm: 0.2467423379421234\n",
      "Step: 2540, train/learning_rate: 2.8868553272332065e-05\n",
      "Step: 2540, train/epoch: 4.226289749145508\n",
      "Step: 2550, train/loss: 0.05420000106096268\n",
      "Step: 2550, train/grad_norm: 25.27022933959961\n",
      "Step: 2550, train/learning_rate: 2.8785358153982088e-05\n",
      "Step: 2550, train/epoch: 4.242928504943848\n",
      "Step: 2560, train/loss: 0.03060000017285347\n",
      "Step: 2560, train/grad_norm: 12.031006813049316\n",
      "Step: 2560, train/learning_rate: 2.870216303563211e-05\n",
      "Step: 2560, train/epoch: 4.2595672607421875\n",
      "Step: 2570, train/loss: 0.06700000166893005\n",
      "Step: 2570, train/grad_norm: 0.12375228852033615\n",
      "Step: 2570, train/learning_rate: 2.8618967917282134e-05\n",
      "Step: 2570, train/epoch: 4.2762064933776855\n",
      "Step: 2580, train/loss: 0.065700002014637\n",
      "Step: 2580, train/grad_norm: 13.125150680541992\n",
      "Step: 2580, train/learning_rate: 2.853577461792156e-05\n",
      "Step: 2580, train/epoch: 4.292845249176025\n",
      "Step: 2590, train/loss: 0.028200000524520874\n",
      "Step: 2590, train/grad_norm: 12.218204498291016\n",
      "Step: 2590, train/learning_rate: 2.8452579499571584e-05\n",
      "Step: 2590, train/epoch: 4.309484004974365\n",
      "Step: 2600, train/loss: 0.012199999764561653\n",
      "Step: 2600, train/grad_norm: 1.5945285558700562\n",
      "Step: 2600, train/learning_rate: 2.8369384381221607e-05\n",
      "Step: 2600, train/epoch: 4.326123237609863\n",
      "Step: 2610, train/loss: 0.006399999838322401\n",
      "Step: 2610, train/grad_norm: 10.258026123046875\n",
      "Step: 2610, train/learning_rate: 2.828618926287163e-05\n",
      "Step: 2610, train/epoch: 4.342761993408203\n",
      "Step: 2620, train/loss: 0.028599999845027924\n",
      "Step: 2620, train/grad_norm: 18.74569320678711\n",
      "Step: 2620, train/learning_rate: 2.8202994144521654e-05\n",
      "Step: 2620, train/epoch: 4.359401226043701\n",
      "Step: 2630, train/loss: 0.022700000554323196\n",
      "Step: 2630, train/grad_norm: 0.528765082359314\n",
      "Step: 2630, train/learning_rate: 2.811980084516108e-05\n",
      "Step: 2630, train/epoch: 4.376039981842041\n",
      "Step: 2640, train/loss: 0.032600000500679016\n",
      "Step: 2640, train/grad_norm: 18.717321395874023\n",
      "Step: 2640, train/learning_rate: 2.8036605726811104e-05\n",
      "Step: 2640, train/epoch: 4.392678737640381\n",
      "Step: 2650, train/loss: 0.026000000536441803\n",
      "Step: 2650, train/grad_norm: 9.144762992858887\n",
      "Step: 2650, train/learning_rate: 2.7953410608461127e-05\n",
      "Step: 2650, train/epoch: 4.409317970275879\n",
      "Step: 2660, train/loss: 0.029999999329447746\n",
      "Step: 2660, train/grad_norm: 1.0208370685577393\n",
      "Step: 2660, train/learning_rate: 2.787021549011115e-05\n",
      "Step: 2660, train/epoch: 4.425956726074219\n",
      "Step: 2670, train/loss: 0.010900000110268593\n",
      "Step: 2670, train/grad_norm: 19.983959197998047\n",
      "Step: 2670, train/learning_rate: 2.7787022190750577e-05\n",
      "Step: 2670, train/epoch: 4.442595481872559\n",
      "Step: 2680, train/loss: 0.031199999153614044\n",
      "Step: 2680, train/grad_norm: 26.656200408935547\n",
      "Step: 2680, train/learning_rate: 2.77038270724006e-05\n",
      "Step: 2680, train/epoch: 4.459234714508057\n",
      "Step: 2690, train/loss: 0.029500000178813934\n",
      "Step: 2690, train/grad_norm: 15.074687004089355\n",
      "Step: 2690, train/learning_rate: 2.7620631954050623e-05\n",
      "Step: 2690, train/epoch: 4.4758734703063965\n",
      "Step: 2700, train/loss: 0.013000000268220901\n",
      "Step: 2700, train/grad_norm: 10.974034309387207\n",
      "Step: 2700, train/learning_rate: 2.7537436835700646e-05\n",
      "Step: 2700, train/epoch: 4.4925127029418945\n",
      "Step: 2710, train/loss: 0.046300001442432404\n",
      "Step: 2710, train/grad_norm: 0.24931395053863525\n",
      "Step: 2710, train/learning_rate: 2.7454243536340073e-05\n",
      "Step: 2710, train/epoch: 4.509151458740234\n",
      "Step: 2720, train/loss: 0.011699999682605267\n",
      "Step: 2720, train/grad_norm: 2.7087059020996094\n",
      "Step: 2720, train/learning_rate: 2.7371048417990096e-05\n",
      "Step: 2720, train/epoch: 4.525790214538574\n",
      "Step: 2730, train/loss: 0.037700001150369644\n",
      "Step: 2730, train/grad_norm: 13.843049049377441\n",
      "Step: 2730, train/learning_rate: 2.728785329964012e-05\n",
      "Step: 2730, train/epoch: 4.542429447174072\n",
      "Step: 2740, train/loss: 0.021700000390410423\n",
      "Step: 2740, train/grad_norm: 0.691882312297821\n",
      "Step: 2740, train/learning_rate: 2.7204658181290142e-05\n",
      "Step: 2740, train/epoch: 4.559068202972412\n",
      "Step: 2750, train/loss: 0.03290000185370445\n",
      "Step: 2750, train/grad_norm: 3.770277976989746\n",
      "Step: 2750, train/learning_rate: 2.712146488192957e-05\n",
      "Step: 2750, train/epoch: 4.575706958770752\n",
      "Step: 2760, train/loss: 0.025200000032782555\n",
      "Step: 2760, train/grad_norm: 16.232309341430664\n",
      "Step: 2760, train/learning_rate: 2.7038269763579592e-05\n",
      "Step: 2760, train/epoch: 4.59234619140625\n",
      "Step: 2770, train/loss: 0.014000000432133675\n",
      "Step: 2770, train/grad_norm: 0.17342562973499298\n",
      "Step: 2770, train/learning_rate: 2.6955074645229615e-05\n",
      "Step: 2770, train/epoch: 4.60898494720459\n",
      "Step: 2780, train/loss: 0.028599999845027924\n",
      "Step: 2780, train/grad_norm: 0.7339286208152771\n",
      "Step: 2780, train/learning_rate: 2.687187952687964e-05\n",
      "Step: 2780, train/epoch: 4.625624179840088\n",
      "Step: 2790, train/loss: 0.07689999788999557\n",
      "Step: 2790, train/grad_norm: 22.684585571289062\n",
      "Step: 2790, train/learning_rate: 2.6788686227519065e-05\n",
      "Step: 2790, train/epoch: 4.642262935638428\n",
      "Step: 2800, train/loss: 0.07660000026226044\n",
      "Step: 2800, train/grad_norm: 8.582754135131836\n",
      "Step: 2800, train/learning_rate: 2.6705491109169088e-05\n",
      "Step: 2800, train/epoch: 4.658901691436768\n",
      "Step: 2810, train/loss: 0.08139999955892563\n",
      "Step: 2810, train/grad_norm: 14.18621826171875\n",
      "Step: 2810, train/learning_rate: 2.662229599081911e-05\n",
      "Step: 2810, train/epoch: 4.675540924072266\n",
      "Step: 2820, train/loss: 0.04879999905824661\n",
      "Step: 2820, train/grad_norm: 13.780893325805664\n",
      "Step: 2820, train/learning_rate: 2.6539100872469135e-05\n",
      "Step: 2820, train/epoch: 4.6921796798706055\n",
      "Step: 2830, train/loss: 0.03269999846816063\n",
      "Step: 2830, train/grad_norm: 19.355100631713867\n",
      "Step: 2830, train/learning_rate: 2.645590757310856e-05\n",
      "Step: 2830, train/epoch: 4.708818435668945\n",
      "Step: 2840, train/loss: 0.025599999353289604\n",
      "Step: 2840, train/grad_norm: 7.03063440322876\n",
      "Step: 2840, train/learning_rate: 2.6372712454758584e-05\n",
      "Step: 2840, train/epoch: 4.725457668304443\n",
      "Step: 2850, train/loss: 0.049800001084804535\n",
      "Step: 2850, train/grad_norm: 3.8975324630737305\n",
      "Step: 2850, train/learning_rate: 2.6289517336408608e-05\n",
      "Step: 2850, train/epoch: 4.742096424102783\n",
      "Step: 2860, train/loss: 0.06469999998807907\n",
      "Step: 2860, train/grad_norm: 18.838376998901367\n",
      "Step: 2860, train/learning_rate: 2.620632221805863e-05\n",
      "Step: 2860, train/epoch: 4.758735656738281\n",
      "Step: 2870, train/loss: 0.056299999356269836\n",
      "Step: 2870, train/grad_norm: 17.43381118774414\n",
      "Step: 2870, train/learning_rate: 2.6123128918698058e-05\n",
      "Step: 2870, train/epoch: 4.775374412536621\n",
      "Step: 2880, train/loss: 0.061400000005960464\n",
      "Step: 2880, train/grad_norm: 9.412975311279297\n",
      "Step: 2880, train/learning_rate: 2.603993380034808e-05\n",
      "Step: 2880, train/epoch: 4.792013168334961\n",
      "Step: 2890, train/loss: 0.02449999935925007\n",
      "Step: 2890, train/grad_norm: 19.34229850769043\n",
      "Step: 2890, train/learning_rate: 2.5956738681998104e-05\n",
      "Step: 2890, train/epoch: 4.808652400970459\n",
      "Step: 2900, train/loss: 0.008299999870359898\n",
      "Step: 2900, train/grad_norm: 0.18387708067893982\n",
      "Step: 2900, train/learning_rate: 2.5873543563648127e-05\n",
      "Step: 2900, train/epoch: 4.825291156768799\n",
      "Step: 2910, train/loss: 0.015599999576807022\n",
      "Step: 2910, train/grad_norm: 0.025898005813360214\n",
      "Step: 2910, train/learning_rate: 2.5790350264287554e-05\n",
      "Step: 2910, train/epoch: 4.841929912567139\n",
      "Step: 2920, train/loss: 0.07020000368356705\n",
      "Step: 2920, train/grad_norm: 14.829462051391602\n",
      "Step: 2920, train/learning_rate: 2.5707155145937577e-05\n",
      "Step: 2920, train/epoch: 4.858569145202637\n",
      "Step: 2930, train/loss: 0.04010000079870224\n",
      "Step: 2930, train/grad_norm: 0.1144370436668396\n",
      "Step: 2930, train/learning_rate: 2.56239600275876e-05\n",
      "Step: 2930, train/epoch: 4.875207901000977\n",
      "Step: 2940, train/loss: 0.08309999853372574\n",
      "Step: 2940, train/grad_norm: 6.68585729598999\n",
      "Step: 2940, train/learning_rate: 2.5540764909237623e-05\n",
      "Step: 2940, train/epoch: 4.891847133636475\n",
      "Step: 2950, train/loss: 0.05719999969005585\n",
      "Step: 2950, train/grad_norm: 3.457932949066162\n",
      "Step: 2950, train/learning_rate: 2.545757160987705e-05\n",
      "Step: 2950, train/epoch: 4.9084858894348145\n",
      "Step: 2960, train/loss: 0.04639999940991402\n",
      "Step: 2960, train/grad_norm: 14.48156452178955\n",
      "Step: 2960, train/learning_rate: 2.5374376491527073e-05\n",
      "Step: 2960, train/epoch: 4.925124645233154\n",
      "Step: 2970, train/loss: 0.03350000083446503\n",
      "Step: 2970, train/grad_norm: 1.9284411668777466\n",
      "Step: 2970, train/learning_rate: 2.5291181373177096e-05\n",
      "Step: 2970, train/epoch: 4.941763877868652\n",
      "Step: 2980, train/loss: 0.009600000455975533\n",
      "Step: 2980, train/grad_norm: 0.6992711424827576\n",
      "Step: 2980, train/learning_rate: 2.520798625482712e-05\n",
      "Step: 2980, train/epoch: 4.958402633666992\n",
      "Step: 2990, train/loss: 0.05869999900460243\n",
      "Step: 2990, train/grad_norm: 0.2763079106807709\n",
      "Step: 2990, train/learning_rate: 2.5124791136477143e-05\n",
      "Step: 2990, train/epoch: 4.975041389465332\n",
      "Step: 3000, train/loss: 0.03720000013709068\n",
      "Step: 3000, train/grad_norm: 0.8803993463516235\n",
      "Step: 3000, train/learning_rate: 2.504159783711657e-05\n",
      "Step: 3000, train/epoch: 4.99168062210083\n",
      "Step: 3005, eval/loss: 0.9418562054634094\n",
      "Step: 3005, eval/accuracy: 0.800916314125061\n",
      "Step: 3005, eval/f1: 0.7994638681411743\n",
      "Step: 3005, eval/runtime: 29.052000045776367\n",
      "Step: 3005, eval/samples_per_second: 247.9340057373047\n",
      "Step: 3005, eval/steps_per_second: 4.440000057220459\n",
      "Step: 3005, train/epoch: 5.0\n",
      "Step: 3010, train/loss: 0.05339999869465828\n",
      "Step: 3010, train/grad_norm: 20.455942153930664\n",
      "Step: 3010, train/learning_rate: 2.4958402718766592e-05\n",
      "Step: 3010, train/epoch: 5.00831937789917\n",
      "Step: 3020, train/loss: 0.04879999905824661\n",
      "Step: 3020, train/grad_norm: 0.14007382094860077\n",
      "Step: 3020, train/learning_rate: 2.4875207600416616e-05\n",
      "Step: 3020, train/epoch: 5.024958610534668\n",
      "Step: 3030, train/loss: 0.02810000069439411\n",
      "Step: 3030, train/grad_norm: 0.04046662524342537\n",
      "Step: 3030, train/learning_rate: 2.479201248206664e-05\n",
      "Step: 3030, train/epoch: 5.041597366333008\n",
      "Step: 3040, train/loss: 0.033399999141693115\n",
      "Step: 3040, train/grad_norm: 0.14241914451122284\n",
      "Step: 3040, train/learning_rate: 2.4708819182706065e-05\n",
      "Step: 3040, train/epoch: 5.058236122131348\n",
      "Step: 3050, train/loss: 0.02239999920129776\n",
      "Step: 3050, train/grad_norm: 0.06199033558368683\n",
      "Step: 3050, train/learning_rate: 2.462562406435609e-05\n",
      "Step: 3050, train/epoch: 5.074875354766846\n",
      "Step: 3060, train/loss: 0.02669999934732914\n",
      "Step: 3060, train/grad_norm: 3.1458170413970947\n",
      "Step: 3060, train/learning_rate: 2.4542428946006112e-05\n",
      "Step: 3060, train/epoch: 5.0915141105651855\n",
      "Step: 3070, train/loss: 0.026900000870227814\n",
      "Step: 3070, train/grad_norm: 0.9757124185562134\n",
      "Step: 3070, train/learning_rate: 2.4459233827656135e-05\n",
      "Step: 3070, train/epoch: 5.108152866363525\n",
      "Step: 3080, train/loss: 0.012299999594688416\n",
      "Step: 3080, train/grad_norm: 0.7434651851654053\n",
      "Step: 3080, train/learning_rate: 2.437604052829556e-05\n",
      "Step: 3080, train/epoch: 5.124792098999023\n",
      "Step: 3090, train/loss: 0.010200000368058681\n",
      "Step: 3090, train/grad_norm: 18.68720054626465\n",
      "Step: 3090, train/learning_rate: 2.4292845409945585e-05\n",
      "Step: 3090, train/epoch: 5.141430854797363\n",
      "Step: 3100, train/loss: 0.03869999945163727\n",
      "Step: 3100, train/grad_norm: 8.449939727783203\n",
      "Step: 3100, train/learning_rate: 2.4209650291595608e-05\n",
      "Step: 3100, train/epoch: 5.158070087432861\n",
      "Step: 3110, train/loss: 0.017500000074505806\n",
      "Step: 3110, train/grad_norm: 0.03583526238799095\n",
      "Step: 3110, train/learning_rate: 2.412645517324563e-05\n",
      "Step: 3110, train/epoch: 5.174708843231201\n",
      "Step: 3120, train/loss: 0.02019999921321869\n",
      "Step: 3120, train/grad_norm: 0.10516361892223358\n",
      "Step: 3120, train/learning_rate: 2.4043261873885058e-05\n",
      "Step: 3120, train/epoch: 5.191347599029541\n",
      "Step: 3130, train/loss: 0.03480000048875809\n",
      "Step: 3130, train/grad_norm: 1.6366349458694458\n",
      "Step: 3130, train/learning_rate: 2.396006675553508e-05\n",
      "Step: 3130, train/epoch: 5.207986831665039\n",
      "Step: 3140, train/loss: 0.0007999999797903001\n",
      "Step: 3140, train/grad_norm: 0.9209414720535278\n",
      "Step: 3140, train/learning_rate: 2.3876871637185104e-05\n",
      "Step: 3140, train/epoch: 5.224625587463379\n",
      "Step: 3150, train/loss: 0.021800000220537186\n",
      "Step: 3150, train/grad_norm: 1.3859413862228394\n",
      "Step: 3150, train/learning_rate: 2.3793676518835127e-05\n",
      "Step: 3150, train/epoch: 5.241264343261719\n",
      "Step: 3160, train/loss: 0.032999999821186066\n",
      "Step: 3160, train/grad_norm: 1.9614253044128418\n",
      "Step: 3160, train/learning_rate: 2.3710483219474554e-05\n",
      "Step: 3160, train/epoch: 5.257903575897217\n",
      "Step: 3170, train/loss: 0.023099999874830246\n",
      "Step: 3170, train/grad_norm: 25.41549301147461\n",
      "Step: 3170, train/learning_rate: 2.3627288101124577e-05\n",
      "Step: 3170, train/epoch: 5.274542331695557\n",
      "Step: 3180, train/loss: 0.004900000058114529\n",
      "Step: 3180, train/grad_norm: 5.920009613037109\n",
      "Step: 3180, train/learning_rate: 2.35440929827746e-05\n",
      "Step: 3180, train/epoch: 5.291181564331055\n",
      "Step: 3190, train/loss: 0.03909999877214432\n",
      "Step: 3190, train/grad_norm: 7.276708602905273\n",
      "Step: 3190, train/learning_rate: 2.3460897864424624e-05\n",
      "Step: 3190, train/epoch: 5.3078203201293945\n",
      "Step: 3200, train/loss: 0.038600001484155655\n",
      "Step: 3200, train/grad_norm: 0.11042533069849014\n",
      "Step: 3200, train/learning_rate: 2.337770456506405e-05\n",
      "Step: 3200, train/epoch: 5.324459075927734\n",
      "Step: 3210, train/loss: 0.014299999922513962\n",
      "Step: 3210, train/grad_norm: 0.005341002717614174\n",
      "Step: 3210, train/learning_rate: 2.3294509446714073e-05\n",
      "Step: 3210, train/epoch: 5.341098308563232\n",
      "Step: 3220, train/loss: 0.002099999925121665\n",
      "Step: 3220, train/grad_norm: 10.235170364379883\n",
      "Step: 3220, train/learning_rate: 2.3211314328364097e-05\n",
      "Step: 3220, train/epoch: 5.357737064361572\n",
      "Step: 3230, train/loss: 0.013899999670684338\n",
      "Step: 3230, train/grad_norm: 8.696850776672363\n",
      "Step: 3230, train/learning_rate: 2.312811921001412e-05\n",
      "Step: 3230, train/epoch: 5.374375820159912\n",
      "Step: 3240, train/loss: 0.02250000089406967\n",
      "Step: 3240, train/grad_norm: 0.004470234736800194\n",
      "Step: 3240, train/learning_rate: 2.3044925910653546e-05\n",
      "Step: 3240, train/epoch: 5.39101505279541\n",
      "Step: 3250, train/loss: 0.0215000007301569\n",
      "Step: 3250, train/grad_norm: 0.006782008334994316\n",
      "Step: 3250, train/learning_rate: 2.296173079230357e-05\n",
      "Step: 3250, train/epoch: 5.40765380859375\n",
      "Step: 3260, train/loss: 0.025599999353289604\n",
      "Step: 3260, train/grad_norm: 0.05081252008676529\n",
      "Step: 3260, train/learning_rate: 2.2878535673953593e-05\n",
      "Step: 3260, train/epoch: 5.424293041229248\n",
      "Step: 3270, train/loss: 0.009399999864399433\n",
      "Step: 3270, train/grad_norm: 24.41499137878418\n",
      "Step: 3270, train/learning_rate: 2.2795340555603616e-05\n",
      "Step: 3270, train/epoch: 5.440931797027588\n",
      "Step: 3280, train/loss: 0.050999999046325684\n",
      "Step: 3280, train/grad_norm: 0.5155833959579468\n",
      "Step: 3280, train/learning_rate: 2.2712147256243043e-05\n",
      "Step: 3280, train/epoch: 5.457570552825928\n",
      "Step: 3290, train/loss: 0.02459999918937683\n",
      "Step: 3290, train/grad_norm: 10.510950088500977\n",
      "Step: 3290, train/learning_rate: 2.2628952137893066e-05\n",
      "Step: 3290, train/epoch: 5.474209785461426\n",
      "Step: 3300, train/loss: 0.026200000196695328\n",
      "Step: 3300, train/grad_norm: 0.8683910965919495\n",
      "Step: 3300, train/learning_rate: 2.254575701954309e-05\n",
      "Step: 3300, train/epoch: 5.490848541259766\n",
      "Step: 3310, train/loss: 0.02019999921321869\n",
      "Step: 3310, train/grad_norm: 0.14000166952610016\n",
      "Step: 3310, train/learning_rate: 2.2462561901193112e-05\n",
      "Step: 3310, train/epoch: 5.5074872970581055\n",
      "Step: 3320, train/loss: 0.004600000102072954\n",
      "Step: 3320, train/grad_norm: 0.009859277866780758\n",
      "Step: 3320, train/learning_rate: 2.237936860183254e-05\n",
      "Step: 3320, train/epoch: 5.5241265296936035\n",
      "Step: 3330, train/loss: 0.04809999838471413\n",
      "Step: 3330, train/grad_norm: 10.623098373413086\n",
      "Step: 3330, train/learning_rate: 2.2296173483482562e-05\n",
      "Step: 3330, train/epoch: 5.540765285491943\n",
      "Step: 3340, train/loss: 0.009499999694526196\n",
      "Step: 3340, train/grad_norm: 0.08828776329755783\n",
      "Step: 3340, train/learning_rate: 2.2212978365132585e-05\n",
      "Step: 3340, train/epoch: 5.557404518127441\n",
      "Step: 3350, train/loss: 0.045099999755620956\n",
      "Step: 3350, train/grad_norm: 0.06679019331932068\n",
      "Step: 3350, train/learning_rate: 2.212978324678261e-05\n",
      "Step: 3350, train/epoch: 5.574043273925781\n",
      "Step: 3360, train/loss: 0.020600000396370888\n",
      "Step: 3360, train/grad_norm: 0.9565596580505371\n",
      "Step: 3360, train/learning_rate: 2.204658812843263e-05\n",
      "Step: 3360, train/epoch: 5.590682029724121\n",
      "Step: 3370, train/loss: 0.009200000204145908\n",
      "Step: 3370, train/grad_norm: 19.564565658569336\n",
      "Step: 3370, train/learning_rate: 2.1963394829072058e-05\n",
      "Step: 3370, train/epoch: 5.607321262359619\n",
      "Step: 3380, train/loss: 0.03269999846816063\n",
      "Step: 3380, train/grad_norm: 10.224997520446777\n",
      "Step: 3380, train/learning_rate: 2.188019971072208e-05\n",
      "Step: 3380, train/epoch: 5.623960018157959\n",
      "Step: 3390, train/loss: 0.03689999878406525\n",
      "Step: 3390, train/grad_norm: 4.912190914154053\n",
      "Step: 3390, train/learning_rate: 2.1797004592372105e-05\n",
      "Step: 3390, train/epoch: 5.640598773956299\n",
      "Step: 3400, train/loss: 0.016699999570846558\n",
      "Step: 3400, train/grad_norm: 0.07849662750959396\n",
      "Step: 3400, train/learning_rate: 2.1713809474022128e-05\n",
      "Step: 3400, train/epoch: 5.657238006591797\n",
      "Step: 3410, train/loss: 0.011800000444054604\n",
      "Step: 3410, train/grad_norm: 0.05416234955191612\n",
      "Step: 3410, train/learning_rate: 2.1630616174661554e-05\n",
      "Step: 3410, train/epoch: 5.673876762390137\n",
      "Step: 3420, train/loss: 0.0142000000923872\n",
      "Step: 3420, train/grad_norm: 2.9926705360412598\n",
      "Step: 3420, train/learning_rate: 2.1547421056311578e-05\n",
      "Step: 3420, train/epoch: 5.690515995025635\n",
      "Step: 3430, train/loss: 0.016699999570846558\n",
      "Step: 3430, train/grad_norm: 0.017627043649554253\n",
      "Step: 3430, train/learning_rate: 2.14642259379616e-05\n",
      "Step: 3430, train/epoch: 5.707154750823975\n",
      "Step: 3440, train/loss: 0.02930000051856041\n",
      "Step: 3440, train/grad_norm: 15.88095760345459\n",
      "Step: 3440, train/learning_rate: 2.1381030819611624e-05\n",
      "Step: 3440, train/epoch: 5.7237935066223145\n",
      "Step: 3450, train/loss: 0.0272000003606081\n",
      "Step: 3450, train/grad_norm: 5.69188928604126\n",
      "Step: 3450, train/learning_rate: 2.129783752025105e-05\n",
      "Step: 3450, train/epoch: 5.7404327392578125\n",
      "Step: 3460, train/loss: 0.02160000056028366\n",
      "Step: 3460, train/grad_norm: 0.06096620112657547\n",
      "Step: 3460, train/learning_rate: 2.1214642401901074e-05\n",
      "Step: 3460, train/epoch: 5.757071495056152\n",
      "Step: 3470, train/loss: 0.045499999076128006\n",
      "Step: 3470, train/grad_norm: 47.02601623535156\n",
      "Step: 3470, train/learning_rate: 2.1131447283551097e-05\n",
      "Step: 3470, train/epoch: 5.773710250854492\n",
      "Step: 3480, train/loss: 0.03869999945163727\n",
      "Step: 3480, train/grad_norm: 4.184459686279297\n",
      "Step: 3480, train/learning_rate: 2.104825216520112e-05\n",
      "Step: 3480, train/epoch: 5.79034948348999\n",
      "Step: 3490, train/loss: 0.00559999980032444\n",
      "Step: 3490, train/grad_norm: 0.06245046108961105\n",
      "Step: 3490, train/learning_rate: 2.0965058865840547e-05\n",
      "Step: 3490, train/epoch: 5.80698823928833\n",
      "Step: 3500, train/loss: 0.027400000020861626\n",
      "Step: 3500, train/grad_norm: 0.020205985754728317\n",
      "Step: 3500, train/learning_rate: 2.088186374749057e-05\n",
      "Step: 3500, train/epoch: 5.823627471923828\n",
      "Step: 3510, train/loss: 0.045899998396635056\n",
      "Step: 3510, train/grad_norm: 17.393047332763672\n",
      "Step: 3510, train/learning_rate: 2.0798668629140593e-05\n",
      "Step: 3510, train/epoch: 5.840266227722168\n",
      "Step: 3520, train/loss: 0.03610000014305115\n",
      "Step: 3520, train/grad_norm: 0.02580263651907444\n",
      "Step: 3520, train/learning_rate: 2.0715473510790616e-05\n",
      "Step: 3520, train/epoch: 5.856904983520508\n",
      "Step: 3530, train/loss: 0.023499999195337296\n",
      "Step: 3530, train/grad_norm: 21.38166046142578\n",
      "Step: 3530, train/learning_rate: 2.0632280211430043e-05\n",
      "Step: 3530, train/epoch: 5.873544216156006\n",
      "Step: 3540, train/loss: 0.031599998474121094\n",
      "Step: 3540, train/grad_norm: 20.634292602539062\n",
      "Step: 3540, train/learning_rate: 2.0549085093080066e-05\n",
      "Step: 3540, train/epoch: 5.890182971954346\n",
      "Step: 3550, train/loss: 0.023399999365210533\n",
      "Step: 3550, train/grad_norm: 13.23369312286377\n",
      "Step: 3550, train/learning_rate: 2.046588997473009e-05\n",
      "Step: 3550, train/epoch: 5.9068217277526855\n",
      "Step: 3560, train/loss: 0.03400000184774399\n",
      "Step: 3560, train/grad_norm: 0.02986166812479496\n",
      "Step: 3560, train/learning_rate: 2.0382694856380112e-05\n",
      "Step: 3560, train/epoch: 5.923460960388184\n",
      "Step: 3570, train/loss: 0.03150000050663948\n",
      "Step: 3570, train/grad_norm: 0.7616087794303894\n",
      "Step: 3570, train/learning_rate: 2.029950155701954e-05\n",
      "Step: 3570, train/epoch: 5.940099716186523\n",
      "Step: 3580, train/loss: 0.0007999999797903001\n",
      "Step: 3580, train/grad_norm: 0.021918414160609245\n",
      "Step: 3580, train/learning_rate: 2.0216306438669562e-05\n",
      "Step: 3580, train/epoch: 5.9567389488220215\n",
      "Step: 3590, train/loss: 0.022299999371170998\n",
      "Step: 3590, train/grad_norm: 8.235747337341309\n",
      "Step: 3590, train/learning_rate: 2.0133111320319586e-05\n",
      "Step: 3590, train/epoch: 5.973377704620361\n",
      "Step: 3600, train/loss: 0.01759999990463257\n",
      "Step: 3600, train/grad_norm: 0.011420663446187973\n",
      "Step: 3600, train/learning_rate: 2.004991620196961e-05\n",
      "Step: 3600, train/epoch: 5.990016460418701\n",
      "Step: 3606, eval/loss: 0.8859734535217285\n",
      "Step: 3606, eval/accuracy: 0.8421490788459778\n",
      "Step: 3606, eval/f1: 0.8381661772727966\n",
      "Step: 3606, eval/runtime: 29.019100189208984\n",
      "Step: 3606, eval/samples_per_second: 248.21600341796875\n",
      "Step: 3606, eval/steps_per_second: 4.445000171661377\n",
      "Step: 3606, train/epoch: 6.0\n",
      "Step: 3610, train/loss: 0.01979999989271164\n",
      "Step: 3610, train/grad_norm: 1.4589790105819702\n",
      "Step: 3610, train/learning_rate: 1.9966722902609035e-05\n",
      "Step: 3610, train/epoch: 6.006655693054199\n",
      "Step: 3620, train/loss: 0.00570000009611249\n",
      "Step: 3620, train/grad_norm: 3.2454802989959717\n",
      "Step: 3620, train/learning_rate: 1.988352778425906e-05\n",
      "Step: 3620, train/epoch: 6.023294448852539\n",
      "Step: 3630, train/loss: 0.01860000006854534\n",
      "Step: 3630, train/grad_norm: 0.03108869306743145\n",
      "Step: 3630, train/learning_rate: 1.9800332665909082e-05\n",
      "Step: 3630, train/epoch: 6.039933681488037\n",
      "Step: 3640, train/loss: 0.011599999852478504\n",
      "Step: 3640, train/grad_norm: 0.023365886881947517\n",
      "Step: 3640, train/learning_rate: 1.9717137547559105e-05\n",
      "Step: 3640, train/epoch: 6.056572437286377\n",
      "Step: 3650, train/loss: 0.018699999898672104\n",
      "Step: 3650, train/grad_norm: 0.007795977406203747\n",
      "Step: 3650, train/learning_rate: 1.963394424819853e-05\n",
      "Step: 3650, train/epoch: 6.073211193084717\n",
      "Step: 3660, train/loss: 0.022600000724196434\n",
      "Step: 3660, train/grad_norm: 25.171998977661133\n",
      "Step: 3660, train/learning_rate: 1.9550749129848555e-05\n",
      "Step: 3660, train/epoch: 6.089850425720215\n",
      "Step: 3670, train/loss: 0.012900000438094139\n",
      "Step: 3670, train/grad_norm: 0.023105284199118614\n",
      "Step: 3670, train/learning_rate: 1.9467554011498578e-05\n",
      "Step: 3670, train/epoch: 6.106489181518555\n",
      "Step: 3680, train/loss: 0.010499999858438969\n",
      "Step: 3680, train/grad_norm: 6.162921905517578\n",
      "Step: 3680, train/learning_rate: 1.93843588931486e-05\n",
      "Step: 3680, train/epoch: 6.1231279373168945\n",
      "Step: 3690, train/loss: 0.013100000098347664\n",
      "Step: 3690, train/grad_norm: 6.162899971008301\n",
      "Step: 3690, train/learning_rate: 1.9301165593788028e-05\n",
      "Step: 3690, train/epoch: 6.139767169952393\n",
      "Step: 3700, train/loss: 0.011900000274181366\n",
      "Step: 3700, train/grad_norm: 0.4817131757736206\n",
      "Step: 3700, train/learning_rate: 1.921797047543805e-05\n",
      "Step: 3700, train/epoch: 6.156405925750732\n",
      "Step: 3710, train/loss: 0.010499999858438969\n",
      "Step: 3710, train/grad_norm: 0.0030303741805255413\n",
      "Step: 3710, train/learning_rate: 1.9134775357088074e-05\n",
      "Step: 3710, train/epoch: 6.1730451583862305\n",
      "Step: 3720, train/loss: 0.015699999406933784\n",
      "Step: 3720, train/grad_norm: 13.943599700927734\n",
      "Step: 3720, train/learning_rate: 1.9051580238738097e-05\n",
      "Step: 3720, train/epoch: 6.18968391418457\n",
      "Step: 3730, train/loss: 0.00930000003427267\n",
      "Step: 3730, train/grad_norm: 0.5864114761352539\n",
      "Step: 3730, train/learning_rate: 1.896838512038812e-05\n",
      "Step: 3730, train/epoch: 6.20632266998291\n",
      "Step: 3740, train/loss: 0.01119999960064888\n",
      "Step: 3740, train/grad_norm: 0.06478598713874817\n",
      "Step: 3740, train/learning_rate: 1.8885191821027547e-05\n",
      "Step: 3740, train/epoch: 6.222961902618408\n",
      "Step: 3750, train/loss: 0.0024999999441206455\n",
      "Step: 3750, train/grad_norm: 0.011449447833001614\n",
      "Step: 3750, train/learning_rate: 1.880199670267757e-05\n",
      "Step: 3750, train/epoch: 6.239600658416748\n",
      "Step: 3760, train/loss: 0.022199999541044235\n",
      "Step: 3760, train/grad_norm: 19.232988357543945\n",
      "Step: 3760, train/learning_rate: 1.8718801584327593e-05\n",
      "Step: 3760, train/epoch: 6.256239414215088\n",
      "Step: 3770, train/loss: 0.02290000021457672\n",
      "Step: 3770, train/grad_norm: 0.14174455404281616\n",
      "Step: 3770, train/learning_rate: 1.8635606465977617e-05\n",
      "Step: 3770, train/epoch: 6.272878646850586\n",
      "Step: 3780, train/loss: 0.006800000090152025\n",
      "Step: 3780, train/grad_norm: 0.005035826470702887\n",
      "Step: 3780, train/learning_rate: 1.8552413166617043e-05\n",
      "Step: 3780, train/epoch: 6.289517402648926\n",
      "Step: 3790, train/loss: 0.022099999710917473\n",
      "Step: 3790, train/grad_norm: 13.53853702545166\n",
      "Step: 3790, train/learning_rate: 1.8469218048267066e-05\n",
      "Step: 3790, train/epoch: 6.306156635284424\n",
      "Step: 3800, train/loss: 0.007699999958276749\n",
      "Step: 3800, train/grad_norm: 6.968522548675537\n",
      "Step: 3800, train/learning_rate: 1.838602292991709e-05\n",
      "Step: 3800, train/epoch: 6.322795391082764\n",
      "Step: 3810, train/loss: 0.01119999960064888\n",
      "Step: 3810, train/grad_norm: 0.06927599012851715\n",
      "Step: 3810, train/learning_rate: 1.8302827811567113e-05\n",
      "Step: 3810, train/epoch: 6.3394341468811035\n",
      "Step: 3820, train/loss: 0.021400000900030136\n",
      "Step: 3820, train/grad_norm: 9.979818344116211\n",
      "Step: 3820, train/learning_rate: 1.821963451220654e-05\n",
      "Step: 3820, train/epoch: 6.356073379516602\n",
      "Step: 3830, train/loss: 0.005799999926239252\n",
      "Step: 3830, train/grad_norm: 0.00756998872384429\n",
      "Step: 3830, train/learning_rate: 1.8136439393856563e-05\n",
      "Step: 3830, train/epoch: 6.372712135314941\n",
      "Step: 3840, train/loss: 0.04659999907016754\n",
      "Step: 3840, train/grad_norm: 24.988143920898438\n",
      "Step: 3840, train/learning_rate: 1.8053244275506586e-05\n",
      "Step: 3840, train/epoch: 6.389350891113281\n",
      "Step: 3850, train/loss: 0.014999999664723873\n",
      "Step: 3850, train/grad_norm: 30.985633850097656\n",
      "Step: 3850, train/learning_rate: 1.797004915715661e-05\n",
      "Step: 3850, train/epoch: 6.405990123748779\n",
      "Step: 3860, train/loss: 0.030500000342726707\n",
      "Step: 3860, train/grad_norm: 7.524229049682617\n",
      "Step: 3860, train/learning_rate: 1.7886855857796036e-05\n",
      "Step: 3860, train/epoch: 6.422628879547119\n",
      "Step: 3870, train/loss: 0.0027000000700354576\n",
      "Step: 3870, train/grad_norm: 0.006462813355028629\n",
      "Step: 3870, train/learning_rate: 1.780366073944606e-05\n",
      "Step: 3870, train/epoch: 6.439268112182617\n",
      "Step: 3880, train/loss: 0.024800000712275505\n",
      "Step: 3880, train/grad_norm: 0.10277259349822998\n",
      "Step: 3880, train/learning_rate: 1.7720465621096082e-05\n",
      "Step: 3880, train/epoch: 6.455906867980957\n",
      "Step: 3890, train/loss: 0.007600000128149986\n",
      "Step: 3890, train/grad_norm: 0.005647964775562286\n",
      "Step: 3890, train/learning_rate: 1.7637270502746105e-05\n",
      "Step: 3890, train/epoch: 6.472545623779297\n",
      "Step: 3900, train/loss: 0.002400000113993883\n",
      "Step: 3900, train/grad_norm: 0.19090142846107483\n",
      "Step: 3900, train/learning_rate: 1.7554077203385532e-05\n",
      "Step: 3900, train/epoch: 6.489184856414795\n",
      "Step: 3910, train/loss: 0.004900000058114529\n",
      "Step: 3910, train/grad_norm: 1.7172479629516602\n",
      "Step: 3910, train/learning_rate: 1.7470882085035555e-05\n",
      "Step: 3910, train/epoch: 6.505823612213135\n",
      "Step: 3920, train/loss: 0.00839999970048666\n",
      "Step: 3920, train/grad_norm: 0.20972050726413727\n",
      "Step: 3920, train/learning_rate: 1.7387686966685578e-05\n",
      "Step: 3920, train/epoch: 6.522462368011475\n",
      "Step: 3930, train/loss: 0.02160000056028366\n",
      "Step: 3930, train/grad_norm: 1.1224076747894287\n",
      "Step: 3930, train/learning_rate: 1.73044918483356e-05\n",
      "Step: 3930, train/epoch: 6.539101600646973\n",
      "Step: 3940, train/loss: 0.03139999881386757\n",
      "Step: 3940, train/grad_norm: 32.22880172729492\n",
      "Step: 3940, train/learning_rate: 1.7221298548975028e-05\n",
      "Step: 3940, train/epoch: 6.5557403564453125\n",
      "Step: 3950, train/loss: 0.056699998676776886\n",
      "Step: 3950, train/grad_norm: 5.2703633308410645\n",
      "Step: 3950, train/learning_rate: 1.713810343062505e-05\n",
      "Step: 3950, train/epoch: 6.5723795890808105\n",
      "Step: 3960, train/loss: 0.01640000008046627\n",
      "Step: 3960, train/grad_norm: 0.01597481407225132\n",
      "Step: 3960, train/learning_rate: 1.7054908312275074e-05\n",
      "Step: 3960, train/epoch: 6.58901834487915\n",
      "Step: 3970, train/loss: 0.018300000578165054\n",
      "Step: 3970, train/grad_norm: 0.005597148090600967\n",
      "Step: 3970, train/learning_rate: 1.6971713193925098e-05\n",
      "Step: 3970, train/epoch: 6.60565710067749\n",
      "Step: 3980, train/loss: 0.0003000000142492354\n",
      "Step: 3980, train/grad_norm: 0.007811917923390865\n",
      "Step: 3980, train/learning_rate: 1.6888519894564524e-05\n",
      "Step: 3980, train/epoch: 6.622296333312988\n",
      "Step: 3990, train/loss: 0.014800000004470348\n",
      "Step: 3990, train/grad_norm: 33.45507049560547\n",
      "Step: 3990, train/learning_rate: 1.6805324776214547e-05\n",
      "Step: 3990, train/epoch: 6.638935089111328\n",
      "Step: 4000, train/loss: 0.004600000102072954\n",
      "Step: 4000, train/grad_norm: 0.0020865323022007942\n",
      "Step: 4000, train/learning_rate: 1.672212965786457e-05\n",
      "Step: 4000, train/epoch: 6.655573844909668\n",
      "Step: 4010, train/loss: 0.0035000001080334187\n",
      "Step: 4010, train/grad_norm: 0.01877662166953087\n",
      "Step: 4010, train/learning_rate: 1.6638934539514594e-05\n",
      "Step: 4010, train/epoch: 6.672213077545166\n",
      "Step: 4020, train/loss: 0.000699999975040555\n",
      "Step: 4020, train/grad_norm: 8.247695922851562\n",
      "Step: 4020, train/learning_rate: 1.655574124015402e-05\n",
      "Step: 4020, train/epoch: 6.688851833343506\n",
      "Step: 4030, train/loss: 0.00860000029206276\n",
      "Step: 4030, train/grad_norm: 18.706422805786133\n",
      "Step: 4030, train/learning_rate: 1.6472546121804044e-05\n",
      "Step: 4030, train/epoch: 6.705491065979004\n",
      "Step: 4040, train/loss: 0.03009999915957451\n",
      "Step: 4040, train/grad_norm: 0.028491301462054253\n",
      "Step: 4040, train/learning_rate: 1.6389351003454067e-05\n",
      "Step: 4040, train/epoch: 6.722129821777344\n",
      "Step: 4050, train/loss: 0.016499999910593033\n",
      "Step: 4050, train/grad_norm: 0.0023361819330602884\n",
      "Step: 4050, train/learning_rate: 1.630615588510409e-05\n",
      "Step: 4050, train/epoch: 6.738768577575684\n",
      "Step: 4060, train/loss: 0.00019999999494757503\n",
      "Step: 4060, train/grad_norm: 0.0013903521467000246\n",
      "Step: 4060, train/learning_rate: 1.6222962585743517e-05\n",
      "Step: 4060, train/epoch: 6.755407810211182\n",
      "Step: 4070, train/loss: 0.01730000041425228\n",
      "Step: 4070, train/grad_norm: 0.08091459423303604\n",
      "Step: 4070, train/learning_rate: 1.613976746739354e-05\n",
      "Step: 4070, train/epoch: 6.7720465660095215\n",
      "Step: 4080, train/loss: 0.0348999984562397\n",
      "Step: 4080, train/grad_norm: 0.08336646854877472\n",
      "Step: 4080, train/learning_rate: 1.6056572349043563e-05\n",
      "Step: 4080, train/epoch: 6.788685321807861\n",
      "Step: 4090, train/loss: 0.006399999838322401\n",
      "Step: 4090, train/grad_norm: 1.6267248392105103\n",
      "Step: 4090, train/learning_rate: 1.5973377230693586e-05\n",
      "Step: 4090, train/epoch: 6.805324554443359\n",
      "Step: 4100, train/loss: 0.005799999926239252\n",
      "Step: 4100, train/grad_norm: 1.8158373832702637\n",
      "Step: 4100, train/learning_rate: 1.5890183931333013e-05\n",
      "Step: 4100, train/epoch: 6.821963310241699\n",
      "Step: 4110, train/loss: 0.027699999511241913\n",
      "Step: 4110, train/grad_norm: 0.661661684513092\n",
      "Step: 4110, train/learning_rate: 1.5806988812983036e-05\n",
      "Step: 4110, train/epoch: 6.838602542877197\n",
      "Step: 4120, train/loss: 0.07159999758005142\n",
      "Step: 4120, train/grad_norm: 0.31173521280288696\n",
      "Step: 4120, train/learning_rate: 1.572379369463306e-05\n",
      "Step: 4120, train/epoch: 6.855241298675537\n",
      "Step: 4130, train/loss: 0.013500000350177288\n",
      "Step: 4130, train/grad_norm: 13.907267570495605\n",
      "Step: 4130, train/learning_rate: 1.5640598576283082e-05\n",
      "Step: 4130, train/epoch: 6.871880054473877\n",
      "Step: 4140, train/loss: 0.0005000000237487257\n",
      "Step: 4140, train/grad_norm: 0.0211899783462286\n",
      "Step: 4140, train/learning_rate: 1.5557403457933106e-05\n",
      "Step: 4140, train/epoch: 6.888519287109375\n",
      "Step: 4150, train/loss: 0.0015999999595806003\n",
      "Step: 4150, train/grad_norm: 0.8616181015968323\n",
      "Step: 4150, train/learning_rate: 1.5474210158572532e-05\n",
      "Step: 4150, train/epoch: 6.905158042907715\n",
      "Step: 4160, train/loss: 0.009399999864399433\n",
      "Step: 4160, train/grad_norm: 26.702816009521484\n",
      "Step: 4160, train/learning_rate: 1.5391015040222555e-05\n",
      "Step: 4160, train/epoch: 6.921796798706055\n",
      "Step: 4170, train/loss: 0.0013000000035390258\n",
      "Step: 4170, train/grad_norm: 0.0673414021730423\n",
      "Step: 4170, train/learning_rate: 1.530781992187258e-05\n",
      "Step: 4170, train/epoch: 6.938436031341553\n",
      "Step: 4180, train/loss: 0.013399999588727951\n",
      "Step: 4180, train/grad_norm: 7.491925239562988\n",
      "Step: 4180, train/learning_rate: 1.5224625713017303e-05\n",
      "Step: 4180, train/epoch: 6.955074787139893\n",
      "Step: 4190, train/loss: 0.014600000344216824\n",
      "Step: 4190, train/grad_norm: 1.134876012802124\n",
      "Step: 4190, train/learning_rate: 1.5141430594667327e-05\n",
      "Step: 4190, train/epoch: 6.971714019775391\n",
      "Step: 4200, train/loss: 0.0364999994635582\n",
      "Step: 4200, train/grad_norm: 0.03521353378891945\n",
      "Step: 4200, train/learning_rate: 1.5058236385812052e-05\n",
      "Step: 4200, train/epoch: 6.9883527755737305\n",
      "Step: 4207, eval/loss: 1.6636353731155396\n",
      "Step: 4207, eval/accuracy: 0.7738442420959473\n",
      "Step: 4207, eval/f1: 0.7735626697540283\n",
      "Step: 4207, eval/runtime: 29.122900009155273\n",
      "Step: 4207, eval/samples_per_second: 247.33099365234375\n",
      "Step: 4207, eval/steps_per_second: 4.429999828338623\n",
      "Step: 4207, train/epoch: 7.0\n",
      "Step: 4210, train/loss: 0.021400000900030136\n",
      "Step: 4210, train/grad_norm: 12.848502159118652\n",
      "Step: 4210, train/learning_rate: 1.4975041267462075e-05\n",
      "Step: 4210, train/epoch: 7.00499153137207\n",
      "Step: 4220, train/loss: 0.01590000092983246\n",
      "Step: 4220, train/grad_norm: 1.0461546182632446\n",
      "Step: 4220, train/learning_rate: 1.48918470586068e-05\n",
      "Step: 4220, train/epoch: 7.021630764007568\n",
      "Step: 4230, train/loss: 0.0017000000225380063\n",
      "Step: 4230, train/grad_norm: 7.56020450592041\n",
      "Step: 4230, train/learning_rate: 1.4808651940256823e-05\n",
      "Step: 4230, train/epoch: 7.038269519805908\n",
      "Step: 4240, train/loss: 0.051100000739097595\n",
      "Step: 4240, train/grad_norm: 0.13341370224952698\n",
      "Step: 4240, train/learning_rate: 1.4725457731401548e-05\n",
      "Step: 4240, train/epoch: 7.054908275604248\n",
      "Step: 4250, train/loss: 0.013500000350177288\n",
      "Step: 4250, train/grad_norm: 0.17776831984519958\n",
      "Step: 4250, train/learning_rate: 1.4642262613051571e-05\n",
      "Step: 4250, train/epoch: 7.071547508239746\n",
      "Step: 4260, train/loss: 0.00419999985024333\n",
      "Step: 4260, train/grad_norm: 8.682639122009277\n",
      "Step: 4260, train/learning_rate: 1.4559068404196296e-05\n",
      "Step: 4260, train/epoch: 7.088186264038086\n",
      "Step: 4270, train/loss: 0.038600001484155655\n",
      "Step: 4270, train/grad_norm: 0.07947897911071777\n",
      "Step: 4270, train/learning_rate: 1.4475873285846319e-05\n",
      "Step: 4270, train/epoch: 7.104825496673584\n",
      "Step: 4280, train/loss: 0.006500000134110451\n",
      "Step: 4280, train/grad_norm: 0.0063033862970769405\n",
      "Step: 4280, train/learning_rate: 1.4392679076991044e-05\n",
      "Step: 4280, train/epoch: 7.121464252471924\n",
      "Step: 4290, train/loss: 0.013500000350177288\n",
      "Step: 4290, train/grad_norm: 0.003598592709749937\n",
      "Step: 4290, train/learning_rate: 1.4309483958641067e-05\n",
      "Step: 4290, train/epoch: 7.138103008270264\n",
      "Step: 4300, train/loss: 0.021400000900030136\n",
      "Step: 4300, train/grad_norm: 0.0036222811322659254\n",
      "Step: 4300, train/learning_rate: 1.4226289749785792e-05\n",
      "Step: 4300, train/epoch: 7.154742240905762\n",
      "Step: 4310, train/loss: 0.003000000026077032\n",
      "Step: 4310, train/grad_norm: 0.015276630409061909\n",
      "Step: 4310, train/learning_rate: 1.4143094631435815e-05\n",
      "Step: 4310, train/epoch: 7.171380996704102\n",
      "Step: 4320, train/loss: 0.003100000089034438\n",
      "Step: 4320, train/grad_norm: 0.0037116617895662785\n",
      "Step: 4320, train/learning_rate: 1.405990042258054e-05\n",
      "Step: 4320, train/epoch: 7.188019752502441\n",
      "Step: 4330, train/loss: 0.0031999999191612005\n",
      "Step: 4330, train/grad_norm: 0.0020294305868446827\n",
      "Step: 4330, train/learning_rate: 1.3976705304230563e-05\n",
      "Step: 4330, train/epoch: 7.2046589851379395\n",
      "Step: 4340, train/loss: 0.013299999758601189\n",
      "Step: 4340, train/grad_norm: 0.11674502491950989\n",
      "Step: 4340, train/learning_rate: 1.3893511095375288e-05\n",
      "Step: 4340, train/epoch: 7.221297740936279\n",
      "Step: 4350, train/loss: 0.00559999980032444\n",
      "Step: 4350, train/grad_norm: 0.00898382905870676\n",
      "Step: 4350, train/learning_rate: 1.3810315977025311e-05\n",
      "Step: 4350, train/epoch: 7.237936973571777\n",
      "Step: 4360, train/loss: 0.01940000057220459\n",
      "Step: 4360, train/grad_norm: 0.07103904336690903\n",
      "Step: 4360, train/learning_rate: 1.3727121768170036e-05\n",
      "Step: 4360, train/epoch: 7.254575729370117\n",
      "Step: 4370, train/loss: 9.999999747378752e-05\n",
      "Step: 4370, train/grad_norm: 0.1535978764295578\n",
      "Step: 4370, train/learning_rate: 1.364392664982006e-05\n",
      "Step: 4370, train/epoch: 7.271214485168457\n",
      "Step: 4380, train/loss: 0.017500000074505806\n",
      "Step: 4380, train/grad_norm: 0.010206697508692741\n",
      "Step: 4380, train/learning_rate: 1.3560732440964784e-05\n",
      "Step: 4380, train/epoch: 7.287853717803955\n",
      "Step: 4390, train/loss: 0.004900000058114529\n",
      "Step: 4390, train/grad_norm: 25.478300094604492\n",
      "Step: 4390, train/learning_rate: 1.3477537322614808e-05\n",
      "Step: 4390, train/epoch: 7.304492473602295\n",
      "Step: 4400, train/loss: 0.008200000040233135\n",
      "Step: 4400, train/grad_norm: 0.004206066485494375\n",
      "Step: 4400, train/learning_rate: 1.3394343113759533e-05\n",
      "Step: 4400, train/epoch: 7.321131229400635\n",
      "Step: 4410, train/loss: 9.999999747378752e-05\n",
      "Step: 4410, train/grad_norm: 0.062411002814769745\n",
      "Step: 4410, train/learning_rate: 1.3311147995409556e-05\n",
      "Step: 4410, train/epoch: 7.337770462036133\n",
      "Step: 4420, train/loss: 0.016899999231100082\n",
      "Step: 4420, train/grad_norm: 0.01371905766427517\n",
      "Step: 4420, train/learning_rate: 1.322795378655428e-05\n",
      "Step: 4420, train/epoch: 7.354409217834473\n",
      "Step: 4430, train/loss: 0.0003000000142492354\n",
      "Step: 4430, train/grad_norm: 0.0031194835901260376\n",
      "Step: 4430, train/learning_rate: 1.3144758668204304e-05\n",
      "Step: 4430, train/epoch: 7.371048450469971\n",
      "Step: 4440, train/loss: 0.011099999770522118\n",
      "Step: 4440, train/grad_norm: 0.02021120674908161\n",
      "Step: 4440, train/learning_rate: 1.3061564459349029e-05\n",
      "Step: 4440, train/epoch: 7.3876872062683105\n",
      "Step: 4450, train/loss: 0.007499999832361937\n",
      "Step: 4450, train/grad_norm: 9.653006553649902\n",
      "Step: 4450, train/learning_rate: 1.2978369340999052e-05\n",
      "Step: 4450, train/epoch: 7.40432596206665\n",
      "Step: 4460, train/loss: 0.01640000008046627\n",
      "Step: 4460, train/grad_norm: 0.1288277506828308\n",
      "Step: 4460, train/learning_rate: 1.2895175132143777e-05\n",
      "Step: 4460, train/epoch: 7.420965194702148\n",
      "Step: 4470, train/loss: 0.03660000115633011\n",
      "Step: 4470, train/grad_norm: 9.593704223632812\n",
      "Step: 4470, train/learning_rate: 1.28119800137938e-05\n",
      "Step: 4470, train/epoch: 7.437603950500488\n",
      "Step: 4480, train/loss: 0.01979999989271164\n",
      "Step: 4480, train/grad_norm: 10.288361549377441\n",
      "Step: 4480, train/learning_rate: 1.2728785804938525e-05\n",
      "Step: 4480, train/epoch: 7.454242706298828\n",
      "Step: 4490, train/loss: 0.0012000000569969416\n",
      "Step: 4490, train/grad_norm: 1.0933964252471924\n",
      "Step: 4490, train/learning_rate: 1.2645590686588548e-05\n",
      "Step: 4490, train/epoch: 7.470881938934326\n",
      "Step: 4500, train/loss: 0.008700000122189522\n",
      "Step: 4500, train/grad_norm: 0.007710547186434269\n",
      "Step: 4500, train/learning_rate: 1.2562395568238571e-05\n",
      "Step: 4500, train/epoch: 7.487520694732666\n",
      "Step: 4510, train/loss: 0.0005000000237487257\n",
      "Step: 4510, train/grad_norm: 0.034365300089120865\n",
      "Step: 4510, train/learning_rate: 1.2479201359383296e-05\n",
      "Step: 4510, train/epoch: 7.504159927368164\n",
      "Step: 4520, train/loss: 0.01640000008046627\n",
      "Step: 4520, train/grad_norm: 22.28125762939453\n",
      "Step: 4520, train/learning_rate: 1.239600624103332e-05\n",
      "Step: 4520, train/epoch: 7.520798683166504\n",
      "Step: 4530, train/loss: 0.015399999916553497\n",
      "Step: 4530, train/grad_norm: 18.134437561035156\n",
      "Step: 4530, train/learning_rate: 1.2312812032178044e-05\n",
      "Step: 4530, train/epoch: 7.537437438964844\n",
      "Step: 4540, train/loss: 0.009999999776482582\n",
      "Step: 4540, train/grad_norm: 0.004080198705196381\n",
      "Step: 4540, train/learning_rate: 1.2229616913828067e-05\n",
      "Step: 4540, train/epoch: 7.554076671600342\n",
      "Step: 4550, train/loss: 0.01679999940097332\n",
      "Step: 4550, train/grad_norm: 0.004280250985175371\n",
      "Step: 4550, train/learning_rate: 1.2146422704972792e-05\n",
      "Step: 4550, train/epoch: 7.570715427398682\n",
      "Step: 4560, train/loss: 0.01119999960064888\n",
      "Step: 4560, train/grad_norm: 0.0028073196299374104\n",
      "Step: 4560, train/learning_rate: 1.2063227586622816e-05\n",
      "Step: 4560, train/epoch: 7.5873541831970215\n",
      "Step: 4570, train/loss: 0.0003000000142492354\n",
      "Step: 4570, train/grad_norm: 0.2795429825782776\n",
      "Step: 4570, train/learning_rate: 1.198003337776754e-05\n",
      "Step: 4570, train/epoch: 7.6039934158325195\n",
      "Step: 4580, train/loss: 0.015599999576807022\n",
      "Step: 4580, train/grad_norm: 0.026183094829320908\n",
      "Step: 4580, train/learning_rate: 1.1896838259417564e-05\n",
      "Step: 4580, train/epoch: 7.620632171630859\n",
      "Step: 4590, train/loss: 0.013799999840557575\n",
      "Step: 4590, train/grad_norm: 0.012471009977161884\n",
      "Step: 4590, train/learning_rate: 1.1813644050562289e-05\n",
      "Step: 4590, train/epoch: 7.637271404266357\n",
      "Step: 4600, train/loss: 0.0003000000142492354\n",
      "Step: 4600, train/grad_norm: 0.0034528966061770916\n",
      "Step: 4600, train/learning_rate: 1.1730448932212312e-05\n",
      "Step: 4600, train/epoch: 7.653910160064697\n",
      "Step: 4610, train/loss: 0.006399999838322401\n",
      "Step: 4610, train/grad_norm: 0.0025654269848018885\n",
      "Step: 4610, train/learning_rate: 1.1647254723357037e-05\n",
      "Step: 4610, train/epoch: 7.670548915863037\n",
      "Step: 4620, train/loss: 0.0006000000284984708\n",
      "Step: 4620, train/grad_norm: 8.983901023864746\n",
      "Step: 4620, train/learning_rate: 1.156405960500706e-05\n",
      "Step: 4620, train/epoch: 7.687188148498535\n",
      "Step: 4630, train/loss: 0.000699999975040555\n",
      "Step: 4630, train/grad_norm: 0.11708751320838928\n",
      "Step: 4630, train/learning_rate: 1.1480865396151785e-05\n",
      "Step: 4630, train/epoch: 7.703826904296875\n",
      "Step: 4640, train/loss: 9.999999747378752e-05\n",
      "Step: 4640, train/grad_norm: 0.0021521630696952343\n",
      "Step: 4640, train/learning_rate: 1.1397670277801808e-05\n",
      "Step: 4640, train/epoch: 7.720465660095215\n",
      "Step: 4650, train/loss: 0.00019999999494757503\n",
      "Step: 4650, train/grad_norm: 0.14395485818386078\n",
      "Step: 4650, train/learning_rate: 1.1314476068946533e-05\n",
      "Step: 4650, train/epoch: 7.737104892730713\n",
      "Step: 4660, train/loss: 0.019600000232458115\n",
      "Step: 4660, train/grad_norm: 0.04312233254313469\n",
      "Step: 4660, train/learning_rate: 1.1231280950596556e-05\n",
      "Step: 4660, train/epoch: 7.753743648529053\n",
      "Step: 4670, train/loss: 0.01720000058412552\n",
      "Step: 4670, train/grad_norm: 14.118322372436523\n",
      "Step: 4670, train/learning_rate: 1.1148086741741281e-05\n",
      "Step: 4670, train/epoch: 7.770382881164551\n",
      "Step: 4680, train/loss: 0.00019999999494757503\n",
      "Step: 4680, train/grad_norm: 0.06538905203342438\n",
      "Step: 4680, train/learning_rate: 1.1064891623391304e-05\n",
      "Step: 4680, train/epoch: 7.787021636962891\n",
      "Step: 4690, train/loss: 9.999999747378752e-05\n",
      "Step: 4690, train/grad_norm: 0.3297805190086365\n",
      "Step: 4690, train/learning_rate: 1.0981697414536029e-05\n",
      "Step: 4690, train/epoch: 7.8036603927612305\n",
      "Step: 4700, train/loss: 0.010599999688565731\n",
      "Step: 4700, train/grad_norm: 0.001532206078991294\n",
      "Step: 4700, train/learning_rate: 1.0898502296186052e-05\n",
      "Step: 4700, train/epoch: 7.8202996253967285\n",
      "Step: 4710, train/loss: 0.019999999552965164\n",
      "Step: 4710, train/grad_norm: 0.003242383012548089\n",
      "Step: 4710, train/learning_rate: 1.0815308087330777e-05\n",
      "Step: 4710, train/epoch: 7.836938381195068\n",
      "Step: 4720, train/loss: 9.999999747378752e-05\n",
      "Step: 4720, train/grad_norm: 0.0033411155454814434\n",
      "Step: 4720, train/learning_rate: 1.07321129689808e-05\n",
      "Step: 4720, train/epoch: 7.853577136993408\n",
      "Step: 4730, train/loss: 0.001500000013038516\n",
      "Step: 4730, train/grad_norm: 0.0045691183768212795\n",
      "Step: 4730, train/learning_rate: 1.0648918760125525e-05\n",
      "Step: 4730, train/epoch: 7.870216369628906\n",
      "Step: 4740, train/loss: 0.0005000000237487257\n",
      "Step: 4740, train/grad_norm: 0.010893233120441437\n",
      "Step: 4740, train/learning_rate: 1.0565723641775548e-05\n",
      "Step: 4740, train/epoch: 7.886855125427246\n",
      "Step: 4750, train/loss: 0.003000000026077032\n",
      "Step: 4750, train/grad_norm: 17.37544059753418\n",
      "Step: 4750, train/learning_rate: 1.0482529432920273e-05\n",
      "Step: 4750, train/epoch: 7.903494358062744\n",
      "Step: 4760, train/loss: 0.0\n",
      "Step: 4760, train/grad_norm: 0.012796732597053051\n",
      "Step: 4760, train/learning_rate: 1.0399334314570297e-05\n",
      "Step: 4760, train/epoch: 7.920133113861084\n",
      "Step: 4770, train/loss: 0.0005000000237487257\n",
      "Step: 4770, train/grad_norm: 0.007261449936777353\n",
      "Step: 4770, train/learning_rate: 1.0316140105715021e-05\n",
      "Step: 4770, train/epoch: 7.936771869659424\n",
      "Step: 4780, train/loss: 0.0010999999940395355\n",
      "Step: 4780, train/grad_norm: 0.009885632432997227\n",
      "Step: 4780, train/learning_rate: 1.0232944987365045e-05\n",
      "Step: 4780, train/epoch: 7.953411102294922\n",
      "Step: 4790, train/loss: 0.0008999999845400453\n",
      "Step: 4790, train/grad_norm: 0.0007861376507207751\n",
      "Step: 4790, train/learning_rate: 1.014975077850977e-05\n",
      "Step: 4790, train/epoch: 7.970049858093262\n",
      "Step: 4800, train/loss: 0.010200000368058681\n",
      "Step: 4800, train/grad_norm: 22.1331844329834\n",
      "Step: 4800, train/learning_rate: 1.0066555660159793e-05\n",
      "Step: 4800, train/epoch: 7.986688613891602\n",
      "Step: 4808, eval/loss: 1.0171829462051392\n",
      "Step: 4808, eval/accuracy: 0.86269611120224\n",
      "Step: 4808, eval/f1: 0.8578797578811646\n",
      "Step: 4808, eval/runtime: 29.155500411987305\n",
      "Step: 4808, eval/samples_per_second: 247.0540008544922\n",
      "Step: 4808, eval/steps_per_second: 4.425000190734863\n",
      "Step: 4808, train/epoch: 8.0\n",
      "Step: 4810, train/loss: 0.007000000216066837\n",
      "Step: 4810, train/grad_norm: 0.002401919336989522\n",
      "Step: 4810, train/learning_rate: 9.983361451304518e-06\n",
      "Step: 4810, train/epoch: 8.003327369689941\n",
      "Step: 4820, train/loss: 9.999999747378752e-05\n",
      "Step: 4820, train/grad_norm: 0.006980856414884329\n",
      "Step: 4820, train/learning_rate: 9.900166332954541e-06\n",
      "Step: 4820, train/epoch: 8.019967079162598\n",
      "Step: 4830, train/loss: 0.0015999999595806003\n",
      "Step: 4830, train/grad_norm: 0.0063322000205516815\n",
      "Step: 4830, train/learning_rate: 9.816972124099266e-06\n",
      "Step: 4830, train/epoch: 8.036605834960938\n",
      "Step: 4840, train/loss: 0.0003000000142492354\n",
      "Step: 4840, train/grad_norm: 0.004772533196955919\n",
      "Step: 4840, train/learning_rate: 9.733777005749289e-06\n",
      "Step: 4840, train/epoch: 8.053244590759277\n",
      "Step: 4850, train/loss: 0.0013000000035390258\n",
      "Step: 4850, train/grad_norm: 22.39468765258789\n",
      "Step: 4850, train/learning_rate: 9.650582796894014e-06\n",
      "Step: 4850, train/epoch: 8.069883346557617\n",
      "Step: 4860, train/loss: 0.008700000122189522\n",
      "Step: 4860, train/grad_norm: 0.0727904662489891\n",
      "Step: 4860, train/learning_rate: 9.567387678544037e-06\n",
      "Step: 4860, train/epoch: 8.086522102355957\n",
      "Step: 4870, train/loss: 9.999999747378752e-05\n",
      "Step: 4870, train/grad_norm: 0.0020708832889795303\n",
      "Step: 4870, train/learning_rate: 9.48419256019406e-06\n",
      "Step: 4870, train/epoch: 8.103161811828613\n",
      "Step: 4880, train/loss: 9.999999747378752e-05\n",
      "Step: 4880, train/grad_norm: 0.030392035841941833\n",
      "Step: 4880, train/learning_rate: 9.400998351338785e-06\n",
      "Step: 4880, train/epoch: 8.119800567626953\n",
      "Step: 4890, train/loss: 0.0010000000474974513\n",
      "Step: 4890, train/grad_norm: 9.026091575622559\n",
      "Step: 4890, train/learning_rate: 9.317803232988808e-06\n",
      "Step: 4890, train/epoch: 8.136439323425293\n",
      "Step: 4900, train/loss: 0.005900000222027302\n",
      "Step: 4900, train/grad_norm: 0.003368866164237261\n",
      "Step: 4900, train/learning_rate: 9.234609024133533e-06\n",
      "Step: 4900, train/epoch: 8.153078079223633\n",
      "Step: 4910, train/loss: 0.01080000028014183\n",
      "Step: 4910, train/grad_norm: 0.233287051320076\n",
      "Step: 4910, train/learning_rate: 9.151413905783556e-06\n",
      "Step: 4910, train/epoch: 8.169716835021973\n",
      "Step: 4920, train/loss: 0.001500000013038516\n",
      "Step: 4920, train/grad_norm: 0.20771144330501556\n",
      "Step: 4920, train/learning_rate: 9.068219696928281e-06\n",
      "Step: 4920, train/epoch: 8.186356544494629\n",
      "Step: 4930, train/loss: 0.0\n",
      "Step: 4930, train/grad_norm: 0.0009588173124939203\n",
      "Step: 4930, train/learning_rate: 8.985024578578304e-06\n",
      "Step: 4930, train/epoch: 8.202995300292969\n",
      "Step: 4940, train/loss: 0.010400000028312206\n",
      "Step: 4940, train/grad_norm: 0.013736930675804615\n",
      "Step: 4940, train/learning_rate: 8.90183036972303e-06\n",
      "Step: 4940, train/epoch: 8.219634056091309\n",
      "Step: 4950, train/loss: 9.999999747378752e-05\n",
      "Step: 4950, train/grad_norm: 0.06405317038297653\n",
      "Step: 4950, train/learning_rate: 8.818635251373053e-06\n",
      "Step: 4950, train/epoch: 8.236272811889648\n",
      "Step: 4960, train/loss: 0.016200000420212746\n",
      "Step: 4960, train/grad_norm: 0.07677968591451645\n",
      "Step: 4960, train/learning_rate: 8.735441042517778e-06\n",
      "Step: 4960, train/epoch: 8.252911567687988\n",
      "Step: 4970, train/loss: 0.0010999999940395355\n",
      "Step: 4970, train/grad_norm: 9.1604585647583\n",
      "Step: 4970, train/learning_rate: 8.6522459241678e-06\n",
      "Step: 4970, train/epoch: 8.269550323486328\n",
      "Step: 4980, train/loss: 0.007699999958276749\n",
      "Step: 4980, train/grad_norm: 0.10355757921934128\n",
      "Step: 4980, train/learning_rate: 8.569051715312526e-06\n",
      "Step: 4980, train/epoch: 8.286190032958984\n",
      "Step: 4990, train/loss: 0.00039999998989515007\n",
      "Step: 4990, train/grad_norm: 0.0355568490922451\n",
      "Step: 4990, train/learning_rate: 8.485856596962549e-06\n",
      "Step: 4990, train/epoch: 8.302828788757324\n",
      "Step: 5000, train/loss: 0.0\n",
      "Step: 5000, train/grad_norm: 0.002583560300990939\n",
      "Step: 5000, train/learning_rate: 8.402662388107274e-06\n",
      "Step: 5000, train/epoch: 8.319467544555664\n",
      "Step: 5010, train/loss: 0.005400000140070915\n",
      "Step: 5010, train/grad_norm: 0.003589397529140115\n",
      "Step: 5010, train/learning_rate: 8.319467269757297e-06\n",
      "Step: 5010, train/epoch: 8.336106300354004\n",
      "Step: 5020, train/loss: 9.999999747378752e-05\n",
      "Step: 5020, train/grad_norm: 0.0021550983656197786\n",
      "Step: 5020, train/learning_rate: 8.236273060902022e-06\n",
      "Step: 5020, train/epoch: 8.352745056152344\n",
      "Step: 5030, train/loss: 0.0\n",
      "Step: 5030, train/grad_norm: 0.0014850938459858298\n",
      "Step: 5030, train/learning_rate: 8.153077942552045e-06\n",
      "Step: 5030, train/epoch: 8.369384765625\n",
      "Step: 5040, train/loss: 0.010200000368058681\n",
      "Step: 5040, train/grad_norm: 0.03543757274746895\n",
      "Step: 5040, train/learning_rate: 8.06988373369677e-06\n",
      "Step: 5040, train/epoch: 8.38602352142334\n",
      "Step: 5050, train/loss: 9.999999747378752e-05\n",
      "Step: 5050, train/grad_norm: 0.21945694088935852\n",
      "Step: 5050, train/learning_rate: 7.986688615346793e-06\n",
      "Step: 5050, train/epoch: 8.40266227722168\n",
      "Step: 5060, train/loss: 0.0\n",
      "Step: 5060, train/grad_norm: 0.0011925745056942105\n",
      "Step: 5060, train/learning_rate: 7.903494406491518e-06\n",
      "Step: 5060, train/epoch: 8.41930103302002\n",
      "Step: 5070, train/loss: 0.0\n",
      "Step: 5070, train/grad_norm: 0.00133678934071213\n",
      "Step: 5070, train/learning_rate: 7.820299288141541e-06\n",
      "Step: 5070, train/epoch: 8.43593978881836\n",
      "Step: 5080, train/loss: 9.999999747378752e-05\n",
      "Step: 5080, train/grad_norm: 0.00465025007724762\n",
      "Step: 5080, train/learning_rate: 7.737105079286266e-06\n",
      "Step: 5080, train/epoch: 8.452579498291016\n",
      "Step: 5090, train/loss: 0.0\n",
      "Step: 5090, train/grad_norm: 0.005176969338208437\n",
      "Step: 5090, train/learning_rate: 7.65390996093629e-06\n",
      "Step: 5090, train/epoch: 8.469218254089355\n",
      "Step: 5100, train/loss: 0.0013000000035390258\n",
      "Step: 5100, train/grad_norm: 0.0009034665417857468\n",
      "Step: 5100, train/learning_rate: 7.570715297333663e-06\n",
      "Step: 5100, train/epoch: 8.485857009887695\n",
      "Step: 5110, train/loss: 0.00039999998989515007\n",
      "Step: 5110, train/grad_norm: 0.004015980288386345\n",
      "Step: 5110, train/learning_rate: 7.487520633731037e-06\n",
      "Step: 5110, train/epoch: 8.502495765686035\n",
      "Step: 5120, train/loss: 0.0\n",
      "Step: 5120, train/grad_norm: 0.00146439706441015\n",
      "Step: 5120, train/learning_rate: 7.4043259701284114e-06\n",
      "Step: 5120, train/epoch: 8.519134521484375\n",
      "Step: 5130, train/loss: 0.00039999998989515007\n",
      "Step: 5130, train/grad_norm: 0.0035678979475051165\n",
      "Step: 5130, train/learning_rate: 7.3211313065257855e-06\n",
      "Step: 5130, train/epoch: 8.535773277282715\n",
      "Step: 5140, train/loss: 0.011699999682605267\n",
      "Step: 5140, train/grad_norm: 0.002981050405651331\n",
      "Step: 5140, train/learning_rate: 7.2379366429231595e-06\n",
      "Step: 5140, train/epoch: 8.552412986755371\n",
      "Step: 5150, train/loss: 0.00930000003427267\n",
      "Step: 5150, train/grad_norm: 0.004229085985571146\n",
      "Step: 5150, train/learning_rate: 7.1547419793205336e-06\n",
      "Step: 5150, train/epoch: 8.569051742553711\n",
      "Step: 5160, train/loss: 0.002199999988079071\n",
      "Step: 5160, train/grad_norm: 0.46101871132850647\n",
      "Step: 5160, train/learning_rate: 7.071547315717908e-06\n",
      "Step: 5160, train/epoch: 8.58569049835205\n",
      "Step: 5170, train/loss: 0.00019999999494757503\n",
      "Step: 5170, train/grad_norm: 0.0036168803926557302\n",
      "Step: 5170, train/learning_rate: 6.988352652115282e-06\n",
      "Step: 5170, train/epoch: 8.60232925415039\n",
      "Step: 5180, train/loss: 0.008299999870359898\n",
      "Step: 5180, train/grad_norm: 0.021892117336392403\n",
      "Step: 5180, train/learning_rate: 6.905157988512656e-06\n",
      "Step: 5180, train/epoch: 8.61896800994873\n",
      "Step: 5190, train/loss: 0.0\n",
      "Step: 5190, train/grad_norm: 0.0008239925373345613\n",
      "Step: 5190, train/learning_rate: 6.82196332491003e-06\n",
      "Step: 5190, train/epoch: 8.635607719421387\n",
      "Step: 5200, train/loss: 0.01209999993443489\n",
      "Step: 5200, train/grad_norm: 0.10151339322328568\n",
      "Step: 5200, train/learning_rate: 6.738768661307404e-06\n",
      "Step: 5200, train/epoch: 8.652246475219727\n",
      "Step: 5210, train/loss: 0.00039999998989515007\n",
      "Step: 5210, train/grad_norm: 0.0047221011482179165\n",
      "Step: 5210, train/learning_rate: 6.655573997704778e-06\n",
      "Step: 5210, train/epoch: 8.668885231018066\n",
      "Step: 5220, train/loss: 0.0010000000474974513\n",
      "Step: 5220, train/grad_norm: 0.0020559814292937517\n",
      "Step: 5220, train/learning_rate: 6.572379334102152e-06\n",
      "Step: 5220, train/epoch: 8.685523986816406\n",
      "Step: 5230, train/loss: 0.019500000402331352\n",
      "Step: 5230, train/grad_norm: 17.797107696533203\n",
      "Step: 5230, train/learning_rate: 6.489184670499526e-06\n",
      "Step: 5230, train/epoch: 8.702162742614746\n",
      "Step: 5240, train/loss: 0.024000000208616257\n",
      "Step: 5240, train/grad_norm: 0.004530814941972494\n",
      "Step: 5240, train/learning_rate: 6.4059900068969e-06\n",
      "Step: 5240, train/epoch: 8.718802452087402\n",
      "Step: 5250, train/loss: 0.007300000172108412\n",
      "Step: 5250, train/grad_norm: 0.004024157766252756\n",
      "Step: 5250, train/learning_rate: 6.322795343294274e-06\n",
      "Step: 5250, train/epoch: 8.735441207885742\n",
      "Step: 5260, train/loss: 0.0027000000700354576\n",
      "Step: 5260, train/grad_norm: 0.20938622951507568\n",
      "Step: 5260, train/learning_rate: 6.239600679691648e-06\n",
      "Step: 5260, train/epoch: 8.752079963684082\n",
      "Step: 5270, train/loss: 0.0013000000035390258\n",
      "Step: 5270, train/grad_norm: 0.004528602119535208\n",
      "Step: 5270, train/learning_rate: 6.156406016089022e-06\n",
      "Step: 5270, train/epoch: 8.768718719482422\n",
      "Step: 5280, train/loss: 9.999999747378752e-05\n",
      "Step: 5280, train/grad_norm: 0.0007773786201141775\n",
      "Step: 5280, train/learning_rate: 6.073211352486396e-06\n",
      "Step: 5280, train/epoch: 8.785357475280762\n",
      "Step: 5290, train/loss: 0.0\n",
      "Step: 5290, train/grad_norm: 0.0008714714786037803\n",
      "Step: 5290, train/learning_rate: 5.99001668888377e-06\n",
      "Step: 5290, train/epoch: 8.801996231079102\n",
      "Step: 5300, train/loss: 0.0\n",
      "Step: 5300, train/grad_norm: 0.018511900678277016\n",
      "Step: 5300, train/learning_rate: 5.906822025281144e-06\n",
      "Step: 5300, train/epoch: 8.818635940551758\n",
      "Step: 5310, train/loss: 0.017100000753998756\n",
      "Step: 5310, train/grad_norm: 0.0015459673013538122\n",
      "Step: 5310, train/learning_rate: 5.823627361678518e-06\n",
      "Step: 5310, train/epoch: 8.835274696350098\n",
      "Step: 5320, train/loss: 9.999999747378752e-05\n",
      "Step: 5320, train/grad_norm: 0.001806468004360795\n",
      "Step: 5320, train/learning_rate: 5.740432698075892e-06\n",
      "Step: 5320, train/epoch: 8.851913452148438\n",
      "Step: 5330, train/loss: 0.00019999999494757503\n",
      "Step: 5330, train/grad_norm: 0.0005511904018931091\n",
      "Step: 5330, train/learning_rate: 5.6572380344732665e-06\n",
      "Step: 5330, train/epoch: 8.868552207946777\n",
      "Step: 5340, train/loss: 0.004000000189989805\n",
      "Step: 5340, train/grad_norm: 0.0007093308959156275\n",
      "Step: 5340, train/learning_rate: 5.5740433708706405e-06\n",
      "Step: 5340, train/epoch: 8.885190963745117\n",
      "Step: 5350, train/loss: 0.007799999788403511\n",
      "Step: 5350, train/grad_norm: 0.012559520080685616\n",
      "Step: 5350, train/learning_rate: 5.4908487072680146e-06\n",
      "Step: 5350, train/epoch: 8.901830673217773\n",
      "Step: 5360, train/loss: 0.00860000029206276\n",
      "Step: 5360, train/grad_norm: 0.0005620013107545674\n",
      "Step: 5360, train/learning_rate: 5.407654043665389e-06\n",
      "Step: 5360, train/epoch: 8.918469429016113\n",
      "Step: 5370, train/loss: 0.0\n",
      "Step: 5370, train/grad_norm: 0.001607195707038045\n",
      "Step: 5370, train/learning_rate: 5.324459380062763e-06\n",
      "Step: 5370, train/epoch: 8.935108184814453\n",
      "Step: 5380, train/loss: 9.999999747378752e-05\n",
      "Step: 5380, train/grad_norm: 0.004267838783562183\n",
      "Step: 5380, train/learning_rate: 5.241264716460137e-06\n",
      "Step: 5380, train/epoch: 8.951746940612793\n",
      "Step: 5390, train/loss: 0.0\n",
      "Step: 5390, train/grad_norm: 0.002194959670305252\n",
      "Step: 5390, train/learning_rate: 5.158070052857511e-06\n",
      "Step: 5390, train/epoch: 8.968385696411133\n",
      "Step: 5400, train/loss: 0.005900000222027302\n",
      "Step: 5400, train/grad_norm: 0.004534502048045397\n",
      "Step: 5400, train/learning_rate: 5.074875389254885e-06\n",
      "Step: 5400, train/epoch: 8.985025405883789\n",
      "Step: 5409, eval/loss: 1.0733091831207275\n",
      "Step: 5409, eval/accuracy: 0.8606136441230774\n",
      "Step: 5409, eval/f1: 0.855655312538147\n",
      "Step: 5409, eval/runtime: 29.08650016784668\n",
      "Step: 5409, eval/samples_per_second: 247.64100646972656\n",
      "Step: 5409, eval/steps_per_second: 4.434999942779541\n",
      "Step: 5409, train/epoch: 9.0\n",
      "Step: 5410, train/loss: 0.0006000000284984708\n",
      "Step: 5410, train/grad_norm: 0.0010541430674493313\n",
      "Step: 5410, train/learning_rate: 4.991680725652259e-06\n",
      "Step: 5410, train/epoch: 9.001664161682129\n",
      "Step: 5420, train/loss: 0.0\n",
      "Step: 5420, train/grad_norm: 0.0007942902739159763\n",
      "Step: 5420, train/learning_rate: 4.908486062049633e-06\n",
      "Step: 5420, train/epoch: 9.018302917480469\n",
      "Step: 5430, train/loss: 0.0003000000142492354\n",
      "Step: 5430, train/grad_norm: 0.0022552856244146824\n",
      "Step: 5430, train/learning_rate: 4.825291398447007e-06\n",
      "Step: 5430, train/epoch: 9.034941673278809\n",
      "Step: 5440, train/loss: 0.00039999998989515007\n",
      "Step: 5440, train/grad_norm: 0.0005022904369980097\n",
      "Step: 5440, train/learning_rate: 4.74209628009703e-06\n",
      "Step: 5440, train/epoch: 9.051580429077148\n",
      "Step: 5450, train/loss: 9.999999747378752e-05\n",
      "Step: 5450, train/grad_norm: 0.21403799951076508\n",
      "Step: 5450, train/learning_rate: 4.658901616494404e-06\n",
      "Step: 5450, train/epoch: 9.068219184875488\n",
      "Step: 5460, train/loss: 0.0035000001080334187\n",
      "Step: 5460, train/grad_norm: 0.004160422366112471\n",
      "Step: 5460, train/learning_rate: 4.575706952891778e-06\n",
      "Step: 5460, train/epoch: 9.084858894348145\n",
      "Step: 5470, train/loss: 0.004000000189989805\n",
      "Step: 5470, train/grad_norm: 0.004836443345993757\n",
      "Step: 5470, train/learning_rate: 4.492512289289152e-06\n",
      "Step: 5470, train/epoch: 9.101497650146484\n",
      "Step: 5480, train/loss: 9.999999747378752e-05\n",
      "Step: 5480, train/grad_norm: 0.006452703848481178\n",
      "Step: 5480, train/learning_rate: 4.409317625686526e-06\n",
      "Step: 5480, train/epoch: 9.118136405944824\n",
      "Step: 5490, train/loss: 9.999999747378752e-05\n",
      "Step: 5490, train/grad_norm: 1.8750914335250854\n",
      "Step: 5490, train/learning_rate: 4.3261229620839e-06\n",
      "Step: 5490, train/epoch: 9.134775161743164\n",
      "Step: 5500, train/loss: 9.999999747378752e-05\n",
      "Step: 5500, train/grad_norm: 0.008523400872945786\n",
      "Step: 5500, train/learning_rate: 4.242928298481274e-06\n",
      "Step: 5500, train/epoch: 9.151413917541504\n",
      "Step: 5510, train/loss: 0.0024999999441206455\n",
      "Step: 5510, train/grad_norm: 3.09185791015625\n",
      "Step: 5510, train/learning_rate: 4.1597336348786484e-06\n",
      "Step: 5510, train/epoch: 9.16805362701416\n",
      "Step: 5520, train/loss: 0.0\n",
      "Step: 5520, train/grad_norm: 0.004206628538668156\n",
      "Step: 5520, train/learning_rate: 4.0765389712760225e-06\n",
      "Step: 5520, train/epoch: 9.1846923828125\n",
      "Step: 5530, train/loss: 0.011599999852478504\n",
      "Step: 5530, train/grad_norm: 14.90774154663086\n",
      "Step: 5530, train/learning_rate: 3.9933443076733965e-06\n",
      "Step: 5530, train/epoch: 9.20133113861084\n",
      "Step: 5540, train/loss: 9.999999747378752e-05\n",
      "Step: 5540, train/grad_norm: 0.001284907921217382\n",
      "Step: 5540, train/learning_rate: 3.910149644070771e-06\n",
      "Step: 5540, train/epoch: 9.21796989440918\n",
      "Step: 5550, train/loss: 0.003800000064074993\n",
      "Step: 5550, train/grad_norm: 0.0005528725450858474\n",
      "Step: 5550, train/learning_rate: 3.826954980468145e-06\n",
      "Step: 5550, train/epoch: 9.23460865020752\n",
      "Step: 5560, train/loss: 9.999999747378752e-05\n",
      "Step: 5560, train/grad_norm: 0.05586698278784752\n",
      "Step: 5560, train/learning_rate: 3.7437603168655187e-06\n",
      "Step: 5560, train/epoch: 9.251248359680176\n",
      "Step: 5570, train/loss: 9.999999747378752e-05\n",
      "Step: 5570, train/grad_norm: 2.9198946952819824\n",
      "Step: 5570, train/learning_rate: 3.6605656532628927e-06\n",
      "Step: 5570, train/epoch: 9.267887115478516\n",
      "Step: 5580, train/loss: 0.0\n",
      "Step: 5580, train/grad_norm: 0.0007518524071201682\n",
      "Step: 5580, train/learning_rate: 3.5773709896602668e-06\n",
      "Step: 5580, train/epoch: 9.284525871276855\n",
      "Step: 5590, train/loss: 0.0\n",
      "Step: 5590, train/grad_norm: 0.0018992893164977431\n",
      "Step: 5590, train/learning_rate: 3.494176326057641e-06\n",
      "Step: 5590, train/epoch: 9.301164627075195\n",
      "Step: 5600, train/loss: 0.00019999999494757503\n",
      "Step: 5600, train/grad_norm: 2.469883441925049\n",
      "Step: 5600, train/learning_rate: 3.410981662455015e-06\n",
      "Step: 5600, train/epoch: 9.317803382873535\n",
      "Step: 5610, train/loss: 0.004399999976158142\n",
      "Step: 5610, train/grad_norm: 0.0006235099281184375\n",
      "Step: 5610, train/learning_rate: 3.327786998852389e-06\n",
      "Step: 5610, train/epoch: 9.334442138671875\n",
      "Step: 5620, train/loss: 0.0\n",
      "Step: 5620, train/grad_norm: 0.018788738176226616\n",
      "Step: 5620, train/learning_rate: 3.244592335249763e-06\n",
      "Step: 5620, train/epoch: 9.351081848144531\n",
      "Step: 5630, train/loss: 0.0\n",
      "Step: 5630, train/grad_norm: 0.048517391085624695\n",
      "Step: 5630, train/learning_rate: 3.161397671647137e-06\n",
      "Step: 5630, train/epoch: 9.367720603942871\n",
      "Step: 5640, train/loss: 0.0\n",
      "Step: 5640, train/grad_norm: 0.00198708800598979\n",
      "Step: 5640, train/learning_rate: 3.078203008044511e-06\n",
      "Step: 5640, train/epoch: 9.384359359741211\n",
      "Step: 5650, train/loss: 0.00019999999494757503\n",
      "Step: 5650, train/grad_norm: 0.0010977769270539284\n",
      "Step: 5650, train/learning_rate: 2.995008344441885e-06\n",
      "Step: 5650, train/epoch: 9.40099811553955\n",
      "Step: 5660, train/loss: 0.0\n",
      "Step: 5660, train/grad_norm: 0.0007359947776421905\n",
      "Step: 5660, train/learning_rate: 2.911813680839259e-06\n",
      "Step: 5660, train/epoch: 9.41763687133789\n",
      "Step: 5670, train/loss: 0.00019999999494757503\n",
      "Step: 5670, train/grad_norm: 5.352827072143555\n",
      "Step: 5670, train/learning_rate: 2.8286190172366332e-06\n",
      "Step: 5670, train/epoch: 9.434276580810547\n",
      "Step: 5680, train/loss: 9.999999747378752e-05\n",
      "Step: 5680, train/grad_norm: 0.002012046054005623\n",
      "Step: 5680, train/learning_rate: 2.7454243536340073e-06\n",
      "Step: 5680, train/epoch: 9.450915336608887\n",
      "Step: 5690, train/loss: 0.004399999976158142\n",
      "Step: 5690, train/grad_norm: 0.0005318606272339821\n",
      "Step: 5690, train/learning_rate: 2.6622296900313813e-06\n",
      "Step: 5690, train/epoch: 9.467554092407227\n",
      "Step: 5700, train/loss: 0.008200000040233135\n",
      "Step: 5700, train/grad_norm: 0.0029326279181987047\n",
      "Step: 5700, train/learning_rate: 2.5790350264287554e-06\n",
      "Step: 5700, train/epoch: 9.484192848205566\n",
      "Step: 5710, train/loss: 0.0\n",
      "Step: 5710, train/grad_norm: 0.0009453538805246353\n",
      "Step: 5710, train/learning_rate: 2.4958403628261294e-06\n",
      "Step: 5710, train/epoch: 9.500831604003906\n",
      "Step: 5720, train/loss: 0.0\n",
      "Step: 5720, train/grad_norm: 0.0012803676072508097\n",
      "Step: 5720, train/learning_rate: 2.4126456992235035e-06\n",
      "Step: 5720, train/epoch: 9.517471313476562\n",
      "Step: 5730, train/loss: 0.0\n",
      "Step: 5730, train/grad_norm: 0.0009512266260571778\n",
      "Step: 5730, train/learning_rate: 2.329450808247202e-06\n",
      "Step: 5730, train/epoch: 9.534110069274902\n",
      "Step: 5740, train/loss: 0.00930000003427267\n",
      "Step: 5740, train/grad_norm: 0.0028958681505173445\n",
      "Step: 5740, train/learning_rate: 2.246256144644576e-06\n",
      "Step: 5740, train/epoch: 9.550748825073242\n",
      "Step: 5750, train/loss: 9.999999747378752e-05\n",
      "Step: 5750, train/grad_norm: 0.0007385508506558836\n",
      "Step: 5750, train/learning_rate: 2.16306148104195e-06\n",
      "Step: 5750, train/epoch: 9.567387580871582\n",
      "Step: 5760, train/loss: 0.0\n",
      "Step: 5760, train/grad_norm: 0.007646854035556316\n",
      "Step: 5760, train/learning_rate: 2.0798668174393242e-06\n",
      "Step: 5760, train/epoch: 9.584026336669922\n",
      "Step: 5770, train/loss: 0.003599999938160181\n",
      "Step: 5770, train/grad_norm: 0.0005286726518534124\n",
      "Step: 5770, train/learning_rate: 1.9966721538366983e-06\n",
      "Step: 5770, train/epoch: 9.600665092468262\n",
      "Step: 5780, train/loss: 0.0\n",
      "Step: 5780, train/grad_norm: 0.001705494592897594\n",
      "Step: 5780, train/learning_rate: 1.9134774902340723e-06\n",
      "Step: 5780, train/epoch: 9.617304801940918\n",
      "Step: 5790, train/loss: 0.0\n",
      "Step: 5790, train/grad_norm: 0.007280933670699596\n",
      "Step: 5790, train/learning_rate: 1.8302828266314464e-06\n",
      "Step: 5790, train/epoch: 9.633943557739258\n",
      "Step: 5800, train/loss: 0.0005000000237487257\n",
      "Step: 5800, train/grad_norm: 0.005906866397708654\n",
      "Step: 5800, train/learning_rate: 1.7470881630288204e-06\n",
      "Step: 5800, train/epoch: 9.650582313537598\n",
      "Step: 5810, train/loss: 0.0\n",
      "Step: 5810, train/grad_norm: 0.016332486644387245\n",
      "Step: 5810, train/learning_rate: 1.6638934994261945e-06\n",
      "Step: 5810, train/epoch: 9.667221069335938\n",
      "Step: 5820, train/loss: 9.999999747378752e-05\n",
      "Step: 5820, train/grad_norm: 0.013448675163090229\n",
      "Step: 5820, train/learning_rate: 1.5806988358235685e-06\n",
      "Step: 5820, train/epoch: 9.683859825134277\n",
      "Step: 5830, train/loss: 0.0\n",
      "Step: 5830, train/grad_norm: 0.0006626576650887728\n",
      "Step: 5830, train/learning_rate: 1.4975041722209426e-06\n",
      "Step: 5830, train/epoch: 9.700499534606934\n",
      "Step: 5840, train/loss: 0.0\n",
      "Step: 5840, train/grad_norm: 0.0005952821811661124\n",
      "Step: 5840, train/learning_rate: 1.4143095086183166e-06\n",
      "Step: 5840, train/epoch: 9.717138290405273\n",
      "Step: 5850, train/loss: 0.0\n",
      "Step: 5850, train/grad_norm: 0.0035065794363617897\n",
      "Step: 5850, train/learning_rate: 1.3311148450156907e-06\n",
      "Step: 5850, train/epoch: 9.733777046203613\n",
      "Step: 5860, train/loss: 0.0\n",
      "Step: 5860, train/grad_norm: 0.01434897817671299\n",
      "Step: 5860, train/learning_rate: 1.2479201814130647e-06\n",
      "Step: 5860, train/epoch: 9.750415802001953\n",
      "Step: 5870, train/loss: 0.0\n",
      "Step: 5870, train/grad_norm: 0.0006373991491273046\n",
      "Step: 5870, train/learning_rate: 1.164725404123601e-06\n",
      "Step: 5870, train/epoch: 9.767054557800293\n",
      "Step: 5880, train/loss: 0.004999999888241291\n",
      "Step: 5880, train/grad_norm: 41.77008056640625\n",
      "Step: 5880, train/learning_rate: 1.081530740520975e-06\n",
      "Step: 5880, train/epoch: 9.78369426727295\n",
      "Step: 5890, train/loss: 0.0\n",
      "Step: 5890, train/grad_norm: 0.0013325173640623689\n",
      "Step: 5890, train/learning_rate: 9.983360769183491e-07\n",
      "Step: 5890, train/epoch: 9.800333023071289\n",
      "Step: 5900, train/loss: 9.999999747378752e-05\n",
      "Step: 5900, train/grad_norm: 0.0016233415808528662\n",
      "Step: 5900, train/learning_rate: 9.151414133157232e-07\n",
      "Step: 5900, train/epoch: 9.816971778869629\n",
      "Step: 5910, train/loss: 0.0\n",
      "Step: 5910, train/grad_norm: 0.0004382632614579052\n",
      "Step: 5910, train/learning_rate: 8.319467497130972e-07\n",
      "Step: 5910, train/epoch: 9.833610534667969\n",
      "Step: 5920, train/loss: 0.0\n",
      "Step: 5920, train/grad_norm: 0.0006874738028272986\n",
      "Step: 5920, train/learning_rate: 7.487520861104713e-07\n",
      "Step: 5920, train/epoch: 9.850249290466309\n",
      "Step: 5930, train/loss: 0.0\n",
      "Step: 5930, train/grad_norm: 0.000906344095710665\n",
      "Step: 5930, train/learning_rate: 6.655574225078453e-07\n",
      "Step: 5930, train/epoch: 9.866888046264648\n",
      "Step: 5940, train/loss: 0.0003000000142492354\n",
      "Step: 5940, train/grad_norm: 0.027518322691321373\n",
      "Step: 5940, train/learning_rate: 5.823627020618005e-07\n",
      "Step: 5940, train/epoch: 9.883527755737305\n",
      "Step: 5950, train/loss: 0.0003000000142492354\n",
      "Step: 5950, train/grad_norm: 0.0007978165522217751\n",
      "Step: 5950, train/learning_rate: 4.991680384591746e-07\n",
      "Step: 5950, train/epoch: 9.900166511535645\n",
      "Step: 5960, train/loss: 0.006099999882280827\n",
      "Step: 5960, train/grad_norm: 0.0015883625019341707\n",
      "Step: 5960, train/learning_rate: 4.159733748565486e-07\n",
      "Step: 5960, train/epoch: 9.916805267333984\n",
      "Step: 5970, train/loss: 9.999999747378752e-05\n",
      "Step: 5970, train/grad_norm: 0.006894948426634073\n",
      "Step: 5970, train/learning_rate: 3.3277871125392267e-07\n",
      "Step: 5970, train/epoch: 9.933444023132324\n",
      "Step: 5980, train/loss: 9.999999747378752e-05\n",
      "Step: 5980, train/grad_norm: 0.001245250809006393\n",
      "Step: 5980, train/learning_rate: 2.495840192295873e-07\n",
      "Step: 5980, train/epoch: 9.950082778930664\n",
      "Step: 5990, train/loss: 0.0003000000142492354\n",
      "Step: 5990, train/grad_norm: 0.00085645163198933\n",
      "Step: 5990, train/learning_rate: 1.6638935562696133e-07\n",
      "Step: 5990, train/epoch: 9.96672248840332\n",
      "Step: 6000, train/loss: 0.0\n",
      "Step: 6000, train/grad_norm: 0.0006117662414908409\n",
      "Step: 6000, train/learning_rate: 8.319467781348067e-08\n",
      "Step: 6000, train/epoch: 9.98336124420166\n",
      "Step: 6010, train/loss: 0.08780000358819962\n",
      "Step: 6010, train/grad_norm: 217.33840942382812\n",
      "Step: 6010, train/learning_rate: 0.0\n",
      "Step: 6010, train/epoch: 10.0\n",
      "Step: 6010, eval/loss: 1.2602921724319458\n",
      "Step: 6010, eval/accuracy: 0.8450645804405212\n",
      "Step: 6010, eval/f1: 0.8416679501533508\n",
      "Step: 6010, eval/runtime: 29.314699172973633\n",
      "Step: 6010, eval/samples_per_second: 245.71299743652344\n",
      "Step: 6010, eval/steps_per_second: 4.401000022888184\n",
      "Step: 6010, train/epoch: 10.0\n",
      "Step: 6010, train/train_runtime: 3237.713134765625\n",
      "Step: 6010, train/train_samples_per_second: 103.81099700927734\n",
      "Step: 6010, train/train_steps_per_second: 1.8559999465942383\n",
      "Step: 6010, train/total_flos: 8.843425279796838e+16\n",
      "Step: 6010, train/train_loss: 0.10376425832509995\n",
      "Step: 6010, train/epoch: 10.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.summary.summary_iterator import summary_iterator\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Construct the logs directory path\n",
    "logs_directory = os.path.join('./', project_name, 'logs')\n",
    "file_pattern = 'events.out.tfevents.*'\n",
    "\n",
    "# Retrieve all event files matching the pattern\n",
    "event_files = glob.glob(os.path.join(logs_directory, file_pattern))\n",
    "\n",
    "# Function to print out TensorBoard event logs\n",
    "def print_events_from_file(event_files):\n",
    "    for event_file in event_files:\n",
    "        print(f\"Reading events from file: {event_file}\")\n",
    "        try:\n",
    "            for e in summary_iterator(event_file):\n",
    "                for v in e.summary.value:\n",
    "                    if v.HasField('simple_value'):\n",
    "                        print(f\"Step: {e.step}, {v.tag}: {v.simple_value}\")\n",
    "        except Exception as e:  # Just in case the event file is not readable\n",
    "            print(f\"Failed to read {event_file}: {e}\")\n",
    "\n",
    "print_events_from_file(event_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f8e0ccc-f7c4-4a02-a1e6-5e3012717125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Step Train Loss Eval Loss  Accuracy        F1\n",
      "0   601   0.441600  0.484352  0.765098  0.764558\n",
      "1  1202   0.288600  0.383758  0.827572  0.824655\n",
      "2  1803   0.177800  0.390909  0.855060  0.848444\n",
      "3  2404   0.043400  0.618340  0.851728  0.843860\n",
      "4  3005   0.037200  0.941856  0.800916  0.799464\n",
      "5  3606   0.017600  0.885973  0.842149  0.838166\n",
      "6  4207   0.036500  1.663635  0.773844  0.773563\n",
      "7  4808   0.010200  1.017183  0.862696  0.857880\n",
      "8  5409   0.005900  1.073309  0.860614  0.855655\n",
      "9  6010   0.087800  1.260292  0.845065  0.841668\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.python.summary.summary_iterator import summary_iterator\n",
    "\n",
    "logs_directory = os.path.join('./', project_name, 'logs')\n",
    "file_pattern = 'events.out.tfevents.*'\n",
    "\n",
    "event_files = glob.glob(os.path.join(logs_directory, file_pattern))\n",
    "\n",
    "def extract_metrics(event_files):\n",
    "    data = []\n",
    "    last_train_loss = None\n",
    "\n",
    "    for event_file in event_files:\n",
    "        for e in summary_iterator(event_file):\n",
    "            for v in e.summary.value:\n",
    "                if v.HasField('simple_value'):\n",
    "                    step = e.step\n",
    "                    metric_name = v.tag.split('/')[-1]\n",
    "                    metric_value = v.simple_value\n",
    "\n",
    "                    formatted_value = f\"{metric_value:.6f}\"\n",
    "\n",
    "                    if 'train/loss' in v.tag:\n",
    "                        last_train_loss = formatted_value\n",
    "\n",
    "                    if 'eval' in v.tag:\n",
    "                        entry = next((item for item in data if item['Step'] == step), None)\n",
    "                        if not entry:\n",
    "                            entry = {'Step': step, 'Train Loss': last_train_loss, 'Eval Loss': None, 'Accuracy': None, 'F1': None}\n",
    "                            data.append(entry)\n",
    "                        if 'loss' in v.tag:\n",
    "                            entry['Eval Loss'] = formatted_value\n",
    "                        elif 'accuracy' in v.tag:\n",
    "                            entry['Accuracy'] = formatted_value\n",
    "                        elif 'f1' in v.tag:\n",
    "                            entry['F1'] = formatted_value\n",
    "    return data\n",
    "\n",
    "metrics_data = extract_metrics(event_files)\n",
    "\n",
    "df = pd.DataFrame(metrics_data)\n",
    "df = df.sort_values(by='Step')\n",
    "\n",
    "file_path = \"../images/\"+model_name+\"_Checkpoint_Data.csv\"\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52633cb6-5847-4d51-86c5-c6153e72cc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Checkpoint Step: checkpoint-1803\n",
      "Step              1803\n",
      "Train Loss    0.177800\n",
      "Eval Loss     0.390909\n",
      "Accuracy       0.85506\n",
      "F1            0.848444\n",
      "Rank Sum           8.0\n",
      "Name: 2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df.fillna({\n",
    "    'Eval Loss': float('inf'),\n",
    "    'Accuracy': 0,\n",
    "    'F1': 0\n",
    "}, inplace=True)\n",
    "\n",
    "df['Eval Loss'] = df['Eval Loss'].astype(float)\n",
    "df['Accuracy'] = df['Accuracy'].astype(float)\n",
    "df['F1'] = df['F1'].astype(float)\n",
    "\n",
    "df['Eval Loss Rank'] = df['Eval Loss'].rank(method='min', ascending=True)\n",
    "df['Accuracy Rank'] = df['Accuracy'].rank(method='min', ascending=False)\n",
    "df['F1 Rank'] = df['F1'].rank(method='min', ascending=False)\n",
    "\n",
    "df['Rank Sum'] = df['Eval Loss Rank'] + df['Accuracy Rank'] + df['F1 Rank']\n",
    "\n",
    "best_checkpoint = df.loc[df['Rank Sum'].idxmin()]\n",
    "\n",
    "checkpoint_folder_name = f\"checkpoint-{best_checkpoint['Step']}\"\n",
    "print(f\"Best Checkpoint Step: {checkpoint_folder_name}\")\n",
    "print(best_checkpoint[['Step', 'Train Loss', 'Eval Loss', 'Accuracy', 'F1', 'Rank Sum']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3495da20-4217-410d-8009-64c41c53e157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Checkpoint Step: checkpoint-1803\n",
      "Step              1803\n",
      "Train Loss    0.177800\n",
      "Eval Loss     0.390909\n",
      "Accuracy       0.85506\n",
      "F1            0.848444\n",
      "Rank Sum           8.0\n",
      "Name: 2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df.fillna({\n",
    "    'Eval Loss': float('inf'),\n",
    "    'Accuracy': 0,\n",
    "    'F1': 0\n",
    "}, inplace=True)\n",
    "\n",
    "df['Eval Loss'] = df['Eval Loss'].astype(float)\n",
    "df['Accuracy'] = df['Accuracy'].astype(float)\n",
    "df['F1'] = df['F1'].astype(float)\n",
    "\n",
    "df['Eval Loss Rank'] = df['Eval Loss'].rank(method='min', ascending=True)\n",
    "df['Accuracy Rank'] = df['Accuracy'].rank(method='min', ascending=False)\n",
    "df['F1 Rank'] = df['F1'].rank(method='min', ascending=False)\n",
    "\n",
    "df['Rank Sum'] = df['Eval Loss Rank'] + df['Accuracy Rank'] + df['F1 Rank']\n",
    "\n",
    "best_checkpoint = df.loc[df['Rank Sum'].idxmin()]\n",
    "\n",
    "checkpoint_folder_name = f\"checkpoint-{best_checkpoint['Step']}\"\n",
    "print(f\"Best Checkpoint Step: {checkpoint_folder_name}\")\n",
    "print(best_checkpoint[['Step', 'Train Loss', 'Eval Loss', 'Accuracy', 'F1', 'Rank Sum']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d4044b-4ffc-409e-ac1a-8b4c58e72499",
   "metadata": {},
   "source": [
    "### Run TensorBoard\n",
    "tensorboard --logdir=~/kuk/Praxis/praxis-Llama-2-7b-hf-small-finetune/logs --host=0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e5a88a-7620-4280-970b-ff171568aebb",
   "metadata": {},
   "source": [
    "### PAUSE SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57fc853d-d293-402d-87d0-e27c83915c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My flag to pause the script, set to True to pause\n",
    "pause_script = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53887034-7922-4ed5-bd9a-8c80d6596ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        print(\"Script Paused\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd8248c6-2c95-4d82-a7f0-2e0ac37ab320",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pause_script:\n",
    "    raise StopExecution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19be6e26-d888-4da0-b3f8-836d68ac2051",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b140136-4f0d-44e4-9955-d95a342f250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import pipeline\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "test_checkpoint_name = checkpoint_folder_name\n",
    "savedmodel = pipeline('text-classification', model=output_dir_path + \"/\" + test_checkpoint_name, device=accelerator.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e0c22fa-0da7-4a30-8d27-128cab0bdfc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['article', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 7203\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50f2ad84-10b4-4f10-bbea-25571bfcda78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc15fc1da79242aab30ff066c812d7d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "max_tokens = 512\n",
    "truncated_articles = []\n",
    "\n",
    "for article in tqdm(tokenized_test_ds['article']):\n",
    "    encoded_article = tokenizer.encode(\n",
    "        article,\n",
    "        max_length=max_tokens,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    truncated_article = tokenizer.decode(encoded_article, skip_special_tokens=True)\n",
    "    truncated_articles.append(truncated_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d687d739-aafa-4ae2-b938-e8465856746e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16fac5d0efe345ecaeb4fac864c0d311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifying articles:   0%|          | 0/7203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "034f106bb0344687b2f331434ed4add0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Labeling articles:   0%|          | 0/7203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Create a dataset from the truncated articles\n",
    "data = {\"text\": truncated_articles}\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Define a function to make predictions\n",
    "def classify_batch(batch):\n",
    "    return savedmodel(batch[\"text\"])\n",
    "\n",
    "# Process the dataset with batching\n",
    "test_predictions = []\n",
    "for batch in tqdm(dataset.to_dict()[\"text\"], desc=\"Classifying articles\"):\n",
    "    predictions = classify_batch({\"text\": batch})\n",
    "    test_predictions.extend(predictions)\n",
    "\n",
    "test_predictions_labels = []\n",
    "for prediction in tqdm(test_predictions, desc=\"Labeling articles\"):\n",
    "    label = 0 if prediction['label'] == 'LABEL_0' else 1\n",
    "    test_predictions_labels.append(label)\n",
    "\n",
    "print(test_predictions_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26d6eead-d836-4e80-8ff6-b86b8599b245",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "true_articles = tokenized_test_ds['label']\n",
    "print(true_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10e33442-ac7f-49d3-a6ad-5eb4b892eedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(prediction):\n",
    "  return int(prediction['label'].split('_')[1])\n",
    "\n",
    "processed_predictions = [get_label(prediction) for prediction in test_predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f06a2c-5ded-4da0-8232-145383967c9d",
   "metadata": {},
   "source": [
    "<h1>Accuracy and F1</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb4d8350-a8d1-4853-a9f8-74ae9ea36bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8511731223101485\n"
     ]
    }
   ],
   "source": [
    "true_articles = tokenized_test_ds['label']\n",
    "accuracy = accuracy_score(true_articles, test_predictions_labels)\n",
    "print(\"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40780784-e77d-4b26-9dae-572137e591bf",
   "metadata": {},
   "source": [
    "<p>precision (how many of the items identified as positive are actually positive) and the recall (how many of the actual positives were identified correctly)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30465934-1b77-4a2c-8c52-34e239e37abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.8443035493781108\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(true_articles, test_predictions_labels, average='macro')\n",
    "print(\"f1_score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcaac14-2b83-4c24-b83a-f6d0e73a2d2a",
   "metadata": {},
   "source": [
    "<h1>Confusion Matrix</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57e64afb-e3ee-4c79-8228-b2d79806229e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(tokenized_test_ds['label'], processed_predictions)\n",
    "\n",
    "labels = ['human', 'machine']\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c87380b0-553d-4642-9697-26c76c98dde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook praxis-bert-base-uncased-small-finetune-v7.ipynb to html\n",
      "[NbConvertApp] WARNING | Alternative text is missing on 1 image(s).\n",
      "[NbConvertApp] Writing 580689 bytes to html/praxis-bert-base-uncased-small-finetune-v7.html\n"
     ]
    }
   ],
   "source": [
    "file_name = f\"{project_name}.ipynb\"\n",
    "html_file_name = f\"{file_name.replace('.ipynb', '.html')}\"\n",
    "\n",
    "command = f\"jupyter nbconvert '{file_name}' --to html --output-dir './html' --output '{html_file_name}'\"\n",
    "get_ipython().system(command)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
