{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c2d2c9-4a4b-48ca-90a3-1ccd120ca08b",
   "metadata": {},
   "source": [
    "# Fine tuning using Llama 3 70B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5bef3d-89b9-42ed-b158-a39bd61f6a31",
   "metadata": {},
   "source": [
    "### Base Model and Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfe5f014-527c-4a83-863b-4b6330c72ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 0: NVIDIA GeForce RTX 4090\n",
    "# GPU 1: NVIDIA GeForce RTX 4090\n",
    "# GPU 2: NVIDIA GeForce RTX 4090\n",
    "# GPU 3: NVIDIA GeForce RTX 3090 Ti\n",
    "# GPU 4: NVIDIA GeForce RTX 3090 Ti\n",
    "# GPU 5: NVIDIA GeForce RTX 3090\n",
    "# GPU 6: NVIDIA GeForce RTX 3090\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # \"\"makes all visible, \"0\" GPU 0 visible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ffaa11-3936-40a0-8f2a-3d083ff2afef",
   "metadata": {},
   "source": [
    "### Supress warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e77f7e3d-3af3-4dc8-9571-d13398c29ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef535c7a-848e-4c6e-a692-208f98610d82",
   "metadata": {},
   "source": [
    "### Inspect the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bfef1a8-c05a-4e14-af0e-358bfb04352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89cce100-9b2e-4675-bfda-f029e7c4056e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1478680e22a2474e83af0f3876df55dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15ec782b-0776-4354-ad08-66f1e9e50187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"Meta-Llama-3-70B\"\n",
    "checkpoint = \"meta-llama/\"+model_name\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbf95826-7975-4943-962c-ccfa26f31030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc0fb8053a744f38eee7cdffb1991c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-70B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, quantization_config=bnb_config, device_map=\"auto\", num_labels=2, token=access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83f74939-3ab3-4011-90cb-8e90c22ea162",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "LlamaForSequenceClassification                          --\n",
       "├─LlamaModel: 1-1                                       --\n",
       "│    └─Embedding: 2-1                                   1,050,673,152\n",
       "│    └─ModuleList: 2-2                                  --\n",
       "│    │    └─LlamaDecoderLayer: 3-1                      427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-2                      427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-3                      427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-4                      427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-5                      427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-6                      427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-7                      427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-8                      427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-9                      427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-10                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-11                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-12                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-13                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-14                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-15                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-16                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-17                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-18                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-19                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-20                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-21                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-22                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-23                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-24                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-25                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-26                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-27                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-28                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-29                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-30                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-31                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-32                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-33                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-34                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-35                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-36                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-37                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-38                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-39                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-40                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-41                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-42                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-43                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-44                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-45                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-46                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-47                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-48                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-49                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-50                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-51                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-52                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-53                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-54                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-55                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-56                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-57                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-58                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-59                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-60                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-61                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-62                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-63                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-64                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-65                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-66                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-67                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-68                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-69                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-70                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-71                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-72                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-73                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-74                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-75                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-76                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-77                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-78                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-79                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-80                     427,835,392\n",
       "│    └─LlamaRMSNorm: 2-3                                8,192\n",
       "├─Linear: 1-2                                           16,384\n",
       "================================================================================\n",
       "Total params: 35,277,529,088\n",
       "Trainable params: 1,052,008,448\n",
       "Non-trainable params: 34,225,520,640\n",
       "================================================================================"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49bbe47c-4430-4f4a-90d8-1113bd320a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 8192)\n",
       "    (layers): ModuleList(\n",
       "      (0-79): 80 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=8192, out_features=28672, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=8192, out_features=28672, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=28672, out_features=8192, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (score): Linear(in_features=8192, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6627b1da-57d6-4a17-8ea3-746b5cb1f3d9",
   "metadata": {},
   "source": [
    "### Load the news dataset from pickle file\n",
    "If any of the check_files don't exist then load the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cc372cc-0cae-4b49-a2d7-8e57f58244a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one of the specified files already exists. Not loading new dataset.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "base_path = './data/'\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "file_name = 'news_small_dataset.pkl'\n",
    "file_path = base_path+file_name\n",
    "\n",
    "def pickle_dataset(dataset, file_path):\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(dataset, file)\n",
    "        print(f\"Dataset has been pickled to: {file_path}\")\n",
    "\n",
    "def load_pickle_dataset(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        dataset = pickle.load(file)\n",
    "        print(f\"Dataset has been loaded from: {file_path}\")\n",
    "    return dataset\n",
    "\n",
    "def check_files_exists(file_names):\n",
    "    for name in file_names:\n",
    "        file_path = os.path.join(base_path, name)\n",
    "        if os.path.exists(file_path):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# if these files exist we do not want to load the news_dataset.pkl to tokenize and make these files\n",
    "check_files = [model_name+'-small_tokenized_train_ds.pkl', model_name+'-small_tokenized_eval_ds.pkl', model_name+'-small_tokenized_test_ds.pkl']\n",
    "\n",
    "if check_files_exists(check_files):\n",
    "    print(\"At least one of the specified files already exists. Not loading new dataset.\")\n",
    "else:\n",
    "    news_split_ds = load_pickle_dataset(file_path)\n",
    "    print(news_split_ds)\n",
    "    total_rows = (news_split_ds['train'].num_rows +\n",
    "              news_split_ds['eval'].num_rows +\n",
    "              news_split_ds['test'].num_rows)\n",
    "    print(\"Total number of rows:\", total_rows)\n",
    "    print(\"Dataset loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad450a2-6cef-432e-9e63-7ff985b4726e",
   "metadata": {},
   "source": [
    "### Tokenization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88c3a38c-1d24-4a1e-8d96-1b7c2ce162c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, token=access_token)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "\n",
    "def tokenize_fn(news):\n",
    "    return tokenizer(news['article'], padding=True, truncation=True, max_length=512, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02efc4f0-742a-4838-887d-5556a26ae15f",
   "metadata": {},
   "source": [
    "### Tokenize train, evaluation, and test datasets\n",
    "If any of the check files exist then don't run tokenization and save some time.\n",
    "Else load the pickle files that already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f3e3101-df30-4af2-9976-49ba0cf44d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already exist, so load datasets\n",
      "Dataset has been loaded from: ./data/Meta-Llama-3-70B-small_tokenized_train_ds.pkl\n",
      "Dataset has been loaded from: ./data/Meta-Llama-3-70B-small_tokenized_eval_ds.pkl\n",
      "Dataset has been loaded from: ./data/Meta-Llama-3-70B-small_tokenized_test_ds.pkl\n"
     ]
    }
   ],
   "source": [
    "if not check_files_exists(check_files):\n",
    "    tokenized_train_ds = news_split_ds['train'].map(tokenize_fn, batched=True)\n",
    "    tokenized_eval_ds = news_split_ds['eval'].map(tokenize_fn, batched=True)\n",
    "    tokenized_test_ds = news_split_ds['test'].map(tokenize_fn, batched=True)\n",
    "\n",
    "    print(tokenized_train_ds.features)\n",
    "    print(tokenized_eval_ds.features)\n",
    "    print(tokenized_test_ds.features)\n",
    "    \n",
    "    pickle_dataset(tokenized_train_ds, base_path+model_name+'-small_tokenized_train_ds.pkl')\n",
    "    pickle_dataset(tokenized_eval_ds, base_path+model_name+'-small_tokenized_eval_ds.pkl')\n",
    "    pickle_dataset(tokenized_test_ds, base_path+model_name+'-small_tokenized_test_ds.pkl')\n",
    "else:\n",
    "    print(\"Files already exist, so load datasets\")\n",
    "    tokenized_train_ds = load_pickle_dataset(base_path+model_name+'-small_tokenized_train_ds.pkl')\n",
    "    tokenized_eval_ds = load_pickle_dataset(base_path+model_name+'-small_tokenized_eval_ds.pkl')\n",
    "    tokenized_test_ds = load_pickle_dataset(base_path+model_name+'-small_tokenized_test_ds.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307cc4fb-9766-4b0f-943b-4a1c90053e09",
   "metadata": {},
   "source": [
    "### Look at the tokenized data\n",
    "Notice what the actual data looks like, and then the tokenized data which is a bunch of numbers, and then the attention mask at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44321182-e1b9-4570-bd24-7a5cf42ba504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in training dataset: 33611\n",
      "Number of records in evaluation dataset: 7203\n",
      "Number of records in test dataset: 7203\n",
      "Total number of records: 48017\n"
     ]
    }
   ],
   "source": [
    "count_train_records = len(tokenized_train_ds)\n",
    "count_eval_records = len(tokenized_eval_ds)\n",
    "count_test_records = len(tokenized_test_ds)\n",
    "print(f\"Number of records in training dataset: {count_train_records}\")\n",
    "print(f\"Number of records in evaluation dataset: {count_eval_records}\")\n",
    "print(f\"Number of records in test dataset: {count_test_records}\")\n",
    "count_total_records = count_train_records + count_eval_records + count_test_records\n",
    "print(f\"Total number of records: {count_total_records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64be678b-d2a8-403e-bcc8-ef9e7eabb0cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'article': \"In a year where homicides, rapes and robberies increased slightly, New York City still saw serious crime drop 1.7 percent in 2015, continuing an overall decline that began in the 1990s, NYPD Commissioner William Bratton said Monday.\\nAt a news conference with Mayor Bill de Blasio, Bratton touted last year’s crime statistics, which he said, when combined with an even larger decline in 2014, put to rest the fear that substantial decreases couldn’t continue under the new administration at City Hall.\\n“While we have had some fluctuation, some increases in certain categories, the overall trend in all our crime categories continues to go down,” Bratton told reporters. “It was a very good year for us, 2015.\\nHomicides increased by 4.5 percent in 2015, rising to 350 from 333 in the prior year, which was the lowest since 1994, said Deputy Commissioner Dermot Shea. Rapes increased 6 percent and robberies rose 2 percent, said Shea, who is in charge of data collection and operations for the NYPD.\\nThe lower overall crime statistics came about due to what Shea called “targeted enforcement,” where cops make quality arrests even though the overall number of apprehensions was the lowest in the city since 2003.\\nTwo boroughs — Manhattan and the Bronx — actually saw serious crimes increase by 3 percent and 4 percent, respectively, Shea said. Manhattan’s increase was driven by more robberies, while the Bronx, although seeing an overall crime increase, had what he said was a “phenomenal” reduction in shootings. Citywide, shootings were down in 2015 about 3 percent, to 1,103 from 1,172 in 2014.\\nShea largely attributed the 2015 increase in rapes to victims coming forward with complaints about attacks from years past.\\nSign up to get the latest updates Get Newsday's Breaking News alerts in your inbox. By clicking Sign up, you agree to our privacy policy.\\n“Twenty percent of these rapes didn’t happen in 2015,” he said.\\nThe NYPD has seen an increase in rapes involving single women who, after a night of drinking, get into cabs of all kinds and are attacked, Shea said.\\n“They get driven, and passing out and waking up in a desolate area, and they get sexually attacked. This is something, really, that people need to be exceptionally aware of, and like any case in New York City, the buddy system works,” said Shea, referring to the need for people to travel in pairs when taking a cab at night.\\nBratton and police brass hope to build upon the continuing drop in overall crime by using technology such as ShotSpotter and a newly minted GPS system for police cars.\\nJessica Tisch, NYPD deputy commissioner for technology, said ShotSpotter, an acoustical system that detects gunfire, identified gunshots in 1,672 cases, mostly in Brooklyn. Of those alerts, 74 percent didn’t have any 911 calls from the public associated with them.\\nTisch said ShotSpotter helped police recover ballistic evidence in 19 percent of the gunfire alerts. In 22 percent of those cases, Tisch said, cops were able to make positive matches of bullets with those from guns used in earlier shootings.\\nTisch also highlighted a special GPS system being tried in about 5,000 patrol cars that allows the NYPD to see where its vehicles are and to track their movements over a 24-hour period, as well as gather information about the officers’ driving.\\n\", 'label': 0, 'input_ids': [128000, 644, 264, 1060, 1405, 89495, 11, 96330, 323, 71650, 552, 7319, 10284, 11, 1561, 4356, 4409, 2103, 5602, 6129, 9977, 6068, 220, 16, 13, 22, 3346, 304, 220, 679, 20, 11, 14691, 459, 8244, 18174, 430, 6137, 304, 279, 220, 2550, 15, 82, 11, 74255, 30454, 12656, 3320, 266, 783, 1071, 7159, 627, 1688, 264, 3754, 10017, 449, 22868, 8766, 409, 93493, 11, 3320, 266, 783, 67528, 1566, 1060, 753, 9977, 13443, 11, 902, 568, 1071, 11, 994, 11093, 449, 459, 1524, 8294, 18174, 304, 220, 679, 19, 11, 2231, 311, 2800, 279, 8850, 430, 12190, 43154, 7846, 1431, 3136, 1234, 279, 502, 8735, 520, 4409, 11166, 627, 2118, 8142, 584, 617, 1047, 1063, 39388, 4090, 11, 1063, 12992, 304, 3738, 11306, 11, 279, 8244, 9327, 304, 682, 1057, 9977, 11306, 9731, 311, 733, 1523, 2476, 3320, 266, 783, 3309, 19578, 13, 1054, 2181, 574, 264, 1633, 1695, 1060, 369, 603, 11, 220, 679, 20, 627, 39, 3151, 3422, 7319, 555, 220, 19, 13, 20, 3346, 304, 220, 679, 20, 11, 16448, 311, 220, 8652, 505, 220, 8765, 304, 279, 4972, 1060, 11, 902, 574, 279, 15821, 2533, 220, 2550, 19, 11, 1071, 32724, 30454, 76508, 354, 86068, 13, 432, 9521, 7319, 220, 21, 3346, 323, 71650, 552, 16392, 220, 17, 3346, 11, 1071, 86068, 11, 889, 374, 304, 6900, 315, 828, 4526, 323, 7677, 369, 279, 74255, 627, 791, 4827, 8244, 9977, 13443, 3782, 922, 4245, 311, 1148, 86068, 2663, 1054, 5775, 291, 13627, 2476, 1405, 35317, 1304, 4367, 38811, 1524, 3582, 279, 8244, 1396, 315, 47291, 4769, 574, 279, 15821, 304, 279, 3363, 2533, 220, 1049, 18, 627, 11874, 66841, 82, 2001, 29890, 323, 279, 66236, 2001, 3604, 5602, 6129, 17073, 5376, 555, 220, 18, 3346, 323, 220, 19, 3346, 11, 15947, 11, 86068, 1071, 13, 29890, 753, 5376, 574, 16625, 555, 810, 71650, 552, 11, 1418, 279, 66236, 11, 8051, 9298, 459, 8244, 9977, 5376, 11, 1047, 1148, 568, 1071, 574, 264, 1054, 15112, 6431, 278, 863, 14278, 304, 44861, 13, 4409, 9328, 11, 44861, 1051, 1523, 304, 220, 679, 20, 922, 220, 18, 3346, 11, 311, 220, 16, 11, 6889, 505, 220, 16, 11, 10861, 304, 220, 679, 19, 627, 8100, 64, 14090, 30706, 279, 220, 679, 20, 5376, 304, 96330, 311, 12697, 5108, 4741, 449, 21859, 922, 8951, 505, 1667, 3347, 627, 7412, 709, 311, 636, 279, 5652, 9013, 2175, 5513, 1316, 596, 52624, 5513, 30350, 304, 701, 23732, 13, 3296, 18965, 7220, 709, 11, 499, 7655, 311, 1057, 12625, 4947, 627, 2118, 76896, 3346, 315, 1521, 96330, 3287, 1431, 3621, 304, 220, 679, 20, 2476, 568, 1071, 627, 791, 74255, 706, 3970, 459, 5376, 304, 96330, 16239, 3254, 3278, 889, 11, 1306, 264, 3814, 315, 16558, 11, 636, 1139, 272, 3518, 315, 682, 13124, 323, 527, 18855, 11, 86068, 1071, 627, 46690, 636, 16625, 11, 323, 12579, 704, 323, 48728, 709, 304, 264, 951, 34166, 3158, 11, 323, 814, 636, 27681, 18855, 13, 1115, 374, 2555, 11, 2216, 11, 430, 1274, 1205, 311, 387, 48298, 8010, 315, 11, 323, 1093, 904, 1162, 304, 1561, 4356], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "first_record = tokenized_train_ds[0]\n",
    "print(first_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d7f4f8-9eb3-4feb-86e7-fb0dad88753f",
   "metadata": {},
   "source": [
    "### Turn on accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3eab29e-fa47-4a13-abc4-d6425ae741b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e040f1-e596-476d-9f26-ffa6f8a4a548",
   "metadata": {},
   "source": [
    "### LoRA - Low-Rank Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0532bdc6-4317-474b-859c-4e5d2fe6f1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd29494-4453-4c01-b878-ef2f4533d34d",
   "metadata": {},
   "source": [
    "### Inspect the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5058f6d-185d-45af-83b9-bb7f61d4bee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b14fe8f2-eccd-4cfe-9d20-ef48bc178842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 103563264 || all params: 35381092352 || trainable%: 0.29270793272765033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForSequenceClassification(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 8192)\n",
       "        (layers): ModuleList(\n",
       "          (0-79): 80 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=28672, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=28672, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=28672, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=28672, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=28672, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=28672, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=8192, out_features=2, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=8192, out_features=2, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_trainable_parameters(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17b4782-03f8-4335-bfda-2a65794bff2c",
   "metadata": {},
   "source": [
    "### Look at hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c27c3328-1926-4adc-8696-b3b343ec4afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 7\n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "GPU 1: NVIDIA GeForce RTX 4090\n",
      "GPU 2: NVIDIA GeForce RTX 4090\n",
      "GPU 3: NVIDIA GeForce RTX 3090 Ti\n",
      "GPU 4: NVIDIA GeForce RTX 3090 Ti\n",
      "GPU 5: NVIDIA GeForce RTX 3090\n",
      "GPU 6: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdf68d0e-8786-489d-a4b8-b7478513efea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun 10 04:47:01 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.67                 Driver Version: 550.67         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off |   00000000:01:00.0 Off |                  N/A |\n",
      "| 31%   38C    P2            124W /  420W |    5069MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        Off |   00000000:02:00.0 Off |                  Off |\n",
      "|  0%   48C    P2             72W /  450W |    8463MiB /  24564MiB |     18%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 3090 Ti     Off |   00000000:2B:00.0 Off |                  Off |\n",
      "| 32%   43C    P2            101W /  450W |    5081MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 3090        Off |   00000000:41:00.0 Off |                  N/A |\n",
      "| 32%   38C    P2            113W /  420W |    7227MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA GeForce RTX 4090        Off |   00000000:42:00.0 Off |                  Off |\n",
      "|  0%   52C    P2             58W /  450W |    5220MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA GeForce RTX 4090        Off |   00000000:61:00.0 Off |                  Off |\n",
      "|  0%   45C    P2             46W /  450W |    5208MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA GeForce RTX 3090 Ti     Off |   00000000:62:00.0 Off |                  Off |\n",
      "| 30%   40C    P2             97W /  450W |    5069MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      2515      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    0   N/A  N/A      4781      C   /usr/bin/python3                             5052MiB |\n",
      "|    1   N/A  N/A      2515      G   /usr/lib/xorg/Xorg                             86MiB |\n",
      "|    1   N/A  N/A      2599      G   /usr/bin/gnome-shell                           13MiB |\n",
      "|    1   N/A  N/A      4781      C   /usr/bin/python3                             8346MiB |\n",
      "|    2   N/A  N/A      2515      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    2   N/A  N/A      4781      C   /usr/bin/python3                             5064MiB |\n",
      "|    3   N/A  N/A      2515      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    3   N/A  N/A      4781      C   /usr/bin/python3                             7210MiB |\n",
      "|    4   N/A  N/A      2515      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    4   N/A  N/A      4781      C   /usr/bin/python3                             5202MiB |\n",
      "|    5   N/A  N/A      2515      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    5   N/A  N/A      4781      C   /usr/bin/python3                             5190MiB |\n",
      "|    6   N/A  N/A      2515      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    6   N/A  N/A      4781      C   /usr/bin/python3                             5052MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2340f2d1-1cc7-454e-bad6-bf4ac2c46fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1199841a-6519-4422-8259-2fdbb114e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(logits_and_labels):\n",
    "    logits, labels = logits_and_labels\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='macro')\n",
    "    return {'accuracy': acc, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8c7f3ee-6997-4ed7-b28e-5d574831fb08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./praxis-Meta-Llama-3-70B-small-finetune'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_name = \"praxis-\"+model_name+\"-small-finetune\"\n",
    "output_dir_path = \"./\" + project_name\n",
    "output_dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7a69bde-1d50-46a0-9838-00812afbdb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f3cd41-fd17-4fd8-83b8-54f464df79ff",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2df4e2a5-2a08-434b-983a-f6cd42f33da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 04:47:02.964765: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-10 04:47:03.617326: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnispoe\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nispoe/data/kuk/Praxis/wandb/run-20240610_044706-koii8j9m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nispoe/huggingface/runs/koii8j9m' target=\"_blank\">vocal-plant-378</a></strong> to <a href='https://wandb.ai/nispoe/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nispoe/huggingface' target=\"_blank\">https://wandb.ai/nispoe/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nispoe/huggingface/runs/koii8j9m' target=\"_blank\">https://wandb.ai/nispoe/huggingface/runs/koii8j9m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12606' max='12606' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12606/12606 35:06:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002042</td>\n",
       "      <td>0.999722</td>\n",
       "      <td>0.999707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12606, training_loss=0.0004971224649754778, metrics={'train_runtime': 126447.9573, 'train_samples_per_second': 0.797, 'train_steps_per_second': 0.1, 'total_flos': 2.1235817723556004e+19, 'train_loss': 0.0004971224649754778, 'epoch': 3.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from pathlib import Path\n",
    "\n",
    "transformers.set_seed(777)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_ds,\n",
    "    eval_dataset=tokenized_eval_ds,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir_path,\n",
    "        num_train_epochs=3,\n",
    "        logging_steps=10,\n",
    "        logging_dir=output_dir_path+\"/logs\",\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        do_eval=True,\n",
    "    ),\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "trainer.train(resume_from_checkpoint=True)  # Turn to True if power goes out..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d13982d-53cb-4c88-bd32-a98d4a5a9874",
   "metadata": {},
   "source": [
    "### Determine best checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8bd570c7-8feb-4b69-bda7-9c678bfd92c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 Jun  7 14:57 checkpoint-4202\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 Jun  9 02:06 checkpoint-8404\n",
      "drwxr-xr-x 2 nispoe nispoe 4096 Jun 10 04:47 logs\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 Jun 11 15:54 checkpoint-12606\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr {output_dir_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97771269-d77c-4c1b-b264-34e082db6cbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading events from file: ./praxis-Meta-Llama-3-70B-small-finetune/logs/events.out.tfevents.1717663737.hephaestus.17728.0\n",
      "WARNING:tensorflow:From /home/nispoe/.local/lib/python3.10/site-packages/tensorflow/python/summary/summary_iterator.py:27: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "Step: 10, train/loss: 1.5233999490737915\n",
      "Step: 10, train/grad_norm: 50.8408203125\n",
      "Step: 10, train/learning_rate: 4.9960337491938844e-05\n",
      "Step: 10, train/epoch: 0.0023798190522938967\n",
      "Step: 20, train/loss: 1.3343000411987305\n",
      "Step: 20, train/grad_norm: 22.7810001373291\n",
      "Step: 20, train/learning_rate: 4.992067260900512e-05\n",
      "Step: 20, train/epoch: 0.004759638104587793\n",
      "Step: 30, train/loss: 0.8616999983787537\n",
      "Step: 30, train/grad_norm: 41.443641662597656\n",
      "Step: 30, train/learning_rate: 4.98810077260714e-05\n",
      "Step: 30, train/epoch: 0.007139457389712334\n",
      "Step: 40, train/loss: 0.49410000443458557\n",
      "Step: 40, train/grad_norm: 36.65349197387695\n",
      "Step: 40, train/learning_rate: 4.984134648111649e-05\n",
      "Step: 40, train/epoch: 0.009519276209175587\n",
      "Step: 50, train/loss: 0.04500000178813934\n",
      "Step: 50, train/grad_norm: 0.9125515818595886\n",
      "Step: 50, train/learning_rate: 4.980168159818277e-05\n",
      "Step: 50, train/epoch: 0.011899095959961414\n",
      "Step: 60, train/loss: 0.06830000132322311\n",
      "Step: 60, train/grad_norm: 0.4801281988620758\n",
      "Step: 60, train/learning_rate: 4.976201671524905e-05\n",
      "Step: 60, train/epoch: 0.014278914779424667\n",
      "Step: 70, train/loss: 0.20759999752044678\n",
      "Step: 70, train/grad_norm: 10.63454532623291\n",
      "Step: 70, train/learning_rate: 4.972235547029413e-05\n",
      "Step: 70, train/epoch: 0.016658734530210495\n",
      "Step: 80, train/loss: 0.04580000042915344\n",
      "Step: 80, train/grad_norm: 2.984823368024081e-05\n",
      "Step: 80, train/learning_rate: 4.968269058736041e-05\n",
      "Step: 80, train/epoch: 0.019038552418351173\n",
      "Step: 90, train/loss: 0.25\n",
      "Step: 90, train/grad_norm: 3.789926097397256e-07\n",
      "Step: 90, train/learning_rate: 4.964302570442669e-05\n",
      "Step: 90, train/epoch: 0.021418372169137\n",
      "Step: 100, train/loss: 0.11320000141859055\n",
      "Step: 100, train/grad_norm: 0.0004901330103166401\n",
      "Step: 100, train/learning_rate: 4.960336445947178e-05\n",
      "Step: 100, train/epoch: 0.02379819191992283\n",
      "Step: 110, train/loss: 0.011599999852478504\n",
      "Step: 110, train/grad_norm: 7.538951081187406e-07\n",
      "Step: 110, train/learning_rate: 4.9563699576538056e-05\n",
      "Step: 110, train/epoch: 0.026178009808063507\n",
      "Step: 120, train/loss: 0.25940001010894775\n",
      "Step: 120, train/grad_norm: 2.2117898623719157e-08\n",
      "Step: 120, train/learning_rate: 4.9524034693604335e-05\n",
      "Step: 120, train/epoch: 0.028557829558849335\n",
      "Step: 130, train/loss: 9.999999747378752e-05\n",
      "Step: 130, train/grad_norm: 1.0396584571026324e-07\n",
      "Step: 130, train/learning_rate: 4.948437344864942e-05\n",
      "Step: 130, train/epoch: 0.030937649309635162\n",
      "Step: 140, train/loss: 0.34060001373291016\n",
      "Step: 140, train/grad_norm: 36.553958892822266\n",
      "Step: 140, train/learning_rate: 4.94447085657157e-05\n",
      "Step: 140, train/epoch: 0.03331746906042099\n",
      "Step: 150, train/loss: 0.0006000000284984708\n",
      "Step: 150, train/grad_norm: 0.019226865842938423\n",
      "Step: 150, train/learning_rate: 4.940504368278198e-05\n",
      "Step: 150, train/epoch: 0.03569728881120682\n",
      "Step: 160, train/loss: 0.18209999799728394\n",
      "Step: 160, train/grad_norm: 2.279935359954834\n",
      "Step: 160, train/learning_rate: 4.9365382437827066e-05\n",
      "Step: 160, train/epoch: 0.03807710483670235\n",
      "Step: 170, train/loss: 0.22370000183582306\n",
      "Step: 170, train/grad_norm: 0.010026735253632069\n",
      "Step: 170, train/learning_rate: 4.9325717554893345e-05\n",
      "Step: 170, train/epoch: 0.040456924587488174\n",
      "Step: 180, train/loss: 0.2768999934196472\n",
      "Step: 180, train/grad_norm: 115.44369506835938\n",
      "Step: 180, train/learning_rate: 4.9286052671959624e-05\n",
      "Step: 180, train/epoch: 0.042836744338274\n",
      "Step: 190, train/loss: 0.3562999963760376\n",
      "Step: 190, train/grad_norm: 211.43601989746094\n",
      "Step: 190, train/learning_rate: 4.924639142700471e-05\n",
      "Step: 190, train/epoch: 0.04521656408905983\n",
      "Step: 200, train/loss: 0.009999999776482582\n",
      "Step: 200, train/grad_norm: 0.010756638832390308\n",
      "Step: 200, train/learning_rate: 4.920672654407099e-05\n",
      "Step: 200, train/epoch: 0.04759638383984566\n",
      "Step: 210, train/loss: 0.006500000134110451\n",
      "Step: 210, train/grad_norm: 0.0005524966982193291\n",
      "Step: 210, train/learning_rate: 4.916706166113727e-05\n",
      "Step: 210, train/epoch: 0.049976203590631485\n",
      "Step: 220, train/loss: 0.21170000731945038\n",
      "Step: 220, train/grad_norm: 5.62481363886036e-06\n",
      "Step: 220, train/learning_rate: 4.9127400416182354e-05\n",
      "Step: 220, train/epoch: 0.052356019616127014\n",
      "Step: 230, train/loss: 0.15629999339580536\n",
      "Step: 230, train/grad_norm: 5.011291432310827e-05\n",
      "Step: 230, train/learning_rate: 4.908773553324863e-05\n",
      "Step: 230, train/epoch: 0.05473583936691284\n",
      "Step: 240, train/loss: 0.1460999995470047\n",
      "Step: 240, train/grad_norm: 0.0005144528695382178\n",
      "Step: 240, train/learning_rate: 4.904807065031491e-05\n",
      "Step: 240, train/epoch: 0.05711565911769867\n",
      "Step: 250, train/loss: 0.0\n",
      "Step: 250, train/grad_norm: 1.691859807806395e-07\n",
      "Step: 250, train/learning_rate: 4.900840940536e-05\n",
      "Step: 250, train/epoch: 0.0594954788684845\n",
      "Step: 260, train/loss: 0.07729999721050262\n",
      "Step: 260, train/grad_norm: 1.8565729988040403e-05\n",
      "Step: 260, train/learning_rate: 4.896874452242628e-05\n",
      "Step: 260, train/epoch: 0.061875298619270325\n",
      "Step: 270, train/loss: 0.0\n",
      "Step: 270, train/grad_norm: 2.9555277336612562e-08\n",
      "Step: 270, train/learning_rate: 4.8929079639492556e-05\n",
      "Step: 270, train/epoch: 0.06425511837005615\n",
      "Step: 280, train/loss: 0.06759999692440033\n",
      "Step: 280, train/grad_norm: 0.0005364647950045764\n",
      "Step: 280, train/learning_rate: 4.888941839453764e-05\n",
      "Step: 280, train/epoch: 0.06663493812084198\n",
      "Step: 290, train/loss: 0.0\n",
      "Step: 290, train/grad_norm: 1.055896336765727e-05\n",
      "Step: 290, train/learning_rate: 4.884975351160392e-05\n",
      "Step: 290, train/epoch: 0.06901475787162781\n",
      "Step: 300, train/loss: 0.0\n",
      "Step: 300, train/grad_norm: 4.5715041778748855e-06\n",
      "Step: 300, train/learning_rate: 4.88100886286702e-05\n",
      "Step: 300, train/epoch: 0.07139457762241364\n",
      "Step: 310, train/loss: 0.0005000000237487257\n",
      "Step: 310, train/grad_norm: 4.02080650019343e-06\n",
      "Step: 310, train/learning_rate: 4.877042738371529e-05\n",
      "Step: 310, train/epoch: 0.07377438992261887\n",
      "Step: 320, train/loss: 0.0\n",
      "Step: 320, train/grad_norm: 0.0001485404500272125\n",
      "Step: 320, train/learning_rate: 4.8730762500781566e-05\n",
      "Step: 320, train/epoch: 0.0761542096734047\n",
      "Step: 330, train/loss: 0.0\n",
      "Step: 330, train/grad_norm: 4.527995770331472e-06\n",
      "Step: 330, train/learning_rate: 4.869110125582665e-05\n",
      "Step: 330, train/epoch: 0.07853402942419052\n",
      "Step: 340, train/loss: 0.04830000177025795\n",
      "Step: 340, train/grad_norm: 218.5382843017578\n",
      "Step: 340, train/learning_rate: 4.865143637289293e-05\n",
      "Step: 340, train/epoch: 0.08091384917497635\n",
      "Step: 350, train/loss: 0.0\n",
      "Step: 350, train/grad_norm: 4.429004093253752e-06\n",
      "Step: 350, train/learning_rate: 4.861177148995921e-05\n",
      "Step: 350, train/epoch: 0.08329366892576218\n",
      "Step: 360, train/loss: 9.999999747378752e-05\n",
      "Step: 360, train/grad_norm: 1.0092267075378913e-05\n",
      "Step: 360, train/learning_rate: 4.8572110245004296e-05\n",
      "Step: 360, train/epoch: 0.085673488676548\n",
      "Step: 370, train/loss: 0.0\n",
      "Step: 370, train/grad_norm: 0.0005187962087802589\n",
      "Step: 370, train/learning_rate: 4.8532445362070575e-05\n",
      "Step: 370, train/epoch: 0.08805330842733383\n",
      "Step: 380, train/loss: 0.0\n",
      "Step: 380, train/grad_norm: 2.4993574697873555e-05\n",
      "Step: 380, train/learning_rate: 4.8492780479136854e-05\n",
      "Step: 380, train/epoch: 0.09043312817811966\n",
      "Step: 390, train/loss: 0.1281999945640564\n",
      "Step: 390, train/grad_norm: 0.29792794585227966\n",
      "Step: 390, train/learning_rate: 4.845311923418194e-05\n",
      "Step: 390, train/epoch: 0.09281294792890549\n",
      "Step: 400, train/loss: 0.00039999998989515007\n",
      "Step: 400, train/grad_norm: 1.5867217371123843e-05\n",
      "Step: 400, train/learning_rate: 4.841345435124822e-05\n",
      "Step: 400, train/epoch: 0.09519276767969131\n",
      "Step: 410, train/loss: 0.0\n",
      "Step: 410, train/grad_norm: 1.8973753412865335e-07\n",
      "Step: 410, train/learning_rate: 4.83737894683145e-05\n",
      "Step: 410, train/epoch: 0.09757258743047714\n",
      "Step: 420, train/loss: 0.06599999964237213\n",
      "Step: 420, train/grad_norm: 0.002930575516074896\n",
      "Step: 420, train/learning_rate: 4.8334128223359585e-05\n",
      "Step: 420, train/epoch: 0.09995240718126297\n",
      "Step: 430, train/loss: 0.0\n",
      "Step: 430, train/grad_norm: 1.3410010069492273e-05\n",
      "Step: 430, train/learning_rate: 4.8294463340425864e-05\n",
      "Step: 430, train/epoch: 0.1023322194814682\n",
      "Step: 440, train/loss: 0.2312999963760376\n",
      "Step: 440, train/grad_norm: 3.762261621886864e-05\n",
      "Step: 440, train/learning_rate: 4.825479845749214e-05\n",
      "Step: 440, train/epoch: 0.10471203923225403\n",
      "Step: 450, train/loss: 0.07429999858140945\n",
      "Step: 450, train/grad_norm: 3.572216033935547\n",
      "Step: 450, train/learning_rate: 4.821513721253723e-05\n",
      "Step: 450, train/epoch: 0.10709185898303986\n",
      "Step: 460, train/loss: 0.0\n",
      "Step: 460, train/grad_norm: 0.014298929832875729\n",
      "Step: 460, train/learning_rate: 4.817547232960351e-05\n",
      "Step: 460, train/epoch: 0.10947167873382568\n",
      "Step: 470, train/loss: 0.23280000686645508\n",
      "Step: 470, train/grad_norm: 0.000524279079400003\n",
      "Step: 470, train/learning_rate: 4.813580744666979e-05\n",
      "Step: 470, train/epoch: 0.11185149848461151\n",
      "Step: 480, train/loss: 0.00039999998989515007\n",
      "Step: 480, train/grad_norm: 0.0002864289563149214\n",
      "Step: 480, train/learning_rate: 4.809614620171487e-05\n",
      "Step: 480, train/epoch: 0.11423131823539734\n",
      "Step: 490, train/loss: 0.002300000051036477\n",
      "Step: 490, train/grad_norm: 0.012695805169641972\n",
      "Step: 490, train/learning_rate: 4.805648131878115e-05\n",
      "Step: 490, train/epoch: 0.11661113798618317\n",
      "Step: 500, train/loss: 0.046300001442432404\n",
      "Step: 500, train/grad_norm: 0.00010031615238403901\n",
      "Step: 500, train/learning_rate: 4.801681643584743e-05\n",
      "Step: 500, train/epoch: 0.118990957736969\n",
      "Step: 510, train/loss: 9.999999747378752e-05\n",
      "Step: 510, train/grad_norm: 9.934209629136603e-06\n",
      "Step: 510, train/learning_rate: 4.797715519089252e-05\n",
      "Step: 510, train/epoch: 0.12137077748775482\n",
      "Step: 520, train/loss: 0.0\n",
      "Step: 520, train/grad_norm: 0.0001492688897997141\n",
      "Step: 520, train/learning_rate: 4.79374903079588e-05\n",
      "Step: 520, train/epoch: 0.12375059723854065\n",
      "Step: 530, train/loss: 0.0940999984741211\n",
      "Step: 530, train/grad_norm: 0.00011478640954010189\n",
      "Step: 530, train/learning_rate: 4.7897825425025076e-05\n",
      "Step: 530, train/epoch: 0.12613041698932648\n",
      "Step: 540, train/loss: 0.0\n",
      "Step: 540, train/grad_norm: 2.9707946325174817e-08\n",
      "Step: 540, train/learning_rate: 4.785816418007016e-05\n",
      "Step: 540, train/epoch: 0.1285102367401123\n",
      "Step: 550, train/loss: 0.08060000091791153\n",
      "Step: 550, train/grad_norm: 78.2262191772461\n",
      "Step: 550, train/learning_rate: 4.781849929713644e-05\n",
      "Step: 550, train/epoch: 0.13089005649089813\n",
      "Step: 560, train/loss: 0.0\n",
      "Step: 560, train/grad_norm: 2.0955974378011888e-07\n",
      "Step: 560, train/learning_rate: 4.777883441420272e-05\n",
      "Step: 560, train/epoch: 0.13326987624168396\n",
      "Step: 570, train/loss: 0.0\n",
      "Step: 570, train/grad_norm: 8.823413963909843e-07\n",
      "Step: 570, train/learning_rate: 4.7739173169247806e-05\n",
      "Step: 570, train/epoch: 0.1356496959924698\n",
      "Step: 580, train/loss: 0.0\n",
      "Step: 580, train/grad_norm: 6.230795406736434e-05\n",
      "Step: 580, train/learning_rate: 4.7699508286314085e-05\n",
      "Step: 580, train/epoch: 0.13802951574325562\n",
      "Step: 590, train/loss: 0.15389999747276306\n",
      "Step: 590, train/grad_norm: 6.679168291157112e-05\n",
      "Step: 590, train/learning_rate: 4.7659843403380364e-05\n",
      "Step: 590, train/epoch: 0.14040933549404144\n",
      "Step: 600, train/loss: 0.18129999935626984\n",
      "Step: 600, train/grad_norm: 0.005633645225316286\n",
      "Step: 600, train/learning_rate: 4.762018215842545e-05\n",
      "Step: 600, train/epoch: 0.14278915524482727\n",
      "Step: 610, train/loss: 0.00570000009611249\n",
      "Step: 610, train/grad_norm: 4.425718307495117\n",
      "Step: 610, train/learning_rate: 4.758051727549173e-05\n",
      "Step: 610, train/epoch: 0.1451689600944519\n",
      "Step: 620, train/loss: 0.0\n",
      "Step: 620, train/grad_norm: 2.6813746444531716e-05\n",
      "Step: 620, train/learning_rate: 4.754085239255801e-05\n",
      "Step: 620, train/epoch: 0.14754877984523773\n",
      "Step: 630, train/loss: 0.0\n",
      "Step: 630, train/grad_norm: 7.475990315697345e-08\n",
      "Step: 630, train/learning_rate: 4.7501191147603095e-05\n",
      "Step: 630, train/epoch: 0.14992859959602356\n",
      "Step: 640, train/loss: 0.0\n",
      "Step: 640, train/grad_norm: 4.81875073177207e-09\n",
      "Step: 640, train/learning_rate: 4.7461526264669374e-05\n",
      "Step: 640, train/epoch: 0.1523084193468094\n",
      "Step: 650, train/loss: 0.0\n",
      "Step: 650, train/grad_norm: 6.414263109370821e-13\n",
      "Step: 650, train/learning_rate: 4.742186138173565e-05\n",
      "Step: 650, train/epoch: 0.15468823909759521\n",
      "Step: 660, train/loss: 0.0\n",
      "Step: 660, train/grad_norm: 3.03088825148734e-08\n",
      "Step: 660, train/learning_rate: 4.738220013678074e-05\n",
      "Step: 660, train/epoch: 0.15706805884838104\n",
      "Step: 670, train/loss: 0.0\n",
      "Step: 670, train/grad_norm: 3.6646254919503463e-09\n",
      "Step: 670, train/learning_rate: 4.734253525384702e-05\n",
      "Step: 670, train/epoch: 0.15944787859916687\n",
      "Step: 680, train/loss: 0.06800000369548798\n",
      "Step: 680, train/grad_norm: 4.9693919118576346e-11\n",
      "Step: 680, train/learning_rate: 4.73028703709133e-05\n",
      "Step: 680, train/epoch: 0.1618276983499527\n",
      "Step: 690, train/loss: 0.20669999718666077\n",
      "Step: 690, train/grad_norm: 5.136827894602902e-05\n",
      "Step: 690, train/learning_rate: 4.726320912595838e-05\n",
      "Step: 690, train/epoch: 0.16420751810073853\n",
      "Step: 700, train/loss: 0.04470000043511391\n",
      "Step: 700, train/grad_norm: 2.2270060640039446e-07\n",
      "Step: 700, train/learning_rate: 4.722354424302466e-05\n",
      "Step: 700, train/epoch: 0.16658733785152435\n",
      "Step: 710, train/loss: 0.0003000000142492354\n",
      "Step: 710, train/grad_norm: 2.0053408661624417e-05\n",
      "Step: 710, train/learning_rate: 4.718387936009094e-05\n",
      "Step: 710, train/epoch: 0.16896715760231018\n",
      "Step: 720, train/loss: 0.0\n",
      "Step: 720, train/grad_norm: 0.0004926729016005993\n",
      "Step: 720, train/learning_rate: 4.714421811513603e-05\n",
      "Step: 720, train/epoch: 0.171346977353096\n",
      "Step: 730, train/loss: 0.0\n",
      "Step: 730, train/grad_norm: 2.679149133655301e-07\n",
      "Step: 730, train/learning_rate: 4.7104553232202306e-05\n",
      "Step: 730, train/epoch: 0.17372679710388184\n",
      "Step: 740, train/loss: 0.5720000267028809\n",
      "Step: 740, train/grad_norm: 1.6526916724046714e-08\n",
      "Step: 740, train/learning_rate: 4.7064888349268585e-05\n",
      "Step: 740, train/epoch: 0.17610661685466766\n",
      "Step: 750, train/loss: 0.03920000046491623\n",
      "Step: 750, train/grad_norm: 0.02579658478498459\n",
      "Step: 750, train/learning_rate: 4.702522710431367e-05\n",
      "Step: 750, train/epoch: 0.1784864366054535\n",
      "Step: 760, train/loss: 0.0003000000142492354\n",
      "Step: 760, train/grad_norm: 1.6827833348997956e-08\n",
      "Step: 760, train/learning_rate: 4.698556222137995e-05\n",
      "Step: 760, train/epoch: 0.18086625635623932\n",
      "Step: 770, train/loss: 0.011500000022351742\n",
      "Step: 770, train/grad_norm: 2.3843705321269226e-07\n",
      "Step: 770, train/learning_rate: 4.694589733844623e-05\n",
      "Step: 770, train/epoch: 0.18324607610702515\n",
      "Step: 780, train/loss: 0.5062999725341797\n",
      "Step: 780, train/grad_norm: 9.143536772171501e-08\n",
      "Step: 780, train/learning_rate: 4.6906236093491316e-05\n",
      "Step: 780, train/epoch: 0.18562589585781097\n",
      "Step: 790, train/loss: 0.0\n",
      "Step: 790, train/grad_norm: 4.263246955815703e-05\n",
      "Step: 790, train/learning_rate: 4.6866571210557595e-05\n",
      "Step: 790, train/epoch: 0.1880057156085968\n",
      "Step: 800, train/loss: 0.0\n",
      "Step: 800, train/grad_norm: 0.0001524948893347755\n",
      "Step: 800, train/learning_rate: 4.6826906327623874e-05\n",
      "Step: 800, train/epoch: 0.19038553535938263\n",
      "Step: 810, train/loss: 0.14830000698566437\n",
      "Step: 810, train/grad_norm: 0.0003993749851360917\n",
      "Step: 810, train/learning_rate: 4.678724508266896e-05\n",
      "Step: 810, train/epoch: 0.19276535511016846\n",
      "Step: 820, train/loss: 0.021199999377131462\n",
      "Step: 820, train/grad_norm: 0.002277626423165202\n",
      "Step: 820, train/learning_rate: 4.674758019973524e-05\n",
      "Step: 820, train/epoch: 0.19514517486095428\n",
      "Step: 830, train/loss: 0.0010000000474974513\n",
      "Step: 830, train/grad_norm: 3.6222292526533195e-11\n",
      "Step: 830, train/learning_rate: 4.670791531680152e-05\n",
      "Step: 830, train/epoch: 0.1975249946117401\n",
      "Step: 840, train/loss: 0.0017000000225380063\n",
      "Step: 840, train/grad_norm: 5.936188429700451e-09\n",
      "Step: 840, train/learning_rate: 4.6668254071846604e-05\n",
      "Step: 840, train/epoch: 0.19990481436252594\n",
      "Step: 850, train/loss: 0.0\n",
      "Step: 850, train/grad_norm: 1.1829839191568325e-11\n",
      "Step: 850, train/learning_rate: 4.6628589188912883e-05\n",
      "Step: 850, train/epoch: 0.20228461921215057\n",
      "Step: 860, train/loss: 0.0\n",
      "Step: 860, train/grad_norm: 4.0914126253621674e-11\n",
      "Step: 860, train/learning_rate: 4.658892430597916e-05\n",
      "Step: 860, train/epoch: 0.2046644389629364\n",
      "Step: 870, train/loss: 0.03790000081062317\n",
      "Step: 870, train/grad_norm: 3.073362148543063e-13\n",
      "Step: 870, train/learning_rate: 4.654926306102425e-05\n",
      "Step: 870, train/epoch: 0.20704425871372223\n",
      "Step: 880, train/loss: 0.0\n",
      "Step: 880, train/grad_norm: 2.016213151989632e-08\n",
      "Step: 880, train/learning_rate: 4.650959817809053e-05\n",
      "Step: 880, train/epoch: 0.20942407846450806\n",
      "Step: 890, train/loss: 0.0\n",
      "Step: 890, train/grad_norm: 1.8680647306812403e-10\n",
      "Step: 890, train/learning_rate: 4.646993329515681e-05\n",
      "Step: 890, train/epoch: 0.21180389821529388\n",
      "Step: 900, train/loss: 0.014499999582767487\n",
      "Step: 900, train/grad_norm: 3.8784132155855744e-11\n",
      "Step: 900, train/learning_rate: 4.643027205020189e-05\n",
      "Step: 900, train/epoch: 0.2141837179660797\n",
      "Step: 910, train/loss: 0.0\n",
      "Step: 910, train/grad_norm: 1.0001135475334877e-09\n",
      "Step: 910, train/learning_rate: 4.639060716726817e-05\n",
      "Step: 910, train/epoch: 0.21656353771686554\n",
      "Step: 920, train/loss: 0.00019999999494757503\n",
      "Step: 920, train/grad_norm: 6.697387977538938e-09\n",
      "Step: 920, train/learning_rate: 4.635094228433445e-05\n",
      "Step: 920, train/epoch: 0.21894335746765137\n",
      "Step: 930, train/loss: 0.0\n",
      "Step: 930, train/grad_norm: 3.8294376736303093e-07\n",
      "Step: 930, train/learning_rate: 4.631128103937954e-05\n",
      "Step: 930, train/epoch: 0.2213231772184372\n",
      "Step: 940, train/loss: 0.33180001378059387\n",
      "Step: 940, train/grad_norm: 95.8254165649414\n",
      "Step: 940, train/learning_rate: 4.6271616156445816e-05\n",
      "Step: 940, train/epoch: 0.22370299696922302\n",
      "Step: 950, train/loss: 0.019700000062584877\n",
      "Step: 950, train/grad_norm: 1.022566475938902e-09\n",
      "Step: 950, train/learning_rate: 4.6231951273512095e-05\n",
      "Step: 950, train/epoch: 0.22608281672000885\n",
      "Step: 960, train/loss: 0.1746000051498413\n",
      "Step: 960, train/grad_norm: 4.5976334718533796e-12\n",
      "Step: 960, train/learning_rate: 4.619229002855718e-05\n",
      "Step: 960, train/epoch: 0.22846263647079468\n",
      "Step: 970, train/loss: 0.0\n",
      "Step: 970, train/grad_norm: 1.0567365871239076e-09\n",
      "Step: 970, train/learning_rate: 4.615262514562346e-05\n",
      "Step: 970, train/epoch: 0.2308424562215805\n",
      "Step: 980, train/loss: 0.012900000438094139\n",
      "Step: 980, train/grad_norm: 9.045571403021313e-08\n",
      "Step: 980, train/learning_rate: 4.611296026268974e-05\n",
      "Step: 980, train/epoch: 0.23322227597236633\n",
      "Step: 990, train/loss: 0.0\n",
      "Step: 990, train/grad_norm: 0.0016425902722403407\n",
      "Step: 990, train/learning_rate: 4.6073299017734826e-05\n",
      "Step: 990, train/epoch: 0.23560209572315216\n",
      "Step: 1000, train/loss: 0.0\n",
      "Step: 1000, train/grad_norm: 3.690364991371098e-08\n",
      "Step: 1000, train/learning_rate: 4.6033634134801105e-05\n",
      "Step: 1000, train/epoch: 0.237981915473938\n",
      "Step: 1010, train/loss: 0.5874999761581421\n",
      "Step: 1010, train/grad_norm: 4.935565812047571e-06\n",
      "Step: 1010, train/learning_rate: 4.599397288984619e-05\n",
      "Step: 1010, train/epoch: 0.24036173522472382\n",
      "Step: 1020, train/loss: 0.0005000000237487257\n",
      "Step: 1020, train/grad_norm: 1.8509109800390888e-09\n",
      "Step: 1020, train/learning_rate: 4.595430800691247e-05\n",
      "Step: 1020, train/epoch: 0.24274155497550964\n",
      "Step: 1030, train/loss: 0.05000000074505806\n",
      "Step: 1030, train/grad_norm: 1.424617090961533e-09\n",
      "Step: 1030, train/learning_rate: 4.591464312397875e-05\n",
      "Step: 1030, train/epoch: 0.24512137472629547\n",
      "Step: 1040, train/loss: 0.0\n",
      "Step: 1040, train/grad_norm: 9.221125252256712e-11\n",
      "Step: 1040, train/learning_rate: 4.5874981879023835e-05\n",
      "Step: 1040, train/epoch: 0.2475011944770813\n",
      "Step: 1050, train/loss: 0.0\n",
      "Step: 1050, train/grad_norm: 8.396066064619845e-09\n",
      "Step: 1050, train/learning_rate: 4.5835316996090114e-05\n",
      "Step: 1050, train/epoch: 0.24988101422786713\n",
      "Step: 1060, train/loss: 0.19059999287128448\n",
      "Step: 1060, train/grad_norm: 0.06466285139322281\n",
      "Step: 1060, train/learning_rate: 4.579565211315639e-05\n",
      "Step: 1060, train/epoch: 0.25226083397865295\n",
      "Step: 1070, train/loss: 0.3174999952316284\n",
      "Step: 1070, train/grad_norm: 6.43745012851582e-09\n",
      "Step: 1070, train/learning_rate: 4.575599086820148e-05\n",
      "Step: 1070, train/epoch: 0.2546406388282776\n",
      "Step: 1080, train/loss: 0.0010999999940395355\n",
      "Step: 1080, train/grad_norm: 1.825443823877393e-10\n",
      "Step: 1080, train/learning_rate: 4.571632598526776e-05\n",
      "Step: 1080, train/epoch: 0.2570204734802246\n",
      "Step: 1090, train/loss: 0.005200000014156103\n",
      "Step: 1090, train/grad_norm: 8.716783406192974e-10\n",
      "Step: 1090, train/learning_rate: 4.567666110233404e-05\n",
      "Step: 1090, train/epoch: 0.25940027832984924\n",
      "Step: 1100, train/loss: 0.0729999989271164\n",
      "Step: 1100, train/grad_norm: 1.1056540216713984e-07\n",
      "Step: 1100, train/learning_rate: 4.5636999857379124e-05\n",
      "Step: 1100, train/epoch: 0.26178011298179626\n",
      "Step: 1110, train/loss: 0.0\n",
      "Step: 1110, train/grad_norm: 1.2589788100925148e-09\n",
      "Step: 1110, train/learning_rate: 4.55973349744454e-05\n",
      "Step: 1110, train/epoch: 0.2641599178314209\n",
      "Step: 1120, train/loss: 0.1687999963760376\n",
      "Step: 1120, train/grad_norm: 3.115429763056454e-07\n",
      "Step: 1120, train/learning_rate: 4.555767009151168e-05\n",
      "Step: 1120, train/epoch: 0.2665397524833679\n",
      "Step: 1130, train/loss: 0.0020000000949949026\n",
      "Step: 1130, train/grad_norm: 3.911979547410738e-06\n",
      "Step: 1130, train/learning_rate: 4.551800884655677e-05\n",
      "Step: 1130, train/epoch: 0.26891955733299255\n",
      "Step: 1140, train/loss: 0.0\n",
      "Step: 1140, train/grad_norm: 2.24438689855333e-10\n",
      "Step: 1140, train/learning_rate: 4.547834396362305e-05\n",
      "Step: 1140, train/epoch: 0.2712993919849396\n",
      "Step: 1150, train/loss: 0.0\n",
      "Step: 1150, train/grad_norm: 8.263723998425121e-07\n",
      "Step: 1150, train/learning_rate: 4.5438679080689326e-05\n",
      "Step: 1150, train/epoch: 0.2736791968345642\n",
      "Step: 1160, train/loss: 0.0\n",
      "Step: 1160, train/grad_norm: 3.1353373231013393e-09\n",
      "Step: 1160, train/learning_rate: 4.539901783573441e-05\n",
      "Step: 1160, train/epoch: 0.27605903148651123\n",
      "Step: 1170, train/loss: 0.0\n",
      "Step: 1170, train/grad_norm: 8.704129754733003e-07\n",
      "Step: 1170, train/learning_rate: 4.535935295280069e-05\n",
      "Step: 1170, train/epoch: 0.27843883633613586\n",
      "Step: 1180, train/loss: 0.0\n",
      "Step: 1180, train/grad_norm: 4.980430512446787e-10\n",
      "Step: 1180, train/learning_rate: 4.531968806986697e-05\n",
      "Step: 1180, train/epoch: 0.2808186709880829\n",
      "Step: 1190, train/loss: 0.0\n",
      "Step: 1190, train/grad_norm: 2.59844729066927e-10\n",
      "Step: 1190, train/learning_rate: 4.5280026824912056e-05\n",
      "Step: 1190, train/epoch: 0.2831984758377075\n",
      "Step: 1200, train/loss: 0.0010000000474974513\n",
      "Step: 1200, train/grad_norm: 1.360734040645184e-05\n",
      "Step: 1200, train/learning_rate: 4.5240361941978335e-05\n",
      "Step: 1200, train/epoch: 0.28557831048965454\n",
      "Step: 1210, train/loss: 0.0026000000070780516\n",
      "Step: 1210, train/grad_norm: 12.662961959838867\n",
      "Step: 1210, train/learning_rate: 4.5200697059044614e-05\n",
      "Step: 1210, train/epoch: 0.2879581153392792\n",
      "Step: 1220, train/loss: 0.0\n",
      "Step: 1220, train/grad_norm: 1.4303750406341464e-09\n",
      "Step: 1220, train/learning_rate: 4.51610358140897e-05\n",
      "Step: 1220, train/epoch: 0.2903379201889038\n",
      "Step: 1230, train/loss: 0.0\n",
      "Step: 1230, train/grad_norm: 8.30621615932614e-09\n",
      "Step: 1230, train/learning_rate: 4.512137093115598e-05\n",
      "Step: 1230, train/epoch: 0.29271775484085083\n",
      "Step: 1240, train/loss: 0.0\n",
      "Step: 1240, train/grad_norm: 3.892339009325951e-06\n",
      "Step: 1240, train/learning_rate: 4.508170604822226e-05\n",
      "Step: 1240, train/epoch: 0.29509755969047546\n",
      "Step: 1250, train/loss: 0.0\n",
      "Step: 1250, train/grad_norm: 2.499133699984668e-08\n",
      "Step: 1250, train/learning_rate: 4.5042044803267345e-05\n",
      "Step: 1250, train/epoch: 0.2974773943424225\n",
      "Step: 1260, train/loss: 0.0\n",
      "Step: 1260, train/grad_norm: 9.524157721166326e-13\n",
      "Step: 1260, train/learning_rate: 4.5002379920333624e-05\n",
      "Step: 1260, train/epoch: 0.2998571991920471\n",
      "Step: 1270, train/loss: 0.0\n",
      "Step: 1270, train/grad_norm: 2.3257316072999856e-09\n",
      "Step: 1270, train/learning_rate: 4.49627150373999e-05\n",
      "Step: 1270, train/epoch: 0.30223703384399414\n",
      "Step: 1280, train/loss: 0.0\n",
      "Step: 1280, train/grad_norm: 9.739671742725609e-12\n",
      "Step: 1280, train/learning_rate: 4.492305379244499e-05\n",
      "Step: 1280, train/epoch: 0.3046168386936188\n",
      "Step: 1290, train/loss: 0.0\n",
      "Step: 1290, train/grad_norm: 2.793044684423762e-09\n",
      "Step: 1290, train/learning_rate: 4.488338890951127e-05\n",
      "Step: 1290, train/epoch: 0.3069966733455658\n",
      "Step: 1300, train/loss: 0.0\n",
      "Step: 1300, train/grad_norm: 4.730994242196918e-13\n",
      "Step: 1300, train/learning_rate: 4.484372402657755e-05\n",
      "Step: 1300, train/epoch: 0.30937647819519043\n",
      "Step: 1310, train/loss: 0.05119999870657921\n",
      "Step: 1310, train/grad_norm: 1.7065580892872134e-10\n",
      "Step: 1310, train/learning_rate: 4.480406278162263e-05\n",
      "Step: 1310, train/epoch: 0.31175631284713745\n",
      "Step: 1320, train/loss: 0.0\n",
      "Step: 1320, train/grad_norm: 0.0006334357894957066\n",
      "Step: 1320, train/learning_rate: 4.476439789868891e-05\n",
      "Step: 1320, train/epoch: 0.3141361176967621\n",
      "Step: 1330, train/loss: 0.0017000000225380063\n",
      "Step: 1330, train/grad_norm: 346.0721130371094\n",
      "Step: 1330, train/learning_rate: 4.472473301575519e-05\n",
      "Step: 1330, train/epoch: 0.3165159523487091\n",
      "Step: 1340, train/loss: 0.0\n",
      "Step: 1340, train/grad_norm: 2.678081933993326e-08\n",
      "Step: 1340, train/learning_rate: 4.468507177080028e-05\n",
      "Step: 1340, train/epoch: 0.31889575719833374\n",
      "Step: 1350, train/loss: 9.999999747378752e-05\n",
      "Step: 1350, train/grad_norm: 2.4852188015844767e-11\n",
      "Step: 1350, train/learning_rate: 4.464540688786656e-05\n",
      "Step: 1350, train/epoch: 0.32127559185028076\n",
      "Step: 1360, train/loss: 0.2312999963760376\n",
      "Step: 1360, train/grad_norm: 1.9689055719140924e-08\n",
      "Step: 1360, train/learning_rate: 4.4605742004932836e-05\n",
      "Step: 1360, train/epoch: 0.3236553966999054\n",
      "Step: 1370, train/loss: 0.0\n",
      "Step: 1370, train/grad_norm: 9.102628590121342e-13\n",
      "Step: 1370, train/learning_rate: 4.456608075997792e-05\n",
      "Step: 1370, train/epoch: 0.3260352313518524\n",
      "Step: 1380, train/loss: 0.0\n",
      "Step: 1380, train/grad_norm: 2.436587237752974e-05\n",
      "Step: 1380, train/learning_rate: 4.45264158770442e-05\n",
      "Step: 1380, train/epoch: 0.32841503620147705\n",
      "Step: 1390, train/loss: 0.0\n",
      "Step: 1390, train/grad_norm: 5.979899952990309e-08\n",
      "Step: 1390, train/learning_rate: 4.448675099411048e-05\n",
      "Step: 1390, train/epoch: 0.3307948708534241\n",
      "Step: 1400, train/loss: 0.0\n",
      "Step: 1400, train/grad_norm: 6.3094334201707625e-09\n",
      "Step: 1400, train/learning_rate: 4.4447089749155566e-05\n",
      "Step: 1400, train/epoch: 0.3331746757030487\n",
      "Step: 1410, train/loss: 0.0\n",
      "Step: 1410, train/grad_norm: 3.119501457149454e-07\n",
      "Step: 1410, train/learning_rate: 4.4407424866221845e-05\n",
      "Step: 1410, train/epoch: 0.3355545103549957\n",
      "Step: 1420, train/loss: 0.0\n",
      "Step: 1420, train/grad_norm: 1.0101390210404126e-11\n",
      "Step: 1420, train/learning_rate: 4.4367759983288124e-05\n",
      "Step: 1420, train/epoch: 0.33793431520462036\n",
      "Step: 1430, train/loss: 0.0\n",
      "Step: 1430, train/grad_norm: 8.036404874900427e-11\n",
      "Step: 1430, train/learning_rate: 4.432809873833321e-05\n",
      "Step: 1430, train/epoch: 0.3403141498565674\n",
      "Step: 1440, train/loss: 0.0\n",
      "Step: 1440, train/grad_norm: 2.887012487917673e-05\n",
      "Step: 1440, train/learning_rate: 4.428843385539949e-05\n",
      "Step: 1440, train/epoch: 0.342693954706192\n",
      "Step: 1450, train/loss: 0.0\n",
      "Step: 1450, train/grad_norm: 9.91395268101769e-08\n",
      "Step: 1450, train/learning_rate: 4.424876897246577e-05\n",
      "Step: 1450, train/epoch: 0.34507375955581665\n",
      "Step: 1460, train/loss: 0.0\n",
      "Step: 1460, train/grad_norm: 6.466303048568989e-09\n",
      "Step: 1460, train/learning_rate: 4.4209107727510855e-05\n",
      "Step: 1460, train/epoch: 0.34745359420776367\n",
      "Step: 1470, train/loss: 0.0\n",
      "Step: 1470, train/grad_norm: 1.1694691437469373e-08\n",
      "Step: 1470, train/learning_rate: 4.4169442844577134e-05\n",
      "Step: 1470, train/epoch: 0.3498333990573883\n",
      "Step: 1480, train/loss: 0.0\n",
      "Step: 1480, train/grad_norm: 5.2888973582632115e-12\n",
      "Step: 1480, train/learning_rate: 4.412977796164341e-05\n",
      "Step: 1480, train/epoch: 0.3522132337093353\n",
      "Step: 1490, train/loss: 0.0\n",
      "Step: 1490, train/grad_norm: 0.00047379129682667553\n",
      "Step: 1490, train/learning_rate: 4.40901167166885e-05\n",
      "Step: 1490, train/epoch: 0.35459303855895996\n",
      "Step: 1500, train/loss: 0.0\n",
      "Step: 1500, train/grad_norm: 5.132459790502253e-09\n",
      "Step: 1500, train/learning_rate: 4.405045183375478e-05\n",
      "Step: 1500, train/epoch: 0.356972873210907\n",
      "Step: 1510, train/loss: 0.0\n",
      "Step: 1510, train/grad_norm: 1.1387646468158374e-14\n",
      "Step: 1510, train/learning_rate: 4.401078695082106e-05\n",
      "Step: 1510, train/epoch: 0.3593526780605316\n",
      "Step: 1520, train/loss: 0.0\n",
      "Step: 1520, train/grad_norm: 6.3767471303322054e-09\n",
      "Step: 1520, train/learning_rate: 4.397112570586614e-05\n",
      "Step: 1520, train/epoch: 0.36173251271247864\n",
      "Step: 1530, train/loss: 0.0\n",
      "Step: 1530, train/grad_norm: 1.0803661076863591e-08\n",
      "Step: 1530, train/learning_rate: 4.393146082293242e-05\n",
      "Step: 1530, train/epoch: 0.36411231756210327\n",
      "Step: 1540, train/loss: 9.999999747378752e-05\n",
      "Step: 1540, train/grad_norm: 2.3947131921886466e-05\n",
      "Step: 1540, train/learning_rate: 4.38917959399987e-05\n",
      "Step: 1540, train/epoch: 0.3664921522140503\n",
      "Step: 1550, train/loss: 0.0\n",
      "Step: 1550, train/grad_norm: 4.936391739818191e-09\n",
      "Step: 1550, train/learning_rate: 4.385213469504379e-05\n",
      "Step: 1550, train/epoch: 0.3688719570636749\n",
      "Step: 1560, train/loss: 0.0\n",
      "Step: 1560, train/grad_norm: 5.5313472552276366e-11\n",
      "Step: 1560, train/learning_rate: 4.3812469812110066e-05\n",
      "Step: 1560, train/epoch: 0.37125179171562195\n",
      "Step: 1570, train/loss: 0.0\n",
      "Step: 1570, train/grad_norm: 0.0031240838579833508\n",
      "Step: 1570, train/learning_rate: 4.3772804929176345e-05\n",
      "Step: 1570, train/epoch: 0.3736315965652466\n",
      "Step: 1580, train/loss: 0.0\n",
      "Step: 1580, train/grad_norm: 0.00022137649648357183\n",
      "Step: 1580, train/learning_rate: 4.373314368422143e-05\n",
      "Step: 1580, train/epoch: 0.3760114312171936\n",
      "Step: 1590, train/loss: 0.0\n",
      "Step: 1590, train/grad_norm: 5.540646341728461e-08\n",
      "Step: 1590, train/learning_rate: 4.369347880128771e-05\n",
      "Step: 1590, train/epoch: 0.37839123606681824\n",
      "Step: 1600, train/loss: 0.0\n",
      "Step: 1600, train/grad_norm: 4.4686598812404554e-06\n",
      "Step: 1600, train/learning_rate: 4.365381391835399e-05\n",
      "Step: 1600, train/epoch: 0.38077107071876526\n",
      "Step: 1610, train/loss: 0.0\n",
      "Step: 1610, train/grad_norm: 0.006647789850831032\n",
      "Step: 1610, train/learning_rate: 4.3614152673399076e-05\n",
      "Step: 1610, train/epoch: 0.3831508755683899\n",
      "Step: 1620, train/loss: 0.010599999688565731\n",
      "Step: 1620, train/grad_norm: 8.06702171729512e-09\n",
      "Step: 1620, train/learning_rate: 4.3574487790465355e-05\n",
      "Step: 1620, train/epoch: 0.3855307102203369\n",
      "Step: 1630, train/loss: 0.0\n",
      "Step: 1630, train/grad_norm: 4.4052574088571106e-11\n",
      "Step: 1630, train/learning_rate: 4.3534822907531634e-05\n",
      "Step: 1630, train/epoch: 0.38791051506996155\n",
      "Step: 1640, train/loss: 0.17659999430179596\n",
      "Step: 1640, train/grad_norm: 1.536458604789459e-08\n",
      "Step: 1640, train/learning_rate: 4.349516166257672e-05\n",
      "Step: 1640, train/epoch: 0.39029034972190857\n",
      "Step: 1650, train/loss: 0.0\n",
      "Step: 1650, train/grad_norm: 1.2085265232997244e-08\n",
      "Step: 1650, train/learning_rate: 4.3455496779643e-05\n",
      "Step: 1650, train/epoch: 0.3926701545715332\n",
      "Step: 1660, train/loss: 0.0\n",
      "Step: 1660, train/grad_norm: 4.2810381017943655e-08\n",
      "Step: 1660, train/learning_rate: 4.3415835534688085e-05\n",
      "Step: 1660, train/epoch: 0.3950499892234802\n",
      "Step: 1670, train/loss: 0.0\n",
      "Step: 1670, train/grad_norm: 1.0193772502541457e-13\n",
      "Step: 1670, train/learning_rate: 4.3376170651754364e-05\n",
      "Step: 1670, train/epoch: 0.39742979407310486\n",
      "Step: 1680, train/loss: 9.999999747378752e-05\n",
      "Step: 1680, train/grad_norm: 3.1054012139541953e-10\n",
      "Step: 1680, train/learning_rate: 4.3336505768820643e-05\n",
      "Step: 1680, train/epoch: 0.3998096287250519\n",
      "Step: 1690, train/loss: 0.0\n",
      "Step: 1690, train/grad_norm: 3.0771003523000218e-09\n",
      "Step: 1690, train/learning_rate: 4.329684452386573e-05\n",
      "Step: 1690, train/epoch: 0.4021894335746765\n",
      "Step: 1700, train/loss: 0.20469999313354492\n",
      "Step: 1700, train/grad_norm: 1.0350902357458835e-07\n",
      "Step: 1700, train/learning_rate: 4.325717964093201e-05\n",
      "Step: 1700, train/epoch: 0.40456923842430115\n",
      "Step: 1710, train/loss: 0.09220000356435776\n",
      "Step: 1710, train/grad_norm: 8.288154731417308e-07\n",
      "Step: 1710, train/learning_rate: 4.321751475799829e-05\n",
      "Step: 1710, train/epoch: 0.40694907307624817\n",
      "Step: 1720, train/loss: 0.0031999999191612005\n",
      "Step: 1720, train/grad_norm: 9.505699836154236e-07\n",
      "Step: 1720, train/learning_rate: 4.3177853513043374e-05\n",
      "Step: 1720, train/epoch: 0.4093288779258728\n",
      "Step: 1730, train/loss: 0.10080000013113022\n",
      "Step: 1730, train/grad_norm: 3.648577546755405e-07\n",
      "Step: 1730, train/learning_rate: 4.313818863010965e-05\n",
      "Step: 1730, train/epoch: 0.4117087125778198\n",
      "Step: 1740, train/loss: 0.0\n",
      "Step: 1740, train/grad_norm: 6.050284273584339e-09\n",
      "Step: 1740, train/learning_rate: 4.309852374717593e-05\n",
      "Step: 1740, train/epoch: 0.41408851742744446\n",
      "Step: 1750, train/loss: 0.0\n",
      "Step: 1750, train/grad_norm: 0.029153674840927124\n",
      "Step: 1750, train/learning_rate: 4.305886250222102e-05\n",
      "Step: 1750, train/epoch: 0.4164683520793915\n",
      "Step: 1760, train/loss: 0.0\n",
      "Step: 1760, train/grad_norm: 2.4525206518410947e-11\n",
      "Step: 1760, train/learning_rate: 4.30191976192873e-05\n",
      "Step: 1760, train/epoch: 0.4188481569290161\n",
      "Step: 1770, train/loss: 0.0005000000237487257\n",
      "Step: 1770, train/grad_norm: 1.2690560602379719e-08\n",
      "Step: 1770, train/learning_rate: 4.2979532736353576e-05\n",
      "Step: 1770, train/epoch: 0.42122799158096313\n",
      "Step: 1780, train/loss: 0.0\n",
      "Step: 1780, train/grad_norm: 9.133740604738705e-06\n",
      "Step: 1780, train/learning_rate: 4.293987149139866e-05\n",
      "Step: 1780, train/epoch: 0.42360779643058777\n",
      "Step: 1790, train/loss: 0.0\n",
      "Step: 1790, train/grad_norm: 1.778073097557977e-14\n",
      "Step: 1790, train/learning_rate: 4.290020660846494e-05\n",
      "Step: 1790, train/epoch: 0.4259876310825348\n",
      "Step: 1800, train/loss: 0.0\n",
      "Step: 1800, train/grad_norm: 1.1457150506100078e-10\n",
      "Step: 1800, train/learning_rate: 4.286054172553122e-05\n",
      "Step: 1800, train/epoch: 0.4283674359321594\n",
      "Step: 1810, train/loss: 0.0\n",
      "Step: 1810, train/grad_norm: 4.3679739825108754e-09\n",
      "Step: 1810, train/learning_rate: 4.2820880480576307e-05\n",
      "Step: 1810, train/epoch: 0.43074727058410645\n",
      "Step: 1820, train/loss: 0.0\n",
      "Step: 1820, train/grad_norm: 0.0013183319242671132\n",
      "Step: 1820, train/learning_rate: 4.2781215597642586e-05\n",
      "Step: 1820, train/epoch: 0.4331270754337311\n",
      "Step: 1830, train/loss: 9.999999747378752e-05\n",
      "Step: 1830, train/grad_norm: 2.8070453947370844e-11\n",
      "Step: 1830, train/learning_rate: 4.2741550714708865e-05\n",
      "Step: 1830, train/epoch: 0.4355069100856781\n",
      "Step: 1840, train/loss: 0.0\n",
      "Step: 1840, train/grad_norm: 1.9839964632110962e-14\n",
      "Step: 1840, train/learning_rate: 4.270188946975395e-05\n",
      "Step: 1840, train/epoch: 0.43788671493530273\n",
      "Step: 1850, train/loss: 0.12120000272989273\n",
      "Step: 1850, train/grad_norm: 235.7143096923828\n",
      "Step: 1850, train/learning_rate: 4.266222458682023e-05\n",
      "Step: 1850, train/epoch: 0.44026654958724976\n",
      "Step: 1860, train/loss: 0.0\n",
      "Step: 1860, train/grad_norm: 6.564560521837848e-07\n",
      "Step: 1860, train/learning_rate: 4.262255970388651e-05\n",
      "Step: 1860, train/epoch: 0.4426463544368744\n",
      "Step: 1870, train/loss: 0.0\n",
      "Step: 1870, train/grad_norm: 3.102418877354296e-10\n",
      "Step: 1870, train/learning_rate: 4.2582898458931595e-05\n",
      "Step: 1870, train/epoch: 0.4450261890888214\n",
      "Step: 1880, train/loss: 0.11089999973773956\n",
      "Step: 1880, train/grad_norm: 1.8477204832834104e-07\n",
      "Step: 1880, train/learning_rate: 4.2543233575997874e-05\n",
      "Step: 1880, train/epoch: 0.44740599393844604\n",
      "Step: 1890, train/loss: 0.0\n",
      "Step: 1890, train/grad_norm: 7.67909796195454e-07\n",
      "Step: 1890, train/learning_rate: 4.250356869306415e-05\n",
      "Step: 1890, train/epoch: 0.44978582859039307\n",
      "Step: 1900, train/loss: 0.0\n",
      "Step: 1900, train/grad_norm: 2.8200872193018256e-10\n",
      "Step: 1900, train/learning_rate: 4.246390744810924e-05\n",
      "Step: 1900, train/epoch: 0.4521656334400177\n",
      "Step: 1910, train/loss: 0.0\n",
      "Step: 1910, train/grad_norm: 1.6435620864285738e-06\n",
      "Step: 1910, train/learning_rate: 4.242424256517552e-05\n",
      "Step: 1910, train/epoch: 0.4545454680919647\n",
      "Step: 1920, train/loss: 0.05739999935030937\n",
      "Step: 1920, train/grad_norm: 1.260685280612961e-06\n",
      "Step: 1920, train/learning_rate: 4.23845776822418e-05\n",
      "Step: 1920, train/epoch: 0.45692527294158936\n",
      "Step: 1930, train/loss: 0.0\n",
      "Step: 1930, train/grad_norm: 5.835721094626933e-05\n",
      "Step: 1930, train/learning_rate: 4.2344916437286884e-05\n",
      "Step: 1930, train/epoch: 0.4593051075935364\n",
      "Step: 1940, train/loss: 0.0\n",
      "Step: 1940, train/grad_norm: 4.004453019179621e-11\n",
      "Step: 1940, train/learning_rate: 4.230525155435316e-05\n",
      "Step: 1940, train/epoch: 0.461684912443161\n",
      "Step: 1950, train/loss: 0.0007999999797903001\n",
      "Step: 1950, train/grad_norm: 6.58177397132309e-13\n",
      "Step: 1950, train/learning_rate: 4.226558667141944e-05\n",
      "Step: 1950, train/epoch: 0.46406471729278564\n",
      "Step: 1960, train/loss: 0.0\n",
      "Step: 1960, train/grad_norm: 9.943308112644877e-13\n",
      "Step: 1960, train/learning_rate: 4.222592542646453e-05\n",
      "Step: 1960, train/epoch: 0.46644455194473267\n",
      "Step: 1970, train/loss: 0.0\n",
      "Step: 1970, train/grad_norm: 7.119020750678828e-08\n",
      "Step: 1970, train/learning_rate: 4.218626054353081e-05\n",
      "Step: 1970, train/epoch: 0.4688243567943573\n",
      "Step: 1980, train/loss: 0.0\n",
      "Step: 1980, train/grad_norm: 0.0003421432920731604\n",
      "Step: 1980, train/learning_rate: 4.2146595660597086e-05\n",
      "Step: 1980, train/epoch: 0.4712041914463043\n",
      "Step: 1990, train/loss: 0.0\n",
      "Step: 1990, train/grad_norm: 6.005745945003582e-08\n",
      "Step: 1990, train/learning_rate: 4.210693441564217e-05\n",
      "Step: 1990, train/epoch: 0.47358399629592896\n",
      "Step: 2000, train/loss: 0.0\n",
      "Step: 2000, train/grad_norm: 1.2884308897564978e-10\n",
      "Step: 2000, train/learning_rate: 4.206726953270845e-05\n",
      "Step: 2000, train/epoch: 0.475963830947876\n",
      "Step: 2010, train/loss: 0.08590000122785568\n",
      "Step: 2010, train/grad_norm: 0.00011393507156753913\n",
      "Step: 2010, train/learning_rate: 4.202760464977473e-05\n",
      "Step: 2010, train/epoch: 0.4783436357975006\n",
      "Step: 2020, train/loss: 0.0\n",
      "Step: 2020, train/grad_norm: 1.3499903843694483e-06\n",
      "Step: 2020, train/learning_rate: 4.1987943404819816e-05\n",
      "Step: 2020, train/epoch: 0.48072347044944763\n",
      "Step: 2030, train/loss: 0.00019999999494757503\n",
      "Step: 2030, train/grad_norm: 1.389158211205499e-12\n",
      "Step: 2030, train/learning_rate: 4.1948278521886095e-05\n",
      "Step: 2030, train/epoch: 0.48310327529907227\n",
      "Step: 2040, train/loss: 0.0210999995470047\n",
      "Step: 2040, train/grad_norm: 7.05028979641753e-10\n",
      "Step: 2040, train/learning_rate: 4.1908613638952374e-05\n",
      "Step: 2040, train/epoch: 0.4854831099510193\n",
      "Step: 2050, train/loss: 9.999999747378752e-05\n",
      "Step: 2050, train/grad_norm: 2.4669077902217396e-05\n",
      "Step: 2050, train/learning_rate: 4.186895239399746e-05\n",
      "Step: 2050, train/epoch: 0.4878629148006439\n",
      "Step: 2060, train/loss: 0.0013000000035390258\n",
      "Step: 2060, train/grad_norm: 6.480795761154567e-11\n",
      "Step: 2060, train/learning_rate: 4.182928751106374e-05\n",
      "Step: 2060, train/epoch: 0.49024274945259094\n",
      "Step: 2070, train/loss: 0.009200000204145908\n",
      "Step: 2070, train/grad_norm: 1.890004302107099e-18\n",
      "Step: 2070, train/learning_rate: 4.178962262813002e-05\n",
      "Step: 2070, train/epoch: 0.4926225543022156\n",
      "Step: 2080, train/loss: 0.0\n",
      "Step: 2080, train/grad_norm: 4.550493013580748e-21\n",
      "Step: 2080, train/learning_rate: 4.1749961383175105e-05\n",
      "Step: 2080, train/epoch: 0.4950023889541626\n",
      "Step: 2090, train/loss: 0.0\n",
      "Step: 2090, train/grad_norm: 0.0\n",
      "Step: 2090, train/learning_rate: 4.1710296500241384e-05\n",
      "Step: 2090, train/epoch: 0.49738219380378723\n",
      "Step: 2100, train/loss: 0.0\n",
      "Step: 2100, train/grad_norm: 0.0\n",
      "Step: 2100, train/learning_rate: 4.167063161730766e-05\n",
      "Step: 2100, train/epoch: 0.49976202845573425\n",
      "Step: 2110, train/loss: 0.0\n",
      "Step: 2110, train/grad_norm: 3.88889411055731e-18\n",
      "Step: 2110, train/learning_rate: 4.163097037235275e-05\n",
      "Step: 2110, train/epoch: 0.5021418333053589\n",
      "Step: 2120, train/loss: 0.8008000254631042\n",
      "Step: 2120, train/grad_norm: 0.00010262933210469782\n",
      "Step: 2120, train/learning_rate: 4.159130548941903e-05\n",
      "Step: 2120, train/epoch: 0.5045216679573059\n",
      "Step: 2130, train/loss: 0.0\n",
      "Step: 2130, train/grad_norm: 7.009639721965166e-13\n",
      "Step: 2130, train/learning_rate: 4.155164060648531e-05\n",
      "Step: 2130, train/epoch: 0.5069015026092529\n",
      "Step: 2140, train/loss: 0.0017000000225380063\n",
      "Step: 2140, train/grad_norm: 34.36040496826172\n",
      "Step: 2140, train/learning_rate: 4.151197936153039e-05\n",
      "Step: 2140, train/epoch: 0.5092812776565552\n",
      "Step: 2150, train/loss: 0.0\n",
      "Step: 2150, train/grad_norm: 3.6149035154142695e-14\n",
      "Step: 2150, train/learning_rate: 4.147231447859667e-05\n",
      "Step: 2150, train/epoch: 0.5116611123085022\n",
      "Step: 2160, train/loss: 0.0\n",
      "Step: 2160, train/grad_norm: 1.937257299022832e-10\n",
      "Step: 2160, train/learning_rate: 4.143264959566295e-05\n",
      "Step: 2160, train/epoch: 0.5140409469604492\n",
      "Step: 2170, train/loss: 0.6812999844551086\n",
      "Step: 2170, train/grad_norm: 4.936901754569638e-18\n",
      "Step: 2170, train/learning_rate: 4.139298835070804e-05\n",
      "Step: 2170, train/epoch: 0.5164207816123962\n",
      "Step: 2180, train/loss: 0.0\n",
      "Step: 2180, train/grad_norm: 8.460387338621311e-13\n",
      "Step: 2180, train/learning_rate: 4.135332346777432e-05\n",
      "Step: 2180, train/epoch: 0.5188005566596985\n",
      "Step: 2190, train/loss: 0.0\n",
      "Step: 2190, train/grad_norm: 7.13236090632563e-07\n",
      "Step: 2190, train/learning_rate: 4.1313658584840596e-05\n",
      "Step: 2190, train/epoch: 0.5211803913116455\n",
      "Step: 2200, train/loss: 0.0\n",
      "Step: 2200, train/grad_norm: 3.314924723607504e-10\n",
      "Step: 2200, train/learning_rate: 4.127399733988568e-05\n",
      "Step: 2200, train/epoch: 0.5235602259635925\n",
      "Step: 2210, train/loss: 0.31459999084472656\n",
      "Step: 2210, train/grad_norm: 2.1370412384147386e-11\n",
      "Step: 2210, train/learning_rate: 4.123433245695196e-05\n",
      "Step: 2210, train/epoch: 0.5259400010108948\n",
      "Step: 2220, train/loss: 0.0\n",
      "Step: 2220, train/grad_norm: 4.04928347796929e-11\n",
      "Step: 2220, train/learning_rate: 4.119466757401824e-05\n",
      "Step: 2220, train/epoch: 0.5283198356628418\n",
      "Step: 2230, train/loss: 0.0\n",
      "Step: 2230, train/grad_norm: 7.522388744893305e-13\n",
      "Step: 2230, train/learning_rate: 4.1155006329063326e-05\n",
      "Step: 2230, train/epoch: 0.5306996703147888\n",
      "Step: 2240, train/loss: 9.999999747378752e-05\n",
      "Step: 2240, train/grad_norm: 0.0002068059693556279\n",
      "Step: 2240, train/learning_rate: 4.1115341446129605e-05\n",
      "Step: 2240, train/epoch: 0.5330795049667358\n",
      "Step: 2250, train/loss: 9.999999747378752e-05\n",
      "Step: 2250, train/grad_norm: 8.382648687601385e-14\n",
      "Step: 2250, train/learning_rate: 4.1075676563195884e-05\n",
      "Step: 2250, train/epoch: 0.5354592800140381\n",
      "Step: 2260, train/loss: 0.13359999656677246\n",
      "Step: 2260, train/grad_norm: 3.438961060364676e-10\n",
      "Step: 2260, train/learning_rate: 4.103601531824097e-05\n",
      "Step: 2260, train/epoch: 0.5378391146659851\n",
      "Step: 2270, train/loss: 0.025599999353289604\n",
      "Step: 2270, train/grad_norm: 0.0011329250410199165\n",
      "Step: 2270, train/learning_rate: 4.099635043530725e-05\n",
      "Step: 2270, train/epoch: 0.5402189493179321\n",
      "Step: 2280, train/loss: 0.15940000116825104\n",
      "Step: 2280, train/grad_norm: 4.688875378633384e-06\n",
      "Step: 2280, train/learning_rate: 4.095668555237353e-05\n",
      "Step: 2280, train/epoch: 0.5425987839698792\n",
      "Step: 2290, train/loss: 0.0\n",
      "Step: 2290, train/grad_norm: 1.0849588534256327e-06\n",
      "Step: 2290, train/learning_rate: 4.0917024307418615e-05\n",
      "Step: 2290, train/epoch: 0.5449785590171814\n",
      "Step: 2300, train/loss: 0.0\n",
      "Step: 2300, train/grad_norm: 1.2128720675908845e-14\n",
      "Step: 2300, train/learning_rate: 4.0877359424484894e-05\n",
      "Step: 2300, train/epoch: 0.5473583936691284\n",
      "Step: 2310, train/loss: 0.0\n",
      "Step: 2310, train/grad_norm: 4.3012366148797873e-17\n",
      "Step: 2310, train/learning_rate: 4.083769454155117e-05\n",
      "Step: 2310, train/epoch: 0.5497382283210754\n",
      "Step: 2320, train/loss: 0.0010000000474974513\n",
      "Step: 2320, train/grad_norm: 2.744756292042591e-18\n",
      "Step: 2320, train/learning_rate: 4.079803329659626e-05\n",
      "Step: 2320, train/epoch: 0.5521180629730225\n",
      "Step: 2330, train/loss: 0.0\n",
      "Step: 2330, train/grad_norm: 7.980738499799387e-15\n",
      "Step: 2330, train/learning_rate: 4.075836841366254e-05\n",
      "Step: 2330, train/epoch: 0.5544978380203247\n",
      "Step: 2340, train/loss: 0.002899999963119626\n",
      "Step: 2340, train/grad_norm: 1.50227838262147e-18\n",
      "Step: 2340, train/learning_rate: 4.0718707168707624e-05\n",
      "Step: 2340, train/epoch: 0.5568776726722717\n",
      "Step: 2350, train/loss: 0.0\n",
      "Step: 2350, train/grad_norm: 1.3939593916003806e-18\n",
      "Step: 2350, train/learning_rate: 4.06790422857739e-05\n",
      "Step: 2350, train/epoch: 0.5592575073242188\n",
      "Step: 2360, train/loss: 0.0\n",
      "Step: 2360, train/grad_norm: 3.815137095131149e-10\n",
      "Step: 2360, train/learning_rate: 4.063937740284018e-05\n",
      "Step: 2360, train/epoch: 0.5616373419761658\n",
      "Step: 2370, train/loss: 0.0\n",
      "Step: 2370, train/grad_norm: 2.7804384229029866e-12\n",
      "Step: 2370, train/learning_rate: 4.059971615788527e-05\n",
      "Step: 2370, train/epoch: 0.564017117023468\n",
      "Step: 2380, train/loss: 0.008700000122189522\n",
      "Step: 2380, train/grad_norm: 1.639957429837996e-13\n",
      "Step: 2380, train/learning_rate: 4.056005127495155e-05\n",
      "Step: 2380, train/epoch: 0.566396951675415\n",
      "Step: 2390, train/loss: 0.0\n",
      "Step: 2390, train/grad_norm: 1.6105803750809712e-17\n",
      "Step: 2390, train/learning_rate: 4.0520386392017826e-05\n",
      "Step: 2390, train/epoch: 0.5687767863273621\n",
      "Step: 2400, train/loss: 0.1671999990940094\n",
      "Step: 2400, train/grad_norm: 1.5367325467696702e-12\n",
      "Step: 2400, train/learning_rate: 4.048072514706291e-05\n",
      "Step: 2400, train/epoch: 0.5711566209793091\n",
      "Step: 2410, train/loss: 0.0\n",
      "Step: 2410, train/grad_norm: 3.1533572516593383e-19\n",
      "Step: 2410, train/learning_rate: 4.044106026412919e-05\n",
      "Step: 2410, train/epoch: 0.5735363960266113\n",
      "Step: 2420, train/loss: 0.00019999999494757503\n",
      "Step: 2420, train/grad_norm: 4.98148966521228e-10\n",
      "Step: 2420, train/learning_rate: 4.040139538119547e-05\n",
      "Step: 2420, train/epoch: 0.5759162306785583\n",
      "Step: 2430, train/loss: 0.0\n",
      "Step: 2430, train/grad_norm: 9.735835471147425e-10\n",
      "Step: 2430, train/learning_rate: 4.036173413624056e-05\n",
      "Step: 2430, train/epoch: 0.5782960653305054\n",
      "Step: 2440, train/loss: 0.0\n",
      "Step: 2440, train/grad_norm: 3.2994611274921037e-13\n",
      "Step: 2440, train/learning_rate: 4.0322069253306836e-05\n",
      "Step: 2440, train/epoch: 0.5806758403778076\n",
      "Step: 2450, train/loss: 0.0\n",
      "Step: 2450, train/grad_norm: 7.610236997777716e-17\n",
      "Step: 2450, train/learning_rate: 4.0282404370373115e-05\n",
      "Step: 2450, train/epoch: 0.5830556750297546\n",
      "Step: 2460, train/loss: 0.0\n",
      "Step: 2460, train/grad_norm: 1.706697838610438e-10\n",
      "Step: 2460, train/learning_rate: 4.02427431254182e-05\n",
      "Step: 2460, train/epoch: 0.5854355096817017\n",
      "Step: 2470, train/loss: 0.0\n",
      "Step: 2470, train/grad_norm: 1.4727382119835528e-20\n",
      "Step: 2470, train/learning_rate: 4.020307824248448e-05\n",
      "Step: 2470, train/epoch: 0.5878153443336487\n",
      "Step: 2480, train/loss: 0.0\n",
      "Step: 2480, train/grad_norm: 9.904084925088661e-23\n",
      "Step: 2480, train/learning_rate: 4.016341335955076e-05\n",
      "Step: 2480, train/epoch: 0.5901951193809509\n",
      "Step: 2490, train/loss: 0.0\n",
      "Step: 2490, train/grad_norm: 1.4994276758666927e-15\n",
      "Step: 2490, train/learning_rate: 4.0123752114595845e-05\n",
      "Step: 2490, train/epoch: 0.592574954032898\n",
      "Step: 2500, train/loss: 0.0\n",
      "Step: 2500, train/grad_norm: 1.8489240571952722e-19\n",
      "Step: 2500, train/learning_rate: 4.0084087231662124e-05\n",
      "Step: 2500, train/epoch: 0.594954788684845\n",
      "Step: 2510, train/loss: 0.0\n",
      "Step: 2510, train/grad_norm: 0.0\n",
      "Step: 2510, train/learning_rate: 4.0044422348728403e-05\n",
      "Step: 2510, train/epoch: 0.597334623336792\n",
      "Step: 2520, train/loss: 0.0006000000284984708\n",
      "Step: 2520, train/grad_norm: 4.713702049227283e-21\n",
      "Step: 2520, train/learning_rate: 4.000476110377349e-05\n",
      "Step: 2520, train/epoch: 0.5997143983840942\n",
      "Step: 2530, train/loss: 0.0\n",
      "Step: 2530, train/grad_norm: 3.622590236752318e-21\n",
      "Step: 2530, train/learning_rate: 3.996509622083977e-05\n",
      "Step: 2530, train/epoch: 0.6020942330360413\n",
      "Step: 2540, train/loss: 0.0\n",
      "Step: 2540, train/grad_norm: 0.0\n",
      "Step: 2540, train/learning_rate: 3.992543133790605e-05\n",
      "Step: 2540, train/epoch: 0.6044740676879883\n",
      "Step: 2550, train/loss: 0.0\n",
      "Step: 2550, train/grad_norm: 1.5380559151306917e-20\n",
      "Step: 2550, train/learning_rate: 3.9885770092951134e-05\n",
      "Step: 2550, train/epoch: 0.6068539023399353\n",
      "Step: 2560, train/loss: 0.0\n",
      "Step: 2560, train/grad_norm: 2.458260063836954e-19\n",
      "Step: 2560, train/learning_rate: 3.984610521001741e-05\n",
      "Step: 2560, train/epoch: 0.6092336773872375\n",
      "Step: 2570, train/loss: 0.1234000027179718\n",
      "Step: 2570, train/grad_norm: 1.6438181878136076e-18\n",
      "Step: 2570, train/learning_rate: 3.980644032708369e-05\n",
      "Step: 2570, train/epoch: 0.6116135120391846\n",
      "Step: 2580, train/loss: 0.0\n",
      "Step: 2580, train/grad_norm: 1.2606011247950672e-15\n",
      "Step: 2580, train/learning_rate: 3.976677908212878e-05\n",
      "Step: 2580, train/epoch: 0.6139933466911316\n",
      "Step: 2590, train/loss: 0.0\n",
      "Step: 2590, train/grad_norm: 1.022158553600427e-20\n",
      "Step: 2590, train/learning_rate: 3.972711419919506e-05\n",
      "Step: 2590, train/epoch: 0.6163731813430786\n",
      "Step: 2600, train/loss: 0.05939999967813492\n",
      "Step: 2600, train/grad_norm: 0.0\n",
      "Step: 2600, train/learning_rate: 3.9687449316261336e-05\n",
      "Step: 2600, train/epoch: 0.6187529563903809\n",
      "Step: 2610, train/loss: 0.0\n",
      "Step: 2610, train/grad_norm: 1.6236020177199035e-17\n",
      "Step: 2610, train/learning_rate: 3.964778807130642e-05\n",
      "Step: 2610, train/epoch: 0.6211327910423279\n",
      "Step: 2620, train/loss: 0.0\n",
      "Step: 2620, train/grad_norm: 0.0\n",
      "Step: 2620, train/learning_rate: 3.96081231883727e-05\n",
      "Step: 2620, train/epoch: 0.6235126256942749\n",
      "Step: 2630, train/loss: 0.002300000051036477\n",
      "Step: 2630, train/grad_norm: 1.3654079085896175e-19\n",
      "Step: 2630, train/learning_rate: 3.956845830543898e-05\n",
      "Step: 2630, train/epoch: 0.6258924603462219\n",
      "Step: 2640, train/loss: 0.0\n",
      "Step: 2640, train/grad_norm: 5.142945375347299e-20\n",
      "Step: 2640, train/learning_rate: 3.9528797060484067e-05\n",
      "Step: 2640, train/epoch: 0.6282722353935242\n",
      "Step: 2650, train/loss: 0.0\n",
      "Step: 2650, train/grad_norm: 5.293955920339377e-23\n",
      "Step: 2650, train/learning_rate: 3.9489132177550346e-05\n",
      "Step: 2650, train/epoch: 0.6306520700454712\n",
      "Step: 2660, train/loss: 0.0\n",
      "Step: 2660, train/grad_norm: 3.1441619480107293e-15\n",
      "Step: 2660, train/learning_rate: 3.9449467294616625e-05\n",
      "Step: 2660, train/epoch: 0.6330319046974182\n",
      "Step: 2670, train/loss: 0.2515999972820282\n",
      "Step: 2670, train/grad_norm: 0.0\n",
      "Step: 2670, train/learning_rate: 3.940980604966171e-05\n",
      "Step: 2670, train/epoch: 0.6354116797447205\n",
      "Step: 2680, train/loss: 0.0\n",
      "Step: 2680, train/grad_norm: 3.3532562000307406e-17\n",
      "Step: 2680, train/learning_rate: 3.937014116672799e-05\n",
      "Step: 2680, train/epoch: 0.6377915143966675\n",
      "Step: 2690, train/loss: 0.0\n",
      "Step: 2690, train/grad_norm: 6.483745598763743e-23\n",
      "Step: 2690, train/learning_rate: 3.933047628379427e-05\n",
      "Step: 2690, train/epoch: 0.6401713490486145\n",
      "Step: 2700, train/loss: 0.0\n",
      "Step: 2700, train/grad_norm: 1.0886032903321694e-19\n",
      "Step: 2700, train/learning_rate: 3.9290815038839355e-05\n",
      "Step: 2700, train/epoch: 0.6425511837005615\n",
      "Step: 2710, train/loss: 0.0\n",
      "Step: 2710, train/grad_norm: 0.0\n",
      "Step: 2710, train/learning_rate: 3.9251150155905634e-05\n",
      "Step: 2710, train/epoch: 0.6449309587478638\n",
      "Step: 2720, train/loss: 0.0\n",
      "Step: 2720, train/grad_norm: 2.729658539803485e-19\n",
      "Step: 2720, train/learning_rate: 3.921148527297191e-05\n",
      "Step: 2720, train/epoch: 0.6473107933998108\n",
      "Step: 2730, train/loss: 0.0\n",
      "Step: 2730, train/grad_norm: 6.530485109927306e-14\n",
      "Step: 2730, train/learning_rate: 3.9171824028017e-05\n",
      "Step: 2730, train/epoch: 0.6496906280517578\n",
      "Step: 2740, train/loss: 0.0\n",
      "Step: 2740, train/grad_norm: 2.2331363977821117e-18\n",
      "Step: 2740, train/learning_rate: 3.913215914508328e-05\n",
      "Step: 2740, train/epoch: 0.6520704627037048\n",
      "Step: 2750, train/loss: 0.0\n",
      "Step: 2750, train/grad_norm: 3.1493599013491758e-18\n",
      "Step: 2750, train/learning_rate: 3.909249426214956e-05\n",
      "Step: 2750, train/epoch: 0.6544502377510071\n",
      "Step: 2760, train/loss: 0.0\n",
      "Step: 2760, train/grad_norm: 0.0\n",
      "Step: 2760, train/learning_rate: 3.9052833017194644e-05\n",
      "Step: 2760, train/epoch: 0.6568300724029541\n",
      "Step: 2770, train/loss: 0.0\n",
      "Step: 2770, train/grad_norm: 0.0\n",
      "Step: 2770, train/learning_rate: 3.901316813426092e-05\n",
      "Step: 2770, train/epoch: 0.6592099070549011\n",
      "Step: 2780, train/loss: 0.0026000000070780516\n",
      "Step: 2780, train/grad_norm: 0.0\n",
      "Step: 2780, train/learning_rate: 3.89735032513272e-05\n",
      "Step: 2780, train/epoch: 0.6615897417068481\n",
      "Step: 2790, train/loss: 0.0\n",
      "Step: 2790, train/grad_norm: 3.400984430902171e-20\n",
      "Step: 2790, train/learning_rate: 3.893384200637229e-05\n",
      "Step: 2790, train/epoch: 0.6639695167541504\n",
      "Step: 2800, train/loss: 0.0\n",
      "Step: 2800, train/grad_norm: 1.6274623174374558e-15\n",
      "Step: 2800, train/learning_rate: 3.889417712343857e-05\n",
      "Step: 2800, train/epoch: 0.6663493514060974\n",
      "Step: 2810, train/loss: 0.0\n",
      "Step: 2810, train/grad_norm: 5.564833533214242e-13\n",
      "Step: 2810, train/learning_rate: 3.8854512240504846e-05\n",
      "Step: 2810, train/epoch: 0.6687291860580444\n",
      "Step: 2820, train/loss: 0.0\n",
      "Step: 2820, train/grad_norm: 2.74140030397519e-20\n",
      "Step: 2820, train/learning_rate: 3.881485099554993e-05\n",
      "Step: 2820, train/epoch: 0.6711090207099915\n",
      "Step: 2830, train/loss: 0.22130000591278076\n",
      "Step: 2830, train/grad_norm: 0.0\n",
      "Step: 2830, train/learning_rate: 3.877518611261621e-05\n",
      "Step: 2830, train/epoch: 0.6734887957572937\n",
      "Step: 2840, train/loss: 0.0\n",
      "Step: 2840, train/grad_norm: 1.3226975336747535e-16\n",
      "Step: 2840, train/learning_rate: 3.873552122968249e-05\n",
      "Step: 2840, train/epoch: 0.6758686304092407\n",
      "Step: 2850, train/loss: 0.5562999844551086\n",
      "Step: 2850, train/grad_norm: 1.356992971266794e-16\n",
      "Step: 2850, train/learning_rate: 3.8695859984727576e-05\n",
      "Step: 2850, train/epoch: 0.6782484650611877\n",
      "Step: 2860, train/loss: 0.0\n",
      "Step: 2860, train/grad_norm: 5.39316769021525e-09\n",
      "Step: 2860, train/learning_rate: 3.8656195101793855e-05\n",
      "Step: 2860, train/epoch: 0.6806282997131348\n",
      "Step: 2870, train/loss: 0.08399999886751175\n",
      "Step: 2870, train/grad_norm: 0.00030527563649229705\n",
      "Step: 2870, train/learning_rate: 3.8616530218860134e-05\n",
      "Step: 2870, train/epoch: 0.683008074760437\n",
      "Step: 2880, train/loss: 0.4440999925136566\n",
      "Step: 2880, train/grad_norm: 5.346047909915441e-11\n",
      "Step: 2880, train/learning_rate: 3.857686897390522e-05\n",
      "Step: 2880, train/epoch: 0.685387909412384\n",
      "Step: 2890, train/loss: 0.0\n",
      "Step: 2890, train/grad_norm: 2.1906775005131607e-11\n",
      "Step: 2890, train/learning_rate: 3.85372040909715e-05\n",
      "Step: 2890, train/epoch: 0.687767744064331\n",
      "Step: 2900, train/loss: 0.05079999938607216\n",
      "Step: 2900, train/grad_norm: 2.7492955134711394e-10\n",
      "Step: 2900, train/learning_rate: 3.849753920803778e-05\n",
      "Step: 2900, train/epoch: 0.6901475191116333\n",
      "Step: 2910, train/loss: 0.0\n",
      "Step: 2910, train/grad_norm: 1.151594227963293e-12\n",
      "Step: 2910, train/learning_rate: 3.8457877963082865e-05\n",
      "Step: 2910, train/epoch: 0.6925273537635803\n",
      "Step: 2920, train/loss: 0.20229999721050262\n",
      "Step: 2920, train/grad_norm: 3.0462229960726006e-11\n",
      "Step: 2920, train/learning_rate: 3.8418213080149144e-05\n",
      "Step: 2920, train/epoch: 0.6949071884155273\n",
      "Step: 2930, train/loss: 0.0\n",
      "Step: 2930, train/grad_norm: 3.719465291318613e-10\n",
      "Step: 2930, train/learning_rate: 3.837854819721542e-05\n",
      "Step: 2930, train/epoch: 0.6972870230674744\n",
      "Step: 2940, train/loss: 0.0\n",
      "Step: 2940, train/grad_norm: 6.303182975564425e-11\n",
      "Step: 2940, train/learning_rate: 3.833888695226051e-05\n",
      "Step: 2940, train/epoch: 0.6996667981147766\n",
      "Step: 2950, train/loss: 0.0\n",
      "Step: 2950, train/grad_norm: 2.3979731977874508e-08\n",
      "Step: 2950, train/learning_rate: 3.829922206932679e-05\n",
      "Step: 2950, train/epoch: 0.7020466327667236\n",
      "Step: 2960, train/loss: 0.529699981212616\n",
      "Step: 2960, train/grad_norm: 2.1298879326892006e-11\n",
      "Step: 2960, train/learning_rate: 3.825955718639307e-05\n",
      "Step: 2960, train/epoch: 0.7044264674186707\n",
      "Step: 2970, train/loss: 0.00039999998989515007\n",
      "Step: 2970, train/grad_norm: 0.009656603448092937\n",
      "Step: 2970, train/learning_rate: 3.821989594143815e-05\n",
      "Step: 2970, train/epoch: 0.7068063020706177\n",
      "Step: 2980, train/loss: 9.999999747378752e-05\n",
      "Step: 2980, train/grad_norm: 0.008048453368246555\n",
      "Step: 2980, train/learning_rate: 3.818023105850443e-05\n",
      "Step: 2980, train/epoch: 0.7091860771179199\n",
      "Step: 2990, train/loss: 0.0\n",
      "Step: 2990, train/grad_norm: 3.971511887357337e-06\n",
      "Step: 2990, train/learning_rate: 3.814056617557071e-05\n",
      "Step: 2990, train/epoch: 0.7115659117698669\n",
      "Step: 3000, train/loss: 9.999999747378752e-05\n",
      "Step: 3000, train/grad_norm: 4.588124738802435e-06\n",
      "Step: 3000, train/learning_rate: 3.81009049306158e-05\n",
      "Step: 3000, train/epoch: 0.713945746421814\n",
      "Step: 3010, train/loss: 0.0\n",
      "Step: 3010, train/grad_norm: 0.0015666419640183449\n",
      "Step: 3010, train/learning_rate: 3.806124004768208e-05\n",
      "Step: 3010, train/epoch: 0.716325581073761\n",
      "Step: 3020, train/loss: 0.0\n",
      "Step: 3020, train/grad_norm: 2.288998643962259e-07\n",
      "Step: 3020, train/learning_rate: 3.802157880272716e-05\n",
      "Step: 3020, train/epoch: 0.7187053561210632\n",
      "Step: 3030, train/loss: 0.0\n",
      "Step: 3030, train/grad_norm: 0.012879861518740654\n",
      "Step: 3030, train/learning_rate: 3.798191391979344e-05\n",
      "Step: 3030, train/epoch: 0.7210851907730103\n",
      "Step: 3040, train/loss: 0.0\n",
      "Step: 3040, train/grad_norm: 6.680140329784479e-10\n",
      "Step: 3040, train/learning_rate: 3.794224903685972e-05\n",
      "Step: 3040, train/epoch: 0.7234650254249573\n",
      "Step: 3050, train/loss: 0.0\n",
      "Step: 3050, train/grad_norm: 1.6069138553120865e-07\n",
      "Step: 3050, train/learning_rate: 3.790258779190481e-05\n",
      "Step: 3050, train/epoch: 0.7258448600769043\n",
      "Step: 3060, train/loss: 0.15549999475479126\n",
      "Step: 3060, train/grad_norm: 9.452224730921444e-06\n",
      "Step: 3060, train/learning_rate: 3.7862922908971086e-05\n",
      "Step: 3060, train/epoch: 0.7282246351242065\n",
      "Step: 3070, train/loss: 9.999999747378752e-05\n",
      "Step: 3070, train/grad_norm: 0.010042889043688774\n",
      "Step: 3070, train/learning_rate: 3.7823258026037365e-05\n",
      "Step: 3070, train/epoch: 0.7306044697761536\n",
      "Step: 3080, train/loss: 0.0\n",
      "Step: 3080, train/grad_norm: 0.00021075828408356756\n",
      "Step: 3080, train/learning_rate: 3.778359678108245e-05\n",
      "Step: 3080, train/epoch: 0.7329843044281006\n",
      "Step: 3090, train/loss: 0.01600000075995922\n",
      "Step: 3090, train/grad_norm: 1.7422431497493562e-08\n",
      "Step: 3090, train/learning_rate: 3.774393189814873e-05\n",
      "Step: 3090, train/epoch: 0.7353641390800476\n",
      "Step: 3100, train/loss: 0.0\n",
      "Step: 3100, train/grad_norm: 1.1468890073751403e-11\n",
      "Step: 3100, train/learning_rate: 3.770426701521501e-05\n",
      "Step: 3100, train/epoch: 0.7377439141273499\n",
      "Step: 3110, train/loss: 0.0\n",
      "Step: 3110, train/grad_norm: 7.377390155206571e-16\n",
      "Step: 3110, train/learning_rate: 3.7664605770260096e-05\n",
      "Step: 3110, train/epoch: 0.7401237487792969\n",
      "Step: 3120, train/loss: 0.0\n",
      "Step: 3120, train/grad_norm: 1.0437181785505345e-08\n",
      "Step: 3120, train/learning_rate: 3.7624940887326375e-05\n",
      "Step: 3120, train/epoch: 0.7425035834312439\n",
      "Step: 3130, train/loss: 0.0\n",
      "Step: 3130, train/grad_norm: 2.5360047395395213e-09\n",
      "Step: 3130, train/learning_rate: 3.7585276004392654e-05\n",
      "Step: 3130, train/epoch: 0.7448834180831909\n",
      "Step: 3140, train/loss: 0.0\n",
      "Step: 3140, train/grad_norm: 8.264501062671403e-12\n",
      "Step: 3140, train/learning_rate: 3.754561475943774e-05\n",
      "Step: 3140, train/epoch: 0.7472631931304932\n",
      "Step: 3150, train/loss: 0.42149999737739563\n",
      "Step: 3150, train/grad_norm: 9.375592316396286e-12\n",
      "Step: 3150, train/learning_rate: 3.750594987650402e-05\n",
      "Step: 3150, train/epoch: 0.7496430277824402\n",
      "Step: 3160, train/loss: 0.12110000103712082\n",
      "Step: 3160, train/grad_norm: 8.005424660950666e-07\n",
      "Step: 3160, train/learning_rate: 3.74662849935703e-05\n",
      "Step: 3160, train/epoch: 0.7520228624343872\n",
      "Step: 3170, train/loss: 0.0\n",
      "Step: 3170, train/grad_norm: 2.4103956677845595e-11\n",
      "Step: 3170, train/learning_rate: 3.7426623748615384e-05\n",
      "Step: 3170, train/epoch: 0.7544026374816895\n",
      "Step: 3180, train/loss: 0.0\n",
      "Step: 3180, train/grad_norm: 1.617319966840114e-08\n",
      "Step: 3180, train/learning_rate: 3.738695886568166e-05\n",
      "Step: 3180, train/epoch: 0.7567824721336365\n",
      "Step: 3190, train/loss: 2.293800115585327\n",
      "Step: 3190, train/grad_norm: 2.651347807969273e-09\n",
      "Step: 3190, train/learning_rate: 3.734729398274794e-05\n",
      "Step: 3190, train/epoch: 0.7591623067855835\n",
      "Step: 3200, train/loss: 0.4269999861717224\n",
      "Step: 3200, train/grad_norm: 39020.6953125\n",
      "Step: 3200, train/learning_rate: 3.730763273779303e-05\n",
      "Step: 3200, train/epoch: 0.7615421414375305\n",
      "Step: 3210, train/loss: 0.0\n",
      "Step: 3210, train/grad_norm: 4.424333383212797e-06\n",
      "Step: 3210, train/learning_rate: 3.726796785485931e-05\n",
      "Step: 3210, train/epoch: 0.7639219164848328\n",
      "Step: 3220, train/loss: 0.3093999922275543\n",
      "Step: 3220, train/grad_norm: 6.434268229327245e-09\n",
      "Step: 3220, train/learning_rate: 3.7228302971925586e-05\n",
      "Step: 3220, train/epoch: 0.7663017511367798\n",
      "Step: 3230, train/loss: 0.0\n",
      "Step: 3230, train/grad_norm: 3.792591201090545e-07\n",
      "Step: 3230, train/learning_rate: 3.718864172697067e-05\n",
      "Step: 3230, train/epoch: 0.7686815857887268\n",
      "Step: 3240, train/loss: 0.03009999915957451\n",
      "Step: 3240, train/grad_norm: 5.159790089237504e-05\n",
      "Step: 3240, train/learning_rate: 3.714897684403695e-05\n",
      "Step: 3240, train/epoch: 0.7710614204406738\n",
      "Step: 3250, train/loss: 0.032999999821186066\n",
      "Step: 3250, train/grad_norm: 1.0221014235867187e-05\n",
      "Step: 3250, train/learning_rate: 3.710931196110323e-05\n",
      "Step: 3250, train/epoch: 0.7734411954879761\n",
      "Step: 3260, train/loss: 0.04010000079870224\n",
      "Step: 3260, train/grad_norm: 0.00568530336022377\n",
      "Step: 3260, train/learning_rate: 3.706965071614832e-05\n",
      "Step: 3260, train/epoch: 0.7758210301399231\n",
      "Step: 3270, train/loss: 0.21879999339580536\n",
      "Step: 3270, train/grad_norm: 3.285085767856799e-05\n",
      "Step: 3270, train/learning_rate: 3.7029985833214596e-05\n",
      "Step: 3270, train/epoch: 0.7782008647918701\n",
      "Step: 3280, train/loss: 0.0\n",
      "Step: 3280, train/grad_norm: 0.0010751362424343824\n",
      "Step: 3280, train/learning_rate: 3.6990320950280875e-05\n",
      "Step: 3280, train/epoch: 0.7805806994438171\n",
      "Step: 3290, train/loss: 0.0\n",
      "Step: 3290, train/grad_norm: 3.5624198062578216e-05\n",
      "Step: 3290, train/learning_rate: 3.695065970532596e-05\n",
      "Step: 3290, train/epoch: 0.7829604744911194\n",
      "Step: 3300, train/loss: 0.0\n",
      "Step: 3300, train/grad_norm: 8.004428309504874e-06\n",
      "Step: 3300, train/learning_rate: 3.691099482239224e-05\n",
      "Step: 3300, train/epoch: 0.7853403091430664\n",
      "Step: 3310, train/loss: 0.0\n",
      "Step: 3310, train/grad_norm: 2.1138653327756884e-10\n",
      "Step: 3310, train/learning_rate: 3.687132993945852e-05\n",
      "Step: 3310, train/epoch: 0.7877201437950134\n",
      "Step: 3320, train/loss: 0.0\n",
      "Step: 3320, train/grad_norm: 3.996141458628699e-05\n",
      "Step: 3320, train/learning_rate: 3.6831668694503605e-05\n",
      "Step: 3320, train/epoch: 0.7900999784469604\n",
      "Step: 3330, train/loss: 0.0\n",
      "Step: 3330, train/grad_norm: 0.00261782668530941\n",
      "Step: 3330, train/learning_rate: 3.6792003811569884e-05\n",
      "Step: 3330, train/epoch: 0.7924797534942627\n",
      "Step: 3340, train/loss: 0.0\n",
      "Step: 3340, train/grad_norm: 0.00034103417419828475\n",
      "Step: 3340, train/learning_rate: 3.6752338928636163e-05\n",
      "Step: 3340, train/epoch: 0.7948595881462097\n",
      "Step: 3350, train/loss: 0.0\n",
      "Step: 3350, train/grad_norm: 5.633263572235592e-05\n",
      "Step: 3350, train/learning_rate: 3.671267768368125e-05\n",
      "Step: 3350, train/epoch: 0.7972394227981567\n",
      "Step: 3360, train/loss: 0.0\n",
      "Step: 3360, train/grad_norm: 1.168680773844244e-05\n",
      "Step: 3360, train/learning_rate: 3.667301280074753e-05\n",
      "Step: 3360, train/epoch: 0.7996192574501038\n",
      "Step: 3370, train/loss: 0.0\n",
      "Step: 3370, train/grad_norm: 1.3070969544060063e-05\n",
      "Step: 3370, train/learning_rate: 3.663334791781381e-05\n",
      "Step: 3370, train/epoch: 0.801999032497406\n",
      "Step: 3380, train/loss: 0.0\n",
      "Step: 3380, train/grad_norm: 1.6768784917076118e-05\n",
      "Step: 3380, train/learning_rate: 3.6593686672858894e-05\n",
      "Step: 3380, train/epoch: 0.804378867149353\n",
      "Step: 3390, train/loss: 0.0\n",
      "Step: 3390, train/grad_norm: 2.5690569600556046e-05\n",
      "Step: 3390, train/learning_rate: 3.655402178992517e-05\n",
      "Step: 3390, train/epoch: 0.8067587018013\n",
      "Step: 3400, train/loss: 0.0\n",
      "Step: 3400, train/grad_norm: 0.00031202170066535473\n",
      "Step: 3400, train/learning_rate: 3.651435690699145e-05\n",
      "Step: 3400, train/epoch: 0.8091384768486023\n",
      "Step: 3410, train/loss: 0.0\n",
      "Step: 3410, train/grad_norm: 1.7713650777295697e-06\n",
      "Step: 3410, train/learning_rate: 3.647469566203654e-05\n",
      "Step: 3410, train/epoch: 0.8115183115005493\n",
      "Step: 3420, train/loss: 0.0\n",
      "Step: 3420, train/grad_norm: 2.3670887117077655e-07\n",
      "Step: 3420, train/learning_rate: 3.643503077910282e-05\n",
      "Step: 3420, train/epoch: 0.8138981461524963\n",
      "Step: 3430, train/loss: 0.0\n",
      "Step: 3430, train/grad_norm: 5.148843229108024e-06\n",
      "Step: 3430, train/learning_rate: 3.6395365896169096e-05\n",
      "Step: 3430, train/epoch: 0.8162779808044434\n",
      "Step: 3440, train/loss: 0.18479999899864197\n",
      "Step: 3440, train/grad_norm: 7.344429468503222e-05\n",
      "Step: 3440, train/learning_rate: 3.635570465121418e-05\n",
      "Step: 3440, train/epoch: 0.8186577558517456\n",
      "Step: 3450, train/loss: 0.0\n",
      "Step: 3450, train/grad_norm: 0.00034649300505407155\n",
      "Step: 3450, train/learning_rate: 3.631603976828046e-05\n",
      "Step: 3450, train/epoch: 0.8210375905036926\n",
      "Step: 3460, train/loss: 9.999999747378752e-05\n",
      "Step: 3460, train/grad_norm: 0.00012622418580576777\n",
      "Step: 3460, train/learning_rate: 3.627637488534674e-05\n",
      "Step: 3460, train/epoch: 0.8234174251556396\n",
      "Step: 3470, train/loss: 0.0\n",
      "Step: 3470, train/grad_norm: 0.048726506531238556\n",
      "Step: 3470, train/learning_rate: 3.623671364039183e-05\n",
      "Step: 3470, train/epoch: 0.8257972598075867\n",
      "Step: 3480, train/loss: 0.36559998989105225\n",
      "Step: 3480, train/grad_norm: 1.9301540305605158e-05\n",
      "Step: 3480, train/learning_rate: 3.6197048757458106e-05\n",
      "Step: 3480, train/epoch: 0.8281770348548889\n",
      "Step: 3490, train/loss: 0.0\n",
      "Step: 3490, train/grad_norm: 0.0015195623273029923\n",
      "Step: 3490, train/learning_rate: 3.6157383874524385e-05\n",
      "Step: 3490, train/epoch: 0.8305568695068359\n",
      "Step: 3500, train/loss: 0.0003000000142492354\n",
      "Step: 3500, train/grad_norm: 3.451445991231594e-07\n",
      "Step: 3500, train/learning_rate: 3.611772262956947e-05\n",
      "Step: 3500, train/epoch: 0.832936704158783\n",
      "Step: 3510, train/loss: 0.0\n",
      "Step: 3510, train/grad_norm: 4.142273155594012e-06\n",
      "Step: 3510, train/learning_rate: 3.607805774663575e-05\n",
      "Step: 3510, train/epoch: 0.83531653881073\n",
      "Step: 3520, train/loss: 0.0\n",
      "Step: 3520, train/grad_norm: 1.0316747648175806e-05\n",
      "Step: 3520, train/learning_rate: 3.603839286370203e-05\n",
      "Step: 3520, train/epoch: 0.8376963138580322\n",
      "Step: 3530, train/loss: 0.0\n",
      "Step: 3530, train/grad_norm: 2.072530878649559e-05\n",
      "Step: 3530, train/learning_rate: 3.5998731618747115e-05\n",
      "Step: 3530, train/epoch: 0.8400761485099792\n",
      "Step: 3540, train/loss: 9.999999747378752e-05\n",
      "Step: 3540, train/grad_norm: 2.4832013878040016e-05\n",
      "Step: 3540, train/learning_rate: 3.5959066735813394e-05\n",
      "Step: 3540, train/epoch: 0.8424559831619263\n",
      "Step: 3550, train/loss: 0.0\n",
      "Step: 3550, train/grad_norm: 5.541634777728177e-07\n",
      "Step: 3550, train/learning_rate: 3.591940185287967e-05\n",
      "Step: 3550, train/epoch: 0.8448358178138733\n",
      "Step: 3560, train/loss: 0.0\n",
      "Step: 3560, train/grad_norm: 2.035056058957707e-05\n",
      "Step: 3560, train/learning_rate: 3.587974060792476e-05\n",
      "Step: 3560, train/epoch: 0.8472155928611755\n",
      "Step: 3570, train/loss: 0.0\n",
      "Step: 3570, train/grad_norm: 4.4986558123127907e-07\n",
      "Step: 3570, train/learning_rate: 3.584007572499104e-05\n",
      "Step: 3570, train/epoch: 0.8495954275131226\n",
      "Step: 3580, train/loss: 0.0\n",
      "Step: 3580, train/grad_norm: 0.00033934172824956477\n",
      "Step: 3580, train/learning_rate: 3.580041084205732e-05\n",
      "Step: 3580, train/epoch: 0.8519752621650696\n",
      "Step: 3590, train/loss: 0.0\n",
      "Step: 3590, train/grad_norm: 4.376797591021386e-08\n",
      "Step: 3590, train/learning_rate: 3.5760749597102404e-05\n",
      "Step: 3590, train/epoch: 0.8543550968170166\n",
      "Step: 3600, train/loss: 0.0\n",
      "Step: 3600, train/grad_norm: 4.2461834937057574e-07\n",
      "Step: 3600, train/learning_rate: 3.572108471416868e-05\n",
      "Step: 3600, train/epoch: 0.8567348718643188\n",
      "Step: 3610, train/loss: 0.0\n",
      "Step: 3610, train/grad_norm: 3.0071078072069213e-07\n",
      "Step: 3610, train/learning_rate: 3.568141983123496e-05\n",
      "Step: 3610, train/epoch: 0.8591147065162659\n",
      "Step: 3620, train/loss: 0.0\n",
      "Step: 3620, train/grad_norm: 4.4438071199692786e-05\n",
      "Step: 3620, train/learning_rate: 3.564175858628005e-05\n",
      "Step: 3620, train/epoch: 0.8614945411682129\n",
      "Step: 3630, train/loss: 0.0\n",
      "Step: 3630, train/grad_norm: 0.0001278221607208252\n",
      "Step: 3630, train/learning_rate: 3.560209370334633e-05\n",
      "Step: 3630, train/epoch: 0.8638743162155151\n",
      "Step: 3640, train/loss: 0.0\n",
      "Step: 3640, train/grad_norm: 5.421488253887219e-07\n",
      "Step: 3640, train/learning_rate: 3.5562428820412606e-05\n",
      "Step: 3640, train/epoch: 0.8662541508674622\n",
      "Step: 3650, train/loss: 0.0\n",
      "Step: 3650, train/grad_norm: 2.5264305804739706e-05\n",
      "Step: 3650, train/learning_rate: 3.552276757545769e-05\n",
      "Step: 3650, train/epoch: 0.8686339855194092\n",
      "Step: 3660, train/loss: 0.0\n",
      "Step: 3660, train/grad_norm: 9.30105525185354e-05\n",
      "Step: 3660, train/learning_rate: 3.548310269252397e-05\n",
      "Step: 3660, train/epoch: 0.8710138201713562\n",
      "Step: 3670, train/loss: 0.10729999840259552\n",
      "Step: 3670, train/grad_norm: 0.0001616941881366074\n",
      "Step: 3670, train/learning_rate: 3.544344144756906e-05\n",
      "Step: 3670, train/epoch: 0.8733935952186584\n",
      "Step: 3680, train/loss: 0.02019999921321869\n",
      "Step: 3680, train/grad_norm: 0.0005882481927983463\n",
      "Step: 3680, train/learning_rate: 3.5403776564635336e-05\n",
      "Step: 3680, train/epoch: 0.8757734298706055\n",
      "Step: 3690, train/loss: 0.0\n",
      "Step: 3690, train/grad_norm: 3.623439326361222e-08\n",
      "Step: 3690, train/learning_rate: 3.5364111681701615e-05\n",
      "Step: 3690, train/epoch: 0.8781532645225525\n",
      "Step: 3700, train/loss: 0.007199999876320362\n",
      "Step: 3700, train/grad_norm: 0.0022612372413277626\n",
      "Step: 3700, train/learning_rate: 3.53244504367467e-05\n",
      "Step: 3700, train/epoch: 0.8805330991744995\n",
      "Step: 3710, train/loss: 0.15860000252723694\n",
      "Step: 3710, train/grad_norm: 2.9209499263771477e-09\n",
      "Step: 3710, train/learning_rate: 3.528478555381298e-05\n",
      "Step: 3710, train/epoch: 0.8829128742218018\n",
      "Step: 3720, train/loss: 0.0\n",
      "Step: 3720, train/grad_norm: 3.29061691672905e-07\n",
      "Step: 3720, train/learning_rate: 3.524512067087926e-05\n",
      "Step: 3720, train/epoch: 0.8852927088737488\n",
      "Step: 3730, train/loss: 0.0\n",
      "Step: 3730, train/grad_norm: 1.6054525531217223e-07\n",
      "Step: 3730, train/learning_rate: 3.5205459425924346e-05\n",
      "Step: 3730, train/epoch: 0.8876725435256958\n",
      "Step: 3740, train/loss: 0.023000000044703484\n",
      "Step: 3740, train/grad_norm: 2.0092134178639753e-08\n",
      "Step: 3740, train/learning_rate: 3.5165794542990625e-05\n",
      "Step: 3740, train/epoch: 0.8900523781776428\n",
      "Step: 3750, train/loss: 0.0\n",
      "Step: 3750, train/grad_norm: 2.3700385654024103e-08\n",
      "Step: 3750, train/learning_rate: 3.5126129660056904e-05\n",
      "Step: 3750, train/epoch: 0.8924321532249451\n",
      "Step: 3760, train/loss: 0.0\n",
      "Step: 3760, train/grad_norm: 2.6608979908360197e-08\n",
      "Step: 3760, train/learning_rate: 3.508646841510199e-05\n",
      "Step: 3760, train/epoch: 0.8948119878768921\n",
      "Step: 3770, train/loss: 9.999999747378752e-05\n",
      "Step: 3770, train/grad_norm: 1.7189084600133242e-09\n",
      "Step: 3770, train/learning_rate: 3.504680353216827e-05\n",
      "Step: 3770, train/epoch: 0.8971918225288391\n",
      "Step: 3780, train/loss: 0.0\n",
      "Step: 3780, train/grad_norm: 2.6175845935227926e-09\n",
      "Step: 3780, train/learning_rate: 3.500713864923455e-05\n",
      "Step: 3780, train/epoch: 0.8995716571807861\n",
      "Step: 3790, train/loss: 0.0\n",
      "Step: 3790, train/grad_norm: 7.017460224023608e-11\n",
      "Step: 3790, train/learning_rate: 3.4967477404279634e-05\n",
      "Step: 3790, train/epoch: 0.9019514322280884\n",
      "Step: 3800, train/loss: 0.0\n",
      "Step: 3800, train/grad_norm: 7.403833796049142e-11\n",
      "Step: 3800, train/learning_rate: 3.4927812521345913e-05\n",
      "Step: 3800, train/epoch: 0.9043312668800354\n",
      "Step: 3810, train/loss: 0.009100000374019146\n",
      "Step: 3810, train/grad_norm: 1.905120949885486e-09\n",
      "Step: 3810, train/learning_rate: 3.488814763841219e-05\n",
      "Step: 3810, train/epoch: 0.9067111015319824\n",
      "Step: 3820, train/loss: 0.21879999339580536\n",
      "Step: 3820, train/grad_norm: 5.33097697930085e-12\n",
      "Step: 3820, train/learning_rate: 3.484848639345728e-05\n",
      "Step: 3820, train/epoch: 0.9090909361839294\n",
      "Step: 3830, train/loss: 0.0\n",
      "Step: 3830, train/grad_norm: 6.507536731703567e-10\n",
      "Step: 3830, train/learning_rate: 3.480882151052356e-05\n",
      "Step: 3830, train/epoch: 0.9114707112312317\n",
      "Step: 3840, train/loss: 0.0010999999940395355\n",
      "Step: 3840, train/grad_norm: 1.2098843399144243e-05\n",
      "Step: 3840, train/learning_rate: 3.476915662758984e-05\n",
      "Step: 3840, train/epoch: 0.9138505458831787\n",
      "Step: 3850, train/loss: 0.009600000455975533\n",
      "Step: 3850, train/grad_norm: 9.97678071144037e-05\n",
      "Step: 3850, train/learning_rate: 3.472949538263492e-05\n",
      "Step: 3850, train/epoch: 0.9162303805351257\n",
      "Step: 3860, train/loss: 0.00930000003427267\n",
      "Step: 3860, train/grad_norm: 4.654207714338554e-06\n",
      "Step: 3860, train/learning_rate: 3.46898304997012e-05\n",
      "Step: 3860, train/epoch: 0.9186102151870728\n",
      "Step: 3870, train/loss: 0.004699999932199717\n",
      "Step: 3870, train/grad_norm: 1.1393556237360158e-10\n",
      "Step: 3870, train/learning_rate: 3.465016561676748e-05\n",
      "Step: 3870, train/epoch: 0.920989990234375\n",
      "Step: 3880, train/loss: 0.0\n",
      "Step: 3880, train/grad_norm: 3.444595009227669e-08\n",
      "Step: 3880, train/learning_rate: 3.461050437181257e-05\n",
      "Step: 3880, train/epoch: 0.923369824886322\n",
      "Step: 3890, train/loss: 0.21559999883174896\n",
      "Step: 3890, train/grad_norm: 5.756211618468399e-10\n",
      "Step: 3890, train/learning_rate: 3.4570839488878846e-05\n",
      "Step: 3890, train/epoch: 0.925749659538269\n",
      "Step: 3900, train/loss: 0.0\n",
      "Step: 3900, train/grad_norm: 3.3361067242054787e-09\n",
      "Step: 3900, train/learning_rate: 3.4531174605945125e-05\n",
      "Step: 3900, train/epoch: 0.9281294345855713\n",
      "Step: 3910, train/loss: 0.0\n",
      "Step: 3910, train/grad_norm: 9.407437573827337e-06\n",
      "Step: 3910, train/learning_rate: 3.449151336099021e-05\n",
      "Step: 3910, train/epoch: 0.9305092692375183\n",
      "Step: 3920, train/loss: 0.0\n",
      "Step: 3920, train/grad_norm: 8.007816177268978e-06\n",
      "Step: 3920, train/learning_rate: 3.445184847805649e-05\n",
      "Step: 3920, train/epoch: 0.9328891038894653\n",
      "Step: 3930, train/loss: 0.0\n",
      "Step: 3930, train/grad_norm: 0.006796570029109716\n",
      "Step: 3930, train/learning_rate: 3.441218359512277e-05\n",
      "Step: 3930, train/epoch: 0.9352689385414124\n",
      "Step: 3940, train/loss: 0.0\n",
      "Step: 3940, train/grad_norm: 9.894490915485221e-08\n",
      "Step: 3940, train/learning_rate: 3.4372522350167856e-05\n",
      "Step: 3940, train/epoch: 0.9376487135887146\n",
      "Step: 3950, train/loss: 0.0003000000142492354\n",
      "Step: 3950, train/grad_norm: 0.0014665836934000254\n",
      "Step: 3950, train/learning_rate: 3.4332857467234135e-05\n",
      "Step: 3950, train/epoch: 0.9400285482406616\n",
      "Step: 3960, train/loss: 0.0\n",
      "Step: 3960, train/grad_norm: 6.4731247029214956e-09\n",
      "Step: 3960, train/learning_rate: 3.4293192584300414e-05\n",
      "Step: 3960, train/epoch: 0.9424083828926086\n",
      "Step: 3970, train/loss: 0.0\n",
      "Step: 3970, train/grad_norm: 0.0004749726504087448\n",
      "Step: 3970, train/learning_rate: 3.42535313393455e-05\n",
      "Step: 3970, train/epoch: 0.9447882175445557\n",
      "Step: 3980, train/loss: 0.0\n",
      "Step: 3980, train/grad_norm: 1.6738400665516906e-09\n",
      "Step: 3980, train/learning_rate: 3.421386645641178e-05\n",
      "Step: 3980, train/epoch: 0.9471679925918579\n",
      "Step: 3990, train/loss: 0.0\n",
      "Step: 3990, train/grad_norm: 3.35412010665781e-11\n",
      "Step: 3990, train/learning_rate: 3.417420157347806e-05\n",
      "Step: 3990, train/epoch: 0.9495478272438049\n",
      "Step: 4000, train/loss: 0.0\n",
      "Step: 4000, train/grad_norm: 4.215165994025938e-10\n",
      "Step: 4000, train/learning_rate: 3.4134540328523144e-05\n",
      "Step: 4000, train/epoch: 0.951927661895752\n",
      "Step: 4010, train/loss: 0.0\n",
      "Step: 4010, train/grad_norm: 1.3667320608590217e-09\n",
      "Step: 4010, train/learning_rate: 3.409487544558942e-05\n",
      "Step: 4010, train/epoch: 0.954307496547699\n",
      "Step: 4020, train/loss: 0.1460999995470047\n",
      "Step: 4020, train/grad_norm: 0.0002072437055176124\n",
      "Step: 4020, train/learning_rate: 3.40552105626557e-05\n",
      "Step: 4020, train/epoch: 0.9566872715950012\n",
      "Step: 4030, train/loss: 0.0\n",
      "Step: 4030, train/grad_norm: 2.503908191897608e-09\n",
      "Step: 4030, train/learning_rate: 3.401554931770079e-05\n",
      "Step: 4030, train/epoch: 0.9590671062469482\n",
      "Step: 4040, train/loss: 9.999999747378752e-05\n",
      "Step: 4040, train/grad_norm: 1.3174698324291967e-05\n",
      "Step: 4040, train/learning_rate: 3.397588443476707e-05\n",
      "Step: 4040, train/epoch: 0.9614469408988953\n",
      "Step: 4050, train/loss: 0.0\n",
      "Step: 4050, train/grad_norm: 7.61060459097962e-09\n",
      "Step: 4050, train/learning_rate: 3.3936219551833346e-05\n",
      "Step: 4050, train/epoch: 0.9638267755508423\n",
      "Step: 4060, train/loss: 0.0\n",
      "Step: 4060, train/grad_norm: 5.297167504636491e-08\n",
      "Step: 4060, train/learning_rate: 3.389655830687843e-05\n",
      "Step: 4060, train/epoch: 0.9662065505981445\n",
      "Step: 4070, train/loss: 0.0\n",
      "Step: 4070, train/grad_norm: 1.0041036148322746e-05\n",
      "Step: 4070, train/learning_rate: 3.385689342394471e-05\n",
      "Step: 4070, train/epoch: 0.9685863852500916\n",
      "Step: 4080, train/loss: 0.0\n",
      "Step: 4080, train/grad_norm: 8.913286081435601e-11\n",
      "Step: 4080, train/learning_rate: 3.381722854101099e-05\n",
      "Step: 4080, train/epoch: 0.9709662199020386\n",
      "Step: 4090, train/loss: 0.0\n",
      "Step: 4090, train/grad_norm: 7.241523491963164e-10\n",
      "Step: 4090, train/learning_rate: 3.377756729605608e-05\n",
      "Step: 4090, train/epoch: 0.9733460545539856\n",
      "Step: 4100, train/loss: 0.0\n",
      "Step: 4100, train/grad_norm: 9.388697597145779e-10\n",
      "Step: 4100, train/learning_rate: 3.3737902413122356e-05\n",
      "Step: 4100, train/epoch: 0.9757258296012878\n",
      "Step: 4110, train/loss: 0.0\n",
      "Step: 4110, train/grad_norm: 7.994186423943006e-10\n",
      "Step: 4110, train/learning_rate: 3.3698237530188635e-05\n",
      "Step: 4110, train/epoch: 0.9781056642532349\n",
      "Step: 4120, train/loss: 0.13600000739097595\n",
      "Step: 4120, train/grad_norm: 5.026940197438989e-09\n",
      "Step: 4120, train/learning_rate: 3.365857628523372e-05\n",
      "Step: 4120, train/epoch: 0.9804854989051819\n",
      "Step: 4130, train/loss: 0.0\n",
      "Step: 4130, train/grad_norm: 9.959474027709803e-07\n",
      "Step: 4130, train/learning_rate: 3.36189114023e-05\n",
      "Step: 4130, train/epoch: 0.9828652739524841\n",
      "Step: 4140, train/loss: 0.13910000026226044\n",
      "Step: 4140, train/grad_norm: 4.38845987327241e-10\n",
      "Step: 4140, train/learning_rate: 3.357924651936628e-05\n",
      "Step: 4140, train/epoch: 0.9852451086044312\n",
      "Step: 4150, train/loss: 0.09809999912977219\n",
      "Step: 4150, train/grad_norm: 6.63751270622015e-05\n",
      "Step: 4150, train/learning_rate: 3.3539585274411365e-05\n",
      "Step: 4150, train/epoch: 0.9876249432563782\n",
      "Step: 4160, train/loss: 0.13130000233650208\n",
      "Step: 4160, train/grad_norm: 1.059717757989631e-09\n",
      "Step: 4160, train/learning_rate: 3.3499920391477644e-05\n",
      "Step: 4160, train/epoch: 0.9900047779083252\n",
      "Step: 4170, train/loss: 0.30550000071525574\n",
      "Step: 4170, train/grad_norm: 0.0012815860100090504\n",
      "Step: 4170, train/learning_rate: 3.3460255508543923e-05\n",
      "Step: 4170, train/epoch: 0.9923845529556274\n",
      "Step: 4180, train/loss: 0.7602999806404114\n",
      "Step: 4180, train/grad_norm: 2.0258219990409998e-08\n",
      "Step: 4180, train/learning_rate: 3.342059426358901e-05\n",
      "Step: 4180, train/epoch: 0.9947643876075745\n",
      "Step: 4190, train/loss: 0.28040000796318054\n",
      "Step: 4190, train/grad_norm: 0.04654674604535103\n",
      "Step: 4190, train/learning_rate: 3.338092938065529e-05\n",
      "Step: 4190, train/epoch: 0.9971442222595215\n",
      "Step: 4200, train/loss: 0.15119999647140503\n",
      "Step: 4200, train/grad_norm: 1.1547553539276123\n",
      "Step: 4200, train/learning_rate: 3.334126449772157e-05\n",
      "Step: 4200, train/epoch: 0.9995240569114685\n",
      "Step: 4202, eval/loss: 0.09178511053323746\n",
      "Step: 4202, eval/accuracy: 0.981674313545227\n",
      "Step: 4202, eval/f1: 0.9807193875312805\n",
      "Step: 4202, eval/runtime: 7990.0068359375\n",
      "Step: 4202, eval/samples_per_second: 0.9020000100135803\n",
      "Step: 4202, eval/steps_per_second: 0.11299999803304672\n",
      "Step: 4202, train/epoch: 1.0\n",
      "Step: 4210, train/loss: 0.6916999816894531\n",
      "Step: 4210, train/grad_norm: 0.00013312208466231823\n",
      "Step: 4210, train/learning_rate: 3.3301603252766654e-05\n",
      "Step: 4210, train/epoch: 1.0019038915634155\n",
      "Step: 4220, train/loss: 0.0017000000225380063\n",
      "Step: 4220, train/grad_norm: 1.0717847544583492e-05\n",
      "Step: 4220, train/learning_rate: 3.326193836983293e-05\n",
      "Step: 4220, train/epoch: 1.0042836666107178\n",
      "Step: 4230, train/loss: 0.0\n",
      "Step: 4230, train/grad_norm: 0.0060021900571882725\n",
      "Step: 4230, train/learning_rate: 3.322227348689921e-05\n",
      "Step: 4230, train/epoch: 1.00666344165802\n",
      "Step: 4240, train/loss: 0.0003000000142492354\n",
      "Step: 4240, train/grad_norm: 3.7011297990829917e-06\n",
      "Step: 4240, train/learning_rate: 3.31826122419443e-05\n",
      "Step: 4240, train/epoch: 1.0090433359146118\n",
      "Step: 4250, train/loss: 0.0\n",
      "Step: 4250, train/grad_norm: 7.907885901659029e-07\n",
      "Step: 4250, train/learning_rate: 3.314294735901058e-05\n",
      "Step: 4250, train/epoch: 1.011423110961914\n",
      "Step: 4260, train/loss: 0.0\n",
      "Step: 4260, train/grad_norm: 6.714225619930403e-09\n",
      "Step: 4260, train/learning_rate: 3.3103282476076856e-05\n",
      "Step: 4260, train/epoch: 1.0138030052185059\n",
      "Step: 4270, train/loss: 0.20630000531673431\n",
      "Step: 4270, train/grad_norm: 3.5274399579066085e-06\n",
      "Step: 4270, train/learning_rate: 3.306362123112194e-05\n",
      "Step: 4270, train/epoch: 1.016182780265808\n",
      "Step: 4280, train/loss: 0.07270000129938126\n",
      "Step: 4280, train/grad_norm: 1.091521426133113e-06\n",
      "Step: 4280, train/learning_rate: 3.302395634818822e-05\n",
      "Step: 4280, train/epoch: 1.0185625553131104\n",
      "Step: 4290, train/loss: 0.0\n",
      "Step: 4290, train/grad_norm: 3.8593892526250784e-08\n",
      "Step: 4290, train/learning_rate: 3.29842914652545e-05\n",
      "Step: 4290, train/epoch: 1.0209424495697021\n",
      "Step: 4300, train/loss: 0.0\n",
      "Step: 4300, train/grad_norm: 2.0301257563914987e-07\n",
      "Step: 4300, train/learning_rate: 3.294463022029959e-05\n",
      "Step: 4300, train/epoch: 1.0233222246170044\n",
      "Step: 4310, train/loss: 0.0\n",
      "Step: 4310, train/grad_norm: 5.0301291594223585e-06\n",
      "Step: 4310, train/learning_rate: 3.2904965337365866e-05\n",
      "Step: 4310, train/epoch: 1.0257019996643066\n",
      "Step: 4320, train/loss: 0.07580000162124634\n",
      "Step: 4320, train/grad_norm: 3.235333451812039e-06\n",
      "Step: 4320, train/learning_rate: 3.2865300454432145e-05\n",
      "Step: 4320, train/epoch: 1.0280818939208984\n",
      "Step: 4330, train/loss: 0.0\n",
      "Step: 4330, train/grad_norm: 1.4709483366459608e-05\n",
      "Step: 4330, train/learning_rate: 3.282563920947723e-05\n",
      "Step: 4330, train/epoch: 1.0304616689682007\n",
      "Step: 4340, train/loss: 0.28780001401901245\n",
      "Step: 4340, train/grad_norm: 0.36763954162597656\n",
      "Step: 4340, train/learning_rate: 3.278597432654351e-05\n",
      "Step: 4340, train/epoch: 1.0328415632247925\n",
      "Step: 4350, train/loss: 0.11800000071525574\n",
      "Step: 4350, train/grad_norm: 5.2062308242284416e-08\n",
      "Step: 4350, train/learning_rate: 3.2746313081588596e-05\n",
      "Step: 4350, train/epoch: 1.0352213382720947\n",
      "Step: 4360, train/loss: 0.0\n",
      "Step: 4360, train/grad_norm: 8.911271152101108e-07\n",
      "Step: 4360, train/learning_rate: 3.2706648198654875e-05\n",
      "Step: 4360, train/epoch: 1.037601113319397\n",
      "Step: 4370, train/loss: 0.0\n",
      "Step: 4370, train/grad_norm: 0.012074485421180725\n",
      "Step: 4370, train/learning_rate: 3.2666983315721154e-05\n",
      "Step: 4370, train/epoch: 1.0399810075759888\n",
      "Step: 4380, train/loss: 0.0\n",
      "Step: 4380, train/grad_norm: 6.665602995781228e-05\n",
      "Step: 4380, train/learning_rate: 3.262732207076624e-05\n",
      "Step: 4380, train/epoch: 1.042360782623291\n",
      "Step: 4390, train/loss: 0.0\n",
      "Step: 4390, train/grad_norm: 3.451589145697653e-05\n",
      "Step: 4390, train/learning_rate: 3.258765718783252e-05\n",
      "Step: 4390, train/epoch: 1.0447405576705933\n",
      "Step: 4400, train/loss: 0.0\n",
      "Step: 4400, train/grad_norm: 0.000791738333646208\n",
      "Step: 4400, train/learning_rate: 3.25479923048988e-05\n",
      "Step: 4400, train/epoch: 1.047120451927185\n",
      "Step: 4410, train/loss: 0.0\n",
      "Step: 4410, train/grad_norm: 4.258257831679657e-06\n",
      "Step: 4410, train/learning_rate: 3.2508331059943885e-05\n",
      "Step: 4410, train/epoch: 1.0495002269744873\n",
      "Step: 4420, train/loss: 0.0\n",
      "Step: 4420, train/grad_norm: 1.7192417089972878e-07\n",
      "Step: 4420, train/learning_rate: 3.2468666177010164e-05\n",
      "Step: 4420, train/epoch: 1.0518800020217896\n",
      "Step: 4430, train/loss: 0.0\n",
      "Step: 4430, train/grad_norm: 6.680774049527827e-07\n",
      "Step: 4430, train/learning_rate: 3.242900129407644e-05\n",
      "Step: 4430, train/epoch: 1.0542598962783813\n",
      "Step: 4440, train/loss: 0.0\n",
      "Step: 4440, train/grad_norm: 5.751868684455985e-06\n",
      "Step: 4440, train/learning_rate: 3.238934004912153e-05\n",
      "Step: 4440, train/epoch: 1.0566396713256836\n",
      "Step: 4450, train/loss: 0.010700000450015068\n",
      "Step: 4450, train/grad_norm: 1.7507804273009242e-07\n",
      "Step: 4450, train/learning_rate: 3.234967516618781e-05\n",
      "Step: 4450, train/epoch: 1.0590195655822754\n",
      "Step: 4460, train/loss: 0.00019999999494757503\n",
      "Step: 4460, train/grad_norm: 0.00032582253334112465\n",
      "Step: 4460, train/learning_rate: 3.231001028325409e-05\n",
      "Step: 4460, train/epoch: 1.0613993406295776\n",
      "Step: 4470, train/loss: 9.999999747378752e-05\n",
      "Step: 4470, train/grad_norm: 0.00013802447938360274\n",
      "Step: 4470, train/learning_rate: 3.227034903829917e-05\n",
      "Step: 4470, train/epoch: 1.0637791156768799\n",
      "Step: 4480, train/loss: 0.0\n",
      "Step: 4480, train/grad_norm: 8.485993019391458e-10\n",
      "Step: 4480, train/learning_rate: 3.223068415536545e-05\n",
      "Step: 4480, train/epoch: 1.0661590099334717\n",
      "Step: 4490, train/loss: 0.0\n",
      "Step: 4490, train/grad_norm: 0.0006961869657970965\n",
      "Step: 4490, train/learning_rate: 3.219101927243173e-05\n",
      "Step: 4490, train/epoch: 1.068538784980774\n",
      "Step: 4500, train/loss: 0.00039999998989515007\n",
      "Step: 4500, train/grad_norm: 7.049311534501612e-08\n",
      "Step: 4500, train/learning_rate: 3.215135802747682e-05\n",
      "Step: 4500, train/epoch: 1.0709185600280762\n",
      "Step: 4510, train/loss: 0.0\n",
      "Step: 4510, train/grad_norm: 5.7300426625772616e-09\n",
      "Step: 4510, train/learning_rate: 3.2111693144543096e-05\n",
      "Step: 4510, train/epoch: 1.073298454284668\n",
      "Step: 4520, train/loss: 0.0\n",
      "Step: 4520, train/grad_norm: 2.1298194496921496e-06\n",
      "Step: 4520, train/learning_rate: 3.2072028261609375e-05\n",
      "Step: 4520, train/epoch: 1.0756782293319702\n",
      "Step: 4530, train/loss: 0.19840000569820404\n",
      "Step: 4530, train/grad_norm: 2.0225174868215845e-09\n",
      "Step: 4530, train/learning_rate: 3.203236701665446e-05\n",
      "Step: 4530, train/epoch: 1.078058123588562\n",
      "Step: 4540, train/loss: 0.0\n",
      "Step: 4540, train/grad_norm: 8.494424719174276e-09\n",
      "Step: 4540, train/learning_rate: 3.199270213372074e-05\n",
      "Step: 4540, train/epoch: 1.0804378986358643\n",
      "Step: 4550, train/loss: 0.0\n",
      "Step: 4550, train/grad_norm: 9.193688477182604e-09\n",
      "Step: 4550, train/learning_rate: 3.195303725078702e-05\n",
      "Step: 4550, train/epoch: 1.0828176736831665\n",
      "Step: 4560, train/loss: 0.0\n",
      "Step: 4560, train/grad_norm: 1.92317628489036e-09\n",
      "Step: 4560, train/learning_rate: 3.1913376005832106e-05\n",
      "Step: 4560, train/epoch: 1.0851975679397583\n",
      "Step: 4570, train/loss: 0.0\n",
      "Step: 4570, train/grad_norm: 3.688764493858798e-08\n",
      "Step: 4570, train/learning_rate: 3.1873711122898385e-05\n",
      "Step: 4570, train/epoch: 1.0875773429870605\n",
      "Step: 4580, train/loss: 0.0\n",
      "Step: 4580, train/grad_norm: 1.1165108304567184e-07\n",
      "Step: 4580, train/learning_rate: 3.1834046239964664e-05\n",
      "Step: 4580, train/epoch: 1.0899571180343628\n",
      "Step: 4590, train/loss: 0.0\n",
      "Step: 4590, train/grad_norm: 5.24319361261405e-08\n",
      "Step: 4590, train/learning_rate: 3.179438499500975e-05\n",
      "Step: 4590, train/epoch: 1.0923370122909546\n",
      "Step: 4600, train/loss: 0.0\n",
      "Step: 4600, train/grad_norm: 1.1548609180067615e-08\n",
      "Step: 4600, train/learning_rate: 3.175472011207603e-05\n",
      "Step: 4600, train/epoch: 1.0947167873382568\n",
      "Step: 4610, train/loss: 0.0\n",
      "Step: 4610, train/grad_norm: 6.153587084334333e-10\n",
      "Step: 4610, train/learning_rate: 3.171505522914231e-05\n",
      "Step: 4610, train/epoch: 1.097096562385559\n",
      "Step: 4620, train/loss: 9.999999747378752e-05\n",
      "Step: 4620, train/grad_norm: 6.889815384880649e-10\n",
      "Step: 4620, train/learning_rate: 3.1675393984187394e-05\n",
      "Step: 4620, train/epoch: 1.0994764566421509\n",
      "Step: 4630, train/loss: 0.0\n",
      "Step: 4630, train/grad_norm: 7.545494229432848e-10\n",
      "Step: 4630, train/learning_rate: 3.1635729101253673e-05\n",
      "Step: 4630, train/epoch: 1.1018562316894531\n",
      "Step: 4640, train/loss: 0.0\n",
      "Step: 4640, train/grad_norm: 9.013175761296566e-10\n",
      "Step: 4640, train/learning_rate: 3.159606421831995e-05\n",
      "Step: 4640, train/epoch: 1.104236125946045\n",
      "Step: 4650, train/loss: 9.999999747378752e-05\n",
      "Step: 4650, train/grad_norm: 2.732052140075325e-09\n",
      "Step: 4650, train/learning_rate: 3.155640297336504e-05\n",
      "Step: 4650, train/epoch: 1.1066159009933472\n",
      "Step: 4660, train/loss: 0.0\n",
      "Step: 4660, train/grad_norm: 2.860353065514687e-09\n",
      "Step: 4660, train/learning_rate: 3.151673809043132e-05\n",
      "Step: 4660, train/epoch: 1.1089956760406494\n",
      "Step: 4670, train/loss: 0.0\n",
      "Step: 4670, train/grad_norm: 2.1424129137415093e-09\n",
      "Step: 4670, train/learning_rate: 3.14770732074976e-05\n",
      "Step: 4670, train/epoch: 1.1113755702972412\n",
      "Step: 4680, train/loss: 0.0\n",
      "Step: 4680, train/grad_norm: 1.400143556651301e-09\n",
      "Step: 4680, train/learning_rate: 3.143741196254268e-05\n",
      "Step: 4680, train/epoch: 1.1137553453445435\n",
      "Step: 4690, train/loss: 0.0\n",
      "Step: 4690, train/grad_norm: 7.245398023769667e-07\n",
      "Step: 4690, train/learning_rate: 3.139774707960896e-05\n",
      "Step: 4690, train/epoch: 1.1161351203918457\n",
      "Step: 4700, train/loss: 0.05820000171661377\n",
      "Step: 4700, train/grad_norm: 3.209754018129729e-09\n",
      "Step: 4700, train/learning_rate: 3.135808219667524e-05\n",
      "Step: 4700, train/epoch: 1.1185150146484375\n",
      "Step: 4710, train/loss: 0.0\n",
      "Step: 4710, train/grad_norm: 5.506830991208744e-09\n",
      "Step: 4710, train/learning_rate: 3.131842095172033e-05\n",
      "Step: 4710, train/epoch: 1.1208947896957397\n",
      "Step: 4720, train/loss: 0.0\n",
      "Step: 4720, train/grad_norm: 1.1855280313000094e-08\n",
      "Step: 4720, train/learning_rate: 3.1278756068786606e-05\n",
      "Step: 4720, train/epoch: 1.1232746839523315\n",
      "Step: 4730, train/loss: 0.0\n",
      "Step: 4730, train/grad_norm: 1.1662704224590925e-07\n",
      "Step: 4730, train/learning_rate: 3.1239091185852885e-05\n",
      "Step: 4730, train/epoch: 1.1256544589996338\n",
      "Step: 4740, train/loss: 0.0\n",
      "Step: 4740, train/grad_norm: 1.7133351093434612e-06\n",
      "Step: 4740, train/learning_rate: 3.119942994089797e-05\n",
      "Step: 4740, train/epoch: 1.128034234046936\n",
      "Step: 4750, train/loss: 0.0\n",
      "Step: 4750, train/grad_norm: 1.0306334452536703e-08\n",
      "Step: 4750, train/learning_rate: 3.115976505796425e-05\n",
      "Step: 4750, train/epoch: 1.1304141283035278\n",
      "Step: 4760, train/loss: 0.0\n",
      "Step: 4760, train/grad_norm: 5.6758083476216026e-12\n",
      "Step: 4760, train/learning_rate: 3.112010017503053e-05\n",
      "Step: 4760, train/epoch: 1.13279390335083\n",
      "Step: 4770, train/loss: 0.0\n",
      "Step: 4770, train/grad_norm: 3.0856843746818186e-09\n",
      "Step: 4770, train/learning_rate: 3.1080438930075616e-05\n",
      "Step: 4770, train/epoch: 1.1351736783981323\n",
      "Step: 4780, train/loss: 0.0\n",
      "Step: 4780, train/grad_norm: 1.2786044223389581e-08\n",
      "Step: 4780, train/learning_rate: 3.1040774047141895e-05\n",
      "Step: 4780, train/epoch: 1.1375535726547241\n",
      "Step: 4790, train/loss: 0.0\n",
      "Step: 4790, train/grad_norm: 3.90594401267208e-09\n",
      "Step: 4790, train/learning_rate: 3.1001109164208174e-05\n",
      "Step: 4790, train/epoch: 1.1399333477020264\n",
      "Step: 4800, train/loss: 0.0\n",
      "Step: 4800, train/grad_norm: 2.2919786069053316e-09\n",
      "Step: 4800, train/learning_rate: 3.096144791925326e-05\n",
      "Step: 4800, train/epoch: 1.1423132419586182\n",
      "Step: 4810, train/loss: 0.0\n",
      "Step: 4810, train/grad_norm: 3.754863797666985e-08\n",
      "Step: 4810, train/learning_rate: 3.092178303631954e-05\n",
      "Step: 4810, train/epoch: 1.1446930170059204\n",
      "Step: 4820, train/loss: 0.0\n",
      "Step: 4820, train/grad_norm: 1.2792521708604454e-08\n",
      "Step: 4820, train/learning_rate: 3.088211815338582e-05\n",
      "Step: 4820, train/epoch: 1.1470727920532227\n",
      "Step: 4830, train/loss: 0.0\n",
      "Step: 4830, train/grad_norm: 4.903812467205171e-09\n",
      "Step: 4830, train/learning_rate: 3.0842456908430904e-05\n",
      "Step: 4830, train/epoch: 1.1494526863098145\n",
      "Step: 4840, train/loss: 0.0\n",
      "Step: 4840, train/grad_norm: 2.1393699256577747e-08\n",
      "Step: 4840, train/learning_rate: 3.080279202549718e-05\n",
      "Step: 4840, train/epoch: 1.1518324613571167\n",
      "Step: 4850, train/loss: 0.0\n",
      "Step: 4850, train/grad_norm: 1.1976351022013887e-09\n",
      "Step: 4850, train/learning_rate: 3.076312714256346e-05\n",
      "Step: 4850, train/epoch: 1.154212236404419\n",
      "Step: 4860, train/loss: 0.0\n",
      "Step: 4860, train/grad_norm: 8.477627488900907e-09\n",
      "Step: 4860, train/learning_rate: 3.072346589760855e-05\n",
      "Step: 4860, train/epoch: 1.1565921306610107\n",
      "Step: 4870, train/loss: 0.0\n",
      "Step: 4870, train/grad_norm: 4.5150969185669965e-08\n",
      "Step: 4870, train/learning_rate: 3.068380101467483e-05\n",
      "Step: 4870, train/epoch: 1.158971905708313\n",
      "Step: 4880, train/loss: 0.0\n",
      "Step: 4880, train/grad_norm: 8.798671302656658e-08\n",
      "Step: 4880, train/learning_rate: 3.0644136131741107e-05\n",
      "Step: 4880, train/epoch: 1.1613516807556152\n",
      "Step: 4890, train/loss: 0.025200000032782555\n",
      "Step: 4890, train/grad_norm: 1.6285317983033565e-08\n",
      "Step: 4890, train/learning_rate: 3.060447488678619e-05\n",
      "Step: 4890, train/epoch: 1.163731575012207\n",
      "Step: 4900, train/loss: 0.0\n",
      "Step: 4900, train/grad_norm: 8.79338657444606e-10\n",
      "Step: 4900, train/learning_rate: 3.056481000385247e-05\n",
      "Step: 4900, train/epoch: 1.1661113500595093\n",
      "Step: 4910, train/loss: 0.0\n",
      "Step: 4910, train/grad_norm: 3.920536784107753e-08\n",
      "Step: 4910, train/learning_rate: 3.052514512091875e-05\n",
      "Step: 4910, train/epoch: 1.168491244316101\n",
      "Step: 4920, train/loss: 0.00039999998989515007\n",
      "Step: 4920, train/grad_norm: 7.831664206037203e-09\n",
      "Step: 4920, train/learning_rate: 3.0485483875963837e-05\n",
      "Step: 4920, train/epoch: 1.1708710193634033\n",
      "Step: 4930, train/loss: 0.0\n",
      "Step: 4930, train/grad_norm: 6.877955843753725e-11\n",
      "Step: 4930, train/learning_rate: 3.0445818993030116e-05\n",
      "Step: 4930, train/epoch: 1.1732507944107056\n",
      "Step: 4940, train/loss: 0.0\n",
      "Step: 4940, train/grad_norm: 6.506161720487569e-10\n",
      "Step: 4940, train/learning_rate: 3.04061559290858e-05\n",
      "Step: 4940, train/epoch: 1.1756306886672974\n",
      "Step: 4950, train/loss: 0.0\n",
      "Step: 4950, train/grad_norm: 1.3491546768662488e-09\n",
      "Step: 4950, train/learning_rate: 3.036649286514148e-05\n",
      "Step: 4950, train/epoch: 1.1780104637145996\n",
      "Step: 4960, train/loss: 0.0\n",
      "Step: 4960, train/grad_norm: 1.4418968241614039e-09\n",
      "Step: 4960, train/learning_rate: 3.032682798220776e-05\n",
      "Step: 4960, train/epoch: 1.1803902387619019\n",
      "Step: 4970, train/loss: 0.0\n",
      "Step: 4970, train/grad_norm: 2.782708641291265e-08\n",
      "Step: 4970, train/learning_rate: 3.0287164918263443e-05\n",
      "Step: 4970, train/epoch: 1.1827701330184937\n",
      "Step: 4980, train/loss: 0.0\n",
      "Step: 4980, train/grad_norm: 1.7492994552892327e-10\n",
      "Step: 4980, train/learning_rate: 3.0247501854319125e-05\n",
      "Step: 4980, train/epoch: 1.185149908065796\n",
      "Step: 4990, train/loss: 0.0\n",
      "Step: 4990, train/grad_norm: 4.8378510086877213e-08\n",
      "Step: 4990, train/learning_rate: 3.0207836971385404e-05\n",
      "Step: 4990, train/epoch: 1.1875298023223877\n",
      "Step: 5000, train/loss: 0.0\n",
      "Step: 5000, train/grad_norm: 7.573822458084578e-09\n",
      "Step: 5000, train/learning_rate: 3.0168173907441087e-05\n",
      "Step: 5000, train/epoch: 1.18990957736969\n",
      "Step: 5010, train/loss: 0.0\n",
      "Step: 5010, train/grad_norm: 0.0001921606162795797\n",
      "Step: 5010, train/learning_rate: 3.012851084349677e-05\n",
      "Step: 5010, train/epoch: 1.1922893524169922\n",
      "Step: 5020, train/loss: 0.0\n",
      "Step: 5020, train/grad_norm: 1.3338291582343231e-10\n",
      "Step: 5020, train/learning_rate: 3.008884596056305e-05\n",
      "Step: 5020, train/epoch: 1.194669246673584\n",
      "Step: 5030, train/loss: 0.0\n",
      "Step: 5030, train/grad_norm: 7.710866667398619e-11\n",
      "Step: 5030, train/learning_rate: 3.004918289661873e-05\n",
      "Step: 5030, train/epoch: 1.1970490217208862\n",
      "Step: 5040, train/loss: 0.0\n",
      "Step: 5040, train/grad_norm: 2.2745576255811528e-10\n",
      "Step: 5040, train/learning_rate: 3.0009519832674414e-05\n",
      "Step: 5040, train/epoch: 1.1994287967681885\n",
      "Step: 5050, train/loss: 0.0\n",
      "Step: 5050, train/grad_norm: 3.8025542714592575e-09\n",
      "Step: 5050, train/learning_rate: 2.9969854949740693e-05\n",
      "Step: 5050, train/epoch: 1.2018086910247803\n",
      "Step: 5060, train/loss: 0.0\n",
      "Step: 5060, train/grad_norm: 1.4390791891472077e-09\n",
      "Step: 5060, train/learning_rate: 2.9930191885796376e-05\n",
      "Step: 5060, train/epoch: 1.2041884660720825\n",
      "Step: 5070, train/loss: 0.0\n",
      "Step: 5070, train/grad_norm: 3.52901791123017e-10\n",
      "Step: 5070, train/learning_rate: 2.9890528821852058e-05\n",
      "Step: 5070, train/epoch: 1.2065683603286743\n",
      "Step: 5080, train/loss: 0.0\n",
      "Step: 5080, train/grad_norm: 8.187809541482238e-09\n",
      "Step: 5080, train/learning_rate: 2.9850863938918337e-05\n",
      "Step: 5080, train/epoch: 1.2089481353759766\n",
      "Step: 5090, train/loss: 0.0\n",
      "Step: 5090, train/grad_norm: 4.407106644066516e-06\n",
      "Step: 5090, train/learning_rate: 2.981120087497402e-05\n",
      "Step: 5090, train/epoch: 1.2113279104232788\n",
      "Step: 5100, train/loss: 0.0\n",
      "Step: 5100, train/grad_norm: 7.205943575172569e-08\n",
      "Step: 5100, train/learning_rate: 2.9771537811029702e-05\n",
      "Step: 5100, train/epoch: 1.2137078046798706\n",
      "Step: 5110, train/loss: 0.0\n",
      "Step: 5110, train/grad_norm: 2.818679734062357e-10\n",
      "Step: 5110, train/learning_rate: 2.973187292809598e-05\n",
      "Step: 5110, train/epoch: 1.2160875797271729\n",
      "Step: 5120, train/loss: 0.0\n",
      "Step: 5120, train/grad_norm: 8.743140100797575e-10\n",
      "Step: 5120, train/learning_rate: 2.9692209864151664e-05\n",
      "Step: 5120, train/epoch: 1.218467354774475\n",
      "Step: 5130, train/loss: 0.0\n",
      "Step: 5130, train/grad_norm: 4.512345608276291e-09\n",
      "Step: 5130, train/learning_rate: 2.9652546800207347e-05\n",
      "Step: 5130, train/epoch: 1.220847249031067\n",
      "Step: 5140, train/loss: 0.0\n",
      "Step: 5140, train/grad_norm: 1.4441859930158785e-09\n",
      "Step: 5140, train/learning_rate: 2.9612881917273626e-05\n",
      "Step: 5140, train/epoch: 1.2232270240783691\n",
      "Step: 5150, train/loss: 0.0\n",
      "Step: 5150, train/grad_norm: 3.038399256638513e-07\n",
      "Step: 5150, train/learning_rate: 2.957321885332931e-05\n",
      "Step: 5150, train/epoch: 1.2256067991256714\n",
      "Step: 5160, train/loss: 9.999999747378752e-05\n",
      "Step: 5160, train/grad_norm: 1.0943774242377913e-07\n",
      "Step: 5160, train/learning_rate: 2.953355578938499e-05\n",
      "Step: 5160, train/epoch: 1.2279866933822632\n",
      "Step: 5170, train/loss: 0.0\n",
      "Step: 5170, train/grad_norm: 3.034463791351527e-10\n",
      "Step: 5170, train/learning_rate: 2.949389090645127e-05\n",
      "Step: 5170, train/epoch: 1.2303664684295654\n",
      "Step: 5180, train/loss: 0.0\n",
      "Step: 5180, train/grad_norm: 9.482094970314492e-11\n",
      "Step: 5180, train/learning_rate: 2.9454227842506953e-05\n",
      "Step: 5180, train/epoch: 1.2327463626861572\n",
      "Step: 5190, train/loss: 0.0\n",
      "Step: 5190, train/grad_norm: 1.6673588065785339e-09\n",
      "Step: 5190, train/learning_rate: 2.9414564778562635e-05\n",
      "Step: 5190, train/epoch: 1.2351261377334595\n",
      "Step: 5200, train/loss: 0.0\n",
      "Step: 5200, train/grad_norm: 5.387816599977668e-06\n",
      "Step: 5200, train/learning_rate: 2.9374901714618318e-05\n",
      "Step: 5200, train/epoch: 1.2375059127807617\n",
      "Step: 5210, train/loss: 0.0\n",
      "Step: 5210, train/grad_norm: 2.9584887872857735e-09\n",
      "Step: 5210, train/learning_rate: 2.9335236831684597e-05\n",
      "Step: 5210, train/epoch: 1.2398858070373535\n",
      "Step: 5220, train/loss: 0.0\n",
      "Step: 5220, train/grad_norm: 7.691957487621082e-11\n",
      "Step: 5220, train/learning_rate: 2.929557376774028e-05\n",
      "Step: 5220, train/epoch: 1.2422655820846558\n",
      "Step: 5230, train/loss: 0.0\n",
      "Step: 5230, train/grad_norm: 1.6653688428291957e-09\n",
      "Step: 5230, train/learning_rate: 2.9255910703795962e-05\n",
      "Step: 5230, train/epoch: 1.244645357131958\n",
      "Step: 5240, train/loss: 0.01730000041425228\n",
      "Step: 5240, train/grad_norm: 39.244319915771484\n",
      "Step: 5240, train/learning_rate: 2.921624582086224e-05\n",
      "Step: 5240, train/epoch: 1.2470252513885498\n",
      "Step: 5250, train/loss: 0.0\n",
      "Step: 5250, train/grad_norm: 3.9665584949943877e-07\n",
      "Step: 5250, train/learning_rate: 2.9176582756917924e-05\n",
      "Step: 5250, train/epoch: 1.249405026435852\n",
      "Step: 5260, train/loss: 0.0\n",
      "Step: 5260, train/grad_norm: 0.00017782856593839824\n",
      "Step: 5260, train/learning_rate: 2.9136919692973606e-05\n",
      "Step: 5260, train/epoch: 1.2517849206924438\n",
      "Step: 5270, train/loss: 0.025800000876188278\n",
      "Step: 5270, train/grad_norm: 1.1264112799835857e-05\n",
      "Step: 5270, train/learning_rate: 2.9097254810039885e-05\n",
      "Step: 5270, train/epoch: 1.254164695739746\n",
      "Step: 5280, train/loss: 0.005200000014156103\n",
      "Step: 5280, train/grad_norm: 1.558488804676017e-08\n",
      "Step: 5280, train/learning_rate: 2.9057591746095568e-05\n",
      "Step: 5280, train/epoch: 1.2565444707870483\n",
      "Step: 5290, train/loss: 0.0\n",
      "Step: 5290, train/grad_norm: 1.3925500834920113e-13\n",
      "Step: 5290, train/learning_rate: 2.901792868215125e-05\n",
      "Step: 5290, train/epoch: 1.2589243650436401\n",
      "Step: 5300, train/loss: 0.0\n",
      "Step: 5300, train/grad_norm: 2.8012173743791058e-12\n",
      "Step: 5300, train/learning_rate: 2.897826379921753e-05\n",
      "Step: 5300, train/epoch: 1.2613041400909424\n",
      "Step: 5310, train/loss: 0.0\n",
      "Step: 5310, train/grad_norm: 7.09449182343877e-14\n",
      "Step: 5310, train/learning_rate: 2.8938600735273212e-05\n",
      "Step: 5310, train/epoch: 1.2636839151382446\n",
      "Step: 5320, train/loss: 0.0\n",
      "Step: 5320, train/grad_norm: 5.33865750185214e-06\n",
      "Step: 5320, train/learning_rate: 2.8898937671328895e-05\n",
      "Step: 5320, train/epoch: 1.2660638093948364\n",
      "Step: 5330, train/loss: 0.0\n",
      "Step: 5330, train/grad_norm: 2.318834143796855e-13\n",
      "Step: 5330, train/learning_rate: 2.8859272788395174e-05\n",
      "Step: 5330, train/epoch: 1.2684435844421387\n",
      "Step: 5340, train/loss: 0.0\n",
      "Step: 5340, train/grad_norm: 2.2967738821932926e-13\n",
      "Step: 5340, train/learning_rate: 2.8819609724450856e-05\n",
      "Step: 5340, train/epoch: 1.270823359489441\n",
      "Step: 5350, train/loss: 0.0\n",
      "Step: 5350, train/grad_norm: 1.6074221489449647e-13\n",
      "Step: 5350, train/learning_rate: 2.877994666050654e-05\n",
      "Step: 5350, train/epoch: 1.2732032537460327\n",
      "Step: 5360, train/loss: 0.0\n",
      "Step: 5360, train/grad_norm: 1.1930928467407398e-11\n",
      "Step: 5360, train/learning_rate: 2.8740281777572818e-05\n",
      "Step: 5360, train/epoch: 1.275583028793335\n",
      "Step: 5370, train/loss: 0.0\n",
      "Step: 5370, train/grad_norm: 2.5069033762932513e-13\n",
      "Step: 5370, train/learning_rate: 2.87006187136285e-05\n",
      "Step: 5370, train/epoch: 1.2779629230499268\n",
      "Step: 5380, train/loss: 0.05000000074505806\n",
      "Step: 5380, train/grad_norm: 1.6597107802690525e-13\n",
      "Step: 5380, train/learning_rate: 2.8660955649684183e-05\n",
      "Step: 5380, train/epoch: 1.280342698097229\n",
      "Step: 5390, train/loss: 0.22499999403953552\n",
      "Step: 5390, train/grad_norm: 68.37811279296875\n",
      "Step: 5390, train/learning_rate: 2.8621290766750462e-05\n",
      "Step: 5390, train/epoch: 1.2827224731445312\n",
      "Step: 5400, train/loss: 0.0\n",
      "Step: 5400, train/grad_norm: 1.911967046908103e-07\n",
      "Step: 5400, train/learning_rate: 2.8581627702806145e-05\n",
      "Step: 5400, train/epoch: 1.285102367401123\n",
      "Step: 5410, train/loss: 0.0003000000142492354\n",
      "Step: 5410, train/grad_norm: 3.145550264704866e-09\n",
      "Step: 5410, train/learning_rate: 2.8541964638861828e-05\n",
      "Step: 5410, train/epoch: 1.2874821424484253\n",
      "Step: 5420, train/loss: 0.0\n",
      "Step: 5420, train/grad_norm: 1.6507378797214756e-09\n",
      "Step: 5420, train/learning_rate: 2.8502299755928107e-05\n",
      "Step: 5420, train/epoch: 1.2898619174957275\n",
      "Step: 5430, train/loss: 0.0\n",
      "Step: 5430, train/grad_norm: 1.5141580211874839e-09\n",
      "Step: 5430, train/learning_rate: 2.846263669198379e-05\n",
      "Step: 5430, train/epoch: 1.2922418117523193\n",
      "Step: 5440, train/loss: 0.22499999403953552\n",
      "Step: 5440, train/grad_norm: 1.1945266123802867e-05\n",
      "Step: 5440, train/learning_rate: 2.8422973628039472e-05\n",
      "Step: 5440, train/epoch: 1.2946215867996216\n",
      "Step: 5450, train/loss: 0.00019999999494757503\n",
      "Step: 5450, train/grad_norm: 0.00028172298334538937\n",
      "Step: 5450, train/learning_rate: 2.838330874510575e-05\n",
      "Step: 5450, train/epoch: 1.2970014810562134\n",
      "Step: 5460, train/loss: 0.0\n",
      "Step: 5460, train/grad_norm: 1.4919373825250659e-05\n",
      "Step: 5460, train/learning_rate: 2.8343645681161433e-05\n",
      "Step: 5460, train/epoch: 1.2993812561035156\n",
      "Step: 5470, train/loss: 0.0\n",
      "Step: 5470, train/grad_norm: 4.4345816263557936e-07\n",
      "Step: 5470, train/learning_rate: 2.8303982617217116e-05\n",
      "Step: 5470, train/epoch: 1.3017610311508179\n",
      "Step: 5480, train/loss: 0.0\n",
      "Step: 5480, train/grad_norm: 7.272899892996065e-06\n",
      "Step: 5480, train/learning_rate: 2.8264317734283395e-05\n",
      "Step: 5480, train/epoch: 1.3041409254074097\n",
      "Step: 5490, train/loss: 0.0\n",
      "Step: 5490, train/grad_norm: 1.12232362425857e-06\n",
      "Step: 5490, train/learning_rate: 2.8224654670339078e-05\n",
      "Step: 5490, train/epoch: 1.306520700454712\n",
      "Step: 5500, train/loss: 0.0\n",
      "Step: 5500, train/grad_norm: 5.702757022163496e-08\n",
      "Step: 5500, train/learning_rate: 2.818499160639476e-05\n",
      "Step: 5500, train/epoch: 1.3089004755020142\n",
      "Step: 5510, train/loss: 0.0\n",
      "Step: 5510, train/grad_norm: 4.2145138650084846e-06\n",
      "Step: 5510, train/learning_rate: 2.814532672346104e-05\n",
      "Step: 5510, train/epoch: 1.311280369758606\n",
      "Step: 5520, train/loss: 0.0\n",
      "Step: 5520, train/grad_norm: 3.338938768138178e-05\n",
      "Step: 5520, train/learning_rate: 2.8105663659516722e-05\n",
      "Step: 5520, train/epoch: 1.3136601448059082\n",
      "Step: 5530, train/loss: 0.0\n",
      "Step: 5530, train/grad_norm: 2.1917379910973978e-08\n",
      "Step: 5530, train/learning_rate: 2.8066000595572405e-05\n",
      "Step: 5530, train/epoch: 1.3160400390625\n",
      "Step: 5540, train/loss: 0.0\n",
      "Step: 5540, train/grad_norm: 4.33049353887327e-06\n",
      "Step: 5540, train/learning_rate: 2.8026337531628087e-05\n",
      "Step: 5540, train/epoch: 1.3184198141098022\n",
      "Step: 5550, train/loss: 0.0\n",
      "Step: 5550, train/grad_norm: 8.231614287979028e-07\n",
      "Step: 5550, train/learning_rate: 2.7986672648694366e-05\n",
      "Step: 5550, train/epoch: 1.3207995891571045\n",
      "Step: 5560, train/loss: 0.0\n",
      "Step: 5560, train/grad_norm: 3.990463483205531e-06\n",
      "Step: 5560, train/learning_rate: 2.794700958475005e-05\n",
      "Step: 5560, train/epoch: 1.3231794834136963\n",
      "Step: 5570, train/loss: 0.0\n",
      "Step: 5570, train/grad_norm: 4.0723878669268743e-07\n",
      "Step: 5570, train/learning_rate: 2.790734652080573e-05\n",
      "Step: 5570, train/epoch: 1.3255592584609985\n",
      "Step: 5580, train/loss: 0.0\n",
      "Step: 5580, train/grad_norm: 3.5924531402997673e-06\n",
      "Step: 5580, train/learning_rate: 2.786768163787201e-05\n",
      "Step: 5580, train/epoch: 1.3279390335083008\n",
      "Step: 5590, train/loss: 0.0\n",
      "Step: 5590, train/grad_norm: 1.9861452926761558e-08\n",
      "Step: 5590, train/learning_rate: 2.7828018573927693e-05\n",
      "Step: 5590, train/epoch: 1.3303189277648926\n",
      "Step: 5600, train/loss: 0.0\n",
      "Step: 5600, train/grad_norm: 4.850135155720636e-07\n",
      "Step: 5600, train/learning_rate: 2.7788355509983376e-05\n",
      "Step: 5600, train/epoch: 1.3326987028121948\n",
      "Step: 5610, train/loss: 0.0\n",
      "Step: 5610, train/grad_norm: 3.494147620131116e-07\n",
      "Step: 5610, train/learning_rate: 2.7748690627049655e-05\n",
      "Step: 5610, train/epoch: 1.335078477859497\n",
      "Step: 5620, train/loss: 0.0\n",
      "Step: 5620, train/grad_norm: 3.102199457316601e-07\n",
      "Step: 5620, train/learning_rate: 2.7709027563105337e-05\n",
      "Step: 5620, train/epoch: 1.3374583721160889\n",
      "Step: 5630, train/loss: 0.0\n",
      "Step: 5630, train/grad_norm: 1.0303118642696063e-06\n",
      "Step: 5630, train/learning_rate: 2.766936449916102e-05\n",
      "Step: 5630, train/epoch: 1.3398381471633911\n",
      "Step: 5640, train/loss: 0.0\n",
      "Step: 5640, train/grad_norm: 2.0936855435138568e-05\n",
      "Step: 5640, train/learning_rate: 2.76296996162273e-05\n",
      "Step: 5640, train/epoch: 1.342218041419983\n",
      "Step: 5650, train/loss: 0.0\n",
      "Step: 5650, train/grad_norm: 1.678413141803503e-08\n",
      "Step: 5650, train/learning_rate: 2.759003655228298e-05\n",
      "Step: 5650, train/epoch: 1.3445978164672852\n",
      "Step: 5660, train/loss: 0.04859999939799309\n",
      "Step: 5660, train/grad_norm: 2.7824187327496475e-06\n",
      "Step: 5660, train/learning_rate: 2.7550373488338664e-05\n",
      "Step: 5660, train/epoch: 1.3469775915145874\n",
      "Step: 5670, train/loss: 0.0\n",
      "Step: 5670, train/grad_norm: 1.4235070011636708e-07\n",
      "Step: 5670, train/learning_rate: 2.7510708605404943e-05\n",
      "Step: 5670, train/epoch: 1.3493574857711792\n",
      "Step: 5680, train/loss: 0.0\n",
      "Step: 5680, train/grad_norm: 1.634514319448499e-07\n",
      "Step: 5680, train/learning_rate: 2.7471045541460626e-05\n",
      "Step: 5680, train/epoch: 1.3517372608184814\n",
      "Step: 5690, train/loss: 0.0\n",
      "Step: 5690, train/grad_norm: 2.482573968620727e-08\n",
      "Step: 5690, train/learning_rate: 2.743138247751631e-05\n",
      "Step: 5690, train/epoch: 1.3541170358657837\n",
      "Step: 5700, train/loss: 0.0\n",
      "Step: 5700, train/grad_norm: 3.5468931969262485e-07\n",
      "Step: 5700, train/learning_rate: 2.7391717594582587e-05\n",
      "Step: 5700, train/epoch: 1.3564969301223755\n",
      "Step: 5710, train/loss: 0.0\n",
      "Step: 5710, train/grad_norm: 5.060107923782198e-07\n",
      "Step: 5710, train/learning_rate: 2.735205453063827e-05\n",
      "Step: 5710, train/epoch: 1.3588767051696777\n",
      "Step: 5720, train/loss: 0.0\n",
      "Step: 5720, train/grad_norm: 9.10122980712913e-06\n",
      "Step: 5720, train/learning_rate: 2.7312391466693953e-05\n",
      "Step: 5720, train/epoch: 1.3612565994262695\n",
      "Step: 5730, train/loss: 0.0\n",
      "Step: 5730, train/grad_norm: 1.0749503331908272e-07\n",
      "Step: 5730, train/learning_rate: 2.7272726583760232e-05\n",
      "Step: 5730, train/epoch: 1.3636363744735718\n",
      "Step: 5740, train/loss: 0.0\n",
      "Step: 5740, train/grad_norm: 3.7820652778464137e-06\n",
      "Step: 5740, train/learning_rate: 2.7233063519815914e-05\n",
      "Step: 5740, train/epoch: 1.366016149520874\n",
      "Step: 5750, train/loss: 0.0\n",
      "Step: 5750, train/grad_norm: 1.8909487664586777e-07\n",
      "Step: 5750, train/learning_rate: 2.7193400455871597e-05\n",
      "Step: 5750, train/epoch: 1.3683960437774658\n",
      "Step: 5760, train/loss: 0.0\n",
      "Step: 5760, train/grad_norm: 1.852089503984189e-08\n",
      "Step: 5760, train/learning_rate: 2.7153735572937876e-05\n",
      "Step: 5760, train/epoch: 1.370775818824768\n",
      "Step: 5770, train/loss: 0.0\n",
      "Step: 5770, train/grad_norm: 4.824296411243267e-07\n",
      "Step: 5770, train/learning_rate: 2.711407250899356e-05\n",
      "Step: 5770, train/epoch: 1.3731555938720703\n",
      "Step: 5780, train/loss: 0.0\n",
      "Step: 5780, train/grad_norm: 3.944917352782795e-07\n",
      "Step: 5780, train/learning_rate: 2.707440944504924e-05\n",
      "Step: 5780, train/epoch: 1.375535488128662\n",
      "Step: 5790, train/loss: 0.0\n",
      "Step: 5790, train/grad_norm: 1.0736293631907756e-07\n",
      "Step: 5790, train/learning_rate: 2.703474456211552e-05\n",
      "Step: 5790, train/epoch: 1.3779152631759644\n",
      "Step: 5800, train/loss: 0.0\n",
      "Step: 5800, train/grad_norm: 1.358862618872081e-06\n",
      "Step: 5800, train/learning_rate: 2.6995081498171203e-05\n",
      "Step: 5800, train/epoch: 1.3802950382232666\n",
      "Step: 5810, train/loss: 0.0\n",
      "Step: 5810, train/grad_norm: 1.3308752500051924e-07\n",
      "Step: 5810, train/learning_rate: 2.6955418434226885e-05\n",
      "Step: 5810, train/epoch: 1.3826749324798584\n",
      "Step: 5820, train/loss: 0.0\n",
      "Step: 5820, train/grad_norm: 1.2032207052925514e-07\n",
      "Step: 5820, train/learning_rate: 2.6915753551293164e-05\n",
      "Step: 5820, train/epoch: 1.3850547075271606\n",
      "Step: 5830, train/loss: 0.11630000174045563\n",
      "Step: 5830, train/grad_norm: 1.9152428976099145e-08\n",
      "Step: 5830, train/learning_rate: 2.6876090487348847e-05\n",
      "Step: 5830, train/epoch: 1.3874346017837524\n",
      "Step: 5840, train/loss: 0.0\n",
      "Step: 5840, train/grad_norm: 6.848740952136723e-08\n",
      "Step: 5840, train/learning_rate: 2.683642742340453e-05\n",
      "Step: 5840, train/epoch: 1.3898143768310547\n",
      "Step: 5850, train/loss: 0.0\n",
      "Step: 5850, train/grad_norm: 3.360124267715037e-08\n",
      "Step: 5850, train/learning_rate: 2.679676254047081e-05\n",
      "Step: 5850, train/epoch: 1.392194151878357\n",
      "Step: 5860, train/loss: 0.0\n",
      "Step: 5860, train/grad_norm: 0.00018269135034643114\n",
      "Step: 5860, train/learning_rate: 2.675709947652649e-05\n",
      "Step: 5860, train/epoch: 1.3945740461349487\n",
      "Step: 5870, train/loss: 0.04690000042319298\n",
      "Step: 5870, train/grad_norm: 5.885506197955692e-07\n",
      "Step: 5870, train/learning_rate: 2.6717436412582174e-05\n",
      "Step: 5870, train/epoch: 1.396953821182251\n",
      "Step: 5880, train/loss: 0.11410000175237656\n",
      "Step: 5880, train/grad_norm: 669.31396484375\n",
      "Step: 5880, train/learning_rate: 2.6677773348637857e-05\n",
      "Step: 5880, train/epoch: 1.3993335962295532\n",
      "Step: 5890, train/loss: 0.0\n",
      "Step: 5890, train/grad_norm: 1.7324407508567674e-07\n",
      "Step: 5890, train/learning_rate: 2.6638108465704136e-05\n",
      "Step: 5890, train/epoch: 1.401713490486145\n",
      "Step: 5900, train/loss: 0.006099999882280827\n",
      "Step: 5900, train/grad_norm: 1.2908772362152376e-07\n",
      "Step: 5900, train/learning_rate: 2.6598445401759818e-05\n",
      "Step: 5900, train/epoch: 1.4040932655334473\n",
      "Step: 5910, train/loss: 0.0\n",
      "Step: 5910, train/grad_norm: 9.438525125915476e-07\n",
      "Step: 5910, train/learning_rate: 2.65587823378155e-05\n",
      "Step: 5910, train/epoch: 1.406473159790039\n",
      "Step: 5920, train/loss: 0.0\n",
      "Step: 5920, train/grad_norm: 0.0003138610045425594\n",
      "Step: 5920, train/learning_rate: 2.651911745488178e-05\n",
      "Step: 5920, train/epoch: 1.4088529348373413\n",
      "Step: 5930, train/loss: 0.0\n",
      "Step: 5930, train/grad_norm: 4.465339387138556e-08\n",
      "Step: 5930, train/learning_rate: 2.6479454390937462e-05\n",
      "Step: 5930, train/epoch: 1.4112327098846436\n",
      "Step: 5940, train/loss: 0.04749999940395355\n",
      "Step: 5940, train/grad_norm: 8.284395391910948e-08\n",
      "Step: 5940, train/learning_rate: 2.6439791326993145e-05\n",
      "Step: 5940, train/epoch: 1.4136126041412354\n",
      "Step: 5950, train/loss: 0.029100000858306885\n",
      "Step: 5950, train/grad_norm: 1.7353634120809147e-06\n",
      "Step: 5950, train/learning_rate: 2.6400126444059424e-05\n",
      "Step: 5950, train/epoch: 1.4159923791885376\n",
      "Step: 5960, train/loss: 0.00019999999494757503\n",
      "Step: 5960, train/grad_norm: 0.0023939749225974083\n",
      "Step: 5960, train/learning_rate: 2.6360463380115107e-05\n",
      "Step: 5960, train/epoch: 1.4183721542358398\n",
      "Step: 5970, train/loss: 0.3167000114917755\n",
      "Step: 5970, train/grad_norm: 4.645646978218565e-09\n",
      "Step: 5970, train/learning_rate: 2.632080031617079e-05\n",
      "Step: 5970, train/epoch: 1.4207520484924316\n",
      "Step: 5980, train/loss: 0.14069999754428864\n",
      "Step: 5980, train/grad_norm: 0.20524893701076508\n",
      "Step: 5980, train/learning_rate: 2.628113543323707e-05\n",
      "Step: 5980, train/epoch: 1.4231318235397339\n",
      "Step: 5990, train/loss: 0.006399999838322401\n",
      "Step: 5990, train/grad_norm: 3.940267561119981e-06\n",
      "Step: 5990, train/learning_rate: 2.624147236929275e-05\n",
      "Step: 5990, train/epoch: 1.4255117177963257\n",
      "Step: 6000, train/loss: 0.0\n",
      "Step: 6000, train/grad_norm: 3.743235943431955e-09\n",
      "Step: 6000, train/learning_rate: 2.6201809305348434e-05\n",
      "Step: 6000, train/epoch: 1.427891492843628\n",
      "Step: 6010, train/loss: 0.0\n",
      "Step: 6010, train/grad_norm: 2.0526315369640002e-10\n",
      "Step: 6010, train/learning_rate: 2.6162144422414713e-05\n",
      "Step: 6010, train/epoch: 1.4302712678909302\n",
      "Step: 6020, train/loss: 0.0\n",
      "Step: 6020, train/grad_norm: 2.1240498943031483e-11\n",
      "Step: 6020, train/learning_rate: 2.6122481358470395e-05\n",
      "Step: 6020, train/epoch: 1.432651162147522\n",
      "Step: 6030, train/loss: 0.0\n",
      "Step: 6030, train/grad_norm: 1.4780053569918294e-10\n",
      "Step: 6030, train/learning_rate: 2.6082818294526078e-05\n",
      "Step: 6030, train/epoch: 1.4350309371948242\n",
      "Step: 6040, train/loss: 0.0\n",
      "Step: 6040, train/grad_norm: 3.690734176609034e-11\n",
      "Step: 6040, train/learning_rate: 2.6043153411592357e-05\n",
      "Step: 6040, train/epoch: 1.4374107122421265\n",
      "Step: 6050, train/loss: 0.0\n",
      "Step: 6050, train/grad_norm: 2.350102577819424e-11\n",
      "Step: 6050, train/learning_rate: 2.600349034764804e-05\n",
      "Step: 6050, train/epoch: 1.4397906064987183\n",
      "Step: 6060, train/loss: 0.0\n",
      "Step: 6060, train/grad_norm: 3.4805980320129493e-10\n",
      "Step: 6060, train/learning_rate: 2.5963827283703722e-05\n",
      "Step: 6060, train/epoch: 1.4421703815460205\n",
      "Step: 6070, train/loss: 0.0\n",
      "Step: 6070, train/grad_norm: 6.548848130449869e-10\n",
      "Step: 6070, train/learning_rate: 2.592416240077e-05\n",
      "Step: 6070, train/epoch: 1.4445501565933228\n",
      "Step: 6080, train/loss: 0.0\n",
      "Step: 6080, train/grad_norm: 4.869382119743193e-10\n",
      "Step: 6080, train/learning_rate: 2.5884499336825684e-05\n",
      "Step: 6080, train/epoch: 1.4469300508499146\n",
      "Step: 6090, train/loss: 0.0\n",
      "Step: 6090, train/grad_norm: 1.8121696365724915e-09\n",
      "Step: 6090, train/learning_rate: 2.5844836272881366e-05\n",
      "Step: 6090, train/epoch: 1.4493098258972168\n",
      "Step: 6100, train/loss: 0.0\n",
      "Step: 6100, train/grad_norm: 2.2119762377315055e-07\n",
      "Step: 6100, train/learning_rate: 2.5805171389947645e-05\n",
      "Step: 6100, train/epoch: 1.4516897201538086\n",
      "Step: 6110, train/loss: 0.0\n",
      "Step: 6110, train/grad_norm: 9.684929835884759e-08\n",
      "Step: 6110, train/learning_rate: 2.5765508326003328e-05\n",
      "Step: 6110, train/epoch: 1.4540694952011108\n",
      "Step: 6120, train/loss: 0.0010000000474974513\n",
      "Step: 6120, train/grad_norm: 4.603568914962608e-10\n",
      "Step: 6120, train/learning_rate: 2.572584526205901e-05\n",
      "Step: 6120, train/epoch: 1.456449270248413\n",
      "Step: 6130, train/loss: 0.0\n",
      "Step: 6130, train/grad_norm: 1.8804943713313094e-10\n",
      "Step: 6130, train/learning_rate: 2.568618037912529e-05\n",
      "Step: 6130, train/epoch: 1.4588291645050049\n",
      "Step: 6140, train/loss: 0.0\n",
      "Step: 6140, train/grad_norm: 3.166659295894547e-11\n",
      "Step: 6140, train/learning_rate: 2.5646517315180972e-05\n",
      "Step: 6140, train/epoch: 1.4612089395523071\n",
      "Step: 6150, train/loss: 0.0\n",
      "Step: 6150, train/grad_norm: 2.1918408776855358e-07\n",
      "Step: 6150, train/learning_rate: 2.5606854251236655e-05\n",
      "Step: 6150, train/epoch: 1.4635887145996094\n",
      "Step: 6160, train/loss: 0.0\n",
      "Step: 6160, train/grad_norm: 9.606888201618702e-12\n",
      "Step: 6160, train/learning_rate: 2.5567189368302934e-05\n",
      "Step: 6160, train/epoch: 1.4659686088562012\n",
      "Step: 6170, train/loss: 0.0\n",
      "Step: 6170, train/grad_norm: 6.307872695288097e-07\n",
      "Step: 6170, train/learning_rate: 2.5527526304358616e-05\n",
      "Step: 6170, train/epoch: 1.4683483839035034\n",
      "Step: 6180, train/loss: 0.0\n",
      "Step: 6180, train/grad_norm: 8.805209006368386e-09\n",
      "Step: 6180, train/learning_rate: 2.54878632404143e-05\n",
      "Step: 6180, train/epoch: 1.4707282781600952\n",
      "Step: 6190, train/loss: 0.0\n",
      "Step: 6190, train/grad_norm: 7.489393993864724e-08\n",
      "Step: 6190, train/learning_rate: 2.544820017646998e-05\n",
      "Step: 6190, train/epoch: 1.4731080532073975\n",
      "Step: 6200, train/loss: 0.0\n",
      "Step: 6200, train/grad_norm: 1.9783079341362253e-10\n",
      "Step: 6200, train/learning_rate: 2.540853529353626e-05\n",
      "Step: 6200, train/epoch: 1.4754878282546997\n",
      "Step: 6210, train/loss: 0.0\n",
      "Step: 6210, train/grad_norm: 9.81361392149438e-09\n",
      "Step: 6210, train/learning_rate: 2.5368872229591943e-05\n",
      "Step: 6210, train/epoch: 1.4778677225112915\n",
      "Step: 6220, train/loss: 0.11559999734163284\n",
      "Step: 6220, train/grad_norm: 5.450878082235988e-10\n",
      "Step: 6220, train/learning_rate: 2.5329209165647626e-05\n",
      "Step: 6220, train/epoch: 1.4802474975585938\n",
      "Step: 6230, train/loss: 0.0\n",
      "Step: 6230, train/grad_norm: 6.649523054576278e-13\n",
      "Step: 6230, train/learning_rate: 2.5289544282713905e-05\n",
      "Step: 6230, train/epoch: 1.482627272605896\n",
      "Step: 6240, train/loss: 0.0\n",
      "Step: 6240, train/grad_norm: 7.550143843459978e-10\n",
      "Step: 6240, train/learning_rate: 2.5249881218769588e-05\n",
      "Step: 6240, train/epoch: 1.4850071668624878\n",
      "Step: 6250, train/loss: 0.0\n",
      "Step: 6250, train/grad_norm: 4.501347905527808e-11\n",
      "Step: 6250, train/learning_rate: 2.521021815482527e-05\n",
      "Step: 6250, train/epoch: 1.48738694190979\n",
      "Step: 6260, train/loss: 0.0\n",
      "Step: 6260, train/grad_norm: 7.153750924970836e-10\n",
      "Step: 6260, train/learning_rate: 2.517055327189155e-05\n",
      "Step: 6260, train/epoch: 1.4897668361663818\n",
      "Step: 6270, train/loss: 9.999999747378752e-05\n",
      "Step: 6270, train/grad_norm: 5.891979681926784e-10\n",
      "Step: 6270, train/learning_rate: 2.5130890207947232e-05\n",
      "Step: 6270, train/epoch: 1.492146611213684\n",
      "Step: 6280, train/loss: 0.0\n",
      "Step: 6280, train/grad_norm: 3.4539920923393197e-10\n",
      "Step: 6280, train/learning_rate: 2.5091227144002914e-05\n",
      "Step: 6280, train/epoch: 1.4945263862609863\n",
      "Step: 6290, train/loss: 0.0\n",
      "Step: 6290, train/grad_norm: 1.100387576197237e-11\n",
      "Step: 6290, train/learning_rate: 2.5051562261069193e-05\n",
      "Step: 6290, train/epoch: 1.4969062805175781\n",
      "Step: 6300, train/loss: 0.0\n",
      "Step: 6300, train/grad_norm: 6.836564647727528e-10\n",
      "Step: 6300, train/learning_rate: 2.5011899197124876e-05\n",
      "Step: 6300, train/epoch: 1.4992860555648804\n",
      "Step: 6310, train/loss: 0.0\n",
      "Step: 6310, train/grad_norm: 3.222563049742888e-10\n",
      "Step: 6310, train/learning_rate: 2.497223613318056e-05\n",
      "Step: 6310, train/epoch: 1.5016658306121826\n",
      "Step: 6320, train/loss: 0.0\n",
      "Step: 6320, train/grad_norm: 1.7575765287602962e-09\n",
      "Step: 6320, train/learning_rate: 2.4932571250246838e-05\n",
      "Step: 6320, train/epoch: 1.5040457248687744\n",
      "Step: 6330, train/loss: 0.0\n",
      "Step: 6330, train/grad_norm: 1.3485063066198677e-09\n",
      "Step: 6330, train/learning_rate: 2.489290818630252e-05\n",
      "Step: 6330, train/epoch: 1.5064254999160767\n",
      "Step: 6340, train/loss: 0.005200000014156103\n",
      "Step: 6340, train/grad_norm: 7.262701551269402e-10\n",
      "Step: 6340, train/learning_rate: 2.4853245122358203e-05\n",
      "Step: 6340, train/epoch: 1.508805274963379\n",
      "Step: 6350, train/loss: 0.0\n",
      "Step: 6350, train/grad_norm: 8.238569271235008e-10\n",
      "Step: 6350, train/learning_rate: 2.4813580239424482e-05\n",
      "Step: 6350, train/epoch: 1.5111851692199707\n",
      "Step: 6360, train/loss: 0.27970001101493835\n",
      "Step: 6360, train/grad_norm: 1.518643699682798e-08\n",
      "Step: 6360, train/learning_rate: 2.4773917175480165e-05\n",
      "Step: 6360, train/epoch: 1.513564944267273\n",
      "Step: 6370, train/loss: 0.24120000004768372\n",
      "Step: 6370, train/grad_norm: 9.492572949909572e-09\n",
      "Step: 6370, train/learning_rate: 2.4734254111535847e-05\n",
      "Step: 6370, train/epoch: 1.5159448385238647\n",
      "Step: 6380, train/loss: 0.2969000041484833\n",
      "Step: 6380, train/grad_norm: 9.875122941593872e-10\n",
      "Step: 6380, train/learning_rate: 2.4694589228602126e-05\n",
      "Step: 6380, train/epoch: 1.518324613571167\n",
      "Step: 6390, train/loss: 0.26249998807907104\n",
      "Step: 6390, train/grad_norm: 4.028849254211764e-09\n",
      "Step: 6390, train/learning_rate: 2.465492616465781e-05\n",
      "Step: 6390, train/epoch: 1.5207043886184692\n",
      "Step: 6400, train/loss: 0.0\n",
      "Step: 6400, train/grad_norm: 5.962701266071235e-08\n",
      "Step: 6400, train/learning_rate: 2.461526310071349e-05\n",
      "Step: 6400, train/epoch: 1.523084282875061\n",
      "Step: 6410, train/loss: 0.0\n",
      "Step: 6410, train/grad_norm: 7.794110246095443e-09\n",
      "Step: 6410, train/learning_rate: 2.457559821777977e-05\n",
      "Step: 6410, train/epoch: 1.5254640579223633\n",
      "Step: 6420, train/loss: 0.0\n",
      "Step: 6420, train/grad_norm: 5.606706765526326e-10\n",
      "Step: 6420, train/learning_rate: 2.4535935153835453e-05\n",
      "Step: 6420, train/epoch: 1.5278438329696655\n",
      "Step: 6430, train/loss: 0.0\n",
      "Step: 6430, train/grad_norm: 1.0990945931155238e-08\n",
      "Step: 6430, train/learning_rate: 2.4496272089891136e-05\n",
      "Step: 6430, train/epoch: 1.5302237272262573\n",
      "Step: 6440, train/loss: 0.0\n",
      "Step: 6440, train/grad_norm: 7.967808812736621e-08\n",
      "Step: 6440, train/learning_rate: 2.4456607206957415e-05\n",
      "Step: 6440, train/epoch: 1.5326035022735596\n",
      "Step: 6450, train/loss: 0.0\n",
      "Step: 6450, train/grad_norm: 1.1855475889888112e-07\n",
      "Step: 6450, train/learning_rate: 2.4416944143013097e-05\n",
      "Step: 6450, train/epoch: 1.5349833965301514\n",
      "Step: 6460, train/loss: 0.0010000000474974513\n",
      "Step: 6460, train/grad_norm: 6.005286223853545e-08\n",
      "Step: 6460, train/learning_rate: 2.437728107906878e-05\n",
      "Step: 6460, train/epoch: 1.5373631715774536\n",
      "Step: 6470, train/loss: 0.0\n",
      "Step: 6470, train/grad_norm: 1.9462740965536796e-06\n",
      "Step: 6470, train/learning_rate: 2.433761619613506e-05\n",
      "Step: 6470, train/epoch: 1.5397429466247559\n",
      "Step: 6480, train/loss: 0.0\n",
      "Step: 6480, train/grad_norm: 1.746255634316185e-06\n",
      "Step: 6480, train/learning_rate: 2.429795313219074e-05\n",
      "Step: 6480, train/epoch: 1.5421228408813477\n",
      "Step: 6490, train/loss: 0.0\n",
      "Step: 6490, train/grad_norm: 5.101633178128395e-07\n",
      "Step: 6490, train/learning_rate: 2.4258290068246424e-05\n",
      "Step: 6490, train/epoch: 1.54450261592865\n",
      "Step: 6500, train/loss: 0.0\n",
      "Step: 6500, train/grad_norm: 0.014356807805597782\n",
      "Step: 6500, train/learning_rate: 2.4218625185312703e-05\n",
      "Step: 6500, train/epoch: 1.5468823909759521\n",
      "Step: 6510, train/loss: 0.0\n",
      "Step: 6510, train/grad_norm: 2.1357786295084225e-07\n",
      "Step: 6510, train/learning_rate: 2.4178962121368386e-05\n",
      "Step: 6510, train/epoch: 1.549262285232544\n",
      "Step: 6520, train/loss: 0.0\n",
      "Step: 6520, train/grad_norm: 2.615084326862416e-07\n",
      "Step: 6520, train/learning_rate: 2.413929905742407e-05\n",
      "Step: 6520, train/epoch: 1.5516420602798462\n",
      "Step: 6530, train/loss: 0.0\n",
      "Step: 6530, train/grad_norm: 1.4906868273101281e-07\n",
      "Step: 6530, train/learning_rate: 2.409963599347975e-05\n",
      "Step: 6530, train/epoch: 1.5540218353271484\n",
      "Step: 6540, train/loss: 0.0\n",
      "Step: 6540, train/grad_norm: 9.211237994577459e-08\n",
      "Step: 6540, train/learning_rate: 2.405997111054603e-05\n",
      "Step: 6540, train/epoch: 1.5564017295837402\n",
      "Step: 6550, train/loss: 0.0\n",
      "Step: 6550, train/grad_norm: 1.4671795334209037e-08\n",
      "Step: 6550, train/learning_rate: 2.4020308046601713e-05\n",
      "Step: 6550, train/epoch: 1.5587815046310425\n",
      "Step: 6560, train/loss: 0.0\n",
      "Step: 6560, train/grad_norm: 3.8685858072540213e-08\n",
      "Step: 6560, train/learning_rate: 2.3980644982657395e-05\n",
      "Step: 6560, train/epoch: 1.5611613988876343\n",
      "Step: 6570, train/loss: 0.0\n",
      "Step: 6570, train/grad_norm: 2.6916842088553494e-08\n",
      "Step: 6570, train/learning_rate: 2.3940980099723674e-05\n",
      "Step: 6570, train/epoch: 1.5635411739349365\n",
      "Step: 6580, train/loss: 0.0\n",
      "Step: 6580, train/grad_norm: 0.00010208435560343787\n",
      "Step: 6580, train/learning_rate: 2.3901317035779357e-05\n",
      "Step: 6580, train/epoch: 1.5659209489822388\n",
      "Step: 6590, train/loss: 0.0\n",
      "Step: 6590, train/grad_norm: 3.9363276527204505e-10\n",
      "Step: 6590, train/learning_rate: 2.386165397183504e-05\n",
      "Step: 6590, train/epoch: 1.5683008432388306\n",
      "Step: 6600, train/loss: 0.0006000000284984708\n",
      "Step: 6600, train/grad_norm: 2.8460272361030547e-08\n",
      "Step: 6600, train/learning_rate: 2.382198908890132e-05\n",
      "Step: 6600, train/epoch: 1.5706806182861328\n",
      "Step: 6610, train/loss: 0.0\n",
      "Step: 6610, train/grad_norm: 6.104500238635069e-10\n",
      "Step: 6610, train/learning_rate: 2.3782326024957e-05\n",
      "Step: 6610, train/epoch: 1.573060393333435\n",
      "Step: 6620, train/loss: 9.999999747378752e-05\n",
      "Step: 6620, train/grad_norm: 2.073455572128296\n",
      "Step: 6620, train/learning_rate: 2.3742662961012684e-05\n",
      "Step: 6620, train/epoch: 1.5754402875900269\n",
      "Step: 6630, train/loss: 0.0\n",
      "Step: 6630, train/grad_norm: 1.0822522966691395e-07\n",
      "Step: 6630, train/learning_rate: 2.3702998078078963e-05\n",
      "Step: 6630, train/epoch: 1.577820062637329\n",
      "Step: 6640, train/loss: 0.0\n",
      "Step: 6640, train/grad_norm: 4.534673969658343e-09\n",
      "Step: 6640, train/learning_rate: 2.3663335014134645e-05\n",
      "Step: 6640, train/epoch: 1.580199956893921\n",
      "Step: 6650, train/loss: 0.0012000000569969416\n",
      "Step: 6650, train/grad_norm: 3.2900748792030754e-10\n",
      "Step: 6650, train/learning_rate: 2.3623671950190328e-05\n",
      "Step: 6650, train/epoch: 1.5825797319412231\n",
      "Step: 6660, train/loss: 0.0\n",
      "Step: 6660, train/grad_norm: 6.9572242864524014e-06\n",
      "Step: 6660, train/learning_rate: 2.3584007067256607e-05\n",
      "Step: 6660, train/epoch: 1.5849595069885254\n",
      "Step: 6670, train/loss: 0.0\n",
      "Step: 6670, train/grad_norm: 2.846187695249025e-11\n",
      "Step: 6670, train/learning_rate: 2.354434400331229e-05\n",
      "Step: 6670, train/epoch: 1.5873394012451172\n",
      "Step: 6680, train/loss: 0.002899999963119626\n",
      "Step: 6680, train/grad_norm: 6.972476285227458e-07\n",
      "Step: 6680, train/learning_rate: 2.3504680939367972e-05\n",
      "Step: 6680, train/epoch: 1.5897191762924194\n",
      "Step: 6690, train/loss: 0.0\n",
      "Step: 6690, train/grad_norm: 8.391038974764342e-10\n",
      "Step: 6690, train/learning_rate: 2.346501605643425e-05\n",
      "Step: 6690, train/epoch: 1.5920989513397217\n",
      "Step: 6700, train/loss: 0.0\n",
      "Step: 6700, train/grad_norm: 4.778123452453542e-10\n",
      "Step: 6700, train/learning_rate: 2.3425352992489934e-05\n",
      "Step: 6700, train/epoch: 1.5944788455963135\n",
      "Step: 6710, train/loss: 0.0\n",
      "Step: 6710, train/grad_norm: 6.067455426972401e-09\n",
      "Step: 6710, train/learning_rate: 2.3385689928545617e-05\n",
      "Step: 6710, train/epoch: 1.5968586206436157\n",
      "Step: 6720, train/loss: 0.0\n",
      "Step: 6720, train/grad_norm: 0.05236198753118515\n",
      "Step: 6720, train/learning_rate: 2.3346025045611896e-05\n",
      "Step: 6720, train/epoch: 1.5992385149002075\n",
      "Step: 6730, train/loss: 0.0\n",
      "Step: 6730, train/grad_norm: 5.421450510745274e-10\n",
      "Step: 6730, train/learning_rate: 2.3306361981667578e-05\n",
      "Step: 6730, train/epoch: 1.6016182899475098\n",
      "Step: 6740, train/loss: 0.0\n",
      "Step: 6740, train/grad_norm: 3.0384022409180034e-08\n",
      "Step: 6740, train/learning_rate: 2.326669891772326e-05\n",
      "Step: 6740, train/epoch: 1.603998064994812\n",
      "Step: 6750, train/loss: 0.0\n",
      "Step: 6750, train/grad_norm: 3.2466749289028485e-09\n",
      "Step: 6750, train/learning_rate: 2.322703403478954e-05\n",
      "Step: 6750, train/epoch: 1.6063779592514038\n",
      "Step: 6760, train/loss: 0.0\n",
      "Step: 6760, train/grad_norm: 1.039581842832149e-08\n",
      "Step: 6760, train/learning_rate: 2.3187370970845222e-05\n",
      "Step: 6760, train/epoch: 1.608757734298706\n",
      "Step: 6770, train/loss: 0.0\n",
      "Step: 6770, train/grad_norm: 1.3373133711525043e-09\n",
      "Step: 6770, train/learning_rate: 2.3147707906900905e-05\n",
      "Step: 6770, train/epoch: 1.6111375093460083\n",
      "Step: 6780, train/loss: 0.0\n",
      "Step: 6780, train/grad_norm: 1.0986740761609326e-07\n",
      "Step: 6780, train/learning_rate: 2.3108043023967184e-05\n",
      "Step: 6780, train/epoch: 1.6135174036026\n",
      "Step: 6790, train/loss: 0.0\n",
      "Step: 6790, train/grad_norm: 4.0397429126590654e-21\n",
      "Step: 6790, train/learning_rate: 2.3068379960022867e-05\n",
      "Step: 6790, train/epoch: 1.6158971786499023\n",
      "Step: 6800, train/loss: 0.0\n",
      "Step: 6800, train/grad_norm: 1.0473723000004043e-09\n",
      "Step: 6800, train/learning_rate: 2.302871689607855e-05\n",
      "Step: 6800, train/epoch: 1.6182769536972046\n",
      "Step: 6810, train/loss: 0.0\n",
      "Step: 6810, train/grad_norm: 2.7433583182912e-10\n",
      "Step: 6810, train/learning_rate: 2.298905201314483e-05\n",
      "Step: 6810, train/epoch: 1.6206568479537964\n",
      "Step: 6820, train/loss: 0.0\n",
      "Step: 6820, train/grad_norm: 2.4986632207735227e-10\n",
      "Step: 6820, train/learning_rate: 2.294938894920051e-05\n",
      "Step: 6820, train/epoch: 1.6230366230010986\n",
      "Step: 6830, train/loss: 0.00039999998989515007\n",
      "Step: 6830, train/grad_norm: 2.3646948288380543e-10\n",
      "Step: 6830, train/learning_rate: 2.2909725885256194e-05\n",
      "Step: 6830, train/epoch: 1.6254165172576904\n",
      "Step: 6840, train/loss: 0.0\n",
      "Step: 6840, train/grad_norm: 1.1080643389504985e-06\n",
      "Step: 6840, train/learning_rate: 2.2870061002322473e-05\n",
      "Step: 6840, train/epoch: 1.6277962923049927\n",
      "Step: 6850, train/loss: 0.0\n",
      "Step: 6850, train/grad_norm: 8.017923214254097e-09\n",
      "Step: 6850, train/learning_rate: 2.2830397938378155e-05\n",
      "Step: 6850, train/epoch: 1.630176067352295\n",
      "Step: 6860, train/loss: 0.0\n",
      "Step: 6860, train/grad_norm: 2.9848227001139094e-08\n",
      "Step: 6860, train/learning_rate: 2.2790734874433838e-05\n",
      "Step: 6860, train/epoch: 1.6325559616088867\n",
      "Step: 6870, train/loss: 0.0\n",
      "Step: 6870, train/grad_norm: 4.132735820228106e-11\n",
      "Step: 6870, train/learning_rate: 2.275107181048952e-05\n",
      "Step: 6870, train/epoch: 1.634935736656189\n",
      "Step: 6880, train/loss: 0.3375000059604645\n",
      "Step: 6880, train/grad_norm: 8.503621140576456e-10\n",
      "Step: 6880, train/learning_rate: 2.27114069275558e-05\n",
      "Step: 6880, train/epoch: 1.6373155117034912\n",
      "Step: 6890, train/loss: 0.0\n",
      "Step: 6890, train/grad_norm: 1.405605321025405e-08\n",
      "Step: 6890, train/learning_rate: 2.2671743863611482e-05\n",
      "Step: 6890, train/epoch: 1.639695405960083\n",
      "Step: 6900, train/loss: 0.0\n",
      "Step: 6900, train/grad_norm: 4.3663952453698585e-08\n",
      "Step: 6900, train/learning_rate: 2.2632080799667165e-05\n",
      "Step: 6900, train/epoch: 1.6420751810073853\n",
      "Step: 6910, train/loss: 0.0\n",
      "Step: 6910, train/grad_norm: 6.74034994485595e-11\n",
      "Step: 6910, train/learning_rate: 2.2592415916733444e-05\n",
      "Step: 6910, train/epoch: 1.644455075263977\n",
      "Step: 6920, train/loss: 0.0\n",
      "Step: 6920, train/grad_norm: 6.75327171961726e-08\n",
      "Step: 6920, train/learning_rate: 2.2552752852789126e-05\n",
      "Step: 6920, train/epoch: 1.6468348503112793\n",
      "Step: 6930, train/loss: 0.0\n",
      "Step: 6930, train/grad_norm: 4.140157670917688e-06\n",
      "Step: 6930, train/learning_rate: 2.251308978884481e-05\n",
      "Step: 6930, train/epoch: 1.6492146253585815\n",
      "Step: 6940, train/loss: 0.0\n",
      "Step: 6940, train/grad_norm: 7.199441256489081e-07\n",
      "Step: 6940, train/learning_rate: 2.2473424905911088e-05\n",
      "Step: 6940, train/epoch: 1.6515945196151733\n",
      "Step: 6950, train/loss: 0.0\n",
      "Step: 6950, train/grad_norm: 1.4014820237662207e-07\n",
      "Step: 6950, train/learning_rate: 2.243376184196677e-05\n",
      "Step: 6950, train/epoch: 1.6539742946624756\n",
      "Step: 6960, train/loss: 0.0\n",
      "Step: 6960, train/grad_norm: 0.002869555726647377\n",
      "Step: 6960, train/learning_rate: 2.2394098778022453e-05\n",
      "Step: 6960, train/epoch: 1.6563540697097778\n",
      "Step: 6970, train/loss: 0.32510000467300415\n",
      "Step: 6970, train/grad_norm: 3.219844657564863e-08\n",
      "Step: 6970, train/learning_rate: 2.2354433895088732e-05\n",
      "Step: 6970, train/epoch: 1.6587339639663696\n",
      "Step: 6980, train/loss: 0.0\n",
      "Step: 6980, train/grad_norm: 1.291607265585526e-08\n",
      "Step: 6980, train/learning_rate: 2.2314770831144415e-05\n",
      "Step: 6980, train/epoch: 1.6611137390136719\n",
      "Step: 6990, train/loss: 0.0649000033736229\n",
      "Step: 6990, train/grad_norm: 3.665135972497069e-09\n",
      "Step: 6990, train/learning_rate: 2.2275107767200097e-05\n",
      "Step: 6990, train/epoch: 1.6634936332702637\n",
      "Step: 7000, train/loss: 0.0\n",
      "Step: 7000, train/grad_norm: 4.325242830560683e-09\n",
      "Step: 7000, train/learning_rate: 2.2235442884266376e-05\n",
      "Step: 7000, train/epoch: 1.665873408317566\n",
      "Step: 7010, train/loss: 0.0\n",
      "Step: 7010, train/grad_norm: 2.0174729442601347e-08\n",
      "Step: 7010, train/learning_rate: 2.219577982032206e-05\n",
      "Step: 7010, train/epoch: 1.6682531833648682\n",
      "Step: 7020, train/loss: 0.0\n",
      "Step: 7020, train/grad_norm: 6.004951558225002e-08\n",
      "Step: 7020, train/learning_rate: 2.2156116756377742e-05\n",
      "Step: 7020, train/epoch: 1.67063307762146\n",
      "Step: 7030, train/loss: 0.0\n",
      "Step: 7030, train/grad_norm: 4.243079665400273e-09\n",
      "Step: 7030, train/learning_rate: 2.211645187344402e-05\n",
      "Step: 7030, train/epoch: 1.6730128526687622\n",
      "Step: 7040, train/loss: 0.0\n",
      "Step: 7040, train/grad_norm: 3.0933162520341284e-07\n",
      "Step: 7040, train/learning_rate: 2.2076788809499703e-05\n",
      "Step: 7040, train/epoch: 1.6753926277160645\n",
      "Step: 7050, train/loss: 0.0\n",
      "Step: 7050, train/grad_norm: 4.733279865831719e-07\n",
      "Step: 7050, train/learning_rate: 2.2037125745555386e-05\n",
      "Step: 7050, train/epoch: 1.6777725219726562\n",
      "Step: 7060, train/loss: 0.0\n",
      "Step: 7060, train/grad_norm: 1.7546610830976306e-09\n",
      "Step: 7060, train/learning_rate: 2.1997460862621665e-05\n",
      "Step: 7060, train/epoch: 1.6801522970199585\n",
      "Step: 7070, train/loss: 0.0\n",
      "Step: 7070, train/grad_norm: 1.7905741334089953e-09\n",
      "Step: 7070, train/learning_rate: 2.1957797798677348e-05\n",
      "Step: 7070, train/epoch: 1.6825320720672607\n",
      "Step: 7080, train/loss: 0.0003000000142492354\n",
      "Step: 7080, train/grad_norm: 1.8445733829253186e-08\n",
      "Step: 7080, train/learning_rate: 2.191813473473303e-05\n",
      "Step: 7080, train/epoch: 1.6849119663238525\n",
      "Step: 7090, train/loss: 0.0\n",
      "Step: 7090, train/grad_norm: 2.152497984297952e-07\n",
      "Step: 7090, train/learning_rate: 2.187846985179931e-05\n",
      "Step: 7090, train/epoch: 1.6872917413711548\n",
      "Step: 7100, train/loss: 0.0\n",
      "Step: 7100, train/grad_norm: 1.4768598788350573e-08\n",
      "Step: 7100, train/learning_rate: 2.1838806787854992e-05\n",
      "Step: 7100, train/epoch: 1.6896716356277466\n",
      "Step: 7110, train/loss: 0.0\n",
      "Step: 7110, train/grad_norm: 1.7945117178896908e-06\n",
      "Step: 7110, train/learning_rate: 2.1799143723910674e-05\n",
      "Step: 7110, train/epoch: 1.6920514106750488\n",
      "Step: 7120, train/loss: 0.0\n",
      "Step: 7120, train/grad_norm: 7.256356866491842e-07\n",
      "Step: 7120, train/learning_rate: 2.1759478840976954e-05\n",
      "Step: 7120, train/epoch: 1.694431185722351\n",
      "Step: 7130, train/loss: 0.0\n",
      "Step: 7130, train/grad_norm: 5.625403787234973e-07\n",
      "Step: 7130, train/learning_rate: 2.1719815777032636e-05\n",
      "Step: 7130, train/epoch: 1.6968110799789429\n",
      "Step: 7140, train/loss: 0.0\n",
      "Step: 7140, train/grad_norm: 1.211247217725031e-05\n",
      "Step: 7140, train/learning_rate: 2.168015271308832e-05\n",
      "Step: 7140, train/epoch: 1.6991908550262451\n",
      "Step: 7150, train/loss: 0.0\n",
      "Step: 7150, train/grad_norm: 9.83937661658274e-06\n",
      "Step: 7150, train/learning_rate: 2.1640487830154598e-05\n",
      "Step: 7150, train/epoch: 1.7015706300735474\n",
      "Step: 7160, train/loss: 0.04450000077486038\n",
      "Step: 7160, train/grad_norm: 52.924598693847656\n",
      "Step: 7160, train/learning_rate: 2.160082476621028e-05\n",
      "Step: 7160, train/epoch: 1.7039505243301392\n",
      "Step: 7170, train/loss: 0.00019999999494757503\n",
      "Step: 7170, train/grad_norm: 8.606611601180703e-08\n",
      "Step: 7170, train/learning_rate: 2.1561161702265963e-05\n",
      "Step: 7170, train/epoch: 1.7063302993774414\n",
      "Step: 7180, train/loss: 0.0\n",
      "Step: 7180, train/grad_norm: 1.1717335990368838e-09\n",
      "Step: 7180, train/learning_rate: 2.1521496819332242e-05\n",
      "Step: 7180, train/epoch: 1.7087101936340332\n",
      "Step: 7190, train/loss: 0.0\n",
      "Step: 7190, train/grad_norm: 0.00016992371820379049\n",
      "Step: 7190, train/learning_rate: 2.1481833755387925e-05\n",
      "Step: 7190, train/epoch: 1.7110899686813354\n",
      "Step: 7200, train/loss: 0.0\n",
      "Step: 7200, train/grad_norm: 1.7394978613083367e-07\n",
      "Step: 7200, train/learning_rate: 2.1442170691443607e-05\n",
      "Step: 7200, train/epoch: 1.7134697437286377\n",
      "Step: 7210, train/loss: 0.0\n",
      "Step: 7210, train/grad_norm: 1.8433497643854935e-06\n",
      "Step: 7210, train/learning_rate: 2.140250762749929e-05\n",
      "Step: 7210, train/epoch: 1.7158496379852295\n",
      "Step: 7220, train/loss: 0.0\n",
      "Step: 7220, train/grad_norm: 4.3020378370783874e-08\n",
      "Step: 7220, train/learning_rate: 2.136284274456557e-05\n",
      "Step: 7220, train/epoch: 1.7182294130325317\n",
      "Step: 7230, train/loss: 0.0\n",
      "Step: 7230, train/grad_norm: 3.389773439721466e-08\n",
      "Step: 7230, train/learning_rate: 2.132317968062125e-05\n",
      "Step: 7230, train/epoch: 1.720609188079834\n",
      "Step: 7240, train/loss: 0.0\n",
      "Step: 7240, train/grad_norm: 5.842885570928047e-07\n",
      "Step: 7240, train/learning_rate: 2.1283516616676934e-05\n",
      "Step: 7240, train/epoch: 1.7229890823364258\n",
      "Step: 7250, train/loss: 0.30160000920295715\n",
      "Step: 7250, train/grad_norm: 6.3560849916655116e-09\n",
      "Step: 7250, train/learning_rate: 2.1243851733743213e-05\n",
      "Step: 7250, train/epoch: 1.725368857383728\n",
      "Step: 7260, train/loss: 0.0\n",
      "Step: 7260, train/grad_norm: 8.171647891686007e-08\n",
      "Step: 7260, train/learning_rate: 2.1204188669798896e-05\n",
      "Step: 7260, train/epoch: 1.7277486324310303\n",
      "Step: 7270, train/loss: 0.0\n",
      "Step: 7270, train/grad_norm: 4.655681095755426e-06\n",
      "Step: 7270, train/learning_rate: 2.116452560585458e-05\n",
      "Step: 7270, train/epoch: 1.730128526687622\n",
      "Step: 7280, train/loss: 0.07660000026226044\n",
      "Step: 7280, train/grad_norm: 181.140869140625\n",
      "Step: 7280, train/learning_rate: 2.1124860722920857e-05\n",
      "Step: 7280, train/epoch: 1.7325083017349243\n",
      "Step: 7290, train/loss: 0.0\n",
      "Step: 7290, train/grad_norm: 1.935525943963512e-07\n",
      "Step: 7290, train/learning_rate: 2.108519765897654e-05\n",
      "Step: 7290, train/epoch: 1.7348881959915161\n",
      "Step: 7300, train/loss: 0.0\n",
      "Step: 7300, train/grad_norm: 4.072388946951833e-06\n",
      "Step: 7300, train/learning_rate: 2.1045534595032223e-05\n",
      "Step: 7300, train/epoch: 1.7372679710388184\n",
      "Step: 7310, train/loss: 9.999999747378752e-05\n",
      "Step: 7310, train/grad_norm: 0.00017708977975416929\n",
      "Step: 7310, train/learning_rate: 2.10058697120985e-05\n",
      "Step: 7310, train/epoch: 1.7396477460861206\n",
      "Step: 7320, train/loss: 0.07180000096559525\n",
      "Step: 7320, train/grad_norm: 1.0391189420033697e-07\n",
      "Step: 7320, train/learning_rate: 2.0966206648154184e-05\n",
      "Step: 7320, train/epoch: 1.7420276403427124\n",
      "Step: 7330, train/loss: 0.0\n",
      "Step: 7330, train/grad_norm: 3.756614887606702e-06\n",
      "Step: 7330, train/learning_rate: 2.0926543584209867e-05\n",
      "Step: 7330, train/epoch: 1.7444074153900146\n",
      "Step: 7340, train/loss: 0.0\n",
      "Step: 7340, train/grad_norm: 3.078534973610658e-07\n",
      "Step: 7340, train/learning_rate: 2.0886878701276146e-05\n",
      "Step: 7340, train/epoch: 1.746787190437317\n",
      "Step: 7350, train/loss: 0.0\n",
      "Step: 7350, train/grad_norm: 8.167371845502203e-08\n",
      "Step: 7350, train/learning_rate: 2.084721563733183e-05\n",
      "Step: 7350, train/epoch: 1.7491670846939087\n",
      "Step: 7360, train/loss: 0.0\n",
      "Step: 7360, train/grad_norm: 1.7213149476447143e-05\n",
      "Step: 7360, train/learning_rate: 2.080755257338751e-05\n",
      "Step: 7360, train/epoch: 1.751546859741211\n",
      "Step: 7370, train/loss: 0.0\n",
      "Step: 7370, train/grad_norm: 3.352625981278834e-06\n",
      "Step: 7370, train/learning_rate: 2.076788769045379e-05\n",
      "Step: 7370, train/epoch: 1.7539267539978027\n",
      "Step: 7380, train/loss: 0.0\n",
      "Step: 7380, train/grad_norm: 7.4959143603337e-06\n",
      "Step: 7380, train/learning_rate: 2.0728224626509473e-05\n",
      "Step: 7380, train/epoch: 1.756306529045105\n",
      "Step: 7390, train/loss: 0.0\n",
      "Step: 7390, train/grad_norm: 1.5567348299327932e-08\n",
      "Step: 7390, train/learning_rate: 2.0688561562565155e-05\n",
      "Step: 7390, train/epoch: 1.7586863040924072\n",
      "Step: 7400, train/loss: 0.0\n",
      "Step: 7400, train/grad_norm: 1.2854193300881889e-05\n",
      "Step: 7400, train/learning_rate: 2.0648896679631434e-05\n",
      "Step: 7400, train/epoch: 1.761066198348999\n",
      "Step: 7410, train/loss: 0.0\n",
      "Step: 7410, train/grad_norm: 1.3471391866914928e-05\n",
      "Step: 7410, train/learning_rate: 2.0609233615687117e-05\n",
      "Step: 7410, train/epoch: 1.7634459733963013\n",
      "Step: 7420, train/loss: 0.00019999999494757503\n",
      "Step: 7420, train/grad_norm: 6.811960702179931e-07\n",
      "Step: 7420, train/learning_rate: 2.05695705517428e-05\n",
      "Step: 7420, train/epoch: 1.7658257484436035\n",
      "Step: 7430, train/loss: 0.0\n",
      "Step: 7430, train/grad_norm: 2.725026752159465e-05\n",
      "Step: 7430, train/learning_rate: 2.052990566880908e-05\n",
      "Step: 7430, train/epoch: 1.7682056427001953\n",
      "Step: 7440, train/loss: 0.0\n",
      "Step: 7440, train/grad_norm: 3.473887772997841e-07\n",
      "Step: 7440, train/learning_rate: 2.049024260486476e-05\n",
      "Step: 7440, train/epoch: 1.7705854177474976\n",
      "Step: 7450, train/loss: 0.0\n",
      "Step: 7450, train/grad_norm: 0.0015676137991249561\n",
      "Step: 7450, train/learning_rate: 2.0450579540920444e-05\n",
      "Step: 7450, train/epoch: 1.7729653120040894\n",
      "Step: 7460, train/loss: 0.0\n",
      "Step: 7460, train/grad_norm: 6.008293951254018e-08\n",
      "Step: 7460, train/learning_rate: 2.0410914657986723e-05\n",
      "Step: 7460, train/epoch: 1.7753450870513916\n",
      "Step: 7470, train/loss: 0.0\n",
      "Step: 7470, train/grad_norm: 5.786895144410664e-06\n",
      "Step: 7470, train/learning_rate: 2.0371251594042405e-05\n",
      "Step: 7470, train/epoch: 1.7777248620986938\n",
      "Step: 7480, train/loss: 0.0\n",
      "Step: 7480, train/grad_norm: 1.078568402590463e-06\n",
      "Step: 7480, train/learning_rate: 2.0331588530098088e-05\n",
      "Step: 7480, train/epoch: 1.7801047563552856\n",
      "Step: 7490, train/loss: 0.0\n",
      "Step: 7490, train/grad_norm: 7.384405034827068e-05\n",
      "Step: 7490, train/learning_rate: 2.0291923647164367e-05\n",
      "Step: 7490, train/epoch: 1.782484531402588\n",
      "Step: 7500, train/loss: 0.0\n",
      "Step: 7500, train/grad_norm: 1.3174483228794998e-06\n",
      "Step: 7500, train/learning_rate: 2.025226058322005e-05\n",
      "Step: 7500, train/epoch: 1.7848643064498901\n",
      "Step: 7510, train/loss: 0.0828000009059906\n",
      "Step: 7510, train/grad_norm: 41.263816833496094\n",
      "Step: 7510, train/learning_rate: 2.0212597519275732e-05\n",
      "Step: 7510, train/epoch: 1.787244200706482\n",
      "Step: 7520, train/loss: 0.0\n",
      "Step: 7520, train/grad_norm: 7.148631766540348e-07\n",
      "Step: 7520, train/learning_rate: 2.017293263634201e-05\n",
      "Step: 7520, train/epoch: 1.7896239757537842\n",
      "Step: 7530, train/loss: 0.0\n",
      "Step: 7530, train/grad_norm: 8.653964869154152e-06\n",
      "Step: 7530, train/learning_rate: 2.0133269572397694e-05\n",
      "Step: 7530, train/epoch: 1.7920037508010864\n",
      "Step: 7540, train/loss: 0.0\n",
      "Step: 7540, train/grad_norm: 2.0503641007252327e-09\n",
      "Step: 7540, train/learning_rate: 2.0093606508453377e-05\n",
      "Step: 7540, train/epoch: 1.7943836450576782\n",
      "Step: 7550, train/loss: 0.08160000294446945\n",
      "Step: 7550, train/grad_norm: 5.621073341899319e-06\n",
      "Step: 7550, train/learning_rate: 2.005394344450906e-05\n",
      "Step: 7550, train/epoch: 1.7967634201049805\n",
      "Step: 7560, train/loss: 0.0\n",
      "Step: 7560, train/grad_norm: 9.653972732337479e-09\n",
      "Step: 7560, train/learning_rate: 2.0014278561575338e-05\n",
      "Step: 7560, train/epoch: 1.7991433143615723\n",
      "Step: 7570, train/loss: 0.0\n",
      "Step: 7570, train/grad_norm: 4.2455300786059524e-07\n",
      "Step: 7570, train/learning_rate: 1.997461549763102e-05\n",
      "Step: 7570, train/epoch: 1.8015230894088745\n",
      "Step: 7580, train/loss: 9.999999747378752e-05\n",
      "Step: 7580, train/grad_norm: 0.1402873992919922\n",
      "Step: 7580, train/learning_rate: 1.9934952433686703e-05\n",
      "Step: 7580, train/epoch: 1.8039028644561768\n",
      "Step: 7590, train/loss: 0.0\n",
      "Step: 7590, train/grad_norm: 5.301940745994216e-06\n",
      "Step: 7590, train/learning_rate: 1.9895287550752982e-05\n",
      "Step: 7590, train/epoch: 1.8062827587127686\n",
      "Step: 7600, train/loss: 0.012500000186264515\n",
      "Step: 7600, train/grad_norm: 4.1092935276765274e-08\n",
      "Step: 7600, train/learning_rate: 1.9855624486808665e-05\n",
      "Step: 7600, train/epoch: 1.8086625337600708\n",
      "Step: 7610, train/loss: 0.0\n",
      "Step: 7610, train/grad_norm: 1.6636074917641963e-07\n",
      "Step: 7610, train/learning_rate: 1.9815961422864348e-05\n",
      "Step: 7610, train/epoch: 1.811042308807373\n",
      "Step: 7620, train/loss: 0.0\n",
      "Step: 7620, train/grad_norm: 3.696059991398215e-08\n",
      "Step: 7620, train/learning_rate: 1.9776296539930627e-05\n",
      "Step: 7620, train/epoch: 1.8134222030639648\n",
      "Step: 7630, train/loss: 0.0005000000237487257\n",
      "Step: 7630, train/grad_norm: 1.9441602105985112e-08\n",
      "Step: 7630, train/learning_rate: 1.973663347598631e-05\n",
      "Step: 7630, train/epoch: 1.815801978111267\n",
      "Step: 7640, train/loss: 0.12189999967813492\n",
      "Step: 7640, train/grad_norm: 1.6722637496968673e-08\n",
      "Step: 7640, train/learning_rate: 1.9696970412041992e-05\n",
      "Step: 7640, train/epoch: 1.8181818723678589\n",
      "Step: 7650, train/loss: 0.0010000000474974513\n",
      "Step: 7650, train/grad_norm: 578.452392578125\n",
      "Step: 7650, train/learning_rate: 1.965730552910827e-05\n",
      "Step: 7650, train/epoch: 1.8205616474151611\n",
      "Step: 7660, train/loss: 0.0\n",
      "Step: 7660, train/grad_norm: 8.26150608190801e-06\n",
      "Step: 7660, train/learning_rate: 1.9617642465163954e-05\n",
      "Step: 7660, train/epoch: 1.8229414224624634\n",
      "Step: 7670, train/loss: 0.0\n",
      "Step: 7670, train/grad_norm: 9.411572632345155e-10\n",
      "Step: 7670, train/learning_rate: 1.9577979401219636e-05\n",
      "Step: 7670, train/epoch: 1.8253213167190552\n",
      "Step: 7680, train/loss: 0.3702999949455261\n",
      "Step: 7680, train/grad_norm: 39.89574432373047\n",
      "Step: 7680, train/learning_rate: 1.9538314518285915e-05\n",
      "Step: 7680, train/epoch: 1.8277010917663574\n",
      "Step: 7690, train/loss: 0.0\n",
      "Step: 7690, train/grad_norm: 0.0001775791752152145\n",
      "Step: 7690, train/learning_rate: 1.9498651454341598e-05\n",
      "Step: 7690, train/epoch: 1.8300808668136597\n",
      "Step: 7700, train/loss: 0.11169999837875366\n",
      "Step: 7700, train/grad_norm: 0.018474606797099113\n",
      "Step: 7700, train/learning_rate: 1.945898839039728e-05\n",
      "Step: 7700, train/epoch: 1.8324607610702515\n",
      "Step: 7710, train/loss: 9.999999747378752e-05\n",
      "Step: 7710, train/grad_norm: 0.11032405495643616\n",
      "Step: 7710, train/learning_rate: 1.941932350746356e-05\n",
      "Step: 7710, train/epoch: 1.8348405361175537\n",
      "Step: 7720, train/loss: 0.0\n",
      "Step: 7720, train/grad_norm: 2.3689739464316517e-05\n",
      "Step: 7720, train/learning_rate: 1.9379660443519242e-05\n",
      "Step: 7720, train/epoch: 1.8372204303741455\n",
      "Step: 7730, train/loss: 0.0\n",
      "Step: 7730, train/grad_norm: 4.230709600960836e-05\n",
      "Step: 7730, train/learning_rate: 1.9339997379574925e-05\n",
      "Step: 7730, train/epoch: 1.8396002054214478\n",
      "Step: 7740, train/loss: 0.0\n",
      "Step: 7740, train/grad_norm: 0.0003731542674358934\n",
      "Step: 7740, train/learning_rate: 1.9300332496641204e-05\n",
      "Step: 7740, train/epoch: 1.84197998046875\n",
      "Step: 7750, train/loss: 0.0\n",
      "Step: 7750, train/grad_norm: 3.504149935906753e-05\n",
      "Step: 7750, train/learning_rate: 1.9260669432696886e-05\n",
      "Step: 7750, train/epoch: 1.8443598747253418\n",
      "Step: 7760, train/loss: 0.0\n",
      "Step: 7760, train/grad_norm: 9.024206519825384e-05\n",
      "Step: 7760, train/learning_rate: 1.922100636875257e-05\n",
      "Step: 7760, train/epoch: 1.846739649772644\n",
      "Step: 7770, train/loss: 0.0\n",
      "Step: 7770, train/grad_norm: 6.74737457302399e-05\n",
      "Step: 7770, train/learning_rate: 1.9181341485818848e-05\n",
      "Step: 7770, train/epoch: 1.8491194248199463\n",
      "Step: 7780, train/loss: 0.0\n",
      "Step: 7780, train/grad_norm: 8.645517482364085e-06\n",
      "Step: 7780, train/learning_rate: 1.914167842187453e-05\n",
      "Step: 7780, train/epoch: 1.851499319076538\n",
      "Step: 7790, train/loss: 0.0\n",
      "Step: 7790, train/grad_norm: 3.0122295356704853e-05\n",
      "Step: 7790, train/learning_rate: 1.9102015357930213e-05\n",
      "Step: 7790, train/epoch: 1.8538790941238403\n",
      "Step: 7800, train/loss: 0.0\n",
      "Step: 7800, train/grad_norm: 7.596353498229291e-06\n",
      "Step: 7800, train/learning_rate: 1.9062350474996492e-05\n",
      "Step: 7800, train/epoch: 1.8562588691711426\n",
      "Step: 7810, train/loss: 0.0\n",
      "Step: 7810, train/grad_norm: 0.0001566413848195225\n",
      "Step: 7810, train/learning_rate: 1.9022687411052175e-05\n",
      "Step: 7810, train/epoch: 1.8586387634277344\n",
      "Step: 7820, train/loss: 0.014800000004470348\n",
      "Step: 7820, train/grad_norm: 2.9778235330013558e-05\n",
      "Step: 7820, train/learning_rate: 1.8983024347107857e-05\n",
      "Step: 7820, train/epoch: 1.8610185384750366\n",
      "Step: 7830, train/loss: 0.0\n",
      "Step: 7830, train/grad_norm: 5.906189812776574e-07\n",
      "Step: 7830, train/learning_rate: 1.8943359464174137e-05\n",
      "Step: 7830, train/epoch: 1.8633984327316284\n",
      "Step: 7840, train/loss: 0.0\n",
      "Step: 7840, train/grad_norm: 1.2313580555201042e-07\n",
      "Step: 7840, train/learning_rate: 1.890369640022982e-05\n",
      "Step: 7840, train/epoch: 1.8657782077789307\n",
      "Step: 7850, train/loss: 0.0\n",
      "Step: 7850, train/grad_norm: 1.6795840451777622e-07\n",
      "Step: 7850, train/learning_rate: 1.8864033336285502e-05\n",
      "Step: 7850, train/epoch: 1.868157982826233\n",
      "Step: 7860, train/loss: 0.003700000001117587\n",
      "Step: 7860, train/grad_norm: 7.086757847218905e-08\n",
      "Step: 7860, train/learning_rate: 1.882436845335178e-05\n",
      "Step: 7860, train/epoch: 1.8705378770828247\n",
      "Step: 7870, train/loss: 0.0\n",
      "Step: 7870, train/grad_norm: 2.4219548322435003e-06\n",
      "Step: 7870, train/learning_rate: 1.8784705389407463e-05\n",
      "Step: 7870, train/epoch: 1.872917652130127\n",
      "Step: 7880, train/loss: 0.0\n",
      "Step: 7880, train/grad_norm: 3.3790868201322155e-06\n",
      "Step: 7880, train/learning_rate: 1.8745042325463146e-05\n",
      "Step: 7880, train/epoch: 1.8752974271774292\n",
      "Step: 7890, train/loss: 0.0\n",
      "Step: 7890, train/grad_norm: 4.62407751911087e-06\n",
      "Step: 7890, train/learning_rate: 1.870537926151883e-05\n",
      "Step: 7890, train/epoch: 1.877677321434021\n",
      "Step: 7900, train/loss: 0.0\n",
      "Step: 7900, train/grad_norm: 3.752388408884144e-07\n",
      "Step: 7900, train/learning_rate: 1.8665714378585108e-05\n",
      "Step: 7900, train/epoch: 1.8800570964813232\n",
      "Step: 7910, train/loss: 0.0\n",
      "Step: 7910, train/grad_norm: 1.8199569922217051e-06\n",
      "Step: 7910, train/learning_rate: 1.862605131464079e-05\n",
      "Step: 7910, train/epoch: 1.882436990737915\n",
      "Step: 7920, train/loss: 0.0\n",
      "Step: 7920, train/grad_norm: 4.4791181608161423e-07\n",
      "Step: 7920, train/learning_rate: 1.8586388250696473e-05\n",
      "Step: 7920, train/epoch: 1.8848167657852173\n",
      "Step: 7930, train/loss: 0.06480000168085098\n",
      "Step: 7930, train/grad_norm: 3.525098009049543e-06\n",
      "Step: 7930, train/learning_rate: 1.8546723367762752e-05\n",
      "Step: 7930, train/epoch: 1.8871965408325195\n",
      "Step: 7940, train/loss: 0.0\n",
      "Step: 7940, train/grad_norm: 1.664148356894657e-07\n",
      "Step: 7940, train/learning_rate: 1.8507060303818434e-05\n",
      "Step: 7940, train/epoch: 1.8895764350891113\n",
      "Step: 7950, train/loss: 0.00019999999494757503\n",
      "Step: 7950, train/grad_norm: 9.393105574417859e-05\n",
      "Step: 7950, train/learning_rate: 1.8467397239874117e-05\n",
      "Step: 7950, train/epoch: 1.8919562101364136\n",
      "Step: 7960, train/loss: 0.0\n",
      "Step: 7960, train/grad_norm: 4.211549094179645e-05\n",
      "Step: 7960, train/learning_rate: 1.8427732356940396e-05\n",
      "Step: 7960, train/epoch: 1.8943359851837158\n",
      "Step: 7970, train/loss: 0.0\n",
      "Step: 7970, train/grad_norm: 4.4759177399100736e-05\n",
      "Step: 7970, train/learning_rate: 1.838806929299608e-05\n",
      "Step: 7970, train/epoch: 1.8967158794403076\n",
      "Step: 7980, train/loss: 0.0\n",
      "Step: 7980, train/grad_norm: 0.0034719433169811964\n",
      "Step: 7980, train/learning_rate: 1.834840622905176e-05\n",
      "Step: 7980, train/epoch: 1.8990956544876099\n",
      "Step: 7990, train/loss: 0.0\n",
      "Step: 7990, train/grad_norm: 0.00025418042787350714\n",
      "Step: 7990, train/learning_rate: 1.830874134611804e-05\n",
      "Step: 7990, train/epoch: 1.901475429534912\n",
      "Step: 8000, train/loss: 0.0\n",
      "Step: 8000, train/grad_norm: 0.0014713361160829663\n",
      "Step: 8000, train/learning_rate: 1.8269078282173723e-05\n",
      "Step: 8000, train/epoch: 1.903855323791504\n",
      "Step: 8010, train/loss: 0.0\n",
      "Step: 8010, train/grad_norm: 5.606191734841559e-06\n",
      "Step: 8010, train/learning_rate: 1.8229415218229406e-05\n",
      "Step: 8010, train/epoch: 1.9062350988388062\n",
      "Step: 8020, train/loss: 0.0\n",
      "Step: 8020, train/grad_norm: 9.725241625346825e-07\n",
      "Step: 8020, train/learning_rate: 1.8189750335295685e-05\n",
      "Step: 8020, train/epoch: 1.908614993095398\n",
      "Step: 8030, train/loss: 0.0\n",
      "Step: 8030, train/grad_norm: 1.6053874787758105e-05\n",
      "Step: 8030, train/learning_rate: 1.8150087271351367e-05\n",
      "Step: 8030, train/epoch: 1.9109947681427002\n",
      "Step: 8040, train/loss: 0.0\n",
      "Step: 8040, train/grad_norm: 0.00038450834108516574\n",
      "Step: 8040, train/learning_rate: 1.811042420740705e-05\n",
      "Step: 8040, train/epoch: 1.9133745431900024\n",
      "Step: 8050, train/loss: 0.0\n",
      "Step: 8050, train/grad_norm: 0.0001398155145579949\n",
      "Step: 8050, train/learning_rate: 1.807075932447333e-05\n",
      "Step: 8050, train/epoch: 1.9157544374465942\n",
      "Step: 8060, train/loss: 0.0\n",
      "Step: 8060, train/grad_norm: 3.58516481355764e-05\n",
      "Step: 8060, train/learning_rate: 1.803109626052901e-05\n",
      "Step: 8060, train/epoch: 1.9181342124938965\n",
      "Step: 8070, train/loss: 0.0\n",
      "Step: 8070, train/grad_norm: 8.500258263666183e-05\n",
      "Step: 8070, train/learning_rate: 1.7991433196584694e-05\n",
      "Step: 8070, train/epoch: 1.9205139875411987\n",
      "Step: 8080, train/loss: 0.0\n",
      "Step: 8080, train/grad_norm: 2.5859892048174515e-05\n",
      "Step: 8080, train/learning_rate: 1.7951768313650973e-05\n",
      "Step: 8080, train/epoch: 1.9228938817977905\n",
      "Step: 8090, train/loss: 0.0\n",
      "Step: 8090, train/grad_norm: 2.7038583993999055e-07\n",
      "Step: 8090, train/learning_rate: 1.7912105249706656e-05\n",
      "Step: 8090, train/epoch: 1.9252736568450928\n",
      "Step: 8100, train/loss: 0.0\n",
      "Step: 8100, train/grad_norm: 3.255211777286604e-05\n",
      "Step: 8100, train/learning_rate: 1.787244218576234e-05\n",
      "Step: 8100, train/epoch: 1.9276535511016846\n",
      "Step: 8110, train/loss: 0.0\n",
      "Step: 8110, train/grad_norm: 3.2859708881005645e-05\n",
      "Step: 8110, train/learning_rate: 1.7832777302828617e-05\n",
      "Step: 8110, train/epoch: 1.9300333261489868\n",
      "Step: 8120, train/loss: 0.0\n",
      "Step: 8120, train/grad_norm: 4.6852392188156955e-06\n",
      "Step: 8120, train/learning_rate: 1.77931142388843e-05\n",
      "Step: 8120, train/epoch: 1.932413101196289\n",
      "Step: 8130, train/loss: 0.0\n",
      "Step: 8130, train/grad_norm: 6.45454420009628e-05\n",
      "Step: 8130, train/learning_rate: 1.7753451174939983e-05\n",
      "Step: 8130, train/epoch: 1.9347929954528809\n",
      "Step: 8140, train/loss: 0.0\n",
      "Step: 8140, train/grad_norm: 6.495561137853656e-06\n",
      "Step: 8140, train/learning_rate: 1.771378629200626e-05\n",
      "Step: 8140, train/epoch: 1.937172770500183\n",
      "Step: 8150, train/loss: 0.0\n",
      "Step: 8150, train/grad_norm: 4.565614290186204e-05\n",
      "Step: 8150, train/learning_rate: 1.7674123228061944e-05\n",
      "Step: 8150, train/epoch: 1.9395525455474854\n",
      "Step: 8160, train/loss: 0.0\n",
      "Step: 8160, train/grad_norm: 1.4191565242072102e-05\n",
      "Step: 8160, train/learning_rate: 1.7634460164117627e-05\n",
      "Step: 8160, train/epoch: 1.9419324398040771\n",
      "Step: 8170, train/loss: 0.00039999998989515007\n",
      "Step: 8170, train/grad_norm: 9.260209480999038e-06\n",
      "Step: 8170, train/learning_rate: 1.7594795281183906e-05\n",
      "Step: 8170, train/epoch: 1.9443122148513794\n",
      "Step: 8180, train/loss: 0.0\n",
      "Step: 8180, train/grad_norm: 1.3151659459254006e-06\n",
      "Step: 8180, train/learning_rate: 1.755513221723959e-05\n",
      "Step: 8180, train/epoch: 1.9466921091079712\n",
      "Step: 8190, train/loss: 0.0\n",
      "Step: 8190, train/grad_norm: 0.0007004595245234668\n",
      "Step: 8190, train/learning_rate: 1.751546915329527e-05\n",
      "Step: 8190, train/epoch: 1.9490718841552734\n",
      "Step: 8200, train/loss: 0.06480000168085098\n",
      "Step: 8200, train/grad_norm: 3.927802794123636e-08\n",
      "Step: 8200, train/learning_rate: 1.747580427036155e-05\n",
      "Step: 8200, train/epoch: 1.9514516592025757\n",
      "Step: 8210, train/loss: 0.13130000233650208\n",
      "Step: 8210, train/grad_norm: 5.744373083871324e-06\n",
      "Step: 8210, train/learning_rate: 1.7436141206417233e-05\n",
      "Step: 8210, train/epoch: 1.9538315534591675\n",
      "Step: 8220, train/loss: 0.0\n",
      "Step: 8220, train/grad_norm: 0.001212593400850892\n",
      "Step: 8220, train/learning_rate: 1.7396478142472915e-05\n",
      "Step: 8220, train/epoch: 1.9562113285064697\n",
      "Step: 8230, train/loss: 0.0\n",
      "Step: 8230, train/grad_norm: 3.47889399563428e-05\n",
      "Step: 8230, train/learning_rate: 1.7356815078528598e-05\n",
      "Step: 8230, train/epoch: 1.958591103553772\n",
      "Step: 8240, train/loss: 0.0\n",
      "Step: 8240, train/grad_norm: 1.0397592632216401e-05\n",
      "Step: 8240, train/learning_rate: 1.7317150195594877e-05\n",
      "Step: 8240, train/epoch: 1.9609709978103638\n",
      "Step: 8250, train/loss: 0.0\n",
      "Step: 8250, train/grad_norm: 2.3188028990261955e-06\n",
      "Step: 8250, train/learning_rate: 1.727748713165056e-05\n",
      "Step: 8250, train/epoch: 1.963350772857666\n",
      "Step: 8260, train/loss: 0.0\n",
      "Step: 8260, train/grad_norm: 8.965178858488798e-05\n",
      "Step: 8260, train/learning_rate: 1.7237824067706242e-05\n",
      "Step: 8260, train/epoch: 1.9657305479049683\n",
      "Step: 8270, train/loss: 0.0\n",
      "Step: 8270, train/grad_norm: 4.6052181801314873e-07\n",
      "Step: 8270, train/learning_rate: 1.719815918477252e-05\n",
      "Step: 8270, train/epoch: 1.96811044216156\n",
      "Step: 8280, train/loss: 0.0\n",
      "Step: 8280, train/grad_norm: 2.2209437702258583e-06\n",
      "Step: 8280, train/learning_rate: 1.7158496120828204e-05\n",
      "Step: 8280, train/epoch: 1.9704902172088623\n",
      "Step: 8290, train/loss: 0.0\n",
      "Step: 8290, train/grad_norm: 2.3009195501799695e-05\n",
      "Step: 8290, train/learning_rate: 1.7118833056883886e-05\n",
      "Step: 8290, train/epoch: 1.972870111465454\n",
      "Step: 8300, train/loss: 0.0\n",
      "Step: 8300, train/grad_norm: 1.55673873791784e-07\n",
      "Step: 8300, train/learning_rate: 1.7079168173950166e-05\n",
      "Step: 8300, train/epoch: 1.9752498865127563\n",
      "Step: 8310, train/loss: 0.0\n",
      "Step: 8310, train/grad_norm: 7.624308636877686e-06\n",
      "Step: 8310, train/learning_rate: 1.7039505110005848e-05\n",
      "Step: 8310, train/epoch: 1.9776296615600586\n",
      "Step: 8320, train/loss: 0.0\n",
      "Step: 8320, train/grad_norm: 3.6779832157662895e-07\n",
      "Step: 8320, train/learning_rate: 1.699984204606153e-05\n",
      "Step: 8320, train/epoch: 1.9800095558166504\n",
      "Step: 8330, train/loss: 0.0\n",
      "Step: 8330, train/grad_norm: 0.0006903172470629215\n",
      "Step: 8330, train/learning_rate: 1.696017716312781e-05\n",
      "Step: 8330, train/epoch: 1.9823893308639526\n",
      "Step: 8340, train/loss: 0.0\n",
      "Step: 8340, train/grad_norm: 2.1568587271758588e-07\n",
      "Step: 8340, train/learning_rate: 1.6920514099183492e-05\n",
      "Step: 8340, train/epoch: 1.9847691059112549\n",
      "Step: 8350, train/loss: 0.0\n",
      "Step: 8350, train/grad_norm: 9.750766594152083e-07\n",
      "Step: 8350, train/learning_rate: 1.6880851035239175e-05\n",
      "Step: 8350, train/epoch: 1.9871490001678467\n",
      "Step: 8360, train/loss: 0.0\n",
      "Step: 8360, train/grad_norm: 4.1111320570053067e-07\n",
      "Step: 8360, train/learning_rate: 1.6841186152305454e-05\n",
      "Step: 8360, train/epoch: 1.989528775215149\n",
      "Step: 8370, train/loss: 0.0\n",
      "Step: 8370, train/grad_norm: 8.584042006987147e-06\n",
      "Step: 8370, train/learning_rate: 1.6801523088361137e-05\n",
      "Step: 8370, train/epoch: 1.9919086694717407\n",
      "Step: 8380, train/loss: 0.0\n",
      "Step: 8380, train/grad_norm: 4.602038461598568e-05\n",
      "Step: 8380, train/learning_rate: 1.676186002441682e-05\n",
      "Step: 8380, train/epoch: 1.994288444519043\n",
      "Step: 8390, train/loss: 0.0\n",
      "Step: 8390, train/grad_norm: 8.405744011952265e-08\n",
      "Step: 8390, train/learning_rate: 1.6722195141483098e-05\n",
      "Step: 8390, train/epoch: 1.9966682195663452\n",
      "Step: 8400, train/loss: 0.0\n",
      "Step: 8400, train/grad_norm: 5.726852325693699e-10\n",
      "Step: 8400, train/learning_rate: 1.668253207753878e-05\n",
      "Step: 8400, train/epoch: 1.999048113822937\n",
      "Step: 8404, eval/loss: 0.0038453158922493458\n",
      "Step: 8404, eval/accuracy: 0.9995834827423096\n",
      "Step: 8404, eval/f1: 0.9995601177215576\n",
      "Step: 8404, eval/runtime: 7990.6611328125\n",
      "Step: 8404, eval/samples_per_second: 0.9010000228881836\n",
      "Step: 8404, eval/steps_per_second: 0.11299999803304672\n",
      "Step: 8404, train/epoch: 2.0\n",
      "Step: 8410, train/loss: 0.0\n",
      "Step: 8410, train/grad_norm: 6.6468186510348914e-09\n",
      "Step: 8410, train/learning_rate: 1.6642869013594463e-05\n",
      "Step: 8410, train/epoch: 2.0014278888702393\n",
      "Step: 8420, train/loss: 0.0\n",
      "Step: 8420, train/grad_norm: 5.904029194425675e-07\n",
      "Step: 8420, train/learning_rate: 1.6603204130660743e-05\n",
      "Step: 8420, train/epoch: 2.003807783126831\n",
      "Step: 8430, train/loss: 0.0\n",
      "Step: 8430, train/grad_norm: 1.6074307040980784e-06\n",
      "Step: 8430, train/learning_rate: 1.6563541066716425e-05\n",
      "Step: 8430, train/epoch: 2.0061874389648438\n",
      "Step: 8440, train/loss: 0.0\n",
      "Step: 8440, train/grad_norm: 4.800179340236355e-06\n",
      "Step: 8440, train/learning_rate: 1.6523878002772108e-05\n",
      "Step: 8440, train/epoch: 2.0085673332214355\n",
      "Step: 8450, train/loss: 0.0\n",
      "Step: 8450, train/grad_norm: 2.5588568064449646e-07\n",
      "Step: 8450, train/learning_rate: 1.6484213119838387e-05\n",
      "Step: 8450, train/epoch: 2.0109472274780273\n",
      "Step: 8460, train/loss: 0.0\n",
      "Step: 8460, train/grad_norm: 6.344654224221813e-08\n",
      "Step: 8460, train/learning_rate: 1.644455005589407e-05\n",
      "Step: 8460, train/epoch: 2.01332688331604\n",
      "Step: 8470, train/loss: 0.0\n",
      "Step: 8470, train/grad_norm: 1.3105039897709503e-07\n",
      "Step: 8470, train/learning_rate: 1.6404886991949752e-05\n",
      "Step: 8470, train/epoch: 2.015706777572632\n",
      "Step: 8480, train/loss: 0.0\n",
      "Step: 8480, train/grad_norm: 8.410524969804101e-07\n",
      "Step: 8480, train/learning_rate: 1.636522210901603e-05\n",
      "Step: 8480, train/epoch: 2.0180866718292236\n",
      "Step: 8490, train/loss: 0.0\n",
      "Step: 8490, train/grad_norm: 8.857202374201734e-06\n",
      "Step: 8490, train/learning_rate: 1.6325559045071714e-05\n",
      "Step: 8490, train/epoch: 2.0204663276672363\n",
      "Step: 8500, train/loss: 0.0\n",
      "Step: 8500, train/grad_norm: 4.3803694893540523e-08\n",
      "Step: 8500, train/learning_rate: 1.6285895981127396e-05\n",
      "Step: 8500, train/epoch: 2.022846221923828\n",
      "Step: 8510, train/loss: 0.0\n",
      "Step: 8510, train/grad_norm: 3.324205977150996e-08\n",
      "Step: 8510, train/learning_rate: 1.6246231098193675e-05\n",
      "Step: 8510, train/epoch: 2.02522611618042\n",
      "Step: 8520, train/loss: 0.0\n",
      "Step: 8520, train/grad_norm: 8.6637349738794e-08\n",
      "Step: 8520, train/learning_rate: 1.6206568034249358e-05\n",
      "Step: 8520, train/epoch: 2.0276060104370117\n",
      "Step: 8530, train/loss: 0.0\n",
      "Step: 8530, train/grad_norm: 2.3769706558596226e-07\n",
      "Step: 8530, train/learning_rate: 1.616690497030504e-05\n",
      "Step: 8530, train/epoch: 2.0299856662750244\n",
      "Step: 8540, train/loss: 0.0\n",
      "Step: 8540, train/grad_norm: 3.9168662624433637e-07\n",
      "Step: 8540, train/learning_rate: 1.612724008737132e-05\n",
      "Step: 8540, train/epoch: 2.032365560531616\n",
      "Step: 8550, train/loss: 0.0\n",
      "Step: 8550, train/grad_norm: 4.6497181216409444e-08\n",
      "Step: 8550, train/learning_rate: 1.6087577023427002e-05\n",
      "Step: 8550, train/epoch: 2.034745454788208\n",
      "Step: 8560, train/loss: 0.0\n",
      "Step: 8560, train/grad_norm: 1.0728447463748125e-08\n",
      "Step: 8560, train/learning_rate: 1.6047913959482685e-05\n",
      "Step: 8560, train/epoch: 2.0371251106262207\n",
      "Step: 8570, train/loss: 0.0\n",
      "Step: 8570, train/grad_norm: 1.0333194495615317e-06\n",
      "Step: 8570, train/learning_rate: 1.6008250895538367e-05\n",
      "Step: 8570, train/epoch: 2.0395050048828125\n",
      "Step: 8580, train/loss: 0.0\n",
      "Step: 8580, train/grad_norm: 6.747817451469018e-07\n",
      "Step: 8580, train/learning_rate: 1.5968586012604646e-05\n",
      "Step: 8580, train/epoch: 2.0418848991394043\n",
      "Step: 8590, train/loss: 0.0\n",
      "Step: 8590, train/grad_norm: 1.1532331001262719e-07\n",
      "Step: 8590, train/learning_rate: 1.592892294866033e-05\n",
      "Step: 8590, train/epoch: 2.044264554977417\n",
      "Step: 8600, train/loss: 0.0\n",
      "Step: 8600, train/grad_norm: 3.825985004368704e-07\n",
      "Step: 8600, train/learning_rate: 1.588925988471601e-05\n",
      "Step: 8600, train/epoch: 2.046644449234009\n",
      "Step: 8610, train/loss: 0.0\n",
      "Step: 8610, train/grad_norm: 3.3623933859416866e-08\n",
      "Step: 8610, train/learning_rate: 1.584959500178229e-05\n",
      "Step: 8610, train/epoch: 2.0490243434906006\n",
      "Step: 8620, train/loss: 0.0020000000949949026\n",
      "Step: 8620, train/grad_norm: 7.920720008769422e-07\n",
      "Step: 8620, train/learning_rate: 1.5809931937837973e-05\n",
      "Step: 8620, train/epoch: 2.0514039993286133\n",
      "Step: 8630, train/loss: 0.0\n",
      "Step: 8630, train/grad_norm: 0.0014508559834212065\n",
      "Step: 8630, train/learning_rate: 1.5770268873893656e-05\n",
      "Step: 8630, train/epoch: 2.053783893585205\n",
      "Step: 8640, train/loss: 0.0\n",
      "Step: 8640, train/grad_norm: 3.725079977812129e-07\n",
      "Step: 8640, train/learning_rate: 1.5730603990959935e-05\n",
      "Step: 8640, train/epoch: 2.056163787841797\n",
      "Step: 8650, train/loss: 0.0\n",
      "Step: 8650, train/grad_norm: 5.712992106055026e-07\n",
      "Step: 8650, train/learning_rate: 1.5690940927015617e-05\n",
      "Step: 8650, train/epoch: 2.0585434436798096\n",
      "Step: 8660, train/loss: 0.0\n",
      "Step: 8660, train/grad_norm: 4.0810920154399355e-07\n",
      "Step: 8660, train/learning_rate: 1.56512778630713e-05\n",
      "Step: 8660, train/epoch: 2.0609233379364014\n",
      "Step: 8670, train/loss: 0.0\n",
      "Step: 8670, train/grad_norm: 4.4770725793341626e-08\n",
      "Step: 8670, train/learning_rate: 1.561161298013758e-05\n",
      "Step: 8670, train/epoch: 2.063303232192993\n",
      "Step: 8680, train/loss: 0.0\n",
      "Step: 8680, train/grad_norm: 5.06534661326441e-06\n",
      "Step: 8680, train/learning_rate: 1.5571949916193262e-05\n",
      "Step: 8680, train/epoch: 2.065683126449585\n",
      "Step: 8690, train/loss: 0.0\n",
      "Step: 8690, train/grad_norm: 5.925532241235487e-05\n",
      "Step: 8690, train/learning_rate: 1.5532286852248944e-05\n",
      "Step: 8690, train/epoch: 2.0680627822875977\n",
      "Step: 8700, train/loss: 0.0\n",
      "Step: 8700, train/grad_norm: 1.0007122455135686e-06\n",
      "Step: 8700, train/learning_rate: 1.5492621969315223e-05\n",
      "Step: 8700, train/epoch: 2.0704426765441895\n",
      "Step: 8710, train/loss: 0.0\n",
      "Step: 8710, train/grad_norm: 6.5738788634917e-08\n",
      "Step: 8710, train/learning_rate: 1.5452958905370906e-05\n",
      "Step: 8710, train/epoch: 2.0728225708007812\n",
      "Step: 8720, train/loss: 0.0\n",
      "Step: 8720, train/grad_norm: 8.60463629237529e-08\n",
      "Step: 8720, train/learning_rate: 1.541329584142659e-05\n",
      "Step: 8720, train/epoch: 2.075202226638794\n",
      "Step: 8730, train/loss: 0.0\n",
      "Step: 8730, train/grad_norm: 9.380063836772479e-09\n",
      "Step: 8730, train/learning_rate: 1.5373630958492868e-05\n",
      "Step: 8730, train/epoch: 2.0775821208953857\n",
      "Step: 8740, train/loss: 0.0\n",
      "Step: 8740, train/grad_norm: 6.636986995545158e-07\n",
      "Step: 8740, train/learning_rate: 1.533396789454855e-05\n",
      "Step: 8740, train/epoch: 2.0799620151519775\n",
      "Step: 8750, train/loss: 0.0\n",
      "Step: 8750, train/grad_norm: 3.0893284019839484e-07\n",
      "Step: 8750, train/learning_rate: 1.5294304830604233e-05\n",
      "Step: 8750, train/epoch: 2.0823416709899902\n",
      "Step: 8760, train/loss: 0.0\n",
      "Step: 8760, train/grad_norm: 1.5529569282080047e-05\n",
      "Step: 8760, train/learning_rate: 1.5254640857165214e-05\n",
      "Step: 8760, train/epoch: 2.084721565246582\n",
      "Step: 8770, train/loss: 0.0\n",
      "Step: 8770, train/grad_norm: 1.0586407483970106e-07\n",
      "Step: 8770, train/learning_rate: 1.5214976883726195e-05\n",
      "Step: 8770, train/epoch: 2.087101459503174\n",
      "Step: 8780, train/loss: 0.0\n",
      "Step: 8780, train/grad_norm: 6.243364225611003e-08\n",
      "Step: 8780, train/learning_rate: 1.5175312910287175e-05\n",
      "Step: 8780, train/epoch: 2.0894811153411865\n",
      "Step: 8790, train/loss: 0.0\n",
      "Step: 8790, train/grad_norm: 9.871941131223139e-08\n",
      "Step: 8790, train/learning_rate: 1.5135649846342858e-05\n",
      "Step: 8790, train/epoch: 2.0918610095977783\n",
      "Step: 8800, train/loss: 0.0\n",
      "Step: 8800, train/grad_norm: 9.677643930672275e-08\n",
      "Step: 8800, train/learning_rate: 1.5095985872903839e-05\n",
      "Step: 8800, train/epoch: 2.09424090385437\n",
      "Step: 8810, train/loss: 0.0\n",
      "Step: 8810, train/grad_norm: 4.5102906369720586e-06\n",
      "Step: 8810, train/learning_rate: 1.5056322808959521e-05\n",
      "Step: 8810, train/epoch: 2.096620559692383\n",
      "Step: 8820, train/loss: 9.999999747378752e-05\n",
      "Step: 8820, train/grad_norm: 7.835222248786522e-08\n",
      "Step: 8820, train/learning_rate: 1.5016658835520502e-05\n",
      "Step: 8820, train/epoch: 2.0990004539489746\n",
      "Step: 8830, train/loss: 0.0\n",
      "Step: 8830, train/grad_norm: 1.0427918795130608e-07\n",
      "Step: 8830, train/learning_rate: 1.4976994862081483e-05\n",
      "Step: 8830, train/epoch: 2.1013803482055664\n",
      "Step: 8840, train/loss: 0.0\n",
      "Step: 8840, train/grad_norm: 9.84856782793031e-08\n",
      "Step: 8840, train/learning_rate: 1.4937331798137166e-05\n",
      "Step: 8840, train/epoch: 2.103760004043579\n",
      "Step: 8850, train/loss: 0.0\n",
      "Step: 8850, train/grad_norm: 9.914163712210211e-09\n",
      "Step: 8850, train/learning_rate: 1.4897667824698146e-05\n",
      "Step: 8850, train/epoch: 2.106139898300171\n",
      "Step: 8860, train/loss: 0.0\n",
      "Step: 8860, train/grad_norm: 9.88638504395567e-10\n",
      "Step: 8860, train/learning_rate: 1.4858003851259127e-05\n",
      "Step: 8860, train/epoch: 2.1085197925567627\n",
      "Step: 8870, train/loss: 0.0\n",
      "Step: 8870, train/grad_norm: 2.027166656759505e-09\n",
      "Step: 8870, train/learning_rate: 1.481834078731481e-05\n",
      "Step: 8870, train/epoch: 2.1108996868133545\n",
      "Step: 8880, train/loss: 0.0\n",
      "Step: 8880, train/grad_norm: 1.1012805778420898e-08\n",
      "Step: 8880, train/learning_rate: 1.477867681387579e-05\n",
      "Step: 8880, train/epoch: 2.113279342651367\n",
      "Step: 8890, train/loss: 0.0\n",
      "Step: 8890, train/grad_norm: 1.0455820209642752e-08\n",
      "Step: 8890, train/learning_rate: 1.4739012840436772e-05\n",
      "Step: 8890, train/epoch: 2.115659236907959\n",
      "Step: 8900, train/loss: 0.0\n",
      "Step: 8900, train/grad_norm: 1.6595886620507372e-07\n",
      "Step: 8900, train/learning_rate: 1.4699349776492454e-05\n",
      "Step: 8900, train/epoch: 2.118039131164551\n",
      "Step: 8910, train/loss: 0.0\n",
      "Step: 8910, train/grad_norm: 1.1352276452214483e-08\n",
      "Step: 8910, train/learning_rate: 1.4659685803053435e-05\n",
      "Step: 8910, train/epoch: 2.1204187870025635\n",
      "Step: 8920, train/loss: 0.0\n",
      "Step: 8920, train/grad_norm: 2.937668552860373e-09\n",
      "Step: 8920, train/learning_rate: 1.4620021829614416e-05\n",
      "Step: 8920, train/epoch: 2.1227986812591553\n",
      "Step: 8930, train/loss: 0.0\n",
      "Step: 8930, train/grad_norm: 1.830717053508124e-07\n",
      "Step: 8930, train/learning_rate: 1.4580358765670098e-05\n",
      "Step: 8930, train/epoch: 2.125178575515747\n",
      "Step: 8940, train/loss: 0.0\n",
      "Step: 8940, train/grad_norm: 2.2069292526794015e-07\n",
      "Step: 8940, train/learning_rate: 1.454069479223108e-05\n",
      "Step: 8940, train/epoch: 2.1275582313537598\n",
      "Step: 8950, train/loss: 0.0\n",
      "Step: 8950, train/grad_norm: 7.956032987976869e-08\n",
      "Step: 8950, train/learning_rate: 1.450103081879206e-05\n",
      "Step: 8950, train/epoch: 2.1299381256103516\n",
      "Step: 8960, train/loss: 0.0\n",
      "Step: 8960, train/grad_norm: 3.969780593138239e-08\n",
      "Step: 8960, train/learning_rate: 1.4461367754847743e-05\n",
      "Step: 8960, train/epoch: 2.1323180198669434\n",
      "Step: 8970, train/loss: 0.0\n",
      "Step: 8970, train/grad_norm: 1.7184139267101273e-07\n",
      "Step: 8970, train/learning_rate: 1.4421703781408723e-05\n",
      "Step: 8970, train/epoch: 2.134697675704956\n",
      "Step: 8980, train/loss: 0.0\n",
      "Step: 8980, train/grad_norm: 4.586526358707488e-08\n",
      "Step: 8980, train/learning_rate: 1.4382040717464406e-05\n",
      "Step: 8980, train/epoch: 2.137077569961548\n",
      "Step: 8990, train/loss: 0.0\n",
      "Step: 8990, train/grad_norm: 1.0863464572352655e-09\n",
      "Step: 8990, train/learning_rate: 1.4342376744025387e-05\n",
      "Step: 8990, train/epoch: 2.1394574642181396\n",
      "Step: 9000, train/loss: 0.0\n",
      "Step: 9000, train/grad_norm: 1.409399175145154e-07\n",
      "Step: 9000, train/learning_rate: 1.4302712770586368e-05\n",
      "Step: 9000, train/epoch: 2.1418371200561523\n",
      "Step: 9010, train/loss: 0.0\n",
      "Step: 9010, train/grad_norm: 3.0592950395202934e-09\n",
      "Step: 9010, train/learning_rate: 1.426304970664205e-05\n",
      "Step: 9010, train/epoch: 2.144217014312744\n",
      "Step: 9020, train/loss: 0.0\n",
      "Step: 9020, train/grad_norm: 3.282381658209488e-05\n",
      "Step: 9020, train/learning_rate: 1.4223385733203031e-05\n",
      "Step: 9020, train/epoch: 2.146596908569336\n",
      "Step: 9030, train/loss: 0.0\n",
      "Step: 9030, train/grad_norm: 1.0337682931904624e-09\n",
      "Step: 9030, train/learning_rate: 1.4183721759764012e-05\n",
      "Step: 9030, train/epoch: 2.1489765644073486\n",
      "Step: 9040, train/loss: 0.0\n",
      "Step: 9040, train/grad_norm: 3.4906022960967675e-08\n",
      "Step: 9040, train/learning_rate: 1.4144058695819695e-05\n",
      "Step: 9040, train/epoch: 2.1513564586639404\n",
      "Step: 9050, train/loss: 0.0\n",
      "Step: 9050, train/grad_norm: 8.507084459097314e-08\n",
      "Step: 9050, train/learning_rate: 1.4104394722380675e-05\n",
      "Step: 9050, train/epoch: 2.1537363529205322\n",
      "Step: 9060, train/loss: 0.0\n",
      "Step: 9060, train/grad_norm: 1.2056152520756314e-08\n",
      "Step: 9060, train/learning_rate: 1.4064730748941656e-05\n",
      "Step: 9060, train/epoch: 2.156116247177124\n",
      "Step: 9070, train/loss: 0.0\n",
      "Step: 9070, train/grad_norm: 1.564768226103297e-08\n",
      "Step: 9070, train/learning_rate: 1.4025067684997339e-05\n",
      "Step: 9070, train/epoch: 2.1584959030151367\n",
      "Step: 9080, train/loss: 0.0\n",
      "Step: 9080, train/grad_norm: 1.2818430761285526e-08\n",
      "Step: 9080, train/learning_rate: 1.398540371155832e-05\n",
      "Step: 9080, train/epoch: 2.1608757972717285\n",
      "Step: 9090, train/loss: 0.0\n",
      "Step: 9090, train/grad_norm: 9.624717023370977e-08\n",
      "Step: 9090, train/learning_rate: 1.39457397381193e-05\n",
      "Step: 9090, train/epoch: 2.1632556915283203\n",
      "Step: 9100, train/loss: 0.0\n",
      "Step: 9100, train/grad_norm: 0.00037625670665875077\n",
      "Step: 9100, train/learning_rate: 1.3906076674174983e-05\n",
      "Step: 9100, train/epoch: 2.165635347366333\n",
      "Step: 9110, train/loss: 0.0\n",
      "Step: 9110, train/grad_norm: 8.06663358332571e-09\n",
      "Step: 9110, train/learning_rate: 1.3866412700735964e-05\n",
      "Step: 9110, train/epoch: 2.168015241622925\n",
      "Step: 9120, train/loss: 0.0\n",
      "Step: 9120, train/grad_norm: 1.0007666872979826e-07\n",
      "Step: 9120, train/learning_rate: 1.3826748727296945e-05\n",
      "Step: 9120, train/epoch: 2.1703951358795166\n",
      "Step: 9130, train/loss: 0.2312999963760376\n",
      "Step: 9130, train/grad_norm: 33.754974365234375\n",
      "Step: 9130, train/learning_rate: 1.3787085663352627e-05\n",
      "Step: 9130, train/epoch: 2.1727747917175293\n",
      "Step: 9140, train/loss: 9.999999747378752e-05\n",
      "Step: 9140, train/grad_norm: 0.11669633537530899\n",
      "Step: 9140, train/learning_rate: 1.3747421689913608e-05\n",
      "Step: 9140, train/epoch: 2.175154685974121\n",
      "Step: 9150, train/loss: 0.0003000000142492354\n",
      "Step: 9150, train/grad_norm: 0.0009885573526844382\n",
      "Step: 9150, train/learning_rate: 1.370775862596929e-05\n",
      "Step: 9150, train/epoch: 2.177534580230713\n",
      "Step: 9160, train/loss: 0.0\n",
      "Step: 9160, train/grad_norm: 8.14058439573273e-05\n",
      "Step: 9160, train/learning_rate: 1.3668094652530272e-05\n",
      "Step: 9160, train/epoch: 2.1799142360687256\n",
      "Step: 9170, train/loss: 0.0\n",
      "Step: 9170, train/grad_norm: 2.3467622668249533e-05\n",
      "Step: 9170, train/learning_rate: 1.3628430679091252e-05\n",
      "Step: 9170, train/epoch: 2.1822941303253174\n",
      "Step: 9180, train/loss: 0.0\n",
      "Step: 9180, train/grad_norm: 3.0116309062577784e-05\n",
      "Step: 9180, train/learning_rate: 1.3588767615146935e-05\n",
      "Step: 9180, train/epoch: 2.184674024581909\n",
      "Step: 9190, train/loss: 0.0\n",
      "Step: 9190, train/grad_norm: 3.32399140461348e-05\n",
      "Step: 9190, train/learning_rate: 1.3549103641707916e-05\n",
      "Step: 9190, train/epoch: 2.187053680419922\n",
      "Step: 9200, train/loss: 0.0\n",
      "Step: 9200, train/grad_norm: 9.49368313740706e-06\n",
      "Step: 9200, train/learning_rate: 1.3509439668268897e-05\n",
      "Step: 9200, train/epoch: 2.1894335746765137\n",
      "Step: 9210, train/loss: 0.0\n",
      "Step: 9210, train/grad_norm: 1.987975883821491e-05\n",
      "Step: 9210, train/learning_rate: 1.346977660432458e-05\n",
      "Step: 9210, train/epoch: 2.1918134689331055\n",
      "Step: 9220, train/loss: 0.0\n",
      "Step: 9220, train/grad_norm: 1.4362782167154364e-05\n",
      "Step: 9220, train/learning_rate: 1.343011263088556e-05\n",
      "Step: 9220, train/epoch: 2.194193124771118\n",
      "Step: 9230, train/loss: 0.0\n",
      "Step: 9230, train/grad_norm: 0.00029297408764250576\n",
      "Step: 9230, train/learning_rate: 1.3390448657446541e-05\n",
      "Step: 9230, train/epoch: 2.19657301902771\n",
      "Step: 9240, train/loss: 0.0\n",
      "Step: 9240, train/grad_norm: 1.0008462595578749e-05\n",
      "Step: 9240, train/learning_rate: 1.3350785593502223e-05\n",
      "Step: 9240, train/epoch: 2.1989529132843018\n",
      "Step: 9250, train/loss: 0.0\n",
      "Step: 9250, train/grad_norm: 1.9844948837999254e-05\n",
      "Step: 9250, train/learning_rate: 1.3311121620063204e-05\n",
      "Step: 9250, train/epoch: 2.2013328075408936\n",
      "Step: 9260, train/loss: 0.0\n",
      "Step: 9260, train/grad_norm: 3.8044829125283286e-05\n",
      "Step: 9260, train/learning_rate: 1.3271457646624185e-05\n",
      "Step: 9260, train/epoch: 2.2037124633789062\n",
      "Step: 9270, train/loss: 0.0\n",
      "Step: 9270, train/grad_norm: 3.087599452555878e-06\n",
      "Step: 9270, train/learning_rate: 1.3231794582679868e-05\n",
      "Step: 9270, train/epoch: 2.206092357635498\n",
      "Step: 9280, train/loss: 0.0\n",
      "Step: 9280, train/grad_norm: 1.2116242942283861e-05\n",
      "Step: 9280, train/learning_rate: 1.3192130609240849e-05\n",
      "Step: 9280, train/epoch: 2.20847225189209\n",
      "Step: 9290, train/loss: 0.0\n",
      "Step: 9290, train/grad_norm: 7.568341516162036e-06\n",
      "Step: 9290, train/learning_rate: 1.315246663580183e-05\n",
      "Step: 9290, train/epoch: 2.2108519077301025\n",
      "Step: 9300, train/loss: 0.0\n",
      "Step: 9300, train/grad_norm: 6.940009916434065e-05\n",
      "Step: 9300, train/learning_rate: 1.3112803571857512e-05\n",
      "Step: 9300, train/epoch: 2.2132318019866943\n",
      "Step: 9310, train/loss: 0.0\n",
      "Step: 9310, train/grad_norm: 6.388818292180076e-06\n",
      "Step: 9310, train/learning_rate: 1.3073139598418493e-05\n",
      "Step: 9310, train/epoch: 2.215611696243286\n",
      "Step: 9320, train/loss: 0.0\n",
      "Step: 9320, train/grad_norm: 6.661860879830783e-06\n",
      "Step: 9320, train/learning_rate: 1.3033476534474175e-05\n",
      "Step: 9320, train/epoch: 2.217991352081299\n",
      "Step: 9330, train/loss: 0.0\n",
      "Step: 9330, train/grad_norm: 3.275743802078068e-05\n",
      "Step: 9330, train/learning_rate: 1.2993812561035156e-05\n",
      "Step: 9330, train/epoch: 2.2203712463378906\n",
      "Step: 9340, train/loss: 0.0\n",
      "Step: 9340, train/grad_norm: 2.8048621970810927e-05\n",
      "Step: 9340, train/learning_rate: 1.2954148587596137e-05\n",
      "Step: 9340, train/epoch: 2.2227511405944824\n",
      "Step: 9350, train/loss: 0.0\n",
      "Step: 9350, train/grad_norm: 3.872327579301782e-05\n",
      "Step: 9350, train/learning_rate: 1.291448552365182e-05\n",
      "Step: 9350, train/epoch: 2.225130796432495\n",
      "Step: 9360, train/loss: 0.0\n",
      "Step: 9360, train/grad_norm: 4.961110244039446e-05\n",
      "Step: 9360, train/learning_rate: 1.28748215502128e-05\n",
      "Step: 9360, train/epoch: 2.227510690689087\n",
      "Step: 9370, train/loss: 0.0\n",
      "Step: 9370, train/grad_norm: 1.809140849218238e-05\n",
      "Step: 9370, train/learning_rate: 1.2835157576773781e-05\n",
      "Step: 9370, train/epoch: 2.2298905849456787\n",
      "Step: 9380, train/loss: 0.0\n",
      "Step: 9380, train/grad_norm: 7.448264568665763e-06\n",
      "Step: 9380, train/learning_rate: 1.2795494512829464e-05\n",
      "Step: 9380, train/epoch: 2.2322702407836914\n",
      "Step: 9390, train/loss: 0.15309999883174896\n",
      "Step: 9390, train/grad_norm: 3.3462422379670897e-06\n",
      "Step: 9390, train/learning_rate: 1.2755830539390445e-05\n",
      "Step: 9390, train/epoch: 2.234650135040283\n",
      "Step: 9400, train/loss: 0.0\n",
      "Step: 9400, train/grad_norm: 2.325878767805989e-06\n",
      "Step: 9400, train/learning_rate: 1.2716166565951426e-05\n",
      "Step: 9400, train/epoch: 2.237030029296875\n",
      "Step: 9410, train/loss: 0.0\n",
      "Step: 9410, train/grad_norm: 4.474156867217971e-06\n",
      "Step: 9410, train/learning_rate: 1.2676503502007108e-05\n",
      "Step: 9410, train/epoch: 2.239409923553467\n",
      "Step: 9420, train/loss: 0.0\n",
      "Step: 9420, train/grad_norm: 0.00013071420835331082\n",
      "Step: 9420, train/learning_rate: 1.2636839528568089e-05\n",
      "Step: 9420, train/epoch: 2.2417895793914795\n",
      "Step: 9430, train/loss: 0.0\n",
      "Step: 9430, train/grad_norm: 2.4311775632668287e-05\n",
      "Step: 9430, train/learning_rate: 1.259717555512907e-05\n",
      "Step: 9430, train/epoch: 2.2441694736480713\n",
      "Step: 9440, train/loss: 0.0\n",
      "Step: 9440, train/grad_norm: 8.459297532681376e-06\n",
      "Step: 9440, train/learning_rate: 1.2557512491184752e-05\n",
      "Step: 9440, train/epoch: 2.246549367904663\n",
      "Step: 9450, train/loss: 0.0\n",
      "Step: 9450, train/grad_norm: 0.00035012385342270136\n",
      "Step: 9450, train/learning_rate: 1.2517848517745733e-05\n",
      "Step: 9450, train/epoch: 2.248929023742676\n",
      "Step: 9460, train/loss: 0.0\n",
      "Step: 9460, train/grad_norm: 5.030080137657933e-05\n",
      "Step: 9460, train/learning_rate: 1.2478184544306714e-05\n",
      "Step: 9460, train/epoch: 2.2513089179992676\n",
      "Step: 9470, train/loss: 0.13359999656677246\n",
      "Step: 9470, train/grad_norm: 3.913531600119313e-06\n",
      "Step: 9470, train/learning_rate: 1.2438521480362397e-05\n",
      "Step: 9470, train/epoch: 2.2536888122558594\n",
      "Step: 9480, train/loss: 0.0\n",
      "Step: 9480, train/grad_norm: 0.00016879108443390578\n",
      "Step: 9480, train/learning_rate: 1.2398857506923378e-05\n",
      "Step: 9480, train/epoch: 2.256068468093872\n",
      "Step: 9490, train/loss: 0.0\n",
      "Step: 9490, train/grad_norm: 8.34030652185902e-05\n",
      "Step: 9490, train/learning_rate: 1.235919444297906e-05\n",
      "Step: 9490, train/epoch: 2.258448362350464\n",
      "Step: 9500, train/loss: 0.0\n",
      "Step: 9500, train/grad_norm: 2.4814980861265212e-05\n",
      "Step: 9500, train/learning_rate: 1.2319530469540041e-05\n",
      "Step: 9500, train/epoch: 2.2608282566070557\n",
      "Step: 9510, train/loss: 0.0\n",
      "Step: 9510, train/grad_norm: 3.771371120819822e-05\n",
      "Step: 9510, train/learning_rate: 1.2279866496101022e-05\n",
      "Step: 9510, train/epoch: 2.2632079124450684\n",
      "Step: 9520, train/loss: 0.0\n",
      "Step: 9520, train/grad_norm: 5.704271461581811e-05\n",
      "Step: 9520, train/learning_rate: 1.2240203432156704e-05\n",
      "Step: 9520, train/epoch: 2.26558780670166\n",
      "Step: 9530, train/loss: 0.0\n",
      "Step: 9530, train/grad_norm: 1.515134863439016e-05\n",
      "Step: 9530, train/learning_rate: 1.2200539458717685e-05\n",
      "Step: 9530, train/epoch: 2.267967700958252\n",
      "Step: 9540, train/loss: 0.0\n",
      "Step: 9540, train/grad_norm: 2.6819061531568877e-05\n",
      "Step: 9540, train/learning_rate: 1.2160875485278666e-05\n",
      "Step: 9540, train/epoch: 2.2703473567962646\n",
      "Step: 9550, train/loss: 0.0\n",
      "Step: 9550, train/grad_norm: 3.76286479877308e-05\n",
      "Step: 9550, train/learning_rate: 1.2121212421334349e-05\n",
      "Step: 9550, train/epoch: 2.2727272510528564\n",
      "Step: 9560, train/loss: 0.0\n",
      "Step: 9560, train/grad_norm: 0.00018650504352990538\n",
      "Step: 9560, train/learning_rate: 1.208154844789533e-05\n",
      "Step: 9560, train/epoch: 2.2751071453094482\n",
      "Step: 9570, train/loss: 0.0\n",
      "Step: 9570, train/grad_norm: 9.04646294657141e-05\n",
      "Step: 9570, train/learning_rate: 1.204188447445631e-05\n",
      "Step: 9570, train/epoch: 2.277486801147461\n",
      "Step: 9580, train/loss: 0.0\n",
      "Step: 9580, train/grad_norm: 0.025157740339636803\n",
      "Step: 9580, train/learning_rate: 1.2002221410511993e-05\n",
      "Step: 9580, train/epoch: 2.2798666954040527\n",
      "Step: 9590, train/loss: 0.0\n",
      "Step: 9590, train/grad_norm: 5.261924889055081e-05\n",
      "Step: 9590, train/learning_rate: 1.1962557437072974e-05\n",
      "Step: 9590, train/epoch: 2.2822465896606445\n",
      "Step: 9600, train/loss: 0.0\n",
      "Step: 9600, train/grad_norm: 6.381997081916779e-05\n",
      "Step: 9600, train/learning_rate: 1.1922893463633955e-05\n",
      "Step: 9600, train/epoch: 2.2846264839172363\n",
      "Step: 9610, train/loss: 0.0\n",
      "Step: 9610, train/grad_norm: 2.0185363103220055e-10\n",
      "Step: 9610, train/learning_rate: 1.1883230399689637e-05\n",
      "Step: 9610, train/epoch: 2.287006139755249\n",
      "Step: 9620, train/loss: 0.0\n",
      "Step: 9620, train/grad_norm: 0.00027646601665765047\n",
      "Step: 9620, train/learning_rate: 1.1843566426250618e-05\n",
      "Step: 9620, train/epoch: 2.289386034011841\n",
      "Step: 9630, train/loss: 0.0\n",
      "Step: 9630, train/grad_norm: 0.00021161064796615392\n",
      "Step: 9630, train/learning_rate: 1.1803902452811599e-05\n",
      "Step: 9630, train/epoch: 2.2917659282684326\n",
      "Step: 9640, train/loss: 0.0\n",
      "Step: 9640, train/grad_norm: 0.0002961415739264339\n",
      "Step: 9640, train/learning_rate: 1.1764239388867281e-05\n",
      "Step: 9640, train/epoch: 2.2941455841064453\n",
      "Step: 9650, train/loss: 0.0\n",
      "Step: 9650, train/grad_norm: 1.1591040674829856e-05\n",
      "Step: 9650, train/learning_rate: 1.1724575415428262e-05\n",
      "Step: 9650, train/epoch: 2.296525478363037\n",
      "Step: 9660, train/loss: 0.0\n",
      "Step: 9660, train/grad_norm: 7.069417188176885e-05\n",
      "Step: 9660, train/learning_rate: 1.1684912351483945e-05\n",
      "Step: 9660, train/epoch: 2.298905372619629\n",
      "Step: 9670, train/loss: 0.0364999994635582\n",
      "Step: 9670, train/grad_norm: 8.717826858628541e-05\n",
      "Step: 9670, train/learning_rate: 1.1645248378044926e-05\n",
      "Step: 9670, train/epoch: 2.3012850284576416\n",
      "Step: 9680, train/loss: 0.0\n",
      "Step: 9680, train/grad_norm: 1.624862852622755e-05\n",
      "Step: 9680, train/learning_rate: 1.1605584404605906e-05\n",
      "Step: 9680, train/epoch: 2.3036649227142334\n",
      "Step: 9690, train/loss: 0.0\n",
      "Step: 9690, train/grad_norm: 6.165591912576929e-05\n",
      "Step: 9690, train/learning_rate: 1.1565921340661589e-05\n",
      "Step: 9690, train/epoch: 2.306044816970825\n",
      "Step: 9700, train/loss: 0.0\n",
      "Step: 9700, train/grad_norm: 6.151827165012946e-06\n",
      "Step: 9700, train/learning_rate: 1.152625736722257e-05\n",
      "Step: 9700, train/epoch: 2.308424472808838\n",
      "Step: 9710, train/loss: 0.0\n",
      "Step: 9710, train/grad_norm: 5.782837433798704e-06\n",
      "Step: 9710, train/learning_rate: 1.148659339378355e-05\n",
      "Step: 9710, train/epoch: 2.3108043670654297\n",
      "Step: 9720, train/loss: 0.0\n",
      "Step: 9720, train/grad_norm: 3.0373080335266422e-06\n",
      "Step: 9720, train/learning_rate: 1.1446930329839233e-05\n",
      "Step: 9720, train/epoch: 2.3131842613220215\n",
      "Step: 9730, train/loss: 0.0\n",
      "Step: 9730, train/grad_norm: 7.714255843893625e-06\n",
      "Step: 9730, train/learning_rate: 1.1407266356400214e-05\n",
      "Step: 9730, train/epoch: 2.315563917160034\n",
      "Step: 9740, train/loss: 0.0\n",
      "Step: 9740, train/grad_norm: 6.813268669247918e-07\n",
      "Step: 9740, train/learning_rate: 1.1367602382961195e-05\n",
      "Step: 9740, train/epoch: 2.317943811416626\n",
      "Step: 9750, train/loss: 0.0\n",
      "Step: 9750, train/grad_norm: 4.413665010361001e-06\n",
      "Step: 9750, train/learning_rate: 1.1327939319016878e-05\n",
      "Step: 9750, train/epoch: 2.3203237056732178\n",
      "Step: 9760, train/loss: 0.0\n",
      "Step: 9760, train/grad_norm: 7.793947588652372e-06\n",
      "Step: 9760, train/learning_rate: 1.1288275345577858e-05\n",
      "Step: 9760, train/epoch: 2.3227033615112305\n",
      "Step: 9770, train/loss: 9.999999747378752e-05\n",
      "Step: 9770, train/grad_norm: 5.267996129987296e-06\n",
      "Step: 9770, train/learning_rate: 1.124861137213884e-05\n",
      "Step: 9770, train/epoch: 2.3250832557678223\n",
      "Step: 9780, train/loss: 0.0\n",
      "Step: 9780, train/grad_norm: 3.5098994430882158e-06\n",
      "Step: 9780, train/learning_rate: 1.1208948308194522e-05\n",
      "Step: 9780, train/epoch: 2.327463150024414\n",
      "Step: 9790, train/loss: 0.0\n",
      "Step: 9790, train/grad_norm: 3.801573939199443e-06\n",
      "Step: 9790, train/learning_rate: 1.1169284334755503e-05\n",
      "Step: 9790, train/epoch: 2.329843044281006\n",
      "Step: 9800, train/loss: 0.0\n",
      "Step: 9800, train/grad_norm: 5.625831818178995e-07\n",
      "Step: 9800, train/learning_rate: 1.1129620361316483e-05\n",
      "Step: 9800, train/epoch: 2.3322227001190186\n",
      "Step: 9810, train/loss: 0.004800000227987766\n",
      "Step: 9810, train/grad_norm: 2.3252798200701363e-05\n",
      "Step: 9810, train/learning_rate: 1.1089957297372166e-05\n",
      "Step: 9810, train/epoch: 2.3346025943756104\n",
      "Step: 9820, train/loss: 0.0\n",
      "Step: 9820, train/grad_norm: 9.493999641563278e-06\n",
      "Step: 9820, train/learning_rate: 1.1050293323933147e-05\n",
      "Step: 9820, train/epoch: 2.336982488632202\n",
      "Step: 9830, train/loss: 0.0\n",
      "Step: 9830, train/grad_norm: 4.702608748630155e-06\n",
      "Step: 9830, train/learning_rate: 1.101063025998883e-05\n",
      "Step: 9830, train/epoch: 2.339362144470215\n",
      "Step: 9840, train/loss: 0.0\n",
      "Step: 9840, train/grad_norm: 0.001118289539590478\n",
      "Step: 9840, train/learning_rate: 1.097096628654981e-05\n",
      "Step: 9840, train/epoch: 2.3417420387268066\n",
      "Step: 9850, train/loss: 0.0\n",
      "Step: 9850, train/grad_norm: 1.0909219781751744e-05\n",
      "Step: 9850, train/learning_rate: 1.0931302313110791e-05\n",
      "Step: 9850, train/epoch: 2.3441219329833984\n",
      "Step: 9860, train/loss: 0.0\n",
      "Step: 9860, train/grad_norm: 7.655491208424792e-05\n",
      "Step: 9860, train/learning_rate: 1.0891639249166474e-05\n",
      "Step: 9860, train/epoch: 2.346501588821411\n",
      "Step: 9870, train/loss: 0.0\n",
      "Step: 9870, train/grad_norm: 0.00013012406998313963\n",
      "Step: 9870, train/learning_rate: 1.0851975275727455e-05\n",
      "Step: 9870, train/epoch: 2.348881483078003\n",
      "Step: 9880, train/loss: 0.0\n",
      "Step: 9880, train/grad_norm: 3.8994789065327495e-05\n",
      "Step: 9880, train/learning_rate: 1.0812311302288435e-05\n",
      "Step: 9880, train/epoch: 2.3512613773345947\n",
      "Step: 9890, train/loss: 0.0\n",
      "Step: 9890, train/grad_norm: 6.411167305486742e-06\n",
      "Step: 9890, train/learning_rate: 1.0772648238344118e-05\n",
      "Step: 9890, train/epoch: 2.3536410331726074\n",
      "Step: 9900, train/loss: 0.0\n",
      "Step: 9900, train/grad_norm: 1.1187254358446808e-06\n",
      "Step: 9900, train/learning_rate: 1.0732984264905099e-05\n",
      "Step: 9900, train/epoch: 2.356020927429199\n",
      "Step: 9910, train/loss: 0.0\n",
      "Step: 9910, train/grad_norm: 1.5734394764876924e-05\n",
      "Step: 9910, train/learning_rate: 1.069332029146608e-05\n",
      "Step: 9910, train/epoch: 2.358400821685791\n",
      "Step: 9920, train/loss: 0.0\n",
      "Step: 9920, train/grad_norm: 1.570803942740895e-05\n",
      "Step: 9920, train/learning_rate: 1.0653657227521762e-05\n",
      "Step: 9920, train/epoch: 2.3607804775238037\n",
      "Step: 9930, train/loss: 0.0\n",
      "Step: 9930, train/grad_norm: 1.0413625659566605e-06\n",
      "Step: 9930, train/learning_rate: 1.0613993254082743e-05\n",
      "Step: 9930, train/epoch: 2.3631603717803955\n",
      "Step: 9940, train/loss: 0.0\n",
      "Step: 9940, train/grad_norm: 2.7172885893378407e-05\n",
      "Step: 9940, train/learning_rate: 1.0574329280643724e-05\n",
      "Step: 9940, train/epoch: 2.3655402660369873\n",
      "Step: 9950, train/loss: 0.0\n",
      "Step: 9950, train/grad_norm: 3.1948165997164324e-05\n",
      "Step: 9950, train/learning_rate: 1.0534666216699407e-05\n",
      "Step: 9950, train/epoch: 2.367919921875\n",
      "Step: 9960, train/loss: 0.0\n",
      "Step: 9960, train/grad_norm: 2.876349526559352e-06\n",
      "Step: 9960, train/learning_rate: 1.0495002243260387e-05\n",
      "Step: 9960, train/epoch: 2.370299816131592\n",
      "Step: 9970, train/loss: 0.0\n",
      "Step: 9970, train/grad_norm: 2.1121859390405007e-06\n",
      "Step: 9970, train/learning_rate: 1.045533917931607e-05\n",
      "Step: 9970, train/epoch: 2.3726797103881836\n",
      "Step: 9980, train/loss: 0.0\n",
      "Step: 9980, train/grad_norm: 3.715870843734592e-05\n",
      "Step: 9980, train/learning_rate: 1.041567520587705e-05\n",
      "Step: 9980, train/epoch: 2.3750596046447754\n",
      "Step: 9990, train/loss: 0.0\n",
      "Step: 9990, train/grad_norm: 1.3126533531249152e-06\n",
      "Step: 9990, train/learning_rate: 1.0376011232438032e-05\n",
      "Step: 9990, train/epoch: 2.377439260482788\n",
      "Step: 10000, train/loss: 0.0\n",
      "Step: 10000, train/grad_norm: 1.536435229354538e-05\n",
      "Step: 10000, train/learning_rate: 1.0336348168493714e-05\n",
      "Step: 10000, train/epoch: 2.37981915473938\n",
      "Step: 10010, train/loss: 0.0\n",
      "Step: 10010, train/grad_norm: 4.123293365410063e-06\n",
      "Step: 10010, train/learning_rate: 1.0296684195054695e-05\n",
      "Step: 10010, train/epoch: 2.3821990489959717\n",
      "Step: 10020, train/loss: 0.0\n",
      "Step: 10020, train/grad_norm: 4.688048647949472e-06\n",
      "Step: 10020, train/learning_rate: 1.0257020221615676e-05\n",
      "Step: 10020, train/epoch: 2.3845787048339844\n",
      "Step: 10030, train/loss: 0.0\n",
      "Step: 10030, train/grad_norm: 3.2029034628067166e-05\n",
      "Step: 10030, train/learning_rate: 1.0217357157671358e-05\n",
      "Step: 10030, train/epoch: 2.386958599090576\n",
      "Step: 10040, train/loss: 0.0\n",
      "Step: 10040, train/grad_norm: 0.005190706811845303\n",
      "Step: 10040, train/learning_rate: 1.017769318423234e-05\n",
      "Step: 10040, train/epoch: 2.389338493347168\n",
      "Step: 10050, train/loss: 0.0\n",
      "Step: 10050, train/grad_norm: 5.376826720748795e-06\n",
      "Step: 10050, train/learning_rate: 1.013802921079332e-05\n",
      "Step: 10050, train/epoch: 2.3917181491851807\n",
      "Step: 10060, train/loss: 0.0\n",
      "Step: 10060, train/grad_norm: 1.9691802663146518e-05\n",
      "Step: 10060, train/learning_rate: 1.0098366146849003e-05\n",
      "Step: 10060, train/epoch: 2.3940980434417725\n",
      "Step: 10070, train/loss: 9.999999747378752e-05\n",
      "Step: 10070, train/grad_norm: 3.6642256873165024e-06\n",
      "Step: 10070, train/learning_rate: 1.0058702173409984e-05\n",
      "Step: 10070, train/epoch: 2.3964779376983643\n",
      "Step: 10080, train/loss: 0.0\n",
      "Step: 10080, train/grad_norm: 7.63335447118152e-06\n",
      "Step: 10080, train/learning_rate: 1.0019038199970964e-05\n",
      "Step: 10080, train/epoch: 2.398857593536377\n",
      "Step: 10090, train/loss: 0.0\n",
      "Step: 10090, train/grad_norm: 4.991442438040394e-06\n",
      "Step: 10090, train/learning_rate: 9.979375136026647e-06\n",
      "Step: 10090, train/epoch: 2.4012374877929688\n",
      "Step: 10100, train/loss: 0.0\n",
      "Step: 10100, train/grad_norm: 3.418341657379642e-05\n",
      "Step: 10100, train/learning_rate: 9.939711162587628e-06\n",
      "Step: 10100, train/epoch: 2.4036173820495605\n",
      "Step: 10110, train/loss: 0.0\n",
      "Step: 10110, train/grad_norm: 5.028519808547571e-06\n",
      "Step: 10110, train/learning_rate: 9.900047189148609e-06\n",
      "Step: 10110, train/epoch: 2.4059970378875732\n",
      "Step: 10120, train/loss: 0.0\n",
      "Step: 10120, train/grad_norm: 0.00041356708970852196\n",
      "Step: 10120, train/learning_rate: 9.860384125204291e-06\n",
      "Step: 10120, train/epoch: 2.408376932144165\n",
      "Step: 10130, train/loss: 0.0\n",
      "Step: 10130, train/grad_norm: 2.832150312315207e-05\n",
      "Step: 10130, train/learning_rate: 9.820720151765272e-06\n",
      "Step: 10130, train/epoch: 2.410756826400757\n",
      "Step: 10140, train/loss: 0.0\n",
      "Step: 10140, train/grad_norm: 1.1124946468044072e-05\n",
      "Step: 10140, train/learning_rate: 9.781057087820955e-06\n",
      "Step: 10140, train/epoch: 2.4131367206573486\n",
      "Step: 10150, train/loss: 0.0\n",
      "Step: 10150, train/grad_norm: 0.00021748672588728368\n",
      "Step: 10150, train/learning_rate: 9.741393114381935e-06\n",
      "Step: 10150, train/epoch: 2.4155163764953613\n",
      "Step: 10160, train/loss: 0.0\n",
      "Step: 10160, train/grad_norm: 9.569732355885208e-05\n",
      "Step: 10160, train/learning_rate: 9.701729140942916e-06\n",
      "Step: 10160, train/epoch: 2.417896270751953\n",
      "Step: 10170, train/loss: 0.0\n",
      "Step: 10170, train/grad_norm: 1.943496499734465e-05\n",
      "Step: 10170, train/learning_rate: 9.662066076998599e-06\n",
      "Step: 10170, train/epoch: 2.420276165008545\n",
      "Step: 10180, train/loss: 0.0\n",
      "Step: 10180, train/grad_norm: 4.507417088461807e-06\n",
      "Step: 10180, train/learning_rate: 9.62240210355958e-06\n",
      "Step: 10180, train/epoch: 2.4226558208465576\n",
      "Step: 10190, train/loss: 0.0\n",
      "Step: 10190, train/grad_norm: 0.011709201149642467\n",
      "Step: 10190, train/learning_rate: 9.58273813012056e-06\n",
      "Step: 10190, train/epoch: 2.4250357151031494\n",
      "Step: 10200, train/loss: 0.0\n",
      "Step: 10200, train/grad_norm: 2.320446219528094e-05\n",
      "Step: 10200, train/learning_rate: 9.543075066176243e-06\n",
      "Step: 10200, train/epoch: 2.427415609359741\n",
      "Step: 10210, train/loss: 0.0\n",
      "Step: 10210, train/grad_norm: 4.583751069731079e-05\n",
      "Step: 10210, train/learning_rate: 9.503411092737224e-06\n",
      "Step: 10210, train/epoch: 2.429795265197754\n",
      "Step: 10220, train/loss: 0.0\n",
      "Step: 10220, train/grad_norm: 0.0001006021848297678\n",
      "Step: 10220, train/learning_rate: 9.463747119298205e-06\n",
      "Step: 10220, train/epoch: 2.4321751594543457\n",
      "Step: 10230, train/loss: 0.0\n",
      "Step: 10230, train/grad_norm: 4.131736568524502e-05\n",
      "Step: 10230, train/learning_rate: 9.424084055353887e-06\n",
      "Step: 10230, train/epoch: 2.4345550537109375\n",
      "Step: 10240, train/loss: 0.0\n",
      "Step: 10240, train/grad_norm: 3.403926166356541e-05\n",
      "Step: 10240, train/learning_rate: 9.384420081914868e-06\n",
      "Step: 10240, train/epoch: 2.43693470954895\n",
      "Step: 10250, train/loss: 0.0\n",
      "Step: 10250, train/grad_norm: 0.0034915092401206493\n",
      "Step: 10250, train/learning_rate: 9.344756108475849e-06\n",
      "Step: 10250, train/epoch: 2.439314603805542\n",
      "Step: 10260, train/loss: 0.0\n",
      "Step: 10260, train/grad_norm: 3.579641634132713e-05\n",
      "Step: 10260, train/learning_rate: 9.305093044531532e-06\n",
      "Step: 10260, train/epoch: 2.441694498062134\n",
      "Step: 10270, train/loss: 0.0\n",
      "Step: 10270, train/grad_norm: 8.747079846216366e-05\n",
      "Step: 10270, train/learning_rate: 9.265429071092512e-06\n",
      "Step: 10270, train/epoch: 2.4440741539001465\n",
      "Step: 10280, train/loss: 0.0\n",
      "Step: 10280, train/grad_norm: 2.022949502133997e-06\n",
      "Step: 10280, train/learning_rate: 9.225765097653493e-06\n",
      "Step: 10280, train/epoch: 2.4464540481567383\n",
      "Step: 10290, train/loss: 0.0\n",
      "Step: 10290, train/grad_norm: 8.01648639026098e-06\n",
      "Step: 10290, train/learning_rate: 9.186102033709176e-06\n",
      "Step: 10290, train/epoch: 2.44883394241333\n",
      "Step: 10300, train/loss: 0.0\n",
      "Step: 10300, train/grad_norm: 4.746341801364906e-06\n",
      "Step: 10300, train/learning_rate: 9.146438060270157e-06\n",
      "Step: 10300, train/epoch: 2.4512135982513428\n",
      "Step: 10310, train/loss: 0.0\n",
      "Step: 10310, train/grad_norm: 4.002145942649804e-06\n",
      "Step: 10310, train/learning_rate: 9.10677499632584e-06\n",
      "Step: 10310, train/epoch: 2.4535934925079346\n",
      "Step: 10320, train/loss: 0.0\n",
      "Step: 10320, train/grad_norm: 4.41535230493173e-05\n",
      "Step: 10320, train/learning_rate: 9.06711102288682e-06\n",
      "Step: 10320, train/epoch: 2.4559733867645264\n",
      "Step: 10330, train/loss: 0.0\n",
      "Step: 10330, train/grad_norm: 8.689972310094163e-05\n",
      "Step: 10330, train/learning_rate: 9.027447049447801e-06\n",
      "Step: 10330, train/epoch: 2.458353281021118\n",
      "Step: 10340, train/loss: 0.0\n",
      "Step: 10340, train/grad_norm: 7.2678362812439445e-06\n",
      "Step: 10340, train/learning_rate: 8.987783985503484e-06\n",
      "Step: 10340, train/epoch: 2.460732936859131\n",
      "Step: 10350, train/loss: 0.0\n",
      "Step: 10350, train/grad_norm: 1.8518019714974798e-05\n",
      "Step: 10350, train/learning_rate: 8.948120012064464e-06\n",
      "Step: 10350, train/epoch: 2.4631128311157227\n",
      "Step: 10360, train/loss: 0.0\n",
      "Step: 10360, train/grad_norm: 4.996558345737867e-06\n",
      "Step: 10360, train/learning_rate: 8.908456038625445e-06\n",
      "Step: 10360, train/epoch: 2.4654927253723145\n",
      "Step: 10370, train/loss: 0.0\n",
      "Step: 10370, train/grad_norm: 1.8601784177008085e-05\n",
      "Step: 10370, train/learning_rate: 8.868792974681128e-06\n",
      "Step: 10370, train/epoch: 2.467872381210327\n",
      "Step: 10380, train/loss: 0.0\n",
      "Step: 10380, train/grad_norm: 1.0594300874799956e-05\n",
      "Step: 10380, train/learning_rate: 8.829129001242109e-06\n",
      "Step: 10380, train/epoch: 2.470252275466919\n",
      "Step: 10390, train/loss: 0.0\n",
      "Step: 10390, train/grad_norm: 4.4566713768290356e-05\n",
      "Step: 10390, train/learning_rate: 8.78946502780309e-06\n",
      "Step: 10390, train/epoch: 2.4726321697235107\n",
      "Step: 10400, train/loss: 0.0\n",
      "Step: 10400, train/grad_norm: 2.342655716347508e-05\n",
      "Step: 10400, train/learning_rate: 8.749801963858772e-06\n",
      "Step: 10400, train/epoch: 2.4750118255615234\n",
      "Step: 10410, train/loss: 0.0\n",
      "Step: 10410, train/grad_norm: 6.062935426598415e-05\n",
      "Step: 10410, train/learning_rate: 8.710137990419753e-06\n",
      "Step: 10410, train/epoch: 2.4773917198181152\n",
      "Step: 10420, train/loss: 0.0\n",
      "Step: 10420, train/grad_norm: 5.50401750842866e-07\n",
      "Step: 10420, train/learning_rate: 8.670474016980734e-06\n",
      "Step: 10420, train/epoch: 2.479771614074707\n",
      "Step: 10430, train/loss: 0.0\n",
      "Step: 10430, train/grad_norm: 8.154725037456956e-06\n",
      "Step: 10430, train/learning_rate: 8.630810953036416e-06\n",
      "Step: 10430, train/epoch: 2.4821512699127197\n",
      "Step: 10440, train/loss: 0.0\n",
      "Step: 10440, train/grad_norm: 0.0001027652106131427\n",
      "Step: 10440, train/learning_rate: 8.591146979597397e-06\n",
      "Step: 10440, train/epoch: 2.4845311641693115\n",
      "Step: 10450, train/loss: 0.0\n",
      "Step: 10450, train/grad_norm: 7.6020201049686875e-06\n",
      "Step: 10450, train/learning_rate: 8.551483006158378e-06\n",
      "Step: 10450, train/epoch: 2.4869110584259033\n",
      "Step: 10460, train/loss: 0.0\n",
      "Step: 10460, train/grad_norm: 1.1784559319494292e-05\n",
      "Step: 10460, train/learning_rate: 8.51181994221406e-06\n",
      "Step: 10460, train/epoch: 2.489290714263916\n",
      "Step: 10470, train/loss: 0.0\n",
      "Step: 10470, train/grad_norm: 0.00017303491767961532\n",
      "Step: 10470, train/learning_rate: 8.472155968775041e-06\n",
      "Step: 10470, train/epoch: 2.491670608520508\n",
      "Step: 10480, train/loss: 0.0\n",
      "Step: 10480, train/grad_norm: 4.291114692023257e-06\n",
      "Step: 10480, train/learning_rate: 8.432492904830724e-06\n",
      "Step: 10480, train/epoch: 2.4940505027770996\n",
      "Step: 10490, train/loss: 0.0\n",
      "Step: 10490, train/grad_norm: 3.193953034497099e-06\n",
      "Step: 10490, train/learning_rate: 8.392828931391705e-06\n",
      "Step: 10490, train/epoch: 2.4964301586151123\n",
      "Step: 10500, train/loss: 0.0\n",
      "Step: 10500, train/grad_norm: 2.54752958426252e-05\n",
      "Step: 10500, train/learning_rate: 8.353164957952686e-06\n",
      "Step: 10500, train/epoch: 2.498810052871704\n",
      "Step: 10510, train/loss: 0.0\n",
      "Step: 10510, train/grad_norm: 9.281835446017794e-06\n",
      "Step: 10510, train/learning_rate: 8.313501894008368e-06\n",
      "Step: 10510, train/epoch: 2.501189947128296\n",
      "Step: 10520, train/loss: 0.0\n",
      "Step: 10520, train/grad_norm: 4.54225073553971e-06\n",
      "Step: 10520, train/learning_rate: 8.273837920569349e-06\n",
      "Step: 10520, train/epoch: 2.5035698413848877\n",
      "Step: 10530, train/loss: 0.0\n",
      "Step: 10530, train/grad_norm: 1.4657674000773113e-05\n",
      "Step: 10530, train/learning_rate: 8.23417394713033e-06\n",
      "Step: 10530, train/epoch: 2.5059494972229004\n",
      "Step: 10540, train/loss: 0.0\n",
      "Step: 10540, train/grad_norm: 1.7285809008171782e-05\n",
      "Step: 10540, train/learning_rate: 8.194510883186013e-06\n",
      "Step: 10540, train/epoch: 2.508329391479492\n",
      "Step: 10550, train/loss: 0.0\n",
      "Step: 10550, train/grad_norm: 4.69275164505234e-06\n",
      "Step: 10550, train/learning_rate: 8.154846909746993e-06\n",
      "Step: 10550, train/epoch: 2.510709285736084\n",
      "Step: 10560, train/loss: 0.0\n",
      "Step: 10560, train/grad_norm: 1.4500245015369728e-05\n",
      "Step: 10560, train/learning_rate: 8.115182936307974e-06\n",
      "Step: 10560, train/epoch: 2.5130889415740967\n",
      "Step: 10570, train/loss: 0.0\n",
      "Step: 10570, train/grad_norm: 1.74237611645367e-05\n",
      "Step: 10570, train/learning_rate: 8.075519872363657e-06\n",
      "Step: 10570, train/epoch: 2.5154688358306885\n",
      "Step: 10580, train/loss: 0.0\n",
      "Step: 10580, train/grad_norm: 3.994915005023358e-06\n",
      "Step: 10580, train/learning_rate: 8.035855898924638e-06\n",
      "Step: 10580, train/epoch: 2.5178487300872803\n",
      "Step: 10590, train/loss: 0.0\n",
      "Step: 10590, train/grad_norm: 9.133829735219479e-06\n",
      "Step: 10590, train/learning_rate: 7.996191925485618e-06\n",
      "Step: 10590, train/epoch: 2.520228385925293\n",
      "Step: 10600, train/loss: 0.0\n",
      "Step: 10600, train/grad_norm: 3.2708101116440957e-06\n",
      "Step: 10600, train/learning_rate: 7.956528861541301e-06\n",
      "Step: 10600, train/epoch: 2.5226082801818848\n",
      "Step: 10610, train/loss: 0.0\n",
      "Step: 10610, train/grad_norm: 1.8364982679486275e-05\n",
      "Step: 10610, train/learning_rate: 7.916864888102282e-06\n",
      "Step: 10610, train/epoch: 2.5249881744384766\n",
      "Step: 10620, train/loss: 0.0\n",
      "Step: 10620, train/grad_norm: 3.7536481158895185e-06\n",
      "Step: 10620, train/learning_rate: 7.877200914663263e-06\n",
      "Step: 10620, train/epoch: 2.5273678302764893\n",
      "Step: 10630, train/loss: 0.0\n",
      "Step: 10630, train/grad_norm: 9.046298146131448e-06\n",
      "Step: 10630, train/learning_rate: 7.837537850718945e-06\n",
      "Step: 10630, train/epoch: 2.529747724533081\n",
      "Step: 10640, train/loss: 0.0\n",
      "Step: 10640, train/grad_norm: 2.9847036785213277e-05\n",
      "Step: 10640, train/learning_rate: 7.797873877279926e-06\n",
      "Step: 10640, train/epoch: 2.532127618789673\n",
      "Step: 10650, train/loss: 0.0\n",
      "Step: 10650, train/grad_norm: 4.2745341488625854e-06\n",
      "Step: 10650, train/learning_rate: 7.758210813335609e-06\n",
      "Step: 10650, train/epoch: 2.5345072746276855\n",
      "Step: 10660, train/loss: 0.0\n",
      "Step: 10660, train/grad_norm: 7.677048415644094e-06\n",
      "Step: 10660, train/learning_rate: 7.71854683989659e-06\n",
      "Step: 10660, train/epoch: 2.5368871688842773\n",
      "Step: 10670, train/loss: 0.0\n",
      "Step: 10670, train/grad_norm: 1.4213008398655802e-05\n",
      "Step: 10670, train/learning_rate: 7.67888286645757e-06\n",
      "Step: 10670, train/epoch: 2.539267063140869\n",
      "Step: 10680, train/loss: 0.0\n",
      "Step: 10680, train/grad_norm: 1.8256598195875995e-05\n",
      "Step: 10680, train/learning_rate: 7.639219802513253e-06\n",
      "Step: 10680, train/epoch: 2.541646718978882\n",
      "Step: 10690, train/loss: 0.0\n",
      "Step: 10690, train/grad_norm: 7.473262485291343e-06\n",
      "Step: 10690, train/learning_rate: 7.599555829074234e-06\n",
      "Step: 10690, train/epoch: 2.5440266132354736\n",
      "Step: 10700, train/loss: 0.0\n",
      "Step: 10700, train/grad_norm: 8.203994184441399e-06\n",
      "Step: 10700, train/learning_rate: 7.5598923103825655e-06\n",
      "Step: 10700, train/epoch: 2.5464065074920654\n",
      "Step: 10710, train/loss: 0.0\n",
      "Step: 10710, train/grad_norm: 1.20201793833985e-05\n",
      "Step: 10710, train/learning_rate: 7.520228336943546e-06\n",
      "Step: 10710, train/epoch: 2.5487864017486572\n",
      "Step: 10720, train/loss: 0.0\n",
      "Step: 10720, train/grad_norm: 1.300138592341682e-06\n",
      "Step: 10720, train/learning_rate: 7.480564818251878e-06\n",
      "Step: 10720, train/epoch: 2.55116605758667\n",
      "Step: 10730, train/loss: 0.0\n",
      "Step: 10730, train/grad_norm: 0.0003960760368499905\n",
      "Step: 10730, train/learning_rate: 7.44090129956021e-06\n",
      "Step: 10730, train/epoch: 2.5535459518432617\n",
      "Step: 10740, train/loss: 0.0\n",
      "Step: 10740, train/grad_norm: 0.00015679550415370613\n",
      "Step: 10740, train/learning_rate: 7.4012373261211906e-06\n",
      "Step: 10740, train/epoch: 2.5559258460998535\n",
      "Step: 10750, train/loss: 0.0\n",
      "Step: 10750, train/grad_norm: 4.070103386766277e-06\n",
      "Step: 10750, train/learning_rate: 7.361573807429522e-06\n",
      "Step: 10750, train/epoch: 2.558305501937866\n",
      "Step: 10760, train/loss: 0.0\n",
      "Step: 10760, train/grad_norm: 6.999061042733956e-06\n",
      "Step: 10760, train/learning_rate: 7.321910288737854e-06\n",
      "Step: 10760, train/epoch: 2.560685396194458\n",
      "Step: 10770, train/loss: 0.0\n",
      "Step: 10770, train/grad_norm: 2.2093408915679902e-05\n",
      "Step: 10770, train/learning_rate: 7.282246770046186e-06\n",
      "Step: 10770, train/epoch: 2.56306529045105\n",
      "Step: 10780, train/loss: 0.0\n",
      "Step: 10780, train/grad_norm: 2.8736999446721256e-09\n",
      "Step: 10780, train/learning_rate: 7.2425827966071665e-06\n",
      "Step: 10780, train/epoch: 2.5654449462890625\n",
      "Step: 10790, train/loss: 0.0\n",
      "Step: 10790, train/grad_norm: 3.1289491744246334e-05\n",
      "Step: 10790, train/learning_rate: 7.202919277915498e-06\n",
      "Step: 10790, train/epoch: 2.5678248405456543\n",
      "Step: 10800, train/loss: 0.0\n",
      "Step: 10800, train/grad_norm: 1.2310108559177024e-06\n",
      "Step: 10800, train/learning_rate: 7.16325575922383e-06\n",
      "Step: 10800, train/epoch: 2.570204734802246\n",
      "Step: 10810, train/loss: 0.0\n",
      "Step: 10810, train/grad_norm: 3.572213927327539e-06\n",
      "Step: 10810, train/learning_rate: 7.123591785784811e-06\n",
      "Step: 10810, train/epoch: 2.572584390640259\n",
      "Step: 10820, train/loss: 0.0\n",
      "Step: 10820, train/grad_norm: 1.3644847058458254e-05\n",
      "Step: 10820, train/learning_rate: 7.0839282670931425e-06\n",
      "Step: 10820, train/epoch: 2.5749642848968506\n",
      "Step: 10830, train/loss: 0.0\n",
      "Step: 10830, train/grad_norm: 8.845143383950926e-06\n",
      "Step: 10830, train/learning_rate: 7.044264748401474e-06\n",
      "Step: 10830, train/epoch: 2.5773441791534424\n",
      "Step: 10840, train/loss: 0.0\n",
      "Step: 10840, train/grad_norm: 5.002059424441541e-06\n",
      "Step: 10840, train/learning_rate: 7.004600774962455e-06\n",
      "Step: 10840, train/epoch: 2.579723834991455\n",
      "Step: 10850, train/loss: 0.0\n",
      "Step: 10850, train/grad_norm: 3.7351230275817215e-05\n",
      "Step: 10850, train/learning_rate: 6.964937256270787e-06\n",
      "Step: 10850, train/epoch: 2.582103729248047\n",
      "Step: 10860, train/loss: 0.0\n",
      "Step: 10860, train/grad_norm: 7.089122391334968e-06\n",
      "Step: 10860, train/learning_rate: 6.9252737375791185e-06\n",
      "Step: 10860, train/epoch: 2.5844836235046387\n",
      "Step: 10870, train/loss: 0.0\n",
      "Step: 10870, train/grad_norm: 7.21228752809111e-06\n",
      "Step: 10870, train/learning_rate: 6.88561021888745e-06\n",
      "Step: 10870, train/epoch: 2.5868632793426514\n",
      "Step: 10880, train/loss: 0.0\n",
      "Step: 10880, train/grad_norm: 1.2508793588494882e-05\n",
      "Step: 10880, train/learning_rate: 6.845946245448431e-06\n",
      "Step: 10880, train/epoch: 2.589243173599243\n",
      "Step: 10890, train/loss: 0.0\n",
      "Step: 10890, train/grad_norm: 1.0319356988475192e-05\n",
      "Step: 10890, train/learning_rate: 6.806282726756763e-06\n",
      "Step: 10890, train/epoch: 2.591623067855835\n",
      "Step: 10900, train/loss: 0.0\n",
      "Step: 10900, train/grad_norm: 5.248638444754761e-06\n",
      "Step: 10900, train/learning_rate: 6.7666192080650944e-06\n",
      "Step: 10900, train/epoch: 2.5940029621124268\n",
      "Step: 10910, train/loss: 0.0\n",
      "Step: 10910, train/grad_norm: 8.629675676274928e-07\n",
      "Step: 10910, train/learning_rate: 6.726955234626075e-06\n",
      "Step: 10910, train/epoch: 2.5963826179504395\n",
      "Step: 10920, train/loss: 0.0\n",
      "Step: 10920, train/grad_norm: 4.244067531544715e-05\n",
      "Step: 10920, train/learning_rate: 6.687291715934407e-06\n",
      "Step: 10920, train/epoch: 2.5987625122070312\n",
      "Step: 10930, train/loss: 0.0\n",
      "Step: 10930, train/grad_norm: 4.237012944940943e-06\n",
      "Step: 10930, train/learning_rate: 6.647628197242739e-06\n",
      "Step: 10930, train/epoch: 2.601142406463623\n",
      "Step: 10940, train/loss: 0.0\n",
      "Step: 10940, train/grad_norm: 4.220479240757413e-06\n",
      "Step: 10940, train/learning_rate: 6.60796467855107e-06\n",
      "Step: 10940, train/epoch: 2.6035220623016357\n",
      "Step: 10950, train/loss: 0.0\n",
      "Step: 10950, train/grad_norm: 3.889479557983577e-06\n",
      "Step: 10950, train/learning_rate: 6.568300705112051e-06\n",
      "Step: 10950, train/epoch: 2.6059019565582275\n",
      "Step: 10960, train/loss: 0.0\n",
      "Step: 10960, train/grad_norm: 3.65254368261958e-06\n",
      "Step: 10960, train/learning_rate: 6.528637186420383e-06\n",
      "Step: 10960, train/epoch: 2.6082818508148193\n",
      "Step: 10970, train/loss: 0.0\n",
      "Step: 10970, train/grad_norm: 6.4475470935576595e-06\n",
      "Step: 10970, train/learning_rate: 6.488973667728715e-06\n",
      "Step: 10970, train/epoch: 2.610661506652832\n",
      "Step: 10980, train/loss: 0.0\n",
      "Step: 10980, train/grad_norm: 2.2636560970568098e-05\n",
      "Step: 10980, train/learning_rate: 6.4493096942896955e-06\n",
      "Step: 10980, train/epoch: 2.613041400909424\n",
      "Step: 10990, train/loss: 0.0\n",
      "Step: 10990, train/grad_norm: 3.1673405374021968e-06\n",
      "Step: 10990, train/learning_rate: 6.409646175598027e-06\n",
      "Step: 10990, train/epoch: 2.6154212951660156\n",
      "Step: 11000, train/loss: 0.0\n",
      "Step: 11000, train/grad_norm: 8.489625543006696e-06\n",
      "Step: 11000, train/learning_rate: 6.369982656906359e-06\n",
      "Step: 11000, train/epoch: 2.6178009510040283\n",
      "Step: 11010, train/loss: 0.0\n",
      "Step: 11010, train/grad_norm: 7.197841114248149e-06\n",
      "Step: 11010, train/learning_rate: 6.33031868346734e-06\n",
      "Step: 11010, train/epoch: 2.62018084526062\n",
      "Step: 11020, train/loss: 0.0\n",
      "Step: 11020, train/grad_norm: 4.427724888955709e-06\n",
      "Step: 11020, train/learning_rate: 6.2906551647756714e-06\n",
      "Step: 11020, train/epoch: 2.622560739517212\n",
      "Step: 11030, train/loss: 0.0\n",
      "Step: 11030, train/grad_norm: 2.0132347344770096e-06\n",
      "Step: 11030, train/learning_rate: 6.250991646084003e-06\n",
      "Step: 11030, train/epoch: 2.6249403953552246\n",
      "Step: 11040, train/loss: 0.0\n",
      "Step: 11040, train/grad_norm: 3.726392606040463e-05\n",
      "Step: 11040, train/learning_rate: 6.211328127392335e-06\n",
      "Step: 11040, train/epoch: 2.6273202896118164\n",
      "Step: 11050, train/loss: 0.0\n",
      "Step: 11050, train/grad_norm: 1.4514060922010685e-06\n",
      "Step: 11050, train/learning_rate: 6.171664153953316e-06\n",
      "Step: 11050, train/epoch: 2.629700183868408\n",
      "Step: 11060, train/loss: 0.0\n",
      "Step: 11060, train/grad_norm: 7.450153134413995e-06\n",
      "Step: 11060, train/learning_rate: 6.132000635261647e-06\n",
      "Step: 11060, train/epoch: 2.632080078125\n",
      "Step: 11070, train/loss: 0.0\n",
      "Step: 11070, train/grad_norm: 4.480380539462203e-06\n",
      "Step: 11070, train/learning_rate: 6.092337116569979e-06\n",
      "Step: 11070, train/epoch: 2.6344597339630127\n",
      "Step: 11080, train/loss: 0.0\n",
      "Step: 11080, train/grad_norm: 9.556881559547037e-05\n",
      "Step: 11080, train/learning_rate: 6.05267314313096e-06\n",
      "Step: 11080, train/epoch: 2.6368396282196045\n",
      "Step: 11090, train/loss: 0.0\n",
      "Step: 11090, train/grad_norm: 8.038309715630021e-06\n",
      "Step: 11090, train/learning_rate: 6.013009624439292e-06\n",
      "Step: 11090, train/epoch: 2.6392195224761963\n",
      "Step: 11100, train/loss: 0.0\n",
      "Step: 11100, train/grad_norm: 3.2150408060260816e-06\n",
      "Step: 11100, train/learning_rate: 5.973346105747623e-06\n",
      "Step: 11100, train/epoch: 2.641599178314209\n",
      "Step: 11110, train/loss: 0.0\n",
      "Step: 11110, train/grad_norm: 1.1106371857749764e-06\n",
      "Step: 11110, train/learning_rate: 5.933682587055955e-06\n",
      "Step: 11110, train/epoch: 2.643979072570801\n",
      "Step: 11120, train/loss: 0.0\n",
      "Step: 11120, train/grad_norm: 2.1042504158685915e-05\n",
      "Step: 11120, train/learning_rate: 5.894018613616936e-06\n",
      "Step: 11120, train/epoch: 2.6463589668273926\n",
      "Step: 11130, train/loss: 0.0\n",
      "Step: 11130, train/grad_norm: 2.0217682958900696e-06\n",
      "Step: 11130, train/learning_rate: 5.854355094925268e-06\n",
      "Step: 11130, train/epoch: 2.6487386226654053\n",
      "Step: 11140, train/loss: 0.24379999935626984\n",
      "Step: 11140, train/grad_norm: 2.387494532740675e-05\n",
      "Step: 11140, train/learning_rate: 5.814691576233599e-06\n",
      "Step: 11140, train/epoch: 2.651118516921997\n",
      "Step: 11150, train/loss: 0.0\n",
      "Step: 11150, train/grad_norm: 0.0003946165961679071\n",
      "Step: 11150, train/learning_rate: 5.77502760279458e-06\n",
      "Step: 11150, train/epoch: 2.653498411178589\n",
      "Step: 11160, train/loss: 0.0\n",
      "Step: 11160, train/grad_norm: 1.0159767043660395e-05\n",
      "Step: 11160, train/learning_rate: 5.735364084102912e-06\n",
      "Step: 11160, train/epoch: 2.6558780670166016\n",
      "Step: 11170, train/loss: 0.0\n",
      "Step: 11170, train/grad_norm: 1.3891265552956611e-05\n",
      "Step: 11170, train/learning_rate: 5.695700565411244e-06\n",
      "Step: 11170, train/epoch: 2.6582579612731934\n",
      "Step: 11180, train/loss: 0.0\n",
      "Step: 11180, train/grad_norm: 1.7065592601284152e-06\n",
      "Step: 11180, train/learning_rate: 5.656036591972224e-06\n",
      "Step: 11180, train/epoch: 2.660637855529785\n",
      "Step: 11190, train/loss: 0.0\n",
      "Step: 11190, train/grad_norm: 0.04703811928629875\n",
      "Step: 11190, train/learning_rate: 5.616373073280556e-06\n",
      "Step: 11190, train/epoch: 2.663017511367798\n",
      "Step: 11200, train/loss: 0.0\n",
      "Step: 11200, train/grad_norm: 9.193481673719361e-06\n",
      "Step: 11200, train/learning_rate: 5.576709554588888e-06\n",
      "Step: 11200, train/epoch: 2.6653974056243896\n",
      "Step: 11210, train/loss: 0.0\n",
      "Step: 11210, train/grad_norm: 4.5113042688171845e-06\n",
      "Step: 11210, train/learning_rate: 5.5370460358972196e-06\n",
      "Step: 11210, train/epoch: 2.6677772998809814\n",
      "Step: 11220, train/loss: 0.0\n",
      "Step: 11220, train/grad_norm: 2.011056312767323e-05\n",
      "Step: 11220, train/learning_rate: 5.4973820624582e-06\n",
      "Step: 11220, train/epoch: 2.670156955718994\n",
      "Step: 11230, train/loss: 0.0\n",
      "Step: 11230, train/grad_norm: 0.0001800102909328416\n",
      "Step: 11230, train/learning_rate: 5.457718543766532e-06\n",
      "Step: 11230, train/epoch: 2.672536849975586\n",
      "Step: 11240, train/loss: 0.0\n",
      "Step: 11240, train/grad_norm: 0.002098712371662259\n",
      "Step: 11240, train/learning_rate: 5.418055025074864e-06\n",
      "Step: 11240, train/epoch: 2.6749167442321777\n",
      "Step: 11250, train/loss: 0.0\n",
      "Step: 11250, train/grad_norm: 5.702187627321109e-05\n",
      "Step: 11250, train/learning_rate: 5.378391051635845e-06\n",
      "Step: 11250, train/epoch: 2.6772966384887695\n",
      "Step: 11260, train/loss: 0.0\n",
      "Step: 11260, train/grad_norm: 5.894266325023878e-11\n",
      "Step: 11260, train/learning_rate: 5.338727532944176e-06\n",
      "Step: 11260, train/epoch: 2.6796762943267822\n",
      "Step: 11270, train/loss: 0.0\n",
      "Step: 11270, train/grad_norm: 8.991481448283878e-10\n",
      "Step: 11270, train/learning_rate: 5.299064014252508e-06\n",
      "Step: 11270, train/epoch: 2.682056188583374\n",
      "Step: 11280, train/loss: 0.0\n",
      "Step: 11280, train/grad_norm: 1.581915967108216e-05\n",
      "Step: 11280, train/learning_rate: 5.25940049556084e-06\n",
      "Step: 11280, train/epoch: 2.684436082839966\n",
      "Step: 11290, train/loss: 0.0\n",
      "Step: 11290, train/grad_norm: 9.571546979714185e-05\n",
      "Step: 11290, train/learning_rate: 5.219736522121821e-06\n",
      "Step: 11290, train/epoch: 2.6868157386779785\n",
      "Step: 11300, train/loss: 0.0\n",
      "Step: 11300, train/grad_norm: 3.713783735292964e-05\n",
      "Step: 11300, train/learning_rate: 5.180073003430152e-06\n",
      "Step: 11300, train/epoch: 2.6891956329345703\n",
      "Step: 11310, train/loss: 0.0\n",
      "Step: 11310, train/grad_norm: 0.00012125865032430738\n",
      "Step: 11310, train/learning_rate: 5.140409484738484e-06\n",
      "Step: 11310, train/epoch: 2.691575527191162\n",
      "Step: 11320, train/loss: 0.0\n",
      "Step: 11320, train/grad_norm: 1.9531675206962973e-05\n",
      "Step: 11320, train/learning_rate: 5.100745511299465e-06\n",
      "Step: 11320, train/epoch: 2.693955183029175\n",
      "Step: 11330, train/loss: 0.0\n",
      "Step: 11330, train/grad_norm: 6.4995001594070345e-06\n",
      "Step: 11330, train/learning_rate: 5.0610819926077966e-06\n",
      "Step: 11330, train/epoch: 2.6963350772857666\n",
      "Step: 11340, train/loss: 0.0\n",
      "Step: 11340, train/grad_norm: 6.329647294478491e-05\n",
      "Step: 11340, train/learning_rate: 5.021418473916128e-06\n",
      "Step: 11340, train/epoch: 2.6987149715423584\n",
      "Step: 11350, train/loss: 0.0\n",
      "Step: 11350, train/grad_norm: 9.297646101913415e-06\n",
      "Step: 11350, train/learning_rate: 4.981754500477109e-06\n",
      "Step: 11350, train/epoch: 2.701094627380371\n",
      "Step: 11360, train/loss: 0.0\n",
      "Step: 11360, train/grad_norm: 1.432850694982335e-06\n",
      "Step: 11360, train/learning_rate: 4.942090981785441e-06\n",
      "Step: 11360, train/epoch: 2.703474521636963\n",
      "Step: 11370, train/loss: 0.0\n",
      "Step: 11370, train/grad_norm: 0.00015792466001585126\n",
      "Step: 11370, train/learning_rate: 4.9024274630937725e-06\n",
      "Step: 11370, train/epoch: 2.7058544158935547\n",
      "Step: 11380, train/loss: 0.0\n",
      "Step: 11380, train/grad_norm: 7.223935654110392e-07\n",
      "Step: 11380, train/learning_rate: 4.862763944402104e-06\n",
      "Step: 11380, train/epoch: 2.7082340717315674\n",
      "Step: 11390, train/loss: 0.0\n",
      "Step: 11390, train/grad_norm: 3.5159988328814507e-05\n",
      "Step: 11390, train/learning_rate: 4.823099970963085e-06\n",
      "Step: 11390, train/epoch: 2.710613965988159\n",
      "Step: 11400, train/loss: 0.0\n",
      "Step: 11400, train/grad_norm: 2.5171355446218513e-05\n",
      "Step: 11400, train/learning_rate: 4.783436452271417e-06\n",
      "Step: 11400, train/epoch: 2.712993860244751\n",
      "Step: 11410, train/loss: 0.0\n",
      "Step: 11410, train/grad_norm: 2.798286368488334e-05\n",
      "Step: 11410, train/learning_rate: 4.7437729335797485e-06\n",
      "Step: 11410, train/epoch: 2.7153735160827637\n",
      "Step: 11420, train/loss: 0.0\n",
      "Step: 11420, train/grad_norm: 1.5711493688286282e-05\n",
      "Step: 11420, train/learning_rate: 4.704108960140729e-06\n",
      "Step: 11420, train/epoch: 2.7177534103393555\n",
      "Step: 11430, train/loss: 0.0\n",
      "Step: 11430, train/grad_norm: 5.866817900823662e-06\n",
      "Step: 11430, train/learning_rate: 4.664445441449061e-06\n",
      "Step: 11430, train/epoch: 2.7201333045959473\n",
      "Step: 11440, train/loss: 0.0\n",
      "Step: 11440, train/grad_norm: 4.789803369931178e-06\n",
      "Step: 11440, train/learning_rate: 4.624781922757393e-06\n",
      "Step: 11440, train/epoch: 2.722513198852539\n",
      "Step: 11450, train/loss: 0.0\n",
      "Step: 11450, train/grad_norm: 0.0001289117499254644\n",
      "Step: 11450, train/learning_rate: 4.5851184040657245e-06\n",
      "Step: 11450, train/epoch: 2.7248928546905518\n",
      "Step: 11460, train/loss: 0.0\n",
      "Step: 11460, train/grad_norm: 7.312505658774171e-06\n",
      "Step: 11460, train/learning_rate: 4.545454430626705e-06\n",
      "Step: 11460, train/epoch: 2.7272727489471436\n",
      "Step: 11470, train/loss: 0.0\n",
      "Step: 11470, train/grad_norm: 1.580660682520829e-05\n",
      "Step: 11470, train/learning_rate: 4.505790911935037e-06\n",
      "Step: 11470, train/epoch: 2.7296526432037354\n",
      "Step: 11480, train/loss: 0.0\n",
      "Step: 11480, train/grad_norm: 1.4748854482604656e-05\n",
      "Step: 11480, train/learning_rate: 4.466127393243369e-06\n",
      "Step: 11480, train/epoch: 2.732032299041748\n",
      "Step: 11490, train/loss: 0.0\n",
      "Step: 11490, train/grad_norm: 8.528769285476301e-06\n",
      "Step: 11490, train/learning_rate: 4.4264634198043495e-06\n",
      "Step: 11490, train/epoch: 2.73441219329834\n",
      "Step: 11500, train/loss: 0.0\n",
      "Step: 11500, train/grad_norm: 5.840317317051813e-05\n",
      "Step: 11500, train/learning_rate: 4.386799901112681e-06\n",
      "Step: 11500, train/epoch: 2.7367920875549316\n",
      "Step: 11510, train/loss: 0.0\n",
      "Step: 11510, train/grad_norm: 4.4365118810674176e-05\n",
      "Step: 11510, train/learning_rate: 4.347136382421013e-06\n",
      "Step: 11510, train/epoch: 2.7391717433929443\n",
      "Step: 11520, train/loss: 0.0\n",
      "Step: 11520, train/grad_norm: 0.0002820575318764895\n",
      "Step: 11520, train/learning_rate: 4.307472408981994e-06\n",
      "Step: 11520, train/epoch: 2.741551637649536\n",
      "Step: 11530, train/loss: 0.0\n",
      "Step: 11530, train/grad_norm: 5.690792022505775e-05\n",
      "Step: 11530, train/learning_rate: 4.2678088902903255e-06\n",
      "Step: 11530, train/epoch: 2.743931531906128\n",
      "Step: 11540, train/loss: 0.0\n",
      "Step: 11540, train/grad_norm: 0.00022590981097891927\n",
      "Step: 11540, train/learning_rate: 4.228145371598657e-06\n",
      "Step: 11540, train/epoch: 2.7463111877441406\n",
      "Step: 11550, train/loss: 0.0\n",
      "Step: 11550, train/grad_norm: 4.9171312639373355e-06\n",
      "Step: 11550, train/learning_rate: 4.188481852906989e-06\n",
      "Step: 11550, train/epoch: 2.7486910820007324\n",
      "Step: 11560, train/loss: 0.0\n",
      "Step: 11560, train/grad_norm: 2.5808063583099283e-05\n",
      "Step: 11560, train/learning_rate: 4.14881787946797e-06\n",
      "Step: 11560, train/epoch: 2.751070976257324\n",
      "Reading events from file: ./praxis-Meta-Llama-3-70B-small-finetune/logs/events.out.tfevents.1718012825.hephaestus.4781.0\n",
      "Step: 8410, train/loss: 0.0\n",
      "Step: 8410, train/grad_norm: 8.272678542198264e-09\n",
      "Step: 8410, train/learning_rate: 1.6642869013594463e-05\n",
      "Step: 8410, train/epoch: 2.0014278888702393\n",
      "Step: 8420, train/loss: 0.0\n",
      "Step: 8420, train/grad_norm: 4.0251356381304504e-07\n",
      "Step: 8420, train/learning_rate: 1.6603204130660743e-05\n",
      "Step: 8420, train/epoch: 2.003807783126831\n",
      "Step: 8430, train/loss: 0.0\n",
      "Step: 8430, train/grad_norm: 6.601164932362735e-05\n",
      "Step: 8430, train/learning_rate: 1.6563541066716425e-05\n",
      "Step: 8430, train/epoch: 2.0061874389648438\n",
      "Step: 8440, train/loss: 0.0\n",
      "Step: 8440, train/grad_norm: 8.639522093289997e-06\n",
      "Step: 8440, train/learning_rate: 1.6523878002772108e-05\n",
      "Step: 8440, train/epoch: 2.0085673332214355\n",
      "Step: 8450, train/loss: 0.0\n",
      "Step: 8450, train/grad_norm: 2.6018406629191304e-07\n",
      "Step: 8450, train/learning_rate: 1.6484213119838387e-05\n",
      "Step: 8450, train/epoch: 2.0109472274780273\n",
      "Step: 8460, train/loss: 0.0\n",
      "Step: 8460, train/grad_norm: 6.224735926707581e-08\n",
      "Step: 8460, train/learning_rate: 1.644455005589407e-05\n",
      "Step: 8460, train/epoch: 2.01332688331604\n",
      "Step: 8470, train/loss: 0.0\n",
      "Step: 8470, train/grad_norm: 7.380370448117901e-08\n",
      "Step: 8470, train/learning_rate: 1.6404886991949752e-05\n",
      "Step: 8470, train/epoch: 2.015706777572632\n",
      "Step: 8480, train/loss: 0.0\n",
      "Step: 8480, train/grad_norm: 8.675490903442551e-07\n",
      "Step: 8480, train/learning_rate: 1.636522210901603e-05\n",
      "Step: 8480, train/epoch: 2.0180866718292236\n",
      "Step: 8490, train/loss: 0.0\n",
      "Step: 8490, train/grad_norm: 8.253558007709216e-06\n",
      "Step: 8490, train/learning_rate: 1.6325559045071714e-05\n",
      "Step: 8490, train/epoch: 2.0204663276672363\n",
      "Step: 8500, train/loss: 0.0\n",
      "Step: 8500, train/grad_norm: 3.97643695748684e-08\n",
      "Step: 8500, train/learning_rate: 1.6285895981127396e-05\n",
      "Step: 8500, train/epoch: 2.022846221923828\n",
      "Step: 8510, train/loss: 0.0\n",
      "Step: 8510, train/grad_norm: 3.9898935710880323e-08\n",
      "Step: 8510, train/learning_rate: 1.6246231098193675e-05\n",
      "Step: 8510, train/epoch: 2.02522611618042\n",
      "Step: 8520, train/loss: 0.0\n",
      "Step: 8520, train/grad_norm: 8.595552003498597e-08\n",
      "Step: 8520, train/learning_rate: 1.6206568034249358e-05\n",
      "Step: 8520, train/epoch: 2.0276060104370117\n",
      "Step: 8530, train/loss: 0.0\n",
      "Step: 8530, train/grad_norm: 1.411584804600352e-07\n",
      "Step: 8530, train/learning_rate: 1.616690497030504e-05\n",
      "Step: 8530, train/epoch: 2.0299856662750244\n",
      "Step: 8540, train/loss: 0.0\n",
      "Step: 8540, train/grad_norm: 3.7522698903558194e-07\n",
      "Step: 8540, train/learning_rate: 1.612724008737132e-05\n",
      "Step: 8540, train/epoch: 2.032365560531616\n",
      "Step: 8550, train/loss: 0.0\n",
      "Step: 8550, train/grad_norm: 5.268335812047553e-08\n",
      "Step: 8550, train/learning_rate: 1.6087577023427002e-05\n",
      "Step: 8550, train/epoch: 2.034745454788208\n",
      "Step: 8560, train/loss: 0.0\n",
      "Step: 8560, train/grad_norm: 1.2077687294720363e-08\n",
      "Step: 8560, train/learning_rate: 1.6047913959482685e-05\n",
      "Step: 8560, train/epoch: 2.0371251106262207\n",
      "Step: 8570, train/loss: 0.0\n",
      "Step: 8570, train/grad_norm: 1.066165054908197e-06\n",
      "Step: 8570, train/learning_rate: 1.6008250895538367e-05\n",
      "Step: 8570, train/epoch: 2.0395050048828125\n",
      "Step: 8580, train/loss: 0.0\n",
      "Step: 8580, train/grad_norm: 5.808769287796167e-07\n",
      "Step: 8580, train/learning_rate: 1.5968586012604646e-05\n",
      "Step: 8580, train/epoch: 2.0418848991394043\n",
      "Step: 8590, train/loss: 0.0\n",
      "Step: 8590, train/grad_norm: 1.3087377226383978e-07\n",
      "Step: 8590, train/learning_rate: 1.592892294866033e-05\n",
      "Step: 8590, train/epoch: 2.044264554977417\n",
      "Step: 8600, train/loss: 0.0\n",
      "Step: 8600, train/grad_norm: 4.280392147393286e-07\n",
      "Step: 8600, train/learning_rate: 1.588925988471601e-05\n",
      "Step: 8600, train/epoch: 2.046644449234009\n",
      "Step: 8610, train/loss: 0.0\n",
      "Step: 8610, train/grad_norm: 3.2230190072368714e-08\n",
      "Step: 8610, train/learning_rate: 1.584959500178229e-05\n",
      "Step: 8610, train/epoch: 2.0490243434906006\n",
      "Step: 8620, train/loss: 0.0032999999821186066\n",
      "Step: 8620, train/grad_norm: 1.6503318533978018e-07\n",
      "Step: 8620, train/learning_rate: 1.5809931937837973e-05\n",
      "Step: 8620, train/epoch: 2.0514039993286133\n",
      "Step: 8630, train/loss: 0.0\n",
      "Step: 8630, train/grad_norm: 0.0008447564905509353\n",
      "Step: 8630, train/learning_rate: 1.5770268873893656e-05\n",
      "Step: 8630, train/epoch: 2.053783893585205\n",
      "Step: 8640, train/loss: 0.0\n",
      "Step: 8640, train/grad_norm: 4.6658149699396745e-07\n",
      "Step: 8640, train/learning_rate: 1.5730603990959935e-05\n",
      "Step: 8640, train/epoch: 2.056163787841797\n",
      "Step: 8650, train/loss: 0.0\n",
      "Step: 8650, train/grad_norm: 8.483963824801322e-07\n",
      "Step: 8650, train/learning_rate: 1.5690940927015617e-05\n",
      "Step: 8650, train/epoch: 2.0585434436798096\n",
      "Step: 8660, train/loss: 0.0\n",
      "Step: 8660, train/grad_norm: 5.651050400956592e-07\n",
      "Step: 8660, train/learning_rate: 1.56512778630713e-05\n",
      "Step: 8660, train/epoch: 2.0609233379364014\n",
      "Step: 8670, train/loss: 0.0\n",
      "Step: 8670, train/grad_norm: 7.985644856489671e-08\n",
      "Step: 8670, train/learning_rate: 1.561161298013758e-05\n",
      "Step: 8670, train/epoch: 2.063303232192993\n",
      "Step: 8680, train/loss: 0.0\n",
      "Step: 8680, train/grad_norm: 4.062620973854791e-06\n",
      "Step: 8680, train/learning_rate: 1.5571949916193262e-05\n",
      "Step: 8680, train/epoch: 2.065683126449585\n",
      "Step: 8690, train/loss: 0.0\n",
      "Step: 8690, train/grad_norm: 3.913785985787399e-05\n",
      "Step: 8690, train/learning_rate: 1.5532286852248944e-05\n",
      "Step: 8690, train/epoch: 2.0680627822875977\n",
      "Step: 8700, train/loss: 0.0\n",
      "Step: 8700, train/grad_norm: 9.570637757860823e-07\n",
      "Step: 8700, train/learning_rate: 1.5492621969315223e-05\n",
      "Step: 8700, train/epoch: 2.0704426765441895\n",
      "Step: 8710, train/loss: 0.0\n",
      "Step: 8710, train/grad_norm: 1.7715163380671584e-07\n",
      "Step: 8710, train/learning_rate: 1.5452958905370906e-05\n",
      "Step: 8710, train/epoch: 2.0728225708007812\n",
      "Step: 8720, train/loss: 0.0\n",
      "Step: 8720, train/grad_norm: 2.427842673569103e-07\n",
      "Step: 8720, train/learning_rate: 1.541329584142659e-05\n",
      "Step: 8720, train/epoch: 2.075202226638794\n",
      "Step: 8730, train/loss: 0.0\n",
      "Step: 8730, train/grad_norm: 5.1993513494608123e-08\n",
      "Step: 8730, train/learning_rate: 1.5373630958492868e-05\n",
      "Step: 8730, train/epoch: 2.0775821208953857\n",
      "Step: 8740, train/loss: 0.0\n",
      "Step: 8740, train/grad_norm: 9.960992883861763e-07\n",
      "Step: 8740, train/learning_rate: 1.533396789454855e-05\n",
      "Step: 8740, train/epoch: 2.0799620151519775\n",
      "Step: 8750, train/loss: 0.0\n",
      "Step: 8750, train/grad_norm: 7.616412176503218e-07\n",
      "Step: 8750, train/learning_rate: 1.5294304830604233e-05\n",
      "Step: 8750, train/epoch: 2.0823416709899902\n",
      "Step: 8760, train/loss: 0.0\n",
      "Step: 8760, train/grad_norm: 2.2154190446599387e-05\n",
      "Step: 8760, train/learning_rate: 1.5254640857165214e-05\n",
      "Step: 8760, train/epoch: 2.084721565246582\n",
      "Step: 8770, train/loss: 0.0\n",
      "Step: 8770, train/grad_norm: 2.634573377235938e-07\n",
      "Step: 8770, train/learning_rate: 1.5214976883726195e-05\n",
      "Step: 8770, train/epoch: 2.087101459503174\n",
      "Step: 8780, train/loss: 0.0\n",
      "Step: 8780, train/grad_norm: 1.997704259792954e-07\n",
      "Step: 8780, train/learning_rate: 1.5175312910287175e-05\n",
      "Step: 8780, train/epoch: 2.0894811153411865\n",
      "Step: 8790, train/loss: 0.0\n",
      "Step: 8790, train/grad_norm: 2.0003457734674157e-07\n",
      "Step: 8790, train/learning_rate: 1.5135649846342858e-05\n",
      "Step: 8790, train/epoch: 2.0918610095977783\n",
      "Step: 8800, train/loss: 0.0\n",
      "Step: 8800, train/grad_norm: 1.4279081028689689e-07\n",
      "Step: 8800, train/learning_rate: 1.5095985872903839e-05\n",
      "Step: 8800, train/epoch: 2.09424090385437\n",
      "Step: 8810, train/loss: 0.0\n",
      "Step: 8810, train/grad_norm: 9.088008482649457e-06\n",
      "Step: 8810, train/learning_rate: 1.5056322808959521e-05\n",
      "Step: 8810, train/epoch: 2.096620559692383\n",
      "Step: 8820, train/loss: 0.0\n",
      "Step: 8820, train/grad_norm: 2.2365857432760095e-07\n",
      "Step: 8820, train/learning_rate: 1.5016658835520502e-05\n",
      "Step: 8820, train/epoch: 2.0990004539489746\n",
      "Step: 8830, train/loss: 0.0\n",
      "Step: 8830, train/grad_norm: 7.666905048608896e-07\n",
      "Step: 8830, train/learning_rate: 1.4976994862081483e-05\n",
      "Step: 8830, train/epoch: 2.1013803482055664\n",
      "Step: 8840, train/loss: 0.0\n",
      "Step: 8840, train/grad_norm: 1.0094266826854437e-06\n",
      "Step: 8840, train/learning_rate: 1.4937331798137166e-05\n",
      "Step: 8840, train/epoch: 2.103760004043579\n",
      "Step: 8850, train/loss: 0.0\n",
      "Step: 8850, train/grad_norm: 9.782515775214051e-08\n",
      "Step: 8850, train/learning_rate: 1.4897667824698146e-05\n",
      "Step: 8850, train/epoch: 2.106139898300171\n",
      "Step: 8860, train/loss: 0.0\n",
      "Step: 8860, train/grad_norm: 1.3557651890039324e-08\n",
      "Step: 8860, train/learning_rate: 1.4858003851259127e-05\n",
      "Step: 8860, train/epoch: 2.1085197925567627\n",
      "Step: 8870, train/loss: 0.0\n",
      "Step: 8870, train/grad_norm: 3.201706277877747e-08\n",
      "Step: 8870, train/learning_rate: 1.481834078731481e-05\n",
      "Step: 8870, train/epoch: 2.1108996868133545\n",
      "Step: 8880, train/loss: 0.0\n",
      "Step: 8880, train/grad_norm: 1.9453734978469583e-07\n",
      "Step: 8880, train/learning_rate: 1.477867681387579e-05\n",
      "Step: 8880, train/epoch: 2.113279342651367\n",
      "Step: 8890, train/loss: 0.0\n",
      "Step: 8890, train/grad_norm: 1.901451440744495e-07\n",
      "Step: 8890, train/learning_rate: 1.4739012840436772e-05\n",
      "Step: 8890, train/epoch: 2.115659236907959\n",
      "Step: 8900, train/loss: 0.0\n",
      "Step: 8900, train/grad_norm: 1.432441877113888e-06\n",
      "Step: 8900, train/learning_rate: 1.4699349776492454e-05\n",
      "Step: 8900, train/epoch: 2.118039131164551\n",
      "Step: 8910, train/loss: 0.0\n",
      "Step: 8910, train/grad_norm: 2.0605713757504418e-07\n",
      "Step: 8910, train/learning_rate: 1.4659685803053435e-05\n",
      "Step: 8910, train/epoch: 2.1204187870025635\n",
      "Step: 8920, train/loss: 0.0\n",
      "Step: 8920, train/grad_norm: 1.2152783668284428e-08\n",
      "Step: 8920, train/learning_rate: 1.4620021829614416e-05\n",
      "Step: 8920, train/epoch: 2.1227986812591553\n",
      "Step: 8930, train/loss: 0.0\n",
      "Step: 8930, train/grad_norm: 2.606211410238757e-06\n",
      "Step: 8930, train/learning_rate: 1.4580358765670098e-05\n",
      "Step: 8930, train/epoch: 2.125178575515747\n",
      "Step: 8940, train/loss: 0.0\n",
      "Step: 8940, train/grad_norm: 2.5745978859958996e-07\n",
      "Step: 8940, train/learning_rate: 1.454069479223108e-05\n",
      "Step: 8940, train/epoch: 2.1275582313537598\n",
      "Step: 8950, train/loss: 0.0\n",
      "Step: 8950, train/grad_norm: 2.921966597568826e-07\n",
      "Step: 8950, train/learning_rate: 1.450103081879206e-05\n",
      "Step: 8950, train/epoch: 2.1299381256103516\n",
      "Step: 8960, train/loss: 0.0\n",
      "Step: 8960, train/grad_norm: 6.907640113240632e-07\n",
      "Step: 8960, train/learning_rate: 1.4461367754847743e-05\n",
      "Step: 8960, train/epoch: 2.1323180198669434\n",
      "Step: 8970, train/loss: 0.0\n",
      "Step: 8970, train/grad_norm: 3.63175468010013e-06\n",
      "Step: 8970, train/learning_rate: 1.4421703781408723e-05\n",
      "Step: 8970, train/epoch: 2.134697675704956\n",
      "Step: 8980, train/loss: 0.0\n",
      "Step: 8980, train/grad_norm: 9.91834667729563e-07\n",
      "Step: 8980, train/learning_rate: 1.4382040717464406e-05\n",
      "Step: 8980, train/epoch: 2.137077569961548\n",
      "Step: 8990, train/loss: 0.0\n",
      "Step: 8990, train/grad_norm: 2.9151072666877553e-08\n",
      "Step: 8990, train/learning_rate: 1.4342376744025387e-05\n",
      "Step: 8990, train/epoch: 2.1394574642181396\n",
      "Step: 9000, train/loss: 0.0\n",
      "Step: 9000, train/grad_norm: 7.382615194728714e-07\n",
      "Step: 9000, train/learning_rate: 1.4302712770586368e-05\n",
      "Step: 9000, train/epoch: 2.1418371200561523\n",
      "Step: 9010, train/loss: 0.0\n",
      "Step: 9010, train/grad_norm: 5.474712949649074e-08\n",
      "Step: 9010, train/learning_rate: 1.426304970664205e-05\n",
      "Step: 9010, train/epoch: 2.144217014312744\n",
      "Step: 9020, train/loss: 0.0\n",
      "Step: 9020, train/grad_norm: 2.7399913960834965e-05\n",
      "Step: 9020, train/learning_rate: 1.4223385733203031e-05\n",
      "Step: 9020, train/epoch: 2.146596908569336\n",
      "Step: 9030, train/loss: 0.0\n",
      "Step: 9030, train/grad_norm: 1.5844027245748293e-07\n",
      "Step: 9030, train/learning_rate: 1.4183721759764012e-05\n",
      "Step: 9030, train/epoch: 2.1489765644073486\n",
      "Step: 9040, train/loss: 0.0\n",
      "Step: 9040, train/grad_norm: 6.119792317349493e-08\n",
      "Step: 9040, train/learning_rate: 1.4144058695819695e-05\n",
      "Step: 9040, train/epoch: 2.1513564586639404\n",
      "Step: 9050, train/loss: 0.0\n",
      "Step: 9050, train/grad_norm: 9.547077297611395e-07\n",
      "Step: 9050, train/learning_rate: 1.4104394722380675e-05\n",
      "Step: 9050, train/epoch: 2.1537363529205322\n",
      "Step: 9060, train/loss: 0.0\n",
      "Step: 9060, train/grad_norm: 5.886004714739101e-07\n",
      "Step: 9060, train/learning_rate: 1.4064730748941656e-05\n",
      "Step: 9060, train/epoch: 2.156116247177124\n",
      "Step: 9070, train/loss: 0.0\n",
      "Step: 9070, train/grad_norm: 1.4681022264539934e-07\n",
      "Step: 9070, train/learning_rate: 1.4025067684997339e-05\n",
      "Step: 9070, train/epoch: 2.1584959030151367\n",
      "Step: 9080, train/loss: 0.0\n",
      "Step: 9080, train/grad_norm: 2.699857759580482e-07\n",
      "Step: 9080, train/learning_rate: 1.398540371155832e-05\n",
      "Step: 9080, train/epoch: 2.1608757972717285\n",
      "Step: 9090, train/loss: 0.0\n",
      "Step: 9090, train/grad_norm: 1.0442103075547493e-06\n",
      "Step: 9090, train/learning_rate: 1.39457397381193e-05\n",
      "Step: 9090, train/epoch: 2.1632556915283203\n",
      "Step: 9100, train/loss: 0.0\n",
      "Step: 9100, train/grad_norm: 0.0017103459686040878\n",
      "Step: 9100, train/learning_rate: 1.3906076674174983e-05\n",
      "Step: 9100, train/epoch: 2.165635347366333\n",
      "Step: 9110, train/loss: 0.0\n",
      "Step: 9110, train/grad_norm: 1.7975933985781012e-07\n",
      "Step: 9110, train/learning_rate: 1.3866412700735964e-05\n",
      "Step: 9110, train/epoch: 2.168015241622925\n",
      "Step: 9120, train/loss: 0.0\n",
      "Step: 9120, train/grad_norm: 1.3053578413746436e-06\n",
      "Step: 9120, train/learning_rate: 1.3826748727296945e-05\n",
      "Step: 9120, train/epoch: 2.1703951358795166\n",
      "Step: 9130, train/loss: 0.19840000569820404\n",
      "Step: 9130, train/grad_norm: 34.629249572753906\n",
      "Step: 9130, train/learning_rate: 1.3787085663352627e-05\n",
      "Step: 9130, train/epoch: 2.1727747917175293\n",
      "Step: 9140, train/loss: 0.01360000018030405\n",
      "Step: 9140, train/grad_norm: 0.25029072165489197\n",
      "Step: 9140, train/learning_rate: 1.3747421689913608e-05\n",
      "Step: 9140, train/epoch: 2.175154685974121\n",
      "Step: 9150, train/loss: 0.0003000000142492354\n",
      "Step: 9150, train/grad_norm: 0.00012378618703223765\n",
      "Step: 9150, train/learning_rate: 1.370775862596929e-05\n",
      "Step: 9150, train/epoch: 2.177534580230713\n",
      "Step: 9160, train/loss: 0.001500000013038516\n",
      "Step: 9160, train/grad_norm: 0.000333303032675758\n",
      "Step: 9160, train/learning_rate: 1.3668094652530272e-05\n",
      "Step: 9160, train/epoch: 2.1799142360687256\n",
      "Step: 9170, train/loss: 0.0\n",
      "Step: 9170, train/grad_norm: 1.8379286075287382e-06\n",
      "Step: 9170, train/learning_rate: 1.3628430679091252e-05\n",
      "Step: 9170, train/epoch: 2.1822941303253174\n",
      "Step: 9180, train/loss: 0.0\n",
      "Step: 9180, train/grad_norm: 6.787193456148088e-07\n",
      "Step: 9180, train/learning_rate: 1.3588767615146935e-05\n",
      "Step: 9180, train/epoch: 2.184674024581909\n",
      "Step: 9190, train/loss: 0.0\n",
      "Step: 9190, train/grad_norm: 1.233456032423419e-06\n",
      "Step: 9190, train/learning_rate: 1.3549103641707916e-05\n",
      "Step: 9190, train/epoch: 2.187053680419922\n",
      "Step: 9200, train/loss: 0.0\n",
      "Step: 9200, train/grad_norm: 2.999808543791005e-07\n",
      "Step: 9200, train/learning_rate: 1.3509439668268897e-05\n",
      "Step: 9200, train/epoch: 2.1894335746765137\n",
      "Step: 9210, train/loss: 0.0\n",
      "Step: 9210, train/grad_norm: 6.467321895797795e-07\n",
      "Step: 9210, train/learning_rate: 1.346977660432458e-05\n",
      "Step: 9210, train/epoch: 2.1918134689331055\n",
      "Step: 9220, train/loss: 0.0\n",
      "Step: 9220, train/grad_norm: 1.064537627826212e-06\n",
      "Step: 9220, train/learning_rate: 1.343011263088556e-05\n",
      "Step: 9220, train/epoch: 2.194193124771118\n",
      "Step: 9230, train/loss: 0.0\n",
      "Step: 9230, train/grad_norm: 2.7263997253612615e-05\n",
      "Step: 9230, train/learning_rate: 1.3390448657446541e-05\n",
      "Step: 9230, train/epoch: 2.19657301902771\n",
      "Step: 9240, train/loss: 0.0\n",
      "Step: 9240, train/grad_norm: 2.084216390585425e-07\n",
      "Step: 9240, train/learning_rate: 1.3350785593502223e-05\n",
      "Step: 9240, train/epoch: 2.1989529132843018\n",
      "Step: 9250, train/loss: 0.0\n",
      "Step: 9250, train/grad_norm: 3.205661414540373e-06\n",
      "Step: 9250, train/learning_rate: 1.3311121620063204e-05\n",
      "Step: 9250, train/epoch: 2.2013328075408936\n",
      "Step: 9260, train/loss: 0.0\n",
      "Step: 9260, train/grad_norm: 1.497843004472088e-05\n",
      "Step: 9260, train/learning_rate: 1.3271457646624185e-05\n",
      "Step: 9260, train/epoch: 2.2037124633789062\n",
      "Step: 9270, train/loss: 0.0\n",
      "Step: 9270, train/grad_norm: 2.184154901385682e-08\n",
      "Step: 9270, train/learning_rate: 1.3231794582679868e-05\n",
      "Step: 9270, train/epoch: 2.206092357635498\n",
      "Step: 9280, train/loss: 0.07270000129938126\n",
      "Step: 9280, train/grad_norm: 2.307835984538542e-07\n",
      "Step: 9280, train/learning_rate: 1.3192130609240849e-05\n",
      "Step: 9280, train/epoch: 2.20847225189209\n",
      "Step: 9290, train/loss: 0.0\n",
      "Step: 9290, train/grad_norm: 5.773244993179105e-07\n",
      "Step: 9290, train/learning_rate: 1.315246663580183e-05\n",
      "Step: 9290, train/epoch: 2.2108519077301025\n",
      "Step: 9300, train/loss: 0.0\n",
      "Step: 9300, train/grad_norm: 5.510816208698088e-06\n",
      "Step: 9300, train/learning_rate: 1.3112803571857512e-05\n",
      "Step: 9300, train/epoch: 2.2132318019866943\n",
      "Step: 9310, train/loss: 0.0\n",
      "Step: 9310, train/grad_norm: 5.188687168811157e-07\n",
      "Step: 9310, train/learning_rate: 1.3073139598418493e-05\n",
      "Step: 9310, train/epoch: 2.215611696243286\n",
      "Step: 9320, train/loss: 0.0\n",
      "Step: 9320, train/grad_norm: 2.7317830131323717e-07\n",
      "Step: 9320, train/learning_rate: 1.3033476534474175e-05\n",
      "Step: 9320, train/epoch: 2.217991352081299\n",
      "Step: 9330, train/loss: 0.0\n",
      "Step: 9330, train/grad_norm: 3.7951454032736365e-06\n",
      "Step: 9330, train/learning_rate: 1.2993812561035156e-05\n",
      "Step: 9330, train/epoch: 2.2203712463378906\n",
      "Step: 9340, train/loss: 0.0\n",
      "Step: 9340, train/grad_norm: 4.788518253917573e-06\n",
      "Step: 9340, train/learning_rate: 1.2954148587596137e-05\n",
      "Step: 9340, train/epoch: 2.2227511405944824\n",
      "Step: 9350, train/loss: 0.0\n",
      "Step: 9350, train/grad_norm: 1.5387269741040654e-05\n",
      "Step: 9350, train/learning_rate: 1.291448552365182e-05\n",
      "Step: 9350, train/epoch: 2.225130796432495\n",
      "Step: 9360, train/loss: 0.0\n",
      "Step: 9360, train/grad_norm: 4.266761607141234e-06\n",
      "Step: 9360, train/learning_rate: 1.28748215502128e-05\n",
      "Step: 9360, train/epoch: 2.227510690689087\n",
      "Step: 9370, train/loss: 0.0\n",
      "Step: 9370, train/grad_norm: 2.402491418251884e-06\n",
      "Step: 9370, train/learning_rate: 1.2835157576773781e-05\n",
      "Step: 9370, train/epoch: 2.2298905849456787\n",
      "Step: 9380, train/loss: 0.0\n",
      "Step: 9380, train/grad_norm: 1.1443446510384092e-06\n",
      "Step: 9380, train/learning_rate: 1.2795494512829464e-05\n",
      "Step: 9380, train/epoch: 2.2322702407836914\n",
      "Step: 9390, train/loss: 0.0\n",
      "Step: 9390, train/grad_norm: 3.4748057942124433e-07\n",
      "Step: 9390, train/learning_rate: 1.2755830539390445e-05\n",
      "Step: 9390, train/epoch: 2.234650135040283\n",
      "Step: 9400, train/loss: 0.0\n",
      "Step: 9400, train/grad_norm: 2.894188071422832e-07\n",
      "Step: 9400, train/learning_rate: 1.2716166565951426e-05\n",
      "Step: 9400, train/epoch: 2.237030029296875\n",
      "Step: 9410, train/loss: 0.0\n",
      "Step: 9410, train/grad_norm: 6.491015938081546e-07\n",
      "Step: 9410, train/learning_rate: 1.2676503502007108e-05\n",
      "Step: 9410, train/epoch: 2.239409923553467\n",
      "Step: 9420, train/loss: 0.0\n",
      "Step: 9420, train/grad_norm: 3.908580038114451e-05\n",
      "Step: 9420, train/learning_rate: 1.2636839528568089e-05\n",
      "Step: 9420, train/epoch: 2.2417895793914795\n",
      "Step: 9430, train/loss: 0.0\n",
      "Step: 9430, train/grad_norm: 1.6387421055696905e-06\n",
      "Step: 9430, train/learning_rate: 1.259717555512907e-05\n",
      "Step: 9430, train/epoch: 2.2441694736480713\n",
      "Step: 9440, train/loss: 0.0\n",
      "Step: 9440, train/grad_norm: 4.692028028330242e-07\n",
      "Step: 9440, train/learning_rate: 1.2557512491184752e-05\n",
      "Step: 9440, train/epoch: 2.246549367904663\n",
      "Step: 9450, train/loss: 0.0\n",
      "Step: 9450, train/grad_norm: 5.6138185755116865e-05\n",
      "Step: 9450, train/learning_rate: 1.2517848517745733e-05\n",
      "Step: 9450, train/epoch: 2.248929023742676\n",
      "Step: 9460, train/loss: 0.0\n",
      "Step: 9460, train/grad_norm: 4.090995389560703e-06\n",
      "Step: 9460, train/learning_rate: 1.2478184544306714e-05\n",
      "Step: 9460, train/epoch: 2.2513089179992676\n",
      "Step: 9470, train/loss: 0.06639999896287918\n",
      "Step: 9470, train/grad_norm: 1.826977751306913e-07\n",
      "Step: 9470, train/learning_rate: 1.2438521480362397e-05\n",
      "Step: 9470, train/epoch: 2.2536888122558594\n",
      "Step: 9480, train/loss: 0.0\n",
      "Step: 9480, train/grad_norm: 7.759087748127058e-05\n",
      "Step: 9480, train/learning_rate: 1.2398857506923378e-05\n",
      "Step: 9480, train/epoch: 2.256068468093872\n",
      "Step: 9490, train/loss: 0.0\n",
      "Step: 9490, train/grad_norm: 3.3966030059673358e-06\n",
      "Step: 9490, train/learning_rate: 1.235919444297906e-05\n",
      "Step: 9490, train/epoch: 2.258448362350464\n",
      "Step: 9500, train/loss: 0.0\n",
      "Step: 9500, train/grad_norm: 7.71327108850528e-07\n",
      "Step: 9500, train/learning_rate: 1.2319530469540041e-05\n",
      "Step: 9500, train/epoch: 2.2608282566070557\n",
      "Step: 9510, train/loss: 0.0\n",
      "Step: 9510, train/grad_norm: 1.258831161976559e-06\n",
      "Step: 9510, train/learning_rate: 1.2279866496101022e-05\n",
      "Step: 9510, train/epoch: 2.2632079124450684\n",
      "Step: 9520, train/loss: 0.0\n",
      "Step: 9520, train/grad_norm: 1.3104305480737821e-06\n",
      "Step: 9520, train/learning_rate: 1.2240203432156704e-05\n",
      "Step: 9520, train/epoch: 2.26558780670166\n",
      "Step: 9530, train/loss: 0.0\n",
      "Step: 9530, train/grad_norm: 3.0764581993025786e-07\n",
      "Step: 9530, train/learning_rate: 1.2200539458717685e-05\n",
      "Step: 9530, train/epoch: 2.267967700958252\n",
      "Step: 9540, train/loss: 0.0\n",
      "Step: 9540, train/grad_norm: 9.660312798587256e-07\n",
      "Step: 9540, train/learning_rate: 1.2160875485278666e-05\n",
      "Step: 9540, train/epoch: 2.2703473567962646\n",
      "Step: 9550, train/loss: 0.0\n",
      "Step: 9550, train/grad_norm: 4.661369530367665e-06\n",
      "Step: 9550, train/learning_rate: 1.2121212421334349e-05\n",
      "Step: 9550, train/epoch: 2.2727272510528564\n",
      "Step: 9560, train/loss: 0.0\n",
      "Step: 9560, train/grad_norm: 1.3558003956859466e-05\n",
      "Step: 9560, train/learning_rate: 1.208154844789533e-05\n",
      "Step: 9560, train/epoch: 2.2751071453094482\n",
      "Step: 9570, train/loss: 0.0\n",
      "Step: 9570, train/grad_norm: 7.716193067608401e-06\n",
      "Step: 9570, train/learning_rate: 1.204188447445631e-05\n",
      "Step: 9570, train/epoch: 2.277486801147461\n",
      "Step: 9580, train/loss: 0.0\n",
      "Step: 9580, train/grad_norm: 2.2691501726512797e-05\n",
      "Step: 9580, train/learning_rate: 1.2002221410511993e-05\n",
      "Step: 9580, train/epoch: 2.2798666954040527\n",
      "Step: 9590, train/loss: 0.0\n",
      "Step: 9590, train/grad_norm: 1.0380680578236934e-05\n",
      "Step: 9590, train/learning_rate: 1.1962557437072974e-05\n",
      "Step: 9590, train/epoch: 2.2822465896606445\n",
      "Step: 9600, train/loss: 0.0\n",
      "Step: 9600, train/grad_norm: 1.391440127918031e-05\n",
      "Step: 9600, train/learning_rate: 1.1922893463633955e-05\n",
      "Step: 9600, train/epoch: 2.2846264839172363\n",
      "Step: 9610, train/loss: 0.0\n",
      "Step: 9610, train/grad_norm: 8.852948929494175e-11\n",
      "Step: 9610, train/learning_rate: 1.1883230399689637e-05\n",
      "Step: 9610, train/epoch: 2.287006139755249\n",
      "Step: 9620, train/loss: 0.0\n",
      "Step: 9620, train/grad_norm: 1.678125227044802e-05\n",
      "Step: 9620, train/learning_rate: 1.1843566426250618e-05\n",
      "Step: 9620, train/epoch: 2.289386034011841\n",
      "Step: 9630, train/loss: 0.0\n",
      "Step: 9630, train/grad_norm: 0.0002509759506210685\n",
      "Step: 9630, train/learning_rate: 1.1803902452811599e-05\n",
      "Step: 9630, train/epoch: 2.2917659282684326\n",
      "Step: 9640, train/loss: 0.0\n",
      "Step: 9640, train/grad_norm: 7.505287794629112e-05\n",
      "Step: 9640, train/learning_rate: 1.1764239388867281e-05\n",
      "Step: 9640, train/epoch: 2.2941455841064453\n",
      "Step: 9650, train/loss: 0.0\n",
      "Step: 9650, train/grad_norm: 3.658377920601197e-07\n",
      "Step: 9650, train/learning_rate: 1.1724575415428262e-05\n",
      "Step: 9650, train/epoch: 2.296525478363037\n",
      "Step: 9660, train/loss: 0.0\n",
      "Step: 9660, train/grad_norm: 4.662669653043849e-06\n",
      "Step: 9660, train/learning_rate: 1.1684912351483945e-05\n",
      "Step: 9660, train/epoch: 2.298905372619629\n",
      "Step: 9670, train/loss: 0.09019999951124191\n",
      "Step: 9670, train/grad_norm: 5.344973942555953e-06\n",
      "Step: 9670, train/learning_rate: 1.1645248378044926e-05\n",
      "Step: 9670, train/epoch: 2.3012850284576416\n",
      "Step: 9680, train/loss: 0.0\n",
      "Step: 9680, train/grad_norm: 5.09535027504171e-07\n",
      "Step: 9680, train/learning_rate: 1.1605584404605906e-05\n",
      "Step: 9680, train/epoch: 2.3036649227142334\n",
      "Step: 9690, train/loss: 0.0\n",
      "Step: 9690, train/grad_norm: 8.29103373689577e-06\n",
      "Step: 9690, train/learning_rate: 1.1565921340661589e-05\n",
      "Step: 9690, train/epoch: 2.306044816970825\n",
      "Step: 9700, train/loss: 0.0\n",
      "Step: 9700, train/grad_norm: 2.6523573524173116e-06\n",
      "Step: 9700, train/learning_rate: 1.152625736722257e-05\n",
      "Step: 9700, train/epoch: 2.308424472808838\n",
      "Step: 9710, train/loss: 0.0\n",
      "Step: 9710, train/grad_norm: 2.67865118530608e-07\n",
      "Step: 9710, train/learning_rate: 1.148659339378355e-05\n",
      "Step: 9710, train/epoch: 2.3108043670654297\n",
      "Step: 9720, train/loss: 9.999999747378752e-05\n",
      "Step: 9720, train/grad_norm: 7.826833581248138e-08\n",
      "Step: 9720, train/learning_rate: 1.1446930329839233e-05\n",
      "Step: 9720, train/epoch: 2.3131842613220215\n",
      "Step: 9730, train/loss: 0.0\n",
      "Step: 9730, train/grad_norm: 9.634192110752338e-07\n",
      "Step: 9730, train/learning_rate: 1.1407266356400214e-05\n",
      "Step: 9730, train/epoch: 2.315563917160034\n",
      "Step: 9740, train/loss: 0.0\n",
      "Step: 9740, train/grad_norm: 3.869255849053843e-08\n",
      "Step: 9740, train/learning_rate: 1.1367602382961195e-05\n",
      "Step: 9740, train/epoch: 2.317943811416626\n",
      "Step: 9750, train/loss: 0.0\n",
      "Step: 9750, train/grad_norm: 1.9895765944966115e-05\n",
      "Step: 9750, train/learning_rate: 1.1327939319016878e-05\n",
      "Step: 9750, train/epoch: 2.3203237056732178\n",
      "Step: 9760, train/loss: 0.0\n",
      "Step: 9760, train/grad_norm: 2.171273763451609e-06\n",
      "Step: 9760, train/learning_rate: 1.1288275345577858e-05\n",
      "Step: 9760, train/epoch: 2.3227033615112305\n",
      "Step: 9770, train/loss: 0.0\n",
      "Step: 9770, train/grad_norm: 5.268548193271272e-07\n",
      "Step: 9770, train/learning_rate: 1.124861137213884e-05\n",
      "Step: 9770, train/epoch: 2.3250832557678223\n",
      "Step: 9780, train/loss: 0.0\n",
      "Step: 9780, train/grad_norm: 7.332826612582721e-08\n",
      "Step: 9780, train/learning_rate: 1.1208948308194522e-05\n",
      "Step: 9780, train/epoch: 2.327463150024414\n",
      "Step: 9790, train/loss: 0.0\n",
      "Step: 9790, train/grad_norm: 4.6792050056865264e-07\n",
      "Step: 9790, train/learning_rate: 1.1169284334755503e-05\n",
      "Step: 9790, train/epoch: 2.329843044281006\n",
      "Step: 9800, train/loss: 0.0\n",
      "Step: 9800, train/grad_norm: 5.5815075228338173e-08\n",
      "Step: 9800, train/learning_rate: 1.1129620361316483e-05\n",
      "Step: 9800, train/epoch: 2.3322227001190186\n",
      "Step: 9810, train/loss: 0.0\n",
      "Step: 9810, train/grad_norm: 4.587133844324853e-06\n",
      "Step: 9810, train/learning_rate: 1.1089957297372166e-05\n",
      "Step: 9810, train/epoch: 2.3346025943756104\n",
      "Step: 9820, train/loss: 0.0\n",
      "Step: 9820, train/grad_norm: 5.062134960098774e-07\n",
      "Step: 9820, train/learning_rate: 1.1050293323933147e-05\n",
      "Step: 9820, train/epoch: 2.336982488632202\n",
      "Step: 9830, train/loss: 0.0\n",
      "Step: 9830, train/grad_norm: 2.27811483455298e-07\n",
      "Step: 9830, train/learning_rate: 1.101063025998883e-05\n",
      "Step: 9830, train/epoch: 2.339362144470215\n",
      "Step: 9840, train/loss: 0.0\n",
      "Step: 9840, train/grad_norm: 2.6520603569224477e-05\n",
      "Step: 9840, train/learning_rate: 1.097096628654981e-05\n",
      "Step: 9840, train/epoch: 2.3417420387268066\n",
      "Step: 9850, train/loss: 0.0\n",
      "Step: 9850, train/grad_norm: 5.059769137005787e-07\n",
      "Step: 9850, train/learning_rate: 1.0931302313110791e-05\n",
      "Step: 9850, train/epoch: 2.3441219329833984\n",
      "Step: 9860, train/loss: 0.0\n",
      "Step: 9860, train/grad_norm: 1.5803807400516234e-06\n",
      "Step: 9860, train/learning_rate: 1.0891639249166474e-05\n",
      "Step: 9860, train/epoch: 2.346501588821411\n",
      "Step: 9870, train/loss: 0.0\n",
      "Step: 9870, train/grad_norm: 3.005767212016508e-05\n",
      "Step: 9870, train/learning_rate: 1.0851975275727455e-05\n",
      "Step: 9870, train/epoch: 2.348881483078003\n",
      "Step: 9880, train/loss: 0.0\n",
      "Step: 9880, train/grad_norm: 0.0002520062553230673\n",
      "Step: 9880, train/learning_rate: 1.0812311302288435e-05\n",
      "Step: 9880, train/epoch: 2.3512613773345947\n",
      "Step: 9890, train/loss: 0.0\n",
      "Step: 9890, train/grad_norm: 6.165467993923812e-07\n",
      "Step: 9890, train/learning_rate: 1.0772648238344118e-05\n",
      "Step: 9890, train/epoch: 2.3536410331726074\n",
      "Step: 9900, train/loss: 0.0\n",
      "Step: 9900, train/grad_norm: 8.35591364989341e-08\n",
      "Step: 9900, train/learning_rate: 1.0732984264905099e-05\n",
      "Step: 9900, train/epoch: 2.356020927429199\n",
      "Step: 9910, train/loss: 0.0\n",
      "Step: 9910, train/grad_norm: 1.8694089476412046e-06\n",
      "Step: 9910, train/learning_rate: 1.069332029146608e-05\n",
      "Step: 9910, train/epoch: 2.358400821685791\n",
      "Step: 9920, train/loss: 0.0\n",
      "Step: 9920, train/grad_norm: 5.361815738069708e-07\n",
      "Step: 9920, train/learning_rate: 1.0653657227521762e-05\n",
      "Step: 9920, train/epoch: 2.3607804775238037\n",
      "Step: 9930, train/loss: 0.0\n",
      "Step: 9930, train/grad_norm: 3.6210478810971836e-06\n",
      "Step: 9930, train/learning_rate: 1.0613993254082743e-05\n",
      "Step: 9930, train/epoch: 2.3631603717803955\n",
      "Step: 9940, train/loss: 0.0\n",
      "Step: 9940, train/grad_norm: 5.213004624238238e-05\n",
      "Step: 9940, train/learning_rate: 1.0574329280643724e-05\n",
      "Step: 9940, train/epoch: 2.3655402660369873\n",
      "Step: 9950, train/loss: 0.0\n",
      "Step: 9950, train/grad_norm: 7.039316642476479e-06\n",
      "Step: 9950, train/learning_rate: 1.0534666216699407e-05\n",
      "Step: 9950, train/epoch: 2.367919921875\n",
      "Step: 9960, train/loss: 0.0\n",
      "Step: 9960, train/grad_norm: 2.991826519860297e-08\n",
      "Step: 9960, train/learning_rate: 1.0495002243260387e-05\n",
      "Step: 9960, train/epoch: 2.370299816131592\n",
      "Step: 9970, train/loss: 0.0\n",
      "Step: 9970, train/grad_norm: 1.3078745553229965e-07\n",
      "Step: 9970, train/learning_rate: 1.045533917931607e-05\n",
      "Step: 9970, train/epoch: 2.3726797103881836\n",
      "Step: 9980, train/loss: 0.0\n",
      "Step: 9980, train/grad_norm: 3.6779306356038433e-06\n",
      "Step: 9980, train/learning_rate: 1.041567520587705e-05\n",
      "Step: 9980, train/epoch: 2.3750596046447754\n",
      "Step: 9990, train/loss: 0.0\n",
      "Step: 9990, train/grad_norm: 2.8324691356829135e-06\n",
      "Step: 9990, train/learning_rate: 1.0376011232438032e-05\n",
      "Step: 9990, train/epoch: 2.377439260482788\n",
      "Step: 10000, train/loss: 0.0\n",
      "Step: 10000, train/grad_norm: 6.482704293375718e-07\n",
      "Step: 10000, train/learning_rate: 1.0336348168493714e-05\n",
      "Step: 10000, train/epoch: 2.37981915473938\n",
      "Step: 10010, train/loss: 0.0\n",
      "Step: 10010, train/grad_norm: 4.652980862829281e-07\n",
      "Step: 10010, train/learning_rate: 1.0296684195054695e-05\n",
      "Step: 10010, train/epoch: 2.3821990489959717\n",
      "Step: 10020, train/loss: 0.0\n",
      "Step: 10020, train/grad_norm: 7.820308383088559e-07\n",
      "Step: 10020, train/learning_rate: 1.0257020221615676e-05\n",
      "Step: 10020, train/epoch: 2.3845787048339844\n",
      "Step: 10030, train/loss: 0.0\n",
      "Step: 10030, train/grad_norm: 1.096255004995328e-06\n",
      "Step: 10030, train/learning_rate: 1.0217357157671358e-05\n",
      "Step: 10030, train/epoch: 2.386958599090576\n",
      "Step: 10040, train/loss: 0.0\n",
      "Step: 10040, train/grad_norm: 0.0016773123061284423\n",
      "Step: 10040, train/learning_rate: 1.017769318423234e-05\n",
      "Step: 10040, train/epoch: 2.389338493347168\n",
      "Step: 10050, train/loss: 0.0\n",
      "Step: 10050, train/grad_norm: 3.384986939636292e-07\n",
      "Step: 10050, train/learning_rate: 1.013802921079332e-05\n",
      "Step: 10050, train/epoch: 2.3917181491851807\n",
      "Step: 10060, train/loss: 0.0\n",
      "Step: 10060, train/grad_norm: 2.0152497199887875e-06\n",
      "Step: 10060, train/learning_rate: 1.0098366146849003e-05\n",
      "Step: 10060, train/epoch: 2.3940980434417725\n",
      "Step: 10070, train/loss: 0.0\n",
      "Step: 10070, train/grad_norm: 1.8408522350910062e-07\n",
      "Step: 10070, train/learning_rate: 1.0058702173409984e-05\n",
      "Step: 10070, train/epoch: 2.3964779376983643\n",
      "Step: 10080, train/loss: 0.0\n",
      "Step: 10080, train/grad_norm: 2.6922333518086816e-07\n",
      "Step: 10080, train/learning_rate: 1.0019038199970964e-05\n",
      "Step: 10080, train/epoch: 2.398857593536377\n",
      "Step: 10090, train/loss: 0.0\n",
      "Step: 10090, train/grad_norm: 1.8346239016864274e-07\n",
      "Step: 10090, train/learning_rate: 9.979375136026647e-06\n",
      "Step: 10090, train/epoch: 2.4012374877929688\n",
      "Step: 10100, train/loss: 0.0\n",
      "Step: 10100, train/grad_norm: 1.000929387373617e-06\n",
      "Step: 10100, train/learning_rate: 9.939711162587628e-06\n",
      "Step: 10100, train/epoch: 2.4036173820495605\n",
      "Step: 10110, train/loss: 0.0\n",
      "Step: 10110, train/grad_norm: 2.557783034262684e-07\n",
      "Step: 10110, train/learning_rate: 9.900047189148609e-06\n",
      "Step: 10110, train/epoch: 2.4059970378875732\n",
      "Step: 10120, train/loss: 0.0\n",
      "Step: 10120, train/grad_norm: 7.102478775777854e-06\n",
      "Step: 10120, train/learning_rate: 9.860384125204291e-06\n",
      "Step: 10120, train/epoch: 2.408376932144165\n",
      "Step: 10130, train/loss: 0.0\n",
      "Step: 10130, train/grad_norm: 1.5277493048415636e-06\n",
      "Step: 10130, train/learning_rate: 9.820720151765272e-06\n",
      "Step: 10130, train/epoch: 2.410756826400757\n",
      "Step: 10140, train/loss: 0.0\n",
      "Step: 10140, train/grad_norm: 4.4590109382625087e-07\n",
      "Step: 10140, train/learning_rate: 9.781057087820955e-06\n",
      "Step: 10140, train/epoch: 2.4131367206573486\n",
      "Step: 10150, train/loss: 0.0\n",
      "Step: 10150, train/grad_norm: 2.444847950755502e-07\n",
      "Step: 10150, train/learning_rate: 9.741393114381935e-06\n",
      "Step: 10150, train/epoch: 2.4155163764953613\n",
      "Step: 10160, train/loss: 0.0\n",
      "Step: 10160, train/grad_norm: 1.8501650345115195e-07\n",
      "Step: 10160, train/learning_rate: 9.701729140942916e-06\n",
      "Step: 10160, train/epoch: 2.417896270751953\n",
      "Step: 10170, train/loss: 0.0\n",
      "Step: 10170, train/grad_norm: 1.0219143860012991e-06\n",
      "Step: 10170, train/learning_rate: 9.662066076998599e-06\n",
      "Step: 10170, train/epoch: 2.420276165008545\n",
      "Step: 10180, train/loss: 0.0\n",
      "Step: 10180, train/grad_norm: 1.2140411342898005e-07\n",
      "Step: 10180, train/learning_rate: 9.62240210355958e-06\n",
      "Step: 10180, train/epoch: 2.4226558208465576\n",
      "Step: 10190, train/loss: 0.0\n",
      "Step: 10190, train/grad_norm: 0.000639421574305743\n",
      "Step: 10190, train/learning_rate: 9.58273813012056e-06\n",
      "Step: 10190, train/epoch: 2.4250357151031494\n",
      "Step: 10200, train/loss: 0.0\n",
      "Step: 10200, train/grad_norm: 9.104043101615389e-07\n",
      "Step: 10200, train/learning_rate: 9.543075066176243e-06\n",
      "Step: 10200, train/epoch: 2.427415609359741\n",
      "Step: 10210, train/loss: 0.0\n",
      "Step: 10210, train/grad_norm: 1.2311015780142043e-05\n",
      "Step: 10210, train/learning_rate: 9.503411092737224e-06\n",
      "Step: 10210, train/epoch: 2.429795265197754\n",
      "Step: 10220, train/loss: 0.0\n",
      "Step: 10220, train/grad_norm: 3.8063133160903817e-06\n",
      "Step: 10220, train/learning_rate: 9.463747119298205e-06\n",
      "Step: 10220, train/epoch: 2.4321751594543457\n",
      "Step: 10230, train/loss: 0.0\n",
      "Step: 10230, train/grad_norm: 5.879660420760047e-06\n",
      "Step: 10230, train/learning_rate: 9.424084055353887e-06\n",
      "Step: 10230, train/epoch: 2.4345550537109375\n",
      "Step: 10240, train/loss: 0.0\n",
      "Step: 10240, train/grad_norm: 5.565618721448118e-06\n",
      "Step: 10240, train/learning_rate: 9.384420081914868e-06\n",
      "Step: 10240, train/epoch: 2.43693470954895\n",
      "Step: 10250, train/loss: 0.0\n",
      "Step: 10250, train/grad_norm: 0.006035094149410725\n",
      "Step: 10250, train/learning_rate: 9.344756108475849e-06\n",
      "Step: 10250, train/epoch: 2.439314603805542\n",
      "Step: 10260, train/loss: 0.0\n",
      "Step: 10260, train/grad_norm: 0.0002137732517439872\n",
      "Step: 10260, train/learning_rate: 9.305093044531532e-06\n",
      "Step: 10260, train/epoch: 2.441694498062134\n",
      "Step: 10270, train/loss: 0.0\n",
      "Step: 10270, train/grad_norm: 0.00021794912754558027\n",
      "Step: 10270, train/learning_rate: 9.265429071092512e-06\n",
      "Step: 10270, train/epoch: 2.4440741539001465\n",
      "Step: 10280, train/loss: 0.0\n",
      "Step: 10280, train/grad_norm: 4.948813980831801e-08\n",
      "Step: 10280, train/learning_rate: 9.225765097653493e-06\n",
      "Step: 10280, train/epoch: 2.4464540481567383\n",
      "Step: 10290, train/loss: 0.0\n",
      "Step: 10290, train/grad_norm: 2.3008040272998187e-07\n",
      "Step: 10290, train/learning_rate: 9.186102033709176e-06\n",
      "Step: 10290, train/epoch: 2.44883394241333\n",
      "Step: 10300, train/loss: 0.0\n",
      "Step: 10300, train/grad_norm: 1.4365002698468743e-07\n",
      "Step: 10300, train/learning_rate: 9.146438060270157e-06\n",
      "Step: 10300, train/epoch: 2.4512135982513428\n",
      "Step: 10310, train/loss: 0.0\n",
      "Step: 10310, train/grad_norm: 1.3414451416338125e-07\n",
      "Step: 10310, train/learning_rate: 9.10677499632584e-06\n",
      "Step: 10310, train/epoch: 2.4535934925079346\n",
      "Step: 10320, train/loss: 0.0\n",
      "Step: 10320, train/grad_norm: 1.3607226492240443e-06\n",
      "Step: 10320, train/learning_rate: 9.06711102288682e-06\n",
      "Step: 10320, train/epoch: 2.4559733867645264\n",
      "Step: 10330, train/loss: 0.0\n",
      "Step: 10330, train/grad_norm: 5.219321337790461e-06\n",
      "Step: 10330, train/learning_rate: 9.027447049447801e-06\n",
      "Step: 10330, train/epoch: 2.458353281021118\n",
      "Step: 10340, train/loss: 0.0\n",
      "Step: 10340, train/grad_norm: 3.5169216516806046e-07\n",
      "Step: 10340, train/learning_rate: 8.987783985503484e-06\n",
      "Step: 10340, train/epoch: 2.460732936859131\n",
      "Step: 10350, train/loss: 0.0\n",
      "Step: 10350, train/grad_norm: 1.0125093012902653e-06\n",
      "Step: 10350, train/learning_rate: 8.948120012064464e-06\n",
      "Step: 10350, train/epoch: 2.4631128311157227\n",
      "Step: 10360, train/loss: 0.0\n",
      "Step: 10360, train/grad_norm: 2.539302101922658e-07\n",
      "Step: 10360, train/learning_rate: 8.908456038625445e-06\n",
      "Step: 10360, train/epoch: 2.4654927253723145\n",
      "Step: 10370, train/loss: 0.0\n",
      "Step: 10370, train/grad_norm: 4.936830464430386e-06\n",
      "Step: 10370, train/learning_rate: 8.868792974681128e-06\n",
      "Step: 10370, train/epoch: 2.467872381210327\n",
      "Step: 10380, train/loss: 0.0\n",
      "Step: 10380, train/grad_norm: 1.9772008386098605e-07\n",
      "Step: 10380, train/learning_rate: 8.829129001242109e-06\n",
      "Step: 10380, train/epoch: 2.470252275466919\n",
      "Step: 10390, train/loss: 0.0\n",
      "Step: 10390, train/grad_norm: 1.6146250345627777e-06\n",
      "Step: 10390, train/learning_rate: 8.78946502780309e-06\n",
      "Step: 10390, train/epoch: 2.4726321697235107\n",
      "Step: 10400, train/loss: 0.0\n",
      "Step: 10400, train/grad_norm: 1.5228989695970085e-06\n",
      "Step: 10400, train/learning_rate: 8.749801963858772e-06\n",
      "Step: 10400, train/epoch: 2.4750118255615234\n",
      "Step: 10410, train/loss: 0.0\n",
      "Step: 10410, train/grad_norm: 8.657108082843479e-06\n",
      "Step: 10410, train/learning_rate: 8.710137990419753e-06\n",
      "Step: 10410, train/epoch: 2.4773917198181152\n",
      "Step: 10420, train/loss: 0.0\n",
      "Step: 10420, train/grad_norm: 2.844574886751161e-08\n",
      "Step: 10420, train/learning_rate: 8.670474016980734e-06\n",
      "Step: 10420, train/epoch: 2.479771614074707\n",
      "Step: 10430, train/loss: 0.0\n",
      "Step: 10430, train/grad_norm: 1.3841111012879992e-06\n",
      "Step: 10430, train/learning_rate: 8.630810953036416e-06\n",
      "Step: 10430, train/epoch: 2.4821512699127197\n",
      "Step: 10440, train/loss: 0.0\n",
      "Step: 10440, train/grad_norm: 3.4282702472410165e-06\n",
      "Step: 10440, train/learning_rate: 8.591146979597397e-06\n",
      "Step: 10440, train/epoch: 2.4845311641693115\n",
      "Step: 10450, train/loss: 0.0\n",
      "Step: 10450, train/grad_norm: 2.5465351427556016e-07\n",
      "Step: 10450, train/learning_rate: 8.551483006158378e-06\n",
      "Step: 10450, train/epoch: 2.4869110584259033\n",
      "Step: 10460, train/loss: 0.0\n",
      "Step: 10460, train/grad_norm: 4.1829505903479003e-07\n",
      "Step: 10460, train/learning_rate: 8.51181994221406e-06\n",
      "Step: 10460, train/epoch: 2.489290714263916\n",
      "Step: 10470, train/loss: 0.0\n",
      "Step: 10470, train/grad_norm: 2.0382291040732525e-05\n",
      "Step: 10470, train/learning_rate: 8.472155968775041e-06\n",
      "Step: 10470, train/epoch: 2.491670608520508\n",
      "Step: 10480, train/loss: 0.0\n",
      "Step: 10480, train/grad_norm: 1.4772244583127758e-07\n",
      "Step: 10480, train/learning_rate: 8.432492904830724e-06\n",
      "Step: 10480, train/epoch: 2.4940505027770996\n",
      "Step: 10490, train/loss: 0.0\n",
      "Step: 10490, train/grad_norm: 7.469992624464794e-08\n",
      "Step: 10490, train/learning_rate: 8.392828931391705e-06\n",
      "Step: 10490, train/epoch: 2.4964301586151123\n",
      "Step: 10500, train/loss: 0.0\n",
      "Step: 10500, train/grad_norm: 1.5229629752866458e-06\n",
      "Step: 10500, train/learning_rate: 8.353164957952686e-06\n",
      "Step: 10500, train/epoch: 2.498810052871704\n",
      "Step: 10510, train/loss: 0.0\n",
      "Step: 10510, train/grad_norm: 5.297832217365794e-07\n",
      "Step: 10510, train/learning_rate: 8.313501894008368e-06\n",
      "Step: 10510, train/epoch: 2.501189947128296\n",
      "Step: 10520, train/loss: 0.0\n",
      "Step: 10520, train/grad_norm: 2.8621462888622773e-07\n",
      "Step: 10520, train/learning_rate: 8.273837920569349e-06\n",
      "Step: 10520, train/epoch: 2.5035698413848877\n",
      "Step: 10530, train/loss: 0.0\n",
      "Step: 10530, train/grad_norm: 7.353266369136691e-07\n",
      "Step: 10530, train/learning_rate: 8.23417394713033e-06\n",
      "Step: 10530, train/epoch: 2.5059494972229004\n",
      "Step: 10540, train/loss: 0.0\n",
      "Step: 10540, train/grad_norm: 8.502826176481904e-07\n",
      "Step: 10540, train/learning_rate: 8.194510883186013e-06\n",
      "Step: 10540, train/epoch: 2.508329391479492\n",
      "Step: 10550, train/loss: 0.0\n",
      "Step: 10550, train/grad_norm: 2.918567645338044e-07\n",
      "Step: 10550, train/learning_rate: 8.154846909746993e-06\n",
      "Step: 10550, train/epoch: 2.510709285736084\n",
      "Step: 10560, train/loss: 0.0\n",
      "Step: 10560, train/grad_norm: 1.2936991424794542e-06\n",
      "Step: 10560, train/learning_rate: 8.115182936307974e-06\n",
      "Step: 10560, train/epoch: 2.5130889415740967\n",
      "Step: 10570, train/loss: 0.0\n",
      "Step: 10570, train/grad_norm: 1.3533165201806696e-06\n",
      "Step: 10570, train/learning_rate: 8.075519872363657e-06\n",
      "Step: 10570, train/epoch: 2.5154688358306885\n",
      "Step: 10580, train/loss: 0.0\n",
      "Step: 10580, train/grad_norm: 1.9227012160172308e-07\n",
      "Step: 10580, train/learning_rate: 8.035855898924638e-06\n",
      "Step: 10580, train/epoch: 2.5178487300872803\n",
      "Step: 10590, train/loss: 0.0\n",
      "Step: 10590, train/grad_norm: 5.580853894571192e-07\n",
      "Step: 10590, train/learning_rate: 7.996191925485618e-06\n",
      "Step: 10590, train/epoch: 2.520228385925293\n",
      "Step: 10600, train/loss: 0.0\n",
      "Step: 10600, train/grad_norm: 1.5223930915908568e-07\n",
      "Step: 10600, train/learning_rate: 7.956528861541301e-06\n",
      "Step: 10600, train/epoch: 2.5226082801818848\n",
      "Step: 10610, train/loss: 0.0\n",
      "Step: 10610, train/grad_norm: 3.6252820336812874e-06\n",
      "Step: 10610, train/learning_rate: 7.916864888102282e-06\n",
      "Step: 10610, train/epoch: 2.5249881744384766\n",
      "Step: 10620, train/loss: 0.0\n",
      "Step: 10620, train/grad_norm: 2.769223215182137e-07\n",
      "Step: 10620, train/learning_rate: 7.877200914663263e-06\n",
      "Step: 10620, train/epoch: 2.5273678302764893\n",
      "Step: 10630, train/loss: 0.0\n",
      "Step: 10630, train/grad_norm: 5.479060405377822e-07\n",
      "Step: 10630, train/learning_rate: 7.837537850718945e-06\n",
      "Step: 10630, train/epoch: 2.529747724533081\n",
      "Step: 10640, train/loss: 0.004800000227987766\n",
      "Step: 10640, train/grad_norm: 3.6123840345680946e-06\n",
      "Step: 10640, train/learning_rate: 7.797873877279926e-06\n",
      "Step: 10640, train/epoch: 2.532127618789673\n",
      "Step: 10650, train/loss: 0.0\n",
      "Step: 10650, train/grad_norm: 7.888422146606899e-07\n",
      "Step: 10650, train/learning_rate: 7.758210813335609e-06\n",
      "Step: 10650, train/epoch: 2.5345072746276855\n",
      "Step: 10660, train/loss: 0.0\n",
      "Step: 10660, train/grad_norm: 5.239828055891849e-07\n",
      "Step: 10660, train/learning_rate: 7.71854683989659e-06\n",
      "Step: 10660, train/epoch: 2.5368871688842773\n",
      "Step: 10670, train/loss: 0.0\n",
      "Step: 10670, train/grad_norm: 1.0192672306175155e-07\n",
      "Step: 10670, train/learning_rate: 7.67888286645757e-06\n",
      "Step: 10670, train/epoch: 2.539267063140869\n",
      "Step: 10680, train/loss: 0.0\n",
      "Step: 10680, train/grad_norm: 1.3229681599113974e-06\n",
      "Step: 10680, train/learning_rate: 7.639219802513253e-06\n",
      "Step: 10680, train/epoch: 2.541646718978882\n",
      "Step: 10690, train/loss: 0.0\n",
      "Step: 10690, train/grad_norm: 4.755667646350048e-07\n",
      "Step: 10690, train/learning_rate: 7.599555829074234e-06\n",
      "Step: 10690, train/epoch: 2.5440266132354736\n",
      "Step: 10700, train/loss: 0.0\n",
      "Step: 10700, train/grad_norm: 7.881952797106351e-07\n",
      "Step: 10700, train/learning_rate: 7.5598923103825655e-06\n",
      "Step: 10700, train/epoch: 2.5464065074920654\n",
      "Step: 10710, train/loss: 0.0\n",
      "Step: 10710, train/grad_norm: 1.2380218095131568e-06\n",
      "Step: 10710, train/learning_rate: 7.520228336943546e-06\n",
      "Step: 10710, train/epoch: 2.5487864017486572\n",
      "Step: 10720, train/loss: 0.0\n",
      "Step: 10720, train/grad_norm: 1.6336062458321976e-07\n",
      "Step: 10720, train/learning_rate: 7.480564818251878e-06\n",
      "Step: 10720, train/epoch: 2.55116605758667\n",
      "Step: 10730, train/loss: 0.0\n",
      "Step: 10730, train/grad_norm: 0.0008340034983120859\n",
      "Step: 10730, train/learning_rate: 7.44090129956021e-06\n",
      "Step: 10730, train/epoch: 2.5535459518432617\n",
      "Step: 10740, train/loss: 0.0\n",
      "Step: 10740, train/grad_norm: 0.0001039263152051717\n",
      "Step: 10740, train/learning_rate: 7.4012373261211906e-06\n",
      "Step: 10740, train/epoch: 2.5559258460998535\n",
      "Step: 10750, train/loss: 0.0\n",
      "Step: 10750, train/grad_norm: 3.7158989130148257e-07\n",
      "Step: 10750, train/learning_rate: 7.361573807429522e-06\n",
      "Step: 10750, train/epoch: 2.558305501937866\n",
      "Step: 10760, train/loss: 0.0\n",
      "Step: 10760, train/grad_norm: 5.509511424861557e-07\n",
      "Step: 10760, train/learning_rate: 7.321910288737854e-06\n",
      "Step: 10760, train/epoch: 2.560685396194458\n",
      "Step: 10770, train/loss: 0.0\n",
      "Step: 10770, train/grad_norm: 0.00012075529957655817\n",
      "Step: 10770, train/learning_rate: 7.282246770046186e-06\n",
      "Step: 10770, train/epoch: 2.56306529045105\n",
      "Step: 10780, train/loss: 0.0\n",
      "Step: 10780, train/grad_norm: 1.809351668491388e-09\n",
      "Step: 10780, train/learning_rate: 7.2425827966071665e-06\n",
      "Step: 10780, train/epoch: 2.5654449462890625\n",
      "Step: 10790, train/loss: 0.0\n",
      "Step: 10790, train/grad_norm: 5.031588898418704e-06\n",
      "Step: 10790, train/learning_rate: 7.202919277915498e-06\n",
      "Step: 10790, train/epoch: 2.5678248405456543\n",
      "Step: 10800, train/loss: 0.0\n",
      "Step: 10800, train/grad_norm: 3.506064061298275e-08\n",
      "Step: 10800, train/learning_rate: 7.16325575922383e-06\n",
      "Step: 10800, train/epoch: 2.570204734802246\n",
      "Step: 10810, train/loss: 0.0\n",
      "Step: 10810, train/grad_norm: 2.0599939887233631e-07\n",
      "Step: 10810, train/learning_rate: 7.123591785784811e-06\n",
      "Step: 10810, train/epoch: 2.572584390640259\n",
      "Step: 10820, train/loss: 0.0\n",
      "Step: 10820, train/grad_norm: 1.3292786206875462e-05\n",
      "Step: 10820, train/learning_rate: 7.0839282670931425e-06\n",
      "Step: 10820, train/epoch: 2.5749642848968506\n",
      "Step: 10830, train/loss: 0.0\n",
      "Step: 10830, train/grad_norm: 6.507860348392569e-07\n",
      "Step: 10830, train/learning_rate: 7.044264748401474e-06\n",
      "Step: 10830, train/epoch: 2.5773441791534424\n",
      "Step: 10840, train/loss: 0.0\n",
      "Step: 10840, train/grad_norm: 3.336818394927832e-07\n",
      "Step: 10840, train/learning_rate: 7.004600774962455e-06\n",
      "Step: 10840, train/epoch: 2.579723834991455\n",
      "Step: 10850, train/loss: 0.0\n",
      "Step: 10850, train/grad_norm: 1.045780663844198e-06\n",
      "Step: 10850, train/learning_rate: 6.964937256270787e-06\n",
      "Step: 10850, train/epoch: 2.582103729248047\n",
      "Step: 10860, train/loss: 0.0\n",
      "Step: 10860, train/grad_norm: 6.00029920860834e-07\n",
      "Step: 10860, train/learning_rate: 6.9252737375791185e-06\n",
      "Step: 10860, train/epoch: 2.5844836235046387\n",
      "Step: 10870, train/loss: 0.0\n",
      "Step: 10870, train/grad_norm: 3.1646384286432294e-06\n",
      "Step: 10870, train/learning_rate: 6.88561021888745e-06\n",
      "Step: 10870, train/epoch: 2.5868632793426514\n",
      "Step: 10880, train/loss: 0.0\n",
      "Step: 10880, train/grad_norm: 5.332294676918536e-07\n",
      "Step: 10880, train/learning_rate: 6.845946245448431e-06\n",
      "Step: 10880, train/epoch: 2.589243173599243\n",
      "Step: 10890, train/loss: 9.999999747378752e-05\n",
      "Step: 10890, train/grad_norm: 4.04497910722057e-07\n",
      "Step: 10890, train/learning_rate: 6.806282726756763e-06\n",
      "Step: 10890, train/epoch: 2.591623067855835\n",
      "Step: 10900, train/loss: 0.0\n",
      "Step: 10900, train/grad_norm: 2.7579406491895497e-07\n",
      "Step: 10900, train/learning_rate: 6.7666192080650944e-06\n",
      "Step: 10900, train/epoch: 2.5940029621124268\n",
      "Step: 10910, train/loss: 0.0\n",
      "Step: 10910, train/grad_norm: 4.600281044986332e-08\n",
      "Step: 10910, train/learning_rate: 6.726955234626075e-06\n",
      "Step: 10910, train/epoch: 2.5963826179504395\n",
      "Step: 10920, train/loss: 0.0\n",
      "Step: 10920, train/grad_norm: 6.151910270091321e-07\n",
      "Step: 10920, train/learning_rate: 6.687291715934407e-06\n",
      "Step: 10920, train/epoch: 2.5987625122070312\n",
      "Step: 10930, train/loss: 0.0\n",
      "Step: 10930, train/grad_norm: 4.397153361423989e-07\n",
      "Step: 10930, train/learning_rate: 6.647628197242739e-06\n",
      "Step: 10930, train/epoch: 2.601142406463623\n",
      "Step: 10940, train/loss: 0.0\n",
      "Step: 10940, train/grad_norm: 3.622941733283369e-07\n",
      "Step: 10940, train/learning_rate: 6.60796467855107e-06\n",
      "Step: 10940, train/epoch: 2.6035220623016357\n",
      "Step: 10950, train/loss: 0.0\n",
      "Step: 10950, train/grad_norm: 1.6635868860248593e-06\n",
      "Step: 10950, train/learning_rate: 6.568300705112051e-06\n",
      "Step: 10950, train/epoch: 2.6059019565582275\n",
      "Step: 10960, train/loss: 0.0\n",
      "Step: 10960, train/grad_norm: 3.857084323044546e-07\n",
      "Step: 10960, train/learning_rate: 6.528637186420383e-06\n",
      "Step: 10960, train/epoch: 2.6082818508148193\n",
      "Step: 10970, train/loss: 0.0\n",
      "Step: 10970, train/grad_norm: 3.0392220651265234e-07\n",
      "Step: 10970, train/learning_rate: 6.488973667728715e-06\n",
      "Step: 10970, train/epoch: 2.610661506652832\n",
      "Step: 10980, train/loss: 0.0\n",
      "Step: 10980, train/grad_norm: 1.0759842552943155e-05\n",
      "Step: 10980, train/learning_rate: 6.4493096942896955e-06\n",
      "Step: 10980, train/epoch: 2.613041400909424\n",
      "Step: 10990, train/loss: 0.0\n",
      "Step: 10990, train/grad_norm: 1.4510881385376706e-07\n",
      "Step: 10990, train/learning_rate: 6.409646175598027e-06\n",
      "Step: 10990, train/epoch: 2.6154212951660156\n",
      "Step: 11000, train/loss: 0.0\n",
      "Step: 11000, train/grad_norm: 1.111689653043868e-05\n",
      "Step: 11000, train/learning_rate: 6.369982656906359e-06\n",
      "Step: 11000, train/epoch: 2.6178009510040283\n",
      "Step: 11010, train/loss: 0.0\n",
      "Step: 11010, train/grad_norm: 5.385870167629037e-07\n",
      "Step: 11010, train/learning_rate: 6.33031868346734e-06\n",
      "Step: 11010, train/epoch: 2.62018084526062\n",
      "Step: 11020, train/loss: 0.0\n",
      "Step: 11020, train/grad_norm: 4.5319964669943147e-07\n",
      "Step: 11020, train/learning_rate: 6.2906551647756714e-06\n",
      "Step: 11020, train/epoch: 2.622560739517212\n",
      "Step: 11030, train/loss: 0.0\n",
      "Step: 11030, train/grad_norm: 1.3579646918060462e-07\n",
      "Step: 11030, train/learning_rate: 6.250991646084003e-06\n",
      "Step: 11030, train/epoch: 2.6249403953552246\n",
      "Step: 11040, train/loss: 0.0\n",
      "Step: 11040, train/grad_norm: 5.392784714786103e-06\n",
      "Step: 11040, train/learning_rate: 6.211328127392335e-06\n",
      "Step: 11040, train/epoch: 2.6273202896118164\n",
      "Step: 11050, train/loss: 0.0\n",
      "Step: 11050, train/grad_norm: 9.391683875037415e-08\n",
      "Step: 11050, train/learning_rate: 6.171664153953316e-06\n",
      "Step: 11050, train/epoch: 2.629700183868408\n",
      "Step: 11060, train/loss: 0.0\n",
      "Step: 11060, train/grad_norm: 7.344280561483174e-07\n",
      "Step: 11060, train/learning_rate: 6.132000635261647e-06\n",
      "Step: 11060, train/epoch: 2.632080078125\n",
      "Step: 11070, train/loss: 0.0\n",
      "Step: 11070, train/grad_norm: 2.9184991944930516e-05\n",
      "Step: 11070, train/learning_rate: 6.092337116569979e-06\n",
      "Step: 11070, train/epoch: 2.6344597339630127\n",
      "Step: 11080, train/loss: 0.0\n",
      "Step: 11080, train/grad_norm: 1.1935340808122419e-05\n",
      "Step: 11080, train/learning_rate: 6.05267314313096e-06\n",
      "Step: 11080, train/epoch: 2.6368396282196045\n",
      "Step: 11090, train/loss: 0.0\n",
      "Step: 11090, train/grad_norm: 6.195123773977684e-07\n",
      "Step: 11090, train/learning_rate: 6.013009624439292e-06\n",
      "Step: 11090, train/epoch: 2.6392195224761963\n",
      "Step: 11100, train/loss: 0.0\n",
      "Step: 11100, train/grad_norm: 1.0635157821070607e-07\n",
      "Step: 11100, train/learning_rate: 5.973346105747623e-06\n",
      "Step: 11100, train/epoch: 2.641599178314209\n",
      "Step: 11110, train/loss: 0.0\n",
      "Step: 11110, train/grad_norm: 6.38227248828116e-08\n",
      "Step: 11110, train/learning_rate: 5.933682587055955e-06\n",
      "Step: 11110, train/epoch: 2.643979072570801\n",
      "Step: 11120, train/loss: 0.0\n",
      "Step: 11120, train/grad_norm: 1.7440153214920429e-06\n",
      "Step: 11120, train/learning_rate: 5.894018613616936e-06\n",
      "Step: 11120, train/epoch: 2.6463589668273926\n",
      "Step: 11130, train/loss: 0.0\n",
      "Step: 11130, train/grad_norm: 2.99884277410456e-07\n",
      "Step: 11130, train/learning_rate: 5.854355094925268e-06\n",
      "Step: 11130, train/epoch: 2.6487386226654053\n",
      "Step: 11140, train/loss: 0.15000000596046448\n",
      "Step: 11140, train/grad_norm: 4.0930444811237976e-05\n",
      "Step: 11140, train/learning_rate: 5.814691576233599e-06\n",
      "Step: 11140, train/epoch: 2.651118516921997\n",
      "Step: 11150, train/loss: 0.0\n",
      "Step: 11150, train/grad_norm: 1.376350724058284e-06\n",
      "Step: 11150, train/learning_rate: 5.77502760279458e-06\n",
      "Step: 11150, train/epoch: 2.653498411178589\n",
      "Step: 11160, train/loss: 0.0\n",
      "Step: 11160, train/grad_norm: 7.180597094702534e-07\n",
      "Step: 11160, train/learning_rate: 5.735364084102912e-06\n",
      "Step: 11160, train/epoch: 2.6558780670166016\n",
      "Step: 11170, train/loss: 0.0\n",
      "Step: 11170, train/grad_norm: 3.845794651624601e-07\n",
      "Step: 11170, train/learning_rate: 5.695700565411244e-06\n",
      "Step: 11170, train/epoch: 2.6582579612731934\n",
      "Step: 11180, train/loss: 0.0\n",
      "Step: 11180, train/grad_norm: 2.8231008286638826e-07\n",
      "Step: 11180, train/learning_rate: 5.656036591972224e-06\n",
      "Step: 11180, train/epoch: 2.660637855529785\n",
      "Step: 11190, train/loss: 0.0\n",
      "Step: 11190, train/grad_norm: 1.7458217144012451\n",
      "Step: 11190, train/learning_rate: 5.616373073280556e-06\n",
      "Step: 11190, train/epoch: 2.663017511367798\n",
      "Step: 11200, train/loss: 0.0\n",
      "Step: 11200, train/grad_norm: 4.886341571364028e-07\n",
      "Step: 11200, train/learning_rate: 5.576709554588888e-06\n",
      "Step: 11200, train/epoch: 2.6653974056243896\n",
      "Step: 11210, train/loss: 0.0\n",
      "Step: 11210, train/grad_norm: 2.412631658899045e-07\n",
      "Step: 11210, train/learning_rate: 5.5370460358972196e-06\n",
      "Step: 11210, train/epoch: 2.6677772998809814\n",
      "Step: 11220, train/loss: 0.0\n",
      "Step: 11220, train/grad_norm: 4.945158593727683e-07\n",
      "Step: 11220, train/learning_rate: 5.4973820624582e-06\n",
      "Step: 11220, train/epoch: 2.670156955718994\n",
      "Step: 11230, train/loss: 0.0\n",
      "Step: 11230, train/grad_norm: 1.227905272571661e-06\n",
      "Step: 11230, train/learning_rate: 5.457718543766532e-06\n",
      "Step: 11230, train/epoch: 2.672536849975586\n",
      "Step: 11240, train/loss: 0.0\n",
      "Step: 11240, train/grad_norm: 3.853615908155916e-06\n",
      "Step: 11240, train/learning_rate: 5.418055025074864e-06\n",
      "Step: 11240, train/epoch: 2.6749167442321777\n",
      "Step: 11250, train/loss: 0.0\n",
      "Step: 11250, train/grad_norm: 7.637470116605982e-05\n",
      "Step: 11250, train/learning_rate: 5.378391051635845e-06\n",
      "Step: 11250, train/epoch: 2.6772966384887695\n",
      "Step: 11260, train/loss: 0.0\n",
      "Step: 11260, train/grad_norm: 1.8313872640618456e-10\n",
      "Step: 11260, train/learning_rate: 5.338727532944176e-06\n",
      "Step: 11260, train/epoch: 2.6796762943267822\n",
      "Step: 11270, train/loss: 0.0\n",
      "Step: 11270, train/grad_norm: 1.2139149685452821e-09\n",
      "Step: 11270, train/learning_rate: 5.299064014252508e-06\n",
      "Step: 11270, train/epoch: 2.682056188583374\n",
      "Step: 11280, train/loss: 0.0\n",
      "Step: 11280, train/grad_norm: 1.2044091590723838e-06\n",
      "Step: 11280, train/learning_rate: 5.25940049556084e-06\n",
      "Step: 11280, train/epoch: 2.684436082839966\n",
      "Step: 11290, train/loss: 0.0\n",
      "Step: 11290, train/grad_norm: 0.0007857456803321838\n",
      "Step: 11290, train/learning_rate: 5.219736522121821e-06\n",
      "Step: 11290, train/epoch: 2.6868157386779785\n",
      "Step: 11300, train/loss: 0.0\n",
      "Step: 11300, train/grad_norm: 0.00042439778917469084\n",
      "Step: 11300, train/learning_rate: 5.180073003430152e-06\n",
      "Step: 11300, train/epoch: 2.6891956329345703\n",
      "Step: 11310, train/loss: 0.0\n",
      "Step: 11310, train/grad_norm: 0.0014928964665159583\n",
      "Step: 11310, train/learning_rate: 5.140409484738484e-06\n",
      "Step: 11310, train/epoch: 2.691575527191162\n",
      "Step: 11320, train/loss: 0.0\n",
      "Step: 11320, train/grad_norm: 3.4070128549501533e-06\n",
      "Step: 11320, train/learning_rate: 5.100745511299465e-06\n",
      "Step: 11320, train/epoch: 2.693955183029175\n",
      "Step: 11330, train/loss: 0.0\n",
      "Step: 11330, train/grad_norm: 4.890596869699948e-07\n",
      "Step: 11330, train/learning_rate: 5.0610819926077966e-06\n",
      "Step: 11330, train/epoch: 2.6963350772857666\n",
      "Step: 11340, train/loss: 0.0\n",
      "Step: 11340, train/grad_norm: 3.420977634505107e-07\n",
      "Step: 11340, train/learning_rate: 5.021418473916128e-06\n",
      "Step: 11340, train/epoch: 2.6987149715423584\n",
      "Step: 11350, train/loss: 0.0\n",
      "Step: 11350, train/grad_norm: 6.646662882303644e-07\n",
      "Step: 11350, train/learning_rate: 4.981754500477109e-06\n",
      "Step: 11350, train/epoch: 2.701094627380371\n",
      "Step: 11360, train/loss: 0.0\n",
      "Step: 11360, train/grad_norm: 1.362605416943552e-06\n",
      "Step: 11360, train/learning_rate: 4.942090981785441e-06\n",
      "Step: 11360, train/epoch: 2.703474521636963\n",
      "Step: 11370, train/loss: 0.0\n",
      "Step: 11370, train/grad_norm: 0.0007557346834801137\n",
      "Step: 11370, train/learning_rate: 4.9024274630937725e-06\n",
      "Step: 11370, train/epoch: 2.7058544158935547\n",
      "Step: 11380, train/loss: 0.0\n",
      "Step: 11380, train/grad_norm: 4.20703190684435e-06\n",
      "Step: 11380, train/learning_rate: 4.862763944402104e-06\n",
      "Step: 11380, train/epoch: 2.7082340717315674\n",
      "Step: 11390, train/loss: 0.0\n",
      "Step: 11390, train/grad_norm: 2.1976720745442435e-05\n",
      "Step: 11390, train/learning_rate: 4.823099970963085e-06\n",
      "Step: 11390, train/epoch: 2.710613965988159\n",
      "Step: 11400, train/loss: 0.0\n",
      "Step: 11400, train/grad_norm: 4.426957730174763e-06\n",
      "Step: 11400, train/learning_rate: 4.783436452271417e-06\n",
      "Step: 11400, train/epoch: 2.712993860244751\n",
      "Step: 11410, train/loss: 0.0\n",
      "Step: 11410, train/grad_norm: 2.185718585678842e-06\n",
      "Step: 11410, train/learning_rate: 4.7437729335797485e-06\n",
      "Step: 11410, train/epoch: 2.7153735160827637\n",
      "Step: 11420, train/loss: 0.0\n",
      "Step: 11420, train/grad_norm: 3.423626822041115e-07\n",
      "Step: 11420, train/learning_rate: 4.704108960140729e-06\n",
      "Step: 11420, train/epoch: 2.7177534103393555\n",
      "Step: 11430, train/loss: 0.0\n",
      "Step: 11430, train/grad_norm: 4.6793311980763974e-07\n",
      "Step: 11430, train/learning_rate: 4.664445441449061e-06\n",
      "Step: 11430, train/epoch: 2.7201333045959473\n",
      "Step: 11440, train/loss: 0.0\n",
      "Step: 11440, train/grad_norm: 7.025956847428461e-07\n",
      "Step: 11440, train/learning_rate: 4.624781922757393e-06\n",
      "Step: 11440, train/epoch: 2.722513198852539\n",
      "Step: 11450, train/loss: 0.0\n",
      "Step: 11450, train/grad_norm: 9.969017264666036e-05\n",
      "Step: 11450, train/learning_rate: 4.5851184040657245e-06\n",
      "Step: 11450, train/epoch: 2.7248928546905518\n",
      "Step: 11460, train/loss: 0.0\n",
      "Step: 11460, train/grad_norm: 1.0346521776227746e-05\n",
      "Step: 11460, train/learning_rate: 4.545454430626705e-06\n",
      "Step: 11460, train/epoch: 2.7272727489471436\n",
      "Step: 11470, train/loss: 0.0\n",
      "Step: 11470, train/grad_norm: 0.0033809314481914043\n",
      "Step: 11470, train/learning_rate: 4.505790911935037e-06\n",
      "Step: 11470, train/epoch: 2.7296526432037354\n",
      "Step: 11480, train/loss: 0.0\n",
      "Step: 11480, train/grad_norm: 5.2972159210185055e-06\n",
      "Step: 11480, train/learning_rate: 4.466127393243369e-06\n",
      "Step: 11480, train/epoch: 2.732032299041748\n",
      "Step: 11490, train/loss: 0.0\n",
      "Step: 11490, train/grad_norm: 1.3090667607684736e-06\n",
      "Step: 11490, train/learning_rate: 4.4264634198043495e-06\n",
      "Step: 11490, train/epoch: 2.73441219329834\n",
      "Step: 11500, train/loss: 0.0\n",
      "Step: 11500, train/grad_norm: 2.445046993670985e-05\n",
      "Step: 11500, train/learning_rate: 4.386799901112681e-06\n",
      "Step: 11500, train/epoch: 2.7367920875549316\n",
      "Step: 11510, train/loss: 0.0\n",
      "Step: 11510, train/grad_norm: 0.00022372719831764698\n",
      "Step: 11510, train/learning_rate: 4.347136382421013e-06\n",
      "Step: 11510, train/epoch: 2.7391717433929443\n",
      "Step: 11520, train/loss: 0.0\n",
      "Step: 11520, train/grad_norm: 8.026527780202741e-07\n",
      "Step: 11520, train/learning_rate: 4.307472408981994e-06\n",
      "Step: 11520, train/epoch: 2.741551637649536\n",
      "Step: 11530, train/loss: 0.0\n",
      "Step: 11530, train/grad_norm: 4.686728061642498e-06\n",
      "Step: 11530, train/learning_rate: 4.2678088902903255e-06\n",
      "Step: 11530, train/epoch: 2.743931531906128\n",
      "Step: 11540, train/loss: 0.0\n",
      "Step: 11540, train/grad_norm: 4.197159796603955e-06\n",
      "Step: 11540, train/learning_rate: 4.228145371598657e-06\n",
      "Step: 11540, train/epoch: 2.7463111877441406\n",
      "Step: 11550, train/loss: 0.0\n",
      "Step: 11550, train/grad_norm: 5.155433200343396e-07\n",
      "Step: 11550, train/learning_rate: 4.188481852906989e-06\n",
      "Step: 11550, train/epoch: 2.7486910820007324\n",
      "Step: 11560, train/loss: 0.0\n",
      "Step: 11560, train/grad_norm: 3.6904355056321947e-06\n",
      "Step: 11560, train/learning_rate: 4.14881787946797e-06\n",
      "Step: 11560, train/epoch: 2.751070976257324\n",
      "Step: 11570, train/loss: 0.0\n",
      "Step: 11570, train/grad_norm: 1.844205144152511e-07\n",
      "Step: 11570, train/learning_rate: 4.1091543607763015e-06\n",
      "Step: 11570, train/epoch: 2.753450632095337\n",
      "Step: 11580, train/loss: 0.0\n",
      "Step: 11580, train/grad_norm: 1.638622677546664e-07\n",
      "Step: 11580, train/learning_rate: 4.069490842084633e-06\n",
      "Step: 11580, train/epoch: 2.7558305263519287\n",
      "Step: 11590, train/loss: 0.0\n",
      "Step: 11590, train/grad_norm: 2.8091168502442088e-08\n",
      "Step: 11590, train/learning_rate: 4.029826868645614e-06\n",
      "Step: 11590, train/epoch: 2.7582104206085205\n",
      "Step: 11600, train/loss: 0.0\n",
      "Step: 11600, train/grad_norm: 7.145136351027759e-06\n",
      "Step: 11600, train/learning_rate: 3.990163349953946e-06\n",
      "Step: 11600, train/epoch: 2.760590076446533\n",
      "Step: 11610, train/loss: 0.0\n",
      "Step: 11610, train/grad_norm: 9.525886071060086e-07\n",
      "Step: 11610, train/learning_rate: 3.9504998312622774e-06\n",
      "Step: 11610, train/epoch: 2.762969970703125\n",
      "Step: 11620, train/loss: 0.0\n",
      "Step: 11620, train/grad_norm: 5.72657711472857e-07\n",
      "Step: 11620, train/learning_rate: 3.910836312570609e-06\n",
      "Step: 11620, train/epoch: 2.765349864959717\n",
      "Step: 11630, train/loss: 0.0\n",
      "Step: 11630, train/grad_norm: 0.0001504120882600546\n",
      "Step: 11630, train/learning_rate: 3.87117233913159e-06\n",
      "Step: 11630, train/epoch: 2.7677297592163086\n",
      "Step: 11640, train/loss: 0.00039999998989515007\n",
      "Step: 11640, train/grad_norm: 1.5518008922299487e-07\n",
      "Step: 11640, train/learning_rate: 3.831508820439922e-06\n",
      "Step: 11640, train/epoch: 2.7701094150543213\n",
      "Step: 11650, train/loss: 0.0\n",
      "Step: 11650, train/grad_norm: 2.7731729801416805e-07\n",
      "Step: 11650, train/learning_rate: 3.791845074374578e-06\n",
      "Step: 11650, train/epoch: 2.772489309310913\n",
      "Step: 11660, train/loss: 0.0\n",
      "Step: 11660, train/grad_norm: 9.9536585196347e-08\n",
      "Step: 11660, train/learning_rate: 3.7521815556829097e-06\n",
      "Step: 11660, train/epoch: 2.774869203567505\n",
      "Step: 11670, train/loss: 0.0\n",
      "Step: 11670, train/grad_norm: 5.142516101841466e-07\n",
      "Step: 11670, train/learning_rate: 3.712517809617566e-06\n",
      "Step: 11670, train/epoch: 2.7772488594055176\n",
      "Step: 11680, train/loss: 0.0\n",
      "Step: 11680, train/grad_norm: 2.9513489607779775e-07\n",
      "Step: 11680, train/learning_rate: 3.6728542909258977e-06\n",
      "Step: 11680, train/epoch: 2.7796287536621094\n",
      "Step: 11690, train/loss: 0.0\n",
      "Step: 11690, train/grad_norm: 7.610855732309574e-07\n",
      "Step: 11690, train/learning_rate: 3.633190544860554e-06\n",
      "Step: 11690, train/epoch: 2.782008647918701\n",
      "Step: 11700, train/loss: 0.0\n",
      "Step: 11700, train/grad_norm: 3.1813587497708795e-07\n",
      "Step: 11700, train/learning_rate: 3.59352679879521e-06\n",
      "Step: 11700, train/epoch: 2.784388303756714\n",
      "Step: 11710, train/loss: 0.0\n",
      "Step: 11710, train/grad_norm: 3.643416732757032e-07\n",
      "Step: 11710, train/learning_rate: 3.553863280103542e-06\n",
      "Step: 11710, train/epoch: 2.7867681980133057\n",
      "Step: 11720, train/loss: 0.0\n",
      "Step: 11720, train/grad_norm: 1.9000402971869335e-05\n",
      "Step: 11720, train/learning_rate: 3.514199534038198e-06\n",
      "Step: 11720, train/epoch: 2.7891480922698975\n",
      "Step: 11730, train/loss: 0.0\n",
      "Step: 11730, train/grad_norm: 2.8185016276438546e-07\n",
      "Step: 11730, train/learning_rate: 3.47453601534653e-06\n",
      "Step: 11730, train/epoch: 2.79152774810791\n",
      "Step: 11740, train/loss: 0.0\n",
      "Step: 11740, train/grad_norm: 0.00047660095151513815\n",
      "Step: 11740, train/learning_rate: 3.434872269281186e-06\n",
      "Step: 11740, train/epoch: 2.793907642364502\n",
      "Step: 11750, train/loss: 0.0\n",
      "Step: 11750, train/grad_norm: 6.385326400959457e-07\n",
      "Step: 11750, train/learning_rate: 3.3952085232158424e-06\n",
      "Step: 11750, train/epoch: 2.7962875366210938\n",
      "Step: 11760, train/loss: 0.00039999998989515007\n",
      "Step: 11760, train/grad_norm: 7.60764748974907e-08\n",
      "Step: 11760, train/learning_rate: 3.355545004524174e-06\n",
      "Step: 11760, train/epoch: 2.7986671924591064\n",
      "Step: 11770, train/loss: 0.0\n",
      "Step: 11770, train/grad_norm: 0.0011859434889629483\n",
      "Step: 11770, train/learning_rate: 3.3158812584588304e-06\n",
      "Step: 11770, train/epoch: 2.8010470867156982\n",
      "Step: 11780, train/loss: 0.0\n",
      "Step: 11780, train/grad_norm: 1.2352078329058713e-06\n",
      "Step: 11780, train/learning_rate: 3.276217739767162e-06\n",
      "Step: 11780, train/epoch: 2.80342698097229\n",
      "Step: 11790, train/loss: 0.0\n",
      "Step: 11790, train/grad_norm: 3.5456490877550095e-05\n",
      "Step: 11790, train/learning_rate: 3.2365539937018184e-06\n",
      "Step: 11790, train/epoch: 2.805806875228882\n",
      "Step: 11800, train/loss: 0.0\n",
      "Step: 11800, train/grad_norm: 3.632276275311597e-05\n",
      "Step: 11800, train/learning_rate: 3.19689047501015e-06\n",
      "Step: 11800, train/epoch: 2.8081865310668945\n",
      "Step: 11810, train/loss: 0.0\n",
      "Step: 11810, train/grad_norm: 4.3913377112403396e-07\n",
      "Step: 11810, train/learning_rate: 3.1572267289448064e-06\n",
      "Step: 11810, train/epoch: 2.8105664253234863\n",
      "Step: 11820, train/loss: 0.0\n",
      "Step: 11820, train/grad_norm: 1.6842284367157845e-06\n",
      "Step: 11820, train/learning_rate: 3.1175629828794627e-06\n",
      "Step: 11820, train/epoch: 2.812946319580078\n",
      "Step: 11830, train/loss: 0.0\n",
      "Step: 11830, train/grad_norm: 1.1017327494755591e-07\n",
      "Step: 11830, train/learning_rate: 3.0778994641877944e-06\n",
      "Step: 11830, train/epoch: 2.815325975418091\n",
      "Step: 11840, train/loss: 0.0\n",
      "Step: 11840, train/grad_norm: 7.346677080022346e-07\n",
      "Step: 11840, train/learning_rate: 3.0382357181224506e-06\n",
      "Step: 11840, train/epoch: 2.8177058696746826\n",
      "Step: 11850, train/loss: 9.999999747378752e-05\n",
      "Step: 11850, train/grad_norm: 0.0011571015929803252\n",
      "Step: 11850, train/learning_rate: 2.9985721994307823e-06\n",
      "Step: 11850, train/epoch: 2.8200857639312744\n",
      "Step: 11860, train/loss: 0.0\n",
      "Step: 11860, train/grad_norm: 2.0806435259146383e-06\n",
      "Step: 11860, train/learning_rate: 2.9589084533654386e-06\n",
      "Step: 11860, train/epoch: 2.822465419769287\n",
      "Step: 11870, train/loss: 0.0\n",
      "Step: 11870, train/grad_norm: 3.3658895404187206e-07\n",
      "Step: 11870, train/learning_rate: 2.919244707300095e-06\n",
      "Step: 11870, train/epoch: 2.824845314025879\n",
      "Step: 11880, train/loss: 0.0\n",
      "Step: 11880, train/grad_norm: 3.8538109947694466e-06\n",
      "Step: 11880, train/learning_rate: 2.8795811886084266e-06\n",
      "Step: 11880, train/epoch: 2.8272252082824707\n",
      "Step: 11890, train/loss: 0.0\n",
      "Step: 11890, train/grad_norm: 1.5089924545463873e-06\n",
      "Step: 11890, train/learning_rate: 2.839917442543083e-06\n",
      "Step: 11890, train/epoch: 2.8296048641204834\n",
      "Step: 11900, train/loss: 0.0\n",
      "Step: 11900, train/grad_norm: 1.531839473045693e-07\n",
      "Step: 11900, train/learning_rate: 2.8002539238514146e-06\n",
      "Step: 11900, train/epoch: 2.831984758377075\n",
      "Step: 11910, train/loss: 0.0\n",
      "Step: 11910, train/grad_norm: 4.6242573148447264e-07\n",
      "Step: 11910, train/learning_rate: 2.760590177786071e-06\n",
      "Step: 11910, train/epoch: 2.834364652633667\n",
      "Step: 11920, train/loss: 0.0\n",
      "Step: 11920, train/grad_norm: 5.038629069531453e-08\n",
      "Step: 11920, train/learning_rate: 2.720926431720727e-06\n",
      "Step: 11920, train/epoch: 2.8367443084716797\n",
      "Step: 11930, train/loss: 0.0\n",
      "Step: 11930, train/grad_norm: 8.66350944761507e-07\n",
      "Step: 11930, train/learning_rate: 2.681262913029059e-06\n",
      "Step: 11930, train/epoch: 2.8391242027282715\n",
      "Step: 11940, train/loss: 0.0\n",
      "Step: 11940, train/grad_norm: 5.230982225157277e-08\n",
      "Step: 11940, train/learning_rate: 2.641599166963715e-06\n",
      "Step: 11940, train/epoch: 2.8415040969848633\n",
      "Step: 11950, train/loss: 0.0\n",
      "Step: 11950, train/grad_norm: 3.9747479263496643e-07\n",
      "Step: 11950, train/learning_rate: 2.601935648272047e-06\n",
      "Step: 11950, train/epoch: 2.843883752822876\n",
      "Step: 11960, train/loss: 0.0\n",
      "Step: 11960, train/grad_norm: 8.202912482602187e-08\n",
      "Step: 11960, train/learning_rate: 2.562271902206703e-06\n",
      "Step: 11960, train/epoch: 2.8462636470794678\n",
      "Step: 11970, train/loss: 0.0\n",
      "Step: 11970, train/grad_norm: 5.698091740669042e-07\n",
      "Step: 11970, train/learning_rate: 2.522608383515035e-06\n",
      "Step: 11970, train/epoch: 2.8486435413360596\n",
      "Step: 11980, train/loss: 0.0\n",
      "Step: 11980, train/grad_norm: 7.317132713069441e-07\n",
      "Step: 11980, train/learning_rate: 2.482944637449691e-06\n",
      "Step: 11980, train/epoch: 2.8510234355926514\n",
      "Step: 11990, train/loss: 0.0203000009059906\n",
      "Step: 11990, train/grad_norm: 3.2125353754963726e-07\n",
      "Step: 11990, train/learning_rate: 2.4432808913843473e-06\n",
      "Step: 11990, train/epoch: 2.853403091430664\n",
      "Step: 12000, train/loss: 0.0\n",
      "Step: 12000, train/grad_norm: 4.070498562214198e-06\n",
      "Step: 12000, train/learning_rate: 2.403617372692679e-06\n",
      "Step: 12000, train/epoch: 2.855782985687256\n",
      "Step: 12010, train/loss: 0.0\n",
      "Step: 12010, train/grad_norm: 1.4901834219926968e-06\n",
      "Step: 12010, train/learning_rate: 2.3639536266273353e-06\n",
      "Step: 12010, train/epoch: 2.8581628799438477\n",
      "Step: 12020, train/loss: 0.0\n",
      "Step: 12020, train/grad_norm: 4.781917596119456e-06\n",
      "Step: 12020, train/learning_rate: 2.324290107935667e-06\n",
      "Step: 12020, train/epoch: 2.8605425357818604\n",
      "Step: 12030, train/loss: 0.0\n",
      "Step: 12030, train/grad_norm: 4.188827006146312e-06\n",
      "Step: 12030, train/learning_rate: 2.2846263618703233e-06\n",
      "Step: 12030, train/epoch: 2.862922430038452\n",
      "Step: 12040, train/loss: 0.0\n",
      "Step: 12040, train/grad_norm: 3.773506614379585e-05\n",
      "Step: 12040, train/learning_rate: 2.2449626158049796e-06\n",
      "Step: 12040, train/epoch: 2.865302324295044\n",
      "Step: 12050, train/loss: 0.003800000064074993\n",
      "Step: 12050, train/grad_norm: 4.288749551051296e-06\n",
      "Step: 12050, train/learning_rate: 2.2052990971133113e-06\n",
      "Step: 12050, train/epoch: 2.8676819801330566\n",
      "Step: 12060, train/loss: 0.0\n",
      "Step: 12060, train/grad_norm: 3.6098893474445504e-07\n",
      "Step: 12060, train/learning_rate: 2.1656353510479676e-06\n",
      "Step: 12060, train/epoch: 2.8700618743896484\n",
      "Step: 12070, train/loss: 0.0\n",
      "Step: 12070, train/grad_norm: 1.5633451084795524e-06\n",
      "Step: 12070, train/learning_rate: 2.1259718323562993e-06\n",
      "Step: 12070, train/epoch: 2.8724417686462402\n",
      "Step: 12080, train/loss: 0.0\n",
      "Step: 12080, train/grad_norm: 2.309685669388273e-06\n",
      "Step: 12080, train/learning_rate: 2.0863080862909555e-06\n",
      "Step: 12080, train/epoch: 2.874821424484253\n",
      "Step: 12090, train/loss: 0.0\n",
      "Step: 12090, train/grad_norm: 3.3771257790249365e-07\n",
      "Step: 12090, train/learning_rate: 2.0466445675992873e-06\n",
      "Step: 12090, train/epoch: 2.8772013187408447\n",
      "Step: 12100, train/loss: 0.0\n",
      "Step: 12100, train/grad_norm: 1.1870354228449287e-06\n",
      "Step: 12100, train/learning_rate: 2.0069808215339435e-06\n",
      "Step: 12100, train/epoch: 2.8795812129974365\n",
      "Step: 12110, train/loss: 0.0\n",
      "Step: 12110, train/grad_norm: 4.090574066140107e-07\n",
      "Step: 12110, train/learning_rate: 1.9673170754686e-06\n",
      "Step: 12110, train/epoch: 2.881960868835449\n",
      "Step: 12120, train/loss: 0.0\n",
      "Step: 12120, train/grad_norm: 1.737922320899088e-05\n",
      "Step: 12120, train/learning_rate: 1.9276535567769315e-06\n",
      "Step: 12120, train/epoch: 2.884340763092041\n",
      "Step: 12130, train/loss: 0.0\n",
      "Step: 12130, train/grad_norm: 1.1947897291975096e-05\n",
      "Step: 12130, train/learning_rate: 1.8879898107115878e-06\n",
      "Step: 12130, train/epoch: 2.886720657348633\n",
      "Step: 12140, train/loss: 0.0\n",
      "Step: 12140, train/grad_norm: 3.3491569411125965e-06\n",
      "Step: 12140, train/learning_rate: 1.8483261783330818e-06\n",
      "Step: 12140, train/epoch: 2.8891003131866455\n",
      "Step: 12150, train/loss: 0.0\n",
      "Step: 12150, train/grad_norm: 8.956165515883185e-07\n",
      "Step: 12150, train/learning_rate: 1.8086625459545758e-06\n",
      "Step: 12150, train/epoch: 2.8914802074432373\n",
      "Step: 12160, train/loss: 0.0\n",
      "Step: 12160, train/grad_norm: 5.5407105037375e-07\n",
      "Step: 12160, train/learning_rate: 1.7689989135760698e-06\n",
      "Step: 12160, train/epoch: 2.893860101699829\n",
      "Step: 12170, train/loss: 0.0\n",
      "Step: 12170, train/grad_norm: 1.9978581633495196e-07\n",
      "Step: 12170, train/learning_rate: 1.7293352811975637e-06\n",
      "Step: 12170, train/epoch: 2.896239995956421\n",
      "Step: 12180, train/loss: 0.0\n",
      "Step: 12180, train/grad_norm: 8.002926733752247e-06\n",
      "Step: 12180, train/learning_rate: 1.68967153513222e-06\n",
      "Step: 12180, train/epoch: 2.8986196517944336\n",
      "Step: 12190, train/loss: 0.0\n",
      "Step: 12190, train/grad_norm: 6.522684543597279e-07\n",
      "Step: 12190, train/learning_rate: 1.650007902753714e-06\n",
      "Step: 12190, train/epoch: 2.9009995460510254\n",
      "Step: 12200, train/loss: 0.0\n",
      "Step: 12200, train/grad_norm: 1.5939214108584565e-06\n",
      "Step: 12200, train/learning_rate: 1.610344270375208e-06\n",
      "Step: 12200, train/epoch: 2.903379440307617\n",
      "Step: 12210, train/loss: 0.0\n",
      "Step: 12210, train/grad_norm: 6.722066245856695e-06\n",
      "Step: 12210, train/learning_rate: 1.570680637996702e-06\n",
      "Step: 12210, train/epoch: 2.90575909614563\n",
      "Step: 12220, train/loss: 0.0\n",
      "Step: 12220, train/grad_norm: 4.37842857081705e-07\n",
      "Step: 12220, train/learning_rate: 1.531017005618196e-06\n",
      "Step: 12220, train/epoch: 2.9081389904022217\n",
      "Step: 12230, train/loss: 0.0\n",
      "Step: 12230, train/grad_norm: 1.5611946935223386e-07\n",
      "Step: 12230, train/learning_rate: 1.49135337323969e-06\n",
      "Step: 12230, train/epoch: 2.9105188846588135\n",
      "Step: 12240, train/loss: 0.0\n",
      "Step: 12240, train/grad_norm: 2.469224682499771e-07\n",
      "Step: 12240, train/learning_rate: 1.4516896271743462e-06\n",
      "Step: 12240, train/epoch: 2.912898540496826\n",
      "Step: 12250, train/loss: 0.0\n",
      "Step: 12250, train/grad_norm: 2.7250337097939337e-07\n",
      "Step: 12250, train/learning_rate: 1.4120259947958402e-06\n",
      "Step: 12250, train/epoch: 2.915278434753418\n",
      "Step: 12260, train/loss: 0.0\n",
      "Step: 12260, train/grad_norm: 8.298639841086697e-06\n",
      "Step: 12260, train/learning_rate: 1.3723623624173342e-06\n",
      "Step: 12260, train/epoch: 2.9176583290100098\n",
      "Step: 12270, train/loss: 0.0\n",
      "Step: 12270, train/grad_norm: 3.29677982335852e-06\n",
      "Step: 12270, train/learning_rate: 1.3326987300388282e-06\n",
      "Step: 12270, train/epoch: 2.9200379848480225\n",
      "Step: 12280, train/loss: 0.0\n",
      "Step: 12280, train/grad_norm: 7.999794433999341e-06\n",
      "Step: 12280, train/learning_rate: 1.2930350976603222e-06\n",
      "Step: 12280, train/epoch: 2.9224178791046143\n",
      "Step: 12290, train/loss: 0.0\n",
      "Step: 12290, train/grad_norm: 3.196610691702517e-07\n",
      "Step: 12290, train/learning_rate: 1.2533714652818162e-06\n",
      "Step: 12290, train/epoch: 2.924797773361206\n",
      "Step: 12300, train/loss: 0.0\n",
      "Step: 12300, train/grad_norm: 2.3995035007828847e-06\n",
      "Step: 12300, train/learning_rate: 1.2137077192164725e-06\n",
      "Step: 12300, train/epoch: 2.9271774291992188\n",
      "Step: 12310, train/loss: 0.0\n",
      "Step: 12310, train/grad_norm: 1.8677587831916753e-07\n",
      "Step: 12310, train/learning_rate: 1.1740440868379665e-06\n",
      "Step: 12310, train/epoch: 2.9295573234558105\n",
      "Step: 12320, train/loss: 0.0\n",
      "Step: 12320, train/grad_norm: 6.0184785979799926e-05\n",
      "Step: 12320, train/learning_rate: 1.1343804544594605e-06\n",
      "Step: 12320, train/epoch: 2.9319372177124023\n",
      "Step: 12330, train/loss: 0.0\n",
      "Step: 12330, train/grad_norm: 3.7861459531995934e-06\n",
      "Step: 12330, train/learning_rate: 1.0947168220809544e-06\n",
      "Step: 12330, train/epoch: 2.934316873550415\n",
      "Step: 12340, train/loss: 0.0\n",
      "Step: 12340, train/grad_norm: 7.493522389268037e-07\n",
      "Step: 12340, train/learning_rate: 1.0550531897024484e-06\n",
      "Step: 12340, train/epoch: 2.936696767807007\n",
      "Step: 12350, train/loss: 0.0\n",
      "Step: 12350, train/grad_norm: 1.3838696304446785e-06\n",
      "Step: 12350, train/learning_rate: 1.0153894436371047e-06\n",
      "Step: 12350, train/epoch: 2.9390766620635986\n",
      "Step: 12360, train/loss: 0.0\n",
      "Step: 12360, train/grad_norm: 3.143482985024093e-08\n",
      "Step: 12360, train/learning_rate: 9.757258112585987e-07\n",
      "Step: 12360, train/epoch: 2.9414565563201904\n",
      "Step: 12370, train/loss: 0.0\n",
      "Step: 12370, train/grad_norm: 1.1886666015925584e-06\n",
      "Step: 12370, train/learning_rate: 9.360621788800927e-07\n",
      "Step: 12370, train/epoch: 2.943836212158203\n",
      "Step: 12380, train/loss: 0.0\n",
      "Step: 12380, train/grad_norm: 5.646356839861255e-06\n",
      "Step: 12380, train/learning_rate: 8.963985465015867e-07\n",
      "Step: 12380, train/epoch: 2.946216106414795\n",
      "Step: 12390, train/loss: 0.0\n",
      "Step: 12390, train/grad_norm: 8.435185350208485e-07\n",
      "Step: 12390, train/learning_rate: 8.567349141230807e-07\n",
      "Step: 12390, train/epoch: 2.9485960006713867\n",
      "Step: 12400, train/loss: 0.0\n",
      "Step: 12400, train/grad_norm: 8.905641152523458e-06\n",
      "Step: 12400, train/learning_rate: 8.170712249011558e-07\n",
      "Step: 12400, train/epoch: 2.9509756565093994\n",
      "Step: 12410, train/loss: 0.0\n",
      "Step: 12410, train/grad_norm: 8.032410505620646e-07\n",
      "Step: 12410, train/learning_rate: 7.774075925226498e-07\n",
      "Step: 12410, train/epoch: 2.953355550765991\n",
      "Step: 12420, train/loss: 0.0\n",
      "Step: 12420, train/grad_norm: 2.6767934286908712e-06\n",
      "Step: 12420, train/learning_rate: 7.377439033007249e-07\n",
      "Step: 12420, train/epoch: 2.955735445022583\n",
      "Step: 12430, train/loss: 0.0\n",
      "Step: 12430, train/grad_norm: 7.411311457872216e-07\n",
      "Step: 12430, train/learning_rate: 6.980802709222189e-07\n",
      "Step: 12430, train/epoch: 2.9581151008605957\n",
      "Step: 12440, train/loss: 0.0\n",
      "Step: 12440, train/grad_norm: 2.405186478426913e-06\n",
      "Step: 12440, train/learning_rate: 6.584166385437129e-07\n",
      "Step: 12440, train/epoch: 2.9604949951171875\n",
      "Step: 12450, train/loss: 0.0\n",
      "Step: 12450, train/grad_norm: 1.845228614003047e-12\n",
      "Step: 12450, train/learning_rate: 6.18752949321788e-07\n",
      "Step: 12450, train/epoch: 2.9628748893737793\n",
      "Step: 12460, train/loss: 0.0\n",
      "Step: 12460, train/grad_norm: 2.4618691440991824e-07\n",
      "Step: 12460, train/learning_rate: 5.79089316943282e-07\n",
      "Step: 12460, train/epoch: 2.965254545211792\n",
      "Step: 12470, train/loss: 0.0\n",
      "Step: 12470, train/grad_norm: 6.315633527265163e-06\n",
      "Step: 12470, train/learning_rate: 5.39425684564776e-07\n",
      "Step: 12470, train/epoch: 2.967634439468384\n",
      "Step: 12480, train/loss: 0.0\n",
      "Step: 12480, train/grad_norm: 1.4557312510987686e-07\n",
      "Step: 12480, train/learning_rate: 4.997619953428512e-07\n",
      "Step: 12480, train/epoch: 2.9700143337249756\n",
      "Step: 12490, train/loss: 0.0\n",
      "Step: 12490, train/grad_norm: 1.2879881978733465e-05\n",
      "Step: 12490, train/learning_rate: 4.6009836296434514e-07\n",
      "Step: 12490, train/epoch: 2.9723939895629883\n",
      "Step: 12500, train/loss: 0.0\n",
      "Step: 12500, train/grad_norm: 8.094503982647439e-07\n",
      "Step: 12500, train/learning_rate: 4.204347021641297e-07\n",
      "Step: 12500, train/epoch: 2.97477388381958\n",
      "Step: 12510, train/loss: 0.0\n",
      "Step: 12510, train/grad_norm: 3.581974624466966e-06\n",
      "Step: 12510, train/learning_rate: 3.807710697856237e-07\n",
      "Step: 12510, train/epoch: 2.977153778076172\n",
      "Step: 12520, train/loss: 0.0\n",
      "Step: 12520, train/grad_norm: 2.9830715675416286e-07\n",
      "Step: 12520, train/learning_rate: 3.4110740898540826e-07\n",
      "Step: 12520, train/epoch: 2.9795336723327637\n",
      "Step: 12530, train/loss: 0.0\n",
      "Step: 12530, train/grad_norm: 2.508977274828794e-07\n",
      "Step: 12530, train/learning_rate: 3.014437481851928e-07\n",
      "Step: 12530, train/epoch: 2.9819133281707764\n",
      "Step: 12540, train/loss: 0.0\n",
      "Step: 12540, train/grad_norm: 6.362961357808672e-07\n",
      "Step: 12540, train/learning_rate: 2.617801158066868e-07\n",
      "Step: 12540, train/epoch: 2.984293222427368\n",
      "Step: 12550, train/loss: 0.0\n",
      "Step: 12550, train/grad_norm: 2.932978588887636e-07\n",
      "Step: 12550, train/learning_rate: 2.2211645500647137e-07\n",
      "Step: 12550, train/epoch: 2.98667311668396\n",
      "Step: 12560, train/loss: 0.0\n",
      "Step: 12560, train/grad_norm: 2.238282650068868e-05\n",
      "Step: 12560, train/learning_rate: 1.8245279420625593e-07\n",
      "Step: 12560, train/epoch: 2.9890527725219727\n",
      "Step: 12570, train/loss: 0.0\n",
      "Step: 12570, train/grad_norm: 5.440108907350805e-06\n",
      "Step: 12570, train/learning_rate: 1.427891476168952e-07\n",
      "Step: 12570, train/epoch: 2.9914326667785645\n",
      "Step: 12580, train/loss: 0.0\n",
      "Step: 12580, train/grad_norm: 4.382226325105876e-05\n",
      "Step: 12580, train/learning_rate: 1.0312549392210713e-07\n",
      "Step: 12580, train/epoch: 2.9938125610351562\n",
      "Step: 12590, train/loss: 0.0\n",
      "Step: 12590, train/grad_norm: 1.015170028040302e-06\n",
      "Step: 12590, train/learning_rate: 6.346184022731904e-08\n",
      "Step: 12590, train/epoch: 2.996192216873169\n",
      "Step: 12600, train/loss: 0.0\n",
      "Step: 12600, train/grad_norm: 0.001176289631985128\n",
      "Step: 12600, train/learning_rate: 2.379819186160148e-08\n",
      "Step: 12600, train/epoch: 2.9985721111297607\n",
      "Step: 12606, eval/loss: 0.002042477484792471\n",
      "Step: 12606, eval/accuracy: 0.9997223615646362\n",
      "Step: 12606, eval/f1: 0.9997068047523499\n",
      "Step: 12606, eval/runtime: 7988.41259765625\n",
      "Step: 12606, eval/samples_per_second: 0.9020000100135803\n",
      "Step: 12606, eval/steps_per_second: 0.11299999803304672\n",
      "Step: 12606, train/epoch: 3.0\n",
      "Step: 12606, train/train_runtime: 126447.9609375\n",
      "Step: 12606, train/train_samples_per_second: 0.796999990940094\n",
      "Step: 12606, train/train_steps_per_second: 0.10000000149011612\n",
      "Step: 12606, train/total_flos: 2.1235818045284286e+19\n",
      "Step: 12606, train/train_loss: 0.0004971224698238075\n",
      "Step: 12606, train/epoch: 3.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.summary.summary_iterator import summary_iterator\n",
    "import glob\n",
    "import os\n",
    "\n",
    "logs_directory = os.path.join('./', project_name, 'logs')\n",
    "file_pattern = 'events.out.tfevents.*'\n",
    "\n",
    "event_files = glob.glob(os.path.join(logs_directory, file_pattern))\n",
    "\n",
    "def print_events_from_file(event_files):\n",
    "    for event_file in event_files:\n",
    "        print(f\"Reading events from file: {event_file}\")\n",
    "        try:\n",
    "            for e in summary_iterator(event_file):\n",
    "                for v in e.summary.value:\n",
    "                    if v.HasField('simple_value'):\n",
    "                        print(f\"Step: {e.step}, {v.tag}: {v.simple_value}\")\n",
    "        except Exception as e:  # Just in case the event file is not readable\n",
    "            print(f\"Failed to read {event_file}: {e}\")\n",
    "\n",
    "print_events_from_file(event_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ffd0164-0f41-4bbe-b905-7dda371dfd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Step Train Loss Eval Loss  Accuracy        F1\n",
      "0   4202   0.151200  0.091785  0.981674  0.980719\n",
      "1   8404   0.000000  0.003845  0.999583  0.999560\n",
      "2  12606   0.000000  0.002042  0.999722  0.999707\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.python.summary.summary_iterator import summary_iterator\n",
    "\n",
    "logs_directory = os.path.join('./', project_name, 'logs')\n",
    "file_pattern = 'events.out.tfevents.*'\n",
    "\n",
    "event_files = glob.glob(os.path.join(logs_directory, file_pattern))\n",
    "\n",
    "def extract_metrics(event_files):\n",
    "    data = []\n",
    "    last_train_loss = None\n",
    "\n",
    "    for event_file in event_files:\n",
    "        for e in summary_iterator(event_file):\n",
    "            for v in e.summary.value:\n",
    "                if v.HasField('simple_value'):\n",
    "                    step = e.step\n",
    "                    metric_name = v.tag.split('/')[-1]\n",
    "                    metric_value = v.simple_value\n",
    "\n",
    "                    formatted_value = f\"{metric_value:.6f}\"\n",
    "\n",
    "                    if 'train/loss' in v.tag:\n",
    "                        last_train_loss = formatted_value\n",
    "\n",
    "                    if 'eval' in v.tag:\n",
    "                        entry = next((item for item in data if item['Step'] == step), None)\n",
    "                        if not entry:\n",
    "                            entry = {'Step': step, 'Train Loss': last_train_loss, 'Eval Loss': None, 'Accuracy': None, 'F1': None}\n",
    "                            data.append(entry)\n",
    "                        if 'loss' in v.tag:\n",
    "                            entry['Eval Loss'] = formatted_value\n",
    "                        elif 'accuracy' in v.tag:\n",
    "                            entry['Accuracy'] = formatted_value\n",
    "                        elif 'f1' in v.tag:\n",
    "                            entry['F1'] = formatted_value\n",
    "\n",
    "    return data\n",
    "\n",
    "metrics_data = extract_metrics(event_files)\n",
    "\n",
    "df = pd.DataFrame(metrics_data)\n",
    "df = df.sort_values(by='Step')\n",
    "\n",
    "file_path = \"./images/\"+model_name+\"_Checkpoint_Data.csv\"\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37cc5f92-d47f-4356-8fad-d9b64b6f5361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Checkpoint Step: checkpoint-12606\n",
      "Step             12606\n",
      "Train Loss    0.000000\n",
      "Eval Loss     0.002042\n",
      "Accuracy      0.999722\n",
      "F1            0.999707\n",
      "Rank Sum           3.0\n",
      "Name: 2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df.fillna({\n",
    "    'Eval Loss': float('inf'),\n",
    "    'Accuracy': 0,\n",
    "    'F1': 0\n",
    "}, inplace=True)\n",
    "\n",
    "df['Eval Loss'] = df['Eval Loss'].astype(float)\n",
    "df['Accuracy'] = df['Accuracy'].astype(float)\n",
    "df['F1'] = df['F1'].astype(float)\n",
    "\n",
    "df['Eval Loss Rank'] = df['Eval Loss'].rank(method='min', ascending=True)\n",
    "df['Accuracy Rank'] = df['Accuracy'].rank(method='min', ascending=False)\n",
    "df['F1 Rank'] = df['F1'].rank(method='min', ascending=False)\n",
    "\n",
    "df['Rank Sum'] = df['Eval Loss Rank'] + df['Accuracy Rank'] + df['F1 Rank']\n",
    "\n",
    "best_checkpoint = df.loc[df['Rank Sum'].idxmin()]\n",
    "\n",
    "checkpoint_folder_name = f\"checkpoint-{best_checkpoint['Step']}\"\n",
    "print(f\"Best Checkpoint Step: {checkpoint_folder_name}\")\n",
    "print(best_checkpoint[['Step', 'Train Loss', 'Eval Loss', 'Accuracy', 'F1', 'Rank Sum']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3c3999-8c32-4a25-aaca-2ec9036b69ed",
   "metadata": {},
   "source": [
    "### Run TensorBoard\n",
    "tensorboard --logdir=~/kuk/Praxis/praxis-Llama-2-7b-hf-small-finetune/logs --host=0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f36022-dc73-492f-998c-43e5ed1a6f46",
   "metadata": {},
   "source": [
    "### PAUSE SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "444ae65b-241c-47ef-bbc2-81f3a2ce50e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My flag to pause the script, set to True to pause\n",
    "pause_script = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8be3672-68e0-44f8-b8b1-31290665658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        print(\"Script Paused\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb3b5ed2-5320-49c0-9e61-90d6f0942f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pause_script:\n",
    "    raise StopExecution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19be6e26-d888-4da0-b3f8-836d68ac2051",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5111194-5eeb-4104-8df0-f977a6b716e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb77b36384f4a03aeb0340c78313ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-70B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    num_labels=2,\n",
    "    token=access_token,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    checkpoint,\n",
    "    token=access_token,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "530fa55f-c8c4-485f-a093-09bf2175d586",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "test_checkpoint_name = checkpoint_folder_name\n",
    "ft_model = PeftModel.from_pretrained(base_model, project_name+'/'+test_checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27354c4f-95cc-4d1f-ac64-c5e6b4a842ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#     ft_model = torch.nn.DataParallel(ft_model)\n",
    "\n",
    "# ft_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eee9ab6f-a7d5-4dd5-9436-2f6f6e2bffaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484fbb5736cd41d1b9fbb698bbc0aa3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "processed_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for record in tqdm(tokenized_test_ds):\n",
    "\n",
    "        eval_prompt = record['article']\n",
    "        model_input = tokenize_fn({'article': eval_prompt})\n",
    "\n",
    "        # model_input = {k: v.to('cuda') for k, v in model_input.items()}\n",
    "        \n",
    "        outputs = ft_model(**model_input)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        prediction = logits[0].argmax(-1).item()  # Use .item() to get a Python number\n",
    "        processed_predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f06a2c-5ded-4da0-8232-145383967c9d",
   "metadata": {},
   "source": [
    "### Accuracy and F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e891bc5-55e0-4c43-a035-0edc37dcb6e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "true_articles = tokenized_test_ds['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6bf33824-d7cc-4d22-af4d-7d44e569c2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9994446758295155\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(true_articles, processed_predictions)\n",
    "print(\"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9b37dc24-3bc0-48a0-ace9-a360bae91169",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(true_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a29ebf68-632f-49d1-b903-407ff05b5569",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(processed_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fb4d8350-a8d1-4853-a9f8-74ae9ea36bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.9994140400971312\n"
     ]
    }
   ],
   "source": [
    "f1_score = f1_score(true_articles, processed_predictions, average='macro')\n",
    "print(\"f1_score:\", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcaac14-2b83-4c24-b83a-f6d0e73a2d2a",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "57e64afb-e3ee-4c79-8228-b2d79806229e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(tokenized_test_ds['label'], processed_predictions)\n",
    "\n",
    "labels = ['human', 'machine']\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af7d6f55-fb4f-4195-ac3f-93852ecd4b65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook praxis-Meta-Llama-3-70B-small-finetune-v12.ipynb to html\n",
      "[NbConvertApp] WARNING | Alternative text is missing on 1 image(s).\n",
      "[NbConvertApp] Writing 779980 bytes to html/praxis-Meta-Llama-3-70B-small-finetune-v12.html\n"
     ]
    }
   ],
   "source": [
    "file_name = f\"{project_name}.ipynb\"\n",
    "html_file_name = f\"{file_name.replace('.ipynb', '.html')}\"\n",
    "\n",
    "command = f\"jupyter nbconvert '{file_name}' --to html --output-dir './html' --output '{html_file_name}'\"\n",
    "get_ipython().system(command)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
