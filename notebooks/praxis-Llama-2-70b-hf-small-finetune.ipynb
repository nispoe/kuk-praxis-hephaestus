{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c2d2c9-4a4b-48ca-90a3-1ccd120ca08b",
   "metadata": {},
   "source": [
    "# Fine tuning using Llama 2 70b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5bef3d-89b9-42ed-b158-a39bd61f6a31",
   "metadata": {},
   "source": [
    "### Base Model and Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfe5f014-527c-4a83-863b-4b6330c72ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 0: NVIDIA GeForce RTX 4090\n",
    "# GPU 1: NVIDIA GeForce RTX 4090\n",
    "# GPU 2: NVIDIA GeForce RTX 4090\n",
    "# GPU 3: NVIDIA GeForce RTX 3090 Ti\n",
    "# GPU 4: NVIDIA GeForce RTX 3090 Ti\n",
    "# GPU 5: NVIDIA GeForce RTX 3090\n",
    "# GPU 6: NVIDIA GeForce RTX 3090\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3,4,5,6\"  # \"\"makes all visible, \"0\" GPU 0 visible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ffaa11-3936-40a0-8f2a-3d083ff2afef",
   "metadata": {},
   "source": [
    "### Supress warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e77f7e3d-3af3-4dc8-9571-d13398c29ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef535c7a-848e-4c6e-a692-208f98610d82",
   "metadata": {},
   "source": [
    "### Inspect the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28753dd6-88ff-402e-a918-1a5a19b2a1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3b6dded-95e9-4912-a786-430fce1831da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab7a6af9c864130ae6518bdcd4c9c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15ec782b-0776-4354-ad08-66f1e9e50187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"Llama-2-70b-hf\"\n",
    "checkpoint = \"meta-llama/\"+model_name\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed149c5-aa97-44d4-b0e9-918a5febf77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, quantization_config=bnb_config, device_map=\"auto\", num_labels=2, token=access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83f74939-3ab3-4011-90cb-8e90c22ea162",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "LlamaForSequenceClassification                          --\n",
       "├─LlamaModel: 1-1                                       --\n",
       "│    └─Embedding: 2-1                                   262,144,000\n",
       "│    └─ModuleList: 2-2                                  --\n",
       "│    │    └─LlamaDecoderLayer: 3-1                      427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-2                      427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-3                      427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-4                      427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-5                      427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-6                      427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-7                      427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-8                      427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-9                      427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-10                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-11                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-12                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-13                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-14                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-15                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-16                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-17                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-18                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-19                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-20                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-21                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-22                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-23                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-24                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-25                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-26                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-27                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-28                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-29                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-30                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-31                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-32                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-33                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-34                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-35                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-36                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-37                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-38                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-39                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-40                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-41                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-42                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-43                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-44                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-45                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-46                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-47                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-48                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-49                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-50                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-51                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-52                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-53                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-54                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-55                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-56                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-57                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-58                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-59                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-60                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-61                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-62                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-63                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-64                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-65                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-66                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-67                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-68                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-69                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-70                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-71                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-72                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-73                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-74                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-75                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-76                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-77                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-78                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-79                     427,835,392\n",
       "│    │    └─LlamaDecoderLayer: 3-80                     427,835,392\n",
       "│    └─LlamaRMSNorm: 2-3                                8,192\n",
       "├─Linear: 1-2                                           16,384\n",
       "================================================================================\n",
       "Total params: 34,488,999,936\n",
       "Trainable params: 263,479,296\n",
       "Non-trainable params: 34,225,520,640\n",
       "================================================================================"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49bbe47c-4430-4f4a-90d8-1113bd320a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 8192)\n",
       "    (layers): ModuleList(\n",
       "      (0-79): 80 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=8192, out_features=28672, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=8192, out_features=28672, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=28672, out_features=8192, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (score): Linear(in_features=8192, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6627b1da-57d6-4a17-8ea3-746b5cb1f3d9",
   "metadata": {},
   "source": [
    "### Load the news dataset from pickle file\n",
    "If any of the check_files don't exist then load the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cc372cc-0cae-4b49-a2d7-8e57f58244a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one of the specified files already exists. Not loading new dataset.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "base_path = './data/'\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "file_name = 'news_small_dataset.pkl'\n",
    "file_path = base_path+file_name\n",
    "\n",
    "def pickle_dataset(dataset, file_path):\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(dataset, file)\n",
    "        print(f\"Dataset has been pickled to: {file_path}\")\n",
    "\n",
    "def load_pickle_dataset(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        dataset = pickle.load(file)\n",
    "        print(f\"Dataset has been loaded from: {file_path}\")\n",
    "    return dataset\n",
    "\n",
    "def check_files_exists(file_names):\n",
    "    for name in file_names:\n",
    "        file_path = os.path.join(base_path, name)\n",
    "        if os.path.exists(file_path):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# if these files exist we do not want to load the news_dataset.pkl to tokenize and make these files\n",
    "check_files = [model_name+'-small_tokenized_train_ds.pkl', model_name+'-small_tokenized_eval_ds.pkl', model_name+'-small_tokenized_test_ds.pkl']\n",
    "\n",
    "if check_files_exists(check_files):\n",
    "    print(\"At least one of the specified files already exists. Not loading new dataset.\")\n",
    "else:\n",
    "    news_split_ds = load_pickle_dataset(file_path)\n",
    "    print(news_split_ds)\n",
    "    total_rows = (news_split_ds['train'].num_rows +\n",
    "              news_split_ds['eval'].num_rows +\n",
    "              news_split_ds['test'].num_rows)\n",
    "    print(\"Total number of rows:\", total_rows)\n",
    "    print(\"Dataset loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad450a2-6cef-432e-9e63-7ff985b4726e",
   "metadata": {},
   "source": [
    "### Tokenization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88c3a38c-1d24-4a1e-8d96-1b7c2ce162c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dbb0dfce1754ce9a48608bff15ed200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "156faa033b994d0486a5fae6a7c0cb7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbfeb78a46c24dd89bead0d365ee31a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4037787a36df460e8112aa59baf8019a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(checkpoint, token\u001b[38;5;241m=\u001b[39maccess_token)\n\u001b[1;32m      5\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokenizer\u001b[38;5;241m.\u001b[39mpad_token)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_fn\u001b[39m(news):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(news[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle\u001b[39m\u001b[38;5;124m'\u001b[39m], padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, token=access_token)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "\n",
    "def tokenize_fn(news):\n",
    "    return tokenizer(news['article'], padding=True, truncation=True, max_length=512, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02efc4f0-742a-4838-887d-5556a26ae15f",
   "metadata": {},
   "source": [
    "### Tokenize train, evaluation, and test datasets\n",
    "If any of the check files exist then don't run tokenization and save some time.\n",
    "Else load the pickle files that already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f3e3101-df30-4af2-9976-49ba0cf44d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already exist, so load datasets\n",
      "Dataset has been loaded from: ./data/Llama-2-70b-hf-small_tokenized_train_ds.pkl\n",
      "Dataset has been loaded from: ./data/Llama-2-70b-hf-small_tokenized_eval_ds.pkl\n",
      "Dataset has been loaded from: ./data/Llama-2-70b-hf-small_tokenized_test_ds.pkl\n"
     ]
    }
   ],
   "source": [
    "if not check_files_exists(check_files):\n",
    "    tokenized_train_ds = news_split_ds['train'].map(tokenize_fn, batched=True)\n",
    "    tokenized_eval_ds = news_split_ds['eval'].map(tokenize_fn, batched=True)\n",
    "    tokenized_test_ds = news_split_ds['test'].map(tokenize_fn, batched=True)\n",
    "\n",
    "    print(tokenized_train_ds.features)\n",
    "    print(tokenized_eval_ds.features)\n",
    "    print(tokenized_test_ds.features)\n",
    "    \n",
    "    pickle_dataset(tokenized_train_ds, base_path+model_name+'-small_tokenized_train_ds.pkl')\n",
    "    pickle_dataset(tokenized_eval_ds, base_path+model_name+'-small_tokenized_eval_ds.pkl')\n",
    "    pickle_dataset(tokenized_test_ds, base_path+model_name+'-small_tokenized_test_ds.pkl')\n",
    "else:\n",
    "    print(\"Files already exist, so load datasets\")\n",
    "    tokenized_train_ds = load_pickle_dataset(base_path+model_name+'-small_tokenized_train_ds.pkl')\n",
    "    tokenized_eval_ds = load_pickle_dataset(base_path+model_name+'-small_tokenized_eval_ds.pkl')\n",
    "    tokenized_test_ds = load_pickle_dataset(base_path+model_name+'-small_tokenized_test_ds.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307cc4fb-9766-4b0f-943b-4a1c90053e09",
   "metadata": {},
   "source": [
    "### Look at the tokenized data\n",
    "Notice what the actual data looks like, and then the tokenized data which is a bunch of numbers, and then the attention mask at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44321182-e1b9-4570-bd24-7a5cf42ba504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in training dataset: 33611\n",
      "Number of records in evaluation dataset: 7203\n",
      "Number of records in test dataset: 7203\n",
      "Total number of records: 48017\n"
     ]
    }
   ],
   "source": [
    "count_train_records = len(tokenized_train_ds)\n",
    "count_eval_records = len(tokenized_eval_ds)\n",
    "count_test_records = len(tokenized_test_ds)\n",
    "print(f\"Number of records in training dataset: {count_train_records}\")\n",
    "print(f\"Number of records in evaluation dataset: {count_eval_records}\")\n",
    "print(f\"Number of records in test dataset: {count_test_records}\")\n",
    "count_total_records = count_train_records + count_eval_records + count_test_records\n",
    "print(f\"Total number of records: {count_total_records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64be678b-d2a8-403e-bcc8-ef9e7eabb0cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'article': \"In a year where homicides, rapes and robberies increased slightly, New York City still saw serious crime drop 1.7 percent in 2015, continuing an overall decline that began in the 1990s, NYPD Commissioner William Bratton said Monday.\\nAt a news conference with Mayor Bill de Blasio, Bratton touted last year’s crime statistics, which he said, when combined with an even larger decline in 2014, put to rest the fear that substantial decreases couldn’t continue under the new administration at City Hall.\\n“While we have had some fluctuation, some increases in certain categories, the overall trend in all our crime categories continues to go down,” Bratton told reporters. “It was a very good year for us, 2015.\\nHomicides increased by 4.5 percent in 2015, rising to 350 from 333 in the prior year, which was the lowest since 1994, said Deputy Commissioner Dermot Shea. Rapes increased 6 percent and robberies rose 2 percent, said Shea, who is in charge of data collection and operations for the NYPD.\\nThe lower overall crime statistics came about due to what Shea called “targeted enforcement,” where cops make quality arrests even though the overall number of apprehensions was the lowest in the city since 2003.\\nTwo boroughs — Manhattan and the Bronx — actually saw serious crimes increase by 3 percent and 4 percent, respectively, Shea said. Manhattan’s increase was driven by more robberies, while the Bronx, although seeing an overall crime increase, had what he said was a “phenomenal” reduction in shootings. Citywide, shootings were down in 2015 about 3 percent, to 1,103 from 1,172 in 2014.\\nShea largely attributed the 2015 increase in rapes to victims coming forward with complaints about attacks from years past.\\nSign up to get the latest updates Get Newsday's Breaking News alerts in your inbox. By clicking Sign up, you agree to our privacy policy.\\n“Twenty percent of these rapes didn’t happen in 2015,” he said.\\nThe NYPD has seen an increase in rapes involving single women who, after a night of drinking, get into cabs of all kinds and are attacked, Shea said.\\n“They get driven, and passing out and waking up in a desolate area, and they get sexually attacked. This is something, really, that people need to be exceptionally aware of, and like any case in New York City, the buddy system works,” said Shea, referring to the need for people to travel in pairs when taking a cab at night.\\nBratton and police brass hope to build upon the continuing drop in overall crime by using technology such as ShotSpotter and a newly minted GPS system for police cars.\\nJessica Tisch, NYPD deputy commissioner for technology, said ShotSpotter, an acoustical system that detects gunfire, identified gunshots in 1,672 cases, mostly in Brooklyn. Of those alerts, 74 percent didn’t have any 911 calls from the public associated with them.\\nTisch said ShotSpotter helped police recover ballistic evidence in 19 percent of the gunfire alerts. In 22 percent of those cases, Tisch said, cops were able to make positive matches of bullets with those from guns used in earlier shootings.\\nTisch also highlighted a special GPS system being tried in about 5,000 patrol cars that allows the NYPD to see where its vehicles are and to track their movements over a 24-hour period, as well as gather information about the officers’ driving.\\n\", 'label': 0, 'input_ids': [1, 512, 263, 1629, 988, 3632, 293, 2247, 29892, 1153, 5547, 322, 10832, 495, 583, 11664, 10029, 29892, 1570, 3088, 4412, 1603, 4446, 10676, 17268, 5768, 29871, 29896, 29889, 29955, 10151, 297, 29871, 29906, 29900, 29896, 29945, 29892, 3133, 292, 385, 12463, 4845, 457, 393, 4689, 297, 278, 29871, 29896, 29929, 29929, 29900, 29879, 29892, 23526, 25014, 11444, 261, 4667, 1771, 271, 880, 1497, 27822, 29889, 13, 4178, 263, 9763, 21362, 411, 22186, 6682, 316, 3164, 294, 601, 29892, 1771, 271, 880, 5646, 287, 1833, 1629, 30010, 29879, 17268, 13964, 29892, 607, 540, 1497, 29892, 746, 12420, 411, 385, 1584, 7200, 4845, 457, 297, 29871, 29906, 29900, 29896, 29946, 29892, 1925, 304, 1791, 278, 8866, 393, 23228, 9263, 2129, 8496, 30010, 29873, 6773, 1090, 278, 716, 17517, 472, 4412, 6573, 29889, 13, 30015, 8809, 488, 591, 505, 750, 777, 1652, 5313, 29884, 362, 29892, 777, 16415, 297, 3058, 13997, 29892, 278, 12463, 534, 355, 297, 599, 1749, 17268, 13997, 18172, 304, 748, 1623, 3995, 1771, 271, 880, 5429, 1634, 272, 2153, 29889, 1346, 3112, 471, 263, 1407, 1781, 1629, 363, 502, 29892, 29871, 29906, 29900, 29896, 29945, 29889, 13, 24259, 293, 2247, 11664, 491, 29871, 29946, 29889, 29945, 10151, 297, 29871, 29906, 29900, 29896, 29945, 29892, 20493, 304, 29871, 29941, 29945, 29900, 515, 29871, 29941, 29941, 29941, 297, 278, 7536, 1629, 29892, 607, 471, 278, 19604, 1951, 29871, 29896, 29929, 29929, 29946, 29892, 1497, 26721, 29891, 11444, 261, 360, 837, 327, 2296, 29874, 29889, 390, 11603, 11664, 29871, 29953, 10151, 322, 10832, 495, 583, 11492, 29871, 29906, 10151, 29892, 1497, 2296, 29874, 29892, 1058, 338, 297, 8323, 310, 848, 4333, 322, 6931, 363, 278, 23526, 25014, 29889, 13, 1576, 5224, 12463, 17268, 13964, 2996, 1048, 2861, 304, 825, 2296, 29874, 2000, 1346, 5182, 287, 24555, 13561, 3995, 988, 274, 3554, 1207, 11029, 3948, 9197, 1584, 2466, 278, 12463, 1353, 310, 623, 9003, 5580, 471, 278, 19604, 297, 278, 4272, 1951, 29871, 29906, 29900, 29900, 29941, 29889, 13, 13985, 9820, 820, 29879, 813, 29093, 23586, 322, 278, 14165, 29916, 813, 2869, 4446, 10676, 2181, 1355, 7910, 491, 29871, 29941, 10151, 322, 29871, 29946, 10151, 29892, 8307, 29892, 2296, 29874, 1497, 29889, 29093, 23586, 30010, 29879, 7910, 471, 18225, 491, 901, 10832, 495, 583, 29892, 1550, 278, 14165, 29916, 29892, 5998, 8790, 385, 12463, 17268, 7910, 29892, 750, 825, 540, 1497, 471, 263, 1346, 9789, 2770, 284, 30024, 20376, 297, 15049, 886, 29889, 4412, 8157, 29892, 15049, 886, 892, 1623, 297, 29871, 29906, 29900, 29896, 29945, 1048, 29871, 29941, 10151, 29892, 304, 29871, 29896, 29892, 29896, 29900, 29941, 515, 29871, 29896, 29892, 29896, 29955, 29906, 297, 29871, 29906, 29900, 29896, 29946, 29889, 13, 13468, 29874, 18425, 29393, 278, 29871, 29906, 29900, 29896, 29945, 7910, 297, 1153, 5547, 304, 6879, 9893, 6421, 6375, 411, 15313, 9466, 1048, 16661, 515, 2440, 4940, 29889, 13, 10140, 701, 304, 679, 278, 9281, 11217, 3617, 10130, 3250, 29915, 29879, 5826, 5086, 10130, 6655, 29879, 297, 596, 297, 1884, 29889, 2648, 14855, 9954, 701, 29892, 366, 8661, 304, 1749, 5999, 4135, 8898, 29889, 13, 30015, 27418, 6478, 10151, 310, 1438], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "first_record = tokenized_train_ds[0]\n",
    "print(first_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d7f4f8-9eb3-4feb-86e7-fb0dad88753f",
   "metadata": {},
   "source": [
    "### Turn on accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3eab29e-fa47-4a13-abc4-d6425ae741b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e040f1-e596-476d-9f26-ffa6f8a4a548",
   "metadata": {},
   "source": [
    "### LoRA - Low-Rank Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0532bdc6-4317-474b-859c-4e5d2fe6f1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd29494-4453-4c01-b878-ef2f4533d34d",
   "metadata": {},
   "source": [
    "### Inspect the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5058f6d-185d-45af-83b9-bb7f61d4bee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b14fe8f2-eccd-4cfe-9d20-ef48bc178842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 103563264 || all params: 34592563200 || trainable%: 0.29938013960179743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForSequenceClassification(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 8192)\n",
       "        (layers): ModuleList(\n",
       "          (0-79): 80 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=28672, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=28672, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=28672, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=28672, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=28672, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=28672, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=8192, out_features=2, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=8192, out_features=2, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_trainable_parameters(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17b4782-03f8-4335-bfda-2a65794bff2c",
   "metadata": {},
   "source": [
    "### Look at hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c27c3328-1926-4adc-8696-b3b343ec4afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 7\n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "GPU 1: NVIDIA GeForce RTX 4090\n",
      "GPU 2: NVIDIA GeForce RTX 4090\n",
      "GPU 3: NVIDIA GeForce RTX 3090 Ti\n",
      "GPU 4: NVIDIA GeForce RTX 3090 Ti\n",
      "GPU 5: NVIDIA GeForce RTX 3090\n",
      "GPU 6: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdf68d0e-8786-489d-a4b8-b7478513efea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun  3 07:38:19 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off |   00000000:01:00.0 Off |                  N/A |\n",
      "| 31%   41C    P2            125W /  420W |    5062MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        Off |   00000000:02:00.0  On |                  Off |\n",
      "|  0%   40C    P2             71W /  450W |    6472MiB /  24564MiB |      6%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 3090 Ti     Off |   00000000:2B:00.0 Off |                  Off |\n",
      "| 37%   44C    P2            102W /  450W |    5070MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 3090        Off |   00000000:41:00.0 Off |                  N/A |\n",
      "| 32%   42C    P2            114W /  420W |    6788MiB /  24576MiB |      1%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA GeForce RTX 4090        Off |   00000000:42:00.0 Off |                  Off |\n",
      "|  0%   46C    P2             52W /  450W |    5213MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA GeForce RTX 4090        Off |   00000000:61:00.0 Off |                  Off |\n",
      "|  0%   46C    P2             49W /  450W |    5221MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA GeForce RTX 3090 Ti     Off |   00000000:62:00.0 Off |                  Off |\n",
      "| 31%   40C    P2             97W /  450W |    5078MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      4238      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    0   N/A  N/A     46914      C   /usr/bin/python3                             5046MiB |\n",
      "|    1   N/A  N/A      4238      G   /usr/lib/xorg/Xorg                            181MiB |\n",
      "|    1   N/A  N/A      4434      G   /usr/bin/gnome-shell                           84MiB |\n",
      "|    1   N/A  N/A      5121      G   ...irefox/4336/usr/lib/firefox/firefox        328MiB |\n",
      "|    1   N/A  N/A     46914      C   /usr/bin/python3                             5766MiB |\n",
      "|    2   N/A  N/A      4238      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    2   N/A  N/A     46914      C   /usr/bin/python3                             5054MiB |\n",
      "|    3   N/A  N/A      4238      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    3   N/A  N/A     46914      C   /usr/bin/python3                             6772MiB |\n",
      "|    4   N/A  N/A      4238      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    4   N/A  N/A     46914      C   /usr/bin/python3                             5196MiB |\n",
      "|    5   N/A  N/A      4238      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    5   N/A  N/A     46914      C   /usr/bin/python3                             5204MiB |\n",
      "|    6   N/A  N/A      4238      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    6   N/A  N/A     46914      C   /usr/bin/python3                             5062MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2340f2d1-1cc7-454e-bad6-bf4ac2c46fc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mis_parallelizable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     model\u001b[38;5;241m.\u001b[39mmodel_parallel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1199841a-6519-4422-8259-2fdbb114e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(logits_and_labels):\n",
    "    logits, labels = logits_and_labels\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='macro')\n",
    "    return {'accuracy': acc, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8c7f3ee-6997-4ed7-b28e-5d574831fb08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./praxis-Llama-2-70b-hf-small-finetune'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_name = \"praxis-\"+model_name+\"-small-finetune\"\n",
    "output_dir_path = \"./\" + project_name\n",
    "output_dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7a69bde-1d50-46a0-9838-00812afbdb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f3cd41-fd17-4fd8-83b8-54f464df79ff",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2df4e2a5-2a08-434b-983a-f6cd42f33da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 07:38:20.274687: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-03 07:38:20.847267: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnispoe\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nispoe/data/kuk/Praxis/wandb/run-20240603_073823-e2qshbtt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nispoe/huggingface/runs/e2qshbtt' target=\"_blank\">ethereal-butterfly-373</a></strong> to <a href='https://wandb.ai/nispoe/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nispoe/huggingface' target=\"_blank\">https://wandb.ai/nispoe/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nispoe/huggingface/runs/e2qshbtt' target=\"_blank\">https://wandb.ai/nispoe/huggingface/runs/e2qshbtt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12606' max='12606' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12606/12606 34:51:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002160</td>\n",
       "      <td>0.999445</td>\n",
       "      <td>0.999414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12606, training_loss=0.00042218187040475445, metrics={'train_runtime': 125524.0856, 'train_samples_per_second': 0.803, 'train_steps_per_second': 0.1, 'total_flos': 2.1235817723556004e+19, 'train_loss': 0.00042218187040475445, 'epoch': 3.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from pathlib import Path\n",
    "\n",
    "transformers.set_seed(777)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_ds,\n",
    "    eval_dataset=tokenized_eval_ds,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir_path,\n",
    "        num_train_epochs=3,\n",
    "        logging_steps=10,\n",
    "        logging_dir=output_dir_path+\"/logs\",\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        do_eval=True,\n",
    "    ),\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "trainer.train(resume_from_checkpoint=True)  # Turn to True if power goes out..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d13982d-53cb-4c88-bd32-a98d4a5a9874",
   "metadata": {},
   "source": [
    "### Determine best checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bd570c7-8feb-4b69-bda7-9c678bfd92c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 29 10:05 checkpoint-4202\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 Jun  3 05:19 checkpoint-8404\n",
      "drwxr-xr-x 4 nispoe nispoe 4096 Jun  3 07:38 logs\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 Jun  4 18:30 checkpoint-12606\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr {output_dir_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97771269-d77c-4c1b-b264-34e082db6cbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading events from file: ./praxis-Llama-2-70b-hf-small-finetune/logs/events.out.tfevents.1717418302.hephaestus.46914.0\n",
      "Step: 8410, train/loss: 0.0\n",
      "Step: 8410, train/grad_norm: 2.4814612515910994e-06\n",
      "Step: 8410, train/learning_rate: 1.6642869013594463e-05\n",
      "Step: 8410, train/epoch: 2.0014278888702393\n",
      "Step: 8420, train/loss: 0.0\n",
      "Step: 8420, train/grad_norm: 1.7544763977639377e-05\n",
      "Step: 8420, train/learning_rate: 1.6603204130660743e-05\n",
      "Step: 8420, train/epoch: 2.003807783126831\n",
      "Step: 8430, train/loss: 0.0\n",
      "Step: 8430, train/grad_norm: 3.5374632716411725e-05\n",
      "Step: 8430, train/learning_rate: 1.6563541066716425e-05\n",
      "Step: 8430, train/epoch: 2.0061874389648438\n",
      "Step: 8440, train/loss: 0.0\n",
      "Step: 8440, train/grad_norm: 9.789927571546286e-05\n",
      "Step: 8440, train/learning_rate: 1.6523878002772108e-05\n",
      "Step: 8440, train/epoch: 2.0085673332214355\n",
      "Step: 8450, train/loss: 0.0\n",
      "Step: 8450, train/grad_norm: 1.6896021406864747e-05\n",
      "Step: 8450, train/learning_rate: 1.6484213119838387e-05\n",
      "Step: 8450, train/epoch: 2.0109472274780273\n",
      "Step: 8460, train/loss: 0.0\n",
      "Step: 8460, train/grad_norm: 1.0174429917242378e-05\n",
      "Step: 8460, train/learning_rate: 1.644455005589407e-05\n",
      "Step: 8460, train/epoch: 2.01332688331604\n",
      "Step: 8470, train/loss: 0.0\n",
      "Step: 8470, train/grad_norm: 2.8834707336500287e-05\n",
      "Step: 8470, train/learning_rate: 1.6404886991949752e-05\n",
      "Step: 8470, train/epoch: 2.015706777572632\n",
      "Step: 8480, train/loss: 0.0\n",
      "Step: 8480, train/grad_norm: 3.702662434079684e-05\n",
      "Step: 8480, train/learning_rate: 1.636522210901603e-05\n",
      "Step: 8480, train/epoch: 2.0180866718292236\n",
      "Step: 8490, train/loss: 0.0\n",
      "Step: 8490, train/grad_norm: 2.5762781660887413e-05\n",
      "Step: 8490, train/learning_rate: 1.6325559045071714e-05\n",
      "Step: 8490, train/epoch: 2.0204663276672363\n",
      "Step: 8500, train/loss: 0.0\n",
      "Step: 8500, train/grad_norm: 2.5055696823983453e-05\n",
      "Step: 8500, train/learning_rate: 1.6285895981127396e-05\n",
      "Step: 8500, train/epoch: 2.022846221923828\n",
      "Step: 8510, train/loss: 0.0\n",
      "Step: 8510, train/grad_norm: 4.410048495628871e-05\n",
      "Step: 8510, train/learning_rate: 1.6246231098193675e-05\n",
      "Step: 8510, train/epoch: 2.02522611618042\n",
      "Step: 8520, train/loss: 0.0\n",
      "Step: 8520, train/grad_norm: 3.877109338645823e-05\n",
      "Step: 8520, train/learning_rate: 1.6206568034249358e-05\n",
      "Step: 8520, train/epoch: 2.0276060104370117\n",
      "Step: 8530, train/loss: 0.0\n",
      "Step: 8530, train/grad_norm: 7.695428394072223e-06\n",
      "Step: 8530, train/learning_rate: 1.616690497030504e-05\n",
      "Step: 8530, train/epoch: 2.0299856662750244\n",
      "Step: 8540, train/loss: 0.0\n",
      "Step: 8540, train/grad_norm: 1.2093461009499151e-05\n",
      "Step: 8540, train/learning_rate: 1.612724008737132e-05\n",
      "Step: 8540, train/epoch: 2.032365560531616\n",
      "Step: 8550, train/loss: 0.0\n",
      "Step: 8550, train/grad_norm: 3.935245331376791e-05\n",
      "Step: 8550, train/learning_rate: 1.6087577023427002e-05\n",
      "Step: 8550, train/epoch: 2.034745454788208\n",
      "Step: 8560, train/loss: 0.0\n",
      "Step: 8560, train/grad_norm: 2.4161596229532734e-05\n",
      "Step: 8560, train/learning_rate: 1.6047913959482685e-05\n",
      "Step: 8560, train/epoch: 2.0371251106262207\n",
      "Step: 8570, train/loss: 0.0\n",
      "Step: 8570, train/grad_norm: 0.0001382659247610718\n",
      "Step: 8570, train/learning_rate: 1.6008250895538367e-05\n",
      "Step: 8570, train/epoch: 2.0395050048828125\n",
      "Step: 8580, train/loss: 0.0\n",
      "Step: 8580, train/grad_norm: 5.1390641601756215e-05\n",
      "Step: 8580, train/learning_rate: 1.5968586012604646e-05\n",
      "Step: 8580, train/epoch: 2.0418848991394043\n",
      "Step: 8590, train/loss: 0.0\n",
      "Step: 8590, train/grad_norm: 6.536544333357597e-06\n",
      "Step: 8590, train/learning_rate: 1.592892294866033e-05\n",
      "Step: 8590, train/epoch: 2.044264554977417\n",
      "Step: 8600, train/loss: 0.0\n",
      "Step: 8600, train/grad_norm: 2.583898458397016e-05\n",
      "Step: 8600, train/learning_rate: 1.588925988471601e-05\n",
      "Step: 8600, train/epoch: 2.046644449234009\n",
      "Step: 8610, train/loss: 0.0\n",
      "Step: 8610, train/grad_norm: 1.8449543858878314e-05\n",
      "Step: 8610, train/learning_rate: 1.584959500178229e-05\n",
      "Step: 8610, train/epoch: 2.0490243434906006\n",
      "Step: 8620, train/loss: 0.0\n",
      "Step: 8620, train/grad_norm: 2.082317769236397e-05\n",
      "Step: 8620, train/learning_rate: 1.5809931937837973e-05\n",
      "Step: 8620, train/epoch: 2.0514039993286133\n",
      "Step: 8630, train/loss: 0.0\n",
      "Step: 8630, train/grad_norm: 3.2855317840585485e-05\n",
      "Step: 8630, train/learning_rate: 1.5770268873893656e-05\n",
      "Step: 8630, train/epoch: 2.053783893585205\n",
      "Step: 8640, train/loss: 0.0\n",
      "Step: 8640, train/grad_norm: 5.879518994333921e-06\n",
      "Step: 8640, train/learning_rate: 1.5730603990959935e-05\n",
      "Step: 8640, train/epoch: 2.056163787841797\n",
      "Step: 8650, train/loss: 0.0\n",
      "Step: 8650, train/grad_norm: 9.517666512692813e-06\n",
      "Step: 8650, train/learning_rate: 1.5690940927015617e-05\n",
      "Step: 8650, train/epoch: 2.0585434436798096\n",
      "Step: 8660, train/loss: 0.0\n",
      "Step: 8660, train/grad_norm: 1.7573116565472446e-05\n",
      "Step: 8660, train/learning_rate: 1.56512778630713e-05\n",
      "Step: 8660, train/epoch: 2.0609233379364014\n",
      "Step: 8670, train/loss: 0.0\n",
      "Step: 8670, train/grad_norm: 5.108449840918183e-06\n",
      "Step: 8670, train/learning_rate: 1.561161298013758e-05\n",
      "Step: 8670, train/epoch: 2.063303232192993\n",
      "Step: 8680, train/loss: 0.0\n",
      "Step: 8680, train/grad_norm: 6.177199975354597e-05\n",
      "Step: 8680, train/learning_rate: 1.5571949916193262e-05\n",
      "Step: 8680, train/epoch: 2.065683126449585\n",
      "Step: 8690, train/loss: 0.0\n",
      "Step: 8690, train/grad_norm: 0.00010928919800790027\n",
      "Step: 8690, train/learning_rate: 1.5532286852248944e-05\n",
      "Step: 8690, train/epoch: 2.0680627822875977\n",
      "Step: 8700, train/loss: 0.0\n",
      "Step: 8700, train/grad_norm: 1.6548940038774163e-05\n",
      "Step: 8700, train/learning_rate: 1.5492621969315223e-05\n",
      "Step: 8700, train/epoch: 2.0704426765441895\n",
      "Step: 8710, train/loss: 0.0\n",
      "Step: 8710, train/grad_norm: 1.2141423212597147e-05\n",
      "Step: 8710, train/learning_rate: 1.5452958905370906e-05\n",
      "Step: 8710, train/epoch: 2.0728225708007812\n",
      "Step: 8720, train/loss: 0.0\n",
      "Step: 8720, train/grad_norm: 2.4105787815642543e-05\n",
      "Step: 8720, train/learning_rate: 1.541329584142659e-05\n",
      "Step: 8720, train/epoch: 2.075202226638794\n",
      "Step: 8730, train/loss: 0.0\n",
      "Step: 8730, train/grad_norm: 1.931637234520167e-05\n",
      "Step: 8730, train/learning_rate: 1.5373630958492868e-05\n",
      "Step: 8730, train/epoch: 2.0775821208953857\n",
      "Step: 8740, train/loss: 0.0\n",
      "Step: 8740, train/grad_norm: 0.00018999911844730377\n",
      "Step: 8740, train/learning_rate: 1.533396789454855e-05\n",
      "Step: 8740, train/epoch: 2.0799620151519775\n",
      "Step: 8750, train/loss: 0.0\n",
      "Step: 8750, train/grad_norm: 5.971825430606259e-06\n",
      "Step: 8750, train/learning_rate: 1.5294304830604233e-05\n",
      "Step: 8750, train/epoch: 2.0823416709899902\n",
      "Step: 8760, train/loss: 0.0\n",
      "Step: 8760, train/grad_norm: 1.4476281648967415e-05\n",
      "Step: 8760, train/learning_rate: 1.5254640857165214e-05\n",
      "Step: 8760, train/epoch: 2.084721565246582\n",
      "Step: 8770, train/loss: 0.0\n",
      "Step: 8770, train/grad_norm: 1.2662044355238322e-05\n",
      "Step: 8770, train/learning_rate: 1.5214976883726195e-05\n",
      "Step: 8770, train/epoch: 2.087101459503174\n",
      "Step: 8780, train/loss: 0.0\n",
      "Step: 8780, train/grad_norm: 1.2525075362646021e-05\n",
      "Step: 8780, train/learning_rate: 1.5175312910287175e-05\n",
      "Step: 8780, train/epoch: 2.0894811153411865\n",
      "Step: 8790, train/loss: 0.0\n",
      "Step: 8790, train/grad_norm: 2.3869069991633296e-05\n",
      "Step: 8790, train/learning_rate: 1.5135649846342858e-05\n",
      "Step: 8790, train/epoch: 2.0918610095977783\n",
      "Step: 8800, train/loss: 0.0\n",
      "Step: 8800, train/grad_norm: 5.093425716040656e-05\n",
      "Step: 8800, train/learning_rate: 1.5095985872903839e-05\n",
      "Step: 8800, train/epoch: 2.09424090385437\n",
      "Step: 8810, train/loss: 0.0\n",
      "Step: 8810, train/grad_norm: 3.1305051379604265e-05\n",
      "Step: 8810, train/learning_rate: 1.5056322808959521e-05\n",
      "Step: 8810, train/epoch: 2.096620559692383\n",
      "Step: 8820, train/loss: 0.0\n",
      "Step: 8820, train/grad_norm: 5.301749115460552e-05\n",
      "Step: 8820, train/learning_rate: 1.5016658835520502e-05\n",
      "Step: 8820, train/epoch: 2.0990004539489746\n",
      "Step: 8830, train/loss: 0.0\n",
      "Step: 8830, train/grad_norm: 1.9037768652196974e-05\n",
      "Step: 8830, train/learning_rate: 1.4976994862081483e-05\n",
      "Step: 8830, train/epoch: 2.1013803482055664\n",
      "Step: 8840, train/loss: 0.0\n",
      "Step: 8840, train/grad_norm: 4.4166925363242626e-05\n",
      "Step: 8840, train/learning_rate: 1.4937331798137166e-05\n",
      "Step: 8840, train/epoch: 2.103760004043579\n",
      "Step: 8850, train/loss: 0.0\n",
      "Step: 8850, train/grad_norm: 3.932020626962185e-05\n",
      "Step: 8850, train/learning_rate: 1.4897667824698146e-05\n",
      "Step: 8850, train/epoch: 2.106139898300171\n",
      "Step: 8860, train/loss: 0.0\n",
      "Step: 8860, train/grad_norm: 2.05399192054756e-06\n",
      "Step: 8860, train/learning_rate: 1.4858003851259127e-05\n",
      "Step: 8860, train/epoch: 2.1085197925567627\n",
      "Step: 8870, train/loss: 0.0\n",
      "Step: 8870, train/grad_norm: 4.909434210276231e-06\n",
      "Step: 8870, train/learning_rate: 1.481834078731481e-05\n",
      "Step: 8870, train/epoch: 2.1108996868133545\n",
      "Step: 8880, train/loss: 0.0\n",
      "Step: 8880, train/grad_norm: 5.3994758673070464e-06\n",
      "Step: 8880, train/learning_rate: 1.477867681387579e-05\n",
      "Step: 8880, train/epoch: 2.113279342651367\n",
      "Step: 8890, train/loss: 0.0\n",
      "Step: 8890, train/grad_norm: 3.2374271086155204e-06\n",
      "Step: 8890, train/learning_rate: 1.4739012840436772e-05\n",
      "Step: 8890, train/epoch: 2.115659236907959\n",
      "Step: 8900, train/loss: 0.0\n",
      "Step: 8900, train/grad_norm: 0.00041105650598183274\n",
      "Step: 8900, train/learning_rate: 1.4699349776492454e-05\n",
      "Step: 8900, train/epoch: 2.118039131164551\n",
      "Step: 8910, train/loss: 0.0\n",
      "Step: 8910, train/grad_norm: 6.014361133566126e-06\n",
      "Step: 8910, train/learning_rate: 1.4659685803053435e-05\n",
      "Step: 8910, train/epoch: 2.1204187870025635\n",
      "Step: 8920, train/loss: 0.0\n",
      "Step: 8920, train/grad_norm: 0.0001525592670077458\n",
      "Step: 8920, train/learning_rate: 1.4620021829614416e-05\n",
      "Step: 8920, train/epoch: 2.1227986812591553\n",
      "Step: 8930, train/loss: 0.0\n",
      "Step: 8930, train/grad_norm: 4.275750325177796e-05\n",
      "Step: 8930, train/learning_rate: 1.4580358765670098e-05\n",
      "Step: 8930, train/epoch: 2.125178575515747\n",
      "Step: 8940, train/loss: 0.0\n",
      "Step: 8940, train/grad_norm: 8.204705409298185e-06\n",
      "Step: 8940, train/learning_rate: 1.454069479223108e-05\n",
      "Step: 8940, train/epoch: 2.1275582313537598\n",
      "Step: 8950, train/loss: 0.0\n",
      "Step: 8950, train/grad_norm: 1.4520798004014068e-06\n",
      "Step: 8950, train/learning_rate: 1.450103081879206e-05\n",
      "Step: 8950, train/epoch: 2.1299381256103516\n",
      "Step: 8960, train/loss: 0.0\n",
      "Step: 8960, train/grad_norm: 3.7293626519385725e-05\n",
      "Step: 8960, train/learning_rate: 1.4461367754847743e-05\n",
      "Step: 8960, train/epoch: 2.1323180198669434\n",
      "Step: 8970, train/loss: 0.0\n",
      "Step: 8970, train/grad_norm: 1.3387864782998804e-05\n",
      "Step: 8970, train/learning_rate: 1.4421703781408723e-05\n",
      "Step: 8970, train/epoch: 2.134697675704956\n",
      "Step: 8980, train/loss: 0.0\n",
      "Step: 8980, train/grad_norm: 3.5757420846493915e-05\n",
      "Step: 8980, train/learning_rate: 1.4382040717464406e-05\n",
      "Step: 8980, train/epoch: 2.137077569961548\n",
      "Step: 8990, train/loss: 0.0\n",
      "Step: 8990, train/grad_norm: 8.416025593760423e-06\n",
      "Step: 8990, train/learning_rate: 1.4342376744025387e-05\n",
      "Step: 8990, train/epoch: 2.1394574642181396\n",
      "Step: 9000, train/loss: 0.0\n",
      "Step: 9000, train/grad_norm: 1.4257103430281859e-05\n",
      "Step: 9000, train/learning_rate: 1.4302712770586368e-05\n",
      "Step: 9000, train/epoch: 2.1418371200561523\n",
      "Step: 9010, train/loss: 0.0\n",
      "Step: 9010, train/grad_norm: 4.845454441237962e-06\n",
      "Step: 9010, train/learning_rate: 1.426304970664205e-05\n",
      "Step: 9010, train/epoch: 2.144217014312744\n",
      "Step: 9020, train/loss: 0.0\n",
      "Step: 9020, train/grad_norm: 7.3504343163222075e-06\n",
      "Step: 9020, train/learning_rate: 1.4223385733203031e-05\n",
      "Step: 9020, train/epoch: 2.146596908569336\n",
      "Step: 9030, train/loss: 0.0\n",
      "Step: 9030, train/grad_norm: 8.52928678796161e-07\n",
      "Step: 9030, train/learning_rate: 1.4183721759764012e-05\n",
      "Step: 9030, train/epoch: 2.1489765644073486\n",
      "Step: 9040, train/loss: 0.0\n",
      "Step: 9040, train/grad_norm: 1.220914964505937e-05\n",
      "Step: 9040, train/learning_rate: 1.4144058695819695e-05\n",
      "Step: 9040, train/epoch: 2.1513564586639404\n",
      "Step: 9050, train/loss: 0.0\n",
      "Step: 9050, train/grad_norm: 2.5285824449383654e-05\n",
      "Step: 9050, train/learning_rate: 1.4104394722380675e-05\n",
      "Step: 9050, train/epoch: 2.1537363529205322\n",
      "Step: 9060, train/loss: 0.0\n",
      "Step: 9060, train/grad_norm: 3.2343425118597224e-05\n",
      "Step: 9060, train/learning_rate: 1.4064730748941656e-05\n",
      "Step: 9060, train/epoch: 2.156116247177124\n",
      "Step: 9070, train/loss: 0.0\n",
      "Step: 9070, train/grad_norm: 2.8969996037631063e-06\n",
      "Step: 9070, train/learning_rate: 1.4025067684997339e-05\n",
      "Step: 9070, train/epoch: 2.1584959030151367\n",
      "Step: 9080, train/loss: 0.15549999475479126\n",
      "Step: 9080, train/grad_norm: 1.7511401892988943e-05\n",
      "Step: 9080, train/learning_rate: 1.398540371155832e-05\n",
      "Step: 9080, train/epoch: 2.1608757972717285\n",
      "Step: 9090, train/loss: 0.0\n",
      "Step: 9090, train/grad_norm: 0.0002132758527295664\n",
      "Step: 9090, train/learning_rate: 1.39457397381193e-05\n",
      "Step: 9090, train/epoch: 2.1632556915283203\n",
      "Step: 9100, train/loss: 0.0\n",
      "Step: 9100, train/grad_norm: 0.0026717132423073053\n",
      "Step: 9100, train/learning_rate: 1.3906076674174983e-05\n",
      "Step: 9100, train/epoch: 2.165635347366333\n",
      "Step: 9110, train/loss: 0.0\n",
      "Step: 9110, train/grad_norm: 1.1468534466985147e-05\n",
      "Step: 9110, train/learning_rate: 1.3866412700735964e-05\n",
      "Step: 9110, train/epoch: 2.168015241622925\n",
      "Step: 9120, train/loss: 0.0\n",
      "Step: 9120, train/grad_norm: 3.870942600769922e-05\n",
      "Step: 9120, train/learning_rate: 1.3826748727296945e-05\n",
      "Step: 9120, train/epoch: 2.1703951358795166\n",
      "Step: 9130, train/loss: 0.0\n",
      "Step: 9130, train/grad_norm: 8.705661457497627e-06\n",
      "Step: 9130, train/learning_rate: 1.3787085663352627e-05\n",
      "Step: 9130, train/epoch: 2.1727747917175293\n",
      "Step: 9140, train/loss: 0.0\n",
      "Step: 9140, train/grad_norm: 7.082499359967187e-05\n",
      "Step: 9140, train/learning_rate: 1.3747421689913608e-05\n",
      "Step: 9140, train/epoch: 2.175154685974121\n",
      "Step: 9150, train/loss: 0.0\n",
      "Step: 9150, train/grad_norm: 1.9677796444739215e-05\n",
      "Step: 9150, train/learning_rate: 1.370775862596929e-05\n",
      "Step: 9150, train/epoch: 2.177534580230713\n",
      "Step: 9160, train/loss: 0.0\n",
      "Step: 9160, train/grad_norm: 9.882602171273902e-06\n",
      "Step: 9160, train/learning_rate: 1.3668094652530272e-05\n",
      "Step: 9160, train/epoch: 2.1799142360687256\n",
      "Step: 9170, train/loss: 0.0\n",
      "Step: 9170, train/grad_norm: 1.1232417818973772e-05\n",
      "Step: 9170, train/learning_rate: 1.3628430679091252e-05\n",
      "Step: 9170, train/epoch: 2.1822941303253174\n",
      "Step: 9180, train/loss: 0.0\n",
      "Step: 9180, train/grad_norm: 2.894659155572299e-05\n",
      "Step: 9180, train/learning_rate: 1.3588767615146935e-05\n",
      "Step: 9180, train/epoch: 2.184674024581909\n",
      "Step: 9190, train/loss: 0.0\n",
      "Step: 9190, train/grad_norm: 9.823319123825058e-05\n",
      "Step: 9190, train/learning_rate: 1.3549103641707916e-05\n",
      "Step: 9190, train/epoch: 2.187053680419922\n",
      "Step: 9200, train/loss: 0.0\n",
      "Step: 9200, train/grad_norm: 3.527701846905984e-05\n",
      "Step: 9200, train/learning_rate: 1.3509439668268897e-05\n",
      "Step: 9200, train/epoch: 2.1894335746765137\n",
      "Step: 9210, train/loss: 0.0\n",
      "Step: 9210, train/grad_norm: 1.1609894499997608e-05\n",
      "Step: 9210, train/learning_rate: 1.346977660432458e-05\n",
      "Step: 9210, train/epoch: 2.1918134689331055\n",
      "Step: 9220, train/loss: 0.0\n",
      "Step: 9220, train/grad_norm: 1.3273544027470052e-05\n",
      "Step: 9220, train/learning_rate: 1.343011263088556e-05\n",
      "Step: 9220, train/epoch: 2.194193124771118\n",
      "Step: 9230, train/loss: 0.0\n",
      "Step: 9230, train/grad_norm: 1.174258522951277e-05\n",
      "Step: 9230, train/learning_rate: 1.3390448657446541e-05\n",
      "Step: 9230, train/epoch: 2.19657301902771\n",
      "Step: 9240, train/loss: 0.0\n",
      "Step: 9240, train/grad_norm: 8.738081669434905e-05\n",
      "Step: 9240, train/learning_rate: 1.3350785593502223e-05\n",
      "Step: 9240, train/epoch: 2.1989529132843018\n",
      "Step: 9250, train/loss: 0.0\n",
      "Step: 9250, train/grad_norm: 9.229847819369752e-06\n",
      "Step: 9250, train/learning_rate: 1.3311121620063204e-05\n",
      "Step: 9250, train/epoch: 2.2013328075408936\n",
      "Step: 9260, train/loss: 0.0\n",
      "Step: 9260, train/grad_norm: 4.339441147749312e-05\n",
      "Step: 9260, train/learning_rate: 1.3271457646624185e-05\n",
      "Step: 9260, train/epoch: 2.2037124633789062\n",
      "Step: 9270, train/loss: 0.0\n",
      "Step: 9270, train/grad_norm: 7.305563030968187e-06\n",
      "Step: 9270, train/learning_rate: 1.3231794582679868e-05\n",
      "Step: 9270, train/epoch: 2.206092357635498\n",
      "Step: 9280, train/loss: 0.0\n",
      "Step: 9280, train/grad_norm: 2.097059041261673e-05\n",
      "Step: 9280, train/learning_rate: 1.3192130609240849e-05\n",
      "Step: 9280, train/epoch: 2.20847225189209\n",
      "Step: 9290, train/loss: 0.0\n",
      "Step: 9290, train/grad_norm: 1.4703166925755795e-05\n",
      "Step: 9290, train/learning_rate: 1.315246663580183e-05\n",
      "Step: 9290, train/epoch: 2.2108519077301025\n",
      "Step: 9300, train/loss: 0.0\n",
      "Step: 9300, train/grad_norm: 2.443771154503338e-05\n",
      "Step: 9300, train/learning_rate: 1.3112803571857512e-05\n",
      "Step: 9300, train/epoch: 2.2132318019866943\n",
      "Step: 9310, train/loss: 0.0\n",
      "Step: 9310, train/grad_norm: 6.103090527176391e-06\n",
      "Step: 9310, train/learning_rate: 1.3073139598418493e-05\n",
      "Step: 9310, train/epoch: 2.215611696243286\n",
      "Step: 9320, train/loss: 0.0\n",
      "Step: 9320, train/grad_norm: 3.440932050580159e-05\n",
      "Step: 9320, train/learning_rate: 1.3033476534474175e-05\n",
      "Step: 9320, train/epoch: 2.217991352081299\n",
      "Step: 9330, train/loss: 0.0\n",
      "Step: 9330, train/grad_norm: 2.2589845684706233e-05\n",
      "Step: 9330, train/learning_rate: 1.2993812561035156e-05\n",
      "Step: 9330, train/epoch: 2.2203712463378906\n",
      "Step: 9340, train/loss: 0.0\n",
      "Step: 9340, train/grad_norm: 1.944733776326757e-05\n",
      "Step: 9340, train/learning_rate: 1.2954148587596137e-05\n",
      "Step: 9340, train/epoch: 2.2227511405944824\n",
      "Step: 9350, train/loss: 0.0\n",
      "Step: 9350, train/grad_norm: 1.0896420462813694e-05\n",
      "Step: 9350, train/learning_rate: 1.291448552365182e-05\n",
      "Step: 9350, train/epoch: 2.225130796432495\n",
      "Step: 9360, train/loss: 0.0\n",
      "Step: 9360, train/grad_norm: 4.344591434346512e-05\n",
      "Step: 9360, train/learning_rate: 1.28748215502128e-05\n",
      "Step: 9360, train/epoch: 2.227510690689087\n",
      "Step: 9370, train/loss: 0.0\n",
      "Step: 9370, train/grad_norm: 2.1224339434411377e-05\n",
      "Step: 9370, train/learning_rate: 1.2835157576773781e-05\n",
      "Step: 9370, train/epoch: 2.2298905849456787\n",
      "Step: 9380, train/loss: 0.0\n",
      "Step: 9380, train/grad_norm: 3.034092515008524e-06\n",
      "Step: 9380, train/learning_rate: 1.2795494512829464e-05\n",
      "Step: 9380, train/epoch: 2.2322702407836914\n",
      "Step: 9390, train/loss: 0.0\n",
      "Step: 9390, train/grad_norm: 5.825199423270533e-06\n",
      "Step: 9390, train/learning_rate: 1.2755830539390445e-05\n",
      "Step: 9390, train/epoch: 2.234650135040283\n",
      "Step: 9400, train/loss: 0.0\n",
      "Step: 9400, train/grad_norm: 1.4912683582224417e-05\n",
      "Step: 9400, train/learning_rate: 1.2716166565951426e-05\n",
      "Step: 9400, train/epoch: 2.237030029296875\n",
      "Step: 9410, train/loss: 0.0\n",
      "Step: 9410, train/grad_norm: 1.3017825949646067e-05\n",
      "Step: 9410, train/learning_rate: 1.2676503502007108e-05\n",
      "Step: 9410, train/epoch: 2.239409923553467\n",
      "Step: 9420, train/loss: 0.0\n",
      "Step: 9420, train/grad_norm: 2.066036540782079e-05\n",
      "Step: 9420, train/learning_rate: 1.2636839528568089e-05\n",
      "Step: 9420, train/epoch: 2.2417895793914795\n",
      "Step: 9430, train/loss: 0.0\n",
      "Step: 9430, train/grad_norm: 3.6278346669860184e-05\n",
      "Step: 9430, train/learning_rate: 1.259717555512907e-05\n",
      "Step: 9430, train/epoch: 2.2441694736480713\n",
      "Step: 9440, train/loss: 0.0\n",
      "Step: 9440, train/grad_norm: 5.131308626005193e-06\n",
      "Step: 9440, train/learning_rate: 1.2557512491184752e-05\n",
      "Step: 9440, train/epoch: 2.246549367904663\n",
      "Step: 9450, train/loss: 0.0\n",
      "Step: 9450, train/grad_norm: 0.000551349192392081\n",
      "Step: 9450, train/learning_rate: 1.2517848517745733e-05\n",
      "Step: 9450, train/epoch: 2.248929023742676\n",
      "Step: 9460, train/loss: 0.0\n",
      "Step: 9460, train/grad_norm: 5.727412644773722e-05\n",
      "Step: 9460, train/learning_rate: 1.2478184544306714e-05\n",
      "Step: 9460, train/epoch: 2.2513089179992676\n",
      "Step: 9470, train/loss: 0.0\n",
      "Step: 9470, train/grad_norm: 1.1593710041779559e-05\n",
      "Step: 9470, train/learning_rate: 1.2438521480362397e-05\n",
      "Step: 9470, train/epoch: 2.2536888122558594\n",
      "Step: 9480, train/loss: 0.0\n",
      "Step: 9480, train/grad_norm: 0.00021329536684788764\n",
      "Step: 9480, train/learning_rate: 1.2398857506923378e-05\n",
      "Step: 9480, train/epoch: 2.256068468093872\n",
      "Step: 9490, train/loss: 0.0\n",
      "Step: 9490, train/grad_norm: 2.4175760700018145e-05\n",
      "Step: 9490, train/learning_rate: 1.235919444297906e-05\n",
      "Step: 9490, train/epoch: 2.258448362350464\n",
      "Step: 9500, train/loss: 0.0\n",
      "Step: 9500, train/grad_norm: 4.133855782129103e-06\n",
      "Step: 9500, train/learning_rate: 1.2319530469540041e-05\n",
      "Step: 9500, train/epoch: 2.2608282566070557\n",
      "Step: 9510, train/loss: 0.0\n",
      "Step: 9510, train/grad_norm: 0.0002660535683389753\n",
      "Step: 9510, train/learning_rate: 1.2279866496101022e-05\n",
      "Step: 9510, train/epoch: 2.2632079124450684\n",
      "Step: 9520, train/loss: 0.0\n",
      "Step: 9520, train/grad_norm: 5.37007679213275e-07\n",
      "Step: 9520, train/learning_rate: 1.2240203432156704e-05\n",
      "Step: 9520, train/epoch: 2.26558780670166\n",
      "Step: 9530, train/loss: 0.0\n",
      "Step: 9530, train/grad_norm: 2.5776535039767623e-06\n",
      "Step: 9530, train/learning_rate: 1.2200539458717685e-05\n",
      "Step: 9530, train/epoch: 2.267967700958252\n",
      "Step: 9540, train/loss: 0.0\n",
      "Step: 9540, train/grad_norm: 1.995822458411567e-06\n",
      "Step: 9540, train/learning_rate: 1.2160875485278666e-05\n",
      "Step: 9540, train/epoch: 2.2703473567962646\n",
      "Step: 9550, train/loss: 0.0\n",
      "Step: 9550, train/grad_norm: 1.4015789020049851e-05\n",
      "Step: 9550, train/learning_rate: 1.2121212421334349e-05\n",
      "Step: 9550, train/epoch: 2.2727272510528564\n",
      "Step: 9560, train/loss: 0.0\n",
      "Step: 9560, train/grad_norm: 6.311426113825291e-05\n",
      "Step: 9560, train/learning_rate: 1.208154844789533e-05\n",
      "Step: 9560, train/epoch: 2.2751071453094482\n",
      "Step: 9570, train/loss: 0.0\n",
      "Step: 9570, train/grad_norm: 1.6190782844205387e-05\n",
      "Step: 9570, train/learning_rate: 1.204188447445631e-05\n",
      "Step: 9570, train/epoch: 2.277486801147461\n",
      "Step: 9580, train/loss: 0.0\n",
      "Step: 9580, train/grad_norm: 3.2617346732877195e-05\n",
      "Step: 9580, train/learning_rate: 1.2002221410511993e-05\n",
      "Step: 9580, train/epoch: 2.2798666954040527\n",
      "Step: 9590, train/loss: 0.0\n",
      "Step: 9590, train/grad_norm: 1.5883673768257722e-05\n",
      "Step: 9590, train/learning_rate: 1.1962557437072974e-05\n",
      "Step: 9590, train/epoch: 2.2822465896606445\n",
      "Step: 9600, train/loss: 0.0\n",
      "Step: 9600, train/grad_norm: 2.6832049115910195e-05\n",
      "Step: 9600, train/learning_rate: 1.1922893463633955e-05\n",
      "Step: 9600, train/epoch: 2.2846264839172363\n",
      "Step: 9610, train/loss: 0.0\n",
      "Step: 9610, train/grad_norm: 4.3772946156650505e-08\n",
      "Step: 9610, train/learning_rate: 1.1883230399689637e-05\n",
      "Step: 9610, train/epoch: 2.287006139755249\n",
      "Step: 9620, train/loss: 0.0\n",
      "Step: 9620, train/grad_norm: 0.00010755031689768657\n",
      "Step: 9620, train/learning_rate: 1.1843566426250618e-05\n",
      "Step: 9620, train/epoch: 2.289386034011841\n",
      "Step: 9630, train/loss: 0.0\n",
      "Step: 9630, train/grad_norm: 4.600522879627533e-05\n",
      "Step: 9630, train/learning_rate: 1.1803902452811599e-05\n",
      "Step: 9630, train/epoch: 2.2917659282684326\n",
      "Step: 9640, train/loss: 0.0\n",
      "Step: 9640, train/grad_norm: 0.00012950118980370462\n",
      "Step: 9640, train/learning_rate: 1.1764239388867281e-05\n",
      "Step: 9640, train/epoch: 2.2941455841064453\n",
      "Step: 9650, train/loss: 0.0\n",
      "Step: 9650, train/grad_norm: 6.0647107602562755e-06\n",
      "Step: 9650, train/learning_rate: 1.1724575415428262e-05\n",
      "Step: 9650, train/epoch: 2.296525478363037\n",
      "Step: 9660, train/loss: 0.0\n",
      "Step: 9660, train/grad_norm: 7.620041287736967e-06\n",
      "Step: 9660, train/learning_rate: 1.1684912351483945e-05\n",
      "Step: 9660, train/epoch: 2.298905372619629\n",
      "Step: 9670, train/loss: 0.0\n",
      "Step: 9670, train/grad_norm: 1.2164382496848702e-05\n",
      "Step: 9670, train/learning_rate: 1.1645248378044926e-05\n",
      "Step: 9670, train/epoch: 2.3012850284576416\n",
      "Step: 9680, train/loss: 0.0\n",
      "Step: 9680, train/grad_norm: 2.0599964045686647e-05\n",
      "Step: 9680, train/learning_rate: 1.1605584404605906e-05\n",
      "Step: 9680, train/epoch: 2.3036649227142334\n",
      "Step: 9690, train/loss: 0.0\n",
      "Step: 9690, train/grad_norm: 8.945700392359868e-05\n",
      "Step: 9690, train/learning_rate: 1.1565921340661589e-05\n",
      "Step: 9690, train/epoch: 2.306044816970825\n",
      "Step: 9700, train/loss: 0.0\n",
      "Step: 9700, train/grad_norm: 1.1748585166060366e-05\n",
      "Step: 9700, train/learning_rate: 1.152625736722257e-05\n",
      "Step: 9700, train/epoch: 2.308424472808838\n",
      "Step: 9710, train/loss: 0.0\n",
      "Step: 9710, train/grad_norm: 5.538355981116183e-05\n",
      "Step: 9710, train/learning_rate: 1.148659339378355e-05\n",
      "Step: 9710, train/epoch: 2.3108043670654297\n",
      "Step: 9720, train/loss: 0.0\n",
      "Step: 9720, train/grad_norm: 8.059148967731744e-06\n",
      "Step: 9720, train/learning_rate: 1.1446930329839233e-05\n",
      "Step: 9720, train/epoch: 2.3131842613220215\n",
      "Step: 9730, train/loss: 0.0\n",
      "Step: 9730, train/grad_norm: 4.4508051360026e-06\n",
      "Step: 9730, train/learning_rate: 1.1407266356400214e-05\n",
      "Step: 9730, train/epoch: 2.315563917160034\n",
      "Step: 9740, train/loss: 0.0\n",
      "Step: 9740, train/grad_norm: 4.305501533963252e-06\n",
      "Step: 9740, train/learning_rate: 1.1367602382961195e-05\n",
      "Step: 9740, train/epoch: 2.317943811416626\n",
      "Step: 9750, train/loss: 0.0\n",
      "Step: 9750, train/grad_norm: 1.1415601875341963e-05\n",
      "Step: 9750, train/learning_rate: 1.1327939319016878e-05\n",
      "Step: 9750, train/epoch: 2.3203237056732178\n",
      "Step: 9760, train/loss: 0.0\n",
      "Step: 9760, train/grad_norm: 1.5557550796074793e-05\n",
      "Step: 9760, train/learning_rate: 1.1288275345577858e-05\n",
      "Step: 9760, train/epoch: 2.3227033615112305\n",
      "Step: 9770, train/loss: 0.0\n",
      "Step: 9770, train/grad_norm: 1.715893449727446e-05\n",
      "Step: 9770, train/learning_rate: 1.124861137213884e-05\n",
      "Step: 9770, train/epoch: 2.3250832557678223\n",
      "Step: 9780, train/loss: 0.0\n",
      "Step: 9780, train/grad_norm: 2.1165062207728624e-05\n",
      "Step: 9780, train/learning_rate: 1.1208948308194522e-05\n",
      "Step: 9780, train/epoch: 2.327463150024414\n",
      "Step: 9790, train/loss: 0.0\n",
      "Step: 9790, train/grad_norm: 1.9086590327788144e-05\n",
      "Step: 9790, train/learning_rate: 1.1169284334755503e-05\n",
      "Step: 9790, train/epoch: 2.329843044281006\n",
      "Step: 9800, train/loss: 0.0\n",
      "Step: 9800, train/grad_norm: 1.2940708984388039e-05\n",
      "Step: 9800, train/learning_rate: 1.1129620361316483e-05\n",
      "Step: 9800, train/epoch: 2.3322227001190186\n",
      "Step: 9810, train/loss: 0.0\n",
      "Step: 9810, train/grad_norm: 0.00020282903278712183\n",
      "Step: 9810, train/learning_rate: 1.1089957297372166e-05\n",
      "Step: 9810, train/epoch: 2.3346025943756104\n",
      "Step: 9820, train/loss: 0.0\n",
      "Step: 9820, train/grad_norm: 2.634525480971206e-05\n",
      "Step: 9820, train/learning_rate: 1.1050293323933147e-05\n",
      "Step: 9820, train/epoch: 2.336982488632202\n",
      "Step: 9830, train/loss: 0.0\n",
      "Step: 9830, train/grad_norm: 1.047268324327888e-05\n",
      "Step: 9830, train/learning_rate: 1.101063025998883e-05\n",
      "Step: 9830, train/epoch: 2.339362144470215\n",
      "Step: 9840, train/loss: 0.0\n",
      "Step: 9840, train/grad_norm: 5.157813575351611e-05\n",
      "Step: 9840, train/learning_rate: 1.097096628654981e-05\n",
      "Step: 9840, train/epoch: 2.3417420387268066\n",
      "Step: 9850, train/loss: 0.0\n",
      "Step: 9850, train/grad_norm: 2.3160751879913732e-05\n",
      "Step: 9850, train/learning_rate: 1.0931302313110791e-05\n",
      "Step: 9850, train/epoch: 2.3441219329833984\n",
      "Step: 9860, train/loss: 0.0\n",
      "Step: 9860, train/grad_norm: 2.447392762405798e-05\n",
      "Step: 9860, train/learning_rate: 1.0891639249166474e-05\n",
      "Step: 9860, train/epoch: 2.346501588821411\n",
      "Step: 9870, train/loss: 0.0\n",
      "Step: 9870, train/grad_norm: 4.149636879446916e-05\n",
      "Step: 9870, train/learning_rate: 1.0851975275727455e-05\n",
      "Step: 9870, train/epoch: 2.348881483078003\n",
      "Step: 9880, train/loss: 0.0\n",
      "Step: 9880, train/grad_norm: 3.6848592571914196e-05\n",
      "Step: 9880, train/learning_rate: 1.0812311302288435e-05\n",
      "Step: 9880, train/epoch: 2.3512613773345947\n",
      "Step: 9890, train/loss: 0.0\n",
      "Step: 9890, train/grad_norm: 5.267689994070679e-05\n",
      "Step: 9890, train/learning_rate: 1.0772648238344118e-05\n",
      "Step: 9890, train/epoch: 2.3536410331726074\n",
      "Step: 9900, train/loss: 0.0\n",
      "Step: 9900, train/grad_norm: 2.757646143436432e-05\n",
      "Step: 9900, train/learning_rate: 1.0732984264905099e-05\n",
      "Step: 9900, train/epoch: 2.356020927429199\n",
      "Step: 9910, train/loss: 0.0\n",
      "Step: 9910, train/grad_norm: 0.00042190708336420357\n",
      "Step: 9910, train/learning_rate: 1.069332029146608e-05\n",
      "Step: 9910, train/epoch: 2.358400821685791\n",
      "Step: 9920, train/loss: 0.0\n",
      "Step: 9920, train/grad_norm: 5.4468523558171e-06\n",
      "Step: 9920, train/learning_rate: 1.0653657227521762e-05\n",
      "Step: 9920, train/epoch: 2.3607804775238037\n",
      "Step: 9930, train/loss: 0.0\n",
      "Step: 9930, train/grad_norm: 2.459519464537152e-06\n",
      "Step: 9930, train/learning_rate: 1.0613993254082743e-05\n",
      "Step: 9930, train/epoch: 2.3631603717803955\n",
      "Step: 9940, train/loss: 0.0\n",
      "Step: 9940, train/grad_norm: 1.2259501090738922e-05\n",
      "Step: 9940, train/learning_rate: 1.0574329280643724e-05\n",
      "Step: 9940, train/epoch: 2.3655402660369873\n",
      "Step: 9950, train/loss: 0.0\n",
      "Step: 9950, train/grad_norm: 7.339406874962151e-05\n",
      "Step: 9950, train/learning_rate: 1.0534666216699407e-05\n",
      "Step: 9950, train/epoch: 2.367919921875\n",
      "Step: 9960, train/loss: 0.0\n",
      "Step: 9960, train/grad_norm: 1.534745388198644e-05\n",
      "Step: 9960, train/learning_rate: 1.0495002243260387e-05\n",
      "Step: 9960, train/epoch: 2.370299816131592\n",
      "Step: 9970, train/loss: 0.0\n",
      "Step: 9970, train/grad_norm: 2.1843839931534603e-05\n",
      "Step: 9970, train/learning_rate: 1.045533917931607e-05\n",
      "Step: 9970, train/epoch: 2.3726797103881836\n",
      "Step: 9980, train/loss: 0.0\n",
      "Step: 9980, train/grad_norm: 1.8696850020205602e-05\n",
      "Step: 9980, train/learning_rate: 1.041567520587705e-05\n",
      "Step: 9980, train/epoch: 2.3750596046447754\n",
      "Step: 9990, train/loss: 0.0\n",
      "Step: 9990, train/grad_norm: 0.0005747277173213661\n",
      "Step: 9990, train/learning_rate: 1.0376011232438032e-05\n",
      "Step: 9990, train/epoch: 2.377439260482788\n",
      "Step: 10000, train/loss: 0.0\n",
      "Step: 10000, train/grad_norm: 1.7105427104979753e-05\n",
      "Step: 10000, train/learning_rate: 1.0336348168493714e-05\n",
      "Step: 10000, train/epoch: 2.37981915473938\n",
      "Step: 10010, train/loss: 0.0\n",
      "Step: 10010, train/grad_norm: 5.057081580162048e-06\n",
      "Step: 10010, train/learning_rate: 1.0296684195054695e-05\n",
      "Step: 10010, train/epoch: 2.3821990489959717\n",
      "Step: 10020, train/loss: 0.0\n",
      "Step: 10020, train/grad_norm: 5.321846401784569e-06\n",
      "Step: 10020, train/learning_rate: 1.0257020221615676e-05\n",
      "Step: 10020, train/epoch: 2.3845787048339844\n",
      "Step: 10030, train/loss: 0.0\n",
      "Step: 10030, train/grad_norm: 7.724122042418458e-06\n",
      "Step: 10030, train/learning_rate: 1.0217357157671358e-05\n",
      "Step: 10030, train/epoch: 2.386958599090576\n",
      "Step: 10040, train/loss: 0.0\n",
      "Step: 10040, train/grad_norm: 2.3267997676157393e-05\n",
      "Step: 10040, train/learning_rate: 1.017769318423234e-05\n",
      "Step: 10040, train/epoch: 2.389338493347168\n",
      "Step: 10050, train/loss: 0.0\n",
      "Step: 10050, train/grad_norm: 1.6451631381642073e-05\n",
      "Step: 10050, train/learning_rate: 1.013802921079332e-05\n",
      "Step: 10050, train/epoch: 2.3917181491851807\n",
      "Step: 10060, train/loss: 0.0\n",
      "Step: 10060, train/grad_norm: 2.6966416044160724e-05\n",
      "Step: 10060, train/learning_rate: 1.0098366146849003e-05\n",
      "Step: 10060, train/epoch: 2.3940980434417725\n",
      "Step: 10070, train/loss: 0.0\n",
      "Step: 10070, train/grad_norm: 2.142935227311682e-05\n",
      "Step: 10070, train/learning_rate: 1.0058702173409984e-05\n",
      "Step: 10070, train/epoch: 2.3964779376983643\n",
      "Step: 10080, train/loss: 0.00039999998989515007\n",
      "Step: 10080, train/grad_norm: 6.196135018399218e-07\n",
      "Step: 10080, train/learning_rate: 1.0019038199970964e-05\n",
      "Step: 10080, train/epoch: 2.398857593536377\n",
      "Step: 10090, train/loss: 0.0\n",
      "Step: 10090, train/grad_norm: 1.3075423339614645e-05\n",
      "Step: 10090, train/learning_rate: 9.979375136026647e-06\n",
      "Step: 10090, train/epoch: 2.4012374877929688\n",
      "Step: 10100, train/loss: 0.0\n",
      "Step: 10100, train/grad_norm: 1.7283518900512718e-05\n",
      "Step: 10100, train/learning_rate: 9.939711162587628e-06\n",
      "Step: 10100, train/epoch: 2.4036173820495605\n",
      "Step: 10110, train/loss: 0.0\n",
      "Step: 10110, train/grad_norm: 2.9289442409208277e-06\n",
      "Step: 10110, train/learning_rate: 9.900047189148609e-06\n",
      "Step: 10110, train/epoch: 2.4059970378875732\n",
      "Step: 10120, train/loss: 0.0\n",
      "Step: 10120, train/grad_norm: 4.70316044811625e-05\n",
      "Step: 10120, train/learning_rate: 9.860384125204291e-06\n",
      "Step: 10120, train/epoch: 2.408376932144165\n",
      "Step: 10130, train/loss: 0.0\n",
      "Step: 10130, train/grad_norm: 0.010390189476311207\n",
      "Step: 10130, train/learning_rate: 9.820720151765272e-06\n",
      "Step: 10130, train/epoch: 2.410756826400757\n",
      "Step: 10140, train/loss: 0.0\n",
      "Step: 10140, train/grad_norm: 7.091887709975708e-06\n",
      "Step: 10140, train/learning_rate: 9.781057087820955e-06\n",
      "Step: 10140, train/epoch: 2.4131367206573486\n",
      "Step: 10150, train/loss: 0.0\n",
      "Step: 10150, train/grad_norm: 1.2796353075827938e-05\n",
      "Step: 10150, train/learning_rate: 9.741393114381935e-06\n",
      "Step: 10150, train/epoch: 2.4155163764953613\n",
      "Step: 10160, train/loss: 0.0\n",
      "Step: 10160, train/grad_norm: 5.111476639285684e-06\n",
      "Step: 10160, train/learning_rate: 9.701729140942916e-06\n",
      "Step: 10160, train/epoch: 2.417896270751953\n",
      "Step: 10170, train/loss: 0.0\n",
      "Step: 10170, train/grad_norm: 1.848490683187265e-05\n",
      "Step: 10170, train/learning_rate: 9.662066076998599e-06\n",
      "Step: 10170, train/epoch: 2.420276165008545\n",
      "Step: 10180, train/loss: 0.0\n",
      "Step: 10180, train/grad_norm: 1.2296043678361457e-05\n",
      "Step: 10180, train/learning_rate: 9.62240210355958e-06\n",
      "Step: 10180, train/epoch: 2.4226558208465576\n",
      "Step: 10190, train/loss: 0.0\n",
      "Step: 10190, train/grad_norm: 8.48134804982692e-05\n",
      "Step: 10190, train/learning_rate: 9.58273813012056e-06\n",
      "Step: 10190, train/epoch: 2.4250357151031494\n",
      "Step: 10200, train/loss: 0.0\n",
      "Step: 10200, train/grad_norm: 2.5490358893875964e-05\n",
      "Step: 10200, train/learning_rate: 9.543075066176243e-06\n",
      "Step: 10200, train/epoch: 2.427415609359741\n",
      "Step: 10210, train/loss: 0.0\n",
      "Step: 10210, train/grad_norm: 1.3945654245617334e-05\n",
      "Step: 10210, train/learning_rate: 9.503411092737224e-06\n",
      "Step: 10210, train/epoch: 2.429795265197754\n",
      "Step: 10220, train/loss: 0.0\n",
      "Step: 10220, train/grad_norm: 4.850257755606435e-05\n",
      "Step: 10220, train/learning_rate: 9.463747119298205e-06\n",
      "Step: 10220, train/epoch: 2.4321751594543457\n",
      "Step: 10230, train/loss: 0.0\n",
      "Step: 10230, train/grad_norm: 3.980980909545906e-05\n",
      "Step: 10230, train/learning_rate: 9.424084055353887e-06\n",
      "Step: 10230, train/epoch: 2.4345550537109375\n",
      "Step: 10240, train/loss: 0.0\n",
      "Step: 10240, train/grad_norm: 3.176520112901926e-05\n",
      "Step: 10240, train/learning_rate: 9.384420081914868e-06\n",
      "Step: 10240, train/epoch: 2.43693470954895\n",
      "Step: 10250, train/loss: 0.0\n",
      "Step: 10250, train/grad_norm: 0.00017137777467723936\n",
      "Step: 10250, train/learning_rate: 9.344756108475849e-06\n",
      "Step: 10250, train/epoch: 2.439314603805542\n",
      "Step: 10260, train/loss: 0.0\n",
      "Step: 10260, train/grad_norm: 0.00010801858297782019\n",
      "Step: 10260, train/learning_rate: 9.305093044531532e-06\n",
      "Step: 10260, train/epoch: 2.441694498062134\n",
      "Step: 10270, train/loss: 0.0\n",
      "Step: 10270, train/grad_norm: 1.2834016160923056e-05\n",
      "Step: 10270, train/learning_rate: 9.265429071092512e-06\n",
      "Step: 10270, train/epoch: 2.4440741539001465\n",
      "Step: 10280, train/loss: 0.0\n",
      "Step: 10280, train/grad_norm: 7.055602054606425e-06\n",
      "Step: 10280, train/learning_rate: 9.225765097653493e-06\n",
      "Step: 10280, train/epoch: 2.4464540481567383\n",
      "Step: 10290, train/loss: 0.0\n",
      "Step: 10290, train/grad_norm: 5.199147835810436e-06\n",
      "Step: 10290, train/learning_rate: 9.186102033709176e-06\n",
      "Step: 10290, train/epoch: 2.44883394241333\n",
      "Step: 10300, train/loss: 0.0\n",
      "Step: 10300, train/grad_norm: 2.8957833819731604e-07\n",
      "Step: 10300, train/learning_rate: 9.146438060270157e-06\n",
      "Step: 10300, train/epoch: 2.4512135982513428\n",
      "Step: 10310, train/loss: 0.0\n",
      "Step: 10310, train/grad_norm: 2.7445450996310683e-06\n",
      "Step: 10310, train/learning_rate: 9.10677499632584e-06\n",
      "Step: 10310, train/epoch: 2.4535934925079346\n",
      "Step: 10320, train/loss: 0.0\n",
      "Step: 10320, train/grad_norm: 4.122452810406685e-05\n",
      "Step: 10320, train/learning_rate: 9.06711102288682e-06\n",
      "Step: 10320, train/epoch: 2.4559733867645264\n",
      "Step: 10330, train/loss: 0.0\n",
      "Step: 10330, train/grad_norm: 8.850081212585792e-05\n",
      "Step: 10330, train/learning_rate: 9.027447049447801e-06\n",
      "Step: 10330, train/epoch: 2.458353281021118\n",
      "Step: 10340, train/loss: 0.0\n",
      "Step: 10340, train/grad_norm: 7.892986104707234e-06\n",
      "Step: 10340, train/learning_rate: 8.987783985503484e-06\n",
      "Step: 10340, train/epoch: 2.460732936859131\n",
      "Step: 10350, train/loss: 0.0\n",
      "Step: 10350, train/grad_norm: 1.7468320947955362e-05\n",
      "Step: 10350, train/learning_rate: 8.948120012064464e-06\n",
      "Step: 10350, train/epoch: 2.4631128311157227\n",
      "Step: 10360, train/loss: 0.0\n",
      "Step: 10360, train/grad_norm: 4.597222141455859e-05\n",
      "Step: 10360, train/learning_rate: 8.908456038625445e-06\n",
      "Step: 10360, train/epoch: 2.4654927253723145\n",
      "Step: 10370, train/loss: 0.15000000596046448\n",
      "Step: 10370, train/grad_norm: 3.913583896064665e-06\n",
      "Step: 10370, train/learning_rate: 8.868792974681128e-06\n",
      "Step: 10370, train/epoch: 2.467872381210327\n",
      "Step: 10380, train/loss: 0.0\n",
      "Step: 10380, train/grad_norm: 1.1533159522514325e-05\n",
      "Step: 10380, train/learning_rate: 8.829129001242109e-06\n",
      "Step: 10380, train/epoch: 2.470252275466919\n",
      "Step: 10390, train/loss: 0.0\n",
      "Step: 10390, train/grad_norm: 1.3881123777537141e-05\n",
      "Step: 10390, train/learning_rate: 8.78946502780309e-06\n",
      "Step: 10390, train/epoch: 2.4726321697235107\n",
      "Step: 10400, train/loss: 0.0\n",
      "Step: 10400, train/grad_norm: 1.820692341425456e-05\n",
      "Step: 10400, train/learning_rate: 8.749801963858772e-06\n",
      "Step: 10400, train/epoch: 2.4750118255615234\n",
      "Step: 10410, train/loss: 0.0\n",
      "Step: 10410, train/grad_norm: 8.962058200268075e-05\n",
      "Step: 10410, train/learning_rate: 8.710137990419753e-06\n",
      "Step: 10410, train/epoch: 2.4773917198181152\n",
      "Step: 10420, train/loss: 0.0\n",
      "Step: 10420, train/grad_norm: 8.006499570001324e-07\n",
      "Step: 10420, train/learning_rate: 8.670474016980734e-06\n",
      "Step: 10420, train/epoch: 2.479771614074707\n",
      "Step: 10430, train/loss: 0.07620000094175339\n",
      "Step: 10430, train/grad_norm: 9.768980817170814e-06\n",
      "Step: 10430, train/learning_rate: 8.630810953036416e-06\n",
      "Step: 10430, train/epoch: 2.4821512699127197\n",
      "Step: 10440, train/loss: 0.0\n",
      "Step: 10440, train/grad_norm: 1.004732712317491e-05\n",
      "Step: 10440, train/learning_rate: 8.591146979597397e-06\n",
      "Step: 10440, train/epoch: 2.4845311641693115\n",
      "Step: 10450, train/loss: 0.0\n",
      "Step: 10450, train/grad_norm: 5.399999736255268e-06\n",
      "Step: 10450, train/learning_rate: 8.551483006158378e-06\n",
      "Step: 10450, train/epoch: 2.4869110584259033\n",
      "Step: 10460, train/loss: 0.0\n",
      "Step: 10460, train/grad_norm: 2.2677048036712222e-05\n",
      "Step: 10460, train/learning_rate: 8.51181994221406e-06\n",
      "Step: 10460, train/epoch: 2.489290714263916\n",
      "Step: 10470, train/loss: 0.0\n",
      "Step: 10470, train/grad_norm: 4.558217824524036e-06\n",
      "Step: 10470, train/learning_rate: 8.472155968775041e-06\n",
      "Step: 10470, train/epoch: 2.491670608520508\n",
      "Step: 10480, train/loss: 0.0\n",
      "Step: 10480, train/grad_norm: 9.73744499788154e-06\n",
      "Step: 10480, train/learning_rate: 8.432492904830724e-06\n",
      "Step: 10480, train/epoch: 2.4940505027770996\n",
      "Step: 10490, train/loss: 0.0\n",
      "Step: 10490, train/grad_norm: 1.780356433300767e-05\n",
      "Step: 10490, train/learning_rate: 8.392828931391705e-06\n",
      "Step: 10490, train/epoch: 2.4964301586151123\n",
      "Step: 10500, train/loss: 0.0\n",
      "Step: 10500, train/grad_norm: 7.721333531662822e-06\n",
      "Step: 10500, train/learning_rate: 8.353164957952686e-06\n",
      "Step: 10500, train/epoch: 2.498810052871704\n",
      "Step: 10510, train/loss: 0.0\n",
      "Step: 10510, train/grad_norm: 2.8747635951731354e-05\n",
      "Step: 10510, train/learning_rate: 8.313501894008368e-06\n",
      "Step: 10510, train/epoch: 2.501189947128296\n",
      "Step: 10520, train/loss: 0.0\n",
      "Step: 10520, train/grad_norm: 2.3621812488272553e-06\n",
      "Step: 10520, train/learning_rate: 8.273837920569349e-06\n",
      "Step: 10520, train/epoch: 2.5035698413848877\n",
      "Step: 10530, train/loss: 0.0\n",
      "Step: 10530, train/grad_norm: 3.213814488844946e-05\n",
      "Step: 10530, train/learning_rate: 8.23417394713033e-06\n",
      "Step: 10530, train/epoch: 2.5059494972229004\n",
      "Step: 10540, train/loss: 0.0\n",
      "Step: 10540, train/grad_norm: 0.0001136584542109631\n",
      "Step: 10540, train/learning_rate: 8.194510883186013e-06\n",
      "Step: 10540, train/epoch: 2.508329391479492\n",
      "Step: 10550, train/loss: 0.0\n",
      "Step: 10550, train/grad_norm: 4.538734629022656e-06\n",
      "Step: 10550, train/learning_rate: 8.154846909746993e-06\n",
      "Step: 10550, train/epoch: 2.510709285736084\n",
      "Step: 10560, train/loss: 0.0\n",
      "Step: 10560, train/grad_norm: 0.0004272063379175961\n",
      "Step: 10560, train/learning_rate: 8.115182936307974e-06\n",
      "Step: 10560, train/epoch: 2.5130889415740967\n",
      "Step: 10570, train/loss: 0.0\n",
      "Step: 10570, train/grad_norm: 1.5200407688098494e-05\n",
      "Step: 10570, train/learning_rate: 8.075519872363657e-06\n",
      "Step: 10570, train/epoch: 2.5154688358306885\n",
      "Step: 10580, train/loss: 0.0\n",
      "Step: 10580, train/grad_norm: 9.952517757483292e-06\n",
      "Step: 10580, train/learning_rate: 8.035855898924638e-06\n",
      "Step: 10580, train/epoch: 2.5178487300872803\n",
      "Step: 10590, train/loss: 0.0\n",
      "Step: 10590, train/grad_norm: 1.0489713531569578e-05\n",
      "Step: 10590, train/learning_rate: 7.996191925485618e-06\n",
      "Step: 10590, train/epoch: 2.520228385925293\n",
      "Step: 10600, train/loss: 0.0\n",
      "Step: 10600, train/grad_norm: 2.114291874022456e-06\n",
      "Step: 10600, train/learning_rate: 7.956528861541301e-06\n",
      "Step: 10600, train/epoch: 2.5226082801818848\n",
      "Step: 10610, train/loss: 0.0\n",
      "Step: 10610, train/grad_norm: 5.2746094297617674e-05\n",
      "Step: 10610, train/learning_rate: 7.916864888102282e-06\n",
      "Step: 10610, train/epoch: 2.5249881744384766\n",
      "Step: 10620, train/loss: 0.0\n",
      "Step: 10620, train/grad_norm: 5.197036898607621e-06\n",
      "Step: 10620, train/learning_rate: 7.877200914663263e-06\n",
      "Step: 10620, train/epoch: 2.5273678302764893\n",
      "Step: 10630, train/loss: 0.0\n",
      "Step: 10630, train/grad_norm: 4.2329306779720355e-06\n",
      "Step: 10630, train/learning_rate: 7.837537850718945e-06\n",
      "Step: 10630, train/epoch: 2.529747724533081\n",
      "Step: 10640, train/loss: 0.0\n",
      "Step: 10640, train/grad_norm: 2.316093559784349e-05\n",
      "Step: 10640, train/learning_rate: 7.797873877279926e-06\n",
      "Step: 10640, train/epoch: 2.532127618789673\n",
      "Step: 10650, train/loss: 0.0\n",
      "Step: 10650, train/grad_norm: 9.281953339268512e-07\n",
      "Step: 10650, train/learning_rate: 7.758210813335609e-06\n",
      "Step: 10650, train/epoch: 2.5345072746276855\n",
      "Step: 10660, train/loss: 0.0\n",
      "Step: 10660, train/grad_norm: 5.1408616855042055e-05\n",
      "Step: 10660, train/learning_rate: 7.71854683989659e-06\n",
      "Step: 10660, train/epoch: 2.5368871688842773\n",
      "Step: 10670, train/loss: 0.0\n",
      "Step: 10670, train/grad_norm: 1.1881435966643039e-05\n",
      "Step: 10670, train/learning_rate: 7.67888286645757e-06\n",
      "Step: 10670, train/epoch: 2.539267063140869\n",
      "Step: 10680, train/loss: 0.0\n",
      "Step: 10680, train/grad_norm: 1.1387151062081102e-05\n",
      "Step: 10680, train/learning_rate: 7.639219802513253e-06\n",
      "Step: 10680, train/epoch: 2.541646718978882\n",
      "Step: 10690, train/loss: 0.0\n",
      "Step: 10690, train/grad_norm: 5.147296178620309e-06\n",
      "Step: 10690, train/learning_rate: 7.599555829074234e-06\n",
      "Step: 10690, train/epoch: 2.5440266132354736\n",
      "Step: 10700, train/loss: 0.0\n",
      "Step: 10700, train/grad_norm: 2.598850551294163e-05\n",
      "Step: 10700, train/learning_rate: 7.5598923103825655e-06\n",
      "Step: 10700, train/epoch: 2.5464065074920654\n",
      "Step: 10710, train/loss: 0.0\n",
      "Step: 10710, train/grad_norm: 2.649674024723936e-05\n",
      "Step: 10710, train/learning_rate: 7.520228336943546e-06\n",
      "Step: 10710, train/epoch: 2.5487864017486572\n",
      "Step: 10720, train/loss: 0.0\n",
      "Step: 10720, train/grad_norm: 4.471161446417682e-05\n",
      "Step: 10720, train/learning_rate: 7.480564818251878e-06\n",
      "Step: 10720, train/epoch: 2.55116605758667\n",
      "Step: 10730, train/loss: 0.0\n",
      "Step: 10730, train/grad_norm: 1.4710341929458082e-05\n",
      "Step: 10730, train/learning_rate: 7.44090129956021e-06\n",
      "Step: 10730, train/epoch: 2.5535459518432617\n",
      "Step: 10740, train/loss: 0.0\n",
      "Step: 10740, train/grad_norm: 3.556718002073467e-05\n",
      "Step: 10740, train/learning_rate: 7.4012373261211906e-06\n",
      "Step: 10740, train/epoch: 2.5559258460998535\n",
      "Step: 10750, train/loss: 0.0\n",
      "Step: 10750, train/grad_norm: 1.3232925084594171e-05\n",
      "Step: 10750, train/learning_rate: 7.361573807429522e-06\n",
      "Step: 10750, train/epoch: 2.558305501937866\n",
      "Step: 10760, train/loss: 0.0\n",
      "Step: 10760, train/grad_norm: 2.319242776138708e-05\n",
      "Step: 10760, train/learning_rate: 7.321910288737854e-06\n",
      "Step: 10760, train/epoch: 2.560685396194458\n",
      "Step: 10770, train/loss: 0.0\n",
      "Step: 10770, train/grad_norm: 0.0001079336361726746\n",
      "Step: 10770, train/learning_rate: 7.282246770046186e-06\n",
      "Step: 10770, train/epoch: 2.56306529045105\n",
      "Step: 10780, train/loss: 0.0\n",
      "Step: 10780, train/grad_norm: 4.527393002717872e-07\n",
      "Step: 10780, train/learning_rate: 7.2425827966071665e-06\n",
      "Step: 10780, train/epoch: 2.5654449462890625\n",
      "Step: 10790, train/loss: 0.0\n",
      "Step: 10790, train/grad_norm: 0.00025901576736941934\n",
      "Step: 10790, train/learning_rate: 7.202919277915498e-06\n",
      "Step: 10790, train/epoch: 2.5678248405456543\n",
      "Step: 10800, train/loss: 0.0\n",
      "Step: 10800, train/grad_norm: 1.475388853577897e-05\n",
      "Step: 10800, train/learning_rate: 7.16325575922383e-06\n",
      "Step: 10800, train/epoch: 2.570204734802246\n",
      "Step: 10810, train/loss: 0.0\n",
      "Step: 10810, train/grad_norm: 2.161357224395033e-05\n",
      "Step: 10810, train/learning_rate: 7.123591785784811e-06\n",
      "Step: 10810, train/epoch: 2.572584390640259\n",
      "Step: 10820, train/loss: 0.0\n",
      "Step: 10820, train/grad_norm: 0.0001487166591687128\n",
      "Step: 10820, train/learning_rate: 7.0839282670931425e-06\n",
      "Step: 10820, train/epoch: 2.5749642848968506\n",
      "Step: 10830, train/loss: 0.0005000000237487257\n",
      "Step: 10830, train/grad_norm: 2.0243312974344008e-05\n",
      "Step: 10830, train/learning_rate: 7.044264748401474e-06\n",
      "Step: 10830, train/epoch: 2.5773441791534424\n",
      "Step: 10840, train/loss: 0.0\n",
      "Step: 10840, train/grad_norm: 5.10776590090245e-06\n",
      "Step: 10840, train/learning_rate: 7.004600774962455e-06\n",
      "Step: 10840, train/epoch: 2.579723834991455\n",
      "Step: 10850, train/loss: 0.0\n",
      "Step: 10850, train/grad_norm: 1.554463779029902e-05\n",
      "Step: 10850, train/learning_rate: 6.964937256270787e-06\n",
      "Step: 10850, train/epoch: 2.582103729248047\n",
      "Step: 10860, train/loss: 0.0\n",
      "Step: 10860, train/grad_norm: 5.269379471428692e-06\n",
      "Step: 10860, train/learning_rate: 6.9252737375791185e-06\n",
      "Step: 10860, train/epoch: 2.5844836235046387\n",
      "Step: 10870, train/loss: 0.0\n",
      "Step: 10870, train/grad_norm: 1.3591197784990072e-05\n",
      "Step: 10870, train/learning_rate: 6.88561021888745e-06\n",
      "Step: 10870, train/epoch: 2.5868632793426514\n",
      "Step: 10880, train/loss: 0.0\n",
      "Step: 10880, train/grad_norm: 2.871768083423376e-05\n",
      "Step: 10880, train/learning_rate: 6.845946245448431e-06\n",
      "Step: 10880, train/epoch: 2.589243173599243\n",
      "Step: 10890, train/loss: 0.0\n",
      "Step: 10890, train/grad_norm: 8.478504605591297e-05\n",
      "Step: 10890, train/learning_rate: 6.806282726756763e-06\n",
      "Step: 10890, train/epoch: 2.591623067855835\n",
      "Step: 10900, train/loss: 0.0\n",
      "Step: 10900, train/grad_norm: 1.0621975889080204e-05\n",
      "Step: 10900, train/learning_rate: 6.7666192080650944e-06\n",
      "Step: 10900, train/epoch: 2.5940029621124268\n",
      "Step: 10910, train/loss: 0.0\n",
      "Step: 10910, train/grad_norm: 1.5773961422382854e-05\n",
      "Step: 10910, train/learning_rate: 6.726955234626075e-06\n",
      "Step: 10910, train/epoch: 2.5963826179504395\n",
      "Step: 10920, train/loss: 0.0\n",
      "Step: 10920, train/grad_norm: 9.02929230051086e-07\n",
      "Step: 10920, train/learning_rate: 6.687291715934407e-06\n",
      "Step: 10920, train/epoch: 2.5987625122070312\n",
      "Step: 10930, train/loss: 0.0\n",
      "Step: 10930, train/grad_norm: 3.5370169371162774e-06\n",
      "Step: 10930, train/learning_rate: 6.647628197242739e-06\n",
      "Step: 10930, train/epoch: 2.601142406463623\n",
      "Step: 10940, train/loss: 0.0\n",
      "Step: 10940, train/grad_norm: 2.3866010451456532e-05\n",
      "Step: 10940, train/learning_rate: 6.60796467855107e-06\n",
      "Step: 10940, train/epoch: 2.6035220623016357\n",
      "Step: 10950, train/loss: 0.0\n",
      "Step: 10950, train/grad_norm: 3.321573103676201e-06\n",
      "Step: 10950, train/learning_rate: 6.568300705112051e-06\n",
      "Step: 10950, train/epoch: 2.6059019565582275\n",
      "Step: 10960, train/loss: 0.0\n",
      "Step: 10960, train/grad_norm: 5.756605332862819e-06\n",
      "Step: 10960, train/learning_rate: 6.528637186420383e-06\n",
      "Step: 10960, train/epoch: 2.6082818508148193\n",
      "Step: 10970, train/loss: 0.0\n",
      "Step: 10970, train/grad_norm: 1.4073045349505264e-05\n",
      "Step: 10970, train/learning_rate: 6.488973667728715e-06\n",
      "Step: 10970, train/epoch: 2.610661506652832\n",
      "Step: 10980, train/loss: 0.0\n",
      "Step: 10980, train/grad_norm: 0.00010822521289810538\n",
      "Step: 10980, train/learning_rate: 6.4493096942896955e-06\n",
      "Step: 10980, train/epoch: 2.613041400909424\n",
      "Step: 10990, train/loss: 0.00430000014603138\n",
      "Step: 10990, train/grad_norm: 1.0124952495971229e-05\n",
      "Step: 10990, train/learning_rate: 6.409646175598027e-06\n",
      "Step: 10990, train/epoch: 2.6154212951660156\n",
      "Step: 11000, train/loss: 0.0\n",
      "Step: 11000, train/grad_norm: 7.41064013709547e-07\n",
      "Step: 11000, train/learning_rate: 6.369982656906359e-06\n",
      "Step: 11000, train/epoch: 2.6178009510040283\n",
      "Step: 11010, train/loss: 0.0\n",
      "Step: 11010, train/grad_norm: 1.2793069799954537e-05\n",
      "Step: 11010, train/learning_rate: 6.33031868346734e-06\n",
      "Step: 11010, train/epoch: 2.62018084526062\n",
      "Step: 11020, train/loss: 0.0\n",
      "Step: 11020, train/grad_norm: 4.809783717973914e-07\n",
      "Step: 11020, train/learning_rate: 6.2906551647756714e-06\n",
      "Step: 11020, train/epoch: 2.622560739517212\n",
      "Step: 11030, train/loss: 0.0\n",
      "Step: 11030, train/grad_norm: 1.4666092283732723e-05\n",
      "Step: 11030, train/learning_rate: 6.250991646084003e-06\n",
      "Step: 11030, train/epoch: 2.6249403953552246\n",
      "Step: 11040, train/loss: 0.0\n",
      "Step: 11040, train/grad_norm: 4.466734026209451e-05\n",
      "Step: 11040, train/learning_rate: 6.211328127392335e-06\n",
      "Step: 11040, train/epoch: 2.6273202896118164\n",
      "Step: 11050, train/loss: 0.0\n",
      "Step: 11050, train/grad_norm: 7.550569080194691e-06\n",
      "Step: 11050, train/learning_rate: 6.171664153953316e-06\n",
      "Step: 11050, train/epoch: 2.629700183868408\n",
      "Step: 11060, train/loss: 0.14219999313354492\n",
      "Step: 11060, train/grad_norm: 1.0063037336749403e-07\n",
      "Step: 11060, train/learning_rate: 6.132000635261647e-06\n",
      "Step: 11060, train/epoch: 2.632080078125\n",
      "Step: 11070, train/loss: 0.0\n",
      "Step: 11070, train/grad_norm: 8.532408173778094e-06\n",
      "Step: 11070, train/learning_rate: 6.092337116569979e-06\n",
      "Step: 11070, train/epoch: 2.6344597339630127\n",
      "Step: 11080, train/loss: 0.0\n",
      "Step: 11080, train/grad_norm: 2.078624675050378e-05\n",
      "Step: 11080, train/learning_rate: 6.05267314313096e-06\n",
      "Step: 11080, train/epoch: 2.6368396282196045\n",
      "Step: 11090, train/loss: 0.0\n",
      "Step: 11090, train/grad_norm: 4.958098543283995e-06\n",
      "Step: 11090, train/learning_rate: 6.013009624439292e-06\n",
      "Step: 11090, train/epoch: 2.6392195224761963\n",
      "Step: 11100, train/loss: 0.0\n",
      "Step: 11100, train/grad_norm: 2.0255896743037738e-05\n",
      "Step: 11100, train/learning_rate: 5.973346105747623e-06\n",
      "Step: 11100, train/epoch: 2.641599178314209\n",
      "Step: 11110, train/loss: 0.0\n",
      "Step: 11110, train/grad_norm: 5.441748726298101e-06\n",
      "Step: 11110, train/learning_rate: 5.933682587055955e-06\n",
      "Step: 11110, train/epoch: 2.643979072570801\n",
      "Step: 11120, train/loss: 0.0\n",
      "Step: 11120, train/grad_norm: 4.048884420626564e-06\n",
      "Step: 11120, train/learning_rate: 5.894018613616936e-06\n",
      "Step: 11120, train/epoch: 2.6463589668273926\n",
      "Step: 11130, train/loss: 0.0\n",
      "Step: 11130, train/grad_norm: 5.754458015871933e-06\n",
      "Step: 11130, train/learning_rate: 5.854355094925268e-06\n",
      "Step: 11130, train/epoch: 2.6487386226654053\n",
      "Step: 11140, train/loss: 0.0\n",
      "Step: 11140, train/grad_norm: 0.000309663824737072\n",
      "Step: 11140, train/learning_rate: 5.814691576233599e-06\n",
      "Step: 11140, train/epoch: 2.651118516921997\n",
      "Step: 11150, train/loss: 0.0\n",
      "Step: 11150, train/grad_norm: 7.0329165282601025e-06\n",
      "Step: 11150, train/learning_rate: 5.77502760279458e-06\n",
      "Step: 11150, train/epoch: 2.653498411178589\n",
      "Step: 11160, train/loss: 0.0\n",
      "Step: 11160, train/grad_norm: 2.789144855341874e-05\n",
      "Step: 11160, train/learning_rate: 5.735364084102912e-06\n",
      "Step: 11160, train/epoch: 2.6558780670166016\n",
      "Step: 11170, train/loss: 0.0\n",
      "Step: 11170, train/grad_norm: 1.4520358490699437e-05\n",
      "Step: 11170, train/learning_rate: 5.695700565411244e-06\n",
      "Step: 11170, train/epoch: 2.6582579612731934\n",
      "Step: 11180, train/loss: 0.0\n",
      "Step: 11180, train/grad_norm: 3.740774263860658e-05\n",
      "Step: 11180, train/learning_rate: 5.656036591972224e-06\n",
      "Step: 11180, train/epoch: 2.660637855529785\n",
      "Step: 11190, train/loss: 0.0\n",
      "Step: 11190, train/grad_norm: 4.0462346078129485e-05\n",
      "Step: 11190, train/learning_rate: 5.616373073280556e-06\n",
      "Step: 11190, train/epoch: 2.663017511367798\n",
      "Step: 11200, train/loss: 0.0\n",
      "Step: 11200, train/grad_norm: 5.424078699434176e-05\n",
      "Step: 11200, train/learning_rate: 5.576709554588888e-06\n",
      "Step: 11200, train/epoch: 2.6653974056243896\n",
      "Step: 11210, train/loss: 0.0\n",
      "Step: 11210, train/grad_norm: 7.979185284057166e-06\n",
      "Step: 11210, train/learning_rate: 5.5370460358972196e-06\n",
      "Step: 11210, train/epoch: 2.6677772998809814\n",
      "Step: 11220, train/loss: 0.0\n",
      "Step: 11220, train/grad_norm: 0.0005763497902080417\n",
      "Step: 11220, train/learning_rate: 5.4973820624582e-06\n",
      "Step: 11220, train/epoch: 2.670156955718994\n",
      "Step: 11230, train/loss: 0.0\n",
      "Step: 11230, train/grad_norm: 1.2287247955100611e-05\n",
      "Step: 11230, train/learning_rate: 5.457718543766532e-06\n",
      "Step: 11230, train/epoch: 2.672536849975586\n",
      "Step: 11240, train/loss: 0.0\n",
      "Step: 11240, train/grad_norm: 1.466723460907815e-05\n",
      "Step: 11240, train/learning_rate: 5.418055025074864e-06\n",
      "Step: 11240, train/epoch: 2.6749167442321777\n",
      "Step: 11250, train/loss: 0.0\n",
      "Step: 11250, train/grad_norm: 5.1879228522011545e-06\n",
      "Step: 11250, train/learning_rate: 5.378391051635845e-06\n",
      "Step: 11250, train/epoch: 2.6772966384887695\n",
      "Step: 11260, train/loss: 0.0\n",
      "Step: 11260, train/grad_norm: 1.2299862239828485e-09\n",
      "Step: 11260, train/learning_rate: 5.338727532944176e-06\n",
      "Step: 11260, train/epoch: 2.6796762943267822\n",
      "Step: 11270, train/loss: 0.0\n",
      "Step: 11270, train/grad_norm: 2.050259837460544e-07\n",
      "Step: 11270, train/learning_rate: 5.299064014252508e-06\n",
      "Step: 11270, train/epoch: 2.682056188583374\n",
      "Step: 11280, train/loss: 0.0\n",
      "Step: 11280, train/grad_norm: 6.0486095208034385e-06\n",
      "Step: 11280, train/learning_rate: 5.25940049556084e-06\n",
      "Step: 11280, train/epoch: 2.684436082839966\n",
      "Step: 11290, train/loss: 0.0\n",
      "Step: 11290, train/grad_norm: 0.019680337980389595\n",
      "Step: 11290, train/learning_rate: 5.219736522121821e-06\n",
      "Step: 11290, train/epoch: 2.6868157386779785\n",
      "Step: 11300, train/loss: 0.0\n",
      "Step: 11300, train/grad_norm: 1.05610306491144e-05\n",
      "Step: 11300, train/learning_rate: 5.180073003430152e-06\n",
      "Step: 11300, train/epoch: 2.6891956329345703\n",
      "Step: 11310, train/loss: 0.0\n",
      "Step: 11310, train/grad_norm: 0.0002496035594958812\n",
      "Step: 11310, train/learning_rate: 5.140409484738484e-06\n",
      "Step: 11310, train/epoch: 2.691575527191162\n",
      "Step: 11320, train/loss: 0.0\n",
      "Step: 11320, train/grad_norm: 2.7277539629722014e-05\n",
      "Step: 11320, train/learning_rate: 5.100745511299465e-06\n",
      "Step: 11320, train/epoch: 2.693955183029175\n",
      "Step: 11330, train/loss: 0.0024999999441206455\n",
      "Step: 11330, train/grad_norm: 9.423360097571276e-06\n",
      "Step: 11330, train/learning_rate: 5.0610819926077966e-06\n",
      "Step: 11330, train/epoch: 2.6963350772857666\n",
      "Step: 11340, train/loss: 0.0\n",
      "Step: 11340, train/grad_norm: 3.495916871543159e-07\n",
      "Step: 11340, train/learning_rate: 5.021418473916128e-06\n",
      "Step: 11340, train/epoch: 2.6987149715423584\n",
      "Step: 11350, train/loss: 0.0\n",
      "Step: 11350, train/grad_norm: 1.1912415175174829e-05\n",
      "Step: 11350, train/learning_rate: 4.981754500477109e-06\n",
      "Step: 11350, train/epoch: 2.701094627380371\n",
      "Step: 11360, train/loss: 0.0\n",
      "Step: 11360, train/grad_norm: 1.3260434570838697e-05\n",
      "Step: 11360, train/learning_rate: 4.942090981785441e-06\n",
      "Step: 11360, train/epoch: 2.703474521636963\n",
      "Step: 11370, train/loss: 0.0\n",
      "Step: 11370, train/grad_norm: 5.952681749477051e-05\n",
      "Step: 11370, train/learning_rate: 4.9024274630937725e-06\n",
      "Step: 11370, train/epoch: 2.7058544158935547\n",
      "Step: 11380, train/loss: 0.0\n",
      "Step: 11380, train/grad_norm: 1.0325353287043981e-05\n",
      "Step: 11380, train/learning_rate: 4.862763944402104e-06\n",
      "Step: 11380, train/epoch: 2.7082340717315674\n",
      "Step: 11390, train/loss: 0.0\n",
      "Step: 11390, train/grad_norm: 1.7793097867979668e-05\n",
      "Step: 11390, train/learning_rate: 4.823099970963085e-06\n",
      "Step: 11390, train/epoch: 2.710613965988159\n",
      "Step: 11400, train/loss: 0.0\n",
      "Step: 11400, train/grad_norm: 4.5529472117777914e-05\n",
      "Step: 11400, train/learning_rate: 4.783436452271417e-06\n",
      "Step: 11400, train/epoch: 2.712993860244751\n",
      "Step: 11410, train/loss: 0.0\n",
      "Step: 11410, train/grad_norm: 3.432688026805408e-05\n",
      "Step: 11410, train/learning_rate: 4.7437729335797485e-06\n",
      "Step: 11410, train/epoch: 2.7153735160827637\n",
      "Step: 11420, train/loss: 0.0\n",
      "Step: 11420, train/grad_norm: 2.005459009524202e-06\n",
      "Step: 11420, train/learning_rate: 4.704108960140729e-06\n",
      "Step: 11420, train/epoch: 2.7177534103393555\n",
      "Step: 11430, train/loss: 0.0\n",
      "Step: 11430, train/grad_norm: 7.074901077430695e-06\n",
      "Step: 11430, train/learning_rate: 4.664445441449061e-06\n",
      "Step: 11430, train/epoch: 2.7201333045959473\n",
      "Step: 11440, train/loss: 0.0\n",
      "Step: 11440, train/grad_norm: 4.6597725145147706e-07\n",
      "Step: 11440, train/learning_rate: 4.624781922757393e-06\n",
      "Step: 11440, train/epoch: 2.722513198852539\n",
      "Step: 11450, train/loss: 0.0\n",
      "Step: 11450, train/grad_norm: 5.737975698139053e-06\n",
      "Step: 11450, train/learning_rate: 4.5851184040657245e-06\n",
      "Step: 11450, train/epoch: 2.7248928546905518\n",
      "Step: 11460, train/loss: 0.0\n",
      "Step: 11460, train/grad_norm: 7.363767508650199e-06\n",
      "Step: 11460, train/learning_rate: 4.545454430626705e-06\n",
      "Step: 11460, train/epoch: 2.7272727489471436\n",
      "Step: 11470, train/loss: 0.0\n",
      "Step: 11470, train/grad_norm: 1.0856952940230258e-05\n",
      "Step: 11470, train/learning_rate: 4.505790911935037e-06\n",
      "Step: 11470, train/epoch: 2.7296526432037354\n",
      "Step: 11480, train/loss: 0.0\n",
      "Step: 11480, train/grad_norm: 8.331982098752633e-05\n",
      "Step: 11480, train/learning_rate: 4.466127393243369e-06\n",
      "Step: 11480, train/epoch: 2.732032299041748\n",
      "Step: 11490, train/loss: 0.0\n",
      "Step: 11490, train/grad_norm: 2.9107559385010973e-05\n",
      "Step: 11490, train/learning_rate: 4.4264634198043495e-06\n",
      "Step: 11490, train/epoch: 2.73441219329834\n",
      "Step: 11500, train/loss: 0.0\n",
      "Step: 11500, train/grad_norm: 2.194302942370996e-05\n",
      "Step: 11500, train/learning_rate: 4.386799901112681e-06\n",
      "Step: 11500, train/epoch: 2.7367920875549316\n",
      "Step: 11510, train/loss: 0.0\n",
      "Step: 11510, train/grad_norm: 5.2564253564924e-05\n",
      "Step: 11510, train/learning_rate: 4.347136382421013e-06\n",
      "Step: 11510, train/epoch: 2.7391717433929443\n",
      "Step: 11520, train/loss: 0.0\n",
      "Step: 11520, train/grad_norm: 1.4358490716404049e-06\n",
      "Step: 11520, train/learning_rate: 4.307472408981994e-06\n",
      "Step: 11520, train/epoch: 2.741551637649536\n",
      "Step: 11530, train/loss: 0.0\n",
      "Step: 11530, train/grad_norm: 4.6490353611261526e-07\n",
      "Step: 11530, train/learning_rate: 4.2678088902903255e-06\n",
      "Step: 11530, train/epoch: 2.743931531906128\n",
      "Step: 11540, train/loss: 0.0\n",
      "Step: 11540, train/grad_norm: 7.571475725853816e-05\n",
      "Step: 11540, train/learning_rate: 4.228145371598657e-06\n",
      "Step: 11540, train/epoch: 2.7463111877441406\n",
      "Step: 11550, train/loss: 0.0\n",
      "Step: 11550, train/grad_norm: 0.000140427247970365\n",
      "Step: 11550, train/learning_rate: 4.188481852906989e-06\n",
      "Step: 11550, train/epoch: 2.7486910820007324\n",
      "Step: 11560, train/loss: 0.0\n",
      "Step: 11560, train/grad_norm: 2.2862725018057972e-05\n",
      "Step: 11560, train/learning_rate: 4.14881787946797e-06\n",
      "Step: 11560, train/epoch: 2.751070976257324\n",
      "Step: 11570, train/loss: 0.0\n",
      "Step: 11570, train/grad_norm: 5.573348062171135e-06\n",
      "Step: 11570, train/learning_rate: 4.1091543607763015e-06\n",
      "Step: 11570, train/epoch: 2.753450632095337\n",
      "Step: 11580, train/loss: 0.0\n",
      "Step: 11580, train/grad_norm: 1.7313501302851364e-05\n",
      "Step: 11580, train/learning_rate: 4.069490842084633e-06\n",
      "Step: 11580, train/epoch: 2.7558305263519287\n",
      "Step: 11590, train/loss: 0.0\n",
      "Step: 11590, train/grad_norm: 4.634410743165063e-06\n",
      "Step: 11590, train/learning_rate: 4.029826868645614e-06\n",
      "Step: 11590, train/epoch: 2.7582104206085205\n",
      "Step: 11600, train/loss: 0.0\n",
      "Step: 11600, train/grad_norm: 6.9916577558615245e-06\n",
      "Step: 11600, train/learning_rate: 3.990163349953946e-06\n",
      "Step: 11600, train/epoch: 2.760590076446533\n",
      "Step: 11610, train/loss: 0.0\n",
      "Step: 11610, train/grad_norm: 3.668186764116399e-05\n",
      "Step: 11610, train/learning_rate: 3.9504998312622774e-06\n",
      "Step: 11610, train/epoch: 2.762969970703125\n",
      "Step: 11620, train/loss: 0.0\n",
      "Step: 11620, train/grad_norm: 6.783396202081349e-06\n",
      "Step: 11620, train/learning_rate: 3.910836312570609e-06\n",
      "Step: 11620, train/epoch: 2.765349864959717\n",
      "Step: 11630, train/loss: 0.0\n",
      "Step: 11630, train/grad_norm: 6.252299044717802e-06\n",
      "Step: 11630, train/learning_rate: 3.87117233913159e-06\n",
      "Step: 11630, train/epoch: 2.7677297592163086\n",
      "Step: 11640, train/loss: 0.0\n",
      "Step: 11640, train/grad_norm: 5.847765351063572e-06\n",
      "Step: 11640, train/learning_rate: 3.831508820439922e-06\n",
      "Step: 11640, train/epoch: 2.7701094150543213\n",
      "Step: 11650, train/loss: 0.0\n",
      "Step: 11650, train/grad_norm: 2.449655585223809e-05\n",
      "Step: 11650, train/learning_rate: 3.791845074374578e-06\n",
      "Step: 11650, train/epoch: 2.772489309310913\n",
      "Step: 11660, train/loss: 0.0\n",
      "Step: 11660, train/grad_norm: 2.2776594050810672e-06\n",
      "Step: 11660, train/learning_rate: 3.7521815556829097e-06\n",
      "Step: 11660, train/epoch: 2.774869203567505\n",
      "Step: 11670, train/loss: 0.0\n",
      "Step: 11670, train/grad_norm: 6.009328444633866e-06\n",
      "Step: 11670, train/learning_rate: 3.712517809617566e-06\n",
      "Step: 11670, train/epoch: 2.7772488594055176\n",
      "Step: 11680, train/loss: 0.0\n",
      "Step: 11680, train/grad_norm: 5.775408681074623e-06\n",
      "Step: 11680, train/learning_rate: 3.6728542909258977e-06\n",
      "Step: 11680, train/epoch: 2.7796287536621094\n",
      "Step: 11690, train/loss: 0.0\n",
      "Step: 11690, train/grad_norm: 1.3671272427018266e-05\n",
      "Step: 11690, train/learning_rate: 3.633190544860554e-06\n",
      "Step: 11690, train/epoch: 2.782008647918701\n",
      "Step: 11700, train/loss: 0.0\n",
      "Step: 11700, train/grad_norm: 3.5502635000739247e-05\n",
      "Step: 11700, train/learning_rate: 3.59352679879521e-06\n",
      "Step: 11700, train/epoch: 2.784388303756714\n",
      "Step: 11710, train/loss: 0.0\n",
      "Step: 11710, train/grad_norm: 2.0856584797002142e-06\n",
      "Step: 11710, train/learning_rate: 3.553863280103542e-06\n",
      "Step: 11710, train/epoch: 2.7867681980133057\n",
      "Step: 11720, train/loss: 0.0\n",
      "Step: 11720, train/grad_norm: 4.0231479943031445e-05\n",
      "Step: 11720, train/learning_rate: 3.514199534038198e-06\n",
      "Step: 11720, train/epoch: 2.7891480922698975\n",
      "Step: 11730, train/loss: 0.0\n",
      "Step: 11730, train/grad_norm: 2.2020909455022775e-05\n",
      "Step: 11730, train/learning_rate: 3.47453601534653e-06\n",
      "Step: 11730, train/epoch: 2.79152774810791\n",
      "Step: 11740, train/loss: 0.0\n",
      "Step: 11740, train/grad_norm: 2.519265308364993e-06\n",
      "Step: 11740, train/learning_rate: 3.434872269281186e-06\n",
      "Step: 11740, train/epoch: 2.793907642364502\n",
      "Step: 11750, train/loss: 0.0\n",
      "Step: 11750, train/grad_norm: 1.2104573215765413e-05\n",
      "Step: 11750, train/learning_rate: 3.3952085232158424e-06\n",
      "Step: 11750, train/epoch: 2.7962875366210938\n",
      "Step: 11760, train/loss: 0.0\n",
      "Step: 11760, train/grad_norm: 1.0279501111654099e-05\n",
      "Step: 11760, train/learning_rate: 3.355545004524174e-06\n",
      "Step: 11760, train/epoch: 2.7986671924591064\n",
      "Step: 11770, train/loss: 0.00019999999494757503\n",
      "Step: 11770, train/grad_norm: 6.664446118520573e-06\n",
      "Step: 11770, train/learning_rate: 3.3158812584588304e-06\n",
      "Step: 11770, train/epoch: 2.8010470867156982\n",
      "Step: 11780, train/loss: 0.0\n",
      "Step: 11780, train/grad_norm: 1.1257784535700921e-05\n",
      "Step: 11780, train/learning_rate: 3.276217739767162e-06\n",
      "Step: 11780, train/epoch: 2.80342698097229\n",
      "Step: 11790, train/loss: 0.0\n",
      "Step: 11790, train/grad_norm: 0.00013521405344363302\n",
      "Step: 11790, train/learning_rate: 3.2365539937018184e-06\n",
      "Step: 11790, train/epoch: 2.805806875228882\n",
      "Step: 11800, train/loss: 0.0\n",
      "Step: 11800, train/grad_norm: 2.526603566366248e-06\n",
      "Step: 11800, train/learning_rate: 3.19689047501015e-06\n",
      "Step: 11800, train/epoch: 2.8081865310668945\n",
      "Step: 11810, train/loss: 0.0\n",
      "Step: 11810, train/grad_norm: 8.464950951747596e-06\n",
      "Step: 11810, train/learning_rate: 3.1572267289448064e-06\n",
      "Step: 11810, train/epoch: 2.8105664253234863\n",
      "Step: 11820, train/loss: 0.0\n",
      "Step: 11820, train/grad_norm: 2.043036147370003e-05\n",
      "Step: 11820, train/learning_rate: 3.1175629828794627e-06\n",
      "Step: 11820, train/epoch: 2.812946319580078\n",
      "Step: 11830, train/loss: 0.0\n",
      "Step: 11830, train/grad_norm: 2.5448052838328294e-06\n",
      "Step: 11830, train/learning_rate: 3.0778994641877944e-06\n",
      "Step: 11830, train/epoch: 2.815325975418091\n",
      "Step: 11840, train/loss: 0.0\n",
      "Step: 11840, train/grad_norm: 4.8212266847258434e-05\n",
      "Step: 11840, train/learning_rate: 3.0382357181224506e-06\n",
      "Step: 11840, train/epoch: 2.8177058696746826\n",
      "Step: 11850, train/loss: 0.0\n",
      "Step: 11850, train/grad_norm: 8.35083847050555e-06\n",
      "Step: 11850, train/learning_rate: 2.9985721994307823e-06\n",
      "Step: 11850, train/epoch: 2.8200857639312744\n",
      "Step: 11860, train/loss: 0.0\n",
      "Step: 11860, train/grad_norm: 1.6200938262045383e-06\n",
      "Step: 11860, train/learning_rate: 2.9589084533654386e-06\n",
      "Step: 11860, train/epoch: 2.822465419769287\n",
      "Step: 11870, train/loss: 0.0\n",
      "Step: 11870, train/grad_norm: 0.0001658774126553908\n",
      "Step: 11870, train/learning_rate: 2.919244707300095e-06\n",
      "Step: 11870, train/epoch: 2.824845314025879\n",
      "Step: 11880, train/loss: 0.0\n",
      "Step: 11880, train/grad_norm: 2.788500023598317e-05\n",
      "Step: 11880, train/learning_rate: 2.8795811886084266e-06\n",
      "Step: 11880, train/epoch: 2.8272252082824707\n",
      "Step: 11890, train/loss: 0.0\n",
      "Step: 11890, train/grad_norm: 2.192450665461365e-06\n",
      "Step: 11890, train/learning_rate: 2.839917442543083e-06\n",
      "Step: 11890, train/epoch: 2.8296048641204834\n",
      "Step: 11900, train/loss: 0.0\n",
      "Step: 11900, train/grad_norm: 5.988907105347607e-06\n",
      "Step: 11900, train/learning_rate: 2.8002539238514146e-06\n",
      "Step: 11900, train/epoch: 2.831984758377075\n",
      "Step: 11910, train/loss: 0.0\n",
      "Step: 11910, train/grad_norm: 0.00022469212126452476\n",
      "Step: 11910, train/learning_rate: 2.760590177786071e-06\n",
      "Step: 11910, train/epoch: 2.834364652633667\n",
      "Step: 11920, train/loss: 0.0\n",
      "Step: 11920, train/grad_norm: 1.0508374543860555e-05\n",
      "Step: 11920, train/learning_rate: 2.720926431720727e-06\n",
      "Step: 11920, train/epoch: 2.8367443084716797\n",
      "Step: 11930, train/loss: 0.0\n",
      "Step: 11930, train/grad_norm: 4.289876414986793e-06\n",
      "Step: 11930, train/learning_rate: 2.681262913029059e-06\n",
      "Step: 11930, train/epoch: 2.8391242027282715\n",
      "Step: 11940, train/loss: 0.0\n",
      "Step: 11940, train/grad_norm: 1.9975193481513998e-06\n",
      "Step: 11940, train/learning_rate: 2.641599166963715e-06\n",
      "Step: 11940, train/epoch: 2.8415040969848633\n",
      "Step: 11950, train/loss: 0.0\n",
      "Step: 11950, train/grad_norm: 1.3485361023413134e-06\n",
      "Step: 11950, train/learning_rate: 2.601935648272047e-06\n",
      "Step: 11950, train/epoch: 2.843883752822876\n",
      "Step: 11960, train/loss: 0.0\n",
      "Step: 11960, train/grad_norm: 2.969815795950126e-05\n",
      "Step: 11960, train/learning_rate: 2.562271902206703e-06\n",
      "Step: 11960, train/epoch: 2.8462636470794678\n",
      "Step: 11970, train/loss: 0.0\n",
      "Step: 11970, train/grad_norm: 7.572700269520283e-06\n",
      "Step: 11970, train/learning_rate: 2.522608383515035e-06\n",
      "Step: 11970, train/epoch: 2.8486435413360596\n",
      "Step: 11980, train/loss: 0.0\n",
      "Step: 11980, train/grad_norm: 3.636911287685507e-06\n",
      "Step: 11980, train/learning_rate: 2.482944637449691e-06\n",
      "Step: 11980, train/epoch: 2.8510234355926514\n",
      "Step: 11990, train/loss: 0.0\n",
      "Step: 11990, train/grad_norm: 6.955315711820731e-06\n",
      "Step: 11990, train/learning_rate: 2.4432808913843473e-06\n",
      "Step: 11990, train/epoch: 2.853403091430664\n",
      "Step: 12000, train/loss: 0.0\n",
      "Step: 12000, train/grad_norm: 7.694437044847291e-06\n",
      "Step: 12000, train/learning_rate: 2.403617372692679e-06\n",
      "Step: 12000, train/epoch: 2.855782985687256\n",
      "Step: 12010, train/loss: 0.0\n",
      "Step: 12010, train/grad_norm: 4.640229599317536e-05\n",
      "Step: 12010, train/learning_rate: 2.3639536266273353e-06\n",
      "Step: 12010, train/epoch: 2.8581628799438477\n",
      "Step: 12020, train/loss: 0.0\n",
      "Step: 12020, train/grad_norm: 2.029075585596729e-05\n",
      "Step: 12020, train/learning_rate: 2.324290107935667e-06\n",
      "Step: 12020, train/epoch: 2.8605425357818604\n",
      "Step: 12030, train/loss: 0.0\n",
      "Step: 12030, train/grad_norm: 7.646326594112907e-06\n",
      "Step: 12030, train/learning_rate: 2.2846263618703233e-06\n",
      "Step: 12030, train/epoch: 2.862922430038452\n",
      "Step: 12040, train/loss: 0.0\n",
      "Step: 12040, train/grad_norm: 1.4474882846116088e-05\n",
      "Step: 12040, train/learning_rate: 2.2449626158049796e-06\n",
      "Step: 12040, train/epoch: 2.865302324295044\n",
      "Step: 12050, train/loss: 0.0\n",
      "Step: 12050, train/grad_norm: 3.1518917239736766e-05\n",
      "Step: 12050, train/learning_rate: 2.2052990971133113e-06\n",
      "Step: 12050, train/epoch: 2.8676819801330566\n",
      "Step: 12060, train/loss: 0.0\n",
      "Step: 12060, train/grad_norm: 8.002469257917255e-06\n",
      "Step: 12060, train/learning_rate: 2.1656353510479676e-06\n",
      "Step: 12060, train/epoch: 2.8700618743896484\n",
      "Step: 12070, train/loss: 0.0\n",
      "Step: 12070, train/grad_norm: 3.68759865523316e-05\n",
      "Step: 12070, train/learning_rate: 2.1259718323562993e-06\n",
      "Step: 12070, train/epoch: 2.8724417686462402\n",
      "Step: 12080, train/loss: 0.0\n",
      "Step: 12080, train/grad_norm: 1.0344900147174485e-05\n",
      "Step: 12080, train/learning_rate: 2.0863080862909555e-06\n",
      "Step: 12080, train/epoch: 2.874821424484253\n",
      "Step: 12090, train/loss: 0.0\n",
      "Step: 12090, train/grad_norm: 2.811024842230836e-06\n",
      "Step: 12090, train/learning_rate: 2.0466445675992873e-06\n",
      "Step: 12090, train/epoch: 2.8772013187408447\n",
      "Step: 12100, train/loss: 0.0\n",
      "Step: 12100, train/grad_norm: 4.1409282857785e-05\n",
      "Step: 12100, train/learning_rate: 2.0069808215339435e-06\n",
      "Step: 12100, train/epoch: 2.8795812129974365\n",
      "Step: 12110, train/loss: 0.0\n",
      "Step: 12110, train/grad_norm: 9.529364433547016e-06\n",
      "Step: 12110, train/learning_rate: 1.9673170754686e-06\n",
      "Step: 12110, train/epoch: 2.881960868835449\n",
      "Step: 12120, train/loss: 0.0\n",
      "Step: 12120, train/grad_norm: 2.4300972654600628e-05\n",
      "Step: 12120, train/learning_rate: 1.9276535567769315e-06\n",
      "Step: 12120, train/epoch: 2.884340763092041\n",
      "Step: 12130, train/loss: 0.0\n",
      "Step: 12130, train/grad_norm: 7.609779459016863e-06\n",
      "Step: 12130, train/learning_rate: 1.8879898107115878e-06\n",
      "Step: 12130, train/epoch: 2.886720657348633\n",
      "Step: 12140, train/loss: 0.0\n",
      "Step: 12140, train/grad_norm: 4.427345629665069e-06\n",
      "Step: 12140, train/learning_rate: 1.8483261783330818e-06\n",
      "Step: 12140, train/epoch: 2.8891003131866455\n",
      "Step: 12150, train/loss: 0.0\n",
      "Step: 12150, train/grad_norm: 5.468750259751687e-06\n",
      "Step: 12150, train/learning_rate: 1.8086625459545758e-06\n",
      "Step: 12150, train/epoch: 2.8914802074432373\n",
      "Step: 12160, train/loss: 0.0\n",
      "Step: 12160, train/grad_norm: 5.913585482630879e-06\n",
      "Step: 12160, train/learning_rate: 1.7689989135760698e-06\n",
      "Step: 12160, train/epoch: 2.893860101699829\n",
      "Step: 12170, train/loss: 0.0\n",
      "Step: 12170, train/grad_norm: 3.0083863293839386e-06\n",
      "Step: 12170, train/learning_rate: 1.7293352811975637e-06\n",
      "Step: 12170, train/epoch: 2.896239995956421\n",
      "Step: 12180, train/loss: 0.0\n",
      "Step: 12180, train/grad_norm: 7.996889507921878e-06\n",
      "Step: 12180, train/learning_rate: 1.68967153513222e-06\n",
      "Step: 12180, train/epoch: 2.8986196517944336\n",
      "Step: 12190, train/loss: 0.0\n",
      "Step: 12190, train/grad_norm: 1.664055889705196e-05\n",
      "Step: 12190, train/learning_rate: 1.650007902753714e-06\n",
      "Step: 12190, train/epoch: 2.9009995460510254\n",
      "Step: 12200, train/loss: 0.0\n",
      "Step: 12200, train/grad_norm: 4.594614438246936e-06\n",
      "Step: 12200, train/learning_rate: 1.610344270375208e-06\n",
      "Step: 12200, train/epoch: 2.903379440307617\n",
      "Step: 12210, train/loss: 0.0\n",
      "Step: 12210, train/grad_norm: 4.7643402467656415e-06\n",
      "Step: 12210, train/learning_rate: 1.570680637996702e-06\n",
      "Step: 12210, train/epoch: 2.90575909614563\n",
      "Step: 12220, train/loss: 0.0\n",
      "Step: 12220, train/grad_norm: 5.5576267186552286e-05\n",
      "Step: 12220, train/learning_rate: 1.531017005618196e-06\n",
      "Step: 12220, train/epoch: 2.9081389904022217\n",
      "Step: 12230, train/loss: 0.0\n",
      "Step: 12230, train/grad_norm: 1.3399421732174233e-05\n",
      "Step: 12230, train/learning_rate: 1.49135337323969e-06\n",
      "Step: 12230, train/epoch: 2.9105188846588135\n",
      "Step: 12240, train/loss: 0.0\n",
      "Step: 12240, train/grad_norm: 4.646580418921076e-05\n",
      "Step: 12240, train/learning_rate: 1.4516896271743462e-06\n",
      "Step: 12240, train/epoch: 2.912898540496826\n",
      "Step: 12250, train/loss: 0.0\n",
      "Step: 12250, train/grad_norm: 4.603638808475807e-05\n",
      "Step: 12250, train/learning_rate: 1.4120259947958402e-06\n",
      "Step: 12250, train/epoch: 2.915278434753418\n",
      "Step: 12260, train/loss: 0.0\n",
      "Step: 12260, train/grad_norm: 2.8955982998013496e-05\n",
      "Step: 12260, train/learning_rate: 1.3723623624173342e-06\n",
      "Step: 12260, train/epoch: 2.9176583290100098\n",
      "Step: 12270, train/loss: 0.0\n",
      "Step: 12270, train/grad_norm: 6.086404027882963e-06\n",
      "Step: 12270, train/learning_rate: 1.3326987300388282e-06\n",
      "Step: 12270, train/epoch: 2.9200379848480225\n",
      "Step: 12280, train/loss: 9.999999747378752e-05\n",
      "Step: 12280, train/grad_norm: 0.00022750663629267365\n",
      "Step: 12280, train/learning_rate: 1.2930350976603222e-06\n",
      "Step: 12280, train/epoch: 2.9224178791046143\n",
      "Step: 12290, train/loss: 0.0\n",
      "Step: 12290, train/grad_norm: 0.0002189926744904369\n",
      "Step: 12290, train/learning_rate: 1.2533714652818162e-06\n",
      "Step: 12290, train/epoch: 2.924797773361206\n",
      "Step: 12300, train/loss: 0.0\n",
      "Step: 12300, train/grad_norm: 8.567282202420756e-05\n",
      "Step: 12300, train/learning_rate: 1.2137077192164725e-06\n",
      "Step: 12300, train/epoch: 2.9271774291992188\n",
      "Step: 12310, train/loss: 0.0\n",
      "Step: 12310, train/grad_norm: 1.7849077266873792e-05\n",
      "Step: 12310, train/learning_rate: 1.1740440868379665e-06\n",
      "Step: 12310, train/epoch: 2.9295573234558105\n",
      "Step: 12320, train/loss: 0.0\n",
      "Step: 12320, train/grad_norm: 1.6893210386115243e-06\n",
      "Step: 12320, train/learning_rate: 1.1343804544594605e-06\n",
      "Step: 12320, train/epoch: 2.9319372177124023\n",
      "Step: 12330, train/loss: 0.0\n",
      "Step: 12330, train/grad_norm: 4.202366653771605e-06\n",
      "Step: 12330, train/learning_rate: 1.0947168220809544e-06\n",
      "Step: 12330, train/epoch: 2.934316873550415\n",
      "Step: 12340, train/loss: 0.0\n",
      "Step: 12340, train/grad_norm: 2.2835085474071093e-05\n",
      "Step: 12340, train/learning_rate: 1.0550531897024484e-06\n",
      "Step: 12340, train/epoch: 2.936696767807007\n",
      "Step: 12350, train/loss: 0.0\n",
      "Step: 12350, train/grad_norm: 2.444789970468264e-05\n",
      "Step: 12350, train/learning_rate: 1.0153894436371047e-06\n",
      "Step: 12350, train/epoch: 2.9390766620635986\n",
      "Step: 12360, train/loss: 0.0\n",
      "Step: 12360, train/grad_norm: 7.985082447703462e-06\n",
      "Step: 12360, train/learning_rate: 9.757258112585987e-07\n",
      "Step: 12360, train/epoch: 2.9414565563201904\n",
      "Step: 12370, train/loss: 0.0\n",
      "Step: 12370, train/grad_norm: 1.4377325896930415e-05\n",
      "Step: 12370, train/learning_rate: 9.360621788800927e-07\n",
      "Step: 12370, train/epoch: 2.943836212158203\n",
      "Step: 12380, train/loss: 0.0\n",
      "Step: 12380, train/grad_norm: 1.4599798305425793e-05\n",
      "Step: 12380, train/learning_rate: 8.963985465015867e-07\n",
      "Step: 12380, train/epoch: 2.946216106414795\n",
      "Step: 12390, train/loss: 0.0\n",
      "Step: 12390, train/grad_norm: 2.534210807425552e-06\n",
      "Step: 12390, train/learning_rate: 8.567349141230807e-07\n",
      "Step: 12390, train/epoch: 2.9485960006713867\n",
      "Step: 12400, train/loss: 0.0\n",
      "Step: 12400, train/grad_norm: 2.8107389880460687e-05\n",
      "Step: 12400, train/learning_rate: 8.170712249011558e-07\n",
      "Step: 12400, train/epoch: 2.9509756565093994\n",
      "Step: 12410, train/loss: 0.0\n",
      "Step: 12410, train/grad_norm: 6.171005679789232e-07\n",
      "Step: 12410, train/learning_rate: 7.774075925226498e-07\n",
      "Step: 12410, train/epoch: 2.953355550765991\n",
      "Step: 12420, train/loss: 0.0\n",
      "Step: 12420, train/grad_norm: 3.767126690945588e-05\n",
      "Step: 12420, train/learning_rate: 7.377439033007249e-07\n",
      "Step: 12420, train/epoch: 2.955735445022583\n",
      "Step: 12430, train/loss: 0.0\n",
      "Step: 12430, train/grad_norm: 2.913128810178023e-06\n",
      "Step: 12430, train/learning_rate: 6.980802709222189e-07\n",
      "Step: 12430, train/epoch: 2.9581151008605957\n",
      "Step: 12440, train/loss: 0.0\n",
      "Step: 12440, train/grad_norm: 6.397834567906102e-06\n",
      "Step: 12440, train/learning_rate: 6.584166385437129e-07\n",
      "Step: 12440, train/epoch: 2.9604949951171875\n",
      "Step: 12450, train/loss: 0.0\n",
      "Step: 12450, train/grad_norm: 9.012282475850952e-08\n",
      "Step: 12450, train/learning_rate: 6.18752949321788e-07\n",
      "Step: 12450, train/epoch: 2.9628748893737793\n",
      "Step: 12460, train/loss: 0.0\n",
      "Step: 12460, train/grad_norm: 0.000511667865794152\n",
      "Step: 12460, train/learning_rate: 5.79089316943282e-07\n",
      "Step: 12460, train/epoch: 2.965254545211792\n",
      "Step: 12470, train/loss: 0.0\n",
      "Step: 12470, train/grad_norm: 4.8844030970940366e-05\n",
      "Step: 12470, train/learning_rate: 5.39425684564776e-07\n",
      "Step: 12470, train/epoch: 2.967634439468384\n",
      "Step: 12480, train/loss: 0.0\n",
      "Step: 12480, train/grad_norm: 6.4258583734044805e-06\n",
      "Step: 12480, train/learning_rate: 4.997619953428512e-07\n",
      "Step: 12480, train/epoch: 2.9700143337249756\n",
      "Step: 12490, train/loss: 0.0\n",
      "Step: 12490, train/grad_norm: 2.414240952930413e-06\n",
      "Step: 12490, train/learning_rate: 4.6009836296434514e-07\n",
      "Step: 12490, train/epoch: 2.9723939895629883\n",
      "Step: 12500, train/loss: 0.0\n",
      "Step: 12500, train/grad_norm: 1.0035414561571088e-05\n",
      "Step: 12500, train/learning_rate: 4.204347021641297e-07\n",
      "Step: 12500, train/epoch: 2.97477388381958\n",
      "Step: 12510, train/loss: 0.0\n",
      "Step: 12510, train/grad_norm: 2.8703960197162814e-05\n",
      "Step: 12510, train/learning_rate: 3.807710697856237e-07\n",
      "Step: 12510, train/epoch: 2.977153778076172\n",
      "Step: 12520, train/loss: 0.0\n",
      "Step: 12520, train/grad_norm: 5.1783540584438015e-06\n",
      "Step: 12520, train/learning_rate: 3.4110740898540826e-07\n",
      "Step: 12520, train/epoch: 2.9795336723327637\n",
      "Step: 12530, train/loss: 0.0\n",
      "Step: 12530, train/grad_norm: 1.1337991054460872e-05\n",
      "Step: 12530, train/learning_rate: 3.014437481851928e-07\n",
      "Step: 12530, train/epoch: 2.9819133281707764\n",
      "Step: 12540, train/loss: 0.0\n",
      "Step: 12540, train/grad_norm: 2.1162682060094085e-06\n",
      "Step: 12540, train/learning_rate: 2.617801158066868e-07\n",
      "Step: 12540, train/epoch: 2.984293222427368\n",
      "Step: 12550, train/loss: 0.0\n",
      "Step: 12550, train/grad_norm: 4.822103619517293e-06\n",
      "Step: 12550, train/learning_rate: 2.2211645500647137e-07\n",
      "Step: 12550, train/epoch: 2.98667311668396\n",
      "Step: 12560, train/loss: 0.0\n",
      "Step: 12560, train/grad_norm: 5.718436113966163e-06\n",
      "Step: 12560, train/learning_rate: 1.8245279420625593e-07\n",
      "Step: 12560, train/epoch: 2.9890527725219727\n",
      "Step: 12570, train/loss: 9.999999747378752e-05\n",
      "Step: 12570, train/grad_norm: 6.002942427585367e-06\n",
      "Step: 12570, train/learning_rate: 1.427891476168952e-07\n",
      "Step: 12570, train/epoch: 2.9914326667785645\n",
      "Step: 12580, train/loss: 0.0\n",
      "Step: 12580, train/grad_norm: 1.964934381248895e-05\n",
      "Step: 12580, train/learning_rate: 1.0312549392210713e-07\n",
      "Step: 12580, train/epoch: 2.9938125610351562\n",
      "Step: 12590, train/loss: 0.0\n",
      "Step: 12590, train/grad_norm: 3.136837676720461e-06\n",
      "Step: 12590, train/learning_rate: 6.346184022731904e-08\n",
      "Step: 12590, train/epoch: 2.996192216873169\n",
      "Step: 12600, train/loss: 0.0\n",
      "Step: 12600, train/grad_norm: 2.2490112314699218e-05\n",
      "Step: 12600, train/learning_rate: 2.379819186160148e-08\n",
      "Step: 12600, train/epoch: 2.9985721111297607\n",
      "Step: 12606, eval/loss: 0.00216034147888422\n",
      "Step: 12606, eval/accuracy: 0.9994446635246277\n",
      "Step: 12606, eval/f1: 0.9994135499000549\n",
      "Step: 12606, eval/runtime: 7928.53564453125\n",
      "Step: 12606, eval/samples_per_second: 0.9079999923706055\n",
      "Step: 12606, eval/steps_per_second: 0.11400000005960464\n",
      "Step: 12606, train/epoch: 3.0\n",
      "Step: 12606, train/train_runtime: 125524.0859375\n",
      "Step: 12606, train/train_samples_per_second: 0.8029999732971191\n",
      "Step: 12606, train/train_steps_per_second: 0.10000000149011612\n",
      "Step: 12606, train/total_flos: 2.1235818045284286e+19\n",
      "Step: 12606, train/train_loss: 0.00042218188173137605\n",
      "Step: 12606, train/epoch: 3.0\n",
      "Reading events from file: ./praxis-Llama-2-70b-hf-small-finetune/logs/events.out.tfevents.1717278811.hephaestus.14749.0\n",
      "Step: 4210, train/loss: 0.0\n",
      "Step: 4210, train/grad_norm: 3.979794627184674e-08\n",
      "Step: 4210, train/learning_rate: 3.3301603252766654e-05\n",
      "Step: 4210, train/epoch: 1.0019038915634155\n",
      "Step: 4220, train/loss: 0.0\n",
      "Step: 4220, train/grad_norm: 7.902798415670986e-07\n",
      "Step: 4220, train/learning_rate: 3.326193836983293e-05\n",
      "Step: 4220, train/epoch: 1.0042836666107178\n",
      "Step: 4230, train/loss: 0.0\n",
      "Step: 4230, train/grad_norm: 5.587580017163418e-05\n",
      "Step: 4230, train/learning_rate: 3.322227348689921e-05\n",
      "Step: 4230, train/epoch: 1.00666344165802\n",
      "Step: 4240, train/loss: 0.0\n",
      "Step: 4240, train/grad_norm: 0.00026259798323735595\n",
      "Step: 4240, train/learning_rate: 3.31826122419443e-05\n",
      "Step: 4240, train/epoch: 1.0090433359146118\n",
      "Step: 4250, train/loss: 0.0\n",
      "Step: 4250, train/grad_norm: 1.1266048716152e-07\n",
      "Step: 4250, train/learning_rate: 3.314294735901058e-05\n",
      "Step: 4250, train/epoch: 1.011423110961914\n",
      "Step: 4260, train/loss: 0.0\n",
      "Step: 4260, train/grad_norm: 0.00010051504068542272\n",
      "Step: 4260, train/learning_rate: 3.3103282476076856e-05\n",
      "Step: 4260, train/epoch: 1.0138030052185059\n",
      "Step: 4270, train/loss: 0.0\n",
      "Step: 4270, train/grad_norm: 9.841569408308715e-07\n",
      "Step: 4270, train/learning_rate: 3.306362123112194e-05\n",
      "Step: 4270, train/epoch: 1.016182780265808\n",
      "Step: 4280, train/loss: 0.0\n",
      "Step: 4280, train/grad_norm: 9.650886568124406e-06\n",
      "Step: 4280, train/learning_rate: 3.302395634818822e-05\n",
      "Step: 4280, train/epoch: 1.0185625553131104\n",
      "Step: 4290, train/loss: 0.0\n",
      "Step: 4290, train/grad_norm: 1.015284738059563e-06\n",
      "Step: 4290, train/learning_rate: 3.29842914652545e-05\n",
      "Step: 4290, train/epoch: 1.0209424495697021\n",
      "Step: 4300, train/loss: 0.0\n",
      "Step: 4300, train/grad_norm: 2.508602747752775e-08\n",
      "Step: 4300, train/learning_rate: 3.294463022029959e-05\n",
      "Step: 4300, train/epoch: 1.0233222246170044\n",
      "Step: 4310, train/loss: 0.0\n",
      "Step: 4310, train/grad_norm: 3.669506156711577e-07\n",
      "Step: 4310, train/learning_rate: 3.2904965337365866e-05\n",
      "Step: 4310, train/epoch: 1.0257019996643066\n",
      "Step: 4320, train/loss: 0.0\n",
      "Step: 4320, train/grad_norm: 4.3436347141323495e-07\n",
      "Step: 4320, train/learning_rate: 3.2865300454432145e-05\n",
      "Step: 4320, train/epoch: 1.0280818939208984\n",
      "Step: 4330, train/loss: 0.0\n",
      "Step: 4330, train/grad_norm: 2.2569810198547202e-07\n",
      "Step: 4330, train/learning_rate: 3.282563920947723e-05\n",
      "Step: 4330, train/epoch: 1.0304616689682007\n",
      "Step: 4340, train/loss: 0.0\n",
      "Step: 4340, train/grad_norm: 0.00015168772370088845\n",
      "Step: 4340, train/learning_rate: 3.278597432654351e-05\n",
      "Step: 4340, train/epoch: 1.0328415632247925\n",
      "Step: 4350, train/loss: 0.0\n",
      "Step: 4350, train/grad_norm: 2.5306408701908367e-07\n",
      "Step: 4350, train/learning_rate: 3.2746313081588596e-05\n",
      "Step: 4350, train/epoch: 1.0352213382720947\n",
      "Step: 4360, train/loss: 0.0\n",
      "Step: 4360, train/grad_norm: 1.9352556535068288e-07\n",
      "Step: 4360, train/learning_rate: 3.2706648198654875e-05\n",
      "Step: 4360, train/epoch: 1.037601113319397\n",
      "Step: 4370, train/loss: 0.0\n",
      "Step: 4370, train/grad_norm: 0.0010507003171369433\n",
      "Step: 4370, train/learning_rate: 3.2666983315721154e-05\n",
      "Step: 4370, train/epoch: 1.0399810075759888\n",
      "Step: 4380, train/loss: 0.0\n",
      "Step: 4380, train/grad_norm: 1.220677745550347e-07\n",
      "Step: 4380, train/learning_rate: 3.262732207076624e-05\n",
      "Step: 4380, train/epoch: 1.042360782623291\n",
      "Step: 4390, train/loss: 0.0\n",
      "Step: 4390, train/grad_norm: 2.5111564383450968e-08\n",
      "Step: 4390, train/learning_rate: 3.258765718783252e-05\n",
      "Step: 4390, train/epoch: 1.0447405576705933\n",
      "Step: 4400, train/loss: 0.0\n",
      "Step: 4400, train/grad_norm: 0.00023366633104160428\n",
      "Step: 4400, train/learning_rate: 3.25479923048988e-05\n",
      "Step: 4400, train/epoch: 1.047120451927185\n",
      "Step: 4410, train/loss: 0.0\n",
      "Step: 4410, train/grad_norm: 5.044088879913033e-07\n",
      "Step: 4410, train/learning_rate: 3.2508331059943885e-05\n",
      "Step: 4410, train/epoch: 1.0495002269744873\n",
      "Step: 4420, train/loss: 0.0\n",
      "Step: 4420, train/grad_norm: 2.623909267640556e-07\n",
      "Step: 4420, train/learning_rate: 3.2468666177010164e-05\n",
      "Step: 4420, train/epoch: 1.0518800020217896\n",
      "Step: 4430, train/loss: 0.0\n",
      "Step: 4430, train/grad_norm: 7.857040600356413e-07\n",
      "Step: 4430, train/learning_rate: 3.242900129407644e-05\n",
      "Step: 4430, train/epoch: 1.0542598962783813\n",
      "Step: 4440, train/loss: 0.0\n",
      "Step: 4440, train/grad_norm: 1.3375860419273522e-07\n",
      "Step: 4440, train/learning_rate: 3.238934004912153e-05\n",
      "Step: 4440, train/epoch: 1.0566396713256836\n",
      "Step: 4450, train/loss: 0.05389999970793724\n",
      "Step: 4450, train/grad_norm: 6.831572818555287e-08\n",
      "Step: 4450, train/learning_rate: 3.234967516618781e-05\n",
      "Step: 4450, train/epoch: 1.0590195655822754\n",
      "Step: 4460, train/loss: 0.0\n",
      "Step: 4460, train/grad_norm: 3.1658114352239863e-08\n",
      "Step: 4460, train/learning_rate: 3.231001028325409e-05\n",
      "Step: 4460, train/epoch: 1.0613993406295776\n",
      "Step: 4470, train/loss: 0.0\n",
      "Step: 4470, train/grad_norm: 2.593258432170842e-05\n",
      "Step: 4470, train/learning_rate: 3.227034903829917e-05\n",
      "Step: 4470, train/epoch: 1.0637791156768799\n",
      "Step: 4480, train/loss: 0.0\n",
      "Step: 4480, train/grad_norm: 8.418613788307994e-07\n",
      "Step: 4480, train/learning_rate: 3.223068415536545e-05\n",
      "Step: 4480, train/epoch: 1.0661590099334717\n",
      "Step: 4490, train/loss: 0.0\n",
      "Step: 4490, train/grad_norm: 9.152439019999292e-07\n",
      "Step: 4490, train/learning_rate: 3.219101927243173e-05\n",
      "Step: 4490, train/epoch: 1.068538784980774\n",
      "Step: 4500, train/loss: 0.0\n",
      "Step: 4500, train/grad_norm: 3.072801746384357e-08\n",
      "Step: 4500, train/learning_rate: 3.215135802747682e-05\n",
      "Step: 4500, train/epoch: 1.0709185600280762\n",
      "Step: 4510, train/loss: 0.0\n",
      "Step: 4510, train/grad_norm: 2.294172918482218e-05\n",
      "Step: 4510, train/learning_rate: 3.2111693144543096e-05\n",
      "Step: 4510, train/epoch: 1.073298454284668\n",
      "Step: 4520, train/loss: 0.0\n",
      "Step: 4520, train/grad_norm: 1.4584414032015047e-07\n",
      "Step: 4520, train/learning_rate: 3.2072028261609375e-05\n",
      "Step: 4520, train/epoch: 1.0756782293319702\n",
      "Step: 4530, train/loss: 0.0\n",
      "Step: 4530, train/grad_norm: 4.766886885221311e-09\n",
      "Step: 4530, train/learning_rate: 3.203236701665446e-05\n",
      "Step: 4530, train/epoch: 1.078058123588562\n",
      "Step: 4540, train/loss: 0.0\n",
      "Step: 4540, train/grad_norm: 2.1933093563575312e-08\n",
      "Step: 4540, train/learning_rate: 3.199270213372074e-05\n",
      "Step: 4540, train/epoch: 1.0804378986358643\n",
      "Step: 4550, train/loss: 0.0\n",
      "Step: 4550, train/grad_norm: 1.4027916961367737e-07\n",
      "Step: 4550, train/learning_rate: 3.195303725078702e-05\n",
      "Step: 4550, train/epoch: 1.0828176736831665\n",
      "Step: 4560, train/loss: 0.014800000004470348\n",
      "Step: 4560, train/grad_norm: 0.0018940444570034742\n",
      "Step: 4560, train/learning_rate: 3.1913376005832106e-05\n",
      "Step: 4560, train/epoch: 1.0851975679397583\n",
      "Step: 4570, train/loss: 0.0\n",
      "Step: 4570, train/grad_norm: 2.6775754236041394e-07\n",
      "Step: 4570, train/learning_rate: 3.1873711122898385e-05\n",
      "Step: 4570, train/epoch: 1.0875773429870605\n",
      "Step: 4580, train/loss: 0.00039999998989515007\n",
      "Step: 4580, train/grad_norm: 1.4624597497459035e-05\n",
      "Step: 4580, train/learning_rate: 3.1834046239964664e-05\n",
      "Step: 4580, train/epoch: 1.0899571180343628\n",
      "Step: 4590, train/loss: 0.0\n",
      "Step: 4590, train/grad_norm: 4.334172576392348e-09\n",
      "Step: 4590, train/learning_rate: 3.179438499500975e-05\n",
      "Step: 4590, train/epoch: 1.0923370122909546\n",
      "Step: 4600, train/loss: 0.0\n",
      "Step: 4600, train/grad_norm: 7.33091587434842e-09\n",
      "Step: 4600, train/learning_rate: 3.175472011207603e-05\n",
      "Step: 4600, train/epoch: 1.0947167873382568\n",
      "Step: 4610, train/loss: 0.0\n",
      "Step: 4610, train/grad_norm: 3.063834583372227e-06\n",
      "Step: 4610, train/learning_rate: 3.171505522914231e-05\n",
      "Step: 4610, train/epoch: 1.097096562385559\n",
      "Step: 4620, train/loss: 0.0010999999940395355\n",
      "Step: 4620, train/grad_norm: 4.480598818190629e-06\n",
      "Step: 4620, train/learning_rate: 3.1675393984187394e-05\n",
      "Step: 4620, train/epoch: 1.0994764566421509\n",
      "Step: 4630, train/loss: 0.0\n",
      "Step: 4630, train/grad_norm: 1.3422958033970644e-07\n",
      "Step: 4630, train/learning_rate: 3.1635729101253673e-05\n",
      "Step: 4630, train/epoch: 1.1018562316894531\n",
      "Step: 4640, train/loss: 0.0\n",
      "Step: 4640, train/grad_norm: 1.9871360112944103e-09\n",
      "Step: 4640, train/learning_rate: 3.159606421831995e-05\n",
      "Step: 4640, train/epoch: 1.104236125946045\n",
      "Step: 4650, train/loss: 0.0\n",
      "Step: 4650, train/grad_norm: 1.2883612726000138e-05\n",
      "Step: 4650, train/learning_rate: 3.155640297336504e-05\n",
      "Step: 4650, train/epoch: 1.1066159009933472\n",
      "Step: 4660, train/loss: 0.0\n",
      "Step: 4660, train/grad_norm: 7.513884625609535e-09\n",
      "Step: 4660, train/learning_rate: 3.151673809043132e-05\n",
      "Step: 4660, train/epoch: 1.1089956760406494\n",
      "Step: 4670, train/loss: 0.0\n",
      "Step: 4670, train/grad_norm: 7.987117633945218e-09\n",
      "Step: 4670, train/learning_rate: 3.14770732074976e-05\n",
      "Step: 4670, train/epoch: 1.1113755702972412\n",
      "Step: 4680, train/loss: 0.0\n",
      "Step: 4680, train/grad_norm: 7.815204039474111e-10\n",
      "Step: 4680, train/learning_rate: 3.143741196254268e-05\n",
      "Step: 4680, train/epoch: 1.1137553453445435\n",
      "Step: 4690, train/loss: 0.0\n",
      "Step: 4690, train/grad_norm: 1.7177953282043745e-08\n",
      "Step: 4690, train/learning_rate: 3.139774707960896e-05\n",
      "Step: 4690, train/epoch: 1.1161351203918457\n",
      "Step: 4700, train/loss: 0.0\n",
      "Step: 4700, train/grad_norm: 8.298658649152912e-09\n",
      "Step: 4700, train/learning_rate: 3.135808219667524e-05\n",
      "Step: 4700, train/epoch: 1.1185150146484375\n",
      "Step: 4710, train/loss: 0.0\n",
      "Step: 4710, train/grad_norm: 1.2288795758763627e-08\n",
      "Step: 4710, train/learning_rate: 3.131842095172033e-05\n",
      "Step: 4710, train/epoch: 1.1208947896957397\n",
      "Step: 4720, train/loss: 0.0\n",
      "Step: 4720, train/grad_norm: 2.4801172457955545e-06\n",
      "Step: 4720, train/learning_rate: 3.1278756068786606e-05\n",
      "Step: 4720, train/epoch: 1.1232746839523315\n",
      "Step: 4730, train/loss: 0.0\n",
      "Step: 4730, train/grad_norm: 2.2472470551093693e-09\n",
      "Step: 4730, train/learning_rate: 3.1239091185852885e-05\n",
      "Step: 4730, train/epoch: 1.1256544589996338\n",
      "Step: 4740, train/loss: 0.0\n",
      "Step: 4740, train/grad_norm: 5.0967354781050744e-08\n",
      "Step: 4740, train/learning_rate: 3.119942994089797e-05\n",
      "Step: 4740, train/epoch: 1.128034234046936\n",
      "Step: 4750, train/loss: 0.0\n",
      "Step: 4750, train/grad_norm: 7.827864578757726e-09\n",
      "Step: 4750, train/learning_rate: 3.115976505796425e-05\n",
      "Step: 4750, train/epoch: 1.1304141283035278\n",
      "Step: 4760, train/loss: 0.0\n",
      "Step: 4760, train/grad_norm: 2.62670437223278e-06\n",
      "Step: 4760, train/learning_rate: 3.112010017503053e-05\n",
      "Step: 4760, train/epoch: 1.13279390335083\n",
      "Step: 4770, train/loss: 0.0\n",
      "Step: 4770, train/grad_norm: 4.391105505874293e-07\n",
      "Step: 4770, train/learning_rate: 3.1080438930075616e-05\n",
      "Step: 4770, train/epoch: 1.1351736783981323\n",
      "Step: 4780, train/loss: 0.0\n",
      "Step: 4780, train/grad_norm: 3.601281059673056e-05\n",
      "Step: 4780, train/learning_rate: 3.1040774047141895e-05\n",
      "Step: 4780, train/epoch: 1.1375535726547241\n",
      "Step: 4790, train/loss: 0.0\n",
      "Step: 4790, train/grad_norm: 1.6321990869982983e-08\n",
      "Step: 4790, train/learning_rate: 3.1001109164208174e-05\n",
      "Step: 4790, train/epoch: 1.1399333477020264\n",
      "Step: 4800, train/loss: 0.0\n",
      "Step: 4800, train/grad_norm: 5.904926769773056e-09\n",
      "Step: 4800, train/learning_rate: 3.096144791925326e-05\n",
      "Step: 4800, train/epoch: 1.1423132419586182\n",
      "Step: 4810, train/loss: 0.0\n",
      "Step: 4810, train/grad_norm: 7.867324569588163e-09\n",
      "Step: 4810, train/learning_rate: 3.092178303631954e-05\n",
      "Step: 4810, train/epoch: 1.1446930170059204\n",
      "Step: 4820, train/loss: 0.0\n",
      "Step: 4820, train/grad_norm: 6.139709629593426e-09\n",
      "Step: 4820, train/learning_rate: 3.088211815338582e-05\n",
      "Step: 4820, train/epoch: 1.1470727920532227\n",
      "Step: 4830, train/loss: 0.0\n",
      "Step: 4830, train/grad_norm: 5.626851873330452e-09\n",
      "Step: 4830, train/learning_rate: 3.0842456908430904e-05\n",
      "Step: 4830, train/epoch: 1.1494526863098145\n",
      "Step: 4840, train/loss: 0.0\n",
      "Step: 4840, train/grad_norm: 1.6347541986760916e-08\n",
      "Step: 4840, train/learning_rate: 3.080279202549718e-05\n",
      "Step: 4840, train/epoch: 1.1518324613571167\n",
      "Step: 4850, train/loss: 0.0\n",
      "Step: 4850, train/grad_norm: 7.298157100876779e-08\n",
      "Step: 4850, train/learning_rate: 3.076312714256346e-05\n",
      "Step: 4850, train/epoch: 1.154212236404419\n",
      "Step: 4860, train/loss: 0.0020000000949949026\n",
      "Step: 4860, train/grad_norm: 3.486939625929608e-09\n",
      "Step: 4860, train/learning_rate: 3.072346589760855e-05\n",
      "Step: 4860, train/epoch: 1.1565921306610107\n",
      "Step: 4870, train/loss: 0.0\n",
      "Step: 4870, train/grad_norm: 5.910641309725406e-09\n",
      "Step: 4870, train/learning_rate: 3.068380101467483e-05\n",
      "Step: 4870, train/epoch: 1.158971905708313\n",
      "Step: 4880, train/loss: 0.0\n",
      "Step: 4880, train/grad_norm: 6.422781950732315e-08\n",
      "Step: 4880, train/learning_rate: 3.0644136131741107e-05\n",
      "Step: 4880, train/epoch: 1.1613516807556152\n",
      "Step: 4890, train/loss: 0.0\n",
      "Step: 4890, train/grad_norm: 1.0703247177445974e-09\n",
      "Step: 4890, train/learning_rate: 3.060447488678619e-05\n",
      "Step: 4890, train/epoch: 1.163731575012207\n",
      "Step: 4900, train/loss: 0.0\n",
      "Step: 4900, train/grad_norm: 0.0005371173610910773\n",
      "Step: 4900, train/learning_rate: 3.056481000385247e-05\n",
      "Step: 4900, train/epoch: 1.1661113500595093\n",
      "Step: 4910, train/loss: 0.0\n",
      "Step: 4910, train/grad_norm: 1.611315929039847e-05\n",
      "Step: 4910, train/learning_rate: 3.052514512091875e-05\n",
      "Step: 4910, train/epoch: 1.168491244316101\n",
      "Step: 4920, train/loss: 0.0008999999845400453\n",
      "Step: 4920, train/grad_norm: 1.5637216677077959e-07\n",
      "Step: 4920, train/learning_rate: 3.0485483875963837e-05\n",
      "Step: 4920, train/epoch: 1.1708710193634033\n",
      "Step: 4930, train/loss: 0.0\n",
      "Step: 4930, train/grad_norm: 2.0450536819538456e-09\n",
      "Step: 4930, train/learning_rate: 3.0445818993030116e-05\n",
      "Step: 4930, train/epoch: 1.1732507944107056\n",
      "Step: 4940, train/loss: 0.0\n",
      "Step: 4940, train/grad_norm: 5.28005683619881e-09\n",
      "Step: 4940, train/learning_rate: 3.04061559290858e-05\n",
      "Step: 4940, train/epoch: 1.1756306886672974\n",
      "Step: 4950, train/loss: 0.0\n",
      "Step: 4950, train/grad_norm: 1.085783374321636e-08\n",
      "Step: 4950, train/learning_rate: 3.036649286514148e-05\n",
      "Step: 4950, train/epoch: 1.1780104637145996\n",
      "Step: 4960, train/loss: 0.0\n",
      "Step: 4960, train/grad_norm: 8.57263682263465e-09\n",
      "Step: 4960, train/learning_rate: 3.032682798220776e-05\n",
      "Step: 4960, train/epoch: 1.1803902387619019\n",
      "Step: 4970, train/loss: 0.0\n",
      "Step: 4970, train/grad_norm: 2.706639747884765e-07\n",
      "Step: 4970, train/learning_rate: 3.0287164918263443e-05\n",
      "Step: 4970, train/epoch: 1.1827701330184937\n",
      "Step: 4980, train/loss: 0.0\n",
      "Step: 4980, train/grad_norm: 4.408911014053274e-09\n",
      "Step: 4980, train/learning_rate: 3.0247501854319125e-05\n",
      "Step: 4980, train/epoch: 1.185149908065796\n",
      "Step: 4990, train/loss: 0.0\n",
      "Step: 4990, train/grad_norm: 3.955708649527878e-09\n",
      "Step: 4990, train/learning_rate: 3.0207836971385404e-05\n",
      "Step: 4990, train/epoch: 1.1875298023223877\n",
      "Step: 5000, train/loss: 0.0\n",
      "Step: 5000, train/grad_norm: 1.3115559305276747e-08\n",
      "Step: 5000, train/learning_rate: 3.0168173907441087e-05\n",
      "Step: 5000, train/epoch: 1.18990957736969\n",
      "Step: 5010, train/loss: 0.0\n",
      "Step: 5010, train/grad_norm: 7.963742443273247e-10\n",
      "Step: 5010, train/learning_rate: 3.012851084349677e-05\n",
      "Step: 5010, train/epoch: 1.1922893524169922\n",
      "Step: 5020, train/loss: 0.0\n",
      "Step: 5020, train/grad_norm: 8.960016728565279e-09\n",
      "Step: 5020, train/learning_rate: 3.008884596056305e-05\n",
      "Step: 5020, train/epoch: 1.194669246673584\n",
      "Step: 5030, train/loss: 0.0\n",
      "Step: 5030, train/grad_norm: 7.373324617532262e-09\n",
      "Step: 5030, train/learning_rate: 3.004918289661873e-05\n",
      "Step: 5030, train/epoch: 1.1970490217208862\n",
      "Step: 5040, train/loss: 0.0\n",
      "Step: 5040, train/grad_norm: 8.895105985118335e-09\n",
      "Step: 5040, train/learning_rate: 3.0009519832674414e-05\n",
      "Step: 5040, train/epoch: 1.1994287967681885\n",
      "Step: 5050, train/loss: 0.0\n",
      "Step: 5050, train/grad_norm: 4.334826719798457e-09\n",
      "Step: 5050, train/learning_rate: 2.9969854949740693e-05\n",
      "Step: 5050, train/epoch: 1.2018086910247803\n",
      "Step: 5060, train/loss: 0.0\n",
      "Step: 5060, train/grad_norm: 3.68899000235956e-09\n",
      "Step: 5060, train/learning_rate: 2.9930191885796376e-05\n",
      "Step: 5060, train/epoch: 1.2041884660720825\n",
      "Step: 5070, train/loss: 0.0\n",
      "Step: 5070, train/grad_norm: 1.0022498386774714e-08\n",
      "Step: 5070, train/learning_rate: 2.9890528821852058e-05\n",
      "Step: 5070, train/epoch: 1.2065683603286743\n",
      "Step: 5080, train/loss: 0.0\n",
      "Step: 5080, train/grad_norm: 9.226081232327488e-09\n",
      "Step: 5080, train/learning_rate: 2.9850863938918337e-05\n",
      "Step: 5080, train/epoch: 1.2089481353759766\n",
      "Step: 5090, train/loss: 0.0\n",
      "Step: 5090, train/grad_norm: 5.615043097151329e-09\n",
      "Step: 5090, train/learning_rate: 2.981120087497402e-05\n",
      "Step: 5090, train/epoch: 1.2113279104232788\n",
      "Step: 5100, train/loss: 0.0\n",
      "Step: 5100, train/grad_norm: 2.544167614360049e-07\n",
      "Step: 5100, train/learning_rate: 2.9771537811029702e-05\n",
      "Step: 5100, train/epoch: 1.2137078046798706\n",
      "Step: 5110, train/loss: 0.0\n",
      "Step: 5110, train/grad_norm: 1.0412942730297914e-09\n",
      "Step: 5110, train/learning_rate: 2.973187292809598e-05\n",
      "Step: 5110, train/epoch: 1.2160875797271729\n",
      "Step: 5120, train/loss: 0.0\n",
      "Step: 5120, train/grad_norm: 1.8307397819938842e-09\n",
      "Step: 5120, train/learning_rate: 2.9692209864151664e-05\n",
      "Step: 5120, train/epoch: 1.218467354774475\n",
      "Step: 5130, train/loss: 0.0\n",
      "Step: 5130, train/grad_norm: 8.602149215164445e-09\n",
      "Step: 5130, train/learning_rate: 2.9652546800207347e-05\n",
      "Step: 5130, train/epoch: 1.220847249031067\n",
      "Step: 5140, train/loss: 0.0\n",
      "Step: 5140, train/grad_norm: 8.49008952030772e-09\n",
      "Step: 5140, train/learning_rate: 2.9612881917273626e-05\n",
      "Step: 5140, train/epoch: 1.2232270240783691\n",
      "Step: 5150, train/loss: 0.0\n",
      "Step: 5150, train/grad_norm: 1.5643909279106083e-08\n",
      "Step: 5150, train/learning_rate: 2.957321885332931e-05\n",
      "Step: 5150, train/epoch: 1.2256067991256714\n",
      "Step: 5160, train/loss: 0.0\n",
      "Step: 5160, train/grad_norm: 8.845868926243838e-09\n",
      "Step: 5160, train/learning_rate: 2.953355578938499e-05\n",
      "Step: 5160, train/epoch: 1.2279866933822632\n",
      "Step: 5170, train/loss: 0.0\n",
      "Step: 5170, train/grad_norm: 3.4997242881473767e-09\n",
      "Step: 5170, train/learning_rate: 2.949389090645127e-05\n",
      "Step: 5170, train/epoch: 1.2303664684295654\n",
      "Step: 5180, train/loss: 0.0\n",
      "Step: 5180, train/grad_norm: 1.3720952551921073e-07\n",
      "Step: 5180, train/learning_rate: 2.9454227842506953e-05\n",
      "Step: 5180, train/epoch: 1.2327463626861572\n",
      "Step: 5190, train/loss: 0.0\n",
      "Step: 5190, train/grad_norm: 3.639109991127043e-08\n",
      "Step: 5190, train/learning_rate: 2.9414564778562635e-05\n",
      "Step: 5190, train/epoch: 1.2351261377334595\n",
      "Step: 5200, train/loss: 0.0\n",
      "Step: 5200, train/grad_norm: 1.0725280219503475e-08\n",
      "Step: 5200, train/learning_rate: 2.9374901714618318e-05\n",
      "Step: 5200, train/epoch: 1.2375059127807617\n",
      "Step: 5210, train/loss: 0.0\n",
      "Step: 5210, train/grad_norm: 2.975156654372313e-08\n",
      "Step: 5210, train/learning_rate: 2.9335236831684597e-05\n",
      "Step: 5210, train/epoch: 1.2398858070373535\n",
      "Step: 5220, train/loss: 0.0\n",
      "Step: 5220, train/grad_norm: 3.9055336742421787e-10\n",
      "Step: 5220, train/learning_rate: 2.929557376774028e-05\n",
      "Step: 5220, train/epoch: 1.2422655820846558\n",
      "Step: 5230, train/loss: 0.0\n",
      "Step: 5230, train/grad_norm: 1.1419507117693684e-08\n",
      "Step: 5230, train/learning_rate: 2.9255910703795962e-05\n",
      "Step: 5230, train/epoch: 1.244645357131958\n",
      "Step: 5240, train/loss: 0.0\n",
      "Step: 5240, train/grad_norm: 0.00014150742208585143\n",
      "Step: 5240, train/learning_rate: 2.921624582086224e-05\n",
      "Step: 5240, train/epoch: 1.2470252513885498\n",
      "Step: 5250, train/loss: 0.07029999792575836\n",
      "Step: 5250, train/grad_norm: 9.14356874659461e-08\n",
      "Step: 5250, train/learning_rate: 2.9176582756917924e-05\n",
      "Step: 5250, train/epoch: 1.249405026435852\n",
      "Step: 5260, train/loss: 0.0\n",
      "Step: 5260, train/grad_norm: 2.098705920161592e-07\n",
      "Step: 5260, train/learning_rate: 2.9136919692973606e-05\n",
      "Step: 5260, train/epoch: 1.2517849206924438\n",
      "Step: 5270, train/loss: 0.0\n",
      "Step: 5270, train/grad_norm: 5.687103111995384e-06\n",
      "Step: 5270, train/learning_rate: 2.9097254810039885e-05\n",
      "Step: 5270, train/epoch: 1.254164695739746\n",
      "Step: 5280, train/loss: 0.0\n",
      "Step: 5280, train/grad_norm: 3.615271566559386e-07\n",
      "Step: 5280, train/learning_rate: 2.9057591746095568e-05\n",
      "Step: 5280, train/epoch: 1.2565444707870483\n",
      "Step: 5290, train/loss: 0.0\n",
      "Step: 5290, train/grad_norm: 2.6834743493964197e-07\n",
      "Step: 5290, train/learning_rate: 2.901792868215125e-05\n",
      "Step: 5290, train/epoch: 1.2589243650436401\n",
      "Step: 5300, train/loss: 0.0\n",
      "Step: 5300, train/grad_norm: 9.027393588212362e-08\n",
      "Step: 5300, train/learning_rate: 2.897826379921753e-05\n",
      "Step: 5300, train/epoch: 1.2613041400909424\n",
      "Step: 5310, train/loss: 0.0\n",
      "Step: 5310, train/grad_norm: 1.0297661674485425e-06\n",
      "Step: 5310, train/learning_rate: 2.8938600735273212e-05\n",
      "Step: 5310, train/epoch: 1.2636839151382446\n",
      "Step: 5320, train/loss: 0.0\n",
      "Step: 5320, train/grad_norm: 7.420304655170185e-07\n",
      "Step: 5320, train/learning_rate: 2.8898937671328895e-05\n",
      "Step: 5320, train/epoch: 1.2660638093948364\n",
      "Step: 5330, train/loss: 0.0\n",
      "Step: 5330, train/grad_norm: 7.512961133215867e-07\n",
      "Step: 5330, train/learning_rate: 2.8859272788395174e-05\n",
      "Step: 5330, train/epoch: 1.2684435844421387\n",
      "Step: 5340, train/loss: 0.0\n",
      "Step: 5340, train/grad_norm: 1.0646050213836133e-05\n",
      "Step: 5340, train/learning_rate: 2.8819609724450856e-05\n",
      "Step: 5340, train/epoch: 1.270823359489441\n",
      "Step: 5350, train/loss: 0.0\n",
      "Step: 5350, train/grad_norm: 3.1162503688619836e-08\n",
      "Step: 5350, train/learning_rate: 2.877994666050654e-05\n",
      "Step: 5350, train/epoch: 1.2732032537460327\n",
      "Step: 5360, train/loss: 0.0\n",
      "Step: 5360, train/grad_norm: 0.00035207404289394617\n",
      "Step: 5360, train/learning_rate: 2.8740281777572818e-05\n",
      "Step: 5360, train/epoch: 1.275583028793335\n",
      "Step: 5370, train/loss: 0.0\n",
      "Step: 5370, train/grad_norm: 6.188904535520123e-07\n",
      "Step: 5370, train/learning_rate: 2.87006187136285e-05\n",
      "Step: 5370, train/epoch: 1.2779629230499268\n",
      "Step: 5380, train/loss: 0.0006000000284984708\n",
      "Step: 5380, train/grad_norm: 0.0004011438868474215\n",
      "Step: 5380, train/learning_rate: 2.8660955649684183e-05\n",
      "Step: 5380, train/epoch: 1.280342698097229\n",
      "Step: 5390, train/loss: 0.0\n",
      "Step: 5390, train/grad_norm: 2.5631636617617914e-06\n",
      "Step: 5390, train/learning_rate: 2.8621290766750462e-05\n",
      "Step: 5390, train/epoch: 1.2827224731445312\n",
      "Step: 5400, train/loss: 0.0\n",
      "Step: 5400, train/grad_norm: 5.408985543908784e-07\n",
      "Step: 5400, train/learning_rate: 2.8581627702806145e-05\n",
      "Step: 5400, train/epoch: 1.285102367401123\n",
      "Step: 5410, train/loss: 0.0\n",
      "Step: 5410, train/grad_norm: 2.2317230730095616e-07\n",
      "Step: 5410, train/learning_rate: 2.8541964638861828e-05\n",
      "Step: 5410, train/epoch: 1.2874821424484253\n",
      "Step: 5420, train/loss: 0.0\n",
      "Step: 5420, train/grad_norm: 4.230602002053274e-08\n",
      "Step: 5420, train/learning_rate: 2.8502299755928107e-05\n",
      "Step: 5420, train/epoch: 1.2898619174957275\n",
      "Step: 5430, train/loss: 0.0\n",
      "Step: 5430, train/grad_norm: 3.073011214382859e-07\n",
      "Step: 5430, train/learning_rate: 2.846263669198379e-05\n",
      "Step: 5430, train/epoch: 1.2922418117523193\n",
      "Step: 5440, train/loss: 0.11479999870061874\n",
      "Step: 5440, train/grad_norm: 2.955029003715026e-07\n",
      "Step: 5440, train/learning_rate: 2.8422973628039472e-05\n",
      "Step: 5440, train/epoch: 1.2946215867996216\n",
      "Step: 5450, train/loss: 0.0\n",
      "Step: 5450, train/grad_norm: 4.2038286096612865e-07\n",
      "Step: 5450, train/learning_rate: 2.838330874510575e-05\n",
      "Step: 5450, train/epoch: 1.2970014810562134\n",
      "Step: 5460, train/loss: 0.0\n",
      "Step: 5460, train/grad_norm: 1.0913553296632017e-06\n",
      "Step: 5460, train/learning_rate: 2.8343645681161433e-05\n",
      "Step: 5460, train/epoch: 1.2993812561035156\n",
      "Step: 5470, train/loss: 0.0\n",
      "Step: 5470, train/grad_norm: 5.860200644747238e-07\n",
      "Step: 5470, train/learning_rate: 2.8303982617217116e-05\n",
      "Step: 5470, train/epoch: 1.3017610311508179\n",
      "Step: 5480, train/loss: 0.0\n",
      "Step: 5480, train/grad_norm: 2.225208817208113e-07\n",
      "Step: 5480, train/learning_rate: 2.8264317734283395e-05\n",
      "Step: 5480, train/epoch: 1.3041409254074097\n",
      "Step: 5490, train/loss: 0.0\n",
      "Step: 5490, train/grad_norm: 1.1609828334258054e-06\n",
      "Step: 5490, train/learning_rate: 2.8224654670339078e-05\n",
      "Step: 5490, train/epoch: 1.306520700454712\n",
      "Step: 5500, train/loss: 0.0\n",
      "Step: 5500, train/grad_norm: 2.8185754672449548e-06\n",
      "Step: 5500, train/learning_rate: 2.818499160639476e-05\n",
      "Step: 5500, train/epoch: 1.3089004755020142\n",
      "Step: 5510, train/loss: 0.0\n",
      "Step: 5510, train/grad_norm: 3.171169851157174e-07\n",
      "Step: 5510, train/learning_rate: 2.814532672346104e-05\n",
      "Step: 5510, train/epoch: 1.311280369758606\n",
      "Step: 5520, train/loss: 0.0\n",
      "Step: 5520, train/grad_norm: 4.2878498788923025e-05\n",
      "Step: 5520, train/learning_rate: 2.8105663659516722e-05\n",
      "Step: 5520, train/epoch: 1.3136601448059082\n",
      "Step: 5530, train/loss: 0.0\n",
      "Step: 5530, train/grad_norm: 1.0256393352392479e-06\n",
      "Step: 5530, train/learning_rate: 2.8066000595572405e-05\n",
      "Step: 5530, train/epoch: 1.3160400390625\n",
      "Step: 5540, train/loss: 0.019099999219179153\n",
      "Step: 5540, train/grad_norm: 6.081115316192154e-06\n",
      "Step: 5540, train/learning_rate: 2.8026337531628087e-05\n",
      "Step: 5540, train/epoch: 1.3184198141098022\n",
      "Step: 5550, train/loss: 0.0\n",
      "Step: 5550, train/grad_norm: 4.385788372474053e-07\n",
      "Step: 5550, train/learning_rate: 2.7986672648694366e-05\n",
      "Step: 5550, train/epoch: 1.3207995891571045\n",
      "Step: 5560, train/loss: 0.0\n",
      "Step: 5560, train/grad_norm: 6.010298238834366e-05\n",
      "Step: 5560, train/learning_rate: 2.794700958475005e-05\n",
      "Step: 5560, train/epoch: 1.3231794834136963\n",
      "Step: 5570, train/loss: 0.0\n",
      "Step: 5570, train/grad_norm: 0.001183380139991641\n",
      "Step: 5570, train/learning_rate: 2.790734652080573e-05\n",
      "Step: 5570, train/epoch: 1.3255592584609985\n",
      "Step: 5580, train/loss: 0.0\n",
      "Step: 5580, train/grad_norm: 3.192102724369761e-07\n",
      "Step: 5580, train/learning_rate: 2.786768163787201e-05\n",
      "Step: 5580, train/epoch: 1.3279390335083008\n",
      "Step: 5590, train/loss: 0.0\n",
      "Step: 5590, train/grad_norm: 9.916017734212801e-05\n",
      "Step: 5590, train/learning_rate: 2.7828018573927693e-05\n",
      "Step: 5590, train/epoch: 1.3303189277648926\n",
      "Step: 5600, train/loss: 0.0\n",
      "Step: 5600, train/grad_norm: 1.2879623682238162e-05\n",
      "Step: 5600, train/learning_rate: 2.7788355509983376e-05\n",
      "Step: 5600, train/epoch: 1.3326987028121948\n",
      "Step: 5610, train/loss: 0.0\n",
      "Step: 5610, train/grad_norm: 5.533150329029013e-07\n",
      "Step: 5610, train/learning_rate: 2.7748690627049655e-05\n",
      "Step: 5610, train/epoch: 1.335078477859497\n",
      "Step: 5620, train/loss: 0.0\n",
      "Step: 5620, train/grad_norm: 5.749354841100285e-07\n",
      "Step: 5620, train/learning_rate: 2.7709027563105337e-05\n",
      "Step: 5620, train/epoch: 1.3374583721160889\n",
      "Step: 5630, train/loss: 0.0\n",
      "Step: 5630, train/grad_norm: 5.12009828526061e-05\n",
      "Step: 5630, train/learning_rate: 2.766936449916102e-05\n",
      "Step: 5630, train/epoch: 1.3398381471633911\n",
      "Step: 5640, train/loss: 0.0\n",
      "Step: 5640, train/grad_norm: 3.9683931163381203e-07\n",
      "Step: 5640, train/learning_rate: 2.76296996162273e-05\n",
      "Step: 5640, train/epoch: 1.342218041419983\n",
      "Step: 5650, train/loss: 0.0\n",
      "Step: 5650, train/grad_norm: 5.391067929849669e-07\n",
      "Step: 5650, train/learning_rate: 2.759003655228298e-05\n",
      "Step: 5650, train/epoch: 1.3445978164672852\n",
      "Step: 5660, train/loss: 0.0\n",
      "Step: 5660, train/grad_norm: 2.3252601977219456e-07\n",
      "Step: 5660, train/learning_rate: 2.7550373488338664e-05\n",
      "Step: 5660, train/epoch: 1.3469775915145874\n",
      "Step: 5670, train/loss: 0.0\n",
      "Step: 5670, train/grad_norm: 9.553732382983071e-08\n",
      "Step: 5670, train/learning_rate: 2.7510708605404943e-05\n",
      "Step: 5670, train/epoch: 1.3493574857711792\n",
      "Step: 5680, train/loss: 0.0\n",
      "Step: 5680, train/grad_norm: 2.660836742052197e-07\n",
      "Step: 5680, train/learning_rate: 2.7471045541460626e-05\n",
      "Step: 5680, train/epoch: 1.3517372608184814\n",
      "Step: 5690, train/loss: 0.0\n",
      "Step: 5690, train/grad_norm: 1.0254348836724603e-07\n",
      "Step: 5690, train/learning_rate: 2.743138247751631e-05\n",
      "Step: 5690, train/epoch: 1.3541170358657837\n",
      "Step: 5700, train/loss: 0.0\n",
      "Step: 5700, train/grad_norm: 6.129946541477693e-07\n",
      "Step: 5700, train/learning_rate: 2.7391717594582587e-05\n",
      "Step: 5700, train/epoch: 1.3564969301223755\n",
      "Step: 5710, train/loss: 0.0\n",
      "Step: 5710, train/grad_norm: 2.973388745886041e-06\n",
      "Step: 5710, train/learning_rate: 2.735205453063827e-05\n",
      "Step: 5710, train/epoch: 1.3588767051696777\n",
      "Step: 5720, train/loss: 0.0\n",
      "Step: 5720, train/grad_norm: 4.71327268769528e-07\n",
      "Step: 5720, train/learning_rate: 2.7312391466693953e-05\n",
      "Step: 5720, train/epoch: 1.3612565994262695\n",
      "Step: 5730, train/loss: 0.0\n",
      "Step: 5730, train/grad_norm: 1.7428941134767229e-07\n",
      "Step: 5730, train/learning_rate: 2.7272726583760232e-05\n",
      "Step: 5730, train/epoch: 1.3636363744735718\n",
      "Step: 5740, train/loss: 0.0\n",
      "Step: 5740, train/grad_norm: 8.931461707106791e-06\n",
      "Step: 5740, train/learning_rate: 2.7233063519815914e-05\n",
      "Step: 5740, train/epoch: 1.366016149520874\n",
      "Step: 5750, train/loss: 0.0\n",
      "Step: 5750, train/grad_norm: 2.6873433967011806e-07\n",
      "Step: 5750, train/learning_rate: 2.7193400455871597e-05\n",
      "Step: 5750, train/epoch: 1.3683960437774658\n",
      "Step: 5760, train/loss: 0.0\n",
      "Step: 5760, train/grad_norm: 4.774951230501756e-05\n",
      "Step: 5760, train/learning_rate: 2.7153735572937876e-05\n",
      "Step: 5760, train/epoch: 1.370775818824768\n",
      "Step: 5770, train/loss: 0.0\n",
      "Step: 5770, train/grad_norm: 5.583099209616194e-07\n",
      "Step: 5770, train/learning_rate: 2.711407250899356e-05\n",
      "Step: 5770, train/epoch: 1.3731555938720703\n",
      "Step: 5780, train/loss: 0.0\n",
      "Step: 5780, train/grad_norm: 2.471235518441972e-07\n",
      "Step: 5780, train/learning_rate: 2.707440944504924e-05\n",
      "Step: 5780, train/epoch: 1.375535488128662\n",
      "Step: 5790, train/loss: 0.0\n",
      "Step: 5790, train/grad_norm: 1.992046776422285e-07\n",
      "Step: 5790, train/learning_rate: 2.703474456211552e-05\n",
      "Step: 5790, train/epoch: 1.3779152631759644\n",
      "Step: 5800, train/loss: 0.0\n",
      "Step: 5800, train/grad_norm: 5.3753602458073146e-08\n",
      "Step: 5800, train/learning_rate: 2.6995081498171203e-05\n",
      "Step: 5800, train/epoch: 1.3802950382232666\n",
      "Step: 5810, train/loss: 0.0\n",
      "Step: 5810, train/grad_norm: 7.573789275738818e-07\n",
      "Step: 5810, train/learning_rate: 2.6955418434226885e-05\n",
      "Step: 5810, train/epoch: 1.3826749324798584\n",
      "Step: 5820, train/loss: 0.0\n",
      "Step: 5820, train/grad_norm: 3.894123778991343e-08\n",
      "Step: 5820, train/learning_rate: 2.6915753551293164e-05\n",
      "Step: 5820, train/epoch: 1.3850547075271606\n",
      "Step: 5830, train/loss: 0.0\n",
      "Step: 5830, train/grad_norm: 3.662627477751812e-06\n",
      "Step: 5830, train/learning_rate: 2.6876090487348847e-05\n",
      "Step: 5830, train/epoch: 1.3874346017837524\n",
      "Step: 5840, train/loss: 0.0\n",
      "Step: 5840, train/grad_norm: 2.0879130602224905e-07\n",
      "Step: 5840, train/learning_rate: 2.683642742340453e-05\n",
      "Step: 5840, train/epoch: 1.3898143768310547\n",
      "Step: 5850, train/loss: 0.0\n",
      "Step: 5850, train/grad_norm: 1.1648749165260597e-07\n",
      "Step: 5850, train/learning_rate: 2.679676254047081e-05\n",
      "Step: 5850, train/epoch: 1.392194151878357\n",
      "Step: 5860, train/loss: 0.0\n",
      "Step: 5860, train/grad_norm: 1.8352015729306004e-07\n",
      "Step: 5860, train/learning_rate: 2.675709947652649e-05\n",
      "Step: 5860, train/epoch: 1.3945740461349487\n",
      "Step: 5870, train/loss: 0.0\n",
      "Step: 5870, train/grad_norm: 5.362662705010734e-07\n",
      "Step: 5870, train/learning_rate: 2.6717436412582174e-05\n",
      "Step: 5870, train/epoch: 1.396953821182251\n",
      "Step: 5880, train/loss: 0.0\n",
      "Step: 5880, train/grad_norm: 3.8194510665334747e-08\n",
      "Step: 5880, train/learning_rate: 2.6677773348637857e-05\n",
      "Step: 5880, train/epoch: 1.3993335962295532\n",
      "Step: 5890, train/loss: 0.0\n",
      "Step: 5890, train/grad_norm: 1.568838854382193e-07\n",
      "Step: 5890, train/learning_rate: 2.6638108465704136e-05\n",
      "Step: 5890, train/epoch: 1.401713490486145\n",
      "Step: 5900, train/loss: 0.0\n",
      "Step: 5900, train/grad_norm: 2.025509729719488e-06\n",
      "Step: 5900, train/learning_rate: 2.6598445401759818e-05\n",
      "Step: 5900, train/epoch: 1.4040932655334473\n",
      "Step: 5910, train/loss: 0.0\n",
      "Step: 5910, train/grad_norm: 3.633070093655988e-07\n",
      "Step: 5910, train/learning_rate: 2.65587823378155e-05\n",
      "Step: 5910, train/epoch: 1.406473159790039\n",
      "Step: 5920, train/loss: 0.0\n",
      "Step: 5920, train/grad_norm: 1.8107652067556046e-05\n",
      "Step: 5920, train/learning_rate: 2.651911745488178e-05\n",
      "Step: 5920, train/epoch: 1.4088529348373413\n",
      "Step: 5930, train/loss: 0.0\n",
      "Step: 5930, train/grad_norm: 1.9442823884219251e-07\n",
      "Step: 5930, train/learning_rate: 2.6479454390937462e-05\n",
      "Step: 5930, train/epoch: 1.4112327098846436\n",
      "Step: 5940, train/loss: 0.0\n",
      "Step: 5940, train/grad_norm: 4.674692536354996e-05\n",
      "Step: 5940, train/learning_rate: 2.6439791326993145e-05\n",
      "Step: 5940, train/epoch: 1.4136126041412354\n",
      "Step: 5950, train/loss: 0.0\n",
      "Step: 5950, train/grad_norm: 0.00020438125648070127\n",
      "Step: 5950, train/learning_rate: 2.6400126444059424e-05\n",
      "Step: 5950, train/epoch: 1.4159923791885376\n",
      "Step: 5960, train/loss: 0.02710000053048134\n",
      "Step: 5960, train/grad_norm: 2.6699569843913196e-07\n",
      "Step: 5960, train/learning_rate: 2.6360463380115107e-05\n",
      "Step: 5960, train/epoch: 1.4183721542358398\n",
      "Step: 5970, train/loss: 0.0\n",
      "Step: 5970, train/grad_norm: 3.2612955891409e-08\n",
      "Step: 5970, train/learning_rate: 2.632080031617079e-05\n",
      "Step: 5970, train/epoch: 1.4207520484924316\n",
      "Step: 5980, train/loss: 0.0\n",
      "Step: 5980, train/grad_norm: 1.1750720574355e-07\n",
      "Step: 5980, train/learning_rate: 2.628113543323707e-05\n",
      "Step: 5980, train/epoch: 1.4231318235397339\n",
      "Step: 5990, train/loss: 0.12099999934434891\n",
      "Step: 5990, train/grad_norm: 3.977475060423785e-08\n",
      "Step: 5990, train/learning_rate: 2.624147236929275e-05\n",
      "Step: 5990, train/epoch: 1.4255117177963257\n",
      "Step: 6000, train/loss: 0.03009999915957451\n",
      "Step: 6000, train/grad_norm: 3.789565425904584e-06\n",
      "Step: 6000, train/learning_rate: 2.6201809305348434e-05\n",
      "Step: 6000, train/epoch: 1.427891492843628\n",
      "Step: 6010, train/loss: 0.0\n",
      "Step: 6010, train/grad_norm: 6.009867138345726e-05\n",
      "Step: 6010, train/learning_rate: 2.6162144422414713e-05\n",
      "Step: 6010, train/epoch: 1.4302712678909302\n",
      "Step: 6020, train/loss: 0.0\n",
      "Step: 6020, train/grad_norm: 0.00047211351920850575\n",
      "Step: 6020, train/learning_rate: 2.6122481358470395e-05\n",
      "Step: 6020, train/epoch: 1.432651162147522\n",
      "Step: 6030, train/loss: 0.20630000531673431\n",
      "Step: 6030, train/grad_norm: 2.969632078020368e-07\n",
      "Step: 6030, train/learning_rate: 2.6082818294526078e-05\n",
      "Step: 6030, train/epoch: 1.4350309371948242\n",
      "Step: 6040, train/loss: 0.0\n",
      "Step: 6040, train/grad_norm: 2.6180011900578393e-06\n",
      "Step: 6040, train/learning_rate: 2.6043153411592357e-05\n",
      "Step: 6040, train/epoch: 1.4374107122421265\n",
      "Step: 6050, train/loss: 0.0\n",
      "Step: 6050, train/grad_norm: 4.3183240450161975e-06\n",
      "Step: 6050, train/learning_rate: 2.600349034764804e-05\n",
      "Step: 6050, train/epoch: 1.4397906064987183\n",
      "Step: 6060, train/loss: 0.0\n",
      "Step: 6060, train/grad_norm: 4.756210182677023e-05\n",
      "Step: 6060, train/learning_rate: 2.5963827283703722e-05\n",
      "Step: 6060, train/epoch: 1.4421703815460205\n",
      "Step: 6070, train/loss: 0.0\n",
      "Step: 6070, train/grad_norm: 0.0004417181189637631\n",
      "Step: 6070, train/learning_rate: 2.592416240077e-05\n",
      "Step: 6070, train/epoch: 1.4445501565933228\n",
      "Step: 6080, train/loss: 0.0\n",
      "Step: 6080, train/grad_norm: 0.00015200306370388716\n",
      "Step: 6080, train/learning_rate: 2.5884499336825684e-05\n",
      "Step: 6080, train/epoch: 1.4469300508499146\n",
      "Step: 6090, train/loss: 0.0\n",
      "Step: 6090, train/grad_norm: 5.738496838603169e-05\n",
      "Step: 6090, train/learning_rate: 2.5844836272881366e-05\n",
      "Step: 6090, train/epoch: 1.4493098258972168\n",
      "Step: 6100, train/loss: 0.010599999688565731\n",
      "Step: 6100, train/grad_norm: 3.332941105327336e-06\n",
      "Step: 6100, train/learning_rate: 2.5805171389947645e-05\n",
      "Step: 6100, train/epoch: 1.4516897201538086\n",
      "Step: 6110, train/loss: 0.0\n",
      "Step: 6110, train/grad_norm: 0.0044080838561058044\n",
      "Step: 6110, train/learning_rate: 2.5765508326003328e-05\n",
      "Step: 6110, train/epoch: 1.4540694952011108\n",
      "Step: 6120, train/loss: 0.0\n",
      "Step: 6120, train/grad_norm: 9.426289580005687e-06\n",
      "Step: 6120, train/learning_rate: 2.572584526205901e-05\n",
      "Step: 6120, train/epoch: 1.456449270248413\n",
      "Step: 6130, train/loss: 0.00019999999494757503\n",
      "Step: 6130, train/grad_norm: 8.75859313964611e-06\n",
      "Step: 6130, train/learning_rate: 2.568618037912529e-05\n",
      "Step: 6130, train/epoch: 1.4588291645050049\n",
      "Step: 6140, train/loss: 0.0\n",
      "Step: 6140, train/grad_norm: 0.0009096562280319631\n",
      "Step: 6140, train/learning_rate: 2.5646517315180972e-05\n",
      "Step: 6140, train/epoch: 1.4612089395523071\n",
      "Step: 6150, train/loss: 0.0\n",
      "Step: 6150, train/grad_norm: 3.595638554543257e-05\n",
      "Step: 6150, train/learning_rate: 2.5606854251236655e-05\n",
      "Step: 6150, train/epoch: 1.4635887145996094\n",
      "Step: 6160, train/loss: 0.0\n",
      "Step: 6160, train/grad_norm: 1.2302650702622486e-06\n",
      "Step: 6160, train/learning_rate: 2.5567189368302934e-05\n",
      "Step: 6160, train/epoch: 1.4659686088562012\n",
      "Step: 6170, train/loss: 0.0\n",
      "Step: 6170, train/grad_norm: 1.786030588846188e-05\n",
      "Step: 6170, train/learning_rate: 2.5527526304358616e-05\n",
      "Step: 6170, train/epoch: 1.4683483839035034\n",
      "Step: 6180, train/loss: 0.0\n",
      "Step: 6180, train/grad_norm: 2.4093656065815594e-06\n",
      "Step: 6180, train/learning_rate: 2.54878632404143e-05\n",
      "Step: 6180, train/epoch: 1.4707282781600952\n",
      "Step: 6190, train/loss: 0.0\n",
      "Step: 6190, train/grad_norm: 4.169956446276046e-05\n",
      "Step: 6190, train/learning_rate: 2.544820017646998e-05\n",
      "Step: 6190, train/epoch: 1.4731080532073975\n",
      "Step: 6200, train/loss: 0.0\n",
      "Step: 6200, train/grad_norm: 5.348278136807494e-05\n",
      "Step: 6200, train/learning_rate: 2.540853529353626e-05\n",
      "Step: 6200, train/epoch: 1.4754878282546997\n",
      "Step: 6210, train/loss: 0.0\n",
      "Step: 6210, train/grad_norm: 1.1602935956034344e-05\n",
      "Step: 6210, train/learning_rate: 2.5368872229591943e-05\n",
      "Step: 6210, train/epoch: 1.4778677225112915\n",
      "Step: 6220, train/loss: 0.0\n",
      "Step: 6220, train/grad_norm: 3.3603319025132805e-06\n",
      "Step: 6220, train/learning_rate: 2.5329209165647626e-05\n",
      "Step: 6220, train/epoch: 1.4802474975585938\n",
      "Step: 6230, train/loss: 0.0\n",
      "Step: 6230, train/grad_norm: 2.3059551494952757e-06\n",
      "Step: 6230, train/learning_rate: 2.5289544282713905e-05\n",
      "Step: 6230, train/epoch: 1.482627272605896\n",
      "Step: 6240, train/loss: 0.0\n",
      "Step: 6240, train/grad_norm: 1.6948903066804633e-05\n",
      "Step: 6240, train/learning_rate: 2.5249881218769588e-05\n",
      "Step: 6240, train/epoch: 1.4850071668624878\n",
      "Step: 6250, train/loss: 0.0\n",
      "Step: 6250, train/grad_norm: 1.1981616808043327e-05\n",
      "Step: 6250, train/learning_rate: 2.521021815482527e-05\n",
      "Step: 6250, train/epoch: 1.48738694190979\n",
      "Step: 6260, train/loss: 0.0\n",
      "Step: 6260, train/grad_norm: 5.2250648877816275e-05\n",
      "Step: 6260, train/learning_rate: 2.517055327189155e-05\n",
      "Step: 6260, train/epoch: 1.4897668361663818\n",
      "Step: 6270, train/loss: 0.0\n",
      "Step: 6270, train/grad_norm: 1.7824480892159045e-05\n",
      "Step: 6270, train/learning_rate: 2.5130890207947232e-05\n",
      "Step: 6270, train/epoch: 1.492146611213684\n",
      "Step: 6280, train/loss: 0.0\n",
      "Step: 6280, train/grad_norm: 7.761020242469385e-05\n",
      "Step: 6280, train/learning_rate: 2.5091227144002914e-05\n",
      "Step: 6280, train/epoch: 1.4945263862609863\n",
      "Step: 6290, train/loss: 0.0\n",
      "Step: 6290, train/grad_norm: 4.0767415043774236e-07\n",
      "Step: 6290, train/learning_rate: 2.5051562261069193e-05\n",
      "Step: 6290, train/epoch: 1.4969062805175781\n",
      "Step: 6300, train/loss: 0.0\n",
      "Step: 6300, train/grad_norm: 4.023466317448765e-06\n",
      "Step: 6300, train/learning_rate: 2.5011899197124876e-05\n",
      "Step: 6300, train/epoch: 1.4992860555648804\n",
      "Step: 6310, train/loss: 0.0\n",
      "Step: 6310, train/grad_norm: 1.7236583516933024e-05\n",
      "Step: 6310, train/learning_rate: 2.497223613318056e-05\n",
      "Step: 6310, train/epoch: 1.5016658306121826\n",
      "Step: 6320, train/loss: 0.0\n",
      "Step: 6320, train/grad_norm: 2.3399135898216628e-05\n",
      "Step: 6320, train/learning_rate: 2.4932571250246838e-05\n",
      "Step: 6320, train/epoch: 1.5040457248687744\n",
      "Step: 6330, train/loss: 0.0\n",
      "Step: 6330, train/grad_norm: 1.2368554962449707e-05\n",
      "Step: 6330, train/learning_rate: 2.489290818630252e-05\n",
      "Step: 6330, train/epoch: 1.5064254999160767\n",
      "Step: 6340, train/loss: 0.0\n",
      "Step: 6340, train/grad_norm: 4.498373164096847e-05\n",
      "Step: 6340, train/learning_rate: 2.4853245122358203e-05\n",
      "Step: 6340, train/epoch: 1.508805274963379\n",
      "Step: 6350, train/loss: 0.0\n",
      "Step: 6350, train/grad_norm: 1.7660217054071836e-05\n",
      "Step: 6350, train/learning_rate: 2.4813580239424482e-05\n",
      "Step: 6350, train/epoch: 1.5111851692199707\n",
      "Step: 6360, train/loss: 0.0\n",
      "Step: 6360, train/grad_norm: 8.051144686760381e-05\n",
      "Step: 6360, train/learning_rate: 2.4773917175480165e-05\n",
      "Step: 6360, train/epoch: 1.513564944267273\n",
      "Step: 6370, train/loss: 0.0\n",
      "Step: 6370, train/grad_norm: 2.7527727070264518e-05\n",
      "Step: 6370, train/learning_rate: 2.4734254111535847e-05\n",
      "Step: 6370, train/epoch: 1.5159448385238647\n",
      "Step: 6380, train/loss: 0.0\n",
      "Step: 6380, train/grad_norm: 1.2372981473163236e-05\n",
      "Step: 6380, train/learning_rate: 2.4694589228602126e-05\n",
      "Step: 6380, train/epoch: 1.518324613571167\n",
      "Step: 6390, train/loss: 0.0\n",
      "Step: 6390, train/grad_norm: 0.0016468253452330828\n",
      "Step: 6390, train/learning_rate: 2.465492616465781e-05\n",
      "Step: 6390, train/epoch: 1.5207043886184692\n",
      "Step: 6400, train/loss: 0.0\n",
      "Step: 6400, train/grad_norm: 1.885696292447392e-05\n",
      "Step: 6400, train/learning_rate: 2.461526310071349e-05\n",
      "Step: 6400, train/epoch: 1.523084282875061\n",
      "Step: 6410, train/loss: 0.0\n",
      "Step: 6410, train/grad_norm: 7.204394023574423e-06\n",
      "Step: 6410, train/learning_rate: 2.457559821777977e-05\n",
      "Step: 6410, train/epoch: 1.5254640579223633\n",
      "Step: 6420, train/loss: 0.0\n",
      "Step: 6420, train/grad_norm: 5.12756344051013e-07\n",
      "Step: 6420, train/learning_rate: 2.4535935153835453e-05\n",
      "Step: 6420, train/epoch: 1.5278438329696655\n",
      "Step: 6430, train/loss: 0.0\n",
      "Step: 6430, train/grad_norm: 8.431898095295765e-06\n",
      "Step: 6430, train/learning_rate: 2.4496272089891136e-05\n",
      "Step: 6430, train/epoch: 1.5302237272262573\n",
      "Step: 6440, train/loss: 0.0\n",
      "Step: 6440, train/grad_norm: 7.637510861968622e-06\n",
      "Step: 6440, train/learning_rate: 2.4456607206957415e-05\n",
      "Step: 6440, train/epoch: 1.5326035022735596\n",
      "Step: 6450, train/loss: 0.0\n",
      "Step: 6450, train/grad_norm: 2.1695950636058114e-05\n",
      "Step: 6450, train/learning_rate: 2.4416944143013097e-05\n",
      "Step: 6450, train/epoch: 1.5349833965301514\n",
      "Step: 6460, train/loss: 0.0\n",
      "Step: 6460, train/grad_norm: 8.425301530223805e-06\n",
      "Step: 6460, train/learning_rate: 2.437728107906878e-05\n",
      "Step: 6460, train/epoch: 1.5373631715774536\n",
      "Step: 6470, train/loss: 0.0\n",
      "Step: 6470, train/grad_norm: 0.00010742941231001168\n",
      "Step: 6470, train/learning_rate: 2.433761619613506e-05\n",
      "Step: 6470, train/epoch: 1.5397429466247559\n",
      "Step: 6480, train/loss: 0.0\n",
      "Step: 6480, train/grad_norm: 1.4292138530436205e-06\n",
      "Step: 6480, train/learning_rate: 2.429795313219074e-05\n",
      "Step: 6480, train/epoch: 1.5421228408813477\n",
      "Step: 6490, train/loss: 0.0\n",
      "Step: 6490, train/grad_norm: 1.1281945262453519e-05\n",
      "Step: 6490, train/learning_rate: 2.4258290068246424e-05\n",
      "Step: 6490, train/epoch: 1.54450261592865\n",
      "Step: 6500, train/loss: 0.0\n",
      "Step: 6500, train/grad_norm: 0.0004095971817150712\n",
      "Step: 6500, train/learning_rate: 2.4218625185312703e-05\n",
      "Step: 6500, train/epoch: 1.5468823909759521\n",
      "Step: 6510, train/loss: 0.0\n",
      "Step: 6510, train/grad_norm: 7.044959147606278e-06\n",
      "Step: 6510, train/learning_rate: 2.4178962121368386e-05\n",
      "Step: 6510, train/epoch: 1.549262285232544\n",
      "Step: 6520, train/loss: 0.0\n",
      "Step: 6520, train/grad_norm: 3.672557431855239e-05\n",
      "Step: 6520, train/learning_rate: 2.413929905742407e-05\n",
      "Step: 6520, train/epoch: 1.5516420602798462\n",
      "Step: 6530, train/loss: 0.0\n",
      "Step: 6530, train/grad_norm: 7.41082985769026e-05\n",
      "Step: 6530, train/learning_rate: 2.409963599347975e-05\n",
      "Step: 6530, train/epoch: 1.5540218353271484\n",
      "Step: 6540, train/loss: 0.0\n",
      "Step: 6540, train/grad_norm: 2.8353958896332188e-06\n",
      "Step: 6540, train/learning_rate: 2.405997111054603e-05\n",
      "Step: 6540, train/epoch: 1.5564017295837402\n",
      "Step: 6550, train/loss: 0.0\n",
      "Step: 6550, train/grad_norm: 3.71191134718174e-07\n",
      "Step: 6550, train/learning_rate: 2.4020308046601713e-05\n",
      "Step: 6550, train/epoch: 1.5587815046310425\n",
      "Step: 6560, train/loss: 0.0\n",
      "Step: 6560, train/grad_norm: 5.727446477976628e-06\n",
      "Step: 6560, train/learning_rate: 2.3980644982657395e-05\n",
      "Step: 6560, train/epoch: 1.5611613988876343\n",
      "Step: 6570, train/loss: 0.0\n",
      "Step: 6570, train/grad_norm: 4.869500571658136e-06\n",
      "Step: 6570, train/learning_rate: 2.3940980099723674e-05\n",
      "Step: 6570, train/epoch: 1.5635411739349365\n",
      "Step: 6580, train/loss: 0.0\n",
      "Step: 6580, train/grad_norm: 1.768184120010119e-05\n",
      "Step: 6580, train/learning_rate: 2.3901317035779357e-05\n",
      "Step: 6580, train/epoch: 1.5659209489822388\n",
      "Step: 6590, train/loss: 0.0\n",
      "Step: 6590, train/grad_norm: 2.132149575118092e-06\n",
      "Step: 6590, train/learning_rate: 2.386165397183504e-05\n",
      "Step: 6590, train/epoch: 1.5683008432388306\n",
      "Step: 6600, train/loss: 0.0\n",
      "Step: 6600, train/grad_norm: 5.11742564412998e-06\n",
      "Step: 6600, train/learning_rate: 2.382198908890132e-05\n",
      "Step: 6600, train/epoch: 1.5706806182861328\n",
      "Step: 6610, train/loss: 0.0\n",
      "Step: 6610, train/grad_norm: 6.096202014305163e-06\n",
      "Step: 6610, train/learning_rate: 2.3782326024957e-05\n",
      "Step: 6610, train/epoch: 1.573060393333435\n",
      "Step: 6620, train/loss: 0.0\n",
      "Step: 6620, train/grad_norm: 0.06477926671504974\n",
      "Step: 6620, train/learning_rate: 2.3742662961012684e-05\n",
      "Step: 6620, train/epoch: 1.5754402875900269\n",
      "Step: 6630, train/loss: 0.0\n",
      "Step: 6630, train/grad_norm: 6.740334356436506e-06\n",
      "Step: 6630, train/learning_rate: 2.3702998078078963e-05\n",
      "Step: 6630, train/epoch: 1.577820062637329\n",
      "Step: 6640, train/loss: 0.0\n",
      "Step: 6640, train/grad_norm: 8.830364095047116e-06\n",
      "Step: 6640, train/learning_rate: 2.3663335014134645e-05\n",
      "Step: 6640, train/epoch: 1.580199956893921\n",
      "Step: 6650, train/loss: 0.0\n",
      "Step: 6650, train/grad_norm: 1.3430363310362736e-07\n",
      "Step: 6650, train/learning_rate: 2.3623671950190328e-05\n",
      "Step: 6650, train/epoch: 1.5825797319412231\n",
      "Step: 6660, train/loss: 0.0\n",
      "Step: 6660, train/grad_norm: 5.164923641132191e-05\n",
      "Step: 6660, train/learning_rate: 2.3584007067256607e-05\n",
      "Step: 6660, train/epoch: 1.5849595069885254\n",
      "Step: 6670, train/loss: 0.0\n",
      "Step: 6670, train/grad_norm: 7.076086603774456e-06\n",
      "Step: 6670, train/learning_rate: 2.354434400331229e-05\n",
      "Step: 6670, train/epoch: 1.5873394012451172\n",
      "Step: 6680, train/loss: 0.0\n",
      "Step: 6680, train/grad_norm: 2.955573791041388e-06\n",
      "Step: 6680, train/learning_rate: 2.3504680939367972e-05\n",
      "Step: 6680, train/epoch: 1.5897191762924194\n",
      "Step: 6690, train/loss: 0.0007999999797903001\n",
      "Step: 6690, train/grad_norm: 1.096960659197066e-06\n",
      "Step: 6690, train/learning_rate: 2.346501605643425e-05\n",
      "Step: 6690, train/epoch: 1.5920989513397217\n",
      "Step: 6700, train/loss: 0.0\n",
      "Step: 6700, train/grad_norm: 2.6866182452067733e-06\n",
      "Step: 6700, train/learning_rate: 2.3425352992489934e-05\n",
      "Step: 6700, train/epoch: 1.5944788455963135\n",
      "Step: 6710, train/loss: 0.0\n",
      "Step: 6710, train/grad_norm: 1.7347621906083077e-05\n",
      "Step: 6710, train/learning_rate: 2.3385689928545617e-05\n",
      "Step: 6710, train/epoch: 1.5968586206436157\n",
      "Step: 6720, train/loss: 0.17030000686645508\n",
      "Step: 6720, train/grad_norm: 27.237638473510742\n",
      "Step: 6720, train/learning_rate: 2.3346025045611896e-05\n",
      "Step: 6720, train/epoch: 1.5992385149002075\n",
      "Step: 6730, train/loss: 0.0\n",
      "Step: 6730, train/grad_norm: 0.0015388624742627144\n",
      "Step: 6730, train/learning_rate: 2.3306361981667578e-05\n",
      "Step: 6730, train/epoch: 1.6016182899475098\n",
      "Step: 6740, train/loss: 9.999999747378752e-05\n",
      "Step: 6740, train/grad_norm: 0.03734290599822998\n",
      "Step: 6740, train/learning_rate: 2.326669891772326e-05\n",
      "Step: 6740, train/epoch: 1.603998064994812\n",
      "Step: 6750, train/loss: 9.999999747378752e-05\n",
      "Step: 6750, train/grad_norm: 0.010669565759599209\n",
      "Step: 6750, train/learning_rate: 2.322703403478954e-05\n",
      "Step: 6750, train/epoch: 1.6063779592514038\n",
      "Step: 6760, train/loss: 0.0017999999690800905\n",
      "Step: 6760, train/grad_norm: 0.0006175402668304741\n",
      "Step: 6760, train/learning_rate: 2.3187370970845222e-05\n",
      "Step: 6760, train/epoch: 1.608757734298706\n",
      "Step: 6770, train/loss: 0.0\n",
      "Step: 6770, train/grad_norm: 0.0012158555909991264\n",
      "Step: 6770, train/learning_rate: 2.3147707906900905e-05\n",
      "Step: 6770, train/epoch: 1.6111375093460083\n",
      "Step: 6780, train/loss: 0.0\n",
      "Step: 6780, train/grad_norm: 4.0351405914407223e-05\n",
      "Step: 6780, train/learning_rate: 2.3108043023967184e-05\n",
      "Step: 6780, train/epoch: 1.6135174036026\n",
      "Step: 6790, train/loss: 0.0\n",
      "Step: 6790, train/grad_norm: 1.0700668099161703e-05\n",
      "Step: 6790, train/learning_rate: 2.3068379960022867e-05\n",
      "Step: 6790, train/epoch: 1.6158971786499023\n",
      "Step: 6800, train/loss: 0.0\n",
      "Step: 6800, train/grad_norm: 0.0001886264217318967\n",
      "Step: 6800, train/learning_rate: 2.302871689607855e-05\n",
      "Step: 6800, train/epoch: 1.6182769536972046\n",
      "Step: 6810, train/loss: 0.0\n",
      "Step: 6810, train/grad_norm: 0.00010321333684260026\n",
      "Step: 6810, train/learning_rate: 2.298905201314483e-05\n",
      "Step: 6810, train/epoch: 1.6206568479537964\n",
      "Step: 6820, train/loss: 0.20469999313354492\n",
      "Step: 6820, train/grad_norm: 27.918066024780273\n",
      "Step: 6820, train/learning_rate: 2.294938894920051e-05\n",
      "Step: 6820, train/epoch: 1.6230366230010986\n",
      "Step: 6830, train/loss: 0.0\n",
      "Step: 6830, train/grad_norm: 0.00036411354085430503\n",
      "Step: 6830, train/learning_rate: 2.2909725885256194e-05\n",
      "Step: 6830, train/epoch: 1.6254165172576904\n",
      "Step: 6840, train/loss: 0.0\n",
      "Step: 6840, train/grad_norm: 0.010890047997236252\n",
      "Step: 6840, train/learning_rate: 2.2870061002322473e-05\n",
      "Step: 6840, train/epoch: 1.6277962923049927\n",
      "Step: 6850, train/loss: 0.0\n",
      "Step: 6850, train/grad_norm: 0.002251389902085066\n",
      "Step: 6850, train/learning_rate: 2.2830397938378155e-05\n",
      "Step: 6850, train/epoch: 1.630176067352295\n",
      "Step: 6860, train/loss: 0.0\n",
      "Step: 6860, train/grad_norm: 0.003540176199749112\n",
      "Step: 6860, train/learning_rate: 2.2790734874433838e-05\n",
      "Step: 6860, train/epoch: 1.6325559616088867\n",
      "Step: 6870, train/loss: 0.0\n",
      "Step: 6870, train/grad_norm: 0.00038241318543441594\n",
      "Step: 6870, train/learning_rate: 2.275107181048952e-05\n",
      "Step: 6870, train/epoch: 1.634935736656189\n",
      "Step: 6880, train/loss: 0.0\n",
      "Step: 6880, train/grad_norm: 0.0003568853426259011\n",
      "Step: 6880, train/learning_rate: 2.27114069275558e-05\n",
      "Step: 6880, train/epoch: 1.6373155117034912\n",
      "Step: 6890, train/loss: 0.0\n",
      "Step: 6890, train/grad_norm: 0.0019221663242205977\n",
      "Step: 6890, train/learning_rate: 2.2671743863611482e-05\n",
      "Step: 6890, train/epoch: 1.639695405960083\n",
      "Step: 6900, train/loss: 0.0\n",
      "Step: 6900, train/grad_norm: 0.005362898111343384\n",
      "Step: 6900, train/learning_rate: 2.2632080799667165e-05\n",
      "Step: 6900, train/epoch: 1.6420751810073853\n",
      "Step: 6910, train/loss: 0.0\n",
      "Step: 6910, train/grad_norm: 2.891673648264259e-05\n",
      "Step: 6910, train/learning_rate: 2.2592415916733444e-05\n",
      "Step: 6910, train/epoch: 1.644455075263977\n",
      "Step: 6920, train/loss: 0.0\n",
      "Step: 6920, train/grad_norm: 0.0028658832889050245\n",
      "Step: 6920, train/learning_rate: 2.2552752852789126e-05\n",
      "Step: 6920, train/epoch: 1.6468348503112793\n",
      "Step: 6930, train/loss: 0.0\n",
      "Step: 6930, train/grad_norm: 0.0006558407912962139\n",
      "Step: 6930, train/learning_rate: 2.251308978884481e-05\n",
      "Step: 6930, train/epoch: 1.6492146253585815\n",
      "Step: 6940, train/loss: 0.0\n",
      "Step: 6940, train/grad_norm: 0.00016509287524968386\n",
      "Step: 6940, train/learning_rate: 2.2473424905911088e-05\n",
      "Step: 6940, train/epoch: 1.6515945196151733\n",
      "Step: 6950, train/loss: 0.0\n",
      "Step: 6950, train/grad_norm: 0.0005544467130675912\n",
      "Step: 6950, train/learning_rate: 2.243376184196677e-05\n",
      "Step: 6950, train/epoch: 1.6539742946624756\n",
      "Step: 6960, train/loss: 0.041999999433755875\n",
      "Step: 6960, train/grad_norm: 0.0019631031900644302\n",
      "Step: 6960, train/learning_rate: 2.2394098778022453e-05\n",
      "Step: 6960, train/epoch: 1.6563540697097778\n",
      "Step: 6970, train/loss: 0.04439999908208847\n",
      "Step: 6970, train/grad_norm: 0.00786555651575327\n",
      "Step: 6970, train/learning_rate: 2.2354433895088732e-05\n",
      "Step: 6970, train/epoch: 1.6587339639663696\n",
      "Step: 6980, train/loss: 0.0\n",
      "Step: 6980, train/grad_norm: 0.010825878009200096\n",
      "Step: 6980, train/learning_rate: 2.2314770831144415e-05\n",
      "Step: 6980, train/epoch: 1.6611137390136719\n",
      "Step: 6990, train/loss: 0.0\n",
      "Step: 6990, train/grad_norm: 0.0018998092273250222\n",
      "Step: 6990, train/learning_rate: 2.2275107767200097e-05\n",
      "Step: 6990, train/epoch: 1.6634936332702637\n",
      "Step: 7000, train/loss: 9.999999747378752e-05\n",
      "Step: 7000, train/grad_norm: 0.0002478361129760742\n",
      "Step: 7000, train/learning_rate: 2.2235442884266376e-05\n",
      "Step: 7000, train/epoch: 1.665873408317566\n",
      "Step: 7010, train/loss: 0.0\n",
      "Step: 7010, train/grad_norm: 0.002788243582472205\n",
      "Step: 7010, train/learning_rate: 2.219577982032206e-05\n",
      "Step: 7010, train/epoch: 1.6682531833648682\n",
      "Step: 7020, train/loss: 0.005799999926239252\n",
      "Step: 7020, train/grad_norm: 0.001298596616834402\n",
      "Step: 7020, train/learning_rate: 2.2156116756377742e-05\n",
      "Step: 7020, train/epoch: 1.67063307762146\n",
      "Step: 7030, train/loss: 0.0\n",
      "Step: 7030, train/grad_norm: 0.0006828157347626984\n",
      "Step: 7030, train/learning_rate: 2.211645187344402e-05\n",
      "Step: 7030, train/epoch: 1.6730128526687622\n",
      "Step: 7040, train/loss: 0.008999999612569809\n",
      "Step: 7040, train/grad_norm: 0.05136638134717941\n",
      "Step: 7040, train/learning_rate: 2.2076788809499703e-05\n",
      "Step: 7040, train/epoch: 1.6753926277160645\n",
      "Step: 7050, train/loss: 0.0\n",
      "Step: 7050, train/grad_norm: 0.002922255313023925\n",
      "Step: 7050, train/learning_rate: 2.2037125745555386e-05\n",
      "Step: 7050, train/epoch: 1.6777725219726562\n",
      "Step: 7060, train/loss: 0.0\n",
      "Step: 7060, train/grad_norm: 0.00010557781206443906\n",
      "Step: 7060, train/learning_rate: 2.1997460862621665e-05\n",
      "Step: 7060, train/epoch: 1.6801522970199585\n",
      "Step: 7070, train/loss: 0.0\n",
      "Step: 7070, train/grad_norm: 0.0005545548629015684\n",
      "Step: 7070, train/learning_rate: 2.1957797798677348e-05\n",
      "Step: 7070, train/epoch: 1.6825320720672607\n",
      "Step: 7080, train/loss: 0.0\n",
      "Step: 7080, train/grad_norm: 0.0002569171483628452\n",
      "Step: 7080, train/learning_rate: 2.191813473473303e-05\n",
      "Step: 7080, train/epoch: 1.6849119663238525\n",
      "Step: 7090, train/loss: 0.0\n",
      "Step: 7090, train/grad_norm: 0.000162983633344993\n",
      "Step: 7090, train/learning_rate: 2.187846985179931e-05\n",
      "Step: 7090, train/epoch: 1.6872917413711548\n",
      "Step: 7100, train/loss: 0.0\n",
      "Step: 7100, train/grad_norm: 2.1624295186484233e-05\n",
      "Step: 7100, train/learning_rate: 2.1838806787854992e-05\n",
      "Step: 7100, train/epoch: 1.6896716356277466\n",
      "Step: 7110, train/loss: 0.0\n",
      "Step: 7110, train/grad_norm: 0.0010687876492738724\n",
      "Step: 7110, train/learning_rate: 2.1799143723910674e-05\n",
      "Step: 7110, train/epoch: 1.6920514106750488\n",
      "Step: 7120, train/loss: 0.0\n",
      "Step: 7120, train/grad_norm: 0.0009759802487678826\n",
      "Step: 7120, train/learning_rate: 2.1759478840976954e-05\n",
      "Step: 7120, train/epoch: 1.694431185722351\n",
      "Step: 7130, train/loss: 0.0\n",
      "Step: 7130, train/grad_norm: 0.0004756074049510062\n",
      "Step: 7130, train/learning_rate: 2.1719815777032636e-05\n",
      "Step: 7130, train/epoch: 1.6968110799789429\n",
      "Step: 7140, train/loss: 0.0\n",
      "Step: 7140, train/grad_norm: 0.0003297829534858465\n",
      "Step: 7140, train/learning_rate: 2.168015271308832e-05\n",
      "Step: 7140, train/epoch: 1.6991908550262451\n",
      "Step: 7150, train/loss: 0.0\n",
      "Step: 7150, train/grad_norm: 0.00027128192596137524\n",
      "Step: 7150, train/learning_rate: 2.1640487830154598e-05\n",
      "Step: 7150, train/epoch: 1.7015706300735474\n",
      "Step: 7160, train/loss: 0.0\n",
      "Step: 7160, train/grad_norm: 0.019047662615776062\n",
      "Step: 7160, train/learning_rate: 2.160082476621028e-05\n",
      "Step: 7160, train/epoch: 1.7039505243301392\n",
      "Step: 7170, train/loss: 0.0\n",
      "Step: 7170, train/grad_norm: 0.0003470915253274143\n",
      "Step: 7170, train/learning_rate: 2.1561161702265963e-05\n",
      "Step: 7170, train/epoch: 1.7063302993774414\n",
      "Step: 7180, train/loss: 0.0\n",
      "Step: 7180, train/grad_norm: 0.00037898647133260965\n",
      "Step: 7180, train/learning_rate: 2.1521496819332242e-05\n",
      "Step: 7180, train/epoch: 1.7087101936340332\n",
      "Step: 7190, train/loss: 9.999999747378752e-05\n",
      "Step: 7190, train/grad_norm: 0.00023451105516869575\n",
      "Step: 7190, train/learning_rate: 2.1481833755387925e-05\n",
      "Step: 7190, train/epoch: 1.7110899686813354\n",
      "Step: 7200, train/loss: 0.0\n",
      "Step: 7200, train/grad_norm: 0.00021045669564045966\n",
      "Step: 7200, train/learning_rate: 2.1442170691443607e-05\n",
      "Step: 7200, train/epoch: 1.7134697437286377\n",
      "Step: 7210, train/loss: 0.0\n",
      "Step: 7210, train/grad_norm: 0.04506479203701019\n",
      "Step: 7210, train/learning_rate: 2.140250762749929e-05\n",
      "Step: 7210, train/epoch: 1.7158496379852295\n",
      "Step: 7220, train/loss: 0.0\n",
      "Step: 7220, train/grad_norm: 1.690777025942225e-05\n",
      "Step: 7220, train/learning_rate: 2.136284274456557e-05\n",
      "Step: 7220, train/epoch: 1.7182294130325317\n",
      "Step: 7230, train/loss: 0.0\n",
      "Step: 7230, train/grad_norm: 9.785930888028815e-05\n",
      "Step: 7230, train/learning_rate: 2.132317968062125e-05\n",
      "Step: 7230, train/epoch: 1.720609188079834\n",
      "Step: 7240, train/loss: 0.0\n",
      "Step: 7240, train/grad_norm: 3.657868001027964e-05\n",
      "Step: 7240, train/learning_rate: 2.1283516616676934e-05\n",
      "Step: 7240, train/epoch: 1.7229890823364258\n",
      "Step: 7250, train/loss: 0.0\n",
      "Step: 7250, train/grad_norm: 3.3877917303470895e-05\n",
      "Step: 7250, train/learning_rate: 2.1243851733743213e-05\n",
      "Step: 7250, train/epoch: 1.725368857383728\n",
      "Step: 7260, train/loss: 0.0\n",
      "Step: 7260, train/grad_norm: 4.987566717318259e-05\n",
      "Step: 7260, train/learning_rate: 2.1204188669798896e-05\n",
      "Step: 7260, train/epoch: 1.7277486324310303\n",
      "Step: 7270, train/loss: 0.0\n",
      "Step: 7270, train/grad_norm: 4.843510396312922e-05\n",
      "Step: 7270, train/learning_rate: 2.116452560585458e-05\n",
      "Step: 7270, train/epoch: 1.730128526687622\n",
      "Step: 7280, train/loss: 0.0940999984741211\n",
      "Step: 7280, train/grad_norm: 27.246248245239258\n",
      "Step: 7280, train/learning_rate: 2.1124860722920857e-05\n",
      "Step: 7280, train/epoch: 1.7325083017349243\n",
      "Step: 7290, train/loss: 0.0\n",
      "Step: 7290, train/grad_norm: 0.00010558468056842685\n",
      "Step: 7290, train/learning_rate: 2.108519765897654e-05\n",
      "Step: 7290, train/epoch: 1.7348881959915161\n",
      "Step: 7300, train/loss: 0.0\n",
      "Step: 7300, train/grad_norm: 0.0001637542009120807\n",
      "Step: 7300, train/learning_rate: 2.1045534595032223e-05\n",
      "Step: 7300, train/epoch: 1.7372679710388184\n",
      "Step: 7310, train/loss: 0.0\n",
      "Step: 7310, train/grad_norm: 5.324429366737604e-05\n",
      "Step: 7310, train/learning_rate: 2.10058697120985e-05\n",
      "Step: 7310, train/epoch: 1.7396477460861206\n",
      "Step: 7320, train/loss: 0.08789999783039093\n",
      "Step: 7320, train/grad_norm: 0.0001758778962539509\n",
      "Step: 7320, train/learning_rate: 2.0966206648154184e-05\n",
      "Step: 7320, train/epoch: 1.7420276403427124\n",
      "Step: 7330, train/loss: 0.0\n",
      "Step: 7330, train/grad_norm: 0.0007605620194226503\n",
      "Step: 7330, train/learning_rate: 2.0926543584209867e-05\n",
      "Step: 7330, train/epoch: 1.7444074153900146\n",
      "Step: 7340, train/loss: 0.0\n",
      "Step: 7340, train/grad_norm: 0.021704545244574547\n",
      "Step: 7340, train/learning_rate: 2.0886878701276146e-05\n",
      "Step: 7340, train/epoch: 1.746787190437317\n",
      "Step: 7350, train/loss: 0.0\n",
      "Step: 7350, train/grad_norm: 0.0001153488046838902\n",
      "Step: 7350, train/learning_rate: 2.084721563733183e-05\n",
      "Step: 7350, train/epoch: 1.7491670846939087\n",
      "Step: 7360, train/loss: 9.999999747378752e-05\n",
      "Step: 7360, train/grad_norm: 0.00026712188264355063\n",
      "Step: 7360, train/learning_rate: 2.080755257338751e-05\n",
      "Step: 7360, train/epoch: 1.751546859741211\n",
      "Step: 7370, train/loss: 0.0\n",
      "Step: 7370, train/grad_norm: 6.251106242416427e-05\n",
      "Step: 7370, train/learning_rate: 2.076788769045379e-05\n",
      "Step: 7370, train/epoch: 1.7539267539978027\n",
      "Step: 7380, train/loss: 0.0\n",
      "Step: 7380, train/grad_norm: 6.510222010547295e-05\n",
      "Step: 7380, train/learning_rate: 2.0728224626509473e-05\n",
      "Step: 7380, train/epoch: 1.756306529045105\n",
      "Step: 7390, train/loss: 0.0\n",
      "Step: 7390, train/grad_norm: 8.391339360969141e-05\n",
      "Step: 7390, train/learning_rate: 2.0688561562565155e-05\n",
      "Step: 7390, train/epoch: 1.7586863040924072\n",
      "Step: 7400, train/loss: 0.0\n",
      "Step: 7400, train/grad_norm: 0.000776324886828661\n",
      "Step: 7400, train/learning_rate: 2.0648896679631434e-05\n",
      "Step: 7400, train/epoch: 1.761066198348999\n",
      "Step: 7410, train/loss: 0.0\n",
      "Step: 7410, train/grad_norm: 0.00040600463398732245\n",
      "Step: 7410, train/learning_rate: 2.0609233615687117e-05\n",
      "Step: 7410, train/epoch: 1.7634459733963013\n",
      "Step: 7420, train/loss: 0.0\n",
      "Step: 7420, train/grad_norm: 0.0003334213397465646\n",
      "Step: 7420, train/learning_rate: 2.05695705517428e-05\n",
      "Step: 7420, train/epoch: 1.7658257484436035\n",
      "Step: 7430, train/loss: 0.0\n",
      "Step: 7430, train/grad_norm: 0.00027903076261281967\n",
      "Step: 7430, train/learning_rate: 2.052990566880908e-05\n",
      "Step: 7430, train/epoch: 1.7682056427001953\n",
      "Step: 7440, train/loss: 0.0\n",
      "Step: 7440, train/grad_norm: 0.00040650003938935697\n",
      "Step: 7440, train/learning_rate: 2.049024260486476e-05\n",
      "Step: 7440, train/epoch: 1.7705854177474976\n",
      "Step: 7450, train/loss: 0.0\n",
      "Step: 7450, train/grad_norm: 0.00045930646592751145\n",
      "Step: 7450, train/learning_rate: 2.0450579540920444e-05\n",
      "Step: 7450, train/epoch: 1.7729653120040894\n",
      "Step: 7460, train/loss: 0.0\n",
      "Step: 7460, train/grad_norm: 2.8493403078755364e-05\n",
      "Step: 7460, train/learning_rate: 2.0410914657986723e-05\n",
      "Step: 7460, train/epoch: 1.7753450870513916\n",
      "Step: 7470, train/loss: 0.0\n",
      "Step: 7470, train/grad_norm: 0.00021360818936955184\n",
      "Step: 7470, train/learning_rate: 2.0371251594042405e-05\n",
      "Step: 7470, train/epoch: 1.7777248620986938\n",
      "Step: 7480, train/loss: 0.0\n",
      "Step: 7480, train/grad_norm: 6.155878509161994e-05\n",
      "Step: 7480, train/learning_rate: 2.0331588530098088e-05\n",
      "Step: 7480, train/epoch: 1.7801047563552856\n",
      "Step: 7490, train/loss: 0.0\n",
      "Step: 7490, train/grad_norm: 0.00022664174321107566\n",
      "Step: 7490, train/learning_rate: 2.0291923647164367e-05\n",
      "Step: 7490, train/epoch: 1.782484531402588\n",
      "Step: 7500, train/loss: 0.0\n",
      "Step: 7500, train/grad_norm: 9.513310214970261e-05\n",
      "Step: 7500, train/learning_rate: 2.025226058322005e-05\n",
      "Step: 7500, train/epoch: 1.7848643064498901\n",
      "Step: 7510, train/loss: 0.0\n",
      "Step: 7510, train/grad_norm: 0.0012031212681904435\n",
      "Step: 7510, train/learning_rate: 2.0212597519275732e-05\n",
      "Step: 7510, train/epoch: 1.787244200706482\n",
      "Step: 7520, train/loss: 0.0\n",
      "Step: 7520, train/grad_norm: 0.000714106485247612\n",
      "Step: 7520, train/learning_rate: 2.017293263634201e-05\n",
      "Step: 7520, train/epoch: 1.7896239757537842\n",
      "Step: 7530, train/loss: 0.09300000220537186\n",
      "Step: 7530, train/grad_norm: 0.001589374034665525\n",
      "Step: 7530, train/learning_rate: 2.0133269572397694e-05\n",
      "Step: 7530, train/epoch: 1.7920037508010864\n",
      "Step: 7540, train/loss: 0.0\n",
      "Step: 7540, train/grad_norm: 5.984208655718248e-06\n",
      "Step: 7540, train/learning_rate: 2.0093606508453377e-05\n",
      "Step: 7540, train/epoch: 1.7943836450576782\n",
      "Step: 7550, train/loss: 0.0\n",
      "Step: 7550, train/grad_norm: 3.886467675329186e-05\n",
      "Step: 7550, train/learning_rate: 2.005394344450906e-05\n",
      "Step: 7550, train/epoch: 1.7967634201049805\n",
      "Step: 7560, train/loss: 0.0\n",
      "Step: 7560, train/grad_norm: 1.28500951177557e-05\n",
      "Step: 7560, train/learning_rate: 2.0014278561575338e-05\n",
      "Step: 7560, train/epoch: 1.7991433143615723\n",
      "Step: 7570, train/loss: 0.0\n",
      "Step: 7570, train/grad_norm: 8.465453720418736e-05\n",
      "Step: 7570, train/learning_rate: 1.997461549763102e-05\n",
      "Step: 7570, train/epoch: 1.8015230894088745\n",
      "Step: 7580, train/loss: 0.0\n",
      "Step: 7580, train/grad_norm: 0.0006326987640932202\n",
      "Step: 7580, train/learning_rate: 1.9934952433686703e-05\n",
      "Step: 7580, train/epoch: 1.8039028644561768\n",
      "Step: 7590, train/loss: 0.0\n",
      "Step: 7590, train/grad_norm: 2.1050274881417863e-05\n",
      "Step: 7590, train/learning_rate: 1.9895287550752982e-05\n",
      "Step: 7590, train/epoch: 1.8062827587127686\n",
      "Step: 7600, train/loss: 0.0\n",
      "Step: 7600, train/grad_norm: 7.444454240612686e-05\n",
      "Step: 7600, train/learning_rate: 1.9855624486808665e-05\n",
      "Step: 7600, train/epoch: 1.8086625337600708\n",
      "Step: 7610, train/loss: 0.0\n",
      "Step: 7610, train/grad_norm: 3.731955439434387e-05\n",
      "Step: 7610, train/learning_rate: 1.9815961422864348e-05\n",
      "Step: 7610, train/epoch: 1.811042308807373\n",
      "Step: 7620, train/loss: 0.0\n",
      "Step: 7620, train/grad_norm: 0.0004472129512578249\n",
      "Step: 7620, train/learning_rate: 1.9776296539930627e-05\n",
      "Step: 7620, train/epoch: 1.8134222030639648\n",
      "Step: 7630, train/loss: 0.0\n",
      "Step: 7630, train/grad_norm: 4.4277418055571616e-05\n",
      "Step: 7630, train/learning_rate: 1.973663347598631e-05\n",
      "Step: 7630, train/epoch: 1.815801978111267\n",
      "Step: 7640, train/loss: 0.03480000048875809\n",
      "Step: 7640, train/grad_norm: 7.744932372588664e-05\n",
      "Step: 7640, train/learning_rate: 1.9696970412041992e-05\n",
      "Step: 7640, train/epoch: 1.8181818723678589\n",
      "Step: 7650, train/loss: 0.0\n",
      "Step: 7650, train/grad_norm: 5.5995733418967575e-05\n",
      "Step: 7650, train/learning_rate: 1.965730552910827e-05\n",
      "Step: 7650, train/epoch: 1.8205616474151611\n",
      "Step: 7660, train/loss: 0.0\n",
      "Step: 7660, train/grad_norm: 0.0001997390645556152\n",
      "Step: 7660, train/learning_rate: 1.9617642465163954e-05\n",
      "Step: 7660, train/epoch: 1.8229414224624634\n",
      "Step: 7670, train/loss: 0.0\n",
      "Step: 7670, train/grad_norm: 5.171909288037568e-05\n",
      "Step: 7670, train/learning_rate: 1.9577979401219636e-05\n",
      "Step: 7670, train/epoch: 1.8253213167190552\n",
      "Step: 7680, train/loss: 0.0\n",
      "Step: 7680, train/grad_norm: 0.0002642738400027156\n",
      "Step: 7680, train/learning_rate: 1.9538314518285915e-05\n",
      "Step: 7680, train/epoch: 1.8277010917663574\n",
      "Step: 7690, train/loss: 0.0\n",
      "Step: 7690, train/grad_norm: 4.481393989408389e-05\n",
      "Step: 7690, train/learning_rate: 1.9498651454341598e-05\n",
      "Step: 7690, train/epoch: 1.8300808668136597\n",
      "Step: 7700, train/loss: 0.0\n",
      "Step: 7700, train/grad_norm: 0.00020167918410152197\n",
      "Step: 7700, train/learning_rate: 1.945898839039728e-05\n",
      "Step: 7700, train/epoch: 1.8324607610702515\n",
      "Step: 7710, train/loss: 0.0\n",
      "Step: 7710, train/grad_norm: 0.00024119803856592625\n",
      "Step: 7710, train/learning_rate: 1.941932350746356e-05\n",
      "Step: 7710, train/epoch: 1.8348405361175537\n",
      "Step: 7720, train/loss: 0.0\n",
      "Step: 7720, train/grad_norm: 7.77764362283051e-05\n",
      "Step: 7720, train/learning_rate: 1.9379660443519242e-05\n",
      "Step: 7720, train/epoch: 1.8372204303741455\n",
      "Step: 7730, train/loss: 0.0\n",
      "Step: 7730, train/grad_norm: 3.67979118891526e-05\n",
      "Step: 7730, train/learning_rate: 1.9339997379574925e-05\n",
      "Step: 7730, train/epoch: 1.8396002054214478\n",
      "Step: 7740, train/loss: 0.0\n",
      "Step: 7740, train/grad_norm: 0.0002723258803598583\n",
      "Step: 7740, train/learning_rate: 1.9300332496641204e-05\n",
      "Step: 7740, train/epoch: 1.84197998046875\n",
      "Step: 7750, train/loss: 0.0\n",
      "Step: 7750, train/grad_norm: 0.00031414898694492877\n",
      "Step: 7750, train/learning_rate: 1.9260669432696886e-05\n",
      "Step: 7750, train/epoch: 1.8443598747253418\n",
      "Step: 7760, train/loss: 0.0\n",
      "Step: 7760, train/grad_norm: 0.000646717322524637\n",
      "Step: 7760, train/learning_rate: 1.922100636875257e-05\n",
      "Step: 7760, train/epoch: 1.846739649772644\n",
      "Step: 7770, train/loss: 0.0\n",
      "Step: 7770, train/grad_norm: 0.00013624093844555318\n",
      "Step: 7770, train/learning_rate: 1.9181341485818848e-05\n",
      "Step: 7770, train/epoch: 1.8491194248199463\n",
      "Step: 7780, train/loss: 0.0008999999845400453\n",
      "Step: 7780, train/grad_norm: 7.99575645942241e-05\n",
      "Step: 7780, train/learning_rate: 1.914167842187453e-05\n",
      "Step: 7780, train/epoch: 1.851499319076538\n",
      "Step: 7790, train/loss: 0.12890000641345978\n",
      "Step: 7790, train/grad_norm: 5.943313226453029e-06\n",
      "Step: 7790, train/learning_rate: 1.9102015357930213e-05\n",
      "Step: 7790, train/epoch: 1.8538790941238403\n",
      "Step: 7800, train/loss: 0.0\n",
      "Step: 7800, train/grad_norm: 1.8922850358649157e-05\n",
      "Step: 7800, train/learning_rate: 1.9062350474996492e-05\n",
      "Step: 7800, train/epoch: 1.8562588691711426\n",
      "Step: 7810, train/loss: 0.0\n",
      "Step: 7810, train/grad_norm: 8.043007255764678e-05\n",
      "Step: 7810, train/learning_rate: 1.9022687411052175e-05\n",
      "Step: 7810, train/epoch: 1.8586387634277344\n",
      "Step: 7820, train/loss: 0.03500000014901161\n",
      "Step: 7820, train/grad_norm: 3.3602882467675954e-05\n",
      "Step: 7820, train/learning_rate: 1.8983024347107857e-05\n",
      "Step: 7820, train/epoch: 1.8610185384750366\n",
      "Step: 7830, train/loss: 0.0007999999797903001\n",
      "Step: 7830, train/grad_norm: 4.5604610932059586e-05\n",
      "Step: 7830, train/learning_rate: 1.8943359464174137e-05\n",
      "Step: 7830, train/epoch: 1.8633984327316284\n",
      "Step: 7840, train/loss: 0.0\n",
      "Step: 7840, train/grad_norm: 4.785147393704392e-05\n",
      "Step: 7840, train/learning_rate: 1.890369640022982e-05\n",
      "Step: 7840, train/epoch: 1.8657782077789307\n",
      "Step: 7850, train/loss: 0.0\n",
      "Step: 7850, train/grad_norm: 3.285616685388959e-06\n",
      "Step: 7850, train/learning_rate: 1.8864033336285502e-05\n",
      "Step: 7850, train/epoch: 1.868157982826233\n",
      "Step: 7860, train/loss: 0.0\n",
      "Step: 7860, train/grad_norm: 5.9164478443562984e-05\n",
      "Step: 7860, train/learning_rate: 1.882436845335178e-05\n",
      "Step: 7860, train/epoch: 1.8705378770828247\n",
      "Step: 7870, train/loss: 0.0\n",
      "Step: 7870, train/grad_norm: 0.016031136736273766\n",
      "Step: 7870, train/learning_rate: 1.8784705389407463e-05\n",
      "Step: 7870, train/epoch: 1.872917652130127\n",
      "Step: 7880, train/loss: 0.0027000000700354576\n",
      "Step: 7880, train/grad_norm: 5.5495245760539547e-05\n",
      "Step: 7880, train/learning_rate: 1.8745042325463146e-05\n",
      "Step: 7880, train/epoch: 1.8752974271774292\n",
      "Step: 7890, train/loss: 0.00019999999494757503\n",
      "Step: 7890, train/grad_norm: 1.7257663785130717e-05\n",
      "Step: 7890, train/learning_rate: 1.870537926151883e-05\n",
      "Step: 7890, train/epoch: 1.877677321434021\n",
      "Step: 7900, train/loss: 0.0\n",
      "Step: 7900, train/grad_norm: 0.00020141503773629665\n",
      "Step: 7900, train/learning_rate: 1.8665714378585108e-05\n",
      "Step: 7900, train/epoch: 1.8800570964813232\n",
      "Step: 7910, train/loss: 0.0026000000070780516\n",
      "Step: 7910, train/grad_norm: 1.9083072402281687e-05\n",
      "Step: 7910, train/learning_rate: 1.862605131464079e-05\n",
      "Step: 7910, train/epoch: 1.882436990737915\n",
      "Step: 7920, train/loss: 0.0\n",
      "Step: 7920, train/grad_norm: 3.4624579711817205e-05\n",
      "Step: 7920, train/learning_rate: 1.8586388250696473e-05\n",
      "Step: 7920, train/epoch: 1.8848167657852173\n",
      "Step: 7930, train/loss: 0.0\n",
      "Step: 7930, train/grad_norm: 1.1925065336981788e-05\n",
      "Step: 7930, train/learning_rate: 1.8546723367762752e-05\n",
      "Step: 7930, train/epoch: 1.8871965408325195\n",
      "Step: 7940, train/loss: 0.0\n",
      "Step: 7940, train/grad_norm: 3.568543615983799e-05\n",
      "Step: 7940, train/learning_rate: 1.8507060303818434e-05\n",
      "Step: 7940, train/epoch: 1.8895764350891113\n",
      "Step: 7950, train/loss: 0.0\n",
      "Step: 7950, train/grad_norm: 4.536229243967682e-05\n",
      "Step: 7950, train/learning_rate: 1.8467397239874117e-05\n",
      "Step: 7950, train/epoch: 1.8919562101364136\n",
      "Step: 7960, train/loss: 0.0\n",
      "Step: 7960, train/grad_norm: 4.3514657591003925e-05\n",
      "Step: 7960, train/learning_rate: 1.8427732356940396e-05\n",
      "Step: 7960, train/epoch: 1.8943359851837158\n",
      "Step: 7970, train/loss: 0.0\n",
      "Step: 7970, train/grad_norm: 9.316555406257976e-06\n",
      "Step: 7970, train/learning_rate: 1.838806929299608e-05\n",
      "Step: 7970, train/epoch: 1.8967158794403076\n",
      "Step: 7980, train/loss: 0.0\n",
      "Step: 7980, train/grad_norm: 2.0117922758799978e-05\n",
      "Step: 7980, train/learning_rate: 1.834840622905176e-05\n",
      "Step: 7980, train/epoch: 1.8990956544876099\n",
      "Step: 7990, train/loss: 0.0\n",
      "Step: 7990, train/grad_norm: 0.0001731573574943468\n",
      "Step: 7990, train/learning_rate: 1.830874134611804e-05\n",
      "Step: 7990, train/epoch: 1.901475429534912\n",
      "Step: 8000, train/loss: 0.0\n",
      "Step: 8000, train/grad_norm: 2.0402778318384662e-05\n",
      "Step: 8000, train/learning_rate: 1.8269078282173723e-05\n",
      "Step: 8000, train/epoch: 1.903855323791504\n",
      "Step: 8010, train/loss: 0.0\n",
      "Step: 8010, train/grad_norm: 1.68229471455561e-05\n",
      "Step: 8010, train/learning_rate: 1.8229415218229406e-05\n",
      "Step: 8010, train/epoch: 1.9062350988388062\n",
      "Step: 8020, train/loss: 0.0\n",
      "Step: 8020, train/grad_norm: 4.0340259147342294e-05\n",
      "Step: 8020, train/learning_rate: 1.8189750335295685e-05\n",
      "Step: 8020, train/epoch: 1.908614993095398\n",
      "Step: 8030, train/loss: 0.0\n",
      "Step: 8030, train/grad_norm: 4.892515789833851e-06\n",
      "Step: 8030, train/learning_rate: 1.8150087271351367e-05\n",
      "Step: 8030, train/epoch: 1.9109947681427002\n",
      "Step: 8040, train/loss: 0.0\n",
      "Step: 8040, train/grad_norm: 4.38118186139036e-05\n",
      "Step: 8040, train/learning_rate: 1.811042420740705e-05\n",
      "Step: 8040, train/epoch: 1.9133745431900024\n",
      "Step: 8050, train/loss: 0.0\n",
      "Step: 8050, train/grad_norm: 5.621469608740881e-05\n",
      "Step: 8050, train/learning_rate: 1.807075932447333e-05\n",
      "Step: 8050, train/epoch: 1.9157544374465942\n",
      "Step: 8060, train/loss: 0.0\n",
      "Step: 8060, train/grad_norm: 1.8038050257018767e-05\n",
      "Step: 8060, train/learning_rate: 1.803109626052901e-05\n",
      "Step: 8060, train/epoch: 1.9181342124938965\n",
      "Step: 8070, train/loss: 0.0\n",
      "Step: 8070, train/grad_norm: 2.0967590899090283e-05\n",
      "Step: 8070, train/learning_rate: 1.7991433196584694e-05\n",
      "Step: 8070, train/epoch: 1.9205139875411987\n",
      "Step: 8080, train/loss: 0.0\n",
      "Step: 8080, train/grad_norm: 1.6611704268143512e-05\n",
      "Step: 8080, train/learning_rate: 1.7951768313650973e-05\n",
      "Step: 8080, train/epoch: 1.9228938817977905\n",
      "Step: 8090, train/loss: 0.0\n",
      "Step: 8090, train/grad_norm: 1.2150177099101711e-05\n",
      "Step: 8090, train/learning_rate: 1.7912105249706656e-05\n",
      "Step: 8090, train/epoch: 1.9252736568450928\n",
      "Step: 8100, train/loss: 0.0\n",
      "Step: 8100, train/grad_norm: 0.0001087923810700886\n",
      "Step: 8100, train/learning_rate: 1.787244218576234e-05\n",
      "Step: 8100, train/epoch: 1.9276535511016846\n",
      "Step: 8110, train/loss: 0.0\n",
      "Step: 8110, train/grad_norm: 4.378973972052336e-05\n",
      "Step: 8110, train/learning_rate: 1.7832777302828617e-05\n",
      "Step: 8110, train/epoch: 1.9300333261489868\n",
      "Step: 8120, train/loss: 0.0\n",
      "Step: 8120, train/grad_norm: 2.850128112186212e-05\n",
      "Step: 8120, train/learning_rate: 1.77931142388843e-05\n",
      "Step: 8120, train/epoch: 1.932413101196289\n",
      "Step: 8130, train/loss: 0.0\n",
      "Step: 8130, train/grad_norm: 2.2214688215171918e-05\n",
      "Step: 8130, train/learning_rate: 1.7753451174939983e-05\n",
      "Step: 8130, train/epoch: 1.9347929954528809\n",
      "Step: 8140, train/loss: 0.0\n",
      "Step: 8140, train/grad_norm: 1.554128175484948e-05\n",
      "Step: 8140, train/learning_rate: 1.771378629200626e-05\n",
      "Step: 8140, train/epoch: 1.937172770500183\n",
      "Step: 8150, train/loss: 0.0\n",
      "Step: 8150, train/grad_norm: 2.9630979042849503e-05\n",
      "Step: 8150, train/learning_rate: 1.7674123228061944e-05\n",
      "Step: 8150, train/epoch: 1.9395525455474854\n",
      "Step: 8160, train/loss: 0.0\n",
      "Step: 8160, train/grad_norm: 8.39049789647106e-06\n",
      "Step: 8160, train/learning_rate: 1.7634460164117627e-05\n",
      "Step: 8160, train/epoch: 1.9419324398040771\n",
      "Step: 8170, train/loss: 0.0\n",
      "Step: 8170, train/grad_norm: 4.820703179575503e-05\n",
      "Step: 8170, train/learning_rate: 1.7594795281183906e-05\n",
      "Step: 8170, train/epoch: 1.9443122148513794\n",
      "Step: 8180, train/loss: 0.0\n",
      "Step: 8180, train/grad_norm: 1.7013704564305954e-05\n",
      "Step: 8180, train/learning_rate: 1.755513221723959e-05\n",
      "Step: 8180, train/epoch: 1.9466921091079712\n",
      "Step: 8190, train/loss: 0.0\n",
      "Step: 8190, train/grad_norm: 0.00018000272393692285\n",
      "Step: 8190, train/learning_rate: 1.751546915329527e-05\n",
      "Step: 8190, train/epoch: 1.9490718841552734\n",
      "Step: 8200, train/loss: 0.0\n",
      "Step: 8200, train/grad_norm: 2.1155050490051508e-05\n",
      "Step: 8200, train/learning_rate: 1.747580427036155e-05\n",
      "Step: 8200, train/epoch: 1.9514516592025757\n",
      "Step: 8210, train/loss: 0.0\n",
      "Step: 8210, train/grad_norm: 7.248367637657793e-06\n",
      "Step: 8210, train/learning_rate: 1.7436141206417233e-05\n",
      "Step: 8210, train/epoch: 1.9538315534591675\n",
      "Step: 8220, train/loss: 0.0\n",
      "Step: 8220, train/grad_norm: 0.00011537472164491192\n",
      "Step: 8220, train/learning_rate: 1.7396478142472915e-05\n",
      "Step: 8220, train/epoch: 1.9562113285064697\n",
      "Step: 8230, train/loss: 0.0\n",
      "Step: 8230, train/grad_norm: 7.293583621503785e-06\n",
      "Step: 8230, train/learning_rate: 1.7356815078528598e-05\n",
      "Step: 8230, train/epoch: 1.958591103553772\n",
      "Step: 8240, train/loss: 0.0\n",
      "Step: 8240, train/grad_norm: 3.311811087769456e-05\n",
      "Step: 8240, train/learning_rate: 1.7317150195594877e-05\n",
      "Step: 8240, train/epoch: 1.9609709978103638\n",
      "Step: 8250, train/loss: 0.0\n",
      "Step: 8250, train/grad_norm: 0.0010498332558199763\n",
      "Step: 8250, train/learning_rate: 1.727748713165056e-05\n",
      "Step: 8250, train/epoch: 1.963350772857666\n",
      "Step: 8260, train/loss: 0.0\n",
      "Step: 8260, train/grad_norm: 1.0226959602732677e-05\n",
      "Step: 8260, train/learning_rate: 1.7237824067706242e-05\n",
      "Step: 8260, train/epoch: 1.9657305479049683\n",
      "Step: 8270, train/loss: 0.0\n",
      "Step: 8270, train/grad_norm: 2.5410763555555604e-05\n",
      "Step: 8270, train/learning_rate: 1.719815918477252e-05\n",
      "Step: 8270, train/epoch: 1.96811044216156\n",
      "Step: 8280, train/loss: 0.0\n",
      "Step: 8280, train/grad_norm: 3.776275843847543e-05\n",
      "Step: 8280, train/learning_rate: 1.7158496120828204e-05\n",
      "Step: 8280, train/epoch: 1.9704902172088623\n",
      "Step: 8290, train/loss: 0.0\n",
      "Step: 8290, train/grad_norm: 1.4561234820575919e-05\n",
      "Step: 8290, train/learning_rate: 1.7118833056883886e-05\n",
      "Step: 8290, train/epoch: 1.972870111465454\n",
      "Step: 8300, train/loss: 0.0\n",
      "Step: 8300, train/grad_norm: 0.000162929150974378\n",
      "Step: 8300, train/learning_rate: 1.7079168173950166e-05\n",
      "Step: 8300, train/epoch: 1.9752498865127563\n",
      "Step: 8310, train/loss: 0.0\n",
      "Step: 8310, train/grad_norm: 3.270544038969092e-05\n",
      "Step: 8310, train/learning_rate: 1.7039505110005848e-05\n",
      "Step: 8310, train/epoch: 1.9776296615600586\n",
      "Step: 8320, train/loss: 0.0\n",
      "Step: 8320, train/grad_norm: 4.665283086069394e-06\n",
      "Step: 8320, train/learning_rate: 1.699984204606153e-05\n",
      "Step: 8320, train/epoch: 1.9800095558166504\n",
      "Step: 8330, train/loss: 0.0\n",
      "Step: 8330, train/grad_norm: 0.005365089513361454\n",
      "Step: 8330, train/learning_rate: 1.696017716312781e-05\n",
      "Step: 8330, train/epoch: 1.9823893308639526\n",
      "Step: 8340, train/loss: 0.0\n",
      "Step: 8340, train/grad_norm: 2.806594238791149e-05\n",
      "Step: 8340, train/learning_rate: 1.6920514099183492e-05\n",
      "Step: 8340, train/epoch: 1.9847691059112549\n",
      "Step: 8350, train/loss: 9.999999747378752e-05\n",
      "Step: 8350, train/grad_norm: 3.872850356856361e-05\n",
      "Step: 8350, train/learning_rate: 1.6880851035239175e-05\n",
      "Step: 8350, train/epoch: 1.9871490001678467\n",
      "Step: 8360, train/loss: 0.0\n",
      "Step: 8360, train/grad_norm: 2.024227796937339e-05\n",
      "Step: 8360, train/learning_rate: 1.6841186152305454e-05\n",
      "Step: 8360, train/epoch: 1.989528775215149\n",
      "Step: 8370, train/loss: 0.0\n",
      "Step: 8370, train/grad_norm: 1.77581277966965e-05\n",
      "Step: 8370, train/learning_rate: 1.6801523088361137e-05\n",
      "Step: 8370, train/epoch: 1.9919086694717407\n",
      "Step: 8380, train/loss: 0.0\n",
      "Step: 8380, train/grad_norm: 3.77365795429796e-05\n",
      "Step: 8380, train/learning_rate: 1.676186002441682e-05\n",
      "Step: 8380, train/epoch: 1.994288444519043\n",
      "Step: 8390, train/loss: 0.0\n",
      "Step: 8390, train/grad_norm: 2.366952867305372e-05\n",
      "Step: 8390, train/learning_rate: 1.6722195141483098e-05\n",
      "Step: 8390, train/epoch: 1.9966682195663452\n",
      "Step: 8400, train/loss: 0.0\n",
      "Step: 8400, train/grad_norm: 2.7516691147866368e-09\n",
      "Step: 8400, train/learning_rate: 1.668253207753878e-05\n",
      "Step: 8400, train/epoch: 1.999048113822937\n",
      "Step: 8404, eval/loss: 0.002820471068844199\n",
      "Step: 8404, eval/accuracy: 0.9993058443069458\n",
      "Step: 8404, eval/f1: 0.9992668032646179\n",
      "Step: 8404, eval/runtime: 8281.814453125\n",
      "Step: 8404, eval/samples_per_second: 0.8700000047683716\n",
      "Step: 8404, eval/steps_per_second: 0.10899999737739563\n",
      "Step: 8404, train/epoch: 2.0\n",
      "Step: 8410, train/loss: 0.0\n",
      "Step: 8410, train/grad_norm: 2.6223669920000248e-06\n",
      "Step: 8410, train/learning_rate: 1.6642869013594463e-05\n",
      "Step: 8410, train/epoch: 2.0014278888702393\n",
      "Step: 8420, train/loss: 0.0\n",
      "Step: 8420, train/grad_norm: 1.7123922589235008e-05\n",
      "Step: 8420, train/learning_rate: 1.6603204130660743e-05\n",
      "Step: 8420, train/epoch: 2.003807783126831\n",
      "Step: 8430, train/loss: 0.0\n",
      "Step: 8430, train/grad_norm: 3.551518966560252e-05\n",
      "Step: 8430, train/learning_rate: 1.6563541066716425e-05\n",
      "Step: 8430, train/epoch: 2.0061874389648438\n",
      "Step: 8440, train/loss: 0.0\n",
      "Step: 8440, train/grad_norm: 0.00010050670243799686\n",
      "Step: 8440, train/learning_rate: 1.6523878002772108e-05\n",
      "Step: 8440, train/epoch: 2.0085673332214355\n",
      "Step: 8450, train/loss: 0.0\n",
      "Step: 8450, train/grad_norm: 1.688677912170533e-05\n",
      "Step: 8450, train/learning_rate: 1.6484213119838387e-05\n",
      "Step: 8450, train/epoch: 2.0109472274780273\n",
      "Step: 8460, train/loss: 0.0\n",
      "Step: 8460, train/grad_norm: 1.0080526408273727e-05\n",
      "Step: 8460, train/learning_rate: 1.644455005589407e-05\n",
      "Step: 8460, train/epoch: 2.01332688331604\n",
      "Step: 8470, train/loss: 0.0\n",
      "Step: 8470, train/grad_norm: 2.872795630537439e-05\n",
      "Step: 8470, train/learning_rate: 1.6404886991949752e-05\n",
      "Step: 8470, train/epoch: 2.015706777572632\n",
      "Step: 8480, train/loss: 0.0\n",
      "Step: 8480, train/grad_norm: 3.735576683538966e-05\n",
      "Step: 8480, train/learning_rate: 1.636522210901603e-05\n",
      "Step: 8480, train/epoch: 2.0180866718292236\n",
      "Step: 8490, train/loss: 0.0\n",
      "Step: 8490, train/grad_norm: 2.5891667974065058e-05\n",
      "Step: 8490, train/learning_rate: 1.6325559045071714e-05\n",
      "Step: 8490, train/epoch: 2.0204663276672363\n",
      "Step: 8500, train/loss: 0.0\n",
      "Step: 8500, train/grad_norm: 2.5021616238518618e-05\n",
      "Step: 8500, train/learning_rate: 1.6285895981127396e-05\n",
      "Step: 8500, train/epoch: 2.022846221923828\n",
      "Step: 8510, train/loss: 0.0\n",
      "Step: 8510, train/grad_norm: 4.1250990761909634e-05\n",
      "Step: 8510, train/learning_rate: 1.6246231098193675e-05\n",
      "Step: 8510, train/epoch: 2.02522611618042\n",
      "Step: 8520, train/loss: 0.0\n",
      "Step: 8520, train/grad_norm: 4.0911887481343e-05\n",
      "Step: 8520, train/learning_rate: 1.6206568034249358e-05\n",
      "Step: 8520, train/epoch: 2.0276060104370117\n",
      "Step: 8530, train/loss: 0.0\n",
      "Step: 8530, train/grad_norm: 7.96455151430564e-06\n",
      "Step: 8530, train/learning_rate: 1.616690497030504e-05\n",
      "Step: 8530, train/epoch: 2.0299856662750244\n",
      "Step: 8540, train/loss: 0.0\n",
      "Step: 8540, train/grad_norm: 1.2110184798075352e-05\n",
      "Step: 8540, train/learning_rate: 1.612724008737132e-05\n",
      "Step: 8540, train/epoch: 2.032365560531616\n",
      "Step: 8550, train/loss: 0.0\n",
      "Step: 8550, train/grad_norm: 3.8231613871175796e-05\n",
      "Step: 8550, train/learning_rate: 1.6087577023427002e-05\n",
      "Step: 8550, train/epoch: 2.034745454788208\n",
      "Step: 8560, train/loss: 0.0\n",
      "Step: 8560, train/grad_norm: 1.8986655049957335e-05\n",
      "Step: 8560, train/learning_rate: 1.6047913959482685e-05\n",
      "Step: 8560, train/epoch: 2.0371251106262207\n",
      "Step: 8570, train/loss: 0.0\n",
      "Step: 8570, train/grad_norm: 0.0001828964741434902\n",
      "Step: 8570, train/learning_rate: 1.6008250895538367e-05\n",
      "Step: 8570, train/epoch: 2.0395050048828125\n",
      "Step: 8580, train/loss: 0.0\n",
      "Step: 8580, train/grad_norm: 4.952055314788595e-05\n",
      "Step: 8580, train/learning_rate: 1.5968586012604646e-05\n",
      "Step: 8580, train/epoch: 2.0418848991394043\n",
      "Step: 8590, train/loss: 0.0\n",
      "Step: 8590, train/grad_norm: 8.795839676167816e-06\n",
      "Step: 8590, train/learning_rate: 1.592892294866033e-05\n",
      "Step: 8590, train/epoch: 2.044264554977417\n",
      "Step: 8600, train/loss: 0.0\n",
      "Step: 8600, train/grad_norm: 2.570145807112567e-05\n",
      "Step: 8600, train/learning_rate: 1.588925988471601e-05\n",
      "Step: 8600, train/epoch: 2.046644449234009\n",
      "Step: 8610, train/loss: 0.0\n",
      "Step: 8610, train/grad_norm: 1.8065074982587248e-05\n",
      "Step: 8610, train/learning_rate: 1.584959500178229e-05\n",
      "Step: 8610, train/epoch: 2.0490243434906006\n",
      "Step: 8620, train/loss: 0.0\n",
      "Step: 8620, train/grad_norm: 2.0335459339548834e-05\n",
      "Step: 8620, train/learning_rate: 1.5809931937837973e-05\n",
      "Step: 8620, train/epoch: 2.0514039993286133\n",
      "Step: 8630, train/loss: 0.0\n",
      "Step: 8630, train/grad_norm: 3.6749333958141506e-05\n",
      "Step: 8630, train/learning_rate: 1.5770268873893656e-05\n",
      "Step: 8630, train/epoch: 2.053783893585205\n",
      "Step: 8640, train/loss: 0.0\n",
      "Step: 8640, train/grad_norm: 5.9250014601275325e-06\n",
      "Step: 8640, train/learning_rate: 1.5730603990959935e-05\n",
      "Step: 8640, train/epoch: 2.056163787841797\n",
      "Step: 8650, train/loss: 0.0\n",
      "Step: 8650, train/grad_norm: 1.2612485079444014e-05\n",
      "Step: 8650, train/learning_rate: 1.5690940927015617e-05\n",
      "Step: 8650, train/epoch: 2.0585434436798096\n",
      "Step: 8660, train/loss: 0.0\n",
      "Step: 8660, train/grad_norm: 1.6375313862226903e-05\n",
      "Step: 8660, train/learning_rate: 1.56512778630713e-05\n",
      "Step: 8660, train/epoch: 2.0609233379364014\n",
      "Reading events from file: ./praxis-Llama-2-70b-hf-small-finetune/logs/events.out.tfevents.1716869591.hephaestus.6764.0\n",
      "Step: 10, train/loss: 0.9555000066757202\n",
      "Step: 10, train/grad_norm: 28.964208602905273\n",
      "Step: 10, train/learning_rate: 4.998810254619457e-05\n",
      "Step: 10, train/epoch: 0.0023798190522938967\n",
      "Step: 20, train/loss: 0.836899995803833\n",
      "Step: 20, train/grad_norm: 21.0864200592041\n",
      "Step: 20, train/learning_rate: 4.997620271751657e-05\n",
      "Step: 20, train/epoch: 0.004759638104587793\n",
      "Step: 30, train/loss: 0.4666000008583069\n",
      "Step: 30, train/grad_norm: 16.812292098999023\n",
      "Step: 30, train/learning_rate: 4.9964302888838574e-05\n",
      "Step: 30, train/epoch: 0.007139457389712334\n",
      "Step: 40, train/loss: 0.3352999985218048\n",
      "Step: 40, train/grad_norm: 21.270137786865234\n",
      "Step: 40, train/learning_rate: 4.995240306016058e-05\n",
      "Step: 40, train/epoch: 0.009519276209175587\n",
      "Step: 50, train/loss: 0.14090000092983246\n",
      "Step: 50, train/grad_norm: 10.75603199005127\n",
      "Step: 50, train/learning_rate: 4.994050323148258e-05\n",
      "Step: 50, train/epoch: 0.011899095959961414\n",
      "Step: 60, train/loss: 0.2282000035047531\n",
      "Step: 60, train/grad_norm: 0.002687480766326189\n",
      "Step: 60, train/learning_rate: 4.992860704078339e-05\n",
      "Step: 60, train/epoch: 0.014278914779424667\n",
      "Step: 70, train/loss: 0.008299999870359898\n",
      "Step: 70, train/grad_norm: 0.0005120589630678296\n",
      "Step: 70, train/learning_rate: 4.9916707212105393e-05\n",
      "Step: 70, train/epoch: 0.016658734530210495\n",
      "Step: 80, train/loss: 9.999999747378752e-05\n",
      "Step: 80, train/grad_norm: 0.0003049468214157969\n",
      "Step: 80, train/learning_rate: 4.9904807383427396e-05\n",
      "Step: 80, train/epoch: 0.019038552418351173\n",
      "Step: 90, train/loss: 0.02710000053048134\n",
      "Step: 90, train/grad_norm: 0.0038412215653806925\n",
      "Step: 90, train/learning_rate: 4.98929075547494e-05\n",
      "Step: 90, train/epoch: 0.021418372169137\n",
      "Step: 100, train/loss: 0.011300000362098217\n",
      "Step: 100, train/grad_norm: 0.017053255811333656\n",
      "Step: 100, train/learning_rate: 4.98810077260714e-05\n",
      "Step: 100, train/epoch: 0.02379819191992283\n",
      "Step: 110, train/loss: 0.06759999692440033\n",
      "Step: 110, train/grad_norm: 0.0062248967587947845\n",
      "Step: 110, train/learning_rate: 4.986911153537221e-05\n",
      "Step: 110, train/epoch: 0.026178009808063507\n",
      "Step: 120, train/loss: 0.27230000495910645\n",
      "Step: 120, train/grad_norm: 0.028072906658053398\n",
      "Step: 120, train/learning_rate: 4.9857211706694216e-05\n",
      "Step: 120, train/epoch: 0.028557829558849335\n",
      "Step: 130, train/loss: 0.12700000405311584\n",
      "Step: 130, train/grad_norm: 0.006201874930411577\n",
      "Step: 130, train/learning_rate: 4.984531187801622e-05\n",
      "Step: 130, train/epoch: 0.030937649309635162\n",
      "Step: 140, train/loss: 0.08169999718666077\n",
      "Step: 140, train/grad_norm: 26.036048889160156\n",
      "Step: 140, train/learning_rate: 4.983341204933822e-05\n",
      "Step: 140, train/epoch: 0.03331746906042099\n",
      "Step: 150, train/loss: 0.005799999926239252\n",
      "Step: 150, train/grad_norm: 0.003366272198036313\n",
      "Step: 150, train/learning_rate: 4.9821512220660225e-05\n",
      "Step: 150, train/epoch: 0.03569728881120682\n",
      "Step: 160, train/loss: 0.19740000367164612\n",
      "Step: 160, train/grad_norm: 0.004967201501131058\n",
      "Step: 160, train/learning_rate: 4.9809616029961035e-05\n",
      "Step: 160, train/epoch: 0.03807710483670235\n",
      "Step: 170, train/loss: 0.07020000368356705\n",
      "Step: 170, train/grad_norm: 0.0003790183982346207\n",
      "Step: 170, train/learning_rate: 4.979771620128304e-05\n",
      "Step: 170, train/epoch: 0.040456924587488174\n",
      "Step: 180, train/loss: 0.10729999840259552\n",
      "Step: 180, train/grad_norm: 0.006603906862437725\n",
      "Step: 180, train/learning_rate: 4.978581637260504e-05\n",
      "Step: 180, train/epoch: 0.042836744338274\n",
      "Step: 190, train/loss: 0.00039999998989515007\n",
      "Step: 190, train/grad_norm: 0.03699866309762001\n",
      "Step: 190, train/learning_rate: 4.9773916543927044e-05\n",
      "Step: 190, train/epoch: 0.04521656408905983\n",
      "Step: 200, train/loss: 0.0010999999940395355\n",
      "Step: 200, train/grad_norm: 1.1966525316238403\n",
      "Step: 200, train/learning_rate: 4.976201671524905e-05\n",
      "Step: 200, train/epoch: 0.04759638383984566\n",
      "Step: 210, train/loss: 9.999999747378752e-05\n",
      "Step: 210, train/grad_norm: 0.0006933657568879426\n",
      "Step: 210, train/learning_rate: 4.975012052454986e-05\n",
      "Step: 210, train/epoch: 0.049976203590631485\n",
      "Step: 220, train/loss: 9.999999747378752e-05\n",
      "Step: 220, train/grad_norm: 0.06514935195446014\n",
      "Step: 220, train/learning_rate: 4.973822069587186e-05\n",
      "Step: 220, train/epoch: 0.052356019616127014\n",
      "Step: 230, train/loss: 0.0\n",
      "Step: 230, train/grad_norm: 0.004198563750833273\n",
      "Step: 230, train/learning_rate: 4.972632086719386e-05\n",
      "Step: 230, train/epoch: 0.05473583936691284\n",
      "Step: 240, train/loss: 9.999999747378752e-05\n",
      "Step: 240, train/grad_norm: 0.00443654228001833\n",
      "Step: 240, train/learning_rate: 4.9714421038515866e-05\n",
      "Step: 240, train/epoch: 0.05711565911769867\n",
      "Step: 250, train/loss: 9.999999747378752e-05\n",
      "Step: 250, train/grad_norm: 0.0001139690721174702\n",
      "Step: 250, train/learning_rate: 4.970252120983787e-05\n",
      "Step: 250, train/epoch: 0.0594954788684845\n",
      "Step: 260, train/loss: 0.0\n",
      "Step: 260, train/grad_norm: 0.0013386339414864779\n",
      "Step: 260, train/learning_rate: 4.969062501913868e-05\n",
      "Step: 260, train/epoch: 0.061875298619270325\n",
      "Step: 270, train/loss: 0.0\n",
      "Step: 270, train/grad_norm: 0.008837332017719746\n",
      "Step: 270, train/learning_rate: 4.967872519046068e-05\n",
      "Step: 270, train/epoch: 0.06425511837005615\n",
      "Step: 280, train/loss: 0.0\n",
      "Step: 280, train/grad_norm: 0.000831456040032208\n",
      "Step: 280, train/learning_rate: 4.9666825361782685e-05\n",
      "Step: 280, train/epoch: 0.06663493812084198\n",
      "Step: 290, train/loss: 0.0010000000474974513\n",
      "Step: 290, train/grad_norm: 0.0009630750864744186\n",
      "Step: 290, train/learning_rate: 4.965492553310469e-05\n",
      "Step: 290, train/epoch: 0.06901475787162781\n",
      "Step: 300, train/loss: 0.00019999999494757503\n",
      "Step: 300, train/grad_norm: 0.00014594975800719112\n",
      "Step: 300, train/learning_rate: 4.964302570442669e-05\n",
      "Step: 300, train/epoch: 0.07139457762241364\n",
      "Step: 310, train/loss: 0.0\n",
      "Step: 310, train/grad_norm: 0.00024908289196901023\n",
      "Step: 310, train/learning_rate: 4.96311295137275e-05\n",
      "Step: 310, train/epoch: 0.07377438992261887\n",
      "Step: 320, train/loss: 0.00019999999494757503\n",
      "Step: 320, train/grad_norm: 0.29063230752944946\n",
      "Step: 320, train/learning_rate: 4.9619229685049504e-05\n",
      "Step: 320, train/epoch: 0.0761542096734047\n",
      "Step: 330, train/loss: 0.2093999981880188\n",
      "Step: 330, train/grad_norm: 0.00010232427302980796\n",
      "Step: 330, train/learning_rate: 4.960732985637151e-05\n",
      "Step: 330, train/epoch: 0.07853402942419052\n",
      "Step: 340, train/loss: 0.13830000162124634\n",
      "Step: 340, train/grad_norm: 0.0012243662495166063\n",
      "Step: 340, train/learning_rate: 4.959543002769351e-05\n",
      "Step: 340, train/epoch: 0.08091384917497635\n",
      "Step: 350, train/loss: 9.999999747378752e-05\n",
      "Step: 350, train/grad_norm: 0.026805220171809196\n",
      "Step: 350, train/learning_rate: 4.958353019901551e-05\n",
      "Step: 350, train/epoch: 0.08329366892576218\n",
      "Step: 360, train/loss: 9.999999747378752e-05\n",
      "Step: 360, train/grad_norm: 0.017733776941895485\n",
      "Step: 360, train/learning_rate: 4.957163400831632e-05\n",
      "Step: 360, train/epoch: 0.085673488676548\n",
      "Step: 370, train/loss: 9.999999747378752e-05\n",
      "Step: 370, train/grad_norm: 0.005878694821149111\n",
      "Step: 370, train/learning_rate: 4.9559734179638326e-05\n",
      "Step: 370, train/epoch: 0.08805330842733383\n",
      "Step: 380, train/loss: 9.999999747378752e-05\n",
      "Step: 380, train/grad_norm: 0.00453741941601038\n",
      "Step: 380, train/learning_rate: 4.954783435096033e-05\n",
      "Step: 380, train/epoch: 0.09043312817811966\n",
      "Step: 390, train/loss: 0.22429999709129333\n",
      "Step: 390, train/grad_norm: 55.797183990478516\n",
      "Step: 390, train/learning_rate: 4.953593452228233e-05\n",
      "Step: 390, train/epoch: 0.09281294792890549\n",
      "Step: 400, train/loss: 0.11959999799728394\n",
      "Step: 400, train/grad_norm: 0.018634231761097908\n",
      "Step: 400, train/learning_rate: 4.9524034693604335e-05\n",
      "Step: 400, train/epoch: 0.09519276767969131\n",
      "Step: 410, train/loss: 0.0032999999821186066\n",
      "Step: 410, train/grad_norm: 0.005568292457610369\n",
      "Step: 410, train/learning_rate: 4.9512138502905145e-05\n",
      "Step: 410, train/epoch: 0.09757258743047714\n",
      "Step: 420, train/loss: 0.00019999999494757503\n",
      "Step: 420, train/grad_norm: 0.011127485893666744\n",
      "Step: 420, train/learning_rate: 4.950023867422715e-05\n",
      "Step: 420, train/epoch: 0.09995240718126297\n",
      "Step: 430, train/loss: 0.05849999934434891\n",
      "Step: 430, train/grad_norm: 0.005193730350583792\n",
      "Step: 430, train/learning_rate: 4.948833884554915e-05\n",
      "Step: 430, train/epoch: 0.1023322194814682\n",
      "Step: 440, train/loss: 0.0\n",
      "Step: 440, train/grad_norm: 0.0029559945687651634\n",
      "Step: 440, train/learning_rate: 4.9476439016871154e-05\n",
      "Step: 440, train/epoch: 0.10471203923225403\n",
      "Step: 450, train/loss: 0.18449999392032623\n",
      "Step: 450, train/grad_norm: 9.605265950085595e-05\n",
      "Step: 450, train/learning_rate: 4.946453918819316e-05\n",
      "Step: 450, train/epoch: 0.10709185898303986\n",
      "Step: 460, train/loss: 0.0\n",
      "Step: 460, train/grad_norm: 2.78947100014193e-05\n",
      "Step: 460, train/learning_rate: 4.945264299749397e-05\n",
      "Step: 460, train/epoch: 0.10947167873382568\n",
      "Step: 470, train/loss: 0.026000000536441803\n",
      "Step: 470, train/grad_norm: 0.0004928740672767162\n",
      "Step: 470, train/learning_rate: 4.944074316881597e-05\n",
      "Step: 470, train/epoch: 0.11185149848461151\n",
      "Step: 480, train/loss: 0.0019000000320374966\n",
      "Step: 480, train/grad_norm: 0.0015327837318181992\n",
      "Step: 480, train/learning_rate: 4.9428843340137973e-05\n",
      "Step: 480, train/epoch: 0.11423131823539734\n",
      "Step: 490, train/loss: 9.999999747378752e-05\n",
      "Step: 490, train/grad_norm: 0.010713033378124237\n",
      "Step: 490, train/learning_rate: 4.9416943511459976e-05\n",
      "Step: 490, train/epoch: 0.11661113798618317\n",
      "Step: 500, train/loss: 9.999999747378752e-05\n",
      "Step: 500, train/grad_norm: 0.002555262763053179\n",
      "Step: 500, train/learning_rate: 4.940504368278198e-05\n",
      "Step: 500, train/epoch: 0.118990957736969\n",
      "Step: 510, train/loss: 9.999999747378752e-05\n",
      "Step: 510, train/grad_norm: 0.00041997560765594244\n",
      "Step: 510, train/learning_rate: 4.939314749208279e-05\n",
      "Step: 510, train/epoch: 0.12137077748775482\n",
      "Step: 520, train/loss: 0.06560000032186508\n",
      "Step: 520, train/grad_norm: 0.00032819819170981646\n",
      "Step: 520, train/learning_rate: 4.938124766340479e-05\n",
      "Step: 520, train/epoch: 0.12375059723854065\n",
      "Step: 530, train/loss: 0.08919999748468399\n",
      "Step: 530, train/grad_norm: 0.02410687506198883\n",
      "Step: 530, train/learning_rate: 4.9369347834726796e-05\n",
      "Step: 530, train/epoch: 0.12613041698932648\n",
      "Step: 540, train/loss: 0.00019999999494757503\n",
      "Step: 540, train/grad_norm: 0.0004909444251097739\n",
      "Step: 540, train/learning_rate: 4.93574480060488e-05\n",
      "Step: 540, train/epoch: 0.1285102367401123\n",
      "Step: 550, train/loss: 0.1657000035047531\n",
      "Step: 550, train/grad_norm: 0.003487286390736699\n",
      "Step: 550, train/learning_rate: 4.93455481773708e-05\n",
      "Step: 550, train/epoch: 0.13089005649089813\n",
      "Step: 560, train/loss: 9.999999747378752e-05\n",
      "Step: 560, train/grad_norm: 0.009381672367453575\n",
      "Step: 560, train/learning_rate: 4.933365198667161e-05\n",
      "Step: 560, train/epoch: 0.13326987624168396\n",
      "Step: 570, train/loss: 0.0\n",
      "Step: 570, train/grad_norm: 0.0005407235003076494\n",
      "Step: 570, train/learning_rate: 4.9321752157993615e-05\n",
      "Step: 570, train/epoch: 0.1356496959924698\n",
      "Step: 580, train/loss: 0.0\n",
      "Step: 580, train/grad_norm: 0.002596723148599267\n",
      "Step: 580, train/learning_rate: 4.930985232931562e-05\n",
      "Step: 580, train/epoch: 0.13802951574325562\n",
      "Step: 590, train/loss: 0.18330000340938568\n",
      "Step: 590, train/grad_norm: 0.06321987509727478\n",
      "Step: 590, train/learning_rate: 4.929795250063762e-05\n",
      "Step: 590, train/epoch: 0.14040933549404144\n",
      "Step: 600, train/loss: 0.18930000066757202\n",
      "Step: 600, train/grad_norm: 1.264965534210205\n",
      "Step: 600, train/learning_rate: 4.9286052671959624e-05\n",
      "Step: 600, train/epoch: 0.14278915524482727\n",
      "Step: 610, train/loss: 0.0026000000070780516\n",
      "Step: 610, train/grad_norm: 0.03099709004163742\n",
      "Step: 610, train/learning_rate: 4.9274156481260434e-05\n",
      "Step: 610, train/epoch: 0.1451689600944519\n",
      "Step: 620, train/loss: 0.23909999430179596\n",
      "Step: 620, train/grad_norm: 0.07239270955324173\n",
      "Step: 620, train/learning_rate: 4.926225665258244e-05\n",
      "Step: 620, train/epoch: 0.14754877984523773\n",
      "Step: 630, train/loss: 0.008299999870359898\n",
      "Step: 630, train/grad_norm: 8.211977547034621e-05\n",
      "Step: 630, train/learning_rate: 4.925035682390444e-05\n",
      "Step: 630, train/epoch: 0.14992859959602356\n",
      "Step: 640, train/loss: 0.0\n",
      "Step: 640, train/grad_norm: 1.719450119708199e-05\n",
      "Step: 640, train/learning_rate: 4.923845699522644e-05\n",
      "Step: 640, train/epoch: 0.1523084193468094\n",
      "Step: 650, train/loss: 0.03229999914765358\n",
      "Step: 650, train/grad_norm: 6.188621046021581e-05\n",
      "Step: 650, train/learning_rate: 4.9226557166548446e-05\n",
      "Step: 650, train/epoch: 0.15468823909759521\n",
      "Step: 660, train/loss: 0.0\n",
      "Step: 660, train/grad_norm: 0.0002533228835090995\n",
      "Step: 660, train/learning_rate: 4.9214660975849256e-05\n",
      "Step: 660, train/epoch: 0.15706805884838104\n",
      "Step: 670, train/loss: 0.15549999475479126\n",
      "Step: 670, train/grad_norm: 0.0001822943304432556\n",
      "Step: 670, train/learning_rate: 4.920276114717126e-05\n",
      "Step: 670, train/epoch: 0.15944787859916687\n",
      "Step: 680, train/loss: 0.1039000004529953\n",
      "Step: 680, train/grad_norm: 0.9121072292327881\n",
      "Step: 680, train/learning_rate: 4.919086131849326e-05\n",
      "Step: 680, train/epoch: 0.1618276983499527\n",
      "Step: 690, train/loss: 0.0003000000142492354\n",
      "Step: 690, train/grad_norm: 0.0017085454892367125\n",
      "Step: 690, train/learning_rate: 4.9178961489815265e-05\n",
      "Step: 690, train/epoch: 0.16420751810073853\n",
      "Step: 700, train/loss: 0.0013000000035390258\n",
      "Step: 700, train/grad_norm: 0.030138792470097542\n",
      "Step: 700, train/learning_rate: 4.916706166113727e-05\n",
      "Step: 700, train/epoch: 0.16658733785152435\n",
      "Step: 710, train/loss: 0.13369999825954437\n",
      "Step: 710, train/grad_norm: 0.005306766368448734\n",
      "Step: 710, train/learning_rate: 4.915516547043808e-05\n",
      "Step: 710, train/epoch: 0.16896715760231018\n",
      "Step: 720, train/loss: 9.999999747378752e-05\n",
      "Step: 720, train/grad_norm: 0.1455756276845932\n",
      "Step: 720, train/learning_rate: 4.914326564176008e-05\n",
      "Step: 720, train/epoch: 0.171346977353096\n",
      "Step: 730, train/loss: 9.999999747378752e-05\n",
      "Step: 730, train/grad_norm: 0.0023487622383981943\n",
      "Step: 730, train/learning_rate: 4.9131365813082084e-05\n",
      "Step: 730, train/epoch: 0.17372679710388184\n",
      "Step: 740, train/loss: 0.0\n",
      "Step: 740, train/grad_norm: 0.0009790295735001564\n",
      "Step: 740, train/learning_rate: 4.911946598440409e-05\n",
      "Step: 740, train/epoch: 0.17610661685466766\n",
      "Step: 750, train/loss: 0.19939999282360077\n",
      "Step: 750, train/grad_norm: 0.009124977514147758\n",
      "Step: 750, train/learning_rate: 4.910756615572609e-05\n",
      "Step: 750, train/epoch: 0.1784864366054535\n",
      "Step: 760, train/loss: 0.0003000000142492354\n",
      "Step: 760, train/grad_norm: 0.017955632880330086\n",
      "Step: 760, train/learning_rate: 4.90956699650269e-05\n",
      "Step: 760, train/epoch: 0.18086625635623932\n",
      "Step: 770, train/loss: 0.0\n",
      "Step: 770, train/grad_norm: 0.0021806079894304276\n",
      "Step: 770, train/learning_rate: 4.90837701363489e-05\n",
      "Step: 770, train/epoch: 0.18324607610702515\n",
      "Step: 780, train/loss: 0.09570000320672989\n",
      "Step: 780, train/grad_norm: 0.020234240218997\n",
      "Step: 780, train/learning_rate: 4.9071870307670906e-05\n",
      "Step: 780, train/epoch: 0.18562589585781097\n",
      "Step: 790, train/loss: 0.13220000267028809\n",
      "Step: 790, train/grad_norm: 0.06451529264450073\n",
      "Step: 790, train/learning_rate: 4.905997047899291e-05\n",
      "Step: 790, train/epoch: 0.1880057156085968\n",
      "Step: 800, train/loss: 0.061400000005960464\n",
      "Step: 800, train/grad_norm: 53.86648178100586\n",
      "Step: 800, train/learning_rate: 4.904807065031491e-05\n",
      "Step: 800, train/epoch: 0.19038553535938263\n",
      "Step: 810, train/loss: 0.14980000257492065\n",
      "Step: 810, train/grad_norm: 0.09640608727931976\n",
      "Step: 810, train/learning_rate: 4.903617445961572e-05\n",
      "Step: 810, train/epoch: 0.19276535511016846\n",
      "Step: 820, train/loss: 0.10000000149011612\n",
      "Step: 820, train/grad_norm: 0.02861805446445942\n",
      "Step: 820, train/learning_rate: 4.9024274630937725e-05\n",
      "Step: 820, train/epoch: 0.19514517486095428\n",
      "Step: 830, train/loss: 0.00570000009611249\n",
      "Step: 830, train/grad_norm: 0.0020146325696259737\n",
      "Step: 830, train/learning_rate: 4.901237480225973e-05\n",
      "Step: 830, train/epoch: 0.1975249946117401\n",
      "Step: 840, train/loss: 0.08709999918937683\n",
      "Step: 840, train/grad_norm: 3.831055164337158\n",
      "Step: 840, train/learning_rate: 4.900047497358173e-05\n",
      "Step: 840, train/epoch: 0.19990481436252594\n",
      "Step: 850, train/loss: 9.999999747378752e-05\n",
      "Step: 850, train/grad_norm: 0.00034269021125510335\n",
      "Step: 850, train/learning_rate: 4.8988575144903734e-05\n",
      "Step: 850, train/epoch: 0.20228461921215057\n",
      "Step: 860, train/loss: 0.05990000069141388\n",
      "Step: 860, train/grad_norm: 0.0003909172664862126\n",
      "Step: 860, train/learning_rate: 4.8976678954204544e-05\n",
      "Step: 860, train/epoch: 0.2046644389629364\n",
      "Step: 870, train/loss: 0.19850000739097595\n",
      "Step: 870, train/grad_norm: 0.010881571099162102\n",
      "Step: 870, train/learning_rate: 4.896477912552655e-05\n",
      "Step: 870, train/epoch: 0.20704425871372223\n",
      "Step: 880, train/loss: 0.0008999999845400453\n",
      "Step: 880, train/grad_norm: 0.020945722237229347\n",
      "Step: 880, train/learning_rate: 4.895287929684855e-05\n",
      "Step: 880, train/epoch: 0.20942407846450806\n",
      "Step: 890, train/loss: 0.0031999999191612005\n",
      "Step: 890, train/grad_norm: 0.0002760414208751172\n",
      "Step: 890, train/learning_rate: 4.8940979468170553e-05\n",
      "Step: 890, train/epoch: 0.21180389821529388\n",
      "Step: 900, train/loss: 0.00139999995008111\n",
      "Step: 900, train/grad_norm: 1.0084843779623043e-05\n",
      "Step: 900, train/learning_rate: 4.8929079639492556e-05\n",
      "Step: 900, train/epoch: 0.2141837179660797\n",
      "Step: 910, train/loss: 0.0\n",
      "Step: 910, train/grad_norm: 7.235634984681383e-05\n",
      "Step: 910, train/learning_rate: 4.8917183448793367e-05\n",
      "Step: 910, train/epoch: 0.21656353771686554\n",
      "Step: 920, train/loss: 0.23749999701976776\n",
      "Step: 920, train/grad_norm: 1.968982360267546e-05\n",
      "Step: 920, train/learning_rate: 4.890528362011537e-05\n",
      "Step: 920, train/epoch: 0.21894335746765137\n",
      "Step: 930, train/loss: 0.0\n",
      "Step: 930, train/grad_norm: 6.174446025397629e-05\n",
      "Step: 930, train/learning_rate: 4.889338379143737e-05\n",
      "Step: 930, train/epoch: 0.2213231772184372\n",
      "Step: 940, train/loss: 0.10090000182390213\n",
      "Step: 940, train/grad_norm: 0.01666569896042347\n",
      "Step: 940, train/learning_rate: 4.8881483962759376e-05\n",
      "Step: 940, train/epoch: 0.22370299696922302\n",
      "Step: 950, train/loss: 0.0003000000142492354\n",
      "Step: 950, train/grad_norm: 0.0030273119919002056\n",
      "Step: 950, train/learning_rate: 4.886958413408138e-05\n",
      "Step: 950, train/epoch: 0.22608281672000885\n",
      "Step: 960, train/loss: 0.05849999934434891\n",
      "Step: 960, train/grad_norm: 0.0011870135786011815\n",
      "Step: 960, train/learning_rate: 4.885768794338219e-05\n",
      "Step: 960, train/epoch: 0.22846263647079468\n",
      "Step: 970, train/loss: 0.0\n",
      "Step: 970, train/grad_norm: 6.401725840987638e-05\n",
      "Step: 970, train/learning_rate: 4.884578811470419e-05\n",
      "Step: 970, train/epoch: 0.2308424562215805\n",
      "Step: 980, train/loss: 0.15230000019073486\n",
      "Step: 980, train/grad_norm: 0.00039881805423647165\n",
      "Step: 980, train/learning_rate: 4.8833888286026195e-05\n",
      "Step: 980, train/epoch: 0.23322227597236633\n",
      "Step: 990, train/loss: 0.022099999710917473\n",
      "Step: 990, train/grad_norm: 5.558937118621543e-05\n",
      "Step: 990, train/learning_rate: 4.88219884573482e-05\n",
      "Step: 990, train/epoch: 0.23560209572315216\n",
      "Step: 1000, train/loss: 0.0\n",
      "Step: 1000, train/grad_norm: 0.006774047389626503\n",
      "Step: 1000, train/learning_rate: 4.88100886286702e-05\n",
      "Step: 1000, train/epoch: 0.237981915473938\n",
      "Step: 1010, train/loss: 0.45179998874664307\n",
      "Step: 1010, train/grad_norm: 0.00631316751241684\n",
      "Step: 1010, train/learning_rate: 4.879819243797101e-05\n",
      "Step: 1010, train/epoch: 0.24036173522472382\n",
      "Step: 1020, train/loss: 0.0\n",
      "Step: 1020, train/grad_norm: 0.002105799736455083\n",
      "Step: 1020, train/learning_rate: 4.8786292609293014e-05\n",
      "Step: 1020, train/epoch: 0.24274155497550964\n",
      "Step: 1030, train/loss: 0.10239999741315842\n",
      "Step: 1030, train/grad_norm: 0.005258259829133749\n",
      "Step: 1030, train/learning_rate: 4.877439278061502e-05\n",
      "Step: 1030, train/epoch: 0.24512137472629547\n",
      "Step: 1040, train/loss: 0.0008999999845400453\n",
      "Step: 1040, train/grad_norm: 0.0002872360637411475\n",
      "Step: 1040, train/learning_rate: 4.876249295193702e-05\n",
      "Step: 1040, train/epoch: 0.2475011944770813\n",
      "Step: 1050, train/loss: 0.00019999999494757503\n",
      "Step: 1050, train/grad_norm: 0.005481978878378868\n",
      "Step: 1050, train/learning_rate: 4.875059676123783e-05\n",
      "Step: 1050, train/epoch: 0.24988101422786713\n",
      "Step: 1060, train/loss: 9.999999747378752e-05\n",
      "Step: 1060, train/grad_norm: 0.0007190806209109724\n",
      "Step: 1060, train/learning_rate: 4.873869693255983e-05\n",
      "Step: 1060, train/epoch: 0.25226083397865295\n",
      "Step: 1070, train/loss: 0.07580000162124634\n",
      "Step: 1070, train/grad_norm: 0.001964989583939314\n",
      "Step: 1070, train/learning_rate: 4.8726797103881836e-05\n",
      "Step: 1070, train/epoch: 0.2546406388282776\n",
      "Step: 1080, train/loss: 0.1882999986410141\n",
      "Step: 1080, train/grad_norm: 0.005983822513371706\n",
      "Step: 1080, train/learning_rate: 4.871489727520384e-05\n",
      "Step: 1080, train/epoch: 0.2570204734802246\n",
      "Step: 1090, train/loss: 9.999999747378752e-05\n",
      "Step: 1090, train/grad_norm: 0.01912638545036316\n",
      "Step: 1090, train/learning_rate: 4.870299744652584e-05\n",
      "Step: 1090, train/epoch: 0.25940027832984924\n",
      "Step: 1100, train/loss: 9.999999747378752e-05\n",
      "Step: 1100, train/grad_norm: 0.019265733659267426\n",
      "Step: 1100, train/learning_rate: 4.869110125582665e-05\n",
      "Step: 1100, train/epoch: 0.26178011298179626\n",
      "Step: 1110, train/loss: 0.18330000340938568\n",
      "Step: 1110, train/grad_norm: 0.0015917073469609022\n",
      "Step: 1110, train/learning_rate: 4.8679201427148655e-05\n",
      "Step: 1110, train/epoch: 0.2641599178314209\n",
      "Step: 1120, train/loss: 0.11060000211000443\n",
      "Step: 1120, train/grad_norm: 0.005923356395214796\n",
      "Step: 1120, train/learning_rate: 4.866730159847066e-05\n",
      "Step: 1120, train/epoch: 0.2665397524833679\n",
      "Step: 1130, train/loss: 0.000699999975040555\n",
      "Step: 1130, train/grad_norm: 0.00029876886401325464\n",
      "Step: 1130, train/learning_rate: 4.865540176979266e-05\n",
      "Step: 1130, train/epoch: 0.26891955733299255\n",
      "Step: 1140, train/loss: 0.00019999999494757503\n",
      "Step: 1140, train/grad_norm: 0.001935155363753438\n",
      "Step: 1140, train/learning_rate: 4.8643501941114664e-05\n",
      "Step: 1140, train/epoch: 0.2712993919849396\n",
      "Step: 1150, train/loss: 9.999999747378752e-05\n",
      "Step: 1150, train/grad_norm: 0.0365050807595253\n",
      "Step: 1150, train/learning_rate: 4.8631605750415474e-05\n",
      "Step: 1150, train/epoch: 0.2736791968345642\n",
      "Step: 1160, train/loss: 0.0\n",
      "Step: 1160, train/grad_norm: 0.010198326781392097\n",
      "Step: 1160, train/learning_rate: 4.861970592173748e-05\n",
      "Step: 1160, train/epoch: 0.27605903148651123\n",
      "Step: 1170, train/loss: 0.036400001496076584\n",
      "Step: 1170, train/grad_norm: 36.329647064208984\n",
      "Step: 1170, train/learning_rate: 4.860780609305948e-05\n",
      "Step: 1170, train/epoch: 0.27843883633613586\n",
      "Step: 1180, train/loss: 9.999999747378752e-05\n",
      "Step: 1180, train/grad_norm: 0.00245843268930912\n",
      "Step: 1180, train/learning_rate: 4.859590626438148e-05\n",
      "Step: 1180, train/epoch: 0.2808186709880829\n",
      "Step: 1190, train/loss: 9.999999747378752e-05\n",
      "Step: 1190, train/grad_norm: 0.001225754152983427\n",
      "Step: 1190, train/learning_rate: 4.8584006435703486e-05\n",
      "Step: 1190, train/epoch: 0.2831984758377075\n",
      "Step: 1200, train/loss: 0.44279998540878296\n",
      "Step: 1200, train/grad_norm: 0.04430631548166275\n",
      "Step: 1200, train/learning_rate: 4.8572110245004296e-05\n",
      "Step: 1200, train/epoch: 0.28557831048965454\n",
      "Step: 1210, train/loss: 0.00039999998989515007\n",
      "Step: 1210, train/grad_norm: 0.003221142338588834\n",
      "Step: 1210, train/learning_rate: 4.85602104163263e-05\n",
      "Step: 1210, train/epoch: 0.2879581153392792\n",
      "Step: 1220, train/loss: 0.0\n",
      "Step: 1220, train/grad_norm: 0.003493694355711341\n",
      "Step: 1220, train/learning_rate: 4.85483105876483e-05\n",
      "Step: 1220, train/epoch: 0.2903379201889038\n",
      "Step: 1230, train/loss: 0.13359999656677246\n",
      "Step: 1230, train/grad_norm: 0.0007221460109576583\n",
      "Step: 1230, train/learning_rate: 4.8536410758970305e-05\n",
      "Step: 1230, train/epoch: 0.29271775484085083\n",
      "Step: 1240, train/loss: 0.08460000157356262\n",
      "Step: 1240, train/grad_norm: 0.03530481830239296\n",
      "Step: 1240, train/learning_rate: 4.852451093029231e-05\n",
      "Step: 1240, train/epoch: 0.29509755969047546\n",
      "Step: 1250, train/loss: 0.009499999694526196\n",
      "Step: 1250, train/grad_norm: 0.007542330771684647\n",
      "Step: 1250, train/learning_rate: 4.851261473959312e-05\n",
      "Step: 1250, train/epoch: 0.2974773943424225\n",
      "Step: 1260, train/loss: 0.00039999998989515007\n",
      "Step: 1260, train/grad_norm: 0.011197597719728947\n",
      "Step: 1260, train/learning_rate: 4.850071491091512e-05\n",
      "Step: 1260, train/epoch: 0.2998571991920471\n",
      "Step: 1270, train/loss: 0.06199999898672104\n",
      "Step: 1270, train/grad_norm: 0.00023941996914800256\n",
      "Step: 1270, train/learning_rate: 4.8488815082237124e-05\n",
      "Step: 1270, train/epoch: 0.30223703384399414\n",
      "Step: 1280, train/loss: 0.0\n",
      "Step: 1280, train/grad_norm: 0.0024384143762290478\n",
      "Step: 1280, train/learning_rate: 4.847691525355913e-05\n",
      "Step: 1280, train/epoch: 0.3046168386936188\n",
      "Step: 1290, train/loss: 9.999999747378752e-05\n",
      "Step: 1290, train/grad_norm: 0.0004316179547458887\n",
      "Step: 1290, train/learning_rate: 4.846501542488113e-05\n",
      "Step: 1290, train/epoch: 0.3069966733455658\n",
      "Step: 1300, train/loss: 0.08630000054836273\n",
      "Step: 1300, train/grad_norm: 0.0022565501276403666\n",
      "Step: 1300, train/learning_rate: 4.845311923418194e-05\n",
      "Step: 1300, train/epoch: 0.30937647819519043\n",
      "Step: 1310, train/loss: 0.0\n",
      "Step: 1310, train/grad_norm: 0.0012304517440497875\n",
      "Step: 1310, train/learning_rate: 4.8441219405503944e-05\n",
      "Step: 1310, train/epoch: 0.31175631284713745\n",
      "Step: 1320, train/loss: 0.00019999999494757503\n",
      "Step: 1320, train/grad_norm: 0.0014770356938242912\n",
      "Step: 1320, train/learning_rate: 4.8429319576825947e-05\n",
      "Step: 1320, train/epoch: 0.3141361176967621\n",
      "Step: 1330, train/loss: 9.999999747378752e-05\n",
      "Step: 1330, train/grad_norm: 0.0037101004272699356\n",
      "Step: 1330, train/learning_rate: 4.841741974814795e-05\n",
      "Step: 1330, train/epoch: 0.3165159523487091\n",
      "Step: 1340, train/loss: 0.0007999999797903001\n",
      "Step: 1340, train/grad_norm: 0.003583295037969947\n",
      "Step: 1340, train/learning_rate: 4.840551991946995e-05\n",
      "Step: 1340, train/epoch: 0.31889575719833374\n",
      "Step: 1350, train/loss: 0.12970000505447388\n",
      "Step: 1350, train/grad_norm: 0.000666956533677876\n",
      "Step: 1350, train/learning_rate: 4.839362372877076e-05\n",
      "Step: 1350, train/epoch: 0.32127559185028076\n",
      "Step: 1360, train/loss: 0.0\n",
      "Step: 1360, train/grad_norm: 0.0021294632460922003\n",
      "Step: 1360, train/learning_rate: 4.8381723900092766e-05\n",
      "Step: 1360, train/epoch: 0.3236553966999054\n",
      "Step: 1370, train/loss: 0.004000000189989805\n",
      "Step: 1370, train/grad_norm: 9.080194473266602\n",
      "Step: 1370, train/learning_rate: 4.836982407141477e-05\n",
      "Step: 1370, train/epoch: 0.3260352313518524\n",
      "Step: 1380, train/loss: 0.0010999999940395355\n",
      "Step: 1380, train/grad_norm: 0.010254057124257088\n",
      "Step: 1380, train/learning_rate: 4.835792424273677e-05\n",
      "Step: 1380, train/epoch: 0.32841503620147705\n",
      "Step: 1390, train/loss: 9.999999747378752e-05\n",
      "Step: 1390, train/grad_norm: 0.008966256864368916\n",
      "Step: 1390, train/learning_rate: 4.8346024414058775e-05\n",
      "Step: 1390, train/epoch: 0.3307948708534241\n",
      "Step: 1400, train/loss: 0.0006000000284984708\n",
      "Step: 1400, train/grad_norm: 3.874319372698665e-05\n",
      "Step: 1400, train/learning_rate: 4.8334128223359585e-05\n",
      "Step: 1400, train/epoch: 0.3331746757030487\n",
      "Step: 1410, train/loss: 0.0\n",
      "Step: 1410, train/grad_norm: 5.991502257529646e-05\n",
      "Step: 1410, train/learning_rate: 4.832222839468159e-05\n",
      "Step: 1410, train/epoch: 0.3355545103549957\n",
      "Step: 1420, train/loss: 0.0\n",
      "Step: 1420, train/grad_norm: 0.006312681827694178\n",
      "Step: 1420, train/learning_rate: 4.831032856600359e-05\n",
      "Step: 1420, train/epoch: 0.33793431520462036\n",
      "Step: 1430, train/loss: 9.999999747378752e-05\n",
      "Step: 1430, train/grad_norm: 2.7410774237068836e-06\n",
      "Step: 1430, train/learning_rate: 4.8298428737325594e-05\n",
      "Step: 1430, train/epoch: 0.3403141498565674\n",
      "Step: 1440, train/loss: 0.0\n",
      "Step: 1440, train/grad_norm: 2.128836786141619e-05\n",
      "Step: 1440, train/learning_rate: 4.82865289086476e-05\n",
      "Step: 1440, train/epoch: 0.342693954706192\n",
      "Step: 1450, train/loss: 0.20000000298023224\n",
      "Step: 1450, train/grad_norm: 0.00029109432944096625\n",
      "Step: 1450, train/learning_rate: 4.827463271794841e-05\n",
      "Step: 1450, train/epoch: 0.34507375955581665\n",
      "Step: 1460, train/loss: 0.0\n",
      "Step: 1460, train/grad_norm: 0.022330040112137794\n",
      "Step: 1460, train/learning_rate: 4.826273288927041e-05\n",
      "Step: 1460, train/epoch: 0.34745359420776367\n",
      "Step: 1470, train/loss: 0.07460000365972519\n",
      "Step: 1470, train/grad_norm: 0.031415749341249466\n",
      "Step: 1470, train/learning_rate: 4.825083306059241e-05\n",
      "Step: 1470, train/epoch: 0.3498333990573883\n",
      "Step: 1480, train/loss: 9.999999747378752e-05\n",
      "Step: 1480, train/grad_norm: 0.02953440696001053\n",
      "Step: 1480, train/learning_rate: 4.8238933231914416e-05\n",
      "Step: 1480, train/epoch: 0.3522132337093353\n",
      "Step: 1490, train/loss: 0.00139999995008111\n",
      "Step: 1490, train/grad_norm: 2.603649139404297\n",
      "Step: 1490, train/learning_rate: 4.822703340323642e-05\n",
      "Step: 1490, train/epoch: 0.35459303855895996\n",
      "Step: 1500, train/loss: 0.00039999998989515007\n",
      "Step: 1500, train/grad_norm: 0.0029524441342800856\n",
      "Step: 1500, train/learning_rate: 4.821513721253723e-05\n",
      "Step: 1500, train/epoch: 0.356972873210907\n",
      "Step: 1510, train/loss: 0.0005000000237487257\n",
      "Step: 1510, train/grad_norm: 1.3692941138288006e-05\n",
      "Step: 1510, train/learning_rate: 4.820323738385923e-05\n",
      "Step: 1510, train/epoch: 0.3593526780605316\n",
      "Step: 1520, train/loss: 0.0012000000569969416\n",
      "Step: 1520, train/grad_norm: 4.6343639041879214e-06\n",
      "Step: 1520, train/learning_rate: 4.8191337555181235e-05\n",
      "Step: 1520, train/epoch: 0.36173251271247864\n",
      "Step: 1530, train/loss: 0.0\n",
      "Step: 1530, train/grad_norm: 2.443760422465857e-05\n",
      "Step: 1530, train/learning_rate: 4.817943772650324e-05\n",
      "Step: 1530, train/epoch: 0.36411231756210327\n",
      "Step: 1540, train/loss: 0.06480000168085098\n",
      "Step: 1540, train/grad_norm: 0.00023254675033967942\n",
      "Step: 1540, train/learning_rate: 4.816753789782524e-05\n",
      "Step: 1540, train/epoch: 0.3664921522140503\n",
      "Step: 1550, train/loss: 0.0\n",
      "Step: 1550, train/grad_norm: 0.00019804170005954802\n",
      "Step: 1550, train/learning_rate: 4.815564170712605e-05\n",
      "Step: 1550, train/epoch: 0.3688719570636749\n",
      "Step: 1560, train/loss: 0.09459999948740005\n",
      "Step: 1560, train/grad_norm: 9.829310329223517e-06\n",
      "Step: 1560, train/learning_rate: 4.8143741878448054e-05\n",
      "Step: 1560, train/epoch: 0.37125179171562195\n",
      "Step: 1570, train/loss: 0.0\n",
      "Step: 1570, train/grad_norm: 1.1912623449461535e-05\n",
      "Step: 1570, train/learning_rate: 4.813184204977006e-05\n",
      "Step: 1570, train/epoch: 0.3736315965652466\n",
      "Step: 1580, train/loss: 0.0\n",
      "Step: 1580, train/grad_norm: 0.000954428396653384\n",
      "Step: 1580, train/learning_rate: 4.811994222109206e-05\n",
      "Step: 1580, train/epoch: 0.3760114312171936\n",
      "Step: 1590, train/loss: 0.0\n",
      "Step: 1590, train/grad_norm: 0.00014095181541051716\n",
      "Step: 1590, train/learning_rate: 4.810804239241406e-05\n",
      "Step: 1590, train/epoch: 0.37839123606681824\n",
      "Step: 1600, train/loss: 0.0\n",
      "Step: 1600, train/grad_norm: 0.0011000403901562095\n",
      "Step: 1600, train/learning_rate: 4.809614620171487e-05\n",
      "Step: 1600, train/epoch: 0.38077107071876526\n",
      "Step: 1610, train/loss: 0.0\n",
      "Step: 1610, train/grad_norm: 0.000772300292737782\n",
      "Step: 1610, train/learning_rate: 4.8084246373036876e-05\n",
      "Step: 1610, train/epoch: 0.3831508755683899\n",
      "Step: 1620, train/loss: 0.0\n",
      "Step: 1620, train/grad_norm: 2.34369290410541e-05\n",
      "Step: 1620, train/learning_rate: 4.807234654435888e-05\n",
      "Step: 1620, train/epoch: 0.3855307102203369\n",
      "Step: 1630, train/loss: 0.0\n",
      "Step: 1630, train/grad_norm: 0.005260440520942211\n",
      "Step: 1630, train/learning_rate: 4.806044671568088e-05\n",
      "Step: 1630, train/epoch: 0.38791051506996155\n",
      "Step: 1640, train/loss: 0.0\n",
      "Step: 1640, train/grad_norm: 5.84841609452269e-06\n",
      "Step: 1640, train/learning_rate: 4.8048546887002885e-05\n",
      "Step: 1640, train/epoch: 0.39029034972190857\n",
      "Step: 1650, train/loss: 0.0\n",
      "Step: 1650, train/grad_norm: 8.156186481755867e-07\n",
      "Step: 1650, train/learning_rate: 4.8036650696303695e-05\n",
      "Step: 1650, train/epoch: 0.3926701545715332\n",
      "Step: 1660, train/loss: 0.0\n",
      "Step: 1660, train/grad_norm: 3.720407767104916e-05\n",
      "Step: 1660, train/learning_rate: 4.80247508676257e-05\n",
      "Step: 1660, train/epoch: 0.3950499892234802\n",
      "Step: 1670, train/loss: 0.0019000000320374966\n",
      "Step: 1670, train/grad_norm: 0.0004860287590418011\n",
      "Step: 1670, train/learning_rate: 4.80128510389477e-05\n",
      "Step: 1670, train/epoch: 0.39742979407310486\n",
      "Step: 1680, train/loss: 0.0\n",
      "Step: 1680, train/grad_norm: 0.0012220332864671946\n",
      "Step: 1680, train/learning_rate: 4.8000951210269704e-05\n",
      "Step: 1680, train/epoch: 0.3998096287250519\n",
      "Step: 1690, train/loss: 0.43050000071525574\n",
      "Step: 1690, train/grad_norm: 1.6244861399172805e-05\n",
      "Step: 1690, train/learning_rate: 4.798905138159171e-05\n",
      "Step: 1690, train/epoch: 0.4021894335746765\n",
      "Step: 1700, train/loss: 0.0\n",
      "Step: 1700, train/grad_norm: 0.0004087104753125459\n",
      "Step: 1700, train/learning_rate: 4.797715519089252e-05\n",
      "Step: 1700, train/epoch: 0.40456923842430115\n",
      "Step: 1710, train/loss: 0.0\n",
      "Step: 1710, train/grad_norm: 0.002486376091837883\n",
      "Step: 1710, train/learning_rate: 4.796525536221452e-05\n",
      "Step: 1710, train/epoch: 0.40694907307624817\n",
      "Step: 1720, train/loss: 9.999999747378752e-05\n",
      "Step: 1720, train/grad_norm: 8.909393363865092e-05\n",
      "Step: 1720, train/learning_rate: 4.7953355533536524e-05\n",
      "Step: 1720, train/epoch: 0.4093288779258728\n",
      "Step: 1730, train/loss: 0.0\n",
      "Step: 1730, train/grad_norm: 3.089475285378285e-05\n",
      "Step: 1730, train/learning_rate: 4.7941455704858527e-05\n",
      "Step: 1730, train/epoch: 0.4117087125778198\n",
      "Step: 1740, train/loss: 0.004699999932199717\n",
      "Step: 1740, train/grad_norm: 2.4389530153712258e-05\n",
      "Step: 1740, train/learning_rate: 4.792955587618053e-05\n",
      "Step: 1740, train/epoch: 0.41408851742744446\n",
      "Step: 1750, train/loss: 0.0\n",
      "Step: 1750, train/grad_norm: 2.266773662995547e-05\n",
      "Step: 1750, train/learning_rate: 4.791765968548134e-05\n",
      "Step: 1750, train/epoch: 0.4164683520793915\n",
      "Step: 1760, train/loss: 0.0\n",
      "Step: 1760, train/grad_norm: 2.2603164325118996e-05\n",
      "Step: 1760, train/learning_rate: 4.790575985680334e-05\n",
      "Step: 1760, train/epoch: 0.4188481569290161\n",
      "Step: 1770, train/loss: 0.0\n",
      "Step: 1770, train/grad_norm: 2.82983819488436e-05\n",
      "Step: 1770, train/learning_rate: 4.7893860028125346e-05\n",
      "Step: 1770, train/epoch: 0.42122799158096313\n",
      "Step: 1780, train/loss: 0.0\n",
      "Step: 1780, train/grad_norm: 5.2632400183938444e-05\n",
      "Step: 1780, train/learning_rate: 4.788196019944735e-05\n",
      "Step: 1780, train/epoch: 0.42360779643058777\n",
      "Step: 1790, train/loss: 0.0\n",
      "Step: 1790, train/grad_norm: 2.5305098461103626e-05\n",
      "Step: 1790, train/learning_rate: 4.787006037076935e-05\n",
      "Step: 1790, train/epoch: 0.4259876310825348\n",
      "Step: 1800, train/loss: 0.0\n",
      "Step: 1800, train/grad_norm: 1.23695772344945e-05\n",
      "Step: 1800, train/learning_rate: 4.785816418007016e-05\n",
      "Step: 1800, train/epoch: 0.4283674359321594\n",
      "Step: 1810, train/loss: 0.0\n",
      "Step: 1810, train/grad_norm: 9.996449989557732e-06\n",
      "Step: 1810, train/learning_rate: 4.7846264351392165e-05\n",
      "Step: 1810, train/epoch: 0.43074727058410645\n",
      "Step: 1820, train/loss: 0.0\n",
      "Step: 1820, train/grad_norm: 1.4826434380665887e-05\n",
      "Step: 1820, train/learning_rate: 4.783436452271417e-05\n",
      "Step: 1820, train/epoch: 0.4331270754337311\n",
      "Step: 1830, train/loss: 0.0\n",
      "Step: 1830, train/grad_norm: 7.89824207458878e-06\n",
      "Step: 1830, train/learning_rate: 4.782246469403617e-05\n",
      "Step: 1830, train/epoch: 0.4355069100856781\n",
      "Step: 1840, train/loss: 0.19380000233650208\n",
      "Step: 1840, train/grad_norm: 30.44223976135254\n",
      "Step: 1840, train/learning_rate: 4.7810564865358174e-05\n",
      "Step: 1840, train/epoch: 0.43788671493530273\n",
      "Step: 1850, train/loss: 9.999999747378752e-05\n",
      "Step: 1850, train/grad_norm: 0.37202340364456177\n",
      "Step: 1850, train/learning_rate: 4.7798668674658984e-05\n",
      "Step: 1850, train/epoch: 0.44026654958724976\n",
      "Step: 1860, train/loss: 9.999999747378752e-05\n",
      "Step: 1860, train/grad_norm: 0.0011696722358465195\n",
      "Step: 1860, train/learning_rate: 4.778676884598099e-05\n",
      "Step: 1860, train/epoch: 0.4426463544368744\n",
      "Step: 1870, train/loss: 0.0\n",
      "Step: 1870, train/grad_norm: 0.0018051565857604146\n",
      "Step: 1870, train/learning_rate: 4.777486901730299e-05\n",
      "Step: 1870, train/epoch: 0.4450261890888214\n",
      "Step: 1880, train/loss: 0.0\n",
      "Step: 1880, train/grad_norm: 0.02203799970448017\n",
      "Step: 1880, train/learning_rate: 4.776296918862499e-05\n",
      "Step: 1880, train/epoch: 0.44740599393844604\n",
      "Step: 1890, train/loss: 0.0\n",
      "Step: 1890, train/grad_norm: 0.012404515407979488\n",
      "Step: 1890, train/learning_rate: 4.7751069359946996e-05\n",
      "Step: 1890, train/epoch: 0.44978582859039307\n",
      "Step: 1900, train/loss: 0.08560000360012054\n",
      "Step: 1900, train/grad_norm: 1.0266964636684861e-05\n",
      "Step: 1900, train/learning_rate: 4.7739173169247806e-05\n",
      "Step: 1900, train/epoch: 0.4521656334400177\n",
      "Step: 1910, train/loss: 0.0\n",
      "Step: 1910, train/grad_norm: 0.00018409860786050558\n",
      "Step: 1910, train/learning_rate: 4.772727334056981e-05\n",
      "Step: 1910, train/epoch: 0.4545454680919647\n",
      "Step: 1920, train/loss: 0.0\n",
      "Step: 1920, train/grad_norm: 0.0005424763076007366\n",
      "Step: 1920, train/learning_rate: 4.771537351189181e-05\n",
      "Step: 1920, train/epoch: 0.45692527294158936\n",
      "Step: 1930, train/loss: 0.0\n",
      "Step: 1930, train/grad_norm: 0.00030382428667508066\n",
      "Step: 1930, train/learning_rate: 4.7703473683213815e-05\n",
      "Step: 1930, train/epoch: 0.4593051075935364\n",
      "Step: 1940, train/loss: 0.0015999999595806003\n",
      "Step: 1940, train/grad_norm: 0.0005796152399852872\n",
      "Step: 1940, train/learning_rate: 4.769157385453582e-05\n",
      "Step: 1940, train/epoch: 0.461684912443161\n",
      "Step: 1950, train/loss: 0.0\n",
      "Step: 1950, train/grad_norm: 0.001721673645079136\n",
      "Step: 1950, train/learning_rate: 4.767967766383663e-05\n",
      "Step: 1950, train/epoch: 0.46406471729278564\n",
      "Step: 1960, train/loss: 0.16329999268054962\n",
      "Step: 1960, train/grad_norm: 0.0005799878272227943\n",
      "Step: 1960, train/learning_rate: 4.766777783515863e-05\n",
      "Step: 1960, train/epoch: 0.46644455194473267\n",
      "Step: 1970, train/loss: 0.0\n",
      "Step: 1970, train/grad_norm: 0.00037964171497151256\n",
      "Step: 1970, train/learning_rate: 4.7655878006480634e-05\n",
      "Step: 1970, train/epoch: 0.4688243567943573\n",
      "Step: 1980, train/loss: 0.00019999999494757503\n",
      "Step: 1980, train/grad_norm: 0.007545245811343193\n",
      "Step: 1980, train/learning_rate: 4.764397817780264e-05\n",
      "Step: 1980, train/epoch: 0.4712041914463043\n",
      "Step: 1990, train/loss: 0.0\n",
      "Step: 1990, train/grad_norm: 0.03569702431559563\n",
      "Step: 1990, train/learning_rate: 4.763207834912464e-05\n",
      "Step: 1990, train/epoch: 0.47358399629592896\n",
      "Step: 2000, train/loss: 0.0\n",
      "Step: 2000, train/grad_norm: 1.1849424481624737e-05\n",
      "Step: 2000, train/learning_rate: 4.762018215842545e-05\n",
      "Step: 2000, train/epoch: 0.475963830947876\n",
      "Step: 2010, train/loss: 0.15160000324249268\n",
      "Step: 2010, train/grad_norm: 0.0005489444592967629\n",
      "Step: 2010, train/learning_rate: 4.760828232974745e-05\n",
      "Step: 2010, train/epoch: 0.4783436357975006\n",
      "Step: 2020, train/loss: 0.00019999999494757503\n",
      "Step: 2020, train/grad_norm: 0.010769590735435486\n",
      "Step: 2020, train/learning_rate: 4.7596382501069456e-05\n",
      "Step: 2020, train/epoch: 0.48072347044944763\n",
      "Step: 2030, train/loss: 9.999999747378752e-05\n",
      "Step: 2030, train/grad_norm: 0.00032825436210259795\n",
      "Step: 2030, train/learning_rate: 4.758448267239146e-05\n",
      "Step: 2030, train/epoch: 0.48310327529907227\n",
      "Step: 2040, train/loss: 0.0005000000237487257\n",
      "Step: 2040, train/grad_norm: 0.0036793469917029142\n",
      "Step: 2040, train/learning_rate: 4.757258284371346e-05\n",
      "Step: 2040, train/epoch: 0.4854831099510193\n",
      "Step: 2050, train/loss: 0.12200000137090683\n",
      "Step: 2050, train/grad_norm: 0.16312475502490997\n",
      "Step: 2050, train/learning_rate: 4.756068665301427e-05\n",
      "Step: 2050, train/epoch: 0.4878629148006439\n",
      "Step: 2060, train/loss: 0.0017999999690800905\n",
      "Step: 2060, train/grad_norm: 0.22075073421001434\n",
      "Step: 2060, train/learning_rate: 4.7548786824336275e-05\n",
      "Step: 2060, train/epoch: 0.49024274945259094\n",
      "Step: 2070, train/loss: 9.999999747378752e-05\n",
      "Step: 2070, train/grad_norm: 0.0024276659823954105\n",
      "Step: 2070, train/learning_rate: 4.753688699565828e-05\n",
      "Step: 2070, train/epoch: 0.4926225543022156\n",
      "Step: 2080, train/loss: 0.0\n",
      "Step: 2080, train/grad_norm: 0.000832952035125345\n",
      "Step: 2080, train/learning_rate: 4.752498716698028e-05\n",
      "Step: 2080, train/epoch: 0.4950023889541626\n",
      "Step: 2090, train/loss: 0.017899999395012856\n",
      "Step: 2090, train/grad_norm: 0.0002580741420388222\n",
      "Step: 2090, train/learning_rate: 4.7513087338302284e-05\n",
      "Step: 2090, train/epoch: 0.49738219380378723\n",
      "Step: 2100, train/loss: 9.999999747378752e-05\n",
      "Step: 2100, train/grad_norm: 0.00023257138673216105\n",
      "Step: 2100, train/learning_rate: 4.7501191147603095e-05\n",
      "Step: 2100, train/epoch: 0.49976202845573425\n",
      "Step: 2110, train/loss: 0.0\n",
      "Step: 2110, train/grad_norm: 0.0007201738771982491\n",
      "Step: 2110, train/learning_rate: 4.74892913189251e-05\n",
      "Step: 2110, train/epoch: 0.5021418333053589\n",
      "Step: 2120, train/loss: 0.030799999833106995\n",
      "Step: 2120, train/grad_norm: 0.0020954154897481203\n",
      "Step: 2120, train/learning_rate: 4.74773914902471e-05\n",
      "Step: 2120, train/epoch: 0.5045216679573059\n",
      "Step: 2130, train/loss: 9.999999747378752e-05\n",
      "Step: 2130, train/grad_norm: 0.0008429082226939499\n",
      "Step: 2130, train/learning_rate: 4.7465491661569104e-05\n",
      "Step: 2130, train/epoch: 0.5069015026092529\n",
      "Step: 2140, train/loss: 0.00019999999494757503\n",
      "Step: 2140, train/grad_norm: 2.7237856556894258e-05\n",
      "Step: 2140, train/learning_rate: 4.7453591832891107e-05\n",
      "Step: 2140, train/epoch: 0.5092812776565552\n",
      "Step: 2150, train/loss: 0.0\n",
      "Step: 2150, train/grad_norm: 0.0008126593311317265\n",
      "Step: 2150, train/learning_rate: 4.744169564219192e-05\n",
      "Step: 2150, train/epoch: 0.5116611123085022\n",
      "Step: 2160, train/loss: 0.0\n",
      "Step: 2160, train/grad_norm: 2.274662983836606e-05\n",
      "Step: 2160, train/learning_rate: 4.742979581351392e-05\n",
      "Step: 2160, train/epoch: 0.5140409469604492\n",
      "Step: 2170, train/loss: 0.010999999940395355\n",
      "Step: 2170, train/grad_norm: 5.772015720140189e-05\n",
      "Step: 2170, train/learning_rate: 4.741789598483592e-05\n",
      "Step: 2170, train/epoch: 0.5164207816123962\n",
      "Step: 2180, train/loss: 0.0\n",
      "Step: 2180, train/grad_norm: 0.003130934201180935\n",
      "Step: 2180, train/learning_rate: 4.7405996156157926e-05\n",
      "Step: 2180, train/epoch: 0.5188005566596985\n",
      "Step: 2190, train/loss: 0.0\n",
      "Step: 2190, train/grad_norm: 4.585676194324151e-08\n",
      "Step: 2190, train/learning_rate: 4.739409632747993e-05\n",
      "Step: 2190, train/epoch: 0.5211803913116455\n",
      "Step: 2200, train/loss: 0.0\n",
      "Step: 2200, train/grad_norm: 5.356303518055938e-06\n",
      "Step: 2200, train/learning_rate: 4.738220013678074e-05\n",
      "Step: 2200, train/epoch: 0.5235602259635925\n",
      "Step: 2210, train/loss: 0.0\n",
      "Step: 2210, train/grad_norm: 1.531591624370776e-05\n",
      "Step: 2210, train/learning_rate: 4.737030030810274e-05\n",
      "Step: 2210, train/epoch: 0.5259400010108948\n",
      "Step: 2220, train/loss: 0.0\n",
      "Step: 2220, train/grad_norm: 4.473395165405236e-06\n",
      "Step: 2220, train/learning_rate: 4.7358400479424745e-05\n",
      "Step: 2220, train/epoch: 0.5283198356628418\n",
      "Step: 2230, train/loss: 0.0\n",
      "Step: 2230, train/grad_norm: 2.452513285788882e-07\n",
      "Step: 2230, train/learning_rate: 4.734650065074675e-05\n",
      "Step: 2230, train/epoch: 0.5306996703147888\n",
      "Step: 2240, train/loss: 0.0\n",
      "Step: 2240, train/grad_norm: 8.187104685930535e-05\n",
      "Step: 2240, train/learning_rate: 4.733460082206875e-05\n",
      "Step: 2240, train/epoch: 0.5330795049667358\n",
      "Step: 2250, train/loss: 0.0\n",
      "Step: 2250, train/grad_norm: 8.067369208220043e-07\n",
      "Step: 2250, train/learning_rate: 4.732270463136956e-05\n",
      "Step: 2250, train/epoch: 0.5354592800140381\n",
      "Step: 2260, train/loss: 0.0\n",
      "Step: 2260, train/grad_norm: 0.0007728057098574936\n",
      "Step: 2260, train/learning_rate: 4.7310804802691564e-05\n",
      "Step: 2260, train/epoch: 0.5378391146659851\n",
      "Step: 2270, train/loss: 0.0\n",
      "Step: 2270, train/grad_norm: 0.0012730290181934834\n",
      "Step: 2270, train/learning_rate: 4.729890497401357e-05\n",
      "Step: 2270, train/epoch: 0.5402189493179321\n",
      "Step: 2280, train/loss: 0.0\n",
      "Step: 2280, train/grad_norm: 9.678396054368932e-06\n",
      "Step: 2280, train/learning_rate: 4.728700514533557e-05\n",
      "Step: 2280, train/epoch: 0.5425987839698792\n",
      "Step: 2290, train/loss: 0.06260000169277191\n",
      "Step: 2290, train/grad_norm: 0.0007357214926742017\n",
      "Step: 2290, train/learning_rate: 4.727510531665757e-05\n",
      "Step: 2290, train/epoch: 0.5449785590171814\n",
      "Step: 2300, train/loss: 0.0\n",
      "Step: 2300, train/grad_norm: 2.8256879886612296e-05\n",
      "Step: 2300, train/learning_rate: 4.726320912595838e-05\n",
      "Step: 2300, train/epoch: 0.5473583936691284\n",
      "Step: 2310, train/loss: 0.0\n",
      "Step: 2310, train/grad_norm: 0.0015931379748508334\n",
      "Step: 2310, train/learning_rate: 4.7251309297280386e-05\n",
      "Step: 2310, train/epoch: 0.5497382283210754\n",
      "Step: 2320, train/loss: 0.0\n",
      "Step: 2320, train/grad_norm: 2.414074344869732e-07\n",
      "Step: 2320, train/learning_rate: 4.723940946860239e-05\n",
      "Step: 2320, train/epoch: 0.5521180629730225\n",
      "Step: 2330, train/loss: 0.0\n",
      "Step: 2330, train/grad_norm: 8.030563549255021e-06\n",
      "Step: 2330, train/learning_rate: 4.722750963992439e-05\n",
      "Step: 2330, train/epoch: 0.5544978380203247\n",
      "Step: 2340, train/loss: 0.0\n",
      "Step: 2340, train/grad_norm: 1.6527740172023186e-06\n",
      "Step: 2340, train/learning_rate: 4.7215609811246395e-05\n",
      "Step: 2340, train/epoch: 0.5568776726722717\n",
      "Step: 2350, train/loss: 9.999999747378752e-05\n",
      "Step: 2350, train/grad_norm: 2.8618549663406156e-07\n",
      "Step: 2350, train/learning_rate: 4.7203713620547205e-05\n",
      "Step: 2350, train/epoch: 0.5592575073242188\n",
      "Step: 2360, train/loss: 9.999999747378752e-05\n",
      "Step: 2360, train/grad_norm: 2.7099706585431704e-06\n",
      "Step: 2360, train/learning_rate: 4.719181379186921e-05\n",
      "Step: 2360, train/epoch: 0.5616373419761658\n",
      "Step: 2370, train/loss: 0.0\n",
      "Step: 2370, train/grad_norm: 8.689782049486894e-08\n",
      "Step: 2370, train/learning_rate: 4.717991396319121e-05\n",
      "Step: 2370, train/epoch: 0.564017117023468\n",
      "Step: 2380, train/loss: 0.16329999268054962\n",
      "Step: 2380, train/grad_norm: 4.691835897574492e-07\n",
      "Step: 2380, train/learning_rate: 4.7168014134513214e-05\n",
      "Step: 2380, train/epoch: 0.566396951675415\n",
      "Step: 2390, train/loss: 0.0\n",
      "Step: 2390, train/grad_norm: 1.7974642105400562e-05\n",
      "Step: 2390, train/learning_rate: 4.7156117943814024e-05\n",
      "Step: 2390, train/epoch: 0.5687767863273621\n",
      "Step: 2400, train/loss: 0.0\n",
      "Step: 2400, train/grad_norm: 0.0008259156602434814\n",
      "Step: 2400, train/learning_rate: 4.714421811513603e-05\n",
      "Step: 2400, train/epoch: 0.5711566209793091\n",
      "Step: 2410, train/loss: 0.0\n",
      "Step: 2410, train/grad_norm: 0.0006871043588034809\n",
      "Step: 2410, train/learning_rate: 4.713231828645803e-05\n",
      "Step: 2410, train/epoch: 0.5735363960266113\n",
      "Step: 2420, train/loss: 0.0\n",
      "Step: 2420, train/grad_norm: 0.00043363095028325915\n",
      "Step: 2420, train/learning_rate: 4.712041845778003e-05\n",
      "Step: 2420, train/epoch: 0.5759162306785583\n",
      "Step: 2430, train/loss: 0.0007999999797903001\n",
      "Step: 2430, train/grad_norm: 0.00022896187147125602\n",
      "Step: 2430, train/learning_rate: 4.7108518629102036e-05\n",
      "Step: 2430, train/epoch: 0.5782960653305054\n",
      "Step: 2440, train/loss: 0.0\n",
      "Step: 2440, train/grad_norm: 0.0016853531124070287\n",
      "Step: 2440, train/learning_rate: 4.7096622438402846e-05\n",
      "Step: 2440, train/epoch: 0.5806758403778076\n",
      "Step: 2450, train/loss: 0.0\n",
      "Step: 2450, train/grad_norm: 8.06106072559487e-06\n",
      "Step: 2450, train/learning_rate: 4.708472260972485e-05\n",
      "Step: 2450, train/epoch: 0.5830556750297546\n",
      "Step: 2460, train/loss: 0.0\n",
      "Step: 2460, train/grad_norm: 4.35596348324907e-06\n",
      "Step: 2460, train/learning_rate: 4.707282278104685e-05\n",
      "Step: 2460, train/epoch: 0.5854355096817017\n",
      "Step: 2470, train/loss: 0.0\n",
      "Step: 2470, train/grad_norm: 8.188621904992033e-06\n",
      "Step: 2470, train/learning_rate: 4.7060922952368855e-05\n",
      "Step: 2470, train/epoch: 0.5878153443336487\n",
      "Step: 2480, train/loss: 0.0\n",
      "Step: 2480, train/grad_norm: 0.0005180759471841156\n",
      "Step: 2480, train/learning_rate: 4.704902312369086e-05\n",
      "Step: 2480, train/epoch: 0.5901951193809509\n",
      "Step: 2490, train/loss: 0.0\n",
      "Step: 2490, train/grad_norm: 3.7560805594694102e-06\n",
      "Step: 2490, train/learning_rate: 4.703712693299167e-05\n",
      "Step: 2490, train/epoch: 0.592574954032898\n",
      "Step: 2500, train/loss: 0.0\n",
      "Step: 2500, train/grad_norm: 1.0208328603766859e-05\n",
      "Step: 2500, train/learning_rate: 4.702522710431367e-05\n",
      "Step: 2500, train/epoch: 0.594954788684845\n",
      "Step: 2510, train/loss: 0.0\n",
      "Step: 2510, train/grad_norm: 1.2323420151005848e-07\n",
      "Step: 2510, train/learning_rate: 4.7013327275635675e-05\n",
      "Step: 2510, train/epoch: 0.597334623336792\n",
      "Step: 2520, train/loss: 0.0003000000142492354\n",
      "Step: 2520, train/grad_norm: 3.4348076383139414e-07\n",
      "Step: 2520, train/learning_rate: 4.700142744695768e-05\n",
      "Step: 2520, train/epoch: 0.5997143983840942\n",
      "Step: 2530, train/loss: 0.0\n",
      "Step: 2530, train/grad_norm: 3.1258106901077554e-05\n",
      "Step: 2530, train/learning_rate: 4.698952761827968e-05\n",
      "Step: 2530, train/epoch: 0.6020942330360413\n",
      "Step: 2540, train/loss: 0.0\n",
      "Step: 2540, train/grad_norm: 2.9065776629977336e-07\n",
      "Step: 2540, train/learning_rate: 4.697763142758049e-05\n",
      "Step: 2540, train/epoch: 0.6044740676879883\n",
      "Step: 2550, train/loss: 0.0\n",
      "Step: 2550, train/grad_norm: 4.400095576784224e-08\n",
      "Step: 2550, train/learning_rate: 4.6965731598902494e-05\n",
      "Step: 2550, train/epoch: 0.6068539023399353\n",
      "Step: 2560, train/loss: 0.0\n",
      "Step: 2560, train/grad_norm: 1.9426651931553351e-07\n",
      "Step: 2560, train/learning_rate: 4.69538317702245e-05\n",
      "Step: 2560, train/epoch: 0.6092336773872375\n",
      "Step: 2570, train/loss: 0.00019999999494757503\n",
      "Step: 2570, train/grad_norm: 4.531178987576823e-08\n",
      "Step: 2570, train/learning_rate: 4.69419319415465e-05\n",
      "Step: 2570, train/epoch: 0.6116135120391846\n",
      "Step: 2580, train/loss: 0.00019999999494757503\n",
      "Step: 2580, train/grad_norm: 2.1710420128329133e-07\n",
      "Step: 2580, train/learning_rate: 4.69300321128685e-05\n",
      "Step: 2580, train/epoch: 0.6139933466911316\n",
      "Step: 2590, train/loss: 0.0\n",
      "Step: 2590, train/grad_norm: 9.614762319642978e-08\n",
      "Step: 2590, train/learning_rate: 4.691813592216931e-05\n",
      "Step: 2590, train/epoch: 0.6163731813430786\n",
      "Step: 2600, train/loss: 0.0\n",
      "Step: 2600, train/grad_norm: 4.6440311507467413e-07\n",
      "Step: 2600, train/learning_rate: 4.6906236093491316e-05\n",
      "Step: 2600, train/epoch: 0.6187529563903809\n",
      "Step: 2610, train/loss: 0.28630000352859497\n",
      "Step: 2610, train/grad_norm: 71.6528549194336\n",
      "Step: 2610, train/learning_rate: 4.689433626481332e-05\n",
      "Step: 2610, train/epoch: 0.6211327910423279\n",
      "Step: 2620, train/loss: 0.19140000641345978\n",
      "Step: 2620, train/grad_norm: 0.0002150239743059501\n",
      "Step: 2620, train/learning_rate: 4.688243643613532e-05\n",
      "Step: 2620, train/epoch: 0.6235126256942749\n",
      "Step: 2630, train/loss: 0.0\n",
      "Step: 2630, train/grad_norm: 0.004896444734185934\n",
      "Step: 2630, train/learning_rate: 4.6870536607457325e-05\n",
      "Step: 2630, train/epoch: 0.6258924603462219\n",
      "Step: 2640, train/loss: 9.999999747378752e-05\n",
      "Step: 2640, train/grad_norm: 0.005530402529984713\n",
      "Step: 2640, train/learning_rate: 4.6858640416758135e-05\n",
      "Step: 2640, train/epoch: 0.6282722353935242\n",
      "Step: 2650, train/loss: 0.0003000000142492354\n",
      "Step: 2650, train/grad_norm: 0.0016552318120375276\n",
      "Step: 2650, train/learning_rate: 4.684674058808014e-05\n",
      "Step: 2650, train/epoch: 0.6306520700454712\n",
      "Step: 2660, train/loss: 0.061400000005960464\n",
      "Step: 2660, train/grad_norm: 0.008623047731816769\n",
      "Step: 2660, train/learning_rate: 4.683484075940214e-05\n",
      "Step: 2660, train/epoch: 0.6330319046974182\n",
      "Step: 2670, train/loss: 0.0\n",
      "Step: 2670, train/grad_norm: 0.0007556799100711942\n",
      "Step: 2670, train/learning_rate: 4.6822940930724144e-05\n",
      "Step: 2670, train/epoch: 0.6354116797447205\n",
      "Step: 2680, train/loss: 0.0\n",
      "Step: 2680, train/grad_norm: 0.01024163793772459\n",
      "Step: 2680, train/learning_rate: 4.681104110204615e-05\n",
      "Step: 2680, train/epoch: 0.6377915143966675\n",
      "Step: 2690, train/loss: 0.0\n",
      "Step: 2690, train/grad_norm: 0.00023859362408984452\n",
      "Step: 2690, train/learning_rate: 4.679914491134696e-05\n",
      "Step: 2690, train/epoch: 0.6401713490486145\n",
      "Step: 2700, train/loss: 0.0\n",
      "Step: 2700, train/grad_norm: 0.0006022009765729308\n",
      "Step: 2700, train/learning_rate: 4.678724508266896e-05\n",
      "Step: 2700, train/epoch: 0.6425511837005615\n",
      "Step: 2710, train/loss: 0.0\n",
      "Step: 2710, train/grad_norm: 7.776650454616174e-05\n",
      "Step: 2710, train/learning_rate: 4.677534525399096e-05\n",
      "Step: 2710, train/epoch: 0.6449309587478638\n",
      "Step: 2720, train/loss: 0.0\n",
      "Step: 2720, train/grad_norm: 0.001655565109103918\n",
      "Step: 2720, train/learning_rate: 4.6763445425312966e-05\n",
      "Step: 2720, train/epoch: 0.6473107933998108\n",
      "Step: 2730, train/loss: 0.0\n",
      "Step: 2730, train/grad_norm: 5.3418953029904515e-05\n",
      "Step: 2730, train/learning_rate: 4.675154559663497e-05\n",
      "Step: 2730, train/epoch: 0.6496906280517578\n",
      "Step: 2740, train/loss: 0.0\n",
      "Step: 2740, train/grad_norm: 0.00043821503641083837\n",
      "Step: 2740, train/learning_rate: 4.673964940593578e-05\n",
      "Step: 2740, train/epoch: 0.6520704627037048\n",
      "Step: 2750, train/loss: 0.07419999688863754\n",
      "Step: 2750, train/grad_norm: 0.00029990929760970175\n",
      "Step: 2750, train/learning_rate: 4.672774957725778e-05\n",
      "Step: 2750, train/epoch: 0.6544502377510071\n",
      "Step: 2760, train/loss: 0.0003000000142492354\n",
      "Step: 2760, train/grad_norm: 0.012372607365250587\n",
      "Step: 2760, train/learning_rate: 4.6715849748579785e-05\n",
      "Step: 2760, train/epoch: 0.6568300724029541\n",
      "Step: 2770, train/loss: 0.0\n",
      "Step: 2770, train/grad_norm: 0.0002635618729982525\n",
      "Step: 2770, train/learning_rate: 4.670394991990179e-05\n",
      "Step: 2770, train/epoch: 0.6592099070549011\n",
      "Step: 2780, train/loss: 0.0003000000142492354\n",
      "Step: 2780, train/grad_norm: 7.995846317498945e-06\n",
      "Step: 2780, train/learning_rate: 4.669205009122379e-05\n",
      "Step: 2780, train/epoch: 0.6615897417068481\n",
      "Step: 2790, train/loss: 0.11169999837875366\n",
      "Step: 2790, train/grad_norm: 8.8528409492028e-08\n",
      "Step: 2790, train/learning_rate: 4.66801539005246e-05\n",
      "Step: 2790, train/epoch: 0.6639695167541504\n",
      "Step: 2800, train/loss: 0.0\n",
      "Step: 2800, train/grad_norm: 3.04221612168476e-05\n",
      "Step: 2800, train/learning_rate: 4.6668254071846604e-05\n",
      "Step: 2800, train/epoch: 0.6663493514060974\n",
      "Step: 2810, train/loss: 0.0\n",
      "Step: 2810, train/grad_norm: 5.503752618096769e-05\n",
      "Step: 2810, train/learning_rate: 4.665635424316861e-05\n",
      "Step: 2810, train/epoch: 0.6687291860580444\n",
      "Step: 2820, train/loss: 0.0\n",
      "Step: 2820, train/grad_norm: 5.498406972037628e-05\n",
      "Step: 2820, train/learning_rate: 4.664445441449061e-05\n",
      "Step: 2820, train/epoch: 0.6711090207099915\n",
      "Step: 2830, train/loss: 0.0\n",
      "Step: 2830, train/grad_norm: 1.2939776183884533e-07\n",
      "Step: 2830, train/learning_rate: 4.663255458581261e-05\n",
      "Step: 2830, train/epoch: 0.6734887957572937\n",
      "Step: 2840, train/loss: 0.0\n",
      "Step: 2840, train/grad_norm: 6.569183460669592e-05\n",
      "Step: 2840, train/learning_rate: 4.6620658395113423e-05\n",
      "Step: 2840, train/epoch: 0.6758686304092407\n",
      "Step: 2850, train/loss: 0.0\n",
      "Step: 2850, train/grad_norm: 2.5152732519018173e-07\n",
      "Step: 2850, train/learning_rate: 4.6608758566435426e-05\n",
      "Step: 2850, train/epoch: 0.6782484650611877\n",
      "Step: 2860, train/loss: 0.0\n",
      "Step: 2860, train/grad_norm: 4.779678420163691e-05\n",
      "Step: 2860, train/learning_rate: 4.659685873775743e-05\n",
      "Step: 2860, train/epoch: 0.6806282997131348\n",
      "Step: 2870, train/loss: 0.03669999912381172\n",
      "Step: 2870, train/grad_norm: 0.0069759744219481945\n",
      "Step: 2870, train/learning_rate: 4.658495890907943e-05\n",
      "Step: 2870, train/epoch: 0.683008074760437\n",
      "Step: 2880, train/loss: 0.25780001282691956\n",
      "Step: 2880, train/grad_norm: 0.00010647274029906839\n",
      "Step: 2880, train/learning_rate: 4.6573059080401435e-05\n",
      "Step: 2880, train/epoch: 0.685387909412384\n",
      "Step: 2890, train/loss: 0.0\n",
      "Step: 2890, train/grad_norm: 1.5526284187217243e-05\n",
      "Step: 2890, train/learning_rate: 4.6561162889702246e-05\n",
      "Step: 2890, train/epoch: 0.687767744064331\n",
      "Step: 2900, train/loss: 0.0052999998442828655\n",
      "Step: 2900, train/grad_norm: 0.0002406770654488355\n",
      "Step: 2900, train/learning_rate: 4.654926306102425e-05\n",
      "Step: 2900, train/epoch: 0.6901475191116333\n",
      "Step: 2910, train/loss: 0.0\n",
      "Step: 2910, train/grad_norm: 0.0001278195413760841\n",
      "Step: 2910, train/learning_rate: 4.653736323234625e-05\n",
      "Step: 2910, train/epoch: 0.6925273537635803\n",
      "Step: 2920, train/loss: 0.0\n",
      "Step: 2920, train/grad_norm: 0.00012518203584477305\n",
      "Step: 2920, train/learning_rate: 4.6525463403668255e-05\n",
      "Step: 2920, train/epoch: 0.6949071884155273\n",
      "Step: 2930, train/loss: 0.0\n",
      "Step: 2930, train/grad_norm: 0.00016982798115350306\n",
      "Step: 2930, train/learning_rate: 4.651356357499026e-05\n",
      "Step: 2930, train/epoch: 0.6972870230674744\n",
      "Step: 2940, train/loss: 0.20309999585151672\n",
      "Step: 2940, train/grad_norm: 5.308316394803114e-05\n",
      "Step: 2940, train/learning_rate: 4.650166738429107e-05\n",
      "Step: 2940, train/epoch: 0.6996667981147766\n",
      "Step: 2950, train/loss: 0.0\n",
      "Step: 2950, train/grad_norm: 0.0036615042481571436\n",
      "Step: 2950, train/learning_rate: 4.648976755561307e-05\n",
      "Step: 2950, train/epoch: 0.7020466327667236\n",
      "Step: 2960, train/loss: 0.2624000012874603\n",
      "Step: 2960, train/grad_norm: 0.0006728698499500751\n",
      "Step: 2960, train/learning_rate: 4.6477867726935074e-05\n",
      "Step: 2960, train/epoch: 0.7044264674186707\n",
      "Step: 2970, train/loss: 0.0008999999845400453\n",
      "Step: 2970, train/grad_norm: 0.011489981785416603\n",
      "Step: 2970, train/learning_rate: 4.646596789825708e-05\n",
      "Step: 2970, train/epoch: 0.7068063020706177\n",
      "Step: 2980, train/loss: 0.010900000110268593\n",
      "Step: 2980, train/grad_norm: 0.023259388282895088\n",
      "Step: 2980, train/learning_rate: 4.645406806957908e-05\n",
      "Step: 2980, train/epoch: 0.7091860771179199\n",
      "Step: 2990, train/loss: 0.00039999998989515007\n",
      "Step: 2990, train/grad_norm: 0.02814267948269844\n",
      "Step: 2990, train/learning_rate: 4.644217187887989e-05\n",
      "Step: 2990, train/epoch: 0.7115659117698669\n",
      "Step: 3000, train/loss: 0.00019999999494757503\n",
      "Step: 3000, train/grad_norm: 0.0022896118462085724\n",
      "Step: 3000, train/learning_rate: 4.643027205020189e-05\n",
      "Step: 3000, train/epoch: 0.713945746421814\n",
      "Step: 3010, train/loss: 0.0\n",
      "Step: 3010, train/grad_norm: 0.00824197381734848\n",
      "Step: 3010, train/learning_rate: 4.6418372221523896e-05\n",
      "Step: 3010, train/epoch: 0.716325581073761\n",
      "Step: 3020, train/loss: 0.0\n",
      "Step: 3020, train/grad_norm: 9.348499588668346e-05\n",
      "Step: 3020, train/learning_rate: 4.64064723928459e-05\n",
      "Step: 3020, train/epoch: 0.7187053561210632\n",
      "Step: 3030, train/loss: 0.0\n",
      "Step: 3030, train/grad_norm: 0.00024064010358415544\n",
      "Step: 3030, train/learning_rate: 4.63945725641679e-05\n",
      "Step: 3030, train/epoch: 0.7210851907730103\n",
      "Step: 3040, train/loss: 0.0471000000834465\n",
      "Step: 3040, train/grad_norm: 0.0006058138678781688\n",
      "Step: 3040, train/learning_rate: 4.638267637346871e-05\n",
      "Step: 3040, train/epoch: 0.7234650254249573\n",
      "Step: 3050, train/loss: 0.0\n",
      "Step: 3050, train/grad_norm: 0.0027494796086102724\n",
      "Step: 3050, train/learning_rate: 4.6370776544790715e-05\n",
      "Step: 3050, train/epoch: 0.7258448600769043\n",
      "Step: 3060, train/loss: 0.0020000000949949026\n",
      "Step: 3060, train/grad_norm: 0.00020905355631839484\n",
      "Step: 3060, train/learning_rate: 4.635887671611272e-05\n",
      "Step: 3060, train/epoch: 0.7282246351242065\n",
      "Step: 3070, train/loss: 0.0\n",
      "Step: 3070, train/grad_norm: 0.10521584749221802\n",
      "Step: 3070, train/learning_rate: 4.634697688743472e-05\n",
      "Step: 3070, train/epoch: 0.7306044697761536\n",
      "Step: 3080, train/loss: 9.999999747378752e-05\n",
      "Step: 3080, train/grad_norm: 0.3419620096683502\n",
      "Step: 3080, train/learning_rate: 4.6335077058756724e-05\n",
      "Step: 3080, train/epoch: 0.7329843044281006\n",
      "Step: 3090, train/loss: 0.07500000298023224\n",
      "Step: 3090, train/grad_norm: 8.915014041122049e-05\n",
      "Step: 3090, train/learning_rate: 4.6323180868057534e-05\n",
      "Step: 3090, train/epoch: 0.7353641390800476\n",
      "Step: 3100, train/loss: 0.0\n",
      "Step: 3100, train/grad_norm: 8.479204552713782e-05\n",
      "Step: 3100, train/learning_rate: 4.631128103937954e-05\n",
      "Step: 3100, train/epoch: 0.7377439141273499\n",
      "Step: 3110, train/loss: 0.0\n",
      "Step: 3110, train/grad_norm: 5.325964593794197e-05\n",
      "Step: 3110, train/learning_rate: 4.629938121070154e-05\n",
      "Step: 3110, train/epoch: 0.7401237487792969\n",
      "Step: 3120, train/loss: 0.0\n",
      "Step: 3120, train/grad_norm: 8.165073813870549e-05\n",
      "Step: 3120, train/learning_rate: 4.628748138202354e-05\n",
      "Step: 3120, train/epoch: 0.7425035834312439\n",
      "Step: 3130, train/loss: 0.11410000175237656\n",
      "Step: 3130, train/grad_norm: 0.0012189409462735057\n",
      "Step: 3130, train/learning_rate: 4.6275581553345546e-05\n",
      "Step: 3130, train/epoch: 0.7448834180831909\n",
      "Step: 3140, train/loss: 0.0\n",
      "Step: 3140, train/grad_norm: 0.0008359839557670057\n",
      "Step: 3140, train/learning_rate: 4.6263685362646356e-05\n",
      "Step: 3140, train/epoch: 0.7472631931304932\n",
      "Step: 3150, train/loss: 0.0\n",
      "Step: 3150, train/grad_norm: 0.0006532088737003505\n",
      "Step: 3150, train/learning_rate: 4.625178553396836e-05\n",
      "Step: 3150, train/epoch: 0.7496430277824402\n",
      "Step: 3160, train/loss: 9.999999747378752e-05\n",
      "Step: 3160, train/grad_norm: 0.007253190968185663\n",
      "Step: 3160, train/learning_rate: 4.623988570529036e-05\n",
      "Step: 3160, train/epoch: 0.7520228624343872\n",
      "Step: 3170, train/loss: 0.0\n",
      "Step: 3170, train/grad_norm: 0.00011157447443110868\n",
      "Step: 3170, train/learning_rate: 4.6227985876612365e-05\n",
      "Step: 3170, train/epoch: 0.7544026374816895\n",
      "Step: 3180, train/loss: 0.0\n",
      "Step: 3180, train/grad_norm: 4.3194151658099145e-05\n",
      "Step: 3180, train/learning_rate: 4.621608604793437e-05\n",
      "Step: 3180, train/epoch: 0.7567824721336365\n",
      "Step: 3190, train/loss: 0.0\n",
      "Step: 3190, train/grad_norm: 0.005174208898097277\n",
      "Step: 3190, train/learning_rate: 4.620418985723518e-05\n",
      "Step: 3190, train/epoch: 0.7591623067855835\n",
      "Step: 3200, train/loss: 0.0\n",
      "Step: 3200, train/grad_norm: 8.57395298226038e-06\n",
      "Step: 3200, train/learning_rate: 4.619229002855718e-05\n",
      "Step: 3200, train/epoch: 0.7615421414375305\n",
      "Step: 3210, train/loss: 0.06639999896287918\n",
      "Step: 3210, train/grad_norm: 0.001327051781117916\n",
      "Step: 3210, train/learning_rate: 4.6180390199879184e-05\n",
      "Step: 3210, train/epoch: 0.7639219164848328\n",
      "Step: 3220, train/loss: 0.0\n",
      "Step: 3220, train/grad_norm: 0.03593937307596207\n",
      "Step: 3220, train/learning_rate: 4.616849037120119e-05\n",
      "Step: 3220, train/epoch: 0.7663017511367798\n",
      "Step: 3230, train/loss: 0.0\n",
      "Step: 3230, train/grad_norm: 0.00011349189298925921\n",
      "Step: 3230, train/learning_rate: 4.615659054252319e-05\n",
      "Step: 3230, train/epoch: 0.7686815857887268\n",
      "Step: 3240, train/loss: 0.125\n",
      "Step: 3240, train/grad_norm: 0.0005200192681513727\n",
      "Step: 3240, train/learning_rate: 4.6144694351824e-05\n",
      "Step: 3240, train/epoch: 0.7710614204406738\n",
      "Step: 3250, train/loss: 0.0\n",
      "Step: 3250, train/grad_norm: 0.00048350539873354137\n",
      "Step: 3250, train/learning_rate: 4.6132794523146003e-05\n",
      "Step: 3250, train/epoch: 0.7734411954879761\n",
      "Step: 3260, train/loss: 0.0\n",
      "Step: 3260, train/grad_norm: 0.004321753513067961\n",
      "Step: 3260, train/learning_rate: 4.6120894694468006e-05\n",
      "Step: 3260, train/epoch: 0.7758210301399231\n",
      "Step: 3270, train/loss: 0.028300000354647636\n",
      "Step: 3270, train/grad_norm: 0.0008542308351024985\n",
      "Step: 3270, train/learning_rate: 4.610899486579001e-05\n",
      "Step: 3270, train/epoch: 0.7782008647918701\n",
      "Step: 3280, train/loss: 0.0024999999441206455\n",
      "Step: 3280, train/grad_norm: 0.004449285566806793\n",
      "Step: 3280, train/learning_rate: 4.609709503711201e-05\n",
      "Step: 3280, train/epoch: 0.7805806994438171\n",
      "Step: 3290, train/loss: 0.0\n",
      "Step: 3290, train/grad_norm: 0.0031101687345653772\n",
      "Step: 3290, train/learning_rate: 4.608519884641282e-05\n",
      "Step: 3290, train/epoch: 0.7829604744911194\n",
      "Step: 3300, train/loss: 0.005400000140070915\n",
      "Step: 3300, train/grad_norm: 2.774201857391745e-05\n",
      "Step: 3300, train/learning_rate: 4.6073299017734826e-05\n",
      "Step: 3300, train/epoch: 0.7853403091430664\n",
      "Step: 3310, train/loss: 0.0\n",
      "Step: 3310, train/grad_norm: 7.218847895273939e-05\n",
      "Step: 3310, train/learning_rate: 4.606139918905683e-05\n",
      "Step: 3310, train/epoch: 0.7877201437950134\n",
      "Step: 3320, train/loss: 0.22040000557899475\n",
      "Step: 3320, train/grad_norm: 0.06344319880008698\n",
      "Step: 3320, train/learning_rate: 4.604949936037883e-05\n",
      "Step: 3320, train/epoch: 0.7900999784469604\n",
      "Step: 3330, train/loss: 0.0015999999595806003\n",
      "Step: 3330, train/grad_norm: 0.05614548549056053\n",
      "Step: 3330, train/learning_rate: 4.6037599531700835e-05\n",
      "Step: 3330, train/epoch: 0.7924797534942627\n",
      "Step: 3340, train/loss: 0.00019999999494757503\n",
      "Step: 3340, train/grad_norm: 0.011986874043941498\n",
      "Step: 3340, train/learning_rate: 4.6025703341001645e-05\n",
      "Step: 3340, train/epoch: 0.7948595881462097\n",
      "Step: 3350, train/loss: 9.999999747378752e-05\n",
      "Step: 3350, train/grad_norm: 0.0239633247256279\n",
      "Step: 3350, train/learning_rate: 4.601380351232365e-05\n",
      "Step: 3350, train/epoch: 0.7972394227981567\n",
      "Step: 3360, train/loss: 0.000699999975040555\n",
      "Step: 3360, train/grad_norm: 4.52924650744535e-05\n",
      "Step: 3360, train/learning_rate: 4.600190368364565e-05\n",
      "Step: 3360, train/epoch: 0.7996192574501038\n",
      "Step: 3370, train/loss: 0.0\n",
      "Step: 3370, train/grad_norm: 0.00010975077020702884\n",
      "Step: 3370, train/learning_rate: 4.5990003854967654e-05\n",
      "Step: 3370, train/epoch: 0.801999032497406\n",
      "Step: 3380, train/loss: 9.999999747378752e-05\n",
      "Step: 3380, train/grad_norm: 1.9357059500180185e-05\n",
      "Step: 3380, train/learning_rate: 4.597810402628966e-05\n",
      "Step: 3380, train/epoch: 0.804378867149353\n",
      "Step: 3390, train/loss: 0.0\n",
      "Step: 3390, train/grad_norm: 0.060089483857154846\n",
      "Step: 3390, train/learning_rate: 4.596620783559047e-05\n",
      "Step: 3390, train/epoch: 0.8067587018013\n",
      "Step: 3400, train/loss: 9.999999747378752e-05\n",
      "Step: 3400, train/grad_norm: 4.103251558262855e-05\n",
      "Step: 3400, train/learning_rate: 4.595430800691247e-05\n",
      "Step: 3400, train/epoch: 0.8091384768486023\n",
      "Step: 3410, train/loss: 0.049300000071525574\n",
      "Step: 3410, train/grad_norm: 0.1094479039311409\n",
      "Step: 3410, train/learning_rate: 4.594240817823447e-05\n",
      "Step: 3410, train/epoch: 0.8115183115005493\n",
      "Step: 3420, train/loss: 0.0\n",
      "Step: 3420, train/grad_norm: 1.8605946024763398e-05\n",
      "Step: 3420, train/learning_rate: 4.5930508349556476e-05\n",
      "Step: 3420, train/epoch: 0.8138981461524963\n",
      "Step: 3430, train/loss: 0.0\n",
      "Step: 3430, train/grad_norm: 0.0004942311788909137\n",
      "Step: 3430, train/learning_rate: 4.591860852087848e-05\n",
      "Step: 3430, train/epoch: 0.8162779808044434\n",
      "Step: 3440, train/loss: 9.999999747378752e-05\n",
      "Step: 3440, train/grad_norm: 0.0007186648435890675\n",
      "Step: 3440, train/learning_rate: 4.590671233017929e-05\n",
      "Step: 3440, train/epoch: 0.8186577558517456\n",
      "Step: 3450, train/loss: 0.0\n",
      "Step: 3450, train/grad_norm: 0.13026171922683716\n",
      "Step: 3450, train/learning_rate: 4.589481250150129e-05\n",
      "Step: 3450, train/epoch: 0.8210375905036926\n",
      "Step: 3460, train/loss: 0.0\n",
      "Step: 3460, train/grad_norm: 0.001049443264491856\n",
      "Step: 3460, train/learning_rate: 4.5882912672823295e-05\n",
      "Step: 3460, train/epoch: 0.8234174251556396\n",
      "Step: 3470, train/loss: 0.00019999999494757503\n",
      "Step: 3470, train/grad_norm: 0.019838280975818634\n",
      "Step: 3470, train/learning_rate: 4.58710128441453e-05\n",
      "Step: 3470, train/epoch: 0.8257972598075867\n",
      "Step: 3480, train/loss: 0.0015999999595806003\n",
      "Step: 3480, train/grad_norm: 3.2786588235467207e-06\n",
      "Step: 3480, train/learning_rate: 4.58591130154673e-05\n",
      "Step: 3480, train/epoch: 0.8281770348548889\n",
      "Step: 3490, train/loss: 0.0\n",
      "Step: 3490, train/grad_norm: 3.5016324545722455e-05\n",
      "Step: 3490, train/learning_rate: 4.584721682476811e-05\n",
      "Step: 3490, train/epoch: 0.8305568695068359\n",
      "Step: 3500, train/loss: 0.0\n",
      "Step: 3500, train/grad_norm: 0.0007378786685876548\n",
      "Step: 3500, train/learning_rate: 4.5835316996090114e-05\n",
      "Step: 3500, train/epoch: 0.832936704158783\n",
      "Step: 3510, train/loss: 0.0\n",
      "Step: 3510, train/grad_norm: 2.5660425308160484e-05\n",
      "Step: 3510, train/learning_rate: 4.582341716741212e-05\n",
      "Step: 3510, train/epoch: 0.83531653881073\n",
      "Step: 3520, train/loss: 0.0\n",
      "Step: 3520, train/grad_norm: 1.0535930414334871e-05\n",
      "Step: 3520, train/learning_rate: 4.581151733873412e-05\n",
      "Step: 3520, train/epoch: 0.8376963138580322\n",
      "Step: 3530, train/loss: 0.0\n",
      "Step: 3530, train/grad_norm: 0.0003132943529635668\n",
      "Step: 3530, train/learning_rate: 4.579961751005612e-05\n",
      "Step: 3530, train/epoch: 0.8400761485099792\n",
      "Step: 3540, train/loss: 0.0\n",
      "Step: 3540, train/grad_norm: 4.85179907627753e-06\n",
      "Step: 3540, train/learning_rate: 4.578772131935693e-05\n",
      "Step: 3540, train/epoch: 0.8424559831619263\n",
      "Step: 3550, train/loss: 0.0\n",
      "Step: 3550, train/grad_norm: 3.066113276872784e-05\n",
      "Step: 3550, train/learning_rate: 4.5775821490678936e-05\n",
      "Step: 3550, train/epoch: 0.8448358178138733\n",
      "Step: 3560, train/loss: 0.0\n",
      "Step: 3560, train/grad_norm: 0.0014541191048920155\n",
      "Step: 3560, train/learning_rate: 4.576392166200094e-05\n",
      "Step: 3560, train/epoch: 0.8472155928611755\n",
      "Step: 3570, train/loss: 0.0\n",
      "Step: 3570, train/grad_norm: 6.012042831571307e-06\n",
      "Step: 3570, train/learning_rate: 4.575202183332294e-05\n",
      "Step: 3570, train/epoch: 0.8495954275131226\n",
      "Step: 3580, train/loss: 0.0\n",
      "Step: 3580, train/grad_norm: 0.0014873340260237455\n",
      "Step: 3580, train/learning_rate: 4.5740122004644945e-05\n",
      "Step: 3580, train/epoch: 0.8519752621650696\n",
      "Step: 3590, train/loss: 0.0\n",
      "Step: 3590, train/grad_norm: 3.7690183489758056e-06\n",
      "Step: 3590, train/learning_rate: 4.5728225813945755e-05\n",
      "Step: 3590, train/epoch: 0.8543550968170166\n",
      "Step: 3600, train/loss: 0.0\n",
      "Step: 3600, train/grad_norm: 9.568021596351173e-06\n",
      "Step: 3600, train/learning_rate: 4.571632598526776e-05\n",
      "Step: 3600, train/epoch: 0.8567348718643188\n",
      "Step: 3610, train/loss: 0.0\n",
      "Step: 3610, train/grad_norm: 4.339888255344704e-05\n",
      "Step: 3610, train/learning_rate: 4.570442615658976e-05\n",
      "Step: 3610, train/epoch: 0.8591147065162659\n",
      "Step: 3620, train/loss: 0.0\n",
      "Step: 3620, train/grad_norm: 8.661360334372148e-05\n",
      "Step: 3620, train/learning_rate: 4.5692526327911764e-05\n",
      "Step: 3620, train/epoch: 0.8614945411682129\n",
      "Step: 3630, train/loss: 0.0\n",
      "Step: 3630, train/grad_norm: 1.1928476851608139e-05\n",
      "Step: 3630, train/learning_rate: 4.568062649923377e-05\n",
      "Step: 3630, train/epoch: 0.8638743162155151\n",
      "Step: 3640, train/loss: 0.0\n",
      "Step: 3640, train/grad_norm: 2.3619613784831017e-06\n",
      "Step: 3640, train/learning_rate: 4.566873030853458e-05\n",
      "Step: 3640, train/epoch: 0.8662541508674622\n",
      "Step: 3650, train/loss: 0.00930000003427267\n",
      "Step: 3650, train/grad_norm: 129.94996643066406\n",
      "Step: 3650, train/learning_rate: 4.565683047985658e-05\n",
      "Step: 3650, train/epoch: 0.8686339855194092\n",
      "Step: 3660, train/loss: 0.0\n",
      "Step: 3660, train/grad_norm: 5.412600512499921e-05\n",
      "Step: 3660, train/learning_rate: 4.5644930651178584e-05\n",
      "Step: 3660, train/epoch: 0.8710138201713562\n",
      "Step: 3670, train/loss: 0.14630000293254852\n",
      "Step: 3670, train/grad_norm: 0.00012425427848938853\n",
      "Step: 3670, train/learning_rate: 4.5633030822500587e-05\n",
      "Step: 3670, train/epoch: 0.8733935952186584\n",
      "Step: 3680, train/loss: 0.002199999988079071\n",
      "Step: 3680, train/grad_norm: 0.0011184565955772996\n",
      "Step: 3680, train/learning_rate: 4.562113099382259e-05\n",
      "Step: 3680, train/epoch: 0.8757734298706055\n",
      "Step: 3690, train/loss: 0.00019999999494757503\n",
      "Step: 3690, train/grad_norm: 0.08407702296972275\n",
      "Step: 3690, train/learning_rate: 4.56092348031234e-05\n",
      "Step: 3690, train/epoch: 0.8781532645225525\n",
      "Step: 3700, train/loss: 0.0013000000035390258\n",
      "Step: 3700, train/grad_norm: 0.008000388741493225\n",
      "Step: 3700, train/learning_rate: 4.55973349744454e-05\n",
      "Step: 3700, train/epoch: 0.8805330991744995\n",
      "Step: 3710, train/loss: 0.0\n",
      "Step: 3710, train/grad_norm: 3.822318103630096e-05\n",
      "Step: 3710, train/learning_rate: 4.5585435145767406e-05\n",
      "Step: 3710, train/epoch: 0.8829128742218018\n",
      "Step: 3720, train/loss: 0.0\n",
      "Step: 3720, train/grad_norm: 2.1974690753268078e-05\n",
      "Step: 3720, train/learning_rate: 4.557353531708941e-05\n",
      "Step: 3720, train/epoch: 0.8852927088737488\n",
      "Step: 3730, train/loss: 0.0\n",
      "Step: 3730, train/grad_norm: 5.634620265482226e-06\n",
      "Step: 3730, train/learning_rate: 4.556163912639022e-05\n",
      "Step: 3730, train/epoch: 0.8876725435256958\n",
      "Step: 3740, train/loss: 0.0\n",
      "Step: 3740, train/grad_norm: 2.1856158127775416e-05\n",
      "Step: 3740, train/learning_rate: 4.554973929771222e-05\n",
      "Step: 3740, train/epoch: 0.8900523781776428\n",
      "Step: 3750, train/loss: 0.0\n",
      "Step: 3750, train/grad_norm: 0.0010830984683707356\n",
      "Step: 3750, train/learning_rate: 4.5537839469034225e-05\n",
      "Step: 3750, train/epoch: 0.8924321532249451\n",
      "Step: 3760, train/loss: 0.0\n",
      "Step: 3760, train/grad_norm: 1.24024600154371e-05\n",
      "Step: 3760, train/learning_rate: 4.552593964035623e-05\n",
      "Step: 3760, train/epoch: 0.8948119878768921\n",
      "Step: 3770, train/loss: 0.08810000121593475\n",
      "Step: 3770, train/grad_norm: 1.2715105185634457e-05\n",
      "Step: 3770, train/learning_rate: 4.551403981167823e-05\n",
      "Step: 3770, train/epoch: 0.8971918225288391\n",
      "Step: 3780, train/loss: 0.018699999898672104\n",
      "Step: 3780, train/grad_norm: 6.330977339530364e-05\n",
      "Step: 3780, train/learning_rate: 4.550214362097904e-05\n",
      "Step: 3780, train/epoch: 0.8995716571807861\n",
      "Step: 3790, train/loss: 0.0003000000142492354\n",
      "Step: 3790, train/grad_norm: 3.294655107310973e-05\n",
      "Step: 3790, train/learning_rate: 4.5490243792301044e-05\n",
      "Step: 3790, train/epoch: 0.9019514322280884\n",
      "Step: 3800, train/loss: 0.0\n",
      "Step: 3800, train/grad_norm: 1.4022387404111214e-05\n",
      "Step: 3800, train/learning_rate: 4.547834396362305e-05\n",
      "Step: 3800, train/epoch: 0.9043312668800354\n",
      "Step: 3810, train/loss: 0.0\n",
      "Step: 3810, train/grad_norm: 0.0002065808221232146\n",
      "Step: 3810, train/learning_rate: 4.546644413494505e-05\n",
      "Step: 3810, train/epoch: 0.9067111015319824\n",
      "Step: 3820, train/loss: 0.0006000000284984708\n",
      "Step: 3820, train/grad_norm: 7.722570444457233e-05\n",
      "Step: 3820, train/learning_rate: 4.545454430626705e-05\n",
      "Step: 3820, train/epoch: 0.9090909361839294\n",
      "Step: 3830, train/loss: 0.0\n",
      "Step: 3830, train/grad_norm: 3.5144885259796865e-06\n",
      "Step: 3830, train/learning_rate: 4.544264811556786e-05\n",
      "Step: 3830, train/epoch: 0.9114707112312317\n",
      "Step: 3840, train/loss: 0.004399999976158142\n",
      "Step: 3840, train/grad_norm: 3.6366298445500433e-05\n",
      "Step: 3840, train/learning_rate: 4.5430748286889866e-05\n",
      "Step: 3840, train/epoch: 0.9138505458831787\n",
      "Step: 3850, train/loss: 0.0\n",
      "Step: 3850, train/grad_norm: 3.2497635402251035e-05\n",
      "Step: 3850, train/learning_rate: 4.541884845821187e-05\n",
      "Step: 3850, train/epoch: 0.9162303805351257\n",
      "Step: 3860, train/loss: 0.0\n",
      "Step: 3860, train/grad_norm: 3.547511005308479e-05\n",
      "Step: 3860, train/learning_rate: 4.540694862953387e-05\n",
      "Step: 3860, train/epoch: 0.9186102151870728\n",
      "Step: 3870, train/loss: 0.0\n",
      "Step: 3870, train/grad_norm: 6.173481779114809e-06\n",
      "Step: 3870, train/learning_rate: 4.5395048800855875e-05\n",
      "Step: 3870, train/epoch: 0.920989990234375\n",
      "Step: 3880, train/loss: 0.0\n",
      "Step: 3880, train/grad_norm: 1.5232224541250616e-05\n",
      "Step: 3880, train/learning_rate: 4.5383152610156685e-05\n",
      "Step: 3880, train/epoch: 0.923369824886322\n",
      "Step: 3890, train/loss: 0.04879999905824661\n",
      "Step: 3890, train/grad_norm: 4.888061084784567e-06\n",
      "Step: 3890, train/learning_rate: 4.537125278147869e-05\n",
      "Step: 3890, train/epoch: 0.925749659538269\n",
      "Step: 3900, train/loss: 0.0\n",
      "Step: 3900, train/grad_norm: 3.890269908879418e-06\n",
      "Step: 3900, train/learning_rate: 4.535935295280069e-05\n",
      "Step: 3900, train/epoch: 0.9281294345855713\n",
      "Step: 3910, train/loss: 0.0\n",
      "Step: 3910, train/grad_norm: 0.0008143681334331632\n",
      "Step: 3910, train/learning_rate: 4.5347453124122694e-05\n",
      "Step: 3910, train/epoch: 0.9305092692375183\n",
      "Step: 3920, train/loss: 0.0\n",
      "Step: 3920, train/grad_norm: 0.00018718518549576402\n",
      "Step: 3920, train/learning_rate: 4.53355532954447e-05\n",
      "Step: 3920, train/epoch: 0.9328891038894653\n",
      "Step: 3930, train/loss: 0.0\n",
      "Step: 3930, train/grad_norm: 5.840112135047093e-05\n",
      "Step: 3930, train/learning_rate: 4.532365710474551e-05\n",
      "Step: 3930, train/epoch: 0.9352689385414124\n",
      "Step: 3940, train/loss: 0.00039999998989515007\n",
      "Step: 3940, train/grad_norm: 1.9512222934281453e-05\n",
      "Step: 3940, train/learning_rate: 4.531175727606751e-05\n",
      "Step: 3940, train/epoch: 0.9376487135887146\n",
      "Step: 3950, train/loss: 0.0\n",
      "Step: 3950, train/grad_norm: 0.003075603861361742\n",
      "Step: 3950, train/learning_rate: 4.529985744738951e-05\n",
      "Step: 3950, train/epoch: 0.9400285482406616\n",
      "Step: 3960, train/loss: 0.00019999999494757503\n",
      "Step: 3960, train/grad_norm: 0.0008318989421240985\n",
      "Step: 3960, train/learning_rate: 4.5287957618711516e-05\n",
      "Step: 3960, train/epoch: 0.9424083828926086\n",
      "Step: 3970, train/loss: 0.0\n",
      "Step: 3970, train/grad_norm: 0.07574056833982468\n",
      "Step: 3970, train/learning_rate: 4.527605779003352e-05\n",
      "Step: 3970, train/epoch: 0.9447882175445557\n",
      "Step: 3980, train/loss: 0.035100001841783524\n",
      "Step: 3980, train/grad_norm: 2.7366979793441715e-06\n",
      "Step: 3980, train/learning_rate: 4.526416159933433e-05\n",
      "Step: 3980, train/epoch: 0.9471679925918579\n",
      "Step: 3990, train/loss: 0.0\n",
      "Step: 3990, train/grad_norm: 1.474197233619634e-06\n",
      "Step: 3990, train/learning_rate: 4.525226177065633e-05\n",
      "Step: 3990, train/epoch: 0.9495478272438049\n",
      "Step: 4000, train/loss: 0.0\n",
      "Step: 4000, train/grad_norm: 1.805365172913298e-05\n",
      "Step: 4000, train/learning_rate: 4.5240361941978335e-05\n",
      "Step: 4000, train/epoch: 0.951927661895752\n",
      "Step: 4010, train/loss: 9.999999747378752e-05\n",
      "Step: 4010, train/grad_norm: 2.7116420824313536e-06\n",
      "Step: 4010, train/learning_rate: 4.522846211330034e-05\n",
      "Step: 4010, train/epoch: 0.954307496547699\n",
      "Step: 4020, train/loss: 0.0\n",
      "Step: 4020, train/grad_norm: 0.00016070545825641602\n",
      "Step: 4020, train/learning_rate: 4.521656228462234e-05\n",
      "Step: 4020, train/epoch: 0.9566872715950012\n",
      "Step: 4030, train/loss: 0.0\n",
      "Step: 4030, train/grad_norm: 0.009714022278785706\n",
      "Step: 4030, train/learning_rate: 4.520466609392315e-05\n",
      "Step: 4030, train/epoch: 0.9590671062469482\n",
      "Step: 4040, train/loss: 0.0\n",
      "Step: 4040, train/grad_norm: 4.372241528471932e-05\n",
      "Step: 4040, train/learning_rate: 4.5192766265245155e-05\n",
      "Step: 4040, train/epoch: 0.9614469408988953\n",
      "Step: 4050, train/loss: 0.0\n",
      "Step: 4050, train/grad_norm: 7.249203122228209e-07\n",
      "Step: 4050, train/learning_rate: 4.518086643656716e-05\n",
      "Step: 4050, train/epoch: 0.9638267755508423\n",
      "Step: 4060, train/loss: 0.0\n",
      "Step: 4060, train/grad_norm: 8.9959692559205e-05\n",
      "Step: 4060, train/learning_rate: 4.516896660788916e-05\n",
      "Step: 4060, train/epoch: 0.9662065505981445\n",
      "Step: 4070, train/loss: 0.0\n",
      "Step: 4070, train/grad_norm: 0.0009964954806491733\n",
      "Step: 4070, train/learning_rate: 4.5157066779211164e-05\n",
      "Step: 4070, train/epoch: 0.9685863852500916\n",
      "Step: 4080, train/loss: 0.0\n",
      "Step: 4080, train/grad_norm: 1.5626075082764146e-06\n",
      "Step: 4080, train/learning_rate: 4.5145170588511974e-05\n",
      "Step: 4080, train/epoch: 0.9709662199020386\n",
      "Step: 4090, train/loss: 0.0\n",
      "Step: 4090, train/grad_norm: 1.1533284123288468e-06\n",
      "Step: 4090, train/learning_rate: 4.513327075983398e-05\n",
      "Step: 4090, train/epoch: 0.9733460545539856\n",
      "Step: 4100, train/loss: 0.0\n",
      "Step: 4100, train/grad_norm: 7.697259570704773e-05\n",
      "Step: 4100, train/learning_rate: 4.512137093115598e-05\n",
      "Step: 4100, train/epoch: 0.9757258296012878\n",
      "Step: 4110, train/loss: 0.0\n",
      "Step: 4110, train/grad_norm: 2.890149517043028e-05\n",
      "Step: 4110, train/learning_rate: 4.510947110247798e-05\n",
      "Step: 4110, train/epoch: 0.9781056642532349\n",
      "Step: 4120, train/loss: 9.999999747378752e-05\n",
      "Step: 4120, train/grad_norm: 2.3230145984598494e-07\n",
      "Step: 4120, train/learning_rate: 4.5097571273799986e-05\n",
      "Step: 4120, train/epoch: 0.9804854989051819\n",
      "Step: 4130, train/loss: 0.0010999999940395355\n",
      "Step: 4130, train/grad_norm: 0.0010189451277256012\n",
      "Step: 4130, train/learning_rate: 4.5085675083100796e-05\n",
      "Step: 4130, train/epoch: 0.9828652739524841\n",
      "Step: 4140, train/loss: 0.0\n",
      "Step: 4140, train/grad_norm: 2.664122291662352e-07\n",
      "Step: 4140, train/learning_rate: 4.50737752544228e-05\n",
      "Step: 4140, train/epoch: 0.9852451086044312\n",
      "Step: 4150, train/loss: 0.0\n",
      "Step: 4150, train/grad_norm: 2.669952721134905e-07\n",
      "Step: 4150, train/learning_rate: 4.50618754257448e-05\n",
      "Step: 4150, train/epoch: 0.9876249432563782\n",
      "Step: 4160, train/loss: 0.0\n",
      "Step: 4160, train/grad_norm: 2.495870830898639e-06\n",
      "Step: 4160, train/learning_rate: 4.5049975597066805e-05\n",
      "Step: 4160, train/epoch: 0.9900047779083252\n",
      "Step: 4170, train/loss: 0.0\n",
      "Step: 4170, train/grad_norm: 7.0745954872109e-08\n",
      "Step: 4170, train/learning_rate: 4.503807576838881e-05\n",
      "Step: 4170, train/epoch: 0.9923845529556274\n",
      "Step: 4180, train/loss: 0.0\n",
      "Step: 4180, train/grad_norm: 2.1536548899803165e-07\n",
      "Step: 4180, train/learning_rate: 4.502617957768962e-05\n",
      "Step: 4180, train/epoch: 0.9947643876075745\n",
      "Step: 4190, train/loss: 0.13910000026226044\n",
      "Step: 4190, train/grad_norm: 2.5103723601205274e-05\n",
      "Step: 4190, train/learning_rate: 4.501427974901162e-05\n",
      "Step: 4190, train/epoch: 0.9971442222595215\n",
      "Step: 4200, train/loss: 0.0\n",
      "Step: 4200, train/grad_norm: 3.55766610482533e-06\n",
      "Step: 4200, train/learning_rate: 4.5002379920333624e-05\n",
      "Step: 4200, train/epoch: 0.9995240569114685\n",
      "Step: 4202, eval/loss: 0.005497355479747057\n",
      "Step: 4202, eval/accuracy: 0.9994446635246277\n",
      "Step: 4202, eval/f1: 0.9994134902954102\n",
      "Step: 4202, eval/runtime: 7929.2236328125\n",
      "Step: 4202, eval/samples_per_second: 0.9079999923706055\n",
      "Step: 4202, eval/steps_per_second: 0.11400000005960464\n",
      "Step: 4202, train/epoch: 1.0\n",
      "Step: 4210, train/loss: 0.0\n",
      "Step: 4210, train/grad_norm: 4.2275360101484694e-08\n",
      "Step: 4210, train/learning_rate: 4.499048009165563e-05\n",
      "Step: 4210, train/epoch: 1.0019038915634155\n",
      "Step: 4220, train/loss: 0.0\n",
      "Step: 4220, train/grad_norm: 8.109643658826826e-07\n",
      "Step: 4220, train/learning_rate: 4.497858026297763e-05\n",
      "Step: 4220, train/epoch: 1.0042836666107178\n",
      "Step: 4230, train/loss: 0.0\n",
      "Step: 4230, train/grad_norm: 3.4104054066119716e-05\n",
      "Step: 4230, train/learning_rate: 4.496668407227844e-05\n",
      "Step: 4230, train/epoch: 1.00666344165802\n",
      "Step: 4240, train/loss: 0.0\n",
      "Step: 4240, train/grad_norm: 0.0001844822836574167\n",
      "Step: 4240, train/learning_rate: 4.495478424360044e-05\n",
      "Step: 4240, train/epoch: 1.0090433359146118\n",
      "Step: 4250, train/loss: 0.0\n",
      "Step: 4250, train/grad_norm: 1.1270364552729006e-07\n",
      "Step: 4250, train/learning_rate: 4.4942884414922446e-05\n",
      "Step: 4250, train/epoch: 1.011423110961914\n",
      "Step: 4260, train/loss: 0.0\n",
      "Step: 4260, train/grad_norm: 0.000123850186355412\n",
      "Step: 4260, train/learning_rate: 4.493098458624445e-05\n",
      "Step: 4260, train/epoch: 1.0138030052185059\n",
      "Step: 4270, train/loss: 0.0\n",
      "Step: 4270, train/grad_norm: 9.85090650829079e-07\n",
      "Step: 4270, train/learning_rate: 4.491908475756645e-05\n",
      "Step: 4270, train/epoch: 1.016182780265808\n",
      "Step: 4280, train/loss: 0.0\n",
      "Step: 4280, train/grad_norm: 7.53973927203333e-06\n",
      "Step: 4280, train/learning_rate: 4.490718856686726e-05\n",
      "Step: 4280, train/epoch: 1.0185625553131104\n",
      "Step: 4290, train/loss: 0.0\n",
      "Step: 4290, train/grad_norm: 8.809216751615168e-07\n",
      "Step: 4290, train/learning_rate: 4.4895288738189265e-05\n",
      "Step: 4290, train/epoch: 1.0209424495697021\n",
      "Step: 4300, train/loss: 0.0\n",
      "Step: 4300, train/grad_norm: 2.6135976938235217e-08\n",
      "Step: 4300, train/learning_rate: 4.488338890951127e-05\n",
      "Step: 4300, train/epoch: 1.0233222246170044\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.summary.summary_iterator import summary_iterator\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Construct the logs directory path\n",
    "logs_directory = os.path.join('./', project_name, 'logs')\n",
    "file_pattern = 'events.out.tfevents.*'\n",
    "\n",
    "# Retrieve all event files matching the pattern\n",
    "event_files = glob.glob(os.path.join(logs_directory, file_pattern))\n",
    "\n",
    "# Function to print out TensorBoard event logs\n",
    "def print_events_from_file(event_files):\n",
    "    for event_file in event_files:\n",
    "        print(f\"Reading events from file: {event_file}\")\n",
    "        try:\n",
    "            for e in summary_iterator(event_file):\n",
    "                for v in e.summary.value:\n",
    "                    if v.HasField('simple_value'):\n",
    "                        print(f\"Step: {e.step}, {v.tag}: {v.simple_value}\")\n",
    "        except Exception as e:  # Just in case the event file is not readable\n",
    "            print(f\"Failed to read {event_file}: {e}\")\n",
    "\n",
    "print_events_from_file(event_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ffd0164-0f41-4bbe-b905-7dda371dfd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Step Train Loss Eval Loss  Accuracy        F1\n",
      "2   4202   0.000000  0.005497  0.999445  0.999413\n",
      "1   8404   0.000000  0.002820  0.999306  0.999267\n",
      "0  12606   0.000000  0.002160  0.999445  0.999414\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.python.summary.summary_iterator import summary_iterator\n",
    "\n",
    "logs_directory = os.path.join('./', project_name, 'logs')\n",
    "file_pattern = 'events.out.tfevents.*'\n",
    "\n",
    "event_files = glob.glob(os.path.join(logs_directory, file_pattern))\n",
    "\n",
    "def extract_metrics(event_files):\n",
    "    data = []\n",
    "    last_train_loss = None\n",
    "\n",
    "    for event_file in event_files:\n",
    "        for e in summary_iterator(event_file):\n",
    "            for v in e.summary.value:\n",
    "                if v.HasField('simple_value'):\n",
    "                    step = e.step\n",
    "                    metric_name = v.tag.split('/')[-1]\n",
    "                    metric_value = v.simple_value\n",
    "\n",
    "                    formatted_value = f\"{metric_value:.6f}\"\n",
    "\n",
    "                    if 'train/loss' in v.tag:\n",
    "                        last_train_loss = formatted_value\n",
    "\n",
    "                    if 'eval' in v.tag:\n",
    "                        entry = next((item for item in data if item['Step'] == step), None)\n",
    "                        if not entry:\n",
    "                            entry = {'Step': step, 'Train Loss': last_train_loss, 'Eval Loss': None, 'Accuracy': None, 'F1': None}\n",
    "                            data.append(entry)\n",
    "                        if 'loss' in v.tag:\n",
    "                            entry['Eval Loss'] = formatted_value\n",
    "                        elif 'accuracy' in v.tag:\n",
    "                            entry['Accuracy'] = formatted_value\n",
    "                        elif 'f1' in v.tag:\n",
    "                            entry['F1'] = formatted_value\n",
    "\n",
    "    return data\n",
    "\n",
    "metrics_data = extract_metrics(event_files)\n",
    "\n",
    "df = pd.DataFrame(metrics_data)\n",
    "df = df.sort_values(by='Step')\n",
    "\n",
    "file_path = \"./images/\"+model_name+\"_Checkpoint_Data.csv\"\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37cc5f92-d47f-4356-8fad-d9b64b6f5361",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Checkpoint Step: checkpoint-12606\n",
      "Step             12606\n",
      "Train Loss    0.000000\n",
      "Eval Loss      0.00216\n",
      "Accuracy      0.999445\n",
      "F1            0.999414\n",
      "Rank Sum           3.0\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df.fillna({\n",
    "    'Eval Loss': float('inf'),\n",
    "    'Accuracy': 0,\n",
    "    'F1': 0\n",
    "}, inplace=True)\n",
    "\n",
    "df['Eval Loss'] = df['Eval Loss'].astype(float)\n",
    "df['Accuracy'] = df['Accuracy'].astype(float)\n",
    "df['F1'] = df['F1'].astype(float)\n",
    "\n",
    "df['Eval Loss Rank'] = df['Eval Loss'].rank(method='min', ascending=True)\n",
    "df['Accuracy Rank'] = df['Accuracy'].rank(method='min', ascending=False)\n",
    "df['F1 Rank'] = df['F1'].rank(method='min', ascending=False)\n",
    "\n",
    "df['Rank Sum'] = df['Eval Loss Rank'] + df['Accuracy Rank'] + df['F1 Rank']\n",
    "\n",
    "best_checkpoint = df.loc[df['Rank Sum'].idxmin()]\n",
    "\n",
    "checkpoint_folder_name = f\"checkpoint-{best_checkpoint['Step']}\"\n",
    "print(f\"Best Checkpoint Step: {checkpoint_folder_name}\")\n",
    "print(best_checkpoint[['Step', 'Train Loss', 'Eval Loss', 'Accuracy', 'F1', 'Rank Sum']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3c3999-8c32-4a25-aaca-2ec9036b69ed",
   "metadata": {},
   "source": [
    "### Run TensorBoard\n",
    "tensorboard --logdir=~/kuk/Praxis/praxis-Llama-2-7b-hf-small-finetune/logs --host=0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f36022-dc73-492f-998c-43e5ed1a6f46",
   "metadata": {},
   "source": [
    "### PAUSE SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "444ae65b-241c-47ef-bbc2-81f3a2ce50e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My flag to pause the script, set to True to pause\n",
    "pause_script = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8be3672-68e0-44f8-b8b1-31290665658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        print(\"Script Paused\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb3b5ed2-5320-49c0-9e61-90d6f0942f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pause_script:\n",
    "    raise StopExecution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19be6e26-d888-4da0-b3f8-836d68ac2051",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5111194-5eeb-4104-8df0-f977a6b716e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6241b019d9d94afd96abad8185040ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-70b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    num_labels=2,\n",
    "    token=access_token,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    checkpoint,\n",
    "    token=access_token,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "530fa55f-c8c4-485f-a093-09bf2175d586",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "test_checkpoint_name = checkpoint_folder_name\n",
    "ft_model = PeftModel.from_pretrained(base_model, project_name+'/'+test_checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27354c4f-95cc-4d1f-ac64-c5e6b4a842ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#     ft_model = torch.nn.DataParallel(ft_model)\n",
    "\n",
    "# ft_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eee9ab6f-a7d5-4dd5-9436-2f6f6e2bffaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef21ca5b0d744a0ba69f451303723ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "processed_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for record in tqdm(tokenized_test_ds):\n",
    "\n",
    "        eval_prompt = record['article']\n",
    "        model_input = tokenize_fn({'article': eval_prompt})\n",
    "\n",
    "        # model_input = {k: v.to('cuda') for k, v in model_input.items()}\n",
    "        \n",
    "        outputs = ft_model(**model_input)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        prediction = logits[0].argmax(-1).item()  # Use .item() to get a Python number\n",
    "        processed_predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f06a2c-5ded-4da0-8232-145383967c9d",
   "metadata": {},
   "source": [
    "### Accuracy and F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e891bc5-55e0-4c43-a035-0edc37dcb6e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "true_articles = tokenized_test_ds['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bf33824-d7cc-4d22-af4d-7d44e569c2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9997223379147577\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(true_articles, processed_predictions)\n",
    "print(\"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b37dc24-3bc0-48a0-ace9-a360bae91169",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(true_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a29ebf68-632f-49d1-b903-407ff05b5569",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(processed_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb4d8350-a8d1-4853-a9f8-74ae9ea36bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.9997070592736664\n"
     ]
    }
   ],
   "source": [
    "f1_score = f1_score(true_articles, processed_predictions, average='macro')\n",
    "print(\"f1_score:\", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcaac14-2b83-4c24-b83a-f6d0e73a2d2a",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57e64afb-e3ee-4c79-8228-b2d79806229e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(tokenized_test_ds['label'], processed_predictions)\n",
    "\n",
    "labels = ['human', 'machine']\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af7d6f55-fb4f-4195-ac3f-93852ecd4b65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook praxis-Llama-2-70b-hf-small-finetune-v11.ipynb to html\n",
      "[NbConvertApp] WARNING | Alternative text is missing on 1 image(s).\n",
      "[NbConvertApp] Writing 730053 bytes to html/praxis-Llama-2-70b-hf-small-finetune-v11.html\n"
     ]
    }
   ],
   "source": [
    "file_name = f\"{project_name}.ipynb\"\n",
    "html_file_name = f\"{file_name.replace('.ipynb', '.html')}\"\n",
    "\n",
    "command = f\"jupyter nbconvert '{file_name}' --to html --output-dir './html' --output '{html_file_name}'\"\n",
    "get_ipython().system(command)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
